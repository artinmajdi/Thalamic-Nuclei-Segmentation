*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
(0/456) train vimp2_845_05312013_VZ
(1/456) train vimp2_823_05202013_AJ
(2/456) train vimp2_915_07112013_LC
(3/456) train vimp2_901_07052013_AS
(4/456) train vimp2_ctrl_911_07082013_TTO
(5/456) train vimp2_ctrl_925_07152013_LS
(6/456) train vimp2_869_06142013_BL
(7/456) train vimp2_ANON724_03272013
(8/456) train vimp2_819_05172013_DS
(9/456) train vimp2_ctrl_918_07112013_TQ
(10/456) train vimp2_ctrl_902_07052013_SI
(11/456) train vimp2_ANON606_20130110
(12/456) train vimp2_943_07242013_PA
(13/456) train vimp2_824_05212013_JS
(14/456) train vimp2_ANON624_20130117
(15/456) train vimp2_ctrl_920_07122013_SW
(16/456) train vimp2_884_06272013_TS
(17/456) train vimp2_668_02282013_CD
(18/456) train vimp2_964_08092013_TG
(19/456) train vimp2_ctrl_921_07122013_MP
(20/456) train vimp2_972_08152013_DC
(21/456) train vimp2_988_08302013_CB
(22/456) train vimp2_ANON702_03152013
(23/456) train vimp2_ANON714_03222013
(24/456) train vimp2_972_08152013_DC_Aug0_Rot_-1_sd0
(25/456) train vimp2_972_08152013_DC_Aug0_Rot_1_sd2
(26/456) train vimp2_972_08152013_DC_Aug0_Rot_-7_sd1
(27/456) train vimp2_972_08152013_DC_Aug1_Rot_2_sd0
(28/456) train vimp2_972_08152013_DC_Aug1_Rot_5_sd1
(29/456) train vimp2_972_08152013_DC_Aug1_Rot_6_sd2
(30/456) train vimp2_972_08152013_DC_Aug2_Rot_-3_sd1
(31/456) train vimp2_972_08152013_DC_Aug2_Rot_-3_sd2
(32/456) train vimp2_972_08152013_DC_Aug2_Rot_-5_sd0
(33/456) train vimp2_972_08152013_DC_Aug3_Rot_2_sd1
(34/456) train vimp2_972_08152013_DC_Aug3_Rot_-3_sd0
(35/456) train vimp2_972_08152013_DC_Aug3_Rot_-6_sd2
(36/456) train vimp2_972_08152013_DC_Aug4_Rot_-2_sd2
(37/456) train vimp2_972_08152013_DC_Aug4_Rot_3_sd0
(38/456) train vimp2_972_08152013_DC_Aug4_Rot_6_sd1
(39/456) train vimp2_972_08152013_DC_Aug5_Rot_-1_sd2
(40/456) train vimp2_972_08152013_DC_Aug5_Rot_7_sd0
(41/456) train vimp2_972_08152013_DC_Aug5_Rot_7_sd1
(42/456) train vimp2_988_08302013_CB_Aug0_Rot_-3_sd2
(43/456) train vimp2_988_08302013_CB_Aug0_Rot_4_sd0
(44/456) train vimp2_988_08302013_CB_Aug0_Rot_-5_sd1
(45/456) train vimp2_988_08302013_CB_Aug1_Rot_-2_sd2
(46/456) train vimp2_988_08302013_CB_Aug1_Rot_-3_sd0
(47/456) train vimp2_988_08302013_CB_Aug1_Rot_-6_sd1
(48/456) train vimp2_988_08302013_CB_Aug2_Rot_-2_sd0
(49/456) train vimp2_988_08302013_CB_Aug2_Rot_4_sd1
(50/456) train vimp2_988_08302013_CB_Aug2_Rot_-5_sd2
(51/456) train vimp2_988_08302013_CB_Aug3_Rot_2_sd2
(52/456) train vimp2_988_08302013_CB_Aug3_Rot_-3_sd0
(53/456) train vimp2_988_08302013_CB_Aug3_Rot_3_sd1
(54/456) train vimp2_988_08302013_CB_Aug4_Rot_-2_sd0
(55/456) train vimp2_988_08302013_CB_Aug4_Rot_-6_sd1
(56/456) train vimp2_988_08302013_CB_Aug4_Rot_7_sd2
(57/456) train vimp2_988_08302013_CB_Aug5_Rot_-1_sd0
(58/456) train vimp2_988_08302013_CB_Aug5_Rot_-6_sd1
(59/456) train vimp2_988_08302013_CB_Aug5_Rot_7_sd2
(60/456) train vimp2_ANON702_03152013_Aug0_Rot_-2_sd1
(61/456) train vimp2_ANON702_03152013_Aug0_Rot_-3_sd2
(62/456) train vimp2_ANON702_03152013_Aug0_Rot_-4_sd0
(63/456) train vimp2_ANON702_03152013_Aug1_Rot_-4_sd1
(64/456) train vimp2_ANON702_03152013_Aug1_Rot_-5_sd2
(65/456) train vimp2_ANON702_03152013_Aug1_Rot_-7_sd0
(66/456) train vimp2_ANON702_03152013_Aug2_Rot_6_sd0
(67/456) train vimp2_ANON702_03152013_Aug2_Rot_6_sd2
(68/456) train vimp2_ANON702_03152013_Aug2_Rot_-7_sd1
(69/456) train vimp2_ANON702_03152013_Aug3_Rot_-1_sd2
(70/456) train vimp2_ANON702_03152013_Aug3_Rot_-3_sd0
(71/456) train vimp2_ANON702_03152013_Aug3_Rot_-6_sd1
(72/456) train vimp2_ANON702_03152013_Aug4_Rot_-2_sd0
(73/456) train vimp2_ANON702_03152013_Aug4_Rot_-3_sd2
(74/456) train vimp2_ANON702_03152013_Aug4_Rot_-7_sd1
(75/456) train vimp2_ANON702_03152013_Aug5_Rot_3_sd0
(76/456) train vimp2_ANON702_03152013_Aug5_Rot_3_sd2
(77/456) train vimp2_ANON702_03152013_Aug5_Rot_4_sd1
(78/456) train vimp2_ANON714_03222013_Aug0_Rot_-1_sd2
(79/456) train vimp2_ANON714_03222013_Aug0_Rot_-2_sd1
(80/456) train vimp2_ANON714_03222013_Aug0_Rot_4_sd0
(81/456) train vimp2_ANON714_03222013_Aug1_Rot_0_sd0
(82/456) train vimp2_ANON714_03222013_Aug1_Rot_1_sd1
(83/456) train vimp2_ANON714_03222013_Aug1_Rot_-6_sd2
(84/456) train vimp2_ANON714_03222013_Aug2_Rot_-2_sd0
(85/456) train vimp2_ANON714_03222013_Aug2_Rot_4_sd1
(86/456) train vimp2_ANON714_03222013_Aug2_Rot_6_sd2
(87/456) train vimp2_ANON714_03222013_Aug3_Rot_2_sd0
(88/456) train vimp2_ANON714_03222013_Aug3_Rot_-3_sd2
(89/456) train vimp2_ANON714_03222013_Aug3_Rot_-7_sd1
(90/456) train vimp2_ANON714_03222013_Aug4_Rot_1_sd0
(91/456) train vimp2_ANON714_03222013_Aug4_Rot_-2_sd1
(92/456) train vimp2_ANON714_03222013_Aug4_Rot_4_sd2
(93/456) train vimp2_ANON714_03222013_Aug5_Rot_1_sd0
(94/456) train vimp2_ANON714_03222013_Aug5_Rot_6_sd1
(95/456) train vimp2_ANON714_03222013_Aug5_Rot_-7_sd2
(96/456) train vimp2_668_02282013_CD_Aug0_Rot_7_sd0
(97/456) train vimp2_668_02282013_CD_Aug1_Rot_-1_sd0
(98/456) train vimp2_668_02282013_CD_Aug2_Rot_-4_sd0
(99/456) train vimp2_668_02282013_CD_Aug3_Rot_3_sd0
(100/456) train vimp2_668_02282013_CD_Aug4_Rot_-5_sd0
(101/456) train vimp2_668_02282013_CD_Aug5_Rot_-6_sd0
(102/456) train vimp2_819_05172013_DS_Aug0_Rot_1_sd0
(103/456) train vimp2_819_05172013_DS_Aug1_Rot_5_sd0
(104/456) train vimp2_819_05172013_DS_Aug2_Rot_-4_sd0
(105/456) train vimp2_819_05172013_DS_Aug3_Rot_2_sd0
(106/456) train vimp2_819_05172013_DS_Aug4_Rot_5_sd0
(107/456) train vimp2_819_05172013_DS_Aug5_Rot_4_sd0
(108/456) train vimp2_823_05202013_AJ_Aug0_Rot_-7_sd0
(109/456) train vimp2_823_05202013_AJ_Aug1_Rot_7_sd0
(110/456) train vimp2_823_05202013_AJ_Aug2_Rot_3_sd0
(111/456) train vimp2_823_05202013_AJ_Aug3_Rot_-5_sd0
(112/456) train vimp2_823_05202013_AJ_Aug4_Rot_-3_sd0
(113/456) train vimp2_823_05202013_AJ_Aug5_Rot_1_sd0
(114/456) train vimp2_824_05212013_JS_Aug0_Rot_-2_sd0
(115/456) train vimp2_824_05212013_JS_Aug1_Rot_6_sd0
(116/456) train vimp2_824_05212013_JS_Aug2_Rot_1_sd0
(117/456) train vimp2_824_05212013_JS_Aug3_Rot_-4_sd0
(118/456) train vimp2_824_05212013_JS_Aug4_Rot_7_sd0
(119/456) train vimp2_824_05212013_JS_Aug5_Rot_2_sd0
(120/456) train vimp2_845_05312013_VZ_Aug0_Rot_1_sd0
(121/456) train vimp2_845_05312013_VZ_Aug1_Rot_5_sd0
(122/456) train vimp2_845_05312013_VZ_Aug2_Rot_-6_sd0
(123/456) train vimp2_845_05312013_VZ_Aug3_Rot_3_sd0
(124/456) train vimp2_845_05312013_VZ_Aug4_Rot_-2_sd0
(125/456) train vimp2_845_05312013_VZ_Aug5_Rot_5_sd0
(126/456) train vimp2_869_06142013_BL_Aug0_Rot_-2_sd0
(127/456) train vimp2_869_06142013_BL_Aug1_Rot_-4_sd0
(128/456) train vimp2_869_06142013_BL_Aug2_Rot_-1_sd0
(129/456) train vimp2_869_06142013_BL_Aug3_Rot_3_sd0
(130/456) train vimp2_869_06142013_BL_Aug4_Rot_3_sd0
(131/456) train vimp2_869_06142013_BL_Aug5_Rot_1_sd0
(132/456) train vimp2_884_06272013_TS_Aug0_Rot_-7_sd0
(133/456) train vimp2_884_06272013_TS_Aug1_Rot_7_sd0
(134/456) train vimp2_884_06272013_TS_Aug2_Rot_-5_sd0
(135/456) train vimp2_884_06272013_TS_Aug3_Rot_-2_sd0
(136/456) train vimp2_884_06272013_TS_Aug4_Rot_6_sd0
(137/456) train vimp2_884_06272013_TS_Aug5_Rot_-1_sd0
(138/456) train vimp2_901_07052013_AS_Aug0_Rot_-4_sd0
(139/456) train vimp2_901_07052013_AS_Aug1_Rot_-2_sd0
(140/456) train vimp2_901_07052013_AS_Aug2_Rot_1_sd0
(141/456) train vimp2_901_07052013_AS_Aug3_Rot_2_sd0
(142/456) train vimp2_901_07052013_AS_Aug4_Rot_-7_sd0
(143/456) train vimp2_901_07052013_AS_Aug5_Rot_-5_sd0
(144/456) train vimp2_915_07112013_LC_Aug0_Rot_-2_sd0
(145/456) train vimp2_915_07112013_LC_Aug1_Rot_4_sd0
(146/456) train vimp2_915_07112013_LC_Aug2_Rot_-2_sd0
(147/456) train vimp2_915_07112013_LC_Aug3_Rot_-1_sd0
(148/456) train vimp2_915_07112013_LC_Aug4_Rot_1_sd0
(149/456) train vimp2_915_07112013_LC_Aug5_Rot_4_sd0
(150/456) train vimp2_943_07242013_PA_Aug0_Rot_-5_sd0
(151/456) train vimp2_943_07242013_PA_Aug1_Rot_-7_sd0
(152/456) train vimp2_943_07242013_PA_Aug2_Rot_-4_sd0
(153/456) train vimp2_943_07242013_PA_Aug3_Rot_6_sd0
(154/456) train vimp2_943_07242013_PA_Aug4_Rot_6_sd0
(155/456) train vimp2_943_07242013_PA_Aug5_Rot_5_sd0
(156/456) train vimp2_964_08092013_TG_Aug0_Rot_5_sd0
(157/456) train vimp2_964_08092013_TG_Aug1_Rot_-4_sd0
(158/456) train vimp2_964_08092013_TG_Aug2_Rot_7_sd0
(159/456) train vimp2_964_08092013_TG_Aug3_Rot_-1_sd0
(160/456) train vimp2_964_08092013_TG_Aug4_Rot_-1_sd0
(161/456) train vimp2_964_08092013_TG_Aug5_Rot_2_sd0
(162/456) train vimp2_ANON606_20130110_Aug0_Rot_1_sd0
(163/456) train vimp2_ANON606_20130110_Aug1_Rot_2_sd0
(164/456) train vimp2_ANON606_20130110_Aug2_Rot_-5_sd0
(165/456) train vimp2_ANON606_20130110_Aug3_Rot_-5_sd0
(166/456) train vimp2_ANON606_20130110_Aug4_Rot_3_sd0
(167/456) train vimp2_ANON606_20130110_Aug5_Rot_-1_sd0
(168/456) train vimp2_ANON624_20130117_Aug0_Rot_1_sd0
(169/456) train vimp2_ANON624_20130117_Aug1_Rot_-2_sd0
(170/456) train vimp2_ANON624_20130117_Aug2_Rot_5_sd0
(171/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd0
(172/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd0
(173/456) train vimp2_ANON624_20130117_Aug5_Rot_3_sd0
(174/456) train vimp2_ANON724_03272013_Aug0_Rot_-4_sd0
(175/456) train vimp2_ANON724_03272013_Aug1_Rot_-3_sd0
(176/456) train vimp2_ANON724_03272013_Aug2_Rot_-4_sd0
(177/456) train vimp2_ANON724_03272013_Aug3_Rot_5_sd0
(178/456) train vimp2_ANON724_03272013_Aug4_Rot_5_sd0
(179/456) train vimp2_ANON724_03272013_Aug5_Rot_-6_sd0
(180/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_5_sd0
(181/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_7_sd0
(182/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_4_sd0
(183/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_-4_sd0
(184/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_3_sd0
(185/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_-7_sd0
(186/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_6_sd0
(187/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_1_sd0
(188/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_4_sd0
(189/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_-2_sd0
(190/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_6_sd0
(191/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_-2_sd0
(192/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_5_sd0
(193/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_-3_sd0
(194/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_-4_sd0
(195/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_1_sd0
(196/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_3_sd0
(197/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_7_sd0
(198/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_7_sd0
(199/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_-6_sd0
(200/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_-4_sd0
(201/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_1_sd0
(202/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-2_sd0
(203/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_4_sd0
(204/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_-1_sd0
(205/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_-3_sd0
(206/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_6_sd0
(207/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_3_sd0
(208/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_1_sd0
(209/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_-5_sd0
(210/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_-5_sd0
(211/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_-2_sd0
(212/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_5_sd0
(213/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_3_sd0
(214/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_4_sd0
(215/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-7_sd0
(216/456) train vimp2_668_02282013_CD_Aug0_Rot_-5_sd1
(217/456) train vimp2_668_02282013_CD_Aug1_Rot_-6_sd1
(218/456) train vimp2_668_02282013_CD_Aug2_Rot_3_sd1
(219/456) train vimp2_668_02282013_CD_Aug3_Rot_4_sd1
(220/456) train vimp2_668_02282013_CD_Aug4_Rot_-6_sd1
(221/456) train vimp2_668_02282013_CD_Aug5_Rot_-2_sd1
(222/456) train vimp2_819_05172013_DS_Aug0_Rot_-1_sd1
(223/456) train vimp2_819_05172013_DS_Aug1_Rot_3_sd1
(224/456) train vimp2_819_05172013_DS_Aug2_Rot_6_sd1
(225/456) train vimp2_819_05172013_DS_Aug3_Rot_3_sd1
(226/456) train vimp2_819_05172013_DS_Aug4_Rot_-7_sd1
(227/456) train vimp2_819_05172013_DS_Aug5_Rot_-5_sd1
(228/456) train vimp2_823_05202013_AJ_Aug0_Rot_-7_sd1
(229/456) train vimp2_823_05202013_AJ_Aug1_Rot_3_sd1
(230/456) train vimp2_823_05202013_AJ_Aug2_Rot_6_sd1
(231/456) train vimp2_823_05202013_AJ_Aug3_Rot_-7_sd1
(232/456) train vimp2_823_05202013_AJ_Aug4_Rot_-4_sd1
(233/456) train vimp2_823_05202013_AJ_Aug5_Rot_-4_sd1
(234/456) train vimp2_824_05212013_JS_Aug0_Rot_7_sd1
(235/456) train vimp2_824_05212013_JS_Aug1_Rot_3_sd1
(236/456) train vimp2_824_05212013_JS_Aug2_Rot_3_sd1
(237/456) train vimp2_824_05212013_JS_Aug3_Rot_-4_sd1
(238/456) train vimp2_824_05212013_JS_Aug4_Rot_4_sd1
(239/456) train vimp2_824_05212013_JS_Aug5_Rot_6_sd1
(240/456) train vimp2_845_05312013_VZ_Aug0_Rot_5_sd1
(241/456) train vimp2_845_05312013_VZ_Aug1_Rot_6_sd1
(242/456) train vimp2_845_05312013_VZ_Aug2_Rot_1_sd1
(243/456) train vimp2_845_05312013_VZ_Aug3_Rot_-6_sd1
(244/456) train vimp2_845_05312013_VZ_Aug4_Rot_-2_sd1
(245/456) train vimp2_845_05312013_VZ_Aug5_Rot_-6_sd1
(246/456) train vimp2_869_06142013_BL_Aug0_Rot_-4_sd1
(247/456) train vimp2_869_06142013_BL_Aug1_Rot_-3_sd1
(248/456) train vimp2_869_06142013_BL_Aug2_Rot_-4_sd1
(249/456) train vimp2_869_06142013_BL_Aug3_Rot_1_sd1
(250/456) train vimp2_869_06142013_BL_Aug4_Rot_7_sd1
(251/456) train vimp2_869_06142013_BL_Aug5_Rot_5_sd1
(252/456) train vimp2_884_06272013_TS_Aug0_Rot_5_sd1
(253/456) train vimp2_884_06272013_TS_Aug1_Rot_-1_sd1
(254/456) train vimp2_884_06272013_TS_Aug2_Rot_-1_sd1
(255/456) train vimp2_884_06272013_TS_Aug3_Rot_3_sd1
(256/456) train vimp2_884_06272013_TS_Aug4_Rot_4_sd1
(257/456) train vimp2_884_06272013_TS_Aug5_Rot_3_sd1
(258/456) train vimp2_901_07052013_AS_Aug0_Rot_3_sd1
(259/456) train vimp2_901_07052013_AS_Aug1_Rot_5_sd1
(260/456) train vimp2_901_07052013_AS_Aug2_Rot_-5_sd1
(261/456) train vimp2_901_07052013_AS_Aug3_Rot_7_sd1
(262/456) train vimp2_901_07052013_AS_Aug4_Rot_7_sd1
(263/456) train vimp2_901_07052013_AS_Aug5_Rot_6_sd1
(264/456) train vimp2_915_07112013_LC_Aug0_Rot_-6_sd1
(265/456) train vimp2_915_07112013_LC_Aug1_Rot_-5_sd1
(266/456) train vimp2_915_07112013_LC_Aug2_Rot_7_sd1
(267/456) train vimp2_915_07112013_LC_Aug3_Rot_1_sd1
(268/456) train vimp2_915_07112013_LC_Aug4_Rot_6_sd1
(269/456) train vimp2_915_07112013_LC_Aug5_Rot_-7_sd1
(270/456) train vimp2_943_07242013_PA_Aug0_Rot_1_sd1
(271/456) train vimp2_943_07242013_PA_Aug1_Rot_6_sd1
(272/456) train vimp2_943_07242013_PA_Aug2_Rot_3_sd1
(273/456) train vimp2_943_07242013_PA_Aug3_Rot_6_sd1
(274/456) train vimp2_943_07242013_PA_Aug4_Rot_3_sd1
(275/456) train vimp2_943_07242013_PA_Aug5_Rot_3_sd1
(276/456) train vimp2_964_08092013_TG_Aug0_Rot_6_sd1
(277/456) train vimp2_964_08092013_TG_Aug1_Rot_-4_sd1
(278/456) train vimp2_964_08092013_TG_Aug2_Rot_5_sd1
(279/456) train vimp2_964_08092013_TG_Aug3_Rot_0_sd1
(280/456) train vimp2_964_08092013_TG_Aug4_Rot_3_sd1
(281/456) train vimp2_964_08092013_TG_Aug5_Rot_-1_sd1
(282/456) train vimp2_ANON606_20130110_Aug0_Rot_-6_sd1
(283/456) train vimp2_ANON606_20130110_Aug1_Rot_7_sd1
(284/456) train vimp2_ANON606_20130110_Aug2_Rot_1_sd1
(285/456) train vimp2_ANON606_20130110_Aug3_Rot_1_sd1
(286/456) train vimp2_ANON606_20130110_Aug4_Rot_4_sd1
(287/456) train vimp2_ANON606_20130110_Aug5_Rot_4_sd1
(288/456) train vimp2_ANON624_20130117_Aug0_Rot_7_sd1
(289/456) train vimp2_ANON624_20130117_Aug1_Rot_6_sd1
(290/456) train vimp2_ANON624_20130117_Aug2_Rot_-1_sd1
(291/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd1
(292/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd1
(293/456) train vimp2_ANON624_20130117_Aug5_Rot_4_sd1
(294/456) train vimp2_ANON724_03272013_Aug0_Rot_-2_sd1
(295/456) train vimp2_ANON724_03272013_Aug1_Rot_4_sd1
(296/456) train vimp2_ANON724_03272013_Aug2_Rot_-1_sd1
(297/456) train vimp2_ANON724_03272013_Aug3_Rot_-6_sd1
(298/456) train vimp2_ANON724_03272013_Aug4_Rot_-5_sd1
(299/456) train vimp2_ANON724_03272013_Aug5_Rot_-4_sd1
(300/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_-2_sd1
(301/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_7_sd1
(302/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_2_sd1
(303/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_-4_sd1
(304/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_-1_sd1
(305/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_-4_sd1
(306/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_7_sd1
(307/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_-1_sd1
(308/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_-5_sd1
(309/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_-3_sd1
(310/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_6_sd1
(311/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_6_sd1
(312/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_1_sd1
(313/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_-2_sd1
(314/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_3_sd1
(315/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_-1_sd1
(316/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_7_sd1
(317/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_6_sd1
(318/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_-3_sd1
(319/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_7_sd1
(320/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_-4_sd1
(321/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_-1_sd1
(322/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-5_sd1
(323/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_-5_sd1
(324/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_4_sd1
(325/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_3_sd1
(326/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_-1_sd1
(327/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_-2_sd1
(328/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_2_sd1
(329/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_4_sd1
(330/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_-2_sd1
(331/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_4_sd1
(332/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_-2_sd1
(333/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_2_sd1
(334/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_3_sd1
(335/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-4_sd1
(336/456) train vimp2_668_02282013_CD_Aug0_Rot_-3_sd2
(337/456) train vimp2_668_02282013_CD_Aug1_Rot_6_sd2
(338/456) train vimp2_668_02282013_CD_Aug2_Rot_-6_sd2
(339/456) train vimp2_668_02282013_CD_Aug3_Rot_7_sd2
(340/456) train vimp2_668_02282013_CD_Aug4_Rot_1_sd2
(341/456) train vimp2_668_02282013_CD_Aug5_Rot_7_sd2
(342/456) train vimp2_819_05172013_DS_Aug0_Rot_3_sd2
(343/456) train vimp2_819_05172013_DS_Aug1_Rot_-1_sd2
(344/456) train vimp2_819_05172013_DS_Aug2_Rot_-2_sd2
(345/456) train vimp2_819_05172013_DS_Aug3_Rot_-1_sd2
(346/456) train vimp2_819_05172013_DS_Aug4_Rot_7_sd2
(347/456) train vimp2_819_05172013_DS_Aug5_Rot_3_sd2
(348/456) train vimp2_823_05202013_AJ_Aug0_Rot_-6_sd2
(349/456) train vimp2_823_05202013_AJ_Aug1_Rot_-7_sd2
(350/456) train vimp2_823_05202013_AJ_Aug2_Rot_7_sd2
(351/456) train vimp2_823_05202013_AJ_Aug3_Rot_-2_sd2
(352/456) train vimp2_823_05202013_AJ_Aug4_Rot_1_sd2
(353/456) train vimp2_823_05202013_AJ_Aug5_Rot_1_sd2
(354/456) train vimp2_824_05212013_JS_Aug0_Rot_-5_sd2
(355/456) train vimp2_824_05212013_JS_Aug1_Rot_3_sd2
(356/456) train vimp2_824_05212013_JS_Aug2_Rot_5_sd2
(357/456) train vimp2_824_05212013_JS_Aug3_Rot_2_sd2
(358/456) train vimp2_824_05212013_JS_Aug4_Rot_5_sd2
(359/456) train vimp2_824_05212013_JS_Aug5_Rot_-7_sd2
(360/456) train vimp2_845_05312013_VZ_Aug0_Rot_-5_sd2
(361/456) train vimp2_845_05312013_VZ_Aug1_Rot_3_sd2
(362/456) train vimp2_845_05312013_VZ_Aug2_Rot_-3_sd2
(363/456) train vimp2_845_05312013_VZ_Aug3_Rot_-1_sd2
(364/456) train vimp2_845_05312013_VZ_Aug4_Rot_4_sd2
(365/456) train vimp2_845_05312013_VZ_Aug5_Rot_-2_sd2
(366/456) train vimp2_869_06142013_BL_Aug0_Rot_3_sd2
(367/456) train vimp2_869_06142013_BL_Aug1_Rot_7_sd2
(368/456) train vimp2_869_06142013_BL_Aug2_Rot_4_sd2
(369/456) train vimp2_869_06142013_BL_Aug3_Rot_1_sd2
(370/456) train vimp2_869_06142013_BL_Aug4_Rot_1_sd2
(371/456) train vimp2_869_06142013_BL_Aug5_Rot_1_sd2
(372/456) train vimp2_884_06272013_TS_Aug0_Rot_3_sd2
(373/456) train vimp2_884_06272013_TS_Aug1_Rot_-5_sd2
(374/456) train vimp2_884_06272013_TS_Aug2_Rot_-7_sd2
(375/456) train vimp2_884_06272013_TS_Aug3_Rot_-6_sd2
(376/456) train vimp2_884_06272013_TS_Aug4_Rot_-2_sd2
(377/456) train vimp2_884_06272013_TS_Aug5_Rot_-6_sd2
(378/456) train vimp2_901_07052013_AS_Aug0_Rot_2_sd2
(379/456) train vimp2_901_07052013_AS_Aug1_Rot_-3_sd2
(380/456) train vimp2_901_07052013_AS_Aug2_Rot_-6_sd2
(381/456) train vimp2_901_07052013_AS_Aug3_Rot_6_sd2
(382/456) train vimp2_901_07052013_AS_Aug4_Rot_-5_sd2
(383/456) train vimp2_901_07052013_AS_Aug5_Rot_1_sd2
(384/456) train vimp2_915_07112013_LC_Aug0_Rot_1_sd2
(385/456) train vimp2_915_07112013_LC_Aug1_Rot_7_sd2
(386/456) train vimp2_915_07112013_LC_Aug2_Rot_3_sd2
(387/456) train vimp2_915_07112013_LC_Aug3_Rot_6_sd2
(388/456) train vimp2_915_07112013_LC_Aug4_Rot_4_sd2
(389/456) train vimp2_915_07112013_LC_Aug5_Rot_4_sd2
(390/456) train vimp2_943_07242013_PA_Aug0_Rot_3_sd2
(391/456) train vimp2_943_07242013_PA_Aug1_Rot_-4_sd2
(392/456) train vimp2_943_07242013_PA_Aug2_Rot_3_sd2
(393/456) train vimp2_943_07242013_PA_Aug3_Rot_2_sd2
(394/456) train vimp2_943_07242013_PA_Aug4_Rot_-1_sd2
(395/456) train vimp2_943_07242013_PA_Aug5_Rot_-3_sd2
(396/456) train vimp2_964_08092013_TG_Aug0_Rot_2_sd2
(397/456) train vimp2_964_08092013_TG_Aug1_Rot_-6_sd2
(398/456) train vimp2_964_08092013_TG_Aug2_Rot_5_sd2
(399/456) train vimp2_964_08092013_TG_Aug3_Rot_-7_sd2
(400/456) train vimp2_964_08092013_TG_Aug4_Rot_-6_sd2
(401/456) train vimp2_964_08092013_TG_Aug5_Rot_5_sd2
(402/456) train vimp2_ANON606_20130110_Aug0_Rot_-6_sd2
(403/456) train vimp2_ANON606_20130110_Aug1_Rot_-5_sd2
(404/456) train vimp2_ANON606_20130110_Aug2_Rot_-3_sd2
(405/456) train vimp2_ANON606_20130110_Aug3_Rot_3_sd2
(406/456) train vimp2_ANON606_20130110_Aug4_Rot_-2_sd2
(407/456) train vimp2_ANON606_20130110_Aug5_Rot_-3_sd2
(408/456) train vimp2_ANON624_20130117_Aug0_Rot_2_sd2
(409/456) train vimp2_ANON624_20130117_Aug1_Rot_-2_sd2
(410/456) train vimp2_ANON624_20130117_Aug2_Rot_-6_sd2
(411/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd2
(412/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd2
(413/456) train vimp2_ANON624_20130117_Aug5_Rot_-7_sd2
(414/456) train vimp2_ANON724_03272013_Aug0_Rot_-5_sd2
(415/456) train vimp2_ANON724_03272013_Aug1_Rot_6_sd2
(416/456) train vimp2_ANON724_03272013_Aug2_Rot_-3_sd2
(417/456) train vimp2_ANON724_03272013_Aug3_Rot_-6_sd2
(418/456) train vimp2_ANON724_03272013_Aug4_Rot_-1_sd2
(419/456) train vimp2_ANON724_03272013_Aug5_Rot_-7_sd2
(420/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_-4_sd2
(421/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_-3_sd2
(422/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_-5_sd2
(423/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_5_sd2
(424/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_-2_sd2
(425/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_5_sd2
(426/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_-3_sd2
(427/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_7_sd2
(428/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_3_sd2
(429/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_5_sd2
(430/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_7_sd2
(431/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_2_sd2
(432/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_-7_sd2
(433/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_5_sd2
(434/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_5_sd2
(435/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_4_sd2
(436/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_5_sd2
(437/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_-4_sd2
(438/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_7_sd2
(439/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_-2_sd2
(440/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_1_sd2
(441/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_-7_sd2
(442/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-3_sd2
(443/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_7_sd2
(444/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_1_sd2
(445/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_-7_sd2
(446/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_6_sd2
(447/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_-2_sd2
(448/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_-5_sd2
(449/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_6_sd2
(450/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_2_sd2
(451/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_5_sd2
(452/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_2_sd2
(453/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_5_sd2
(454/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_-4_sd2
(455/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-3_sd22019-07-06 16:57:53.996821: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-06 16:57:55.516876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-07-06 16:57:55.516949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 16:57:55.883431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 16:57:55.883495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 16:57:55.883508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 16:57:55.883948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a’: File exists
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<05:58,  1.26s/it]Loading train:   1%|          | 2/285 [00:02<05:49,  1.23s/it]Loading train:   1%|          | 3/285 [00:03<05:35,  1.19s/it]Loading train:   1%|▏         | 4/285 [00:04<05:57,  1.27s/it]Loading train:   2%|▏         | 5/285 [00:05<05:25,  1.16s/it]Loading train:   2%|▏         | 6/285 [00:07<05:56,  1.28s/it]Loading train:   2%|▏         | 7/285 [00:09<06:23,  1.38s/it]Loading train:   3%|▎         | 8/285 [00:10<06:26,  1.40s/it]Loading train:   3%|▎         | 9/285 [00:11<06:09,  1.34s/it]Loading train:   4%|▎         | 10/285 [00:12<05:45,  1.26s/it]Loading train:   4%|▍         | 11/285 [00:13<05:42,  1.25s/it]Loading train:   4%|▍         | 12/285 [00:14<05:17,  1.16s/it]Loading train:   5%|▍         | 13/285 [00:15<04:51,  1.07s/it]Loading train:   5%|▍         | 14/285 [00:16<04:43,  1.04s/it]Loading train:   5%|▌         | 15/285 [00:17<04:44,  1.05s/it]Loading train:   6%|▌         | 16/285 [00:18<04:43,  1.06s/it]Loading train:   6%|▌         | 17/285 [00:20<04:56,  1.11s/it]Loading train:   6%|▋         | 18/285 [00:21<04:43,  1.06s/it]Loading train:   7%|▋         | 19/285 [00:22<04:39,  1.05s/it]Loading train:   7%|▋         | 20/285 [00:23<04:29,  1.02s/it]Loading train:   7%|▋         | 21/285 [00:24<04:36,  1.05s/it]Loading train:   8%|▊         | 22/285 [00:25<04:27,  1.02s/it]Loading train:   8%|▊         | 23/285 [00:26<04:16,  1.02it/s]Loading train:   8%|▊         | 24/285 [00:26<04:13,  1.03it/s]Loading train:   9%|▉         | 25/285 [00:27<04:10,  1.04it/s]Loading train:   9%|▉         | 26/285 [00:28<04:05,  1.05it/s]Loading train:   9%|▉         | 27/285 [00:29<04:07,  1.04it/s]Loading train:  10%|▉         | 28/285 [00:31<04:34,  1.07s/it]Loading train:  10%|█         | 29/285 [00:32<04:30,  1.06s/it]Loading train:  11%|█         | 30/285 [00:33<04:14,  1.00it/s]Loading train:  11%|█         | 31/285 [00:34<04:11,  1.01it/s]Loading train:  11%|█         | 32/285 [00:34<04:02,  1.04it/s]Loading train:  12%|█▏        | 33/285 [00:35<03:59,  1.05it/s]Loading train:  12%|█▏        | 34/285 [00:36<03:48,  1.10it/s]Loading train:  12%|█▏        | 35/285 [00:37<03:49,  1.09it/s]Loading train:  13%|█▎        | 36/285 [00:38<03:46,  1.10it/s]Loading train:  13%|█▎        | 37/285 [00:39<03:52,  1.07it/s]Loading train:  13%|█▎        | 38/285 [00:40<04:01,  1.02it/s]Loading train:  14%|█▎        | 39/285 [00:41<03:56,  1.04it/s]Loading train:  14%|█▍        | 40/285 [00:42<03:53,  1.05it/s]Loading train:  14%|█▍        | 41/285 [00:43<03:50,  1.06it/s]Loading train:  15%|█▍        | 42/285 [00:44<04:02,  1.00it/s]Loading train:  15%|█▌        | 43/285 [00:45<04:07,  1.02s/it]Loading train:  15%|█▌        | 44/285 [00:46<04:08,  1.03s/it]Loading train:  16%|█▌        | 45/285 [00:47<04:09,  1.04s/it]Loading train:  16%|█▌        | 46/285 [00:48<04:01,  1.01s/it]Loading train:  16%|█▋        | 47/285 [00:49<03:58,  1.00s/it]Loading train:  17%|█▋        | 48/285 [00:50<04:02,  1.02s/it]Loading train:  17%|█▋        | 49/285 [00:51<03:41,  1.06it/s]Loading train:  18%|█▊        | 50/285 [00:52<03:56,  1.01s/it]Loading train:  18%|█▊        | 51/285 [00:53<03:43,  1.05it/s]Loading train:  18%|█▊        | 52/285 [00:54<03:34,  1.09it/s]Loading train:  19%|█▊        | 53/285 [00:55<03:42,  1.04it/s]Loading train:  19%|█▉        | 54/285 [00:56<03:31,  1.09it/s]Loading train:  19%|█▉        | 55/285 [00:57<03:33,  1.08it/s]Loading train:  20%|█▉        | 56/285 [00:57<03:22,  1.13it/s]Loading train:  20%|██        | 57/285 [00:58<03:18,  1.15it/s]Loading train:  20%|██        | 58/285 [00:59<03:13,  1.18it/s]Loading train:  21%|██        | 59/285 [01:00<03:21,  1.12it/s]Loading train:  21%|██        | 60/285 [01:01<03:37,  1.04it/s]Loading train:  21%|██▏       | 61/285 [01:02<03:26,  1.09it/s]Loading train:  22%|██▏       | 62/285 [01:03<03:19,  1.12it/s]Loading train:  22%|██▏       | 63/285 [01:03<03:07,  1.18it/s]Loading train:  22%|██▏       | 64/285 [01:05<03:32,  1.04it/s]Loading train:  23%|██▎       | 65/285 [01:06<04:02,  1.10s/it]Loading train:  23%|██▎       | 66/285 [01:07<04:10,  1.14s/it]Loading train:  24%|██▎       | 67/285 [01:08<03:49,  1.05s/it]Loading train:  24%|██▍       | 68/285 [01:09<03:32,  1.02it/s]Loading train:  24%|██▍       | 69/285 [01:10<03:20,  1.08it/s]Loading train:  25%|██▍       | 70/285 [01:11<03:33,  1.01it/s]Loading train:  25%|██▍       | 71/285 [01:12<03:31,  1.01it/s]Loading train:  25%|██▌       | 72/285 [01:13<03:27,  1.02it/s]Loading train:  26%|██▌       | 73/285 [01:14<03:26,  1.03it/s]Loading train:  26%|██▌       | 74/285 [01:15<03:13,  1.09it/s]Loading train:  26%|██▋       | 75/285 [01:15<03:06,  1.12it/s]Loading train:  27%|██▋       | 76/285 [01:16<02:59,  1.16it/s]Loading train:  27%|██▋       | 77/285 [01:17<03:11,  1.09it/s]Loading train:  27%|██▋       | 78/285 [01:18<03:07,  1.10it/s]Loading train:  28%|██▊       | 79/285 [01:19<03:13,  1.06it/s]Loading train:  28%|██▊       | 80/285 [01:20<03:06,  1.10it/s]Loading train:  28%|██▊       | 81/285 [01:21<03:01,  1.12it/s]Loading train:  29%|██▉       | 82/285 [01:22<02:54,  1.16it/s]Loading train:  29%|██▉       | 83/285 [01:23<03:09,  1.07it/s]Loading train:  29%|██▉       | 84/285 [01:24<03:05,  1.09it/s]Loading train:  30%|██▉       | 85/285 [01:25<03:14,  1.03it/s]Loading train:  30%|███       | 86/285 [01:26<03:11,  1.04it/s]Loading train:  31%|███       | 87/285 [01:27<03:18,  1.00s/it]Loading train:  31%|███       | 88/285 [01:28<03:15,  1.01it/s]Loading train:  31%|███       | 89/285 [01:29<03:14,  1.01it/s]Loading train:  32%|███▏      | 90/285 [01:30<03:26,  1.06s/it]Loading train:  32%|███▏      | 91/285 [01:31<03:19,  1.03s/it]Loading train:  32%|███▏      | 92/285 [01:32<03:16,  1.02s/it]Loading train:  33%|███▎      | 93/285 [01:33<03:11,  1.00it/s]Loading train:  33%|███▎      | 94/285 [01:34<03:10,  1.00it/s]Loading train:  33%|███▎      | 95/285 [01:35<03:05,  1.03it/s]Loading train:  34%|███▎      | 96/285 [01:36<03:06,  1.02it/s]Loading train:  34%|███▍      | 97/285 [01:37<03:03,  1.02it/s]Loading train:  34%|███▍      | 98/285 [01:38<02:58,  1.05it/s]Loading train:  35%|███▍      | 99/285 [01:39<03:09,  1.02s/it]Loading train:  35%|███▌      | 100/285 [01:40<03:02,  1.01it/s]Loading train:  35%|███▌      | 101/285 [01:41<03:02,  1.01it/s]Loading train:  36%|███▌      | 102/285 [01:42<03:00,  1.01it/s]Loading train:  36%|███▌      | 103/285 [01:43<03:01,  1.00it/s]Loading train:  36%|███▋      | 104/285 [01:44<02:57,  1.02it/s]Loading train:  37%|███▋      | 105/285 [01:45<02:52,  1.05it/s]Loading train:  37%|███▋      | 106/285 [01:45<02:46,  1.08it/s]Loading train:  38%|███▊      | 107/285 [01:46<02:40,  1.11it/s]Loading train:  38%|███▊      | 108/285 [01:47<02:35,  1.14it/s]Loading train:  38%|███▊      | 109/285 [01:48<02:36,  1.13it/s]Loading train:  39%|███▊      | 110/285 [01:49<02:34,  1.13it/s]Loading train:  39%|███▉      | 111/285 [01:50<02:33,  1.14it/s]Loading train:  39%|███▉      | 112/285 [01:51<02:30,  1.15it/s]Loading train:  40%|███▉      | 113/285 [01:51<02:29,  1.15it/s]Loading train:  40%|████      | 114/285 [01:52<02:27,  1.16it/s]Loading train:  40%|████      | 115/285 [01:53<02:24,  1.18it/s]Loading train:  41%|████      | 116/285 [01:54<02:20,  1.21it/s]Loading train:  41%|████      | 117/285 [01:55<02:16,  1.23it/s]Loading train:  41%|████▏     | 118/285 [01:55<02:13,  1.25it/s]Loading train:  42%|████▏     | 119/285 [01:56<02:17,  1.20it/s]Loading train:  42%|████▏     | 120/285 [01:58<02:38,  1.04it/s]Loading train:  42%|████▏     | 121/285 [01:59<03:20,  1.22s/it]Loading train:  43%|████▎     | 122/285 [02:01<03:37,  1.33s/it]Loading train:  43%|████▎     | 123/285 [02:03<03:51,  1.43s/it]Loading train:  44%|████▎     | 124/285 [02:04<03:56,  1.47s/it]Loading train:  44%|████▍     | 125/285 [02:06<03:51,  1.45s/it]Loading train:  44%|████▍     | 126/285 [02:07<03:46,  1.42s/it]Loading train:  45%|████▍     | 127/285 [02:09<03:46,  1.43s/it]Loading train:  45%|████▍     | 128/285 [02:10<03:35,  1.37s/it]Loading train:  45%|████▌     | 129/285 [02:11<03:28,  1.34s/it]Loading train:  46%|████▌     | 130/285 [02:12<03:22,  1.31s/it]Loading train:  46%|████▌     | 131/285 [02:13<03:12,  1.25s/it]Loading train:  46%|████▋     | 132/285 [02:15<03:41,  1.45s/it]Loading train:  47%|████▋     | 133/285 [02:17<04:01,  1.59s/it]Loading train:  47%|████▋     | 134/285 [02:18<03:37,  1.44s/it]Loading train:  47%|████▋     | 135/285 [02:20<03:28,  1.39s/it]Loading train:  48%|████▊     | 136/285 [02:21<03:44,  1.51s/it]Loading train:  48%|████▊     | 137/285 [02:23<03:33,  1.44s/it]Loading train:  48%|████▊     | 138/285 [02:25<03:54,  1.59s/it]Loading train:  49%|████▉     | 139/285 [02:26<04:01,  1.66s/it]Loading train:  49%|████▉     | 140/285 [02:28<04:00,  1.66s/it]Loading train:  49%|████▉     | 141/285 [02:29<03:47,  1.58s/it]Loading train:  50%|████▉     | 142/285 [02:31<03:58,  1.67s/it]Loading train:  50%|█████     | 143/285 [02:33<04:04,  1.72s/it]Loading train:  51%|█████     | 144/285 [02:34<03:26,  1.47s/it]Loading train:  51%|█████     | 145/285 [02:35<03:21,  1.44s/it]Loading train:  51%|█████     | 146/285 [02:37<03:11,  1.38s/it]Loading train:  52%|█████▏    | 147/285 [02:38<02:59,  1.30s/it]Loading train:  52%|█████▏    | 148/285 [02:39<02:56,  1.29s/it]Loading train:  52%|█████▏    | 149/285 [02:40<02:51,  1.26s/it]Loading train:  53%|█████▎    | 150/285 [02:42<03:08,  1.40s/it]Loading train:  53%|█████▎    | 151/285 [02:43<02:55,  1.31s/it]Loading train:  53%|█████▎    | 152/285 [02:44<02:49,  1.28s/it]Loading train:  54%|█████▎    | 153/285 [02:45<02:44,  1.25s/it]Loading train:  54%|█████▍    | 154/285 [02:47<02:44,  1.26s/it]Loading train:  54%|█████▍    | 155/285 [02:48<02:37,  1.21s/it]Loading train:  55%|█████▍    | 156/285 [02:49<02:41,  1.25s/it]Loading train:  55%|█████▌    | 157/285 [02:50<02:36,  1.22s/it]Loading train:  55%|█████▌    | 158/285 [02:52<02:43,  1.29s/it]Loading train:  56%|█████▌    | 159/285 [02:53<02:49,  1.35s/it]Loading train:  56%|█████▌    | 160/285 [02:55<02:53,  1.39s/it]Loading train:  56%|█████▋    | 161/285 [02:56<02:56,  1.42s/it]Loading train:  57%|█████▋    | 162/285 [02:57<02:48,  1.37s/it]Loading train:  57%|█████▋    | 163/285 [02:59<02:42,  1.33s/it]Loading train:  58%|█████▊    | 164/285 [03:00<02:33,  1.27s/it]Loading train:  58%|█████▊    | 165/285 [03:01<02:32,  1.27s/it]Loading train:  58%|█████▊    | 166/285 [03:03<02:41,  1.35s/it]Loading train:  59%|█████▊    | 167/285 [03:04<02:41,  1.37s/it]Loading train:  59%|█████▉    | 168/285 [03:05<02:27,  1.26s/it]Loading train:  59%|█████▉    | 169/285 [03:06<02:30,  1.30s/it]Loading train:  60%|█████▉    | 170/285 [03:08<02:20,  1.22s/it]Loading train:  60%|██████    | 171/285 [03:08<02:10,  1.14s/it]Loading train:  60%|██████    | 172/285 [03:10<02:24,  1.28s/it]Loading train:  61%|██████    | 173/285 [03:12<02:45,  1.48s/it]Loading train:  61%|██████    | 174/285 [03:13<02:36,  1.41s/it]Loading train:  61%|██████▏   | 175/285 [03:14<02:23,  1.31s/it]Loading train:  62%|██████▏   | 176/285 [03:16<02:19,  1.28s/it]Loading train:  62%|██████▏   | 177/285 [03:17<02:14,  1.25s/it]Loading train:  62%|██████▏   | 178/285 [03:18<02:16,  1.28s/it]Loading train:  63%|██████▎   | 179/285 [03:19<02:17,  1.30s/it]Loading train:  63%|██████▎   | 180/285 [03:20<02:03,  1.17s/it]Loading train:  64%|██████▎   | 181/285 [03:21<01:58,  1.14s/it]Loading train:  64%|██████▍   | 182/285 [03:23<01:59,  1.16s/it]Loading train:  64%|██████▍   | 183/285 [03:24<01:58,  1.16s/it]Loading train:  65%|██████▍   | 184/285 [03:25<02:06,  1.25s/it]Loading train:  65%|██████▍   | 185/285 [03:27<02:06,  1.27s/it]Loading train:  65%|██████▌   | 186/285 [03:28<02:17,  1.39s/it]Loading train:  66%|██████▌   | 187/285 [03:30<02:24,  1.47s/it]Loading train:  66%|██████▌   | 188/285 [03:31<02:24,  1.49s/it]Loading train:  66%|██████▋   | 189/285 [03:32<02:11,  1.37s/it]Loading train:  67%|██████▋   | 190/285 [03:34<02:01,  1.28s/it]Loading train:  67%|██████▋   | 191/285 [03:34<01:47,  1.15s/it]Loading train:  67%|██████▋   | 192/285 [03:36<01:53,  1.22s/it]Loading train:  68%|██████▊   | 193/285 [03:37<01:49,  1.19s/it]Loading train:  68%|██████▊   | 194/285 [03:38<01:47,  1.19s/it]Loading train:  68%|██████▊   | 195/285 [03:39<01:46,  1.18s/it]Loading train:  69%|██████▉   | 196/285 [03:41<01:49,  1.23s/it]Loading train:  69%|██████▉   | 197/285 [03:42<01:56,  1.33s/it]Loading train:  69%|██████▉   | 198/285 [03:43<01:49,  1.26s/it]Loading train:  70%|██████▉   | 199/285 [03:44<01:45,  1.23s/it]Loading train:  70%|███████   | 200/285 [03:46<01:50,  1.30s/it]Loading train:  71%|███████   | 201/285 [03:47<01:46,  1.27s/it]Loading train:  71%|███████   | 202/285 [03:49<01:52,  1.36s/it]Loading train:  71%|███████   | 203/285 [03:50<01:48,  1.33s/it]Loading train:  72%|███████▏  | 204/285 [03:51<01:50,  1.36s/it]Loading train:  72%|███████▏  | 205/285 [03:53<01:55,  1.45s/it]Loading train:  72%|███████▏  | 206/285 [03:54<01:56,  1.47s/it]Loading train:  73%|███████▎  | 207/285 [03:56<01:54,  1.47s/it]Loading train:  73%|███████▎  | 208/285 [03:58<01:58,  1.53s/it]Loading train:  73%|███████▎  | 209/285 [03:59<01:49,  1.44s/it]Loading train:  74%|███████▎  | 210/285 [04:00<01:39,  1.32s/it]Loading train:  74%|███████▍  | 211/285 [04:01<01:34,  1.28s/it]Loading train:  74%|███████▍  | 212/285 [04:02<01:29,  1.23s/it]Loading train:  75%|███████▍  | 213/285 [04:04<01:30,  1.26s/it]Loading train:  75%|███████▌  | 214/285 [04:05<01:30,  1.28s/it]Loading train:  75%|███████▌  | 215/285 [04:06<01:30,  1.29s/it]Loading train:  76%|███████▌  | 216/285 [04:07<01:27,  1.27s/it]Loading train:  76%|███████▌  | 217/285 [04:09<01:23,  1.23s/it]Loading train:  76%|███████▋  | 218/285 [04:10<01:18,  1.17s/it]Loading train:  77%|███████▋  | 219/285 [04:11<01:26,  1.32s/it]Loading train:  77%|███████▋  | 220/285 [04:13<01:29,  1.38s/it]Loading train:  78%|███████▊  | 221/285 [04:15<01:41,  1.59s/it]Loading train:  78%|███████▊  | 222/285 [04:16<01:38,  1.56s/it]Loading train:  78%|███████▊  | 223/285 [04:18<01:41,  1.63s/it]Loading train:  79%|███████▊  | 224/285 [04:20<01:42,  1.67s/it]Loading train:  79%|███████▉  | 225/285 [04:21<01:38,  1.65s/it]Loading train:  79%|███████▉  | 226/285 [04:23<01:34,  1.60s/it]Loading train:  80%|███████▉  | 227/285 [04:25<01:32,  1.60s/it]Loading train:  80%|████████  | 228/285 [04:26<01:32,  1.63s/it]Loading train:  80%|████████  | 229/285 [04:27<01:20,  1.44s/it]Loading train:  81%|████████  | 230/285 [04:29<01:23,  1.52s/it]Loading train:  81%|████████  | 231/285 [04:30<01:17,  1.44s/it]Loading train:  81%|████████▏ | 232/285 [04:32<01:20,  1.53s/it]Loading train:  82%|████████▏ | 233/285 [04:33<01:19,  1.52s/it]Loading train:  82%|████████▏ | 234/285 [04:35<01:16,  1.51s/it]Loading train:  82%|████████▏ | 235/285 [04:37<01:18,  1.56s/it]Loading train:  83%|████████▎ | 236/285 [04:38<01:21,  1.66s/it]Loading train:  83%|████████▎ | 237/285 [04:40<01:20,  1.68s/it]Loading train:  84%|████████▎ | 238/285 [04:42<01:23,  1.78s/it]Loading train:  84%|████████▍ | 239/285 [04:44<01:21,  1.78s/it]Loading train:  84%|████████▍ | 240/285 [04:46<01:28,  1.97s/it]Loading train:  85%|████████▍ | 241/285 [04:49<01:30,  2.05s/it]Loading train:  85%|████████▍ | 242/285 [04:51<01:33,  2.16s/it]Loading train:  85%|████████▌ | 243/285 [04:53<01:33,  2.23s/it]Loading train:  86%|████████▌ | 244/285 [04:56<01:32,  2.26s/it]Loading train:  86%|████████▌ | 245/285 [04:58<01:32,  2.32s/it]Loading train:  86%|████████▋ | 246/285 [05:00<01:23,  2.13s/it]Loading train:  87%|████████▋ | 247/285 [05:02<01:17,  2.05s/it]Loading train:  87%|████████▋ | 248/285 [05:04<01:14,  2.01s/it]Loading train:  87%|████████▋ | 249/285 [05:06<01:10,  1.97s/it]Loading train:  88%|████████▊ | 250/285 [05:07<01:07,  1.92s/it]Loading train:  88%|████████▊ | 251/285 [05:09<01:04,  1.89s/it]Loading train:  88%|████████▊ | 252/285 [05:11<01:00,  1.83s/it]Loading train:  89%|████████▉ | 253/285 [05:13<00:56,  1.78s/it]Loading train:  89%|████████▉ | 254/285 [05:14<00:53,  1.71s/it]Loading train:  89%|████████▉ | 255/285 [05:16<00:48,  1.61s/it]Loading train:  90%|████████▉ | 256/285 [05:17<00:45,  1.57s/it]Loading train:  90%|█████████ | 257/285 [05:19<00:44,  1.58s/it]Loading train:  91%|█████████ | 258/285 [05:20<00:42,  1.57s/it]Loading train:  91%|█████████ | 259/285 [05:22<00:39,  1.52s/it]Loading train:  91%|█████████ | 260/285 [05:23<00:38,  1.54s/it]Loading train:  92%|█████████▏| 261/285 [05:25<00:36,  1.53s/it]Loading train:  92%|█████████▏| 262/285 [05:26<00:36,  1.57s/it]Loading train:  92%|█████████▏| 263/285 [05:28<00:37,  1.69s/it]Loading train:  93%|█████████▎| 264/285 [05:30<00:32,  1.57s/it]Loading train:  93%|█████████▎| 265/285 [05:31<00:28,  1.45s/it]Loading train:  93%|█████████▎| 266/285 [05:32<00:29,  1.55s/it]Loading train:  94%|█████████▎| 267/285 [05:34<00:28,  1.61s/it]Loading train:  94%|█████████▍| 268/285 [05:36<00:30,  1.78s/it]Loading train:  94%|█████████▍| 269/285 [05:39<00:30,  1.90s/it]Loading train:  95%|█████████▍| 270/285 [05:41<00:29,  1.95s/it]Loading train:  95%|█████████▌| 271/285 [05:42<00:26,  1.91s/it]Loading train:  95%|█████████▌| 272/285 [05:44<00:23,  1.83s/it]Loading train:  96%|█████████▌| 273/285 [05:47<00:24,  2.02s/it]Loading train:  96%|█████████▌| 274/285 [05:48<00:21,  1.93s/it]Loading train:  96%|█████████▋| 275/285 [05:50<00:19,  1.91s/it]Loading train:  97%|█████████▋| 276/285 [05:52<00:17,  1.89s/it]Loading train:  97%|█████████▋| 277/285 [05:54<00:15,  1.97s/it]Loading train:  98%|█████████▊| 278/285 [05:56<00:14,  2.01s/it]Loading train:  98%|█████████▊| 279/285 [05:58<00:11,  1.93s/it]Loading train:  98%|█████████▊| 280/285 [06:00<00:09,  1.97s/it]Loading train:  99%|█████████▊| 281/285 [06:02<00:07,  1.98s/it]Loading train:  99%|█████████▉| 282/285 [06:04<00:06,  2.04s/it]Loading train:  99%|█████████▉| 283/285 [06:06<00:03,  1.90s/it]Loading train: 100%|█████████▉| 284/285 [06:07<00:01,  1.78s/it]Loading train: 100%|██████████| 285/285 [06:09<00:00,  1.73s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|▏         | 4/285 [00:00<00:09, 29.14it/s]concatenating: train:   2%|▏         | 7/285 [00:00<00:09, 29.39it/s]concatenating: train:   4%|▍         | 12/285 [00:00<00:08, 32.75it/s]concatenating: train:   6%|▋         | 18/285 [00:00<00:07, 37.07it/s]concatenating: train:   9%|▉         | 25/285 [00:00<00:06, 42.17it/s]concatenating: train:  11%|█         | 30/285 [00:00<00:06, 37.54it/s]concatenating: train:  12%|█▏        | 34/285 [00:00<00:07, 32.95it/s]concatenating: train:  13%|█▎        | 38/285 [00:01<00:07, 31.02it/s]concatenating: train:  15%|█▍        | 42/285 [00:01<00:12, 19.32it/s]concatenating: train:  16%|█▌        | 45/285 [00:01<00:11, 21.44it/s]concatenating: train:  17%|█▋        | 48/285 [00:01<00:11, 21.08it/s]concatenating: train:  18%|█▊        | 51/285 [00:01<00:10, 21.92it/s]concatenating: train:  19%|█▉        | 55/285 [00:01<00:09, 23.91it/s]concatenating: train:  21%|██        | 59/285 [00:02<00:08, 26.59it/s]concatenating: train:  28%|██▊       | 81/285 [00:02<00:05, 36.08it/s]concatenating: train:  33%|███▎      | 95/285 [00:02<00:04, 44.52it/s]concatenating: train:  36%|███▋      | 104/285 [00:02<00:05, 33.95it/s]concatenating: train:  39%|███▉      | 111/285 [00:02<00:04, 34.87it/s]concatenating: train:  41%|████      | 117/285 [00:03<00:04, 37.21it/s]concatenating: train:  43%|████▎     | 123/285 [00:03<00:04, 36.75it/s]concatenating: train:  45%|████▌     | 129/285 [00:03<00:03, 41.24it/s]concatenating: train:  47%|████▋     | 135/285 [00:03<00:03, 40.69it/s]concatenating: train:  49%|████▉     | 140/285 [00:03<00:03, 43.05it/s]concatenating: train:  51%|█████     | 145/285 [00:03<00:03, 36.67it/s]concatenating: train:  53%|█████▎    | 150/285 [00:03<00:03, 34.85it/s]concatenating: train:  54%|█████▍    | 154/285 [00:04<00:03, 33.69it/s]concatenating: train:  56%|█████▌    | 159/285 [00:04<00:03, 36.97it/s]concatenating: train:  58%|█████▊    | 165/285 [00:04<00:02, 40.47it/s]concatenating: train:  60%|█████▉    | 170/285 [00:04<00:03, 33.58it/s]concatenating: train:  61%|██████▏   | 175/285 [00:04<00:03, 35.66it/s]concatenating: train:  63%|██████▎   | 180/285 [00:04<00:02, 36.91it/s]concatenating: train:  65%|██████▍   | 185/285 [00:04<00:02, 38.25it/s]concatenating: train:  67%|██████▋   | 191/285 [00:04<00:02, 42.17it/s]concatenating: train:  69%|██████▉   | 198/285 [00:05<00:01, 46.16it/s]concatenating: train:  72%|███████▏  | 204/285 [00:05<00:01, 42.66it/s]concatenating: train:  73%|███████▎  | 209/285 [00:05<00:01, 41.51it/s]concatenating: train:  75%|███████▌  | 214/285 [00:05<00:01, 42.79it/s]concatenating: train:  77%|███████▋  | 219/285 [00:05<00:01, 43.34it/s]concatenating: train:  79%|███████▉  | 225/285 [00:05<00:01, 46.44it/s]concatenating: train:  82%|████████▏ | 234/285 [00:05<00:00, 53.73it/s]concatenating: train:  86%|████████▋ | 246/285 [00:05<00:00, 64.17it/s]concatenating: train:  96%|█████████▌| 273/285 [00:05<00:00, 83.14it/s]concatenating: train: 100%|██████████| 285/285 [00:06<00:00, 46.16it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:02<00:04,  2.43s/it]Loading test:  67%|██████▋   | 2/3 [00:04<00:02,  2.34s/it]Loading test: 100%|██████████| 3/3 [00:06<00:00,  2.20s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 20.19it/s]
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:11,  3.49it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:01<00:15,  2.46it/s]loading the weights for Unet:  10%|█         | 4/40 [00:02<00:16,  2.15it/s]loading the weights for Unet:  20%|██        | 8/40 [00:02<00:11,  2.75it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:03<00:10,  2.98it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:03<00:08,  3.50it/s]loading the weights for Unet:  30%|███       | 12/40 [00:03<00:08,  3.23it/s]loading the weights for Unet:  40%|████      | 16/40 [00:04<00:05,  4.14it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:05<00:13,  1.69it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:06<00:11,  1.86it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:06<00:09,  2.15it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:06<00:06,  2.79it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:07<00:04,  3.16it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:07<00:04,  2.85it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:09<00:05,  2.34it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:09<00:05,  2.06it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:10<00:03,  2.61it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:10<00:02,  2.99it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:11<00:01,  2.54it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:11<00:00,  3.15it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:12<00:01,  1.51it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:12<00:00,  3.13it/s]
(0/4) test vimp2_ctrl_991_08302013_JF
(1/4) test vimp2_967_08132013_KW
(2/4) test vimp2_765_04162013_AW
(3/4) test vimp2_ANON695_03132013
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 80, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 80, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 80, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 80, 10)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 80, 10)   910         dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 80, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 80, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 80, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 80, 10)   910         dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 80, 10)   40          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 80, 10)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 80, 10)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 80, 20)   1820        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 80, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 80, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 80, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 80, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 80, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 40, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 40, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 40, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 40, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 40, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 40, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 40, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 20, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 20, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 20, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 20, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 20, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 20, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 20, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 20, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 40, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 40, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 40, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 40, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 40, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 40, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 13)   273         dropout_8[0][0]                  
==================================================================================================
Total params: 187,773
Trainable params: 44,233
Non-trainable params: 143,540
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 29s - loss: 241.4219 - acc: 0.7490 - mDice: 0.0165 - val_loss: 59.4786 - val_acc: 0.9047 - val_mDice: 0.0123

Epoch 00001: val_mDice improved from -inf to 0.01227, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 12s - loss: 48.7631 - acc: 0.8599 - mDice: 0.0129 - val_loss: 17.4941 - val_acc: 0.9047 - val_mDice: 0.0099

Epoch 00002: val_mDice did not improve from 0.01227
Epoch 3/300
 - 13s - loss: 22.7329 - acc: 0.8672 - mDice: 0.0112 - val_loss: 9.6445 - val_acc: 0.9047 - val_mDice: 0.0091

Epoch 00003: val_mDice did not improve from 0.01227
Epoch 4/300
 - 13s - loss: 15.6539 - acc: 0.8686 - mDice: 0.0118 - val_loss: 7.6357 - val_acc: 0.9047 - val_mDice: 0.0096

Epoch 00004: val_mDice did not improve from 0.01227
Epoch 5/300
 - 13s - loss: 12.5608 - acc: 0.8689 - mDice: 0.0137 - val_loss: 6.9492 - val_acc: 0.9047 - val_mDice: 0.0107

Epoch 00005: val_mDice did not improve from 0.01227
Epoch 6/300
 - 12s - loss: 10.7844 - acc: 0.8690 - mDice: 0.0160 - val_loss: 6.6281 - val_acc: 0.9047 - val_mDice: 0.0119

Epoch 00006: val_mDice did not improve from 0.01227
Epoch 7/300
 - 11s - loss: 9.6845 - acc: 0.8690 - mDice: 0.0180 - val_loss: 6.3663 - val_acc: 0.9047 - val_mDice: 0.0127

Epoch 00007: val_mDice improved from 0.01227 to 0.01271, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 10s - loss: 8.9441 - acc: 0.8690 - mDice: 0.0202 - val_loss: 6.1926 - val_acc: 0.9047 - val_mDice: 0.0152

Epoch 00008: val_mDice improved from 0.01271 to 0.01525, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 10s - loss: 8.4073 - acc: 0.8690 - mDice: 0.0221 - val_loss: 6.1812 - val_acc: 0.9047 - val_mDice: 0.0157

Epoch 00009: val_mDice improved from 0.01525 to 0.01571, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 9s - loss: 8.0108 - acc: 0.8690 - mDice: 0.0236 - val_loss: 5.8503 - val_acc: 0.9047 - val_mDice: 0.0197

Epoch 00010: val_mDice improved from 0.01571 to 0.01968, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 10s - loss: 7.6535 - acc: 0.8690 - mDice: 0.0259 - val_loss: 5.9242 - val_acc: 0.9047 - val_mDice: 0.0186

Epoch 00011: val_mDice did not improve from 0.01968
Epoch 12/300
 - 9s - loss: 7.3438 - acc: 0.8689 - mDice: 0.0282 - val_loss: 5.8122 - val_acc: 0.9047 - val_mDice: 0.0213

Epoch 00012: val_mDice improved from 0.01968 to 0.02131, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 9s - loss: 7.0799 - acc: 0.8688 - mDice: 0.0310 - val_loss: 5.6288 - val_acc: 0.9047 - val_mDice: 0.0254

Epoch 00013: val_mDice improved from 0.02131 to 0.02539, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 10s - loss: 6.8336 - acc: 0.8685 - mDice: 0.0338 - val_loss: 5.6336 - val_acc: 0.9047 - val_mDice: 0.0270

Epoch 00014: val_mDice improved from 0.02539 to 0.02699, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 9s - loss: 6.6000 - acc: 0.8683 - mDice: 0.0371 - val_loss: 6.1591 - val_acc: 0.9047 - val_mDice: 0.0189

Epoch 00015: val_mDice did not improve from 0.02699
Epoch 16/300
 - 9s - loss: 6.3862 - acc: 0.8683 - mDice: 0.0403 - val_loss: 5.7679 - val_acc: 0.9047 - val_mDice: 0.0228

Epoch 00016: val_mDice did not improve from 0.02699
Epoch 17/300
 - 10s - loss: 6.1159 - acc: 0.8683 - mDice: 0.0472 - val_loss: 5.2559 - val_acc: 0.9047 - val_mDice: 0.0425

Epoch 00017: val_mDice improved from 0.02699 to 0.04255, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 9s - loss: 5.9105 - acc: 0.8682 - mDice: 0.0541 - val_loss: 5.3096 - val_acc: 0.9047 - val_mDice: 0.0451

Epoch 00018: val_mDice improved from 0.04255 to 0.04512, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 9s - loss: 5.7500 - acc: 0.8682 - mDice: 0.0595 - val_loss: 5.2407 - val_acc: 0.9047 - val_mDice: 0.0464

Epoch 00019: val_mDice improved from 0.04512 to 0.04637, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 10s - loss: 5.5880 - acc: 0.8683 - mDice: 0.0648 - val_loss: 5.0068 - val_acc: 0.9047 - val_mDice: 0.0580

Epoch 00020: val_mDice improved from 0.04637 to 0.05802, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 9s - loss: 5.4389 - acc: 0.8684 - mDice: 0.0710 - val_loss: 5.0587 - val_acc: 0.9047 - val_mDice: 0.0612

Epoch 00021: val_mDice improved from 0.05802 to 0.06123, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 9s - loss: 5.3235 - acc: 0.8685 - mDice: 0.0755 - val_loss: 4.7029 - val_acc: 0.9047 - val_mDice: 0.0741

Epoch 00022: val_mDice improved from 0.06123 to 0.07407, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 9s - loss: 5.1966 - acc: 0.8685 - mDice: 0.0808 - val_loss: 4.3117 - val_acc: 0.9048 - val_mDice: 0.0893

Epoch 00023: val_mDice improved from 0.07407 to 0.08929, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 10s - loss: 5.0505 - acc: 0.8689 - mDice: 0.0875 - val_loss: 4.2746 - val_acc: 0.9048 - val_mDice: 0.0928

Epoch 00024: val_mDice improved from 0.08929 to 0.09283, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 9s - loss: 4.9362 - acc: 0.8692 - mDice: 0.0932 - val_loss: 5.0218 - val_acc: 0.9048 - val_mDice: 0.0713

Epoch 00025: val_mDice did not improve from 0.09283
Epoch 26/300
 - 9s - loss: 4.8246 - acc: 0.8694 - mDice: 0.0988 - val_loss: 4.1068 - val_acc: 0.9053 - val_mDice: 0.1048

Epoch 00026: val_mDice improved from 0.09283 to 0.10478, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 9s - loss: 4.7406 - acc: 0.8696 - mDice: 0.1039 - val_loss: 4.1176 - val_acc: 0.9052 - val_mDice: 0.1064

Epoch 00027: val_mDice improved from 0.10478 to 0.10638, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 10s - loss: 4.6208 - acc: 0.8704 - mDice: 0.1116 - val_loss: 4.1111 - val_acc: 0.9052 - val_mDice: 0.1106

Epoch 00028: val_mDice improved from 0.10638 to 0.11061, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 10s - loss: 4.5051 - acc: 0.8707 - mDice: 0.1195 - val_loss: 4.5565 - val_acc: 0.9049 - val_mDice: 0.0962

Epoch 00029: val_mDice did not improve from 0.11061
Epoch 30/300
 - 10s - loss: 4.3922 - acc: 0.8714 - mDice: 0.1279 - val_loss: 4.8353 - val_acc: 0.9049 - val_mDice: 0.0933

Epoch 00030: val_mDice did not improve from 0.11061
Epoch 31/300
 - 10s - loss: 4.2906 - acc: 0.8720 - mDice: 0.1363 - val_loss: 4.6713 - val_acc: 0.9049 - val_mDice: 0.1074

Epoch 00031: val_mDice did not improve from 0.11061
Epoch 32/300
 - 11s - loss: 4.1872 - acc: 0.8728 - mDice: 0.1450 - val_loss: 4.0251 - val_acc: 0.9050 - val_mDice: 0.1354

Epoch 00032: val_mDice improved from 0.11061 to 0.13541, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 10s - loss: 4.0986 - acc: 0.8735 - mDice: 0.1529 - val_loss: 3.7433 - val_acc: 0.9055 - val_mDice: 0.1512

Epoch 00033: val_mDice improved from 0.13541 to 0.15119, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 10s - loss: 4.0215 - acc: 0.8741 - mDice: 0.1601 - val_loss: 3.5923 - val_acc: 0.9087 - val_mDice: 0.1670

Epoch 00034: val_mDice improved from 0.15119 to 0.16700, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 11s - loss: 3.9519 - acc: 0.8748 - mDice: 0.1664 - val_loss: 3.8284 - val_acc: 0.9059 - val_mDice: 0.1602

Epoch 00035: val_mDice did not improve from 0.16700
Epoch 36/300
 - 10s - loss: 3.8748 - acc: 0.8758 - mDice: 0.1744 - val_loss: 3.4869 - val_acc: 0.9088 - val_mDice: 0.1814

Epoch 00036: val_mDice improved from 0.16700 to 0.18143, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 10s - loss: 3.8100 - acc: 0.8765 - mDice: 0.1810 - val_loss: 3.4480 - val_acc: 0.9097 - val_mDice: 0.1897

Epoch 00037: val_mDice improved from 0.18143 to 0.18969, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 10s - loss: 3.7310 - acc: 0.8775 - mDice: 0.1893 - val_loss: 3.4449 - val_acc: 0.9122 - val_mDice: 0.1918

Epoch 00038: val_mDice improved from 0.18969 to 0.19178, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 10s - loss: 3.6644 - acc: 0.8781 - mDice: 0.1965 - val_loss: 3.5596 - val_acc: 0.9117 - val_mDice: 0.2015

Epoch 00039: val_mDice improved from 0.19178 to 0.20152, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 9s - loss: 3.6004 - acc: 0.8789 - mDice: 0.2049 - val_loss: 4.1599 - val_acc: 0.9104 - val_mDice: 0.1850

Epoch 00040: val_mDice did not improve from 0.20152
Epoch 41/300
 - 10s - loss: 3.5559 - acc: 0.8797 - mDice: 0.2103 - val_loss: 3.9193 - val_acc: 0.9085 - val_mDice: 0.1928

Epoch 00041: val_mDice did not improve from 0.20152
Epoch 42/300
 - 9s - loss: 3.4896 - acc: 0.8804 - mDice: 0.2181 - val_loss: 3.4196 - val_acc: 0.9139 - val_mDice: 0.2253

Epoch 00042: val_mDice improved from 0.20152 to 0.22534, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 10s - loss: 3.4307 - acc: 0.8818 - mDice: 0.2258 - val_loss: 4.1549 - val_acc: 0.9082 - val_mDice: 0.1970

Epoch 00043: val_mDice did not improve from 0.22534
Epoch 44/300
 - 9s - loss: 3.3822 - acc: 0.8820 - mDice: 0.2327 - val_loss: 3.3868 - val_acc: 0.9136 - val_mDice: 0.2330

Epoch 00044: val_mDice improved from 0.22534 to 0.23304, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 9s - loss: 3.3389 - acc: 0.8830 - mDice: 0.2385 - val_loss: 3.5073 - val_acc: 0.9156 - val_mDice: 0.2404

Epoch 00045: val_mDice improved from 0.23304 to 0.24044, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 10s - loss: 3.2964 - acc: 0.8836 - mDice: 0.2441 - val_loss: 3.8308 - val_acc: 0.9132 - val_mDice: 0.2305

Epoch 00046: val_mDice did not improve from 0.24044
Epoch 47/300
 - 9s - loss: 3.2655 - acc: 0.8844 - mDice: 0.2497 - val_loss: 3.0551 - val_acc: 0.9203 - val_mDice: 0.2693

Epoch 00047: val_mDice improved from 0.24044 to 0.26929, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 9s - loss: 3.2105 - acc: 0.8849 - mDice: 0.2564 - val_loss: 3.1078 - val_acc: 0.9187 - val_mDice: 0.2662

Epoch 00048: val_mDice did not improve from 0.26929
Epoch 49/300
 - 10s - loss: 3.1829 - acc: 0.8854 - mDice: 0.2613 - val_loss: 3.1533 - val_acc: 0.9212 - val_mDice: 0.2700

Epoch 00049: val_mDice improved from 0.26929 to 0.26995, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 9s - loss: 3.1445 - acc: 0.8862 - mDice: 0.2661 - val_loss: 3.9761 - val_acc: 0.9159 - val_mDice: 0.2426

Epoch 00050: val_mDice did not improve from 0.26995
Epoch 51/300
 - 9s - loss: 3.1021 - acc: 0.8871 - mDice: 0.2734 - val_loss: 3.0868 - val_acc: 0.9211 - val_mDice: 0.2809

Epoch 00051: val_mDice improved from 0.26995 to 0.28095, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 9s - loss: 3.0705 - acc: 0.8875 - mDice: 0.2776 - val_loss: 3.0185 - val_acc: 0.9216 - val_mDice: 0.2863

Epoch 00052: val_mDice improved from 0.28095 to 0.28635, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 10s - loss: 3.0294 - acc: 0.8885 - mDice: 0.2841 - val_loss: 3.0237 - val_acc: 0.9218 - val_mDice: 0.2931

Epoch 00053: val_mDice improved from 0.28635 to 0.29312, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 9s - loss: 3.0005 - acc: 0.8886 - mDice: 0.2875 - val_loss: 3.1377 - val_acc: 0.9225 - val_mDice: 0.2872

Epoch 00054: val_mDice did not improve from 0.29312
Epoch 55/300
 - 9s - loss: 2.9686 - acc: 0.8890 - mDice: 0.2924 - val_loss: 3.1064 - val_acc: 0.9223 - val_mDice: 0.2916

Epoch 00055: val_mDice did not improve from 0.29312
Epoch 56/300
 - 10s - loss: 2.9347 - acc: 0.8897 - mDice: 0.2979 - val_loss: 2.9764 - val_acc: 0.9218 - val_mDice: 0.3019

Epoch 00056: val_mDice improved from 0.29312 to 0.30191, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 57/300
 - 10s - loss: 2.9110 - acc: 0.8901 - mDice: 0.3012 - val_loss: 2.9406 - val_acc: 0.9230 - val_mDice: 0.3043

Epoch 00057: val_mDice improved from 0.30191 to 0.30430, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 58/300
 - 10s - loss: 2.8845 - acc: 0.8907 - mDice: 0.3059 - val_loss: 2.9158 - val_acc: 0.9226 - val_mDice: 0.3080

Epoch 00058: val_mDice improved from 0.30430 to 0.30801, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 10s - loss: 2.8636 - acc: 0.8911 - mDice: 0.3087 - val_loss: 3.0228 - val_acc: 0.9249 - val_mDice: 0.3061

Epoch 00059: val_mDice did not improve from 0.30801
Epoch 60/300
 - 11s - loss: 2.8348 - acc: 0.8920 - mDice: 0.3134 - val_loss: 3.3659 - val_acc: 0.9229 - val_mDice: 0.2985

Epoch 00060: val_mDice did not improve from 0.30801
Epoch 61/300
 - 11s - loss: 2.8224 - acc: 0.8923 - mDice: 0.3159 - val_loss: 2.9624 - val_acc: 0.9247 - val_mDice: 0.3123

Epoch 00061: val_mDice improved from 0.30801 to 0.31233, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 62/300
 - 11s - loss: 2.7980 - acc: 0.8929 - mDice: 0.3195 - val_loss: 3.0398 - val_acc: 0.9267 - val_mDice: 0.3159

Epoch 00062: val_mDice improved from 0.31233 to 0.31594, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 10s - loss: 2.7768 - acc: 0.8932 - mDice: 0.3227 - val_loss: 3.0243 - val_acc: 0.9270 - val_mDice: 0.3210

Epoch 00063: val_mDice improved from 0.31594 to 0.32096, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 11s - loss: 2.7555 - acc: 0.8940 - mDice: 0.3268 - val_loss: 3.2176 - val_acc: 0.9260 - val_mDice: 0.3087

Epoch 00064: val_mDice did not improve from 0.32096
Epoch 65/300
 - 11s - loss: 2.7333 - acc: 0.8948 - mDice: 0.3310 - val_loss: 3.3511 - val_acc: 0.9238 - val_mDice: 0.3093

Epoch 00065: val_mDice did not improve from 0.32096
Epoch 66/300
 - 11s - loss: 2.7180 - acc: 0.8951 - mDice: 0.3334 - val_loss: 2.9936 - val_acc: 0.9278 - val_mDice: 0.3261

Epoch 00066: val_mDice improved from 0.32096 to 0.32606, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 67/300
 - 10s - loss: 2.6952 - acc: 0.8958 - mDice: 0.3378 - val_loss: 2.8916 - val_acc: 0.9280 - val_mDice: 0.3344

Epoch 00067: val_mDice improved from 0.32606 to 0.33441, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 68/300
 - 11s - loss: 2.6817 - acc: 0.8961 - mDice: 0.3394 - val_loss: 3.4444 - val_acc: 0.9254 - val_mDice: 0.3132

Epoch 00068: val_mDice did not improve from 0.33441
Epoch 69/300
 - 11s - loss: 2.6646 - acc: 0.8966 - mDice: 0.3430 - val_loss: 2.9379 - val_acc: 0.9264 - val_mDice: 0.3329

Epoch 00069: val_mDice did not improve from 0.33441
Epoch 70/300
 - 11s - loss: 2.6465 - acc: 0.8973 - mDice: 0.3453 - val_loss: 3.3330 - val_acc: 0.9288 - val_mDice: 0.3247

Epoch 00070: val_mDice did not improve from 0.33441
Epoch 71/300
 - 10s - loss: 2.6337 - acc: 0.8978 - mDice: 0.3488 - val_loss: 2.8782 - val_acc: 0.9283 - val_mDice: 0.3440

Epoch 00071: val_mDice improved from 0.33441 to 0.34396, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 72/300
 - 11s - loss: 2.6102 - acc: 0.8985 - mDice: 0.3522 - val_loss: 2.9887 - val_acc: 0.9302 - val_mDice: 0.3405

Epoch 00072: val_mDice did not improve from 0.34396
Epoch 73/300
 - 10s - loss: 2.5946 - acc: 0.8990 - mDice: 0.3556 - val_loss: 2.9095 - val_acc: 0.9291 - val_mDice: 0.3472

Epoch 00073: val_mDice improved from 0.34396 to 0.34719, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 10s - loss: 2.5819 - acc: 0.8994 - mDice: 0.3583 - val_loss: 2.9079 - val_acc: 0.9309 - val_mDice: 0.3525

Epoch 00074: val_mDice improved from 0.34719 to 0.35247, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 75/300
 - 11s - loss: 2.5661 - acc: 0.8999 - mDice: 0.3608 - val_loss: 2.9549 - val_acc: 0.9305 - val_mDice: 0.3482

Epoch 00075: val_mDice did not improve from 0.35247
Epoch 76/300
 - 10s - loss: 2.5615 - acc: 0.9003 - mDice: 0.3622 - val_loss: 3.1086 - val_acc: 0.9311 - val_mDice: 0.3433

Epoch 00076: val_mDice did not improve from 0.35247
Epoch 77/300
 - 11s - loss: 2.5441 - acc: 0.9005 - mDice: 0.3652 - val_loss: 2.9595 - val_acc: 0.9249 - val_mDice: 0.3486

Epoch 00077: val_mDice did not improve from 0.35247
Epoch 78/300
 - 10s - loss: 2.5295 - acc: 0.9011 - mDice: 0.3689 - val_loss: 2.8608 - val_acc: 0.9319 - val_mDice: 0.3596

Epoch 00078: val_mDice improved from 0.35247 to 0.35961, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 79/300
 - 11s - loss: 2.5146 - acc: 0.9012 - mDice: 0.3707 - val_loss: 2.9310 - val_acc: 0.9318 - val_mDice: 0.3581

Epoch 00079: val_mDice did not improve from 0.35961
Epoch 80/300
 - 11s - loss: 2.4998 - acc: 0.9020 - mDice: 0.3744 - val_loss: 3.0783 - val_acc: 0.9318 - val_mDice: 0.3539

Epoch 00080: val_mDice did not improve from 0.35961
Epoch 81/300
 - 10s - loss: 2.4854 - acc: 0.9021 - mDice: 0.3763 - val_loss: 3.3184 - val_acc: 0.9307 - val_mDice: 0.3450

Epoch 00081: val_mDice did not improve from 0.35961
Epoch 82/300
 - 11s - loss: 2.4829 - acc: 0.9026 - mDice: 0.3776 - val_loss: 3.2419 - val_acc: 0.9318 - val_mDice: 0.3487

Epoch 00082: val_mDice did not improve from 0.35961
Epoch 83/300
 - 10s - loss: 2.4732 - acc: 0.9027 - mDice: 0.3790 - val_loss: 2.8752 - val_acc: 0.9308 - val_mDice: 0.3653

Epoch 00083: val_mDice improved from 0.35961 to 0.36526, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 84/300
 - 11s - loss: 2.4576 - acc: 0.9035 - mDice: 0.3823 - val_loss: 2.9948 - val_acc: 0.9331 - val_mDice: 0.3607

Epoch 00084: val_mDice did not improve from 0.36526
Epoch 85/300
 - 10s - loss: 2.4498 - acc: 0.9035 - mDice: 0.3836 - val_loss: 2.8716 - val_acc: 0.9267 - val_mDice: 0.3632

Epoch 00085: val_mDice did not improve from 0.36526
Epoch 86/300
 - 10s - loss: 2.4316 - acc: 0.9038 - mDice: 0.3872 - val_loss: 2.9958 - val_acc: 0.9297 - val_mDice: 0.3593

Epoch 00086: val_mDice did not improve from 0.36526
Epoch 87/300
 - 11s - loss: 2.4231 - acc: 0.9042 - mDice: 0.3889 - val_loss: 3.3170 - val_acc: 0.9298 - val_mDice: 0.3451

Epoch 00087: val_mDice did not improve from 0.36526
Epoch 88/300
 - 10s - loss: 2.4140 - acc: 0.9047 - mDice: 0.3905 - val_loss: 2.9275 - val_acc: 0.9317 - val_mDice: 0.3662

Epoch 00088: val_mDice improved from 0.36526 to 0.36623, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 89/300
 - 10s - loss: 2.4015 - acc: 0.9047 - mDice: 0.3930 - val_loss: 2.8282 - val_acc: 0.9324 - val_mDice: 0.3697

Epoch 00089: val_mDice improved from 0.36623 to 0.36969, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 90/300
 - 10s - loss: 2.3984 - acc: 0.9049 - mDice: 0.3934 - val_loss: 3.0958 - val_acc: 0.9329 - val_mDice: 0.3603

Epoch 00090: val_mDice did not improve from 0.36969
Epoch 91/300
 - 10s - loss: 2.3834 - acc: 0.9053 - mDice: 0.3964 - val_loss: 2.8865 - val_acc: 0.9319 - val_mDice: 0.3723

Epoch 00091: val_mDice improved from 0.36969 to 0.37235, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 92/300
 - 11s - loss: 2.3791 - acc: 0.9056 - mDice: 0.3978 - val_loss: 2.9531 - val_acc: 0.9324 - val_mDice: 0.3696

Epoch 00092: val_mDice did not improve from 0.37235
Epoch 93/300
 - 10s - loss: 2.3647 - acc: 0.9059 - mDice: 0.3998 - val_loss: 2.8128 - val_acc: 0.9310 - val_mDice: 0.3783

Epoch 00093: val_mDice improved from 0.37235 to 0.37829, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 94/300
 - 10s - loss: 2.3573 - acc: 0.9060 - mDice: 0.4017 - val_loss: 2.8308 - val_acc: 0.9320 - val_mDice: 0.3776

Epoch 00094: val_mDice did not improve from 0.37829
Epoch 95/300
 - 10s - loss: 2.3454 - acc: 0.9066 - mDice: 0.4044 - val_loss: 2.9517 - val_acc: 0.9344 - val_mDice: 0.3810

Epoch 00095: val_mDice improved from 0.37829 to 0.38101, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 96/300
 - 10s - loss: 2.3413 - acc: 0.9067 - mDice: 0.4050 - val_loss: 2.9304 - val_acc: 0.9338 - val_mDice: 0.3811

Epoch 00096: val_mDice improved from 0.38101 to 0.38114, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 97/300
 - 10s - loss: 2.3340 - acc: 0.9070 - mDice: 0.4065 - val_loss: 2.8607 - val_acc: 0.9332 - val_mDice: 0.3882

Epoch 00097: val_mDice improved from 0.38114 to 0.38819, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 98/300
 - 11s - loss: 2.3274 - acc: 0.9072 - mDice: 0.4082 - val_loss: 3.1555 - val_acc: 0.9339 - val_mDice: 0.3704

Epoch 00098: val_mDice did not improve from 0.38819
Epoch 99/300
 - 10s - loss: 2.3160 - acc: 0.9074 - mDice: 0.4102 - val_loss: 2.9461 - val_acc: 0.9356 - val_mDice: 0.3849

Epoch 00099: val_mDice did not improve from 0.38819
Epoch 100/300
 - 10s - loss: 2.3052 - acc: 0.9076 - mDice: 0.4114 - val_loss: 2.8866 - val_acc: 0.9317 - val_mDice: 0.3796

Epoch 00100: val_mDice did not improve from 0.38819
Epoch 101/300
 - 11s - loss: 2.2970 - acc: 0.9080 - mDice: 0.4143 - val_loss: 2.8802 - val_acc: 0.9331 - val_mDice: 0.3888

Epoch 00101: val_mDice improved from 0.38819 to 0.38876, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 102/300
 - 10s - loss: 2.2912 - acc: 0.9080 - mDice: 0.4151 - val_loss: 2.7934 - val_acc: 0.9346 - val_mDice: 0.3934

Epoch 00102: val_mDice improved from 0.38876 to 0.39337, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 103/300
 - 10s - loss: 2.2842 - acc: 0.9083 - mDice: 0.4174 - val_loss: 2.9074 - val_acc: 0.9344 - val_mDice: 0.3887

Epoch 00103: val_mDice did not improve from 0.39337
Epoch 104/300
 - 11s - loss: 2.2760 - acc: 0.9087 - mDice: 0.4185 - val_loss: 2.8100 - val_acc: 0.9317 - val_mDice: 0.3890

Epoch 00104: val_mDice did not improve from 0.39337
Epoch 105/300
 - 9s - loss: 2.2685 - acc: 0.9089 - mDice: 0.4204 - val_loss: 2.9484 - val_acc: 0.9348 - val_mDice: 0.3874

Epoch 00105: val_mDice did not improve from 0.39337
Epoch 106/300
 - 10s - loss: 2.2658 - acc: 0.9091 - mDice: 0.4208 - val_loss: 3.0415 - val_acc: 0.9351 - val_mDice: 0.3915

Epoch 00106: val_mDice did not improve from 0.39337
Epoch 107/300
 - 10s - loss: 2.2509 - acc: 0.9091 - mDice: 0.4238 - val_loss: 3.0744 - val_acc: 0.9346 - val_mDice: 0.3913

Epoch 00107: val_mDice did not improve from 0.39337
Epoch 108/300
 - 10s - loss: 2.2436 - acc: 0.9097 - mDice: 0.4261 - val_loss: 3.3737 - val_acc: 0.9337 - val_mDice: 0.3736

Epoch 00108: val_mDice did not improve from 0.39337
Epoch 109/300
 - 10s - loss: 2.2422 - acc: 0.9101 - mDice: 0.4268 - val_loss: 3.0172 - val_acc: 0.9350 - val_mDice: 0.3886

Epoch 00109: val_mDice did not improve from 0.39337
Epoch 110/300
 - 10s - loss: 2.2272 - acc: 0.9105 - mDice: 0.4299 - val_loss: 2.8794 - val_acc: 0.9343 - val_mDice: 0.3892

Epoch 00110: val_mDice did not improve from 0.39337
Epoch 111/300
 - 10s - loss: 2.2194 - acc: 0.9108 - mDice: 0.4316 - val_loss: 2.9389 - val_acc: 0.9356 - val_mDice: 0.3953

Epoch 00111: val_mDice improved from 0.39337 to 0.39534, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 112/300
 - 10s - loss: 2.2080 - acc: 0.9110 - mDice: 0.4348 - val_loss: 2.8320 - val_acc: 0.9322 - val_mDice: 0.3936

Epoch 00112: val_mDice did not improve from 0.39534
Epoch 113/300
 - 10s - loss: 2.2020 - acc: 0.9115 - mDice: 0.4353 - val_loss: 2.8937 - val_acc: 0.9343 - val_mDice: 0.3944

Epoch 00113: val_mDice did not improve from 0.39534
Epoch 114/300
 - 10s - loss: 2.1995 - acc: 0.9116 - mDice: 0.4364 - val_loss: 2.9128 - val_acc: 0.9340 - val_mDice: 0.3905

Epoch 00114: val_mDice did not improve from 0.39534
Epoch 115/300
 - 10s - loss: 2.1824 - acc: 0.9118 - mDice: 0.4395 - val_loss: 2.9730 - val_acc: 0.9370 - val_mDice: 0.4048

Epoch 00115: val_mDice improved from 0.39534 to 0.40476, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 116/300
 - 10s - loss: 2.1843 - acc: 0.9120 - mDice: 0.4395 - val_loss: 3.1542 - val_acc: 0.9353 - val_mDice: 0.3890

Epoch 00116: val_mDice did not improve from 0.40476
Epoch 117/300
 - 10s - loss: 2.1717 - acc: 0.9124 - mDice: 0.4425 - val_loss: 2.9142 - val_acc: 0.9324 - val_mDice: 0.4035

Epoch 00117: val_mDice did not improve from 0.40476
Epoch 118/300
 - 10s - loss: 2.1669 - acc: 0.9126 - mDice: 0.4436 - val_loss: 2.8796 - val_acc: 0.9350 - val_mDice: 0.4093

Epoch 00118: val_mDice improved from 0.40476 to 0.40933, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 119/300
 - 9s - loss: 2.1537 - acc: 0.9130 - mDice: 0.4469 - val_loss: 3.3328 - val_acc: 0.9349 - val_mDice: 0.3893

Epoch 00119: val_mDice did not improve from 0.40933
Epoch 120/300
 - 10s - loss: 2.1521 - acc: 0.9129 - mDice: 0.4475 - val_loss: 2.9113 - val_acc: 0.9363 - val_mDice: 0.4063

Epoch 00120: val_mDice did not improve from 0.40933
Epoch 121/300
 - 10s - loss: 2.1493 - acc: 0.9132 - mDice: 0.4483 - val_loss: 3.0404 - val_acc: 0.9374 - val_mDice: 0.4072

Epoch 00121: val_mDice did not improve from 0.40933
Epoch 122/300
 - 10s - loss: 2.1365 - acc: 0.9133 - mDice: 0.4516 - val_loss: 3.3448 - val_acc: 0.9349 - val_mDice: 0.3880

Epoch 00122: val_mDice did not improve from 0.40933
Epoch 123/300
 - 10s - loss: 2.1400 - acc: 0.9132 - mDice: 0.4503 - val_loss: 3.0463 - val_acc: 0.9362 - val_mDice: 0.4031

Epoch 00123: val_mDice did not improve from 0.40933
Epoch 124/300
 - 10s - loss: 2.1231 - acc: 0.9138 - mDice: 0.4536 - val_loss: 3.2009 - val_acc: 0.9355 - val_mDice: 0.3951

Epoch 00124: val_mDice did not improve from 0.40933
Epoch 125/300
 - 10s - loss: 2.1201 - acc: 0.9138 - mDice: 0.4556 - val_loss: 3.1162 - val_acc: 0.9363 - val_mDice: 0.4115

Epoch 00125: val_mDice improved from 0.40933 to 0.41153, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 126/300
 - 10s - loss: 2.1124 - acc: 0.9141 - mDice: 0.4573 - val_loss: 4.0344 - val_acc: 0.9310 - val_mDice: 0.3503

Epoch 00126: val_mDice did not improve from 0.41153
Epoch 127/300
 - 10s - loss: 2.1130 - acc: 0.9142 - mDice: 0.4572 - val_loss: 3.1925 - val_acc: 0.9363 - val_mDice: 0.3987

Epoch 00127: val_mDice did not improve from 0.41153
Epoch 128/300
 - 10s - loss: 2.0957 - acc: 0.9146 - mDice: 0.4609 - val_loss: 2.9627 - val_acc: 0.9366 - val_mDice: 0.4108

Epoch 00128: val_mDice did not improve from 0.41153
Epoch 129/300
 - 10s - loss: 2.0996 - acc: 0.9145 - mDice: 0.4597 - val_loss: 2.8400 - val_acc: 0.9365 - val_mDice: 0.4164

Epoch 00129: val_mDice improved from 0.41153 to 0.41640, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 130/300
 - 10s - loss: 2.0836 - acc: 0.9150 - mDice: 0.4632 - val_loss: 3.1802 - val_acc: 0.9364 - val_mDice: 0.4043

Epoch 00130: val_mDice did not improve from 0.41640
Epoch 131/300
 - 10s - loss: 2.0793 - acc: 0.9151 - mDice: 0.4652 - val_loss: 2.9562 - val_acc: 0.9362 - val_mDice: 0.4115

Epoch 00131: val_mDice did not improve from 0.41640
Epoch 132/300
 - 10s - loss: 2.0740 - acc: 0.9151 - mDice: 0.4654 - val_loss: 3.2794 - val_acc: 0.9350 - val_mDice: 0.3946

Epoch 00132: val_mDice did not improve from 0.41640
Epoch 133/300
 - 10s - loss: 2.0689 - acc: 0.9154 - mDice: 0.4676 - val_loss: 2.9401 - val_acc: 0.9357 - val_mDice: 0.4071

Epoch 00133: val_mDice did not improve from 0.41640
Epoch 134/300
 - 10s - loss: 2.0670 - acc: 0.9155 - mDice: 0.4675 - val_loss: 3.1610 - val_acc: 0.9365 - val_mDice: 0.4042

Epoch 00134: val_mDice did not improve from 0.41640
Epoch 135/300
 - 10s - loss: 2.0647 - acc: 0.9154 - mDice: 0.4679 - val_loss: 3.0606 - val_acc: 0.9354 - val_mDice: 0.4023

Epoch 00135: val_mDice did not improve from 0.41640
Epoch 136/300
 - 10s - loss: 2.0605 - acc: 0.9156 - mDice: 0.4692 - val_loss: 3.0582 - val_acc: 0.9348 - val_mDice: 0.4056

Epoch 00136: val_mDice did not improve from 0.41640
Epoch 137/300
 - 10s - loss: 2.0518 - acc: 0.9159 - mDice: 0.4711 - val_loss: 2.9773 - val_acc: 0.9359 - val_mDice: 0.4165

Epoch 00137: val_mDice improved from 0.41640 to 0.41655, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 138/300
 - 10s - loss: 2.0430 - acc: 0.9162 - mDice: 0.4734 - val_loss: 2.9253 - val_acc: 0.9372 - val_mDice: 0.4165

Epoch 00138: val_mDice did not improve from 0.41655
Epoch 139/300
 - 10s - loss: 2.0354 - acc: 0.9164 - mDice: 0.4744 - val_loss: 2.9942 - val_acc: 0.9361 - val_mDice: 0.4164

Epoch 00139: val_mDice did not improve from 0.41655
Epoch 140/300
 - 10s - loss: 2.0358 - acc: 0.9162 - mDice: 0.4741 - val_loss: 3.0660 - val_acc: 0.9364 - val_mDice: 0.4104

Epoch 00140: val_mDice did not improve from 0.41655
Epoch 141/300
 - 10s - loss: 2.0364 - acc: 0.9165 - mDice: 0.4751 - val_loss: 2.9320 - val_acc: 0.9367 - val_mDice: 0.4241

Epoch 00141: val_mDice improved from 0.41655 to 0.42410, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 142/300
 - 9s - loss: 2.0293 - acc: 0.9165 - mDice: 0.4769 - val_loss: 2.9012 - val_acc: 0.9352 - val_mDice: 0.4116

Epoch 00142: val_mDice did not improve from 0.42410
Epoch 143/300
 - 10s - loss: 2.0229 - acc: 0.9169 - mDice: 0.4779 - val_loss: 3.2584 - val_acc: 0.9362 - val_mDice: 0.4057

Epoch 00143: val_mDice did not improve from 0.42410
Epoch 144/300
 - 10s - loss: 2.0169 - acc: 0.9169 - mDice: 0.4787 - val_loss: 2.9006 - val_acc: 0.9384 - val_mDice: 0.4235

Epoch 00144: val_mDice did not improve from 0.42410
Epoch 145/300
 - 10s - loss: 2.0134 - acc: 0.9168 - mDice: 0.4804 - val_loss: 2.8763 - val_acc: 0.9354 - val_mDice: 0.4183

Epoch 00145: val_mDice did not improve from 0.42410
Epoch 146/300
 - 10s - loss: 2.0125 - acc: 0.9171 - mDice: 0.4811 - val_loss: 3.1768 - val_acc: 0.9367 - val_mDice: 0.4109

Epoch 00146: val_mDice did not improve from 0.42410
Epoch 147/300
 - 10s - loss: 2.0121 - acc: 0.9171 - mDice: 0.4811 - val_loss: 3.1122 - val_acc: 0.9362 - val_mDice: 0.4142

Epoch 00147: val_mDice did not improve from 0.42410
Epoch 148/300
 - 10s - loss: 2.0053 - acc: 0.9172 - mDice: 0.4819 - val_loss: 2.9939 - val_acc: 0.9353 - val_mDice: 0.4160

Epoch 00148: val_mDice did not improve from 0.42410
Epoch 149/300
 - 10s - loss: 2.0006 - acc: 0.9172 - mDice: 0.4825 - val_loss: 3.0797 - val_acc: 0.9377 - val_mDice: 0.4221

Epoch 00149: val_mDice did not improve from 0.42410
Epoch 150/300
 - 10s - loss: 1.9924 - acc: 0.9175 - mDice: 0.4848 - val_loss: 3.3794 - val_acc: 0.9357 - val_mDice: 0.4034

Epoch 00150: val_mDice did not improve from 0.42410
Epoch 151/300
 - 10s - loss: 1.9923 - acc: 0.9176 - mDice: 0.4862 - val_loss: 3.0723 - val_acc: 0.9362 - val_mDice: 0.4116

Epoch 00151: val_mDice did not improve from 0.42410
Epoch 152/300
 - 10s - loss: 1.9891 - acc: 0.9177 - mDice: 0.4860 - val_loss: 3.1669 - val_acc: 0.9369 - val_mDice: 0.4080

Epoch 00152: val_mDice did not improve from 0.42410
Epoch 153/300
 - 10s - loss: 1.9847 - acc: 0.9181 - mDice: 0.4879 - val_loss: 3.1142 - val_acc: 0.9377 - val_mDice: 0.4255

Epoch 00153: val_mDice improved from 0.42410 to 0.42553, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 154/300
 - 10s - loss: 1.9784 - acc: 0.9182 - mDice: 0.4886 - val_loss: 3.0775 - val_acc: 0.9378 - val_mDice: 0.4255

Epoch 00154: val_mDice did not improve from 0.42553
Epoch 155/300
 - 10s - loss: 1.9795 - acc: 0.9181 - mDice: 0.4885 - val_loss: 3.4279 - val_acc: 0.9366 - val_mDice: 0.4094

Epoch 00155: val_mDice did not improve from 0.42553
Epoch 156/300
 - 10s - loss: 1.9732 - acc: 0.9183 - mDice: 0.4895 - val_loss: 3.1112 - val_acc: 0.9356 - val_mDice: 0.4095

Epoch 00156: val_mDice did not improve from 0.42553
Epoch 157/300
 - 10s - loss: 1.9693 - acc: 0.9184 - mDice: 0.4907 - val_loss: 3.0831 - val_acc: 0.9350 - val_mDice: 0.4159

Epoch 00157: val_mDice did not improve from 0.42553
Epoch 158/300
 - 10s - loss: 1.9704 - acc: 0.9184 - mDice: 0.4910 - val_loss: 2.8938 - val_acc: 0.9375 - val_mDice: 0.4279

Epoch 00158: val_mDice improved from 0.42553 to 0.42787, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 159/300
 - 10s - loss: 1.9569 - acc: 0.9188 - mDice: 0.4938 - val_loss: 3.0061 - val_acc: 0.9363 - val_mDice: 0.4186

Epoch 00159: val_mDice did not improve from 0.42787
Epoch 160/300
 - 10s - loss: 1.9607 - acc: 0.9188 - mDice: 0.4937 - val_loss: 3.0127 - val_acc: 0.9357 - val_mDice: 0.4217

Epoch 00160: val_mDice did not improve from 0.42787
Epoch 161/300
 - 9s - loss: 1.9598 - acc: 0.9187 - mDice: 0.4931 - val_loss: 2.8950 - val_acc: 0.9380 - val_mDice: 0.4278

Epoch 00161: val_mDice did not improve from 0.42787
Epoch 162/300
 - 10s - loss: 1.9553 - acc: 0.9189 - mDice: 0.4941 - val_loss: 3.4276 - val_acc: 0.9368 - val_mDice: 0.4103

Epoch 00162: val_mDice did not improve from 0.42787
Epoch 163/300
 - 10s - loss: 1.9539 - acc: 0.9190 - mDice: 0.4944 - val_loss: 3.3501 - val_acc: 0.9368 - val_mDice: 0.4074

Epoch 00163: val_mDice did not improve from 0.42787
Epoch 164/300
 - 10s - loss: 1.9485 - acc: 0.9192 - mDice: 0.4960 - val_loss: 3.1657 - val_acc: 0.9372 - val_mDice: 0.4159

Epoch 00164: val_mDice did not improve from 0.42787
Epoch 165/300
 - 10s - loss: 1.9505 - acc: 0.9190 - mDice: 0.4953 - val_loss: 2.9314 - val_acc: 0.9368 - val_mDice: 0.4241

Epoch 00165: val_mDice did not improve from 0.42787
Epoch 166/300
 - 10s - loss: 1.9338 - acc: 0.9195 - mDice: 0.4989 - val_loss: 2.8484 - val_acc: 0.9359 - val_mDice: 0.4214

Epoch 00166: val_mDice did not improve from 0.42787
Epoch 167/300
 - 10s - loss: 1.9350 - acc: 0.9198 - mDice: 0.4994 - val_loss: 2.8575 - val_acc: 0.9376 - val_mDice: 0.4286

Epoch 00167: val_mDice improved from 0.42787 to 0.42857, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 168/300
 - 10s - loss: 1.9339 - acc: 0.9197 - mDice: 0.4995 - val_loss: 3.1409 - val_acc: 0.9376 - val_mDice: 0.4210

Epoch 00168: val_mDice did not improve from 0.42857
Epoch 169/300
 - 10s - loss: 1.9266 - acc: 0.9200 - mDice: 0.5007 - val_loss: 3.2479 - val_acc: 0.9383 - val_mDice: 0.4194

Epoch 00169: val_mDice did not improve from 0.42857
Epoch 170/300
 - 10s - loss: 1.9355 - acc: 0.9199 - mDice: 0.4994 - val_loss: 2.9919 - val_acc: 0.9372 - val_mDice: 0.4202

Epoch 00170: val_mDice did not improve from 0.42857
Epoch 171/300
 - 10s - loss: 1.9285 - acc: 0.9198 - mDice: 0.5006 - val_loss: 2.9763 - val_acc: 0.9369 - val_mDice: 0.4265

Epoch 00171: val_mDice did not improve from 0.42857
Epoch 172/300
 - 10s - loss: 1.9269 - acc: 0.9200 - mDice: 0.5015 - val_loss: 3.1255 - val_acc: 0.9369 - val_mDice: 0.4190

Epoch 00172: val_mDice did not improve from 0.42857
Epoch 173/300
 - 10s - loss: 1.9255 - acc: 0.9202 - mDice: 0.5015 - val_loss: 2.8647 - val_acc: 0.9372 - val_mDice: 0.4271

Epoch 00173: val_mDice did not improve from 0.42857
Epoch 174/300
 - 10s - loss: 1.9231 - acc: 0.9202 - mDice: 0.5019 - val_loss: 2.8784 - val_acc: 0.9372 - val_mDice: 0.4295

Epoch 00174: val_mDice improved from 0.42857 to 0.42950, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 175/300
 - 10s - loss: 1.9152 - acc: 0.9205 - mDice: 0.5036 - val_loss: 2.9547 - val_acc: 0.9367 - val_mDice: 0.4317

Epoch 00175: val_mDice improved from 0.42950 to 0.43174, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 176/300
 - 10s - loss: 1.9190 - acc: 0.9204 - mDice: 0.5028 - val_loss: 3.2610 - val_acc: 0.9375 - val_mDice: 0.4178

Epoch 00176: val_mDice did not improve from 0.43174
Epoch 177/300
 - 9s - loss: 1.9189 - acc: 0.9204 - mDice: 0.5030 - val_loss: 2.8851 - val_acc: 0.9361 - val_mDice: 0.4190

Epoch 00177: val_mDice did not improve from 0.43174
Epoch 178/300
 - 10s - loss: 1.9094 - acc: 0.9208 - mDice: 0.5053 - val_loss: 2.8825 - val_acc: 0.9352 - val_mDice: 0.4201

Epoch 00178: val_mDice did not improve from 0.43174
Epoch 179/300
 - 10s - loss: 1.9027 - acc: 0.9208 - mDice: 0.5069 - val_loss: 2.9407 - val_acc: 0.9378 - val_mDice: 0.4243

Epoch 00179: val_mDice did not improve from 0.43174
Epoch 180/300
 - 10s - loss: 1.9004 - acc: 0.9211 - mDice: 0.5077 - val_loss: 2.8266 - val_acc: 0.9364 - val_mDice: 0.4219

Epoch 00180: val_mDice did not improve from 0.43174
Epoch 181/300
 - 10s - loss: 1.8977 - acc: 0.9213 - mDice: 0.5084 - val_loss: 3.0150 - val_acc: 0.9377 - val_mDice: 0.4249

Epoch 00181: val_mDice did not improve from 0.43174
Epoch 182/300
 - 10s - loss: 1.8956 - acc: 0.9211 - mDice: 0.5082 - val_loss: 2.7250 - val_acc: 0.9395 - val_mDice: 0.4366

Epoch 00182: val_mDice improved from 0.43174 to 0.43660, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 183/300
 - 10s - loss: 1.8935 - acc: 0.9213 - mDice: 0.5091 - val_loss: 3.0652 - val_acc: 0.9385 - val_mDice: 0.4280

Epoch 00183: val_mDice did not improve from 0.43660
Epoch 184/300
 - 10s - loss: 1.8926 - acc: 0.9213 - mDice: 0.5099 - val_loss: 3.4247 - val_acc: 0.9367 - val_mDice: 0.4075

Epoch 00184: val_mDice did not improve from 0.43660
Epoch 185/300
 - 10s - loss: 1.8907 - acc: 0.9215 - mDice: 0.5093 - val_loss: 2.8644 - val_acc: 0.9395 - val_mDice: 0.4376

Epoch 00185: val_mDice improved from 0.43660 to 0.43761, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 186/300
 - 10s - loss: 1.8919 - acc: 0.9215 - mDice: 0.5099 - val_loss: 3.2039 - val_acc: 0.9375 - val_mDice: 0.4181

Epoch 00186: val_mDice did not improve from 0.43761
Epoch 187/300
 - 10s - loss: 1.8847 - acc: 0.9216 - mDice: 0.5114 - val_loss: 3.2172 - val_acc: 0.9385 - val_mDice: 0.4229

Epoch 00187: val_mDice did not improve from 0.43761
Epoch 188/300
 - 10s - loss: 1.8873 - acc: 0.9217 - mDice: 0.5110 - val_loss: 3.1169 - val_acc: 0.9377 - val_mDice: 0.4247

Epoch 00188: val_mDice did not improve from 0.43761
Epoch 189/300
 - 10s - loss: 1.8814 - acc: 0.9218 - mDice: 0.5121 - val_loss: 2.9721 - val_acc: 0.9370 - val_mDice: 0.4283

Epoch 00189: val_mDice did not improve from 0.43761
Epoch 190/300
 - 10s - loss: 1.8789 - acc: 0.9218 - mDice: 0.5126 - val_loss: 3.0684 - val_acc: 0.9380 - val_mDice: 0.4268

Epoch 00190: val_mDice did not improve from 0.43761
Epoch 191/300
 - 10s - loss: 1.8808 - acc: 0.9219 - mDice: 0.5126 - val_loss: 2.8386 - val_acc: 0.9388 - val_mDice: 0.4333

Epoch 00191: val_mDice did not improve from 0.43761
Epoch 192/300
 - 9s - loss: 1.8745 - acc: 0.9221 - mDice: 0.5136 - val_loss: 3.0173 - val_acc: 0.9387 - val_mDice: 0.4279

Epoch 00192: val_mDice did not improve from 0.43761
Epoch 193/300
 - 10s - loss: 1.8790 - acc: 0.9222 - mDice: 0.5132 - val_loss: 3.1566 - val_acc: 0.9373 - val_mDice: 0.4232

Epoch 00193: val_mDice did not improve from 0.43761
Epoch 194/300
 - 10s - loss: 1.8764 - acc: 0.9221 - mDice: 0.5135 - val_loss: 2.9562 - val_acc: 0.9394 - val_mDice: 0.4323

Epoch 00194: val_mDice did not improve from 0.43761
Epoch 195/300
 - 10s - loss: 1.8760 - acc: 0.9220 - mDice: 0.5137 - val_loss: 3.4630 - val_acc: 0.9371 - val_mDice: 0.4033

Epoch 00195: val_mDice did not improve from 0.43761
Epoch 196/300
 - 10s - loss: 1.8711 - acc: 0.9222 - mDice: 0.5145 - val_loss: 2.9448 - val_acc: 0.9375 - val_mDice: 0.4267

Epoch 00196: val_mDice did not improve from 0.43761
Epoch 197/300
 - 10s - loss: 1.8714 - acc: 0.9221 - mDice: 0.5148 - val_loss: 2.9505 - val_acc: 0.9393 - val_mDice: 0.4287

Epoch 00197: val_mDice did not improve from 0.43761
Epoch 198/300
 - 9s - loss: 1.8644 - acc: 0.9225 - mDice: 0.5161 - val_loss: 3.1779 - val_acc: 0.9387 - val_mDice: 0.4229

Epoch 00198: val_mDice did not improve from 0.43761
Epoch 199/300
 - 10s - loss: 1.8637 - acc: 0.9224 - mDice: 0.5167 - val_loss: 2.9877 - val_acc: 0.9388 - val_mDice: 0.4328

Epoch 00199: val_mDice did not improve from 0.43761
Epoch 200/300
 - 10s - loss: 1.8620 - acc: 0.9226 - mDice: 0.5174 - val_loss: 3.3856 - val_acc: 0.9362 - val_mDice: 0.4048

Epoch 00200: val_mDice did not improve from 0.43761
Epoch 201/300
 - 10s - loss: 1.8635 - acc: 0.9226 - mDice: 0.5162 - val_loss: 3.0709 - val_acc: 0.9381 - val_mDice: 0.4285

Epoch 00201: val_mDice did not improve from 0.43761
Epoch 202/300
 - 10s - loss: 1.8580 - acc: 0.9225 - mDice: 0.5178 - val_loss: 3.2548 - val_acc: 0.9377 - val_mDice: 0.4162

Epoch 00202: val_mDice did not improve from 0.43761
Epoch 203/300
 - 10s - loss: 1.8538 - acc: 0.9228 - mDice: 0.5192 - val_loss: 3.0808 - val_acc: 0.9394 - val_mDice: 0.4295

Epoch 00203: val_mDice did not improve from 0.43761
Epoch 204/300
 - 10s - loss: 1.8563 - acc: 0.9228 - mDice: 0.5185 - val_loss: 3.4071 - val_acc: 0.9383 - val_mDice: 0.4182

Epoch 00204: val_mDice did not improve from 0.43761
Epoch 205/300
 - 10s - loss: 1.8595 - acc: 0.9228 - mDice: 0.5179 - val_loss: 3.2574 - val_acc: 0.9381 - val_mDice: 0.4198

Epoch 00205: val_mDice did not improve from 0.43761
Epoch 206/300
 - 10s - loss: 1.8630 - acc: 0.9227 - mDice: 0.5177 - val_loss: 3.1975 - val_acc: 0.9393 - val_mDice: 0.4280

Epoch 00206: val_mDice did not improve from 0.43761
Epoch 207/300
 - 10s - loss: 1.8496 - acc: 0.9234 - mDice: 0.5203 - val_loss: 3.0880 - val_acc: 0.9390 - val_mDice: 0.4249

Epoch 00207: val_mDice did not improve from 0.43761
Epoch 208/300
 - 10s - loss: 1.8497 - acc: 0.9231 - mDice: 0.5200 - val_loss: 3.3154 - val_acc: 0.9389 - val_mDice: 0.4194

Epoch 00208: val_mDice did not improve from 0.43761
Epoch 209/300
 - 10s - loss: 1.8434 - acc: 0.9232 - mDice: 0.5218 - val_loss: 2.9523 - val_acc: 0.9400 - val_mDice: 0.4333

Epoch 00209: val_mDice did not improve from 0.43761
Epoch 210/300
 - 10s - loss: 1.8446 - acc: 0.9233 - mDice: 0.5215 - val_loss: 3.2127 - val_acc: 0.9396 - val_mDice: 0.4260

Epoch 00210: val_mDice did not improve from 0.43761
Epoch 211/300
 - 10s - loss: 1.8415 - acc: 0.9234 - mDice: 0.5218 - val_loss: 3.1060 - val_acc: 0.9398 - val_mDice: 0.4300

Epoch 00211: val_mDice did not improve from 0.43761
Epoch 212/300
 - 10s - loss: 1.8398 - acc: 0.9235 - mDice: 0.5225 - val_loss: 3.0271 - val_acc: 0.9396 - val_mDice: 0.4326

Epoch 00212: val_mDice did not improve from 0.43761
Epoch 213/300
 - 9s - loss: 1.8383 - acc: 0.9236 - mDice: 0.5226 - val_loss: 3.3839 - val_acc: 0.9371 - val_mDice: 0.4095

Epoch 00213: val_mDice did not improve from 0.43761
Epoch 214/300
 - 10s - loss: 1.8382 - acc: 0.9235 - mDice: 0.5232 - val_loss: 3.1127 - val_acc: 0.9400 - val_mDice: 0.4282

Epoch 00214: val_mDice did not improve from 0.43761
Epoch 215/300
 - 10s - loss: 1.8325 - acc: 0.9239 - mDice: 0.5239 - val_loss: 3.1430 - val_acc: 0.9386 - val_mDice: 0.4259

Epoch 00215: val_mDice did not improve from 0.43761
Restoring model weights from the end of the best epoch
Epoch 00215: early stopping
{'val_loss': [59.47857474145435, 17.494075917062304, 9.644453443232036, 7.635699834142413, 6.94918808589379, 6.628062281580198, 6.366253711815391, 6.19264239196976, 6.181212272495031, 5.85033106094315, 5.924150726092713, 5.81220105591984, 5.628806773395765, 5.633588897507815, 6.159075021388984, 5.7678855662899355, 5.255868366963806, 5.309561872500039, 5.240683068388274, 5.006830036063635, 5.058661127196891, 4.702938479593112, 4.3117160356293125, 4.2745972732525495, 5.021838077344, 4.106751644274309, 4.117619177060468, 4.111094112641045, 4.556475898472681, 4.8352710617085295, 4.671301595600588, 4.025059877983516, 3.743317886477425, 3.5922933413336673, 3.8283955403825356, 3.486851712067922, 3.4480458266500915, 3.4448970629877986, 3.559573064247767, 4.159936658816323, 3.919312948183644, 3.419594920462086, 4.154903641724515, 3.3868213365564035, 3.5073488684637204, 3.8308077160535112, 3.0550531817688826, 3.107779034191654, 3.1533340905748664, 3.9761058086795464, 3.0867867366011654, 3.018526765739634, 3.0237391340945448, 3.1376611848494838, 3.106427008907, 2.976401461997912, 2.9406151428286518, 2.9157896826842, 3.022778682588112, 3.365874993082668, 2.962407126135769, 3.03980191663972, 3.0243065074263584, 3.2175830747222616, 3.3511414962953756, 2.9935694998573688, 2.8915984458511783, 3.444438469773602, 2.9378778326014676, 3.333005276552978, 2.878165009564587, 2.98866762034595, 2.9094811311612525, 2.9078506826467456, 2.9548904229665087, 3.108566653293868, 2.95946844303537, 2.860794884950987, 2.9309527229606394, 3.0783006441114202, 3.31837590154083, 3.241878920678227, 2.875217683524603, 2.994794336945883, 2.8716433673564876, 2.995827985395278, 3.316962572435538, 2.9275095645959177, 2.8282491030792394, 3.095847148004742, 2.88650555261189, 2.9530521994900134, 2.812777818091923, 2.830832741888506, 2.951678165589415, 2.9303511934177506, 2.8607490453869104, 3.155533722025298, 2.94614603186381, 2.886608673969195, 2.880213023651214, 2.793437096512034, 2.907376339925187, 2.809976858264279, 2.9484367849127877, 3.041507224197544, 3.074396377828504, 3.3737074454714144, 3.017249089266573, 2.879381575532967, 2.938854282012298, 2.8320074756408022, 2.893664198883233, 2.912832445509377, 2.9729776324792985, 3.154157168143207, 2.914200641880078, 2.8796414632704996, 3.3328280685957345, 2.9113257877706062, 3.040422466450504, 3.344789369187007, 3.0462607492116236, 3.2008817191900953, 3.116169263630928, 4.034366419206241, 3.1924833704362667, 2.962699518494663, 2.840018893636408, 3.180164348156679, 2.956201186714073, 3.279381412303164, 2.9401457847229073, 3.161022282471614, 3.060594423407955, 3.0582221238652156, 2.9773377029313925, 2.925323823866035, 2.994207721669227, 3.0660149320915697, 2.9320129638182975, 2.9011744182734263, 3.258369969514509, 2.9005805425168503, 2.87627062166021, 3.176847831966976, 3.112208079413644, 2.99391982433874, 3.079731969295868, 3.379392136398348, 3.072314545290456, 3.1668849760843885, 3.114188993194451, 3.0775377750840214, 3.4279125156324532, 3.1111898504729782, 3.0831218509535705, 2.8937510114074465, 3.006100646863204, 3.01267852651931, 2.8950042271365724, 3.427562392405456, 3.3500537717980996, 3.1657385044243362, 2.931362585962883, 2.8483667641966823, 2.8575118302057185, 3.1409003593116287, 3.2478928734947528, 2.9919220682322267, 2.9763404691503164, 3.1255250423259677, 2.864683664802994, 2.8783506348374344, 2.9546579400166157, 3.2610022359793738, 2.8851267082971477, 2.88251274872926, 2.940735924013314, 2.826560847682967, 3.0150082262144204, 2.725033271569936, 3.0651722216446484, 3.424708475159215, 2.8644072331842922, 3.2038938201564764, 3.217154938328479, 3.1169197467776635, 2.9721140553731296, 3.0684008811866597, 2.838639511238961, 3.017346254017736, 3.1566161356555917, 2.956185634292307, 3.463004557016705, 2.94480327184179, 2.950499891835664, 3.1778824318289045, 2.9876951288786673, 3.3856333874476454, 3.0709071468473192, 3.2548050558017123, 3.080825244209596, 3.4071272109147337, 3.25742687859262, 3.1975334039889276, 3.0880259772807004, 3.3154230407394825, 2.9522590854293886, 3.2127069366563643, 3.106010469874101, 3.0271227674647454, 3.383885903505697, 3.112685085274279, 3.1430164198169397], 'val_acc': [0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9048031256312415, 0.9048282731147039, 0.904784790107182, 0.9052518265587943, 0.9052220611345201, 0.9052243317876544, 0.9049221646218073, 0.904887820993151, 0.9048946897188822, 0.9049633769761949, 0.905505949542636, 0.9087339611280532, 0.905872262659527, 0.9087591426713126, 0.909743572984423, 0.9121794615473066, 0.9117170089767093, 0.9104189446994236, 0.908543952873775, 0.9139171044031779, 0.90821886914117, 0.913633215995062, 0.9155540494691758, 0.9131959932191032, 0.9202907454399836, 0.918738529795692, 0.9212385274115062, 0.9158814010165987, 0.9211080641973586, 0.9216415087381998, 0.9217971847170875, 0.922458773567563, 0.922328321706681, 0.921783447265625, 0.922964768750327, 0.9226442263239906, 0.9249107326780047, 0.92287544409434, 0.9247092746552967, 0.9266895793733143, 0.9270421323322114, 0.9259592550141471, 0.9237820562862215, 0.9278342610313779, 0.9279624394008091, 0.9254258473714193, 0.9264262971423921, 0.9288232383273897, 0.9282623699733189, 0.9301625490188599, 0.929132342338562, 0.9309317725045341, 0.9304647190230233, 0.9310828759556725, 0.9248878161112467, 0.9319345496949696, 0.9317605410303388, 0.9317880045799982, 0.93065478404363, 0.9317880216098967, 0.9307944036665416, 0.9331295859246027, 0.9266712296576727, 0.9296611774535406, 0.9297710571970258, 0.9316506442569551, 0.9324175573530651, 0.9328869041942415, 0.9319047644024804, 0.9324061331294832, 0.9310164735430763, 0.931991750285739, 0.9343818511281695, 0.9337706083343142, 0.9331868234134856, 0.93385758854094, 0.9355838128498623, 0.9317330576124645, 0.933099817662012, 0.9346039351962862, 0.9343841728709993, 0.9316643533252534, 0.9348191562153044, 0.9350801053501311, 0.9345993541535877, 0.9337179405348641, 0.9349748378708249, 0.9343269353821164, 0.9355837645984831, 0.9321634684290204, 0.934345231169746, 0.9340155578794933, 0.9370100895563761, 0.9353342226573399, 0.932419879095895, 0.9350229132743109, 0.9348717899549575, 0.9362729021481105, 0.937355773789542, 0.9348694994336083, 0.9361538262594313, 0.9355219517435346, 0.9362591817265465, 0.930954669203077, 0.9362912121273222, 0.9365910944484529, 0.9364675027983529, 0.9363667482421512, 0.9361607148533776, 0.9349999881926037, 0.9357120110875085, 0.9365087066377912, 0.9354303933325268, 0.9347939349356151, 0.9359478042239234, 0.9372344442776271, 0.9361355247951689, 0.9363873742875599, 0.9366803963979086, 0.9352358267420814, 0.9362339576085409, 0.9384294918605259, 0.9353823179290408, 0.936668940952846, 0.9361858793667385, 0.9352815747261047, 0.9377243376913524, 0.9356891087123326, 0.9361996480396816, 0.9368704188437689, 0.9376785641624814, 0.9377907571338472, 0.9365819692611694, 0.9355837731134324, 0.9350091304097857, 0.9375229137284415, 0.9363049524171012, 0.9356868267059326, 0.9379555838448661, 0.936785706451961, 0.9368223377636501, 0.9371657371520996, 0.9367582599322001, 0.9358905468668256, 0.9375503517332531, 0.9375709948085603, 0.9383058519590468, 0.9371657740502131, 0.9368978965850103, 0.9368635472797212, 0.9372046419552394, 0.9371588939712161, 0.9367193295842126, 0.9375251985731579, 0.9361172148159572, 0.9352403822399321, 0.9378159244855245, 0.9364125359626043, 0.9377449410302299, 0.9394528411683583, 0.9385347905613127, 0.9366987205687023, 0.9395123663402739, 0.9374747900735765, 0.9385050620351519, 0.9377014409928095, 0.9370215308098566, 0.9380174052147638, 0.9387751789320082, 0.9386767631485349, 0.9372825253577459, 0.9394230700674511, 0.9370627431642442, 0.9374587876456124, 0.9393223495710463, 0.93870419831503, 0.9388369775953747, 0.9362179636955261, 0.9380929242996943, 0.9376648267110189, 0.9394322264762152, 0.9383493548347837, 0.9381044223195031, 0.9392696817715963, 0.9390224502200172, 0.9388553301493326, 0.9399610644295102, 0.9395627521333241, 0.9398054083188375, 0.9396268441563561, 0.937110824244363, 0.9399999834242321, 0.9385805811200824], 'val_mDice': [0.012268627206573174, 0.009947887383445743, 0.009055225719099067, 0.009556479968263634, 0.010697190350453769, 0.011860910093500501, 0.012706929507354895, 0.015249259803178055, 0.01570625485674966, 0.019675541136945997, 0.018567970199953942, 0.021306754666424933, 0.025387003042158626, 0.02699425058173282, 0.018855440474691846, 0.02276012853586248, 0.04254608596896842, 0.045121821840958934, 0.0463716211240916, 0.058022879906708284, 0.06123361053566138, 0.07406970162299417, 0.08929110318422318, 0.09283111523836851, 0.07129275887494996, 0.10477805754081124, 0.10638339436125188, 0.11061198894111883, 0.09623130450823478, 0.09328099204936907, 0.10736467609448093, 0.13540890404865855, 0.15118758677549304, 0.16700277601679167, 0.16021977897201264, 0.18143151620669024, 0.1896913924387523, 0.19178473505945431, 0.2015150413804111, 0.18496994585508392, 0.19275075658446267, 0.225344669960794, 0.197013774354543, 0.23304181226662227, 0.24044108772206874, 0.23049422238199485, 0.26929085994405405, 0.26622303832499755, 0.2699534876183385, 0.24261370638296717, 0.2809463104321843, 0.28634859302214216, 0.2931217499786899, 0.28718537296212854, 0.29160097188183237, 0.3019115533679724, 0.3042956383987552, 0.308012771819319, 0.30612360650584813, 0.29847345110915957, 0.3123295637113707, 0.31593670873414903, 0.32096291120563236, 0.3087157681584358, 0.3093215666179146, 0.3260571388084264, 0.3344110528982821, 0.3131812873872973, 0.3329103248459952, 0.3247338364876452, 0.34396027826837133, 0.3405032344162464, 0.34719277767553214, 0.3524720345934232, 0.3481967182209094, 0.34334716307265417, 0.3485555317962453, 0.35961036800983404, 0.358134320804051, 0.35392039969918276, 0.3450059959931033, 0.34867516151141553, 0.36525580535332364, 0.360693126384701, 0.3632477137836672, 0.35934399405405637, 0.34506725555374507, 0.3662286317419438, 0.3696885202966985, 0.360286334973006, 0.37234820425510406, 0.3696301117362011, 0.3782916759096441, 0.37755944544360753, 0.3810114753210828, 0.3811393724310966, 0.388188822904513, 0.3704444148710796, 0.38494216721682323, 0.3795704723646243, 0.38875578627699897, 0.39336702706558363, 0.3886785361738432, 0.3889794461429119, 0.3873618415423802, 0.39151810038657414, 0.3913176838485968, 0.3736421302670524, 0.3886184538049357, 0.3892342465974036, 0.3953396612334819, 0.39364421438603175, 0.39436067392428714, 0.3905428227569376, 0.4047612448533376, 0.3889549119131906, 0.4035264741451967, 0.40932931875189144, 0.38926031795286, 0.40628789276594207, 0.40718196474370505, 0.38804288474576815, 0.40312117444617407, 0.3951234601083256, 0.41153460421732496, 0.35032385445776437, 0.3986525329805556, 0.41076804520118804, 0.4163978277217774, 0.4042676701432183, 0.41148720930020016, 0.39457522456844646, 0.40712679869362284, 0.40424992463418413, 0.4023017358212244, 0.40560488615717205, 0.41654866632251514, 0.4165211154946259, 0.4164266660809517, 0.41042752209163846, 0.42409735243944896, 0.4116424895113423, 0.4057276774020422, 0.423516537461962, 0.4182961100623721, 0.41086268708819434, 0.41422836464785395, 0.4159908032133466, 0.4221274689549491, 0.40337419350232395, 0.41158674905697507, 0.4080401353892826, 0.4255270248367673, 0.4254740902355739, 0.4094119164205733, 0.4095127445956071, 0.4158679371078809, 0.4278674592219648, 0.4186055990202086, 0.42167905451995985, 0.4278448724320957, 0.4103459804540589, 0.4073530499424253, 0.4159449314077695, 0.42406377160833, 0.42136373317667414, 0.4285702733766465, 0.42103296447367894, 0.4193976862089975, 0.4201864855630057, 0.42648514377928914, 0.4189650819060348, 0.4271183331452665, 0.4294966885021755, 0.431738182192757, 0.4177772372606255, 0.41900871268340517, 0.42012965661429225, 0.4243316909386998, 0.42188552518685657, 0.4249046789038749, 0.43659888882012593, 0.42795690716732115, 0.40754698784578414, 0.4376087990545091, 0.4181205486612661, 0.4229412433646974, 0.42466585249418304, 0.4283139313615504, 0.4267694613053685, 0.4333238846489361, 0.42790431671199347, 0.4231507194538911, 0.43233174759717213, 0.4032785670743102, 0.42670391287122456, 0.4286863732905615, 0.4228844458148593, 0.43276382166714894, 0.40478024010856944, 0.42846835936818806, 0.41618371559750467, 0.4294876873138405, 0.4182488214047182, 0.4198219045287087, 0.4279634906422524, 0.42493611574172974, 0.41937007417991046, 0.43327076360583305, 0.4260417405693304, 0.4299991051001208, 0.4326049873516673, 0.40949623339942526, 0.42821490338870455, 0.4259264621706236], 'loss': [241.42188635069166, 48.763127756275054, 22.73294844534532, 15.653900976238026, 12.560821361111518, 10.784378786088874, 9.684469809990892, 8.944124409061695, 8.407309341688066, 8.010789982634103, 7.653474634820288, 7.343842694495992, 7.0799253334860275, 6.833582541835499, 6.600048143325432, 6.386182423935981, 6.115896199786964, 5.910518463576762, 5.750049862669501, 5.58803066785381, 5.438853496390085, 5.323538272487007, 5.196578256652285, 5.050497132136409, 4.936211076066837, 4.824608905434448, 4.740642397578842, 4.620788323159895, 4.505080951760908, 4.392191997446811, 4.290628177112574, 4.187165379087567, 4.098627982351944, 4.021506173959559, 3.9519000358151315, 3.874839798802467, 3.809961772181061, 3.7310085722723056, 3.6643839007470107, 3.600418647176295, 3.555863702782138, 3.4896023513863077, 3.430695235212115, 3.3822141912187114, 3.3389499146101307, 3.2964499110726924, 3.2655136590291707, 3.2105106622101323, 3.1828597869306763, 3.1444947513652393, 3.1020889150767696, 3.0704781128901932, 3.029408993186899, 3.000505981450828, 2.9686246196359987, 2.934702272012511, 2.9110440547144067, 2.8844598703860593, 2.8635899941999834, 2.834756257700879, 2.82237760327054, 2.797976175599276, 2.776787262673504, 2.755471059116344, 2.7333407714598117, 2.7180438101670865, 2.6952288898172374, 2.681747424848419, 2.664608954165238, 2.646508151909876, 2.633657907736377, 2.610169805403916, 2.5945685759936543, 2.581894841087515, 2.5661272150899763, 2.5615058862705644, 2.5440517455506053, 2.529511716385579, 2.514612426320229, 2.49976797606735, 2.485380696128829, 2.482939422142076, 2.4731600686996824, 2.4576356884645825, 2.449792288024002, 2.431564128879226, 2.4231154151980485, 2.4139687913172456, 2.4015090904737773, 2.398383656631942, 2.383368521934533, 2.3790841322770717, 2.364741213408559, 2.3573430507702935, 2.3454061819681065, 2.3413309852167297, 2.3340112212675486, 2.3274132992137897, 2.316047907427186, 2.3052021345697846, 2.297048642085149, 2.291225622425149, 2.284234860510134, 2.2760159698598112, 2.26845819800259, 2.2658140585788855, 2.2508863296631976, 2.24356304284624, 2.2422024950899324, 2.2272267805396972, 2.2194179954477327, 2.208026609256223, 2.2019687874689025, 2.199515555735519, 2.1823967085285454, 2.184267398224921, 2.171744088228438, 2.166944416589261, 2.1536812711731876, 2.152140042536838, 2.1493181510768644, 2.136485743159523, 2.1400282404108824, 2.12309891878352, 2.1200527095693555, 2.112363439707575, 2.113036702128304, 2.095744907155027, 2.099631925044732, 2.0835884652294956, 2.079293731368068, 2.074004200260051, 2.068853927734203, 2.067044406675387, 2.0647257430396695, 2.0604670107973684, 2.0518122888241104, 2.042980272139753, 2.0353704928110763, 2.03584297109666, 2.0363657940369433, 2.029317636997318, 2.0228575433589877, 2.0168721488107253, 2.013370845665793, 2.012519761871138, 2.0121167799124304, 2.005291252759511, 2.0006136108230756, 1.992430828544752, 1.992337358648294, 1.9891308107333812, 1.984683700027966, 1.9784319123160938, 1.979526630481222, 1.973183105785722, 1.9693021687878471, 1.9703718010851108, 1.9568813097414908, 1.9606907314617141, 1.9598054854608304, 1.9552894450346352, 1.9538790063175917, 1.9484647774204529, 1.9504881571638624, 1.933812433546716, 1.9349709371622297, 1.9339444509057242, 1.9266139406079568, 1.9355130226413984, 1.9285496510543736, 1.9269425517588377, 1.925490355431884, 1.9230725387628766, 1.9151609432773138, 1.9189989043265747, 1.9189425024058702, 1.9093695997111664, 1.9026624346735483, 1.9004170075992595, 1.8976855634241083, 1.8956370270502736, 1.8934952625080135, 1.892626148854144, 1.8906769621043575, 1.8919060738164382, 1.8846793492589127, 1.8873316098298323, 1.881385854473228, 1.8789024242022743, 1.880790699670511, 1.8744666405626866, 1.8790377925902588, 1.8764396419593175, 1.875959902295079, 1.871105585595604, 1.8713702031222523, 1.8643614818577778, 1.8637122937840995, 1.8619832506937306, 1.863524105315818, 1.858028180640121, 1.8537854751640601, 1.8563036913584026, 1.8594598372317657, 1.863028996846155, 1.8495590410320797, 1.8496988460648514, 1.8433503578089325, 1.8445717585392487, 1.8415269572449393, 1.839826817753367, 1.8383210072610243, 1.8382338959287765, 1.832518311624286], 'acc': [0.7489863720982664, 0.8598802613343305, 0.8672467267924658, 0.8685717636711087, 0.868894751772431, 0.8689686721268016, 0.8690075329073091, 0.8690329732429736, 0.8690458579125008, 0.8690280396055343, 0.8689822749708208, 0.8689104620146803, 0.8687535442666141, 0.8685467379939379, 0.8683027607847184, 0.8682541686582593, 0.8682723598763367, 0.8682448079481689, 0.8682366069373952, 0.8682959961574018, 0.8684460332404218, 0.8684635919046374, 0.8685200661736829, 0.8689237190437428, 0.8691646605299138, 0.8694021288016087, 0.8696214670571423, 0.8703960815858629, 0.8707202131724757, 0.8713760898134855, 0.8720220010221315, 0.872769894536943, 0.8735455001207223, 0.8741286217143638, 0.874751738415349, 0.8757608696080045, 0.8765290641637581, 0.8775026511084213, 0.8780676534186445, 0.878944378161628, 0.8796799014201623, 0.8804301557132728, 0.8818310596751229, 0.8820043132901306, 0.8829963433161071, 0.8836094964233142, 0.8844107298555001, 0.8849256550252196, 0.8854057726566792, 0.8862195488619211, 0.8870856173585002, 0.8875074856240373, 0.8885338117397427, 0.8886385466382755, 0.889011543082529, 0.889737846397678, 0.8900835185191616, 0.8907175489144907, 0.8911405740755014, 0.892037205620724, 0.8922965015913679, 0.892898832071671, 0.8931649807263873, 0.8939912214383974, 0.89482118812443, 0.8951030493770833, 0.8958499955032426, 0.8960669754970412, 0.8966059982489363, 0.8972676955529759, 0.8978375350553359, 0.898523835701958, 0.8990259031719019, 0.8993916702173982, 0.8999221192657407, 0.9003003095532511, 0.9005329786685475, 0.9010572362571225, 0.9011743241522109, 0.9019845746053949, 0.9021421225286528, 0.902595825810779, 0.9026945131303534, 0.903495771808617, 0.9035182044022065, 0.9037593046272562, 0.9041657154790372, 0.904673131325352, 0.9046832380070492, 0.9048839939773623, 0.9053171723018107, 0.9055975943441815, 0.9059191053603595, 0.905952911915199, 0.9066488331215032, 0.90673181080786, 0.9069613532820533, 0.9072175196813484, 0.9074037505386651, 0.9075942220015863, 0.9079706229201994, 0.9079536862609611, 0.908260853218853, 0.9086948847733919, 0.9088815554318594, 0.9091132031209624, 0.9091384359952317, 0.9097266331421334, 0.910089318005823, 0.9105184125100484, 0.9107943925527517, 0.9110400118946409, 0.9115402231401208, 0.9115971799344486, 0.9118171024350089, 0.9119974720769933, 0.9124201062972637, 0.9125603157039412, 0.9129930490447258, 0.9129154009007295, 0.9131621583615376, 0.9132886772466483, 0.9131647564552643, 0.9137922003592327, 0.9137522063435491, 0.9140548528861007, 0.9142448399162072, 0.9146373011782286, 0.9145015130181843, 0.9149711131819738, 0.9150793272474034, 0.9150571531934133, 0.9153658906595772, 0.9155481157101532, 0.9153992394870569, 0.9156065307556146, 0.915861839443267, 0.916170071792529, 0.9163559578001464, 0.9161952593686472, 0.9164984628072658, 0.9164717913202544, 0.9169218349783136, 0.9169295990147617, 0.9168214510579732, 0.9171354079811886, 0.9171455139044693, 0.9172487436104814, 0.9172102076965466, 0.9175273617821115, 0.9176453052744692, 0.9176860183149904, 0.9180860351063692, 0.9182330345243348, 0.9181313371069978, 0.9183485463953946, 0.9184428123534243, 0.9183842292727361, 0.9188302440498062, 0.9188078837356104, 0.9186999028602822, 0.9188914411394771, 0.9190324447001936, 0.9192460638170921, 0.9190283162942713, 0.9195365702942752, 0.91979894313781, 0.9196610240456409, 0.9199957204435013, 0.9198668824010716, 0.919782395477306, 0.9200011631622925, 0.9201858687865061, 0.9201996282435392, 0.9205132379613677, 0.9203867448164544, 0.9204024749958656, 0.9207605301173225, 0.9208494135012901, 0.9210979122175103, 0.9213304179859731, 0.921113040685332, 0.9213318973578583, 0.9213241349416637, 0.9214524841386733, 0.9215163220117288, 0.9216316286391874, 0.9216815877539909, 0.9217765196362097, 0.9217995998370802, 0.9219306842288416, 0.9220820878405596, 0.922167870128272, 0.9221020170850333, 0.9220235554129029, 0.9221773011757501, 0.9221382539511784, 0.9225291888884366, 0.9224216942385439, 0.9226145978743386, 0.9226384412835553, 0.9225310861478312, 0.9228440927521488, 0.9227900770971351, 0.9228262486475292, 0.9226780643301339, 0.9233630270955189, 0.9231462047489387, 0.9232197069285208, 0.9233491241368849, 0.923432195464701, 0.923510655378684, 0.9235619569826797, 0.9235302794294502, 0.9238888643967845], 'mDice': [0.01646519822582203, 0.012850789148206079, 0.011245010136992044, 0.011824288213898825, 0.01365078887477566, 0.01603104648057674, 0.018027923733329232, 0.02024583016556882, 0.022087919352536994, 0.023623745779664685, 0.025871510942239787, 0.028238373688085128, 0.030994576183496953, 0.033760293503045026, 0.03706215747262394, 0.04031894054369405, 0.047168042945975576, 0.05412695483048343, 0.05951734569698647, 0.0648312515991948, 0.07101841432087128, 0.07553237293677591, 0.0807504146983795, 0.0874665210891464, 0.09317178615708789, 0.0987751647479529, 0.10385350739223916, 0.11164758614987474, 0.11948464088341577, 0.1278800770643246, 0.13634080433916765, 0.14504331897880293, 0.15285898566085157, 0.160061683605603, 0.16639403575115266, 0.1743555324159653, 0.18102574588614942, 0.18932576555380223, 0.1964910911392196, 0.20491427033191612, 0.2103282181265636, 0.2180958910842886, 0.22576451585693627, 0.2326690545339724, 0.23852054209396148, 0.24413241715198633, 0.24969912126893043, 0.25639299093435924, 0.26134834975343185, 0.26613166009573114, 0.2733563123375367, 0.277614236257199, 0.2841068500725088, 0.28748330628455937, 0.29237931987465704, 0.2979251663763445, 0.30115718929000457, 0.30590930579369163, 0.3086908670411258, 0.31336544625028745, 0.3158687780318013, 0.3195411626006083, 0.3227017966681003, 0.32678618176791946, 0.3310389392105214, 0.3333863731820574, 0.3378257633910281, 0.33938305715685385, 0.34301937778423075, 0.34533186052630166, 0.34883532085491326, 0.3521510397902982, 0.3556018464133669, 0.35829165473893493, 0.3608220574688953, 0.3622046744568352, 0.36515804034150906, 0.3688588964759855, 0.37067298658647496, 0.37438646763281624, 0.3762953868256108, 0.37755034282208866, 0.37895397564131605, 0.3823410592167415, 0.3836272528848598, 0.387213796773627, 0.38887493965918557, 0.3904553535748981, 0.3930291427406751, 0.39336032912362073, 0.3963763858855104, 0.39776823738762607, 0.39982518572694237, 0.4017107152676844, 0.4044196532081496, 0.4049832780558364, 0.406451429472922, 0.4082473191321873, 0.41021021506518374, 0.4114410745021992, 0.41431903900589034, 0.4150900365944563, 0.41736577212500253, 0.41852490346234544, 0.42036228072948945, 0.42082104052976993, 0.42380364171705287, 0.42610635928319834, 0.4268365403188776, 0.429944593678418, 0.4315735608919613, 0.43478139864357307, 0.435348233879934, 0.4363913093489766, 0.4395168877783276, 0.43951554882841254, 0.44250311423685595, 0.4436127114024768, 0.44685249604860755, 0.4474512717846572, 0.44830590607758225, 0.4515570424093501, 0.45033921992845427, 0.45355362975576696, 0.45560734327620384, 0.457257683023865, 0.45722613864168393, 0.4609182866432774, 0.45968937648613606, 0.4632465858920958, 0.4651750526102793, 0.46537589518946215, 0.4675652378897504, 0.46753989107488825, 0.467924214085104, 0.46919040325889017, 0.47107536590051075, 0.47336932861646386, 0.47435109057889774, 0.4740882146466507, 0.47506851395683664, 0.47693994479853224, 0.4778770869904822, 0.47868540910068497, 0.48042969253680506, 0.48112027982261707, 0.4810926601080989, 0.4819014168254484, 0.4824501698833452, 0.4848113938439253, 0.48617485877473343, 0.48601669479891907, 0.4879492685035035, 0.488615053569234, 0.48852956081852417, 0.48946387957510795, 0.49072670759046055, 0.49098246197422873, 0.4938479554843921, 0.49373501104864514, 0.4931237217730494, 0.49413627091919454, 0.49439834039611624, 0.49595143973930783, 0.49534142816000093, 0.49886432166547245, 0.49935479000489424, 0.4995237976249436, 0.5007366572291446, 0.4994257360334821, 0.5005683300581102, 0.5015307506293214, 0.5015098707039783, 0.5019191717719995, 0.5035845648755176, 0.5028273852110047, 0.5029768555098149, 0.5052798220721517, 0.5069333199294564, 0.507698613358114, 0.5083673989954357, 0.5082176059386807, 0.5090720944605008, 0.5098871981991907, 0.5093441180050292, 0.5099053971910376, 0.5114352261374308, 0.5110278934188631, 0.5121357184121069, 0.5125741621812276, 0.5126363334339525, 0.513649513594144, 0.5132365431747893, 0.5134612353109592, 0.5137063612799114, 0.5145213368864263, 0.5148348209714071, 0.5161087287053773, 0.5166777311112349, 0.5173986140808803, 0.5162330160072963, 0.5178338796825384, 0.5192260315865479, 0.5185347904767378, 0.517936051454943, 0.5176709629516278, 0.5202615778869532, 0.5199501111324936, 0.5217533660849694, 0.5215043521855914, 0.5217948562266488, 0.5224783093409431, 0.5226419323759776, 0.5231793206253331, 0.5239226789701552]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.42s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.14s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.95s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:54,  1.46s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:07,  1.51s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:09,  1.52s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:35,  1.62s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<07:16,  1.56s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:44,  1.66s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<08:13,  1.78s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<08:23,  1.82s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<08:05,  1.76s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:22,  1.83s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:35,  1.88s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:47,  1.93s/it]predicting train subjects:   5%|▍         | 13/285 [00:23<08:48,  1.94s/it]predicting train subjects:   5%|▍         | 14/285 [00:25<08:59,  1.99s/it]predicting train subjects:   5%|▌         | 15/285 [00:27<09:08,  2.03s/it]predicting train subjects:   6%|▌         | 16/285 [00:29<09:06,  2.03s/it]predicting train subjects:   6%|▌         | 17/285 [00:31<08:54,  1.99s/it]predicting train subjects:   6%|▋         | 18/285 [00:33<08:59,  2.02s/it]predicting train subjects:   7%|▋         | 19/285 [00:35<08:48,  1.99s/it]predicting train subjects:   7%|▋         | 20/285 [00:37<08:49,  2.00s/it]predicting train subjects:   7%|▋         | 21/285 [00:39<08:44,  1.99s/it]predicting train subjects:   8%|▊         | 22/285 [00:41<08:50,  2.02s/it]predicting train subjects:   8%|▊         | 23/285 [00:43<08:55,  2.04s/it]predicting train subjects:   8%|▊         | 24/285 [00:45<08:58,  2.06s/it]predicting train subjects:   9%|▉         | 25/285 [00:47<09:03,  2.09s/it]predicting train subjects:   9%|▉         | 26/285 [00:49<08:49,  2.04s/it]predicting train subjects:   9%|▉         | 27/285 [00:52<09:06,  2.12s/it]predicting train subjects:  10%|▉         | 28/285 [00:54<08:47,  2.05s/it]predicting train subjects:  10%|█         | 29/285 [00:56<08:32,  2.00s/it]predicting train subjects:  11%|█         | 30/285 [00:57<08:26,  1.98s/it]predicting train subjects:  11%|█         | 31/285 [00:59<08:17,  1.96s/it]predicting train subjects:  11%|█         | 32/285 [01:01<08:18,  1.97s/it]predicting train subjects:  12%|█▏        | 33/285 [01:03<08:14,  1.96s/it]predicting train subjects:  12%|█▏        | 34/285 [01:05<08:03,  1.93s/it]predicting train subjects:  12%|█▏        | 35/285 [01:07<08:04,  1.94s/it]predicting train subjects:  13%|█▎        | 36/285 [01:09<07:57,  1.92s/it]predicting train subjects:  13%|█▎        | 37/285 [01:11<07:50,  1.90s/it]predicting train subjects:  13%|█▎        | 38/285 [01:13<07:45,  1.89s/it]predicting train subjects:  14%|█▎        | 39/285 [01:15<07:52,  1.92s/it]predicting train subjects:  14%|█▍        | 40/285 [01:17<07:53,  1.93s/it]predicting train subjects:  14%|█▍        | 41/285 [01:19<07:53,  1.94s/it]predicting train subjects:  15%|█▍        | 42/285 [01:20<07:42,  1.90s/it]predicting train subjects:  15%|█▌        | 43/285 [01:22<07:33,  1.88s/it]predicting train subjects:  15%|█▌        | 44/285 [01:24<07:35,  1.89s/it]predicting train subjects:  16%|█▌        | 45/285 [01:26<07:32,  1.88s/it]predicting train subjects:  16%|█▌        | 46/285 [01:28<07:12,  1.81s/it]predicting train subjects:  16%|█▋        | 47/285 [01:29<06:57,  1.76s/it]predicting train subjects:  17%|█▋        | 48/285 [01:31<06:37,  1.68s/it]predicting train subjects:  17%|█▋        | 49/285 [01:32<06:31,  1.66s/it]predicting train subjects:  18%|█▊        | 50/285 [01:34<06:24,  1.64s/it]predicting train subjects:  18%|█▊        | 51/285 [01:36<06:21,  1.63s/it]predicting train subjects:  18%|█▊        | 52/285 [01:37<06:18,  1.63s/it]predicting train subjects:  19%|█▊        | 53/285 [01:39<06:17,  1.63s/it]predicting train subjects:  19%|█▉        | 54/285 [01:40<06:12,  1.61s/it]predicting train subjects:  19%|█▉        | 55/285 [01:42<06:04,  1.59s/it]predicting train subjects:  20%|█▉        | 56/285 [01:44<06:09,  1.61s/it]predicting train subjects:  20%|██        | 57/285 [01:45<06:16,  1.65s/it]predicting train subjects:  20%|██        | 58/285 [01:47<06:20,  1.68s/it]predicting train subjects:  21%|██        | 59/285 [01:49<06:18,  1.67s/it]predicting train subjects:  21%|██        | 60/285 [01:51<06:21,  1.70s/it]predicting train subjects:  21%|██▏       | 61/285 [01:52<06:28,  1.73s/it]predicting train subjects:  22%|██▏       | 62/285 [01:54<06:16,  1.69s/it]predicting train subjects:  22%|██▏       | 63/285 [01:55<06:02,  1.63s/it]predicting train subjects:  22%|██▏       | 64/285 [01:57<06:01,  1.64s/it]predicting train subjects:  23%|██▎       | 65/285 [01:59<06:18,  1.72s/it]predicting train subjects:  23%|██▎       | 66/285 [02:01<06:24,  1.76s/it]predicting train subjects:  24%|██▎       | 67/285 [02:02<06:16,  1.73s/it]predicting train subjects:  24%|██▍       | 68/285 [02:04<06:14,  1.72s/it]predicting train subjects:  24%|██▍       | 69/285 [02:06<06:11,  1.72s/it]predicting train subjects:  25%|██▍       | 70/285 [02:08<06:06,  1.71s/it]predicting train subjects:  25%|██▍       | 71/285 [02:09<06:06,  1.71s/it]predicting train subjects:  25%|██▌       | 72/285 [02:11<05:58,  1.68s/it]predicting train subjects:  26%|██▌       | 73/285 [02:13<06:02,  1.71s/it]predicting train subjects:  26%|██▌       | 74/285 [02:14<06:00,  1.71s/it]predicting train subjects:  26%|██▋       | 75/285 [02:16<05:56,  1.70s/it]predicting train subjects:  27%|██▋       | 76/285 [02:18<05:53,  1.69s/it]predicting train subjects:  27%|██▋       | 77/285 [02:20<05:56,  1.71s/it]predicting train subjects:  27%|██▋       | 78/285 [02:21<05:48,  1.68s/it]predicting train subjects:  28%|██▊       | 79/285 [02:23<05:42,  1.66s/it]predicting train subjects:  28%|██▊       | 80/285 [02:24<05:39,  1.65s/it]predicting train subjects:  28%|██▊       | 81/285 [02:26<05:45,  1.69s/it]predicting train subjects:  29%|██▉       | 82/285 [02:28<05:42,  1.69s/it]predicting train subjects:  29%|██▉       | 83/285 [02:29<05:38,  1.68s/it]predicting train subjects:  29%|██▉       | 84/285 [02:31<05:38,  1.68s/it]predicting train subjects:  30%|██▉       | 85/285 [02:33<05:59,  1.80s/it]predicting train subjects:  30%|███       | 86/285 [02:35<06:08,  1.85s/it]predicting train subjects:  31%|███       | 87/285 [02:37<06:13,  1.89s/it]predicting train subjects:  31%|███       | 88/285 [02:39<06:15,  1.91s/it]predicting train subjects:  31%|███       | 89/285 [02:41<06:19,  1.94s/it]predicting train subjects:  32%|███▏      | 90/285 [02:43<06:21,  1.96s/it]predicting train subjects:  32%|███▏      | 91/285 [02:45<06:18,  1.95s/it]predicting train subjects:  32%|███▏      | 92/285 [02:47<06:16,  1.95s/it]predicting train subjects:  33%|███▎      | 93/285 [02:49<06:29,  2.03s/it]predicting train subjects:  33%|███▎      | 94/285 [02:51<06:19,  1.99s/it]predicting train subjects:  33%|███▎      | 95/285 [02:53<06:12,  1.96s/it]predicting train subjects:  34%|███▎      | 96/285 [02:55<06:08,  1.95s/it]predicting train subjects:  34%|███▍      | 97/285 [02:57<06:11,  1.98s/it]predicting train subjects:  34%|███▍      | 98/285 [02:59<06:10,  1.98s/it]predicting train subjects:  35%|███▍      | 99/285 [03:01<06:14,  2.01s/it]predicting train subjects:  35%|███▌      | 100/285 [03:03<06:08,  1.99s/it]predicting train subjects:  35%|███▌      | 101/285 [03:05<06:05,  1.99s/it]predicting train subjects:  36%|███▌      | 102/285 [03:07<06:04,  1.99s/it]predicting train subjects:  36%|███▌      | 103/285 [03:09<06:04,  2.00s/it]predicting train subjects:  36%|███▋      | 104/285 [03:11<06:01,  2.00s/it]predicting train subjects:  37%|███▋      | 105/285 [03:13<06:02,  2.01s/it]predicting train subjects:  37%|███▋      | 106/285 [03:15<05:58,  2.00s/it]predicting train subjects:  38%|███▊      | 107/285 [03:17<05:49,  1.96s/it]predicting train subjects:  38%|███▊      | 108/285 [03:19<05:37,  1.90s/it]predicting train subjects:  38%|███▊      | 109/285 [03:20<05:26,  1.86s/it]predicting train subjects:  39%|███▊      | 110/285 [03:22<05:20,  1.83s/it]predicting train subjects:  39%|███▉      | 111/285 [03:24<05:19,  1.83s/it]predicting train subjects:  39%|███▉      | 112/285 [03:26<05:16,  1.83s/it]predicting train subjects:  40%|███▉      | 113/285 [03:28<05:13,  1.82s/it]predicting train subjects:  40%|████      | 114/285 [03:29<05:09,  1.81s/it]predicting train subjects:  40%|████      | 115/285 [03:31<05:04,  1.79s/it]predicting train subjects:  41%|████      | 116/285 [03:33<05:01,  1.78s/it]predicting train subjects:  41%|████      | 117/285 [03:35<04:58,  1.78s/it]predicting train subjects:  41%|████▏     | 118/285 [03:36<04:53,  1.76s/it]predicting train subjects:  42%|████▏     | 119/285 [03:38<04:52,  1.76s/it]predicting train subjects:  42%|████▏     | 120/285 [03:40<04:51,  1.76s/it]predicting train subjects:  42%|████▏     | 121/285 [03:42<04:43,  1.73s/it]predicting train subjects:  43%|████▎     | 122/285 [03:43<04:30,  1.66s/it]predicting train subjects:  43%|████▎     | 123/285 [03:45<04:20,  1.61s/it]predicting train subjects:  44%|████▎     | 124/285 [03:46<04:19,  1.61s/it]predicting train subjects:  44%|████▍     | 125/285 [03:48<04:17,  1.61s/it]predicting train subjects:  44%|████▍     | 126/285 [03:49<04:16,  1.62s/it]predicting train subjects:  45%|████▍     | 127/285 [03:51<04:16,  1.62s/it]predicting train subjects:  45%|████▍     | 128/285 [03:53<04:16,  1.63s/it]predicting train subjects:  45%|████▌     | 129/285 [03:54<04:10,  1.61s/it]predicting train subjects:  46%|████▌     | 130/285 [03:56<04:10,  1.61s/it]predicting train subjects:  46%|████▌     | 131/285 [03:58<04:09,  1.62s/it]predicting train subjects:  46%|████▋     | 132/285 [03:59<04:05,  1.61s/it]predicting train subjects:  47%|████▋     | 133/285 [04:01<04:05,  1.61s/it]predicting train subjects:  47%|████▋     | 134/285 [04:02<04:07,  1.64s/it]predicting train subjects:  47%|████▋     | 135/285 [04:04<04:04,  1.63s/it]predicting train subjects:  48%|████▊     | 136/285 [04:06<04:01,  1.62s/it]predicting train subjects:  48%|████▊     | 137/285 [04:07<03:59,  1.62s/it]predicting train subjects:  48%|████▊     | 138/285 [04:09<03:58,  1.62s/it]predicting train subjects:  49%|████▉     | 139/285 [04:11<03:57,  1.62s/it]predicting train subjects:  49%|████▉     | 140/285 [04:12<03:51,  1.60s/it]predicting train subjects:  49%|████▉     | 141/285 [04:14<03:52,  1.62s/it]predicting train subjects:  50%|████▉     | 142/285 [04:15<03:45,  1.58s/it]predicting train subjects:  50%|█████     | 143/285 [04:17<03:39,  1.55s/it]predicting train subjects:  51%|█████     | 144/285 [04:18<03:34,  1.52s/it]predicting train subjects:  51%|█████     | 145/285 [04:20<03:33,  1.53s/it]predicting train subjects:  51%|█████     | 146/285 [04:21<03:28,  1.50s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:23<03:24,  1.48s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:24<03:24,  1.49s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:26<03:23,  1.49s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:27<03:21,  1.49s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:29<03:18,  1.48s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:30<03:15,  1.47s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:31<03:11,  1.45s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:33<03:19,  1.52s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:35<03:25,  1.58s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:36<03:25,  1.59s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:38<03:27,  1.62s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:40<03:23,  1.60s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:41<03:28,  1.66s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:43<03:30,  1.68s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:45<03:24,  1.65s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:46<03:23,  1.66s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:48<03:32,  1.74s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:50<03:25,  1.70s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:52<03:21,  1.68s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:53<03:21,  1.69s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:55<03:21,  1.71s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:57<03:20,  1.71s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:59<03:20,  1.73s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:00<03:14,  1.69s/it]predicting train subjects:  60%|██████    | 171/285 [05:02<03:10,  1.67s/it]predicting train subjects:  60%|██████    | 172/285 [05:04<03:15,  1.73s/it]predicting train subjects:  61%|██████    | 173/285 [05:05<03:12,  1.72s/it]predicting train subjects:  61%|██████    | 174/285 [05:07<03:09,  1.71s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:09<03:06,  1.70s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:10<03:03,  1.68s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:12<03:05,  1.72s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:14<02:58,  1.67s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:15<02:54,  1.65s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:17<02:52,  1.64s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:19<02:48,  1.62s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:20<02:44,  1.60s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:22<02:52,  1.70s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:24<02:55,  1.74s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:26<02:59,  1.80s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:28<03:04,  1.86s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:30<03:01,  1.85s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:32<03:18,  2.05s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:34<03:23,  2.12s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:37<03:28,  2.20s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:39<03:15,  2.08s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:41<03:17,  2.13s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:43<03:05,  2.01s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:44<02:56,  1.94s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:46<02:53,  1.93s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:48<02:56,  1.98s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:51<03:03,  2.09s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:53<03:06,  2.15s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:55<03:05,  2.15s/it]predicting train subjects:  70%|███████   | 200/285 [05:58<03:20,  2.36s/it]predicting train subjects:  71%|███████   | 201/285 [06:00<03:11,  2.28s/it]predicting train subjects:  71%|███████   | 202/285 [06:02<03:00,  2.18s/it]predicting train subjects:  71%|███████   | 203/285 [06:04<02:53,  2.11s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:06<02:46,  2.06s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:08<02:44,  2.06s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:10<02:36,  1.98s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:12<02:31,  1.94s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:14<02:27,  1.92s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:15<02:27,  1.93s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:17<02:25,  1.94s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:19<02:22,  1.93s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:21<02:19,  1.91s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:23<02:16,  1.90s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:25<02:14,  1.90s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:27<02:15,  1.94s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:29<02:09,  1.88s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:31<02:08,  1.89s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:32<02:02,  1.83s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:34<02:02,  1.85s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:36<01:56,  1.79s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:38<01:54,  1.79s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:39<01:51,  1.77s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:41<01:49,  1.77s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:43<01:44,  1.71s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:44<01:41,  1.69s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:46<01:37,  1.66s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:48<01:38,  1.69s/it]predicting train subjects:  80%|████████  | 228/285 [06:49<01:34,  1.65s/it]predicting train subjects:  80%|████████  | 229/285 [06:51<01:34,  1.70s/it]predicting train subjects:  81%|████████  | 230/285 [06:53<01:31,  1.66s/it]predicting train subjects:  81%|████████  | 231/285 [06:55<01:32,  1.71s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:57<01:39,  1.88s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:59<01:40,  1.93s/it]predicting train subjects:  82%|████████▏ | 234/285 [07:01<01:40,  1.96s/it]predicting train subjects:  82%|████████▏ | 235/285 [07:03<01:37,  1.95s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:05<01:39,  2.04s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:07<01:40,  2.09s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:09<01:40,  2.13s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:12<01:36,  2.11s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:14<01:38,  2.18s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:16<01:38,  2.24s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:18<01:33,  2.18s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:20<01:30,  2.16s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:23<01:28,  2.16s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:25<01:25,  2.13s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:27<01:24,  2.16s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:29<01:23,  2.20s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:31<01:20,  2.19s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:33<01:17,  2.14s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:35<01:09,  2.00s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:37<01:02,  1.85s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:38<00:57,  1.75s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:40<00:54,  1.71s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:41<00:52,  1.68s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:43<00:50,  1.68s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:45<00:48,  1.68s/it]predicting train subjects:  90%|█████████ | 257/285 [07:46<00:46,  1.68s/it]predicting train subjects:  91%|█████████ | 258/285 [07:48<00:45,  1.67s/it]predicting train subjects:  91%|█████████ | 259/285 [07:50<00:42,  1.64s/it]predicting train subjects:  91%|█████████ | 260/285 [07:51<00:41,  1.65s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:53<00:39,  1.63s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:54<00:36,  1.60s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:56<00:35,  1.63s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:58<00:33,  1.61s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:59<00:32,  1.62s/it]predicting train subjects:  93%|█████████▎| 266/285 [08:01<00:30,  1.63s/it]predicting train subjects:  94%|█████████▎| 267/285 [08:03<00:29,  1.66s/it]predicting train subjects:  94%|█████████▍| 268/285 [08:05<00:31,  1.86s/it]predicting train subjects:  94%|█████████▍| 269/285 [08:07<00:30,  1.90s/it]predicting train subjects:  95%|█████████▍| 270/285 [08:09<00:29,  1.99s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:11<00:28,  2.04s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:13<00:27,  2.09s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:16<00:25,  2.14s/it]predicting train subjects:  96%|█████████▌| 274/285 [08:18<00:23,  2.11s/it]predicting train subjects:  96%|█████████▋| 275/285 [08:20<00:21,  2.17s/it]predicting train subjects:  97%|█████████▋| 276/285 [08:22<00:19,  2.19s/it]predicting train subjects:  97%|█████████▋| 277/285 [08:24<00:17,  2.19s/it]predicting train subjects:  98%|█████████▊| 278/285 [08:27<00:15,  2.18s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:29<00:13,  2.17s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:31<00:10,  2.20s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:33<00:08,  2.17s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:35<00:06,  2.19s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:38<00:04,  2.22s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:40<00:02,  2.19s/it]predicting train subjects: 100%|██████████| 285/285 [08:42<00:00,  2.20s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:02<09:58,  2.11s/it]Loading train:   1%|          | 2/285 [00:03<09:18,  1.97s/it]Loading train:   1%|          | 3/285 [00:05<08:56,  1.90s/it]Loading train:   1%|▏         | 4/285 [00:07<09:21,  2.00s/it]Loading train:   2%|▏         | 5/285 [00:09<09:37,  2.06s/it]Loading train:   2%|▏         | 6/285 [00:11<09:22,  2.02s/it]Loading train:   2%|▏         | 7/285 [00:14<10:05,  2.18s/it]Loading train:   3%|▎         | 8/285 [00:16<09:42,  2.10s/it]Loading train:   3%|▎         | 9/285 [00:18<09:07,  1.98s/it]Loading train:   4%|▎         | 10/285 [00:19<09:00,  1.97s/it]Loading train:   4%|▍         | 11/285 [00:21<08:40,  1.90s/it]Loading train:   4%|▍         | 12/285 [00:23<08:51,  1.95s/it]Loading train:   5%|▍         | 13/285 [00:25<08:41,  1.92s/it]Loading train:   5%|▍         | 14/285 [00:27<08:08,  1.80s/it]Loading train:   5%|▌         | 15/285 [00:28<08:03,  1.79s/it]Loading train:   6%|▌         | 16/285 [00:30<08:02,  1.79s/it]Loading train:   6%|▌         | 17/285 [00:32<07:57,  1.78s/it]Loading train:   6%|▋         | 18/285 [00:34<08:47,  1.98s/it]Loading train:   7%|▋         | 19/285 [00:37<09:04,  2.05s/it]Loading train:   7%|▋         | 20/285 [00:38<08:46,  1.99s/it]Loading train:   7%|▋         | 21/285 [00:40<08:05,  1.84s/it]Loading train:   8%|▊         | 22/285 [00:41<07:39,  1.75s/it]Loading train:   8%|▊         | 23/285 [00:43<07:37,  1.75s/it]Loading train:   8%|▊         | 24/285 [00:45<07:16,  1.67s/it]Loading train:   9%|▉         | 25/285 [00:47<07:32,  1.74s/it]Loading train:   9%|▉         | 26/285 [00:49<07:50,  1.82s/it]Loading train:   9%|▉         | 27/285 [00:51<08:32,  1.99s/it]Loading train:  10%|▉         | 28/285 [00:53<08:43,  2.04s/it]Loading train:  10%|█         | 29/285 [00:55<08:24,  1.97s/it]Loading train:  11%|█         | 30/285 [00:57<08:29,  2.00s/it]Loading train:  11%|█         | 31/285 [00:59<08:11,  1.94s/it]Loading train:  11%|█         | 32/285 [01:00<07:49,  1.86s/it]Loading train:  12%|█▏        | 33/285 [01:02<07:55,  1.89s/it]Loading train:  12%|█▏        | 34/285 [01:05<08:10,  1.95s/it]Loading train:  12%|█▏        | 35/285 [01:07<08:26,  2.03s/it]Loading train:  13%|█▎        | 36/285 [01:09<08:39,  2.09s/it]Loading train:  13%|█▎        | 37/285 [01:11<08:40,  2.10s/it]Loading train:  13%|█▎        | 38/285 [01:14<09:21,  2.27s/it]Loading train:  14%|█▎        | 39/285 [01:16<09:40,  2.36s/it]Loading train:  14%|█▍        | 40/285 [01:19<09:37,  2.36s/it]Loading train:  14%|█▍        | 41/285 [01:21<09:57,  2.45s/it]Loading train:  15%|█▍        | 42/285 [01:23<09:27,  2.34s/it]Loading train:  15%|█▌        | 43/285 [01:26<09:34,  2.38s/it]Loading train:  15%|█▌        | 44/285 [01:28<09:29,  2.36s/it]Loading train:  16%|█▌        | 45/285 [01:30<09:07,  2.28s/it]Loading train:  16%|█▌        | 46/285 [01:32<08:33,  2.15s/it]Loading train:  16%|█▋        | 47/285 [01:33<07:08,  1.80s/it]Loading train:  17%|█▋        | 48/285 [01:34<06:08,  1.56s/it]Loading train:  17%|█▋        | 49/285 [01:35<05:48,  1.48s/it]Loading train:  18%|█▊        | 50/285 [01:37<05:49,  1.49s/it]Loading train:  18%|█▊        | 51/285 [01:38<05:51,  1.50s/it]Loading train:  18%|█▊        | 52/285 [01:40<06:21,  1.64s/it]Loading train:  19%|█▊        | 53/285 [01:42<06:16,  1.62s/it]Loading train:  19%|█▉        | 54/285 [01:44<06:39,  1.73s/it]Loading train:  19%|█▉        | 55/285 [01:47<07:58,  2.08s/it]Loading train:  20%|█▉        | 56/285 [01:48<06:40,  1.75s/it]Loading train:  20%|██        | 57/285 [01:49<05:37,  1.48s/it]Loading train:  20%|██        | 58/285 [01:50<04:59,  1.32s/it]Loading train:  21%|██        | 59/285 [01:51<05:31,  1.47s/it]Loading train:  21%|██        | 60/285 [01:53<05:17,  1.41s/it]Loading train:  21%|██▏       | 61/285 [01:54<04:59,  1.34s/it]Loading train:  22%|██▏       | 62/285 [01:55<05:06,  1.37s/it]Loading train:  22%|██▏       | 63/285 [01:56<04:36,  1.25s/it]Loading train:  22%|██▏       | 64/285 [01:58<04:53,  1.33s/it]Loading train:  23%|██▎       | 65/285 [01:59<05:08,  1.40s/it]Loading train:  23%|██▎       | 66/285 [02:01<05:05,  1.40s/it]Loading train:  24%|██▎       | 67/285 [02:02<04:49,  1.33s/it]Loading train:  24%|██▍       | 68/285 [02:03<04:25,  1.23s/it]Loading train:  24%|██▍       | 69/285 [02:04<04:10,  1.16s/it]Loading train:  25%|██▍       | 70/285 [02:05<04:02,  1.13s/it]Loading train:  25%|██▍       | 71/285 [02:06<03:54,  1.10s/it]Loading train:  25%|██▌       | 72/285 [02:07<03:49,  1.08s/it]Loading train:  26%|██▌       | 73/285 [02:08<03:44,  1.06s/it]Loading train:  26%|██▌       | 74/285 [02:09<03:39,  1.04s/it]Loading train:  26%|██▋       | 75/285 [02:10<03:35,  1.03s/it]Loading train:  27%|██▋       | 76/285 [02:11<03:31,  1.01s/it]Loading train:  27%|██▋       | 77/285 [02:12<03:26,  1.01it/s]Loading train:  27%|██▋       | 78/285 [02:13<03:28,  1.01s/it]Loading train:  28%|██▊       | 79/285 [02:14<03:25,  1.00it/s]Loading train:  28%|██▊       | 80/285 [02:15<03:24,  1.00it/s]Loading train:  28%|██▊       | 81/285 [02:16<03:27,  1.02s/it]Loading train:  29%|██▉       | 82/285 [02:17<03:28,  1.03s/it]Loading train:  29%|██▉       | 83/285 [02:18<03:27,  1.03s/it]Loading train:  29%|██▉       | 84/285 [02:19<03:24,  1.02s/it]Loading train:  30%|██▉       | 85/285 [02:20<03:32,  1.06s/it]Loading train:  30%|███       | 86/285 [02:21<03:34,  1.08s/it]Loading train:  31%|███       | 87/285 [02:23<03:32,  1.07s/it]Loading train:  31%|███       | 88/285 [02:24<03:30,  1.07s/it]Loading train:  31%|███       | 89/285 [02:25<03:27,  1.06s/it]Loading train:  32%|███▏      | 90/285 [02:26<03:29,  1.07s/it]Loading train:  32%|███▏      | 91/285 [02:27<03:27,  1.07s/it]Loading train:  32%|███▏      | 92/285 [02:28<03:24,  1.06s/it]Loading train:  33%|███▎      | 93/285 [02:29<03:23,  1.06s/it]Loading train:  33%|███▎      | 94/285 [02:30<03:26,  1.08s/it]Loading train:  33%|███▎      | 95/285 [02:31<03:23,  1.07s/it]Loading train:  34%|███▎      | 96/285 [02:32<03:30,  1.11s/it]Loading train:  34%|███▍      | 97/285 [02:34<03:38,  1.16s/it]Loading train:  34%|███▍      | 98/285 [02:35<03:38,  1.17s/it]Loading train:  35%|███▍      | 99/285 [02:36<03:38,  1.18s/it]Loading train:  35%|███▌      | 100/285 [02:37<03:30,  1.14s/it]Loading train:  35%|███▌      | 101/285 [02:38<03:28,  1.13s/it]Loading train:  36%|███▌      | 102/285 [02:39<03:30,  1.15s/it]Loading train:  36%|███▌      | 103/285 [02:40<03:28,  1.14s/it]Loading train:  36%|███▋      | 104/285 [02:41<03:21,  1.11s/it]Loading train:  37%|███▋      | 105/285 [02:42<03:15,  1.08s/it]Loading train:  37%|███▋      | 106/285 [02:44<03:14,  1.09s/it]Loading train:  38%|███▊      | 107/285 [02:45<03:08,  1.06s/it]Loading train:  38%|███▊      | 108/285 [02:46<03:04,  1.04s/it]Loading train:  38%|███▊      | 109/285 [02:47<03:05,  1.05s/it]Loading train:  39%|███▊      | 110/285 [02:48<03:08,  1.08s/it]Loading train:  39%|███▉      | 111/285 [02:49<03:10,  1.09s/it]Loading train:  39%|███▉      | 112/285 [02:50<03:15,  1.13s/it]Loading train:  40%|███▉      | 113/285 [02:51<03:20,  1.16s/it]Loading train:  40%|████      | 114/285 [02:52<03:11,  1.12s/it]Loading train:  40%|████      | 115/285 [02:53<03:06,  1.10s/it]Loading train:  41%|████      | 116/285 [02:54<03:04,  1.09s/it]Loading train:  41%|████      | 117/285 [02:56<03:06,  1.11s/it]Loading train:  41%|████▏     | 118/285 [02:57<03:00,  1.08s/it]Loading train:  42%|████▏     | 119/285 [02:58<02:58,  1.08s/it]Loading train:  42%|████▏     | 120/285 [02:59<02:57,  1.08s/it]Loading train:  42%|████▏     | 121/285 [03:00<03:12,  1.17s/it]Loading train:  43%|████▎     | 122/285 [03:02<03:17,  1.21s/it]Loading train:  43%|████▎     | 123/285 [03:03<03:20,  1.24s/it]Loading train:  44%|████▎     | 124/285 [03:04<03:07,  1.17s/it]Loading train:  44%|████▍     | 125/285 [03:05<02:55,  1.10s/it]Loading train:  44%|████▍     | 126/285 [03:06<03:08,  1.18s/it]Loading train:  45%|████▍     | 127/285 [03:07<03:04,  1.17s/it]Loading train:  45%|████▍     | 128/285 [03:08<03:03,  1.17s/it]Loading train:  45%|████▌     | 129/285 [03:10<03:18,  1.27s/it]Loading train:  46%|████▌     | 130/285 [03:11<03:11,  1.24s/it]Loading train:  46%|████▌     | 131/285 [03:12<03:12,  1.25s/it]Loading train:  46%|████▋     | 132/285 [03:14<03:20,  1.31s/it]Loading train:  47%|████▋     | 133/285 [03:15<03:15,  1.29s/it]Loading train:  47%|████▋     | 134/285 [03:16<03:13,  1.28s/it]Loading train:  47%|████▋     | 135/285 [03:18<03:11,  1.27s/it]Loading train:  48%|████▊     | 136/285 [03:19<03:15,  1.31s/it]Loading train:  48%|████▊     | 137/285 [03:20<03:10,  1.29s/it]Loading train:  48%|████▊     | 138/285 [03:21<03:05,  1.26s/it]Loading train:  49%|████▉     | 139/285 [03:23<03:03,  1.26s/it]Loading train:  49%|████▉     | 140/285 [03:24<03:07,  1.29s/it]Loading train:  49%|████▉     | 141/285 [03:25<02:58,  1.24s/it]Loading train:  50%|████▉     | 142/285 [03:26<02:56,  1.23s/it]Loading train:  50%|█████     | 143/285 [03:28<02:54,  1.23s/it]Loading train:  51%|█████     | 144/285 [03:29<02:46,  1.18s/it]Loading train:  51%|█████     | 145/285 [03:30<02:56,  1.26s/it]Loading train:  51%|█████     | 146/285 [03:32<03:07,  1.35s/it]Loading train:  52%|█████▏    | 147/285 [03:33<03:03,  1.33s/it]Loading train:  52%|█████▏    | 148/285 [03:34<02:55,  1.28s/it]Loading train:  52%|█████▏    | 149/285 [03:35<02:58,  1.31s/it]Loading train:  53%|█████▎    | 150/285 [03:37<02:52,  1.27s/it]Loading train:  53%|█████▎    | 151/285 [03:38<02:42,  1.21s/it]Loading train:  53%|█████▎    | 152/285 [03:39<02:41,  1.22s/it]Loading train:  54%|█████▎    | 153/285 [03:40<02:34,  1.17s/it]Loading train:  54%|█████▍    | 154/285 [03:41<02:30,  1.15s/it]Loading train:  54%|█████▍    | 155/285 [03:42<02:26,  1.13s/it]Loading train:  55%|█████▍    | 156/285 [03:44<02:36,  1.21s/it]Loading train:  55%|█████▌    | 157/285 [03:45<02:35,  1.21s/it]Loading train:  55%|█████▌    | 158/285 [03:46<02:22,  1.12s/it]Loading train:  56%|█████▌    | 159/285 [03:47<02:12,  1.05s/it]Loading train:  56%|█████▌    | 160/285 [03:48<02:07,  1.02s/it]Loading train:  56%|█████▋    | 161/285 [03:48<02:01,  1.02it/s]Loading train:  57%|█████▋    | 162/285 [03:49<01:56,  1.06it/s]Loading train:  57%|█████▋    | 163/285 [03:50<01:52,  1.09it/s]Loading train:  58%|█████▊    | 164/285 [03:51<01:46,  1.13it/s]Loading train:  58%|█████▊    | 165/285 [03:52<01:45,  1.14it/s]Loading train:  58%|█████▊    | 166/285 [03:53<01:45,  1.13it/s]Loading train:  59%|█████▊    | 167/285 [03:54<01:45,  1.12it/s]Loading train:  59%|█████▉    | 168/285 [03:55<01:43,  1.13it/s]Loading train:  59%|█████▉    | 169/285 [03:55<01:42,  1.13it/s]Loading train:  60%|█████▉    | 170/285 [03:56<01:41,  1.13it/s]Loading train:  60%|██████    | 171/285 [03:57<01:38,  1.16it/s]Loading train:  60%|██████    | 172/285 [03:58<01:36,  1.17it/s]Loading train:  61%|██████    | 173/285 [03:59<01:35,  1.18it/s]Loading train:  61%|██████    | 174/285 [04:00<01:33,  1.19it/s]Loading train:  61%|██████▏   | 175/285 [04:00<01:30,  1.22it/s]Loading train:  62%|██████▏   | 176/285 [04:01<01:28,  1.23it/s]Loading train:  62%|██████▏   | 177/285 [04:02<01:31,  1.18it/s]Loading train:  62%|██████▏   | 178/285 [04:03<01:35,  1.12it/s]Loading train:  63%|██████▎   | 179/285 [04:04<01:36,  1.10it/s]Loading train:  63%|██████▎   | 180/285 [04:05<01:33,  1.13it/s]Loading train:  64%|██████▎   | 181/285 [04:06<01:30,  1.15it/s]Loading train:  64%|██████▍   | 182/285 [04:07<01:28,  1.17it/s]Loading train:  64%|██████▍   | 183/285 [04:07<01:28,  1.15it/s]Loading train:  65%|██████▍   | 184/285 [04:08<01:28,  1.14it/s]Loading train:  65%|██████▍   | 185/285 [04:09<01:26,  1.16it/s]Loading train:  65%|██████▌   | 186/285 [04:10<01:28,  1.12it/s]Loading train:  66%|██████▌   | 187/285 [04:11<01:29,  1.10it/s]Loading train:  66%|██████▌   | 188/285 [04:12<01:26,  1.13it/s]Loading train:  66%|██████▋   | 189/285 [04:13<01:22,  1.16it/s]Loading train:  67%|██████▋   | 190/285 [04:14<01:21,  1.17it/s]Loading train:  67%|██████▋   | 191/285 [04:14<01:23,  1.13it/s]Loading train:  67%|██████▋   | 192/285 [04:15<01:23,  1.11it/s]Loading train:  68%|██████▊   | 193/285 [04:16<01:24,  1.09it/s]Loading train:  68%|██████▊   | 194/285 [04:17<01:25,  1.07it/s]Loading train:  68%|██████▊   | 195/285 [04:18<01:22,  1.09it/s]Loading train:  69%|██████▉   | 196/285 [04:19<01:22,  1.08it/s]Loading train:  69%|██████▉   | 197/285 [04:20<01:21,  1.08it/s]Loading train:  69%|██████▉   | 198/285 [04:21<01:27,  1.00s/it]Loading train:  70%|██████▉   | 199/285 [04:22<01:26,  1.01s/it]Loading train:  70%|███████   | 200/285 [04:23<01:23,  1.01it/s]Loading train:  71%|███████   | 201/285 [04:24<01:24,  1.01s/it]Loading train:  71%|███████   | 202/285 [04:25<01:26,  1.04s/it]Loading train:  71%|███████   | 203/285 [04:26<01:22,  1.01s/it]Loading train:  72%|███████▏  | 204/285 [04:27<01:20,  1.00it/s]Loading train:  72%|███████▏  | 205/285 [04:28<01:18,  1.02it/s]Loading train:  72%|███████▏  | 206/285 [04:29<01:18,  1.01it/s]Loading train:  73%|███████▎  | 207/285 [04:31<01:22,  1.06s/it]Loading train:  73%|███████▎  | 208/285 [04:31<01:18,  1.02s/it]Loading train:  73%|███████▎  | 209/285 [04:32<01:13,  1.03it/s]Loading train:  74%|███████▎  | 210/285 [04:33<01:11,  1.05it/s]Loading train:  74%|███████▍  | 211/285 [04:34<01:11,  1.04it/s]Loading train:  74%|███████▍  | 212/285 [04:35<01:09,  1.05it/s]Loading train:  75%|███████▍  | 213/285 [04:36<01:07,  1.07it/s]Loading train:  75%|███████▌  | 214/285 [04:37<01:03,  1.12it/s]Loading train:  75%|███████▌  | 215/285 [04:38<01:01,  1.13it/s]Loading train:  76%|███████▌  | 216/285 [04:38<00:58,  1.17it/s]Loading train:  76%|███████▌  | 217/285 [04:39<01:00,  1.13it/s]Loading train:  76%|███████▋  | 218/285 [04:40<00:59,  1.13it/s]Loading train:  77%|███████▋  | 219/285 [04:41<00:57,  1.14it/s]Loading train:  77%|███████▋  | 220/285 [04:42<00:58,  1.10it/s]Loading train:  78%|███████▊  | 221/285 [04:43<00:57,  1.11it/s]Loading train:  78%|███████▊  | 222/285 [04:44<00:56,  1.12it/s]Loading train:  78%|███████▊  | 223/285 [04:45<00:57,  1.07it/s]Loading train:  79%|███████▊  | 224/285 [04:46<00:57,  1.05it/s]Loading train:  79%|███████▉  | 225/285 [04:47<00:55,  1.07it/s]Loading train:  79%|███████▉  | 226/285 [04:48<00:52,  1.12it/s]Loading train:  80%|███████▉  | 227/285 [04:49<00:54,  1.07it/s]Loading train:  80%|████████  | 228/285 [04:49<00:52,  1.10it/s]Loading train:  80%|████████  | 229/285 [04:50<00:48,  1.15it/s]Loading train:  81%|████████  | 230/285 [04:51<00:46,  1.18it/s]Loading train:  81%|████████  | 231/285 [04:52<00:44,  1.20it/s]Loading train:  81%|████████▏ | 232/285 [04:53<00:49,  1.07it/s]Loading train:  82%|████████▏ | 233/285 [04:54<00:50,  1.04it/s]Loading train:  82%|████████▏ | 234/285 [04:55<00:51,  1.01s/it]Loading train:  82%|████████▏ | 235/285 [04:56<00:52,  1.05s/it]Loading train:  83%|████████▎ | 236/285 [04:57<00:51,  1.06s/it]Loading train:  83%|████████▎ | 237/285 [04:58<00:50,  1.05s/it]Loading train:  84%|████████▎ | 238/285 [04:59<00:49,  1.05s/it]Loading train:  84%|████████▍ | 239/285 [05:01<00:48,  1.05s/it]Loading train:  84%|████████▍ | 240/285 [05:02<00:48,  1.07s/it]Loading train:  85%|████████▍ | 241/285 [05:03<00:50,  1.15s/it]Loading train:  85%|████████▍ | 242/285 [05:04<00:52,  1.21s/it]Loading train:  85%|████████▌ | 243/285 [05:06<00:53,  1.26s/it]Loading train:  86%|████████▌ | 244/285 [05:07<00:53,  1.30s/it]Loading train:  86%|████████▌ | 245/285 [05:09<00:59,  1.49s/it]Loading train:  86%|████████▋ | 246/285 [05:10<00:56,  1.44s/it]Loading train:  87%|████████▋ | 247/285 [05:12<00:53,  1.40s/it]Loading train:  87%|████████▋ | 248/285 [05:13<00:51,  1.39s/it]Loading train:  87%|████████▋ | 249/285 [05:14<00:49,  1.36s/it]Loading train:  88%|████████▊ | 250/285 [05:16<00:45,  1.31s/it]Loading train:  88%|████████▊ | 251/285 [05:17<00:48,  1.42s/it]Loading train:  88%|████████▊ | 252/285 [05:18<00:44,  1.35s/it]Loading train:  89%|████████▉ | 253/285 [05:20<00:42,  1.33s/it]Loading train:  89%|████████▉ | 254/285 [05:21<00:41,  1.33s/it]Loading train:  89%|████████▉ | 255/285 [05:22<00:38,  1.27s/it]Loading train:  90%|████████▉ | 256/285 [05:23<00:36,  1.27s/it]Loading train:  90%|█████████ | 257/285 [05:25<00:34,  1.23s/it]Loading train:  91%|█████████ | 258/285 [05:26<00:32,  1.20s/it]Loading train:  91%|█████████ | 259/285 [05:27<00:31,  1.23s/it]Loading train:  91%|█████████ | 260/285 [05:28<00:30,  1.21s/it]Loading train:  92%|█████████▏| 261/285 [05:29<00:28,  1.19s/it]Loading train:  92%|█████████▏| 262/285 [05:31<00:28,  1.24s/it]Loading train:  92%|█████████▏| 263/285 [05:32<00:27,  1.24s/it]Loading train:  93%|█████████▎| 264/285 [05:33<00:25,  1.22s/it]Loading train:  93%|█████████▎| 265/285 [05:34<00:24,  1.22s/it]Loading train:  93%|█████████▎| 266/285 [05:35<00:23,  1.22s/it]Loading train:  94%|█████████▎| 267/285 [05:37<00:21,  1.19s/it]Loading train:  94%|█████████▍| 268/285 [05:38<00:22,  1.31s/it]Loading train:  94%|█████████▍| 269/285 [05:40<00:21,  1.35s/it]Loading train:  95%|█████████▍| 270/285 [05:41<00:20,  1.35s/it]Loading train:  95%|█████████▌| 271/285 [05:42<00:17,  1.28s/it]Loading train:  95%|█████████▌| 272/285 [05:43<00:15,  1.23s/it]Loading train:  96%|█████████▌| 273/285 [05:44<00:14,  1.19s/it]Loading train:  96%|█████████▌| 274/285 [05:45<00:12,  1.15s/it]Loading train:  96%|█████████▋| 275/285 [05:46<00:11,  1.15s/it]Loading train:  97%|█████████▋| 276/285 [05:48<00:09,  1.11s/it]Loading train:  97%|█████████▋| 277/285 [05:49<00:08,  1.10s/it]Loading train:  98%|█████████▊| 278/285 [05:50<00:07,  1.10s/it]Loading train:  98%|█████████▊| 279/285 [05:51<00:06,  1.08s/it]Loading train:  98%|█████████▊| 280/285 [05:52<00:05,  1.05s/it]Loading train:  99%|█████████▊| 281/285 [05:53<00:04,  1.07s/it]Loading train:  99%|█████████▉| 282/285 [05:54<00:03,  1.05s/it]Loading train:  99%|█████████▉| 283/285 [05:55<00:02,  1.05s/it]Loading train: 100%|█████████▉| 284/285 [05:56<00:01,  1.04s/it]Loading train: 100%|██████████| 285/285 [05:57<00:00,  1.03s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 21/285 [00:00<00:01, 205.22it/s]concatenating: train:  17%|█▋        | 49/285 [00:00<00:01, 222.53it/s]concatenating: train:  26%|██▌       | 74/285 [00:00<00:00, 229.44it/s]concatenating: train:  35%|███▌      | 100/285 [00:00<00:00, 237.43it/s]concatenating: train:  44%|████▎     | 124/285 [00:00<00:00, 236.73it/s]concatenating: train:  53%|█████▎    | 151/285 [00:00<00:00, 243.43it/s]concatenating: train:  63%|██████▎   | 179/285 [00:00<00:00, 252.12it/s]concatenating: train:  73%|███████▎  | 207/285 [00:00<00:00, 257.68it/s]concatenating: train:  81%|████████▏ | 232/285 [00:00<00:00, 243.80it/s]concatenating: train:  90%|████████▉ | 256/285 [00:01<00:00, 242.40it/s]concatenating: train:  98%|█████████▊| 280/285 [00:01<00:00, 226.48it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 240.73it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.48s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.49s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 72.57it/s]2019-07-06 17:56:36.632585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 17:56:36.632715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 17:56:36.632734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 17:56:36.632769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 17:56:36.633241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:08,  4.49it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:07,  5.25it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:07,  4.97it/s]loading the weights for Unet:  20%|██        | 8/40 [00:00<00:05,  6.36it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:05,  5.72it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:04,  6.45it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  4.75it/s]loading the weights for Unet:  40%|████      | 16/40 [00:01<00:03,  6.20it/s]loading the weights for Unet:  45%|████▌     | 18/40 [00:02<00:03,  7.03it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  6.18it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:02<00:02,  7.45it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:02<00:01,  8.02it/s]loading the weights for Unet:  68%|██████▊   | 27/40 [00:03<00:01,  8.37it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:03<00:01,  6.81it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:03<00:01,  7.40it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  7.83it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  6.33it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:04<00:00,  6.98it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:04<00:00,  5.96it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:04<00:00,  8.45it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 52, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 52, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 52, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 52, 10)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 52, 10)   910         dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 52, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 52, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 52, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 52, 10)   910         dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 52, 10)   40          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 52, 10)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 52, 10)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 52, 20)   1820        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 52, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 52, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 52, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 52, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 52, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 26, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 26, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 26, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 26, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 26, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 26, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 26, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 13, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 13, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 13, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 13, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 13, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 13, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 13, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 13, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 26, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 26, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 26, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 26, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 26, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 26, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 26, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 26, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 52, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 52, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 52, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 52, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 52, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 52, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 52, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 52, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 52, 13)   273         dropout_8[0][0]                  
==================================================================================================
Total params: 187,773
Trainable params: 44,233
Non-trainable params: 143,540
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.49841486e-02 3.19966680e-02 7.50970181e-02 9.33357939e-03
 2.71292049e-02 7.07427267e-03 8.46489586e-02 1.12779077e-01
 8.61338510e-02 1.32649165e-02 2.94521391e-01 1.92807035e-01
 2.29878984e-04]
Train on 18361 samples, validate on 179 samples
Epoch 1/300
 - 22s - loss: 230.7206 - acc: 0.5858 - mDice: 0.0131 - val_loss: 28.1589 - val_acc: 0.9136 - val_mDice: 0.0082

Epoch 00001: val_mDice improved from -inf to 0.00816, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 12s - loss: 30.6434 - acc: 0.8757 - mDice: 0.0119 - val_loss: 9.2352 - val_acc: 0.9136 - val_mDice: 0.0086

Epoch 00002: val_mDice improved from 0.00816 to 0.00862, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 13s - loss: 15.3410 - acc: 0.8852 - mDice: 0.0134 - val_loss: 6.3974 - val_acc: 0.9136 - val_mDice: 0.0108

Epoch 00003: val_mDice improved from 0.00862 to 0.01085, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 12s - loss: 10.6887 - acc: 0.8861 - mDice: 0.0226 - val_loss: 5.3974 - val_acc: 0.9136 - val_mDice: 0.0211

Epoch 00004: val_mDice improved from 0.01085 to 0.02106, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 12s - loss: 8.6093 - acc: 0.8860 - mDice: 0.0314 - val_loss: 4.7754 - val_acc: 0.9136 - val_mDice: 0.0304

Epoch 00005: val_mDice improved from 0.02106 to 0.03040, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 13s - loss: 7.3850 - acc: 0.8859 - mDice: 0.0380 - val_loss: 4.3480 - val_acc: 0.9136 - val_mDice: 0.0460

Epoch 00006: val_mDice improved from 0.03040 to 0.04604, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 12s - loss: 6.5018 - acc: 0.8857 - mDice: 0.0472 - val_loss: 4.1691 - val_acc: 0.9136 - val_mDice: 0.0475

Epoch 00007: val_mDice improved from 0.04604 to 0.04751, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 13s - loss: 5.8167 - acc: 0.8862 - mDice: 0.0606 - val_loss: 3.7407 - val_acc: 0.9136 - val_mDice: 0.0732

Epoch 00008: val_mDice improved from 0.04751 to 0.07316, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 12s - loss: 5.2015 - acc: 0.8875 - mDice: 0.0818 - val_loss: 3.3206 - val_acc: 0.9129 - val_mDice: 0.1057

Epoch 00009: val_mDice improved from 0.07316 to 0.10569, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 12s - loss: 4.7287 - acc: 0.8893 - mDice: 0.1011 - val_loss: 3.0550 - val_acc: 0.9149 - val_mDice: 0.1386

Epoch 00010: val_mDice improved from 0.10569 to 0.13855, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 13s - loss: 4.3433 - acc: 0.8908 - mDice: 0.1204 - val_loss: 2.8697 - val_acc: 0.9185 - val_mDice: 0.1623

Epoch 00011: val_mDice improved from 0.13855 to 0.16228, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 12s - loss: 4.0336 - acc: 0.8926 - mDice: 0.1413 - val_loss: 2.6939 - val_acc: 0.9191 - val_mDice: 0.1857

Epoch 00012: val_mDice improved from 0.16228 to 0.18568, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 13s - loss: 3.7620 - acc: 0.8951 - mDice: 0.1638 - val_loss: 2.7328 - val_acc: 0.9180 - val_mDice: 0.1994

Epoch 00013: val_mDice improved from 0.18568 to 0.19939, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 13s - loss: 3.5209 - acc: 0.8980 - mDice: 0.1856 - val_loss: 3.2075 - val_acc: 0.9187 - val_mDice: 0.1863

Epoch 00014: val_mDice did not improve from 0.19939
Epoch 15/300
 - 13s - loss: 3.3091 - acc: 0.9010 - mDice: 0.2099 - val_loss: 2.7082 - val_acc: 0.9259 - val_mDice: 0.2392

Epoch 00015: val_mDice improved from 0.19939 to 0.23919, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 12s - loss: 3.1253 - acc: 0.9037 - mDice: 0.2320 - val_loss: 2.6423 - val_acc: 0.9272 - val_mDice: 0.2590

Epoch 00016: val_mDice improved from 0.23919 to 0.25895, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 13s - loss: 2.9647 - acc: 0.9063 - mDice: 0.2527 - val_loss: 2.4378 - val_acc: 0.9350 - val_mDice: 0.3007

Epoch 00017: val_mDice improved from 0.25895 to 0.30065, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 13s - loss: 2.8369 - acc: 0.9086 - mDice: 0.2713 - val_loss: 2.3721 - val_acc: 0.9359 - val_mDice: 0.3080

Epoch 00018: val_mDice improved from 0.30065 to 0.30800, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 13s - loss: 2.7253 - acc: 0.9106 - mDice: 0.2878 - val_loss: 2.5370 - val_acc: 0.9358 - val_mDice: 0.3069

Epoch 00019: val_mDice did not improve from 0.30800
Epoch 20/300
 - 12s - loss: 2.6299 - acc: 0.9124 - mDice: 0.3018 - val_loss: 2.0465 - val_acc: 0.9394 - val_mDice: 0.3578

Epoch 00020: val_mDice improved from 0.30800 to 0.35782, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 13s - loss: 2.5448 - acc: 0.9137 - mDice: 0.3151 - val_loss: 2.6588 - val_acc: 0.9376 - val_mDice: 0.3127

Epoch 00021: val_mDice did not improve from 0.35782
Epoch 22/300
 - 12s - loss: 2.4745 - acc: 0.9153 - mDice: 0.3268 - val_loss: 2.3446 - val_acc: 0.9400 - val_mDice: 0.3420

Epoch 00022: val_mDice did not improve from 0.35782
Epoch 23/300
 - 13s - loss: 2.4073 - acc: 0.9166 - mDice: 0.3377 - val_loss: 2.2456 - val_acc: 0.9409 - val_mDice: 0.3605

Epoch 00023: val_mDice improved from 0.35782 to 0.36049, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 12s - loss: 2.3505 - acc: 0.9178 - mDice: 0.3476 - val_loss: 2.0767 - val_acc: 0.9412 - val_mDice: 0.3715

Epoch 00024: val_mDice improved from 0.36049 to 0.37146, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 13s - loss: 2.3022 - acc: 0.9188 - mDice: 0.3571 - val_loss: 2.0784 - val_acc: 0.9423 - val_mDice: 0.3896

Epoch 00025: val_mDice improved from 0.37146 to 0.38961, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 12s - loss: 2.2521 - acc: 0.9195 - mDice: 0.3666 - val_loss: 2.1863 - val_acc: 0.9411 - val_mDice: 0.3751

Epoch 00026: val_mDice did not improve from 0.38961
Epoch 27/300
 - 13s - loss: 2.2101 - acc: 0.9203 - mDice: 0.3754 - val_loss: 1.9754 - val_acc: 0.9423 - val_mDice: 0.4074

Epoch 00027: val_mDice improved from 0.38961 to 0.40742, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 13s - loss: 2.1662 - acc: 0.9211 - mDice: 0.3836 - val_loss: 2.0520 - val_acc: 0.9424 - val_mDice: 0.4021

Epoch 00028: val_mDice did not improve from 0.40742
Epoch 29/300
 - 13s - loss: 2.1388 - acc: 0.9216 - mDice: 0.3901 - val_loss: 2.1550 - val_acc: 0.9416 - val_mDice: 0.3915

Epoch 00029: val_mDice did not improve from 0.40742
Epoch 30/300
 - 13s - loss: 2.0935 - acc: 0.9224 - mDice: 0.3991 - val_loss: 2.1002 - val_acc: 0.9423 - val_mDice: 0.4080

Epoch 00030: val_mDice improved from 0.40742 to 0.40796, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 12s - loss: 2.0664 - acc: 0.9230 - mDice: 0.4063 - val_loss: 2.0879 - val_acc: 0.9417 - val_mDice: 0.4052

Epoch 00031: val_mDice did not improve from 0.40796
Epoch 32/300
 - 13s - loss: 2.0264 - acc: 0.9235 - mDice: 0.4148 - val_loss: 2.2651 - val_acc: 0.9406 - val_mDice: 0.3939

Epoch 00032: val_mDice did not improve from 0.40796
Epoch 33/300
 - 12s - loss: 1.9968 - acc: 0.9238 - mDice: 0.4215 - val_loss: 2.0161 - val_acc: 0.9419 - val_mDice: 0.4265

Epoch 00033: val_mDice improved from 0.40796 to 0.42653, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 13s - loss: 1.9697 - acc: 0.9242 - mDice: 0.4278 - val_loss: 2.0526 - val_acc: 0.9409 - val_mDice: 0.4214

Epoch 00034: val_mDice did not improve from 0.42653
Epoch 35/300
 - 12s - loss: 1.9417 - acc: 0.9245 - mDice: 0.4353 - val_loss: 2.0751 - val_acc: 0.9405 - val_mDice: 0.4217

Epoch 00035: val_mDice did not improve from 0.42653
Epoch 36/300
 - 12s - loss: 1.9169 - acc: 0.9245 - mDice: 0.4410 - val_loss: 1.9613 - val_acc: 0.9423 - val_mDice: 0.4422

Epoch 00036: val_mDice improved from 0.42653 to 0.44223, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 14s - loss: 1.8956 - acc: 0.9247 - mDice: 0.4462 - val_loss: 2.0661 - val_acc: 0.9417 - val_mDice: 0.4298

Epoch 00037: val_mDice did not improve from 0.44223
Epoch 38/300
 - 13s - loss: 1.8717 - acc: 0.9250 - mDice: 0.4523 - val_loss: 2.2503 - val_acc: 0.9421 - val_mDice: 0.4139

Epoch 00038: val_mDice did not improve from 0.44223
Epoch 39/300
 - 13s - loss: 1.8519 - acc: 0.9254 - mDice: 0.4570 - val_loss: 2.0556 - val_acc: 0.9419 - val_mDice: 0.4339

Epoch 00039: val_mDice did not improve from 0.44223
Epoch 40/300
 - 13s - loss: 1.8317 - acc: 0.9259 - mDice: 0.4619 - val_loss: 2.0572 - val_acc: 0.9428 - val_mDice: 0.4394

Epoch 00040: val_mDice did not improve from 0.44223
Epoch 41/300
 - 13s - loss: 1.8196 - acc: 0.9262 - mDice: 0.4643 - val_loss: 1.9629 - val_acc: 0.9430 - val_mDice: 0.4514

Epoch 00041: val_mDice improved from 0.44223 to 0.45141, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 13s - loss: 1.7949 - acc: 0.9269 - mDice: 0.4710 - val_loss: 2.1028 - val_acc: 0.9422 - val_mDice: 0.4342

Epoch 00042: val_mDice did not improve from 0.45141
Epoch 43/300
 - 13s - loss: 1.7834 - acc: 0.9270 - mDice: 0.4742 - val_loss: 2.0813 - val_acc: 0.9434 - val_mDice: 0.4411

Epoch 00043: val_mDice did not improve from 0.45141
Epoch 44/300
 - 14s - loss: 1.7708 - acc: 0.9274 - mDice: 0.4769 - val_loss: 2.0829 - val_acc: 0.9438 - val_mDice: 0.4426

Epoch 00044: val_mDice did not improve from 0.45141
Epoch 45/300
 - 13s - loss: 1.7558 - acc: 0.9278 - mDice: 0.4809 - val_loss: 1.9801 - val_acc: 0.9443 - val_mDice: 0.4605

Epoch 00045: val_mDice improved from 0.45141 to 0.46045, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 13s - loss: 1.7382 - acc: 0.9283 - mDice: 0.4853 - val_loss: 2.1713 - val_acc: 0.9441 - val_mDice: 0.4426

Epoch 00046: val_mDice did not improve from 0.46045
Epoch 47/300
 - 13s - loss: 1.7353 - acc: 0.9284 - mDice: 0.4861 - val_loss: 2.1917 - val_acc: 0.9440 - val_mDice: 0.4427

Epoch 00047: val_mDice did not improve from 0.46045
Epoch 48/300
 - 13s - loss: 1.7170 - acc: 0.9291 - mDice: 0.4913 - val_loss: 2.2393 - val_acc: 0.9443 - val_mDice: 0.4388

Epoch 00048: val_mDice did not improve from 0.46045
Epoch 49/300
 - 14s - loss: 1.7076 - acc: 0.9293 - mDice: 0.4934 - val_loss: 2.1516 - val_acc: 0.9442 - val_mDice: 0.4446

Epoch 00049: val_mDice did not improve from 0.46045
Epoch 50/300
 - 14s - loss: 1.6984 - acc: 0.9295 - mDice: 0.4956 - val_loss: 2.1844 - val_acc: 0.9438 - val_mDice: 0.4412

Epoch 00050: val_mDice did not improve from 0.46045
Epoch 51/300
 - 13s - loss: 1.6851 - acc: 0.9298 - mDice: 0.4986 - val_loss: 2.1026 - val_acc: 0.9446 - val_mDice: 0.4572

Epoch 00051: val_mDice did not improve from 0.46045
Epoch 52/300
 - 13s - loss: 1.6764 - acc: 0.9301 - mDice: 0.5002 - val_loss: 2.2759 - val_acc: 0.9440 - val_mDice: 0.4376

Epoch 00052: val_mDice did not improve from 0.46045
Epoch 53/300
 - 14s - loss: 1.6674 - acc: 0.9304 - mDice: 0.5036 - val_loss: 2.1238 - val_acc: 0.9456 - val_mDice: 0.4609

Epoch 00053: val_mDice improved from 0.46045 to 0.46088, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 13s - loss: 1.6550 - acc: 0.9308 - mDice: 0.5063 - val_loss: 2.1601 - val_acc: 0.9454 - val_mDice: 0.4558

Epoch 00054: val_mDice did not improve from 0.46088
Epoch 55/300
 - 13s - loss: 1.6439 - acc: 0.9310 - mDice: 0.5089 - val_loss: 2.0827 - val_acc: 0.9458 - val_mDice: 0.4611

Epoch 00055: val_mDice improved from 0.46088 to 0.46114, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 14s - loss: 1.6387 - acc: 0.9313 - mDice: 0.5110 - val_loss: 2.1354 - val_acc: 0.9454 - val_mDice: 0.4554

Epoch 00056: val_mDice did not improve from 0.46114
Epoch 57/300
 - 13s - loss: 1.6286 - acc: 0.9315 - mDice: 0.5133 - val_loss: 2.1539 - val_acc: 0.9453 - val_mDice: 0.4578

Epoch 00057: val_mDice did not improve from 0.46114
Epoch 58/300
 - 13s - loss: 1.6256 - acc: 0.9317 - mDice: 0.5143 - val_loss: 2.1474 - val_acc: 0.9470 - val_mDice: 0.4649

Epoch 00058: val_mDice improved from 0.46114 to 0.46491, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 14s - loss: 1.6163 - acc: 0.9320 - mDice: 0.5168 - val_loss: 2.0370 - val_acc: 0.9455 - val_mDice: 0.4747
