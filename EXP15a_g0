*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
(0/456) train vimp2_845_05312013_VZ
(1/456) train vimp2_823_05202013_AJ
(2/456) train vimp2_915_07112013_LC
(3/456) train vimp2_901_07052013_AS
(4/456) train vimp2_ctrl_911_07082013_TTO
(5/456) train vimp2_ctrl_925_07152013_LS
(6/456) train vimp2_869_06142013_BL
(7/456) train vimp2_ANON724_03272013
(8/456) train vimp2_819_05172013_DS
(9/456) train vimp2_ctrl_918_07112013_TQ
(10/456) train vimp2_ctrl_902_07052013_SI
(11/456) train vimp2_ANON606_20130110
(12/456) train vimp2_943_07242013_PA
(13/456) train vimp2_824_05212013_JS
(14/456) train vimp2_ANON624_20130117
(15/456) train vimp2_ctrl_920_07122013_SW
(16/456) train vimp2_884_06272013_TS
(17/456) train vimp2_668_02282013_CD
(18/456) train vimp2_964_08092013_TG
(19/456) train vimp2_ctrl_921_07122013_MP
(20/456) train vimp2_972_08152013_DC
(21/456) train vimp2_988_08302013_CB
(22/456) train vimp2_ANON702_03152013
(23/456) train vimp2_ANON714_03222013
(24/456) train vimp2_972_08152013_DC_Aug0_Rot_-1_sd0
(25/456) train vimp2_972_08152013_DC_Aug0_Rot_1_sd2
(26/456) train vimp2_972_08152013_DC_Aug0_Rot_-7_sd1
(27/456) train vimp2_972_08152013_DC_Aug1_Rot_2_sd0
(28/456) train vimp2_972_08152013_DC_Aug1_Rot_5_sd1
(29/456) train vimp2_972_08152013_DC_Aug1_Rot_6_sd2
(30/456) train vimp2_972_08152013_DC_Aug2_Rot_-3_sd1
(31/456) train vimp2_972_08152013_DC_Aug2_Rot_-3_sd2
(32/456) train vimp2_972_08152013_DC_Aug2_Rot_-5_sd0
(33/456) train vimp2_972_08152013_DC_Aug3_Rot_2_sd1
(34/456) train vimp2_972_08152013_DC_Aug3_Rot_-3_sd0
(35/456) train vimp2_972_08152013_DC_Aug3_Rot_-6_sd2
(36/456) train vimp2_972_08152013_DC_Aug4_Rot_-2_sd2
(37/456) train vimp2_972_08152013_DC_Aug4_Rot_3_sd0
(38/456) train vimp2_972_08152013_DC_Aug4_Rot_6_sd1
(39/456) train vimp2_972_08152013_DC_Aug5_Rot_-1_sd2
(40/456) train vimp2_972_08152013_DC_Aug5_Rot_7_sd0
(41/456) train vimp2_972_08152013_DC_Aug5_Rot_7_sd1
(42/456) train vimp2_988_08302013_CB_Aug0_Rot_-3_sd2
(43/456) train vimp2_988_08302013_CB_Aug0_Rot_4_sd0
(44/456) train vimp2_988_08302013_CB_Aug0_Rot_-5_sd1
(45/456) train vimp2_988_08302013_CB_Aug1_Rot_-2_sd2
(46/456) train vimp2_988_08302013_CB_Aug1_Rot_-3_sd0
(47/456) train vimp2_988_08302013_CB_Aug1_Rot_-6_sd1
(48/456) train vimp2_988_08302013_CB_Aug2_Rot_-2_sd0
(49/456) train vimp2_988_08302013_CB_Aug2_Rot_4_sd1
(50/456) train vimp2_988_08302013_CB_Aug2_Rot_-5_sd2
(51/456) train vimp2_988_08302013_CB_Aug3_Rot_2_sd2
(52/456) train vimp2_988_08302013_CB_Aug3_Rot_-3_sd0
(53/456) train vimp2_988_08302013_CB_Aug3_Rot_3_sd1
(54/456) train vimp2_988_08302013_CB_Aug4_Rot_-2_sd0
(55/456) train vimp2_988_08302013_CB_Aug4_Rot_-6_sd1
(56/456) train vimp2_988_08302013_CB_Aug4_Rot_7_sd2
(57/456) train vimp2_988_08302013_CB_Aug5_Rot_-1_sd0
(58/456) train vimp2_988_08302013_CB_Aug5_Rot_-6_sd1
(59/456) train vimp2_988_08302013_CB_Aug5_Rot_7_sd2
(60/456) train vimp2_ANON702_03152013_Aug0_Rot_-2_sd1
(61/456) train vimp2_ANON702_03152013_Aug0_Rot_-3_sd2
(62/456) train vimp2_ANON702_03152013_Aug0_Rot_-4_sd0
(63/456) train vimp2_ANON702_03152013_Aug1_Rot_-4_sd1
(64/456) train vimp2_ANON702_03152013_Aug1_Rot_-5_sd2
(65/456) train vimp2_ANON702_03152013_Aug1_Rot_-7_sd0
(66/456) train vimp2_ANON702_03152013_Aug2_Rot_6_sd0
(67/456) train vimp2_ANON702_03152013_Aug2_Rot_6_sd2
(68/456) train vimp2_ANON702_03152013_Aug2_Rot_-7_sd1
(69/456) train vimp2_ANON702_03152013_Aug3_Rot_-1_sd2
(70/456) train vimp2_ANON702_03152013_Aug3_Rot_-3_sd0
(71/456) train vimp2_ANON702_03152013_Aug3_Rot_-6_sd1
(72/456) train vimp2_ANON702_03152013_Aug4_Rot_-2_sd0
(73/456) train vimp2_ANON702_03152013_Aug4_Rot_-3_sd2
(74/456) train vimp2_ANON702_03152013_Aug4_Rot_-7_sd1
(75/456) train vimp2_ANON702_03152013_Aug5_Rot_3_sd0
(76/456) train vimp2_ANON702_03152013_Aug5_Rot_3_sd2
(77/456) train vimp2_ANON702_03152013_Aug5_Rot_4_sd1
(78/456) train vimp2_ANON714_03222013_Aug0_Rot_-1_sd2
(79/456) train vimp2_ANON714_03222013_Aug0_Rot_-2_sd1
(80/456) train vimp2_ANON714_03222013_Aug0_Rot_4_sd0
(81/456) train vimp2_ANON714_03222013_Aug1_Rot_0_sd0
(82/456) train vimp2_ANON714_03222013_Aug1_Rot_1_sd1
(83/456) train vimp2_ANON714_03222013_Aug1_Rot_-6_sd2
(84/456) train vimp2_ANON714_03222013_Aug2_Rot_-2_sd0
(85/456) train vimp2_ANON714_03222013_Aug2_Rot_4_sd1
(86/456) train vimp2_ANON714_03222013_Aug2_Rot_6_sd2
(87/456) train vimp2_ANON714_03222013_Aug3_Rot_2_sd0
(88/456) train vimp2_ANON714_03222013_Aug3_Rot_-3_sd2
(89/456) train vimp2_ANON714_03222013_Aug3_Rot_-7_sd1
(90/456) train vimp2_ANON714_03222013_Aug4_Rot_1_sd0
(91/456) train vimp2_ANON714_03222013_Aug4_Rot_-2_sd1
(92/456) train vimp2_ANON714_03222013_Aug4_Rot_4_sd2
(93/456) train vimp2_ANON714_03222013_Aug5_Rot_1_sd0
(94/456) train vimp2_ANON714_03222013_Aug5_Rot_6_sd1
(95/456) train vimp2_ANON714_03222013_Aug5_Rot_-7_sd2
(96/456) train vimp2_668_02282013_CD_Aug0_Rot_7_sd0
(97/456) train vimp2_668_02282013_CD_Aug1_Rot_-1_sd0
(98/456) train vimp2_668_02282013_CD_Aug2_Rot_-4_sd0
(99/456) train vimp2_668_02282013_CD_Aug3_Rot_3_sd0
(100/456) train vimp2_668_02282013_CD_Aug4_Rot_-5_sd0
(101/456) train vimp2_668_02282013_CD_Aug5_Rot_-6_sd0
(102/456) train vimp2_819_05172013_DS_Aug0_Rot_1_sd0
(103/456) train vimp2_819_05172013_DS_Aug1_Rot_5_sd0
(104/456) train vimp2_819_05172013_DS_Aug2_Rot_-4_sd0
(105/456) train vimp2_819_05172013_DS_Aug3_Rot_2_sd0
(106/456) train vimp2_819_05172013_DS_Aug4_Rot_5_sd0
(107/456) train vimp2_819_05172013_DS_Aug5_Rot_4_sd0
(108/456) train vimp2_823_05202013_AJ_Aug0_Rot_-7_sd0
(109/456) train vimp2_823_05202013_AJ_Aug1_Rot_7_sd0
(110/456) train vimp2_823_05202013_AJ_Aug2_Rot_3_sd0
(111/456) train vimp2_823_05202013_AJ_Aug3_Rot_-5_sd0
(112/456) train vimp2_823_05202013_AJ_Aug4_Rot_-3_sd0
(113/456) train vimp2_823_05202013_AJ_Aug5_Rot_1_sd0
(114/456) train vimp2_824_05212013_JS_Aug0_Rot_-2_sd0
(115/456) train vimp2_824_05212013_JS_Aug1_Rot_6_sd0
(116/456) train vimp2_824_05212013_JS_Aug2_Rot_1_sd0
(117/456) train vimp2_824_05212013_JS_Aug3_Rot_-4_sd0
(118/456) train vimp2_824_05212013_JS_Aug4_Rot_7_sd0
(119/456) train vimp2_824_05212013_JS_Aug5_Rot_2_sd0
(120/456) train vimp2_845_05312013_VZ_Aug0_Rot_1_sd0
(121/456) train vimp2_845_05312013_VZ_Aug1_Rot_5_sd0
(122/456) train vimp2_845_05312013_VZ_Aug2_Rot_-6_sd0
(123/456) train vimp2_845_05312013_VZ_Aug3_Rot_3_sd0
(124/456) train vimp2_845_05312013_VZ_Aug4_Rot_-2_sd0
(125/456) train vimp2_845_05312013_VZ_Aug5_Rot_5_sd0
(126/456) train vimp2_869_06142013_BL_Aug0_Rot_-2_sd0
(127/456) train vimp2_869_06142013_BL_Aug1_Rot_-4_sd0
(128/456) train vimp2_869_06142013_BL_Aug2_Rot_-1_sd0
(129/456) train vimp2_869_06142013_BL_Aug3_Rot_3_sd0
(130/456) train vimp2_869_06142013_BL_Aug4_Rot_3_sd0
(131/456) train vimp2_869_06142013_BL_Aug5_Rot_1_sd0
(132/456) train vimp2_884_06272013_TS_Aug0_Rot_-7_sd0
(133/456) train vimp2_884_06272013_TS_Aug1_Rot_7_sd0
(134/456) train vimp2_884_06272013_TS_Aug2_Rot_-5_sd0
(135/456) train vimp2_884_06272013_TS_Aug3_Rot_-2_sd0
(136/456) train vimp2_884_06272013_TS_Aug4_Rot_6_sd0
(137/456) train vimp2_884_06272013_TS_Aug5_Rot_-1_sd0
(138/456) train vimp2_901_07052013_AS_Aug0_Rot_-4_sd0
(139/456) train vimp2_901_07052013_AS_Aug1_Rot_-2_sd0
(140/456) train vimp2_901_07052013_AS_Aug2_Rot_1_sd0
(141/456) train vimp2_901_07052013_AS_Aug3_Rot_2_sd0
(142/456) train vimp2_901_07052013_AS_Aug4_Rot_-7_sd0
(143/456) train vimp2_901_07052013_AS_Aug5_Rot_-5_sd0
(144/456) train vimp2_915_07112013_LC_Aug0_Rot_-2_sd0
(145/456) train vimp2_915_07112013_LC_Aug1_Rot_4_sd0
(146/456) train vimp2_915_07112013_LC_Aug2_Rot_-2_sd0
(147/456) train vimp2_915_07112013_LC_Aug3_Rot_-1_sd0
(148/456) train vimp2_915_07112013_LC_Aug4_Rot_1_sd0
(149/456) train vimp2_915_07112013_LC_Aug5_Rot_4_sd0
(150/456) train vimp2_943_07242013_PA_Aug0_Rot_-5_sd0
(151/456) train vimp2_943_07242013_PA_Aug1_Rot_-7_sd0
(152/456) train vimp2_943_07242013_PA_Aug2_Rot_-4_sd0
(153/456) train vimp2_943_07242013_PA_Aug3_Rot_6_sd0
(154/456) train vimp2_943_07242013_PA_Aug4_Rot_6_sd0
(155/456) train vimp2_943_07242013_PA_Aug5_Rot_5_sd0
(156/456) train vimp2_964_08092013_TG_Aug0_Rot_5_sd0
(157/456) train vimp2_964_08092013_TG_Aug1_Rot_-4_sd0
(158/456) train vimp2_964_08092013_TG_Aug2_Rot_7_sd0
(159/456) train vimp2_964_08092013_TG_Aug3_Rot_-1_sd0
(160/456) train vimp2_964_08092013_TG_Aug4_Rot_-1_sd0
(161/456) train vimp2_964_08092013_TG_Aug5_Rot_2_sd0
(162/456) train vimp2_ANON606_20130110_Aug0_Rot_1_sd0
(163/456) train vimp2_ANON606_20130110_Aug1_Rot_2_sd0
(164/456) train vimp2_ANON606_20130110_Aug2_Rot_-5_sd0
(165/456) train vimp2_ANON606_20130110_Aug3_Rot_-5_sd0
(166/456) train vimp2_ANON606_20130110_Aug4_Rot_3_sd0
(167/456) train vimp2_ANON606_20130110_Aug5_Rot_-1_sd0
(168/456) train vimp2_ANON624_20130117_Aug0_Rot_1_sd0
(169/456) train vimp2_ANON624_20130117_Aug1_Rot_-2_sd0
(170/456) train vimp2_ANON624_20130117_Aug2_Rot_5_sd0
(171/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd0
(172/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd0
(173/456) train vimp2_ANON624_20130117_Aug5_Rot_3_sd0
(174/456) train vimp2_ANON724_03272013_Aug0_Rot_-4_sd0
(175/456) train vimp2_ANON724_03272013_Aug1_Rot_-3_sd0
(176/456) train vimp2_ANON724_03272013_Aug2_Rot_-4_sd0
(177/456) train vimp2_ANON724_03272013_Aug3_Rot_5_sd0
(178/456) train vimp2_ANON724_03272013_Aug4_Rot_5_sd0
(179/456) train vimp2_ANON724_03272013_Aug5_Rot_-6_sd0
(180/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_5_sd0
(181/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_7_sd0
(182/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_4_sd0
(183/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_-4_sd0
(184/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_3_sd0
(185/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_-7_sd0
(186/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_6_sd0
(187/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_1_sd0
(188/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_4_sd0
(189/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_-2_sd0
(190/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_6_sd0
(191/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_-2_sd0
(192/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_5_sd0
(193/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_-3_sd0
(194/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_-4_sd0
(195/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_1_sd0
(196/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_3_sd0
(197/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_7_sd0
(198/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_7_sd0
(199/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_-6_sd0
(200/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_-4_sd0
(201/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_1_sd0
(202/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-2_sd0
(203/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_4_sd0
(204/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_-1_sd0
(205/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_-3_sd0
(206/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_6_sd0
(207/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_3_sd0
(208/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_1_sd0
(209/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_-5_sd0
(210/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_-5_sd0
(211/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_-2_sd0
(212/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_5_sd0
(213/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_3_sd0
(214/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_4_sd0
(215/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-7_sd0
(216/456) train vimp2_668_02282013_CD_Aug0_Rot_-5_sd1
(217/456) train vimp2_668_02282013_CD_Aug1_Rot_-6_sd1
(218/456) train vimp2_668_02282013_CD_Aug2_Rot_3_sd1
(219/456) train vimp2_668_02282013_CD_Aug3_Rot_4_sd1
(220/456) train vimp2_668_02282013_CD_Aug4_Rot_-6_sd1
(221/456) train vimp2_668_02282013_CD_Aug5_Rot_-2_sd1
(222/456) train vimp2_819_05172013_DS_Aug0_Rot_-1_sd1
(223/456) train vimp2_819_05172013_DS_Aug1_Rot_3_sd1
(224/456) train vimp2_819_05172013_DS_Aug2_Rot_6_sd1
(225/456) train vimp2_819_05172013_DS_Aug3_Rot_3_sd1
(226/456) train vimp2_819_05172013_DS_Aug4_Rot_-7_sd1
(227/456) train vimp2_819_05172013_DS_Aug5_Rot_-5_sd1
(228/456) train vimp2_823_05202013_AJ_Aug0_Rot_-7_sd1
(229/456) train vimp2_823_05202013_AJ_Aug1_Rot_3_sd1
(230/456) train vimp2_823_05202013_AJ_Aug2_Rot_6_sd1
(231/456) train vimp2_823_05202013_AJ_Aug3_Rot_-7_sd1
(232/456) train vimp2_823_05202013_AJ_Aug4_Rot_-4_sd1
(233/456) train vimp2_823_05202013_AJ_Aug5_Rot_-4_sd1
(234/456) train vimp2_824_05212013_JS_Aug0_Rot_7_sd1
(235/456) train vimp2_824_05212013_JS_Aug1_Rot_3_sd1
(236/456) train vimp2_824_05212013_JS_Aug2_Rot_3_sd1
(237/456) train vimp2_824_05212013_JS_Aug3_Rot_-4_sd1
(238/456) train vimp2_824_05212013_JS_Aug4_Rot_4_sd1
(239/456) train vimp2_824_05212013_JS_Aug5_Rot_6_sd1
(240/456) train vimp2_845_05312013_VZ_Aug0_Rot_5_sd1
(241/456) train vimp2_845_05312013_VZ_Aug1_Rot_6_sd1
(242/456) train vimp2_845_05312013_VZ_Aug2_Rot_1_sd1
(243/456) train vimp2_845_05312013_VZ_Aug3_Rot_-6_sd1
(244/456) train vimp2_845_05312013_VZ_Aug4_Rot_-2_sd1
(245/456) train vimp2_845_05312013_VZ_Aug5_Rot_-6_sd1
(246/456) train vimp2_869_06142013_BL_Aug0_Rot_-4_sd1
(247/456) train vimp2_869_06142013_BL_Aug1_Rot_-3_sd1
(248/456) train vimp2_869_06142013_BL_Aug2_Rot_-4_sd1
(249/456) train vimp2_869_06142013_BL_Aug3_Rot_1_sd1
(250/456) train vimp2_869_06142013_BL_Aug4_Rot_7_sd1
(251/456) train vimp2_869_06142013_BL_Aug5_Rot_5_sd1
(252/456) train vimp2_884_06272013_TS_Aug0_Rot_5_sd1
(253/456) train vimp2_884_06272013_TS_Aug1_Rot_-1_sd1
(254/456) train vimp2_884_06272013_TS_Aug2_Rot_-1_sd1
(255/456) train vimp2_884_06272013_TS_Aug3_Rot_3_sd1
(256/456) train vimp2_884_06272013_TS_Aug4_Rot_4_sd1
(257/456) train vimp2_884_06272013_TS_Aug5_Rot_3_sd1
(258/456) train vimp2_901_07052013_AS_Aug0_Rot_3_sd1
(259/456) train vimp2_901_07052013_AS_Aug1_Rot_5_sd1
(260/456) train vimp2_901_07052013_AS_Aug2_Rot_-5_sd1
(261/456) train vimp2_901_07052013_AS_Aug3_Rot_7_sd1
(262/456) train vimp2_901_07052013_AS_Aug4_Rot_7_sd1
(263/456) train vimp2_901_07052013_AS_Aug5_Rot_6_sd1
(264/456) train vimp2_915_07112013_LC_Aug0_Rot_-6_sd1
(265/456) train vimp2_915_07112013_LC_Aug1_Rot_-5_sd1
(266/456) train vimp2_915_07112013_LC_Aug2_Rot_7_sd1
(267/456) train vimp2_915_07112013_LC_Aug3_Rot_1_sd1
(268/456) train vimp2_915_07112013_LC_Aug4_Rot_6_sd1
(269/456) train vimp2_915_07112013_LC_Aug5_Rot_-7_sd1
(270/456) train vimp2_943_07242013_PA_Aug0_Rot_1_sd1
(271/456) train vimp2_943_07242013_PA_Aug1_Rot_6_sd1
(272/456) train vimp2_943_07242013_PA_Aug2_Rot_3_sd1
(273/456) train vimp2_943_07242013_PA_Aug3_Rot_6_sd1
(274/456) train vimp2_943_07242013_PA_Aug4_Rot_3_sd1
(275/456) train vimp2_943_07242013_PA_Aug5_Rot_3_sd1
(276/456) train vimp2_964_08092013_TG_Aug0_Rot_6_sd1
(277/456) train vimp2_964_08092013_TG_Aug1_Rot_-4_sd1
(278/456) train vimp2_964_08092013_TG_Aug2_Rot_5_sd1
(279/456) train vimp2_964_08092013_TG_Aug3_Rot_0_sd1
(280/456) train vimp2_964_08092013_TG_Aug4_Rot_3_sd1
(281/456) train vimp2_964_08092013_TG_Aug5_Rot_-1_sd1
(282/456) train vimp2_ANON606_20130110_Aug0_Rot_-6_sd1
(283/456) train vimp2_ANON606_20130110_Aug1_Rot_7_sd1
(284/456) train vimp2_ANON606_20130110_Aug2_Rot_1_sd1
(285/456) train vimp2_ANON606_20130110_Aug3_Rot_1_sd1
(286/456) train vimp2_ANON606_20130110_Aug4_Rot_4_sd1
(287/456) train vimp2_ANON606_20130110_Aug5_Rot_4_sd1
(288/456) train vimp2_ANON624_20130117_Aug0_Rot_7_sd1
(289/456) train vimp2_ANON624_20130117_Aug1_Rot_6_sd1
(290/456) train vimp2_ANON624_20130117_Aug2_Rot_-1_sd1
(291/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd1
(292/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd1
(293/456) train vimp2_ANON624_20130117_Aug5_Rot_4_sd1
(294/456) train vimp2_ANON724_03272013_Aug0_Rot_-2_sd1
(295/456) train vimp2_ANON724_03272013_Aug1_Rot_4_sd1
(296/456) train vimp2_ANON724_03272013_Aug2_Rot_-1_sd1
(297/456) train vimp2_ANON724_03272013_Aug3_Rot_-6_sd1
(298/456) train vimp2_ANON724_03272013_Aug4_Rot_-5_sd1
(299/456) train vimp2_ANON724_03272013_Aug5_Rot_-4_sd1
(300/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_-2_sd1
(301/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_7_sd1
(302/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_2_sd1
(303/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_-4_sd1
(304/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_-1_sd1
(305/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_-4_sd1
(306/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_7_sd1
(307/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_-1_sd1
(308/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_-5_sd1
(309/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_-3_sd1
(310/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_6_sd1
(311/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_6_sd1
(312/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_1_sd1
(313/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_-2_sd1
(314/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_3_sd1
(315/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_-1_sd1
(316/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_7_sd1
(317/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_6_sd1
(318/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_-3_sd1
(319/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_7_sd1
(320/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_-4_sd1
(321/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_-1_sd1
(322/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-5_sd1
(323/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_-5_sd1
(324/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_4_sd1
(325/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_3_sd1
(326/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_-1_sd1
(327/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_-2_sd1
(328/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_2_sd1
(329/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_4_sd1
(330/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_-2_sd1
(331/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_4_sd1
(332/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_-2_sd1
(333/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_2_sd1
(334/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_3_sd1
(335/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-4_sd1
(336/456) train vimp2_668_02282013_CD_Aug0_Rot_-3_sd2
(337/456) train vimp2_668_02282013_CD_Aug1_Rot_6_sd2
(338/456) train vimp2_668_02282013_CD_Aug2_Rot_-6_sd2
(339/456) train vimp2_668_02282013_CD_Aug3_Rot_7_sd2
(340/456) train vimp2_668_02282013_CD_Aug4_Rot_1_sd2
(341/456) train vimp2_668_02282013_CD_Aug5_Rot_7_sd2
(342/456) train vimp2_819_05172013_DS_Aug0_Rot_3_sd2
(343/456) train vimp2_819_05172013_DS_Aug1_Rot_-1_sd2
(344/456) train vimp2_819_05172013_DS_Aug2_Rot_-2_sd2
(345/456) train vimp2_819_05172013_DS_Aug3_Rot_-1_sd2
(346/456) train vimp2_819_05172013_DS_Aug4_Rot_7_sd2
(347/456) train vimp2_819_05172013_DS_Aug5_Rot_3_sd2
(348/456) train vimp2_823_05202013_AJ_Aug0_Rot_-6_sd2
(349/456) train vimp2_823_05202013_AJ_Aug1_Rot_-7_sd2
(350/456) train vimp2_823_05202013_AJ_Aug2_Rot_7_sd2
(351/456) train vimp2_823_05202013_AJ_Aug3_Rot_-2_sd2
(352/456) train vimp2_823_05202013_AJ_Aug4_Rot_1_sd2
(353/456) train vimp2_823_05202013_AJ_Aug5_Rot_1_sd2
(354/456) train vimp2_824_05212013_JS_Aug0_Rot_-5_sd2
(355/456) train vimp2_824_05212013_JS_Aug1_Rot_3_sd2
(356/456) train vimp2_824_05212013_JS_Aug2_Rot_5_sd2
(357/456) train vimp2_824_05212013_JS_Aug3_Rot_2_sd2
(358/456) train vimp2_824_05212013_JS_Aug4_Rot_5_sd2
(359/456) train vimp2_824_05212013_JS_Aug5_Rot_-7_sd2
(360/456) train vimp2_845_05312013_VZ_Aug0_Rot_-5_sd2
(361/456) train vimp2_845_05312013_VZ_Aug1_Rot_3_sd2
(362/456) train vimp2_845_05312013_VZ_Aug2_Rot_-3_sd2
(363/456) train vimp2_845_05312013_VZ_Aug3_Rot_-1_sd2
(364/456) train vimp2_845_05312013_VZ_Aug4_Rot_4_sd2
(365/456) train vimp2_845_05312013_VZ_Aug5_Rot_-2_sd2
(366/456) train vimp2_869_06142013_BL_Aug0_Rot_3_sd2
(367/456) train vimp2_869_06142013_BL_Aug1_Rot_7_sd2
(368/456) train vimp2_869_06142013_BL_Aug2_Rot_4_sd2
(369/456) train vimp2_869_06142013_BL_Aug3_Rot_1_sd2
(370/456) train vimp2_869_06142013_BL_Aug4_Rot_1_sd2
(371/456) train vimp2_869_06142013_BL_Aug5_Rot_1_sd2
(372/456) train vimp2_884_06272013_TS_Aug0_Rot_3_sd2
(373/456) train vimp2_884_06272013_TS_Aug1_Rot_-5_sd2
(374/456) train vimp2_884_06272013_TS_Aug2_Rot_-7_sd2
(375/456) train vimp2_884_06272013_TS_Aug3_Rot_-6_sd2
(376/456) train vimp2_884_06272013_TS_Aug4_Rot_-2_sd2
(377/456) train vimp2_884_06272013_TS_Aug5_Rot_-6_sd2
(378/456) train vimp2_901_07052013_AS_Aug0_Rot_2_sd2
(379/456) train vimp2_901_07052013_AS_Aug1_Rot_-3_sd2
(380/456) train vimp2_901_07052013_AS_Aug2_Rot_-6_sd2
(381/456) train vimp2_901_07052013_AS_Aug3_Rot_6_sd2
(382/456) train vimp2_901_07052013_AS_Aug4_Rot_-5_sd2
(383/456) train vimp2_901_07052013_AS_Aug5_Rot_1_sd2
(384/456) train vimp2_915_07112013_LC_Aug0_Rot_1_sd2
(385/456) train vimp2_915_07112013_LC_Aug1_Rot_7_sd2
(386/456) train vimp2_915_07112013_LC_Aug2_Rot_3_sd2
(387/456) train vimp2_915_07112013_LC_Aug3_Rot_6_sd2
(388/456) train vimp2_915_07112013_LC_Aug4_Rot_4_sd2
(389/456) train vimp2_915_07112013_LC_Aug5_Rot_4_sd2
(390/456) train vimp2_943_07242013_PA_Aug0_Rot_3_sd2
(391/456) train vimp2_943_07242013_PA_Aug1_Rot_-4_sd2
(392/456) train vimp2_943_07242013_PA_Aug2_Rot_3_sd2
(393/456) train vimp2_943_07242013_PA_Aug3_Rot_2_sd2
(394/456) train vimp2_943_07242013_PA_Aug4_Rot_-1_sd2
(395/456) train vimp2_943_07242013_PA_Aug5_Rot_-3_sd2
(396/456) train vimp2_964_08092013_TG_Aug0_Rot_2_sd2
(397/456) train vimp2_964_08092013_TG_Aug1_Rot_-6_sd2
(398/456) train vimp2_964_08092013_TG_Aug2_Rot_5_sd2
(399/456) train vimp2_964_08092013_TG_Aug3_Rot_-7_sd2
(400/456) train vimp2_964_08092013_TG_Aug4_Rot_-6_sd2
(401/456) train vimp2_964_08092013_TG_Aug5_Rot_5_sd2
(402/456) train vimp2_ANON606_20130110_Aug0_Rot_-6_sd2
(403/456) train vimp2_ANON606_20130110_Aug1_Rot_-5_sd2
(404/456) train vimp2_ANON606_20130110_Aug2_Rot_-3_sd2
(405/456) train vimp2_ANON606_20130110_Aug3_Rot_3_sd2
(406/456) train vimp2_ANON606_20130110_Aug4_Rot_-2_sd2
(407/456) train vimp2_ANON606_20130110_Aug5_Rot_-3_sd2
(408/456) train vimp2_ANON624_20130117_Aug0_Rot_2_sd2
(409/456) train vimp2_ANON624_20130117_Aug1_Rot_-2_sd2
(410/456) train vimp2_ANON624_20130117_Aug2_Rot_-6_sd2
(411/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd2
(412/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd2
(413/456) train vimp2_ANON624_20130117_Aug5_Rot_-7_sd2
(414/456) train vimp2_ANON724_03272013_Aug0_Rot_-5_sd2
(415/456) train vimp2_ANON724_03272013_Aug1_Rot_6_sd2
(416/456) train vimp2_ANON724_03272013_Aug2_Rot_-3_sd2
(417/456) train vimp2_ANON724_03272013_Aug3_Rot_-6_sd2
(418/456) train vimp2_ANON724_03272013_Aug4_Rot_-1_sd2
(419/456) train vimp2_ANON724_03272013_Aug5_Rot_-7_sd2
(420/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_-4_sd2
(421/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_-3_sd2
(422/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_-5_sd2
(423/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_5_sd2
(424/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_-2_sd2
(425/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_5_sd2
(426/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_-3_sd2
(427/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_7_sd2
(428/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_3_sd2
(429/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_5_sd2
(430/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_7_sd2
(431/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_2_sd2
(432/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_-7_sd2
(433/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_5_sd2
(434/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_5_sd2
(435/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_4_sd2
(436/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_5_sd2
(437/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_-4_sd2
(438/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_7_sd2
(439/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_-2_sd2
(440/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_1_sd2
(441/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_-7_sd2
(442/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-3_sd2
(443/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_7_sd2
(444/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_1_sd2
(445/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_-7_sd2
(446/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_6_sd2
(447/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_-2_sd2
(448/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_-5_sd2
(449/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_6_sd2
(450/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_2_sd2
(451/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_5_sd2
(452/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_2_sd2
(453/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_5_sd2
(454/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_-4_sd2
(455/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-3_sd22019-07-06 16:57:53.996821: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-06 16:57:55.516876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-07-06 16:57:55.516949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 16:57:55.883431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 16:57:55.883495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 16:57:55.883508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 16:57:55.883948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a’: File exists
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<05:58,  1.26s/it]Loading train:   1%|          | 2/285 [00:02<05:49,  1.23s/it]Loading train:   1%|          | 3/285 [00:03<05:35,  1.19s/it]Loading train:   1%|▏         | 4/285 [00:04<05:57,  1.27s/it]Loading train:   2%|▏         | 5/285 [00:05<05:25,  1.16s/it]Loading train:   2%|▏         | 6/285 [00:07<05:56,  1.28s/it]Loading train:   2%|▏         | 7/285 [00:09<06:23,  1.38s/it]Loading train:   3%|▎         | 8/285 [00:10<06:26,  1.40s/it]Loading train:   3%|▎         | 9/285 [00:11<06:09,  1.34s/it]Loading train:   4%|▎         | 10/285 [00:12<05:45,  1.26s/it]Loading train:   4%|▍         | 11/285 [00:13<05:42,  1.25s/it]Loading train:   4%|▍         | 12/285 [00:14<05:17,  1.16s/it]Loading train:   5%|▍         | 13/285 [00:15<04:51,  1.07s/it]Loading train:   5%|▍         | 14/285 [00:16<04:43,  1.04s/it]Loading train:   5%|▌         | 15/285 [00:17<04:44,  1.05s/it]Loading train:   6%|▌         | 16/285 [00:18<04:43,  1.06s/it]Loading train:   6%|▌         | 17/285 [00:20<04:56,  1.11s/it]Loading train:   6%|▋         | 18/285 [00:21<04:43,  1.06s/it]Loading train:   7%|▋         | 19/285 [00:22<04:39,  1.05s/it]Loading train:   7%|▋         | 20/285 [00:23<04:29,  1.02s/it]Loading train:   7%|▋         | 21/285 [00:24<04:36,  1.05s/it]Loading train:   8%|▊         | 22/285 [00:25<04:27,  1.02s/it]Loading train:   8%|▊         | 23/285 [00:26<04:16,  1.02it/s]Loading train:   8%|▊         | 24/285 [00:26<04:13,  1.03it/s]Loading train:   9%|▉         | 25/285 [00:27<04:10,  1.04it/s]Loading train:   9%|▉         | 26/285 [00:28<04:05,  1.05it/s]Loading train:   9%|▉         | 27/285 [00:29<04:07,  1.04it/s]Loading train:  10%|▉         | 28/285 [00:31<04:34,  1.07s/it]Loading train:  10%|█         | 29/285 [00:32<04:30,  1.06s/it]Loading train:  11%|█         | 30/285 [00:33<04:14,  1.00it/s]Loading train:  11%|█         | 31/285 [00:34<04:11,  1.01it/s]Loading train:  11%|█         | 32/285 [00:34<04:02,  1.04it/s]Loading train:  12%|█▏        | 33/285 [00:35<03:59,  1.05it/s]Loading train:  12%|█▏        | 34/285 [00:36<03:48,  1.10it/s]Loading train:  12%|█▏        | 35/285 [00:37<03:49,  1.09it/s]Loading train:  13%|█▎        | 36/285 [00:38<03:46,  1.10it/s]Loading train:  13%|█▎        | 37/285 [00:39<03:52,  1.07it/s]Loading train:  13%|█▎        | 38/285 [00:40<04:01,  1.02it/s]Loading train:  14%|█▎        | 39/285 [00:41<03:56,  1.04it/s]Loading train:  14%|█▍        | 40/285 [00:42<03:53,  1.05it/s]Loading train:  14%|█▍        | 41/285 [00:43<03:50,  1.06it/s]Loading train:  15%|█▍        | 42/285 [00:44<04:02,  1.00it/s]Loading train:  15%|█▌        | 43/285 [00:45<04:07,  1.02s/it]Loading train:  15%|█▌        | 44/285 [00:46<04:08,  1.03s/it]Loading train:  16%|█▌        | 45/285 [00:47<04:09,  1.04s/it]Loading train:  16%|█▌        | 46/285 [00:48<04:01,  1.01s/it]Loading train:  16%|█▋        | 47/285 [00:49<03:58,  1.00s/it]Loading train:  17%|█▋        | 48/285 [00:50<04:02,  1.02s/it]Loading train:  17%|█▋        | 49/285 [00:51<03:41,  1.06it/s]Loading train:  18%|█▊        | 50/285 [00:52<03:56,  1.01s/it]Loading train:  18%|█▊        | 51/285 [00:53<03:43,  1.05it/s]Loading train:  18%|█▊        | 52/285 [00:54<03:34,  1.09it/s]Loading train:  19%|█▊        | 53/285 [00:55<03:42,  1.04it/s]Loading train:  19%|█▉        | 54/285 [00:56<03:31,  1.09it/s]Loading train:  19%|█▉        | 55/285 [00:57<03:33,  1.08it/s]Loading train:  20%|█▉        | 56/285 [00:57<03:22,  1.13it/s]Loading train:  20%|██        | 57/285 [00:58<03:18,  1.15it/s]Loading train:  20%|██        | 58/285 [00:59<03:13,  1.18it/s]Loading train:  21%|██        | 59/285 [01:00<03:21,  1.12it/s]Loading train:  21%|██        | 60/285 [01:01<03:37,  1.04it/s]Loading train:  21%|██▏       | 61/285 [01:02<03:26,  1.09it/s]Loading train:  22%|██▏       | 62/285 [01:03<03:19,  1.12it/s]Loading train:  22%|██▏       | 63/285 [01:03<03:07,  1.18it/s]Loading train:  22%|██▏       | 64/285 [01:05<03:32,  1.04it/s]Loading train:  23%|██▎       | 65/285 [01:06<04:02,  1.10s/it]Loading train:  23%|██▎       | 66/285 [01:07<04:10,  1.14s/it]Loading train:  24%|██▎       | 67/285 [01:08<03:49,  1.05s/it]Loading train:  24%|██▍       | 68/285 [01:09<03:32,  1.02it/s]Loading train:  24%|██▍       | 69/285 [01:10<03:20,  1.08it/s]Loading train:  25%|██▍       | 70/285 [01:11<03:33,  1.01it/s]Loading train:  25%|██▍       | 71/285 [01:12<03:31,  1.01it/s]Loading train:  25%|██▌       | 72/285 [01:13<03:27,  1.02it/s]Loading train:  26%|██▌       | 73/285 [01:14<03:26,  1.03it/s]Loading train:  26%|██▌       | 74/285 [01:15<03:13,  1.09it/s]Loading train:  26%|██▋       | 75/285 [01:15<03:06,  1.12it/s]Loading train:  27%|██▋       | 76/285 [01:16<02:59,  1.16it/s]Loading train:  27%|██▋       | 77/285 [01:17<03:11,  1.09it/s]Loading train:  27%|██▋       | 78/285 [01:18<03:07,  1.10it/s]Loading train:  28%|██▊       | 79/285 [01:19<03:13,  1.06it/s]Loading train:  28%|██▊       | 80/285 [01:20<03:06,  1.10it/s]Loading train:  28%|██▊       | 81/285 [01:21<03:01,  1.12it/s]Loading train:  29%|██▉       | 82/285 [01:22<02:54,  1.16it/s]Loading train:  29%|██▉       | 83/285 [01:23<03:09,  1.07it/s]Loading train:  29%|██▉       | 84/285 [01:24<03:05,  1.09it/s]Loading train:  30%|██▉       | 85/285 [01:25<03:14,  1.03it/s]Loading train:  30%|███       | 86/285 [01:26<03:11,  1.04it/s]Loading train:  31%|███       | 87/285 [01:27<03:18,  1.00s/it]Loading train:  31%|███       | 88/285 [01:28<03:15,  1.01it/s]Loading train:  31%|███       | 89/285 [01:29<03:14,  1.01it/s]Loading train:  32%|███▏      | 90/285 [01:30<03:26,  1.06s/it]Loading train:  32%|███▏      | 91/285 [01:31<03:19,  1.03s/it]Loading train:  32%|███▏      | 92/285 [01:32<03:16,  1.02s/it]Loading train:  33%|███▎      | 93/285 [01:33<03:11,  1.00it/s]Loading train:  33%|███▎      | 94/285 [01:34<03:10,  1.00it/s]Loading train:  33%|███▎      | 95/285 [01:35<03:05,  1.03it/s]Loading train:  34%|███▎      | 96/285 [01:36<03:06,  1.02it/s]Loading train:  34%|███▍      | 97/285 [01:37<03:03,  1.02it/s]Loading train:  34%|███▍      | 98/285 [01:38<02:58,  1.05it/s]Loading train:  35%|███▍      | 99/285 [01:39<03:09,  1.02s/it]Loading train:  35%|███▌      | 100/285 [01:40<03:02,  1.01it/s]Loading train:  35%|███▌      | 101/285 [01:41<03:02,  1.01it/s]Loading train:  36%|███▌      | 102/285 [01:42<03:00,  1.01it/s]Loading train:  36%|███▌      | 103/285 [01:43<03:01,  1.00it/s]Loading train:  36%|███▋      | 104/285 [01:44<02:57,  1.02it/s]Loading train:  37%|███▋      | 105/285 [01:45<02:52,  1.05it/s]Loading train:  37%|███▋      | 106/285 [01:45<02:46,  1.08it/s]Loading train:  38%|███▊      | 107/285 [01:46<02:40,  1.11it/s]Loading train:  38%|███▊      | 108/285 [01:47<02:35,  1.14it/s]Loading train:  38%|███▊      | 109/285 [01:48<02:36,  1.13it/s]Loading train:  39%|███▊      | 110/285 [01:49<02:34,  1.13it/s]Loading train:  39%|███▉      | 111/285 [01:50<02:33,  1.14it/s]Loading train:  39%|███▉      | 112/285 [01:51<02:30,  1.15it/s]Loading train:  40%|███▉      | 113/285 [01:51<02:29,  1.15it/s]Loading train:  40%|████      | 114/285 [01:52<02:27,  1.16it/s]Loading train:  40%|████      | 115/285 [01:53<02:24,  1.18it/s]Loading train:  41%|████      | 116/285 [01:54<02:20,  1.21it/s]Loading train:  41%|████      | 117/285 [01:55<02:16,  1.23it/s]Loading train:  41%|████▏     | 118/285 [01:55<02:13,  1.25it/s]Loading train:  42%|████▏     | 119/285 [01:56<02:17,  1.20it/s]Loading train:  42%|████▏     | 120/285 [01:58<02:38,  1.04it/s]Loading train:  42%|████▏     | 121/285 [01:59<03:20,  1.22s/it]Loading train:  43%|████▎     | 122/285 [02:01<03:37,  1.33s/it]Loading train:  43%|████▎     | 123/285 [02:03<03:51,  1.43s/it]Loading train:  44%|████▎     | 124/285 [02:04<03:56,  1.47s/it]Loading train:  44%|████▍     | 125/285 [02:06<03:51,  1.45s/it]Loading train:  44%|████▍     | 126/285 [02:07<03:46,  1.42s/it]Loading train:  45%|████▍     | 127/285 [02:09<03:46,  1.43s/it]Loading train:  45%|████▍     | 128/285 [02:10<03:35,  1.37s/it]Loading train:  45%|████▌     | 129/285 [02:11<03:28,  1.34s/it]Loading train:  46%|████▌     | 130/285 [02:12<03:22,  1.31s/it]Loading train:  46%|████▌     | 131/285 [02:13<03:12,  1.25s/it]Loading train:  46%|████▋     | 132/285 [02:15<03:41,  1.45s/it]Loading train:  47%|████▋     | 133/285 [02:17<04:01,  1.59s/it]Loading train:  47%|████▋     | 134/285 [02:18<03:37,  1.44s/it]Loading train:  47%|████▋     | 135/285 [02:20<03:28,  1.39s/it]Loading train:  48%|████▊     | 136/285 [02:21<03:44,  1.51s/it]Loading train:  48%|████▊     | 137/285 [02:23<03:33,  1.44s/it]Loading train:  48%|████▊     | 138/285 [02:25<03:54,  1.59s/it]Loading train:  49%|████▉     | 139/285 [02:26<04:01,  1.66s/it]Loading train:  49%|████▉     | 140/285 [02:28<04:00,  1.66s/it]Loading train:  49%|████▉     | 141/285 [02:29<03:47,  1.58s/it]Loading train:  50%|████▉     | 142/285 [02:31<03:58,  1.67s/it]Loading train:  50%|█████     | 143/285 [02:33<04:04,  1.72s/it]Loading train:  51%|█████     | 144/285 [02:34<03:26,  1.47s/it]Loading train:  51%|█████     | 145/285 [02:35<03:21,  1.44s/it]Loading train:  51%|█████     | 146/285 [02:37<03:11,  1.38s/it]Loading train:  52%|█████▏    | 147/285 [02:38<02:59,  1.30s/it]Loading train:  52%|█████▏    | 148/285 [02:39<02:56,  1.29s/it]Loading train:  52%|█████▏    | 149/285 [02:40<02:51,  1.26s/it]Loading train:  53%|█████▎    | 150/285 [02:42<03:08,  1.40s/it]Loading train:  53%|█████▎    | 151/285 [02:43<02:55,  1.31s/it]Loading train:  53%|█████▎    | 152/285 [02:44<02:49,  1.28s/it]Loading train:  54%|█████▎    | 153/285 [02:45<02:44,  1.25s/it]Loading train:  54%|█████▍    | 154/285 [02:47<02:44,  1.26s/it]Loading train:  54%|█████▍    | 155/285 [02:48<02:37,  1.21s/it]Loading train:  55%|█████▍    | 156/285 [02:49<02:41,  1.25s/it]Loading train:  55%|█████▌    | 157/285 [02:50<02:36,  1.22s/it]Loading train:  55%|█████▌    | 158/285 [02:52<02:43,  1.29s/it]Loading train:  56%|█████▌    | 159/285 [02:53<02:49,  1.35s/it]Loading train:  56%|█████▌    | 160/285 [02:55<02:53,  1.39s/it]Loading train:  56%|█████▋    | 161/285 [02:56<02:56,  1.42s/it]Loading train:  57%|█████▋    | 162/285 [02:57<02:48,  1.37s/it]Loading train:  57%|█████▋    | 163/285 [02:59<02:42,  1.33s/it]Loading train:  58%|█████▊    | 164/285 [03:00<02:33,  1.27s/it]Loading train:  58%|█████▊    | 165/285 [03:01<02:32,  1.27s/it]Loading train:  58%|█████▊    | 166/285 [03:03<02:41,  1.35s/it]Loading train:  59%|█████▊    | 167/285 [03:04<02:41,  1.37s/it]Loading train:  59%|█████▉    | 168/285 [03:05<02:27,  1.26s/it]Loading train:  59%|█████▉    | 169/285 [03:06<02:30,  1.30s/it]Loading train:  60%|█████▉    | 170/285 [03:08<02:20,  1.22s/it]Loading train:  60%|██████    | 171/285 [03:08<02:10,  1.14s/it]Loading train:  60%|██████    | 172/285 [03:10<02:24,  1.28s/it]Loading train:  61%|██████    | 173/285 [03:12<02:45,  1.48s/it]Loading train:  61%|██████    | 174/285 [03:13<02:36,  1.41s/it]Loading train:  61%|██████▏   | 175/285 [03:14<02:23,  1.31s/it]Loading train:  62%|██████▏   | 176/285 [03:16<02:19,  1.28s/it]Loading train:  62%|██████▏   | 177/285 [03:17<02:14,  1.25s/it]Loading train:  62%|██████▏   | 178/285 [03:18<02:16,  1.28s/it]Loading train:  63%|██████▎   | 179/285 [03:19<02:17,  1.30s/it]Loading train:  63%|██████▎   | 180/285 [03:20<02:03,  1.17s/it]Loading train:  64%|██████▎   | 181/285 [03:21<01:58,  1.14s/it]Loading train:  64%|██████▍   | 182/285 [03:23<01:59,  1.16s/it]Loading train:  64%|██████▍   | 183/285 [03:24<01:58,  1.16s/it]Loading train:  65%|██████▍   | 184/285 [03:25<02:06,  1.25s/it]Loading train:  65%|██████▍   | 185/285 [03:27<02:06,  1.27s/it]Loading train:  65%|██████▌   | 186/285 [03:28<02:17,  1.39s/it]Loading train:  66%|██████▌   | 187/285 [03:30<02:24,  1.47s/it]Loading train:  66%|██████▌   | 188/285 [03:31<02:24,  1.49s/it]Loading train:  66%|██████▋   | 189/285 [03:32<02:11,  1.37s/it]Loading train:  67%|██████▋   | 190/285 [03:34<02:01,  1.28s/it]Loading train:  67%|██████▋   | 191/285 [03:34<01:47,  1.15s/it]Loading train:  67%|██████▋   | 192/285 [03:36<01:53,  1.22s/it]Loading train:  68%|██████▊   | 193/285 [03:37<01:49,  1.19s/it]Loading train:  68%|██████▊   | 194/285 [03:38<01:47,  1.19s/it]Loading train:  68%|██████▊   | 195/285 [03:39<01:46,  1.18s/it]Loading train:  69%|██████▉   | 196/285 [03:41<01:49,  1.23s/it]Loading train:  69%|██████▉   | 197/285 [03:42<01:56,  1.33s/it]Loading train:  69%|██████▉   | 198/285 [03:43<01:49,  1.26s/it]Loading train:  70%|██████▉   | 199/285 [03:44<01:45,  1.23s/it]Loading train:  70%|███████   | 200/285 [03:46<01:50,  1.30s/it]Loading train:  71%|███████   | 201/285 [03:47<01:46,  1.27s/it]Loading train:  71%|███████   | 202/285 [03:49<01:52,  1.36s/it]Loading train:  71%|███████   | 203/285 [03:50<01:48,  1.33s/it]Loading train:  72%|███████▏  | 204/285 [03:51<01:50,  1.36s/it]Loading train:  72%|███████▏  | 205/285 [03:53<01:55,  1.45s/it]Loading train:  72%|███████▏  | 206/285 [03:54<01:56,  1.47s/it]Loading train:  73%|███████▎  | 207/285 [03:56<01:54,  1.47s/it]Loading train:  73%|███████▎  | 208/285 [03:58<01:58,  1.53s/it]Loading train:  73%|███████▎  | 209/285 [03:59<01:49,  1.44s/it]Loading train:  74%|███████▎  | 210/285 [04:00<01:39,  1.32s/it]Loading train:  74%|███████▍  | 211/285 [04:01<01:34,  1.28s/it]Loading train:  74%|███████▍  | 212/285 [04:02<01:29,  1.23s/it]Loading train:  75%|███████▍  | 213/285 [04:04<01:30,  1.26s/it]Loading train:  75%|███████▌  | 214/285 [04:05<01:30,  1.28s/it]Loading train:  75%|███████▌  | 215/285 [04:06<01:30,  1.29s/it]Loading train:  76%|███████▌  | 216/285 [04:07<01:27,  1.27s/it]Loading train:  76%|███████▌  | 217/285 [04:09<01:23,  1.23s/it]Loading train:  76%|███████▋  | 218/285 [04:10<01:18,  1.17s/it]Loading train:  77%|███████▋  | 219/285 [04:11<01:26,  1.32s/it]Loading train:  77%|███████▋  | 220/285 [04:13<01:29,  1.38s/it]Loading train:  78%|███████▊  | 221/285 [04:15<01:41,  1.59s/it]Loading train:  78%|███████▊  | 222/285 [04:16<01:38,  1.56s/it]Loading train:  78%|███████▊  | 223/285 [04:18<01:41,  1.63s/it]Loading train:  79%|███████▊  | 224/285 [04:20<01:42,  1.67s/it]Loading train:  79%|███████▉  | 225/285 [04:21<01:38,  1.65s/it]Loading train:  79%|███████▉  | 226/285 [04:23<01:34,  1.60s/it]Loading train:  80%|███████▉  | 227/285 [04:25<01:32,  1.60s/it]Loading train:  80%|████████  | 228/285 [04:26<01:32,  1.63s/it]Loading train:  80%|████████  | 229/285 [04:27<01:20,  1.44s/it]Loading train:  81%|████████  | 230/285 [04:29<01:23,  1.52s/it]Loading train:  81%|████████  | 231/285 [04:30<01:17,  1.44s/it]Loading train:  81%|████████▏ | 232/285 [04:32<01:20,  1.53s/it]Loading train:  82%|████████▏ | 233/285 [04:33<01:19,  1.52s/it]Loading train:  82%|████████▏ | 234/285 [04:35<01:16,  1.51s/it]Loading train:  82%|████████▏ | 235/285 [04:37<01:18,  1.56s/it]Loading train:  83%|████████▎ | 236/285 [04:38<01:21,  1.66s/it]Loading train:  83%|████████▎ | 237/285 [04:40<01:20,  1.68s/it]Loading train:  84%|████████▎ | 238/285 [04:42<01:23,  1.78s/it]Loading train:  84%|████████▍ | 239/285 [04:44<01:21,  1.78s/it]Loading train:  84%|████████▍ | 240/285 [04:46<01:28,  1.97s/it]Loading train:  85%|████████▍ | 241/285 [04:49<01:30,  2.05s/it]Loading train:  85%|████████▍ | 242/285 [04:51<01:33,  2.16s/it]Loading train:  85%|████████▌ | 243/285 [04:53<01:33,  2.23s/it]Loading train:  86%|████████▌ | 244/285 [04:56<01:32,  2.26s/it]Loading train:  86%|████████▌ | 245/285 [04:58<01:32,  2.32s/it]Loading train:  86%|████████▋ | 246/285 [05:00<01:23,  2.13s/it]Loading train:  87%|████████▋ | 247/285 [05:02<01:17,  2.05s/it]Loading train:  87%|████████▋ | 248/285 [05:04<01:14,  2.01s/it]Loading train:  87%|████████▋ | 249/285 [05:06<01:10,  1.97s/it]Loading train:  88%|████████▊ | 250/285 [05:07<01:07,  1.92s/it]Loading train:  88%|████████▊ | 251/285 [05:09<01:04,  1.89s/it]Loading train:  88%|████████▊ | 252/285 [05:11<01:00,  1.83s/it]Loading train:  89%|████████▉ | 253/285 [05:13<00:56,  1.78s/it]Loading train:  89%|████████▉ | 254/285 [05:14<00:53,  1.71s/it]Loading train:  89%|████████▉ | 255/285 [05:16<00:48,  1.61s/it]Loading train:  90%|████████▉ | 256/285 [05:17<00:45,  1.57s/it]Loading train:  90%|█████████ | 257/285 [05:19<00:44,  1.58s/it]Loading train:  91%|█████████ | 258/285 [05:20<00:42,  1.57s/it]Loading train:  91%|█████████ | 259/285 [05:22<00:39,  1.52s/it]Loading train:  91%|█████████ | 260/285 [05:23<00:38,  1.54s/it]Loading train:  92%|█████████▏| 261/285 [05:25<00:36,  1.53s/it]Loading train:  92%|█████████▏| 262/285 [05:26<00:36,  1.57s/it]Loading train:  92%|█████████▏| 263/285 [05:28<00:37,  1.69s/it]Loading train:  93%|█████████▎| 264/285 [05:30<00:32,  1.57s/it]Loading train:  93%|█████████▎| 265/285 [05:31<00:28,  1.45s/it]Loading train:  93%|█████████▎| 266/285 [05:32<00:29,  1.55s/it]Loading train:  94%|█████████▎| 267/285 [05:34<00:28,  1.61s/it]Loading train:  94%|█████████▍| 268/285 [05:36<00:30,  1.78s/it]Loading train:  94%|█████████▍| 269/285 [05:39<00:30,  1.90s/it]Loading train:  95%|█████████▍| 270/285 [05:41<00:29,  1.95s/it]Loading train:  95%|█████████▌| 271/285 [05:42<00:26,  1.91s/it]Loading train:  95%|█████████▌| 272/285 [05:44<00:23,  1.83s/it]Loading train:  96%|█████████▌| 273/285 [05:47<00:24,  2.02s/it]Loading train:  96%|█████████▌| 274/285 [05:48<00:21,  1.93s/it]Loading train:  96%|█████████▋| 275/285 [05:50<00:19,  1.91s/it]Loading train:  97%|█████████▋| 276/285 [05:52<00:17,  1.89s/it]Loading train:  97%|█████████▋| 277/285 [05:54<00:15,  1.97s/it]Loading train:  98%|█████████▊| 278/285 [05:56<00:14,  2.01s/it]Loading train:  98%|█████████▊| 279/285 [05:58<00:11,  1.93s/it]Loading train:  98%|█████████▊| 280/285 [06:00<00:09,  1.97s/it]Loading train:  99%|█████████▊| 281/285 [06:02<00:07,  1.98s/it]Loading train:  99%|█████████▉| 282/285 [06:04<00:06,  2.04s/it]Loading train:  99%|█████████▉| 283/285 [06:06<00:03,  1.90s/it]Loading train: 100%|█████████▉| 284/285 [06:07<00:01,  1.78s/it]Loading train: 100%|██████████| 285/285 [06:09<00:00,  1.73s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|▏         | 4/285 [00:00<00:09, 29.14it/s]concatenating: train:   2%|▏         | 7/285 [00:00<00:09, 29.39it/s]concatenating: train:   4%|▍         | 12/285 [00:00<00:08, 32.75it/s]concatenating: train:   6%|▋         | 18/285 [00:00<00:07, 37.07it/s]concatenating: train:   9%|▉         | 25/285 [00:00<00:06, 42.17it/s]concatenating: train:  11%|█         | 30/285 [00:00<00:06, 37.54it/s]concatenating: train:  12%|█▏        | 34/285 [00:00<00:07, 32.95it/s]concatenating: train:  13%|█▎        | 38/285 [00:01<00:07, 31.02it/s]concatenating: train:  15%|█▍        | 42/285 [00:01<00:12, 19.32it/s]concatenating: train:  16%|█▌        | 45/285 [00:01<00:11, 21.44it/s]concatenating: train:  17%|█▋        | 48/285 [00:01<00:11, 21.08it/s]concatenating: train:  18%|█▊        | 51/285 [00:01<00:10, 21.92it/s]concatenating: train:  19%|█▉        | 55/285 [00:01<00:09, 23.91it/s]concatenating: train:  21%|██        | 59/285 [00:02<00:08, 26.59it/s]concatenating: train:  28%|██▊       | 81/285 [00:02<00:05, 36.08it/s]concatenating: train:  33%|███▎      | 95/285 [00:02<00:04, 44.52it/s]concatenating: train:  36%|███▋      | 104/285 [00:02<00:05, 33.95it/s]concatenating: train:  39%|███▉      | 111/285 [00:02<00:04, 34.87it/s]concatenating: train:  41%|████      | 117/285 [00:03<00:04, 37.21it/s]concatenating: train:  43%|████▎     | 123/285 [00:03<00:04, 36.75it/s]concatenating: train:  45%|████▌     | 129/285 [00:03<00:03, 41.24it/s]concatenating: train:  47%|████▋     | 135/285 [00:03<00:03, 40.69it/s]concatenating: train:  49%|████▉     | 140/285 [00:03<00:03, 43.05it/s]concatenating: train:  51%|█████     | 145/285 [00:03<00:03, 36.67it/s]concatenating: train:  53%|█████▎    | 150/285 [00:03<00:03, 34.85it/s]concatenating: train:  54%|█████▍    | 154/285 [00:04<00:03, 33.69it/s]concatenating: train:  56%|█████▌    | 159/285 [00:04<00:03, 36.97it/s]concatenating: train:  58%|█████▊    | 165/285 [00:04<00:02, 40.47it/s]concatenating: train:  60%|█████▉    | 170/285 [00:04<00:03, 33.58it/s]concatenating: train:  61%|██████▏   | 175/285 [00:04<00:03, 35.66it/s]concatenating: train:  63%|██████▎   | 180/285 [00:04<00:02, 36.91it/s]concatenating: train:  65%|██████▍   | 185/285 [00:04<00:02, 38.25it/s]concatenating: train:  67%|██████▋   | 191/285 [00:04<00:02, 42.17it/s]concatenating: train:  69%|██████▉   | 198/285 [00:05<00:01, 46.16it/s]concatenating: train:  72%|███████▏  | 204/285 [00:05<00:01, 42.66it/s]concatenating: train:  73%|███████▎  | 209/285 [00:05<00:01, 41.51it/s]concatenating: train:  75%|███████▌  | 214/285 [00:05<00:01, 42.79it/s]concatenating: train:  77%|███████▋  | 219/285 [00:05<00:01, 43.34it/s]concatenating: train:  79%|███████▉  | 225/285 [00:05<00:01, 46.44it/s]concatenating: train:  82%|████████▏ | 234/285 [00:05<00:00, 53.73it/s]concatenating: train:  86%|████████▋ | 246/285 [00:05<00:00, 64.17it/s]concatenating: train:  96%|█████████▌| 273/285 [00:05<00:00, 83.14it/s]concatenating: train: 100%|██████████| 285/285 [00:06<00:00, 46.16it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:02<00:04,  2.43s/it]Loading test:  67%|██████▋   | 2/3 [00:04<00:02,  2.34s/it]Loading test: 100%|██████████| 3/3 [00:06<00:00,  2.20s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 20.19it/s]
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:11,  3.49it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:01<00:15,  2.46it/s]loading the weights for Unet:  10%|█         | 4/40 [00:02<00:16,  2.15it/s]loading the weights for Unet:  20%|██        | 8/40 [00:02<00:11,  2.75it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:03<00:10,  2.98it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:03<00:08,  3.50it/s]loading the weights for Unet:  30%|███       | 12/40 [00:03<00:08,  3.23it/s]loading the weights for Unet:  40%|████      | 16/40 [00:04<00:05,  4.14it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:05<00:13,  1.69it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:06<00:11,  1.86it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:06<00:09,  2.15it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:06<00:06,  2.79it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:07<00:04,  3.16it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:07<00:04,  2.85it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:09<00:05,  2.34it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:09<00:05,  2.06it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:10<00:03,  2.61it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:10<00:02,  2.99it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:11<00:01,  2.54it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:11<00:00,  3.15it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:12<00:01,  1.51it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:12<00:00,  3.13it/s]
(0/4) test vimp2_ctrl_991_08302013_JF
(1/4) test vimp2_967_08132013_KW
(2/4) test vimp2_765_04162013_AW
(3/4) test vimp2_ANON695_03132013
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 80, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 80, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 80, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 80, 10)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 80, 10)   910         dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 80, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 80, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 80, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 80, 10)   910         dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 80, 10)   40          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 80, 10)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 80, 10)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 80, 20)   1820        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 80, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 80, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 80, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 80, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 80, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 40, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 40, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 40, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 40, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 40, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 40, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 40, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 20, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 20, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 20, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 20, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 20, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 20, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 20, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 20, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 40, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 40, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 40, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 40, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 40, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 40, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 13)   273         dropout_8[0][0]                  
==================================================================================================
Total params: 187,773
Trainable params: 44,233
Non-trainable params: 143,540
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 29s - loss: 241.4219 - acc: 0.7490 - mDice: 0.0165 - val_loss: 59.4786 - val_acc: 0.9047 - val_mDice: 0.0123

Epoch 00001: val_mDice improved from -inf to 0.01227, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 12s - loss: 48.7631 - acc: 0.8599 - mDice: 0.0129 - val_loss: 17.4941 - val_acc: 0.9047 - val_mDice: 0.0099

Epoch 00002: val_mDice did not improve from 0.01227
Epoch 3/300
 - 13s - loss: 22.7329 - acc: 0.8672 - mDice: 0.0112 - val_loss: 9.6445 - val_acc: 0.9047 - val_mDice: 0.0091

Epoch 00003: val_mDice did not improve from 0.01227
Epoch 4/300
 - 13s - loss: 15.6539 - acc: 0.8686 - mDice: 0.0118 - val_loss: 7.6357 - val_acc: 0.9047 - val_mDice: 0.0096

Epoch 00004: val_mDice did not improve from 0.01227
Epoch 5/300
 - 13s - loss: 12.5608 - acc: 0.8689 - mDice: 0.0137 - val_loss: 6.9492 - val_acc: 0.9047 - val_mDice: 0.0107

Epoch 00005: val_mDice did not improve from 0.01227
Epoch 6/300
 - 12s - loss: 10.7844 - acc: 0.8690 - mDice: 0.0160 - val_loss: 6.6281 - val_acc: 0.9047 - val_mDice: 0.0119

Epoch 00006: val_mDice did not improve from 0.01227
Epoch 7/300
 - 11s - loss: 9.6845 - acc: 0.8690 - mDice: 0.0180 - val_loss: 6.3663 - val_acc: 0.9047 - val_mDice: 0.0127

Epoch 00007: val_mDice improved from 0.01227 to 0.01271, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 10s - loss: 8.9441 - acc: 0.8690 - mDice: 0.0202 - val_loss: 6.1926 - val_acc: 0.9047 - val_mDice: 0.0152

Epoch 00008: val_mDice improved from 0.01271 to 0.01525, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 10s - loss: 8.4073 - acc: 0.8690 - mDice: 0.0221 - val_loss: 6.1812 - val_acc: 0.9047 - val_mDice: 0.0157

Epoch 00009: val_mDice improved from 0.01525 to 0.01571, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 9s - loss: 8.0108 - acc: 0.8690 - mDice: 0.0236 - val_loss: 5.8503 - val_acc: 0.9047 - val_mDice: 0.0197

Epoch 00010: val_mDice improved from 0.01571 to 0.01968, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 10s - loss: 7.6535 - acc: 0.8690 - mDice: 0.0259 - val_loss: 5.9242 - val_acc: 0.9047 - val_mDice: 0.0186

Epoch 00011: val_mDice did not improve from 0.01968
Epoch 12/300
 - 9s - loss: 7.3438 - acc: 0.8689 - mDice: 0.0282 - val_loss: 5.8122 - val_acc: 0.9047 - val_mDice: 0.0213

Epoch 00012: val_mDice improved from 0.01968 to 0.02131, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 9s - loss: 7.0799 - acc: 0.8688 - mDice: 0.0310 - val_loss: 5.6288 - val_acc: 0.9047 - val_mDice: 0.0254

Epoch 00013: val_mDice improved from 0.02131 to 0.02539, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 10s - loss: 6.8336 - acc: 0.8685 - mDice: 0.0338 - val_loss: 5.6336 - val_acc: 0.9047 - val_mDice: 0.0270

Epoch 00014: val_mDice improved from 0.02539 to 0.02699, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 9s - loss: 6.6000 - acc: 0.8683 - mDice: 0.0371 - val_loss: 6.1591 - val_acc: 0.9047 - val_mDice: 0.0189

Epoch 00015: val_mDice did not improve from 0.02699
Epoch 16/300
 - 9s - loss: 6.3862 - acc: 0.8683 - mDice: 0.0403 - val_loss: 5.7679 - val_acc: 0.9047 - val_mDice: 0.0228

Epoch 00016: val_mDice did not improve from 0.02699
Epoch 17/300
 - 10s - loss: 6.1159 - acc: 0.8683 - mDice: 0.0472 - val_loss: 5.2559 - val_acc: 0.9047 - val_mDice: 0.0425

Epoch 00017: val_mDice improved from 0.02699 to 0.04255, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 9s - loss: 5.9105 - acc: 0.8682 - mDice: 0.0541 - val_loss: 5.3096 - val_acc: 0.9047 - val_mDice: 0.0451

Epoch 00018: val_mDice improved from 0.04255 to 0.04512, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 9s - loss: 5.7500 - acc: 0.8682 - mDice: 0.0595 - val_loss: 5.2407 - val_acc: 0.9047 - val_mDice: 0.0464

Epoch 00019: val_mDice improved from 0.04512 to 0.04637, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 10s - loss: 5.5880 - acc: 0.8683 - mDice: 0.0648 - val_loss: 5.0068 - val_acc: 0.9047 - val_mDice: 0.0580

Epoch 00020: val_mDice improved from 0.04637 to 0.05802, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 9s - loss: 5.4389 - acc: 0.8684 - mDice: 0.0710 - val_loss: 5.0587 - val_acc: 0.9047 - val_mDice: 0.0612

Epoch 00021: val_mDice improved from 0.05802 to 0.06123, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 9s - loss: 5.3235 - acc: 0.8685 - mDice: 0.0755 - val_loss: 4.7029 - val_acc: 0.9047 - val_mDice: 0.0741

Epoch 00022: val_mDice improved from 0.06123 to 0.07407, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 9s - loss: 5.1966 - acc: 0.8685 - mDice: 0.0808 - val_loss: 4.3117 - val_acc: 0.9048 - val_mDice: 0.0893

Epoch 00023: val_mDice improved from 0.07407 to 0.08929, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 10s - loss: 5.0505 - acc: 0.8689 - mDice: 0.0875 - val_loss: 4.2746 - val_acc: 0.9048 - val_mDice: 0.0928

Epoch 00024: val_mDice improved from 0.08929 to 0.09283, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 9s - loss: 4.9362 - acc: 0.8692 - mDice: 0.0932 - val_loss: 5.0218 - val_acc: 0.9048 - val_mDice: 0.0713

Epoch 00025: val_mDice did not improve from 0.09283
Epoch 26/300
 - 9s - loss: 4.8246 - acc: 0.8694 - mDice: 0.0988 - val_loss: 4.1068 - val_acc: 0.9053 - val_mDice: 0.1048

Epoch 00026: val_mDice improved from 0.09283 to 0.10478, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 9s - loss: 4.7406 - acc: 0.8696 - mDice: 0.1039 - val_loss: 4.1176 - val_acc: 0.9052 - val_mDice: 0.1064

Epoch 00027: val_mDice improved from 0.10478 to 0.10638, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 10s - loss: 4.6208 - acc: 0.8704 - mDice: 0.1116 - val_loss: 4.1111 - val_acc: 0.9052 - val_mDice: 0.1106

Epoch 00028: val_mDice improved from 0.10638 to 0.11061, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 10s - loss: 4.5051 - acc: 0.8707 - mDice: 0.1195 - val_loss: 4.5565 - val_acc: 0.9049 - val_mDice: 0.0962

Epoch 00029: val_mDice did not improve from 0.11061
Epoch 30/300
 - 10s - loss: 4.3922 - acc: 0.8714 - mDice: 0.1279 - val_loss: 4.8353 - val_acc: 0.9049 - val_mDice: 0.0933

Epoch 00030: val_mDice did not improve from 0.11061
Epoch 31/300
 - 10s - loss: 4.2906 - acc: 0.8720 - mDice: 0.1363 - val_loss: 4.6713 - val_acc: 0.9049 - val_mDice: 0.1074

Epoch 00031: val_mDice did not improve from 0.11061
Epoch 32/300
 - 11s - loss: 4.1872 - acc: 0.8728 - mDice: 0.1450 - val_loss: 4.0251 - val_acc: 0.9050 - val_mDice: 0.1354

Epoch 00032: val_mDice improved from 0.11061 to 0.13541, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 10s - loss: 4.0986 - acc: 0.8735 - mDice: 0.1529 - val_loss: 3.7433 - val_acc: 0.9055 - val_mDice: 0.1512

Epoch 00033: val_mDice improved from 0.13541 to 0.15119, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 10s - loss: 4.0215 - acc: 0.8741 - mDice: 0.1601 - val_loss: 3.5923 - val_acc: 0.9087 - val_mDice: 0.1670

Epoch 00034: val_mDice improved from 0.15119 to 0.16700, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 11s - loss: 3.9519 - acc: 0.8748 - mDice: 0.1664 - val_loss: 3.8284 - val_acc: 0.9059 - val_mDice: 0.1602

Epoch 00035: val_mDice did not improve from 0.16700
Epoch 36/300
 - 10s - loss: 3.8748 - acc: 0.8758 - mDice: 0.1744 - val_loss: 3.4869 - val_acc: 0.9088 - val_mDice: 0.1814

Epoch 00036: val_mDice improved from 0.16700 to 0.18143, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 10s - loss: 3.8100 - acc: 0.8765 - mDice: 0.1810 - val_loss: 3.4480 - val_acc: 0.9097 - val_mDice: 0.1897

Epoch 00037: val_mDice improved from 0.18143 to 0.18969, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 10s - loss: 3.7310 - acc: 0.8775 - mDice: 0.1893 - val_loss: 3.4449 - val_acc: 0.9122 - val_mDice: 0.1918

Epoch 00038: val_mDice improved from 0.18969 to 0.19178, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 10s - loss: 3.6644 - acc: 0.8781 - mDice: 0.1965 - val_loss: 3.5596 - val_acc: 0.9117 - val_mDice: 0.2015

Epoch 00039: val_mDice improved from 0.19178 to 0.20152, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 9s - loss: 3.6004 - acc: 0.8789 - mDice: 0.2049 - val_loss: 4.1599 - val_acc: 0.9104 - val_mDice: 0.1850

Epoch 00040: val_mDice did not improve from 0.20152
Epoch 41/300
 - 10s - loss: 3.5559 - acc: 0.8797 - mDice: 0.2103 - val_loss: 3.9193 - val_acc: 0.9085 - val_mDice: 0.1928

Epoch 00041: val_mDice did not improve from 0.20152
Epoch 42/300
 - 9s - loss: 3.4896 - acc: 0.8804 - mDice: 0.2181 - val_loss: 3.4196 - val_acc: 0.9139 - val_mDice: 0.2253

Epoch 00042: val_mDice improved from 0.20152 to 0.22534, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 10s - loss: 3.4307 - acc: 0.8818 - mDice: 0.2258 - val_loss: 4.1549 - val_acc: 0.9082 - val_mDice: 0.1970

Epoch 00043: val_mDice did not improve from 0.22534
Epoch 44/300
 - 9s - loss: 3.3822 - acc: 0.8820 - mDice: 0.2327 - val_loss: 3.3868 - val_acc: 0.9136 - val_mDice: 0.2330

Epoch 00044: val_mDice improved from 0.22534 to 0.23304, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 9s - loss: 3.3389 - acc: 0.8830 - mDice: 0.2385 - val_loss: 3.5073 - val_acc: 0.9156 - val_mDice: 0.2404

Epoch 00045: val_mDice improved from 0.23304 to 0.24044, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 10s - loss: 3.2964 - acc: 0.8836 - mDice: 0.2441 - val_loss: 3.8308 - val_acc: 0.9132 - val_mDice: 0.2305

Epoch 00046: val_mDice did not improve from 0.24044
Epoch 47/300
 - 9s - loss: 3.2655 - acc: 0.8844 - mDice: 0.2497 - val_loss: 3.0551 - val_acc: 0.9203 - val_mDice: 0.2693

Epoch 00047: val_mDice improved from 0.24044 to 0.26929, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 9s - loss: 3.2105 - acc: 0.8849 - mDice: 0.2564 - val_loss: 3.1078 - val_acc: 0.9187 - val_mDice: 0.2662

Epoch 00048: val_mDice did not improve from 0.26929
Epoch 49/300
 - 10s - loss: 3.1829 - acc: 0.8854 - mDice: 0.2613 - val_loss: 3.1533 - val_acc: 0.9212 - val_mDice: 0.2700

Epoch 00049: val_mDice improved from 0.26929 to 0.26995, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 9s - loss: 3.1445 - acc: 0.8862 - mDice: 0.2661 - val_loss: 3.9761 - val_acc: 0.9159 - val_mDice: 0.2426

Epoch 00050: val_mDice did not improve from 0.26995
Epoch 51/300
 - 9s - loss: 3.1021 - acc: 0.8871 - mDice: 0.2734 - val_loss: 3.0868 - val_acc: 0.9211 - val_mDice: 0.2809

Epoch 00051: val_mDice improved from 0.26995 to 0.28095, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 9s - loss: 3.0705 - acc: 0.8875 - mDice: 0.2776 - val_loss: 3.0185 - val_acc: 0.9216 - val_mDice: 0.2863

Epoch 00052: val_mDice improved from 0.28095 to 0.28635, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 10s - loss: 3.0294 - acc: 0.8885 - mDice: 0.2841 - val_loss: 3.0237 - val_acc: 0.9218 - val_mDice: 0.2931

Epoch 00053: val_mDice improved from 0.28635 to 0.29312, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 9s - loss: 3.0005 - acc: 0.8886 - mDice: 0.2875 - val_loss: 3.1377 - val_acc: 0.9225 - val_mDice: 0.2872

Epoch 00054: val_mDice did not improve from 0.29312
Epoch 55/300
 - 9s - loss: 2.9686 - acc: 0.8890 - mDice: 0.2924 - val_loss: 3.1064 - val_acc: 0.9223 - val_mDice: 0.2916

Epoch 00055: val_mDice did not improve from 0.29312
Epoch 56/300
 - 10s - loss: 2.9347 - acc: 0.8897 - mDice: 0.2979 - val_loss: 2.9764 - val_acc: 0.9218 - val_mDice: 0.3019

Epoch 00056: val_mDice improved from 0.29312 to 0.30191, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 57/300
 - 10s - loss: 2.9110 - acc: 0.8901 - mDice: 0.3012 - val_loss: 2.9406 - val_acc: 0.9230 - val_mDice: 0.3043

Epoch 00057: val_mDice improved from 0.30191 to 0.30430, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 58/300
 - 10s - loss: 2.8845 - acc: 0.8907 - mDice: 0.3059 - val_loss: 2.9158 - val_acc: 0.9226 - val_mDice: 0.3080

Epoch 00058: val_mDice improved from 0.30430 to 0.30801, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 10s - loss: 2.8636 - acc: 0.8911 - mDice: 0.3087 - val_loss: 3.0228 - val_acc: 0.9249 - val_mDice: 0.3061

Epoch 00059: val_mDice did not improve from 0.30801
Epoch 60/300
 - 11s - loss: 2.8348 - acc: 0.8920 - mDice: 0.3134 - val_loss: 3.3659 - val_acc: 0.9229 - val_mDice: 0.2985

Epoch 00060: val_mDice did not improve from 0.30801
Epoch 61/300
 - 11s - loss: 2.8224 - acc: 0.8923 - mDice: 0.3159 - val_loss: 2.9624 - val_acc: 0.9247 - val_mDice: 0.3123

Epoch 00061: val_mDice improved from 0.30801 to 0.31233, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 62/300
 - 11s - loss: 2.7980 - acc: 0.8929 - mDice: 0.3195 - val_loss: 3.0398 - val_acc: 0.9267 - val_mDice: 0.3159

Epoch 00062: val_mDice improved from 0.31233 to 0.31594, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 10s - loss: 2.7768 - acc: 0.8932 - mDice: 0.3227 - val_loss: 3.0243 - val_acc: 0.9270 - val_mDice: 0.3210

Epoch 00063: val_mDice improved from 0.31594 to 0.32096, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 11s - loss: 2.7555 - acc: 0.8940 - mDice: 0.3268 - val_loss: 3.2176 - val_acc: 0.9260 - val_mDice: 0.3087

Epoch 00064: val_mDice did not improve from 0.32096
Epoch 65/300
 - 11s - loss: 2.7333 - acc: 0.8948 - mDice: 0.3310 - val_loss: 3.3511 - val_acc: 0.9238 - val_mDice: 0.3093

Epoch 00065: val_mDice did not improve from 0.32096
Epoch 66/300
 - 11s - loss: 2.7180 - acc: 0.8951 - mDice: 0.3334 - val_loss: 2.9936 - val_acc: 0.9278 - val_mDice: 0.3261

Epoch 00066: val_mDice improved from 0.32096 to 0.32606, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 67/300
 - 10s - loss: 2.6952 - acc: 0.8958 - mDice: 0.3378 - val_loss: 2.8916 - val_acc: 0.9280 - val_mDice: 0.3344

Epoch 00067: val_mDice improved from 0.32606 to 0.33441, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 68/300
 - 11s - loss: 2.6817 - acc: 0.8961 - mDice: 0.3394 - val_loss: 3.4444 - val_acc: 0.9254 - val_mDice: 0.3132

Epoch 00068: val_mDice did not improve from 0.33441
Epoch 69/300
 - 11s - loss: 2.6646 - acc: 0.8966 - mDice: 0.3430 - val_loss: 2.9379 - val_acc: 0.9264 - val_mDice: 0.3329

Epoch 00069: val_mDice did not improve from 0.33441
Epoch 70/300
 - 11s - loss: 2.6465 - acc: 0.8973 - mDice: 0.3453 - val_loss: 3.3330 - val_acc: 0.9288 - val_mDice: 0.3247

Epoch 00070: val_mDice did not improve from 0.33441
Epoch 71/300
 - 10s - loss: 2.6337 - acc: 0.8978 - mDice: 0.3488 - val_loss: 2.8782 - val_acc: 0.9283 - val_mDice: 0.3440

Epoch 00071: val_mDice improved from 0.33441 to 0.34396, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 72/300
 - 11s - loss: 2.6102 - acc: 0.8985 - mDice: 0.3522 - val_loss: 2.9887 - val_acc: 0.9302 - val_mDice: 0.3405

Epoch 00072: val_mDice did not improve from 0.34396
Epoch 73/300
 - 10s - loss: 2.5946 - acc: 0.8990 - mDice: 0.3556 - val_loss: 2.9095 - val_acc: 0.9291 - val_mDice: 0.3472

Epoch 00073: val_mDice improved from 0.34396 to 0.34719, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 10s - loss: 2.5819 - acc: 0.8994 - mDice: 0.3583 - val_loss: 2.9079 - val_acc: 0.9309 - val_mDice: 0.3525

Epoch 00074: val_mDice improved from 0.34719 to 0.35247, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 75/300
 - 11s - loss: 2.5661 - acc: 0.8999 - mDice: 0.3608 - val_loss: 2.9549 - val_acc: 0.9305 - val_mDice: 0.3482

Epoch 00075: val_mDice did not improve from 0.35247
Epoch 76/300
 - 10s - loss: 2.5615 - acc: 0.9003 - mDice: 0.3622 - val_loss: 3.1086 - val_acc: 0.9311 - val_mDice: 0.3433

Epoch 00076: val_mDice did not improve from 0.35247
Epoch 77/300
 - 11s - loss: 2.5441 - acc: 0.9005 - mDice: 0.3652 - val_loss: 2.9595 - val_acc: 0.9249 - val_mDice: 0.3486

Epoch 00077: val_mDice did not improve from 0.35247
Epoch 78/300
 - 10s - loss: 2.5295 - acc: 0.9011 - mDice: 0.3689 - val_loss: 2.8608 - val_acc: 0.9319 - val_mDice: 0.3596

Epoch 00078: val_mDice improved from 0.35247 to 0.35961, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 79/300
 - 11s - loss: 2.5146 - acc: 0.9012 - mDice: 0.3707 - val_loss: 2.9310 - val_acc: 0.9318 - val_mDice: 0.3581

Epoch 00079: val_mDice did not improve from 0.35961
Epoch 80/300
 - 11s - loss: 2.4998 - acc: 0.9020 - mDice: 0.3744 - val_loss: 3.0783 - val_acc: 0.9318 - val_mDice: 0.3539

Epoch 00080: val_mDice did not improve from 0.35961
Epoch 81/300
 - 10s - loss: 2.4854 - acc: 0.9021 - mDice: 0.3763 - val_loss: 3.3184 - val_acc: 0.9307 - val_mDice: 0.3450

Epoch 00081: val_mDice did not improve from 0.35961
Epoch 82/300
 - 11s - loss: 2.4829 - acc: 0.9026 - mDice: 0.3776 - val_loss: 3.2419 - val_acc: 0.9318 - val_mDice: 0.3487

Epoch 00082: val_mDice did not improve from 0.35961
Epoch 83/300
 - 10s - loss: 2.4732 - acc: 0.9027 - mDice: 0.3790 - val_loss: 2.8752 - val_acc: 0.9308 - val_mDice: 0.3653

Epoch 00083: val_mDice improved from 0.35961 to 0.36526, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 84/300
 - 11s - loss: 2.4576 - acc: 0.9035 - mDice: 0.3823 - val_loss: 2.9948 - val_acc: 0.9331 - val_mDice: 0.3607

Epoch 00084: val_mDice did not improve from 0.36526
Epoch 85/300
 - 10s - loss: 2.4498 - acc: 0.9035 - mDice: 0.3836 - val_loss: 2.8716 - val_acc: 0.9267 - val_mDice: 0.3632

Epoch 00085: val_mDice did not improve from 0.36526
Epoch 86/300
 - 10s - loss: 2.4316 - acc: 0.9038 - mDice: 0.3872 - val_loss: 2.9958 - val_acc: 0.9297 - val_mDice: 0.3593

Epoch 00086: val_mDice did not improve from 0.36526
Epoch 87/300
 - 11s - loss: 2.4231 - acc: 0.9042 - mDice: 0.3889 - val_loss: 3.3170 - val_acc: 0.9298 - val_mDice: 0.3451

Epoch 00087: val_mDice did not improve from 0.36526
Epoch 88/300
 - 10s - loss: 2.4140 - acc: 0.9047 - mDice: 0.3905 - val_loss: 2.9275 - val_acc: 0.9317 - val_mDice: 0.3662

Epoch 00088: val_mDice improved from 0.36526 to 0.36623, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 89/300
 - 10s - loss: 2.4015 - acc: 0.9047 - mDice: 0.3930 - val_loss: 2.8282 - val_acc: 0.9324 - val_mDice: 0.3697

Epoch 00089: val_mDice improved from 0.36623 to 0.36969, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 90/300
 - 10s - loss: 2.3984 - acc: 0.9049 - mDice: 0.3934 - val_loss: 3.0958 - val_acc: 0.9329 - val_mDice: 0.3603

Epoch 00090: val_mDice did not improve from 0.36969
Epoch 91/300
 - 10s - loss: 2.3834 - acc: 0.9053 - mDice: 0.3964 - val_loss: 2.8865 - val_acc: 0.9319 - val_mDice: 0.3723

Epoch 00091: val_mDice improved from 0.36969 to 0.37235, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 92/300
 - 11s - loss: 2.3791 - acc: 0.9056 - mDice: 0.3978 - val_loss: 2.9531 - val_acc: 0.9324 - val_mDice: 0.3696

Epoch 00092: val_mDice did not improve from 0.37235
Epoch 93/300
 - 10s - loss: 2.3647 - acc: 0.9059 - mDice: 0.3998 - val_loss: 2.8128 - val_acc: 0.9310 - val_mDice: 0.3783

Epoch 00093: val_mDice improved from 0.37235 to 0.37829, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 94/300
 - 10s - loss: 2.3573 - acc: 0.9060 - mDice: 0.4017 - val_loss: 2.8308 - val_acc: 0.9320 - val_mDice: 0.3776

Epoch 00094: val_mDice did not improve from 0.37829
Epoch 95/300
 - 10s - loss: 2.3454 - acc: 0.9066 - mDice: 0.4044 - val_loss: 2.9517 - val_acc: 0.9344 - val_mDice: 0.3810

Epoch 00095: val_mDice improved from 0.37829 to 0.38101, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 96/300
 - 10s - loss: 2.3413 - acc: 0.9067 - mDice: 0.4050 - val_loss: 2.9304 - val_acc: 0.9338 - val_mDice: 0.3811

Epoch 00096: val_mDice improved from 0.38101 to 0.38114, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 97/300
 - 10s - loss: 2.3340 - acc: 0.9070 - mDice: 0.4065 - val_loss: 2.8607 - val_acc: 0.9332 - val_mDice: 0.3882

Epoch 00097: val_mDice improved from 0.38114 to 0.38819, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 98/300
 - 11s - loss: 2.3274 - acc: 0.9072 - mDice: 0.4082 - val_loss: 3.1555 - val_acc: 0.9339 - val_mDice: 0.3704

Epoch 00098: val_mDice did not improve from 0.38819
Epoch 99/300
 - 10s - loss: 2.3160 - acc: 0.9074 - mDice: 0.4102 - val_loss: 2.9461 - val_acc: 0.9356 - val_mDice: 0.3849

Epoch 00099: val_mDice did not improve from 0.38819
Epoch 100/300
 - 10s - loss: 2.3052 - acc: 0.9076 - mDice: 0.4114 - val_loss: 2.8866 - val_acc: 0.9317 - val_mDice: 0.3796

Epoch 00100: val_mDice did not improve from 0.38819
Epoch 101/300
 - 11s - loss: 2.2970 - acc: 0.9080 - mDice: 0.4143 - val_loss: 2.8802 - val_acc: 0.9331 - val_mDice: 0.3888

Epoch 00101: val_mDice improved from 0.38819 to 0.38876, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 102/300
 - 10s - loss: 2.2912 - acc: 0.9080 - mDice: 0.4151 - val_loss: 2.7934 - val_acc: 0.9346 - val_mDice: 0.3934

Epoch 00102: val_mDice improved from 0.38876 to 0.39337, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 103/300
 - 10s - loss: 2.2842 - acc: 0.9083 - mDice: 0.4174 - val_loss: 2.9074 - val_acc: 0.9344 - val_mDice: 0.3887

Epoch 00103: val_mDice did not improve from 0.39337
Epoch 104/300
 - 11s - loss: 2.2760 - acc: 0.9087 - mDice: 0.4185 - val_loss: 2.8100 - val_acc: 0.9317 - val_mDice: 0.3890

Epoch 00104: val_mDice did not improve from 0.39337
Epoch 105/300
 - 9s - loss: 2.2685 - acc: 0.9089 - mDice: 0.4204 - val_loss: 2.9484 - val_acc: 0.9348 - val_mDice: 0.3874

Epoch 00105: val_mDice did not improve from 0.39337
Epoch 106/300
 - 10s - loss: 2.2658 - acc: 0.9091 - mDice: 0.4208 - val_loss: 3.0415 - val_acc: 0.9351 - val_mDice: 0.3915

Epoch 00106: val_mDice did not improve from 0.39337
Epoch 107/300
 - 10s - loss: 2.2509 - acc: 0.9091 - mDice: 0.4238 - val_loss: 3.0744 - val_acc: 0.9346 - val_mDice: 0.3913

Epoch 00107: val_mDice did not improve from 0.39337
Epoch 108/300
 - 10s - loss: 2.2436 - acc: 0.9097 - mDice: 0.4261 - val_loss: 3.3737 - val_acc: 0.9337 - val_mDice: 0.3736

Epoch 00108: val_mDice did not improve from 0.39337
Epoch 109/300
 - 10s - loss: 2.2422 - acc: 0.9101 - mDice: 0.4268 - val_loss: 3.0172 - val_acc: 0.9350 - val_mDice: 0.3886

Epoch 00109: val_mDice did not improve from 0.39337
Epoch 110/300
 - 10s - loss: 2.2272 - acc: 0.9105 - mDice: 0.4299 - val_loss: 2.8794 - val_acc: 0.9343 - val_mDice: 0.3892

Epoch 00110: val_mDice did not improve from 0.39337
Epoch 111/300
 - 10s - loss: 2.2194 - acc: 0.9108 - mDice: 0.4316 - val_loss: 2.9389 - val_acc: 0.9356 - val_mDice: 0.3953

Epoch 00111: val_mDice improved from 0.39337 to 0.39534, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 112/300
 - 10s - loss: 2.2080 - acc: 0.9110 - mDice: 0.4348 - val_loss: 2.8320 - val_acc: 0.9322 - val_mDice: 0.3936

Epoch 00112: val_mDice did not improve from 0.39534
Epoch 113/300
 - 10s - loss: 2.2020 - acc: 0.9115 - mDice: 0.4353 - val_loss: 2.8937 - val_acc: 0.9343 - val_mDice: 0.3944

Epoch 00113: val_mDice did not improve from 0.39534
Epoch 114/300
 - 10s - loss: 2.1995 - acc: 0.9116 - mDice: 0.4364 - val_loss: 2.9128 - val_acc: 0.9340 - val_mDice: 0.3905

Epoch 00114: val_mDice did not improve from 0.39534
Epoch 115/300
 - 10s - loss: 2.1824 - acc: 0.9118 - mDice: 0.4395 - val_loss: 2.9730 - val_acc: 0.9370 - val_mDice: 0.4048

Epoch 00115: val_mDice improved from 0.39534 to 0.40476, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 116/300
 - 10s - loss: 2.1843 - acc: 0.9120 - mDice: 0.4395 - val_loss: 3.1542 - val_acc: 0.9353 - val_mDice: 0.3890

Epoch 00116: val_mDice did not improve from 0.40476
Epoch 117/300
 - 10s - loss: 2.1717 - acc: 0.9124 - mDice: 0.4425 - val_loss: 2.9142 - val_acc: 0.9324 - val_mDice: 0.4035

Epoch 00117: val_mDice did not improve from 0.40476
Epoch 118/300
 - 10s - loss: 2.1669 - acc: 0.9126 - mDice: 0.4436 - val_loss: 2.8796 - val_acc: 0.9350 - val_mDice: 0.4093

Epoch 00118: val_mDice improved from 0.40476 to 0.40933, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 119/300
 - 9s - loss: 2.1537 - acc: 0.9130 - mDice: 0.4469 - val_loss: 3.3328 - val_acc: 0.9349 - val_mDice: 0.3893

Epoch 00119: val_mDice did not improve from 0.40933
Epoch 120/300
 - 10s - loss: 2.1521 - acc: 0.9129 - mDice: 0.4475 - val_loss: 2.9113 - val_acc: 0.9363 - val_mDice: 0.4063

Epoch 00120: val_mDice did not improve from 0.40933
Epoch 121/300
 - 10s - loss: 2.1493 - acc: 0.9132 - mDice: 0.4483 - val_loss: 3.0404 - val_acc: 0.9374 - val_mDice: 0.4072

Epoch 00121: val_mDice did not improve from 0.40933
Epoch 122/300
 - 10s - loss: 2.1365 - acc: 0.9133 - mDice: 0.4516 - val_loss: 3.3448 - val_acc: 0.9349 - val_mDice: 0.3880

Epoch 00122: val_mDice did not improve from 0.40933
Epoch 123/300
 - 10s - loss: 2.1400 - acc: 0.9132 - mDice: 0.4503 - val_loss: 3.0463 - val_acc: 0.9362 - val_mDice: 0.4031

Epoch 00123: val_mDice did not improve from 0.40933
Epoch 124/300
 - 10s - loss: 2.1231 - acc: 0.9138 - mDice: 0.4536 - val_loss: 3.2009 - val_acc: 0.9355 - val_mDice: 0.3951

Epoch 00124: val_mDice did not improve from 0.40933
Epoch 125/300
 - 10s - loss: 2.1201 - acc: 0.9138 - mDice: 0.4556 - val_loss: 3.1162 - val_acc: 0.9363 - val_mDice: 0.4115

Epoch 00125: val_mDice improved from 0.40933 to 0.41153, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 126/300
 - 10s - loss: 2.1124 - acc: 0.9141 - mDice: 0.4573 - val_loss: 4.0344 - val_acc: 0.9310 - val_mDice: 0.3503

Epoch 00126: val_mDice did not improve from 0.41153
Epoch 127/300
 - 10s - loss: 2.1130 - acc: 0.9142 - mDice: 0.4572 - val_loss: 3.1925 - val_acc: 0.9363 - val_mDice: 0.3987

Epoch 00127: val_mDice did not improve from 0.41153
Epoch 128/300
 - 10s - loss: 2.0957 - acc: 0.9146 - mDice: 0.4609 - val_loss: 2.9627 - val_acc: 0.9366 - val_mDice: 0.4108

Epoch 00128: val_mDice did not improve from 0.41153
Epoch 129/300
 - 10s - loss: 2.0996 - acc: 0.9145 - mDice: 0.4597 - val_loss: 2.8400 - val_acc: 0.9365 - val_mDice: 0.4164

Epoch 00129: val_mDice improved from 0.41153 to 0.41640, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 130/300
 - 10s - loss: 2.0836 - acc: 0.9150 - mDice: 0.4632 - val_loss: 3.1802 - val_acc: 0.9364 - val_mDice: 0.4043

Epoch 00130: val_mDice did not improve from 0.41640
Epoch 131/300
 - 10s - loss: 2.0793 - acc: 0.9151 - mDice: 0.4652 - val_loss: 2.9562 - val_acc: 0.9362 - val_mDice: 0.4115

Epoch 00131: val_mDice did not improve from 0.41640
Epoch 132/300
 - 10s - loss: 2.0740 - acc: 0.9151 - mDice: 0.4654 - val_loss: 3.2794 - val_acc: 0.9350 - val_mDice: 0.3946

Epoch 00132: val_mDice did not improve from 0.41640
Epoch 133/300
 - 10s - loss: 2.0689 - acc: 0.9154 - mDice: 0.4676 - val_loss: 2.9401 - val_acc: 0.9357 - val_mDice: 0.4071

Epoch 00133: val_mDice did not improve from 0.41640
Epoch 134/300
 - 10s - loss: 2.0670 - acc: 0.9155 - mDice: 0.4675 - val_loss: 3.1610 - val_acc: 0.9365 - val_mDice: 0.4042

Epoch 00134: val_mDice did not improve from 0.41640
Epoch 135/300
 - 10s - loss: 2.0647 - acc: 0.9154 - mDice: 0.4679 - val_loss: 3.0606 - val_acc: 0.9354 - val_mDice: 0.4023

Epoch 00135: val_mDice did not improve from 0.41640
Epoch 136/300
 - 10s - loss: 2.0605 - acc: 0.9156 - mDice: 0.4692 - val_loss: 3.0582 - val_acc: 0.9348 - val_mDice: 0.4056

Epoch 00136: val_mDice did not improve from 0.41640
Epoch 137/300
 - 10s - loss: 2.0518 - acc: 0.9159 - mDice: 0.4711 - val_loss: 2.9773 - val_acc: 0.9359 - val_mDice: 0.4165

Epoch 00137: val_mDice improved from 0.41640 to 0.41655, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 138/300
 - 10s - loss: 2.0430 - acc: 0.9162 - mDice: 0.4734 - val_loss: 2.9253 - val_acc: 0.9372 - val_mDice: 0.4165

Epoch 00138: val_mDice did not improve from 0.41655
Epoch 139/300
 - 10s - loss: 2.0354 - acc: 0.9164 - mDice: 0.4744 - val_loss: 2.9942 - val_acc: 0.9361 - val_mDice: 0.4164

Epoch 00139: val_mDice did not improve from 0.41655
Epoch 140/300
 - 10s - loss: 2.0358 - acc: 0.9162 - mDice: 0.4741 - val_loss: 3.0660 - val_acc: 0.9364 - val_mDice: 0.4104

Epoch 00140: val_mDice did not improve from 0.41655
Epoch 141/300
 - 10s - loss: 2.0364 - acc: 0.9165 - mDice: 0.4751 - val_loss: 2.9320 - val_acc: 0.9367 - val_mDice: 0.4241

Epoch 00141: val_mDice improved from 0.41655 to 0.42410, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 142/300
 - 9s - loss: 2.0293 - acc: 0.9165 - mDice: 0.4769 - val_loss: 2.9012 - val_acc: 0.9352 - val_mDice: 0.4116

Epoch 00142: val_mDice did not improve from 0.42410
Epoch 143/300
 - 10s - loss: 2.0229 - acc: 0.9169 - mDice: 0.4779 - val_loss: 3.2584 - val_acc: 0.9362 - val_mDice: 0.4057

Epoch 00143: val_mDice did not improve from 0.42410
Epoch 144/300
 - 10s - loss: 2.0169 - acc: 0.9169 - mDice: 0.4787 - val_loss: 2.9006 - val_acc: 0.9384 - val_mDice: 0.4235

Epoch 00144: val_mDice did not improve from 0.42410
Epoch 145/300
 - 10s - loss: 2.0134 - acc: 0.9168 - mDice: 0.4804 - val_loss: 2.8763 - val_acc: 0.9354 - val_mDice: 0.4183

Epoch 00145: val_mDice did not improve from 0.42410
Epoch 146/300
 - 10s - loss: 2.0125 - acc: 0.9171 - mDice: 0.4811 - val_loss: 3.1768 - val_acc: 0.9367 - val_mDice: 0.4109

Epoch 00146: val_mDice did not improve from 0.42410
Epoch 147/300
 - 10s - loss: 2.0121 - acc: 0.9171 - mDice: 0.4811 - val_loss: 3.1122 - val_acc: 0.9362 - val_mDice: 0.4142

Epoch 00147: val_mDice did not improve from 0.42410
Epoch 148/300
 - 10s - loss: 2.0053 - acc: 0.9172 - mDice: 0.4819 - val_loss: 2.9939 - val_acc: 0.9353 - val_mDice: 0.4160

Epoch 00148: val_mDice did not improve from 0.42410
Epoch 149/300
 - 10s - loss: 2.0006 - acc: 0.9172 - mDice: 0.4825 - val_loss: 3.0797 - val_acc: 0.9377 - val_mDice: 0.4221

Epoch 00149: val_mDice did not improve from 0.42410
Epoch 150/300
 - 10s - loss: 1.9924 - acc: 0.9175 - mDice: 0.4848 - val_loss: 3.3794 - val_acc: 0.9357 - val_mDice: 0.4034

Epoch 00150: val_mDice did not improve from 0.42410
Epoch 151/300
 - 10s - loss: 1.9923 - acc: 0.9176 - mDice: 0.4862 - val_loss: 3.0723 - val_acc: 0.9362 - val_mDice: 0.4116

Epoch 00151: val_mDice did not improve from 0.42410
Epoch 152/300
 - 10s - loss: 1.9891 - acc: 0.9177 - mDice: 0.4860 - val_loss: 3.1669 - val_acc: 0.9369 - val_mDice: 0.4080

Epoch 00152: val_mDice did not improve from 0.42410
Epoch 153/300
 - 10s - loss: 1.9847 - acc: 0.9181 - mDice: 0.4879 - val_loss: 3.1142 - val_acc: 0.9377 - val_mDice: 0.4255

Epoch 00153: val_mDice improved from 0.42410 to 0.42553, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 154/300
 - 10s - loss: 1.9784 - acc: 0.9182 - mDice: 0.4886 - val_loss: 3.0775 - val_acc: 0.9378 - val_mDice: 0.4255

Epoch 00154: val_mDice did not improve from 0.42553
Epoch 155/300
 - 10s - loss: 1.9795 - acc: 0.9181 - mDice: 0.4885 - val_loss: 3.4279 - val_acc: 0.9366 - val_mDice: 0.4094

Epoch 00155: val_mDice did not improve from 0.42553
Epoch 156/300
 - 10s - loss: 1.9732 - acc: 0.9183 - mDice: 0.4895 - val_loss: 3.1112 - val_acc: 0.9356 - val_mDice: 0.4095

Epoch 00156: val_mDice did not improve from 0.42553
Epoch 157/300
 - 10s - loss: 1.9693 - acc: 0.9184 - mDice: 0.4907 - val_loss: 3.0831 - val_acc: 0.9350 - val_mDice: 0.4159

Epoch 00157: val_mDice did not improve from 0.42553
Epoch 158/300
 - 10s - loss: 1.9704 - acc: 0.9184 - mDice: 0.4910 - val_loss: 2.8938 - val_acc: 0.9375 - val_mDice: 0.4279

Epoch 00158: val_mDice improved from 0.42553 to 0.42787, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 159/300
 - 10s - loss: 1.9569 - acc: 0.9188 - mDice: 0.4938 - val_loss: 3.0061 - val_acc: 0.9363 - val_mDice: 0.4186

Epoch 00159: val_mDice did not improve from 0.42787
Epoch 160/300
 - 10s - loss: 1.9607 - acc: 0.9188 - mDice: 0.4937 - val_loss: 3.0127 - val_acc: 0.9357 - val_mDice: 0.4217

Epoch 00160: val_mDice did not improve from 0.42787
Epoch 161/300
 - 9s - loss: 1.9598 - acc: 0.9187 - mDice: 0.4931 - val_loss: 2.8950 - val_acc: 0.9380 - val_mDice: 0.4278

Epoch 00161: val_mDice did not improve from 0.42787
Epoch 162/300
 - 10s - loss: 1.9553 - acc: 0.9189 - mDice: 0.4941 - val_loss: 3.4276 - val_acc: 0.9368 - val_mDice: 0.4103

Epoch 00162: val_mDice did not improve from 0.42787
Epoch 163/300
 - 10s - loss: 1.9539 - acc: 0.9190 - mDice: 0.4944 - val_loss: 3.3501 - val_acc: 0.9368 - val_mDice: 0.4074

Epoch 00163: val_mDice did not improve from 0.42787
Epoch 164/300
 - 10s - loss: 1.9485 - acc: 0.9192 - mDice: 0.4960 - val_loss: 3.1657 - val_acc: 0.9372 - val_mDice: 0.4159

Epoch 00164: val_mDice did not improve from 0.42787
Epoch 165/300
 - 10s - loss: 1.9505 - acc: 0.9190 - mDice: 0.4953 - val_loss: 2.9314 - val_acc: 0.9368 - val_mDice: 0.4241

Epoch 00165: val_mDice did not improve from 0.42787
Epoch 166/300
 - 10s - loss: 1.9338 - acc: 0.9195 - mDice: 0.4989 - val_loss: 2.8484 - val_acc: 0.9359 - val_mDice: 0.4214

Epoch 00166: val_mDice did not improve from 0.42787
Epoch 167/300
 - 10s - loss: 1.9350 - acc: 0.9198 - mDice: 0.4994 - val_loss: 2.8575 - val_acc: 0.9376 - val_mDice: 0.4286

Epoch 00167: val_mDice improved from 0.42787 to 0.42857, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 168/300
 - 10s - loss: 1.9339 - acc: 0.9197 - mDice: 0.4995 - val_loss: 3.1409 - val_acc: 0.9376 - val_mDice: 0.4210

Epoch 00168: val_mDice did not improve from 0.42857
Epoch 169/300
 - 10s - loss: 1.9266 - acc: 0.9200 - mDice: 0.5007 - val_loss: 3.2479 - val_acc: 0.9383 - val_mDice: 0.4194

Epoch 00169: val_mDice did not improve from 0.42857
Epoch 170/300
 - 10s - loss: 1.9355 - acc: 0.9199 - mDice: 0.4994 - val_loss: 2.9919 - val_acc: 0.9372 - val_mDice: 0.4202

Epoch 00170: val_mDice did not improve from 0.42857
Epoch 171/300
 - 10s - loss: 1.9285 - acc: 0.9198 - mDice: 0.5006 - val_loss: 2.9763 - val_acc: 0.9369 - val_mDice: 0.4265

Epoch 00171: val_mDice did not improve from 0.42857
Epoch 172/300
 - 10s - loss: 1.9269 - acc: 0.9200 - mDice: 0.5015 - val_loss: 3.1255 - val_acc: 0.9369 - val_mDice: 0.4190

Epoch 00172: val_mDice did not improve from 0.42857
Epoch 173/300
 - 10s - loss: 1.9255 - acc: 0.9202 - mDice: 0.5015 - val_loss: 2.8647 - val_acc: 0.9372 - val_mDice: 0.4271

Epoch 00173: val_mDice did not improve from 0.42857
Epoch 174/300
 - 10s - loss: 1.9231 - acc: 0.9202 - mDice: 0.5019 - val_loss: 2.8784 - val_acc: 0.9372 - val_mDice: 0.4295

Epoch 00174: val_mDice improved from 0.42857 to 0.42950, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 175/300
 - 10s - loss: 1.9152 - acc: 0.9205 - mDice: 0.5036 - val_loss: 2.9547 - val_acc: 0.9367 - val_mDice: 0.4317

Epoch 00175: val_mDice improved from 0.42950 to 0.43174, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 176/300
 - 10s - loss: 1.9190 - acc: 0.9204 - mDice: 0.5028 - val_loss: 3.2610 - val_acc: 0.9375 - val_mDice: 0.4178

Epoch 00176: val_mDice did not improve from 0.43174
Epoch 177/300
 - 9s - loss: 1.9189 - acc: 0.9204 - mDice: 0.5030 - val_loss: 2.8851 - val_acc: 0.9361 - val_mDice: 0.4190

Epoch 00177: val_mDice did not improve from 0.43174
Epoch 178/300
 - 10s - loss: 1.9094 - acc: 0.9208 - mDice: 0.5053 - val_loss: 2.8825 - val_acc: 0.9352 - val_mDice: 0.4201

Epoch 00178: val_mDice did not improve from 0.43174
Epoch 179/300
 - 10s - loss: 1.9027 - acc: 0.9208 - mDice: 0.5069 - val_loss: 2.9407 - val_acc: 0.9378 - val_mDice: 0.4243

Epoch 00179: val_mDice did not improve from 0.43174
Epoch 180/300
 - 10s - loss: 1.9004 - acc: 0.9211 - mDice: 0.5077 - val_loss: 2.8266 - val_acc: 0.9364 - val_mDice: 0.4219

Epoch 00180: val_mDice did not improve from 0.43174
Epoch 181/300
 - 10s - loss: 1.8977 - acc: 0.9213 - mDice: 0.5084 - val_loss: 3.0150 - val_acc: 0.9377 - val_mDice: 0.4249

Epoch 00181: val_mDice did not improve from 0.43174
Epoch 182/300
 - 10s - loss: 1.8956 - acc: 0.9211 - mDice: 0.5082 - val_loss: 2.7250 - val_acc: 0.9395 - val_mDice: 0.4366

Epoch 00182: val_mDice improved from 0.43174 to 0.43660, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 183/300
 - 10s - loss: 1.8935 - acc: 0.9213 - mDice: 0.5091 - val_loss: 3.0652 - val_acc: 0.9385 - val_mDice: 0.4280

Epoch 00183: val_mDice did not improve from 0.43660
Epoch 184/300
 - 10s - loss: 1.8926 - acc: 0.9213 - mDice: 0.5099 - val_loss: 3.4247 - val_acc: 0.9367 - val_mDice: 0.4075

Epoch 00184: val_mDice did not improve from 0.43660
Epoch 185/300
 - 10s - loss: 1.8907 - acc: 0.9215 - mDice: 0.5093 - val_loss: 2.8644 - val_acc: 0.9395 - val_mDice: 0.4376

Epoch 00185: val_mDice improved from 0.43660 to 0.43761, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 186/300
 - 10s - loss: 1.8919 - acc: 0.9215 - mDice: 0.5099 - val_loss: 3.2039 - val_acc: 0.9375 - val_mDice: 0.4181

Epoch 00186: val_mDice did not improve from 0.43761
Epoch 187/300
 - 10s - loss: 1.8847 - acc: 0.9216 - mDice: 0.5114 - val_loss: 3.2172 - val_acc: 0.9385 - val_mDice: 0.4229

Epoch 00187: val_mDice did not improve from 0.43761
Epoch 188/300
 - 10s - loss: 1.8873 - acc: 0.9217 - mDice: 0.5110 - val_loss: 3.1169 - val_acc: 0.9377 - val_mDice: 0.4247

Epoch 00188: val_mDice did not improve from 0.43761
Epoch 189/300
 - 10s - loss: 1.8814 - acc: 0.9218 - mDice: 0.5121 - val_loss: 2.9721 - val_acc: 0.9370 - val_mDice: 0.4283

Epoch 00189: val_mDice did not improve from 0.43761
Epoch 190/300
 - 10s - loss: 1.8789 - acc: 0.9218 - mDice: 0.5126 - val_loss: 3.0684 - val_acc: 0.9380 - val_mDice: 0.4268

Epoch 00190: val_mDice did not improve from 0.43761
Epoch 191/300
 - 10s - loss: 1.8808 - acc: 0.9219 - mDice: 0.5126 - val_loss: 2.8386 - val_acc: 0.9388 - val_mDice: 0.4333

Epoch 00191: val_mDice did not improve from 0.43761
Epoch 192/300
 - 9s - loss: 1.8745 - acc: 0.9221 - mDice: 0.5136 - val_loss: 3.0173 - val_acc: 0.9387 - val_mDice: 0.4279

Epoch 00192: val_mDice did not improve from 0.43761
Epoch 193/300
 - 10s - loss: 1.8790 - acc: 0.9222 - mDice: 0.5132 - val_loss: 3.1566 - val_acc: 0.9373 - val_mDice: 0.4232

Epoch 00193: val_mDice did not improve from 0.43761
Epoch 194/300
 - 10s - loss: 1.8764 - acc: 0.9221 - mDice: 0.5135 - val_loss: 2.9562 - val_acc: 0.9394 - val_mDice: 0.4323

Epoch 00194: val_mDice did not improve from 0.43761
Epoch 195/300
 - 10s - loss: 1.8760 - acc: 0.9220 - mDice: 0.5137 - val_loss: 3.4630 - val_acc: 0.9371 - val_mDice: 0.4033

Epoch 00195: val_mDice did not improve from 0.43761
Epoch 196/300
 - 10s - loss: 1.8711 - acc: 0.9222 - mDice: 0.5145 - val_loss: 2.9448 - val_acc: 0.9375 - val_mDice: 0.4267

Epoch 00196: val_mDice did not improve from 0.43761
Epoch 197/300
 - 10s - loss: 1.8714 - acc: 0.9221 - mDice: 0.5148 - val_loss: 2.9505 - val_acc: 0.9393 - val_mDice: 0.4287

Epoch 00197: val_mDice did not improve from 0.43761
Epoch 198/300
 - 9s - loss: 1.8644 - acc: 0.9225 - mDice: 0.5161 - val_loss: 3.1779 - val_acc: 0.9387 - val_mDice: 0.4229

Epoch 00198: val_mDice did not improve from 0.43761
Epoch 199/300
 - 10s - loss: 1.8637 - acc: 0.9224 - mDice: 0.5167 - val_loss: 2.9877 - val_acc: 0.9388 - val_mDice: 0.4328

Epoch 00199: val_mDice did not improve from 0.43761
Epoch 200/300
 - 10s - loss: 1.8620 - acc: 0.9226 - mDice: 0.5174 - val_loss: 3.3856 - val_acc: 0.9362 - val_mDice: 0.4048

Epoch 00200: val_mDice did not improve from 0.43761
Epoch 201/300
 - 10s - loss: 1.8635 - acc: 0.9226 - mDice: 0.5162 - val_loss: 3.0709 - val_acc: 0.9381 - val_mDice: 0.4285

Epoch 00201: val_mDice did not improve from 0.43761
Epoch 202/300
 - 10s - loss: 1.8580 - acc: 0.9225 - mDice: 0.5178 - val_loss: 3.2548 - val_acc: 0.9377 - val_mDice: 0.4162

Epoch 00202: val_mDice did not improve from 0.43761
Epoch 203/300
 - 10s - loss: 1.8538 - acc: 0.9228 - mDice: 0.5192 - val_loss: 3.0808 - val_acc: 0.9394 - val_mDice: 0.4295

Epoch 00203: val_mDice did not improve from 0.43761
Epoch 204/300
 - 10s - loss: 1.8563 - acc: 0.9228 - mDice: 0.5185 - val_loss: 3.4071 - val_acc: 0.9383 - val_mDice: 0.4182

Epoch 00204: val_mDice did not improve from 0.43761
Epoch 205/300
 - 10s - loss: 1.8595 - acc: 0.9228 - mDice: 0.5179 - val_loss: 3.2574 - val_acc: 0.9381 - val_mDice: 0.4198

Epoch 00205: val_mDice did not improve from 0.43761
Epoch 206/300
 - 10s - loss: 1.8630 - acc: 0.9227 - mDice: 0.5177 - val_loss: 3.1975 - val_acc: 0.9393 - val_mDice: 0.4280

Epoch 00206: val_mDice did not improve from 0.43761
Epoch 207/300
 - 10s - loss: 1.8496 - acc: 0.9234 - mDice: 0.5203 - val_loss: 3.0880 - val_acc: 0.9390 - val_mDice: 0.4249

Epoch 00207: val_mDice did not improve from 0.43761
Epoch 208/300
 - 10s - loss: 1.8497 - acc: 0.9231 - mDice: 0.5200 - val_loss: 3.3154 - val_acc: 0.9389 - val_mDice: 0.4194

Epoch 00208: val_mDice did not improve from 0.43761
Epoch 209/300
 - 10s - loss: 1.8434 - acc: 0.9232 - mDice: 0.5218 - val_loss: 2.9523 - val_acc: 0.9400 - val_mDice: 0.4333

Epoch 00209: val_mDice did not improve from 0.43761
Epoch 210/300
 - 10s - loss: 1.8446 - acc: 0.9233 - mDice: 0.5215 - val_loss: 3.2127 - val_acc: 0.9396 - val_mDice: 0.4260

Epoch 00210: val_mDice did not improve from 0.43761
Epoch 211/300
 - 10s - loss: 1.8415 - acc: 0.9234 - mDice: 0.5218 - val_loss: 3.1060 - val_acc: 0.9398 - val_mDice: 0.4300

Epoch 00211: val_mDice did not improve from 0.43761
Epoch 212/300
 - 10s - loss: 1.8398 - acc: 0.9235 - mDice: 0.5225 - val_loss: 3.0271 - val_acc: 0.9396 - val_mDice: 0.4326

Epoch 00212: val_mDice did not improve from 0.43761
Epoch 213/300
 - 9s - loss: 1.8383 - acc: 0.9236 - mDice: 0.5226 - val_loss: 3.3839 - val_acc: 0.9371 - val_mDice: 0.4095

Epoch 00213: val_mDice did not improve from 0.43761
Epoch 214/300
 - 10s - loss: 1.8382 - acc: 0.9235 - mDice: 0.5232 - val_loss: 3.1127 - val_acc: 0.9400 - val_mDice: 0.4282

Epoch 00214: val_mDice did not improve from 0.43761
Epoch 215/300
 - 10s - loss: 1.8325 - acc: 0.9239 - mDice: 0.5239 - val_loss: 3.1430 - val_acc: 0.9386 - val_mDice: 0.4259

Epoch 00215: val_mDice did not improve from 0.43761
Restoring model weights from the end of the best epoch
Epoch 00215: early stopping
{'val_loss': [59.47857474145435, 17.494075917062304, 9.644453443232036, 7.635699834142413, 6.94918808589379, 6.628062281580198, 6.366253711815391, 6.19264239196976, 6.181212272495031, 5.85033106094315, 5.924150726092713, 5.81220105591984, 5.628806773395765, 5.633588897507815, 6.159075021388984, 5.7678855662899355, 5.255868366963806, 5.309561872500039, 5.240683068388274, 5.006830036063635, 5.058661127196891, 4.702938479593112, 4.3117160356293125, 4.2745972732525495, 5.021838077344, 4.106751644274309, 4.117619177060468, 4.111094112641045, 4.556475898472681, 4.8352710617085295, 4.671301595600588, 4.025059877983516, 3.743317886477425, 3.5922933413336673, 3.8283955403825356, 3.486851712067922, 3.4480458266500915, 3.4448970629877986, 3.559573064247767, 4.159936658816323, 3.919312948183644, 3.419594920462086, 4.154903641724515, 3.3868213365564035, 3.5073488684637204, 3.8308077160535112, 3.0550531817688826, 3.107779034191654, 3.1533340905748664, 3.9761058086795464, 3.0867867366011654, 3.018526765739634, 3.0237391340945448, 3.1376611848494838, 3.106427008907, 2.976401461997912, 2.9406151428286518, 2.9157896826842, 3.022778682588112, 3.365874993082668, 2.962407126135769, 3.03980191663972, 3.0243065074263584, 3.2175830747222616, 3.3511414962953756, 2.9935694998573688, 2.8915984458511783, 3.444438469773602, 2.9378778326014676, 3.333005276552978, 2.878165009564587, 2.98866762034595, 2.9094811311612525, 2.9078506826467456, 2.9548904229665087, 3.108566653293868, 2.95946844303537, 2.860794884950987, 2.9309527229606394, 3.0783006441114202, 3.31837590154083, 3.241878920678227, 2.875217683524603, 2.994794336945883, 2.8716433673564876, 2.995827985395278, 3.316962572435538, 2.9275095645959177, 2.8282491030792394, 3.095847148004742, 2.88650555261189, 2.9530521994900134, 2.812777818091923, 2.830832741888506, 2.951678165589415, 2.9303511934177506, 2.8607490453869104, 3.155533722025298, 2.94614603186381, 2.886608673969195, 2.880213023651214, 2.793437096512034, 2.907376339925187, 2.809976858264279, 2.9484367849127877, 3.041507224197544, 3.074396377828504, 3.3737074454714144, 3.017249089266573, 2.879381575532967, 2.938854282012298, 2.8320074756408022, 2.893664198883233, 2.912832445509377, 2.9729776324792985, 3.154157168143207, 2.914200641880078, 2.8796414632704996, 3.3328280685957345, 2.9113257877706062, 3.040422466450504, 3.344789369187007, 3.0462607492116236, 3.2008817191900953, 3.116169263630928, 4.034366419206241, 3.1924833704362667, 2.962699518494663, 2.840018893636408, 3.180164348156679, 2.956201186714073, 3.279381412303164, 2.9401457847229073, 3.161022282471614, 3.060594423407955, 3.0582221238652156, 2.9773377029313925, 2.925323823866035, 2.994207721669227, 3.0660149320915697, 2.9320129638182975, 2.9011744182734263, 3.258369969514509, 2.9005805425168503, 2.87627062166021, 3.176847831966976, 3.112208079413644, 2.99391982433874, 3.079731969295868, 3.379392136398348, 3.072314545290456, 3.1668849760843885, 3.114188993194451, 3.0775377750840214, 3.4279125156324532, 3.1111898504729782, 3.0831218509535705, 2.8937510114074465, 3.006100646863204, 3.01267852651931, 2.8950042271365724, 3.427562392405456, 3.3500537717980996, 3.1657385044243362, 2.931362585962883, 2.8483667641966823, 2.8575118302057185, 3.1409003593116287, 3.2478928734947528, 2.9919220682322267, 2.9763404691503164, 3.1255250423259677, 2.864683664802994, 2.8783506348374344, 2.9546579400166157, 3.2610022359793738, 2.8851267082971477, 2.88251274872926, 2.940735924013314, 2.826560847682967, 3.0150082262144204, 2.725033271569936, 3.0651722216446484, 3.424708475159215, 2.8644072331842922, 3.2038938201564764, 3.217154938328479, 3.1169197467776635, 2.9721140553731296, 3.0684008811866597, 2.838639511238961, 3.017346254017736, 3.1566161356555917, 2.956185634292307, 3.463004557016705, 2.94480327184179, 2.950499891835664, 3.1778824318289045, 2.9876951288786673, 3.3856333874476454, 3.0709071468473192, 3.2548050558017123, 3.080825244209596, 3.4071272109147337, 3.25742687859262, 3.1975334039889276, 3.0880259772807004, 3.3154230407394825, 2.9522590854293886, 3.2127069366563643, 3.106010469874101, 3.0271227674647454, 3.383885903505697, 3.112685085274279, 3.1430164198169397], 'val_acc': [0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9048031256312415, 0.9048282731147039, 0.904784790107182, 0.9052518265587943, 0.9052220611345201, 0.9052243317876544, 0.9049221646218073, 0.904887820993151, 0.9048946897188822, 0.9049633769761949, 0.905505949542636, 0.9087339611280532, 0.905872262659527, 0.9087591426713126, 0.909743572984423, 0.9121794615473066, 0.9117170089767093, 0.9104189446994236, 0.908543952873775, 0.9139171044031779, 0.90821886914117, 0.913633215995062, 0.9155540494691758, 0.9131959932191032, 0.9202907454399836, 0.918738529795692, 0.9212385274115062, 0.9158814010165987, 0.9211080641973586, 0.9216415087381998, 0.9217971847170875, 0.922458773567563, 0.922328321706681, 0.921783447265625, 0.922964768750327, 0.9226442263239906, 0.9249107326780047, 0.92287544409434, 0.9247092746552967, 0.9266895793733143, 0.9270421323322114, 0.9259592550141471, 0.9237820562862215, 0.9278342610313779, 0.9279624394008091, 0.9254258473714193, 0.9264262971423921, 0.9288232383273897, 0.9282623699733189, 0.9301625490188599, 0.929132342338562, 0.9309317725045341, 0.9304647190230233, 0.9310828759556725, 0.9248878161112467, 0.9319345496949696, 0.9317605410303388, 0.9317880045799982, 0.93065478404363, 0.9317880216098967, 0.9307944036665416, 0.9331295859246027, 0.9266712296576727, 0.9296611774535406, 0.9297710571970258, 0.9316506442569551, 0.9324175573530651, 0.9328869041942415, 0.9319047644024804, 0.9324061331294832, 0.9310164735430763, 0.931991750285739, 0.9343818511281695, 0.9337706083343142, 0.9331868234134856, 0.93385758854094, 0.9355838128498623, 0.9317330576124645, 0.933099817662012, 0.9346039351962862, 0.9343841728709993, 0.9316643533252534, 0.9348191562153044, 0.9350801053501311, 0.9345993541535877, 0.9337179405348641, 0.9349748378708249, 0.9343269353821164, 0.9355837645984831, 0.9321634684290204, 0.934345231169746, 0.9340155578794933, 0.9370100895563761, 0.9353342226573399, 0.932419879095895, 0.9350229132743109, 0.9348717899549575, 0.9362729021481105, 0.937355773789542, 0.9348694994336083, 0.9361538262594313, 0.9355219517435346, 0.9362591817265465, 0.930954669203077, 0.9362912121273222, 0.9365910944484529, 0.9364675027983529, 0.9363667482421512, 0.9361607148533776, 0.9349999881926037, 0.9357120110875085, 0.9365087066377912, 0.9354303933325268, 0.9347939349356151, 0.9359478042239234, 0.9372344442776271, 0.9361355247951689, 0.9363873742875599, 0.9366803963979086, 0.9352358267420814, 0.9362339576085409, 0.9384294918605259, 0.9353823179290408, 0.936668940952846, 0.9361858793667385, 0.9352815747261047, 0.9377243376913524, 0.9356891087123326, 0.9361996480396816, 0.9368704188437689, 0.9376785641624814, 0.9377907571338472, 0.9365819692611694, 0.9355837731134324, 0.9350091304097857, 0.9375229137284415, 0.9363049524171012, 0.9356868267059326, 0.9379555838448661, 0.936785706451961, 0.9368223377636501, 0.9371657371520996, 0.9367582599322001, 0.9358905468668256, 0.9375503517332531, 0.9375709948085603, 0.9383058519590468, 0.9371657740502131, 0.9368978965850103, 0.9368635472797212, 0.9372046419552394, 0.9371588939712161, 0.9367193295842126, 0.9375251985731579, 0.9361172148159572, 0.9352403822399321, 0.9378159244855245, 0.9364125359626043, 0.9377449410302299, 0.9394528411683583, 0.9385347905613127, 0.9366987205687023, 0.9395123663402739, 0.9374747900735765, 0.9385050620351519, 0.9377014409928095, 0.9370215308098566, 0.9380174052147638, 0.9387751789320082, 0.9386767631485349, 0.9372825253577459, 0.9394230700674511, 0.9370627431642442, 0.9374587876456124, 0.9393223495710463, 0.93870419831503, 0.9388369775953747, 0.9362179636955261, 0.9380929242996943, 0.9376648267110189, 0.9394322264762152, 0.9383493548347837, 0.9381044223195031, 0.9392696817715963, 0.9390224502200172, 0.9388553301493326, 0.9399610644295102, 0.9395627521333241, 0.9398054083188375, 0.9396268441563561, 0.937110824244363, 0.9399999834242321, 0.9385805811200824], 'val_mDice': [0.012268627206573174, 0.009947887383445743, 0.009055225719099067, 0.009556479968263634, 0.010697190350453769, 0.011860910093500501, 0.012706929507354895, 0.015249259803178055, 0.01570625485674966, 0.019675541136945997, 0.018567970199953942, 0.021306754666424933, 0.025387003042158626, 0.02699425058173282, 0.018855440474691846, 0.02276012853586248, 0.04254608596896842, 0.045121821840958934, 0.0463716211240916, 0.058022879906708284, 0.06123361053566138, 0.07406970162299417, 0.08929110318422318, 0.09283111523836851, 0.07129275887494996, 0.10477805754081124, 0.10638339436125188, 0.11061198894111883, 0.09623130450823478, 0.09328099204936907, 0.10736467609448093, 0.13540890404865855, 0.15118758677549304, 0.16700277601679167, 0.16021977897201264, 0.18143151620669024, 0.1896913924387523, 0.19178473505945431, 0.2015150413804111, 0.18496994585508392, 0.19275075658446267, 0.225344669960794, 0.197013774354543, 0.23304181226662227, 0.24044108772206874, 0.23049422238199485, 0.26929085994405405, 0.26622303832499755, 0.2699534876183385, 0.24261370638296717, 0.2809463104321843, 0.28634859302214216, 0.2931217499786899, 0.28718537296212854, 0.29160097188183237, 0.3019115533679724, 0.3042956383987552, 0.308012771819319, 0.30612360650584813, 0.29847345110915957, 0.3123295637113707, 0.31593670873414903, 0.32096291120563236, 0.3087157681584358, 0.3093215666179146, 0.3260571388084264, 0.3344110528982821, 0.3131812873872973, 0.3329103248459952, 0.3247338364876452, 0.34396027826837133, 0.3405032344162464, 0.34719277767553214, 0.3524720345934232, 0.3481967182209094, 0.34334716307265417, 0.3485555317962453, 0.35961036800983404, 0.358134320804051, 0.35392039969918276, 0.3450059959931033, 0.34867516151141553, 0.36525580535332364, 0.360693126384701, 0.3632477137836672, 0.35934399405405637, 0.34506725555374507, 0.3662286317419438, 0.3696885202966985, 0.360286334973006, 0.37234820425510406, 0.3696301117362011, 0.3782916759096441, 0.37755944544360753, 0.3810114753210828, 0.3811393724310966, 0.388188822904513, 0.3704444148710796, 0.38494216721682323, 0.3795704723646243, 0.38875578627699897, 0.39336702706558363, 0.3886785361738432, 0.3889794461429119, 0.3873618415423802, 0.39151810038657414, 0.3913176838485968, 0.3736421302670524, 0.3886184538049357, 0.3892342465974036, 0.3953396612334819, 0.39364421438603175, 0.39436067392428714, 0.3905428227569376, 0.4047612448533376, 0.3889549119131906, 0.4035264741451967, 0.40932931875189144, 0.38926031795286, 0.40628789276594207, 0.40718196474370505, 0.38804288474576815, 0.40312117444617407, 0.3951234601083256, 0.41153460421732496, 0.35032385445776437, 0.3986525329805556, 0.41076804520118804, 0.4163978277217774, 0.4042676701432183, 0.41148720930020016, 0.39457522456844646, 0.40712679869362284, 0.40424992463418413, 0.4023017358212244, 0.40560488615717205, 0.41654866632251514, 0.4165211154946259, 0.4164266660809517, 0.41042752209163846, 0.42409735243944896, 0.4116424895113423, 0.4057276774020422, 0.423516537461962, 0.4182961100623721, 0.41086268708819434, 0.41422836464785395, 0.4159908032133466, 0.4221274689549491, 0.40337419350232395, 0.41158674905697507, 0.4080401353892826, 0.4255270248367673, 0.4254740902355739, 0.4094119164205733, 0.4095127445956071, 0.4158679371078809, 0.4278674592219648, 0.4186055990202086, 0.42167905451995985, 0.4278448724320957, 0.4103459804540589, 0.4073530499424253, 0.4159449314077695, 0.42406377160833, 0.42136373317667414, 0.4285702733766465, 0.42103296447367894, 0.4193976862089975, 0.4201864855630057, 0.42648514377928914, 0.4189650819060348, 0.4271183331452665, 0.4294966885021755, 0.431738182192757, 0.4177772372606255, 0.41900871268340517, 0.42012965661429225, 0.4243316909386998, 0.42188552518685657, 0.4249046789038749, 0.43659888882012593, 0.42795690716732115, 0.40754698784578414, 0.4376087990545091, 0.4181205486612661, 0.4229412433646974, 0.42466585249418304, 0.4283139313615504, 0.4267694613053685, 0.4333238846489361, 0.42790431671199347, 0.4231507194538911, 0.43233174759717213, 0.4032785670743102, 0.42670391287122456, 0.4286863732905615, 0.4228844458148593, 0.43276382166714894, 0.40478024010856944, 0.42846835936818806, 0.41618371559750467, 0.4294876873138405, 0.4182488214047182, 0.4198219045287087, 0.4279634906422524, 0.42493611574172974, 0.41937007417991046, 0.43327076360583305, 0.4260417405693304, 0.4299991051001208, 0.4326049873516673, 0.40949623339942526, 0.42821490338870455, 0.4259264621706236], 'loss': [241.42188635069166, 48.763127756275054, 22.73294844534532, 15.653900976238026, 12.560821361111518, 10.784378786088874, 9.684469809990892, 8.944124409061695, 8.407309341688066, 8.010789982634103, 7.653474634820288, 7.343842694495992, 7.0799253334860275, 6.833582541835499, 6.600048143325432, 6.386182423935981, 6.115896199786964, 5.910518463576762, 5.750049862669501, 5.58803066785381, 5.438853496390085, 5.323538272487007, 5.196578256652285, 5.050497132136409, 4.936211076066837, 4.824608905434448, 4.740642397578842, 4.620788323159895, 4.505080951760908, 4.392191997446811, 4.290628177112574, 4.187165379087567, 4.098627982351944, 4.021506173959559, 3.9519000358151315, 3.874839798802467, 3.809961772181061, 3.7310085722723056, 3.6643839007470107, 3.600418647176295, 3.555863702782138, 3.4896023513863077, 3.430695235212115, 3.3822141912187114, 3.3389499146101307, 3.2964499110726924, 3.2655136590291707, 3.2105106622101323, 3.1828597869306763, 3.1444947513652393, 3.1020889150767696, 3.0704781128901932, 3.029408993186899, 3.000505981450828, 2.9686246196359987, 2.934702272012511, 2.9110440547144067, 2.8844598703860593, 2.8635899941999834, 2.834756257700879, 2.82237760327054, 2.797976175599276, 2.776787262673504, 2.755471059116344, 2.7333407714598117, 2.7180438101670865, 2.6952288898172374, 2.681747424848419, 2.664608954165238, 2.646508151909876, 2.633657907736377, 2.610169805403916, 2.5945685759936543, 2.581894841087515, 2.5661272150899763, 2.5615058862705644, 2.5440517455506053, 2.529511716385579, 2.514612426320229, 2.49976797606735, 2.485380696128829, 2.482939422142076, 2.4731600686996824, 2.4576356884645825, 2.449792288024002, 2.431564128879226, 2.4231154151980485, 2.4139687913172456, 2.4015090904737773, 2.398383656631942, 2.383368521934533, 2.3790841322770717, 2.364741213408559, 2.3573430507702935, 2.3454061819681065, 2.3413309852167297, 2.3340112212675486, 2.3274132992137897, 2.316047907427186, 2.3052021345697846, 2.297048642085149, 2.291225622425149, 2.284234860510134, 2.2760159698598112, 2.26845819800259, 2.2658140585788855, 2.2508863296631976, 2.24356304284624, 2.2422024950899324, 2.2272267805396972, 2.2194179954477327, 2.208026609256223, 2.2019687874689025, 2.199515555735519, 2.1823967085285454, 2.184267398224921, 2.171744088228438, 2.166944416589261, 2.1536812711731876, 2.152140042536838, 2.1493181510768644, 2.136485743159523, 2.1400282404108824, 2.12309891878352, 2.1200527095693555, 2.112363439707575, 2.113036702128304, 2.095744907155027, 2.099631925044732, 2.0835884652294956, 2.079293731368068, 2.074004200260051, 2.068853927734203, 2.067044406675387, 2.0647257430396695, 2.0604670107973684, 2.0518122888241104, 2.042980272139753, 2.0353704928110763, 2.03584297109666, 2.0363657940369433, 2.029317636997318, 2.0228575433589877, 2.0168721488107253, 2.013370845665793, 2.012519761871138, 2.0121167799124304, 2.005291252759511, 2.0006136108230756, 1.992430828544752, 1.992337358648294, 1.9891308107333812, 1.984683700027966, 1.9784319123160938, 1.979526630481222, 1.973183105785722, 1.9693021687878471, 1.9703718010851108, 1.9568813097414908, 1.9606907314617141, 1.9598054854608304, 1.9552894450346352, 1.9538790063175917, 1.9484647774204529, 1.9504881571638624, 1.933812433546716, 1.9349709371622297, 1.9339444509057242, 1.9266139406079568, 1.9355130226413984, 1.9285496510543736, 1.9269425517588377, 1.925490355431884, 1.9230725387628766, 1.9151609432773138, 1.9189989043265747, 1.9189425024058702, 1.9093695997111664, 1.9026624346735483, 1.9004170075992595, 1.8976855634241083, 1.8956370270502736, 1.8934952625080135, 1.892626148854144, 1.8906769621043575, 1.8919060738164382, 1.8846793492589127, 1.8873316098298323, 1.881385854473228, 1.8789024242022743, 1.880790699670511, 1.8744666405626866, 1.8790377925902588, 1.8764396419593175, 1.875959902295079, 1.871105585595604, 1.8713702031222523, 1.8643614818577778, 1.8637122937840995, 1.8619832506937306, 1.863524105315818, 1.858028180640121, 1.8537854751640601, 1.8563036913584026, 1.8594598372317657, 1.863028996846155, 1.8495590410320797, 1.8496988460648514, 1.8433503578089325, 1.8445717585392487, 1.8415269572449393, 1.839826817753367, 1.8383210072610243, 1.8382338959287765, 1.832518311624286], 'acc': [0.7489863720982664, 0.8598802613343305, 0.8672467267924658, 0.8685717636711087, 0.868894751772431, 0.8689686721268016, 0.8690075329073091, 0.8690329732429736, 0.8690458579125008, 0.8690280396055343, 0.8689822749708208, 0.8689104620146803, 0.8687535442666141, 0.8685467379939379, 0.8683027607847184, 0.8682541686582593, 0.8682723598763367, 0.8682448079481689, 0.8682366069373952, 0.8682959961574018, 0.8684460332404218, 0.8684635919046374, 0.8685200661736829, 0.8689237190437428, 0.8691646605299138, 0.8694021288016087, 0.8696214670571423, 0.8703960815858629, 0.8707202131724757, 0.8713760898134855, 0.8720220010221315, 0.872769894536943, 0.8735455001207223, 0.8741286217143638, 0.874751738415349, 0.8757608696080045, 0.8765290641637581, 0.8775026511084213, 0.8780676534186445, 0.878944378161628, 0.8796799014201623, 0.8804301557132728, 0.8818310596751229, 0.8820043132901306, 0.8829963433161071, 0.8836094964233142, 0.8844107298555001, 0.8849256550252196, 0.8854057726566792, 0.8862195488619211, 0.8870856173585002, 0.8875074856240373, 0.8885338117397427, 0.8886385466382755, 0.889011543082529, 0.889737846397678, 0.8900835185191616, 0.8907175489144907, 0.8911405740755014, 0.892037205620724, 0.8922965015913679, 0.892898832071671, 0.8931649807263873, 0.8939912214383974, 0.89482118812443, 0.8951030493770833, 0.8958499955032426, 0.8960669754970412, 0.8966059982489363, 0.8972676955529759, 0.8978375350553359, 0.898523835701958, 0.8990259031719019, 0.8993916702173982, 0.8999221192657407, 0.9003003095532511, 0.9005329786685475, 0.9010572362571225, 0.9011743241522109, 0.9019845746053949, 0.9021421225286528, 0.902595825810779, 0.9026945131303534, 0.903495771808617, 0.9035182044022065, 0.9037593046272562, 0.9041657154790372, 0.904673131325352, 0.9046832380070492, 0.9048839939773623, 0.9053171723018107, 0.9055975943441815, 0.9059191053603595, 0.905952911915199, 0.9066488331215032, 0.90673181080786, 0.9069613532820533, 0.9072175196813484, 0.9074037505386651, 0.9075942220015863, 0.9079706229201994, 0.9079536862609611, 0.908260853218853, 0.9086948847733919, 0.9088815554318594, 0.9091132031209624, 0.9091384359952317, 0.9097266331421334, 0.910089318005823, 0.9105184125100484, 0.9107943925527517, 0.9110400118946409, 0.9115402231401208, 0.9115971799344486, 0.9118171024350089, 0.9119974720769933, 0.9124201062972637, 0.9125603157039412, 0.9129930490447258, 0.9129154009007295, 0.9131621583615376, 0.9132886772466483, 0.9131647564552643, 0.9137922003592327, 0.9137522063435491, 0.9140548528861007, 0.9142448399162072, 0.9146373011782286, 0.9145015130181843, 0.9149711131819738, 0.9150793272474034, 0.9150571531934133, 0.9153658906595772, 0.9155481157101532, 0.9153992394870569, 0.9156065307556146, 0.915861839443267, 0.916170071792529, 0.9163559578001464, 0.9161952593686472, 0.9164984628072658, 0.9164717913202544, 0.9169218349783136, 0.9169295990147617, 0.9168214510579732, 0.9171354079811886, 0.9171455139044693, 0.9172487436104814, 0.9172102076965466, 0.9175273617821115, 0.9176453052744692, 0.9176860183149904, 0.9180860351063692, 0.9182330345243348, 0.9181313371069978, 0.9183485463953946, 0.9184428123534243, 0.9183842292727361, 0.9188302440498062, 0.9188078837356104, 0.9186999028602822, 0.9188914411394771, 0.9190324447001936, 0.9192460638170921, 0.9190283162942713, 0.9195365702942752, 0.91979894313781, 0.9196610240456409, 0.9199957204435013, 0.9198668824010716, 0.919782395477306, 0.9200011631622925, 0.9201858687865061, 0.9201996282435392, 0.9205132379613677, 0.9203867448164544, 0.9204024749958656, 0.9207605301173225, 0.9208494135012901, 0.9210979122175103, 0.9213304179859731, 0.921113040685332, 0.9213318973578583, 0.9213241349416637, 0.9214524841386733, 0.9215163220117288, 0.9216316286391874, 0.9216815877539909, 0.9217765196362097, 0.9217995998370802, 0.9219306842288416, 0.9220820878405596, 0.922167870128272, 0.9221020170850333, 0.9220235554129029, 0.9221773011757501, 0.9221382539511784, 0.9225291888884366, 0.9224216942385439, 0.9226145978743386, 0.9226384412835553, 0.9225310861478312, 0.9228440927521488, 0.9227900770971351, 0.9228262486475292, 0.9226780643301339, 0.9233630270955189, 0.9231462047489387, 0.9232197069285208, 0.9233491241368849, 0.923432195464701, 0.923510655378684, 0.9235619569826797, 0.9235302794294502, 0.9238888643967845], 'mDice': [0.01646519822582203, 0.012850789148206079, 0.011245010136992044, 0.011824288213898825, 0.01365078887477566, 0.01603104648057674, 0.018027923733329232, 0.02024583016556882, 0.022087919352536994, 0.023623745779664685, 0.025871510942239787, 0.028238373688085128, 0.030994576183496953, 0.033760293503045026, 0.03706215747262394, 0.04031894054369405, 0.047168042945975576, 0.05412695483048343, 0.05951734569698647, 0.0648312515991948, 0.07101841432087128, 0.07553237293677591, 0.0807504146983795, 0.0874665210891464, 0.09317178615708789, 0.0987751647479529, 0.10385350739223916, 0.11164758614987474, 0.11948464088341577, 0.1278800770643246, 0.13634080433916765, 0.14504331897880293, 0.15285898566085157, 0.160061683605603, 0.16639403575115266, 0.1743555324159653, 0.18102574588614942, 0.18932576555380223, 0.1964910911392196, 0.20491427033191612, 0.2103282181265636, 0.2180958910842886, 0.22576451585693627, 0.2326690545339724, 0.23852054209396148, 0.24413241715198633, 0.24969912126893043, 0.25639299093435924, 0.26134834975343185, 0.26613166009573114, 0.2733563123375367, 0.277614236257199, 0.2841068500725088, 0.28748330628455937, 0.29237931987465704, 0.2979251663763445, 0.30115718929000457, 0.30590930579369163, 0.3086908670411258, 0.31336544625028745, 0.3158687780318013, 0.3195411626006083, 0.3227017966681003, 0.32678618176791946, 0.3310389392105214, 0.3333863731820574, 0.3378257633910281, 0.33938305715685385, 0.34301937778423075, 0.34533186052630166, 0.34883532085491326, 0.3521510397902982, 0.3556018464133669, 0.35829165473893493, 0.3608220574688953, 0.3622046744568352, 0.36515804034150906, 0.3688588964759855, 0.37067298658647496, 0.37438646763281624, 0.3762953868256108, 0.37755034282208866, 0.37895397564131605, 0.3823410592167415, 0.3836272528848598, 0.387213796773627, 0.38887493965918557, 0.3904553535748981, 0.3930291427406751, 0.39336032912362073, 0.3963763858855104, 0.39776823738762607, 0.39982518572694237, 0.4017107152676844, 0.4044196532081496, 0.4049832780558364, 0.406451429472922, 0.4082473191321873, 0.41021021506518374, 0.4114410745021992, 0.41431903900589034, 0.4150900365944563, 0.41736577212500253, 0.41852490346234544, 0.42036228072948945, 0.42082104052976993, 0.42380364171705287, 0.42610635928319834, 0.4268365403188776, 0.429944593678418, 0.4315735608919613, 0.43478139864357307, 0.435348233879934, 0.4363913093489766, 0.4395168877783276, 0.43951554882841254, 0.44250311423685595, 0.4436127114024768, 0.44685249604860755, 0.4474512717846572, 0.44830590607758225, 0.4515570424093501, 0.45033921992845427, 0.45355362975576696, 0.45560734327620384, 0.457257683023865, 0.45722613864168393, 0.4609182866432774, 0.45968937648613606, 0.4632465858920958, 0.4651750526102793, 0.46537589518946215, 0.4675652378897504, 0.46753989107488825, 0.467924214085104, 0.46919040325889017, 0.47107536590051075, 0.47336932861646386, 0.47435109057889774, 0.4740882146466507, 0.47506851395683664, 0.47693994479853224, 0.4778770869904822, 0.47868540910068497, 0.48042969253680506, 0.48112027982261707, 0.4810926601080989, 0.4819014168254484, 0.4824501698833452, 0.4848113938439253, 0.48617485877473343, 0.48601669479891907, 0.4879492685035035, 0.488615053569234, 0.48852956081852417, 0.48946387957510795, 0.49072670759046055, 0.49098246197422873, 0.4938479554843921, 0.49373501104864514, 0.4931237217730494, 0.49413627091919454, 0.49439834039611624, 0.49595143973930783, 0.49534142816000093, 0.49886432166547245, 0.49935479000489424, 0.4995237976249436, 0.5007366572291446, 0.4994257360334821, 0.5005683300581102, 0.5015307506293214, 0.5015098707039783, 0.5019191717719995, 0.5035845648755176, 0.5028273852110047, 0.5029768555098149, 0.5052798220721517, 0.5069333199294564, 0.507698613358114, 0.5083673989954357, 0.5082176059386807, 0.5090720944605008, 0.5098871981991907, 0.5093441180050292, 0.5099053971910376, 0.5114352261374308, 0.5110278934188631, 0.5121357184121069, 0.5125741621812276, 0.5126363334339525, 0.513649513594144, 0.5132365431747893, 0.5134612353109592, 0.5137063612799114, 0.5145213368864263, 0.5148348209714071, 0.5161087287053773, 0.5166777311112349, 0.5173986140808803, 0.5162330160072963, 0.5178338796825384, 0.5192260315865479, 0.5185347904767378, 0.517936051454943, 0.5176709629516278, 0.5202615778869532, 0.5199501111324936, 0.5217533660849694, 0.5215043521855914, 0.5217948562266488, 0.5224783093409431, 0.5226419323759776, 0.5231793206253331, 0.5239226789701552]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.42s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.14s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.95s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:54,  1.46s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:07,  1.51s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:09,  1.52s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:35,  1.62s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<07:16,  1.56s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:44,  1.66s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<08:13,  1.78s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<08:23,  1.82s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<08:05,  1.76s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:22,  1.83s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:35,  1.88s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:47,  1.93s/it]predicting train subjects:   5%|▍         | 13/285 [00:23<08:48,  1.94s/it]predicting train subjects:   5%|▍         | 14/285 [00:25<08:59,  1.99s/it]predicting train subjects:   5%|▌         | 15/285 [00:27<09:08,  2.03s/it]predicting train subjects:   6%|▌         | 16/285 [00:29<09:06,  2.03s/it]predicting train subjects:   6%|▌         | 17/285 [00:31<08:54,  1.99s/it]predicting train subjects:   6%|▋         | 18/285 [00:33<08:59,  2.02s/it]predicting train subjects:   7%|▋         | 19/285 [00:35<08:48,  1.99s/it]predicting train subjects:   7%|▋         | 20/285 [00:37<08:49,  2.00s/it]predicting train subjects:   7%|▋         | 21/285 [00:39<08:44,  1.99s/it]predicting train subjects:   8%|▊         | 22/285 [00:41<08:50,  2.02s/it]predicting train subjects:   8%|▊         | 23/285 [00:43<08:55,  2.04s/it]predicting train subjects:   8%|▊         | 24/285 [00:45<08:58,  2.06s/it]predicting train subjects:   9%|▉         | 25/285 [00:47<09:03,  2.09s/it]predicting train subjects:   9%|▉         | 26/285 [00:49<08:49,  2.04s/it]predicting train subjects:   9%|▉         | 27/285 [00:52<09:06,  2.12s/it]predicting train subjects:  10%|▉         | 28/285 [00:54<08:47,  2.05s/it]predicting train subjects:  10%|█         | 29/285 [00:56<08:32,  2.00s/it]predicting train subjects:  11%|█         | 30/285 [00:57<08:26,  1.98s/it]predicting train subjects:  11%|█         | 31/285 [00:59<08:17,  1.96s/it]predicting train subjects:  11%|█         | 32/285 [01:01<08:18,  1.97s/it]predicting train subjects:  12%|█▏        | 33/285 [01:03<08:14,  1.96s/it]predicting train subjects:  12%|█▏        | 34/285 [01:05<08:03,  1.93s/it]predicting train subjects:  12%|█▏        | 35/285 [01:07<08:04,  1.94s/it]predicting train subjects:  13%|█▎        | 36/285 [01:09<07:57,  1.92s/it]predicting train subjects:  13%|█▎        | 37/285 [01:11<07:50,  1.90s/it]predicting train subjects:  13%|█▎        | 38/285 [01:13<07:45,  1.89s/it]predicting train subjects:  14%|█▎        | 39/285 [01:15<07:52,  1.92s/it]predicting train subjects:  14%|█▍        | 40/285 [01:17<07:53,  1.93s/it]predicting train subjects:  14%|█▍        | 41/285 [01:19<07:53,  1.94s/it]predicting train subjects:  15%|█▍        | 42/285 [01:20<07:42,  1.90s/it]predicting train subjects:  15%|█▌        | 43/285 [01:22<07:33,  1.88s/it]predicting train subjects:  15%|█▌        | 44/285 [01:24<07:35,  1.89s/it]predicting train subjects:  16%|█▌        | 45/285 [01:26<07:32,  1.88s/it]predicting train subjects:  16%|█▌        | 46/285 [01:28<07:12,  1.81s/it]predicting train subjects:  16%|█▋        | 47/285 [01:29<06:57,  1.76s/it]predicting train subjects:  17%|█▋        | 48/285 [01:31<06:37,  1.68s/it]predicting train subjects:  17%|█▋        | 49/285 [01:32<06:31,  1.66s/it]predicting train subjects:  18%|█▊        | 50/285 [01:34<06:24,  1.64s/it]predicting train subjects:  18%|█▊        | 51/285 [01:36<06:21,  1.63s/it]predicting train subjects:  18%|█▊        | 52/285 [01:37<06:18,  1.63s/it]predicting train subjects:  19%|█▊        | 53/285 [01:39<06:17,  1.63s/it]predicting train subjects:  19%|█▉        | 54/285 [01:40<06:12,  1.61s/it]predicting train subjects:  19%|█▉        | 55/285 [01:42<06:04,  1.59s/it]predicting train subjects:  20%|█▉        | 56/285 [01:44<06:09,  1.61s/it]predicting train subjects:  20%|██        | 57/285 [01:45<06:16,  1.65s/it]predicting train subjects:  20%|██        | 58/285 [01:47<06:20,  1.68s/it]predicting train subjects:  21%|██        | 59/285 [01:49<06:18,  1.67s/it]predicting train subjects:  21%|██        | 60/285 [01:51<06:21,  1.70s/it]predicting train subjects:  21%|██▏       | 61/285 [01:52<06:28,  1.73s/it]predicting train subjects:  22%|██▏       | 62/285 [01:54<06:16,  1.69s/it]predicting train subjects:  22%|██▏       | 63/285 [01:55<06:02,  1.63s/it]predicting train subjects:  22%|██▏       | 64/285 [01:57<06:01,  1.64s/it]predicting train subjects:  23%|██▎       | 65/285 [01:59<06:18,  1.72s/it]predicting train subjects:  23%|██▎       | 66/285 [02:01<06:24,  1.76s/it]predicting train subjects:  24%|██▎       | 67/285 [02:02<06:16,  1.73s/it]predicting train subjects:  24%|██▍       | 68/285 [02:04<06:14,  1.72s/it]predicting train subjects:  24%|██▍       | 69/285 [02:06<06:11,  1.72s/it]predicting train subjects:  25%|██▍       | 70/285 [02:08<06:06,  1.71s/it]predicting train subjects:  25%|██▍       | 71/285 [02:09<06:06,  1.71s/it]predicting train subjects:  25%|██▌       | 72/285 [02:11<05:58,  1.68s/it]predicting train subjects:  26%|██▌       | 73/285 [02:13<06:02,  1.71s/it]predicting train subjects:  26%|██▌       | 74/285 [02:14<06:00,  1.71s/it]predicting train subjects:  26%|██▋       | 75/285 [02:16<05:56,  1.70s/it]predicting train subjects:  27%|██▋       | 76/285 [02:18<05:53,  1.69s/it]predicting train subjects:  27%|██▋       | 77/285 [02:20<05:56,  1.71s/it]predicting train subjects:  27%|██▋       | 78/285 [02:21<05:48,  1.68s/it]predicting train subjects:  28%|██▊       | 79/285 [02:23<05:42,  1.66s/it]predicting train subjects:  28%|██▊       | 80/285 [02:24<05:39,  1.65s/it]predicting train subjects:  28%|██▊       | 81/285 [02:26<05:45,  1.69s/it]predicting train subjects:  29%|██▉       | 82/285 [02:28<05:42,  1.69s/it]predicting train subjects:  29%|██▉       | 83/285 [02:29<05:38,  1.68s/it]predicting train subjects:  29%|██▉       | 84/285 [02:31<05:38,  1.68s/it]predicting train subjects:  30%|██▉       | 85/285 [02:33<05:59,  1.80s/it]predicting train subjects:  30%|███       | 86/285 [02:35<06:08,  1.85s/it]predicting train subjects:  31%|███       | 87/285 [02:37<06:13,  1.89s/it]predicting train subjects:  31%|███       | 88/285 [02:39<06:15,  1.91s/it]predicting train subjects:  31%|███       | 89/285 [02:41<06:19,  1.94s/it]predicting train subjects:  32%|███▏      | 90/285 [02:43<06:21,  1.96s/it]predicting train subjects:  32%|███▏      | 91/285 [02:45<06:18,  1.95s/it]predicting train subjects:  32%|███▏      | 92/285 [02:47<06:16,  1.95s/it]predicting train subjects:  33%|███▎      | 93/285 [02:49<06:29,  2.03s/it]predicting train subjects:  33%|███▎      | 94/285 [02:51<06:19,  1.99s/it]predicting train subjects:  33%|███▎      | 95/285 [02:53<06:12,  1.96s/it]predicting train subjects:  34%|███▎      | 96/285 [02:55<06:08,  1.95s/it]predicting train subjects:  34%|███▍      | 97/285 [02:57<06:11,  1.98s/it]predicting train subjects:  34%|███▍      | 98/285 [02:59<06:10,  1.98s/it]predicting train subjects:  35%|███▍      | 99/285 [03:01<06:14,  2.01s/it]predicting train subjects:  35%|███▌      | 100/285 [03:03<06:08,  1.99s/it]predicting train subjects:  35%|███▌      | 101/285 [03:05<06:05,  1.99s/it]predicting train subjects:  36%|███▌      | 102/285 [03:07<06:04,  1.99s/it]predicting train subjects:  36%|███▌      | 103/285 [03:09<06:04,  2.00s/it]predicting train subjects:  36%|███▋      | 104/285 [03:11<06:01,  2.00s/it]predicting train subjects:  37%|███▋      | 105/285 [03:13<06:02,  2.01s/it]predicting train subjects:  37%|███▋      | 106/285 [03:15<05:58,  2.00s/it]predicting train subjects:  38%|███▊      | 107/285 [03:17<05:49,  1.96s/it]predicting train subjects:  38%|███▊      | 108/285 [03:19<05:37,  1.90s/it]predicting train subjects:  38%|███▊      | 109/285 [03:20<05:26,  1.86s/it]predicting train subjects:  39%|███▊      | 110/285 [03:22<05:20,  1.83s/it]predicting train subjects:  39%|███▉      | 111/285 [03:24<05:19,  1.83s/it]predicting train subjects:  39%|███▉      | 112/285 [03:26<05:16,  1.83s/it]predicting train subjects:  40%|███▉      | 113/285 [03:28<05:13,  1.82s/it]predicting train subjects:  40%|████      | 114/285 [03:29<05:09,  1.81s/it]predicting train subjects:  40%|████      | 115/285 [03:31<05:04,  1.79s/it]predicting train subjects:  41%|████      | 116/285 [03:33<05:01,  1.78s/it]predicting train subjects:  41%|████      | 117/285 [03:35<04:58,  1.78s/it]predicting train subjects:  41%|████▏     | 118/285 [03:36<04:53,  1.76s/it]predicting train subjects:  42%|████▏     | 119/285 [03:38<04:52,  1.76s/it]predicting train subjects:  42%|████▏     | 120/285 [03:40<04:51,  1.76s/it]predicting train subjects:  42%|████▏     | 121/285 [03:42<04:43,  1.73s/it]predicting train subjects:  43%|████▎     | 122/285 [03:43<04:30,  1.66s/it]predicting train subjects:  43%|████▎     | 123/285 [03:45<04:20,  1.61s/it]predicting train subjects:  44%|████▎     | 124/285 [03:46<04:19,  1.61s/it]predicting train subjects:  44%|████▍     | 125/285 [03:48<04:17,  1.61s/it]predicting train subjects:  44%|████▍     | 126/285 [03:49<04:16,  1.62s/it]predicting train subjects:  45%|████▍     | 127/285 [03:51<04:16,  1.62s/it]predicting train subjects:  45%|████▍     | 128/285 [03:53<04:16,  1.63s/it]predicting train subjects:  45%|████▌     | 129/285 [03:54<04:10,  1.61s/it]predicting train subjects:  46%|████▌     | 130/285 [03:56<04:10,  1.61s/it]predicting train subjects:  46%|████▌     | 131/285 [03:58<04:09,  1.62s/it]predicting train subjects:  46%|████▋     | 132/285 [03:59<04:05,  1.61s/it]predicting train subjects:  47%|████▋     | 133/285 [04:01<04:05,  1.61s/it]predicting train subjects:  47%|████▋     | 134/285 [04:02<04:07,  1.64s/it]predicting train subjects:  47%|████▋     | 135/285 [04:04<04:04,  1.63s/it]predicting train subjects:  48%|████▊     | 136/285 [04:06<04:01,  1.62s/it]predicting train subjects:  48%|████▊     | 137/285 [04:07<03:59,  1.62s/it]predicting train subjects:  48%|████▊     | 138/285 [04:09<03:58,  1.62s/it]predicting train subjects:  49%|████▉     | 139/285 [04:11<03:57,  1.62s/it]predicting train subjects:  49%|████▉     | 140/285 [04:12<03:51,  1.60s/it]predicting train subjects:  49%|████▉     | 141/285 [04:14<03:52,  1.62s/it]predicting train subjects:  50%|████▉     | 142/285 [04:15<03:45,  1.58s/it]predicting train subjects:  50%|█████     | 143/285 [04:17<03:39,  1.55s/it]predicting train subjects:  51%|█████     | 144/285 [04:18<03:34,  1.52s/it]predicting train subjects:  51%|█████     | 145/285 [04:20<03:33,  1.53s/it]predicting train subjects:  51%|█████     | 146/285 [04:21<03:28,  1.50s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:23<03:24,  1.48s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:24<03:24,  1.49s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:26<03:23,  1.49s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:27<03:21,  1.49s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:29<03:18,  1.48s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:30<03:15,  1.47s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:31<03:11,  1.45s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:33<03:19,  1.52s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:35<03:25,  1.58s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:36<03:25,  1.59s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:38<03:27,  1.62s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:40<03:23,  1.60s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:41<03:28,  1.66s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:43<03:30,  1.68s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:45<03:24,  1.65s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:46<03:23,  1.66s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:48<03:32,  1.74s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:50<03:25,  1.70s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:52<03:21,  1.68s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:53<03:21,  1.69s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:55<03:21,  1.71s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:57<03:20,  1.71s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:59<03:20,  1.73s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:00<03:14,  1.69s/it]predicting train subjects:  60%|██████    | 171/285 [05:02<03:10,  1.67s/it]predicting train subjects:  60%|██████    | 172/285 [05:04<03:15,  1.73s/it]predicting train subjects:  61%|██████    | 173/285 [05:05<03:12,  1.72s/it]predicting train subjects:  61%|██████    | 174/285 [05:07<03:09,  1.71s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:09<03:06,  1.70s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:10<03:03,  1.68s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:12<03:05,  1.72s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:14<02:58,  1.67s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:15<02:54,  1.65s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:17<02:52,  1.64s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:19<02:48,  1.62s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:20<02:44,  1.60s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:22<02:52,  1.70s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:24<02:55,  1.74s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:26<02:59,  1.80s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:28<03:04,  1.86s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:30<03:01,  1.85s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:32<03:18,  2.05s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:34<03:23,  2.12s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:37<03:28,  2.20s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:39<03:15,  2.08s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:41<03:17,  2.13s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:43<03:05,  2.01s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:44<02:56,  1.94s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:46<02:53,  1.93s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:48<02:56,  1.98s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:51<03:03,  2.09s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:53<03:06,  2.15s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:55<03:05,  2.15s/it]predicting train subjects:  70%|███████   | 200/285 [05:58<03:20,  2.36s/it]predicting train subjects:  71%|███████   | 201/285 [06:00<03:11,  2.28s/it]predicting train subjects:  71%|███████   | 202/285 [06:02<03:00,  2.18s/it]predicting train subjects:  71%|███████   | 203/285 [06:04<02:53,  2.11s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:06<02:46,  2.06s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:08<02:44,  2.06s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:10<02:36,  1.98s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:12<02:31,  1.94s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:14<02:27,  1.92s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:15<02:27,  1.93s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:17<02:25,  1.94s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:19<02:22,  1.93s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:21<02:19,  1.91s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:23<02:16,  1.90s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:25<02:14,  1.90s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:27<02:15,  1.94s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:29<02:09,  1.88s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:31<02:08,  1.89s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:32<02:02,  1.83s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:34<02:02,  1.85s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:36<01:56,  1.79s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:38<01:54,  1.79s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:39<01:51,  1.77s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:41<01:49,  1.77s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:43<01:44,  1.71s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:44<01:41,  1.69s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:46<01:37,  1.66s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:48<01:38,  1.69s/it]predicting train subjects:  80%|████████  | 228/285 [06:49<01:34,  1.65s/it]predicting train subjects:  80%|████████  | 229/285 [06:51<01:34,  1.70s/it]predicting train subjects:  81%|████████  | 230/285 [06:53<01:31,  1.66s/it]predicting train subjects:  81%|████████  | 231/285 [06:55<01:32,  1.71s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:57<01:39,  1.88s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:59<01:40,  1.93s/it]predicting train subjects:  82%|████████▏ | 234/285 [07:01<01:40,  1.96s/it]predicting train subjects:  82%|████████▏ | 235/285 [07:03<01:37,  1.95s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:05<01:39,  2.04s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:07<01:40,  2.09s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:09<01:40,  2.13s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:12<01:36,  2.11s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:14<01:38,  2.18s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:16<01:38,  2.24s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:18<01:33,  2.18s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:20<01:30,  2.16s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:23<01:28,  2.16s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:25<01:25,  2.13s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:27<01:24,  2.16s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:29<01:23,  2.20s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:31<01:20,  2.19s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:33<01:17,  2.14s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:35<01:09,  2.00s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:37<01:02,  1.85s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:38<00:57,  1.75s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:40<00:54,  1.71s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:41<00:52,  1.68s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:43<00:50,  1.68s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:45<00:48,  1.68s/it]predicting train subjects:  90%|█████████ | 257/285 [07:46<00:46,  1.68s/it]predicting train subjects:  91%|█████████ | 258/285 [07:48<00:45,  1.67s/it]predicting train subjects:  91%|█████████ | 259/285 [07:50<00:42,  1.64s/it]predicting train subjects:  91%|█████████ | 260/285 [07:51<00:41,  1.65s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:53<00:39,  1.63s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:54<00:36,  1.60s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:56<00:35,  1.63s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:58<00:33,  1.61s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:59<00:32,  1.62s/it]predicting train subjects:  93%|█████████▎| 266/285 [08:01<00:30,  1.63s/it]predicting train subjects:  94%|█████████▎| 267/285 [08:03<00:29,  1.66s/it]predicting train subjects:  94%|█████████▍| 268/285 [08:05<00:31,  1.86s/it]predicting train subjects:  94%|█████████▍| 269/285 [08:07<00:30,  1.90s/it]predicting train subjects:  95%|█████████▍| 270/285 [08:09<00:29,  1.99s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:11<00:28,  2.04s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:13<00:27,  2.09s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:16<00:25,  2.14s/it]predicting train subjects:  96%|█████████▌| 274/285 [08:18<00:23,  2.11s/it]predicting train subjects:  96%|█████████▋| 275/285 [08:20<00:21,  2.17s/it]predicting train subjects:  97%|█████████▋| 276/285 [08:22<00:19,  2.19s/it]predicting train subjects:  97%|█████████▋| 277/285 [08:24<00:17,  2.19s/it]predicting train subjects:  98%|█████████▊| 278/285 [08:27<00:15,  2.18s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:29<00:13,  2.17s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:31<00:10,  2.20s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:33<00:08,  2.17s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:35<00:06,  2.19s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:38<00:04,  2.22s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:40<00:02,  2.19s/it]predicting train subjects: 100%|██████████| 285/285 [08:42<00:00,  2.20s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:02<09:58,  2.11s/it]Loading train:   1%|          | 2/285 [00:03<09:18,  1.97s/it]Loading train:   1%|          | 3/285 [00:05<08:56,  1.90s/it]Loading train:   1%|▏         | 4/285 [00:07<09:21,  2.00s/it]Loading train:   2%|▏         | 5/285 [00:09<09:37,  2.06s/it]Loading train:   2%|▏         | 6/285 [00:11<09:22,  2.02s/it]Loading train:   2%|▏         | 7/285 [00:14<10:05,  2.18s/it]Loading train:   3%|▎         | 8/285 [00:16<09:42,  2.10s/it]Loading train:   3%|▎         | 9/285 [00:18<09:07,  1.98s/it]Loading train:   4%|▎         | 10/285 [00:19<09:00,  1.97s/it]Loading train:   4%|▍         | 11/285 [00:21<08:40,  1.90s/it]Loading train:   4%|▍         | 12/285 [00:23<08:51,  1.95s/it]Loading train:   5%|▍         | 13/285 [00:25<08:41,  1.92s/it]Loading train:   5%|▍         | 14/285 [00:27<08:08,  1.80s/it]Loading train:   5%|▌         | 15/285 [00:28<08:03,  1.79s/it]Loading train:   6%|▌         | 16/285 [00:30<08:02,  1.79s/it]Loading train:   6%|▌         | 17/285 [00:32<07:57,  1.78s/it]Loading train:   6%|▋         | 18/285 [00:34<08:47,  1.98s/it]Loading train:   7%|▋         | 19/285 [00:37<09:04,  2.05s/it]Loading train:   7%|▋         | 20/285 [00:38<08:46,  1.99s/it]Loading train:   7%|▋         | 21/285 [00:40<08:05,  1.84s/it]Loading train:   8%|▊         | 22/285 [00:41<07:39,  1.75s/it]Loading train:   8%|▊         | 23/285 [00:43<07:37,  1.75s/it]Loading train:   8%|▊         | 24/285 [00:45<07:16,  1.67s/it]Loading train:   9%|▉         | 25/285 [00:47<07:32,  1.74s/it]Loading train:   9%|▉         | 26/285 [00:49<07:50,  1.82s/it]Loading train:   9%|▉         | 27/285 [00:51<08:32,  1.99s/it]Loading train:  10%|▉         | 28/285 [00:53<08:43,  2.04s/it]Loading train:  10%|█         | 29/285 [00:55<08:24,  1.97s/it]Loading train:  11%|█         | 30/285 [00:57<08:29,  2.00s/it]Loading train:  11%|█         | 31/285 [00:59<08:11,  1.94s/it]Loading train:  11%|█         | 32/285 [01:00<07:49,  1.86s/it]Loading train:  12%|█▏        | 33/285 [01:02<07:55,  1.89s/it]Loading train:  12%|█▏        | 34/285 [01:05<08:10,  1.95s/it]Loading train:  12%|█▏        | 35/285 [01:07<08:26,  2.03s/it]Loading train:  13%|█▎        | 36/285 [01:09<08:39,  2.09s/it]Loading train:  13%|█▎        | 37/285 [01:11<08:40,  2.10s/it]Loading train:  13%|█▎        | 38/285 [01:14<09:21,  2.27s/it]Loading train:  14%|█▎        | 39/285 [01:16<09:40,  2.36s/it]Loading train:  14%|█▍        | 40/285 [01:19<09:37,  2.36s/it]Loading train:  14%|█▍        | 41/285 [01:21<09:57,  2.45s/it]Loading train:  15%|█▍        | 42/285 [01:23<09:27,  2.34s/it]Loading train:  15%|█▌        | 43/285 [01:26<09:34,  2.38s/it]Loading train:  15%|█▌        | 44/285 [01:28<09:29,  2.36s/it]Loading train:  16%|█▌        | 45/285 [01:30<09:07,  2.28s/it]Loading train:  16%|█▌        | 46/285 [01:32<08:33,  2.15s/it]Loading train:  16%|█▋        | 47/285 [01:33<07:08,  1.80s/it]Loading train:  17%|█▋        | 48/285 [01:34<06:08,  1.56s/it]Loading train:  17%|█▋        | 49/285 [01:35<05:48,  1.48s/it]Loading train:  18%|█▊        | 50/285 [01:37<05:49,  1.49s/it]Loading train:  18%|█▊        | 51/285 [01:38<05:51,  1.50s/it]Loading train:  18%|█▊        | 52/285 [01:40<06:21,  1.64s/it]Loading train:  19%|█▊        | 53/285 [01:42<06:16,  1.62s/it]Loading train:  19%|█▉        | 54/285 [01:44<06:39,  1.73s/it]Loading train:  19%|█▉        | 55/285 [01:47<07:58,  2.08s/it]Loading train:  20%|█▉        | 56/285 [01:48<06:40,  1.75s/it]Loading train:  20%|██        | 57/285 [01:49<05:37,  1.48s/it]Loading train:  20%|██        | 58/285 [01:50<04:59,  1.32s/it]Loading train:  21%|██        | 59/285 [01:51<05:31,  1.47s/it]Loading train:  21%|██        | 60/285 [01:53<05:17,  1.41s/it]Loading train:  21%|██▏       | 61/285 [01:54<04:59,  1.34s/it]Loading train:  22%|██▏       | 62/285 [01:55<05:06,  1.37s/it]Loading train:  22%|██▏       | 63/285 [01:56<04:36,  1.25s/it]Loading train:  22%|██▏       | 64/285 [01:58<04:53,  1.33s/it]Loading train:  23%|██▎       | 65/285 [01:59<05:08,  1.40s/it]Loading train:  23%|██▎       | 66/285 [02:01<05:05,  1.40s/it]Loading train:  24%|██▎       | 67/285 [02:02<04:49,  1.33s/it]Loading train:  24%|██▍       | 68/285 [02:03<04:25,  1.23s/it]Loading train:  24%|██▍       | 69/285 [02:04<04:10,  1.16s/it]Loading train:  25%|██▍       | 70/285 [02:05<04:02,  1.13s/it]Loading train:  25%|██▍       | 71/285 [02:06<03:54,  1.10s/it]Loading train:  25%|██▌       | 72/285 [02:07<03:49,  1.08s/it]Loading train:  26%|██▌       | 73/285 [02:08<03:44,  1.06s/it]Loading train:  26%|██▌       | 74/285 [02:09<03:39,  1.04s/it]Loading train:  26%|██▋       | 75/285 [02:10<03:35,  1.03s/it]Loading train:  27%|██▋       | 76/285 [02:11<03:31,  1.01s/it]Loading train:  27%|██▋       | 77/285 [02:12<03:26,  1.01it/s]Loading train:  27%|██▋       | 78/285 [02:13<03:28,  1.01s/it]Loading train:  28%|██▊       | 79/285 [02:14<03:25,  1.00it/s]Loading train:  28%|██▊       | 80/285 [02:15<03:24,  1.00it/s]Loading train:  28%|██▊       | 81/285 [02:16<03:27,  1.02s/it]Loading train:  29%|██▉       | 82/285 [02:17<03:28,  1.03s/it]Loading train:  29%|██▉       | 83/285 [02:18<03:27,  1.03s/it]Loading train:  29%|██▉       | 84/285 [02:19<03:24,  1.02s/it]Loading train:  30%|██▉       | 85/285 [02:20<03:32,  1.06s/it]Loading train:  30%|███       | 86/285 [02:21<03:34,  1.08s/it]Loading train:  31%|███       | 87/285 [02:23<03:32,  1.07s/it]Loading train:  31%|███       | 88/285 [02:24<03:30,  1.07s/it]Loading train:  31%|███       | 89/285 [02:25<03:27,  1.06s/it]Loading train:  32%|███▏      | 90/285 [02:26<03:29,  1.07s/it]Loading train:  32%|███▏      | 91/285 [02:27<03:27,  1.07s/it]Loading train:  32%|███▏      | 92/285 [02:28<03:24,  1.06s/it]Loading train:  33%|███▎      | 93/285 [02:29<03:23,  1.06s/it]Loading train:  33%|███▎      | 94/285 [02:30<03:26,  1.08s/it]Loading train:  33%|███▎      | 95/285 [02:31<03:23,  1.07s/it]Loading train:  34%|███▎      | 96/285 [02:32<03:30,  1.11s/it]Loading train:  34%|███▍      | 97/285 [02:34<03:38,  1.16s/it]Loading train:  34%|███▍      | 98/285 [02:35<03:38,  1.17s/it]Loading train:  35%|███▍      | 99/285 [02:36<03:38,  1.18s/it]Loading train:  35%|███▌      | 100/285 [02:37<03:30,  1.14s/it]Loading train:  35%|███▌      | 101/285 [02:38<03:28,  1.13s/it]Loading train:  36%|███▌      | 102/285 [02:39<03:30,  1.15s/it]Loading train:  36%|███▌      | 103/285 [02:40<03:28,  1.14s/it]Loading train:  36%|███▋      | 104/285 [02:41<03:21,  1.11s/it]Loading train:  37%|███▋      | 105/285 [02:42<03:15,  1.08s/it]Loading train:  37%|███▋      | 106/285 [02:44<03:14,  1.09s/it]Loading train:  38%|███▊      | 107/285 [02:45<03:08,  1.06s/it]Loading train:  38%|███▊      | 108/285 [02:46<03:04,  1.04s/it]Loading train:  38%|███▊      | 109/285 [02:47<03:05,  1.05s/it]Loading train:  39%|███▊      | 110/285 [02:48<03:08,  1.08s/it]Loading train:  39%|███▉      | 111/285 [02:49<03:10,  1.09s/it]Loading train:  39%|███▉      | 112/285 [02:50<03:15,  1.13s/it]Loading train:  40%|███▉      | 113/285 [02:51<03:20,  1.16s/it]Loading train:  40%|████      | 114/285 [02:52<03:11,  1.12s/it]Loading train:  40%|████      | 115/285 [02:53<03:06,  1.10s/it]Loading train:  41%|████      | 116/285 [02:54<03:04,  1.09s/it]Loading train:  41%|████      | 117/285 [02:56<03:06,  1.11s/it]Loading train:  41%|████▏     | 118/285 [02:57<03:00,  1.08s/it]Loading train:  42%|████▏     | 119/285 [02:58<02:58,  1.08s/it]Loading train:  42%|████▏     | 120/285 [02:59<02:57,  1.08s/it]Loading train:  42%|████▏     | 121/285 [03:00<03:12,  1.17s/it]Loading train:  43%|████▎     | 122/285 [03:02<03:17,  1.21s/it]Loading train:  43%|████▎     | 123/285 [03:03<03:20,  1.24s/it]Loading train:  44%|████▎     | 124/285 [03:04<03:07,  1.17s/it]Loading train:  44%|████▍     | 125/285 [03:05<02:55,  1.10s/it]Loading train:  44%|████▍     | 126/285 [03:06<03:08,  1.18s/it]Loading train:  45%|████▍     | 127/285 [03:07<03:04,  1.17s/it]Loading train:  45%|████▍     | 128/285 [03:08<03:03,  1.17s/it]Loading train:  45%|████▌     | 129/285 [03:10<03:18,  1.27s/it]Loading train:  46%|████▌     | 130/285 [03:11<03:11,  1.24s/it]Loading train:  46%|████▌     | 131/285 [03:12<03:12,  1.25s/it]Loading train:  46%|████▋     | 132/285 [03:14<03:20,  1.31s/it]Loading train:  47%|████▋     | 133/285 [03:15<03:15,  1.29s/it]Loading train:  47%|████▋     | 134/285 [03:16<03:13,  1.28s/it]Loading train:  47%|████▋     | 135/285 [03:18<03:11,  1.27s/it]Loading train:  48%|████▊     | 136/285 [03:19<03:15,  1.31s/it]Loading train:  48%|████▊     | 137/285 [03:20<03:10,  1.29s/it]Loading train:  48%|████▊     | 138/285 [03:21<03:05,  1.26s/it]Loading train:  49%|████▉     | 139/285 [03:23<03:03,  1.26s/it]Loading train:  49%|████▉     | 140/285 [03:24<03:07,  1.29s/it]Loading train:  49%|████▉     | 141/285 [03:25<02:58,  1.24s/it]Loading train:  50%|████▉     | 142/285 [03:26<02:56,  1.23s/it]Loading train:  50%|█████     | 143/285 [03:28<02:54,  1.23s/it]Loading train:  51%|█████     | 144/285 [03:29<02:46,  1.18s/it]Loading train:  51%|█████     | 145/285 [03:30<02:56,  1.26s/it]Loading train:  51%|█████     | 146/285 [03:32<03:07,  1.35s/it]Loading train:  52%|█████▏    | 147/285 [03:33<03:03,  1.33s/it]Loading train:  52%|█████▏    | 148/285 [03:34<02:55,  1.28s/it]Loading train:  52%|█████▏    | 149/285 [03:35<02:58,  1.31s/it]Loading train:  53%|█████▎    | 150/285 [03:37<02:52,  1.27s/it]Loading train:  53%|█████▎    | 151/285 [03:38<02:42,  1.21s/it]Loading train:  53%|█████▎    | 152/285 [03:39<02:41,  1.22s/it]Loading train:  54%|█████▎    | 153/285 [03:40<02:34,  1.17s/it]Loading train:  54%|█████▍    | 154/285 [03:41<02:30,  1.15s/it]Loading train:  54%|█████▍    | 155/285 [03:42<02:26,  1.13s/it]Loading train:  55%|█████▍    | 156/285 [03:44<02:36,  1.21s/it]Loading train:  55%|█████▌    | 157/285 [03:45<02:35,  1.21s/it]Loading train:  55%|█████▌    | 158/285 [03:46<02:22,  1.12s/it]Loading train:  56%|█████▌    | 159/285 [03:47<02:12,  1.05s/it]Loading train:  56%|█████▌    | 160/285 [03:48<02:07,  1.02s/it]Loading train:  56%|█████▋    | 161/285 [03:48<02:01,  1.02it/s]Loading train:  57%|█████▋    | 162/285 [03:49<01:56,  1.06it/s]Loading train:  57%|█████▋    | 163/285 [03:50<01:52,  1.09it/s]Loading train:  58%|█████▊    | 164/285 [03:51<01:46,  1.13it/s]Loading train:  58%|█████▊    | 165/285 [03:52<01:45,  1.14it/s]Loading train:  58%|█████▊    | 166/285 [03:53<01:45,  1.13it/s]Loading train:  59%|█████▊    | 167/285 [03:54<01:45,  1.12it/s]Loading train:  59%|█████▉    | 168/285 [03:55<01:43,  1.13it/s]Loading train:  59%|█████▉    | 169/285 [03:55<01:42,  1.13it/s]Loading train:  60%|█████▉    | 170/285 [03:56<01:41,  1.13it/s]Loading train:  60%|██████    | 171/285 [03:57<01:38,  1.16it/s]Loading train:  60%|██████    | 172/285 [03:58<01:36,  1.17it/s]Loading train:  61%|██████    | 173/285 [03:59<01:35,  1.18it/s]Loading train:  61%|██████    | 174/285 [04:00<01:33,  1.19it/s]Loading train:  61%|██████▏   | 175/285 [04:00<01:30,  1.22it/s]Loading train:  62%|██████▏   | 176/285 [04:01<01:28,  1.23it/s]Loading train:  62%|██████▏   | 177/285 [04:02<01:31,  1.18it/s]Loading train:  62%|██████▏   | 178/285 [04:03<01:35,  1.12it/s]Loading train:  63%|██████▎   | 179/285 [04:04<01:36,  1.10it/s]Loading train:  63%|██████▎   | 180/285 [04:05<01:33,  1.13it/s]Loading train:  64%|██████▎   | 181/285 [04:06<01:30,  1.15it/s]Loading train:  64%|██████▍   | 182/285 [04:07<01:28,  1.17it/s]Loading train:  64%|██████▍   | 183/285 [04:07<01:28,  1.15it/s]Loading train:  65%|██████▍   | 184/285 [04:08<01:28,  1.14it/s]Loading train:  65%|██████▍   | 185/285 [04:09<01:26,  1.16it/s]Loading train:  65%|██████▌   | 186/285 [04:10<01:28,  1.12it/s]Loading train:  66%|██████▌   | 187/285 [04:11<01:29,  1.10it/s]Loading train:  66%|██████▌   | 188/285 [04:12<01:26,  1.13it/s]Loading train:  66%|██████▋   | 189/285 [04:13<01:22,  1.16it/s]Loading train:  67%|██████▋   | 190/285 [04:14<01:21,  1.17it/s]Loading train:  67%|██████▋   | 191/285 [04:14<01:23,  1.13it/s]Loading train:  67%|██████▋   | 192/285 [04:15<01:23,  1.11it/s]Loading train:  68%|██████▊   | 193/285 [04:16<01:24,  1.09it/s]Loading train:  68%|██████▊   | 194/285 [04:17<01:25,  1.07it/s]Loading train:  68%|██████▊   | 195/285 [04:18<01:22,  1.09it/s]Loading train:  69%|██████▉   | 196/285 [04:19<01:22,  1.08it/s]Loading train:  69%|██████▉   | 197/285 [04:20<01:21,  1.08it/s]Loading train:  69%|██████▉   | 198/285 [04:21<01:27,  1.00s/it]Loading train:  70%|██████▉   | 199/285 [04:22<01:26,  1.01s/it]Loading train:  70%|███████   | 200/285 [04:23<01:23,  1.01it/s]Loading train:  71%|███████   | 201/285 [04:24<01:24,  1.01s/it]Loading train:  71%|███████   | 202/285 [04:25<01:26,  1.04s/it]Loading train:  71%|███████   | 203/285 [04:26<01:22,  1.01s/it]Loading train:  72%|███████▏  | 204/285 [04:27<01:20,  1.00it/s]Loading train:  72%|███████▏  | 205/285 [04:28<01:18,  1.02it/s]Loading train:  72%|███████▏  | 206/285 [04:29<01:18,  1.01it/s]Loading train:  73%|███████▎  | 207/285 [04:31<01:22,  1.06s/it]Loading train:  73%|███████▎  | 208/285 [04:31<01:18,  1.02s/it]Loading train:  73%|███████▎  | 209/285 [04:32<01:13,  1.03it/s]Loading train:  74%|███████▎  | 210/285 [04:33<01:11,  1.05it/s]Loading train:  74%|███████▍  | 211/285 [04:34<01:11,  1.04it/s]Loading train:  74%|███████▍  | 212/285 [04:35<01:09,  1.05it/s]Loading train:  75%|███████▍  | 213/285 [04:36<01:07,  1.07it/s]Loading train:  75%|███████▌  | 214/285 [04:37<01:03,  1.12it/s]Loading train:  75%|███████▌  | 215/285 [04:38<01:01,  1.13it/s]Loading train:  76%|███████▌  | 216/285 [04:38<00:58,  1.17it/s]Loading train:  76%|███████▌  | 217/285 [04:39<01:00,  1.13it/s]Loading train:  76%|███████▋  | 218/285 [04:40<00:59,  1.13it/s]Loading train:  77%|███████▋  | 219/285 [04:41<00:57,  1.14it/s]Loading train:  77%|███████▋  | 220/285 [04:42<00:58,  1.10it/s]Loading train:  78%|███████▊  | 221/285 [04:43<00:57,  1.11it/s]Loading train:  78%|███████▊  | 222/285 [04:44<00:56,  1.12it/s]Loading train:  78%|███████▊  | 223/285 [04:45<00:57,  1.07it/s]Loading train:  79%|███████▊  | 224/285 [04:46<00:57,  1.05it/s]Loading train:  79%|███████▉  | 225/285 [04:47<00:55,  1.07it/s]Loading train:  79%|███████▉  | 226/285 [04:48<00:52,  1.12it/s]Loading train:  80%|███████▉  | 227/285 [04:49<00:54,  1.07it/s]Loading train:  80%|████████  | 228/285 [04:49<00:52,  1.10it/s]Loading train:  80%|████████  | 229/285 [04:50<00:48,  1.15it/s]Loading train:  81%|████████  | 230/285 [04:51<00:46,  1.18it/s]Loading train:  81%|████████  | 231/285 [04:52<00:44,  1.20it/s]Loading train:  81%|████████▏ | 232/285 [04:53<00:49,  1.07it/s]Loading train:  82%|████████▏ | 233/285 [04:54<00:50,  1.04it/s]Loading train:  82%|████████▏ | 234/285 [04:55<00:51,  1.01s/it]Loading train:  82%|████████▏ | 235/285 [04:56<00:52,  1.05s/it]Loading train:  83%|████████▎ | 236/285 [04:57<00:51,  1.06s/it]Loading train:  83%|████████▎ | 237/285 [04:58<00:50,  1.05s/it]Loading train:  84%|████████▎ | 238/285 [04:59<00:49,  1.05s/it]Loading train:  84%|████████▍ | 239/285 [05:01<00:48,  1.05s/it]Loading train:  84%|████████▍ | 240/285 [05:02<00:48,  1.07s/it]Loading train:  85%|████████▍ | 241/285 [05:03<00:50,  1.15s/it]Loading train:  85%|████████▍ | 242/285 [05:04<00:52,  1.21s/it]Loading train:  85%|████████▌ | 243/285 [05:06<00:53,  1.26s/it]Loading train:  86%|████████▌ | 244/285 [05:07<00:53,  1.30s/it]Loading train:  86%|████████▌ | 245/285 [05:09<00:59,  1.49s/it]Loading train:  86%|████████▋ | 246/285 [05:10<00:56,  1.44s/it]Loading train:  87%|████████▋ | 247/285 [05:12<00:53,  1.40s/it]Loading train:  87%|████████▋ | 248/285 [05:13<00:51,  1.39s/it]Loading train:  87%|████████▋ | 249/285 [05:14<00:49,  1.36s/it]Loading train:  88%|████████▊ | 250/285 [05:16<00:45,  1.31s/it]Loading train:  88%|████████▊ | 251/285 [05:17<00:48,  1.42s/it]Loading train:  88%|████████▊ | 252/285 [05:18<00:44,  1.35s/it]Loading train:  89%|████████▉ | 253/285 [05:20<00:42,  1.33s/it]Loading train:  89%|████████▉ | 254/285 [05:21<00:41,  1.33s/it]Loading train:  89%|████████▉ | 255/285 [05:22<00:38,  1.27s/it]Loading train:  90%|████████▉ | 256/285 [05:23<00:36,  1.27s/it]Loading train:  90%|█████████ | 257/285 [05:25<00:34,  1.23s/it]Loading train:  91%|█████████ | 258/285 [05:26<00:32,  1.20s/it]Loading train:  91%|█████████ | 259/285 [05:27<00:31,  1.23s/it]Loading train:  91%|█████████ | 260/285 [05:28<00:30,  1.21s/it]Loading train:  92%|█████████▏| 261/285 [05:29<00:28,  1.19s/it]Loading train:  92%|█████████▏| 262/285 [05:31<00:28,  1.24s/it]Loading train:  92%|█████████▏| 263/285 [05:32<00:27,  1.24s/it]Loading train:  93%|█████████▎| 264/285 [05:33<00:25,  1.22s/it]Loading train:  93%|█████████▎| 265/285 [05:34<00:24,  1.22s/it]Loading train:  93%|█████████▎| 266/285 [05:35<00:23,  1.22s/it]Loading train:  94%|█████████▎| 267/285 [05:37<00:21,  1.19s/it]Loading train:  94%|█████████▍| 268/285 [05:38<00:22,  1.31s/it]Loading train:  94%|█████████▍| 269/285 [05:40<00:21,  1.35s/it]Loading train:  95%|█████████▍| 270/285 [05:41<00:20,  1.35s/it]Loading train:  95%|█████████▌| 271/285 [05:42<00:17,  1.28s/it]Loading train:  95%|█████████▌| 272/285 [05:43<00:15,  1.23s/it]Loading train:  96%|█████████▌| 273/285 [05:44<00:14,  1.19s/it]Loading train:  96%|█████████▌| 274/285 [05:45<00:12,  1.15s/it]Loading train:  96%|█████████▋| 275/285 [05:46<00:11,  1.15s/it]Loading train:  97%|█████████▋| 276/285 [05:48<00:09,  1.11s/it]Loading train:  97%|█████████▋| 277/285 [05:49<00:08,  1.10s/it]Loading train:  98%|█████████▊| 278/285 [05:50<00:07,  1.10s/it]Loading train:  98%|█████████▊| 279/285 [05:51<00:06,  1.08s/it]Loading train:  98%|█████████▊| 280/285 [05:52<00:05,  1.05s/it]Loading train:  99%|█████████▊| 281/285 [05:53<00:04,  1.07s/it]Loading train:  99%|█████████▉| 282/285 [05:54<00:03,  1.05s/it]Loading train:  99%|█████████▉| 283/285 [05:55<00:02,  1.05s/it]Loading train: 100%|█████████▉| 284/285 [05:56<00:01,  1.04s/it]Loading train: 100%|██████████| 285/285 [05:57<00:00,  1.03s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 21/285 [00:00<00:01, 205.22it/s]concatenating: train:  17%|█▋        | 49/285 [00:00<00:01, 222.53it/s]concatenating: train:  26%|██▌       | 74/285 [00:00<00:00, 229.44it/s]concatenating: train:  35%|███▌      | 100/285 [00:00<00:00, 237.43it/s]concatenating: train:  44%|████▎     | 124/285 [00:00<00:00, 236.73it/s]concatenating: train:  53%|█████▎    | 151/285 [00:00<00:00, 243.43it/s]concatenating: train:  63%|██████▎   | 179/285 [00:00<00:00, 252.12it/s]concatenating: train:  73%|███████▎  | 207/285 [00:00<00:00, 257.68it/s]concatenating: train:  81%|████████▏ | 232/285 [00:00<00:00, 243.80it/s]concatenating: train:  90%|████████▉ | 256/285 [00:01<00:00, 242.40it/s]concatenating: train:  98%|█████████▊| 280/285 [00:01<00:00, 226.48it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 240.73it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.48s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.49s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 72.57it/s]2019-07-06 17:56:36.632585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 17:56:36.632715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 17:56:36.632734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 17:56:36.632769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 17:56:36.633241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:08,  4.49it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:07,  5.25it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:07,  4.97it/s]loading the weights for Unet:  20%|██        | 8/40 [00:00<00:05,  6.36it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:05,  5.72it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:04,  6.45it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  4.75it/s]loading the weights for Unet:  40%|████      | 16/40 [00:01<00:03,  6.20it/s]loading the weights for Unet:  45%|████▌     | 18/40 [00:02<00:03,  7.03it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  6.18it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:02<00:02,  7.45it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:02<00:01,  8.02it/s]loading the weights for Unet:  68%|██████▊   | 27/40 [00:03<00:01,  8.37it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:03<00:01,  6.81it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:03<00:01,  7.40it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  7.83it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  6.33it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:04<00:00,  6.98it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:04<00:00,  5.96it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:04<00:00,  8.45it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 52, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 52, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 52, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 52, 10)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 52, 10)   910         dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 52, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 52, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 52, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 52, 10)   910         dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 52, 10)   40          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 52, 10)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 52, 10)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 52, 20)   1820        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 52, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 52, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 52, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 52, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 52, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 26, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 26, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 26, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 26, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 26, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 26, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 26, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 13, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 13, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 13, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 13, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 13, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 13, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 13, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 13, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 26, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 26, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 26, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 26, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 26, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 26, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 26, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 26, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 52, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 52, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 52, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 52, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 52, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 52, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 52, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 52, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 52, 13)   273         dropout_8[0][0]                  
==================================================================================================
Total params: 187,773
Trainable params: 44,233
Non-trainable params: 143,540
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.49841486e-02 3.19966680e-02 7.50970181e-02 9.33357939e-03
 2.71292049e-02 7.07427267e-03 8.46489586e-02 1.12779077e-01
 8.61338510e-02 1.32649165e-02 2.94521391e-01 1.92807035e-01
 2.29878984e-04]
Train on 18361 samples, validate on 179 samples
Epoch 1/300
 - 22s - loss: 230.7206 - acc: 0.5858 - mDice: 0.0131 - val_loss: 28.1589 - val_acc: 0.9136 - val_mDice: 0.0082

Epoch 00001: val_mDice improved from -inf to 0.00816, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 12s - loss: 30.6434 - acc: 0.8757 - mDice: 0.0119 - val_loss: 9.2352 - val_acc: 0.9136 - val_mDice: 0.0086

Epoch 00002: val_mDice improved from 0.00816 to 0.00862, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 13s - loss: 15.3410 - acc: 0.8852 - mDice: 0.0134 - val_loss: 6.3974 - val_acc: 0.9136 - val_mDice: 0.0108

Epoch 00003: val_mDice improved from 0.00862 to 0.01085, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 12s - loss: 10.6887 - acc: 0.8861 - mDice: 0.0226 - val_loss: 5.3974 - val_acc: 0.9136 - val_mDice: 0.0211

Epoch 00004: val_mDice improved from 0.01085 to 0.02106, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 12s - loss: 8.6093 - acc: 0.8860 - mDice: 0.0314 - val_loss: 4.7754 - val_acc: 0.9136 - val_mDice: 0.0304

Epoch 00005: val_mDice improved from 0.02106 to 0.03040, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 13s - loss: 7.3850 - acc: 0.8859 - mDice: 0.0380 - val_loss: 4.3480 - val_acc: 0.9136 - val_mDice: 0.0460

Epoch 00006: val_mDice improved from 0.03040 to 0.04604, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 12s - loss: 6.5018 - acc: 0.8857 - mDice: 0.0472 - val_loss: 4.1691 - val_acc: 0.9136 - val_mDice: 0.0475

Epoch 00007: val_mDice improved from 0.04604 to 0.04751, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 13s - loss: 5.8167 - acc: 0.8862 - mDice: 0.0606 - val_loss: 3.7407 - val_acc: 0.9136 - val_mDice: 0.0732

Epoch 00008: val_mDice improved from 0.04751 to 0.07316, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 12s - loss: 5.2015 - acc: 0.8875 - mDice: 0.0818 - val_loss: 3.3206 - val_acc: 0.9129 - val_mDice: 0.1057

Epoch 00009: val_mDice improved from 0.07316 to 0.10569, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 12s - loss: 4.7287 - acc: 0.8893 - mDice: 0.1011 - val_loss: 3.0550 - val_acc: 0.9149 - val_mDice: 0.1386

Epoch 00010: val_mDice improved from 0.10569 to 0.13855, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 13s - loss: 4.3433 - acc: 0.8908 - mDice: 0.1204 - val_loss: 2.8697 - val_acc: 0.9185 - val_mDice: 0.1623

Epoch 00011: val_mDice improved from 0.13855 to 0.16228, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 12s - loss: 4.0336 - acc: 0.8926 - mDice: 0.1413 - val_loss: 2.6939 - val_acc: 0.9191 - val_mDice: 0.1857

Epoch 00012: val_mDice improved from 0.16228 to 0.18568, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 13s - loss: 3.7620 - acc: 0.8951 - mDice: 0.1638 - val_loss: 2.7328 - val_acc: 0.9180 - val_mDice: 0.1994

Epoch 00013: val_mDice improved from 0.18568 to 0.19939, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 13s - loss: 3.5209 - acc: 0.8980 - mDice: 0.1856 - val_loss: 3.2075 - val_acc: 0.9187 - val_mDice: 0.1863

Epoch 00014: val_mDice did not improve from 0.19939
Epoch 15/300
 - 13s - loss: 3.3091 - acc: 0.9010 - mDice: 0.2099 - val_loss: 2.7082 - val_acc: 0.9259 - val_mDice: 0.2392

Epoch 00015: val_mDice improved from 0.19939 to 0.23919, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 12s - loss: 3.1253 - acc: 0.9037 - mDice: 0.2320 - val_loss: 2.6423 - val_acc: 0.9272 - val_mDice: 0.2590

Epoch 00016: val_mDice improved from 0.23919 to 0.25895, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 13s - loss: 2.9647 - acc: 0.9063 - mDice: 0.2527 - val_loss: 2.4378 - val_acc: 0.9350 - val_mDice: 0.3007

Epoch 00017: val_mDice improved from 0.25895 to 0.30065, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 13s - loss: 2.8369 - acc: 0.9086 - mDice: 0.2713 - val_loss: 2.3721 - val_acc: 0.9359 - val_mDice: 0.3080

Epoch 00018: val_mDice improved from 0.30065 to 0.30800, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 13s - loss: 2.7253 - acc: 0.9106 - mDice: 0.2878 - val_loss: 2.5370 - val_acc: 0.9358 - val_mDice: 0.3069

Epoch 00019: val_mDice did not improve from 0.30800
Epoch 20/300
 - 12s - loss: 2.6299 - acc: 0.9124 - mDice: 0.3018 - val_loss: 2.0465 - val_acc: 0.9394 - val_mDice: 0.3578

Epoch 00020: val_mDice improved from 0.30800 to 0.35782, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 13s - loss: 2.5448 - acc: 0.9137 - mDice: 0.3151 - val_loss: 2.6588 - val_acc: 0.9376 - val_mDice: 0.3127

Epoch 00021: val_mDice did not improve from 0.35782
Epoch 22/300
 - 12s - loss: 2.4745 - acc: 0.9153 - mDice: 0.3268 - val_loss: 2.3446 - val_acc: 0.9400 - val_mDice: 0.3420

Epoch 00022: val_mDice did not improve from 0.35782
Epoch 23/300
 - 13s - loss: 2.4073 - acc: 0.9166 - mDice: 0.3377 - val_loss: 2.2456 - val_acc: 0.9409 - val_mDice: 0.3605

Epoch 00023: val_mDice improved from 0.35782 to 0.36049, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 12s - loss: 2.3505 - acc: 0.9178 - mDice: 0.3476 - val_loss: 2.0767 - val_acc: 0.9412 - val_mDice: 0.3715

Epoch 00024: val_mDice improved from 0.36049 to 0.37146, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 13s - loss: 2.3022 - acc: 0.9188 - mDice: 0.3571 - val_loss: 2.0784 - val_acc: 0.9423 - val_mDice: 0.3896

Epoch 00025: val_mDice improved from 0.37146 to 0.38961, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 12s - loss: 2.2521 - acc: 0.9195 - mDice: 0.3666 - val_loss: 2.1863 - val_acc: 0.9411 - val_mDice: 0.3751

Epoch 00026: val_mDice did not improve from 0.38961
Epoch 27/300
 - 13s - loss: 2.2101 - acc: 0.9203 - mDice: 0.3754 - val_loss: 1.9754 - val_acc: 0.9423 - val_mDice: 0.4074

Epoch 00027: val_mDice improved from 0.38961 to 0.40742, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 13s - loss: 2.1662 - acc: 0.9211 - mDice: 0.3836 - val_loss: 2.0520 - val_acc: 0.9424 - val_mDice: 0.4021

Epoch 00028: val_mDice did not improve from 0.40742
Epoch 29/300
 - 13s - loss: 2.1388 - acc: 0.9216 - mDice: 0.3901 - val_loss: 2.1550 - val_acc: 0.9416 - val_mDice: 0.3915

Epoch 00029: val_mDice did not improve from 0.40742
Epoch 30/300
 - 13s - loss: 2.0935 - acc: 0.9224 - mDice: 0.3991 - val_loss: 2.1002 - val_acc: 0.9423 - val_mDice: 0.4080

Epoch 00030: val_mDice improved from 0.40742 to 0.40796, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 12s - loss: 2.0664 - acc: 0.9230 - mDice: 0.4063 - val_loss: 2.0879 - val_acc: 0.9417 - val_mDice: 0.4052

Epoch 00031: val_mDice did not improve from 0.40796
Epoch 32/300
 - 13s - loss: 2.0264 - acc: 0.9235 - mDice: 0.4148 - val_loss: 2.2651 - val_acc: 0.9406 - val_mDice: 0.3939

Epoch 00032: val_mDice did not improve from 0.40796
Epoch 33/300
 - 12s - loss: 1.9968 - acc: 0.9238 - mDice: 0.4215 - val_loss: 2.0161 - val_acc: 0.9419 - val_mDice: 0.4265

Epoch 00033: val_mDice improved from 0.40796 to 0.42653, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 13s - loss: 1.9697 - acc: 0.9242 - mDice: 0.4278 - val_loss: 2.0526 - val_acc: 0.9409 - val_mDice: 0.4214

Epoch 00034: val_mDice did not improve from 0.42653
Epoch 35/300
 - 12s - loss: 1.9417 - acc: 0.9245 - mDice: 0.4353 - val_loss: 2.0751 - val_acc: 0.9405 - val_mDice: 0.4217

Epoch 00035: val_mDice did not improve from 0.42653
Epoch 36/300
 - 12s - loss: 1.9169 - acc: 0.9245 - mDice: 0.4410 - val_loss: 1.9613 - val_acc: 0.9423 - val_mDice: 0.4422

Epoch 00036: val_mDice improved from 0.42653 to 0.44223, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 14s - loss: 1.8956 - acc: 0.9247 - mDice: 0.4462 - val_loss: 2.0661 - val_acc: 0.9417 - val_mDice: 0.4298

Epoch 00037: val_mDice did not improve from 0.44223
Epoch 38/300
 - 13s - loss: 1.8717 - acc: 0.9250 - mDice: 0.4523 - val_loss: 2.2503 - val_acc: 0.9421 - val_mDice: 0.4139

Epoch 00038: val_mDice did not improve from 0.44223
Epoch 39/300
 - 13s - loss: 1.8519 - acc: 0.9254 - mDice: 0.4570 - val_loss: 2.0556 - val_acc: 0.9419 - val_mDice: 0.4339

Epoch 00039: val_mDice did not improve from 0.44223
Epoch 40/300
 - 13s - loss: 1.8317 - acc: 0.9259 - mDice: 0.4619 - val_loss: 2.0572 - val_acc: 0.9428 - val_mDice: 0.4394

Epoch 00040: val_mDice did not improve from 0.44223
Epoch 41/300
 - 13s - loss: 1.8196 - acc: 0.9262 - mDice: 0.4643 - val_loss: 1.9629 - val_acc: 0.9430 - val_mDice: 0.4514

Epoch 00041: val_mDice improved from 0.44223 to 0.45141, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 13s - loss: 1.7949 - acc: 0.9269 - mDice: 0.4710 - val_loss: 2.1028 - val_acc: 0.9422 - val_mDice: 0.4342

Epoch 00042: val_mDice did not improve from 0.45141
Epoch 43/300
 - 13s - loss: 1.7834 - acc: 0.9270 - mDice: 0.4742 - val_loss: 2.0813 - val_acc: 0.9434 - val_mDice: 0.4411

Epoch 00043: val_mDice did not improve from 0.45141
Epoch 44/300
 - 14s - loss: 1.7708 - acc: 0.9274 - mDice: 0.4769 - val_loss: 2.0829 - val_acc: 0.9438 - val_mDice: 0.4426

Epoch 00044: val_mDice did not improve from 0.45141
Epoch 45/300
 - 13s - loss: 1.7558 - acc: 0.9278 - mDice: 0.4809 - val_loss: 1.9801 - val_acc: 0.9443 - val_mDice: 0.4605

Epoch 00045: val_mDice improved from 0.45141 to 0.46045, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 13s - loss: 1.7382 - acc: 0.9283 - mDice: 0.4853 - val_loss: 2.1713 - val_acc: 0.9441 - val_mDice: 0.4426

Epoch 00046: val_mDice did not improve from 0.46045
Epoch 47/300
 - 13s - loss: 1.7353 - acc: 0.9284 - mDice: 0.4861 - val_loss: 2.1917 - val_acc: 0.9440 - val_mDice: 0.4427

Epoch 00047: val_mDice did not improve from 0.46045
Epoch 48/300
 - 13s - loss: 1.7170 - acc: 0.9291 - mDice: 0.4913 - val_loss: 2.2393 - val_acc: 0.9443 - val_mDice: 0.4388

Epoch 00048: val_mDice did not improve from 0.46045
Epoch 49/300
 - 14s - loss: 1.7076 - acc: 0.9293 - mDice: 0.4934 - val_loss: 2.1516 - val_acc: 0.9442 - val_mDice: 0.4446

Epoch 00049: val_mDice did not improve from 0.46045
Epoch 50/300
 - 14s - loss: 1.6984 - acc: 0.9295 - mDice: 0.4956 - val_loss: 2.1844 - val_acc: 0.9438 - val_mDice: 0.4412

Epoch 00050: val_mDice did not improve from 0.46045
Epoch 51/300
 - 13s - loss: 1.6851 - acc: 0.9298 - mDice: 0.4986 - val_loss: 2.1026 - val_acc: 0.9446 - val_mDice: 0.4572

Epoch 00051: val_mDice did not improve from 0.46045
Epoch 52/300
 - 13s - loss: 1.6764 - acc: 0.9301 - mDice: 0.5002 - val_loss: 2.2759 - val_acc: 0.9440 - val_mDice: 0.4376

Epoch 00052: val_mDice did not improve from 0.46045
Epoch 53/300
 - 14s - loss: 1.6674 - acc: 0.9304 - mDice: 0.5036 - val_loss: 2.1238 - val_acc: 0.9456 - val_mDice: 0.4609

Epoch 00053: val_mDice improved from 0.46045 to 0.46088, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 13s - loss: 1.6550 - acc: 0.9308 - mDice: 0.5063 - val_loss: 2.1601 - val_acc: 0.9454 - val_mDice: 0.4558

Epoch 00054: val_mDice did not improve from 0.46088
Epoch 55/300
 - 13s - loss: 1.6439 - acc: 0.9310 - mDice: 0.5089 - val_loss: 2.0827 - val_acc: 0.9458 - val_mDice: 0.4611

Epoch 00055: val_mDice improved from 0.46088 to 0.46114, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 14s - loss: 1.6387 - acc: 0.9313 - mDice: 0.5110 - val_loss: 2.1354 - val_acc: 0.9454 - val_mDice: 0.4554

Epoch 00056: val_mDice did not improve from 0.46114
Epoch 57/300
 - 13s - loss: 1.6286 - acc: 0.9315 - mDice: 0.5133 - val_loss: 2.1539 - val_acc: 0.9453 - val_mDice: 0.4578

Epoch 00057: val_mDice did not improve from 0.46114
Epoch 58/300
 - 13s - loss: 1.6256 - acc: 0.9317 - mDice: 0.5143 - val_loss: 2.1474 - val_acc: 0.9470 - val_mDice: 0.4649

Epoch 00058: val_mDice improved from 0.46114 to 0.46491, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 14s - loss: 1.6163 - acc: 0.9320 - mDice: 0.5168 - val_loss: 2.0370 - val_acc: 0.9455 - val_mDice: 0.4747

Epoch 00059: val_mDice improved from 0.46491 to 0.47466, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 60/300
 - 13s - loss: 1.6094 - acc: 0.9322 - mDice: 0.5182 - val_loss: 2.1175 - val_acc: 0.9451 - val_mDice: 0.4645

Epoch 00060: val_mDice did not improve from 0.47466
Epoch 61/300
 - 13s - loss: 1.6054 - acc: 0.9322 - mDice: 0.5198 - val_loss: 2.2050 - val_acc: 0.9457 - val_mDice: 0.4600

Epoch 00061: val_mDice did not improve from 0.47466
Epoch 62/300
 - 14s - loss: 1.5940 - acc: 0.9326 - mDice: 0.5233 - val_loss: 2.1055 - val_acc: 0.9464 - val_mDice: 0.4677

Epoch 00062: val_mDice did not improve from 0.47466
Epoch 63/300
 - 14s - loss: 1.5842 - acc: 0.9330 - mDice: 0.5252 - val_loss: 2.1614 - val_acc: 0.9453 - val_mDice: 0.4586

Epoch 00063: val_mDice did not improve from 0.47466
Epoch 64/300
 - 13s - loss: 1.5777 - acc: 0.9332 - mDice: 0.5268 - val_loss: 2.1520 - val_acc: 0.9454 - val_mDice: 0.4597

Epoch 00064: val_mDice did not improve from 0.47466
Epoch 65/300
 - 14s - loss: 1.5745 - acc: 0.9333 - mDice: 0.5268 - val_loss: 2.2135 - val_acc: 0.9438 - val_mDice: 0.4594

Epoch 00065: val_mDice did not improve from 0.47466
Epoch 66/300
 - 13s - loss: 1.5665 - acc: 0.9336 - mDice: 0.5299 - val_loss: 2.1329 - val_acc: 0.9470 - val_mDice: 0.4651

Epoch 00066: val_mDice did not improve from 0.47466
Epoch 67/300
 - 13s - loss: 1.5612 - acc: 0.9338 - mDice: 0.5312 - val_loss: 2.1904 - val_acc: 0.9467 - val_mDice: 0.4617

Epoch 00067: val_mDice did not improve from 0.47466
Epoch 68/300
 - 12s - loss: 1.5515 - acc: 0.9341 - mDice: 0.5337 - val_loss: 2.2118 - val_acc: 0.9457 - val_mDice: 0.4599

Epoch 00068: val_mDice did not improve from 0.47466
Epoch 69/300
 - 13s - loss: 1.5495 - acc: 0.9341 - mDice: 0.5345 - val_loss: 2.2908 - val_acc: 0.9461 - val_mDice: 0.4594

Epoch 00069: val_mDice did not improve from 0.47466
Epoch 70/300
 - 13s - loss: 1.5449 - acc: 0.9342 - mDice: 0.5354 - val_loss: 2.2031 - val_acc: 0.9454 - val_mDice: 0.4650

Epoch 00070: val_mDice did not improve from 0.47466
Epoch 71/300
 - 12s - loss: 1.5387 - acc: 0.9344 - mDice: 0.5369 - val_loss: 2.1085 - val_acc: 0.9469 - val_mDice: 0.4721

Epoch 00071: val_mDice did not improve from 0.47466
Epoch 72/300
 - 13s - loss: 1.5364 - acc: 0.9346 - mDice: 0.5382 - val_loss: 2.1192 - val_acc: 0.9466 - val_mDice: 0.4688

Epoch 00072: val_mDice did not improve from 0.47466
Epoch 73/300
 - 12s - loss: 1.5271 - acc: 0.9349 - mDice: 0.5404 - val_loss: 2.0462 - val_acc: 0.9474 - val_mDice: 0.4828

Epoch 00073: val_mDice improved from 0.47466 to 0.48285, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 12s - loss: 1.5205 - acc: 0.9351 - mDice: 0.5422 - val_loss: 2.1264 - val_acc: 0.9464 - val_mDice: 0.4730

Epoch 00074: val_mDice did not improve from 0.48285
Epoch 75/300
 - 13s - loss: 1.5171 - acc: 0.9352 - mDice: 0.5428 - val_loss: 2.3363 - val_acc: 0.9477 - val_mDice: 0.4538

Epoch 00075: val_mDice did not improve from 0.48285
Epoch 76/300
 - 13s - loss: 1.5111 - acc: 0.9354 - mDice: 0.5440 - val_loss: 2.1514 - val_acc: 0.9459 - val_mDice: 0.4707

Epoch 00076: val_mDice did not improve from 0.48285
Epoch 77/300
 - 13s - loss: 1.5088 - acc: 0.9356 - mDice: 0.5447 - val_loss: 2.1365 - val_acc: 0.9465 - val_mDice: 0.4659

Epoch 00077: val_mDice did not improve from 0.48285
Epoch 78/300
 - 13s - loss: 1.5029 - acc: 0.9357 - mDice: 0.5463 - val_loss: 2.2347 - val_acc: 0.9456 - val_mDice: 0.4629

Epoch 00078: val_mDice did not improve from 0.48285
Epoch 79/300
 - 13s - loss: 1.5004 - acc: 0.9359 - mDice: 0.5469 - val_loss: 2.2797 - val_acc: 0.9460 - val_mDice: 0.4621

Epoch 00079: val_mDice did not improve from 0.48285
Epoch 80/300
 - 13s - loss: 1.4929 - acc: 0.9360 - mDice: 0.5490 - val_loss: 2.1511 - val_acc: 0.9448 - val_mDice: 0.4729

Epoch 00080: val_mDice did not improve from 0.48285
Epoch 81/300
 - 12s - loss: 1.4939 - acc: 0.9360 - mDice: 0.5493 - val_loss: 2.2351 - val_acc: 0.9476 - val_mDice: 0.4684

Epoch 00081: val_mDice did not improve from 0.48285
Epoch 82/300
 - 12s - loss: 1.4873 - acc: 0.9363 - mDice: 0.5502 - val_loss: 2.2388 - val_acc: 0.9451 - val_mDice: 0.4590

Epoch 00082: val_mDice did not improve from 0.48285
Epoch 83/300
 - 13s - loss: 1.4848 - acc: 0.9363 - mDice: 0.5511 - val_loss: 2.2277 - val_acc: 0.9465 - val_mDice: 0.4682

Epoch 00083: val_mDice did not improve from 0.48285
Epoch 84/300
 - 12s - loss: 1.4780 - acc: 0.9366 - mDice: 0.5531 - val_loss: 2.3441 - val_acc: 0.9467 - val_mDice: 0.4666

Epoch 00084: val_mDice did not improve from 0.48285
Epoch 85/300
 - 12s - loss: 1.4730 - acc: 0.9369 - mDice: 0.5550 - val_loss: 2.3530 - val_acc: 0.9481 - val_mDice: 0.4604

Epoch 00085: val_mDice did not improve from 0.48285
Epoch 86/300
 - 13s - loss: 1.4639 - acc: 0.9371 - mDice: 0.5573 - val_loss: 2.3269 - val_acc: 0.9474 - val_mDice: 0.4621

Epoch 00086: val_mDice did not improve from 0.48285
Epoch 87/300
 - 12s - loss: 1.4706 - acc: 0.9370 - mDice: 0.5556 - val_loss: 2.1796 - val_acc: 0.9475 - val_mDice: 0.4753

Epoch 00087: val_mDice did not improve from 0.48285
Epoch 88/300
 - 12s - loss: 1.4656 - acc: 0.9372 - mDice: 0.5572 - val_loss: 2.2425 - val_acc: 0.9472 - val_mDice: 0.4624

Epoch 00088: val_mDice did not improve from 0.48285
Epoch 89/300
 - 12s - loss: 1.4611 - acc: 0.9372 - mDice: 0.5580 - val_loss: 2.2549 - val_acc: 0.9467 - val_mDice: 0.4699

Epoch 00089: val_mDice did not improve from 0.48285
Epoch 90/300
 - 12s - loss: 1.4634 - acc: 0.9372 - mDice: 0.5576 - val_loss: 2.2004 - val_acc: 0.9479 - val_mDice: 0.4719

Epoch 00090: val_mDice did not improve from 0.48285
Epoch 91/300
 - 13s - loss: 1.4563 - acc: 0.9374 - mDice: 0.5593 - val_loss: 2.3558 - val_acc: 0.9476 - val_mDice: 0.4581

Epoch 00091: val_mDice did not improve from 0.48285
Epoch 92/300
 - 13s - loss: 1.4555 - acc: 0.9375 - mDice: 0.5595 - val_loss: 2.2716 - val_acc: 0.9467 - val_mDice: 0.4658

Epoch 00092: val_mDice did not improve from 0.48285
Epoch 93/300
 - 13s - loss: 1.4468 - acc: 0.9377 - mDice: 0.5617 - val_loss: 2.2650 - val_acc: 0.9463 - val_mDice: 0.4612

Epoch 00093: val_mDice did not improve from 0.48285
Epoch 94/300
 - 12s - loss: 1.4452 - acc: 0.9377 - mDice: 0.5621 - val_loss: 2.1314 - val_acc: 0.9481 - val_mDice: 0.4851

Epoch 00094: val_mDice improved from 0.48285 to 0.48509, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 95/300
 - 13s - loss: 1.4416 - acc: 0.9379 - mDice: 0.5638 - val_loss: 2.1513 - val_acc: 0.9456 - val_mDice: 0.4726

Epoch 00095: val_mDice did not improve from 0.48509
Epoch 96/300
 - 12s - loss: 1.4387 - acc: 0.9379 - mDice: 0.5645 - val_loss: 2.1360 - val_acc: 0.9481 - val_mDice: 0.4856

Epoch 00096: val_mDice improved from 0.48509 to 0.48563, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 97/300
 - 12s - loss: 1.4341 - acc: 0.9382 - mDice: 0.5655 - val_loss: 2.3478 - val_acc: 0.9458 - val_mDice: 0.4625

Epoch 00097: val_mDice did not improve from 0.48563
Epoch 98/300
 - 13s - loss: 1.4317 - acc: 0.9382 - mDice: 0.5662 - val_loss: 2.2745 - val_acc: 0.9475 - val_mDice: 0.4688

Epoch 00098: val_mDice did not improve from 0.48563
Epoch 99/300
 - 12s - loss: 1.4315 - acc: 0.9384 - mDice: 0.5666 - val_loss: 2.3092 - val_acc: 0.9473 - val_mDice: 0.4659

Epoch 00099: val_mDice did not improve from 0.48563
Epoch 100/300
 - 13s - loss: 1.4301 - acc: 0.9383 - mDice: 0.5677 - val_loss: 2.1239 - val_acc: 0.9465 - val_mDice: 0.4750

Epoch 00100: val_mDice did not improve from 0.48563
Epoch 101/300
 - 13s - loss: 1.4238 - acc: 0.9384 - mDice: 0.5688 - val_loss: 2.3294 - val_acc: 0.9467 - val_mDice: 0.4678

Epoch 00101: val_mDice did not improve from 0.48563
Epoch 102/300
 - 13s - loss: 1.4185 - acc: 0.9386 - mDice: 0.5701 - val_loss: 2.1810 - val_acc: 0.9476 - val_mDice: 0.4801

Epoch 00102: val_mDice did not improve from 0.48563
Epoch 103/300
 - 13s - loss: 1.4143 - acc: 0.9388 - mDice: 0.5716 - val_loss: 2.3644 - val_acc: 0.9479 - val_mDice: 0.4677

Epoch 00103: val_mDice did not improve from 0.48563
Epoch 104/300
 - 13s - loss: 1.4157 - acc: 0.9389 - mDice: 0.5712 - val_loss: 2.1752 - val_acc: 0.9471 - val_mDice: 0.4789

Epoch 00104: val_mDice did not improve from 0.48563
Epoch 105/300
 - 13s - loss: 1.4131 - acc: 0.9388 - mDice: 0.5723 - val_loss: 2.1813 - val_acc: 0.9471 - val_mDice: 0.4731

Epoch 00105: val_mDice did not improve from 0.48563
Epoch 106/300
 - 13s - loss: 1.4115 - acc: 0.9390 - mDice: 0.5728 - val_loss: 2.2945 - val_acc: 0.9445 - val_mDice: 0.4657

Epoch 00106: val_mDice did not improve from 0.48563
Epoch 107/300
 - 13s - loss: 1.4116 - acc: 0.9390 - mDice: 0.5727 - val_loss: 2.1600 - val_acc: 0.9472 - val_mDice: 0.4863

Epoch 00107: val_mDice improved from 0.48563 to 0.48635, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 108/300
 - 12s - loss: 1.4079 - acc: 0.9392 - mDice: 0.5736 - val_loss: 2.2064 - val_acc: 0.9482 - val_mDice: 0.4715

Epoch 00108: val_mDice did not improve from 0.48635
Epoch 109/300
 - 12s - loss: 1.4045 - acc: 0.9393 - mDice: 0.5746 - val_loss: 2.1590 - val_acc: 0.9457 - val_mDice: 0.4734

Epoch 00109: val_mDice did not improve from 0.48635
Epoch 110/300
 - 12s - loss: 1.4001 - acc: 0.9395 - mDice: 0.5760 - val_loss: 2.1823 - val_acc: 0.9478 - val_mDice: 0.4737

Epoch 00110: val_mDice did not improve from 0.48635
Epoch 111/300
 - 13s - loss: 1.4003 - acc: 0.9395 - mDice: 0.5754 - val_loss: 2.2744 - val_acc: 0.9475 - val_mDice: 0.4725

Epoch 00111: val_mDice did not improve from 0.48635
Epoch 112/300
 - 12s - loss: 1.3953 - acc: 0.9395 - mDice: 0.5775 - val_loss: 2.2642 - val_acc: 0.9466 - val_mDice: 0.4690

Epoch 00112: val_mDice did not improve from 0.48635
Epoch 113/300
 - 13s - loss: 1.3930 - acc: 0.9397 - mDice: 0.5780 - val_loss: 2.2740 - val_acc: 0.9458 - val_mDice: 0.4781

Epoch 00113: val_mDice did not improve from 0.48635
Epoch 114/300
 - 12s - loss: 1.3924 - acc: 0.9398 - mDice: 0.5777 - val_loss: 2.1866 - val_acc: 0.9466 - val_mDice: 0.4827

Epoch 00114: val_mDice did not improve from 0.48635
Epoch 115/300
 - 13s - loss: 1.3964 - acc: 0.9398 - mDice: 0.5772 - val_loss: 2.3414 - val_acc: 0.9452 - val_mDice: 0.4660

Epoch 00115: val_mDice did not improve from 0.48635
Epoch 116/300
 - 12s - loss: 1.3901 - acc: 0.9400 - mDice: 0.5792 - val_loss: 2.2836 - val_acc: 0.9469 - val_mDice: 0.4753

Epoch 00116: val_mDice did not improve from 0.48635
Epoch 117/300
 - 13s - loss: 1.3847 - acc: 0.9400 - mDice: 0.5804 - val_loss: 2.2827 - val_acc: 0.9480 - val_mDice: 0.4721

Epoch 00117: val_mDice did not improve from 0.48635
Epoch 118/300
 - 13s - loss: 1.3875 - acc: 0.9400 - mDice: 0.5799 - val_loss: 2.2082 - val_acc: 0.9479 - val_mDice: 0.4806

Epoch 00118: val_mDice did not improve from 0.48635
Epoch 119/300
 - 12s - loss: 1.3812 - acc: 0.9402 - mDice: 0.5818 - val_loss: 2.2576 - val_acc: 0.9457 - val_mDice: 0.4775

Epoch 00119: val_mDice did not improve from 0.48635
Epoch 120/300
 - 13s - loss: 1.3766 - acc: 0.9403 - mDice: 0.5825 - val_loss: 2.2729 - val_acc: 0.9484 - val_mDice: 0.4784

Epoch 00120: val_mDice did not improve from 0.48635
Epoch 121/300
 - 12s - loss: 1.3778 - acc: 0.9403 - mDice: 0.5826 - val_loss: 2.2494 - val_acc: 0.9476 - val_mDice: 0.4759

Epoch 00121: val_mDice did not improve from 0.48635
Epoch 122/300
 - 13s - loss: 1.3744 - acc: 0.9405 - mDice: 0.5832 - val_loss: 2.1566 - val_acc: 0.9459 - val_mDice: 0.4888

Epoch 00122: val_mDice improved from 0.48635 to 0.48884, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 123/300
 - 13s - loss: 1.3802 - acc: 0.9404 - mDice: 0.5823 - val_loss: 2.2752 - val_acc: 0.9465 - val_mDice: 0.4697

Epoch 00123: val_mDice did not improve from 0.48884
Epoch 124/300
 - 13s - loss: 1.3721 - acc: 0.9406 - mDice: 0.5835 - val_loss: 2.1646 - val_acc: 0.9461 - val_mDice: 0.4825

Epoch 00124: val_mDice did not improve from 0.48884
Epoch 125/300
 - 13s - loss: 1.3731 - acc: 0.9407 - mDice: 0.5837 - val_loss: 2.1669 - val_acc: 0.9458 - val_mDice: 0.4817

Epoch 00125: val_mDice did not improve from 0.48884
Epoch 126/300
 - 12s - loss: 1.3703 - acc: 0.9406 - mDice: 0.5847 - val_loss: 2.3229 - val_acc: 0.9466 - val_mDice: 0.4718

Epoch 00126: val_mDice did not improve from 0.48884
Epoch 127/300
 - 12s - loss: 1.3691 - acc: 0.9408 - mDice: 0.5853 - val_loss: 2.4567 - val_acc: 0.9456 - val_mDice: 0.4560

Epoch 00127: val_mDice did not improve from 0.48884
Epoch 128/300
 - 13s - loss: 1.3648 - acc: 0.9409 - mDice: 0.5858 - val_loss: 2.1648 - val_acc: 0.9474 - val_mDice: 0.4817

Epoch 00128: val_mDice did not improve from 0.48884
Epoch 129/300
 - 13s - loss: 1.3678 - acc: 0.9409 - mDice: 0.5858 - val_loss: 2.1277 - val_acc: 0.9475 - val_mDice: 0.4937

Epoch 00129: val_mDice improved from 0.48884 to 0.49367, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 130/300
 - 13s - loss: 1.3620 - acc: 0.9411 - mDice: 0.5871 - val_loss: 2.2266 - val_acc: 0.9479 - val_mDice: 0.4794

Epoch 00130: val_mDice did not improve from 0.49367
Epoch 131/300
 - 13s - loss: 1.3571 - acc: 0.9411 - mDice: 0.5882 - val_loss: 2.2523 - val_acc: 0.9459 - val_mDice: 0.4809

Epoch 00131: val_mDice did not improve from 0.49367
Epoch 132/300
 - 12s - loss: 1.3593 - acc: 0.9411 - mDice: 0.5879 - val_loss: 2.2925 - val_acc: 0.9469 - val_mDice: 0.4768

Epoch 00132: val_mDice did not improve from 0.49367
Epoch 133/300
 - 13s - loss: 1.3606 - acc: 0.9411 - mDice: 0.5873 - val_loss: 2.2871 - val_acc: 0.9488 - val_mDice: 0.4747

Epoch 00133: val_mDice did not improve from 0.49367
Epoch 134/300
 - 12s - loss: 1.3599 - acc: 0.9413 - mDice: 0.5877 - val_loss: 2.2620 - val_acc: 0.9462 - val_mDice: 0.4750

Epoch 00134: val_mDice did not improve from 0.49367
Epoch 135/300
 - 13s - loss: 1.3515 - acc: 0.9413 - mDice: 0.5900 - val_loss: 2.1832 - val_acc: 0.9462 - val_mDice: 0.4808

Epoch 00135: val_mDice did not improve from 0.49367
Epoch 136/300
 - 12s - loss: 1.3535 - acc: 0.9413 - mDice: 0.5894 - val_loss: 2.3284 - val_acc: 0.9466 - val_mDice: 0.4777

Epoch 00136: val_mDice did not improve from 0.49367
Epoch 137/300
 - 13s - loss: 1.3515 - acc: 0.9414 - mDice: 0.5899 - val_loss: 2.3597 - val_acc: 0.9459 - val_mDice: 0.4725

Epoch 00137: val_mDice did not improve from 0.49367
Epoch 138/300
 - 13s - loss: 1.3496 - acc: 0.9415 - mDice: 0.5904 - val_loss: 2.2172 - val_acc: 0.9470 - val_mDice: 0.4852

Epoch 00138: val_mDice did not improve from 0.49367
Epoch 139/300
 - 12s - loss: 1.3531 - acc: 0.9415 - mDice: 0.5898 - val_loss: 2.3460 - val_acc: 0.9458 - val_mDice: 0.4733

Epoch 00139: val_mDice did not improve from 0.49367
Epoch 140/300
 - 13s - loss: 1.3514 - acc: 0.9416 - mDice: 0.5903 - val_loss: 2.4875 - val_acc: 0.9457 - val_mDice: 0.4644

Epoch 00140: val_mDice did not improve from 0.49367
Epoch 141/300
 - 13s - loss: 1.3465 - acc: 0.9417 - mDice: 0.5906 - val_loss: 2.3279 - val_acc: 0.9451 - val_mDice: 0.4842

Epoch 00141: val_mDice did not improve from 0.49367
Epoch 142/300
 - 13s - loss: 1.3453 - acc: 0.9417 - mDice: 0.5919 - val_loss: 2.4373 - val_acc: 0.9468 - val_mDice: 0.4670

Epoch 00142: val_mDice did not improve from 0.49367
Epoch 143/300
 - 13s - loss: 1.3410 - acc: 0.9416 - mDice: 0.5926 - val_loss: 2.3353 - val_acc: 0.9457 - val_mDice: 0.4776

Epoch 00143: val_mDice did not improve from 0.49367
Epoch 144/300
 - 12s - loss: 1.3437 - acc: 0.9417 - mDice: 0.5925 - val_loss: 2.3947 - val_acc: 0.9451 - val_mDice: 0.4668

Epoch 00144: val_mDice did not improve from 0.49367
Epoch 145/300
 - 12s - loss: 1.3433 - acc: 0.9416 - mDice: 0.5921 - val_loss: 2.3515 - val_acc: 0.9455 - val_mDice: 0.4728

Epoch 00145: val_mDice did not improve from 0.49367
Epoch 146/300
 - 13s - loss: 1.3393 - acc: 0.9419 - mDice: 0.5938 - val_loss: 2.4505 - val_acc: 0.9472 - val_mDice: 0.4765

Epoch 00146: val_mDice did not improve from 0.49367
Epoch 147/300
 - 13s - loss: 1.3429 - acc: 0.9417 - mDice: 0.5926 - val_loss: 2.1973 - val_acc: 0.9468 - val_mDice: 0.4886

Epoch 00147: val_mDice did not improve from 0.49367
Epoch 148/300
 - 13s - loss: 1.3403 - acc: 0.9420 - mDice: 0.5938 - val_loss: 2.1968 - val_acc: 0.9480 - val_mDice: 0.4882

Epoch 00148: val_mDice did not improve from 0.49367
Epoch 149/300
 - 13s - loss: 1.3371 - acc: 0.9421 - mDice: 0.5951 - val_loss: 2.5340 - val_acc: 0.9483 - val_mDice: 0.4664

Epoch 00149: val_mDice did not improve from 0.49367
Epoch 150/300
 - 13s - loss: 1.3391 - acc: 0.9420 - mDice: 0.5935 - val_loss: 2.2843 - val_acc: 0.9474 - val_mDice: 0.4898

Epoch 00150: val_mDice did not improve from 0.49367
Epoch 151/300
 - 13s - loss: 1.3319 - acc: 0.9422 - mDice: 0.5958 - val_loss: 2.3657 - val_acc: 0.9456 - val_mDice: 0.4780

Epoch 00151: val_mDice did not improve from 0.49367
Epoch 152/300
 - 13s - loss: 1.3319 - acc: 0.9422 - mDice: 0.5958 - val_loss: 2.3970 - val_acc: 0.9477 - val_mDice: 0.4780

Epoch 00152: val_mDice did not improve from 0.49367
Epoch 153/300
 - 13s - loss: 1.3309 - acc: 0.9422 - mDice: 0.5965 - val_loss: 2.3815 - val_acc: 0.9471 - val_mDice: 0.4844

Epoch 00153: val_mDice did not improve from 0.49367
Epoch 154/300
 - 12s - loss: 1.3266 - acc: 0.9423 - mDice: 0.5974 - val_loss: 2.3488 - val_acc: 0.9469 - val_mDice: 0.4794

Epoch 00154: val_mDice did not improve from 0.49367
Epoch 155/300
 - 13s - loss: 1.3276 - acc: 0.9423 - mDice: 0.5971 - val_loss: 2.3204 - val_acc: 0.9464 - val_mDice: 0.4792

Epoch 00155: val_mDice did not improve from 0.49367
Epoch 156/300
 - 13s - loss: 1.3288 - acc: 0.9424 - mDice: 0.5966 - val_loss: 2.2501 - val_acc: 0.9472 - val_mDice: 0.4852

Epoch 00156: val_mDice did not improve from 0.49367
Epoch 157/300
 - 13s - loss: 1.3290 - acc: 0.9424 - mDice: 0.5962 - val_loss: 2.1770 - val_acc: 0.9479 - val_mDice: 0.4970

Epoch 00157: val_mDice improved from 0.49367 to 0.49697, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 158/300
 - 14s - loss: 1.3267 - acc: 0.9425 - mDice: 0.5971 - val_loss: 2.4486 - val_acc: 0.9478 - val_mDice: 0.4768

Epoch 00158: val_mDice did not improve from 0.49697
Epoch 159/300
 - 14s - loss: 1.3247 - acc: 0.9426 - mDice: 0.5974 - val_loss: 2.3576 - val_acc: 0.9461 - val_mDice: 0.4770

Epoch 00159: val_mDice did not improve from 0.49697
Epoch 160/300
 - 13s - loss: 1.3234 - acc: 0.9426 - mDice: 0.5984 - val_loss: 2.3769 - val_acc: 0.9461 - val_mDice: 0.4813

Epoch 00160: val_mDice did not improve from 0.49697
Epoch 161/300
 - 14s - loss: 1.3198 - acc: 0.9426 - mDice: 0.5992 - val_loss: 2.3038 - val_acc: 0.9485 - val_mDice: 0.4812

Epoch 00161: val_mDice did not improve from 0.49697
Epoch 162/300
 - 14s - loss: 1.3232 - acc: 0.9425 - mDice: 0.5980 - val_loss: 2.4726 - val_acc: 0.9471 - val_mDice: 0.4667

Epoch 00162: val_mDice did not improve from 0.49697
Epoch 163/300
 - 14s - loss: 1.3282 - acc: 0.9426 - mDice: 0.5968 - val_loss: 2.4128 - val_acc: 0.9472 - val_mDice: 0.4728

Epoch 00163: val_mDice did not improve from 0.49697
Epoch 164/300
 - 14s - loss: 1.3222 - acc: 0.9428 - mDice: 0.5989 - val_loss: 2.3858 - val_acc: 0.9479 - val_mDice: 0.4824

Epoch 00164: val_mDice did not improve from 0.49697
Epoch 165/300
 - 14s - loss: 1.3175 - acc: 0.9428 - mDice: 0.6000 - val_loss: 2.3259 - val_acc: 0.9479 - val_mDice: 0.4815

Epoch 00165: val_mDice did not improve from 0.49697
Epoch 166/300
 - 14s - loss: 1.3186 - acc: 0.9428 - mDice: 0.5995 - val_loss: 2.3402 - val_acc: 0.9457 - val_mDice: 0.4704

Epoch 00166: val_mDice did not improve from 0.49697
Epoch 167/300
 - 14s - loss: 1.3182 - acc: 0.9429 - mDice: 0.5996 - val_loss: 2.2864 - val_acc: 0.9468 - val_mDice: 0.4829

Epoch 00167: val_mDice did not improve from 0.49697
Epoch 168/300
 - 14s - loss: 1.3214 - acc: 0.9429 - mDice: 0.5989 - val_loss: 2.4583 - val_acc: 0.9482 - val_mDice: 0.4741

Epoch 00168: val_mDice did not improve from 0.49697
Epoch 169/300
 - 14s - loss: 1.3176 - acc: 0.9429 - mDice: 0.6000 - val_loss: 2.3463 - val_acc: 0.9453 - val_mDice: 0.4796

Epoch 00169: val_mDice did not improve from 0.49697
Epoch 170/300
 - 14s - loss: 1.3142 - acc: 0.9429 - mDice: 0.6008 - val_loss: 2.3572 - val_acc: 0.9472 - val_mDice: 0.4807

Epoch 00170: val_mDice did not improve from 0.49697
Epoch 171/300
 - 14s - loss: 1.3176 - acc: 0.9431 - mDice: 0.5997 - val_loss: 2.2941 - val_acc: 0.9479 - val_mDice: 0.4814

Epoch 00171: val_mDice did not improve from 0.49697
Epoch 172/300
 - 15s - loss: 1.3119 - acc: 0.9432 - mDice: 0.6012 - val_loss: 2.4197 - val_acc: 0.9477 - val_mDice: 0.4753

Epoch 00172: val_mDice did not improve from 0.49697
Epoch 173/300
 - 14s - loss: 1.3112 - acc: 0.9432 - mDice: 0.6015 - val_loss: 2.4690 - val_acc: 0.9459 - val_mDice: 0.4636

Epoch 00173: val_mDice did not improve from 0.49697
Epoch 174/300
 - 14s - loss: 1.3063 - acc: 0.9434 - mDice: 0.6031 - val_loss: 2.3412 - val_acc: 0.9472 - val_mDice: 0.4829

Epoch 00174: val_mDice did not improve from 0.49697
Epoch 175/300
 - 14s - loss: 1.3091 - acc: 0.9433 - mDice: 0.6021 - val_loss: 2.3454 - val_acc: 0.9484 - val_mDice: 0.4820

Epoch 00175: val_mDice did not improve from 0.49697
Epoch 176/300
 - 14s - loss: 1.3066 - acc: 0.9434 - mDice: 0.6034 - val_loss: 2.5148 - val_acc: 0.9459 - val_mDice: 0.4638

Epoch 00176: val_mDice did not improve from 0.49697
Epoch 177/300
 - 14s - loss: 1.3042 - acc: 0.9435 - mDice: 0.6046 - val_loss: 2.3772 - val_acc: 0.9450 - val_mDice: 0.4760

Epoch 00177: val_mDice did not improve from 0.49697
Epoch 178/300
 - 14s - loss: 1.3082 - acc: 0.9433 - mDice: 0.6024 - val_loss: 2.5630 - val_acc: 0.9454 - val_mDice: 0.4638

Epoch 00178: val_mDice did not improve from 0.49697
Epoch 179/300
 - 14s - loss: 1.3043 - acc: 0.9434 - mDice: 0.6036 - val_loss: 2.4470 - val_acc: 0.9468 - val_mDice: 0.4734

Epoch 00179: val_mDice did not improve from 0.49697
Epoch 180/300
 - 13s - loss: 1.3019 - acc: 0.9435 - mDice: 0.6045 - val_loss: 2.2948 - val_acc: 0.9475 - val_mDice: 0.4916

Epoch 00180: val_mDice did not improve from 0.49697
Epoch 181/300
 - 13s - loss: 1.3027 - acc: 0.9435 - mDice: 0.6040 - val_loss: 2.4145 - val_acc: 0.9467 - val_mDice: 0.4729

Epoch 00181: val_mDice did not improve from 0.49697
Epoch 182/300
 - 13s - loss: 1.3043 - acc: 0.9436 - mDice: 0.6036 - val_loss: 2.4186 - val_acc: 0.9482 - val_mDice: 0.4771

Epoch 00182: val_mDice did not improve from 0.49697
Epoch 183/300
 - 13s - loss: 1.3050 - acc: 0.9435 - mDice: 0.6030 - val_loss: 2.3878 - val_acc: 0.9477 - val_mDice: 0.4800

Epoch 00183: val_mDice did not improve from 0.49697
Epoch 184/300
 - 13s - loss: 1.2956 - acc: 0.9437 - mDice: 0.6058 - val_loss: 2.3970 - val_acc: 0.9476 - val_mDice: 0.4780

Epoch 00184: val_mDice did not improve from 0.49697
Epoch 185/300
 - 13s - loss: 1.3006 - acc: 0.9436 - mDice: 0.6055 - val_loss: 2.3364 - val_acc: 0.9466 - val_mDice: 0.4792

Epoch 00185: val_mDice did not improve from 0.49697
Epoch 186/300
 - 12s - loss: 1.3003 - acc: 0.9436 - mDice: 0.6048 - val_loss: 2.3214 - val_acc: 0.9465 - val_mDice: 0.4961

Epoch 00186: val_mDice did not improve from 0.49697
Epoch 187/300
 - 13s - loss: 1.2980 - acc: 0.9438 - mDice: 0.6057 - val_loss: 2.3914 - val_acc: 0.9461 - val_mDice: 0.4837

Epoch 00187: val_mDice did not improve from 0.49697
Restoring model weights from the end of the best epoch
Epoch 00187: early stopping
{'val_loss': [28.15891973399583, 9.235240318255718, 6.397378580530263, 5.3974296580479795, 4.775379591147993, 4.348018598290129, 4.169144551847234, 3.7406688775430177, 3.320552351754471, 3.0550129693313686, 2.8697322267393828, 2.6938830953736543, 2.7328204922169945, 3.207472875797549, 2.708211705671342, 2.6423225735818874, 2.437789878365714, 2.372066810810366, 2.5370085399244084, 2.046524906957616, 2.6588167318418705, 2.3445876417213314, 2.245557912901127, 2.0767210741948814, 2.078402932129759, 2.1863096506235986, 1.9753669427093847, 2.051973771782561, 2.1550030228811936, 2.100244574040674, 2.087870945477619, 2.2651029818550836, 2.0160952353610675, 2.052629429534827, 2.0750951447300405, 1.9612587310748393, 2.06614274818804, 2.2503084603634624, 2.055579738243998, 2.057232979289646, 1.962857704588821, 2.1027634423538295, 2.0812550949650768, 2.0828704434400165, 1.980092311704625, 2.171255461996494, 2.191673457289541, 2.239325206372991, 2.151637711338491, 2.1843905089287783, 2.1026051963507797, 2.275936454368037, 2.123768940984204, 2.1601278915085604, 2.0827110786011764, 2.1354176958180004, 2.1538898611867894, 2.1473665197468335, 2.037003704955458, 2.1174636100257573, 2.205011632855378, 2.105506762445972, 2.1613587866948305, 2.1520164918633147, 2.2135494514550578, 2.132895115367527, 2.190388180024131, 2.2118190959845174, 2.29084261036452, 2.2031232271780516, 2.108453438934667, 2.1191732110923893, 2.0461844665378166, 2.1264324894164526, 2.336294235463915, 2.151360601020259, 2.136539668344253, 2.2347026150985805, 2.279739858051918, 2.151109556912044, 2.2351029505276814, 2.238799204373493, 2.227687405474359, 2.3440713922404712, 2.3530141108528864, 2.326903168715578, 2.179630014483489, 2.242466804035549, 2.254866773189779, 2.200367362805585, 2.355845467338349, 2.2716113308954506, 2.2650395518574635, 2.131389383497185, 2.1513222555874445, 2.135988330041896, 2.3477709759547056, 2.2745208913387533, 2.30916813365574, 2.1239097184975053, 2.329387113368711, 2.1809753545835697, 2.364351673499166, 2.175195897757674, 2.1813246338061116, 2.294477874340292, 2.160017956568542, 2.2064448814818314, 2.15895888259291, 2.1823019928106384, 2.2744404070870172, 2.264155586338576, 2.273958037019442, 2.186591340176886, 2.341449220753249, 2.283566332396182, 2.282652845595802, 2.2081862894516417, 2.257623998812457, 2.272915194154452, 2.249365078004379, 2.1565647364994667, 2.2752000499704033, 2.164558001736689, 2.166855408492701, 2.3229217422741084, 2.4566644356903415, 2.1648006146180565, 2.127717496296547, 2.2266109282743995, 2.252257297824881, 2.2925335521804553, 2.2870847526209315, 2.262038992769891, 2.183159848165246, 2.3283990375156507, 2.3596767500126163, 2.2172310592075966, 2.3460160433913075, 2.4874684157984217, 2.327876838225892, 2.4372680813240604, 2.335284475507683, 2.394663593622559, 2.3515447704485677, 2.450500608156513, 2.1972870307261716, 2.1968101496137056, 2.5339804574763973, 2.2843462009003708, 2.365680493456025, 2.396981894636953, 2.381466588494498, 2.348782302281044, 2.320435522654869, 2.2500661804689375, 2.177029560398123, 2.448640931251995, 2.3576461762689345, 2.376916309974713, 2.30375439361487, 2.4726350573854075, 2.412789147659387, 2.3858108653702548, 2.3258931357101353, 2.3401694137956843, 2.286420036294607, 2.4583155802508307, 2.3462595926316756, 2.357217864617289, 2.2940617899654963, 2.419738383266513, 2.4690212577414914, 2.341180536334075, 2.3454185051625003, 2.5147873755939845, 2.3771748582744068, 2.5629877711141575, 2.4470057540765686, 2.294835466246365, 2.414517463918505, 2.418642543547646, 2.3877773884288427, 2.3969865564527457, 2.336435404569743, 2.321362336920626, 2.3913804565728043], 'val_acc': [0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136288948565222, 0.9129326110445587, 0.9148747255016305, 0.9184655021022818, 0.9191473039834859, 0.9179675928707229, 0.9186886355863603, 0.9258764056520089, 0.927169779825477, 0.9349814700014765, 0.9359049983530737, 0.9358120360853952, 0.9394276192068388, 0.9376446205144487, 0.9399999260236431, 0.9409317074541274, 0.9412230309827367, 0.9423015011089474, 0.9411424238588557, 0.942274635730509, 0.9423551969022058, 0.941596976871597, 0.9422539878823903, 0.9417374806697142, 0.940644541599231, 0.9418986339808842, 0.9408883142737703, 0.9405391542605182, 0.9422519040507311, 0.941714735004489, 0.9420515049103252, 0.9419213363578199, 0.9428221413543104, 0.9429626041950461, 0.9421899448559937, 0.9433737773469041, 0.9437911150841739, 0.9443324314815372, 0.9440968865788849, 0.9439605154804678, 0.9442518396750509, 0.9441588923917802, 0.943817953823665, 0.9446340649487586, 0.9439729209052784, 0.9455823718502535, 0.945381976039716, 0.945799336753078, 0.9453902457679451, 0.9452910792894204, 0.9470265277937138, 0.9454935266318933, 0.9450823897756012, 0.9457021981644231, 0.9464335901111198, 0.9453138043094613, 0.9453819713779001, 0.9437849138702095, 0.9470368673681547, 0.9467207726153581, 0.9456939467504704, 0.9460658197962372, 0.9453799218438858, 0.9468633312752793, 0.9466009283198991, 0.9474294188968296, 0.9463695604041968, 0.9476979924313849, 0.9459046854653173, 0.9464625193420069, 0.9455927160865102, 0.9460017924202221, 0.9448468432080146, 0.9475554584124901, 0.9450927043760289, 0.9464666676920885, 0.946741434781911, 0.9480740218189175, 0.9473839781803792, 0.9475120892071857, 0.9472331394696368, 0.9466587771250549, 0.9478839625859393, 0.9475885190111298, 0.9467269678355595, 0.946280699868442, 0.9481153295026811, 0.9455658427163876, 0.9481277462490444, 0.9457931298783372, 0.9474521499106338, 0.9473178693036127, 0.946501777491756, 0.9466608682824247, 0.9475657726799309, 0.9479273304592963, 0.9471484332777268, 0.94714431922529, 0.9445245662215036, 0.9472476227323436, 0.9481504719350591, 0.9457249155257668, 0.9477641219533356, 0.9474686980247498, 0.946613350061065, 0.9458137644069821, 0.9465679003539698, 0.9451670972994586, 0.9469170184108798, 0.9480161840023276, 0.9479418083942136, 0.945698093102631, 0.9483880680366601, 0.9476132975610275, 0.9458902328373999, 0.9465306820816168, 0.9460513788228594, 0.9458447415069495, 0.9466071275359426, 0.9455947566298799, 0.9474294395420139, 0.947505848367787, 0.9478694799892063, 0.9458654326433577, 0.9469397594142892, 0.9488198943644263, 0.9461960039991241, 0.9462331713244901, 0.9465947294368424, 0.9458902045335184, 0.9470472009488324, 0.9458261744936085, 0.9456918855619164, 0.9450637858007207, 0.946795160876972, 0.9456670943585188, 0.945148533615986, 0.9455472447352702, 0.9472372811599816, 0.9468095961895735, 0.9480492605843358, 0.9483116448924528, 0.9473798274993896, 0.9456443653426356, 0.9476897483431427, 0.9471277904244109, 0.9469149815303654, 0.9463881560544062, 0.9472352193054541, 0.9479314771444438, 0.9478240292831506, 0.9460782205592321, 0.9461154128586113, 0.9485182259335864, 0.947142258036736, 0.9472269445824224, 0.9479128618480107, 0.9479438739115965, 0.9457414766263695, 0.946824060805017, 0.9481814546958028, 0.9453323713228023, 0.9472104127846617, 0.9479025552392671, 0.9477186599257272, 0.945896403416575, 0.9471959664834945, 0.9483694707215166, 0.9459377350753913, 0.944987342344316, 0.9453716377972224, 0.946791002870272, 0.9474872766926302, 0.9466670285390076, 0.9482083377225439, 0.947733132532855, 0.9475988569206366, 0.946600938975478, 0.9464542352953437, 0.9460658364455793], 'val_mDice': [0.008155989580314252, 0.008618712503275726, 0.010846890222968003, 0.021061555518867583, 0.030396567459878975, 0.04604481085718677, 0.047510505804802455, 0.07315696925757317, 0.10569084181798903, 0.13855331678297267, 0.1622789558751623, 0.18567548800447134, 0.19939433404520238, 0.18627614698596506, 0.2391941374240641, 0.25895101997439424, 0.30065291936837096, 0.3080049972960403, 0.30693140675901703, 0.3578158961661035, 0.3126669149825027, 0.34204944368847257, 0.3604916037793932, 0.3714552092152601, 0.389605401281538, 0.37512067260022935, 0.4074245804514965, 0.40212851879317, 0.39153613957612876, 0.40795566865851757, 0.405217479061148, 0.3938773863808403, 0.4265256009621327, 0.42136306269874785, 0.4217481606499443, 0.4422297211332694, 0.42983449404466084, 0.41390285012442307, 0.4339483325374859, 0.43943888648262236, 0.451413855206367, 0.43415943737136586, 0.441071933541218, 0.44264320904316184, 0.46045114940771176, 0.44264632180416386, 0.4427109359029951, 0.4388140095345801, 0.44461429468746294, 0.4411876078424507, 0.4571775947203183, 0.4376181590490501, 0.46087505780784777, 0.45577255540719913, 0.4611414444513161, 0.4553621276463876, 0.4578280978362653, 0.4649127390464591, 0.47466294126137676, 0.4645262578679197, 0.4599525364084617, 0.46765167370188837, 0.45862964178596793, 0.45967956395122594, 0.45938022942516393, 0.46511581223770226, 0.4617023796009618, 0.459870446993652, 0.4593716103937373, 0.4649587231974362, 0.47214327927408273, 0.46879843793101816, 0.4828454535766687, 0.47297213097524377, 0.4537886254614292, 0.4707016089109069, 0.46594762668929285, 0.4629254730720094, 0.4620966751482234, 0.4729210148310528, 0.4683696885681685, 0.4590346741609733, 0.46823662253065484, 0.4666229829441902, 0.46035370780103035, 0.46211076132412066, 0.4752795393240518, 0.46244315428440796, 0.46987042297198117, 0.4719173525298774, 0.4580968278746365, 0.4657961406521291, 0.46119221262425686, 0.4850859139218677, 0.4725601932855958, 0.4856341401291959, 0.46246469020843506, 0.46877795774177466, 0.46592126811682844, 0.47496615941298076, 0.4677973056638707, 0.48011793121279284, 0.467722412261217, 0.4789472538665686, 0.4731043898526517, 0.46566742492121693, 0.48634790974622333, 0.47149545253988084, 0.47337433479351704, 0.47371028804912246, 0.47250101652891274, 0.46904304656902507, 0.47812529449356334, 0.48269475771728176, 0.46597713931312773, 0.4752938669154098, 0.4720712180910164, 0.48063775847078033, 0.47748583671766953, 0.47840320281476284, 0.4758659577902469, 0.4888406599034144, 0.46970392205861694, 0.48246793906781926, 0.481664878030063, 0.47178963009871583, 0.4560189480222137, 0.4816928505897522, 0.493668991094195, 0.4793994556592163, 0.4808896573562196, 0.4767758594877893, 0.47468962149913085, 0.47500956624579826, 0.4807675625691867, 0.4777361009706998, 0.47247525300393556, 0.48521293641468666, 0.473259505945877, 0.4643949260258808, 0.4842275868913981, 0.46695128715904066, 0.4776417353299743, 0.46682676190104566, 0.47282869056616417, 0.4765015530852632, 0.4886323768000363, 0.4882224282072909, 0.4664027008264424, 0.4897906821866275, 0.47796692295447407, 0.47802151997662123, 0.48437628216583634, 0.47937437176038433, 0.4792142467125834, 0.4852330178521865, 0.4969743546826879, 0.47680365589743884, 0.4769914033692642, 0.48126371895800757, 0.48124340026738255, 0.46665071775127387, 0.4727915498797454, 0.482416069707391, 0.48150739989467173, 0.4703679101427174, 0.4828639533266675, 0.47405506645500994, 0.47964332822980826, 0.4807170804319435, 0.4814016525971823, 0.47527564837279934, 0.46361265402266433, 0.48289942974484834, 0.48201391900051904, 0.4637830803514193, 0.47596554616310077, 0.46384337727583985, 0.47338008347836286, 0.49164669273951866, 0.4728750163616415, 0.4770573306017082, 0.4800261082595953, 0.4780026416205827, 0.4791999705676926, 0.4961245666003094, 0.4836930446118616], 'loss': [230.72057835473117, 30.64344340465108, 15.34104317272484, 10.688693291156303, 8.60928148179589, 7.384975198657741, 6.501799908866485, 5.816709900738581, 5.201471672475705, 4.72874611924205, 4.343284699023598, 4.033633858697502, 3.7619586594061527, 3.5208837183645936, 3.309080986511499, 3.1253491284040757, 2.964679811133401, 2.8368817621453593, 2.7252886323041694, 2.6299153954938217, 2.5448362459629217, 2.4744547323544075, 2.407322799475791, 2.3504611536812585, 2.3022451770701027, 2.2520824824599592, 2.21006559836086, 2.166248337141238, 2.1388291596236635, 2.09354071011114, 2.06637131221855, 2.026381753107917, 1.9968064385271755, 1.9696785849973297, 1.9416552447109243, 1.9168691005226859, 1.8956384168043354, 1.8716602896132584, 1.8519004719117507, 1.831654169435991, 1.8195932137343522, 1.7949000061821117, 1.7833607638299702, 1.7708057378879865, 1.755767117891363, 1.7381834676014976, 1.7352812464451077, 1.7169965705180257, 1.7075983295869959, 1.6983692005863442, 1.685091345134004, 1.676382554355877, 1.6674138336917634, 1.655042846947803, 1.6439246484594572, 1.6387071311425152, 1.628638277372746, 1.625579278094207, 1.6162648723637199, 1.6094359138563668, 1.6053509763445277, 1.5939837377300308, 1.5842366595003614, 1.5776572431603078, 1.5744689963208156, 1.5664659936108247, 1.5612391553266098, 1.5514844287896778, 1.5495270069151506, 1.544872923076143, 1.5386797754529917, 1.5364472631450883, 1.5270679969393113, 1.5204607859675086, 1.5171183985917438, 1.511099447508561, 1.5088334373708163, 1.50287618879451, 1.50039146388449, 1.492902157622987, 1.493924597812552, 1.4872843402123233, 1.4847936050174193, 1.477977910064403, 1.4730117928714939, 1.4638811322989713, 1.4706362105813084, 1.4655867542136787, 1.4611488700231159, 1.463395598257921, 1.4562998830697536, 1.455469626628796, 1.4468016837535422, 1.4452346404986498, 1.4415509549856356, 1.4387449222279494, 1.4341125523697986, 1.4316876138206478, 1.4314962033936935, 1.4301244651340892, 1.4238010883545553, 1.4184597029865686, 1.4143326010788964, 1.4157416767741877, 1.4131018819291377, 1.4115073570699501, 1.4115627026948772, 1.407903906966105, 1.4045263653220672, 1.4000708617699718, 1.4002714642875367, 1.3952984923624563, 1.3930285612317928, 1.3923892647467837, 1.3964303304839905, 1.390132019458906, 1.3847443036319942, 1.387515600847202, 1.3811712279882535, 1.3766118470415247, 1.3778365970884985, 1.3743652144773524, 1.3801742713310239, 1.3721015436717754, 1.3731341896756504, 1.370255038698127, 1.369098651945432, 1.3648420727452282, 1.3677955569735825, 1.361959861505331, 1.357146159001902, 1.3593463903808054, 1.3605633530473404, 1.3599348948104573, 1.3514944036005239, 1.3534747024914984, 1.3514911058285248, 1.3496017380059657, 1.3530992412059424, 1.3513643875743275, 1.3464736302793237, 1.3453192606216928, 1.340954619219838, 1.3436924492499323, 1.3433312601555174, 1.3393123038979464, 1.34289921566874, 1.3402761581072484, 1.337106908330548, 1.3390568482525878, 1.3318639831062262, 1.3319042635395093, 1.3309091595703268, 1.3265792560287826, 1.327566016130046, 1.3288109337677787, 1.3289812693447338, 1.3266987447988998, 1.3246603603521714, 1.3234468391372594, 1.3197916860635241, 1.3232307872262683, 1.3281816034221967, 1.3221679060269944, 1.3175181416597548, 1.318559471009558, 1.3181524996787113, 1.3213780522443885, 1.3175611963563478, 1.3142034142477426, 1.3175591280774466, 1.3119411320606094, 1.3111604362146754, 1.3063361809849836, 1.309051948057645, 1.306606427061487, 1.304201147485017, 1.3082338336843609, 1.3042890270007839, 1.3019374003825082, 1.302703919529389, 1.304338510639884, 1.3049943817316931, 1.2955837921537636, 1.3005629861142523, 1.3002840787209649, 1.29799467160625], 'acc': [0.5857853011756828, 0.8756995816894569, 0.88524964772683, 0.8860705465307216, 0.8860398896132268, 0.885850337233901, 0.8857039864381008, 0.8861978798869337, 0.8874933390463887, 0.8892580153577099, 0.8908289093628706, 0.8926143740609115, 0.8951423038137056, 0.8979969718676885, 0.9010067715908953, 0.903700708904711, 0.9062820361380055, 0.9085546671146336, 0.9106176656209646, 0.91238697865165, 0.9137480940884232, 0.9152569924166478, 0.9166443755044819, 0.9177980732699637, 0.9188008715070077, 0.919537213430264, 0.9202701928834888, 0.921098402110549, 0.9216484286661638, 0.9223722449432472, 0.9229757722430055, 0.9235094294303396, 0.9238347949775747, 0.9241891917330034, 0.9245054364061571, 0.9244720607397051, 0.9246521883914365, 0.924985189348529, 0.9253922956784912, 0.9258709678618219, 0.9262101928118196, 0.9268774877290022, 0.9269915698212311, 0.9274289464549323, 0.9278284373914979, 0.928343324501279, 0.9283690831739264, 0.9290544675180942, 0.9292915172614821, 0.929460925121244, 0.9297824922674564, 0.9300697534589828, 0.9303931895083289, 0.9308223066445004, 0.930980705139093, 0.9312815184583072, 0.9314842258466562, 0.9316935784241115, 0.9319812026939831, 0.9321664669312304, 0.9322093295853653, 0.9325580009544904, 0.9329529208436029, 0.9331619733461958, 0.9333034271339752, 0.9336201782339792, 0.9338092473388062, 0.9341403398532752, 0.9340976587009809, 0.9341548001426646, 0.934438496566755, 0.9345729416228673, 0.9348607057547571, 0.93511189610524, 0.9352008412914152, 0.9354197018371546, 0.9355833321042027, 0.9357040405539631, 0.9358640853748765, 0.9360306623662312, 0.935986530202308, 0.9362895809599702, 0.9362971972720594, 0.936597710305404, 0.9368792331969905, 0.9371015948996319, 0.9370407094610561, 0.937227141327357, 0.9372497798765311, 0.9372053442973404, 0.9374414087023574, 0.9374687612189829, 0.9377428099004202, 0.9377130801190374, 0.9379265413916155, 0.9379350818792244, 0.9381939452864831, 0.9382236538924889, 0.9383829535238689, 0.9383267802867772, 0.9384314546098923, 0.9386318072917693, 0.9388062332062269, 0.93887042531481, 0.938824399770926, 0.9389747792836544, 0.9389847468968849, 0.9391933773624943, 0.9392998249830358, 0.9394826510718534, 0.939485934660927, 0.9395356861816941, 0.9396888238335414, 0.939784313399355, 0.9397503564666931, 0.9399799724483755, 0.940048675188201, 0.9400455720881337, 0.9402108561511549, 0.9403332974628831, 0.9403366621793287, 0.9404575744327601, 0.9404278851160524, 0.940626985629681, 0.9406655342300618, 0.940648598680958, 0.9408167205009577, 0.940933786097659, 0.9409172257062468, 0.9410656682157249, 0.9410884722172127, 0.9411252330580631, 0.941080375172449, 0.941298388670644, 0.9412760132928084, 0.9413223193236007, 0.9414447198460275, 0.9414820207709886, 0.9414890911784166, 0.941570222695911, 0.9416614034279623, 0.9416702078140728, 0.9416407590552318, 0.9416804169655417, 0.9416468032181221, 0.9419103360573429, 0.9417439066800837, 0.9419866728046401, 0.942051086587575, 0.9420419019179227, 0.9421993722440916, 0.9421654921277638, 0.9422318573598216, 0.9423358125024668, 0.9423283577147122, 0.94244411358042, 0.9423594770754327, 0.9424727951920011, 0.9425527158369653, 0.9425504617776481, 0.9425799071084349, 0.942534368143746, 0.94256107393975, 0.9427624932853922, 0.9428496278810654, 0.9428275499486188, 0.9429381056377542, 0.9428620114707459, 0.9428981852832686, 0.9429219140803123, 0.9430746499453682, 0.9431662322824202, 0.9432380989118441, 0.9433695621580335, 0.9433172570744291, 0.943407350480248, 0.9434791556741344, 0.9433424947282716, 0.9433778016780575, 0.9435282021032154, 0.9434810293135667, 0.9435792397027, 0.9434907765671823, 0.9437012601266219, 0.9436242368400347, 0.9436145890519338, 0.943780436581902], 'mDice': [0.01306680220834447, 0.011926574407417357, 0.013409463802654755, 0.02258063367024258, 0.03136362752869922, 0.03801268406070712, 0.0472433441733231, 0.060643790544692766, 0.0818329805148225, 0.1011049124521242, 0.12039696977104888, 0.1412903368278277, 0.163751233866399, 0.1855974099431693, 0.20991421690106515, 0.2319753535607627, 0.2527445484208201, 0.27128559163658, 0.28783800749534366, 0.30175496003420144, 0.31508440915960795, 0.32677177441684346, 0.33769378522649895, 0.34758029624849784, 0.357100585311932, 0.3666309030848997, 0.375422391261185, 0.38360435264990045, 0.39011824774746023, 0.39911206555089934, 0.40627071932246306, 0.4147775820232504, 0.4215151759691406, 0.4278451865417727, 0.4353055062717838, 0.44100823686972246, 0.44622407158172556, 0.4522611342054405, 0.45702479864204315, 0.46185079801446843, 0.46431517433505987, 0.4709723480214195, 0.4741598219320565, 0.47694534524470966, 0.48087602104862026, 0.48534345870625445, 0.48609632301853867, 0.4913484680809088, 0.49343559426566913, 0.4956048166470735, 0.49855554015228326, 0.5002101575501778, 0.5036278672451014, 0.5062556300448421, 0.5089009027875565, 0.5109722029766118, 0.5132987065406401, 0.5142785032475529, 0.5167699004886885, 0.518205698311936, 0.5198383987238838, 0.5232569428968193, 0.5252053705275134, 0.5268081392469791, 0.5267631951828069, 0.529881284281187, 0.5312364524527902, 0.5336792502980575, 0.534453003580914, 0.5353748564217372, 0.536894590797121, 0.5382315417493287, 0.5403761624199173, 0.5421912820015485, 0.5428038616337799, 0.5440348814751911, 0.5446540697500938, 0.5463081782507316, 0.5469446812642893, 0.5490284149543269, 0.5492740000562272, 0.5502471734830662, 0.551123200129803, 0.5531453240002807, 0.5549542919027735, 0.5572877198345721, 0.5555544052305295, 0.557174479005046, 0.558026295583185, 0.5575796032054486, 0.5593295192823685, 0.5595473860429592, 0.5616851841778181, 0.5621364164193999, 0.563777142844619, 0.5645381216065088, 0.5655281368303609, 0.5661657035665687, 0.5666351868341343, 0.5676732524197954, 0.568790367109818, 0.5701158368621905, 0.5715728645347821, 0.571157673167985, 0.5723191854818851, 0.5728391144414735, 0.5726856470828703, 0.5735549985677496, 0.5746425478606824, 0.575959435651183, 0.5754457202899639, 0.5774876058871018, 0.5780098089224684, 0.5776922010880696, 0.5771843391977282, 0.5791955194009181, 0.580431195306178, 0.5799418570630321, 0.5818008087012615, 0.5824627486600263, 0.5826241031892094, 0.5831946919319008, 0.5823267490819262, 0.5835228388809793, 0.5837331465026742, 0.5846759934848926, 0.5853451381522543, 0.5858312836497653, 0.5858188090932881, 0.5871079305562116, 0.5881679782056645, 0.5878627091527706, 0.5873207084091587, 0.5876725978317893, 0.5900327003337831, 0.5893640980066852, 0.5899168265150897, 0.5903950827514273, 0.5897754825646371, 0.5902917788092473, 0.5905717096390545, 0.5919330100349388, 0.592619760267213, 0.5925381655663449, 0.5920527715788422, 0.5938442115670461, 0.5926436264266519, 0.5938415736143421, 0.5951178720116739, 0.5934905412604714, 0.595795443995405, 0.5958200228655106, 0.5964575609534428, 0.5973703913890966, 0.5971198329692585, 0.5966451760568323, 0.596174944824178, 0.5971092953218898, 0.5974407931421001, 0.5984099815694421, 0.5991977355908831, 0.5979909984045224, 0.5968247346061032, 0.5989105409120502, 0.5999767947746228, 0.5995190687658973, 0.5995777409052409, 0.598860703644478, 0.5999713653501657, 0.6007582440263831, 0.5996935520418316, 0.6011810941079636, 0.6015459081425555, 0.6030982275666459, 0.6021372548934324, 0.603371966580512, 0.6046426098861308, 0.6024152467202909, 0.6035995784456918, 0.6044731144356862, 0.6039806357669347, 0.603579349351224, 0.6030244253168802, 0.6058105479189984, 0.6054771229869029, 0.6048171848625173, 0.6056801124138741]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.75s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.46s/it]predicting test subjects: 100%|██████████| 3/3 [00:06<00:00,  2.21s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:31,  1.59s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:57,  1.69s/it]predicting train subjects:   1%|          | 3/285 [00:05<07:50,  1.67s/it]predicting train subjects:   1%|▏         | 4/285 [00:07<08:20,  1.78s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<08:07,  1.74s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<08:33,  1.84s/it]predicting train subjects:   2%|▏         | 7/285 [00:13<09:08,  1.97s/it]predicting train subjects:   3%|▎         | 8/285 [00:15<09:16,  2.01s/it]predicting train subjects:   3%|▎         | 9/285 [00:16<08:50,  1.92s/it]predicting train subjects:   4%|▎         | 10/285 [00:19<09:13,  2.01s/it]predicting train subjects:   4%|▍         | 11/285 [00:21<09:29,  2.08s/it]predicting train subjects:   4%|▍         | 12/285 [00:23<09:58,  2.19s/it]predicting train subjects:   5%|▍         | 13/285 [00:26<09:58,  2.20s/it]predicting train subjects:   5%|▍         | 14/285 [00:28<09:50,  2.18s/it]predicting train subjects:   5%|▌         | 15/285 [00:30<09:50,  2.19s/it]predicting train subjects:   6%|▌         | 16/285 [00:32<09:55,  2.21s/it]predicting train subjects:   6%|▌         | 17/285 [00:35<10:07,  2.27s/it]predicting train subjects:   6%|▋         | 18/285 [00:37<09:57,  2.24s/it]predicting train subjects:   7%|▋         | 19/285 [00:39<09:48,  2.21s/it]predicting train subjects:   7%|▋         | 20/285 [00:41<09:59,  2.26s/it]predicting train subjects:   7%|▋         | 21/285 [00:44<09:56,  2.26s/it]predicting train subjects:   8%|▊         | 22/285 [00:46<09:51,  2.25s/it]predicting train subjects:   8%|▊         | 23/285 [00:48<09:38,  2.21s/it]predicting train subjects:   8%|▊         | 24/285 [00:50<09:29,  2.18s/it]predicting train subjects:   9%|▉         | 25/285 [00:52<09:29,  2.19s/it]predicting train subjects:   9%|▉         | 26/285 [00:54<09:19,  2.16s/it]predicting train subjects:   9%|▉         | 27/285 [00:57<09:19,  2.17s/it]predicting train subjects:  10%|▉         | 28/285 [00:58<08:59,  2.10s/it]predicting train subjects:  10%|█         | 29/285 [01:00<08:46,  2.05s/it]predicting train subjects:  11%|█         | 30/285 [01:02<08:41,  2.04s/it]predicting train subjects:  11%|█         | 31/285 [01:05<08:41,  2.05s/it]predicting train subjects:  11%|█         | 32/285 [01:06<08:29,  2.02s/it]predicting train subjects:  12%|█▏        | 33/285 [01:08<08:27,  2.01s/it]predicting train subjects:  12%|█▏        | 34/285 [01:10<08:20,  1.99s/it]predicting train subjects:  12%|█▏        | 35/285 [01:12<08:20,  2.00s/it]predicting train subjects:  13%|█▎        | 36/285 [01:14<08:21,  2.01s/it]predicting train subjects:  13%|█▎        | 37/285 [01:17<08:24,  2.03s/it]predicting train subjects:  13%|█▎        | 38/285 [01:19<08:24,  2.04s/it]predicting train subjects:  14%|█▎        | 39/285 [01:21<08:20,  2.03s/it]predicting train subjects:  14%|█▍        | 40/285 [01:23<08:26,  2.07s/it]predicting train subjects:  14%|█▍        | 41/285 [01:25<08:31,  2.09s/it]predicting train subjects:  15%|█▍        | 42/285 [01:27<08:29,  2.10s/it]predicting train subjects:  15%|█▌        | 43/285 [01:29<08:26,  2.09s/it]predicting train subjects:  15%|█▌        | 44/285 [01:31<08:20,  2.08s/it]predicting train subjects:  16%|█▌        | 45/285 [01:33<08:16,  2.07s/it]predicting train subjects:  16%|█▌        | 46/285 [01:35<07:59,  2.01s/it]predicting train subjects:  16%|█▋        | 47/285 [01:37<07:44,  1.95s/it]predicting train subjects:  17%|█▋        | 48/285 [01:39<07:28,  1.89s/it]predicting train subjects:  17%|█▋        | 49/285 [01:40<07:15,  1.85s/it]predicting train subjects:  18%|█▊        | 50/285 [01:42<07:10,  1.83s/it]predicting train subjects:  18%|█▊        | 51/285 [01:44<07:10,  1.84s/it]predicting train subjects:  18%|█▊        | 52/285 [01:46<07:04,  1.82s/it]predicting train subjects:  19%|█▊        | 53/285 [01:48<06:56,  1.79s/it]predicting train subjects:  19%|█▉        | 54/285 [01:49<06:51,  1.78s/it]predicting train subjects:  19%|█▉        | 55/285 [01:51<06:54,  1.80s/it]predicting train subjects:  20%|█▉        | 56/285 [01:53<06:49,  1.79s/it]predicting train subjects:  20%|██        | 57/285 [01:55<06:46,  1.78s/it]predicting train subjects:  20%|██        | 58/285 [01:56<06:42,  1.78s/it]predicting train subjects:  21%|██        | 59/285 [01:58<06:42,  1.78s/it]predicting train subjects:  21%|██        | 60/285 [02:00<06:44,  1.80s/it]predicting train subjects:  21%|██▏       | 61/285 [02:02<06:46,  1.82s/it]predicting train subjects:  22%|██▏       | 62/285 [02:04<06:44,  1.81s/it]predicting train subjects:  22%|██▏       | 63/285 [02:06<06:44,  1.82s/it]predicting train subjects:  22%|██▏       | 64/285 [02:07<06:44,  1.83s/it]predicting train subjects:  23%|██▎       | 65/285 [02:09<06:57,  1.90s/it]predicting train subjects:  23%|██▎       | 66/285 [02:11<07:00,  1.92s/it]predicting train subjects:  24%|██▎       | 67/285 [02:13<06:54,  1.90s/it]predicting train subjects:  24%|██▍       | 68/285 [02:15<06:51,  1.90s/it]predicting train subjects:  24%|██▍       | 69/285 [02:17<06:43,  1.87s/it]predicting train subjects:  25%|██▍       | 70/285 [02:19<06:37,  1.85s/it]predicting train subjects:  25%|██▍       | 71/285 [02:21<06:35,  1.85s/it]predicting train subjects:  25%|██▌       | 72/285 [02:22<06:33,  1.85s/it]predicting train subjects:  26%|██▌       | 73/285 [02:24<06:33,  1.86s/it]predicting train subjects:  26%|██▌       | 74/285 [02:26<06:29,  1.85s/it]predicting train subjects:  26%|██▋       | 75/285 [02:28<06:34,  1.88s/it]predicting train subjects:  27%|██▋       | 76/285 [02:30<06:31,  1.87s/it]predicting train subjects:  27%|██▋       | 77/285 [02:32<06:25,  1.85s/it]predicting train subjects:  27%|██▋       | 78/285 [02:34<06:27,  1.87s/it]predicting train subjects:  28%|██▊       | 79/285 [02:36<06:25,  1.87s/it]predicting train subjects:  28%|██▊       | 80/285 [02:37<06:23,  1.87s/it]predicting train subjects:  28%|██▊       | 81/285 [02:39<06:21,  1.87s/it]predicting train subjects:  29%|██▉       | 82/285 [02:41<06:22,  1.88s/it]predicting train subjects:  29%|██▉       | 83/285 [02:43<06:19,  1.88s/it]predicting train subjects:  29%|██▉       | 84/285 [02:45<06:14,  1.86s/it]predicting train subjects:  30%|██▉       | 85/285 [02:47<06:30,  1.95s/it]predicting train subjects:  30%|███       | 86/285 [02:49<06:37,  2.00s/it]predicting train subjects:  31%|███       | 87/285 [02:51<06:40,  2.02s/it]predicting train subjects:  31%|███       | 88/285 [02:53<06:43,  2.05s/it]predicting train subjects:  31%|███       | 89/285 [02:56<06:45,  2.07s/it]predicting train subjects:  32%|███▏      | 90/285 [02:58<06:41,  2.06s/it]predicting train subjects:  32%|███▏      | 91/285 [03:00<06:44,  2.09s/it]predicting train subjects:  32%|███▏      | 92/285 [03:02<06:40,  2.07s/it]predicting train subjects:  33%|███▎      | 93/285 [03:04<06:36,  2.07s/it]predicting train subjects:  33%|███▎      | 94/285 [03:06<06:37,  2.08s/it]predicting train subjects:  33%|███▎      | 95/285 [03:08<06:39,  2.10s/it]predicting train subjects:  34%|███▎      | 96/285 [03:10<06:37,  2.10s/it]predicting train subjects:  34%|███▍      | 97/285 [03:12<06:44,  2.15s/it]predicting train subjects:  34%|███▍      | 98/285 [03:15<06:41,  2.15s/it]predicting train subjects:  35%|███▍      | 99/285 [03:17<06:36,  2.13s/it]predicting train subjects:  35%|███▌      | 100/285 [03:19<06:35,  2.14s/it]predicting train subjects:  35%|███▌      | 101/285 [03:21<06:32,  2.13s/it]predicting train subjects:  36%|███▌      | 102/285 [03:23<06:34,  2.15s/it]predicting train subjects:  36%|███▌      | 103/285 [03:25<06:25,  2.12s/it]predicting train subjects:  36%|███▋      | 104/285 [03:27<06:20,  2.10s/it]predicting train subjects:  37%|███▋      | 105/285 [03:29<06:09,  2.05s/it]predicting train subjects:  37%|███▋      | 106/285 [03:31<06:06,  2.05s/it]predicting train subjects:  38%|███▊      | 107/285 [03:33<06:05,  2.05s/it]predicting train subjects:  38%|███▊      | 108/285 [03:35<06:08,  2.08s/it]predicting train subjects:  38%|███▊      | 109/285 [03:37<06:05,  2.08s/it]predicting train subjects:  39%|███▊      | 110/285 [03:40<06:03,  2.08s/it]predicting train subjects:  39%|███▉      | 111/285 [03:42<06:01,  2.08s/it]predicting train subjects:  39%|███▉      | 112/285 [03:44<06:00,  2.08s/it]predicting train subjects:  40%|███▉      | 113/285 [03:46<05:57,  2.08s/it]predicting train subjects:  40%|████      | 114/285 [03:48<05:57,  2.09s/it]predicting train subjects:  40%|████      | 115/285 [03:50<05:54,  2.09s/it]predicting train subjects:  41%|████      | 116/285 [03:52<05:48,  2.06s/it]predicting train subjects:  41%|████      | 117/285 [03:54<05:47,  2.07s/it]predicting train subjects:  41%|████▏     | 118/285 [03:56<05:43,  2.06s/it]predicting train subjects:  42%|████▏     | 119/285 [03:58<05:44,  2.07s/it]predicting train subjects:  42%|████▏     | 120/285 [04:00<05:39,  2.06s/it]predicting train subjects:  42%|████▏     | 121/285 [04:02<05:26,  1.99s/it]predicting train subjects:  43%|████▎     | 122/285 [04:04<05:07,  1.89s/it]predicting train subjects:  43%|████▎     | 123/285 [04:05<04:54,  1.82s/it]predicting train subjects:  44%|████▎     | 124/285 [04:07<04:53,  1.82s/it]predicting train subjects:  44%|████▍     | 125/285 [04:09<04:52,  1.83s/it]predicting train subjects:  44%|████▍     | 126/285 [04:11<04:47,  1.81s/it]predicting train subjects:  45%|████▍     | 127/285 [04:13<04:46,  1.81s/it]predicting train subjects:  45%|████▍     | 128/285 [04:14<04:45,  1.82s/it]predicting train subjects:  45%|████▌     | 129/285 [04:16<04:41,  1.81s/it]predicting train subjects:  46%|████▌     | 130/285 [04:18<04:36,  1.78s/it]predicting train subjects:  46%|████▌     | 131/285 [04:20<04:35,  1.79s/it]predicting train subjects:  46%|████▋     | 132/285 [04:22<04:34,  1.79s/it]predicting train subjects:  47%|████▋     | 133/285 [04:23<04:33,  1.80s/it]predicting train subjects:  47%|████▋     | 134/285 [04:25<04:32,  1.80s/it]predicting train subjects:  47%|████▋     | 135/285 [04:27<04:34,  1.83s/it]predicting train subjects:  48%|████▊     | 136/285 [04:29<04:29,  1.81s/it]predicting train subjects:  48%|████▊     | 137/285 [04:31<04:23,  1.78s/it]predicting train subjects:  48%|████▊     | 138/285 [04:32<04:17,  1.75s/it]predicting train subjects:  49%|████▉     | 139/285 [04:34<04:18,  1.77s/it]predicting train subjects:  49%|████▉     | 140/285 [04:36<04:14,  1.76s/it]predicting train subjects:  49%|████▉     | 141/285 [04:38<04:12,  1.75s/it]predicting train subjects:  50%|████▉     | 142/285 [04:39<04:01,  1.69s/it]predicting train subjects:  50%|█████     | 143/285 [04:41<03:53,  1.65s/it]predicting train subjects:  51%|█████     | 144/285 [04:42<03:55,  1.67s/it]predicting train subjects:  51%|█████     | 145/285 [04:44<03:53,  1.67s/it]predicting train subjects:  51%|█████     | 146/285 [04:46<03:48,  1.64s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:47<03:48,  1.65s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:49<03:47,  1.66s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:51<03:44,  1.65s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:52<03:40,  1.63s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:54<03:41,  1.65s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:56<03:38,  1.64s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:57<03:39,  1.66s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:59<03:38,  1.66s/it]predicting train subjects:  54%|█████▍    | 155/285 [05:00<03:34,  1.65s/it]predicting train subjects:  55%|█████▍    | 156/285 [05:02<03:31,  1.64s/it]predicting train subjects:  55%|█████▌    | 157/285 [05:04<03:30,  1.64s/it]predicting train subjects:  55%|█████▌    | 158/285 [05:05<03:26,  1.63s/it]predicting train subjects:  56%|█████▌    | 159/285 [05:07<03:31,  1.68s/it]predicting train subjects:  56%|█████▌    | 160/285 [05:09<03:27,  1.66s/it]predicting train subjects:  56%|█████▋    | 161/285 [05:10<03:24,  1.65s/it]predicting train subjects:  57%|█████▋    | 162/285 [05:12<03:19,  1.62s/it]predicting train subjects:  57%|█████▋    | 163/285 [05:14<03:16,  1.61s/it]predicting train subjects:  58%|█████▊    | 164/285 [05:15<03:12,  1.59s/it]predicting train subjects:  58%|█████▊    | 165/285 [05:17<03:12,  1.61s/it]predicting train subjects:  58%|█████▊    | 166/285 [05:18<03:12,  1.62s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:20<03:09,  1.61s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:22<03:07,  1.60s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:23<03:05,  1.60s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:25<03:03,  1.60s/it]predicting train subjects:  60%|██████    | 171/285 [05:26<03:01,  1.60s/it]predicting train subjects:  60%|██████    | 172/285 [05:28<03:01,  1.61s/it]predicting train subjects:  61%|██████    | 173/285 [05:30<03:04,  1.65s/it]predicting train subjects:  61%|██████    | 174/285 [05:31<03:02,  1.65s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:33<02:56,  1.61s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:34<02:54,  1.60s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:36<02:52,  1.60s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:38<02:49,  1.59s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:39<02:46,  1.57s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:41<02:41,  1.54s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:42<02:39,  1.53s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:44<02:37,  1.53s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:45<02:36,  1.53s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:47<02:32,  1.51s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:48<02:33,  1.53s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:50<02:35,  1.57s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:51<02:34,  1.57s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:53<02:34,  1.59s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:55<02:31,  1.58s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:56<02:27,  1.56s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:58<02:26,  1.55s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:59<02:24,  1.55s/it]predicting train subjects:  68%|██████▊   | 193/285 [06:01<02:25,  1.58s/it]predicting train subjects:  68%|██████▊   | 194/285 [06:02<02:21,  1.56s/it]predicting train subjects:  68%|██████▊   | 195/285 [06:04<02:20,  1.56s/it]predicting train subjects:  69%|██████▉   | 196/285 [06:06<02:27,  1.65s/it]predicting train subjects:  69%|██████▉   | 197/285 [06:08<02:28,  1.69s/it]predicting train subjects:  69%|██████▉   | 198/285 [06:09<02:31,  1.74s/it]predicting train subjects:  70%|██████▉   | 199/285 [06:11<02:35,  1.81s/it]predicting train subjects:  70%|███████   | 200/285 [06:13<02:36,  1.84s/it]predicting train subjects:  71%|███████   | 201/285 [06:15<02:38,  1.88s/it]predicting train subjects:  71%|███████   | 202/285 [06:17<02:38,  1.91s/it]predicting train subjects:  71%|███████   | 203/285 [06:19<02:35,  1.89s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:21<02:33,  1.90s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:23<02:31,  1.90s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:25<02:28,  1.88s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:27<02:26,  1.87s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:29<02:24,  1.88s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:30<02:23,  1.88s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:32<02:22,  1.90s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:34<02:20,  1.90s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:36<02:17,  1.89s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:38<02:14,  1.87s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:40<02:06,  1.78s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:41<02:01,  1.73s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:43<01:57,  1.71s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:45<01:56,  1.72s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:46<01:52,  1.68s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:48<01:48,  1.64s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:49<01:44,  1.61s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:51<01:41,  1.58s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:52<01:40,  1.60s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:54<01:42,  1.66s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:56<01:41,  1.67s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:57<01:38,  1.64s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:59<01:36,  1.63s/it]predicting train subjects:  80%|███████▉  | 227/285 [07:01<01:34,  1.62s/it]predicting train subjects:  80%|████████  | 228/285 [07:02<01:31,  1.60s/it]predicting train subjects:  80%|████████  | 229/285 [07:04<01:29,  1.59s/it]predicting train subjects:  81%|████████  | 230/285 [07:05<01:27,  1.60s/it]predicting train subjects:  81%|████████  | 231/285 [07:07<01:27,  1.61s/it]predicting train subjects:  81%|████████▏ | 232/285 [07:09<01:36,  1.82s/it]predicting train subjects:  82%|████████▏ | 233/285 [07:11<01:37,  1.88s/it]predicting train subjects:  82%|████████▏ | 234/285 [07:13<01:37,  1.91s/it]predicting train subjects:  82%|████████▏ | 235/285 [07:15<01:36,  1.92s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:17<01:35,  1.95s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:19<01:34,  1.97s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:21<01:33,  1.99s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:23<01:32,  2.02s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:26<01:32,  2.05s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:28<01:30,  2.06s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:30<01:28,  2.06s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:32<01:25,  2.03s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:34<01:22,  2.02s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:36<01:21,  2.03s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:38<01:19,  2.03s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:40<01:16,  2.02s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:42<01:14,  2.03s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:44<01:12,  2.02s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:45<01:06,  1.89s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:47<01:00,  1.79s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:49<00:57,  1.75s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:50<00:54,  1.71s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:52<00:52,  1.70s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:53<00:50,  1.67s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:55<00:48,  1.69s/it]predicting train subjects:  90%|█████████ | 257/285 [07:57<00:47,  1.69s/it]predicting train subjects:  91%|█████████ | 258/285 [07:59<00:44,  1.67s/it]predicting train subjects:  91%|█████████ | 259/285 [08:00<00:42,  1.65s/it]predicting train subjects:  91%|█████████ | 260/285 [08:02<00:40,  1.62s/it]predicting train subjects:  92%|█████████▏| 261/285 [08:03<00:38,  1.60s/it]predicting train subjects:  92%|█████████▏| 262/285 [08:05<00:37,  1.62s/it]predicting train subjects:  92%|█████████▏| 263/285 [08:06<00:35,  1.61s/it]predicting train subjects:  93%|█████████▎| 264/285 [08:08<00:33,  1.61s/it]predicting train subjects:  93%|█████████▎| 265/285 [08:10<00:32,  1.61s/it]predicting train subjects:  93%|█████████▎| 266/285 [08:11<00:30,  1.60s/it]predicting train subjects:  94%|█████████▎| 267/285 [08:13<00:29,  1.61s/it]predicting train subjects:  94%|█████████▍| 268/285 [08:15<00:30,  1.77s/it]predicting train subjects:  94%|█████████▍| 269/285 [08:17<00:30,  1.88s/it]predicting train subjects:  95%|█████████▍| 270/285 [08:19<00:28,  1.93s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:21<00:27,  1.97s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:24<00:27,  2.08s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:26<00:25,  2.14s/it]predicting train subjects:  96%|█████████▌| 274/285 [08:28<00:23,  2.17s/it]predicting train subjects:  96%|█████████▋| 275/285 [08:31<00:22,  2.24s/it]predicting train subjects:  97%|█████████▋| 276/285 [08:33<00:20,  2.25s/it]predicting train subjects:  97%|█████████▋| 277/285 [08:35<00:18,  2.28s/it]predicting train subjects:  98%|█████████▊| 278/285 [08:37<00:15,  2.22s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:39<00:13,  2.21s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:42<00:11,  2.23s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:44<00:08,  2.23s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:46<00:06,  2.27s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:49<00:04,  2.29s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:51<00:02,  2.32s/it]predicting train subjects: 100%|██████████| 285/285 [08:53<00:00,  2.34s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<08:18,  1.76s/it]Loading train:   1%|          | 2/285 [00:03<08:41,  1.84s/it]Loading train:   1%|          | 3/285 [00:05<08:32,  1.82s/it]Loading train:   1%|▏         | 4/285 [00:07<08:34,  1.83s/it]Loading train:   2%|▏         | 5/285 [00:08<07:54,  1.69s/it]Loading train:   2%|▏         | 6/285 [00:10<08:32,  1.84s/it]Loading train:   2%|▏         | 7/285 [00:13<09:09,  1.98s/it]Loading train:   3%|▎         | 8/285 [00:15<09:15,  2.01s/it]Loading train:   3%|▎         | 9/285 [00:17<09:07,  1.98s/it]Loading train:   4%|▎         | 10/285 [00:18<08:28,  1.85s/it]Loading train:   4%|▍         | 11/285 [00:20<07:51,  1.72s/it]Loading train:   4%|▍         | 12/285 [00:22<08:02,  1.77s/it]Loading train:   5%|▍         | 13/285 [00:23<08:06,  1.79s/it]Loading train:   5%|▍         | 14/285 [00:26<08:28,  1.88s/it]Loading train:   5%|▌         | 15/285 [00:27<08:31,  1.89s/it]Loading train:   6%|▌         | 16/285 [00:30<08:41,  1.94s/it]Loading train:   6%|▌         | 17/285 [00:31<08:15,  1.85s/it]Loading train:   6%|▋         | 18/285 [00:33<07:55,  1.78s/it]Loading train:   7%|▋         | 19/285 [00:34<07:07,  1.61s/it]Loading train:   7%|▋         | 20/285 [00:36<07:05,  1.61s/it]Loading train:   7%|▋         | 21/285 [00:37<06:54,  1.57s/it]Loading train:   8%|▊         | 22/285 [00:39<06:58,  1.59s/it]Loading train:   8%|▊         | 23/285 [00:40<06:29,  1.49s/it]Loading train:   8%|▊         | 24/285 [00:41<06:24,  1.47s/it]Loading train:   9%|▉         | 25/285 [00:43<06:40,  1.54s/it]Loading train:   9%|▉         | 26/285 [00:45<07:25,  1.72s/it]Loading train:   9%|▉         | 27/285 [00:47<07:35,  1.77s/it]Loading train:  10%|▉         | 28/285 [00:49<07:32,  1.76s/it]Loading train:  10%|█         | 29/285 [00:50<06:52,  1.61s/it]Loading train:  11%|█         | 30/285 [00:51<06:25,  1.51s/it]Loading train:  11%|█         | 31/285 [00:53<06:21,  1.50s/it]Loading train:  11%|█         | 32/285 [00:55<06:49,  1.62s/it]Loading train:  12%|█▏        | 33/285 [00:57<07:10,  1.71s/it]Loading train:  12%|█▏        | 34/285 [00:58<06:39,  1.59s/it]Loading train:  12%|█▏        | 35/285 [00:59<05:58,  1.43s/it]Loading train:  13%|█▎        | 36/285 [01:01<06:01,  1.45s/it]Loading train:  13%|█▎        | 37/285 [01:02<05:46,  1.40s/it]Loading train:  13%|█▎        | 38/285 [01:03<05:29,  1.33s/it]Loading train:  14%|█▎        | 39/285 [01:04<05:28,  1.34s/it]Loading train:  14%|█▍        | 40/285 [01:06<05:39,  1.39s/it]Loading train:  14%|█▍        | 41/285 [01:08<06:22,  1.57s/it]Loading train:  15%|█▍        | 42/285 [01:10<06:27,  1.60s/it]Loading train:  15%|█▌        | 43/285 [01:11<05:45,  1.43s/it]Loading train:  15%|█▌        | 44/285 [01:12<05:49,  1.45s/it]Loading train:  16%|█▌        | 45/285 [01:14<06:22,  1.59s/it]Loading train:  16%|█▌        | 46/285 [01:15<06:14,  1.57s/it]Loading train:  16%|█▋        | 47/285 [01:16<05:34,  1.40s/it]Loading train:  17%|█▋        | 48/285 [01:18<06:05,  1.54s/it]Loading train:  17%|█▋        | 49/285 [01:19<05:26,  1.38s/it]Loading train:  18%|█▊        | 50/285 [01:21<05:47,  1.48s/it]Loading train:  18%|█▊        | 51/285 [01:23<05:43,  1.47s/it]Loading train:  18%|█▊        | 52/285 [01:25<06:30,  1.68s/it]Loading train:  19%|█▊        | 53/285 [01:26<06:36,  1.71s/it]Loading train:  19%|█▉        | 54/285 [01:28<06:08,  1.59s/it]Loading train:  19%|█▉        | 55/285 [01:29<05:28,  1.43s/it]Loading train:  20%|█▉        | 56/285 [01:30<05:03,  1.33s/it]Loading train:  20%|██        | 57/285 [01:31<05:10,  1.36s/it]Loading train:  20%|██        | 58/285 [01:33<05:30,  1.46s/it]Loading train:  21%|██        | 59/285 [01:34<04:58,  1.32s/it]Loading train:  21%|██        | 60/285 [01:35<05:01,  1.34s/it]Loading train:  21%|██▏       | 61/285 [01:37<05:31,  1.48s/it]Loading train:  22%|██▏       | 62/285 [01:39<05:51,  1.58s/it]Loading train:  22%|██▏       | 63/285 [01:41<06:05,  1.65s/it]Loading train:  22%|██▏       | 64/285 [01:43<06:19,  1.72s/it]Loading train:  23%|██▎       | 65/285 [01:45<06:44,  1.84s/it]Loading train:  23%|██▎       | 66/285 [01:47<07:07,  1.95s/it]Loading train:  24%|██▎       | 67/285 [01:48<06:22,  1.75s/it]Loading train:  24%|██▍       | 68/285 [01:50<05:43,  1.58s/it]Loading train:  24%|██▍       | 69/285 [01:51<05:56,  1.65s/it]Loading train:  25%|██▍       | 70/285 [01:52<05:11,  1.45s/it]Loading train:  25%|██▍       | 71/285 [01:53<04:49,  1.35s/it]Loading train:  25%|██▌       | 72/285 [01:55<04:53,  1.38s/it]Loading train:  26%|██▌       | 73/285 [01:57<05:19,  1.51s/it]Loading train:  26%|██▌       | 74/285 [01:58<05:15,  1.49s/it]Loading train:  26%|██▋       | 75/285 [01:59<04:45,  1.36s/it]Loading train:  27%|██▋       | 76/285 [02:01<04:53,  1.41s/it]Loading train:  27%|██▋       | 77/285 [02:02<05:13,  1.50s/it]Loading train:  27%|██▋       | 78/285 [02:04<04:42,  1.36s/it]Loading train:  28%|██▊       | 79/285 [02:05<04:30,  1.31s/it]Loading train:  28%|██▊       | 80/285 [02:06<04:58,  1.46s/it]Loading train:  28%|██▊       | 81/285 [02:08<04:41,  1.38s/it]Loading train:  29%|██▉       | 82/285 [02:09<04:18,  1.27s/it]Loading train:  29%|██▉       | 83/285 [02:10<03:56,  1.17s/it]Loading train:  29%|██▉       | 84/285 [02:11<04:17,  1.28s/it]Loading train:  30%|██▉       | 85/285 [02:13<05:06,  1.53s/it]Loading train:  30%|███       | 86/285 [02:15<04:51,  1.46s/it]Loading train:  31%|███       | 87/285 [02:16<04:59,  1.51s/it]Loading train:  31%|███       | 88/285 [02:17<04:39,  1.42s/it]Loading train:  31%|███       | 89/285 [02:19<04:46,  1.46s/it]Loading train:  32%|███▏      | 90/285 [02:20<04:34,  1.41s/it]Loading train:  32%|███▏      | 91/285 [02:22<04:40,  1.44s/it]Loading train:  32%|███▏      | 92/285 [02:24<04:52,  1.52s/it]Loading train:  33%|███▎      | 93/285 [02:25<05:12,  1.63s/it]Loading train:  33%|███▎      | 94/285 [02:27<05:31,  1.74s/it]Loading train:  33%|███▎      | 95/285 [02:29<05:44,  1.81s/it]Loading train:  34%|███▎      | 96/285 [02:31<05:42,  1.81s/it]Loading train:  34%|███▍      | 97/285 [02:33<05:44,  1.83s/it]Loading train:  34%|███▍      | 98/285 [02:34<04:52,  1.56s/it]Loading train:  35%|███▍      | 99/285 [02:35<04:46,  1.54s/it]Loading train:  35%|███▌      | 100/285 [02:38<05:11,  1.68s/it]Loading train:  35%|███▌      | 101/285 [02:39<05:05,  1.66s/it]Loading train:  36%|███▌      | 102/285 [02:40<04:40,  1.53s/it]Loading train:  36%|███▌      | 103/285 [02:42<04:35,  1.52s/it]Loading train:  36%|███▋      | 104/285 [02:43<04:09,  1.38s/it]Loading train:  37%|███▋      | 105/285 [02:44<04:02,  1.35s/it]Loading train:  37%|███▋      | 106/285 [02:46<04:01,  1.35s/it]Loading train:  38%|███▊      | 107/285 [02:47<04:04,  1.37s/it]Loading train:  38%|███▊      | 108/285 [02:49<04:13,  1.43s/it]Loading train:  38%|███▊      | 109/285 [02:50<03:56,  1.34s/it]Loading train:  39%|███▊      | 110/285 [02:51<03:42,  1.27s/it]Loading train:  39%|███▉      | 111/285 [02:52<03:44,  1.29s/it]Loading train:  39%|███▉      | 112/285 [02:54<03:50,  1.33s/it]Loading train:  40%|███▉      | 113/285 [02:55<03:50,  1.34s/it]Loading train:  40%|████      | 114/285 [02:56<03:40,  1.29s/it]Loading train:  40%|████      | 115/285 [02:57<03:44,  1.32s/it]Loading train:  41%|████      | 116/285 [02:59<04:03,  1.44s/it]Loading train:  41%|████      | 117/285 [03:01<04:03,  1.45s/it]Loading train:  41%|████▏     | 118/285 [03:02<03:44,  1.34s/it]Loading train:  42%|████▏     | 119/285 [03:03<03:43,  1.35s/it]Loading train:  42%|████▏     | 120/285 [03:04<03:28,  1.26s/it]Loading train:  42%|████▏     | 121/285 [03:06<03:40,  1.34s/it]Loading train:  43%|████▎     | 122/285 [03:07<03:55,  1.45s/it]Loading train:  43%|████▎     | 123/285 [03:09<04:25,  1.64s/it]Loading train:  44%|████▎     | 124/285 [03:11<04:39,  1.73s/it]Loading train:  44%|████▍     | 125/285 [03:12<03:58,  1.49s/it]Loading train:  44%|████▍     | 126/285 [03:14<03:54,  1.48s/it]Loading train:  45%|████▍     | 127/285 [03:15<04:01,  1.53s/it]Loading train:  45%|████▍     | 128/285 [03:17<03:52,  1.48s/it]Loading train:  45%|████▌     | 129/285 [03:18<03:35,  1.38s/it]Loading train:  46%|████▌     | 130/285 [03:20<03:58,  1.54s/it]Loading train:  46%|████▌     | 131/285 [03:22<04:11,  1.63s/it]Loading train:  46%|████▋     | 132/285 [03:23<03:55,  1.54s/it]Loading train:  47%|████▋     | 133/285 [03:24<03:26,  1.36s/it]Loading train:  47%|████▋     | 134/285 [03:25<03:14,  1.29s/it]Loading train:  47%|████▋     | 135/285 [03:27<03:21,  1.34s/it]Loading train:  48%|████▊     | 136/285 [03:28<03:37,  1.46s/it]Loading train:  48%|████▊     | 137/285 [03:30<03:37,  1.47s/it]Loading train:  48%|████▊     | 138/285 [03:31<03:17,  1.34s/it]Loading train:  49%|████▉     | 139/285 [03:32<03:11,  1.31s/it]Loading train:  49%|████▉     | 140/285 [03:33<03:06,  1.29s/it]Loading train:  49%|████▉     | 141/285 [03:35<03:08,  1.31s/it]Loading train:  50%|████▉     | 142/285 [03:36<03:28,  1.46s/it]Loading train:  50%|█████     | 143/285 [03:38<03:19,  1.41s/it]Loading train:  51%|█████     | 144/285 [03:39<03:30,  1.49s/it]Loading train:  51%|█████     | 145/285 [03:41<03:12,  1.37s/it]Loading train:  51%|█████     | 146/285 [03:42<03:05,  1.34s/it]Loading train:  52%|█████▏    | 147/285 [03:43<03:11,  1.39s/it]Loading train:  52%|█████▏    | 148/285 [03:44<02:57,  1.29s/it]Loading train:  52%|█████▏    | 149/285 [03:46<03:11,  1.41s/it]Loading train:  53%|█████▎    | 150/285 [03:47<02:56,  1.31s/it]Loading train:  53%|█████▎    | 151/285 [03:49<02:59,  1.34s/it]Loading train:  53%|█████▎    | 152/285 [03:50<03:01,  1.37s/it]Loading train:  54%|█████▎    | 153/285 [03:51<02:41,  1.22s/it]Loading train:  54%|█████▍    | 154/285 [03:52<02:51,  1.31s/it]Loading train:  54%|█████▍    | 155/285 [03:54<02:59,  1.38s/it]Loading train:  55%|█████▍    | 156/285 [03:55<02:41,  1.25s/it]Loading train:  55%|█████▌    | 157/285 [03:56<02:46,  1.30s/it]Loading train:  55%|█████▌    | 158/285 [03:57<02:39,  1.26s/it]Loading train:  56%|█████▌    | 159/285 [03:58<02:25,  1.16s/it]Loading train:  56%|█████▌    | 160/285 [04:00<02:26,  1.17s/it]Loading train:  56%|█████▋    | 161/285 [04:01<02:39,  1.29s/it]Loading train:  57%|█████▋    | 162/285 [04:02<02:28,  1.21s/it]Loading train:  57%|█████▋    | 163/285 [04:03<02:21,  1.16s/it]Loading train:  58%|█████▊    | 164/285 [04:05<02:30,  1.25s/it]Loading train:  58%|█████▊    | 165/285 [04:06<02:17,  1.15s/it]Loading train:  58%|█████▊    | 166/285 [04:07<02:10,  1.10s/it]Loading train:  59%|█████▊    | 167/285 [04:08<02:31,  1.28s/it]Loading train:  59%|█████▉    | 168/285 [04:09<02:18,  1.19s/it]Loading train:  59%|█████▉    | 169/285 [04:10<02:06,  1.09s/it]Loading train:  60%|█████▉    | 170/285 [04:12<02:18,  1.21s/it]Loading train:  60%|██████    | 171/285 [04:13<02:17,  1.21s/it]Loading train:  60%|██████    | 172/285 [04:14<02:10,  1.16s/it]Loading train:  61%|██████    | 173/285 [04:15<02:07,  1.14s/it]Loading train:  61%|██████    | 174/285 [04:17<02:25,  1.31s/it]Loading train:  61%|██████▏   | 175/285 [04:18<02:30,  1.37s/it]Loading train:  62%|██████▏   | 176/285 [04:19<02:15,  1.25s/it]Loading train:  62%|██████▏   | 177/285 [04:20<02:08,  1.19s/it]Loading train:  62%|██████▏   | 178/285 [04:22<02:22,  1.33s/it]Loading train:  63%|██████▎   | 179/285 [04:23<02:17,  1.30s/it]Loading train:  63%|██████▎   | 180/285 [04:24<02:09,  1.24s/it]Loading train:  64%|██████▎   | 181/285 [04:25<02:01,  1.17s/it]Loading train:  64%|██████▍   | 182/285 [04:27<02:15,  1.32s/it]Loading train:  64%|██████▍   | 183/285 [04:28<02:10,  1.28s/it]Loading train:  65%|██████▍   | 184/285 [04:29<01:59,  1.18s/it]Loading train:  65%|██████▍   | 185/285 [04:30<01:57,  1.17s/it]Loading train:  65%|██████▌   | 186/285 [04:31<01:53,  1.15s/it]Loading train:  66%|██████▌   | 187/285 [04:32<01:48,  1.11s/it]Loading train:  66%|██████▌   | 188/285 [04:33<01:43,  1.07s/it]Loading train:  66%|██████▋   | 189/285 [04:35<01:51,  1.16s/it]Loading train:  67%|██████▋   | 190/285 [04:36<01:58,  1.24s/it]Loading train:  67%|██████▋   | 191/285 [04:37<01:55,  1.22s/it]Loading train:  67%|██████▋   | 192/285 [04:38<01:45,  1.14s/it]Loading train:  68%|██████▊   | 193/285 [04:40<01:55,  1.25s/it]Loading train:  68%|██████▊   | 194/285 [04:40<01:43,  1.14s/it]Loading train:  68%|██████▊   | 195/285 [04:42<01:48,  1.21s/it]Loading train:  69%|██████▉   | 196/285 [04:44<02:02,  1.38s/it]Loading train:  69%|██████▉   | 197/285 [04:45<02:12,  1.50s/it]Loading train:  69%|██████▉   | 198/285 [04:47<02:00,  1.38s/it]Loading train:  70%|██████▉   | 199/285 [04:48<01:58,  1.38s/it]Loading train:  70%|███████   | 200/285 [04:50<02:06,  1.49s/it]Loading train:  71%|███████   | 201/285 [04:51<01:51,  1.33s/it]Loading train:  71%|███████   | 202/285 [04:52<01:47,  1.29s/it]Loading train:  71%|███████   | 203/285 [04:53<01:53,  1.39s/it]Loading train:  72%|███████▏  | 204/285 [04:55<02:05,  1.55s/it]Loading train:  72%|███████▏  | 205/285 [04:56<01:54,  1.44s/it]Loading train:  72%|███████▏  | 206/285 [04:58<01:48,  1.38s/it]Loading train:  73%|███████▎  | 207/285 [04:59<01:49,  1.40s/it]Loading train:  73%|███████▎  | 208/285 [05:01<01:49,  1.42s/it]Loading train:  73%|███████▎  | 209/285 [05:02<01:56,  1.53s/it]Loading train:  74%|███████▎  | 210/285 [05:03<01:41,  1.36s/it]Loading train:  74%|███████▍  | 211/285 [05:05<01:35,  1.29s/it]Loading train:  74%|███████▍  | 212/285 [05:06<01:41,  1.39s/it]Loading train:  75%|███████▍  | 213/285 [05:08<01:46,  1.47s/it]Loading train:  75%|███████▌  | 214/285 [05:09<01:41,  1.43s/it]Loading train:  75%|███████▌  | 215/285 [05:10<01:30,  1.30s/it]Loading train:  76%|███████▌  | 216/285 [05:11<01:23,  1.22s/it]Loading train:  76%|███████▌  | 217/285 [05:13<01:26,  1.27s/it]Loading train:  76%|███████▋  | 218/285 [05:14<01:20,  1.20s/it]Loading train:  77%|███████▋  | 219/285 [05:15<01:16,  1.16s/it]Loading train:  77%|███████▋  | 220/285 [05:16<01:15,  1.17s/it]Loading train:  78%|███████▊  | 221/285 [05:18<01:24,  1.32s/it]Loading train:  78%|███████▊  | 222/285 [05:19<01:18,  1.25s/it]Loading train:  78%|███████▊  | 223/285 [05:20<01:13,  1.19s/it]Loading train:  79%|███████▊  | 224/285 [05:21<01:12,  1.18s/it]Loading train:  79%|███████▉  | 225/285 [05:22<01:15,  1.26s/it]Loading train:  79%|███████▉  | 226/285 [05:24<01:29,  1.51s/it]Loading train:  80%|███████▉  | 227/285 [05:26<01:30,  1.56s/it]Loading train:  80%|████████  | 228/285 [05:27<01:21,  1.43s/it]Loading train:  80%|████████  | 229/285 [05:29<01:20,  1.44s/it]Loading train:  81%|████████  | 230/285 [05:30<01:21,  1.48s/it]Loading train:  81%|████████  | 231/285 [05:31<01:16,  1.42s/it]Loading train:  81%|████████▏ | 232/285 [05:33<01:12,  1.36s/it]Loading train:  82%|████████▏ | 233/285 [05:34<01:14,  1.44s/it]Loading train:  82%|████████▏ | 234/285 [05:36<01:18,  1.54s/it]Loading train:  82%|████████▏ | 235/285 [05:38<01:22,  1.65s/it]Loading train:  83%|████████▎ | 236/285 [05:39<01:16,  1.56s/it]Loading train:  83%|████████▎ | 237/285 [05:41<01:16,  1.58s/it]Loading train:  84%|████████▎ | 238/285 [05:43<01:22,  1.75s/it]Loading train:  84%|████████▍ | 239/285 [05:45<01:22,  1.80s/it]Loading train:  84%|████████▍ | 240/285 [05:46<01:15,  1.68s/it]Loading train:  85%|████████▍ | 241/285 [05:48<01:09,  1.57s/it]Loading train:  85%|████████▍ | 242/285 [05:49<01:07,  1.57s/it]Loading train:  85%|████████▌ | 243/285 [05:51<01:05,  1.57s/it]Loading train:  86%|████████▌ | 244/285 [05:52<00:59,  1.44s/it]Loading train:  86%|████████▌ | 245/285 [05:53<00:55,  1.39s/it]Loading train:  86%|████████▋ | 246/285 [05:55<00:57,  1.48s/it]Loading train:  87%|████████▋ | 247/285 [05:56<00:56,  1.47s/it]Loading train:  87%|████████▋ | 248/285 [05:58<00:50,  1.37s/it]Loading train:  87%|████████▋ | 249/285 [05:59<00:46,  1.29s/it]Loading train:  88%|████████▊ | 250/285 [06:00<00:44,  1.28s/it]Loading train:  88%|████████▊ | 251/285 [06:01<00:44,  1.31s/it]Loading train:  88%|████████▊ | 252/285 [06:02<00:39,  1.20s/it]Loading train:  89%|████████▉ | 253/285 [06:04<00:38,  1.21s/it]Loading train:  89%|████████▉ | 254/285 [06:05<00:40,  1.29s/it]Loading train:  89%|████████▉ | 255/285 [06:07<00:43,  1.44s/it]Loading train:  90%|████████▉ | 256/285 [06:08<00:37,  1.30s/it]Loading train:  90%|█████████ | 257/285 [06:09<00:34,  1.24s/it]Loading train:  91%|█████████ | 258/285 [06:11<00:39,  1.45s/it]Loading train:  91%|█████████ | 259/285 [06:12<00:39,  1.50s/it]Loading train:  91%|█████████ | 260/285 [06:13<00:33,  1.33s/it]Loading train:  92%|█████████▏| 261/285 [06:14<00:29,  1.23s/it]Loading train:  92%|█████████▏| 262/285 [06:16<00:30,  1.33s/it]Loading train:  92%|█████████▏| 263/285 [06:17<00:25,  1.16s/it]Loading train:  93%|█████████▎| 264/285 [06:18<00:24,  1.16s/it]Loading train:  93%|█████████▎| 265/285 [06:19<00:22,  1.14s/it]Loading train:  93%|█████████▎| 266/285 [06:20<00:20,  1.06s/it]Loading train:  94%|█████████▎| 267/285 [06:21<00:19,  1.10s/it]Loading train:  94%|█████████▍| 268/285 [06:22<00:19,  1.12s/it]Loading train:  94%|█████████▍| 269/285 [06:24<00:19,  1.19s/it]Loading train:  95%|█████████▍| 270/285 [06:25<00:18,  1.22s/it]Loading train:  95%|█████████▌| 271/285 [06:26<00:18,  1.31s/it]Loading train:  95%|█████████▌| 272/285 [06:28<00:16,  1.29s/it]Loading train:  96%|█████████▌| 273/285 [06:29<00:15,  1.26s/it]Loading train:  96%|█████████▌| 274/285 [06:30<00:14,  1.32s/it]Loading train:  96%|█████████▋| 275/285 [06:31<00:12,  1.25s/it]Loading train:  97%|█████████▋| 276/285 [06:32<00:10,  1.22s/it]Loading train:  97%|█████████▋| 277/285 [06:34<00:11,  1.39s/it]Loading train:  98%|█████████▊| 278/285 [06:36<00:10,  1.50s/it]Loading train:  98%|█████████▊| 279/285 [06:37<00:08,  1.47s/it]Loading train:  98%|█████████▊| 280/285 [06:39<00:06,  1.39s/it]Loading train:  99%|█████████▊| 281/285 [06:40<00:05,  1.47s/it]Loading train:  99%|█████████▉| 282/285 [06:42<00:04,  1.63s/it]Loading train:  99%|█████████▉| 283/285 [06:44<00:03,  1.78s/it]Loading train: 100%|█████████▉| 284/285 [06:46<00:01,  1.60s/it]Loading train: 100%|██████████| 285/285 [06:47<00:00,  1.50s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 2/285 [00:00<00:16, 17.00it/s]concatenating: train:   2%|▏         | 5/285 [00:00<00:14, 18.82it/s]concatenating: train:   4%|▎         | 10/285 [00:00<00:13, 20.93it/s]concatenating: train:   4%|▍         | 12/285 [00:00<00:13, 20.63it/s]concatenating: train:   6%|▋         | 18/285 [00:00<00:10, 25.68it/s]concatenating: train:  12%|█▏        | 35/285 [00:00<00:07, 34.38it/s]concatenating: train:  20%|█▉        | 56/285 [00:00<00:04, 45.83it/s]concatenating: train:  27%|██▋       | 77/285 [00:00<00:03, 59.71it/s]concatenating: train:  35%|███▍      | 99/285 [00:01<00:02, 75.75it/s]concatenating: train:  41%|████      | 116/285 [00:01<00:01, 89.88it/s]concatenating: train:  46%|████▋     | 132/285 [00:01<00:02, 64.15it/s]concatenating: train:  51%|█████     | 145/285 [00:01<00:02, 51.29it/s]concatenating: train:  54%|█████▍    | 155/285 [00:02<00:03, 39.95it/s]concatenating: train:  57%|█████▋    | 163/285 [00:02<00:03, 38.58it/s]concatenating: train:  61%|██████    | 173/285 [00:02<00:02, 46.51it/s]concatenating: train:  64%|██████▎   | 181/285 [00:02<00:01, 53.15it/s]concatenating: train:  71%|███████   | 203/285 [00:02<00:01, 68.07it/s]concatenating: train:  75%|███████▌  | 215/285 [00:02<00:00, 74.16it/s]concatenating: train:  80%|███████▉  | 227/285 [00:03<00:00, 83.42it/s]concatenating: train:  84%|████████▍ | 240/285 [00:03<00:00, 92.14it/s]concatenating: train:  88%|████████▊ | 252/285 [00:03<00:00, 74.70it/s]concatenating: train:  92%|█████████▏| 262/285 [00:03<00:00, 64.06it/s]concatenating: train:  95%|█████████▌| 271/285 [00:03<00:00, 59.08it/s]concatenating: train:  98%|█████████▊| 279/285 [00:04<00:00, 51.68it/s]concatenating: train: 100%|██████████| 285/285 [00:04<00:00, 68.28it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.86s/it]Loading test:  67%|██████▋   | 2/3 [00:03<00:01,  1.88s/it]Loading test: 100%|██████████| 3/3 [00:05<00:00,  1.86s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 65.02it/s]2019-07-06 18:53:49.055378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 18:53:49.055524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 18:53:49.055540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 18:53:49.055549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 18:53:49.056035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:13,  2.90it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:10,  3.53it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:09,  3.73it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:06,  4.96it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:07,  4.15it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:06,  4.59it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:06,  4.45it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:04,  5.82it/s]loading the weights for Unet:  45%|████▌     | 18/40 [00:02<00:03,  6.01it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:04,  4.92it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:03<00:04,  4.30it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:03,  5.45it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  5.92it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:03,  4.61it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:04<00:02,  5.30it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  5.33it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  6.66it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  6.57it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:05<00:00,  5.18it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  5.97it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  5.74it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  7.31it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 80, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 80, 52, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 80, 52, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 80, 52, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 80, 52, 10)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 80, 52, 10)   910         dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 80, 52, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 80, 52, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 80, 52, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 80, 52, 10)   910         dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 80, 52, 10)   40          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 80, 52, 10)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 80, 52, 10)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 80, 52, 10)   910         dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 80, 52, 10)   40          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 80, 52, 10)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 80, 52, 10)   910         activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 80, 52, 10)   40          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 80, 52, 10)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 40, 26, 10)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 40, 26, 10)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 40, 26, 20)   1820        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 40, 26, 20)   80          conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 40, 26, 20)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 40, 26, 20)   3620        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 40, 26, 20)   80          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 40, 26, 20)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 20, 13, 20)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 20, 13, 20)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 20, 13, 40)   7240        dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 20, 13, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 20, 13, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 13, 40)   14440       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 20, 13, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 20, 13, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 20, 13, 40)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 40, 26, 20)   3220        dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 40, 26, 40)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 40, 26, 20)   7220        concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 40, 26, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 40, 26, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 40, 26, 20)   3620        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 40, 26, 20)   80          conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 40, 26, 20)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 40, 26, 20)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 80, 52, 10)   810         dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 80, 52, 20)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 80, 52, 10)   1810        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 80, 52, 10)   40          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 80, 52, 10)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 80, 52, 10)   910         activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 80, 52, 10)   40          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 80, 52, 10)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 80, 52, 10)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 80, 52, 13)   143         dropout_8[0][0]                  
==================================================================================================
Total params: 49,513
Trainable params: 13,313
Non-trainable params: 36,200
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.48913484e-02 3.19509754e-02 7.49897764e-02 9.32025064e-03
 2.70904632e-02 7.06417031e-03 8.46180096e-02 1.12618024e-01
 8.60108482e-02 1.32459736e-02 2.94100802e-01 1.93843398e-01
 2.55960049e-04]
Train on 10843 samples, validate on 104 samples
Epoch 1/300
 - 18s - loss: 571.3998 - acc: 0.2642 - mDice: 0.0152 - val_loss: 964.7102 - val_acc: 0.4097 - val_mDice: 0.0103

Epoch 00001: val_mDice improved from -inf to 0.01029, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 9s - loss: 196.5580 - acc: 0.6531 - mDice: 0.0155 - val_loss: 426.0421 - val_acc: 0.4508 - val_mDice: 0.0020

Epoch 00002: val_mDice did not improve from 0.01029
Epoch 3/300
 - 9s - loss: 87.6886 - acc: 0.7863 - mDice: 0.0156 - val_loss: 266.5821 - val_acc: 0.6259 - val_mDice: 0.0036

Epoch 00003: val_mDice did not improve from 0.01029
Epoch 4/300
 - 9s - loss: 52.8626 - acc: 0.8330 - mDice: 0.0153 - val_loss: 16.1036 - val_acc: 0.9033 - val_mDice: 0.0077

Epoch 00004: val_mDice did not improve from 0.01029
Epoch 5/300
 - 9s - loss: 37.2146 - acc: 0.8558 - mDice: 0.0152 - val_loss: 15.9839 - val_acc: 0.9030 - val_mDice: 0.0156

Epoch 00005: val_mDice improved from 0.01029 to 0.01562, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 9s - loss: 28.9185 - acc: 0.8653 - mDice: 0.0157 - val_loss: 15.2387 - val_acc: 0.9031 - val_mDice: 0.0181

Epoch 00006: val_mDice improved from 0.01562 to 0.01811, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 9s - loss: 24.0994 - acc: 0.8699 - mDice: 0.0165 - val_loss: 13.1334 - val_acc: 0.9034 - val_mDice: 0.0170

Epoch 00007: val_mDice did not improve from 0.01811
Epoch 8/300
 - 8s - loss: 21.2231 - acc: 0.8719 - mDice: 0.0166 - val_loss: 11.2273 - val_acc: 0.9034 - val_mDice: 0.0165

Epoch 00008: val_mDice did not improve from 0.01811
Epoch 9/300
 - 9s - loss: 18.8965 - acc: 0.8728 - mDice: 0.0182 - val_loss: 10.6888 - val_acc: 0.9034 - val_mDice: 0.0143

Epoch 00009: val_mDice did not improve from 0.01811
Epoch 10/300
 - 8s - loss: 17.6752 - acc: 0.8736 - mDice: 0.0176 - val_loss: 10.2678 - val_acc: 0.9034 - val_mDice: 0.0153

Epoch 00010: val_mDice did not improve from 0.01811
Epoch 11/300
 - 10s - loss: 16.1766 - acc: 0.8742 - mDice: 0.0194 - val_loss: 10.1412 - val_acc: 0.9034 - val_mDice: 0.0140

Epoch 00011: val_mDice did not improve from 0.01811
Epoch 12/300
 - 8s - loss: 15.3961 - acc: 0.8745 - mDice: 0.0191 - val_loss: 9.6185 - val_acc: 0.9034 - val_mDice: 0.0158

Epoch 00012: val_mDice did not improve from 0.01811
Epoch 13/300
 - 9s - loss: 14.5318 - acc: 0.8746 - mDice: 0.0207 - val_loss: 9.4896 - val_acc: 0.9034 - val_mDice: 0.0159

Epoch 00013: val_mDice did not improve from 0.01811
Epoch 14/300
 - 8s - loss: 14.0046 - acc: 0.8746 - mDice: 0.0218 - val_loss: 10.4935 - val_acc: 0.9034 - val_mDice: 0.0106

Epoch 00014: val_mDice did not improve from 0.01811
Epoch 15/300
 - 9s - loss: 13.5461 - acc: 0.8746 - mDice: 0.0227 - val_loss: 9.0639 - val_acc: 0.9034 - val_mDice: 0.0186

Epoch 00015: val_mDice improved from 0.01811 to 0.01859, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 9s - loss: 12.8994 - acc: 0.8746 - mDice: 0.0244 - val_loss: 8.9035 - val_acc: 0.9034 - val_mDice: 0.0196

Epoch 00016: val_mDice improved from 0.01859 to 0.01961, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 9s - loss: 12.5435 - acc: 0.8746 - mDice: 0.0253 - val_loss: 8.8272 - val_acc: 0.9034 - val_mDice: 0.0196

Epoch 00017: val_mDice did not improve from 0.01961
Epoch 18/300
 - 9s - loss: 12.1379 - acc: 0.8747 - mDice: 0.0264 - val_loss: 8.8020 - val_acc: 0.9034 - val_mDice: 0.0213

Epoch 00018: val_mDice improved from 0.01961 to 0.02132, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 9s - loss: 11.7606 - acc: 0.8747 - mDice: 0.0279 - val_loss: 8.7854 - val_acc: 0.9034 - val_mDice: 0.0237

Epoch 00019: val_mDice improved from 0.02132 to 0.02370, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 9s - loss: 11.3767 - acc: 0.8747 - mDice: 0.0295 - val_loss: 8.7084 - val_acc: 0.9034 - val_mDice: 0.0243

Epoch 00020: val_mDice improved from 0.02370 to 0.02427, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 9s - loss: 11.1217 - acc: 0.8747 - mDice: 0.0305 - val_loss: 8.9222 - val_acc: 0.9034 - val_mDice: 0.0221

Epoch 00021: val_mDice did not improve from 0.02427
Epoch 22/300
 - 9s - loss: 10.8343 - acc: 0.8746 - mDice: 0.0319 - val_loss: 8.7528 - val_acc: 0.9034 - val_mDice: 0.0262

Epoch 00022: val_mDice improved from 0.02427 to 0.02618, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 8s - loss: 10.6230 - acc: 0.8746 - mDice: 0.0334 - val_loss: 8.7759 - val_acc: 0.9034 - val_mDice: 0.0278

Epoch 00023: val_mDice improved from 0.02618 to 0.02776, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 9s - loss: 10.3837 - acc: 0.8745 - mDice: 0.0352 - val_loss: 8.6678 - val_acc: 0.9034 - val_mDice: 0.0297

Epoch 00024: val_mDice improved from 0.02776 to 0.02966, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 8s - loss: 10.1435 - acc: 0.8743 - mDice: 0.0371 - val_loss: 8.4186 - val_acc: 0.9034 - val_mDice: 0.0314

Epoch 00025: val_mDice improved from 0.02966 to 0.03140, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 9s - loss: 9.9617 - acc: 0.8742 - mDice: 0.0385 - val_loss: 8.2724 - val_acc: 0.9034 - val_mDice: 0.0381

Epoch 00026: val_mDice improved from 0.03140 to 0.03809, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 8s - loss: 9.8157 - acc: 0.8741 - mDice: 0.0404 - val_loss: 8.1303 - val_acc: 0.9033 - val_mDice: 0.0394

Epoch 00027: val_mDice improved from 0.03809 to 0.03944, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 9s - loss: 9.6260 - acc: 0.8740 - mDice: 0.0423 - val_loss: 8.1946 - val_acc: 0.9033 - val_mDice: 0.0395

Epoch 00028: val_mDice improved from 0.03944 to 0.03948, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 8s - loss: 9.4361 - acc: 0.8739 - mDice: 0.0446 - val_loss: 8.0249 - val_acc: 0.9033 - val_mDice: 0.0429

Epoch 00029: val_mDice improved from 0.03948 to 0.04293, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 8s - loss: 9.2776 - acc: 0.8737 - mDice: 0.0466 - val_loss: 7.9316 - val_acc: 0.9032 - val_mDice: 0.0446

Epoch 00030: val_mDice improved from 0.04293 to 0.04462, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 8s - loss: 9.0977 - acc: 0.8736 - mDice: 0.0490 - val_loss: 7.8261 - val_acc: 0.9032 - val_mDice: 0.0522

Epoch 00031: val_mDice improved from 0.04462 to 0.05216, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 8s - loss: 8.9544 - acc: 0.8736 - mDice: 0.0512 - val_loss: 7.4878 - val_acc: 0.9030 - val_mDice: 0.0551

Epoch 00032: val_mDice improved from 0.05216 to 0.05514, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 8s - loss: 8.8170 - acc: 0.8737 - mDice: 0.0532 - val_loss: 7.5558 - val_acc: 0.9032 - val_mDice: 0.0584

Epoch 00033: val_mDice improved from 0.05514 to 0.05839, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 8s - loss: 8.6699 - acc: 0.8740 - mDice: 0.0560 - val_loss: 7.3993 - val_acc: 0.9034 - val_mDice: 0.0586

Epoch 00034: val_mDice improved from 0.05839 to 0.05862, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 8s - loss: 8.5659 - acc: 0.8743 - mDice: 0.0576 - val_loss: 7.1770 - val_acc: 0.9034 - val_mDice: 0.0649

Epoch 00035: val_mDice improved from 0.05862 to 0.06490, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 8s - loss: 8.4585 - acc: 0.8743 - mDice: 0.0595 - val_loss: 7.1160 - val_acc: 0.9035 - val_mDice: 0.0657

Epoch 00036: val_mDice improved from 0.06490 to 0.06566, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 8s - loss: 8.3557 - acc: 0.8744 - mDice: 0.0616 - val_loss: 6.9337 - val_acc: 0.9035 - val_mDice: 0.0720

Epoch 00037: val_mDice improved from 0.06566 to 0.07205, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 8s - loss: 8.2503 - acc: 0.8746 - mDice: 0.0640 - val_loss: 6.9005 - val_acc: 0.9035 - val_mDice: 0.0711

Epoch 00038: val_mDice did not improve from 0.07205
Epoch 39/300
 - 8s - loss: 8.1597 - acc: 0.8747 - mDice: 0.0663 - val_loss: 6.9563 - val_acc: 0.9034 - val_mDice: 0.0723

Epoch 00039: val_mDice improved from 0.07205 to 0.07230, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 8s - loss: 8.0496 - acc: 0.8753 - mDice: 0.0690 - val_loss: 6.7396 - val_acc: 0.9032 - val_mDice: 0.0778

Epoch 00040: val_mDice improved from 0.07230 to 0.07781, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 9s - loss: 7.9392 - acc: 0.8761 - mDice: 0.0723 - val_loss: 6.7932 - val_acc: 0.9032 - val_mDice: 0.0791

Epoch 00041: val_mDice improved from 0.07781 to 0.07911, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 8s - loss: 7.8238 - acc: 0.8766 - mDice: 0.0756 - val_loss: 6.6345 - val_acc: 0.9013 - val_mDice: 0.0866

Epoch 00042: val_mDice improved from 0.07911 to 0.08657, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 8s - loss: 7.6968 - acc: 0.8774 - mDice: 0.0794 - val_loss: 6.4498 - val_acc: 0.9032 - val_mDice: 0.0961

Epoch 00043: val_mDice improved from 0.08657 to 0.09614, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 44/300
 - 8s - loss: 7.5718 - acc: 0.8780 - mDice: 0.0834 - val_loss: 6.2740 - val_acc: 0.9022 - val_mDice: 0.0993

Epoch 00044: val_mDice improved from 0.09614 to 0.09934, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 8s - loss: 7.4514 - acc: 0.8782 - mDice: 0.0868 - val_loss: 6.2398 - val_acc: 0.9013 - val_mDice: 0.1037

Epoch 00045: val_mDice improved from 0.09934 to 0.10367, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 8s - loss: 7.3404 - acc: 0.8784 - mDice: 0.0903 - val_loss: 6.1373 - val_acc: 0.9042 - val_mDice: 0.1095

Epoch 00046: val_mDice improved from 0.10367 to 0.10953, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 8s - loss: 7.2390 - acc: 0.8789 - mDice: 0.0939 - val_loss: 6.0100 - val_acc: 0.9014 - val_mDice: 0.1134

Epoch 00047: val_mDice improved from 0.10953 to 0.11340, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 8s - loss: 7.1337 - acc: 0.8794 - mDice: 0.0972 - val_loss: 5.8905 - val_acc: 0.9065 - val_mDice: 0.1191

Epoch 00048: val_mDice improved from 0.11340 to 0.11905, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 8s - loss: 7.0279 - acc: 0.8800 - mDice: 0.1010 - val_loss: 5.8725 - val_acc: 0.9038 - val_mDice: 0.1218

Epoch 00049: val_mDice improved from 0.11905 to 0.12176, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 8s - loss: 6.9213 - acc: 0.8804 - mDice: 0.1043 - val_loss: 5.7291 - val_acc: 0.9063 - val_mDice: 0.1275

Epoch 00050: val_mDice improved from 0.12176 to 0.12755, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 8s - loss: 6.8207 - acc: 0.8809 - mDice: 0.1081 - val_loss: 5.6570 - val_acc: 0.9048 - val_mDice: 0.1321

Epoch 00051: val_mDice improved from 0.12755 to 0.13208, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 8s - loss: 6.7288 - acc: 0.8814 - mDice: 0.1116 - val_loss: 5.6096 - val_acc: 0.9045 - val_mDice: 0.1355

Epoch 00052: val_mDice improved from 0.13208 to 0.13550, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 8s - loss: 6.6452 - acc: 0.8814 - mDice: 0.1153 - val_loss: 5.5791 - val_acc: 0.9055 - val_mDice: 0.1423

Epoch 00053: val_mDice improved from 0.13550 to 0.14233, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 8s - loss: 6.5277 - acc: 0.8822 - mDice: 0.1204 - val_loss: 5.4868 - val_acc: 0.9008 - val_mDice: 0.1473

Epoch 00054: val_mDice improved from 0.14233 to 0.14731, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 8s - loss: 6.4708 - acc: 0.8825 - mDice: 0.1242 - val_loss: 5.4513 - val_acc: 0.9049 - val_mDice: 0.1476

Epoch 00055: val_mDice improved from 0.14731 to 0.14765, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 8s - loss: 6.3841 - acc: 0.8828 - mDice: 0.1281 - val_loss: 5.4113 - val_acc: 0.9022 - val_mDice: 0.1563

Epoch 00056: val_mDice improved from 0.14765 to 0.15634, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 57/300
 - 8s - loss: 6.3167 - acc: 0.8830 - mDice: 0.1316 - val_loss: 5.3461 - val_acc: 0.9056 - val_mDice: 0.1550

Epoch 00057: val_mDice did not improve from 0.15634
Epoch 58/300
 - 8s - loss: 6.2413 - acc: 0.8834 - mDice: 0.1352 - val_loss: 5.2527 - val_acc: 0.9045 - val_mDice: 0.1628

Epoch 00058: val_mDice improved from 0.15634 to 0.16282, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 8s - loss: 6.1931 - acc: 0.8837 - mDice: 0.1377 - val_loss: 5.3797 - val_acc: 0.8989 - val_mDice: 0.1634

Epoch 00059: val_mDice improved from 0.16282 to 0.16336, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 60/300
 - 8s - loss: 6.1352 - acc: 0.8840 - mDice: 0.1405 - val_loss: 5.2867 - val_acc: 0.9059 - val_mDice: 0.1664

Epoch 00060: val_mDice improved from 0.16336 to 0.16638, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 61/300
 - 8s - loss: 6.0729 - acc: 0.8843 - mDice: 0.1437 - val_loss: 5.1066 - val_acc: 0.9069 - val_mDice: 0.1719

Epoch 00061: val_mDice improved from 0.16638 to 0.17194, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 62/300
 - 8s - loss: 6.0089 - acc: 0.8845 - mDice: 0.1465 - val_loss: 5.2970 - val_acc: 0.9037 - val_mDice: 0.1672

Epoch 00062: val_mDice did not improve from 0.17194
Epoch 63/300
 - 8s - loss: 5.9559 - acc: 0.8851 - mDice: 0.1491 - val_loss: 5.2172 - val_acc: 0.9026 - val_mDice: 0.1745

Epoch 00063: val_mDice improved from 0.17194 to 0.17447, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 8s - loss: 5.9077 - acc: 0.8856 - mDice: 0.1523 - val_loss: 5.0517 - val_acc: 0.9033 - val_mDice: 0.1823

Epoch 00064: val_mDice improved from 0.17447 to 0.18232, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 65/300
 - 8s - loss: 5.8703 - acc: 0.8858 - mDice: 0.1545 - val_loss: 5.1272 - val_acc: 0.9077 - val_mDice: 0.1743

Epoch 00065: val_mDice did not improve from 0.18232
Epoch 66/300
 - 8s - loss: 5.8211 - acc: 0.8862 - mDice: 0.1570 - val_loss: 5.2072 - val_acc: 0.9031 - val_mDice: 0.1801

Epoch 00066: val_mDice did not improve from 0.18232
Epoch 67/300
 - 8s - loss: 5.7701 - acc: 0.8866 - mDice: 0.1601 - val_loss: 5.0611 - val_acc: 0.9057 - val_mDice: 0.1821

Epoch 00067: val_mDice did not improve from 0.18232
Epoch 68/300
 - 8s - loss: 5.7357 - acc: 0.8870 - mDice: 0.1630 - val_loss: 5.0931 - val_acc: 0.9040 - val_mDice: 0.1868

Epoch 00068: val_mDice improved from 0.18232 to 0.18682, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 69/300
 - 8s - loss: 5.6923 - acc: 0.8871 - mDice: 0.1648 - val_loss: 4.9539 - val_acc: 0.9075 - val_mDice: 0.1911

Epoch 00069: val_mDice improved from 0.18682 to 0.19112, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 70/300
 - 8s - loss: 5.6460 - acc: 0.8872 - mDice: 0.1675 - val_loss: 5.1377 - val_acc: 0.9031 - val_mDice: 0.1847

Epoch 00070: val_mDice did not improve from 0.19112
Epoch 71/300
 - 8s - loss: 5.6074 - acc: 0.8875 - mDice: 0.1704 - val_loss: 4.9275 - val_acc: 0.9053 - val_mDice: 0.1942

Epoch 00071: val_mDice improved from 0.19112 to 0.19419, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 72/300
 - 8s - loss: 5.5686 - acc: 0.8875 - mDice: 0.1725 - val_loss: 5.0283 - val_acc: 0.9063 - val_mDice: 0.1904

Epoch 00072: val_mDice did not improve from 0.19419
Epoch 73/300
 - 8s - loss: 5.5327 - acc: 0.8876 - mDice: 0.1751 - val_loss: 4.8893 - val_acc: 0.9105 - val_mDice: 0.1974

Epoch 00073: val_mDice improved from 0.19419 to 0.19743, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 8s - loss: 5.5037 - acc: 0.8880 - mDice: 0.1769 - val_loss: 4.9487 - val_acc: 0.9080 - val_mDice: 0.1976

Epoch 00074: val_mDice improved from 0.19743 to 0.19759, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 75/300
 - 8s - loss: 5.4733 - acc: 0.8879 - mDice: 0.1780 - val_loss: 4.9088 - val_acc: 0.9068 - val_mDice: 0.2011

Epoch 00075: val_mDice improved from 0.19759 to 0.20106, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 76/300
 - 8s - loss: 5.4257 - acc: 0.8881 - mDice: 0.1817 - val_loss: 5.1525 - val_acc: 0.9104 - val_mDice: 0.1940

Epoch 00076: val_mDice did not improve from 0.20106
Epoch 77/300
 - 8s - loss: 5.3962 - acc: 0.8884 - mDice: 0.1832 - val_loss: 4.8700 - val_acc: 0.9070 - val_mDice: 0.2035

Epoch 00077: val_mDice improved from 0.20106 to 0.20346, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 78/300
 - 8s - loss: 5.3846 - acc: 0.8885 - mDice: 0.1844 - val_loss: 4.8701 - val_acc: 0.9087 - val_mDice: 0.1994

Epoch 00078: val_mDice did not improve from 0.20346
Epoch 79/300
 - 8s - loss: 5.3346 - acc: 0.8886 - mDice: 0.1872 - val_loss: 4.9212 - val_acc: 0.9069 - val_mDice: 0.2011

Epoch 00079: val_mDice did not improve from 0.20346
Epoch 80/300
 - 8s - loss: 5.3056 - acc: 0.8888 - mDice: 0.1889 - val_loss: 5.1658 - val_acc: 0.9068 - val_mDice: 0.1991

Epoch 00080: val_mDice did not improve from 0.20346
Epoch 81/300
 - 8s - loss: 5.2688 - acc: 0.8891 - mDice: 0.1915 - val_loss: 5.0439 - val_acc: 0.9090 - val_mDice: 0.2026

Epoch 00081: val_mDice did not improve from 0.20346
Epoch 82/300
 - 8s - loss: 5.2565 - acc: 0.8889 - mDice: 0.1921 - val_loss: 5.0041 - val_acc: 0.9116 - val_mDice: 0.2016

Epoch 00082: val_mDice did not improve from 0.20346
Epoch 83/300
 - 8s - loss: 5.2161 - acc: 0.8891 - mDice: 0.1949 - val_loss: 4.8215 - val_acc: 0.9100 - val_mDice: 0.2166

Epoch 00083: val_mDice improved from 0.20346 to 0.21658, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 84/300
 - 8s - loss: 5.1876 - acc: 0.8893 - mDice: 0.1971 - val_loss: 4.9009 - val_acc: 0.9078 - val_mDice: 0.2121

Epoch 00084: val_mDice did not improve from 0.21658
Epoch 85/300
 - 8s - loss: 5.1517 - acc: 0.8895 - mDice: 0.1995 - val_loss: 4.8092 - val_acc: 0.9089 - val_mDice: 0.2149

Epoch 00085: val_mDice did not improve from 0.21658
Epoch 86/300
 - 8s - loss: 5.1463 - acc: 0.8897 - mDice: 0.2005 - val_loss: 4.8783 - val_acc: 0.9094 - val_mDice: 0.2189

Epoch 00086: val_mDice improved from 0.21658 to 0.21892, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 87/300
 - 8s - loss: 5.1097 - acc: 0.8900 - mDice: 0.2032 - val_loss: 4.9137 - val_acc: 0.9112 - val_mDice: 0.2154

Epoch 00087: val_mDice did not improve from 0.21892
Epoch 88/300
 - 8s - loss: 5.0845 - acc: 0.8902 - mDice: 0.2048 - val_loss: 4.8687 - val_acc: 0.9125 - val_mDice: 0.2208

Epoch 00088: val_mDice improved from 0.21892 to 0.22079, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 89/300
 - 8s - loss: 5.0575 - acc: 0.8904 - mDice: 0.2066 - val_loss: 4.8010 - val_acc: 0.9136 - val_mDice: 0.2240

Epoch 00089: val_mDice improved from 0.22079 to 0.22404, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 90/300
 - 8s - loss: 5.0255 - acc: 0.8905 - mDice: 0.2097 - val_loss: 4.7212 - val_acc: 0.9101 - val_mDice: 0.2268

Epoch 00090: val_mDice improved from 0.22404 to 0.22683, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 91/300
 - 8s - loss: 5.0153 - acc: 0.8906 - mDice: 0.2105 - val_loss: 4.6886 - val_acc: 0.9138 - val_mDice: 0.2285

Epoch 00091: val_mDice improved from 0.22683 to 0.22846, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 92/300
 - 8s - loss: 4.9850 - acc: 0.8910 - mDice: 0.2130 - val_loss: 4.6619 - val_acc: 0.9120 - val_mDice: 0.2299

Epoch 00092: val_mDice improved from 0.22846 to 0.22992, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 93/300
 - 8s - loss: 4.9752 - acc: 0.8909 - mDice: 0.2137 - val_loss: 4.8107 - val_acc: 0.9127 - val_mDice: 0.2276

Epoch 00093: val_mDice did not improve from 0.22992
Epoch 94/300
 - 8s - loss: 4.9566 - acc: 0.8910 - mDice: 0.2149 - val_loss: 4.8739 - val_acc: 0.9138 - val_mDice: 0.2230

Epoch 00094: val_mDice did not improve from 0.22992
Epoch 95/300
 - 8s - loss: 4.9218 - acc: 0.8912 - mDice: 0.2178 - val_loss: 4.7849 - val_acc: 0.9112 - val_mDice: 0.2286

Epoch 00095: val_mDice did not improve from 0.22992
Epoch 96/300
 - 8s - loss: 4.9242 - acc: 0.8912 - mDice: 0.2179 - val_loss: 4.7415 - val_acc: 0.9132 - val_mDice: 0.2323

Epoch 00096: val_mDice improved from 0.22992 to 0.23230, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 97/300
 - 8s - loss: 4.8883 - acc: 0.8915 - mDice: 0.2197 - val_loss: 4.6394 - val_acc: 0.9132 - val_mDice: 0.2416

Epoch 00097: val_mDice improved from 0.23230 to 0.24158, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 98/300
 - 8s - loss: 4.8719 - acc: 0.8916 - mDice: 0.2218 - val_loss: 4.7612 - val_acc: 0.9158 - val_mDice: 0.2362

Epoch 00098: val_mDice did not improve from 0.24158
Epoch 99/300
 - 8s - loss: 4.8626 - acc: 0.8918 - mDice: 0.2232 - val_loss: 4.7089 - val_acc: 0.9143 - val_mDice: 0.2405

Epoch 00099: val_mDice did not improve from 0.24158
Epoch 100/300
 - 8s - loss: 4.8341 - acc: 0.8921 - mDice: 0.2258 - val_loss: 4.6333 - val_acc: 0.9153 - val_mDice: 0.2394

Epoch 00100: val_mDice did not improve from 0.24158
Epoch 101/300
 - 8s - loss: 4.8156 - acc: 0.8922 - mDice: 0.2277 - val_loss: 4.6006 - val_acc: 0.9137 - val_mDice: 0.2459

Epoch 00101: val_mDice improved from 0.24158 to 0.24586, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 102/300
 - 8s - loss: 4.8025 - acc: 0.8922 - mDice: 0.2291 - val_loss: 4.6121 - val_acc: 0.9137 - val_mDice: 0.2475

Epoch 00102: val_mDice improved from 0.24586 to 0.24747, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 103/300
 - 8s - loss: 4.7795 - acc: 0.8924 - mDice: 0.2310 - val_loss: 4.6720 - val_acc: 0.9117 - val_mDice: 0.2508

Epoch 00103: val_mDice improved from 0.24747 to 0.25078, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 104/300
 - 8s - loss: 4.7696 - acc: 0.8925 - mDice: 0.2323 - val_loss: 4.4749 - val_acc: 0.9163 - val_mDice: 0.2499

Epoch 00104: val_mDice did not improve from 0.25078
Epoch 105/300
 - 9s - loss: 4.7566 - acc: 0.8929 - mDice: 0.2341 - val_loss: 4.4613 - val_acc: 0.9162 - val_mDice: 0.2534

Epoch 00105: val_mDice improved from 0.25078 to 0.25342, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 106/300
 - 8s - loss: 4.7492 - acc: 0.8927 - mDice: 0.2340 - val_loss: 4.5660 - val_acc: 0.9156 - val_mDice: 0.2497

Epoch 00106: val_mDice did not improve from 0.25342
Epoch 107/300
 - 8s - loss: 4.7265 - acc: 0.8929 - mDice: 0.2364 - val_loss: 4.3961 - val_acc: 0.9151 - val_mDice: 0.2545

Epoch 00107: val_mDice improved from 0.25342 to 0.25447, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 108/300
 - 9s - loss: 4.7023 - acc: 0.8930 - mDice: 0.2388 - val_loss: 4.5710 - val_acc: 0.9145 - val_mDice: 0.2576

Epoch 00108: val_mDice improved from 0.25447 to 0.25760, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 109/300
 - 8s - loss: 4.6986 - acc: 0.8930 - mDice: 0.2403 - val_loss: 4.5102 - val_acc: 0.9130 - val_mDice: 0.2587

Epoch 00109: val_mDice improved from 0.25760 to 0.25866, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 110/300
 - 8s - loss: 4.6883 - acc: 0.8931 - mDice: 0.2407 - val_loss: 4.6701 - val_acc: 0.9144 - val_mDice: 0.2475

Epoch 00110: val_mDice did not improve from 0.25866
Epoch 111/300
 - 9s - loss: 4.6700 - acc: 0.8931 - mDice: 0.2417 - val_loss: 4.5993 - val_acc: 0.9142 - val_mDice: 0.2521

Epoch 00111: val_mDice did not improve from 0.25866
Epoch 112/300
 - 8s - loss: 4.6639 - acc: 0.8935 - mDice: 0.2433 - val_loss: 4.5951 - val_acc: 0.9135 - val_mDice: 0.2522

Epoch 00112: val_mDice did not improve from 0.25866
Epoch 113/300
 - 8s - loss: 4.6498 - acc: 0.8934 - mDice: 0.2444 - val_loss: 4.5360 - val_acc: 0.9141 - val_mDice: 0.2620

Epoch 00113: val_mDice improved from 0.25866 to 0.26198, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 114/300
 - 8s - loss: 4.6410 - acc: 0.8935 - mDice: 0.2456 - val_loss: 4.7493 - val_acc: 0.9145 - val_mDice: 0.2510

Epoch 00114: val_mDice did not improve from 0.26198
Epoch 115/300
 - 9s - loss: 4.6171 - acc: 0.8936 - mDice: 0.2475 - val_loss: 4.4533 - val_acc: 0.9137 - val_mDice: 0.2598

Epoch 00115: val_mDice did not improve from 0.26198
Epoch 116/300
 - 8s - loss: 4.6177 - acc: 0.8936 - mDice: 0.2477 - val_loss: 4.7146 - val_acc: 0.9134 - val_mDice: 0.2564

Epoch 00116: val_mDice did not improve from 0.26198
Epoch 117/300
 - 9s - loss: 4.6074 - acc: 0.8938 - mDice: 0.2492 - val_loss: 4.5506 - val_acc: 0.9144 - val_mDice: 0.2602

Epoch 00117: val_mDice did not improve from 0.26198
Epoch 118/300
 - 8s - loss: 4.5889 - acc: 0.8940 - mDice: 0.2509 - val_loss: 4.5187 - val_acc: 0.9150 - val_mDice: 0.2633

Epoch 00118: val_mDice improved from 0.26198 to 0.26333, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 119/300
 - 8s - loss: 4.5807 - acc: 0.8940 - mDice: 0.2525 - val_loss: 4.5024 - val_acc: 0.9148 - val_mDice: 0.2645

Epoch 00119: val_mDice improved from 0.26333 to 0.26454, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 120/300
 - 9s - loss: 4.5757 - acc: 0.8940 - mDice: 0.2532 - val_loss: 4.4472 - val_acc: 0.9139 - val_mDice: 0.2635

Epoch 00120: val_mDice did not improve from 0.26454
Epoch 121/300
 - 8s - loss: 4.5560 - acc: 0.8939 - mDice: 0.2545 - val_loss: 4.6407 - val_acc: 0.9140 - val_mDice: 0.2630

Epoch 00121: val_mDice did not improve from 0.26454
Epoch 122/300
 - 9s - loss: 4.5613 - acc: 0.8940 - mDice: 0.2551 - val_loss: 4.5422 - val_acc: 0.9135 - val_mDice: 0.2634

Epoch 00122: val_mDice did not improve from 0.26454
Epoch 123/300
 - 8s - loss: 4.5368 - acc: 0.8941 - mDice: 0.2571 - val_loss: 4.5897 - val_acc: 0.9143 - val_mDice: 0.2688

Epoch 00123: val_mDice improved from 0.26454 to 0.26883, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 124/300
 - 8s - loss: 4.5401 - acc: 0.8940 - mDice: 0.2571 - val_loss: 4.5412 - val_acc: 0.9131 - val_mDice: 0.2669

Epoch 00124: val_mDice did not improve from 0.26883
Epoch 125/300
 - 8s - loss: 4.5130 - acc: 0.8943 - mDice: 0.2602 - val_loss: 4.8969 - val_acc: 0.9156 - val_mDice: 0.2558

Epoch 00125: val_mDice did not improve from 0.26883
Epoch 126/300
 - 8s - loss: 4.5121 - acc: 0.8943 - mDice: 0.2604 - val_loss: 4.5425 - val_acc: 0.9136 - val_mDice: 0.2721

Epoch 00126: val_mDice improved from 0.26883 to 0.27207, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 127/300
 - 8s - loss: 4.5134 - acc: 0.8942 - mDice: 0.2607 - val_loss: 4.6728 - val_acc: 0.9136 - val_mDice: 0.2720

Epoch 00127: val_mDice did not improve from 0.27207
Epoch 128/300
 - 8s - loss: 4.5015 - acc: 0.8941 - mDice: 0.2620 - val_loss: 4.6691 - val_acc: 0.9134 - val_mDice: 0.2630

Epoch 00128: val_mDice did not improve from 0.27207
Epoch 129/300
 - 9s - loss: 4.4865 - acc: 0.8943 - mDice: 0.2630 - val_loss: 4.6176 - val_acc: 0.9129 - val_mDice: 0.2636

Epoch 00129: val_mDice did not improve from 0.27207
Epoch 130/300
 - 8s - loss: 4.4874 - acc: 0.8942 - mDice: 0.2636 - val_loss: 4.6146 - val_acc: 0.9131 - val_mDice: 0.2694

Epoch 00130: val_mDice did not improve from 0.27207
Epoch 131/300
 - 8s - loss: 4.4651 - acc: 0.8943 - mDice: 0.2654 - val_loss: 4.5900 - val_acc: 0.9156 - val_mDice: 0.2744

Epoch 00131: val_mDice improved from 0.27207 to 0.27440, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 132/300
 - 8s - loss: 4.4555 - acc: 0.8946 - mDice: 0.2674 - val_loss: 4.5774 - val_acc: 0.9152 - val_mDice: 0.2695

Epoch 00132: val_mDice did not improve from 0.27440
Epoch 133/300
 - 9s - loss: 4.4551 - acc: 0.8944 - mDice: 0.2677 - val_loss: 4.6137 - val_acc: 0.9134 - val_mDice: 0.2684

Epoch 00133: val_mDice did not improve from 0.27440
Epoch 134/300
 - 8s - loss: 4.4559 - acc: 0.8943 - mDice: 0.2673 - val_loss: 4.5151 - val_acc: 0.9141 - val_mDice: 0.2712

Epoch 00134: val_mDice did not improve from 0.27440
Epoch 135/300
 - 8s - loss: 4.4481 - acc: 0.8943 - mDice: 0.2690 - val_loss: 4.6769 - val_acc: 0.9141 - val_mDice: 0.2653

Epoch 00135: val_mDice did not improve from 0.27440
Epoch 136/300
 - 9s - loss: 4.4411 - acc: 0.8945 - mDice: 0.2699 - val_loss: 4.6080 - val_acc: 0.9144 - val_mDice: 0.2710

Epoch 00136: val_mDice did not improve from 0.27440
Epoch 137/300
 - 8s - loss: 4.4358 - acc: 0.8943 - mDice: 0.2709 - val_loss: 4.5786 - val_acc: 0.9128 - val_mDice: 0.2711

Epoch 00137: val_mDice did not improve from 0.27440
Epoch 138/300
 - 8s - loss: 4.4381 - acc: 0.8944 - mDice: 0.2712 - val_loss: 4.4132 - val_acc: 0.9140 - val_mDice: 0.2809

Epoch 00138: val_mDice improved from 0.27440 to 0.28092, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 139/300
 - 9s - loss: 4.4201 - acc: 0.8942 - mDice: 0.2732 - val_loss: 4.5312 - val_acc: 0.9155 - val_mDice: 0.2690

Epoch 00139: val_mDice did not improve from 0.28092
Epoch 140/300
 - 8s - loss: 4.4138 - acc: 0.8942 - mDice: 0.2734 - val_loss: 4.5281 - val_acc: 0.9143 - val_mDice: 0.2824

Epoch 00140: val_mDice improved from 0.28092 to 0.28242, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 141/300
 - 8s - loss: 4.4045 - acc: 0.8945 - mDice: 0.2750 - val_loss: 4.5443 - val_acc: 0.9132 - val_mDice: 0.2782

Epoch 00141: val_mDice did not improve from 0.28242
Epoch 142/300
 - 9s - loss: 4.4043 - acc: 0.8943 - mDice: 0.2755 - val_loss: 4.6582 - val_acc: 0.9125 - val_mDice: 0.2707

Epoch 00142: val_mDice did not improve from 0.28242
Epoch 143/300
 - 8s - loss: 4.3967 - acc: 0.8943 - mDice: 0.2763 - val_loss: 4.6160 - val_acc: 0.9128 - val_mDice: 0.2680

Epoch 00143: val_mDice did not improve from 0.28242
Epoch 144/300
 - 8s - loss: 4.3890 - acc: 0.8944 - mDice: 0.2769 - val_loss: 4.6946 - val_acc: 0.9138 - val_mDice: 0.2690

Epoch 00144: val_mDice did not improve from 0.28242
Epoch 145/300
 - 9s - loss: 4.3866 - acc: 0.8943 - mDice: 0.2774 - val_loss: 4.7014 - val_acc: 0.9127 - val_mDice: 0.2698

Epoch 00145: val_mDice did not improve from 0.28242
Epoch 146/300
 - 8s - loss: 4.3902 - acc: 0.8943 - mDice: 0.2775 - val_loss: 4.7650 - val_acc: 0.9145 - val_mDice: 0.2726

Epoch 00146: val_mDice did not improve from 0.28242
Epoch 147/300
 - 8s - loss: 4.3947 - acc: 0.8941 - mDice: 0.2773 - val_loss: 4.6519 - val_acc: 0.9142 - val_mDice: 0.2765

Epoch 00147: val_mDice did not improve from 0.28242
Epoch 148/300
 - 9s - loss: 4.3722 - acc: 0.8942 - mDice: 0.2794 - val_loss: 4.7000 - val_acc: 0.9142 - val_mDice: 0.2768

Epoch 00148: val_mDice did not improve from 0.28242
Epoch 149/300
 - 8s - loss: 4.3684 - acc: 0.8943 - mDice: 0.2798 - val_loss: 4.7210 - val_acc: 0.9147 - val_mDice: 0.2822

Epoch 00149: val_mDice did not improve from 0.28242
Epoch 150/300
 - 8s - loss: 4.3758 - acc: 0.8944 - mDice: 0.2800 - val_loss: 4.5696 - val_acc: 0.9135 - val_mDice: 0.2777

Epoch 00150: val_mDice did not improve from 0.28242
Epoch 151/300
 - 8s - loss: 4.3718 - acc: 0.8942 - mDice: 0.2796 - val_loss: 4.7093 - val_acc: 0.9146 - val_mDice: 0.2723

Epoch 00151: val_mDice did not improve from 0.28242
Epoch 152/300
 - 9s - loss: 4.3690 - acc: 0.8943 - mDice: 0.2805 - val_loss: 4.7270 - val_acc: 0.9134 - val_mDice: 0.2668

Epoch 00152: val_mDice did not improve from 0.28242
Epoch 153/300
 - 8s - loss: 4.3587 - acc: 0.8942 - mDice: 0.2820 - val_loss: 4.4380 - val_acc: 0.9127 - val_mDice: 0.2782

Epoch 00153: val_mDice did not improve from 0.28242
Epoch 154/300
 - 8s - loss: 4.3578 - acc: 0.8944 - mDice: 0.2818 - val_loss: 4.6161 - val_acc: 0.9141 - val_mDice: 0.2811

Epoch 00154: val_mDice did not improve from 0.28242
Epoch 155/300
 - 9s - loss: 4.3461 - acc: 0.8944 - mDice: 0.2830 - val_loss: 4.5812 - val_acc: 0.9132 - val_mDice: 0.2846

Epoch 00155: val_mDice improved from 0.28242 to 0.28463, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 156/300
 - 8s - loss: 4.3553 - acc: 0.8944 - mDice: 0.2831 - val_loss: 4.4591 - val_acc: 0.9148 - val_mDice: 0.2834

Epoch 00156: val_mDice did not improve from 0.28463
Epoch 157/300
 - 9s - loss: 4.3344 - acc: 0.8943 - mDice: 0.2842 - val_loss: 4.6441 - val_acc: 0.9150 - val_mDice: 0.2817

Epoch 00157: val_mDice did not improve from 0.28463
Epoch 158/300
 - 8s - loss: 4.3429 - acc: 0.8942 - mDice: 0.2842 - val_loss: 4.5508 - val_acc: 0.9138 - val_mDice: 0.2771

Epoch 00158: val_mDice did not improve from 0.28463
Epoch 159/300
 - 8s - loss: 4.3332 - acc: 0.8941 - mDice: 0.2858 - val_loss: 4.7530 - val_acc: 0.9147 - val_mDice: 0.2776

Epoch 00159: val_mDice did not improve from 0.28463
Epoch 160/300
 - 8s - loss: 4.3260 - acc: 0.8940 - mDice: 0.2858 - val_loss: 4.5322 - val_acc: 0.9136 - val_mDice: 0.2812

Epoch 00160: val_mDice did not improve from 0.28463
Epoch 161/300
 - 8s - loss: 4.3248 - acc: 0.8943 - mDice: 0.2860 - val_loss: 4.7074 - val_acc: 0.9126 - val_mDice: 0.2793

Epoch 00161: val_mDice did not improve from 0.28463
Epoch 162/300
 - 9s - loss: 4.3258 - acc: 0.8942 - mDice: 0.2871 - val_loss: 4.6998 - val_acc: 0.9143 - val_mDice: 0.2888

Epoch 00162: val_mDice improved from 0.28463 to 0.28884, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 163/300
 - 9s - loss: 4.3164 - acc: 0.8944 - mDice: 0.2874 - val_loss: 4.7346 - val_acc: 0.9129 - val_mDice: 0.2776

Epoch 00163: val_mDice did not improve from 0.28884
Epoch 164/300
 - 9s - loss: 4.3253 - acc: 0.8940 - mDice: 0.2863 - val_loss: 4.7493 - val_acc: 0.9148 - val_mDice: 0.2765

Epoch 00164: val_mDice did not improve from 0.28884
Epoch 165/300
 - 9s - loss: 4.3168 - acc: 0.8944 - mDice: 0.2877 - val_loss: 4.5900 - val_acc: 0.9134 - val_mDice: 0.2855

Epoch 00165: val_mDice did not improve from 0.28884
Epoch 166/300
 - 9s - loss: 4.3008 - acc: 0.8943 - mDice: 0.2892 - val_loss: 4.6718 - val_acc: 0.9125 - val_mDice: 0.2852

Epoch 00166: val_mDice did not improve from 0.28884
Epoch 167/300
 - 9s - loss: 4.3013 - acc: 0.8943 - mDice: 0.2893 - val_loss: 4.7313 - val_acc: 0.9140 - val_mDice: 0.2743

Epoch 00167: val_mDice did not improve from 0.28884
Epoch 168/300
 - 9s - loss: 4.3104 - acc: 0.8943 - mDice: 0.2889 - val_loss: 4.7153 - val_acc: 0.9130 - val_mDice: 0.2763

Epoch 00168: val_mDice did not improve from 0.28884
Epoch 169/300
 - 9s - loss: 4.2983 - acc: 0.8942 - mDice: 0.2902 - val_loss: 4.7223 - val_acc: 0.9126 - val_mDice: 0.2781

Epoch 00169: val_mDice did not improve from 0.28884
Epoch 170/300
 - 9s - loss: 4.2928 - acc: 0.8944 - mDice: 0.2907 - val_loss: 4.6242 - val_acc: 0.9144 - val_mDice: 0.2821

Epoch 00170: val_mDice did not improve from 0.28884
Epoch 171/300
 - 9s - loss: 4.2990 - acc: 0.8944 - mDice: 0.2904 - val_loss: 4.8636 - val_acc: 0.9140 - val_mDice: 0.2734

Epoch 00171: val_mDice did not improve from 0.28884
Epoch 172/300
 - 9s - loss: 4.3020 - acc: 0.8942 - mDice: 0.2896 - val_loss: 4.7467 - val_acc: 0.9144 - val_mDice: 0.2790

Epoch 00172: val_mDice did not improve from 0.28884
Epoch 173/300
 - 10s - loss: 4.2881 - acc: 0.8945 - mDice: 0.2914 - val_loss: 4.5195 - val_acc: 0.9138 - val_mDice: 0.2857

Epoch 00173: val_mDice did not improve from 0.28884
Epoch 174/300
 - 9s - loss: 4.2801 - acc: 0.8944 - mDice: 0.2919 - val_loss: 4.7601 - val_acc: 0.9134 - val_mDice: 0.2810

Epoch 00174: val_mDice did not improve from 0.28884
Epoch 175/300
 - 10s - loss: 4.2949 - acc: 0.8944 - mDice: 0.2910 - val_loss: 4.6300 - val_acc: 0.9135 - val_mDice: 0.2897

Epoch 00175: val_mDice improved from 0.28884 to 0.28969, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 176/300
 - 9s - loss: 4.2894 - acc: 0.8944 - mDice: 0.2916 - val_loss: 4.4829 - val_acc: 0.9137 - val_mDice: 0.2891

Epoch 00176: val_mDice did not improve from 0.28969
Epoch 177/300
 - 10s - loss: 4.2786 - acc: 0.8944 - mDice: 0.2928 - val_loss: 4.4567 - val_acc: 0.9136 - val_mDice: 0.2870

Epoch 00177: val_mDice did not improve from 0.28969
Epoch 178/300
 - 9s - loss: 4.2686 - acc: 0.8945 - mDice: 0.2938 - val_loss: 4.6598 - val_acc: 0.9131 - val_mDice: 0.2830

Epoch 00178: val_mDice did not improve from 0.28969
Epoch 179/300
 - 9s - loss: 4.2584 - acc: 0.8945 - mDice: 0.2954 - val_loss: 4.6538 - val_acc: 0.9141 - val_mDice: 0.2819

Epoch 00179: val_mDice did not improve from 0.28969
Epoch 180/300
 - 9s - loss: 4.2646 - acc: 0.8945 - mDice: 0.2940 - val_loss: 4.6694 - val_acc: 0.9140 - val_mDice: 0.2873

Epoch 00180: val_mDice did not improve from 0.28969
Epoch 181/300
 - 9s - loss: 4.2691 - acc: 0.8945 - mDice: 0.2943 - val_loss: 4.6652 - val_acc: 0.9131 - val_mDice: 0.2808

Epoch 00181: val_mDice did not improve from 0.28969
Epoch 182/300
 - 9s - loss: 4.2476 - acc: 0.8945 - mDice: 0.2964 - val_loss: 4.7578 - val_acc: 0.9136 - val_mDice: 0.2759

Epoch 00182: val_mDice did not improve from 0.28969
Epoch 183/300
 - 9s - loss: 4.2578 - acc: 0.8943 - mDice: 0.2956 - val_loss: 4.5175 - val_acc: 0.9131 - val_mDice: 0.2869

Epoch 00183: val_mDice did not improve from 0.28969
Epoch 184/300
 - 9s - loss: 4.2574 - acc: 0.8942 - mDice: 0.2949 - val_loss: 4.6820 - val_acc: 0.9137 - val_mDice: 0.2886

Epoch 00184: val_mDice did not improve from 0.28969
Epoch 185/300
 - 9s - loss: 4.2512 - acc: 0.8943 - mDice: 0.2961 - val_loss: 4.7504 - val_acc: 0.9127 - val_mDice: 0.2840

Epoch 00185: val_mDice did not improve from 0.28969
Epoch 186/300
 - 9s - loss: 4.2496 - acc: 0.8944 - mDice: 0.2965 - val_loss: 4.6541 - val_acc: 0.9133 - val_mDice: 0.2881

Epoch 00186: val_mDice did not improve from 0.28969
Epoch 187/300
 - 9s - loss: 4.2420 - acc: 0.8943 - mDice: 0.2968 - val_loss: 4.6305 - val_acc: 0.9143 - val_mDice: 0.2864

Epoch 00187: val_mDice did not improve from 0.28969
Epoch 188/300
 - 10s - loss: 4.2450 - acc: 0.8945 - mDice: 0.2975 - val_loss: 4.6730 - val_acc: 0.9139 - val_mDice: 0.2883

Epoch 00188: val_mDice did not improve from 0.28969
Epoch 189/300
 - 9s - loss: 4.2400 - acc: 0.8944 - mDice: 0.2973 - val_loss: 4.7229 - val_acc: 0.9132 - val_mDice: 0.2817

Epoch 00189: val_mDice did not improve from 0.28969
Epoch 190/300
 - 10s - loss: 4.2459 - acc: 0.8943 - mDice: 0.2972 - val_loss: 4.5432 - val_acc: 0.9122 - val_mDice: 0.2869

Epoch 00190: val_mDice did not improve from 0.28969
Epoch 191/300
 - 9s - loss: 4.2358 - acc: 0.8945 - mDice: 0.2984 - val_loss: 4.6569 - val_acc: 0.9145 - val_mDice: 0.2888

Epoch 00191: val_mDice did not improve from 0.28969
Epoch 192/300
 - 9s - loss: 4.2346 - acc: 0.8945 - mDice: 0.2989 - val_loss: 4.6348 - val_acc: 0.9127 - val_mDice: 0.2897

Epoch 00192: val_mDice improved from 0.28969 to 0.28972, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 193/300
 - 8s - loss: 4.2295 - acc: 0.8944 - mDice: 0.2987 - val_loss: 4.6540 - val_acc: 0.9128 - val_mDice: 0.2848

Epoch 00193: val_mDice did not improve from 0.28972
Epoch 194/300
 - 8s - loss: 4.2270 - acc: 0.8943 - mDice: 0.2990 - val_loss: 4.6582 - val_acc: 0.9134 - val_mDice: 0.2899

Epoch 00194: val_mDice improved from 0.28972 to 0.28987, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 195/300
 - 8s - loss: 4.2280 - acc: 0.8943 - mDice: 0.2994 - val_loss: 4.7080 - val_acc: 0.9116 - val_mDice: 0.2825

Epoch 00195: val_mDice did not improve from 0.28987
Epoch 196/300
 - 8s - loss: 4.2206 - acc: 0.8945 - mDice: 0.3004 - val_loss: 4.5449 - val_acc: 0.9126 - val_mDice: 0.2878

Epoch 00196: val_mDice did not improve from 0.28987
Epoch 197/300
 - 8s - loss: 4.2258 - acc: 0.8945 - mDice: 0.2998 - val_loss: 4.9033 - val_acc: 0.9125 - val_mDice: 0.2718

Epoch 00197: val_mDice did not improve from 0.28987
Epoch 198/300
 - 8s - loss: 4.2195 - acc: 0.8944 - mDice: 0.3003 - val_loss: 4.6117 - val_acc: 0.9129 - val_mDice: 0.2918

Epoch 00198: val_mDice improved from 0.28987 to 0.29177, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 199/300
 - 8s - loss: 4.2259 - acc: 0.8945 - mDice: 0.3000 - val_loss: 4.8136 - val_acc: 0.9153 - val_mDice: 0.2853

Epoch 00199: val_mDice did not improve from 0.29177
Epoch 200/300
 - 8s - loss: 4.2119 - acc: 0.8947 - mDice: 0.3013 - val_loss: 4.6367 - val_acc: 0.9126 - val_mDice: 0.2874

Epoch 00200: val_mDice did not improve from 0.29177
Epoch 201/300
 - 8s - loss: 4.2113 - acc: 0.8945 - mDice: 0.3016 - val_loss: 4.6034 - val_acc: 0.9123 - val_mDice: 0.2957

Epoch 00201: val_mDice improved from 0.29177 to 0.29565, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 202/300
 - 8s - loss: 4.2088 - acc: 0.8945 - mDice: 0.3023 - val_loss: 4.7455 - val_acc: 0.9139 - val_mDice: 0.2888

Epoch 00202: val_mDice did not improve from 0.29565
Epoch 203/300
 - 8s - loss: 4.2138 - acc: 0.8946 - mDice: 0.3015 - val_loss: 4.8076 - val_acc: 0.9129 - val_mDice: 0.2850

Epoch 00203: val_mDice did not improve from 0.29565
Epoch 204/300
 - 8s - loss: 4.1931 - acc: 0.8948 - mDice: 0.3037 - val_loss: 4.6689 - val_acc: 0.9132 - val_mDice: 0.2857

Epoch 00204: val_mDice did not improve from 0.29565
Epoch 205/300
 - 8s - loss: 4.1964 - acc: 0.8946 - mDice: 0.3033 - val_loss: 4.5946 - val_acc: 0.9125 - val_mDice: 0.2886

Epoch 00205: val_mDice did not improve from 0.29565
Epoch 206/300
 - 8s - loss: 4.2004 - acc: 0.8947 - mDice: 0.3029 - val_loss: 4.5009 - val_acc: 0.9127 - val_mDice: 0.2947

Epoch 00206: val_mDice did not improve from 0.29565
Epoch 207/300
 - 8s - loss: 4.1995 - acc: 0.8947 - mDice: 0.3033 - val_loss: 4.8812 - val_acc: 0.9137 - val_mDice: 0.2869

Epoch 00207: val_mDice did not improve from 0.29565
Epoch 208/300
 - 8s - loss: 4.1967 - acc: 0.8947 - mDice: 0.3037 - val_loss: 4.5135 - val_acc: 0.9134 - val_mDice: 0.2930

Epoch 00208: val_mDice did not improve from 0.29565
Epoch 209/300
 - 8s - loss: 4.1885 - acc: 0.8948 - mDice: 0.3045 - val_loss: 4.5487 - val_acc: 0.9130 - val_mDice: 0.2860

Epoch 00209: val_mDice did not improve from 0.29565
Epoch 210/300
 - 8s - loss: 4.1938 - acc: 0.8947 - mDice: 0.3042 - val_loss: 4.5340 - val_acc: 0.9132 - val_mDice: 0.2905

Epoch 00210: val_mDice did not improve from 0.29565
Epoch 211/300
 - 8s - loss: 4.2004 - acc: 0.8946 - mDice: 0.3035 - val_loss: 4.6380 - val_acc: 0.9129 - val_mDice: 0.2886

Epoch 00211: val_mDice did not improve from 0.29565
Epoch 212/300
 - 8s - loss: 4.1849 - acc: 0.8948 - mDice: 0.3051 - val_loss: 4.8042 - val_acc: 0.9127 - val_mDice: 0.2805

Epoch 00212: val_mDice did not improve from 0.29565
Epoch 213/300
 - 8s - loss: 4.1839 - acc: 0.8948 - mDice: 0.3052 - val_loss: 4.6340 - val_acc: 0.9124 - val_mDice: 0.2848

Epoch 00213: val_mDice did not improve from 0.29565
Epoch 214/300
 - 8s - loss: 4.1918 - acc: 0.8946 - mDice: 0.3050 - val_loss: 4.6019 - val_acc: 0.9133 - val_mDice: 0.2902

Epoch 00214: val_mDice did not improve from 0.29565
Epoch 215/300
 - 8s - loss: 4.1730 - acc: 0.8947 - mDice: 0.3062 - val_loss: 4.7657 - val_acc: 0.9123 - val_mDice: 0.2845

Epoch 00215: val_mDice did not improve from 0.29565
Epoch 216/300
 - 8s - loss: 4.1926 - acc: 0.8947 - mDice: 0.3045 - val_loss: 4.8172 - val_acc: 0.9147 - val_mDice: 0.2788

Epoch 00216: val_mDice did not improve from 0.29565
Epoch 217/300
 - 8s - loss: 4.1723 - acc: 0.8949 - mDice: 0.3069 - val_loss: 4.6722 - val_acc: 0.9135 - val_mDice: 0.2870

Epoch 00217: val_mDice did not improve from 0.29565
Epoch 218/300
 - 8s - loss: 4.1683 - acc: 0.8947 - mDice: 0.3067 - val_loss: 4.5811 - val_acc: 0.9128 - val_mDice: 0.2913

Epoch 00218: val_mDice did not improve from 0.29565
Epoch 219/300
 - 8s - loss: 4.1700 - acc: 0.8947 - mDice: 0.3064 - val_loss: 4.6834 - val_acc: 0.9129 - val_mDice: 0.2868

Epoch 00219: val_mDice did not improve from 0.29565
Epoch 220/300
 - 8s - loss: 4.1678 - acc: 0.8948 - mDice: 0.3073 - val_loss: 4.6405 - val_acc: 0.9133 - val_mDice: 0.2851

Epoch 00220: val_mDice did not improve from 0.29565
Epoch 221/300
 - 8s - loss: 4.1703 - acc: 0.8948 - mDice: 0.3076 - val_loss: 4.7600 - val_acc: 0.9138 - val_mDice: 0.2884

Epoch 00221: val_mDice did not improve from 0.29565
Epoch 222/300
 - 8s - loss: 4.1818 - acc: 0.8948 - mDice: 0.3056 - val_loss: 4.5091 - val_acc: 0.9124 - val_mDice: 0.2916

Epoch 00222: val_mDice did not improve from 0.29565
Epoch 223/300
 - 8s - loss: 4.1650 - acc: 0.8949 - mDice: 0.3075 - val_loss: 4.8673 - val_acc: 0.9117 - val_mDice: 0.2840

Epoch 00223: val_mDice did not improve from 0.29565
Epoch 224/300
 - 8s - loss: 4.1648 - acc: 0.8948 - mDice: 0.3073 - val_loss: 4.5706 - val_acc: 0.9111 - val_mDice: 0.2948

Epoch 00224: val_mDice did not improve from 0.29565
Epoch 225/300
 - 8s - loss: 4.1591 - acc: 0.8948 - mDice: 0.3076 - val_loss: 4.4387 - val_acc: 0.9116 - val_mDice: 0.2944

Epoch 00225: val_mDice did not improve from 0.29565
Epoch 226/300
 - 8s - loss: 4.1563 - acc: 0.8947 - mDice: 0.3075 - val_loss: 4.8868 - val_acc: 0.9132 - val_mDice: 0.2806

Epoch 00226: val_mDice did not improve from 0.29565
Epoch 227/300
 - 8s - loss: 4.1553 - acc: 0.8950 - mDice: 0.3081 - val_loss: 4.6366 - val_acc: 0.9105 - val_mDice: 0.2920

Epoch 00227: val_mDice did not improve from 0.29565
Epoch 228/300
 - 8s - loss: 4.1446 - acc: 0.8949 - mDice: 0.3086 - val_loss: 4.7542 - val_acc: 0.9125 - val_mDice: 0.2897

Epoch 00228: val_mDice did not improve from 0.29565
Epoch 229/300
 - 8s - loss: 4.1590 - acc: 0.8948 - mDice: 0.3071 - val_loss: 4.7370 - val_acc: 0.9116 - val_mDice: 0.2852

Epoch 00229: val_mDice did not improve from 0.29565
Epoch 230/300
 - 8s - loss: 4.1494 - acc: 0.8949 - mDice: 0.3084 - val_loss: 4.6331 - val_acc: 0.9116 - val_mDice: 0.2911

Epoch 00230: val_mDice did not improve from 0.29565
Epoch 231/300
 - 8s - loss: 4.1383 - acc: 0.8947 - mDice: 0.3098 - val_loss: 4.5975 - val_acc: 0.9120 - val_mDice: 0.2936

Epoch 00231: val_mDice did not improve from 0.29565
Restoring model weights from the end of the best epoch
Epoch 00231: early stopping
{'val_loss': [964.7102414644681, 426.0421325977032, 266.58206165753876, 16.103575447430977, 15.9838931789765, 15.238707333803177, 13.133414603196657, 11.22732794055572, 10.688785264125237, 10.26780938414427, 10.141230509831356, 9.618478800241764, 9.489626932602663, 10.49352825146455, 9.063893827108236, 8.903504830140333, 8.82715022105437, 8.802045077085495, 8.78538019152788, 8.70844615652011, 8.922152370214462, 8.752817293772331, 8.775918119228804, 8.667761128682356, 8.418647924294838, 8.272395968437195, 8.130281755557426, 8.194572418928146, 8.024869792736494, 7.9315558557326975, 7.826115548610687, 7.4877832394379835, 7.5557777881622314, 7.399270733961692, 7.17695271051847, 7.115993871138646, 6.933688564942433, 6.900533341444456, 6.9562699519670925, 6.739562474764311, 6.793177164517916, 6.6344740322003, 6.449752764059947, 6.274011770120034, 6.239794435409399, 6.137251961689729, 6.01001238822937, 5.89049538052999, 5.872475000528189, 5.729058836515133, 5.656980067491531, 5.6095540294280415, 5.5791057623349705, 5.486771381818331, 5.451295160330259, 5.4113174218397875, 5.346100655885843, 5.2527282788203316, 5.379658320775399, 5.286703538436156, 5.10664179462653, 5.296953403032743, 5.217224824887055, 5.05172770527693, 5.127242851715821, 5.207178702721229, 5.061054972501902, 5.093137294054031, 4.953889910991375, 5.137747523876337, 4.927525084752303, 5.0283443079544945, 4.889262247544068, 4.9487166335949535, 4.908826821125471, 5.152455199223298, 4.869962281905687, 4.870056663568203, 4.921197059062811, 5.165834172413899, 5.043896484833497, 5.004141880915715, 4.8214636376270885, 4.900891063305048, 4.809249368997721, 4.87828515126155, 4.913690188756356, 4.868659273936198, 4.800970824865194, 4.721176722875009, 4.6885902629448815, 4.661932635765809, 4.8106995820999146, 4.873885526106908, 4.784938617394521, 4.741542699245306, 4.639434491212551, 4.76122041619741, 4.708851986206495, 4.633309391828684, 4.600592081363384, 4.612056610675959, 4.672019490828881, 4.474872288795618, 4.461316954631072, 4.566044830358946, 4.396067142486572, 4.570958375930786, 4.510192960500717, 4.67009209211056, 4.599302693055226, 4.595062090800359, 4.536014715066323, 4.749313232990412, 4.453320899835, 4.7146250651432915, 4.550595753468, 4.518723655205506, 4.502362827842052, 4.447222472383426, 4.640655178290147, 4.542231701887571, 4.589681776670309, 4.541159178201969, 4.8969254562487965, 4.542524757293554, 4.672760955416239, 4.66908026429323, 4.617565929889679, 4.614583044097974, 4.590026285785895, 4.57735118155296, 4.613691747188568, 4.515116833723509, 4.676884117034765, 4.607982585063348, 4.57858663224257, 4.413212711994465, 4.531160267499777, 4.528071999549866, 4.544310628221585, 4.658209394950133, 4.616016559875929, 4.694585928550134, 4.701426992049584, 4.764978060355554, 4.651922918283022, 4.7000428438186646, 4.721033472281236, 4.569577877338116, 4.709321248989839, 4.727010841553028, 4.438032255722926, 4.6160829204779406, 4.581246545681586, 4.459084357206638, 4.644053635688929, 4.550825721942461, 4.753018156840251, 4.532226133805055, 4.707447361487609, 4.699765757872508, 4.734593943907664, 4.749320135666774, 4.589961409568787, 4.671821467005289, 4.731279279177006, 4.715315018708889, 4.722271105417838, 4.624204025818751, 4.863634226413874, 4.746683682386692, 4.519500120328023, 4.76010386301921, 4.630020313538038, 4.482925687844936, 4.456698337426553, 4.659811606773963, 4.653805255889893, 4.6693888902664185, 4.665166079998016, 4.757768940467101, 4.517515751031729, 4.6819643676280975, 4.750405057118489, 4.6540664090560036, 4.63046437731156, 4.673006186118493, 4.72285686318691, 4.54323369035354, 4.656909137964249, 4.634818450762675, 4.654026969121053, 4.658211011153001, 4.707952359547982, 4.544903113291814, 4.90326429101137, 4.611707728642684, 4.81364780664444, 4.636675504537729, 4.603387750112093, 4.745473451339281, 4.80762932392267, 4.668873241314521, 4.5946096915465136, 4.500867747343504, 4.881179119531925, 4.513509115347495, 4.548700128610317, 4.534025160165934, 4.6380282915555515, 4.80423690034793, 4.634038766989341, 4.6018964029275455, 4.765731712946525, 4.817173197865486, 4.672208951069758, 4.581075358849305, 4.683391789977367, 4.640495944481629, 4.760016876917619, 4.509135260031774, 4.867287661020573, 4.570593720445266, 4.438683218680895, 4.886849048045965, 4.63664617217504, 4.754219791063895, 4.736972471842399, 4.633137457645857, 4.597471333467043], 'val_acc': [0.4096870491137871, 0.4508136212825775, 0.6259361253334925, 0.9033307158029996, 0.9030325504449698, 0.9030648997196784, 0.9033584686426016, 0.9033792729561145, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.9033792729561145, 0.9033491840729346, 0.9033099252444047, 0.9032752513885498, 0.9032174807328445, 0.9032105459616735, 0.9030279494248904, 0.9032267079903529, 0.9033584686426016, 0.9033676959001101, 0.9034971067538629, 0.9035271956370428, 0.9035040415250338, 0.9034208815831405, 0.9032382850463574, 0.903233642761524, 0.9012782115202683, 0.903210495526974, 0.902211980177806, 0.9013359890534327, 0.9041558893827292, 0.9013683291581961, 0.9065412626816676, 0.903783727150697, 0.9063309110127963, 0.904770722756019, 0.9045418569674859, 0.9055334696402917, 0.900778958430657, 0.9049324782995077, 0.902158824297098, 0.905632848923023, 0.904500289605214, 0.8989321291446686, 0.9058616780317746, 0.906927227973938, 0.9036751251954299, 0.9025933948846964, 0.9032937104885395, 0.90767614199565, 0.9030903279781342, 0.9056559617702777, 0.9040148693781632, 0.9075259084884937, 0.9031481169737302, 0.9052607394181765, 0.9063031535882217, 0.910452127456665, 0.9079627417601072, 0.9067723842767569, 0.9103712210288415, 0.9069734811782837, 0.9087162568018987, 0.9068809816470513, 0.9068301503474896, 0.9090098027999585, 0.9115846844819876, 0.9100198722802676, 0.9078055872366979, 0.9089335409494547, 0.9094142867968633, 0.9111686601088598, 0.9125439203702487, 0.913574796456557, 0.9100730648407569, 0.9137897583154532, 0.9120192367296952, 0.9127034132297223, 0.91381285740779, 0.9112195051633395, 0.9131564108225015, 0.9132419778750493, 0.915837668455564, 0.9143213721422049, 0.9153083196053138, 0.9137296814184922, 0.913718079145138, 0.9117372540327219, 0.9163415431976318, 0.9161566014473255, 0.9155880396182721, 0.915056416621575, 0.914478565637882, 0.912992346745271, 0.9143560666304368, 0.9142104295583872, 0.9134661463590769, 0.9141064171607678, 0.9145455727210412, 0.9136973023414612, 0.913438434784229, 0.9144068910525396, 0.9150355733357943, 0.9148044654956231, 0.9138937890529633, 0.9139654154960926, 0.9134869369176718, 0.9142774824912732, 0.9130824896005484, 0.9155533405450674, 0.913595607647529, 0.9135886407815493, 0.913438448539147, 0.9129484272920169, 0.9130524305196909, 0.915606501010748, 0.9152367069171026, 0.9134314702107356, 0.9141087325719687, 0.9141410680917593, 0.9144415763708261, 0.9127611724229959, 0.9140139703567212, 0.9155002030042502, 0.9143190429760859, 0.9132003692480234, 0.9125392895478469, 0.9128120404023391, 0.9138290423613328, 0.9126640970890338, 0.9144947414214795, 0.9141942308499262, 0.9142034970797025, 0.9146888737495129, 0.9134707794739649, 0.914568671813378, 0.9134060557071979, 0.9127011184508984, 0.9140671147749975, 0.9131610485223624, 0.9147790051423587, 0.9150263460782858, 0.9137897422680488, 0.914661134664829, 0.9136302792108976, 0.9125786056885352, 0.9142820903888116, 0.9129229898636158, 0.9147767126560211, 0.9133806457886329, 0.9125045950596149, 0.9139677148598891, 0.9130015464929434, 0.9126132382796361, 0.9144438826120816, 0.9140070493404682, 0.9143884090276865, 0.9137689356620495, 0.9133644493726584, 0.9135077458161575, 0.9137227237224579, 0.913563262957793, 0.913075518149596, 0.9141457080841064, 0.9139723548522363, 0.9130639777733729, 0.9136141171822181, 0.9131148503376887, 0.9136718488656558, 0.9126848991100605, 0.9132789533871871, 0.9142982684648954, 0.91391921043396, 0.913232704767814, 0.9122203038288996, 0.9145062749202435, 0.9126756672675793, 0.912832817206016, 0.9133667556139139, 0.9115985448543842, 0.9125646925889529, 0.9124930707307962, 0.9129114082226386, 0.91533841078098, 0.9126224999244397, 0.9122943121653336, 0.9138706807906811, 0.9129391495998089, 0.9131795557645651, 0.9124953815570245, 0.9126918476361495, 0.9137204312361203, 0.9134361422978915, 0.9130131212564615, 0.9132026709043063, 0.9129068278349363, 0.9126849128649785, 0.9123890262383682, 0.9133321207303268, 0.9122595970447247, 0.9147189603402064, 0.913549393415451, 0.9127819652740772, 0.9128582454644717, 0.9133367034105154, 0.9137828395916865, 0.9124005895394546, 0.9117187238656558, 0.9110785126686096, 0.9116470859600947, 0.9132280808228713, 0.9105144991324499, 0.9125346976977128, 0.9115546620809115, 0.9116262655991775, 0.9119545060854691], 'val_mDice': [0.010288270476918954, 0.001992235475112326, 0.0036476629857833567, 0.007657058420591056, 0.015623633212481555, 0.018108085800821964, 0.017038776610906307, 0.01648747149066856, 0.014316954363424044, 0.015289292527505985, 0.014021124547490707, 0.015841713202042647, 0.015875734532108672, 0.010647111303674487, 0.018593743031557936, 0.01961127516383735, 0.019579126105572168, 0.021316102526795406, 0.023700216265681844, 0.024270647026311893, 0.0221256697908617, 0.026177652645856142, 0.02775662997737527, 0.029662053423145644, 0.03139917445010864, 0.038088896025258764, 0.03944150329782413, 0.03948198989606821, 0.04293443456005592, 0.04461974485848959, 0.052159456321253225, 0.05513714046145861, 0.05838899772900801, 0.05862383143259929, 0.06489548531289284, 0.06565721891820431, 0.07204745666911969, 0.07112527087044257, 0.07230181189683768, 0.07780836336314678, 0.0791135229743444, 0.08656745289380734, 0.09614301573198575, 0.09933856860376321, 0.10367266919750434, 0.1095253534328479, 0.11340058150772865, 0.11905438624895535, 0.12175556788077721, 0.12754713657956857, 0.13207712167730698, 0.13550144348006982, 0.14233440934465483, 0.1473057555178037, 0.1476494804597818, 0.1563432480280216, 0.15504297413505042, 0.16281938882401356, 0.16335598665934342, 0.16638476115006667, 0.17193958793695158, 0.16720404003102046, 0.1744685097096058, 0.1823171001787369, 0.1742754575724785, 0.18007498721663767, 0.182081152470066, 0.18682450213684484, 0.19111824293549246, 0.18466366053773806, 0.1941884168638633, 0.1904437934549955, 0.1974328848031851, 0.19759000766162688, 0.20106290667676008, 0.19404969479028994, 0.20346218204269043, 0.19943540514661715, 0.20111151412129402, 0.1991313171501343, 0.2025879408017947, 0.20162688639874643, 0.21658467372449544, 0.21212810134658447, 0.21491221782679742, 0.21891730593947265, 0.2153873028090367, 0.22078965016855642, 0.22404040381885493, 0.2268280593248514, 0.22845644031006557, 0.22991810337855265, 0.22763956166230714, 0.22300514105993968, 0.22856684530583712, 0.23229862964497164, 0.24157963621501738, 0.23617347272542807, 0.24048510351433203, 0.23940551452911818, 0.24585769783992034, 0.2474654150696901, 0.25077923831458276, 0.24985689841783965, 0.2534192089851086, 0.24969906976016668, 0.2544723213292085, 0.257598943148668, 0.2586582466386832, 0.24752700271514746, 0.25205636339691967, 0.25222058751835275, 0.2619815804064274, 0.25099564615923625, 0.25984820093099886, 0.2564419283030125, 0.26017368169358146, 0.26332821954901403, 0.26454313271320784, 0.2635242405992288, 0.26295413420750546, 0.2634022320405795, 0.26882737846328664, 0.26688232301519466, 0.25577700754197746, 0.2720726682589604, 0.27203485627587026, 0.26304782339586663, 0.2636100477897204, 0.2693846859037876, 0.2743951337268719, 0.2694715794462424, 0.26841352679408514, 0.27118887566030025, 0.26528673561719746, 0.27101096195670277, 0.2711328643445785, 0.2809227381188136, 0.2690162449502028, 0.28242415762864626, 0.27819691985272443, 0.2706771475764421, 0.26803294440301567, 0.26897621541642225, 0.26977652511917627, 0.2725781692335239, 0.2765163504160367, 0.2768475516484334, 0.2821527410012025, 0.2776502850823678, 0.27227562121473825, 0.26675468052809054, 0.2781917991546484, 0.2810896761142291, 0.28462637989566875, 0.28338415691485774, 0.2816605198268707, 0.27709950721607757, 0.2775627999351575, 0.2811943513269608, 0.2793278106703208, 0.288841736717866, 0.2775876894593239, 0.27653396931978375, 0.2855155919320308, 0.28522833135838693, 0.27430566457601696, 0.27632291729633623, 0.27808722796348423, 0.2820780207044803, 0.2734001688659191, 0.2790462803095579, 0.28565075162511605, 0.28095516791710484, 0.28969128086016727, 0.2890964812384202, 0.28703698974389297, 0.2830477568965692, 0.2818959355354309, 0.28729888940086734, 0.2808462590552293, 0.2758533275471284, 0.2868961485532614, 0.2886409540302478, 0.2840267514380125, 0.28805407451895565, 0.2864226896602374, 0.28834385009339225, 0.2816870350104112, 0.28694045486358494, 0.28878408756393653, 0.28971629518155867, 0.2848220203931515, 0.2898661968513177, 0.2824934124946594, 0.2878224849700928, 0.271765872836113, 0.29177426661436373, 0.28528262144671035, 0.2873673519262901, 0.29565205711584824, 0.28879502988778627, 0.28498357735001123, 0.285705889073702, 0.28862434425033057, 0.2946772432098022, 0.28693545495088285, 0.2930118404328823, 0.28599271393166137, 0.29047443574437726, 0.2886006986865631, 0.2804695854966457, 0.28477102661362064, 0.29018706942980105, 0.28452635656755704, 0.27875673369719434, 0.2870185375213623, 0.29129289807035375, 0.28683883868730986, 0.28510717761058074, 0.2884426466547526, 0.29162731408499754, 0.2840395839168475, 0.2948050175148707, 0.2944239661670648, 0.2805916348902079, 0.2920468161598994, 0.2896565657395583, 0.28520483953448444, 0.29108967603399205, 0.29363236690943056], 'loss': [571.3997939096697, 196.5579658679985, 87.68864618758923, 52.86260893995072, 37.21460825304346, 28.918499700677668, 24.099438837662312, 21.22312723546959, 18.896478691961253, 17.675228354742437, 16.176569483327018, 15.39610213177083, 14.53184233166509, 14.004587060794128, 13.546094917364911, 12.899371318312289, 12.543537030347125, 12.137898686252528, 11.760607253050635, 11.376738534822, 11.121652108765634, 10.83426921385016, 10.62298958844434, 10.383745100822965, 10.143463423669223, 9.961737869125137, 9.815742085321489, 9.626035127892196, 9.43609924865233, 9.277611304789488, 9.097733080832377, 8.95443718664037, 8.817028715438337, 8.669936592438383, 8.565900542513312, 8.458487645886892, 8.355685804137156, 8.250251796867182, 8.159664052511003, 8.049576850831482, 7.939201055654441, 7.823844299516059, 7.696773337557078, 7.5717890949296764, 7.451413064626138, 7.340371377065944, 7.239024071744873, 7.133669120803878, 7.027930976741375, 6.9213033662250885, 6.820698992392239, 6.728814511372574, 6.645157974122316, 6.527705081041521, 6.470837018283808, 6.3840569510491125, 6.316694998800518, 6.241335933678794, 6.193076257895667, 6.1352106681182725, 6.072912843247396, 6.008885340025248, 5.955872040442256, 5.907705324707376, 5.870280682947169, 5.821138779791389, 5.7701407093453385, 5.735726779868085, 5.692296670963653, 5.646008065010056, 5.6074009062145995, 5.568585881203576, 5.532727350423908, 5.503708627383823, 5.473303458220754, 5.425740285211303, 5.396216999788538, 5.384588332740427, 5.334599142968187, 5.305635660345304, 5.2688058912341775, 5.256467635853863, 5.216085046065269, 5.1876313295199905, 5.151699793478401, 5.146345083315325, 5.109729979516043, 5.084457816952743, 5.057482617945619, 5.025466689475951, 5.015341469054841, 4.985034820196385, 4.975151127602664, 4.956642622713738, 4.921765304299988, 4.9242486115213495, 4.888274332039648, 4.87190206787699, 4.862555200473349, 4.834123982040136, 4.815626182800662, 4.802496086074606, 4.779487888124566, 4.76961465388946, 4.756568051215678, 4.749209363115545, 4.7265007783266535, 4.702342340078322, 4.698586181555135, 4.688332857103197, 4.6700132093690785, 4.663904371862316, 4.649762287226086, 4.641028186783056, 4.6171287936851195, 4.617722591834458, 4.607385710751693, 4.5888783864228255, 4.58065623665586, 4.575735651115358, 4.555957426062196, 4.561333599273518, 4.536764171459265, 4.540083348778554, 4.513022096704801, 4.512135971805151, 4.513367785823023, 4.501497316314298, 4.4864785662658715, 4.4874347193175375, 4.4650977966827865, 4.455503462458805, 4.455120110681089, 4.455855535941954, 4.448137070863743, 4.4410672896926195, 4.435784565965968, 4.438074791705181, 4.4200741510972605, 4.413833835374976, 4.404539281521777, 4.404259798822066, 4.396661209090438, 4.3889907976033715, 4.386615365762877, 4.390152854991115, 4.394695990333849, 4.372203564254668, 4.368391878026818, 4.375839581043642, 4.371758966794217, 4.368966157790601, 4.358718417089474, 4.357810004142491, 4.34607692203717, 4.35529807473003, 4.334417655680613, 4.34291665852867, 4.333159308784025, 4.3259622954820935, 4.324821692270337, 4.325766848910947, 4.316400866331259, 4.3252541850130575, 4.3168172280880555, 4.300811987174764, 4.301289341309673, 4.310384033087555, 4.298315370180703, 4.292814609969904, 4.2989641170401445, 4.301956962518869, 4.288063245407607, 4.280145235170707, 4.294931060887676, 4.289424565932764, 4.2785812283776, 4.2685774747575325, 4.258352314206625, 4.264579372180407, 4.2690546806366765, 4.247639450476771, 4.257845110683482, 4.257378303860646, 4.251169361598068, 4.249610849987105, 4.242003476521873, 4.245001621476335, 4.239974389060402, 4.245902791615178, 4.235782770920117, 4.234556496544626, 4.229532228430446, 4.226983262737727, 4.22803395921142, 4.220564443203375, 4.225786235481349, 4.219497598197136, 4.2259385665052545, 4.211918875738603, 4.2113063589932676, 4.208828189304193, 4.213834377350121, 4.193137040253023, 4.196404030234416, 4.200368654592708, 4.199495961577495, 4.196726492923816, 4.188509301418947, 4.193811420946667, 4.2003719642858695, 4.1849320917060036, 4.1838551152294485, 4.191842682069379, 4.17301563856886, 4.19260255893328, 4.172329516733176, 4.168275812054147, 4.1699747164443135, 4.167831022801197, 4.170345649226261, 4.1817755583829355, 4.165031430785708, 4.164843644546559, 4.159129949268199, 4.156319512978346, 4.155322455770579, 4.14459361510447, 4.15901135549569, 4.149428311621671, 4.138306982543848], 'acc': [0.2641572420099922, 0.6531372813185478, 0.7862704768069146, 0.832950139050113, 0.8557690774713903, 0.8653394341204952, 0.8699077135421777, 0.8718846886072016, 0.8728305989884807, 0.8736036731277039, 0.8742231587076016, 0.8744970172036236, 0.8745831035274779, 0.8746118556306468, 0.8746078653462224, 0.8746345103155321, 0.8746400966664516, 0.874651516741584, 0.8746708044759091, 0.8746754351350904, 0.8746656372704805, 0.8746201680571372, 0.8745763806754523, 0.8744650490552442, 0.8743271082186187, 0.8742241082096478, 0.874106147396359, 0.8739669867231652, 0.8738861555742248, 0.8737243650353813, 0.8736100152532058, 0.8735561613160109, 0.8736927028479836, 0.8739990210066991, 0.8742584060294529, 0.8742779804960362, 0.8744059226383838, 0.874568469880813, 0.8747134167066439, 0.8752607383334877, 0.8760553387096862, 0.876589135035019, 0.8773597946501653, 0.877964871176866, 0.8782303924656416, 0.8784114985704357, 0.8789051700086927, 0.8794155757299463, 0.8800150232375311, 0.8803945844390895, 0.8808904765035508, 0.8814128146041399, 0.8814356227944854, 0.8822305364922317, 0.8825140648482745, 0.8828090105254217, 0.8829806475998632, 0.8834220385305184, 0.8837427231676847, 0.8839734414676387, 0.8842933293221391, 0.8845378790330064, 0.88509874619055, 0.8855712704032723, 0.8858336946087593, 0.886195763389002, 0.8865885193757133, 0.886957690288778, 0.8870833658879614, 0.8872277126599862, 0.8875379509771045, 0.8875142291805286, 0.8876447005266634, 0.8879674007935926, 0.887900624115353, 0.8880584938086609, 0.8884009257526533, 0.8884811392022182, 0.888638272520076, 0.8887985179007168, 0.8890935715181761, 0.8889395165056427, 0.8891307483912944, 0.8893340223776418, 0.8895410402206151, 0.8896727492412847, 0.8900077570400521, 0.8902497809663048, 0.8903968086005642, 0.8905178757541945, 0.8906385892928114, 0.8909778528950987, 0.8909353945601155, 0.8909840795493132, 0.8912362607271043, 0.8911842081551841, 0.8915231568362785, 0.8916443111843083, 0.8918354371195844, 0.8921236629186164, 0.8921639450161722, 0.8922439978085991, 0.8924352082007377, 0.8925352412143954, 0.892894190126027, 0.892696192060506, 0.8928944312326694, 0.8929609167425216, 0.8930371813335224, 0.8930729220826654, 0.8930542087198722, 0.8934531074371503, 0.8933907640790583, 0.893535064923129, 0.8935787616362708, 0.8935774333316726, 0.8937970662066431, 0.8940233960424766, 0.8939530316626554, 0.8940091147904773, 0.8939396170994611, 0.8939607658037058, 0.8941203016895304, 0.8940454520368273, 0.894316765673429, 0.8942963004958792, 0.8941923704114796, 0.8940721692320365, 0.8943263446450245, 0.8942305664893919, 0.8942762823284283, 0.8945801393919564, 0.8943974417832289, 0.8943046181500756, 0.8943486888778284, 0.8944500718710003, 0.8943226195993452, 0.894429012884307, 0.8942395471827324, 0.8942406361672334, 0.8944631269153365, 0.894296901060912, 0.8943035532427067, 0.8943532321172305, 0.8943351877139172, 0.8943078977743066, 0.8940711475590614, 0.894220861931915, 0.8943174944354481, 0.8944244226505206, 0.8941618451264735, 0.8943367405515156, 0.8941856076342917, 0.89436059534951, 0.8944106308294488, 0.89435596304121, 0.8943264323726391, 0.8941711589637679, 0.8941001906217293, 0.894006300646474, 0.8943091400608766, 0.8942387045765565, 0.8943971065284031, 0.8940416667818265, 0.8944414701066443, 0.8942528052115851, 0.8943034439556143, 0.8942995596325597, 0.8941923052218197, 0.8943665350735721, 0.8944299389029038, 0.8942390623968224, 0.8944864290387035, 0.8943843815716542, 0.8943658706381743, 0.8943830301189273, 0.8943501518769402, 0.8945191049228391, 0.8945474440647119, 0.8945467064469258, 0.8945120579200533, 0.8945427863105113, 0.8942910041039933, 0.8941730838205435, 0.8942541780094043, 0.8943785515579583, 0.8943465151085819, 0.8944736161454516, 0.8944480291572123, 0.8943322633373267, 0.8944632602630699, 0.8945113487000902, 0.8944439521236502, 0.8943152781134436, 0.8943044178811077, 0.8945143659275572, 0.894520257238993, 0.894449052121997, 0.89452380808827, 0.8947130020040983, 0.8945438031405747, 0.8945185537654008, 0.8945704760789366, 0.8947750997094774, 0.8946451641949716, 0.8946674469651238, 0.8947128005697533, 0.8946501735467637, 0.8947818576437536, 0.894733598616404, 0.8945673921831, 0.8948161104426199, 0.8947842104138346, 0.8946342772579079, 0.8947235567481999, 0.8947106498386943, 0.8948782305815096, 0.8947319326547638, 0.8947021588291463, 0.8947852285412046, 0.8948047168083564, 0.8948044715184497, 0.8949386189149489, 0.8947897324106476, 0.8948016351223388, 0.8946749362505352, 0.8950233525932138, 0.8948504307124268, 0.8947695764098607, 0.8949024646877858, 0.8947467198343743], 'mDice': [0.015241987255653486, 0.015473474646822877, 0.01555669464632686, 0.015314738661872607, 0.015235364340904684, 0.015668567093533283, 0.01649449399577882, 0.016587201039078604, 0.018165427813500393, 0.017555515969246503, 0.01943954745092664, 0.019061880378425534, 0.020742750561470687, 0.021805165094008837, 0.022661429420450258, 0.02444906580348379, 0.025338892748074857, 0.026390714954128237, 0.027922974768340627, 0.029462130409430493, 0.03047171678834875, 0.03194442909871598, 0.033406865735022356, 0.035245004923517585, 0.03714554542356956, 0.03847974703405755, 0.04042893307364087, 0.042340001192541346, 0.044562320355410114, 0.04655861920655395, 0.04901317455905902, 0.05116818059226535, 0.05323698735485589, 0.05601535245424365, 0.057619362789184135, 0.05954669344901247, 0.06164347205661841, 0.06400233195335522, 0.06630867078967677, 0.06902631585155808, 0.07233948830797214, 0.07564791833618893, 0.07937697813507157, 0.08340706325326032, 0.08684588823631374, 0.09025990392180988, 0.09386355622078807, 0.09717310546322479, 0.10104749422514558, 0.10426297331993116, 0.10811572413564136, 0.11155172300621294, 0.11531502229488722, 0.12037527905028982, 0.12419180106637905, 0.12808273765232295, 0.13161277112035769, 0.13521956931584922, 0.13769785770161241, 0.1405138093464535, 0.14366163596350098, 0.14648750051134551, 0.14914860136557537, 0.15233685465566152, 0.15450361010982774, 0.15699226763413796, 0.16011531807823748, 0.16300775679392668, 0.16476025678547923, 0.16752055994833043, 0.17039562645658163, 0.17249805280217426, 0.1750525989735752, 0.17694937132479663, 0.17803802183938072, 0.18165566821627366, 0.18320955100682215, 0.18443445713056317, 0.18718229554834748, 0.18891933139296438, 0.19149782411794672, 0.19207748226504653, 0.1949071249024714, 0.19714678500308372, 0.19950257607692834, 0.20045046716101175, 0.20318359265350103, 0.20478047402755126, 0.20659676471792618, 0.20965282108459835, 0.21049842423539777, 0.21304061404593172, 0.21366260752850086, 0.21487891818064775, 0.2177711880595704, 0.21789510039499102, 0.21972118181204056, 0.2217739089728391, 0.22318064842833854, 0.225805390876652, 0.22774542763006045, 0.22908162242546298, 0.23095285159728188, 0.23227121102065604, 0.2341287350353363, 0.23404177031530882, 0.2364107531619162, 0.23882925701841684, 0.24032826848375527, 0.2406737834866566, 0.24169185328179044, 0.24329889560277904, 0.2443617845602958, 0.24564278608404996, 0.24752123968732936, 0.2476736316709053, 0.2492331071546088, 0.25093359671471954, 0.2524537024881241, 0.253220662804456, 0.2545420091321281, 0.255062529898477, 0.2571241830609705, 0.25709054163161277, 0.26018568269828035, 0.26043687238471597, 0.26069670697370273, 0.2620054553088959, 0.26303133100510345, 0.2635778030609937, 0.2654162392420137, 0.2674061162307079, 0.26772029789008855, 0.2673227554519579, 0.26901357681628335, 0.26990072603574444, 0.270860049449286, 0.27122626568595387, 0.2732230360427928, 0.2733540303052093, 0.2749712899647482, 0.2754681381662097, 0.2763152107490579, 0.2769375218051524, 0.27736216004589853, 0.2775155839792072, 0.2773301562655624, 0.27943081650403684, 0.27978729310372563, 0.27998277479835093, 0.2796007701361805, 0.28051466643562906, 0.2820456343241133, 0.28176736902851995, 0.2830361778430698, 0.2831217082462814, 0.2842426819273832, 0.28424420994299754, 0.28577323690129636, 0.2858475203291138, 0.28601261741679873, 0.28711481851113413, 0.2873972825933446, 0.28628040710124847, 0.2877147551047058, 0.2891531423714112, 0.2892893781595363, 0.2889433322709875, 0.2902056307664252, 0.29069882246232676, 0.2903997680225749, 0.28963248449685464, 0.2913503818633778, 0.2918873427182032, 0.2909974864155211, 0.2916208136117976, 0.2927737451892579, 0.2937843791263015, 0.29542263709871047, 0.2940071324773087, 0.29433094665791554, 0.2963864858889507, 0.2955550860734176, 0.29493858665708883, 0.29614376589995317, 0.2965019434528312, 0.29684972721008995, 0.29754168767611566, 0.2972864245264598, 0.2972327007404566, 0.2984488292873032, 0.2988953448129098, 0.2986759584701821, 0.29903962131598927, 0.29938198385407516, 0.30037577212939764, 0.2997885376426618, 0.3002898089617235, 0.2999972777426886, 0.30127781841748974, 0.30163154725294916, 0.30227197159054553, 0.3014595133950655, 0.3037368184821166, 0.3032794536413395, 0.3028876154523686, 0.30334989894890957, 0.3037095506351988, 0.3044688960651207, 0.3042423910379168, 0.303473558022309, 0.30505223754619215, 0.3052084150486279, 0.30499737536809346, 0.3062282866446787, 0.3044743733103509, 0.30691233705542004, 0.30669862878484216, 0.3064289961800764, 0.3072898979502763, 0.30759631285728545, 0.3055910917926096, 0.3074862977339862, 0.30729712649903634, 0.3076451080790703, 0.3075115830956774, 0.3081407466025345, 0.30863358532932755, 0.30705025959782284, 0.30840952257537035, 0.30977392523180763]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.57s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.31s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  2.09s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:48,  1.65s/it]predicting train subjects:   1%|          | 2/285 [00:03<08:11,  1.74s/it]predicting train subjects:   1%|          | 3/285 [00:05<08:06,  1.73s/it]predicting train subjects:   1%|▏         | 4/285 [00:07<08:47,  1.88s/it]predicting train subjects:   2%|▏         | 5/285 [00:09<08:17,  1.78s/it]predicting train subjects:   2%|▏         | 6/285 [00:11<08:42,  1.87s/it]predicting train subjects:   2%|▏         | 7/285 [00:13<09:19,  2.01s/it]predicting train subjects:   3%|▎         | 8/285 [00:15<09:36,  2.08s/it]predicting train subjects:   3%|▎         | 9/285 [00:17<09:21,  2.03s/it]predicting train subjects:   4%|▎         | 10/285 [00:19<09:27,  2.06s/it]predicting train subjects:   4%|▍         | 11/285 [00:22<09:43,  2.13s/it]predicting train subjects:   4%|▍         | 12/285 [00:24<09:55,  2.18s/it]predicting train subjects:   5%|▍         | 13/285 [00:26<10:07,  2.23s/it]predicting train subjects:   5%|▍         | 14/285 [00:28<10:07,  2.24s/it]predicting train subjects:   5%|▌         | 15/285 [00:31<10:05,  2.24s/it]predicting train subjects:   6%|▌         | 16/285 [00:33<09:58,  2.23s/it]predicting train subjects:   6%|▌         | 17/285 [00:35<10:02,  2.25s/it]predicting train subjects:   6%|▋         | 18/285 [00:38<10:09,  2.28s/it]predicting train subjects:   7%|▋         | 19/285 [00:40<09:55,  2.24s/it]predicting train subjects:   7%|▋         | 20/285 [00:42<09:50,  2.23s/it]predicting train subjects:   7%|▋         | 21/285 [00:44<09:51,  2.24s/it]predicting train subjects:   8%|▊         | 22/285 [00:47<10:00,  2.28s/it]predicting train subjects:   8%|▊         | 23/285 [00:49<10:08,  2.32s/it]predicting train subjects:   8%|▊         | 24/285 [00:51<10:10,  2.34s/it]predicting train subjects:   9%|▉         | 25/285 [00:54<10:07,  2.34s/it]predicting train subjects:   9%|▉         | 26/285 [00:56<09:46,  2.27s/it]predicting train subjects:   9%|▉         | 27/285 [00:58<09:44,  2.26s/it]predicting train subjects:  10%|▉         | 28/285 [01:00<09:41,  2.26s/it]predicting train subjects:  10%|█         | 29/285 [01:02<09:29,  2.22s/it]predicting train subjects:  11%|█         | 30/285 [01:05<09:32,  2.25s/it]predicting train subjects:  11%|█         | 31/285 [01:07<09:23,  2.22s/it]predicting train subjects:  11%|█         | 32/285 [01:09<09:15,  2.20s/it]predicting train subjects:  12%|█▏        | 33/285 [01:11<08:56,  2.13s/it]predicting train subjects:  12%|█▏        | 34/285 [01:13<08:37,  2.06s/it]predicting train subjects:  12%|█▏        | 35/285 [01:15<08:45,  2.10s/it]predicting train subjects:  13%|█▎        | 36/285 [01:17<08:44,  2.11s/it]predicting train subjects:  13%|█▎        | 37/285 [01:19<08:46,  2.12s/it]predicting train subjects:  13%|█▎        | 38/285 [01:21<08:38,  2.10s/it]predicting train subjects:  14%|█▎        | 39/285 [01:24<08:35,  2.09s/it]predicting train subjects:  14%|█▍        | 40/285 [01:26<08:34,  2.10s/it]predicting train subjects:  14%|█▍        | 41/285 [01:28<08:34,  2.11s/it]predicting train subjects:  15%|█▍        | 42/285 [01:30<08:37,  2.13s/it]predicting train subjects:  15%|█▌        | 43/285 [01:32<08:30,  2.11s/it]predicting train subjects:  15%|█▌        | 44/285 [01:34<08:32,  2.13s/it]predicting train subjects:  16%|█▌        | 45/285 [01:36<08:36,  2.15s/it]predicting train subjects:  16%|█▌        | 46/285 [01:38<08:21,  2.10s/it]predicting train subjects:  16%|█▋        | 47/285 [01:40<08:11,  2.07s/it]predicting train subjects:  17%|█▋        | 48/285 [01:42<07:54,  2.00s/it]predicting train subjects:  17%|█▋        | 49/285 [01:44<07:55,  2.02s/it]predicting train subjects:  18%|█▊        | 50/285 [01:46<07:47,  1.99s/it]predicting train subjects:  18%|█▊        | 51/285 [01:48<07:31,  1.93s/it]predicting train subjects:  18%|█▊        | 52/285 [01:50<07:30,  1.94s/it]predicting train subjects:  19%|█▊        | 53/285 [01:52<07:39,  1.98s/it]predicting train subjects:  19%|█▉        | 54/285 [01:54<07:35,  1.97s/it]predicting train subjects:  19%|█▉        | 55/285 [01:56<07:21,  1.92s/it]predicting train subjects:  20%|█▉        | 56/285 [01:58<07:22,  1.93s/it]predicting train subjects:  20%|██        | 57/285 [02:00<07:29,  1.97s/it]predicting train subjects:  20%|██        | 58/285 [02:02<07:26,  1.97s/it]predicting train subjects:  21%|██        | 59/285 [02:04<07:17,  1.94s/it]predicting train subjects:  21%|██        | 60/285 [02:05<07:08,  1.90s/it]predicting train subjects:  21%|██▏       | 61/285 [02:07<06:59,  1.87s/it]predicting train subjects:  22%|██▏       | 62/285 [02:09<07:04,  1.90s/it]predicting train subjects:  22%|██▏       | 63/285 [02:11<07:11,  1.94s/it]predicting train subjects:  22%|██▏       | 64/285 [02:13<07:09,  1.95s/it]predicting train subjects:  23%|██▎       | 65/285 [02:15<07:27,  2.04s/it]predicting train subjects:  23%|██▎       | 66/285 [02:18<07:50,  2.15s/it]predicting train subjects:  24%|██▎       | 67/285 [02:20<07:36,  2.09s/it]predicting train subjects:  24%|██▍       | 68/285 [02:22<07:27,  2.06s/it]predicting train subjects:  24%|██▍       | 69/285 [02:24<07:11,  2.00s/it]predicting train subjects:  25%|██▍       | 70/285 [02:26<07:09,  2.00s/it]predicting train subjects:  25%|██▍       | 71/285 [02:28<07:03,  1.98s/it]predicting train subjects:  25%|██▌       | 72/285 [02:30<07:09,  2.02s/it]predicting train subjects:  26%|██▌       | 73/285 [02:31<06:49,  1.93s/it]predicting train subjects:  26%|██▌       | 74/285 [02:33<06:44,  1.92s/it]predicting train subjects:  26%|██▋       | 75/285 [02:35<06:41,  1.91s/it]predicting train subjects:  27%|██▋       | 76/285 [02:37<06:37,  1.90s/it]predicting train subjects:  27%|██▋       | 77/285 [02:39<06:36,  1.91s/it]predicting train subjects:  27%|██▋       | 78/285 [02:41<06:37,  1.92s/it]predicting train subjects:  28%|██▊       | 79/285 [02:43<06:42,  1.96s/it]predicting train subjects:  28%|██▊       | 80/285 [02:45<06:34,  1.92s/it]predicting train subjects:  28%|██▊       | 81/285 [02:47<06:37,  1.95s/it]predicting train subjects:  29%|██▉       | 82/285 [02:49<06:30,  1.92s/it]predicting train subjects:  29%|██▉       | 83/285 [02:51<06:29,  1.93s/it]predicting train subjects:  29%|██▉       | 84/285 [02:53<06:25,  1.92s/it]predicting train subjects:  30%|██▉       | 85/285 [02:55<06:29,  1.95s/it]predicting train subjects:  30%|███       | 86/285 [02:57<06:28,  1.95s/it]predicting train subjects:  31%|███       | 87/285 [02:59<06:34,  1.99s/it]predicting train subjects:  31%|███       | 88/285 [03:01<06:48,  2.07s/it]predicting train subjects:  31%|███       | 89/285 [03:03<06:44,  2.06s/it]predicting train subjects:  32%|███▏      | 90/285 [03:05<06:45,  2.08s/it]predicting train subjects:  32%|███▏      | 91/285 [03:07<06:40,  2.06s/it]predicting train subjects:  32%|███▏      | 92/285 [03:09<06:46,  2.11s/it]predicting train subjects:  33%|███▎      | 93/285 [03:11<06:44,  2.11s/it]predicting train subjects:  33%|███▎      | 94/285 [03:14<06:48,  2.14s/it]predicting train subjects:  33%|███▎      | 95/285 [03:16<06:51,  2.17s/it]predicting train subjects:  34%|███▎      | 96/285 [03:18<06:47,  2.16s/it]predicting train subjects:  34%|███▍      | 97/285 [03:20<06:42,  2.14s/it]predicting train subjects:  34%|███▍      | 98/285 [03:22<06:32,  2.10s/it]predicting train subjects:  35%|███▍      | 99/285 [03:24<06:33,  2.12s/it]predicting train subjects:  35%|███▌      | 100/285 [03:26<06:29,  2.10s/it]predicting train subjects:  35%|███▌      | 101/285 [03:28<06:25,  2.10s/it]predicting train subjects:  36%|███▌      | 102/285 [03:30<06:23,  2.10s/it]predicting train subjects:  36%|███▌      | 103/285 [03:33<06:23,  2.11s/it]predicting train subjects:  36%|███▋      | 104/285 [03:34<06:10,  2.05s/it]predicting train subjects:  37%|███▋      | 105/285 [03:37<06:13,  2.07s/it]predicting train subjects:  37%|███▋      | 106/285 [03:39<06:04,  2.03s/it]predicting train subjects:  38%|███▊      | 107/285 [03:41<06:14,  2.10s/it]predicting train subjects:  38%|███▊      | 108/285 [03:43<06:08,  2.08s/it]predicting train subjects:  38%|███▊      | 109/285 [03:45<05:59,  2.04s/it]predicting train subjects:  39%|███▊      | 110/285 [03:47<06:05,  2.09s/it]predicting train subjects:  39%|███▉      | 111/285 [03:49<05:59,  2.07s/it]predicting train subjects:  39%|███▉      | 112/285 [03:51<06:03,  2.10s/it]predicting train subjects:  40%|███▉      | 113/285 [03:53<05:59,  2.09s/it]predicting train subjects:  40%|████      | 114/285 [03:55<05:53,  2.07s/it]predicting train subjects:  40%|████      | 115/285 [03:57<05:56,  2.10s/it]predicting train subjects:  41%|████      | 116/285 [03:59<05:49,  2.07s/it]predicting train subjects:  41%|████      | 117/285 [04:02<05:53,  2.10s/it]predicting train subjects:  41%|████▏     | 118/285 [04:04<05:49,  2.09s/it]predicting train subjects:  42%|████▏     | 119/285 [04:06<05:53,  2.13s/it]predicting train subjects:  42%|████▏     | 120/285 [04:08<05:36,  2.04s/it]predicting train subjects:  42%|████▏     | 121/285 [04:10<05:24,  1.98s/it]predicting train subjects:  43%|████▎     | 122/285 [04:12<05:19,  1.96s/it]predicting train subjects:  43%|████▎     | 123/285 [04:13<04:59,  1.85s/it]predicting train subjects:  44%|████▎     | 124/285 [04:15<04:56,  1.84s/it]predicting train subjects:  44%|████▍     | 125/285 [04:17<05:03,  1.89s/it]predicting train subjects:  44%|████▍     | 126/285 [04:19<04:56,  1.86s/it]predicting train subjects:  45%|████▍     | 127/285 [04:20<04:45,  1.80s/it]predicting train subjects:  45%|████▍     | 128/285 [04:22<04:55,  1.88s/it]predicting train subjects:  45%|████▌     | 129/285 [04:24<04:56,  1.90s/it]predicting train subjects:  46%|████▌     | 130/285 [04:26<04:53,  1.89s/it]predicting train subjects:  46%|████▌     | 131/285 [04:28<04:48,  1.87s/it]predicting train subjects:  46%|████▋     | 132/285 [04:30<04:44,  1.86s/it]predicting train subjects:  47%|████▋     | 133/285 [04:32<04:45,  1.88s/it]predicting train subjects:  47%|████▋     | 134/285 [04:34<04:42,  1.87s/it]predicting train subjects:  47%|████▋     | 135/285 [04:36<04:42,  1.88s/it]predicting train subjects:  48%|████▊     | 136/285 [04:38<04:48,  1.94s/it]predicting train subjects:  48%|████▊     | 137/285 [04:40<04:42,  1.91s/it]predicting train subjects:  48%|████▊     | 138/285 [04:41<04:35,  1.87s/it]predicting train subjects:  49%|████▉     | 139/285 [04:43<04:33,  1.87s/it]predicting train subjects:  49%|████▉     | 140/285 [04:45<04:20,  1.80s/it]predicting train subjects:  49%|████▉     | 141/285 [04:47<04:20,  1.81s/it]predicting train subjects:  50%|████▉     | 142/285 [04:48<04:10,  1.75s/it]predicting train subjects:  50%|█████     | 143/285 [04:50<04:15,  1.80s/it]predicting train subjects:  51%|█████     | 144/285 [04:52<04:12,  1.79s/it]predicting train subjects:  51%|█████     | 145/285 [04:54<04:12,  1.80s/it]predicting train subjects:  51%|█████     | 146/285 [04:56<04:07,  1.78s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:57<04:04,  1.77s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:59<04:02,  1.77s/it]predicting train subjects:  52%|█████▏    | 149/285 [05:01<04:01,  1.78s/it]predicting train subjects:  53%|█████▎    | 150/285 [05:02<03:51,  1.72s/it]predicting train subjects:  53%|█████▎    | 151/285 [05:04<03:45,  1.68s/it]predicting train subjects:  53%|█████▎    | 152/285 [05:06<03:44,  1.68s/it]predicting train subjects:  54%|█████▎    | 153/285 [05:08<03:47,  1.72s/it]predicting train subjects:  54%|█████▍    | 154/285 [05:09<03:38,  1.67s/it]predicting train subjects:  54%|█████▍    | 155/285 [05:11<03:44,  1.73s/it]predicting train subjects:  55%|█████▍    | 156/285 [05:13<03:41,  1.72s/it]predicting train subjects:  55%|█████▌    | 157/285 [05:14<03:33,  1.67s/it]predicting train subjects:  55%|█████▌    | 158/285 [05:16<03:35,  1.70s/it]predicting train subjects:  56%|█████▌    | 159/285 [05:17<03:27,  1.65s/it]predicting train subjects:  56%|█████▌    | 160/285 [05:19<03:25,  1.64s/it]predicting train subjects:  56%|█████▋    | 161/285 [05:21<03:25,  1.66s/it]predicting train subjects:  57%|█████▋    | 162/285 [05:22<03:23,  1.66s/it]predicting train subjects:  57%|█████▋    | 163/285 [05:24<03:22,  1.66s/it]predicting train subjects:  58%|█████▊    | 164/285 [05:26<03:24,  1.69s/it]predicting train subjects:  58%|█████▊    | 165/285 [05:28<03:22,  1.69s/it]predicting train subjects:  58%|█████▊    | 166/285 [05:29<03:22,  1.70s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:31<03:21,  1.71s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:33<03:22,  1.74s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:35<03:20,  1.73s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:36<03:18,  1.73s/it]predicting train subjects:  60%|██████    | 171/285 [05:38<03:14,  1.71s/it]predicting train subjects:  60%|██████    | 172/285 [05:39<03:06,  1.65s/it]predicting train subjects:  61%|██████    | 173/285 [05:41<03:00,  1.61s/it]predicting train subjects:  61%|██████    | 174/285 [05:43<02:59,  1.62s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:44<02:59,  1.63s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:46<03:00,  1.65s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:48<02:57,  1.65s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:49<02:53,  1.62s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:51<02:50,  1.61s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:52<02:46,  1.59s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:54<02:47,  1.61s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:55<02:44,  1.60s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:57<02:42,  1.59s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:59<02:40,  1.59s/it]predicting train subjects:  65%|██████▍   | 185/285 [06:00<02:42,  1.62s/it]predicting train subjects:  65%|██████▌   | 186/285 [06:02<02:37,  1.59s/it]predicting train subjects:  66%|██████▌   | 187/285 [06:03<02:35,  1.58s/it]predicting train subjects:  66%|██████▌   | 188/285 [06:05<02:32,  1.57s/it]predicting train subjects:  66%|██████▋   | 189/285 [06:07<02:33,  1.60s/it]predicting train subjects:  67%|██████▋   | 190/285 [06:08<02:30,  1.58s/it]predicting train subjects:  67%|██████▋   | 191/285 [06:10<02:33,  1.63s/it]predicting train subjects:  67%|██████▋   | 192/285 [06:12<02:32,  1.64s/it]predicting train subjects:  68%|██████▊   | 193/285 [06:13<02:27,  1.60s/it]predicting train subjects:  68%|██████▊   | 194/285 [06:15<02:20,  1.54s/it]predicting train subjects:  68%|██████▊   | 195/285 [06:16<02:19,  1.55s/it]predicting train subjects:  69%|██████▉   | 196/285 [06:18<02:24,  1.62s/it]predicting train subjects:  69%|██████▉   | 197/285 [06:20<02:28,  1.69s/it]predicting train subjects:  69%|██████▉   | 198/285 [06:22<02:30,  1.74s/it]predicting train subjects:  70%|██████▉   | 199/285 [06:24<02:34,  1.80s/it]predicting train subjects:  70%|███████   | 200/285 [06:26<02:39,  1.87s/it]predicting train subjects:  71%|███████   | 201/285 [06:28<02:39,  1.90s/it]predicting train subjects:  71%|███████   | 202/285 [06:29<02:34,  1.86s/it]predicting train subjects:  71%|███████   | 203/285 [06:31<02:32,  1.86s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:33<02:31,  1.87s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:35<02:31,  1.90s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:37<02:30,  1.91s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:39<02:29,  1.92s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:41<02:24,  1.87s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:43<02:22,  1.88s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:45<02:24,  1.92s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:47<02:28,  2.00s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:49<02:25,  1.99s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:51<02:22,  1.97s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:52<02:13,  1.88s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:54<02:06,  1.81s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:56<02:00,  1.75s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:57<01:56,  1.71s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:59<01:49,  1.64s/it]predicting train subjects:  77%|███████▋  | 219/285 [07:00<01:50,  1.67s/it]predicting train subjects:  77%|███████▋  | 220/285 [07:02<01:46,  1.64s/it]predicting train subjects:  78%|███████▊  | 221/285 [07:04<01:46,  1.67s/it]predicting train subjects:  78%|███████▊  | 222/285 [07:05<01:45,  1.67s/it]predicting train subjects:  78%|███████▊  | 223/285 [07:07<01:46,  1.71s/it]predicting train subjects:  79%|███████▊  | 224/285 [07:09<01:43,  1.70s/it]predicting train subjects:  79%|███████▉  | 225/285 [07:11<01:42,  1.71s/it]predicting train subjects:  79%|███████▉  | 226/285 [07:12<01:38,  1.66s/it]predicting train subjects:  80%|███████▉  | 227/285 [07:14<01:34,  1.62s/it]predicting train subjects:  80%|████████  | 228/285 [07:15<01:33,  1.64s/it]predicting train subjects:  80%|████████  | 229/285 [07:17<01:29,  1.61s/it]predicting train subjects:  81%|████████  | 230/285 [07:18<01:27,  1.58s/it]predicting train subjects:  81%|████████  | 231/285 [07:20<01:25,  1.59s/it]predicting train subjects:  81%|████████▏ | 232/285 [07:22<01:32,  1.75s/it]predicting train subjects:  82%|████████▏ | 233/285 [07:24<01:35,  1.84s/it]predicting train subjects:  82%|████████▏ | 234/285 [07:26<01:36,  1.90s/it]predicting train subjects:  82%|████████▏ | 235/285 [07:28<01:35,  1.92s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:30<01:35,  1.95s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:32<01:33,  1.94s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:34<01:33,  1.98s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:36<01:32,  2.01s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:38<01:29,  1.98s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:40<01:25,  1.94s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:42<01:25,  1.99s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:44<01:26,  2.06s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:46<01:21,  2.00s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:48<01:19,  1.98s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:50<01:19,  2.03s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:52<01:16,  2.03s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:54<01:13,  1.98s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:56<01:12,  2.01s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:58<01:05,  1.86s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:59<00:59,  1.76s/it]predicting train subjects:  88%|████████▊ | 252/285 [08:01<00:55,  1.68s/it]predicting train subjects:  89%|████████▉ | 253/285 [08:02<00:52,  1.63s/it]predicting train subjects:  89%|████████▉ | 254/285 [08:04<00:49,  1.60s/it]predicting train subjects:  89%|████████▉ | 255/285 [08:05<00:47,  1.59s/it]predicting train subjects:  90%|████████▉ | 256/285 [08:07<00:45,  1.58s/it]predicting train subjects:  90%|█████████ | 257/285 [08:09<00:43,  1.56s/it]predicting train subjects:  91%|█████████ | 258/285 [08:10<00:41,  1.52s/it]predicting train subjects:  91%|█████████ | 259/285 [08:11<00:39,  1.51s/it]predicting train subjects:  91%|█████████ | 260/285 [08:13<00:37,  1.49s/it]predicting train subjects:  92%|█████████▏| 261/285 [08:14<00:35,  1.47s/it]predicting train subjects:  92%|█████████▏| 262/285 [08:16<00:33,  1.45s/it]predicting train subjects:  92%|█████████▏| 263/285 [08:17<00:31,  1.45s/it]predicting train subjects:  93%|█████████▎| 264/285 [08:19<00:30,  1.46s/it]predicting train subjects:  93%|█████████▎| 265/285 [08:20<00:29,  1.47s/it]predicting train subjects:  93%|█████████▎| 266/285 [08:22<00:28,  1.51s/it]predicting train subjects:  94%|█████████▎| 267/285 [08:23<00:27,  1.54s/it]predicting train subjects:  94%|█████████▍| 268/285 [08:25<00:28,  1.69s/it]predicting train subjects:  94%|█████████▍| 269/285 [08:28<00:29,  1.85s/it]predicting train subjects:  95%|█████████▍| 270/285 [08:30<00:29,  1.94s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:32<00:29,  2.09s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:34<00:26,  2.03s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:36<00:23,  2.00s/it]predicting train subjects:  96%|█████████▌| 274/285 [08:38<00:22,  2.03s/it]predicting train subjects:  96%|█████████▋| 275/285 [08:40<00:20,  2.07s/it]predicting train subjects:  97%|█████████▋| 276/285 [08:42<00:18,  2.04s/it]predicting train subjects:  97%|█████████▋| 277/285 [08:44<00:16,  2.01s/it]predicting train subjects:  98%|█████████▊| 278/285 [08:46<00:14,  2.07s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:48<00:12,  2.04s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:51<00:10,  2.08s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:53<00:08,  2.05s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:55<00:06,  2.04s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:57<00:04,  2.08s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:59<00:02,  2.07s/it]predicting train subjects: 100%|██████████| 285/285 [09:01<00:00,  2.04s/it]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<07:31,  1.59s/it]Loading train:   1%|          | 2/285 [00:03<07:46,  1.65s/it]Loading train:   1%|          | 3/285 [00:05<07:58,  1.70s/it]Loading train:   1%|▏         | 4/285 [00:07<08:12,  1.75s/it]Loading train:   2%|▏         | 5/285 [00:08<07:56,  1.70s/it]Loading train:   2%|▏         | 6/285 [00:10<08:43,  1.88s/it]Loading train:   2%|▏         | 7/285 [00:13<09:03,  1.96s/it]Loading train:   3%|▎         | 8/285 [00:14<08:43,  1.89s/it]Loading train:   3%|▎         | 9/285 [00:16<08:49,  1.92s/it]Loading train:   4%|▎         | 10/285 [00:18<08:17,  1.81s/it]Loading train:   4%|▍         | 11/285 [00:20<08:10,  1.79s/it]Loading train:   4%|▍         | 12/285 [00:21<08:03,  1.77s/it]Loading train:   5%|▍         | 13/285 [00:23<08:14,  1.82s/it]Loading train:   5%|▍         | 14/285 [00:25<07:49,  1.73s/it]Loading train:   5%|▌         | 15/285 [00:27<07:53,  1.75s/it]Loading train:   6%|▌         | 16/285 [00:28<07:38,  1.70s/it]Loading train:   6%|▌         | 17/285 [00:30<07:11,  1.61s/it]Loading train:   6%|▋         | 18/285 [00:31<06:56,  1.56s/it]Loading train:   7%|▋         | 19/285 [00:33<06:56,  1.57s/it]Loading train:   7%|▋         | 20/285 [00:34<07:04,  1.60s/it]Loading train:   7%|▋         | 21/285 [00:36<06:50,  1.56s/it]Loading train:   8%|▊         | 22/285 [00:38<07:12,  1.64s/it]Loading train:   8%|▊         | 23/285 [00:39<07:31,  1.72s/it]Loading train:   8%|▊         | 24/285 [00:41<07:30,  1.73s/it]Loading train:   9%|▉         | 25/285 [00:43<07:12,  1.66s/it]Loading train:   9%|▉         | 26/285 [00:44<07:06,  1.65s/it]Loading train:   9%|▉         | 27/285 [00:46<07:08,  1.66s/it]Loading train:  10%|▉         | 28/285 [00:48<07:26,  1.74s/it]Loading train:  10%|█         | 29/285 [00:50<07:32,  1.77s/it]Loading train:  11%|█         | 30/285 [00:51<07:17,  1.72s/it]Loading train:  11%|█         | 31/285 [00:53<07:02,  1.66s/it]Loading train:  11%|█         | 32/285 [00:55<07:04,  1.68s/it]Loading train:  12%|█▏        | 33/285 [00:56<06:44,  1.61s/it]Loading train:  12%|█▏        | 34/285 [00:58<06:33,  1.57s/it]Loading train:  12%|█▏        | 35/285 [00:59<06:19,  1.52s/it]Loading train:  13%|█▎        | 36/285 [01:01<06:25,  1.55s/it]Loading train:  13%|█▎        | 37/285 [01:02<06:04,  1.47s/it]Loading train:  13%|█▎        | 38/285 [01:03<06:07,  1.49s/it]Loading train:  14%|█▎        | 39/285 [01:05<06:10,  1.51s/it]Loading train:  14%|█▍        | 40/285 [01:06<05:51,  1.43s/it]Loading train:  14%|█▍        | 41/285 [01:08<06:21,  1.56s/it]Loading train:  15%|█▍        | 42/285 [01:10<06:13,  1.53s/it]Loading train:  15%|█▌        | 43/285 [01:12<07:03,  1.75s/it]Loading train:  15%|█▌        | 44/285 [01:13<06:35,  1.64s/it]Loading train:  16%|█▌        | 45/285 [01:15<06:16,  1.57s/it]Loading train:  16%|█▌        | 46/285 [01:16<06:17,  1.58s/it]Loading train:  16%|█▋        | 47/285 [01:17<05:51,  1.48s/it]Loading train:  17%|█▋        | 48/285 [01:19<05:40,  1.44s/it]Loading train:  17%|█▋        | 49/285 [01:20<05:26,  1.38s/it]Loading train:  18%|█▊        | 50/285 [01:22<05:48,  1.48s/it]Loading train:  18%|█▊        | 51/285 [01:23<05:45,  1.48s/it]Loading train:  18%|█▊        | 52/285 [01:25<05:41,  1.47s/it]Loading train:  19%|█▊        | 53/285 [01:26<05:32,  1.43s/it]Loading train:  19%|█▉        | 54/285 [01:28<05:41,  1.48s/it]Loading train:  19%|█▉        | 55/285 [01:29<05:28,  1.43s/it]Loading train:  20%|█▉        | 56/285 [01:30<04:57,  1.30s/it]Loading train:  20%|██        | 57/285 [01:31<05:07,  1.35s/it]Loading train:  20%|██        | 58/285 [01:33<05:12,  1.38s/it]Loading train:  21%|██        | 59/285 [01:34<05:12,  1.38s/it]Loading train:  21%|██        | 60/285 [01:36<05:09,  1.37s/it]Loading train:  21%|██▏       | 61/285 [01:37<05:33,  1.49s/it]Loading train:  22%|██▏       | 62/285 [01:39<05:18,  1.43s/it]Loading train:  22%|██▏       | 63/285 [01:40<05:09,  1.39s/it]Loading train:  22%|██▏       | 64/285 [01:42<05:26,  1.48s/it]Loading train:  23%|██▎       | 65/285 [01:44<06:05,  1.66s/it]Loading train:  23%|██▎       | 66/285 [01:46<06:22,  1.75s/it]Loading train:  24%|██▎       | 67/285 [01:47<06:03,  1.67s/it]Loading train:  24%|██▍       | 68/285 [01:49<06:05,  1.69s/it]Loading train:  24%|██▍       | 69/285 [01:50<05:38,  1.57s/it]Loading train:  25%|██▍       | 70/285 [01:51<05:21,  1.50s/it]Loading train:  25%|██▍       | 71/285 [01:53<04:57,  1.39s/it]Loading train:  25%|██▌       | 72/285 [01:54<04:51,  1.37s/it]Loading train:  26%|██▌       | 73/285 [01:55<05:04,  1.44s/it]Loading train:  26%|██▌       | 74/285 [01:57<04:58,  1.41s/it]Loading train:  26%|██▋       | 75/285 [01:58<05:01,  1.43s/it]Loading train:  27%|██▋       | 76/285 [02:00<05:10,  1.49s/it]Loading train:  27%|██▋       | 77/285 [02:02<05:15,  1.52s/it]Loading train:  27%|██▋       | 78/285 [02:03<05:14,  1.52s/it]Loading train:  28%|██▊       | 79/285 [02:04<04:57,  1.45s/it]Loading train:  28%|██▊       | 80/285 [02:06<05:06,  1.49s/it]Loading train:  28%|██▊       | 81/285 [02:08<05:15,  1.55s/it]Loading train:  29%|██▉       | 82/285 [02:09<05:23,  1.59s/it]Loading train:  29%|██▉       | 83/285 [02:11<05:15,  1.56s/it]Loading train:  29%|██▉       | 84/285 [02:12<05:01,  1.50s/it]Loading train:  30%|██▉       | 85/285 [02:14<05:09,  1.55s/it]Loading train:  30%|███       | 86/285 [02:15<04:54,  1.48s/it]Loading train:  31%|███       | 87/285 [02:17<05:05,  1.54s/it]Loading train:  31%|███       | 88/285 [02:18<04:53,  1.49s/it]Loading train:  31%|███       | 89/285 [02:19<04:21,  1.34s/it]Loading train:  32%|███▏      | 90/285 [02:20<04:08,  1.27s/it]Loading train:  32%|███▏      | 91/285 [02:21<03:51,  1.19s/it]Loading train:  32%|███▏      | 92/285 [02:23<04:02,  1.26s/it]Loading train:  33%|███▎      | 93/285 [02:24<04:27,  1.39s/it]Loading train:  33%|███▎      | 94/285 [02:26<04:41,  1.47s/it]Loading train:  33%|███▎      | 95/285 [02:28<04:37,  1.46s/it]Loading train:  34%|███▎      | 96/285 [02:29<04:57,  1.57s/it]Loading train:  34%|███▍      | 97/285 [02:31<04:59,  1.59s/it]Loading train:  34%|███▍      | 98/285 [02:33<04:59,  1.60s/it]Loading train:  35%|███▍      | 99/285 [02:34<04:54,  1.58s/it]Loading train:  35%|███▌      | 100/285 [02:36<04:47,  1.55s/it]Loading train:  35%|███▌      | 101/285 [02:37<04:55,  1.60s/it]Loading train:  36%|███▌      | 102/285 [02:39<04:38,  1.52s/it]Loading train:  36%|███▌      | 103/285 [02:40<04:27,  1.47s/it]Loading train:  36%|███▋      | 104/285 [02:41<04:20,  1.44s/it]Loading train:  37%|███▋      | 105/285 [02:43<04:21,  1.45s/it]Loading train:  37%|███▋      | 106/285 [02:44<04:14,  1.42s/it]Loading train:  38%|███▊      | 107/285 [02:46<04:08,  1.40s/it]Loading train:  38%|███▊      | 108/285 [02:47<04:06,  1.39s/it]Loading train:  38%|███▊      | 109/285 [02:48<03:58,  1.35s/it]Loading train:  39%|███▊      | 110/285 [02:50<03:57,  1.36s/it]Loading train:  39%|███▉      | 111/285 [02:50<03:29,  1.20s/it]Loading train:  39%|███▉      | 112/285 [02:51<03:17,  1.14s/it]Loading train:  40%|███▉      | 113/285 [02:52<03:05,  1.08s/it]Loading train:  40%|████      | 114/285 [02:53<02:58,  1.04s/it]Loading train:  40%|████      | 115/285 [02:54<02:54,  1.03s/it]Loading train:  41%|████      | 116/285 [02:55<02:50,  1.01s/it]Loading train:  41%|████      | 117/285 [02:56<02:44,  1.02it/s]Loading train:  41%|████▏     | 118/285 [02:57<02:40,  1.04it/s]Loading train:  42%|████▏     | 119/285 [02:58<02:36,  1.06it/s]Loading train:  42%|████▏     | 120/285 [02:59<02:28,  1.11it/s]Loading train:  42%|████▏     | 121/285 [03:00<02:53,  1.06s/it]Loading train:  43%|████▎     | 122/285 [03:01<02:52,  1.06s/it]Loading train:  43%|████▎     | 123/285 [03:03<02:59,  1.11s/it]Loading train:  44%|████▎     | 124/285 [03:03<02:48,  1.05s/it]Loading train:  44%|████▍     | 125/285 [03:04<02:42,  1.02s/it]Loading train:  44%|████▍     | 126/285 [03:05<02:32,  1.05it/s]Loading train:  45%|████▍     | 127/285 [03:06<02:23,  1.10it/s]Loading train:  45%|████▍     | 128/285 [03:07<02:23,  1.10it/s]Loading train:  45%|████▌     | 129/285 [03:08<02:16,  1.14it/s]Loading train:  46%|████▌     | 130/285 [03:09<02:15,  1.14it/s]Loading train:  46%|████▌     | 131/285 [03:09<02:13,  1.16it/s]Loading train:  46%|████▋     | 132/285 [03:10<02:09,  1.18it/s]Loading train:  47%|████▋     | 133/285 [03:11<02:10,  1.17it/s]Loading train:  47%|████▋     | 134/285 [03:12<02:04,  1.21it/s]Loading train:  47%|████▋     | 135/285 [03:13<02:05,  1.20it/s]Loading train:  48%|████▊     | 136/285 [03:13<02:02,  1.22it/s]Loading train:  48%|████▊     | 137/285 [03:14<02:05,  1.18it/s]Loading train:  48%|████▊     | 138/285 [03:15<02:09,  1.13it/s]Loading train:  49%|████▉     | 139/285 [03:16<02:07,  1.14it/s]Loading train:  49%|████▉     | 140/285 [03:17<02:04,  1.17it/s]Loading train:  49%|████▉     | 141/285 [03:18<02:02,  1.17it/s]Loading train:  50%|████▉     | 142/285 [03:19<02:00,  1.19it/s]Loading train:  50%|█████     | 143/285 [03:20<01:59,  1.19it/s]Loading train:  51%|█████     | 144/285 [03:20<01:54,  1.23it/s]Loading train:  51%|█████     | 145/285 [03:21<01:51,  1.26it/s]Loading train:  51%|█████     | 146/285 [03:22<01:47,  1.29it/s]Loading train:  52%|█████▏    | 147/285 [03:23<01:47,  1.28it/s]Loading train:  52%|█████▏    | 148/285 [03:23<01:47,  1.27it/s]Loading train:  52%|█████▏    | 149/285 [03:24<01:42,  1.33it/s]Loading train:  53%|█████▎    | 150/285 [03:25<01:41,  1.33it/s]Loading train:  53%|█████▎    | 151/285 [03:26<01:41,  1.32it/s]Loading train:  53%|█████▎    | 152/285 [03:26<01:39,  1.34it/s]Loading train:  54%|█████▎    | 153/285 [03:27<01:38,  1.34it/s]Loading train:  54%|█████▍    | 154/285 [03:28<01:39,  1.32it/s]Loading train:  54%|█████▍    | 155/285 [03:29<01:37,  1.33it/s]Loading train:  55%|█████▍    | 156/285 [03:29<01:36,  1.33it/s]Loading train:  55%|█████▌    | 157/285 [03:30<01:41,  1.26it/s]Loading train:  55%|█████▌    | 158/285 [03:31<01:38,  1.29it/s]Loading train:  56%|█████▌    | 159/285 [03:32<01:37,  1.29it/s]Loading train:  56%|█████▌    | 160/285 [03:33<01:45,  1.19it/s]Loading train:  56%|█████▋    | 161/285 [03:33<01:42,  1.21it/s]Loading train:  57%|█████▋    | 162/285 [03:34<01:36,  1.28it/s]Loading train:  57%|█████▋    | 163/285 [03:35<01:37,  1.25it/s]Loading train:  58%|█████▊    | 164/285 [03:36<01:36,  1.26it/s]Loading train:  58%|█████▊    | 165/285 [03:36<01:31,  1.31it/s]Loading train:  58%|█████▊    | 166/285 [03:37<01:34,  1.26it/s]Loading train:  59%|█████▊    | 167/285 [03:38<01:34,  1.25it/s]Loading train:  59%|█████▉    | 168/285 [03:39<01:30,  1.29it/s]Loading train:  59%|█████▉    | 169/285 [03:40<01:29,  1.30it/s]Loading train:  60%|█████▉    | 170/285 [03:40<01:25,  1.34it/s]Loading train:  60%|██████    | 171/285 [03:41<01:25,  1.34it/s]Loading train:  60%|██████    | 172/285 [03:42<01:23,  1.35it/s]Loading train:  61%|██████    | 173/285 [03:43<01:22,  1.35it/s]Loading train:  61%|██████    | 174/285 [03:43<01:24,  1.31it/s]Loading train:  61%|██████▏   | 175/285 [03:44<01:25,  1.29it/s]Loading train:  62%|██████▏   | 176/285 [03:45<01:22,  1.32it/s]Loading train:  62%|██████▏   | 177/285 [03:46<01:22,  1.30it/s]Loading train:  62%|██████▏   | 178/285 [03:46<01:24,  1.27it/s]Loading train:  63%|██████▎   | 179/285 [03:47<01:20,  1.31it/s]Loading train:  63%|██████▎   | 180/285 [03:48<01:18,  1.34it/s]Loading train:  64%|██████▎   | 181/285 [03:49<01:19,  1.30it/s]Loading train:  64%|██████▍   | 182/285 [03:49<01:16,  1.34it/s]Loading train:  64%|██████▍   | 183/285 [03:50<01:15,  1.36it/s]Loading train:  65%|██████▍   | 184/285 [03:51<01:13,  1.37it/s]Loading train:  65%|██████▍   | 185/285 [03:52<01:12,  1.38it/s]Loading train:  65%|██████▌   | 186/285 [03:52<01:15,  1.32it/s]Loading train:  66%|██████▌   | 187/285 [03:53<01:10,  1.40it/s]Loading train:  66%|██████▌   | 188/285 [03:54<01:10,  1.37it/s]Loading train:  66%|██████▋   | 189/285 [03:54<01:09,  1.38it/s]Loading train:  67%|██████▋   | 190/285 [03:55<01:10,  1.34it/s]Loading train:  67%|██████▋   | 191/285 [03:56<01:09,  1.35it/s]Loading train:  67%|██████▋   | 192/285 [03:57<01:07,  1.37it/s]Loading train:  68%|██████▊   | 193/285 [03:58<01:10,  1.30it/s]Loading train:  68%|██████▊   | 194/285 [03:58<01:08,  1.32it/s]Loading train:  68%|██████▊   | 195/285 [03:59<01:07,  1.32it/s]Loading train:  69%|██████▉   | 196/285 [04:00<01:13,  1.21it/s]Loading train:  69%|██████▉   | 197/285 [04:01<01:15,  1.17it/s]Loading train:  69%|██████▉   | 198/285 [04:02<01:15,  1.15it/s]Loading train:  70%|██████▉   | 199/285 [04:03<01:13,  1.16it/s]Loading train:  70%|███████   | 200/285 [04:04<01:13,  1.16it/s]Loading train:  71%|███████   | 201/285 [04:04<01:13,  1.14it/s]Loading train:  71%|███████   | 202/285 [04:05<01:11,  1.16it/s]Loading train:  71%|███████   | 203/285 [04:06<01:08,  1.20it/s]Loading train:  72%|███████▏  | 204/285 [04:07<01:09,  1.16it/s]Loading train:  72%|███████▏  | 205/285 [04:08<01:06,  1.20it/s]Loading train:  72%|███████▏  | 206/285 [04:09<01:07,  1.17it/s]Loading train:  73%|███████▎  | 207/285 [04:10<01:08,  1.14it/s]Loading train:  73%|███████▎  | 208/285 [04:10<01:06,  1.16it/s]Loading train:  73%|███████▎  | 209/285 [04:11<01:04,  1.17it/s]Loading train:  74%|███████▎  | 210/285 [04:12<01:03,  1.19it/s]Loading train:  74%|███████▍  | 211/285 [04:13<01:01,  1.20it/s]Loading train:  74%|███████▍  | 212/285 [04:14<01:04,  1.14it/s]Loading train:  75%|███████▍  | 213/285 [04:15<01:02,  1.15it/s]Loading train:  75%|███████▌  | 214/285 [04:16<01:00,  1.17it/s]Loading train:  75%|███████▌  | 215/285 [04:16<01:01,  1.15it/s]Loading train:  76%|███████▌  | 216/285 [04:17<00:56,  1.21it/s]Loading train:  76%|███████▌  | 217/285 [04:18<00:54,  1.25it/s]Loading train:  76%|███████▋  | 218/285 [04:19<00:54,  1.24it/s]Loading train:  77%|███████▋  | 219/285 [04:20<00:52,  1.26it/s]Loading train:  77%|███████▋  | 220/285 [04:20<00:49,  1.32it/s]Loading train:  78%|███████▊  | 221/285 [04:21<00:50,  1.27it/s]Loading train:  78%|███████▊  | 222/285 [04:22<00:49,  1.28it/s]Loading train:  78%|███████▊  | 223/285 [04:23<00:47,  1.30it/s]Loading train:  79%|███████▊  | 224/285 [04:23<00:45,  1.34it/s]Loading train:  79%|███████▉  | 225/285 [04:24<00:44,  1.36it/s]Loading train:  79%|███████▉  | 226/285 [04:25<00:44,  1.33it/s]Loading train:  80%|███████▉  | 227/285 [04:25<00:42,  1.35it/s]Loading train:  80%|████████  | 228/285 [04:26<00:41,  1.36it/s]Loading train:  80%|████████  | 229/285 [04:27<00:42,  1.31it/s]Loading train:  81%|████████  | 230/285 [04:28<00:41,  1.32it/s]Loading train:  81%|████████  | 231/285 [04:28<00:40,  1.35it/s]Loading train:  81%|████████▏ | 232/285 [04:29<00:43,  1.21it/s]Loading train:  82%|████████▏ | 233/285 [04:31<00:46,  1.13it/s]Loading train:  82%|████████▏ | 234/285 [04:31<00:45,  1.11it/s]Loading train:  82%|████████▏ | 235/285 [04:32<00:47,  1.06it/s]Loading train:  83%|████████▎ | 236/285 [04:33<00:46,  1.05it/s]Loading train:  83%|████████▎ | 237/285 [04:34<00:45,  1.06it/s]Loading train:  84%|████████▎ | 238/285 [04:35<00:43,  1.08it/s]Loading train:  84%|████████▍ | 239/285 [04:36<00:42,  1.08it/s]Loading train:  84%|████████▍ | 240/285 [04:37<00:42,  1.06it/s]Loading train:  85%|████████▍ | 241/285 [04:38<00:42,  1.02it/s]Loading train:  85%|████████▍ | 242/285 [04:39<00:42,  1.02it/s]Loading train:  85%|████████▌ | 243/285 [04:40<00:43,  1.03s/it]Loading train:  86%|████████▌ | 244/285 [04:41<00:40,  1.01it/s]Loading train:  86%|████████▌ | 245/285 [04:42<00:39,  1.01it/s]Loading train:  86%|████████▋ | 246/285 [04:43<00:37,  1.03it/s]Loading train:  87%|████████▋ | 247/285 [04:44<00:36,  1.04it/s]Loading train:  87%|████████▋ | 248/285 [04:45<00:36,  1.00it/s]Loading train:  87%|████████▋ | 249/285 [04:46<00:34,  1.03it/s]Loading train:  88%|████████▊ | 250/285 [04:47<00:32,  1.07it/s]Loading train:  88%|████████▊ | 251/285 [04:48<00:28,  1.19it/s]Loading train:  88%|████████▊ | 252/285 [04:48<00:26,  1.24it/s]Loading train:  89%|████████▉ | 253/285 [04:49<00:25,  1.25it/s]Loading train:  89%|████████▉ | 254/285 [04:50<00:23,  1.31it/s]Loading train:  89%|████████▉ | 255/285 [04:50<00:21,  1.38it/s]Loading train:  90%|████████▉ | 256/285 [04:51<00:20,  1.42it/s]Loading train:  90%|█████████ | 257/285 [04:52<00:21,  1.33it/s]Loading train:  91%|█████████ | 258/285 [04:53<00:20,  1.31it/s]Loading train:  91%|█████████ | 259/285 [04:54<00:20,  1.26it/s]Loading train:  91%|█████████ | 260/285 [04:54<00:19,  1.27it/s]Loading train:  92%|█████████▏| 261/285 [04:55<00:18,  1.33it/s]Loading train:  92%|█████████▏| 262/285 [04:56<00:17,  1.28it/s]Loading train:  92%|█████████▏| 263/285 [04:57<00:17,  1.29it/s]Loading train:  93%|█████████▎| 264/285 [04:57<00:15,  1.33it/s]Loading train:  93%|█████████▎| 265/285 [04:58<00:15,  1.26it/s]Loading train:  93%|█████████▎| 266/285 [04:59<00:15,  1.24it/s]Loading train:  94%|█████████▎| 267/285 [05:00<00:15,  1.18it/s]Loading train:  94%|█████████▍| 268/285 [05:01<00:15,  1.11it/s]Loading train:  94%|█████████▍| 269/285 [05:02<00:15,  1.05it/s]Loading train:  95%|█████████▍| 270/285 [05:03<00:14,  1.02it/s]Loading train:  95%|█████████▌| 271/285 [05:04<00:13,  1.03it/s]Loading train:  95%|█████████▌| 272/285 [05:05<00:12,  1.04it/s]Loading train:  96%|█████████▌| 273/285 [05:06<00:11,  1.01it/s]Loading train:  96%|█████████▌| 274/285 [05:07<00:10,  1.03it/s]Loading train:  96%|█████████▋| 275/285 [05:08<00:10,  1.00s/it]Loading train:  97%|█████████▋| 276/285 [05:09<00:08,  1.03it/s]Loading train:  97%|█████████▋| 277/285 [05:10<00:07,  1.01it/s]Loading train:  98%|█████████▊| 278/285 [05:11<00:06,  1.01it/s]Loading train:  98%|█████████▊| 279/285 [05:12<00:06,  1.01s/it]Loading train:  98%|█████████▊| 280/285 [05:13<00:04,  1.03it/s]Loading train:  99%|█████████▊| 281/285 [05:14<00:03,  1.04it/s]Loading train:  99%|█████████▉| 282/285 [05:15<00:02,  1.02it/s]Loading train:  99%|█████████▉| 283/285 [05:16<00:01,  1.03it/s]Loading train: 100%|█████████▉| 284/285 [05:17<00:01,  1.02s/it]Loading train: 100%|██████████| 285/285 [05:18<00:00,  1.02it/s]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   9%|▉         | 25/285 [00:00<00:01, 248.27it/s]concatenating: train:  19%|█▊        | 53/285 [00:00<00:00, 254.64it/s]concatenating: train:  29%|██▉       | 83/285 [00:00<00:00, 264.83it/s]concatenating: train:  40%|████      | 114/285 [00:00<00:00, 275.67it/s]concatenating: train:  52%|█████▏    | 147/285 [00:00<00:00, 283.72it/s]concatenating: train:  63%|██████▎   | 180/285 [00:00<00:00, 296.09it/s]concatenating: train:  75%|███████▍  | 213/285 [00:00<00:00, 303.26it/s]concatenating: train:  86%|████████▋ | 246/285 [00:00<00:00, 308.98it/s]concatenating: train:  97%|█████████▋| 276/285 [00:01<00:00, 195.80it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 243.90it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.24s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.19s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 135.44it/s]2019-07-06 19:41:31.273910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 19:41:31.274010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 19:41:31.274025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 19:41:31.274033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 19:41:31.274501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:08,  4.72it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:06,  5.48it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:07,  4.96it/s]loading the weights for Unet:  20%|██        | 8/40 [00:00<00:05,  6.34it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:05,  5.60it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:04,  6.32it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  5.59it/s]loading the weights for Unet:  40%|████      | 16/40 [00:01<00:03,  7.05it/s]loading the weights for Unet:  45%|████▌     | 18/40 [00:02<00:03,  7.11it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.84it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  5.24it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:02<00:02,  6.36it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  7.04it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  6.03it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:01,  6.72it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:03<00:01,  5.63it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:03<00:01,  6.60it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  7.31it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.94it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:04<00:00,  6.57it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:04<00:00,  5.56it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:04<00:00,  8.19it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 80, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 80, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 80, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 80, 20)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 80, 20)   3620        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 80, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 80, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 80, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 80, 20)   3620        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 80, 20)   80          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 80, 20)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 80, 20)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 80, 20)   3620        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 80, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 80, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 80, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 80, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 80, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 40, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 40, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 40, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 40, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 40, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 40, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 40, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 20, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 20, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 20, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 20, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 20, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 20, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 20, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 20, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 40, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 40, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 40, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 40, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 40, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 40, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 13)   273         dropout_8[0][0]                  
==================================================================================================
Total params: 195,213
Trainable params: 51,613
Non-trainable params: 143,600
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 18s - loss: 297.1925 - acc: 0.0904 - mDice: 0.0160 - val_loss: 83.9037 - val_acc: 0.7988 - val_mDice: 0.0116

Epoch 00001: val_mDice improved from -inf to 0.01162, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 11s - loss: 65.3348 - acc: 0.7349 - mDice: 0.0143 - val_loss: 21.9020 - val_acc: 0.9047 - val_mDice: 0.0122

Epoch 00002: val_mDice improved from 0.01162 to 0.01219, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 10s - loss: 28.0522 - acc: 0.8547 - mDice: 0.0139 - val_loss: 11.2329 - val_acc: 0.9047 - val_mDice: 0.0121

Epoch 00003: val_mDice did not improve from 0.01219
Epoch 4/300
 - 11s - loss: 17.5619 - acc: 0.8646 - mDice: 0.0178 - val_loss: 8.2089 - val_acc: 0.9047 - val_mDice: 0.0091

Epoch 00004: val_mDice did not improve from 0.01219
Epoch 5/300
 - 10s - loss: 13.1741 - acc: 0.8670 - mDice: 0.0237 - val_loss: 6.9589 - val_acc: 0.9047 - val_mDice: 0.0116

Epoch 00005: val_mDice did not improve from 0.01219
Epoch 6/300
 - 11s - loss: 10.8754 - acc: 0.8678 - mDice: 0.0283 - val_loss: 6.1109 - val_acc: 0.9047 - val_mDice: 0.0186

Epoch 00006: val_mDice improved from 0.01219 to 0.01859, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 10s - loss: 9.3661 - acc: 0.8681 - mDice: 0.0338 - val_loss: 5.4033 - val_acc: 0.9047 - val_mDice: 0.0360

Epoch 00007: val_mDice improved from 0.01859 to 0.03603, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 10s - loss: 8.3470 - acc: 0.8684 - mDice: 0.0408 - val_loss: 5.2084 - val_acc: 0.9047 - val_mDice: 0.0434

Epoch 00008: val_mDice improved from 0.03603 to 0.04338, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 10s - loss: 7.6294 - acc: 0.8687 - mDice: 0.0473 - val_loss: 5.2210 - val_acc: 0.9047 - val_mDice: 0.0446

Epoch 00009: val_mDice improved from 0.04338 to 0.04461, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 10s - loss: 7.0860 - acc: 0.8690 - mDice: 0.0538 - val_loss: 4.8247 - val_acc: 0.9047 - val_mDice: 0.0575

Epoch 00010: val_mDice improved from 0.04461 to 0.05749, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 11s - loss: 6.6484 - acc: 0.8695 - mDice: 0.0609 - val_loss: 4.6328 - val_acc: 0.9047 - val_mDice: 0.0712

Epoch 00011: val_mDice improved from 0.05749 to 0.07115, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 10s - loss: 6.2774 - acc: 0.8698 - mDice: 0.0690 - val_loss: 4.4224 - val_acc: 0.9047 - val_mDice: 0.0798

Epoch 00012: val_mDice improved from 0.07115 to 0.07978, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 11s - loss: 5.9760 - acc: 0.8699 - mDice: 0.0762 - val_loss: 4.2710 - val_acc: 0.9047 - val_mDice: 0.0884

Epoch 00013: val_mDice improved from 0.07978 to 0.08839, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 10s - loss: 5.6985 - acc: 0.8698 - mDice: 0.0847 - val_loss: 4.2279 - val_acc: 0.9047 - val_mDice: 0.0941

Epoch 00014: val_mDice improved from 0.08839 to 0.09406, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 11s - loss: 5.4540 - acc: 0.8699 - mDice: 0.0927 - val_loss: 3.9865 - val_acc: 0.9050 - val_mDice: 0.1093

Epoch 00015: val_mDice improved from 0.09406 to 0.10935, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 10s - loss: 5.2468 - acc: 0.8701 - mDice: 0.1003 - val_loss: 3.9076 - val_acc: 0.9048 - val_mDice: 0.1166

Epoch 00016: val_mDice improved from 0.10935 to 0.11656, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 11s - loss: 5.0683 - acc: 0.8701 - mDice: 0.1077 - val_loss: 4.1199 - val_acc: 0.9047 - val_mDice: 0.1098

Epoch 00017: val_mDice did not improve from 0.11656
Epoch 18/300
 - 10s - loss: 4.8908 - acc: 0.8705 - mDice: 0.1158 - val_loss: 3.7979 - val_acc: 0.9049 - val_mDice: 0.1327

Epoch 00018: val_mDice improved from 0.11656 to 0.13275, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 11s - loss: 4.7435 - acc: 0.8707 - mDice: 0.1234 - val_loss: 3.6200 - val_acc: 0.9051 - val_mDice: 0.1489

Epoch 00019: val_mDice improved from 0.13275 to 0.14886, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 10s - loss: 4.6137 - acc: 0.8711 - mDice: 0.1325 - val_loss: 4.0120 - val_acc: 0.9048 - val_mDice: 0.1318

Epoch 00020: val_mDice did not improve from 0.14886
Epoch 21/300
 - 10s - loss: 4.4699 - acc: 0.8714 - mDice: 0.1417 - val_loss: 3.8748 - val_acc: 0.9047 - val_mDice: 0.1444

Epoch 00021: val_mDice did not improve from 0.14886
Epoch 22/300
 - 10s - loss: 4.3504 - acc: 0.8719 - mDice: 0.1501 - val_loss: 3.4872 - val_acc: 0.9060 - val_mDice: 0.1791

Epoch 00022: val_mDice improved from 0.14886 to 0.17907, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 10s - loss: 4.2342 - acc: 0.8721 - mDice: 0.1593 - val_loss: 3.5615 - val_acc: 0.9052 - val_mDice: 0.1750

Epoch 00023: val_mDice did not improve from 0.17907
Epoch 24/300
 - 10s - loss: 4.1182 - acc: 0.8727 - mDice: 0.1686 - val_loss: 3.6840 - val_acc: 0.9048 - val_mDice: 0.1748

Epoch 00024: val_mDice did not improve from 0.17907
Epoch 25/300
 - 11s - loss: 4.0085 - acc: 0.8733 - mDice: 0.1776 - val_loss: 3.5221 - val_acc: 0.9049 - val_mDice: 0.1900

Epoch 00025: val_mDice improved from 0.17907 to 0.19001, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 10s - loss: 3.9283 - acc: 0.8734 - mDice: 0.1847 - val_loss: 3.5106 - val_acc: 0.9053 - val_mDice: 0.2000

Epoch 00026: val_mDice improved from 0.19001 to 0.19999, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 10s - loss: 3.8421 - acc: 0.8739 - mDice: 0.1932 - val_loss: 3.3169 - val_acc: 0.9054 - val_mDice: 0.2158

Epoch 00027: val_mDice improved from 0.19999 to 0.21578, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 10s - loss: 3.7519 - acc: 0.8743 - mDice: 0.2022 - val_loss: 3.7276 - val_acc: 0.9049 - val_mDice: 0.1983

Epoch 00028: val_mDice did not improve from 0.21578
Epoch 29/300
 - 11s - loss: 3.6863 - acc: 0.8748 - mDice: 0.2089 - val_loss: 3.6556 - val_acc: 0.9058 - val_mDice: 0.2108

Epoch 00029: val_mDice did not improve from 0.21578
Epoch 30/300
 - 10s - loss: 3.6000 - acc: 0.8754 - mDice: 0.2179 - val_loss: 3.6395 - val_acc: 0.9056 - val_mDice: 0.2165

Epoch 00030: val_mDice improved from 0.21578 to 0.21652, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 10s - loss: 3.5280 - acc: 0.8760 - mDice: 0.2257 - val_loss: 3.4279 - val_acc: 0.9060 - val_mDice: 0.2280

Epoch 00031: val_mDice improved from 0.21652 to 0.22798, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 11s - loss: 3.4770 - acc: 0.8766 - mDice: 0.2312 - val_loss: 3.3451 - val_acc: 0.9071 - val_mDice: 0.2414

Epoch 00032: val_mDice improved from 0.22798 to 0.24140, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 10s - loss: 3.4142 - acc: 0.8773 - mDice: 0.2385 - val_loss: 4.0806 - val_acc: 0.9055 - val_mDice: 0.2002

Epoch 00033: val_mDice did not improve from 0.24140
Epoch 34/300
 - 11s - loss: 3.3528 - acc: 0.8781 - mDice: 0.2455 - val_loss: 3.1340 - val_acc: 0.9105 - val_mDice: 0.2601

Epoch 00034: val_mDice improved from 0.24140 to 0.26011, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 10s - loss: 3.2907 - acc: 0.8790 - mDice: 0.2527 - val_loss: 3.3355 - val_acc: 0.9111 - val_mDice: 0.2590

Epoch 00035: val_mDice did not improve from 0.26011
Epoch 36/300
 - 10s - loss: 3.2374 - acc: 0.8800 - mDice: 0.2597 - val_loss: 3.3159 - val_acc: 0.9104 - val_mDice: 0.2556

Epoch 00036: val_mDice did not improve from 0.26011
Epoch 37/300
 - 11s - loss: 3.1846 - acc: 0.8811 - mDice: 0.2659 - val_loss: 3.2404 - val_acc: 0.9115 - val_mDice: 0.2660

Epoch 00037: val_mDice improved from 0.26011 to 0.26604, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 10s - loss: 3.1358 - acc: 0.8818 - mDice: 0.2728 - val_loss: 3.3637 - val_acc: 0.9124 - val_mDice: 0.2641

Epoch 00038: val_mDice did not improve from 0.26604
Epoch 39/300
 - 10s - loss: 3.0914 - acc: 0.8829 - mDice: 0.2786 - val_loss: 3.2573 - val_acc: 0.9145 - val_mDice: 0.2810

Epoch 00039: val_mDice improved from 0.26604 to 0.28095, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 11s - loss: 3.0299 - acc: 0.8842 - mDice: 0.2880 - val_loss: 3.1512 - val_acc: 0.9156 - val_mDice: 0.2912

Epoch 00040: val_mDice improved from 0.28095 to 0.29122, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 10s - loss: 2.9903 - acc: 0.8851 - mDice: 0.2952 - val_loss: 3.0771 - val_acc: 0.9172 - val_mDice: 0.2997

Epoch 00041: val_mDice improved from 0.29122 to 0.29968, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 11s - loss: 2.9414 - acc: 0.8862 - mDice: 0.3019 - val_loss: 3.3296 - val_acc: 0.9133 - val_mDice: 0.2872

Epoch 00042: val_mDice did not improve from 0.29968
Epoch 43/300
 - 10s - loss: 2.9162 - acc: 0.8871 - mDice: 0.3065 - val_loss: 3.3562 - val_acc: 0.9152 - val_mDice: 0.2957

Epoch 00043: val_mDice did not improve from 0.29968
Epoch 44/300
 - 10s - loss: 2.8655 - acc: 0.8884 - mDice: 0.3137 - val_loss: 3.1555 - val_acc: 0.9166 - val_mDice: 0.3025

Epoch 00044: val_mDice improved from 0.29968 to 0.30252, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 10s - loss: 2.8270 - acc: 0.8891 - mDice: 0.3192 - val_loss: 3.5669 - val_acc: 0.9149 - val_mDice: 0.2856

Epoch 00045: val_mDice did not improve from 0.30252
Epoch 46/300
 - 11s - loss: 2.8053 - acc: 0.8899 - mDice: 0.3240 - val_loss: 3.0173 - val_acc: 0.9186 - val_mDice: 0.3188

Epoch 00046: val_mDice improved from 0.30252 to 0.31876, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 10s - loss: 2.7628 - acc: 0.8906 - mDice: 0.3299 - val_loss: 3.3197 - val_acc: 0.9176 - val_mDice: 0.3026

Epoch 00047: val_mDice did not improve from 0.31876
Epoch 48/300
 - 10s - loss: 2.7351 - acc: 0.8913 - mDice: 0.3349 - val_loss: 3.3628 - val_acc: 0.9178 - val_mDice: 0.3119

Epoch 00048: val_mDice did not improve from 0.31876
Epoch 49/300
 - 11s - loss: 2.7168 - acc: 0.8918 - mDice: 0.3384 - val_loss: 3.2697 - val_acc: 0.9168 - val_mDice: 0.3152

Epoch 00049: val_mDice did not improve from 0.31876
Epoch 50/300
 - 10s - loss: 2.6718 - acc: 0.8931 - mDice: 0.3469 - val_loss: 3.4040 - val_acc: 0.9175 - val_mDice: 0.3126

Epoch 00050: val_mDice did not improve from 0.31876
Epoch 51/300
 - 11s - loss: 2.6453 - acc: 0.8941 - mDice: 0.3518 - val_loss: 2.9665 - val_acc: 0.9216 - val_mDice: 0.3388

Epoch 00051: val_mDice improved from 0.31876 to 0.33884, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 10s - loss: 2.6062 - acc: 0.8950 - mDice: 0.3592 - val_loss: 3.3363 - val_acc: 0.9202 - val_mDice: 0.3167

Epoch 00052: val_mDice did not improve from 0.33884
Epoch 53/300
 - 10s - loss: 2.5885 - acc: 0.8957 - mDice: 0.3629 - val_loss: 3.4750 - val_acc: 0.9199 - val_mDice: 0.3202

Epoch 00053: val_mDice did not improve from 0.33884
Epoch 54/300
 - 11s - loss: 2.5566 - acc: 0.8966 - mDice: 0.3683 - val_loss: 3.6058 - val_acc: 0.9201 - val_mDice: 0.3203

Epoch 00054: val_mDice did not improve from 0.33884
Epoch 55/300
 - 10s - loss: 2.5377 - acc: 0.8970 - mDice: 0.3716 - val_loss: 3.8295 - val_acc: 0.9208 - val_mDice: 0.3082

Epoch 00055: val_mDice did not improve from 0.33884
Epoch 56/300
 - 11s - loss: 2.5100 - acc: 0.8980 - mDice: 0.3773 - val_loss: 3.3562 - val_acc: 0.9230 - val_mDice: 0.3283

Epoch 00056: val_mDice did not improve from 0.33884
Epoch 57/300
 - 11s - loss: 2.4897 - acc: 0.8990 - mDice: 0.3813 - val_loss: 3.3758 - val_acc: 0.9237 - val_mDice: 0.3337

Epoch 00057: val_mDice did not improve from 0.33884
Epoch 58/300
 - 10s - loss: 2.4657 - acc: 0.8995 - mDice: 0.3857 - val_loss: 3.2652 - val_acc: 0.9272 - val_mDice: 0.3389

Epoch 00058: val_mDice improved from 0.33884 to 0.33889, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 11s - loss: 2.4455 - acc: 0.9001 - mDice: 0.3902 - val_loss: 2.8688 - val_acc: 0.9289 - val_mDice: 0.3659

Epoch 00059: val_mDice improved from 0.33889 to 0.36590, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 60/300
 - 10s - loss: 2.4319 - acc: 0.9008 - mDice: 0.3929 - val_loss: 2.9480 - val_acc: 0.9264 - val_mDice: 0.3643

Epoch 00060: val_mDice did not improve from 0.36590
Epoch 61/300
 - 11s - loss: 2.4153 - acc: 0.9012 - mDice: 0.3964 - val_loss: 3.0068 - val_acc: 0.9291 - val_mDice: 0.3630

Epoch 00061: val_mDice did not improve from 0.36590
Epoch 62/300
 - 10s - loss: 2.4038 - acc: 0.9019 - mDice: 0.3990 - val_loss: 2.9952 - val_acc: 0.9297 - val_mDice: 0.3699

Epoch 00062: val_mDice improved from 0.36590 to 0.36994, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 11s - loss: 2.3812 - acc: 0.9025 - mDice: 0.4034 - val_loss: 3.2907 - val_acc: 0.9267 - val_mDice: 0.3494

Epoch 00063: val_mDice did not improve from 0.36994
Epoch 64/300
 - 10s - loss: 2.3651 - acc: 0.9032 - mDice: 0.4066 - val_loss: 3.1707 - val_acc: 0.9279 - val_mDice: 0.3579

Epoch 00064: val_mDice did not improve from 0.36994
Epoch 65/300
 - 10s - loss: 2.3493 - acc: 0.9037 - mDice: 0.4102 - val_loss: 3.2101 - val_acc: 0.9288 - val_mDice: 0.3596

Epoch 00065: val_mDice did not improve from 0.36994
Epoch 66/300
 - 11s - loss: 2.3293 - acc: 0.9043 - mDice: 0.4138 - val_loss: 3.0688 - val_acc: 0.9309 - val_mDice: 0.3704

Epoch 00066: val_mDice improved from 0.36994 to 0.37044, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 67/300
 - 10s - loss: 2.3086 - acc: 0.9049 - mDice: 0.4185 - val_loss: 3.2212 - val_acc: 0.9283 - val_mDice: 0.3592

Epoch 00067: val_mDice did not improve from 0.37044
Epoch 68/300
 - 11s - loss: 2.3095 - acc: 0.9049 - mDice: 0.4184 - val_loss: 2.9244 - val_acc: 0.9308 - val_mDice: 0.3825

Epoch 00068: val_mDice improved from 0.37044 to 0.38251, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 69/300
 - 10s - loss: 2.2863 - acc: 0.9056 - mDice: 0.4224 - val_loss: 3.0328 - val_acc: 0.9288 - val_mDice: 0.3746

Epoch 00069: val_mDice did not improve from 0.38251
Epoch 70/300
 - 11s - loss: 2.2678 - acc: 0.9062 - mDice: 0.4272 - val_loss: 3.0021 - val_acc: 0.9307 - val_mDice: 0.3778

Epoch 00070: val_mDice did not improve from 0.38251
Epoch 71/300
 - 10s - loss: 2.2721 - acc: 0.9064 - mDice: 0.4263 - val_loss: 3.3230 - val_acc: 0.9289 - val_mDice: 0.3618

Epoch 00071: val_mDice did not improve from 0.38251
Epoch 72/300
 - 10s - loss: 2.2570 - acc: 0.9069 - mDice: 0.4293 - val_loss: 3.3503 - val_acc: 0.9300 - val_mDice: 0.3622

Epoch 00072: val_mDice did not improve from 0.38251
Epoch 73/300
 - 11s - loss: 2.2440 - acc: 0.9073 - mDice: 0.4317 - val_loss: 3.2686 - val_acc: 0.9317 - val_mDice: 0.3651

Epoch 00073: val_mDice did not improve from 0.38251
Epoch 74/300
 - 10s - loss: 2.2303 - acc: 0.9077 - mDice: 0.4348 - val_loss: 3.1150 - val_acc: 0.9309 - val_mDice: 0.3730

Epoch 00074: val_mDice did not improve from 0.38251
Epoch 75/300
 - 11s - loss: 2.2227 - acc: 0.9083 - mDice: 0.4363 - val_loss: 3.0704 - val_acc: 0.9304 - val_mDice: 0.3776

Epoch 00075: val_mDice did not improve from 0.38251
Epoch 76/300
 - 11s - loss: 2.2086 - acc: 0.9085 - mDice: 0.4392 - val_loss: 3.2838 - val_acc: 0.9259 - val_mDice: 0.3640

Epoch 00076: val_mDice did not improve from 0.38251
Epoch 77/300
 - 11s - loss: 2.1973 - acc: 0.9091 - mDice: 0.4417 - val_loss: 3.1537 - val_acc: 0.9284 - val_mDice: 0.3792

Epoch 00077: val_mDice did not improve from 0.38251
Epoch 78/300
 - 11s - loss: 2.1953 - acc: 0.9094 - mDice: 0.4422 - val_loss: 3.2047 - val_acc: 0.9288 - val_mDice: 0.3750

Epoch 00078: val_mDice did not improve from 0.38251
Epoch 79/300
 - 11s - loss: 2.1808 - acc: 0.9096 - mDice: 0.4454 - val_loss: 3.1669 - val_acc: 0.9311 - val_mDice: 0.3843

Epoch 00079: val_mDice improved from 0.38251 to 0.38426, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 80/300
 - 11s - loss: 2.1642 - acc: 0.9103 - mDice: 0.4492 - val_loss: 3.3850 - val_acc: 0.9320 - val_mDice: 0.3750

Epoch 00080: val_mDice did not improve from 0.38426
Epoch 81/300
 - 11s - loss: 2.1627 - acc: 0.9104 - mDice: 0.4491 - val_loss: 3.0596 - val_acc: 0.9312 - val_mDice: 0.3857

Epoch 00081: val_mDice improved from 0.38426 to 0.38566, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 82/300
 - 12s - loss: 2.1629 - acc: 0.9106 - mDice: 0.4489 - val_loss: 3.1429 - val_acc: 0.9313 - val_mDice: 0.3800

Epoch 00082: val_mDice did not improve from 0.38566
Epoch 83/300
 - 11s - loss: 2.1405 - acc: 0.9112 - mDice: 0.4542 - val_loss: 3.3706 - val_acc: 0.9312 - val_mDice: 0.3710

Epoch 00083: val_mDice did not improve from 0.38566
Epoch 84/300
 - 12s - loss: 2.1326 - acc: 0.9115 - mDice: 0.4558 - val_loss: 3.3857 - val_acc: 0.9281 - val_mDice: 0.3755

Epoch 00084: val_mDice did not improve from 0.38566
Epoch 85/300
 - 11s - loss: 2.1302 - acc: 0.9119 - mDice: 0.4560 - val_loss: 3.3174 - val_acc: 0.9285 - val_mDice: 0.3685

Epoch 00085: val_mDice did not improve from 0.38566
Epoch 86/300
 - 11s - loss: 2.1176 - acc: 0.9123 - mDice: 0.4590 - val_loss: 3.3320 - val_acc: 0.9317 - val_mDice: 0.3807

Epoch 00086: val_mDice did not improve from 0.38566
Epoch 87/300
 - 12s - loss: 2.1173 - acc: 0.9121 - mDice: 0.4593 - val_loss: 3.3951 - val_acc: 0.9341 - val_mDice: 0.3788

Epoch 00087: val_mDice did not improve from 0.38566
Epoch 88/300
 - 11s - loss: 2.1009 - acc: 0.9129 - mDice: 0.4627 - val_loss: 3.1846 - val_acc: 0.9315 - val_mDice: 0.3832

Epoch 00088: val_mDice did not improve from 0.38566
Epoch 89/300
 - 12s - loss: 2.0960 - acc: 0.9131 - mDice: 0.4637 - val_loss: 3.3193 - val_acc: 0.9345 - val_mDice: 0.3781

Epoch 00089: val_mDice did not improve from 0.38566
Epoch 90/300
 - 11s - loss: 2.0857 - acc: 0.9132 - mDice: 0.4658 - val_loss: 3.2745 - val_acc: 0.9335 - val_mDice: 0.3883

Epoch 00090: val_mDice improved from 0.38566 to 0.38835, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 91/300
 - 11s - loss: 2.0711 - acc: 0.9138 - mDice: 0.4687 - val_loss: 3.2118 - val_acc: 0.9319 - val_mDice: 0.3892

Epoch 00091: val_mDice improved from 0.38835 to 0.38917, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 92/300
 - 12s - loss: 2.0769 - acc: 0.9139 - mDice: 0.4681 - val_loss: 3.3670 - val_acc: 0.9326 - val_mDice: 0.3891

Epoch 00092: val_mDice did not improve from 0.38917
Epoch 93/300
 - 11s - loss: 2.0731 - acc: 0.9141 - mDice: 0.4694 - val_loss: 3.2235 - val_acc: 0.9335 - val_mDice: 0.3912

Epoch 00093: val_mDice improved from 0.38917 to 0.39118, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 94/300
 - 12s - loss: 2.0580 - acc: 0.9143 - mDice: 0.4715 - val_loss: 3.5991 - val_acc: 0.9326 - val_mDice: 0.3774

Epoch 00094: val_mDice did not improve from 0.39118
Epoch 95/300
 - 12s - loss: 2.0466 - acc: 0.9149 - mDice: 0.4747 - val_loss: 3.4668 - val_acc: 0.9342 - val_mDice: 0.3903

Epoch 00095: val_mDice did not improve from 0.39118
Epoch 96/300
 - 11s - loss: 2.0535 - acc: 0.9148 - mDice: 0.4735 - val_loss: 3.3360 - val_acc: 0.9325 - val_mDice: 0.3891

Epoch 00096: val_mDice did not improve from 0.39118
Epoch 97/300
 - 12s - loss: 2.0404 - acc: 0.9153 - mDice: 0.4766 - val_loss: 3.4065 - val_acc: 0.9329 - val_mDice: 0.3815

Epoch 00097: val_mDice did not improve from 0.39118
Epoch 98/300
 - 11s - loss: 2.0285 - acc: 0.9154 - mDice: 0.4781 - val_loss: 3.2331 - val_acc: 0.9325 - val_mDice: 0.3957

Epoch 00098: val_mDice improved from 0.39118 to 0.39575, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 99/300
 - 12s - loss: 2.0203 - acc: 0.9160 - mDice: 0.4802 - val_loss: 3.7505 - val_acc: 0.9287 - val_mDice: 0.3660

Epoch 00099: val_mDice did not improve from 0.39575
Epoch 100/300
 - 11s - loss: 2.0305 - acc: 0.9158 - mDice: 0.4792 - val_loss: 3.2329 - val_acc: 0.9322 - val_mDice: 0.3901

Epoch 00100: val_mDice did not improve from 0.39575
Epoch 101/300
 - 11s - loss: 2.0103 - acc: 0.9163 - mDice: 0.4821 - val_loss: 3.4637 - val_acc: 0.9319 - val_mDice: 0.3856

Epoch 00101: val_mDice did not improve from 0.39575
Epoch 102/300
 - 11s - loss: 2.0017 - acc: 0.9167 - mDice: 0.4848 - val_loss: 3.2046 - val_acc: 0.9318 - val_mDice: 0.3997

Epoch 00102: val_mDice improved from 0.39575 to 0.39971, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 103/300
 - 11s - loss: 1.9916 - acc: 0.9172 - mDice: 0.4863 - val_loss: 2.9722 - val_acc: 0.9335 - val_mDice: 0.4126

Epoch 00103: val_mDice improved from 0.39971 to 0.41256, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 104/300
 - 12s - loss: 1.9921 - acc: 0.9172 - mDice: 0.4873 - val_loss: 3.0790 - val_acc: 0.9348 - val_mDice: 0.4075

Epoch 00104: val_mDice did not improve from 0.41256
Epoch 105/300
 - 11s - loss: 1.9860 - acc: 0.9173 - mDice: 0.4883 - val_loss: 3.1911 - val_acc: 0.9353 - val_mDice: 0.4066

Epoch 00105: val_mDice did not improve from 0.41256
Epoch 106/300
 - 11s - loss: 1.9898 - acc: 0.9174 - mDice: 0.4875 - val_loss: 3.2167 - val_acc: 0.9373 - val_mDice: 0.4113

Epoch 00106: val_mDice did not improve from 0.41256
Epoch 107/300
 - 11s - loss: 1.9779 - acc: 0.9179 - mDice: 0.4896 - val_loss: 3.3591 - val_acc: 0.9341 - val_mDice: 0.3966

Epoch 00107: val_mDice did not improve from 0.41256
Epoch 108/300
 - 11s - loss: 1.9696 - acc: 0.9182 - mDice: 0.4917 - val_loss: 3.5421 - val_acc: 0.9353 - val_mDice: 0.3978

Epoch 00108: val_mDice did not improve from 0.41256
Epoch 109/300
 - 12s - loss: 1.9651 - acc: 0.9183 - mDice: 0.4926 - val_loss: 3.0554 - val_acc: 0.9314 - val_mDice: 0.4100

Epoch 00109: val_mDice did not improve from 0.41256
Epoch 110/300
 - 11s - loss: 1.9606 - acc: 0.9185 - mDice: 0.4943 - val_loss: 3.3424 - val_acc: 0.9320 - val_mDice: 0.3938

Epoch 00110: val_mDice did not improve from 0.41256
Epoch 111/300
 - 12s - loss: 1.9496 - acc: 0.9189 - mDice: 0.4957 - val_loss: 3.2056 - val_acc: 0.9303 - val_mDice: 0.4027

Epoch 00111: val_mDice did not improve from 0.41256
Epoch 112/300
 - 11s - loss: 1.9494 - acc: 0.9192 - mDice: 0.4964 - val_loss: 3.3734 - val_acc: 0.9333 - val_mDice: 0.3943

Epoch 00112: val_mDice did not improve from 0.41256
Epoch 113/300
 - 11s - loss: 1.9493 - acc: 0.9192 - mDice: 0.4964 - val_loss: 3.2430 - val_acc: 0.9355 - val_mDice: 0.4034

Epoch 00113: val_mDice did not improve from 0.41256
Epoch 114/300
 - 11s - loss: 1.9403 - acc: 0.9194 - mDice: 0.4979 - val_loss: 2.9788 - val_acc: 0.9358 - val_mDice: 0.4213

Epoch 00114: val_mDice improved from 0.41256 to 0.42130, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 115/300
 - 11s - loss: 1.9345 - acc: 0.9198 - mDice: 0.4997 - val_loss: 3.4522 - val_acc: 0.9239 - val_mDice: 0.3800

Epoch 00115: val_mDice did not improve from 0.42130
Epoch 116/300
 - 12s - loss: 1.9300 - acc: 0.9196 - mDice: 0.4996 - val_loss: 3.0510 - val_acc: 0.9326 - val_mDice: 0.4152

Epoch 00116: val_mDice did not improve from 0.42130
Epoch 117/300
 - 11s - loss: 1.9151 - acc: 0.9204 - mDice: 0.5048 - val_loss: 3.0922 - val_acc: 0.9325 - val_mDice: 0.4085

Epoch 00117: val_mDice did not improve from 0.42130
Epoch 118/300
 - 12s - loss: 1.9126 - acc: 0.9205 - mDice: 0.5039 - val_loss: 3.1972 - val_acc: 0.9349 - val_mDice: 0.4127

Epoch 00118: val_mDice did not improve from 0.42130
Epoch 119/300
 - 11s - loss: 1.9130 - acc: 0.9205 - mDice: 0.5048 - val_loss: 3.1519 - val_acc: 0.9321 - val_mDice: 0.4116

Epoch 00119: val_mDice did not improve from 0.42130
Epoch 120/300
 - 11s - loss: 1.9011 - acc: 0.9208 - mDice: 0.5073 - val_loss: 2.9480 - val_acc: 0.9324 - val_mDice: 0.4181

Epoch 00120: val_mDice did not improve from 0.42130
Epoch 121/300
 - 11s - loss: 1.9052 - acc: 0.9207 - mDice: 0.5066 - val_loss: 3.3190 - val_acc: 0.9299 - val_mDice: 0.3935

Epoch 00121: val_mDice did not improve from 0.42130
Epoch 122/300
 - 11s - loss: 1.8859 - acc: 0.9211 - mDice: 0.5107 - val_loss: 3.0952 - val_acc: 0.9346 - val_mDice: 0.4144

Epoch 00122: val_mDice did not improve from 0.42130
Epoch 123/300
 - 11s - loss: 1.8852 - acc: 0.9214 - mDice: 0.5114 - val_loss: 3.0479 - val_acc: 0.9339 - val_mDice: 0.4209

Epoch 00123: val_mDice did not improve from 0.42130
Epoch 124/300
 - 10s - loss: 1.8848 - acc: 0.9215 - mDice: 0.5115 - val_loss: 3.2565 - val_acc: 0.9340 - val_mDice: 0.4032

Epoch 00124: val_mDice did not improve from 0.42130
Epoch 125/300
 - 11s - loss: 1.8831 - acc: 0.9216 - mDice: 0.5116 - val_loss: 3.5909 - val_acc: 0.9364 - val_mDice: 0.3930

Epoch 00125: val_mDice did not improve from 0.42130
Epoch 126/300
 - 11s - loss: 1.8778 - acc: 0.9218 - mDice: 0.5121 - val_loss: 3.1427 - val_acc: 0.9343 - val_mDice: 0.4151

Epoch 00126: val_mDice did not improve from 0.42130
Epoch 127/300
 - 11s - loss: 1.8606 - acc: 0.9224 - mDice: 0.5171 - val_loss: 3.1324 - val_acc: 0.9347 - val_mDice: 0.4194

Epoch 00127: val_mDice did not improve from 0.42130
Epoch 128/300
 - 11s - loss: 1.8638 - acc: 0.9223 - mDice: 0.5164 - val_loss: 3.1229 - val_acc: 0.9345 - val_mDice: 0.4099

Epoch 00128: val_mDice did not improve from 0.42130
Epoch 129/300
 - 11s - loss: 1.8553 - acc: 0.9224 - mDice: 0.5179 - val_loss: 3.2083 - val_acc: 0.9327 - val_mDice: 0.4122

Epoch 00129: val_mDice did not improve from 0.42130
Epoch 130/300
 - 10s - loss: 1.8538 - acc: 0.9226 - mDice: 0.5184 - val_loss: 3.0488 - val_acc: 0.9373 - val_mDice: 0.4284

Epoch 00130: val_mDice improved from 0.42130 to 0.42836, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 131/300
 - 11s - loss: 1.8527 - acc: 0.9227 - mDice: 0.5189 - val_loss: 3.2037 - val_acc: 0.9368 - val_mDice: 0.4175

Epoch 00131: val_mDice did not improve from 0.42836
Epoch 132/300
 - 10s - loss: 1.8483 - acc: 0.9228 - mDice: 0.5198 - val_loss: 3.4423 - val_acc: 0.9382 - val_mDice: 0.4036

Epoch 00132: val_mDice did not improve from 0.42836
Epoch 133/300
 - 10s - loss: 1.8549 - acc: 0.9226 - mDice: 0.5186 - val_loss: 3.4658 - val_acc: 0.9381 - val_mDice: 0.4132

Epoch 00133: val_mDice did not improve from 0.42836
Epoch 134/300
 - 10s - loss: 1.8425 - acc: 0.9230 - mDice: 0.5210 - val_loss: 3.1019 - val_acc: 0.9338 - val_mDice: 0.4087

Epoch 00134: val_mDice did not improve from 0.42836
Epoch 135/300
 - 10s - loss: 1.8414 - acc: 0.9230 - mDice: 0.5213 - val_loss: 3.2698 - val_acc: 0.9369 - val_mDice: 0.4134

Epoch 00135: val_mDice did not improve from 0.42836
Epoch 136/300
 - 11s - loss: 1.8237 - acc: 0.9236 - mDice: 0.5257 - val_loss: 3.2754 - val_acc: 0.9379 - val_mDice: 0.4237

Epoch 00136: val_mDice did not improve from 0.42836
Epoch 137/300
 - 10s - loss: 1.8209 - acc: 0.9236 - mDice: 0.5261 - val_loss: 3.2114 - val_acc: 0.9382 - val_mDice: 0.4254

Epoch 00137: val_mDice did not improve from 0.42836
Epoch 138/300
 - 10s - loss: 1.8252 - acc: 0.9234 - mDice: 0.5257 - val_loss: 3.0894 - val_acc: 0.9354 - val_mDice: 0.4194

Epoch 00138: val_mDice did not improve from 0.42836
Epoch 139/300
 - 11s - loss: 1.8200 - acc: 0.9239 - mDice: 0.5267 - val_loss: 3.2953 - val_acc: 0.9365 - val_mDice: 0.4079

Epoch 00139: val_mDice did not improve from 0.42836
Epoch 140/300
 - 10s - loss: 1.8145 - acc: 0.9240 - mDice: 0.5279 - val_loss: 3.0537 - val_acc: 0.9334 - val_mDice: 0.4224

Epoch 00140: val_mDice did not improve from 0.42836
Epoch 141/300
 - 10s - loss: 1.8093 - acc: 0.9243 - mDice: 0.5298 - val_loss: 3.1123 - val_acc: 0.9360 - val_mDice: 0.4257

Epoch 00141: val_mDice did not improve from 0.42836
Epoch 142/300
 - 11s - loss: 1.8030 - acc: 0.9243 - mDice: 0.5306 - val_loss: 3.1467 - val_acc: 0.9354 - val_mDice: 0.4192

Epoch 00142: val_mDice did not improve from 0.42836
Epoch 143/300
 - 11s - loss: 1.8055 - acc: 0.9243 - mDice: 0.5304 - val_loss: 3.1749 - val_acc: 0.9343 - val_mDice: 0.4106

Epoch 00143: val_mDice did not improve from 0.42836
Epoch 144/300
 - 11s - loss: 1.8016 - acc: 0.9239 - mDice: 0.5314 - val_loss: 3.0362 - val_acc: 0.9332 - val_mDice: 0.4246

Epoch 00144: val_mDice did not improve from 0.42836
Epoch 145/300
 - 11s - loss: 1.7992 - acc: 0.9243 - mDice: 0.5318 - val_loss: 3.0658 - val_acc: 0.9356 - val_mDice: 0.4313

Epoch 00145: val_mDice improved from 0.42836 to 0.43127, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 146/300
 - 11s - loss: 1.7988 - acc: 0.9245 - mDice: 0.5315 - val_loss: 3.0748 - val_acc: 0.9373 - val_mDice: 0.4305

Epoch 00146: val_mDice did not improve from 0.43127
Epoch 147/300
 - 11s - loss: 1.7880 - acc: 0.9246 - mDice: 0.5346 - val_loss: 3.0563 - val_acc: 0.9363 - val_mDice: 0.4285

Epoch 00147: val_mDice did not improve from 0.43127
Epoch 148/300
 - 11s - loss: 1.7845 - acc: 0.9249 - mDice: 0.5355 - val_loss: 3.2825 - val_acc: 0.9370 - val_mDice: 0.4206

Epoch 00148: val_mDice did not improve from 0.43127
Epoch 149/300
 - 11s - loss: 1.7808 - acc: 0.9248 - mDice: 0.5355 - val_loss: 3.2297 - val_acc: 0.9336 - val_mDice: 0.4210

Epoch 00149: val_mDice did not improve from 0.43127
Epoch 150/300
 - 11s - loss: 1.7782 - acc: 0.9250 - mDice: 0.5365 - val_loss: 3.2589 - val_acc: 0.9366 - val_mDice: 0.4170

Epoch 00150: val_mDice did not improve from 0.43127
Epoch 151/300
 - 11s - loss: 1.7705 - acc: 0.9253 - mDice: 0.5382 - val_loss: 3.0800 - val_acc: 0.9344 - val_mDice: 0.4298

Epoch 00151: val_mDice did not improve from 0.43127
Epoch 152/300
 - 10s - loss: 1.7804 - acc: 0.9251 - mDice: 0.5367 - val_loss: 3.2092 - val_acc: 0.9402 - val_mDice: 0.4290

Epoch 00152: val_mDice did not improve from 0.43127
Epoch 153/300
 - 12s - loss: 1.7691 - acc: 0.9256 - mDice: 0.5392 - val_loss: 3.2354 - val_acc: 0.9362 - val_mDice: 0.4212

Epoch 00153: val_mDice did not improve from 0.43127
Epoch 154/300
 - 10s - loss: 1.7613 - acc: 0.9258 - mDice: 0.5409 - val_loss: 3.3373 - val_acc: 0.9365 - val_mDice: 0.4149

Epoch 00154: val_mDice did not improve from 0.43127
Epoch 155/300
 - 11s - loss: 1.7684 - acc: 0.9257 - mDice: 0.5394 - val_loss: 3.3567 - val_acc: 0.9354 - val_mDice: 0.4138

Epoch 00155: val_mDice did not improve from 0.43127
Epoch 156/300
 - 11s - loss: 1.7639 - acc: 0.9256 - mDice: 0.5403 - val_loss: 3.2294 - val_acc: 0.9343 - val_mDice: 0.4163

Epoch 00156: val_mDice did not improve from 0.43127
Epoch 157/300
 - 11s - loss: 1.7553 - acc: 0.9258 - mDice: 0.5424 - val_loss: 3.1603 - val_acc: 0.9358 - val_mDice: 0.4216

Epoch 00157: val_mDice did not improve from 0.43127
Epoch 158/300
 - 12s - loss: 1.7500 - acc: 0.9261 - mDice: 0.5432 - val_loss: 3.1172 - val_acc: 0.9347 - val_mDice: 0.4197

Epoch 00158: val_mDice did not improve from 0.43127
Epoch 159/300
 - 11s - loss: 1.7577 - acc: 0.9259 - mDice: 0.5426 - val_loss: 3.2677 - val_acc: 0.9360 - val_mDice: 0.4145

Epoch 00159: val_mDice did not improve from 0.43127
Epoch 160/300
 - 11s - loss: 1.7446 - acc: 0.9264 - mDice: 0.5451 - val_loss: 3.1035 - val_acc: 0.9366 - val_mDice: 0.4276

Epoch 00160: val_mDice did not improve from 0.43127
Epoch 161/300
 - 11s - loss: 1.7447 - acc: 0.9263 - mDice: 0.5446 - val_loss: 3.2012 - val_acc: 0.9380 - val_mDice: 0.4258

Epoch 00161: val_mDice did not improve from 0.43127
Epoch 162/300
 - 11s - loss: 1.7461 - acc: 0.9263 - mDice: 0.5445 - val_loss: 3.0083 - val_acc: 0.9365 - val_mDice: 0.4386

Epoch 00162: val_mDice improved from 0.43127 to 0.43862, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 163/300
 - 11s - loss: 1.7516 - acc: 0.9262 - mDice: 0.5433 - val_loss: 3.2407 - val_acc: 0.9344 - val_mDice: 0.4230

Epoch 00163: val_mDice did not improve from 0.43862
Epoch 164/300
 - 11s - loss: 1.7377 - acc: 0.9267 - mDice: 0.5469 - val_loss: 3.2128 - val_acc: 0.9397 - val_mDice: 0.4270

Epoch 00164: val_mDice did not improve from 0.43862
Epoch 165/300
 - 11s - loss: 1.7395 - acc: 0.9266 - mDice: 0.5460 - val_loss: 3.2687 - val_acc: 0.9389 - val_mDice: 0.4326

Epoch 00165: val_mDice did not improve from 0.43862
Epoch 166/300
 - 11s - loss: 1.7267 - acc: 0.9269 - mDice: 0.5492 - val_loss: 3.2030 - val_acc: 0.9366 - val_mDice: 0.4194

Epoch 00166: val_mDice did not improve from 0.43862
Epoch 167/300
 - 12s - loss: 1.7337 - acc: 0.9266 - mDice: 0.5471 - val_loss: 2.9488 - val_acc: 0.9358 - val_mDice: 0.4479

Epoch 00167: val_mDice improved from 0.43862 to 0.44789, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 168/300
 - 11s - loss: 1.7311 - acc: 0.9270 - mDice: 0.5482 - val_loss: 3.2992 - val_acc: 0.9345 - val_mDice: 0.4122

Epoch 00168: val_mDice did not improve from 0.44789
Epoch 169/300
 - 11s - loss: 1.7230 - acc: 0.9272 - mDice: 0.5503 - val_loss: 3.0798 - val_acc: 0.9371 - val_mDice: 0.4296

Epoch 00169: val_mDice did not improve from 0.44789
Epoch 170/300
 - 10s - loss: 1.7270 - acc: 0.9273 - mDice: 0.5492 - val_loss: 3.0931 - val_acc: 0.9360 - val_mDice: 0.4312

Epoch 00170: val_mDice did not improve from 0.44789
Epoch 171/300
 - 11s - loss: 1.7311 - acc: 0.9271 - mDice: 0.5481 - val_loss: 3.0729 - val_acc: 0.9360 - val_mDice: 0.4322

Epoch 00171: val_mDice did not improve from 0.44789
Epoch 172/300
 - 10s - loss: 1.7176 - acc: 0.9275 - mDice: 0.5514 - val_loss: 3.1196 - val_acc: 0.9334 - val_mDice: 0.4190

Epoch 00172: val_mDice did not improve from 0.44789
Epoch 173/300
 - 10s - loss: 1.7241 - acc: 0.9273 - mDice: 0.5499 - val_loss: 3.0575 - val_acc: 0.9382 - val_mDice: 0.4365

Epoch 00173: val_mDice did not improve from 0.44789
Epoch 174/300
 - 10s - loss: 1.7203 - acc: 0.9273 - mDice: 0.5502 - val_loss: 3.1194 - val_acc: 0.9376 - val_mDice: 0.4348

Epoch 00174: val_mDice did not improve from 0.44789
Epoch 175/300
 - 11s - loss: 1.7166 - acc: 0.9275 - mDice: 0.5520 - val_loss: 3.1881 - val_acc: 0.9348 - val_mDice: 0.4200

Epoch 00175: val_mDice did not improve from 0.44789
Epoch 176/300
 - 10s - loss: 1.7067 - acc: 0.9279 - mDice: 0.5539 - val_loss: 3.1259 - val_acc: 0.9372 - val_mDice: 0.4258

Epoch 00176: val_mDice did not improve from 0.44789
Epoch 177/300
 - 10s - loss: 1.7024 - acc: 0.9279 - mDice: 0.5555 - val_loss: 3.0189 - val_acc: 0.9360 - val_mDice: 0.4333

Epoch 00177: val_mDice did not improve from 0.44789
Epoch 178/300
 - 10s - loss: 1.7098 - acc: 0.9279 - mDice: 0.5537 - val_loss: 3.2098 - val_acc: 0.9380 - val_mDice: 0.4374

Epoch 00178: val_mDice did not improve from 0.44789
Epoch 179/300
 - 10s - loss: 1.7082 - acc: 0.9279 - mDice: 0.5542 - val_loss: 3.0614 - val_acc: 0.9379 - val_mDice: 0.4442

Epoch 00179: val_mDice did not improve from 0.44789
Epoch 180/300
 - 10s - loss: 1.7106 - acc: 0.9277 - mDice: 0.5529 - val_loss: 3.3661 - val_acc: 0.9365 - val_mDice: 0.4175

Epoch 00180: val_mDice did not improve from 0.44789
Epoch 181/300
 - 11s - loss: 1.6967 - acc: 0.9283 - mDice: 0.5567 - val_loss: 3.2316 - val_acc: 0.9371 - val_mDice: 0.4294

Epoch 00181: val_mDice did not improve from 0.44789
Epoch 182/300
 - 10s - loss: 1.7017 - acc: 0.9282 - mDice: 0.5558 - val_loss: 3.0391 - val_acc: 0.9365 - val_mDice: 0.4417

Epoch 00182: val_mDice did not improve from 0.44789
Epoch 183/300
 - 10s - loss: 1.6967 - acc: 0.9284 - mDice: 0.5565 - val_loss: 3.1485 - val_acc: 0.9384 - val_mDice: 0.4331

Epoch 00183: val_mDice did not improve from 0.44789
Epoch 184/300
 - 10s - loss: 1.6902 - acc: 0.9285 - mDice: 0.5584 - val_loss: 3.1580 - val_acc: 0.9369 - val_mDice: 0.4206

Epoch 00184: val_mDice did not improve from 0.44789
Epoch 185/300
 - 10s - loss: 1.6863 - acc: 0.9286 - mDice: 0.5590 - val_loss: 3.1450 - val_acc: 0.9366 - val_mDice: 0.4295

Epoch 00185: val_mDice did not improve from 0.44789
Epoch 186/300
 - 10s - loss: 1.6879 - acc: 0.9286 - mDice: 0.5588 - val_loss: 3.2799 - val_acc: 0.9397 - val_mDice: 0.4265

Epoch 00186: val_mDice did not improve from 0.44789
Epoch 187/300
 - 10s - loss: 1.6877 - acc: 0.9287 - mDice: 0.5591 - val_loss: 3.1820 - val_acc: 0.9375 - val_mDice: 0.4320

Epoch 00187: val_mDice did not improve from 0.44789
Epoch 188/300
 - 10s - loss: 1.6918 - acc: 0.9288 - mDice: 0.5589 - val_loss: 3.2269 - val_acc: 0.9391 - val_mDice: 0.4337

Epoch 00188: val_mDice did not improve from 0.44789
Epoch 189/300
 - 10s - loss: 1.6794 - acc: 0.9288 - mDice: 0.5608 - val_loss: 3.1654 - val_acc: 0.9367 - val_mDice: 0.4293

Epoch 00189: val_mDice did not improve from 0.44789
Epoch 190/300
 - 10s - loss: 1.6803 - acc: 0.9289 - mDice: 0.5608 - val_loss: 3.1642 - val_acc: 0.9380 - val_mDice: 0.4330

Epoch 00190: val_mDice did not improve from 0.44789
Epoch 191/300
 - 10s - loss: 1.6833 - acc: 0.9289 - mDice: 0.5604 - val_loss: 3.1987 - val_acc: 0.9378 - val_mDice: 0.4286

Epoch 00191: val_mDice did not improve from 0.44789
Epoch 192/300
 - 10s - loss: 1.6751 - acc: 0.9291 - mDice: 0.5623 - val_loss: 3.3626 - val_acc: 0.9361 - val_mDice: 0.4073

Epoch 00192: val_mDice did not improve from 0.44789
Epoch 193/300
 - 10s - loss: 1.6788 - acc: 0.9290 - mDice: 0.5616 - val_loss: 3.1650 - val_acc: 0.9346 - val_mDice: 0.4286

Epoch 00193: val_mDice did not improve from 0.44789
Epoch 194/300
 - 10s - loss: 1.6780 - acc: 0.9291 - mDice: 0.5616 - val_loss: 3.2870 - val_acc: 0.9384 - val_mDice: 0.4277

Epoch 00194: val_mDice did not improve from 0.44789
Epoch 195/300
 - 10s - loss: 1.6710 - acc: 0.9293 - mDice: 0.5627 - val_loss: 3.3424 - val_acc: 0.9374 - val_mDice: 0.4169

Epoch 00195: val_mDice did not improve from 0.44789
Epoch 196/300
 - 10s - loss: 1.6688 - acc: 0.9294 - mDice: 0.5639 - val_loss: 3.1967 - val_acc: 0.9373 - val_mDice: 0.4224

Epoch 00196: val_mDice did not improve from 0.44789
Epoch 197/300
 - 10s - loss: 1.6775 - acc: 0.9291 - mDice: 0.5620 - val_loss: 3.1864 - val_acc: 0.9372 - val_mDice: 0.4314

Epoch 00197: val_mDice did not improve from 0.44789
Restoring model weights from the end of the best epoch
Epoch 00197: early stopping
{'val_loss': [83.90374306270054, 21.9020246834982, 11.232880687429791, 8.208926127780051, 6.958920730366593, 6.110945875800791, 5.403260399365709, 5.208382577590999, 5.220965224362555, 4.82469747162291, 4.6328414759288234, 4.422379740203421, 4.270955146955592, 4.227941258322625, 3.9865036547361385, 3.9076428487009944, 4.119946987501213, 3.7978764132463505, 3.6200353199882165, 4.011997403665667, 3.8748340393372236, 3.4871645165341243, 3.5614508464488956, 3.6840028381418612, 3.52207996110831, 3.5106244167490375, 3.3168720978179147, 3.7276221337240365, 3.6556132230020704, 3.639501425038491, 3.427918061658385, 3.3450710556602905, 4.080649932513812, 3.1339529288400496, 3.335543316877669, 3.315921151567073, 3.240425449662975, 3.3637393851365363, 3.2572691330597516, 3.151193224558873, 3.0771318199556497, 3.329563104148422, 3.356203846411691, 3.155493039504758, 3.5668549902204956, 3.0173339069643546, 3.319663263635621, 3.362821772916331, 3.2696957555821253, 3.404001950308503, 2.9664969249140647, 3.3363349396247592, 3.474952520403479, 3.6057778209270466, 3.8295218495519032, 3.3562477666086385, 3.3757702836855534, 3.2652371024845968, 2.868769730796062, 2.948011986733902, 3.006780337586644, 2.995155449357948, 3.2906561767948523, 3.1707375326992144, 3.210114255591872, 3.0688493471326574, 3.221220825266625, 2.924406610695379, 3.0328431580925272, 3.002057933026836, 3.3230358014947603, 3.3502770432138016, 3.268622976173425, 3.1150078308280733, 3.0703852754086256, 3.2838145029686747, 3.153671051242522, 3.20471698032426, 3.1668748418756185, 3.3849892423355152, 3.05963302044464, 3.142919233911449, 3.37055463028983, 3.385730080616971, 3.317432289499612, 3.3320312556101097, 3.3950796701191437, 3.184585069306195, 3.3192981349836503, 3.274456965493127, 3.211771680773901, 3.3670035136331404, 3.2235442110603407, 3.599098520331262, 3.466753232048913, 3.3359620032965074, 3.406499697893326, 3.233141073424901, 3.7504546366898075, 3.232924736725787, 3.463728995239806, 3.2046166295185685, 2.972194825476479, 3.079019639663221, 3.1910849579476883, 3.21673146905821, 3.3591333725267933, 3.542068591385725, 3.055388078381795, 3.342433726548084, 3.2056137374380516, 3.3733610434989845, 3.242978907811145, 2.9787837055822215, 3.4521663429304246, 3.0510372145633613, 3.0921971178835346, 3.1972001595422626, 3.151900619224069, 2.9479816130229404, 3.318990431792502, 3.0952253817758035, 3.0479181075379964, 3.2565238272904287, 3.5909045663706602, 3.1427171176654243, 3.132367388788788, 3.122874163046834, 3.2083470844885422, 3.0488042737845156, 3.203697294772913, 3.4423447625622865, 3.4658479043060826, 3.101885522760096, 3.269785140219721, 3.275446875176082, 3.2113652801850723, 3.0893741250038147, 3.295291289265844, 3.0537069172908864, 3.1122978120997904, 3.146683719962658, 3.1749111273813817, 3.0361979638802863, 3.065790082239324, 3.0748026991218684, 3.056291177469705, 3.28254027167956, 3.2296608742769983, 3.258913971528056, 3.0799836383777714, 3.2091793978498098, 3.2354181145894385, 3.337269040645056, 3.3567445795273496, 3.2294041241873943, 3.1603113139225614, 3.1172259859740734, 3.267742357837657, 3.103531128815597, 3.2012260255758607, 3.008273383292059, 3.2407425841139186, 3.2127937512427924, 3.268675084137136, 3.202980582136661, 2.948759881247367, 3.299239353010697, 3.0798294133994553, 3.0930624599346803, 3.0729386093361035, 3.1196163277629587, 3.0574853746663955, 3.1194123933978735, 3.188105776107737, 3.1258828042607223, 3.018946714211433, 3.2097664293167845, 3.061373516296347, 3.3660646540423236, 3.231613508558699, 3.0391486140322828, 3.1485105006556426, 3.1580323129005374, 3.144975078469586, 3.2799061282449182, 3.1819826212622937, 3.2268655085492703, 3.1654307514074302, 3.1642179360746274, 3.1986948564826023, 3.362580517306924, 3.1650288028287745, 3.2869508558041636, 3.342445479085048, 3.1966592296070995, 3.1864147275420174], 'val_acc': [0.7988347042174566, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9049565082504636, 0.9048282731147039, 0.9047459051722572, 0.9048512067113604, 0.9050595192682176, 0.9047619132768541, 0.9047435777527946, 0.906025645278749, 0.9052174999600365, 0.9048191337358384, 0.9048763513565063, 0.9053228213673546, 0.9054281030382428, 0.9049107318832761, 0.9058012848808652, 0.9055929524557931, 0.9060416505450294, 0.9070833353769212, 0.9055082201957703, 0.9104968195869809, 0.9111149339448839, 0.9104326764742533, 0.9114697944550287, 0.9123603673208327, 0.9145352357909793, 0.9155860940615336, 0.917163440159389, 0.9132875346002125, 0.9151671017919268, 0.9165728234109425, 0.9148534876959664, 0.9185920414470491, 0.9176396528879801, 0.9178342705681211, 0.9168291858264378, 0.9175389352298918, 0.9216231930823553, 0.9202197591463724, 0.9198832710584005, 0.9200870252790905, 0.9208218994594756, 0.9229784693036761, 0.9237065003031776, 0.9271565846034459, 0.9288644563584101, 0.9263507099378676, 0.9290865461031595, 0.9297413258325486, 0.9267330481892541, 0.927944160643078, 0.9288003842035929, 0.9309180322147551, 0.9283104510534377, 0.9308196221079145, 0.9287591548193068, 0.930700523512704, 0.9289308871541705, 0.9299725180580503, 0.9316643590018863, 0.9309111464591253, 0.9304098231451852, 0.9258951573144822, 0.9283790985743204, 0.9288141103017897, 0.9310943115325201, 0.9319620047296796, 0.9312316945620945, 0.9312889093444461, 0.9311561300641015, 0.9280563223929632, 0.9285210371017456, 0.9317284850847154, 0.9340682427088419, 0.9315246996425447, 0.9344917762847174, 0.9334890218008132, 0.931856686160678, 0.9325709740320841, 0.9334912896156311, 0.9326396414211818, 0.9342376334326608, 0.9324816664059957, 0.9329075075331188, 0.9324725099972316, 0.9286675538335528, 0.9322390102204823, 0.931874984786624, 0.9318246529215858, 0.9334844237282163, 0.9347504405748277, 0.9352998847053164, 0.937255030586606, 0.9341048541523161, 0.9352816059475854, 0.9314194009417579, 0.9320237948780968, 0.9303182108061654, 0.9332989851633707, 0.9354968070983887, 0.9357921055385044, 0.9239285559881301, 0.9325526385080247, 0.9324519208499363, 0.9348855330830529, 0.9321337143580118, 0.9324175658680144, 0.9298901359240214, 0.934647452263605, 0.9338644970031011, 0.9340018317812965, 0.9364400165421622, 0.9342880078724453, 0.9346977841286432, 0.934532954579308, 0.9326533646810622, 0.9372595860844567, 0.936836078053429, 0.9382188831056867, 0.9380631900969005, 0.9337728931790307, 0.9368612397284735, 0.9378915116900489, 0.9381684916360038, 0.9354464525268191, 0.9364812061900184, 0.9334294937905812, 0.9359913127762931, 0.9353594098772321, 0.9342742675826663, 0.933177630106608, 0.9355883541561308, 0.9372756339254833, 0.9362729135013762, 0.9370261232058207, 0.9336286612919399, 0.9366277399517241, 0.9343772842770531, 0.940164810135251, 0.9361629968597776, 0.9364926729883466, 0.9354350055967059, 0.934290315423693, 0.9358264860652742, 0.9346543238276527, 0.9360347815922329, 0.9365933963230678, 0.9380128184954325, 0.9365132649739584, 0.9343978876159305, 0.9397436068171546, 0.9388988074802217, 0.9366139826320466, 0.9357943988981701, 0.9345169635046096, 0.9370925085885184, 0.9360096341087705, 0.9360256422133673, 0.9333928426106771, 0.9382188433692569, 0.9376053384372166, 0.9348054130872091, 0.9372000807807559, 0.9359523937815711, 0.9380013715653193, 0.9378571169716972, 0.9364835336094811, 0.937065030847277, 0.9364514634722755, 0.9384432207970392, 0.9368520776430765, 0.9366048801512945, 0.9396611821083796, 0.937506840342567, 0.9390705085936046, 0.9367101646604992, 0.9379556008747646, 0.9377769941375369, 0.9360782787913368, 0.9346428343227932, 0.9383859634399414, 0.9374106980505443, 0.9372756367637998, 0.9371863376526606], 'val_mDice': [0.011616945643687532, 0.012192553738021247, 0.012099751376635617, 0.009136053353791968, 0.011622055006834367, 0.018585211533631775, 0.03603122729275908, 0.04338392198440574, 0.04461067954876593, 0.057489886125993164, 0.07115371613985016, 0.07977642402762458, 0.08839403110600653, 0.09406284681920495, 0.10934625069300334, 0.11656259656662032, 0.10980870415057455, 0.13274662329682282, 0.14885847917979672, 0.1317782237726663, 0.14444782491773367, 0.17907468947981084, 0.17504184262915737, 0.17482380684287774, 0.19000517089097274, 0.1999867477764686, 0.2157773141350065, 0.1982766151250828, 0.21077465265989304, 0.21651811205915042, 0.22798067490969384, 0.2414022402039596, 0.2002139597953785, 0.2601116037084943, 0.25899109501569045, 0.25559413787864504, 0.26603737881495837, 0.26411530322262217, 0.28095210290380884, 0.291219159783352, 0.29968223570003394, 0.2872466106145155, 0.2957090507483199, 0.3025230894840899, 0.28556793085521176, 0.3187609662612279, 0.3025513694932063, 0.311867544871001, 0.31520112368854736, 0.3126164812239863, 0.3388381712138653, 0.31671683578973725, 0.32015033607326804, 0.3202958151343323, 0.3081803469076043, 0.32834533593129545, 0.3336737459259374, 0.338893507208143, 0.3658998188163553, 0.36428082681127955, 0.36295735082101255, 0.3699351761135317, 0.3494341954411495, 0.3578775949066594, 0.359596291468257, 0.3704367796225207, 0.3592315284269197, 0.38250653640854926, 0.3746099825948477, 0.37782098982660545, 0.36178927389638765, 0.36222245012010845, 0.36507071057955426, 0.37300879792088554, 0.3776184886339165, 0.3640463748680694, 0.37922170181714354, 0.37501267645330655, 0.3842564289058958, 0.3750069444733007, 0.38566149753474055, 0.3800195031577632, 0.37102746590971947, 0.37549989989825655, 0.368489214352199, 0.38071869916859125, 0.37876665574454127, 0.38319490956408636, 0.37813288204017137, 0.38834773731373606, 0.38916713283175514, 0.3891414769348644, 0.39118420102057005, 0.37736786706816583, 0.3903092802280471, 0.38905992287965047, 0.3814535928624017, 0.3957457184081986, 0.3660302513412067, 0.3901182430840674, 0.3855797531349318, 0.399713174395618, 0.4125631107460885, 0.40754711344128564, 0.406574735861449, 0.4112944837127413, 0.3965749392906825, 0.39784224260421025, 0.41002293959969566, 0.3937673086211795, 0.4026699530936423, 0.39434509653420674, 0.40342955149355386, 0.4213025543306555, 0.3799614787456535, 0.415234428076517, 0.4084787620674996, 0.4126614690536544, 0.41162162735348656, 0.4180605986288616, 0.3935479314199516, 0.41435492961179643, 0.4208951124123165, 0.4032213459057467, 0.3929899519398099, 0.415083419354189, 0.4193907349946953, 0.4098718230213438, 0.4121671755399023, 0.4283603099840028, 0.4174684163715158, 0.40359493770769667, 0.41317163549718405, 0.408670932586704, 0.41337823246916133, 0.4237271857758363, 0.42535550856874105, 0.41942758219582693, 0.4079059688817887, 0.4224207252264023, 0.42568295041010495, 0.41920358563462895, 0.4105704504819143, 0.4245972411618346, 0.43127021406378063, 0.43053835772332694, 0.42854731033245724, 0.42064104832354043, 0.4210426088954721, 0.4169657092009272, 0.4297516044406664, 0.428999971420992, 0.421200636419512, 0.414933870945658, 0.41380590697129566, 0.4163222937356858, 0.4215930572577885, 0.41967555082270075, 0.4144931845366955, 0.42763515810171765, 0.4258193166128227, 0.4386229690696512, 0.4229710102081299, 0.4269552504022916, 0.4326221453292029, 0.4193767172594865, 0.44788817174377893, 0.41220812570481075, 0.42960392346694354, 0.4311645996002924, 0.4321941151505425, 0.41900637107236044, 0.43646542727947235, 0.4347927667910144, 0.420046889710994, 0.42578715795562383, 0.43332442907350405, 0.4374094930078302, 0.44422057625793276, 0.41748535810481935, 0.4293616499219622, 0.4417473514165197, 0.4330554205392088, 0.42057509578409646, 0.42954725736663457, 0.42653003583351773, 0.4320304326358296, 0.4337479319600832, 0.42934288705388707, 0.4330006114074162, 0.42857906062688145, 0.4072797681604113, 0.42857393409524647, 0.42769123507397516, 0.41693528351329623, 0.42242699010031565, 0.4314242573011489], 'loss': [297.19245222866454, 65.33475370390558, 28.05224352009797, 17.561856096825526, 13.174086822074388, 10.875356937847501, 9.366104368026347, 8.347008265901076, 7.629424816155126, 7.085985794310076, 6.648429919603773, 6.277432797845444, 5.976015894680049, 5.698468518022722, 5.4540470607114795, 5.246765899364029, 5.068298700895984, 4.890846813991393, 4.743452814150712, 4.613707755971143, 4.469862228509018, 4.35036540357782, 4.234222506934424, 4.1181898920527304, 4.0085298491967585, 3.9282516869180393, 3.8420694082119295, 3.7519296522841006, 3.6862766518398864, 3.600026335494698, 3.5279699372077737, 3.476972140672006, 3.4142364536334675, 3.3527989303856565, 3.2907030697339574, 3.23743580636524, 3.184645552246332, 3.1358316486594715, 3.0913964183385234, 3.029853438237096, 2.9903036453647145, 2.9414323932154227, 2.9162252848838275, 2.865457934866251, 2.8269737435600675, 2.8053348707238075, 2.762823572732452, 2.7350822953284304, 2.7167867081735735, 2.6717671609600546, 2.6452751476823972, 2.606158949034003, 2.588518184941698, 2.5566407543674745, 2.53774843874156, 2.510033873020626, 2.4897407318600253, 2.465725209525906, 2.445529756528554, 2.4318867230061416, 2.4153160924095167, 2.4038019620409266, 2.38122368238095, 2.3651090882889494, 2.3493116577397384, 2.3293396549599925, 2.3086136954585, 2.3095212395719327, 2.2862808962697487, 2.267830317250699, 2.2721470300002617, 2.2570235533867815, 2.243983065689003, 2.2302984924060842, 2.2226733245531536, 2.2085741417748586, 2.1973308288042133, 2.195345665738834, 2.180796517833065, 2.1641944796680046, 2.162706269334455, 2.162861057828106, 2.1405055949240723, 2.1326275507057475, 2.1301878299790356, 2.1176111106126094, 2.1173254743301735, 2.100925951075733, 2.0960222031997997, 2.0856735207153374, 2.0711021357150763, 2.0768571318011766, 2.073112151607604, 2.0579991862565676, 2.046563399695284, 2.0535324873757865, 2.0404471523251084, 2.028470564881386, 2.0202658496795647, 2.0305019743253623, 2.01030293817761, 2.0017383387949694, 1.991604210968672, 1.9920716629750885, 1.986010333718145, 1.9898202839167978, 1.9778870558357202, 1.9695907931854575, 1.9651094582904987, 1.9606393164974292, 1.949550414788647, 1.9493688851715583, 1.949317426701743, 1.9403177937814122, 1.934479158462898, 1.9300068319705679, 1.9150948524475098, 1.9125590086534852, 1.9130379388114955, 1.9011426409338397, 1.9051568144567903, 1.885908296854252, 1.885159996770723, 1.8847652796217074, 1.883057594850892, 1.877754532557727, 1.8606394644024424, 1.8638486692573102, 1.8553287903881497, 1.853818020564135, 1.8527452714966743, 1.8482869895685379, 1.8548848062023988, 1.8425263464738115, 1.8414366801901545, 1.8237458761657206, 1.8208664965946733, 1.8251555514790683, 1.820010208132089, 1.8144861022378522, 1.8093240654305363, 1.802972654788876, 1.8054702852143243, 1.8015829048474814, 1.799193873165521, 1.798757438326884, 1.7880066290558108, 1.784543233936362, 1.7808478137121648, 1.7781593556897775, 1.7705161738354322, 1.7804040009285504, 1.7690551567335038, 1.7613343044767864, 1.7683698195171926, 1.7639083037699075, 1.7553006235473483, 1.749984679043764, 1.7577355284669713, 1.7445593871936096, 1.744692158464616, 1.7461447856640895, 1.7515886447690276, 1.7377204989017216, 1.7395284237005773, 1.7267311074863814, 1.7336697157164265, 1.7311162933210245, 1.7229918942921236, 1.726976046563114, 1.731133815546773, 1.7175913483783507, 1.724144218996216, 1.7203312941040634, 1.7165762733259435, 1.7067479936286436, 1.7024043455090807, 1.7097521578160144, 1.708179067020957, 1.7106095113482163, 1.6967013572161889, 1.7016776779471552, 1.6967051478969308, 1.6901773383030432, 1.686290688636056, 1.6878758523421455, 1.6877141016996364, 1.691780686953037, 1.6794219953098668, 1.6802840296510144, 1.6832936686404942, 1.6750815426289793, 1.6788150269286077, 1.6780349532602288, 1.671002293864909, 1.6688193009036203, 1.6775001319635574], 'acc': [0.09039230919566062, 0.7348835061849922, 0.8547204928359522, 0.8645807410541334, 0.8669688936164206, 0.8678031068719695, 0.8681490853018031, 0.8684367869707897, 0.868734846288399, 0.8690141128257265, 0.8694600075316701, 0.8697969969329702, 0.8698521872372992, 0.8698237361718585, 0.8698984898995958, 0.870098879589197, 0.8701272412304338, 0.8705050571244738, 0.8707431246716323, 0.871121288046847, 0.8714237966380276, 0.8718833189169658, 0.8720599832000681, 0.872656240912791, 0.8732884129807741, 0.8734243170176716, 0.8739321510157926, 0.8743440516656457, 0.8747675378979436, 0.8754071767950232, 0.8760233607645184, 0.8766161689345539, 0.8772839793746173, 0.8780639199294037, 0.8790132673186145, 0.880016888052093, 0.8810814018306508, 0.8818128667908274, 0.8828695497348265, 0.8842004270068202, 0.8851480359674442, 0.8862426732692825, 0.8870794566260934, 0.888354673237614, 0.8891068962881141, 0.8898525894067411, 0.890635473893193, 0.8913189969311933, 0.8917971966536444, 0.893149826001266, 0.8940624987341339, 0.8950231299824124, 0.895742939873102, 0.8965529802563977, 0.8969961194512195, 0.8980252025374241, 0.8990104501018042, 0.899479141142503, 0.9001124777131074, 0.9007814900710078, 0.9011788637118232, 0.9018720070567001, 0.9024735702148219, 0.9031899726664329, 0.9036509810855491, 0.9043088942013519, 0.904890625261263, 0.9048556813114476, 0.9056274868606808, 0.9061686435209112, 0.9063846745632599, 0.9068543002374236, 0.907255981994406, 0.9077062116828478, 0.9083274214258997, 0.9085186829864806, 0.90908956775919, 0.9093549804510995, 0.9096275972727064, 0.9103463172958562, 0.910445400738684, 0.9105791935852687, 0.9112291412245959, 0.9114757358556725, 0.9118769780987831, 0.9122635797427434, 0.9120797306465831, 0.9129194073279928, 0.9130621006522355, 0.9131541671448644, 0.913763371338062, 0.9139414957568484, 0.9140592786142862, 0.9143325194288408, 0.914943171703496, 0.914848257667048, 0.9152811055057467, 0.9154475482415206, 0.9159668999897714, 0.9158172319453416, 0.9162560604844499, 0.9167335651066576, 0.9172286048706773, 0.9171691938174674, 0.9173076677028213, 0.9173618899918669, 0.9179114127218873, 0.9182245753876432, 0.9182856354567273, 0.9185387663742883, 0.9188507978617858, 0.919202270663173, 0.9191663509875059, 0.9194062952593569, 0.91981493066174, 0.9196081671821421, 0.920448055358344, 0.9205324715195489, 0.9205033903133899, 0.9208480707133279, 0.9206706202280689, 0.9211220768850755, 0.921446111957439, 0.9215312449141966, 0.9216274349290051, 0.9217765189122666, 0.9223507634012691, 0.922276085515185, 0.922402529891617, 0.9226174969674995, 0.922696256892677, 0.9228307902525679, 0.9225962686129611, 0.9229715617263342, 0.923035053198384, 0.9236092960556417, 0.9236482704722263, 0.9233910387619993, 0.9238734781362887, 0.9239917016452233, 0.9242698803259638, 0.9242986372056347, 0.924317889862123, 0.9238928525645953, 0.9243025520828295, 0.9245114665810993, 0.9246037386160998, 0.9248843968018001, 0.9248188685791695, 0.9250447930158023, 0.925304060166618, 0.9250643938084616, 0.9255751704857118, 0.9257584590545986, 0.9256816475129849, 0.9256479567430693, 0.9258247761683173, 0.926057676520218, 0.925889379984615, 0.9263672086690142, 0.9263119201612353, 0.9262879384329307, 0.9262048678290575, 0.9267428001848195, 0.9265949845842619, 0.9269315365400989, 0.9265917440542762, 0.9270100189078077, 0.9271798199924265, 0.9272548515189468, 0.9271473807132103, 0.9275456108590967, 0.927267942259439, 0.9273317840854534, 0.9275267952114074, 0.9278527998685239, 0.9278703651286652, 0.9279201375925635, 0.9278560190019705, 0.9276614013103385, 0.9282736009041055, 0.9282302022187127, 0.9283789873513868, 0.9285215624201061, 0.9286463624575659, 0.9286360094513717, 0.9287418360100653, 0.9287507558618181, 0.9287945490846842, 0.9288700679742649, 0.9289162950362225, 0.9290608622698235, 0.929005597904971, 0.9290929805549878, 0.9293475928553869, 0.9293651097147639, 0.9290803761548427], 'mDice': [0.015954116190756207, 0.014282037899923711, 0.013875505895410771, 0.017805648195920523, 0.02372902927657003, 0.028288896613042777, 0.03380376399597589, 0.04075312302240476, 0.04731841173352638, 0.053814222240766305, 0.060885518654270256, 0.06901450110884882, 0.07618621066973943, 0.08469623592701035, 0.09274099439135493, 0.10034824779078706, 0.10769153966370726, 0.11582300224089544, 0.12337711179373649, 0.13245218031285055, 0.14171201047558118, 0.1501028803942956, 0.15931290139323695, 0.1685522633703637, 0.1775628614740517, 0.18473348278047402, 0.19317782538829337, 0.20217603634369954, 0.20894931145881399, 0.21790877822876345, 0.22567721168292104, 0.23119167895624582, 0.23850021671761615, 0.2454634449983898, 0.25269974694905634, 0.2596810258248419, 0.26589238732702725, 0.27276627732329867, 0.2785614012741091, 0.28795705202206356, 0.29521885318909624, 0.3018772750202346, 0.3065470038568994, 0.3136540144791097, 0.3191629757489546, 0.323974517531907, 0.329865835680308, 0.3349416310821215, 0.33837705386358347, 0.34690818990933914, 0.3517536316185483, 0.3592261185484262, 0.3629357992959248, 0.36826177149672074, 0.37160922336697877, 0.3772543764927954, 0.3812801276577997, 0.3856965232750389, 0.39021483520471406, 0.39289503294492656, 0.3964052360963794, 0.3990000849724368, 0.4033835455243795, 0.4065577188897781, 0.41015281743550086, 0.41380024975564683, 0.41851261385677174, 0.4184326180286529, 0.42238132171555476, 0.42719891796641574, 0.4262580018133609, 0.42933724480188257, 0.43165972170421607, 0.43476956026710845, 0.43634607563456385, 0.43924887192509, 0.4416872163613637, 0.4421703770931962, 0.44541907550076515, 0.4491856131614323, 0.4491440959235986, 0.4489257664539829, 0.4542045280472355, 0.45583139921581445, 0.45601175664844185, 0.45900663274985093, 0.4593434035892957, 0.4627273653154316, 0.46369713277101565, 0.4657612724963332, 0.4687332845111102, 0.46805708697243464, 0.46943377138494136, 0.47152164675538105, 0.474673706510289, 0.47345668307292066, 0.4766071173343581, 0.4781124660831806, 0.48020984137910905, 0.4791841497822536, 0.48209395231896335, 0.48483221321085734, 0.4862825485215335, 0.4872516200610018, 0.48826697249851037, 0.4875068083787576, 0.489591273104085, 0.4917472337214777, 0.49260793455377444, 0.4943117890453118, 0.4957184715723288, 0.49644443285311257, 0.49643197876882617, 0.4978604929420055, 0.4996710465159838, 0.4995885232532785, 0.5048429646633575, 0.5038597838874495, 0.5047537333845333, 0.5073076510891327, 0.5066316929745127, 0.5107062258413814, 0.5114106815491473, 0.5115334949259908, 0.5115825940424108, 0.5120671903188458, 0.5170567843503567, 0.5163590440590166, 0.5179375301373587, 0.5184366811854728, 0.5189431011389509, 0.5198488356705366, 0.5185721945459267, 0.5209568951179095, 0.5213019046016757, 0.5256655917292687, 0.5260900778763092, 0.52567434845252, 0.5267056690053902, 0.5278665571973943, 0.529791796416567, 0.5305917294057136, 0.5304323613747066, 0.5313746959045716, 0.5318303229447066, 0.5315493809043821, 0.5346145955427364, 0.535501652263092, 0.5354548749009929, 0.5364741912369831, 0.5382034902469671, 0.5367416748541822, 0.539167833564965, 0.540889572196322, 0.5393516941982495, 0.5402903362048576, 0.5424152918695737, 0.5432406109676027, 0.5425618212186741, 0.5451274962238616, 0.5446352355852967, 0.5445000967217337, 0.5432752751191733, 0.5468595871834344, 0.5459666975964372, 0.5491731838382966, 0.5471245210731974, 0.5481862387423665, 0.5502534410915371, 0.5491789086535644, 0.54810325293176, 0.5513788304945764, 0.5498727286209002, 0.5502444792281048, 0.5520251015603761, 0.5538562974052804, 0.5554551182396269, 0.5537080994593699, 0.5542384504651389, 0.5529182407238682, 0.5567379576842266, 0.555825276183604, 0.5565254288140763, 0.5583954631351473, 0.5590403777959606, 0.5587911406003696, 0.559077545652873, 0.5588866010874205, 0.5607960624503612, 0.5607795523429666, 0.5603948428907995, 0.5623345195339299, 0.5615651352409685, 0.561562894028185, 0.5626941892552381, 0.5639346009255007, 0.5619701592821548]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.44s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.18s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.94s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:39,  1.41s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:01,  1.49s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:14,  1.54s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:44,  1.65s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<07:25,  1.59s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<07:49,  1.68s/it]predicting train subjects:   2%|▏         | 7/285 [00:12<08:16,  1.78s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<08:26,  1.83s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<08:06,  1.76s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:24,  1.84s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:38,  1.89s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:42,  1.91s/it]predicting train subjects:   5%|▍         | 13/285 [00:23<08:47,  1.94s/it]predicting train subjects:   5%|▍         | 14/285 [00:25<08:50,  1.96s/it]predicting train subjects:   5%|▌         | 15/285 [00:27<09:05,  2.02s/it]predicting train subjects:   6%|▌         | 16/285 [00:29<09:01,  2.01s/it]predicting train subjects:   6%|▌         | 17/285 [00:31<09:02,  2.03s/it]predicting train subjects:   6%|▋         | 18/285 [00:33<09:03,  2.04s/it]predicting train subjects:   7%|▋         | 19/285 [00:35<08:54,  2.01s/it]predicting train subjects:   7%|▋         | 20/285 [00:37<08:53,  2.01s/it]predicting train subjects:   7%|▋         | 21/285 [00:39<08:48,  2.00s/it]predicting train subjects:   8%|▊         | 22/285 [00:41<08:47,  2.01s/it]predicting train subjects:   8%|▊         | 23/285 [00:43<08:51,  2.03s/it]predicting train subjects:   8%|▊         | 24/285 [00:45<08:43,  2.01s/it]predicting train subjects:   9%|▉         | 25/285 [00:47<08:40,  2.00s/it]predicting train subjects:   9%|▉         | 26/285 [00:49<08:41,  2.01s/it]predicting train subjects:   9%|▉         | 27/285 [00:51<08:38,  2.01s/it]predicting train subjects:  10%|▉         | 28/285 [00:53<08:32,  1.99s/it]predicting train subjects:  10%|█         | 29/285 [00:55<08:24,  1.97s/it]predicting train subjects:  11%|█         | 30/285 [00:57<08:16,  1.95s/it]predicting train subjects:  11%|█         | 31/285 [00:59<08:14,  1.95s/it]predicting train subjects:  11%|█         | 32/285 [01:01<08:03,  1.91s/it]predicting train subjects:  12%|█▏        | 33/285 [01:03<08:00,  1.91s/it]predicting train subjects:  12%|█▏        | 34/285 [01:05<07:57,  1.90s/it]predicting train subjects:  12%|█▏        | 35/285 [01:07<07:54,  1.90s/it]predicting train subjects:  13%|█▎        | 36/285 [01:08<07:48,  1.88s/it]predicting train subjects:  13%|█▎        | 37/285 [01:10<07:52,  1.91s/it]predicting train subjects:  13%|█▎        | 38/285 [01:12<07:49,  1.90s/it]predicting train subjects:  14%|█▎        | 39/285 [01:14<07:44,  1.89s/it]predicting train subjects:  14%|█▍        | 40/285 [01:16<07:40,  1.88s/it]predicting train subjects:  14%|█▍        | 41/285 [01:18<07:42,  1.89s/it]predicting train subjects:  15%|█▍        | 42/285 [01:20<07:38,  1.88s/it]predicting train subjects:  15%|█▌        | 43/285 [01:22<07:38,  1.90s/it]predicting train subjects:  15%|█▌        | 44/285 [01:24<07:39,  1.91s/it]predicting train subjects:  16%|█▌        | 45/285 [01:26<07:35,  1.90s/it]predicting train subjects:  16%|█▌        | 46/285 [01:27<07:14,  1.82s/it]predicting train subjects:  16%|█▋        | 47/285 [01:29<07:22,  1.86s/it]predicting train subjects:  17%|█▋        | 48/285 [01:31<07:01,  1.78s/it]predicting train subjects:  17%|█▋        | 49/285 [01:32<06:46,  1.72s/it]predicting train subjects:  18%|█▊        | 50/285 [01:34<06:36,  1.69s/it]predicting train subjects:  18%|█▊        | 51/285 [01:36<06:31,  1.67s/it]predicting train subjects:  18%|█▊        | 52/285 [01:37<06:26,  1.66s/it]predicting train subjects:  19%|█▊        | 53/285 [01:39<06:23,  1.65s/it]predicting train subjects:  19%|█▉        | 54/285 [01:40<06:20,  1.65s/it]predicting train subjects:  19%|█▉        | 55/285 [01:42<06:16,  1.64s/it]predicting train subjects:  20%|█▉        | 56/285 [01:44<06:19,  1.66s/it]predicting train subjects:  20%|██        | 57/285 [01:45<06:19,  1.66s/it]predicting train subjects:  20%|██        | 58/285 [01:47<06:15,  1.65s/it]predicting train subjects:  21%|██        | 59/285 [01:49<06:08,  1.63s/it]predicting train subjects:  21%|██        | 60/285 [01:50<06:09,  1.64s/it]predicting train subjects:  21%|██▏       | 61/285 [01:52<06:08,  1.64s/it]predicting train subjects:  22%|██▏       | 62/285 [01:54<06:07,  1.65s/it]predicting train subjects:  22%|██▏       | 63/285 [01:55<06:03,  1.64s/it]predicting train subjects:  22%|██▏       | 64/285 [01:57<06:05,  1.65s/it]predicting train subjects:  23%|██▎       | 65/285 [01:59<06:19,  1.73s/it]predicting train subjects:  23%|██▎       | 66/285 [02:01<06:24,  1.75s/it]predicting train subjects:  24%|██▎       | 67/285 [02:02<06:16,  1.73s/it]predicting train subjects:  24%|██▍       | 68/285 [02:04<06:12,  1.71s/it]predicting train subjects:  24%|██▍       | 69/285 [02:06<06:09,  1.71s/it]predicting train subjects:  25%|██▍       | 70/285 [02:07<06:07,  1.71s/it]predicting train subjects:  25%|██▍       | 71/285 [02:09<06:03,  1.70s/it]predicting train subjects:  25%|██▌       | 72/285 [02:11<06:00,  1.69s/it]predicting train subjects:  26%|██▌       | 73/285 [02:12<05:58,  1.69s/it]predicting train subjects:  26%|██▌       | 74/285 [02:14<05:57,  1.70s/it]predicting train subjects:  26%|██▋       | 75/285 [02:16<05:55,  1.69s/it]predicting train subjects:  27%|██▋       | 76/285 [02:18<05:54,  1.70s/it]predicting train subjects:  27%|██▋       | 77/285 [02:19<05:53,  1.70s/it]predicting train subjects:  27%|██▋       | 78/285 [02:21<05:53,  1.71s/it]predicting train subjects:  28%|██▊       | 79/285 [02:23<05:51,  1.70s/it]predicting train subjects:  28%|██▊       | 80/285 [02:24<05:50,  1.71s/it]predicting train subjects:  28%|██▊       | 81/285 [02:26<05:48,  1.71s/it]predicting train subjects:  29%|██▉       | 82/285 [02:28<05:45,  1.70s/it]predicting train subjects:  29%|██▉       | 83/285 [02:30<05:48,  1.72s/it]predicting train subjects:  29%|██▉       | 84/285 [02:31<05:46,  1.72s/it]predicting train subjects:  30%|██▉       | 85/285 [02:33<05:57,  1.79s/it]predicting train subjects:  30%|███       | 86/285 [02:35<06:04,  1.83s/it]predicting train subjects:  31%|███       | 87/285 [02:37<06:09,  1.87s/it]predicting train subjects:  31%|███       | 88/285 [02:39<06:10,  1.88s/it]predicting train subjects:  31%|███       | 89/285 [02:41<06:10,  1.89s/it]predicting train subjects:  32%|███▏      | 90/285 [02:43<06:07,  1.88s/it]predicting train subjects:  32%|███▏      | 91/285 [02:45<06:04,  1.88s/it]predicting train subjects:  32%|███▏      | 92/285 [02:47<06:06,  1.90s/it]predicting train subjects:  33%|███▎      | 93/285 [02:49<06:07,  1.91s/it]predicting train subjects:  33%|███▎      | 94/285 [02:50<06:02,  1.90s/it]predicting train subjects:  33%|███▎      | 95/285 [02:52<05:58,  1.89s/it]predicting train subjects:  34%|███▎      | 96/285 [02:54<05:57,  1.89s/it]predicting train subjects:  34%|███▍      | 97/285 [02:56<05:54,  1.89s/it]predicting train subjects:  34%|███▍      | 98/285 [02:58<05:52,  1.89s/it]predicting train subjects:  35%|███▍      | 99/285 [03:00<05:50,  1.88s/it]predicting train subjects:  35%|███▌      | 100/285 [03:02<05:49,  1.89s/it]predicting train subjects:  35%|███▌      | 101/285 [03:04<05:49,  1.90s/it]predicting train subjects:  36%|███▌      | 102/285 [03:06<05:54,  1.94s/it]predicting train subjects:  36%|███▌      | 103/285 [03:08<05:53,  1.94s/it]predicting train subjects:  36%|███▋      | 104/285 [03:09<05:47,  1.92s/it]predicting train subjects:  37%|███▋      | 105/285 [03:11<05:40,  1.89s/it]predicting train subjects:  37%|███▋      | 106/285 [03:13<05:37,  1.89s/it]predicting train subjects:  38%|███▊      | 107/285 [03:15<05:35,  1.89s/it]predicting train subjects:  38%|███▊      | 108/285 [03:17<05:33,  1.88s/it]predicting train subjects:  38%|███▊      | 109/285 [03:19<05:56,  2.02s/it]predicting train subjects:  39%|███▊      | 110/285 [03:21<05:43,  1.96s/it]predicting train subjects:  39%|███▉      | 111/285 [03:23<05:33,  1.92s/it]predicting train subjects:  39%|███▉      | 112/285 [03:25<05:32,  1.92s/it]predicting train subjects:  40%|███▉      | 113/285 [03:27<05:25,  1.89s/it]predicting train subjects:  40%|████      | 114/285 [03:28<05:20,  1.87s/it]predicting train subjects:  40%|████      | 115/285 [03:30<05:15,  1.85s/it]predicting train subjects:  41%|████      | 116/285 [03:32<05:10,  1.84s/it]predicting train subjects:  41%|████      | 117/285 [03:34<05:06,  1.82s/it]predicting train subjects:  41%|████▏     | 118/285 [03:36<05:03,  1.82s/it]predicting train subjects:  42%|████▏     | 119/285 [03:38<05:04,  1.84s/it]predicting train subjects:  42%|████▏     | 120/285 [03:39<05:03,  1.84s/it]predicting train subjects:  42%|████▏     | 121/285 [03:41<04:51,  1.78s/it]predicting train subjects:  43%|████▎     | 122/285 [03:43<04:36,  1.70s/it]predicting train subjects:  43%|████▎     | 123/285 [03:44<04:27,  1.65s/it]predicting train subjects:  44%|████▎     | 124/285 [03:46<04:27,  1.66s/it]predicting train subjects:  44%|████▍     | 125/285 [03:48<04:34,  1.72s/it]predicting train subjects:  44%|████▍     | 126/285 [03:49<04:29,  1.70s/it]predicting train subjects:  45%|████▍     | 127/285 [03:51<04:25,  1.68s/it]predicting train subjects:  45%|████▍     | 128/285 [03:53<04:22,  1.67s/it]predicting train subjects:  45%|████▌     | 129/285 [03:54<04:19,  1.66s/it]predicting train subjects:  46%|████▌     | 130/285 [03:56<04:16,  1.65s/it]predicting train subjects:  46%|████▌     | 131/285 [03:58<04:15,  1.66s/it]predicting train subjects:  46%|████▋     | 132/285 [03:59<04:13,  1.66s/it]predicting train subjects:  47%|████▋     | 133/285 [04:01<04:12,  1.66s/it]predicting train subjects:  47%|████▋     | 134/285 [04:02<04:10,  1.66s/it]predicting train subjects:  47%|████▋     | 135/285 [04:04<04:10,  1.67s/it]predicting train subjects:  48%|████▊     | 136/285 [04:06<04:11,  1.69s/it]predicting train subjects:  48%|████▊     | 137/285 [04:08<04:12,  1.71s/it]predicting train subjects:  48%|████▊     | 138/285 [04:09<04:10,  1.70s/it]predicting train subjects:  49%|████▉     | 139/285 [04:11<04:09,  1.71s/it]predicting train subjects:  49%|████▉     | 140/285 [04:13<04:05,  1.69s/it]predicting train subjects:  49%|████▉     | 141/285 [04:14<04:00,  1.67s/it]predicting train subjects:  50%|████▉     | 142/285 [04:16<03:51,  1.62s/it]predicting train subjects:  50%|█████     | 143/285 [04:17<03:49,  1.61s/it]predicting train subjects:  51%|█████     | 144/285 [04:19<03:45,  1.60s/it]predicting train subjects:  51%|█████     | 145/285 [04:21<03:40,  1.57s/it]predicting train subjects:  51%|█████     | 146/285 [04:22<03:36,  1.56s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:24<03:33,  1.55s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:25<03:31,  1.55s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:27<03:30,  1.55s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:28<03:27,  1.54s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:30<03:23,  1.52s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:31<03:20,  1.51s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:33<03:21,  1.53s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:34<03:21,  1.54s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:36<03:20,  1.54s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:37<03:18,  1.54s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:39<03:14,  1.52s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:40<03:13,  1.52s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:42<03:12,  1.53s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:43<03:11,  1.54s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:46<04:01,  1.95s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:48<03:45,  1.83s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:49<03:30,  1.73s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:51<03:19,  1.65s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:53<03:20,  1.67s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:54<03:13,  1.63s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:56<03:12,  1.63s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:57<03:06,  1.59s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:59<03:00,  1.56s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:00<02:56,  1.54s/it]predicting train subjects:  60%|██████    | 171/285 [05:02<02:54,  1.53s/it]predicting train subjects:  60%|██████    | 172/285 [05:03<02:51,  1.52s/it]predicting train subjects:  61%|██████    | 173/285 [05:05<02:47,  1.49s/it]predicting train subjects:  61%|██████    | 174/285 [05:06<02:45,  1.49s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:08<02:42,  1.48s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:09<02:42,  1.49s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:11<02:40,  1.48s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:12<02:38,  1.48s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:14<02:36,  1.48s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:15<02:33,  1.46s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:16<02:29,  1.44s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:18<02:29,  1.45s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:19<02:27,  1.45s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:21<02:24,  1.44s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:22<02:21,  1.42s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:24<02:20,  1.42s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:25<02:21,  1.44s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:26<02:20,  1.45s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:28<02:18,  1.44s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:29<02:17,  1.44s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:31<02:14,  1.43s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:32<02:13,  1.43s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:34<02:13,  1.45s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:35<02:11,  1.44s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:36<02:08,  1.43s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:38<02:17,  1.54s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:40<02:20,  1.59s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:42<02:23,  1.65s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:43<02:23,  1.67s/it]predicting train subjects:  70%|███████   | 200/285 [05:45<02:23,  1.69s/it]predicting train subjects:  71%|███████   | 201/285 [05:47<02:24,  1.72s/it]predicting train subjects:  71%|███████   | 202/285 [05:49<02:23,  1.73s/it]predicting train subjects:  71%|███████   | 203/285 [05:50<02:20,  1.72s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:52<02:20,  1.73s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:54<02:18,  1.73s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:56<02:20,  1.78s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:58<02:18,  1.77s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:59<02:13,  1.74s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:01<02:11,  1.73s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:03<02:12,  1.76s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:05<02:10,  1.77s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:06<02:10,  1.79s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:08<02:07,  1.77s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:10<02:00,  1.70s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:11<01:56,  1.67s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:13<01:51,  1.62s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:14<01:48,  1.60s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:16<01:47,  1.60s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:18<01:45,  1.60s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:19<01:42,  1.58s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:21<01:40,  1.57s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:22<01:37,  1.55s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:24<01:36,  1.56s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:25<01:33,  1.54s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:27<01:31,  1.53s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:28<01:29,  1.52s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:30<01:26,  1.49s/it]predicting train subjects:  80%|████████  | 228/285 [06:31<01:26,  1.51s/it]predicting train subjects:  80%|████████  | 229/285 [06:33<01:26,  1.54s/it]predicting train subjects:  81%|████████  | 230/285 [06:34<01:24,  1.54s/it]predicting train subjects:  81%|████████  | 231/285 [06:36<01:22,  1.52s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:38<01:26,  1.64s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:40<01:28,  1.71s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:41<01:29,  1.76s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:43<01:30,  1.82s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:45<01:30,  1.84s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:47<01:28,  1.84s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:49<01:26,  1.84s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:51<01:26,  1.87s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:53<01:24,  1.89s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:55<01:22,  1.89s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:57<01:22,  1.91s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:59<01:20,  1.92s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:01<01:18,  1.92s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:03<01:17,  1.94s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:04<01:14,  1.91s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:06<01:14,  1.96s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:08<01:11,  1.94s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:10<01:09,  1.92s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:12<01:01,  1.77s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:13<00:57,  1.70s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:15<00:53,  1.62s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:16<00:50,  1.57s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:18<00:48,  1.55s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:19<00:45,  1.51s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:20<00:43,  1.49s/it]predicting train subjects:  90%|█████████ | 257/285 [07:22<00:41,  1.48s/it]predicting train subjects:  91%|█████████ | 258/285 [07:23<00:39,  1.48s/it]predicting train subjects:  91%|█████████ | 259/285 [07:25<00:38,  1.49s/it]predicting train subjects:  91%|█████████ | 260/285 [07:27<00:39,  1.59s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:28<00:37,  1.55s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:30<00:35,  1.53s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:31<00:33,  1.52s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:33<00:31,  1.49s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:34<00:29,  1.48s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:36<00:28,  1.51s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:37<00:27,  1.54s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:39<00:28,  1.69s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:41<00:28,  1.76s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:43<00:26,  1.80s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:45<00:25,  1.82s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:47<00:23,  1.84s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:49<00:22,  1.91s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:51<00:20,  1.91s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:53<00:19,  1.94s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:55<00:17,  1.95s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:57<00:15,  1.95s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:59<00:13,  1.95s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:01<00:11,  1.95s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:03<00:09,  1.93s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:04<00:07,  1.94s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:06<00:05,  1.94s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:08<00:03,  1.93s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:10<00:01,  1.92s/it]predicting train subjects: 100%|██████████| 285/285 [08:12<00:00,  1.92s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<07:18,  1.55s/it]Loading train:   1%|          | 2/285 [00:03<07:47,  1.65s/it]Loading train:   1%|          | 3/285 [00:04<07:30,  1.60s/it]Loading train:   1%|▏         | 4/285 [00:06<07:40,  1.64s/it]Loading train:   2%|▏         | 5/285 [00:08<07:37,  1.63s/it]Loading train:   2%|▏         | 6/285 [00:09<07:38,  1.64s/it]Loading train:   2%|▏         | 7/285 [00:11<07:57,  1.72s/it]Loading train:   3%|▎         | 8/285 [00:13<08:00,  1.73s/it]Loading train:   3%|▎         | 9/285 [00:15<07:41,  1.67s/it]Loading train:   4%|▎         | 10/285 [00:16<07:23,  1.61s/it]Loading train:   4%|▍         | 11/285 [00:17<06:55,  1.52s/it]Loading train:   4%|▍         | 12/285 [00:19<06:31,  1.44s/it]Loading train:   5%|▍         | 13/285 [00:20<06:42,  1.48s/it]Loading train:   5%|▍         | 14/285 [00:21<06:23,  1.42s/it]Loading train:   5%|▌         | 15/285 [00:23<06:12,  1.38s/it]Loading train:   6%|▌         | 16/285 [00:24<05:55,  1.32s/it]Loading train:   6%|▌         | 17/285 [00:25<05:46,  1.29s/it]Loading train:   6%|▋         | 18/285 [00:26<05:37,  1.26s/it]Loading train:   7%|▋         | 19/285 [00:28<05:28,  1.23s/it]Loading train:   7%|▋         | 20/285 [00:29<05:44,  1.30s/it]Loading train:   7%|▋         | 21/285 [00:30<05:38,  1.28s/it]Loading train:   8%|▊         | 22/285 [00:31<05:32,  1.26s/it]Loading train:   8%|▊         | 23/285 [00:33<05:30,  1.26s/it]Loading train:   8%|▊         | 24/285 [00:34<05:32,  1.27s/it]Loading train:   9%|▉         | 25/285 [00:35<05:36,  1.29s/it]Loading train:   9%|▉         | 26/285 [00:37<05:36,  1.30s/it]Loading train:   9%|▉         | 27/285 [00:38<05:40,  1.32s/it]Loading train:  10%|▉         | 28/285 [00:40<05:50,  1.36s/it]Loading train:  10%|█         | 29/285 [00:41<05:22,  1.26s/it]Loading train:  11%|█         | 30/285 [00:42<05:19,  1.25s/it]Loading train:  11%|█         | 31/285 [00:43<04:58,  1.18s/it]Loading train:  11%|█         | 32/285 [00:44<05:01,  1.19s/it]Loading train:  12%|█▏        | 33/285 [00:45<04:46,  1.14s/it]Loading train:  12%|█▏        | 34/285 [00:46<04:48,  1.15s/it]Loading train:  12%|█▏        | 35/285 [00:47<04:38,  1.11s/it]Loading train:  13%|█▎        | 36/285 [00:49<04:58,  1.20s/it]Loading train:  13%|█▎        | 37/285 [00:50<04:42,  1.14s/it]Loading train:  13%|█▎        | 38/285 [00:51<04:37,  1.12s/it]Loading train:  14%|█▎        | 39/285 [00:52<04:30,  1.10s/it]Loading train:  14%|█▍        | 40/285 [00:53<04:21,  1.07s/it]Loading train:  14%|█▍        | 41/285 [00:54<04:25,  1.09s/it]Loading train:  15%|█▍        | 42/285 [00:55<04:23,  1.09s/it]Loading train:  15%|█▌        | 43/285 [00:56<04:28,  1.11s/it]Loading train:  15%|█▌        | 44/285 [00:57<04:21,  1.08s/it]Loading train:  16%|█▌        | 45/285 [00:58<04:32,  1.13s/it]Loading train:  16%|█▌        | 46/285 [01:00<04:32,  1.14s/it]Loading train:  16%|█▋        | 47/285 [01:01<04:20,  1.10s/it]Loading train:  17%|█▋        | 48/285 [01:02<04:11,  1.06s/it]Loading train:  17%|█▋        | 49/285 [01:02<03:53,  1.01it/s]Loading train:  18%|█▊        | 50/285 [01:03<03:54,  1.00it/s]Loading train:  18%|█▊        | 51/285 [01:04<03:53,  1.00it/s]Loading train:  18%|█▊        | 52/285 [01:05<03:50,  1.01it/s]Loading train:  19%|█▊        | 53/285 [01:06<03:47,  1.02it/s]Loading train:  19%|█▉        | 54/285 [01:07<03:45,  1.02it/s]Loading train:  19%|█▉        | 55/285 [01:08<04:00,  1.05s/it]Loading train:  20%|█▉        | 56/285 [01:09<03:52,  1.02s/it]Loading train:  20%|██        | 57/285 [01:10<03:43,  1.02it/s]Loading train:  20%|██        | 58/285 [01:11<03:42,  1.02it/s]Loading train:  21%|██        | 59/285 [01:12<03:41,  1.02it/s]Loading train:  21%|██        | 60/285 [01:13<03:39,  1.02it/s]Loading train:  21%|██▏       | 61/285 [01:14<03:48,  1.02s/it]Loading train:  22%|██▏       | 62/285 [01:15<03:45,  1.01s/it]Loading train:  22%|██▏       | 63/285 [01:16<03:40,  1.01it/s]Loading train:  22%|██▏       | 64/285 [01:18<04:22,  1.19s/it]Loading train:  23%|██▎       | 65/285 [01:20<04:59,  1.36s/it]Loading train:  23%|██▎       | 66/285 [01:21<05:10,  1.42s/it]Loading train:  24%|██▎       | 67/285 [01:22<04:48,  1.32s/it]Loading train:  24%|██▍       | 68/285 [01:23<04:33,  1.26s/it]Loading train:  24%|██▍       | 69/285 [01:25<04:17,  1.19s/it]Loading train:  25%|██▍       | 70/285 [01:26<04:03,  1.13s/it]Loading train:  25%|██▍       | 71/285 [01:26<03:50,  1.08s/it]Loading train:  25%|██▌       | 72/285 [01:27<03:46,  1.06s/it]Loading train:  26%|██▌       | 73/285 [01:29<03:45,  1.06s/it]Loading train:  26%|██▌       | 74/285 [01:30<03:44,  1.07s/it]Loading train:  26%|██▋       | 75/285 [01:31<03:41,  1.05s/it]Loading train:  27%|██▋       | 76/285 [01:32<03:39,  1.05s/it]Loading train:  27%|██▋       | 77/285 [01:33<03:31,  1.02s/it]Loading train:  27%|██▋       | 78/285 [01:34<03:29,  1.01s/it]Loading train:  28%|██▊       | 79/285 [01:35<03:33,  1.04s/it]Loading train:  28%|██▊       | 80/285 [01:36<03:38,  1.06s/it]Loading train:  28%|██▊       | 81/285 [01:37<03:40,  1.08s/it]Loading train:  29%|██▉       | 82/285 [01:38<03:38,  1.08s/it]Loading train:  29%|██▉       | 83/285 [01:39<03:34,  1.06s/it]Loading train:  29%|██▉       | 84/285 [01:40<03:32,  1.06s/it]Loading train:  30%|██▉       | 85/285 [01:41<03:37,  1.09s/it]Loading train:  30%|███       | 86/285 [01:43<03:48,  1.15s/it]Loading train:  31%|███       | 87/285 [01:44<03:43,  1.13s/it]Loading train:  31%|███       | 88/285 [01:45<03:49,  1.16s/it]Loading train:  31%|███       | 89/285 [01:46<03:43,  1.14s/it]Loading train:  32%|███▏      | 90/285 [01:47<03:47,  1.17s/it]Loading train:  32%|███▏      | 91/285 [01:48<03:41,  1.14s/it]Loading train:  32%|███▏      | 92/285 [01:49<03:41,  1.15s/it]Loading train:  33%|███▎      | 93/285 [01:51<03:35,  1.12s/it]Loading train:  33%|███▎      | 94/285 [01:52<03:34,  1.12s/it]Loading train:  33%|███▎      | 95/285 [01:53<03:32,  1.12s/it]Loading train:  34%|███▎      | 96/285 [01:54<03:29,  1.11s/it]Loading train:  34%|███▍      | 97/285 [01:55<03:31,  1.13s/it]Loading train:  34%|███▍      | 98/285 [01:56<03:29,  1.12s/it]Loading train:  35%|███▍      | 99/285 [01:57<03:29,  1.12s/it]Loading train:  35%|███▌      | 100/285 [01:58<03:22,  1.09s/it]Loading train:  35%|███▌      | 101/285 [01:59<03:21,  1.09s/it]Loading train:  36%|███▌      | 102/285 [02:00<03:13,  1.06s/it]Loading train:  36%|███▌      | 103/285 [02:02<03:21,  1.11s/it]Loading train:  36%|███▋      | 104/285 [02:03<03:18,  1.10s/it]Loading train:  37%|███▋      | 105/285 [02:04<03:20,  1.11s/it]Loading train:  37%|███▋      | 106/285 [02:05<03:14,  1.09s/it]Loading train:  38%|███▊      | 107/285 [02:06<03:17,  1.11s/it]Loading train:  38%|███▊      | 108/285 [02:07<03:13,  1.10s/it]Loading train:  38%|███▊      | 109/285 [02:08<03:15,  1.11s/it]Loading train:  39%|███▊      | 110/285 [02:09<03:14,  1.11s/it]Loading train:  39%|███▉      | 111/285 [02:10<03:08,  1.08s/it]Loading train:  39%|███▉      | 112/285 [02:12<03:13,  1.12s/it]Loading train:  40%|███▉      | 113/285 [02:13<03:07,  1.09s/it]Loading train:  40%|████      | 114/285 [02:14<03:06,  1.09s/it]Loading train:  40%|████      | 115/285 [02:15<03:03,  1.08s/it]Loading train:  41%|████      | 116/285 [02:16<02:59,  1.06s/it]Loading train:  41%|████      | 117/285 [02:17<03:01,  1.08s/it]Loading train:  41%|████▏     | 118/285 [02:18<03:00,  1.08s/it]Loading train:  42%|████▏     | 119/285 [02:19<02:59,  1.08s/it]Loading train:  42%|████▏     | 120/285 [02:20<02:57,  1.08s/it]Loading train:  42%|████▏     | 121/285 [02:22<03:16,  1.20s/it]Loading train:  43%|████▎     | 122/285 [02:23<03:20,  1.23s/it]Loading train:  43%|████▎     | 123/285 [02:24<03:26,  1.28s/it]Loading train:  44%|████▎     | 124/285 [02:25<03:14,  1.21s/it]Loading train:  44%|████▍     | 125/285 [02:26<03:01,  1.13s/it]Loading train:  44%|████▍     | 126/285 [02:27<02:51,  1.08s/it]Loading train:  45%|████▍     | 127/285 [02:28<02:43,  1.04s/it]Loading train:  45%|████▍     | 128/285 [02:29<02:36,  1.00it/s]Loading train:  45%|████▌     | 129/285 [02:30<02:34,  1.01it/s]Loading train:  46%|████▌     | 130/285 [02:31<02:33,  1.01it/s]Loading train:  46%|████▌     | 131/285 [02:32<02:35,  1.01s/it]Loading train:  46%|████▋     | 132/285 [02:33<02:37,  1.03s/it]Loading train:  47%|████▋     | 133/285 [02:34<02:34,  1.02s/it]Loading train:  47%|████▋     | 134/285 [02:35<02:31,  1.00s/it]Loading train:  47%|████▋     | 135/285 [02:36<02:48,  1.12s/it]Loading train:  48%|████▊     | 136/285 [02:37<02:38,  1.06s/it]Loading train:  48%|████▊     | 137/285 [02:38<02:34,  1.04s/it]Loading train:  48%|████▊     | 138/285 [02:39<02:28,  1.01s/it]Loading train:  49%|████▉     | 139/285 [02:40<02:22,  1.02it/s]Loading train:  49%|████▉     | 140/285 [02:41<02:19,  1.04it/s]Loading train:  49%|████▉     | 141/285 [02:42<02:25,  1.01s/it]Loading train:  50%|████▉     | 142/285 [02:43<02:23,  1.00s/it]Loading train:  50%|█████     | 143/285 [02:44<02:19,  1.02it/s]Loading train:  51%|█████     | 144/285 [02:45<02:14,  1.05it/s]Loading train:  51%|█████     | 145/285 [02:46<02:14,  1.04it/s]Loading train:  51%|█████     | 146/285 [02:47<02:14,  1.03it/s]Loading train:  52%|█████▏    | 147/285 [02:48<02:15,  1.02it/s]Loading train:  52%|█████▏    | 148/285 [02:49<02:11,  1.04it/s]Loading train:  52%|█████▏    | 149/285 [02:50<02:08,  1.06it/s]Loading train:  53%|█████▎    | 150/285 [02:51<02:10,  1.04it/s]Loading train:  53%|█████▎    | 151/285 [02:52<02:05,  1.07it/s]Loading train:  53%|█████▎    | 152/285 [02:53<02:01,  1.09it/s]Loading train:  54%|█████▎    | 153/285 [02:54<02:02,  1.08it/s]Loading train:  54%|█████▍    | 154/285 [02:55<02:02,  1.07it/s]Loading train:  54%|█████▍    | 155/285 [02:55<01:58,  1.09it/s]Loading train:  55%|█████▍    | 156/285 [02:56<02:02,  1.05it/s]Loading train:  55%|█████▌    | 157/285 [02:57<02:00,  1.06it/s]Loading train:  55%|█████▌    | 158/285 [02:59<02:16,  1.07s/it]Loading train:  56%|█████▌    | 159/285 [03:00<02:09,  1.03s/it]Loading train:  56%|█████▌    | 160/285 [03:01<02:08,  1.03s/it]Loading train:  56%|█████▋    | 161/285 [03:02<02:02,  1.01it/s]Loading train:  57%|█████▋    | 162/285 [03:03<02:00,  1.02it/s]Loading train:  57%|█████▋    | 163/285 [03:03<01:57,  1.04it/s]Loading train:  58%|█████▊    | 164/285 [03:04<01:52,  1.08it/s]Loading train:  58%|█████▊    | 165/285 [03:05<01:53,  1.05it/s]Loading train:  58%|█████▊    | 166/285 [03:06<01:51,  1.06it/s]Loading train:  59%|█████▊    | 167/285 [03:07<01:50,  1.07it/s]Loading train:  59%|█████▉    | 168/285 [03:08<01:57,  1.01s/it]Loading train:  59%|█████▉    | 169/285 [03:09<01:52,  1.03it/s]Loading train:  60%|█████▉    | 170/285 [03:10<01:47,  1.07it/s]Loading train:  60%|██████    | 171/285 [03:11<01:46,  1.07it/s]Loading train:  60%|██████    | 172/285 [03:12<01:47,  1.05it/s]Loading train:  61%|██████    | 173/285 [03:13<01:44,  1.08it/s]Loading train:  61%|██████    | 174/285 [03:14<01:40,  1.10it/s]Loading train:  61%|██████▏   | 175/285 [03:15<01:41,  1.09it/s]Loading train:  62%|██████▏   | 176/285 [03:16<01:38,  1.10it/s]Loading train:  62%|██████▏   | 177/285 [03:16<01:35,  1.13it/s]Loading train:  62%|██████▏   | 178/285 [03:17<01:37,  1.10it/s]Loading train:  63%|██████▎   | 179/285 [03:18<01:38,  1.08it/s]Loading train:  63%|██████▎   | 180/285 [03:19<01:33,  1.13it/s]Loading train:  64%|██████▎   | 181/285 [03:20<01:33,  1.11it/s]Loading train:  64%|██████▍   | 182/285 [03:21<01:30,  1.14it/s]Loading train:  64%|██████▍   | 183/285 [03:22<01:30,  1.13it/s]Loading train:  65%|██████▍   | 184/285 [03:23<01:28,  1.14it/s]Loading train:  65%|██████▍   | 185/285 [03:24<01:29,  1.12it/s]Loading train:  65%|██████▌   | 186/285 [03:24<01:28,  1.12it/s]Loading train:  66%|██████▌   | 187/285 [03:25<01:28,  1.11it/s]Loading train:  66%|██████▌   | 188/285 [03:26<01:27,  1.11it/s]Loading train:  66%|██████▋   | 189/285 [03:27<01:30,  1.06it/s]Loading train:  67%|██████▋   | 190/285 [03:28<01:28,  1.07it/s]Loading train:  67%|██████▋   | 191/285 [03:29<01:30,  1.03it/s]Loading train:  67%|██████▋   | 192/285 [03:30<01:27,  1.07it/s]Loading train:  68%|██████▊   | 193/285 [03:31<01:26,  1.06it/s]Loading train:  68%|██████▊   | 194/285 [03:32<01:24,  1.08it/s]Loading train:  68%|██████▊   | 195/285 [03:33<01:25,  1.05it/s]Loading train:  69%|██████▉   | 196/285 [03:34<01:30,  1.02s/it]Loading train:  69%|██████▉   | 197/285 [03:35<01:29,  1.02s/it]Loading train:  69%|██████▉   | 198/285 [03:36<01:28,  1.01s/it]Loading train:  70%|██████▉   | 199/285 [03:37<01:25,  1.01it/s]Loading train:  70%|███████   | 200/285 [03:38<01:26,  1.01s/it]Loading train:  71%|███████   | 201/285 [03:39<01:26,  1.03s/it]Loading train:  71%|███████   | 202/285 [03:40<01:21,  1.01it/s]Loading train:  71%|███████   | 203/285 [03:41<01:20,  1.01it/s]Loading train:  72%|███████▏  | 204/285 [03:42<01:21,  1.01s/it]Loading train:  72%|███████▏  | 205/285 [03:43<01:20,  1.01s/it]Loading train:  72%|███████▏  | 206/285 [03:44<01:19,  1.01s/it]Loading train:  73%|███████▎  | 207/285 [03:45<01:18,  1.01s/it]Loading train:  73%|███████▎  | 208/285 [03:46<01:16,  1.01it/s]Loading train:  73%|███████▎  | 209/285 [03:47<01:15,  1.00it/s]Loading train:  74%|███████▎  | 210/285 [03:48<01:15,  1.01s/it]Loading train:  74%|███████▍  | 211/285 [03:49<01:16,  1.03s/it]Loading train:  74%|███████▍  | 212/285 [03:50<01:14,  1.02s/it]Loading train:  75%|███████▍  | 213/285 [03:51<01:11,  1.00it/s]Loading train:  75%|███████▌  | 214/285 [03:52<01:10,  1.01it/s]Loading train:  75%|███████▌  | 215/285 [03:53<01:06,  1.04it/s]Loading train:  76%|███████▌  | 216/285 [03:54<01:05,  1.05it/s]Loading train:  76%|███████▌  | 217/285 [03:55<01:05,  1.05it/s]Loading train:  76%|███████▋  | 218/285 [03:56<01:04,  1.03it/s]Loading train:  77%|███████▋  | 219/285 [03:57<01:02,  1.05it/s]Loading train:  77%|███████▋  | 220/285 [03:58<01:00,  1.07it/s]Loading train:  78%|███████▊  | 221/285 [03:59<00:58,  1.10it/s]Loading train:  78%|███████▊  | 222/285 [04:00<00:58,  1.08it/s]Loading train:  78%|███████▊  | 223/285 [04:00<00:55,  1.12it/s]Loading train:  79%|███████▊  | 224/285 [04:01<00:56,  1.08it/s]Loading train:  79%|███████▉  | 225/285 [04:02<00:54,  1.10it/s]Loading train:  79%|███████▉  | 226/285 [04:03<00:55,  1.06it/s]Loading train:  80%|███████▉  | 227/285 [04:04<00:53,  1.08it/s]Loading train:  80%|████████  | 228/285 [04:05<00:52,  1.09it/s]Loading train:  80%|████████  | 229/285 [04:06<00:49,  1.14it/s]Loading train:  81%|████████  | 230/285 [04:07<00:47,  1.15it/s]Loading train:  81%|████████  | 231/285 [04:08<00:47,  1.13it/s]Loading train:  81%|████████▏ | 232/285 [04:09<00:49,  1.07it/s]Loading train:  82%|████████▏ | 233/285 [04:10<00:52,  1.00s/it]Loading train:  82%|████████▏ | 234/285 [04:11<00:51,  1.01s/it]Loading train:  82%|████████▏ | 235/285 [04:12<00:52,  1.04s/it]Loading train:  83%|████████▎ | 236/285 [04:13<00:51,  1.05s/it]Loading train:  83%|████████▎ | 237/285 [04:14<00:50,  1.05s/it]Loading train:  84%|████████▎ | 238/285 [04:15<00:48,  1.03s/it]Loading train:  84%|████████▍ | 239/285 [04:16<00:47,  1.03s/it]Loading train:  84%|████████▍ | 240/285 [04:17<00:46,  1.04s/it]Loading train:  85%|████████▍ | 241/285 [04:18<00:46,  1.05s/it]Loading train:  85%|████████▍ | 242/285 [04:19<00:46,  1.08s/it]Loading train:  85%|████████▌ | 243/285 [04:21<00:45,  1.09s/it]Loading train:  86%|████████▌ | 244/285 [04:22<00:45,  1.11s/it]Loading train:  86%|████████▌ | 245/285 [04:23<00:44,  1.11s/it]Loading train:  86%|████████▋ | 246/285 [04:24<00:43,  1.11s/it]Loading train:  87%|████████▋ | 247/285 [04:25<00:42,  1.13s/it]Loading train:  87%|████████▋ | 248/285 [04:26<00:39,  1.08s/it]Loading train:  87%|████████▋ | 249/285 [04:27<00:40,  1.11s/it]Loading train:  88%|████████▊ | 250/285 [04:28<00:36,  1.04s/it]Loading train:  88%|████████▊ | 251/285 [04:29<00:34,  1.01s/it]Loading train:  88%|████████▊ | 252/285 [04:30<00:32,  1.03it/s]Loading train:  89%|████████▉ | 253/285 [04:31<00:31,  1.03it/s]Loading train:  89%|████████▉ | 254/285 [04:32<00:30,  1.01it/s]Loading train:  89%|████████▉ | 255/285 [04:33<00:29,  1.02it/s]Loading train:  90%|████████▉ | 256/285 [04:34<00:28,  1.01it/s]Loading train:  90%|█████████ | 257/285 [04:35<00:27,  1.03it/s]Loading train:  91%|█████████ | 258/285 [04:36<00:25,  1.05it/s]Loading train:  91%|█████████ | 259/285 [04:37<00:24,  1.06it/s]Loading train:  91%|█████████ | 260/285 [04:38<00:23,  1.06it/s]Loading train:  92%|█████████▏| 261/285 [04:39<00:22,  1.08it/s]Loading train:  92%|█████████▏| 262/285 [04:40<00:22,  1.02it/s]Loading train:  92%|█████████▏| 263/285 [04:41<00:21,  1.04it/s]Loading train:  93%|█████████▎| 264/285 [04:41<00:20,  1.05it/s]Loading train:  93%|█████████▎| 265/285 [04:43<00:19,  1.02it/s]Loading train:  93%|█████████▎| 266/285 [04:43<00:18,  1.04it/s]Loading train:  94%|█████████▎| 267/285 [04:44<00:17,  1.05it/s]Loading train:  94%|█████████▍| 268/285 [04:45<00:17,  1.00s/it]Loading train:  94%|█████████▍| 269/285 [04:47<00:16,  1.04s/it]Loading train:  95%|█████████▍| 270/285 [04:48<00:15,  1.07s/it]Loading train:  95%|█████████▌| 271/285 [04:49<00:15,  1.09s/it]Loading train:  95%|█████████▌| 272/285 [04:50<00:13,  1.06s/it]Loading train:  96%|█████████▌| 273/285 [04:51<00:12,  1.06s/it]Loading train:  96%|█████████▌| 274/285 [04:52<00:12,  1.12s/it]Loading train:  96%|█████████▋| 275/285 [04:53<00:11,  1.11s/it]Loading train:  97%|█████████▋| 276/285 [04:54<00:09,  1.10s/it]Loading train:  97%|█████████▋| 277/285 [04:55<00:08,  1.09s/it]Loading train:  98%|█████████▊| 278/285 [04:56<00:07,  1.07s/it]Loading train:  98%|█████████▊| 279/285 [04:58<00:06,  1.09s/it]Loading train:  98%|█████████▊| 280/285 [04:59<00:05,  1.08s/it]Loading train:  99%|█████████▊| 281/285 [05:00<00:04,  1.11s/it]Loading train:  99%|█████████▉| 282/285 [05:01<00:03,  1.11s/it]Loading train:  99%|█████████▉| 283/285 [05:02<00:02,  1.13s/it]Loading train: 100%|█████████▉| 284/285 [05:03<00:01,  1.12s/it]Loading train: 100%|██████████| 285/285 [05:04<00:00,  1.15s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   6%|▋         | 18/285 [00:00<00:01, 173.75it/s]concatenating: train:  14%|█▍        | 40/285 [00:00<00:01, 165.89it/s]concatenating: train:  24%|██▍       | 68/285 [00:00<00:01, 187.69it/s]concatenating: train:  34%|███▍      | 97/285 [00:00<00:00, 208.83it/s]concatenating: train:  44%|████▎     | 124/285 [00:00<00:00, 223.36it/s]concatenating: train:  53%|█████▎    | 152/285 [00:00<00:00, 237.14it/s]concatenating: train:  64%|██████▍   | 182/285 [00:00<00:00, 251.41it/s]concatenating: train:  74%|███████▍  | 211/285 [00:00<00:00, 260.89it/s]concatenating: train:  85%|████████▍ | 242/285 [00:00<00:00, 270.47it/s]concatenating: train:  95%|█████████▍| 270/285 [00:01<00:00, 169.75it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 207.28it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.41s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.40s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.38s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 58.87it/s]2019-07-06 20:30:58.571578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 20:30:58.571681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 20:30:58.571695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 20:30:58.571705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 20:30:58.572159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:09,  4.13it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:07,  4.89it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:07,  4.58it/s]loading the weights for Unet:  20%|██        | 8/40 [00:00<00:05,  5.85it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:06,  5.00it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.70it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  5.04it/s]loading the weights for Unet:  40%|████      | 16/40 [00:01<00:03,  6.39it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.49it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  6.08it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  5.29it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:02<00:02,  6.46it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.96it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  5.76it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:01,  6.36it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:03<00:02,  5.37it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  6.39it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  6.71it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.25it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:04<00:00,  5.77it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  4.97it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  7.64it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 52, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 52, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 52, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 52, 20)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 52, 20)   3620        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 52, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 52, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 52, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 52, 20)   3620        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 52, 20)   80          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 52, 20)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 52, 20)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 52, 20)   3620        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 52, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 52, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 52, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 52, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 52, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 26, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 26, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 26, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 26, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 26, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 26, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 26, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 13, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 13, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 13, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 13, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 13, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 13, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 13, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 13, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 26, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 26, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 26, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 26, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 26, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 26, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 26, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 26, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 52, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 52, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 52, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 52, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 52, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 52, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 52, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 52, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 52, 13)   273         dropout_8[0][0]                  
==================================================================================================
Total params: 195,213
Trainable params: 51,613
Non-trainable params: 143,600
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.49841486e-02 3.19966680e-02 7.50970181e-02 9.33357939e-03
 2.71292049e-02 7.07427267e-03 8.46489586e-02 1.12779077e-01
 8.61338510e-02 1.32649165e-02 2.94521391e-01 1.92807035e-01
 2.29878984e-04]
Train on 18361 samples, validate on 179 samples
Epoch 1/300
 - 22s - loss: 102.9297 - acc: 0.8062 - mDice: 0.0138 - val_loss: 13.1018 - val_acc: 0.9136 - val_mDice: 0.0097

Epoch 00001: val_mDice improved from -inf to 0.00966, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 13s - loss: 16.9577 - acc: 0.8841 - mDice: 0.0111 - val_loss: 7.0566 - val_acc: 0.9136 - val_mDice: 0.0072

Epoch 00002: val_mDice did not improve from 0.00966
Epoch 3/300
 - 13s - loss: 10.5353 - acc: 0.8859 - mDice: 0.0132 - val_loss: 5.8929 - val_acc: 0.9136 - val_mDice: 0.0182

Epoch 00003: val_mDice improved from 0.00966 to 0.01817, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 13s - loss: 8.1430 - acc: 0.8858 - mDice: 0.0244 - val_loss: 4.9965 - val_acc: 0.9136 - val_mDice: 0.0305

Epoch 00004: val_mDice improved from 0.01817 to 0.03051, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 13s - loss: 6.9374 - acc: 0.8854 - mDice: 0.0371 - val_loss: 4.5053 - val_acc: 0.9136 - val_mDice: 0.0443

Epoch 00005: val_mDice improved from 0.03051 to 0.04427, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 13s - loss: 6.1437 - acc: 0.8864 - mDice: 0.0485 - val_loss: 4.1037 - val_acc: 0.9140 - val_mDice: 0.0603

Epoch 00006: val_mDice improved from 0.04427 to 0.06030, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 13s - loss: 5.5461 - acc: 0.8877 - mDice: 0.0593 - val_loss: 3.7494 - val_acc: 0.9148 - val_mDice: 0.0785

Epoch 00007: val_mDice improved from 0.06030 to 0.07850, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 13s - loss: 5.0011 - acc: 0.8897 - mDice: 0.0787 - val_loss: 3.3947 - val_acc: 0.9152 - val_mDice: 0.1052

Epoch 00008: val_mDice improved from 0.07850 to 0.10520, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 14s - loss: 4.4129 - acc: 0.8927 - mDice: 0.1100 - val_loss: 2.8882 - val_acc: 0.9212 - val_mDice: 0.1607

Epoch 00009: val_mDice improved from 0.10520 to 0.16071, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 13s - loss: 3.8857 - acc: 0.8961 - mDice: 0.1508 - val_loss: 2.5871 - val_acc: 0.9268 - val_mDice: 0.2157

Epoch 00010: val_mDice improved from 0.16071 to 0.21570, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 13s - loss: 3.5256 - acc: 0.8994 - mDice: 0.1876 - val_loss: 2.4318 - val_acc: 0.9309 - val_mDice: 0.2563

Epoch 00011: val_mDice improved from 0.21570 to 0.25629, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 13s - loss: 3.2488 - acc: 0.9022 - mDice: 0.2200 - val_loss: 2.3881 - val_acc: 0.9318 - val_mDice: 0.2901

Epoch 00012: val_mDice improved from 0.25629 to 0.29014, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 13s - loss: 3.0358 - acc: 0.9051 - mDice: 0.2478 - val_loss: 2.2432 - val_acc: 0.9366 - val_mDice: 0.3238

Epoch 00013: val_mDice improved from 0.29014 to 0.32380, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 14s - loss: 2.8614 - acc: 0.9083 - mDice: 0.2724 - val_loss: 2.1724 - val_acc: 0.9365 - val_mDice: 0.3475

Epoch 00014: val_mDice improved from 0.32380 to 0.34748, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 13s - loss: 2.7035 - acc: 0.9113 - mDice: 0.2961 - val_loss: 2.2175 - val_acc: 0.9373 - val_mDice: 0.3576

Epoch 00015: val_mDice improved from 0.34748 to 0.35762, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 13s - loss: 2.5737 - acc: 0.9144 - mDice: 0.3175 - val_loss: 2.1571 - val_acc: 0.9408 - val_mDice: 0.3808

Epoch 00016: val_mDice improved from 0.35762 to 0.38080, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 13s - loss: 2.4709 - acc: 0.9169 - mDice: 0.3352 - val_loss: 2.0863 - val_acc: 0.9419 - val_mDice: 0.3964

Epoch 00017: val_mDice improved from 0.38080 to 0.39643, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 13s - loss: 2.3812 - acc: 0.9189 - mDice: 0.3505 - val_loss: 2.0494 - val_acc: 0.9430 - val_mDice: 0.4129

Epoch 00018: val_mDice improved from 0.39643 to 0.41288, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 13s - loss: 2.2910 - acc: 0.9202 - mDice: 0.3670 - val_loss: 2.0672 - val_acc: 0.9442 - val_mDice: 0.4128

Epoch 00019: val_mDice did not improve from 0.41288
Epoch 20/300
 - 14s - loss: 2.2278 - acc: 0.9218 - mDice: 0.3799 - val_loss: 1.9788 - val_acc: 0.9456 - val_mDice: 0.4343

Epoch 00020: val_mDice improved from 0.41288 to 0.43429, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 13s - loss: 2.1771 - acc: 0.9230 - mDice: 0.3894 - val_loss: 2.0237 - val_acc: 0.9448 - val_mDice: 0.4281

Epoch 00021: val_mDice did not improve from 0.43429
Epoch 22/300
 - 13s - loss: 2.1164 - acc: 0.9240 - mDice: 0.4014 - val_loss: 2.0186 - val_acc: 0.9453 - val_mDice: 0.4393

Epoch 00022: val_mDice improved from 0.43429 to 0.43930, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 13s - loss: 2.0705 - acc: 0.9249 - mDice: 0.4103 - val_loss: 1.9817 - val_acc: 0.9468 - val_mDice: 0.4418

Epoch 00023: val_mDice improved from 0.43930 to 0.44181, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 13s - loss: 2.0331 - acc: 0.9258 - mDice: 0.4177 - val_loss: 2.2162 - val_acc: 0.9429 - val_mDice: 0.4230

Epoch 00024: val_mDice did not improve from 0.44181
Epoch 25/300
 - 13s - loss: 1.9974 - acc: 0.9265 - mDice: 0.4250 - val_loss: 1.9996 - val_acc: 0.9476 - val_mDice: 0.4481

Epoch 00025: val_mDice improved from 0.44181 to 0.44811, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 13s - loss: 1.9642 - acc: 0.9273 - mDice: 0.4327 - val_loss: 2.0742 - val_acc: 0.9478 - val_mDice: 0.4507

Epoch 00026: val_mDice improved from 0.44811 to 0.45072, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 14s - loss: 1.9361 - acc: 0.9280 - mDice: 0.4392 - val_loss: 2.0673 - val_acc: 0.9469 - val_mDice: 0.4478

Epoch 00027: val_mDice did not improve from 0.45072
Epoch 28/300
 - 13s - loss: 1.9143 - acc: 0.9287 - mDice: 0.4433 - val_loss: 2.0698 - val_acc: 0.9482 - val_mDice: 0.4536

Epoch 00028: val_mDice improved from 0.45072 to 0.45358, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 13s - loss: 1.8895 - acc: 0.9291 - mDice: 0.4483 - val_loss: 2.0751 - val_acc: 0.9502 - val_mDice: 0.4590

Epoch 00029: val_mDice improved from 0.45358 to 0.45905, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 14s - loss: 1.8547 - acc: 0.9302 - mDice: 0.4569 - val_loss: 2.0624 - val_acc: 0.9495 - val_mDice: 0.4612

Epoch 00030: val_mDice improved from 0.45905 to 0.46119, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 13s - loss: 1.8341 - acc: 0.9306 - mDice: 0.4616 - val_loss: 2.2267 - val_acc: 0.9460 - val_mDice: 0.4387

Epoch 00031: val_mDice did not improve from 0.46119
Epoch 32/300
 - 13s - loss: 1.8235 - acc: 0.9308 - mDice: 0.4645 - val_loss: 2.1473 - val_acc: 0.9484 - val_mDice: 0.4516

Epoch 00032: val_mDice did not improve from 0.46119
Epoch 33/300
 - 13s - loss: 1.7942 - acc: 0.9313 - mDice: 0.4710 - val_loss: 2.0244 - val_acc: 0.9496 - val_mDice: 0.4694

Epoch 00033: val_mDice improved from 0.46119 to 0.46938, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 14s - loss: 1.7741 - acc: 0.9320 - mDice: 0.4764 - val_loss: 2.0680 - val_acc: 0.9499 - val_mDice: 0.4670

Epoch 00034: val_mDice did not improve from 0.46938
Epoch 35/300
 - 13s - loss: 1.7606 - acc: 0.9322 - mDice: 0.4795 - val_loss: 2.3622 - val_acc: 0.9493 - val_mDice: 0.4525

Epoch 00035: val_mDice did not improve from 0.46938
Epoch 36/300
 - 13s - loss: 1.7392 - acc: 0.9326 - mDice: 0.4852 - val_loss: 2.0558 - val_acc: 0.9499 - val_mDice: 0.4738

Epoch 00036: val_mDice improved from 0.46938 to 0.47379, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 13s - loss: 1.7255 - acc: 0.9328 - mDice: 0.4885 - val_loss: 2.1332 - val_acc: 0.9480 - val_mDice: 0.4629

Epoch 00037: val_mDice did not improve from 0.47379
Epoch 38/300
 - 13s - loss: 1.7078 - acc: 0.9332 - mDice: 0.4929 - val_loss: 2.0283 - val_acc: 0.9507 - val_mDice: 0.4875

Epoch 00038: val_mDice improved from 0.47379 to 0.48745, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 13s - loss: 1.6897 - acc: 0.9334 - mDice: 0.4980 - val_loss: 2.0581 - val_acc: 0.9507 - val_mDice: 0.4782

Epoch 00039: val_mDice did not improve from 0.48745
Epoch 40/300
 - 13s - loss: 1.6781 - acc: 0.9336 - mDice: 0.5011 - val_loss: 2.0974 - val_acc: 0.9494 - val_mDice: 0.4740

Epoch 00040: val_mDice did not improve from 0.48745
Epoch 41/300
 - 13s - loss: 1.6738 - acc: 0.9336 - mDice: 0.5025 - val_loss: 2.0857 - val_acc: 0.9497 - val_mDice: 0.4717

Epoch 00041: val_mDice did not improve from 0.48745
Epoch 42/300
 - 14s - loss: 1.6521 - acc: 0.9339 - mDice: 0.5070 - val_loss: 2.0933 - val_acc: 0.9504 - val_mDice: 0.4808

Epoch 00042: val_mDice did not improve from 0.48745
Epoch 43/300
 - 13s - loss: 1.6520 - acc: 0.9337 - mDice: 0.5079 - val_loss: 2.0840 - val_acc: 0.9501 - val_mDice: 0.4863

Epoch 00043: val_mDice did not improve from 0.48745
Epoch 44/300
 - 13s - loss: 1.6290 - acc: 0.9342 - mDice: 0.5126 - val_loss: 2.2725 - val_acc: 0.9494 - val_mDice: 0.4671

Epoch 00044: val_mDice did not improve from 0.48745
Epoch 45/300
 - 13s - loss: 1.6167 - acc: 0.9345 - mDice: 0.5167 - val_loss: 2.1489 - val_acc: 0.9498 - val_mDice: 0.4798

Epoch 00045: val_mDice did not improve from 0.48745
Epoch 46/300
 - 13s - loss: 1.6018 - acc: 0.9348 - mDice: 0.5206 - val_loss: 2.0280 - val_acc: 0.9505 - val_mDice: 0.4920

Epoch 00046: val_mDice improved from 0.48745 to 0.49203, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 13s - loss: 1.5968 - acc: 0.9350 - mDice: 0.5222 - val_loss: 2.0752 - val_acc: 0.9502 - val_mDice: 0.4864

Epoch 00047: val_mDice did not improve from 0.49203
Epoch 48/300
 - 13s - loss: 1.5934 - acc: 0.9349 - mDice: 0.5238 - val_loss: 2.0874 - val_acc: 0.9500 - val_mDice: 0.4908

Epoch 00048: val_mDice did not improve from 0.49203
Epoch 49/300
 - 14s - loss: 1.5818 - acc: 0.9351 - mDice: 0.5255 - val_loss: 2.0337 - val_acc: 0.9504 - val_mDice: 0.4986

Epoch 00049: val_mDice improved from 0.49203 to 0.49857, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 13s - loss: 1.5609 - acc: 0.9356 - mDice: 0.5311 - val_loss: 2.2254 - val_acc: 0.9501 - val_mDice: 0.4809

Epoch 00050: val_mDice did not improve from 0.49857
Epoch 51/300
 - 13s - loss: 1.5568 - acc: 0.9358 - mDice: 0.5329 - val_loss: 2.2195 - val_acc: 0.9497 - val_mDice: 0.4853

Epoch 00051: val_mDice did not improve from 0.49857
Epoch 52/300
 - 13s - loss: 1.5427 - acc: 0.9360 - mDice: 0.5362 - val_loss: 2.1375 - val_acc: 0.9502 - val_mDice: 0.4908

Epoch 00052: val_mDice did not improve from 0.49857
Epoch 53/300
 - 13s - loss: 1.5412 - acc: 0.9359 - mDice: 0.5364 - val_loss: 2.2246 - val_acc: 0.9499 - val_mDice: 0.4845

Epoch 00053: val_mDice did not improve from 0.49857
Epoch 54/300
 - 13s - loss: 1.5370 - acc: 0.9361 - mDice: 0.5381 - val_loss: 2.2814 - val_acc: 0.9493 - val_mDice: 0.4714

Epoch 00054: val_mDice did not improve from 0.49857
Epoch 55/300
 - 13s - loss: 1.5311 - acc: 0.9361 - mDice: 0.5392 - val_loss: 2.0977 - val_acc: 0.9511 - val_mDice: 0.4974

Epoch 00055: val_mDice did not improve from 0.49857
Epoch 56/300
 - 14s - loss: 1.5138 - acc: 0.9366 - mDice: 0.5437 - val_loss: 2.2694 - val_acc: 0.9507 - val_mDice: 0.4903

Epoch 00056: val_mDice did not improve from 0.49857
Epoch 57/300
 - 14s - loss: 1.5108 - acc: 0.9367 - mDice: 0.5451 - val_loss: 2.1546 - val_acc: 0.9506 - val_mDice: 0.4903

Epoch 00057: val_mDice did not improve from 0.49857
Epoch 58/300
 - 14s - loss: 1.5027 - acc: 0.9368 - mDice: 0.5467 - val_loss: 2.0812 - val_acc: 0.9508 - val_mDice: 0.5040

Epoch 00058: val_mDice improved from 0.49857 to 0.50398, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 14s - loss: 1.5032 - acc: 0.9369 - mDice: 0.5475 - val_loss: 2.1435 - val_acc: 0.9505 - val_mDice: 0.4900

Epoch 00059: val_mDice did not improve from 0.50398
Epoch 60/300
 - 14s - loss: 1.4869 - acc: 0.9372 - mDice: 0.5502 - val_loss: 2.2480 - val_acc: 0.9507 - val_mDice: 0.4953

Epoch 00060: val_mDice did not improve from 0.50398
Epoch 61/300
 - 14s - loss: 1.4793 - acc: 0.9376 - mDice: 0.5527 - val_loss: 2.1359 - val_acc: 0.9508 - val_mDice: 0.5029

Epoch 00061: val_mDice did not improve from 0.50398
Epoch 62/300
 - 14s - loss: 1.4735 - acc: 0.9377 - mDice: 0.5540 - val_loss: 2.1851 - val_acc: 0.9512 - val_mDice: 0.4992

Epoch 00062: val_mDice did not improve from 0.50398
Epoch 63/300
 - 14s - loss: 1.4689 - acc: 0.9378 - mDice: 0.5557 - val_loss: 2.2692 - val_acc: 0.9508 - val_mDice: 0.4937

Epoch 00063: val_mDice did not improve from 0.50398
Epoch 64/300
 - 14s - loss: 1.4641 - acc: 0.9380 - mDice: 0.5574 - val_loss: 2.1705 - val_acc: 0.9510 - val_mDice: 0.4998

Epoch 00064: val_mDice did not improve from 0.50398
Epoch 65/300
 - 15s - loss: 1.4557 - acc: 0.9382 - mDice: 0.5592 - val_loss: 2.4091 - val_acc: 0.9467 - val_mDice: 0.4738

Epoch 00065: val_mDice did not improve from 0.50398
Epoch 66/300
 - 14s - loss: 1.4493 - acc: 0.9383 - mDice: 0.5612 - val_loss: 2.1283 - val_acc: 0.9503 - val_mDice: 0.5084

Epoch 00066: val_mDice improved from 0.50398 to 0.50835, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 67/300
 - 13s - loss: 1.4387 - acc: 0.9386 - mDice: 0.5642 - val_loss: 2.2264 - val_acc: 0.9500 - val_mDice: 0.5034

Epoch 00067: val_mDice did not improve from 0.50835
Epoch 68/300
 - 14s - loss: 1.4357 - acc: 0.9386 - mDice: 0.5654 - val_loss: 2.1809 - val_acc: 0.9503 - val_mDice: 0.5032

Epoch 00068: val_mDice did not improve from 0.50835
Epoch 69/300
 - 14s - loss: 1.4302 - acc: 0.9388 - mDice: 0.5664 - val_loss: 2.2481 - val_acc: 0.9501 - val_mDice: 0.4978

Epoch 00069: val_mDice did not improve from 0.50835
Epoch 70/300
 - 13s - loss: 1.4260 - acc: 0.9389 - mDice: 0.5683 - val_loss: 2.2791 - val_acc: 0.9506 - val_mDice: 0.4948

Epoch 00070: val_mDice did not improve from 0.50835
Epoch 71/300
 - 14s - loss: 1.4207 - acc: 0.9389 - mDice: 0.5692 - val_loss: 2.1962 - val_acc: 0.9507 - val_mDice: 0.5022

Epoch 00071: val_mDice did not improve from 0.50835
Epoch 72/300
 - 14s - loss: 1.4152 - acc: 0.9391 - mDice: 0.5709 - val_loss: 2.1020 - val_acc: 0.9501 - val_mDice: 0.5098

Epoch 00072: val_mDice improved from 0.50835 to 0.50982, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 73/300
 - 14s - loss: 1.4089 - acc: 0.9393 - mDice: 0.5724 - val_loss: 2.2937 - val_acc: 0.9513 - val_mDice: 0.5100

Epoch 00073: val_mDice improved from 0.50982 to 0.51003, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 15s - loss: 1.4073 - acc: 0.9394 - mDice: 0.5732 - val_loss: 2.2206 - val_acc: 0.9506 - val_mDice: 0.5000

Epoch 00074: val_mDice did not improve from 0.51003
Epoch 75/300
 - 14s - loss: 1.3972 - acc: 0.9397 - mDice: 0.5763 - val_loss: 2.1909 - val_acc: 0.9497 - val_mDice: 0.5031

Epoch 00075: val_mDice did not improve from 0.51003
Epoch 76/300
 - 13s - loss: 1.3948 - acc: 0.9398 - mDice: 0.5770 - val_loss: 2.3138 - val_acc: 0.9511 - val_mDice: 0.5041

Epoch 00076: val_mDice did not improve from 0.51003
Epoch 77/300
 - 13s - loss: 1.3899 - acc: 0.9399 - mDice: 0.5778 - val_loss: 2.1779 - val_acc: 0.9510 - val_mDice: 0.5112

Epoch 00077: val_mDice improved from 0.51003 to 0.51119, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 78/300
 - 14s - loss: 1.3859 - acc: 0.9400 - mDice: 0.5795 - val_loss: 2.1509 - val_acc: 0.9508 - val_mDice: 0.5152

Epoch 00078: val_mDice improved from 0.51119 to 0.51515, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 79/300
 - 14s - loss: 1.3867 - acc: 0.9400 - mDice: 0.5797 - val_loss: 2.1473 - val_acc: 0.9508 - val_mDice: 0.5116

Epoch 00079: val_mDice did not improve from 0.51515
Epoch 80/300
 - 13s - loss: 1.3835 - acc: 0.9402 - mDice: 0.5800 - val_loss: 2.2084 - val_acc: 0.9505 - val_mDice: 0.5108

Epoch 00080: val_mDice did not improve from 0.51515
Epoch 81/300
 - 13s - loss: 1.3727 - acc: 0.9404 - mDice: 0.5830 - val_loss: 2.0864 - val_acc: 0.9511 - val_mDice: 0.5149

Epoch 00081: val_mDice did not improve from 0.51515
Epoch 82/300
 - 14s - loss: 1.3715 - acc: 0.9406 - mDice: 0.5835 - val_loss: 2.1512 - val_acc: 0.9514 - val_mDice: 0.5160

Epoch 00082: val_mDice improved from 0.51515 to 0.51598, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 83/300
 - 13s - loss: 1.3687 - acc: 0.9407 - mDice: 0.5842 - val_loss: 2.1988 - val_acc: 0.9511 - val_mDice: 0.5104

Epoch 00083: val_mDice did not improve from 0.51598
Epoch 84/300
 - 14s - loss: 1.3615 - acc: 0.9408 - mDice: 0.5858 - val_loss: 2.3189 - val_acc: 0.9507 - val_mDice: 0.5009

Epoch 00084: val_mDice did not improve from 0.51598
Epoch 85/300
 - 14s - loss: 1.3620 - acc: 0.9408 - mDice: 0.5860 - val_loss: 2.1097 - val_acc: 0.9513 - val_mDice: 0.5203

Epoch 00085: val_mDice improved from 0.51598 to 0.52026, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 86/300
 - 13s - loss: 1.3667 - acc: 0.9408 - mDice: 0.5848 - val_loss: 2.1566 - val_acc: 0.9505 - val_mDice: 0.5157

Epoch 00086: val_mDice did not improve from 0.52026
Epoch 87/300
 - 13s - loss: 1.3579 - acc: 0.9412 - mDice: 0.5876 - val_loss: 2.1672 - val_acc: 0.9503 - val_mDice: 0.5139

Epoch 00087: val_mDice did not improve from 0.52026
Epoch 88/300
 - 14s - loss: 1.3499 - acc: 0.9413 - mDice: 0.5898 - val_loss: 2.1650 - val_acc: 0.9507 - val_mDice: 0.5140

Epoch 00088: val_mDice did not improve from 0.52026
Epoch 89/300
 - 13s - loss: 1.3512 - acc: 0.9413 - mDice: 0.5896 - val_loss: 2.2089 - val_acc: 0.9505 - val_mDice: 0.5138

Epoch 00089: val_mDice did not improve from 0.52026
Epoch 90/300
 - 13s - loss: 1.3465 - acc: 0.9415 - mDice: 0.5904 - val_loss: 2.2567 - val_acc: 0.9499 - val_mDice: 0.5042

Epoch 00090: val_mDice did not improve from 0.52026
Epoch 91/300
 - 14s - loss: 1.3455 - acc: 0.9415 - mDice: 0.5909 - val_loss: 2.2684 - val_acc: 0.9499 - val_mDice: 0.5029

Epoch 00091: val_mDice did not improve from 0.52026
Epoch 92/300
 - 13s - loss: 1.3383 - acc: 0.9418 - mDice: 0.5931 - val_loss: 2.2703 - val_acc: 0.9502 - val_mDice: 0.5091

Epoch 00092: val_mDice did not improve from 0.52026
Epoch 93/300
 - 13s - loss: 1.3381 - acc: 0.9418 - mDice: 0.5933 - val_loss: 2.2111 - val_acc: 0.9512 - val_mDice: 0.5137

Epoch 00093: val_mDice did not improve from 0.52026
Epoch 94/300
 - 13s - loss: 1.3280 - acc: 0.9420 - mDice: 0.5961 - val_loss: 2.2950 - val_acc: 0.9504 - val_mDice: 0.5100

Epoch 00094: val_mDice did not improve from 0.52026
Epoch 95/300
 - 13s - loss: 1.3320 - acc: 0.9420 - mDice: 0.5946 - val_loss: 2.2334 - val_acc: 0.9500 - val_mDice: 0.5081

Epoch 00095: val_mDice did not improve from 0.52026
Epoch 96/300
 - 13s - loss: 1.3304 - acc: 0.9420 - mDice: 0.5957 - val_loss: 2.1894 - val_acc: 0.9506 - val_mDice: 0.5149

Epoch 00096: val_mDice did not improve from 0.52026
Epoch 97/300
 - 13s - loss: 1.3314 - acc: 0.9421 - mDice: 0.5953 - val_loss: 2.1891 - val_acc: 0.9489 - val_mDice: 0.5132

Epoch 00097: val_mDice did not improve from 0.52026
Epoch 98/300
 - 14s - loss: 1.3265 - acc: 0.9422 - mDice: 0.5966 - val_loss: 2.3365 - val_acc: 0.9498 - val_mDice: 0.5005

Epoch 00098: val_mDice did not improve from 0.52026
Epoch 99/300
 - 13s - loss: 1.3179 - acc: 0.9425 - mDice: 0.5986 - val_loss: 2.1729 - val_acc: 0.9514 - val_mDice: 0.5208

Epoch 00099: val_mDice improved from 0.52026 to 0.52079, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 100/300
 - 13s - loss: 1.3216 - acc: 0.9424 - mDice: 0.5981 - val_loss: 2.3082 - val_acc: 0.9492 - val_mDice: 0.5051

Epoch 00100: val_mDice did not improve from 0.52079
Epoch 101/300
 - 13s - loss: 1.3151 - acc: 0.9425 - mDice: 0.5995 - val_loss: 2.1969 - val_acc: 0.9506 - val_mDice: 0.5123

Epoch 00101: val_mDice did not improve from 0.52079
Epoch 102/300
 - 13s - loss: 1.3170 - acc: 0.9426 - mDice: 0.5991 - val_loss: 2.2823 - val_acc: 0.9508 - val_mDice: 0.5186

Epoch 00102: val_mDice did not improve from 0.52079
Epoch 103/300
 - 13s - loss: 1.3162 - acc: 0.9428 - mDice: 0.5998 - val_loss: 2.3406 - val_acc: 0.9504 - val_mDice: 0.4994

Epoch 00103: val_mDice did not improve from 0.52079
Epoch 104/300
 - 13s - loss: 1.3082 - acc: 0.9428 - mDice: 0.6015 - val_loss: 2.2078 - val_acc: 0.9509 - val_mDice: 0.5153

Epoch 00104: val_mDice did not improve from 0.52079
Epoch 105/300
 - 14s - loss: 1.3126 - acc: 0.9428 - mDice: 0.6008 - val_loss: 2.2736 - val_acc: 0.9509 - val_mDice: 0.5098

Epoch 00105: val_mDice did not improve from 0.52079
Epoch 106/300
 - 13s - loss: 1.3031 - acc: 0.9431 - mDice: 0.6030 - val_loss: 2.3789 - val_acc: 0.9510 - val_mDice: 0.5042

Epoch 00106: val_mDice did not improve from 0.52079
Epoch 107/300
 - 13s - loss: 1.3061 - acc: 0.9430 - mDice: 0.6028 - val_loss: 2.3347 - val_acc: 0.9485 - val_mDice: 0.4994

Epoch 00107: val_mDice did not improve from 0.52079
Epoch 108/300
 - 13s - loss: 1.3029 - acc: 0.9431 - mDice: 0.6030 - val_loss: 2.2493 - val_acc: 0.9503 - val_mDice: 0.5183

Epoch 00108: val_mDice did not improve from 0.52079
Epoch 109/300
 - 13s - loss: 1.2991 - acc: 0.9432 - mDice: 0.6046 - val_loss: 2.2185 - val_acc: 0.9490 - val_mDice: 0.5140

Epoch 00109: val_mDice did not improve from 0.52079
Epoch 110/300
 - 14s - loss: 1.2991 - acc: 0.9432 - mDice: 0.6047 - val_loss: 2.3710 - val_acc: 0.9496 - val_mDice: 0.5045

Epoch 00110: val_mDice did not improve from 0.52079
Epoch 111/300
 - 13s - loss: 1.2934 - acc: 0.9434 - mDice: 0.6060 - val_loss: 2.1905 - val_acc: 0.9501 - val_mDice: 0.5102

Epoch 00111: val_mDice did not improve from 0.52079
Epoch 112/300
 - 13s - loss: 1.2985 - acc: 0.9433 - mDice: 0.6047 - val_loss: 2.1946 - val_acc: 0.9512 - val_mDice: 0.5224

Epoch 00112: val_mDice improved from 0.52079 to 0.52242, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 113/300
 - 13s - loss: 1.2893 - acc: 0.9436 - mDice: 0.6075 - val_loss: 2.5402 - val_acc: 0.9482 - val_mDice: 0.4814

Epoch 00113: val_mDice did not improve from 0.52242
Epoch 114/300
 - 13s - loss: 1.2928 - acc: 0.9434 - mDice: 0.6062 - val_loss: 2.3486 - val_acc: 0.9506 - val_mDice: 0.5103

Epoch 00114: val_mDice did not improve from 0.52242
Epoch 115/300
 - 14s - loss: 1.2933 - acc: 0.9434 - mDice: 0.6062 - val_loss: 2.2170 - val_acc: 0.9504 - val_mDice: 0.5135

Epoch 00115: val_mDice did not improve from 0.52242
Epoch 116/300
 - 13s - loss: 1.2884 - acc: 0.9436 - mDice: 0.6082 - val_loss: 2.3028 - val_acc: 0.9500 - val_mDice: 0.5081

Epoch 00116: val_mDice did not improve from 0.52242
Epoch 117/300
 - 14s - loss: 1.2846 - acc: 0.9437 - mDice: 0.6090 - val_loss: 2.3624 - val_acc: 0.9517 - val_mDice: 0.5138

Epoch 00117: val_mDice did not improve from 0.52242
Epoch 118/300
 - 13s - loss: 1.2855 - acc: 0.9436 - mDice: 0.6084 - val_loss: 2.2600 - val_acc: 0.9508 - val_mDice: 0.5137

Epoch 00118: val_mDice did not improve from 0.52242
Epoch 119/300
 - 14s - loss: 1.2801 - acc: 0.9438 - mDice: 0.6103 - val_loss: 2.2498 - val_acc: 0.9513 - val_mDice: 0.5181

Epoch 00119: val_mDice did not improve from 0.52242
Epoch 120/300
 - 14s - loss: 1.2817 - acc: 0.9437 - mDice: 0.6092 - val_loss: 2.2488 - val_acc: 0.9508 - val_mDice: 0.5193

Epoch 00120: val_mDice did not improve from 0.52242
Epoch 121/300
 - 14s - loss: 1.2776 - acc: 0.9439 - mDice: 0.6109 - val_loss: 2.2226 - val_acc: 0.9514 - val_mDice: 0.5241

Epoch 00121: val_mDice improved from 0.52242 to 0.52409, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 122/300
 - 13s - loss: 1.2784 - acc: 0.9439 - mDice: 0.6107 - val_loss: 2.2104 - val_acc: 0.9502 - val_mDice: 0.5145

Epoch 00122: val_mDice did not improve from 0.52409
Epoch 123/300
 - 14s - loss: 1.2761 - acc: 0.9438 - mDice: 0.6113 - val_loss: 2.1523 - val_acc: 0.9512 - val_mDice: 0.5298

Epoch 00123: val_mDice improved from 0.52409 to 0.52979, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 124/300
 - 14s - loss: 1.2727 - acc: 0.9441 - mDice: 0.6122 - val_loss: 2.2231 - val_acc: 0.9512 - val_mDice: 0.5185

Epoch 00124: val_mDice did not improve from 0.52979
Epoch 125/300
 - 13s - loss: 1.2734 - acc: 0.9440 - mDice: 0.6128 - val_loss: 2.1447 - val_acc: 0.9510 - val_mDice: 0.5244

Epoch 00125: val_mDice did not improve from 0.52979
Epoch 126/300
 - 14s - loss: 1.2738 - acc: 0.9440 - mDice: 0.6117 - val_loss: 2.3213 - val_acc: 0.9509 - val_mDice: 0.5124

Epoch 00126: val_mDice did not improve from 0.52979
Epoch 127/300
 - 13s - loss: 1.2658 - acc: 0.9443 - mDice: 0.6140 - val_loss: 2.2091 - val_acc: 0.9511 - val_mDice: 0.5170

Epoch 00127: val_mDice did not improve from 0.52979
Epoch 128/300
 - 14s - loss: 1.2665 - acc: 0.9442 - mDice: 0.6140 - val_loss: 2.2364 - val_acc: 0.9513 - val_mDice: 0.5180

Epoch 00128: val_mDice did not improve from 0.52979
Epoch 129/300
 - 14s - loss: 1.2658 - acc: 0.9443 - mDice: 0.6148 - val_loss: 2.3161 - val_acc: 0.9515 - val_mDice: 0.5197

Epoch 00129: val_mDice did not improve from 0.52979
Epoch 130/300
 - 13s - loss: 1.2620 - acc: 0.9443 - mDice: 0.6153 - val_loss: 2.2453 - val_acc: 0.9513 - val_mDice: 0.5209

Epoch 00130: val_mDice did not improve from 0.52979
Epoch 131/300
 - 13s - loss: 1.2601 - acc: 0.9445 - mDice: 0.6159 - val_loss: 2.3827 - val_acc: 0.9518 - val_mDice: 0.5185

Epoch 00131: val_mDice did not improve from 0.52979
Epoch 132/300
 - 13s - loss: 1.2641 - acc: 0.9444 - mDice: 0.6156 - val_loss: 2.1896 - val_acc: 0.9511 - val_mDice: 0.5252

Epoch 00132: val_mDice did not improve from 0.52979
Epoch 133/300
 - 13s - loss: 1.2607 - acc: 0.9444 - mDice: 0.6156 - val_loss: 2.3773 - val_acc: 0.9523 - val_mDice: 0.5182

Epoch 00133: val_mDice did not improve from 0.52979
Epoch 134/300
 - 13s - loss: 1.2554 - acc: 0.9446 - mDice: 0.6176 - val_loss: 2.3004 - val_acc: 0.9515 - val_mDice: 0.5199

Epoch 00134: val_mDice did not improve from 0.52979
Epoch 135/300
 - 14s - loss: 1.2587 - acc: 0.9447 - mDice: 0.6167 - val_loss: 2.2199 - val_acc: 0.9523 - val_mDice: 0.5283

Epoch 00135: val_mDice did not improve from 0.52979
Epoch 136/300
 - 14s - loss: 1.2542 - acc: 0.9448 - mDice: 0.6178 - val_loss: 2.2730 - val_acc: 0.9516 - val_mDice: 0.5197

Epoch 00136: val_mDice did not improve from 0.52979
Epoch 137/300
 - 13s - loss: 1.2548 - acc: 0.9449 - mDice: 0.6176 - val_loss: 2.3616 - val_acc: 0.9518 - val_mDice: 0.5173

Epoch 00137: val_mDice did not improve from 0.52979
Epoch 138/300
 - 14s - loss: 1.2469 - acc: 0.9449 - mDice: 0.6198 - val_loss: 2.4063 - val_acc: 0.9519 - val_mDice: 0.5162

Epoch 00138: val_mDice did not improve from 0.52979
Epoch 139/300
 - 14s - loss: 1.2533 - acc: 0.9448 - mDice: 0.6183 - val_loss: 2.2862 - val_acc: 0.9512 - val_mDice: 0.5175

Epoch 00139: val_mDice did not improve from 0.52979
Epoch 140/300
 - 13s - loss: 1.2547 - acc: 0.9448 - mDice: 0.6186 - val_loss: 2.1905 - val_acc: 0.9510 - val_mDice: 0.5265

Epoch 00140: val_mDice did not improve from 0.52979
Epoch 141/300
 - 13s - loss: 1.2475 - acc: 0.9450 - mDice: 0.6200 - val_loss: 2.2562 - val_acc: 0.9515 - val_mDice: 0.5234

Epoch 00141: val_mDice did not improve from 0.52979
Epoch 142/300
 - 14s - loss: 1.2460 - acc: 0.9451 - mDice: 0.6201 - val_loss: 2.2315 - val_acc: 0.9521 - val_mDice: 0.5267

Epoch 00142: val_mDice did not improve from 0.52979
Epoch 143/300
 - 14s - loss: 1.2466 - acc: 0.9451 - mDice: 0.6200 - val_loss: 2.4092 - val_acc: 0.9520 - val_mDice: 0.5177

Epoch 00143: val_mDice did not improve from 0.52979
Epoch 144/300
 - 13s - loss: 1.2478 - acc: 0.9451 - mDice: 0.6199 - val_loss: 2.3473 - val_acc: 0.9514 - val_mDice: 0.5176

Epoch 00144: val_mDice did not improve from 0.52979
Epoch 145/300
 - 14s - loss: 1.2423 - acc: 0.9453 - mDice: 0.6214 - val_loss: 2.4014 - val_acc: 0.9518 - val_mDice: 0.5225

Epoch 00145: val_mDice did not improve from 0.52979
Epoch 146/300
 - 14s - loss: 1.2411 - acc: 0.9453 - mDice: 0.6215 - val_loss: 2.1421 - val_acc: 0.9520 - val_mDice: 0.5338

Epoch 00146: val_mDice improved from 0.52979 to 0.53378, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 147/300
 - 14s - loss: 1.2366 - acc: 0.9454 - mDice: 0.6228 - val_loss: 2.3072 - val_acc: 0.9512 - val_mDice: 0.5161

Epoch 00147: val_mDice did not improve from 0.53378
Epoch 148/300
 - 14s - loss: 1.2407 - acc: 0.9454 - mDice: 0.6222 - val_loss: 2.2941 - val_acc: 0.9520 - val_mDice: 0.5274

Epoch 00148: val_mDice did not improve from 0.53378
Epoch 149/300
 - 14s - loss: 1.2390 - acc: 0.9454 - mDice: 0.6229 - val_loss: 2.2781 - val_acc: 0.9524 - val_mDice: 0.5299

Epoch 00149: val_mDice did not improve from 0.53378
Epoch 150/300
 - 14s - loss: 1.2384 - acc: 0.9454 - mDice: 0.6227 - val_loss: 2.2530 - val_acc: 0.9522 - val_mDice: 0.5295

Epoch 00150: val_mDice did not improve from 0.53378
Epoch 151/300
 - 15s - loss: 1.2325 - acc: 0.9455 - mDice: 0.6237 - val_loss: 2.2854 - val_acc: 0.9515 - val_mDice: 0.5224

Epoch 00151: val_mDice did not improve from 0.53378
Epoch 152/300
 - 14s - loss: 1.2390 - acc: 0.9454 - mDice: 0.6226 - val_loss: 2.3861 - val_acc: 0.9515 - val_mDice: 0.5186

Epoch 00152: val_mDice did not improve from 0.53378
Epoch 153/300
 - 14s - loss: 1.2333 - acc: 0.9457 - mDice: 0.6246 - val_loss: 2.3265 - val_acc: 0.9514 - val_mDice: 0.5229

Epoch 00153: val_mDice did not improve from 0.53378
Epoch 154/300
 - 15s - loss: 1.2311 - acc: 0.9458 - mDice: 0.6250 - val_loss: 2.3065 - val_acc: 0.9515 - val_mDice: 0.5275

Epoch 00154: val_mDice did not improve from 0.53378
Epoch 155/300
 - 15s - loss: 1.2328 - acc: 0.9457 - mDice: 0.6247 - val_loss: 2.2218 - val_acc: 0.9516 - val_mDice: 0.5274

Epoch 00155: val_mDice did not improve from 0.53378
Epoch 156/300
 - 15s - loss: 1.2317 - acc: 0.9458 - mDice: 0.6247 - val_loss: 2.4119 - val_acc: 0.9518 - val_mDice: 0.5234

Epoch 00156: val_mDice did not improve from 0.53378
Epoch 157/300
 - 14s - loss: 1.2284 - acc: 0.9459 - mDice: 0.6258 - val_loss: 2.2847 - val_acc: 0.9515 - val_mDice: 0.5234

Epoch 00157: val_mDice did not improve from 0.53378
Epoch 158/300
 - 15s - loss: 1.2251 - acc: 0.9459 - mDice: 0.6267 - val_loss: 2.2704 - val_acc: 0.9502 - val_mDice: 0.5200

Epoch 00158: val_mDice did not improve from 0.53378
Epoch 159/300
 - 15s - loss: 1.2259 - acc: 0.9459 - mDice: 0.6262 - val_loss: 2.2398 - val_acc: 0.9526 - val_mDice: 0.5342

Epoch 00159: val_mDice improved from 0.53378 to 0.53422, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 160/300
 - 15s - loss: 1.2269 - acc: 0.9459 - mDice: 0.6264 - val_loss: 2.3512 - val_acc: 0.9524 - val_mDice: 0.5295

Epoch 00160: val_mDice did not improve from 0.53422
Epoch 161/300
 - 15s - loss: 1.2272 - acc: 0.9459 - mDice: 0.6259 - val_loss: 2.5232 - val_acc: 0.9513 - val_mDice: 0.5056

Epoch 00161: val_mDice did not improve from 0.53422
Epoch 162/300
 - 15s - loss: 1.2260 - acc: 0.9459 - mDice: 0.6262 - val_loss: 2.2292 - val_acc: 0.9519 - val_mDice: 0.5306

Epoch 00162: val_mDice did not improve from 0.53422
Epoch 163/300
 - 15s - loss: 1.2183 - acc: 0.9462 - mDice: 0.6288 - val_loss: 2.3760 - val_acc: 0.9527 - val_mDice: 0.5276

Epoch 00163: val_mDice did not improve from 0.53422
Epoch 164/300
 - 15s - loss: 1.2248 - acc: 0.9460 - mDice: 0.6273 - val_loss: 2.5088 - val_acc: 0.9511 - val_mDice: 0.5125

Epoch 00164: val_mDice did not improve from 0.53422
Epoch 165/300
 - 15s - loss: 1.2209 - acc: 0.9460 - mDice: 0.6275 - val_loss: 2.3143 - val_acc: 0.9519 - val_mDice: 0.5288

Epoch 00165: val_mDice did not improve from 0.53422
Epoch 166/300
 - 15s - loss: 1.2211 - acc: 0.9460 - mDice: 0.6282 - val_loss: 2.2488 - val_acc: 0.9504 - val_mDice: 0.5241

Epoch 00166: val_mDice did not improve from 0.53422
Epoch 167/300
 - 14s - loss: 1.2213 - acc: 0.9461 - mDice: 0.6278 - val_loss: 2.3583 - val_acc: 0.9520 - val_mDice: 0.5232

Epoch 00167: val_mDice did not improve from 0.53422
Epoch 168/300
 - 15s - loss: 1.2197 - acc: 0.9461 - mDice: 0.6280 - val_loss: 2.3785 - val_acc: 0.9512 - val_mDice: 0.5194

Epoch 00168: val_mDice did not improve from 0.53422
Epoch 169/300
 - 13s - loss: 1.2153 - acc: 0.9462 - mDice: 0.6291 - val_loss: 2.2558 - val_acc: 0.9522 - val_mDice: 0.5303

Epoch 00169: val_mDice did not improve from 0.53422
Epoch 170/300
 - 14s - loss: 1.2108 - acc: 0.9464 - mDice: 0.6307 - val_loss: 2.2923 - val_acc: 0.9520 - val_mDice: 0.5316

Epoch 00170: val_mDice did not improve from 0.53422
Epoch 171/300
 - 13s - loss: 1.2086 - acc: 0.9464 - mDice: 0.6313 - val_loss: 2.3706 - val_acc: 0.9498 - val_mDice: 0.5143

Epoch 00171: val_mDice did not improve from 0.53422
Epoch 172/300
 - 14s - loss: 1.2165 - acc: 0.9462 - mDice: 0.6293 - val_loss: 2.3421 - val_acc: 0.9515 - val_mDice: 0.5239

Epoch 00172: val_mDice did not improve from 0.53422
Epoch 173/300
 - 13s - loss: 1.2170 - acc: 0.9461 - mDice: 0.6289 - val_loss: 2.3447 - val_acc: 0.9520 - val_mDice: 0.5285

Epoch 00173: val_mDice did not improve from 0.53422
Epoch 174/300
 - 14s - loss: 1.2128 - acc: 0.9464 - mDice: 0.6307 - val_loss: 2.4232 - val_acc: 0.9514 - val_mDice: 0.5131

Epoch 00174: val_mDice did not improve from 0.53422
Epoch 175/300
 - 14s - loss: 1.2084 - acc: 0.9464 - mDice: 0.6317 - val_loss: 2.3036 - val_acc: 0.9522 - val_mDice: 0.5264

Epoch 00175: val_mDice did not improve from 0.53422
Epoch 176/300
 - 13s - loss: 1.2125 - acc: 0.9463 - mDice: 0.6304 - val_loss: 2.3637 - val_acc: 0.9523 - val_mDice: 0.5253

Epoch 00176: val_mDice did not improve from 0.53422
Epoch 177/300
 - 14s - loss: 1.2063 - acc: 0.9465 - mDice: 0.6329 - val_loss: 2.2859 - val_acc: 0.9527 - val_mDice: 0.5301

Epoch 00177: val_mDice did not improve from 0.53422
Epoch 178/300
 - 13s - loss: 1.2060 - acc: 0.9465 - mDice: 0.6325 - val_loss: 2.4107 - val_acc: 0.9521 - val_mDice: 0.5239

Epoch 00178: val_mDice did not improve from 0.53422
Epoch 179/300
 - 14s - loss: 1.2072 - acc: 0.9465 - mDice: 0.6320 - val_loss: 2.3860 - val_acc: 0.9516 - val_mDice: 0.5188

Epoch 00179: val_mDice did not improve from 0.53422
Epoch 180/300
 - 13s - loss: 1.2023 - acc: 0.9467 - mDice: 0.6332 - val_loss: 2.2976 - val_acc: 0.9526 - val_mDice: 0.5300

Epoch 00180: val_mDice did not improve from 0.53422
Epoch 181/300
 - 13s - loss: 1.2083 - acc: 0.9466 - mDice: 0.6320 - val_loss: 2.2754 - val_acc: 0.9515 - val_mDice: 0.5298

Epoch 00181: val_mDice did not improve from 0.53422
Epoch 182/300
 - 14s - loss: 1.2024 - acc: 0.9467 - mDice: 0.6334 - val_loss: 2.3844 - val_acc: 0.9528 - val_mDice: 0.5261

Epoch 00182: val_mDice did not improve from 0.53422
Epoch 183/300
 - 13s - loss: 1.2039 - acc: 0.9467 - mDice: 0.6330 - val_loss: 2.3926 - val_acc: 0.9518 - val_mDice: 0.5236

Epoch 00183: val_mDice did not improve from 0.53422
Epoch 184/300
 - 14s - loss: 1.2033 - acc: 0.9467 - mDice: 0.6333 - val_loss: 2.3539 - val_acc: 0.9532 - val_mDice: 0.5355

Epoch 00184: val_mDice improved from 0.53422 to 0.53547, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 185/300
 - 13s - loss: 1.2032 - acc: 0.9467 - mDice: 0.6332 - val_loss: 2.3012 - val_acc: 0.9519 - val_mDice: 0.5366

Epoch 00185: val_mDice improved from 0.53547 to 0.53660, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 186/300
 - 14s - loss: 1.2020 - acc: 0.9468 - mDice: 0.6341 - val_loss: 2.3204 - val_acc: 0.9531 - val_mDice: 0.5327

Epoch 00186: val_mDice did not improve from 0.53660
Epoch 187/300
 - 13s - loss: 1.2004 - acc: 0.9468 - mDice: 0.6345 - val_loss: 2.3025 - val_acc: 0.9525 - val_mDice: 0.5313

Epoch 00187: val_mDice did not improve from 0.53660
Epoch 188/300
 - 13s - loss: 1.2029 - acc: 0.9468 - mDice: 0.6332 - val_loss: 2.4650 - val_acc: 0.9524 - val_mDice: 0.5254

Epoch 00188: val_mDice did not improve from 0.53660
Epoch 189/300
 - 13s - loss: 1.1991 - acc: 0.9469 - mDice: 0.6343 - val_loss: 2.3899 - val_acc: 0.9527 - val_mDice: 0.5302

Epoch 00189: val_mDice did not improve from 0.53660
Epoch 190/300
 - 14s - loss: 1.1941 - acc: 0.9471 - mDice: 0.6355 - val_loss: 2.2093 - val_acc: 0.9518 - val_mDice: 0.5336

Epoch 00190: val_mDice did not improve from 0.53660
Epoch 191/300
 - 14s - loss: 1.2015 - acc: 0.9469 - mDice: 0.6340 - val_loss: 2.3469 - val_acc: 0.9526 - val_mDice: 0.5319

Epoch 00191: val_mDice did not improve from 0.53660
Epoch 192/300
 - 13s - loss: 1.1954 - acc: 0.9470 - mDice: 0.6355 - val_loss: 2.3558 - val_acc: 0.9523 - val_mDice: 0.5323

Epoch 00192: val_mDice did not improve from 0.53660
Epoch 193/300
 - 14s - loss: 1.1904 - acc: 0.9471 - mDice: 0.6375 - val_loss: 2.3443 - val_acc: 0.9515 - val_mDice: 0.5279

Epoch 00193: val_mDice did not improve from 0.53660
Epoch 194/300
 - 13s - loss: 1.1978 - acc: 0.9470 - mDice: 0.6350 - val_loss: 2.4390 - val_acc: 0.9520 - val_mDice: 0.5173

Epoch 00194: val_mDice did not improve from 0.53660
Epoch 195/300
 - 14s - loss: 1.1948 - acc: 0.9471 - mDice: 0.6351 - val_loss: 2.1990 - val_acc: 0.9524 - val_mDice: 0.5396

Epoch 00195: val_mDice improved from 0.53660 to 0.53964, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 196/300
 - 13s - loss: 1.1905 - acc: 0.9472 - mDice: 0.6376 - val_loss: 2.1777 - val_acc: 0.9514 - val_mDice: 0.5358

Epoch 00196: val_mDice did not improve from 0.53964
Epoch 197/300
 - 13s - loss: 1.1929 - acc: 0.9472 - mDice: 0.6367 - val_loss: 2.3481 - val_acc: 0.9529 - val_mDice: 0.5345

Epoch 00197: val_mDice did not improve from 0.53964
Epoch 198/300
 - 14s - loss: 1.1990 - acc: 0.9471 - mDice: 0.6348 - val_loss: 2.2590 - val_acc: 0.9520 - val_mDice: 0.5302

Epoch 00198: val_mDice did not improve from 0.53964
Epoch 199/300
 - 14s - loss: 1.1927 - acc: 0.9473 - mDice: 0.6365 - val_loss: 2.3040 - val_acc: 0.9528 - val_mDice: 0.5322

Epoch 00199: val_mDice did not improve from 0.53964
Epoch 200/300
 - 14s - loss: 1.1934 - acc: 0.9472 - mDice: 0.6365 - val_loss: 2.2541 - val_acc: 0.9526 - val_mDice: 0.5380

Epoch 00200: val_mDice did not improve from 0.53964
Epoch 201/300
 - 14s - loss: 1.1880 - acc: 0.9473 - mDice: 0.6379 - val_loss: 2.3148 - val_acc: 0.9521 - val_mDice: 0.5335

Epoch 00201: val_mDice did not improve from 0.53964
Epoch 202/300
 - 14s - loss: 1.1874 - acc: 0.9473 - mDice: 0.6386 - val_loss: 2.2423 - val_acc: 0.9523 - val_mDice: 0.5306

Epoch 00202: val_mDice did not improve from 0.53964
Epoch 203/300
 - 14s - loss: 1.1837 - acc: 0.9474 - mDice: 0.6389 - val_loss: 2.3858 - val_acc: 0.9515 - val_mDice: 0.5271

Epoch 00203: val_mDice did not improve from 0.53964
Epoch 204/300
 - 13s - loss: 1.1870 - acc: 0.9474 - mDice: 0.6381 - val_loss: 2.3106 - val_acc: 0.9524 - val_mDice: 0.5351

Epoch 00204: val_mDice did not improve from 0.53964
Epoch 205/300
 - 14s - loss: 1.1819 - acc: 0.9475 - mDice: 0.6397 - val_loss: 2.4014 - val_acc: 0.9527 - val_mDice: 0.5327

Epoch 00205: val_mDice did not improve from 0.53964
Epoch 206/300
 - 13s - loss: 1.1852 - acc: 0.9475 - mDice: 0.6389 - val_loss: 2.3220 - val_acc: 0.9520 - val_mDice: 0.5309

Epoch 00206: val_mDice did not improve from 0.53964
Epoch 207/300
 - 14s - loss: 1.1861 - acc: 0.9474 - mDice: 0.6387 - val_loss: 2.3434 - val_acc: 0.9524 - val_mDice: 0.5391

Epoch 00207: val_mDice did not improve from 0.53964
Epoch 208/300
 - 13s - loss: 1.1825 - acc: 0.9476 - mDice: 0.6399 - val_loss: 2.3446 - val_acc: 0.9521 - val_mDice: 0.5362

Epoch 00208: val_mDice did not improve from 0.53964
Epoch 209/300
 - 13s - loss: 1.1805 - acc: 0.9476 - mDice: 0.6403 - val_loss: 2.3041 - val_acc: 0.9528 - val_mDice: 0.5372

Epoch 00209: val_mDice did not improve from 0.53964
Epoch 210/300
 - 13s - loss: 1.1820 - acc: 0.9476 - mDice: 0.6403 - val_loss: 2.4169 - val_acc: 0.9529 - val_mDice: 0.5313

Epoch 00210: val_mDice did not improve from 0.53964
Epoch 211/300
 - 14s - loss: 1.1818 - acc: 0.9477 - mDice: 0.6404 - val_loss: 2.2364 - val_acc: 0.9528 - val_mDice: 0.5319

Epoch 00211: val_mDice did not improve from 0.53964
Epoch 212/300
 - 13s - loss: 1.1800 - acc: 0.9477 - mDice: 0.6403 - val_loss: 2.4074 - val_acc: 0.9530 - val_mDice: 0.5268

Epoch 00212: val_mDice did not improve from 0.53964
Epoch 213/300
 - 14s - loss: 1.1809 - acc: 0.9476 - mDice: 0.6396 - val_loss: 2.4105 - val_acc: 0.9523 - val_mDice: 0.5302

Epoch 00213: val_mDice did not improve from 0.53964
Epoch 214/300
 - 13s - loss: 1.1793 - acc: 0.9477 - mDice: 0.6403 - val_loss: 2.3975 - val_acc: 0.9526 - val_mDice: 0.5273

Epoch 00214: val_mDice did not improve from 0.53964
Epoch 215/300
 - 13s - loss: 1.1765 - acc: 0.9477 - mDice: 0.6413 - val_loss: 2.4200 - val_acc: 0.9522 - val_mDice: 0.5250

Epoch 00215: val_mDice did not improve from 0.53964
Epoch 216/300
 - 13s - loss: 1.1712 - acc: 0.9479 - mDice: 0.6426 - val_loss: 2.4475 - val_acc: 0.9522 - val_mDice: 0.5277

Epoch 00216: val_mDice did not improve from 0.53964
Epoch 217/300
 - 13s - loss: 1.1772 - acc: 0.9478 - mDice: 0.6415 - val_loss: 2.3396 - val_acc: 0.9530 - val_mDice: 0.5279

Epoch 00217: val_mDice did not improve from 0.53964
Epoch 218/300
 - 14s - loss: 1.1770 - acc: 0.9477 - mDice: 0.6413 - val_loss: 2.3882 - val_acc: 0.9522 - val_mDice: 0.5291

Epoch 00218: val_mDice did not improve from 0.53964
Epoch 219/300
 - 13s - loss: 1.1768 - acc: 0.9477 - mDice: 0.6414 - val_loss: 2.3803 - val_acc: 0.9523 - val_mDice: 0.5238

Epoch 00219: val_mDice did not improve from 0.53964
Epoch 220/300
 - 14s - loss: 1.1742 - acc: 0.9478 - mDice: 0.6426 - val_loss: 2.4020 - val_acc: 0.9533 - val_mDice: 0.5310

Epoch 00220: val_mDice did not improve from 0.53964
Epoch 221/300
 - 13s - loss: 1.1795 - acc: 0.9478 - mDice: 0.6409 - val_loss: 2.4183 - val_acc: 0.9526 - val_mDice: 0.5355

Epoch 00221: val_mDice did not improve from 0.53964
Epoch 222/300
 - 13s - loss: 1.1799 - acc: 0.9478 - mDice: 0.6404 - val_loss: 2.3639 - val_acc: 0.9522 - val_mDice: 0.5379

Epoch 00222: val_mDice did not improve from 0.53964
Epoch 223/300
 - 14s - loss: 1.1729 - acc: 0.9479 - mDice: 0.6426 - val_loss: 2.2534 - val_acc: 0.9528 - val_mDice: 0.5359

Epoch 00223: val_mDice did not improve from 0.53964
Epoch 224/300
 - 13s - loss: 1.1745 - acc: 0.9479 - mDice: 0.6418 - val_loss: 2.3148 - val_acc: 0.9531 - val_mDice: 0.5391

Epoch 00224: val_mDice did not improve from 0.53964
Epoch 225/300
 - 14s - loss: 1.1735 - acc: 0.9480 - mDice: 0.6427 - val_loss: 2.3425 - val_acc: 0.9526 - val_mDice: 0.5324

Epoch 00225: val_mDice did not improve from 0.53964
Restoring model weights from the end of the best epoch
Epoch 00225: early stopping
{'val_loss': [13.101827365726066, 7.056563023082371, 5.892922611875907, 4.9964870900415175, 4.5052511518893965, 4.103662616047779, 3.7494016319679813, 3.3946859996412053, 2.888176210765732, 2.5871117954147596, 2.4318159918545343, 2.388135630325232, 2.2431542087533622, 2.1724225502440384, 2.217542274038219, 2.157133956195256, 2.0863233291903023, 2.049435032146603, 2.067223227889844, 1.9787865697338594, 2.0236562830109834, 2.0186144426548283, 1.9817267742902873, 2.216232068045845, 1.9996263248294426, 2.0742288994389537, 2.0673251085441207, 2.0697842882998163, 2.0750882918608253, 2.0624024255315683, 2.226674034608809, 2.1473086213266384, 2.024418710996319, 2.068015079924514, 2.3621871431446606, 2.055824776601525, 2.133162678287016, 2.028303760390042, 2.0580776696764556, 2.0973639967721267, 2.085731441082235, 2.093335507302311, 2.0840101295343323, 2.2724946450920744, 2.1489313008398985, 2.0279532304689205, 2.0752285006326003, 2.087373348587718, 2.033667296670669, 2.225392786484191, 2.2194726626966252, 2.137510194458775, 2.2245524915237, 2.2813962664684104, 2.0976951322076043, 2.2693921241014365, 2.1546296367432154, 2.081217933633474, 2.1435484992725224, 2.2480276949578823, 2.1359231165667487, 2.1851372718811035, 2.2692252177765915, 2.1704607409471905, 2.4091028780910557, 2.128272806465959, 2.2264065569339517, 2.180867963663027, 2.248134477178478, 2.279103548167138, 2.196240582279653, 2.1019587716576775, 2.2936798713726705, 2.2205672756919648, 2.1909180606543686, 2.313823418910277, 2.1779471629158746, 2.1508718389372588, 2.1473146084300634, 2.2083975403002523, 2.086388256296765, 2.151243239141709, 2.198775013065871, 2.318949925832908, 2.1096965627297344, 2.1566201681531343, 2.167196341733027, 2.1650284388877825, 2.208937281336864, 2.256701595956387, 2.268395222765107, 2.2703355903732043, 2.2110596409057104, 2.2949647330705014, 2.23338167361041, 2.189430412633459, 2.189053777875847, 2.336527282299276, 2.172877231789701, 2.308214481982439, 2.1968730215253776, 2.2823395302841782, 2.3405864997949015, 2.207848350429002, 2.2735575670636567, 2.378917480980218, 2.3347112826128913, 2.249341062993311, 2.2184572699349685, 2.3710037796191, 2.1904920732508826, 2.19463859190488, 2.540218178786379, 2.3485676576305368, 2.217005290132661, 2.3028118024325237, 2.362418266648021, 2.2600180117111632, 2.249843847818215, 2.248777904989999, 2.222639327608673, 2.210421171934245, 2.1522628328653686, 2.2231281363098314, 2.144666910171509, 2.3212964667954257, 2.209097459995547, 2.2364027793181007, 2.3160900177236377, 2.2453309370818753, 2.3826928085455017, 2.189588056596298, 2.3772533792357207, 2.300421955865189, 2.219904935559747, 2.273030330348947, 2.3615562196550424, 2.4063053344215093, 2.2862168170886332, 2.190530037746749, 2.256202379418485, 2.2315437753773266, 2.409213947850233, 2.3473122666001984, 2.4014016849368645, 2.1420681742982493, 2.3071861839827212, 2.2940847514061953, 2.278071396843681, 2.253034988595121, 2.2853942223767327, 2.3861100926745538, 2.3265224981574373, 2.3065246629981355, 2.2218208752530915, 2.411920566132615, 2.284676061662216, 2.270424994676473, 2.2398057356893015, 2.3512215640957796, 2.52322413268702, 2.2292104087062388, 2.375964894641045, 2.5087824933355747, 2.3142580759591898, 2.2488315158716126, 2.3582607261295423, 2.3785364001822873, 2.2557980175124865, 2.292310509601785, 2.370606210644685, 2.34206088827975, 2.344699539951772, 2.4231674870965203, 2.303616495771781, 2.3636850538200505, 2.2859116199962255, 2.4106558434790073, 2.3860478334586714, 2.297588441624988, 2.2753766994902542, 2.3844163204704585, 2.3926346741575104, 2.353881915854342, 2.301166736879828, 2.3203957307272116, 2.3024683091893543, 2.465010174159897, 2.3899303548162876, 2.2092847810777205, 2.3469115169354655, 2.3558288299837593, 2.3442751162544977, 2.438951159322728, 2.1990255323868224, 2.1776876209834435, 2.3480653669581066, 2.2590294163986293, 2.304047380745744, 2.254091003087646, 2.314770549369258, 2.2423139737305027, 2.385834749850481, 2.3106364204896894, 2.4013870135365916, 2.321980784059237, 2.3434047286070925, 2.3446193473965096, 2.3041118563220486, 2.416913432115949, 2.236426667794169, 2.407440326733296, 2.4105416556310386, 2.397462372007317, 2.4199551870037057, 2.4474744756794506, 2.3395701466991916, 2.388195645209797, 2.3803384943381367, 2.402049059308441, 2.418328234603285, 2.363862475869376, 2.253431743749693, 2.3147909548029553, 2.342454761100215], 'val_acc': [0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136020404666496, 0.9139759966781019, 0.914808612629022, 0.9151557057929438, 0.9212401975466552, 0.9268495350576645, 0.9308679889700266, 0.9318472969465416, 0.9366343374358875, 0.9365496282470959, 0.9373140438308929, 0.9407891571188772, 0.9419089668955882, 0.9429688027451159, 0.9442001737695832, 0.9455989019830799, 0.9447724679328876, 0.9452559258684766, 0.94677242120551, 0.9428882452362742, 0.9475595721319401, 0.9478281563220743, 0.9469067094712283, 0.9481649472060816, 0.9502454469989798, 0.9495016782643408, 0.9459522000238216, 0.9484438509914462, 0.9496462691429607, 0.9499479319130242, 0.9492599338126582, 0.9499045493882462, 0.9479955301604457, 0.9507433678850782, 0.9507123724708344, 0.9493818506182239, 0.9497289307956589, 0.9503652912944389, 0.9501173612791732, 0.9494128277181914, 0.9497888574387108, 0.9504851066200427, 0.9501731225898146, 0.9500243633819025, 0.9503817924574101, 0.9500553571312121, 0.9497310026397918, 0.9501937970768806, 0.949933470627449, 0.949268195549203, 0.9511338379130018, 0.9507123857903081, 0.9506421482096837, 0.9507537007997822, 0.9504500061440069, 0.9507433715479334, 0.9507908861064378, 0.9512350779005935, 0.9508198006859039, 0.951022275333298, 0.9466691147015748, 0.9502681730179813, 0.9500450332071528, 0.9502909056967197, 0.9501028923349008, 0.9505532783502973, 0.9507433489048281, 0.9500822268384795, 0.9513487129904038, 0.9506338808123626, 0.9496958775227297, 0.9511007916327961, 0.9510160677925834, 0.9507536848164138, 0.950784672904947, 0.9504892506412954, 0.9510821876579156, 0.9513631536307947, 0.9511008166068093, 0.9506751621901656, 0.951292927371723, 0.9505202353999601, 0.9502908980380224, 0.950668973629701, 0.9505057711175035, 0.9499004276771119, 0.9498694152805393, 0.950226848018902, 0.9512102920249854, 0.9504479163185844, 0.9500161316141736, 0.9506049236105807, 0.9489169716835022, 0.9498466879295904, 0.951398316708357, 0.9492165749299459, 0.9506359366731271, 0.9507867554046588, 0.9503797605716983, 0.950885920884223, 0.950921040007522, 0.9509602971583105, 0.9485058568043416, 0.9503115665313252, 0.9489541759704079, 0.9495863851222246, 0.9501380147880683, 0.9511586557553467, 0.9481979928203135, 0.9506070057773057, 0.9503508120275742, 0.9499685817590638, 0.9517226678699089, 0.9508301439232001, 0.9512970837134889, 0.9508218701991289, 0.9514458126195983, 0.9502330315845638, 0.9511524375590532, 0.9512206239407289, 0.9509540966103197, 0.9509437753501551, 0.951073929917213, 0.9513094421871547, 0.9515140036630897, 0.9513342357214603, 0.9518135539646255, 0.9510718783852774, 0.9522804847642696, 0.9514685423014551, 0.9523445484358505, 0.9516276204386237, 0.9518032553475663, 0.9518983001149567, 0.9511792972767154, 0.9510305330740007, 0.9515160785041041, 0.9521214236094299, 0.9519891695603312, 0.9514251694332954, 0.9518445793476851, 0.9519706028799771, 0.9511917073633418, 0.951993328233005, 0.952379679213689, 0.9522019801193109, 0.9515449904196756, 0.9515243202614385, 0.9513941510429595, 0.9515016082278843, 0.9515759931596298, 0.9517825911830924, 0.951483010246767, 0.9501710760526817, 0.9526441340339916, 0.9523652116013639, 0.9513301100144839, 0.9519334069177425, 0.9527143852670765, 0.951067708391051, 0.9519003439881948, 0.9503921217092589, 0.951997443950376, 0.9511813687878614, 0.9521648157908263, 0.9519995161274958, 0.9498466959212746, 0.9514685419684682, 0.9519891765530549, 0.951361102431846, 0.952216471040715, 0.9523073578014054, 0.9527288085921517, 0.9520573619357701, 0.9515718314900744, 0.9526462025482562, 0.9515222827149503, 0.9528197409720395, 0.951819791141169, 0.9532246739504724, 0.9519230630144727, 0.9531048316529344, 0.9524643830746912, 0.9524375077066475, 0.9527309117370477, 0.9517929004557306, 0.95263585931096, 0.9523342404951597, 0.9514540910054852, 0.952001595630326, 0.9523672717909574, 0.9513900559707726, 0.9529478470040433, 0.951989186875647, 0.9527742736166416, 0.9525594198503974, 0.9521358699105972, 0.95229084864675, 0.9515387868748031, 0.9524272107545224, 0.9526523964365101, 0.9519602479881415, 0.9524333923222632, 0.9520656356598411, 0.9527763447948008, 0.9529065100174376, 0.9527598146619743, 0.9530263426583573, 0.9522908143491052, 0.9526172493423164, 0.9521606617799684, 0.9522433017885219, 0.9529767612505226, 0.9522061048273268, 0.9523176813924779, 0.9532618792363385, 0.9525862809000069, 0.9522288554873546, 0.9527970052964194, 0.953067668989384, 0.952571814619629], 'val_mDice': [0.009656773926950368, 0.007200184085682118, 0.01816950493892811, 0.03051246703711635, 0.044270415982887064, 0.0603043001623793, 0.0785047006090926, 0.10519639698153767, 0.16070819408866946, 0.215699921107159, 0.2562934649723202, 0.2901416321872999, 0.32380340169261956, 0.3474823058317493, 0.3576193115564698, 0.38080425668695117, 0.39643115188156425, 0.41288097943673585, 0.4128121047712571, 0.4342900860908977, 0.4281197095383479, 0.4393042101207392, 0.4418097561298136, 0.4230221078715511, 0.44810502322692447, 0.4507168265028373, 0.44778275156820285, 0.45357902373015546, 0.4590490470385418, 0.4611875086856288, 0.4387400586511836, 0.4515805237786064, 0.46938136536315833, 0.4669580982384069, 0.45250289866378185, 0.4737941372993938, 0.462881485177152, 0.4874528814627472, 0.4782163354271617, 0.4739833152826938, 0.4716556333629779, 0.48082370331833485, 0.48631879104582293, 0.467130790209637, 0.4798170889223088, 0.4920271548478963, 0.48637263951354853, 0.4907728883141246, 0.49856623607640826, 0.480916099341888, 0.4852732881154428, 0.4908449386751186, 0.4845410275725679, 0.47139290890880137, 0.4974094416176141, 0.49027558029030954, 0.4903168939678363, 0.5039823661303388, 0.4899708009298953, 0.49526552248267486, 0.5029216282194553, 0.49918177907027345, 0.49372670803656127, 0.499776088325671, 0.47380195316655677, 0.5083510877366838, 0.5034392639245401, 0.503186595007028, 0.49778343179372436, 0.4948036993682052, 0.5021745747361104, 0.5098237355328139, 0.5100347865893188, 0.5000255322656152, 0.5030829408648294, 0.5041018391454686, 0.5111903365097898, 0.5151533860734056, 0.511565985792842, 0.5108394536226155, 0.5149380364897531, 0.5159781232226495, 0.5103761905065461, 0.5009183817069623, 0.5202568358216206, 0.5156896074390944, 0.5139264178009673, 0.5140436749551549, 0.5138387089002066, 0.5041981482638993, 0.5028915360320214, 0.5091299251471152, 0.5137309097377948, 0.5099692667662764, 0.5081037484400766, 0.5148711219513217, 0.5132253662833954, 0.5005202469879022, 0.5207928840341515, 0.505096847451599, 0.5123294305535002, 0.5185797760606478, 0.4994279026319195, 0.51532113235756, 0.5097827313665572, 0.504150211145092, 0.49938105770995495, 0.5183185415560972, 0.5140475391009667, 0.5044910664664967, 0.5102073661774896, 0.5224191320009072, 0.481370602406603, 0.5102628572693084, 0.513488505472684, 0.5081446746874122, 0.513768881559372, 0.5136996597551101, 0.5180867832133224, 0.5193406984459754, 0.5240933908430557, 0.5145317935410825, 0.52979241669511, 0.5185467121321395, 0.5243821227350715, 0.5124342318353706, 0.5170160147397878, 0.5180176962687316, 0.5196815630910117, 0.5208549106587245, 0.5185231786200454, 0.5251706259210682, 0.5182135759452202, 0.5198959831418938, 0.5282698070536779, 0.5197009850480703, 0.5173259440747053, 0.5161853396026782, 0.5174556098170786, 0.5265239204108382, 0.523385430347986, 0.5267302483819717, 0.5176649335043391, 0.5175796893721852, 0.5224807597072431, 0.5337770444720817, 0.5161460335028238, 0.5274425981431033, 0.5299002688024297, 0.5295421685919416, 0.5224179548924196, 0.5186337806992025, 0.5228631090185496, 0.5274890103486664, 0.5274239109215124, 0.5234178311331978, 0.5233529761183862, 0.519982952978358, 0.5342241920239432, 0.5295422663235797, 0.5056296147447724, 0.5306224839647389, 0.5276133416418257, 0.5125054220913509, 0.5288397102049609, 0.5241235915842003, 0.5232252451960601, 0.5193601803406657, 0.5302847960141784, 0.5315945682911899, 0.514281756551572, 0.5239381245727646, 0.5284547649282317, 0.5130569097383062, 0.5264304405151132, 0.5253024077948245, 0.5300551196716351, 0.5238788156203051, 0.5187943491189839, 0.5300458593075502, 0.5297756734507044, 0.5261364717390284, 0.5236402691742561, 0.5354725470423033, 0.5365973459608728, 0.5327276787278372, 0.5312990669431633, 0.5253564054406555, 0.5302087970952082, 0.5335807021103758, 0.5319030281551723, 0.5323046372589453, 0.5279290772683127, 0.5172719677400323, 0.5396408442012425, 0.5358177540022567, 0.5345482113641068, 0.5302195738813731, 0.5322390603286594, 0.5380171272674752, 0.5335190597526188, 0.5306330636226931, 0.5270614644002648, 0.5350951099528947, 0.5326681351861474, 0.5309389300186541, 0.5391497944986354, 0.5362155137781325, 0.5372322697879216, 0.5312733237303835, 0.5318667072823594, 0.5268196766602926, 0.5302160355631865, 0.5272732235532899, 0.5250200769088788, 0.5277139164881999, 0.5278921423677626, 0.5290500475041693, 0.5237510770392817, 0.5309805145809771, 0.5354586483379982, 0.5378700761155709, 0.5359282397025125, 0.5391237282886185, 0.5324153553840169], 'loss': [102.92972073867413, 16.957735281716204, 10.535259649518672, 8.143034423490306, 6.937367928339099, 6.143706706424507, 5.546139034737456, 5.001088112035963, 4.412875621447759, 3.885697503422582, 3.525582725572843, 3.248801077758102, 3.03578124571701, 2.861355014904594, 2.703548265096896, 2.5737310621215266, 2.4709341146413437, 2.381156613855189, 2.2909556481126776, 2.2278094439197225, 2.1770675734596883, 2.1164434232598044, 2.070535406133499, 2.0330884231019204, 1.997424581076713, 1.9641739428806237, 1.9361248942581333, 1.9143204095868607, 1.8894643130161413, 1.8546993524561677, 1.8340966297282832, 1.8235030738066422, 1.7941570597129686, 1.7741184571126818, 1.7606292859861699, 1.7391665576947073, 1.7254703025803328, 1.707780383100801, 1.6897370750157052, 1.6781205101526353, 1.673798316022772, 1.6520966307334304, 1.6520442407016018, 1.6290324411583872, 1.6166518453282797, 1.6017795400314099, 1.5967559759850973, 1.5934083142264217, 1.5817727198249394, 1.5608698430777337, 1.5567694208433824, 1.5427482814273572, 1.5411595998814887, 1.536954790748196, 1.5310814733189972, 1.5138147478886377, 1.510804458213368, 1.5027474817396813, 1.5031526035410951, 1.4868812409434649, 1.4793136939102058, 1.4734729825947177, 1.4689116151879136, 1.4640656964502707, 1.4556937608497607, 1.4492791722389706, 1.4386779723438288, 1.4356532706770786, 1.4301720285174127, 1.4259857217715357, 1.4207055356194658, 1.415164728880409, 1.4089021877157981, 1.4072684472659387, 1.3972046397998503, 1.3947991621948121, 1.389914161167817, 1.3859152462830322, 1.386747528515794, 1.3834596094200342, 1.372653263303152, 1.3715359972398606, 1.3687000104229368, 1.3614818353251976, 1.361989517862725, 1.366735950156979, 1.3579272185981575, 1.3498995964981997, 1.351178375180462, 1.3464919127745862, 1.3455032262983395, 1.338345399937672, 1.3381146839595908, 1.3280183546495465, 1.3319899761548886, 1.3303733657150576, 1.3313699926915015, 1.3264932694871936, 1.317859501985675, 1.3215746466652916, 1.315127063444202, 1.317021684688548, 1.3161970079026317, 1.3081787540939767, 1.3125820872434493, 1.3030518072082953, 1.3061018013355552, 1.302945466156093, 1.299109241326295, 1.2991378242741884, 1.2934382644803895, 1.2984971969639656, 1.2893208210702414, 1.2928336754024927, 1.2932577709682653, 1.2884492818309774, 1.284610719915426, 1.2854757841055018, 1.28005391634937, 1.281656196584084, 1.2775839492780088, 1.2784321293462875, 1.2761140276998628, 1.2727375569754413, 1.2733939561661562, 1.2738384916777055, 1.2658115285497678, 1.2664969139230011, 1.2658255601018857, 1.2620020592941787, 1.2600724866957544, 1.264086956389881, 1.2606765394941615, 1.2553917430375736, 1.2587015068087597, 1.2541577112219586, 1.25479964639982, 1.2468717840526704, 1.2533424232080217, 1.254679136757731, 1.247535425215841, 1.245964529496653, 1.2466357488283164, 1.2477611233599728, 1.2422911736526414, 1.2410872775149706, 1.2365790593339092, 1.2407494168948727, 1.2390422209513796, 1.238381884753928, 1.2325199541654224, 1.239028669969623, 1.2333436626850678, 1.2310829349842136, 1.2327879074774084, 1.231664917202961, 1.22836016332861, 1.2251157341852121, 1.225865192629791, 1.2269075663285436, 1.2271516742312072, 1.2259917854840743, 1.2182622923387967, 1.224814653130413, 1.2209329053205276, 1.221146201819261, 1.221326251382123, 1.219680416298993, 1.215297701889754, 1.210833790138535, 1.2086409656552601, 1.216454571056688, 1.2170098916708167, 1.2127545197657605, 1.208372251045781, 1.2124767632832696, 1.206287194295592, 1.2060011809547835, 1.2072113778854623, 1.2022512398295384, 1.208269691043323, 1.2024081193324976, 1.2038976309911196, 1.2033261510354685, 1.2031937823122372, 1.2020141714201351, 1.2003637390207877, 1.2029400591003396, 1.1991015124422266, 1.1941208721317436, 1.2014911755011315, 1.1953825232678914, 1.190403563759048, 1.1978184420145335, 1.1948090375044347, 1.1905179683253575, 1.1929389215081834, 1.1989918809188722, 1.1927329044762498, 1.193415690933098, 1.1879914143000947, 1.1874424853321515, 1.1837241268094332, 1.1870254103047067, 1.1818711681242868, 1.185174160400745, 1.1860942453069694, 1.1825429530491998, 1.180480417742474, 1.1820233205805988, 1.1817918663924507, 1.1800334091104978, 1.1809212361426857, 1.1792602489264818, 1.1764603349600382, 1.1711539628393905, 1.1772393161360653, 1.1769598852146115, 1.1768225223097066, 1.1741961286735525, 1.17953323956623, 1.1799244535728306, 1.1729366416005578, 1.1744978076121846, 1.173463289751076], 'acc': [0.8061551124654456, 0.8840980443889023, 0.8859200677862095, 0.8858001608194385, 0.8854472574296389, 0.886412992692839, 0.8876919924586332, 0.889718899250446, 0.8926958475333141, 0.896051077195797, 0.8993969445725645, 0.9022415404446757, 0.9051476527608224, 0.9083126450174482, 0.9112824435834239, 0.9144118055331993, 0.916913733861005, 0.9188795451712527, 0.9202361918568293, 0.9218152047963089, 0.9229716409051946, 0.9239747011676462, 0.9248612795992294, 0.9258337646493932, 0.9265393454500951, 0.9272846742092112, 0.9279915908830669, 0.9286700408934352, 0.9290546702504983, 0.9301558582911686, 0.9306138006471275, 0.9308443435019744, 0.931298539961977, 0.9319704676997616, 0.9321541591580349, 0.9325833780937846, 0.9328154129800463, 0.933160137356928, 0.93342494489022, 0.9335914136771185, 0.9335813641314425, 0.933891225793472, 0.9336612447240074, 0.9341830379640502, 0.9345082257686179, 0.9347912190487803, 0.9349917472853432, 0.9348828622532738, 0.9351495596280993, 0.9356221256903539, 0.9357618292064541, 0.9360047989528908, 0.9358829182871066, 0.9360618394455172, 0.9361274609293828, 0.9365962812385167, 0.9366900575497943, 0.9367809984543882, 0.9368988081433858, 0.9371625250782116, 0.9376161183694642, 0.9377406950636072, 0.9378472840069855, 0.9380068870374018, 0.9381583539283027, 0.9382712464175201, 0.9386045319096117, 0.9386086634649222, 0.9387749919677884, 0.9388827086566263, 0.938921948636476, 0.9391086984158575, 0.9393162402340052, 0.939372818455458, 0.9397177073010354, 0.9397691865157187, 0.9399184598998562, 0.940029217402703, 0.9399652293185368, 0.9401556087484235, 0.940407863230586, 0.9405504664391268, 0.9407180868380145, 0.9408149272130405, 0.9408366394538998, 0.9407572010024285, 0.9411718983106393, 0.9413423981225593, 0.9412567757208437, 0.9415168887512804, 0.9415337054806463, 0.9418008869480291, 0.9417913967368066, 0.9420406951551623, 0.9420349953721392, 0.9420332408153922, 0.942135320013758, 0.9422225931210038, 0.9424664898607169, 0.9423760731887287, 0.9425496763052306, 0.942564720042319, 0.9427750228660363, 0.9428437066217126, 0.9427885584215486, 0.9431072622594423, 0.942985181536175, 0.9430655650658569, 0.9432099195264483, 0.9431814617869394, 0.9433931714828403, 0.9432713937982742, 0.9435652630741148, 0.9434260445270629, 0.9434298289074733, 0.9436376309331209, 0.9436544505354302, 0.943597043127375, 0.9437536717525956, 0.9437490755985404, 0.9438829964674945, 0.9438901484340476, 0.9438178995700777, 0.9440534971494131, 0.9440319852549541, 0.9439643913511032, 0.9442636572186914, 0.9442309484224188, 0.9442767082109385, 0.9443187447007423, 0.9445071514152337, 0.9443999353781433, 0.944442957625086, 0.9446457868943994, 0.944672351740887, 0.9447625265240404, 0.9448783006759506, 0.9449416882597954, 0.9448477852338669, 0.9448202140684072, 0.9450065033782912, 0.9450626609391762, 0.945120305171895, 0.9450733557965953, 0.945286779604046, 0.9453149342431747, 0.9454400764721346, 0.9453521773781524, 0.9454432554471195, 0.945422390494028, 0.9455185068906065, 0.9454433174897081, 0.945725745952478, 0.9457836729055344, 0.9456927517057602, 0.9458127177123261, 0.9459152598213795, 0.9459253315422816, 0.9458911089613822, 0.9459177162430267, 0.9459357440104822, 0.9459410196399579, 0.9461693260659451, 0.9460256950384287, 0.9460181821944972, 0.946043643376308, 0.9460541576343479, 0.9461308343887952, 0.9462140623660046, 0.9463764437045753, 0.9464346729789085, 0.9461692492139021, 0.9461111378535992, 0.9463851463820029, 0.9463830104152606, 0.9463168042923175, 0.9465175791410285, 0.9465273463526723, 0.9464614226676418, 0.9467003234662065, 0.9465918407727935, 0.9467261452234932, 0.9466500300369286, 0.9467038497784047, 0.9466569406989742, 0.9467878618840785, 0.9468227238944916, 0.9467946867441327, 0.9469485906213011, 0.9470902684012644, 0.946924682825286, 0.9470342941503886, 0.9471133105778615, 0.9470307287337, 0.9470827555865492, 0.9472407255873403, 0.9472071694286357, 0.9471042877701553, 0.9472506143098745, 0.9472386315720617, 0.9473017966342799, 0.9473397019096309, 0.9473834899501418, 0.9473527575505273, 0.9475427933374486, 0.9475101233892117, 0.9473878643324573, 0.9476107699725241, 0.9475734896710752, 0.9475829157361303, 0.9476623730613778, 0.9477071293551745, 0.9475886536952126, 0.9476518819297193, 0.9477121218844999, 0.9479118895799491, 0.9477886011460126, 0.9477235640155536, 0.9477494864979054, 0.9478234258990591, 0.9477941612617934, 0.9478390752655652, 0.9479033494883844, 0.9479443396717419, 0.9480313327400638], 'mDice': [0.013754481497808523, 0.011092988392186495, 0.013223723031876264, 0.024436498127675226, 0.037144481566053454, 0.04845460386494654, 0.05934190341133443, 0.07867290109215813, 0.11002738973630305, 0.1508314548083935, 0.18764771645585668, 0.22003205724125055, 0.2478361273424135, 0.2724213531516449, 0.29606997610001995, 0.3175466064988421, 0.3351812467898531, 0.3504654362478792, 0.36696831708406025, 0.3798723677947795, 0.38938581117337895, 0.40138378177266465, 0.41031553004342586, 0.4177170518852061, 0.4250212203790196, 0.4326786791977686, 0.43919502995410364, 0.4433217150756239, 0.44829789356081373, 0.45688124794981055, 0.46159501965480537, 0.4645088084801292, 0.47100555362910995, 0.4763773277293986, 0.4795248954538673, 0.48519546970303856, 0.48846920748031203, 0.4929419748534188, 0.49800243098052405, 0.501075608251752, 0.502490486578188, 0.5070266396409449, 0.50794324583331, 0.5126079991423326, 0.5166782910031344, 0.5205743096309916, 0.5221758805019365, 0.5237813875388362, 0.525459130915621, 0.5311257097626438, 0.532868953988895, 0.5362445796687166, 0.5363654700715482, 0.5381421152591368, 0.5391636613108586, 0.5436924307066257, 0.5450988321819569, 0.5467103153822644, 0.5475193836526552, 0.5501509690737907, 0.5527414022840632, 0.5540156306105563, 0.5557447297271447, 0.557370179190718, 0.559216098863051, 0.5612270341713661, 0.564245281149493, 0.5653713685443904, 0.5664245360401823, 0.5682945402870575, 0.5692005963323163, 0.5709079268983581, 0.5723556329479315, 0.5731587217989119, 0.5763048829871853, 0.5770194996163565, 0.5778139432779903, 0.5795480228705276, 0.5796575311235168, 0.5800031668401922, 0.5829753303643399, 0.5834719079634533, 0.584225274824926, 0.5858150693749714, 0.5860498245002255, 0.5847501611275894, 0.5875829526440691, 0.5898133196522225, 0.5895597559617148, 0.5904206133123824, 0.5908802122690859, 0.5930809572393433, 0.5933427841378598, 0.5960653574792241, 0.5946457667273118, 0.5957066508057811, 0.5953008209033948, 0.5965709986169122, 0.5986174519535608, 0.5981035232186857, 0.5995020893695119, 0.5991391422253706, 0.5997593915577412, 0.601543406769897, 0.6008160172266597, 0.6029977682830007, 0.6027546244459431, 0.6030092130305428, 0.6046235195354636, 0.6046645454342124, 0.6060156863334749, 0.6046548910659355, 0.6074934004244024, 0.6061742240927732, 0.6062361676541946, 0.608162293691377, 0.6090154778262017, 0.6083537299942979, 0.6102651089736627, 0.6091804144254055, 0.6108616215751216, 0.610678664298302, 0.6113224535568575, 0.6122457999223769, 0.6127714083920727, 0.6117274345093892, 0.6140382258527569, 0.6139999871277081, 0.6148187058805984, 0.6153432742701812, 0.615906372512973, 0.6156370305955283, 0.6156462779407907, 0.6175562840882914, 0.616724376415909, 0.6177855435166183, 0.6175842280384243, 0.6197904089321583, 0.618315894095624, 0.6186294130858017, 0.6199887713428468, 0.6201341550507697, 0.62002125149504, 0.6199229738141099, 0.6213874127355694, 0.6215397492907326, 0.6228070084055708, 0.6222205669536718, 0.6228740971551067, 0.6226735425783615, 0.6236839761733229, 0.6225805038603606, 0.6246243406177041, 0.6250178820978718, 0.6247268632134674, 0.6247449187819237, 0.6257891581018379, 0.6267369285264012, 0.6262323394020907, 0.626449215276446, 0.625930216881024, 0.6262381162514101, 0.6287768156295651, 0.6272735435597407, 0.6275429648385901, 0.6281601184756205, 0.6278156411069312, 0.6280171977987373, 0.6291347286889616, 0.6307231858758182, 0.6313392404838362, 0.6292743761810458, 0.6288566506218659, 0.6307355281438101, 0.6316699430506367, 0.6304340382870438, 0.6329438732273016, 0.6325136719429898, 0.6320368474452144, 0.6331738815134397, 0.6320448459399096, 0.6334131061689935, 0.6329543686018265, 0.6332954191822651, 0.6331697712081702, 0.6341230240264728, 0.6345381055981445, 0.633233538114663, 0.6343318910992461, 0.635470576246322, 0.6340484467994595, 0.6355130949783128, 0.6374672164416288, 0.6350376993591414, 0.6350683103647311, 0.6375899207493974, 0.6366913170539491, 0.6348115034848996, 0.6364818072242419, 0.6364526875261505, 0.6379074852137152, 0.6385773038638196, 0.6389344668091009, 0.6381109750197478, 0.6397455844424154, 0.6389127124220021, 0.6386862078930232, 0.6399091080015349, 0.6402703605896182, 0.6403025559600964, 0.6404287607495441, 0.6402878055456424, 0.639588417417895, 0.6403373618588443, 0.6412867340774852, 0.6425792630848091, 0.6414978702917162, 0.6412578718730174, 0.6413694518459472, 0.6425641307182337, 0.6408842398374859, 0.640423305494568, 0.642578924742992, 0.6418175997768351, 0.6427374013751481]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.61s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.33s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  2.13s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:45,  1.64s/it]predicting train subjects:   1%|          | 2/285 [00:03<08:07,  1.72s/it]predicting train subjects:   1%|          | 3/285 [00:05<08:03,  1.72s/it]predicting train subjects:   1%|▏         | 4/285 [00:07<08:26,  1.80s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<08:04,  1.73s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<08:17,  1.78s/it]predicting train subjects:   2%|▏         | 7/285 [00:12<08:53,  1.92s/it]predicting train subjects:   3%|▎         | 8/285 [00:14<08:49,  1.91s/it]predicting train subjects:   3%|▎         | 9/285 [00:16<08:47,  1.91s/it]predicting train subjects:   4%|▎         | 10/285 [00:18<09:09,  2.00s/it]predicting train subjects:   4%|▍         | 11/285 [00:21<09:22,  2.05s/it]predicting train subjects:   4%|▍         | 12/285 [00:23<09:32,  2.10s/it]predicting train subjects:   5%|▍         | 13/285 [00:25<09:43,  2.14s/it]predicting train subjects:   5%|▍         | 14/285 [00:27<09:45,  2.16s/it]predicting train subjects:   5%|▌         | 15/285 [00:30<09:50,  2.19s/it]predicting train subjects:   6%|▌         | 16/285 [00:32<09:50,  2.19s/it]predicting train subjects:   6%|▌         | 17/285 [00:34<09:51,  2.21s/it]predicting train subjects:   6%|▋         | 18/285 [00:36<09:47,  2.20s/it]predicting train subjects:   7%|▋         | 19/285 [00:38<09:51,  2.22s/it]predicting train subjects:   7%|▋         | 20/285 [00:41<09:44,  2.21s/it]predicting train subjects:   7%|▋         | 21/285 [00:43<09:44,  2.22s/it]predicting train subjects:   8%|▊         | 22/285 [00:45<09:38,  2.20s/it]predicting train subjects:   8%|▊         | 23/285 [00:47<09:34,  2.19s/it]predicting train subjects:   8%|▊         | 24/285 [00:49<09:25,  2.17s/it]predicting train subjects:   9%|▉         | 25/285 [00:52<09:25,  2.18s/it]predicting train subjects:   9%|▉         | 26/285 [00:54<09:27,  2.19s/it]predicting train subjects:   9%|▉         | 27/285 [00:56<09:28,  2.20s/it]predicting train subjects:  10%|▉         | 28/285 [00:58<09:20,  2.18s/it]predicting train subjects:  10%|█         | 29/285 [01:00<09:01,  2.12s/it]predicting train subjects:  11%|█         | 30/285 [01:02<08:44,  2.06s/it]predicting train subjects:  11%|█         | 31/285 [01:04<08:37,  2.04s/it]predicting train subjects:  11%|█         | 32/285 [01:06<08:23,  1.99s/it]predicting train subjects:  12%|█▏        | 33/285 [01:08<08:18,  1.98s/it]predicting train subjects:  12%|█▏        | 34/285 [01:10<08:14,  1.97s/it]predicting train subjects:  12%|█▏        | 35/285 [01:12<08:17,  1.99s/it]predicting train subjects:  13%|█▎        | 36/285 [01:14<08:19,  2.01s/it]predicting train subjects:  13%|█▎        | 37/285 [01:16<08:13,  1.99s/it]predicting train subjects:  13%|█▎        | 38/285 [01:18<08:08,  1.98s/it]predicting train subjects:  14%|█▎        | 39/285 [01:20<08:03,  1.97s/it]predicting train subjects:  14%|█▍        | 40/285 [01:22<07:56,  1.94s/it]predicting train subjects:  14%|█▍        | 41/285 [01:24<07:54,  1.95s/it]predicting train subjects:  15%|█▍        | 42/285 [01:25<07:48,  1.93s/it]predicting train subjects:  15%|█▌        | 43/285 [01:27<07:57,  1.97s/it]predicting train subjects:  15%|█▌        | 44/285 [01:30<08:01,  2.00s/it]predicting train subjects:  16%|█▌        | 45/285 [01:32<08:01,  2.00s/it]predicting train subjects:  16%|█▌        | 46/285 [01:33<07:48,  1.96s/it]predicting train subjects:  16%|█▋        | 47/285 [01:35<07:35,  1.91s/it]predicting train subjects:  17%|█▋        | 48/285 [01:37<07:19,  1.85s/it]predicting train subjects:  17%|█▋        | 49/285 [01:39<07:13,  1.84s/it]predicting train subjects:  18%|█▊        | 50/285 [01:40<06:58,  1.78s/it]predicting train subjects:  18%|█▊        | 51/285 [01:42<06:52,  1.76s/it]predicting train subjects:  18%|█▊        | 52/285 [01:44<06:52,  1.77s/it]predicting train subjects:  19%|█▊        | 53/285 [01:46<06:49,  1.76s/it]predicting train subjects:  19%|█▉        | 54/285 [01:47<06:44,  1.75s/it]predicting train subjects:  19%|█▉        | 55/285 [01:49<06:44,  1.76s/it]predicting train subjects:  20%|█▉        | 56/285 [01:51<06:41,  1.75s/it]predicting train subjects:  20%|██        | 57/285 [01:53<06:38,  1.75s/it]predicting train subjects:  20%|██        | 58/285 [01:54<06:35,  1.74s/it]predicting train subjects:  21%|██        | 59/285 [01:56<06:35,  1.75s/it]predicting train subjects:  21%|██        | 60/285 [01:58<06:34,  1.75s/it]predicting train subjects:  21%|██▏       | 61/285 [02:00<06:34,  1.76s/it]predicting train subjects:  22%|██▏       | 62/285 [02:01<06:34,  1.77s/it]predicting train subjects:  22%|██▏       | 63/285 [02:03<06:32,  1.77s/it]predicting train subjects:  22%|██▏       | 64/285 [02:05<06:34,  1.79s/it]predicting train subjects:  23%|██▎       | 65/285 [02:07<06:54,  1.89s/it]predicting train subjects:  23%|██▎       | 66/285 [02:09<07:02,  1.93s/it]predicting train subjects:  24%|██▎       | 67/285 [02:11<06:48,  1.87s/it]predicting train subjects:  24%|██▍       | 68/285 [02:13<06:44,  1.86s/it]predicting train subjects:  24%|██▍       | 69/285 [02:15<06:40,  1.85s/it]predicting train subjects:  25%|██▍       | 70/285 [02:16<06:37,  1.85s/it]predicting train subjects:  25%|██▍       | 71/285 [02:18<06:27,  1.81s/it]predicting train subjects:  25%|██▌       | 72/285 [02:20<06:26,  1.81s/it]predicting train subjects:  26%|██▌       | 73/285 [02:22<06:18,  1.78s/it]predicting train subjects:  26%|██▌       | 74/285 [02:24<06:19,  1.80s/it]predicting train subjects:  26%|██▋       | 75/285 [02:25<06:14,  1.78s/it]predicting train subjects:  27%|██▋       | 76/285 [02:27<06:13,  1.79s/it]predicting train subjects:  27%|██▋       | 77/285 [02:29<06:13,  1.79s/it]predicting train subjects:  27%|██▋       | 78/285 [02:31<06:13,  1.80s/it]predicting train subjects:  28%|██▊       | 79/285 [02:32<06:08,  1.79s/it]predicting train subjects:  28%|██▊       | 80/285 [02:34<06:07,  1.79s/it]predicting train subjects:  28%|██▊       | 81/285 [02:36<06:06,  1.79s/it]predicting train subjects:  29%|██▉       | 82/285 [02:38<06:04,  1.80s/it]predicting train subjects:  29%|██▉       | 83/285 [02:40<06:03,  1.80s/it]predicting train subjects:  29%|██▉       | 84/285 [02:41<06:00,  1.79s/it]predicting train subjects:  30%|██▉       | 85/285 [02:43<06:14,  1.87s/it]predicting train subjects:  30%|███       | 86/285 [02:45<06:19,  1.91s/it]predicting train subjects:  31%|███       | 87/285 [02:47<06:23,  1.94s/it]predicting train subjects:  31%|███       | 88/285 [02:49<06:25,  1.96s/it]predicting train subjects:  31%|███       | 89/285 [02:51<06:21,  1.95s/it]predicting train subjects:  32%|███▏      | 90/285 [02:53<06:21,  1.96s/it]predicting train subjects:  32%|███▏      | 91/285 [02:55<06:20,  1.96s/it]predicting train subjects:  32%|███▏      | 92/285 [02:57<06:19,  1.97s/it]predicting train subjects:  33%|███▎      | 93/285 [02:59<06:19,  1.98s/it]predicting train subjects:  33%|███▎      | 94/285 [03:01<06:18,  1.98s/it]predicting train subjects:  33%|███▎      | 95/285 [03:03<06:15,  1.97s/it]predicting train subjects:  34%|███▎      | 96/285 [03:05<06:09,  1.96s/it]predicting train subjects:  34%|███▍      | 97/285 [03:07<06:09,  1.97s/it]predicting train subjects:  34%|███▍      | 98/285 [03:09<06:07,  1.96s/it]predicting train subjects:  35%|███▍      | 99/285 [03:11<06:04,  1.96s/it]predicting train subjects:  35%|███▌      | 100/285 [03:13<06:02,  1.96s/it]predicting train subjects:  35%|███▌      | 101/285 [03:15<06:00,  1.96s/it]predicting train subjects:  36%|███▌      | 102/285 [03:17<05:54,  1.93s/it]predicting train subjects:  36%|███▌      | 103/285 [03:19<05:53,  1.94s/it]predicting train subjects:  36%|███▋      | 104/285 [03:21<06:16,  2.08s/it]predicting train subjects:  37%|███▋      | 105/285 [03:23<06:17,  2.09s/it]predicting train subjects:  37%|███▋      | 106/285 [03:26<06:23,  2.14s/it]predicting train subjects:  38%|███▊      | 107/285 [03:28<06:21,  2.14s/it]predicting train subjects:  38%|███▊      | 108/285 [03:30<06:29,  2.20s/it]predicting train subjects:  38%|███▊      | 109/285 [03:32<06:30,  2.22s/it]predicting train subjects:  39%|███▊      | 110/285 [03:35<06:36,  2.27s/it]predicting train subjects:  39%|███▉      | 111/285 [03:37<06:41,  2.31s/it]predicting train subjects:  39%|███▉      | 112/285 [03:39<06:39,  2.31s/it]predicting train subjects:  40%|███▉      | 113/285 [03:42<06:44,  2.35s/it]predicting train subjects:  40%|████      | 114/285 [03:44<06:37,  2.32s/it]predicting train subjects:  40%|████      | 115/285 [03:47<06:42,  2.37s/it]predicting train subjects:  41%|████      | 116/285 [03:49<06:36,  2.35s/it]predicting train subjects:  41%|████      | 117/285 [03:51<06:33,  2.34s/it]predicting train subjects:  41%|████▏     | 118/285 [03:54<06:34,  2.36s/it]predicting train subjects:  42%|████▏     | 119/285 [03:56<06:17,  2.28s/it]predicting train subjects:  42%|████▏     | 120/285 [03:58<06:15,  2.28s/it]predicting train subjects:  42%|████▏     | 121/285 [04:00<06:19,  2.31s/it]predicting train subjects:  43%|████▎     | 122/285 [04:02<05:51,  2.15s/it]predicting train subjects:  43%|████▎     | 123/285 [04:04<05:36,  2.07s/it]predicting train subjects:  44%|████▎     | 124/285 [04:07<05:51,  2.19s/it]predicting train subjects:  44%|████▍     | 125/285 [04:09<05:39,  2.12s/it]predicting train subjects:  44%|████▍     | 126/285 [04:11<05:32,  2.09s/it]predicting train subjects:  45%|████▍     | 127/285 [04:13<05:26,  2.07s/it]predicting train subjects:  45%|████▍     | 128/285 [04:15<05:27,  2.08s/it]predicting train subjects:  45%|████▌     | 129/285 [04:17<05:25,  2.08s/it]predicting train subjects:  46%|████▌     | 130/285 [04:19<05:23,  2.09s/it]predicting train subjects:  46%|████▌     | 131/285 [04:21<05:20,  2.08s/it]predicting train subjects:  46%|████▋     | 132/285 [04:23<05:19,  2.09s/it]predicting train subjects:  47%|████▋     | 133/285 [04:25<05:24,  2.14s/it]predicting train subjects:  47%|████▋     | 134/285 [04:27<05:20,  2.12s/it]predicting train subjects:  47%|████▋     | 135/285 [04:29<05:13,  2.09s/it]predicting train subjects:  48%|████▊     | 136/285 [04:31<05:10,  2.08s/it]predicting train subjects:  48%|████▊     | 137/285 [04:33<05:03,  2.05s/it]predicting train subjects:  48%|████▊     | 138/285 [04:35<05:00,  2.04s/it]predicting train subjects:  49%|████▉     | 139/285 [04:38<05:02,  2.07s/it]predicting train subjects:  49%|████▉     | 140/285 [04:40<05:01,  2.08s/it]predicting train subjects:  49%|████▉     | 141/285 [04:42<05:00,  2.09s/it]predicting train subjects:  50%|████▉     | 142/285 [04:44<04:50,  2.03s/it]predicting train subjects:  50%|█████     | 143/285 [04:45<04:36,  1.95s/it]predicting train subjects:  51%|█████     | 144/285 [04:47<04:28,  1.90s/it]predicting train subjects:  51%|█████     | 145/285 [04:49<04:26,  1.90s/it]predicting train subjects:  51%|█████     | 146/285 [04:51<04:30,  1.94s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:53<04:24,  1.92s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:55<04:23,  1.92s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:57<04:20,  1.92s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:59<04:15,  1.90s/it]predicting train subjects:  53%|█████▎    | 151/285 [05:01<04:20,  1.94s/it]predicting train subjects:  53%|█████▎    | 152/285 [05:03<04:11,  1.89s/it]predicting train subjects:  54%|█████▎    | 153/285 [05:05<04:12,  1.91s/it]predicting train subjects:  54%|█████▍    | 154/285 [05:07<04:33,  2.09s/it]predicting train subjects:  54%|█████▍    | 155/285 [05:09<04:24,  2.04s/it]predicting train subjects:  55%|█████▍    | 156/285 [05:11<04:18,  2.00s/it]predicting train subjects:  55%|█████▌    | 157/285 [05:13<04:09,  1.95s/it]predicting train subjects:  55%|█████▌    | 158/285 [05:15<04:05,  1.93s/it]predicting train subjects:  56%|█████▌    | 159/285 [05:17<04:04,  1.94s/it]predicting train subjects:  56%|█████▌    | 160/285 [05:18<03:54,  1.88s/it]predicting train subjects:  56%|█████▋    | 161/285 [05:20<03:56,  1.91s/it]predicting train subjects:  57%|█████▋    | 162/285 [05:22<03:46,  1.84s/it]predicting train subjects:  57%|█████▋    | 163/285 [05:24<03:40,  1.81s/it]predicting train subjects:  58%|█████▊    | 164/285 [05:25<03:39,  1.82s/it]predicting train subjects:  58%|█████▊    | 165/285 [05:27<03:43,  1.86s/it]predicting train subjects:  58%|█████▊    | 166/285 [05:29<03:38,  1.83s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:31<03:41,  1.88s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:33<03:38,  1.86s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:35<03:33,  1.84s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:37<03:39,  1.91s/it]predicting train subjects:  60%|██████    | 171/285 [05:39<03:31,  1.86s/it]predicting train subjects:  60%|██████    | 172/285 [05:41<03:30,  1.86s/it]predicting train subjects:  61%|██████    | 173/285 [05:42<03:26,  1.84s/it]predicting train subjects:  61%|██████    | 174/285 [05:44<03:18,  1.79s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:46<03:15,  1.78s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:48<03:13,  1.78s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:49<03:12,  1.78s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:51<03:07,  1.76s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:53<03:03,  1.73s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:54<02:59,  1.71s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:56<03:02,  1.75s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:58<03:00,  1.76s/it]predicting train subjects:  64%|██████▍   | 183/285 [06:00<02:55,  1.72s/it]predicting train subjects:  65%|██████▍   | 184/285 [06:01<02:49,  1.68s/it]predicting train subjects:  65%|██████▍   | 185/285 [06:03<02:47,  1.68s/it]predicting train subjects:  65%|██████▌   | 186/285 [06:04<02:44,  1.67s/it]predicting train subjects:  66%|██████▌   | 187/285 [06:06<02:46,  1.70s/it]predicting train subjects:  66%|██████▌   | 188/285 [06:08<02:45,  1.71s/it]predicting train subjects:  66%|██████▋   | 189/285 [06:10<02:47,  1.74s/it]predicting train subjects:  67%|██████▋   | 190/285 [06:12<02:52,  1.81s/it]predicting train subjects:  67%|██████▋   | 191/285 [06:14<02:55,  1.87s/it]predicting train subjects:  67%|██████▋   | 192/285 [06:16<02:51,  1.84s/it]predicting train subjects:  68%|██████▊   | 193/285 [06:17<02:49,  1.84s/it]predicting train subjects:  68%|██████▊   | 194/285 [06:19<02:45,  1.82s/it]predicting train subjects:  68%|██████▊   | 195/285 [06:21<02:41,  1.79s/it]predicting train subjects:  69%|██████▉   | 196/285 [06:23<02:47,  1.88s/it]predicting train subjects:  69%|██████▉   | 197/285 [06:25<02:47,  1.90s/it]predicting train subjects:  69%|██████▉   | 198/285 [06:27<02:54,  2.00s/it]predicting train subjects:  70%|██████▉   | 199/285 [06:29<02:57,  2.06s/it]predicting train subjects:  70%|███████   | 200/285 [06:31<02:55,  2.06s/it]predicting train subjects:  71%|███████   | 201/285 [06:33<02:50,  2.03s/it]predicting train subjects:  71%|███████   | 202/285 [06:36<02:51,  2.07s/it]predicting train subjects:  71%|███████   | 203/285 [06:37<02:45,  2.02s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:40<02:47,  2.07s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:42<02:42,  2.03s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:44<02:42,  2.06s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:46<02:44,  2.11s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:48<02:36,  2.03s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:50<02:35,  2.04s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:52<02:32,  2.04s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:54<02:30,  2.03s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:56<02:29,  2.05s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:58<02:29,  2.08s/it]predicting train subjects:  75%|███████▌  | 214/285 [07:00<02:23,  2.02s/it]predicting train subjects:  75%|███████▌  | 215/285 [07:02<02:17,  1.97s/it]predicting train subjects:  76%|███████▌  | 216/285 [07:04<02:14,  1.95s/it]predicting train subjects:  76%|███████▌  | 217/285 [07:06<02:10,  1.92s/it]predicting train subjects:  76%|███████▋  | 218/285 [07:07<02:05,  1.88s/it]predicting train subjects:  77%|███████▋  | 219/285 [07:09<02:02,  1.85s/it]predicting train subjects:  77%|███████▋  | 220/285 [07:11<02:08,  1.98s/it]predicting train subjects:  78%|███████▊  | 221/285 [07:13<02:00,  1.89s/it]predicting train subjects:  78%|███████▊  | 222/285 [07:15<01:56,  1.85s/it]predicting train subjects:  78%|███████▊  | 223/285 [07:17<01:54,  1.84s/it]predicting train subjects:  79%|███████▊  | 224/285 [07:18<01:51,  1.82s/it]predicting train subjects:  79%|███████▉  | 225/285 [07:20<01:50,  1.85s/it]predicting train subjects:  79%|███████▉  | 226/285 [07:22<01:49,  1.85s/it]predicting train subjects:  80%|███████▉  | 227/285 [07:24<01:46,  1.83s/it]predicting train subjects:  80%|████████  | 228/285 [07:26<01:42,  1.80s/it]predicting train subjects:  80%|████████  | 229/285 [07:28<01:40,  1.79s/it]predicting train subjects:  81%|████████  | 230/285 [07:29<01:36,  1.76s/it]predicting train subjects:  81%|████████  | 231/285 [07:31<01:36,  1.79s/it]predicting train subjects:  81%|████████▏ | 232/285 [07:33<01:41,  1.92s/it]predicting train subjects:  82%|████████▏ | 233/285 [07:36<01:44,  2.01s/it]predicting train subjects:  82%|████████▏ | 234/285 [07:38<01:46,  2.09s/it]predicting train subjects:  82%|████████▏ | 235/285 [07:40<01:47,  2.14s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:42<01:44,  2.13s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:44<01:44,  2.18s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:47<01:42,  2.19s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:49<01:39,  2.15s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:51<01:38,  2.18s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:53<01:37,  2.22s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:56<01:37,  2.26s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:58<01:33,  2.22s/it]predicting train subjects:  86%|████████▌ | 244/285 [08:00<01:31,  2.24s/it]predicting train subjects:  86%|████████▌ | 245/285 [08:02<01:29,  2.23s/it]predicting train subjects:  86%|████████▋ | 246/285 [08:05<01:28,  2.26s/it]predicting train subjects:  87%|████████▋ | 247/285 [08:07<01:26,  2.28s/it]predicting train subjects:  87%|████████▋ | 248/285 [08:09<01:22,  2.22s/it]predicting train subjects:  87%|████████▋ | 249/285 [08:11<01:20,  2.23s/it]predicting train subjects:  88%|████████▊ | 250/285 [08:13<01:13,  2.09s/it]predicting train subjects:  88%|████████▊ | 251/285 [08:15<01:08,  2.01s/it]predicting train subjects:  88%|████████▊ | 252/285 [08:17<01:04,  1.96s/it]predicting train subjects:  89%|████████▉ | 253/285 [08:18<01:00,  1.89s/it]predicting train subjects:  89%|████████▉ | 254/285 [08:20<00:56,  1.83s/it]predicting train subjects:  89%|████████▉ | 255/285 [08:22<00:54,  1.81s/it]predicting train subjects:  90%|████████▉ | 256/285 [08:24<00:53,  1.83s/it]predicting train subjects:  90%|█████████ | 257/285 [08:25<00:50,  1.79s/it]predicting train subjects:  91%|█████████ | 258/285 [08:27<00:48,  1.78s/it]predicting train subjects:  91%|█████████ | 259/285 [08:29<00:47,  1.81s/it]predicting train subjects:  91%|█████████ | 260/285 [08:31<00:46,  1.87s/it]predicting train subjects:  92%|█████████▏| 261/285 [08:33<00:44,  1.85s/it]predicting train subjects:  92%|█████████▏| 262/285 [08:35<00:42,  1.87s/it]predicting train subjects:  92%|█████████▏| 263/285 [08:36<00:39,  1.80s/it]predicting train subjects:  93%|█████████▎| 264/285 [08:38<00:37,  1.80s/it]predicting train subjects:  93%|█████████▎| 265/285 [08:40<00:36,  1.84s/it]predicting train subjects:  93%|█████████▎| 266/285 [08:42<00:34,  1.83s/it]predicting train subjects:  94%|█████████▎| 267/285 [08:44<00:33,  1.85s/it]predicting train subjects:  94%|█████████▍| 268/285 [08:46<00:33,  1.96s/it]predicting train subjects:  94%|█████████▍| 269/285 [08:49<00:34,  2.13s/it]predicting train subjects:  95%|█████████▍| 270/285 [08:51<00:32,  2.14s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:53<00:30,  2.16s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:55<00:29,  2.26s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:58<00:27,  2.30s/it]predicting train subjects:  96%|█████████▌| 274/285 [09:00<00:25,  2.31s/it]predicting train subjects:  96%|█████████▋| 275/285 [09:02<00:22,  2.30s/it]predicting train subjects:  97%|█████████▋| 276/285 [09:05<00:20,  2.31s/it]predicting train subjects:  97%|█████████▋| 277/285 [09:07<00:18,  2.33s/it]predicting train subjects:  98%|█████████▊| 278/285 [09:09<00:15,  2.27s/it]predicting train subjects:  98%|█████████▊| 279/285 [09:12<00:13,  2.28s/it]predicting train subjects:  98%|█████████▊| 280/285 [09:14<00:11,  2.27s/it]predicting train subjects:  99%|█████████▊| 281/285 [09:16<00:08,  2.24s/it]predicting train subjects:  99%|█████████▉| 282/285 [09:18<00:06,  2.23s/it]predicting train subjects:  99%|█████████▉| 283/285 [09:21<00:04,  2.25s/it]predicting train subjects: 100%|█████████▉| 284/285 [09:23<00:02,  2.28s/it]predicting train subjects: 100%|██████████| 285/285 [09:25<00:00,  2.32s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<07:05,  1.50s/it]Loading train:   1%|          | 2/285 [00:03<07:29,  1.59s/it]Loading train:   1%|          | 3/285 [00:05<07:47,  1.66s/it]Loading train:   1%|▏         | 4/285 [00:07<08:30,  1.82s/it]Loading train:   2%|▏         | 5/285 [00:09<08:21,  1.79s/it]Loading train:   2%|▏         | 6/285 [00:11<08:38,  1.86s/it]Loading train:   2%|▏         | 7/285 [00:13<09:25,  2.04s/it]Loading train:   3%|▎         | 8/285 [00:15<09:34,  2.07s/it]Loading train:   3%|▎         | 9/285 [00:17<09:24,  2.04s/it]Loading train:   4%|▎         | 10/285 [00:19<09:23,  2.05s/it]Loading train:   4%|▍         | 11/285 [00:21<08:55,  1.96s/it]Loading train:   4%|▍         | 12/285 [00:22<08:14,  1.81s/it]Loading train:   5%|▍         | 13/285 [00:24<07:36,  1.68s/it]Loading train:   5%|▍         | 14/285 [00:25<07:19,  1.62s/it]Loading train:   5%|▌         | 15/285 [00:27<06:54,  1.53s/it]Loading train:   6%|▌         | 16/285 [00:28<06:39,  1.49s/it]Loading train:   6%|▌         | 17/285 [00:29<06:41,  1.50s/it]Loading train:   6%|▋         | 18/285 [00:31<06:43,  1.51s/it]Loading train:   7%|▋         | 19/285 [00:32<06:24,  1.45s/it]Loading train:   7%|▋         | 20/285 [00:34<06:30,  1.47s/it]Loading train:   7%|▋         | 21/285 [00:35<06:25,  1.46s/it]Loading train:   8%|▊         | 22/285 [00:37<06:24,  1.46s/it]Loading train:   8%|▊         | 23/285 [00:38<06:18,  1.45s/it]Loading train:   8%|▊         | 24/285 [00:39<06:00,  1.38s/it]Loading train:   9%|▉         | 25/285 [00:41<06:17,  1.45s/it]Loading train:   9%|▉         | 26/285 [00:43<06:45,  1.57s/it]Loading train:   9%|▉         | 27/285 [00:44<06:30,  1.51s/it]Loading train:  10%|▉         | 28/285 [00:46<06:32,  1.53s/it]Loading train:  10%|█         | 29/285 [00:48<06:51,  1.61s/it]Loading train:  11%|█         | 30/285 [00:49<06:21,  1.49s/it]Loading train:  11%|█         | 31/285 [00:50<06:05,  1.44s/it]Loading train:  11%|█         | 32/285 [00:52<06:01,  1.43s/it]Loading train:  12%|█▏        | 33/285 [00:53<05:42,  1.36s/it]Loading train:  12%|█▏        | 34/285 [00:54<05:25,  1.30s/it]Loading train:  12%|█▏        | 35/285 [00:55<05:23,  1.29s/it]Loading train:  13%|█▎        | 36/285 [00:57<05:29,  1.32s/it]Loading train:  13%|█▎        | 37/285 [00:58<05:09,  1.25s/it]Loading train:  13%|█▎        | 38/285 [00:59<05:28,  1.33s/it]Loading train:  14%|█▎        | 39/285 [01:01<05:39,  1.38s/it]Loading train:  14%|█▍        | 40/285 [01:02<05:52,  1.44s/it]Loading train:  14%|█▍        | 41/285 [01:04<06:15,  1.54s/it]Loading train:  15%|█▍        | 42/285 [01:06<06:16,  1.55s/it]Loading train:  15%|█▌        | 43/285 [01:07<06:01,  1.49s/it]Loading train:  15%|█▌        | 44/285 [01:08<05:59,  1.49s/it]Loading train:  16%|█▌        | 45/285 [01:10<05:57,  1.49s/it]Loading train:  16%|█▌        | 46/285 [01:11<06:01,  1.51s/it]Loading train:  16%|█▋        | 47/285 [01:13<05:34,  1.41s/it]Loading train:  17%|█▋        | 48/285 [01:14<05:30,  1.39s/it]Loading train:  17%|█▋        | 49/285 [01:15<05:17,  1.34s/it]Loading train:  18%|█▊        | 50/285 [01:17<05:17,  1.35s/it]Loading train:  18%|█▊        | 51/285 [01:18<05:20,  1.37s/it]Loading train:  18%|█▊        | 52/285 [01:19<05:24,  1.39s/it]Loading train:  19%|█▊        | 53/285 [01:21<05:17,  1.37s/it]Loading train:  19%|█▉        | 54/285 [01:22<04:56,  1.28s/it]Loading train:  19%|█▉        | 55/285 [01:23<04:52,  1.27s/it]Loading train:  20%|█▉        | 56/285 [01:25<05:00,  1.31s/it]Loading train:  20%|██        | 57/285 [01:26<04:50,  1.27s/it]Loading train:  20%|██        | 58/285 [01:27<04:42,  1.24s/it]Loading train:  21%|██        | 59/285 [01:28<04:46,  1.27s/it]Loading train:  21%|██        | 60/285 [01:30<04:58,  1.33s/it]Loading train:  21%|██▏       | 61/285 [01:31<04:51,  1.30s/it]Loading train:  22%|██▏       | 62/285 [01:32<04:39,  1.25s/it]Loading train:  22%|██▏       | 63/285 [01:33<04:41,  1.27s/it]Loading train:  22%|██▏       | 64/285 [01:35<05:06,  1.39s/it]Loading train:  23%|██▎       | 65/285 [01:37<05:37,  1.53s/it]Loading train:  23%|██▎       | 66/285 [01:39<06:21,  1.74s/it]Loading train:  24%|██▎       | 67/285 [01:41<05:58,  1.65s/it]Loading train:  24%|██▍       | 68/285 [01:42<05:35,  1.55s/it]Loading train:  24%|██▍       | 69/285 [01:43<05:28,  1.52s/it]Loading train:  25%|██▍       | 70/285 [01:45<05:30,  1.54s/it]Loading train:  25%|██▍       | 71/285 [01:46<05:02,  1.41s/it]Loading train:  25%|██▌       | 72/285 [01:47<05:02,  1.42s/it]Loading train:  26%|██▌       | 73/285 [01:49<04:59,  1.41s/it]Loading train:  26%|██▌       | 74/285 [01:50<05:00,  1.42s/it]Loading train:  26%|██▋       | 75/285 [01:51<04:40,  1.34s/it]Loading train:  27%|██▋       | 76/285 [01:53<04:25,  1.27s/it]Loading train:  27%|██▋       | 77/285 [01:54<04:42,  1.36s/it]Loading train:  27%|██▋       | 78/285 [01:55<04:24,  1.28s/it]Loading train:  28%|██▊       | 79/285 [01:56<04:25,  1.29s/it]Loading train:  28%|██▊       | 80/285 [01:58<04:24,  1.29s/it]Loading train:  28%|██▊       | 81/285 [01:59<04:20,  1.27s/it]Loading train:  29%|██▉       | 82/285 [02:00<04:26,  1.31s/it]Loading train:  29%|██▉       | 83/285 [02:02<04:23,  1.31s/it]Loading train:  29%|██▉       | 84/285 [02:03<04:31,  1.35s/it]Loading train:  30%|██▉       | 85/285 [02:05<04:40,  1.40s/it]Loading train:  30%|███       | 86/285 [02:06<04:53,  1.47s/it]Loading train:  31%|███       | 87/285 [02:08<04:47,  1.45s/it]Loading train:  31%|███       | 88/285 [02:10<05:07,  1.56s/it]Loading train:  31%|███       | 89/285 [02:11<05:17,  1.62s/it]Loading train:  32%|███▏      | 90/285 [02:12<04:48,  1.48s/it]Loading train:  32%|███▏      | 91/285 [02:14<04:37,  1.43s/it]Loading train:  32%|███▏      | 92/285 [02:15<04:26,  1.38s/it]Loading train:  33%|███▎      | 93/285 [02:16<04:26,  1.39s/it]Loading train:  33%|███▎      | 94/285 [02:18<04:13,  1.33s/it]Loading train:  33%|███▎      | 95/285 [02:19<04:10,  1.32s/it]Loading train:  34%|███▎      | 96/285 [02:20<04:15,  1.35s/it]Loading train:  34%|███▍      | 97/285 [02:22<04:25,  1.41s/it]Loading train:  34%|███▍      | 98/285 [02:23<04:30,  1.44s/it]Loading train:  35%|███▍      | 99/285 [02:25<04:20,  1.40s/it]Loading train:  35%|███▌      | 100/285 [02:26<04:14,  1.38s/it]Loading train:  35%|███▌      | 101/285 [02:28<04:23,  1.43s/it]Loading train:  36%|███▌      | 102/285 [02:30<04:51,  1.59s/it]Loading train:  36%|███▌      | 103/285 [02:32<05:22,  1.77s/it]Loading train:  36%|███▋      | 104/285 [02:33<05:11,  1.72s/it]Loading train:  37%|███▋      | 105/285 [02:35<04:40,  1.56s/it]Loading train:  37%|███▋      | 106/285 [02:36<04:18,  1.44s/it]Loading train:  38%|███▊      | 107/285 [02:38<04:34,  1.54s/it]Loading train:  38%|███▊      | 108/285 [02:39<04:40,  1.58s/it]Loading train:  38%|███▊      | 109/285 [02:41<04:33,  1.55s/it]Loading train:  39%|███▊      | 110/285 [02:42<04:32,  1.56s/it]Loading train:  39%|███▉      | 111/285 [02:44<04:18,  1.49s/it]Loading train:  39%|███▉      | 112/285 [02:45<04:14,  1.47s/it]Loading train:  40%|███▉      | 113/285 [02:46<03:59,  1.39s/it]Loading train:  40%|████      | 114/285 [02:48<03:56,  1.38s/it]Loading train:  40%|████      | 115/285 [02:49<03:43,  1.32s/it]Loading train:  41%|████      | 116/285 [02:50<03:41,  1.31s/it]Loading train:  41%|████      | 117/285 [02:51<03:34,  1.28s/it]Loading train:  41%|████▏     | 118/285 [02:53<03:37,  1.31s/it]Loading train:  42%|████▏     | 119/285 [02:54<03:35,  1.30s/it]Loading train:  42%|████▏     | 120/285 [02:55<03:34,  1.30s/it]Loading train:  42%|████▏     | 121/285 [02:57<04:05,  1.50s/it]Loading train:  43%|████▎     | 122/285 [02:59<04:05,  1.50s/it]Loading train:  43%|████▎     | 123/285 [03:00<04:15,  1.58s/it]Loading train:  44%|████▎     | 124/285 [03:02<04:12,  1.57s/it]Loading train:  44%|████▍     | 125/285 [03:03<03:51,  1.45s/it]Loading train:  44%|████▍     | 126/285 [03:04<03:41,  1.39s/it]Loading train:  45%|████▍     | 127/285 [03:06<03:34,  1.36s/it]Loading train:  45%|████▍     | 128/285 [03:07<03:41,  1.41s/it]Loading train:  45%|████▌     | 129/285 [03:09<03:48,  1.47s/it]Loading train:  46%|████▌     | 130/285 [03:10<03:32,  1.37s/it]Loading train:  46%|████▌     | 131/285 [03:11<03:33,  1.39s/it]Loading train:  46%|████▋     | 132/285 [03:13<03:28,  1.36s/it]Loading train:  47%|████▋     | 133/285 [03:14<03:19,  1.31s/it]Loading train:  47%|████▋     | 134/285 [03:15<03:10,  1.26s/it]Loading train:  47%|████▋     | 135/285 [03:16<03:12,  1.28s/it]Loading train:  48%|████▊     | 136/285 [03:18<03:15,  1.31s/it]Loading train:  48%|████▊     | 137/285 [03:19<03:21,  1.36s/it]Loading train:  48%|████▊     | 138/285 [03:20<03:17,  1.34s/it]Loading train:  49%|████▉     | 139/285 [03:22<03:12,  1.32s/it]Loading train:  49%|████▉     | 140/285 [03:23<03:05,  1.28s/it]Loading train:  49%|████▉     | 141/285 [03:24<03:14,  1.35s/it]Loading train:  50%|████▉     | 142/285 [03:26<03:28,  1.46s/it]Loading train:  50%|█████     | 143/285 [03:28<03:25,  1.44s/it]Loading train:  51%|█████     | 144/285 [03:29<03:06,  1.32s/it]Loading train:  51%|█████     | 145/285 [03:30<03:08,  1.35s/it]Loading train:  51%|█████     | 146/285 [03:31<02:50,  1.22s/it]Loading train:  52%|█████▏    | 147/285 [03:32<02:59,  1.30s/it]Loading train:  52%|█████▏    | 148/285 [03:34<03:00,  1.32s/it]Loading train:  52%|█████▏    | 149/285 [03:35<02:52,  1.27s/it]Loading train:  53%|█████▎    | 150/285 [03:36<02:50,  1.26s/it]Loading train:  53%|█████▎    | 151/285 [03:38<02:56,  1.32s/it]Loading train:  53%|█████▎    | 152/285 [03:39<02:54,  1.31s/it]Loading train:  54%|█████▎    | 153/285 [03:40<02:44,  1.24s/it]Loading train:  54%|█████▍    | 154/285 [03:41<02:45,  1.27s/it]Loading train:  54%|█████▍    | 155/285 [03:43<02:43,  1.26s/it]Loading train:  55%|█████▍    | 156/285 [03:44<02:46,  1.29s/it]Loading train:  55%|█████▌    | 157/285 [03:46<03:05,  1.45s/it]Loading train:  55%|█████▌    | 158/285 [03:48<03:17,  1.55s/it]Loading train:  56%|█████▌    | 159/285 [03:49<03:03,  1.46s/it]Loading train:  56%|█████▌    | 160/285 [03:50<03:10,  1.53s/it]Loading train:  56%|█████▋    | 161/285 [03:52<03:02,  1.47s/it]Loading train:  57%|█████▋    | 162/285 [03:53<02:48,  1.37s/it]Loading train:  57%|█████▋    | 163/285 [03:54<02:40,  1.32s/it]Loading train:  58%|█████▊    | 164/285 [03:56<02:42,  1.35s/it]Loading train:  58%|█████▊    | 165/285 [03:57<02:51,  1.43s/it]Loading train:  58%|█████▊    | 166/285 [03:59<02:53,  1.46s/it]Loading train:  59%|█████▊    | 167/285 [04:00<02:41,  1.36s/it]Loading train:  59%|█████▉    | 168/285 [04:01<02:24,  1.24s/it]Loading train:  59%|█████▉    | 169/285 [04:02<02:23,  1.23s/it]Loading train:  60%|█████▉    | 170/285 [04:04<02:34,  1.35s/it]Loading train:  60%|██████    | 171/285 [04:05<02:32,  1.34s/it]Loading train:  60%|██████    | 172/285 [04:06<02:27,  1.31s/it]Loading train:  61%|██████    | 173/285 [04:07<02:23,  1.28s/it]Loading train:  61%|██████    | 174/285 [04:09<02:19,  1.26s/it]Loading train:  61%|██████▏   | 175/285 [04:10<02:25,  1.32s/it]Loading train:  62%|██████▏   | 176/285 [04:11<02:24,  1.32s/it]Loading train:  62%|██████▏   | 177/285 [04:14<02:54,  1.62s/it]Loading train:  62%|██████▏   | 178/285 [04:16<03:02,  1.71s/it]Loading train:  63%|██████▎   | 179/285 [04:17<02:52,  1.63s/it]Loading train:  63%|██████▎   | 180/285 [04:19<02:53,  1.66s/it]Loading train:  64%|██████▎   | 181/285 [04:20<02:24,  1.39s/it]Loading train:  64%|██████▍   | 182/285 [04:20<02:01,  1.18s/it]Loading train:  64%|██████▍   | 183/285 [04:21<01:47,  1.05s/it]Loading train:  65%|██████▍   | 184/285 [04:22<01:37,  1.04it/s]Loading train:  65%|██████▍   | 185/285 [04:23<01:29,  1.12it/s]Loading train:  65%|██████▌   | 186/285 [04:23<01:22,  1.20it/s]Loading train:  66%|██████▌   | 187/285 [04:24<01:17,  1.26it/s]Loading train:  66%|██████▌   | 188/285 [04:25<01:19,  1.22it/s]Loading train:  66%|██████▋   | 189/285 [04:26<01:32,  1.04it/s]Loading train:  67%|██████▋   | 190/285 [04:27<01:40,  1.06s/it]Loading train:  67%|██████▋   | 191/285 [04:29<01:51,  1.19s/it]Loading train:  67%|██████▋   | 192/285 [04:31<02:10,  1.41s/it]Loading train:  68%|██████▊   | 193/285 [04:32<02:03,  1.35s/it]Loading train:  68%|██████▊   | 194/285 [04:33<01:59,  1.31s/it]Loading train:  68%|██████▊   | 195/285 [04:35<02:07,  1.42s/it]Loading train:  69%|██████▉   | 196/285 [04:36<02:06,  1.42s/it]Loading train:  69%|██████▉   | 197/285 [04:38<02:09,  1.47s/it]Loading train:  69%|██████▉   | 198/285 [04:40<02:15,  1.55s/it]Loading train:  70%|██████▉   | 199/285 [04:41<02:09,  1.51s/it]Loading train:  70%|███████   | 200/285 [04:43<02:16,  1.60s/it]Loading train:  71%|███████   | 201/285 [04:44<02:12,  1.57s/it]Loading train:  71%|███████   | 202/285 [04:46<02:13,  1.61s/it]Loading train:  71%|███████   | 203/285 [04:47<02:07,  1.55s/it]Loading train:  72%|███████▏  | 204/285 [04:49<02:01,  1.50s/it]Loading train:  72%|███████▏  | 205/285 [04:50<01:53,  1.42s/it]Loading train:  72%|███████▏  | 206/285 [04:51<01:38,  1.25s/it]Loading train:  73%|███████▎  | 207/285 [04:52<01:29,  1.14s/it]Loading train:  73%|███████▎  | 208/285 [04:53<01:21,  1.06s/it]Loading train:  73%|███████▎  | 209/285 [04:54<01:15,  1.00it/s]Loading train:  74%|███████▎  | 210/285 [04:54<01:10,  1.06it/s]Loading train:  74%|███████▍  | 211/285 [04:56<01:16,  1.03s/it]Loading train:  74%|███████▍  | 212/285 [04:56<01:11,  1.03it/s]Loading train:  75%|███████▍  | 213/285 [04:57<01:07,  1.06it/s]Loading train:  75%|███████▌  | 214/285 [04:58<01:07,  1.04it/s]Loading train:  75%|███████▌  | 215/285 [04:59<01:03,  1.09it/s]Loading train:  76%|███████▌  | 216/285 [05:00<01:00,  1.15it/s]Loading train:  76%|███████▌  | 217/285 [05:01<00:57,  1.18it/s]Loading train:  76%|███████▋  | 218/285 [05:02<00:56,  1.19it/s]Loading train:  77%|███████▋  | 219/285 [05:02<00:56,  1.18it/s]Loading train:  77%|███████▋  | 220/285 [05:03<00:53,  1.22it/s]Loading train:  78%|███████▊  | 221/285 [05:04<00:51,  1.25it/s]Loading train:  78%|███████▊  | 222/285 [05:05<00:50,  1.26it/s]Loading train:  78%|███████▊  | 223/285 [05:05<00:49,  1.25it/s]Loading train:  79%|███████▊  | 224/285 [05:06<00:48,  1.25it/s]Loading train:  79%|███████▉  | 225/285 [05:07<00:48,  1.24it/s]Loading train:  79%|███████▉  | 226/285 [05:08<00:47,  1.25it/s]Loading train:  80%|███████▉  | 227/285 [05:09<00:45,  1.27it/s]Loading train:  80%|████████  | 228/285 [05:09<00:44,  1.29it/s]Loading train:  80%|████████  | 229/285 [05:10<00:43,  1.28it/s]Loading train:  81%|████████  | 230/285 [05:11<00:44,  1.23it/s]Loading train:  81%|████████  | 231/285 [05:12<00:43,  1.24it/s]Loading train:  81%|████████▏ | 232/285 [05:13<00:47,  1.11it/s]Loading train:  82%|████████▏ | 233/285 [05:14<00:48,  1.08it/s]Loading train:  82%|████████▏ | 234/285 [05:15<00:46,  1.09it/s]Loading train:  82%|████████▏ | 235/285 [05:16<00:47,  1.06it/s]Loading train:  83%|████████▎ | 236/285 [05:17<00:46,  1.04it/s]Loading train:  83%|████████▎ | 237/285 [05:18<00:46,  1.04it/s]Loading train:  84%|████████▎ | 238/285 [05:19<00:45,  1.03it/s]Loading train:  84%|████████▍ | 239/285 [05:20<00:43,  1.05it/s]Loading train:  84%|████████▍ | 240/285 [05:21<00:43,  1.04it/s]Loading train:  85%|████████▍ | 241/285 [05:22<00:42,  1.05it/s]Loading train:  85%|████████▍ | 242/285 [05:23<00:40,  1.05it/s]Loading train:  85%|████████▌ | 243/285 [05:24<00:39,  1.07it/s]Loading train:  86%|████████▌ | 244/285 [05:25<00:39,  1.05it/s]Loading train:  86%|████████▌ | 245/285 [05:25<00:38,  1.04it/s]Loading train:  86%|████████▋ | 246/285 [05:26<00:36,  1.06it/s]Loading train:  87%|████████▋ | 247/285 [05:27<00:35,  1.06it/s]Loading train:  87%|████████▋ | 248/285 [05:28<00:34,  1.07it/s]Loading train:  87%|████████▋ | 249/285 [05:29<00:33,  1.07it/s]Loading train:  88%|████████▊ | 250/285 [05:30<00:31,  1.13it/s]Loading train:  88%|████████▊ | 251/285 [05:31<00:32,  1.05it/s]Loading train:  88%|████████▊ | 252/285 [05:32<00:28,  1.14it/s]Loading train:  89%|████████▉ | 253/285 [05:33<00:26,  1.20it/s]Loading train:  89%|████████▉ | 254/285 [05:33<00:24,  1.24it/s]Loading train:  89%|████████▉ | 255/285 [05:34<00:23,  1.29it/s]Loading train:  90%|████████▉ | 256/285 [05:35<00:22,  1.30it/s]Loading train:  90%|█████████ | 257/285 [05:35<00:21,  1.33it/s]Loading train:  91%|█████████ | 258/285 [05:36<00:19,  1.35it/s]Loading train:  91%|█████████ | 259/285 [05:37<00:18,  1.38it/s]Loading train:  91%|█████████ | 260/285 [05:38<00:18,  1.39it/s]Loading train:  92%|█████████▏| 261/285 [05:38<00:17,  1.41it/s]Loading train:  92%|█████████▏| 262/285 [05:39<00:16,  1.39it/s]Loading train:  92%|█████████▏| 263/285 [05:40<00:15,  1.42it/s]Loading train:  93%|█████████▎| 264/285 [05:40<00:15,  1.39it/s]Loading train:  93%|█████████▎| 265/285 [05:41<00:14,  1.42it/s]Loading train:  93%|█████████▎| 266/285 [05:42<00:13,  1.39it/s]Loading train:  94%|█████████▎| 267/285 [05:43<00:12,  1.39it/s]Loading train:  94%|█████████▍| 268/285 [05:44<00:13,  1.25it/s]Loading train:  94%|█████████▍| 269/285 [05:44<00:13,  1.23it/s]Loading train:  95%|█████████▍| 270/285 [05:45<00:12,  1.20it/s]Loading train:  95%|█████████▌| 271/285 [05:46<00:11,  1.18it/s]Loading train:  95%|█████████▌| 272/285 [05:47<00:11,  1.13it/s]Loading train:  96%|█████████▌| 273/285 [05:48<00:10,  1.12it/s]Loading train:  96%|█████████▌| 274/285 [05:49<00:09,  1.12it/s]Loading train:  96%|█████████▋| 275/285 [05:50<00:08,  1.13it/s]Loading train:  97%|█████████▋| 276/285 [05:51<00:07,  1.13it/s]Loading train:  97%|█████████▋| 277/285 [05:52<00:07,  1.11it/s]Loading train:  98%|█████████▊| 278/285 [05:53<00:06,  1.10it/s]Loading train:  98%|█████████▊| 279/285 [05:53<00:05,  1.10it/s]Loading train:  98%|█████████▊| 280/285 [05:54<00:04,  1.09it/s]Loading train:  99%|█████████▊| 281/285 [05:55<00:03,  1.08it/s]Loading train:  99%|█████████▉| 282/285 [05:56<00:02,  1.07it/s]Loading train:  99%|█████████▉| 283/285 [05:57<00:01,  1.05it/s]Loading train: 100%|█████████▉| 284/285 [05:58<00:00,  1.05it/s]Loading train: 100%|██████████| 285/285 [05:59<00:00,  1.07it/s]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 19/285 [00:00<00:01, 184.35it/s]concatenating: train:  16%|█▌        | 46/285 [00:00<00:01, 203.24it/s]concatenating: train:  26%|██▌       | 74/285 [00:00<00:00, 220.58it/s]concatenating: train:  36%|███▌      | 103/285 [00:00<00:00, 236.58it/s]concatenating: train:  46%|████▌     | 131/285 [00:00<00:00, 247.36it/s]concatenating: train:  56%|█████▌    | 159/285 [00:00<00:00, 255.40it/s]concatenating: train:  66%|██████▌   | 187/285 [00:00<00:00, 261.41it/s]concatenating: train:  76%|███████▋  | 218/285 [00:00<00:00, 273.33it/s]concatenating: train:  87%|████████▋ | 247/285 [00:00<00:00, 276.47it/s]concatenating: train:  98%|█████████▊| 278/285 [00:01<00:00, 284.51it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 275.18it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.24s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.23s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 119.32it/s]2019-07-06 21:38:07.233362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 21:38:07.233509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 21:38:07.233526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 21:38:07.233535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 21:38:07.234011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:09,  4.29it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:07,  4.98it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:07,  4.59it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:05,  5.68it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:06,  4.85it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.49it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  4.97it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:03,  6.26it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.15it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.74it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:04,  4.96it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:02,  6.03it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.49it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  5.31it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:02,  5.88it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  5.01it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  6.08it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  6.65it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.49it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  5.95it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  4.88it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  7.31it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 80, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 80, 52, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 80, 52, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 80, 52, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 80, 52, 20)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 80, 52, 20)   3620        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 80, 52, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 80, 52, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 80, 52, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 80, 52, 20)   3620        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 80, 52, 20)   80          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 80, 52, 20)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 80, 52, 20)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 80, 52, 10)   1810        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 80, 52, 10)   40          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 80, 52, 10)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 80, 52, 10)   910         activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 80, 52, 10)   40          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 80, 52, 10)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 40, 26, 10)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 40, 26, 10)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 40, 26, 20)   1820        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 40, 26, 20)   80          conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 40, 26, 20)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 40, 26, 20)   3620        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 40, 26, 20)   80          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 40, 26, 20)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 20, 13, 20)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 20, 13, 20)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 20, 13, 40)   7240        dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 20, 13, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 20, 13, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 13, 40)   14440       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 20, 13, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 20, 13, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 20, 13, 40)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 40, 26, 20)   3220        dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 40, 26, 40)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 40, 26, 20)   7220        concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 40, 26, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 40, 26, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 40, 26, 20)   3620        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 40, 26, 20)   80          conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 40, 26, 20)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 40, 26, 20)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 80, 52, 10)   810         dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 80, 52, 20)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 80, 52, 10)   1810        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 80, 52, 10)   40          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 80, 52, 10)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 80, 52, 10)   910         activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 80, 52, 10)   40          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 80, 52, 10)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 80, 52, 10)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 80, 52, 13)   143         dropout_8[0][0]                  
==================================================================================================
Total params: 56,053
Trainable params: 19,793
Non-trainable params: 36,260
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.48913484e-02 3.19509754e-02 7.49897764e-02 9.32025064e-03
 2.70904632e-02 7.06417031e-03 8.46180096e-02 1.12618024e-01
 8.60108482e-02 1.32459736e-02 2.94100802e-01 1.93843398e-01
 2.55960049e-04]
Train on 10843 samples, validate on 104 samples
Epoch 1/300
 - 18s - loss: 701.9134 - acc: 0.1338 - mDice: 0.0169 - val_loss: 516.6811 - val_acc: 0.2045 - val_mDice: 0.0198

Epoch 00001: val_mDice improved from -inf to 0.01984, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 9s - loss: 266.3984 - acc: 0.5819 - mDice: 0.0148 - val_loss: 131.9879 - val_acc: 0.9032 - val_mDice: 0.0110

Epoch 00002: val_mDice did not improve from 0.01984
Epoch 3/300
 - 8s - loss: 116.8420 - acc: 0.8385 - mDice: 0.0135 - val_loss: 78.3833 - val_acc: 0.9031 - val_mDice: 0.0127

Epoch 00003: val_mDice did not improve from 0.01984
Epoch 4/300
 - 9s - loss: 66.2728 - acc: 0.8625 - mDice: 0.0125 - val_loss: 49.3755 - val_acc: 0.9034 - val_mDice: 0.0109

Epoch 00004: val_mDice did not improve from 0.01984
Epoch 5/300
 - 9s - loss: 45.6961 - acc: 0.8699 - mDice: 0.0120 - val_loss: 36.3717 - val_acc: 0.9034 - val_mDice: 0.0109

Epoch 00005: val_mDice did not improve from 0.01984
Epoch 6/300
 - 9s - loss: 35.2259 - acc: 0.8727 - mDice: 0.0115 - val_loss: 30.0940 - val_acc: 0.9034 - val_mDice: 0.0093

Epoch 00006: val_mDice did not improve from 0.01984
Epoch 7/300
 - 8s - loss: 29.1344 - acc: 0.8737 - mDice: 0.0112 - val_loss: 28.5008 - val_acc: 0.9034 - val_mDice: 0.0077

Epoch 00007: val_mDice did not improve from 0.01984
Epoch 8/300
 - 8s - loss: 25.2206 - acc: 0.8742 - mDice: 0.0109 - val_loss: 23.0070 - val_acc: 0.9034 - val_mDice: 0.0063

Epoch 00008: val_mDice did not improve from 0.01984
Epoch 9/300
 - 8s - loss: 22.5632 - acc: 0.8745 - mDice: 0.0107 - val_loss: 17.0982 - val_acc: 0.9034 - val_mDice: 0.0057

Epoch 00009: val_mDice did not improve from 0.01984
Epoch 10/300
 - 8s - loss: 20.6623 - acc: 0.8747 - mDice: 0.0106 - val_loss: 14.1410 - val_acc: 0.9034 - val_mDice: 0.0054

Epoch 00010: val_mDice did not improve from 0.01984
Epoch 11/300
 - 8s - loss: 19.2419 - acc: 0.8747 - mDice: 0.0105 - val_loss: 13.2538 - val_acc: 0.9034 - val_mDice: 0.0054

Epoch 00011: val_mDice did not improve from 0.01984
Epoch 12/300
 - 8s - loss: 18.1586 - acc: 0.8748 - mDice: 0.0104 - val_loss: 12.7018 - val_acc: 0.9034 - val_mDice: 0.0054

Epoch 00012: val_mDice did not improve from 0.01984
Epoch 13/300
 - 9s - loss: 17.3133 - acc: 0.8748 - mDice: 0.0104 - val_loss: 12.2419 - val_acc: 0.9034 - val_mDice: 0.0054

Epoch 00013: val_mDice did not improve from 0.01984
Epoch 14/300
 - 9s - loss: 16.6247 - acc: 0.8748 - mDice: 0.0104 - val_loss: 12.1132 - val_acc: 0.9034 - val_mDice: 0.0053

Epoch 00014: val_mDice did not improve from 0.01984
Epoch 15/300
 - 9s - loss: 16.0624 - acc: 0.8748 - mDice: 0.0103 - val_loss: 12.0328 - val_acc: 0.9034 - val_mDice: 0.0051

Epoch 00015: val_mDice did not improve from 0.01984
Epoch 16/300
 - 8s - loss: 15.5891 - acc: 0.8748 - mDice: 0.0103 - val_loss: 11.9389 - val_acc: 0.9034 - val_mDice: 0.0050

Epoch 00016: val_mDice did not improve from 0.01984
Epoch 17/300
 - 8s - loss: 15.1949 - acc: 0.8748 - mDice: 0.0104 - val_loss: 11.7634 - val_acc: 0.9034 - val_mDice: 0.0053

Epoch 00017: val_mDice did not improve from 0.01984
Epoch 18/300
 - 8s - loss: 14.8436 - acc: 0.8748 - mDice: 0.0104 - val_loss: 11.5482 - val_acc: 0.9034 - val_mDice: 0.0059

Epoch 00018: val_mDice did not improve from 0.01984
Epoch 19/300
 - 8s - loss: 14.5480 - acc: 0.8748 - mDice: 0.0105 - val_loss: 11.3040 - val_acc: 0.9034 - val_mDice: 0.0064

Epoch 00019: val_mDice did not improve from 0.01984
Epoch 20/300
 - 8s - loss: 14.2955 - acc: 0.8748 - mDice: 0.0106 - val_loss: 11.0973 - val_acc: 0.9034 - val_mDice: 0.0077

Epoch 00020: val_mDice did not improve from 0.01984
Epoch 21/300
 - 8s - loss: 14.0670 - acc: 0.8748 - mDice: 0.0106 - val_loss: 10.9525 - val_acc: 0.9034 - val_mDice: 0.0082

Epoch 00021: val_mDice did not improve from 0.01984
Epoch 22/300
 - 8s - loss: 13.8651 - acc: 0.8748 - mDice: 0.0107 - val_loss: 10.7060 - val_acc: 0.9034 - val_mDice: 0.0085

Epoch 00022: val_mDice did not improve from 0.01984
Epoch 23/300
 - 9s - loss: 13.6654 - acc: 0.8748 - mDice: 0.0108 - val_loss: 10.4035 - val_acc: 0.9034 - val_mDice: 0.0101

Epoch 00023: val_mDice did not improve from 0.01984
Epoch 24/300
 - 9s - loss: 13.4984 - acc: 0.8748 - mDice: 0.0110 - val_loss: 10.3133 - val_acc: 0.9034 - val_mDice: 0.0100

Epoch 00024: val_mDice did not improve from 0.01984
Epoch 25/300
 - 8s - loss: 13.3311 - acc: 0.8748 - mDice: 0.0111 - val_loss: 10.2412 - val_acc: 0.9034 - val_mDice: 0.0104

Epoch 00025: val_mDice did not improve from 0.01984
Epoch 26/300
 - 8s - loss: 13.1797 - acc: 0.8748 - mDice: 0.0113 - val_loss: 10.1808 - val_acc: 0.9034 - val_mDice: 0.0105

Epoch 00026: val_mDice did not improve from 0.01984
Epoch 27/300
 - 8s - loss: 13.0267 - acc: 0.8748 - mDice: 0.0117 - val_loss: 10.9239 - val_acc: 0.9034 - val_mDice: 0.0104

Epoch 00027: val_mDice did not improve from 0.01984
Epoch 28/300
 - 8s - loss: 12.8243 - acc: 0.8748 - mDice: 0.0126 - val_loss: 10.9297 - val_acc: 0.9034 - val_mDice: 0.0106

Epoch 00028: val_mDice did not improve from 0.01984
Epoch 29/300
 - 8s - loss: 12.5986 - acc: 0.8748 - mDice: 0.0139 - val_loss: 9.9027 - val_acc: 0.9034 - val_mDice: 0.0113

Epoch 00029: val_mDice did not improve from 0.01984
Epoch 30/300
 - 8s - loss: 12.3889 - acc: 0.8748 - mDice: 0.0147 - val_loss: 9.7677 - val_acc: 0.9034 - val_mDice: 0.0119

Epoch 00030: val_mDice did not improve from 0.01984
Epoch 31/300
 - 8s - loss: 12.1719 - acc: 0.8748 - mDice: 0.0155 - val_loss: 9.6704 - val_acc: 0.9034 - val_mDice: 0.0129

predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.38s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.13s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.91s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:32,  1.38s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:04,  1.50s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:06,  1.51s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:32,  1.61s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<07:13,  1.55s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:39,  1.65s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<08:08,  1.76s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<08:17,  1.80s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<07:57,  1.73s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:16,  1.80s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:27,  1.85s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:35,  1.89s/it]predicting train subjects:   5%|▍         | 13/285 [00:23<08:36,  1.90s/it]predicting train subjects:   5%|▍         | 14/285 [00:25<08:36,  1.91s/it]predicting train subjects:   5%|▌         | 15/285 [00:27<08:42,  1.93s/it]predicting train subjects:   6%|▌         | 16/285 [00:28<08:37,  1.92s/it]predicting train subjects:   6%|▌         | 17/285 [00:30<08:35,  1.92s/it]predicting train subjects:   6%|▋         | 18/285 [00:32<08:42,  1.96s/it]predicting train subjects:   7%|▋         | 19/285 [00:34<08:39,  1.95s/it]predicting train subjects:   7%|▋         | 20/285 [00:37<08:54,  2.02s/it]predicting train subjects:   7%|▋         | 21/285 [00:39<08:58,  2.04s/it]predicting train subjects:   8%|▊         | 22/285 [00:41<08:49,  2.01s/it]predicting train subjects:   8%|▊         | 23/285 [00:43<08:44,  2.00s/it]predicting train subjects:   8%|▊         | 24/285 [00:44<08:37,  1.98s/it]predicting train subjects:   9%|▉         | 25/285 [00:46<08:37,  1.99s/it]predicting train subjects:   9%|▉         | 26/285 [00:48<08:29,  1.97s/it]predicting train subjects:   9%|▉         | 27/285 [00:50<08:31,  1.98s/it]predicting train subjects:  10%|▉         | 28/285 [00:52<08:17,  1.94s/it]predicting train subjects:  10%|█         | 29/285 [00:54<08:09,  1.91s/it]predicting train subjects:  11%|█         | 30/285 [00:56<08:00,  1.89s/it]predicting train subjects:  11%|█         | 31/285 [00:58<07:46,  1.84s/it]predicting train subjects:  11%|█         | 32/285 [00:59<07:40,  1.82s/it]predicting train subjects:  12%|█▏        | 33/285 [01:01<07:39,  1.82s/it]predicting train subjects:  12%|█▏        | 34/285 [01:03<07:39,  1.83s/it]predicting train subjects:  12%|█▏        | 35/285 [01:05<07:36,  1.83s/it]predicting train subjects:  13%|█▎        | 36/285 [01:07<07:32,  1.82s/it]predicting train subjects:  13%|█▎        | 37/285 [01:09<07:35,  1.84s/it]predicting train subjects:  13%|█▎        | 38/285 [01:10<07:32,  1.83s/it]predicting train subjects:  14%|█▎        | 39/285 [01:12<07:27,  1.82s/it]predicting train subjects:  14%|█▍        | 40/285 [01:14<07:25,  1.82s/it]predicting train subjects:  14%|█▍        | 41/285 [01:16<07:30,  1.85s/it]predicting train subjects:  15%|█▍        | 42/285 [01:18<07:38,  1.89s/it]predicting train subjects:  15%|█▌        | 43/285 [01:20<07:30,  1.86s/it]predicting train subjects:  15%|█▌        | 44/285 [01:22<07:25,  1.85s/it]predicting train subjects:  16%|█▌        | 45/285 [01:23<07:24,  1.85s/it]predicting train subjects:  16%|█▌        | 46/285 [01:25<07:05,  1.78s/it]predicting train subjects:  16%|█▋        | 47/285 [01:27<06:48,  1.72s/it]predicting train subjects:  17%|█▋        | 48/285 [01:28<06:39,  1.69s/it]predicting train subjects:  17%|█▋        | 49/285 [01:30<06:37,  1.69s/it]predicting train subjects:  18%|█▊        | 50/285 [01:32<06:35,  1.68s/it]predicting train subjects:  18%|█▊        | 51/285 [01:33<06:27,  1.66s/it]predicting train subjects:  18%|█▊        | 52/285 [01:35<06:26,  1.66s/it]predicting train subjects:  19%|█▊        | 53/285 [01:36<06:23,  1.65s/it]predicting train subjects:  19%|█▉        | 54/285 [01:38<06:17,  1.63s/it]predicting train subjects:  19%|█▉        | 55/285 [01:40<06:13,  1.62s/it]predicting train subjects:  20%|█▉        | 56/285 [01:41<06:15,  1.64s/it]predicting train subjects:  20%|██        | 57/285 [01:43<06:15,  1.65s/it]predicting train subjects:  20%|██        | 58/285 [01:45<06:13,  1.65s/it]predicting train subjects:  21%|██        | 59/285 [01:46<06:10,  1.64s/it]predicting train subjects:  21%|██        | 60/285 [01:48<06:05,  1.62s/it]predicting train subjects:  21%|██▏       | 61/285 [01:49<06:04,  1.63s/it]predicting train subjects:  22%|██▏       | 62/285 [01:51<06:03,  1.63s/it]predicting train subjects:  22%|██▏       | 63/285 [01:53<05:59,  1.62s/it]predicting train subjects:  22%|██▏       | 64/285 [01:54<05:54,  1.60s/it]predicting train subjects:  23%|██▎       | 65/285 [01:56<06:06,  1.67s/it]predicting train subjects:  23%|██▎       | 66/285 [01:58<06:16,  1.72s/it]predicting train subjects:  24%|██▎       | 67/285 [01:59<06:04,  1.67s/it]predicting train subjects:  24%|██▍       | 68/285 [02:01<06:00,  1.66s/it]predicting train subjects:  24%|██▍       | 69/285 [02:03<05:58,  1.66s/it]predicting train subjects:  25%|██▍       | 70/285 [02:04<05:56,  1.66s/it]predicting train subjects:  25%|██▍       | 71/285 [02:06<05:46,  1.62s/it]predicting train subjects:  25%|██▌       | 72/285 [02:08<05:41,  1.60s/it]predicting train subjects:  26%|██▌       | 73/285 [02:09<05:44,  1.63s/it]predicting train subjects:  26%|██▌       | 74/285 [02:11<05:37,  1.60s/it]predicting train subjects:  26%|██▋       | 75/285 [02:12<05:39,  1.62s/it]predicting train subjects:  27%|██▋       | 76/285 [02:14<05:36,  1.61s/it]predicting train subjects:  27%|██▋       | 77/285 [02:16<05:28,  1.58s/it]predicting train subjects:  27%|██▋       | 78/285 [02:17<05:30,  1.60s/it]predicting train subjects:  28%|██▊       | 79/285 [02:19<05:33,  1.62s/it]predicting train subjects:  28%|██▊       | 80/285 [02:21<05:38,  1.65s/it]predicting train subjects:  28%|██▊       | 81/285 [02:22<05:52,  1.73s/it]predicting train subjects:  29%|██▉       | 82/285 [02:24<05:48,  1.71s/it]predicting train subjects:  29%|██▉       | 83/285 [02:26<05:39,  1.68s/it]predicting train subjects:  29%|██▉       | 84/285 [02:28<05:44,  1.71s/it]predicting train subjects:  30%|██▉       | 85/285 [02:29<05:49,  1.75s/it]predicting train subjects:  30%|███       | 86/285 [02:31<05:58,  1.80s/it]predicting train subjects:  31%|███       | 87/285 [02:33<05:55,  1.79s/it]predicting train subjects:  31%|███       | 88/285 [02:35<05:56,  1.81s/it]predicting train subjects:  31%|███       | 89/285 [02:37<05:52,  1.80s/it]predicting train subjects:  32%|███▏      | 90/285 [02:38<05:51,  1.81s/it]predicting train subjects:  32%|███▏      | 91/285 [02:40<05:47,  1.79s/it]predicting train subjects:  32%|███▏      | 92/285 [02:42<05:52,  1.83s/it]predicting train subjects:  33%|███▎      | 93/285 [02:44<05:50,  1.82s/it]predicting train subjects:  33%|███▎      | 94/285 [02:46<05:44,  1.81s/it]predicting train subjects:  33%|███▎      | 95/285 [02:48<05:43,  1.81s/it]predicting train subjects:  34%|███▎      | 96/285 [02:49<05:45,  1.83s/it]predicting train subjects:  34%|███▍      | 97/285 [02:51<05:45,  1.84s/it]predicting train subjects:  34%|███▍      | 98/285 [02:53<05:52,  1.89s/it]predicting train subjects:  35%|███▍      | 99/285 [02:55<05:47,  1.87s/it]predicting train subjects:  35%|███▌      | 100/285 [02:57<05:41,  1.85s/it]predicting train subjects:  35%|███▌      | 101/285 [02:59<05:35,  1.82s/it]predicting train subjects:  36%|███▌      | 102/285 [03:01<05:35,  1.83s/it]predicting train subjects:  36%|███▌      | 103/285 [03:02<05:27,  1.80s/it]predicting train subjects:  36%|███▋      | 104/285 [03:04<05:24,  1.79s/it]predicting train subjects:  37%|███▋      | 105/285 [03:06<05:31,  1.84s/it]predicting train subjects:  37%|███▋      | 106/285 [03:08<05:29,  1.84s/it]predicting train subjects:  38%|███▊      | 107/285 [03:10<05:27,  1.84s/it]predicting train subjects:  38%|███▊      | 108/285 [03:11<05:20,  1.81s/it]predicting train subjects:  38%|███▊      | 109/285 [03:13<05:19,  1.82s/it]predicting train subjects:  39%|███▊      | 110/285 [03:15<05:18,  1.82s/it]predicting train subjects:  39%|███▉      | 111/285 [03:17<05:20,  1.84s/it]predicting train subjects:  39%|███▉      | 112/285 [03:19<05:18,  1.84s/it]predicting train subjects:  40%|███▉      | 113/285 [03:21<05:15,  1.84s/it]predicting train subjects:  40%|████      | 114/285 [03:23<05:17,  1.86s/it]predicting train subjects:  40%|████      | 115/285 [03:24<05:12,  1.84s/it]predicting train subjects:  41%|████      | 116/285 [03:26<05:10,  1.84s/it]predicting train subjects:  41%|████      | 117/285 [03:28<05:08,  1.84s/it]predicting train subjects:  41%|████▏     | 118/285 [03:30<05:08,  1.85s/it]predicting train subjects:  42%|████▏     | 119/285 [03:32<05:07,  1.85s/it]predicting train subjects:  42%|████▏     | 120/285 [03:34<05:04,  1.85s/it]predicting train subjects:  42%|████▏     | 121/285 [03:35<04:54,  1.80s/it]predicting train subjects:  43%|████▎     | 122/285 [03:37<04:41,  1.73s/it]predicting train subjects:  43%|████▎     | 123/285 [03:38<04:25,  1.64s/it]predicting train subjects:  44%|████▎     | 124/285 [03:40<04:25,  1.65s/it]predicting train subjects:  44%|████▍     | 125/285 [03:42<04:22,  1.64s/it]predicting train subjects:  44%|████▍     | 126/285 [03:43<04:21,  1.64s/it]predicting train subjects:  45%|████▍     | 127/285 [03:45<04:20,  1.65s/it]predicting train subjects:  45%|████▍     | 128/285 [03:46<04:15,  1.63s/it]predicting train subjects:  45%|████▌     | 129/285 [03:48<04:11,  1.61s/it]predicting train subjects:  46%|████▌     | 130/285 [03:50<04:16,  1.65s/it]predicting train subjects:  46%|████▌     | 131/285 [03:51<04:14,  1.65s/it]predicting train subjects:  46%|████▋     | 132/285 [03:53<04:10,  1.63s/it]predicting train subjects:  47%|████▋     | 133/285 [03:55<04:10,  1.65s/it]predicting train subjects:  47%|████▋     | 134/285 [03:56<04:06,  1.64s/it]predicting train subjects:  47%|████▋     | 135/285 [03:58<04:08,  1.66s/it]predicting train subjects:  48%|████▊     | 136/285 [04:00<04:05,  1.65s/it]predicting train subjects:  48%|████▊     | 137/285 [04:01<04:01,  1.63s/it]predicting train subjects:  48%|████▊     | 138/285 [04:03<04:01,  1.64s/it]predicting train subjects:  49%|████▉     | 139/285 [04:05<04:01,  1.65s/it]predicting train subjects:  49%|████▉     | 140/285 [04:06<03:56,  1.63s/it]predicting train subjects:  49%|████▉     | 141/285 [04:08<03:51,  1.60s/it]predicting train subjects:  50%|████▉     | 142/285 [04:09<03:43,  1.56s/it]predicting train subjects:  50%|█████     | 143/285 [04:11<03:36,  1.53s/it]predicting train subjects:  51%|█████     | 144/285 [04:12<03:33,  1.51s/it]predicting train subjects:  51%|█████     | 145/285 [04:14<03:31,  1.51s/it]predicting train subjects:  51%|█████     | 146/285 [04:15<03:26,  1.48s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:16<03:24,  1.48s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:18<03:23,  1.48s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:19<03:23,  1.50s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:21<03:22,  1.50s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:22<03:18,  1.48s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:24<03:16,  1.48s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:25<03:16,  1.49s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:27<03:17,  1.51s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:28<03:16,  1.51s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:30<03:11,  1.48s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:31<03:11,  1.49s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:33<03:11,  1.51s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:34<03:11,  1.52s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:36<03:06,  1.49s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:37<03:00,  1.45s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:39<02:59,  1.46s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:40<03:02,  1.49s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:42<03:01,  1.50s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:43<03:01,  1.51s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:45<03:00,  1.52s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:46<02:59,  1.52s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:48<02:57,  1.52s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:49<02:53,  1.50s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:51<02:54,  1.52s/it]predicting train subjects:  60%|██████    | 171/285 [04:52<02:51,  1.51s/it]predicting train subjects:  60%|██████    | 172/285 [04:54<02:50,  1.51s/it]predicting train subjects:  61%|██████    | 173/285 [04:56<02:50,  1.52s/it]predicting train subjects:  61%|██████    | 174/285 [04:57<02:48,  1.52s/it]predicting train subjects:  61%|██████▏   | 175/285 [04:59<02:46,  1.52s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:00<02:44,  1.51s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:02<02:53,  1.61s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:03<02:46,  1.56s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:05<02:40,  1.51s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:06<02:36,  1.49s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:08<02:34,  1.49s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:09<02:31,  1.47s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:11<02:29,  1.46s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:12<02:31,  1.50s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:14<02:27,  1.48s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:15<02:24,  1.46s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:16<02:24,  1.47s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:18<02:22,  1.47s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:19<02:18,  1.45s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:21<02:18,  1.46s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:22<02:15,  1.44s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:24<02:12,  1.43s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:25<02:09,  1.41s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:26<02:08,  1.41s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:28<02:08,  1.43s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:30<02:15,  1.52s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:31<02:21,  1.60s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:33<02:23,  1.65s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:35<02:22,  1.65s/it]predicting train subjects:  70%|███████   | 200/285 [05:37<02:22,  1.68s/it]predicting train subjects:  71%|███████   | 201/285 [05:38<02:20,  1.67s/it]predicting train subjects:  71%|███████   | 202/285 [05:40<02:18,  1.67s/it]predicting train subjects:  71%|███████   | 203/285 [05:42<02:17,  1.68s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:43<02:15,  1.68s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:45<02:15,  1.69s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:47<02:16,  1.73s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:48<02:13,  1.72s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:50<02:12,  1.73s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:52<02:10,  1.72s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:54<02:07,  1.70s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:55<02:06,  1.72s/it]predicting train subjects:  74%|███████▍  | 212/285 [05:57<02:04,  1.71s/it]predicting train subjects:  75%|███████▍  | 213/285 [05:59<02:01,  1.69s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:00<01:53,  1.60s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:01<01:48,  1.55s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:03<01:42,  1.49s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:04<01:39,  1.46s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:06<01:37,  1.46s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:07<01:37,  1.47s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:09<01:36,  1.48s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:10<01:34,  1.48s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:12<01:34,  1.51s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:13<01:35,  1.53s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:15<01:32,  1.51s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:16<01:30,  1.51s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:18<01:28,  1.50s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:19<01:26,  1.49s/it]predicting train subjects:  80%|████████  | 228/285 [06:21<01:25,  1.49s/it]predicting train subjects:  80%|████████  | 229/285 [06:22<01:22,  1.47s/it]predicting train subjects:  81%|████████  | 230/285 [06:24<01:21,  1.48s/it]predicting train subjects:  81%|████████  | 231/285 [06:25<01:19,  1.48s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:27<01:24,  1.59s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:29<01:26,  1.66s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:31<01:27,  1.72s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:33<01:27,  1.76s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:34<01:27,  1.79s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:36<01:27,  1.82s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:38<01:26,  1.84s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:40<01:25,  1.86s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:42<01:23,  1.85s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:44<01:21,  1.86s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:46<01:20,  1.88s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:48<01:19,  1.90s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:49<01:17,  1.88s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:51<01:15,  1.88s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:53<01:13,  1.88s/it]predicting train subjects:  87%|████████▋ | 247/285 [06:55<01:12,  1.92s/it]predicting train subjects:  87%|████████▋ | 248/285 [06:57<01:09,  1.88s/it]predicting train subjects:  87%|████████▋ | 249/285 [06:59<01:07,  1.87s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:00<00:59,  1.71s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:02<00:54,  1.60s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:03<00:49,  1.51s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:04<00:47,  1.49s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:06<00:45,  1.47s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:07<00:43,  1.46s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:09<00:41,  1.43s/it]predicting train subjects:  90%|█████████ | 257/285 [07:10<00:40,  1.44s/it]predicting train subjects:  91%|█████████ | 258/285 [07:11<00:38,  1.42s/it]predicting train subjects:  91%|█████████ | 259/285 [07:13<00:36,  1.42s/it]predicting train subjects:  91%|█████████ | 260/285 [07:14<00:35,  1.41s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:15<00:32,  1.37s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:17<00:31,  1.38s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:18<00:30,  1.39s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:20<00:29,  1.40s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:21<00:27,  1.38s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:22<00:25,  1.36s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:24<00:24,  1.37s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:26<00:25,  1.52s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:27<00:25,  1.62s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:29<00:25,  1.68s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:31<00:24,  1.73s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:33<00:22,  1.77s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:35<00:21,  1.82s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:37<00:20,  1.82s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:39<00:18,  1.83s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:40<00:16,  1.85s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:42<00:14,  1.82s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:44<00:12,  1.81s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:46<00:10,  1.82s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:48<00:09,  1.83s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:50<00:07,  1.84s/it]predicting train subjects:  99%|█████████▉| 282/285 [07:52<00:05,  1.87s/it]predicting train subjects:  99%|█████████▉| 283/285 [07:53<00:03,  1.85s/it]predicting train subjects: 100%|█████████▉| 284/285 [07:55<00:01,  1.85s/it]predicting train subjects: 100%|██████████| 285/285 [07:57<00:00,  1.85s/it]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:05,  1.29s/it]Loading train:   1%|          | 2/285 [00:02<06:18,  1.34s/it]Loading train:   1%|          | 3/285 [00:04<06:13,  1.33s/it]Loading train:   1%|▏         | 4/285 [00:05<06:33,  1.40s/it]Loading train:   2%|▏         | 5/285 [00:06<06:18,  1.35s/it]Loading train:   2%|▏         | 6/285 [00:08<06:28,  1.39s/it]Loading train:   2%|▏         | 7/285 [00:09<06:42,  1.45s/it]Loading train:   3%|▎         | 8/285 [00:11<06:53,  1.49s/it]Loading train:   3%|▎         | 9/285 [00:12<06:41,  1.46s/it]Loading train:   4%|▎         | 10/285 [00:14<06:34,  1.43s/it]Loading train:   4%|▍         | 11/285 [00:15<06:44,  1.48s/it]Loading train:   4%|▍         | 12/285 [00:17<06:58,  1.53s/it]Loading train:   5%|▍         | 13/285 [00:18<06:33,  1.45s/it]Loading train:   5%|▍         | 14/285 [00:20<06:17,  1.39s/it]Loading train:   5%|▌         | 15/285 [00:21<06:29,  1.44s/it]Loading train:   6%|▌         | 16/285 [00:22<06:18,  1.41s/it]Loading train:   6%|▌         | 17/285 [00:24<06:03,  1.36s/it]Loading train:   6%|▋         | 18/285 [00:25<06:26,  1.45s/it]Loading train:   7%|▋         | 19/285 [00:27<06:32,  1.48s/it]Loading train:   7%|▋         | 20/285 [00:28<06:39,  1.51s/it]Loading train:   7%|▋         | 21/285 [00:30<06:39,  1.51s/it]Loading train:   8%|▊         | 22/285 [00:31<06:30,  1.49s/it]Loading train:   8%|▊         | 23/285 [00:33<06:36,  1.51s/it]Loading train:   8%|▊         | 24/285 [00:34<06:18,  1.45s/it]Loading train:   9%|▉         | 25/285 [00:35<05:54,  1.36s/it]Loading train:   9%|▉         | 26/285 [00:37<05:38,  1.31s/it]Loading train:   9%|▉         | 27/285 [00:38<06:05,  1.41s/it]Loading train:  10%|▉         | 28/285 [00:40<06:25,  1.50s/it]Loading train:  10%|█         | 29/285 [00:41<06:14,  1.46s/it]Loading train:  11%|█         | 30/285 [00:43<05:51,  1.38s/it]Loading train:  11%|█         | 31/285 [00:44<05:57,  1.41s/it]Loading train:  11%|█         | 32/285 [00:45<05:52,  1.39s/it]Loading train:  12%|█▏        | 33/285 [00:47<05:46,  1.38s/it]Loading train:  12%|█▏        | 34/285 [00:48<05:55,  1.42s/it]Loading train:  12%|█▏        | 35/285 [00:50<05:56,  1.43s/it]Loading train:  13%|█▎        | 36/285 [00:51<05:33,  1.34s/it]Loading train:  13%|█▎        | 37/285 [00:52<05:27,  1.32s/it]Loading train:  13%|█▎        | 38/285 [00:53<05:29,  1.33s/it]Loading train:  14%|█▎        | 39/285 [00:55<05:18,  1.29s/it]Loading train:  14%|█▍        | 40/285 [00:56<05:03,  1.24s/it]Loading train:  14%|█▍        | 41/285 [00:57<05:17,  1.30s/it]Loading train:  15%|█▍        | 42/285 [00:59<05:39,  1.40s/it]Loading train:  15%|█▌        | 43/285 [01:00<05:19,  1.32s/it]Loading train:  15%|█▌        | 44/285 [01:01<05:13,  1.30s/it]Loading train:  16%|█▌        | 45/285 [01:03<05:15,  1.31s/it]Loading train:  16%|█▌        | 46/285 [01:04<05:13,  1.31s/it]Loading train:  16%|█▋        | 47/285 [01:05<04:54,  1.24s/it]Loading train:  17%|█▋        | 48/285 [01:06<05:07,  1.30s/it]Loading train:  17%|█▋        | 49/285 [01:08<04:58,  1.27s/it]Loading train:  18%|█▊        | 50/285 [01:09<04:47,  1.23s/it]Loading train:  18%|█▊        | 51/285 [01:10<04:40,  1.20s/it]Loading train:  18%|█▊        | 52/285 [01:11<04:38,  1.19s/it]Loading train:  19%|█▊        | 53/285 [01:12<04:28,  1.16s/it]Loading train:  19%|█▉        | 54/285 [01:13<04:18,  1.12s/it]Loading train:  19%|█▉        | 55/285 [01:14<04:10,  1.09s/it]Loading train:  20%|█▉        | 56/285 [01:15<04:23,  1.15s/it]Loading train:  20%|██        | 57/285 [01:17<04:51,  1.28s/it]Loading train:  20%|██        | 58/285 [01:18<04:46,  1.26s/it]Loading train:  21%|██        | 59/285 [01:19<04:31,  1.20s/it]Loading train:  21%|██        | 60/285 [01:20<04:22,  1.17s/it]Loading train:  21%|██▏       | 61/285 [01:21<04:17,  1.15s/it]Loading train:  22%|██▏       | 62/285 [01:23<04:25,  1.19s/it]Loading train:  22%|██▏       | 63/285 [01:24<04:18,  1.16s/it]Loading train:  22%|██▏       | 64/285 [01:26<04:52,  1.33s/it]Loading train:  23%|██▎       | 65/285 [01:28<05:43,  1.56s/it]Loading train:  23%|██▎       | 66/285 [01:30<06:21,  1.74s/it]Loading train:  24%|██▎       | 67/285 [01:32<06:23,  1.76s/it]Loading train:  24%|██▍       | 68/285 [01:33<06:14,  1.73s/it]Loading train:  24%|██▍       | 69/285 [01:35<05:42,  1.59s/it]Loading train:  25%|██▍       | 70/285 [01:36<05:21,  1.49s/it]Loading train:  25%|██▍       | 71/285 [01:37<04:55,  1.38s/it]Loading train:  25%|██▌       | 72/285 [01:38<04:43,  1.33s/it]Loading train:  26%|██▌       | 73/285 [01:40<05:09,  1.46s/it]Loading train:  26%|██▌       | 74/285 [01:42<05:29,  1.56s/it]Loading train:  26%|██▋       | 75/285 [01:43<05:14,  1.50s/it]Loading train:  27%|██▋       | 76/285 [01:44<04:47,  1.38s/it]Loading train:  27%|██▋       | 77/285 [01:45<04:33,  1.32s/it]Loading train:  27%|██▋       | 78/285 [01:46<04:21,  1.26s/it]Loading train:  28%|██▊       | 79/285 [01:48<04:17,  1.25s/it]Loading train:  28%|██▊       | 80/285 [01:49<04:23,  1.29s/it]Loading train:  28%|██▊       | 81/285 [01:50<04:15,  1.25s/it]Loading train:  29%|██▉       | 82/285 [01:51<04:03,  1.20s/it]Loading train:  29%|██▉       | 83/285 [01:53<04:04,  1.21s/it]Loading train:  29%|██▉       | 84/285 [01:54<04:14,  1.27s/it]Loading train:  30%|██▉       | 85/285 [01:56<04:52,  1.46s/it]Loading train:  30%|███       | 86/285 [01:58<05:03,  1.53s/it]Loading train:  31%|███       | 87/285 [01:59<05:02,  1.53s/it]Loading train:  31%|███       | 88/285 [02:01<05:01,  1.53s/it]Loading train:  31%|███       | 89/285 [02:02<05:00,  1.54s/it]Loading train:  32%|███▏      | 90/285 [02:04<05:10,  1.59s/it]Loading train:  32%|███▏      | 91/285 [02:05<05:05,  1.57s/it]Loading train:  32%|███▏      | 92/285 [02:07<04:49,  1.50s/it]Loading train:  33%|███▎      | 93/285 [02:08<04:51,  1.52s/it]Loading train:  33%|███▎      | 94/285 [02:10<04:49,  1.52s/it]Loading train:  33%|███▎      | 95/285 [02:11<04:36,  1.45s/it]Loading train:  34%|███▎      | 96/285 [02:12<04:19,  1.37s/it]Loading train:  34%|███▍      | 97/285 [02:13<04:03,  1.29s/it]Loading train:  34%|███▍      | 98/285 [02:15<04:03,  1.30s/it]Loading train:  35%|███▍      | 99/285 [02:16<04:07,  1.33s/it]Loading train:  35%|███▌      | 100/285 [02:17<04:05,  1.32s/it]Loading train:  35%|███▌      | 101/285 [02:19<04:15,  1.39s/it]Loading train:  36%|███▌      | 102/285 [02:20<04:08,  1.36s/it]Loading train:  36%|███▌      | 103/285 [02:22<04:23,  1.45s/it]Loading train:  36%|███▋      | 104/285 [02:23<04:06,  1.36s/it]Loading train:  37%|███▋      | 105/285 [02:24<03:55,  1.31s/it]Loading train:  37%|███▋      | 106/285 [02:25<03:49,  1.28s/it]Loading train:  38%|███▊      | 107/285 [02:27<03:43,  1.25s/it]Loading train:  38%|███▊      | 108/285 [02:28<03:56,  1.34s/it]Loading train:  38%|███▊      | 109/285 [02:29<03:51,  1.32s/it]Loading train:  39%|███▊      | 110/285 [02:31<03:45,  1.29s/it]Loading train:  39%|███▉      | 111/285 [02:32<03:38,  1.26s/it]Loading train:  39%|███▉      | 112/285 [02:33<03:42,  1.29s/it]Loading train:  40%|███▉      | 113/285 [02:35<03:46,  1.31s/it]Loading train:  40%|████      | 114/285 [02:36<03:52,  1.36s/it]Loading train:  40%|████      | 115/285 [02:37<03:39,  1.29s/it]Loading train:  41%|████      | 116/285 [02:39<03:45,  1.33s/it]Loading train:  41%|████      | 117/285 [02:40<03:41,  1.32s/it]Loading train:  41%|████▏     | 118/285 [02:41<03:26,  1.23s/it]Loading train:  42%|████▏     | 119/285 [02:42<03:35,  1.30s/it]Loading train:  42%|████▏     | 120/285 [02:43<03:21,  1.22s/it]Loading train:  42%|████▏     | 121/285 [02:45<03:52,  1.41s/it]Loading train:  43%|████▎     | 122/285 [02:47<03:48,  1.40s/it]Loading train:  43%|████▎     | 123/285 [02:48<03:56,  1.46s/it]Loading train:  44%|████▎     | 124/285 [02:50<04:01,  1.50s/it]Loading train:  44%|████▍     | 125/285 [02:52<04:09,  1.56s/it]Loading train:  44%|████▍     | 126/285 [02:53<04:02,  1.52s/it]Loading train:  45%|████▍     | 127/285 [02:55<04:01,  1.53s/it]Loading train:  45%|████▍     | 128/285 [02:56<03:53,  1.49s/it]Loading train:  45%|████▌     | 129/285 [02:57<03:44,  1.44s/it]Loading train:  46%|████▌     | 130/285 [02:58<03:29,  1.35s/it]Loading train:  46%|████▌     | 131/285 [03:00<03:30,  1.37s/it]Loading train:  46%|████▋     | 132/285 [03:01<03:09,  1.24s/it]Loading train:  47%|████▋     | 133/285 [03:02<03:27,  1.36s/it]Loading train:  47%|████▋     | 134/285 [03:04<03:17,  1.31s/it]Loading train:  47%|████▋     | 135/285 [03:05<03:05,  1.23s/it]Loading train:  48%|████▊     | 136/285 [03:06<02:58,  1.20s/it]Loading train:  48%|████▊     | 137/285 [03:07<02:59,  1.21s/it]Loading train:  48%|████▊     | 138/285 [03:08<02:53,  1.18s/it]Loading train:  49%|████▉     | 139/285 [03:09<02:57,  1.21s/it]Loading train:  49%|████▉     | 140/285 [03:11<03:10,  1.31s/it]Loading train:  49%|████▉     | 141/285 [03:12<03:05,  1.29s/it]Loading train:  50%|████▉     | 142/285 [03:14<03:05,  1.30s/it]Loading train:  50%|█████     | 143/285 [03:15<02:52,  1.22s/it]Loading train:  51%|█████     | 144/285 [03:16<02:45,  1.17s/it]Loading train:  51%|█████     | 145/285 [03:17<02:48,  1.20s/it]Loading train:  51%|█████     | 146/285 [03:18<03:01,  1.31s/it]Loading train:  52%|█████▏    | 147/285 [03:20<02:53,  1.25s/it]Loading train:  52%|█████▏    | 148/285 [03:21<02:48,  1.23s/it]Loading train:  52%|█████▏    | 149/285 [03:23<03:12,  1.42s/it]Loading train:  53%|█████▎    | 150/285 [03:24<03:04,  1.36s/it]Loading train:  53%|█████▎    | 151/285 [03:25<02:52,  1.29s/it]Loading train:  53%|█████▎    | 152/285 [03:26<02:37,  1.19s/it]Loading train:  54%|█████▎    | 153/285 [03:27<02:36,  1.18s/it]Loading train:  54%|█████▍    | 154/285 [03:28<02:42,  1.24s/it]Loading train:  54%|█████▍    | 155/285 [03:30<02:33,  1.18s/it]Loading train:  55%|█████▍    | 156/285 [03:31<02:40,  1.25s/it]Loading train:  55%|█████▌    | 157/285 [03:32<02:36,  1.23s/it]Loading train:  55%|█████▌    | 158/285 [03:33<02:41,  1.27s/it]Loading train:  56%|█████▌    | 159/285 [03:35<02:37,  1.25s/it]Loading train:  56%|█████▌    | 160/285 [03:36<02:58,  1.42s/it]Loading train:  56%|█████▋    | 161/285 [03:38<02:53,  1.40s/it]Loading train:  57%|█████▋    | 162/285 [03:39<02:38,  1.29s/it]Loading train:  57%|█████▋    | 163/285 [03:40<02:46,  1.37s/it]Loading train:  58%|█████▊    | 164/285 [03:42<02:41,  1.33s/it]Loading train:  58%|█████▊    | 165/285 [03:43<02:46,  1.39s/it]Loading train:  58%|█████▊    | 166/285 [03:45<02:42,  1.37s/it]Loading train:  59%|█████▊    | 167/285 [03:46<02:38,  1.35s/it]Loading train:  59%|█████▉    | 168/285 [03:47<02:30,  1.28s/it]Loading train:  59%|█████▉    | 169/285 [03:48<02:25,  1.25s/it]Loading train:  60%|█████▉    | 170/285 [03:49<02:20,  1.22s/it]Loading train:  60%|██████    | 171/285 [03:50<02:09,  1.13s/it]Loading train:  60%|██████    | 172/285 [03:51<02:07,  1.13s/it]Loading train:  61%|██████    | 173/285 [03:53<02:08,  1.15s/it]Loading train:  61%|██████    | 174/285 [03:54<02:07,  1.15s/it]Loading train:  61%|██████▏   | 175/285 [03:55<02:08,  1.17s/it]Loading train:  62%|██████▏   | 176/285 [03:56<02:05,  1.15s/it]Loading train:  62%|██████▏   | 177/285 [03:57<02:13,  1.24s/it]Loading train:  62%|██████▏   | 178/285 [03:59<02:15,  1.27s/it]Loading train:  63%|██████▎   | 179/285 [04:00<02:10,  1.23s/it]Loading train:  63%|██████▎   | 180/285 [04:01<02:09,  1.23s/it]Loading train:  64%|██████▎   | 181/285 [04:02<02:03,  1.19s/it]Loading train:  64%|██████▍   | 182/285 [04:03<01:59,  1.17s/it]Loading train:  64%|██████▍   | 183/285 [04:04<01:57,  1.15s/it]Loading train:  65%|██████▍   | 184/285 [04:06<01:57,  1.16s/it]Loading train:  65%|██████▍   | 185/285 [04:07<01:48,  1.09s/it]Loading train:  65%|██████▌   | 186/285 [04:08<01:51,  1.13s/it]Loading train:  66%|██████▌   | 187/285 [04:09<01:46,  1.09s/it]Loading train:  66%|██████▌   | 188/285 [04:10<01:46,  1.10s/it]Loading train:  66%|██████▋   | 189/285 [04:11<01:50,  1.15s/it]Loading train:  67%|██████▋   | 190/285 [04:13<01:58,  1.24s/it]Loading train:  67%|██████▋   | 191/285 [04:14<01:48,  1.15s/it]Loading train:  67%|██████▋   | 192/285 [04:15<01:45,  1.13s/it]Loading train:  68%|██████▊   | 193/285 [04:16<01:52,  1.22s/it]Loading train:  68%|██████▊   | 194/285 [04:17<01:48,  1.19s/it]Loading train:  68%|██████▊   | 195/285 [04:19<01:54,  1.28s/it]Loading train:  69%|██████▉   | 196/285 [04:20<02:01,  1.36s/it]Loading train:  69%|██████▉   | 197/285 [04:22<02:01,  1.38s/it]Loading train:  69%|██████▉   | 198/285 [04:23<02:00,  1.39s/it]Loading train:  70%|██████▉   | 199/285 [04:24<01:54,  1.33s/it]Loading train:  70%|███████   | 200/285 [04:25<01:49,  1.29s/it]Loading train:  71%|███████   | 201/285 [04:27<01:43,  1.23s/it]Loading train:  71%|███████   | 202/285 [04:28<01:43,  1.25s/it]Loading train:  71%|███████   | 203/285 [04:29<01:37,  1.19s/it]Loading train:  72%|███████▏  | 204/285 [04:30<01:45,  1.30s/it]Loading train:  72%|███████▏  | 205/285 [04:32<01:42,  1.28s/it]Loading train:  72%|███████▏  | 206/285 [04:33<01:42,  1.30s/it]Loading train:  73%|███████▎  | 207/285 [04:34<01:40,  1.29s/it]Loading train:  73%|███████▎  | 208/285 [04:36<01:42,  1.33s/it]Loading train:  73%|███████▎  | 209/285 [04:37<01:39,  1.31s/it]Loading train:  74%|███████▎  | 210/285 [04:38<01:34,  1.26s/it]Loading train:  74%|███████▍  | 211/285 [04:39<01:35,  1.29s/it]Loading train:  74%|███████▍  | 212/285 [04:41<01:37,  1.34s/it]Loading train:  75%|███████▍  | 213/285 [04:42<01:40,  1.39s/it]Loading train:  75%|███████▌  | 214/285 [04:44<01:36,  1.36s/it]Loading train:  75%|███████▌  | 215/285 [04:45<01:29,  1.28s/it]Loading train:  76%|███████▌  | 216/285 [04:46<01:25,  1.24s/it]Loading train:  76%|███████▌  | 217/285 [04:47<01:27,  1.29s/it]Loading train:  76%|███████▋  | 218/285 [04:49<01:31,  1.37s/it]Loading train:  77%|███████▋  | 219/285 [04:50<01:33,  1.42s/it]Loading train:  77%|███████▋  | 220/285 [04:52<01:30,  1.39s/it]Loading train:  78%|███████▊  | 221/285 [04:53<01:26,  1.35s/it]Loading train:  78%|███████▊  | 222/285 [04:54<01:26,  1.37s/it]Loading train:  78%|███████▊  | 223/285 [04:56<01:24,  1.36s/it]Loading train:  79%|███████▊  | 224/285 [04:57<01:27,  1.44s/it]Loading train:  79%|███████▉  | 225/285 [04:58<01:19,  1.32s/it]Loading train:  79%|███████▉  | 226/285 [05:00<01:17,  1.32s/it]Loading train:  80%|███████▉  | 227/285 [05:01<01:15,  1.30s/it]Loading train:  80%|████████  | 228/285 [05:02<01:12,  1.28s/it]Loading train:  80%|████████  | 229/285 [05:04<01:12,  1.30s/it]Loading train:  81%|████████  | 230/285 [05:05<01:17,  1.41s/it]Loading train:  81%|████████  | 231/285 [05:07<01:16,  1.42s/it]Loading train:  81%|████████▏ | 232/285 [05:09<01:21,  1.54s/it]Loading train:  82%|████████▏ | 233/285 [05:10<01:17,  1.50s/it]Loading train:  82%|████████▏ | 234/285 [05:11<01:12,  1.43s/it]Loading train:  82%|████████▏ | 235/285 [05:13<01:11,  1.44s/it]Loading train:  83%|████████▎ | 236/285 [05:14<01:14,  1.52s/it]Loading train:  83%|████████▎ | 237/285 [05:16<01:08,  1.42s/it]Loading train:  84%|████████▎ | 238/285 [05:17<01:09,  1.48s/it]Loading train:  84%|████████▍ | 239/285 [05:19<01:07,  1.47s/it]Loading train:  84%|████████▍ | 240/285 [05:20<01:03,  1.40s/it]Loading train:  85%|████████▍ | 241/285 [05:21<00:59,  1.35s/it]Loading train:  85%|████████▍ | 242/285 [05:23<00:59,  1.39s/it]Loading train:  85%|████████▌ | 243/285 [05:24<00:57,  1.38s/it]Loading train:  86%|████████▌ | 244/285 [05:25<00:55,  1.35s/it]Loading train:  86%|████████▌ | 245/285 [05:27<00:53,  1.33s/it]Loading train:  86%|████████▋ | 246/285 [05:28<00:52,  1.34s/it]Loading train:  87%|████████▋ | 247/285 [05:29<00:51,  1.35s/it]Loading train:  87%|████████▋ | 248/285 [05:31<00:50,  1.35s/it]Loading train:  87%|████████▋ | 249/285 [05:32<00:47,  1.33s/it]Loading train:  88%|████████▊ | 250/285 [05:33<00:46,  1.32s/it]Loading train:  88%|████████▊ | 251/285 [05:34<00:39,  1.16s/it]Loading train:  88%|████████▊ | 252/285 [05:35<00:38,  1.17s/it]Loading train:  89%|████████▉ | 253/285 [05:36<00:35,  1.11s/it]Loading train:  89%|████████▉ | 254/285 [05:37<00:34,  1.10s/it]Loading train:  89%|████████▉ | 255/285 [05:38<00:33,  1.12s/it]Loading train:  90%|████████▉ | 256/285 [05:40<00:33,  1.14s/it]Loading train:  90%|█████████ | 257/285 [05:41<00:30,  1.10s/it]Loading train:  91%|█████████ | 258/285 [05:42<00:30,  1.14s/it]Loading train:  91%|█████████ | 259/285 [05:43<00:30,  1.18s/it]Loading train:  91%|█████████ | 260/285 [05:44<00:29,  1.18s/it]Loading train:  92%|█████████▏| 261/285 [05:46<00:31,  1.30s/it]Loading train:  92%|█████████▏| 262/285 [05:47<00:28,  1.22s/it]Loading train:  92%|█████████▏| 263/285 [05:48<00:25,  1.18s/it]Loading train:  93%|█████████▎| 264/285 [05:49<00:24,  1.17s/it]Loading train:  93%|█████████▎| 265/285 [05:50<00:23,  1.18s/it]Loading train:  93%|█████████▎| 266/285 [05:51<00:21,  1.14s/it]Loading train:  94%|█████████▎| 267/285 [05:52<00:19,  1.08s/it]Loading train:  94%|█████████▍| 268/285 [05:54<00:21,  1.25s/it]Loading train:  94%|█████████▍| 269/285 [05:55<00:20,  1.29s/it]Loading train:  95%|█████████▍| 270/285 [05:57<00:18,  1.26s/it]Loading train:  95%|█████████▌| 271/285 [05:58<00:17,  1.25s/it]Loading train:  95%|█████████▌| 272/285 [05:59<00:17,  1.32s/it]Loading train:  96%|█████████▌| 273/285 [06:01<00:16,  1.35s/it]Loading train:  96%|█████████▌| 274/285 [06:02<00:15,  1.40s/it]Loading train:  96%|█████████▋| 275/285 [06:04<00:14,  1.48s/it]Loading train:  97%|█████████▋| 276/285 [06:05<00:13,  1.45s/it]Loading train:  97%|█████████▋| 277/285 [06:07<00:12,  1.58s/it]Loading train:  98%|█████████▊| 278/285 [06:09<00:11,  1.58s/it]Loading train:  98%|█████████▊| 279/285 [06:10<00:09,  1.55s/it]Loading train:  98%|█████████▊| 280/285 [06:12<00:07,  1.54s/it]Loading train:  99%|█████████▊| 281/285 [06:13<00:06,  1.53s/it]Loading train:  99%|█████████▉| 282/285 [06:15<00:04,  1.58s/it]Loading train:  99%|█████████▉| 283/285 [06:16<00:03,  1.58s/it]Loading train: 100%|█████████▉| 284/285 [06:18<00:01,  1.48s/it]Loading train: 100%|██████████| 285/285 [06:19<00:00,  1.46s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 2/285 [00:00<00:24, 11.46it/s]concatenating: train:   1%|▏         | 4/285 [00:00<00:24, 11.43it/s]concatenating: train:   2%|▏         | 7/285 [00:00<00:20, 13.43it/s]concatenating: train:   3%|▎         | 9/285 [00:00<00:18, 14.89it/s]concatenating: train:   5%|▍         | 13/285 [00:00<00:15, 17.20it/s]concatenating: train:   7%|▋         | 19/285 [00:00<00:12, 21.20it/s]concatenating: train:   8%|▊         | 22/285 [00:01<00:13, 18.94it/s]concatenating: train:   9%|▉         | 25/285 [00:01<00:16, 15.39it/s]concatenating: train:  10%|█         | 29/285 [00:01<00:14, 18.06it/s]concatenating: train:  12%|█▏        | 33/285 [00:01<00:11, 21.58it/s]concatenating: train:  13%|█▎        | 36/285 [00:01<00:10, 22.77it/s]concatenating: train:  14%|█▎        | 39/285 [00:01<00:10, 24.04it/s]concatenating: train:  15%|█▍        | 42/285 [00:01<00:11, 21.61it/s]concatenating: train:  16%|█▌        | 45/285 [00:02<00:11, 21.06it/s]concatenating: train:  17%|█▋        | 48/285 [00:02<00:11, 20.59it/s]concatenating: train:  18%|█▊        | 51/285 [00:02<00:10, 21.38it/s]concatenating: train:  19%|█▉        | 54/285 [00:02<00:10, 22.15it/s]concatenating: train:  20%|██        | 57/285 [00:02<00:09, 23.81it/s]concatenating: train:  21%|██        | 60/285 [00:02<00:09, 24.27it/s]concatenating: train:  22%|██▏       | 64/285 [00:02<00:08, 24.95it/s]concatenating: train:  24%|██▎       | 67/285 [00:03<00:08, 24.96it/s]concatenating: train:  25%|██▍       | 70/285 [00:03<00:09, 21.53it/s]concatenating: train:  26%|██▌       | 73/285 [00:03<00:10, 20.28it/s]concatenating: train:  27%|██▋       | 77/285 [00:03<00:09, 22.07it/s]concatenating: train:  29%|██▉       | 83/285 [00:03<00:07, 26.51it/s]concatenating: train:  31%|███       | 87/285 [00:03<00:08, 24.15it/s]concatenating: train:  32%|███▏      | 90/285 [00:04<00:09, 20.58it/s]concatenating: train:  33%|███▎      | 93/285 [00:04<00:11, 17.24it/s]concatenating: train:  34%|███▎      | 96/285 [00:04<00:13, 14.16it/s]concatenating: train:  35%|███▌      | 101/285 [00:04<00:10, 17.81it/s]concatenating: train:  37%|███▋      | 105/285 [00:04<00:09, 19.96it/s]concatenating: train:  38%|███▊      | 109/285 [00:04<00:07, 23.31it/s]concatenating: train:  40%|████      | 115/285 [00:05<00:06, 28.17it/s]concatenating: train:  42%|████▏     | 119/285 [00:05<00:05, 27.92it/s]concatenating: train:  43%|████▎     | 123/285 [00:05<00:05, 29.88it/s]concatenating: train:  45%|████▍     | 127/285 [00:05<00:05, 31.42it/s]concatenating: train:  46%|████▌     | 131/285 [00:05<00:05, 25.87it/s]concatenating: train:  47%|████▋     | 135/285 [00:05<00:05, 27.84it/s]concatenating: train:  53%|█████▎    | 152/285 [00:05<00:03, 37.16it/s]concatenating: train:  61%|██████▏   | 175/285 [00:05<00:02, 49.61it/s]concatenating: train:  71%|███████   | 201/285 [00:06<00:01, 65.46it/s]concatenating: train:  78%|███████▊  | 223/285 [00:06<00:00, 82.69it/s]concatenating: train:  87%|████████▋ | 247/285 [00:06<00:00, 102.69it/s]concatenating: train:  94%|█████████▎| 267/285 [00:06<00:00, 116.99it/s]concatenating: train: 100%|██████████| 285/285 [00:06<00:00, 43.01it/s] 
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.75s/it]Loading test:  67%|██████▋   | 2/3 [00:03<00:01,  1.70s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.56s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 331.07it/s]2019-07-06 21:58:21.604981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 21:58:21.605076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 21:58:21.605090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 21:58:21.605100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 21:58:21.605555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:10,  3.81it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:08,  4.56it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.40it/s]loading the weights for Unet:  20%|██        | 8/40 [00:00<00:05,  5.63it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:05,  5.27it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:04,  5.96it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  5.11it/s]loading the weights for Unet:  40%|████      | 16/40 [00:01<00:03,  6.48it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.40it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.99it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  5.18it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:02<00:02,  6.22it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.76it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  5.59it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:01,  6.11it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:03<00:02,  5.36it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  6.39it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  6.82it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.56it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:04<00:00,  6.12it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  5.28it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  7.69it/s]
Epoch 00031: val_mDice did not improve from 0.01984
Restoring model weights from the end of the best epoch
Epoch 00031: early stopping
{'val_loss': [516.6810790942266, 131.9878950852614, 78.38326329451341, 49.375489354133606, 36.371659269699684, 30.094025410138645, 28.500808225228237, 23.006952647979443, 17.098216361724415, 14.140971394685598, 13.25384047627449, 12.701788702836403, 12.241868376731873, 12.113229632377625, 12.032782749487804, 11.938915527783907, 11.763400639478977, 11.54816794853944, 11.304031952069355, 11.097310994680111, 10.952512580614824, 10.705969026455513, 10.403461451713856, 10.313256609898348, 10.241155505180359, 10.180822842396223, 10.92392909526825, 10.929735254782896, 9.902737115438168, 9.76770370052411, 9.670351429627491], 'val_acc': [0.20446329563856125, 0.9031688937774072, 0.9031018821092752, 0.903388500213623, 0.9033815654424521, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623], 'val_mDice': [0.019836349341158684, 0.010965222546544213, 0.012709768238262488, 0.010946201876952099, 0.010908375673282605, 0.009311207014924059, 0.007698003704158159, 0.006294059683568776, 0.005696591088333382, 0.005449788465809364, 0.005409309703211945, 0.005378646704439933, 0.005373692193713326, 0.005269142799079418, 0.005054874068054442, 0.0050206892550564725, 0.005299582871465156, 0.005924212012010125, 0.006429517704348724, 0.007697399293717284, 0.008222004344973426, 0.008490165865693528, 0.01006573811173439, 0.01002985815732525, 0.010416878960453547, 0.010501261299046187, 0.010436560842208564, 0.010629238945861848, 0.011306211128472708, 0.011938187365348522, 0.012913169303479103], 'loss': [701.913350241173, 266.3983825434511, 116.84196883823162, 66.27280557286439, 45.6960725746482, 35.22590134511693, 29.134405553093544, 25.22056315229529, 22.563170824171397, 20.66231906179857, 19.241864585595014, 18.15858847886272, 17.313339795374095, 16.624718962038486, 16.06239314546755, 15.589120577569709, 15.1949403026299, 14.843565117353048, 14.548005387889253, 14.295499546220247, 14.06704037059533, 13.865067569551835, 13.665445253086697, 13.49837135360501, 13.331085088364501, 13.179717157002052, 13.026745386430614, 12.824250043279557, 12.598592874892335, 12.38893842503647, 12.171925101719538], 'acc': [0.13375172457395526, 0.5819309613784499, 0.8385418374182432, 0.8625062323854515, 0.8699387749226718, 0.872687336264595, 0.8737224792737116, 0.8742246860553228, 0.8744681729147226, 0.8746524699101708, 0.8747384029594414, 0.8748162797837968, 0.8748250366752464, 0.8748284976640487, 0.8748365681263548, 0.8748467627194363, 0.8748469861475269, 0.8748470080533195, 0.8748470069374159, 0.8748470079378812, 0.8748470109227859, 0.8748469628124981, 0.874847006772504, 0.8748469631808012, 0.874846986950098, 0.8748469834814517, 0.8748469198309693, 0.8748468998821307, 0.8748468994478629, 0.8748469159280551, 0.8748469878021427], 'mDice': [0.016919341616186172, 0.01482721966680407, 0.013525602092187745, 0.012494804839141763, 0.011966333966257304, 0.011536464575163791, 0.01116924881327007, 0.01089417254113339, 0.010708472265110463, 0.010570426466549678, 0.010477897953827845, 0.010405562586314858, 0.010377835894037351, 0.010369834220334908, 0.010343645033662354, 0.010347640571989862, 0.010366844836518375, 0.010420282219743515, 0.010465929356840752, 0.010561083911142589, 0.010606413022237521, 0.01069589072259001, 0.010796983914339354, 0.01096729509979775, 0.011086981410057076, 0.011260200181329541, 0.011660315493884632, 0.012588167755134397, 0.013878312941912377, 0.014691266837891183, 0.015450142384266695]}
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 80, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 80, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 80, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 80, 30)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 80, 30)   8130        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 80, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 80, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 80, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 80, 30)   8130        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 80, 30)   120         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 80, 30)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 80, 30)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 80, 20)   5420        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 80, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 80, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 80, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 80, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 80, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 40, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 40, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 40, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 40, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 40, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 40, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 40, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 20, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 20, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 20, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 20, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 20, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 20, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 20, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 20, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 40, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 40, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 40, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 40, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 40, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 40, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 13)   273         dropout_8[0][0]                  
==================================================================================================
Total params: 206,253
Trainable params: 62,593
Non-trainable params: 143,660
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 19s - loss: 189.0831 - acc: 0.3972 - mDice: 0.0172 - val_loss: 52.8824 - val_acc: 0.9047 - val_mDice: 0.0148

Epoch 00001: val_mDice improved from -inf to 0.01480, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 11s - loss: 43.3709 - acc: 0.7878 - mDice: 0.0162 - val_loss: 17.0058 - val_acc: 0.9047 - val_mDice: 0.0149

Epoch 00002: val_mDice improved from 0.01480 to 0.01495, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 11s - loss: 19.9990 - acc: 0.8596 - mDice: 0.0161 - val_loss: 9.5983 - val_acc: 0.9047 - val_mDice: 0.0151

Epoch 00003: val_mDice improved from 0.01495 to 0.01506, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 11s - loss: 13.4151 - acc: 0.8676 - mDice: 0.0198 - val_loss: 7.3222 - val_acc: 0.9047 - val_mDice: 0.0203

Epoch 00004: val_mDice improved from 0.01506 to 0.02034, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 11s - loss: 10.6087 - acc: 0.8685 - mDice: 0.0240 - val_loss: 6.3143 - val_acc: 0.9047 - val_mDice: 0.0270

Epoch 00005: val_mDice improved from 0.02034 to 0.02703, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 11s - loss: 8.9051 - acc: 0.8686 - mDice: 0.0333 - val_loss: 5.7042 - val_acc: 0.9047 - val_mDice: 0.0332

Epoch 00006: val_mDice improved from 0.02703 to 0.03320, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 11s - loss: 7.8883 - acc: 0.8683 - mDice: 0.0439 - val_loss: 5.2785 - val_acc: 0.9047 - val_mDice: 0.0521

Epoch 00007: val_mDice improved from 0.03320 to 0.05205, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 11s - loss: 7.1830 - acc: 0.8679 - mDice: 0.0533 - val_loss: 5.0499 - val_acc: 0.9047 - val_mDice: 0.0610

Epoch 00008: val_mDice improved from 0.05205 to 0.06105, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 11s - loss: 6.6797 - acc: 0.8680 - mDice: 0.0615 - val_loss: 4.8969 - val_acc: 0.9047 - val_mDice: 0.0649

Epoch 00009: val_mDice improved from 0.06105 to 0.06491, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 11s - loss: 6.2647 - acc: 0.8684 - mDice: 0.0718 - val_loss: 4.6914 - val_acc: 0.9047 - val_mDice: 0.0739

Epoch 00010: val_mDice improved from 0.06491 to 0.07388, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 11s - loss: 5.8887 - acc: 0.8693 - mDice: 0.0820 - val_loss: 4.5813 - val_acc: 0.9049 - val_mDice: 0.0836

Epoch 00011: val_mDice improved from 0.07388 to 0.08362, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 11s - loss: 5.5440 - acc: 0.8706 - mDice: 0.0943 - val_loss: 4.2130 - val_acc: 0.9060 - val_mDice: 0.1048

Epoch 00012: val_mDice improved from 0.08362 to 0.10481, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 11s - loss: 5.2190 - acc: 0.8719 - mDice: 0.1084 - val_loss: 4.0882 - val_acc: 0.9073 - val_mDice: 0.1181

Epoch 00013: val_mDice improved from 0.10481 to 0.11808, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 11s - loss: 4.9261 - acc: 0.8728 - mDice: 0.1224 - val_loss: 4.1610 - val_acc: 0.9060 - val_mDice: 0.1247

Epoch 00014: val_mDice improved from 0.11808 to 0.12472, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 11s - loss: 4.6542 - acc: 0.8739 - mDice: 0.1389 - val_loss: 3.8302 - val_acc: 0.9075 - val_mDice: 0.1469

Epoch 00015: val_mDice improved from 0.12472 to 0.14690, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 11s - loss: 4.4121 - acc: 0.8752 - mDice: 0.1561 - val_loss: 3.7337 - val_acc: 0.9076 - val_mDice: 0.1695

Epoch 00016: val_mDice improved from 0.14690 to 0.16946, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 11s - loss: 4.2163 - acc: 0.8767 - mDice: 0.1719 - val_loss: 3.6545 - val_acc: 0.9086 - val_mDice: 0.1882

Epoch 00017: val_mDice improved from 0.16946 to 0.18817, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 11s - loss: 4.0412 - acc: 0.8780 - mDice: 0.1875 - val_loss: 3.7448 - val_acc: 0.9091 - val_mDice: 0.1970

Epoch 00018: val_mDice improved from 0.18817 to 0.19703, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 11s - loss: 3.8814 - acc: 0.8795 - mDice: 0.2026 - val_loss: 3.7282 - val_acc: 0.9095 - val_mDice: 0.2104

Epoch 00019: val_mDice improved from 0.19703 to 0.21044, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 11s - loss: 3.7558 - acc: 0.8808 - mDice: 0.2157 - val_loss: 3.4241 - val_acc: 0.9126 - val_mDice: 0.2410

Epoch 00020: val_mDice improved from 0.21044 to 0.24103, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 11s - loss: 3.6193 - acc: 0.8826 - mDice: 0.2310 - val_loss: 3.6896 - val_acc: 0.9129 - val_mDice: 0.2387

Epoch 00021: val_mDice did not improve from 0.24103
Epoch 22/300
 - 11s - loss: 3.5080 - acc: 0.8843 - mDice: 0.2449 - val_loss: 3.4664 - val_acc: 0.9153 - val_mDice: 0.2623

Epoch 00022: val_mDice improved from 0.24103 to 0.26232, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 11s - loss: 3.3905 - acc: 0.8861 - mDice: 0.2593 - val_loss: 3.0434 - val_acc: 0.9180 - val_mDice: 0.2990

Epoch 00023: val_mDice improved from 0.26232 to 0.29901, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 11s - loss: 3.3069 - acc: 0.8876 - mDice: 0.2714 - val_loss: 3.2713 - val_acc: 0.9162 - val_mDice: 0.2885

Epoch 00024: val_mDice did not improve from 0.29901
Epoch 25/300
 - 11s - loss: 3.1974 - acc: 0.8900 - mDice: 0.2865 - val_loss: 3.2126 - val_acc: 0.9185 - val_mDice: 0.3031

Epoch 00025: val_mDice improved from 0.29901 to 0.30314, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 11s - loss: 3.1100 - acc: 0.8913 - mDice: 0.2978 - val_loss: 3.2052 - val_acc: 0.9214 - val_mDice: 0.3095

Epoch 00026: val_mDice improved from 0.30314 to 0.30951, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 11s - loss: 3.0398 - acc: 0.8927 - mDice: 0.3073 - val_loss: 3.1087 - val_acc: 0.9224 - val_mDice: 0.3219

Epoch 00027: val_mDice improved from 0.30951 to 0.32188, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 11s - loss: 2.9736 - acc: 0.8942 - mDice: 0.3173 - val_loss: 3.2281 - val_acc: 0.9226 - val_mDice: 0.3201

Epoch 00028: val_mDice did not improve from 0.32188
Epoch 29/300
 - 11s - loss: 2.9155 - acc: 0.8956 - mDice: 0.3254 - val_loss: 3.1086 - val_acc: 0.9234 - val_mDice: 0.3354

Epoch 00029: val_mDice improved from 0.32188 to 0.33537, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 11s - loss: 2.8572 - acc: 0.8970 - mDice: 0.3341 - val_loss: 3.1338 - val_acc: 0.9210 - val_mDice: 0.3368

Epoch 00030: val_mDice improved from 0.33537 to 0.33683, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 11s - loss: 2.8014 - acc: 0.8985 - mDice: 0.3423 - val_loss: 3.2481 - val_acc: 0.9228 - val_mDice: 0.3296

Epoch 00031: val_mDice did not improve from 0.33683
Epoch 32/300
 - 11s - loss: 2.7593 - acc: 0.8996 - mDice: 0.3484 - val_loss: 3.0018 - val_acc: 0.9225 - val_mDice: 0.3514

Epoch 00032: val_mDice improved from 0.33683 to 0.35139, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 11s - loss: 2.7126 - acc: 0.9007 - mDice: 0.3560 - val_loss: 3.0950 - val_acc: 0.9210 - val_mDice: 0.3484

Epoch 00033: val_mDice did not improve from 0.35139
Epoch 34/300
 - 11s - loss: 2.6732 - acc: 0.9019 - mDice: 0.3619 - val_loss: 3.0509 - val_acc: 0.9279 - val_mDice: 0.3597

Epoch 00034: val_mDice improved from 0.35139 to 0.35971, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 11s - loss: 2.6345 - acc: 0.9034 - mDice: 0.3690 - val_loss: 3.0968 - val_acc: 0.9234 - val_mDice: 0.3550

Epoch 00035: val_mDice did not improve from 0.35971
Epoch 36/300
 - 11s - loss: 2.5961 - acc: 0.9043 - mDice: 0.3753 - val_loss: 3.1044 - val_acc: 0.9246 - val_mDice: 0.3560

Epoch 00036: val_mDice did not improve from 0.35971
Epoch 37/300
 - 11s - loss: 2.5649 - acc: 0.9050 - mDice: 0.3799 - val_loss: 3.0306 - val_acc: 0.9300 - val_mDice: 0.3661

Epoch 00037: val_mDice improved from 0.35971 to 0.36608, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 11s - loss: 2.5483 - acc: 0.9056 - mDice: 0.3831 - val_loss: 2.8882 - val_acc: 0.9250 - val_mDice: 0.3778

Epoch 00038: val_mDice improved from 0.36608 to 0.37778, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 11s - loss: 2.5060 - acc: 0.9069 - mDice: 0.3901 - val_loss: 3.0644 - val_acc: 0.9311 - val_mDice: 0.3707

Epoch 00039: val_mDice did not improve from 0.37778
Epoch 40/300
 - 11s - loss: 2.4750 - acc: 0.9077 - mDice: 0.3952 - val_loss: 3.0583 - val_acc: 0.9291 - val_mDice: 0.3721

Epoch 00040: val_mDice did not improve from 0.37778
Epoch 41/300
 - 11s - loss: 2.4496 - acc: 0.9085 - mDice: 0.3996 - val_loss: 3.1947 - val_acc: 0.9277 - val_mDice: 0.3668

Epoch 00041: val_mDice did not improve from 0.37778
Epoch 42/300
 - 11s - loss: 2.4234 - acc: 0.9094 - mDice: 0.4044 - val_loss: 3.0463 - val_acc: 0.9305 - val_mDice: 0.3820

Epoch 00042: val_mDice improved from 0.37778 to 0.38199, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 11s - loss: 2.4145 - acc: 0.9096 - mDice: 0.4061 - val_loss: 3.0885 - val_acc: 0.9291 - val_mDice: 0.3793

Epoch 00043: val_mDice did not improve from 0.38199
Epoch 44/300
 - 11s - loss: 2.3738 - acc: 0.9107 - mDice: 0.4142 - val_loss: 3.0946 - val_acc: 0.9307 - val_mDice: 0.3869

Epoch 00044: val_mDice improved from 0.38199 to 0.38695, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 11s - loss: 2.3553 - acc: 0.9115 - mDice: 0.4174 - val_loss: 2.9753 - val_acc: 0.9301 - val_mDice: 0.3861

Epoch 00045: val_mDice did not improve from 0.38695
Epoch 46/300
 - 11s - loss: 2.3340 - acc: 0.9119 - mDice: 0.4217 - val_loss: 3.3448 - val_acc: 0.9254 - val_mDice: 0.3680

Epoch 00046: val_mDice did not improve from 0.38695
Epoch 47/300
 - 11s - loss: 2.3154 - acc: 0.9127 - mDice: 0.4262 - val_loss: 3.0191 - val_acc: 0.9319 - val_mDice: 0.3964

Epoch 00047: val_mDice improved from 0.38695 to 0.39638, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 11s - loss: 2.3041 - acc: 0.9126 - mDice: 0.4282 - val_loss: 3.1172 - val_acc: 0.9307 - val_mDice: 0.3878

Epoch 00048: val_mDice did not improve from 0.39638
Epoch 49/300
 - 11s - loss: 2.2860 - acc: 0.9133 - mDice: 0.4320 - val_loss: 3.0055 - val_acc: 0.9329 - val_mDice: 0.3950

Epoch 00049: val_mDice did not improve from 0.39638
Epoch 50/300
 - 11s - loss: 2.2588 - acc: 0.9141 - mDice: 0.4367 - val_loss: 3.1778 - val_acc: 0.9325 - val_mDice: 0.3855

Epoch 00050: val_mDice did not improve from 0.39638
Epoch 51/300
 - 11s - loss: 2.2458 - acc: 0.9145 - mDice: 0.4402 - val_loss: 3.3870 - val_acc: 0.9287 - val_mDice: 0.3739

Epoch 00051: val_mDice did not improve from 0.39638
Epoch 52/300
 - 11s - loss: 2.2274 - acc: 0.9149 - mDice: 0.4430 - val_loss: 3.1786 - val_acc: 0.9332 - val_mDice: 0.3928

Epoch 00052: val_mDice did not improve from 0.39638
Epoch 53/300
 - 11s - loss: 2.2203 - acc: 0.9150 - mDice: 0.4444 - val_loss: 3.0100 - val_acc: 0.9333 - val_mDice: 0.4006

Epoch 00053: val_mDice improved from 0.39638 to 0.40059, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 11s - loss: 2.1957 - acc: 0.9158 - mDice: 0.4493 - val_loss: 2.9784 - val_acc: 0.9329 - val_mDice: 0.4090

Epoch 00054: val_mDice improved from 0.40059 to 0.40898, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 11s - loss: 2.1720 - acc: 0.9164 - mDice: 0.4544 - val_loss: 3.0225 - val_acc: 0.9337 - val_mDice: 0.4065

Epoch 00055: val_mDice did not improve from 0.40898
Epoch 56/300
 - 11s - loss: 2.1513 - acc: 0.9168 - mDice: 0.4577 - val_loss: 3.1181 - val_acc: 0.9334 - val_mDice: 0.4046

Epoch 00056: val_mDice did not improve from 0.40898
Epoch 57/300
 - 11s - loss: 2.1493 - acc: 0.9172 - mDice: 0.4600 - val_loss: 3.3013 - val_acc: 0.9341 - val_mDice: 0.3922

Epoch 00057: val_mDice did not improve from 0.40898
Epoch 58/300
 - 11s - loss: 2.1333 - acc: 0.9175 - mDice: 0.4628 - val_loss: 3.0467 - val_acc: 0.9335 - val_mDice: 0.4145

Epoch 00058: val_mDice improved from 0.40898 to 0.41451, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 11s - loss: 2.1208 - acc: 0.9178 - mDice: 0.4655 - val_loss: 3.1822 - val_acc: 0.9340 - val_mDice: 0.4025

Epoch 00059: val_mDice did not improve from 0.41451
Epoch 60/300
 - 11s - loss: 2.1026 - acc: 0.9184 - mDice: 0.4693 - val_loss: 3.0989 - val_acc: 0.9322 - val_mDice: 0.3980

Epoch 00060: val_mDice did not improve from 0.41451
Epoch 61/300
 - 11s - loss: 2.0963 - acc: 0.9184 - mDice: 0.4699 - val_loss: 3.5237 - val_acc: 0.9338 - val_mDice: 0.3875

Epoch 00061: val_mDice did not improve from 0.41451
Epoch 62/300
 - 11s - loss: 2.0866 - acc: 0.9186 - mDice: 0.4721 - val_loss: 2.9650 - val_acc: 0.9321 - val_mDice: 0.4167

Epoch 00062: val_mDice improved from 0.41451 to 0.41673, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 11s - loss: 2.0727 - acc: 0.9191 - mDice: 0.4746 - val_loss: 3.2920 - val_acc: 0.9303 - val_mDice: 0.3988

Epoch 00063: val_mDice did not improve from 0.41673
Epoch 64/300
 - 11s - loss: 2.0616 - acc: 0.9196 - mDice: 0.4771 - val_loss: 2.9322 - val_acc: 0.9355 - val_mDice: 0.4226

Epoch 00064: val_mDice improved from 0.41673 to 0.42260, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 65/300
 - 11s - loss: 2.0439 - acc: 0.9200 - mDice: 0.4814 - val_loss: 3.0191 - val_acc: 0.9354 - val_mDice: 0.4184

Epoch 00065: val_mDice did not improve from 0.42260
Epoch 66/300
 - 11s - loss: 2.0308 - acc: 0.9203 - mDice: 0.4839 - val_loss: 3.1293 - val_acc: 0.9333 - val_mDice: 0.4170

Epoch 00066: val_mDice did not improve from 0.42260
Epoch 67/300
 - 11s - loss: 2.0160 - acc: 0.9207 - mDice: 0.4867 - val_loss: 3.0258 - val_acc: 0.9348 - val_mDice: 0.4219

Epoch 00067: val_mDice did not improve from 0.42260
Epoch 68/300
 - 11s - loss: 2.0270 - acc: 0.9202 - mDice: 0.4852 - val_loss: 3.1646 - val_acc: 0.9371 - val_mDice: 0.4216

Epoch 00068: val_mDice did not improve from 0.42260
Epoch 69/300
 - 11s - loss: 1.9967 - acc: 0.9214 - mDice: 0.4912 - val_loss: 3.1063 - val_acc: 0.9346 - val_mDice: 0.4234

Epoch 00069: val_mDice improved from 0.42260 to 0.42336, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 70/300
 - 11s - loss: 1.9965 - acc: 0.9213 - mDice: 0.4909 - val_loss: 3.2288 - val_acc: 0.9329 - val_mDice: 0.4150

Epoch 00070: val_mDice did not improve from 0.42336
Epoch 71/300
 - 11s - loss: 1.9785 - acc: 0.9217 - mDice: 0.4948 - val_loss: 3.2795 - val_acc: 0.9340 - val_mDice: 0.4140

Epoch 00071: val_mDice did not improve from 0.42336
Epoch 72/300
 - 11s - loss: 1.9760 - acc: 0.9221 - mDice: 0.4959 - val_loss: 3.1474 - val_acc: 0.9369 - val_mDice: 0.4264

Epoch 00072: val_mDice improved from 0.42336 to 0.42641, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 73/300
 - 11s - loss: 1.9620 - acc: 0.9225 - mDice: 0.4986 - val_loss: 2.9579 - val_acc: 0.9367 - val_mDice: 0.4359

Epoch 00073: val_mDice improved from 0.42641 to 0.43589, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 11s - loss: 1.9571 - acc: 0.9226 - mDice: 0.5000 - val_loss: 3.0928 - val_acc: 0.9375 - val_mDice: 0.4290

Epoch 00074: val_mDice did not improve from 0.43589
Epoch 75/300
 - 11s - loss: 1.9628 - acc: 0.9226 - mDice: 0.4987 - val_loss: 3.1342 - val_acc: 0.9352 - val_mDice: 0.4281

Epoch 00075: val_mDice did not improve from 0.43589
Epoch 76/300
 - 11s - loss: 1.9335 - acc: 0.9235 - mDice: 0.5048 - val_loss: 3.1394 - val_acc: 0.9358 - val_mDice: 0.4262

Epoch 00076: val_mDice did not improve from 0.43589
Epoch 77/300
 - 11s - loss: 1.9229 - acc: 0.9237 - mDice: 0.5065 - val_loss: 3.1006 - val_acc: 0.9362 - val_mDice: 0.4316

Epoch 00077: val_mDice did not improve from 0.43589
Epoch 78/300
 - 11s - loss: 1.9269 - acc: 0.9237 - mDice: 0.5067 - val_loss: 3.1843 - val_acc: 0.9328 - val_mDice: 0.4264

Epoch 00078: val_mDice did not improve from 0.43589
Epoch 79/300
 - 11s - loss: 1.9184 - acc: 0.9238 - mDice: 0.5079 - val_loss: 3.1852 - val_acc: 0.9348 - val_mDice: 0.4238

Epoch 00079: val_mDice did not improve from 0.43589
Epoch 80/300
 - 11s - loss: 1.9142 - acc: 0.9240 - mDice: 0.5091 - val_loss: 3.0824 - val_acc: 0.9363 - val_mDice: 0.4350

Epoch 00080: val_mDice did not improve from 0.43589
Epoch 81/300
 - 11s - loss: 1.9059 - acc: 0.9243 - mDice: 0.5109 - val_loss: 3.3062 - val_acc: 0.9337 - val_mDice: 0.4235

Epoch 00081: val_mDice did not improve from 0.43589
Epoch 82/300
 - 11s - loss: 1.8899 - acc: 0.9249 - mDice: 0.5142 - val_loss: 3.3516 - val_acc: 0.9344 - val_mDice: 0.4178

Epoch 00082: val_mDice did not improve from 0.43589
Epoch 83/300
 - 11s - loss: 1.8860 - acc: 0.9250 - mDice: 0.5154 - val_loss: 3.4205 - val_acc: 0.9385 - val_mDice: 0.4232

Epoch 00083: val_mDice did not improve from 0.43589
Epoch 84/300
 - 11s - loss: 1.8735 - acc: 0.9254 - mDice: 0.5181 - val_loss: 3.2704 - val_acc: 0.9368 - val_mDice: 0.4272

Epoch 00084: val_mDice did not improve from 0.43589
Epoch 85/300
 - 11s - loss: 1.8649 - acc: 0.9256 - mDice: 0.5203 - val_loss: 3.1567 - val_acc: 0.9354 - val_mDice: 0.4378

Epoch 00085: val_mDice improved from 0.43589 to 0.43780, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 86/300
 - 11s - loss: 1.8595 - acc: 0.9258 - mDice: 0.5211 - val_loss: 3.2239 - val_acc: 0.9388 - val_mDice: 0.4424

Epoch 00086: val_mDice improved from 0.43780 to 0.44245, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 87/300
 - 11s - loss: 1.8463 - acc: 0.9259 - mDice: 0.5240 - val_loss: 3.3584 - val_acc: 0.9385 - val_mDice: 0.4350

Epoch 00087: val_mDice did not improve from 0.44245
Epoch 88/300
 - 11s - loss: 1.8480 - acc: 0.9259 - mDice: 0.5236 - val_loss: 3.2520 - val_acc: 0.9377 - val_mDice: 0.4397

Epoch 00088: val_mDice did not improve from 0.44245
Epoch 89/300
 - 11s - loss: 1.8532 - acc: 0.9255 - mDice: 0.5227 - val_loss: 3.2267 - val_acc: 0.9330 - val_mDice: 0.4267

Epoch 00089: val_mDice did not improve from 0.44245
Epoch 90/300
 - 11s - loss: 1.8362 - acc: 0.9260 - mDice: 0.5258 - val_loss: 3.1448 - val_acc: 0.9391 - val_mDice: 0.4432

Epoch 00090: val_mDice improved from 0.44245 to 0.44316, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 91/300
 - 11s - loss: 1.8290 - acc: 0.9259 - mDice: 0.5271 - val_loss: 3.1581 - val_acc: 0.9399 - val_mDice: 0.4381

Epoch 00091: val_mDice did not improve from 0.44316
Epoch 92/300
 - 11s - loss: 1.8167 - acc: 0.9262 - mDice: 0.5304 - val_loss: 3.4067 - val_acc: 0.9331 - val_mDice: 0.4265

Epoch 00092: val_mDice did not improve from 0.44316
Epoch 93/300
 - 11s - loss: 1.8101 - acc: 0.9263 - mDice: 0.5320 - val_loss: 3.3445 - val_acc: 0.9394 - val_mDice: 0.4330

Epoch 00093: val_mDice did not improve from 0.44316
Epoch 94/300
 - 11s - loss: 1.8090 - acc: 0.9261 - mDice: 0.5320 - val_loss: 3.4710 - val_acc: 0.9376 - val_mDice: 0.4275

Epoch 00094: val_mDice did not improve from 0.44316
Epoch 95/300
 - 11s - loss: 1.8038 - acc: 0.9261 - mDice: 0.5333 - val_loss: 3.3203 - val_acc: 0.9381 - val_mDice: 0.4411

Epoch 00095: val_mDice did not improve from 0.44316
Epoch 96/300
 - 11s - loss: 1.8016 - acc: 0.9259 - mDice: 0.5342 - val_loss: 3.0973 - val_acc: 0.9373 - val_mDice: 0.4438

Epoch 00096: val_mDice improved from 0.44316 to 0.44378, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 97/300
 - 11s - loss: 1.7890 - acc: 0.9263 - mDice: 0.5370 - val_loss: 3.5354 - val_acc: 0.9371 - val_mDice: 0.4191

Epoch 00097: val_mDice did not improve from 0.44378
Epoch 98/300
 - 11s - loss: 1.7921 - acc: 0.9263 - mDice: 0.5362 - val_loss: 3.2885 - val_acc: 0.9403 - val_mDice: 0.4377

Epoch 00098: val_mDice did not improve from 0.44378
Epoch 99/300
 - 11s - loss: 1.7804 - acc: 0.9265 - mDice: 0.5382 - val_loss: 3.6403 - val_acc: 0.9392 - val_mDice: 0.4231

Epoch 00099: val_mDice did not improve from 0.44378
Epoch 100/300
 - 11s - loss: 1.7733 - acc: 0.9268 - mDice: 0.5402 - val_loss: 3.2054 - val_acc: 0.9373 - val_mDice: 0.4436

Epoch 00100: val_mDice did not improve from 0.44378
Epoch 101/300
 - 11s - loss: 1.7713 - acc: 0.9268 - mDice: 0.5403 - val_loss: 3.2871 - val_acc: 0.9383 - val_mDice: 0.4334

Epoch 00101: val_mDice did not improve from 0.44378
Epoch 102/300
 - 11s - loss: 1.7679 - acc: 0.9269 - mDice: 0.5412 - val_loss: 3.2434 - val_acc: 0.9368 - val_mDice: 0.4410

Epoch 00102: val_mDice did not improve from 0.44378
Epoch 103/300
 - 11s - loss: 1.7595 - acc: 0.9273 - mDice: 0.5436 - val_loss: 3.1225 - val_acc: 0.9396 - val_mDice: 0.4518

Epoch 00103: val_mDice improved from 0.44378 to 0.45183, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 104/300
 - 11s - loss: 1.7617 - acc: 0.9271 - mDice: 0.5434 - val_loss: 3.2833 - val_acc: 0.9398 - val_mDice: 0.4429

Epoch 00104: val_mDice did not improve from 0.45183
Epoch 105/300
 - 11s - loss: 1.7568 - acc: 0.9272 - mDice: 0.5440 - val_loss: 3.3837 - val_acc: 0.9362 - val_mDice: 0.4367

Epoch 00105: val_mDice did not improve from 0.45183
Epoch 106/300
 - 11s - loss: 1.7454 - acc: 0.9274 - mDice: 0.5463 - val_loss: 3.4621 - val_acc: 0.9372 - val_mDice: 0.4353

Epoch 00106: val_mDice did not improve from 0.45183
Epoch 107/300
 - 11s - loss: 1.7429 - acc: 0.9274 - mDice: 0.5466 - val_loss: 3.2883 - val_acc: 0.9374 - val_mDice: 0.4417

Epoch 00107: val_mDice did not improve from 0.45183
Epoch 108/300
 - 11s - loss: 1.7445 - acc: 0.9275 - mDice: 0.5467 - val_loss: 3.3030 - val_acc: 0.9354 - val_mDice: 0.4316

Epoch 00108: val_mDice did not improve from 0.45183
Epoch 109/300
 - 11s - loss: 1.7345 - acc: 0.9279 - mDice: 0.5494 - val_loss: 3.3601 - val_acc: 0.9377 - val_mDice: 0.4354

Epoch 00109: val_mDice did not improve from 0.45183
Epoch 110/300
 - 11s - loss: 1.7267 - acc: 0.9280 - mDice: 0.5506 - val_loss: 3.2930 - val_acc: 0.9359 - val_mDice: 0.4410

Epoch 00110: val_mDice did not improve from 0.45183
Epoch 111/300
 - 11s - loss: 1.7136 - acc: 0.9284 - mDice: 0.5538 - val_loss: 3.3627 - val_acc: 0.9384 - val_mDice: 0.4396

Epoch 00111: val_mDice did not improve from 0.45183
Epoch 112/300
 - 11s - loss: 1.7171 - acc: 0.9285 - mDice: 0.5529 - val_loss: 3.2384 - val_acc: 0.9393 - val_mDice: 0.4432

Epoch 00112: val_mDice did not improve from 0.45183
Epoch 113/300
 - 11s - loss: 1.7130 - acc: 0.9283 - mDice: 0.5538 - val_loss: 3.4217 - val_acc: 0.9332 - val_mDice: 0.4284

Epoch 00113: val_mDice did not improve from 0.45183
Epoch 114/300
 - 11s - loss: 1.7045 - acc: 0.9289 - mDice: 0.5566 - val_loss: 3.2183 - val_acc: 0.9375 - val_mDice: 0.4475

Epoch 00114: val_mDice did not improve from 0.45183
Epoch 115/300
 - 11s - loss: 1.7034 - acc: 0.9287 - mDice: 0.5561 - val_loss: 3.4410 - val_acc: 0.9368 - val_mDice: 0.4352

Epoch 00115: val_mDice did not improve from 0.45183
Epoch 116/300
 - 11s - loss: 1.7028 - acc: 0.9287 - mDice: 0.5564 - val_loss: 3.3234 - val_acc: 0.9399 - val_mDice: 0.4478

Epoch 00116: val_mDice did not improve from 0.45183
Epoch 117/300
 - 11s - loss: 1.6937 - acc: 0.9291 - mDice: 0.5594 - val_loss: 3.3298 - val_acc: 0.9396 - val_mDice: 0.4425

Epoch 00117: val_mDice did not improve from 0.45183
Epoch 118/300
 - 11s - loss: 1.6888 - acc: 0.9290 - mDice: 0.5598 - val_loss: 3.5977 - val_acc: 0.9395 - val_mDice: 0.4380

Epoch 00118: val_mDice did not improve from 0.45183
Epoch 119/300
 - 11s - loss: 1.6934 - acc: 0.9288 - mDice: 0.5585 - val_loss: 3.4290 - val_acc: 0.9385 - val_mDice: 0.4469

Epoch 00119: val_mDice did not improve from 0.45183
Epoch 120/300
 - 11s - loss: 1.6768 - acc: 0.9294 - mDice: 0.5626 - val_loss: 3.4175 - val_acc: 0.9385 - val_mDice: 0.4363

Epoch 00120: val_mDice did not improve from 0.45183
Epoch 121/300
 - 11s - loss: 1.6781 - acc: 0.9293 - mDice: 0.5622 - val_loss: 3.3982 - val_acc: 0.9361 - val_mDice: 0.4421

Epoch 00121: val_mDice did not improve from 0.45183
Epoch 122/300
 - 11s - loss: 1.6725 - acc: 0.9295 - mDice: 0.5640 - val_loss: 3.2398 - val_acc: 0.9385 - val_mDice: 0.4433

Epoch 00122: val_mDice did not improve from 0.45183
Epoch 123/300
 - 11s - loss: 1.6664 - acc: 0.9294 - mDice: 0.5652 - val_loss: 3.3584 - val_acc: 0.9374 - val_mDice: 0.4377

Epoch 00123: val_mDice did not improve from 0.45183
Epoch 124/300
 - 11s - loss: 1.6702 - acc: 0.9292 - mDice: 0.5652 - val_loss: 3.4486 - val_acc: 0.9406 - val_mDice: 0.4453

Epoch 00124: val_mDice did not improve from 0.45183
Epoch 125/300
 - 11s - loss: 1.6708 - acc: 0.9293 - mDice: 0.5647 - val_loss: 3.3198 - val_acc: 0.9409 - val_mDice: 0.4523

Epoch 00125: val_mDice improved from 0.45183 to 0.45230, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 126/300
 - 11s - loss: 1.6626 - acc: 0.9295 - mDice: 0.5665 - val_loss: 3.4061 - val_acc: 0.9389 - val_mDice: 0.4476

Epoch 00126: val_mDice did not improve from 0.45230
Epoch 127/300
 - 11s - loss: 1.6615 - acc: 0.9297 - mDice: 0.5673 - val_loss: 3.4159 - val_acc: 0.9416 - val_mDice: 0.4455

Epoch 00127: val_mDice did not improve from 0.45230
Epoch 128/300
 - 12s - loss: 1.6528 - acc: 0.9298 - mDice: 0.5688 - val_loss: 3.3847 - val_acc: 0.9380 - val_mDice: 0.4503

Epoch 00128: val_mDice did not improve from 0.45230
Epoch 129/300
 - 11s - loss: 1.6621 - acc: 0.9295 - mDice: 0.5674 - val_loss: 3.5722 - val_acc: 0.9391 - val_mDice: 0.4386

Epoch 00129: val_mDice did not improve from 0.45230
Epoch 130/300
 - 11s - loss: 1.6451 - acc: 0.9300 - mDice: 0.5710 - val_loss: 3.4809 - val_acc: 0.9375 - val_mDice: 0.4455

Epoch 00130: val_mDice did not improve from 0.45230
Epoch 131/300
 - 11s - loss: 1.6463 - acc: 0.9300 - mDice: 0.5705 - val_loss: 3.4832 - val_acc: 0.9345 - val_mDice: 0.4393

Epoch 00131: val_mDice did not improve from 0.45230
Epoch 132/300
 - 11s - loss: 1.6481 - acc: 0.9300 - mDice: 0.5710 - val_loss: 3.6830 - val_acc: 0.9350 - val_mDice: 0.4222

Epoch 00132: val_mDice did not improve from 0.45230
Epoch 133/300
 - 11s - loss: 1.6352 - acc: 0.9302 - mDice: 0.5729 - val_loss: 3.5743 - val_acc: 0.9390 - val_mDice: 0.4380

Epoch 00133: val_mDice did not improve from 0.45230
Epoch 134/300
 - 11s - loss: 1.6356 - acc: 0.9303 - mDice: 0.5729 - val_loss: 3.4939 - val_acc: 0.9399 - val_mDice: 0.4513

Epoch 00134: val_mDice did not improve from 0.45230
Epoch 135/300
 - 11s - loss: 1.6293 - acc: 0.9304 - mDice: 0.5750 - val_loss: 3.5212 - val_acc: 0.9395 - val_mDice: 0.4408

Epoch 00135: val_mDice did not improve from 0.45230
Epoch 136/300
 - 11s - loss: 1.6317 - acc: 0.9303 - mDice: 0.5741 - val_loss: 3.3138 - val_acc: 0.9383 - val_mDice: 0.4425

Epoch 00136: val_mDice did not improve from 0.45230
Epoch 137/300
 - 11s - loss: 1.6334 - acc: 0.9304 - mDice: 0.5741 - val_loss: 3.3025 - val_acc: 0.9400 - val_mDice: 0.4565

Epoch 00137: val_mDice improved from 0.45230 to 0.45645, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 138/300
 - 11s - loss: 1.6210 - acc: 0.9307 - mDice: 0.5769 - val_loss: 3.4596 - val_acc: 0.9396 - val_mDice: 0.4481

Epoch 00138: val_mDice did not improve from 0.45645
Epoch 139/300
 - 11s - loss: 1.6173 - acc: 0.9310 - mDice: 0.5776 - val_loss: 3.3900 - val_acc: 0.9381 - val_mDice: 0.4500

Epoch 00139: val_mDice did not improve from 0.45645
Epoch 140/300
 - 11s - loss: 1.6223 - acc: 0.9308 - mDice: 0.5769 - val_loss: 3.3877 - val_acc: 0.9396 - val_mDice: 0.4509

Epoch 00140: val_mDice did not improve from 0.45645
Epoch 141/300
 - 11s - loss: 1.6209 - acc: 0.9308 - mDice: 0.5770 - val_loss: 3.6194 - val_acc: 0.9396 - val_mDice: 0.4468

Epoch 00141: val_mDice did not improve from 0.45645
Epoch 142/300
 - 11s - loss: 1.6117 - acc: 0.9312 - mDice: 0.5791 - val_loss: 3.5637 - val_acc: 0.9380 - val_mDice: 0.4429

Epoch 00142: val_mDice did not improve from 0.45645
Epoch 143/300
 - 11s - loss: 1.6172 - acc: 0.9309 - mDice: 0.5781 - val_loss: 3.4041 - val_acc: 0.9380 - val_mDice: 0.4545

Epoch 00143: val_mDice did not improve from 0.45645
Epoch 144/300
 - 11s - loss: 1.6107 - acc: 0.9312 - mDice: 0.5794 - val_loss: 3.4501 - val_acc: 0.9400 - val_mDice: 0.4513

Epoch 00144: val_mDice did not improve from 0.45645
Epoch 145/300
 - 11s - loss: 1.6068 - acc: 0.9314 - mDice: 0.5806 - val_loss: 3.5370 - val_acc: 0.9392 - val_mDice: 0.4414

Epoch 00145: val_mDice did not improve from 0.45645
Epoch 146/300
 - 11s - loss: 1.6049 - acc: 0.9313 - mDice: 0.5807 - val_loss: 3.4893 - val_acc: 0.9412 - val_mDice: 0.4476

Epoch 00146: val_mDice did not improve from 0.45645
Epoch 147/300
 - 11s - loss: 1.6040 - acc: 0.9314 - mDice: 0.5811 - val_loss: 3.5711 - val_acc: 0.9382 - val_mDice: 0.4507

Epoch 00147: val_mDice did not improve from 0.45645
Epoch 148/300
 - 11s - loss: 1.6015 - acc: 0.9315 - mDice: 0.5821 - val_loss: 3.4287 - val_acc: 0.9386 - val_mDice: 0.4499

Epoch 00148: val_mDice did not improve from 0.45645
Epoch 149/300
 - 11s - loss: 1.6012 - acc: 0.9313 - mDice: 0.5822 - val_loss: 3.5597 - val_acc: 0.9385 - val_mDice: 0.4462

Epoch 00149: val_mDice did not improve from 0.45645
Epoch 150/300
 - 11s - loss: 1.5960 - acc: 0.9316 - mDice: 0.5834 - val_loss: 3.4828 - val_acc: 0.9396 - val_mDice: 0.4547

Epoch 00150: val_mDice did not improve from 0.45645
Epoch 151/300
 - 11s - loss: 1.5830 - acc: 0.9321 - mDice: 0.5862 - val_loss: 3.5511 - val_acc: 0.9396 - val_mDice: 0.4527

Epoch 00151: val_mDice did not improve from 0.45645
Epoch 152/300
 - 11s - loss: 1.5912 - acc: 0.9318 - mDice: 0.5845 - val_loss: 3.6373 - val_acc: 0.9390 - val_mDice: 0.4513

Epoch 00152: val_mDice did not improve from 0.45645
Epoch 153/300
 - 11s - loss: 1.5960 - acc: 0.9317 - mDice: 0.5838 - val_loss: 3.4233 - val_acc: 0.9398 - val_mDice: 0.4506

Epoch 00153: val_mDice did not improve from 0.45645
Epoch 154/300
 - 11s - loss: 1.5891 - acc: 0.9319 - mDice: 0.5851 - val_loss: 3.4147 - val_acc: 0.9404 - val_mDice: 0.4565

Epoch 00154: val_mDice improved from 0.45645 to 0.45648, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 155/300
 - 11s - loss: 1.5864 - acc: 0.9321 - mDice: 0.5859 - val_loss: 3.5807 - val_acc: 0.9388 - val_mDice: 0.4462

Epoch 00155: val_mDice did not improve from 0.45648
Epoch 156/300
 - 11s - loss: 1.5881 - acc: 0.9319 - mDice: 0.5856 - val_loss: 3.4178 - val_acc: 0.9372 - val_mDice: 0.4497

Epoch 00156: val_mDice did not improve from 0.45648
Epoch 157/300
 - 11s - loss: 1.5818 - acc: 0.9322 - mDice: 0.5870 - val_loss: 3.3746 - val_acc: 0.9385 - val_mDice: 0.4610

Epoch 00157: val_mDice improved from 0.45648 to 0.46104, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 158/300
 - 11s - loss: 1.5733 - acc: 0.9323 - mDice: 0.5889 - val_loss: 3.6095 - val_acc: 0.9392 - val_mDice: 0.4585

Epoch 00158: val_mDice did not improve from 0.46104
Epoch 159/300
 - 11s - loss: 1.5758 - acc: 0.9323 - mDice: 0.5888 - val_loss: 3.7095 - val_acc: 0.9391 - val_mDice: 0.4447

Epoch 00159: val_mDice did not improve from 0.46104
Epoch 160/300
 - 11s - loss: 1.5794 - acc: 0.9322 - mDice: 0.5881 - val_loss: 3.4366 - val_acc: 0.9394 - val_mDice: 0.4540

Epoch 00160: val_mDice did not improve from 0.46104
Epoch 161/300
 - 11s - loss: 1.5724 - acc: 0.9324 - mDice: 0.5894 - val_loss: 3.5544 - val_acc: 0.9385 - val_mDice: 0.4548

Epoch 00161: val_mDice did not improve from 0.46104
Epoch 162/300
 - 11s - loss: 1.5718 - acc: 0.9325 - mDice: 0.5901 - val_loss: 3.5497 - val_acc: 0.9414 - val_mDice: 0.4618

Epoch 00162: val_mDice improved from 0.46104 to 0.46179, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 163/300
 - 11s - loss: 1.5663 - acc: 0.9325 - mDice: 0.5909 - val_loss: 3.6041 - val_acc: 0.9400 - val_mDice: 0.4470

Epoch 00163: val_mDice did not improve from 0.46179
Epoch 164/300
 - 11s - loss: 1.5718 - acc: 0.9324 - mDice: 0.5897 - val_loss: 3.4958 - val_acc: 0.9380 - val_mDice: 0.4521

Epoch 00164: val_mDice did not improve from 0.46179
Epoch 165/300
 - 11s - loss: 1.5671 - acc: 0.9325 - mDice: 0.5902 - val_loss: 3.3109 - val_acc: 0.9402 - val_mDice: 0.4670

Epoch 00165: val_mDice improved from 0.46179 to 0.46698, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 166/300
 - 11s - loss: 1.5602 - acc: 0.9328 - mDice: 0.5927 - val_loss: 3.5404 - val_acc: 0.9399 - val_mDice: 0.4543

Epoch 00166: val_mDice did not improve from 0.46698
Epoch 167/300
 - 11s - loss: 1.5636 - acc: 0.9328 - mDice: 0.5916 - val_loss: 3.6603 - val_acc: 0.9385 - val_mDice: 0.4476

Epoch 00167: val_mDice did not improve from 0.46698
Epoch 168/300
 - 11s - loss: 1.5589 - acc: 0.9328 - mDice: 0.5925 - val_loss: 3.6737 - val_acc: 0.9388 - val_mDice: 0.4503

Epoch 00168: val_mDice did not improve from 0.46698
Epoch 169/300
 - 11s - loss: 1.5613 - acc: 0.9326 - mDice: 0.5931 - val_loss: 3.5101 - val_acc: 0.9397 - val_mDice: 0.4506

Epoch 00169: val_mDice did not improve from 0.46698
Epoch 170/300
 - 11s - loss: 1.5573 - acc: 0.9328 - mDice: 0.5931 - val_loss: 3.4971 - val_acc: 0.9395 - val_mDice: 0.4543

Epoch 00170: val_mDice did not improve from 0.46698
Epoch 171/300
 - 11s - loss: 1.5508 - acc: 0.9331 - mDice: 0.5953 - val_loss: 3.6433 - val_acc: 0.9398 - val_mDice: 0.4508

Epoch 00171: val_mDice did not improve from 0.46698
Epoch 172/300
 - 12s - loss: 1.5536 - acc: 0.9329 - mDice: 0.5940 - val_loss: 3.4995 - val_acc: 0.9399 - val_mDice: 0.4573

Epoch 00172: val_mDice did not improve from 0.46698
Epoch 173/300
 - 11s - loss: 1.5569 - acc: 0.9329 - mDice: 0.5940 - val_loss: 3.7165 - val_acc: 0.9357 - val_mDice: 0.4302

Epoch 00173: val_mDice did not improve from 0.46698
Epoch 174/300
 - 11s - loss: 1.5554 - acc: 0.9329 - mDice: 0.5946 - val_loss: 3.6820 - val_acc: 0.9380 - val_mDice: 0.4490

Epoch 00174: val_mDice did not improve from 0.46698
Epoch 175/300
 - 11s - loss: 1.5443 - acc: 0.9331 - mDice: 0.5967 - val_loss: 3.4628 - val_acc: 0.9385 - val_mDice: 0.4515

Epoch 00175: val_mDice did not improve from 0.46698
Epoch 176/300
 - 12s - loss: 1.5438 - acc: 0.9331 - mDice: 0.5970 - val_loss: 3.7199 - val_acc: 0.9402 - val_mDice: 0.4450

Epoch 00176: val_mDice did not improve from 0.46698
Epoch 177/300
 - 11s - loss: 1.5454 - acc: 0.9331 - mDice: 0.5967 - val_loss: 3.5605 - val_acc: 0.9413 - val_mDice: 0.4558

Epoch 00177: val_mDice did not improve from 0.46698
Epoch 178/300
 - 11s - loss: 1.5365 - acc: 0.9332 - mDice: 0.5985 - val_loss: 3.4494 - val_acc: 0.9388 - val_mDice: 0.4561

Epoch 00178: val_mDice did not improve from 0.46698
Epoch 179/300
 - 11s - loss: 1.5383 - acc: 0.9333 - mDice: 0.5990 - val_loss: 3.5063 - val_acc: 0.9387 - val_mDice: 0.4561

Epoch 00179: val_mDice did not improve from 0.46698
Epoch 180/300
 - 12s - loss: 1.5377 - acc: 0.9334 - mDice: 0.5987 - val_loss: 3.5937 - val_acc: 0.9397 - val_mDice: 0.4494

Epoch 00180: val_mDice did not improve from 0.46698
Epoch 181/300
 - 11s - loss: 1.5391 - acc: 0.9333 - mDice: 0.5979 - val_loss: 3.5247 - val_acc: 0.9397 - val_mDice: 0.4576

Epoch 00181: val_mDice did not improve from 0.46698
Epoch 182/300
 - 11s - loss: 1.5315 - acc: 0.9335 - mDice: 0.6000 - val_loss: 3.5208 - val_acc: 0.9413 - val_mDice: 0.4602

Epoch 00182: val_mDice did not improve from 0.46698
Epoch 183/300
 - 11s - loss: 1.5344 - acc: 0.9332 - mDice: 0.5989 - val_loss: 3.5617 - val_acc: 0.9393 - val_mDice: 0.4567

Epoch 00183: val_mDice did not improve from 0.46698
Epoch 184/300
 - 11s - loss: 1.5264 - acc: 0.9335 - mDice: 0.6012 - val_loss: 3.4921 - val_acc: 0.9402 - val_mDice: 0.4609

Epoch 00184: val_mDice did not improve from 0.46698
Epoch 185/300
 - 11s - loss: 1.5362 - acc: 0.9332 - mDice: 0.5996 - val_loss: 3.6678 - val_acc: 0.9390 - val_mDice: 0.4451

Epoch 00185: val_mDice did not improve from 0.46698
Epoch 186/300
 - 11s - loss: 1.5332 - acc: 0.9332 - mDice: 0.5995 - val_loss: 4.0231 - val_acc: 0.9315 - val_mDice: 0.4263

Epoch 00186: val_mDice did not improve from 0.46698
Epoch 187/300
 - 11s - loss: 1.5318 - acc: 0.9334 - mDice: 0.6008 - val_loss: 3.5764 - val_acc: 0.9383 - val_mDice: 0.4563

Epoch 00187: val_mDice did not improve from 0.46698
Epoch 188/300
 - 11s - loss: 1.5227 - acc: 0.9337 - mDice: 0.6013 - val_loss: 3.6278 - val_acc: 0.9392 - val_mDice: 0.4517

Epoch 00188: val_mDice did not improve from 0.46698
Epoch 189/300
 - 11s - loss: 1.5277 - acc: 0.9335 - mDice: 0.6006 - val_loss: 3.4010 - val_acc: 0.9401 - val_mDice: 0.4558

Epoch 00189: val_mDice did not improve from 0.46698
Epoch 190/300
 - 11s - loss: 1.5254 - acc: 0.9335 - mDice: 0.6015 - val_loss: 3.7477 - val_acc: 0.9369 - val_mDice: 0.4397

Epoch 00190: val_mDice did not improve from 0.46698
Epoch 191/300
 - 11s - loss: 1.5193 - acc: 0.9337 - mDice: 0.6024 - val_loss: 3.5770 - val_acc: 0.9398 - val_mDice: 0.4592

Epoch 00191: val_mDice did not improve from 0.46698
Epoch 192/300
 - 11s - loss: 1.5212 - acc: 0.9337 - mDice: 0.6022 - val_loss: 3.5381 - val_acc: 0.9369 - val_mDice: 0.4537

Epoch 00192: val_mDice did not improve from 0.46698
Epoch 193/300
 - 11s - loss: 1.5156 - acc: 0.9339 - mDice: 0.6037 - val_loss: 3.4232 - val_acc: 0.9402 - val_mDice: 0.4643

Epoch 00193: val_mDice did not improve from 0.46698
Epoch 194/300
 - 11s - loss: 1.5150 - acc: 0.9339 - mDice: 0.6040 - val_loss: 3.7847 - val_acc: 0.9375 - val_mDice: 0.4499

Epoch 00194: val_mDice did not improve from 0.46698
Epoch 195/300
 - 11s - loss: 1.5143 - acc: 0.9340 - mDice: 0.6041 - val_loss: 3.6287 - val_acc: 0.9389 - val_mDice: 0.4492

Epoch 00195: val_mDice did not improve from 0.46698
Restoring model weights from the end of the best epoch
Epoch 00195: early stopping
{'val_loss': [52.882362649554295, 17.005795095648086, 9.59827507961364, 7.322239640213194, 6.314307068430242, 5.7041767963341306, 5.278515402759824, 5.04986138890187, 4.8968803107383705, 4.691367598960087, 4.5813118269046145, 4.212985253050213, 4.088197561158311, 4.160984407312104, 3.8301575972831676, 3.733749672903546, 3.654499433446853, 3.7447899669586193, 3.728199242569861, 3.4241018427003707, 3.6895877713160146, 3.4663588832620356, 3.0433855372968885, 3.2713301346770356, 3.2125782468577935, 3.2051664885754385, 3.108732963734794, 3.2281438539336835, 3.108643632736944, 3.1337535682444773, 3.2481068608218004, 3.001825700292275, 3.0950112899854068, 3.050940417728963, 3.09676629366974, 3.1043511167434708, 3.0306057384947227, 2.88824941568254, 3.064373548009566, 3.058290697767266, 3.194718272780024, 3.046286529639647, 3.0885458076372743, 3.0945597038648667, 2.9752644829984223, 3.344829239882529, 3.0191364282564748, 3.1171920447654666, 3.005487259743469, 3.1777914853855256, 3.3870095427458486, 3.178617213741832, 3.009973770584024, 2.9784322092974826, 3.0224760243935243, 3.1181130630540705, 3.3013350902613077, 3.0466971707397272, 3.182225171254859, 3.0989179144657792, 3.523691039133285, 2.9649910674031292, 3.291982032846482, 2.9321682091270174, 3.019092626381843, 3.1293319539566125, 3.025791729201696, 3.1645605784752187, 3.106252777390182, 3.2287867387889753, 3.2795197429312837, 3.147413116879761, 2.957910338461044, 3.092759796373901, 3.1342227117468915, 3.139351795427501, 3.1006357919069982, 3.184312308251503, 3.1852123527565883, 3.082412056979679, 3.306244282051921, 3.351587345734948, 3.4205291881891235, 3.2703719612299684, 3.1567207426719723, 3.223949653273892, 3.3583642708669816, 3.25202105371725, 3.226729526228848, 3.144788897064115, 3.1581348624923047, 3.406735808366821, 3.344496151831533, 3.471018858387002, 3.320269317632275, 3.0972850948039974, 3.535397106205069, 3.2885249261300835, 3.6402594716110754, 3.2053639003120007, 3.2871385962436244, 3.2433971993270374, 3.122523758693465, 3.283322880798507, 3.3837385553023998, 3.4621189517368163, 3.2883118272625973, 3.3029617038777186, 3.3600708891948066, 3.2930091499750103, 3.3626767959711805, 3.2384321490480077, 3.421711097648811, 3.2182797851218354, 3.441017718054354, 3.323352058817233, 3.3298258853721476, 3.597749491044808, 3.4290401432219717, 3.417489510140426, 3.398169908938663, 3.2397597504424906, 3.3583822133729146, 3.448603010292919, 3.319752750075644, 3.4061280041046085, 3.4158608595441495, 3.3846827369360697, 3.5721633933218464, 3.4809315322587886, 3.483249337722858, 3.6829782785138203, 3.574347172952479, 3.493925941708897, 3.521171912550926, 3.3137515180611183, 3.302526232032549, 3.4595643416313187, 3.38999682434258, 3.387692636943289, 3.6193841364500776, 3.5637228613985434, 3.4041091047138687, 3.4501089977011796, 3.5369728871931634, 3.4893043779635002, 3.5711155646436272, 3.428743569224718, 3.5597243292168494, 3.4828014512147223, 3.5510531627085236, 3.6373431858650984, 3.4232839466560456, 3.4146778504585935, 3.580699679264355, 3.417776630304399, 3.3745852923464206, 3.6094898285699033, 3.709522421427426, 3.4366174431606415, 3.5543731470547972, 3.5497308735336577, 3.6040753118161644, 3.495779762443687, 3.3108544745704247, 3.5404298156499863, 3.660254395212091, 3.673653952555642, 3.510069303524991, 3.4970869368297004, 3.643316220713868, 3.499474840726526, 3.7164894687898813, 3.682000819593668, 3.4627908400836445, 3.719869698264769, 3.560472430883064, 3.449421389205825, 3.5062985150143504, 3.59367929044224, 3.5247049374239787, 3.5208072723111226, 3.5616987375099036, 3.4921176537339176, 3.6677503354758736, 4.023144112057274, 3.5764468043066917, 3.627807761365104, 3.4010342561329403, 3.7476889715929116, 3.5769759753630277, 3.5380921900449764, 3.4231622659911713, 3.7847070599950494, 3.628654485745799], 'val_acc': [0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9048649441628229, 0.9060050419398716, 0.9072756426674979, 0.9059546873683021, 0.9075114727020264, 0.9075572150094169, 0.9085599978764852, 0.909116288026174, 0.9094848717961993, 0.9126304813793727, 0.9129395768755958, 0.915329669203077, 0.9179647394589016, 0.9162293899626959, 0.9184844266800654, 0.9214354441279456, 0.9224175612131754, 0.9225961651120868, 0.9233997066815695, 0.9209569862910679, 0.9227884440194993, 0.9224588161423093, 0.920975270725432, 0.9279074839183262, 0.923422634601593, 0.9246405675297692, 0.9300435242198762, 0.9250343612262181, 0.9311492812065851, 0.9291140153294518, 0.9277403808775402, 0.9304716132936024, 0.9290842470668611, 0.9307234599476769, 0.9301030068170457, 0.9254349697203863, 0.9318521306628272, 0.9307371633393424, 0.9328525634039015, 0.9324817004657927, 0.9287202443395343, 0.9332096860522315, 0.9333333401452928, 0.932870898927961, 0.9337133538155329, 0.9334157449858529, 0.934125474521092, 0.9335119326909384, 0.9340361668950036, 0.93216115520114, 0.933800376596905, 0.9320879039310274, 0.9302724628221422, 0.9355196754137675, 0.9353571392240978, 0.933285239196959, 0.9347596310433888, 0.9370558630852472, 0.9345833233424595, 0.9329189459482828, 0.93400867496218, 0.9368978653635297, 0.9366575224058968, 0.9374725279353914, 0.9352289182799203, 0.9357898490769523, 0.9361996224948338, 0.9327930410703024, 0.9347802400588989, 0.936336997009459, 0.9336767196655273, 0.9343818709963844, 0.9384798663003104, 0.9368475306601751, 0.9354120861916315, 0.9388026595115662, 0.9384935640153431, 0.9376579750151861, 0.9329647450220018, 0.9391346119699024, 0.9399267264774868, 0.9330723683039347, 0.9394093212627229, 0.9376350896699088, 0.9381433186076936, 0.937344335374378, 0.9371474243345714, 0.9403159107480731, 0.939203313418797, 0.937275645278749, 0.9382829609371367, 0.9367902761413938, 0.9395581767672584, 0.9398397547858102, 0.936169874100458, 0.9372344329243615, 0.9373534690766108, 0.9353502818516323, 0.937662524836404, 0.9359294914063954, 0.9384295145670573, 0.9393292182967776, 0.93315475327628, 0.937500019868215, 0.9367673822811672, 0.9399381904374986, 0.9395901759465536, 0.9395420977047512, 0.9384890028408596, 0.9384821483067104, 0.9361355361484346, 0.938548553557623, 0.937424449693589, 0.9405998019945054, 0.940913472856794, 0.9389468828837076, 0.9415544981048221, 0.9380059242248535, 0.9390705369767689, 0.937456522669111, 0.9344597118241447, 0.9349862365495591, 0.939029282047635, 0.9398717908632188, 0.9394619975771222, 0.9382875363032023, 0.9399839639663696, 0.939585634640285, 0.9381318887074789, 0.939567282086327, 0.9395902156829834, 0.937992200964973, 0.9380242427190145, 0.9400205952780587, 0.9391781290372213, 0.9411653053192865, 0.9381730499721709, 0.9385508384023394, 0.9384615478061494, 0.9396497436932155, 0.9395833469572521, 0.9390384640012469, 0.9397802296138945, 0.9404372629665193, 0.9387522708801996, 0.9371771925971621, 0.9384706900233314, 0.9392307542619251, 0.9391048408689953, 0.939409335454305, 0.9384821596599761, 0.9413987937427702, 0.9399656511488415, 0.9380220061256772, 0.9401785617782956, 0.939892399878729, 0.9385027431306385, 0.93881870167596, 0.9397160978544326, 0.9394986118589129, 0.9397618912515187, 0.9399061174619765, 0.9357394859904334, 0.9379555923598153, 0.9384707042149135, 0.940190022899991, 0.9413072239784968, 0.9388026765414647, 0.938665284996941, 0.9396519916398185, 0.9396543020293826, 0.9412980845996312, 0.9393246599606105, 0.9402174807730175, 0.938990382921128, 0.9315361919857207, 0.9383081254504976, 0.9391574831235976, 0.9401465200242543, 0.9368566842306227, 0.9398351595515296, 0.9369116368747893, 0.9401717072441464, 0.9375091620853969, 0.9389263079279945], 'val_mDice': [0.014797833160541597, 0.014945488273432212, 0.015062040738051846, 0.02033592942392542, 0.027031818382619394, 0.033197366122511174, 0.05205303599082288, 0.06104804770577522, 0.0649084537955267, 0.0738798842898437, 0.0836235432042962, 0.10480940448386329, 0.11807998527018797, 0.12471514864869061, 0.14689736831046285, 0.16946038603782654, 0.18817036818446858, 0.19702968266349108, 0.21044033395481251, 0.241031967724363, 0.2386933271107929, 0.2623230380760062, 0.29901415739385856, 0.2884682493195647, 0.3031352050602436, 0.30950952640601564, 0.3218846665251823, 0.32008095475889387, 0.3353670494897025, 0.33682611930583206, 0.32961161842658404, 0.3513879818575723, 0.3484021088197118, 0.3597118418131556, 0.35504150621238206, 0.35601462743112017, 0.36607796148884864, 0.3777790715297063, 0.37074428388760206, 0.37206967139527913, 0.3667630092019126, 0.38199476968674434, 0.3792785760902223, 0.38694557991056217, 0.38606681107055574, 0.3680179807401839, 0.3963832789588542, 0.3878304309078625, 0.39498353093152955, 0.38551031620729537, 0.373933520168066, 0.39276792144491557, 0.400586897063823, 0.408977366629101, 0.4064510595940408, 0.4045562517075312, 0.3921654529514767, 0.41451016618382364, 0.4024664635459582, 0.39803148415826617, 0.38750104996420087, 0.4167333844871748, 0.3988227517831893, 0.4226014938737665, 0.41835200431801023, 0.4169922630701746, 0.4218790045096761, 0.42158014362766627, 0.4233592145499729, 0.41500323868933175, 0.4139870233124211, 0.42641267730366617, 0.43589356754507336, 0.42901651845091865, 0.4281390422866458, 0.4261940827681905, 0.43162584943430765, 0.4263548890040034, 0.4237729593047074, 0.4349988753951731, 0.4234898775106385, 0.41782998683906736, 0.42321474317993435, 0.4272245635234174, 0.4378049543925694, 0.4424473959065619, 0.43495721902166096, 0.43967527399460477, 0.42674844428187325, 0.4431620410510472, 0.43810232409409117, 0.42653557587237584, 0.4329555259928817, 0.4275232594282854, 0.44113691754284357, 0.443784670283397, 0.41914260298723266, 0.4376527225332601, 0.4231045359656924, 0.4435932267279852, 0.4334056863472575, 0.4410064545060907, 0.4518348553350994, 0.442875706014179, 0.43671112543060664, 0.4352894556664285, 0.4417239647536051, 0.43158808119949843, 0.43537610343524386, 0.4410468496027447, 0.4396159867090838, 0.4432448496421178, 0.4284077249467373, 0.44753971092757727, 0.43519301747991923, 0.4478402368369557, 0.44250623012582463, 0.4380362307031949, 0.44685219707233564, 0.436304757637637, 0.4420683609233016, 0.44329804962589625, 0.43773284412565683, 0.4452995437951315, 0.4522989931560698, 0.4476352578827313, 0.4454538566725595, 0.45028588530563174, 0.43860538073238875, 0.44546633018624215, 0.43926512893466724, 0.422189540628876, 0.438023570392813, 0.4512642692951929, 0.440823235504684, 0.44247922194855555, 0.45645331343015033, 0.4480845779180527, 0.4500122347048351, 0.4509045460394451, 0.44681583486852194, 0.4429440794601327, 0.4545220596094926, 0.4513166841296923, 0.4413650355168751, 0.4475542101122084, 0.450742195582106, 0.44991756461205934, 0.44618340120429084, 0.45466351154304685, 0.45265347599273636, 0.45125775943909374, 0.4506392432820229, 0.45647758103552316, 0.4462329339058626, 0.44972444396643413, 0.46104176253789947, 0.4584992503126462, 0.4447283780290967, 0.4539817016394365, 0.4547957685731706, 0.4617913549854642, 0.4469986433784167, 0.4520646594464779, 0.4669788437230246, 0.4542863667011261, 0.44755737235148746, 0.4502634959561484, 0.4506052326233614, 0.45432621151918456, 0.4508006739474478, 0.45734648193631855, 0.4301695990420523, 0.4490309325712068, 0.45147705060385523, 0.44504897633478757, 0.4557536881239641, 0.45613824239089373, 0.4560997457731338, 0.44936896913817953, 0.45755858577433084, 0.4602400105269182, 0.4566747996778715, 0.46087418407911346, 0.4450907444670087, 0.4262511808247793, 0.4563380728165309, 0.45174958787503694, 0.4558082517413866, 0.43965273226300877, 0.4592034268592085, 0.4536596563245569, 0.46430887477028937, 0.4498934531140895, 0.44915376887435005], 'loss': [189.08306254351416, 43.37089612926618, 19.998967265770204, 13.415055311183513, 10.60872780961845, 8.905088663698736, 7.888347941877204, 7.183043469347567, 6.679733056656216, 6.264730198203609, 5.88867256134398, 5.544011240040426, 5.21895511591161, 4.926100915627877, 4.654231678986793, 4.412108489950889, 4.216325344229471, 4.041189588883582, 3.8813577906713563, 3.7557907993907937, 3.61929213635283, 3.5079687667623842, 3.3905219850637605, 3.30694903788687, 3.1974094559375503, 3.110040643483153, 3.0398314298865468, 2.9735677842807053, 2.915511635748527, 2.8571699734296736, 2.8013903510025844, 2.759332845683086, 2.712618771972789, 2.673152719294149, 2.634467378273069, 2.5960635356138226, 2.5649441759182303, 2.548283770798212, 2.506036418284528, 2.47500015516283, 2.4495582740523347, 2.4234242106945683, 2.4144508816590537, 2.373775043835961, 2.3552955115763052, 2.334030457859396, 2.315375471427672, 2.3040687462624865, 2.2859509666002347, 2.258813466643331, 2.245805090692247, 2.2274138122389076, 2.220296057750201, 2.1956828102018418, 2.1720443491947683, 2.1512997089288173, 2.1493300240876843, 2.1333393604741153, 2.120799373748423, 2.1025847879613506, 2.0963114067978177, 2.086597224662638, 2.0727287065047437, 2.0616242322982425, 2.043858701607752, 2.030754692181701, 2.0160182265387765, 2.026967600013839, 1.9966703276195714, 1.9965123307941643, 1.9784516774782996, 1.9759744618792559, 1.9619564621669607, 1.9571278405966317, 1.9628353839851838, 1.9335111720910854, 1.9228776968902124, 1.9269292943390757, 1.9184216653218222, 1.91418699690067, 1.9058854194512045, 1.8898744306882407, 1.8860486860516124, 1.8735300239304116, 1.8649111232570033, 1.8595373453330002, 1.846257246940242, 1.8480391917670878, 1.8532136008071605, 1.8362442177569727, 1.8290222198351376, 1.8166670357994843, 1.8100607379544142, 1.8090497730599677, 1.803752151830867, 1.8015537105223474, 1.788970511635765, 1.792147475031729, 1.7804097858540742, 1.7733426102098988, 1.7712665762495898, 1.767906794617901, 1.7595147129793904, 1.761728634810388, 1.756770723261814, 1.7454199887250508, 1.7428866654479345, 1.7445286744726125, 1.7345327862384163, 1.726713144597645, 1.7135657853466113, 1.7171258497311703, 1.7129866778563276, 1.704496065959412, 1.7034337048082884, 1.7028183238640073, 1.6936922460386512, 1.688829758320329, 1.6933564725629116, 1.676811092548433, 1.6780876217721308, 1.6724936447323735, 1.6664450573787906, 1.6701977278010933, 1.6708461620087383, 1.6626046414960896, 1.6614984745967358, 1.6527553936270867, 1.6621175294945598, 1.6450550372324309, 1.6462888475877369, 1.64813976789316, 1.6351618682209734, 1.6356413230193656, 1.6292888699226902, 1.6316594169942864, 1.6333554929820058, 1.6209590993038785, 1.617319723717252, 1.6222785341502937, 1.6209283069951101, 1.611683944137151, 1.617157351517921, 1.610660034911077, 1.6067778567300726, 1.6048980704019449, 1.6039819031155176, 1.6014685958411299, 1.6011862742411325, 1.5959635268108956, 1.5830111100950843, 1.5911659102277165, 1.5960344675305678, 1.5891425417226612, 1.5863937799793137, 1.5880951841994013, 1.5818119557441166, 1.5732617385359382, 1.5758286255367095, 1.5794110157964592, 1.5724251407498635, 1.5717781735725984, 1.5663189912361746, 1.5717791606861893, 1.567093054821249, 1.5602212722576811, 1.5636377756320283, 1.5589022316729515, 1.5612674378144482, 1.557285491110664, 1.5508150240118757, 1.5536303408738388, 1.5568510532976692, 1.5554466971042, 1.5442656736188205, 1.5437533614353705, 1.5453771463542723, 1.5364667589043477, 1.538308067888981, 1.5376911910025628, 1.53914262302951, 1.5314805027789087, 1.5343625275459458, 1.5263817772737514, 1.536168022998753, 1.5331529812797178, 1.5317607676337077, 1.5226633180789642, 1.527674883736198, 1.5254306185330548, 1.5192757235433823, 1.5212236024842962, 1.515569029871108, 1.515035527966398, 1.5142550271945996], 'acc': [0.3971805206631244, 0.7877586239998066, 0.859597124988136, 0.8675526414552773, 0.8684553511114692, 0.868608305351753, 0.8682928167259116, 0.8679064078461534, 0.868002546323111, 0.8684423710308439, 0.869346512025874, 0.8705644945728772, 0.8718529628930534, 0.8728475643647574, 0.8739193154364991, 0.8752364716778974, 0.8767072785698152, 0.877990653169208, 0.8795268742799621, 0.880824636443171, 0.8825644231587217, 0.8842897043651944, 0.8860864479095462, 0.8875863169437892, 0.8900255927787665, 0.891254767071702, 0.8926835856801723, 0.8942396888297165, 0.895642099753726, 0.8969923868927625, 0.898509586537668, 0.8996173414003649, 0.900727038774643, 0.9019041667445123, 0.9033918712027436, 0.9042666083543269, 0.9049731814978508, 0.9055820735796628, 0.9069450417878611, 0.907727203905548, 0.9085440801268762, 0.9093872327921315, 0.9095865575040446, 0.9106801077651684, 0.9115054420889838, 0.9119215619938669, 0.9126503892387985, 0.912633284084687, 0.9133040637484584, 0.9140987624454664, 0.9145207704205216, 0.9149080866307497, 0.9149919711879299, 0.9158104682603001, 0.9164307102983297, 0.9167836604031934, 0.9171911379563364, 0.9175116765446256, 0.9178427305896318, 0.918407544892993, 0.9184157937644464, 0.9185524364875557, 0.9191435941938308, 0.9195566567722855, 0.9199707638167085, 0.9202767461049941, 0.9206952765908617, 0.9201549317121092, 0.9213954136163393, 0.9213186677007379, 0.9217286942688652, 0.922103083257802, 0.9224708175323638, 0.9225892268648399, 0.9226059309889729, 0.9235051835872591, 0.9236826578592183, 0.923744663915033, 0.9238474827891074, 0.9239899413259773, 0.9243192118510866, 0.9249366719942919, 0.924993536652502, 0.9254357012590968, 0.9256050366529822, 0.9257549357271755, 0.9259429978616486, 0.9258695011863318, 0.9254871844234139, 0.9259515986728466, 0.9258897053682899, 0.9262312311806246, 0.9262504415611369, 0.9261331058477008, 0.9260944717992632, 0.9259350074263742, 0.9263394466364846, 0.9262754695939953, 0.9265278570565135, 0.9267626128055145, 0.9268373408734925, 0.9268600269160979, 0.9272963054979471, 0.9271101437254818, 0.9271644365357736, 0.9274013893715053, 0.9274476647308068, 0.9275425747223666, 0.9278964576565831, 0.9279679884473, 0.9283765294727894, 0.9285142643501793, 0.9282702419117556, 0.9289051702033124, 0.9286757957988192, 0.9286825152543875, 0.929120694818767, 0.9290094924772868, 0.9287717742613797, 0.9293620753821459, 0.9292793071106814, 0.9295047642708009, 0.929350163680592, 0.9292288817765512, 0.9292680906472648, 0.9294690824161725, 0.929652926617099, 0.9297915004410678, 0.9295370252302025, 0.9300153106537218, 0.9299519141216326, 0.9300109543206491, 0.9301880576809592, 0.9303221530667937, 0.9304033006366011, 0.9302984743718584, 0.9304094190470672, 0.9307308083833562, 0.9309834761842036, 0.9308267640819295, 0.9307569694215676, 0.9311646329361464, 0.9308922041673662, 0.9312158680982573, 0.9314213062679466, 0.9312536853780539, 0.931427890438093, 0.9314945094475563, 0.9313434771611692, 0.931588215924008, 0.932094983806816, 0.9317811945281278, 0.9317158702735798, 0.9318713306093667, 0.9320904175302577, 0.9318869710887859, 0.9322283169840351, 0.9323088599234987, 0.9323473927233291, 0.9322417774870604, 0.9324252990736813, 0.9324786181231098, 0.9325013682519123, 0.9324216125258274, 0.9325236621931704, 0.9328138921815627, 0.9328119207926975, 0.9327890280471134, 0.9326488349349001, 0.9328068708072675, 0.9330714690747325, 0.9329411717469646, 0.9328889907360168, 0.9329029867845348, 0.9331221229680269, 0.9330598119942927, 0.9330708635710604, 0.9331944419029063, 0.9332709999265574, 0.9333953887758536, 0.933290045661735, 0.9334563539969064, 0.9332047542303956, 0.9335055732993646, 0.9332114955306582, 0.9332125385717227, 0.9334146909247664, 0.9336675870664459, 0.9334590173489284, 0.933507607567855, 0.9337097354562199, 0.9336625356216387, 0.9339268132168042, 0.9338612348582442, 0.9340042271939505], 'mDice': [0.017234630320750496, 0.016240648729595603, 0.016071035280171585, 0.01981883313911187, 0.0240105883325539, 0.03330612510761614, 0.04386192459145538, 0.05328247506700776, 0.06149803564391983, 0.0717770889795008, 0.08199642942645588, 0.09433532403237975, 0.10841003684023567, 0.12236602711256975, 0.138854600025184, 0.1561172356868907, 0.17192118154547453, 0.18750306665437724, 0.20260132754701674, 0.21565412594526517, 0.23101197744188037, 0.24487825522871864, 0.25928019993517104, 0.2713772021370493, 0.28650527429507144, 0.297781843549143, 0.3073090820476134, 0.31733527440589954, 0.32541516197998677, 0.33411766424651596, 0.34225553308053586, 0.34843938430426136, 0.35595895262359306, 0.3619445823039626, 0.3690351361205221, 0.37529954429718027, 0.3798526523658851, 0.3830507374152251, 0.39009974522583285, 0.39515245786999287, 0.3995806148901642, 0.4043637681080929, 0.4061024898229731, 0.41415155538938997, 0.4173680537809222, 0.4216680727081766, 0.42616227269747226, 0.42822623735497356, 0.4320409738111983, 0.43672181187003295, 0.4402476038941074, 0.44297900824089925, 0.4444425831945135, 0.4492840214411785, 0.4543915857337035, 0.45767290301582175, 0.4600255515434297, 0.46284475705976175, 0.46545045735956914, 0.46930496128901183, 0.46993557794810675, 0.47210422809215463, 0.4746179433716545, 0.4770613510656476, 0.4813955447934417, 0.4838830756329837, 0.48669884595334106, 0.4851531574083519, 0.4911988106609563, 0.4909024227267634, 0.49480108508536047, 0.49588393675515663, 0.49864221840711925, 0.4999521774479896, 0.49871984414335435, 0.5047512461230409, 0.5065025203669717, 0.5067342727979943, 0.50793376277126, 0.5091393629388127, 0.5109324029054121, 0.5141580487368583, 0.5153592842402476, 0.5181234714749096, 0.5202517821445432, 0.521057870706566, 0.5239611202543908, 0.5236310267669055, 0.5227214522115385, 0.525819942490177, 0.5270559373149206, 0.5304382512260085, 0.5319512572066963, 0.5320497122439709, 0.5333428124322445, 0.5341877432165519, 0.5369914823445323, 0.5361679291964175, 0.538243577249114, 0.5401627367715374, 0.5402683244060408, 0.5411928062473897, 0.5436298959582027, 0.5434222858904045, 0.5440340864881285, 0.5463427408756906, 0.5465750631762994, 0.5466764157784106, 0.5494030526614221, 0.5505771085846877, 0.5537619463896876, 0.5529062413562579, 0.5537801373549595, 0.5566135874506088, 0.5560817016855655, 0.5563530296127485, 0.5594235420204069, 0.5597741699227943, 0.5584764276967472, 0.5626219595951774, 0.5622305612884001, 0.5640312812680888, 0.5652375422922292, 0.5652062926468004, 0.5647211831866695, 0.5664503216766451, 0.5673025097947187, 0.5687950671764842, 0.5673635054903268, 0.5709594680735756, 0.5704929999287519, 0.5710187167681916, 0.5728616371581337, 0.5729357360851243, 0.5750044520383445, 0.5741485138033221, 0.5740839556643837, 0.576894500408647, 0.577631158246823, 0.576884186081696, 0.5769844172155786, 0.5791367620821516, 0.5781480980121678, 0.5793791728854433, 0.5806346457427881, 0.5807229401818447, 0.5810576312478071, 0.5821327519642036, 0.5821521090178768, 0.5834328583584416, 0.5862233246225417, 0.5845225980246437, 0.5838277911965227, 0.5850618800975372, 0.5859173415873307, 0.5855686361099774, 0.5870311250685726, 0.588899575861888, 0.5887689478185473, 0.5880966592621018, 0.589415911295659, 0.5901166552037478, 0.5909082970109913, 0.589658823929381, 0.5901758423057484, 0.5926598302886416, 0.5916217059695654, 0.5924939522284498, 0.593121609546234, 0.5931361586872451, 0.5952566212970718, 0.5939543689057987, 0.593975934893588, 0.5946185340857446, 0.5966767911532262, 0.5969842272685492, 0.5966682674247267, 0.5985055009087363, 0.5989555749149481, 0.5987365322350214, 0.5978669239326964, 0.599967499826831, 0.5989058245485496, 0.6011505807689419, 0.5995635319151399, 0.5995474445675066, 0.6008248928886216, 0.6013187007313867, 0.6006308427453616, 0.6015415021811052, 0.6024152396247868, 0.6022083212443576, 0.6037134855083586, 0.6040039320591444, 0.6041050452314546]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.34s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.09s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.90s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:24,  1.35s/it]predicting train subjects:   1%|          | 2/285 [00:03<06:49,  1.45s/it]predicting train subjects:   1%|          | 3/285 [00:04<06:48,  1.45s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:24,  1.58s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<07:16,  1.56s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:42,  1.66s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<08:05,  1.75s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<08:16,  1.79s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<07:58,  1.73s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:17,  1.81s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:31,  1.87s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:31,  1.87s/it]predicting train subjects:   5%|▍         | 13/285 [00:22<08:31,  1.88s/it]predicting train subjects:   5%|▍         | 14/285 [00:24<08:30,  1.89s/it]predicting train subjects:   5%|▌         | 15/285 [00:26<08:30,  1.89s/it]predicting train subjects:   6%|▌         | 16/285 [00:28<08:40,  1.94s/it]predicting train subjects:   6%|▌         | 17/285 [00:30<08:49,  1.98s/it]predicting train subjects:   6%|▋         | 18/285 [00:32<08:53,  2.00s/it]predicting train subjects:   7%|▋         | 19/285 [00:35<09:01,  2.04s/it]predicting train subjects:   7%|▋         | 20/285 [00:37<09:26,  2.14s/it]predicting train subjects:   7%|▋         | 21/285 [00:39<09:36,  2.18s/it]predicting train subjects:   8%|▊         | 22/285 [00:41<09:36,  2.19s/it]predicting train subjects:   8%|▊         | 23/285 [00:44<09:43,  2.23s/it]predicting train subjects:   8%|▊         | 24/285 [00:46<09:38,  2.22s/it]predicting train subjects:   9%|▉         | 25/285 [00:48<09:39,  2.23s/it]predicting train subjects:   9%|▉         | 26/285 [00:50<09:33,  2.21s/it]predicting train subjects:   9%|▉         | 27/285 [00:53<09:36,  2.23s/it]predicting train subjects:  10%|▉         | 28/285 [00:55<09:37,  2.25s/it]predicting train subjects:  10%|█         | 29/285 [00:57<09:30,  2.23s/it]predicting train subjects:  11%|█         | 30/285 [00:59<09:16,  2.18s/it]predicting train subjects:  11%|█         | 31/285 [01:01<09:11,  2.17s/it]predicting train subjects:  11%|█         | 32/285 [01:04<09:08,  2.17s/it]predicting train subjects:  12%|█▏        | 33/285 [01:06<09:07,  2.17s/it]predicting train subjects:  12%|█▏        | 34/285 [01:08<09:09,  2.19s/it]predicting train subjects:  12%|█▏        | 35/285 [01:10<09:08,  2.20s/it]predicting train subjects:  13%|█▎        | 36/285 [01:12<09:05,  2.19s/it]predicting train subjects:  13%|█▎        | 37/285 [01:14<08:57,  2.17s/it]predicting train subjects:  13%|█▎        | 38/285 [01:17<08:50,  2.15s/it]predicting train subjects:  14%|█▎        | 39/285 [01:19<09:00,  2.20s/it]predicting train subjects:  14%|█▍        | 40/285 [01:21<08:50,  2.17s/it]predicting train subjects:  14%|█▍        | 41/285 [01:23<08:45,  2.16s/it]predicting train subjects:  15%|█▍        | 42/285 [01:25<08:42,  2.15s/it]predicting train subjects:  15%|█▌        | 43/285 [01:27<08:29,  2.10s/it]predicting train subjects:  15%|█▌        | 44/285 [01:29<08:36,  2.14s/it]predicting train subjects:  16%|█▌        | 45/285 [01:32<08:34,  2.14s/it]predicting train subjects:  16%|█▌        | 46/285 [01:33<08:05,  2.03s/it]predicting train subjects:  16%|█▋        | 47/285 [01:35<07:48,  1.97s/it]predicting train subjects:  17%|█▋        | 48/285 [01:37<07:35,  1.92s/it]predicting train subjects:  17%|█▋        | 49/285 [01:39<07:17,  1.86s/it]predicting train subjects:  18%|█▊        | 50/285 [01:41<07:19,  1.87s/it]predicting train subjects:  18%|█▊        | 51/285 [01:42<07:17,  1.87s/it]predicting train subjects:  18%|█▊        | 52/285 [01:44<07:01,  1.81s/it]predicting train subjects:  19%|█▊        | 53/285 [01:46<06:54,  1.79s/it]predicting train subjects:  19%|█▉        | 54/285 [01:48<07:00,  1.82s/it]predicting train subjects:  19%|█▉        | 55/285 [01:49<06:49,  1.78s/it]predicting train subjects:  20%|█▉        | 56/285 [01:51<06:52,  1.80s/it]predicting train subjects:  20%|██        | 57/285 [01:53<06:50,  1.80s/it]predicting train subjects:  20%|██        | 58/285 [01:55<06:42,  1.78s/it]predicting train subjects:  21%|██        | 59/285 [01:57<06:48,  1.81s/it]predicting train subjects:  21%|██        | 60/285 [01:58<06:29,  1.73s/it]predicting train subjects:  21%|██▏       | 61/285 [02:00<06:29,  1.74s/it]predicting train subjects:  22%|██▏       | 62/285 [02:02<06:52,  1.85s/it]predicting train subjects:  22%|██▏       | 63/285 [02:04<06:47,  1.84s/it]predicting train subjects:  22%|██▏       | 64/285 [02:06<06:56,  1.89s/it]predicting train subjects:  23%|██▎       | 65/285 [02:08<07:09,  1.95s/it]predicting train subjects:  23%|██▎       | 66/285 [02:10<07:07,  1.95s/it]predicting train subjects:  24%|██▎       | 67/285 [02:12<07:00,  1.93s/it]predicting train subjects:  24%|██▍       | 68/285 [02:14<06:56,  1.92s/it]predicting train subjects:  24%|██▍       | 69/285 [02:16<06:55,  1.92s/it]predicting train subjects:  25%|██▍       | 70/285 [02:17<06:44,  1.88s/it]predicting train subjects:  25%|██▍       | 71/285 [02:19<06:37,  1.86s/it]predicting train subjects:  25%|██▌       | 72/285 [02:21<06:30,  1.83s/it]predicting train subjects:  26%|██▌       | 73/285 [02:23<06:28,  1.83s/it]predicting train subjects:  26%|██▌       | 74/285 [02:25<06:28,  1.84s/it]predicting train subjects:  26%|██▋       | 75/285 [02:27<06:30,  1.86s/it]predicting train subjects:  27%|██▋       | 76/285 [02:28<06:29,  1.86s/it]predicting train subjects:  27%|██▋       | 77/285 [02:30<06:25,  1.85s/it]predicting train subjects:  27%|██▋       | 78/285 [02:32<06:13,  1.80s/it]predicting train subjects:  28%|██▊       | 79/285 [02:34<06:14,  1.82s/it]predicting train subjects:  28%|██▊       | 80/285 [02:36<06:15,  1.83s/it]predicting train subjects:  28%|██▊       | 81/285 [02:38<06:21,  1.87s/it]predicting train subjects:  29%|██▉       | 82/285 [02:40<06:17,  1.86s/it]predicting train subjects:  29%|██▉       | 83/285 [02:42<06:25,  1.91s/it]predicting train subjects:  29%|██▉       | 84/285 [02:44<06:26,  1.92s/it]predicting train subjects:  30%|██▉       | 85/285 [02:46<06:42,  2.01s/it]predicting train subjects:  30%|███       | 86/285 [02:48<06:45,  2.04s/it]predicting train subjects:  31%|███       | 87/285 [02:50<06:45,  2.05s/it]predicting train subjects:  31%|███       | 88/285 [02:52<06:43,  2.05s/it]predicting train subjects:  31%|███       | 89/285 [02:54<06:53,  2.11s/it]predicting train subjects:  32%|███▏      | 90/285 [02:56<06:49,  2.10s/it]predicting train subjects:  32%|███▏      | 91/285 [02:58<06:44,  2.08s/it]predicting train subjects:  32%|███▏      | 92/285 [03:00<06:45,  2.10s/it]predicting train subjects:  33%|███▎      | 93/285 [03:03<06:49,  2.13s/it]predicting train subjects:  33%|███▎      | 94/285 [03:05<06:44,  2.12s/it]predicting train subjects:  33%|███▎      | 95/285 [03:07<06:43,  2.12s/it]predicting train subjects:  34%|███▎      | 96/285 [03:09<06:41,  2.12s/it]predicting train subjects:  34%|███▍      | 97/285 [03:11<06:45,  2.16s/it]predicting train subjects:  34%|███▍      | 98/285 [03:13<06:38,  2.13s/it]predicting train subjects:  35%|███▍      | 99/285 [03:15<06:30,  2.10s/it]predicting train subjects:  35%|███▌      | 100/285 [03:17<06:29,  2.10s/it]predicting train subjects:  35%|███▌      | 101/285 [03:20<06:26,  2.10s/it]predicting train subjects:  36%|███▌      | 102/285 [03:22<06:29,  2.13s/it]predicting train subjects:  36%|███▌      | 103/285 [03:24<06:22,  2.10s/it]predicting train subjects:  36%|███▋      | 104/285 [03:26<06:14,  2.07s/it]predicting train subjects:  37%|███▋      | 105/285 [03:28<06:07,  2.04s/it]predicting train subjects:  37%|███▋      | 106/285 [03:30<06:11,  2.08s/it]predicting train subjects:  38%|███▊      | 107/285 [03:32<06:04,  2.05s/it]predicting train subjects:  38%|███▊      | 108/285 [03:34<05:57,  2.02s/it]predicting train subjects:  38%|███▊      | 109/285 [03:36<05:51,  2.00s/it]predicting train subjects:  39%|███▊      | 110/285 [03:38<05:50,  2.00s/it]predicting train subjects:  39%|███▉      | 111/285 [03:40<05:55,  2.04s/it]predicting train subjects:  39%|███▉      | 112/285 [03:42<05:49,  2.02s/it]predicting train subjects:  40%|███▉      | 113/285 [03:44<05:56,  2.07s/it]predicting train subjects:  40%|████      | 114/285 [03:46<05:47,  2.03s/it]predicting train subjects:  40%|████      | 115/285 [03:48<05:42,  2.01s/it]predicting train subjects:  41%|████      | 116/285 [03:50<05:42,  2.03s/it]predicting train subjects:  41%|████      | 117/285 [03:52<05:47,  2.07s/it]predicting train subjects:  41%|████▏     | 118/285 [03:54<05:50,  2.10s/it]predicting train subjects:  42%|████▏     | 119/285 [03:56<05:44,  2.07s/it]predicting train subjects:  42%|████▏     | 120/285 [03:58<05:36,  2.04s/it]predicting train subjects:  42%|████▏     | 121/285 [04:00<05:27,  2.00s/it]predicting train subjects:  43%|████▎     | 122/285 [04:02<05:20,  1.97s/it]predicting train subjects:  43%|████▎     | 123/285 [04:04<05:01,  1.86s/it]predicting train subjects:  44%|████▎     | 124/285 [04:06<04:57,  1.85s/it]predicting train subjects:  44%|████▍     | 125/285 [04:07<04:57,  1.86s/it]predicting train subjects:  44%|████▍     | 126/285 [04:09<04:58,  1.88s/it]predicting train subjects:  45%|████▍     | 127/285 [04:11<04:59,  1.90s/it]predicting train subjects:  45%|████▍     | 128/285 [04:13<04:54,  1.87s/it]predicting train subjects:  45%|████▌     | 129/285 [04:15<04:47,  1.84s/it]predicting train subjects:  46%|████▌     | 130/285 [04:17<04:43,  1.83s/it]predicting train subjects:  46%|████▌     | 131/285 [04:19<04:40,  1.82s/it]predicting train subjects:  46%|████▋     | 132/285 [04:20<04:40,  1.83s/it]predicting train subjects:  47%|████▋     | 133/285 [04:22<04:44,  1.87s/it]predicting train subjects:  47%|████▋     | 134/285 [04:24<04:43,  1.88s/it]predicting train subjects:  47%|████▋     | 135/285 [04:26<04:43,  1.89s/it]predicting train subjects:  48%|████▊     | 136/285 [04:28<04:34,  1.84s/it]predicting train subjects:  48%|████▊     | 137/285 [04:30<04:27,  1.80s/it]predicting train subjects:  48%|████▊     | 138/285 [04:31<04:28,  1.83s/it]predicting train subjects:  49%|████▉     | 139/285 [04:33<04:28,  1.84s/it]predicting train subjects:  49%|████▉     | 140/285 [04:35<04:35,  1.90s/it]predicting train subjects:  49%|████▉     | 141/285 [04:37<04:31,  1.88s/it]predicting train subjects:  50%|████▉     | 142/285 [04:39<04:25,  1.86s/it]predicting train subjects:  50%|█████     | 143/285 [04:41<04:13,  1.78s/it]predicting train subjects:  51%|█████     | 144/285 [04:42<04:05,  1.74s/it]predicting train subjects:  51%|█████     | 145/285 [04:44<03:56,  1.69s/it]predicting train subjects:  51%|█████     | 146/285 [04:45<03:50,  1.66s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:47<03:44,  1.63s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:49<03:48,  1.67s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:50<03:46,  1.66s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:52<03:47,  1.69s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:54<03:39,  1.64s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:55<03:35,  1.62s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:57<03:32,  1.61s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:59<03:33,  1.63s/it]predicting train subjects:  54%|█████▍    | 155/285 [05:00<03:36,  1.66s/it]predicting train subjects:  55%|█████▍    | 156/285 [05:02<03:32,  1.65s/it]predicting train subjects:  55%|█████▌    | 157/285 [05:03<03:27,  1.62s/it]predicting train subjects:  55%|█████▌    | 158/285 [05:05<03:27,  1.63s/it]predicting train subjects:  56%|█████▌    | 159/285 [05:07<03:30,  1.67s/it]predicting train subjects:  56%|█████▌    | 160/285 [05:09<03:31,  1.69s/it]predicting train subjects:  56%|█████▋    | 161/285 [05:10<03:27,  1.67s/it]predicting train subjects:  57%|█████▋    | 162/285 [05:12<03:29,  1.71s/it]predicting train subjects:  57%|█████▋    | 163/285 [05:14<03:25,  1.69s/it]predicting train subjects:  58%|█████▊    | 164/285 [05:15<03:22,  1.67s/it]predicting train subjects:  58%|█████▊    | 165/285 [05:17<03:17,  1.64s/it]predicting train subjects:  58%|█████▊    | 166/285 [05:19<03:17,  1.66s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:20<03:11,  1.62s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:22<03:06,  1.59s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:23<03:00,  1.56s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:25<02:59,  1.56s/it]predicting train subjects:  60%|██████    | 171/285 [05:26<02:52,  1.52s/it]predicting train subjects:  60%|██████    | 172/285 [05:28<02:49,  1.50s/it]predicting train subjects:  61%|██████    | 173/285 [05:29<02:48,  1.51s/it]predicting train subjects:  61%|██████    | 174/285 [05:31<02:58,  1.61s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:32<02:51,  1.56s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:34<02:48,  1.55s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:35<02:43,  1.51s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:37<02:37,  1.47s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:38<02:32,  1.44s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:39<02:28,  1.41s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:41<02:24,  1.39s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:42<02:23,  1.40s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:44<02:21,  1.39s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:45<02:19,  1.38s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:46<02:16,  1.37s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:48<02:15,  1.37s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:49<02:15,  1.38s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:50<02:12,  1.37s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:52<02:11,  1.37s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:53<02:08,  1.36s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:54<02:06,  1.34s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:56<02:04,  1.34s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:57<02:03,  1.34s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:58<02:00,  1.32s/it]predicting train subjects:  68%|██████▊   | 195/285 [06:00<02:00,  1.34s/it]predicting train subjects:  69%|██████▉   | 196/285 [06:01<02:08,  1.44s/it]predicting train subjects:  69%|██████▉   | 197/285 [06:03<02:15,  1.54s/it]predicting train subjects:  69%|██████▉   | 198/285 [06:05<02:16,  1.57s/it]predicting train subjects:  70%|██████▉   | 199/285 [06:06<02:16,  1.59s/it]predicting train subjects:  70%|███████   | 200/285 [06:08<02:15,  1.60s/it]predicting train subjects:  71%|███████   | 201/285 [06:10<02:13,  1.59s/it]predicting train subjects:  71%|███████   | 202/285 [06:11<02:11,  1.59s/it]predicting train subjects:  71%|███████   | 203/285 [06:13<02:10,  1.60s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:14<02:10,  1.62s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:16<02:08,  1.60s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:18<02:06,  1.60s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:19<02:06,  1.63s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:21<02:05,  1.63s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:23<02:03,  1.62s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:24<02:01,  1.62s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:26<01:58,  1.61s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:27<01:57,  1.60s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:29<01:55,  1.60s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:30<01:52,  1.59s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:32<01:48,  1.55s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:33<01:45,  1.53s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:35<01:46,  1.57s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:37<01:42,  1.53s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:38<01:40,  1.53s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:40<01:38,  1.52s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:41<01:35,  1.49s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:42<01:33,  1.48s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:44<01:32,  1.49s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:45<01:30,  1.49s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:47<01:27,  1.47s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:48<01:25,  1.45s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:50<01:24,  1.46s/it]predicting train subjects:  80%|████████  | 228/285 [06:51<01:23,  1.46s/it]predicting train subjects:  80%|████████  | 229/285 [06:53<01:22,  1.47s/it]predicting train subjects:  81%|████████  | 230/285 [06:54<01:20,  1.47s/it]predicting train subjects:  81%|████████  | 231/285 [06:56<01:19,  1.46s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:57<01:23,  1.58s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:59<01:25,  1.65s/it]predicting train subjects:  82%|████████▏ | 234/285 [07:01<01:26,  1.70s/it]predicting train subjects:  82%|████████▏ | 235/285 [07:03<01:26,  1.74s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:05<01:26,  1.76s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:07<01:26,  1.80s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:08<01:25,  1.81s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:10<01:24,  1.85s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:12<01:23,  1.86s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:14<01:21,  1.85s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:16<01:19,  1.85s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:18<01:17,  1.85s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:20<01:14,  1.83s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:21<01:13,  1.85s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:23<01:11,  1.83s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:25<01:09,  1.82s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:27<01:08,  1.85s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:29<01:06,  1.83s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:30<00:59,  1.70s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:32<00:54,  1.61s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:33<00:51,  1.55s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:34<00:48,  1.52s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:36<00:45,  1.47s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:37<00:42,  1.42s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:38<00:41,  1.41s/it]predicting train subjects:  90%|█████████ | 257/285 [07:40<00:39,  1.43s/it]predicting train subjects:  91%|█████████ | 258/285 [07:41<00:38,  1.43s/it]predicting train subjects:  91%|█████████ | 259/285 [07:43<00:37,  1.43s/it]predicting train subjects:  91%|█████████ | 260/285 [07:44<00:35,  1.42s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:46<00:33,  1.39s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:47<00:31,  1.38s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:48<00:30,  1.37s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:50<00:28,  1.35s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:51<00:26,  1.35s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:52<00:25,  1.35s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:54<00:24,  1.37s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:56<00:25,  1.52s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:57<00:25,  1.61s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:59<00:25,  1.67s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:01<00:24,  1.73s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:03<00:23,  1.78s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:05<00:21,  1.80s/it]predicting train subjects:  96%|█████████▌| 274/285 [08:07<00:19,  1.81s/it]predicting train subjects:  96%|█████████▋| 275/285 [08:09<00:18,  1.84s/it]predicting train subjects:  97%|█████████▋| 276/285 [08:10<00:16,  1.86s/it]predicting train subjects:  97%|█████████▋| 277/285 [08:12<00:14,  1.86s/it]predicting train subjects:  98%|█████████▊| 278/285 [08:14<00:13,  1.86s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:16<00:11,  1.87s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:18<00:09,  1.87s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:20<00:07,  1.85s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:22<00:05,  1.86s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:23<00:03,  1.86s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:25<00:01,  1.87s/it]predicting train subjects: 100%|██████████| 285/285 [08:27<00:00,  1.87s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:43,  1.42s/it]Loading train:   1%|          | 2/285 [00:02<06:47,  1.44s/it]Loading train:   1%|          | 3/285 [00:04<06:30,  1.38s/it]Loading train:   1%|▏         | 4/285 [00:05<06:53,  1.47s/it]Loading train:   2%|▏         | 5/285 [00:07<06:27,  1.38s/it]Loading train:   2%|▏         | 6/285 [00:08<06:50,  1.47s/it]Loading train:   2%|▏         | 7/285 [00:10<07:19,  1.58s/it]Loading train:   3%|▎         | 8/285 [00:12<07:27,  1.62s/it]Loading train:   3%|▎         | 9/285 [00:13<07:02,  1.53s/it]Loading train:   4%|▎         | 10/285 [00:14<06:37,  1.45s/it]Loading train:   4%|▍         | 11/285 [00:16<06:19,  1.39s/it]Loading train:   4%|▍         | 12/285 [00:17<06:03,  1.33s/it]Loading train:   5%|▍         | 13/285 [00:18<05:46,  1.27s/it]Loading train:   5%|▍         | 14/285 [00:19<05:44,  1.27s/it]Loading train:   5%|▌         | 15/285 [00:20<05:42,  1.27s/it]Loading train:   6%|▌         | 16/285 [00:22<05:36,  1.25s/it]Loading train:   6%|▌         | 17/285 [00:23<05:26,  1.22s/it]Loading train:   6%|▋         | 18/285 [00:24<05:21,  1.20s/it]Loading train:   7%|▋         | 19/285 [00:25<05:15,  1.19s/it]Loading train:   7%|▋         | 20/285 [00:26<05:13,  1.18s/it]Loading train:   7%|▋         | 21/285 [00:27<05:11,  1.18s/it]Loading train:   8%|▊         | 22/285 [00:29<05:09,  1.18s/it]Loading train:   8%|▊         | 23/285 [00:30<05:03,  1.16s/it]Loading train:   8%|▊         | 24/285 [00:31<04:56,  1.13s/it]Loading train:   9%|▉         | 25/285 [00:32<04:54,  1.13s/it]Loading train:   9%|▉         | 26/285 [00:33<04:46,  1.11s/it]Loading train:   9%|▉         | 27/285 [00:34<04:43,  1.10s/it]Loading train:  10%|▉         | 28/285 [00:35<04:47,  1.12s/it]Loading train:  10%|█         | 29/285 [00:36<04:38,  1.09s/it]Loading train:  11%|█         | 30/285 [00:37<04:31,  1.07s/it]Loading train:  11%|█         | 31/285 [00:38<04:27,  1.05s/it]Loading train:  11%|█         | 32/285 [00:39<04:21,  1.03s/it]Loading train:  12%|█▏        | 33/285 [00:40<04:15,  1.01s/it]Loading train:  12%|█▏        | 34/285 [00:41<04:12,  1.01s/it]Loading train:  12%|█▏        | 35/285 [00:42<04:11,  1.01s/it]Loading train:  13%|█▎        | 36/285 [00:43<04:10,  1.01s/it]Loading train:  13%|█▎        | 37/285 [00:44<04:09,  1.01s/it]Loading train:  13%|█▎        | 38/285 [00:45<04:16,  1.04s/it]Loading train:  14%|█▎        | 39/285 [00:46<04:20,  1.06s/it]Loading train:  14%|█▍        | 40/285 [00:48<04:19,  1.06s/it]Loading train:  14%|█▍        | 41/285 [00:49<04:16,  1.05s/it]Loading train:  15%|█▍        | 42/285 [00:50<04:09,  1.03s/it]Loading train:  15%|█▌        | 43/285 [00:50<04:03,  1.01s/it]Loading train:  15%|█▌        | 44/285 [00:52<04:06,  1.02s/it]Loading train:  16%|█▌        | 45/285 [00:53<04:04,  1.02s/it]Loading train:  16%|█▌        | 46/285 [00:54<04:05,  1.03s/it]Loading train:  16%|█▋        | 47/285 [00:54<03:55,  1.01it/s]Loading train:  17%|█▋        | 48/285 [00:55<03:47,  1.04it/s]Loading train:  17%|█▋        | 49/285 [00:56<03:43,  1.05it/s]Loading train:  18%|█▊        | 50/285 [00:57<03:48,  1.03it/s]Loading train:  18%|█▊        | 51/285 [00:58<03:50,  1.01it/s]Loading train:  18%|█▊        | 52/285 [00:59<03:46,  1.03it/s]Loading train:  19%|█▊        | 53/285 [01:00<03:46,  1.03it/s]Loading train:  19%|█▉        | 54/285 [01:01<03:44,  1.03it/s]Loading train:  19%|█▉        | 55/285 [01:02<03:37,  1.06it/s]Loading train:  20%|█▉        | 56/285 [01:03<03:39,  1.04it/s]Loading train:  20%|██        | 57/285 [01:04<03:35,  1.06it/s]Loading train:  20%|██        | 58/285 [01:05<03:35,  1.05it/s]Loading train:  21%|██        | 59/285 [01:06<03:38,  1.03it/s]Loading train:  21%|██        | 60/285 [01:07<03:47,  1.01s/it]Loading train:  21%|██▏       | 61/285 [01:08<03:42,  1.01it/s]Loading train:  22%|██▏       | 62/285 [01:09<03:35,  1.03it/s]Loading train:  22%|██▏       | 63/285 [01:10<03:31,  1.05it/s]Loading train:  22%|██▏       | 64/285 [01:11<03:59,  1.08s/it]Loading train:  23%|██▎       | 65/285 [01:13<04:37,  1.26s/it]Loading train:  23%|██▎       | 66/285 [01:14<04:43,  1.30s/it]Loading train:  24%|██▎       | 67/285 [01:15<04:19,  1.19s/it]Loading train:  24%|██▍       | 68/285 [01:16<04:00,  1.11s/it]Loading train:  24%|██▍       | 69/285 [01:17<03:53,  1.08s/it]Loading train:  25%|██▍       | 70/285 [01:18<03:48,  1.06s/it]Loading train:  25%|██▍       | 71/285 [01:19<03:47,  1.06s/it]Loading train:  25%|██▌       | 72/285 [01:20<03:41,  1.04s/it]Loading train:  26%|██▌       | 73/285 [01:21<03:40,  1.04s/it]Loading train:  26%|██▌       | 74/285 [01:22<03:38,  1.03s/it]Loading train:  26%|██▋       | 75/285 [01:23<03:29,  1.00it/s]Loading train:  27%|██▋       | 76/285 [01:24<03:29,  1.00s/it]Loading train:  27%|██▋       | 77/285 [01:25<03:28,  1.00s/it]Loading train:  27%|██▋       | 78/285 [01:26<03:20,  1.03it/s]Loading train:  28%|██▊       | 79/285 [01:27<03:17,  1.04it/s]Loading train:  28%|██▊       | 80/285 [01:28<03:16,  1.04it/s]Loading train:  28%|██▊       | 81/285 [01:29<03:20,  1.02it/s]Loading train:  29%|██▉       | 82/285 [01:30<03:25,  1.01s/it]Loading train:  29%|██▉       | 83/285 [01:31<03:19,  1.01it/s]Loading train:  29%|██▉       | 84/285 [01:32<03:19,  1.01it/s]Loading train:  30%|██▉       | 85/285 [01:33<03:26,  1.03s/it]Loading train:  30%|███       | 86/285 [01:34<03:32,  1.07s/it]Loading train:  31%|███       | 87/285 [01:36<03:37,  1.10s/it]Loading train:  31%|███       | 88/285 [01:37<03:37,  1.10s/it]Loading train:  31%|███       | 89/285 [01:38<03:39,  1.12s/it]Loading train:  32%|███▏      | 90/285 [01:39<03:42,  1.14s/it]Loading train:  32%|███▏      | 91/285 [01:40<03:40,  1.14s/it]Loading train:  32%|███▏      | 92/285 [01:41<03:37,  1.13s/it]Loading train:  33%|███▎      | 93/285 [01:42<03:31,  1.10s/it]Loading train:  33%|███▎      | 94/285 [01:43<03:27,  1.09s/it]Loading train:  33%|███▎      | 95/285 [01:45<03:32,  1.12s/it]Loading train:  34%|███▎      | 96/285 [01:46<03:25,  1.09s/it]Loading train:  34%|███▍      | 97/285 [01:47<03:24,  1.09s/it]Loading train:  34%|███▍      | 98/285 [01:48<03:21,  1.08s/it]Loading train:  35%|███▍      | 99/285 [01:49<03:20,  1.08s/it]Loading train:  35%|███▌      | 100/285 [01:50<03:18,  1.07s/it]Loading train:  35%|███▌      | 101/285 [01:51<03:14,  1.06s/it]Loading train:  36%|███▌      | 102/285 [01:52<03:13,  1.06s/it]Loading train:  36%|███▌      | 103/285 [01:53<03:15,  1.08s/it]Loading train:  36%|███▋      | 104/285 [01:54<03:14,  1.07s/it]Loading train:  37%|███▋      | 105/285 [01:55<03:06,  1.03s/it]Loading train:  37%|███▋      | 106/285 [01:56<03:04,  1.03s/it]Loading train:  38%|███▊      | 107/285 [01:57<03:07,  1.05s/it]Loading train:  38%|███▊      | 108/285 [01:58<03:06,  1.05s/it]Loading train:  38%|███▊      | 109/285 [01:59<03:05,  1.05s/it]Loading train:  39%|███▊      | 110/285 [02:00<02:57,  1.01s/it]Loading train:  39%|███▉      | 111/285 [02:01<02:58,  1.02s/it]Loading train:  39%|███▉      | 112/285 [02:02<02:54,  1.01s/it]Loading train:  40%|███▉      | 113/285 [02:03<02:49,  1.01it/s]Loading train:  40%|████      | 114/285 [02:04<02:49,  1.01it/s]Loading train:  40%|████      | 115/285 [02:05<02:47,  1.01it/s]Loading train:  41%|████      | 116/285 [02:06<02:50,  1.01s/it]Loading train:  41%|████      | 117/285 [02:07<02:53,  1.03s/it]Loading train:  41%|████▏     | 118/285 [02:08<02:53,  1.04s/it]Loading train:  42%|████▏     | 119/285 [02:10<02:59,  1.08s/it]Loading train:  42%|████▏     | 120/285 [02:11<02:55,  1.06s/it]Loading train:  42%|████▏     | 121/285 [02:12<03:13,  1.18s/it]Loading train:  43%|████▎     | 122/285 [02:13<03:22,  1.24s/it]Loading train:  43%|████▎     | 123/285 [02:15<03:28,  1.28s/it]Loading train:  44%|████▎     | 124/285 [02:16<03:11,  1.19s/it]Loading train:  44%|████▍     | 125/285 [02:17<02:57,  1.11s/it]Loading train:  44%|████▍     | 126/285 [02:18<02:46,  1.05s/it]Loading train:  45%|████▍     | 127/285 [02:18<02:36,  1.01it/s]Loading train:  45%|████▍     | 128/285 [02:19<02:32,  1.03it/s]Loading train:  45%|████▌     | 129/285 [02:20<02:31,  1.03it/s]Loading train:  46%|████▌     | 130/285 [02:21<02:28,  1.05it/s]Loading train:  46%|████▌     | 131/285 [02:22<02:24,  1.06it/s]Loading train:  46%|████▋     | 132/285 [02:23<02:22,  1.07it/s]Loading train:  47%|████▋     | 133/285 [02:24<02:24,  1.05it/s]Loading train:  47%|████▋     | 134/285 [02:25<02:22,  1.06it/s]Loading train:  47%|████▋     | 135/285 [02:26<02:19,  1.08it/s]Loading train:  48%|████▊     | 136/285 [02:27<02:14,  1.11it/s]Loading train:  48%|████▊     | 137/285 [02:28<02:14,  1.10it/s]Loading train:  48%|████▊     | 138/285 [02:29<02:13,  1.10it/s]Loading train:  49%|████▉     | 139/285 [02:29<02:15,  1.08it/s]Loading train:  49%|████▉     | 140/285 [02:30<02:10,  1.11it/s]Loading train:  49%|████▉     | 141/285 [02:31<02:11,  1.10it/s]Loading train:  50%|████▉     | 142/285 [02:32<02:09,  1.10it/s]Loading train:  50%|█████     | 143/285 [02:33<02:10,  1.09it/s]Loading train:  51%|█████     | 144/285 [02:34<02:07,  1.11it/s]Loading train:  51%|█████     | 145/285 [02:35<02:06,  1.11it/s]Loading train:  51%|█████     | 146/285 [02:36<02:03,  1.13it/s]Loading train:  52%|█████▏    | 147/285 [02:37<02:01,  1.14it/s]Loading train:  52%|█████▏    | 148/285 [02:38<02:02,  1.12it/s]Loading train:  52%|█████▏    | 149/285 [02:38<02:02,  1.11it/s]Loading train:  53%|█████▎    | 150/285 [02:39<02:01,  1.11it/s]Loading train:  53%|█████▎    | 151/285 [02:40<02:01,  1.11it/s]Loading train:  53%|█████▎    | 152/285 [02:41<01:57,  1.13it/s]Loading train:  54%|█████▎    | 153/285 [02:42<01:56,  1.14it/s]Loading train:  54%|█████▍    | 154/285 [02:43<01:57,  1.12it/s]Loading train:  54%|█████▍    | 155/285 [02:44<01:54,  1.13it/s]Loading train:  55%|█████▍    | 156/285 [02:45<01:53,  1.14it/s]Loading train:  55%|█████▌    | 157/285 [02:46<01:53,  1.13it/s]Loading train:  55%|█████▌    | 158/285 [02:46<01:53,  1.12it/s]Loading train:  56%|█████▌    | 159/285 [02:47<01:50,  1.14it/s]Loading train:  56%|█████▌    | 160/285 [02:48<01:50,  1.13it/s]Loading train:  56%|█████▋    | 161/285 [02:49<01:49,  1.14it/s]Loading train:  57%|█████▋    | 162/285 [02:50<01:48,  1.14it/s]Loading train:  57%|█████▋    | 163/285 [02:51<01:46,  1.14it/s]Loading train:  58%|█████▊    | 164/285 [02:52<01:46,  1.14it/s]Loading train:  58%|█████▊    | 165/285 [02:53<01:46,  1.13it/s]Loading train:  58%|█████▊    | 166/285 [02:53<01:43,  1.14it/s]Loading train:  59%|█████▊    | 167/285 [02:54<01:42,  1.15it/s]Loading train:  59%|█████▉    | 168/285 [02:55<01:41,  1.16it/s]Loading train:  59%|█████▉    | 169/285 [02:56<01:39,  1.17it/s]Loading train:  60%|█████▉    | 170/285 [02:57<01:41,  1.13it/s]Loading train:  60%|██████    | 171/285 [02:58<01:43,  1.11it/s]Loading train:  60%|██████    | 172/285 [02:59<01:38,  1.14it/s]Loading train:  61%|██████    | 173/285 [03:00<01:37,  1.15it/s]Loading train:  61%|██████    | 174/285 [03:00<01:34,  1.17it/s]Loading train:  61%|██████▏   | 175/285 [03:01<01:33,  1.17it/s]Loading train:  62%|██████▏   | 176/285 [03:02<01:30,  1.21it/s]Loading train:  62%|██████▏   | 177/285 [03:03<01:29,  1.21it/s]Loading train:  62%|██████▏   | 178/285 [03:04<01:29,  1.20it/s]Loading train:  63%|██████▎   | 179/285 [03:05<01:29,  1.18it/s]Loading train:  63%|██████▎   | 180/285 [03:05<01:28,  1.18it/s]Loading train:  64%|██████▎   | 181/285 [03:06<01:28,  1.17it/s]Loading train:  64%|██████▍   | 182/285 [03:07<01:26,  1.19it/s]Loading train:  64%|██████▍   | 183/285 [03:08<01:26,  1.19it/s]Loading train:  65%|██████▍   | 184/285 [03:09<01:25,  1.19it/s]Loading train:  65%|██████▍   | 185/285 [03:10<01:22,  1.21it/s]Loading train:  65%|██████▌   | 186/285 [03:10<01:22,  1.20it/s]Loading train:  66%|██████▌   | 187/285 [03:11<01:24,  1.16it/s]Loading train:  66%|██████▌   | 188/285 [03:12<01:23,  1.16it/s]Loading train:  66%|██████▋   | 189/285 [03:13<01:23,  1.15it/s]Loading train:  67%|██████▋   | 190/285 [03:14<01:22,  1.15it/s]Loading train:  67%|██████▋   | 191/285 [03:15<01:20,  1.17it/s]Loading train:  67%|██████▋   | 192/285 [03:16<01:18,  1.18it/s]Loading train:  68%|██████▊   | 193/285 [03:16<01:17,  1.19it/s]Loading train:  68%|██████▊   | 194/285 [03:17<01:15,  1.20it/s]Loading train:  68%|██████▊   | 195/285 [03:18<01:16,  1.18it/s]Loading train:  69%|██████▉   | 196/285 [03:19<01:18,  1.13it/s]Loading train:  69%|██████▉   | 197/285 [03:20<01:20,  1.09it/s]Loading train:  69%|██████▉   | 198/285 [03:21<01:19,  1.09it/s]Loading train:  70%|██████▉   | 199/285 [03:22<01:22,  1.04it/s]Loading train:  70%|███████   | 200/285 [03:23<01:22,  1.03it/s]Loading train:  71%|███████   | 201/285 [03:24<01:22,  1.02it/s]Loading train:  71%|███████   | 202/285 [03:25<01:21,  1.02it/s]Loading train:  71%|███████   | 203/285 [03:26<01:18,  1.04it/s]Loading train:  72%|███████▏  | 204/285 [03:27<01:15,  1.07it/s]Loading train:  72%|███████▏  | 205/285 [03:28<01:14,  1.07it/s]Loading train:  72%|███████▏  | 206/285 [03:29<01:15,  1.05it/s]Loading train:  73%|███████▎  | 207/285 [03:30<01:14,  1.05it/s]Loading train:  73%|███████▎  | 208/285 [03:31<01:12,  1.07it/s]Loading train:  73%|███████▎  | 209/285 [03:31<01:10,  1.08it/s]Loading train:  74%|███████▎  | 210/285 [03:32<01:07,  1.11it/s]Loading train:  74%|███████▍  | 211/285 [03:33<01:06,  1.12it/s]Loading train:  74%|███████▍  | 212/285 [03:34<01:05,  1.11it/s]Loading train:  75%|███████▍  | 213/285 [03:35<01:05,  1.09it/s]Loading train:  75%|███████▌  | 214/285 [03:36<01:03,  1.11it/s]Loading train:  75%|███████▌  | 215/285 [03:37<01:01,  1.13it/s]Loading train:  76%|███████▌  | 216/285 [03:38<00:59,  1.16it/s]Loading train:  76%|███████▌  | 217/285 [03:38<00:58,  1.17it/s]Loading train:  76%|███████▋  | 218/285 [03:39<00:56,  1.18it/s]Loading train:  77%|███████▋  | 219/285 [03:40<00:56,  1.16it/s]Loading train:  77%|███████▋  | 220/285 [03:41<00:55,  1.16it/s]Loading train:  78%|███████▊  | 221/285 [03:42<00:54,  1.18it/s]Loading train:  78%|███████▊  | 222/285 [03:43<00:54,  1.15it/s]Loading train:  78%|███████▊  | 223/285 [03:44<00:55,  1.11it/s]Loading train:  79%|███████▊  | 224/285 [03:45<00:53,  1.14it/s]Loading train:  79%|███████▉  | 225/285 [03:45<00:52,  1.13it/s]Loading train:  79%|███████▉  | 226/285 [03:46<00:52,  1.13it/s]Loading train:  80%|███████▉  | 227/285 [03:47<00:49,  1.18it/s]Loading train:  80%|████████  | 228/285 [03:48<00:48,  1.18it/s]Loading train:  80%|████████  | 229/285 [03:49<00:47,  1.18it/s]Loading train:  81%|████████  | 230/285 [03:50<00:46,  1.19it/s]Loading train:  81%|████████  | 231/285 [03:50<00:45,  1.18it/s]Loading train:  81%|████████▏ | 232/285 [03:52<00:48,  1.08it/s]Loading train:  82%|████████▏ | 233/285 [03:53<00:49,  1.05it/s]Loading train:  82%|████████▏ | 234/285 [03:54<00:49,  1.03it/s]Loading train:  82%|████████▏ | 235/285 [03:55<00:49,  1.02it/s]Loading train:  83%|████████▎ | 236/285 [03:56<00:47,  1.03it/s]Loading train:  83%|████████▎ | 237/285 [03:57<00:47,  1.02it/s]Loading train:  84%|████████▎ | 238/285 [03:58<00:45,  1.03it/s]Loading train:  84%|████████▍ | 239/285 [03:59<00:45,  1.02it/s]Loading train:  84%|████████▍ | 240/285 [04:00<00:44,  1.02it/s]Loading train:  85%|████████▍ | 241/285 [04:01<00:43,  1.01it/s]Loading train:  85%|████████▍ | 242/285 [04:01<00:42,  1.02it/s]Loading train:  85%|████████▌ | 243/285 [04:02<00:41,  1.01it/s]Loading train:  86%|████████▌ | 244/285 [04:03<00:40,  1.02it/s]Loading train:  86%|████████▌ | 245/285 [04:04<00:39,  1.01it/s]Loading train:  86%|████████▋ | 246/285 [04:05<00:38,  1.00it/s]Loading train:  87%|████████▋ | 247/285 [04:06<00:38,  1.00s/it]Loading train:  87%|████████▋ | 248/285 [04:07<00:37,  1.00s/it]Loading train:  87%|████████▋ | 249/285 [04:08<00:35,  1.01it/s]Loading train:  88%|████████▊ | 250/285 [04:09<00:33,  1.04it/s]Loading train:  88%|████████▊ | 251/285 [04:10<00:31,  1.07it/s]Loading train:  88%|████████▊ | 252/285 [04:11<00:29,  1.11it/s]Loading train:  89%|████████▉ | 253/285 [04:12<00:28,  1.13it/s]Loading train:  89%|████████▉ | 254/285 [04:13<00:27,  1.15it/s]Loading train:  89%|████████▉ | 255/285 [04:14<00:25,  1.18it/s]Loading train:  90%|████████▉ | 256/285 [04:14<00:24,  1.16it/s]Loading train:  90%|█████████ | 257/285 [04:15<00:25,  1.12it/s]Loading train:  91%|█████████ | 258/285 [04:16<00:23,  1.13it/s]Loading train:  91%|█████████ | 259/285 [04:17<00:23,  1.11it/s]Loading train:  91%|█████████ | 260/285 [04:18<00:23,  1.08it/s]Loading train:  92%|█████████▏| 261/285 [04:19<00:22,  1.07it/s]Loading train:  92%|█████████▏| 262/285 [04:20<00:21,  1.06it/s]Loading train:  92%|█████████▏| 263/285 [04:21<00:20,  1.07it/s]Loading train:  93%|█████████▎| 264/285 [04:22<00:19,  1.10it/s]Loading train:  93%|█████████▎| 265/285 [04:23<00:18,  1.09it/s]Loading train:  93%|█████████▎| 266/285 [04:24<00:16,  1.13it/s]Loading train:  94%|█████████▎| 267/285 [04:24<00:15,  1.16it/s]Loading train:  94%|█████████▍| 268/285 [04:26<00:15,  1.07it/s]Loading train:  94%|█████████▍| 269/285 [04:27<00:15,  1.02it/s]Loading train:  95%|█████████▍| 270/285 [04:28<00:14,  1.00it/s]Loading train:  95%|█████████▌| 271/285 [04:29<00:13,  1.00it/s]Loading train:  95%|█████████▌| 272/285 [04:30<00:12,  1.00it/s]Loading train:  96%|█████████▌| 273/285 [04:31<00:11,  1.01it/s]Loading train:  96%|█████████▌| 274/285 [04:32<00:10,  1.01it/s]Loading train:  96%|█████████▋| 275/285 [04:33<00:09,  1.01it/s]Loading train:  97%|█████████▋| 276/285 [04:34<00:09,  1.02s/it]Loading train:  97%|█████████▋| 277/285 [04:35<00:07,  1.00it/s]Loading train:  98%|█████████▊| 278/285 [04:36<00:06,  1.00it/s]Loading train:  98%|█████████▊| 279/285 [04:37<00:05,  1.00it/s]Loading train:  98%|█████████▊| 280/285 [04:38<00:04,  1.00it/s]Loading train:  99%|█████████▊| 281/285 [04:39<00:04,  1.00s/it]Loading train:  99%|█████████▉| 282/285 [04:40<00:03,  1.00s/it]Loading train:  99%|█████████▉| 283/285 [04:41<00:01,  1.00it/s]Loading train: 100%|█████████▉| 284/285 [04:42<00:01,  1.01s/it]Loading train: 100%|██████████| 285/285 [04:43<00:00,  1.03s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   4%|▎         | 10/285 [00:00<00:02, 99.94it/s]concatenating: train:  11%|█         | 31/285 [00:00<00:02, 117.99it/s]concatenating: train:  15%|█▌        | 44/285 [00:00<00:01, 120.58it/s]concatenating: train:  21%|██        | 59/285 [00:00<00:01, 126.14it/s]concatenating: train:  27%|██▋       | 76/285 [00:00<00:01, 136.16it/s]concatenating: train:  34%|███▎      | 96/285 [00:00<00:01, 148.55it/s]concatenating: train:  41%|████      | 117/285 [00:00<00:01, 162.02it/s]concatenating: train:  49%|████▉     | 140/285 [00:00<00:00, 177.11it/s]concatenating: train:  58%|█████▊    | 164/285 [00:00<00:00, 190.48it/s]concatenating: train:  65%|██████▍   | 184/285 [00:01<00:00, 176.30it/s]concatenating: train:  71%|███████   | 203/285 [00:01<00:00, 177.14it/s]concatenating: train:  81%|████████▏ | 232/285 [00:01<00:00, 199.61it/s]concatenating: train:  91%|█████████ | 260/285 [00:01<00:00, 218.35it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 194.89it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.41s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.40s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.34s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 262.29it/s]2019-07-06 22:47:46.832659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 22:47:46.832795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 22:47:46.832814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 22:47:46.832824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 22:47:46.833286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:08,  4.67it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:06,  5.51it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:06,  5.30it/s]loading the weights for Unet:  20%|██        | 8/40 [00:00<00:04,  6.83it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:05,  6.11it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:04,  6.94it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:04,  6.30it/s]loading the weights for Unet:  40%|████      | 16/40 [00:01<00:03,  7.88it/s]loading the weights for Unet:  45%|████▌     | 18/40 [00:01<00:02,  8.02it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  6.47it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:02<00:02,  7.65it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:02<00:01,  7.98it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:02<00:02,  6.60it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:01,  7.32it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:03<00:01,  6.18it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:03<00:01,  7.42it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:03<00:00,  7.61it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.96it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:04<00:00,  6.40it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:04<00:00,  5.34it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:04<00:00,  8.63it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 52, 30)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 52, 30)   8130        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 52, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 52, 30)   8130        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 52, 30)   120         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 52, 30)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 52, 30)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 52, 20)   5420        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 52, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 52, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 52, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 52, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 52, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 26, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 26, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 26, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 26, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 26, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 26, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 26, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 13, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 13, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 13, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 13, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 13, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 13, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 13, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 13, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 26, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 26, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 26, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 26, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 26, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 26, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 26, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 26, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 52, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 52, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 52, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 52, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 52, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 52, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 52, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 52, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 52, 13)   273         dropout_8[0][0]                  
==================================================================================================
Total params: 206,253
Trainable params: 62,593
Non-trainable params: 143,660
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.49841486e-02 3.19966680e-02 7.50970181e-02 9.33357939e-03
 2.71292049e-02 7.07427267e-03 8.46489586e-02 1.12779077e-01
 8.61338510e-02 1.32649165e-02 2.94521391e-01 1.92807035e-01
 2.29878984e-04]
Train on 18361 samples, validate on 179 samples
Epoch 1/300
 - 23s - loss: 127.7645 - acc: 0.7630 - mDice: 0.0122 - val_loss: 12.6103 - val_acc: 0.9136 - val_mDice: 0.0096

Epoch 00001: val_mDice improved from -inf to 0.00955, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 14s - loss: 19.2976 - acc: 0.8818 - mDice: 0.0129 - val_loss: 6.3151 - val_acc: 0.9136 - val_mDice: 0.0139

Epoch 00002: val_mDice improved from 0.00955 to 0.01390, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 14s - loss: 10.8788 - acc: 0.8852 - mDice: 0.0209 - val_loss: 5.1371 - val_acc: 0.9136 - val_mDice: 0.0289

Epoch 00003: val_mDice improved from 0.01390 to 0.02889, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 14s - loss: 8.1957 - acc: 0.8856 - mDice: 0.0317 - val_loss: 4.8012 - val_acc: 0.9136 - val_mDice: 0.0401

Epoch 00004: val_mDice improved from 0.02889 to 0.04010, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 14s - loss: 6.8999 - acc: 0.8855 - mDice: 0.0395 - val_loss: 4.2679 - val_acc: 0.9131 - val_mDice: 0.0472

Epoch 00005: val_mDice improved from 0.04010 to 0.04721, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 13s - loss: 5.9989 - acc: 0.8858 - mDice: 0.0547 - val_loss: 3.7695 - val_acc: 0.9132 - val_mDice: 0.0763

Epoch 00006: val_mDice improved from 0.04721 to 0.07634, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 14s - loss: 5.2523 - acc: 0.8866 - mDice: 0.0760 - val_loss: 3.4182 - val_acc: 0.9137 - val_mDice: 0.1109

Epoch 00007: val_mDice improved from 0.07634 to 0.11086, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 14s - loss: 4.6237 - acc: 0.8881 - mDice: 0.1026 - val_loss: 3.0868 - val_acc: 0.9133 - val_mDice: 0.1470

Epoch 00008: val_mDice improved from 0.11086 to 0.14702, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 14s - loss: 4.1511 - acc: 0.8900 - mDice: 0.1288 - val_loss: 2.8608 - val_acc: 0.9139 - val_mDice: 0.1765

Epoch 00009: val_mDice improved from 0.14702 to 0.17653, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 14s - loss: 3.7757 - acc: 0.8922 - mDice: 0.1566 - val_loss: 2.6484 - val_acc: 0.9161 - val_mDice: 0.2158

Epoch 00010: val_mDice improved from 0.17653 to 0.21581, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 14s - loss: 3.4517 - acc: 0.8954 - mDice: 0.1885 - val_loss: 2.4567 - val_acc: 0.9198 - val_mDice: 0.2575

Epoch 00011: val_mDice improved from 0.21581 to 0.25748, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 13s - loss: 3.1955 - acc: 0.9001 - mDice: 0.2190 - val_loss: 2.3896 - val_acc: 0.9288 - val_mDice: 0.2823

Epoch 00012: val_mDice improved from 0.25748 to 0.28232, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 14s - loss: 2.9937 - acc: 0.9046 - mDice: 0.2470 - val_loss: 2.2729 - val_acc: 0.9315 - val_mDice: 0.3140

Epoch 00013: val_mDice improved from 0.28232 to 0.31398, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 14s - loss: 2.8076 - acc: 0.9092 - mDice: 0.2748 - val_loss: 2.1293 - val_acc: 0.9349 - val_mDice: 0.3481

Epoch 00014: val_mDice improved from 0.31398 to 0.34809, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 14s - loss: 2.6552 - acc: 0.9133 - mDice: 0.3007 - val_loss: 2.1273 - val_acc: 0.9374 - val_mDice: 0.3668

Epoch 00015: val_mDice improved from 0.34809 to 0.36679, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 14s - loss: 2.5305 - acc: 0.9160 - mDice: 0.3211 - val_loss: 2.0413 - val_acc: 0.9385 - val_mDice: 0.3845

Epoch 00016: val_mDice improved from 0.36679 to 0.38446, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 14s - loss: 2.4383 - acc: 0.9180 - mDice: 0.3376 - val_loss: 2.0525 - val_acc: 0.9402 - val_mDice: 0.3909

Epoch 00017: val_mDice improved from 0.38446 to 0.39087, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 14s - loss: 2.3384 - acc: 0.9200 - mDice: 0.3541 - val_loss: 1.9563 - val_acc: 0.9404 - val_mDice: 0.4106

Epoch 00018: val_mDice improved from 0.39087 to 0.41060, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 14s - loss: 2.2609 - acc: 0.9214 - mDice: 0.3694 - val_loss: 2.0199 - val_acc: 0.9418 - val_mDice: 0.4151

Epoch 00019: val_mDice improved from 0.41060 to 0.41514, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 14s - loss: 2.1959 - acc: 0.9228 - mDice: 0.3820 - val_loss: 1.9668 - val_acc: 0.9413 - val_mDice: 0.4196

Epoch 00020: val_mDice improved from 0.41514 to 0.41955, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 14s - loss: 2.1323 - acc: 0.9242 - mDice: 0.3941 - val_loss: 1.9099 - val_acc: 0.9436 - val_mDice: 0.4392

Epoch 00021: val_mDice improved from 0.41955 to 0.43923, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 14s - loss: 2.0756 - acc: 0.9253 - mDice: 0.4061 - val_loss: 2.1013 - val_acc: 0.9394 - val_mDice: 0.4142

Epoch 00022: val_mDice did not improve from 0.43923
Epoch 23/300
 - 14s - loss: 2.0243 - acc: 0.9263 - mDice: 0.4169 - val_loss: 1.9527 - val_acc: 0.9440 - val_mDice: 0.4444

Epoch 00023: val_mDice improved from 0.43923 to 0.44436, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 14s - loss: 1.9835 - acc: 0.9272 - mDice: 0.4255 - val_loss: 1.9778 - val_acc: 0.9469 - val_mDice: 0.4495

Epoch 00024: val_mDice improved from 0.44436 to 0.44950, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 14s - loss: 1.9477 - acc: 0.9280 - mDice: 0.4341 - val_loss: 1.9376 - val_acc: 0.9461 - val_mDice: 0.4505

Epoch 00025: val_mDice improved from 0.44950 to 0.45046, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 14s - loss: 1.8996 - acc: 0.9291 - mDice: 0.4443 - val_loss: 2.0054 - val_acc: 0.9423 - val_mDice: 0.4479

Epoch 00026: val_mDice did not improve from 0.45046
Epoch 27/300
 - 14s - loss: 1.8714 - acc: 0.9298 - mDice: 0.4512 - val_loss: 1.8785 - val_acc: 0.9484 - val_mDice: 0.4728

Epoch 00027: val_mDice improved from 0.45046 to 0.47276, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 14s - loss: 1.8331 - acc: 0.9305 - mDice: 0.4600 - val_loss: 1.9918 - val_acc: 0.9447 - val_mDice: 0.4612

Epoch 00028: val_mDice did not improve from 0.47276
Epoch 29/300
 - 14s - loss: 1.8072 - acc: 0.9312 - mDice: 0.4665 - val_loss: 2.0217 - val_acc: 0.9485 - val_mDice: 0.4682

Epoch 00029: val_mDice did not improve from 0.47276
Epoch 30/300
 - 14s - loss: 1.7807 - acc: 0.9318 - mDice: 0.4725 - val_loss: 2.0523 - val_acc: 0.9476 - val_mDice: 0.4729

Epoch 00030: val_mDice improved from 0.47276 to 0.47294, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 14s - loss: 1.7522 - acc: 0.9323 - mDice: 0.4803 - val_loss: 2.1389 - val_acc: 0.9446 - val_mDice: 0.4553

Epoch 00031: val_mDice did not improve from 0.47294
Epoch 32/300
 - 14s - loss: 1.7257 - acc: 0.9330 - mDice: 0.4870 - val_loss: 2.0900 - val_acc: 0.9485 - val_mDice: 0.4741

Epoch 00032: val_mDice improved from 0.47294 to 0.47409, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 14s - loss: 1.7051 - acc: 0.9333 - mDice: 0.4917 - val_loss: 1.9646 - val_acc: 0.9462 - val_mDice: 0.4859

Epoch 00033: val_mDice improved from 0.47409 to 0.48589, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 14s - loss: 1.6878 - acc: 0.9337 - mDice: 0.4967 - val_loss: 2.0022 - val_acc: 0.9486 - val_mDice: 0.4841

Epoch 00034: val_mDice did not improve from 0.48589
Epoch 35/300
 - 14s - loss: 1.6671 - acc: 0.9341 - mDice: 0.5018 - val_loss: 2.0567 - val_acc: 0.9436 - val_mDice: 0.4692

Epoch 00035: val_mDice did not improve from 0.48589
Epoch 36/300
 - 14s - loss: 1.6534 - acc: 0.9344 - mDice: 0.5056 - val_loss: 2.1322 - val_acc: 0.9447 - val_mDice: 0.4715

Epoch 00036: val_mDice did not improve from 0.48589
Epoch 37/300
 - 14s - loss: 1.6352 - acc: 0.9347 - mDice: 0.5103 - val_loss: 2.0050 - val_acc: 0.9482 - val_mDice: 0.4886

Epoch 00037: val_mDice improved from 0.48589 to 0.48864, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 14s - loss: 1.6207 - acc: 0.9351 - mDice: 0.5144 - val_loss: 2.0095 - val_acc: 0.9471 - val_mDice: 0.4858

Epoch 00038: val_mDice did not improve from 0.48864
Epoch 39/300
 - 14s - loss: 1.6071 - acc: 0.9353 - mDice: 0.5178 - val_loss: 2.0823 - val_acc: 0.9438 - val_mDice: 0.4752

Epoch 00039: val_mDice did not improve from 0.48864
Epoch 40/300
 - 14s - loss: 1.5914 - acc: 0.9356 - mDice: 0.5225 - val_loss: 2.1978 - val_acc: 0.9410 - val_mDice: 0.4517

Epoch 00040: val_mDice did not improve from 0.48864
Epoch 41/300
 - 14s - loss: 1.5838 - acc: 0.9356 - mDice: 0.5239 - val_loss: 2.1082 - val_acc: 0.9484 - val_mDice: 0.4899

Epoch 00041: val_mDice improved from 0.48864 to 0.48991, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 14s - loss: 1.5684 - acc: 0.9359 - mDice: 0.5285 - val_loss: 1.9901 - val_acc: 0.9478 - val_mDice: 0.4979

Epoch 00042: val_mDice improved from 0.48991 to 0.49789, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 14s - loss: 1.5567 - acc: 0.9361 - mDice: 0.5314 - val_loss: 2.1021 - val_acc: 0.9485 - val_mDice: 0.4897

Epoch 00043: val_mDice did not improve from 0.49789
Epoch 44/300
 - 14s - loss: 1.5415 - acc: 0.9362 - mDice: 0.5353 - val_loss: 2.1770 - val_acc: 0.9448 - val_mDice: 0.4765

Epoch 00044: val_mDice did not improve from 0.49789
Epoch 45/300
 - 14s - loss: 1.5329 - acc: 0.9364 - mDice: 0.5388 - val_loss: 2.0889 - val_acc: 0.9475 - val_mDice: 0.4938

Epoch 00045: val_mDice did not improve from 0.49789
Epoch 46/300
 - 14s - loss: 1.5247 - acc: 0.9365 - mDice: 0.5402 - val_loss: 2.0609 - val_acc: 0.9492 - val_mDice: 0.4999

Epoch 00046: val_mDice improved from 0.49789 to 0.49994, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 14s - loss: 1.5131 - acc: 0.9367 - mDice: 0.5440 - val_loss: 2.1854 - val_acc: 0.9487 - val_mDice: 0.4944

Epoch 00047: val_mDice did not improve from 0.49994
Epoch 48/300
 - 14s - loss: 1.5019 - acc: 0.9369 - mDice: 0.5470 - val_loss: 2.1117 - val_acc: 0.9489 - val_mDice: 0.4983

Epoch 00048: val_mDice did not improve from 0.49994
Epoch 49/300
 - 14s - loss: 1.4895 - acc: 0.9372 - mDice: 0.5501 - val_loss: 2.0647 - val_acc: 0.9456 - val_mDice: 0.4907

Epoch 00049: val_mDice did not improve from 0.49994
Epoch 50/300
 - 14s - loss: 1.4809 - acc: 0.9372 - mDice: 0.5534 - val_loss: 2.0042 - val_acc: 0.9476 - val_mDice: 0.5054

Epoch 00050: val_mDice improved from 0.49994 to 0.50538, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 14s - loss: 1.4759 - acc: 0.9372 - mDice: 0.5545 - val_loss: 2.0436 - val_acc: 0.9469 - val_mDice: 0.5017

Epoch 00051: val_mDice did not improve from 0.50538
Epoch 52/300
 - 14s - loss: 1.4668 - acc: 0.9374 - mDice: 0.5569 - val_loss: 2.1186 - val_acc: 0.9477 - val_mDice: 0.5022

Epoch 00052: val_mDice did not improve from 0.50538
Epoch 53/300
 - 14s - loss: 1.4548 - acc: 0.9375 - mDice: 0.5599 - val_loss: 2.1820 - val_acc: 0.9473 - val_mDice: 0.4949

Epoch 00053: val_mDice did not improve from 0.50538
Epoch 54/300
 - 14s - loss: 1.4489 - acc: 0.9376 - mDice: 0.5619 - val_loss: 2.0220 - val_acc: 0.9479 - val_mDice: 0.5095

Epoch 00054: val_mDice improved from 0.50538 to 0.50949, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 14s - loss: 1.4413 - acc: 0.9379 - mDice: 0.5640 - val_loss: 2.0776 - val_acc: 0.9485 - val_mDice: 0.5120

Epoch 00055: val_mDice improved from 0.50949 to 0.51201, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 14s - loss: 1.4391 - acc: 0.9380 - mDice: 0.5649 - val_loss: 2.2000 - val_acc: 0.9444 - val_mDice: 0.4897

Epoch 00056: val_mDice did not improve from 0.51201
Epoch 57/300
 - 14s - loss: 1.4293 - acc: 0.9382 - mDice: 0.5673 - val_loss: 2.1336 - val_acc: 0.9477 - val_mDice: 0.5081

Epoch 00057: val_mDice did not improve from 0.51201
Epoch 58/300
 - 14s - loss: 1.4227 - acc: 0.9383 - mDice: 0.5691 - val_loss: 2.1239 - val_acc: 0.9461 - val_mDice: 0.5043

Epoch 00058: val_mDice did not improve from 0.51201
Epoch 59/300
 - 14s - loss: 1.4050 - acc: 0.9388 - mDice: 0.5740 - val_loss: 2.1320 - val_acc: 0.9472 - val_mDice: 0.5079

Epoch 00059: val_mDice did not improve from 0.51201
Epoch 60/300
 - 14s - loss: 1.4043 - acc: 0.9388 - mDice: 0.5741 - val_loss: 2.1255 - val_acc: 0.9486 - val_mDice: 0.5152

Epoch 00060: val_mDice improved from 0.51201 to 0.51517, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 61/300
 - 14s - loss: 1.3969 - acc: 0.9390 - mDice: 0.5764 - val_loss: 2.2507 - val_acc: 0.9490 - val_mDice: 0.5124

Epoch 00061: val_mDice did not improve from 0.51517
Epoch 62/300
 - 14s - loss: 1.3896 - acc: 0.9391 - mDice: 0.5787 - val_loss: 2.2419 - val_acc: 0.9482 - val_mDice: 0.5054

Epoch 00062: val_mDice did not improve from 0.51517
Epoch 63/300
 - 14s - loss: 1.3878 - acc: 0.9391 - mDice: 0.5790 - val_loss: 2.1549 - val_acc: 0.9439 - val_mDice: 0.4992

Epoch 00063: val_mDice did not improve from 0.51517
Epoch 64/300
 - 14s - loss: 1.3759 - acc: 0.9392 - mDice: 0.5825 - val_loss: 2.4033 - val_acc: 0.9486 - val_mDice: 0.5002

Epoch 00064: val_mDice did not improve from 0.51517
Epoch 65/300
 - 14s - loss: 1.3732 - acc: 0.9394 - mDice: 0.5828 - val_loss: 2.2102 - val_acc: 0.9495 - val_mDice: 0.5185

Epoch 00065: val_mDice improved from 0.51517 to 0.51851, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 66/300
 - 14s - loss: 1.3708 - acc: 0.9393 - mDice: 0.5838 - val_loss: 2.1764 - val_acc: 0.9458 - val_mDice: 0.5086

Epoch 00066: val_mDice did not improve from 0.51851
Epoch 67/300
 - 14s - loss: 1.3606 - acc: 0.9395 - mDice: 0.5868 - val_loss: 2.1614 - val_acc: 0.9485 - val_mDice: 0.5123

Epoch 00067: val_mDice did not improve from 0.51851
Epoch 68/300
 - 14s - loss: 1.3579 - acc: 0.9396 - mDice: 0.5878 - val_loss: 2.1165 - val_acc: 0.9485 - val_mDice: 0.5174

Epoch 00068: val_mDice did not improve from 0.51851
Epoch 69/300
 - 14s - loss: 1.3535 - acc: 0.9398 - mDice: 0.5889 - val_loss: 2.1372 - val_acc: 0.9489 - val_mDice: 0.5168

Epoch 00069: val_mDice did not improve from 0.51851
Epoch 70/300
 - 14s - loss: 1.3476 - acc: 0.9399 - mDice: 0.5903 - val_loss: 2.0804 - val_acc: 0.9491 - val_mDice: 0.5261

Epoch 00070: val_mDice improved from 0.51851 to 0.52610, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 71/300
 - 14s - loss: 1.3441 - acc: 0.9401 - mDice: 0.5916 - val_loss: 2.2117 - val_acc: 0.9483 - val_mDice: 0.5177

Epoch 00071: val_mDice did not improve from 0.52610
Epoch 72/300
 - 14s - loss: 1.3401 - acc: 0.9402 - mDice: 0.5920 - val_loss: 2.1563 - val_acc: 0.9489 - val_mDice: 0.5221

Epoch 00072: val_mDice did not improve from 0.52610
Epoch 73/300
 - 14s - loss: 1.3383 - acc: 0.9402 - mDice: 0.5933 - val_loss: 2.1419 - val_acc: 0.9476 - val_mDice: 0.5156

Epoch 00073: val_mDice did not improve from 0.52610
Epoch 74/300
 - 13s - loss: 1.3327 - acc: 0.9405 - mDice: 0.5950 - val_loss: 2.1808 - val_acc: 0.9492 - val_mDice: 0.5219

Epoch 00074: val_mDice did not improve from 0.52610
Epoch 75/300
 - 14s - loss: 1.3216 - acc: 0.9407 - mDice: 0.5980 - val_loss: 2.1562 - val_acc: 0.9487 - val_mDice: 0.5259

Epoch 00075: val_mDice did not improve from 0.52610
Epoch 76/300
 - 14s - loss: 1.3188 - acc: 0.9409 - mDice: 0.5987 - val_loss: 2.1802 - val_acc: 0.9490 - val_mDice: 0.5199

Epoch 00076: val_mDice did not improve from 0.52610
Epoch 77/300
 - 15s - loss: 1.3139 - acc: 0.9411 - mDice: 0.6001 - val_loss: 2.2138 - val_acc: 0.9490 - val_mDice: 0.5208

Epoch 00077: val_mDice did not improve from 0.52610
Epoch 78/300
 - 14s - loss: 1.3128 - acc: 0.9410 - mDice: 0.6004 - val_loss: 2.1662 - val_acc: 0.9486 - val_mDice: 0.5212

Epoch 00078: val_mDice did not improve from 0.52610
Epoch 79/300
 - 14s - loss: 1.3132 - acc: 0.9410 - mDice: 0.6003 - val_loss: 2.1658 - val_acc: 0.9496 - val_mDice: 0.5210

Epoch 00079: val_mDice did not improve from 0.52610
Epoch 80/300
 - 14s - loss: 1.3086 - acc: 0.9412 - mDice: 0.6016 - val_loss: 2.2307 - val_acc: 0.9472 - val_mDice: 0.5141

Epoch 00080: val_mDice did not improve from 0.52610
Epoch 81/300
 - 13s - loss: 1.3026 - acc: 0.9415 - mDice: 0.6036 - val_loss: 2.0739 - val_acc: 0.9493 - val_mDice: 0.5291

Epoch 00081: val_mDice improved from 0.52610 to 0.52905, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 82/300
 - 14s - loss: 1.2986 - acc: 0.9416 - mDice: 0.6042 - val_loss: 2.1495 - val_acc: 0.9505 - val_mDice: 0.5252

Epoch 00082: val_mDice did not improve from 0.52905
Epoch 83/300
 - 14s - loss: 1.2918 - acc: 0.9418 - mDice: 0.6059 - val_loss: 2.2111 - val_acc: 0.9488 - val_mDice: 0.5193

Epoch 00083: val_mDice did not improve from 0.52905
Epoch 84/300
 - 14s - loss: 1.2892 - acc: 0.9419 - mDice: 0.6075 - val_loss: 2.1921 - val_acc: 0.9506 - val_mDice: 0.5259

Epoch 00084: val_mDice did not improve from 0.52905
Epoch 85/300
 - 14s - loss: 1.2834 - acc: 0.9421 - mDice: 0.6088 - val_loss: 2.1889 - val_acc: 0.9504 - val_mDice: 0.5276

Epoch 00085: val_mDice did not improve from 0.52905
Epoch 86/300
 - 14s - loss: 1.2894 - acc: 0.9419 - mDice: 0.6073 - val_loss: 2.2670 - val_acc: 0.9499 - val_mDice: 0.5146

Epoch 00086: val_mDice did not improve from 0.52905
Epoch 87/300
 - 14s - loss: 1.2825 - acc: 0.9421 - mDice: 0.6091 - val_loss: 2.1519 - val_acc: 0.9469 - val_mDice: 0.5139

Epoch 00087: val_mDice did not improve from 0.52905
Epoch 88/300
 - 14s - loss: 1.2857 - acc: 0.9421 - mDice: 0.6089 - val_loss: 2.2159 - val_acc: 0.9500 - val_mDice: 0.5221

Epoch 00088: val_mDice did not improve from 0.52905
Epoch 89/300
 - 14s - loss: 1.2748 - acc: 0.9423 - mDice: 0.6111 - val_loss: 2.2873 - val_acc: 0.9506 - val_mDice: 0.5197

Epoch 00089: val_mDice did not improve from 0.52905
Epoch 90/300
 - 14s - loss: 1.2736 - acc: 0.9424 - mDice: 0.6117 - val_loss: 2.2377 - val_acc: 0.9499 - val_mDice: 0.5261

Epoch 00090: val_mDice did not improve from 0.52905
Epoch 91/300
 - 15s - loss: 1.2701 - acc: 0.9425 - mDice: 0.6127 - val_loss: 2.2624 - val_acc: 0.9487 - val_mDice: 0.5142

Epoch 00091: val_mDice did not improve from 0.52905
Epoch 92/300
 - 14s - loss: 1.2710 - acc: 0.9426 - mDice: 0.6128 - val_loss: 2.2748 - val_acc: 0.9499 - val_mDice: 0.5198

Epoch 00092: val_mDice did not improve from 0.52905
Epoch 93/300
 - 15s - loss: 1.2697 - acc: 0.9427 - mDice: 0.6129 - val_loss: 2.2454 - val_acc: 0.9495 - val_mDice: 0.5220

Epoch 00093: val_mDice did not improve from 0.52905
Epoch 94/300
 - 14s - loss: 1.2637 - acc: 0.9428 - mDice: 0.6147 - val_loss: 2.1864 - val_acc: 0.9498 - val_mDice: 0.5275

Epoch 00094: val_mDice did not improve from 0.52905
Epoch 95/300
 - 14s - loss: 1.2612 - acc: 0.9429 - mDice: 0.6155 - val_loss: 2.2314 - val_acc: 0.9472 - val_mDice: 0.5133

Epoch 00095: val_mDice did not improve from 0.52905
Epoch 96/300
 - 15s - loss: 1.2593 - acc: 0.9430 - mDice: 0.6159 - val_loss: 2.2253 - val_acc: 0.9497 - val_mDice: 0.5169

Epoch 00096: val_mDice did not improve from 0.52905
Epoch 97/300
 - 14s - loss: 1.2544 - acc: 0.9431 - mDice: 0.6170 - val_loss: 2.3661 - val_acc: 0.9500 - val_mDice: 0.5201

Epoch 00097: val_mDice did not improve from 0.52905
Epoch 98/300
 - 15s - loss: 1.2569 - acc: 0.9431 - mDice: 0.6172 - val_loss: 2.2523 - val_acc: 0.9500 - val_mDice: 0.5242

Epoch 00098: val_mDice did not improve from 0.52905
Epoch 99/300
 - 14s - loss: 1.2500 - acc: 0.9432 - mDice: 0.6185 - val_loss: 2.2735 - val_acc: 0.9505 - val_mDice: 0.5314

Epoch 00099: val_mDice improved from 0.52905 to 0.53144, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 100/300
 - 14s - loss: 1.2490 - acc: 0.9434 - mDice: 0.6189 - val_loss: 2.3027 - val_acc: 0.9513 - val_mDice: 0.5200

Epoch 00100: val_mDice did not improve from 0.53144
Epoch 101/300
 - 15s - loss: 1.2451 - acc: 0.9435 - mDice: 0.6201 - val_loss: 2.3059 - val_acc: 0.9495 - val_mDice: 0.5243

Epoch 00101: val_mDice did not improve from 0.53144
Epoch 102/300
 - 15s - loss: 1.2452 - acc: 0.9435 - mDice: 0.6204 - val_loss: 2.3495 - val_acc: 0.9495 - val_mDice: 0.5215

Epoch 00102: val_mDice did not improve from 0.53144
Epoch 103/300
 - 15s - loss: 1.2379 - acc: 0.9437 - mDice: 0.6225 - val_loss: 2.2527 - val_acc: 0.9500 - val_mDice: 0.5258

Epoch 00103: val_mDice did not improve from 0.53144
Epoch 104/300
 - 15s - loss: 1.2383 - acc: 0.9437 - mDice: 0.6220 - val_loss: 2.2560 - val_acc: 0.9485 - val_mDice: 0.5237

Epoch 00104: val_mDice did not improve from 0.53144
Epoch 105/300
 - 14s - loss: 1.2391 - acc: 0.9437 - mDice: 0.6219 - val_loss: 2.1902 - val_acc: 0.9495 - val_mDice: 0.5283

Epoch 00105: val_mDice did not improve from 0.53144
Epoch 106/300
 - 15s - loss: 1.2281 - acc: 0.9440 - mDice: 0.6248 - val_loss: 2.2133 - val_acc: 0.9486 - val_mDice: 0.5270

Epoch 00106: val_mDice did not improve from 0.53144
Epoch 107/300
 - 15s - loss: 1.2355 - acc: 0.9439 - mDice: 0.6231 - val_loss: 2.2925 - val_acc: 0.9499 - val_mDice: 0.5274

Epoch 00107: val_mDice did not improve from 0.53144
Epoch 108/300
 - 15s - loss: 1.2314 - acc: 0.9440 - mDice: 0.6244 - val_loss: 2.2554 - val_acc: 0.9509 - val_mDice: 0.5278

Epoch 00108: val_mDice did not improve from 0.53144
Epoch 109/300
 - 15s - loss: 1.2300 - acc: 0.9441 - mDice: 0.6244 - val_loss: 2.2334 - val_acc: 0.9495 - val_mDice: 0.5252

Epoch 00109: val_mDice did not improve from 0.53144
Epoch 110/300
 - 15s - loss: 1.2244 - acc: 0.9442 - mDice: 0.6260 - val_loss: 2.2518 - val_acc: 0.9502 - val_mDice: 0.5297

Epoch 00110: val_mDice did not improve from 0.53144
Epoch 111/300
 - 15s - loss: 1.2191 - acc: 0.9444 - mDice: 0.6280 - val_loss: 2.5031 - val_acc: 0.9506 - val_mDice: 0.5163

Epoch 00111: val_mDice did not improve from 0.53144
Epoch 112/300
 - 15s - loss: 1.2249 - acc: 0.9442 - mDice: 0.6265 - val_loss: 2.2733 - val_acc: 0.9510 - val_mDice: 0.5262

Epoch 00112: val_mDice did not improve from 0.53144
Epoch 113/300
 - 15s - loss: 1.2241 - acc: 0.9443 - mDice: 0.6265 - val_loss: 2.4244 - val_acc: 0.9506 - val_mDice: 0.5141

Epoch 00113: val_mDice did not improve from 0.53144
Epoch 114/300
 - 15s - loss: 1.2152 - acc: 0.9446 - mDice: 0.6291 - val_loss: 2.3307 - val_acc: 0.9503 - val_mDice: 0.5253

Epoch 00114: val_mDice did not improve from 0.53144
Epoch 115/300
 - 15s - loss: 1.2190 - acc: 0.9445 - mDice: 0.6281 - val_loss: 2.4350 - val_acc: 0.9496 - val_mDice: 0.5213

Epoch 00115: val_mDice did not improve from 0.53144
Epoch 116/300
 - 15s - loss: 1.2152 - acc: 0.9446 - mDice: 0.6287 - val_loss: 2.2928 - val_acc: 0.9466 - val_mDice: 0.5137

Epoch 00116: val_mDice did not improve from 0.53144
Epoch 117/300
 - 15s - loss: 1.2168 - acc: 0.9446 - mDice: 0.6290 - val_loss: 2.3016 - val_acc: 0.9507 - val_mDice: 0.5259

Epoch 00117: val_mDice did not improve from 0.53144
Epoch 118/300
 - 15s - loss: 1.2198 - acc: 0.9446 - mDice: 0.6284 - val_loss: 2.3457 - val_acc: 0.9508 - val_mDice: 0.5239

Epoch 00118: val_mDice did not improve from 0.53144
Epoch 119/300
 - 14s - loss: 1.2136 - acc: 0.9447 - mDice: 0.6298 - val_loss: 2.4227 - val_acc: 0.9497 - val_mDice: 0.5188

Epoch 00119: val_mDice did not improve from 0.53144
Epoch 120/300
 - 15s - loss: 1.2081 - acc: 0.9449 - mDice: 0.6315 - val_loss: 2.2740 - val_acc: 0.9503 - val_mDice: 0.5280

Epoch 00120: val_mDice did not improve from 0.53144
Epoch 121/300
 - 15s - loss: 1.2045 - acc: 0.9449 - mDice: 0.6322 - val_loss: 2.2388 - val_acc: 0.9516 - val_mDice: 0.5370

Epoch 00121: val_mDice improved from 0.53144 to 0.53704, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 122/300
 - 15s - loss: 1.2078 - acc: 0.9449 - mDice: 0.6317 - val_loss: 2.4620 - val_acc: 0.9488 - val_mDice: 0.5036

Epoch 00122: val_mDice did not improve from 0.53704
Epoch 123/300
 - 15s - loss: 1.2034 - acc: 0.9450 - mDice: 0.6329 - val_loss: 2.2499 - val_acc: 0.9502 - val_mDice: 0.5275

Epoch 00123: val_mDice did not improve from 0.53704
Epoch 124/300
 - 15s - loss: 1.2014 - acc: 0.9452 - mDice: 0.6335 - val_loss: 2.3418 - val_acc: 0.9512 - val_mDice: 0.5260

Epoch 00124: val_mDice did not improve from 0.53704
Epoch 125/300
 - 14s - loss: 1.2030 - acc: 0.9451 - mDice: 0.6329 - val_loss: 2.3286 - val_acc: 0.9476 - val_mDice: 0.5186

Epoch 00125: val_mDice did not improve from 0.53704
Epoch 126/300
 - 16s - loss: 1.2017 - acc: 0.9452 - mDice: 0.6335 - val_loss: 2.3382 - val_acc: 0.9511 - val_mDice: 0.5364

Epoch 00126: val_mDice did not improve from 0.53704
Epoch 127/300
 - 16s - loss: 1.2003 - acc: 0.9452 - mDice: 0.6338 - val_loss: 2.3575 - val_acc: 0.9510 - val_mDice: 0.5274

Epoch 00127: val_mDice did not improve from 0.53704
Epoch 128/300
 - 16s - loss: 1.1923 - acc: 0.9454 - mDice: 0.6366 - val_loss: 2.3101 - val_acc: 0.9509 - val_mDice: 0.5325

Epoch 00128: val_mDice did not improve from 0.53704
Epoch 129/300
 - 16s - loss: 1.1965 - acc: 0.9453 - mDice: 0.6349 - val_loss: 2.3382 - val_acc: 0.9491 - val_mDice: 0.5255

Epoch 00129: val_mDice did not improve from 0.53704
Epoch 130/300
 - 16s - loss: 1.1930 - acc: 0.9455 - mDice: 0.6361 - val_loss: 2.4628 - val_acc: 0.9509 - val_mDice: 0.5267

Epoch 00130: val_mDice did not improve from 0.53704
Epoch 131/300
 - 16s - loss: 1.1944 - acc: 0.9454 - mDice: 0.6357 - val_loss: 2.3321 - val_acc: 0.9513 - val_mDice: 0.5331

Epoch 00131: val_mDice did not improve from 0.53704
Epoch 132/300
 - 16s - loss: 1.1923 - acc: 0.9454 - mDice: 0.6359 - val_loss: 2.3767 - val_acc: 0.9518 - val_mDice: 0.5275

Epoch 00132: val_mDice did not improve from 0.53704
Epoch 133/300
 - 16s - loss: 1.1884 - acc: 0.9456 - mDice: 0.6371 - val_loss: 2.4435 - val_acc: 0.9505 - val_mDice: 0.5234

Epoch 00133: val_mDice did not improve from 0.53704
Epoch 134/300
 - 17s - loss: 1.1893 - acc: 0.9456 - mDice: 0.6370 - val_loss: 2.3810 - val_acc: 0.9494 - val_mDice: 0.5205

Epoch 00134: val_mDice did not improve from 0.53704
Epoch 135/300
 - 16s - loss: 1.1894 - acc: 0.9454 - mDice: 0.6370 - val_loss: 2.2884 - val_acc: 0.9506 - val_mDice: 0.5316

Epoch 00135: val_mDice did not improve from 0.53704
Epoch 136/300
 - 16s - loss: 1.1876 - acc: 0.9456 - mDice: 0.6377 - val_loss: 2.3842 - val_acc: 0.9504 - val_mDice: 0.5241

Epoch 00136: val_mDice did not improve from 0.53704
Epoch 137/300
 - 16s - loss: 1.1835 - acc: 0.9458 - mDice: 0.6387 - val_loss: 2.4409 - val_acc: 0.9487 - val_mDice: 0.5136

Epoch 00137: val_mDice did not improve from 0.53704
Epoch 138/300
 - 16s - loss: 1.1838 - acc: 0.9459 - mDice: 0.6392 - val_loss: 2.3913 - val_acc: 0.9480 - val_mDice: 0.5184

Epoch 00138: val_mDice did not improve from 0.53704
Epoch 139/300
 - 16s - loss: 1.1812 - acc: 0.9459 - mDice: 0.6396 - val_loss: 2.3760 - val_acc: 0.9495 - val_mDice: 0.5287

Epoch 00139: val_mDice did not improve from 0.53704
Epoch 140/300
 - 17s - loss: 1.1812 - acc: 0.9459 - mDice: 0.6400 - val_loss: 2.3230 - val_acc: 0.9500 - val_mDice: 0.5288

Epoch 00140: val_mDice did not improve from 0.53704
Epoch 141/300
 - 16s - loss: 1.1822 - acc: 0.9459 - mDice: 0.6389 - val_loss: 2.3558 - val_acc: 0.9507 - val_mDice: 0.5290

Epoch 00141: val_mDice did not improve from 0.53704
Epoch 142/300
 - 16s - loss: 1.1781 - acc: 0.9460 - mDice: 0.6404 - val_loss: 2.3618 - val_acc: 0.9495 - val_mDice: 0.5249

Epoch 00142: val_mDice did not improve from 0.53704
Epoch 143/300
 - 15s - loss: 1.1765 - acc: 0.9461 - mDice: 0.6413 - val_loss: 2.2880 - val_acc: 0.9510 - val_mDice: 0.5330

Epoch 00143: val_mDice did not improve from 0.53704
Epoch 144/300
 - 15s - loss: 1.1823 - acc: 0.9460 - mDice: 0.6394 - val_loss: 2.4166 - val_acc: 0.9513 - val_mDice: 0.5237

Epoch 00144: val_mDice did not improve from 0.53704
Epoch 145/300
 - 15s - loss: 1.1765 - acc: 0.9460 - mDice: 0.6412 - val_loss: 2.3482 - val_acc: 0.9504 - val_mDice: 0.5261

Epoch 00145: val_mDice did not improve from 0.53704
Epoch 146/300
 - 14s - loss: 1.1711 - acc: 0.9463 - mDice: 0.6423 - val_loss: 2.3834 - val_acc: 0.9505 - val_mDice: 0.5335

Epoch 00146: val_mDice did not improve from 0.53704
Epoch 147/300
 - 15s - loss: 1.1765 - acc: 0.9462 - mDice: 0.6412 - val_loss: 2.4477 - val_acc: 0.9497 - val_mDice: 0.5250

Epoch 00147: val_mDice did not improve from 0.53704
Epoch 148/300
 - 14s - loss: 1.1749 - acc: 0.9463 - mDice: 0.6419 - val_loss: 2.4703 - val_acc: 0.9492 - val_mDice: 0.5182

Epoch 00148: val_mDice did not improve from 0.53704
Epoch 149/300
 - 15s - loss: 1.1749 - acc: 0.9463 - mDice: 0.6420 - val_loss: 2.4083 - val_acc: 0.9506 - val_mDice: 0.5305

Epoch 00149: val_mDice did not improve from 0.53704
Epoch 150/300
 - 14s - loss: 1.1682 - acc: 0.9464 - mDice: 0.6437 - val_loss: 2.4215 - val_acc: 0.9501 - val_mDice: 0.5254

Epoch 00150: val_mDice did not improve from 0.53704
Epoch 151/300
 - 14s - loss: 1.1666 - acc: 0.9465 - mDice: 0.6442 - val_loss: 2.3927 - val_acc: 0.9505 - val_mDice: 0.5280

Epoch 00151: val_mDice did not improve from 0.53704
Restoring model weights from the end of the best epoch
Epoch 00151: early stopping
{'val_loss': [12.610307384469655, 6.315123153132434, 5.137103189969196, 4.801161790027299, 4.267919632309642, 3.769470244146592, 3.4181930645884084, 3.0868338379780007, 2.8608478420939525, 2.6483746054452224, 2.456661579995182, 2.3895686445289486, 2.272855469634413, 2.1293295508656422, 2.1273429087420417, 2.04129132345402, 2.0524544129824505, 1.956255876152209, 2.0198777521122766, 1.966772532329879, 1.909935564968173, 2.1013110203449954, 1.9527426485242791, 1.977819769076129, 1.937628133336925, 2.00535994734844, 1.8784904966141258, 1.991778943125762, 2.0216749074072813, 2.052322670068155, 2.1388892501426144, 2.0900416827068646, 1.9646122468916398, 2.002155792779763, 2.056686681076135, 2.1321632688937906, 2.005016900973613, 2.0095299768714265, 2.082314805611552, 2.1978253465790987, 2.1082350994621577, 1.9901253311327716, 2.1021009756866116, 2.176999129396577, 2.0888646221693667, 2.060913691973553, 2.185386565810475, 2.111695714503027, 2.0647291631005995, 2.004219725145308, 2.0435841602986087, 2.118557036255991, 2.181975568473006, 2.021959860231623, 2.0775685696628505, 2.200027146152944, 2.133644085356643, 2.123866226420056, 2.131988968929099, 2.1255321276254495, 2.250656650053056, 2.2418519731340463, 2.1549315492534107, 2.4033366408428, 2.210181810336406, 2.1764366746614767, 2.161370987332733, 2.1164993227527127, 2.137165411890552, 2.080378208746457, 2.211725594611141, 2.1563092716579333, 2.1419023401910366, 2.1807694701509104, 2.15619793151344, 2.1802128533411294, 2.213845323583933, 2.1661688762004148, 2.1657556621722005, 2.2307286808610627, 2.073903844343217, 2.1495027022654782, 2.211058146460762, 2.192128518440204, 2.1888789637794708, 2.2670421347271796, 2.1519042859530315, 2.2158993182901563, 2.2873045532397054, 2.237678866146663, 2.2624466858762604, 2.2747691610005982, 2.2454416552069465, 2.1863921927340204, 2.2314003885791287, 2.2253305965295715, 2.3661427804211663, 2.252287483748111, 2.273536620859327, 2.302655691541107, 2.305874226479557, 2.3494997504037185, 2.252670411957043, 2.2560135479079944, 2.1901695049008842, 2.213298481935895, 2.2925209972445524, 2.2553555059699373, 2.2334114572855346, 2.2517503679797635, 2.503079899196518, 2.273329092803614, 2.4244093242304285, 2.330715930661676, 2.4350274355051904, 2.2928349665423347, 2.301618569390068, 2.3457174314466935, 2.422680078272047, 2.2740158528589003, 2.23884847443863, 2.4619903271424706, 2.24989015696435, 2.3417858651230454, 2.3286012844000448, 2.3381882140090346, 2.35750591688316, 2.3101006273450797, 2.3382182294430014, 2.4628184241289532, 2.332132149009065, 2.376688233966934, 2.4434921315262437, 2.3810177129074184, 2.2884108087869994, 2.384188686669206, 2.440915170328577, 2.391318972550291, 2.375966297181625, 2.323044856833346, 2.3558472694631396, 2.3618232937498465, 2.2880195985293255, 2.416591625639846, 2.3482322186731093, 2.3834294646811887, 2.4476630687713623, 2.470288995923943, 2.408346005658198, 2.421519493923507, 2.392679891106803], 'val_acc': [0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9130937820040314, 0.9131888127859744, 0.9137136000494718, 0.9132652589062739, 0.913924348420937, 0.9160771659632635, 0.9197877878583344, 0.9288081506777076, 0.9315353025937213, 0.9349029889985836, 0.9373822285476343, 0.9384565899491976, 0.9402147851176768, 0.9403676693665914, 0.9417581268528986, 0.9412994514630494, 0.9435948263333497, 0.9393573733015433, 0.9440163230762801, 0.9469005199118034, 0.9461133510040838, 0.9423345577117451, 0.9483694514082797, 0.944671240265809, 0.9484810669328914, 0.9475781987499259, 0.9446443902047653, 0.9484603701357069, 0.9461732530061093, 0.9486442511308126, 0.9435741848119811, 0.9446877773913591, 0.9481731856335475, 0.9470967740319961, 0.9437787196489685, 0.9410246920319243, 0.9484169719605472, 0.947803368781532, 0.9484521320412279, 0.9447952319123891, 0.9474769214678077, 0.9491773284347363, 0.9487434355906268, 0.9488694641177214, 0.945557584642698, 0.9475637011687849, 0.9469459626261748, 0.947656679086845, 0.9472682862308438, 0.9478798232265024, 0.9485389050824682, 0.9444150615004854, 0.9476814796138742, 0.9461071574488166, 0.947218711482746, 0.9486360003828337, 0.9489934371170385, 0.9482145162934031, 0.9439026636784303, 0.9485967442310056, 0.9495430089241965, 0.945811686569086, 0.9485347560664129, 0.9485285705028299, 0.9488901043071427, 0.9490822596922933, 0.94834882054249, 0.9489314576101037, 0.9475740327515416, 0.949235164919379, 0.9487269014619583, 0.9490492057533904, 0.9489582873589499, 0.9485863983298147, 0.9496442182769989, 0.9471773561818639, 0.9492991946263021, 0.9505284931406629, 0.9488116269671051, 0.9506070041123715, 0.9504003887735931, 0.9498611558749023, 0.9468984563923415, 0.9499996041452419, 0.9505884134569648, 0.9498652902395366, 0.9487165635524515, 0.9498549699783325, 0.9494541373998759, 0.9498219180373506, 0.9471608536869454, 0.9496938336494914, 0.9499975083260562, 0.9500223221725592, 0.9505243780892655, 0.9512578235658188, 0.949489279832254, 0.9494686196636222, 0.9499789473064785, 0.9485409569473906, 0.9495429732946045, 0.948605006966511, 0.9498797485282301, 0.9509127709452666, 0.9495326566962556, 0.9501917425480635, 0.950555356854167, 0.9509520304269631, 0.9506235312483164, 0.9502805631253972, 0.9496483493117647, 0.9465947527459214, 0.9507227103803411, 0.9508156646563354, 0.94968348042259, 0.9502619634793458, 0.9515656326070178, 0.9488033539090077, 0.9501669416880475, 0.9511503670468676, 0.9476318978730527, 0.9510739342460419, 0.9510119507432649, 0.9509065863806442, 0.9490698805734432, 0.9509375598177564, 0.9512867278226927, 0.9518321742558612, 0.9505305483354537, 0.9493983471193793, 0.9505966855161017, 0.9504251976252934, 0.9487310338286714, 0.9479831200738192, 0.9494500093619916, 0.9500037401748103, 0.9506751798384683, 0.949514044729691, 0.9509830171835489, 0.951334238385355, 0.9503818017810417, 0.9505057624598455, 0.949739251056863, 0.949210368388192, 0.9506255977646598, 0.9501132309103811, 0.950501635087935], 'val_mDice': [0.009552725346228265, 0.013902051806158527, 0.028891709846491254, 0.04009548383218616, 0.04720601422826671, 0.07634351381709456, 0.11085708123012628, 0.1470178433137233, 0.1765344549157766, 0.2158149968645426, 0.25747952031689647, 0.28232269413644373, 0.3139775851585346, 0.3480865955352783, 0.3667886626787026, 0.3844575116088271, 0.3908716460180016, 0.41060207809149885, 0.4151416797211716, 0.41955494630936135, 0.4392339257554635, 0.41416579784627733, 0.44436139700799016, 0.4495024749353611, 0.45045864082581505, 0.4478920901287867, 0.47275735762532195, 0.46123338245146767, 0.46821461376531165, 0.4729413581627041, 0.45529425760221215, 0.47409067490247375, 0.48588742457288603, 0.48411998802057193, 0.46923285589537805, 0.47153067821896943, 0.4886380189290926, 0.48578488793453023, 0.47520360167466064, 0.4516953829280491, 0.48990522849493184, 0.4978915514559719, 0.4896884052780087, 0.4764980900221031, 0.4938388158822193, 0.49994099606348813, 0.4943624610008474, 0.49829715157354343, 0.49069930621365593, 0.5053842913505086, 0.5017270365906827, 0.5021804631089365, 0.4948896665812871, 0.5094938113369756, 0.512010268991886, 0.4897144799791901, 0.5081368947828282, 0.5043281559837597, 0.507921601806939, 0.51517298984128, 0.5123805208912109, 0.5053909351039865, 0.4992039391781365, 0.5001963723305217, 0.518512455777749, 0.5086114822819247, 0.5122924485353119, 0.5174076776930739, 0.51676144563286, 0.5261025884963947, 0.5177258855803719, 0.5220857300904876, 0.5155546453745006, 0.5219273250862206, 0.5258757282235769, 0.519869034183758, 0.520774640184541, 0.5212247594774768, 0.5210152240105848, 0.5141396930430855, 0.5290502452983536, 0.5252352360240574, 0.5192578355027311, 0.5258614539101137, 0.5275868270650256, 0.5145671264419343, 0.5138907832140364, 0.5221465028531058, 0.5197115105956627, 0.5260589512366822, 0.5142078388003664, 0.5198074980488037, 0.5219594410011889, 0.5275383285304022, 0.5132998784161147, 0.5168871546590794, 0.5200600485894933, 0.5242079622918667, 0.5314383581696942, 0.5200161674169189, 0.5242616953796515, 0.5215034999327952, 0.5258079480858489, 0.5236622191674216, 0.5283410029704344, 0.5269613792110421, 0.5273681279667263, 0.5278064288906545, 0.5252274702714143, 0.529717731908713, 0.5162905591160225, 0.5262304501160563, 0.5140749988609186, 0.5252666689830119, 0.5212754778688846, 0.5137059483781207, 0.5258679290057561, 0.5239111731172273, 0.5187893243475333, 0.5280197993670096, 0.5370429138231544, 0.5036381771111621, 0.5275458298914926, 0.5259526095576792, 0.518582039372215, 0.5363585669235145, 0.5274392414692394, 0.5325323245045859, 0.5254754064469364, 0.5267026799018156, 0.5330532566129162, 0.5274749756192362, 0.5234415699316802, 0.5204685350370141, 0.5316408616204501, 0.5241151828339646, 0.5135825694273304, 0.518424763359837, 0.5286992019115213, 0.5288207299549487, 0.5290499279618929, 0.5248684380307543, 0.5330155292036813, 0.5236512231094211, 0.5260863623805552, 0.5334892036528561, 0.52502559566631, 0.5181651401786165, 0.530499177938067, 0.5253646102364503, 0.5280080257847323], 'loss': [127.76453572899601, 19.29758372121657, 10.87879165287183, 8.19572857951853, 6.8999484052430065, 5.998855599663134, 5.252307977676911, 4.623678381293306, 4.151050411092125, 3.775716589788006, 3.4516509521969083, 3.195500587782084, 2.9937233352874086, 2.8075560017365997, 2.6552209719600817, 2.5305243081422186, 2.4383062209044724, 2.338364818031106, 2.26086593780582, 2.195948351118392, 2.1323117748285116, 2.075579684593924, 2.024285743604149, 1.9835052761492156, 1.9477145682946704, 1.8995754145425452, 1.8713861871314954, 1.8330818749897284, 1.807172280823663, 1.780657580070107, 1.7521504916615505, 1.7256785593133601, 1.7050618149831556, 1.6878491472453894, 1.6670624092963844, 1.6533501491483784, 1.6352271413719996, 1.6206896563876358, 1.6070938264224455, 1.5913768682167697, 1.5837974001604138, 1.56842300696581, 1.5567199702057974, 1.5415081064021052, 1.5329389482222056, 1.524738830137432, 1.513073651900083, 1.5019277809790865, 1.4894803044881821, 1.4808899704702707, 1.4759382085598642, 1.4667507998077616, 1.4547652344215176, 1.4489240922489481, 1.4412549355016946, 1.4390660823803634, 1.429334056565045, 1.4226689611824697, 1.4049688059418466, 1.4042517733719346, 1.3968630787996572, 1.3896492405119794, 1.3878486462975825, 1.3759447115028964, 1.3731789088386848, 1.370795360175582, 1.3605543660881225, 1.3578686650772835, 1.3534744191608634, 1.347586544447167, 1.3440926017198198, 1.3400580208842112, 1.3382674980011648, 1.3326977096827914, 1.321615400796596, 1.318782552551661, 1.3138830129516312, 1.31277564208717, 1.3131544467871903, 1.3085617708047033, 1.30259644933908, 1.2986448753192388, 1.291810721141127, 1.2891962754460866, 1.2833541112491218, 1.289411649657363, 1.2825157243431176, 1.285732243537747, 1.2748090488076644, 1.2735774897878502, 1.2700738058319525, 1.2710341404910233, 1.2696730595360615, 1.2636513366242663, 1.261232261135456, 1.259281856926313, 1.2543879726647122, 1.2568595203710053, 1.2500401456192047, 1.2489694086082568, 1.24510051371011, 1.2452080802397818, 1.2379135636065068, 1.2383173194159338, 1.2391451451618825, 1.2281097821125595, 1.2354628497833169, 1.2313562679521906, 1.229957314677945, 1.2243862001245742, 1.2191333071571713, 1.2249261147023727, 1.2241012160310973, 1.2152318019219248, 1.2189588945913494, 1.2152199699925308, 1.2168031071897205, 1.219819120933311, 1.2136126851426523, 1.2080819466347645, 1.2044515329393892, 1.2078011841998888, 1.2033806287783317, 1.2014006445282042, 1.2029572620993574, 1.2017052882208907, 1.200261368879299, 1.1922723335752585, 1.1965371045624273, 1.1930170522153745, 1.1943851380811512, 1.192326155043747, 1.188358443055782, 1.1893481194216613, 1.1894063600903795, 1.1875854001040813, 1.1834797657339229, 1.1838352469751035, 1.181233203412401, 1.1812296567396376, 1.1821944957776473, 1.1780831372839817, 1.1765397339044823, 1.1823430218102398, 1.176453221990849, 1.1710825294205893, 1.1764988251503994, 1.1749100025154253, 1.174867931764556, 1.1682207624082908, 1.1665599996517326], 'acc': [0.7630144239776983, 0.8818191464420861, 0.8852365362502009, 0.8855868200968569, 0.8855426284613858, 0.8857501703671826, 0.8865791451573262, 0.8880545049183484, 0.8900187283869591, 0.8921875932735917, 0.8953675514835852, 0.9000694970797365, 0.904638449007971, 0.9092080070720403, 0.9133149260406199, 0.9160177063126494, 0.9179670423550074, 0.9199676028128237, 0.9213996377037239, 0.9228351428795598, 0.924191201958974, 0.9252782150081258, 0.9262821576699317, 0.9272093854953726, 0.9280040962563239, 0.9290630868931532, 0.9297635565265826, 0.930530959420017, 0.9311737820678959, 0.9318365081126222, 0.9322817969131324, 0.933015159357283, 0.9333028466793312, 0.9336682959979581, 0.9340851475948225, 0.9344400273780341, 0.9347070053404021, 0.9350592449537115, 0.935264386189191, 0.9355884881153598, 0.9355796450630044, 0.9359334167633744, 0.9360625251764877, 0.9362217253553381, 0.9364247941209901, 0.9364595196519917, 0.9367095177732362, 0.9369273890428869, 0.9371636331870232, 0.9372350363831109, 0.9372296577204767, 0.9373659759972534, 0.9375166169705392, 0.9376244551670213, 0.9379160653745601, 0.9379967945566443, 0.9382085879606967, 0.938290060098885, 0.9388014792902564, 0.9387779544819207, 0.9390170353344924, 0.9391334543112063, 0.9391129505925823, 0.9392468117984588, 0.9393728587967749, 0.939259884657537, 0.9395265425026115, 0.9396464041515435, 0.9398049189272961, 0.9399260928685236, 0.9400598321382493, 0.9401962536202362, 0.9402301316200002, 0.9404753182300665, 0.9407039455005101, 0.9408543440460815, 0.9410609203020956, 0.9410274230371042, 0.9410302423423207, 0.9412302490017213, 0.9414870999106252, 0.9416315527037853, 0.9417994761998069, 0.9418951504780156, 0.9420851860734075, 0.9419343036787486, 0.942051026684274, 0.9420611171620868, 0.9423491851280122, 0.9423934766380273, 0.9425436708020928, 0.9426019828626386, 0.9426878466662495, 0.9428056573453575, 0.9428952467265611, 0.9429927138901505, 0.9430905392075032, 0.9431298163246079, 0.943241303964064, 0.9433944794024188, 0.9435013108416831, 0.9435441949536199, 0.9437380799163719, 0.9436860320861676, 0.943736912183438, 0.9439828819221197, 0.9439253761425406, 0.9440026796595813, 0.9440753307485884, 0.944236161158785, 0.9443749373180272, 0.9441947922858009, 0.9443475273879848, 0.9446274764066064, 0.9444764553478007, 0.9446065504075292, 0.9445986732906275, 0.9445661854577749, 0.9447470179285062, 0.9448945346880269, 0.9449095409048012, 0.9449247695003665, 0.9450485418807785, 0.9451537623909796, 0.9450803048911551, 0.9452140844470113, 0.9451794028314628, 0.9454377977931547, 0.9452729389614402, 0.9454542965380223, 0.9453962261325802, 0.9454074460146935, 0.9455881779551357, 0.9456428426957638, 0.9454229127042116, 0.9456258026169742, 0.9457882839404617, 0.9458717152214174, 0.9458764854563548, 0.9459411225075569, 0.945909255850478, 0.9459986064430286, 0.9461138764469655, 0.9459517960597484, 0.9460208210951102, 0.9462734989192542, 0.946201937750244, 0.9462972269761498, 0.946296000943569, 0.9463892748079153, 0.9464682524360465], 'mDice': [0.012210529132030815, 0.012922508895590339, 0.020900366924421914, 0.03165951167287004, 0.039510294375253864, 0.05466640271263882, 0.07597548392626216, 0.1025895426567912, 0.12883878654156836, 0.15655302310436614, 0.18847076412795333, 0.218968655013433, 0.2469852195573745, 0.27478412784306117, 0.3006731670352474, 0.3211448936185561, 0.33764373013139726, 0.35408270926486174, 0.36939338738606275, 0.38203096477419624, 0.39410283882064395, 0.40610487360553305, 0.4169401549515813, 0.42546570882786333, 0.43413321943715744, 0.4443010235720941, 0.45119333916302307, 0.4599817772328701, 0.4664500111228182, 0.47251962710196893, 0.48031153180138114, 0.4869654802535005, 0.49165464193562164, 0.4966590864798205, 0.5017550877415597, 0.5055583567539076, 0.5103101836726736, 0.514399127745692, 0.5177966585333821, 0.5225105389946979, 0.5238633253275585, 0.5284851075004345, 0.5313614692558657, 0.5352799808055148, 0.538848136597278, 0.5402460815366217, 0.5440227366709175, 0.546980118415213, 0.5501069084705009, 0.5534410565372491, 0.5544557521808225, 0.5568832411991214, 0.5598601420313554, 0.5619202180727071, 0.5639833442870454, 0.5649467893697541, 0.5672913121733035, 0.5690569780528769, 0.5739884548188294, 0.5740969681115226, 0.5763997428914923, 0.5787337066677815, 0.5789519066703092, 0.5825290693890574, 0.5828034155256224, 0.583812071541336, 0.5867747684740746, 0.5877787799599137, 0.5889357259788905, 0.5903417494008629, 0.5916180191564115, 0.5919645262481301, 0.5932709322951665, 0.5949669845625827, 0.5979813245521717, 0.5987121801180009, 0.6000701623001647, 0.6004057319963766, 0.6003031212353419, 0.6016447639667899, 0.6036118871051173, 0.6041833599287274, 0.6059492422172131, 0.6074869477971565, 0.608843887529435, 0.6073351175844406, 0.6090923797836894, 0.6088886769805675, 0.611070564269604, 0.6116534451709633, 0.6127200077369669, 0.6127736748358853, 0.6129022197755591, 0.6146710511029374, 0.6155309597103628, 0.6159172990497281, 0.6170247792613305, 0.6172194907130398, 0.6185367267204911, 0.6188862299900949, 0.6201268492892354, 0.6203873537184204, 0.622506818596194, 0.622000521508757, 0.6219464903965696, 0.6247958798637305, 0.6231167490111866, 0.6244066879644249, 0.6243765741046182, 0.6259684878739323, 0.6279968120882853, 0.6264619544261253, 0.6265032898431172, 0.6290931670203863, 0.6281099380909777, 0.6286874603396713, 0.6289627098334679, 0.6283954963900413, 0.6297558226309325, 0.6315461344159538, 0.6322408279856594, 0.6317364284853407, 0.6328741552477718, 0.6334670619970832, 0.6328959351829646, 0.6334531758613403, 0.6338413315630642, 0.6365937708827978, 0.6349343412498082, 0.6361271665932651, 0.6357373341022835, 0.6359071465536086, 0.6371435018174237, 0.6369792914236042, 0.6369588698484326, 0.6376502813677936, 0.6387114314548357, 0.6391662837022738, 0.6395705063081659, 0.6399776703794177, 0.6389276351383858, 0.6404032321493323, 0.6413300974969868, 0.6394403232682031, 0.6411655692452731, 0.6423324090801718, 0.641236115950436, 0.6418925249059114, 0.642048596160668, 0.6436890610279415, 0.6441544237742464]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.48s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.22s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  2.01s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:34,  1.60s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:58,  1.69s/it]predicting train subjects:   1%|          | 3/285 [00:05<07:43,  1.65s/it]predicting train subjects:   1%|▏         | 4/285 [00:07<08:10,  1.74s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<07:58,  1.71s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<08:24,  1.81s/it]predicting train subjects:   2%|▏         | 7/285 [00:12<08:59,  1.94s/it]predicting train subjects:   3%|▎         | 8/285 [00:14<09:06,  1.97s/it]predicting train subjects:   3%|▎         | 9/285 [00:16<08:48,  1.91s/it]predicting train subjects:   4%|▎         | 10/285 [00:18<09:06,  1.99s/it]predicting train subjects:   4%|▍         | 11/285 [00:21<09:16,  2.03s/it]predicting train subjects:   4%|▍         | 12/285 [00:23<09:19,  2.05s/it]predicting train subjects:   5%|▍         | 13/285 [00:25<09:23,  2.07s/it]predicting train subjects:   5%|▍         | 14/285 [00:27<09:32,  2.11s/it]predicting train subjects:   5%|▌         | 15/285 [00:29<09:32,  2.12s/it]predicting train subjects:   6%|▌         | 16/285 [00:31<09:39,  2.15s/it]predicting train subjects:   6%|▌         | 17/285 [00:34<09:38,  2.16s/it]predicting train subjects:   6%|▋         | 18/285 [00:36<09:48,  2.20s/it]predicting train subjects:   7%|▋         | 19/285 [00:38<09:40,  2.18s/it]predicting train subjects:   7%|▋         | 20/285 [00:40<09:40,  2.19s/it]predicting train subjects:   7%|▋         | 21/285 [00:42<09:39,  2.20s/it]predicting train subjects:   8%|▊         | 22/285 [00:44<09:32,  2.17s/it]predicting train subjects:   8%|▊         | 23/285 [00:47<09:29,  2.17s/it]predicting train subjects:   8%|▊         | 24/285 [00:49<09:28,  2.18s/it]predicting train subjects:   9%|▉         | 25/285 [00:51<09:25,  2.17s/it]predicting train subjects:   9%|▉         | 26/285 [00:53<09:19,  2.16s/it]predicting train subjects:   9%|▉         | 27/285 [00:55<09:29,  2.21s/it]predicting train subjects:  10%|▉         | 28/285 [00:58<09:15,  2.16s/it]predicting train subjects:  10%|█         | 29/285 [01:00<09:01,  2.11s/it]predicting train subjects:  11%|█         | 30/285 [01:02<08:50,  2.08s/it]predicting train subjects:  11%|█         | 31/285 [01:03<08:38,  2.04s/it]predicting train subjects:  11%|█         | 32/285 [01:06<08:42,  2.06s/it]predicting train subjects:  12%|█▏        | 33/285 [01:08<08:45,  2.09s/it]predicting train subjects:  12%|█▏        | 34/285 [01:10<08:49,  2.11s/it]predicting train subjects:  12%|█▏        | 35/285 [01:12<08:42,  2.09s/it]predicting train subjects:  13%|█▎        | 36/285 [01:14<08:33,  2.06s/it]predicting train subjects:  13%|█▎        | 37/285 [01:16<08:37,  2.09s/it]predicting train subjects:  13%|█▎        | 38/285 [01:18<08:24,  2.04s/it]predicting train subjects:  14%|█▎        | 39/285 [01:20<08:41,  2.12s/it]predicting train subjects:  14%|█▍        | 40/285 [01:22<08:33,  2.09s/it]predicting train subjects:  14%|█▍        | 41/285 [01:24<08:29,  2.09s/it]predicting train subjects:  15%|█▍        | 42/285 [01:26<08:20,  2.06s/it]predicting train subjects:  15%|█▌        | 43/285 [01:28<08:14,  2.04s/it]predicting train subjects:  15%|█▌        | 44/285 [01:30<08:10,  2.03s/it]predicting train subjects:  16%|█▌        | 45/285 [01:32<08:07,  2.03s/it]predicting train subjects:  16%|█▌        | 46/285 [01:34<07:49,  1.96s/it]predicting train subjects:  16%|█▋        | 47/285 [01:36<07:34,  1.91s/it]predicting train subjects:  17%|█▋        | 48/285 [01:38<07:24,  1.88s/it]predicting train subjects:  17%|█▋        | 49/285 [01:40<07:16,  1.85s/it]predicting train subjects:  18%|█▊        | 50/285 [01:41<07:03,  1.80s/it]predicting train subjects:  18%|█▊        | 51/285 [01:43<07:05,  1.82s/it]predicting train subjects:  18%|█▊        | 52/285 [01:45<07:04,  1.82s/it]predicting train subjects:  19%|█▊        | 53/285 [01:47<07:01,  1.82s/it]predicting train subjects:  19%|█▉        | 54/285 [01:49<06:57,  1.81s/it]predicting train subjects:  19%|█▉        | 55/285 [01:50<06:54,  1.80s/it]predicting train subjects:  20%|█▉        | 56/285 [01:52<06:48,  1.79s/it]predicting train subjects:  20%|██        | 57/285 [01:54<06:45,  1.78s/it]predicting train subjects:  20%|██        | 58/285 [01:56<06:50,  1.81s/it]predicting train subjects:  21%|██        | 59/285 [01:58<06:53,  1.83s/it]predicting train subjects:  21%|██        | 60/285 [02:00<06:54,  1.84s/it]predicting train subjects:  21%|██▏       | 61/285 [02:01<06:52,  1.84s/it]predicting train subjects:  22%|██▏       | 62/285 [02:03<06:51,  1.85s/it]predicting train subjects:  22%|██▏       | 63/285 [02:05<06:41,  1.81s/it]predicting train subjects:  22%|██▏       | 64/285 [02:07<06:49,  1.85s/it]predicting train subjects:  23%|██▎       | 65/285 [02:09<06:59,  1.91s/it]predicting train subjects:  23%|██▎       | 66/285 [02:11<07:05,  1.94s/it]predicting train subjects:  24%|██▎       | 67/285 [02:13<06:54,  1.90s/it]predicting train subjects:  24%|██▍       | 68/285 [02:15<06:46,  1.87s/it]predicting train subjects:  24%|██▍       | 69/285 [02:17<06:58,  1.94s/it]predicting train subjects:  25%|██▍       | 70/285 [02:19<06:55,  1.93s/it]predicting train subjects:  25%|██▍       | 71/285 [02:20<06:47,  1.90s/it]predicting train subjects:  25%|██▌       | 72/285 [02:22<06:43,  1.89s/it]predicting train subjects:  26%|██▌       | 73/285 [02:24<06:41,  1.90s/it]predicting train subjects:  26%|██▌       | 74/285 [02:26<06:34,  1.87s/it]predicting train subjects:  26%|██▋       | 75/285 [02:28<06:27,  1.85s/it]predicting train subjects:  27%|██▋       | 76/285 [02:30<06:30,  1.87s/it]predicting train subjects:  27%|██▋       | 77/285 [02:32<06:24,  1.85s/it]predicting train subjects:  27%|██▋       | 78/285 [02:33<06:23,  1.85s/it]predicting train subjects:  28%|██▊       | 79/285 [02:35<06:27,  1.88s/it]predicting train subjects:  28%|██▊       | 80/285 [02:37<06:28,  1.90s/it]predicting train subjects:  28%|██▊       | 81/285 [02:39<06:28,  1.90s/it]predicting train subjects:  29%|██▉       | 82/285 [02:41<06:21,  1.88s/it]predicting train subjects:  29%|██▉       | 83/285 [02:43<06:18,  1.87s/it]predicting train subjects:  29%|██▉       | 84/285 [02:45<06:13,  1.86s/it]predicting train subjects:  30%|██▉       | 85/285 [02:47<06:20,  1.90s/it]predicting train subjects:  30%|███       | 86/285 [02:49<06:25,  1.94s/it]predicting train subjects:  31%|███       | 87/285 [02:51<06:23,  1.94s/it]predicting train subjects:  31%|███       | 88/285 [02:53<06:27,  1.97s/it]predicting train subjects:  31%|███       | 89/285 [02:55<06:26,  1.97s/it]predicting train subjects:  32%|███▏      | 90/285 [02:57<06:26,  1.98s/it]predicting train subjects:  32%|███▏      | 91/285 [02:59<06:26,  1.99s/it]predicting train subjects:  32%|███▏      | 92/285 [03:01<06:23,  1.99s/it]predicting train subjects:  33%|███▎      | 93/285 [03:03<06:21,  1.99s/it]predicting train subjects:  33%|███▎      | 94/285 [03:05<06:23,  2.01s/it]predicting train subjects:  33%|███▎      | 95/285 [03:07<06:23,  2.02s/it]predicting train subjects:  34%|███▎      | 96/285 [03:09<06:24,  2.04s/it]predicting train subjects:  34%|███▍      | 97/285 [03:11<06:27,  2.06s/it]predicting train subjects:  34%|███▍      | 98/285 [03:13<06:16,  2.01s/it]predicting train subjects:  35%|███▍      | 99/285 [03:15<06:12,  2.00s/it]predicting train subjects:  35%|███▌      | 100/285 [03:17<06:08,  1.99s/it]predicting train subjects:  35%|███▌      | 101/285 [03:19<06:10,  2.02s/it]predicting train subjects:  36%|███▌      | 102/285 [03:21<06:09,  2.02s/it]predicting train subjects:  36%|███▌      | 103/285 [03:23<06:10,  2.04s/it]predicting train subjects:  36%|███▋      | 104/285 [03:25<06:04,  2.02s/it]predicting train subjects:  37%|███▋      | 105/285 [03:27<06:12,  2.07s/it]predicting train subjects:  37%|███▋      | 106/285 [03:29<06:06,  2.04s/it]predicting train subjects:  38%|███▊      | 107/285 [03:31<05:59,  2.02s/it]predicting train subjects:  38%|███▊      | 108/285 [03:33<05:56,  2.01s/it]predicting train subjects:  38%|███▊      | 109/285 [03:35<05:55,  2.02s/it]predicting train subjects:  39%|███▊      | 110/285 [03:37<05:55,  2.03s/it]predicting train subjects:  39%|███▉      | 111/285 [03:39<05:55,  2.04s/it]predicting train subjects:  39%|███▉      | 112/285 [03:41<05:48,  2.02s/it]predicting train subjects:  40%|███▉      | 113/285 [03:43<05:42,  1.99s/it]predicting train subjects:  40%|████      | 114/285 [03:45<05:40,  1.99s/it]predicting train subjects:  40%|████      | 115/285 [03:47<05:41,  2.01s/it]predicting train subjects:  41%|████      | 116/285 [03:49<05:43,  2.03s/it]predicting train subjects:  41%|████      | 117/285 [03:51<05:40,  2.03s/it]predicting train subjects:  41%|████▏     | 118/285 [03:53<05:47,  2.08s/it]predicting train subjects:  42%|████▏     | 119/285 [03:55<05:39,  2.05s/it]predicting train subjects:  42%|████▏     | 120/285 [03:58<05:38,  2.05s/it]predicting train subjects:  42%|████▏     | 121/285 [04:00<05:34,  2.04s/it]predicting train subjects:  43%|████▎     | 122/285 [04:01<05:25,  2.00s/it]predicting train subjects:  43%|████▎     | 123/285 [04:03<05:12,  1.93s/it]predicting train subjects:  44%|████▎     | 124/285 [04:05<05:16,  1.97s/it]predicting train subjects:  44%|████▍     | 125/285 [04:07<05:26,  2.04s/it]predicting train subjects:  44%|████▍     | 126/285 [04:10<05:29,  2.07s/it]predicting train subjects:  45%|████▍     | 127/285 [04:12<05:21,  2.04s/it]predicting train subjects:  45%|████▍     | 128/285 [04:14<05:14,  2.01s/it]predicting train subjects:  45%|████▌     | 129/285 [04:16<05:13,  2.01s/it]predicting train subjects:  46%|████▌     | 130/285 [04:18<05:21,  2.08s/it]predicting train subjects:  46%|████▌     | 131/285 [04:20<05:24,  2.11s/it]predicting train subjects:  46%|████▋     | 132/285 [04:22<05:12,  2.04s/it]predicting train subjects:  47%|████▋     | 133/285 [04:24<05:12,  2.06s/it]predicting train subjects:  47%|████▋     | 134/285 [04:26<04:59,  1.98s/it]predicting train subjects:  47%|████▋     | 135/285 [04:28<04:59,  1.99s/it]predicting train subjects:  48%|████▊     | 136/285 [04:30<04:47,  1.93s/it]predicting train subjects:  48%|████▊     | 137/285 [04:32<04:55,  2.00s/it]predicting train subjects:  48%|████▊     | 138/285 [04:34<04:57,  2.02s/it]predicting train subjects:  49%|████▉     | 139/285 [04:36<04:53,  2.01s/it]predicting train subjects:  49%|████▉     | 140/285 [04:38<04:49,  1.99s/it]predicting train subjects:  49%|████▉     | 141/285 [04:40<04:53,  2.04s/it]predicting train subjects:  50%|████▉     | 142/285 [04:42<04:39,  1.96s/it]predicting train subjects:  50%|█████     | 143/285 [04:44<04:38,  1.96s/it]predicting train subjects:  51%|█████     | 144/285 [04:45<04:28,  1.90s/it]predicting train subjects:  51%|█████     | 145/285 [04:47<04:20,  1.86s/it]predicting train subjects:  51%|█████     | 146/285 [04:49<04:19,  1.86s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:51<04:22,  1.90s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:53<04:17,  1.88s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:55<04:13,  1.86s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:57<04:16,  1.90s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:58<04:09,  1.86s/it]predicting train subjects:  53%|█████▎    | 152/285 [05:00<03:58,  1.79s/it]predicting train subjects:  54%|█████▎    | 153/285 [05:02<03:52,  1.76s/it]predicting train subjects:  54%|█████▍    | 154/285 [05:03<03:50,  1.76s/it]predicting train subjects:  54%|█████▍    | 155/285 [05:05<03:52,  1.79s/it]predicting train subjects:  55%|█████▍    | 156/285 [05:07<03:54,  1.82s/it]predicting train subjects:  55%|█████▌    | 157/285 [05:09<03:56,  1.85s/it]predicting train subjects:  55%|█████▌    | 158/285 [05:11<04:02,  1.91s/it]predicting train subjects:  56%|█████▌    | 159/285 [05:13<03:54,  1.86s/it]predicting train subjects:  56%|█████▌    | 160/285 [05:15<03:52,  1.86s/it]predicting train subjects:  56%|█████▋    | 161/285 [05:17<03:57,  1.92s/it]predicting train subjects:  57%|█████▋    | 162/285 [05:19<04:07,  2.01s/it]predicting train subjects:  57%|█████▋    | 163/285 [05:21<04:03,  2.00s/it]predicting train subjects:  58%|█████▊    | 164/285 [05:23<04:05,  2.03s/it]predicting train subjects:  58%|█████▊    | 165/285 [05:25<04:00,  2.00s/it]predicting train subjects:  58%|█████▊    | 166/285 [05:27<03:59,  2.01s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:29<04:04,  2.07s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:32<04:10,  2.14s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:34<04:13,  2.18s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:36<04:18,  2.25s/it]predicting train subjects:  60%|██████    | 171/285 [05:38<04:05,  2.16s/it]predicting train subjects:  60%|██████    | 172/285 [05:41<04:09,  2.21s/it]predicting train subjects:  61%|██████    | 173/285 [05:43<04:10,  2.24s/it]predicting train subjects:  61%|██████    | 174/285 [05:45<04:06,  2.22s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:47<04:03,  2.21s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:49<03:56,  2.17s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:52<04:01,  2.24s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:54<04:03,  2.27s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:56<03:52,  2.19s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:58<03:44,  2.13s/it]predicting train subjects:  64%|██████▎   | 181/285 [06:00<03:42,  2.14s/it]predicting train subjects:  64%|██████▍   | 182/285 [06:02<03:33,  2.07s/it]predicting train subjects:  64%|██████▍   | 183/285 [06:04<03:26,  2.02s/it]predicting train subjects:  65%|██████▍   | 184/285 [06:06<03:32,  2.11s/it]predicting train subjects:  65%|██████▍   | 185/285 [06:08<03:22,  2.02s/it]predicting train subjects:  65%|██████▌   | 186/285 [06:10<03:20,  2.03s/it]predicting train subjects:  66%|██████▌   | 187/285 [06:12<03:24,  2.09s/it]predicting train subjects:  66%|██████▌   | 188/285 [06:14<03:19,  2.06s/it]predicting train subjects:  66%|██████▋   | 189/285 [06:16<03:12,  2.00s/it]predicting train subjects:  67%|██████▋   | 190/285 [06:19<03:16,  2.07s/it]predicting train subjects:  67%|██████▋   | 191/285 [06:21<03:23,  2.17s/it]predicting train subjects:  67%|██████▋   | 192/285 [06:23<03:19,  2.14s/it]predicting train subjects:  68%|██████▊   | 193/285 [06:25<03:13,  2.10s/it]predicting train subjects:  68%|██████▊   | 194/285 [06:27<03:15,  2.15s/it]predicting train subjects:  68%|██████▊   | 195/285 [06:30<03:18,  2.20s/it]predicting train subjects:  69%|██████▉   | 196/285 [06:32<03:24,  2.30s/it]predicting train subjects:  69%|██████▉   | 197/285 [06:34<03:19,  2.27s/it]predicting train subjects:  69%|██████▉   | 198/285 [06:37<03:31,  2.43s/it]predicting train subjects:  70%|██████▉   | 199/285 [06:39<03:23,  2.37s/it]predicting train subjects:  70%|███████   | 200/285 [06:42<03:24,  2.41s/it]predicting train subjects:  71%|███████   | 201/285 [06:44<03:21,  2.40s/it]predicting train subjects:  71%|███████   | 202/285 [06:46<03:14,  2.35s/it]predicting train subjects:  71%|███████   | 203/285 [06:49<03:13,  2.36s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:51<03:14,  2.41s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:54<03:10,  2.38s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:56<03:15,  2.47s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:59<03:12,  2.47s/it]predicting train subjects:  73%|███████▎  | 208/285 [07:01<03:12,  2.50s/it]predicting train subjects:  73%|███████▎  | 209/285 [07:04<03:09,  2.49s/it]predicting train subjects:  74%|███████▎  | 210/285 [07:06<03:07,  2.50s/it]predicting train subjects:  74%|███████▍  | 211/285 [07:09<03:00,  2.44s/it]predicting train subjects:  74%|███████▍  | 212/285 [07:11<03:01,  2.48s/it]predicting train subjects:  75%|███████▍  | 213/285 [07:14<02:58,  2.48s/it]predicting train subjects:  75%|███████▌  | 214/285 [07:16<02:52,  2.43s/it]predicting train subjects:  75%|███████▌  | 215/285 [07:18<02:43,  2.33s/it]predicting train subjects:  76%|███████▌  | 216/285 [07:20<02:35,  2.26s/it]predicting train subjects:  76%|███████▌  | 217/285 [07:22<02:28,  2.19s/it]predicting train subjects:  76%|███████▋  | 218/285 [07:25<02:35,  2.32s/it]predicting train subjects:  77%|███████▋  | 219/285 [07:27<02:32,  2.30s/it]predicting train subjects:  77%|███████▋  | 220/285 [07:29<02:26,  2.25s/it]predicting train subjects:  78%|███████▊  | 221/285 [07:31<02:18,  2.17s/it]predicting train subjects:  78%|███████▊  | 222/285 [07:34<02:19,  2.21s/it]predicting train subjects:  78%|███████▊  | 223/285 [07:36<02:15,  2.19s/it]predicting train subjects:  79%|███████▊  | 224/285 [07:38<02:10,  2.14s/it]predicting train subjects:  79%|███████▉  | 225/285 [07:40<02:12,  2.20s/it]predicting train subjects:  79%|███████▉  | 226/285 [07:42<02:04,  2.12s/it]predicting train subjects:  80%|███████▉  | 227/285 [07:44<02:01,  2.10s/it]predicting train subjects:  80%|████████  | 228/285 [07:46<02:03,  2.16s/it]predicting train subjects:  80%|████████  | 229/285 [07:48<01:55,  2.06s/it]predicting train subjects:  81%|████████  | 230/285 [07:51<02:00,  2.19s/it]predicting train subjects:  81%|████████  | 231/285 [07:53<01:59,  2.21s/it]predicting train subjects:  81%|████████▏ | 232/285 [07:56<02:03,  2.33s/it]predicting train subjects:  82%|████████▏ | 233/285 [07:58<02:02,  2.35s/it]predicting train subjects:  82%|████████▏ | 234/285 [08:01<02:11,  2.58s/it]predicting train subjects:  82%|████████▏ | 235/285 [08:04<02:08,  2.57s/it]predicting train subjects:  83%|████████▎ | 236/285 [08:07<02:11,  2.67s/it]predicting train subjects:  83%|████████▎ | 237/285 [08:09<02:07,  2.65s/it]predicting train subjects:  84%|████████▎ | 238/285 [08:12<02:06,  2.69s/it]predicting train subjects:  84%|████████▍ | 239/285 [08:15<02:05,  2.72s/it]predicting train subjects:  84%|████████▍ | 240/285 [08:17<02:01,  2.69s/it]predicting train subjects:  85%|████████▍ | 241/285 [08:20<01:52,  2.57s/it]predicting train subjects:  85%|████████▍ | 242/285 [08:22<01:51,  2.59s/it]predicting train subjects:  85%|████████▌ | 243/285 [08:24<01:44,  2.49s/it]predicting train subjects:  86%|████████▌ | 244/285 [08:27<01:42,  2.49s/it]predicting train subjects:  86%|████████▌ | 245/285 [08:30<01:42,  2.57s/it]predicting train subjects:  86%|████████▋ | 246/285 [08:33<01:45,  2.70s/it]predicting train subjects:  87%|████████▋ | 247/285 [08:36<01:49,  2.87s/it]predicting train subjects:  87%|████████▋ | 248/285 [08:39<01:45,  2.84s/it]predicting train subjects:  87%|████████▋ | 249/285 [08:42<01:41,  2.81s/it]predicting train subjects:  88%|████████▊ | 250/285 [08:44<01:32,  2.64s/it]predicting train subjects:  88%|████████▊ | 251/285 [08:46<01:26,  2.55s/it]predicting train subjects:  88%|████████▊ | 252/285 [08:49<01:22,  2.51s/it]predicting train subjects:  89%|████████▉ | 253/285 [08:51<01:17,  2.41s/it]predicting train subjects:  89%|████████▉ | 254/285 [08:53<01:13,  2.38s/it]predicting train subjects:  89%|████████▉ | 255/285 [08:55<01:11,  2.37s/it]predicting train subjects:  90%|████████▉ | 256/285 [08:57<01:05,  2.27s/it]predicting train subjects:  90%|█████████ | 257/285 [08:59<01:01,  2.18s/it]predicting train subjects:  91%|█████████ | 258/285 [09:02<01:02,  2.31s/it]predicting train subjects:  91%|█████████ | 259/285 [09:04<01:00,  2.33s/it]predicting train subjects:  91%|█████████ | 260/285 [09:06<00:56,  2.24s/it]predicting train subjects:  92%|█████████▏| 261/285 [09:09<00:54,  2.29s/it]predicting train subjects:  92%|█████████▏| 262/285 [09:11<00:52,  2.28s/it]predicting train subjects:  92%|█████████▏| 263/285 [09:13<00:48,  2.19s/it]predicting train subjects:  93%|█████████▎| 264/285 [09:15<00:44,  2.12s/it]predicting train subjects:  93%|█████████▎| 265/285 [09:17<00:43,  2.18s/it]predicting train subjects:  93%|█████████▎| 266/285 [09:19<00:40,  2.14s/it]predicting train subjects:  94%|█████████▎| 267/285 [09:22<00:38,  2.16s/it]predicting train subjects:  94%|█████████▍| 268/285 [09:24<00:38,  2.25s/it]predicting train subjects:  94%|█████████▍| 269/285 [09:27<00:37,  2.35s/it]predicting train subjects:  95%|█████████▍| 270/285 [09:29<00:35,  2.36s/it]predicting train subjects:  95%|█████████▌| 271/285 [09:32<00:34,  2.50s/it]predicting train subjects:  95%|█████████▌| 272/285 [09:35<00:34,  2.65s/it]predicting train subjects:  96%|█████████▌| 273/285 [09:38<00:32,  2.73s/it]predicting train subjects:  96%|█████████▌| 274/285 [09:40<00:29,  2.65s/it]predicting train subjects:  96%|█████████▋| 275/285 [09:43<00:26,  2.64s/it]predicting train subjects:  97%|█████████▋| 276/285 [09:46<00:24,  2.75s/it]predicting train subjects:  97%|█████████▋| 277/285 [09:48<00:21,  2.73s/it]predicting train subjects:  98%|█████████▊| 278/285 [09:51<00:18,  2.71s/it]predicting train subjects:  98%|█████████▊| 279/285 [09:54<00:15,  2.66s/it]predicting train subjects:  98%|█████████▊| 280/285 [09:57<00:13,  2.74s/it]predicting train subjects:  99%|█████████▊| 281/285 [10:00<00:11,  2.89s/it]predicting train subjects:  99%|█████████▉| 282/285 [10:03<00:08,  2.84s/it]predicting train subjects:  99%|█████████▉| 283/285 [10:05<00:05,  2.69s/it]predicting train subjects: 100%|█████████▉| 284/285 [10:08<00:02,  2.73s/it]predicting train subjects: 100%|██████████| 285/285 [10:10<00:00,  2.70s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:02<09:58,  2.11s/it]Loading train:   1%|          | 2/285 [00:04<10:31,  2.23s/it]Loading train:   1%|          | 3/285 [00:06<09:59,  2.13s/it]Loading train:   1%|▏         | 4/285 [00:08<10:17,  2.20s/it]Loading train:   2%|▏         | 5/285 [00:11<10:10,  2.18s/it]Loading train:   2%|▏         | 6/285 [00:13<10:27,  2.25s/it]Loading train:   2%|▏         | 7/285 [00:16<11:08,  2.40s/it]Loading train:   3%|▎         | 8/285 [00:18<11:22,  2.46s/it]Loading train:   3%|▎         | 9/285 [00:21<11:15,  2.45s/it]Loading train:   4%|▎         | 10/285 [00:23<10:56,  2.39s/it]Loading train:   4%|▍         | 11/285 [00:25<10:29,  2.30s/it]Loading train:   4%|▍         | 12/285 [00:27<10:07,  2.23s/it]Loading train:   5%|▍         | 13/285 [00:29<09:25,  2.08s/it]Loading train:   5%|▍         | 14/285 [00:30<08:35,  1.90s/it]Loading train:   5%|▌         | 15/285 [00:32<08:41,  1.93s/it]Loading train:   6%|▌         | 16/285 [00:34<08:53,  1.98s/it]Loading train:   6%|▌         | 17/285 [00:37<09:21,  2.09s/it]Loading train:   6%|▋         | 18/285 [00:39<09:07,  2.05s/it]Loading train:   7%|▋         | 19/285 [00:41<09:27,  2.13s/it]Loading train:   7%|▋         | 20/285 [00:43<09:31,  2.16s/it]Loading train:   7%|▋         | 21/285 [00:46<10:02,  2.28s/it]Loading train:   8%|▊         | 22/285 [00:48<09:43,  2.22s/it]Loading train:   8%|▊         | 23/285 [00:50<09:05,  2.08s/it]Loading train:   8%|▊         | 24/285 [00:52<09:32,  2.19s/it]Loading train:   9%|▉         | 25/285 [00:55<10:05,  2.33s/it]Loading train:   9%|▉         | 26/285 [00:57<09:56,  2.30s/it]Loading train:   9%|▉         | 27/285 [00:59<09:48,  2.28s/it]Loading train:  10%|▉         | 28/285 [01:01<09:36,  2.24s/it]Loading train:  10%|█         | 29/285 [01:03<09:00,  2.11s/it]Loading train:  11%|█         | 30/285 [01:05<08:41,  2.04s/it]Loading train:  11%|█         | 31/285 [01:07<08:15,  1.95s/it]Loading train:  11%|█         | 32/285 [01:09<08:39,  2.05s/it]Loading train:  12%|█▏        | 33/285 [01:12<09:11,  2.19s/it]Loading train:  12%|█▏        | 34/285 [01:14<09:15,  2.21s/it]Loading train:  12%|█▏        | 35/285 [01:15<08:27,  2.03s/it]Loading train:  13%|█▎        | 36/285 [01:17<08:03,  1.94s/it]Loading train:  13%|█▎        | 37/285 [01:19<07:45,  1.88s/it]Loading train:  13%|█▎        | 38/285 [01:21<07:43,  1.88s/it]Loading train:  14%|█▎        | 39/285 [01:23<07:35,  1.85s/it]Loading train:  14%|█▍        | 40/285 [01:24<07:12,  1.77s/it]Loading train:  14%|█▍        | 41/285 [01:26<07:26,  1.83s/it]Loading train:  15%|█▍        | 42/285 [01:28<07:48,  1.93s/it]Loading train:  15%|█▌        | 43/285 [01:30<07:56,  1.97s/it]Loading train:  15%|█▌        | 44/285 [01:33<08:20,  2.08s/it]Loading train:  16%|█▌        | 45/285 [01:35<08:42,  2.18s/it]Loading train:  16%|█▌        | 46/285 [01:38<09:14,  2.32s/it]Loading train:  16%|█▋        | 47/285 [01:40<08:49,  2.23s/it]Loading train:  17%|█▋        | 48/285 [01:41<07:51,  1.99s/it]Loading train:  17%|█▋        | 49/285 [01:44<08:10,  2.08s/it]Loading train:  18%|█▊        | 50/285 [01:45<07:40,  1.96s/it]Loading train:  18%|█▊        | 51/285 [01:47<07:09,  1.84s/it]Loading train:  18%|█▊        | 52/285 [01:49<07:19,  1.89s/it]Loading train:  19%|█▊        | 53/285 [01:51<07:26,  1.93s/it]Loading train:  19%|█▉        | 54/285 [01:53<07:32,  1.96s/it]Loading train:  19%|█▉        | 55/285 [01:54<06:32,  1.71s/it]Loading train:  20%|█▉        | 56/285 [01:56<06:55,  1.81s/it]Loading train:  20%|██        | 57/285 [01:58<07:22,  1.94s/it]Loading train:  20%|██        | 58/285 [02:00<06:50,  1.81s/it]Loading train:  21%|██        | 59/285 [02:01<06:23,  1.70s/it]Loading train:  21%|██        | 60/285 [02:03<07:02,  1.88s/it]Loading train:  21%|██▏       | 61/285 [02:06<07:22,  1.98s/it]Loading train:  22%|██▏       | 62/285 [02:08<07:58,  2.15s/it]Loading train:  22%|██▏       | 63/285 [02:10<07:43,  2.09s/it]Loading train:  22%|██▏       | 64/285 [02:13<08:19,  2.26s/it]Loading train:  23%|██▎       | 65/285 [02:15<08:34,  2.34s/it]Loading train:  23%|██▎       | 66/285 [02:18<09:11,  2.52s/it]Loading train:  24%|██▎       | 67/285 [02:20<07:55,  2.18s/it]Loading train:  24%|██▍       | 68/285 [02:22<07:40,  2.12s/it]Loading train:  24%|██▍       | 69/285 [02:24<07:59,  2.22s/it]Loading train:  25%|██▍       | 70/285 [02:26<07:39,  2.14s/it]Loading train:  25%|██▍       | 71/285 [02:28<06:59,  1.96s/it]Loading train:  25%|██▌       | 72/285 [02:29<06:43,  1.89s/it]Loading train:  26%|██▌       | 73/285 [02:31<06:38,  1.88s/it]Loading train:  26%|██▌       | 74/285 [02:32<05:57,  1.69s/it]Loading train:  26%|██▋       | 75/285 [02:34<06:08,  1.75s/it]Loading train:  27%|██▋       | 76/285 [02:36<06:20,  1.82s/it]Loading train:  27%|██▋       | 77/285 [02:38<05:55,  1.71s/it]Loading train:  27%|██▋       | 78/285 [02:40<05:58,  1.73s/it]Loading train:  28%|██▊       | 79/285 [02:42<06:17,  1.83s/it]Loading train:  28%|██▊       | 80/285 [02:44<06:22,  1.87s/it]Loading train:  28%|██▊       | 81/285 [02:46<07:01,  2.07s/it]Loading train:  29%|██▉       | 82/285 [02:48<06:51,  2.03s/it]Loading train:  29%|██▉       | 83/285 [02:50<06:51,  2.04s/it]Loading train:  29%|██▉       | 84/285 [02:52<06:44,  2.01s/it]Loading train:  30%|██▉       | 85/285 [02:54<07:01,  2.11s/it]Loading train:  30%|███       | 86/285 [02:56<06:40,  2.01s/it]Loading train:  31%|███       | 87/285 [02:58<06:35,  1.99s/it]Loading train:  31%|███       | 88/285 [03:01<07:24,  2.26s/it]Loading train:  31%|███       | 89/285 [03:03<07:32,  2.31s/it]Loading train:  32%|███▏      | 90/285 [03:06<07:44,  2.38s/it]Loading train:  32%|███▏      | 91/285 [03:09<07:50,  2.43s/it]Loading train:  32%|███▏      | 92/285 [03:10<07:14,  2.25s/it]Loading train:  33%|███▎      | 93/285 [03:12<06:30,  2.04s/it]Loading train:  33%|███▎      | 94/285 [03:14<06:30,  2.04s/it]Loading train:  33%|███▎      | 95/285 [03:16<06:37,  2.09s/it]Loading train:  34%|███▎      | 96/285 [03:18<06:33,  2.08s/it]Loading train:  34%|███▍      | 97/285 [03:20<06:15,  2.00s/it]Loading train:  34%|███▍      | 98/285 [03:22<06:23,  2.05s/it]Loading train:  35%|███▍      | 99/285 [03:25<06:43,  2.17s/it]Loading train:  35%|███▌      | 100/285 [03:27<06:34,  2.13s/it]Loading train:  35%|███▌      | 101/285 [03:29<06:54,  2.25s/it]Loading train:  36%|███▌      | 102/285 [03:31<06:45,  2.22s/it]Loading train:  36%|███▌      | 103/285 [03:33<06:14,  2.06s/it]Loading train:  36%|███▋      | 104/285 [03:35<06:21,  2.11s/it]Loading train:  37%|███▋      | 105/285 [03:37<06:14,  2.08s/it]Loading train:  37%|███▋      | 106/285 [03:39<06:03,  2.03s/it]Loading train:  38%|███▊      | 107/285 [03:41<05:41,  1.92s/it]Loading train:  38%|███▊      | 108/285 [03:42<05:04,  1.72s/it]Loading train:  38%|███▊      | 109/285 [03:44<05:35,  1.91s/it]Loading train:  39%|███▊      | 110/285 [03:47<06:07,  2.10s/it]Loading train:  39%|███▉      | 111/285 [03:49<06:15,  2.16s/it]Loading train:  39%|███▉      | 112/285 [03:51<05:36,  1.94s/it]Loading train:  40%|███▉      | 113/285 [03:52<05:16,  1.84s/it]Loading train:  40%|████      | 114/285 [03:54<05:17,  1.86s/it]Loading train:  40%|████      | 115/285 [03:56<05:25,  1.91s/it]Loading train:  41%|████      | 116/285 [03:58<05:08,  1.83s/it]Loading train:  41%|████      | 117/285 [04:00<05:07,  1.83s/it]Loading train:  41%|████▏     | 118/285 [04:02<05:23,  1.94s/it]Loading train:  42%|████▏     | 119/285 [04:04<05:21,  1.94s/it]Loading train:  42%|████▏     | 120/285 [04:06<05:38,  2.05s/it]Loading train:  42%|████▏     | 121/285 [04:09<05:49,  2.13s/it]Loading train:  43%|████▎     | 122/285 [04:11<05:55,  2.18s/it]Loading train:  43%|████▎     | 123/285 [04:13<05:41,  2.11s/it]Loading train:  44%|████▎     | 124/285 [04:15<05:34,  2.08s/it]Loading train:  44%|████▍     | 125/285 [04:17<05:44,  2.15s/it]Loading train:  44%|████▍     | 126/285 [04:19<05:52,  2.22s/it]Loading train:  45%|████▍     | 127/285 [04:21<05:29,  2.08s/it]Loading train:  45%|████▍     | 128/285 [04:23<05:03,  1.93s/it]Loading train:  45%|████▌     | 129/285 [04:24<04:48,  1.85s/it]Loading train:  46%|████▌     | 130/285 [04:27<05:03,  1.96s/it]Loading train:  46%|████▌     | 131/285 [04:29<05:14,  2.04s/it]Loading train:  46%|████▋     | 132/285 [04:31<05:14,  2.05s/it]Loading train:  47%|████▋     | 133/285 [04:33<05:24,  2.13s/it]Loading train:  47%|████▋     | 134/285 [04:36<05:32,  2.20s/it]Loading train:  47%|████▋     | 135/285 [04:37<05:04,  2.03s/it]Loading train:  48%|████▊     | 136/285 [04:39<04:54,  1.98s/it]Loading train:  48%|████▊     | 137/285 [04:41<04:27,  1.81s/it]Loading train:  48%|████▊     | 138/285 [04:42<04:28,  1.83s/it]Loading train:  49%|████▉     | 139/285 [04:44<04:36,  1.90s/it]Loading train:  49%|████▉     | 140/285 [04:46<04:05,  1.69s/it]Loading train:  49%|████▉     | 141/285 [04:47<03:57,  1.65s/it]Loading train:  50%|████▉     | 142/285 [04:49<04:08,  1.74s/it]Loading train:  50%|█████     | 143/285 [04:51<04:20,  1.83s/it]Loading train:  51%|█████     | 144/285 [04:53<03:58,  1.69s/it]Loading train:  51%|█████     | 145/285 [04:54<03:40,  1.57s/it]Loading train:  51%|█████     | 146/285 [04:56<03:55,  1.69s/it]Loading train:  52%|█████▏    | 147/285 [04:58<03:57,  1.72s/it]Loading train:  52%|█████▏    | 148/285 [04:59<03:46,  1.65s/it]Loading train:  52%|█████▏    | 149/285 [05:01<03:54,  1.72s/it]Loading train:  53%|█████▎    | 150/285 [05:02<03:38,  1.62s/it]Loading train:  53%|█████▎    | 151/285 [05:04<03:55,  1.76s/it]Loading train:  53%|█████▎    | 152/285 [05:06<04:00,  1.81s/it]Loading train:  54%|█████▎    | 153/285 [05:08<03:48,  1.73s/it]Loading train:  54%|█████▍    | 154/285 [05:10<03:40,  1.68s/it]Loading train:  54%|█████▍    | 155/285 [05:11<03:46,  1.75s/it]Loading train:  55%|█████▍    | 156/285 [05:13<03:43,  1.73s/it]Loading train:  55%|█████▌    | 157/285 [05:15<03:43,  1.75s/it]Loading train:  55%|█████▌    | 158/285 [05:17<03:47,  1.79s/it]Loading train:  56%|█████▌    | 159/285 [05:18<03:41,  1.76s/it]Loading train:  56%|█████▌    | 160/285 [05:20<03:45,  1.80s/it]Loading train:  56%|█████▋    | 161/285 [05:22<03:49,  1.85s/it]Loading train:  57%|█████▋    | 162/285 [05:24<03:41,  1.80s/it]Loading train:  57%|█████▋    | 163/285 [05:26<03:36,  1.77s/it]Loading train:  58%|█████▊    | 164/285 [05:27<03:25,  1.70s/it]Loading train:  58%|█████▊    | 165/285 [05:29<03:19,  1.66s/it]Loading train:  58%|█████▊    | 166/285 [05:31<03:18,  1.67s/it]Loading train:  59%|█████▊    | 167/285 [05:32<03:16,  1.66s/it]Loading train:  59%|█████▉    | 168/285 [05:34<03:11,  1.64s/it]Loading train:  59%|█████▉    | 169/285 [05:36<03:31,  1.82s/it]Loading train:  60%|█████▉    | 170/285 [05:38<03:34,  1.87s/it]Loading train:  60%|██████    | 171/285 [05:40<03:22,  1.78s/it]Loading train:  60%|██████    | 172/285 [05:42<03:28,  1.84s/it]Loading train:  61%|██████    | 173/285 [05:43<03:20,  1.79s/it]Loading train:  61%|██████    | 174/285 [05:45<03:11,  1.73s/it]Loading train:  61%|██████▏   | 175/285 [05:46<02:59,  1.63s/it]Loading train:  62%|██████▏   | 176/285 [05:48<03:11,  1.76s/it]Loading train:  62%|██████▏   | 177/285 [05:50<03:10,  1.76s/it]Loading train:  62%|██████▏   | 178/285 [05:52<03:08,  1.76s/it]Loading train:  63%|██████▎   | 179/285 [05:54<03:18,  1.88s/it]Loading train:  63%|██████▎   | 180/285 [05:56<03:09,  1.81s/it]Loading train:  64%|██████▎   | 181/285 [05:57<03:09,  1.82s/it]Loading train:  64%|██████▍   | 182/285 [05:59<02:54,  1.69s/it]Loading train:  64%|██████▍   | 183/285 [06:01<02:54,  1.71s/it]Loading train:  65%|██████▍   | 184/285 [06:03<03:00,  1.78s/it]Loading train:  65%|██████▍   | 185/285 [06:04<02:50,  1.70s/it]Loading train:  65%|██████▌   | 186/285 [06:05<02:29,  1.51s/it]Loading train:  66%|██████▌   | 187/285 [06:07<02:42,  1.66s/it]Loading train:  66%|██████▌   | 188/285 [06:09<02:35,  1.61s/it]Loading train:  66%|██████▋   | 189/285 [06:10<02:25,  1.51s/it]Loading train:  67%|██████▋   | 190/285 [06:11<02:20,  1.47s/it]Loading train:  67%|██████▋   | 191/285 [06:13<02:26,  1.56s/it]Loading train:  67%|██████▋   | 192/285 [06:15<02:35,  1.68s/it]Loading train:  68%|██████▊   | 193/285 [06:17<02:31,  1.64s/it]Loading train:  68%|██████▊   | 194/285 [06:18<02:29,  1.64s/it]Loading train:  68%|██████▊   | 195/285 [06:20<02:37,  1.75s/it]Loading train:  69%|██████▉   | 196/285 [06:22<02:38,  1.78s/it]Loading train:  69%|██████▉   | 197/285 [06:24<02:45,  1.88s/it]Loading train:  69%|██████▉   | 198/285 [06:26<02:41,  1.86s/it]Loading train:  70%|██████▉   | 199/285 [06:28<02:42,  1.89s/it]Loading train:  70%|███████   | 200/285 [06:30<02:47,  1.96s/it]Loading train:  71%|███████   | 201/285 [06:32<02:31,  1.81s/it]Loading train:  71%|███████   | 202/285 [06:34<02:37,  1.90s/it]Loading train:  71%|███████   | 203/285 [06:35<02:32,  1.86s/it]Loading train:  72%|███████▏  | 204/285 [06:37<02:23,  1.77s/it]Loading train:  72%|███████▏  | 205/285 [06:39<02:26,  1.83s/it]Loading train:  72%|███████▏  | 206/285 [06:41<02:21,  1.79s/it]Loading train:  73%|███████▎  | 207/285 [06:42<02:19,  1.78s/it]Loading train:  73%|███████▎  | 208/285 [06:45<02:26,  1.90s/it]Loading train:  73%|███████▎  | 209/285 [06:47<02:28,  1.95s/it]Loading train:  74%|███████▎  | 210/285 [06:48<02:07,  1.70s/it]Loading train:  74%|███████▍  | 211/285 [06:49<01:58,  1.61s/it]Loading train:  74%|███████▍  | 212/285 [06:50<01:50,  1.52s/it]Loading train:  75%|███████▍  | 213/285 [06:52<01:58,  1.65s/it]Loading train:  75%|███████▌  | 214/285 [06:54<01:54,  1.61s/it]Loading train:  75%|███████▌  | 215/285 [06:55<01:49,  1.56s/it]Loading train:  76%|███████▌  | 216/285 [06:56<01:38,  1.42s/it]Loading train:  76%|███████▌  | 217/285 [06:58<01:46,  1.57s/it]Loading train:  76%|███████▋  | 218/285 [07:00<01:45,  1.57s/it]Loading train:  77%|███████▋  | 219/285 [07:01<01:39,  1.51s/it]Loading train:  77%|███████▋  | 220/285 [07:03<01:36,  1.49s/it]Loading train:  78%|███████▊  | 221/285 [07:04<01:38,  1.54s/it]Loading train:  78%|███████▊  | 222/285 [07:06<01:44,  1.66s/it]Loading train:  78%|███████▊  | 223/285 [07:08<01:37,  1.57s/it]Loading train:  79%|███████▊  | 224/285 [07:09<01:29,  1.47s/it]Loading train:  79%|███████▉  | 225/285 [07:11<01:30,  1.51s/it]Loading train:  79%|███████▉  | 226/285 [07:12<01:35,  1.62s/it]Loading train:  80%|███████▉  | 227/285 [07:14<01:41,  1.74s/it]Loading train:  80%|████████  | 228/285 [07:16<01:41,  1.77s/it]Loading train:  80%|████████  | 229/285 [07:18<01:42,  1.83s/it]Loading train:  81%|████████  | 230/285 [07:19<01:30,  1.65s/it]Loading train:  81%|████████  | 231/285 [07:21<01:20,  1.49s/it]Loading train:  81%|████████▏ | 232/285 [07:22<01:23,  1.58s/it]Loading train:  82%|████████▏ | 233/285 [07:25<01:32,  1.77s/it]Loading train:  82%|████████▏ | 234/285 [07:26<01:23,  1.63s/it]Loading train:  82%|████████▏ | 235/285 [07:28<01:27,  1.75s/it]Loading train:  83%|████████▎ | 236/285 [07:29<01:22,  1.68s/it]Loading train:  83%|████████▎ | 237/285 [07:31<01:20,  1.67s/it]Loading train:  84%|████████▎ | 238/285 [07:33<01:16,  1.62s/it]Loading train:  84%|████████▍ | 239/285 [07:35<01:26,  1.89s/it]Loading train:  84%|████████▍ | 240/285 [07:37<01:28,  1.96s/it]Loading train:  85%|████████▍ | 241/285 [07:40<01:32,  2.11s/it]Loading train:  85%|████████▍ | 242/285 [07:41<01:25,  1.99s/it]Loading train:  85%|████████▌ | 243/285 [07:43<01:20,  1.92s/it]Loading train:  86%|████████▌ | 244/285 [07:45<01:22,  2.01s/it]Loading train:  86%|████████▌ | 245/285 [07:47<01:20,  2.02s/it]Loading train:  86%|████████▋ | 246/285 [07:50<01:21,  2.08s/it]Loading train:  87%|████████▋ | 247/285 [07:52<01:18,  2.07s/it]Loading train:  87%|████████▋ | 248/285 [07:53<01:06,  1.80s/it]Loading train:  87%|████████▋ | 249/285 [07:54<00:56,  1.58s/it]Loading train:  88%|████████▊ | 250/285 [07:55<00:49,  1.41s/it]Loading train:  88%|████████▊ | 251/285 [07:57<00:49,  1.45s/it]Loading train:  88%|████████▊ | 252/285 [07:58<00:49,  1.51s/it]Loading train:  89%|████████▉ | 253/285 [08:00<00:52,  1.64s/it]Loading train:  89%|████████▉ | 254/285 [08:01<00:47,  1.54s/it]Loading train:  89%|████████▉ | 255/285 [08:03<00:46,  1.53s/it]Loading train:  90%|████████▉ | 256/285 [08:05<00:45,  1.58s/it]Loading train:  90%|█████████ | 257/285 [08:06<00:45,  1.63s/it]Loading train:  91%|█████████ | 258/285 [08:08<00:43,  1.60s/it]Loading train:  91%|█████████ | 259/285 [08:09<00:37,  1.46s/it]Loading train:  91%|█████████ | 260/285 [08:11<00:41,  1.66s/it]Loading train:  92%|█████████▏| 261/285 [08:12<00:36,  1.53s/it]Loading train:  92%|█████████▏| 262/285 [08:13<00:31,  1.35s/it]Loading train:  92%|█████████▏| 263/285 [08:14<00:26,  1.22s/it]Loading train:  93%|█████████▎| 264/285 [08:15<00:25,  1.20s/it]Loading train:  93%|█████████▎| 265/285 [08:16<00:23,  1.17s/it]Loading train:  93%|█████████▎| 266/285 [08:18<00:21,  1.13s/it]Loading train:  94%|█████████▎| 267/285 [08:19<00:23,  1.30s/it]Loading train:  94%|█████████▍| 268/285 [08:21<00:24,  1.47s/it]Loading train:  94%|█████████▍| 269/285 [08:23<00:24,  1.51s/it]Loading train:  95%|█████████▍| 270/285 [08:24<00:21,  1.44s/it]Loading train:  95%|█████████▌| 271/285 [08:25<00:19,  1.37s/it]Loading train:  95%|█████████▌| 272/285 [08:26<00:16,  1.28s/it]Loading train:  96%|█████████▌| 273/285 [08:28<00:15,  1.30s/it]Loading train:  96%|█████████▌| 274/285 [08:29<00:14,  1.35s/it]Loading train:  96%|█████████▋| 275/285 [08:30<00:13,  1.36s/it]Loading train:  97%|█████████▋| 276/285 [08:32<00:13,  1.49s/it]Loading train:  97%|█████████▋| 277/285 [08:34<00:12,  1.52s/it]Loading train:  98%|█████████▊| 278/285 [08:35<00:09,  1.41s/it]Loading train:  98%|█████████▊| 279/285 [08:36<00:07,  1.33s/it]Loading train:  98%|█████████▊| 280/285 [08:37<00:06,  1.28s/it]Loading train:  99%|█████████▊| 281/285 [08:38<00:05,  1.25s/it]Loading train:  99%|█████████▉| 282/285 [08:40<00:03,  1.25s/it]Loading train:  99%|█████████▉| 283/285 [08:41<00:02,  1.31s/it]Loading train: 100%|█████████▉| 284/285 [08:43<00:01,  1.35s/it]Loading train: 100%|██████████| 285/285 [08:44<00:00,  1.39s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 3/285 [00:00<00:11, 24.25it/s]concatenating: train:   2%|▏         | 5/285 [00:00<00:13, 21.34it/s]concatenating: train:   3%|▎         | 8/285 [00:00<00:12, 21.80it/s]concatenating: train:   4%|▎         | 10/285 [00:00<00:14, 18.76it/s]concatenating: train:   5%|▍         | 13/285 [00:00<00:15, 17.09it/s]concatenating: train:   5%|▌         | 15/285 [00:00<00:18, 14.96it/s]concatenating: train:   6%|▌         | 17/285 [00:01<00:17, 15.75it/s]concatenating: train:   7%|▋         | 19/285 [00:01<00:16, 15.84it/s]concatenating: train:  10%|█         | 29/285 [00:01<00:12, 21.03it/s]concatenating: train:  16%|█▌        | 45/285 [00:01<00:08, 28.39it/s]concatenating: train:  25%|██▍       | 70/285 [00:01<00:05, 38.64it/s]concatenating: train:  35%|███▌      | 101/285 [00:01<00:03, 52.30it/s]concatenating: train:  44%|████▍     | 125/285 [00:01<00:02, 68.17it/s]concatenating: train:  51%|█████     | 145/285 [00:02<00:03, 42.73it/s]concatenating: train:  56%|█████▌    | 160/285 [00:03<00:03, 34.36it/s]concatenating: train:  60%|██████    | 171/285 [00:03<00:03, 36.84it/s]concatenating: train:  65%|██████▍   | 184/285 [00:03<00:02, 46.88it/s]concatenating: train:  76%|███████▌  | 216/285 [00:03<00:01, 62.95it/s]concatenating: train:  85%|████████▌ | 243/285 [00:03<00:00, 81.40it/s]concatenating: train:  93%|█████████▎| 266/285 [00:03<00:00, 100.40it/s]concatenating: train: 100%|██████████| 285/285 [00:04<00:00, 66.84it/s] 
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.61s/it]Loading test:  67%|██████▋   | 2/3 [00:03<00:01,  1.58s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 34.98it/s]2019-07-06 23:43:53.902515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 23:43:53.902619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 23:43:53.902634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 23:43:53.902643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 23:43:53.903053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:12,  3.15it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:09,  3.93it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.22it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:06,  5.31it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:07,  4.42it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.15it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  5.05it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:03,  6.17it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  4.84it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.74it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  5.11it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:02,  6.09it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.04it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  5.51it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:02,  5.93it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  4.97it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  5.85it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  6.79it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.49it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  5.77it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  4.84it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  7.17it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 80, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 80, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 80, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 80, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 80, 52, 30)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 80, 52, 30)   8130        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 80, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 80, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 80, 52, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 80, 52, 30)   8130        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 80, 52, 30)   120         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 80, 52, 30)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 80, 52, 30)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 80, 52, 10)   2710        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 80, 52, 10)   40          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 80, 52, 10)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 80, 52, 10)   910         activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 80, 52, 10)   40          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 80, 52, 10)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 40, 26, 10)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 40, 26, 10)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 40, 26, 20)   1820        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 40, 26, 20)   80          conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 40, 26, 20)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 40, 26, 20)   3620        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 40, 26, 20)   80          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 40, 26, 20)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 20, 13, 20)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 20, 13, 20)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 20, 13, 40)   7240        dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 20, 13, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 20, 13, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 13, 40)   14440       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 20, 13, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 20, 13, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 20, 13, 40)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 40, 26, 20)   3220        dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 40, 26, 40)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 40, 26, 20)   7220        concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 40, 26, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 40, 26, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 40, 26, 20)   3620        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 40, 26, 20)   80          conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 40, 26, 20)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 40, 26, 20)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 80, 52, 10)   810         dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 80, 52, 20)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 80, 52, 10)   1810        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 80, 52, 10)   40          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 80, 52, 10)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 80, 52, 10)   910         activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 80, 52, 10)   40          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 80, 52, 10)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 80, 52, 10)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 80, 52, 13)   143         dropout_8[0][0]                  
==================================================================================================
Total params: 66,193
Trainable params: 29,873
Non-trainable params: 36,320
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.48913484e-02 3.19509754e-02 7.49897764e-02 9.32025064e-03
 2.70904632e-02 7.06417031e-03 8.46180096e-02 1.12618024e-01
 8.60108482e-02 1.32459736e-02 2.94100802e-01 1.93843398e-01
 2.55960049e-04]
Train on 10843 samples, validate on 104 samples
Epoch 1/300
 - 19s - loss: 315.9716 - acc: 0.0306 - mDice: 0.0158 - val_loss: 121.6527 - val_acc: 0.0164 - val_mDice: 0.0104

Epoch 00001: val_mDice improved from -inf to 0.01041, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 10s - loss: 108.3789 - acc: 0.4429 - mDice: 0.0146 - val_loss: 35.4358 - val_acc: 0.9021 - val_mDice: 0.0107

Epoch 00002: val_mDice improved from 0.01041 to 0.01070, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 11s - loss: 57.0495 - acc: 0.7817 - mDice: 0.0136 - val_loss: 24.0227 - val_acc: 0.9034 - val_mDice: 0.0095

Epoch 00003: val_mDice did not improve from 0.01070
Epoch 4/300
 - 10s - loss: 38.2459 - acc: 0.8495 - mDice: 0.0127 - val_loss: 17.2217 - val_acc: 0.9034 - val_mDice: 0.0090

Epoch 00004: val_mDice did not improve from 0.01070
Epoch 5/300
 - 10s - loss: 29.2272 - acc: 0.8638 - mDice: 0.0122 - val_loss: 14.3220 - val_acc: 0.9034 - val_mDice: 0.0083

Epoch 00005: val_mDice did not improve from 0.01070
Epoch 6/300
 - 10s - loss: 24.3302 - acc: 0.8693 - mDice: 0.0117 - val_loss: 17.2740 - val_acc: 0.9034 - val_mDice: 0.0079

Epoch 00006: val_mDice did not improve from 0.01070
Epoch 7/300
 - 11s - loss: 21.2187 - acc: 0.8716 - mDice: 0.0118 - val_loss: 12.9098 - val_acc: 0.9034 - val_mDice: 0.0077

Epoch 00007: val_mDice did not improve from 0.01070
Epoch 8/300
 - 10s - loss: 19.2294 - acc: 0.8729 - mDice: 0.0115 - val_loss: 12.8612 - val_acc: 0.9034 - val_mDice: 0.0076

Epoch 00008: val_mDice did not improve from 0.01070
Epoch 9/300
 - 10s - loss: 17.6165 - acc: 0.8735 - mDice: 0.0140 - val_loss: 11.5391 - val_acc: 0.9034 - val_mDice: 0.0046

Epoch 00009: val_mDice did not improve from 0.01070
Epoch 10/300
 - 10s - loss: 16.0409 - acc: 0.8740 - mDice: 0.0185 - val_loss: 11.9570 - val_acc: 0.9034 - val_mDice: 0.0030

Epoch 00010: val_mDice did not improve from 0.01070
Epoch 11/300
 - 10s - loss: 14.6919 - acc: 0.8745 - mDice: 0.0214 - val_loss: 11.6590 - val_acc: 0.9034 - val_mDice: 0.0037

Epoch 00011: val_mDice did not improve from 0.01070
Epoch 12/300
 - 9s - loss: 13.8014 - acc: 0.8747 - mDice: 0.0228 - val_loss: 10.9088 - val_acc: 0.9034 - val_mDice: 0.0057

Epoch 00012: val_mDice did not improve from 0.01070
Epoch 13/300
 - 10s - loss: 13.0750 - acc: 0.8747 - mDice: 0.0241 - val_loss: 9.2196 - val_acc: 0.9034 - val_mDice: 0.0136

Epoch 00013: val_mDice improved from 0.01070 to 0.01364, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 9s - loss: 12.4232 - acc: 0.8748 - mDice: 0.0258 - val_loss: 8.8635 - val_acc: 0.9034 - val_mDice: 0.0180

Epoch 00014: val_mDice improved from 0.01364 to 0.01797, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 9s - loss: 11.8447 - acc: 0.8748 - mDice: 0.0279 - val_loss: 8.4596 - val_acc: 0.9034 - val_mDice: 0.0273

Epoch 00015: val_mDice improved from 0.01797 to 0.02730, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 9s - loss: 11.3491 - acc: 0.8748 - mDice: 0.0303 - val_loss: 8.4079 - val_acc: 0.9034 - val_mDice: 0.0305

Epoch 00016: val_mDice improved from 0.02730 to 0.03049, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 10s - loss: 10.9491 - acc: 0.8748 - mDice: 0.0324 - val_loss: 8.2781 - val_acc: 0.9034 - val_mDice: 0.0343

Epoch 00017: val_mDice improved from 0.03049 to 0.03429, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 10s - loss: 10.6324 - acc: 0.8748 - mDice: 0.0342 - val_loss: 8.0470 - val_acc: 0.9034 - val_mDice: 0.0398

Epoch 00018: val_mDice improved from 0.03429 to 0.03984, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 9s - loss: 10.3493 - acc: 0.8748 - mDice: 0.0357 - val_loss: 8.3500 - val_acc: 0.9034 - val_mDice: 0.0503

Epoch 00019: val_mDice improved from 0.03984 to 0.05026, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 9s - loss: 10.0958 - acc: 0.8748 - mDice: 0.0374 - val_loss: 7.9728 - val_acc: 0.9034 - val_mDice: 0.0471

Epoch 00020: val_mDice did not improve from 0.05026
Epoch 21/300
 - 9s - loss: 9.8906 - acc: 0.8748 - mDice: 0.0388 - val_loss: 7.9190 - val_acc: 0.9034 - val_mDice: 0.0467

Epoch 00021: val_mDice did not improve from 0.05026
Epoch 22/300
 - 9s - loss: 9.6988 - acc: 0.8748 - mDice: 0.0403 - val_loss: 7.8230 - val_acc: 0.9034 - val_mDice: 0.0506

Epoch 00022: val_mDice improved from 0.05026 to 0.05062, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 9s - loss: 9.5324 - acc: 0.8748 - mDice: 0.0420 - val_loss: 8.0210 - val_acc: 0.9034 - val_mDice: 0.0472

Epoch 00023: val_mDice did not improve from 0.05062
Epoch 24/300
 - 9s - loss: 9.3860 - acc: 0.8748 - mDice: 0.0437 - val_loss: 7.9845 - val_acc: 0.9034 - val_mDice: 0.0502

Epoch 00024: val_mDice did not improve from 0.05062
Epoch 25/300
 - 10s - loss: 9.2452 - acc: 0.8748 - mDice: 0.0448 - val_loss: 7.7898 - val_acc: 0.9034 - val_mDice: 0.0588

Epoch 00025: val_mDice improved from 0.05062 to 0.05878, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 9s - loss: 9.1268 - acc: 0.8748 - mDice: 0.0462 - val_loss: 7.7560 - val_acc: 0.9034 - val_mDice: 0.0610

Epoch 00026: val_mDice improved from 0.05878 to 0.06096, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 9s - loss: 9.0031 - acc: 0.8748 - mDice: 0.0474 - val_loss: 7.8434 - val_acc: 0.9034 - val_mDice: 0.0588

Epoch 00027: val_mDice did not improve from 0.06096
Epoch 28/300
 - 9s - loss: 8.8829 - acc: 0.8748 - mDice: 0.0486 - val_loss: 7.8932 - val_acc: 0.9034 - val_mDice: 0.0563

Epoch 00028: val_mDice did not improve from 0.06096
Epoch 29/300
 - 9s - loss: 8.8022 - acc: 0.8748 - mDice: 0.0495 - val_loss: 7.7662 - val_acc: 0.9034 - val_mDice: 0.0571

Epoch 00029: val_mDice did not improve from 0.06096
Epoch 30/300
 - 9s - loss: 8.7071 - acc: 0.8748 - mDice: 0.0506 - val_loss: 7.6288 - val_acc: 0.9034 - val_mDice: 0.0632

Epoch 00030: val_mDice improved from 0.06096 to 0.06317, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 9s - loss: 8.6046 - acc: 0.8748 - mDice: 0.0521 - val_loss: 7.6724 - val_acc: 0.9034 - val_mDice: 0.0642

Epoch 00031: val_mDice improved from 0.06317 to 0.06418, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 9s - loss: 8.5251 - acc: 0.8748 - mDice: 0.0533 - val_loss: 7.6591 - val_acc: 0.9034 - val_mDice: 0.0611

Epoch 00032: val_mDice did not improve from 0.06418
Epoch 33/300
 - 9s - loss: 8.4303 - acc: 0.8748 - mDice: 0.0547 - val_loss: 7.6007 - val_acc: 0.9034 - val_mDice: 0.0645

Epoch 00033: val_mDice improved from 0.06418 to 0.06453, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 10s - loss: 8.3676 - acc: 0.8748 - mDice: 0.0558 - val_loss: 8.1311 - val_acc: 0.9034 - val_mDice: 0.0664

Epoch 00034: val_mDice improved from 0.06453 to 0.06641, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 9s - loss: 8.2822 - acc: 0.8748 - mDice: 0.0574 - val_loss: 7.6674 - val_acc: 0.9034 - val_mDice: 0.0713

Epoch 00035: val_mDice improved from 0.06641 to 0.07131, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 9s - loss: 8.1976 - acc: 0.8748 - mDice: 0.0588 - val_loss: 7.4888 - val_acc: 0.9034 - val_mDice: 0.0685

Epoch 00036: val_mDice did not improve from 0.07131
Epoch 37/300
 - 9s - loss: 8.1181 - acc: 0.8748 - mDice: 0.0604 - val_loss: 7.7609 - val_acc: 0.9034 - val_mDice: 0.0643

Epoch 00037: val_mDice did not improve from 0.07131
Epoch 38/300
 - 9s - loss: 8.0661 - acc: 0.8748 - mDice: 0.0618 - val_loss: 7.6889 - val_acc: 0.9034 - val_mDice: 0.0628

Epoch 00038: val_mDice did not improve from 0.07131
Epoch 39/300
 - 9s - loss: 7.9739 - acc: 0.8748 - mDice: 0.0638 - val_loss: 7.3426 - val_acc: 0.9034 - val_mDice: 0.0752

Epoch 00039: val_mDice improved from 0.07131 to 0.07521, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 9s - loss: 7.8944 - acc: 0.8748 - mDice: 0.0658 - val_loss: 7.3198 - val_acc: 0.9034 - val_mDice: 0.0781

Epoch 00040: val_mDice improved from 0.07521 to 0.07808, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 9s - loss: 7.8095 - acc: 0.8748 - mDice: 0.0679 - val_loss: 7.4037 - val_acc: 0.9034 - val_mDice: 0.0744

Epoch 00041: val_mDice did not improve from 0.07808
Epoch 42/300
 - 10s - loss: 7.7366 - acc: 0.8748 - mDice: 0.0701 - val_loss: 7.6294 - val_acc: 0.9034 - val_mDice: 0.0758

Epoch 00042: val_mDice did not improve from 0.07808
Epoch 43/300
 - 9s - loss: 7.6839 - acc: 0.8747 - mDice: 0.0732 - val_loss: 7.5431 - val_acc: 0.9034 - val_mDice: 0.0784

Epoch 00043: val_mDice improved from 0.07808 to 0.07835, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 44/300
 - 9s - loss: 7.5789 - acc: 0.8747 - mDice: 0.0778 - val_loss: 7.2104 - val_acc: 0.9034 - val_mDice: 0.0838

Epoch 00044: val_mDice improved from 0.07835 to 0.08377, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 9s - loss: 7.4995 - acc: 0.8747 - mDice: 0.0821 - val_loss: 7.1130 - val_acc: 0.9034 - val_mDice: 0.0892

Epoch 00045: val_mDice improved from 0.08377 to 0.08921, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 9s - loss: 7.4325 - acc: 0.8747 - mDice: 0.0849 - val_loss: 7.3126 - val_acc: 0.9034 - val_mDice: 0.0828

Epoch 00046: val_mDice did not improve from 0.08921
Epoch 47/300
 - 9s - loss: 7.3744 - acc: 0.8747 - mDice: 0.0868 - val_loss: 7.2122 - val_acc: 0.9034 - val_mDice: 0.0914

Epoch 00047: val_mDice improved from 0.08921 to 0.09137, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 9s - loss: 7.3184 - acc: 0.8747 - mDice: 0.0893 - val_loss: 7.4895 - val_acc: 0.9034 - val_mDice: 0.0863

Epoch 00048: val_mDice did not improve from 0.09137
Epoch 49/300
 - 9s - loss: 7.2625 - acc: 0.8746 - mDice: 0.0909 - val_loss: 7.9858 - val_acc: 0.9034 - val_mDice: 0.0812

Epoch 00049: val_mDice did not improve from 0.09137
Epoch 50/300
 - 9s - loss: 7.2365 - acc: 0.8746 - mDice: 0.0927 - val_loss: 7.3947 - val_acc: 0.9034 - val_mDice: 0.0912

Epoch 00050: val_mDice did not improve from 0.09137
Epoch 51/300
 - 9s - loss: 7.1665 - acc: 0.8745 - mDice: 0.0946 - val_loss: 7.5995 - val_acc: 0.9033 - val_mDice: 0.0873

Epoch 00051: val_mDice did not improve from 0.09137
Epoch 52/300
 - 9s - loss: 7.1139 - acc: 0.8744 - mDice: 0.0965 - val_loss: 7.4756 - val_acc: 0.9034 - val_mDice: 0.0915

Epoch 00052: val_mDice improved from 0.09137 to 0.09154, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 10s - loss: 7.0533 - acc: 0.8744 - mDice: 0.0983 - val_loss: 7.3337 - val_acc: 0.9033 - val_mDice: 0.0950

Epoch 00053: val_mDice improved from 0.09154 to 0.09505, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 9s - loss: 7.0131 - acc: 0.8743 - mDice: 0.0995 - val_loss: 7.2716 - val_acc: 0.9033 - val_mDice: 0.1014

Epoch 00054: val_mDice improved from 0.09505 to 0.10137, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 9s - loss: 7.0011 - acc: 0.8742 - mDice: 0.1006 - val_loss: 7.3722 - val_acc: 0.9031 - val_mDice: 0.1030

Epoch 00055: val_mDice improved from 0.10137 to 0.10304, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 9s - loss: 6.9219 - acc: 0.8740 - mDice: 0.1033 - val_loss: 7.4539 - val_acc: 0.9033 - val_mDice: 0.0953

Epoch 00056: val_mDice did not improve from 0.10304
Epoch 57/300
 - 9s - loss: 6.8869 - acc: 0.8739 - mDice: 0.1060 - val_loss: 7.2400 - val_acc: 0.9030 - val_mDice: 0.1032

Epoch 00057: val_mDice improved from 0.10304 to 0.10320, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 58/300
 - 9s - loss: 6.8410 - acc: 0.8737 - mDice: 0.1073 - val_loss: 7.1512 - val_acc: 0.9028 - val_mDice: 0.1080

Epoch 00058: val_mDice improved from 0.10320 to 0.10800, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 9s - loss: 6.7956 - acc: 0.8737 - mDice: 0.1089 - val_loss: 7.5078 - val_acc: 0.9033 - val_mDice: 0.1010

Epoch 00059: val_mDice did not improve from 0.10800
Epoch 60/300
 - 9s - loss: 6.7620 - acc: 0.8735 - mDice: 0.1109 - val_loss: 7.1056 - val_acc: 0.9024 - val_mDice: 0.1136

Epoch 00060: val_mDice improved from 0.10800 to 0.11356, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 61/300
 - 9s - loss: 6.7238 - acc: 0.8734 - mDice: 0.1124 - val_loss: 7.1000 - val_acc: 0.9028 - val_mDice: 0.1109

Epoch 00061: val_mDice did not improve from 0.11356
Epoch 62/300
 - 9s - loss: 6.6864 - acc: 0.8734 - mDice: 0.1135 - val_loss: 7.1772 - val_acc: 0.9028 - val_mDice: 0.1060

Epoch 00062: val_mDice did not improve from 0.11356
Epoch 63/300
 - 10s - loss: 6.6840 - acc: 0.8732 - mDice: 0.1140 - val_loss: 7.4554 - val_acc: 0.9032 - val_mDice: 0.1024

Epoch 00063: val_mDice did not improve from 0.11356
Epoch 64/300
 - 9s - loss: 6.6557 - acc: 0.8731 - mDice: 0.1152 - val_loss: 7.1069 - val_acc: 0.9026 - val_mDice: 0.1101

Epoch 00064: val_mDice did not improve from 0.11356
Epoch 65/300
 - 9s - loss: 6.6014 - acc: 0.8730 - mDice: 0.1168 - val_loss: 7.2135 - val_acc: 0.9026 - val_mDice: 0.1085

Epoch 00065: val_mDice did not improve from 0.11356
Epoch 66/300
 - 9s - loss: 6.5859 - acc: 0.8729 - mDice: 0.1176 - val_loss: 7.3250 - val_acc: 0.9010 - val_mDice: 0.1151

Epoch 00066: val_mDice improved from 0.11356 to 0.11515, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 67/300
 - 9s - loss: 6.5399 - acc: 0.8728 - mDice: 0.1191 - val_loss: 7.0789 - val_acc: 0.9017 - val_mDice: 0.1120

Epoch 00067: val_mDice did not improve from 0.11515
Epoch 68/300
 - 9s - loss: 6.5266 - acc: 0.8727 - mDice: 0.1200 - val_loss: 7.2140 - val_acc: 0.9027 - val_mDice: 0.1085

Epoch 00068: val_mDice did not improve from 0.11515
Epoch 69/300
 - 9s - loss: 6.4947 - acc: 0.8727 - mDice: 0.1210 - val_loss: 7.6030 - val_acc: 0.9031 - val_mDice: 0.1009

Epoch 00069: val_mDice did not improve from 0.11515
Epoch 70/300
 - 9s - loss: 6.4579 - acc: 0.8726 - mDice: 0.1225 - val_loss: 7.3322 - val_acc: 0.9023 - val_mDice: 0.1113

Epoch 00070: val_mDice did not improve from 0.11515
Epoch 71/300
 - 9s - loss: 6.4400 - acc: 0.8724 - mDice: 0.1229 - val_loss: 7.3093 - val_acc: 0.9020 - val_mDice: 0.1123

Epoch 00071: val_mDice did not improve from 0.11515
Epoch 72/300
 - 9s - loss: 6.4008 - acc: 0.8723 - mDice: 0.1241 - val_loss: 6.9523 - val_acc: 0.9003 - val_mDice: 0.1208

Epoch 00072: val_mDice improved from 0.11515 to 0.12080, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 73/300
 - 9s - loss: 6.3710 - acc: 0.8722 - mDice: 0.1239 - val_loss: 7.1839 - val_acc: 0.9006 - val_mDice: 0.1162

Epoch 00073: val_mDice did not improve from 0.12080
Epoch 74/300
 - 9s - loss: 6.3033 - acc: 0.8721 - mDice: 0.1253 - val_loss: 6.9938 - val_acc: 0.8934 - val_mDice: 0.1255

Epoch 00074: val_mDice improved from 0.12080 to 0.12548, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 75/300
 - 10s - loss: 6.2396 - acc: 0.8721 - mDice: 0.1275 - val_loss: 6.9854 - val_acc: 0.8937 - val_mDice: 0.1256

Epoch 00075: val_mDice improved from 0.12548 to 0.12563, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 76/300
 - 9s - loss: 6.1751 - acc: 0.8721 - mDice: 0.1305 - val_loss: 6.9362 - val_acc: 0.8950 - val_mDice: 0.1291

Epoch 00076: val_mDice improved from 0.12563 to 0.12914, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 77/300
 - 9s - loss: 6.1393 - acc: 0.8720 - mDice: 0.1323 - val_loss: 7.1000 - val_acc: 0.8999 - val_mDice: 0.1227

Epoch 00077: val_mDice did not improve from 0.12914
Epoch 78/300
 - 9s - loss: 6.1079 - acc: 0.8720 - mDice: 0.1331 - val_loss: 6.7994 - val_acc: 0.8999 - val_mDice: 0.1288

Epoch 00078: val_mDice did not improve from 0.12914
Epoch 79/300
 - 9s - loss: 6.0752 - acc: 0.8720 - mDice: 0.1347 - val_loss: 6.9205 - val_acc: 0.9015 - val_mDice: 0.1228

Epoch 00079: val_mDice did not improve from 0.12914
Epoch 80/300
 - 9s - loss: 6.0418 - acc: 0.8719 - mDice: 0.1362 - val_loss: 6.9712 - val_acc: 0.8994 - val_mDice: 0.1255

Epoch 00080: val_mDice did not improve from 0.12914
Epoch 81/300
 - 9s - loss: 6.0194 - acc: 0.8719 - mDice: 0.1368 - val_loss: 6.6078 - val_acc: 0.8970 - val_mDice: 0.1343

Epoch 00081: val_mDice improved from 0.12914 to 0.13427, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 82/300
 - 9s - loss: 5.9883 - acc: 0.8718 - mDice: 0.1379 - val_loss: 6.8823 - val_acc: 0.9015 - val_mDice: 0.1222

Epoch 00082: val_mDice did not improve from 0.13427
Epoch 83/300
 - 9s - loss: 5.9784 - acc: 0.8717 - mDice: 0.1387 - val_loss: 6.9375 - val_acc: 0.8977 - val_mDice: 0.1238

Epoch 00083: val_mDice did not improve from 0.13427
Epoch 84/300
 - 10s - loss: 5.9545 - acc: 0.8717 - mDice: 0.1397 - val_loss: 6.6062 - val_acc: 0.8975 - val_mDice: 0.1365

Epoch 00084: val_mDice improved from 0.13427 to 0.13648, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 85/300
 - 9s - loss: 5.9212 - acc: 0.8717 - mDice: 0.1417 - val_loss: 7.0410 - val_acc: 0.8983 - val_mDice: 0.1277

Epoch 00085: val_mDice did not improve from 0.13648
Epoch 86/300
 - 9s - loss: 5.8805 - acc: 0.8717 - mDice: 0.1432 - val_loss: 6.6935 - val_acc: 0.8956 - val_mDice: 0.1395

Epoch 00086: val_mDice improved from 0.13648 to 0.13951, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 87/300
 - 9s - loss: 5.8677 - acc: 0.8715 - mDice: 0.1437 - val_loss: 6.8040 - val_acc: 0.8993 - val_mDice: 0.1331

Epoch 00087: val_mDice did not improve from 0.13951
Epoch 88/300
 - 9s - loss: 5.8357 - acc: 0.8715 - mDice: 0.1455 - val_loss: 7.1261 - val_acc: 0.8996 - val_mDice: 0.1319

Epoch 00088: val_mDice did not improve from 0.13951
Epoch 89/300
 - 9s - loss: 5.8229 - acc: 0.8713 - mDice: 0.1455 - val_loss: 7.1088 - val_acc: 0.9011 - val_mDice: 0.1212

Epoch 00089: val_mDice did not improve from 0.13951
Epoch 90/300
 - 9s - loss: 5.7974 - acc: 0.8713 - mDice: 0.1467 - val_loss: 7.0811 - val_acc: 0.9016 - val_mDice: 0.1244

Epoch 00090: val_mDice did not improve from 0.13951
Epoch 91/300
 - 9s - loss: 5.7804 - acc: 0.8712 - mDice: 0.1476 - val_loss: 7.1759 - val_acc: 0.9020 - val_mDice: 0.1263

Epoch 00091: val_mDice did not improve from 0.13951
Epoch 92/300
 - 9s - loss: 5.7528 - acc: 0.8712 - mDice: 0.1493 - val_loss: 7.4835 - val_acc: 0.9027 - val_mDice: 0.1169

Epoch 00092: val_mDice did not improve from 0.13951
Epoch 93/300
 - 10s - loss: 5.7490 - acc: 0.8709 - mDice: 0.1494 - val_loss: 7.3118 - val_acc: 0.9020 - val_mDice: 0.1217

Epoch 00093: val_mDice did not improve from 0.13951
Epoch 94/300
 - 9s - loss: 5.7129 - acc: 0.8708 - mDice: 0.1509 - val_loss: 6.7741 - val_acc: 0.8982 - val_mDice: 0.1381

Epoch 00094: val_mDice did not improve from 0.13951
Epoch 95/300
 - 9s - loss: 5.6958 - acc: 0.8698 - mDice: 0.1523 - val_loss: 6.3151 - val_acc: 0.8915 - val_mDice: 0.1470

Epoch 00095: val_mDice improved from 0.13951 to 0.14697, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 96/300
 - 9s - loss: 5.6591 - acc: 0.8688 - mDice: 0.1552 - val_loss: 6.6599 - val_acc: 0.8950 - val_mDice: 0.1481

Epoch 00096: val_mDice improved from 0.14697 to 0.14807, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 97/300
 - 9s - loss: 5.6163 - acc: 0.8681 - mDice: 0.1596 - val_loss: 6.6758 - val_acc: 0.8971 - val_mDice: 0.1514

Epoch 00097: val_mDice improved from 0.14807 to 0.15142, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 98/300
 - 9s - loss: 5.5896 - acc: 0.8690 - mDice: 0.1631 - val_loss: 6.6443 - val_acc: 0.8967 - val_mDice: 0.1484

Epoch 00098: val_mDice did not improve from 0.15142
Epoch 99/300
 - 9s - loss: 5.5590 - acc: 0.8697 - mDice: 0.1650 - val_loss: 6.5110 - val_acc: 0.8969 - val_mDice: 0.1577

Epoch 00099: val_mDice improved from 0.15142 to 0.15769, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 100/300
 - 9s - loss: 5.5334 - acc: 0.8701 - mDice: 0.1674 - val_loss: 6.4060 - val_acc: 0.8974 - val_mDice: 0.1548

Epoch 00100: val_mDice did not improve from 0.15769
Epoch 101/300
 - 9s - loss: 5.5166 - acc: 0.8709 - mDice: 0.1691 - val_loss: 6.4766 - val_acc: 0.8953 - val_mDice: 0.1620

Epoch 00101: val_mDice improved from 0.15769 to 0.16201, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 102/300
 - 10s - loss: 5.4946 - acc: 0.8712 - mDice: 0.1708 - val_loss: 7.0933 - val_acc: 0.8975 - val_mDice: 0.1530

Epoch 00102: val_mDice did not improve from 0.16201
Epoch 103/300
 - 10s - loss: 5.4710 - acc: 0.8716 - mDice: 0.1723 - val_loss: 6.5909 - val_acc: 0.8976 - val_mDice: 0.1610

Epoch 00103: val_mDice did not improve from 0.16201
Epoch 104/300
 - 9s - loss: 5.4467 - acc: 0.8722 - mDice: 0.1746 - val_loss: 6.6971 - val_acc: 0.8987 - val_mDice: 0.1561

Epoch 00104: val_mDice did not improve from 0.16201
Epoch 105/300
 - 9s - loss: 5.4319 - acc: 0.8725 - mDice: 0.1760 - val_loss: 6.5343 - val_acc: 0.8990 - val_mDice: 0.1674

Epoch 00105: val_mDice improved from 0.16201 to 0.16739, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 106/300
 - 9s - loss: 5.4120 - acc: 0.8728 - mDice: 0.1782 - val_loss: 6.5552 - val_acc: 0.8991 - val_mDice: 0.1640

Epoch 00106: val_mDice did not improve from 0.16739
Epoch 107/300
 - 9s - loss: 5.3946 - acc: 0.8727 - mDice: 0.1793 - val_loss: 6.5124 - val_acc: 0.9008 - val_mDice: 0.1690

Epoch 00107: val_mDice improved from 0.16739 to 0.16903, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 108/300
 - 9s - loss: 5.3528 - acc: 0.8744 - mDice: 0.1832 - val_loss: 6.2460 - val_acc: 0.9020 - val_mDice: 0.1751

Epoch 00108: val_mDice improved from 0.16903 to 0.17510, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 109/300
 - 9s - loss: 5.3073 - acc: 0.8800 - mDice: 0.1892 - val_loss: 6.0343 - val_acc: 0.9123 - val_mDice: 0.1866

Epoch 00109: val_mDice improved from 0.17510 to 0.18657, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 110/300
 - 10s - loss: 5.2353 - acc: 0.8840 - mDice: 0.1963 - val_loss: 5.9722 - val_acc: 0.9146 - val_mDice: 0.2005

Epoch 00110: val_mDice improved from 0.18657 to 0.20048, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 111/300
 - 9s - loss: 5.0941 - acc: 0.8858 - mDice: 0.2050 - val_loss: 5.7636 - val_acc: 0.9119 - val_mDice: 0.2038

Epoch 00111: val_mDice improved from 0.20048 to 0.20381, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 112/300
 - 9s - loss: 5.0061 - acc: 0.8863 - mDice: 0.2110 - val_loss: 5.5900 - val_acc: 0.9113 - val_mDice: 0.2135

Epoch 00112: val_mDice improved from 0.20381 to 0.21354, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 113/300
 - 9s - loss: 4.9212 - acc: 0.8864 - mDice: 0.2160 - val_loss: 6.1465 - val_acc: 0.9142 - val_mDice: 0.2072

Epoch 00113: val_mDice did not improve from 0.21354
Epoch 114/300
 - 9s - loss: 4.8836 - acc: 0.8870 - mDice: 0.2195 - val_loss: 5.5925 - val_acc: 0.9089 - val_mDice: 0.2145

Epoch 00114: val_mDice improved from 0.21354 to 0.21455, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 115/300
 - 9s - loss: 4.8250 - acc: 0.8875 - mDice: 0.2248 - val_loss: 5.5950 - val_acc: 0.9134 - val_mDice: 0.2183

Epoch 00115: val_mDice improved from 0.21455 to 0.21825, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 116/300
 - 9s - loss: 4.7997 - acc: 0.8880 - mDice: 0.2279 - val_loss: 5.5046 - val_acc: 0.9120 - val_mDice: 0.2216

Epoch 00116: val_mDice improved from 0.21825 to 0.22156, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 117/300
 - 9s - loss: 4.7603 - acc: 0.8885 - mDice: 0.2320 - val_loss: 5.2601 - val_acc: 0.9129 - val_mDice: 0.2234

Epoch 00117: val_mDice improved from 0.22156 to 0.22337, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 118/300
 - 10s - loss: 4.7435 - acc: 0.8886 - mDice: 0.2335 - val_loss: 5.3199 - val_acc: 0.9059 - val_mDice: 0.2238

Epoch 00118: val_mDice improved from 0.22337 to 0.22375, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 119/300
 - 9s - loss: 4.7145 - acc: 0.8888 - mDice: 0.2366 - val_loss: 5.3853 - val_acc: 0.9138 - val_mDice: 0.2297

Epoch 00119: val_mDice improved from 0.22375 to 0.22974, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 120/300
 - 9s - loss: 4.6896 - acc: 0.8888 - mDice: 0.2389 - val_loss: 5.5997 - val_acc: 0.9163 - val_mDice: 0.2295

Epoch 00120: val_mDice did not improve from 0.22974
Epoch 121/300
 - 9s - loss: 4.6646 - acc: 0.8895 - mDice: 0.2415 - val_loss: 5.3672 - val_acc: 0.9132 - val_mDice: 0.2355

Epoch 00121: val_mDice improved from 0.22974 to 0.23550, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 122/300
 - 9s - loss: 4.6529 - acc: 0.8895 - mDice: 0.2430 - val_loss: 5.4534 - val_acc: 0.9165 - val_mDice: 0.2333

Epoch 00122: val_mDice did not improve from 0.23550
Epoch 123/300
 - 9s - loss: 4.6170 - acc: 0.8899 - mDice: 0.2466 - val_loss: 5.4550 - val_acc: 0.9167 - val_mDice: 0.2402

Epoch 00123: val_mDice improved from 0.23550 to 0.24019, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 124/300
 - 9s - loss: 4.6044 - acc: 0.8902 - mDice: 0.2480 - val_loss: 5.3082 - val_acc: 0.9123 - val_mDice: 0.2378

Epoch 00124: val_mDice did not improve from 0.24019
Epoch 125/300
 - 9s - loss: 4.5954 - acc: 0.8904 - mDice: 0.2494 - val_loss: 5.2864 - val_acc: 0.9124 - val_mDice: 0.2418

Epoch 00125: val_mDice improved from 0.24019 to 0.24184, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 126/300
 - 10s - loss: 4.5601 - acc: 0.8910 - mDice: 0.2519 - val_loss: 5.3066 - val_acc: 0.9183 - val_mDice: 0.2502

Epoch 00126: val_mDice improved from 0.24184 to 0.25024, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 127/300
 - 9s - loss: 4.5400 - acc: 0.8913 - mDice: 0.2548 - val_loss: 5.3976 - val_acc: 0.9168 - val_mDice: 0.2493

Epoch 00127: val_mDice did not improve from 0.25024
Epoch 128/300
 - 9s - loss: 4.5273 - acc: 0.8916 - mDice: 0.2573 - val_loss: 5.3072 - val_acc: 0.9204 - val_mDice: 0.2500

Epoch 00128: val_mDice did not improve from 0.25024
Epoch 129/300
 - 9s - loss: 4.5200 - acc: 0.8919 - mDice: 0.2581 - val_loss: 5.3971 - val_acc: 0.9205 - val_mDice: 0.2500

Epoch 00129: val_mDice did not improve from 0.25024
Epoch 130/300
 - 9s - loss: 4.5037 - acc: 0.8924 - mDice: 0.2603 - val_loss: 5.1858 - val_acc: 0.9207 - val_mDice: 0.2572

Epoch 00130: val_mDice improved from 0.25024 to 0.25724, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 131/300
 - 9s - loss: 4.4908 - acc: 0.8922 - mDice: 0.2636 - val_loss: 5.0507 - val_acc: 0.9183 - val_mDice: 0.2633

Epoch 00131: val_mDice improved from 0.25724 to 0.26328, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 132/300
 - 9s - loss: 4.4822 - acc: 0.8929 - mDice: 0.2658 - val_loss: 5.0580 - val_acc: 0.9170 - val_mDice: 0.2592

Epoch 00132: val_mDice did not improve from 0.26328
Epoch 133/300
 - 10s - loss: 4.4759 - acc: 0.8931 - mDice: 0.2669 - val_loss: 4.9764 - val_acc: 0.9190 - val_mDice: 0.2657

Epoch 00133: val_mDice improved from 0.26328 to 0.26570, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 134/300
 - 10s - loss: 4.4343 - acc: 0.8940 - mDice: 0.2721 - val_loss: 5.0837 - val_acc: 0.9207 - val_mDice: 0.2647

Epoch 00134: val_mDice did not improve from 0.26570
Epoch 135/300
 - 10s - loss: 4.4142 - acc: 0.8943 - mDice: 0.2745 - val_loss: 5.2141 - val_acc: 0.9196 - val_mDice: 0.2691

Epoch 00135: val_mDice improved from 0.26570 to 0.26911, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 136/300
 - 10s - loss: 4.4389 - acc: 0.8941 - mDice: 0.2733 - val_loss: 5.3046 - val_acc: 0.9219 - val_mDice: 0.2685

Epoch 00136: val_mDice did not improve from 0.26911
Epoch 137/300
 - 9s - loss: 4.3933 - acc: 0.8947 - mDice: 0.2775 - val_loss: 5.0742 - val_acc: 0.9231 - val_mDice: 0.2766

Epoch 00137: val_mDice improved from 0.26911 to 0.27656, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 138/300
 - 10s - loss: 4.3966 - acc: 0.8949 - mDice: 0.2785 - val_loss: 5.0138 - val_acc: 0.9237 - val_mDice: 0.2783

Epoch 00138: val_mDice improved from 0.27656 to 0.27834, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 139/300
 - 9s - loss: 4.3752 - acc: 0.8952 - mDice: 0.2804 - val_loss: 5.3195 - val_acc: 0.9208 - val_mDice: 0.2694

Epoch 00139: val_mDice did not improve from 0.27834
Epoch 140/300
 - 10s - loss: 4.3700 - acc: 0.8953 - mDice: 0.2816 - val_loss: 4.8607 - val_acc: 0.9217 - val_mDice: 0.2799

Epoch 00140: val_mDice improved from 0.27834 to 0.27994, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 141/300
 - 10s - loss: 4.3614 - acc: 0.8958 - mDice: 0.2833 - val_loss: 4.9923 - val_acc: 0.9210 - val_mDice: 0.2691

Epoch 00141: val_mDice did not improve from 0.27994
Epoch 142/300
 - 10s - loss: 4.3658 - acc: 0.8959 - mDice: 0.2831 - val_loss: 5.2734 - val_acc: 0.9223 - val_mDice: 0.2731

Epoch 00142: val_mDice did not improve from 0.27994
Epoch 143/300
 - 10s - loss: 4.3356 - acc: 0.8960 - mDice: 0.2854 - val_loss: 5.1209 - val_acc: 0.9229 - val_mDice: 0.2806

Epoch 00143: val_mDice improved from 0.27994 to 0.28055, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 144/300
 - 9s - loss: 4.3179 - acc: 0.8965 - mDice: 0.2884 - val_loss: 5.1739 - val_acc: 0.9226 - val_mDice: 0.2814

Epoch 00144: val_mDice improved from 0.28055 to 0.28140, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 145/300
 - 10s - loss: 4.3283 - acc: 0.8966 - mDice: 0.2882 - val_loss: 5.0274 - val_acc: 0.9208 - val_mDice: 0.2755

Epoch 00145: val_mDice did not improve from 0.28140
Epoch 146/300
 - 9s - loss: 4.3073 - acc: 0.8969 - mDice: 0.2905 - val_loss: 4.9132 - val_acc: 0.9229 - val_mDice: 0.2885

Epoch 00146: val_mDice improved from 0.28140 to 0.28853, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 147/300
 - 10s - loss: 4.3010 - acc: 0.8972 - mDice: 0.2915 - val_loss: 5.0683 - val_acc: 0.9233 - val_mDice: 0.2873

Epoch 00147: val_mDice did not improve from 0.28853
Epoch 148/300
 - 9s - loss: 4.2912 - acc: 0.8974 - mDice: 0.2923 - val_loss: 5.1816 - val_acc: 0.9229 - val_mDice: 0.2869

Epoch 00148: val_mDice did not improve from 0.28853
Epoch 149/300
 - 10s - loss: 4.2678 - acc: 0.8977 - mDice: 0.2963 - val_loss: 5.0972 - val_acc: 0.9227 - val_mDice: 0.2957

Epoch 00149: val_mDice improved from 0.28853 to 0.29574, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 150/300
 - 10s - loss: 4.2561 - acc: 0.8977 - mDice: 0.2994 - val_loss: 5.0967 - val_acc: 0.9218 - val_mDice: 0.2959

Epoch 00150: val_mDice improved from 0.29574 to 0.29585, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 151/300
 - 10s - loss: 4.2387 - acc: 0.8974 - mDice: 0.3014 - val_loss: 5.0276 - val_acc: 0.9201 - val_mDice: 0.2829

Epoch 00151: val_mDice did not improve from 0.29585
Epoch 152/300
 - 10s - loss: 4.2164 - acc: 0.8978 - mDice: 0.3039 - val_loss: 4.8772 - val_acc: 0.9233 - val_mDice: 0.3083

Epoch 00152: val_mDice improved from 0.29585 to 0.30828, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 153/300
 - 10s - loss: 4.1999 - acc: 0.8979 - mDice: 0.3060 - val_loss: 5.1685 - val_acc: 0.9241 - val_mDice: 0.2928

Epoch 00153: val_mDice did not improve from 0.30828
Epoch 154/300
 - 10s - loss: 4.1949 - acc: 0.8981 - mDice: 0.3060 - val_loss: 5.2292 - val_acc: 0.9245 - val_mDice: 0.2985

Epoch 00154: val_mDice did not improve from 0.30828
Epoch 155/300
 - 10s - loss: 4.1844 - acc: 0.8982 - mDice: 0.3086 - val_loss: 5.3152 - val_acc: 0.9221 - val_mDice: 0.2953

Epoch 00155: val_mDice did not improve from 0.30828
Epoch 156/300
 - 10s - loss: 4.1695 - acc: 0.8983 - mDice: 0.3092 - val_loss: 4.8663 - val_acc: 0.9226 - val_mDice: 0.3002

Epoch 00156: val_mDice did not improve from 0.30828
Epoch 157/300
 - 9s - loss: 4.1537 - acc: 0.8985 - mDice: 0.3116 - val_loss: 5.1109 - val_acc: 0.9246 - val_mDice: 0.3071

Epoch 00157: val_mDice did not improve from 0.30828
Epoch 158/300
 - 10s - loss: 4.1406 - acc: 0.8988 - mDice: 0.3133 - val_loss: 5.0713 - val_acc: 0.9210 - val_mDice: 0.2892

Epoch 00158: val_mDice did not improve from 0.30828
Epoch 159/300
 - 10s - loss: 4.1501 - acc: 0.8986 - mDice: 0.3123 - val_loss: 5.2438 - val_acc: 0.9238 - val_mDice: 0.3005

Epoch 00159: val_mDice did not improve from 0.30828
Epoch 160/300
 - 9s - loss: 4.1300 - acc: 0.8989 - mDice: 0.3138 - val_loss: 4.8481 - val_acc: 0.9228 - val_mDice: 0.3073

Epoch 00160: val_mDice did not improve from 0.30828
Epoch 161/300
 - 10s - loss: 4.1269 - acc: 0.8988 - mDice: 0.3145 - val_loss: 4.8746 - val_acc: 0.9247 - val_mDice: 0.3156

Epoch 00161: val_mDice improved from 0.30828 to 0.31560, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 162/300
 - 9s - loss: 4.1171 - acc: 0.8989 - mDice: 0.3157 - val_loss: 4.9435 - val_acc: 0.9255 - val_mDice: 0.3172

Epoch 00162: val_mDice improved from 0.31560 to 0.31722, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 163/300
 - 10s - loss: 4.1135 - acc: 0.8990 - mDice: 0.3163 - val_loss: 5.2179 - val_acc: 0.9237 - val_mDice: 0.3081

Epoch 00163: val_mDice did not improve from 0.31722
Epoch 164/300
 - 10s - loss: 4.0936 - acc: 0.8993 - mDice: 0.3185 - val_loss: 4.9121 - val_acc: 0.9254 - val_mDice: 0.3214

Epoch 00164: val_mDice improved from 0.31722 to 0.32142, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 165/300
 - 10s - loss: 4.0899 - acc: 0.8992 - mDice: 0.3192 - val_loss: 5.0627 - val_acc: 0.9254 - val_mDice: 0.3184

Epoch 00165: val_mDice did not improve from 0.32142
Epoch 166/300
 - 10s - loss: 4.0943 - acc: 0.8993 - mDice: 0.3191 - val_loss: 4.8084 - val_acc: 0.9240 - val_mDice: 0.3162

Epoch 00166: val_mDice did not improve from 0.32142
Epoch 167/300
 - 10s - loss: 4.0777 - acc: 0.8993 - mDice: 0.3206 - val_loss: 4.8759 - val_acc: 0.9249 - val_mDice: 0.3192

Epoch 00167: val_mDice did not improve from 0.32142
Epoch 168/300
 - 10s - loss: 4.0803 - acc: 0.8992 - mDice: 0.3202 - val_loss: 4.6652 - val_acc: 0.9238 - val_mDice: 0.3191

Epoch 00168: val_mDice did not improve from 0.32142
Epoch 169/300
 - 9s - loss: 4.0729 - acc: 0.8996 - mDice: 0.3218 - val_loss: 4.8571 - val_acc: 0.9256 - val_mDice: 0.3211

Epoch 00169: val_mDice did not improve from 0.32142
Epoch 170/300
 - 10s - loss: 4.0705 - acc: 0.8994 - mDice: 0.3214 - val_loss: 4.6004 - val_acc: 0.9246 - val_mDice: 0.3264

Epoch 00170: val_mDice improved from 0.32142 to 0.32636, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 171/300
 - 10s - loss: 4.0617 - acc: 0.8994 - mDice: 0.3221 - val_loss: 5.0309 - val_acc: 0.9260 - val_mDice: 0.3207

Epoch 00171: val_mDice did not improve from 0.32636
Epoch 172/300
 - 9s - loss: 4.0468 - acc: 0.8998 - mDice: 0.3245 - val_loss: 4.8066 - val_acc: 0.9255 - val_mDice: 0.3225

Epoch 00172: val_mDice did not improve from 0.32636
Epoch 173/300
 - 10s - loss: 4.0367 - acc: 0.8996 - mDice: 0.3253 - val_loss: 5.1178 - val_acc: 0.9265 - val_mDice: 0.3213

Epoch 00173: val_mDice did not improve from 0.32636
Epoch 174/300
 - 10s - loss: 4.0296 - acc: 0.8997 - mDice: 0.3263 - val_loss: 4.9706 - val_acc: 0.9263 - val_mDice: 0.3261

Epoch 00174: val_mDice did not improve from 0.32636
Epoch 175/300
 - 10s - loss: 4.0508 - acc: 0.8994 - mDice: 0.3248 - val_loss: 4.9407 - val_acc: 0.9256 - val_mDice: 0.3298

Epoch 00175: val_mDice improved from 0.32636 to 0.32985, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 176/300
 - 9s - loss: 4.0301 - acc: 0.8996 - mDice: 0.3265 - val_loss: 4.8747 - val_acc: 0.9218 - val_mDice: 0.3115

Epoch 00176: val_mDice did not improve from 0.32985
Epoch 177/300
 - 10s - loss: 4.0252 - acc: 0.8998 - mDice: 0.3279 - val_loss: 4.8533 - val_acc: 0.9262 - val_mDice: 0.3268

Epoch 00177: val_mDice did not improve from 0.32985
Epoch 178/300
 - 9s - loss: 4.0071 - acc: 0.8997 - mDice: 0.3290 - val_loss: 4.7134 - val_acc: 0.9268 - val_mDice: 0.3342

Epoch 00178: val_mDice improved from 0.32985 to 0.33420, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 179/300
 - 9s - loss: 4.0141 - acc: 0.8998 - mDice: 0.3291 - val_loss: 5.1187 - val_acc: 0.9253 - val_mDice: 0.3197

Epoch 00179: val_mDice did not improve from 0.33420
Epoch 180/300
 - 9s - loss: 4.0161 - acc: 0.8999 - mDice: 0.3290 - val_loss: 4.9678 - val_acc: 0.9242 - val_mDice: 0.3177

Epoch 00180: val_mDice did not improve from 0.33420
Epoch 181/300
 - 10s - loss: 4.0130 - acc: 0.8998 - mDice: 0.3293 - val_loss: 4.8110 - val_acc: 0.9242 - val_mDice: 0.3223

Epoch 00181: val_mDice did not improve from 0.33420
Epoch 182/300
 - 10s - loss: 3.9930 - acc: 0.8998 - mDice: 0.3304 - val_loss: 4.7045 - val_acc: 0.9252 - val_mDice: 0.3298

Epoch 00182: val_mDice did not improve from 0.33420
Epoch 183/300
 - 11s - loss: 3.9985 - acc: 0.8999 - mDice: 0.3298 - val_loss: 4.9575 - val_acc: 0.9234 - val_mDice: 0.3141

Epoch 00183: val_mDice did not improve from 0.33420
Epoch 184/300
 - 11s - loss: 3.9890 - acc: 0.9002 - mDice: 0.3319 - val_loss: 4.8109 - val_acc: 0.9235 - val_mDice: 0.3244

Epoch 00184: val_mDice did not improve from 0.33420
Epoch 185/300
 - 11s - loss: 3.9923 - acc: 0.9000 - mDice: 0.3314 - val_loss: 4.9132 - val_acc: 0.9275 - val_mDice: 0.3334

Epoch 00185: val_mDice did not improve from 0.33420
Epoch 186/300
 - 10s - loss: 3.9782 - acc: 0.9001 - mDice: 0.3327 - val_loss: 4.9836 - val_acc: 0.9248 - val_mDice: 0.3219

Epoch 00186: val_mDice did not improve from 0.33420
Epoch 187/300
 - 11s - loss: 3.9709 - acc: 0.9003 - mDice: 0.3338 - val_loss: 5.0350 - val_acc: 0.9232 - val_mDice: 0.3168

Epoch 00187: val_mDice did not improve from 0.33420
Epoch 188/300
 - 10s - loss: 3.9796 - acc: 0.9002 - mDice: 0.3330 - val_loss: 4.8511 - val_acc: 0.9267 - val_mDice: 0.3288

Epoch 00188: val_mDice did not improve from 0.33420
Epoch 189/300
 - 11s - loss: 3.9602 - acc: 0.9002 - mDice: 0.3347 - val_loss: 4.5631 - val_acc: 0.9266 - val_mDice: 0.3420

Epoch 00189: val_mDice improved from 0.33420 to 0.34195, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 190/300
 - 10s - loss: 3.9705 - acc: 0.9001 - mDice: 0.3343 - val_loss: 4.8217 - val_acc: 0.9259 - val_mDice: 0.3358

Epoch 00190: val_mDice did not improve from 0.34195
Epoch 191/300
 - 11s - loss: 3.9390 - acc: 0.9006 - mDice: 0.3377 - val_loss: 5.0471 - val_acc: 0.9276 - val_mDice: 0.3254

Epoch 00191: val_mDice did not improve from 0.34195
Epoch 192/300
 - 10s - loss: 3.9561 - acc: 0.9004 - mDice: 0.3354 - val_loss: 4.7318 - val_acc: 0.9242 - val_mDice: 0.3278

Epoch 00192: val_mDice did not improve from 0.34195
Epoch 193/300
 - 11s - loss: 3.9330 - acc: 0.9007 - mDice: 0.3382 - val_loss: 4.6808 - val_acc: 0.9260 - val_mDice: 0.3310

Epoch 00193: val_mDice did not improve from 0.34195
Epoch 194/300
 - 10s - loss: 3.9406 - acc: 0.9004 - mDice: 0.3372 - val_loss: 4.9033 - val_acc: 0.9256 - val_mDice: 0.3234

Epoch 00194: val_mDice did not improve from 0.34195
Epoch 195/300
 - 11s - loss: 3.9363 - acc: 0.9007 - mDice: 0.3379 - val_loss: 4.7165 - val_acc: 0.9286 - val_mDice: 0.3349

Epoch 00195: val_mDice did not improve from 0.34195
Epoch 196/300
 - 10s - loss: 3.9358 - acc: 0.9006 - mDice: 0.3386 - val_loss: 4.7723 - val_acc: 0.9264 - val_mDice: 0.3317

Epoch 00196: val_mDice did not improve from 0.34195
Epoch 197/300
 - 10s - loss: 3.9287 - acc: 0.9008 - mDice: 0.3388 - val_loss: 4.8468 - val_acc: 0.9253 - val_mDice: 0.3335

Epoch 00197: val_mDice did not improve from 0.34195
Epoch 198/300
 - 10s - loss: 3.9264 - acc: 0.9008 - mDice: 0.3395 - val_loss: 4.5808 - val_acc: 0.9251 - val_mDice: 0.3387

Epoch 00198: val_mDice did not improve from 0.34195
Epoch 199/300
 - 10s - loss: 3.9164 - acc: 0.9009 - mDice: 0.3405 - val_loss: 4.9497 - val_acc: 0.9271 - val_mDice: 0.3351

Epoch 00199: val_mDice did not improve from 0.34195
Epoch 200/300
 - 10s - loss: 3.9188 - acc: 0.9009 - mDice: 0.3398 - val_loss: 4.7750 - val_acc: 0.9234 - val_mDice: 0.3295

Epoch 00200: val_mDice did not improve from 0.34195
Epoch 201/300
 - 10s - loss: 3.9187 - acc: 0.9010 - mDice: 0.3406 - val_loss: 4.8706 - val_acc: 0.9253 - val_mDice: 0.3367

Epoch 00201: val_mDice did not improve from 0.34195
Epoch 202/300
 - 10s - loss: 3.9105 - acc: 0.9010 - mDice: 0.3420 - val_loss: 4.8328 - val_acc: 0.9269 - val_mDice: 0.3360

Epoch 00202: val_mDice did not improve from 0.34195
Epoch 203/300
 - 10s - loss: 3.9085 - acc: 0.9010 - mDice: 0.3423 - val_loss: 4.8658 - val_acc: 0.9215 - val_mDice: 0.3268

Epoch 00203: val_mDice did not improve from 0.34195
Epoch 204/300
 - 10s - loss: 3.9098 - acc: 0.9008 - mDice: 0.3413 - val_loss: 4.8277 - val_acc: 0.9267 - val_mDice: 0.3407

Epoch 00204: val_mDice did not improve from 0.34195
Epoch 205/300
 - 10s - loss: 3.8929 - acc: 0.9010 - mDice: 0.3426 - val_loss: 4.8775 - val_acc: 0.9265 - val_mDice: 0.3343

Epoch 00205: val_mDice did not improve from 0.34195
Epoch 206/300
 - 9s - loss: 3.8896 - acc: 0.9013 - mDice: 0.3444 - val_loss: 4.7099 - val_acc: 0.9269 - val_mDice: 0.3386

Epoch 00206: val_mDice did not improve from 0.34195
Epoch 207/300
 - 9s - loss: 3.8859 - acc: 0.9011 - mDice: 0.3446 - val_loss: 4.7270 - val_acc: 0.9250 - val_mDice: 0.3409

Epoch 00207: val_mDice did not improve from 0.34195
Epoch 208/300
 - 9s - loss: 3.8927 - acc: 0.9012 - mDice: 0.3441 - val_loss: 4.8307 - val_acc: 0.9246 - val_mDice: 0.3270

Epoch 00208: val_mDice did not improve from 0.34195
Epoch 209/300
 - 10s - loss: 3.8848 - acc: 0.9013 - mDice: 0.3447 - val_loss: 4.6881 - val_acc: 0.9257 - val_mDice: 0.3383

Epoch 00209: val_mDice did not improve from 0.34195
Epoch 210/300
 - 9s - loss: 3.8883 - acc: 0.9012 - mDice: 0.3448 - val_loss: 4.6419 - val_acc: 0.9228 - val_mDice: 0.3364

Epoch 00210: val_mDice did not improve from 0.34195
Epoch 211/300
 - 9s - loss: 3.8954 - acc: 0.9011 - mDice: 0.3433 - val_loss: 4.7055 - val_acc: 0.9255 - val_mDice: 0.3400

Epoch 00211: val_mDice did not improve from 0.34195
Epoch 212/300
 - 9s - loss: 3.8675 - acc: 0.9014 - mDice: 0.3467 - val_loss: 4.6422 - val_acc: 0.9252 - val_mDice: 0.3370

Epoch 00212: val_mDice did not improve from 0.34195
Epoch 213/300
 - 9s - loss: 3.8790 - acc: 0.9013 - mDice: 0.3459 - val_loss: 4.7953 - val_acc: 0.9255 - val_mDice: 0.3417

Epoch 00213: val_mDice did not improve from 0.34195
Epoch 214/300
 - 9s - loss: 3.8665 - acc: 0.9015 - mDice: 0.3474 - val_loss: 4.5980 - val_acc: 0.9260 - val_mDice: 0.3385

Epoch 00214: val_mDice did not improve from 0.34195
Epoch 215/300
 - 9s - loss: 3.8635 - acc: 0.9016 - mDice: 0.3475 - val_loss: 4.6748 - val_acc: 0.9273 - val_mDice: 0.3424

Epoch 00215: val_mDice improved from 0.34195 to 0.34242, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 216/300
 - 9s - loss: 3.8648 - acc: 0.9015 - mDice: 0.3478 - val_loss: 5.1060 - val_acc: 0.9266 - val_mDice: 0.3329

Epoch 00216: val_mDice did not improve from 0.34242
Epoch 217/300
 - 9s - loss: 3.8642 - acc: 0.9016 - mDice: 0.3475 - val_loss: 4.7827 - val_acc: 0.9245 - val_mDice: 0.3372

Epoch 00217: val_mDice did not improve from 0.34242
Epoch 218/300
 - 10s - loss: 3.8701 - acc: 0.9015 - mDice: 0.3466 - val_loss: 4.6936 - val_acc: 0.9254 - val_mDice: 0.3461

Epoch 00218: val_mDice improved from 0.34242 to 0.34614, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 219/300
 - 9s - loss: 3.8511 - acc: 0.9017 - mDice: 0.3484 - val_loss: 4.4949 - val_acc: 0.9248 - val_mDice: 0.3484

Epoch 00219: val_mDice improved from 0.34614 to 0.34841, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 220/300
 - 9s - loss: 3.8503 - acc: 0.9017 - mDice: 0.3498 - val_loss: 4.7705 - val_acc: 0.9275 - val_mDice: 0.3403

Epoch 00220: val_mDice did not improve from 0.34841
Epoch 221/300
 - 9s - loss: 3.8342 - acc: 0.9018 - mDice: 0.3507 - val_loss: 4.7545 - val_acc: 0.9245 - val_mDice: 0.3417

Epoch 00221: val_mDice did not improve from 0.34841
Epoch 222/300
 - 9s - loss: 3.8418 - acc: 0.9018 - mDice: 0.3505 - val_loss: 4.7946 - val_acc: 0.9254 - val_mDice: 0.3402

Epoch 00222: val_mDice did not improve from 0.34841
Epoch 223/300
 - 9s - loss: 3.8279 - acc: 0.9021 - mDice: 0.3526 - val_loss: 4.7712 - val_acc: 0.9259 - val_mDice: 0.3376

Epoch 00223: val_mDice did not improve from 0.34841
Epoch 224/300
 - 9s - loss: 3.8430 - acc: 0.9017 - mDice: 0.3502 - val_loss: 4.7377 - val_acc: 0.9262 - val_mDice: 0.3442

Epoch 00224: val_mDice did not improve from 0.34841
Epoch 225/300
 - 9s - loss: 3.8355 - acc: 0.9018 - mDice: 0.3508 - val_loss: 4.6963 - val_acc: 0.9257 - val_mDice: 0.3480

Epoch 00225: val_mDice did not improve from 0.34841
Epoch 226/300
 - 9s - loss: 3.8146 - acc: 0.9021 - mDice: 0.3534 - val_loss: 4.7243 - val_acc: 0.9251 - val_mDice: 0.3462

Epoch 00226: val_mDice did not improve from 0.34841
Epoch 227/300
 - 9s - loss: 3.8174 - acc: 0.9020 - mDice: 0.3533 - val_loss: 4.8895 - val_acc: 0.9285 - val_mDice: 0.3480

Epoch 00227: val_mDice did not improve from 0.34841
Epoch 228/300
 - 10s - loss: 3.8176 - acc: 0.9021 - mDice: 0.3542 - val_loss: 4.5573 - val_acc: 0.9260 - val_mDice: 0.3518

Epoch 00228: val_mDice improved from 0.34841 to 0.35178, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 229/300
 - 9s - loss: 3.8309 - acc: 0.9019 - mDice: 0.3519 - val_loss: 4.5288 - val_acc: 0.9261 - val_mDice: 0.3536

Epoch 00229: val_mDice improved from 0.35178 to 0.35363, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 230/300
 - 9s - loss: 3.8181 - acc: 0.9021 - mDice: 0.3535 - val_loss: 4.4502 - val_acc: 0.9251 - val_mDice: 0.3547

Epoch 00230: val_mDice improved from 0.35363 to 0.35466, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 231/300
 - 9s - loss: 3.8065 - acc: 0.9022 - mDice: 0.3546 - val_loss: 4.7892 - val_acc: 0.9273 - val_mDice: 0.3457

Epoch 00231: val_mDice did not improve from 0.35466
Epoch 232/300
 - 9s - loss: 3.8153 - acc: 0.9021 - mDice: 0.3546 - val_loss: 4.5447 - val_acc: 0.9274 - val_mDice: 0.3522

Epoch 00232: val_mDice did not improve from 0.35466
Epoch 233/300
 - 9s - loss: 3.8206 - acc: 0.9020 - mDice: 0.3534 - val_loss: 4.6160 - val_acc: 0.9250 - val_mDice: 0.3414

Epoch 00233: val_mDice did not improve from 0.35466
Epoch 234/300
 - 9s - loss: 3.8142 - acc: 0.9019 - mDice: 0.3534 - val_loss: 4.9632 - val_acc: 0.9271 - val_mDice: 0.3439

Epoch 00234: val_mDice did not improve from 0.35466
Epoch 235/300
 - 9s - loss: 3.8048 - acc: 0.9021 - mDice: 0.3558 - val_loss: 5.1909 - val_acc: 0.9264 - val_mDice: 0.3378

Epoch 00235: val_mDice did not improve from 0.35466
Epoch 236/300
 - 9s - loss: 3.7999 - acc: 0.9022 - mDice: 0.3560 - val_loss: 4.5440 - val_acc: 0.9218 - val_mDice: 0.3344

Epoch 00236: val_mDice did not improve from 0.35466
Epoch 237/300
 - 9s - loss: 3.8043 - acc: 0.9023 - mDice: 0.3556 - val_loss: 5.0443 - val_acc: 0.9273 - val_mDice: 0.3419

Epoch 00237: val_mDice did not improve from 0.35466
Epoch 238/300
 - 9s - loss: 3.7862 - acc: 0.9022 - mDice: 0.3581 - val_loss: 4.5642 - val_acc: 0.9246 - val_mDice: 0.3485

Epoch 00238: val_mDice did not improve from 0.35466
Epoch 239/300
 - 10s - loss: 3.7966 - acc: 0.9022 - mDice: 0.3563 - val_loss: 4.4999 - val_acc: 0.9239 - val_mDice: 0.3473

Epoch 00239: val_mDice did not improve from 0.35466
Epoch 240/300
 - 9s - loss: 3.7989 - acc: 0.9023 - mDice: 0.3569 - val_loss: 4.7533 - val_acc: 0.9249 - val_mDice: 0.3442

Epoch 00240: val_mDice did not improve from 0.35466
Epoch 241/300
 - 9s - loss: 3.7928 - acc: 0.9022 - mDice: 0.3570 - val_loss: 4.5816 - val_acc: 0.9263 - val_mDice: 0.3550

Epoch 00241: val_mDice improved from 0.35466 to 0.35495, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 242/300
 - 9s - loss: 3.7780 - acc: 0.9024 - mDice: 0.3595 - val_loss: 4.5474 - val_acc: 0.9239 - val_mDice: 0.3464

Epoch 00242: val_mDice did not improve from 0.35495
Epoch 243/300
 - 9s - loss: 3.7725 - acc: 0.9022 - mDice: 0.3588 - val_loss: 4.6491 - val_acc: 0.9266 - val_mDice: 0.3517

Epoch 00243: val_mDice did not improve from 0.35495
Epoch 244/300
 - 9s - loss: 3.7775 - acc: 0.9024 - mDice: 0.3596 - val_loss: 4.7715 - val_acc: 0.9237 - val_mDice: 0.3422

Epoch 00244: val_mDice did not improve from 0.35495
Epoch 245/300
 - 9s - loss: 3.7911 - acc: 0.9021 - mDice: 0.3585 - val_loss: 4.6909 - val_acc: 0.9267 - val_mDice: 0.3482

Epoch 00245: val_mDice did not improve from 0.35495
Epoch 246/300
 - 9s - loss: 3.7809 - acc: 0.9024 - mDice: 0.3590 - val_loss: 4.8426 - val_acc: 0.9261 - val_mDice: 0.3460

Epoch 00246: val_mDice did not improve from 0.35495
Epoch 247/300
 - 9s - loss: 3.7755 - acc: 0.9023 - mDice: 0.3599 - val_loss: 4.8375 - val_acc: 0.9248 - val_mDice: 0.3352

Epoch 00247: val_mDice did not improve from 0.35495
Epoch 248/300
 - 10s - loss: 3.7657 - acc: 0.9025 - mDice: 0.3604 - val_loss: 4.7342 - val_acc: 0.9260 - val_mDice: 0.3509

Epoch 00248: val_mDice did not improve from 0.35495
Epoch 249/300
 - 9s - loss: 3.7689 - acc: 0.9021 - mDice: 0.3593 - val_loss: 4.9010 - val_acc: 0.9262 - val_mDice: 0.3464

Epoch 00249: val_mDice did not improve from 0.35495
Epoch 250/300
 - 9s - loss: 3.7656 - acc: 0.9024 - mDice: 0.3609 - val_loss: 4.9740 - val_acc: 0.9252 - val_mDice: 0.3381

Epoch 00250: val_mDice did not improve from 0.35495
Epoch 251/300
 - 9s - loss: 3.7563 - acc: 0.9023 - mDice: 0.3621 - val_loss: 4.5307 - val_acc: 0.9256 - val_mDice: 0.3600

Epoch 00251: val_mDice improved from 0.35495 to 0.35998, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 252/300
 - 9s - loss: 3.7571 - acc: 0.9025 - mDice: 0.3619 - val_loss: 4.9210 - val_acc: 0.9243 - val_mDice: 0.3468

Epoch 00252: val_mDice did not improve from 0.35998
Epoch 253/300
 - 9s - loss: 3.7546 - acc: 0.9025 - mDice: 0.3630 - val_loss: 4.7960 - val_acc: 0.9240 - val_mDice: 0.3491

Epoch 00253: val_mDice did not improve from 0.35998
Epoch 254/300
 - 9s - loss: 3.7432 - acc: 0.9024 - mDice: 0.3639 - val_loss: 4.5491 - val_acc: 0.9268 - val_mDice: 0.3604

Epoch 00254: val_mDice improved from 0.35998 to 0.36042, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 255/300
 - 9s - loss: 3.7626 - acc: 0.9023 - mDice: 0.3626 - val_loss: 4.8673 - val_acc: 0.9260 - val_mDice: 0.3419

Epoch 00255: val_mDice did not improve from 0.36042
Epoch 256/300
 - 9s - loss: 3.7575 - acc: 0.9023 - mDice: 0.3618 - val_loss: 4.6460 - val_acc: 0.9261 - val_mDice: 0.3606

Epoch 00256: val_mDice improved from 0.36042 to 0.36064, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 257/300
 - 10s - loss: 3.7354 - acc: 0.9025 - mDice: 0.3648 - val_loss: 4.8017 - val_acc: 0.9272 - val_mDice: 0.3528

Epoch 00257: val_mDice did not improve from 0.36064
Epoch 258/300
 - 9s - loss: 3.7460 - acc: 0.9024 - mDice: 0.3636 - val_loss: 4.7576 - val_acc: 0.9254 - val_mDice: 0.3555

Epoch 00258: val_mDice did not improve from 0.36064
Epoch 259/300
 - 9s - loss: 3.7396 - acc: 0.9025 - mDice: 0.3643 - val_loss: 4.9803 - val_acc: 0.9259 - val_mDice: 0.3405

Epoch 00259: val_mDice did not improve from 0.36064
Epoch 260/300
 - 9s - loss: 3.7407 - acc: 0.9025 - mDice: 0.3644 - val_loss: 4.8994 - val_acc: 0.9227 - val_mDice: 0.3364

Epoch 00260: val_mDice did not improve from 0.36064
Epoch 261/300
 - 9s - loss: 3.7359 - acc: 0.9025 - mDice: 0.3652 - val_loss: 4.7748 - val_acc: 0.9245 - val_mDice: 0.3489

Epoch 00261: val_mDice did not improve from 0.36064
Epoch 262/300
 - 9s - loss: 3.7468 - acc: 0.9022 - mDice: 0.3638 - val_loss: 4.7933 - val_acc: 0.9236 - val_mDice: 0.3384

Epoch 00262: val_mDice did not improve from 0.36064
Epoch 263/300
 - 10s - loss: 3.7378 - acc: 0.9024 - mDice: 0.3648 - val_loss: 4.8093 - val_acc: 0.9246 - val_mDice: 0.3468

Epoch 00263: val_mDice did not improve from 0.36064
Epoch 264/300
 - 10s - loss: 3.7289 - acc: 0.9024 - mDice: 0.3661 - val_loss: 4.3439 - val_acc: 0.9261 - val_mDice: 0.3602

Epoch 00264: val_mDice did not improve from 0.36064
Epoch 265/300
 - 9s - loss: 3.7280 - acc: 0.9024 - mDice: 0.3660 - val_loss: 5.1165 - val_acc: 0.9241 - val_mDice: 0.3390

Epoch 00265: val_mDice did not improve from 0.36064
Epoch 266/300
 - 10s - loss: 3.7349 - acc: 0.9024 - mDice: 0.3652 - val_loss: 4.6327 - val_acc: 0.9250 - val_mDice: 0.3472

Epoch 00266: val_mDice did not improve from 0.36064
Epoch 267/300
 - 9s - loss: 3.7279 - acc: 0.9025 - mDice: 0.3665 - val_loss: 4.4343 - val_acc: 0.9272 - val_mDice: 0.3610

Epoch 00267: val_mDice improved from 0.36064 to 0.36101, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 268/300
 - 10s - loss: 3.7376 - acc: 0.9025 - mDice: 0.3649 - val_loss: 4.8480 - val_acc: 0.9249 - val_mDice: 0.3417

Epoch 00268: val_mDice did not improve from 0.36101
Epoch 269/300
 - 9s - loss: 3.7213 - acc: 0.9025 - mDice: 0.3663 - val_loss: 4.6601 - val_acc: 0.9277 - val_mDice: 0.3608

Epoch 00269: val_mDice did not improve from 0.36101
Epoch 270/300
 - 10s - loss: 3.7242 - acc: 0.9024 - mDice: 0.3665 - val_loss: 4.9403 - val_acc: 0.9246 - val_mDice: 0.3377

Epoch 00270: val_mDice did not improve from 0.36101
Epoch 271/300
 - 10s - loss: 3.7220 - acc: 0.9024 - mDice: 0.3664 - val_loss: 4.5399 - val_acc: 0.9259 - val_mDice: 0.3556

Epoch 00271: val_mDice did not improve from 0.36101
Epoch 272/300
 - 9s - loss: 3.7150 - acc: 0.9026 - mDice: 0.3677 - val_loss: 4.9542 - val_acc: 0.9250 - val_mDice: 0.3510

Epoch 00272: val_mDice did not improve from 0.36101
Epoch 273/300
 - 9s - loss: 3.7140 - acc: 0.9024 - mDice: 0.3678 - val_loss: 4.7540 - val_acc: 0.9244 - val_mDice: 0.3466

Epoch 00273: val_mDice did not improve from 0.36101
Epoch 274/300
 - 10s - loss: 3.7158 - acc: 0.9025 - mDice: 0.3679 - val_loss: 4.6603 - val_acc: 0.9247 - val_mDice: 0.3521

Epoch 00274: val_mDice did not improve from 0.36101
Epoch 275/300
 - 9s - loss: 3.7233 - acc: 0.9024 - mDice: 0.3675 - val_loss: 4.7143 - val_acc: 0.9265 - val_mDice: 0.3518

Epoch 00275: val_mDice did not improve from 0.36101
Epoch 276/300
 - 9s - loss: 3.7114 - acc: 0.9024 - mDice: 0.3678 - val_loss: 4.6253 - val_acc: 0.9272 - val_mDice: 0.3621

Epoch 00276: val_mDice improved from 0.36101 to 0.36209, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 277/300
 - 9s - loss: 3.7072 - acc: 0.9026 - mDice: 0.3692 - val_loss: 4.8665 - val_acc: 0.9259 - val_mDice: 0.3538

Epoch 00277: val_mDice did not improve from 0.36209
Epoch 278/300
 - 9s - loss: 3.7042 - acc: 0.9024 - mDice: 0.3691 - val_loss: 4.5054 - val_acc: 0.9255 - val_mDice: 0.3609

Epoch 00278: val_mDice did not improve from 0.36209
Epoch 279/300
 - 9s - loss: 3.7032 - acc: 0.9024 - mDice: 0.3692 - val_loss: 4.8280 - val_acc: 0.9252 - val_mDice: 0.3519

Epoch 00279: val_mDice did not improve from 0.36209
Epoch 280/300
 - 10s - loss: 3.7141 - acc: 0.9024 - mDice: 0.3688 - val_loss: 4.7537 - val_acc: 0.9277 - val_mDice: 0.3510

Epoch 00280: val_mDice did not improve from 0.36209
Epoch 281/300
 - 9s - loss: 3.6947 - acc: 0.9024 - mDice: 0.3705 - val_loss: 4.6943 - val_acc: 0.9260 - val_mDice: 0.3551

Epoch 00281: val_mDice did not improve from 0.36209
Epoch 282/300
 - 9s - loss: 3.6883 - acc: 0.9025 - mDice: 0.3709 - val_loss: 4.7787 - val_acc: 0.9269 - val_mDice: 0.3567

Epoch 00282: val_mDice did not improve from 0.36209
Epoch 283/300
 - 10s - loss: 3.6921 - acc: 0.9024 - mDice: 0.3702 - val_loss: 4.8575 - val_acc: 0.9253 - val_mDice: 0.3529

Epoch 00283: val_mDice did not improve from 0.36209
Epoch 284/300
 - 9s - loss: 3.6907 - acc: 0.9024 - mDice: 0.3711 - val_loss: 4.8827 - val_acc: 0.9262 - val_mDice: 0.3540

Epoch 00284: val_mDice did not improve from 0.36209
Epoch 285/300
 - 9s - loss: 3.6916 - acc: 0.9025 - mDice: 0.3709 - val_loss: 4.8753 - val_acc: 0.9243 - val_mDice: 0.3457

Epoch 00285: val_mDice did not improve from 0.36209
Epoch 286/300
 - 10s - loss: 3.6872 - acc: 0.9025 - mDice: 0.3714 - val_loss: 4.8600 - val_acc: 0.9263 - val_mDice: 0.3530

Epoch 00286: val_mDice did not improve from 0.36209
Epoch 287/300
 - 9s - loss: 3.6891 - acc: 0.9025 - mDice: 0.3716 - val_loss: 4.6467 - val_acc: 0.9260 - val_mDice: 0.3640

Epoch 00287: val_mDice improved from 0.36209 to 0.36398, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 288/300
 - 10s - loss: 3.6890 - acc: 0.9022 - mDice: 0.3710 - val_loss: 4.5388 - val_acc: 0.9260 - val_mDice: 0.3576

Epoch 00288: val_mDice did not improve from 0.36398
Epoch 289/300
 - 10s - loss: 3.6905 - acc: 0.9024 - mDice: 0.3715 - val_loss: 4.9903 - val_acc: 0.9255 - val_mDice: 0.3462

Epoch 00289: val_mDice did not improve from 0.36398
Epoch 290/300
 - 9s - loss: 3.6690 - acc: 0.9024 - mDice: 0.3739 - val_loss: 4.7520 - val_acc: 0.9257 - val_mDice: 0.3511

Epoch 00290: val_mDice did not improve from 0.36398
Epoch 291/300
 - 10s - loss: 3.6670 - acc: 0.9026 - mDice: 0.3745 - val_loss: 4.7346 - val_acc: 0.9280 - val_mDice: 0.3558

Epoch 00291: val_mDice did not improve from 0.36398
Epoch 292/300
 - 9s - loss: 3.6735 - acc: 0.9024 - mDice: 0.3736 - val_loss: 4.4371 - val_acc: 0.9271 - val_mDice: 0.3625

Epoch 00292: val_mDice did not improve from 0.36398
Epoch 293/300
 - 9s - loss: 3.6702 - acc: 0.9024 - mDice: 0.3734 - val_loss: 4.6987 - val_acc: 0.9270 - val_mDice: 0.3573

Epoch 00293: val_mDice did not improve from 0.36398
Epoch 294/300
 - 10s - loss: 3.6674 - acc: 0.9025 - mDice: 0.3739 - val_loss: 4.8263 - val_acc: 0.9259 - val_mDice: 0.3527

Epoch 00294: val_mDice did not improve from 0.36398
Epoch 295/300
 - 9s - loss: 3.6694 - acc: 0.9023 - mDice: 0.3738 - val_loss: 4.5391 - val_acc: 0.9274 - val_mDice: 0.3630

Epoch 00295: val_mDice did not improve from 0.36398
Epoch 296/300
 - 10s - loss: 3.6739 - acc: 0.9024 - mDice: 0.3744 - val_loss: 4.9905 - val_acc: 0.9270 - val_mDice: 0.3528

Epoch 00296: val_mDice did not improve from 0.36398
Epoch 297/300
 - 9s - loss: 3.6486 - acc: 0.9023 - mDice: 0.3766 - val_loss: 4.9054 - val_acc: 0.9255 - val_mDice: 0.3503

Epoch 00297: val_mDice did not improve from 0.36398
Epoch 298/300
 - 10s - loss: 3.6560 - acc: 0.9024 - mDice: 0.3755 - val_loss: 4.7529 - val_acc: 0.9264 - val_mDice: 0.3542

Epoch 00298: val_mDice did not improve from 0.36398
Epoch 299/300
 - 9s - loss: 3.6468 - acc: 0.9026 - mDice: 0.3769 - val_loss: 4.7138 - val_acc: 0.9262 - val_mDice: 0.3509

Epoch 00299: val_mDice did not improve from 0.36398
Epoch 300/300
 - 10s - loss: 3.6546 - acc: 0.9024 - mDice: 0.3761 - val_loss: 4.5942 - val_acc: 0.9268 - val_mDice: 0.3620

Epoch 00300: val_mDice did not improve from 0.36398
{'val_loss': [121.65266146109654, 35.435817690996025, 24.02267031486218, 17.221740763921005, 14.321996464179112, 17.27403839505636, 12.90984239257299, 12.86122934634869, 11.53910541992921, 11.956982887708223, 11.659041879268793, 10.908810620124523, 9.219645066903187, 8.863506871920366, 8.459556806545992, 8.40787320641371, 8.2780590401246, 8.046999647067143, 8.35002055993447, 7.97278508085471, 7.918953613593028, 7.822993079057107, 8.02102254674985, 7.984491889293377, 7.7898395497065325, 7.755999040145141, 7.843395985089815, 7.89321250640429, 7.7661775430807705, 7.628833374151816, 7.672390683339192, 7.659091979265213, 7.600741183528533, 8.13105030816335, 7.6673965992835855, 7.488781151863245, 7.760946637162795, 7.6889073092203875, 7.3425579231518965, 7.319831895140501, 7.4037156999111176, 7.629359570833353, 7.543114007665561, 7.210376404798948, 7.113013489888265, 7.312552327146897, 7.212245454008762, 7.489469304680824, 7.985845960103548, 7.394655218491187, 7.599533824966504, 7.4755708781572485, 7.333699989777345, 7.2716368184639855, 7.372194656958947, 7.453860491514206, 7.2399841867960415, 7.151242523239209, 7.507822318718984, 7.105623821799572, 7.099994587210508, 7.177229934013807, 7.455374777317047, 7.106927335262299, 7.213478748614971, 7.324966627817887, 7.078943179203914, 7.21402220084117, 7.6030120597435875, 7.332240487520512, 7.309302096183483, 6.952297490376693, 7.183912254296816, 6.993804019231063, 6.9854085789277, 6.936230854346202, 7.100015080892122, 6.799365832255437, 6.920507087157323, 6.971223363509545, 6.60776205934011, 6.882260359250582, 6.937473223759578, 6.60618566091244, 7.04101472405287, 6.69349602323312, 6.804005024524836, 7.126092718197749, 7.10878651875716, 7.081082630615968, 7.1758863146488485, 7.483513378179991, 7.311829961263216, 6.774075952860025, 6.31513941517243, 6.659902302118448, 6.675783608968441, 6.644319827740009, 6.511023934070881, 6.405970827891276, 6.476605791311997, 7.093257523500002, 6.590897532609793, 6.697123071322074, 6.534270112331097, 6.555214691620606, 6.512377681640478, 6.245951290314014, 6.034316367827929, 5.9721778860458965, 5.763575125199098, 5.590047549742919, 6.146541045262263, 5.592513868441949, 5.5950420177899876, 5.504569083452225, 5.26009014248848, 5.319926692889287, 5.385282527941924, 5.599658152231803, 5.36720948952895, 5.453367898097405, 5.455033941910817, 5.308210535691335, 5.286373590047543, 5.3065729806056385, 5.397560919706638, 5.307155350079904, 5.3971275343344765, 5.185842697436993, 5.050703722697038, 5.058015678937618, 4.976388841867447, 5.0836995129401865, 5.214145538898615, 5.304637388541148, 5.074234549815838, 5.0137989566876335, 5.319501042366028, 4.860713284749251, 4.992305055260658, 5.2734454938998585, 5.1208721330532665, 5.173864160592739, 5.027414640555015, 4.913191444598711, 5.06825225857588, 5.181587244455631, 5.097243629969084, 5.096704824612691, 5.027645647525787, 4.877159953117371, 5.168534884086022, 5.22917147553884, 5.3152487598932705, 4.866273638147574, 5.110854864120483, 5.07129892706871, 5.243817964425454, 4.848132354708818, 4.874600632832601, 4.9434843842799845, 5.217850788281514, 4.912131816148758, 5.062732978509023, 4.808416444521684, 4.875867774853339, 4.665233820676804, 4.8570808332699995, 4.600398130141771, 5.030903506737489, 4.8066083169900455, 5.117848057013291, 4.97056682407856, 4.940698953775259, 4.874651233737286, 4.85327907708975, 4.713361934973643, 5.118734756341348, 4.9677789612458305, 4.811035868067008, 4.704518911930231, 4.957464748850236, 4.810850906830567, 4.913213272507374, 4.983556208702234, 5.034960545026339, 4.8510697552791004, 4.56308110172932, 4.821744666649745, 5.047108253607383, 4.731816808764751, 4.680791980945147, 4.903345482853743, 4.716465132740828, 4.772301006775636, 4.846810411948424, 4.580809001739208, 4.949744098461592, 4.77498213832195, 4.870558972542103, 4.832826201732342, 4.865839143212025, 4.827695412131456, 4.87746309546324, 4.709893133777839, 4.726962824280445, 4.83074054809717, 4.688138676377443, 4.641922143789438, 4.70550908950659, 4.642210114460725, 4.79526909498068, 4.597998473506707, 4.674794427477396, 5.105953374734292, 4.782672172555556, 4.693597938005741, 4.494914293289185, 4.770528840330931, 4.754457814189104, 4.794613666259325, 4.771176239618888, 4.737704857037618, 4.696291834115982, 4.724282609728666, 4.88952562212944, 4.557290533414254, 4.52879570883054, 4.450160848406645, 4.789224883684745, 4.544748293665739, 4.616044943149273, 4.963150991843297, 5.190862456193337, 4.544032651644486, 5.0443135511416655, 4.564219602025473, 4.499946871629128, 4.753313642281753, 4.5815571420467815, 4.547369898511813, 4.649067156589949, 4.7715102686331825, 4.690933663111466, 4.842579935605709, 4.837518086800208, 4.734159194506132, 4.900970943845236, 4.973998356323976, 4.53065622655245, 4.920983167795034, 4.795963414586508, 4.549086519158804, 4.867261565648592, 4.646038309885905, 4.801707730843471, 4.757646295886773, 4.9802767886565285, 4.899434642149852, 4.774822707359608, 4.79329930131252, 4.809263136524421, 4.34386069728778, 5.116458097329507, 4.632679249231632, 4.434309314076717, 4.847955641838221, 4.6600800672402745, 4.940259154026325, 4.539935243817476, 4.95419440131921, 4.753962527100857, 4.660302616082705, 4.714279092275179, 4.6253308688218775, 4.866535308269354, 4.5053655459330635, 4.8279645798298025, 4.753740092882743, 4.694273004165063, 4.778749963411918, 4.857512287222422, 4.882700612911811, 4.875316484616353, 4.86003006192354, 4.646732830084288, 4.538750355060284, 4.990253407221574, 4.751980089224302, 4.734577229389777, 4.437072623234529, 4.698659313412813, 4.8263488962100105, 4.5390784087089395, 4.990538413708027, 4.905362436404595, 4.75289449095726, 4.713802861479612, 4.594160786041846], 'val_acc': [0.0163854482368781, 0.902073296216818, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.9033700273587153, 0.9033862077272855, 0.903388500213623, 0.9033699883864477, 0.9033006406747378, 0.9033838579287896, 0.9033191525019132, 0.9032590893598703, 0.9031434907363012, 0.9033399568154261, 0.9029701214570266, 0.9028453345482166, 0.9032891209308918, 0.9024084623043354, 0.9027944986636822, 0.9028314466659839, 0.9032220863378965, 0.9025702614050645, 0.9026188185581794, 0.9010285574656266, 0.9016780967895801, 0.9026950620687925, 0.903081096135653, 0.9022628527421218, 0.9020155301460853, 0.9002958627847525, 0.900573214659324, 0.8933755778349363, 0.8936829406481522, 0.8950189627133883, 0.8998520786945636, 0.8999006198002741, 0.9015370699075552, 0.8994476038676041, 0.8969697906420782, 0.9015278266026423, 0.8977116965330564, 0.8974990890576289, 0.8982595411630777, 0.8956152773820437, 0.8993366291889777, 0.8996463624330667, 0.9010909520662748, 0.901627221932778, 0.9020317311470325, 0.902678879407736, 0.9019670073802655, 0.8982318020783938, 0.8914778920320364, 0.894998135475012, 0.8971015100295727, 0.8967108909900372, 0.8968749756996448, 0.8973881487662976, 0.8953471848597894, 0.8974505617068365, 0.8975799909004798, 0.8986524549814371, 0.8990407815346351, 0.8991239987886869, 0.9008182424765366, 0.9019531378379235, 0.9123197014515216, 0.9145686695208917, 0.9119221797356238, 0.9113443241669581, 0.914226587002094, 0.9088965608523443, 0.9134384141518519, 0.911952209014159, 0.9128674956468436, 0.9058639934429755, 0.9138174974001371, 0.9163045218357673, 0.9132003738329961, 0.9164802386210515, 0.9167206012285672, 0.9122780928244958, 0.9124213892679948, 0.9182692124293401, 0.9167552544520452, 0.9204164858047779, 0.9205367244206942, 0.9206568782146161, 0.9182831071890317, 0.9169887212606577, 0.9189972946277032, 0.9206985303988824, 0.9196422008367685, 0.9219143023857703, 0.9231139031740335, 0.9236570573770083, 0.92084873181123, 0.9216854618145869, 0.9209897128435282, 0.9223303221739255, 0.9228758353453416, 0.9226239186066848, 0.9208464255699744, 0.9229243741585658, 0.923342750622676, 0.9228527431304638, 0.9226909119349259, 0.9218079791619227, 0.9200628422773801, 0.9233034207270696, 0.9240846633911133, 0.9245099998437442, 0.9221338927745819, 0.9226285333816822, 0.9246139801465548, 0.9210359889727372, 0.9237703337119176, 0.9228249559035668, 0.9246509717060969, 0.9255062135366293, 0.923717187001155, 0.9253628964607532, 0.9253698128920335, 0.9240222710829514, 0.9249144517458402, 0.9237749828742101, 0.925577856027163, 0.9246255594950455, 0.9260285588411185, 0.9255131276754233, 0.9264654104526227, 0.9262758653897506, 0.9255778514421903, 0.9218472425754254, 0.9261996172941648, 0.9268074838014749, 0.9253282088499802, 0.9242187646719126, 0.924184072476167, 0.9252057235974532, 0.9234374967905191, 0.9234975736874801, 0.9274523854255676, 0.9248335774128253, 0.9231924712657928, 0.9267289271721473, 0.9266225879008954, 0.9259245854157668, 0.9275864385641538, 0.9242395345981305, 0.925970767553036, 0.925552425476221, 0.9286311796078315, 0.9264029998045701, 0.9252657867394961, 0.925062392766659, 0.9270918231744033, 0.9233658497150128, 0.92525426699565, 0.9268837800392737, 0.9214982436253474, 0.9266526630291572, 0.926465398990191, 0.9269230526227218, 0.9250485667815576, 0.9245677429896134, 0.9256841861284696, 0.9228296325756953, 0.9254530232686263, 0.9252218718712146, 0.9254923210694239, 0.9259707858929267, 0.9272512839390681, 0.9265602093476516, 0.924505366728856, 0.9253883040868319, 0.9247550070285797, 0.9274916786413926, 0.9244730266240927, 0.9254276293974656, 0.9258552285341116, 0.9261556749160473, 0.9256679644951453, 0.9250994141285236, 0.9285364426099337, 0.9260378159009494, 0.9260586110445169, 0.9251224971734561, 0.927325251010748, 0.9274431444131411, 0.9249722728362451, 0.9271126206104572, 0.9263822367558112, 0.9218125847669748, 0.9272951965148633, 0.9246417490335611, 0.9239044143603399, 0.9249329544030703, 0.9262643020886642, 0.9238974681267371, 0.9265555601853591, 0.9236663213142982, 0.9267381865244645, 0.9261441574646876, 0.9247550047360934, 0.9259615311255822, 0.926215818295112, 0.925235771215879, 0.9255847999682794, 0.9242857878024762, 0.9240107329992148, 0.9268167592011965, 0.9259569346904755, 0.9260540237793555, 0.927223570071734, 0.9254414783074305, 0.9258644764239972, 0.9227487009305221, 0.9245053552664243, 0.9236339743320758, 0.9245677911318265, 0.9261117715101975, 0.924105506676894, 0.9250069627395043, 0.9272328156691331, 0.9248705758498266, 0.9276511783783252, 0.9246463661010449, 0.9258621243330148, 0.9249745928324186, 0.9243897910301502, 0.9246648366634662, 0.9264815724813021, 0.9271657948310559, 0.9258922132161947, 0.9254715488507197, 0.9252265049861028, 0.9276973742705125, 0.9260031672624441, 0.9268606626070462, 0.9252657982019278, 0.9262412259211907, 0.9243042996296515, 0.926331334389173, 0.9260401519445273, 0.9260123899349799, 0.9255246818065643, 0.9257142131145184, 0.9279631880613474, 0.9271056766693409, 0.9269531346284426, 0.9258875915637383, 0.9273506655142858, 0.9270386466613183, 0.9254622505261347, 0.9264307595216311, 0.9262157884927896, 0.9268213946085709], 'val_mDice': [0.010411134240432428, 0.010697547310533432, 0.009494822167863067, 0.009008847374039201, 0.008339306214251198, 0.007949068246839138, 0.007707283259011232, 0.007616316236985417, 0.004628160932602791, 0.003046535542055678, 0.003672002125173234, 0.005718446838167997, 0.013640231273781795, 0.0179716946485524, 0.027304573324867167, 0.03049485462431151, 0.034286909647142656, 0.03984303077539572, 0.050256794031996, 0.04707657653265274, 0.046713349982523, 0.05061581691440482, 0.04718757019593166, 0.050207396467717796, 0.05878452007443859, 0.06096058346044559, 0.05877736425743653, 0.05625844238182673, 0.057061575568066195, 0.06317299962616883, 0.06417936068744613, 0.06107378779695584, 0.06452705811422604, 0.06640654888290626, 0.07131465467122886, 0.06853559527259606, 0.06426328391982959, 0.06283498383485354, 0.07521064579486847, 0.07808325497003701, 0.07439041381271985, 0.07582006933024296, 0.07835027403556384, 0.08377026107448798, 0.08921111475389737, 0.08275046457464878, 0.09137133537576748, 0.08626217781924285, 0.08115820830258039, 0.09120151515190418, 0.08730364977740325, 0.09154383451319657, 0.09504895994009879, 0.10136869349158727, 0.10303888670527019, 0.09529544140856999, 0.10320090502500534, 0.1079954936240728, 0.10099092011268322, 0.1135620460487329, 0.11092224029394296, 0.1060065647157339, 0.10239600017666817, 0.1101320139490641, 0.10851823388097379, 0.11514599750248286, 0.11201695152200185, 0.10848268479681931, 0.10092827025800943, 0.11132342253740017, 0.11225484641125569, 0.12079971598891112, 0.11623811607177441, 0.12548363352051148, 0.12562610839421934, 0.12914370172298872, 0.12271166248963429, 0.1288451085296961, 0.12284294286599526, 0.12545841058286336, 0.13427227993424123, 0.12224555717637905, 0.12375942753771177, 0.13648099753145987, 0.12770323956815097, 0.1395117395485823, 0.13313236846946752, 0.13194863311946392, 0.1212447639554739, 0.12439603802676384, 0.12627000411829123, 0.11694829982633774, 0.12172375669559607, 0.1380587391412029, 0.14697173094520202, 0.14807015299223936, 0.1514203521924523, 0.14841749848654637, 0.157687122718646, 0.15480779827787325, 0.16200723456075558, 0.15302522924657053, 0.16099500111662424, 0.15613702584344608, 0.16739363251970366, 0.16395852170311487, 0.16903077610410178, 0.17510290243304694, 0.18656964757694647, 0.20047770612514937, 0.20381260720583108, 0.2135431130345051, 0.20723209157586098, 0.21454753898657286, 0.21825034314623246, 0.22156042132813197, 0.22336614619080836, 0.2237508878684961, 0.22974060289561749, 0.22950801640175855, 0.23549527015823585, 0.23326924013403746, 0.2401937865293943, 0.23778796926713908, 0.24183691164048818, 0.25024345287909877, 0.24928578963646522, 0.2500042376609949, 0.2500333341841514, 0.2572358396763985, 0.2632753410591529, 0.25919480277941775, 0.26570044773129314, 0.2646725197824148, 0.26911381632089615, 0.26852427867169565, 0.27655794442846227, 0.2783393458678172, 0.2694477100784962, 0.27994359829104865, 0.2691052212164952, 0.27313492146249, 0.28055275747409236, 0.28139959247066426, 0.2754812077260934, 0.2885335116432263, 0.2872912943936311, 0.28690670946469676, 0.2957378290593624, 0.2958519412921025, 0.2829069122672081, 0.3082753557425279, 0.2928459311907108, 0.29853673393909747, 0.2952957571699069, 0.30022054624099, 0.30707431441316235, 0.2891972821492415, 0.30051845140182054, 0.3072778863402513, 0.31559969284213507, 0.31721963504186046, 0.3080575575049107, 0.32141692764483965, 0.3183710598028623, 0.31622611845915133, 0.31921704887197566, 0.31905822398570866, 0.32105089294222683, 0.3263580065507155, 0.3206672238615843, 0.3225120145540971, 0.32133041522823846, 0.32612932186860305, 0.3298477375736603, 0.3114554589757553, 0.32681810741241163, 0.3342027182762439, 0.3196784303738521, 0.3177271938094726, 0.32225984879411185, 0.32983488131027955, 0.31409489191495454, 0.32444739886201346, 0.3333518353219216, 0.32192618399858475, 0.31681182235479355, 0.3287899113045289, 0.34195147168177825, 0.33577548970396703, 0.32537968972554576, 0.32781784465679753, 0.3310364390221926, 0.32343415858653873, 0.3348855490867908, 0.33171573825753653, 0.33348551478523475, 0.3386924473138956, 0.33509655640675473, 0.32950852008966297, 0.33673308703761834, 0.33603413145129496, 0.3267998635195769, 0.3406515462467304, 0.3342718126682135, 0.3385702996299817, 0.340936123751677, 0.3270021419112499, 0.3382779276714875, 0.3364371597193755, 0.33998909764564955, 0.33699755714489865, 0.34172560933690804, 0.3385441778944089, 0.3424224349168631, 0.33293949554745966, 0.33717908595617, 0.3461407861457421, 0.34841247447408163, 0.3402603721389404, 0.3417417338261238, 0.34024249103206855, 0.3375820700938885, 0.3441605545007266, 0.347996728351483, 0.3461844571507894, 0.3480109517964033, 0.351778333576826, 0.35363362815517646, 0.354660647133222, 0.3456995246502069, 0.35224935240470445, 0.34141165906420123, 0.34393866188251054, 0.3377688584419397, 0.33443824316446596, 0.34193103359295773, 0.3484750441633738, 0.3473365017427848, 0.3441893750658402, 0.35495103293886554, 0.34635441205822504, 0.35165870734132254, 0.3422173232986377, 0.3481705721754294, 0.3460105104515186, 0.33517957822634625, 0.3508938258657089, 0.3464185675749412, 0.33806126937270164, 0.3599827513098717, 0.34683583906063664, 0.3490708086353082, 0.36041749784579646, 0.3419181432288427, 0.36063680052757263, 0.3527725293086125, 0.3554861003962847, 0.34047632148632634, 0.33641968868099725, 0.3489355407655239, 0.3384364516689227, 0.34682545724969643, 0.36024902990231145, 0.33897914909399474, 0.3472043559528314, 0.3610091862770227, 0.34173828478042895, 0.3607635208620475, 0.3376602576329158, 0.35564081600079167, 0.35095601643507296, 0.3466404859836285, 0.3521315131623011, 0.3518092093559412, 0.3620891129741302, 0.3538234471701659, 0.36089997738599777, 0.3519273463350076, 0.35095134721352506, 0.35512473720770615, 0.3566602843885238, 0.3529355778143956, 0.35400522958773833, 0.34570108640652436, 0.35297735780477524, 0.3639759627672342, 0.35757812599723154, 0.3461723579810216, 0.35110131249977994, 0.35581693740991444, 0.36251385481311726, 0.3572882250524484, 0.3526598822612029, 0.36303021328953594, 0.3527701800832382, 0.3503059693253957, 0.35416345498882806, 0.35092527247392213, 0.36198264417740017], 'loss': [315.97162534380755, 108.37893206737012, 57.049467838614184, 38.24589369167956, 29.227163818174322, 24.33023477227312, 21.218687493654016, 19.229411512000375, 17.61648408269174, 16.04092353438909, 14.691896326854353, 13.801364804088195, 13.074956033388803, 12.423204350007545, 11.844749341973692, 11.349109363679043, 10.949107511802364, 10.63239880469214, 10.349263576193401, 10.095794471484604, 9.890581740781542, 9.698848754621064, 9.53235203918799, 9.386019505797458, 9.245207054659216, 9.12675051219479, 9.003115235577317, 8.882946920232184, 8.802170715527287, 8.707084062518772, 8.604629285615012, 8.5250556094064, 8.430340899869677, 8.367630599708912, 8.282222048290583, 8.197611630430172, 8.11814438618385, 8.066148072582852, 7.973881341473683, 7.894427279248335, 7.809538554872609, 7.736625732052296, 7.683947709459204, 7.578928337450739, 7.499500659870329, 7.4324790815316275, 7.374382222632162, 7.318437019177692, 7.262489989423273, 7.236512188474752, 7.16653863800789, 7.113897100559605, 7.053320933562478, 7.0130840257932965, 7.001098045616322, 6.921874136323585, 6.886877118491229, 6.841042568419105, 6.795576798589337, 6.762012616389463, 6.723820674928431, 6.686446888095216, 6.684038657802591, 6.655708832240771, 6.601424249217023, 6.585923550523186, 6.539918908969091, 6.526558802772754, 6.494730829851748, 6.457850473542544, 6.439989823295898, 6.400842283392793, 6.370953575025392, 6.303263407642745, 6.239565916236428, 6.175139843994298, 6.139290462282985, 6.107871933835663, 6.075157323424957, 6.041764268045726, 6.019407791085627, 5.988259614882715, 5.978406668522916, 5.9545455998537955, 5.921170673191817, 5.880490031308775, 5.867656305620594, 5.835651913395472, 5.822910409566057, 5.797448318008786, 5.780425253713042, 5.7528478518344155, 5.749034157716666, 5.712873489777782, 5.695782958279344, 5.659053005987919, 5.616276800132244, 5.589626800966672, 5.559013601335819, 5.533437979020151, 5.51656216905751, 5.494580455122317, 5.4709564820213705, 5.446725927946764, 5.4318880144107125, 5.4119951509521265, 5.394627519587491, 5.35283399732045, 5.307315371677047, 5.235344199537076, 5.094115908401188, 5.006069055553909, 4.921176223641424, 4.883610762299113, 4.825032346727751, 4.7997250740327795, 4.760289298139089, 4.743454112938868, 4.714478632823948, 4.689593577884082, 4.664628923252633, 4.652938097041988, 4.616961926208105, 4.6043616830450596, 4.595377147610003, 4.5601347075455925, 4.5399683791861305, 4.527273145481019, 4.520020172075516, 4.503743238371258, 4.490788181736429, 4.482182243681038, 4.475918759326043, 4.43428894581417, 4.414164242814368, 4.43892973140851, 4.393329244560172, 4.396558902343458, 4.375186309992118, 4.370031152034965, 4.361421622155195, 4.365819149732832, 4.33559015638701, 4.317865966123419, 4.328305721722724, 4.307289882833356, 4.301020686525068, 4.291175093079564, 4.2677981200434125, 4.256077707478957, 4.238746529431138, 4.216382866062758, 4.199888986623409, 4.19489505798254, 4.184369707628665, 4.169480964397137, 4.15367900174141, 4.140646774544769, 4.150077581537569, 4.129972046276723, 4.126895745104226, 4.1171022242509405, 4.113501216018925, 4.093593779782643, 4.089922132971849, 4.094325320610512, 4.077697424192463, 4.080288777940225, 4.072918462388291, 4.070469317044387, 4.061674625101998, 4.046761193179143, 4.036740444116154, 4.029636563294314, 4.050818445892953, 4.030088381673603, 4.025249932037227, 4.007123272489118, 4.014107490623722, 4.016059913319063, 4.012952966212039, 3.9929542049516846, 3.998503909552547, 3.9889624925114004, 3.9922766157767873, 3.978227063528365, 3.970920257239415, 3.979559041956172, 3.9602158395916205, 3.970532959208743, 3.938983067808298, 3.9560659418222177, 3.9330469417822687, 3.940642436623936, 3.936291873845339, 3.9357705148374817, 3.92869538189157, 3.9263666221975213, 3.916375426835793, 3.9187671128293857, 3.9186755619420235, 3.9105333690217616, 3.9085258098256817, 3.9098498051889727, 3.8929160455490712, 3.889591426622007, 3.8859069744159536, 3.8926875902961497, 3.8847787893204346, 3.888302493412029, 3.895365808037147, 3.8675376473675462, 3.8789969654768406, 3.8665110598320256, 3.8635423436415963, 3.864814092144803, 3.8642190589815075, 3.87012258148123, 3.8511384857280286, 3.8502745332065285, 3.834193013750692, 3.8418343365769605, 3.827907303476162, 3.8429732356664714, 3.835507479868812, 3.814564218108179, 3.8173796421023938, 3.8175569905419415, 3.8308744460775834, 3.818147916124501, 3.8064713244903, 3.8153215040747646, 3.820570487504646, 3.8142226979069127, 3.804790411379838, 3.799924184395842, 3.8042975230090765, 3.786194059183201, 3.796585538637305, 3.798913667664409, 3.7928209053923654, 3.778008203582109, 3.7725140671772697, 3.777501153119165, 3.79107485213953, 3.7809137078016697, 3.7754651062890723, 3.7657044553057486, 3.7688507156311384, 3.765592422452809, 3.756290186169424, 3.757119399435041, 3.754643748532821, 3.7431936285366687, 3.7626099335710843, 3.7575248919937936, 3.7354495796805036, 3.7460383372828825, 3.7395558588005438, 3.740658282486042, 3.735898970919958, 3.7467631450231957, 3.7378159684689845, 3.7289000663174194, 3.72802584130397, 3.7348626776066123, 3.7278889423035437, 3.7375514160099255, 3.721309027636193, 3.7241912498296457, 3.721952243692528, 3.714959158719735, 3.7140016642859597, 3.715757298874127, 3.723304807969392, 3.711370652785966, 3.707244577843644, 3.704213711868669, 3.703212299140674, 3.7141449017398505, 3.6947040840740146, 3.688293584870927, 3.692143081618248, 3.690682535867217, 3.6915965071905696, 3.6871891223470694, 3.6890908749706246, 3.689017221473146, 3.6904558641471006, 3.6690404053094103, 3.667017600638152, 3.6735162061375357, 3.6701810122151928, 3.6673869573794025, 3.669384894331014, 3.673898375662097, 3.648607205445285, 3.655996114315169, 3.6468207567124904, 3.6545707650106793], 'acc': [0.03063448417918396, 0.4429330735378992, 0.7816883126693074, 0.8494794609625159, 0.8638317453941494, 0.8693407306061628, 0.8715779723347548, 0.8728581969614653, 0.8734888091338447, 0.8740465533499193, 0.8745468545969283, 0.8746779424440791, 0.8747492187810122, 0.8748089445920313, 0.8748303136899882, 0.8748338828444822, 0.8748359016020734, 0.8748342382240524, 0.8748346821667907, 0.8748348172241105, 0.8748335498544461, 0.8748329963882416, 0.8748323491531512, 0.8748328624303343, 0.8748325741259163, 0.8748323510221523, 0.8748331294281395, 0.8748325106403456, 0.8748312900012065, 0.8748309353637397, 0.8748312883850702, 0.8748229747327351, 0.8748235953730388, 0.8748226450134509, 0.8748202276089133, 0.8748192735717909, 0.87481688090953, 0.87480249229441, 0.8747979010931407, 0.8747935604754491, 0.8747824724063787, 0.8747665317122549, 0.8747293531130592, 0.8747085361620118, 0.8747233681206904, 0.8747051003827365, 0.8746823998963296, 0.8746590770105589, 0.8746141385495592, 0.8745617946828191, 0.8745080107177281, 0.8744403725124248, 0.8743929305024267, 0.8742812134282215, 0.8741628354703721, 0.8740178007410206, 0.8738899912317883, 0.87374657760805, 0.8736969205018923, 0.8735463174684762, 0.8734488834767921, 0.873376854669011, 0.873213622294745, 0.8731293548882849, 0.8730463720572937, 0.8729421773773052, 0.8728297308209658, 0.8727142292338632, 0.8726952100090355, 0.8725581750937474, 0.8724193057593501, 0.8722673125134219, 0.872225457612938, 0.8721117702762238, 0.8721227220841179, 0.8720770517992356, 0.872014248205113, 0.8720288345880376, 0.8720093257398851, 0.8719315515841591, 0.8718730933242076, 0.8718465551793064, 0.8716814819543682, 0.871662948878722, 0.8716629917722979, 0.8716866038310224, 0.8715148335902506, 0.8714686993313354, 0.871333331202247, 0.8712775031710907, 0.8711975865793725, 0.8711588140630331, 0.8709446104824197, 0.8707649253462003, 0.8697544801425398, 0.868756940540084, 0.8681362792320448, 0.8689649328912392, 0.869717502087165, 0.8701079517319822, 0.8708806539124958, 0.8711656429863698, 0.8716309804499924, 0.8721835553937431, 0.8724923797125615, 0.8728405733362705, 0.8727021041055408, 0.8743584143887869, 0.8800202790116248, 0.8839856403390497, 0.8858095916898711, 0.8863023104652712, 0.886410855641172, 0.8869608792279751, 0.8875010887778114, 0.8879995928593011, 0.8885064302836024, 0.8885538302960457, 0.8887855494625947, 0.8888439191263957, 0.8894968765155532, 0.8895107540798003, 0.8898944934333788, 0.8901860668333125, 0.8904074242576114, 0.890955949053931, 0.8912811066175775, 0.8915593799966008, 0.8919404527473238, 0.8923616993694434, 0.8922497403860246, 0.8928973135952138, 0.893145946325944, 0.8940082077752243, 0.8942791854533764, 0.8941151748710174, 0.8946782171830671, 0.8949297958388711, 0.8952249415870919, 0.8952687922289678, 0.8958129225143986, 0.8958890102264307, 0.8960482550043357, 0.8964892509930646, 0.8966268149071335, 0.8968829815924019, 0.89720849960391, 0.8973558329352921, 0.8976829256022382, 0.8977453786212228, 0.8974012826560531, 0.8978274972425839, 0.8978677774821328, 0.8981186656189791, 0.8982041319786683, 0.8982710405863993, 0.898538623878154, 0.8987865696575197, 0.8985811264754815, 0.8988530753965606, 0.8987649369094685, 0.8988823635567381, 0.8990363335791939, 0.8992562548300442, 0.8992206979136531, 0.8992843208613788, 0.8993336700221154, 0.8992155970742559, 0.8995973799794426, 0.8993847496415223, 0.8993593843937756, 0.899814017021512, 0.8996488772601338, 0.8996754610141595, 0.899427448731336, 0.8995896626319171, 0.8997983009868208, 0.8996786517399017, 0.899816502754532, 0.8998561890475107, 0.8998299362770935, 0.8998467421318391, 0.8998812635941204, 0.9002037815949601, 0.8999584525326109, 0.9000913398320589, 0.9002631543681574, 0.9002136021789277, 0.9001585139698794, 0.9001382946044573, 0.9006126986695074, 0.9004385135418527, 0.9007363865310214, 0.9004179599990288, 0.9006907136570228, 0.900576341919581, 0.9007928480546127, 0.9008414221185935, 0.9009185748754212, 0.9009443580098316, 0.9009543339297812, 0.9009688551065547, 0.9010383573161559, 0.9007958432223693, 0.9009735116953781, 0.9012728859651994, 0.9011159735302924, 0.9011554553435538, 0.9012855661868565, 0.9012178185130577, 0.9010946650484397, 0.9013527212333803, 0.9013428545125701, 0.9014711973962447, 0.901564196270242, 0.901521943283512, 0.9015548616430586, 0.9015197250870094, 0.9016931569687304, 0.9017448113609545, 0.9018451045175809, 0.901809568138214, 0.9020610641841683, 0.9017456955359143, 0.9017660914944577, 0.9020848908536957, 0.901968460213618, 0.9020612375889935, 0.9018821951912677, 0.9020736746151246, 0.9022374000648703, 0.9020667803866896, 0.9020414175026797, 0.9019471985832259, 0.902142775308313, 0.902203566928219, 0.9022697893761625, 0.9021741274449149, 0.9021515779957872, 0.9022800067215841, 0.9021529776742403, 0.9024010977379612, 0.9022195068087776, 0.902392073986133, 0.902106818722522, 0.902369479928346, 0.9023391988393313, 0.9024569631106605, 0.9021237348373401, 0.9023889045670036, 0.9023464674451832, 0.9025075761724154, 0.9024856080321119, 0.9024458561756729, 0.9023205762495252, 0.9022591676338676, 0.902465076241163, 0.9023905435435657, 0.9025173544564898, 0.9024811735905636, 0.9024919035315326, 0.9022479064952929, 0.902378017811343, 0.9023759353702868, 0.9024296952142692, 0.9024143792282752, 0.9025291929075245, 0.9024924833616471, 0.9024824586277858, 0.9023572886153955, 0.9024268548382853, 0.902579718884821, 0.9023516348595001, 0.9024504899516884, 0.902392784442935, 0.9023720958043121, 0.9026260516701353, 0.9024302057154359, 0.9024085672944165, 0.9023662034869139, 0.9024042656344042, 0.9025002178984859, 0.9024118266289913, 0.902399234591205, 0.9024906859157776, 0.9025412516808099, 0.902519679301495, 0.9022361102397071, 0.9023522377442931, 0.9024269246784599, 0.9026167862124849, 0.9023838053162573, 0.9024465234145724, 0.9024652104959116, 0.9023196459047405, 0.902428363913772, 0.9023468493095989, 0.9023763786203953, 0.9025678557959538, 0.9024274776224434], 'mDice': [0.015823838904219657, 0.014598441644946466, 0.013550722994405736, 0.012743973831044817, 0.012225096224865867, 0.011670427803246642, 0.011813204918380709, 0.011513796724471139, 0.014025930693265033, 0.018525337997762455, 0.02143404875587963, 0.022795078256432448, 0.02407361157231612, 0.02579830611887524, 0.02792013101509925, 0.03027056586222379, 0.03244712301726206, 0.034218766495419155, 0.03570554684804648, 0.03739123858314241, 0.03876398565416174, 0.040262103397861286, 0.04202214874726407, 0.04370048264166773, 0.04480096933699991, 0.046209057546855885, 0.04744632176578699, 0.04861235098741013, 0.04954607625676842, 0.0505842688748271, 0.05208835321772553, 0.053331011674530686, 0.05468218496963787, 0.0557927965989365, 0.05740898590042595, 0.058766893357420545, 0.06043429357598961, 0.061791552551434974, 0.06379469339174504, 0.06580120786360728, 0.06794968563357281, 0.07014359837499456, 0.07317259281263529, 0.07775657218941505, 0.08214845607009286, 0.08490152561451846, 0.08677488313542074, 0.0893427667318043, 0.09091503999939728, 0.09270747739659853, 0.0946162009903242, 0.09648572958137984, 0.09832995747187959, 0.09949970789707907, 0.10060444433335876, 0.10331128953095348, 0.10596607968913424, 0.1072820012925692, 0.10891028978414974, 0.11087733114528708, 0.11241499515564517, 0.11353749019885474, 0.11398474626599736, 0.11522304866420688, 0.11675446039167242, 0.11762987024670518, 0.11913155654400112, 0.119952861441932, 0.12096869353080252, 0.12247722006438898, 0.12293967354658245, 0.12406323817103791, 0.1239462665973703, 0.12533146971504286, 0.12748340348041684, 0.13047144470960906, 0.13232263293071592, 0.13311985065540594, 0.1346925652347059, 0.13620832069222694, 0.13680829700619535, 0.13786419352476506, 0.13873902486829118, 0.13967179451153242, 0.14168609665827173, 0.14316900070771446, 0.14373955833926189, 0.1454507888042321, 0.14552971439245915, 0.1466828062738713, 0.14755579778643033, 0.1493103744704862, 0.14937841946800773, 0.15093763426315912, 0.15231345489622536, 0.155205682688266, 0.15957866599751522, 0.1631036611607316, 0.1650367858820952, 0.16743421887824822, 0.16905779904471788, 0.17078060310264911, 0.17233502748574783, 0.1745934964688537, 0.17602999719055795, 0.17823880110226312, 0.17932349143296075, 0.18323026661815475, 0.18923248254009403, 0.1963246200279392, 0.20501402933928708, 0.21099011570195986, 0.21603154319244963, 0.21954323074468704, 0.22481007730950953, 0.2279120366269676, 0.23200939965101885, 0.23352711201181361, 0.23657275030371736, 0.23886948597037594, 0.24149108780279135, 0.24296852460203713, 0.24659161325047665, 0.24804390297212647, 0.24940708614380763, 0.25185574568759295, 0.25480774579579835, 0.25732903190889583, 0.25806426240060315, 0.2602809008517015, 0.26357319784771305, 0.26578555101454726, 0.2668514722972122, 0.27208164180929145, 0.2745056178289883, 0.2732635955660807, 0.27745920114606043, 0.2784859463758028, 0.28041349669273713, 0.28163972219659605, 0.2833408529903796, 0.2831353302383669, 0.2854289099248388, 0.2883996958278768, 0.28818116629298496, 0.29050569904034157, 0.29151539328759485, 0.2923157893016111, 0.2963007248619941, 0.29937414905632004, 0.30135312440880635, 0.3039160760568427, 0.30599069483196845, 0.3060313519143667, 0.3085726705642355, 0.30923987314466467, 0.3115516092370909, 0.3132766304086036, 0.3123340416546074, 0.31378156211964814, 0.3144881491741109, 0.3156634287938392, 0.3163476717776614, 0.3185357541233414, 0.3191736138725615, 0.31913572105657323, 0.32059357388206683, 0.3201661339454146, 0.32177477210064737, 0.32144196673555697, 0.3221348889455863, 0.3245458392202882, 0.3252663128349401, 0.3263179497460229, 0.32480179457649977, 0.3265189984556517, 0.3278824582724821, 0.32902594060626494, 0.3290814171800048, 0.32898104655877874, 0.32934242978391093, 0.33039886796649265, 0.3297918687461103, 0.33188047446052843, 0.33141047159904025, 0.33266242238347915, 0.333840421872595, 0.3329946684252454, 0.33470308004484994, 0.33428097545501434, 0.33774520070759734, 0.3354171626431294, 0.3382088695627445, 0.3371729375520536, 0.33794792787093997, 0.3386400300677188, 0.3387892584563027, 0.3395053194582402, 0.3404562787316794, 0.33981853486052915, 0.3405613004309241, 0.3419789699932905, 0.3423203079806893, 0.3413291697180888, 0.3425928548776849, 0.3444030444186453, 0.34457517105044666, 0.3441173480432829, 0.3446786034528283, 0.34482961094908243, 0.3432652494965208, 0.34669836371914436, 0.3459284344821533, 0.3473630037835897, 0.3474952642832758, 0.34782788986420043, 0.3475379815903535, 0.34663507854939607, 0.348420494202812, 0.3498301768719595, 0.35070182275455464, 0.3505347811361086, 0.35263566925529405, 0.3501540353973666, 0.35081555913330875, 0.35337868574183884, 0.3532838977142903, 0.35420976563110396, 0.3519284545215395, 0.35354626749336504, 0.35459435112821786, 0.3545718117854389, 0.353353251478477, 0.353404550460897, 0.3558235303135054, 0.3560181438296217, 0.35559137712366673, 0.35805444197165265, 0.3563079017450765, 0.3568869698595145, 0.3569834726296326, 0.3594970516169917, 0.3588399258798244, 0.3596089729455348, 0.3585173544447921, 0.3590123059536757, 0.3599217307338919, 0.3603650687706819, 0.3592833226472263, 0.3609405404102271, 0.36206847903671974, 0.3619232060894319, 0.3629803534644914, 0.36392156766435707, 0.36255230701773367, 0.36180673898734106, 0.36476394042673665, 0.3635565648409402, 0.3643125925300021, 0.364388183049255, 0.365159736123899, 0.363814411389919, 0.36475761864311324, 0.36608346188507057, 0.3659857093551266, 0.3652369014085312, 0.3665026999906257, 0.3649114429082178, 0.36633465187934167, 0.3664609236788906, 0.3664259466668243, 0.3676645542179473, 0.36777682278491164, 0.36794279323208656, 0.36747029432783834, 0.36784314915206634, 0.36921047604217044, 0.36911993867564025, 0.369225682507447, 0.3688214868293371, 0.3704834688108895, 0.37089780873174016, 0.3702113257335757, 0.3711279099452411, 0.37087674680137, 0.3713802565763569, 0.3715664226334, 0.37101434860393967, 0.37150072823049707, 0.37389802867049365, 0.3745331368047923, 0.37356815976728475, 0.3734180587338289, 0.37387272685059847, 0.3737818233165136, 0.37442979087669515, 0.37656148460082195, 0.3754923815443805, 0.37690194314348296, 0.3761172066543591]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.37s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.14s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.91s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:31,  1.38s/it]predicting train subjects:   1%|          | 2/285 [00:03<06:53,  1.46s/it]predicting train subjects:   1%|          | 3/285 [00:04<06:49,  1.45s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:14,  1.55s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<06:54,  1.48s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:20,  1.58s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<07:45,  1.68s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<07:53,  1.71s/it]predicting train subjects:   3%|▎         | 9/285 [00:14<07:40,  1.67s/it]predicting train subjects:   4%|▎         | 10/285 [00:16<08:00,  1.75s/it]predicting train subjects:   4%|▍         | 11/285 [00:18<08:09,  1.79s/it]predicting train subjects:   4%|▍         | 12/285 [00:20<08:12,  1.81s/it]predicting train subjects:   5%|▍         | 13/285 [00:22<08:19,  1.84s/it]predicting train subjects:   5%|▍         | 14/285 [00:24<08:21,  1.85s/it]predicting train subjects:   5%|▌         | 15/285 [00:25<08:17,  1.84s/it]predicting train subjects:   6%|▌         | 16/285 [00:27<08:23,  1.87s/it]predicting train subjects:   6%|▌         | 17/285 [00:29<08:20,  1.87s/it]predicting train subjects:   6%|▋         | 18/285 [00:31<08:22,  1.88s/it]predicting train subjects:   7%|▋         | 19/285 [00:33<08:17,  1.87s/it]predicting train subjects:   7%|▋         | 20/285 [00:35<08:27,  1.92s/it]predicting train subjects:   7%|▋         | 21/285 [00:37<08:26,  1.92s/it]predicting train subjects:   8%|▊         | 22/285 [00:39<08:18,  1.89s/it]predicting train subjects:   8%|▊         | 23/285 [00:41<08:16,  1.89s/it]predicting train subjects:   8%|▊         | 24/285 [00:42<08:09,  1.88s/it]predicting train subjects:   9%|▉         | 25/285 [00:44<08:05,  1.87s/it]predicting train subjects:   9%|▉         | 26/285 [00:46<08:02,  1.86s/it]predicting train subjects:   9%|▉         | 27/285 [00:48<07:55,  1.84s/it]predicting train subjects:  10%|▉         | 28/285 [00:50<07:41,  1.80s/it]predicting train subjects:  10%|█         | 29/285 [00:51<07:34,  1.77s/it]predicting train subjects:  11%|█         | 30/285 [00:53<07:32,  1.77s/it]predicting train subjects:  11%|█         | 31/285 [00:55<07:28,  1.77s/it]predicting train subjects:  11%|█         | 32/285 [00:57<07:23,  1.75s/it]predicting train subjects:  12%|█▏        | 33/285 [00:58<07:21,  1.75s/it]predicting train subjects:  12%|█▏        | 34/285 [01:00<07:17,  1.74s/it]predicting train subjects:  12%|█▏        | 35/285 [01:02<07:15,  1.74s/it]predicting train subjects:  13%|█▎        | 36/285 [01:03<07:07,  1.72s/it]predicting train subjects:  13%|█▎        | 37/285 [01:05<07:06,  1.72s/it]predicting train subjects:  13%|█▎        | 38/285 [01:07<07:08,  1.74s/it]predicting train subjects:  14%|█▎        | 39/285 [01:09<07:08,  1.74s/it]predicting train subjects:  14%|█▍        | 40/285 [01:11<07:07,  1.75s/it]predicting train subjects:  14%|█▍        | 41/285 [01:12<07:13,  1.78s/it]predicting train subjects:  15%|█▍        | 42/285 [01:14<07:13,  1.79s/it]predicting train subjects:  15%|█▌        | 43/285 [01:16<07:06,  1.76s/it]predicting train subjects:  15%|█▌        | 44/285 [01:18<07:02,  1.75s/it]predicting train subjects:  16%|█▌        | 45/285 [01:19<06:59,  1.75s/it]predicting train subjects:  16%|█▌        | 46/285 [01:21<06:41,  1.68s/it]predicting train subjects:  16%|█▋        | 47/285 [01:22<06:22,  1.61s/it]predicting train subjects:  17%|█▋        | 48/285 [01:24<06:20,  1.60s/it]predicting train subjects:  17%|█▋        | 49/285 [01:25<06:16,  1.59s/it]predicting train subjects:  18%|█▊        | 50/285 [01:27<06:15,  1.60s/it]predicting train subjects:  18%|█▊        | 51/285 [01:29<06:08,  1.58s/it]predicting train subjects:  18%|█▊        | 52/285 [01:30<06:00,  1.55s/it]predicting train subjects:  19%|█▊        | 53/285 [01:32<05:51,  1.52s/it]predicting train subjects:  19%|█▉        | 54/285 [01:33<05:50,  1.52s/it]predicting train subjects:  19%|█▉        | 55/285 [01:35<05:51,  1.53s/it]predicting train subjects:  20%|█▉        | 56/285 [01:36<05:51,  1.54s/it]predicting train subjects:  20%|██        | 57/285 [01:38<05:44,  1.51s/it]predicting train subjects:  20%|██        | 58/285 [01:39<05:48,  1.54s/it]predicting train subjects:  21%|██        | 59/285 [01:41<05:41,  1.51s/it]predicting train subjects:  21%|██        | 60/285 [01:42<05:40,  1.51s/it]predicting train subjects:  21%|██▏       | 61/285 [01:44<05:42,  1.53s/it]predicting train subjects:  22%|██▏       | 62/285 [01:45<05:38,  1.52s/it]predicting train subjects:  22%|██▏       | 63/285 [01:47<05:38,  1.52s/it]predicting train subjects:  22%|██▏       | 64/285 [01:48<05:40,  1.54s/it]predicting train subjects:  23%|██▎       | 65/285 [01:50<05:49,  1.59s/it]predicting train subjects:  23%|██▎       | 66/285 [01:52<05:57,  1.63s/it]predicting train subjects:  24%|██▎       | 67/285 [01:53<05:53,  1.62s/it]predicting train subjects:  24%|██▍       | 68/285 [01:55<05:45,  1.59s/it]predicting train subjects:  24%|██▍       | 69/285 [01:56<05:44,  1.60s/it]predicting train subjects:  25%|██▍       | 70/285 [01:58<05:45,  1.61s/it]predicting train subjects:  25%|██▍       | 71/285 [02:00<05:38,  1.58s/it]predicting train subjects:  25%|██▌       | 72/285 [02:01<05:39,  1.59s/it]predicting train subjects:  26%|██▌       | 73/285 [02:03<05:33,  1.57s/it]predicting train subjects:  26%|██▌       | 74/285 [02:04<05:32,  1.58s/it]predicting train subjects:  26%|██▋       | 75/285 [02:06<05:27,  1.56s/it]predicting train subjects:  27%|██▋       | 76/285 [02:07<05:25,  1.56s/it]predicting train subjects:  27%|██▋       | 77/285 [02:09<05:21,  1.55s/it]predicting train subjects:  27%|██▋       | 78/285 [02:11<05:21,  1.55s/it]predicting train subjects:  28%|██▊       | 79/285 [02:12<05:17,  1.54s/it]predicting train subjects:  28%|██▊       | 80/285 [02:14<05:11,  1.52s/it]predicting train subjects:  28%|██▊       | 81/285 [02:15<05:12,  1.53s/it]predicting train subjects:  29%|██▉       | 82/285 [02:17<05:12,  1.54s/it]predicting train subjects:  29%|██▉       | 83/285 [02:18<05:08,  1.53s/it]predicting train subjects:  29%|██▉       | 84/285 [02:20<05:04,  1.51s/it]predicting train subjects:  30%|██▉       | 85/285 [02:21<05:15,  1.58s/it]predicting train subjects:  30%|███       | 86/285 [02:23<05:30,  1.66s/it]predicting train subjects:  31%|███       | 87/285 [02:25<05:29,  1.66s/it]predicting train subjects:  31%|███       | 88/285 [02:27<05:28,  1.67s/it]predicting train subjects:  31%|███       | 89/285 [02:28<05:28,  1.67s/it]predicting train subjects:  32%|███▏      | 90/285 [02:30<05:28,  1.69s/it]predicting train subjects:  32%|███▏      | 91/285 [02:32<05:27,  1.69s/it]predicting train subjects:  32%|███▏      | 92/285 [02:33<05:26,  1.69s/it]predicting train subjects:  33%|███▎      | 93/285 [02:35<05:27,  1.71s/it]predicting train subjects:  33%|███▎      | 94/285 [02:37<05:28,  1.72s/it]predicting train subjects:  33%|███▎      | 95/285 [02:39<05:25,  1.71s/it]predicting train subjects:  34%|███▎      | 96/285 [02:40<05:21,  1.70s/it]predicting train subjects:  34%|███▍      | 97/285 [02:42<05:23,  1.72s/it]predicting train subjects:  34%|███▍      | 98/285 [02:44<05:22,  1.73s/it]predicting train subjects:  35%|███▍      | 99/285 [02:45<05:23,  1.74s/it]predicting train subjects:  35%|███▌      | 100/285 [02:47<05:21,  1.74s/it]predicting train subjects:  35%|███▌      | 101/285 [02:49<05:23,  1.76s/it]predicting train subjects:  36%|███▌      | 102/285 [02:51<05:19,  1.74s/it]predicting train subjects:  36%|███▌      | 103/285 [02:52<05:18,  1.75s/it]predicting train subjects:  36%|███▋      | 104/285 [02:54<05:15,  1.74s/it]predicting train subjects:  37%|███▋      | 105/285 [02:56<05:08,  1.71s/it]predicting train subjects:  37%|███▋      | 106/285 [02:58<05:12,  1.74s/it]predicting train subjects:  38%|███▊      | 107/285 [02:59<05:09,  1.74s/it]predicting train subjects:  38%|███▊      | 108/285 [03:01<05:04,  1.72s/it]predicting train subjects:  38%|███▊      | 109/285 [03:03<04:58,  1.69s/it]predicting train subjects:  39%|███▊      | 110/285 [03:04<05:01,  1.72s/it]predicting train subjects:  39%|███▉      | 111/285 [03:06<04:59,  1.72s/it]predicting train subjects:  39%|███▉      | 112/285 [03:08<04:57,  1.72s/it]predicting train subjects:  40%|███▉      | 113/285 [03:10<04:53,  1.71s/it]predicting train subjects:  40%|████      | 114/285 [03:11<04:51,  1.71s/it]predicting train subjects:  40%|████      | 115/285 [03:13<04:52,  1.72s/it]predicting train subjects:  41%|████      | 116/285 [03:15<04:49,  1.71s/it]predicting train subjects:  41%|████      | 117/285 [03:16<04:46,  1.70s/it]predicting train subjects:  41%|████▏     | 118/285 [03:18<04:46,  1.71s/it]predicting train subjects:  42%|████▏     | 119/285 [03:20<04:44,  1.71s/it]predicting train subjects:  42%|████▏     | 120/285 [03:22<04:47,  1.74s/it]predicting train subjects:  42%|████▏     | 121/285 [03:23<04:39,  1.70s/it]predicting train subjects:  43%|████▎     | 122/285 [03:25<04:24,  1.62s/it]predicting train subjects:  43%|████▎     | 123/285 [03:26<04:14,  1.57s/it]predicting train subjects:  44%|████▎     | 124/285 [03:28<04:09,  1.55s/it]predicting train subjects:  44%|████▍     | 125/285 [03:29<04:09,  1.56s/it]predicting train subjects:  44%|████▍     | 126/285 [03:31<04:05,  1.54s/it]predicting train subjects:  45%|████▍     | 127/285 [03:32<04:03,  1.54s/it]predicting train subjects:  45%|████▍     | 128/285 [03:34<04:05,  1.56s/it]predicting train subjects:  45%|████▌     | 129/285 [03:36<04:04,  1.56s/it]predicting train subjects:  46%|████▌     | 130/285 [03:37<04:02,  1.57s/it]predicting train subjects:  46%|████▌     | 131/285 [03:39<03:58,  1.55s/it]predicting train subjects:  46%|████▋     | 132/285 [03:40<03:56,  1.55s/it]predicting train subjects:  47%|████▋     | 133/285 [03:42<03:55,  1.55s/it]predicting train subjects:  47%|████▋     | 134/285 [03:43<03:53,  1.55s/it]predicting train subjects:  47%|████▋     | 135/285 [03:45<03:52,  1.55s/it]predicting train subjects:  48%|████▊     | 136/285 [03:46<03:53,  1.57s/it]predicting train subjects:  48%|████▊     | 137/285 [03:48<03:54,  1.58s/it]predicting train subjects:  48%|████▊     | 138/285 [03:50<03:55,  1.60s/it]predicting train subjects:  49%|████▉     | 139/285 [03:51<03:49,  1.58s/it]predicting train subjects:  49%|████▉     | 140/285 [03:53<03:42,  1.54s/it]predicting train subjects:  49%|████▉     | 141/285 [03:54<03:42,  1.54s/it]predicting train subjects:  50%|████▉     | 142/285 [03:56<03:36,  1.51s/it]predicting train subjects:  50%|█████     | 143/285 [03:57<03:38,  1.54s/it]predicting train subjects:  51%|█████     | 144/285 [03:59<03:32,  1.51s/it]predicting train subjects:  51%|█████     | 145/285 [04:00<03:28,  1.49s/it]predicting train subjects:  51%|█████     | 146/285 [04:02<03:23,  1.46s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:03<03:19,  1.45s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:04<03:14,  1.42s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:06<03:08,  1.39s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:07<03:05,  1.37s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:08<03:02,  1.36s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:10<03:01,  1.36s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:11<03:01,  1.37s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:12<03:00,  1.38s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:14<02:59,  1.38s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:15<02:56,  1.36s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:17<02:56,  1.38s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:18<02:59,  1.41s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:19<02:58,  1.42s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:21<02:55,  1.40s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:22<02:53,  1.40s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:24<02:52,  1.40s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:25<02:47,  1.37s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:26<02:47,  1.38s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:28<02:44,  1.37s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:29<02:43,  1.37s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:30<02:41,  1.37s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:32<02:39,  1.36s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:33<02:35,  1.34s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:34<02:34,  1.34s/it]predicting train subjects:  60%|██████    | 171/285 [04:36<02:38,  1.39s/it]predicting train subjects:  60%|██████    | 172/285 [04:37<02:37,  1.39s/it]predicting train subjects:  61%|██████    | 173/285 [04:39<02:37,  1.41s/it]predicting train subjects:  61%|██████    | 174/285 [04:40<02:34,  1.39s/it]predicting train subjects:  61%|██████▏   | 175/285 [04:41<02:31,  1.38s/it]predicting train subjects:  62%|██████▏   | 176/285 [04:43<02:29,  1.37s/it]predicting train subjects:  62%|██████▏   | 177/285 [04:44<02:28,  1.37s/it]predicting train subjects:  62%|██████▏   | 178/285 [04:46<02:28,  1.39s/it]predicting train subjects:  63%|██████▎   | 179/285 [04:47<02:24,  1.36s/it]predicting train subjects:  63%|██████▎   | 180/285 [04:48<02:21,  1.35s/it]predicting train subjects:  64%|██████▎   | 181/285 [04:50<02:20,  1.35s/it]predicting train subjects:  64%|██████▍   | 182/285 [04:51<02:18,  1.35s/it]predicting train subjects:  64%|██████▍   | 183/285 [04:52<02:15,  1.33s/it]predicting train subjects:  65%|██████▍   | 184/285 [04:53<02:12,  1.31s/it]predicting train subjects:  65%|██████▍   | 185/285 [04:55<02:12,  1.32s/it]predicting train subjects:  65%|██████▌   | 186/285 [04:56<02:10,  1.32s/it]predicting train subjects:  66%|██████▌   | 187/285 [04:57<02:10,  1.33s/it]predicting train subjects:  66%|██████▌   | 188/285 [04:59<02:10,  1.34s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:00<02:08,  1.33s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:02<02:07,  1.34s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:03<02:04,  1.32s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:04<02:04,  1.34s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:06<02:03,  1.35s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:07<02:02,  1.34s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:08<02:01,  1.35s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:10<02:06,  1.42s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:11<02:10,  1.48s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:13<02:13,  1.53s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:15<02:14,  1.56s/it]predicting train subjects:  70%|███████   | 200/285 [05:16<02:12,  1.56s/it]predicting train subjects:  71%|███████   | 201/285 [05:18<02:10,  1.56s/it]predicting train subjects:  71%|███████   | 202/285 [05:19<02:07,  1.54s/it]predicting train subjects:  71%|███████   | 203/285 [05:21<02:05,  1.53s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:22<02:04,  1.54s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:24<02:04,  1.55s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:26<02:04,  1.58s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:27<02:03,  1.58s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:29<02:01,  1.57s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:30<02:01,  1.60s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:32<01:59,  1.59s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:34<01:59,  1.61s/it]predicting train subjects:  74%|███████▍  | 212/285 [05:35<01:58,  1.62s/it]predicting train subjects:  75%|███████▍  | 213/285 [05:37<01:56,  1.61s/it]predicting train subjects:  75%|███████▌  | 214/285 [05:38<01:49,  1.55s/it]predicting train subjects:  75%|███████▌  | 215/285 [05:40<01:45,  1.50s/it]predicting train subjects:  76%|███████▌  | 216/285 [05:41<01:42,  1.48s/it]predicting train subjects:  76%|███████▌  | 217/285 [05:43<01:40,  1.48s/it]predicting train subjects:  76%|███████▋  | 218/285 [05:44<01:38,  1.47s/it]predicting train subjects:  77%|███████▋  | 219/285 [05:46<01:36,  1.46s/it]predicting train subjects:  77%|███████▋  | 220/285 [05:47<01:34,  1.46s/it]predicting train subjects:  78%|███████▊  | 221/285 [05:48<01:32,  1.45s/it]predicting train subjects:  78%|███████▊  | 222/285 [05:50<01:30,  1.44s/it]predicting train subjects:  78%|███████▊  | 223/285 [05:51<01:28,  1.43s/it]predicting train subjects:  79%|███████▊  | 224/285 [05:53<01:27,  1.43s/it]predicting train subjects:  79%|███████▉  | 225/285 [05:54<01:27,  1.45s/it]predicting train subjects:  79%|███████▉  | 226/285 [05:56<01:25,  1.45s/it]predicting train subjects:  80%|███████▉  | 227/285 [05:57<01:23,  1.43s/it]predicting train subjects:  80%|████████  | 228/285 [05:58<01:21,  1.43s/it]predicting train subjects:  80%|████████  | 229/285 [06:00<01:20,  1.44s/it]predicting train subjects:  81%|████████  | 230/285 [06:01<01:17,  1.42s/it]predicting train subjects:  81%|████████  | 231/285 [06:03<01:17,  1.43s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:05<01:22,  1.56s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:06<01:24,  1.62s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:08<01:24,  1.65s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:10<01:24,  1.69s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:12<01:24,  1.72s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:13<01:23,  1.74s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:15<01:22,  1.75s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:17<01:20,  1.75s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:19<01:18,  1.74s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:20<01:16,  1.75s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:22<01:14,  1.73s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:24<01:13,  1.76s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:26<01:12,  1.76s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:27<01:09,  1.75s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:29<01:08,  1.75s/it]predicting train subjects:  87%|████████▋ | 247/285 [06:31<01:07,  1.77s/it]predicting train subjects:  87%|████████▋ | 248/285 [06:33<01:05,  1.77s/it]predicting train subjects:  87%|████████▋ | 249/285 [06:34<01:03,  1.77s/it]predicting train subjects:  88%|████████▊ | 250/285 [06:36<00:57,  1.66s/it]predicting train subjects:  88%|████████▊ | 251/285 [06:37<00:53,  1.56s/it]predicting train subjects:  88%|████████▊ | 252/285 [06:39<00:49,  1.49s/it]predicting train subjects:  89%|████████▉ | 253/285 [06:40<00:46,  1.45s/it]predicting train subjects:  89%|████████▉ | 254/285 [06:41<00:43,  1.41s/it]predicting train subjects:  89%|████████▉ | 255/285 [06:43<00:43,  1.44s/it]predicting train subjects:  90%|████████▉ | 256/285 [06:44<00:41,  1.42s/it]predicting train subjects:  90%|█████████ | 257/285 [06:45<00:39,  1.40s/it]predicting train subjects:  91%|█████████ | 258/285 [06:47<00:37,  1.39s/it]predicting train subjects:  91%|█████████ | 259/285 [06:48<00:35,  1.38s/it]predicting train subjects:  91%|█████████ | 260/285 [06:50<00:34,  1.37s/it]predicting train subjects:  92%|█████████▏| 261/285 [06:51<00:32,  1.36s/it]predicting train subjects:  92%|█████████▏| 262/285 [06:52<00:31,  1.35s/it]predicting train subjects:  92%|█████████▏| 263/285 [06:54<00:29,  1.36s/it]predicting train subjects:  93%|█████████▎| 264/285 [06:55<00:28,  1.35s/it]predicting train subjects:  93%|█████████▎| 265/285 [06:56<00:27,  1.38s/it]predicting train subjects:  93%|█████████▎| 266/285 [06:58<00:26,  1.38s/it]predicting train subjects:  94%|█████████▎| 267/285 [06:59<00:25,  1.40s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:01<00:25,  1.53s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:03<00:25,  1.58s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:04<00:24,  1.62s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:06<00:23,  1.68s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:08<00:22,  1.73s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:10<00:20,  1.74s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:12<00:19,  1.76s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:14<00:18,  1.80s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:16<00:16,  1.87s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:17<00:14,  1.86s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:19<00:12,  1.85s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:21<00:10,  1.81s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:23<00:09,  1.81s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:25<00:07,  1.80s/it]predicting train subjects:  99%|█████████▉| 282/285 [07:26<00:05,  1.79s/it]predicting train subjects:  99%|█████████▉| 283/285 [07:28<00:03,  1.80s/it]predicting train subjects: 100%|█████████▉| 284/285 [07:30<00:01,  1.83s/it]predicting train subjects: 100%|██████████| 285/285 [07:32<00:00,  1.81s/it]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:03,  1.28s/it]Loading train:   1%|          | 2/285 [00:02<06:22,  1.35s/it]Loading train:   1%|          | 3/285 [00:03<06:05,  1.30s/it]Loading train:   1%|▏         | 4/285 [00:05<06:31,  1.39s/it]Loading train:   2%|▏         | 5/285 [00:06<06:08,  1.32s/it]Loading train:   2%|▏         | 6/285 [00:08<06:24,  1.38s/it]Loading train:   2%|▏         | 7/285 [00:09<06:45,  1.46s/it]Loading train:   3%|▎         | 8/285 [00:11<06:59,  1.51s/it]Loading train:   3%|▎         | 9/285 [00:12<06:48,  1.48s/it]Loading train:   4%|▎         | 10/285 [00:14<06:12,  1.36s/it]Loading train:   4%|▍         | 11/285 [00:15<05:58,  1.31s/it]Loading train:   4%|▍         | 12/285 [00:16<05:36,  1.23s/it]Loading train:   5%|▍         | 13/285 [00:17<05:19,  1.18s/it]Loading train:   5%|▍         | 14/285 [00:18<05:19,  1.18s/it]Loading train:   5%|▌         | 15/285 [00:19<05:06,  1.14s/it]Loading train:   6%|▌         | 16/285 [00:20<05:03,  1.13s/it]Loading train:   6%|▌         | 17/285 [00:21<04:53,  1.10s/it]Loading train:   6%|▋         | 18/285 [00:22<04:53,  1.10s/it]Loading train:   7%|▋         | 19/285 [00:23<04:40,  1.05s/it]Loading train:   7%|▋         | 20/285 [00:24<04:36,  1.04s/it]Loading train:   7%|▋         | 21/285 [00:25<04:30,  1.02s/it]Loading train:   8%|▊         | 22/285 [00:26<04:22,  1.00it/s]Loading train:   8%|▊         | 23/285 [00:27<04:38,  1.06s/it]Loading train:   8%|▊         | 24/285 [00:28<04:36,  1.06s/it]Loading train:   9%|▉         | 25/285 [00:30<04:39,  1.07s/it]Loading train:   9%|▉         | 26/285 [00:30<04:25,  1.02s/it]Loading train:   9%|▉         | 27/285 [00:32<04:35,  1.07s/it]Loading train:  10%|▉         | 28/285 [00:33<04:30,  1.05s/it]Loading train:  10%|█         | 29/285 [00:34<04:50,  1.13s/it]Loading train:  11%|█         | 30/285 [00:35<04:38,  1.09s/it]Loading train:  11%|█         | 31/285 [00:36<04:35,  1.08s/it]Loading train:  11%|█         | 32/285 [00:37<04:39,  1.10s/it]Loading train:  12%|█▏        | 33/285 [00:38<04:31,  1.08s/it]Loading train:  12%|█▏        | 34/285 [00:39<04:26,  1.06s/it]Loading train:  12%|█▏        | 35/285 [00:40<04:19,  1.04s/it]Loading train:  13%|█▎        | 36/285 [00:41<04:10,  1.01s/it]Loading train:  13%|█▎        | 37/285 [00:42<03:53,  1.06it/s]Loading train:  13%|█▎        | 38/285 [00:43<03:56,  1.04it/s]Loading train:  14%|█▎        | 39/285 [00:44<03:57,  1.04it/s]Loading train:  14%|█▍        | 40/285 [00:45<03:57,  1.03it/s]Loading train:  14%|█▍        | 41/285 [00:46<03:53,  1.05it/s]Loading train:  15%|█▍        | 42/285 [00:47<03:55,  1.03it/s]Loading train:  15%|█▌        | 43/285 [00:48<03:55,  1.03it/s]Loading train:  15%|█▌        | 44/285 [00:49<03:49,  1.05it/s]Loading train:  16%|█▌        | 45/285 [00:50<03:47,  1.05it/s]Loading train:  16%|█▌        | 46/285 [00:51<03:50,  1.04it/s]Loading train:  16%|█▋        | 47/285 [00:51<03:43,  1.07it/s]Loading train:  17%|█▋        | 48/285 [00:52<03:29,  1.13it/s]Loading train:  17%|█▋        | 49/285 [00:53<03:25,  1.15it/s]Loading train:  18%|█▊        | 50/285 [00:54<03:16,  1.20it/s]Loading train:  18%|█▊        | 51/285 [00:55<03:26,  1.14it/s]Loading train:  18%|█▊        | 52/285 [00:56<03:18,  1.18it/s]Loading train:  19%|█▊        | 53/285 [00:56<03:17,  1.18it/s]Loading train:  19%|█▉        | 54/285 [00:57<03:15,  1.18it/s]Loading train:  19%|█▉        | 55/285 [00:58<03:09,  1.21it/s]Loading train:  20%|█▉        | 56/285 [00:59<03:08,  1.22it/s]Loading train:  20%|██        | 57/285 [01:00<03:06,  1.22it/s]Loading train:  20%|██        | 58/285 [01:01<03:08,  1.20it/s]Loading train:  21%|██        | 59/285 [01:01<03:11,  1.18it/s]Loading train:  21%|██        | 60/285 [01:02<03:04,  1.22it/s]Loading train:  21%|██▏       | 61/285 [01:03<03:14,  1.15it/s]Loading train:  22%|██▏       | 62/285 [01:04<03:15,  1.14it/s]Loading train:  22%|██▏       | 63/285 [01:05<03:11,  1.16it/s]Loading train:  22%|██▏       | 64/285 [01:06<03:43,  1.01s/it]Loading train:  23%|██▎       | 65/285 [01:08<04:18,  1.17s/it]Loading train:  23%|██▎       | 66/285 [01:09<04:23,  1.20s/it]Loading train:  24%|██▎       | 67/285 [01:10<04:15,  1.17s/it]Loading train:  24%|██▍       | 68/285 [01:11<03:49,  1.06s/it]Loading train:  24%|██▍       | 69/285 [01:12<03:39,  1.01s/it]Loading train:  25%|██▍       | 70/285 [01:13<03:33,  1.01it/s]Loading train:  25%|██▍       | 71/285 [01:14<03:27,  1.03it/s]Loading train:  25%|██▌       | 72/285 [01:15<03:31,  1.01it/s]Loading train:  26%|██▌       | 73/285 [01:16<03:20,  1.06it/s]Loading train:  26%|██▌       | 74/285 [01:17<03:20,  1.05it/s]Loading train:  26%|██▋       | 75/285 [01:18<03:21,  1.04it/s]Loading train:  27%|██▋       | 76/285 [01:18<03:17,  1.06it/s]Loading train:  27%|██▋       | 77/285 [01:20<03:21,  1.03it/s]Loading train:  27%|██▋       | 78/285 [01:20<03:13,  1.07it/s]Loading train:  28%|██▊       | 79/285 [01:21<03:11,  1.08it/s]Loading train:  28%|██▊       | 80/285 [01:22<03:08,  1.09it/s]Loading train:  28%|██▊       | 81/285 [01:23<03:01,  1.12it/s]Loading train:  29%|██▉       | 82/285 [01:24<03:10,  1.07it/s]Loading train:  29%|██▉       | 83/285 [01:25<03:01,  1.11it/s]Loading train:  29%|██▉       | 84/285 [01:26<02:53,  1.16it/s]Loading train:  30%|██▉       | 85/285 [01:27<03:10,  1.05it/s]Loading train:  30%|███       | 86/285 [01:28<03:13,  1.03it/s]Loading train:  31%|███       | 87/285 [01:29<03:16,  1.01it/s]Loading train:  31%|███       | 88/285 [01:30<03:08,  1.04it/s]Loading train:  31%|███       | 89/285 [01:31<03:10,  1.03it/s]Loading train:  32%|███▏      | 90/285 [01:32<03:17,  1.01s/it]Loading train:  32%|███▏      | 91/285 [01:33<03:22,  1.04s/it]Loading train:  32%|███▏      | 92/285 [01:34<03:23,  1.05s/it]Loading train:  33%|███▎      | 93/285 [01:35<03:30,  1.09s/it]Loading train:  33%|███▎      | 94/285 [01:36<03:29,  1.09s/it]Loading train:  33%|███▎      | 95/285 [01:37<03:28,  1.10s/it]Loading train:  34%|███▎      | 96/285 [01:39<03:33,  1.13s/it]Loading train:  34%|███▍      | 97/285 [01:40<03:29,  1.11s/it]Loading train:  34%|███▍      | 98/285 [01:41<03:26,  1.10s/it]Loading train:  35%|███▍      | 99/285 [01:42<03:16,  1.06s/it]Loading train:  35%|███▌      | 100/285 [01:43<03:08,  1.02s/it]Loading train:  35%|███▌      | 101/285 [01:44<03:00,  1.02it/s]Loading train:  36%|███▌      | 102/285 [01:45<03:06,  1.02s/it]Loading train:  36%|███▌      | 103/285 [01:46<03:00,  1.01it/s]Loading train:  36%|███▋      | 104/285 [01:47<03:07,  1.04s/it]Loading train:  37%|███▋      | 105/285 [01:48<02:57,  1.01it/s]Loading train:  37%|███▋      | 106/285 [01:49<03:02,  1.02s/it]Loading train:  38%|███▊      | 107/285 [01:50<02:56,  1.01it/s]Loading train:  38%|███▊      | 108/285 [01:51<02:56,  1.00it/s]Loading train:  38%|███▊      | 109/285 [01:52<02:49,  1.04it/s]Loading train:  39%|███▊      | 110/285 [01:52<02:50,  1.03it/s]Loading train:  39%|███▉      | 111/285 [01:53<02:48,  1.03it/s]Loading train:  39%|███▉      | 112/285 [01:54<02:41,  1.07it/s]Loading train:  40%|███▉      | 113/285 [01:55<02:45,  1.04it/s]Loading train:  40%|████      | 114/285 [01:56<02:47,  1.02it/s]Loading train:  40%|████      | 115/285 [01:57<02:41,  1.05it/s]Loading train:  41%|████      | 116/285 [01:58<02:35,  1.09it/s]Loading train:  41%|████      | 117/285 [01:59<02:39,  1.06it/s]Loading train:  41%|████▏     | 118/285 [02:00<02:35,  1.07it/s]Loading train:  42%|████▏     | 119/285 [02:01<02:41,  1.03it/s]Loading train:  42%|████▏     | 120/285 [02:02<02:35,  1.06it/s]Loading train:  42%|████▏     | 121/285 [02:03<02:57,  1.08s/it]Loading train:  43%|████▎     | 122/285 [02:04<02:56,  1.08s/it]Loading train:  43%|████▎     | 123/285 [02:06<03:07,  1.16s/it]Loading train:  44%|████▎     | 124/285 [02:07<02:53,  1.08s/it]Loading train:  44%|████▍     | 125/285 [02:08<02:45,  1.04s/it]Loading train:  44%|████▍     | 126/285 [02:08<02:31,  1.05it/s]Loading train:  45%|████▍     | 127/285 [02:09<02:23,  1.10it/s]Loading train:  45%|████▍     | 128/285 [02:10<02:27,  1.07it/s]Loading train:  45%|████▌     | 129/285 [02:11<02:16,  1.14it/s]Loading train:  46%|████▌     | 130/285 [02:12<02:11,  1.17it/s]Loading train:  46%|████▌     | 131/285 [02:13<02:10,  1.18it/s]Loading train:  46%|████▋     | 132/285 [02:13<02:05,  1.22it/s]Loading train:  47%|████▋     | 133/285 [02:14<02:17,  1.11it/s]Loading train:  47%|████▋     | 134/285 [02:15<02:25,  1.04it/s]Loading train:  47%|████▋     | 135/285 [02:16<02:15,  1.11it/s]Loading train:  48%|████▊     | 136/285 [02:17<02:11,  1.13it/s]Loading train:  48%|████▊     | 137/285 [02:18<02:13,  1.11it/s]Loading train:  48%|████▊     | 138/285 [02:19<02:22,  1.03it/s]Loading train:  49%|████▉     | 139/285 [02:20<02:21,  1.04it/s]Loading train:  49%|████▉     | 140/285 [02:21<02:20,  1.04it/s]Loading train:  49%|████▉     | 141/285 [02:22<02:16,  1.05it/s]Loading train:  50%|████▉     | 142/285 [02:23<02:09,  1.11it/s]Loading train:  50%|█████     | 143/285 [02:24<02:10,  1.09it/s]Loading train:  51%|█████     | 144/285 [02:24<02:01,  1.16it/s]Loading train:  51%|█████     | 145/285 [02:25<01:57,  1.20it/s]Loading train:  51%|█████     | 146/285 [02:26<01:55,  1.21it/s]Loading train:  52%|█████▏    | 147/285 [02:27<01:48,  1.27it/s]Loading train:  52%|█████▏    | 148/285 [02:27<01:44,  1.31it/s]Loading train:  52%|█████▏    | 149/285 [02:28<01:47,  1.26it/s]Loading train:  53%|█████▎    | 150/285 [02:29<01:44,  1.29it/s]Loading train:  53%|█████▎    | 151/285 [02:30<01:43,  1.30it/s]Loading train:  53%|█████▎    | 152/285 [02:31<01:43,  1.29it/s]Loading train:  54%|█████▎    | 153/285 [02:31<01:41,  1.30it/s]Loading train:  54%|█████▍    | 154/285 [02:32<01:48,  1.21it/s]Loading train:  54%|█████▍    | 155/285 [02:33<01:44,  1.25it/s]Loading train:  55%|█████▍    | 156/285 [02:34<01:42,  1.26it/s]Loading train:  55%|█████▌    | 157/285 [02:35<01:45,  1.21it/s]Loading train:  55%|█████▌    | 158/285 [02:35<01:41,  1.25it/s]Loading train:  56%|█████▌    | 159/285 [02:36<01:40,  1.26it/s]Loading train:  56%|█████▌    | 160/285 [02:37<01:42,  1.22it/s]Loading train:  56%|█████▋    | 161/285 [02:38<01:38,  1.26it/s]Loading train:  57%|█████▋    | 162/285 [02:39<01:39,  1.24it/s]Loading train:  57%|█████▋    | 163/285 [02:40<01:38,  1.23it/s]Loading train:  58%|█████▊    | 164/285 [02:40<01:38,  1.23it/s]Loading train:  58%|█████▊    | 165/285 [02:41<01:34,  1.27it/s]Loading train:  58%|█████▊    | 166/285 [02:42<01:33,  1.27it/s]Loading train:  59%|█████▊    | 167/285 [02:43<01:35,  1.24it/s]Loading train:  59%|█████▉    | 168/285 [02:43<01:31,  1.28it/s]Loading train:  59%|█████▉    | 169/285 [02:44<01:30,  1.29it/s]Loading train:  60%|█████▉    | 170/285 [02:45<01:29,  1.29it/s]Loading train:  60%|██████    | 171/285 [02:46<01:26,  1.32it/s]Loading train:  60%|██████    | 172/285 [02:46<01:24,  1.34it/s]Loading train:  61%|██████    | 173/285 [02:47<01:24,  1.32it/s]Loading train:  61%|██████    | 174/285 [02:48<01:27,  1.27it/s]Loading train:  61%|██████▏   | 175/285 [02:49<01:26,  1.27it/s]Loading train:  62%|██████▏   | 176/285 [02:50<01:22,  1.31it/s]Loading train:  62%|██████▏   | 177/285 [02:50<01:24,  1.28it/s]Loading train:  62%|██████▏   | 178/285 [02:51<01:22,  1.29it/s]Loading train:  63%|██████▎   | 179/285 [02:52<01:22,  1.29it/s]Loading train:  63%|██████▎   | 180/285 [02:53<01:17,  1.35it/s]Loading train:  64%|██████▎   | 181/285 [02:53<01:22,  1.27it/s]Loading train:  64%|██████▍   | 182/285 [02:54<01:22,  1.24it/s]Loading train:  64%|██████▍   | 183/285 [02:55<01:20,  1.26it/s]Loading train:  65%|██████▍   | 184/285 [02:56<01:20,  1.25it/s]Loading train:  65%|██████▍   | 185/285 [02:57<01:17,  1.29it/s]Loading train:  65%|██████▌   | 186/285 [02:57<01:14,  1.32it/s]Loading train:  66%|██████▌   | 187/285 [02:58<01:13,  1.34it/s]Loading train:  66%|██████▌   | 188/285 [02:59<01:14,  1.30it/s]Loading train:  66%|██████▋   | 189/285 [03:00<01:12,  1.33it/s]Loading train:  67%|██████▋   | 190/285 [03:00<01:10,  1.35it/s]Loading train:  67%|██████▋   | 191/285 [03:01<01:10,  1.33it/s]Loading train:  67%|██████▋   | 192/285 [03:02<01:11,  1.30it/s]Loading train:  68%|██████▊   | 193/285 [03:03<01:08,  1.35it/s]Loading train:  68%|██████▊   | 194/285 [03:03<01:05,  1.40it/s]Loading train:  68%|██████▊   | 195/285 [03:04<01:01,  1.46it/s]Loading train:  69%|██████▉   | 196/285 [03:05<01:04,  1.38it/s]Loading train:  69%|██████▉   | 197/285 [03:06<01:07,  1.30it/s]Loading train:  69%|██████▉   | 198/285 [03:06<01:08,  1.27it/s]Loading train:  70%|██████▉   | 199/285 [03:07<01:11,  1.20it/s]Loading train:  70%|███████   | 200/285 [03:08<01:08,  1.23it/s]Loading train:  71%|███████   | 201/285 [03:09<01:09,  1.21it/s]Loading train:  71%|███████   | 202/285 [03:10<01:12,  1.15it/s]Loading train:  71%|███████   | 203/285 [03:11<01:13,  1.12it/s]Loading train:  72%|███████▏  | 204/285 [03:12<01:12,  1.12it/s]Loading train:  72%|███████▏  | 205/285 [03:13<01:11,  1.12it/s]Loading train:  72%|███████▏  | 206/285 [03:13<01:06,  1.19it/s]Loading train:  73%|███████▎  | 207/285 [03:14<01:08,  1.15it/s]Loading train:  73%|███████▎  | 208/285 [03:15<01:06,  1.16it/s]Loading train:  73%|███████▎  | 209/285 [03:16<01:06,  1.15it/s]Loading train:  74%|███████▎  | 210/285 [03:17<01:10,  1.07it/s]Loading train:  74%|███████▍  | 211/285 [03:18<01:04,  1.14it/s]Loading train:  74%|███████▍  | 212/285 [03:19<01:09,  1.05it/s]Loading train:  75%|███████▍  | 213/285 [03:20<01:05,  1.10it/s]Loading train:  75%|███████▌  | 214/285 [03:21<01:03,  1.11it/s]Loading train:  75%|███████▌  | 215/285 [03:21<00:59,  1.18it/s]Loading train:  76%|███████▌  | 216/285 [03:22<00:56,  1.22it/s]Loading train:  76%|███████▌  | 217/285 [03:23<00:54,  1.25it/s]Loading train:  76%|███████▋  | 218/285 [03:24<00:53,  1.25it/s]Loading train:  77%|███████▋  | 219/285 [03:25<00:53,  1.23it/s]Loading train:  77%|███████▋  | 220/285 [03:25<00:52,  1.25it/s]Loading train:  78%|███████▊  | 221/285 [03:26<00:56,  1.14it/s]Loading train:  78%|███████▊  | 222/285 [03:27<00:51,  1.22it/s]Loading train:  78%|███████▊  | 223/285 [03:28<00:52,  1.18it/s]Loading train:  79%|███████▊  | 224/285 [03:29<00:52,  1.17it/s]Loading train:  79%|███████▉  | 225/285 [03:30<00:49,  1.22it/s]Loading train:  79%|███████▉  | 226/285 [03:30<00:49,  1.20it/s]Loading train:  80%|███████▉  | 227/285 [03:31<00:48,  1.19it/s]Loading train:  80%|████████  | 228/285 [03:32<00:46,  1.22it/s]Loading train:  80%|████████  | 229/285 [03:33<00:47,  1.17it/s]Loading train:  81%|████████  | 230/285 [03:34<00:46,  1.18it/s]Loading train:  81%|████████  | 231/285 [03:35<00:46,  1.16it/s]Loading train:  81%|████████▏ | 232/285 [03:36<00:48,  1.10it/s]Loading train:  82%|████████▏ | 233/285 [03:37<00:50,  1.03it/s]Loading train:  82%|████████▏ | 234/285 [03:38<00:49,  1.03it/s]Loading train:  82%|████████▏ | 235/285 [03:39<00:47,  1.05it/s]Loading train:  83%|████████▎ | 236/285 [03:40<00:48,  1.00it/s]Loading train:  83%|████████▎ | 237/285 [03:41<00:46,  1.02it/s]Loading train:  84%|████████▎ | 238/285 [03:42<00:45,  1.04it/s]Loading train:  84%|████████▍ | 239/285 [03:43<00:43,  1.05it/s]Loading train:  84%|████████▍ | 240/285 [03:44<00:43,  1.05it/s]Loading train:  85%|████████▍ | 241/285 [03:45<00:54,  1.24s/it]Loading train:  85%|████████▍ | 242/285 [03:47<00:57,  1.33s/it]Loading train:  85%|████████▌ | 243/285 [03:48<00:56,  1.34s/it]Loading train:  86%|████████▌ | 244/285 [03:50<00:56,  1.38s/it]Loading train:  86%|████████▌ | 245/285 [03:51<00:53,  1.34s/it]Loading train:  86%|████████▋ | 246/285 [03:53<00:55,  1.42s/it]Loading train:  87%|████████▋ | 247/285 [03:54<00:57,  1.51s/it]Loading train:  87%|████████▋ | 248/285 [03:56<00:59,  1.61s/it]Loading train:  87%|████████▋ | 249/285 [03:58<01:02,  1.74s/it]Loading train:  88%|████████▊ | 250/285 [04:00<01:00,  1.72s/it]Loading train:  88%|████████▊ | 251/285 [04:01<00:50,  1.49s/it]Loading train:  88%|████████▊ | 252/285 [04:02<00:48,  1.47s/it]Loading train:  89%|████████▉ | 253/285 [04:04<00:47,  1.49s/it]Loading train:  89%|████████▉ | 254/285 [04:05<00:41,  1.34s/it]Loading train:  89%|████████▉ | 255/285 [04:06<00:40,  1.36s/it]Loading train:  90%|████████▉ | 256/285 [04:08<00:42,  1.48s/it]Loading train:  90%|█████████ | 257/285 [04:10<00:41,  1.48s/it]Loading train:  91%|█████████ | 258/285 [04:11<00:37,  1.39s/it]Loading train:  91%|█████████ | 259/285 [04:12<00:33,  1.29s/it]Loading train:  91%|█████████ | 260/285 [04:13<00:35,  1.41s/it]Loading train:  92%|█████████▏| 261/285 [04:15<00:37,  1.56s/it]Loading train:  92%|█████████▏| 262/285 [04:17<00:34,  1.52s/it]Loading train:  92%|█████████▏| 263/285 [04:18<00:29,  1.35s/it]Loading train:  93%|█████████▎| 264/285 [04:19<00:28,  1.38s/it]Loading train:  93%|█████████▎| 265/285 [04:21<00:28,  1.42s/it]Loading train:  93%|█████████▎| 266/285 [04:22<00:28,  1.48s/it]Loading train:  94%|█████████▎| 267/285 [04:23<00:23,  1.33s/it]Loading train:  94%|█████████▍| 268/285 [04:25<00:23,  1.39s/it]Loading train:  94%|█████████▍| 269/285 [04:26<00:21,  1.36s/it]Loading train:  95%|█████████▍| 270/285 [04:27<00:19,  1.30s/it]Loading train:  95%|█████████▌| 271/285 [04:29<00:20,  1.44s/it]Loading train:  95%|█████████▌| 272/285 [04:31<00:19,  1.50s/it]Loading train:  96%|█████████▌| 273/285 [04:33<00:20,  1.70s/it]Loading train:  96%|█████████▌| 274/285 [04:35<00:18,  1.69s/it]Loading train:  96%|█████████▋| 275/285 [04:36<00:16,  1.61s/it]Loading train:  97%|█████████▋| 276/285 [04:37<00:13,  1.52s/it]Loading train:  97%|█████████▋| 277/285 [04:39<00:12,  1.54s/it]Loading train:  98%|█████████▊| 278/285 [04:40<00:10,  1.47s/it]Loading train:  98%|█████████▊| 279/285 [04:42<00:09,  1.51s/it]Loading train:  98%|█████████▊| 280/285 [04:44<00:08,  1.65s/it]Loading train:  99%|█████████▊| 281/285 [04:46<00:06,  1.68s/it]Loading train:  99%|█████████▉| 282/285 [04:47<00:05,  1.76s/it]Loading train:  99%|█████████▉| 283/285 [04:49<00:03,  1.82s/it]Loading train: 100%|█████████▉| 284/285 [04:51<00:01,  1.79s/it]Loading train: 100%|██████████| 285/285 [04:53<00:00,  1.82s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 2/285 [00:00<00:19, 14.82it/s]concatenating: train:   4%|▍         | 11/285 [00:00<00:13, 19.72it/s]concatenating: train:  12%|█▏        | 34/285 [00:00<00:09, 27.14it/s]concatenating: train:  21%|██        | 60/285 [00:00<00:06, 37.11it/s]concatenating: train:  26%|██▌       | 74/285 [00:00<00:04, 47.55it/s]concatenating: train:  31%|███       | 88/285 [00:00<00:03, 54.61it/s]concatenating: train:  35%|███▌      | 101/285 [00:01<00:04, 45.54it/s]concatenating: train:  39%|███▉      | 111/285 [00:01<00:04, 42.94it/s]concatenating: train:  42%|████▏     | 119/285 [00:01<00:03, 42.82it/s]concatenating: train:  44%|████▍     | 126/285 [00:01<00:03, 40.41it/s]concatenating: train:  46%|████▋     | 132/285 [00:01<00:03, 42.84it/s]concatenating: train:  48%|████▊     | 138/285 [00:02<00:04, 32.12it/s]concatenating: train:  50%|█████     | 143/285 [00:02<00:04, 32.85it/s]concatenating: train:  52%|█████▏    | 149/285 [00:02<00:03, 36.57it/s]concatenating: train:  54%|█████▍    | 155/285 [00:02<00:03, 40.51it/s]concatenating: train:  56%|█████▌    | 160/285 [00:02<00:03, 39.68it/s]concatenating: train:  59%|█████▉    | 168/285 [00:02<00:02, 45.45it/s]concatenating: train:  62%|██████▏   | 178/285 [00:02<00:01, 54.34it/s]concatenating: train:  69%|██████▉   | 197/285 [00:03<00:01, 68.57it/s]concatenating: train:  80%|████████  | 228/285 [00:03<00:00, 89.37it/s]concatenating: train:  86%|████████▌ | 245/285 [00:03<00:00, 100.69it/s]concatenating: train:  92%|█████████▏| 261/285 [00:03<00:00, 68.62it/s] concatenating: train:  96%|█████████▌| 274/285 [00:04<00:00, 51.61it/s]concatenating: train: 100%|█████████▉| 284/285 [00:04<00:00, 38.59it/s]concatenating: train: 100%|██████████| 285/285 [00:04<00:00, 63.72it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.94s/it]Loading test:  67%|██████▋   | 2/3 [00:03<00:01,  1.88s/it]Loading test: 100%|██████████| 3/3 [00:05<00:00,  1.78s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 35.35it/s]2019-07-07 00:45:14.558230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-07 00:45:14.558321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-07 00:45:14.558336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-07 00:45:14.558344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-07 00:45:14.558784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:12,  3.25it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:09,  3.78it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.01it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:06,  5.06it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:07,  4.00it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:06,  4.57it/s]loading the weights for Unet:  30%|███       | 12/40 [00:02<00:06,  4.45it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:04,  5.48it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:05,  4.33it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:04,  5.07it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:03<00:05,  3.56it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:03,  4.32it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  5.28it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:04<00:03,  4.52it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:04<00:02,  5.20it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  4.83it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:05<00:01,  5.72it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:05<00:00,  6.15it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:05<00:00,  5.23it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  5.72it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:06<00:00,  5.01it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:06<00:00,  6.52it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 80, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 80, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 80, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 80, 40)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 80, 40)   14440       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 80, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 80, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 80, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 80, 40)   14440       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 80, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 80, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 80, 40)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 80, 20)   7220        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 80, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 80, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 80, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 80, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 80, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 40, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 40, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 40, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 40, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 40, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 40, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 40, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 20, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 20, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 20, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 20, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 20, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 20, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 20, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 20, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 40, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 40, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 40, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 40, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 40, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 40, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 13)   273         dropout_8[0][0]                  
==================================================================================================
Total params: 220,893
Trainable params: 77,173
Non-trainable params: 143,720
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 24s - loss: 179.2356 - acc: 0.4395 - mDice: 0.0164 - val_loss: 40.0732 - val_acc: 0.9041 - val_mDice: 0.0146

Epoch 00001: val_mDice improved from -inf to 0.01457, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 14s - loss: 40.0219 - acc: 0.8308 - mDice: 0.0145 - val_loss: 14.8750 - val_acc: 0.9045 - val_mDice: 0.0130

Epoch 00002: val_mDice did not improve from 0.01457
Epoch 3/300
 - 13s - loss: 20.6670 - acc: 0.8591 - mDice: 0.0153 - val_loss: 9.1350 - val_acc: 0.9047 - val_mDice: 0.0153

Epoch 00003: val_mDice improved from 0.01457 to 0.01528, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 13s - loss: 14.3451 - acc: 0.8653 - mDice: 0.0203 - val_loss: 7.0861 - val_acc: 0.9047 - val_mDice: 0.0190

Epoch 00004: val_mDice improved from 0.01528 to 0.01905, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 14s - loss: 11.4497 - acc: 0.8675 - mDice: 0.0260 - val_loss: 6.4290 - val_acc: 0.9047 - val_mDice: 0.0185

Epoch 00005: val_mDice did not improve from 0.01905
Epoch 6/300
 - 14s - loss: 9.8285 - acc: 0.8684 - mDice: 0.0313 - val_loss: 5.9922 - val_acc: 0.9047 - val_mDice: 0.0233

Epoch 00006: val_mDice improved from 0.01905 to 0.02334, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 13s - loss: 8.7667 - acc: 0.8687 - mDice: 0.0369 - val_loss: 5.6265 - val_acc: 0.9047 - val_mDice: 0.0330

Epoch 00007: val_mDice improved from 0.02334 to 0.03303, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 14s - loss: 7.9695 - acc: 0.8689 - mDice: 0.0417 - val_loss: 5.8051 - val_acc: 0.9047 - val_mDice: 0.0226

Epoch 00008: val_mDice did not improve from 0.03303
Epoch 9/300
 - 14s - loss: 7.3319 - acc: 0.8690 - mDice: 0.0474 - val_loss: 5.6789 - val_acc: 0.9047 - val_mDice: 0.0253

Epoch 00009: val_mDice did not improve from 0.03303
Epoch 10/300
 - 13s - loss: 6.8268 - acc: 0.8691 - mDice: 0.0547 - val_loss: 5.2981 - val_acc: 0.9047 - val_mDice: 0.0405

Epoch 00010: val_mDice improved from 0.03303 to 0.04045, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 14s - loss: 6.3983 - acc: 0.8691 - mDice: 0.0640 - val_loss: 4.7743 - val_acc: 0.9048 - val_mDice: 0.0629

Epoch 00011: val_mDice improved from 0.04045 to 0.06292, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 13s - loss: 5.9919 - acc: 0.8690 - mDice: 0.0759 - val_loss: 4.5600 - val_acc: 0.9049 - val_mDice: 0.0758

Epoch 00012: val_mDice improved from 0.06292 to 0.07581, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 13s - loss: 5.6309 - acc: 0.8689 - mDice: 0.0900 - val_loss: 4.2089 - val_acc: 0.9052 - val_mDice: 0.1024

Epoch 00013: val_mDice improved from 0.07581 to 0.10240, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 14s - loss: 5.2998 - acc: 0.8692 - mDice: 0.1055 - val_loss: 4.1727 - val_acc: 0.9052 - val_mDice: 0.1155

Epoch 00014: val_mDice improved from 0.10240 to 0.11550, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 14s - loss: 4.9937 - acc: 0.8697 - mDice: 0.1240 - val_loss: 4.0969 - val_acc: 0.9051 - val_mDice: 0.1263

Epoch 00015: val_mDice improved from 0.11550 to 0.12625, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 14s - loss: 4.7113 - acc: 0.8709 - mDice: 0.1431 - val_loss: 3.7040 - val_acc: 0.9064 - val_mDice: 0.1704

Epoch 00016: val_mDice improved from 0.12625 to 0.17037, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 14s - loss: 4.4257 - acc: 0.8729 - mDice: 0.1667 - val_loss: 3.8464 - val_acc: 0.9063 - val_mDice: 0.1717

Epoch 00017: val_mDice improved from 0.17037 to 0.17173, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 14s - loss: 4.1956 - acc: 0.8750 - mDice: 0.1868 - val_loss: 3.5153 - val_acc: 0.9082 - val_mDice: 0.2122

Epoch 00018: val_mDice improved from 0.17173 to 0.21220, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 14s - loss: 3.9962 - acc: 0.8771 - mDice: 0.2043 - val_loss: 3.2866 - val_acc: 0.9086 - val_mDice: 0.2385

Epoch 00019: val_mDice improved from 0.21220 to 0.23853, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 14s - loss: 3.8317 - acc: 0.8796 - mDice: 0.2202 - val_loss: 3.8551 - val_acc: 0.9070 - val_mDice: 0.2101

Epoch 00020: val_mDice did not improve from 0.23853
Epoch 21/300
 - 14s - loss: 3.6736 - acc: 0.8825 - mDice: 0.2372 - val_loss: 3.7104 - val_acc: 0.9079 - val_mDice: 0.2272

Epoch 00021: val_mDice did not improve from 0.23853
Epoch 22/300
 - 14s - loss: 3.5498 - acc: 0.8848 - mDice: 0.2505 - val_loss: 3.3279 - val_acc: 0.9168 - val_mDice: 0.2745

Epoch 00022: val_mDice improved from 0.23853 to 0.27448, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 14s - loss: 3.4201 - acc: 0.8874 - mDice: 0.2660 - val_loss: 3.1296 - val_acc: 0.9192 - val_mDice: 0.2928

Epoch 00023: val_mDice improved from 0.27448 to 0.29279, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 14s - loss: 3.3102 - acc: 0.8895 - mDice: 0.2792 - val_loss: 3.2943 - val_acc: 0.9194 - val_mDice: 0.2951

Epoch 00024: val_mDice improved from 0.29279 to 0.29509, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 14s - loss: 3.2128 - acc: 0.8911 - mDice: 0.2907 - val_loss: 3.0331 - val_acc: 0.9219 - val_mDice: 0.3182

Epoch 00025: val_mDice improved from 0.29509 to 0.31817, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 14s - loss: 3.1226 - acc: 0.8931 - mDice: 0.3025 - val_loss: 3.2376 - val_acc: 0.9219 - val_mDice: 0.3080

Epoch 00026: val_mDice did not improve from 0.31817
Epoch 27/300
 - 13s - loss: 3.0391 - acc: 0.8945 - mDice: 0.3143 - val_loss: 3.4516 - val_acc: 0.9204 - val_mDice: 0.3000

Epoch 00027: val_mDice did not improve from 0.31817
Epoch 28/300
 - 14s - loss: 2.9579 - acc: 0.8962 - mDice: 0.3251 - val_loss: 2.9286 - val_acc: 0.9274 - val_mDice: 0.3470

Epoch 00028: val_mDice improved from 0.31817 to 0.34696, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 13s - loss: 2.8777 - acc: 0.8976 - mDice: 0.3359 - val_loss: 3.2024 - val_acc: 0.9259 - val_mDice: 0.3300

Epoch 00029: val_mDice did not improve from 0.34696
Epoch 30/300
 - 14s - loss: 2.8198 - acc: 0.8989 - mDice: 0.3445 - val_loss: 3.1099 - val_acc: 0.9280 - val_mDice: 0.3360

Epoch 00030: val_mDice did not improve from 0.34696
Epoch 31/300
 - 14s - loss: 2.7704 - acc: 0.9001 - mDice: 0.3514 - val_loss: 3.1881 - val_acc: 0.9286 - val_mDice: 0.3430

Epoch 00031: val_mDice did not improve from 0.34696
Epoch 32/300
 - 14s - loss: 2.7147 - acc: 0.9010 - mDice: 0.3592 - val_loss: 3.0081 - val_acc: 0.9307 - val_mDice: 0.3596

Epoch 00032: val_mDice improved from 0.34696 to 0.35959, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 13s - loss: 2.6633 - acc: 0.9021 - mDice: 0.3674 - val_loss: 3.1185 - val_acc: 0.9315 - val_mDice: 0.3602

Epoch 00033: val_mDice improved from 0.35959 to 0.36017, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 13s - loss: 2.6196 - acc: 0.9029 - mDice: 0.3733 - val_loss: 3.1784 - val_acc: 0.9295 - val_mDice: 0.3564

Epoch 00034: val_mDice did not improve from 0.36017
Epoch 35/300
 - 13s - loss: 2.5763 - acc: 0.9039 - mDice: 0.3816 - val_loss: 2.9691 - val_acc: 0.9329 - val_mDice: 0.3739

Epoch 00035: val_mDice improved from 0.36017 to 0.37387, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 13s - loss: 2.5283 - acc: 0.9046 - mDice: 0.3883 - val_loss: 3.1205 - val_acc: 0.9318 - val_mDice: 0.3675

Epoch 00036: val_mDice did not improve from 0.37387
Epoch 37/300
 - 13s - loss: 2.4937 - acc: 0.9055 - mDice: 0.3942 - val_loss: 3.8136 - val_acc: 0.9243 - val_mDice: 0.3297

Epoch 00037: val_mDice did not improve from 0.37387
Epoch 38/300
 - 13s - loss: 2.4629 - acc: 0.9062 - mDice: 0.3998 - val_loss: 3.2525 - val_acc: 0.9327 - val_mDice: 0.3631

Epoch 00038: val_mDice did not improve from 0.37387
Epoch 39/300
 - 13s - loss: 2.4343 - acc: 0.9069 - mDice: 0.4048 - val_loss: 3.1181 - val_acc: 0.9328 - val_mDice: 0.3731

Epoch 00039: val_mDice did not improve from 0.37387
Epoch 40/300
 - 13s - loss: 2.3936 - acc: 0.9074 - mDice: 0.4112 - val_loss: 3.4135 - val_acc: 0.9320 - val_mDice: 0.3628

Epoch 00040: val_mDice did not improve from 0.37387
Epoch 41/300
 - 13s - loss: 2.3732 - acc: 0.9081 - mDice: 0.4154 - val_loss: 3.2726 - val_acc: 0.9328 - val_mDice: 0.3716

Epoch 00041: val_mDice did not improve from 0.37387
Epoch 42/300
 - 13s - loss: 2.3407 - acc: 0.9086 - mDice: 0.4216 - val_loss: 3.4735 - val_acc: 0.9293 - val_mDice: 0.3559

Epoch 00042: val_mDice did not improve from 0.37387
Epoch 43/300
 - 13s - loss: 2.3212 - acc: 0.9091 - mDice: 0.4250 - val_loss: 2.9972 - val_acc: 0.9352 - val_mDice: 0.3956

Epoch 00043: val_mDice improved from 0.37387 to 0.39561, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 44/300
 - 13s - loss: 2.2926 - acc: 0.9098 - mDice: 0.4300 - val_loss: 3.0420 - val_acc: 0.9357 - val_mDice: 0.3883

Epoch 00044: val_mDice did not improve from 0.39561
Epoch 45/300
 - 13s - loss: 2.2565 - acc: 0.9107 - mDice: 0.4367 - val_loss: 3.0235 - val_acc: 0.9354 - val_mDice: 0.3963

Epoch 00045: val_mDice improved from 0.39561 to 0.39631, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 13s - loss: 2.2344 - acc: 0.9112 - mDice: 0.4407 - val_loss: 3.1196 - val_acc: 0.9351 - val_mDice: 0.3934

Epoch 00046: val_mDice did not improve from 0.39631
Epoch 47/300
 - 13s - loss: 2.2161 - acc: 0.9118 - mDice: 0.4448 - val_loss: 3.1132 - val_acc: 0.9352 - val_mDice: 0.3923

Epoch 00047: val_mDice did not improve from 0.39631
Epoch 48/300
 - 12s - loss: 2.1978 - acc: 0.9125 - mDice: 0.4490 - val_loss: 3.4839 - val_acc: 0.9335 - val_mDice: 0.3710

Epoch 00048: val_mDice did not improve from 0.39631
Epoch 49/300
 - 12s - loss: 2.1782 - acc: 0.9128 - mDice: 0.4524 - val_loss: 2.9512 - val_acc: 0.9353 - val_mDice: 0.4043

Epoch 00049: val_mDice improved from 0.39631 to 0.40430, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 12s - loss: 2.1656 - acc: 0.9132 - mDice: 0.4554 - val_loss: 3.0572 - val_acc: 0.9355 - val_mDice: 0.4005

Epoch 00050: val_mDice did not improve from 0.40430
Epoch 51/300
 - 13s - loss: 2.1460 - acc: 0.9138 - mDice: 0.4591 - val_loss: 3.3479 - val_acc: 0.9347 - val_mDice: 0.3886

Epoch 00051: val_mDice did not improve from 0.40430
Epoch 52/300
 - 12s - loss: 2.1244 - acc: 0.9145 - mDice: 0.4631 - val_loss: 2.9007 - val_acc: 0.9368 - val_mDice: 0.4171

Epoch 00052: val_mDice improved from 0.40430 to 0.41706, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 12s - loss: 2.0962 - acc: 0.9150 - mDice: 0.4690 - val_loss: 3.1281 - val_acc: 0.9357 - val_mDice: 0.4031

Epoch 00053: val_mDice did not improve from 0.41706
Epoch 54/300
 - 12s - loss: 2.0825 - acc: 0.9158 - mDice: 0.4718 - val_loss: 3.1431 - val_acc: 0.9372 - val_mDice: 0.4110

Epoch 00054: val_mDice did not improve from 0.41706
Epoch 55/300
 - 13s - loss: 2.0594 - acc: 0.9163 - mDice: 0.4773 - val_loss: 3.2467 - val_acc: 0.9346 - val_mDice: 0.4002

Epoch 00055: val_mDice did not improve from 0.41706
Epoch 56/300
 - 15s - loss: 2.0479 - acc: 0.9168 - mDice: 0.4799 - val_loss: 3.3613 - val_acc: 0.9340 - val_mDice: 0.3933

Epoch 00056: val_mDice did not improve from 0.41706
Epoch 57/300
 - 14s - loss: 2.0385 - acc: 0.9168 - mDice: 0.4819 - val_loss: 3.2053 - val_acc: 0.9376 - val_mDice: 0.4090

Epoch 00057: val_mDice did not improve from 0.41706
Epoch 58/300
 - 15s - loss: 2.0234 - acc: 0.9177 - mDice: 0.4855 - val_loss: 3.3177 - val_acc: 0.9329 - val_mDice: 0.3956

Epoch 00058: val_mDice did not improve from 0.41706
Epoch 59/300
 - 15s - loss: 2.0110 - acc: 0.9178 - mDice: 0.4879 - val_loss: 3.2524 - val_acc: 0.9369 - val_mDice: 0.4113

Epoch 00059: val_mDice did not improve from 0.41706
Epoch 60/300
 - 15s - loss: 1.9865 - acc: 0.9187 - mDice: 0.4931 - val_loss: 3.3413 - val_acc: 0.9353 - val_mDice: 0.4052

Epoch 00060: val_mDice did not improve from 0.41706
Epoch 61/300
 - 15s - loss: 1.9747 - acc: 0.9189 - mDice: 0.4964 - val_loss: 3.0882 - val_acc: 0.9376 - val_mDice: 0.4188

Epoch 00061: val_mDice improved from 0.41706 to 0.41880, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 62/300
 - 15s - loss: 1.9657 - acc: 0.9193 - mDice: 0.4980 - val_loss: 3.1977 - val_acc: 0.9367 - val_mDice: 0.4187

Epoch 00062: val_mDice did not improve from 0.41880
Epoch 63/300
 - 16s - loss: 1.9527 - acc: 0.9197 - mDice: 0.5010 - val_loss: 3.2448 - val_acc: 0.9370 - val_mDice: 0.4169

Epoch 00063: val_mDice did not improve from 0.41880
Epoch 64/300
 - 14s - loss: 1.9369 - acc: 0.9202 - mDice: 0.5043 - val_loss: 3.3636 - val_acc: 0.9359 - val_mDice: 0.4088

Epoch 00064: val_mDice did not improve from 0.41880
Epoch 65/300
 - 16s - loss: 1.9248 - acc: 0.9203 - mDice: 0.5072 - val_loss: 3.0569 - val_acc: 0.9385 - val_mDice: 0.4235

Epoch 00065: val_mDice improved from 0.41880 to 0.42352, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 66/300
 - 15s - loss: 1.9165 - acc: 0.9207 - mDice: 0.5096 - val_loss: 3.5148 - val_acc: 0.9385 - val_mDice: 0.4136

Epoch 00066: val_mDice did not improve from 0.42352
Epoch 67/300
 - 15s - loss: 1.9027 - acc: 0.9211 - mDice: 0.5118 - val_loss: 3.3855 - val_acc: 0.9388 - val_mDice: 0.4195

Epoch 00067: val_mDice did not improve from 0.42352
Epoch 68/300
 - 14s - loss: 1.8884 - acc: 0.9217 - mDice: 0.5160 - val_loss: 3.3708 - val_acc: 0.9359 - val_mDice: 0.4082

Epoch 00068: val_mDice did not improve from 0.42352
Epoch 69/300
 - 15s - loss: 1.8863 - acc: 0.9217 - mDice: 0.5163 - val_loss: 3.1213 - val_acc: 0.9398 - val_mDice: 0.4281

Epoch 00069: val_mDice improved from 0.42352 to 0.42811, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 70/300
 - 15s - loss: 1.8701 - acc: 0.9222 - mDice: 0.5202 - val_loss: 3.2629 - val_acc: 0.9377 - val_mDice: 0.4190

Epoch 00070: val_mDice did not improve from 0.42811
Epoch 71/300
 - 15s - loss: 1.8565 - acc: 0.9226 - mDice: 0.5226 - val_loss: 3.2105 - val_acc: 0.9405 - val_mDice: 0.4293

Epoch 00071: val_mDice improved from 0.42811 to 0.42929, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 72/300
 - 15s - loss: 1.8386 - acc: 0.9230 - mDice: 0.5266 - val_loss: 3.6322 - val_acc: 0.9390 - val_mDice: 0.4181

Epoch 00072: val_mDice did not improve from 0.42929
Epoch 73/300
 - 14s - loss: 1.8463 - acc: 0.9232 - mDice: 0.5266 - val_loss: 3.4232 - val_acc: 0.9386 - val_mDice: 0.4209

Epoch 00073: val_mDice did not improve from 0.42929
Epoch 74/300
 - 15s - loss: 1.8281 - acc: 0.9236 - mDice: 0.5294 - val_loss: 3.1009 - val_acc: 0.9393 - val_mDice: 0.4333

Epoch 00074: val_mDice improved from 0.42929 to 0.43334, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 75/300
 - 14s - loss: 1.8221 - acc: 0.9238 - mDice: 0.5308 - val_loss: 3.1895 - val_acc: 0.9369 - val_mDice: 0.4253

Epoch 00075: val_mDice did not improve from 0.43334
Epoch 76/300
 - 15s - loss: 1.8068 - acc: 0.9241 - mDice: 0.5343 - val_loss: 3.1174 - val_acc: 0.9381 - val_mDice: 0.4295

Epoch 00076: val_mDice did not improve from 0.43334
Epoch 77/300
 - 14s - loss: 1.7957 - acc: 0.9245 - mDice: 0.5368 - val_loss: 3.4528 - val_acc: 0.9396 - val_mDice: 0.4258

Epoch 00077: val_mDice did not improve from 0.43334
Epoch 78/300
 - 15s - loss: 1.7905 - acc: 0.9246 - mDice: 0.5382 - val_loss: 2.9703 - val_acc: 0.9415 - val_mDice: 0.4531

Epoch 00078: val_mDice improved from 0.43334 to 0.45309, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 79/300
 - 15s - loss: 1.7863 - acc: 0.9247 - mDice: 0.5392 - val_loss: 3.2296 - val_acc: 0.9423 - val_mDice: 0.4404

Epoch 00079: val_mDice did not improve from 0.45309
Epoch 80/300
 - 16s - loss: 1.7727 - acc: 0.9251 - mDice: 0.5422 - val_loss: 3.3224 - val_acc: 0.9414 - val_mDice: 0.4384

Epoch 00080: val_mDice did not improve from 0.45309
Epoch 81/300
 - 15s - loss: 1.7626 - acc: 0.9253 - mDice: 0.5445 - val_loss: 3.7044 - val_acc: 0.9402 - val_mDice: 0.4209

Epoch 00081: val_mDice did not improve from 0.45309
Epoch 82/300
 - 15s - loss: 1.7553 - acc: 0.9258 - mDice: 0.5463 - val_loss: 3.3078 - val_acc: 0.9404 - val_mDice: 0.4358

Epoch 00082: val_mDice did not improve from 0.45309
Epoch 83/300
 - 13s - loss: 1.7474 - acc: 0.9260 - mDice: 0.5487 - val_loss: 3.6677 - val_acc: 0.9401 - val_mDice: 0.4275

Epoch 00083: val_mDice did not improve from 0.45309
Epoch 84/300
 - 12s - loss: 1.7430 - acc: 0.9261 - mDice: 0.5497 - val_loss: 3.4695 - val_acc: 0.9411 - val_mDice: 0.4326

Epoch 00084: val_mDice did not improve from 0.45309
Epoch 85/300
 - 13s - loss: 1.7383 - acc: 0.9262 - mDice: 0.5501 - val_loss: 3.3429 - val_acc: 0.9407 - val_mDice: 0.4385

Epoch 00085: val_mDice did not improve from 0.45309
Epoch 86/300
 - 12s - loss: 1.7354 - acc: 0.9264 - mDice: 0.5511 - val_loss: 3.5904 - val_acc: 0.9393 - val_mDice: 0.4175

Epoch 00086: val_mDice did not improve from 0.45309
Epoch 87/300
 - 12s - loss: 1.7258 - acc: 0.9266 - mDice: 0.5532 - val_loss: 3.2792 - val_acc: 0.9401 - val_mDice: 0.4383

Epoch 00087: val_mDice did not improve from 0.45309
Epoch 88/300
 - 12s - loss: 1.7159 - acc: 0.9269 - mDice: 0.5556 - val_loss: 3.4217 - val_acc: 0.9395 - val_mDice: 0.4340

Epoch 00088: val_mDice did not improve from 0.45309
Epoch 89/300
 - 13s - loss: 1.7067 - acc: 0.9273 - mDice: 0.5578 - val_loss: 3.5951 - val_acc: 0.9402 - val_mDice: 0.4270

Epoch 00089: val_mDice did not improve from 0.45309
Epoch 90/300
 - 12s - loss: 1.7048 - acc: 0.9275 - mDice: 0.5577 - val_loss: 3.2587 - val_acc: 0.9401 - val_mDice: 0.4397

Epoch 00090: val_mDice did not improve from 0.45309
Epoch 91/300
 - 12s - loss: 1.6892 - acc: 0.9280 - mDice: 0.5612 - val_loss: 3.7039 - val_acc: 0.9402 - val_mDice: 0.4237

Epoch 00091: val_mDice did not improve from 0.45309
Epoch 92/300
 - 13s - loss: 1.6866 - acc: 0.9278 - mDice: 0.5620 - val_loss: 3.4348 - val_acc: 0.9406 - val_mDice: 0.4340

Epoch 00092: val_mDice did not improve from 0.45309
Epoch 93/300
 - 12s - loss: 1.6800 - acc: 0.9280 - mDice: 0.5630 - val_loss: 3.6302 - val_acc: 0.9404 - val_mDice: 0.4297

Epoch 00093: val_mDice did not improve from 0.45309
Epoch 94/300
 - 12s - loss: 1.6755 - acc: 0.9285 - mDice: 0.5656 - val_loss: 3.6232 - val_acc: 0.9393 - val_mDice: 0.4251

Epoch 00094: val_mDice did not improve from 0.45309
Epoch 95/300
 - 13s - loss: 1.6698 - acc: 0.9286 - mDice: 0.5662 - val_loss: 3.5852 - val_acc: 0.9409 - val_mDice: 0.4329

Epoch 00095: val_mDice did not improve from 0.45309
Epoch 96/300
 - 12s - loss: 1.6626 - acc: 0.9289 - mDice: 0.5687 - val_loss: 3.2117 - val_acc: 0.9403 - val_mDice: 0.4412

Epoch 00096: val_mDice did not improve from 0.45309
Epoch 97/300
 - 12s - loss: 1.6621 - acc: 0.9289 - mDice: 0.5684 - val_loss: 3.4674 - val_acc: 0.9411 - val_mDice: 0.4359

Epoch 00097: val_mDice did not improve from 0.45309
Epoch 98/300
 - 12s - loss: 1.6483 - acc: 0.9293 - mDice: 0.5720 - val_loss: 3.4307 - val_acc: 0.9416 - val_mDice: 0.4425

Epoch 00098: val_mDice did not improve from 0.45309
Epoch 99/300
 - 12s - loss: 1.6519 - acc: 0.9293 - mDice: 0.5710 - val_loss: 3.1674 - val_acc: 0.9412 - val_mDice: 0.4497

Epoch 00099: val_mDice did not improve from 0.45309
Epoch 100/300
 - 12s - loss: 1.6399 - acc: 0.9297 - mDice: 0.5740 - val_loss: 3.5294 - val_acc: 0.9403 - val_mDice: 0.4381

Epoch 00100: val_mDice did not improve from 0.45309
Epoch 101/300
 - 12s - loss: 1.6450 - acc: 0.9296 - mDice: 0.5725 - val_loss: 3.3758 - val_acc: 0.9407 - val_mDice: 0.4449

Epoch 00101: val_mDice did not improve from 0.45309
Epoch 102/300
 - 12s - loss: 1.6304 - acc: 0.9300 - mDice: 0.5757 - val_loss: 3.9611 - val_acc: 0.9326 - val_mDice: 0.4117

Epoch 00102: val_mDice did not improve from 0.45309
Epoch 103/300
 - 12s - loss: 1.6241 - acc: 0.9304 - mDice: 0.5772 - val_loss: 3.5796 - val_acc: 0.9409 - val_mDice: 0.4462

Epoch 00103: val_mDice did not improve from 0.45309
Epoch 104/300
 - 12s - loss: 1.6140 - acc: 0.9304 - mDice: 0.5790 - val_loss: 3.4177 - val_acc: 0.9403 - val_mDice: 0.4428

Epoch 00104: val_mDice did not improve from 0.45309
Epoch 105/300
 - 12s - loss: 1.6105 - acc: 0.9306 - mDice: 0.5804 - val_loss: 3.3605 - val_acc: 0.9413 - val_mDice: 0.4473

Epoch 00105: val_mDice did not improve from 0.45309
Epoch 106/300
 - 12s - loss: 1.6120 - acc: 0.9306 - mDice: 0.5805 - val_loss: 3.5851 - val_acc: 0.9425 - val_mDice: 0.4382

Epoch 00106: val_mDice did not improve from 0.45309
Epoch 107/300
 - 12s - loss: 1.6032 - acc: 0.9310 - mDice: 0.5823 - val_loss: 3.4204 - val_acc: 0.9393 - val_mDice: 0.4449

Epoch 00107: val_mDice did not improve from 0.45309
Epoch 108/300
 - 13s - loss: 1.5903 - acc: 0.9313 - mDice: 0.5854 - val_loss: 3.5667 - val_acc: 0.9387 - val_mDice: 0.4490

Epoch 00108: val_mDice did not improve from 0.45309
Restoring model weights from the end of the best epoch
Epoch 00108: early stopping
{'val_loss': [40.07315782138279, 14.875029226144155, 9.135014921426773, 7.086098153321516, 6.4290333933063915, 5.992160502466417, 5.626471407888901, 5.8050847831403924, 5.678872522055393, 5.2980727939201255, 4.7743285314313, 4.559960081286373, 4.208915610842052, 4.1727028373806245, 4.096854456922128, 3.7039713507429477, 3.846433252539663, 3.5153263390862515, 3.2865795423498465, 3.8551123927214315, 3.710387995067452, 3.327916270121932, 3.1295525329630998, 3.2943391942729554, 3.0330813624230877, 3.237604572082914, 3.4515943362688026, 2.9286182650054493, 3.2023500194329593, 3.109876326063559, 3.188105372401575, 3.00805734594663, 3.1185336570654596, 3.1783594518367733, 2.9690745221450925, 3.1204712727949735, 3.8136414760901105, 3.252478792021672, 3.118070785798842, 3.413545326418465, 3.2725941397781884, 3.4734605561853167, 2.997156505783399, 3.0420427039886513, 3.0235473685676144, 3.1196177525978004, 3.113234408199787, 3.4839257264864587, 2.9511879484745718, 3.0572081235725257, 3.347886698320508, 2.9006817301823977, 3.1281399875879288, 3.1430879365209314, 3.246684640912073, 3.361268926278821, 3.2053401930745515, 3.317669474120651, 3.2523997613849738, 3.341331599590679, 3.088208425013969, 3.197675500885539, 3.244785886080492, 3.3636110520345115, 3.0569014117520834, 3.5147683096029576, 3.385470292863569, 3.370814426957319, 3.121344932266289, 3.262890355617163, 3.2104808242681124, 3.632206403007287, 3.4231949765235186, 3.100874947384, 3.189464552948872, 3.1174355226657573, 3.452768752905762, 2.970321164377743, 3.229583060524116, 3.322389973283169, 3.704425464007294, 3.307837887001889, 3.667741669474968, 3.4694817037809464, 3.342871045294617, 3.590447349074696, 3.279230885008084, 3.421698334139018, 3.5950720778089904, 3.2587398463150574, 3.703884208242276, 3.4347771270023215, 3.6301550236752345, 3.623182343984289, 3.5851632330921435, 3.2116605791351978, 3.4674361519781605, 3.430693410248274, 3.1674181571052897, 3.5294031184388412, 3.3758070107460734, 3.9610510332997713, 3.579611166779484, 3.4177458475654325, 3.3604957962275614, 3.5850590366559723, 3.4204471383155104, 3.566747094238443], 'val_acc': [0.9041254719098409, 0.9044757542156038, 0.9047275696481977, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9048053962843758, 0.9049427707990011, 0.9052380692391169, 0.9052312005133856, 0.905068678515298, 0.9064194361368815, 0.9063461479686555, 0.9081799274399167, 0.9085897235643297, 0.9070352344285875, 0.9078937683786664, 0.9167628458568028, 0.919246772925059, 0.9193864720208305, 0.9219368327231634, 0.9218727265085492, 0.9203640052250454, 0.9274153198514666, 0.9258814340546018, 0.9279945123763311, 0.9285531100772676, 0.930746350969587, 0.9315132867722284, 0.92954440060116, 0.9329372871489752, 0.9318383818580991, 0.9243040453820002, 0.9327472448348999, 0.932815949122111, 0.9319665744191125, 0.9328228206861586, 0.929294867174966, 0.9352175110862369, 0.9357028262955802, 0.9353686031841096, 0.935087019488925, 0.9352060357729594, 0.9334523933274406, 0.93532737663814, 0.9354715858186994, 0.9346725940704346, 0.93682690177645, 0.9356662120137896, 0.9372000892957052, 0.9345604635420299, 0.9340361498651051, 0.9376465053785414, 0.9329349824360439, 0.9369299184708368, 0.9352769908450899, 0.937573254108429, 0.9367284604481289, 0.9369780080659049, 0.9359386165936788, 0.9384569525718689, 0.9385485563959394, 0.9388438747042701, 0.9359340752874102, 0.9398191287404015, 0.9376739859580994, 0.940512824626196, 0.9390430280140468, 0.938649282568977, 0.9392742770058768, 0.9369391288076129, 0.938113564536685, 0.939624547958374, 0.9414766743069604, 0.9422733471507118, 0.9414308865865072, 0.9402014584768386, 0.9403823358672005, 0.9401076038678488, 0.9411423972674778, 0.9406753437859672, 0.9393200391814822, 0.9400892768587384, 0.9395398185366676, 0.9402129394667489, 0.9401053161848159, 0.9402495395569574, 0.9406021322522845, 0.9403892216228303, 0.9393498159590221, 0.9409111766588121, 0.9403433941659474, 0.9411400868779137, 0.9415911095482963, 0.941243154661996, 0.9403136684781029, 0.9407371963773455, 0.9326282171975999, 0.940883681887672, 0.9402609722954887, 0.9413461685180664, 0.9425297748474848, 0.9392902680805751, 0.9387362769671849], 'val_mDice': [0.01457099374827175, 0.013042818228270681, 0.015283461366336615, 0.019045084714889526, 0.018535976741640342, 0.023335707418265798, 0.03303033197742133, 0.022644947005790613, 0.025267864333554393, 0.040454932887639315, 0.06291926834022715, 0.07580611852574207, 0.10239651000925473, 0.11550026058795906, 0.12625428643964587, 0.1703725005721762, 0.17173386986056963, 0.212195527429382, 0.23852599035238936, 0.2100711435168272, 0.22715317307128793, 0.2744763462493817, 0.2927944740901391, 0.29509080866617815, 0.3181668810014214, 0.30798878112719175, 0.3000029000852789, 0.3469569944732246, 0.3300039092344897, 0.3360383169991629, 0.3429721524672849, 0.3595935292541981, 0.3601741898982298, 0.3564059257152535, 0.3738664033867064, 0.36746731365010854, 0.3296800178608724, 0.363144218832964, 0.373140437528491, 0.3627981755527712, 0.37157871733818737, 0.3559308953228451, 0.3956062165754182, 0.3883142733857745, 0.3963134363293648, 0.39338859135196325, 0.3923381327518395, 0.371040323837882, 0.4043003735797746, 0.4004997864720367, 0.388587586581707, 0.41705626781497684, 0.4031215544257845, 0.4110195351143678, 0.4001596170876707, 0.3933007713584673, 0.40904415327878224, 0.3955827656955946, 0.4112770887357848, 0.4052495326669443, 0.4188037731108211, 0.418707216779391, 0.41685529195127036, 0.4087580545317559, 0.42352480423592387, 0.4135783884142126, 0.41953567415475845, 0.408157365662711, 0.4281074303601469, 0.418955354818276, 0.4292923318488257, 0.4181174717488743, 0.4209164008498192, 0.43333778991585686, 0.42530072161129545, 0.42950613743492533, 0.4257802830210754, 0.4530864597431251, 0.44038257747888565, 0.4383682592638901, 0.4208595170861199, 0.43584483879662694, 0.42750575074127745, 0.43261123617135344, 0.4384760203815642, 0.41750488164169447, 0.4382892939306441, 0.43399360598552794, 0.4270223134330341, 0.4397361941990398, 0.4236594630139215, 0.4339694215783051, 0.4296698029197398, 0.4251001959755307, 0.43289902061223984, 0.4411642249851, 0.4359122988368784, 0.4424827237214361, 0.44974328701694805, 0.43812307963768643, 0.4449382019894464, 0.41165091735976084, 0.44617219109620365, 0.4428172480492365, 0.44730465433427263, 0.4382261034278643, 0.4449014833995274, 0.4490257286954494], 'loss': [179.23558460836446, 40.02188201729263, 20.667023359430715, 14.345148992235085, 11.449715476976507, 9.828543690603686, 8.766660840703913, 7.969546164991954, 7.331896175831665, 6.82681716391639, 6.398260752635814, 5.99189222500185, 5.630944104008025, 5.299821627307816, 4.993737386972293, 4.711260940427837, 4.425669224385882, 4.1955535198099145, 3.9962315635688506, 3.8316934818428146, 3.673558223079757, 3.5498246915863962, 3.420091560586974, 3.310245817190699, 3.2127745737568913, 3.1226445384307797, 3.0391133302251383, 2.9579302511717143, 2.8776957656874327, 2.8197930506343485, 2.7704384881268354, 2.71465438377979, 2.6632780521757597, 2.6195913111058093, 2.5763371368812322, 2.528331518678829, 2.4936899617386157, 2.462883757446367, 2.434306866589001, 2.393577231891081, 2.3731777564119185, 2.340718793436951, 2.321170842714201, 2.29255201532957, 2.256469990514436, 2.2343516628797837, 2.2160728301573562, 2.1977818030623406, 2.1782413167440526, 2.1655916024063373, 2.146030305689967, 2.12440490214747, 2.096166047696918, 2.0824893587054327, 2.0594438720535444, 2.047861029744631, 2.038540381536747, 2.023388593417959, 2.010967258017073, 1.9865004911067883, 1.9746881228134952, 1.965677561248822, 1.9527449259437162, 1.9369466873591452, 1.9248308451713017, 1.916454930560125, 1.9026833032168977, 1.8884041608126838, 1.8863358920953948, 1.8701125090122683, 1.8565325138310098, 1.8385532702557585, 1.8463331198035027, 1.8281166600840892, 1.8221161129020969, 1.806840974056861, 1.7956909281407245, 1.7904764728231743, 1.7862596757291989, 1.772661568963599, 1.7625584275777202, 1.7552947531275973, 1.7473798770538662, 1.743024707621914, 1.7383174685262544, 1.7353694093739893, 1.7258482476477681, 1.7159359257460698, 1.706656436090964, 1.7048202460502093, 1.6892088520428061, 1.6865844803093624, 1.6799600286614305, 1.6755390535413082, 1.6697809043499645, 1.662571525490994, 1.6620703961363366, 1.6483167949751527, 1.651929200902159, 1.63993092828434, 1.6449887515585249, 1.6304189112918648, 1.6241012751906736, 1.6139813892547625, 1.6105003105966669, 1.612023039845021, 1.603174390143826, 1.5903278238308642], 'acc': [0.43946663883493337, 0.8307681178596178, 0.8590934146500884, 0.865271446400119, 0.8675353071789349, 0.8683631240643586, 0.8687322270509387, 0.8688787648805147, 0.8690111482329559, 0.8690511417430284, 0.8690615706138581, 0.8689679555840545, 0.8689240176185422, 0.8691540228647144, 0.8696608847222256, 0.8709401770412106, 0.8728729168390157, 0.875029079551432, 0.8771092619582456, 0.8796185648048317, 0.8824638584594586, 0.8847617852060419, 0.8874216781591941, 0.8894703940594153, 0.8910580556218464, 0.8930524832967482, 0.8944849656775942, 0.8962272525362273, 0.8976248611782428, 0.8989201447902582, 0.9000667380257014, 0.9010424784021799, 0.9021363955300645, 0.9029234762202804, 0.903857299707886, 0.904626420290226, 0.905542078897492, 0.906209796052322, 0.9069153779799111, 0.907433549275811, 0.9081099325116255, 0.9085888250820527, 0.9091007395679606, 0.9097898414260462, 0.9107052700952089, 0.9112407048704171, 0.9117796103435375, 0.9124555818281216, 0.9127812804641672, 0.913201154507905, 0.9137900670254555, 0.9145305971201155, 0.9149964179104639, 0.9157632673455498, 0.9162996008788227, 0.9167620892099086, 0.9167773615930129, 0.9176576530540382, 0.9178300296982198, 0.9186775190926297, 0.9188507508399607, 0.9192569796717739, 0.9196635706474815, 0.9201564849951328, 0.9202551969747358, 0.920742359847629, 0.9211158023786977, 0.9216802648572258, 0.9216713883386358, 0.9222301343674602, 0.9226292204668417, 0.9230365595974765, 0.9232399356211223, 0.9236002839873332, 0.9237592888862062, 0.9240931259062057, 0.92450671835352, 0.924610503726045, 0.9247190464271478, 0.9250752872438911, 0.9253015346970199, 0.9258065179682706, 0.9259887623929417, 0.9261230912791388, 0.926201271463824, 0.9263884779205895, 0.9266129703748793, 0.9269095191450875, 0.9272630985288324, 0.9274799849156449, 0.927988426865055, 0.9278216334305817, 0.9280343527207375, 0.928549577568408, 0.9285910527715798, 0.928894421004918, 0.9288911102754385, 0.9293302600039484, 0.9293101477857405, 0.9296572379048261, 0.929634320246408, 0.9299656324747879, 0.9303605726787022, 0.9304306888846917, 0.9306070731512173, 0.9306088600150417, 0.9309596334184919, 0.9312738861586286], 'mDice': [0.01643583673717361, 0.014524569518394453, 0.015307062566754376, 0.020307685308343693, 0.025972912031658035, 0.031296344388851614, 0.03691371445834395, 0.041700184813510115, 0.047441047170481886, 0.054653249000862936, 0.06395706502354073, 0.07589351197664095, 0.08998973608011222, 0.10548731828898807, 0.12402120462836203, 0.14309852245813715, 0.1666940036096025, 0.1867599720632683, 0.20427869582350874, 0.22021255741717843, 0.23719200831468978, 0.25045077907503605, 0.26599549774331166, 0.27915342808551175, 0.2907056273500745, 0.3025406799450623, 0.31426418341261037, 0.32513996280754187, 0.33588889460699717, 0.3444949203496359, 0.3513782564008399, 0.3591699314549499, 0.36739334416384867, 0.3732975958938977, 0.381557234839619, 0.38833140721320186, 0.3942194725245852, 0.39977775428803963, 0.4048370484766161, 0.41117831227497625, 0.41540824318980124, 0.42159753366773334, 0.42504668346093943, 0.42998607916617776, 0.43665421217053707, 0.44070651592927007, 0.4448278478650383, 0.44895362273700345, 0.4524472550445378, 0.455383625096247, 0.459092607916677, 0.46313986672310786, 0.4689532586586873, 0.47175751051646286, 0.4773282037147649, 0.4799134933824412, 0.4819345380705125, 0.48548359179832307, 0.48793335469536225, 0.4931446792464829, 0.49641468072411, 0.4979793863395792, 0.5009786878186935, 0.5042846143211224, 0.5071514823635764, 0.5095817414533432, 0.5117641204189928, 0.5159667792728417, 0.5162585553967565, 0.520188542374853, 0.5225960355239818, 0.5265606282440849, 0.5266182414249393, 0.5293963642646197, 0.5307778377902514, 0.5343332907839595, 0.5367995927116419, 0.5382497830958134, 0.5391869140034447, 0.5421945506238928, 0.5444962072974573, 0.5462544259019319, 0.548655802260848, 0.5496578739400495, 0.5500937529742799, 0.5511351846559258, 0.5532381239713261, 0.5556064578800595, 0.557805308607104, 0.5577185133047271, 0.5612186448110283, 0.5620482375632367, 0.563036412753327, 0.5655748623976111, 0.5661645894889913, 0.5687064342768132, 0.5683534939509264, 0.5719690507194176, 0.5709646260873968, 0.5740390599681758, 0.5725062034574574, 0.5756610541852057, 0.5771660800611718, 0.5789554087739242, 0.580378498131539, 0.5804740728062426, 0.5822826169441264, 0.585357724482645]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.32s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.08s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.89s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:37,  1.40s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:04,  1.50s/it]predicting train subjects:   1%|          | 3/285 [00:04<06:59,  1.49s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:35,  1.62s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<07:17,  1.56s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:38,  1.64s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<08:02,  1.74s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<08:19,  1.80s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<08:02,  1.75s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:22,  1.83s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:35,  1.88s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:46,  1.93s/it]predicting train subjects:   5%|▍         | 13/285 [00:23<08:42,  1.92s/it]predicting train subjects:   5%|▍         | 14/285 [00:25<08:39,  1.92s/it]predicting train subjects:   5%|▌         | 15/285 [00:27<08:44,  1.94s/it]predicting train subjects:   6%|▌         | 16/285 [00:29<08:47,  1.96s/it]predicting train subjects:   6%|▌         | 17/285 [00:31<08:41,  1.94s/it]predicting train subjects:   6%|▋         | 18/285 [00:33<08:44,  1.97s/it]predicting train subjects:   7%|▋         | 19/285 [00:35<08:46,  1.98s/it]predicting train subjects:   7%|▋         | 20/285 [00:37<08:48,  1.99s/it]predicting train subjects:   7%|▋         | 21/285 [00:39<08:42,  1.98s/it]predicting train subjects:   8%|▊         | 22/285 [00:41<08:39,  1.98s/it]predicting train subjects:   8%|▊         | 23/285 [00:43<08:40,  1.99s/it]predicting train subjects:   8%|▊         | 24/285 [00:45<08:37,  1.98s/it]predicting train subjects:   9%|▉         | 25/285 [00:47<08:35,  1.98s/it]predicting train subjects:   9%|▉         | 26/285 [00:49<08:33,  1.98s/it]predicting train subjects:   9%|▉         | 27/285 [00:51<08:33,  1.99s/it]predicting train subjects:  10%|▉         | 28/285 [00:52<08:27,  1.97s/it]predicting train subjects:  10%|█         | 29/285 [00:54<08:25,  1.98s/it]predicting train subjects:  11%|█         | 30/285 [00:56<08:15,  1.94s/it]predicting train subjects:  11%|█         | 31/285 [00:58<08:28,  2.00s/it]predicting train subjects:  11%|█         | 32/285 [01:00<08:18,  1.97s/it]predicting train subjects:  12%|█▏        | 33/285 [01:02<08:09,  1.94s/it]predicting train subjects:  12%|█▏        | 34/285 [01:04<08:05,  1.93s/it]predicting train subjects:  12%|█▏        | 35/285 [01:06<08:00,  1.92s/it]predicting train subjects:  13%|█▎        | 36/285 [01:08<07:53,  1.90s/it]predicting train subjects:  13%|█▎        | 37/285 [01:10<07:46,  1.88s/it]predicting train subjects:  13%|█▎        | 38/285 [01:12<07:42,  1.87s/it]predicting train subjects:  14%|█▎        | 39/285 [01:13<07:37,  1.86s/it]predicting train subjects:  14%|█▍        | 40/285 [01:15<07:30,  1.84s/it]predicting train subjects:  14%|█▍        | 41/285 [01:17<07:27,  1.83s/it]predicting train subjects:  15%|█▍        | 42/285 [01:19<07:27,  1.84s/it]predicting train subjects:  15%|█▌        | 43/285 [01:21<07:36,  1.89s/it]predicting train subjects:  15%|█▌        | 44/285 [01:23<07:34,  1.89s/it]predicting train subjects:  16%|█▌        | 45/285 [01:25<07:32,  1.88s/it]predicting train subjects:  16%|█▌        | 46/285 [01:26<07:08,  1.79s/it]predicting train subjects:  16%|█▋        | 47/285 [01:28<06:51,  1.73s/it]predicting train subjects:  17%|█▋        | 48/285 [01:29<06:42,  1.70s/it]predicting train subjects:  17%|█▋        | 49/285 [01:31<06:36,  1.68s/it]predicting train subjects:  18%|█▊        | 50/285 [01:33<06:26,  1.64s/it]predicting train subjects:  18%|█▊        | 51/285 [01:34<06:25,  1.65s/it]predicting train subjects:  18%|█▊        | 52/285 [01:36<06:23,  1.64s/it]predicting train subjects:  19%|█▊        | 53/285 [01:38<06:19,  1.63s/it]predicting train subjects:  19%|█▉        | 54/285 [01:39<06:11,  1.61s/it]predicting train subjects:  19%|█▉        | 55/285 [01:41<06:11,  1.61s/it]predicting train subjects:  20%|█▉        | 56/285 [01:42<06:09,  1.62s/it]predicting train subjects:  20%|██        | 57/285 [01:44<06:06,  1.61s/it]predicting train subjects:  20%|██        | 58/285 [01:46<06:05,  1.61s/it]predicting train subjects:  21%|██        | 59/285 [01:47<06:02,  1.60s/it]predicting train subjects:  21%|██        | 60/285 [01:49<06:04,  1.62s/it]predicting train subjects:  21%|██▏       | 61/285 [01:50<06:05,  1.63s/it]predicting train subjects:  22%|██▏       | 62/285 [01:52<06:02,  1.63s/it]predicting train subjects:  22%|██▏       | 63/285 [01:54<06:00,  1.63s/it]predicting train subjects:  22%|██▏       | 64/285 [01:55<06:07,  1.66s/it]predicting train subjects:  23%|██▎       | 65/285 [01:57<06:25,  1.75s/it]predicting train subjects:  23%|██▎       | 66/285 [01:59<06:26,  1.76s/it]predicting train subjects:  24%|██▎       | 67/285 [02:01<06:16,  1.73s/it]predicting train subjects:  24%|██▍       | 68/285 [02:02<06:09,  1.70s/it]predicting train subjects:  24%|██▍       | 69/285 [02:04<06:05,  1.69s/it]predicting train subjects:  25%|██▍       | 70/285 [02:06<06:01,  1.68s/it]predicting train subjects:  25%|██▍       | 71/285 [02:07<05:56,  1.67s/it]predicting train subjects:  25%|██▌       | 72/285 [02:09<05:52,  1.66s/it]predicting train subjects:  26%|██▌       | 73/285 [02:11<05:55,  1.67s/it]predicting train subjects:  26%|██▌       | 74/285 [02:12<05:53,  1.68s/it]predicting train subjects:  26%|██▋       | 75/285 [02:14<05:50,  1.67s/it]predicting train subjects:  27%|██▋       | 76/285 [02:16<05:45,  1.65s/it]predicting train subjects:  27%|██▋       | 77/285 [02:17<05:43,  1.65s/it]predicting train subjects:  27%|██▋       | 78/285 [02:19<05:43,  1.66s/it]predicting train subjects:  28%|██▊       | 79/285 [02:21<05:41,  1.66s/it]predicting train subjects:  28%|██▊       | 80/285 [02:22<05:39,  1.65s/it]predicting train subjects:  28%|██▊       | 81/285 [02:24<05:38,  1.66s/it]predicting train subjects:  29%|██▉       | 82/285 [02:26<05:43,  1.69s/it]predicting train subjects:  29%|██▉       | 83/285 [02:27<05:42,  1.70s/it]predicting train subjects:  29%|██▉       | 84/285 [02:29<05:55,  1.77s/it]predicting train subjects:  30%|██▉       | 85/285 [02:31<06:04,  1.82s/it]predicting train subjects:  30%|███       | 86/285 [02:33<06:06,  1.84s/it]predicting train subjects:  31%|███       | 87/285 [02:35<06:07,  1.86s/it]predicting train subjects:  31%|███       | 88/285 [02:37<06:16,  1.91s/it]predicting train subjects:  31%|███       | 89/285 [02:39<06:16,  1.92s/it]predicting train subjects:  32%|███▏      | 90/285 [02:41<06:08,  1.89s/it]predicting train subjects:  32%|███▏      | 91/285 [02:43<06:07,  1.90s/it]predicting train subjects:  32%|███▏      | 92/285 [02:45<06:04,  1.89s/it]predicting train subjects:  33%|███▎      | 93/285 [02:47<06:06,  1.91s/it]predicting train subjects:  33%|███▎      | 94/285 [02:49<06:02,  1.90s/it]predicting train subjects:  33%|███▎      | 95/285 [02:50<05:57,  1.88s/it]predicting train subjects:  34%|███▎      | 96/285 [02:52<05:54,  1.87s/it]predicting train subjects:  34%|███▍      | 97/285 [02:54<05:48,  1.85s/it]predicting train subjects:  34%|███▍      | 98/285 [02:56<05:47,  1.86s/it]predicting train subjects:  35%|███▍      | 99/285 [02:58<05:48,  1.87s/it]predicting train subjects:  35%|███▌      | 100/285 [03:00<05:43,  1.86s/it]predicting train subjects:  35%|███▌      | 101/285 [03:01<05:40,  1.85s/it]predicting train subjects:  36%|███▌      | 102/285 [03:03<05:41,  1.87s/it]predicting train subjects:  36%|███▌      | 103/285 [03:05<05:38,  1.86s/it]predicting train subjects:  36%|███▋      | 104/285 [03:07<05:37,  1.86s/it]predicting train subjects:  37%|███▋      | 105/285 [03:09<05:32,  1.85s/it]predicting train subjects:  37%|███▋      | 106/285 [03:11<05:29,  1.84s/it]predicting train subjects:  38%|███▊      | 107/285 [03:13<05:23,  1.82s/it]predicting train subjects:  38%|███▊      | 108/285 [03:14<05:19,  1.81s/it]predicting train subjects:  38%|███▊      | 109/285 [03:16<05:17,  1.80s/it]predicting train subjects:  39%|███▊      | 110/285 [03:18<05:17,  1.81s/it]predicting train subjects:  39%|███▉      | 111/285 [03:20<05:18,  1.83s/it]predicting train subjects:  39%|███▉      | 112/285 [03:22<05:12,  1.81s/it]predicting train subjects:  40%|███▉      | 113/285 [03:23<05:12,  1.82s/it]predicting train subjects:  40%|████      | 114/285 [03:25<05:10,  1.82s/it]predicting train subjects:  40%|████      | 115/285 [03:27<05:07,  1.81s/it]predicting train subjects:  41%|████      | 116/285 [03:29<05:04,  1.80s/it]predicting train subjects:  41%|████      | 117/285 [03:31<05:01,  1.79s/it]predicting train subjects:  41%|████▏     | 118/285 [03:32<05:00,  1.80s/it]predicting train subjects:  42%|████▏     | 119/285 [03:34<04:59,  1.80s/it]predicting train subjects:  42%|████▏     | 120/285 [03:36<04:57,  1.81s/it]predicting train subjects:  42%|████▏     | 121/285 [03:38<04:47,  1.75s/it]predicting train subjects:  43%|████▎     | 122/285 [03:39<04:29,  1.65s/it]predicting train subjects:  43%|████▎     | 123/285 [03:40<04:18,  1.59s/it]predicting train subjects:  44%|████▎     | 124/285 [03:42<04:19,  1.61s/it]predicting train subjects:  44%|████▍     | 125/285 [03:44<04:21,  1.64s/it]predicting train subjects:  44%|████▍     | 126/285 [03:46<04:25,  1.67s/it]predicting train subjects:  45%|████▍     | 127/285 [03:47<04:22,  1.66s/it]predicting train subjects:  45%|████▍     | 128/285 [03:49<04:17,  1.64s/it]predicting train subjects:  45%|████▌     | 129/285 [03:50<04:15,  1.64s/it]predicting train subjects:  46%|████▌     | 130/285 [03:52<04:14,  1.64s/it]predicting train subjects:  46%|████▌     | 131/285 [03:54<04:09,  1.62s/it]predicting train subjects:  46%|████▋     | 132/285 [03:55<04:09,  1.63s/it]predicting train subjects:  47%|████▋     | 133/285 [03:57<04:05,  1.62s/it]predicting train subjects:  47%|████▋     | 134/285 [03:59<04:03,  1.61s/it]predicting train subjects:  47%|████▋     | 135/285 [04:00<04:02,  1.62s/it]predicting train subjects:  48%|████▊     | 136/285 [04:02<04:04,  1.64s/it]predicting train subjects:  48%|████▊     | 137/285 [04:03<04:02,  1.64s/it]predicting train subjects:  48%|████▊     | 138/285 [04:05<04:01,  1.64s/it]predicting train subjects:  49%|████▉     | 139/285 [04:07<04:00,  1.65s/it]predicting train subjects:  49%|████▉     | 140/285 [04:08<03:59,  1.65s/it]predicting train subjects:  49%|████▉     | 141/285 [04:10<03:56,  1.64s/it]predicting train subjects:  50%|████▉     | 142/285 [04:12<03:48,  1.60s/it]predicting train subjects:  50%|█████     | 143/285 [04:13<03:40,  1.55s/it]predicting train subjects:  51%|█████     | 144/285 [04:14<03:33,  1.52s/it]predicting train subjects:  51%|█████     | 145/285 [04:16<03:29,  1.50s/it]predicting train subjects:  51%|█████     | 146/285 [04:17<03:26,  1.48s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:19<03:24,  1.48s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:20<03:22,  1.48s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:22<03:18,  1.46s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:23<03:15,  1.45s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:25<03:13,  1.44s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:26<03:10,  1.43s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:27<03:10,  1.44s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:29<03:11,  1.46s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:30<03:09,  1.46s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:32<03:10,  1.47s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:33<03:10,  1.49s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:35<03:07,  1.48s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:36<03:06,  1.48s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:38<03:04,  1.47s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:39<02:58,  1.44s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:41<02:54,  1.42s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:42<02:53,  1.42s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:43<02:51,  1.42s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:45<02:49,  1.41s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:46<02:47,  1.41s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:48<02:46,  1.41s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:49<02:44,  1.40s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:50<02:43,  1.41s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:52<02:43,  1.42s/it]predicting train subjects:  60%|██████    | 171/285 [04:53<02:42,  1.43s/it]predicting train subjects:  60%|██████    | 172/285 [04:55<02:43,  1.44s/it]predicting train subjects:  61%|██████    | 173/285 [04:56<02:40,  1.44s/it]predicting train subjects:  61%|██████    | 174/285 [04:58<02:38,  1.43s/it]predicting train subjects:  61%|██████▏   | 175/285 [04:59<02:35,  1.41s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:00<02:33,  1.41s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:02<02:33,  1.42s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:03<02:29,  1.40s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:05<02:25,  1.38s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:06<02:24,  1.37s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:07<02:23,  1.38s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:09<02:21,  1.37s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:10<02:19,  1.37s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:11<02:18,  1.37s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:13<02:17,  1.37s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:14<02:16,  1.38s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:16<02:16,  1.39s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:17<02:15,  1.39s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:18<02:13,  1.39s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:20<02:11,  1.39s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:21<02:10,  1.39s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:22<02:08,  1.38s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:24<02:06,  1.38s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:25<02:05,  1.38s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:27<02:04,  1.39s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:28<02:10,  1.47s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:30<02:14,  1.52s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:32<02:16,  1.57s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:33<02:16,  1.59s/it]predicting train subjects:  70%|███████   | 200/285 [05:35<02:17,  1.61s/it]predicting train subjects:  71%|███████   | 201/285 [05:37<02:17,  1.64s/it]predicting train subjects:  71%|███████   | 202/285 [05:38<02:17,  1.66s/it]predicting train subjects:  71%|███████   | 203/285 [05:40<02:15,  1.65s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:42<02:14,  1.66s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:43<02:13,  1.67s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:45<02:12,  1.67s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:47<02:10,  1.67s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:48<02:07,  1.66s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:50<02:05,  1.65s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:52<02:03,  1.64s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:53<02:01,  1.64s/it]predicting train subjects:  74%|███████▍  | 212/285 [05:55<01:59,  1.63s/it]predicting train subjects:  75%|███████▍  | 213/285 [05:56<01:58,  1.65s/it]predicting train subjects:  75%|███████▌  | 214/285 [05:58<01:53,  1.60s/it]predicting train subjects:  75%|███████▌  | 215/285 [05:59<01:48,  1.55s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:01<01:44,  1.52s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:02<01:43,  1.52s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:04<01:39,  1.49s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:05<01:38,  1.49s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:07<01:36,  1.48s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:08<01:34,  1.47s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:10<01:31,  1.44s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:11<01:29,  1.44s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:12<01:27,  1.43s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:14<01:25,  1.42s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:15<01:23,  1.41s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:17<01:22,  1.42s/it]predicting train subjects:  80%|████████  | 228/285 [06:18<01:21,  1.42s/it]predicting train subjects:  80%|████████  | 229/285 [06:20<01:20,  1.44s/it]predicting train subjects:  81%|████████  | 230/285 [06:21<01:19,  1.44s/it]predicting train subjects:  81%|████████  | 231/285 [06:22<01:18,  1.45s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:24<01:23,  1.57s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:26<01:26,  1.66s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:28<01:29,  1.76s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:30<01:30,  1.82s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:32<01:29,  1.83s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:34<01:27,  1.83s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:36<01:25,  1.82s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:37<01:24,  1.83s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:39<01:22,  1.84s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:41<01:20,  1.82s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:43<01:18,  1.83s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:45<01:17,  1.84s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:47<01:17,  1.89s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:49<01:15,  1.88s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:51<01:12,  1.87s/it]predicting train subjects:  87%|████████▋ | 247/285 [06:52<01:10,  1.87s/it]predicting train subjects:  87%|████████▋ | 248/285 [06:54<01:09,  1.87s/it]predicting train subjects:  87%|████████▋ | 249/285 [06:56<01:06,  1.85s/it]predicting train subjects:  88%|████████▊ | 250/285 [06:58<01:00,  1.73s/it]predicting train subjects:  88%|████████▊ | 251/285 [06:59<00:56,  1.66s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:00<00:52,  1.58s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:02<00:48,  1.52s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:03<00:45,  1.47s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:05<00:43,  1.44s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:06<00:41,  1.42s/it]predicting train subjects:  90%|█████████ | 257/285 [07:07<00:39,  1.40s/it]predicting train subjects:  91%|█████████ | 258/285 [07:09<00:38,  1.41s/it]predicting train subjects:  91%|█████████ | 259/285 [07:10<00:36,  1.39s/it]predicting train subjects:  91%|█████████ | 260/285 [07:11<00:34,  1.39s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:13<00:32,  1.37s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:14<00:31,  1.38s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:16<00:30,  1.38s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:17<00:29,  1.39s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:18<00:27,  1.40s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:20<00:26,  1.39s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:21<00:24,  1.38s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:23<00:26,  1.56s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:25<00:26,  1.65s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:27<00:25,  1.71s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:29<00:25,  1.79s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:31<00:23,  1.84s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:33<00:22,  1.86s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:34<00:20,  1.86s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:36<00:18,  1.86s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:38<00:16,  1.86s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:40<00:15,  1.88s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:42<00:13,  1.90s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:44<00:11,  1.88s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:46<00:09,  1.87s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:48<00:07,  1.85s/it]predicting train subjects:  99%|█████████▉| 282/285 [07:49<00:05,  1.84s/it]predicting train subjects:  99%|█████████▉| 283/285 [07:51<00:03,  1.87s/it]predicting train subjects: 100%|█████████▉| 284/285 [07:53<00:01,  1.88s/it]predicting train subjects: 100%|██████████| 285/285 [07:55<00:00,  1.87s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:41,  1.41s/it]Loading train:   1%|          | 2/285 [00:02<06:49,  1.45s/it]Loading train:   1%|          | 3/285 [00:04<06:32,  1.39s/it]Loading train:   1%|▏         | 4/285 [00:05<06:50,  1.46s/it]Loading train:   2%|▏         | 5/285 [00:07<06:32,  1.40s/it]Loading train:   2%|▏         | 6/285 [00:08<06:51,  1.47s/it]Loading train:   2%|▏         | 7/285 [00:10<07:30,  1.62s/it]Loading train:   3%|▎         | 8/285 [00:12<07:32,  1.63s/it]Loading train:   3%|▎         | 9/285 [00:13<07:10,  1.56s/it]Loading train:   4%|▎         | 10/285 [00:15<06:46,  1.48s/it]Loading train:   4%|▍         | 11/285 [00:16<06:24,  1.40s/it]Loading train:   4%|▍         | 12/285 [00:17<06:05,  1.34s/it]Loading train:   5%|▍         | 13/285 [00:18<05:52,  1.29s/it]Loading train:   5%|▍         | 14/285 [00:19<05:43,  1.27s/it]Loading train:   5%|▌         | 15/285 [00:21<05:52,  1.30s/it]Loading train:   6%|▌         | 16/285 [00:22<05:50,  1.30s/it]Loading train:   6%|▌         | 17/285 [00:23<05:46,  1.29s/it]Loading train:   6%|▋         | 18/285 [00:24<05:32,  1.24s/it]Loading train:   7%|▋         | 19/285 [00:26<05:22,  1.21s/it]Loading train:   7%|▋         | 20/285 [00:27<05:16,  1.19s/it]Loading train:   7%|▋         | 21/285 [00:28<05:10,  1.18s/it]Loading train:   8%|▊         | 22/285 [00:29<05:14,  1.20s/it]Loading train:   8%|▊         | 23/285 [00:30<05:12,  1.19s/it]Loading train:   8%|▊         | 24/285 [00:32<05:14,  1.20s/it]Loading train:   9%|▉         | 25/285 [00:33<05:19,  1.23s/it]Loading train:   9%|▉         | 26/285 [00:34<05:18,  1.23s/it]Loading train:   9%|▉         | 27/285 [00:35<05:31,  1.28s/it]Loading train:  10%|▉         | 28/285 [00:37<05:13,  1.22s/it]Loading train:  10%|█         | 29/285 [00:38<04:57,  1.16s/it]Loading train:  11%|█         | 30/285 [00:39<04:50,  1.14s/it]Loading train:  11%|█         | 31/285 [00:40<04:38,  1.10s/it]Loading train:  11%|█         | 32/285 [00:41<04:36,  1.09s/it]Loading train:  12%|█▏        | 33/285 [00:42<04:29,  1.07s/it]Loading train:  12%|█▏        | 34/285 [00:43<04:19,  1.03s/it]Loading train:  12%|█▏        | 35/285 [00:44<04:20,  1.04s/it]Loading train:  13%|█▎        | 36/285 [00:45<04:19,  1.04s/it]Loading train:  13%|█▎        | 37/285 [00:46<04:13,  1.02s/it]Loading train:  13%|█▎        | 38/285 [00:47<04:08,  1.00s/it]Loading train:  14%|█▎        | 39/285 [00:48<04:06,  1.00s/it]Loading train:  14%|█▍        | 40/285 [00:49<04:03,  1.01it/s]Loading train:  14%|█▍        | 41/285 [00:50<03:59,  1.02it/s]Loading train:  15%|█▍        | 42/285 [00:51<03:59,  1.01it/s]Loading train:  15%|█▌        | 43/285 [00:52<03:58,  1.02it/s]Loading train:  15%|█▌        | 44/285 [00:53<03:54,  1.03it/s]Loading train:  16%|█▌        | 45/285 [00:54<03:55,  1.02it/s]Loading train:  16%|█▌        | 46/285 [00:55<03:56,  1.01it/s]Loading train:  16%|█▋        | 47/285 [00:55<03:49,  1.04it/s]Loading train:  17%|█▋        | 48/285 [00:56<03:46,  1.05it/s]Loading train:  17%|█▋        | 49/285 [00:57<03:44,  1.05it/s]Loading train:  18%|█▊        | 50/285 [00:58<03:45,  1.04it/s]Loading train:  18%|█▊        | 51/285 [00:59<03:48,  1.03it/s]Loading train:  18%|█▊        | 52/285 [01:00<03:45,  1.03it/s]Loading train:  19%|█▊        | 53/285 [01:01<03:38,  1.06it/s]Loading train:  19%|█▉        | 54/285 [01:02<03:34,  1.08it/s]Loading train:  19%|█▉        | 55/285 [01:03<03:33,  1.08it/s]Loading train:  20%|█▉        | 56/285 [01:04<03:30,  1.09it/s]Loading train:  20%|██        | 57/285 [01:05<03:27,  1.10it/s]Loading train:  20%|██        | 58/285 [01:06<03:30,  1.08it/s]Loading train:  21%|██        | 59/285 [01:07<03:29,  1.08it/s]Loading train:  21%|██        | 60/285 [01:08<03:25,  1.09it/s]Loading train:  21%|██▏       | 61/285 [01:08<03:21,  1.11it/s]Loading train:  22%|██▏       | 62/285 [01:09<03:23,  1.10it/s]Loading train:  22%|██▏       | 63/285 [01:10<03:26,  1.08it/s]Loading train:  22%|██▏       | 64/285 [01:12<04:03,  1.10s/it]Loading train:  23%|██▎       | 65/285 [01:13<04:38,  1.27s/it]Loading train:  23%|██▎       | 66/285 [01:15<04:43,  1.29s/it]Loading train:  24%|██▎       | 67/285 [01:16<04:26,  1.22s/it]Loading train:  24%|██▍       | 68/285 [01:17<04:08,  1.14s/it]Loading train:  24%|██▍       | 69/285 [01:18<04:05,  1.14s/it]Loading train:  25%|██▍       | 70/285 [01:19<03:52,  1.08s/it]Loading train:  25%|██▍       | 71/285 [01:20<03:45,  1.05s/it]Loading train:  25%|██▌       | 72/285 [01:21<03:39,  1.03s/it]Loading train:  26%|██▌       | 73/285 [01:22<03:34,  1.01s/it]Loading train:  26%|██▌       | 74/285 [01:23<03:32,  1.01s/it]Loading train:  26%|██▋       | 75/285 [01:24<03:28,  1.01it/s]Loading train:  27%|██▋       | 76/285 [01:25<03:32,  1.02s/it]Loading train:  27%|██▋       | 77/285 [01:26<03:25,  1.01it/s]Loading train:  27%|██▋       | 78/285 [01:27<03:22,  1.02it/s]Loading train:  28%|██▊       | 79/285 [01:28<03:24,  1.01it/s]Loading train:  28%|██▊       | 80/285 [01:29<03:21,  1.02it/s]Loading train:  28%|██▊       | 81/285 [01:30<03:21,  1.01it/s]Loading train:  29%|██▉       | 82/285 [01:31<03:17,  1.03it/s]Loading train:  29%|██▉       | 83/285 [01:32<03:17,  1.02it/s]Loading train:  29%|██▉       | 84/285 [01:33<03:13,  1.04it/s]Loading train:  30%|██▉       | 85/285 [01:34<03:17,  1.01it/s]Loading train:  30%|███       | 86/285 [01:35<03:18,  1.00it/s]Loading train:  31%|███       | 87/285 [01:36<03:17,  1.00it/s]Loading train:  31%|███       | 88/285 [01:37<03:13,  1.02it/s]Loading train:  31%|███       | 89/285 [01:38<03:15,  1.00it/s]Loading train:  32%|███▏      | 90/285 [01:39<03:20,  1.03s/it]Loading train:  32%|███▏      | 91/285 [01:40<03:22,  1.04s/it]Loading train:  32%|███▏      | 92/285 [01:41<03:22,  1.05s/it]Loading train:  33%|███▎      | 93/285 [01:42<03:21,  1.05s/it]Loading train:  33%|███▎      | 94/285 [01:43<03:22,  1.06s/it]Loading train:  33%|███▎      | 95/285 [01:44<03:21,  1.06s/it]Loading train:  34%|███▎      | 96/285 [01:45<03:20,  1.06s/it]Loading train:  34%|███▍      | 97/285 [01:46<03:20,  1.06s/it]Loading train:  34%|███▍      | 98/285 [01:47<03:24,  1.09s/it]Loading train:  35%|███▍      | 99/285 [01:48<03:20,  1.08s/it]Loading train:  35%|███▌      | 100/285 [01:50<03:20,  1.09s/it]Loading train:  35%|███▌      | 101/285 [01:51<03:22,  1.10s/it]Loading train:  36%|███▌      | 102/285 [01:52<03:22,  1.11s/it]Loading train:  36%|███▌      | 103/285 [01:53<03:26,  1.13s/it]Loading train:  36%|███▋      | 104/285 [01:54<03:20,  1.11s/it]Loading train:  37%|███▋      | 105/285 [01:55<03:17,  1.09s/it]Loading train:  37%|███▋      | 106/285 [01:56<03:17,  1.11s/it]Loading train:  38%|███▊      | 107/285 [01:57<03:13,  1.09s/it]Loading train:  38%|███▊      | 108/285 [01:58<03:12,  1.08s/it]Loading train:  38%|███▊      | 109/285 [01:59<03:08,  1.07s/it]Loading train:  39%|███▊      | 110/285 [02:00<03:05,  1.06s/it]Loading train:  39%|███▉      | 111/285 [02:01<03:01,  1.04s/it]Loading train:  39%|███▉      | 112/285 [02:02<03:01,  1.05s/it]Loading train:  40%|███▉      | 113/285 [02:04<03:00,  1.05s/it]Loading train:  40%|████      | 114/285 [02:05<02:58,  1.05s/it]Loading train:  40%|████      | 115/285 [02:06<02:59,  1.06s/it]Loading train:  41%|████      | 116/285 [02:07<02:57,  1.05s/it]Loading train:  41%|████      | 117/285 [02:08<02:56,  1.05s/it]Loading train:  41%|████▏     | 118/285 [02:09<03:00,  1.08s/it]Loading train:  42%|████▏     | 119/285 [02:10<02:56,  1.07s/it]Loading train:  42%|████▏     | 120/285 [02:11<02:53,  1.05s/it]Loading train:  42%|████▏     | 121/285 [02:12<03:10,  1.16s/it]Loading train:  43%|████▎     | 122/285 [02:14<03:10,  1.17s/it]Loading train:  43%|████▎     | 123/285 [02:15<03:18,  1.22s/it]Loading train:  44%|████▎     | 124/285 [02:16<03:07,  1.17s/it]Loading train:  44%|████▍     | 125/285 [02:17<02:55,  1.09s/it]Loading train:  44%|████▍     | 126/285 [02:18<02:44,  1.04s/it]Loading train:  45%|████▍     | 127/285 [02:19<02:36,  1.01it/s]Loading train:  45%|████▍     | 128/285 [02:20<02:33,  1.02it/s]Loading train:  45%|████▌     | 129/285 [02:20<02:29,  1.04it/s]Loading train:  46%|████▌     | 130/285 [02:21<02:24,  1.07it/s]Loading train:  46%|████▌     | 131/285 [02:22<02:23,  1.08it/s]Loading train:  46%|████▋     | 132/285 [02:23<02:19,  1.09it/s]Loading train:  47%|████▋     | 133/285 [02:24<02:17,  1.10it/s]Loading train:  47%|████▋     | 134/285 [02:25<02:15,  1.11it/s]Loading train:  47%|████▋     | 135/285 [02:26<02:16,  1.10it/s]Loading train:  48%|████▊     | 136/285 [02:27<02:15,  1.10it/s]Loading train:  48%|████▊     | 137/285 [02:28<02:14,  1.10it/s]Loading train:  48%|████▊     | 138/285 [02:29<02:13,  1.10it/s]Loading train:  49%|████▉     | 139/285 [02:29<02:11,  1.11it/s]Loading train:  49%|████▉     | 140/285 [02:30<02:10,  1.11it/s]Loading train:  49%|████▉     | 141/285 [02:31<02:13,  1.08it/s]Loading train:  50%|████▉     | 142/285 [02:32<02:12,  1.08it/s]Loading train:  50%|█████     | 143/285 [02:33<02:13,  1.07it/s]Loading train:  51%|█████     | 144/285 [02:34<02:10,  1.08it/s]Loading train:  51%|█████     | 145/285 [02:35<02:09,  1.08it/s]Loading train:  51%|█████     | 146/285 [02:36<02:05,  1.11it/s]Loading train:  52%|█████▏    | 147/285 [02:37<02:02,  1.13it/s]Loading train:  52%|█████▏    | 148/285 [02:38<02:02,  1.11it/s]Loading train:  52%|█████▏    | 149/285 [02:39<02:03,  1.10it/s]Loading train:  53%|█████▎    | 150/285 [02:40<02:06,  1.06it/s]Loading train:  53%|█████▎    | 151/285 [02:41<02:03,  1.08it/s]Loading train:  53%|█████▎    | 152/285 [02:41<02:02,  1.09it/s]Loading train:  54%|█████▎    | 153/285 [02:42<02:00,  1.09it/s]Loading train:  54%|█████▍    | 154/285 [02:43<01:57,  1.11it/s]Loading train:  54%|█████▍    | 155/285 [02:44<01:54,  1.13it/s]Loading train:  55%|█████▍    | 156/285 [02:45<01:52,  1.15it/s]Loading train:  55%|█████▌    | 157/285 [02:46<01:54,  1.12it/s]Loading train:  55%|█████▌    | 158/285 [02:47<01:55,  1.10it/s]Loading train:  56%|█████▌    | 159/285 [02:48<01:54,  1.10it/s]Loading train:  56%|█████▌    | 160/285 [02:49<01:53,  1.10it/s]Loading train:  56%|█████▋    | 161/285 [02:49<01:50,  1.12it/s]Loading train:  57%|█████▋    | 162/285 [02:50<01:45,  1.16it/s]Loading train:  57%|█████▋    | 163/285 [02:51<01:44,  1.16it/s]Loading train:  58%|█████▊    | 164/285 [02:52<01:44,  1.16it/s]Loading train:  58%|█████▊    | 165/285 [02:53<01:42,  1.17it/s]Loading train:  58%|█████▊    | 166/285 [02:54<01:40,  1.18it/s]Loading train:  59%|█████▊    | 167/285 [02:55<01:42,  1.16it/s]Loading train:  59%|█████▉    | 168/285 [02:55<01:42,  1.14it/s]Loading train:  59%|█████▉    | 169/285 [02:56<01:38,  1.17it/s]Loading train:  60%|█████▉    | 170/285 [02:57<01:36,  1.19it/s]Loading train:  60%|██████    | 171/285 [02:58<01:36,  1.18it/s]Loading train:  60%|██████    | 172/285 [02:59<01:37,  1.15it/s]Loading train:  61%|██████    | 173/285 [03:00<01:37,  1.15it/s]Loading train:  61%|██████    | 174/285 [03:01<01:40,  1.11it/s]Loading train:  61%|██████▏   | 175/285 [03:02<01:38,  1.11it/s]Loading train:  62%|██████▏   | 176/285 [03:02<01:37,  1.11it/s]Loading train:  62%|██████▏   | 177/285 [03:03<01:35,  1.13it/s]Loading train:  62%|██████▏   | 178/285 [03:04<01:38,  1.09it/s]Loading train:  63%|██████▎   | 179/285 [03:05<01:34,  1.12it/s]Loading train:  63%|██████▎   | 180/285 [03:06<01:35,  1.09it/s]Loading train:  64%|██████▎   | 181/285 [03:07<01:35,  1.09it/s]Loading train:  64%|██████▍   | 182/285 [03:08<01:35,  1.08it/s]Loading train:  64%|██████▍   | 183/285 [03:09<01:33,  1.09it/s]Loading train:  65%|██████▍   | 184/285 [03:10<01:30,  1.12it/s]Loading train:  65%|██████▍   | 185/285 [03:11<01:27,  1.14it/s]Loading train:  65%|██████▌   | 186/285 [03:11<01:26,  1.14it/s]Loading train:  66%|██████▌   | 187/285 [03:12<01:25,  1.15it/s]Loading train:  66%|██████▌   | 188/285 [03:13<01:24,  1.15it/s]Loading train:  66%|██████▋   | 189/285 [03:14<01:24,  1.13it/s]Loading train:  67%|██████▋   | 190/285 [03:15<01:22,  1.15it/s]Loading train:  67%|██████▋   | 191/285 [03:16<01:20,  1.17it/s]Loading train:  67%|██████▋   | 192/285 [03:17<01:19,  1.17it/s]Loading train:  68%|██████▊   | 193/285 [03:17<01:19,  1.16it/s]Loading train:  68%|██████▊   | 194/285 [03:18<01:18,  1.15it/s]Loading train:  68%|██████▊   | 195/285 [03:19<01:20,  1.12it/s]Loading train:  69%|██████▉   | 196/285 [03:20<01:24,  1.06it/s]Loading train:  69%|██████▉   | 197/285 [03:21<01:23,  1.06it/s]Loading train:  69%|██████▉   | 198/285 [03:22<01:21,  1.07it/s]Loading train:  70%|██████▉   | 199/285 [03:23<01:20,  1.07it/s]Loading train:  70%|███████   | 200/285 [03:24<01:20,  1.06it/s]Loading train:  71%|███████   | 201/285 [03:25<01:20,  1.05it/s]Loading train:  71%|███████   | 202/285 [03:26<01:20,  1.04it/s]Loading train:  71%|███████   | 203/285 [03:27<01:19,  1.03it/s]Loading train:  72%|███████▏  | 204/285 [03:28<01:22,  1.02s/it]Loading train:  72%|███████▏  | 205/285 [03:29<01:19,  1.01it/s]Loading train:  72%|███████▏  | 206/285 [03:30<01:16,  1.03it/s]Loading train:  73%|███████▎  | 207/285 [03:31<01:13,  1.06it/s]Loading train:  73%|███████▎  | 208/285 [03:32<01:12,  1.06it/s]Loading train:  73%|███████▎  | 209/285 [03:33<01:10,  1.08it/s]Loading train:  74%|███████▎  | 210/285 [03:34<01:08,  1.09it/s]Loading train:  74%|███████▍  | 211/285 [03:35<01:07,  1.10it/s]Loading train:  74%|███████▍  | 212/285 [03:36<01:07,  1.08it/s]Loading train:  75%|███████▍  | 213/285 [03:36<01:06,  1.08it/s]Loading train:  75%|███████▌  | 214/285 [03:37<01:04,  1.11it/s]Loading train:  75%|███████▌  | 215/285 [03:38<01:06,  1.06it/s]Loading train:  76%|███████▌  | 216/285 [03:39<01:04,  1.06it/s]Loading train:  76%|███████▌  | 217/285 [03:40<01:02,  1.10it/s]Loading train:  76%|███████▋  | 218/285 [03:41<00:59,  1.12it/s]Loading train:  77%|███████▋  | 219/285 [03:42<00:57,  1.14it/s]Loading train:  77%|███████▋  | 220/285 [03:43<00:54,  1.19it/s]Loading train:  78%|███████▊  | 221/285 [03:43<00:53,  1.20it/s]Loading train:  78%|███████▊  | 222/285 [03:44<00:52,  1.20it/s]Loading train:  78%|███████▊  | 223/285 [03:45<00:51,  1.20it/s]Loading train:  79%|███████▊  | 224/285 [03:46<00:51,  1.19it/s]Loading train:  79%|███████▉  | 225/285 [03:47<00:49,  1.22it/s]Loading train:  79%|███████▉  | 226/285 [03:48<00:50,  1.17it/s]Loading train:  80%|███████▉  | 227/285 [03:48<00:48,  1.19it/s]Loading train:  80%|████████  | 228/285 [03:49<00:47,  1.20it/s]Loading train:  80%|████████  | 229/285 [03:50<00:47,  1.18it/s]Loading train:  81%|████████  | 230/285 [03:51<00:46,  1.18it/s]Loading train:  81%|████████  | 231/285 [03:52<00:46,  1.16it/s]Loading train:  81%|████████▏ | 232/285 [03:53<00:48,  1.09it/s]Loading train:  82%|████████▏ | 233/285 [03:54<00:49,  1.06it/s]Loading train:  82%|████████▏ | 234/285 [03:55<00:49,  1.02it/s]Loading train:  82%|████████▏ | 235/285 [03:56<00:48,  1.02it/s]Loading train:  83%|████████▎ | 236/285 [03:57<00:48,  1.01it/s]Loading train:  83%|████████▎ | 237/285 [03:58<00:46,  1.03it/s]Loading train:  84%|████████▎ | 238/285 [03:59<00:45,  1.03it/s]Loading train:  84%|████████▍ | 239/285 [04:00<00:45,  1.01it/s]Loading train:  84%|████████▍ | 240/285 [04:01<00:44,  1.01it/s]Loading train:  85%|████████▍ | 241/285 [04:02<00:42,  1.03it/s]Loading train:  85%|████████▍ | 242/285 [04:03<00:42,  1.01it/s]Loading train:  85%|████████▌ | 243/285 [04:04<00:40,  1.03it/s]Loading train:  86%|████████▌ | 244/285 [04:05<00:40,  1.01it/s]Loading train:  86%|████████▌ | 245/285 [04:06<00:39,  1.01it/s]Loading train:  86%|████████▋ | 246/285 [04:07<00:38,  1.00it/s]Loading train:  87%|████████▋ | 247/285 [04:08<00:38,  1.01s/it]Loading train:  87%|████████▋ | 248/285 [04:09<00:37,  1.02s/it]Loading train:  87%|████████▋ | 249/285 [04:10<00:35,  1.00it/s]Loading train:  88%|████████▊ | 250/285 [04:11<00:33,  1.04it/s]Loading train:  88%|████████▊ | 251/285 [04:12<00:31,  1.08it/s]Loading train:  88%|████████▊ | 252/285 [04:12<00:29,  1.10it/s]Loading train:  89%|████████▉ | 253/285 [04:13<00:28,  1.10it/s]Loading train:  89%|████████▉ | 254/285 [04:14<00:27,  1.13it/s]Loading train:  89%|████████▉ | 255/285 [04:15<00:26,  1.14it/s]Loading train:  90%|████████▉ | 256/285 [04:16<00:25,  1.16it/s]Loading train:  90%|█████████ | 257/285 [04:17<00:23,  1.19it/s]Loading train:  91%|█████████ | 258/285 [04:17<00:22,  1.19it/s]Loading train:  91%|█████████ | 259/285 [04:18<00:22,  1.18it/s]Loading train:  91%|█████████ | 260/285 [04:19<00:21,  1.18it/s]Loading train:  92%|█████████▏| 261/285 [04:20<00:20,  1.16it/s]Loading train:  92%|█████████▏| 262/285 [04:21<00:19,  1.16it/s]Loading train:  92%|█████████▏| 263/285 [04:22<00:18,  1.18it/s]Loading train:  93%|█████████▎| 264/285 [04:23<00:18,  1.15it/s]Loading train:  93%|█████████▎| 265/285 [04:24<00:17,  1.16it/s]Loading train:  93%|█████████▎| 266/285 [04:24<00:16,  1.18it/s]Loading train:  94%|█████████▎| 267/285 [04:25<00:14,  1.20it/s]Loading train:  94%|█████████▍| 268/285 [04:26<00:15,  1.08it/s]Loading train:  94%|█████████▍| 269/285 [04:27<00:15,  1.03it/s]Loading train:  95%|█████████▍| 270/285 [04:28<00:15,  1.01s/it]Loading train:  95%|█████████▌| 271/285 [04:30<00:14,  1.03s/it]Loading train:  95%|█████████▌| 272/285 [04:31<00:13,  1.04s/it]Loading train:  96%|█████████▌| 273/285 [04:32<00:12,  1.04s/it]Loading train:  96%|█████████▌| 274/285 [04:33<00:11,  1.04s/it]Loading train:  96%|█████████▋| 275/285 [04:34<00:10,  1.06s/it]Loading train:  97%|█████████▋| 276/285 [04:35<00:09,  1.06s/it]Loading train:  97%|█████████▋| 277/285 [04:36<00:08,  1.05s/it]Loading train:  98%|█████████▊| 278/285 [04:37<00:07,  1.05s/it]Loading train:  98%|█████████▊| 279/285 [04:38<00:06,  1.05s/it]Loading train:  98%|█████████▊| 280/285 [04:39<00:05,  1.07s/it]Loading train:  99%|█████████▊| 281/285 [04:40<00:04,  1.06s/it]Loading train:  99%|█████████▉| 282/285 [04:41<00:03,  1.05s/it]Loading train:  99%|█████████▉| 283/285 [04:42<00:02,  1.03s/it]Loading train: 100%|█████████▉| 284/285 [04:43<00:01,  1.02s/it]Loading train: 100%|██████████| 285/285 [04:44<00:00,  1.03s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   5%|▌         | 15/285 [00:00<00:01, 144.44it/s]concatenating: train:  15%|█▌        | 43/285 [00:00<00:01, 168.56it/s]concatenating: train:  22%|██▏       | 63/285 [00:00<00:01, 176.51it/s]concatenating: train:  31%|███       | 88/285 [00:00<00:01, 193.19it/s]concatenating: train:  39%|███▊      | 110/285 [00:00<00:00, 199.87it/s]concatenating: train:  48%|████▊     | 138/285 [00:00<00:00, 218.42it/s]concatenating: train:  60%|█████▉    | 170/285 [00:00<00:00, 241.15it/s]concatenating: train:  70%|███████   | 200/285 [00:00<00:00, 252.74it/s]concatenating: train:  79%|███████▉  | 226/285 [00:00<00:00, 240.11it/s]concatenating: train:  89%|████████▉ | 254/285 [00:01<00:00, 249.31it/s]concatenating: train:  98%|█████████▊| 280/285 [00:01<00:00, 242.39it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 244.79it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.36s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.35s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.30s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 122.22it/s]2019-07-07 01:23:18.204517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-07 01:23:18.204631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-07 01:23:18.204648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-07 01:23:18.204657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-07 01:23:18.205078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:09,  4.31it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:07,  5.06it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:07,  4.70it/s]loading the weights for Unet:  20%|██        | 8/40 [00:00<00:05,  6.03it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:05,  5.22it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.79it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  4.99it/s]loading the weights for Unet:  40%|████      | 16/40 [00:01<00:03,  6.39it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.68it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  6.43it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  5.51it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:02<00:02,  6.73it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  7.34it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  6.15it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:01,  6.78it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:03<00:01,  5.52it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:03<00:01,  6.65it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  7.09it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.77it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:04<00:00,  6.44it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:04<00:00,  5.60it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:04<00:00,  8.11it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 52, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 52, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 52, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 52, 40)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 52, 40)   14440       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 52, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 52, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 52, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 52, 40)   14440       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 52, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 52, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 52, 40)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 52, 20)   7220        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 52, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 52, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 52, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 52, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 52, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 26, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 26, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 26, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 26, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 26, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 26, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 26, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 13, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 13, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 13, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 13, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 13, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 13, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 13, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 13, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 26, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 26, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 26, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 26, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 26, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 26, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 26, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 26, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 52, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 52, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 52, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 52, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 52, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 52, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 52, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 52, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 52, 13)   273         dropout_8[0][0]                  
==================================================================================================
Total params: 220,893
Trainable params: 77,173
Non-trainable params: 143,720
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.49841486e-02 3.19966680e-02 7.50970181e-02 9.33357939e-03
 2.71292049e-02 7.07427267e-03 8.46489586e-02 1.12779077e-01
 8.61338510e-02 1.32649165e-02 2.94521391e-01 1.92807035e-01
 2.29878984e-04]
Train on 18361 samples, validate on 179 samples
Epoch 1/300
 - 24s - loss: 117.3485 - acc: 0.7321 - mDice: 0.0137 - val_loss: 15.1709 - val_acc: 0.9136 - val_mDice: 0.0100

Epoch 00001: val_mDice improved from -inf to 0.00998, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 15s - loss: 17.5041 - acc: 0.8815 - mDice: 0.0143 - val_loss: 6.9399 - val_acc: 0.9136 - val_mDice: 0.0144

Epoch 00002: val_mDice improved from 0.00998 to 0.01443, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 16s - loss: 10.3806 - acc: 0.8858 - mDice: 0.0200 - val_loss: 5.5671 - val_acc: 0.9136 - val_mDice: 0.0230

Epoch 00003: val_mDice improved from 0.01443 to 0.02298, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 16s - loss: 8.0128 - acc: 0.8860 - mDice: 0.0282 - val_loss: 4.7584 - val_acc: 0.9136 - val_mDice: 0.0300

Epoch 00004: val_mDice improved from 0.02298 to 0.03000, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 15s - loss: 6.6221 - acc: 0.8856 - mDice: 0.0397 - val_loss: 4.6386 - val_acc: 0.9136 - val_mDice: 0.0298

Epoch 00005: val_mDice did not improve from 0.03000
Epoch 6/300
 - 16s - loss: 5.5333 - acc: 0.8855 - mDice: 0.0710 - val_loss: 3.6426 - val_acc: 0.9136 - val_mDice: 0.0923

Epoch 00006: val_mDice improved from 0.03000 to 0.09230, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 16s - loss: 4.7112 - acc: 0.8880 - mDice: 0.1145 - val_loss: 3.2469 - val_acc: 0.9187 - val_mDice: 0.1496

Epoch 00007: val_mDice improved from 0.09230 to 0.14957, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 15s - loss: 4.1831 - acc: 0.8919 - mDice: 0.1478 - val_loss: 2.9249 - val_acc: 0.9212 - val_mDice: 0.1915

Epoch 00008: val_mDice improved from 0.14957 to 0.19154, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 15s - loss: 3.8233 - acc: 0.8947 - mDice: 0.1760 - val_loss: 2.9125 - val_acc: 0.9217 - val_mDice: 0.2207

Epoch 00009: val_mDice improved from 0.19154 to 0.22070, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 15s - loss: 3.5249 - acc: 0.8974 - mDice: 0.2049 - val_loss: 2.6605 - val_acc: 0.9225 - val_mDice: 0.2528

Epoch 00010: val_mDice improved from 0.22070 to 0.25284, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 16s - loss: 3.2862 - acc: 0.8999 - mDice: 0.2298 - val_loss: 2.7671 - val_acc: 0.9257 - val_mDice: 0.2595

Epoch 00011: val_mDice improved from 0.25284 to 0.25954, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 15s - loss: 3.0781 - acc: 0.9023 - mDice: 0.2529 - val_loss: 2.6579 - val_acc: 0.9268 - val_mDice: 0.2851

Epoch 00012: val_mDice improved from 0.25954 to 0.28512, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 15s - loss: 2.9071 - acc: 0.9047 - mDice: 0.2742 - val_loss: 2.7139 - val_acc: 0.9285 - val_mDice: 0.2905

Epoch 00013: val_mDice improved from 0.28512 to 0.29054, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 15s - loss: 2.7803 - acc: 0.9064 - mDice: 0.2906 - val_loss: 2.6377 - val_acc: 0.9244 - val_mDice: 0.3033

Epoch 00014: val_mDice improved from 0.29054 to 0.30330, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 15s - loss: 2.6530 - acc: 0.9085 - mDice: 0.3084 - val_loss: 2.3395 - val_acc: 0.9289 - val_mDice: 0.3402

Epoch 00015: val_mDice improved from 0.30330 to 0.34022, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 15s - loss: 2.5438 - acc: 0.9107 - mDice: 0.3237 - val_loss: 2.6458 - val_acc: 0.9303 - val_mDice: 0.3298

Epoch 00016: val_mDice did not improve from 0.34022
Epoch 17/300
 - 16s - loss: 2.4495 - acc: 0.9129 - mDice: 0.3384 - val_loss: 2.3500 - val_acc: 0.9324 - val_mDice: 0.3659

Epoch 00017: val_mDice improved from 0.34022 to 0.36590, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 15s - loss: 2.3713 - acc: 0.9152 - mDice: 0.3525 - val_loss: 2.7010 - val_acc: 0.9356 - val_mDice: 0.3420

Epoch 00018: val_mDice did not improve from 0.36590
Epoch 19/300
 - 15s - loss: 2.2958 - acc: 0.9173 - mDice: 0.3655 - val_loss: 2.4896 - val_acc: 0.9368 - val_mDice: 0.3634

Epoch 00019: val_mDice did not improve from 0.36590
Epoch 20/300
 - 15s - loss: 2.2285 - acc: 0.9194 - mDice: 0.3784 - val_loss: 2.4554 - val_acc: 0.9382 - val_mDice: 0.3807

Epoch 00020: val_mDice improved from 0.36590 to 0.38072, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 15s - loss: 2.1677 - acc: 0.9210 - mDice: 0.3913 - val_loss: 2.4002 - val_acc: 0.9403 - val_mDice: 0.3944

Epoch 00021: val_mDice improved from 0.38072 to 0.39442, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 16s - loss: 2.1036 - acc: 0.9223 - mDice: 0.4044 - val_loss: 2.3559 - val_acc: 0.9412 - val_mDice: 0.4013

Epoch 00022: val_mDice improved from 0.39442 to 0.40128, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 16s - loss: 2.0509 - acc: 0.9232 - mDice: 0.4163 - val_loss: 2.3873 - val_acc: 0.9408 - val_mDice: 0.4080

Epoch 00023: val_mDice improved from 0.40128 to 0.40795, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 15s - loss: 2.0022 - acc: 0.9242 - mDice: 0.4264 - val_loss: 2.2885 - val_acc: 0.9419 - val_mDice: 0.4196

Epoch 00024: val_mDice improved from 0.40795 to 0.41959, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 15s - loss: 1.9570 - acc: 0.9252 - mDice: 0.4363 - val_loss: 2.2980 - val_acc: 0.9416 - val_mDice: 0.4285

Epoch 00025: val_mDice improved from 0.41959 to 0.42852, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 16s - loss: 1.9269 - acc: 0.9261 - mDice: 0.4434 - val_loss: 2.3015 - val_acc: 0.9409 - val_mDice: 0.4333

Epoch 00026: val_mDice improved from 0.42852 to 0.43330, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 15s - loss: 1.8858 - acc: 0.9271 - mDice: 0.4522 - val_loss: 2.2330 - val_acc: 0.9423 - val_mDice: 0.4388

Epoch 00027: val_mDice improved from 0.43330 to 0.43876, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 15s - loss: 1.8515 - acc: 0.9279 - mDice: 0.4603 - val_loss: 2.3643 - val_acc: 0.9418 - val_mDice: 0.4380

Epoch 00028: val_mDice did not improve from 0.43876
Epoch 29/300
 - 15s - loss: 1.8201 - acc: 0.9289 - mDice: 0.4677 - val_loss: 2.3699 - val_acc: 0.9439 - val_mDice: 0.4441

Epoch 00029: val_mDice improved from 0.43876 to 0.44409, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 17s - loss: 1.7960 - acc: 0.9295 - mDice: 0.4732 - val_loss: 2.2238 - val_acc: 0.9431 - val_mDice: 0.4512

Epoch 00030: val_mDice improved from 0.44409 to 0.45115, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 16s - loss: 1.7661 - acc: 0.9303 - mDice: 0.4811 - val_loss: 2.2294 - val_acc: 0.9430 - val_mDice: 0.4540

Epoch 00031: val_mDice improved from 0.45115 to 0.45400, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 16s - loss: 1.7436 - acc: 0.9310 - mDice: 0.4859 - val_loss: 2.3591 - val_acc: 0.9440 - val_mDice: 0.4455

Epoch 00032: val_mDice did not improve from 0.45400
Epoch 33/300
 - 15s - loss: 1.7194 - acc: 0.9316 - mDice: 0.4920 - val_loss: 2.4395 - val_acc: 0.9431 - val_mDice: 0.4417

Epoch 00033: val_mDice did not improve from 0.45400
Epoch 34/300
 - 16s - loss: 1.7024 - acc: 0.9320 - mDice: 0.4965 - val_loss: 2.4345 - val_acc: 0.9417 - val_mDice: 0.4453

Epoch 00034: val_mDice did not improve from 0.45400
Epoch 35/300
 - 15s - loss: 1.6791 - acc: 0.9327 - mDice: 0.5024 - val_loss: 2.2513 - val_acc: 0.9450 - val_mDice: 0.4690

Epoch 00035: val_mDice improved from 0.45400 to 0.46904, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 15s - loss: 1.6692 - acc: 0.9330 - mDice: 0.5057 - val_loss: 2.2507 - val_acc: 0.9436 - val_mDice: 0.4670

Epoch 00036: val_mDice did not improve from 0.46904
Epoch 37/300
 - 15s - loss: 1.6464 - acc: 0.9334 - mDice: 0.5109 - val_loss: 2.2640 - val_acc: 0.9423 - val_mDice: 0.4611

Epoch 00037: val_mDice did not improve from 0.46904
Epoch 38/300
 - 15s - loss: 1.6261 - acc: 0.9340 - mDice: 0.5164 - val_loss: 2.2541 - val_acc: 0.9446 - val_mDice: 0.4703

Epoch 00038: val_mDice improved from 0.46904 to 0.47027, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 16s - loss: 1.6111 - acc: 0.9342 - mDice: 0.5205 - val_loss: 2.3318 - val_acc: 0.9441 - val_mDice: 0.4682

Epoch 00039: val_mDice did not improve from 0.47027
Epoch 40/300
 - 15s - loss: 1.5952 - acc: 0.9347 - mDice: 0.5236 - val_loss: 2.3594 - val_acc: 0.9444 - val_mDice: 0.4654

Epoch 00040: val_mDice did not improve from 0.47027
Epoch 41/300
 - 15s - loss: 1.5834 - acc: 0.9349 - mDice: 0.5277 - val_loss: 2.3661 - val_acc: 0.9452 - val_mDice: 0.4699

Epoch 00041: val_mDice did not improve from 0.47027
Epoch 42/300
 - 15s - loss: 1.5623 - acc: 0.9354 - mDice: 0.5327 - val_loss: 2.2348 - val_acc: 0.9456 - val_mDice: 0.4808

Epoch 00042: val_mDice improved from 0.47027 to 0.48081, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 15s - loss: 1.5500 - acc: 0.9356 - mDice: 0.5358 - val_loss: 2.1577 - val_acc: 0.9443 - val_mDice: 0.4850

Epoch 00043: val_mDice improved from 0.48081 to 0.48503, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 44/300
 - 15s - loss: 1.5469 - acc: 0.9358 - mDice: 0.5370 - val_loss: 2.3633 - val_acc: 0.9434 - val_mDice: 0.4712

Epoch 00044: val_mDice did not improve from 0.48503
Epoch 45/300
 - 16s - loss: 1.5321 - acc: 0.9360 - mDice: 0.5406 - val_loss: 2.7224 - val_acc: 0.9439 - val_mDice: 0.4551

Epoch 00045: val_mDice did not improve from 0.48503
Epoch 46/300
 - 16s - loss: 1.5176 - acc: 0.9363 - mDice: 0.5442 - val_loss: 2.2741 - val_acc: 0.9459 - val_mDice: 0.4888

Epoch 00046: val_mDice improved from 0.48503 to 0.48883, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 15s - loss: 1.5032 - acc: 0.9365 - mDice: 0.5482 - val_loss: 2.5103 - val_acc: 0.9445 - val_mDice: 0.4752

Epoch 00047: val_mDice did not improve from 0.48883
Epoch 48/300
 - 16s - loss: 1.4901 - acc: 0.9366 - mDice: 0.5517 - val_loss: 2.2192 - val_acc: 0.9455 - val_mDice: 0.4910

Epoch 00048: val_mDice improved from 0.48883 to 0.49100, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 16s - loss: 1.4809 - acc: 0.9368 - mDice: 0.5544 - val_loss: 2.2952 - val_acc: 0.9461 - val_mDice: 0.4884

Epoch 00049: val_mDice did not improve from 0.49100
Epoch 50/300
 - 16s - loss: 1.4694 - acc: 0.9370 - mDice: 0.5575 - val_loss: 2.2796 - val_acc: 0.9450 - val_mDice: 0.4941

Epoch 00050: val_mDice improved from 0.49100 to 0.49413, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 16s - loss: 1.4583 - acc: 0.9373 - mDice: 0.5603 - val_loss: 2.3633 - val_acc: 0.9434 - val_mDice: 0.4911

Epoch 00051: val_mDice did not improve from 0.49413
Epoch 52/300
 - 16s - loss: 1.4432 - acc: 0.9376 - mDice: 0.5643 - val_loss: 2.6345 - val_acc: 0.9460 - val_mDice: 0.4701

Epoch 00052: val_mDice did not improve from 0.49413
Epoch 53/300
 - 16s - loss: 1.4404 - acc: 0.9377 - mDice: 0.5653 - val_loss: 2.3817 - val_acc: 0.9446 - val_mDice: 0.4911

Epoch 00053: val_mDice did not improve from 0.49413
Epoch 54/300
 - 16s - loss: 1.4341 - acc: 0.9378 - mDice: 0.5666 - val_loss: 2.4995 - val_acc: 0.9449 - val_mDice: 0.4874

Epoch 00054: val_mDice did not improve from 0.49413
Epoch 55/300
 - 16s - loss: 1.4200 - acc: 0.9381 - mDice: 0.5711 - val_loss: 2.7567 - val_acc: 0.9433 - val_mDice: 0.4722

Epoch 00055: val_mDice did not improve from 0.49413
Epoch 56/300
 - 18s - loss: 1.4118 - acc: 0.9382 - mDice: 0.5727 - val_loss: 2.6127 - val_acc: 0.9463 - val_mDice: 0.4855

Epoch 00056: val_mDice did not improve from 0.49413
Epoch 57/300
 - 18s - loss: 1.4008 - acc: 0.9387 - mDice: 0.5765 - val_loss: 2.3555 - val_acc: 0.9437 - val_mDice: 0.4994
