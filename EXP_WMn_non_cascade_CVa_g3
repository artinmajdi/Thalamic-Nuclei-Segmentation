2020-04-26 22:32:08.899969: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-04-26 22:32:09.249268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:09:00.0
totalMemory: 15.90GiB freeMemory: 15.64GiB
2020-04-26 22:32:09.249330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-04-26 22:32:09.656000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-26 22:32:09.656067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-04-26 22:32:09.656079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-04-26 22:32:09.656192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:08<05:42,  8.56s/it]Loading train:   5%|▍         | 2/41 [00:12<04:42,  7.25s/it]Loading train:   7%|▋         | 3/41 [00:28<06:12,  9.79s/it]Loading train:  10%|▉         | 4/41 [00:52<08:44, 14.18s/it]Loading train:  12%|█▏        | 5/41 [01:07<08:35, 14.31s/it]Loading train:  15%|█▍        | 6/41 [01:16<07:20, 12.59s/it]Loading train:  17%|█▋        | 7/41 [01:19<05:31,  9.75s/it]Loading train:  20%|█▉        | 8/41 [01:26<04:53,  8.89s/it]Loading train:  22%|██▏       | 9/41 [01:26<03:26,  6.46s/it]Loading train:  24%|██▍       | 10/41 [01:27<02:30,  4.85s/it]Loading train:  27%|██▋       | 11/41 [01:29<01:57,  3.91s/it]Loading train:  29%|██▉       | 12/41 [01:31<01:35,  3.31s/it]Loading train:  32%|███▏      | 13/41 [01:32<01:15,  2.70s/it]Loading train:  34%|███▍      | 14/41 [01:34<01:03,  2.34s/it]Loading train:  37%|███▋      | 15/41 [01:36<01:00,  2.32s/it]Loading train:  39%|███▉      | 16/41 [01:38<00:52,  2.09s/it]Loading train:  41%|████▏     | 17/41 [01:40<00:53,  2.22s/it]Loading train:  44%|████▍     | 18/41 [01:42<00:44,  1.95s/it]Loading train:  46%|████▋     | 19/41 [01:42<00:35,  1.63s/it]Loading train:  49%|████▉     | 20/41 [01:44<00:35,  1.67s/it]Loading train:  51%|█████     | 21/41 [01:46<00:36,  1.81s/it]Loading train:  54%|█████▎    | 22/41 [01:49<00:36,  1.93s/it]Loading train:  56%|█████▌    | 23/41 [01:53<00:46,  2.59s/it]Loading train:  59%|█████▊    | 24/41 [01:53<00:34,  2.05s/it]Loading train:  61%|██████    | 25/41 [01:54<00:27,  1.73s/it]Loading train:  63%|██████▎   | 26/41 [01:55<00:22,  1.52s/it]Loading train:  66%|██████▌   | 27/41 [01:56<00:19,  1.37s/it]Loading train:  68%|██████▊   | 28/41 [01:58<00:18,  1.40s/it]Loading train:  71%|███████   | 29/41 [01:59<00:15,  1.29s/it]Loading train:  73%|███████▎  | 30/41 [02:01<00:15,  1.43s/it]Loading train:  76%|███████▌  | 31/41 [02:03<00:17,  1.71s/it]Loading train:  78%|███████▊  | 32/41 [02:05<00:17,  1.90s/it]Loading train:  80%|████████  | 33/41 [02:08<00:15,  1.99s/it]Loading train:  83%|████████▎ | 34/41 [02:09<00:13,  1.93s/it]Loading train:  85%|████████▌ | 35/41 [02:13<00:15,  2.54s/it]Loading train:  88%|████████▊ | 36/41 [02:16<00:12,  2.58s/it]Loading train:  90%|█████████ | 37/41 [02:18<00:09,  2.34s/it]Loading train:  93%|█████████▎| 38/41 [02:19<00:06,  2.05s/it]Loading train:  95%|█████████▌| 39/41 [02:22<00:04,  2.22s/it]Loading train:  98%|█████████▊| 40/41 [02:29<00:03,  3.83s/it]Loading train: 100%|██████████| 41/41 [02:51<00:00,  9.27s/it]Loading train: 100%|██████████| 41/41 [02:51<00:00,  4.19s/it]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   5%|▍         | 2/41 [00:00<00:03, 11.02it/s]concatenating: train:  10%|▉         | 4/41 [00:00<00:03, 11.12it/s]concatenating: train:  15%|█▍        | 6/41 [00:00<00:03, 11.30it/s]concatenating: train:  20%|█▉        | 8/41 [00:00<00:02, 11.46it/s]concatenating: train:  24%|██▍       | 10/41 [00:00<00:02, 11.56it/s]concatenating: train:  29%|██▉       | 12/41 [00:01<00:02, 11.60it/s]concatenating: train:  34%|███▍      | 14/41 [00:01<00:02, 11.52it/s]concatenating: train:  39%|███▉      | 16/41 [00:01<00:02, 11.42it/s]concatenating: train:  44%|████▍     | 18/41 [00:01<00:02, 11.07it/s]concatenating: train:  49%|████▉     | 20/41 [00:01<00:01, 11.43it/s]concatenating: train:  54%|█████▎    | 22/41 [00:01<00:01, 11.78it/s]concatenating: train:  59%|█████▊    | 24/41 [00:02<00:01, 11.84it/s]concatenating: train:  63%|██████▎   | 26/41 [00:02<00:01, 11.49it/s]concatenating: train:  68%|██████▊   | 28/41 [00:02<00:01, 11.43it/s]concatenating: train:  73%|███████▎  | 30/41 [00:02<00:00, 11.51it/s]concatenating: train:  78%|███████▊  | 32/41 [00:02<00:00, 11.49it/s]concatenating: train:  83%|████████▎ | 34/41 [00:02<00:00, 11.55it/s]concatenating: train:  88%|████████▊ | 36/41 [00:03<00:00, 11.07it/s]concatenating: train:  93%|█████████▎| 38/41 [00:03<00:00, 10.96it/s]concatenating: train:  98%|█████████▊| 40/41 [00:03<00:00, 11.04it/s]concatenating: train: 100%|██████████| 41/41 [00:03<00:00, 11.38it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:28<04:47, 28.76s/it]Loading test:  18%|█▊        | 2/11 [00:50<04:00, 26.75s/it]Loading test:  27%|██▋       | 3/11 [01:21<03:43, 27.89s/it]Loading test:  36%|███▋      | 4/11 [01:43<03:03, 26.19s/it]Loading test:  45%|████▌     | 5/11 [01:57<02:15, 22.59s/it]Loading test:  55%|█████▍    | 6/11 [02:12<01:40, 20.14s/it]Loading test:  64%|██████▎   | 7/11 [02:26<01:13, 18.50s/it]Loading test:  73%|███████▎  | 8/11 [02:40<00:51, 17.06s/it]Loading test:  82%|████████▏ | 9/11 [02:56<00:33, 16.85s/it]Loading test:  91%|█████████ | 10/11 [03:12<00:16, 16.43s/it]Loading test: 100%|██████████| 11/11 [03:29<00:00, 16.72s/it]Loading test: 100%|██████████| 11/11 [03:29<00:00, 19.07s/it]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  18%|█▊        | 2/11 [00:00<00:00, 11.64it/s]concatenating: validation:  36%|███▋      | 4/11 [00:00<00:00, 11.81it/s]concatenating: validation:  55%|█████▍    | 6/11 [00:00<00:00, 11.82it/s]concatenating: validation:  73%|███████▎  | 8/11 [00:00<00:00, 11.79it/s]concatenating: validation:  91%|█████████ | 10/11 [00:00<00:00, 11.85it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 11.95it/s]----------+++ 
CrossVal ['a']
CrossVal ['a']
(0/11) test vimp2_967_08132013_KW
(1/11) test vimp2_ANON695_03132013
(2/11) test vimp2_ctrl_991_08302013_JF
(3/11) test vimp2_A_3T_ET
(4/11) test vimp2_A_7T_ET
(5/11) test vimp2_G_3T_ET
(6/11) test vimp2_G_7T_ET
(7/11) test vimp2_J_3T_ET
(8/11) test vimp2_J_7T_ET
(9/11) test vimp2_M_3T_ET
(10/11) test vimp2_M_7T_ET
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 3  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 3  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 3  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 128, 92, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 128, 92, 40)  400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 128, 92, 40)  160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 128, 92, 40)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 128, 92, 40)  14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 128, 92, 40)  160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 128, 92, 40)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 64, 46, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 64, 46, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 64, 46, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 64, 46, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 64, 46, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 64, 46, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 64, 46, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 64, 46, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 64, 46, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 32, 23, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 32, 23, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 23, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 23, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 23, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 23, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 23, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 23, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 32, 23, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 32, 23, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 64, 46, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 64, 46, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 64, 46, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 64, 46, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 64, 46, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 64, 46, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 64, 46, 80)   320         conv2d_8[0][0]                   