run_script_csfn_te8: line 2: syntax error near unexpected token `;'
run_script_csfn_te8: line 2: `for x in a b c d; do ; python main.py -wc csfn -cv $x -lr 1e-4 -te 8 -g 0; done'
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
----------+++ 
Traceback (most recent call last):
  File "main.py", line 6, in <module>
    import otherFuncs.smallFuncs as smallFuncs
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/smallFuncs.py", line 15, in <module>
    import modelFuncs.Metrics as metrics
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/Metrics.py", line 3, in <module>
    import tensorflow as tf
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/__init__.py", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/__init__.py", line 88, in <module>
    from tensorflow.python import keras
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py", line 30, in <module>
    from tensorflow.python.keras import estimator
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/keras/estimator/__init__.py", line 28, in <module>
    from tensorflow.python.estimator import keras as keras_lib  # pylint: disable=g-import-not-at-top
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/estimator/__init__.py", line 25, in <module>
    import tensorflow.python.estimator.estimator_lib
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/estimator/estimator_lib.py", line 22, in <module>
    from tensorflow.python.estimator.canned.baseline import BaselineClassifier
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/estimator/canned/baseline.py", line 50, in <module>
    from tensorflow.python.estimator import estimator
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py", line 34, in <module>
    from tensorflow.python.estimator import model_fn as model_fn_lib
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/estimator/model_fn.py", line 26, in <module>
    from tensorflow.python.estimator.export import export_output as export_output_lib
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 951, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 894, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1157, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1129, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1269, in find_spec
KeyboardInterrupt
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
Traceback (most recent call last):
  File "main.py", line 7, in <module>
    from otherFuncs.datasets import preAnalysis
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 17, in <module>
    from skimage.transform import AffineTransform , warp
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/skimage/transform/__init__.py", line 2, in <module>
    from .hough_transform import (hough_line, hough_line_peaks,
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/skimage/transform/hough_transform.py", line 2, in <module>
    from ._hough_transform import (_hough_circle,
  File "skimage/transform/_hough_transform.pyx", line 13, in init skimage.transform._hough_transform
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/skimage/draw/__init__.py", line 1, in <module>
    from .draw import (circle, ellipse, set_color, polygon_perimeter,
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/skimage/draw/draw.py", line 3, in <module>
    from .._shared._geometry import polygon_clip
  File "<frozen importlib._bootstrap>", line 966, in _find_and_load
KeyboardInterrupt
2020-01-21 17:25:51.462320: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-21 17:25:53.049440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-21 17:25:53.049509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 17:25:53.452245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 17:25:53.452315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 17:25:53.452329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 17:25:53.452783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['c']
Traceback (most recent call last):
  File "main.py", line 240, in <module>
    UserInfoB, K = preMode(UserInfo.__dict__)
  File "main.py", line 237, in preMode
    K = smallFuncs.gpuSetting(str(UserInfoB['simulation'].GPU_Index)) # params.WhichExperiment.HardParams.Machine.GPU_Index)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/smallFuncs.py", line 265, in gpuSetting
    K.set_session(tf.Session(   config=tf.ConfigProto( allow_soft_placement=True )   ))
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1551, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 676, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
KeyboardInterrupt
2020-01-21 17:26:00.399927: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-21 17:26:03.493477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-21 17:26:03.493537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 17:26:03.911423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 17:26:03.911487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 17:26:03.911502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 17:26:03.911998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:20,  3.30it/s]Loading train:   1%|          | 2/266 [00:00<01:18,  3.37it/s]Loading train:   1%|          | 3/266 [00:00<01:12,  3.64it/s]Loading train:   2%|▏         | 4/266 [00:01<01:09,  3.80it/s]Loading train:   2%|▏         | 5/266 [00:01<01:08,  3.80it/s]Loading train:   2%|▏         | 6/266 [00:01<01:08,  3.82it/s]Loading train:   3%|▎         | 7/266 [00:01<01:07,  3.85it/s]Loading train:   3%|▎         | 8/266 [00:02<01:06,  3.86it/s]Loading train:   3%|▎         | 9/266 [00:02<01:06,  3.87it/s]Loading train:   4%|▍         | 10/266 [00:02<01:06,  3.85it/s]Loading train:   4%|▍         | 11/266 [00:02<01:06,  3.82it/s]Loading train:   5%|▍         | 12/266 [00:03<01:05,  3.85it/s]Loading train:   5%|▍         | 13/266 [00:03<01:05,  3.86it/s]Loading train:   5%|▌         | 14/266 [00:03<01:05,  3.83it/s]Loading train:   6%|▌         | 15/266 [00:03<01:05,  3.84it/s]Loading train:   6%|▌         | 16/266 [00:04<01:04,  3.85it/s]Loading train:   6%|▋         | 17/266 [00:04<01:04,  3.83it/s]Loading train:   7%|▋         | 18/266 [00:04<01:05,  3.81it/s]Loading train:   7%|▋         | 19/266 [00:04<01:04,  3.82it/s]Loading train:   8%|▊         | 20/266 [00:05<01:03,  3.85it/s]Loading train:   8%|▊         | 21/266 [00:05<01:03,  3.86it/s]Loading train:   8%|▊         | 22/266 [00:05<01:02,  3.88it/s]Loading train:   9%|▊         | 23/266 [00:05<01:02,  3.88it/s]Loading train:   9%|▉         | 24/266 [00:06<01:01,  3.91it/s]Loading train:   9%|▉         | 25/266 [00:06<01:00,  3.96it/s]Loading train:  10%|▉         | 26/266 [00:06<01:00,  3.99it/s]Loading train:  10%|█         | 27/266 [00:06<00:59,  4.00it/s]Loading train:  11%|█         | 28/266 [00:07<00:59,  4.02it/s]Loading train:  11%|█         | 29/266 [00:07<00:58,  4.05it/s]Loading train:  11%|█▏        | 30/266 [00:07<00:58,  4.05it/s]Loading train:  12%|█▏        | 31/266 [00:07<00:58,  4.02it/s]Loading train:  12%|█▏        | 32/266 [00:08<00:57,  4.04it/s]Loading train:  12%|█▏        | 33/266 [00:08<00:58,  3.98it/s]Loading train:  13%|█▎        | 34/266 [00:08<00:57,  4.01it/s]Loading train:  13%|█▎        | 35/266 [00:08<00:57,  4.03it/s]Loading train:  14%|█▎        | 36/266 [00:09<00:56,  4.06it/s]Loading train:  14%|█▍        | 37/266 [00:09<00:56,  4.09it/s]Loading train:  14%|█▍        | 38/266 [00:09<00:55,  4.10it/s]Loading train:  15%|█▍        | 39/266 [00:09<00:55,  4.10it/s]Loading train:  15%|█▌        | 40/266 [00:10<00:55,  4.07it/s]Loading train:  15%|█▌        | 41/266 [00:10<00:56,  3.99it/s]Loading train:  16%|█▌        | 42/266 [00:10<00:52,  4.24it/s]Loading train:  16%|█▌        | 43/266 [00:10<00:50,  4.44it/s]Loading train:  17%|█▋        | 44/266 [00:11<00:48,  4.59it/s]Loading train:  17%|█▋        | 45/266 [00:11<00:48,  4.60it/s]Loading train:  17%|█▋        | 46/266 [00:11<00:47,  4.63it/s]Loading train:  18%|█▊        | 47/266 [00:11<00:47,  4.64it/s]Loading train:  18%|█▊        | 48/266 [00:11<00:46,  4.74it/s]Loading train:  18%|█▊        | 49/266 [00:12<00:45,  4.79it/s]Loading train:  19%|█▉        | 50/266 [00:12<00:44,  4.83it/s]Loading train:  19%|█▉        | 51/266 [00:12<00:44,  4.86it/s]Loading train:  20%|█▉        | 52/266 [00:12<00:43,  4.88it/s]Loading train:  20%|█▉        | 53/266 [00:12<00:43,  4.90it/s]Loading train:  20%|██        | 54/266 [00:13<00:43,  4.93it/s]Loading train:  21%|██        | 55/266 [00:13<00:43,  4.90it/s]Loading train:  21%|██        | 56/266 [00:13<00:43,  4.86it/s]Loading train:  21%|██▏       | 57/266 [00:13<00:42,  4.87it/s]Loading train:  22%|██▏       | 58/266 [00:13<00:42,  4.85it/s]Loading train:  22%|██▏       | 59/266 [00:14<00:42,  4.86it/s]Loading train:  23%|██▎       | 60/266 [00:14<00:42,  4.83it/s]Loading train:  23%|██▎       | 61/266 [00:14<00:42,  4.80it/s]Loading train:  23%|██▎       | 62/266 [00:14<00:42,  4.77it/s]Loading train:  24%|██▎       | 63/266 [00:14<00:42,  4.75it/s]Loading train:  24%|██▍       | 64/266 [00:15<00:42,  4.74it/s]Loading train:  24%|██▍       | 65/266 [00:15<00:42,  4.70it/s]Loading train:  25%|██▍       | 66/266 [00:15<00:42,  4.72it/s]Loading train:  25%|██▌       | 67/266 [00:15<00:42,  4.68it/s]Loading train:  26%|██▌       | 68/266 [00:16<00:42,  4.71it/s]Loading train:  26%|██▌       | 69/266 [00:16<00:41,  4.70it/s]Loading train:  26%|██▋       | 70/266 [00:16<00:41,  4.73it/s]Loading train:  27%|██▋       | 71/266 [00:16<00:41,  4.75it/s]Loading train:  27%|██▋       | 72/266 [00:16<00:40,  4.76it/s]Loading train:  27%|██▋       | 73/266 [00:17<00:41,  4.70it/s]Loading train:  28%|██▊       | 74/266 [00:17<00:40,  4.72it/s]Loading train:  28%|██▊       | 75/266 [00:17<00:40,  4.71it/s]Loading train:  29%|██▊       | 76/266 [00:17<00:40,  4.72it/s]Loading train:  29%|██▉       | 77/266 [00:17<00:40,  4.72it/s]Loading train:  29%|██▉       | 78/266 [00:18<00:41,  4.53it/s]Loading train:  30%|██▉       | 79/266 [00:18<00:42,  4.43it/s]Loading train:  30%|███       | 80/266 [00:18<00:42,  4.36it/s]Loading train:  30%|███       | 81/266 [00:18<00:43,  4.28it/s]Loading train:  31%|███       | 82/266 [00:19<00:43,  4.24it/s]Loading train:  31%|███       | 83/266 [00:19<00:43,  4.22it/s]Loading train:  32%|███▏      | 84/266 [00:19<00:43,  4.22it/s]Loading train:  32%|███▏      | 85/266 [00:19<00:42,  4.21it/s]Loading train:  32%|███▏      | 86/266 [00:20<00:42,  4.22it/s]Loading train:  33%|███▎      | 87/266 [00:20<00:42,  4.21it/s]Loading train:  33%|███▎      | 88/266 [00:20<00:42,  4.20it/s]Loading train:  33%|███▎      | 89/266 [00:20<00:42,  4.14it/s]Loading train:  34%|███▍      | 90/266 [00:21<00:42,  4.12it/s]Loading train:  34%|███▍      | 91/266 [00:21<00:42,  4.14it/s]Loading train:  35%|███▍      | 92/266 [00:21<00:41,  4.15it/s]Loading train:  35%|███▍      | 93/266 [00:21<00:41,  4.15it/s]Loading train:  35%|███▌      | 94/266 [00:22<00:41,  4.17it/s]Loading train:  36%|███▌      | 95/266 [00:22<00:40,  4.18it/s]Loading train:  36%|███▌      | 96/266 [00:22<00:40,  4.24it/s]Loading train:  36%|███▋      | 97/266 [00:22<00:42,  3.99it/s]Loading train:  37%|███▋      | 98/266 [00:23<00:41,  4.05it/s]Loading train:  37%|███▋      | 99/266 [00:23<00:38,  4.28it/s]Loading train:  38%|███▊      | 100/266 [00:23<00:38,  4.31it/s]Loading train:  38%|███▊      | 101/266 [00:23<00:37,  4.42it/s]Loading train:  38%|███▊      | 102/266 [00:23<00:36,  4.52it/s]Loading train:  39%|███▊      | 103/266 [00:24<00:35,  4.57it/s]Loading train:  39%|███▉      | 104/266 [00:24<00:35,  4.59it/s]Loading train:  39%|███▉      | 105/266 [00:24<00:35,  4.56it/s]Loading train:  40%|███▉      | 106/266 [00:24<00:34,  4.60it/s]Loading train:  40%|████      | 107/266 [00:24<00:34,  4.58it/s]Loading train:  41%|████      | 108/266 [00:25<00:34,  4.59it/s]Loading train:  41%|████      | 109/266 [00:25<00:33,  4.63it/s]Loading train:  41%|████▏     | 110/266 [00:25<00:33,  4.64it/s]Loading train:  42%|████▏     | 111/266 [00:25<00:33,  4.66it/s]Loading train:  42%|████▏     | 112/266 [00:26<00:32,  4.68it/s]Loading train:  42%|████▏     | 113/266 [00:26<00:32,  4.69it/s]Loading train:  43%|████▎     | 114/266 [00:26<00:32,  4.64it/s]Loading train:  43%|████▎     | 115/266 [00:26<00:32,  4.65it/s]Loading train:  44%|████▎     | 116/266 [00:26<00:32,  4.64it/s]Loading train:  44%|████▍     | 117/266 [00:27<00:32,  4.65it/s]Loading train:  44%|████▍     | 118/266 [00:27<00:31,  4.67it/s]Loading train:  45%|████▍     | 119/266 [00:27<00:32,  4.46it/s]Loading train:  45%|████▌     | 120/266 [00:27<00:34,  4.28it/s]Loading train:  45%|████▌     | 121/266 [00:28<00:34,  4.19it/s]Loading train:  46%|████▌     | 122/266 [00:28<00:35,  4.10it/s]Loading train:  46%|████▌     | 123/266 [00:28<00:34,  4.09it/s]Loading train:  47%|████▋     | 124/266 [00:28<00:35,  4.04it/s]Loading train:  47%|████▋     | 125/266 [00:29<00:34,  4.03it/s]Loading train:  47%|████▋     | 126/266 [00:29<00:34,  4.02it/s]Loading train:  48%|████▊     | 127/266 [00:29<00:34,  4.02it/s]Loading train:  48%|████▊     | 128/266 [00:29<00:34,  4.01it/s]Loading train:  48%|████▊     | 129/266 [00:30<00:34,  4.00it/s]Loading train:  49%|████▉     | 130/266 [00:30<00:34,  3.95it/s]Loading train:  49%|████▉     | 131/266 [00:30<00:34,  3.93it/s]Loading train:  50%|████▉     | 132/266 [00:30<00:34,  3.94it/s]Loading train:  50%|█████     | 133/266 [00:31<00:33,  3.94it/s]Loading train:  50%|█████     | 134/266 [00:31<00:33,  3.95it/s]Loading train:  51%|█████     | 135/266 [00:31<00:33,  3.96it/s]Loading train:  51%|█████     | 136/266 [00:31<00:32,  3.97it/s]Loading train:  52%|█████▏    | 137/266 [00:32<00:31,  4.09it/s]Loading train:  52%|█████▏    | 138/266 [00:32<00:30,  4.18it/s]Loading train:  52%|█████▏    | 139/266 [00:32<00:29,  4.26it/s]Loading train:  53%|█████▎    | 140/266 [00:32<00:29,  4.31it/s]Loading train:  53%|█████▎    | 141/266 [00:32<00:28,  4.31it/s]Loading train:  53%|█████▎    | 142/266 [00:33<00:28,  4.34it/s]Loading train:  54%|█████▍    | 143/266 [00:33<00:28,  4.36it/s]Loading train:  54%|█████▍    | 144/266 [00:33<00:27,  4.37it/s]Loading train:  55%|█████▍    | 145/266 [00:33<00:27,  4.34it/s]Loading train:  55%|█████▍    | 146/266 [00:34<00:27,  4.36it/s]Loading train:  55%|█████▌    | 147/266 [00:34<00:27,  4.36it/s]Loading train:  56%|█████▌    | 148/266 [00:34<00:26,  4.38it/s]Loading train:  56%|█████▌    | 149/266 [00:34<00:26,  4.39it/s]Loading train:  56%|█████▋    | 150/266 [00:35<00:26,  4.41it/s]Loading train:  57%|█████▋    | 151/266 [00:35<00:26,  4.41it/s]Loading train:  57%|█████▋    | 152/266 [00:35<00:25,  4.42it/s]Loading train:  58%|█████▊    | 153/266 [00:35<00:25,  4.42it/s]Loading train:  58%|█████▊    | 154/266 [00:35<00:25,  4.43it/s]Loading train:  58%|█████▊    | 155/266 [00:36<00:24,  4.60it/s]Loading train:  59%|█████▊    | 156/266 [00:36<00:22,  4.78it/s]Loading train:  59%|█████▉    | 157/266 [00:36<00:22,  4.88it/s]Loading train:  59%|█████▉    | 158/266 [00:36<00:21,  4.99it/s]Loading train:  60%|█████▉    | 159/266 [00:36<00:21,  5.07it/s]Loading train:  60%|██████    | 160/266 [00:37<00:20,  5.11it/s]Loading train:  61%|██████    | 161/266 [00:37<00:20,  5.17it/s]Loading train:  61%|██████    | 162/266 [00:37<00:20,  5.19it/s]Loading train:  61%|██████▏   | 163/266 [00:37<00:19,  5.17it/s]Loading train:  62%|██████▏   | 164/266 [00:37<00:19,  5.21it/s]Loading train:  62%|██████▏   | 165/266 [00:38<00:19,  5.21it/s]Loading train:  62%|██████▏   | 166/266 [00:38<00:19,  5.23it/s]Loading train:  63%|██████▎   | 167/266 [00:38<00:18,  5.25it/s]Loading train:  63%|██████▎   | 168/266 [00:38<00:18,  5.24it/s]Loading train:  64%|██████▎   | 169/266 [00:38<00:18,  5.25it/s]Loading train:  64%|██████▍   | 170/266 [00:38<00:18,  5.26it/s]Loading train:  64%|██████▍   | 171/266 [00:39<00:18,  5.26it/s]Loading train:  65%|██████▍   | 172/266 [00:39<00:17,  5.25it/s]Loading train:  65%|██████▌   | 173/266 [00:39<00:18,  5.04it/s]Loading train:  65%|██████▌   | 174/266 [00:40<00:28,  3.21it/s]Loading train:  66%|██████▌   | 175/266 [00:41<00:44,  2.03it/s]Loading train:  66%|██████▌   | 176/266 [00:42<00:57,  1.57it/s]Loading train:  67%|██████▋   | 177/266 [00:42<00:55,  1.61it/s]Loading train:  67%|██████▋   | 178/266 [00:43<00:54,  1.60it/s]Loading train:  67%|██████▋   | 179/266 [00:44<01:05,  1.32it/s]Loading train:  68%|██████▊   | 180/266 [00:45<01:02,  1.37it/s]Loading train:  68%|██████▊   | 181/266 [00:45<01:00,  1.41it/s]Loading train:  68%|██████▊   | 182/266 [00:46<01:01,  1.36it/s]Loading train:  69%|██████▉   | 183/266 [00:47<01:00,  1.38it/s]Loading train:  69%|██████▉   | 184/266 [00:47<00:57,  1.43it/s]Loading train:  70%|██████▉   | 185/266 [00:48<00:54,  1.49it/s]Loading train:  70%|██████▉   | 186/266 [00:49<00:55,  1.44it/s]Loading train:  70%|███████   | 187/266 [00:49<00:54,  1.46it/s]Loading train:  71%|███████   | 188/266 [00:50<00:57,  1.35it/s]Loading train:  71%|███████   | 189/266 [00:51<00:55,  1.39it/s]Loading train:  71%|███████▏  | 190/266 [00:52<00:54,  1.39it/s]Loading train:  72%|███████▏  | 191/266 [00:52<00:56,  1.34it/s]Loading train:  72%|███████▏  | 192/266 [00:53<00:55,  1.33it/s]Loading train:  73%|███████▎  | 193/266 [00:54<00:54,  1.35it/s]Loading train:  73%|███████▎  | 194/266 [00:55<00:55,  1.30it/s]Loading train:  73%|███████▎  | 195/266 [00:55<00:54,  1.30it/s]Loading train:  74%|███████▎  | 196/266 [00:56<00:51,  1.37it/s]Loading train:  74%|███████▍  | 197/266 [00:57<00:52,  1.31it/s]Loading train:  74%|███████▍  | 198/266 [00:57<00:43,  1.55it/s]Loading train:  75%|███████▍  | 199/266 [00:58<00:38,  1.73it/s]Loading train:  75%|███████▌  | 200/266 [00:58<00:35,  1.84it/s]Loading train:  76%|███████▌  | 201/266 [00:59<00:32,  2.02it/s]Loading train:  76%|███████▌  | 202/266 [00:59<00:29,  2.14it/s]Loading train:  76%|███████▋  | 203/266 [00:59<00:29,  2.11it/s]Loading train:  77%|███████▋  | 204/266 [01:00<00:28,  2.21it/s]Loading train:  77%|███████▋  | 205/266 [01:00<00:27,  2.24it/s]Loading train:  77%|███████▋  | 206/266 [01:01<00:25,  2.34it/s]Loading train:  78%|███████▊  | 207/266 [01:01<00:26,  2.24it/s]Loading train:  78%|███████▊  | 208/266 [01:02<00:25,  2.23it/s]Loading train:  79%|███████▊  | 209/266 [01:02<00:24,  2.35it/s]Loading train:  79%|███████▉  | 210/266 [01:03<00:25,  2.17it/s]Loading train:  79%|███████▉  | 211/266 [01:03<00:26,  2.07it/s]Loading train:  80%|███████▉  | 212/266 [01:03<00:24,  2.22it/s]Loading train:  80%|████████  | 213/266 [01:04<00:23,  2.26it/s]Loading train:  80%|████████  | 214/266 [01:04<00:22,  2.34it/s]Loading train:  81%|████████  | 215/266 [01:05<00:25,  2.02it/s]Loading train:  81%|████████  | 216/266 [01:06<00:27,  1.85it/s]Loading train:  82%|████████▏ | 217/266 [01:06<00:26,  1.83it/s]Loading train:  82%|████████▏ | 218/266 [01:07<00:26,  1.78it/s]Loading train:  82%|████████▏ | 219/266 [01:07<00:27,  1.72it/s]Loading train:  83%|████████▎ | 220/266 [01:08<00:26,  1.74it/s]Loading train:  83%|████████▎ | 221/266 [01:09<00:26,  1.72it/s]Loading train:  83%|████████▎ | 222/266 [01:09<00:25,  1.75it/s]Loading train:  84%|████████▍ | 223/266 [01:10<00:24,  1.76it/s]Loading train:  84%|████████▍ | 224/266 [01:10<00:24,  1.74it/s]Loading train:  85%|████████▍ | 225/266 [01:11<00:23,  1.78it/s]Loading train:  85%|████████▍ | 226/266 [01:11<00:23,  1.72it/s]Loading train:  85%|████████▌ | 227/266 [01:12<00:22,  1.74it/s]Loading train:  86%|████████▌ | 228/266 [01:13<00:22,  1.66it/s]Loading train:  86%|████████▌ | 229/266 [01:13<00:22,  1.66it/s]Loading train:  86%|████████▋ | 230/266 [01:14<00:21,  1.68it/s]Loading train:  87%|████████▋ | 231/266 [01:14<00:21,  1.64it/s]Loading train:  87%|████████▋ | 232/266 [01:15<00:20,  1.62it/s]Loading train:  88%|████████▊ | 233/266 [01:16<00:20,  1.63it/s]Loading train:  88%|████████▊ | 234/266 [01:16<00:20,  1.58it/s]Loading train:  88%|████████▊ | 235/266 [01:17<00:20,  1.53it/s]Loading train:  89%|████████▊ | 236/266 [01:18<00:19,  1.54it/s]Loading train:  89%|████████▉ | 237/266 [01:18<00:18,  1.54it/s]Loading train:  89%|████████▉ | 238/266 [01:19<00:17,  1.56it/s]Loading train:  90%|████████▉ | 239/266 [01:20<00:17,  1.55it/s]Loading train:  90%|█████████ | 240/266 [01:20<00:16,  1.58it/s]Loading train:  91%|█████████ | 241/266 [01:21<00:15,  1.59it/s]Loading train:  91%|█████████ | 242/266 [01:21<00:15,  1.58it/s]Loading train:  91%|█████████▏| 243/266 [01:22<00:14,  1.64it/s]Loading train:  92%|█████████▏| 244/266 [01:23<00:13,  1.64it/s]Loading train:  92%|█████████▏| 245/266 [01:23<00:12,  1.66it/s]Loading train:  92%|█████████▏| 246/266 [01:24<00:12,  1.66it/s]Loading train:  93%|█████████▎| 247/266 [01:24<00:11,  1.67it/s]Loading train:  93%|█████████▎| 248/266 [01:25<00:10,  1.69it/s]Loading train:  94%|█████████▎| 249/266 [01:26<00:10,  1.59it/s]Loading train:  94%|█████████▍| 250/266 [01:26<00:10,  1.57it/s]Loading train:  94%|█████████▍| 251/266 [01:27<00:09,  1.57it/s]Loading train:  95%|█████████▍| 252/266 [01:28<00:09,  1.51it/s]Loading train:  95%|█████████▌| 253/266 [01:28<00:08,  1.53it/s]Loading train:  95%|█████████▌| 254/266 [01:29<00:08,  1.50it/s]Loading train:  96%|█████████▌| 255/266 [01:30<00:07,  1.44it/s]Loading train:  96%|█████████▌| 256/266 [01:30<00:06,  1.50it/s]Loading train:  97%|█████████▋| 257/266 [01:31<00:06,  1.44it/s]Loading train:  97%|█████████▋| 258/266 [01:32<00:05,  1.56it/s]Loading train:  97%|█████████▋| 259/266 [01:32<00:04,  1.61it/s]Loading train:  98%|█████████▊| 260/266 [01:33<00:03,  1.65it/s]Loading train:  98%|█████████▊| 261/266 [01:34<00:03,  1.60it/s]Loading train:  98%|█████████▊| 262/266 [01:34<00:02,  1.58it/s]Loading train:  99%|█████████▉| 263/266 [01:35<00:01,  1.62it/s]Loading train:  99%|█████████▉| 264/266 [01:35<00:01,  1.73it/s]Loading train: 100%|█████████▉| 265/266 [01:36<00:00,  1.72it/s]Loading train: 100%|██████████| 266/266 [01:36<00:00,  1.81it/s]Loading train: 100%|██████████| 266/266 [01:36<00:00,  2.75it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/266 [00:00<00:04, 54.29it/s]concatenating: train:   5%|▍         | 12/266 [00:00<00:04, 55.02it/s]concatenating: train:   7%|▋         | 18/266 [00:00<00:04, 55.39it/s]concatenating: train:   9%|▉         | 24/266 [00:00<00:04, 55.92it/s]concatenating: train:  11%|█▏        | 30/266 [00:00<00:04, 56.29it/s]concatenating: train:  14%|█▎        | 36/266 [00:00<00:04, 55.90it/s]concatenating: train:  16%|█▌        | 42/266 [00:00<00:03, 56.26it/s]concatenating: train:  18%|█▊        | 49/266 [00:00<00:03, 57.67it/s]concatenating: train:  21%|██        | 56/266 [00:00<00:03, 60.42it/s]concatenating: train:  24%|██▎       | 63/266 [00:01<00:03, 61.87it/s]concatenating: train:  26%|██▋       | 70/266 [00:01<00:03, 62.31it/s]concatenating: train:  29%|██▉       | 77/266 [00:01<00:02, 63.01it/s]concatenating: train:  32%|███▏      | 84/266 [00:01<00:02, 61.71it/s]concatenating: train:  34%|███▍      | 91/266 [00:01<00:02, 60.73it/s]concatenating: train:  37%|███▋      | 98/266 [00:01<00:02, 60.16it/s]concatenating: train:  39%|███▉      | 105/266 [00:01<00:02, 61.82it/s]concatenating: train:  42%|████▏     | 112/266 [00:01<00:02, 62.70it/s]concatenating: train:  45%|████▍     | 119/266 [00:01<00:02, 62.93it/s]concatenating: train:  47%|████▋     | 126/266 [00:02<00:02, 60.60it/s]concatenating: train:  50%|█████     | 133/266 [00:02<00:02, 59.00it/s]concatenating: train:  52%|█████▏    | 139/266 [00:02<00:02, 58.63it/s]concatenating: train:  55%|█████▍    | 146/266 [00:02<00:02, 59.51it/s]concatenating: train:  58%|█████▊    | 153/266 [00:02<00:01, 60.24it/s]concatenating: train:  61%|██████    | 161/266 [00:02<00:01, 63.16it/s]concatenating: train:  64%|██████▎   | 169/266 [00:02<00:01, 65.45it/s]concatenating: train:  66%|██████▌   | 176/266 [00:02<00:01, 64.76it/s]concatenating: train:  69%|██████▉   | 183/266 [00:03<00:01, 59.98it/s]concatenating: train:  71%|███████▏  | 190/266 [00:03<00:01, 57.12it/s]concatenating: train:  74%|███████▎  | 196/266 [00:03<00:01, 54.34it/s]concatenating: train:  76%|███████▌  | 202/266 [00:03<00:01, 54.69it/s]concatenating: train:  78%|███████▊  | 208/266 [00:03<00:01, 55.26it/s]concatenating: train:  80%|████████  | 214/266 [00:03<00:00, 55.20it/s]concatenating: train:  83%|████████▎ | 220/266 [00:03<00:00, 54.08it/s]concatenating: train:  85%|████████▍ | 226/266 [00:03<00:00, 53.39it/s]concatenating: train:  87%|████████▋ | 232/266 [00:03<00:00, 52.45it/s]concatenating: train:  89%|████████▉ | 238/266 [00:04<00:00, 51.55it/s]concatenating: train:  92%|█████████▏| 244/266 [00:04<00:00, 50.82it/s]concatenating: train:  94%|█████████▍| 250/266 [00:04<00:00, 50.01it/s]concatenating: train:  96%|█████████▌| 256/266 [00:04<00:00, 48.92it/s]concatenating: train:  98%|█████████▊| 261/266 [00:04<00:00, 48.24it/s]concatenating: train: 100%|██████████| 266/266 [00:04<00:00, 57.29it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:01,  1.59it/s]Loading test:  50%|█████     | 2/4 [00:01<00:01,  1.71it/s]Loading test:  75%|███████▌  | 3/4 [00:01<00:00,  1.76it/s]Loading test: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s]Loading test: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 69.29it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:00<03:13,  1.37it/s]Loading trainS:   1%|          | 2/266 [00:01<03:02,  1.44it/s]Loading trainS:   1%|          | 3/266 [00:01<02:44,  1.59it/s]Loading trainS:   2%|▏         | 4/266 [00:02<02:37,  1.66it/s]Loading trainS:   2%|▏         | 5/266 [00:02<02:32,  1.72it/s]Loading trainS:   2%|▏         | 6/266 [00:03<02:29,  1.74it/s]Loading trainS:   3%|▎         | 7/266 [00:04<02:27,  1.75it/s]Loading trainS:   3%|▎         | 8/266 [00:04<02:33,  1.68it/s]Loading trainS:   3%|▎         | 9/266 [00:05<02:34,  1.67it/s]Loading trainS:   4%|▍         | 10/266 [00:05<02:36,  1.63it/s]Loading trainS:   4%|▍         | 11/266 [00:06<02:39,  1.60it/s]Loading trainS:   5%|▍         | 12/266 [00:07<02:35,  1.63it/s]Loading trainS:   5%|▍         | 13/266 [00:07<02:33,  1.65it/s]Loading trainS:   5%|▌         | 14/266 [00:08<02:32,  1.66it/s]Loading trainS:   6%|▌         | 15/266 [00:09<02:43,  1.54it/s]Loading trainS:   6%|▌         | 16/266 [00:09<02:39,  1.57it/s]Loading trainS:   6%|▋         | 17/266 [00:10<02:35,  1.60it/s]Loading trainS:   7%|▋         | 18/266 [00:10<02:36,  1.59it/s]Loading trainS:   7%|▋         | 19/266 [00:11<02:37,  1.57it/s]Loading trainS:   8%|▊         | 20/266 [00:12<02:33,  1.60it/s]Loading trainS:   8%|▊         | 21/266 [00:12<02:30,  1.63it/s]Loading trainS:   8%|▊         | 22/266 [00:13<02:25,  1.68it/s]Loading trainS:   9%|▊         | 23/266 [00:13<02:24,  1.68it/s]Loading trainS:   9%|▉         | 24/266 [00:14<02:29,  1.61it/s]Loading trainS:   9%|▉         | 25/266 [00:15<02:24,  1.67it/s]Loading trainS:  10%|▉         | 26/266 [00:15<02:21,  1.70it/s]Loading trainS:  10%|█         | 27/266 [00:16<02:22,  1.68it/s]Loading trainS:  11%|█         | 28/266 [00:16<02:22,  1.68it/s]Loading trainS:  11%|█         | 29/266 [00:17<02:19,  1.70it/s]Loading trainS:  11%|█▏        | 30/266 [00:18<02:16,  1.73it/s]Loading trainS:  12%|█▏        | 31/266 [00:18<02:16,  1.72it/s]Loading trainS:  12%|█▏        | 32/266 [00:19<02:16,  1.71it/s]Loading trainS:  12%|█▏        | 33/266 [00:19<02:12,  1.76it/s]Loading trainS:  13%|█▎        | 34/266 [00:20<02:17,  1.68it/s]Loading trainS:  13%|█▎        | 35/266 [00:21<02:20,  1.64it/s]Loading trainS:  14%|█▎        | 36/266 [00:21<02:22,  1.61it/s]Loading trainS:  14%|█▍        | 37/266 [00:22<02:20,  1.63it/s]Loading trainS:  14%|█▍        | 38/266 [00:22<02:21,  1.61it/s]Loading trainS:  15%|█▍        | 39/266 [00:23<02:19,  1.63it/s]Loading trainS:  15%|█▌        | 40/266 [00:24<02:14,  1.68it/s]Loading trainS:  15%|█▌        | 41/266 [00:24<02:16,  1.65it/s]Loading trainS:  16%|█▌        | 42/266 [00:25<02:05,  1.78it/s]Loading trainS:  16%|█▌        | 43/266 [00:25<01:57,  1.91it/s]Loading trainS:  17%|█▋        | 44/266 [00:26<01:57,  1.89it/s]Loading trainS:  17%|█▋        | 45/266 [00:26<01:54,  1.94it/s]Loading trainS:  17%|█▋        | 46/266 [00:27<01:52,  1.95it/s]Loading trainS:  18%|█▊        | 47/266 [00:27<01:49,  2.00it/s]Loading trainS:  18%|█▊        | 48/266 [00:28<01:48,  2.01it/s]Loading trainS:  18%|█▊        | 49/266 [00:28<01:46,  2.04it/s]Loading trainS:  19%|█▉        | 50/266 [00:29<01:41,  2.14it/s]Loading trainS:  19%|█▉        | 51/266 [00:29<01:46,  2.02it/s]Loading trainS:  20%|█▉        | 52/266 [00:30<01:53,  1.89it/s]Loading trainS:  20%|█▉        | 53/266 [00:30<01:49,  1.94it/s]Loading trainS:  20%|██        | 54/266 [00:31<01:43,  2.04it/s]Loading trainS:  21%|██        | 55/266 [00:31<01:46,  1.99it/s]Loading trainS:  21%|██        | 56/266 [00:32<01:46,  1.96it/s]Loading trainS:  21%|██▏       | 57/266 [00:32<01:48,  1.93it/s]Loading trainS:  22%|██▏       | 58/266 [00:33<01:45,  1.98it/s]Loading trainS:  22%|██▏       | 59/266 [00:33<01:44,  1.97it/s]Loading trainS:  23%|██▎       | 60/266 [00:34<01:48,  1.90it/s]Loading trainS:  23%|██▎       | 61/266 [00:34<01:50,  1.85it/s]Loading trainS:  23%|██▎       | 62/266 [00:35<01:56,  1.75it/s]Loading trainS:  24%|██▎       | 63/266 [00:35<01:51,  1.83it/s]Loading trainS:  24%|██▍       | 64/266 [00:36<01:50,  1.83it/s]Loading trainS:  24%|██▍       | 65/266 [00:37<01:47,  1.86it/s]Loading trainS:  25%|██▍       | 66/266 [00:37<01:45,  1.90it/s]Loading trainS:  25%|██▌       | 67/266 [00:38<01:42,  1.94it/s]Loading trainS:  26%|██▌       | 68/266 [00:38<01:39,  1.98it/s]Loading trainS:  26%|██▌       | 69/266 [00:38<01:37,  2.02it/s]Loading trainS:  26%|██▋       | 70/266 [00:39<01:42,  1.91it/s]Loading trainS:  27%|██▋       | 71/266 [00:40<01:42,  1.89it/s]Loading trainS:  27%|██▋       | 72/266 [00:40<01:40,  1.92it/s]Loading trainS:  27%|██▋       | 73/266 [00:41<01:41,  1.91it/s]Loading trainS:  28%|██▊       | 74/266 [00:41<01:37,  1.97it/s]Loading trainS:  28%|██▊       | 75/266 [00:42<01:44,  1.82it/s]Loading trainS:  29%|██▊       | 76/266 [00:42<01:49,  1.74it/s]Loading trainS:  29%|██▉       | 77/266 [00:43<01:52,  1.68it/s]Loading trainS:  29%|██▉       | 78/266 [00:44<01:53,  1.66it/s]Loading trainS:  30%|██▉       | 79/266 [00:44<01:57,  1.60it/s]Loading trainS:  30%|███       | 80/266 [00:45<01:58,  1.57it/s]Loading trainS:  30%|███       | 81/266 [00:46<01:55,  1.60it/s]Loading trainS:  31%|███       | 82/266 [00:46<01:52,  1.63it/s]Loading trainS:  31%|███       | 83/266 [00:47<01:51,  1.65it/s]Loading trainS:  32%|███▏      | 84/266 [00:47<01:49,  1.66it/s]Loading trainS:  32%|███▏      | 85/266 [00:48<01:53,  1.59it/s]Loading trainS:  32%|███▏      | 86/266 [00:49<01:52,  1.60it/s]Loading trainS:  33%|███▎      | 87/266 [00:49<01:47,  1.67it/s]Loading trainS:  33%|███▎      | 88/266 [00:50<01:48,  1.63it/s]Loading trainS:  33%|███▎      | 89/266 [00:50<01:51,  1.59it/s]Loading trainS:  34%|███▍      | 90/266 [00:51<01:47,  1.64it/s]Loading trainS:  34%|███▍      | 91/266 [00:52<01:45,  1.66it/s]Loading trainS:  35%|███▍      | 92/266 [00:52<01:45,  1.65it/s]Loading trainS:  35%|███▍      | 93/266 [00:53<01:47,  1.61it/s]Loading trainS:  35%|███▌      | 94/266 [00:54<01:48,  1.58it/s]Loading trainS:  36%|███▌      | 95/266 [00:54<01:45,  1.62it/s]Loading trainS:  36%|███▌      | 96/266 [00:55<01:47,  1.58it/s]Loading trainS:  36%|███▋      | 97/266 [00:55<01:48,  1.56it/s]Loading trainS:  37%|███▋      | 98/266 [00:56<01:46,  1.58it/s]Loading trainS:  37%|███▋      | 99/266 [00:57<01:39,  1.67it/s]Loading trainS:  38%|███▊      | 100/266 [00:57<01:38,  1.68it/s]Loading trainS:  38%|███▊      | 101/266 [00:58<01:35,  1.73it/s]Loading trainS:  38%|███▊      | 102/266 [00:58<01:33,  1.75it/s]Loading trainS:  39%|███▊      | 103/266 [00:59<01:35,  1.70it/s]Loading trainS:  39%|███▉      | 104/266 [00:59<01:34,  1.71it/s]Loading trainS:  39%|███▉      | 105/266 [01:00<01:33,  1.73it/s]Loading trainS:  40%|███▉      | 106/266 [01:01<01:30,  1.77it/s]Loading trainS:  40%|████      | 107/266 [01:01<01:29,  1.78it/s]Loading trainS:  41%|████      | 108/266 [01:02<01:25,  1.85it/s]Loading trainS:  41%|████      | 109/266 [01:02<01:25,  1.84it/s]Loading trainS:  41%|████▏     | 110/266 [01:03<01:27,  1.79it/s]Loading trainS:  42%|████▏     | 111/266 [01:03<01:28,  1.75it/s]Loading trainS:  42%|████▏     | 112/266 [01:04<01:29,  1.72it/s]Loading trainS:  42%|████▏     | 113/266 [01:05<01:27,  1.75it/s]Loading trainS:  43%|████▎     | 114/266 [01:05<01:22,  1.83it/s]Loading trainS:  43%|████▎     | 115/266 [01:06<01:20,  1.87it/s]Loading trainS:  44%|████▎     | 116/266 [01:06<01:21,  1.84it/s]Loading trainS:  44%|████▍     | 117/266 [01:07<01:20,  1.85it/s]Loading trainS:  44%|████▍     | 118/266 [01:07<01:19,  1.85it/s]Loading trainS:  45%|████▍     | 119/266 [01:08<01:23,  1.76it/s]Loading trainS:  45%|████▌     | 120/266 [01:08<01:27,  1.68it/s]Loading trainS:  45%|████▌     | 121/266 [01:09<01:26,  1.67it/s]Loading trainS:  46%|████▌     | 122/266 [01:10<01:23,  1.72it/s]Loading trainS:  46%|████▌     | 123/266 [01:10<01:28,  1.61it/s]Loading trainS:  47%|████▋     | 124/266 [01:11<01:32,  1.54it/s]Loading trainS:  47%|████▋     | 125/266 [01:12<01:28,  1.59it/s]Loading trainS:  47%|████▋     | 126/266 [01:12<01:26,  1.61it/s]Loading trainS:  48%|████▊     | 127/266 [01:13<01:25,  1.63it/s]Loading trainS:  48%|████▊     | 128/266 [01:14<01:29,  1.54it/s]Loading trainS:  48%|████▊     | 129/266 [01:14<01:30,  1.51it/s]Loading trainS:  49%|████▉     | 130/266 [01:15<01:28,  1.55it/s]Loading trainS:  49%|████▉     | 131/266 [01:15<01:25,  1.59it/s]Loading trainS:  50%|████▉     | 132/266 [01:16<01:27,  1.53it/s]Loading trainS:  50%|█████     | 133/266 [01:17<01:28,  1.50it/s]Loading trainS:  50%|█████     | 134/266 [01:18<01:27,  1.51it/s]Loading trainS:  51%|█████     | 135/266 [01:18<01:28,  1.47it/s]Loading trainS:  51%|█████     | 136/266 [01:19<01:23,  1.56it/s]Loading trainS:  52%|█████▏    | 137/266 [01:19<01:23,  1.54it/s]Loading trainS:  52%|█████▏    | 138/266 [01:20<01:19,  1.61it/s]Loading trainS:  52%|█████▏    | 139/266 [01:21<01:17,  1.64it/s]Loading trainS:  53%|█████▎    | 140/266 [01:21<01:18,  1.61it/s]Loading trainS:  53%|█████▎    | 141/266 [01:22<01:18,  1.59it/s]Loading trainS:  53%|█████▎    | 142/266 [01:23<01:18,  1.57it/s]Loading trainS:  54%|█████▍    | 143/266 [01:23<01:16,  1.61it/s]Loading trainS:  54%|█████▍    | 144/266 [01:24<01:14,  1.63it/s]Loading trainS:  55%|█████▍    | 145/266 [01:24<01:13,  1.65it/s]Loading trainS:  55%|█████▍    | 146/266 [01:25<01:13,  1.62it/s]Loading trainS:  55%|█████▌    | 147/266 [01:25<01:11,  1.67it/s]Loading trainS:  56%|█████▌    | 148/266 [01:26<01:10,  1.66it/s]Loading trainS:  56%|█████▌    | 149/266 [01:27<01:11,  1.65it/s]Loading trainS:  56%|█████▋    | 150/266 [01:27<01:11,  1.62it/s]Loading trainS:  57%|█████▋    | 151/266 [01:28<01:08,  1.69it/s]Loading trainS:  57%|█████▋    | 152/266 [01:28<01:03,  1.79it/s]Loading trainS:  58%|█████▊    | 153/266 [01:29<01:06,  1.71it/s]Loading trainS:  58%|█████▊    | 154/266 [01:30<01:07,  1.65it/s]Loading trainS:  58%|█████▊    | 155/266 [01:30<01:02,  1.79it/s]Loading trainS:  59%|█████▊    | 156/266 [01:31<00:59,  1.85it/s]Loading trainS:  59%|█████▉    | 157/266 [01:31<00:58,  1.86it/s]Loading trainS:  59%|█████▉    | 158/266 [01:32<00:59,  1.82it/s]Loading trainS:  60%|█████▉    | 159/266 [01:32<00:56,  1.89it/s]Loading trainS:  60%|██████    | 160/266 [01:33<00:56,  1.89it/s]Loading trainS:  61%|██████    | 161/266 [01:33<00:52,  2.00it/s]Loading trainS:  61%|██████    | 162/266 [01:34<00:52,  1.97it/s]Loading trainS:  61%|██████▏   | 163/266 [01:34<00:51,  1.99it/s]Loading trainS:  62%|██████▏   | 164/266 [01:35<00:50,  2.00it/s]Loading trainS:  62%|██████▏   | 165/266 [01:35<00:50,  1.99it/s]Loading trainS:  62%|██████▏   | 166/266 [01:36<00:53,  1.88it/s]Loading trainS:  63%|██████▎   | 167/266 [01:36<00:53,  1.85it/s]Loading trainS:  63%|██████▎   | 168/266 [01:37<00:53,  1.85it/s]Loading trainS:  64%|██████▎   | 169/266 [01:37<00:49,  1.97it/s]Loading trainS:  64%|██████▍   | 170/266 [01:38<00:50,  1.92it/s]Loading trainS:  64%|██████▍   | 171/266 [01:38<00:48,  1.96it/s]Loading trainS:  65%|██████▍   | 172/266 [01:39<00:45,  2.07it/s]Loading trainS:  65%|██████▌   | 173/266 [01:39<00:47,  1.96it/s]Loading trainS:  65%|██████▌   | 174/266 [01:40<00:48,  1.88it/s]Loading trainS:  66%|██████▌   | 175/266 [01:40<00:48,  1.86it/s]Loading trainS:  66%|██████▌   | 176/266 [01:41<00:48,  1.85it/s]Loading trainS:  67%|██████▋   | 177/266 [01:42<00:49,  1.80it/s]Loading trainS:  67%|██████▋   | 178/266 [01:42<00:47,  1.87it/s]Loading trainS:  67%|██████▋   | 179/266 [01:43<00:48,  1.80it/s]Loading trainS:  68%|██████▊   | 180/266 [01:43<00:48,  1.78it/s]Loading trainS:  68%|██████▊   | 181/266 [01:44<00:48,  1.75it/s]Loading trainS:  68%|██████▊   | 182/266 [01:45<00:49,  1.68it/s]Loading trainS:  69%|██████▉   | 183/266 [01:45<00:47,  1.74it/s]Loading trainS:  69%|██████▉   | 184/266 [01:46<00:45,  1.82it/s]Loading trainS:  70%|██████▉   | 185/266 [01:46<00:45,  1.76it/s]Loading trainS:  70%|██████▉   | 186/266 [01:47<00:46,  1.73it/s]Loading trainS:  70%|███████   | 187/266 [01:47<00:44,  1.77it/s]Loading trainS:  71%|███████   | 188/266 [01:48<00:42,  1.84it/s]Loading trainS:  71%|███████   | 189/266 [01:48<00:41,  1.85it/s]Loading trainS:  71%|███████▏  | 190/266 [01:49<00:41,  1.82it/s]Loading trainS:  72%|███████▏  | 191/266 [01:49<00:42,  1.78it/s]Loading trainS:  72%|███████▏  | 192/266 [01:50<00:39,  1.88it/s]Loading trainS:  73%|███████▎  | 193/266 [01:50<00:38,  1.87it/s]Loading trainS:  73%|███████▎  | 194/266 [01:51<00:40,  1.77it/s]Loading trainS:  73%|███████▎  | 195/266 [01:52<00:41,  1.73it/s]Loading trainS:  74%|███████▎  | 196/266 [01:52<00:39,  1.77it/s]Loading trainS:  74%|███████▍  | 197/266 [01:53<00:38,  1.80it/s]Loading trainS:  74%|███████▍  | 198/266 [01:53<00:38,  1.76it/s]Loading trainS:  75%|███████▍  | 199/266 [01:54<00:37,  1.80it/s]Loading trainS:  75%|███████▌  | 200/266 [01:54<00:36,  1.81it/s]Loading trainS:  76%|███████▌  | 201/266 [01:55<00:37,  1.74it/s]Loading trainS:  76%|███████▌  | 202/266 [01:56<00:36,  1.76it/s]Loading trainS:  76%|███████▋  | 203/266 [01:56<00:33,  1.87it/s]Loading trainS:  77%|███████▋  | 204/266 [01:57<00:33,  1.83it/s]Loading trainS:  77%|███████▋  | 205/266 [01:57<00:32,  1.88it/s]Loading trainS:  77%|███████▋  | 206/266 [01:58<00:34,  1.75it/s]Loading trainS:  78%|███████▊  | 207/266 [01:59<00:35,  1.68it/s]Loading trainS:  78%|███████▊  | 208/266 [01:59<00:33,  1.75it/s]Loading trainS:  79%|███████▊  | 209/266 [02:00<00:32,  1.74it/s]Loading trainS:  79%|███████▉  | 210/266 [02:00<00:30,  1.83it/s]Loading trainS:  79%|███████▉  | 211/266 [02:01<00:30,  1.82it/s]Loading trainS:  80%|███████▉  | 212/266 [02:01<00:30,  1.77it/s]Loading trainS:  80%|████████  | 213/266 [02:02<00:29,  1.79it/s]Loading trainS:  80%|████████  | 214/266 [02:02<00:28,  1.83it/s]Loading trainS:  81%|████████  | 215/266 [02:03<00:28,  1.79it/s]Loading trainS:  81%|████████  | 216/266 [02:03<00:28,  1.77it/s]Loading trainS:  82%|████████▏ | 217/266 [02:04<00:26,  1.82it/s]Loading trainS:  82%|████████▏ | 218/266 [02:04<00:25,  1.88it/s]Loading trainS:  82%|████████▏ | 219/266 [02:05<00:25,  1.85it/s]Loading trainS:  83%|████████▎ | 220/266 [02:06<00:25,  1.80it/s]Loading trainS:  83%|████████▎ | 221/266 [02:06<00:25,  1.76it/s]Loading trainS:  83%|████████▎ | 222/266 [02:07<00:24,  1.78it/s]Loading trainS:  84%|████████▍ | 223/266 [02:07<00:24,  1.75it/s]Loading trainS:  84%|████████▍ | 224/266 [02:08<00:23,  1.76it/s]Loading trainS:  85%|████████▍ | 225/266 [02:08<00:22,  1.80it/s]Loading trainS:  85%|████████▍ | 226/266 [02:09<00:22,  1.81it/s]Loading trainS:  85%|████████▌ | 227/266 [02:10<00:22,  1.73it/s]Loading trainS:  86%|████████▌ | 228/266 [02:10<00:21,  1.80it/s]Loading trainS:  86%|████████▌ | 229/266 [02:11<00:21,  1.75it/s]Loading trainS:  86%|████████▋ | 230/266 [02:11<00:20,  1.75it/s]Loading trainS:  87%|████████▋ | 231/266 [02:12<00:20,  1.71it/s]Loading trainS:  87%|████████▋ | 232/266 [02:13<00:20,  1.64it/s]Loading trainS:  88%|████████▊ | 233/266 [02:13<00:20,  1.64it/s]Loading trainS:  88%|████████▊ | 234/266 [02:14<00:18,  1.72it/s]Loading trainS:  88%|████████▊ | 235/266 [02:14<00:17,  1.73it/s]Loading trainS:  89%|████████▊ | 236/266 [02:15<00:17,  1.73it/s]Loading trainS:  89%|████████▉ | 237/266 [02:15<00:16,  1.71it/s]Loading trainS:  89%|████████▉ | 238/266 [02:16<00:15,  1.75it/s]Loading trainS:  90%|████████▉ | 239/266 [02:17<00:15,  1.73it/s]Loading trainS:  90%|█████████ | 240/266 [02:17<00:15,  1.68it/s]Loading trainS:  91%|█████████ | 241/266 [02:18<00:15,  1.63it/s]Loading trainS:  91%|█████████ | 242/266 [02:18<00:14,  1.65it/s]Loading trainS:  91%|█████████▏| 243/266 [02:19<00:14,  1.62it/s]Loading trainS:  92%|█████████▏| 244/266 [02:20<00:13,  1.62it/s]Loading trainS:  92%|█████████▏| 245/266 [02:20<00:13,  1.60it/s]Loading trainS:  92%|█████████▏| 246/266 [02:21<00:12,  1.61it/s]Loading trainS:  93%|█████████▎| 247/266 [02:22<00:11,  1.62it/s]Loading trainS:  93%|█████████▎| 248/266 [02:22<00:11,  1.63it/s]Loading trainS:  94%|█████████▎| 249/266 [02:23<00:10,  1.59it/s]Loading trainS:  94%|█████████▍| 250/266 [02:24<00:10,  1.54it/s]Loading trainS:  94%|█████████▍| 251/266 [02:24<00:09,  1.56it/s]Loading trainS:  95%|█████████▍| 252/266 [02:25<00:09,  1.52it/s]Loading trainS:  95%|█████████▌| 253/266 [02:26<00:08,  1.50it/s]Loading trainS:  95%|█████████▌| 254/266 [02:26<00:07,  1.55it/s]Loading trainS:  96%|█████████▌| 255/266 [02:27<00:06,  1.60it/s]Loading trainS:  96%|█████████▌| 256/266 [02:27<00:06,  1.55it/s]Loading trainS:  97%|█████████▋| 257/266 [02:28<00:05,  1.58it/s]Loading trainS:  97%|█████████▋| 258/266 [02:29<00:04,  1.62it/s]Loading trainS:  97%|█████████▋| 259/266 [02:29<00:04,  1.59it/s]Loading trainS:  98%|█████████▊| 260/266 [02:30<00:03,  1.58it/s]Loading trainS:  98%|█████████▊| 261/266 [02:31<00:03,  1.54it/s]Loading trainS:  98%|█████████▊| 262/266 [02:31<00:02,  1.55it/s]Loading trainS:  99%|█████████▉| 263/266 [02:32<00:01,  1.59it/s]Loading trainS:  99%|█████████▉| 264/266 [02:33<00:01,  1.53it/s]Loading trainS: 100%|█████████▉| 265/266 [02:33<00:00,  1.54it/s]Loading trainS: 100%|██████████| 266/266 [02:34<00:00,  1.52it/s]Loading trainS: 100%|██████████| 266/266 [02:34<00:00,  1.72it/s]
Loading testS:   0%|          | 0/4 [00:00<?, ?it/s]Loading testS:  25%|██▌       | 1/4 [00:00<00:01,  1.58it/s]Loading testS:  50%|█████     | 2/4 [00:01<00:01,  1.57it/s]Loading testS:  75%|███████▌  | 3/4 [00:01<00:00,  1.56it/s]Loading testS: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]Loading testS: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]----------+++ 
CrossVal ['a']
CrossVal ['a']
(0/4) test vimp2_A_CSFn2
(1/4) test vimp2_ANON967_CSFn2
(2/4) test vimp2_B_CSFn2
(3/4) test vimp2_E_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 256 samples
Epoch 1/300
 - 83s - loss: 0.1645 - acc: 0.9811 - mDice: 0.6811 - val_loss: 0.2917 - val_acc: 0.9894 - val_mDice: 0.4169

Epoch 00001: val_mDice improved from -inf to 0.41692, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 78s - loss: 0.1021 - acc: 0.9892 - mDice: 0.8014 - val_loss: 0.2929 - val_acc: 0.9893 - val_mDice: 0.4200

Epoch 00002: val_mDice improved from 0.41692 to 0.42001, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 78s - loss: 0.0898 - acc: 0.9907 - mDice: 0.8252 - val_loss: 0.2842 - val_acc: 0.9914 - val_mDice: 0.4362

Epoch 00003: val_mDice improved from 0.42001 to 0.43624, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300
 - 79s - loss: 0.0790 - acc: 0.9916 - mDice: 0.8464 - val_loss: 0.2822 - val_acc: 0.9911 - val_mDice: 0.4404

Epoch 00004: val_mDice improved from 0.43624 to 0.44044, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300
 - 79s - loss: 0.0739 - acc: 0.9922 - mDice: 0.8563 - val_loss: 0.2254 - val_acc: 0.9924 - val_mDice: 0.4458

Epoch 00005: val_mDice improved from 0.44044 to 0.44577, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300
 - 78s - loss: 0.0689 - acc: 0.9926 - mDice: 0.8659 - val_loss: 0.2428 - val_acc: 0.9924 - val_mDice: 0.4501

Epoch 00006: val_mDice improved from 0.44577 to 0.45014, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/300
 - 78s - loss: 0.0647 - acc: 0.9929 - mDice: 0.8743 - val_loss: 0.2143 - val_acc: 0.9924 - val_mDice: 0.4540

Epoch 00007: val_mDice improved from 0.45014 to 0.45398, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/300
 - 78s - loss: 0.0643 - acc: 0.9931 - mDice: 0.8750 - val_loss: 0.2379 - val_acc: 0.9909 - val_mDice: 0.4532

Epoch 00008: val_mDice did not improve from 0.45398
Epoch 9/300
 - 77s - loss: 0.0614 - acc: 0.9934 - mDice: 0.8807 - val_loss: 0.2039 - val_acc: 0.9927 - val_mDice: 0.4718

Epoch 00009: val_mDice improved from 0.45398 to 0.47177, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 10/300
 - 78s - loss: 0.0604 - acc: 0.9936 - mDice: 0.8825 - val_loss: 0.2072 - val_acc: 0.9934 - val_mDice: 0.4642

Epoch 00010: val_mDice did not improve from 0.47177
Epoch 11/300
 - 78s - loss: 0.0585 - acc: 0.9937 - mDice: 0.8863 - val_loss: 0.2032 - val_acc: 0.9933 - val_mDice: 0.4723

Epoch 00011: val_mDice improved from 0.47177 to 0.47229, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 12/300
 - 78s - loss: 0.0576 - acc: 0.9938 - mDice: 0.8880 - val_loss: 0.1327 - val_acc: 0.9933 - val_mDice: 0.4722

Epoch 00012: val_mDice did not improve from 0.47229
Epoch 13/300
 - 78s - loss: 0.0569 - acc: 0.9940 - mDice: 0.8893 - val_loss: 0.1633 - val_acc: 0.9936 - val_mDice: 0.4857

Epoch 00013: val_mDice improved from 0.47229 to 0.48574, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 14/300
 - 78s - loss: 0.0560 - acc: 0.9940 - mDice: 0.8910 - val_loss: 0.1551 - val_acc: 0.9934 - val_mDice: 0.4906

Epoch 00014: val_mDice improved from 0.48574 to 0.49061, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 15/300
 - 78s - loss: 0.0528 - acc: 0.9942 - mDice: 0.8974 - val_loss: 0.1938 - val_acc: 0.9934 - val_mDice: 0.4911

Epoch 00015: val_mDice improved from 0.49061 to 0.49110, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 16/300
 - 78s - loss: 0.0536 - acc: 0.9942 - mDice: 0.8957 - val_loss: 0.1451 - val_acc: 0.9932 - val_mDice: 0.4907

Epoch 00016: val_mDice did not improve from 0.49110
Epoch 17/300
 - 78s - loss: 0.0505 - acc: 0.9944 - mDice: 0.9018 - val_loss: 0.1910 - val_acc: 0.9936 - val_mDice: 0.4966

Epoch 00017: val_mDice improved from 0.49110 to 0.49662, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 18/300
 - 78s - loss: 0.0503 - acc: 0.9945 - mDice: 0.9023 - val_loss: 0.1522 - val_acc: 0.9937 - val_mDice: 0.4962

Epoch 00018: val_mDice did not improve from 0.49662
Epoch 19/300
 - 78s - loss: 0.0500 - acc: 0.9945 - mDice: 0.9028 - val_loss: 0.2267 - val_acc: 0.9914 - val_mDice: 0.4733

Epoch 00019: val_mDice did not improve from 0.49662
Epoch 20/300
 - 78s - loss: 0.0495 - acc: 0.9946 - mDice: 0.9037 - val_loss: 0.1499 - val_acc: 0.9937 - val_mDice: 0.5014

Epoch 00020: val_mDice improved from 0.49662 to 0.50141, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 21/300
 - 78s - loss: 0.0484 - acc: 0.9947 - mDice: 0.9060 - val_loss: 0.1423 - val_acc: 0.9937 - val_mDice: 0.5030

Epoch 00021: val_mDice improved from 0.50141 to 0.50297, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 22/300
 - 78s - loss: 0.0486 - acc: 0.9947 - mDice: 0.9055 - val_loss: 0.1114 - val_acc: 0.9937 - val_mDice: 0.4997

Epoch 00022: val_mDice did not improve from 0.50297
Epoch 23/300
 - 78s - loss: 0.0476 - acc: 0.9948 - mDice: 0.9074 - val_loss: 0.1109 - val_acc: 0.9937 - val_mDice: 0.5007

Epoch 00023: val_mDice did not improve from 0.50297
Epoch 24/300
 - 78s - loss: 0.0469 - acc: 0.9948 - mDice: 0.9088 - val_loss: 0.1526 - val_acc: 0.9929 - val_mDice: 0.4973

Epoch 00024: val_mDice did not improve from 0.50297
Epoch 25/300
 - 79s - loss: 0.0456 - acc: 0.9949 - mDice: 0.9114 - val_loss: 0.1084 - val_acc: 0.9938 - val_mDice: 0.5054

Epoch 00025: val_mDice improved from 0.50297 to 0.50536, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 26/300
 - 79s - loss: 0.0451 - acc: 0.9949 - mDice: 0.9123 - val_loss: 0.1893 - val_acc: 0.9935 - val_mDice: 0.5000

Epoch 00026: val_mDice did not improve from 0.50536
Epoch 27/300
 - 79s - loss: 0.0454 - acc: 0.9950 - mDice: 0.9117 - val_loss: 0.1107 - val_acc: 0.9937 - val_mDice: 0.5009

Epoch 00027: val_mDice did not improve from 0.50536
Epoch 28/300
 - 78s - loss: 0.0440 - acc: 0.9951 - mDice: 0.9144 - val_loss: 0.1881 - val_acc: 0.9938 - val_mDice: 0.5004

Epoch 00028: val_mDice did not improve from 0.50536
Epoch 29/300
 - 78s - loss: 0.0437 - acc: 0.9951 - mDice: 0.9151 - val_loss: 0.1093 - val_acc: 0.9940 - val_mDice: 0.5027

Epoch 00029: val_mDice did not improve from 0.50536
Epoch 30/300
 - 79s - loss: 0.0432 - acc: 0.9951 - mDice: 0.9161 - val_loss: 0.1493 - val_acc: 0.9938 - val_mDice: 0.5014

Epoch 00030: val_mDice did not improve from 0.50536
Epoch 31/300
 - 79s - loss: 0.0427 - acc: 0.9952 - mDice: 0.9170 - val_loss: 0.1469 - val_acc: 0.9939 - val_mDice: 0.5065

Epoch 00031: val_mDice improved from 0.50536 to 0.50651, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 32/300
 - 79s - loss: 0.0422 - acc: 0.9952 - mDice: 0.9181 - val_loss: 0.1524 - val_acc: 0.9933 - val_mDice: 0.4958

Epoch 00032: val_mDice did not improve from 0.50651
Epoch 33/300
 - 78s - loss: 0.0419 - acc: 0.9952 - mDice: 0.9187 - val_loss: 0.1093 - val_acc: 0.9937 - val_mDice: 0.5043

Epoch 00033: val_mDice did not improve from 0.50651
Epoch 34/300
 - 78s - loss: 0.0413 - acc: 0.9952 - mDice: 0.9197 - val_loss: 0.1506 - val_acc: 0.9935 - val_mDice: 0.4994

Epoch 00034: val_mDice did not improve from 0.50651
Epoch 35/300
 - 79s - loss: 0.0409 - acc: 0.9953 - mDice: 0.9205 - val_loss: 0.1486 - val_acc: 0.9938 - val_mDice: 0.5034

Epoch 00035: val_mDice did not improve from 0.50651
Epoch 36/300
 - 78s - loss: 0.0412 - acc: 0.9953 - mDice: 0.9199 - val_loss: 0.0726 - val_acc: 0.9936 - val_mDice: 0.4992

Epoch 00036: val_mDice did not improve from 0.50651
Epoch 37/300
 - 78s - loss: 0.0408 - acc: 0.9953 - mDice: 0.9207 - val_loss: 0.1095 - val_acc: 0.9938 - val_mDice: 0.5032

Epoch 00037: val_mDice did not improve from 0.50651
Epoch 38/300
 - 78s - loss: 0.0404 - acc: 0.9954 - mDice: 0.9214 - val_loss: 0.1496 - val_acc: 0.9937 - val_mDice: 0.5011

Epoch 00038: val_mDice did not improve from 0.50651
Epoch 39/300
 - 78s - loss: 0.0399 - acc: 0.9954 - mDice: 0.9225 - val_loss: 0.1879 - val_acc: 0.9936 - val_mDice: 0.5028

Epoch 00039: val_mDice did not improve from 0.50651
Epoch 40/300
 - 79s - loss: 0.0393 - acc: 0.9954 - mDice: 0.9237 - val_loss: 0.1835 - val_acc: 0.9940 - val_mDice: 0.5045

Epoch 00040: val_mDice did not improve from 0.50651
Epoch 41/300
 - 79s - loss: 0.0392 - acc: 0.9955 - mDice: 0.9238 - val_loss: 0.1139 - val_acc: 0.9937 - val_mDice: 0.4977

Epoch 00041: val_mDice did not improve from 0.50651
Epoch 42/300
 - 79s - loss: 0.0390 - acc: 0.9955 - mDice: 0.9242 - val_loss: 0.1518 - val_acc: 0.9937 - val_mDice: 0.4959

Epoch 00042: val_mDice did not improve from 0.50651
Epoch 43/300
 - 79s - loss: 0.0383 - acc: 0.9955 - mDice: 0.9256 - val_loss: 0.0738 - val_acc: 0.9938 - val_mDice: 0.4984

Epoch 00043: val_mDice did not improve from 0.50651
Epoch 44/300
 - 80s - loss: 0.0387 - acc: 0.9955 - mDice: 0.9248 - val_loss: 0.0713 - val_acc: 0.9939 - val_mDice: 0.4991

Epoch 00044: val_mDice did not improve from 0.50651
Epoch 45/300
 - 79s - loss: 0.0381 - acc: 0.9955 - mDice: 0.9261 - val_loss: 0.2179 - val_acc: 0.9928 - val_mDice: 0.4905

Epoch 00045: val_mDice did not improve from 0.50651
Epoch 46/300
 - 79s - loss: 0.0375 - acc: 0.9956 - mDice: 0.9271 - val_loss: 0.1082 - val_acc: 0.9940 - val_mDice: 0.5022

Epoch 00046: val_mDice did not improve from 0.50651

Epoch 00046: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 47/300
 - 79s - loss: 0.0372 - acc: 0.9956 - mDice: 0.9277 - val_loss: 0.1096 - val_acc: 0.9938 - val_mDice: 0.5032

Epoch 00047: val_mDice did not improve from 0.50651
Epoch 48/300
 - 79s - loss: 0.0372 - acc: 0.9956 - mDice: 0.9277 - val_loss: 0.1117 - val_acc: 0.9934 - val_mDice: 0.4999

Epoch 00048: val_mDice did not improve from 0.50651
Epoch 49/300
 - 79s - loss: 0.0364 - acc: 0.9957 - mDice: 0.9294 - val_loss: 0.1488 - val_acc: 0.9939 - val_mDice: 0.5025

Epoch 00049: val_mDice did not improve from 0.50651
Epoch 50/300
 - 79s - loss: 0.0366 - acc: 0.9957 - mDice: 0.9290 - val_loss: 0.1502 - val_acc: 0.9935 - val_mDice: 0.4991

Epoch 00050: val_mDice did not improve from 0.50651
Epoch 51/300
 - 79s - loss: 0.0362 - acc: 0.9957 - mDice: 0.9297 - val_loss: 0.0714 - val_acc: 0.9940 - val_mDice: 0.5028

Epoch 00051: val_mDice did not improve from 0.50651
Epoch 52/300
 - 80s - loss: 0.0364 - acc: 0.9957 - mDice: 0.9293 - val_loss: 0.1115 - val_acc: 0.9938 - val_mDice: 0.4992

Epoch 00052: val_mDice did not improve from 0.50651
Epoch 53/300
 - 79s - loss: 0.0362 - acc: 0.9957 - mDice: 0.9298 - val_loss: 0.0744 - val_acc: 0.9936 - val_mDice: 0.4957

Epoch 00053: val_mDice did not improve from 0.50651
Epoch 54/300
 - 79s - loss: 0.0365 - acc: 0.9957 - mDice: 0.9291 - val_loss: 0.1126 - val_acc: 0.9939 - val_mDice: 0.4972

Epoch 00054: val_mDice did not improve from 0.50651
Epoch 55/300
 - 79s - loss: 0.0357 - acc: 0.9957 - mDice: 0.9307 - val_loss: 0.0726 - val_acc: 0.9941 - val_mDice: 0.4992

Epoch 00055: val_mDice did not improve from 0.50651
Epoch 56/300
 - 80s - loss: 0.0373 - acc: 0.9957 - mDice: 0.9275 - val_loss: 0.0704 - val_acc: 0.9940 - val_mDice: 0.5004

Epoch 00056: val_mDice did not improve from 0.50651
Epoch 57/300
 - 78s - loss: 0.0358 - acc: 0.9957 - mDice: 0.9305 - val_loss: 0.0716 - val_acc: 0.9940 - val_mDice: 0.5024

Epoch 00057: val_mDice did not improve from 0.50651
Epoch 58/300
 - 78s - loss: 0.0351 - acc: 0.9958 - mDice: 0.9318 - val_loss: 0.1412 - val_acc: 0.9940 - val_mDice: 0.5046

Epoch 00058: val_mDice did not improve from 0.50651
Epoch 59/300
 - 78s - loss: 0.0359 - acc: 0.9958 - mDice: 0.9304 - val_loss: 0.1485 - val_acc: 0.9937 - val_mDice: 0.5012

Epoch 00059: val_mDice did not improve from 0.50651
Epoch 60/300
 - 78s - loss: 0.0357 - acc: 0.9958 - mDice: 0.9308 - val_loss: 0.0725 - val_acc: 0.9938 - val_mDice: 0.5008

Epoch 00060: val_mDice did not improve from 0.50651
Epoch 61/300
 - 79s - loss: 0.0360 - acc: 0.9958 - mDice: 0.9302 - val_loss: 0.0721 - val_acc: 0.9940 - val_mDice: 0.5026

Epoch 00061: val_mDice did not improve from 0.50651

Epoch 00061: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 62/300
 - 79s - loss: 0.0343 - acc: 0.9958 - mDice: 0.9334 - val_loss: 0.1002 - val_acc: 0.9936 - val_mDice: 0.5011

Epoch 00062: val_mDice did not improve from 0.50651
Epoch 63/300
 - 78s - loss: 0.0357 - acc: 0.9958 - mDice: 0.9307 - val_loss: 0.1116 - val_acc: 0.9937 - val_mDice: 0.5011

Epoch 00063: val_mDice did not improve from 0.50651
Epoch 64/300
 - 78s - loss: 0.0343 - acc: 0.9958 - mDice: 0.9335 - val_loss: 0.1325 - val_acc: 0.9937 - val_mDice: 0.5022

Epoch 00064: val_mDice did not improve from 0.50651
Epoch 65/300
 - 78s - loss: 0.0349 - acc: 0.9958 - mDice: 0.9322 - val_loss: 0.0719 - val_acc: 0.9939 - val_mDice: 0.5045

Epoch 00065: val_mDice did not improve from 0.50651
Epoch 66/300
 - 79s - loss: 0.0345 - acc: 0.9958 - mDice: 0.9331 - val_loss: 0.1125 - val_acc: 0.9939 - val_mDice: 0.5060

Epoch 00066: val_mDice did not improve from 0.50651
Epoch 67/300
 - 79s - loss: 0.0352 - acc: 0.9958 - mDice: 0.9316 - val_loss: 0.0909 - val_acc: 0.9940 - val_mDice: 0.5052

Epoch 00067: val_mDice did not improve from 0.50651
Epoch 68/300
 - 79s - loss: 0.0336 - acc: 0.9958 - mDice: 0.9348 - val_loss: 0.0936 - val_acc: 0.9940 - val_mDice: 0.5057

Epoch 00068: val_mDice did not improve from 0.50651
Epoch 69/300
 - 79s - loss: 0.0344 - acc: 0.9958 - mDice: 0.9333 - val_loss: 0.1091 - val_acc: 0.9939 - val_mDice: 0.5039

Epoch 00069: val_mDice did not improve from 0.50651
Epoch 70/300
 - 79s - loss: 0.0339 - acc: 0.9959 - mDice: 0.9342 - val_loss: 0.0967 - val_acc: 0.9940 - val_mDice: 0.5076

Epoch 00070: val_mDice improved from 0.50651 to 0.50757, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 71/300
 - 79s - loss: 0.0342 - acc: 0.9959 - mDice: 0.9335 - val_loss: 0.1086 - val_acc: 0.9939 - val_mDice: 0.5048

Epoch 00071: val_mDice did not improve from 0.50757
Epoch 72/300
 - 79s - loss: 0.0345 - acc: 0.9958 - mDice: 0.9331 - val_loss: 0.1058 - val_acc: 0.9938 - val_mDice: 0.5048

Epoch 00072: val_mDice did not improve from 0.50757
Epoch 73/300
 - 78s - loss: 0.0342 - acc: 0.9959 - mDice: 0.9336 - val_loss: 0.1100 - val_acc: 0.9938 - val_mDice: 0.5031

Epoch 00073: val_mDice did not improve from 0.50757
Epoch 74/300
 - 78s - loss: 0.0348 - acc: 0.9959 - mDice: 0.9324 - val_loss: 0.0694 - val_acc: 0.9940 - val_mDice: 0.5055

Epoch 00074: val_mDice did not improve from 0.50757
Epoch 75/300
 - 78s - loss: 0.0347 - acc: 0.9959 - mDice: 0.9327 - val_loss: 0.1108 - val_acc: 0.9936 - val_mDice: 0.5018

Epoch 00075: val_mDice did not improve from 0.50757
Epoch 76/300
 - 78s - loss: 0.0341 - acc: 0.9959 - mDice: 0.9337 - val_loss: 0.1181 - val_acc: 0.9938 - val_mDice: 0.5020

Epoch 00076: val_mDice did not improve from 0.50757
Epoch 77/300
 - 78s - loss: 0.0348 - acc: 0.9959 - mDice: 0.9324 - val_loss: 0.0702 - val_acc: 0.9940 - val_mDice: 0.5042

Epoch 00077: val_mDice did not improve from 0.50757
Epoch 78/300
 - 78s - loss: 0.0342 - acc: 0.9959 - mDice: 0.9337 - val_loss: 0.1102 - val_acc: 0.9937 - val_mDice: 0.5041

Epoch 00078: val_mDice did not improve from 0.50757
Epoch 79/300
 - 78s - loss: 0.0340 - acc: 0.9959 - mDice: 0.9340 - val_loss: 0.0712 - val_acc: 0.9939 - val_mDice: 0.5042

Epoch 00079: val_mDice did not improve from 0.50757
Epoch 80/300
 - 78s - loss: 0.0344 - acc: 0.9959 - mDice: 0.9332 - val_loss: 0.1089 - val_acc: 0.9939 - val_mDice: 0.5050

Epoch 00080: val_mDice did not improve from 0.50757
Epoch 81/300
 - 79s - loss: 0.0336 - acc: 0.9959 - mDice: 0.9347 - val_loss: 0.1460 - val_acc: 0.9939 - val_mDice: 0.5024

Epoch 00081: val_mDice did not improve from 0.50757
Epoch 82/300
 - 79s - loss: 0.0337 - acc: 0.9959 - mDice: 0.9346 - val_loss: 0.1090 - val_acc: 0.9938 - val_mDice: 0.5036

Epoch 00082: val_mDice did not improve from 0.50757
Epoch 83/300
 - 79s - loss: 0.0341 - acc: 0.9959 - mDice: 0.9339 - val_loss: 0.0699 - val_acc: 0.9939 - val_mDice: 0.5046

Epoch 00083: val_mDice did not improve from 0.50757
Epoch 84/300
 - 78s - loss: 0.0338 - acc: 0.9959 - mDice: 0.9344 - val_loss: 0.1075 - val_acc: 0.9939 - val_mDice: 0.5066

Epoch 00084: val_mDice did not improve from 0.50757
Epoch 85/300
 - 78s - loss: 0.0335 - acc: 0.9959 - mDice: 0.9351 - val_loss: 0.1105 - val_acc: 0.9937 - val_mDice: 0.5026

Epoch 00085: val_mDice did not improve from 0.50757

Epoch 00085: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 86/300
 - 78s - loss: 0.0335 - acc: 0.9959 - mDice: 0.9350 - val_loss: 0.1134 - val_acc: 0.9938 - val_mDice: 0.5038

Epoch 00086: val_mDice did not improve from 0.50757
Epoch 87/300
 - 79s - loss: 0.0335 - acc: 0.9959 - mDice: 0.9349 - val_loss: 0.1476 - val_acc: 0.9938 - val_mDice: 0.5039

Epoch 00087: val_mDice did not improve from 0.50757
Epoch 88/300
 - 79s - loss: 0.0339 - acc: 0.9959 - mDice: 0.9342 - val_loss: 0.1436 - val_acc: 0.9940 - val_mDice: 0.5040

Epoch 00088: val_mDice did not improve from 0.50757
Epoch 89/300
 - 80s - loss: 0.0336 - acc: 0.9959 - mDice: 0.9348 - val_loss: 0.1400 - val_acc: 0.9937 - val_mDice: 0.5035

Epoch 00089: val_mDice did not improve from 0.50757
Epoch 90/300
 - 79s - loss: 0.0332 - acc: 0.9959 - mDice: 0.9356 - val_loss: 0.1290 - val_acc: 0.9939 - val_mDice: 0.5050

Epoch 00090: val_mDice did not improve from 0.50757
Epoch 91/300
 - 79s - loss: 0.0333 - acc: 0.9959 - mDice: 0.9354 - val_loss: 0.1464 - val_acc: 0.9939 - val_mDice: 0.5055

Epoch 00091: val_mDice did not improve from 0.50757
Epoch 92/300
 - 78s - loss: 0.0333 - acc: 0.9959 - mDice: 0.9353 - val_loss: 0.1095 - val_acc: 0.9938 - val_mDice: 0.5042

Epoch 00092: val_mDice did not improve from 0.50757
Epoch 93/300
 - 78s - loss: 0.0337 - acc: 0.9959 - mDice: 0.9346 - val_loss: 0.1087 - val_acc: 0.9940 - val_mDice: 0.5057

Epoch 00093: val_mDice did not improve from 0.50757
Epoch 94/300
 - 78s - loss: 0.0342 - acc: 0.9959 - mDice: 0.9337 - val_loss: 0.1279 - val_acc: 0.9939 - val_mDice: 0.5044

Epoch 00094: val_mDice did not improve from 0.50757
Epoch 95/300
 - 78s - loss: 0.0334 - acc: 0.9959 - mDice: 0.9351 - val_loss: 0.1488 - val_acc: 0.9938 - val_mDice: 0.5035

Epoch 00095: val_mDice did not improve from 0.50757
Epoch 96/300
 - 78s - loss: 0.0333 - acc: 0.9959 - mDice: 0.9354 - val_loss: 0.1479 - val_acc: 0.9939 - val_mDice: 0.5048

Epoch 00096: val_mDice did not improve from 0.50757
Epoch 97/300
 - 78s - loss: 0.0330 - acc: 0.9960 - mDice: 0.9360 - val_loss: 0.1389 - val_acc: 0.9940 - val_mDice: 0.5058

Epoch 00097: val_mDice did not improve from 0.50757
Epoch 98/300
 - 79s - loss: 0.0337 - acc: 0.9959 - mDice: 0.9345 - val_loss: 0.1095 - val_acc: 0.9940 - val_mDice: 0.5068

Epoch 00098: val_mDice did not improve from 0.50757
Epoch 99/300
 - 78s - loss: 0.0334 - acc: 0.9959 - mDice: 0.9352 - val_loss: 0.1488 - val_acc: 0.9938 - val_mDice: 0.5032

Epoch 00099: val_mDice did not improve from 0.50757
Epoch 100/300
 - 78s - loss: 0.0333 - acc: 0.9959 - mDice: 0.9353 - val_loss: 0.1495 - val_acc: 0.9935 - val_mDice: 0.5019

Epoch 00100: val_mDice did not improve from 0.50757

Epoch 00100: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.
Epoch 101/300
 - 78s - loss: 0.0331 - acc: 0.9960 - mDice: 0.9358 - val_loss: 0.1478 - val_acc: 0.9939 - val_mDice: 0.5047

Epoch 00101: val_mDice did not improve from 0.50757
Epoch 102/300
 - 79s - loss: 0.0337 - acc: 0.9960 - mDice: 0.9346 - val_loss: 0.1477 - val_acc: 0.9939 - val_mDice: 0.5055

Epoch 00102: val_mDice did not improve from 0.50757
Epoch 103/300
 - 80s - loss: 0.0338 - acc: 0.9959 - mDice: 0.9345 - val_loss: 0.1484 - val_acc: 0.9938 - val_mDice: 0.5041

Epoch 00103: val_mDice did not improve from 0.50757
Epoch 104/300
 - 80s - loss: 0.0329 - acc: 0.9960 - mDice: 0.9363 - val_loss: 0.1206 - val_acc: 0.9939 - val_mDice: 0.5047

Epoch 00104: val_mDice did not improve from 0.50757
Epoch 105/300
 - 78s - loss: 0.0332 - acc: 0.9960 - mDice: 0.9355 - val_loss: 0.1479 - val_acc: 0.9938 - val_mDice: 0.5049

Epoch 00105: val_mDice did not improve from 0.50757
Epoch 106/300
 - 78s - loss: 0.0335 - acc: 0.9960 - mDice: 0.9349 - val_loss: 0.1481 - val_acc: 0.9938 - val_mDice: 0.5044

Epoch 00106: val_mDice did not improve from 0.50757
Epoch 107/300
 - 79s - loss: 0.0338 - acc: 0.9960 - mDice: 0.9343 - val_loss: 0.1168 - val_acc: 0.9939 - val_mDice: 0.5056

Epoch 00107: val_mDice did not improve from 0.50757
Epoch 108/300
 - 79s - loss: 0.0338 - acc: 0.9960 - mDice: 0.9344 - val_loss: 0.1476 - val_acc: 0.9940 - val_mDice: 0.5054

Epoch 00108: val_mDice did not improve from 0.50757
Epoch 109/300
 - 78s - loss: 0.0334 - acc: 0.9960 - mDice: 0.9352 - val_loss: 0.1481 - val_acc: 0.9939 - val_mDice: 0.5044

Epoch 00109: val_mDice did not improve from 0.50757
Epoch 110/300
 - 78s - loss: 0.0337 - acc: 0.9960 - mDice: 0.9346 - val_loss: 0.1483 - val_acc: 0.9938 - val_mDice: 0.5041

Epoch 00110: val_mDice did not improve from 0.50757
Restoring model weights from the end of the best epoch
Epoch 00110: early stopping
{'val_loss': [0.29172553174430504, 0.29288719105534256, 0.2841920849168673, 0.2821982891764492, 0.22543160477653146, 0.24277062225155532, 0.21429239062126726, 0.2378577794879675, 0.20388283370994031, 0.2072109819855541, 0.20320117729716003, 0.13268481940031052, 0.16328517638612539, 0.15505559358280152, 0.19380516035016626, 0.14508921944070607, 0.19098037108778954, 0.15218025381909683, 0.22672355128452182, 0.14991193590685725, 0.14234555239090696, 0.1113946873228997, 0.11094694375060499, 0.15261249081231654, 0.10840017523150891, 0.18928708496969193, 0.11070763773750514, 0.1881180687341839, 0.10934035334503278, 0.1492530683754012, 0.14692013192689046, 0.15244124981109053, 0.10925918072462082, 0.15060848277062178, 0.1485547439660877, 0.07255093636922538, 0.10953791439533234, 0.14964526053518057, 0.18793720297981054, 0.18354661931516603, 0.1138933168258518, 0.15179177140817046, 0.07381987251574174, 0.07134335424052551, 0.21786208299454302, 0.10822207643650472, 0.10955618927255273, 0.1116592523176223, 0.1487611138727516, 0.15016532386653125, 0.07135953049873933, 0.11151543963933364, 0.07435419864486903, 0.11255466646980494, 0.07259698415873572, 0.0704429226461798, 0.07157584466040134, 0.14124083012575284, 0.1484699093271047, 0.07246765354648232, 0.07208613818511367, 0.10022229398600757, 0.11160143930464983, 0.13249112362973392, 0.07185195467900485, 0.11252849019365385, 0.09085552068427205, 0.09362180414609611, 0.10906901909038424, 0.09668176085688174, 0.10860182961914688, 0.10584545065648854, 0.1099508433835581, 0.06944867270067334, 0.11083258897997439, 0.11811981932260096, 0.07015359087381512, 0.11021043604705483, 0.07120871730148792, 0.10890496429055929, 0.1459806188941002, 0.10901013715192676, 0.06992587633430958, 0.10745546565158293, 0.11048100527841598, 0.11341771436855197, 0.14761224784888327, 0.14364682871382684, 0.14002077444456518, 0.12895991234108806, 0.1463726522633806, 0.10948447976261377, 0.10874449973925948, 0.1278535770252347, 0.14875917183235288, 0.1479284588713199, 0.1389089532312937, 0.1094673330662772, 0.14876061049290001, 0.14954958041198552, 0.1477673661429435, 0.14772151806391776, 0.14839635509997606, 0.12062821700237691, 0.14794544386677444, 0.14811252686195076, 0.11678533675149083, 0.14760211762040854, 0.14809415373019874, 0.14829468296375126], 'val_acc': [0.9893731824122369, 0.9893419975414872, 0.9914310732856393, 0.9911173954606056, 0.9924229085445404, 0.9923633551225066, 0.9924331973306835, 0.9909353009425104, 0.9927300401031971, 0.9933620542287827, 0.9932713294401765, 0.9932906473986804, 0.9936086870729923, 0.9933651695027947, 0.9934072699397802, 0.9931709258817136, 0.9936230340972543, 0.9936925643123686, 0.9913992630317807, 0.9937302921898663, 0.9937253049574792, 0.9937131465412676, 0.9937480688095093, 0.9928662972524762, 0.9937923438847065, 0.9935447704046965, 0.9936754116788507, 0.9937645997852087, 0.9939893977716565, 0.9937761249020696, 0.993859997484833, 0.9933380470611155, 0.9937119032256305, 0.9934693123213947, 0.993777375202626, 0.9935734551399946, 0.9938403582200408, 0.9937122035771608, 0.9935588031075895, 0.9939607153646648, 0.9937421409413218, 0.9936726130545139, 0.993805440608412, 0.9938765307888389, 0.9927768036723137, 0.9939675675705075, 0.993786106351763, 0.9934300291351974, 0.9938693549484015, 0.993534479290247, 0.9939884641207755, 0.993777375202626, 0.9935915390960872, 0.9938899371773005, 0.9940910446457565, 0.9940383597277105, 0.9940473912283778, 0.9940106016583741, 0.9937100242823362, 0.9937505647540092, 0.9939700658433139, 0.9935943470336497, 0.9937302945181727, 0.9937440175563097, 0.9938674876466393, 0.9938871315680444, 0.9939675722271204, 0.9939579074271023, 0.993882761336863, 0.9939710041508079, 0.993911141064018, 0.9938437901437283, 0.9937761295586824, 0.9940224550664425, 0.9936261516995728, 0.9937727046199143, 0.9939965712837875, 0.9937486904673278, 0.9938531429506838, 0.9938952387310565, 0.993903657887131, 0.9938172847032547, 0.9939073994755745, 0.9939479306340218, 0.9937118985690176, 0.9938048096373677, 0.9937811098061502, 0.9939510505646467, 0.9937480641528964, 0.9938690452836454, 0.9939214275218546, 0.9937998224049807, 0.9939859681762755, 0.9938833876512945, 0.9937592865899205, 0.9939211155287921, 0.9939738120883703, 0.9939710064791143, 0.9937817361205816, 0.9935167096555233, 0.993882451672107, 0.9939407547935843, 0.9937811167910695, 0.9938578181900084, 0.9938350543379784, 0.9938437924720347, 0.9939295370131731, 0.9939703824929893, 0.9939416977576911, 0.9938210262916982], 'val_mDice': [0.41692198137752723, 0.4200118448352443, 0.4362396412761883, 0.44043793925084185, 0.44576586631592363, 0.45014063402403437, 0.45398266157549605, 0.45318300602957606, 0.4717733757570386, 0.464153863285901, 0.4722885513911024, 0.4722425213549286, 0.48574254440609366, 0.49060768040362746, 0.49109696585219353, 0.4907460225513205, 0.496619789628312, 0.4961586545687169, 0.4732583637814969, 0.5014075885992497, 0.5029728432418779, 0.4996730905259028, 0.5006866686744615, 0.497258166433312, 0.505364541313611, 0.5000311543699354, 0.5008928844472393, 0.5004304036265239, 0.5027203407371417, 0.5013780843000859, 0.506508753169328, 0.4957821767311543, 0.504272302496247, 0.49943047983106226, 0.5033960001310334, 0.4992001038044691, 0.5031640047673136, 0.5011416756315157, 0.5028065986698493, 0.5044843500945717, 0.49771609948948026, 0.4958928661653772, 0.49835329642519355, 0.4991042264737189, 0.4904678126331419, 0.502177721937187, 0.5031836777925491, 0.4999071318889037, 0.5024673673324287, 0.49914027680642903, 0.502828317694366, 0.49917444004677236, 0.49574534234125167, 0.49724852142389864, 0.4992214374942705, 0.5004298186395317, 0.5023610492935404, 0.5046273936750367, 0.5011831852607429, 0.5007560859667137, 0.5025539989583194, 0.5010865209624171, 0.5011383362580091, 0.5021631455747411, 0.5044536478817463, 0.5060083616990596, 0.5052208161214367, 0.5057181674055755, 0.5039157799910754, 0.5075719940941781, 0.5047859210753813, 0.504818944609724, 0.5031115806195885, 0.5054584750905633, 0.5017622635932639, 0.5020079034147784, 0.5042448540916666, 0.5040710238972679, 0.5042244220385328, 0.5050491273868829, 0.5024390981998295, 0.5035795946605504, 0.5046296934597194, 0.5065779067808762, 0.5026054149493575, 0.5037884955527261, 0.5038597562815994, 0.5039978108834475, 0.5034894536947832, 0.5049854511162266, 0.5055467318743467, 0.5041946313576773, 0.5057280673645437, 0.50441475934349, 0.5035035166656598, 0.5048347916454077, 0.5058115476276726, 0.5068336386466399, 0.5031516129383817, 0.5018677556654438, 0.5047105508856475, 0.5055224301759154, 0.504076320794411, 0.5046899401349947, 0.5049333191709593, 0.5044365825597197, 0.5056146223796532, 0.5053896101890132, 0.504394574672915, 0.504067448200658], 'loss': [0.1645165175542303, 0.10212498685565576, 0.08983579436231603, 0.07899658046935484, 0.07386618595433121, 0.06894847189900877, 0.06469136362598238, 0.064287226849266, 0.06137100555564491, 0.060407680320493214, 0.05845664849129789, 0.05758909159169541, 0.056882193636448246, 0.055995803776069344, 0.052757134834856084, 0.05361447977755867, 0.05051227743127447, 0.05025355205713726, 0.050005705878476144, 0.049522668492008724, 0.048351757555587155, 0.048610955982295784, 0.047621918377995946, 0.04688834708105663, 0.045618005751337724, 0.045101785878451206, 0.0454146423149541, 0.04402128398501528, 0.04368009011051139, 0.04320558160889699, 0.04270400844260737, 0.04215363111852062, 0.041873211339090124, 0.04131574924605447, 0.0409151546425959, 0.04121860463323252, 0.04078689139573101, 0.04044592268249916, 0.03991260844748846, 0.03929524244598857, 0.0392306787693862, 0.039020523601179684, 0.03832651689549945, 0.03873879740178495, 0.03808168455762731, 0.037548904915408306, 0.037219204580768606, 0.03722003671808416, 0.036374107089776175, 0.03656913477036268, 0.036204181679058006, 0.036427907843934376, 0.03617060189932304, 0.03648451790412682, 0.035710917475214675, 0.03730901662510032, 0.035782891686949925, 0.03512618748986847, 0.0358529915881445, 0.035665271841608524, 0.035960941712130816, 0.03433400995365429, 0.03568347873201046, 0.03428191111483778, 0.03494871045908918, 0.0344914398620848, 0.03520917174492589, 0.033607128921446516, 0.03438614203398882, 0.03393372586376786, 0.034242883702043016, 0.03445090235155662, 0.03422462157416014, 0.03481374361141226, 0.034659659901818456, 0.034148292939976226, 0.03479825296706822, 0.03417090038808226, 0.034023184917475045, 0.03441263128479308, 0.033635124602259085, 0.03367888207332147, 0.03405660982866438, 0.03380288655810258, 0.03347378649424354, 0.03351468913862206, 0.03352643545114081, 0.0338852816329845, 0.033614423975609896, 0.033179879963287175, 0.03328182633290574, 0.03334748857757751, 0.0336919116403592, 0.03415865948062554, 0.033441429269345666, 0.033281491119970745, 0.032967963958085725, 0.03372034994123344, 0.033390319656762035, 0.033349305712464826, 0.03307021612699015, 0.03367130500684843, 0.033760533062259644, 0.032855978459587244, 0.033216309964490626, 0.03351786337854334, 0.03381657271632005, 0.033800019438436836, 0.033383137898687784, 0.03368850414195071], 'acc': [0.981068385801097, 0.9891863740444793, 0.9906798854125728, 0.9915987888085547, 0.9921596183432815, 0.9925996927873202, 0.9929356106249757, 0.9931460279294861, 0.9933909866000158, 0.9935657497661183, 0.9937304849060651, 0.9938323537661, 0.9939795545173242, 0.9940484189067524, 0.9941927364468921, 0.9942438547738937, 0.9943757771240705, 0.9944565390839704, 0.9945378723606576, 0.9945833052134034, 0.9946733288292936, 0.9947209785842475, 0.9947646910601049, 0.9948268640508106, 0.9948775650691222, 0.9949454183748573, 0.9949730963564035, 0.9950704501940285, 0.9950754351700115, 0.9950952949708965, 0.9951604533253666, 0.9951808054757143, 0.9952204941152016, 0.9952416008560586, 0.9952723303641178, 0.9953085180842476, 0.9953493136166833, 0.9953728652352224, 0.9954003208506547, 0.9954391840000912, 0.9954762188166958, 0.9954632125181148, 0.9955237898951805, 0.9955180956329274, 0.9955390552215714, 0.9955649157704495, 0.995615315039814, 0.9956404290867417, 0.9956719283790584, 0.9956519564671481, 0.9956819347295388, 0.9956978315105972, 0.9957003207991759, 0.9957204448701724, 0.9957199989952968, 0.9957400737247, 0.9957496162919033, 0.9957621915838757, 0.9957796966238721, 0.995760754341169, 0.9957828544304459, 0.995802138766387, 0.9957912706866198, 0.995824086606551, 0.9958133750002296, 0.9958282076856796, 0.9958261775543871, 0.9958482319654676, 0.9958428013569888, 0.9958522654970279, 0.995851686050131, 0.9958486951670331, 0.9958766225227236, 0.9958595498882132, 0.9958616604688096, 0.9958644779603545, 0.9958636258016054, 0.9958800864757056, 0.9958953845095133, 0.9958880585328868, 0.9959094210328632, 0.995913659271441, 0.9959078353221676, 0.995904412511418, 0.9959118374897875, 0.995923109339604, 0.9959258913052017, 0.9959134967038755, 0.9959044458143966, 0.9959254880822382, 0.9959266056883719, 0.9959466837903161, 0.9959218528083189, 0.9959404472206699, 0.9959447968292835, 0.9959366306259171, 0.9959677720202847, 0.9959448846954079, 0.9959349009901219, 0.995946225498672, 0.9959519837630312, 0.9959653240678423, 0.9959354201398606, 0.9959539782160235, 0.9959759436945086, 0.9959625565065273, 0.9959636320563107, 0.9959586514916392, 0.9959636647014013, 0.9959530829898541], 'mDice': [0.6810974392535573, 0.8014198126780816, 0.8251909236939201, 0.8463704050998031, 0.8563280855796016, 0.8659275647898134, 0.8742570714853887, 0.8749509426754961, 0.8806539022944676, 0.882485086673316, 0.8862933326707872, 0.8879767126371617, 0.8893141832319101, 0.8910480623040532, 0.8974490360895405, 0.8957068503683521, 0.9018379726219088, 0.9023091983640043, 0.9027628594572513, 0.9037059565980744, 0.9059992558770276, 0.9054522373451371, 0.9074052865010487, 0.9088399169010694, 0.9113500636290407, 0.9123441121976273, 0.9117041362679571, 0.9144371514316495, 0.9151146681041159, 0.916055715851381, 0.9170259790033118, 0.9181095738229235, 0.9186569024520121, 0.9197487765430808, 0.9205331491741193, 0.9199101745085086, 0.9207490982391062, 0.9214213191070252, 0.9224690226494981, 0.9236889306477059, 0.9237920034910947, 0.9242144146627065, 0.9255761298358239, 0.9247500910719637, 0.9260503462386682, 0.9271068653329624, 0.9277310084213539, 0.9277210733097467, 0.9293983845438181, 0.9290101426437478, 0.9297258596505051, 0.929267593654375, 0.9297810293032038, 0.9291398495598455, 0.9306891651897047, 0.9274856079479287, 0.9305289812490779, 0.9318374587543005, 0.9303759102678969, 0.9307537777569009, 0.9301522209127149, 0.9333913179658301, 0.9307009846377181, 0.9334887924947237, 0.9321571893083712, 0.93306982546328, 0.9316338585605047, 0.9348269529052904, 0.9332674342303156, 0.9341685383165008, 0.9335494326758471, 0.9331312256088181, 0.9335757770943589, 0.9324010300819116, 0.9327143178773395, 0.9337275972678795, 0.9324306177320112, 0.9336788892344314, 0.9339658117978515, 0.9331912042386332, 0.9347312475408677, 0.9346432433015339, 0.9338947110015701, 0.9344031813115045, 0.9350593131538937, 0.9349661389573762, 0.9349353312130269, 0.9342280799425395, 0.9347667395341434, 0.9356309788469738, 0.9354241889944417, 0.9352912927243135, 0.934604561839963, 0.9336660246933823, 0.9351004816672245, 0.935420311112236, 0.9360406734019022, 0.9345420334166568, 0.9352038368726422, 0.9352840285337496, 0.9358388170750559, 0.9346298823563551, 0.934462333661804, 0.9362593067222817, 0.9355366169416047, 0.9349368127973788, 0.93433596419723, 0.9343697661162808, 0.9352017330648288, 0.9345983107206062], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:00<00:02,  1.36it/s]predicting test subjects:  50%|█████     | 2/4 [00:00<00:01,  1.79it/s]predicting test subjects:  75%|███████▌  | 3/4 [00:01<00:00,  2.30it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  2.84it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  3.36it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:04,  4.10it/s]predicting train subjects:   1%|          | 2/266 [00:00<00:59,  4.43it/s]predicting train subjects:   1%|          | 3/266 [00:00<01:05,  4.00it/s]predicting train subjects:   2%|▏         | 4/266 [00:01<01:11,  3.68it/s]predicting train subjects:   2%|▏         | 5/266 [00:01<01:07,  3.85it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<01:01,  4.22it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<00:56,  4.57it/s]predicting train subjects:   3%|▎         | 8/266 [00:01<00:53,  4.86it/s]predicting train subjects:   3%|▎         | 9/266 [00:01<00:50,  5.09it/s]predicting train subjects:   4%|▍         | 10/266 [00:02<00:48,  5.24it/s]predicting train subjects:   4%|▍         | 11/266 [00:02<00:47,  5.34it/s]predicting train subjects:   5%|▍         | 12/266 [00:02<00:46,  5.44it/s]predicting train subjects:   5%|▍         | 13/266 [00:02<00:45,  5.50it/s]predicting train subjects:   5%|▌         | 14/266 [00:02<00:45,  5.57it/s]predicting train subjects:   6%|▌         | 15/266 [00:03<00:44,  5.61it/s]predicting train subjects:   6%|▌         | 16/266 [00:03<00:44,  5.64it/s]predicting train subjects:   6%|▋         | 17/266 [00:03<00:43,  5.66it/s]predicting train subjects:   7%|▋         | 18/266 [00:03<00:43,  5.64it/s]predicting train subjects:   7%|▋         | 19/266 [00:03<00:43,  5.69it/s]predicting train subjects:   8%|▊         | 20/266 [00:03<00:43,  5.66it/s]predicting train subjects:   8%|▊         | 21/266 [00:04<00:43,  5.65it/s]predicting train subjects:   8%|▊         | 22/266 [00:04<00:43,  5.65it/s]predicting train subjects:   9%|▊         | 23/266 [00:04<00:42,  5.67it/s]predicting train subjects:   9%|▉         | 24/266 [00:04<00:42,  5.71it/s]predicting train subjects:   9%|▉         | 25/266 [00:04<00:41,  5.75it/s]predicting train subjects:  10%|▉         | 26/266 [00:04<00:41,  5.73it/s]predicting train subjects:  10%|█         | 27/266 [00:05<00:41,  5.73it/s]predicting train subjects:  11%|█         | 28/266 [00:05<00:41,  5.77it/s]predicting train subjects:  11%|█         | 29/266 [00:05<00:41,  5.73it/s]predicting train subjects:  11%|█▏        | 30/266 [00:05<00:41,  5.70it/s]predicting train subjects:  12%|█▏        | 31/266 [00:05<00:42,  5.60it/s]predicting train subjects:  12%|█▏        | 32/266 [00:06<00:41,  5.64it/s]predicting train subjects:  12%|█▏        | 33/266 [00:06<00:40,  5.69it/s]predicting train subjects:  13%|█▎        | 34/266 [00:06<00:40,  5.72it/s]predicting train subjects:  13%|█▎        | 35/266 [00:06<00:40,  5.72it/s]predicting train subjects:  14%|█▎        | 36/266 [00:06<00:40,  5.67it/s]predicting train subjects:  14%|█▍        | 37/266 [00:06<00:39,  5.73it/s]predicting train subjects:  14%|█▍        | 38/266 [00:07<00:39,  5.78it/s]predicting train subjects:  15%|█▍        | 39/266 [00:07<00:39,  5.82it/s]predicting train subjects:  15%|█▌        | 40/266 [00:07<00:38,  5.84it/s]predicting train subjects:  15%|█▌        | 41/266 [00:07<00:38,  5.83it/s]predicting train subjects:  16%|█▌        | 42/266 [00:07<00:36,  6.10it/s]predicting train subjects:  16%|█▌        | 43/266 [00:07<00:35,  6.37it/s]predicting train subjects:  17%|█▋        | 44/266 [00:08<00:34,  6.50it/s]predicting train subjects:  17%|█▋        | 45/266 [00:08<00:33,  6.65it/s]predicting train subjects:  17%|█▋        | 46/266 [00:08<00:32,  6.78it/s]predicting train subjects:  18%|█▊        | 47/266 [00:08<00:31,  6.86it/s]predicting train subjects:  18%|█▊        | 48/266 [00:08<00:31,  6.82it/s]predicting train subjects:  18%|█▊        | 49/266 [00:08<00:31,  6.86it/s]predicting train subjects:  19%|█▉        | 50/266 [00:08<00:31,  6.93it/s]predicting train subjects:  19%|█▉        | 51/266 [00:09<00:30,  6.94it/s]predicting train subjects:  20%|█▉        | 52/266 [00:09<00:30,  6.98it/s]predicting train subjects:  20%|█▉        | 53/266 [00:09<00:30,  7.01it/s]predicting train subjects:  20%|██        | 54/266 [00:09<00:30,  7.05it/s]predicting train subjects:  21%|██        | 55/266 [00:09<00:30,  6.98it/s]predicting train subjects:  21%|██        | 56/266 [00:09<00:30,  6.91it/s]predicting train subjects:  21%|██▏       | 57/266 [00:09<00:30,  6.91it/s]predicting train subjects:  22%|██▏       | 58/266 [00:10<00:29,  6.95it/s]predicting train subjects:  22%|██▏       | 59/266 [00:10<00:29,  7.00it/s]predicting train subjects:  23%|██▎       | 60/266 [00:10<00:29,  7.00it/s]predicting train subjects:  23%|██▎       | 61/266 [00:10<00:29,  6.98it/s]predicting train subjects:  23%|██▎       | 62/266 [00:10<00:29,  6.96it/s]predicting train subjects:  24%|██▎       | 63/266 [00:10<00:29,  6.99it/s]predicting train subjects:  24%|██▍       | 64/266 [00:10<00:28,  7.01it/s]predicting train subjects:  24%|██▍       | 65/266 [00:11<00:29,  6.88it/s]predicting train subjects:  25%|██▍       | 66/266 [00:11<00:28,  6.90it/s]predicting train subjects:  25%|██▌       | 67/266 [00:11<00:29,  6.79it/s]predicting train subjects:  26%|██▌       | 68/266 [00:11<00:28,  6.83it/s]predicting train subjects:  26%|██▌       | 69/266 [00:11<00:29,  6.64it/s]predicting train subjects:  26%|██▋       | 70/266 [00:11<00:29,  6.74it/s]predicting train subjects:  27%|██▋       | 71/266 [00:11<00:28,  6.82it/s]predicting train subjects:  27%|██▋       | 72/266 [00:12<00:28,  6.78it/s]predicting train subjects:  27%|██▋       | 73/266 [00:12<00:28,  6.83it/s]predicting train subjects:  28%|██▊       | 74/266 [00:12<00:28,  6.75it/s]predicting train subjects:  28%|██▊       | 75/266 [00:12<00:28,  6.75it/s]predicting train subjects:  29%|██▊       | 76/266 [00:12<00:27,  6.80it/s]predicting train subjects:  29%|██▉       | 77/266 [00:12<00:27,  6.83it/s]predicting train subjects:  29%|██▉       | 78/266 [00:12<00:28,  6.51it/s]predicting train subjects:  30%|██▉       | 79/266 [00:13<00:29,  6.32it/s]predicting train subjects:  30%|███       | 80/266 [00:13<00:29,  6.21it/s]predicting train subjects:  30%|███       | 81/266 [00:13<00:30,  6.14it/s]predicting train subjects:  31%|███       | 82/266 [00:13<00:30,  6.03it/s]predicting train subjects:  31%|███       | 83/266 [00:13<00:30,  6.00it/s]predicting train subjects:  32%|███▏      | 84/266 [00:13<00:30,  5.98it/s]predicting train subjects:  32%|███▏      | 85/266 [00:14<00:30,  5.91it/s]predicting train subjects:  32%|███▏      | 86/266 [00:14<00:30,  5.92it/s]predicting train subjects:  33%|███▎      | 87/266 [00:14<00:31,  5.67it/s]predicting train subjects:  33%|███▎      | 88/266 [00:14<00:31,  5.70it/s]predicting train subjects:  33%|███▎      | 89/266 [00:14<00:30,  5.76it/s]predicting train subjects:  34%|███▍      | 90/266 [00:15<00:30,  5.70it/s]predicting train subjects:  34%|███▍      | 91/266 [00:15<00:31,  5.58it/s]predicting train subjects:  35%|███▍      | 92/266 [00:15<00:30,  5.66it/s]predicting train subjects:  35%|███▍      | 93/266 [00:15<00:30,  5.75it/s]predicting train subjects:  35%|███▌      | 94/266 [00:15<00:29,  5.81it/s]predicting train subjects:  36%|███▌      | 95/266 [00:15<00:29,  5.86it/s]predicting train subjects:  36%|███▌      | 96/266 [00:16<00:36,  4.60it/s]predicting train subjects:  36%|███▋      | 97/266 [00:16<00:38,  4.39it/s]predicting train subjects:  37%|███▋      | 98/266 [00:16<00:37,  4.54it/s]predicting train subjects:  37%|███▋      | 99/266 [00:16<00:39,  4.28it/s]predicting train subjects:  38%|███▊      | 100/266 [00:17<00:34,  4.83it/s]predicting train subjects:  38%|███▊      | 101/266 [00:17<00:31,  5.27it/s]predicting train subjects:  38%|███▊      | 102/266 [00:17<00:29,  5.64it/s]predicting train subjects:  39%|███▊      | 103/266 [00:17<00:27,  5.83it/s]predicting train subjects:  39%|███▉      | 104/266 [00:17<00:26,  6.09it/s]predicting train subjects:  39%|███▉      | 105/266 [00:17<00:25,  6.25it/s]predicting train subjects:  40%|███▉      | 106/266 [00:18<00:24,  6.40it/s]predicting train subjects:  40%|████      | 107/266 [00:18<00:25,  6.15it/s]predicting train subjects:  41%|████      | 108/266 [00:18<00:25,  6.32it/s]predicting train subjects:  41%|████      | 109/266 [00:18<00:24,  6.38it/s]predicting train subjects:  41%|████▏     | 110/266 [00:18<00:24,  6.49it/s]predicting train subjects:  42%|████▏     | 111/266 [00:18<00:23,  6.58it/s]predicting train subjects:  42%|████▏     | 112/266 [00:18<00:23,  6.67it/s]predicting train subjects:  42%|████▏     | 113/266 [00:19<00:22,  6.68it/s]predicting train subjects:  43%|████▎     | 114/266 [00:19<00:22,  6.68it/s]predicting train subjects:  43%|████▎     | 115/266 [00:19<00:22,  6.69it/s]predicting train subjects:  44%|████▎     | 116/266 [00:19<00:22,  6.71it/s]predicting train subjects:  44%|████▍     | 117/266 [00:19<00:22,  6.72it/s]predicting train subjects:  44%|████▍     | 118/266 [00:19<00:22,  6.72it/s]predicting train subjects:  45%|████▍     | 119/266 [00:19<00:22,  6.40it/s]predicting train subjects:  45%|████▌     | 120/266 [00:20<00:23,  6.22it/s]predicting train subjects:  45%|████▌     | 121/266 [00:20<00:23,  6.08it/s]predicting train subjects:  46%|████▌     | 122/266 [00:20<00:24,  5.99it/s]predicting train subjects:  46%|████▌     | 123/266 [00:20<00:24,  5.93it/s]predicting train subjects:  47%|████▋     | 124/266 [00:20<00:24,  5.87it/s]predicting train subjects:  47%|████▋     | 125/266 [00:21<00:24,  5.78it/s]predicting train subjects:  47%|████▋     | 126/266 [00:21<00:24,  5.78it/s]predicting train subjects:  48%|████▊     | 127/266 [00:21<00:24,  5.79it/s]predicting train subjects:  48%|████▊     | 128/266 [00:21<00:23,  5.76it/s]predicting train subjects:  48%|████▊     | 129/266 [00:21<00:23,  5.74it/s]predicting train subjects:  49%|████▉     | 130/266 [00:21<00:23,  5.77it/s]predicting train subjects:  49%|████▉     | 131/266 [00:22<00:23,  5.76it/s]predicting train subjects:  50%|████▉     | 132/266 [00:22<00:23,  5.77it/s]predicting train subjects:  50%|█████     | 133/266 [00:22<00:23,  5.76it/s]predicting train subjects:  50%|█████     | 134/266 [00:22<00:22,  5.77it/s]predicting train subjects:  51%|█████     | 135/266 [00:22<00:22,  5.78it/s]predicting train subjects:  51%|█████     | 136/266 [00:22<00:22,  5.74it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:23<00:21,  5.91it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:23<00:21,  5.98it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:23<00:21,  6.02it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:23<00:20,  6.08it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:23<00:20,  6.15it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:23<00:20,  6.16it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:24<00:19,  6.21it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:24<00:19,  6.25it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:24<00:19,  6.26it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:24<00:19,  6.27it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:24<00:18,  6.29it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:24<00:18,  6.25it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:25<00:18,  6.25it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:25<00:18,  6.24it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:25<00:18,  6.26it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:25<00:18,  6.28it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:25<00:18,  6.27it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:25<00:17,  6.28it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:25<00:16,  6.63it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:26<00:15,  6.90it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:26<00:15,  7.10it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:26<00:14,  7.21it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:26<00:14,  7.35it/s]predicting train subjects:  60%|██████    | 160/266 [00:26<00:14,  7.46it/s]predicting train subjects:  61%|██████    | 161/266 [00:26<00:14,  7.50it/s]predicting train subjects:  61%|██████    | 162/266 [00:26<00:13,  7.56it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:27<00:13,  7.55it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:27<00:13,  7.47it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:27<00:13,  7.46it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:27<00:13,  7.54it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:27<00:13,  7.59it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:27<00:12,  7.63it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:27<00:12,  7.63it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:27<00:12,  7.64it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:28<00:12,  7.53it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:28<00:12,  7.56it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:28<00:12,  7.39it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:28<00:12,  7.22it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:28<00:12,  7.13it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:28<00:12,  7.03it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:28<00:12,  6.99it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:29<00:12,  6.80it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:29<00:12,  6.82it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:29<00:12,  6.71it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:29<00:12,  6.76it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:29<00:12,  6.75it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:29<00:12,  6.70it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:29<00:12,  6.71it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:30<00:12,  6.72it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:30<00:11,  6.68it/s]predicting train subjects:  70%|███████   | 187/266 [00:30<00:11,  6.71it/s]predicting train subjects:  71%|███████   | 188/266 [00:30<00:11,  6.68it/s]predicting train subjects:  71%|███████   | 189/266 [00:30<00:11,  6.69it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:30<00:11,  6.66it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:31<00:11,  6.69it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:31<00:14,  5.08it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:31<00:13,  5.46it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:31<00:12,  5.58it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:31<00:12,  5.87it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:31<00:11,  6.06it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:32<00:11,  6.25it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:32<00:10,  6.40it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:32<00:10,  6.31it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:32<00:10,  6.35it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:32<00:10,  6.45it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:32<00:09,  6.54it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:33<00:09,  6.63it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:33<00:09,  6.67it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:33<00:09,  6.70it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:33<00:08,  6.73it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:33<00:08,  6.76it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:33<00:08,  6.70it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:33<00:08,  6.69it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:34<00:08,  6.72it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:34<00:08,  6.58it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:34<00:08,  6.59it/s]predicting train subjects:  80%|████████  | 213/266 [00:34<00:07,  6.74it/s]predicting train subjects:  80%|████████  | 214/266 [00:34<00:07,  6.86it/s]predicting train subjects:  81%|████████  | 215/266 [00:34<00:07,  6.91it/s]predicting train subjects:  81%|████████  | 216/266 [00:34<00:07,  6.97it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:35<00:07,  6.94it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:35<00:06,  6.97it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:35<00:06,  6.94it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:35<00:06,  6.97it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:35<00:06,  6.95it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:35<00:06,  6.97it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:35<00:06,  7.01it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:36<00:06,  6.87it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:36<00:05,  6.94it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:36<00:05,  7.00it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:36<00:05,  7.00it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:36<00:05,  6.97it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:36<00:05,  6.89it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:36<00:05,  6.92it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:37<00:05,  6.85it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:37<00:04,  6.83it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:37<00:04,  6.79it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:37<00:04,  6.75it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:37<00:04,  6.71it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:37<00:04,  6.71it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:37<00:04,  6.75it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:38<00:04,  6.77it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:38<00:04,  6.62it/s]predicting train subjects:  90%|█████████ | 240/266 [00:38<00:03,  6.64it/s]predicting train subjects:  91%|█████████ | 241/266 [00:38<00:03,  6.67it/s]predicting train subjects:  91%|█████████ | 242/266 [00:38<00:03,  6.68it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:38<00:03,  6.69it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:39<00:03,  6.69it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:39<00:03,  6.68it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:39<00:03,  6.59it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:39<00:02,  6.55it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:39<00:02,  6.57it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:39<00:02,  6.36it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:39<00:02,  6.21it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:40<00:02,  6.15it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:40<00:02,  6.09it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:40<00:02,  6.05it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:40<00:01,  6.03it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:40<00:01,  5.98it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:40<00:01,  5.90it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:41<00:01,  5.89it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:41<00:01,  5.89it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:41<00:01,  5.91it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:41<00:01,  5.91it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:41<00:00,  5.88it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:42<00:00,  5.85it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:42<00:00,  5.82it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:42<00:00,  5.81it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:42<00:00,  5.86it/s]predicting train subjects: 100%|██████████| 266/266 [00:42<00:00,  5.88it/s]predicting train subjects: 100%|██████████| 266/266 [00:42<00:00,  6.23it/s]
predicting test subjects sagittal:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects sagittal:  25%|██▌       | 1/4 [00:00<00:00,  6.12it/s]predicting test subjects sagittal:  50%|█████     | 2/4 [00:00<00:00,  6.12it/s]predicting test subjects sagittal:  75%|███████▌  | 3/4 [00:00<00:00,  6.18it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  6.25it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  6.24it/s]
predicting train subjects sagittal:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/266 [00:00<00:50,  5.20it/s]predicting train subjects sagittal:   1%|          | 2/266 [00:00<00:49,  5.34it/s]predicting train subjects sagittal:   1%|          | 3/266 [00:00<00:46,  5.65it/s]predicting train subjects sagittal:   2%|▏         | 4/266 [00:00<00:43,  5.96it/s]predicting train subjects sagittal:   2%|▏         | 5/266 [00:00<00:45,  5.71it/s]predicting train subjects sagittal:   2%|▏         | 6/266 [00:01<00:46,  5.58it/s]predicting train subjects sagittal:   3%|▎         | 7/266 [00:01<00:46,  5.56it/s]predicting train subjects sagittal:   3%|▎         | 8/266 [00:01<00:46,  5.58it/s]predicting train subjects sagittal:   3%|▎         | 9/266 [00:01<00:45,  5.59it/s]predicting train subjects sagittal:   4%|▍         | 10/266 [00:01<00:45,  5.61it/s]predicting train subjects sagittal:   4%|▍         | 11/266 [00:01<00:46,  5.49it/s]predicting train subjects sagittal:   5%|▍         | 12/266 [00:02<00:47,  5.37it/s]predicting train subjects sagittal:   5%|▍         | 13/266 [00:02<00:46,  5.39it/s]predicting train subjects sagittal:   5%|▌         | 14/266 [00:02<00:46,  5.44it/s]predicting train subjects sagittal:   6%|▌         | 15/266 [00:02<00:45,  5.50it/s]predicting train subjects sagittal:   6%|▌         | 16/266 [00:02<00:45,  5.52it/s]predicting train subjects sagittal:   6%|▋         | 17/266 [00:03<00:44,  5.56it/s]predicting train subjects sagittal:   7%|▋         | 18/266 [00:03<00:44,  5.57it/s]predicting train subjects sagittal:   7%|▋         | 19/266 [00:03<00:44,  5.51it/s]predicting train subjects sagittal:   8%|▊         | 20/266 [00:03<00:44,  5.53it/s]predicting train subjects sagittal:   8%|▊         | 21/266 [00:03<00:44,  5.55it/s]predicting train subjects sagittal:   8%|▊         | 22/266 [00:03<00:44,  5.52it/s]predicting train subjects sagittal:   9%|▊         | 23/266 [00:04<00:43,  5.53it/s]predicting train subjects sagittal:   9%|▉         | 24/266 [00:04<00:43,  5.59it/s]predicting train subjects sagittal:   9%|▉         | 25/266 [00:04<00:42,  5.61it/s]predicting train subjects sagittal:  10%|▉         | 26/266 [00:04<00:42,  5.66it/s]predicting train subjects sagittal:  10%|█         | 27/266 [00:04<00:42,  5.66it/s]predicting train subjects sagittal:  11%|█         | 28/266 [00:05<00:42,  5.66it/s]predicting train subjects sagittal:  11%|█         | 29/266 [00:05<00:42,  5.60it/s]predicting train subjects sagittal:  11%|█▏        | 30/266 [00:05<00:42,  5.56it/s]predicting train subjects sagittal:  12%|█▏        | 31/266 [00:05<00:45,  5.14it/s]predicting train subjects sagittal:  12%|█▏        | 32/266 [00:05<00:45,  5.17it/s]predicting train subjects sagittal:  12%|█▏        | 33/266 [00:05<00:43,  5.33it/s]predicting train subjects sagittal:  13%|█▎        | 34/266 [00:06<00:43,  5.38it/s]predicting train subjects sagittal:  13%|█▎        | 35/266 [00:06<00:43,  5.35it/s]predicting train subjects sagittal:  14%|█▎        | 36/266 [00:06<00:42,  5.42it/s]predicting train subjects sagittal:  14%|█▍        | 37/266 [00:06<00:41,  5.52it/s]predicting train subjects sagittal:  14%|█▍        | 38/266 [00:06<00:40,  5.59it/s]predicting train subjects sagittal:  15%|█▍        | 39/266 [00:07<00:40,  5.56it/s]predicting train subjects sagittal:  15%|█▌        | 40/266 [00:07<00:40,  5.54it/s]predicting train subjects sagittal:  15%|█▌        | 41/266 [00:07<00:40,  5.49it/s]predicting train subjects sagittal:  16%|█▌        | 42/266 [00:07<00:38,  5.84it/s]predicting train subjects sagittal:  16%|█▌        | 43/266 [00:07<00:36,  6.14it/s]predicting train subjects sagittal:  17%|█▋        | 44/266 [00:07<00:34,  6.37it/s]predicting train subjects sagittal:  17%|█▋        | 45/266 [00:07<00:34,  6.50it/s]predicting train subjects sagittal:  17%|█▋        | 46/266 [00:08<00:33,  6.62it/s]predicting train subjects sagittal:  18%|█▊        | 47/266 [00:08<00:32,  6.71it/s]predicting train subjects sagittal:  18%|█▊        | 48/266 [00:08<00:32,  6.79it/s]predicting train subjects sagittal:  18%|█▊        | 49/266 [00:08<00:31,  6.86it/s]predicting train subjects sagittal:  19%|█▉        | 50/266 [00:08<00:32,  6.74it/s]predicting train subjects sagittal:  19%|█▉        | 51/266 [00:08<00:32,  6.69it/s]predicting train subjects sagittal:  20%|█▉        | 52/266 [00:09<00:31,  6.77it/s]predicting train subjects sagittal:  20%|█▉        | 53/266 [00:09<00:31,  6.79it/s]predicting train subjects sagittal:  20%|██        | 54/266 [00:09<00:31,  6.83it/s]predicting train subjects sagittal:  21%|██        | 55/266 [00:09<00:30,  6.84it/s]predicting train subjects sagittal:  21%|██        | 56/266 [00:09<00:30,  6.90it/s]predicting train subjects sagittal:  21%|██▏       | 57/266 [00:09<00:30,  6.90it/s]predicting train subjects sagittal:  22%|██▏       | 58/266 [00:09<00:30,  6.93it/s]predicting train subjects sagittal:  22%|██▏       | 59/266 [00:10<00:29,  6.94it/s]predicting train subjects sagittal:  23%|██▎       | 60/266 [00:10<00:30,  6.85it/s]predicting train subjects sagittal:  23%|██▎       | 61/266 [00:10<00:29,  6.85it/s]predicting train subjects sagittal:  23%|██▎       | 62/266 [00:10<00:30,  6.75it/s]predicting train subjects sagittal:  24%|██▎       | 63/266 [00:10<00:30,  6.72it/s]predicting train subjects sagittal:  24%|██▍       | 64/266 [00:10<00:30,  6.72it/s]predicting train subjects sagittal:  24%|██▍       | 65/266 [00:10<00:29,  6.73it/s]predicting train subjects sagittal:  25%|██▍       | 66/266 [00:11<00:29,  6.77it/s]predicting train subjects sagittal:  25%|██▌       | 67/266 [00:11<00:29,  6.77it/s]predicting train subjects sagittal:  26%|██▌       | 68/266 [00:11<00:29,  6.77it/s]predicting train subjects sagittal:  26%|██▌       | 69/266 [00:11<00:29,  6.76it/s]predicting train subjects sagittal:  26%|██▋       | 70/266 [00:11<00:28,  6.79it/s]predicting train subjects sagittal:  27%|██▋       | 71/266 [00:11<00:28,  6.78it/s]predicting train subjects sagittal:  27%|██▋       | 72/266 [00:11<00:28,  6.82it/s]predicting train subjects sagittal:  27%|██▋       | 73/266 [00:12<00:28,  6.83it/s]predicting train subjects sagittal:  28%|██▊       | 74/266 [00:12<00:28,  6.83it/s]predicting train subjects sagittal:  28%|██▊       | 75/266 [00:12<00:27,  6.82it/s]predicting train subjects sagittal:  29%|██▊       | 76/266 [00:12<00:27,  6.82it/s]predicting train subjects sagittal:  29%|██▉       | 77/266 [00:12<00:28,  6.67it/s]predicting train subjects sagittal:  29%|██▉       | 78/266 [00:12<00:29,  6.36it/s]predicting train subjects sagittal:  30%|██▉       | 79/266 [00:13<00:30,  6.14it/s]predicting train subjects sagittal:  30%|███       | 80/266 [00:13<00:31,  5.96it/s]predicting train subjects sagittal:  30%|███       | 81/266 [00:13<00:32,  5.66it/s]predicting train subjects sagittal:  31%|███       | 82/266 [00:13<00:32,  5.68it/s]predicting train subjects sagittal:  31%|███       | 83/266 [00:13<00:32,  5.61it/s]predicting train subjects sagittal:  32%|███▏      | 84/266 [00:13<00:32,  5.68it/s]predicting train subjects sagittal:  32%|███▏      | 85/266 [00:14<00:31,  5.73it/s]predicting train subjects sagittal:  32%|███▏      | 86/266 [00:14<00:31,  5.75it/s]predicting train subjects sagittal:  33%|███▎      | 87/266 [00:14<00:31,  5.77it/s]predicting train subjects sagittal:  33%|███▎      | 88/266 [00:14<00:31,  5.71it/s]predicting train subjects sagittal:  33%|███▎      | 89/266 [00:14<00:30,  5.77it/s]predicting train subjects sagittal:  34%|███▍      | 90/266 [00:14<00:30,  5.75it/s]predicting train subjects sagittal:  34%|███▍      | 91/266 [00:15<00:30,  5.74it/s]predicting train subjects sagittal:  35%|███▍      | 92/266 [00:15<00:30,  5.78it/s]predicting train subjects sagittal:  35%|███▍      | 93/266 [00:15<00:30,  5.74it/s]predicting train subjects sagittal:  35%|███▌      | 94/266 [00:15<00:29,  5.78it/s]predicting train subjects sagittal:  36%|███▌      | 95/266 [00:15<00:29,  5.80it/s]predicting train subjects sagittal:  36%|███▌      | 96/266 [00:16<00:28,  5.96it/s]predicting train subjects sagittal:  36%|███▋      | 97/266 [00:16<00:30,  5.45it/s]predicting train subjects sagittal:  37%|███▋      | 98/266 [00:16<00:30,  5.58it/s]predicting train subjects sagittal:  37%|███▋      | 99/266 [00:16<00:27,  6.05it/s]predicting train subjects sagittal:  38%|███▊      | 100/266 [00:16<00:26,  6.18it/s]predicting train subjects sagittal:  38%|███▊      | 101/266 [00:16<00:26,  6.28it/s]predicting train subjects sagittal:  38%|███▊      | 102/266 [00:16<00:25,  6.39it/s]predicting train subjects sagittal:  39%|███▊      | 103/266 [00:17<00:25,  6.48it/s]predicting train subjects sagittal:  39%|███▉      | 104/266 [00:17<00:25,  6.48it/s]predicting train subjects sagittal:  39%|███▉      | 105/266 [00:17<00:25,  6.43it/s]predicting train subjects sagittal:  40%|███▉      | 106/266 [00:17<00:24,  6.50it/s]predicting train subjects sagittal:  40%|████      | 107/266 [00:17<00:24,  6.44it/s]predicting train subjects sagittal:  41%|████      | 108/266 [00:17<00:24,  6.50it/s]predicting train subjects sagittal:  41%|████      | 109/266 [00:18<00:24,  6.36it/s]predicting train subjects sagittal:  41%|████▏     | 110/266 [00:18<00:24,  6.41it/s]predicting train subjects sagittal:  42%|████▏     | 111/266 [00:18<00:24,  6.36it/s]predicting train subjects sagittal:  42%|████▏     | 112/266 [00:18<00:23,  6.44it/s]predicting train subjects sagittal:  42%|████▏     | 113/266 [00:18<00:23,  6.46it/s]predicting train subjects sagittal:  43%|████▎     | 114/266 [00:18<00:23,  6.46it/s]predicting train subjects sagittal:  43%|████▎     | 115/266 [00:19<00:23,  6.51it/s]predicting train subjects sagittal:  44%|████▎     | 116/266 [00:19<00:23,  6.52it/s]predicting train subjects sagittal:  44%|████▍     | 117/266 [00:19<00:22,  6.53it/s]predicting train subjects sagittal:  44%|████▍     | 118/266 [00:19<00:22,  6.53it/s]predicting train subjects sagittal:  45%|████▍     | 119/266 [00:19<00:24,  6.11it/s]predicting train subjects sagittal:  45%|████▌     | 120/266 [00:19<00:25,  5.84it/s]predicting train subjects sagittal:  45%|████▌     | 121/266 [00:20<00:25,  5.70it/s]predicting train subjects sagittal:  46%|████▌     | 122/266 [00:20<00:27,  5.33it/s]predicting train subjects sagittal:  46%|████▌     | 123/266 [00:20<00:28,  5.07it/s]predicting train subjects sagittal:  47%|████▋     | 124/266 [00:20<00:27,  5.23it/s]predicting train subjects sagittal:  47%|████▋     | 125/266 [00:20<00:26,  5.34it/s]predicting train subjects sagittal:  47%|████▋     | 126/266 [00:21<00:26,  5.30it/s]predicting train subjects sagittal:  48%|████▊     | 127/266 [00:21<00:26,  5.28it/s]predicting train subjects sagittal:  48%|████▊     | 128/266 [00:21<00:26,  5.26it/s]predicting train subjects sagittal:  48%|████▊     | 129/266 [00:21<00:25,  5.36it/s]predicting train subjects sagittal:  49%|████▉     | 130/266 [00:21<00:25,  5.43it/s]predicting train subjects sagittal:  49%|████▉     | 131/266 [00:21<00:24,  5.47it/s]predicting train subjects sagittal:  50%|████▉     | 132/266 [00:22<00:24,  5.45it/s]predicting train subjects sagittal:  50%|█████     | 133/266 [00:22<00:24,  5.50it/s]predicting train subjects sagittal:  50%|█████     | 134/266 [00:22<00:23,  5.52it/s]predicting train subjects sagittal:  51%|█████     | 135/266 [00:22<00:23,  5.51it/s]predicting train subjects sagittal:  51%|█████     | 136/266 [00:22<00:23,  5.49it/s]predicting train subjects sagittal:  52%|█████▏    | 137/266 [00:23<00:23,  5.50it/s]predicting train subjects sagittal:  52%|█████▏    | 138/266 [00:23<00:23,  5.56it/s]predicting train subjects sagittal:  52%|█████▏    | 139/266 [00:23<00:22,  5.71it/s]predicting train subjects sagittal:  53%|█████▎    | 140/266 [00:23<00:21,  5.83it/s]predicting train subjects sagittal:  53%|█████▎    | 141/266 [00:23<00:21,  5.92it/s]predicting train subjects sagittal:  53%|█████▎    | 142/266 [00:23<00:21,  5.90it/s]predicting train subjects sagittal:  54%|█████▍    | 143/266 [00:24<00:20,  5.97it/s]predicting train subjects sagittal:  54%|█████▍    | 144/266 [00:24<00:21,  5.68it/s]predicting train subjects sagittal:  55%|█████▍    | 145/266 [00:24<00:21,  5.59it/s]predicting train subjects sagittal:  55%|█████▍    | 146/266 [00:24<00:21,  5.66it/s]predicting train subjects sagittal:  55%|█████▌    | 147/266 [00:24<00:20,  5.69it/s]predicting train subjects sagittal:  56%|█████▌    | 148/266 [00:24<00:21,  5.59it/s]predicting train subjects sagittal:  56%|█████▌    | 149/266 [00:25<00:20,  5.72it/s]predicting train subjects sagittal:  56%|█████▋    | 150/266 [00:25<00:19,  5.85it/s]predicting train subjects sagittal:  57%|█████▋    | 151/266 [00:25<00:19,  5.91it/s]predicting train subjects sagittal:  57%|█████▋    | 152/266 [00:25<00:19,  5.71it/s]predicting train subjects sagittal:  58%|█████▊    | 153/266 [00:25<00:19,  5.75it/s]predicting train subjects sagittal:  58%|█████▊    | 154/266 [00:25<00:19,  5.73it/s]predicting train subjects sagittal:  58%|█████▊    | 155/266 [00:26<00:18,  6.08it/s]predicting train subjects sagittal:  59%|█████▊    | 156/266 [00:26<00:17,  6.35it/s]predicting train subjects sagittal:  59%|█████▉    | 157/266 [00:26<00:16,  6.63it/s]predicting train subjects sagittal:  59%|█████▉    | 158/266 [00:26<00:15,  6.84it/s]predicting train subjects sagittal:  60%|█████▉    | 159/266 [00:26<00:15,  7.01it/s]predicting train subjects sagittal:  60%|██████    | 160/266 [00:26<00:14,  7.11it/s]predicting train subjects sagittal:  61%|██████    | 161/266 [00:26<00:14,  7.05it/s]predicting train subjects sagittal:  61%|██████    | 162/266 [00:27<00:14,  7.18it/s]predicting train subjects sagittal:  61%|██████▏   | 163/266 [00:27<00:14,  7.23it/s]predicting train subjects sagittal:  62%|██████▏   | 164/266 [00:27<00:14,  7.19it/s]predicting train subjects sagittal:  62%|██████▏   | 165/266 [00:27<00:13,  7.25it/s]predicting train subjects sagittal:  62%|██████▏   | 166/266 [00:27<00:13,  7.33it/s]predicting train subjects sagittal:  63%|██████▎   | 167/266 [00:27<00:13,  7.40it/s]predicting train subjects sagittal:  63%|██████▎   | 168/266 [00:27<00:13,  7.41it/s]predicting train subjects sagittal:  64%|██████▎   | 169/266 [00:28<00:13,  7.31it/s]predicting train subjects sagittal:  64%|██████▍   | 170/266 [00:28<00:13,  7.36it/s]predicting train subjects sagittal:  64%|██████▍   | 171/266 [00:28<00:12,  7.40it/s]predicting train subjects sagittal:  65%|██████▍   | 172/266 [00:28<00:12,  7.33it/s]predicting train subjects sagittal:  65%|██████▌   | 173/266 [00:28<00:13,  7.12it/s]predicting train subjects sagittal:  65%|██████▌   | 174/266 [00:28<00:13,  7.04it/s]predicting train subjects sagittal:  66%|██████▌   | 175/266 [00:28<00:13,  6.98it/s]predicting train subjects sagittal:  66%|██████▌   | 176/266 [00:29<00:13,  6.89it/s]predicting train subjects sagittal:  67%|██████▋   | 177/266 [00:29<00:13,  6.80it/s]predicting train subjects sagittal:  67%|██████▋   | 178/266 [00:29<00:13,  6.66it/s]predicting train subjects sagittal:  67%|██████▋   | 179/266 [00:29<00:13,  6.43it/s]predicting train subjects sagittal:  68%|██████▊   | 180/266 [00:29<00:13,  6.51it/s]predicting train subjects sagittal:  68%|██████▊   | 181/266 [00:29<00:12,  6.56it/s]predicting train subjects sagittal:  68%|██████▊   | 182/266 [00:29<00:12,  6.58it/s]predicting train subjects sagittal:  69%|██████▉   | 183/266 [00:30<00:12,  6.52it/s]predicting train subjects sagittal:  69%|██████▉   | 184/266 [00:30<00:12,  6.62it/s]predicting train subjects sagittal:  70%|██████▉   | 185/266 [00:30<00:12,  6.67it/s]predicting train subjects sagittal:  70%|██████▉   | 186/266 [00:30<00:11,  6.69it/s]predicting train subjects sagittal:  70%|███████   | 187/266 [00:30<00:11,  6.69it/s]predicting train subjects sagittal:  71%|███████   | 188/266 [00:30<00:11,  6.51it/s]predicting train subjects sagittal:  71%|███████   | 189/266 [00:31<00:11,  6.42it/s]predicting train subjects sagittal:  71%|███████▏  | 190/266 [00:31<00:11,  6.41it/s]predicting train subjects sagittal:  72%|███████▏  | 191/266 [00:31<00:11,  6.44it/s]predicting train subjects sagittal:  72%|███████▏  | 192/266 [00:31<00:11,  6.55it/s]predicting train subjects sagittal:  73%|███████▎  | 193/266 [00:31<00:11,  6.46it/s]predicting train subjects sagittal:  73%|███████▎  | 194/266 [00:31<00:11,  6.29it/s]predicting train subjects sagittal:  73%|███████▎  | 195/266 [00:31<00:11,  6.33it/s]predicting train subjects sagittal:  74%|███████▎  | 196/266 [00:32<00:10,  6.42it/s]predicting train subjects sagittal:  74%|███████▍  | 197/266 [00:32<00:10,  6.51it/s]predicting train subjects sagittal:  74%|███████▍  | 198/266 [00:32<00:10,  6.59it/s]predicting train subjects sagittal:  75%|███████▍  | 199/266 [00:32<00:10,  6.60it/s]predicting train subjects sagittal:  75%|███████▌  | 200/266 [00:32<00:09,  6.62it/s]predicting train subjects sagittal:  76%|███████▌  | 201/266 [00:32<00:09,  6.66it/s]predicting train subjects sagittal:  76%|███████▌  | 202/266 [00:32<00:09,  6.57it/s]predicting train subjects sagittal:  76%|███████▋  | 203/266 [00:33<00:09,  6.46it/s]predicting train subjects sagittal:  77%|███████▋  | 204/266 [00:33<00:09,  6.54it/s]predicting train subjects sagittal:  77%|███████▋  | 205/266 [00:33<00:09,  6.45it/s]predicting train subjects sagittal:  77%|███████▋  | 206/266 [00:33<00:09,  6.41it/s]predicting train subjects sagittal:  78%|███████▊  | 207/266 [00:33<00:09,  6.49it/s]predicting train subjects sagittal:  78%|███████▊  | 208/266 [00:33<00:08,  6.47it/s]predicting train subjects sagittal:  79%|███████▊  | 209/266 [00:34<00:08,  6.56it/s]predicting train subjects sagittal:  79%|███████▉  | 210/266 [00:34<00:08,  6.58it/s]predicting train subjects sagittal:  79%|███████▉  | 211/266 [00:34<00:08,  6.58it/s]predicting train subjects sagittal:  80%|███████▉  | 212/266 [00:34<00:08,  6.60it/s]predicting train subjects sagittal:  80%|████████  | 213/266 [00:34<00:07,  6.72it/s]predicting train subjects sagittal:  80%|████████  | 214/266 [00:34<00:07,  6.82it/s]predicting train subjects sagittal:  81%|████████  | 215/266 [00:34<00:07,  6.89it/s]predicting train subjects sagittal:  81%|████████  | 216/266 [00:35<00:07,  6.93it/s]predicting train subjects sagittal:  82%|████████▏ | 217/266 [00:35<00:07,  6.97it/s]predicting train subjects sagittal:  82%|████████▏ | 218/266 [00:35<00:06,  7.00it/s]predicting train subjects sagittal:  82%|████████▏ | 219/266 [00:35<00:06,  6.87it/s]predicting train subjects sagittal:  83%|████████▎ | 220/266 [00:35<00:06,  6.91it/s]predicting train subjects sagittal:  83%|████████▎ | 221/266 [00:35<00:06,  6.94it/s]predicting train subjects sagittal:  83%|████████▎ | 222/266 [00:35<00:06,  6.97it/s]predicting train subjects sagittal:  84%|████████▍ | 223/266 [00:36<00:06,  6.87it/s]predicting train subjects sagittal:  84%|████████▍ | 224/266 [00:36<00:06,  6.84it/s]predicting train subjects sagittal:  85%|████████▍ | 225/266 [00:36<00:05,  6.87it/s]predicting train subjects sagittal:  85%|████████▍ | 226/266 [00:36<00:05,  6.86it/s]predicting train subjects sagittal:  85%|████████▌ | 227/266 [00:36<00:05,  6.74it/s]predicting train subjects sagittal:  86%|████████▌ | 228/266 [00:36<00:05,  6.81it/s]predicting train subjects sagittal:  86%|████████▌ | 229/266 [00:36<00:05,  6.83it/s]predicting train subjects sagittal:  86%|████████▋ | 230/266 [00:37<00:05,  6.91it/s]predicting train subjects sagittal:  87%|████████▋ | 231/266 [00:37<00:05,  6.82it/s]predicting train subjects sagittal:  87%|████████▋ | 232/266 [00:37<00:05,  6.74it/s]predicting train subjects sagittal:  88%|████████▊ | 233/266 [00:37<00:04,  6.73it/s]predicting train subjects sagittal:  88%|████████▊ | 234/266 [00:37<00:04,  6.73it/s]predicting train subjects sagittal:  88%|████████▊ | 235/266 [00:37<00:04,  6.68it/s]predicting train subjects sagittal:  89%|████████▊ | 236/266 [00:38<00:04,  6.70it/s]predicting train subjects sagittal:  89%|████████▉ | 237/266 [00:38<00:04,  6.72it/s]predicting train subjects sagittal:  89%|████████▉ | 238/266 [00:38<00:04,  6.64it/s]predicting train subjects sagittal:  90%|████████▉ | 239/266 [00:38<00:04,  6.56it/s]predicting train subjects sagittal:  90%|█████████ | 240/266 [00:38<00:03,  6.63it/s]predicting train subjects sagittal:  91%|█████████ | 241/266 [00:38<00:03,  6.69it/s]predicting train subjects sagittal:  91%|█████████ | 242/266 [00:38<00:03,  6.41it/s]predicting train subjects sagittal:  91%|█████████▏| 243/266 [00:39<00:03,  6.44it/s]predicting train subjects sagittal:  92%|█████████▏| 244/266 [00:39<00:03,  6.38it/s]predicting train subjects sagittal:  92%|█████████▏| 245/266 [00:39<00:03,  6.31it/s]predicting train subjects sagittal:  92%|█████████▏| 246/266 [00:39<00:03,  6.39it/s]predicting train subjects sagittal:  93%|█████████▎| 247/266 [00:39<00:02,  6.38it/s]predicting train subjects sagittal:  93%|█████████▎| 248/266 [00:39<00:02,  6.45it/s]predicting train subjects sagittal:  94%|█████████▎| 249/266 [00:40<00:02,  6.00it/s]predicting train subjects sagittal:  94%|█████████▍| 250/266 [00:40<00:02,  5.95it/s]predicting train subjects sagittal:  94%|█████████▍| 251/266 [00:40<00:02,  5.89it/s]predicting train subjects sagittal:  95%|█████████▍| 252/266 [00:40<00:02,  5.90it/s]predicting train subjects sagittal:  95%|█████████▌| 253/266 [00:40<00:02,  5.87it/s]predicting train subjects sagittal:  95%|█████████▌| 254/266 [00:40<00:02,  5.87it/s]predicting train subjects sagittal:  96%|█████████▌| 255/266 [00:41<00:01,  5.89it/s]predicting train subjects sagittal:  96%|█████████▌| 256/266 [00:41<00:01,  5.88it/s]predicting train subjects sagittal:  97%|█████████▋| 257/266 [00:41<00:01,  5.87it/s]predicting train subjects sagittal:  97%|█████████▋| 258/266 [00:41<00:01,  5.88it/s]predicting train subjects sagittal:  97%|█████████▋| 259/266 [00:41<00:01,  5.88it/s]predicting train subjects sagittal:  98%|█████████▊| 260/266 [00:41<00:01,  5.89it/s]predicting train subjects sagittal:  98%|█████████▊| 261/266 [00:42<00:00,  5.66it/s]predicting train subjects sagittal:  98%|█████████▊| 262/266 [00:42<00:00,  5.70it/s]predicting train subjects sagittal:  99%|█████████▉| 263/266 [00:42<00:00,  5.76it/s]predicting train subjects sagittal:  99%|█████████▉| 264/266 [00:42<00:00,  5.73it/s]predicting train subjects sagittal: 100%|█████████▉| 265/266 [00:42<00:00,  5.78it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:43<00:00,  5.83it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:43<00:00,  6.18it/s]
saving BB  test1-THALAMUS:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 4/4 [00:00<00:00, 71.46it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 7/266 [00:00<00:03, 69.64it/s]saving BB  train1-THALAMUS:   5%|▌         | 14/266 [00:00<00:03, 69.60it/s]saving BB  train1-THALAMUS:   8%|▊         | 21/266 [00:00<00:03, 68.99it/s]saving BB  train1-THALAMUS:  11%|█         | 28/266 [00:00<00:03, 68.84it/s]saving BB  train1-THALAMUS:  14%|█▎        | 36/266 [00:00<00:03, 70.81it/s]saving BB  train1-THALAMUS:  17%|█▋        | 44/266 [00:00<00:03, 72.77it/s]saving BB  train1-THALAMUS:  20%|█▉        | 53/266 [00:00<00:02, 75.12it/s]saving BB  train1-THALAMUS:  23%|██▎       | 61/266 [00:00<00:02, 76.28it/s]saving BB  train1-THALAMUS:  26%|██▋       | 70/266 [00:00<00:02, 78.83it/s]saving BB  train1-THALAMUS:  30%|██▉       | 79/266 [00:01<00:02, 80.26it/s]saving BB  train1-THALAMUS:  33%|███▎      | 87/266 [00:01<00:02, 78.06it/s]saving BB  train1-THALAMUS:  36%|███▌      | 95/266 [00:01<00:02, 75.80it/s]saving BB  train1-THALAMUS:  39%|███▊      | 103/266 [00:01<00:02, 73.72it/s]saving BB  train1-THALAMUS:  42%|████▏     | 112/266 [00:01<00:02, 76.34it/s]saving BB  train1-THALAMUS:  45%|████▌     | 121/266 [00:01<00:01, 77.34it/s]saving BB  train1-THALAMUS:  48%|████▊     | 129/266 [00:01<00:01, 73.55it/s]saving BB  train1-THALAMUS:  52%|█████▏    | 137/266 [00:01<00:01, 73.98it/s]saving BB  train1-THALAMUS:  55%|█████▍    | 145/266 [00:01<00:01, 73.60it/s]saving BB  train1-THALAMUS:  58%|█████▊    | 153/266 [00:02<00:01, 73.65it/s]saving BB  train1-THALAMUS:  61%|██████    | 162/266 [00:02<00:01, 77.30it/s]saving BB  train1-THALAMUS:  64%|██████▍   | 171/266 [00:02<00:01, 80.64it/s]saving BB  train1-THALAMUS:  68%|██████▊   | 181/266 [00:02<00:01, 84.43it/s]saving BB  train1-THALAMUS:  72%|███████▏  | 191/266 [00:02<00:00, 85.67it/s]saving BB  train1-THALAMUS:  75%|███████▌  | 200/266 [00:02<00:00, 84.80it/s]saving BB  train1-THALAMUS:  79%|███████▊  | 209/266 [00:02<00:00, 84.08it/s]saving BB  train1-THALAMUS:  82%|████████▏ | 218/266 [00:02<00:00, 84.34it/s]saving BB  train1-THALAMUS:  85%|████████▌ | 227/266 [00:02<00:00, 85.45it/s]saving BB  train1-THALAMUS:  89%|████████▊ | 236/266 [00:03<00:00, 84.60it/s]saving BB  train1-THALAMUS:  92%|█████████▏| 245/266 [00:03<00:00, 84.44it/s]saving BB  train1-THALAMUS:  95%|█████████▌| 254/266 [00:03<00:00, 81.07it/s]saving BB  train1-THALAMUS:  99%|█████████▉| 263/266 [00:03<00:00, 80.45it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 78.53it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 4/4 [00:00<00:00, 82.17it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   3%|▎         | 7/266 [00:00<00:03, 65.87it/s]saving BB  train1-THALAMUS Sagittal:   6%|▌         | 15/266 [00:00<00:03, 67.36it/s]saving BB  train1-THALAMUS Sagittal:   9%|▊         | 23/266 [00:00<00:03, 68.51it/s]saving BB  train1-THALAMUS Sagittal:  12%|█▏        | 31/266 [00:00<00:03, 70.56it/s]saving BB  train1-THALAMUS Sagittal:  15%|█▍        | 39/266 [00:00<00:03, 72.82it/s]saving BB  train1-THALAMUS Sagittal:  18%|█▊        | 48/266 [00:00<00:02, 75.54it/s]saving BB  train1-THALAMUS Sagittal:  21%|██▏       | 57/266 [00:00<00:02, 77.44it/s]saving BB  train1-THALAMUS Sagittal:  25%|██▌       | 67/266 [00:00<00:02, 80.96it/s]saving BB  train1-THALAMUS Sagittal:  29%|██▉       | 77/266 [00:00<00:02, 84.03it/s]saving BB  train1-THALAMUS Sagittal:  32%|███▏      | 86/266 [00:01<00:02, 81.63it/s]saving BB  train1-THALAMUS Sagittal:  36%|███▌      | 95/266 [00:01<00:02, 79.88it/s]saving BB  train1-THALAMUS Sagittal:  39%|███▉      | 104/266 [00:01<00:02, 80.33it/s]saving BB  train1-THALAMUS Sagittal:  42%|████▏     | 113/266 [00:01<00:01, 81.09it/s]saving BB  train1-THALAMUS Sagittal:  46%|████▌     | 122/266 [00:01<00:01, 81.20it/s]saving BB  train1-THALAMUS Sagittal:  49%|████▉     | 131/266 [00:01<00:01, 79.93it/s]saving BB  train1-THALAMUS Sagittal:  52%|█████▏    | 139/266 [00:01<00:01, 78.29it/s]saving BB  train1-THALAMUS Sagittal:  55%|█████▌    | 147/266 [00:01<00:01, 77.24it/s]saving BB  train1-THALAMUS Sagittal:  58%|█████▊    | 155/266 [00:01<00:01, 77.60it/s]saving BB  train1-THALAMUS Sagittal:  62%|██████▏   | 165/266 [00:02<00:01, 80.69it/s]saving BB  train1-THALAMUS Sagittal:  66%|██████▌   | 175/266 [00:02<00:01, 84.01it/s]saving BB  train1-THALAMUS Sagittal:  69%|██████▉   | 184/266 [00:02<00:00, 85.66it/s]saving BB  train1-THALAMUS Sagittal:  73%|███████▎  | 194/266 [00:02<00:00, 86.66it/s]saving BB  train1-THALAMUS Sagittal:  76%|███████▋  | 203/266 [00:02<00:00, 84.50it/s]saving BB  train1-THALAMUS Sagittal:  80%|███████▉  | 212/266 [00:02<00:00, 82.97it/s]saving BB  train1-THALAMUS Sagittal:  83%|████████▎ | 221/266 [00:02<00:00, 84.43it/s]saving BB  train1-THALAMUS Sagittal:  86%|████████▋ | 230/266 [00:02<00:00, 85.26it/s]saving BB  train1-THALAMUS Sagittal:  90%|████████▉ | 239/266 [00:02<00:00, 86.17it/s]saving BB  train1-THALAMUS Sagittal:  93%|█████████▎| 248/266 [00:03<00:00, 86.43it/s]saving BB  train1-THALAMUS Sagittal:  97%|█████████▋| 257/266 [00:03<00:00, 82.88it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 80.02it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 81.03it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<04:52,  1.10s/it]Loading train:   1%|          | 2/266 [00:02<04:37,  1.05s/it]Loading train:   1%|          | 3/266 [00:02<04:14,  1.03it/s]Loading train:   2%|▏         | 4/266 [00:03<03:56,  1.11it/s]Loading train:   2%|▏         | 5/266 [00:04<03:59,  1.09it/s]Loading train:   2%|▏         | 6/266 [00:05<03:35,  1.21it/s]Loading train:   3%|▎         | 7/266 [00:05<03:19,  1.30it/s]Loading train:   3%|▎         | 8/266 [00:06<03:08,  1.37it/s]Loading train:   3%|▎         | 9/266 [00:07<02:59,  1.43it/s]Loading train:   4%|▍         | 10/266 [00:07<02:51,  1.49it/s]Loading train:   4%|▍         | 11/266 [00:08<02:45,  1.54it/s]Loading train:   5%|▍         | 12/266 [00:08<02:42,  1.56it/s]Loading train:   5%|▍         | 13/266 [00:09<02:43,  1.55it/s]Loading train:   5%|▌         | 14/266 [00:10<02:46,  1.51it/s]Loading train:   6%|▌         | 15/266 [00:10<02:41,  1.55it/s]Loading train:   6%|▌         | 16/266 [00:11<02:44,  1.52it/s]Loading train:   6%|▋         | 17/266 [00:12<02:43,  1.52it/s]Loading train:   7%|▋         | 18/266 [00:12<02:41,  1.54it/s]Loading train:   7%|▋         | 19/266 [00:13<02:38,  1.56it/s]Loading train:   8%|▊         | 20/266 [00:14<02:34,  1.59it/s]Loading train:   8%|▊         | 21/266 [00:14<02:33,  1.59it/s]Loading train:   8%|▊         | 22/266 [00:15<02:36,  1.56it/s]Loading train:   9%|▊         | 23/266 [00:15<02:35,  1.56it/s]Loading train:   9%|▉         | 24/266 [00:16<02:33,  1.58it/s]Loading train:   9%|▉         | 25/266 [00:17<02:29,  1.61it/s]Loading train:  10%|▉         | 26/266 [00:17<02:25,  1.65it/s]Loading train:  10%|█         | 27/266 [00:18<02:21,  1.69it/s]Loading train:  11%|█         | 28/266 [00:18<02:19,  1.71it/s]Loading train:  11%|█         | 29/266 [00:19<02:19,  1.70it/s]Loading train:  11%|█▏        | 30/266 [00:20<02:22,  1.65it/s]Loading train:  12%|█▏        | 31/266 [00:20<02:21,  1.67it/s]Loading train:  12%|█▏        | 32/266 [00:21<02:18,  1.69it/s]Loading train:  12%|█▏        | 33/266 [00:21<02:16,  1.71it/s]Loading train:  13%|█▎        | 34/266 [00:22<02:13,  1.74it/s]Loading train:  13%|█▎        | 35/266 [00:22<02:12,  1.75it/s]Loading train:  14%|█▎        | 36/266 [00:23<02:15,  1.70it/s]Loading train:  14%|█▍        | 37/266 [00:24<02:15,  1.69it/s]Loading train:  14%|█▍        | 38/266 [00:24<02:12,  1.72it/s]Loading train:  15%|█▍        | 39/266 [00:25<02:09,  1.75it/s]Loading train:  15%|█▌        | 40/266 [00:25<02:08,  1.76it/s]Loading train:  15%|█▌        | 41/266 [00:26<02:08,  1.75it/s]Loading train:  16%|█▌        | 42/266 [00:26<02:08,  1.75it/s]Loading train:  16%|█▌        | 43/266 [00:27<02:08,  1.74it/s]Loading train:  17%|█▋        | 44/266 [00:28<02:06,  1.76it/s]Loading train:  17%|█▋        | 45/266 [00:28<02:03,  1.79it/s]Loading train:  17%|█▋        | 46/266 [00:29<02:04,  1.77it/s]Loading train:  18%|█▊        | 47/266 [00:29<02:01,  1.80it/s]Loading train:  18%|█▊        | 48/266 [00:30<02:00,  1.82it/s]Loading train:  18%|█▊        | 49/266 [00:30<01:58,  1.82it/s]Loading train:  19%|█▉        | 50/266 [00:31<02:00,  1.80it/s]Loading train:  19%|█▉        | 51/266 [00:31<02:00,  1.78it/s]Loading train:  20%|█▉        | 52/266 [00:32<01:59,  1.80it/s]Loading train:  20%|█▉        | 53/266 [00:33<01:56,  1.84it/s]Loading train:  20%|██        | 54/266 [00:33<01:55,  1.84it/s]Loading train:  21%|██        | 55/266 [00:34<01:53,  1.86it/s]Loading train:  21%|██        | 56/266 [00:34<01:53,  1.86it/s]Loading train:  21%|██▏       | 57/266 [00:35<01:53,  1.84it/s]Loading train:  22%|██▏       | 58/266 [00:35<01:54,  1.82it/s]Loading train:  22%|██▏       | 59/266 [00:36<01:55,  1.79it/s]Loading train:  23%|██▎       | 60/266 [00:36<01:52,  1.84it/s]Loading train:  23%|██▎       | 61/266 [00:37<01:48,  1.88it/s]Loading train:  23%|██▎       | 62/266 [00:37<01:47,  1.90it/s]Loading train:  24%|██▎       | 63/266 [00:38<01:45,  1.93it/s]Loading train:  24%|██▍       | 64/266 [00:38<01:44,  1.94it/s]Loading train:  24%|██▍       | 65/266 [00:39<01:43,  1.95it/s]Loading train:  25%|██▍       | 66/266 [00:39<01:43,  1.93it/s]Loading train:  25%|██▌       | 67/266 [00:40<01:43,  1.92it/s]Loading train:  26%|██▌       | 68/266 [00:40<01:41,  1.95it/s]Loading train:  26%|██▌       | 69/266 [00:41<01:41,  1.94it/s]Loading train:  26%|██▋       | 70/266 [00:41<01:40,  1.95it/s]Loading train:  27%|██▋       | 71/266 [00:42<01:40,  1.94it/s]Loading train:  27%|██▋       | 72/266 [00:43<01:39,  1.95it/s]Loading train:  27%|██▋       | 73/266 [00:43<01:38,  1.96it/s]Loading train:  28%|██▊       | 74/266 [00:44<01:38,  1.95it/s]Loading train:  28%|██▊       | 75/266 [00:44<01:36,  1.97it/s]Loading train:  29%|██▊       | 76/266 [00:45<01:35,  2.00it/s]Loading train:  29%|██▉       | 77/266 [00:45<01:36,  1.96it/s]Loading train:  29%|██▉       | 78/266 [00:46<01:42,  1.83it/s]Loading train:  30%|██▉       | 79/266 [00:46<01:46,  1.76it/s]Loading train:  30%|███       | 80/266 [00:47<01:47,  1.72it/s]Loading train:  30%|███       | 81/266 [00:48<01:49,  1.69it/s]Loading train:  31%|███       | 82/266 [00:48<01:49,  1.67it/s]Loading train:  31%|███       | 83/266 [00:49<01:48,  1.68it/s]Loading train:  32%|███▏      | 84/266 [00:49<01:47,  1.69it/s]Loading train:  32%|███▏      | 85/266 [00:50<01:47,  1.68it/s]Loading train:  32%|███▏      | 86/266 [00:50<01:46,  1.69it/s]Loading train:  33%|███▎      | 87/266 [00:51<01:44,  1.71it/s]Loading train:  33%|███▎      | 88/266 [00:52<01:48,  1.64it/s]Loading train:  33%|███▎      | 89/266 [00:52<01:47,  1.64it/s]Loading train:  34%|███▍      | 90/266 [00:53<01:46,  1.66it/s]Loading train:  34%|███▍      | 91/266 [00:53<01:43,  1.70it/s]Loading train:  35%|███▍      | 92/266 [00:54<01:42,  1.70it/s]Loading train:  35%|███▍      | 93/266 [00:55<01:41,  1.71it/s]Loading train:  35%|███▌      | 94/266 [00:55<01:37,  1.76it/s]Loading train:  36%|███▌      | 95/266 [00:56<01:36,  1.76it/s]Loading train:  36%|███▌      | 96/266 [00:57<01:47,  1.58it/s]Loading train:  36%|███▋      | 97/266 [00:57<02:01,  1.39it/s]Loading train:  37%|███▋      | 98/266 [00:58<02:04,  1.35it/s]Loading train:  37%|███▋      | 99/266 [00:59<02:00,  1.38it/s]Loading train:  38%|███▊      | 100/266 [01:00<02:01,  1.37it/s]Loading train:  38%|███▊      | 101/266 [01:00<01:49,  1.51it/s]Loading train:  38%|███▊      | 102/266 [01:01<01:41,  1.62it/s]Loading train:  39%|███▊      | 103/266 [01:01<01:34,  1.73it/s]Loading train:  39%|███▉      | 104/266 [01:02<01:30,  1.79it/s]Loading train:  39%|███▉      | 105/266 [01:02<01:27,  1.85it/s]Loading train:  40%|███▉      | 106/266 [01:03<01:25,  1.88it/s]Loading train:  40%|████      | 107/266 [01:03<01:24,  1.89it/s]Loading train:  41%|████      | 108/266 [01:04<01:24,  1.87it/s]Loading train:  41%|████      | 109/266 [01:04<01:23,  1.88it/s]Loading train:  41%|████▏     | 110/266 [01:05<01:24,  1.85it/s]Loading train:  42%|████▏     | 111/266 [01:05<01:22,  1.88it/s]Loading train:  42%|████▏     | 112/266 [01:06<01:19,  1.93it/s]Loading train:  42%|████▏     | 113/266 [01:06<01:20,  1.91it/s]Loading train:  43%|████▎     | 114/266 [01:07<01:20,  1.89it/s]Loading train:  43%|████▎     | 115/266 [01:07<01:18,  1.92it/s]Loading train:  44%|████▎     | 116/266 [01:08<01:17,  1.93it/s]Loading train:  44%|████▍     | 117/266 [01:08<01:16,  1.96it/s]Loading train:  44%|████▍     | 118/266 [01:09<01:14,  1.98it/s]Loading train:  45%|████▍     | 119/266 [01:09<01:16,  1.91it/s]Loading train:  45%|████▌     | 120/266 [01:10<01:17,  1.89it/s]Loading train:  45%|████▌     | 121/266 [01:11<01:18,  1.85it/s]Loading train:  46%|████▌     | 122/266 [01:11<01:19,  1.82it/s]Loading train:  46%|████▌     | 123/266 [01:12<01:18,  1.82it/s]Loading train:  47%|████▋     | 124/266 [01:12<01:16,  1.85it/s]Loading train:  47%|████▋     | 125/266 [01:13<01:15,  1.87it/s]Loading train:  47%|████▋     | 126/266 [01:13<01:14,  1.87it/s]Loading train:  48%|████▊     | 127/266 [01:14<01:13,  1.89it/s]Loading train:  48%|████▊     | 128/266 [01:14<01:14,  1.86it/s]Loading train:  48%|████▊     | 129/266 [01:15<01:13,  1.87it/s]Loading train:  49%|████▉     | 130/266 [01:15<01:13,  1.86it/s]Loading train:  49%|████▉     | 131/266 [01:16<01:12,  1.85it/s]Loading train:  50%|████▉     | 132/266 [01:17<01:12,  1.85it/s]Loading train:  50%|█████     | 133/266 [01:17<01:12,  1.83it/s]Loading train:  50%|█████     | 134/266 [01:18<01:11,  1.83it/s]Loading train:  51%|█████     | 135/266 [01:18<01:11,  1.83it/s]Loading train:  51%|█████     | 136/266 [01:19<01:10,  1.83it/s]Loading train:  52%|█████▏    | 137/266 [01:19<01:12,  1.79it/s]Loading train:  52%|█████▏    | 138/266 [01:20<01:12,  1.77it/s]Loading train:  52%|█████▏    | 139/266 [01:20<01:11,  1.78it/s]Loading train:  53%|█████▎    | 140/266 [01:21<01:10,  1.79it/s]Loading train:  53%|█████▎    | 141/266 [01:22<01:10,  1.78it/s]Loading train:  53%|█████▎    | 142/266 [01:22<01:09,  1.79it/s]Loading train:  54%|█████▍    | 143/266 [01:23<01:09,  1.77it/s]Loading train:  54%|█████▍    | 144/266 [01:23<01:09,  1.76it/s]Loading train:  55%|█████▍    | 145/266 [01:24<01:10,  1.71it/s]Loading train:  55%|█████▍    | 146/266 [01:25<01:13,  1.64it/s]Loading train:  55%|█████▌    | 147/266 [01:25<01:14,  1.60it/s]Loading train:  56%|█████▌    | 148/266 [01:26<01:11,  1.65it/s]Loading train:  56%|█████▌    | 149/266 [01:26<01:09,  1.68it/s]Loading train:  56%|█████▋    | 150/266 [01:27<01:07,  1.71it/s]Loading train:  57%|█████▋    | 151/266 [01:27<01:06,  1.73it/s]Loading train:  57%|█████▋    | 152/266 [01:28<01:04,  1.76it/s]Loading train:  58%|█████▊    | 153/266 [01:29<01:04,  1.75it/s]Loading train:  58%|█████▊    | 154/266 [01:29<01:04,  1.75it/s]Loading train:  58%|█████▊    | 155/266 [01:30<01:00,  1.83it/s]Loading train:  59%|█████▊    | 156/266 [01:30<00:57,  1.91it/s]Loading train:  59%|█████▉    | 157/266 [01:31<00:54,  1.99it/s]Loading train:  59%|█████▉    | 158/266 [01:31<00:53,  2.03it/s]Loading train:  60%|█████▉    | 159/266 [01:32<00:52,  2.05it/s]Loading train:  60%|██████    | 160/266 [01:32<00:51,  2.07it/s]Loading train:  61%|██████    | 161/266 [01:32<00:49,  2.11it/s]Loading train:  61%|██████    | 162/266 [01:33<00:48,  2.13it/s]Loading train:  61%|██████▏   | 163/266 [01:33<00:49,  2.09it/s]Loading train:  62%|██████▏   | 164/266 [01:34<00:48,  2.12it/s]Loading train:  62%|██████▏   | 165/266 [01:34<00:47,  2.15it/s]Loading train:  62%|██████▏   | 166/266 [01:35<00:46,  2.17it/s]Loading train:  63%|██████▎   | 167/266 [01:35<00:46,  2.15it/s]Loading train:  63%|██████▎   | 168/266 [01:36<00:45,  2.15it/s]Loading train:  64%|██████▎   | 169/266 [01:36<00:44,  2.18it/s]Loading train:  64%|██████▍   | 170/266 [01:37<00:43,  2.19it/s]Loading train:  64%|██████▍   | 171/266 [01:37<00:43,  2.16it/s]Loading train:  65%|██████▍   | 172/266 [01:38<00:43,  2.15it/s]Loading train:  65%|██████▌   | 173/266 [01:38<00:43,  2.13it/s]Loading train:  65%|██████▌   | 174/266 [01:39<00:42,  2.15it/s]Loading train:  66%|██████▌   | 175/266 [01:39<00:41,  2.17it/s]Loading train:  66%|██████▌   | 176/266 [01:39<00:41,  2.17it/s]Loading train:  67%|██████▋   | 177/266 [01:40<00:41,  2.15it/s]Loading train:  67%|██████▋   | 178/266 [01:40<00:41,  2.13it/s]Loading train:  67%|██████▋   | 179/266 [01:41<00:40,  2.13it/s]Loading train:  68%|██████▊   | 180/266 [01:41<00:40,  2.10it/s]Loading train:  68%|██████▊   | 181/266 [01:42<00:40,  2.08it/s]Loading train:  68%|██████▊   | 182/266 [01:42<00:40,  2.09it/s]Loading train:  69%|██████▉   | 183/266 [01:43<00:39,  2.12it/s]Loading train:  69%|██████▉   | 184/266 [01:43<00:38,  2.11it/s]Loading train:  70%|██████▉   | 185/266 [01:44<00:38,  2.13it/s]Loading train:  70%|██████▉   | 186/266 [01:44<00:37,  2.15it/s]Loading train:  70%|███████   | 187/266 [01:45<00:37,  2.13it/s]Loading train:  71%|███████   | 188/266 [01:45<00:36,  2.13it/s]Loading train:  71%|███████   | 189/266 [01:46<00:35,  2.15it/s]Loading train:  71%|███████▏  | 190/266 [01:46<00:35,  2.15it/s]Loading train:  72%|███████▏  | 191/266 [01:47<00:43,  1.73it/s]Loading train:  72%|███████▏  | 192/266 [01:48<00:46,  1.58it/s]Loading train:  73%|███████▎  | 193/266 [01:48<00:48,  1.50it/s]Loading train:  73%|███████▎  | 194/266 [01:49<00:52,  1.37it/s]Loading train:  73%|███████▎  | 195/266 [01:50<00:47,  1.51it/s]Loading train:  74%|███████▎  | 196/266 [01:50<00:43,  1.61it/s]Loading train:  74%|███████▍  | 197/266 [01:51<00:41,  1.68it/s]Loading train:  74%|███████▍  | 198/266 [01:51<00:38,  1.75it/s]Loading train:  75%|███████▍  | 199/266 [01:52<00:37,  1.78it/s]Loading train:  75%|███████▌  | 200/266 [01:52<00:36,  1.83it/s]Loading train:  76%|███████▌  | 201/266 [01:53<00:36,  1.79it/s]Loading train:  76%|███████▌  | 202/266 [01:54<00:35,  1.79it/s]Loading train:  76%|███████▋  | 203/266 [01:54<00:34,  1.81it/s]Loading train:  77%|███████▋  | 204/266 [01:55<00:33,  1.83it/s]Loading train:  77%|███████▋  | 205/266 [01:55<00:33,  1.82it/s]Loading train:  77%|███████▋  | 206/266 [01:56<00:33,  1.81it/s]Loading train:  78%|███████▊  | 207/266 [01:56<00:32,  1.82it/s]Loading train:  78%|███████▊  | 208/266 [01:57<00:31,  1.84it/s]Loading train:  79%|███████▊  | 209/266 [01:57<00:30,  1.86it/s]Loading train:  79%|███████▉  | 210/266 [01:58<00:29,  1.87it/s]Loading train:  79%|███████▉  | 211/266 [01:58<00:29,  1.88it/s]Loading train:  80%|███████▉  | 212/266 [01:59<00:28,  1.88it/s]Loading train:  80%|████████  | 213/266 [01:59<00:28,  1.89it/s]Loading train:  80%|████████  | 214/266 [02:00<00:27,  1.91it/s]Loading train:  81%|████████  | 215/266 [02:00<00:26,  1.92it/s]Loading train:  81%|████████  | 216/266 [02:01<00:26,  1.91it/s]Loading train:  82%|████████▏ | 217/266 [02:01<00:25,  1.90it/s]Loading train:  82%|████████▏ | 218/266 [02:02<00:24,  1.92it/s]Loading train:  82%|████████▏ | 219/266 [02:03<00:24,  1.91it/s]Loading train:  83%|████████▎ | 220/266 [02:03<00:24,  1.91it/s]Loading train:  83%|████████▎ | 221/266 [02:04<00:23,  1.92it/s]Loading train:  83%|████████▎ | 222/266 [02:04<00:22,  1.92it/s]Loading train:  84%|████████▍ | 223/266 [02:05<00:22,  1.93it/s]Loading train:  84%|████████▍ | 224/266 [02:05<00:21,  1.95it/s]Loading train:  85%|████████▍ | 225/266 [02:06<00:21,  1.93it/s]Loading train:  85%|████████▍ | 226/266 [02:06<00:21,  1.89it/s]Loading train:  85%|████████▌ | 227/266 [02:08<00:37,  1.04it/s]Loading train:  86%|████████▌ | 228/266 [02:11<00:56,  1.50s/it]Loading train:  86%|████████▌ | 229/266 [02:15<01:24,  2.29s/it]Loading train:  86%|████████▋ | 230/266 [02:21<01:58,  3.28s/it]Loading train:  87%|████████▋ | 231/266 [02:27<02:23,  4.10s/it]Loading train:  87%|████████▋ | 232/266 [02:32<02:29,  4.38s/it]Loading train:  88%|████████▊ | 233/266 [02:36<02:28,  4.50s/it]Loading train:  88%|████████▊ | 234/266 [02:40<02:18,  4.33s/it]Loading train:  88%|████████▊ | 235/266 [02:44<02:07,  4.10s/it]Loading train:  89%|████████▊ | 236/266 [02:47<01:55,  3.84s/it]Loading train:  89%|████████▉ | 237/266 [02:50<01:45,  3.63s/it]Loading train:  89%|████████▉ | 238/266 [02:53<01:37,  3.49s/it]Loading train:  90%|████████▉ | 239/266 [02:57<01:31,  3.39s/it]Loading train:  90%|█████████ | 240/266 [03:00<01:25,  3.29s/it]Loading train:  91%|█████████ | 241/266 [03:03<01:21,  3.25s/it]Loading train:  91%|█████████ | 242/266 [03:06<01:16,  3.19s/it]Loading train:  91%|█████████▏| 243/266 [03:09<01:12,  3.15s/it]Loading train:  92%|█████████▏| 244/266 [03:12<01:09,  3.14s/it]Loading train:  92%|█████████▏| 245/266 [03:15<01:05,  3.11s/it]Loading train:  92%|█████████▏| 246/266 [03:18<01:02,  3.11s/it]Loading train:  93%|█████████▎| 247/266 [03:21<00:58,  3.08s/it]Loading train:  93%|█████████▎| 248/266 [03:24<00:55,  3.09s/it]Loading train:  94%|█████████▎| 249/266 [03:32<01:14,  4.38s/it]Loading train:  94%|█████████▍| 250/266 [03:36<01:11,  4.47s/it]Loading train:  94%|█████████▍| 251/266 [03:41<01:08,  4.57s/it]Loading train:  95%|█████████▍| 252/266 [03:46<01:05,  4.65s/it]Loading train:  95%|█████████▌| 253/266 [03:51<01:01,  4.75s/it]Loading train:  95%|█████████▌| 254/266 [03:56<00:57,  4.82s/it]Loading train:  96%|█████████▌| 255/266 [04:01<00:53,  4.86s/it]Loading train:  96%|█████████▌| 256/266 [04:06<00:48,  4.86s/it]Loading train:  97%|█████████▋| 257/266 [04:11<00:43,  4.88s/it]Loading train:  97%|█████████▋| 258/266 [04:16<00:39,  4.90s/it]Loading train:  97%|█████████▋| 259/266 [04:21<00:34,  4.94s/it]Loading train:  98%|█████████▊| 260/266 [04:26<00:29,  4.96s/it]Loading train:  98%|█████████▊| 261/266 [04:31<00:24,  4.97s/it]Loading train:  98%|█████████▊| 262/266 [04:36<00:19,  4.92s/it]Loading train:  99%|█████████▉| 263/266 [04:41<00:14,  4.97s/it]Loading train:  99%|█████████▉| 264/266 [04:46<00:09,  4.93s/it]Loading train: 100%|█████████▉| 265/266 [04:50<00:04,  4.93s/it]Loading train: 100%|██████████| 266/266 [04:56<00:00,  4.98s/it]Loading train: 100%|██████████| 266/266 [04:56<00:00,  1.11s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/266 [00:00<00:04, 56.62it/s]concatenating: train:   5%|▍         | 12/266 [00:00<00:04, 55.78it/s]concatenating: train:   7%|▋         | 18/266 [00:00<00:04, 55.17it/s]concatenating: train:   9%|▉         | 25/266 [00:00<00:04, 55.84it/s]concatenating: train:  12%|█▏        | 31/266 [00:00<00:04, 55.18it/s]concatenating: train:  14%|█▍        | 37/266 [00:00<00:04, 54.74it/s]concatenating: train:  16%|█▌        | 43/266 [00:00<00:04, 54.73it/s]concatenating: train:  18%|█▊        | 49/266 [00:00<00:04, 53.45it/s]concatenating: train:  21%|██        | 55/266 [00:01<00:03, 53.63it/s]concatenating: train:  23%|██▎       | 62/266 [00:01<00:03, 55.83it/s]concatenating: train:  26%|██▌       | 68/266 [00:01<00:03, 56.76it/s]concatenating: train:  28%|██▊       | 74/266 [00:01<00:03, 55.18it/s]concatenating: train:  30%|███       | 80/266 [00:01<00:03, 55.75it/s]concatenating: train:  33%|███▎      | 87/266 [00:01<00:03, 57.68it/s]concatenating: train:  35%|███▍      | 93/266 [00:01<00:03, 57.64it/s]concatenating: train:  38%|███▊      | 100/266 [00:01<00:02, 58.40it/s]concatenating: train:  40%|████      | 107/266 [00:01<00:02, 58.91it/s]concatenating: train:  43%|████▎     | 114/266 [00:02<00:02, 58.64it/s]concatenating: train:  45%|████▌     | 120/266 [00:02<00:02, 58.94it/s]concatenating: train:  48%|████▊     | 127/266 [00:02<00:02, 60.20it/s]concatenating: train:  50%|█████     | 134/266 [00:02<00:02, 61.46it/s]concatenating: train:  53%|█████▎    | 141/266 [00:02<00:02, 60.17it/s]concatenating: train:  56%|█████▌    | 148/266 [00:02<00:02, 58.85it/s]concatenating: train:  58%|█████▊    | 154/266 [00:02<00:01, 57.63it/s]concatenating: train:  60%|██████    | 160/266 [00:02<00:01, 56.81it/s]concatenating: train:  63%|██████▎   | 167/266 [00:02<00:01, 58.38it/s]concatenating: train:  66%|██████▌   | 175/266 [00:03<00:01, 61.96it/s]concatenating: train:  69%|██████▉   | 183/266 [00:03<00:01, 64.64it/s]concatenating: train:  71%|███████▏  | 190/266 [00:03<00:01, 64.81it/s]concatenating: train:  74%|███████▍  | 197/266 [00:03<00:01, 59.75it/s]concatenating: train:  77%|███████▋  | 204/266 [00:03<00:01, 56.74it/s]concatenating: train:  79%|███████▉  | 210/266 [00:03<00:01, 53.94it/s]concatenating: train:  81%|████████  | 216/266 [00:03<00:00, 52.48it/s]concatenating: train:  83%|████████▎ | 222/266 [00:03<00:00, 51.18it/s]concatenating: train:  86%|████████▌ | 228/266 [00:04<00:00, 47.25it/s]concatenating: train:  88%|████████▊ | 233/266 [00:04<00:00, 47.74it/s]concatenating: train:  90%|████████▉ | 239/266 [00:04<00:00, 49.19it/s]concatenating: train:  92%|█████████▏| 245/266 [00:04<00:00, 50.01it/s]concatenating: train:  94%|█████████▍| 251/266 [00:04<00:00, 50.88it/s]concatenating: train:  97%|█████████▋| 257/266 [00:04<00:00, 50.88it/s]concatenating: train:  99%|█████████▉| 263/266 [00:04<00:00, 51.61it/s]concatenating: train: 100%|██████████| 266/266 [00:04<00:00, 55.77it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:05<00:16,  5.36s/it]Loading test:  50%|█████     | 2/4 [00:10<00:10,  5.31s/it]Loading test:  75%|███████▌  | 3/4 [00:15<00:05,  5.34s/it]Loading test: 100%|██████████| 4/4 [00:26<00:00,  6.87s/it]Loading test: 100%|██████████| 4/4 [00:26<00:00,  6.59s/it]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 60.08it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 0  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 0  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 0  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 84, 48, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 84, 48, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 84, 48, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 84, 48, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 84, 48, 40)   14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 84, 48, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 84, 48, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 42, 24, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 42, 24, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 42, 24, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 42, 24, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 42, 24, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 42, 24, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 42, 24, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 42, 24, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 42, 24, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 21, 12, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 21, 12, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 21, 12, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 21, 12, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 21, 12, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 21, 12, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 21, 12, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 21, 12, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 21, 12, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 21, 12, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 42, 24, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 42, 24, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 42, 24, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 42, 24, 80)   320         conv2d_7[0][0]                   2020-01-21 20:05:29.455715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 20:05:29.455852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 20:05:29.455869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 20:05:29.455881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 20:05:29.456259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
activation_7 (Activation)       (None, 42, 24, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 42, 24, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 42, 24, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 42, 24, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 42, 24, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 42, 24, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 84, 48, 40)   44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 84, 48, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 84, 48, 40)   28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 84, 48, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 84, 48, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 84, 48, 40)   14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 84, 48, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 84, 48, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 84, 48, 120)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 84, 48, 120)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 84, 48, 13)   1573        dropout_5[0][0]                  
==================================================================================================
Total params: 889,253
Trainable params: 887,653
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0
------------------------------------------------------------------
class_weights [6.33911086e-02 3.28547315e-02 7.68262708e-02 9.54605046e-03
 2.76284438e-02 7.22820352e-03 8.43740678e-02 1.14189603e-01
 8.96613775e-02 1.36226833e-02 2.90699821e-01 1.89712958e-01
 2.64680678e-04]
Train on 10343 samples, validate on 150 samples
Epoch 1/300
 - 29s - loss: 0.6865 - acc: 0.8979 - mDice: 0.2602 - val_loss: 0.4813 - val_acc: 0.9353 - val_mDice: 0.2222

Epoch 00001: val_mDice improved from -inf to 0.22220, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/300
 - 25s - loss: 0.5606 - acc: 0.9224 - mDice: 0.3953 - val_loss: 0.2975 - val_acc: 0.9395 - val_mDice: 0.2351

Epoch 00002: val_mDice improved from 0.22220 to 0.23510, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 3/300
 - 25s - loss: 0.5342 - acc: 0.9278 - mDice: 0.4238 - val_loss: 0.2612 - val_acc: 0.9429 - val_mDice: 0.2397

Epoch 00003: val_mDice improved from 0.23510 to 0.23974, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/300
 - 24s - loss: 0.5175 - acc: 0.9311 - mDice: 0.4417 - val_loss: 0.1960 - val_acc: 0.9438 - val_mDice: 0.2424

Epoch 00004: val_mDice improved from 0.23974 to 0.24239, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 5/300
 - 25s - loss: 0.5049 - acc: 0.9331 - mDice: 0.4553 - val_loss: 0.2038 - val_acc: 0.9445 - val_mDice: 0.2449

Epoch 00005: val_mDice improved from 0.24239 to 0.24488, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 6/300
 - 25s - loss: 0.4958 - acc: 0.9348 - mDice: 0.4652 - val_loss: 0.1953 - val_acc: 0.9456 - val_mDice: 0.2470

Epoch 00006: val_mDice improved from 0.24488 to 0.24702, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 7/300
 - 25s - loss: 0.4883 - acc: 0.9362 - mDice: 0.4732 - val_loss: 0.1802 - val_acc: 0.9452 - val_mDice: 0.2473

Epoch 00007: val_mDice improved from 0.24702 to 0.24729, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 8/300
 - 25s - loss: 0.4835 - acc: 0.9370 - mDice: 0.4784 - val_loss: 0.1993 - val_acc: 0.9458 - val_mDice: 0.2471

Epoch 00008: val_mDice did not improve from 0.24729
Epoch 9/300
 - 24s - loss: 0.4725 - acc: 0.9375 - mDice: 0.4904 - val_loss: 0.1545 - val_acc: 0.9453 - val_mDice: 0.2663

Epoch 00009: val_mDice improved from 0.24729 to 0.26632, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 10/300
 - 24s - loss: 0.4345 - acc: 0.9385 - mDice: 0.5314 - val_loss: 0.1330 - val_acc: 0.9451 - val_mDice: 0.2800

Epoch 00010: val_mDice improved from 0.26632 to 0.27998, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 11/300
 - 24s - loss: 0.4196 - acc: 0.9394 - mDice: 0.5476 - val_loss: 0.1434 - val_acc: 0.9449 - val_mDice: 0.2821

Epoch 00011: val_mDice improved from 0.27998 to 0.28210, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 12/300
 - 24s - loss: 0.4106 - acc: 0.9400 - mDice: 0.5573 - val_loss: 0.1330 - val_acc: 0.9461 - val_mDice: 0.2852

Epoch 00012: val_mDice improved from 0.28210 to 0.28523, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 13/300
 - 25s - loss: 0.4031 - acc: 0.9405 - mDice: 0.5654 - val_loss: 0.1274 - val_acc: 0.9470 - val_mDice: 0.2883

Epoch 00013: val_mDice improved from 0.28523 to 0.28835, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 14/300
 - 24s - loss: 0.3962 - acc: 0.9412 - mDice: 0.5729 - val_loss: 0.1010 - val_acc: 0.9469 - val_mDice: 0.2895

Epoch 00014: val_mDice improved from 0.28835 to 0.28954, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 15/300
 - 25s - loss: 0.3915 - acc: 0.9417 - mDice: 0.5780 - val_loss: 0.0936 - val_acc: 0.9474 - val_mDice: 0.2915

Epoch 00015: val_mDice improved from 0.28954 to 0.29153, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 16/300
 - 24s - loss: 0.3861 - acc: 0.9423 - mDice: 0.5837 - val_loss: 0.1123 - val_acc: 0.9469 - val_mDice: 0.2910

Epoch 00016: val_mDice did not improve from 0.29153
Epoch 17/300
 - 25s - loss: 0.3811 - acc: 0.9425 - mDice: 0.5891 - val_loss: 0.0858 - val_acc: 0.9477 - val_mDice: 0.2917

Epoch 00017: val_mDice improved from 0.29153 to 0.29169, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 18/300
 - 25s - loss: 0.3777 - acc: 0.9431 - mDice: 0.5929 - val_loss: 0.1527 - val_acc: 0.9474 - val_mDice: 0.2906

Epoch 00018: val_mDice did not improve from 0.29169
Epoch 19/300
 - 24s - loss: 0.3733 - acc: 0.9434 - mDice: 0.5976 - val_loss: 0.1352 - val_acc: 0.9483 - val_mDice: 0.2964

Epoch 00019: val_mDice improved from 0.29169 to 0.29639, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 20/300
 - 25s - loss: 0.3709 - acc: 0.9438 - mDice: 0.6002 - val_loss: 0.0914 - val_acc: 0.9484 - val_mDice: 0.2953

Epoch 00020: val_mDice did not improve from 0.29639
Epoch 21/300
 - 24s - loss: 0.3693 - acc: 0.9440 - mDice: 0.6019 - val_loss: 0.1316 - val_acc: 0.9481 - val_mDice: 0.2937

Epoch 00021: val_mDice did not improve from 0.29639
Epoch 22/300
 - 25s - loss: 0.3662 - acc: 0.9444 - mDice: 0.6053 - val_loss: 0.0998 - val_acc: 0.9478 - val_mDice: 0.2936

Epoch 00022: val_mDice did not improve from 0.29639
Epoch 23/300
 - 25s - loss: 0.3657 - acc: 0.9446 - mDice: 0.6058 - val_loss: 0.0886 - val_acc: 0.9487 - val_mDice: 0.2993

Epoch 00023: val_mDice improved from 0.29639 to 0.29929, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 24/300
 - 25s - loss: 0.3636 - acc: 0.9448 - mDice: 0.6080 - val_loss: 0.1188 - val_acc: 0.9486 - val_mDice: 0.2959

Epoch 00024: val_mDice did not improve from 0.29929
Epoch 25/300
 - 25s - loss: 0.3584 - acc: 0.9451 - mDice: 0.6136 - val_loss: 0.1136 - val_acc: 0.9481 - val_mDice: 0.2950

Epoch 00025: val_mDice did not improve from 0.29929
Epoch 26/300
 - 24s - loss: 0.3579 - acc: 0.9455 - mDice: 0.6142 - val_loss: 0.1217 - val_acc: 0.9489 - val_mDice: 0.2942

Epoch 00026: val_mDice did not improve from 0.29929
Epoch 27/300
 - 25s - loss: 0.3563 - acc: 0.9455 - mDice: 0.6160 - val_loss: 0.1479 - val_acc: 0.9482 - val_mDice: 0.2972

Epoch 00027: val_mDice did not improve from 0.29929
Epoch 28/300
 - 24s - loss: 0.3526 - acc: 0.9459 - mDice: 0.6200 - val_loss: 0.1436 - val_acc: 0.9486 - val_mDice: 0.2955

Epoch 00028: val_mDice did not improve from 0.29929
Epoch 29/300
 - 24s - loss: 0.3528 - acc: 0.9461 - mDice: 0.6197 - val_loss: 0.1186 - val_acc: 0.9481 - val_mDice: 0.2903

Epoch 00029: val_mDice did not improve from 0.29929
Epoch 30/300
 - 25s - loss: 0.3498 - acc: 0.9463 - mDice: 0.6230 - val_loss: 0.1069 - val_acc: 0.9492 - val_mDice: 0.2941

Epoch 00030: val_mDice did not improve from 0.29929
Epoch 31/300
 - 25s - loss: 0.3466 - acc: 0.9465 - mDice: 0.6264 - val_loss: 0.1233 - val_acc: 0.9490 - val_mDice: 0.2932

Epoch 00031: val_mDice did not improve from 0.29929
Epoch 32/300
 - 24s - loss: 0.3475 - acc: 0.9467 - mDice: 0.6254 - val_loss: 0.1202 - val_acc: 0.9487 - val_mDice: 0.2872

Epoch 00032: val_mDice did not improve from 0.29929
Epoch 33/300
 - 24s - loss: 0.3460 - acc: 0.9468 - mDice: 0.6270 - val_loss: 0.1195 - val_acc: 0.9491 - val_mDice: 0.2927

Epoch 00033: val_mDice did not improve from 0.29929
Epoch 34/300
 - 24s - loss: 0.3436 - acc: 0.9470 - mDice: 0.6296 - val_loss: 0.1302 - val_acc: 0.9491 - val_mDice: 0.2943

Epoch 00034: val_mDice did not improve from 0.29929
Epoch 35/300
 - 24s - loss: 0.3384 - acc: 0.9473 - mDice: 0.6352 - val_loss: 0.1350 - val_acc: 0.9483 - val_mDice: 0.2923

Epoch 00035: val_mDice did not improve from 0.29929
Epoch 36/300
 - 24s - loss: 0.3401 - acc: 0.9473 - mDice: 0.6334 - val_loss: 0.4046 - val_acc: 0.9480 - val_mDice: 0.3029

Epoch 00036: val_mDice improved from 0.29929 to 0.30292, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 37/300
 - 25s - loss: 0.3200 - acc: 0.9470 - mDice: 0.6551 - val_loss: 0.2631 - val_acc: 0.9456 - val_mDice: 0.3034

Epoch 00037: val_mDice improved from 0.30292 to 0.30340, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 38/300
 - 25s - loss: 0.3089 - acc: 0.9477 - mDice: 0.6672 - val_loss: 0.1301 - val_acc: 0.9477 - val_mDice: 0.3060

Epoch 00038: val_mDice improved from 0.30340 to 0.30599, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 39/300
 - 24s - loss: 0.3081 - acc: 0.9479 - mDice: 0.6680 - val_loss: 0.1090 - val_acc: 0.9471 - val_mDice: 0.3038

Epoch 00039: val_mDice did not improve from 0.30599
Epoch 40/300
 - 24s - loss: 0.2996 - acc: 0.9482 - mDice: 0.6772 - val_loss: 0.0531 - val_acc: 0.9471 - val_mDice: 0.3083

Epoch 00040: val_mDice improved from 0.30599 to 0.30826, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 41/300
 - 25s - loss: 0.2995 - acc: 0.9485 - mDice: 0.6774 - val_loss: 0.1042 - val_acc: 0.9478 - val_mDice: 0.3110

Epoch 00041: val_mDice improved from 0.30826 to 0.31102, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 42/300
 - 24s - loss: 0.2981 - acc: 0.9486 - mDice: 0.6789 - val_loss: 0.0349 - val_acc: 0.9486 - val_mDice: 0.3133

Epoch 00042: val_mDice improved from 0.31102 to 0.31331, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 43/300
 - 24s - loss: 0.2961 - acc: 0.9489 - mDice: 0.6810 - val_loss: 0.0456 - val_acc: 0.9474 - val_mDice: 0.3078

Epoch 00043: val_mDice did not improve from 0.31331
Epoch 44/300
 - 24s - loss: 0.2946 - acc: 0.9491 - mDice: 0.6826 - val_loss: 0.0351 - val_acc: 0.9480 - val_mDice: 0.3088

Epoch 00044: val_mDice did not improve from 0.31331
Epoch 45/300
 - 25s - loss: 0.2927 - acc: 0.9494 - mDice: 0.6847 - val_loss: 0.0326 - val_acc: 0.9483 - val_mDice: 0.3133

Epoch 00045: val_mDice did not improve from 0.31331
Epoch 46/300
 - 24s - loss: 0.2901 - acc: 0.9495 - mDice: 0.6875 - val_loss: 0.0176 - val_acc: 0.9483 - val_mDice: 0.3092

Epoch 00046: val_mDice did not improve from 0.31331
Epoch 47/300
 - 24s - loss: 0.2885 - acc: 0.9496 - mDice: 0.6892 - val_loss: 0.0038 - val_acc: 0.9481 - val_mDice: 0.3099

Epoch 00047: val_mDice did not improve from 0.31331
Epoch 48/300
 - 24s - loss: 0.2860 - acc: 0.9498 - mDice: 0.6919 - val_loss: 0.0050 - val_acc: 0.9480 - val_mDice: 0.3139

Epoch 00048: val_mDice improved from 0.31331 to 0.31392, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 49/300
 - 24s - loss: 0.2872 - acc: 0.9500 - mDice: 0.6906 - val_loss: 0.0369 - val_acc: 0.9467 - val_mDice: 0.3097

Epoch 00049: val_mDice did not improve from 0.31392
Epoch 50/300
 - 25s - loss: 0.2850 - acc: 0.9500 - mDice: 0.6930 - val_loss: 0.0137 - val_acc: 0.9485 - val_mDice: 0.3109

Epoch 00050: val_mDice did not improve from 0.31392
Epoch 51/300
 - 24s - loss: 0.2863 - acc: 0.9502 - mDice: 0.6915 - val_loss: 0.0192 - val_acc: 0.9485 - val_mDice: 0.3094

Epoch 00051: val_mDice did not improve from 0.31392
Epoch 52/300
 - 25s - loss: 0.2792 - acc: 0.9504 - mDice: 0.6993 - val_loss: 0.0378 - val_acc: 0.9484 - val_mDice: 0.3088

Epoch 00052: val_mDice did not improve from 0.31392
Epoch 53/300
 - 25s - loss: 0.2812 - acc: 0.9506 - mDice: 0.6971 - val_loss: 0.0115 - val_acc: 0.9488 - val_mDice: 0.3078

Epoch 00053: val_mDice did not improve from 0.31392
Epoch 54/300
 - 25s - loss: 0.2788 - acc: 0.9507 - mDice: 0.6997 - val_loss: -4.1856e-04 - val_acc: 0.9489 - val_mDice: 0.3127

Epoch 00054: val_mDice did not improve from 0.31392
Epoch 55/300
 - 24s - loss: 0.2814 - acc: 0.9508 - mDice: 0.6969 - val_loss: 0.0059 - val_acc: 0.9484 - val_mDice: 0.3097

Epoch 00055: val_mDice did not improve from 0.31392
Epoch 56/300
 - 24s - loss: 0.2772 - acc: 0.9509 - mDice: 0.7014 - val_loss: 0.0106 - val_acc: 0.9479 - val_mDice: 0.3080

Epoch 00056: val_mDice did not improve from 0.31392
Epoch 57/300
 - 24s - loss: 0.2741 - acc: 0.9510 - mDice: 0.7047 - val_loss: 0.0141 - val_acc: 0.9484 - val_mDice: 0.3102

Epoch 00057: val_mDice did not improve from 0.31392

Epoch 00057: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 58/300
 - 25s - loss: 0.2744 - acc: 0.9512 - mDice: 0.7044 - val_loss: 0.0173 - val_acc: 0.9493 - val_mDice: 0.3129

Epoch 00058: val_mDice did not improve from 0.31392
Epoch 59/300
 - 24s - loss: 0.2716 - acc: 0.9515 - mDice: 0.7075 - val_loss: 0.0361 - val_acc: 0.9487 - val_mDice: 0.3111

Epoch 00059: val_mDice did not improve from 0.31392
Epoch 60/300
 - 25s - loss: 0.2702 - acc: 0.9515 - mDice: 0.7090 - val_loss: 0.0463 - val_acc: 0.9485 - val_mDice: 0.3097

Epoch 00060: val_mDice did not improve from 0.31392
Epoch 61/300
 - 24s - loss: 0.2692 - acc: 0.9516 - mDice: 0.7101 - val_loss: 0.0263 - val_acc: 0.9487 - val_mDice: 0.3087

Epoch 00061: val_mDice did not improve from 0.31392
Epoch 62/300
 - 24s - loss: 0.2700 - acc: 0.9517 - mDice: 0.7092 - val_loss: 0.0499 - val_acc: 0.9483 - val_mDice: 0.3107

Epoch 00062: val_mDice did not improve from 0.31392
Epoch 63/300
 - 25s - loss: 0.2700 - acc: 0.9517 - mDice: 0.7092 - val_loss: 0.0072 - val_acc: 0.9489 - val_mDice: 0.3117

Epoch 00063: val_mDice did not improve from 0.31392
Epoch 64/300
 - 25s - loss: 0.2676 - acc: 0.9518 - mDice: 0.7118 - val_loss: 0.0272 - val_acc: 0.9488 - val_mDice: 0.3123

Epoch 00064: val_mDice did not improve from 0.31392
Epoch 65/300
 - 25s - loss: 0.2673 - acc: 0.9518 - mDice: 0.7121 - val_loss: 0.0171 - val_acc: 0.9490 - val_mDice: 0.3107

Epoch 00065: val_mDice did not improve from 0.31392
Epoch 66/300
 - 25s - loss: 0.2723 - acc: 0.9519 - mDice: 0.7067 - val_loss: 0.0206 - val_acc: 0.9487 - val_mDice: 0.3097

Epoch 00066: val_mDice did not improve from 0.31392
Epoch 67/300
 - 25s - loss: 0.2667 - acc: 0.9519 - mDice: 0.7128 - val_loss: 0.0425 - val_acc: 0.9488 - val_mDice: 0.3101

Epoch 00067: val_mDice did not improve from 0.31392
Epoch 68/300
 - 24s - loss: 0.2667 - acc: 0.9519 - mDice: 0.7128 - val_loss: 0.0586 - val_acc: 0.9484 - val_mDice: 0.3107

Epoch 00068: val_mDice did not improve from 0.31392
Epoch 69/300
 - 25s - loss: 0.2670 - acc: 0.9520 - mDice: 0.7124 - val_loss: 0.0207 - val_acc: 0.9497 - val_mDice: 0.3114

Epoch 00069: val_mDice did not improve from 0.31392
Epoch 70/300
 - 24s - loss: 0.2685 - acc: 0.9521 - mDice: 0.7108 - val_loss: 0.0366 - val_acc: 0.9485 - val_mDice: 0.3091

Epoch 00070: val_mDice did not improve from 0.31392
Epoch 71/300
 - 24s - loss: 0.2673 - acc: 0.9521 - mDice: 0.7121 - val_loss: 0.0468 - val_acc: 0.9482 - val_mDice: 0.3096

Epoch 00071: val_mDice did not improve from 0.31392
Epoch 72/300
 - 24s - loss: 0.2654 - acc: 0.9522 - mDice: 0.7142 - val_loss: 0.0511 - val_acc: 0.9480 - val_mDice: 0.3080

Epoch 00072: val_mDice did not improve from 0.31392

Epoch 00072: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 73/300
 - 24s - loss: 0.2654 - acc: 0.9522 - mDice: 0.7142 - val_loss: 0.0664 - val_acc: 0.9484 - val_mDice: 0.3078

Epoch 00073: val_mDice did not improve from 0.31392
Epoch 74/300
 - 24s - loss: 0.2620 - acc: 0.9524 - mDice: 0.7178 - val_loss: 0.0208 - val_acc: 0.9488 - val_mDice: 0.3126

Epoch 00074: val_mDice did not improve from 0.31392
Epoch 75/300
 - 24s - loss: 0.2636 - acc: 0.9523 - mDice: 0.7161 - val_loss: 0.0053 - val_acc: 0.9485 - val_mDice: 0.3096

Epoch 00075: val_mDice did not improve from 0.31392
Epoch 76/300
 - 25s - loss: 0.2630 - acc: 0.9524 - mDice: 0.7167 - val_loss: 0.0259 - val_acc: 0.9486 - val_mDice: 0.3111

Epoch 00076: val_mDice did not improve from 0.31392
Epoch 77/300
 - 24s - loss: 0.2633 - acc: 0.9524 - mDice: 0.7165 - val_loss: 0.0210 - val_acc: 0.9479 - val_mDice: 0.3084

Epoch 00077: val_mDice did not improve from 0.31392
Epoch 78/300
 - 25s - loss: 0.2637 - acc: 0.9525 - mDice: 0.7160 - val_loss: 0.0122 - val_acc: 0.9483 - val_mDice: 0.3102

Epoch 00078: val_mDice did not improve from 0.31392
Epoch 79/300
 - 25s - loss: 0.2641 - acc: 0.9525 - mDice: 0.7155 - val_loss: 0.0413 - val_acc: 0.9484 - val_mDice: 0.3076

Epoch 00079: val_mDice did not improve from 0.31392
Epoch 80/300
 - 26s - loss: 0.2622 - acc: 0.9526 - mDice: 0.7177 - val_loss: -6.4093e-04 - val_acc: 0.9482 - val_mDice: 0.3069

Epoch 00080: val_mDice did not improve from 0.31392
Epoch 81/300
 - 26s - loss: 0.2630 - acc: 0.9524 - mDice: 0.7167 - val_loss: 0.0408 - val_acc: 0.9479 - val_mDice: 0.3077

Epoch 00081: val_mDice did not improve from 0.31392
Epoch 82/300
 - 25s - loss: 0.2653 - acc: 0.9526 - mDice: 0.7143 - val_loss: 0.0255 - val_acc: 0.9482 - val_mDice: 0.3059

Epoch 00082: val_mDice did not improve from 0.31392
Epoch 83/300
 - 25s - loss: 0.2606 - acc: 0.9526 - mDice: 0.7194 - val_loss: 0.0255 - val_acc: 0.9485 - val_mDice: 0.3091

Epoch 00083: val_mDice did not improve from 0.31392
Epoch 84/300
 - 25s - loss: 0.2633 - acc: 0.9526 - mDice: 0.7164 - val_loss: 0.0219 - val_acc: 0.9485 - val_mDice: 0.3100

Epoch 00084: val_mDice did not improve from 0.31392
Epoch 85/300
 - 25s - loss: 0.2629 - acc: 0.9526 - mDice: 0.7168 - val_loss: 0.0401 - val_acc: 0.9483 - val_mDice: 0.3089

Epoch 00085: val_mDice did not improve from 0.31392
Epoch 86/300
 - 25s - loss: 0.2592 - acc: 0.9527 - mDice: 0.7208 - val_loss: 0.0421 - val_acc: 0.9483 - val_mDice: 0.3088

Epoch 00086: val_mDice did not improve from 0.31392
Epoch 87/300
 - 24s - loss: 0.2599 - acc: 0.9527 - mDice: 0.7201 - val_loss: 0.0265 - val_acc: 0.9487 - val_mDice: 0.3108

Epoch 00087: val_mDice did not improve from 0.31392

Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 88/300
 - 24s - loss: 0.2614 - acc: 0.9527 - mDice: 0.7185 - val_loss: 0.0354 - val_acc: 0.9488 - val_mDice: 0.3094

Epoch 00088: val_mDice did not improve from 0.31392
Restoring model weights from the end of the best epoch
Epoch 00088: early stopping
{'val_loss': [0.48126808802286786, 0.2975075483322144, 0.2612152730425199, 0.19603825956583024, 0.20377586099008718, 0.19529012654190106, 0.18023509920264283, 0.19928785699109236, 0.15450887034336727, 0.1329795115161687, 0.14342403337359427, 0.13301518112421035, 0.12737532719038427, 0.1009809916994224, 0.09358579951028029, 0.1122858751565218, 0.08578365532060464, 0.15272900379883747, 0.13522772441307704, 0.09136319210131963, 0.1315765179693699, 0.09979979329509661, 0.08862742260098458, 0.11876698918640613, 0.11356328632682562, 0.12172259603006144, 0.14789708482664235, 0.14358988973932962, 0.1185661505907774, 0.10694785613256196, 0.12329209049542746, 0.12022330143954604, 0.1194843349357446, 0.130150730907917, 0.1350360938037435, 0.40455453197161356, 0.2630740883449713, 0.13012503708402315, 0.10899817521373431, 0.053104143341382345, 0.10423802013198534, 0.034882523119449615, 0.04557083398103714, 0.03513435175021489, 0.032566815117994946, 0.017612557982405026, 0.0038380641490221024, 0.004991760849952698, 0.036905183394749956, 0.013732846577962239, 0.019201171398162842, 0.037840305765469866, 0.011489460368951161, -0.0004185567299524943, 0.005870884905258815, 0.010567124684651692, 0.014126464972893398, 0.017272625863552094, 0.03613759378592173, 0.04632102847099304, 0.02628877411286036, 0.04988443826635679, 0.00722959836324056, 0.027219110727310182, 0.017141891022523243, 0.02061819831530253, 0.04248284747203191, 0.05858640211323897, 0.020675606032212576, 0.03663671612739563, 0.04675278216600418, 0.05110566765069961, 0.06636760830879211, 0.020846193532148997, 0.005344141771396001, 0.025927875439325967, 0.02103546212116877, 0.012205715477466583, 0.04134201854467392, -0.0006409262617429098, 0.04081864058971405, 0.02548361470301946, 0.025469747185707093, 0.02192749405900637, 0.04013787011305491, 0.04208978861570358, 0.02649238022665183, 0.03538505663940062], 'val_acc': [0.9353124936421712, 0.9395271182060242, 0.9428951740264893, 0.9437615791956584, 0.9445469617843628, 0.9455588658650717, 0.9452430526415507, 0.9457903464635213, 0.9453372995058695, 0.9451140880584716, 0.9449024558067322, 0.9461210290590922, 0.9469956994056702, 0.9468667348225911, 0.9474107106526692, 0.9469477534294128, 0.9477232098579407, 0.9474074165026347, 0.9482969562212626, 0.9484441161155701, 0.9480952342351278, 0.9477595965067546, 0.9487499992052714, 0.948645826180776, 0.9480985403060913, 0.9489021142323811, 0.9481696327527364, 0.9486177245775859, 0.9480803569157918, 0.9491848468780517, 0.9489632924397786, 0.9487334609031677, 0.9490707715352377, 0.94907572666804, 0.9482804258664449, 0.9480258027712504, 0.9455522497495016, 0.9477380911509196, 0.9470949053764344, 0.9471478263537089, 0.9478389581044515, 0.948624336719513, 0.9473892172177633, 0.9479778448740641, 0.9483465552330017, 0.9483283758163452, 0.9480968872706096, 0.9480489412943522, 0.9467113137245178, 0.9485218207041423, 0.9484771847724914, 0.9483829339345297, 0.9487516482671102, 0.9489451050758362, 0.9484391570091247, 0.9478902141253154, 0.948373019695282, 0.9493435819943746, 0.9487235387166341, 0.9485350529352824, 0.9487334609031677, 0.9482936541239421, 0.9488574822743734, 0.948824405670166, 0.9489880998929342, 0.9487367709477742, 0.9487582683563233, 0.948394509156545, 0.949699068069458, 0.9484920620918273, 0.9481663386027018, 0.9480406721433003, 0.9483763217926026, 0.9487781047821044, 0.9484556913375854, 0.9485515832901001, 0.9478720307350159, 0.9483052213986715, 0.948373015721639, 0.9482175906499227, 0.9479464332262675, 0.9481994112332662, 0.9484507282574971, 0.9484573364257812, 0.9482853889465332, 0.9483167926470438, 0.9487483421961467, 0.948759913444519], 'val_mDice': [0.22220184355974198, 0.23509508992234865, 0.2397421680390835, 0.24238777334491413, 0.24487570424874625, 0.24702435334523518, 0.2472939151028792, 0.24713396926720937, 0.26632213195165, 0.27997826834519707, 0.28209697802861533, 0.2852308561404546, 0.28834830621878305, 0.28953521996736525, 0.29152844548225404, 0.29097529749075574, 0.29168915897607806, 0.29056283036867775, 0.29638721644878385, 0.295336551964283, 0.2936861366033554, 0.29355661273002626, 0.29929434657096865, 0.2958942944804827, 0.29499935656785964, 0.2942012722293536, 0.29716430058081944, 0.2954571271936099, 0.29034889886776605, 0.2941135048866272, 0.29322596540053686, 0.28717567523320514, 0.29272301693757374, 0.2943212742606799, 0.29227430919806163, 0.3029239818453789, 0.3033953706423442, 0.30599203606446584, 0.30380383084217705, 0.30826234420140586, 0.3110191096862157, 0.3133083845178286, 0.307767960925897, 0.3087654327352842, 0.313250562051932, 0.3092113857467969, 0.3098578398426374, 0.31392340660095214, 0.30965432673692705, 0.31090160757303237, 0.3094100167353948, 0.3087799698114395, 0.30778136948744456, 0.3127338627974192, 0.30966512660185497, 0.3079594522714615, 0.31016047298908234, 0.31286067167917886, 0.31113035380840304, 0.309712590277195, 0.30873224437236785, 0.3107413470745087, 0.31165868292252225, 0.31233446250359215, 0.31068557848532996, 0.30974675019582115, 0.3101344888408979, 0.3106781547268232, 0.3113887955745061, 0.30914079397916794, 0.309566393494606, 0.3080479035774867, 0.30779083967208865, 0.3126456504066785, 0.30956243773301445, 0.31106158196926115, 0.30839588890473046, 0.31019483705361683, 0.3075590451558431, 0.30689799835284554, 0.3076743260025978, 0.30585249116023383, 0.30905913313229877, 0.30995580504337944, 0.3088778550426165, 0.30878274341424305, 0.3108463774124781, 0.3094004680713018], 'loss': [0.6864550520531688, 0.5606141762681364, 0.5341767189950601, 0.5175140624890922, 0.5049196679022998, 0.49577300016992754, 0.488337271808022, 0.483496543460504, 0.4724736074080242, 0.43450571529918053, 0.4195845102498555, 0.41059831306042965, 0.40313205153202397, 0.3961646921985283, 0.3914630222594866, 0.38612621160513844, 0.38113456476959884, 0.3776783921997978, 0.3733000134108987, 0.37090098834540297, 0.36933690600834096, 0.36616562105615147, 0.3656886689127915, 0.3636221354927233, 0.3584308485845112, 0.35793898306282046, 0.3562865954331523, 0.3525687318803447, 0.3527638877451241, 0.3497813409063787, 0.34663614576528834, 0.3474979370322435, 0.34603883293349724, 0.3436199057457192, 0.3384383895935893, 0.3400668381757182, 0.3200433276486353, 0.30893353681482266, 0.30812246821886285, 0.2996322602843289, 0.2994904580279244, 0.29807598868272045, 0.2961012154146215, 0.2946286787611482, 0.29272419452252313, 0.29007843430402097, 0.2885451202269024, 0.2860156749544897, 0.28721050750548843, 0.2849853923637033, 0.2863471020435579, 0.27916413587100686, 0.28124210794278826, 0.2787730312665519, 0.28138569899549526, 0.2771998523749547, 0.27414502208218816, 0.2744194326318464, 0.2716098486322574, 0.27015341777849, 0.2692052438361011, 0.2700126732149496, 0.26997837934574237, 0.2675895709589442, 0.26732414661771864, 0.27226862502976334, 0.2666814114679097, 0.26668528573524325, 0.2670234742949966, 0.2685489769854652, 0.26732245476612626, 0.26538768096610055, 0.2653850675130575, 0.2620165095267738, 0.263582431228712, 0.263008283756808, 0.2632748001746464, 0.2636945337607461, 0.2641450577561127, 0.2621601391009202, 0.263012268865845, 0.2652925329790186, 0.2605653689843041, 0.2633410796003001, 0.26289668519607795, 0.2592495444372914, 0.25989923863107595, 0.261393020346791], 'acc': [0.8979475945911428, 0.9223514176493836, 0.9278133284373681, 0.9310676045474492, 0.933146641434916, 0.9347590450667487, 0.9361881258685251, 0.9369953114751528, 0.9375418916518967, 0.938460795236005, 0.9394344183735468, 0.9399891276084327, 0.9405490389578373, 0.9412250579135348, 0.9416573781991597, 0.9423030624015758, 0.9424742498746361, 0.9431400539323416, 0.9434387132829432, 0.9437827172747758, 0.9440126047827779, 0.9443721948734487, 0.944598462046155, 0.9447968182533354, 0.9451278018027063, 0.9454522620291195, 0.945547291305394, 0.9459284637694224, 0.9460677094977263, 0.9463157013388582, 0.9465161190495827, 0.9467363189212545, 0.9467996720371931, 0.9470204468577003, 0.9472677919166894, 0.9473340219897204, 0.9470161068869388, 0.9476726545790092, 0.9479499737264155, 0.9482194977469132, 0.9484586186903675, 0.9486440481985605, 0.9489176270925571, 0.9491167247128258, 0.9493654365351915, 0.9495444639985616, 0.9496016549404822, 0.9497964843887201, 0.9499539073380536, 0.9500425103311668, 0.9502137448056275, 0.9504034445143571, 0.9505723296679692, 0.9507456982803105, 0.9507933918850964, 0.9508802683720169, 0.9510498729882106, 0.9511867453705511, 0.9514892169807538, 0.9514683316469216, 0.9515983943412258, 0.9516542174536521, 0.9517204481663538, 0.951840104430231, 0.9517803227810406, 0.9518745381539603, 0.9518560250538709, 0.9519297135794842, 0.9520029687252172, 0.9520572587137445, 0.9520865856454368, 0.9521960497225183, 0.9522399541938211, 0.9524028684407614, 0.9523230423137112, 0.9524262967889272, 0.9523810727170067, 0.9525180408309878, 0.952544201962505, 0.9525583263039646, 0.95244581597024, 0.9525706261082335, 0.9525976750740723, 0.9525621384944121, 0.9526120386005635, 0.9526887721726319, 0.9526927767772226, 0.9526978608234863], 'mDice': [0.26016167123139017, 0.39533170966360726, 0.4237989590117978, 0.44174604878127777, 0.4553282608809444, 0.4651858331524841, 0.47320780360893483, 0.4784328304947915, 0.49036157563405103, 0.531444521461132, 0.5475771763385132, 0.5573022486934646, 0.5653763564529888, 0.5728960952969246, 0.5779732578089812, 0.5837326080491599, 0.5891375821499786, 0.5928566862982915, 0.5975914947630231, 0.6001747423152819, 0.6018661804094566, 0.6052817510709096, 0.6058012927470603, 0.6080300344940681, 0.6136389059133206, 0.6141577015085146, 0.6159511102873295, 0.6199643854348973, 0.6197415654627921, 0.6229639916007136, 0.6263602823408017, 0.6254225774704805, 0.6270030086620004, 0.6296082923044195, 0.6352137191653148, 0.6334435278334586, 0.6551259804524594, 0.6671505741451599, 0.668034880494721, 0.6772230683310769, 0.6773776914929786, 0.6789038186952476, 0.6810286714308476, 0.6826135331151979, 0.6846695008008655, 0.6875256020246202, 0.6891828134524849, 0.6919150604254778, 0.6906173615786182, 0.6930253124209399, 0.6915463915844438, 0.6993189458112634, 0.697068228679435, 0.6997286525914139, 0.696896918716611, 0.7014275629227896, 0.7047308854492931, 0.7044278782300121, 0.7074613395372765, 0.7090370401842364, 0.7100588845167056, 0.7091822961157483, 0.7092134386983933, 0.7118011686691956, 0.7120924343583801, 0.7067339607551989, 0.7127894216898641, 0.712780586123916, 0.7124093047749381, 0.7107509936664456, 0.7120815692983627, 0.714175028622548, 0.7141708455613166, 0.7178128712685325, 0.7161259356128701, 0.7167433859197128, 0.7164575494743, 0.7159966057249117, 0.7155032152308033, 0.7176519104466959, 0.7167354817145177, 0.7142567085099499, 0.7193788828672759, 0.7163741746579703, 0.7168491011135908, 0.7208026502806203, 0.7200955142369068, 0.7184820184276123], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 1.25e-05]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]predicting test subjects:  50%|█████     | 2/4 [00:02<00:02,  1.29s/it]predicting test subjects:  75%|███████▌  | 3/4 [00:03<00:01,  1.14s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.11s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:35,  2.76it/s]Loading train:   1%|          | 2/266 [00:00<01:32,  2.86it/s]Loading train:   1%|          | 3/266 [00:00<01:27,  3.00it/s]Loading train:   2%|▏         | 4/266 [00:01<01:23,  3.14it/s]Loading train:   2%|▏         | 5/266 [00:01<01:24,  3.10it/s]Loading train:   2%|▏         | 6/266 [00:01<01:24,  3.07it/s]Loading train:   3%|▎         | 7/266 [00:02<01:23,  3.10it/s]Loading train:   3%|▎         | 8/266 [00:02<01:23,  3.09it/s]Loading train:   3%|▎         | 9/266 [00:02<01:23,  3.08it/s]Loading train:   4%|▍         | 10/266 [00:03<01:24,  3.04it/s]Loading train:   4%|▍         | 11/266 [00:03<01:23,  3.04it/s]Loading train:   5%|▍         | 12/266 [00:03<01:23,  3.03it/s]Loading train:   5%|▍         | 13/266 [00:04<01:23,  3.03it/s]Loading train:   5%|▌         | 14/266 [00:04<01:22,  3.05it/s]Loading train:   6%|▌         | 15/266 [00:04<01:21,  3.09it/s]Loading train:   6%|▌         | 16/266 [00:05<01:21,  3.08it/s]Loading train:   6%|▋         | 17/266 [00:05<01:20,  3.08it/s]Loading train:   7%|▋         | 18/266 [00:05<01:20,  3.09it/s]Loading train:   7%|▋         | 19/266 [00:06<01:19,  3.09it/s]Loading train:   8%|▊         | 20/266 [00:06<01:18,  3.13it/s]Loading train:   8%|▊         | 21/266 [00:06<01:17,  3.15it/s]Loading train:   8%|▊         | 22/266 [00:07<01:18,  3.11it/s]Loading train:   9%|▊         | 23/266 [00:07<01:18,  3.11it/s]Loading train:   9%|▉         | 24/266 [00:07<01:15,  3.19it/s]Loading train:   9%|▉         | 25/266 [00:08<01:14,  3.25it/s]Loading train:  10%|▉         | 26/266 [00:08<01:12,  3.29it/s]Loading train:  10%|█         | 27/266 [00:08<01:12,  3.32it/s]Loading train:  11%|█         | 28/266 [00:08<01:11,  3.34it/s]Loading train:  11%|█         | 29/266 [00:09<01:11,  3.31it/s]Loading train:  11%|█▏        | 30/266 [00:09<01:10,  3.33it/s]Loading train:  12%|█▏        | 31/266 [00:09<01:10,  3.33it/s]Loading train:  12%|█▏        | 32/266 [00:10<01:10,  3.31it/s]Loading train:  12%|█▏        | 33/266 [00:10<01:10,  3.30it/s]Loading train:  13%|█▎        | 34/266 [00:10<01:10,  3.30it/s]Loading train:  13%|█▎        | 35/266 [00:11<01:11,  3.25it/s]Loading train:  14%|█▎        | 36/266 [00:11<01:11,  3.22it/s]Loading train:  14%|█▍        | 37/266 [00:11<01:11,  3.22it/s]Loading train:  14%|█▍        | 38/266 [00:11<01:10,  3.25it/s]Loading train:  15%|█▍        | 39/266 [00:12<01:09,  3.26it/s]Loading train:  15%|█▌        | 40/266 [00:12<01:09,  3.27it/s]Loading train:  15%|█▌        | 41/266 [00:12<01:08,  3.28it/s]Loading train:  16%|█▌        | 42/266 [00:13<01:06,  3.36it/s]Loading train:  16%|█▌        | 43/266 [00:13<01:05,  3.42it/s]Loading train:  17%|█▋        | 44/266 [00:13<01:04,  3.46it/s]Loading train:  17%|█▋        | 45/266 [00:14<01:03,  3.50it/s]Loading train:  17%|█▋        | 46/266 [00:14<01:02,  3.52it/s]Loading train:  18%|█▊        | 47/266 [00:14<01:01,  3.54it/s]Loading train:  18%|█▊        | 48/266 [00:14<01:01,  3.56it/s]Loading train:  18%|█▊        | 49/266 [00:15<01:00,  3.58it/s]Loading train:  19%|█▉        | 50/266 [00:15<01:00,  3.58it/s]Loading train:  19%|█▉        | 51/266 [00:15<01:00,  3.56it/s]Loading train:  20%|█▉        | 52/266 [00:15<00:59,  3.57it/s]Loading train:  20%|█▉        | 53/266 [00:16<00:59,  3.60it/s]Loading train:  20%|██        | 54/266 [00:16<00:58,  3.60it/s]Loading train:  21%|██        | 55/266 [00:16<00:59,  3.55it/s]Loading train:  21%|██        | 56/266 [00:17<00:59,  3.55it/s]Loading train:  21%|██▏       | 57/266 [00:17<00:58,  3.57it/s]Loading train:  22%|██▏       | 58/266 [00:17<00:57,  3.61it/s]Loading train:  22%|██▏       | 59/266 [00:17<00:57,  3.61it/s]Loading train:  23%|██▎       | 60/266 [00:18<00:56,  3.66it/s]Loading train:  23%|██▎       | 61/266 [00:18<00:55,  3.71it/s]Loading train:  23%|██▎       | 62/266 [00:18<00:54,  3.73it/s]Loading train:  24%|██▎       | 63/266 [00:18<00:54,  3.73it/s]Loading train:  24%|██▍       | 64/266 [00:19<00:54,  3.74it/s]Loading train:  24%|██▍       | 65/266 [00:19<00:53,  3.74it/s]Loading train:  25%|██▍       | 66/266 [00:19<00:53,  3.76it/s]Loading train:  25%|██▌       | 67/266 [00:20<00:53,  3.74it/s]Loading train:  26%|██▌       | 68/266 [00:20<00:52,  3.76it/s]Loading train:  26%|██▌       | 69/266 [00:20<00:51,  3.79it/s]Loading train:  26%|██▋       | 70/266 [00:20<00:51,  3.79it/s]Loading train:  27%|██▋       | 71/266 [00:21<00:51,  3.77it/s]Loading train:  27%|██▋       | 72/266 [00:21<00:52,  3.69it/s]Loading train:  27%|██▋       | 73/266 [00:21<00:51,  3.74it/s]Loading train:  28%|██▊       | 74/266 [00:21<00:51,  3.76it/s]Loading train:  28%|██▊       | 75/266 [00:22<00:50,  3.76it/s]Loading train:  29%|██▊       | 76/266 [00:22<00:50,  3.79it/s]Loading train:  29%|██▉       | 77/266 [00:22<00:49,  3.79it/s]Loading train:  29%|██▉       | 78/266 [00:22<00:52,  3.59it/s]Loading train:  30%|██▉       | 79/266 [00:23<00:55,  3.40it/s]Loading train:  30%|███       | 80/266 [00:23<00:57,  3.22it/s]Loading train:  30%|███       | 81/266 [00:23<00:57,  3.22it/s]Loading train:  31%|███       | 82/266 [00:24<00:56,  3.23it/s]Loading train:  31%|███       | 83/266 [00:24<00:57,  3.21it/s]Loading train:  32%|███▏      | 84/266 [00:24<00:56,  3.20it/s]Loading train:  32%|███▏      | 85/266 [00:25<00:56,  3.20it/s]Loading train:  32%|███▏      | 86/266 [00:25<00:56,  3.18it/s]Loading train:  33%|███▎      | 87/266 [00:25<00:56,  3.19it/s]Loading train:  33%|███▎      | 88/266 [00:26<00:56,  3.18it/s]Loading train:  33%|███▎      | 89/266 [00:26<00:55,  3.17it/s]Loading train:  34%|███▍      | 90/266 [00:26<00:56,  3.14it/s]Loading train:  34%|███▍      | 91/266 [00:27<00:55,  3.17it/s]Loading train:  35%|███▍      | 92/266 [00:27<00:54,  3.18it/s]Loading train:  35%|███▍      | 93/266 [00:27<00:54,  3.17it/s]Loading train:  35%|███▌      | 94/266 [00:28<00:54,  3.17it/s]Loading train:  36%|███▌      | 95/266 [00:28<00:53,  3.19it/s]Loading train:  36%|███▌      | 96/266 [00:28<00:53,  3.21it/s]Loading train:  36%|███▋      | 97/266 [00:29<00:53,  3.17it/s]Loading train:  37%|███▋      | 98/266 [00:29<00:52,  3.19it/s]Loading train:  37%|███▋      | 99/266 [00:29<00:50,  3.30it/s]Loading train:  38%|███▊      | 100/266 [00:29<00:49,  3.36it/s]Loading train:  38%|███▊      | 101/266 [00:30<00:48,  3.37it/s]Loading train:  38%|███▊      | 102/266 [00:30<00:48,  3.38it/s]Loading train:  39%|███▊      | 103/266 [00:30<00:48,  3.36it/s]Loading train:  39%|███▉      | 104/266 [00:31<00:48,  3.35it/s]Loading train:  39%|███▉      | 105/266 [00:31<00:47,  3.36it/s]Loading train:  40%|███▉      | 106/266 [00:31<00:47,  3.37it/s]Loading train:  40%|████      | 107/266 [00:31<00:47,  3.37it/s]Loading train:  41%|████      | 108/266 [00:32<00:47,  3.31it/s]Loading train:  41%|████      | 109/266 [00:32<00:47,  3.34it/s]Loading train:  41%|████▏     | 110/266 [00:32<00:47,  3.31it/s]Loading train:  42%|████▏     | 111/266 [00:33<00:46,  3.33it/s]Loading train:  42%|████▏     | 112/266 [00:33<00:46,  3.32it/s]Loading train:  42%|████▏     | 113/266 [00:33<00:46,  3.32it/s]Loading train:  43%|████▎     | 114/266 [00:34<00:45,  3.33it/s]Loading train:  43%|████▎     | 115/266 [00:34<00:45,  3.36it/s]Loading train:  44%|████▎     | 116/266 [00:34<00:44,  3.39it/s]Loading train:  44%|████▍     | 117/266 [00:34<00:43,  3.41it/s]Loading train:  44%|████▍     | 118/266 [00:35<00:43,  3.39it/s]Loading train:  45%|████▍     | 119/266 [00:35<00:44,  3.32it/s]Loading train:  45%|████▌     | 120/266 [00:35<00:44,  3.28it/s]Loading train:  45%|████▌     | 121/266 [00:36<00:44,  3.27it/s]Loading train:  46%|████▌     | 122/266 [00:36<00:44,  3.25it/s]Loading train:  46%|████▌     | 123/266 [00:36<00:43,  3.26it/s]Loading train:  47%|████▋     | 124/266 [00:37<00:44,  3.19it/s]Loading train:  47%|████▋     | 125/266 [00:37<00:44,  3.17it/s]Loading train:  47%|████▋     | 126/266 [00:37<00:43,  3.20it/s]Loading train:  48%|████▊     | 127/266 [00:38<00:43,  3.21it/s]Loading train:  48%|████▊     | 128/266 [00:38<00:42,  3.22it/s]Loading train:  48%|████▊     | 129/266 [00:38<00:42,  3.23it/s]Loading train:  49%|████▉     | 130/266 [00:38<00:42,  3.23it/s]Loading train:  49%|████▉     | 131/266 [00:39<00:41,  3.24it/s]Loading train:  50%|████▉     | 132/266 [00:39<00:41,  3.23it/s]Loading train:  50%|█████     | 133/266 [00:39<00:40,  3.25it/s]Loading train:  50%|█████     | 134/266 [00:40<00:40,  3.24it/s]Loading train:  51%|█████     | 135/266 [00:40<00:40,  3.26it/s]Loading train:  51%|█████     | 136/266 [00:40<00:40,  3.24it/s]Loading train:  52%|█████▏    | 137/266 [00:41<00:39,  3.29it/s]Loading train:  52%|█████▏    | 138/266 [00:41<00:38,  3.31it/s]Loading train:  52%|█████▏    | 139/266 [00:41<00:38,  3.34it/s]Loading train:  53%|█████▎    | 140/266 [00:42<00:37,  3.33it/s]Loading train:  53%|█████▎    | 141/266 [00:42<00:37,  3.33it/s]Loading train:  53%|█████▎    | 142/266 [00:42<00:37,  3.32it/s]Loading train:  54%|█████▍    | 143/266 [00:42<00:36,  3.34it/s]Loading train:  54%|█████▍    | 144/266 [00:43<00:36,  3.35it/s]Loading train:  55%|█████▍    | 145/266 [00:43<00:36,  3.35it/s]Loading train:  55%|█████▍    | 146/266 [00:43<00:35,  3.38it/s]Loading train:  55%|█████▌    | 147/266 [00:44<00:35,  3.33it/s]Loading train:  56%|█████▌    | 148/266 [00:44<00:35,  3.37it/s]Loading train:  56%|█████▌    | 149/266 [00:44<00:34,  3.37it/s]Loading train:  56%|█████▋    | 150/266 [00:44<00:34,  3.39it/s]Loading train:  57%|█████▋    | 151/266 [00:45<00:33,  3.39it/s]Loading train:  57%|█████▋    | 152/266 [00:45<00:33,  3.40it/s]Loading train:  58%|█████▊    | 153/266 [00:45<00:33,  3.41it/s]Loading train:  58%|█████▊    | 154/266 [00:46<00:32,  3.43it/s]Loading train:  58%|█████▊    | 155/266 [00:46<00:31,  3.54it/s]Loading train:  59%|█████▊    | 156/266 [00:46<00:30,  3.64it/s]Loading train:  59%|█████▉    | 157/266 [00:46<00:29,  3.72it/s]Loading train:  59%|█████▉    | 158/266 [00:47<00:28,  3.77it/s]Loading train:  60%|█████▉    | 159/266 [00:47<00:28,  3.80it/s]Loading train:  60%|██████    | 160/266 [00:47<00:27,  3.83it/s]Loading train:  61%|██████    | 161/266 [00:47<00:27,  3.80it/s]Loading train:  61%|██████    | 162/266 [00:48<00:27,  3.80it/s]Loading train:  61%|██████▏   | 163/266 [00:48<00:27,  3.81it/s]Loading train:  62%|██████▏   | 164/266 [00:48<00:27,  3.74it/s]Loading train:  62%|██████▏   | 165/266 [00:49<00:26,  3.80it/s]Loading train:  62%|██████▏   | 166/266 [00:49<00:25,  3.86it/s]Loading train:  63%|██████▎   | 167/266 [00:49<00:25,  3.82it/s]Loading train:  63%|██████▎   | 168/266 [00:49<00:25,  3.84it/s]Loading train:  64%|██████▎   | 169/266 [00:50<00:25,  3.85it/s]Loading train:  64%|██████▍   | 170/266 [00:50<00:24,  3.85it/s]Loading train:  64%|██████▍   | 171/266 [00:50<00:24,  3.86it/s]Loading train:  65%|██████▍   | 172/266 [00:50<00:24,  3.86it/s]Loading train:  65%|██████▌   | 173/266 [00:51<00:24,  3.82it/s]Loading train:  65%|██████▌   | 174/266 [00:51<00:24,  3.78it/s]Loading train:  66%|██████▌   | 175/266 [00:51<00:24,  3.76it/s]Loading train:  66%|██████▌   | 176/266 [00:51<00:24,  3.71it/s]Loading train:  67%|██████▋   | 177/266 [00:52<00:23,  3.71it/s]Loading train:  67%|██████▋   | 178/266 [00:52<00:23,  3.70it/s]Loading train:  67%|██████▋   | 179/266 [00:52<00:23,  3.71it/s]Loading train:  68%|██████▊   | 180/266 [00:53<00:23,  3.70it/s]Loading train:  68%|██████▊   | 181/266 [00:53<00:22,  3.71it/s]Loading train:  68%|██████▊   | 182/266 [00:53<00:22,  3.69it/s]Loading train:  69%|██████▉   | 183/266 [00:53<00:22,  3.64it/s]Loading train:  69%|██████▉   | 184/266 [00:54<00:22,  3.63it/s]Loading train:  70%|██████▉   | 185/266 [00:54<00:22,  3.60it/s]Loading train:  70%|██████▉   | 186/266 [00:54<00:22,  3.56it/s]Loading train:  70%|███████   | 187/266 [00:54<00:21,  3.60it/s]Loading train:  71%|███████   | 188/266 [00:55<00:21,  3.56it/s]Loading train:  71%|███████   | 189/266 [00:55<00:21,  3.58it/s]Loading train:  71%|███████▏  | 190/266 [00:55<00:21,  3.60it/s]Loading train:  72%|███████▏  | 191/266 [00:56<00:20,  3.59it/s]Loading train:  72%|███████▏  | 192/266 [00:56<00:20,  3.64it/s]Loading train:  73%|███████▎  | 193/266 [00:56<00:20,  3.64it/s]Loading train:  73%|███████▎  | 194/266 [00:56<00:20,  3.44it/s]Loading train:  73%|███████▎  | 195/266 [00:57<00:20,  3.53it/s]Loading train:  74%|███████▎  | 196/266 [00:57<00:19,  3.58it/s]Loading train:  74%|███████▍  | 197/266 [00:57<00:19,  3.60it/s]Loading train:  74%|███████▍  | 198/266 [00:58<00:18,  3.63it/s]Loading train:  75%|███████▍  | 199/266 [00:58<00:18,  3.64it/s]Loading train:  75%|███████▌  | 200/266 [00:58<00:18,  3.66it/s]Loading train:  76%|███████▌  | 201/266 [00:58<00:17,  3.69it/s]Loading train:  76%|███████▌  | 202/266 [00:59<00:17,  3.68it/s]Loading train:  76%|███████▋  | 203/266 [00:59<00:17,  3.71it/s]Loading train:  77%|███████▋  | 204/266 [00:59<00:16,  3.71it/s]Loading train:  77%|███████▋  | 205/266 [00:59<00:16,  3.70it/s]Loading train:  77%|███████▋  | 206/266 [01:00<00:16,  3.71it/s]Loading train:  78%|███████▊  | 207/266 [01:00<00:15,  3.71it/s]Loading train:  78%|███████▊  | 208/266 [01:00<00:15,  3.72it/s]Loading train:  79%|███████▊  | 209/266 [01:00<00:15,  3.73it/s]Loading train:  79%|███████▉  | 210/266 [01:01<00:15,  3.68it/s]Loading train:  79%|███████▉  | 211/266 [01:01<00:14,  3.70it/s]Loading train:  80%|███████▉  | 212/266 [01:01<00:14,  3.70it/s]Loading train:  80%|████████  | 213/266 [01:02<00:14,  3.74it/s]Loading train:  80%|████████  | 214/266 [01:02<00:13,  3.82it/s]Loading train:  81%|████████  | 215/266 [01:02<00:13,  3.79it/s]Loading train:  81%|████████  | 216/266 [01:02<00:12,  3.87it/s]Loading train:  82%|████████▏ | 217/266 [01:03<00:12,  3.91it/s]Loading train:  82%|████████▏ | 218/266 [01:03<00:12,  3.94it/s]Loading train:  82%|████████▏ | 219/266 [01:03<00:11,  3.96it/s]Loading train:  83%|████████▎ | 220/266 [01:03<00:11,  3.96it/s]Loading train:  83%|████████▎ | 221/266 [01:04<00:11,  3.99it/s]Loading train:  83%|████████▎ | 222/266 [01:04<00:11,  4.00it/s]Loading train:  84%|████████▍ | 223/266 [01:04<00:10,  3.98it/s]Loading train:  84%|████████▍ | 224/266 [01:04<00:10,  3.98it/s]Loading train:  85%|████████▍ | 225/266 [01:05<00:10,  3.99it/s]Loading train:  85%|████████▍ | 226/266 [01:05<00:10,  3.97it/s]Loading train:  85%|████████▌ | 227/266 [01:05<00:09,  3.98it/s]Loading train:  86%|████████▌ | 228/266 [01:05<00:09,  3.98it/s]Loading train:  86%|████████▌ | 229/266 [01:06<00:09,  3.93it/s]Loading train:  86%|████████▋ | 230/266 [01:06<00:09,  3.94it/s]Loading train:  87%|████████▋ | 231/266 [01:06<00:09,  3.86it/s]Loading train:  87%|████████▋ | 232/266 [01:06<00:08,  3.82it/s]Loading train:  88%|████████▊ | 233/266 [01:07<00:08,  3.79it/s]Loading train:  88%|████████▊ | 234/266 [01:07<00:08,  3.80it/s]Loading train:  88%|████████▊ | 235/266 [01:07<00:08,  3.79it/s]Loading train:  89%|████████▊ | 236/266 [01:07<00:08,  3.74it/s]Loading train:  89%|████████▉ | 237/266 [01:08<00:07,  3.69it/s]Loading train:  89%|████████▉ | 238/266 [01:08<00:07,  3.62it/s]Loading train:  90%|████████▉ | 239/266 [01:08<00:07,  3.64it/s]Loading train:  90%|█████████ | 240/266 [01:09<00:07,  3.64it/s]Loading train:  91%|█████████ | 241/266 [01:09<00:06,  3.58it/s]Loading train:  91%|█████████ | 242/266 [01:09<00:06,  3.60it/s]Loading train:  91%|█████████▏| 243/266 [01:09<00:06,  3.62it/s]Loading train:  92%|█████████▏| 244/266 [01:10<00:06,  3.66it/s]Loading train:  92%|█████████▏| 245/266 [01:10<00:05,  3.67it/s]Loading train:  92%|█████████▏| 246/266 [01:10<00:05,  3.66it/s]Loading train:  93%|█████████▎| 247/266 [01:11<00:05,  3.54it/s]Loading train:  93%|█████████▎| 248/266 [01:11<00:05,  3.55it/s]Loading train:  94%|█████████▎| 249/266 [01:11<00:04,  3.47it/s]Loading train:  94%|█████████▍| 250/266 [01:11<00:04,  3.42it/s]Loading train:  94%|█████████▍| 251/266 [01:12<00:04,  3.39it/s]Loading train:  95%|█████████▍| 252/266 [01:12<00:04,  3.36it/s]Loading train:  95%|█████████▌| 253/266 [01:12<00:03,  3.34it/s]Loading train:  95%|█████████▌| 254/266 [01:13<00:03,  3.31it/s]Loading train:  96%|█████████▌| 255/266 [01:13<00:03,  3.31it/s]Loading train:  96%|█████████▌| 256/266 [01:13<00:03,  3.31it/s]Loading train:  97%|█████████▋| 257/266 [01:14<00:02,  3.26it/s]Loading train:  97%|█████████▋| 258/266 [01:14<00:02,  3.24it/s]Loading train:  97%|█████████▋| 259/266 [01:14<00:02,  3.25it/s]Loading train:  98%|█████████▊| 260/266 [01:14<00:01,  3.24it/s]Loading train:  98%|█████████▊| 261/266 [01:15<00:01,  3.22it/s]Loading train:  98%|█████████▊| 262/266 [01:15<00:01,  3.18it/s]Loading train:  99%|█████████▉| 263/266 [01:15<00:00,  3.12it/s]Loading train:  99%|█████████▉| 264/266 [01:16<00:00,  3.14it/s]Loading train: 100%|█████████▉| 265/266 [01:16<00:00,  3.19it/s]Loading train: 100%|██████████| 266/266 [01:16<00:00,  3.22it/s]Loading train: 100%|██████████| 266/266 [01:16<00:00,  3.46it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:05, 49.69it/s]concatenating: train:   4%|▍         | 10/266 [00:00<00:05, 47.12it/s]concatenating: train:   6%|▌         | 15/266 [00:00<00:05, 47.46it/s]concatenating: train:   8%|▊         | 20/266 [00:00<00:05, 46.69it/s]concatenating: train:   9%|▉         | 25/266 [00:00<00:05, 46.80it/s]concatenating: train:  11%|█▏        | 30/266 [00:00<00:05, 46.75it/s]concatenating: train:  13%|█▎        | 35/266 [00:00<00:04, 46.39it/s]concatenating: train:  15%|█▌        | 40/266 [00:00<00:05, 45.15it/s]concatenating: train:  17%|█▋        | 45/266 [00:00<00:04, 45.49it/s]concatenating: train:  19%|█▉        | 51/266 [00:01<00:04, 48.28it/s]concatenating: train:  21%|██▏       | 57/266 [00:01<00:04, 49.20it/s]concatenating: train:  24%|██▎       | 63/266 [00:01<00:04, 49.82it/s]concatenating: train:  26%|██▌       | 69/266 [00:01<00:03, 51.82it/s]concatenating: train:  28%|██▊       | 75/266 [00:01<00:03, 52.43it/s]concatenating: train:  30%|███       | 81/266 [00:01<00:03, 52.39it/s]concatenating: train:  33%|███▎      | 87/266 [00:01<00:03, 51.27it/s]concatenating: train:  35%|███▍      | 93/266 [00:01<00:03, 49.71it/s]concatenating: train:  37%|███▋      | 98/266 [00:02<00:03, 48.64it/s]concatenating: train:  39%|███▉      | 104/266 [00:02<00:03, 49.96it/s]concatenating: train:  41%|████▏     | 110/266 [00:02<00:03, 49.20it/s]concatenating: train:  43%|████▎     | 115/266 [00:02<00:03, 49.04it/s]concatenating: train:  45%|████▌     | 120/266 [00:02<00:03, 46.97it/s]concatenating: train:  47%|████▋     | 125/266 [00:02<00:03, 44.91it/s]concatenating: train:  49%|████▉     | 130/266 [00:02<00:03, 43.35it/s]concatenating: train:  51%|█████     | 135/266 [00:02<00:03, 43.62it/s]concatenating: train:  53%|█████▎    | 140/266 [00:02<00:03, 40.94it/s]concatenating: train:  55%|█████▍    | 145/266 [00:03<00:02, 40.85it/s]concatenating: train:  56%|█████▋    | 150/266 [00:03<00:02, 40.89it/s]concatenating: train:  58%|█████▊    | 155/266 [00:03<00:02, 41.13it/s]concatenating: train:  61%|██████    | 161/266 [00:03<00:02, 43.62it/s]concatenating: train:  62%|██████▏   | 166/266 [00:03<00:02, 44.36it/s]concatenating: train:  64%|██████▍   | 171/266 [00:03<00:02, 44.32it/s]concatenating: train:  66%|██████▌   | 176/266 [00:03<00:02, 44.13it/s]concatenating: train:  68%|██████▊   | 181/266 [00:03<00:01, 43.40it/s]concatenating: train:  70%|██████▉   | 186/266 [00:04<00:01, 42.39it/s]concatenating: train:  72%|███████▏  | 191/266 [00:04<00:01, 42.55it/s]concatenating: train:  74%|███████▎  | 196/266 [00:04<00:01, 44.31it/s]concatenating: train:  76%|███████▌  | 201/266 [00:04<00:01, 44.46it/s]concatenating: train:  77%|███████▋  | 206/266 [00:04<00:01, 44.68it/s]concatenating: train:  79%|███████▉  | 211/266 [00:04<00:01, 45.72it/s]concatenating: train:  82%|████████▏ | 217/266 [00:04<00:01, 46.98it/s]concatenating: train:  83%|████████▎ | 222/266 [00:04<00:00, 46.90it/s]concatenating: train:  85%|████████▌ | 227/266 [00:04<00:00, 47.11it/s]concatenating: train:  87%|████████▋ | 232/266 [00:05<00:00, 45.67it/s]concatenating: train:  89%|████████▉ | 237/266 [00:05<00:00, 45.10it/s]concatenating: train:  91%|█████████ | 242/266 [00:05<00:00, 44.32it/s]concatenating: train:  93%|█████████▎| 247/266 [00:05<00:00, 44.00it/s]concatenating: train:  95%|█████████▍| 252/266 [00:05<00:00, 42.16it/s]concatenating: train:  97%|█████████▋| 257/266 [00:05<00:00, 40.51it/s]concatenating: train:  98%|█████████▊| 262/266 [00:05<00:00, 38.73it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 38.22it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 45.15it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:00,  3.15it/s]Loading test:  50%|█████     | 2/4 [00:00<00:00,  3.30it/s]Loading test:  75%|███████▌  | 3/4 [00:00<00:00,  3.48it/s]Loading test: 100%|██████████| 4/4 [00:01<00:00,  3.43it/s]Loading test: 100%|██████████| 4/4 [00:01<00:00,  3.51it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 382.81it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 76, 108, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 76, 108, 30)  300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 76, 108, 30)  120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 76, 108, 30)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 76, 108, 30)  8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 76, 108, 30)  120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 76, 108, 30)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 38, 54, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 38, 54, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 38, 54, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 38, 54, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 38, 54, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 38, 54, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 38, 54, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 38, 54, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 38, 54, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 19, 27, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 19, 27, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 19, 27, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 19, 27, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 19, 27, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 19, 27, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 19, 27, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 19, 27, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 19, 27, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 19, 27, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 38, 54, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 38, 54, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 38, 54, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 38, 54, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 38, 54, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 38, 54, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 38, 54, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 38, 54, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 38, 54, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2020-01-21 20:44:41.829557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 20:44:41.829633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 20:44:41.829645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 20:44:41.829652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 20:44:41.829964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 38, 54, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 76, 108, 30)  25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 76, 108, 60)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 76, 108, 30)  16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 76, 108, 30)  120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 76, 108, 30)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 76, 108, 30)  8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 76, 108, 30)  120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 76, 108, 30)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 76, 108, 90)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 76, 108, 90)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 76, 108, 2)   182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 499,142
Non-trainable params: 1,200
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1
------------------------------------------------------------------
class_weights [0.97453182 0.02546818]
Train on 27987 samples, validate on 396 samples
Epoch 1/300
 - 74s - loss: 0.1368 - acc: 0.9859 - mDice: 0.7339 - val_loss: 0.0128 - val_acc: 0.9932 - val_mDice: 0.5345

Epoch 00001: val_mDice improved from -inf to 0.53449, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300
 - 70s - loss: 0.0759 - acc: 0.9920 - mDice: 0.8524 - val_loss: 0.0108 - val_acc: 0.9939 - val_mDice: 0.5429

Epoch 00002: val_mDice improved from 0.53449 to 0.54291, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 3/300
 - 70s - loss: 0.0666 - acc: 0.9928 - mDice: 0.8706 - val_loss: -1.7131e-02 - val_acc: 0.9941 - val_mDice: 0.5460

Epoch 00003: val_mDice improved from 0.54291 to 0.54596, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 4/300
 - 70s - loss: 0.0617 - acc: 0.9933 - mDice: 0.8800 - val_loss: -3.7638e-02 - val_acc: 0.9943 - val_mDice: 0.5519

Epoch 00004: val_mDice improved from 0.54596 to 0.55193, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 5/300
 - 71s - loss: 0.0582 - acc: 0.9937 - mDice: 0.8868 - val_loss: -2.9467e-02 - val_acc: 0.9944 - val_mDice: 0.5550

Epoch 00005: val_mDice improved from 0.55193 to 0.55505, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 6/300
 - 70s - loss: 0.0558 - acc: 0.9939 - mDice: 0.8916 - val_loss: -2.0769e-02 - val_acc: 0.9946 - val_mDice: 0.5517

Epoch 00006: val_mDice did not improve from 0.55505
Epoch 7/300
 - 70s - loss: 0.0536 - acc: 0.9941 - mDice: 0.8959 - val_loss: -1.9219e-02 - val_acc: 0.9947 - val_mDice: 0.5509

Epoch 00007: val_mDice did not improve from 0.55505
Epoch 8/300
 - 70s - loss: 0.0516 - acc: 0.9943 - mDice: 0.8997 - val_loss: 0.0014 - val_acc: 0.9946 - val_mDice: 0.5539

Epoch 00008: val_mDice did not improve from 0.55505
Epoch 9/300
 - 70s - loss: 0.0504 - acc: 0.9944 - mDice: 0.9021 - val_loss: -1.7845e-02 - val_acc: 0.9946 - val_mDice: 0.5540

Epoch 00009: val_mDice did not improve from 0.55505
Epoch 10/300
 - 70s - loss: 0.0494 - acc: 0.9945 - mDice: 0.9039 - val_loss: -2.9279e-02 - val_acc: 0.9948 - val_mDice: 0.5479

Epoch 00010: val_mDice did not improve from 0.55505
Epoch 11/300
 - 70s - loss: 0.0481 - acc: 0.9946 - mDice: 0.9066 - val_loss: -2.0182e-02 - val_acc: 0.9948 - val_mDice: 0.5522

Epoch 00011: val_mDice did not improve from 0.55505
Epoch 12/300
 - 70s - loss: 0.0470 - acc: 0.9947 - mDice: 0.9086 - val_loss: -1.1126e-02 - val_acc: 0.9949 - val_mDice: 0.5532

Epoch 00012: val_mDice did not improve from 0.55505
Epoch 13/300
 - 70s - loss: 0.0462 - acc: 0.9948 - mDice: 0.9102 - val_loss: -2.3900e-02 - val_acc: 0.9949 - val_mDice: 0.5535

Epoch 00013: val_mDice did not improve from 0.55505
Epoch 14/300
 - 70s - loss: 0.0457 - acc: 0.9949 - mDice: 0.9113 - val_loss: -6.1290e-02 - val_acc: 0.9947 - val_mDice: 0.5509

Epoch 00014: val_mDice did not improve from 0.55505
Epoch 15/300
 - 70s - loss: 0.0450 - acc: 0.9950 - mDice: 0.9126 - val_loss: -5.2565e-02 - val_acc: 0.9950 - val_mDice: 0.5516

Epoch 00015: val_mDice did not improve from 0.55505
Epoch 16/300
 - 70s - loss: 0.0440 - acc: 0.9950 - mDice: 0.9144 - val_loss: -5.9584e-02 - val_acc: 0.9948 - val_mDice: 0.5530

Epoch 00016: val_mDice did not improve from 0.55505
Epoch 17/300
 - 69s - loss: 0.0442 - acc: 0.9951 - mDice: 0.9140 - val_loss: -5.7795e-02 - val_acc: 0.9949 - val_mDice: 0.5512

Epoch 00017: val_mDice did not improve from 0.55505
Epoch 18/300
 - 70s - loss: 0.0431 - acc: 0.9951 - mDice: 0.9163 - val_loss: -7.6075e-02 - val_acc: 0.9948 - val_mDice: 0.5569

Epoch 00018: val_mDice improved from 0.55505 to 0.55692, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 19/300
 - 70s - loss: 0.0428 - acc: 0.9952 - mDice: 0.9169 - val_loss: -6.8877e-02 - val_acc: 0.9949 - val_mDice: 0.5554

Epoch 00019: val_mDice did not improve from 0.55692
Epoch 20/300
 - 70s - loss: 0.0422 - acc: 0.9952 - mDice: 0.9181 - val_loss: -8.1417e-02 - val_acc: 0.9949 - val_mDice: 0.5508

Epoch 00020: val_mDice did not improve from 0.55692
Epoch 21/300
 - 70s - loss: 0.0417 - acc: 0.9953 - mDice: 0.9190 - val_loss: -8.9726e-02 - val_acc: 0.9949 - val_mDice: 0.5525

Epoch 00021: val_mDice did not improve from 0.55692
Epoch 22/300
 - 70s - loss: 0.0411 - acc: 0.9953 - mDice: 0.9201 - val_loss: -9.5265e-02 - val_acc: 0.9949 - val_mDice: 0.5591

Epoch 00022: val_mDice improved from 0.55692 to 0.55913, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 23/300
 - 70s - loss: 0.0406 - acc: 0.9954 - mDice: 0.9210 - val_loss: -9.3975e-02 - val_acc: 0.9949 - val_mDice: 0.5576

Epoch 00023: val_mDice did not improve from 0.55913
Epoch 24/300
 - 72s - loss: 0.0407 - acc: 0.9954 - mDice: 0.9209 - val_loss: -9.4686e-02 - val_acc: 0.9949 - val_mDice: 0.5583

Epoch 00024: val_mDice did not improve from 0.55913
Epoch 25/300
 - 72s - loss: 0.0399 - acc: 0.9954 - mDice: 0.9225 - val_loss: -9.5543e-02 - val_acc: 0.9949 - val_mDice: 0.5588

Epoch 00025: val_mDice did not improve from 0.55913
Epoch 26/300
 - 72s - loss: 0.0400 - acc: 0.9954 - mDice: 0.9222 - val_loss: -9.5292e-02 - val_acc: 0.9948 - val_mDice: 0.5575

Epoch 00026: val_mDice did not improve from 0.55913
Epoch 27/300
 - 71s - loss: 0.0393 - acc: 0.9955 - mDice: 0.9237 - val_loss: -9.5220e-02 - val_acc: 0.9948 - val_mDice: 0.5576

Epoch 00027: val_mDice did not improve from 0.55913
Epoch 28/300
 - 70s - loss: 0.0392 - acc: 0.9955 - mDice: 0.9239 - val_loss: -9.3292e-02 - val_acc: 0.9949 - val_mDice: 0.5589

Epoch 00028: val_mDice did not improve from 0.55913
Epoch 29/300
 - 70s - loss: 0.0386 - acc: 0.9955 - mDice: 0.9250 - val_loss: -9.0523e-02 - val_acc: 0.9949 - val_mDice: 0.5535

Epoch 00029: val_mDice did not improve from 0.55913
Epoch 30/300
 - 70s - loss: 0.0383 - acc: 0.9956 - mDice: 0.9256 - val_loss: -9.6433e-02 - val_acc: 0.9948 - val_mDice: 0.5608

Epoch 00030: val_mDice improved from 0.55913 to 0.56081, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 31/300
 - 70s - loss: 0.0382 - acc: 0.9956 - mDice: 0.9258 - val_loss: -9.3163e-02 - val_acc: 0.9950 - val_mDice: 0.5539

Epoch 00031: val_mDice did not improve from 0.56081
Epoch 32/300
 - 70s - loss: 0.0378 - acc: 0.9956 - mDice: 0.9267 - val_loss: -9.6118e-02 - val_acc: 0.9949 - val_mDice: 0.5595

Epoch 00032: val_mDice did not improve from 0.56081
Epoch 33/300
 - 69s - loss: 0.0378 - acc: 0.9956 - mDice: 0.9265 - val_loss: -9.5169e-02 - val_acc: 0.9948 - val_mDice: 0.5573

Epoch 00033: val_mDice did not improve from 0.56081
Epoch 34/300
 - 70s - loss: 0.0374 - acc: 0.9957 - mDice: 0.9274 - val_loss: -9.6063e-02 - val_acc: 0.9947 - val_mDice: 0.5645

Epoch 00034: val_mDice improved from 0.56081 to 0.56447, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 35/300
 - 70s - loss: 0.0375 - acc: 0.9957 - mDice: 0.9272 - val_loss: -9.4045e-02 - val_acc: 0.9949 - val_mDice: 0.5551

Epoch 00035: val_mDice did not improve from 0.56447
Epoch 36/300
 - 70s - loss: 0.0368 - acc: 0.9957 - mDice: 0.9284 - val_loss: -9.3729e-02 - val_acc: 0.9949 - val_mDice: 0.5567

Epoch 00036: val_mDice did not improve from 0.56447
Epoch 37/300
 - 70s - loss: 0.0366 - acc: 0.9957 - mDice: 0.9289 - val_loss: -9.6107e-02 - val_acc: 0.9948 - val_mDice: 0.5595

Epoch 00037: val_mDice did not improve from 0.56447
Epoch 38/300
 - 70s - loss: 0.0366 - acc: 0.9957 - mDice: 0.9289 - val_loss: -9.5194e-02 - val_acc: 0.9948 - val_mDice: 0.5574

Epoch 00038: val_mDice did not improve from 0.56447
Epoch 39/300
 - 70s - loss: 0.0368 - acc: 0.9958 - mDice: 0.9285 - val_loss: -9.7237e-02 - val_acc: 0.9949 - val_mDice: 0.5618

Epoch 00039: val_mDice did not improve from 0.56447
Epoch 40/300
 - 70s - loss: 0.0359 - acc: 0.9958 - mDice: 0.9303 - val_loss: -9.5080e-02 - val_acc: 0.9949 - val_mDice: 0.5572

Epoch 00040: val_mDice did not improve from 0.56447
Epoch 41/300
 - 70s - loss: 0.0362 - acc: 0.9958 - mDice: 0.9297 - val_loss: -9.4703e-02 - val_acc: 0.9948 - val_mDice: 0.5560

Epoch 00041: val_mDice did not improve from 0.56447
Epoch 42/300
 - 70s - loss: 0.0358 - acc: 0.9958 - mDice: 0.9305 - val_loss: -9.6319e-02 - val_acc: 0.9949 - val_mDice: 0.5592

Epoch 00042: val_mDice did not improve from 0.56447
Epoch 43/300
 - 69s - loss: 0.0354 - acc: 0.9958 - mDice: 0.9312 - val_loss: -9.7065e-02 - val_acc: 0.9949 - val_mDice: 0.5606

Epoch 00043: val_mDice did not improve from 0.56447
Epoch 44/300
 - 70s - loss: 0.0354 - acc: 0.9958 - mDice: 0.9313 - val_loss: -9.5816e-02 - val_acc: 0.9949 - val_mDice: 0.5585

Epoch 00044: val_mDice did not improve from 0.56447
Epoch 45/300
 - 71s - loss: 0.0352 - acc: 0.9958 - mDice: 0.9317 - val_loss: -9.5451e-02 - val_acc: 0.9948 - val_mDice: 0.5575

Epoch 00045: val_mDice did not improve from 0.56447
Epoch 46/300
 - 70s - loss: 0.0351 - acc: 0.9959 - mDice: 0.9318 - val_loss: -9.7674e-02 - val_acc: 0.9949 - val_mDice: 0.5621

Epoch 00046: val_mDice did not improve from 0.56447
Epoch 47/300
 - 69s - loss: 0.0348 - acc: 0.9959 - mDice: 0.9324 - val_loss: -9.7729e-02 - val_acc: 0.9948 - val_mDice: 0.5620

Epoch 00047: val_mDice did not improve from 0.56447
Epoch 48/300
 - 69s - loss: 0.0347 - acc: 0.9959 - mDice: 0.9326 - val_loss: -9.6714e-02 - val_acc: 0.9949 - val_mDice: 0.5599

Epoch 00048: val_mDice did not improve from 0.56447
Epoch 49/300
 - 70s - loss: 0.0344 - acc: 0.9959 - mDice: 0.9332 - val_loss: -9.5219e-02 - val_acc: 0.9948 - val_mDice: 0.5569

Epoch 00049: val_mDice did not improve from 0.56447

Epoch 00049: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 50/300
 - 70s - loss: 0.0344 - acc: 0.9959 - mDice: 0.9332 - val_loss: -9.7261e-02 - val_acc: 0.9947 - val_mDice: 0.5611

Epoch 00050: val_mDice did not improve from 0.56447
Epoch 51/300
 - 69s - loss: 0.0344 - acc: 0.9959 - mDice: 0.9332 - val_loss: -9.5923e-02 - val_acc: 0.9948 - val_mDice: 0.5583

Epoch 00051: val_mDice did not improve from 0.56447
Epoch 52/300
 - 70s - loss: 0.0340 - acc: 0.9960 - mDice: 0.9340 - val_loss: -9.7881e-02 - val_acc: 0.9949 - val_mDice: 0.5622

Epoch 00052: val_mDice did not improve from 0.56447
Epoch 53/300
 - 70s - loss: 0.0339 - acc: 0.9960 - mDice: 0.9342 - val_loss: -9.7420e-02 - val_acc: 0.9947 - val_mDice: 0.5614

Epoch 00053: val_mDice did not improve from 0.56447
Epoch 54/300
 - 70s - loss: 0.0337 - acc: 0.9960 - mDice: 0.9345 - val_loss: -9.7935e-02 - val_acc: 0.9948 - val_mDice: 0.5624

Epoch 00054: val_mDice did not improve from 0.56447
Epoch 55/300
 - 70s - loss: 0.0339 - acc: 0.9960 - mDice: 0.9342 - val_loss: -9.8264e-02 - val_acc: 0.9949 - val_mDice: 0.5630

Epoch 00055: val_mDice did not improve from 0.56447
Epoch 56/300
 - 70s - loss: 0.0337 - acc: 0.9960 - mDice: 0.9346 - val_loss: -9.7635e-02 - val_acc: 0.9948 - val_mDice: 0.5618

Epoch 00056: val_mDice did not improve from 0.56447
Epoch 57/300
 - 70s - loss: 0.0343 - acc: 0.9960 - mDice: 0.9335 - val_loss: -9.8924e-02 - val_acc: 0.9948 - val_mDice: 0.5644

Epoch 00057: val_mDice did not improve from 0.56447
Epoch 58/300
 - 70s - loss: 0.0337 - acc: 0.9960 - mDice: 0.9345 - val_loss: -9.7579e-02 - val_acc: 0.9948 - val_mDice: 0.5617

Epoch 00058: val_mDice did not improve from 0.56447
Epoch 59/300
 - 71s - loss: 0.0334 - acc: 0.9960 - mDice: 0.9351 - val_loss: -9.7943e-02 - val_acc: 0.9947 - val_mDice: 0.5625

Epoch 00059: val_mDice did not improve from 0.56447
Epoch 60/300
 - 72s - loss: 0.0336 - acc: 0.9960 - mDice: 0.9347 - val_loss: -9.7487e-02 - val_acc: 0.9947 - val_mDice: 0.5616

Epoch 00060: val_mDice did not improve from 0.56447
Epoch 61/300
 - 71s - loss: 0.0334 - acc: 0.9960 - mDice: 0.9351 - val_loss: -9.8468e-02 - val_acc: 0.9948 - val_mDice: 0.5635

Epoch 00061: val_mDice did not improve from 0.56447
Epoch 62/300
 - 72s - loss: 0.0334 - acc: 0.9960 - mDice: 0.9352 - val_loss: -9.8652e-02 - val_acc: 0.9948 - val_mDice: 0.5638

Epoch 00062: val_mDice did not improve from 0.56447
Epoch 63/300
 - 72s - loss: 0.0332 - acc: 0.9960 - mDice: 0.9355 - val_loss: -1.0004e-01 - val_acc: 0.9948 - val_mDice: 0.5666

Epoch 00063: val_mDice improved from 0.56447 to 0.56659, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 64/300
 - 72s - loss: 0.0333 - acc: 0.9960 - mDice: 0.9353 - val_loss: -9.8884e-02 - val_acc: 0.9948 - val_mDice: 0.5643

Epoch 00064: val_mDice did not improve from 0.56659
Epoch 65/300
 - 72s - loss: 0.0330 - acc: 0.9960 - mDice: 0.9359 - val_loss: -9.7337e-02 - val_acc: 0.9949 - val_mDice: 0.5611

Epoch 00065: val_mDice did not improve from 0.56659
Epoch 66/300
 - 73s - loss: 0.0329 - acc: 0.9961 - mDice: 0.9362 - val_loss: -9.9315e-02 - val_acc: 0.9948 - val_mDice: 0.5652

Epoch 00066: val_mDice did not improve from 0.56659
Epoch 67/300
 - 70s - loss: 0.0330 - acc: 0.9961 - mDice: 0.9360 - val_loss: -9.9644e-02 - val_acc: 0.9948 - val_mDice: 0.5658

Epoch 00067: val_mDice did not improve from 0.56659
Epoch 68/300
 - 70s - loss: 0.0330 - acc: 0.9961 - mDice: 0.9359 - val_loss: -9.8490e-02 - val_acc: 0.9948 - val_mDice: 0.5635

Epoch 00068: val_mDice did not improve from 0.56659
Epoch 69/300
 - 70s - loss: 0.0327 - acc: 0.9960 - mDice: 0.9364 - val_loss: -9.8132e-02 - val_acc: 0.9948 - val_mDice: 0.5628

Epoch 00069: val_mDice did not improve from 0.56659
Epoch 70/300
 - 70s - loss: 0.0328 - acc: 0.9961 - mDice: 0.9364 - val_loss: -9.8696e-02 - val_acc: 0.9948 - val_mDice: 0.5639

Epoch 00070: val_mDice did not improve from 0.56659
Epoch 71/300
 - 69s - loss: 0.0326 - acc: 0.9961 - mDice: 0.9367 - val_loss: -9.9304e-02 - val_acc: 0.9948 - val_mDice: 0.5652

Epoch 00071: val_mDice did not improve from 0.56659
Epoch 72/300
 - 70s - loss: 0.0326 - acc: 0.9961 - mDice: 0.9367 - val_loss: -9.8656e-02 - val_acc: 0.9948 - val_mDice: 0.5638

Epoch 00072: val_mDice did not improve from 0.56659
Epoch 73/300
 - 71s - loss: 0.0327 - acc: 0.9961 - mDice: 0.9366 - val_loss: -9.8925e-02 - val_acc: 0.9948 - val_mDice: 0.5644

Epoch 00073: val_mDice did not improve from 0.56659
Epoch 74/300
 - 70s - loss: 0.0324 - acc: 0.9961 - mDice: 0.9370 - val_loss: -1.0006e-01 - val_acc: 0.9948 - val_mDice: 0.5667

Epoch 00074: val_mDice improved from 0.56659 to 0.56666, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 75/300
 - 71s - loss: 0.0325 - acc: 0.9961 - mDice: 0.9370 - val_loss: -9.7682e-02 - val_acc: 0.9949 - val_mDice: 0.5618

Epoch 00075: val_mDice did not improve from 0.56666
Epoch 76/300
 - 71s - loss: 0.0323 - acc: 0.9961 - mDice: 0.9374 - val_loss: -9.9233e-02 - val_acc: 0.9948 - val_mDice: 0.5650

Epoch 00076: val_mDice did not improve from 0.56666
Epoch 77/300
 - 70s - loss: 0.0327 - acc: 0.9961 - mDice: 0.9365 - val_loss: -9.9015e-02 - val_acc: 0.9948 - val_mDice: 0.5646

Epoch 00077: val_mDice did not improve from 0.56666
Epoch 78/300
 - 70s - loss: 0.0322 - acc: 0.9961 - mDice: 0.9375 - val_loss: -9.8427e-02 - val_acc: 0.9948 - val_mDice: 0.5634

Epoch 00078: val_mDice did not improve from 0.56666

Epoch 00078: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 79/300
 - 72s - loss: 0.0321 - acc: 0.9961 - mDice: 0.9377 - val_loss: -9.8130e-02 - val_acc: 0.9949 - val_mDice: 0.5628

Epoch 00079: val_mDice did not improve from 0.56666
Epoch 80/300
 - 72s - loss: 0.0321 - acc: 0.9961 - mDice: 0.9376 - val_loss: -9.8663e-02 - val_acc: 0.9948 - val_mDice: 0.5639

Epoch 00080: val_mDice did not improve from 0.56666
Epoch 81/300
 - 73s - loss: 0.0320 - acc: 0.9961 - mDice: 0.9380 - val_loss: -9.8467e-02 - val_acc: 0.9948 - val_mDice: 0.5634

Epoch 00081: val_mDice did not improve from 0.56666
Epoch 82/300
 - 73s - loss: 0.0319 - acc: 0.9961 - mDice: 0.9381 - val_loss: -9.8324e-02 - val_acc: 0.9948 - val_mDice: 0.5632

Epoch 00082: val_mDice did not improve from 0.56666
Epoch 83/300
 - 72s - loss: 0.0321 - acc: 0.9961 - mDice: 0.9377 - val_loss: -9.8859e-02 - val_acc: 0.9948 - val_mDice: 0.5642

Epoch 00083: val_mDice did not improve from 0.56666
Epoch 84/300
 - 70s - loss: 0.0319 - acc: 0.9961 - mDice: 0.9381 - val_loss: -9.8704e-02 - val_acc: 0.9948 - val_mDice: 0.5640

Epoch 00084: val_mDice did not improve from 0.56666
Epoch 85/300
 - 70s - loss: 0.0319 - acc: 0.9961 - mDice: 0.9381 - val_loss: -9.8379e-02 - val_acc: 0.9949 - val_mDice: 0.5632

Epoch 00085: val_mDice did not improve from 0.56666
Epoch 86/300
 - 71s - loss: 0.0322 - acc: 0.9961 - mDice: 0.9376 - val_loss: -9.8901e-02 - val_acc: 0.9948 - val_mDice: 0.5643

Epoch 00086: val_mDice did not improve from 0.56666
Epoch 87/300
 - 70s - loss: 0.0322 - acc: 0.9961 - mDice: 0.9374 - val_loss: -9.8619e-02 - val_acc: 0.9948 - val_mDice: 0.5637

Epoch 00087: val_mDice did not improve from 0.56666
Epoch 88/300
 - 70s - loss: 0.0322 - acc: 0.9961 - mDice: 0.9375 - val_loss: -9.8650e-02 - val_acc: 0.9948 - val_mDice: 0.5638

Epoch 00088: val_mDice did not improve from 0.56666
Epoch 89/300
 - 70s - loss: 0.0318 - acc: 0.9962 - mDice: 0.9382 - val_loss: -9.9151e-02 - val_acc: 0.9948 - val_mDice: 0.5648

Epoch 00089: val_mDice did not improve from 0.56666
Epoch 90/300
 - 70s - loss: 0.0322 - acc: 0.9961 - mDice: 0.9374 - val_loss: -9.9071e-02 - val_acc: 0.9948 - val_mDice: 0.5647

Epoch 00090: val_mDice did not improve from 0.56666
Epoch 91/300
 - 70s - loss: 0.0318 - acc: 0.9962 - mDice: 0.9382 - val_loss: -9.9166e-02 - val_acc: 0.9948 - val_mDice: 0.5649

Epoch 00091: val_mDice did not improve from 0.56666
Epoch 92/300
 - 70s - loss: 0.0318 - acc: 0.9962 - mDice: 0.9383 - val_loss: -9.8280e-02 - val_acc: 0.9949 - val_mDice: 0.5630

Epoch 00092: val_mDice did not improve from 0.56666
Epoch 93/300
 - 70s - loss: 0.0316 - acc: 0.9962 - mDice: 0.9388 - val_loss: -9.8864e-02 - val_acc: 0.9948 - val_mDice: 0.5643

Epoch 00093: val_mDice did not improve from 0.56666

Epoch 00093: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 94/300
 - 70s - loss: 0.0318 - acc: 0.9962 - mDice: 0.9383 - val_loss: -9.8895e-02 - val_acc: 0.9948 - val_mDice: 0.5643

Epoch 00094: val_mDice did not improve from 0.56666
Epoch 95/300
 - 70s - loss: 0.0317 - acc: 0.9962 - mDice: 0.9384 - val_loss: -9.8732e-02 - val_acc: 0.9948 - val_mDice: 0.5640

Epoch 00095: val_mDice did not improve from 0.56666
Epoch 96/300
 - 71s - loss: 0.0320 - acc: 0.9962 - mDice: 0.9379 - val_loss: -9.9184e-02 - val_acc: 0.9948 - val_mDice: 0.5649

Epoch 00096: val_mDice did not improve from 0.56666
Epoch 97/300
 - 70s - loss: 0.0318 - acc: 0.9962 - mDice: 0.9383 - val_loss: -9.9118e-02 - val_acc: 0.9948 - val_mDice: 0.5648

Epoch 00097: val_mDice did not improve from 0.56666
Epoch 98/300
 - 71s - loss: 0.0316 - acc: 0.9962 - mDice: 0.9387 - val_loss: -9.8702e-02 - val_acc: 0.9948 - val_mDice: 0.5639

Epoch 00098: val_mDice did not improve from 0.56666
Epoch 99/300
 - 71s - loss: 0.0315 - acc: 0.9962 - mDice: 0.9388 - val_loss: -9.9040e-02 - val_acc: 0.9948 - val_mDice: 0.5646

Epoch 00099: val_mDice did not improve from 0.56666
Epoch 100/300
 - 70s - loss: 0.0316 - acc: 0.9962 - mDice: 0.9386 - val_loss: -9.8851e-02 - val_acc: 0.9948 - val_mDice: 0.5642

Epoch 00100: val_mDice did not improve from 0.56666
Epoch 101/300
 - 70s - loss: 0.0315 - acc: 0.9962 - mDice: 0.9388 - val_loss: -9.8780e-02 - val_acc: 0.9948 - val_mDice: 0.5641

Epoch 00101: val_mDice did not improve from 0.56666
Epoch 102/300
 - 70s - loss: 0.0315 - acc: 0.9962 - mDice: 0.9389 - val_loss: -9.8263e-02 - val_acc: 0.9948 - val_mDice: 0.5630

Epoch 00102: val_mDice did not improve from 0.56666
Epoch 103/300
 - 70s - loss: 0.0315 - acc: 0.9962 - mDice: 0.9388 - val_loss: -9.8688e-02 - val_acc: 0.9948 - val_mDice: 0.5639

Epoch 00103: val_mDice did not improve from 0.56666
Epoch 104/300
 - 70s - loss: 0.0314 - acc: 0.9962 - mDice: 0.9391 - val_loss: -9.7625e-02 - val_acc: 0.9949 - val_mDice: 0.5617

Epoch 00104: val_mDice did not improve from 0.56666
Epoch 105/300
 - 70s - loss: 0.0314 - acc: 0.9962 - mDice: 0.9391 - val_loss: -9.8576e-02 - val_acc: 0.9949 - val_mDice: 0.5636

Epoch 00105: val_mDice did not improve from 0.56666
Epoch 106/300
 - 71s - loss: 0.0316 - acc: 0.9962 - mDice: 0.9386 - val_loss: -9.8770e-02 - val_acc: 0.9948 - val_mDice: 0.5641

Epoch 00106: val_mDice did not improve from 0.56666
Epoch 107/300
 - 70s - loss: 0.0314 - acc: 0.9962 - mDice: 0.9390 - val_loss: -9.8825e-02 - val_acc: 0.9948 - val_mDice: 0.5642

Epoch 00107: val_mDice did not improve from 0.56666
Epoch 108/300
 - 70s - loss: 0.0316 - acc: 0.9962 - mDice: 0.9386 - val_loss: -9.8557e-02 - val_acc: 0.9948 - val_mDice: 0.5637

Epoch 00108: val_mDice did not improve from 0.56666

Epoch 00108: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.
Epoch 109/300
 - 69s - loss: 0.0314 - acc: 0.9962 - mDice: 0.9391 - val_loss: -9.8641e-02 - val_acc: 0.9948 - val_mDice: 0.5638

Epoch 00109: val_mDice did not improve from 0.56666
Epoch 110/300
 - 70s - loss: 0.0316 - acc: 0.9962 - mDice: 0.9387 - val_loss: -9.8689e-02 - val_acc: 0.9948 - val_mDice: 0.5639

Epoch 00110: val_mDice did not improve from 0.56666
Epoch 111/300
 - 70s - loss: 0.0314 - acc: 0.9962 - mDice: 0.9392 - val_loss: -9.8697e-02 - val_acc: 0.9948 - val_mDice: 0.5639

Epoch 00111: val_mDice did not improve from 0.56666
Epoch 112/300
 - 70s - loss: 0.0314 - acc: 0.9962 - mDice: 0.9390 - val_loss: -9.8654e-02 - val_acc: 0.9948 - val_mDice: 0.5638

Epoch 00112: val_mDice did not improve from 0.56666
Epoch 113/300
 - 70s - loss: 0.0316 - acc: 0.9962 - mDice: 0.9386 - val_loss: -9.8742e-02 - val_acc: 0.9948 - val_mDice: 0.5640

Epoch 00113: val_mDice did not improve from 0.56666
Epoch 114/300
 - 70s - loss: 0.0316 - acc: 0.9962 - mDice: 0.9386 - val_loss: -9.8785e-02 - val_acc: 0.9949 - val_mDice: 0.5641

Epoch 00114: val_mDice did not improve from 0.56666
Restoring model weights from the end of the best epoch
Epoch 00114: early stopping
{'val_loss': [0.012829472997573891, 0.010849696518194795, -0.01713148241091256, -0.037637744926744036, -0.029466505265898175, -0.0207694172106608, -0.019218658407529194, 0.0013947367592893466, -0.017845308314068147, -0.029278856485781043, -0.020182395297469513, -0.01112586244790241, -0.023899673775892066, -0.0612902847504375, -0.0525652982971885, -0.059584355113482236, -0.05779517268893695, -0.07607480070807716, -0.0688772505581981, -0.081417404491492, -0.08972645915969453, -0.09526469547188643, -0.09397467447802274, -0.09468595171817626, -0.09554342707299222, -0.09529173867118479, -0.09522009569436612, -0.0932922571370698, -0.09052262282130694, -0.09643319775961866, -0.09316290064592554, -0.09611838397504104, -0.0951686789366332, -0.09606285513651491, -0.0940453682583992, -0.09372853299584051, -0.09610696471851281, -0.09519439292224971, -0.09723708654443423, -0.09508011873924371, -0.09470300075381693, -0.0963187331352571, -0.09706452981841684, -0.09581603629119469, -0.09545091204721519, -0.09767423826034623, -0.09772884890888676, -0.09671434936950905, -0.0952192878331801, -0.0972605175667941, -0.0959232837864847, -0.0978810852299435, -0.09742013831632305, -0.097935208843814, -0.09826358678666028, -0.09763467518819703, -0.09892401434104851, -0.09757888670822587, -0.09794298698655282, -0.09748689739993124, -0.09846811497000733, -0.09865207469674071, -0.10004057983557384, -0.09888406026393476, -0.09733729294002658, -0.09931498050990731, -0.0996438664468852, -0.09849032767192283, -0.09813235624872073, -0.09869638100416973, -0.09930418296293779, -0.09865578692970854, -0.09892510077116465, -0.10005769558777713, -0.0976815688790697, -0.0992333119866824, -0.09901468169809592, -0.09842709933567528, -0.09813022192078408, -0.09866284816102548, -0.09846713115470578, -0.0983240996120554, -0.09885949603836945, -0.0987037428927542, -0.09837946615586377, -0.09890061647000939, -0.09861922290439558, -0.09865008524120455, -0.09915075992996042, -0.09907086972485889, -0.09916616419349054, -0.0982797308401628, -0.0988641981825684, -0.09889452592140496, -0.09873237859721136, -0.0991837021076318, -0.09911842151272177, -0.09870219998287433, -0.0990397169282942, -0.09885069137119283, -0.09878023107997094, -0.0982631974750095, -0.09868779996729861, -0.09762484674351384, -0.09857553273740441, -0.09876987624047982, -0.09882499890947583, -0.09855732833496247, -0.09864071685105863, -0.09868907412946826, -0.09869687834923918, -0.09865371137857437, -0.09874153453292268, -0.09878489043977526], 'val_acc': [0.9931980022276291, 0.9938905344466971, 0.9941280484199524, 0.9942929500883276, 0.9943947882363291, 0.994577845238676, 0.9946661398868368, 0.9946461482481523, 0.9946350687079959, 0.994820278100293, 0.9947910492468361, 0.994900884351345, 0.9949331897677798, 0.9947458249751968, 0.9950254866571138, 0.9948264342365842, 0.994933499832346, 0.9947655155803218, 0.9948507381810082, 0.9949276477399499, 0.9949335028426816, 0.9948713484436574, 0.994851660848868, 0.9948793469053326, 0.9949341109304717, 0.9948085874620111, 0.9948172045476509, 0.9948750451357677, 0.9949153475087098, 0.9948495039434144, 0.9949805709448728, 0.9949227318619237, 0.9948476631231983, 0.994654142194324, 0.9948959579371442, 0.9949181140071214, 0.9947781334019671, 0.9948313546301139, 0.9948858116010223, 0.994922424807693, 0.9948267412908149, 0.9948602734190045, 0.9948784257426406, 0.9948861186552529, 0.9948002789357696, 0.9948633484768145, 0.9948147375776311, 0.9948851989977288, 0.9948267397856472, 0.9947310577739369, 0.9948356669358532, 0.9948553515203071, 0.9947495171518037, 0.9948215093275513, 0.9948781141729066, 0.9948279695077376, 0.9947885867923197, 0.9947990537291825, 0.9947307522248741, 0.9946993694762991, 0.9948147405879666, 0.9948390460375583, 0.9948455092280802, 0.9947741281504583, 0.9948861201604208, 0.9948347382473223, 0.9948485827807224, 0.9948085844516754, 0.9947873600805649, 0.9948325858573721, 0.9947962827152677, 0.9948368966579437, 0.9947716672011097, 0.9947922849895978, 0.9948753521899985, 0.9948073562347528, 0.9948251984938227, 0.9948285851213667, 0.99485565706937, 0.9947938202607511, 0.9948375077560695, 0.9947882812432568, 0.994812591208352, 0.9947892054162845, 0.9948655068874359, 0.9947775147780024, 0.9948491998995194, 0.9947993577730776, 0.9948433538277944, 0.9948110469061919, 0.9947901250738086, 0.9948861141397496, 0.9948101287538355, 0.9948322803083093, 0.9948036655633137, 0.9947904366435427, 0.9947648969563571, 0.9947888998672215, 0.9947925860231573, 0.9947922819792622, 0.994762439017344, 0.9948344326982594, 0.9947922819792622, 0.9948802710783602, 0.9948535076897553, 0.9948313561352816, 0.994798129556155, 0.9947845890666499, 0.9948273523889407, 0.994812279638618, 0.9948172030424831, 0.994838120359363, 0.9948073577399206, 0.9948510497507422], 'val_mDice': [0.5344902957328642, 0.5429062096759526, 0.5459572871526083, 0.5519282456600305, 0.5550495542661108, 0.5516638840087736, 0.5509175834330645, 0.5539314313368364, 0.5540318004410676, 0.5479246152169777, 0.5522207373922522, 0.5531551192204157, 0.5535369388984911, 0.5508604292014633, 0.5516478938586784, 0.5530260331542766, 0.5511809272614743, 0.556923105249727, 0.5553898177283824, 0.5508202037370191, 0.5524665162418828, 0.5591284558929578, 0.5576104904064024, 0.558333606129945, 0.5587676111044306, 0.557497524552875, 0.5575900628334947, 0.5588605326384005, 0.5534772335956193, 0.560811258701965, 0.5539493529935076, 0.5595298672113755, 0.5572674818562738, 0.5644699329077595, 0.5550859740587196, 0.5567176858283053, 0.5594831302460997, 0.5573686742872903, 0.5617941313921803, 0.557152373020095, 0.5559574270790274, 0.5592240167386604, 0.5606431739799904, 0.5585285336381257, 0.5574607612057165, 0.5621392325018392, 0.5619979852979834, 0.55993669638128, 0.5569289515566345, 0.5610855111871103, 0.5583369491076229, 0.5622456553909514, 0.5613908958103921, 0.5624065886844288, 0.563017941936098, 0.5617742970435307, 0.5643922323831404, 0.5616944870262435, 0.5624643250396757, 0.5615751693646113, 0.5634714331891801, 0.5638346452303608, 0.5665917619310245, 0.5643226921257346, 0.5611445100018473, 0.5651525611227209, 0.5657988432982956, 0.5635188745729851, 0.5628197911110792, 0.5639194343427215, 0.565180533161067, 0.5638189267630529, 0.564425771284585, 0.5666567392722525, 0.5618433345748921, 0.5649816959795325, 0.5645606162572148, 0.5633816446619805, 0.5627538030496751, 0.5638624351434033, 0.5634415691549127, 0.5631928416815671, 0.5642499955314578, 0.5639526466227541, 0.5632469810620703, 0.564348285246377, 0.5637444954628896, 0.5638452965502787, 0.5648161892337028, 0.5646839068092481, 0.5648912165803138, 0.5630408927346721, 0.564261805077996, 0.5643078443979976, 0.5640013696569385, 0.5649096902572748, 0.5647950273270559, 0.5639436344305674, 0.5646150130214114, 0.5642431628222417, 0.5641220321859977, 0.5630376731807535, 0.5639131043595497, 0.5617253903788749, 0.5636497150466899, 0.5640547248450193, 0.5641862253348032, 0.5636544111702178, 0.5638005072721327, 0.5638956587121944, 0.5639147909000667, 0.5638177602580099, 0.5640063893915427, 0.5640721825337169], 'loss': [0.1368339296762986, 0.07589123216186956, 0.06657688800175908, 0.06174502895813082, 0.05821096969376203, 0.05575989180055126, 0.05355712377363136, 0.05162846311640926, 0.050366937050368064, 0.04943697287789554, 0.04808257311592149, 0.04703614659806806, 0.04619412996652498, 0.045663299524384124, 0.044996953215651704, 0.04403863328401182, 0.044223513420522546, 0.043072707165466156, 0.04277885844223871, 0.042168831372263606, 0.041705646071526194, 0.0411250563208286, 0.040648124206656915, 0.04073723306221249, 0.039923880016189954, 0.04004768054906666, 0.039258402763495945, 0.03919835091031206, 0.03862222447508456, 0.038294539337403205, 0.038227707868510555, 0.037769190680368596, 0.037821893581356035, 0.037405294581582725, 0.03747479717746043, 0.03684751976893834, 0.03659343090627543, 0.036607048066243285, 0.03678076310667547, 0.03588995129092567, 0.036181769041308515, 0.03576934743488521, 0.035449236239523826, 0.03537401089706779, 0.035177935799861657, 0.03513323756499012, 0.03480668827972812, 0.0347069476916441, 0.034427440306609366, 0.034387387551733865, 0.034383981940779275, 0.03398929420219304, 0.03392126054230851, 0.033735846557468786, 0.03386577312665281, 0.03366455906635313, 0.03425401962786944, 0.03374220622419847, 0.03344594497864161, 0.03362796290967269, 0.03341435860119291, 0.03336687086117239, 0.033239329073570266, 0.033313706048147367, 0.03300072896619844, 0.03289369844728401, 0.0329816659496039, 0.03303973861849312, 0.03274803638232501, 0.03276082104087349, 0.03260119255364216, 0.0325974766948358, 0.03265971826798672, 0.032436597468440936, 0.03245385159896527, 0.03225097641405809, 0.03272259432807691, 0.03221871744620777, 0.03208709126134375, 0.03214469587180581, 0.03195743782154748, 0.03191023671018369, 0.03211681834476726, 0.03188418815253261, 0.031904605962449206, 0.032154252092380056, 0.032234844736515035, 0.03217750967164411, 0.03182542603912174, 0.03223660013451387, 0.031822687264581234, 0.03180521159515455, 0.03155215921801344, 0.03176635909162287, 0.03171170307312355, 0.03200172486700344, 0.03181011839680432, 0.03160534220132924, 0.03153374543085476, 0.03163179530790987, 0.031532696999166755, 0.03148103405834927, 0.03151777054960502, 0.03139606423072503, 0.031388482501593956, 0.03162892160829839, 0.031423510838565274, 0.0316423828041634, 0.03139312111535299, 0.031581352505286236, 0.031350422570844014, 0.03143434386508583, 0.031645561476827504, 0.03161522764295279], 'acc': [0.9858751039172426, 0.9919500668771176, 0.9928362335845087, 0.9933094891013748, 0.9936557014932305, 0.9938980347836657, 0.9941063563437879, 0.9942811272614306, 0.9944196234742012, 0.9945310077751743, 0.9946428887921693, 0.9947332791348091, 0.9948204374349134, 0.9949034139666777, 0.9949726730518011, 0.9950286942023779, 0.995058252006651, 0.9951408623405288, 0.9951703812048895, 0.9952264071814256, 0.9952578407930878, 0.9952995526418769, 0.9953613898827979, 0.9953755774451813, 0.9954184030044261, 0.9954475307956993, 0.9954834441158658, 0.9955041916478496, 0.9955432440991203, 0.9955681486946213, 0.9955843994164001, 0.9956045628411809, 0.9956366843586375, 0.995656587492552, 0.9956737129609974, 0.9956964279637824, 0.9957147237295063, 0.9957442426662777, 0.9957598759378613, 0.99578556747994, 0.9957885582500636, 0.9958108554048443, 0.9958325041559905, 0.9958363261326599, 0.9958452150088805, 0.9958750825945952, 0.9958897960033942, 0.9959008483533696, 0.9959179565071417, 0.995945185511281, 0.9959497996821733, 0.9959572790758349, 0.9959596822673018, 0.9959753404907565, 0.9959780956577989, 0.995989775757325, 0.9959952125690049, 0.9960026824385312, 0.9960086335833273, 0.9960170881019066, 0.9960190023252954, 0.9960205305230474, 0.9960268775498526, 0.9960459703059426, 0.9960421177208494, 0.9960558655075685, 0.9960580592745476, 0.9960565575620695, 0.9960492307973597, 0.9960674320415792, 0.9960681104487953, 0.996097255147964, 0.9960831421834971, 0.9960941648450905, 0.9960862981394377, 0.996108717357341, 0.9961011770364747, 0.9961186112598339, 0.9961325116925426, 0.996131910815876, 0.9961295292709453, 0.9961306532722375, 0.9961255026787097, 0.9961369740693359, 0.9961406565148693, 0.9961353758674865, 0.9961496542754671, 0.9961457018147379, 0.996151113018591, 0.9961364251409487, 0.9961510297761161, 0.9961572723355959, 0.996160363590457, 0.9961600151970809, 0.9961745415988892, 0.9961716294880699, 0.9961679767351781, 0.9961632145823963, 0.9961721166927349, 0.9961797650375691, 0.9961853639682603, 0.9961791732250168, 0.9961642377581947, 0.9961760475921168, 0.9961757861980524, 0.9961782937077638, 0.9961916583524953, 0.9961822678576235, 0.9961849674473333, 0.9961864640825443, 0.996189041915812, 0.9961749679956141, 0.9961735838908715, 0.9961899168371162], 'mDice': [0.7339140902061969, 0.8524096592255489, 0.8705534011676099, 0.879954744691689, 0.8868356317374366, 0.8916084975590935, 0.8958980879220702, 0.8996609233325882, 0.9021099147534898, 0.9039094122156315, 0.9065559361062513, 0.9086023417193692, 0.9102369914587473, 0.9112558235008115, 0.912555260938025, 0.9144356920689586, 0.9140487483584785, 0.9163088046154605, 0.9168796149758437, 0.9180696168679202, 0.9189780935840506, 0.9201176182336992, 0.9210383090345228, 0.9208503665914531, 0.9224534880379353, 0.9221895950742783, 0.9237486753372763, 0.9238559857022023, 0.9249883674091263, 0.925628062519063, 0.9257530716172019, 0.9266540355210766, 0.9265315805076296, 0.9273582791644891, 0.9272062712472688, 0.9284479220154482, 0.9289435226889922, 0.9289025562656404, 0.9285468749070247, 0.930316967008692, 0.9297309878785097, 0.9305407831334657, 0.9311672084800989, 0.9313099226814615, 0.9317004038989661, 0.9317753092330355, 0.9324201644329488, 0.9326142043628148, 0.9331638307081063, 0.9332295251194208, 0.9332287013967325, 0.9340145211598538, 0.9341504499664958, 0.9345117821210228, 0.9342483111370388, 0.9346486523006832, 0.9334652189226716, 0.9344823065646426, 0.9350714220338582, 0.9347070356982099, 0.9351290326799641, 0.9352224474509191, 0.9354778151614032, 0.9353205181716037, 0.9359458612531874, 0.936151335105408, 0.935974544208567, 0.9358558763209512, 0.9364406955788644, 0.9364086015327451, 0.9367261362679796, 0.9367249136816655, 0.9366033187474614, 0.9370443196581564, 0.9370107283868918, 0.9374090349678058, 0.9364680404188072, 0.9374671692858769, 0.9377220679859463, 0.9376070090525854, 0.9379816507417409, 0.9380736659035471, 0.9376626474410854, 0.9381232488580032, 0.938079265170735, 0.9375820990760861, 0.9374158537757994, 0.9375303214005336, 0.9382295515920016, 0.9374174210806894, 0.9382358077455232, 0.9382707050628089, 0.9387688338894994, 0.9383435363732175, 0.9384472422475075, 0.9378669270360764, 0.9382538236396789, 0.9386662533651506, 0.9388037884708608, 0.9386073768162313, 0.9388016025583116, 0.9389082598469838, 0.9388350740857253, 0.9390738120662415, 0.9390931205763653, 0.9386123906536561, 0.9390145481329203, 0.9385808998524586, 0.9390783616936543, 0.9387053499464761, 0.939165249627158, 0.9389965314763227, 0.9385753319774337, 0.9386287427887331], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:00<00:02,  1.44it/s]predicting test subjects:  50%|█████     | 2/4 [00:00<00:01,  1.86it/s]predicting test subjects:  75%|███████▌  | 3/4 [00:01<00:00,  2.28it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  2.81it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  3.24it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:02,  4.21it/s]predicting train subjects:   1%|          | 2/266 [00:00<01:01,  4.30it/s]predicting train subjects:   1%|          | 3/266 [00:00<00:54,  4.81it/s]predicting train subjects:   2%|▏         | 4/266 [00:00<00:52,  5.02it/s]predicting train subjects:   2%|▏         | 5/266 [00:01<00:54,  4.77it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<00:51,  5.01it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<00:49,  5.21it/s]predicting train subjects:   3%|▎         | 8/266 [00:01<00:48,  5.37it/s]predicting train subjects:   3%|▎         | 9/266 [00:01<00:46,  5.49it/s]predicting train subjects:   4%|▍         | 10/266 [00:01<00:45,  5.57it/s]predicting train subjects:   4%|▍         | 11/266 [00:02<00:45,  5.64it/s]predicting train subjects:   5%|▍         | 12/266 [00:02<00:46,  5.47it/s]predicting train subjects:   5%|▍         | 13/266 [00:02<00:45,  5.53it/s]predicting train subjects:   5%|▌         | 14/266 [00:02<00:45,  5.51it/s]predicting train subjects:   6%|▌         | 15/266 [00:02<00:44,  5.58it/s]predicting train subjects:   6%|▌         | 16/266 [00:02<00:44,  5.64it/s]predicting train subjects:   6%|▋         | 17/266 [00:03<00:43,  5.71it/s]predicting train subjects:   7%|▋         | 18/266 [00:03<00:43,  5.71it/s]predicting train subjects:   7%|▋         | 19/266 [00:03<00:43,  5.72it/s]predicting train subjects:   8%|▊         | 20/266 [00:03<00:42,  5.74it/s]predicting train subjects:   8%|▊         | 21/266 [00:03<00:42,  5.73it/s]predicting train subjects:   8%|▊         | 22/266 [00:04<00:42,  5.73it/s]predicting train subjects:   9%|▊         | 23/266 [00:04<00:42,  5.74it/s]predicting train subjects:   9%|▉         | 24/266 [00:04<00:41,  5.86it/s]predicting train subjects:   9%|▉         | 25/266 [00:04<00:40,  5.96it/s]predicting train subjects:  10%|▉         | 26/266 [00:04<00:39,  6.00it/s]predicting train subjects:  10%|█         | 27/266 [00:04<00:40,  5.90it/s]predicting train subjects:  11%|█         | 28/266 [00:05<00:40,  5.89it/s]predicting train subjects:  11%|█         | 29/266 [00:05<00:40,  5.88it/s]predicting train subjects:  11%|█▏        | 30/266 [00:05<00:40,  5.89it/s]predicting train subjects:  12%|█▏        | 31/266 [00:05<00:40,  5.83it/s]predicting train subjects:  12%|█▏        | 32/266 [00:05<00:40,  5.75it/s]predicting train subjects:  12%|█▏        | 33/266 [00:05<00:39,  5.83it/s]predicting train subjects:  13%|█▎        | 34/266 [00:06<00:39,  5.86it/s]predicting train subjects:  13%|█▎        | 35/266 [00:06<00:39,  5.88it/s]predicting train subjects:  14%|█▎        | 36/266 [00:06<00:39,  5.86it/s]predicting train subjects:  14%|█▍        | 37/266 [00:06<00:39,  5.86it/s]predicting train subjects:  14%|█▍        | 38/266 [00:06<00:38,  5.85it/s]predicting train subjects:  15%|█▍        | 39/266 [00:06<00:39,  5.71it/s]predicting train subjects:  15%|█▌        | 40/266 [00:07<00:39,  5.71it/s]predicting train subjects:  15%|█▌        | 41/266 [00:07<00:39,  5.69it/s]predicting train subjects:  16%|█▌        | 42/266 [00:07<00:37,  5.93it/s]predicting train subjects:  16%|█▌        | 43/266 [00:07<00:37,  5.92it/s]predicting train subjects:  17%|█▋        | 44/266 [00:07<00:37,  5.95it/s]predicting train subjects:  17%|█▋        | 45/266 [00:07<00:36,  6.02it/s]predicting train subjects:  17%|█▋        | 46/266 [00:08<00:35,  6.12it/s]predicting train subjects:  18%|█▊        | 47/266 [00:08<00:34,  6.30it/s]predicting train subjects:  18%|█▊        | 48/266 [00:08<00:33,  6.45it/s]predicting train subjects:  18%|█▊        | 49/266 [00:08<00:33,  6.45it/s]predicting train subjects:  19%|█▉        | 50/266 [00:08<00:33,  6.48it/s]predicting train subjects:  19%|█▉        | 51/266 [00:08<00:32,  6.56it/s]predicting train subjects:  20%|█▉        | 52/266 [00:08<00:32,  6.58it/s]predicting train subjects:  20%|█▉        | 53/266 [00:09<00:33,  6.29it/s]predicting train subjects:  20%|██        | 54/266 [00:09<00:33,  6.39it/s]predicting train subjects:  21%|██        | 55/266 [00:09<00:33,  6.34it/s]predicting train subjects:  21%|██        | 56/266 [00:09<00:32,  6.42it/s]predicting train subjects:  21%|██▏       | 57/266 [00:09<00:32,  6.43it/s]predicting train subjects:  22%|██▏       | 58/266 [00:09<00:32,  6.50it/s]predicting train subjects:  22%|██▏       | 59/266 [00:10<00:31,  6.51it/s]predicting train subjects:  23%|██▎       | 60/266 [00:10<00:31,  6.64it/s]predicting train subjects:  23%|██▎       | 61/266 [00:10<00:30,  6.79it/s]predicting train subjects:  23%|██▎       | 62/266 [00:10<00:29,  6.85it/s]predicting train subjects:  24%|██▎       | 63/266 [00:10<00:29,  6.79it/s]predicting train subjects:  24%|██▍       | 64/266 [00:10<00:29,  6.82it/s]predicting train subjects:  24%|██▍       | 65/266 [00:10<00:29,  6.80it/s]predicting train subjects:  25%|██▍       | 66/266 [00:11<00:30,  6.51it/s]predicting train subjects:  25%|██▌       | 67/266 [00:11<00:30,  6.61it/s]predicting train subjects:  26%|██▌       | 68/266 [00:11<00:29,  6.72it/s]predicting train subjects:  26%|██▌       | 69/266 [00:11<00:30,  6.48it/s]predicting train subjects:  26%|██▋       | 70/266 [00:11<00:29,  6.62it/s]predicting train subjects:  27%|██▋       | 71/266 [00:11<00:28,  6.74it/s]predicting train subjects:  27%|██▋       | 72/266 [00:11<00:28,  6.83it/s]predicting train subjects:  27%|██▋       | 73/266 [00:12<00:27,  6.92it/s]predicting train subjects:  28%|██▊       | 74/266 [00:12<00:27,  6.96it/s]predicting train subjects:  28%|██▊       | 75/266 [00:12<00:27,  6.97it/s]predicting train subjects:  29%|██▊       | 76/266 [00:12<00:27,  7.00it/s]predicting train subjects:  29%|██▉       | 77/266 [00:12<00:27,  6.99it/s]predicting train subjects:  29%|██▉       | 78/266 [00:12<00:29,  6.33it/s]predicting train subjects:  30%|██▉       | 79/266 [00:13<00:30,  6.15it/s]predicting train subjects:  30%|███       | 80/266 [00:13<00:30,  6.03it/s]predicting train subjects:  30%|███       | 81/266 [00:13<00:31,  5.94it/s]predicting train subjects:  31%|███       | 82/266 [00:13<00:31,  5.92it/s]predicting train subjects:  31%|███       | 83/266 [00:13<00:31,  5.85it/s]predicting train subjects:  32%|███▏      | 84/266 [00:13<00:31,  5.70it/s]predicting train subjects:  32%|███▏      | 85/266 [00:14<00:31,  5.73it/s]predicting train subjects:  32%|███▏      | 86/266 [00:14<00:31,  5.72it/s]predicting train subjects:  33%|███▎      | 87/266 [00:14<00:31,  5.77it/s]predicting train subjects:  33%|███▎      | 88/266 [00:14<00:30,  5.79it/s]predicting train subjects:  33%|███▎      | 89/266 [00:14<00:30,  5.76it/s]predicting train subjects:  34%|███▍      | 90/266 [00:14<00:30,  5.76it/s]predicting train subjects:  34%|███▍      | 91/266 [00:15<00:30,  5.72it/s]predicting train subjects:  35%|███▍      | 92/266 [00:15<00:30,  5.75it/s]predicting train subjects:  35%|███▍      | 93/266 [00:15<00:30,  5.72it/s]predicting train subjects:  35%|███▌      | 94/266 [00:15<00:30,  5.70it/s]predicting train subjects:  36%|███▌      | 95/266 [00:15<00:30,  5.68it/s]predicting train subjects:  36%|███▌      | 96/266 [00:16<00:32,  5.25it/s]predicting train subjects:  36%|███▋      | 97/266 [00:16<00:34,  4.88it/s]predicting train subjects:  37%|███▋      | 98/266 [00:16<00:35,  4.75it/s]predicting train subjects:  37%|███▋      | 99/266 [00:16<00:31,  5.33it/s]predicting train subjects:  38%|███▊      | 100/266 [00:16<00:30,  5.36it/s]predicting train subjects:  38%|███▊      | 101/266 [00:17<00:29,  5.67it/s]predicting train subjects:  38%|███▊      | 102/266 [00:17<00:27,  5.93it/s]predicting train subjects:  39%|███▊      | 103/266 [00:17<00:27,  6.01it/s]predicting train subjects:  39%|███▉      | 104/266 [00:17<00:26,  6.08it/s]predicting train subjects:  39%|███▉      | 105/266 [00:17<00:25,  6.19it/s]predicting train subjects:  40%|███▉      | 106/266 [00:17<00:25,  6.22it/s]predicting train subjects:  40%|████      | 107/266 [00:17<00:25,  6.32it/s]predicting train subjects:  41%|████      | 108/266 [00:18<00:24,  6.37it/s]predicting train subjects:  41%|████      | 109/266 [00:18<00:24,  6.40it/s]predicting train subjects:  41%|████▏     | 110/266 [00:18<00:24,  6.39it/s]predicting train subjects:  42%|████▏     | 111/266 [00:18<00:24,  6.41it/s]predicting train subjects:  42%|████▏     | 112/266 [00:18<00:23,  6.42it/s]predicting train subjects:  42%|████▏     | 113/266 [00:18<00:23,  6.43it/s]predicting train subjects:  43%|████▎     | 114/266 [00:19<00:23,  6.46it/s]predicting train subjects:  43%|████▎     | 115/266 [00:19<00:23,  6.47it/s]predicting train subjects:  44%|████▎     | 116/266 [00:19<00:23,  6.50it/s]predicting train subjects:  44%|████▍     | 117/266 [00:19<00:23,  6.44it/s]predicting train subjects:  44%|████▍     | 118/266 [00:19<00:22,  6.46it/s]predicting train subjects:  45%|████▍     | 119/266 [00:19<00:23,  6.28it/s]predicting train subjects:  45%|████▌     | 120/266 [00:20<00:23,  6.09it/s]predicting train subjects:  45%|████▌     | 121/266 [00:20<00:24,  6.00it/s]predicting train subjects:  46%|████▌     | 122/266 [00:20<00:24,  5.96it/s]predicting train subjects:  46%|████▌     | 123/266 [00:20<00:24,  5.92it/s]predicting train subjects:  47%|████▋     | 124/266 [00:20<00:24,  5.91it/s]predicting train subjects:  47%|████▋     | 125/266 [00:20<00:23,  5.89it/s]predicting train subjects:  47%|████▋     | 126/266 [00:21<00:23,  5.85it/s]predicting train subjects:  48%|████▊     | 127/266 [00:21<00:24,  5.78it/s]predicting train subjects:  48%|████▊     | 128/266 [00:21<00:23,  5.81it/s]predicting train subjects:  48%|████▊     | 129/266 [00:21<00:23,  5.80it/s]predicting train subjects:  49%|████▉     | 130/266 [00:21<00:23,  5.81it/s]predicting train subjects:  49%|████▉     | 131/266 [00:21<00:23,  5.79it/s]predicting train subjects:  50%|████▉     | 132/266 [00:22<00:22,  5.83it/s]predicting train subjects:  50%|█████     | 133/266 [00:22<00:22,  5.84it/s]predicting train subjects:  50%|█████     | 134/266 [00:22<00:22,  5.87it/s]predicting train subjects:  51%|█████     | 135/266 [00:22<00:22,  5.71it/s]predicting train subjects:  51%|█████     | 136/266 [00:22<00:22,  5.69it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:22<00:22,  5.83it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:23<00:21,  5.94it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:23<00:20,  6.05it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:23<00:20,  6.09it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:23<00:20,  6.03it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:23<00:20,  6.07it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:23<00:20,  6.08it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:24<00:19,  6.13it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:24<00:19,  6.11it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:24<00:19,  6.16it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:24<00:19,  6.20it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:24<00:19,  6.20it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:24<00:19,  6.07it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:25<00:18,  6.16it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:25<00:18,  6.21it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:25<00:18,  6.19it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:25<00:18,  6.22it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:25<00:18,  6.15it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:25<00:17,  6.41it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:25<00:16,  6.67it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:26<00:15,  6.82it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:26<00:15,  6.78it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:26<00:15,  6.89it/s]predicting train subjects:  60%|██████    | 160/266 [00:26<00:15,  7.02it/s]predicting train subjects:  61%|██████    | 161/266 [00:26<00:14,  7.11it/s]predicting train subjects:  61%|██████    | 162/266 [00:26<00:14,  7.15it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:26<00:14,  7.19it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:27<00:13,  7.29it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:27<00:13,  7.32it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:27<00:13,  7.41it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:27<00:13,  7.42it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:27<00:13,  7.25it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:27<00:13,  7.16it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:27<00:13,  7.15it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:28<00:13,  7.19it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:28<00:13,  7.08it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:28<00:13,  6.99it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:28<00:13,  6.99it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:28<00:13,  6.97it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:28<00:13,  6.89it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:28<00:12,  6.88it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:29<00:12,  6.87it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:29<00:13,  6.67it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:29<00:12,  6.68it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:29<00:12,  6.71it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:29<00:12,  6.71it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:29<00:12,  6.70it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:29<00:12,  6.76it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:30<00:12,  6.73it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:30<00:11,  6.79it/s]predicting train subjects:  70%|███████   | 187/266 [00:30<00:11,  6.84it/s]predicting train subjects:  71%|███████   | 188/266 [00:30<00:11,  6.58it/s]predicting train subjects:  71%|███████   | 189/266 [00:30<00:11,  6.64it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:30<00:11,  6.70it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:31<00:11,  6.32it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:31<00:13,  5.66it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:31<00:12,  6.04it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:31<00:13,  5.51it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:31<00:12,  5.87it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:31<00:11,  6.15it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:32<00:10,  6.31it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:32<00:10,  6.50it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:32<00:10,  6.63it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:32<00:09,  6.69it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:32<00:09,  6.62it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:32<00:09,  6.68it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:32<00:09,  6.77it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:33<00:09,  6.83it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:33<00:08,  6.88it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:33<00:08,  6.87it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:33<00:08,  6.88it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:33<00:08,  6.77it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:33<00:08,  6.80it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:33<00:08,  6.81it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:34<00:08,  6.82it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:34<00:07,  6.81it/s]predicting train subjects:  80%|████████  | 213/266 [00:34<00:07,  7.03it/s]predicting train subjects:  80%|████████  | 214/266 [00:34<00:07,  7.17it/s]predicting train subjects:  81%|████████  | 215/266 [00:34<00:07,  7.27it/s]predicting train subjects:  81%|████████  | 216/266 [00:34<00:06,  7.39it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:34<00:06,  7.42it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:35<00:06,  7.48it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:35<00:06,  7.53it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:35<00:06,  7.53it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:35<00:06,  7.46it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:35<00:05,  7.50it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:35<00:05,  7.51it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:35<00:05,  7.47it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:35<00:05,  7.43it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:36<00:05,  7.45it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:36<00:05,  7.37it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:36<00:05,  7.30it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:36<00:05,  7.18it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:36<00:04,  7.22it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:36<00:04,  7.15it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:36<00:04,  6.98it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:37<00:04,  7.00it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:37<00:04,  7.00it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:37<00:04,  7.01it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:37<00:04,  6.96it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:37<00:04,  7.00it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:37<00:03,  7.01it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:37<00:03,  6.99it/s]predicting train subjects:  90%|█████████ | 240/266 [00:38<00:03,  7.03it/s]predicting train subjects:  91%|█████████ | 241/266 [00:38<00:03,  7.01it/s]predicting train subjects:  91%|█████████ | 242/266 [00:38<00:03,  7.04it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:38<00:03,  6.97it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:38<00:03,  7.00it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:38<00:02,  7.04it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:38<00:02,  7.08it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:39<00:02,  7.10it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:39<00:02,  6.93it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:39<00:02,  6.63it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:39<00:02,  6.46it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:39<00:02,  6.31it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:39<00:02,  6.08it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:40<00:02,  5.94it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:40<00:02,  5.97it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:40<00:01,  6.01it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:40<00:01,  5.97it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:40<00:01,  6.01it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:40<00:01,  6.05it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:41<00:01,  5.95it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:41<00:01,  5.88it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:41<00:00,  5.88it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:41<00:00,  5.81it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:41<00:00,  5.86it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:41<00:00,  5.80it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:42<00:00,  5.84it/s]predicting train subjects: 100%|██████████| 266/266 [00:42<00:00,  5.80it/s]predicting train subjects: 100%|██████████| 266/266 [00:42<00:00,  6.29it/s]
saving BB  test1-THALAMUS:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 4/4 [00:00<00:00, 79.10it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 8/266 [00:00<00:03, 70.97it/s]saving BB  train1-THALAMUS:   6%|▌         | 15/266 [00:00<00:03, 70.59it/s]saving BB  train1-THALAMUS:   8%|▊         | 22/266 [00:00<00:03, 68.89it/s]saving BB  train1-THALAMUS:  11%|█▏        | 30/266 [00:00<00:03, 69.05it/s]saving BB  train1-THALAMUS:  14%|█▍        | 38/266 [00:00<00:03, 70.66it/s]saving BB  train1-THALAMUS:  17%|█▋        | 46/266 [00:00<00:03, 72.90it/s]saving BB  train1-THALAMUS:  21%|██        | 55/266 [00:00<00:02, 75.46it/s]saving BB  train1-THALAMUS:  24%|██▍       | 64/266 [00:00<00:02, 78.64it/s]saving BB  train1-THALAMUS:  27%|██▋       | 73/266 [00:00<00:02, 81.64it/s]saving BB  train1-THALAMUS:  31%|███       | 82/266 [00:01<00:02, 81.51it/s]saving BB  train1-THALAMUS:  34%|███▍      | 91/266 [00:01<00:02, 78.97it/s]saving BB  train1-THALAMUS:  37%|███▋      | 99/266 [00:01<00:02, 78.27it/s]saving BB  train1-THALAMUS:  40%|████      | 107/266 [00:01<00:02, 77.89it/s]saving BB  train1-THALAMUS:  43%|████▎     | 115/266 [00:01<00:01, 76.51it/s]saving BB  train1-THALAMUS:  46%|████▌     | 123/266 [00:01<00:01, 75.74it/s]saving BB  train1-THALAMUS:  49%|████▉     | 131/266 [00:01<00:01, 74.33it/s]saving BB  train1-THALAMUS:  52%|█████▏    | 139/266 [00:01<00:01, 71.29it/s]saving BB  train1-THALAMUS:  55%|█████▌    | 147/266 [00:01<00:01, 71.46it/s]saving BB  train1-THALAMUS:  58%|█████▊    | 155/266 [00:02<00:01, 70.74it/s]saving BB  train1-THALAMUS:  61%|██████▏   | 163/266 [00:02<00:01, 70.59it/s]saving BB  train1-THALAMUS:  64%|██████▍   | 171/266 [00:02<00:01, 72.43it/s]saving BB  train1-THALAMUS:  68%|██████▊   | 181/266 [00:02<00:01, 77.32it/s]saving BB  train1-THALAMUS:  72%|███████▏  | 191/266 [00:02<00:00, 80.43it/s]saving BB  train1-THALAMUS:  75%|███████▌  | 200/266 [00:02<00:00, 80.75it/s]saving BB  train1-THALAMUS:  79%|███████▊  | 209/266 [00:02<00:00, 80.75it/s]saving BB  train1-THALAMUS:  82%|████████▏ | 218/266 [00:02<00:00, 81.40it/s]saving BB  train1-THALAMUS:  85%|████████▌ | 227/266 [00:02<00:00, 82.95it/s]saving BB  train1-THALAMUS:  89%|████████▊ | 236/266 [00:03<00:00, 84.60it/s]saving BB  train1-THALAMUS:  92%|█████████▏| 246/266 [00:03<00:00, 86.89it/s]saving BB  train1-THALAMUS:  96%|█████████▌| 255/266 [00:03<00:00, 85.29it/s]saving BB  train1-THALAMUS:  99%|█████████▉| 264/266 [00:03<00:00, 83.09it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 77.97it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<05:40,  1.29s/it]Loading train:   1%|          | 2/266 [00:02<05:18,  1.21s/it]Loading train:   1%|          | 3/266 [00:03<04:58,  1.14s/it]Loading train:   2%|▏         | 4/266 [00:04<04:35,  1.05s/it]Loading train:   2%|▏         | 5/266 [00:05<04:32,  1.04s/it]Loading train:   2%|▏         | 6/266 [00:05<04:15,  1.02it/s]Loading train:   3%|▎         | 7/266 [00:06<04:00,  1.08it/s]Loading train:   3%|▎         | 8/266 [00:07<03:48,  1.13it/s]Loading train:   3%|▎         | 9/266 [00:08<03:41,  1.16it/s]Loading train:   4%|▍         | 10/266 [00:09<03:36,  1.18it/s]Loading train:   4%|▍         | 11/266 [00:09<03:30,  1.21it/s]Loading train:   5%|▍         | 12/266 [00:10<03:31,  1.20it/s]Loading train:   5%|▍         | 13/266 [00:11<03:29,  1.21it/s]Loading train:   5%|▌         | 14/266 [00:12<03:22,  1.24it/s]Loading train:   6%|▌         | 15/266 [00:13<03:20,  1.25it/s]Loading train:   6%|▌         | 16/266 [00:13<03:18,  1.26it/s]Loading train:   6%|▋         | 17/266 [00:14<03:19,  1.25it/s]Loading train:   7%|▋         | 18/266 [00:15<03:19,  1.25it/s]Loading train:   7%|▋         | 19/266 [00:16<03:17,  1.25it/s]Loading train:   8%|▊         | 20/266 [00:17<03:17,  1.24it/s]Loading train:   8%|▊         | 21/266 [00:17<03:13,  1.27it/s]Loading train:   8%|▊         | 22/266 [00:18<03:10,  1.28it/s]Loading train:   9%|▊         | 23/266 [00:19<03:09,  1.29it/s]Loading train:   9%|▉         | 24/266 [00:20<03:01,  1.34it/s]Loading train:   9%|▉         | 25/266 [00:20<02:53,  1.39it/s]Loading train:  10%|▉         | 26/266 [00:21<02:50,  1.41it/s]Loading train:  10%|█         | 27/266 [00:22<02:45,  1.44it/s]Loading train:  11%|█         | 28/266 [00:22<02:42,  1.46it/s]Loading train:  11%|█         | 29/266 [00:23<02:42,  1.46it/s]Loading train:  11%|█▏        | 30/266 [00:24<02:41,  1.46it/s]Loading train:  12%|█▏        | 31/266 [00:24<02:38,  1.49it/s]Loading train:  12%|█▏        | 32/266 [00:25<02:36,  1.49it/s]Loading train:  12%|█▏        | 33/266 [00:26<02:35,  1.50it/s]Loading train:  13%|█▎        | 34/266 [00:26<02:33,  1.51it/s]Loading train:  13%|█▎        | 35/266 [00:27<02:32,  1.51it/s]Loading train:  14%|█▎        | 36/266 [00:28<02:32,  1.51it/s]Loading train:  14%|█▍        | 37/266 [00:28<02:30,  1.52it/s]Loading train:  14%|█▍        | 38/266 [00:29<02:31,  1.50it/s]Loading train:  15%|█▍        | 39/266 [00:30<02:33,  1.48it/s]Loading train:  15%|█▌        | 40/266 [00:30<02:31,  1.49it/s]Loading train:  15%|█▌        | 41/266 [00:31<02:30,  1.50it/s]Loading train:  16%|█▌        | 42/266 [00:32<02:30,  1.49it/s]Loading train:  16%|█▌        | 43/266 [00:32<02:26,  1.52it/s]Loading train:  17%|█▋        | 44/266 [00:33<02:24,  1.54it/s]Loading train:  17%|█▋        | 45/266 [00:34<02:21,  1.56it/s]Loading train:  17%|█▋        | 46/266 [00:34<02:21,  1.56it/s]Loading train:  18%|█▊        | 47/266 [00:35<02:19,  1.57it/s]Loading train:  18%|█▊        | 48/266 [00:35<02:19,  1.56it/s]Loading train:  18%|█▊        | 49/266 [00:36<02:18,  1.56it/s]Loading train:  19%|█▉        | 50/266 [00:37<02:18,  1.56it/s]Loading train:  19%|█▉        | 51/266 [00:37<02:17,  1.57it/s]Loading train:  20%|█▉        | 52/266 [00:38<02:16,  1.56it/s]Loading train:  20%|█▉        | 53/266 [00:39<02:16,  1.56it/s]Loading train:  20%|██        | 54/266 [00:39<02:17,  1.54it/s]Loading train:  21%|██        | 55/266 [00:40<02:18,  1.53it/s]Loading train:  21%|██        | 56/266 [00:41<02:18,  1.52it/s]Loading train:  21%|██▏       | 57/266 [00:41<02:19,  1.50it/s]Loading train:  22%|██▏       | 58/266 [00:42<02:16,  1.52it/s]Loading train:  22%|██▏       | 59/266 [00:43<02:16,  1.52it/s]Loading train:  23%|██▎       | 60/266 [00:43<02:13,  1.54it/s]Loading train:  23%|██▎       | 61/266 [00:44<02:09,  1.59it/s]Loading train:  23%|██▎       | 62/266 [00:44<02:07,  1.61it/s]Loading train:  24%|██▎       | 63/266 [00:45<02:03,  1.64it/s]Loading train:  24%|██▍       | 64/266 [00:46<02:02,  1.65it/s]Loading train:  24%|██▍       | 65/266 [00:46<01:59,  1.68it/s]Loading train:  25%|██▍       | 66/266 [00:47<02:00,  1.66it/s]Loading train:  25%|██▌       | 67/266 [00:47<02:01,  1.64it/s]Loading train:  26%|██▌       | 68/266 [00:48<01:58,  1.67it/s]Loading train:  26%|██▌       | 69/266 [00:49<01:57,  1.67it/s]Loading train:  26%|██▋       | 70/266 [00:49<01:57,  1.67it/s]Loading train:  27%|██▋       | 71/266 [00:50<01:57,  1.67it/s]Loading train:  27%|██▋       | 72/266 [00:50<01:56,  1.66it/s]Loading train:  27%|██▋       | 73/266 [00:51<01:55,  1.67it/s]Loading train:  28%|██▊       | 74/266 [00:52<01:55,  1.67it/s]Loading train:  28%|██▊       | 75/266 [00:52<01:53,  1.68it/s]Loading train:  29%|██▊       | 76/266 [00:53<01:51,  1.71it/s]Loading train:  29%|██▉       | 77/266 [00:53<01:50,  1.71it/s]Loading train:  29%|██▉       | 78/266 [00:54<01:57,  1.60it/s]Loading train:  30%|██▉       | 79/266 [00:55<02:00,  1.55it/s]Loading train:  30%|███       | 80/266 [00:55<02:01,  1.53it/s]Loading train:  30%|███       | 81/266 [00:56<02:01,  1.53it/s]Loading train:  31%|███       | 82/266 [00:57<02:02,  1.50it/s]Loading train:  31%|███       | 83/266 [00:57<02:00,  1.52it/s]Loading train:  32%|███▏      | 84/266 [00:58<02:00,  1.51it/s]Loading train:  32%|███▏      | 85/266 [00:59<01:58,  1.53it/s]Loading train:  32%|███▏      | 86/266 [00:59<01:58,  1.52it/s]Loading train:  33%|███▎      | 87/266 [01:00<01:57,  1.53it/s]Loading train:  33%|███▎      | 88/266 [01:01<01:56,  1.53it/s]Loading train:  33%|███▎      | 89/266 [01:01<01:55,  1.53it/s]Loading train:  34%|███▍      | 90/266 [01:02<01:54,  1.53it/s]Loading train:  34%|███▍      | 91/266 [01:03<01:53,  1.54it/s]Loading train:  35%|███▍      | 92/266 [01:03<01:52,  1.55it/s]Loading train:  35%|███▍      | 93/266 [01:04<01:52,  1.54it/s]Loading train:  35%|███▌      | 94/266 [01:05<01:53,  1.52it/s]Loading train:  36%|███▌      | 95/266 [01:05<01:53,  1.51it/s]Loading train:  36%|███▌      | 96/266 [01:06<02:07,  1.33it/s]Loading train:  36%|███▋      | 97/266 [01:07<02:19,  1.21it/s]Loading train:  37%|███▋      | 98/266 [01:08<02:24,  1.17it/s]Loading train:  37%|███▋      | 99/266 [01:09<02:19,  1.20it/s]Loading train:  38%|███▊      | 100/266 [01:10<02:18,  1.20it/s]Loading train:  38%|███▊      | 101/266 [01:10<02:10,  1.27it/s]Loading train:  38%|███▊      | 102/266 [01:11<02:04,  1.32it/s]Loading train:  39%|███▊      | 103/266 [01:12<01:58,  1.38it/s]Loading train:  39%|███▉      | 104/266 [01:12<01:53,  1.42it/s]Loading train:  39%|███▉      | 105/266 [01:13<01:51,  1.45it/s]Loading train:  40%|███▉      | 106/266 [01:14<01:50,  1.45it/s]Loading train:  40%|████      | 107/266 [01:14<01:48,  1.46it/s]Loading train:  41%|████      | 108/266 [01:15<01:48,  1.45it/s]Loading train:  41%|████      | 109/266 [01:16<01:46,  1.47it/s]Loading train:  41%|████▏     | 110/266 [01:17<01:46,  1.47it/s]Loading train:  42%|████▏     | 111/266 [01:17<01:45,  1.47it/s]Loading train:  42%|████▏     | 112/266 [01:18<01:43,  1.48it/s]Loading train:  42%|████▏     | 113/266 [01:19<01:42,  1.49it/s]Loading train:  43%|████▎     | 114/266 [01:19<01:43,  1.47it/s]Loading train:  43%|████▎     | 115/266 [01:20<01:43,  1.46it/s]Loading train:  44%|████▎     | 116/266 [01:21<01:43,  1.45it/s]Loading train:  44%|████▍     | 117/266 [01:21<01:41,  1.46it/s]Loading train:  44%|████▍     | 118/266 [01:22<01:40,  1.47it/s]Loading train:  45%|████▍     | 119/266 [01:23<01:39,  1.48it/s]Loading train:  45%|████▌     | 120/266 [01:23<01:38,  1.48it/s]Loading train:  45%|████▌     | 121/266 [01:24<01:37,  1.49it/s]Loading train:  46%|████▌     | 122/266 [01:25<01:35,  1.50it/s]Loading train:  46%|████▌     | 123/266 [01:25<01:35,  1.50it/s]Loading train:  47%|████▋     | 124/266 [01:26<01:33,  1.52it/s]Loading train:  47%|████▋     | 125/266 [01:27<01:32,  1.53it/s]Loading train:  47%|████▋     | 126/266 [01:27<01:31,  1.52it/s]Loading train:  48%|████▊     | 127/266 [01:28<01:30,  1.54it/s]Loading train:  48%|████▊     | 128/266 [01:28<01:29,  1.54it/s]Loading train:  48%|████▊     | 129/266 [01:29<01:28,  1.55it/s]Loading train:  49%|████▉     | 130/266 [01:30<01:28,  1.53it/s]Loading train:  49%|████▉     | 131/266 [01:30<01:28,  1.52it/s]Loading train:  50%|████▉     | 132/266 [01:31<01:28,  1.52it/s]Loading train:  50%|█████     | 133/266 [01:32<01:27,  1.51it/s]Loading train:  50%|█████     | 134/266 [01:32<01:28,  1.49it/s]Loading train:  51%|█████     | 135/266 [01:33<01:28,  1.49it/s]Loading train:  51%|█████     | 136/266 [01:34<01:28,  1.48it/s]Loading train:  52%|█████▏    | 137/266 [01:35<01:29,  1.45it/s]Loading train:  52%|█████▏    | 138/266 [01:35<01:28,  1.44it/s]Loading train:  52%|█████▏    | 139/266 [01:36<01:27,  1.45it/s]Loading train:  53%|█████▎    | 140/266 [01:37<01:27,  1.45it/s]Loading train:  53%|█████▎    | 141/266 [01:37<01:26,  1.45it/s]Loading train:  53%|█████▎    | 142/266 [01:38<01:25,  1.45it/s]Loading train:  54%|█████▍    | 143/266 [01:39<01:25,  1.43it/s]Loading train:  54%|█████▍    | 144/266 [01:39<01:24,  1.44it/s]Loading train:  55%|█████▍    | 145/266 [01:40<01:24,  1.44it/s]Loading train:  55%|█████▍    | 146/266 [01:41<01:22,  1.46it/s]Loading train:  55%|█████▌    | 147/266 [01:41<01:21,  1.47it/s]Loading train:  56%|█████▌    | 148/266 [01:42<01:20,  1.46it/s]Loading train:  56%|█████▌    | 149/266 [01:43<01:20,  1.45it/s]Loading train:  56%|█████▋    | 150/266 [01:44<01:19,  1.46it/s]Loading train:  57%|█████▋    | 151/266 [01:44<01:18,  1.46it/s]Loading train:  57%|█████▋    | 152/266 [01:45<01:19,  1.44it/s]Loading train:  58%|█████▊    | 153/266 [01:46<01:18,  1.44it/s]Loading train:  58%|█████▊    | 154/266 [01:46<01:17,  1.44it/s]Loading train:  58%|█████▊    | 155/266 [01:47<01:15,  1.48it/s]Loading train:  59%|█████▊    | 156/266 [01:48<01:12,  1.53it/s]Loading train:  59%|█████▉    | 157/266 [01:48<01:08,  1.58it/s]Loading train:  59%|█████▉    | 158/266 [01:49<01:06,  1.63it/s]Loading train:  60%|█████▉    | 159/266 [01:49<01:03,  1.67it/s]Loading train:  60%|██████    | 160/266 [01:50<01:02,  1.70it/s]Loading train:  61%|██████    | 161/266 [01:50<01:01,  1.71it/s]Loading train:  61%|██████    | 162/266 [01:51<01:00,  1.72it/s]Loading train:  61%|██████▏   | 163/266 [01:52<00:59,  1.74it/s]Loading train:  62%|██████▏   | 164/266 [01:52<00:59,  1.72it/s]Loading train:  62%|██████▏   | 165/266 [01:53<00:59,  1.71it/s]Loading train:  62%|██████▏   | 166/266 [01:53<00:59,  1.69it/s]Loading train:  63%|██████▎   | 167/266 [01:54<00:58,  1.69it/s]Loading train:  63%|██████▎   | 168/266 [01:55<00:58,  1.67it/s]Loading train:  64%|██████▎   | 169/266 [01:55<00:59,  1.64it/s]Loading train:  64%|██████▍   | 170/266 [01:56<00:58,  1.65it/s]Loading train:  64%|██████▍   | 171/266 [01:56<00:57,  1.66it/s]Loading train:  65%|██████▍   | 172/266 [01:57<00:57,  1.64it/s]Loading train:  65%|██████▌   | 173/266 [01:58<00:57,  1.61it/s]Loading train:  65%|██████▌   | 174/266 [01:58<00:56,  1.63it/s]Loading train:  66%|██████▌   | 175/266 [01:59<00:54,  1.67it/s]Loading train:  66%|██████▌   | 176/266 [01:59<00:52,  1.70it/s]Loading train:  67%|██████▋   | 177/266 [02:00<00:52,  1.70it/s]Loading train:  67%|██████▋   | 178/266 [02:01<00:51,  1.71it/s]Loading train:  67%|██████▋   | 179/266 [02:01<00:50,  1.74it/s]Loading train:  68%|██████▊   | 180/266 [02:02<00:50,  1.71it/s]Loading train:  68%|██████▊   | 181/266 [02:02<00:48,  1.74it/s]Loading train:  68%|██████▊   | 182/266 [02:03<00:47,  1.77it/s]Loading train:  69%|██████▉   | 183/266 [02:03<00:46,  1.80it/s]Loading train:  69%|██████▉   | 184/266 [02:04<00:44,  1.82it/s]Loading train:  70%|██████▉   | 185/266 [02:04<00:44,  1.82it/s]Loading train:  70%|██████▉   | 186/266 [02:05<00:44,  1.82it/s]Loading train:  70%|███████   | 187/266 [02:06<00:43,  1.82it/s]Loading train:  71%|███████   | 188/266 [02:06<00:43,  1.80it/s]Loading train:  71%|███████   | 189/266 [02:07<00:42,  1.80it/s]Loading train:  71%|███████▏  | 190/266 [02:07<00:42,  1.78it/s]Loading train:  72%|███████▏  | 191/266 [02:08<00:50,  1.47it/s]Loading train:  72%|███████▏  | 192/266 [02:09<00:53,  1.37it/s]Loading train:  73%|███████▎  | 193/266 [02:10<00:55,  1.33it/s]Loading train:  73%|███████▎  | 194/266 [02:11<00:59,  1.21it/s]Loading train:  73%|███████▎  | 195/266 [02:11<00:54,  1.31it/s]Loading train:  74%|███████▎  | 196/266 [02:12<00:50,  1.39it/s]Loading train:  74%|███████▍  | 197/266 [02:13<00:47,  1.45it/s]Loading train:  74%|███████▍  | 198/266 [02:13<00:45,  1.51it/s]Loading train:  75%|███████▍  | 199/266 [02:14<00:43,  1.53it/s]Loading train:  75%|███████▌  | 200/266 [02:15<00:42,  1.56it/s]Loading train:  76%|███████▌  | 201/266 [02:15<00:40,  1.59it/s]Loading train:  76%|███████▌  | 202/266 [02:16<00:40,  1.60it/s]Loading train:  76%|███████▋  | 203/266 [02:16<00:38,  1.62it/s]Loading train:  77%|███████▋  | 204/266 [02:17<00:38,  1.63it/s]Loading train:  77%|███████▋  | 205/266 [02:18<00:37,  1.63it/s]Loading train:  77%|███████▋  | 206/266 [02:18<00:36,  1.63it/s]Loading train:  78%|███████▊  | 207/266 [02:19<00:36,  1.62it/s]Loading train:  78%|███████▊  | 208/266 [02:19<00:35,  1.62it/s]Loading train:  79%|███████▊  | 209/266 [02:20<00:35,  1.60it/s]Loading train:  79%|███████▉  | 210/266 [02:21<00:34,  1.60it/s]Loading train:  79%|███████▉  | 211/266 [02:21<00:34,  1.60it/s]Loading train:  80%|███████▉  | 212/266 [02:22<00:33,  1.61it/s]Loading train:  80%|████████  | 213/266 [02:23<00:32,  1.62it/s]Loading train:  80%|████████  | 214/266 [02:23<00:32,  1.62it/s]Loading train:  81%|████████  | 215/266 [02:24<00:31,  1.62it/s]Loading train:  81%|████████  | 216/266 [02:24<00:30,  1.62it/s]Loading train:  82%|████████▏ | 217/266 [02:25<00:30,  1.63it/s]Loading train:  82%|████████▏ | 218/266 [02:26<00:33,  1.44it/s]Loading train:  82%|████████▏ | 219/266 [02:27<00:39,  1.19it/s]Loading train:  83%|████████▎ | 220/266 [02:29<00:53,  1.16s/it]Loading train:  83%|████████▎ | 221/266 [02:31<01:08,  1.53s/it]Loading train:  83%|████████▎ | 222/266 [02:34<01:21,  1.85s/it]Loading train:  84%|████████▍ | 223/266 [02:36<01:26,  2.01s/it]Loading train:  84%|████████▍ | 224/266 [02:39<01:30,  2.15s/it]Loading train:  85%|████████▍ | 225/266 [02:41<01:31,  2.23s/it]Loading train:  85%|████████▍ | 226/266 [02:44<01:33,  2.35s/it]Loading train:  85%|████████▌ | 227/266 [02:46<01:34,  2.41s/it]Loading train:  86%|████████▌ | 228/266 [02:49<01:33,  2.46s/it]Loading train:  86%|████████▌ | 229/266 [02:52<01:32,  2.50s/it]Loading train:  86%|████████▋ | 230/266 [02:54<01:31,  2.55s/it]Loading train:  87%|████████▋ | 231/266 [02:57<01:26,  2.48s/it]Loading train:  87%|████████▋ | 232/266 [02:59<01:24,  2.48s/it]Loading train:  88%|████████▊ | 233/266 [03:02<01:22,  2.50s/it]Loading train:  88%|████████▊ | 234/266 [03:04<01:20,  2.50s/it]Loading train:  88%|████████▊ | 235/266 [03:07<01:17,  2.50s/it]Loading train:  89%|████████▊ | 236/266 [03:09<01:12,  2.43s/it]Loading train:  89%|████████▉ | 237/266 [03:11<01:11,  2.46s/it]Loading train:  89%|████████▉ | 238/266 [03:14<01:09,  2.47s/it]Loading train:  90%|████████▉ | 239/266 [03:17<01:08,  2.54s/it]Loading train:  90%|█████████ | 240/266 [03:19<01:06,  2.54s/it]Loading train:  91%|█████████ | 241/266 [03:21<01:01,  2.44s/it]Loading train:  91%|█████████ | 242/266 [03:24<00:57,  2.38s/it]Loading train:  91%|█████████▏| 243/266 [03:26<00:55,  2.42s/it]Loading train:  92%|█████████▏| 244/266 [03:28<00:53,  2.41s/it]Loading train:  92%|█████████▏| 245/266 [03:31<00:49,  2.38s/it]Loading train:  92%|█████████▏| 246/266 [03:33<00:46,  2.34s/it]Loading train:  93%|█████████▎| 247/266 [03:35<00:45,  2.37s/it]Loading train:  93%|█████████▎| 248/266 [03:38<00:42,  2.38s/it]Loading train:  94%|█████████▎| 249/266 [03:42<00:50,  2.95s/it]Loading train:  94%|█████████▍| 250/266 [03:48<00:59,  3.70s/it]Loading train:  94%|█████████▍| 251/266 [03:53<01:02,  4.13s/it]Loading train:  95%|█████████▍| 252/266 [04:00<01:10,  5.02s/it]Loading train:  95%|█████████▌| 253/266 [04:08<01:16,  5.85s/it]Loading train:  95%|█████████▌| 254/266 [04:14<01:11,  5.94s/it]Loading train:  96%|█████████▌| 255/266 [04:19<01:03,  5.79s/it]Loading train:  96%|█████████▌| 256/266 [04:24<00:55,  5.54s/it]Loading train:  97%|█████████▋| 257/266 [04:29<00:46,  5.21s/it]Loading train:  97%|█████████▋| 258/266 [04:32<00:36,  4.61s/it]Loading train:  97%|█████████▋| 259/266 [04:35<00:29,  4.23s/it]Loading train:  98%|█████████▊| 260/266 [04:38<00:23,  3.94s/it]Loading train:  98%|█████████▊| 261/266 [04:42<00:18,  3.74s/it]Loading train:  98%|█████████▊| 262/266 [04:45<00:14,  3.57s/it]Loading train:  99%|█████████▉| 263/266 [04:48<00:10,  3.46s/it]Loading train:  99%|█████████▉| 264/266 [04:51<00:06,  3.40s/it]Loading train: 100%|█████████▉| 265/266 [04:55<00:03,  3.37s/it]Loading train: 100%|██████████| 266/266 [04:58<00:00,  3.32s/it]Loading train: 100%|██████████| 266/266 [04:58<00:00,  1.12s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/266 [00:00<00:04, 52.13it/s]concatenating: train:   4%|▍         | 11/266 [00:00<00:05, 50.77it/s]concatenating: train:   6%|▌         | 16/266 [00:00<00:05, 49.19it/s]concatenating: train:   8%|▊         | 21/266 [00:00<00:04, 49.38it/s]concatenating: train:  11%|█         | 28/266 [00:00<00:04, 52.50it/s]concatenating: train:  13%|█▎        | 35/266 [00:00<00:04, 55.69it/s]concatenating: train:  16%|█▌        | 42/266 [00:00<00:03, 57.66it/s]concatenating: train:  18%|█▊        | 49/266 [00:00<00:03, 58.55it/s]concatenating: train:  21%|██        | 55/266 [00:00<00:03, 58.36it/s]concatenating: train:  23%|██▎       | 61/266 [00:01<00:03, 58.11it/s]concatenating: train:  26%|██▌       | 68/266 [00:01<00:03, 59.08it/s]concatenating: train:  28%|██▊       | 75/266 [00:01<00:03, 59.72it/s]concatenating: train:  30%|███       | 81/266 [00:01<00:03, 59.46it/s]concatenating: train:  33%|███▎      | 88/266 [00:01<00:02, 60.61it/s]concatenating: train:  36%|███▌      | 95/266 [00:01<00:02, 60.25it/s]concatenating: train:  38%|███▊      | 102/266 [00:01<00:02, 60.08it/s]concatenating: train:  41%|████      | 108/266 [00:01<00:02, 59.32it/s]concatenating: train:  43%|████▎     | 114/266 [00:01<00:02, 57.73it/s]concatenating: train:  45%|████▌     | 120/266 [00:02<00:02, 56.98it/s]concatenating: train:  47%|████▋     | 126/266 [00:02<00:02, 56.51it/s]concatenating: train:  50%|████▉     | 132/266 [00:02<00:02, 57.13it/s]concatenating: train:  52%|█████▏    | 138/266 [00:02<00:02, 57.68it/s]concatenating: train:  54%|█████▍    | 144/266 [00:02<00:02, 57.78it/s]concatenating: train:  56%|█████▋    | 150/266 [00:02<00:02, 57.15it/s]concatenating: train:  59%|█████▊    | 156/266 [00:02<00:01, 57.24it/s]concatenating: train:  61%|██████    | 162/266 [00:02<00:01, 57.45it/s]concatenating: train:  63%|██████▎   | 168/266 [00:02<00:01, 57.42it/s]concatenating: train:  65%|██████▌   | 174/266 [00:03<00:01, 57.51it/s]concatenating: train:  68%|██████▊   | 181/266 [00:03<00:01, 59.11it/s]concatenating: train:  71%|███████   | 188/266 [00:03<00:01, 60.36it/s]concatenating: train:  73%|███████▎  | 195/266 [00:03<00:01, 59.17it/s]concatenating: train:  76%|███████▌  | 202/266 [00:03<00:01, 59.60it/s]concatenating: train:  78%|███████▊  | 208/266 [00:03<00:00, 58.63it/s]concatenating: train:  80%|████████  | 214/266 [00:03<00:00, 57.61it/s]concatenating: train:  83%|████████▎ | 220/266 [00:03<00:00, 56.00it/s]concatenating: train:  85%|████████▍ | 226/266 [00:03<00:00, 52.94it/s]concatenating: train:  87%|████████▋ | 232/266 [00:04<00:00, 52.85it/s]concatenating: train:  89%|████████▉ | 238/266 [00:04<00:00, 52.54it/s]concatenating: train:  92%|█████████▏| 244/266 [00:04<00:00, 53.47it/s]concatenating: train:  94%|█████████▍| 250/266 [00:04<00:00, 53.73it/s]concatenating: train:  96%|█████████▌| 256/266 [00:04<00:00, 54.00it/s]concatenating: train:  98%|█████████▊| 262/266 [00:04<00:00, 51.94it/s]concatenating: train: 100%|██████████| 266/266 [00:04<00:00, 56.72it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:06<00:18,  6.18s/it]Loading test:  50%|█████     | 2/4 [00:10<00:11,  5.66s/it]Loading test:  75%|███████▌  | 3/4 [00:14<00:05,  5.23s/it]Loading test: 100%|██████████| 4/4 [00:20<00:00,  5.32s/it]Loading test: 100%|██████████| 4/4 [00:20<00:00,  5.10s/it]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 82.15it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 48, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 48, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 48, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 48, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 48, 52, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 48, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 48, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 24, 26, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 24, 26, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 24, 26, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 24, 26, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 24, 26, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 24, 26, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 24, 26, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 24, 26, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 24, 26, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 12, 13, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 12, 13, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 12, 13, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 12, 13, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 12, 13, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 12, 13, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 12, 13, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 12, 13, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 12, 13, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 12, 13, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 24, 26, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 24, 26, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 24, 26, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 24, 26, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 24, 26, 60)   0           batch_normalization_7[0][0]      2020-01-21 23:07:18.200571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 23:07:18.200648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 23:07:18.200660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 23:07:18.200667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 23:07:18.200961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 24, 26, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 24, 26, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 24, 26, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 24, 26, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24, 26, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 48, 52, 30)   25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 48, 52, 60)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 48, 52, 30)   16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 48, 52, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 48, 52, 30)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 48, 52, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 48, 52, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 48, 52, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 48, 52, 90)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 48, 52, 90)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 48, 52, 13)   1183        dropout_5[0][0]                  
==================================================================================================
Total params: 501,343
Trainable params: 500,143
Non-trainable params: 1,200
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1
------------------------------------------------------------------
class_weights [6.34711044e-02 3.28961921e-02 7.69232209e-02 9.55809699e-03
 2.76633092e-02 7.23732507e-03 8.42717903e-02 1.14333704e-01
 8.97745247e-02 1.36398743e-02 2.91066667e-01 1.88903930e-01
 2.60261512e-04]
Train on 16975 samples, validate on 248 samples
Epoch 1/300
 - 37s - loss: 0.6589 - acc: 0.9062 - mDice: 0.2896 - val_loss: 0.4462 - val_acc: 0.9379 - val_mDice: 0.1829

Epoch 00001: val_mDice improved from -inf to 0.18292, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300
 - 33s - loss: 0.4931 - acc: 0.9300 - mDice: 0.4683 - val_loss: 0.1185 - val_acc: 0.9421 - val_mDice: 0.1985

Epoch 00002: val_mDice improved from 0.18292 to 0.19846, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300
 - 32s - loss: 0.4332 - acc: 0.9349 - mDice: 0.5330 - val_loss: -4.0520e-02 - val_acc: 0.9447 - val_mDice: 0.2101

Epoch 00003: val_mDice improved from 0.19846 to 0.21014, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 4/300
 - 32s - loss: 0.4099 - acc: 0.9379 - mDice: 0.5582 - val_loss: -5.8177e-02 - val_acc: 0.9456 - val_mDice: 0.2110

Epoch 00004: val_mDice improved from 0.21014 to 0.21096, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 5/300
 - 33s - loss: 0.3954 - acc: 0.9396 - mDice: 0.5739 - val_loss: -3.4565e-02 - val_acc: 0.9464 - val_mDice: 0.2152

Epoch 00005: val_mDice improved from 0.21096 to 0.21523, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 6/300
 - 32s - loss: 0.3858 - acc: 0.9411 - mDice: 0.5843 - val_loss: -5.8394e-02 - val_acc: 0.9474 - val_mDice: 0.2185

Epoch 00006: val_mDice improved from 0.21523 to 0.21848, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 7/300
 - 35s - loss: 0.3784 - acc: 0.9420 - mDice: 0.5922 - val_loss: -4.0601e-02 - val_acc: 0.9468 - val_mDice: 0.2175

Epoch 00007: val_mDice did not improve from 0.21848
Epoch 8/300
 - 32s - loss: 0.3715 - acc: 0.9429 - mDice: 0.5997 - val_loss: -6.0324e-02 - val_acc: 0.9466 - val_mDice: 0.2155

Epoch 00008: val_mDice did not improve from 0.21848
Epoch 9/300
 - 33s - loss: 0.3657 - acc: 0.9436 - mDice: 0.6059 - val_loss: -8.6317e-02 - val_acc: 0.9476 - val_mDice: 0.2176

Epoch 00009: val_mDice did not improve from 0.21848
Epoch 10/300
 - 32s - loss: 0.3602 - acc: 0.9442 - mDice: 0.6118 - val_loss: -3.4056e-02 - val_acc: 0.9483 - val_mDice: 0.2222

Epoch 00010: val_mDice improved from 0.21848 to 0.22222, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 11/300
 - 33s - loss: 0.3565 - acc: 0.9448 - mDice: 0.6159 - val_loss: -6.3028e-02 - val_acc: 0.9472 - val_mDice: 0.2133

Epoch 00011: val_mDice did not improve from 0.22222
Epoch 12/300
 - 32s - loss: 0.3531 - acc: 0.9453 - mDice: 0.6195 - val_loss: -9.0105e-02 - val_acc: 0.9475 - val_mDice: 0.2187

Epoch 00012: val_mDice did not improve from 0.22222
Epoch 13/300
 - 32s - loss: 0.3470 - acc: 0.9458 - mDice: 0.6261 - val_loss: -7.7922e-02 - val_acc: 0.9473 - val_mDice: 0.2160

Epoch 00013: val_mDice did not improve from 0.22222
Epoch 14/300
 - 32s - loss: 0.3452 - acc: 0.9460 - mDice: 0.6281 - val_loss: -6.8891e-02 - val_acc: 0.9475 - val_mDice: 0.2166

Epoch 00014: val_mDice did not improve from 0.22222
Epoch 15/300
 - 32s - loss: 0.3427 - acc: 0.9464 - mDice: 0.6308 - val_loss: -7.9529e-02 - val_acc: 0.9477 - val_mDice: 0.2206

Epoch 00015: val_mDice did not improve from 0.22222
Epoch 16/300
 - 32s - loss: 0.3422 - acc: 0.9468 - mDice: 0.6313 - val_loss: -9.5248e-02 - val_acc: 0.9478 - val_mDice: 0.2162

Epoch 00016: val_mDice did not improve from 0.22222
Epoch 17/300
 - 32s - loss: 0.3364 - acc: 0.9471 - mDice: 0.6376 - val_loss: -1.0510e-01 - val_acc: 0.9485 - val_mDice: 0.2154

Epoch 00017: val_mDice did not improve from 0.22222
Epoch 18/300
 - 32s - loss: 0.3361 - acc: 0.9474 - mDice: 0.6379 - val_loss: -9.2050e-02 - val_acc: 0.9474 - val_mDice: 0.2150

Epoch 00018: val_mDice did not improve from 0.22222
Epoch 19/300
 - 32s - loss: 0.3312 - acc: 0.9479 - mDice: 0.6431 - val_loss: -1.1103e-01 - val_acc: 0.9483 - val_mDice: 0.2158

Epoch 00019: val_mDice did not improve from 0.22222
Epoch 20/300
 - 32s - loss: 0.3306 - acc: 0.9481 - mDice: 0.6438 - val_loss: -6.9224e-02 - val_acc: 0.9479 - val_mDice: 0.2166

Epoch 00020: val_mDice did not improve from 0.22222
Epoch 21/300
 - 32s - loss: 0.3286 - acc: 0.9483 - mDice: 0.6459 - val_loss: -1.2550e-01 - val_acc: 0.9490 - val_mDice: 0.2223

Epoch 00021: val_mDice improved from 0.22222 to 0.22234, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 22/300
 - 32s - loss: 0.3241 - acc: 0.9486 - mDice: 0.6508 - val_loss: -8.1246e-02 - val_acc: 0.9478 - val_mDice: 0.2157

Epoch 00022: val_mDice did not improve from 0.22234
Epoch 23/300
 - 32s - loss: 0.3244 - acc: 0.9487 - mDice: 0.6505 - val_loss: -1.2796e-01 - val_acc: 0.9480 - val_mDice: 0.2175

Epoch 00023: val_mDice did not improve from 0.22234
Epoch 24/300
 - 32s - loss: 0.3217 - acc: 0.9489 - mDice: 0.6535 - val_loss: -1.0797e-01 - val_acc: 0.9486 - val_mDice: 0.2163

Epoch 00024: val_mDice did not improve from 0.22234
Epoch 25/300
 - 32s - loss: 0.3191 - acc: 0.9492 - mDice: 0.6562 - val_loss: -1.3912e-01 - val_acc: 0.9495 - val_mDice: 0.2204

Epoch 00025: val_mDice did not improve from 0.22234

Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 26/300
 - 32s - loss: 0.3161 - acc: 0.9495 - mDice: 0.6595 - val_loss: -1.1735e-01 - val_acc: 0.9486 - val_mDice: 0.2194

Epoch 00026: val_mDice did not improve from 0.22234
Epoch 27/300
 - 32s - loss: 0.3146 - acc: 0.9497 - mDice: 0.6611 - val_loss: -1.0877e-01 - val_acc: 0.9480 - val_mDice: 0.2168

Epoch 00027: val_mDice did not improve from 0.22234
Epoch 28/300
 - 32s - loss: 0.3141 - acc: 0.9498 - mDice: 0.6616 - val_loss: -1.2416e-01 - val_acc: 0.9481 - val_mDice: 0.2154

Epoch 00028: val_mDice did not improve from 0.22234
Epoch 29/300
 - 32s - loss: 0.3137 - acc: 0.9499 - mDice: 0.6621 - val_loss: -1.2234e-01 - val_acc: 0.9482 - val_mDice: 0.2177

Epoch 00029: val_mDice did not improve from 0.22234
Epoch 30/300
 - 33s - loss: 0.3146 - acc: 0.9500 - mDice: 0.6611 - val_loss: -1.5215e-01 - val_acc: 0.9487 - val_mDice: 0.2179

Epoch 00030: val_mDice did not improve from 0.22234
Epoch 31/300
 - 32s - loss: 0.3101 - acc: 0.9501 - mDice: 0.6659 - val_loss: -1.4847e-01 - val_acc: 0.9487 - val_mDice: 0.2165

Epoch 00031: val_mDice did not improve from 0.22234
Epoch 32/300
 - 32s - loss: 0.3128 - acc: 0.9501 - mDice: 0.6630 - val_loss: -1.2819e-01 - val_acc: 0.9486 - val_mDice: 0.2176

Epoch 00032: val_mDice did not improve from 0.22234
Epoch 33/300
 - 32s - loss: 0.3110 - acc: 0.9502 - mDice: 0.6650 - val_loss: -1.6008e-01 - val_acc: 0.9492 - val_mDice: 0.2184

Epoch 00033: val_mDice did not improve from 0.22234
Epoch 34/300
 - 32s - loss: 0.3105 - acc: 0.9503 - mDice: 0.6655 - val_loss: -1.3013e-01 - val_acc: 0.9488 - val_mDice: 0.2178

Epoch 00034: val_mDice did not improve from 0.22234
Epoch 35/300
 - 32s - loss: 0.3083 - acc: 0.9504 - mDice: 0.6679 - val_loss: -1.3072e-01 - val_acc: 0.9491 - val_mDice: 0.2185

Epoch 00035: val_mDice did not improve from 0.22234
Epoch 36/300
 - 33s - loss: 0.3090 - acc: 0.9504 - mDice: 0.6671 - val_loss: -1.4322e-01 - val_acc: 0.9486 - val_mDice: 0.2155

Epoch 00036: val_mDice did not improve from 0.22234
Epoch 37/300
 - 32s - loss: 0.3073 - acc: 0.9505 - mDice: 0.6690 - val_loss: -1.5769e-01 - val_acc: 0.9493 - val_mDice: 0.2186

Epoch 00037: val_mDice did not improve from 0.22234
Epoch 38/300
 - 33s - loss: 0.3091 - acc: 0.9506 - mDice: 0.6670 - val_loss: -1.5131e-01 - val_acc: 0.9482 - val_mDice: 0.2146

Epoch 00038: val_mDice did not improve from 0.22234
Epoch 39/300
 - 33s - loss: 0.3084 - acc: 0.9507 - mDice: 0.6678 - val_loss: -1.4973e-01 - val_acc: 0.9492 - val_mDice: 0.2179

Epoch 00039: val_mDice did not improve from 0.22234
Epoch 40/300
 - 32s - loss: 0.3049 - acc: 0.9508 - mDice: 0.6716 - val_loss: -1.5395e-01 - val_acc: 0.9498 - val_mDice: 0.2183

Epoch 00040: val_mDice did not improve from 0.22234

Epoch 00040: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 41/300
 - 33s - loss: 0.3067 - acc: 0.9510 - mDice: 0.6696 - val_loss: -1.5528e-01 - val_acc: 0.9494 - val_mDice: 0.2170

Epoch 00041: val_mDice did not improve from 0.22234
Epoch 42/300
 - 32s - loss: 0.3049 - acc: 0.9511 - mDice: 0.6716 - val_loss: -1.5197e-01 - val_acc: 0.9490 - val_mDice: 0.2174

Epoch 00042: val_mDice did not improve from 0.22234
Epoch 43/300
 - 32s - loss: 0.3029 - acc: 0.9510 - mDice: 0.6737 - val_loss: -1.6523e-01 - val_acc: 0.9495 - val_mDice: 0.2180

Epoch 00043: val_mDice did not improve from 0.22234
Epoch 44/300
 - 32s - loss: 0.3043 - acc: 0.9510 - mDice: 0.6722 - val_loss: -1.5576e-01 - val_acc: 0.9495 - val_mDice: 0.2190

Epoch 00044: val_mDice did not improve from 0.22234
Epoch 45/300
 - 32s - loss: 0.3029 - acc: 0.9512 - mDice: 0.6737 - val_loss: -1.5365e-01 - val_acc: 0.9496 - val_mDice: 0.2190

Epoch 00045: val_mDice did not improve from 0.22234
Epoch 46/300
 - 32s - loss: 0.3027 - acc: 0.9511 - mDice: 0.6740 - val_loss: -1.2674e-01 - val_acc: 0.9496 - val_mDice: 0.2180

Epoch 00046: val_mDice did not improve from 0.22234
Epoch 47/300
 - 32s - loss: 0.3021 - acc: 0.9513 - mDice: 0.6745 - val_loss: -1.5673e-01 - val_acc: 0.9501 - val_mDice: 0.2201

Epoch 00047: val_mDice did not improve from 0.22234
Epoch 48/300
 - 32s - loss: 0.3022 - acc: 0.9513 - mDice: 0.6745 - val_loss: -1.6324e-01 - val_acc: 0.9489 - val_mDice: 0.2163

Epoch 00048: val_mDice did not improve from 0.22234
Epoch 49/300
 - 33s - loss: 0.3019 - acc: 0.9513 - mDice: 0.6747 - val_loss: -1.6326e-01 - val_acc: 0.9496 - val_mDice: 0.2181

Epoch 00049: val_mDice did not improve from 0.22234
Epoch 50/300
 - 32s - loss: 0.3003 - acc: 0.9513 - mDice: 0.6765 - val_loss: -1.6985e-01 - val_acc: 0.9501 - val_mDice: 0.2193

Epoch 00050: val_mDice did not improve from 0.22234
Epoch 51/300
 - 33s - loss: 0.3032 - acc: 0.9513 - mDice: 0.6734 - val_loss: -1.6327e-01 - val_acc: 0.9493 - val_mDice: 0.2183

Epoch 00051: val_mDice did not improve from 0.22234
Epoch 52/300
 - 32s - loss: 0.3004 - acc: 0.9515 - mDice: 0.6764 - val_loss: -1.4452e-01 - val_acc: 0.9493 - val_mDice: 0.2174

Epoch 00052: val_mDice did not improve from 0.22234
Epoch 53/300
 - 33s - loss: 0.3004 - acc: 0.9515 - mDice: 0.6765 - val_loss: -1.5700e-01 - val_acc: 0.9495 - val_mDice: 0.2179

Epoch 00053: val_mDice did not improve from 0.22234
Epoch 54/300
 - 32s - loss: 0.3002 - acc: 0.9515 - mDice: 0.6766 - val_loss: -1.6307e-01 - val_acc: 0.9493 - val_mDice: 0.2193

Epoch 00054: val_mDice did not improve from 0.22234
Epoch 55/300
 - 33s - loss: 0.2997 - acc: 0.9515 - mDice: 0.6772 - val_loss: -1.7586e-01 - val_acc: 0.9497 - val_mDice: 0.2195

Epoch 00055: val_mDice did not improve from 0.22234

Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 56/300
 - 33s - loss: 0.3006 - acc: 0.9515 - mDice: 0.6762 - val_loss: -1.6664e-01 - val_acc: 0.9496 - val_mDice: 0.2178

Epoch 00056: val_mDice did not improve from 0.22234
Epoch 57/300
 - 32s - loss: 0.3003 - acc: 0.9516 - mDice: 0.6765 - val_loss: -1.6545e-01 - val_acc: 0.9498 - val_mDice: 0.2185

Epoch 00057: val_mDice did not improve from 0.22234
Epoch 58/300
 - 32s - loss: 0.2990 - acc: 0.9517 - mDice: 0.6779 - val_loss: -1.6170e-01 - val_acc: 0.9498 - val_mDice: 0.2190

Epoch 00058: val_mDice did not improve from 0.22234
Epoch 59/300
 - 32s - loss: 0.2972 - acc: 0.9516 - mDice: 0.6799 - val_loss: -1.6199e-01 - val_acc: 0.9492 - val_mDice: 0.2185

Epoch 00059: val_mDice did not improve from 0.22234
Epoch 60/300
 - 32s - loss: 0.2984 - acc: 0.9517 - mDice: 0.6786 - val_loss: -1.7176e-01 - val_acc: 0.9497 - val_mDice: 0.2185

Epoch 00060: val_mDice did not improve from 0.22234
Epoch 61/300
 - 32s - loss: 0.2976 - acc: 0.9518 - mDice: 0.6795 - val_loss: -1.6998e-01 - val_acc: 0.9495 - val_mDice: 0.2172

predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:01<00:04,  1.51s/it]predicting test subjects:  50%|█████     | 2/4 [00:02<00:02,  1.33s/it]predicting test subjects:  75%|███████▌  | 3/4 [00:03<00:01,  1.19s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.14s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]
Epoch 00061: val_mDice did not improve from 0.22234
Restoring model weights from the end of the best epoch
Epoch 00061: early stopping
{'val_loss': [0.4462063738535489, 0.11849396860211968, -0.040520042618511304, -0.058177429641927444, -0.03456505153688692, -0.058394210018788376, -0.040600901514652274, -0.06032435794270808, -0.08631720335312909, -0.03405561603252746, -0.06302808243638156, -0.09010462016768513, -0.07792203457704032, -0.06889097131187877, -0.0795292820473532, -0.0952478017012078, -0.10509683969881266, -0.09205048934992162, -0.11103159128387849, -0.06922411247186604, -0.12550393440159818, -0.08124561752417996, -0.12795619109284975, -0.10797481562739479, -0.13912302311960487, -0.11735181917902082, -0.10876727359549652, -0.12416458367219856, -0.12234157179256962, -0.15214982468845142, -0.14846919797691366, -0.12819128068945101, -0.16007886214872763, -0.1301349892371124, -0.13072117277601314, -0.14321827124654046, -0.15768959174954122, -0.1513118867221619, -0.14973000691914481, -0.15395078486988262, -0.15527688725400837, -0.15197224117394897, -0.16523434481613578, -0.15576086903081066, -0.153647582249945, -0.1267425581106856, -0.15672849605588482, -0.16324478235366124, -0.16325982040425221, -0.1698457161715675, -0.1632707744776722, -0.1445170153953856, -0.15700405208213675, -0.16307242436244362, -0.1758606722428193, -0.16664447835946997, -0.16544813429936767, -0.1616957614108199, -0.16198607107564325, -0.17176206985248194, -0.16997642567261093], 'val_acc': [0.9379377985192884, 0.9421396779437219, 0.9447163838532663, 0.9455596703675485, 0.9463544891726586, 0.9474239455115411, 0.9468326741649259, 0.946638818710081, 0.9476113367465234, 0.9483447651709279, 0.9471622349754456, 0.9474837126270417, 0.947310863002654, 0.9475192514158064, 0.9477292664589421, 0.9478342792680187, 0.9484626972867597, 0.9474158666787609, 0.9482801542166741, 0.9479150430810067, 0.9489635022417191, 0.9478051917206857, 0.9480426758527756, 0.9485984036037999, 0.9495289287259502, 0.9486258746154846, 0.9479812811459264, 0.9481105304533436, 0.9481767608273414, 0.9486775609754747, 0.9486549419741477, 0.9486194065501613, 0.9491945135016595, 0.948839113596947, 0.9491137439204801, 0.9485790157510389, 0.9492752869283, 0.9482252208455917, 0.9491880531272581, 0.9497954758905596, 0.9494481480890705, 0.9489812725974668, 0.9494529880823628, 0.9495337677578772, 0.9495790096059922, 0.9495935411222519, 0.9501444264765708, 0.9488794952630997, 0.9495757722085522, 0.9500652700662613, 0.9492849837387761, 0.9492946834333481, 0.9495483103298372, 0.9493447595065639, 0.9496775519463324, 0.949604852545646, 0.9497922442613109, 0.9498261737246667, 0.949231673152216, 0.9496904722144527, 0.9494659232516443], 'val_mDice': [0.18291838905744015, 0.19845747124523885, 0.21014303686998545, 0.21096463113903038, 0.21522762416110885, 0.21848410950793373, 0.21748007746833947, 0.2155350133656494, 0.2175572425607712, 0.22221878699717984, 0.21327912332790513, 0.2186593503180531, 0.21598326802373893, 0.21655879860683794, 0.22055095902854396, 0.21617907976671571, 0.21535476026756148, 0.21497644538119917, 0.21577162836347857, 0.21661130504141893, 0.22234435091095586, 0.21567723518537898, 0.21745801269407233, 0.21626926447835662, 0.2203589192020797, 0.2193924378483526, 0.21681447474346047, 0.21536846657193476, 0.21769448979607514, 0.21792858322301217, 0.21652912456662424, 0.21755851573881604, 0.21838281093345535, 0.21775358993439906, 0.21854524728992292, 0.21553360253211953, 0.2185625753275329, 0.21460059501471057, 0.21791349778011923, 0.21828371322443407, 0.21700141951441765, 0.2173637537766368, 0.21802296258148648, 0.21900091855035675, 0.21900163257434482, 0.21797032483042247, 0.22012879819639267, 0.21628896417396684, 0.21811131688375626, 0.21931947100787394, 0.21829051133846084, 0.21744626433017752, 0.2179146829331594, 0.21934433578844031, 0.21949348637774105, 0.21776530627281435, 0.21854597173871532, 0.2190332362848905, 0.21849174109557945, 0.21846821678862458, 0.21724630832191436], 'loss': [0.6589026301177506, 0.49308406757149675, 0.4332391914194217, 0.40994347904497463, 0.39541518747894333, 0.3857570313564632, 0.37844558107133114, 0.3714587211608887, 0.365729881693583, 0.36024783059669346, 0.3564584619166749, 0.3531434510698726, 0.3470355810227205, 0.3451696690034796, 0.34266993032990567, 0.34215338831446485, 0.33637914459203233, 0.33610093375017785, 0.33121000981295756, 0.3305516283624478, 0.328632595709392, 0.3241012816488831, 0.32441952944855273, 0.3216657155128923, 0.3191472818117605, 0.31606559232864884, 0.3146039491198375, 0.3141315246014602, 0.31368244654651245, 0.31462531417213235, 0.31014086511946215, 0.31281677444132155, 0.31097848913982096, 0.3105017569876209, 0.3082976807024061, 0.30900629516553807, 0.3073075945054308, 0.3090725527245096, 0.30835816122821924, 0.3048521349900601, 0.306684848486236, 0.30487471516950493, 0.30294604503353495, 0.30434728249446635, 0.3029056570930228, 0.302679973208676, 0.3021385503271718, 0.302175190856074, 0.301949684591813, 0.30030128764035247, 0.3032279269596965, 0.30040628442181405, 0.3003541176291032, 0.3002226775454492, 0.29970449021357676, 0.30058494350695997, 0.30028675397822363, 0.2990433811439155, 0.29717785256074, 0.29840439511504196, 0.2975768423835378], 'acc': [0.9062074691276663, 0.9299792310801802, 0.9349432848690185, 0.9378691794770456, 0.9396022854567627, 0.9410625072747513, 0.941981515055728, 0.9428502987164987, 0.9435928117544206, 0.9442153344449309, 0.9448191626433596, 0.9453493780169817, 0.9457864358955349, 0.9460303620491534, 0.9463918709965623, 0.9467841568154685, 0.9471417005704674, 0.9474440402064653, 0.9478596213464358, 0.9480727696453876, 0.9483265599551363, 0.9486241316233538, 0.948719765511458, 0.9488867014778205, 0.9491605777452551, 0.9494602970241272, 0.9496588593027904, 0.9498076920481248, 0.9498592384258321, 0.9499664147402296, 0.9500798693812824, 0.9500970513725843, 0.9502274733228782, 0.9503256109396383, 0.9504058099109048, 0.950422755730872, 0.9505124660409077, 0.9506148744050814, 0.950725403204231, 0.9508262292274846, 0.9509644411273839, 0.9511111270345539, 0.9510481805794369, 0.9510459863209058, 0.9512205454140945, 0.9511415975258873, 0.9512768827938366, 0.9512542487068626, 0.951289769107358, 0.9513151174265787, 0.9513407953533403, 0.9514641154497115, 0.951461000751501, 0.9515081807334574, 0.9515151666085042, 0.951545919269455, 0.9515938561865086, 0.9517147906750045, 0.9516421681300179, 0.951682291571627, 0.9517704675404769], 'mDice': [0.2895975401808833, 0.46833287055257145, 0.5330190789014847, 0.5581786163601152, 0.5738677446199623, 0.5842870422596432, 0.592171040023725, 0.5997081715860493, 0.6058867872551369, 0.6118093144033373, 0.6158983178974426, 0.6194806171973429, 0.6260829404369546, 0.6280958620543332, 0.630789258237791, 0.6313312537628, 0.6375767467123771, 0.637866845731356, 0.6431455363405646, 0.6438472918277286, 0.6459227984186827, 0.6508263252445103, 0.6504778907822228, 0.6534633909304117, 0.6561831965274417, 0.6595079060945666, 0.6610877704199003, 0.6615935384612722, 0.6620836152770558, 0.661057318714125, 0.6659108282013388, 0.663010401824086, 0.665004226683868, 0.6655171169448144, 0.6679032161239321, 0.667131520880164, 0.6689684430348505, 0.6670475800539503, 0.6678184689583588, 0.6716141511308954, 0.6696291378829546, 0.6715808347328422, 0.6736730576088221, 0.6721538053169869, 0.6737117812805569, 0.6739600037264367, 0.6745405033867208, 0.674502094961231, 0.6747475874968235, 0.6765293059538672, 0.6733547715928312, 0.6764089854283255, 0.6764634431549656, 0.6766096899892163, 0.677173272051762, 0.6762174159244515, 0.6765401145319876, 0.6778819833746363, 0.6799016184940254, 0.6785714848227634, 0.6794685668025346], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05]}
---------------------- check Layers Step ------------------------------
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:15,  3.49it/s]Loading train:   1%|          | 2/266 [00:00<01:13,  3.58it/s]Loading train:   1%|          | 3/266 [00:00<01:08,  3.83it/s]Loading train:   2%|▏         | 4/266 [00:01<01:06,  3.96it/s]Loading train:   2%|▏         | 5/266 [00:01<01:06,  3.91it/s]Loading train:   2%|▏         | 6/266 [00:01<01:06,  3.89it/s]Loading train:   3%|▎         | 7/266 [00:01<01:06,  3.92it/s]Loading train:   3%|▎         | 8/266 [00:02<01:05,  3.93it/s]Loading train:   3%|▎         | 9/266 [00:02<01:05,  3.95it/s]Loading train:   4%|▍         | 10/266 [00:02<01:04,  3.95it/s]Loading train:   4%|▍         | 11/266 [00:02<01:04,  3.97it/s]Loading train:   5%|▍         | 12/266 [00:03<01:03,  3.97it/s]Loading train:   5%|▍         | 13/266 [00:03<01:03,  3.96it/s]Loading train:   5%|▌         | 14/266 [00:03<01:03,  3.95it/s]Loading train:   6%|▌         | 15/266 [00:03<01:03,  3.93it/s]Loading train:   6%|▌         | 16/266 [00:04<01:03,  3.93it/s]Loading train:   6%|▋         | 17/266 [00:04<01:03,  3.95it/s]Loading train:   7%|▋         | 18/266 [00:04<01:02,  3.94it/s]Loading train:   7%|▋         | 19/266 [00:04<01:02,  3.97it/s]Loading train:   8%|▊         | 20/266 [00:05<01:02,  3.96it/s]Loading train:   8%|▊         | 21/266 [00:05<01:02,  3.95it/s]Loading train:   8%|▊         | 22/266 [00:05<01:01,  3.96it/s]Loading train:   9%|▊         | 23/266 [00:05<01:01,  3.97it/s]Loading train:   9%|▉         | 24/266 [00:06<00:59,  4.04it/s]Loading train:   9%|▉         | 25/266 [00:06<00:58,  4.10it/s]Loading train:  10%|▉         | 26/266 [00:06<00:58,  4.11it/s]Loading train:  10%|█         | 27/266 [00:06<00:58,  4.10it/s]Loading train:  11%|█         | 28/266 [00:07<00:57,  4.12it/s]Loading train:  11%|█         | 29/266 [00:07<00:57,  4.14it/s]Loading train:  11%|█▏        | 30/266 [00:07<00:56,  4.14it/s]Loading train:  12%|█▏        | 31/266 [00:07<00:56,  4.14it/s]Loading train:  12%|█▏        | 32/266 [00:07<00:56,  4.16it/s]Loading train:  12%|█▏        | 33/266 [00:08<00:55,  4.17it/s]Loading train:  13%|█▎        | 34/266 [00:08<00:55,  4.16it/s]Loading train:  13%|█▎        | 35/266 [00:08<00:55,  4.15it/s]Loading train:  14%|█▎        | 36/266 [00:08<00:55,  4.13it/s]Loading train:  14%|█▍        | 37/266 [00:09<00:55,  4.16it/s]Loading train:  14%|█▍        | 38/266 [00:09<00:54,  4.16it/s]Loading train:  15%|█▍        | 39/266 [00:09<00:54,  4.14it/s]Loading train:  15%|█▌        | 40/266 [00:09<00:54,  4.15it/s]Loading train:  15%|█▌        | 41/266 [00:10<00:54,  4.16it/s]Loading train:  16%|█▌        | 42/266 [00:10<00:51,  4.32it/s]Loading train:  16%|█▌        | 43/266 [00:10<00:50,  4.45it/s]Loading train:  17%|█▋        | 44/266 [00:10<00:48,  4.57it/s]Loading train:  17%|█▋        | 45/266 [00:10<00:47,  4.67it/s]Loading train:  17%|█▋        | 46/266 [00:11<00:46,  4.73it/s]Loading train:  18%|█▊        | 47/266 [00:11<00:46,  4.74it/s]Loading train:  18%|█▊        | 48/266 [00:11<00:45,  4.79it/s]Loading train:  18%|█▊        | 49/266 [00:11<00:45,  4.79it/s]Loading train:  19%|█▉        | 50/266 [00:12<00:45,  4.78it/s]Loading train:  19%|█▉        | 51/266 [00:12<00:44,  4.82it/s]Loading train:  20%|█▉        | 52/266 [00:12<00:44,  4.82it/s]Loading train:  20%|█▉        | 53/266 [00:12<00:44,  4.82it/s]Loading train:  20%|██        | 54/266 [00:12<00:44,  4.81it/s]Loading train:  21%|██        | 55/266 [00:13<00:43,  4.83it/s]Loading train:  21%|██        | 56/266 [00:13<00:43,  4.85it/s]Loading train:  21%|██▏       | 57/266 [00:13<00:42,  4.89it/s]Loading train:  22%|██▏       | 58/266 [00:13<00:42,  4.89it/s]Loading train:  22%|██▏       | 59/266 [00:13<00:42,  4.89it/s]Loading train:  23%|██▎       | 60/266 [00:14<00:42,  4.82it/s]Loading train:  23%|██▎       | 61/266 [00:14<00:42,  4.79it/s]Loading train:  23%|██▎       | 62/266 [00:14<00:42,  4.77it/s]Loading train:  24%|██▎       | 63/266 [00:14<00:42,  4.74it/s]Loading train:  24%|██▍       | 64/266 [00:14<00:42,  4.71it/s]Loading train:  24%|██▍       | 65/266 [00:15<00:42,  4.70it/s]Loading train:  25%|██▍       | 66/266 [00:15<00:42,  4.68it/s]Loading train:  25%|██▌       | 67/266 [00:15<00:42,  4.67it/s]Loading train:  26%|██▌       | 68/266 [00:15<00:42,  4.65it/s]Loading train:  26%|██▌       | 69/266 [00:15<00:42,  4.64it/s]Loading train:  26%|██▋       | 70/266 [00:16<00:42,  4.60it/s]Loading train:  27%|██▋       | 71/266 [00:16<00:41,  4.65it/s]Loading train:  27%|██▋       | 72/266 [00:16<00:41,  4.65it/s]Loading train:  27%|██▋       | 73/266 [00:16<00:41,  4.67it/s]Loading train:  28%|██▊       | 74/266 [00:17<00:41,  4.65it/s]Loading train:  28%|██▊       | 75/266 [00:17<00:40,  4.68it/s]Loading train:  29%|██▊       | 76/266 [00:17<00:40,  4.66it/s]Loading train:  29%|██▉       | 77/266 [00:17<00:40,  4.66it/s]Loading train:  29%|██▉       | 78/266 [00:17<00:41,  4.52it/s]Loading train:  30%|██▉       | 79/266 [00:18<00:42,  4.42it/s]Loading train:  30%|███       | 80/266 [00:18<00:42,  4.36it/s]Loading train:  30%|███       | 81/266 [00:18<00:43,  4.27it/s]Loading train:  31%|███       | 82/266 [00:18<00:43,  4.23it/s]Loading train:  31%|███       | 83/266 [00:19<00:43,  4.21it/s]Loading train:  32%|███▏      | 84/266 [00:19<00:43,  4.20it/s]Loading train:  32%|███▏      | 85/266 [00:19<00:43,  4.20it/s]Loading train:  32%|███▏      | 86/266 [00:19<00:42,  4.19it/s]Loading train:  33%|███▎      | 87/266 [00:20<00:42,  4.17it/s]Loading train:  33%|███▎      | 88/266 [00:20<00:42,  4.18it/s]Loading train:  33%|███▎      | 89/266 [00:20<00:42,  4.18it/s]Loading train:  34%|███▍      | 90/266 [00:20<00:42,  4.15it/s]Loading train:  34%|███▍      | 91/266 [00:21<00:42,  4.11it/s]Loading train:  35%|███▍      | 92/266 [00:21<00:42,  4.05it/s]Loading train:  35%|███▍      | 93/266 [00:21<00:43,  4.02it/s]Loading train:  35%|███▌      | 94/266 [00:21<00:42,  4.03it/s]Loading train:  36%|███▌      | 95/266 [00:22<00:42,  4.03it/s]Loading train:  36%|███▌      | 96/266 [00:22<00:41,  4.05it/s]Loading train:  36%|███▋      | 97/266 [00:22<00:43,  3.91it/s]Loading train:  37%|███▋      | 98/266 [00:22<00:43,  3.90it/s]Loading train:  37%|███▋      | 99/266 [00:23<00:40,  4.10it/s]Loading train:  38%|███▊      | 100/266 [00:23<00:40,  4.06it/s]Loading train:  38%|███▊      | 101/266 [00:23<00:39,  4.21it/s]Loading train:  38%|███▊      | 102/266 [00:23<00:39,  4.19it/s]Loading train:  39%|███▊      | 103/266 [00:24<00:38,  4.27it/s]Loading train:  39%|███▉      | 104/266 [00:24<00:37,  4.31it/s]Loading train:  39%|███▉      | 105/266 [00:24<00:36,  4.38it/s]Loading train:  40%|███▉      | 106/266 [00:24<00:36,  4.41it/s]Loading train:  40%|████      | 107/266 [00:24<00:35,  4.47it/s]Loading train:  41%|████      | 108/266 [00:25<00:35,  4.47it/s]Loading train:  41%|████      | 109/266 [00:25<00:34,  4.54it/s]Loading train:  41%|████▏     | 110/266 [00:25<00:34,  4.53it/s]Loading train:  42%|████▏     | 111/266 [00:25<00:34,  4.53it/s]Loading train:  42%|████▏     | 112/266 [00:25<00:33,  4.57it/s]Loading train:  42%|████▏     | 113/266 [00:26<00:33,  4.54it/s]Loading train:  43%|████▎     | 114/266 [00:26<00:33,  4.54it/s]Loading train:  43%|████▎     | 115/266 [00:26<00:33,  4.54it/s]Loading train:  44%|████▎     | 116/266 [00:26<00:33,  4.50it/s]Loading train:  44%|████▍     | 117/266 [00:27<00:33,  4.51it/s]Loading train:  44%|████▍     | 118/266 [00:27<00:33,  4.47it/s]Loading train:  45%|████▍     | 119/266 [00:27<00:34,  4.25it/s]Loading train:  45%|████▌     | 120/266 [00:27<00:35,  4.11it/s]Loading train:  45%|████▌     | 121/266 [00:28<00:36,  3.98it/s]Loading train:  46%|████▌     | 122/266 [00:28<00:37,  3.80it/s]Loading train:  46%|████▌     | 123/266 [00:28<00:38,  3.73it/s]Loading train:  47%|████▋     | 124/266 [00:28<00:37,  3.78it/s]Loading train:  47%|████▋     | 125/266 [00:29<00:37,  3.80it/s]Loading train:  47%|████▋     | 126/266 [00:29<00:36,  3.82it/s]Loading train:  48%|████▊     | 127/266 [00:29<00:36,  3.83it/s]Loading train:  48%|████▊     | 128/266 [00:29<00:35,  3.84it/s]Loading train:  48%|████▊     | 129/266 [00:30<00:35,  3.83it/s]Loading train:  49%|████▉     | 130/266 [00:30<00:35,  3.84it/s]Loading train:  49%|████▉     | 131/266 [00:30<00:35,  3.78it/s]Loading train:  50%|████▉     | 132/266 [00:31<00:35,  3.80it/s]Loading train:  50%|█████     | 133/266 [00:31<00:34,  3.82it/s]Loading train:  50%|█████     | 134/266 [00:31<00:34,  3.86it/s]Loading train:  51%|█████     | 135/266 [00:31<00:34,  3.82it/s]Loading train:  51%|█████     | 136/266 [00:32<00:33,  3.83it/s]Loading train:  52%|█████▏    | 137/266 [00:32<00:32,  3.95it/s]Loading train:  52%|█████▏    | 138/266 [00:32<00:32,  4.00it/s]Loading train:  52%|█████▏    | 139/266 [00:32<00:31,  4.08it/s]Loading train:  53%|█████▎    | 140/266 [00:33<00:30,  4.08it/s]Loading train:  53%|█████▎    | 141/266 [00:33<00:30,  4.15it/s]Loading train:  53%|█████▎    | 142/266 [00:33<00:29,  4.20it/s]Loading train:  54%|█████▍    | 143/266 [00:33<00:29,  4.22it/s]Loading train:  54%|█████▍    | 144/266 [00:33<00:28,  4.28it/s]Loading train:  55%|█████▍    | 145/266 [00:34<00:28,  4.32it/s]Loading train:  55%|█████▍    | 146/266 [00:34<00:27,  4.36it/s]Loading train:  55%|█████▌    | 147/266 [00:34<00:27,  4.36it/s]Loading train:  56%|█████▌    | 148/266 [00:34<00:26,  4.38it/s]Loading train:  56%|█████▌    | 149/266 [00:35<00:26,  4.35it/s]Loading train:  56%|█████▋    | 150/266 [00:35<00:26,  4.33it/s]Loading train:  57%|█████▋    | 151/266 [00:35<00:26,  4.27it/s]Loading train:  57%|█████▋    | 152/266 [00:35<00:26,  4.25it/s]Loading train:  58%|█████▊    | 153/266 [00:36<00:26,  4.24it/s]Loading train:  58%|█████▊    | 154/266 [00:36<00:26,  4.25it/s]Loading train:  58%|█████▊    | 155/266 [00:36<00:24,  4.47it/s]Loading train:  59%|█████▊    | 156/266 [00:36<00:23,  4.65it/s]Loading train:  59%|█████▉    | 157/266 [00:36<00:22,  4.80it/s]Loading train:  59%|█████▉    | 158/266 [00:37<00:22,  4.85it/s]Loading train:  60%|█████▉    | 159/266 [00:37<00:22,  4.82it/s]Loading train:  60%|██████    | 160/266 [00:37<00:21,  4.89it/s]Loading train:  61%|██████    | 161/266 [00:37<00:21,  4.90it/s]Loading train:  61%|██████    | 162/266 [00:37<00:21,  4.94it/s]Loading train:  61%|██████▏   | 163/266 [00:38<00:20,  5.01it/s]Loading train:  62%|██████▏   | 164/266 [00:38<00:20,  5.02it/s]Loading train:  62%|██████▏   | 165/266 [00:38<00:19,  5.06it/s]Loading train:  62%|██████▏   | 166/266 [00:38<00:19,  5.06it/s]Loading train:  63%|██████▎   | 167/266 [00:38<00:19,  5.11it/s]Loading train:  63%|██████▎   | 168/266 [00:39<00:19,  5.12it/s]Loading train:  64%|██████▎   | 169/266 [00:39<00:18,  5.15it/s]Loading train:  64%|██████▍   | 170/266 [00:39<00:18,  5.16it/s]Loading train:  64%|██████▍   | 171/266 [00:39<00:18,  5.19it/s]Loading train:  65%|██████▍   | 172/266 [00:39<00:18,  5.20it/s]Loading train:  65%|██████▌   | 173/266 [00:40<00:18,  4.93it/s]Loading train:  65%|██████▌   | 174/266 [00:40<00:19,  4.83it/s]Loading train:  66%|██████▌   | 175/266 [00:40<00:19,  4.74it/s]Loading train:  66%|██████▌   | 176/266 [00:40<00:19,  4.68it/s]Loading train:  67%|██████▋   | 177/266 [00:40<00:19,  4.59it/s]Loading train:  67%|██████▋   | 178/266 [00:41<00:19,  4.61it/s]Loading train:  67%|██████▋   | 179/266 [00:41<00:18,  4.62it/s]Loading train:  68%|██████▊   | 180/266 [00:41<00:18,  4.59it/s]Loading train:  68%|██████▊   | 181/266 [00:41<00:18,  4.57it/s]Loading train:  68%|██████▊   | 182/266 [00:42<00:18,  4.53it/s]Loading train:  69%|██████▉   | 183/266 [00:42<00:18,  4.45it/s]Loading train:  69%|██████▉   | 184/266 [00:42<00:18,  4.47it/s]Loading train:  70%|██████▉   | 185/266 [00:42<00:17,  4.54it/s]Loading train:  70%|██████▉   | 186/266 [00:42<00:17,  4.56it/s]Loading train:  70%|███████   | 187/266 [00:43<00:17,  4.61it/s]Loading train:  71%|███████   | 188/266 [00:43<00:16,  4.64it/s]Loading train:  71%|███████   | 189/266 [00:43<00:16,  4.64it/s]Loading train:  71%|███████▏  | 190/266 [00:43<00:16,  4.63it/s]Loading train:  72%|███████▏  | 191/266 [00:44<00:16,  4.52it/s]Loading train:  72%|███████▏  | 192/266 [00:44<00:16,  4.50it/s]Loading train:  73%|███████▎  | 193/266 [00:44<00:16,  4.43it/s]Loading train:  73%|███████▎  | 194/266 [00:44<00:16,  4.24it/s]Loading train:  73%|███████▎  | 195/266 [00:44<00:16,  4.34it/s]Loading train:  74%|███████▎  | 196/266 [00:45<00:15,  4.41it/s]Loading train:  74%|███████▍  | 197/266 [00:45<00:15,  4.44it/s]Loading train:  74%|███████▍  | 198/266 [00:45<00:15,  4.49it/s]Loading train:  75%|███████▍  | 199/266 [00:45<00:14,  4.50it/s]Loading train:  75%|███████▌  | 200/266 [00:46<00:14,  4.44it/s]Loading train:  76%|███████▌  | 201/266 [00:46<00:14,  4.48it/s]Loading train:  76%|███████▌  | 202/266 [00:46<00:14,  4.52it/s]Loading train:  76%|███████▋  | 203/266 [00:46<00:13,  4.56it/s]Loading train:  77%|███████▋  | 204/266 [00:46<00:13,  4.59it/s]Loading train:  77%|███████▋  | 205/266 [00:47<00:13,  4.62it/s]Loading train:  77%|███████▋  | 206/266 [00:47<00:13,  4.60it/s]Loading train:  78%|███████▊  | 207/266 [00:47<00:12,  4.59it/s]Loading train:  78%|███████▊  | 208/266 [00:47<00:12,  4.58it/s]Loading train:  79%|███████▊  | 209/266 [00:48<00:12,  4.55it/s]Loading train:  79%|███████▉  | 210/266 [00:48<00:12,  4.56it/s]Loading train:  79%|███████▉  | 211/266 [00:48<00:12,  4.48it/s]Loading train:  80%|███████▉  | 212/266 [00:48<00:12,  4.49it/s]Loading train:  80%|████████  | 213/266 [00:48<00:11,  4.55it/s]Loading train:  80%|████████  | 214/266 [00:49<00:11,  4.59it/s]Loading train:  81%|████████  | 215/266 [00:49<00:10,  4.68it/s]Loading train:  81%|████████  | 216/266 [00:49<00:10,  4.68it/s]Loading train:  82%|████████▏ | 217/266 [00:49<00:10,  4.73it/s]Loading train:  82%|████████▏ | 218/266 [00:49<00:10,  4.76it/s]Loading train:  82%|████████▏ | 219/266 [00:50<00:10,  4.69it/s]Loading train:  83%|████████▎ | 220/266 [00:50<00:09,  4.68it/s]Loading train:  83%|████████▎ | 221/266 [00:50<00:09,  4.69it/s]Loading train:  83%|████████▎ | 222/266 [00:50<00:09,  4.71it/s]Loading train:  84%|████████▍ | 223/266 [00:51<00:09,  4.73it/s]Loading train:  84%|████████▍ | 224/266 [00:51<00:08,  4.72it/s]Loading train:  85%|████████▍ | 225/266 [00:51<00:08,  4.76it/s]Loading train:  85%|████████▍ | 226/266 [00:51<00:08,  4.78it/s]Loading train:  85%|████████▌ | 227/266 [00:51<00:08,  4.82it/s]Loading train:  86%|████████▌ | 228/266 [00:52<00:07,  4.83it/s]Loading train:  86%|████████▌ | 229/266 [00:52<00:07,  4.84it/s]Loading train:  86%|████████▋ | 230/266 [00:52<00:07,  4.87it/s]Loading train:  87%|████████▋ | 231/266 [00:52<00:07,  4.76it/s]Loading train:  87%|████████▋ | 232/266 [00:52<00:07,  4.72it/s]Loading train:  88%|████████▊ | 233/266 [00:53<00:07,  4.66it/s]Loading train:  88%|████████▊ | 234/266 [00:53<00:06,  4.64it/s]Loading train:  88%|████████▊ | 235/266 [00:53<00:06,  4.55it/s]Loading train:  89%|████████▊ | 236/266 [00:53<00:06,  4.56it/s]Loading train:  89%|████████▉ | 237/266 [00:53<00:06,  4.51it/s]Loading train:  89%|████████▉ | 238/266 [00:54<00:06,  4.48it/s]Loading train:  90%|████████▉ | 239/266 [00:54<00:05,  4.51it/s]Loading train:  90%|█████████ | 240/266 [00:54<00:05,  4.52it/s]Loading train:  91%|█████████ | 241/266 [00:54<00:05,  4.52it/s]Loading train:  91%|█████████ | 242/266 [00:55<00:05,  4.51it/s]Loading train:  91%|█████████▏| 243/266 [00:55<00:05,  4.50it/s]Loading train:  92%|█████████▏| 244/266 [00:55<00:04,  4.51it/s]Loading train:  92%|█████████▏| 245/266 [00:55<00:04,  4.49it/s]Loading train:  92%|█████████▏| 246/266 [00:55<00:04,  4.51it/s]Loading train:  93%|█████████▎| 247/266 [00:56<00:04,  4.53it/s]Loading train:  93%|█████████▎| 248/266 [00:56<00:03,  4.53it/s]Loading train:  94%|█████████▎| 249/266 [00:56<00:03,  4.39it/s]Loading train:  94%|█████████▍| 250/266 [00:56<00:03,  4.31it/s]Loading train:  94%|█████████▍| 251/266 [00:57<00:03,  4.25it/s]Loading train:  95%|█████████▍| 252/266 [00:57<00:03,  4.18it/s]Loading train:  95%|█████████▌| 253/266 [00:57<00:03,  4.16it/s]Loading train:  95%|█████████▌| 254/266 [00:57<00:02,  4.15it/s]Loading train:  96%|█████████▌| 255/266 [00:58<00:02,  4.09it/s]Loading train:  96%|█████████▌| 256/266 [00:58<00:02,  4.09it/s]Loading train:  97%|█████████▋| 257/266 [00:58<00:02,  4.11it/s]Loading train:  97%|█████████▋| 258/266 [00:58<00:01,  4.14it/s]Loading train:  97%|█████████▋| 259/266 [00:59<00:01,  4.15it/s]Loading train:  98%|█████████▊| 260/266 [00:59<00:01,  4.17it/s]Loading train:  98%|█████████▊| 261/266 [00:59<00:01,  4.15it/s]Loading train:  98%|█████████▊| 262/266 [00:59<00:00,  4.14it/s]Loading train:  99%|█████████▉| 263/266 [01:00<00:00,  4.17it/s]Loading train:  99%|█████████▉| 264/266 [01:00<00:00,  4.15it/s]Loading train: 100%|█████████▉| 265/266 [01:00<00:00,  4.19it/s]Loading train: 100%|██████████| 266/266 [01:00<00:00,  4.20it/s]Loading train: 100%|██████████| 266/266 [01:00<00:00,  4.38it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   3%|▎         | 7/266 [00:00<00:03, 66.31it/s]concatenating: train:   5%|▍         | 13/266 [00:00<00:03, 63.87it/s]concatenating: train:   7%|▋         | 19/266 [00:00<00:03, 61.82it/s]concatenating: train:   9%|▉         | 25/266 [00:00<00:03, 60.87it/s]concatenating: train:  12%|█▏        | 32/266 [00:00<00:03, 62.76it/s]concatenating: train:  14%|█▍        | 38/266 [00:00<00:03, 61.84it/s]concatenating: train:  17%|█▋        | 45/266 [00:00<00:03, 63.27it/s]concatenating: train:  20%|█▉        | 53/266 [00:00<00:03, 66.16it/s]concatenating: train:  23%|██▎       | 61/266 [00:00<00:02, 68.70it/s]concatenating: train:  26%|██▌       | 68/266 [00:01<00:02, 67.43it/s]concatenating: train:  28%|██▊       | 75/266 [00:01<00:02, 68.17it/s]concatenating: train:  31%|███       | 82/266 [00:01<00:02, 66.34it/s]concatenating: train:  33%|███▎      | 89/266 [00:01<00:02, 66.22it/s]concatenating: train:  36%|███▌      | 96/266 [00:01<00:02, 65.38it/s]concatenating: train:  39%|███▉      | 104/266 [00:01<00:02, 66.99it/s]concatenating: train:  42%|████▏     | 111/266 [00:01<00:02, 66.98it/s]concatenating: train:  44%|████▍     | 118/266 [00:01<00:02, 66.40it/s]concatenating: train:  47%|████▋     | 125/266 [00:01<00:02, 61.49it/s]concatenating: train:  50%|████▉     | 132/266 [00:02<00:02, 58.43it/s]concatenating: train:  52%|█████▏    | 138/266 [00:02<00:02, 58.29it/s]concatenating: train:  55%|█████▍    | 145/266 [00:02<00:02, 59.79it/s]concatenating: train:  58%|█████▊    | 153/266 [00:02<00:01, 62.40it/s]concatenating: train:  61%|██████    | 161/266 [00:02<00:01, 65.85it/s]concatenating: train:  64%|██████▎   | 169/266 [00:02<00:01, 68.78it/s]concatenating: train:  67%|██████▋   | 177/266 [00:02<00:01, 70.20it/s]concatenating: train:  70%|██████▉   | 185/266 [00:02<00:01, 66.92it/s]concatenating: train:  73%|███████▎  | 193/266 [00:02<00:01, 67.42it/s]concatenating: train:  75%|███████▌  | 200/266 [00:03<00:01, 63.58it/s]concatenating: train:  78%|███████▊  | 208/266 [00:03<00:00, 65.62it/s]concatenating: train:  81%|████████  | 215/266 [00:03<00:00, 64.72it/s]concatenating: train:  84%|████████▍ | 223/266 [00:03<00:00, 66.41it/s]concatenating: train:  87%|████████▋ | 231/266 [00:03<00:00, 69.11it/s]concatenating: train:  89%|████████▉ | 238/266 [00:03<00:00, 65.59it/s]concatenating: train:  92%|█████████▏| 245/266 [00:03<00:00, 63.05it/s]concatenating: train:  95%|█████████▍| 252/266 [00:03<00:00, 60.98it/s]concatenating: train:  97%|█████████▋| 259/266 [00:04<00:00, 59.17it/s]concatenating: train: 100%|█████████▉| 265/266 [00:04<00:00, 55.71it/s]concatenating: train: 100%|██████████| 266/266 [00:04<00:00, 63.92it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:00,  4.08it/s]Loading test:  50%|█████     | 2/4 [00:00<00:00,  4.14it/s]Loading test:  75%|███████▌  | 3/4 [00:00<00:00,  4.21it/s]Loading test: 100%|██████████| 4/4 [00:00<00:00,  4.22it/s]Loading test: 100%|██████████| 4/4 [00:00<00:00,  4.24it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 295.20it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:00<01:14,  3.56it/s]Loading trainS:   1%|          | 2/266 [00:00<01:15,  3.52it/s]Loading trainS:   1%|          | 3/266 [00:00<01:09,  3.77it/s]Loading trainS:   2%|▏         | 4/266 [00:01<01:06,  3.93it/s]Loading trainS:   2%|▏         | 5/266 [00:01<01:06,  3.90it/s]Loading trainS:   2%|▏         | 6/266 [00:01<01:07,  3.88it/s]Loading trainS:   3%|▎         | 7/266 [00:01<01:06,  3.87it/s]Loading trainS:   3%|▎         | 8/266 [00:02<01:06,  3.89it/s]Loading trainS:   3%|▎         | 9/266 [00:02<01:06,  3.88it/s]Loading trainS:   4%|▍         | 10/266 [00:02<01:05,  3.91it/s]Loading trainS:   4%|▍         | 11/266 [00:02<01:05,  3.90it/s]Loading trainS:   5%|▍         | 12/266 [00:03<01:04,  3.91it/s]Loading trainS:   5%|▍         | 13/266 [00:03<01:05,  3.88it/s]Loading trainS:   5%|▌         | 14/266 [00:03<01:04,  3.89it/s]Loading trainS:   6%|▌         | 15/266 [00:03<01:04,  3.89it/s]Loading trainS:   6%|▌         | 16/266 [00:04<01:04,  3.86it/s]Loading trainS:   6%|▋         | 17/266 [00:04<01:04,  3.84it/s]Loading trainS:   7%|▋         | 18/266 [00:04<01:04,  3.86it/s]Loading trainS:   7%|▋         | 19/266 [00:04<01:04,  3.85it/s]Loading trainS:   8%|▊         | 20/266 [00:05<01:03,  3.87it/s]Loading trainS:   8%|▊         | 21/266 [00:05<01:05,  3.74it/s]Loading trainS:   8%|▊         | 22/266 [00:05<01:04,  3.80it/s]Loading trainS:   9%|▊         | 23/266 [00:05<01:03,  3.84it/s]Loading trainS:   9%|▉         | 24/266 [00:06<01:02,  3.90it/s]Loading trainS:   9%|▉         | 25/266 [00:06<01:00,  3.98it/s]Loading trainS:  10%|▉         | 26/266 [00:06<00:59,  4.02it/s]Loading trainS:  10%|█         | 27/266 [00:06<00:58,  4.08it/s]Loading trainS:  11%|█         | 28/266 [00:07<00:57,  4.12it/s]Loading trainS:  11%|█         | 29/266 [00:07<00:57,  4.14it/s]Loading trainS:  11%|█▏        | 30/266 [00:07<00:57,  4.10it/s]Loading trainS:  12%|█▏        | 31/266 [00:07<00:57,  4.09it/s]Loading trainS:  12%|█▏        | 32/266 [00:08<00:57,  4.10it/s]Loading trainS:  12%|█▏        | 33/266 [00:08<00:57,  4.08it/s]Loading trainS:  13%|█▎        | 34/266 [00:08<00:57,  4.05it/s]Loading trainS:  13%|█▎        | 35/266 [00:08<00:57,  4.04it/s]Loading trainS:  14%|█▎        | 36/266 [00:09<00:56,  4.05it/s]Loading trainS:  14%|█▍        | 37/266 [00:09<00:56,  4.03it/s]Loading trainS:  14%|█▍        | 38/266 [00:09<00:57,  3.99it/s]Loading trainS:  15%|█▍        | 39/266 [00:09<00:57,  3.94it/s]Loading trainS:  15%|█▌        | 40/266 [00:10<00:56,  4.00it/s]Loading trainS:  15%|█▌        | 41/266 [00:10<00:55,  4.02it/s]Loading trainS:  16%|█▌        | 42/266 [00:10<00:52,  4.26it/s]Loading trainS:  16%|█▌        | 43/266 [00:10<00:49,  4.46it/s]Loading trainS:  17%|█▋        | 44/266 [00:10<00:48,  4.61it/s]Loading trainS:  17%|█▋        | 45/266 [00:11<00:46,  4.71it/s]Loading trainS:  17%|█▋        | 46/266 [00:11<00:46,  4.77it/s]Loading trainS:  18%|█▊        | 47/266 [00:11<00:45,  4.83it/s]Loading trainS:  18%|█▊        | 48/266 [00:11<00:44,  4.86it/s]Loading trainS:  18%|█▊        | 49/266 [00:11<00:44,  4.88it/s]Loading trainS:  19%|█▉        | 50/266 [00:12<00:44,  4.90it/s]Loading trainS:  19%|█▉        | 51/266 [00:12<00:43,  4.91it/s]Loading trainS:  20%|█▉        | 52/266 [00:12<00:43,  4.91it/s]Loading trainS:  20%|█▉        | 53/266 [00:12<00:43,  4.87it/s]Loading trainS:  20%|██        | 54/266 [00:13<00:43,  4.86it/s]Loading trainS:  21%|██        | 55/266 [00:13<00:43,  4.87it/s]Loading trainS:  21%|██        | 56/266 [00:13<00:43,  4.88it/s]Loading trainS:  21%|██▏       | 57/266 [00:13<00:42,  4.89it/s]Loading trainS:  22%|██▏       | 58/266 [00:13<00:42,  4.91it/s]Loading trainS:  22%|██▏       | 59/266 [00:14<00:41,  4.93it/s]Loading trainS:  23%|██▎       | 60/266 [00:14<00:42,  4.85it/s]Loading trainS:  23%|██▎       | 61/266 [00:14<00:42,  4.78it/s]Loading trainS:  23%|██▎       | 62/266 [00:14<00:44,  4.54it/s]Loading trainS:  24%|██▎       | 63/266 [00:14<00:44,  4.55it/s]Loading trainS:  24%|██▍       | 64/266 [00:15<00:45,  4.43it/s]Loading trainS:  24%|██▍       | 65/266 [00:15<00:44,  4.47it/s]Loading trainS:  25%|██▍       | 66/266 [00:15<00:44,  4.51it/s]Loading trainS:  25%|██▌       | 67/266 [00:15<00:43,  4.55it/s]Loading trainS:  26%|██▌       | 68/266 [00:16<00:43,  4.56it/s]Loading trainS:  26%|██▌       | 69/266 [00:16<00:43,  4.57it/s]Loading trainS:  26%|██▋       | 70/266 [00:16<00:42,  4.60it/s]Loading trainS:  27%|██▋       | 71/266 [00:16<00:42,  4.61it/s]Loading trainS:  27%|██▋       | 72/266 [00:16<00:41,  4.63it/s]Loading trainS:  27%|██▋       | 73/266 [00:17<00:42,  4.56it/s]Loading trainS:  28%|██▊       | 74/266 [00:17<00:42,  4.53it/s]Loading trainS:  28%|██▊       | 75/266 [00:17<00:42,  4.52it/s]Loading trainS:  29%|██▊       | 76/266 [00:17<00:41,  4.54it/s]Loading trainS:  29%|██▉       | 77/266 [00:18<00:41,  4.56it/s]Loading trainS:  29%|██▉       | 78/266 [00:18<00:42,  4.40it/s]Loading trainS:  30%|██▉       | 79/266 [00:18<00:43,  4.29it/s]Loading trainS:  30%|███       | 80/266 [00:18<00:45,  4.12it/s]Loading trainS:  30%|███       | 81/266 [00:19<00:45,  4.09it/s]Loading trainS:  31%|███       | 82/266 [00:19<00:45,  4.02it/s]Loading trainS:  31%|███       | 83/266 [00:19<00:45,  4.03it/s]Loading trainS:  32%|███▏      | 84/266 [00:19<00:46,  3.91it/s]Loading trainS:  32%|███▏      | 85/266 [00:20<00:45,  3.95it/s]Loading trainS:  32%|███▏      | 86/266 [00:20<00:45,  3.99it/s]Loading trainS:  33%|███▎      | 87/266 [00:20<00:44,  4.01it/s]Loading trainS:  33%|███▎      | 88/266 [00:20<00:44,  4.01it/s]Loading trainS:  33%|███▎      | 89/266 [00:21<00:43,  4.03it/s]Loading trainS:  34%|███▍      | 90/266 [00:21<00:43,  4.03it/s]Loading trainS:  34%|███▍      | 91/266 [00:21<00:43,  4.05it/s]Loading trainS:  35%|███▍      | 92/266 [00:21<00:42,  4.06it/s]Loading trainS:  35%|███▍      | 93/266 [00:22<00:42,  4.08it/s]Loading trainS:  35%|███▌      | 94/266 [00:22<00:42,  4.07it/s]Loading trainS:  36%|███▌      | 95/266 [00:22<00:41,  4.07it/s]Loading trainS:  36%|███▌      | 96/266 [00:22<00:41,  4.11it/s]Loading trainS:  36%|███▋      | 97/266 [00:23<00:42,  3.93it/s]Loading trainS:  37%|███▋      | 98/266 [00:23<00:42,  3.93it/s]Loading trainS:  37%|███▋      | 99/266 [00:23<00:40,  4.15it/s]Loading trainS:  38%|███▊      | 100/266 [00:23<00:39,  4.19it/s]Loading trainS:  38%|███▊      | 101/266 [00:23<00:38,  4.30it/s]Loading trainS:  38%|███▊      | 102/266 [00:24<00:37,  4.32it/s]Loading trainS:  39%|███▊      | 103/266 [00:24<00:36,  4.41it/s]Loading trainS:  39%|███▉      | 104/266 [00:24<00:36,  4.41it/s]Loading trainS:  39%|███▉      | 105/266 [00:24<00:36,  4.45it/s]Loading trainS:  40%|███▉      | 106/266 [00:25<00:35,  4.53it/s]Loading trainS:  40%|████      | 107/266 [00:25<00:34,  4.58it/s]Loading trainS:  41%|████      | 108/266 [00:25<00:34,  4.60it/s]Loading trainS:  41%|████      | 109/266 [00:25<00:33,  4.64it/s]Loading trainS:  41%|████▏     | 110/266 [00:25<00:33,  4.66it/s]Loading trainS:  42%|████▏     | 111/266 [00:26<00:33,  4.68it/s]Loading trainS:  42%|████▏     | 112/266 [00:26<00:32,  4.68it/s]Loading trainS:  42%|████▏     | 113/266 [00:26<00:32,  4.69it/s]Loading trainS:  43%|████▎     | 114/266 [00:26<00:32,  4.69it/s]Loading trainS:  43%|████▎     | 115/266 [00:26<00:32,  4.70it/s]Loading trainS:  44%|████▎     | 116/266 [00:27<00:31,  4.71it/s]Loading trainS:  44%|████▍     | 117/266 [00:27<00:31,  4.67it/s]Loading trainS:  44%|████▍     | 118/266 [00:27<00:31,  4.69it/s]Loading trainS:  45%|████▍     | 119/266 [00:27<00:33,  4.45it/s]Loading trainS:  45%|████▌     | 120/266 [00:28<00:34,  4.26it/s]Loading trainS:  45%|████▌     | 121/266 [00:28<00:35,  4.13it/s]Loading trainS:  46%|████▌     | 122/266 [00:28<00:35,  4.01it/s]Loading trainS:  46%|████▌     | 123/266 [00:28<00:35,  3.99it/s]Loading trainS:  47%|████▋     | 124/266 [00:29<00:35,  3.95it/s]Loading trainS:  47%|████▋     | 125/266 [00:29<00:35,  3.94it/s]Loading trainS:  47%|████▋     | 126/266 [00:29<00:35,  3.95it/s]Loading trainS:  48%|████▊     | 127/266 [00:29<00:35,  3.95it/s]Loading trainS:  48%|████▊     | 128/266 [00:30<00:34,  3.96it/s]Loading trainS:  48%|████▊     | 129/266 [00:30<00:34,  3.97it/s]Loading trainS:  49%|████▉     | 130/266 [00:30<00:34,  3.98it/s]Loading trainS:  49%|████▉     | 131/266 [00:30<00:33,  3.97it/s]Loading trainS:  50%|████▉     | 132/266 [00:31<00:34,  3.94it/s]Loading trainS:  50%|█████     | 133/266 [00:31<00:34,  3.90it/s]Loading trainS:  50%|█████     | 134/266 [00:31<00:35,  3.77it/s]Loading trainS:  51%|█████     | 135/266 [00:31<00:34,  3.84it/s]Loading trainS:  51%|█████     | 136/266 [00:32<00:33,  3.89it/s]Loading trainS:  52%|█████▏    | 137/266 [00:32<00:31,  4.05it/s]Loading trainS:  52%|█████▏    | 138/266 [00:32<00:30,  4.13it/s]Loading trainS:  52%|█████▏    | 139/266 [00:32<00:30,  4.19it/s]Loading trainS:  53%|█████▎    | 140/266 [00:33<00:29,  4.20it/s]Loading trainS:  53%|█████▎    | 141/266 [00:33<00:29,  4.24it/s]Loading trainS:  53%|█████▎    | 142/266 [00:33<00:29,  4.27it/s]Loading trainS:  54%|█████▍    | 143/266 [00:33<00:28,  4.30it/s]Loading trainS:  54%|█████▍    | 144/266 [00:34<00:28,  4.33it/s]Loading trainS:  55%|█████▍    | 145/266 [00:34<00:27,  4.34it/s]Loading trainS:  55%|█████▍    | 146/266 [00:34<00:27,  4.34it/s]Loading trainS:  55%|█████▌    | 147/266 [00:34<00:27,  4.26it/s]Loading trainS:  56%|█████▌    | 148/266 [00:34<00:27,  4.25it/s]Loading trainS:  56%|█████▌    | 149/266 [00:35<00:27,  4.28it/s]Loading trainS:  56%|█████▋    | 150/266 [00:35<00:26,  4.31it/s]Loading trainS:  57%|█████▋    | 151/266 [00:35<00:26,  4.32it/s]Loading trainS:  57%|█████▋    | 152/266 [00:35<00:26,  4.33it/s]Loading trainS:  58%|█████▊    | 153/266 [00:36<00:26,  4.34it/s]Loading trainS:  58%|█████▊    | 154/266 [00:36<00:25,  4.35it/s]Loading trainS:  58%|█████▊    | 155/266 [00:36<00:24,  4.57it/s]Loading trainS:  59%|█████▊    | 156/266 [00:36<00:23,  4.67it/s]Loading trainS:  59%|█████▉    | 157/266 [00:36<00:22,  4.79it/s]Loading trainS:  59%|█████▉    | 158/266 [00:37<00:23,  4.64it/s]Loading trainS:  60%|█████▉    | 159/266 [00:37<00:22,  4.75it/s]Loading trainS:  60%|██████    | 160/266 [00:37<00:21,  4.87it/s]Loading trainS:  61%|██████    | 161/266 [00:37<00:21,  4.96it/s]Loading trainS:  61%|██████    | 162/266 [00:37<00:20,  5.01it/s]Loading trainS:  61%|██████▏   | 163/266 [00:38<00:20,  5.04it/s]Loading trainS:  62%|██████▏   | 164/266 [00:38<00:20,  5.08it/s]Loading trainS:  62%|██████▏   | 165/266 [00:38<00:19,  5.10it/s]Loading trainS:  62%|██████▏   | 166/266 [00:38<00:19,  5.12it/s]Loading trainS:  63%|██████▎   | 167/266 [00:38<00:19,  5.15it/s]Loading trainS:  63%|██████▎   | 168/266 [00:39<00:18,  5.16it/s]Loading trainS:  64%|██████▎   | 169/266 [00:39<00:18,  5.18it/s]Loading trainS:  64%|██████▍   | 170/266 [00:39<00:18,  5.19it/s]Loading trainS:  64%|██████▍   | 171/266 [00:39<00:18,  5.18it/s]Loading trainS:  65%|██████▍   | 172/266 [00:39<00:18,  5.16it/s]Loading trainS:  65%|██████▌   | 173/266 [00:40<00:18,  4.96it/s]Loading trainS:  65%|██████▌   | 174/266 [00:40<00:18,  4.87it/s]Loading trainS:  66%|██████▌   | 175/266 [00:40<00:19,  4.77it/s]Loading trainS:  66%|██████▌   | 176/266 [00:40<00:19,  4.72it/s]Loading trainS:  67%|██████▋   | 177/266 [00:40<00:18,  4.69it/s]Loading trainS:  67%|██████▋   | 178/266 [00:41<00:18,  4.68it/s]Loading trainS:  67%|██████▋   | 179/266 [00:41<00:18,  4.68it/s]Loading trainS:  68%|██████▊   | 180/266 [00:41<00:18,  4.67it/s]Loading trainS:  68%|██████▊   | 181/266 [00:41<00:18,  4.60it/s]Loading trainS:  68%|██████▊   | 182/266 [00:42<00:18,  4.53it/s]Loading trainS:  69%|██████▉   | 183/266 [00:42<00:18,  4.41it/s]Loading trainS:  69%|██████▉   | 184/266 [00:42<00:18,  4.39it/s]Loading trainS:  70%|██████▉   | 185/266 [00:42<00:18,  4.45it/s]Loading trainS:  70%|██████▉   | 186/266 [00:42<00:17,  4.49it/s]Loading trainS:  70%|███████   | 187/266 [00:43<00:17,  4.53it/s]Loading trainS:  71%|███████   | 188/266 [00:43<00:17,  4.57it/s]Loading trainS:  71%|███████   | 189/266 [00:43<00:16,  4.60it/s]Loading trainS:  71%|███████▏  | 190/266 [00:43<00:16,  4.63it/s]Loading trainS:  72%|███████▏  | 191/266 [00:44<00:16,  4.52it/s]Loading trainS:  72%|███████▏  | 192/266 [00:44<00:16,  4.53it/s]Loading trainS:  73%|███████▎  | 193/266 [00:44<00:16,  4.45it/s]Loading trainS:  73%|███████▎  | 194/266 [00:44<00:16,  4.26it/s]Loading trainS:  73%|███████▎  | 195/266 [00:45<00:16,  4.33it/s]Loading trainS:  74%|███████▎  | 196/266 [00:45<00:16,  4.32it/s]Loading trainS:  74%|███████▍  | 197/266 [00:45<00:15,  4.34it/s]Loading trainS:  74%|███████▍  | 198/266 [00:45<00:15,  4.38it/s]Loading trainS:  75%|███████▍  | 199/266 [00:45<00:15,  4.28it/s]Loading trainS:  75%|███████▌  | 200/266 [00:46<00:15,  4.35it/s]Loading trainS:  76%|███████▌  | 201/266 [00:46<00:14,  4.42it/s]Loading trainS:  76%|███████▌  | 202/266 [00:46<00:14,  4.49it/s]Loading trainS:  76%|███████▋  | 203/266 [00:46<00:13,  4.52it/s]Loading trainS:  77%|███████▋  | 204/266 [00:47<00:13,  4.55it/s]Loading trainS:  77%|███████▋  | 205/266 [00:47<00:13,  4.58it/s]Loading trainS:  77%|███████▋  | 206/266 [00:47<00:13,  4.58it/s]Loading trainS:  78%|███████▊  | 207/266 [00:47<00:12,  4.59it/s]Loading trainS:  78%|███████▊  | 208/266 [00:47<00:12,  4.57it/s]Loading trainS:  79%|███████▊  | 209/266 [00:48<00:12,  4.53it/s]Loading trainS:  79%|███████▉  | 210/266 [00:48<00:12,  4.51it/s]Loading trainS:  79%|███████▉  | 211/266 [00:48<00:12,  4.30it/s]Loading trainS:  80%|███████▉  | 212/266 [00:48<00:12,  4.41it/s]Loading trainS:  80%|████████  | 213/266 [00:49<00:11,  4.52it/s]Loading trainS:  80%|████████  | 214/266 [00:49<00:11,  4.59it/s]Loading trainS:  81%|████████  | 215/266 [00:49<00:10,  4.66it/s]Loading trainS:  81%|████████  | 216/266 [00:49<00:10,  4.70it/s]Loading trainS:  82%|████████▏ | 217/266 [00:49<00:10,  4.74it/s]Loading trainS:  82%|████████▏ | 218/266 [00:50<00:10,  4.78it/s]Loading trainS:  82%|████████▏ | 219/266 [00:50<00:09,  4.80it/s]Loading trainS:  83%|████████▎ | 220/266 [00:50<00:09,  4.83it/s]Loading trainS:  83%|████████▎ | 221/266 [00:50<00:09,  4.84it/s]Loading trainS:  83%|████████▎ | 222/266 [00:50<00:09,  4.86it/s]Loading trainS:  84%|████████▍ | 223/266 [00:51<00:08,  4.86it/s]Loading trainS:  84%|████████▍ | 224/266 [00:51<00:08,  4.85it/s]Loading trainS:  85%|████████▍ | 225/266 [00:51<00:08,  4.85it/s]Loading trainS:  85%|████████▍ | 226/266 [00:51<00:08,  4.86it/s]Loading trainS:  85%|████████▌ | 227/266 [00:51<00:08,  4.87it/s]Loading trainS:  86%|████████▌ | 228/266 [00:52<00:07,  4.85it/s]Loading trainS:  86%|████████▌ | 229/266 [00:52<00:07,  4.83it/s]Loading trainS:  86%|████████▋ | 230/266 [00:52<00:07,  4.79it/s]Loading trainS:  87%|████████▋ | 231/266 [00:52<00:07,  4.70it/s]Loading trainS:  87%|████████▋ | 232/266 [00:52<00:07,  4.62it/s]Loading trainS:  88%|████████▊ | 233/266 [00:53<00:07,  4.57it/s]Loading trainS:  88%|████████▊ | 234/266 [00:53<00:07,  4.56it/s]Loading trainS:  88%|████████▊ | 235/266 [00:53<00:06,  4.55it/s]Loading trainS:  89%|████████▊ | 236/266 [00:53<00:06,  4.56it/s]Loading trainS:  89%|████████▉ | 237/266 [00:54<00:06,  4.55it/s]Loading trainS:  89%|████████▉ | 238/266 [00:54<00:06,  4.52it/s]Loading trainS:  90%|████████▉ | 239/266 [00:54<00:05,  4.53it/s]Loading trainS:  90%|█████████ | 240/266 [00:54<00:05,  4.53it/s]Loading trainS:  91%|█████████ | 241/266 [00:54<00:05,  4.51it/s]Loading trainS:  91%|█████████ | 242/266 [00:55<00:05,  4.49it/s]Loading trainS:  91%|█████████▏| 243/266 [00:55<00:05,  4.50it/s]Loading trainS:  92%|█████████▏| 244/266 [00:55<00:05,  4.36it/s]Loading trainS:  92%|█████████▏| 245/266 [00:55<00:04,  4.40it/s]Loading trainS:  92%|█████████▏| 246/266 [00:56<00:04,  4.45it/s]Loading trainS:  93%|█████████▎| 247/266 [00:56<00:04,  4.47it/s]Loading trainS:  93%|█████████▎| 248/266 [00:56<00:04,  4.48it/s]Loading trainS:  94%|█████████▎| 249/266 [00:56<00:03,  4.38it/s]Loading trainS:  94%|█████████▍| 250/266 [00:57<00:03,  4.32it/s]Loading trainS:  94%|█████████▍| 251/266 [00:57<00:03,  4.27it/s]Loading trainS:  95%|█████████▍| 252/266 [00:57<00:03,  4.26it/s]Loading trainS:  95%|█████████▌| 253/266 [00:57<00:03,  4.23it/s]Loading trainS:  95%|█████████▌| 254/266 [00:57<00:02,  4.21it/s]Loading trainS:  96%|█████████▌| 255/266 [00:58<00:02,  4.17it/s]Loading trainS:  96%|█████████▌| 256/266 [00:58<00:02,  4.15it/s]Loading trainS:  97%|█████████▋| 257/266 [00:58<00:02,  4.13it/s]Loading trainS:  97%|█████████▋| 258/266 [00:58<00:01,  4.12it/s]Loading trainS:  97%|█████████▋| 259/266 [00:59<00:01,  4.14it/s]Loading trainS:  98%|█████████▊| 260/266 [00:59<00:01,  4.15it/s]Loading trainS:  98%|█████████▊| 261/266 [00:59<00:01,  4.16it/s]Loading trainS:  98%|█████████▊| 262/266 [00:59<00:00,  4.16it/s]Loading trainS:  99%|█████████▉| 263/266 [01:00<00:00,  4.15it/s]Loading trainS:  99%|█████████▉| 264/266 [01:00<00:00,  4.13it/s]Loading trainS: 100%|█████████▉| 265/266 [01:00<00:00,  4.16it/s]Loading trainS: 100%|██████████| 266/266 [01:00<00:00,  4.07it/s]Loading trainS: 100%|██████████| 266/266 [01:00<00:00,  4.37it/s]
Loading testS:   0%|          | 0/4 [00:00<?, ?it/s]Loading testS:  25%|██▌       | 1/4 [00:00<00:00,  4.14it/s]Loading testS:  50%|█████     | 2/4 [00:00<00:00,  4.16it/s]Loading testS:  75%|███████▌  | 3/4 [00:00<00:00,  3.70it/s]Loading testS: 100%|██████████| 4/4 [00:01<00:00,  1.92it/s]Loading testS: 100%|██████████| 4/4 [00:01<00:00,  2.08it/s]
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2020-01-21 23:44:40.552675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 23:44:40.552751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 23:44:40.552764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 23:44:40.552771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 23:44:40.553694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 54, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 256 samples
Epoch 1/300
 - 48s - loss: 0.3055 - acc: 0.9742 - mDice: 0.4025 - val_loss: 0.3122 - val_acc: 0.9868 - val_mDice: 0.3826

Epoch 00001: val_mDice improved from -inf to 0.38264, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 44s - loss: 0.1392 - acc: 0.9851 - mDice: 0.7293 - val_loss: 0.3003 - val_acc: 0.9886 - val_mDice: 0.4055

Epoch 00002: val_mDice improved from 0.38264 to 0.40550, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 44s - loss: 0.1164 - acc: 0.9876 - mDice: 0.7737 - val_loss: 0.2955 - val_acc: 0.9895 - val_mDice: 0.4146

Epoch 00003: val_mDice improved from 0.40550 to 0.41460, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300
 - 44s - loss: 0.1069 - acc: 0.9886 - mDice: 0.7921 - val_loss: 0.2894 - val_acc: 0.9901 - val_mDice: 0.4263

Epoch 00004: val_mDice improved from 0.41460 to 0.42633, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300
 - 44s - loss: 0.1009 - acc: 0.9894 - mDice: 0.8037 - val_loss: 0.2904 - val_acc: 0.9905 - val_mDice: 0.4243

Epoch 00005: val_mDice did not improve from 0.42633
Epoch 6/300
 - 43s - loss: 0.0958 - acc: 0.9900 - mDice: 0.8137 - val_loss: 0.2895 - val_acc: 0.9905 - val_mDice: 0.4261

Epoch 00006: val_mDice did not improve from 0.42633
Epoch 7/300
 - 44s - loss: 0.0901 - acc: 0.9905 - mDice: 0.8248 - val_loss: 0.2871 - val_acc: 0.9911 - val_mDice: 0.4303

Epoch 00007: val_mDice improved from 0.42633 to 0.43034, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/300
 - 43s - loss: 0.0883 - acc: 0.9908 - mDice: 0.8283 - val_loss: 0.2854 - val_acc: 0.9911 - val_mDice: 0.4335

Epoch 00008: val_mDice improved from 0.43034 to 0.43351, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 9/300
 - 44s - loss: 0.0862 - acc: 0.9910 - mDice: 0.8323 - val_loss: 0.2887 - val_acc: 0.9902 - val_mDice: 0.4272

Epoch 00009: val_mDice did not improve from 0.43351
Epoch 10/300
 - 43s - loss: 0.0841 - acc: 0.9913 - mDice: 0.8364 - val_loss: 0.2836 - val_acc: 0.9913 - val_mDice: 0.4370

Epoch 00010: val_mDice improved from 0.43351 to 0.43702, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 11/300
 - 44s - loss: 0.0810 - acc: 0.9915 - mDice: 0.8424 - val_loss: 0.2802 - val_acc: 0.9917 - val_mDice: 0.4422

Epoch 00011: val_mDice improved from 0.43702 to 0.44219, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 12/300
 - 43s - loss: 0.0790 - acc: 0.9917 - mDice: 0.8463 - val_loss: 0.2793 - val_acc: 0.9919 - val_mDice: 0.4439

Epoch 00012: val_mDice improved from 0.44219 to 0.44394, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 13/300
 - 44s - loss: 0.0791 - acc: 0.9918 - mDice: 0.8461 - val_loss: 0.2792 - val_acc: 0.9919 - val_mDice: 0.4448

Epoch 00013: val_mDice improved from 0.44394 to 0.44476, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 14/300
 - 44s - loss: 0.0772 - acc: 0.9919 - mDice: 0.8498 - val_loss: 0.2768 - val_acc: 0.9920 - val_mDice: 0.4467

Epoch 00014: val_mDice improved from 0.44476 to 0.44671, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 15/300
 - 44s - loss: 0.0765 - acc: 0.9920 - mDice: 0.8512 - val_loss: 0.2739 - val_acc: 0.9922 - val_mDice: 0.4468

Epoch 00015: val_mDice improved from 0.44671 to 0.44680, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 16/300
 - 44s - loss: 0.0756 - acc: 0.9922 - mDice: 0.8529 - val_loss: 0.2737 - val_acc: 0.9919 - val_mDice: 0.4460

Epoch 00016: val_mDice did not improve from 0.44680
Epoch 17/300
 - 43s - loss: 0.0728 - acc: 0.9923 - mDice: 0.8585 - val_loss: 0.2553 - val_acc: 0.9921 - val_mDice: 0.4476

Epoch 00017: val_mDice improved from 0.44680 to 0.44756, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 18/300
 - 44s - loss: 0.0716 - acc: 0.9924 - mDice: 0.8607 - val_loss: 0.2744 - val_acc: 0.9920 - val_mDice: 0.4481

Epoch 00018: val_mDice improved from 0.44756 to 0.44810, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 19/300
 - 43s - loss: 0.0712 - acc: 0.9925 - mDice: 0.8615 - val_loss: 0.2534 - val_acc: 0.9920 - val_mDice: 0.4506

Epoch 00019: val_mDice improved from 0.44810 to 0.45056, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 20/300
 - 44s - loss: 0.0710 - acc: 0.9925 - mDice: 0.8618 - val_loss: 0.2390 - val_acc: 0.9921 - val_mDice: 0.4500

Epoch 00020: val_mDice did not improve from 0.45056
Epoch 21/300
 - 44s - loss: 0.0705 - acc: 0.9926 - mDice: 0.8629 - val_loss: 0.2123 - val_acc: 0.9920 - val_mDice: 0.4533

Epoch 00021: val_mDice improved from 0.45056 to 0.45331, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 22/300
 - 44s - loss: 0.0698 - acc: 0.9927 - mDice: 0.8641 - val_loss: 0.2073 - val_acc: 0.9927 - val_mDice: 0.4541

Epoch 00022: val_mDice improved from 0.45331 to 0.45411, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 23/300
 - 44s - loss: 0.0690 - acc: 0.9928 - mDice: 0.8658 - val_loss: 0.1962 - val_acc: 0.9922 - val_mDice: 0.4537

Epoch 00023: val_mDice did not improve from 0.45411
Epoch 24/300
 - 44s - loss: 0.0700 - acc: 0.9928 - mDice: 0.8637 - val_loss: 0.1761 - val_acc: 0.9916 - val_mDice: 0.4470

Epoch 00024: val_mDice did not improve from 0.45411
Epoch 25/300
 - 44s - loss: 0.0671 - acc: 0.9929 - mDice: 0.8695 - val_loss: 0.1303 - val_acc: 0.9926 - val_mDice: 0.4542

Epoch 00025: val_mDice improved from 0.45411 to 0.45422, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 26/300
 - 44s - loss: 0.0676 - acc: 0.9929 - mDice: 0.8685 - val_loss: 0.1611 - val_acc: 0.9922 - val_mDice: 0.4549

Epoch 00026: val_mDice improved from 0.45422 to 0.45489, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 27/300
 - 44s - loss: 0.0664 - acc: 0.9930 - mDice: 0.8708 - val_loss: 0.1569 - val_acc: 0.9924 - val_mDice: 0.4563

Epoch 00027: val_mDice improved from 0.45489 to 0.45630, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 28/300
 - 43s - loss: 0.0668 - acc: 0.9930 - mDice: 0.8700 - val_loss: 0.1177 - val_acc: 0.9927 - val_mDice: 0.4579

Epoch 00028: val_mDice improved from 0.45630 to 0.45786, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 29/300
 - 44s - loss: 0.0655 - acc: 0.9931 - mDice: 0.8725 - val_loss: 0.1113 - val_acc: 0.9929 - val_mDice: 0.4584

Epoch 00029: val_mDice improved from 0.45786 to 0.45843, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 30/300
 - 44s - loss: 0.0644 - acc: 0.9931 - mDice: 0.8747 - val_loss: 0.1241 - val_acc: 0.9928 - val_mDice: 0.4569

Epoch 00030: val_mDice did not improve from 0.45843
Epoch 31/300
 - 44s - loss: 0.0641 - acc: 0.9932 - mDice: 0.8753 - val_loss: 0.1137 - val_acc: 0.9928 - val_mDice: 0.4597

Epoch 00031: val_mDice improved from 0.45843 to 0.45975, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 32/300
 - 44s - loss: 0.0635 - acc: 0.9932 - mDice: 0.8765 - val_loss: 0.0952 - val_acc: 0.9928 - val_mDice: 0.4615

Epoch 00032: val_mDice improved from 0.45975 to 0.46150, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 33/300
 - 43s - loss: 0.0633 - acc: 0.9933 - mDice: 0.8769 - val_loss: 0.0231 - val_acc: 0.9927 - val_mDice: 0.4555

Epoch 00033: val_mDice did not improve from 0.46150
Epoch 34/300
 - 43s - loss: 0.0634 - acc: 0.9933 - mDice: 0.8767 - val_loss: 0.1320 - val_acc: 0.9928 - val_mDice: 0.4629

Epoch 00034: val_mDice improved from 0.46150 to 0.46288, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 35/300
 - 44s - loss: 0.0621 - acc: 0.9934 - mDice: 0.8792 - val_loss: 0.0550 - val_acc: 0.9927 - val_mDice: 0.4610

Epoch 00035: val_mDice did not improve from 0.46288
Epoch 36/300
 - 44s - loss: 0.0625 - acc: 0.9934 - mDice: 0.8784 - val_loss: 0.0903 - val_acc: 0.9929 - val_mDice: 0.4616

Epoch 00036: val_mDice did not improve from 0.46288
Epoch 37/300
 - 44s - loss: 0.0630 - acc: 0.9934 - mDice: 0.8775 - val_loss: 0.0949 - val_acc: 0.9928 - val_mDice: 0.4649

Epoch 00037: val_mDice improved from 0.46288 to 0.46488, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 38/300
 - 44s - loss: 0.0617 - acc: 0.9935 - mDice: 0.8800 - val_loss: 0.0953 - val_acc: 0.9929 - val_mDice: 0.4644

Epoch 00038: val_mDice did not improve from 0.46488
Epoch 39/300
 - 43s - loss: 0.0616 - acc: 0.9935 - mDice: 0.8802 - val_loss: 0.1068 - val_acc: 0.9928 - val_mDice: 0.4655

Epoch 00039: val_mDice improved from 0.46488 to 0.46545, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 40/300
 - 44s - loss: 0.0602 - acc: 0.9935 - mDice: 0.8828 - val_loss: 0.0895 - val_acc: 0.9929 - val_mDice: 0.4668

Epoch 00040: val_mDice improved from 0.46545 to 0.46675, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 41/300
 - 43s - loss: 0.0609 - acc: 0.9936 - mDice: 0.8815 - val_loss: 0.0930 - val_acc: 0.9930 - val_mDice: 0.4650

Epoch 00041: val_mDice did not improve from 0.46675
Epoch 42/300
 - 44s - loss: 0.0602 - acc: 0.9936 - mDice: 0.8830 - val_loss: 0.0901 - val_acc: 0.9926 - val_mDice: 0.4647

Epoch 00042: val_mDice did not improve from 0.46675
Epoch 43/300
 - 43s - loss: 0.0601 - acc: 0.9936 - mDice: 0.8831 - val_loss: 0.0874 - val_acc: 0.9929 - val_mDice: 0.4646

Epoch 00043: val_mDice did not improve from 0.46675
Epoch 44/300
 - 44s - loss: 0.0598 - acc: 0.9936 - mDice: 0.8836 - val_loss: 0.0844 - val_acc: 0.9930 - val_mDice: 0.4671

Epoch 00044: val_mDice improved from 0.46675 to 0.46707, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 45/300
 - 44s - loss: 0.0596 - acc: 0.9937 - mDice: 0.8840 - val_loss: 0.0904 - val_acc: 0.9927 - val_mDice: 0.4637

Epoch 00045: val_mDice did not improve from 0.46707
Epoch 46/300
 - 44s - loss: 0.0587 - acc: 0.9937 - mDice: 0.8857 - val_loss: 0.0893 - val_acc: 0.9931 - val_mDice: 0.4659

Epoch 00046: val_mDice did not improve from 0.46707
Epoch 47/300
 - 44s - loss: 0.0576 - acc: 0.9937 - mDice: 0.8881 - val_loss: 0.0498 - val_acc: 0.9931 - val_mDice: 0.4667

Epoch 00047: val_mDice did not improve from 0.46707
Epoch 48/300
 - 44s - loss: 0.0583 - acc: 0.9938 - mDice: 0.8865 - val_loss: 0.0502 - val_acc: 0.9931 - val_mDice: 0.4659

Epoch 00048: val_mDice did not improve from 0.46707
Epoch 49/300
 - 44s - loss: 0.0582 - acc: 0.9938 - mDice: 0.8869 - val_loss: 0.0109 - val_acc: 0.9930 - val_mDice: 0.4665

Epoch 00049: val_mDice did not improve from 0.46707
Epoch 50/300
 - 44s - loss: 0.0579 - acc: 0.9938 - mDice: 0.8874 - val_loss: 0.0894 - val_acc: 0.9929 - val_mDice: 0.4656

Epoch 00050: val_mDice did not improve from 0.46707
Epoch 51/300
 - 44s - loss: 0.0581 - acc: 0.9938 - mDice: 0.8869 - val_loss: 0.0880 - val_acc: 0.9929 - val_mDice: 0.4639

Epoch 00051: val_mDice did not improve from 0.46707
Epoch 52/300
 - 44s - loss: 0.0581 - acc: 0.9938 - mDice: 0.8870 - val_loss: 0.0110 - val_acc: 0.9931 - val_mDice: 0.4663

Epoch 00052: val_mDice did not improve from 0.46707
Epoch 53/300
 - 45s - loss: 0.0580 - acc: 0.9939 - mDice: 0.8872 - val_loss: 0.0109 - val_acc: 0.9930 - val_mDice: 0.4665

Epoch 00053: val_mDice did not improve from 0.46707
Epoch 54/300
 - 47s - loss: 0.0569 - acc: 0.9939 - mDice: 0.8894 - val_loss: 0.0112 - val_acc: 0.9932 - val_mDice: 0.4656

Epoch 00054: val_mDice did not improve from 0.46707
Epoch 55/300
 - 47s - loss: 0.0561 - acc: 0.9939 - mDice: 0.8908 - val_loss: 0.0105 - val_acc: 0.9930 - val_mDice: 0.4672

Epoch 00055: val_mDice improved from 0.46707 to 0.46718, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5

Epoch 00055: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 56/300
 - 46s - loss: 0.0560 - acc: 0.9940 - mDice: 0.8910 - val_loss: 0.0895 - val_acc: 0.9930 - val_mDice: 0.4670

Epoch 00056: val_mDice did not improve from 0.46718
Epoch 57/300
 - 44s - loss: 0.0554 - acc: 0.9940 - mDice: 0.8922 - val_loss: 0.1275 - val_acc: 0.9930 - val_mDice: 0.4674

Epoch 00057: val_mDice improved from 0.46718 to 0.46743, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 58/300
 - 45s - loss: 0.0563 - acc: 0.9940 - mDice: 0.8904 - val_loss: 0.0880 - val_acc: 0.9931 - val_mDice: 0.4682

Epoch 00058: val_mDice improved from 0.46743 to 0.46820, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 59/300
 - 44s - loss: 0.0568 - acc: 0.9940 - mDice: 0.8895 - val_loss: 0.0090 - val_acc: 0.9931 - val_mDice: 0.4702

Epoch 00059: val_mDice improved from 0.46820 to 0.47020, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 60/300
 - 44s - loss: 0.0558 - acc: 0.9940 - mDice: 0.8915 - val_loss: 0.0504 - val_acc: 0.9929 - val_mDice: 0.4686

Epoch 00060: val_mDice did not improve from 0.47020
Epoch 61/300
 - 44s - loss: 0.0548 - acc: 0.9940 - mDice: 0.8934 - val_loss: 0.0857 - val_acc: 0.9931 - val_mDice: 0.4710

Epoch 00061: val_mDice improved from 0.47020 to 0.47098, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 62/300
 - 44s - loss: 0.0550 - acc: 0.9940 - mDice: 0.8930 - val_loss: 0.0083 - val_acc: 0.9933 - val_mDice: 0.4715

Epoch 00062: val_mDice improved from 0.47098 to 0.47152, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 63/300
 - 44s - loss: 0.0561 - acc: 0.9940 - mDice: 0.8909 - val_loss: 0.0461 - val_acc: 0.9933 - val_mDice: 0.4711

Epoch 00063: val_mDice did not improve from 0.47152
Epoch 64/300
 - 45s - loss: 0.0546 - acc: 0.9941 - mDice: 0.8939 - val_loss: 0.0849 - val_acc: 0.9931 - val_mDice: 0.4745

Epoch 00064: val_mDice improved from 0.47152 to 0.47455, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 65/300
 - 45s - loss: 0.0558 - acc: 0.9941 - mDice: 0.8915 - val_loss: 0.0074 - val_acc: 0.9933 - val_mDice: 0.4733

Epoch 00065: val_mDice did not improve from 0.47455
Epoch 66/300
 - 46s - loss: 0.0535 - acc: 0.9941 - mDice: 0.8960 - val_loss: 0.0083 - val_acc: 0.9933 - val_mDice: 0.4714

Epoch 00066: val_mDice did not improve from 0.47455
Epoch 67/300
 - 46s - loss: 0.0554 - acc: 0.9941 - mDice: 0.8923 - val_loss: 0.0036 - val_acc: 0.9932 - val_mDice: 0.4711

Epoch 00067: val_mDice did not improve from 0.47455
Epoch 68/300
 - 46s - loss: 0.0539 - acc: 0.9941 - mDice: 0.8952 - val_loss: 0.0072 - val_acc: 0.9932 - val_mDice: 0.4738

Epoch 00068: val_mDice did not improve from 0.47455
Epoch 69/300
 - 46s - loss: 0.0537 - acc: 0.9941 - mDice: 0.8956 - val_loss: 0.0149 - val_acc: 0.9932 - val_mDice: 0.4735

Epoch 00069: val_mDice did not improve from 0.47455
Epoch 70/300
 - 46s - loss: 0.0533 - acc: 0.9941 - mDice: 0.8963 - val_loss: 0.0235 - val_acc: 0.9931 - val_mDice: 0.4725

Epoch 00070: val_mDice did not improve from 0.47455
Epoch 71/300
 - 46s - loss: 0.0537 - acc: 0.9942 - mDice: 0.8955 - val_loss: 0.0970 - val_acc: 0.9933 - val_mDice: 0.4740

Epoch 00071: val_mDice did not improve from 0.47455
Epoch 72/300
 - 46s - loss: 0.0561 - acc: 0.9941 - mDice: 0.8909 - val_loss: 0.0327 - val_acc: 0.9933 - val_mDice: 0.4748

Epoch 00072: val_mDice improved from 0.47455 to 0.47483, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 73/300
 - 46s - loss: 0.0531 - acc: 0.9942 - mDice: 0.8967 - val_loss: 0.0842 - val_acc: 0.9932 - val_mDice: 0.4744

Epoch 00073: val_mDice did not improve from 0.47483
Epoch 74/300
 - 45s - loss: 0.0538 - acc: 0.9942 - mDice: 0.8954 - val_loss: 0.0988 - val_acc: 0.9930 - val_mDice: 0.4728

Epoch 00074: val_mDice did not improve from 0.47483
Epoch 75/300
 - 44s - loss: 0.0541 - acc: 0.9942 - mDice: 0.8948 - val_loss: 0.0463 - val_acc: 0.9933 - val_mDice: 0.4732

Epoch 00075: val_mDice did not improve from 0.47483
Epoch 76/300
 - 44s - loss: 0.0540 - acc: 0.9942 - mDice: 0.8949 - val_loss: 0.0462 - val_acc: 0.9933 - val_mDice: 0.4736

Epoch 00076: val_mDice did not improve from 0.47483
Epoch 77/300
 - 44s - loss: 0.0541 - acc: 0.9942 - mDice: 0.8947 - val_loss: 0.0133 - val_acc: 0.9933 - val_mDice: 0.4748

Epoch 00077: val_mDice did not improve from 0.47483
Epoch 78/300
 - 44s - loss: 0.0525 - acc: 0.9942 - mDice: 0.8981 - val_loss: 0.0519 - val_acc: 0.9931 - val_mDice: 0.4758

Epoch 00078: val_mDice improved from 0.47483 to 0.47583, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 79/300
 - 44s - loss: 0.0529 - acc: 0.9942 - mDice: 0.8972 - val_loss: 0.0521 - val_acc: 0.9933 - val_mDice: 0.4772

Epoch 00079: val_mDice improved from 0.47583 to 0.47722, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 80/300
 - 44s - loss: 0.0536 - acc: 0.9942 - mDice: 0.8958 - val_loss: 0.0053 - val_acc: 0.9933 - val_mDice: 0.4774

Epoch 00080: val_mDice improved from 0.47722 to 0.47745, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 81/300
 - 44s - loss: 0.0534 - acc: 0.9942 - mDice: 0.8961 - val_loss: 0.1096 - val_acc: 0.9931 - val_mDice: 0.4823

Epoch 00081: val_mDice improved from 0.47745 to 0.48226, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 82/300
 - 44s - loss: 0.0529 - acc: 0.9943 - mDice: 0.8971 - val_loss: 0.0045 - val_acc: 0.9932 - val_mDice: 0.4790

Epoch 00082: val_mDice did not improve from 0.48226
Epoch 83/300
 - 44s - loss: 0.0518 - acc: 0.9943 - mDice: 0.8993 - val_loss: 0.0438 - val_acc: 0.9930 - val_mDice: 0.4788

Epoch 00083: val_mDice did not improve from 0.48226
Epoch 84/300
 - 45s - loss: 0.0525 - acc: 0.9943 - mDice: 0.8979 - val_loss: 0.1220 - val_acc: 0.9930 - val_mDice: 0.4772

Epoch 00084: val_mDice did not improve from 0.48226
Epoch 85/300
 - 45s - loss: 0.0523 - acc: 0.9943 - mDice: 0.8982 - val_loss: 0.0548 - val_acc: 0.9930 - val_mDice: 0.4777

Epoch 00085: val_mDice did not improve from 0.48226
Epoch 86/300
 - 46s - loss: 0.0522 - acc: 0.9943 - mDice: 0.8985 - val_loss: 0.0420 - val_acc: 0.9933 - val_mDice: 0.4784

Epoch 00086: val_mDice did not improve from 0.48226
Epoch 87/300
 - 45s - loss: 0.0527 - acc: 0.9943 - mDice: 0.8976 - val_loss: 0.0424 - val_acc: 0.9933 - val_mDice: 0.4792

Epoch 00087: val_mDice did not improve from 0.48226
Epoch 88/300
 - 46s - loss: 0.0524 - acc: 0.9943 - mDice: 0.8980 - val_loss: 0.0046 - val_acc: 0.9932 - val_mDice: 0.4789

Epoch 00088: val_mDice did not improve from 0.48226
Epoch 89/300
 - 46s - loss: 0.0525 - acc: 0.9943 - mDice: 0.8980 - val_loss: 0.0054 - val_acc: 0.9933 - val_mDice: 0.4772

Epoch 00089: val_mDice did not improve from 0.48226
Epoch 90/300
 - 46s - loss: 0.0520 - acc: 0.9944 - mDice: 0.8990 - val_loss: 0.0026 - val_acc: 0.9931 - val_mDice: 0.4830

Epoch 00090: val_mDice improved from 0.48226 to 0.48303, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 91/300
 - 44s - loss: 0.0515 - acc: 0.9943 - mDice: 0.8998 - val_loss: 0.0038 - val_acc: 0.9932 - val_mDice: 0.4804

Epoch 00091: val_mDice did not improve from 0.48303
Epoch 92/300
 - 44s - loss: 0.0533 - acc: 0.9943 - mDice: 0.8963 - val_loss: 0.0021 - val_acc: 0.9933 - val_mDice: 0.4837

Epoch 00092: val_mDice improved from 0.48303 to 0.48374, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 93/300
 - 44s - loss: 0.0509 - acc: 0.9944 - mDice: 0.9012 - val_loss: 0.0046 - val_acc: 0.9933 - val_mDice: 0.4788

Epoch 00093: val_mDice did not improve from 0.48374
Epoch 94/300
 - 44s - loss: 0.0518 - acc: 0.9943 - mDice: 0.8992 - val_loss: 0.0036 - val_acc: 0.9934 - val_mDice: 0.4806

Epoch 00094: val_mDice did not improve from 0.48374
Epoch 95/300
 - 44s - loss: 0.0514 - acc: 0.9944 - mDice: 0.9000 - val_loss: 9.6471e-04 - val_acc: 0.9933 - val_mDice: 0.4861

Epoch 00095: val_mDice improved from 0.48374 to 0.48609, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 96/300
 - 44s - loss: 0.0515 - acc: 0.9944 - mDice: 0.8999 - val_loss: 0.0026 - val_acc: 0.9934 - val_mDice: 0.4827

Epoch 00096: val_mDice did not improve from 0.48609
Epoch 97/300
 - 44s - loss: 0.0511 - acc: 0.9944 - mDice: 0.9007 - val_loss: 0.0029 - val_acc: 0.9934 - val_mDice: 0.4821

Epoch 00097: val_mDice did not improve from 0.48609
Epoch 98/300
 - 43s - loss: 0.0516 - acc: 0.9944 - mDice: 0.8997 - val_loss: 0.0029 - val_acc: 0.9934 - val_mDice: 0.4822

Epoch 00098: val_mDice did not improve from 0.48609
Epoch 99/300
 - 45s - loss: 0.0509 - acc: 0.9944 - mDice: 0.9011 - val_loss: -3.5158e-02 - val_acc: 0.9933 - val_mDice: 0.4802

Epoch 00099: val_mDice did not improve from 0.48609
Epoch 100/300
 - 45s - loss: 0.0515 - acc: 0.9944 - mDice: 0.8999 - val_loss: 0.0039 - val_acc: 0.9934 - val_mDice: 0.4803

Epoch 00100: val_mDice did not improve from 0.48609
Epoch 101/300
 - 45s - loss: 0.0516 - acc: 0.9944 - mDice: 0.8997 - val_loss: 0.0025 - val_acc: 0.9933 - val_mDice: 0.4830

Epoch 00101: val_mDice did not improve from 0.48609
Epoch 102/300
 - 45s - loss: 0.0520 - acc: 0.9944 - mDice: 0.8988 - val_loss: 0.0442 - val_acc: 0.9929 - val_mDice: 0.4780

Epoch 00102: val_mDice did not improve from 0.48609
Epoch 103/300
 - 45s - loss: 0.0513 - acc: 0.9944 - mDice: 0.9002 - val_loss: 0.0028 - val_acc: 0.9934 - val_mDice: 0.4825

Epoch 00103: val_mDice did not improve from 0.48609
Epoch 104/300
 - 46s - loss: 0.0509 - acc: 0.9944 - mDice: 0.9011 - val_loss: 0.0023 - val_acc: 0.9933 - val_mDice: 0.4834

Epoch 00104: val_mDice did not improve from 0.48609
Epoch 105/300
 - 46s - loss: 0.0503 - acc: 0.9944 - mDice: 0.9022 - val_loss: 0.0031 - val_acc: 0.9933 - val_mDice: 0.4819

Epoch 00105: val_mDice did not improve from 0.48609
Epoch 106/300
 - 46s - loss: 0.0508 - acc: 0.9944 - mDice: 0.9012 - val_loss: 0.0034 - val_acc: 0.9934 - val_mDice: 0.4812

Epoch 00106: val_mDice did not improve from 0.48609
Epoch 107/300
 - 46s - loss: 0.0509 - acc: 0.9944 - mDice: 0.9010 - val_loss: 0.0035 - val_acc: 0.9933 - val_mDice: 0.4810

Epoch 00107: val_mDice did not improve from 0.48609
Epoch 108/300
 - 45s - loss: 0.0494 - acc: 0.9945 - mDice: 0.9040 - val_loss: 0.0022 - val_acc: 0.9934 - val_mDice: 0.4836

Epoch 00108: val_mDice did not improve from 0.48609
Epoch 109/300
 - 44s - loss: 0.0514 - acc: 0.9945 - mDice: 0.9001 - val_loss: 1.1033e-04 - val_acc: 0.9933 - val_mDice: 0.4875

Epoch 00109: val_mDice improved from 0.48609 to 0.48751, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 110/300
 - 44s - loss: 0.0508 - acc: 0.9945 - mDice: 0.9012 - val_loss: 4.6300e-04 - val_acc: 0.9934 - val_mDice: 0.4870

Epoch 00110: val_mDice did not improve from 0.48751
Epoch 111/300
 - 44s - loss: 0.0497 - acc: 0.9945 - mDice: 0.9033 - val_loss: 0.0386 - val_acc: 0.9935 - val_mDice: 0.4889

Epoch 00111: val_mDice improved from 0.48751 to 0.48890, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 112/300
 - 44s - loss: 0.0511 - acc: 0.9945 - mDice: 0.9005 - val_loss: 0.0018 - val_acc: 0.9933 - val_mDice: 0.4844

Epoch 00112: val_mDice did not improve from 0.48890
Epoch 113/300
 - 45s - loss: 0.0494 - acc: 0.9945 - mDice: 0.9041 - val_loss: -7.8558e-04 - val_acc: 0.9934 - val_mDice: 0.4897

Epoch 00113: val_mDice improved from 0.48890 to 0.48971, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 114/300
 - 44s - loss: 0.0495 - acc: 0.9945 - mDice: 0.9037 - val_loss: -6.8222e-04 - val_acc: 0.9933 - val_mDice: 0.4894

Epoch 00114: val_mDice did not improve from 0.48971
Epoch 115/300
 - 44s - loss: 0.0498 - acc: 0.9945 - mDice: 0.9032 - val_loss: 0.0018 - val_acc: 0.9935 - val_mDice: 0.4844

Epoch 00115: val_mDice did not improve from 0.48971
Epoch 116/300
 - 44s - loss: 0.0495 - acc: 0.9945 - mDice: 0.9039 - val_loss: 0.0014 - val_acc: 0.9934 - val_mDice: 0.4853

Epoch 00116: val_mDice did not improve from 0.48971
Epoch 117/300
 - 43s - loss: 0.0500 - acc: 0.9945 - mDice: 0.9028 - val_loss: 0.0017 - val_acc: 0.9935 - val_mDice: 0.4846

Epoch 00117: val_mDice did not improve from 0.48971
Epoch 118/300
 - 44s - loss: 0.0506 - acc: 0.9945 - mDice: 0.9016 - val_loss: 9.4208e-04 - val_acc: 0.9934 - val_mDice: 0.4861

Epoch 00118: val_mDice did not improve from 0.48971
Epoch 119/300
 - 43s - loss: 0.0494 - acc: 0.9945 - mDice: 0.9041 - val_loss: 0.0011 - val_acc: 0.9934 - val_mDice: 0.4857

Epoch 00119: val_mDice did not improve from 0.48971
Epoch 120/300
 - 45s - loss: 0.0489 - acc: 0.9946 - mDice: 0.9050 - val_loss: 0.0029 - val_acc: 0.9934 - val_mDice: 0.4822

Epoch 00120: val_mDice did not improve from 0.48971
Epoch 121/300
 - 44s - loss: 0.0498 - acc: 0.9946 - mDice: 0.9031 - val_loss: 0.0012 - val_acc: 0.9933 - val_mDice: 0.4856

Epoch 00121: val_mDice did not improve from 0.48971
Epoch 122/300
 - 44s - loss: 0.0500 - acc: 0.9946 - mDice: 0.9027 - val_loss: 0.0014 - val_acc: 0.9934 - val_mDice: 0.4851

Epoch 00122: val_mDice did not improve from 0.48971
Epoch 123/300
 - 44s - loss: 0.0492 - acc: 0.9945 - mDice: 0.9044 - val_loss: 0.0023 - val_acc: 0.9934 - val_mDice: 0.4833

Epoch 00123: val_mDice did not improve from 0.48971
Epoch 124/300
 - 45s - loss: 0.0494 - acc: 0.9946 - mDice: 0.9040 - val_loss: 0.0014 - val_acc: 0.9934 - val_mDice: 0.4852

Epoch 00124: val_mDice did not improve from 0.48971
Epoch 125/300
 - 44s - loss: 0.0495 - acc: 0.9946 - mDice: 0.9038 - val_loss: 9.5784e-04 - val_acc: 0.9933 - val_mDice: 0.4861

Epoch 00125: val_mDice did not improve from 0.48971
Epoch 126/300
 - 45s - loss: 0.0483 - acc: 0.9946 - mDice: 0.9061 - val_loss: 0.0027 - val_acc: 0.9935 - val_mDice: 0.4825

Epoch 00126: val_mDice did not improve from 0.48971

Epoch 00126: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 127/300
 - 46s - loss: 0.0490 - acc: 0.9946 - mDice: 0.9048 - val_loss: 8.3209e-04 - val_acc: 0.9934 - val_mDice: 0.4863

Epoch 00127: val_mDice did not improve from 0.48971
Epoch 128/300
 - 45s - loss: 0.0486 - acc: 0.9946 - mDice: 0.9056 - val_loss: 0.0027 - val_acc: 0.9934 - val_mDice: 0.4827

Epoch 00128: val_mDice did not improve from 0.48971
Epoch 129/300
 - 44s - loss: 0.0494 - acc: 0.9946 - mDice: 0.9040 - val_loss: 0.0022 - val_acc: 0.9934 - val_mDice: 0.4835

Epoch 00129: val_mDice did not improve from 0.48971
Epoch 130/300
 - 44s - loss: 0.0483 - acc: 0.9946 - mDice: 0.9062 - val_loss: 0.0019 - val_acc: 0.9934 - val_mDice: 0.4843

Epoch 00130: val_mDice did not improve from 0.48971
Epoch 131/300
 - 44s - loss: 0.0493 - acc: 0.9946 - mDice: 0.9041 - val_loss: 0.0015 - val_acc: 0.9933 - val_mDice: 0.4850

Epoch 00131: val_mDice did not improve from 0.48971
Epoch 132/300
 - 44s - loss: 0.0490 - acc: 0.9946 - mDice: 0.9047 - val_loss: 7.1002e-04 - val_acc: 0.9934 - val_mDice: 0.4866

Epoch 00132: val_mDice did not improve from 0.48971
Epoch 133/300
 - 44s - loss: 0.0490 - acc: 0.9946 - mDice: 0.9046 - val_loss: 7.3673e-04 - val_acc: 0.9933 - val_mDice: 0.4866

Epoch 00133: val_mDice did not improve from 0.48971
Epoch 134/300
 - 44s - loss: 0.0495 - acc: 0.9946 - mDice: 0.9037 - val_loss: 0.0011 - val_acc: 0.9934 - val_mDice: 0.4859

Epoch 00134: val_mDice did not improve from 0.48971
Epoch 135/300
 - 44s - loss: 0.0490 - acc: 0.9947 - mDice: 0.9048 - val_loss: 0.0014 - val_acc: 0.9934 - val_mDice: 0.4852

Epoch 00135: val_mDice did not improve from 0.48971
Epoch 136/300
 - 44s - loss: 0.0491 - acc: 0.9946 - mDice: 0.9045 - val_loss: 0.0017 - val_acc: 0.9934 - val_mDice: 0.4846

Epoch 00136: val_mDice did not improve from 0.48971
Epoch 137/300
 - 44s - loss: 0.0488 - acc: 0.9946 - mDice: 0.9051 - val_loss: 0.0015 - val_acc: 0.9933 - val_mDice: 0.4851

Epoch 00137: val_mDice did not improve from 0.48971
Epoch 138/300
 - 45s - loss: 0.0478 - acc: 0.9947 - mDice: 0.9071 - val_loss: 0.0029 - val_acc: 0.9934 - val_mDice: 0.4822

Epoch 00138: val_mDice did not improve from 0.48971
Epoch 139/300
 - 45s - loss: 0.0479 - acc: 0.9947 - mDice: 0.9068 - val_loss: 0.0018 - val_acc: 0.9934 - val_mDice: 0.4844

Epoch 00139: val_mDice did not improve from 0.48971
Epoch 140/300
 - 45s - loss: 0.0491 - acc: 0.9946 - mDice: 0.9046 - val_loss: 0.0019 - val_acc: 0.9934 - val_mDice: 0.4842

Epoch 00140: val_mDice did not improve from 0.48971
Epoch 141/300
 - 45s - loss: 0.0489 - acc: 0.9946 - mDice: 0.9050 - val_loss: 0.0017 - val_acc: 0.9934 - val_mDice: 0.4846

Epoch 00141: val_mDice did not improve from 0.48971

Epoch 00141: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 142/300
 - 45s - loss: 0.0487 - acc: 0.9947 - mDice: 0.9053 - val_loss: 0.0017 - val_acc: 0.9934 - val_mDice: 0.4847

Epoch 00142: val_mDice did not improve from 0.48971
Epoch 143/300
 - 45s - loss: 0.0480 - acc: 0.9947 - mDice: 0.9067 - val_loss: 0.0011 - val_acc: 0.9934 - val_mDice: 0.4858

Epoch 00143: val_mDice did not improve from 0.48971
Epoch 144/300
 - 45s - loss: 0.0481 - acc: 0.9946 - mDice: 0.9065 - val_loss: 0.0018 - val_acc: 0.9934 - val_mDice: 0.4844

Epoch 00144: val_mDice did not improve from 0.48971
Epoch 145/300
 - 45s - loss: 0.0488 - acc: 0.9946 - mDice: 0.9050 - val_loss: 4.2987e-04 - val_acc: 0.9934 - val_mDice: 0.4872

Epoch 00145: val_mDice did not improve from 0.48971
Epoch 146/300
 - 44s - loss: 0.0487 - acc: 0.9946 - mDice: 0.9053 - val_loss: 0.0022 - val_acc: 0.9934 - val_mDice: 0.4836

Epoch 00146: val_mDice did not improve from 0.48971
Epoch 147/300
 - 44s - loss: 0.0482 - acc: 0.9947 - mDice: 0.9063 - val_loss: 0.0012 - val_acc: 0.9934 - val_mDice: 0.4856

Epoch 00147: val_mDice did not improve from 0.48971
Epoch 148/300
 - 44s - loss: 0.0488 - acc: 0.9947 - mDice: 0.9052 - val_loss: 0.0022 - val_acc: 0.9934 - val_mDice: 0.4836

Epoch 00148: val_mDice did not improve from 0.48971
Epoch 149/300
 - 45s - loss: 0.0491 - acc: 0.9947 - mDice: 0.9045 - val_loss: 0.0017 - val_acc: 0.9934 - val_mDice: 0.4846

Epoch 00149: val_mDice did not improve from 0.48971
Epoch 150/300
 - 45s - loss: 0.0493 - acc: 0.9947 - mDice: 0.9042 - val_loss: 0.0152 - val_acc: 0.9934 - val_mDice: 0.4841

Epoch 00150: val_mDice did not improve from 0.48971
Epoch 151/300
 - 44s - loss: 0.0477 - acc: 0.9947 - mDice: 0.9074 - val_loss: 0.0403 - val_acc: 0.9933 - val_mDice: 0.4849

Epoch 00151: val_mDice did not improve from 0.48971
Epoch 152/300
 - 44s - loss: 0.0491 - acc: 0.9946 - mDice: 0.9044 - val_loss: 0.0013 - val_acc: 0.9934 - val_mDice: 0.4855

Epoch 00152: val_mDice did not improve from 0.48971
Epoch 153/300
 - 44s - loss: 0.0492 - acc: 0.9947 - mDice: 0.9044 - val_loss: 0.0013 - val_acc: 0.9934 - val_mDice: 0.4855

Epoch 00153: val_mDice did not improve from 0.48971
Restoring model weights from the end of the best epoch
Epoch 00153: early stopping
{'val_loss': [0.3122281940304674, 0.3002903612796217, 0.2954890506225638, 0.2894308196846396, 0.29040077555691823, 0.28946111915865913, 0.2871232107281685, 0.2854110049083829, 0.28874077077489346, 0.2835912572336383, 0.28022981103276834, 0.2793332489673048, 0.2791947324294597, 0.27682019327767193, 0.2738818640355021, 0.27367268537636846, 0.25530200777575374, 0.2743696335237473, 0.25340166315436363, 0.23901984514668584, 0.21225152211263776, 0.20733501808717847, 0.1962266645859927, 0.1760940831154585, 0.13032119465060532, 0.16114182525780052, 0.15688887145370245, 0.11769019742496312, 0.11127138533629477, 0.12405029020737857, 0.1137333819642663, 0.09516696317587048, 0.02312094735680148, 0.13199552218429744, 0.05501725501380861, 0.0902618319960311, 0.09489557787310332, 0.09527314698789269, 0.10678319283761084, 0.0894975746050477, 0.09298635472077876, 0.09006458590738475, 0.08742086100392044, 0.08442340418696404, 0.0904431149829179, 0.08926181378774345, 0.04977502999827266, 0.05015057744458318, 0.010854870080947876, 0.08942429348826408, 0.08799505443312228, 0.010957713006064296, 0.010850887862034142, 0.011231780052185059, 0.010480154072865844, 0.08950233366340399, 0.12752567324787378, 0.08797428419347852, 0.008952816016972065, 0.05042576161213219, 0.08572098123840988, 0.008259803522378206, 0.04608934326097369, 0.08492909069173038, 0.0073861111886799335, 0.00828681536950171, 0.003609345876611769, 0.007156748557463288, 0.0149041754193604, 0.023503717267885804, 0.09703238867223263, 0.032666757702827454, 0.08417165512219071, 0.09880488715134561, 0.046328134718351066, 0.04615649953484535, 0.013285437016747892, 0.05190312536433339, 0.05213756486773491, 0.005296149989590049, 0.1095705593470484, 0.004500249866396189, 0.04378358507528901, 0.12200446776114404, 0.05482288566417992, 0.042023033602163196, 0.04244075750466436, 0.004638827755115926, 0.005423107766546309, 0.0025843670591712, 0.003843417391180992, 0.0021376616787165403, 0.004596548154950142, 0.003644350916147232, 0.0009647118858993053, 0.0025857307482510805, 0.002924507833085954, 0.0028880289755761623, -0.03515829297248274, 0.003857617499306798, 0.002523423172533512, 0.04419810092076659, 0.0027691503055393696, 0.0023112085182219744, 0.003059544018469751, 0.0033767432905733585, 0.003515073040034622, 0.002197147812694311, 0.00011032796464860439, 0.000463003758341074, 0.038595706690102816, 0.0018024866003543139, -0.0007855822332203388, -0.0006822230061516166, 0.0017787603428587317, 0.0013539899373427033, 0.0016969991847872734, 0.000942084938287735, 0.0011030297027900815, 0.002909383736550808, 0.0012029773788526654, 0.0014337938046082854, 0.0023371640127152205, 0.0013827993534505367, 0.000957840122282505, 0.002741591539233923, 0.0008320873603224754, 0.002661418402567506, 0.0022451758850365877, 0.0018584085046313703, 0.0015090782544575632, 0.0007100158254615963, 0.0007367264479398727, 0.001076956046745181, 0.0014074693899601698, 0.0017332581337541342, 0.0015103330370038748, 0.0029248374048620462, 0.0018478736747056246, 0.0019200531532987952, 0.0017145306337624788, 0.0016564082470722497, 0.001113277394324541, 0.001807906141038984, 0.00042987120104953647, 0.002178418915718794, 0.0012088550720363855, 0.002193972934037447, 0.0017049115267582238, 0.015153622254729271, 0.040311733493581414, 0.00126151111908257, 0.0013489548582583666], 'val_acc': [0.9867864758707583, 0.9885768345557153, 0.989468902349472, 0.9900887627154589, 0.9904785170219839, 0.9905003407038748, 0.9910840368829668, 0.9910902730189264, 0.9901545564644039, 0.9912611348554492, 0.9917297763749957, 0.9919106182642281, 0.9918950302526355, 0.9920294177718461, 0.9922199314460158, 0.9919471028260887, 0.992111106403172, 0.9919517710804939, 0.9919717386364937, 0.9921011319383979, 0.9919745442457497, 0.9927306571044028, 0.9922130699269474, 0.9915975681506097, 0.9926252639852464, 0.992228343617171, 0.9923783214762807, 0.9926670547574759, 0.9928825069218874, 0.9928011279553175, 0.9927612184546888, 0.9928385415114462, 0.9926851340569556, 0.9928466510027647, 0.9927381426095963, 0.9928628606721759, 0.9928400968201458, 0.9928709678351879, 0.9927574652247131, 0.9929448682814837, 0.9930134601891041, 0.9926461605355144, 0.992874707095325, 0.9930477631278336, 0.9927303497679532, 0.9930730182677507, 0.9931378685869277, 0.9931185366585851, 0.993031860794872, 0.9929367564618587, 0.9928553844802082, 0.9930652207694948, 0.9929984961636364, 0.9931625020690262, 0.9930430809035897, 0.9930290505290031, 0.9929882073774934, 0.9931210395880044, 0.9931363109499216, 0.9929261556826532, 0.9930979544296861, 0.9933015694841743, 0.9932504282332957, 0.9930948438122869, 0.9933131015859544, 0.9932756880298257, 0.9932395177893341, 0.9931861995719373, 0.9931961740367115, 0.993121346924454, 0.9932722537778318, 0.9932816135697067, 0.9932429497130215, 0.9930243729613721, 0.9932772456668317, 0.9932738160714507, 0.9932862841524184, 0.9930767551995814, 0.9932996951974928, 0.9932588473893702, 0.9930698960088193, 0.9932479369454086, 0.9930365313775837, 0.993005043361336, 0.9930455721914768, 0.9932937743142247, 0.9932569777593017, 0.993226423393935, 0.9932990735396743, 0.9930521310307086, 0.9931818409822881, 0.9933339934796095, 0.99332619830966, 0.9934094469062984, 0.9933224567212164, 0.993430650793016, 0.9933898053131998, 0.9933773395605385, 0.9933118536137044, 0.9933573766611516, 0.9933180911466479, 0.992923669051379, 0.9933782685548067, 0.9933078046888113, 0.9933018791489303, 0.9933682917617261, 0.9932887800969183, 0.9933726550079882, 0.9932741280645132, 0.9934091395698488, 0.9934580852277577, 0.993341478984803, 0.9933798308484256, 0.993338983040303, 0.9934546602889895, 0.9933982267975807, 0.993465889710933, 0.9933788971975446, 0.9934315891005099, 0.9934262875467539, 0.9933489575050771, 0.9934066436253488, 0.9934156821109354, 0.9933685990981758, 0.9933486501686275, 0.993464017752558, 0.9933760892599821, 0.9933957331813872, 0.9934119428507984, 0.9933801428414881, 0.9933352414518595, 0.993437509983778, 0.993333681486547, 0.9933982267975807, 0.9933502078056335, 0.9933910556137562, 0.9933265103027225, 0.9933623615652323, 0.9933982291258872, 0.9933586316183209, 0.9933795141987503, 0.9933882523328066, 0.9933854420669377, 0.9934184877201915, 0.9933835724368691, 0.9934378243051469, 0.9933848204091191, 0.9934188066981733, 0.993430650793016, 0.9933692254126072, 0.9933115462772548, 0.9934319034218788, 0.9934375146403909], 'val_mDice': [0.382642510230653, 0.4054983670357615, 0.4145957331638783, 0.4263313679257408, 0.42427197680808604, 0.4261173808481544, 0.43034102476667613, 0.4335059874574654, 0.4272455387399532, 0.43701597809558734, 0.4421948146773502, 0.4439383922726847, 0.44475807895651087, 0.44670524453977123, 0.4468025721143931, 0.4460197748267092, 0.4475633453694172, 0.44809881306719035, 0.4505637293914333, 0.4500053747324273, 0.45330502500291914, 0.4541067103855312, 0.4537420772248879, 0.44697805831674486, 0.45422498951666057, 0.4548870027065277, 0.45630442968104035, 0.45785627036821097, 0.45843175961636007, 0.45692497165873647, 0.45974935637786984, 0.46149905072525144, 0.4555122123565525, 0.4628816602053121, 0.46099935250822455, 0.4616215085843578, 0.4648834594991058, 0.4644380905665457, 0.4654543416108936, 0.4667525237891823, 0.4649714636616409, 0.46474099333863705, 0.46460820478387177, 0.4670695762615651, 0.4637299099361719, 0.4658844118239358, 0.46668710245285183, 0.46593067992944426, 0.466476480360143, 0.46560474481146086, 0.46390535950196354, 0.46626394381746816, 0.46650685079037857, 0.4656088220144069, 0.46717855788301677, 0.46703565286655646, 0.46743159474524876, 0.4682006994089605, 0.47020402394991834, 0.4685735673410818, 0.4709826211910695, 0.47151543214567937, 0.47108214304898866, 0.4745470543275587, 0.4732514705392532, 0.4714492062339559, 0.4710852999414783, 0.4737792941159569, 0.47352085064630955, 0.47247974580386654, 0.47402653144672513, 0.4748255020240322, 0.4744116944493726, 0.47276632831199095, 0.47322056168923154, 0.4736364251584746, 0.47480601322604343, 0.4758339503314346, 0.4772232664981857, 0.47744759474880993, 0.4822585126385093, 0.4790346318623051, 0.4787849390413612, 0.4772128723561764, 0.4776542983017862, 0.47841181803960353, 0.4791992629179731, 0.4788687190739438, 0.4771754646208137, 0.48302778392098844, 0.4804165370296687, 0.48374308447819203, 0.478826166363433, 0.4806469695176929, 0.4860883398214355, 0.48268816084600985, 0.4821101075503975, 0.4822401510318741, 0.4802052752347663, 0.4802617523819208, 0.48298217239789665, 0.4780114907771349, 0.48246970982290804, 0.48342835856601596, 0.48192739544901997, 0.48124418884981424, 0.4810286225983873, 0.4835981811629608, 0.4875082115177065, 0.48704188549891114, 0.48889665747992694, 0.4844242741819471, 0.4897101502865553, 0.4894029116258025, 0.48436647455673665, 0.4852640745230019, 0.4845592565834522, 0.486123621230945, 0.4857260320568457, 0.4821527167223394, 0.4856190225109458, 0.4851192486239597, 0.48331410449463874, 0.48524845158681273, 0.48613682680297643, 0.4824570141499862, 0.4863489273702726, 0.48266143072396517, 0.48350491968449205, 0.4842708585783839, 0.4850322165293619, 0.4865813534706831, 0.486595073598437, 0.48586564837023616, 0.4852412751642987, 0.4845691111404449, 0.48506966093555093, 0.482194097712636, 0.484378834371455, 0.4841967299580574, 0.48457288765348494, 0.4846791422460228, 0.4857928864657879, 0.4843625973444432, 0.48717164900153875, 0.48361498047597706, 0.4856179904891178, 0.48359586275182664, 0.4845727194333449, 0.4840616916771978, 0.48493006790522486, 0.4854636115487665, 0.48545135010499507], 'loss': [0.3055336955691085, 0.13924967720836273, 0.11637323902513254, 0.1069060651254568, 0.1008904746884985, 0.09577993875293903, 0.09009917259763395, 0.08825070286314426, 0.08619145076214207, 0.0840690551585155, 0.08097898900903613, 0.07902888785231829, 0.07906664580546481, 0.07719448886952833, 0.07645567386487198, 0.0755677898622728, 0.07275737395900266, 0.0716253203826041, 0.0712016311139682, 0.07103146929642291, 0.07046969450401681, 0.06984171343864956, 0.06896032150685476, 0.07001750212129405, 0.06705668521953041, 0.06760168690340464, 0.06641663660449933, 0.06678890696493774, 0.06552408595104257, 0.06441096329925866, 0.0640865449917099, 0.06347190406321926, 0.06329141481299842, 0.06338011876719885, 0.062101351442833706, 0.06250602301057379, 0.06295301874447495, 0.06167580073229095, 0.061577823352831726, 0.060240857196668586, 0.060891105049721825, 0.060153265092082596, 0.06008386286543653, 0.059835249029400316, 0.05964614622328663, 0.05873595775506825, 0.057556860165353906, 0.05833680717229317, 0.05815386622572826, 0.05786419014690007, 0.05812009327798351, 0.058088908392896606, 0.057951072987722604, 0.05686052936588387, 0.05614273875356205, 0.05604962866778876, 0.055412396003459825, 0.056334475909942006, 0.05678034967227695, 0.05577812101456074, 0.054827365242148214, 0.05499756364778344, 0.05609078178394543, 0.05455095203039579, 0.055764542408032164, 0.05352339934004964, 0.055364924962025865, 0.053898865201442214, 0.05370495741920935, 0.053347246415166055, 0.05372619473523513, 0.056065252322352585, 0.0531221374097194, 0.053776837238745226, 0.0541042673571525, 0.05401269281812145, 0.054125558526497676, 0.05245030149635682, 0.0528587102794794, 0.05357154507496859, 0.05343254669796123, 0.05292113103250379, 0.05183093311604659, 0.05251511944490567, 0.052344152552610324, 0.05220248032955532, 0.052663779760281, 0.05244753349616135, 0.05247973966826348, 0.05196061238267356, 0.051538446145328036, 0.053317093089667845, 0.0508512978709172, 0.051842567414492, 0.051420741633275, 0.05147707353413029, 0.05108442539081932, 0.051584339663308436, 0.05087052857947153, 0.051469562183734476, 0.05158818511769956, 0.052008996531843404, 0.051327539450119246, 0.05086178994177387, 0.0503113827383226, 0.05080642079841638, 0.05090328657064092, 0.04938542813606291, 0.05135589353528133, 0.05081481002924735, 0.0497490607588995, 0.05113110680055629, 0.04936152772266607, 0.049529511142624674, 0.04981558660697311, 0.04946303605320036, 0.04999752992711528, 0.05061316507226487, 0.049363741668706655, 0.048867662988349485, 0.04981389557361686, 0.050039839959939596, 0.049191797825847476, 0.04935954831799411, 0.049471720525081864, 0.04831662787590014, 0.0489943320308397, 0.04856186397170251, 0.049351119196649015, 0.048274741240691484, 0.04932263540369134, 0.04900043318724402, 0.04904116641769457, 0.0495168375708548, 0.04897231636070078, 0.049101494068299986, 0.048815328436950724, 0.0478255234990259, 0.04793844541959595, 0.0490526710022849, 0.04885513155845672, 0.04870253746464123, 0.0479961430325635, 0.04811914351893119, 0.048838354963118905, 0.04872114029362931, 0.048190760369415525, 0.04875631893835542, 0.04912513004365083, 0.04927179802512546, 0.04767044079183244, 0.04914741183204685, 0.04917405623878989], 'acc': [0.9741565066097477, 0.9850758095575401, 0.9876155930140493, 0.9885766836177794, 0.9893926549445354, 0.9899599817872006, 0.990450460091969, 0.9907522773069553, 0.9910281228038565, 0.9912615485677213, 0.9914781290977636, 0.9916600433669606, 0.9917679137921547, 0.9919266936822512, 0.9920323059106518, 0.9921601522781852, 0.9922625603708529, 0.9923780996638529, 0.9924733435233684, 0.9925010266087397, 0.9925818546829122, 0.9926670307659917, 0.9927614586782938, 0.9927903381224444, 0.9928945784537208, 0.9928889570854482, 0.992964522201757, 0.9930463552378024, 0.9930972885597981, 0.9931131241919064, 0.9931843296096786, 0.9932360090111916, 0.9932825637498051, 0.9933304223599329, 0.9933541034369625, 0.9933879724484613, 0.9933964207264636, 0.993472449445996, 0.9934912480044285, 0.9935328228362584, 0.9935668580302437, 0.9935592486424301, 0.9936177256950258, 0.9936295508424261, 0.9936777566788684, 0.9937099838168194, 0.9937427952354123, 0.9937504041592419, 0.9938015363018895, 0.9938085278706281, 0.993821735683739, 0.9938376069941476, 0.9938834256946909, 0.9939190144459655, 0.9939181382846954, 0.9939783359704095, 0.9940337800212309, 0.9940198738635646, 0.994020235861194, 0.9940352958850056, 0.9940323192053616, 0.9940457391492357, 0.9940323380902072, 0.994097617800759, 0.9940713905728596, 0.9940848333696822, 0.9940954202335305, 0.994149946156758, 0.9941146631225333, 0.9941403843030512, 0.9941541795754413, 0.9941241928574598, 0.9941742851701751, 0.9941738450154619, 0.9941790115408079, 0.9941743422263699, 0.9941986995498464, 0.9942120215376731, 0.9942327949371129, 0.9942488020909875, 0.9942078573564986, 0.9942517469842578, 0.9942667757480059, 0.9942610720398674, 0.994270897519648, 0.9942966193095779, 0.9942502260512835, 0.9943123525327765, 0.994293985299509, 0.9943682461426456, 0.9943484277748488, 0.9942862414047806, 0.9943892655713891, 0.9943399984786441, 0.9943676530393807, 0.9943758467216864, 0.9943774092401018, 0.9943944598042038, 0.9943977650884719, 0.9944191417849769, 0.9943957853513935, 0.9943861353753322, 0.9944139392212236, 0.9944281471139866, 0.9944478495273078, 0.9944401989695613, 0.9944490050762611, 0.9944794278285001, 0.9944659290696543, 0.9944500474684936, 0.9944723425626727, 0.994513069335434, 0.9945052700188429, 0.9945095775849564, 0.994519422372016, 0.9945227929603158, 0.9945222694477415, 0.9944923373898577, 0.9945222134857182, 0.9945597360050192, 0.9945512783434577, 0.9945503918706602, 0.994536713176008, 0.9945924178820198, 0.9945683701605945, 0.9945816086866831, 0.9945856978963362, 0.9946017932418166, 0.9946000225884416, 0.9946151524453237, 0.9945986310516318, 0.9946161963195354, 0.9946223071979649, 0.9946284418503859, 0.9946561208914779, 0.9946403223019216, 0.9946112994105056, 0.994655818581595, 0.9946614715077119, 0.994643837279305, 0.9946418059707395, 0.9946646011913087, 0.9946670596214818, 0.9946447180942666, 0.9946434524079517, 0.9946453118623446, 0.9946561028445738, 0.9946681954268727, 0.9946761063766546, 0.9946786424306762, 0.9946646803456122, 0.9946435451562969, 0.9946564445861806], 'mDice': [0.40254480697778416, 0.7293170137508823, 0.77371832287388, 0.7921374680157792, 0.8037302817736459, 0.813653171953766, 0.8247541853886902, 0.8282922364774052, 0.8322697486439025, 0.8363860022380278, 0.8424492207012558, 0.8462533274956743, 0.8461220267711789, 0.8497814628895352, 0.8512064579000411, 0.8529093510514464, 0.8584779906142114, 0.8606795644831461, 0.8614781934351811, 0.8618026388983285, 0.8628824372667687, 0.8640950286645397, 0.8658080169249437, 0.8636742082905527, 0.8695434812706108, 0.8684515448953066, 0.8707844528270706, 0.8699979244291943, 0.872501011759319, 0.8747161658282121, 0.8753291649047722, 0.8765296366046794, 0.8768653839588103, 0.8766644525511459, 0.8792093886828379, 0.8783817898218761, 0.8774851927279392, 0.8800010188160575, 0.8801831350393899, 0.8828307668482286, 0.8815154793324492, 0.8829927885906936, 0.8831065531888878, 0.883591938565192, 0.8839517450596408, 0.8857473781732951, 0.8880898672085378, 0.8865267743858324, 0.8868679524620166, 0.8874412892031192, 0.8869282779475177, 0.8869729562230765, 0.8872267921487179, 0.8893865959177149, 0.8908236794172851, 0.8909837444224202, 0.8922227512423109, 0.8903908723002772, 0.8894984529895682, 0.8914931473263025, 0.8933955429426341, 0.8930449985600493, 0.8908670325519102, 0.8939143815369415, 0.8914995398013227, 0.8959726465573167, 0.892283476178538, 0.8951897224971531, 0.8955980394586559, 0.8962999646175028, 0.8955327614675936, 0.8908683445845226, 0.8967306644773599, 0.8954145911299187, 0.8947623578100738, 0.8949495513562169, 0.8947059571002022, 0.8980532181557981, 0.8972282020683084, 0.8957920470460584, 0.8960834632677332, 0.8970912613292503, 0.8992617533564831, 0.8978929510403936, 0.8982326510623505, 0.8985031784460368, 0.8976026838583662, 0.8980034631041272, 0.8979500505730875, 0.8989539292201415, 0.8998033260390772, 0.8962775463476627, 0.9011558937643555, 0.8991970790603665, 0.9000295773425542, 0.8999103695888125, 0.9006946117492646, 0.8996863745103608, 0.9011129427139001, 0.8999041225540166, 0.8996793059358423, 0.8988396060983381, 0.9001874484687064, 0.901111638198781, 0.9022050164185488, 0.901222548687224, 0.901021405420911, 0.904039619137106, 0.90010382519848, 0.9011938287358784, 0.9033130496010695, 0.9005274743489221, 0.9040678415312866, 0.9037359366294044, 0.9031552556771025, 0.9038589131898608, 0.902789371226284, 0.9015747722750329, 0.9040614033847842, 0.9050269443924812, 0.903141284296775, 0.9026893408951954, 0.9043928928473304, 0.9040333255770537, 0.9038166719175762, 0.906120331618821, 0.9047573305760802, 0.9056163400366593, 0.9040424262923248, 0.9061855290033571, 0.9040985010539223, 0.9047300761687406, 0.9046478700046411, 0.9036939255647028, 0.9047702610236477, 0.9045219986893317, 0.9051045530852339, 0.9070649434054617, 0.9068389324252997, 0.90461411488739, 0.9050103660291587, 0.9053055342895804, 0.9067131484065223, 0.9064790306813462, 0.9050465333928516, 0.9052738377547499, 0.9063331487789738, 0.9051973329329167, 0.9044547332618476, 0.904156754417703, 0.9073653059610401, 0.9044232193198378, 0.90436479651313], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:00<00:01,  1.68it/s]predicting test subjects:  50%|█████     | 2/4 [00:00<00:00,  2.21it/s]predicting test subjects:  75%|███████▌  | 3/4 [00:00<00:00,  2.85it/s]predicting test subjects: 100%|██████████| 4/4 [00:00<00:00,  3.53it/s]predicting test subjects: 100%|██████████| 4/4 [00:00<00:00,  4.20it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<00:45,  5.85it/s]predicting train subjects:   1%|          | 2/266 [00:00<00:42,  6.18it/s]predicting train subjects:   1%|          | 3/266 [00:00<00:43,  5.98it/s]predicting train subjects:   2%|▏         | 4/266 [00:00<00:46,  5.66it/s]predicting train subjects:   2%|▏         | 5/266 [00:00<00:46,  5.67it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<00:43,  6.00it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<00:41,  6.23it/s]predicting train subjects:   3%|▎         | 8/266 [00:01<00:39,  6.46it/s]predicting train subjects:   3%|▎         | 9/266 [00:01<00:38,  6.61it/s]predicting train subjects:   4%|▍         | 10/266 [00:01<00:38,  6.67it/s]predicting train subjects:   4%|▍         | 11/266 [00:01<00:37,  6.76it/s]predicting train subjects:   5%|▍         | 12/266 [00:01<00:37,  6.86it/s]predicting train subjects:   5%|▍         | 13/266 [00:02<00:36,  6.86it/s]predicting train subjects:   5%|▌         | 14/266 [00:02<00:36,  6.88it/s]predicting train subjects:   6%|▌         | 15/266 [00:02<00:36,  6.87it/s]predicting train subjects:   6%|▌         | 16/266 [00:02<00:36,  6.90it/s]predicting train subjects:   6%|▋         | 17/266 [00:02<00:35,  6.95it/s]predicting train subjects:   7%|▋         | 18/266 [00:02<00:35,  6.97it/s]predicting train subjects:   7%|▋         | 19/266 [00:02<00:35,  6.98it/s]predicting train subjects:   8%|▊         | 20/266 [00:03<00:35,  6.95it/s]predicting train subjects:   8%|▊         | 21/266 [00:03<00:35,  6.88it/s]predicting train subjects:   8%|▊         | 22/266 [00:03<00:35,  6.92it/s]predicting train subjects:   9%|▊         | 23/266 [00:03<00:34,  6.94it/s]predicting train subjects:   9%|▉         | 24/266 [00:03<00:34,  7.01it/s]predicting train subjects:   9%|▉         | 25/266 [00:03<00:34,  7.03it/s]predicting train subjects:  10%|▉         | 26/266 [00:03<00:33,  7.06it/s]predicting train subjects:  10%|█         | 27/266 [00:04<00:33,  7.10it/s]predicting train subjects:  11%|█         | 28/266 [00:04<00:33,  7.07it/s]predicting train subjects:  11%|█         | 29/266 [00:04<00:34,  6.96it/s]predicting train subjects:  11%|█▏        | 30/266 [00:04<00:33,  7.05it/s]predicting train subjects:  12%|█▏        | 31/266 [00:04<00:33,  7.06it/s]predicting train subjects:  12%|█▏        | 32/266 [00:04<00:33,  7.04it/s]predicting train subjects:  12%|█▏        | 33/266 [00:04<00:33,  7.01it/s]predicting train subjects:  13%|█▎        | 34/266 [00:05<00:32,  7.03it/s]predicting train subjects:  13%|█▎        | 35/266 [00:05<00:32,  7.02it/s]predicting train subjects:  14%|█▎        | 36/266 [00:05<00:32,  7.01it/s]predicting train subjects:  14%|█▍        | 37/266 [00:05<00:32,  7.03it/s]predicting train subjects:  14%|█▍        | 38/266 [00:05<00:32,  7.02it/s]predicting train subjects:  15%|█▍        | 39/266 [00:05<00:32,  7.03it/s]predicting train subjects:  15%|█▌        | 40/266 [00:05<00:32,  7.01it/s]predicting train subjects:  15%|█▌        | 41/266 [00:06<00:32,  6.98it/s]predicting train subjects:  16%|█▌        | 42/266 [00:06<00:30,  7.41it/s]predicting train subjects:  16%|█▌        | 43/266 [00:06<00:28,  7.75it/s]predicting train subjects:  17%|█▋        | 44/266 [00:06<00:27,  8.05it/s]predicting train subjects:  17%|█▋        | 45/266 [00:06<00:26,  8.23it/s]predicting train subjects:  17%|█▋        | 46/266 [00:06<00:26,  8.34it/s]predicting train subjects:  18%|█▊        | 47/266 [00:06<00:25,  8.45it/s]predicting train subjects:  18%|█▊        | 48/266 [00:06<00:25,  8.53it/s]predicting train subjects:  18%|█▊        | 49/266 [00:06<00:25,  8.56it/s]predicting train subjects:  19%|█▉        | 50/266 [00:07<00:25,  8.62it/s]predicting train subjects:  19%|█▉        | 51/266 [00:07<00:25,  8.31it/s]predicting train subjects:  20%|█▉        | 52/266 [00:07<00:25,  8.48it/s]predicting train subjects:  20%|█▉        | 53/266 [00:07<00:24,  8.56it/s]predicting train subjects:  20%|██        | 54/266 [00:07<00:24,  8.58it/s]predicting train subjects:  21%|██        | 55/266 [00:07<00:24,  8.62it/s]predicting train subjects:  21%|██        | 56/266 [00:07<00:24,  8.66it/s]predicting train subjects:  21%|██▏       | 57/266 [00:07<00:24,  8.65it/s]predicting train subjects:  22%|██▏       | 58/266 [00:07<00:24,  8.57it/s]predicting train subjects:  22%|██▏       | 59/266 [00:08<00:23,  8.66it/s]predicting train subjects:  23%|██▎       | 60/266 [00:08<00:24,  8.49it/s]predicting train subjects:  23%|██▎       | 61/266 [00:08<00:24,  8.30it/s]predicting train subjects:  23%|██▎       | 62/266 [00:08<00:24,  8.30it/s]predicting train subjects:  24%|██▎       | 63/266 [00:08<00:24,  8.39it/s]predicting train subjects:  24%|██▍       | 64/266 [00:08<00:24,  8.38it/s]predicting train subjects:  24%|██▍       | 65/266 [00:08<00:23,  8.42it/s]predicting train subjects:  25%|██▍       | 66/266 [00:08<00:23,  8.44it/s]predicting train subjects:  25%|██▌       | 67/266 [00:09<00:23,  8.44it/s]predicting train subjects:  26%|██▌       | 68/266 [00:09<00:23,  8.44it/s]predicting train subjects:  26%|██▌       | 69/266 [00:09<00:23,  8.49it/s]predicting train subjects:  26%|██▋       | 70/266 [00:09<00:23,  8.51it/s]predicting train subjects:  27%|██▋       | 71/266 [00:09<00:23,  8.38it/s]predicting train subjects:  27%|██▋       | 72/266 [00:09<00:23,  8.40it/s]predicting train subjects:  27%|██▋       | 73/266 [00:09<00:23,  8.14it/s]predicting train subjects:  28%|██▊       | 74/266 [00:09<00:23,  8.20it/s]predicting train subjects:  28%|██▊       | 75/266 [00:10<00:23,  8.23it/s]predicting train subjects:  29%|██▊       | 76/266 [00:10<00:22,  8.29it/s]predicting train subjects:  29%|██▉       | 77/266 [00:10<00:22,  8.37it/s]predicting train subjects:  29%|██▉       | 78/266 [00:10<00:23,  7.95it/s]predicting train subjects:  30%|██▉       | 79/266 [00:10<00:24,  7.72it/s]predicting train subjects:  30%|███       | 80/266 [00:10<00:24,  7.58it/s]predicting train subjects:  30%|███       | 81/266 [00:10<00:25,  7.39it/s]predicting train subjects:  31%|███       | 82/266 [00:10<00:25,  7.23it/s]predicting train subjects:  31%|███       | 83/266 [00:11<00:25,  7.11it/s]predicting train subjects:  32%|███▏      | 84/266 [00:11<00:25,  7.07it/s]predicting train subjects:  32%|███▏      | 85/266 [00:11<00:25,  7.08it/s]predicting train subjects:  32%|███▏      | 86/266 [00:11<00:25,  7.11it/s]predicting train subjects:  33%|███▎      | 87/266 [00:11<00:25,  7.07it/s]predicting train subjects:  33%|███▎      | 88/266 [00:11<00:25,  7.07it/s]predicting train subjects:  33%|███▎      | 89/266 [00:11<00:24,  7.13it/s]predicting train subjects:  34%|███▍      | 90/266 [00:12<00:24,  7.18it/s]predicting train subjects:  34%|███▍      | 91/266 [00:12<00:24,  7.09it/s]predicting train subjects:  35%|███▍      | 92/266 [00:12<00:24,  7.13it/s]predicting train subjects:  35%|███▍      | 93/266 [00:12<00:24,  7.20it/s]predicting train subjects:  35%|███▌      | 94/266 [00:12<00:23,  7.23it/s]predicting train subjects:  36%|███▌      | 95/266 [00:12<00:23,  7.18it/s]predicting train subjects:  36%|███▌      | 96/266 [00:12<00:26,  6.31it/s]predicting train subjects:  36%|███▋      | 97/266 [00:13<00:27,  6.09it/s]predicting train subjects:  37%|███▋      | 98/266 [00:13<00:27,  6.21it/s]predicting train subjects:  37%|███▋      | 99/266 [00:13<00:27,  6.18it/s]predicting train subjects:  38%|███▊      | 100/266 [00:13<00:24,  6.76it/s]predicting train subjects:  38%|███▊      | 101/266 [00:13<00:23,  7.08it/s]predicting train subjects:  38%|███▊      | 102/266 [00:13<00:22,  7.41it/s]predicting train subjects:  39%|███▊      | 103/266 [00:13<00:21,  7.64it/s]predicting train subjects:  39%|███▉      | 104/266 [00:14<00:20,  7.83it/s]predicting train subjects:  39%|███▉      | 105/266 [00:14<00:20,  7.95it/s]predicting train subjects:  40%|███▉      | 106/266 [00:14<00:19,  8.01it/s]predicting train subjects:  40%|████      | 107/266 [00:14<00:19,  8.06it/s]predicting train subjects:  41%|████      | 108/266 [00:14<00:19,  8.13it/s]predicting train subjects:  41%|████      | 109/266 [00:14<00:19,  8.20it/s]predicting train subjects:  41%|████▏     | 110/266 [00:14<00:18,  8.28it/s]predicting train subjects:  42%|████▏     | 111/266 [00:14<00:18,  8.31it/s]predicting train subjects:  42%|████▏     | 112/266 [00:15<00:18,  8.29it/s]predicting train subjects:  42%|████▏     | 113/266 [00:15<00:18,  8.31it/s]predicting train subjects:  43%|████▎     | 114/266 [00:15<00:18,  8.25it/s]predicting train subjects:  43%|████▎     | 115/266 [00:15<00:18,  8.29it/s]predicting train subjects:  44%|████▎     | 116/266 [00:15<00:18,  8.30it/s]predicting train subjects:  44%|████▍     | 117/266 [00:15<00:17,  8.35it/s]predicting train subjects:  44%|████▍     | 118/266 [00:15<00:17,  8.34it/s]predicting train subjects:  45%|████▍     | 119/266 [00:15<00:18,  7.88it/s]predicting train subjects:  45%|████▌     | 120/266 [00:16<00:19,  7.62it/s]predicting train subjects:  45%|████▌     | 121/266 [00:16<00:19,  7.45it/s]predicting train subjects:  46%|████▌     | 122/266 [00:16<00:19,  7.31it/s]predicting train subjects:  46%|████▌     | 123/266 [00:16<00:19,  7.24it/s]predicting train subjects:  47%|████▋     | 124/266 [00:16<00:19,  7.17it/s]predicting train subjects:  47%|████▋     | 125/266 [00:16<00:19,  7.13it/s]predicting train subjects:  47%|████▋     | 126/266 [00:16<00:19,  7.08it/s]predicting train subjects:  48%|████▊     | 127/266 [00:17<00:19,  7.06it/s]predicting train subjects:  48%|████▊     | 128/266 [00:17<00:19,  7.05it/s]predicting train subjects:  48%|████▊     | 129/266 [00:17<00:19,  7.00it/s]predicting train subjects:  49%|████▉     | 130/266 [00:17<00:19,  6.94it/s]predicting train subjects:  49%|████▉     | 131/266 [00:17<00:20,  6.74it/s]predicting train subjects:  50%|████▉     | 132/266 [00:17<00:19,  6.72it/s]predicting train subjects:  50%|█████     | 133/266 [00:17<00:19,  6.85it/s]predicting train subjects:  50%|█████     | 134/266 [00:18<00:20,  6.58it/s]predicting train subjects:  51%|█████     | 135/266 [00:18<00:19,  6.70it/s]predicting train subjects:  51%|█████     | 136/266 [00:18<00:19,  6.78it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:18<00:18,  6.97it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:18<00:18,  7.10it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:18<00:17,  7.20it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:18<00:17,  7.31it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:19<00:16,  7.39it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:19<00:16,  7.51it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:19<00:16,  7.56it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:19<00:16,  7.57it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:19<00:16,  7.55it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:19<00:15,  7.56it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:19<00:15,  7.59it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:19<00:15,  7.69it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:20<00:15,  7.73it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:20<00:14,  7.77it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:20<00:15,  7.63it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:20<00:14,  7.65it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:20<00:14,  7.61it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:20<00:14,  7.59it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:20<00:13,  8.04it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:20<00:13,  8.45it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:21<00:12,  8.74it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:21<00:12,  8.98it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:21<00:11,  9.05it/s]predicting train subjects:  60%|██████    | 160/266 [00:21<00:11,  9.08it/s]predicting train subjects:  61%|██████    | 161/266 [00:21<00:11,  9.18it/s]predicting train subjects:  61%|██████    | 162/266 [00:21<00:11,  9.35it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:21<00:10,  9.47it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:21<00:10,  9.53it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:21<00:10,  9.56it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:22<00:10,  9.53it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:22<00:10,  9.58it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:22<00:10,  9.63it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:22<00:10,  9.65it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:22<00:09,  9.61it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:22<00:09,  9.63it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:22<00:09,  9.59it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:22<00:10,  9.25it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:22<00:10,  9.06it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:22<00:10,  8.91it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:23<00:10,  8.77it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:23<00:10,  8.64it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:23<00:10,  8.49it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:23<00:10,  8.45it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:23<00:10,  8.49it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:23<00:10,  8.44it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:23<00:09,  8.49it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:23<00:09,  8.52it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:24<00:09,  8.48it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:24<00:09,  8.46it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:24<00:09,  8.52it/s]predicting train subjects:  70%|███████   | 187/266 [00:24<00:09,  8.49it/s]predicting train subjects:  71%|███████   | 188/266 [00:24<00:09,  8.51it/s]predicting train subjects:  71%|███████   | 189/266 [00:24<00:09,  8.51it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:24<00:09,  8.42it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:24<00:09,  8.31it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:25<00:10,  7.02it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:25<00:09,  7.39it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:25<00:09,  7.40it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:25<00:09,  7.61it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:25<00:09,  7.76it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:25<00:08,  7.93it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:25<00:08,  7.98it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:25<00:08,  8.05it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:26<00:08,  8.12it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:26<00:08,  8.09it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:26<00:07,  8.22it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:26<00:07,  8.30it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:26<00:07,  8.38it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:26<00:07,  8.38it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:26<00:07,  8.32it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:26<00:07,  8.36it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:27<00:06,  8.36it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:27<00:06,  8.32it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:27<00:06,  8.30it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:27<00:06,  8.32it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:27<00:06,  8.32it/s]predicting train subjects:  80%|████████  | 213/266 [00:27<00:06,  8.47it/s]predicting train subjects:  80%|████████  | 214/266 [00:27<00:06,  8.60it/s]predicting train subjects:  81%|████████  | 215/266 [00:27<00:05,  8.71it/s]predicting train subjects:  81%|████████  | 216/266 [00:27<00:05,  8.81it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:28<00:05,  8.80it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:28<00:05,  8.78it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:28<00:05,  8.82it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:28<00:05,  8.87it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:28<00:05,  8.91it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:28<00:04,  8.87it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:28<00:04,  8.85it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:28<00:04,  8.77it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:28<00:04,  8.85it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:29<00:04,  8.88it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:29<00:04,  7.89it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:29<00:04,  8.17it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:29<00:04,  8.36it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:29<00:04,  8.50it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:29<00:04,  8.53it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:29<00:04,  8.50it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:29<00:03,  8.39it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:30<00:03,  8.43it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:30<00:03,  8.46it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:30<00:03,  8.50it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:30<00:03,  8.46it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:30<00:03,  8.44it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:30<00:03,  8.49it/s]predicting train subjects:  90%|█████████ | 240/266 [00:30<00:03,  8.44it/s]predicting train subjects:  91%|█████████ | 241/266 [00:30<00:02,  8.45it/s]predicting train subjects:  91%|█████████ | 242/266 [00:31<00:02,  8.47it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:31<00:02,  8.47it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:31<00:02,  8.50it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:31<00:02,  8.51it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:31<00:02,  8.53it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:31<00:02,  8.50it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:31<00:02,  8.48it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:31<00:02,  8.04it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:31<00:02,  7.84it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:32<00:01,  7.63it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:32<00:01,  7.51it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:32<00:01,  7.47it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:32<00:01,  7.44it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:32<00:01,  7.39it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:32<00:01,  7.36it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:32<00:01,  7.32it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:33<00:01,  7.28it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:33<00:00,  7.27it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:33<00:00,  7.24it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:33<00:00,  7.22it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:33<00:00,  7.21it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:33<00:00,  7.24it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:33<00:00,  7.26it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:34<00:00,  7.24it/s]predicting train subjects: 100%|██████████| 266/266 [00:34<00:00,  7.23it/s]predicting train subjects: 100%|██████████| 266/266 [00:34<00:00,  7.78it/s]
predicting test subjects sagittal:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects sagittal:  25%|██▌       | 1/4 [00:00<00:00,  7.69it/s]predicting test subjects sagittal:  50%|█████     | 2/4 [00:00<00:00,  7.86it/s]predicting test subjects sagittal:  75%|███████▌  | 3/4 [00:00<00:00,  8.02it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  7.97it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  8.05it/s]
predicting train subjects sagittal:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/266 [00:00<00:41,  6.34it/s]predicting train subjects sagittal:   1%|          | 2/266 [00:00<00:40,  6.53it/s]predicting train subjects sagittal:   1%|          | 3/266 [00:00<00:37,  7.00it/s]predicting train subjects sagittal:   2%|▏         | 4/266 [00:00<00:35,  7.41it/s]predicting train subjects sagittal:   2%|▏         | 5/266 [00:00<00:36,  7.12it/s]predicting train subjects sagittal:   2%|▏         | 6/266 [00:00<00:37,  7.01it/s]predicting train subjects sagittal:   3%|▎         | 7/266 [00:00<00:37,  6.98it/s]predicting train subjects sagittal:   3%|▎         | 8/266 [00:01<00:36,  6.98it/s]predicting train subjects sagittal:   3%|▎         | 9/266 [00:01<00:37,  6.94it/s]predicting train subjects sagittal:   4%|▍         | 10/266 [00:01<00:37,  6.84it/s]predicting train subjects sagittal:   4%|▍         | 11/266 [00:01<00:37,  6.87it/s]predicting train subjects sagittal:   5%|▍         | 12/266 [00:01<00:36,  6.88it/s]predicting train subjects sagittal:   5%|▍         | 13/266 [00:01<00:36,  6.87it/s]predicting train subjects sagittal:   5%|▌         | 14/266 [00:02<00:37,  6.79it/s]predicting train subjects sagittal:   6%|▌         | 15/266 [00:02<00:37,  6.66it/s]predicting train subjects sagittal:   6%|▌         | 16/266 [00:02<00:37,  6.65it/s]predicting train subjects sagittal:   6%|▋         | 17/266 [00:02<00:36,  6.74it/s]predicting train subjects sagittal:   7%|▋         | 18/266 [00:02<00:36,  6.78it/s]predicting train subjects sagittal:   7%|▋         | 19/266 [00:02<00:36,  6.82it/s]predicting train subjects sagittal:   8%|▊         | 20/266 [00:02<00:35,  6.84it/s]predicting train subjects sagittal:   8%|▊         | 21/266 [00:03<00:36,  6.81it/s]predicting train subjects sagittal:   8%|▊         | 22/266 [00:03<00:35,  6.81it/s]predicting train subjects sagittal:   9%|▊         | 23/266 [00:03<00:35,  6.86it/s]predicting train subjects sagittal:   9%|▉         | 24/266 [00:03<00:35,  6.88it/s]predicting train subjects sagittal:   9%|▉         | 25/266 [00:03<00:34,  6.94it/s]predicting train subjects sagittal:  10%|▉         | 26/266 [00:03<00:34,  7.00it/s]predicting train subjects sagittal:  10%|█         | 27/266 [00:03<00:33,  7.11it/s]predicting train subjects sagittal:  11%|█         | 28/266 [00:04<00:33,  7.16it/s]predicting train subjects sagittal:  11%|█         | 29/266 [00:04<00:32,  7.21it/s]predicting train subjects sagittal:  11%|█▏        | 30/266 [00:04<00:32,  7.23it/s]predicting train subjects sagittal:  12%|█▏        | 31/266 [00:04<00:32,  7.16it/s]predicting train subjects sagittal:  12%|█▏        | 32/266 [00:04<00:32,  7.18it/s]predicting train subjects sagittal:  12%|█▏        | 33/266 [00:04<00:32,  7.22it/s]predicting train subjects sagittal:  13%|█▎        | 34/266 [00:04<00:32,  7.19it/s]predicting train subjects sagittal:  13%|█▎        | 35/266 [00:05<00:32,  7.20it/s]predicting train subjects sagittal:  14%|█▎        | 36/266 [00:05<00:31,  7.20it/s]predicting train subjects sagittal:  14%|█▍        | 37/266 [00:05<00:32,  7.05it/s]predicting train subjects sagittal:  14%|█▍        | 38/266 [00:05<00:32,  7.10it/s]predicting train subjects sagittal:  15%|█▍        | 39/266 [00:05<00:31,  7.13it/s]predicting train subjects sagittal:  15%|█▌        | 40/266 [00:05<00:31,  7.23it/s]predicting train subjects sagittal:  15%|█▌        | 41/266 [00:05<00:31,  7.25it/s]predicting train subjects sagittal:  16%|█▌        | 42/266 [00:05<00:29,  7.65it/s]predicting train subjects sagittal:  16%|█▌        | 43/266 [00:06<00:28,  7.86it/s]predicting train subjects sagittal:  17%|█▋        | 44/266 [00:06<00:27,  8.10it/s]predicting train subjects sagittal:  17%|█▋        | 45/266 [00:06<00:27,  8.16it/s]predicting train subjects sagittal:  17%|█▋        | 46/266 [00:06<00:26,  8.38it/s]predicting train subjects sagittal:  18%|█▊        | 47/266 [00:06<00:25,  8.50it/s]predicting train subjects sagittal:  18%|█▊        | 48/266 [00:06<00:25,  8.57it/s]predicting train subjects sagittal:  18%|█▊        | 49/266 [00:06<00:24,  8.69it/s]predicting train subjects sagittal:  19%|█▉        | 50/266 [00:06<00:25,  8.54it/s]predicting train subjects sagittal:  19%|█▉        | 51/266 [00:06<00:24,  8.62it/s]predicting train subjects sagittal:  20%|█▉        | 52/266 [00:07<00:24,  8.68it/s]predicting train subjects sagittal:  20%|█▉        | 53/266 [00:07<00:24,  8.68it/s]predicting train subjects sagittal:  20%|██        | 54/266 [00:07<00:24,  8.74it/s]predicting train subjects sagittal:  21%|██        | 55/266 [00:07<00:24,  8.76it/s]predicting train subjects sagittal:  21%|██        | 56/266 [00:07<00:24,  8.74it/s]predicting train subjects sagittal:  21%|██▏       | 57/266 [00:07<00:24,  8.60it/s]predicting train subjects sagittal:  22%|██▏       | 58/266 [00:07<00:24,  8.66it/s]predicting train subjects sagittal:  22%|██▏       | 59/266 [00:07<00:23,  8.74it/s]predicting train subjects sagittal:  23%|██▎       | 60/266 [00:08<00:23,  8.73it/s]predicting train subjects sagittal:  23%|██▎       | 61/266 [00:08<00:23,  8.76it/s]predicting train subjects sagittal:  23%|██▎       | 62/266 [00:08<00:23,  8.75it/s]predicting train subjects sagittal:  24%|██▎       | 63/266 [00:08<00:23,  8.69it/s]predicting train subjects sagittal:  24%|██▍       | 64/266 [00:08<00:23,  8.62it/s]predicting train subjects sagittal:  24%|██▍       | 65/266 [00:08<00:23,  8.61it/s]predicting train subjects sagittal:  25%|██▍       | 66/266 [00:08<00:23,  8.56it/s]predicting train subjects sagittal:  25%|██▌       | 67/266 [00:08<00:23,  8.53it/s]predicting train subjects sagittal:  26%|██▌       | 68/266 [00:08<00:23,  8.47it/s]predicting train subjects sagittal:  26%|██▌       | 69/266 [00:09<00:23,  8.45it/s]predicting train subjects sagittal:  26%|██▋       | 70/266 [00:09<00:23,  8.44it/s]predicting train subjects sagittal:  27%|██▋       | 71/266 [00:09<00:23,  8.47it/s]predicting train subjects sagittal:  27%|██▋       | 72/266 [00:09<00:22,  8.48it/s]predicting train subjects sagittal:  27%|██▋       | 73/266 [00:09<00:22,  8.46it/s]predicting train subjects sagittal:  28%|██▊       | 74/266 [00:09<00:22,  8.47it/s]predicting train subjects sagittal:  28%|██▊       | 75/266 [00:09<00:22,  8.31it/s]predicting train subjects sagittal:  29%|██▊       | 76/266 [00:09<00:22,  8.29it/s]predicting train subjects sagittal:  29%|██▉       | 77/266 [00:10<00:22,  8.40it/s]predicting train subjects sagittal:  29%|██▉       | 78/266 [00:10<00:23,  7.99it/s]predicting train subjects sagittal:  30%|██▉       | 79/266 [00:10<00:24,  7.75it/s]predicting train subjects sagittal:  30%|███       | 80/266 [00:10<00:24,  7.63it/s]predicting train subjects sagittal:  30%|███       | 81/266 [00:10<00:25,  7.25it/s]predicting train subjects sagittal:  31%|███       | 82/266 [00:10<00:25,  7.26it/s]predicting train subjects sagittal:  31%|███       | 83/266 [00:10<00:25,  7.19it/s]predicting train subjects sagittal:  32%|███▏      | 84/266 [00:11<00:25,  7.14it/s]predicting train subjects sagittal:  32%|███▏      | 85/266 [00:11<00:25,  7.22it/s]predicting train subjects sagittal:  32%|███▏      | 86/266 [00:11<00:24,  7.22it/s]predicting train subjects sagittal:  33%|███▎      | 87/266 [00:11<00:24,  7.22it/s]predicting train subjects sagittal:  33%|███▎      | 88/266 [00:11<00:25,  7.03it/s]predicting train subjects sagittal:  33%|███▎      | 89/266 [00:11<00:25,  7.06it/s]predicting train subjects sagittal:  34%|███▍      | 90/266 [00:11<00:24,  7.13it/s]predicting train subjects sagittal:  34%|███▍      | 91/266 [00:11<00:24,  7.18it/s]predicting train subjects sagittal:  35%|███▍      | 92/266 [00:12<00:24,  7.15it/s]predicting train subjects sagittal:  35%|███▍      | 93/266 [00:12<00:24,  7.17it/s]predicting train subjects sagittal:  35%|███▌      | 94/266 [00:12<00:24,  7.15it/s]predicting train subjects sagittal:  36%|███▌      | 95/266 [00:12<00:23,  7.15it/s]predicting train subjects sagittal:  36%|███▌      | 96/266 [00:12<00:22,  7.49it/s]predicting train subjects sagittal:  36%|███▋      | 97/266 [00:12<00:23,  7.14it/s]predicting train subjects sagittal:  37%|███▋      | 98/266 [00:12<00:23,  7.20it/s]predicting train subjects sagittal:  37%|███▋      | 99/266 [00:13<00:21,  7.76it/s]predicting train subjects sagittal:  38%|███▊      | 100/266 [00:13<00:20,  7.99it/s]predicting train subjects sagittal:  38%|███▊      | 101/266 [00:13<00:20,  8.09it/s]predicting train subjects sagittal:  38%|███▊      | 102/266 [00:13<00:19,  8.23it/s]predicting train subjects sagittal:  39%|███▊      | 103/266 [00:13<00:19,  8.25it/s]predicting train subjects sagittal:  39%|███▉      | 104/266 [00:13<00:19,  8.30it/s]predicting train subjects sagittal:  39%|███▉      | 105/266 [00:13<00:19,  8.17it/s]predicting train subjects sagittal:  40%|███▉      | 106/266 [00:13<00:19,  8.24it/s]predicting train subjects sagittal:  40%|████      | 107/266 [00:14<00:19,  8.26it/s]predicting train subjects sagittal:  41%|████      | 108/266 [00:14<00:18,  8.34it/s]predicting train subjects sagittal:  41%|████      | 109/266 [00:14<00:18,  8.36it/s]predicting train subjects sagittal:  41%|████▏     | 110/266 [00:14<00:18,  8.37it/s]predicting train subjects sagittal:  42%|████▏     | 111/266 [00:14<00:18,  8.36it/s]predicting train subjects sagittal:  42%|████▏     | 112/266 [00:14<00:18,  8.43it/s]predicting train subjects sagittal:  42%|████▏     | 113/266 [00:14<00:17,  8.52it/s]predicting train subjects sagittal:  43%|████▎     | 114/266 [00:14<00:17,  8.50it/s]predicting train subjects sagittal:  43%|████▎     | 115/266 [00:14<00:17,  8.46it/s]predicting train subjects sagittal:  44%|████▎     | 116/266 [00:15<00:17,  8.40it/s]predicting train subjects sagittal:  44%|████▍     | 117/266 [00:15<00:17,  8.30it/s]predicting train subjects sagittal:  44%|████▍     | 118/266 [00:15<00:17,  8.29it/s]predicting train subjects sagittal:  45%|████▍     | 119/266 [00:15<00:18,  7.77it/s]predicting train subjects sagittal:  45%|████▌     | 120/266 [00:15<00:19,  7.52it/s]predicting train subjects sagittal:  45%|████▌     | 121/266 [00:15<00:19,  7.31it/s]predicting train subjects sagittal:  46%|████▌     | 122/266 [00:15<00:19,  7.28it/s]predicting train subjects sagittal:  46%|████▌     | 123/266 [00:16<00:19,  7.23it/s]predicting train subjects sagittal:  47%|████▋     | 124/266 [00:16<00:19,  7.18it/s]predicting train subjects sagittal:  47%|████▋     | 125/266 [00:16<00:19,  7.16it/s]predicting train subjects sagittal:  47%|████▋     | 126/266 [00:16<00:19,  7.16it/s]predicting train subjects sagittal:  48%|████▊     | 127/266 [00:16<00:19,  7.18it/s]predicting train subjects sagittal:  48%|████▊     | 128/266 [00:16<00:19,  7.20it/s]predicting train subjects sagittal:  48%|████▊     | 129/266 [00:16<00:19,  7.15it/s]predicting train subjects sagittal:  49%|████▉     | 130/266 [00:17<00:19,  7.15it/s]predicting train subjects sagittal:  49%|████▉     | 131/266 [00:17<00:18,  7.19it/s]predicting train subjects sagittal:  50%|████▉     | 132/266 [00:17<00:18,  7.14it/s]predicting train subjects sagittal:  50%|█████     | 133/266 [00:17<00:18,  7.16it/s]predicting train subjects sagittal:  50%|█████     | 134/266 [00:17<00:18,  7.06it/s]predicting train subjects sagittal:  51%|█████     | 135/266 [00:17<00:18,  7.05it/s]predicting train subjects sagittal:  51%|█████     | 136/266 [00:17<00:18,  7.09it/s]predicting train subjects sagittal:  52%|█████▏    | 137/266 [00:18<00:17,  7.30it/s]predicting train subjects sagittal:  52%|█████▏    | 138/266 [00:18<00:17,  7.43it/s]predicting train subjects sagittal:  52%|█████▏    | 139/266 [00:18<00:16,  7.51it/s]predicting train subjects sagittal:  53%|█████▎    | 140/266 [00:18<00:16,  7.56it/s]predicting train subjects sagittal:  53%|█████▎    | 141/266 [00:18<00:16,  7.53it/s]predicting train subjects sagittal:  53%|█████▎    | 142/266 [00:18<00:16,  7.56it/s]predicting train subjects sagittal:  54%|█████▍    | 143/266 [00:18<00:16,  7.56it/s]predicting train subjects sagittal:  54%|█████▍    | 144/266 [00:18<00:16,  7.27it/s]predicting train subjects sagittal:  55%|█████▍    | 145/266 [00:19<00:16,  7.21it/s]predicting train subjects sagittal:  55%|█████▍    | 146/266 [00:19<00:16,  7.27it/s]predicting train subjects sagittal:  55%|█████▌    | 147/266 [00:19<00:16,  7.36it/s]predicting train subjects sagittal:  56%|█████▌    | 148/266 [00:19<00:16,  7.19it/s]predicting train subjects sagittal:  56%|█████▌    | 149/266 [00:19<00:15,  7.33it/s]predicting train subjects sagittal:  56%|█████▋    | 150/266 [00:19<00:15,  7.29it/s]predicting train subjects sagittal:  57%|█████▋    | 151/266 [00:19<00:15,  7.36it/s]predicting train subjects sagittal:  57%|█████▋    | 152/266 [00:20<00:15,  7.28it/s]predicting train subjects sagittal:  58%|█████▊    | 153/266 [00:20<00:15,  7.28it/s]predicting train subjects sagittal:  58%|█████▊    | 154/266 [00:20<00:15,  7.36it/s]predicting train subjects sagittal:  58%|█████▊    | 155/266 [00:20<00:14,  7.89it/s]predicting train subjects sagittal:  59%|█████▊    | 156/266 [00:20<00:13,  8.21it/s]predicting train subjects sagittal:  59%|█████▉    | 157/266 [00:20<00:12,  8.50it/s]predicting train subjects sagittal:  59%|█████▉    | 158/266 [00:20<00:12,  8.71it/s]predicting train subjects sagittal:  60%|█████▉    | 159/266 [00:20<00:12,  8.89it/s]predicting train subjects sagittal:  60%|██████    | 160/266 [00:20<00:11,  9.05it/s]predicting train subjects sagittal:  61%|██████    | 161/266 [00:21<00:11,  9.12it/s]predicting train subjects sagittal:  61%|██████    | 162/266 [00:21<00:11,  9.15it/s]predicting train subjects sagittal:  61%|██████▏   | 163/266 [00:21<00:11,  9.22it/s]predicting train subjects sagittal:  62%|██████▏   | 164/266 [00:21<00:11,  9.26it/s]predicting train subjects sagittal:  62%|██████▏   | 165/266 [00:21<00:10,  9.21it/s]predicting train subjects sagittal:  62%|██████▏   | 166/266 [00:21<00:10,  9.21it/s]predicting train subjects sagittal:  63%|██████▎   | 167/266 [00:21<00:10,  9.22it/s]predicting train subjects sagittal:  63%|██████▎   | 168/266 [00:21<00:10,  9.08it/s]predicting train subjects sagittal:  64%|██████▎   | 169/266 [00:21<00:10,  9.13it/s]predicting train subjects sagittal:  64%|██████▍   | 170/266 [00:22<00:10,  9.15it/s]predicting train subjects sagittal:  64%|██████▍   | 171/266 [00:22<00:10,  9.14it/s]predicting train subjects sagittal:  65%|██████▍   | 172/266 [00:22<00:10,  8.99it/s]predicting train subjects sagittal:  65%|██████▌   | 173/266 [00:22<00:10,  8.70it/s]predicting train subjects sagittal:  65%|██████▌   | 174/266 [00:22<00:10,  8.54it/s]predicting train subjects sagittal:  66%|██████▌   | 175/266 [00:22<00:10,  8.45it/s]predicting train subjects sagittal:  66%|██████▌   | 176/266 [00:22<00:10,  8.35it/s]predicting train subjects sagittal:  67%|██████▋   | 177/266 [00:22<00:10,  8.27it/s]predicting train subjects sagittal:  67%|██████▋   | 178/266 [00:23<00:10,  8.27it/s]predicting train subjects sagittal:  67%|██████▋   | 179/266 [00:23<00:10,  8.34it/s]predicting train subjects sagittal:  68%|██████▊   | 180/266 [00:23<00:10,  8.38it/s]predicting train subjects sagittal:  68%|██████▊   | 181/266 [00:23<00:10,  8.35it/s]predicting train subjects sagittal:  68%|██████▊   | 182/266 [00:23<00:10,  8.32it/s]predicting train subjects sagittal:  69%|██████▉   | 183/266 [00:23<00:10,  8.23it/s]predicting train subjects sagittal:  69%|██████▉   | 184/266 [00:23<00:09,  8.25it/s]predicting train subjects sagittal:  70%|██████▉   | 185/266 [00:23<00:09,  8.30it/s]predicting train subjects sagittal:  70%|██████▉   | 186/266 [00:23<00:09,  8.38it/s]predicting train subjects sagittal:  70%|███████   | 187/266 [00:24<00:09,  8.45it/s]predicting train subjects sagittal:  71%|███████   | 188/266 [00:24<00:09,  8.47it/s]predicting train subjects sagittal:  71%|███████   | 189/266 [00:24<00:09,  8.36it/s]predicting train subjects sagittal:  71%|███████▏  | 190/266 [00:24<00:09,  8.36it/s]predicting train subjects sagittal:  72%|███████▏  | 191/266 [00:24<00:09,  8.33it/s]predicting train subjects sagittal:  72%|███████▏  | 192/266 [00:24<00:08,  8.45it/s]predicting train subjects sagittal:  73%|███████▎  | 193/266 [00:24<00:08,  8.28it/s]predicting train subjects sagittal:  73%|███████▎  | 194/266 [00:24<00:09,  7.93it/s]predicting train subjects sagittal:  73%|███████▎  | 195/266 [00:25<00:09,  7.74it/s]predicting train subjects sagittal:  74%|███████▎  | 196/266 [00:25<00:08,  7.92it/s]predicting train subjects sagittal:  74%|███████▍  | 197/266 [00:25<00:08,  8.01it/s]predicting train subjects sagittal:  74%|███████▍  | 198/266 [00:25<00:08,  8.07it/s]predicting train subjects sagittal:  75%|███████▍  | 199/266 [00:25<00:08,  8.13it/s]predicting train subjects sagittal:  75%|███████▌  | 200/266 [00:25<00:08,  8.14it/s]predicting train subjects sagittal:  76%|███████▌  | 201/266 [00:25<00:07,  8.13it/s]predicting train subjects sagittal:  76%|███████▌  | 202/266 [00:25<00:07,  8.13it/s]predicting train subjects sagittal:  76%|███████▋  | 203/266 [00:26<00:07,  8.21it/s]predicting train subjects sagittal:  77%|███████▋  | 204/266 [00:26<00:07,  8.15it/s]predicting train subjects sagittal:  77%|███████▋  | 205/266 [00:26<00:07,  8.14it/s]predicting train subjects sagittal:  77%|███████▋  | 206/266 [00:26<00:07,  8.15it/s]predicting train subjects sagittal:  78%|███████▊  | 207/266 [00:26<00:07,  8.16it/s]predicting train subjects sagittal:  78%|███████▊  | 208/266 [00:26<00:07,  8.17it/s]predicting train subjects sagittal:  79%|███████▊  | 209/266 [00:26<00:07,  8.14it/s]predicting train subjects sagittal:  79%|███████▉  | 210/266 [00:26<00:06,  8.14it/s]predicting train subjects sagittal:  79%|███████▉  | 211/266 [00:27<00:06,  8.04it/s]predicting train subjects sagittal:  80%|███████▉  | 212/266 [00:27<00:06,  8.07it/s]predicting train subjects sagittal:  80%|████████  | 213/266 [00:27<00:06,  8.26it/s]predicting train subjects sagittal:  80%|████████  | 214/266 [00:27<00:06,  8.33it/s]predicting train subjects sagittal:  81%|████████  | 215/266 [00:27<00:06,  8.42it/s]predicting train subjects sagittal:  81%|████████  | 216/266 [00:27<00:05,  8.47it/s]predicting train subjects sagittal:  82%|████████▏ | 217/266 [00:27<00:05,  8.54it/s]predicting train subjects sagittal:  82%|████████▏ | 218/266 [00:27<00:05,  8.17it/s]predicting train subjects sagittal:  82%|████████▏ | 219/266 [00:27<00:05,  8.32it/s]predicting train subjects sagittal:  83%|████████▎ | 220/266 [00:28<00:05,  8.39it/s]predicting train subjects sagittal:  83%|████████▎ | 221/266 [00:28<00:05,  8.47it/s]predicting train subjects sagittal:  83%|████████▎ | 222/266 [00:28<00:05,  8.59it/s]predicting train subjects sagittal:  84%|████████▍ | 223/266 [00:28<00:05,  8.45it/s]predicting train subjects sagittal:  84%|████████▍ | 224/266 [00:28<00:04,  8.66it/s]predicting train subjects sagittal:  85%|████████▍ | 225/266 [00:28<00:04,  8.61it/s]predicting train subjects sagittal:  85%|████████▍ | 226/266 [00:28<00:04,  8.66it/s]predicting train subjects sagittal:  85%|████████▌ | 227/266 [00:28<00:04,  8.68it/s]predicting train subjects sagittal:  86%|████████▌ | 228/266 [00:29<00:04,  8.70it/s]predicting train subjects sagittal:  86%|████████▌ | 229/266 [00:29<00:04,  8.71it/s]predicting train subjects sagittal:  86%|████████▋ | 230/266 [00:29<00:04,  8.65it/s]predicting train subjects sagittal:  87%|████████▋ | 231/266 [00:29<00:04,  8.41it/s]predicting train subjects sagittal:  87%|████████▋ | 232/266 [00:29<00:04,  8.48it/s]predicting train subjects sagittal:  88%|████████▊ | 233/266 [00:29<00:03,  8.52it/s]predicting train subjects sagittal:  88%|████████▊ | 234/266 [00:29<00:03,  8.50it/s]predicting train subjects sagittal:  88%|████████▊ | 235/266 [00:29<00:03,  8.34it/s]predicting train subjects sagittal:  89%|████████▊ | 236/266 [00:29<00:03,  8.27it/s]predicting train subjects sagittal:  89%|████████▉ | 237/266 [00:30<00:03,  8.32it/s]predicting train subjects sagittal:  89%|████████▉ | 238/266 [00:30<00:03,  8.31it/s]predicting train subjects sagittal:  90%|████████▉ | 239/266 [00:30<00:03,  8.32it/s]predicting train subjects sagittal:  90%|█████████ | 240/266 [00:30<00:03,  8.39it/s]predicting train subjects sagittal:  91%|█████████ | 241/266 [00:30<00:02,  8.39it/s]predicting train subjects sagittal:  91%|█████████ | 242/266 [00:30<00:02,  8.36it/s]predicting train subjects sagittal:  91%|█████████▏| 243/266 [00:30<00:02,  8.40it/s]predicting train subjects sagittal:  92%|█████████▏| 244/266 [00:30<00:02,  8.42it/s]predicting train subjects sagittal:  92%|█████████▏| 245/266 [00:31<00:02,  8.39it/s]predicting train subjects sagittal:  92%|█████████▏| 246/266 [00:31<00:02,  8.13it/s]predicting train subjects sagittal:  93%|█████████▎| 247/266 [00:31<00:02,  8.16it/s]predicting train subjects sagittal:  93%|█████████▎| 248/266 [00:31<00:02,  8.04it/s]predicting train subjects sagittal:  94%|█████████▎| 249/266 [00:31<00:02,  7.76it/s]predicting train subjects sagittal:  94%|█████████▍| 250/266 [00:31<00:02,  7.55it/s]predicting train subjects sagittal:  94%|█████████▍| 251/266 [00:31<00:02,  7.45it/s]predicting train subjects sagittal:  95%|█████████▍| 252/266 [00:31<00:01,  7.35it/s]predicting train subjects sagittal:  95%|█████████▌| 253/266 [00:32<00:01,  7.28it/s]predicting train subjects sagittal:  95%|█████████▌| 254/266 [00:32<00:01,  7.22it/s]predicting train subjects sagittal:  96%|█████████▌| 255/266 [00:32<00:01,  7.28it/s]predicting train subjects sagittal:  96%|█████████▌| 256/266 [00:32<00:01,  7.31it/s]predicting train subjects sagittal:  97%|█████████▋| 257/266 [00:32<00:01,  7.34it/s]predicting train subjects sagittal:  97%|█████████▋| 258/266 [00:32<00:01,  7.29it/s]predicting train subjects sagittal:  97%|█████████▋| 259/266 [00:32<00:00,  7.16it/s]predicting train subjects sagittal:  98%|█████████▊| 260/266 [00:33<00:00,  7.19it/s]predicting train subjects sagittal:  98%|█████████▊| 261/266 [00:33<00:00,  7.05it/s]predicting train subjects sagittal:  98%|█████████▊| 262/266 [00:33<00:00,  7.11it/s]predicting train subjects sagittal:  99%|█████████▉| 263/266 [00:33<00:00,  7.08it/s]predicting train subjects sagittal:  99%|█████████▉| 264/266 [00:33<00:00,  7.04it/s]predicting train subjects sagittal: 100%|█████████▉| 265/266 [00:33<00:00,  7.08it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:33<00:00,  7.13it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:33<00:00,  7.83it/s]
saving BB  test1-THALAMUS:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 4/4 [00:00<00:00, 74.08it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 8/266 [00:00<00:03, 69.60it/s]saving BB  train1-THALAMUS:   6%|▌         | 15/266 [00:00<00:03, 67.93it/s]saving BB  train1-THALAMUS:   8%|▊         | 22/266 [00:00<00:03, 66.96it/s]saving BB  train1-THALAMUS:  11%|█▏        | 30/266 [00:00<00:03, 68.21it/s]saving BB  train1-THALAMUS:  14%|█▍        | 37/266 [00:00<00:03, 67.39it/s]saving BB  train1-THALAMUS:  17%|█▋        | 45/266 [00:00<00:03, 69.41it/s]saving BB  train1-THALAMUS:  20%|█▉        | 53/266 [00:00<00:02, 71.89it/s]saving BB  train1-THALAMUS:  23%|██▎       | 61/266 [00:00<00:02, 73.33it/s]saving BB  train1-THALAMUS:  26%|██▋       | 70/266 [00:00<00:02, 76.28it/s]saving BB  train1-THALAMUS:  30%|██▉       | 79/266 [00:01<00:02, 78.25it/s]saving BB  train1-THALAMUS:  33%|███▎      | 87/266 [00:01<00:02, 76.88it/s]saving BB  train1-THALAMUS:  36%|███▌      | 95/266 [00:01<00:02, 75.04it/s]saving BB  train1-THALAMUS:  39%|███▊      | 103/266 [00:01<00:02, 75.44it/s]saving BB  train1-THALAMUS:  42%|████▏     | 111/266 [00:01<00:02, 75.82it/s]saving BB  train1-THALAMUS:  45%|████▍     | 119/266 [00:01<00:01, 75.90it/s]saving BB  train1-THALAMUS:  48%|████▊     | 127/266 [00:01<00:01, 75.39it/s]saving BB  train1-THALAMUS:  51%|█████     | 135/266 [00:01<00:01, 75.04it/s]saving BB  train1-THALAMUS:  54%|█████▍    | 143/266 [00:01<00:01, 74.43it/s]saving BB  train1-THALAMUS:  57%|█████▋    | 151/266 [00:02<00:01, 73.28it/s]saving BB  train1-THALAMUS:  60%|█████▉    | 159/266 [00:02<00:01, 73.88it/s]saving BB  train1-THALAMUS:  63%|██████▎   | 168/266 [00:02<00:01, 77.35it/s]saving BB  train1-THALAMUS:  67%|██████▋   | 177/266 [00:02<00:01, 79.89it/s]saving BB  train1-THALAMUS:  70%|██████▉   | 186/266 [00:02<00:00, 82.32it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 195/266 [00:02<00:00, 81.59it/s]saving BB  train1-THALAMUS:  77%|███████▋  | 204/266 [00:02<00:00, 79.58it/s]saving BB  train1-THALAMUS:  80%|███████▉  | 212/266 [00:02<00:00, 78.92it/s]saving BB  train1-THALAMUS:  83%|████████▎ | 221/266 [00:02<00:00, 80.51it/s]saving BB  train1-THALAMUS:  86%|████████▋ | 230/266 [00:03<00:00, 80.54it/s]saving BB  train1-THALAMUS:  90%|████████▉ | 239/266 [00:03<00:00, 81.13it/s]saving BB  train1-THALAMUS:  93%|█████████▎| 248/266 [00:03<00:00, 81.98it/s]saving BB  train1-THALAMUS:  97%|█████████▋| 257/266 [00:03<00:00, 77.05it/s]saving BB  train1-THALAMUS: 100%|█████████▉| 265/266 [00:03<00:00, 75.99it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 76.11it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 4/4 [00:00<00:00, 85.94it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   3%|▎         | 8/266 [00:00<00:03, 70.69it/s]saving BB  train1-THALAMUS Sagittal:   6%|▌         | 15/266 [00:00<00:03, 69.98it/s]saving BB  train1-THALAMUS Sagittal:   8%|▊         | 22/266 [00:00<00:03, 69.96it/s]saving BB  train1-THALAMUS Sagittal:  11%|█         | 29/266 [00:00<00:03, 69.85it/s]saving BB  train1-THALAMUS Sagittal:  14%|█▍        | 37/266 [00:00<00:03, 71.67it/s]saving BB  train1-THALAMUS Sagittal:  17%|█▋        | 45/266 [00:00<00:03, 73.19it/s]saving BB  train1-THALAMUS Sagittal:  20%|█▉        | 53/266 [00:00<00:02, 73.53it/s]saving BB  train1-THALAMUS Sagittal:  23%|██▎       | 61/266 [00:00<00:02, 75.01it/s]saving BB  train1-THALAMUS Sagittal:  26%|██▋       | 70/266 [00:00<00:02, 77.84it/s]saving BB  train1-THALAMUS Sagittal:  30%|██▉       | 79/266 [00:01<00:02, 79.64it/s]saving BB  train1-THALAMUS Sagittal:  33%|███▎      | 87/266 [00:01<00:02, 79.09it/s]saving BB  train1-THALAMUS Sagittal:  36%|███▌      | 95/266 [00:01<00:02, 73.99it/s]saving BB  train1-THALAMUS Sagittal:  39%|███▊      | 103/266 [00:01<00:02, 75.70it/s]saving BB  train1-THALAMUS Sagittal:  42%|████▏     | 112/266 [00:01<00:01, 77.32it/s]saving BB  train1-THALAMUS Sagittal:  45%|████▌     | 121/266 [00:01<00:01, 78.33it/s]saving BB  train1-THALAMUS Sagittal:  48%|████▊     | 129/266 [00:01<00:01, 77.67it/s]saving BB  train1-THALAMUS Sagittal:  52%|█████▏    | 137/266 [00:01<00:01, 76.88it/s]saving BB  train1-THALAMUS Sagittal:  55%|█████▍    | 145/266 [00:01<00:01, 75.52it/s]saving BB  train1-THALAMUS Sagittal:  58%|█████▊    | 153/266 [00:02<00:01, 74.28it/s]saving BB  train1-THALAMUS Sagittal:  61%|██████    | 162/266 [00:02<00:01, 77.15it/s]saving BB  train1-THALAMUS Sagittal:  64%|██████▍   | 171/266 [00:02<00:01, 80.39it/s]saving BB  train1-THALAMUS Sagittal:  68%|██████▊   | 180/266 [00:02<00:01, 81.40it/s]saving BB  train1-THALAMUS Sagittal:  71%|███████   | 189/266 [00:02<00:00, 80.60it/s]saving BB  train1-THALAMUS Sagittal:  74%|███████▍  | 198/266 [00:02<00:00, 80.21it/s]saving BB  train1-THALAMUS Sagittal:  78%|███████▊  | 207/266 [00:02<00:00, 77.67it/s]saving BB  train1-THALAMUS Sagittal:  81%|████████  | 215/266 [00:02<00:00, 77.40it/s]saving BB  train1-THALAMUS Sagittal:  84%|████████▍ | 223/266 [00:02<00:00, 75.74it/s]saving BB  train1-THALAMUS Sagittal:  87%|████████▋ | 232/266 [00:03<00:00, 77.81it/s]saving BB  train1-THALAMUS Sagittal:  91%|█████████ | 241/266 [00:03<00:00, 79.45it/s]saving BB  train1-THALAMUS Sagittal:  94%|█████████▍| 250/266 [00:03<00:00, 81.58it/s]saving BB  train1-THALAMUS Sagittal:  97%|█████████▋| 259/266 [00:03<00:00, 79.35it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 77.24it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<04:52,  1.10s/it]Loading train:   1%|          | 2/266 [00:02<04:39,  1.06s/it]Loading train:   1%|          | 3/266 [00:02<04:19,  1.01it/s]Loading train:   2%|▏         | 4/266 [00:03<04:01,  1.08it/s]Loading train:   2%|▏         | 5/266 [00:04<04:07,  1.05it/s]Loading train:   2%|▏         | 6/266 [00:05<03:46,  1.15it/s]Loading train:   3%|▎         | 7/266 [00:06<03:30,  1.23it/s]Loading train:   3%|▎         | 8/266 [00:06<03:14,  1.32it/s]Loading train:   3%|▎         | 9/266 [00:07<03:01,  1.41it/s]Loading train:   4%|▍         | 10/266 [00:07<02:52,  1.48it/s]Loading train:   4%|▍         | 11/266 [00:08<02:49,  1.51it/s]Loading train:   5%|▍         | 12/266 [00:09<02:46,  1.52it/s]Loading train:   5%|▍         | 13/266 [00:09<02:44,  1.53it/s]Loading train:   5%|▌         | 14/266 [00:10<02:44,  1.53it/s]Loading train:   6%|▌         | 15/266 [00:11<02:41,  1.55it/s]Loading train:   6%|▌         | 16/266 [00:11<02:42,  1.54it/s]Loading train:   6%|▋         | 17/266 [00:12<02:39,  1.56it/s]Loading train:   7%|▋         | 18/266 [00:12<02:38,  1.56it/s]Loading train:   7%|▋         | 19/266 [00:13<02:35,  1.59it/s]Loading train:   8%|▊         | 20/266 [00:14<02:34,  1.59it/s]Loading train:   8%|▊         | 21/266 [00:14<02:32,  1.61it/s]Loading train:   8%|▊         | 22/266 [00:15<02:32,  1.60it/s]Loading train:   9%|▊         | 23/266 [00:16<02:29,  1.62it/s]Loading train:   9%|▉         | 24/266 [00:16<02:31,  1.59it/s]Loading train:   9%|▉         | 25/266 [00:17<02:28,  1.62it/s]Loading train:  10%|▉         | 26/266 [00:17<02:24,  1.66it/s]Loading train:  10%|█         | 27/266 [00:18<02:24,  1.65it/s]Loading train:  11%|█         | 28/266 [00:19<02:25,  1.64it/s]Loading train:  11%|█         | 29/266 [00:19<02:23,  1.65it/s]Loading train:  11%|█▏        | 30/266 [00:20<02:21,  1.67it/s]Loading train:  12%|█▏        | 31/266 [00:20<02:18,  1.70it/s]Loading train:  12%|█▏        | 32/266 [00:21<02:16,  1.71it/s]Loading train:  12%|█▏        | 33/266 [00:21<02:16,  1.71it/s]Loading train:  13%|█▎        | 34/266 [00:22<02:14,  1.72it/s]Loading train:  13%|█▎        | 35/266 [00:23<02:16,  1.70it/s]Loading train:  14%|█▎        | 36/266 [00:23<02:17,  1.68it/s]Loading train:  14%|█▍        | 37/266 [00:24<02:16,  1.68it/s]Loading train:  14%|█▍        | 38/266 [00:24<02:16,  1.67it/s]Loading train:  15%|█▍        | 39/266 [00:25<02:20,  1.62it/s]Loading train:  15%|█▌        | 40/266 [00:26<02:17,  1.64it/s]Loading train:  15%|█▌        | 41/266 [00:26<02:13,  1.69it/s]Loading train:  16%|█▌        | 42/266 [00:27<02:09,  1.73it/s]Loading train:  16%|█▌        | 43/266 [00:27<02:05,  1.78it/s]Loading train:  17%|█▋        | 44/266 [00:28<02:01,  1.83it/s]Loading train:  17%|█▋        | 45/266 [00:28<01:59,  1.84it/s]Loading train:  17%|█▋        | 46/266 [00:29<01:57,  1.87it/s]Loading train:  18%|█▊        | 47/266 [00:29<01:58,  1.85it/s]Loading train:  18%|█▊        | 48/266 [00:30<01:56,  1.87it/s]Loading train:  18%|█▊        | 49/266 [00:31<01:57,  1.85it/s]Loading train:  19%|█▉        | 50/266 [00:31<01:55,  1.88it/s]Loading train:  19%|█▉        | 51/266 [00:32<01:53,  1.90it/s]Loading train:  20%|█▉        | 52/266 [00:32<01:51,  1.92it/s]Loading train:  20%|█▉        | 53/266 [00:33<01:50,  1.93it/s]Loading train:  20%|██        | 54/266 [00:33<01:49,  1.94it/s]Loading train:  21%|██        | 55/266 [00:34<01:48,  1.95it/s]Loading train:  21%|██        | 56/266 [00:34<01:48,  1.93it/s]Loading train:  21%|██▏       | 57/266 [00:35<01:49,  1.91it/s]Loading train:  22%|██▏       | 58/266 [00:35<01:50,  1.89it/s]Loading train:  22%|██▏       | 59/266 [00:36<01:50,  1.87it/s]Loading train:  23%|██▎       | 60/266 [00:36<01:48,  1.89it/s]Loading train:  23%|██▎       | 61/266 [00:37<01:44,  1.96it/s]Loading train:  23%|██▎       | 62/266 [00:37<01:42,  1.98it/s]Loading train:  24%|██▎       | 63/266 [00:38<01:44,  1.95it/s]Loading train:  24%|██▍       | 64/266 [00:38<01:41,  1.99it/s]Loading train:  24%|██▍       | 65/266 [00:39<01:39,  2.01it/s]Loading train:  25%|██▍       | 66/266 [00:39<01:38,  2.04it/s]Loading train:  25%|██▌       | 67/266 [00:40<01:37,  2.04it/s]Loading train:  26%|██▌       | 68/266 [00:40<01:36,  2.05it/s]Loading train:  26%|██▌       | 69/266 [00:41<01:35,  2.05it/s]Loading train:  26%|██▋       | 70/266 [00:41<01:35,  2.04it/s]Loading train:  27%|██▋       | 71/266 [00:42<01:34,  2.07it/s]Loading train:  27%|██▋       | 72/266 [00:42<01:33,  2.08it/s]Loading train:  27%|██▋       | 73/266 [00:43<01:32,  2.09it/s]Loading train:  28%|██▊       | 74/266 [00:43<01:31,  2.10it/s]Loading train:  28%|██▊       | 75/266 [00:44<01:32,  2.06it/s]Loading train:  29%|██▊       | 76/266 [00:44<01:32,  2.05it/s]Loading train:  29%|██▉       | 77/266 [00:45<01:32,  2.04it/s]Loading train:  29%|██▉       | 78/266 [00:45<01:40,  1.88it/s]Loading train:  30%|██▉       | 79/266 [00:46<01:43,  1.81it/s]Loading train:  30%|███       | 80/266 [00:46<01:45,  1.77it/s]Loading train:  30%|███       | 81/266 [00:47<01:43,  1.78it/s]Loading train:  31%|███       | 82/266 [00:47<01:44,  1.76it/s]Loading train:  31%|███       | 83/266 [00:48<01:45,  1.74it/s]Loading train:  32%|███▏      | 84/266 [00:49<01:46,  1.72it/s]Loading train:  32%|███▏      | 85/266 [00:49<01:44,  1.73it/s]Loading train:  32%|███▏      | 86/266 [00:50<01:44,  1.72it/s]Loading train:  33%|███▎      | 87/266 [00:50<01:44,  1.72it/s]Loading train:  33%|███▎      | 88/266 [00:51<01:43,  1.72it/s]Loading train:  33%|███▎      | 89/266 [00:52<01:42,  1.73it/s]Loading train:  34%|███▍      | 90/266 [00:52<01:44,  1.69it/s]Loading train:  34%|███▍      | 91/266 [00:53<01:43,  1.69it/s]Loading train:  35%|███▍      | 92/266 [00:53<01:41,  1.71it/s]Loading train:  35%|███▍      | 93/266 [00:54<01:40,  1.72it/s]Loading train:  35%|███▌      | 94/266 [00:54<01:39,  1.74it/s]Loading train:  36%|███▌      | 95/266 [00:55<01:38,  1.74it/s]Loading train:  36%|███▌      | 96/266 [00:56<01:50,  1.53it/s]Loading train:  36%|███▋      | 97/266 [00:57<02:05,  1.35it/s]Loading train:  37%|███▋      | 98/266 [00:58<02:08,  1.31it/s]Loading train:  37%|███▋      | 99/266 [00:58<02:03,  1.35it/s]Loading train:  38%|███▊      | 100/266 [00:59<02:04,  1.33it/s]Loading train:  38%|███▊      | 101/266 [01:00<01:55,  1.43it/s]Loading train:  38%|███▊      | 102/266 [01:00<01:47,  1.53it/s]Loading train:  39%|███▊      | 103/266 [01:01<01:40,  1.63it/s]Loading train:  39%|███▉      | 104/266 [01:01<01:36,  1.67it/s]Loading train:  39%|███▉      | 105/266 [01:02<01:33,  1.72it/s]Loading train:  40%|███▉      | 106/266 [01:02<01:31,  1.75it/s]Loading train:  40%|████      | 107/266 [01:03<01:29,  1.78it/s]Loading train:  41%|████      | 108/266 [01:04<01:29,  1.77it/s]Loading train:  41%|████      | 109/266 [01:04<01:29,  1.75it/s]Loading train:  41%|████▏     | 110/266 [01:05<01:27,  1.78it/s]Loading train:  42%|████▏     | 111/266 [01:05<01:27,  1.76it/s]Loading train:  42%|████▏     | 112/266 [01:06<01:26,  1.78it/s]Loading train:  42%|████▏     | 113/266 [01:06<01:23,  1.82it/s]Loading train:  43%|████▎     | 114/266 [01:07<01:22,  1.85it/s]Loading train:  43%|████▎     | 115/266 [01:07<01:21,  1.84it/s]Loading train:  44%|████▎     | 116/266 [01:08<01:20,  1.85it/s]Loading train:  44%|████▍     | 117/266 [01:08<01:21,  1.83it/s]Loading train:  44%|████▍     | 118/266 [01:09<01:21,  1.81it/s]Loading train:  45%|████▍     | 119/266 [01:10<01:25,  1.73it/s]Loading train:  45%|████▌     | 120/266 [01:10<01:25,  1.70it/s]Loading train:  45%|████▌     | 121/266 [01:11<01:26,  1.67it/s]Loading train:  46%|████▌     | 122/266 [01:12<01:27,  1.65it/s]Loading train:  46%|████▌     | 123/266 [01:12<01:27,  1.63it/s]Loading train:  47%|████▋     | 124/266 [01:13<01:26,  1.65it/s]Loading train:  47%|████▋     | 125/266 [01:13<01:26,  1.62it/s]Loading train:  47%|████▋     | 126/266 [01:14<01:25,  1.64it/s]Loading train:  48%|████▊     | 127/266 [01:15<01:25,  1.63it/s]Loading train:  48%|████▊     | 128/266 [01:15<01:24,  1.64it/s]Loading train:  48%|████▊     | 129/266 [01:16<01:22,  1.65it/s]Loading train:  49%|████▉     | 130/266 [01:16<01:22,  1.64it/s]Loading train:  49%|████▉     | 131/266 [01:17<01:23,  1.62it/s]Loading train:  50%|████▉     | 132/266 [01:18<01:21,  1.64it/s]Loading train:  50%|█████     | 133/266 [01:18<01:20,  1.66it/s]Loading train:  50%|█████     | 134/266 [01:19<01:19,  1.65it/s]Loading train:  51%|█████     | 135/266 [01:19<01:19,  1.65it/s]Loading train:  51%|█████     | 136/266 [01:20<01:19,  1.64it/s]Loading train:  52%|█████▏    | 137/266 [01:21<01:19,  1.63it/s]Loading train:  52%|█████▏    | 138/266 [01:21<01:16,  1.67it/s]Loading train:  52%|█████▏    | 139/266 [01:22<01:14,  1.71it/s]Loading train:  53%|█████▎    | 140/266 [01:22<01:12,  1.74it/s]Loading train:  53%|█████▎    | 141/266 [01:23<01:11,  1.74it/s]Loading train:  53%|█████▎    | 142/266 [01:24<01:11,  1.74it/s]Loading train:  54%|█████▍    | 143/266 [01:24<01:10,  1.75it/s]Loading train:  54%|█████▍    | 144/266 [01:25<01:09,  1.74it/s]Loading train:  55%|█████▍    | 145/266 [01:25<01:08,  1.77it/s]Loading train:  55%|█████▍    | 146/266 [01:26<01:06,  1.79it/s]Loading train:  55%|█████▌    | 147/266 [01:26<01:06,  1.80it/s]Loading train:  56%|█████▌    | 148/266 [01:27<01:06,  1.79it/s]Loading train:  56%|█████▌    | 149/266 [01:27<01:06,  1.77it/s]Loading train:  56%|█████▋    | 150/266 [01:28<01:05,  1.78it/s]Loading train:  57%|█████▋    | 151/266 [01:29<01:04,  1.78it/s]Loading train:  57%|█████▋    | 152/266 [01:29<01:04,  1.77it/s]Loading train:  58%|█████▊    | 153/266 [01:30<01:03,  1.78it/s]Loading train:  58%|█████▊    | 154/266 [01:30<01:03,  1.75it/s]Loading train:  58%|█████▊    | 155/266 [01:31<01:00,  1.83it/s]Loading train:  59%|█████▊    | 156/266 [01:31<00:57,  1.92it/s]Loading train:  59%|█████▉    | 157/266 [01:32<00:54,  2.00it/s]Loading train:  59%|█████▉    | 158/266 [01:32<00:52,  2.04it/s]Loading train:  60%|█████▉    | 159/266 [01:33<00:52,  2.05it/s]Loading train:  60%|██████    | 160/266 [01:33<00:51,  2.05it/s]Loading train:  61%|██████    | 161/266 [01:34<00:51,  2.03it/s]Loading train:  61%|██████    | 162/266 [01:34<00:50,  2.07it/s]Loading train:  61%|██████▏   | 163/266 [01:35<00:49,  2.09it/s]Loading train:  62%|██████▏   | 164/266 [01:35<00:48,  2.11it/s]Loading train:  62%|██████▏   | 165/266 [01:35<00:48,  2.10it/s]Loading train:  62%|██████▏   | 166/266 [01:36<00:47,  2.11it/s]Loading train:  63%|██████▎   | 167/266 [01:36<00:46,  2.14it/s]Loading train:  63%|██████▎   | 168/266 [01:37<00:45,  2.14it/s]Loading train:  64%|██████▎   | 169/266 [01:37<00:45,  2.15it/s]Loading train:  64%|██████▍   | 170/266 [01:38<00:44,  2.16it/s]Loading train:  64%|██████▍   | 171/266 [01:38<00:43,  2.16it/s]Loading train:  65%|██████▍   | 172/266 [01:39<00:43,  2.16it/s]Loading train:  65%|██████▌   | 173/266 [01:39<00:45,  2.04it/s]Loading train:  65%|██████▌   | 174/266 [01:40<00:45,  2.03it/s]Loading train:  66%|██████▌   | 175/266 [01:40<00:45,  2.00it/s]Loading train:  66%|██████▌   | 176/266 [01:41<00:47,  1.89it/s]Loading train:  67%|██████▋   | 177/266 [01:41<00:46,  1.89it/s]Loading train:  67%|██████▋   | 178/266 [01:42<00:45,  1.93it/s]Loading train:  67%|██████▋   | 179/266 [01:42<00:45,  1.92it/s]Loading train:  68%|██████▊   | 180/266 [01:43<00:44,  1.94it/s]Loading train:  68%|██████▊   | 181/266 [01:43<00:42,  1.98it/s]Loading train:  68%|██████▊   | 182/266 [01:44<00:41,  2.01it/s]Loading train:  69%|██████▉   | 183/266 [01:44<00:41,  2.01it/s]Loading train:  69%|██████▉   | 184/266 [01:45<00:40,  2.03it/s]Loading train:  70%|██████▉   | 185/266 [01:45<00:39,  2.04it/s]Loading train:  70%|██████▉   | 186/266 [01:46<00:39,  2.03it/s]Loading train:  70%|███████   | 187/266 [01:46<00:40,  1.97it/s]Loading train:  71%|███████   | 188/266 [01:47<00:39,  1.96it/s]Loading train:  71%|███████   | 189/266 [01:47<00:39,  1.95it/s]Loading train:  71%|███████▏  | 190/266 [01:48<00:39,  1.92it/s]Loading train:  72%|███████▏  | 191/266 [01:49<00:47,  1.60it/s]Loading train:  72%|███████▏  | 192/266 [01:50<00:48,  1.51it/s]Loading train:  73%|███████▎  | 193/266 [01:50<00:50,  1.46it/s]Loading train:  73%|███████▎  | 194/266 [01:51<00:53,  1.34it/s]Loading train:  73%|███████▎  | 195/266 [01:52<00:48,  1.47it/s]Loading train:  74%|███████▎  | 196/266 [01:52<00:44,  1.58it/s]Loading train:  74%|███████▍  | 197/266 [01:53<00:41,  1.66it/s]Loading train:  74%|███████▍  | 198/266 [01:53<00:40,  1.68it/s]Loading train:  75%|███████▍  | 199/266 [01:54<00:39,  1.71it/s]Loading train:  75%|███████▌  | 200/266 [01:55<00:38,  1.73it/s]Loading train:  76%|███████▌  | 201/266 [01:55<00:36,  1.77it/s]Loading train:  76%|███████▌  | 202/266 [01:56<00:35,  1.80it/s]Loading train:  76%|███████▋  | 203/266 [01:56<00:35,  1.76it/s]Loading train:  77%|███████▋  | 204/266 [01:57<00:35,  1.76it/s]Loading train:  77%|███████▋  | 205/266 [01:57<00:34,  1.78it/s]Loading train:  77%|███████▋  | 206/266 [01:58<00:33,  1.81it/s]Loading train:  78%|███████▊  | 207/266 [01:58<00:32,  1.81it/s]Loading train:  78%|███████▊  | 208/266 [01:59<00:31,  1.82it/s]Loading train:  79%|███████▊  | 209/266 [01:59<00:31,  1.82it/s]Loading train:  79%|███████▉  | 210/266 [02:00<00:30,  1.81it/s]Loading train:  79%|███████▉  | 211/266 [02:01<00:30,  1.80it/s]Loading train:  80%|███████▉  | 212/266 [02:01<00:30,  1.76it/s]Loading train:  80%|████████  | 213/266 [02:02<00:28,  1.83it/s]Loading train:  80%|████████  | 214/266 [02:02<00:27,  1.87it/s]Loading train:  81%|████████  | 215/266 [02:03<00:26,  1.92it/s]Loading train:  81%|████████  | 216/266 [02:03<00:25,  1.98it/s]Loading train:  82%|████████▏ | 217/266 [02:04<00:24,  2.01it/s]Loading train:  82%|████████▏ | 218/266 [02:04<00:23,  2.02it/s]Loading train:  82%|████████▏ | 219/266 [02:05<00:23,  2.03it/s]Loading train:  83%|████████▎ | 220/266 [02:05<00:23,  1.98it/s]Loading train:  83%|████████▎ | 221/266 [02:06<00:22,  1.99it/s]Loading train:  83%|████████▎ | 222/266 [02:06<00:21,  2.03it/s]Loading train:  84%|████████▍ | 223/266 [02:07<00:20,  2.05it/s]Loading train:  84%|████████▍ | 224/266 [02:07<00:20,  2.06it/s]Loading train:  85%|████████▍ | 225/266 [02:08<00:20,  2.04it/s]Loading train:  85%|████████▍ | 226/266 [02:08<00:19,  2.04it/s]Loading train:  85%|████████▌ | 227/266 [02:09<00:19,  2.05it/s]Loading train:  86%|████████▌ | 228/266 [02:09<00:18,  2.04it/s]Loading train:  86%|████████▌ | 229/266 [02:10<00:18,  2.04it/s]Loading train:  86%|████████▋ | 230/266 [02:10<00:17,  2.02it/s]Loading train:  87%|████████▋ | 231/266 [02:11<00:18,  1.92it/s]Loading train:  87%|████████▋ | 232/266 [02:11<00:17,  1.97it/s]Loading train:  88%|████████▊ | 233/266 [02:13<00:28,  1.18it/s]Loading train:  88%|████████▊ | 234/266 [02:16<00:49,  1.54s/it]Loading train:  88%|████████▊ | 235/266 [02:20<01:13,  2.39s/it]Loading train:  89%|████████▊ | 236/266 [02:24<01:25,  2.85s/it]Loading train:  89%|████████▉ | 237/266 [02:28<01:30,  3.11s/it]Loading train:  89%|████████▉ | 238/266 [02:32<01:33,  3.34s/it]Loading train:  90%|████████▉ | 239/266 [02:36<01:35,  3.53s/it]Loading train:  90%|█████████ | 240/266 [02:40<01:36,  3.71s/it]Loading train:  91%|█████████ | 241/266 [02:44<01:35,  3.82s/it]Loading train:  91%|█████████ | 242/266 [02:48<01:32,  3.87s/it]Loading train:  91%|█████████▏| 243/266 [02:52<01:28,  3.86s/it]Loading train:  92%|█████████▏| 244/266 [02:56<01:25,  3.89s/it]Loading train:  92%|█████████▏| 245/266 [03:00<01:21,  3.88s/it]Loading train:  92%|█████████▏| 246/266 [03:04<01:18,  3.93s/it]Loading train:  93%|█████████▎| 247/266 [03:07<01:12,  3.84s/it]Loading train:  93%|█████████▎| 248/266 [03:10<01:04,  3.58s/it]Loading train:  94%|█████████▎| 249/266 [03:16<01:10,  4.16s/it]Loading train:  94%|█████████▍| 250/266 [03:21<01:09,  4.37s/it]Loading train:  94%|█████████▍| 251/266 [03:26<01:10,  4.69s/it]Loading train:  95%|█████████▍| 252/266 [03:32<01:10,  5.00s/it]Loading train:  95%|█████████▌| 253/266 [03:37<01:06,  5.09s/it]Loading train:  95%|█████████▌| 254/266 [03:43<01:03,  5.28s/it]Loading train:  96%|█████████▌| 255/266 [03:48<00:58,  5.29s/it]Loading train:  96%|█████████▌| 256/266 [03:54<00:53,  5.38s/it]Loading train:  97%|█████████▋| 257/266 [03:59<00:49,  5.47s/it]Loading train:  97%|█████████▋| 258/266 [04:05<00:43,  5.49s/it]Loading train:  97%|█████████▋| 259/266 [04:11<00:39,  5.62s/it]Loading train:  98%|█████████▊| 260/266 [04:17<00:34,  5.75s/it]Loading train:  98%|█████████▊| 261/266 [04:23<00:28,  5.73s/it]Loading train:  98%|█████████▊| 262/266 [04:28<00:22,  5.73s/it]Loading train:  99%|█████████▉| 263/266 [04:33<00:16,  5.56s/it]Loading train:  99%|█████████▉| 264/266 [04:39<00:11,  5.67s/it]Loading train: 100%|█████████▉| 265/266 [04:45<00:05,  5.63s/it]Loading train: 100%|██████████| 266/266 [04:50<00:00,  5.57s/it]Loading train: 100%|██████████| 266/266 [04:50<00:00,  1.09s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/266 [00:00<00:04, 59.83it/s]concatenating: train:   5%|▍         | 12/266 [00:00<00:04, 59.03it/s]concatenating: train:   7%|▋         | 18/266 [00:00<00:04, 56.21it/s]concatenating: train:   9%|▊         | 23/266 [00:00<00:04, 53.59it/s]concatenating: train:  11%|█         | 29/266 [00:00<00:04, 54.47it/s]concatenating: train:  13%|█▎        | 35/266 [00:00<00:04, 53.93it/s]concatenating: train:  15%|█▌        | 41/266 [00:00<00:04, 54.33it/s]concatenating: train:  18%|█▊        | 48/266 [00:00<00:03, 56.36it/s]concatenating: train:  21%|██        | 55/266 [00:00<00:03, 58.25it/s]concatenating: train:  23%|██▎       | 61/266 [00:01<00:03, 58.61it/s]concatenating: train:  26%|██▌       | 68/266 [00:01<00:03, 59.03it/s]concatenating: train:  28%|██▊       | 75/266 [00:01<00:03, 60.35it/s]concatenating: train:  30%|███       | 81/266 [00:01<00:03, 59.08it/s]concatenating: train:  33%|███▎      | 87/266 [00:01<00:03, 56.85it/s]concatenating: train:  35%|███▍      | 93/266 [00:01<00:03, 56.09it/s]concatenating: train:  37%|███▋      | 99/266 [00:01<00:02, 56.09it/s]concatenating: train:  39%|███▉      | 105/266 [00:01<00:02, 54.97it/s]concatenating: train:  42%|████▏     | 111/266 [00:01<00:02, 53.41it/s]concatenating: train:  44%|████▍     | 117/266 [00:02<00:02, 52.58it/s]concatenating: train:  46%|████▌     | 123/266 [00:02<00:02, 51.63it/s]concatenating: train:  48%|████▊     | 129/266 [00:02<00:02, 51.01it/s]concatenating: train:  51%|█████     | 135/266 [00:02<00:02, 49.89it/s]concatenating: train:  53%|█████▎    | 141/266 [00:02<00:02, 50.40it/s]concatenating: train:  55%|█████▌    | 147/266 [00:02<00:02, 51.17it/s]concatenating: train:  58%|█████▊    | 153/266 [00:02<00:02, 52.05it/s]concatenating: train:  60%|█████▉    | 159/266 [00:02<00:02, 53.29it/s]concatenating: train:  62%|██████▏   | 165/266 [00:03<00:01, 54.43it/s]concatenating: train:  64%|██████▍   | 171/266 [00:03<00:01, 55.19it/s]concatenating: train:  67%|██████▋   | 177/266 [00:03<00:01, 54.29it/s]concatenating: train:  69%|██████▉   | 183/266 [00:03<00:01, 54.34it/s]concatenating: train:  71%|███████   | 189/266 [00:03<00:01, 54.40it/s]concatenating: train:  73%|███████▎  | 195/266 [00:03<00:01, 54.07it/s]concatenating: train:  76%|███████▌  | 201/266 [00:03<00:01, 53.76it/s]concatenating: train:  78%|███████▊  | 207/266 [00:03<00:01, 53.31it/s]concatenating: train:  80%|████████  | 213/266 [00:03<00:01, 51.62it/s]concatenating: train:  82%|████████▏ | 219/266 [00:04<00:00, 53.48it/s]concatenating: train:  85%|████████▍ | 225/266 [00:04<00:00, 53.46it/s]concatenating: train:  87%|████████▋ | 231/266 [00:04<00:00, 54.32it/s]concatenating: train:  89%|████████▉ | 238/266 [00:04<00:00, 56.74it/s]concatenating: train:  92%|█████████▏| 245/266 [00:04<00:00, 58.55it/s]concatenating: train:  95%|█████████▍| 252/266 [00:04<00:00, 58.11it/s]concatenating: train:  97%|█████████▋| 259/266 [00:04<00:00, 58.75it/s]concatenating: train: 100%|██████████| 266/266 [00:04<00:00, 59.61it/s]concatenating: train: 100%|██████████| 266/266 [00:04<00:00, 55.21it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:11<00:34, 11.41s/it]Loading test:  50%|█████     | 2/4 [00:18<00:20, 10.22s/it]Loading test:  75%|███████▌  | 3/4 [00:26<00:09,  9.34s/it]Loading test: 100%|██████████| 4/4 [00:38<00:00, 10.24s/it]Loading test: 100%|██████████| 4/4 [00:38<00:00,  9.62s/it]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 70.25it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 84, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 84, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 84, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 84, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 84, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 84, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 84, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 42, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 42, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 42, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 42, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 42, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 42, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 42, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 42, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 42, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 21, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 21, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 21, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 21, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 21, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 21, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 21, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 21, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 21, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 21, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 42, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 42, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 42, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 42, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 42, 40)   0           batch_normalization_7[0][0]      2020-01-22 01:48:53.885319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 01:48:53.885428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 01:48:53.885442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 01:48:53.885449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 01:48:53.885752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 42, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 42, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 42, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 42, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 42, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 84, 20)   11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 84, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 84, 20)   7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 84, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 84, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 84, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 84, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 84, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 84, 60)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 84, 60)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 84, 13)   793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2
------------------------------------------------------------------
class_weights [6.33286950e-02 3.28223833e-02 7.67506292e-02 9.53665160e-03
 2.76012413e-02 7.22108677e-03 8.43168259e-02 1.14077175e-01
 8.95730987e-02 1.36092707e-02 2.90413604e-01 1.90489678e-01
 2.59661083e-04]
Train on 9700 samples, validate on 141 samples
Epoch 1/300
 - 25s - loss: 0.7455 - acc: 0.8705 - mDice: 0.1977 - val_loss: 0.3752 - val_acc: 0.9047 - val_mDice: 0.1983

Epoch 00001: val_mDice improved from -inf to 0.19832, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/300
 - 21s - loss: 0.6594 - acc: 0.8954 - mDice: 0.2899 - val_loss: 0.3997 - val_acc: 0.9159 - val_mDice: 0.2356

Epoch 00002: val_mDice improved from 0.19832 to 0.23562, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 3/300
 - 20s - loss: 0.6151 - acc: 0.9058 - mDice: 0.3374 - val_loss: 0.3300 - val_acc: 0.9234 - val_mDice: 0.2543

Epoch 00003: val_mDice improved from 0.23562 to 0.25427, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/300
 - 20s - loss: 0.5855 - acc: 0.9104 - mDice: 0.3694 - val_loss: 0.3523 - val_acc: 0.9264 - val_mDice: 0.2769

Epoch 00004: val_mDice improved from 0.25427 to 0.27686, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/300
 - 20s - loss: 0.5536 - acc: 0.9134 - mDice: 0.4038 - val_loss: 0.3451 - val_acc: 0.9301 - val_mDice: 0.2936

Epoch 00005: val_mDice improved from 0.27686 to 0.29355, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 6/300
 - 21s - loss: 0.5342 - acc: 0.9163 - mDice: 0.4248 - val_loss: 0.3099 - val_acc: 0.9299 - val_mDice: 0.3028

Epoch 00006: val_mDice improved from 0.29355 to 0.30280, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 7/300
 - 20s - loss: 0.5196 - acc: 0.9185 - mDice: 0.4405 - val_loss: 0.2675 - val_acc: 0.9320 - val_mDice: 0.3112

Epoch 00007: val_mDice improved from 0.30280 to 0.31118, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 8/300
 - 20s - loss: 0.5094 - acc: 0.9199 - mDice: 0.4515 - val_loss: 0.2950 - val_acc: 0.9351 - val_mDice: 0.3072

Epoch 00008: val_mDice did not improve from 0.31118
Epoch 9/300
 - 21s - loss: 0.5001 - acc: 0.9214 - mDice: 0.4615 - val_loss: 0.2875 - val_acc: 0.9360 - val_mDice: 0.3151

Epoch 00009: val_mDice improved from 0.31118 to 0.31511, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 10/300
 - 20s - loss: 0.4913 - acc: 0.9222 - mDice: 0.4710 - val_loss: 0.3318 - val_acc: 0.9339 - val_mDice: 0.3232

Epoch 00010: val_mDice improved from 0.31511 to 0.32316, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 11/300
 - 21s - loss: 0.4755 - acc: 0.9230 - mDice: 0.4881 - val_loss: 0.2841 - val_acc: 0.9366 - val_mDice: 0.3351

Epoch 00011: val_mDice improved from 0.32316 to 0.33514, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 12/300
 - 21s - loss: 0.4559 - acc: 0.9241 - mDice: 0.5092 - val_loss: 0.2639 - val_acc: 0.9382 - val_mDice: 0.3347

Epoch 00012: val_mDice did not improve from 0.33514
Epoch 13/300
 - 21s - loss: 0.4435 - acc: 0.9253 - mDice: 0.5227 - val_loss: 0.2527 - val_acc: 0.9372 - val_mDice: 0.3392

Epoch 00013: val_mDice improved from 0.33514 to 0.33924, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 14/300
 - 21s - loss: 0.4387 - acc: 0.9260 - mDice: 0.5279 - val_loss: 0.2506 - val_acc: 0.9374 - val_mDice: 0.3393

Epoch 00014: val_mDice improved from 0.33924 to 0.33932, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 15/300
 - 21s - loss: 0.4309 - acc: 0.9268 - mDice: 0.5363 - val_loss: 0.2399 - val_acc: 0.9392 - val_mDice: 0.3407

Epoch 00015: val_mDice improved from 0.33932 to 0.34070, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 16/300
 - 21s - loss: 0.4244 - acc: 0.9278 - mDice: 0.5433 - val_loss: 0.2283 - val_acc: 0.9387 - val_mDice: 0.3440

Epoch 00016: val_mDice improved from 0.34070 to 0.34398, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 17/300
 - 21s - loss: 0.4230 - acc: 0.9280 - mDice: 0.5448 - val_loss: 0.2252 - val_acc: 0.9383 - val_mDice: 0.3424

Epoch 00017: val_mDice did not improve from 0.34398
Epoch 18/300
 - 21s - loss: 0.4166 - acc: 0.9286 - mDice: 0.5517 - val_loss: 0.2149 - val_acc: 0.9410 - val_mDice: 0.3418

Epoch 00018: val_mDice did not improve from 0.34398
Epoch 19/300
 - 21s - loss: 0.4166 - acc: 0.9290 - mDice: 0.5516 - val_loss: 0.2313 - val_acc: 0.9401 - val_mDice: 0.3455

Epoch 00019: val_mDice improved from 0.34398 to 0.34549, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 20/300
 - 21s - loss: 0.4101 - acc: 0.9297 - mDice: 0.5587 - val_loss: 0.2239 - val_acc: 0.9395 - val_mDice: 0.3442

Epoch 00020: val_mDice did not improve from 0.34549
Epoch 21/300
 - 22s - loss: 0.4069 - acc: 0.9300 - mDice: 0.5621 - val_loss: 0.2216 - val_acc: 0.9396 - val_mDice: 0.3462

Epoch 00021: val_mDice improved from 0.34549 to 0.34619, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 22/300
 - 21s - loss: 0.4078 - acc: 0.9302 - mDice: 0.5611 - val_loss: 0.1869 - val_acc: 0.9406 - val_mDice: 0.3480

Epoch 00022: val_mDice improved from 0.34619 to 0.34800, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 23/300
 - 21s - loss: 0.4021 - acc: 0.9307 - mDice: 0.5673 - val_loss: 0.1663 - val_acc: 0.9420 - val_mDice: 0.3477

Epoch 00023: val_mDice did not improve from 0.34800
Epoch 24/300
 - 21s - loss: 0.3989 - acc: 0.9312 - mDice: 0.5708 - val_loss: 0.2498 - val_acc: 0.9398 - val_mDice: 0.3509

Epoch 00024: val_mDice improved from 0.34800 to 0.35086, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 25/300
 - 22s - loss: 0.3949 - acc: 0.9318 - mDice: 0.5751 - val_loss: 0.2091 - val_acc: 0.9418 - val_mDice: 0.3481

Epoch 00025: val_mDice did not improve from 0.35086
Epoch 26/300
 - 21s - loss: 0.3951 - acc: 0.9321 - mDice: 0.5748 - val_loss: 0.1858 - val_acc: 0.9428 - val_mDice: 0.3491

Epoch 00026: val_mDice did not improve from 0.35086
Epoch 27/300
 - 20s - loss: 0.3932 - acc: 0.9323 - mDice: 0.5769 - val_loss: 0.1945 - val_acc: 0.9443 - val_mDice: 0.3398

Epoch 00027: val_mDice did not improve from 0.35086
Epoch 28/300
 - 20s - loss: 0.3900 - acc: 0.9325 - mDice: 0.5804 - val_loss: 0.1766 - val_acc: 0.9432 - val_mDice: 0.3483

Epoch 00028: val_mDice did not improve from 0.35086
Epoch 29/300
 - 20s - loss: 0.3878 - acc: 0.9330 - mDice: 0.5827 - val_loss: 0.1815 - val_acc: 0.9441 - val_mDice: 0.3546

Epoch 00029: val_mDice improved from 0.35086 to 0.35457, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 30/300
 - 20s - loss: 0.3863 - acc: 0.9330 - mDice: 0.5844 - val_loss: 0.1756 - val_acc: 0.9437 - val_mDice: 0.3504

Epoch 00030: val_mDice did not improve from 0.35457
Epoch 31/300
 - 20s - loss: 0.3840 - acc: 0.9333 - mDice: 0.5868 - val_loss: 0.1975 - val_acc: 0.9431 - val_mDice: 0.3538

Epoch 00031: val_mDice did not improve from 0.35457
Epoch 32/300
 - 21s - loss: 0.3806 - acc: 0.9338 - mDice: 0.5905 - val_loss: 0.1541 - val_acc: 0.9450 - val_mDice: 0.3483

Epoch 00032: val_mDice did not improve from 0.35457
Epoch 33/300
 - 21s - loss: 0.3798 - acc: 0.9340 - mDice: 0.5914 - val_loss: 0.1804 - val_acc: 0.9420 - val_mDice: 0.3553

Epoch 00033: val_mDice improved from 0.35457 to 0.35535, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 34/300
 - 21s - loss: 0.3798 - acc: 0.9341 - mDice: 0.5914 - val_loss: 0.1645 - val_acc: 0.9446 - val_mDice: 0.3489

Epoch 00034: val_mDice did not improve from 0.35535
Epoch 35/300
 - 21s - loss: 0.3755 - acc: 0.9346 - mDice: 0.5959 - val_loss: 0.1736 - val_acc: 0.9453 - val_mDice: 0.3545

Epoch 00035: val_mDice did not improve from 0.35535
Epoch 36/300
 - 20s - loss: 0.3749 - acc: 0.9347 - mDice: 0.5966 - val_loss: 0.1802 - val_acc: 0.9448 - val_mDice: 0.3572

Epoch 00036: val_mDice improved from 0.35535 to 0.35721, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 37/300
 - 20s - loss: 0.3702 - acc: 0.9350 - mDice: 0.6017 - val_loss: 0.1660 - val_acc: 0.9454 - val_mDice: 0.3542

Epoch 00037: val_mDice did not improve from 0.35721
Epoch 38/300
 - 20s - loss: 0.3746 - acc: 0.9350 - mDice: 0.5970 - val_loss: 0.1791 - val_acc: 0.9445 - val_mDice: 0.3585

Epoch 00038: val_mDice improved from 0.35721 to 0.35852, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 39/300
 - 20s - loss: 0.3686 - acc: 0.9353 - mDice: 0.6034 - val_loss: 0.1641 - val_acc: 0.9435 - val_mDice: 0.3589

Epoch 00039: val_mDice improved from 0.35852 to 0.35893, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 40/300
 - 20s - loss: 0.3674 - acc: 0.9355 - mDice: 0.6047 - val_loss: 0.1886 - val_acc: 0.9422 - val_mDice: 0.3607

Epoch 00040: val_mDice improved from 0.35893 to 0.36074, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 41/300
 - 19s - loss: 0.3656 - acc: 0.9358 - mDice: 0.6066 - val_loss: 0.1658 - val_acc: 0.9459 - val_mDice: 0.3540

Epoch 00041: val_mDice did not improve from 0.36074
Epoch 42/300
 - 19s - loss: 0.3664 - acc: 0.9359 - mDice: 0.6058 - val_loss: 0.1598 - val_acc: 0.9464 - val_mDice: 0.3540

Epoch 00042: val_mDice did not improve from 0.36074
Epoch 43/300
 - 20s - loss: 0.3663 - acc: 0.9360 - mDice: 0.6059 - val_loss: 0.1467 - val_acc: 0.9463 - val_mDice: 0.3564

Epoch 00043: val_mDice did not improve from 0.36074
Epoch 44/300
 - 20s - loss: 0.3621 - acc: 0.9363 - mDice: 0.6104 - val_loss: 0.1187 - val_acc: 0.9472 - val_mDice: 0.3509

Epoch 00044: val_mDice did not improve from 0.36074
Epoch 45/300
 - 19s - loss: 0.3631 - acc: 0.9363 - mDice: 0.6094 - val_loss: 0.1751 - val_acc: 0.9455 - val_mDice: 0.3605

Epoch 00045: val_mDice did not improve from 0.36074
Epoch 46/300
 - 19s - loss: 0.3608 - acc: 0.9366 - mDice: 0.6118 - val_loss: 0.1295 - val_acc: 0.9463 - val_mDice: 0.3557

Epoch 00046: val_mDice did not improve from 0.36074
Epoch 47/300
 - 20s - loss: 0.3617 - acc: 0.9367 - mDice: 0.6108 - val_loss: 0.1442 - val_acc: 0.9456 - val_mDice: 0.3590

Epoch 00047: val_mDice did not improve from 0.36074
Epoch 48/300
 - 20s - loss: 0.3577 - acc: 0.9368 - mDice: 0.6152 - val_loss: 0.1398 - val_acc: 0.9462 - val_mDice: 0.3642

Epoch 00048: val_mDice improved from 0.36074 to 0.36417, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 49/300
 - 20s - loss: 0.3559 - acc: 0.9370 - mDice: 0.6171 - val_loss: 0.1462 - val_acc: 0.9463 - val_mDice: 0.3588

Epoch 00049: val_mDice did not improve from 0.36417
Epoch 50/300
 - 20s - loss: 0.3575 - acc: 0.9370 - mDice: 0.6154 - val_loss: 0.1717 - val_acc: 0.9435 - val_mDice: 0.3626

Epoch 00050: val_mDice did not improve from 0.36417
Epoch 51/300
 - 20s - loss: 0.3557 - acc: 0.9373 - mDice: 0.6173 - val_loss: 0.1430 - val_acc: 0.9470 - val_mDice: 0.3601

Epoch 00051: val_mDice did not improve from 0.36417
Epoch 52/300
 - 20s - loss: 0.3561 - acc: 0.9374 - mDice: 0.6168 - val_loss: 0.1671 - val_acc: 0.9435 - val_mDice: 0.3581

Epoch 00052: val_mDice did not improve from 0.36417
Epoch 53/300
 - 20s - loss: 0.3545 - acc: 0.9378 - mDice: 0.6186 - val_loss: 0.1756 - val_acc: 0.9404 - val_mDice: 0.3620

Epoch 00053: val_mDice did not improve from 0.36417
Epoch 54/300
 - 20s - loss: 0.3524 - acc: 0.9378 - mDice: 0.6209 - val_loss: 0.1034 - val_acc: 0.9481 - val_mDice: 0.3557

Epoch 00054: val_mDice did not improve from 0.36417
Epoch 55/300
 - 19s - loss: 0.3501 - acc: 0.9380 - mDice: 0.6233 - val_loss: 0.1250 - val_acc: 0.9472 - val_mDice: 0.3680

Epoch 00055: val_mDice improved from 0.36417 to 0.36799, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 56/300
 - 19s - loss: 0.3518 - acc: 0.9379 - mDice: 0.6215 - val_loss: 0.1275 - val_acc: 0.9473 - val_mDice: 0.3656

Epoch 00056: val_mDice did not improve from 0.36799
Epoch 57/300
 - 20s - loss: 0.3497 - acc: 0.9381 - mDice: 0.6238 - val_loss: 0.1576 - val_acc: 0.9462 - val_mDice: 0.3682

Epoch 00057: val_mDice improved from 0.36799 to 0.36822, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 58/300
 - 20s - loss: 0.3518 - acc: 0.9384 - mDice: 0.6215 - val_loss: 0.1492 - val_acc: 0.9453 - val_mDice: 0.3655

Epoch 00058: val_mDice did not improve from 0.36822
Epoch 59/300
 - 19s - loss: 0.3468 - acc: 0.9384 - mDice: 0.6269 - val_loss: 0.1100 - val_acc: 0.9480 - val_mDice: 0.3611

Epoch 00059: val_mDice did not improve from 0.36822
Epoch 60/300
 - 19s - loss: 0.3472 - acc: 0.9386 - mDice: 0.6265 - val_loss: 0.1310 - val_acc: 0.9474 - val_mDice: 0.3617

Epoch 00060: val_mDice did not improve from 0.36822
Epoch 61/300
 - 20s - loss: 0.3459 - acc: 0.9387 - mDice: 0.6279 - val_loss: 0.1356 - val_acc: 0.9463 - val_mDice: 0.3683

Epoch 00061: val_mDice improved from 0.36822 to 0.36835, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 62/300
 - 20s - loss: 0.3457 - acc: 0.9388 - mDice: 0.6280 - val_loss: 0.1504 - val_acc: 0.9474 - val_mDice: 0.3626

Epoch 00062: val_mDice did not improve from 0.36835
Epoch 63/300
 - 20s - loss: 0.3432 - acc: 0.9390 - mDice: 0.6308 - val_loss: 0.1618 - val_acc: 0.9448 - val_mDice: 0.3637

Epoch 00063: val_mDice did not improve from 0.36835
Epoch 64/300
 - 20s - loss: 0.3416 - acc: 0.9390 - mDice: 0.6325 - val_loss: 0.1282 - val_acc: 0.9465 - val_mDice: 0.3646

Epoch 00064: val_mDice did not improve from 0.36835
Epoch 65/300
 - 20s - loss: 0.3414 - acc: 0.9393 - mDice: 0.6327 - val_loss: 0.1435 - val_acc: 0.9478 - val_mDice: 0.3597

Epoch 00065: val_mDice did not improve from 0.36835
Epoch 66/300
 - 21s - loss: 0.3397 - acc: 0.9392 - mDice: 0.6346 - val_loss: 0.1678 - val_acc: 0.9445 - val_mDice: 0.3690

Epoch 00066: val_mDice improved from 0.36835 to 0.36902, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 67/300
 - 20s - loss: 0.3399 - acc: 0.9393 - mDice: 0.6344 - val_loss: 0.1067 - val_acc: 0.9484 - val_mDice: 0.3639

Epoch 00067: val_mDice did not improve from 0.36902
Epoch 68/300
 - 21s - loss: 0.3388 - acc: 0.9396 - mDice: 0.6355 - val_loss: 0.1522 - val_acc: 0.9458 - val_mDice: 0.3654

Epoch 00068: val_mDice did not improve from 0.36902
Epoch 69/300
 - 21s - loss: 0.3386 - acc: 0.9396 - mDice: 0.6355 - val_loss: 0.1494 - val_acc: 0.9484 - val_mDice: 0.3649

Epoch 00069: val_mDice did not improve from 0.36902
Epoch 70/300
 - 20s - loss: 0.3376 - acc: 0.9396 - mDice: 0.6368 - val_loss: 0.1138 - val_acc: 0.9486 - val_mDice: 0.3666

Epoch 00070: val_mDice did not improve from 0.36902
Epoch 71/300
 - 20s - loss: 0.3369 - acc: 0.9398 - mDice: 0.6376 - val_loss: 0.1222 - val_acc: 0.9482 - val_mDice: 0.3673

Epoch 00071: val_mDice did not improve from 0.36902
Epoch 72/300
 - 20s - loss: 0.3366 - acc: 0.9398 - mDice: 0.6379 - val_loss: 0.1391 - val_acc: 0.9479 - val_mDice: 0.3618

Epoch 00072: val_mDice did not improve from 0.36902
Epoch 73/300
 - 21s - loss: 0.3382 - acc: 0.9400 - mDice: 0.6362 - val_loss: 0.1481 - val_acc: 0.9464 - val_mDice: 0.3670

Epoch 00073: val_mDice did not improve from 0.36902
Epoch 74/300
 - 21s - loss: 0.3364 - acc: 0.9400 - mDice: 0.6381 - val_loss: 0.1460 - val_acc: 0.9481 - val_mDice: 0.3687

Epoch 00074: val_mDice did not improve from 0.36902
Epoch 75/300
 - 21s - loss: 0.3328 - acc: 0.9402 - mDice: 0.6420 - val_loss: 0.1453 - val_acc: 0.9484 - val_mDice: 0.3701

Epoch 00075: val_mDice improved from 0.36902 to 0.37008, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 76/300
 - 21s - loss: 0.3335 - acc: 0.9404 - mDice: 0.6412 - val_loss: 0.1381 - val_acc: 0.9480 - val_mDice: 0.3670

Epoch 00076: val_mDice did not improve from 0.37008
Epoch 77/300
 - 21s - loss: 0.3375 - acc: 0.9403 - mDice: 0.6369 - val_loss: 0.1134 - val_acc: 0.9485 - val_mDice: 0.3688

Epoch 00077: val_mDice did not improve from 0.37008
Epoch 78/300
 - 21s - loss: 0.3343 - acc: 0.9405 - mDice: 0.6404 - val_loss: 0.1556 - val_acc: 0.9476 - val_mDice: 0.3732

Epoch 00078: val_mDice improved from 0.37008 to 0.37324, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 79/300
 - 21s - loss: 0.3351 - acc: 0.9404 - mDice: 0.6395 - val_loss: 0.1318 - val_acc: 0.9484 - val_mDice: 0.3601

Epoch 00079: val_mDice did not improve from 0.37324
Epoch 80/300
 - 20s - loss: 0.3335 - acc: 0.9404 - mDice: 0.6413 - val_loss: 0.1411 - val_acc: 0.9494 - val_mDice: 0.3654

Epoch 00080: val_mDice did not improve from 0.37324
Epoch 81/300
 - 21s - loss: 0.3328 - acc: 0.9406 - mDice: 0.6419 - val_loss: 0.1675 - val_acc: 0.9473 - val_mDice: 0.3695

Epoch 00081: val_mDice did not improve from 0.37324
Epoch 82/300
 - 21s - loss: 0.3318 - acc: 0.9407 - mDice: 0.6430 - val_loss: 0.1541 - val_acc: 0.9459 - val_mDice: 0.3718

Epoch 00082: val_mDice did not improve from 0.37324
Epoch 83/300
 - 21s - loss: 0.3292 - acc: 0.9409 - mDice: 0.6458 - val_loss: 0.1680 - val_acc: 0.9492 - val_mDice: 0.3685

Epoch 00083: val_mDice did not improve from 0.37324
Epoch 84/300
 - 20s - loss: 0.3296 - acc: 0.9409 - mDice: 0.6454 - val_loss: 0.1473 - val_acc: 0.9465 - val_mDice: 0.3732

Epoch 00084: val_mDice did not improve from 0.37324
Epoch 85/300
 - 20s - loss: 0.3275 - acc: 0.9411 - mDice: 0.6477 - val_loss: 0.1559 - val_acc: 0.9490 - val_mDice: 0.3701

Epoch 00085: val_mDice did not improve from 0.37324
Epoch 86/300
 - 21s - loss: 0.3305 - acc: 0.9412 - mDice: 0.6444 - val_loss: 0.1613 - val_acc: 0.9490 - val_mDice: 0.3688

Epoch 00086: val_mDice did not improve from 0.37324
Epoch 87/300
 - 21s - loss: 0.3275 - acc: 0.9411 - mDice: 0.6477 - val_loss: 0.1455 - val_acc: 0.9475 - val_mDice: 0.3666

Epoch 00087: val_mDice did not improve from 0.37324
Epoch 88/300
 - 20s - loss: 0.3248 - acc: 0.9412 - mDice: 0.6506 - val_loss: 0.1229 - val_acc: 0.9486 - val_mDice: 0.3703

Epoch 00088: val_mDice did not improve from 0.37324
Epoch 89/300
 - 20s - loss: 0.3234 - acc: 0.9414 - mDice: 0.6521 - val_loss: 0.1329 - val_acc: 0.9465 - val_mDice: 0.3668

Epoch 00089: val_mDice did not improve from 0.37324
Epoch 90/300
 - 20s - loss: 0.3260 - acc: 0.9415 - mDice: 0.6492 - val_loss: 0.1443 - val_acc: 0.9494 - val_mDice: 0.3707

Epoch 00090: val_mDice did not improve from 0.37324
Epoch 91/300
 - 20s - loss: 0.3256 - acc: 0.9416 - mDice: 0.6497 - val_loss: 0.1497 - val_acc: 0.9481 - val_mDice: 0.3678

Epoch 00091: val_mDice did not improve from 0.37324
Epoch 92/300
 - 20s - loss: 0.3251 - acc: 0.9414 - mDice: 0.6503 - val_loss: 0.1588 - val_acc: 0.9472 - val_mDice: 0.3709

Epoch 00092: val_mDice did not improve from 0.37324
Epoch 93/300
 - 19s - loss: 0.3257 - acc: 0.9416 - mDice: 0.6496 - val_loss: 0.1117 - val_acc: 0.9499 - val_mDice: 0.3717

Epoch 00093: val_mDice did not improve from 0.37324

Epoch 00093: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 94/300
 - 20s - loss: 0.3237 - acc: 0.9419 - mDice: 0.6517 - val_loss: 0.1400 - val_acc: 0.9491 - val_mDice: 0.3644

Epoch 00094: val_mDice did not improve from 0.37324
Epoch 95/300
 - 20s - loss: 0.3261 - acc: 0.9418 - mDice: 0.6492 - val_loss: 0.1359 - val_acc: 0.9486 - val_mDice: 0.3680

Epoch 00095: val_mDice did not improve from 0.37324
Epoch 96/300
 - 20s - loss: 0.3207 - acc: 0.9420 - mDice: 0.6550 - val_loss: 0.1571 - val_acc: 0.9484 - val_mDice: 0.3689

Epoch 00096: val_mDice did not improve from 0.37324
Epoch 97/300
 - 20s - loss: 0.3213 - acc: 0.9420 - mDice: 0.6544 - val_loss: 0.1420 - val_acc: 0.9491 - val_mDice: 0.3660

Epoch 00097: val_mDice did not improve from 0.37324
Epoch 98/300
 - 21s - loss: 0.3223 - acc: 0.9420 - mDice: 0.6533 - val_loss: 0.1446 - val_acc: 0.9494 - val_mDice: 0.3709

Epoch 00098: val_mDice did not improve from 0.37324
Epoch 99/300
 - 21s - loss: 0.3212 - acc: 0.9421 - mDice: 0.6544 - val_loss: 0.1117 - val_acc: 0.9495 - val_mDice: 0.3724

Epoch 00099: val_mDice did not improve from 0.37324
Epoch 100/300
 - 20s - loss: 0.3205 - acc: 0.9423 - mDice: 0.6552 - val_loss: 0.1317 - val_acc: 0.9490 - val_mDice: 0.3668

Epoch 00100: val_mDice did not improve from 0.37324
Epoch 101/300
 - 21s - loss: 0.3202 - acc: 0.9420 - mDice: 0.6555 - val_loss: 0.1409 - val_acc: 0.9489 - val_mDice: 0.3667

Epoch 00101: val_mDice did not improve from 0.37324
Epoch 102/300
 - 22s - loss: 0.3199 - acc: 0.9423 - mDice: 0.6559 - val_loss: 0.1443 - val_acc: 0.9490 - val_mDice: 0.3704

Epoch 00102: val_mDice did not improve from 0.37324
Epoch 103/300
 - 21s - loss: 0.3214 - acc: 0.9422 - mDice: 0.6542 - val_loss: 0.1327 - val_acc: 0.9491 - val_mDice: 0.3665

Epoch 00103: val_mDice did not improve from 0.37324
Epoch 104/300
 - 21s - loss: 0.3182 - acc: 0.9425 - mDice: 0.6577 - val_loss: 0.1487 - val_acc: 0.9485 - val_mDice: 0.3696

Epoch 00104: val_mDice did not improve from 0.37324
Epoch 105/300
 - 20s - loss: 0.3191 - acc: 0.9424 - mDice: 0.6568 - val_loss: 0.1611 - val_acc: 0.9485 - val_mDice: 0.3748

Epoch 00105: val_mDice improved from 0.37324 to 0.37476, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 106/300
 - 21s - loss: 0.3178 - acc: 0.9423 - mDice: 0.6582 - val_loss: 0.1209 - val_acc: 0.9489 - val_mDice: 0.3726

Epoch 00106: val_mDice did not improve from 0.37476
Epoch 107/300
 - 21s - loss: 0.3187 - acc: 0.9423 - mDice: 0.6571 - val_loss: 0.1560 - val_acc: 0.9487 - val_mDice: 0.3698

Epoch 00107: val_mDice did not improve from 0.37476
Epoch 108/300
 - 21s - loss: 0.3162 - acc: 0.9426 - mDice: 0.6599 - val_loss: 0.0970 - val_acc: 0.9496 - val_mDice: 0.3628

Epoch 00108: val_mDice did not improve from 0.37476
Epoch 109/300
 - 22s - loss: 0.3178 - acc: 0.9425 - mDice: 0.6582 - val_loss: 0.1533 - val_acc: 0.9488 - val_mDice: 0.3729

Epoch 00109: val_mDice did not improve from 0.37476
Epoch 110/300
 - 22s - loss: 0.3184 - acc: 0.9425 - mDice: 0.6574 - val_loss: 0.1327 - val_acc: 0.9484 - val_mDice: 0.3715

Epoch 00110: val_mDice did not improve from 0.37476
Epoch 111/300
 - 22s - loss: 0.3176 - acc: 0.9425 - mDice: 0.6583 - val_loss: 0.1442 - val_acc: 0.9493 - val_mDice: 0.3708

Epoch 00111: val_mDice did not improve from 0.37476
Epoch 112/300
 - 22s - loss: 0.3140 - acc: 0.9427 - mDice: 0.6622 - val_loss: 0.1529 - val_acc: 0.9485 - val_mDice: 0.3733

Epoch 00112: val_mDice did not improve from 0.37476
Epoch 113/300
 - 21s - loss: 0.3181 - acc: 0.9426 - mDice: 0.6578 - val_loss: 0.1280 - val_acc: 0.9489 - val_mDice: 0.3670

Epoch 00113: val_mDice did not improve from 0.37476
Epoch 114/300
 - 21s - loss: 0.3164 - acc: 0.9426 - mDice: 0.6597 - val_loss: 0.1344 - val_acc: 0.9487 - val_mDice: 0.3697

Epoch 00114: val_mDice did not improve from 0.37476
Epoch 115/300
 - 21s - loss: 0.3173 - acc: 0.9427 - mDice: 0.6587 - val_loss: 0.1329 - val_acc: 0.9491 - val_mDice: 0.3713

Epoch 00115: val_mDice did not improve from 0.37476
Epoch 116/300
 - 20s - loss: 0.3180 - acc: 0.9427 - mDice: 0.6579 - val_loss: 0.1214 - val_acc: 0.9498 - val_mDice: 0.3692

Epoch 00116: val_mDice did not improve from 0.37476
Epoch 117/300
 - 20s - loss: 0.3158 - acc: 0.9428 - mDice: 0.6602 - val_loss: 0.1316 - val_acc: 0.9486 - val_mDice: 0.3730

Epoch 00117: val_mDice did not improve from 0.37476
Epoch 118/300
 - 20s - loss: 0.3160 - acc: 0.9427 - mDice: 0.6600 - val_loss: 0.1167 - val_acc: 0.9496 - val_mDice: 0.3654

Epoch 00118: val_mDice did not improve from 0.37476
Epoch 119/300
 - 19s - loss: 0.3171 - acc: 0.9429 - mDice: 0.6588 - val_loss: 0.1217 - val_acc: 0.9496 - val_mDice: 0.3715

Epoch 00119: val_mDice did not improve from 0.37476
Epoch 120/300
 - 19s - loss: 0.3140 - acc: 0.9429 - mDice: 0.6622 - val_loss: 0.1289 - val_acc: 0.9490 - val_mDice: 0.3747

Epoch 00120: val_mDice did not improve from 0.37476

Epoch 00120: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 121/300
 - 20s - loss: 0.3146 - acc: 0.9431 - mDice: 0.6615 - val_loss: 0.1321 - val_acc: 0.9493 - val_mDice: 0.3724

Epoch 00121: val_mDice did not improve from 0.37476
Epoch 122/300
 - 19s - loss: 0.3120 - acc: 0.9429 - mDice: 0.6643 - val_loss: 0.1321 - val_acc: 0.9496 - val_mDice: 0.3716

Epoch 00122: val_mDice did not improve from 0.37476
Epoch 123/300
 - 19s - loss: 0.3130 - acc: 0.9431 - mDice: 0.6633 - val_loss: 0.1596 - val_acc: 0.9486 - val_mDice: 0.3730

Epoch 00123: val_mDice did not improve from 0.37476
Epoch 124/300
 - 19s - loss: 0.3149 - acc: 0.9430 - mDice: 0.6613 - val_loss: 0.1517 - val_acc: 0.9465 - val_mDice: 0.3747

Epoch 00124: val_mDice did not improve from 0.37476
Epoch 125/300
 - 20s - loss: 0.3128 - acc: 0.9430 - mDice: 0.6635 - val_loss: 0.1317 - val_acc: 0.9493 - val_mDice: 0.3724

Epoch 00125: val_mDice did not improve from 0.37476
Epoch 126/300
 - 20s - loss: 0.3144 - acc: 0.9432 - mDice: 0.6617 - val_loss: 0.1298 - val_acc: 0.9490 - val_mDice: 0.3738

Epoch 00126: val_mDice did not improve from 0.37476
Epoch 127/300
 - 21s - loss: 0.3159 - acc: 0.9430 - mDice: 0.6602 - val_loss: 0.1226 - val_acc: 0.9486 - val_mDice: 0.3727

Epoch 00127: val_mDice did not improve from 0.37476
Epoch 128/300
 - 21s - loss: 0.3133 - acc: 0.9431 - mDice: 0.6629 - val_loss: 0.1224 - val_acc: 0.9492 - val_mDice: 0.3708

Epoch 00128: val_mDice did not improve from 0.37476
Epoch 129/300
 - 21s - loss: 0.3119 - acc: 0.9432 - mDice: 0.6644 - val_loss: 0.1345 - val_acc: 0.9493 - val_mDice: 0.3735

Epoch 00129: val_mDice did not improve from 0.37476
Epoch 130/300
 - 20s - loss: 0.3135 - acc: 0.9431 - mDice: 0.6626 - val_loss: 0.1294 - val_acc: 0.9495 - val_mDice: 0.3742

Epoch 00130: val_mDice did not improve from 0.37476
Epoch 131/300
 - 19s - loss: 0.3143 - acc: 0.9431 - mDice: 0.6619 - val_loss: 0.1258 - val_acc: 0.9495 - val_mDice: 0.3709

Epoch 00131: val_mDice did not improve from 0.37476
Epoch 132/300
 - 20s - loss: 0.3127 - acc: 0.9432 - mDice: 0.6636 - val_loss: 0.1322 - val_acc: 0.9491 - val_mDice: 0.3719

Epoch 00132: val_mDice did not improve from 0.37476
Epoch 133/300
 - 19s - loss: 0.3137 - acc: 0.9431 - mDice: 0.6625 - val_loss: 0.1295 - val_acc: 0.9493 - val_mDice: 0.3749

Epoch 00133: val_mDice improved from 0.37476 to 0.37492, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 134/300
 - 19s - loss: 0.3111 - acc: 0.9431 - mDice: 0.6653 - val_loss: 0.1302 - val_acc: 0.9497 - val_mDice: 0.3702

Epoch 00134: val_mDice did not improve from 0.37492
Epoch 135/300
 - 20s - loss: 0.3102 - acc: 0.9433 - mDice: 0.6663 - val_loss: 0.1363 - val_acc: 0.9496 - val_mDice: 0.3711

Epoch 00135: val_mDice did not improve from 0.37492

Epoch 00135: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 136/300
 - 19s - loss: 0.3152 - acc: 0.9432 - mDice: 0.6609 - val_loss: 0.1306 - val_acc: 0.9491 - val_mDice: 0.3718

Epoch 00136: val_mDice did not improve from 0.37492
Epoch 137/300
 - 19s - loss: 0.3136 - acc: 0.9433 - mDice: 0.6626 - val_loss: 0.1293 - val_acc: 0.9495 - val_mDice: 0.3735

Epoch 00137: val_mDice did not improve from 0.37492
Epoch 138/300
 - 20s - loss: 0.3161 - acc: 0.9432 - mDice: 0.6599 - val_loss: 0.1438 - val_acc: 0.9491 - val_mDice: 0.3711

Epoch 00138: val_mDice did not improve from 0.37492
Epoch 139/300
 - 19s - loss: 0.3121 - acc: 0.9435 - mDice: 0.6643 - val_loss: 0.1312 - val_acc: 0.9487 - val_mDice: 0.3730

Epoch 00139: val_mDice did not improve from 0.37492
Epoch 140/300
 - 19s - loss: 0.3124 - acc: 0.9434 - mDice: 0.6640 - val_loss: 0.1348 - val_acc: 0.9494 - val_mDice: 0.3696

Epoch 00140: val_mDice did not improve from 0.37492
Epoch 141/300
 - 20s - loss: 0.3123 - acc: 0.9433 - mDice: 0.6641 - val_loss: 0.1321 - val_acc: 0.9492 - val_mDice: 0.3720

Epoch 00141: val_mDice did not improve from 0.37492
Epoch 142/300
 - 20s - loss: 0.3122 - acc: 0.9433 - mDice: 0.6640 - val_loss: 0.1338 - val_acc: 0.9493 - val_mDice: 0.3702

Epoch 00142: val_mDice did not improve from 0.37492
Epoch 143/300
 - 20s - loss: 0.3132 - acc: 0.9433 - mDice: 0.6629 - val_loss: 0.1325 - val_acc: 0.9490 - val_mDice: 0.3716

Epoch 00143: val_mDice did not improve from 0.37492
Epoch 144/300
 - 19s - loss: 0.3117 - acc: 0.9431 - mDice: 0.6647 - val_loss: 0.1458 - val_acc: 0.9491 - val_mDice: 0.3690

Epoch 00144: val_mDice did not improve from 0.37492
Epoch 145/300
 - 20s - loss: 0.3096 - acc: 0.9434 - mDice: 0.6669 - val_loss: 0.1440 - val_acc: 0.9494 - val_mDice: 0.3691

Epoch 00145: val_mDice did not improve from 0.37492
Epoch 146/300
 - 20s - loss: 0.3106 - acc: 0.9435 - mDice: 0.6659 - val_loss: 0.1309 - val_acc: 0.9492 - val_mDice: 0.3734

Epoch 00146: val_mDice did not improve from 0.37492
Epoch 147/300
 - 19s - loss: 0.3114 - acc: 0.9434 - mDice: 0.6650 - val_loss: 0.1337 - val_acc: 0.9493 - val_mDice: 0.3717

Epoch 00147: val_mDice did not improve from 0.37492
Epoch 148/300
 - 19s - loss: 0.3111 - acc: 0.9435 - mDice: 0.6653 - val_loss: 0.1405 - val_acc: 0.9496 - val_mDice: 0.3728

Epoch 00148: val_mDice did not improve from 0.37492
Epoch 149/300
 - 19s - loss: 0.3110 - acc: 0.9435 - mDice: 0.6654 - val_loss: 0.1321 - val_acc: 0.9490 - val_mDice: 0.3720

Epoch 00149: val_mDice did not improve from 0.37492
Epoch 150/300
 - 20s - loss: 0.3102 - acc: 0.9434 - mDice: 0.6663 - val_loss: 0.1333 - val_acc: 0.9495 - val_mDice: 0.3709

Epoch 00150: val_mDice did not improve from 0.37492

Epoch 00150: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.
Epoch 151/300
 - 19s - loss: 0.3093 - acc: 0.9434 - mDice: 0.6673 - val_loss: 0.1340 - val_acc: 0.9493 - val_mDice: 0.3700

Epoch 00151: val_mDice did not improve from 0.37492
Epoch 152/300
 - 19s - loss: 0.3095 - acc: 0.9435 - mDice: 0.6670 - val_loss: 0.1327 - val_acc: 0.9492 - val_mDice: 0.3714

Epoch 00152: val_mDice did not improve from 0.37492
Epoch 153/300
 - 19s - loss: 0.3103 - acc: 0.9434 - mDice: 0.6661 - val_loss: 0.1333 - val_acc: 0.9494 - val_mDice: 0.3708

Epoch 00153: val_mDice did not improve from 0.37492
Epoch 154/300
 - 20s - loss: 0.3112 - acc: 0.9434 - mDice: 0.6652 - val_loss: 0.1329 - val_acc: 0.9491 - val_mDice: 0.3712

Epoch 00154: val_mDice did not improve from 0.37492
Epoch 155/300
 - 20s - loss: 0.3103 - acc: 0.9435 - mDice: 0.6662 - val_loss: 0.1323 - val_acc: 0.9494 - val_mDice: 0.3718

Epoch 00155: val_mDice did not improve from 0.37492
Epoch 156/300
 - 20s - loss: 0.3098 - acc: 0.9435 - mDice: 0.6666 - val_loss: 0.1334 - val_acc: 0.9494 - val_mDice: 0.3706

Epoch 00156: val_mDice did not improve from 0.37492
Epoch 157/300
 - 20s - loss: 0.3099 - acc: 0.9435 - mDice: 0.6666 - val_loss: 0.1318 - val_acc: 0.9491 - val_mDice: 0.3724

Epoch 00157: val_mDice did not improve from 0.37492
Epoch 158/300
 - 20s - loss: 0.3119 - acc: 0.9435 - mDice: 0.6645 - val_loss: 0.1312 - val_acc: 0.9494 - val_mDice: 0.3705

Epoch 00158: val_mDice did not improve from 0.37492
Epoch 159/300
 - 20s - loss: 0.3127 - acc: 0.9434 - mDice: 0.6636 - val_loss: 0.1333 - val_acc: 0.9496 - val_mDice: 0.3708

Epoch 00159: val_mDice did not improve from 0.37492
Epoch 160/300
 - 20s - loss: 0.3092 - acc: 0.9435 - mDice: 0.6674 - val_loss: 0.1317 - val_acc: 0.9493 - val_mDice: 0.3725

Epoch 00160: val_mDice did not improve from 0.37492
Epoch 161/300
 - 19s - loss: 0.3108 - acc: 0.9434 - mDice: 0.6656 - val_loss: 0.1321 - val_acc: 0.9493 - val_mDice: 0.3721

Epoch 00161: val_mDice did not improve from 0.37492
Epoch 162/300
 - 19s - loss: 0.3101 - acc: 0.9434 - mDice: 0.6664 - val_loss: 0.1316 - val_acc: 0.9489 - val_mDice: 0.3726

Epoch 00162: val_mDice did not improve from 0.37492
Epoch 163/300
 - 20s - loss: 0.3116 - acc: 0.9434 - mDice: 0.6648 - val_loss: 0.1333 - val_acc: 0.9493 - val_mDice: 0.3707

Epoch 00163: val_mDice did not improve from 0.37492
Epoch 164/300
 - 19s - loss: 0.3120 - acc: 0.9436 - mDice: 0.6644 - val_loss: 0.1332 - val_acc: 0.9492 - val_mDice: 0.3708

Epoch 00164: val_mDice did not improve from 0.37492
Epoch 165/300
 - 19s - loss: 0.3115 - acc: 0.9435 - mDice: 0.6648 - val_loss: 0.1344 - val_acc: 0.9496 - val_mDice: 0.3695

Epoch 00165: val_mDice did not improve from 0.37492

Epoch 00165: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.
Epoch 166/300
 - 19s - loss: 0.3101 - acc: 0.9435 - mDice: 0.6663 - val_loss: 0.1334 - val_acc: 0.9495 - val_mDice: 0.3706

Epoch 00166: val_mDice did not improve from 0.37492
Epoch 167/300
 - 20s - loss: 0.3106 - acc: 0.9434 - mDice: 0.6659 - val_loss: 0.1322 - val_acc: 0.9494 - val_mDice: 0.3719

Epoch 00167: val_mDice did not improve from 0.37492
Epoch 168/300
 - 19s - loss: 0.3097 - acc: 0.9435 - mDice: 0.6669 - val_loss: 0.1323 - val_acc: 0.9493 - val_mDice: 0.3718

Epoch 00168: val_mDice did not improve from 0.37492
Epoch 169/300
 - 19s - loss: 0.3079 - acc: 0.9435 - mDice: 0.6689 - val_loss: 0.1322 - val_acc: 0.9493 - val_mDice: 0.3719

Epoch 00169: val_mDice did not improve from 0.37492
Epoch 170/300
 - 20s - loss: 0.3095 - acc: 0.9435 - mDice: 0.6670 - val_loss: 0.1318 - val_acc: 0.9491 - val_mDice: 0.3724

Epoch 00170: val_mDice did not improve from 0.37492
Epoch 171/300
 - 18s - loss: 0.3113 - acc: 0.9435 - mDice: 0.6650 - val_loss: 0.1329 - val_acc: 0.9492 - val_mDice: 0.3711

Epoch 00171: val_mDice did not improve from 0.37492
Epoch 172/300
 - 19s - loss: 0.3117 - acc: 0.9435 - mDice: 0.6646 - val_loss: 0.1333 - val_acc: 0.9490 - val_mDice: 0.3707

Epoch 00172: val_mDice did not improve from 0.37492
Epoch 173/300
 - 20s - loss: 0.3106 - acc: 0.9435 - mDice: 0.6658 - val_loss: 0.1321 - val_acc: 0.9493 - val_mDice: 0.3720

Epoch 00173: val_mDice did not improve from 0.37492
Restoring model weights from the end of the best epoch
Epoch 00173: early stopping
{'val_loss': [0.375158983447873, 0.39973252018292743, 0.3300224285810552, 0.3523441863820908, 0.3451195713793132, 0.30992928607032655, 0.26746501689050217, 0.29503892605186355, 0.2874949887289223, 0.33177069273400817, 0.28406487141095155, 0.26386175896470426, 0.25269059762887075, 0.2505811744547905, 0.23986254938950774, 0.2283391406138738, 0.22522717312718113, 0.214853214033952, 0.23128676625853734, 0.2238753651473539, 0.22157198592280666, 0.18686800582189086, 0.16629653957718654, 0.2498311604379762, 0.20911113928395805, 0.18576930357632063, 0.19448085372329604, 0.17658070067987375, 0.18147620822943694, 0.17559360918846537, 0.1975379136014492, 0.1541133723783155, 0.18042422640830913, 0.1645123321325221, 0.17360903660879068, 0.18021776395063874, 0.16597632841860993, 0.17908158528466597, 0.16410238586419018, 0.18863112970869592, 0.16584703442475474, 0.1598215401172638, 0.1466602585839887, 0.11870284156596407, 0.17506078699379102, 0.12947505103516663, 0.14424048068252862, 0.13977744446155874, 0.146151657645584, 0.17171633867084557, 0.14298759950390943, 0.1670767199908588, 0.17556629662818096, 0.10337445615453923, 0.12502785604975536, 0.1275150125965159, 0.15759789373012298, 0.14921994147993994, 0.11000126905422261, 0.13101514106523907, 0.13558536630573, 0.15043132398145417, 0.16182279671337588, 0.1282296471878992, 0.1435268922107862, 0.16784864188508786, 0.10670963682709857, 0.15220008124696446, 0.14942084955619583, 0.11383709627870761, 0.12219573031030992, 0.1391068583895974, 0.14805992465492682, 0.14599422212188126, 0.14530419531866168, 0.13806053953813324, 0.11343881842541567, 0.15559637007561136, 0.1318055689995382, 0.14114253402601742, 0.1674807665618599, 0.15411472378681737, 0.16795434448735932, 0.14727721157226156, 0.1559352031413545, 0.16127655449065756, 0.1455127439388992, 0.12289537826605829, 0.13291345441595037, 0.14429292959947113, 0.14967787318618586, 0.15878158120821553, 0.11168447553992589, 0.13999124939044846, 0.135859035774537, 0.15706267942350807, 0.14200325521594243, 0.14461779259871824, 0.111737533664027, 0.13167257765506177, 0.14086440495043623, 0.14432563454388303, 0.13273900985723552, 0.1486579975549211, 0.16109199304107233, 0.12089279960471054, 0.15603966028132338, 0.0969712681381415, 0.15334060925541196, 0.13269113701708773, 0.14419732618323983, 0.15285367946675482, 0.12800304729042325, 0.13438280281802337, 0.13285650034164284, 0.12135851773427608, 0.13155206814672507, 0.1166787224537726, 0.12166807514570772, 0.12886710571957397, 0.132145267785758, 0.13205190150873036, 0.15955192421345002, 0.15172381635676038, 0.13173745087750838, 0.12983118858954584, 0.12261649099647576, 0.12239570175925045, 0.1344673883166587, 0.12943108633477637, 0.12581170154801496, 0.13215476169468873, 0.12945045784432838, 0.13019271955845205, 0.13632466661222015, 0.13061557081197753, 0.12933990721267166, 0.1438452323687309, 0.1312125172162204, 0.13482210781682838, 0.13210740863140805, 0.133809364465204, 0.13254932578362472, 0.14580369290915576, 0.14395705206775444, 0.1308619150204967, 0.13374282131037601, 0.14053400739340494, 0.13213982975361407, 0.13326457990267387, 0.13396783083419692, 0.1327417168819101, 0.13325688574159295, 0.13290395540456082, 0.13232751157554196, 0.13337522693199655, 0.13181449158995656, 0.1312029913549992, 0.13332103052612473, 0.1316577492988543, 0.13206083647550104, 0.13164175778019724, 0.13332767084782535, 0.13323702181999883, 0.13439856849095605, 0.13340510307078032, 0.13222077027201018, 0.13229859099188385, 0.13219766739544506, 0.1317934577108909, 0.13292486901256315, 0.13330003588149944, 0.13207192428357212], 'val_acc': [0.9047407977124478, 0.9159441353581476, 0.9234276333599226, 0.9263599832007225, 0.9301496380609824, 0.9298508843631609, 0.9319616581531281, 0.9350661216898167, 0.9359591379233286, 0.9339409119693945, 0.9366329536370351, 0.9381673293756255, 0.937212606693836, 0.9374447900352748, 0.9391853767083892, 0.9386674165725708, 0.9383361922933701, 0.9410493449961885, 0.9401059936969838, 0.9395198475384543, 0.9396091488236231, 0.9405654863262853, 0.9420073100015627, 0.9398218486325961, 0.9418303307066572, 0.9428142850280653, 0.9442983138645794, 0.9431779748159097, 0.9441359482758435, 0.9436829445209909, 0.9431487516308508, 0.9449932448407437, 0.9419586117385973, 0.9445662134928061, 0.9453001326703011, 0.9448048983905333, 0.9454007926562153, 0.9444525588488748, 0.9434588691021534, 0.9422151484387986, 0.9459057369976179, 0.9463766136913435, 0.9462710669700135, 0.9471592226772444, 0.9454738442779432, 0.9462889356816069, 0.9456394733266628, 0.9462353591377853, 0.9462743177481577, 0.9435384349620088, 0.9469822391550592, 0.9434751145383145, 0.9403609113490328, 0.9480603645879326, 0.9471819569878545, 0.9472939968109131, 0.9462385930068103, 0.9452984924857498, 0.9480197784748483, 0.9474238757546066, 0.9463327725728353, 0.9474433635143523, 0.9448308834792875, 0.9464578028266312, 0.9478151865884767, 0.9444899068656543, 0.94841919300404, 0.9458424208012033, 0.9483867148135571, 0.9486140452378186, 0.9482146095722279, 0.9479320835559926, 0.9464464356713261, 0.9480717317432377, 0.948401336974286, 0.94799054260795, 0.9485198614445138, 0.9476317141918426, 0.9484240628303365, 0.9494193843070496, 0.9472501472378454, 0.9459301072654994, 0.9491888073319239, 0.9465243782557494, 0.9490410639039168, 0.9489955952826966, 0.9474514925733526, 0.9485750612637676, 0.9464577985993514, 0.9493625527578042, 0.9481009549282967, 0.9472095653520408, 0.9498642632301818, 0.9491352350153821, 0.948612409280547, 0.9483623699093542, 0.9491108732020601, 0.9493722924103973, 0.9494924401560574, 0.9489826048519594, 0.9489160294228411, 0.9490069624380018, 0.9491254911355093, 0.9484711420451496, 0.9485425915278441, 0.9489030432193837, 0.948674110656089, 0.94956226213604, 0.9487845229764357, 0.9483526175749217, 0.9493365592144906, 0.9484955038584716, 0.9489079172729601, 0.9486871053141059, 0.9491401048416787, 0.9498301617642666, 0.9485799395446236, 0.9496434385894884, 0.9495638811841924, 0.9489777434802225, 0.9493333168909059, 0.94959960169826, 0.9485848093709202, 0.9465373644592069, 0.9493154650884317, 0.948982613306519, 0.9486432557410382, 0.949211541642534, 0.9492894884542371, 0.9494875618752013, 0.9494794497253202, 0.9491189853519413, 0.9493040937058469, 0.9497067674677423, 0.9495801139385143, 0.9491271101836617, 0.9495346537718536, 0.9491287419136535, 0.9487309337507749, 0.9494323662832274, 0.9492310378568393, 0.9492911075023894, 0.9490053433898493, 0.9491141239802042, 0.9494356128340917, 0.9492115458698137, 0.9492521362101778, 0.94956226213604, 0.9490118407188578, 0.9494680783427354, 0.949294349825974, 0.949234275953144, 0.9494323662832274, 0.9491417281171108, 0.9493593019796601, 0.9493592977523804, 0.9491401048416787, 0.9494145102534733, 0.9496207127334378, 0.9492683731917794, 0.9492992196522706, 0.9489485118406039, 0.9493349486208976, 0.949245643108449, 0.9496466893676325, 0.9495119279158031, 0.9493950098118884, 0.9492829784433893, 0.9493333168909059, 0.9491043843276111, 0.9492066718162374, 0.9489647530494852, 0.9492553827610422], 'val_mDice': [0.19832222847968128, 0.23561547067782557, 0.2542691836331753, 0.27685505806977023, 0.29355040995787224, 0.30280080881524596, 0.3111764767491226, 0.3071645208707092, 0.3151089217223174, 0.3231599218879186, 0.3351406626244809, 0.33467148424040344, 0.33924223260676606, 0.3393228225251462, 0.34069579966524816, 0.3439839247693407, 0.3424361527811551, 0.34176113123589374, 0.3454939640582876, 0.344198780490997, 0.34618682261054395, 0.3480013053045205, 0.3477306399785035, 0.3508592497372458, 0.34812324541680356, 0.3491198963730048, 0.33979984885411907, 0.34830486837853775, 0.354566530767062, 0.3504208998477205, 0.35383896413424337, 0.3482559133083262, 0.3553487751500826, 0.34891848128738134, 0.3544952324096193, 0.357207864100206, 0.35420394429923796, 0.35851675250851517, 0.35893365635094066, 0.3607379513006684, 0.3540276804714338, 0.3539699559093367, 0.35635335436949495, 0.3509170007198415, 0.36048750822425735, 0.35570317654744954, 0.3590415217352252, 0.364174226708446, 0.35882315842817863, 0.36261647939682007, 0.3600856598387373, 0.35807661765010645, 0.36198025054120003, 0.35573907989136716, 0.3679942640852421, 0.3656398922416335, 0.36822094667887856, 0.3654994059961738, 0.36105001120702596, 0.36170260703310053, 0.368345782477805, 0.3626022345208107, 0.36368741634044244, 0.36455122925711014, 0.3597015744828163, 0.3690205081134823, 0.36388546017044826, 0.36538815033351274, 0.36494044547385357, 0.366632435338717, 0.36727507909138996, 0.3617631818385834, 0.3670418421004681, 0.3686963064028016, 0.3700766738847638, 0.3670066457690922, 0.3687521920981982, 0.3732411415441662, 0.36012216451320245, 0.365389569642696, 0.3694715155354628, 0.3717886957716435, 0.36851080279823734, 0.37317200121304667, 0.37009561843905886, 0.36878827510150614, 0.36657677368914826, 0.37025584079695084, 0.36679415309682806, 0.37065154905860304, 0.36778831207160406, 0.3708500198438658, 0.37168462965505344, 0.3643526718126121, 0.3679722230485145, 0.3688770300107645, 0.36602335196014835, 0.37089586574980554, 0.3723886598509254, 0.3668277916756082, 0.3667002599290077, 0.3703546069615276, 0.3665250613757059, 0.36955414934361236, 0.3747635657060231, 0.37258702918147363, 0.36977402495999706, 0.362810029628429, 0.3729339884528032, 0.37148398393434834, 0.37077846150871707, 0.37328020372289294, 0.36696802639792153, 0.36968772292982605, 0.3712816874608926, 0.36919107673861457, 0.37302147729177004, 0.3654444513591469, 0.3715248659570166, 0.37472497275535094, 0.372404083081171, 0.3716190349548421, 0.37296160104426934, 0.3746607709438243, 0.37241570915736205, 0.3738089233425492, 0.37273660407844167, 0.3708232210037556, 0.37347904122467585, 0.3741678162246731, 0.37093912033324544, 0.37194487685007405, 0.3749219650495137, 0.37024422317531935, 0.3710997009530981, 0.3718294361804394, 0.3735254292792462, 0.3710985426784407, 0.3730144959392277, 0.3696299265039728, 0.37201364623739364, 0.37017539249244313, 0.3716050553406384, 0.3689784107478798, 0.3691233779098971, 0.3733920374660627, 0.3717009427276909, 0.37279767030519795, 0.37201178834793414, 0.37088772506578593, 0.37004671515302456, 0.37136722541024497, 0.3707911602571501, 0.3712184367873145, 0.37180685299508115, 0.3706387477986356, 0.372388039497619, 0.37053501458032756, 0.3708290345702611, 0.3725271296839342, 0.372062179636448, 0.3725590393052879, 0.370709978519602, 0.37083790763050106, 0.3694812181993579, 0.37057387384962526, 0.37188698636724593, 0.3717609553049642, 0.3718643535113504, 0.37235741708295567, 0.37107043338160145, 0.37071429985634824, 0.37198731662533807], 'loss': [0.7454919466652821, 0.6594330967087106, 0.6151490393987636, 0.5854752248709963, 0.5536431711051882, 0.5341752457250025, 0.5196281347078147, 0.5093760153374721, 0.5001014387484678, 0.4912858282782368, 0.47550458815908925, 0.45593884384509215, 0.4434516048922981, 0.43866881181284323, 0.4308733894038446, 0.42435592738623473, 0.4229608498283268, 0.41659166597828423, 0.4166170967608383, 0.410111840084656, 0.4069055473374337, 0.4078187157198326, 0.4020680151034876, 0.3988717638340193, 0.39487879955277, 0.3951487619237801, 0.3931785830517405, 0.3899848349930085, 0.38781550053468683, 0.3862707949790758, 0.38399299358584216, 0.3805921295561741, 0.37975994631187204, 0.37977625839796264, 0.37553191688871873, 0.3749156469844051, 0.3702324215898809, 0.37458423053480916, 0.36862193181649927, 0.3674091299477312, 0.36563447922775427, 0.36636220874245634, 0.36625472795717495, 0.3621009277314255, 0.36305112975467113, 0.36077267346308406, 0.3617450001006274, 0.35769441517972456, 0.3559230020365764, 0.3574920520032804, 0.3557376383659766, 0.35611755408577084, 0.35448543341811173, 0.352359413455442, 0.3501202033967087, 0.35176250939209436, 0.34970288043169634, 0.3517857775860226, 0.3467932627219515, 0.3471883240895173, 0.34586026551797217, 0.345654527458948, 0.3431621767657319, 0.341564322009529, 0.3413911740497215, 0.33968430794391435, 0.33985615022403676, 0.33877607958832967, 0.33863048313819255, 0.3375880249382294, 0.3368596301404471, 0.33660859305834034, 0.3381652547987466, 0.33642672561493114, 0.3328145842576764, 0.33352850021160757, 0.33748732467287595, 0.3342552578019113, 0.33508517338135807, 0.33345136699295536, 0.3328334348103435, 0.33180434110852863, 0.3292477524157652, 0.32958461660699745, 0.3275163305788925, 0.33048600264124034, 0.32749775592813785, 0.32478504251573503, 0.3234139307872536, 0.32604269227109006, 0.32556965003923044, 0.32508197663371097, 0.3256786267935615, 0.3237425955607719, 0.32605565302458006, 0.3207230288804192, 0.32125285057370195, 0.3223093098125507, 0.32122249905903316, 0.32053906773169016, 0.32019273156357797, 0.3198781549930573, 0.32139120152623385, 0.3181928063944443, 0.3190658653519817, 0.31777274183084053, 0.31868321062977784, 0.3161714396372284, 0.3177594374135597, 0.3184207988154028, 0.3176400578513588, 0.31403294135000287, 0.31807988401233533, 0.3163863394063773, 0.3172935784784789, 0.31798592509375406, 0.3158010155884261, 0.3160374860634509, 0.31713757617879157, 0.31397338242260453, 0.31462149747560936, 0.3119777695880723, 0.31297782777818206, 0.3148916938870224, 0.3128348964852156, 0.314441479020512, 0.3158787244834851, 0.31334931263604116, 0.31194321909516126, 0.313545753476546, 0.3142970465968565, 0.3127153623503508, 0.31371880805062263, 0.31108743966240243, 0.31021360804125203, 0.31516783202124626, 0.31360631191853394, 0.31611761539867245, 0.3120854810187497, 0.3123760172386759, 0.3122733862744164, 0.3122477709632559, 0.313181597670329, 0.31167880353546634, 0.3096353029467396, 0.310556936217952, 0.3114140385512224, 0.31109849421941127, 0.31103446706361376, 0.3102096340398199, 0.309264214982077, 0.3094563391405283, 0.3102562268677446, 0.31124061263099156, 0.310322072856205, 0.3098198121509601, 0.3098964898856645, 0.31189318459058546, 0.3127171369250288, 0.309232090843707, 0.3107698514596703, 0.3101150050452075, 0.3116136440143143, 0.311973715396886, 0.3114639669810374, 0.31014944559212815, 0.3105806785453226, 0.3096560425518714, 0.3078600845539693, 0.30952819032767387, 0.3113300251131205, 0.3117438439855871, 0.31062995438108737], 'acc': [0.87050824140765, 0.8954219297035453, 0.9058339705786754, 0.9104174213925588, 0.9134131305611011, 0.9163263510182961, 0.9184723482304012, 0.9199449842123641, 0.9214276038494307, 0.9221883615267645, 0.9229894549576277, 0.9240661936322438, 0.9252639863294424, 0.9260138408424928, 0.9268281963068186, 0.9277574951501236, 0.9279672219581211, 0.9286325337960548, 0.9290431108056885, 0.9296748620947611, 0.9300027141251515, 0.9302454590797424, 0.930711265138744, 0.9312012146428689, 0.9317768399862899, 0.9320566154632371, 0.9323304927840675, 0.9325348604585707, 0.9330474438741035, 0.9330142829836029, 0.9332940834699218, 0.933815234960969, 0.9339562323904529, 0.9341278181862586, 0.9346077855100337, 0.9347459034206941, 0.9349790422572303, 0.9349875153340015, 0.9352826080371424, 0.9354700771803709, 0.9357825893102233, 0.9359237044742427, 0.936015185007115, 0.9362877394120718, 0.9362995167368466, 0.9366320192199392, 0.936676177904778, 0.9368024010019204, 0.9370426673864581, 0.9369530279611804, 0.9373378079576591, 0.9374237192045782, 0.9377806734178484, 0.9378001214917173, 0.937952306282889, 0.9379397028500271, 0.9381446124966611, 0.9383713328961245, 0.9383652196102535, 0.9386282378865272, 0.9387245569032492, 0.938803150850473, 0.9390447391677148, 0.9390011230080398, 0.939319299729829, 0.9392421452040525, 0.9393120542014997, 0.9395953471513139, 0.9396438951959315, 0.9395695498923665, 0.9398126258063562, 0.9398093917935165, 0.9399741564214844, 0.9399710167314589, 0.9402390148221832, 0.9403626660710758, 0.940309914915832, 0.9405349351696133, 0.9404198064632022, 0.9404115216019228, 0.9406133879705803, 0.9406947673596058, 0.9409259705199409, 0.9409065236750337, 0.9411108437887172, 0.941177282689773, 0.9411347281687038, 0.9412199079990387, 0.9414467927721358, 0.9414624169929741, 0.941635560313451, 0.9414355092441913, 0.9415946812973809, 0.9418892082479812, 0.9418189685983757, 0.9419618073812465, 0.9420256276106097, 0.9420209778338363, 0.9420557194149372, 0.9422917602602968, 0.9420415349227866, 0.9423381145467463, 0.9422288167107965, 0.9424846586492873, 0.9423768915466426, 0.942305379921628, 0.9423467301830803, 0.942551994692419, 0.9425321461613645, 0.9424821105199991, 0.9424713232468084, 0.9426676669686112, 0.9426389912969059, 0.9426346017527826, 0.9427007572552593, 0.9426804361269646, 0.9428062821786428, 0.9427043680677709, 0.942894245054304, 0.9428574262206088, 0.9431357860565186, 0.9429003103492186, 0.943073925775351, 0.9430181075002729, 0.9430496865326596, 0.9432047977889936, 0.9429761669684931, 0.9430766169557866, 0.9431506310541605, 0.9431157013190161, 0.9431228993479739, 0.943186955107856, 0.9430665858013114, 0.9431475159433699, 0.9432678387951605, 0.943179118448926, 0.9433064044136362, 0.9431690637598332, 0.9434719935520408, 0.9434130609035491, 0.9433094723322957, 0.9433087414072961, 0.9433055299459044, 0.9431403176686198, 0.9434298654192502, 0.9434511061796208, 0.9433913230281515, 0.9434898364175226, 0.9434828274643299, 0.9433922193714024, 0.9434315404941126, 0.9434501392325175, 0.9433997726932014, 0.9434173085640386, 0.9435395662932052, 0.9435277652494686, 0.9435335005681539, 0.943481600714713, 0.943427104249443, 0.9434755349282137, 0.9433780821942791, 0.9434406987785064, 0.9434133191698605, 0.9435777553578013, 0.9434746841794437, 0.9435116684313902, 0.9434128003636586, 0.9435421857637228, 0.9435241774185417, 0.943543318497766, 0.9434837239919249, 0.9434794282175831, 0.9435327456169522], 'mDice': [0.19770214249639168, 0.289881526248664, 0.3374355000683788, 0.3694085325700106, 0.40377543983385733, 0.42477064352367344, 0.44045555391876967, 0.45149299828047607, 0.46148610815559465, 0.47100843728817615, 0.4880681940575236, 0.5092314933532293, 0.5227218816882556, 0.5278778120875358, 0.5362937533517473, 0.5433060372305899, 0.5448163131155919, 0.5516960342520291, 0.5516454888680546, 0.5586633033973655, 0.5621307542643597, 0.5611441362457177, 0.5673404952299964, 0.5707971820511769, 0.5750921769854949, 0.574798732289334, 0.5769247656323246, 0.5803716328955189, 0.5827016961943243, 0.5843770888355589, 0.5868333530794714, 0.5904930166055247, 0.5913962322412077, 0.59136733400453, 0.5959420027192106, 0.5965998118992933, 0.6016774110880094, 0.5969554049606176, 0.6033909604106982, 0.6046933637144639, 0.606611929787803, 0.6058210589222073, 0.6059399458243675, 0.6104313860541767, 0.6093907466868764, 0.611838351021108, 0.6107815717913441, 0.6151622994044392, 0.6170803059314944, 0.6153894549024473, 0.6172695688980142, 0.6168489834236116, 0.6185926771041044, 0.6209063305068262, 0.6233222487968267, 0.6215492830755784, 0.6237665814222749, 0.6215063218910669, 0.6269073277711869, 0.6264727997411158, 0.6279051519239072, 0.6279911442208536, 0.6308112616391526, 0.6325449913740158, 0.6327198016274835, 0.6345704061468852, 0.6343737179778286, 0.6355143023827641, 0.6355208725198028, 0.636823008939163, 0.6376028925180435, 0.6378516263568524, 0.6361822560429573, 0.6380552955816702, 0.6419693568625401, 0.6411804749486373, 0.6368884529649597, 0.6403923629485455, 0.639500307913908, 0.6412611940472397, 0.6419287071092842, 0.643035569418337, 0.6457838949776188, 0.6454216767524935, 0.6476609544348471, 0.6444233960097598, 0.647692421846783, 0.6505975129370837, 0.6521049953920325, 0.6492422460280742, 0.6497433776400753, 0.6502891980188409, 0.6496396879252699, 0.6517201804930401, 0.6492136635116695, 0.6549851817568553, 0.6544060618607039, 0.6532639547875247, 0.6544296653614831, 0.6551707750743198, 0.6554869426279953, 0.6558889407165271, 0.6542348059489554, 0.6577021045168651, 0.6567643022414336, 0.6581605805871413, 0.6570830334707634, 0.6598853471352882, 0.658151528823007, 0.6574384587941711, 0.6582954502597298, 0.6622032380595649, 0.6577867761100691, 0.6596545273495703, 0.6586755572520581, 0.6578978500108129, 0.6601763044435953, 0.6600337454953145, 0.6588250147620427, 0.6622243627444985, 0.6615141999475734, 0.6643308351027597, 0.663265221665815, 0.6612594243177433, 0.6634701105122713, 0.6617070352908262, 0.6601528718299472, 0.6629252836569068, 0.6644438075650598, 0.6626314914103636, 0.6618971772875982, 0.663600775107895, 0.6625208970197697, 0.6652569524098917, 0.6662837175978827, 0.6609314514804132, 0.6625933194897838, 0.6599135058442341, 0.6642677399915519, 0.6639661199038791, 0.6640824011920654, 0.6640336482488003, 0.6629372804742498, 0.6647147506168208, 0.6669363521423537, 0.665927669221593, 0.6650128529858343, 0.6653486083156055, 0.6654155721062237, 0.6662743042424782, 0.6673335408119812, 0.666965695816217, 0.666134456875398, 0.665199848395033, 0.6661656431316101, 0.6666060378871013, 0.6666302792497517, 0.6644871327680411, 0.6635946559844558, 0.6673647674395866, 0.6655733365987994, 0.6664177998439553, 0.6647894853783637, 0.6643946137010437, 0.6648402951427341, 0.6663214417462496, 0.6659120676136508, 0.6668566197464146, 0.6688510246498068, 0.667029220603176, 0.6649831390565204, 0.6646045809861311, 0.6658443398082379], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:01<00:04,  1.40s/it]predicting test subjects:  50%|█████     | 2/4 [00:02<00:02,  1.21s/it]predicting test subjects:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.00s/it]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM00_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a’: File exists
mkdir: cannot create directory ‘sd0’: File exists
mkdir: cannot create directory ‘sd1’: File exists
mkdir: cannot create directory ‘sd2’: File exists

  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:01,  2.90it/s] 50%|█████     | 2/4 [00:00<00:00,  3.05it/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.18it/s]100%|██████████| 4/4 [00:01<00:00,  3.11it/s]100%|██████████| 4/4 [00:01<00:00,  3.19it/s]

CrossVal ['a']
2020-01-22 02:48:03.769466: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-22 02:48:07.298286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-22 02:48:07.298366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 02:48:07.725475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 02:48:07.725537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 02:48:07.725547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 02:48:07.725999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
Loading train:   0%|          | 0/247 [00:00<?, ?it/s]Loading train:   0%|          | 1/247 [00:00<01:06,  3.68it/s]Loading train:   1%|          | 2/247 [00:00<01:05,  3.77it/s]Loading train:   1%|          | 3/247 [00:00<01:03,  3.84it/s]Loading train:   2%|▏         | 4/247 [00:01<01:03,  3.83it/s]Loading train:   2%|▏         | 5/247 [00:01<01:01,  3.95it/s]Loading train:   2%|▏         | 6/247 [00:01<00:59,  4.08it/s]Loading train:   3%|▎         | 7/247 [00:01<00:57,  4.17it/s]Loading train:   3%|▎         | 8/247 [00:01<00:56,  4.22it/s]Loading train:   4%|▎         | 9/247 [00:02<00:55,  4.27it/s]Loading train:   4%|▍         | 10/247 [00:02<00:54,  4.32it/s]Loading train:   4%|▍         | 11/247 [00:02<00:54,  4.34it/s]Loading train:   5%|▍         | 12/247 [00:02<00:53,  4.35it/s]Loading train:   5%|▌         | 13/247 [00:03<00:53,  4.34it/s]Loading train:   6%|▌         | 14/247 [00:03<00:53,  4.34it/s]Loading train:   6%|▌         | 15/247 [00:03<00:53,  4.34it/s]Loading train:   6%|▋         | 16/247 [00:03<00:53,  4.35it/s]Loading train:   7%|▋         | 17/247 [00:04<00:52,  4.34it/s]Loading train:   7%|▋         | 18/247 [00:04<00:53,  4.32it/s]Loading train:   8%|▊         | 19/247 [00:04<00:52,  4.31it/s]Loading train:   8%|▊         | 20/247 [00:04<00:52,  4.32it/s]Loading train:   9%|▊         | 21/247 [00:04<00:51,  4.35it/s]Loading train:   9%|▉         | 22/247 [00:05<00:51,  4.39it/s]Loading train:   9%|▉         | 23/247 [00:05<00:50,  4.39it/s]Loading train:  10%|▉         | 24/247 [00:05<00:50,  4.44it/s]Loading train:  10%|█         | 25/247 [00:05<00:49,  4.44it/s]Loading train:  11%|█         | 26/247 [00:06<00:49,  4.46it/s]Loading train:  11%|█         | 27/247 [00:06<00:49,  4.49it/s]Loading train:  11%|█▏        | 28/247 [00:06<00:48,  4.50it/s]Loading train:  12%|█▏        | 29/247 [00:06<00:48,  4.50it/s]Loading train:  12%|█▏        | 30/247 [00:06<00:48,  4.49it/s]Loading train:  13%|█▎        | 31/247 [00:07<00:47,  4.51it/s]Loading train:  13%|█▎        | 32/247 [00:07<00:47,  4.52it/s]Loading train:  13%|█▎        | 33/247 [00:07<00:47,  4.53it/s]Loading train:  14%|█▍        | 34/247 [00:07<00:46,  4.55it/s]Loading train:  14%|█▍        | 35/247 [00:08<00:46,  4.55it/s]Loading train:  15%|█▍        | 36/247 [00:08<00:46,  4.54it/s]Loading train:  15%|█▍        | 37/247 [00:08<00:46,  4.55it/s]Loading train:  15%|█▌        | 38/247 [00:08<00:45,  4.56it/s]Loading train:  16%|█▌        | 39/247 [00:08<00:45,  4.57it/s]Loading train:  16%|█▌        | 40/247 [00:09<00:45,  4.54it/s]Loading train:  17%|█▋        | 41/247 [00:09<00:45,  4.56it/s]Loading train:  17%|█▋        | 42/247 [00:09<00:44,  4.56it/s]Loading train:  17%|█▋        | 43/247 [00:09<00:44,  4.57it/s]Loading train:  18%|█▊        | 44/247 [00:10<00:44,  4.58it/s]Loading train:  18%|█▊        | 45/247 [00:10<00:44,  4.56it/s]Loading train:  19%|█▊        | 46/247 [00:10<00:43,  4.58it/s]Loading train:  19%|█▉        | 47/247 [00:10<00:43,  4.58it/s]Loading train:  19%|█▉        | 48/247 [00:10<00:43,  4.57it/s]Loading train:  20%|█▉        | 49/247 [00:11<00:43,  4.58it/s]Loading train:  20%|██        | 50/247 [00:11<00:42,  4.58it/s]Loading train:  21%|██        | 51/247 [00:11<00:42,  4.58it/s]Loading train:  21%|██        | 52/247 [00:11<00:42,  4.59it/s]Loading train:  21%|██▏       | 53/247 [00:11<00:42,  4.58it/s]Loading train:  22%|██▏       | 54/247 [00:12<00:42,  4.58it/s]Loading train:  22%|██▏       | 55/247 [00:12<00:42,  4.56it/s]Loading train:  23%|██▎       | 56/247 [00:12<00:42,  4.55it/s]Loading train:  23%|██▎       | 57/247 [00:12<00:41,  4.57it/s]Loading train:  23%|██▎       | 58/247 [00:13<00:41,  4.58it/s]Loading train:  24%|██▍       | 59/247 [00:13<00:41,  4.55it/s]Loading train:  24%|██▍       | 60/247 [00:13<00:41,  4.53it/s]Loading train:  25%|██▍       | 61/247 [00:13<00:41,  4.48it/s]Loading train:  25%|██▌       | 62/247 [00:13<00:41,  4.45it/s]Loading train:  26%|██▌       | 63/247 [00:14<00:41,  4.44it/s]Loading train:  26%|██▌       | 64/247 [00:14<00:41,  4.40it/s]Loading train:  26%|██▋       | 65/247 [00:14<00:41,  4.40it/s]Loading train:  27%|██▋       | 66/247 [00:14<00:41,  4.36it/s]Loading train:  27%|██▋       | 67/247 [00:15<00:41,  4.31it/s]Loading train:  28%|██▊       | 68/247 [00:15<00:41,  4.32it/s]Loading train:  28%|██▊       | 69/247 [00:15<00:40,  4.37it/s]Loading train:  28%|██▊       | 70/247 [00:15<00:40,  4.40it/s]Loading train:  29%|██▊       | 71/247 [00:16<00:40,  4.40it/s]Loading train:  29%|██▉       | 72/247 [00:16<00:39,  4.40it/s]Loading train:  30%|██▉       | 73/247 [00:16<00:39,  4.42it/s]Loading train:  30%|██▉       | 74/247 [00:16<00:39,  4.39it/s]Loading train:  30%|███       | 75/247 [00:16<00:39,  4.35it/s]Loading train:  31%|███       | 76/247 [00:17<00:39,  4.33it/s]Loading train:  31%|███       | 77/247 [00:17<00:39,  4.29it/s]Loading train:  32%|███▏      | 78/247 [00:17<00:42,  4.01it/s]Loading train:  32%|███▏      | 79/247 [00:17<00:41,  4.01it/s]Loading train:  32%|███▏      | 80/247 [00:18<00:39,  4.25it/s]Loading train:  33%|███▎      | 81/247 [00:18<00:38,  4.26it/s]Loading train:  33%|███▎      | 82/247 [00:18<00:38,  4.32it/s]Loading train:  34%|███▎      | 83/247 [00:18<00:37,  4.39it/s]Loading train:  34%|███▍      | 84/247 [00:19<00:36,  4.45it/s]Loading train:  34%|███▍      | 85/247 [00:19<00:36,  4.44it/s]Loading train:  35%|███▍      | 86/247 [00:19<00:36,  4.46it/s]Loading train:  35%|███▌      | 87/247 [00:19<00:35,  4.47it/s]Loading train:  36%|███▌      | 88/247 [00:19<00:35,  4.48it/s]Loading train:  36%|███▌      | 89/247 [00:20<00:34,  4.52it/s]Loading train:  36%|███▋      | 90/247 [00:20<00:34,  4.56it/s]Loading train:  37%|███▋      | 91/247 [00:20<00:34,  4.58it/s]Loading train:  37%|███▋      | 92/247 [00:20<00:33,  4.59it/s]Loading train:  38%|███▊      | 93/247 [00:21<00:33,  4.59it/s]Loading train:  38%|███▊      | 94/247 [00:21<00:33,  4.61it/s]Loading train:  38%|███▊      | 95/247 [00:21<00:32,  4.61it/s]Loading train:  39%|███▉      | 96/247 [00:21<00:33,  4.49it/s]Loading train:  39%|███▉      | 97/247 [00:21<00:33,  4.42it/s]Loading train:  40%|███▉      | 98/247 [00:22<00:34,  4.38it/s]Loading train:  40%|████      | 99/247 [00:22<00:34,  4.33it/s]Loading train:  40%|████      | 100/247 [00:22<00:36,  4.04it/s]Loading train:  41%|████      | 101/247 [00:22<00:37,  3.91it/s]Loading train:  41%|████▏     | 102/247 [00:23<00:37,  3.83it/s]Loading train:  42%|████▏     | 103/247 [00:23<00:38,  3.72it/s]Loading train:  42%|████▏     | 104/247 [00:23<00:38,  3.68it/s]Loading train:  43%|████▎     | 105/247 [00:24<00:38,  3.67it/s]Loading train:  43%|████▎     | 106/247 [00:24<00:37,  3.73it/s]Loading train:  43%|████▎     | 107/247 [00:24<00:37,  3.77it/s]Loading train:  44%|████▎     | 108/247 [00:24<00:36,  3.82it/s]Loading train:  44%|████▍     | 109/247 [00:25<00:36,  3.82it/s]Loading train:  45%|████▍     | 110/247 [00:25<00:35,  3.83it/s]Loading train:  45%|████▍     | 111/247 [00:25<00:35,  3.84it/s]Loading train:  45%|████▌     | 112/247 [00:25<00:35,  3.84it/s]Loading train:  46%|████▌     | 113/247 [00:26<00:35,  3.82it/s]Loading train:  46%|████▌     | 114/247 [00:26<00:34,  3.84it/s]Loading train:  47%|████▋     | 115/247 [00:26<00:34,  3.84it/s]Loading train:  47%|████▋     | 116/247 [00:26<00:33,  3.87it/s]Loading train:  47%|████▋     | 117/247 [00:27<00:33,  3.88it/s]Loading train:  48%|████▊     | 118/247 [00:27<00:32,  4.01it/s]Loading train:  48%|████▊     | 119/247 [00:27<00:31,  4.05it/s]Loading train:  49%|████▊     | 120/247 [00:27<00:30,  4.14it/s]Loading train:  49%|████▉     | 121/247 [00:28<00:30,  4.19it/s]Loading train:  49%|████▉     | 122/247 [00:28<00:29,  4.23it/s]Loading train:  50%|████▉     | 123/247 [00:28<00:29,  4.26it/s]Loading train:  50%|█████     | 124/247 [00:28<00:28,  4.29it/s]Loading train:  51%|█████     | 125/247 [00:29<00:28,  4.31it/s]Loading train:  51%|█████     | 126/247 [00:29<00:27,  4.32it/s]Loading train:  51%|█████▏    | 127/247 [00:29<00:27,  4.34it/s]Loading train:  52%|█████▏    | 128/247 [00:29<00:27,  4.34it/s]Loading train:  52%|█████▏    | 129/247 [00:29<00:27,  4.35it/s]Loading train:  53%|█████▎    | 130/247 [00:30<00:26,  4.35it/s]Loading train:  53%|█████▎    | 131/247 [00:30<00:26,  4.36it/s]Loading train:  53%|█████▎    | 132/247 [00:30<00:26,  4.34it/s]Loading train:  54%|█████▍    | 133/247 [00:30<00:26,  4.35it/s]Loading train:  54%|█████▍    | 134/247 [00:31<00:25,  4.36it/s]Loading train:  55%|█████▍    | 135/247 [00:31<00:25,  4.37it/s]Loading train:  55%|█████▌    | 136/247 [00:31<00:24,  4.58it/s]Loading train:  55%|█████▌    | 137/247 [00:31<00:23,  4.74it/s]Loading train:  56%|█████▌    | 138/247 [00:31<00:22,  4.83it/s]Loading train:  56%|█████▋    | 139/247 [00:32<00:21,  4.92it/s]Loading train:  57%|█████▋    | 140/247 [00:32<00:21,  4.97it/s]Loading train:  57%|█████▋    | 141/247 [00:32<00:21,  5.04it/s]Loading train:  57%|█████▋    | 142/247 [00:32<00:20,  5.06it/s]Loading train:  58%|█████▊    | 143/247 [00:32<00:20,  5.08it/s]Loading train:  58%|█████▊    | 144/247 [00:33<00:20,  4.97it/s]Loading train:  59%|█████▊    | 145/247 [00:33<00:20,  4.95it/s]Loading train:  59%|█████▉    | 146/247 [00:33<00:20,  5.00it/s]Loading train:  60%|█████▉    | 147/247 [00:33<00:19,  5.06it/s]Loading train:  60%|█████▉    | 148/247 [00:33<00:19,  5.07it/s]Loading train:  60%|██████    | 149/247 [00:34<00:19,  5.09it/s]Loading train:  61%|██████    | 150/247 [00:34<00:19,  5.10it/s]Loading train:  61%|██████    | 151/247 [00:34<00:18,  5.06it/s]Loading train:  62%|██████▏   | 152/247 [00:34<00:18,  5.04it/s]Loading train:  62%|██████▏   | 153/247 [00:34<00:19,  4.93it/s]Loading train:  62%|██████▏   | 154/247 [00:35<00:19,  4.78it/s]Loading train:  63%|██████▎   | 155/247 [00:35<00:19,  4.73it/s]Loading train:  63%|██████▎   | 156/247 [00:35<00:19,  4.69it/s]Loading train:  64%|██████▎   | 157/247 [00:35<00:20,  4.41it/s]Loading train:  64%|██████▍   | 158/247 [00:36<00:19,  4.47it/s]Loading train:  64%|██████▍   | 159/247 [00:36<00:19,  4.52it/s]Loading train:  65%|██████▍   | 160/247 [00:36<00:19,  4.54it/s]Loading train:  65%|██████▌   | 161/247 [00:36<00:18,  4.53it/s]Loading train:  66%|██████▌   | 162/247 [00:36<00:18,  4.52it/s]Loading train:  66%|██████▌   | 163/247 [00:37<00:18,  4.50it/s]Loading train:  66%|██████▋   | 164/247 [00:37<00:18,  4.37it/s]Loading train:  67%|██████▋   | 165/247 [00:37<00:18,  4.42it/s]Loading train:  67%|██████▋   | 166/247 [00:37<00:18,  4.48it/s]Loading train:  68%|██████▊   | 167/247 [00:38<00:17,  4.49it/s]Loading train:  68%|██████▊   | 168/247 [00:38<00:17,  4.52it/s]Loading train:  68%|██████▊   | 169/247 [00:38<00:17,  4.45it/s]Loading train:  69%|██████▉   | 170/247 [00:38<00:17,  4.44it/s]Loading train:  69%|██████▉   | 171/247 [00:38<00:16,  4.48it/s]Loading train:  70%|██████▉   | 172/247 [00:39<00:17,  4.40it/s]Loading train:  70%|███████   | 173/247 [00:39<00:16,  4.45it/s]Loading train:  70%|███████   | 174/247 [00:39<00:16,  4.30it/s]Loading train:  71%|███████   | 175/247 [00:39<00:18,  3.91it/s]Loading train:  71%|███████▏  | 176/247 [00:40<00:18,  3.83it/s]Loading train:  72%|███████▏  | 177/247 [00:40<00:17,  4.02it/s]Loading train:  72%|███████▏  | 178/247 [00:40<00:16,  4.16it/s]Loading train:  72%|███████▏  | 179/247 [00:40<00:15,  4.27it/s]Loading train:  73%|███████▎  | 180/247 [00:41<00:15,  4.35it/s]Loading train:  73%|███████▎  | 181/247 [00:41<00:14,  4.43it/s]Loading train:  74%|███████▎  | 182/247 [00:41<00:14,  4.45it/s]Loading train:  74%|███████▍  | 183/247 [00:41<00:14,  4.49it/s]Loading train:  74%|███████▍  | 184/247 [00:41<00:14,  4.48it/s]Loading train:  75%|███████▍  | 185/247 [00:42<00:13,  4.51it/s]Loading train:  75%|███████▌  | 186/247 [00:42<00:13,  4.52it/s]Loading train:  76%|███████▌  | 187/247 [00:42<00:13,  4.54it/s]Loading train:  76%|███████▌  | 188/247 [00:42<00:12,  4.55it/s]Loading train:  77%|███████▋  | 189/247 [00:43<00:12,  4.49it/s]Loading train:  77%|███████▋  | 190/247 [00:43<00:13,  4.37it/s]Loading train:  77%|███████▋  | 191/247 [00:43<00:12,  4.41it/s]Loading train:  78%|███████▊  | 192/247 [00:43<00:12,  4.43it/s]Loading train:  78%|███████▊  | 193/247 [00:43<00:12,  4.46it/s]Loading train:  79%|███████▊  | 194/247 [00:44<00:11,  4.53it/s]Loading train:  79%|███████▉  | 195/247 [00:44<00:11,  4.58it/s]Loading train:  79%|███████▉  | 196/247 [00:44<00:11,  4.61it/s]Loading train:  80%|███████▉  | 197/247 [00:44<00:10,  4.63it/s]Loading train:  80%|████████  | 198/247 [00:45<00:10,  4.65it/s]Loading train:  81%|████████  | 199/247 [00:45<00:10,  4.69it/s]Loading train:  81%|████████  | 200/247 [00:45<00:09,  4.71it/s]Loading train:  81%|████████▏ | 201/247 [00:45<00:09,  4.73it/s]Loading train:  82%|████████▏ | 202/247 [00:45<00:09,  4.75it/s]Loading train:  82%|████████▏ | 203/247 [00:46<00:09,  4.75it/s]Loading train:  83%|████████▎ | 204/247 [00:46<00:09,  4.75it/s]Loading train:  83%|████████▎ | 205/247 [00:46<00:08,  4.74it/s]Loading train:  83%|████████▎ | 206/247 [00:46<00:08,  4.75it/s]Loading train:  84%|████████▍ | 207/247 [00:47<00:20,  1.91it/s]Loading train:  84%|████████▍ | 208/247 [00:48<00:21,  1.82it/s]Loading train:  85%|████████▍ | 209/247 [00:49<00:22,  1.70it/s]Loading train:  85%|████████▌ | 210/247 [00:49<00:23,  1.60it/s]Loading train:  85%|████████▌ | 211/247 [00:50<00:22,  1.63it/s]Loading train:  86%|████████▌ | 212/247 [00:51<00:22,  1.56it/s]Loading train:  86%|████████▌ | 213/247 [00:51<00:22,  1.51it/s]Loading train:  87%|████████▋ | 214/247 [00:52<00:21,  1.53it/s]Loading train:  87%|████████▋ | 215/247 [00:53<00:21,  1.51it/s]Loading train:  87%|████████▋ | 216/247 [00:53<00:20,  1.51it/s]Loading train:  88%|████████▊ | 217/247 [00:54<00:19,  1.53it/s]Loading train:  88%|████████▊ | 218/247 [00:55<00:19,  1.52it/s]Loading train:  89%|████████▊ | 219/247 [00:55<00:18,  1.48it/s]Loading train:  89%|████████▉ | 220/247 [00:56<00:18,  1.48it/s]Loading train:  89%|████████▉ | 221/247 [00:57<00:17,  1.49it/s]Loading train:  90%|████████▉ | 222/247 [00:58<00:17,  1.45it/s]Loading train:  90%|█████████ | 223/247 [00:58<00:16,  1.43it/s]Loading train:  91%|█████████ | 224/247 [00:59<00:15,  1.50it/s]Loading train:  91%|█████████ | 225/247 [01:00<00:14,  1.51it/s]Loading train:  91%|█████████▏| 226/247 [01:00<00:14,  1.44it/s]Loading train:  92%|█████████▏| 227/247 [01:01<00:13,  1.47it/s]Loading train:  92%|█████████▏| 228/247 [01:02<00:12,  1.48it/s]Loading train:  93%|█████████▎| 229/247 [01:02<00:12,  1.48it/s]Loading train:  93%|█████████▎| 230/247 [01:03<00:11,  1.45it/s]Loading train:  94%|█████████▎| 231/247 [01:04<00:11,  1.42it/s]Loading train:  94%|█████████▍| 232/247 [01:04<00:10,  1.42it/s]Loading train:  94%|█████████▍| 233/247 [01:05<00:09,  1.41it/s]Loading train:  95%|█████████▍| 234/247 [01:06<00:09,  1.39it/s]Loading train:  95%|█████████▌| 235/247 [01:07<00:08,  1.42it/s]Loading train:  96%|█████████▌| 236/247 [01:07<00:08,  1.37it/s]Loading train:  96%|█████████▌| 237/247 [01:08<00:07,  1.42it/s]Loading train:  96%|█████████▋| 238/247 [01:09<00:06,  1.37it/s]Loading train:  97%|█████████▋| 239/247 [01:10<00:06,  1.32it/s]Loading train:  97%|█████████▋| 240/247 [01:10<00:05,  1.36it/s]Loading train:  98%|█████████▊| 241/247 [01:11<00:04,  1.34it/s]Loading train:  98%|█████████▊| 242/247 [01:12<00:03,  1.42it/s]Loading train:  98%|█████████▊| 243/247 [01:12<00:02,  1.45it/s]Loading train:  99%|█████████▉| 244/247 [01:13<00:02,  1.37it/s]Loading train:  99%|█████████▉| 245/247 [01:14<00:01,  1.37it/s]Loading train: 100%|█████████▉| 246/247 [01:15<00:00,  1.34it/s]Loading train: 100%|██████████| 247/247 [01:15<00:00,  1.32it/s]Loading train: 100%|██████████| 247/247 [01:15<00:00,  3.25it/s]
concatenating: train:   0%|          | 0/247 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/247 [00:00<00:04, 52.13it/s]concatenating: train:   5%|▌         | 13/247 [00:00<00:04, 54.77it/s]concatenating: train:   8%|▊         | 20/247 [00:00<00:04, 56.29it/s]concatenating: train:  10%|█         | 25/247 [00:00<00:04, 53.84it/s]concatenating: train:  12%|█▏        | 30/247 [00:00<00:04, 51.42it/s]concatenating: train:  14%|█▍        | 35/247 [00:00<00:04, 50.62it/s]concatenating: train:  16%|█▌        | 40/247 [00:00<00:04, 49.70it/s]concatenating: train:  18%|█▊        | 45/247 [00:00<00:04, 49.31it/s]concatenating: train:  20%|██        | 50/247 [00:00<00:04, 48.82it/s]concatenating: train:  22%|██▏       | 55/247 [00:01<00:03, 48.57it/s]concatenating: train:  25%|██▍       | 61/247 [00:01<00:03, 50.84it/s]concatenating: train:  27%|██▋       | 67/247 [00:01<00:03, 53.11it/s]concatenating: train:  30%|██▉       | 73/247 [00:01<00:03, 54.97it/s]concatenating: train:  32%|███▏      | 79/247 [00:01<00:03, 55.45it/s]concatenating: train:  35%|███▍      | 86/247 [00:01<00:02, 57.62it/s]concatenating: train:  37%|███▋      | 92/247 [00:01<00:02, 57.77it/s]concatenating: train:  40%|███▉      | 98/247 [00:01<00:02, 58.32it/s]concatenating: train:  42%|████▏     | 104/247 [00:01<00:02, 54.53it/s]concatenating: train:  45%|████▍     | 110/247 [00:02<00:02, 53.51it/s]concatenating: train:  47%|████▋     | 116/247 [00:02<00:02, 52.80it/s]concatenating: train:  49%|████▉     | 122/247 [00:02<00:02, 53.84it/s]concatenating: train:  52%|█████▏    | 128/247 [00:02<00:02, 54.77it/s]concatenating: train:  54%|█████▍    | 134/247 [00:02<00:02, 55.16it/s]concatenating: train:  57%|█████▋    | 141/247 [00:02<00:01, 57.72it/s]concatenating: train:  60%|█████▉    | 148/247 [00:02<00:01, 59.65it/s]concatenating: train:  63%|██████▎   | 155/247 [00:02<00:01, 59.78it/s]concatenating: train:  66%|██████▌   | 162/247 [00:02<00:01, 59.81it/s]concatenating: train:  68%|██████▊   | 169/247 [00:03<00:01, 59.78it/s]concatenating: train:  71%|███████   | 175/247 [00:03<00:01, 58.37it/s]concatenating: train:  73%|███████▎  | 181/247 [00:03<00:01, 58.49it/s]concatenating: train:  76%|███████▌  | 187/247 [00:03<00:01, 57.05it/s]concatenating: train:  78%|███████▊  | 193/247 [00:03<00:00, 56.33it/s]concatenating: train:  81%|████████  | 200/247 [00:03<00:00, 57.88it/s]concatenating: train:  84%|████████▍ | 207/247 [00:03<00:00, 58.65it/s]concatenating: train:  86%|████████▌ | 213/247 [00:03<00:00, 53.01it/s]concatenating: train:  89%|████████▊ | 219/247 [00:03<00:00, 48.50it/s]concatenating: train:  91%|█████████ | 225/247 [00:04<00:00, 46.07it/s]concatenating: train:  93%|█████████▎| 230/247 [00:04<00:00, 45.50it/s]concatenating: train:  95%|█████████▌| 235/247 [00:04<00:00, 44.65it/s]concatenating: train:  97%|█████████▋| 240/247 [00:04<00:00, 43.80it/s]concatenating: train:  99%|█████████▉| 245/247 [00:04<00:00, 43.22it/s]concatenating: train: 100%|██████████| 247/247 [00:04<00:00, 53.09it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:01<00:07,  1.83s/it]Loading test:  40%|████      | 2/5 [00:04<00:05,  1.98s/it]Loading test:  60%|██████    | 3/5 [00:05<00:03,  1.92s/it]Loading test:  80%|████████  | 4/5 [00:07<00:01,  1.89s/it]Loading test: 100%|██████████| 5/5 [00:09<00:00,  1.90s/it]Loading test: 100%|██████████| 5/5 [00:09<00:00,  1.93s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 57.96it/s]
Loading trainS:   0%|          | 0/247 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/247 [00:02<08:22,  2.04s/it]Loading trainS:   1%|          | 2/247 [00:03<08:12,  2.01s/it]Loading trainS:   1%|          | 3/247 [00:05<07:32,  1.86s/it]Loading trainS:   2%|▏         | 4/247 [00:07<07:12,  1.78s/it]Loading trainS:   2%|▏         | 5/247 [00:08<06:43,  1.67s/it]Loading trainS:   2%|▏         | 6/247 [00:10<06:48,  1.70s/it]Loading trainS:   3%|▎         | 7/247 [00:11<06:43,  1.68s/it]Loading trainS:   3%|▎         | 8/247 [00:14<07:19,  1.84s/it]Loading trainS:   4%|▎         | 9/247 [00:16<07:36,  1.92s/it]Loading trainS:   4%|▍         | 10/247 [00:18<07:27,  1.89s/it]Loading trainS:   4%|▍         | 11/247 [00:19<07:10,  1.82s/it]Loading trainS:   5%|▍         | 12/247 [00:21<06:48,  1.74s/it]Loading trainS:   5%|▌         | 13/247 [00:22<06:39,  1.71s/it]Loading trainS:   6%|▌         | 14/247 [00:24<06:57,  1.79s/it]Loading trainS:   6%|▌         | 15/247 [00:26<07:08,  1.85s/it]Loading trainS:   6%|▋         | 16/247 [00:28<06:57,  1.81s/it]Loading trainS:   7%|▋         | 17/247 [00:29<06:27,  1.68s/it]Loading trainS:   7%|▋         | 18/247 [00:31<06:15,  1.64s/it]Loading trainS:   8%|▊         | 19/247 [00:33<06:49,  1.80s/it]Loading trainS:   8%|▊         | 20/247 [00:35<06:37,  1.75s/it]Loading trainS:   9%|▊         | 21/247 [00:36<06:24,  1.70s/it]Loading trainS:   9%|▉         | 22/247 [00:38<06:00,  1.60s/it]Loading trainS:   9%|▉         | 23/247 [00:39<05:29,  1.47s/it]Loading trainS:  10%|▉         | 24/247 [00:41<06:07,  1.65s/it]Loading trainS:  10%|█         | 25/247 [00:43<06:20,  1.72s/it]Loading trainS:  11%|█         | 26/247 [00:44<05:55,  1.61s/it]Loading trainS:  11%|█         | 27/247 [00:46<05:37,  1.53s/it]Loading trainS:  11%|█▏        | 28/247 [00:47<05:43,  1.57s/it]Loading trainS:  12%|█▏        | 29/247 [00:48<05:04,  1.40s/it]Loading trainS:  12%|█▏        | 30/247 [00:49<04:41,  1.30s/it]Loading trainS:  13%|█▎        | 31/247 [00:51<05:18,  1.47s/it]Loading trainS:  13%|█▎        | 32/247 [00:53<05:35,  1.56s/it]Loading trainS:  13%|█▎        | 33/247 [00:55<05:43,  1.60s/it]Loading trainS:  14%|█▍        | 34/247 [00:57<06:05,  1.72s/it]Loading trainS:  14%|█▍        | 35/247 [00:58<06:05,  1.72s/it]Loading trainS:  15%|█▍        | 36/247 [01:00<06:08,  1.75s/it]Loading trainS:  15%|█▍        | 37/247 [01:02<06:03,  1.73s/it]Loading trainS:  15%|█▌        | 38/247 [01:03<05:53,  1.69s/it]Loading trainS:  16%|█▌        | 39/247 [01:05<05:43,  1.65s/it]Loading trainS:  16%|█▌        | 40/247 [01:06<05:17,  1.54s/it]Loading trainS:  17%|█▋        | 41/247 [01:08<05:23,  1.57s/it]Loading trainS:  17%|█▋        | 42/247 [01:09<05:18,  1.56s/it]Loading trainS:  17%|█▋        | 43/247 [01:11<05:09,  1.52s/it]Loading trainS:  18%|█▊        | 44/247 [01:12<05:06,  1.51s/it]Loading trainS:  18%|█▊        | 45/247 [01:14<05:31,  1.64s/it]Loading trainS:  19%|█▊        | 46/247 [01:16<05:23,  1.61s/it]Loading trainS:  19%|█▉        | 47/247 [01:17<04:57,  1.49s/it]Loading trainS:  19%|█▉        | 48/247 [01:19<05:02,  1.52s/it]Loading trainS:  20%|█▉        | 49/247 [01:20<05:15,  1.59s/it]Loading trainS:  20%|██        | 50/247 [01:21<04:34,  1.39s/it]Loading trainS:  21%|██        | 51/247 [01:22<04:00,  1.23s/it]Loading trainS:  21%|██        | 52/247 [01:23<03:20,  1.03s/it]Loading trainS:  21%|██▏       | 53/247 [01:23<02:53,  1.12it/s]Loading trainS:  22%|██▏       | 54/247 [01:24<03:03,  1.05it/s]Loading trainS:  22%|██▏       | 55/247 [01:26<03:32,  1.11s/it]Loading trainS:  23%|██▎       | 56/247 [01:27<03:21,  1.06s/it]Loading trainS:  23%|██▎       | 57/247 [01:27<02:58,  1.07it/s]Loading trainS:  23%|██▎       | 58/247 [01:28<02:35,  1.21it/s]Loading trainS:  24%|██▍       | 59/247 [01:29<03:04,  1.02it/s]Loading trainS:  24%|██▍       | 60/247 [01:30<03:00,  1.04it/s]Loading trainS:  25%|██▍       | 61/247 [01:31<02:52,  1.08it/s]Loading trainS:  25%|██▌       | 62/247 [01:32<02:34,  1.19it/s]Loading trainS:  26%|██▌       | 63/247 [01:33<02:34,  1.19it/s]Loading trainS:  26%|██▌       | 64/247 [01:34<02:57,  1.03it/s]Loading trainS:  26%|██▋       | 65/247 [01:35<02:53,  1.05it/s]Loading trainS:  27%|██▋       | 66/247 [01:36<02:55,  1.03it/s]Loading trainS:  27%|██▋       | 67/247 [01:36<02:36,  1.15it/s]Loading trainS:  28%|██▊       | 68/247 [01:37<02:30,  1.19it/s]Loading trainS:  28%|██▊       | 69/247 [01:38<02:51,  1.04it/s]Loading trainS:  28%|██▊       | 70/247 [01:40<02:58,  1.01s/it]Loading trainS:  29%|██▊       | 71/247 [01:41<02:58,  1.01s/it]Loading trainS:  29%|██▉       | 72/247 [01:41<02:29,  1.17it/s]Loading trainS:  30%|██▉       | 73/247 [01:41<02:03,  1.41it/s]Loading trainS:  30%|██▉       | 74/247 [01:42<02:13,  1.29it/s]Loading trainS:  30%|███       | 75/247 [01:44<02:38,  1.08it/s]Loading trainS:  31%|███       | 76/247 [01:45<02:41,  1.06it/s]Loading trainS:  31%|███       | 77/247 [01:45<02:25,  1.17it/s]Loading trainS:  32%|███▏      | 78/247 [01:46<02:12,  1.28it/s]Loading trainS:  32%|███▏      | 79/247 [01:47<02:38,  1.06it/s]Loading trainS:  32%|███▏      | 80/247 [01:48<02:23,  1.16it/s]Loading trainS:  33%|███▎      | 81/247 [01:48<01:59,  1.39it/s]Loading trainS:  33%|███▎      | 82/247 [01:49<01:52,  1.46it/s]Loading trainS:  34%|███▎      | 83/247 [01:50<01:55,  1.42it/s]Loading trainS:  34%|███▍      | 84/247 [01:50<01:47,  1.52it/s]Loading trainS:  34%|███▍      | 85/247 [01:51<01:57,  1.37it/s]Loading trainS:  35%|███▍      | 86/247 [01:51<01:42,  1.58it/s]Loading trainS:  35%|███▌      | 87/247 [01:52<01:40,  1.59it/s]Loading trainS:  36%|███▌      | 88/247 [01:53<01:38,  1.62it/s]Loading trainS:  36%|███▌      | 89/247 [01:54<02:00,  1.32it/s]Loading trainS:  36%|███▋      | 90/247 [01:55<02:02,  1.28it/s]Loading trainS:  37%|███▋      | 91/247 [01:56<02:07,  1.22it/s]Loading trainS:  37%|███▋      | 92/247 [01:56<02:11,  1.18it/s]Loading trainS:  38%|███▊      | 93/247 [01:57<01:51,  1.38it/s]Loading trainS:  38%|███▊      | 94/247 [01:57<01:41,  1.51it/s]Loading trainS:  38%|███▊      | 95/247 [01:58<01:50,  1.37it/s]Loading trainS:  39%|███▉      | 96/247 [01:59<01:33,  1.62it/s]Loading trainS:  39%|███▉      | 97/247 [01:59<01:40,  1.49it/s]Loading trainS:  40%|███▉      | 98/247 [02:00<01:51,  1.33it/s]Loading trainS:  40%|████      | 99/247 [02:01<01:45,  1.40it/s]Loading trainS:  40%|████      | 100/247 [02:02<01:45,  1.39it/s]Loading trainS:  41%|████      | 101/247 [02:02<01:40,  1.45it/s]Loading trainS:  41%|████▏     | 102/247 [02:04<02:17,  1.06it/s]Loading trainS:  42%|████▏     | 103/247 [02:05<02:23,  1.00it/s]Loading trainS:  42%|████▏     | 104/247 [02:06<02:30,  1.05s/it]Loading trainS:  43%|████▎     | 105/247 [02:07<02:08,  1.11it/s]Loading trainS:  43%|████▎     | 106/247 [02:07<01:55,  1.22it/s]Loading trainS:  43%|████▎     | 107/247 [02:09<02:24,  1.03s/it]Loading trainS:  44%|████▎     | 108/247 [02:10<02:32,  1.09s/it]Loading trainS:  44%|████▍     | 109/247 [02:11<02:31,  1.10s/it]Loading trainS:  45%|████▍     | 110/247 [02:12<02:07,  1.07it/s]Loading trainS:  45%|████▍     | 111/247 [02:12<01:46,  1.28it/s]Loading trainS:  45%|████▌     | 112/247 [02:14<02:20,  1.04s/it]Loading trainS:  46%|████▌     | 113/247 [02:15<02:40,  1.20s/it]Loading trainS:  46%|████▌     | 114/247 [02:16<02:32,  1.15s/it]Loading trainS:  47%|████▋     | 115/247 [02:17<02:20,  1.07s/it]Loading trainS:  47%|████▋     | 116/247 [02:18<02:03,  1.06it/s]Loading trainS:  47%|████▋     | 117/247 [02:19<01:45,  1.23it/s]Loading trainS:  48%|████▊     | 118/247 [02:19<01:35,  1.35it/s]Loading trainS:  48%|████▊     | 119/247 [02:20<01:58,  1.08it/s]Loading trainS:  49%|████▊     | 120/247 [02:21<01:58,  1.07it/s]Loading trainS:  49%|████▉     | 121/247 [02:23<02:04,  1.02it/s]Loading trainS:  49%|████▉     | 122/247 [02:23<01:57,  1.07it/s]Loading trainS:  50%|████▉     | 123/247 [02:24<01:43,  1.19it/s]Loading trainS:  50%|█████     | 124/247 [02:24<01:28,  1.39it/s]Loading trainS:  51%|█████     | 125/247 [02:25<01:22,  1.48it/s]Loading trainS:  51%|█████     | 126/247 [02:26<01:34,  1.28it/s]Loading trainS:  51%|█████▏    | 127/247 [02:27<01:33,  1.28it/s]Loading trainS:  52%|█████▏    | 128/247 [02:28<01:45,  1.13it/s]Loading trainS:  52%|█████▏    | 129/247 [02:29<01:39,  1.19it/s]Loading trainS:  53%|█████▎    | 130/247 [02:29<01:37,  1.20it/s]Loading trainS:  53%|█████▎    | 131/247 [02:30<01:30,  1.28it/s]Loading trainS:  53%|█████▎    | 132/247 [02:31<01:20,  1.42it/s]Loading trainS:  54%|█████▍    | 133/247 [02:31<01:10,  1.62it/s]Loading trainS:  54%|█████▍    | 134/247 [02:32<01:30,  1.25it/s]Loading trainS:  55%|█████▍    | 135/247 [02:33<01:24,  1.32it/s]Loading trainS:  55%|█████▌    | 136/247 [02:34<01:28,  1.25it/s]Loading trainS:  55%|█████▌    | 137/247 [02:35<01:30,  1.21it/s]Loading trainS:  56%|█████▌    | 138/247 [02:35<01:28,  1.23it/s]Loading trainS:  56%|█████▋    | 139/247 [02:36<01:23,  1.30it/s]Loading trainS:  57%|█████▋    | 140/247 [02:37<01:16,  1.40it/s]Loading trainS:  57%|█████▋    | 141/247 [02:37<01:07,  1.58it/s]Loading trainS:  57%|█████▋    | 142/247 [02:38<01:01,  1.70it/s]Loading trainS:  58%|█████▊    | 143/247 [02:38<00:54,  1.92it/s]Loading trainS:  58%|█████▊    | 144/247 [02:38<00:48,  2.14it/s]Loading trainS:  59%|█████▊    | 145/247 [02:39<00:44,  2.31it/s]Loading trainS:  59%|█████▉    | 146/247 [02:39<00:41,  2.45it/s]Loading trainS:  60%|█████▉    | 147/247 [02:40<00:48,  2.05it/s]Loading trainS:  60%|█████▉    | 148/247 [02:40<00:47,  2.10it/s]Loading trainS:  60%|██████    | 149/247 [02:41<00:55,  1.77it/s]Loading trainS:  61%|██████    | 150/247 [02:42<00:59,  1.62it/s]Loading trainS:  61%|██████    | 151/247 [02:42<00:57,  1.68it/s]Loading trainS:  62%|██████▏   | 152/247 [02:43<01:03,  1.51it/s]Loading trainS:  62%|██████▏   | 153/247 [02:44<01:06,  1.42it/s]Loading trainS:  62%|██████▏   | 154/247 [02:44<01:00,  1.53it/s]Loading trainS:  63%|██████▎   | 155/247 [02:45<00:53,  1.74it/s]Loading trainS:  63%|██████▎   | 156/247 [02:45<00:48,  1.89it/s]Loading trainS:  64%|██████▎   | 157/247 [02:46<00:44,  2.00it/s]Loading trainS:  64%|██████▍   | 158/247 [02:46<00:42,  2.08it/s]Loading trainS:  64%|██████▍   | 159/247 [02:47<00:47,  1.87it/s]Loading trainS:  65%|██████▍   | 160/247 [02:47<00:46,  1.89it/s]Loading trainS:  65%|██████▌   | 161/247 [02:48<00:48,  1.76it/s]Loading trainS:  66%|██████▌   | 162/247 [02:49<00:51,  1.66it/s]Loading trainS:  66%|██████▌   | 163/247 [02:49<00:49,  1.69it/s]Loading trainS:  66%|██████▋   | 164/247 [02:50<00:52,  1.59it/s]Loading trainS:  67%|██████▋   | 165/247 [02:51<00:52,  1.58it/s]Loading trainS:  67%|██████▋   | 166/247 [02:51<00:49,  1.65it/s]Loading trainS:  68%|██████▊   | 167/247 [02:52<00:46,  1.72it/s]Loading trainS:  68%|██████▊   | 168/247 [02:52<00:41,  1.92it/s]Loading trainS:  68%|██████▊   | 169/247 [02:52<00:38,  2.00it/s]Loading trainS:  69%|██████▉   | 170/247 [02:53<00:37,  2.05it/s]Loading trainS:  69%|██████▉   | 171/247 [02:53<00:35,  2.16it/s]Loading trainS:  70%|██████▉   | 172/247 [02:54<00:35,  2.12it/s]Loading trainS:  70%|███████   | 173/247 [02:54<00:36,  2.01it/s]Loading trainS:  70%|███████   | 174/247 [02:55<00:38,  1.89it/s]Loading trainS:  71%|███████   | 175/247 [02:56<00:43,  1.67it/s]Loading trainS:  71%|███████▏  | 176/247 [02:56<00:42,  1.66it/s]Loading trainS:  72%|███████▏  | 177/247 [02:57<00:43,  1.63it/s]Loading trainS:  72%|███████▏  | 178/247 [02:58<00:42,  1.64it/s]Loading trainS:  72%|███████▏  | 179/247 [02:58<00:38,  1.75it/s]Loading trainS:  73%|███████▎  | 180/247 [02:59<00:37,  1.78it/s]Loading trainS:  73%|███████▎  | 181/247 [02:59<00:34,  1.93it/s]Loading trainS:  74%|███████▎  | 182/247 [02:59<00:32,  2.02it/s]Loading trainS:  74%|███████▍  | 183/247 [03:00<00:31,  2.02it/s]Loading trainS:  74%|███████▍  | 184/247 [03:00<00:28,  2.21it/s]Loading trainS:  75%|███████▍  | 185/247 [03:01<00:29,  2.13it/s]Loading trainS:  75%|███████▌  | 186/247 [03:01<00:28,  2.12it/s]Loading trainS:  76%|███████▌  | 187/247 [03:02<00:29,  2.02it/s]Loading trainS:  76%|███████▌  | 188/247 [03:02<00:29,  1.99it/s]Loading trainS:  77%|███████▋  | 189/247 [03:03<00:30,  1.93it/s]Loading trainS:  77%|███████▋  | 190/247 [03:03<00:29,  1.96it/s]Loading trainS:  77%|███████▋  | 191/247 [03:04<00:28,  1.97it/s]Loading trainS:  78%|███████▊  | 192/247 [03:04<00:27,  1.97it/s]Loading trainS:  78%|███████▊  | 193/247 [03:05<00:28,  1.93it/s]Loading trainS:  79%|███████▊  | 194/247 [03:05<00:27,  1.96it/s]Loading trainS:  79%|███████▉  | 195/247 [03:06<00:27,  1.91it/s]Loading trainS:  79%|███████▉  | 196/247 [03:07<00:27,  1.86it/s]Loading trainS:  80%|███████▉  | 197/247 [03:07<00:25,  1.96it/s]Loading trainS:  80%|████████  | 198/247 [03:08<00:25,  1.95it/s]Loading trainS:  81%|████████  | 199/247 [03:08<00:24,  1.98it/s]Loading trainS:  81%|████████  | 200/247 [03:08<00:22,  2.11it/s]Loading trainS:  81%|████████▏ | 201/247 [03:09<00:20,  2.28it/s]Loading trainS:  82%|████████▏ | 202/247 [03:09<00:18,  2.42it/s]Loading trainS:  82%|████████▏ | 203/247 [03:10<00:19,  2.20it/s]Loading trainS:  83%|████████▎ | 204/247 [03:10<00:23,  1.87it/s]Loading trainS:  83%|████████▎ | 205/247 [03:11<00:24,  1.75it/s]Loading trainS:  83%|████████▎ | 206/247 [03:12<00:23,  1.77it/s]Loading trainS:  84%|████████▍ | 207/247 [03:12<00:22,  1.74it/s]Loading trainS:  84%|████████▍ | 208/247 [03:13<00:22,  1.70it/s]Loading trainS:  85%|████████▍ | 209/247 [03:14<00:24,  1.56it/s]Loading trainS:  85%|████████▌ | 210/247 [03:14<00:23,  1.58it/s]Loading trainS:  85%|████████▌ | 211/247 [03:15<00:22,  1.59it/s]Loading trainS:  86%|████████▌ | 212/247 [03:15<00:21,  1.61it/s]Loading trainS:  86%|████████▌ | 213/247 [03:16<00:22,  1.53it/s]Loading trainS:  87%|████████▋ | 214/247 [03:17<00:21,  1.52it/s]Loading trainS:  87%|████████▋ | 215/247 [03:17<00:20,  1.53it/s]Loading trainS:  87%|████████▋ | 216/247 [03:18<00:21,  1.47it/s]Loading trainS:  88%|████████▊ | 217/247 [03:19<00:21,  1.38it/s]Loading trainS:  88%|████████▊ | 218/247 [03:20<00:19,  1.47it/s]Loading trainS:  89%|████████▊ | 219/247 [03:20<00:18,  1.48it/s]Loading trainS:  89%|████████▉ | 220/247 [03:21<00:17,  1.51it/s]Loading trainS:  89%|████████▉ | 221/247 [03:22<00:17,  1.48it/s]Loading trainS:  90%|████████▉ | 222/247 [03:22<00:17,  1.43it/s]Loading trainS:  90%|█████████ | 223/247 [03:23<00:16,  1.44it/s]Loading trainS:  91%|█████████ | 224/247 [03:24<00:15,  1.50it/s]Loading trainS:  91%|█████████ | 225/247 [03:24<00:14,  1.49it/s]Loading trainS:  91%|█████████▏| 226/247 [03:25<00:13,  1.51it/s]Loading trainS:  92%|█████████▏| 227/247 [03:26<00:14,  1.43it/s]Loading trainS:  92%|█████████▏| 228/247 [03:26<00:12,  1.51it/s]Loading trainS:  93%|█████████▎| 229/247 [03:27<00:12,  1.49it/s]Loading trainS:  93%|█████████▎| 230/247 [03:28<00:11,  1.43it/s]Loading trainS:  94%|█████████▎| 231/247 [03:29<00:11,  1.42it/s]Loading trainS:  94%|█████████▍| 232/247 [03:29<00:10,  1.41it/s]Loading trainS:  94%|█████████▍| 233/247 [03:30<00:10,  1.38it/s]Loading trainS:  95%|█████████▍| 234/247 [03:31<00:09,  1.39it/s]Loading trainS:  95%|█████████▌| 235/247 [03:32<00:09,  1.32it/s]Loading trainS:  96%|█████████▌| 236/247 [03:32<00:08,  1.33it/s]Loading trainS:  96%|█████████▌| 237/247 [03:33<00:07,  1.32it/s]Loading trainS:  96%|█████████▋| 238/247 [03:34<00:06,  1.30it/s]Loading trainS:  97%|█████████▋| 239/247 [03:35<00:05,  1.35it/s]Loading trainS:  97%|█████████▋| 240/247 [03:35<00:05,  1.30it/s]Loading trainS:  98%|█████████▊| 241/247 [03:36<00:04,  1.35it/s]Loading trainS:  98%|█████████▊| 242/247 [03:37<00:03,  1.37it/s]Loading trainS:  98%|█████████▊| 243/247 [03:37<00:02,  1.38it/s]Loading trainS:  99%|█████████▉| 244/247 [03:38<00:02,  1.31it/s]Loading trainS:  99%|█████████▉| 245/247 [03:39<00:01,  1.32it/s]Loading trainS: 100%|█████████▉| 246/247 [03:40<00:00,  1.32it/s]Loading trainS: 100%|██████████| 247/247 [03:41<00:00,  1.36it/s]Loading trainS: 100%|██████████| 247/247 [03:41<00:00,  1.12it/s]
Loading testS:   0%|          | 0/5 [00:00<?, ?it/s]Loading testS:  20%|██        | 1/5 [00:00<00:03,  1.22it/s]Loading testS:  40%|████      | 2/5 [00:01<00:02,  1.25it/s]Loading testS:  60%|██████    | 3/5 [00:02<00:01,  1.30it/s]Loading testS:  80%|████████  | 4/5 [00:02<00:00,  1.38it/s]Loading testS: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]Loading testS: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]----------+++ 
CrossVal ['b']
CrossVal ['b']
(0/5) test vimp2_ANON911_CSFn2
(1/5) test vimp2_D_CSFn2
(2/5) test vimp2_F_CSFn2
(3/5) test vimp2_G_CSFn2
(4/5) test vimp2_J_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97300229 0.02699771]
Train on 15751 samples, validate on 333 samples
Epoch 1/300
 - 75s - loss: 0.1621 - acc: 0.9818 - mDice: 0.6855 - val_loss: 0.2686 - val_acc: 0.9819 - val_mDice: 0.3943

Epoch 00001: val_mDice improved from -inf to 0.39431, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 71s - loss: 0.0986 - acc: 0.9895 - mDice: 0.8083 - val_loss: 0.2946 - val_acc: 0.9808 - val_mDice: 0.3984

Epoch 00002: val_mDice improved from 0.39431 to 0.39840, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 71s - loss: 0.0857 - acc: 0.9909 - mDice: 0.8334 - val_loss: 0.2849 - val_acc: 0.9876 - val_mDice: 0.4184

Epoch 00003: val_mDice improved from 0.39840 to 0.41839, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300
 - 71s - loss: 0.0769 - acc: 0.9918 - mDice: 0.8504 - val_loss: 0.2836 - val_acc: 0.9871 - val_mDice: 0.4200

Epoch 00004: val_mDice improved from 0.41839 to 0.42000, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300
 - 72s - loss: 0.0733 - acc: 0.9923 - mDice: 0.8573 - val_loss: 0.2783 - val_acc: 0.9902 - val_mDice: 0.4296

Epoch 00005: val_mDice improved from 0.42000 to 0.42960, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300
 - 72s - loss: 0.0692 - acc: 0.9927 - mDice: 0.8653 - val_loss: 0.2645 - val_acc: 0.9900 - val_mDice: 0.4348

Epoch 00006: val_mDice improved from 0.42960 to 0.43476, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/300
 - 72s - loss: 0.0654 - acc: 0.9930 - mDice: 0.8729 - val_loss: 0.2483 - val_acc: 0.9883 - val_mDice: 0.4344

Epoch 00007: val_mDice did not improve from 0.43476
Epoch 8/300
 - 72s - loss: 0.0651 - acc: 0.9932 - mDice: 0.8732 - val_loss: 0.2409 - val_acc: 0.9896 - val_mDice: 0.4414

Epoch 00008: val_mDice improved from 0.43476 to 0.44137, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 9/300
 - 72s - loss: 0.0621 - acc: 0.9934 - mDice: 0.8792 - val_loss: 0.2395 - val_acc: 0.9904 - val_mDice: 0.4474

Epoch 00009: val_mDice improved from 0.44137 to 0.44740, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 10/300
 - 73s - loss: 0.0610 - acc: 0.9936 - mDice: 0.8814 - val_loss: 0.2413 - val_acc: 0.9893 - val_mDice: 0.4509

Epoch 00010: val_mDice improved from 0.44740 to 0.45087, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 11/300
 - 72s - loss: 0.0592 - acc: 0.9937 - mDice: 0.8848 - val_loss: 0.2405 - val_acc: 0.9871 - val_mDice: 0.4460

Epoch 00011: val_mDice did not improve from 0.45087
Epoch 12/300
 - 73s - loss: 0.0573 - acc: 0.9938 - mDice: 0.8886 - val_loss: 0.2299 - val_acc: 0.9902 - val_mDice: 0.4551

Epoch 00012: val_mDice improved from 0.45087 to 0.45511, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 13/300
 - 72s - loss: 0.0562 - acc: 0.9939 - mDice: 0.8907 - val_loss: 0.2309 - val_acc: 0.9909 - val_mDice: 0.4615

Epoch 00013: val_mDice improved from 0.45511 to 0.46148, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 14/300
 - 73s - loss: 0.0543 - acc: 0.9941 - mDice: 0.8944 - val_loss: 0.1991 - val_acc: 0.9870 - val_mDice: 0.4516

Epoch 00014: val_mDice did not improve from 0.46148
Epoch 15/300
 - 72s - loss: 0.0533 - acc: 0.9942 - mDice: 0.8964 - val_loss: 0.1984 - val_acc: 0.9901 - val_mDice: 0.4662

Epoch 00015: val_mDice improved from 0.46148 to 0.46622, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 16/300
 - 72s - loss: 0.0541 - acc: 0.9942 - mDice: 0.8947 - val_loss: 0.1695 - val_acc: 0.9910 - val_mDice: 0.4710

Epoch 00016: val_mDice improved from 0.46622 to 0.47100, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 17/300
 - 72s - loss: 0.0528 - acc: 0.9943 - mDice: 0.8973 - val_loss: 0.1691 - val_acc: 0.9911 - val_mDice: 0.4750

Epoch 00017: val_mDice improved from 0.47100 to 0.47505, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 18/300
 - 72s - loss: 0.0506 - acc: 0.9944 - mDice: 0.9015 - val_loss: 0.1946 - val_acc: 0.9916 - val_mDice: 0.4748

Epoch 00018: val_mDice did not improve from 0.47505
Epoch 19/300
 - 72s - loss: 0.0497 - acc: 0.9945 - mDice: 0.9034 - val_loss: 0.2111 - val_acc: 0.9900 - val_mDice: 0.4682

Epoch 00019: val_mDice did not improve from 0.47505
Epoch 20/300
 - 72s - loss: 0.0496 - acc: 0.9946 - mDice: 0.9036 - val_loss: 0.2069 - val_acc: 0.9903 - val_mDice: 0.4695

Epoch 00020: val_mDice did not improve from 0.47505
Epoch 21/300
 - 72s - loss: 0.0495 - acc: 0.9946 - mDice: 0.9037 - val_loss: 0.2036 - val_acc: 0.9885 - val_mDice: 0.4624

Epoch 00021: val_mDice did not improve from 0.47505
Epoch 22/300
 - 73s - loss: 0.0472 - acc: 0.9947 - mDice: 0.9083 - val_loss: 0.1932 - val_acc: 0.9913 - val_mDice: 0.4757

Epoch 00022: val_mDice improved from 0.47505 to 0.47571, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 23/300
 - 73s - loss: 0.0464 - acc: 0.9948 - mDice: 0.9099 - val_loss: 0.1547 - val_acc: 0.9917 - val_mDice: 0.4760

Epoch 00023: val_mDice improved from 0.47571 to 0.47601, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 24/300
 - 72s - loss: 0.0465 - acc: 0.9948 - mDice: 0.9097 - val_loss: 0.1905 - val_acc: 0.9921 - val_mDice: 0.4785

Epoch 00024: val_mDice improved from 0.47601 to 0.47852, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 25/300
 - 72s - loss: 0.0452 - acc: 0.9949 - mDice: 0.9122 - val_loss: 0.1968 - val_acc: 0.9918 - val_mDice: 0.4783

Epoch 00025: val_mDice did not improve from 0.47852
Epoch 26/300
 - 72s - loss: 0.0455 - acc: 0.9949 - mDice: 0.9115 - val_loss: 0.1996 - val_acc: 0.9910 - val_mDice: 0.4691

Epoch 00026: val_mDice did not improve from 0.47852
Epoch 27/300
 - 72s - loss: 0.0448 - acc: 0.9950 - mDice: 0.9129 - val_loss: 0.1719 - val_acc: 0.9914 - val_mDice: 0.4730

Epoch 00027: val_mDice did not improve from 0.47852
Epoch 28/300
 - 72s - loss: 0.0434 - acc: 0.9950 - mDice: 0.9158 - val_loss: 0.1809 - val_acc: 0.9915 - val_mDice: 0.4759

Epoch 00028: val_mDice did not improve from 0.47852
Epoch 29/300
 - 72s - loss: 0.0435 - acc: 0.9951 - mDice: 0.9154 - val_loss: 0.2144 - val_acc: 0.9911 - val_mDice: 0.4722

Epoch 00029: val_mDice did not improve from 0.47852
Epoch 30/300
 - 73s - loss: 0.0427 - acc: 0.9951 - mDice: 0.9170 - val_loss: 0.1329 - val_acc: 0.9904 - val_mDice: 0.4677

Epoch 00030: val_mDice did not improve from 0.47852
Epoch 31/300
 - 73s - loss: 0.0444 - acc: 0.9951 - mDice: 0.9137 - val_loss: 0.1516 - val_acc: 0.9921 - val_mDice: 0.4771

Epoch 00031: val_mDice did not improve from 0.47852
Epoch 32/300
 - 73s - loss: 0.0426 - acc: 0.9952 - mDice: 0.9173 - val_loss: 0.1436 - val_acc: 0.9918 - val_mDice: 0.4776

Epoch 00032: val_mDice did not improve from 0.47852
Epoch 33/300
 - 72s - loss: 0.0418 - acc: 0.9952 - mDice: 0.9189 - val_loss: 0.1713 - val_acc: 0.9926 - val_mDice: 0.4761

Epoch 00033: val_mDice did not improve from 0.47852
Epoch 34/300
 - 72s - loss: 0.0423 - acc: 0.9952 - mDice: 0.9177 - val_loss: 0.2404 - val_acc: 0.9912 - val_mDice: 0.4771

Epoch 00034: val_mDice did not improve from 0.47852
Epoch 35/300
 - 72s - loss: 0.0408 - acc: 0.9953 - mDice: 0.9208 - val_loss: 0.1353 - val_acc: 0.9919 - val_mDice: 0.4789

Epoch 00035: val_mDice improved from 0.47852 to 0.47889, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 36/300
 - 72s - loss: 0.0407 - acc: 0.9953 - mDice: 0.9210 - val_loss: 0.1592 - val_acc: 0.9922 - val_mDice: 0.4825

Epoch 00036: val_mDice improved from 0.47889 to 0.48252, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 37/300
 - 72s - loss: 0.0403 - acc: 0.9953 - mDice: 0.9217 - val_loss: 0.1196 - val_acc: 0.9920 - val_mDice: 0.4788

Epoch 00037: val_mDice did not improve from 0.48252
Epoch 38/300
 - 72s - loss: 0.0397 - acc: 0.9954 - mDice: 0.9230 - val_loss: 0.1051 - val_acc: 0.9919 - val_mDice: 0.4832

Epoch 00038: val_mDice improved from 0.48252 to 0.48315, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 39/300
 - 73s - loss: 0.0402 - acc: 0.9954 - mDice: 0.9219 - val_loss: 0.0963 - val_acc: 0.9923 - val_mDice: 0.4835

Epoch 00039: val_mDice improved from 0.48315 to 0.48352, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 40/300
 - 73s - loss: 0.0395 - acc: 0.9954 - mDice: 0.9233 - val_loss: 0.1598 - val_acc: 0.9916 - val_mDice: 0.4831

Epoch 00040: val_mDice did not improve from 0.48352
Epoch 41/300
 - 73s - loss: 0.0392 - acc: 0.9954 - mDice: 0.9239 - val_loss: 0.0950 - val_acc: 0.9926 - val_mDice: 0.4860

Epoch 00041: val_mDice improved from 0.48352 to 0.48603, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 42/300
 - 73s - loss: 0.0401 - acc: 0.9954 - mDice: 0.9221 - val_loss: 0.1311 - val_acc: 0.9914 - val_mDice: 0.4818

Epoch 00042: val_mDice did not improve from 0.48603
Epoch 43/300
 - 73s - loss: 0.0382 - acc: 0.9955 - mDice: 0.9258 - val_loss: 0.2010 - val_acc: 0.9906 - val_mDice: 0.4787

Epoch 00043: val_mDice did not improve from 0.48603
Epoch 44/300
 - 73s - loss: 0.0385 - acc: 0.9955 - mDice: 0.9252 - val_loss: 0.2105 - val_acc: 0.9910 - val_mDice: 0.4751

Epoch 00044: val_mDice did not improve from 0.48603
Epoch 45/300
 - 73s - loss: 0.0388 - acc: 0.9956 - mDice: 0.9246 - val_loss: 0.1621 - val_acc: 0.9920 - val_mDice: 0.4795

Epoch 00045: val_mDice did not improve from 0.48603
Epoch 46/300
 - 73s - loss: 0.0380 - acc: 0.9956 - mDice: 0.9263 - val_loss: 0.1625 - val_acc: 0.9912 - val_mDice: 0.4758

Epoch 00046: val_mDice did not improve from 0.48603
Epoch 47/300
 - 73s - loss: 0.0372 - acc: 0.9956 - mDice: 0.9277 - val_loss: 0.1543 - val_acc: 0.9924 - val_mDice: 0.4842

Epoch 00047: val_mDice did not improve from 0.48603
Epoch 48/300
 - 73s - loss: 0.0377 - acc: 0.9956 - mDice: 0.9269 - val_loss: 0.1283 - val_acc: 0.9915 - val_mDice: 0.4743

Epoch 00048: val_mDice did not improve from 0.48603
Epoch 49/300
 - 73s - loss: 0.0371 - acc: 0.9956 - mDice: 0.9280 - val_loss: 0.1554 - val_acc: 0.9919 - val_mDice: 0.4782

Epoch 00049: val_mDice did not improve from 0.48603
Epoch 50/300
 - 73s - loss: 0.0370 - acc: 0.9957 - mDice: 0.9282 - val_loss: 0.1702 - val_acc: 0.9914 - val_mDice: 0.4793

Epoch 00050: val_mDice did not improve from 0.48603
Epoch 51/300
 - 73s - loss: 0.0362 - acc: 0.9957 - mDice: 0.9298 - val_loss: 0.1684 - val_acc: 0.9921 - val_mDice: 0.4803

Epoch 00051: val_mDice did not improve from 0.48603
Epoch 52/300
 - 73s - loss: 0.0364 - acc: 0.9957 - mDice: 0.9294 - val_loss: 0.1280 - val_acc: 0.9921 - val_mDice: 0.4772

Epoch 00052: val_mDice did not improve from 0.48603
Epoch 53/300
 - 73s - loss: 0.0370 - acc: 0.9957 - mDice: 0.9282 - val_loss: 0.1227 - val_acc: 0.9922 - val_mDice: 0.4770

Epoch 00053: val_mDice did not improve from 0.48603
Epoch 54/300
 - 73s - loss: 0.0358 - acc: 0.9957 - mDice: 0.9305 - val_loss: 0.1825 - val_acc: 0.9922 - val_mDice: 0.4787

Epoch 00054: val_mDice did not improve from 0.48603
Epoch 55/300
 - 73s - loss: 0.0362 - acc: 0.9957 - mDice: 0.9298 - val_loss: 0.0955 - val_acc: 0.9923 - val_mDice: 0.4726

Epoch 00055: val_mDice did not improve from 0.48603
Epoch 56/300
 - 73s - loss: 0.0360 - acc: 0.9958 - mDice: 0.9300 - val_loss: 0.1332 - val_acc: 0.9910 - val_mDice: 0.4746

Epoch 00056: val_mDice did not improve from 0.48603

Epoch 00056: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 57/300
 - 73s - loss: 0.0359 - acc: 0.9958 - mDice: 0.9304 - val_loss: 0.1301 - val_acc: 0.9925 - val_mDice: 0.4795

Epoch 00057: val_mDice did not improve from 0.48603
Epoch 58/300
 - 73s - loss: 0.0352 - acc: 0.9958 - mDice: 0.9316 - val_loss: 0.0923 - val_acc: 0.9922 - val_mDice: 0.4769

Epoch 00058: val_mDice did not improve from 0.48603
Epoch 59/300
 - 73s - loss: 0.0349 - acc: 0.9958 - mDice: 0.9322 - val_loss: 0.1551 - val_acc: 0.9922 - val_mDice: 0.4818

Epoch 00059: val_mDice did not improve from 0.48603
Epoch 60/300
 - 73s - loss: 0.0348 - acc: 0.9958 - mDice: 0.9324 - val_loss: 0.1239 - val_acc: 0.9924 - val_mDice: 0.4791

Epoch 00060: val_mDice did not improve from 0.48603
Epoch 61/300
 - 73s - loss: 0.0343 - acc: 0.9958 - mDice: 0.9334 - val_loss: 0.0906 - val_acc: 0.9920 - val_mDice: 0.4775

Epoch 00061: val_mDice did not improve from 0.48603
Epoch 62/300
 - 73s - loss: 0.0344 - acc: 0.9958 - mDice: 0.9332 - val_loss: 0.1281 - val_acc: 0.9922 - val_mDice: 0.4784

Epoch 00062: val_mDice did not improve from 0.48603
Epoch 63/300
 - 73s - loss: 0.0344 - acc: 0.9958 - mDice: 0.9332 - val_loss: 0.1416 - val_acc: 0.9928 - val_mDice: 0.4795

Epoch 00063: val_mDice did not improve from 0.48603
Epoch 64/300
 - 73s - loss: 0.0351 - acc: 0.9959 - mDice: 0.9318 - val_loss: 0.1265 - val_acc: 0.9926 - val_mDice: 0.4819

Epoch 00064: val_mDice did not improve from 0.48603
Epoch 65/300
 - 73s - loss: 0.0337 - acc: 0.9959 - mDice: 0.9345 - val_loss: 0.1075 - val_acc: 0.9924 - val_mDice: 0.4798

Epoch 00065: val_mDice did not improve from 0.48603
Epoch 66/300
 - 73s - loss: 0.0335 - acc: 0.9959 - mDice: 0.9349 - val_loss: 0.1125 - val_acc: 0.9921 - val_mDice: 0.4813

Epoch 00066: val_mDice did not improve from 0.48603
Epoch 67/300
 - 73s - loss: 0.0345 - acc: 0.9959 - mDice: 0.9330 - val_loss: 0.1075 - val_acc: 0.9917 - val_mDice: 0.4794

Epoch 00067: val_mDice did not improve from 0.48603
Epoch 68/300
 - 74s - loss: 0.0342 - acc: 0.9959 - mDice: 0.9335 - val_loss: 0.0844 - val_acc: 0.9925 - val_mDice: 0.4761

Epoch 00068: val_mDice did not improve from 0.48603
Epoch 69/300
 - 72s - loss: 0.0336 - acc: 0.9959 - mDice: 0.9348 - val_loss: 0.1362 - val_acc: 0.9923 - val_mDice: 0.4790

Epoch 00069: val_mDice did not improve from 0.48603
Epoch 70/300
 - 72s - loss: 0.0349 - acc: 0.9959 - mDice: 0.9323 - val_loss: 0.0837 - val_acc: 0.9922 - val_mDice: 0.4754

Epoch 00070: val_mDice did not improve from 0.48603
Epoch 71/300
 - 72s - loss: 0.0336 - acc: 0.9959 - mDice: 0.9348 - val_loss: 0.0870 - val_acc: 0.9919 - val_mDice: 0.4762

Epoch 00071: val_mDice did not improve from 0.48603

Epoch 00071: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 72/300
 - 73s - loss: 0.0334 - acc: 0.9960 - mDice: 0.9352 - val_loss: 0.1199 - val_acc: 0.9920 - val_mDice: 0.4772

Epoch 00072: val_mDice did not improve from 0.48603
Epoch 73/300
 - 73s - loss: 0.0337 - acc: 0.9960 - mDice: 0.9346 - val_loss: 0.0827 - val_acc: 0.9918 - val_mDice: 0.4750

Epoch 00073: val_mDice did not improve from 0.48603
Epoch 74/300
 - 73s - loss: 0.0334 - acc: 0.9960 - mDice: 0.9352 - val_loss: 0.0978 - val_acc: 0.9919 - val_mDice: 0.4770

Epoch 00074: val_mDice did not improve from 0.48603
Epoch 75/300
 - 72s - loss: 0.0332 - acc: 0.9959 - mDice: 0.9355 - val_loss: 0.0789 - val_acc: 0.9921 - val_mDice: 0.4762

Epoch 00075: val_mDice did not improve from 0.48603
Epoch 76/300
 - 72s - loss: 0.0330 - acc: 0.9960 - mDice: 0.9360 - val_loss: 0.1402 - val_acc: 0.9923 - val_mDice: 0.4777

Epoch 00076: val_mDice did not improve from 0.48603
Epoch 77/300
 - 72s - loss: 0.0327 - acc: 0.9960 - mDice: 0.9367 - val_loss: 0.1152 - val_acc: 0.9925 - val_mDice: 0.4820

Epoch 00077: val_mDice did not improve from 0.48603
Epoch 78/300
 - 73s - loss: 0.0334 - acc: 0.9960 - mDice: 0.9352 - val_loss: 0.1413 - val_acc: 0.9924 - val_mDice: 0.4786

Epoch 00078: val_mDice did not improve from 0.48603
Epoch 79/300
 - 72s - loss: 0.0336 - acc: 0.9960 - mDice: 0.9348 - val_loss: 0.1112 - val_acc: 0.9921 - val_mDice: 0.4791

Epoch 00079: val_mDice did not improve from 0.48603
Epoch 80/300
 - 73s - loss: 0.0328 - acc: 0.9960 - mDice: 0.9363 - val_loss: 0.1081 - val_acc: 0.9920 - val_mDice: 0.4754

Epoch 00080: val_mDice did not improve from 0.48603
Epoch 81/300
 - 73s - loss: 0.0327 - acc: 0.9960 - mDice: 0.9366 - val_loss: 0.0921 - val_acc: 0.9920 - val_mDice: 0.4758

Epoch 00081: val_mDice did not improve from 0.48603
Restoring model weights from the end of the best epoch
Epoch 00081: early stopping
{'val_loss': [0.26857479064314216, 0.2945702331918138, 0.28488925958538913, 0.2836101979643733, 0.2783307299778626, 0.26450475056966144, 0.24826891961935404, 0.24086899236515835, 0.2394967065738128, 0.24127895758674667, 0.24046824760623164, 0.22990663980578516, 0.23092500968380375, 0.1991231928806047, 0.19836409430246096, 0.16953518346802252, 0.16912620901703476, 0.19463317288650764, 0.2110630110935406, 0.20694503197082886, 0.20360888151435164, 0.19322063937201514, 0.15466721074001208, 0.19047936582350516, 0.19677549385809684, 0.19964186851684754, 0.17189672517704893, 0.18086370767594817, 0.2144211847502906, 0.13291822781075943, 0.15160340920940893, 0.14358914301202103, 0.17127641055498039, 0.2404270784274952, 0.13528458259485146, 0.15918523672822718, 0.119612610375917, 0.10513294915537219, 0.09634284348459216, 0.1597555465884395, 0.0949824839919895, 0.13105645247765849, 0.20097764163045911, 0.21045151227587336, 0.1621127294169532, 0.162477906222816, 0.15425125569910617, 0.12831282069733194, 0.15542143502750913, 0.170202902218959, 0.16841945777068268, 0.127954517667358, 0.12265682251782746, 0.1824872041786755, 0.09546284328351866, 0.13323512324341782, 0.13008240250137834, 0.09227152531211441, 0.1551119829262341, 0.12390110030904547, 0.09059729438286286, 0.12808478827233072, 0.14157846780331643, 0.1265057146370232, 0.10748754334342372, 0.11250825365026434, 0.10746264099716782, 0.08438213997417027, 0.1361761051523793, 0.0836719577018921, 0.08700185620390975, 0.11987388080304807, 0.08274043483419104, 0.09778987206854262, 0.07888837190003724, 0.14016263585190875, 0.11523349301235096, 0.14126274983088175, 0.11115951348353435, 0.10807177330459561, 0.09205814609835455], 'val_acc': [0.981886103704527, 0.9807865787912775, 0.9875685044595072, 0.9871468633502811, 0.9902368880606987, 0.9900432087637641, 0.9883384371662999, 0.9896146155692436, 0.9904367998913601, 0.9892509860677404, 0.9870915044177402, 0.9902066865840832, 0.9909042202316605, 0.9869975349208614, 0.9900734084504502, 0.990991232273457, 0.9910703310379395, 0.9915612424100125, 0.9900494412974911, 0.9903349242768846, 0.9884788982861035, 0.9913054776263308, 0.9916930850203689, 0.992084036002288, 0.9918400185244219, 0.9909552725943717, 0.9913821868352346, 0.9915123451221455, 0.9910813570022583, 0.9903907736500462, 0.9920878771905068, 0.991814608688469, 0.9925958072101032, 0.9912491521319827, 0.9918529579231331, 0.9922072475736922, 0.992015725141531, 0.9918635077662654, 0.9923280570600126, 0.9915744306088926, 0.9925972427333798, 0.991397524739171, 0.9906081820393468, 0.9910084871916441, 0.9920020554516766, 0.9912271002033451, 0.9924083550771078, 0.991475431410758, 0.9919469202602947, 0.9914488276919803, 0.9920806763050435, 0.992103456734895, 0.9921650589407385, 0.9921636144678156, 0.9922731599292239, 0.9910252659886449, 0.9924603740016261, 0.9922489529257422, 0.9921995723569715, 0.9923776738994472, 0.991982643668716, 0.9921890225138392, 0.9928168115673123, 0.9925569729046063, 0.9923795909136981, 0.9920921891301244, 0.9916914024868527, 0.9924800292149678, 0.9922556651605142, 0.9921535514854454, 0.9919071319224956, 0.9919780865087882, 0.9918208448020546, 0.9919289529502571, 0.9921391694037406, 0.992269567541174, 0.9924752339944467, 0.9923508285402177, 0.992128617770679, 0.992043769753373, 0.9920200352912193], 'val_mDice': [0.39430909709342093, 0.39839720907964565, 0.4183870296668004, 0.4200015322224156, 0.42959840985031816, 0.43476458829086584, 0.43443331043319194, 0.441369174030213, 0.4474013631005545, 0.4508677223423222, 0.4460294491004553, 0.4551073680589865, 0.4614792805019621, 0.45159000355202156, 0.46621739595859973, 0.47100155561177937, 0.47504951869761264, 0.47478196141240114, 0.4681560240529321, 0.46945127683716853, 0.46238685692394815, 0.47571098885020696, 0.47601374330463353, 0.47852462297445303, 0.47826562513102283, 0.4690758508067947, 0.47303802839986553, 0.47593364531213456, 0.4722021945245989, 0.4676777894074494, 0.47706184533981233, 0.47758971427654, 0.47611457718027245, 0.47707501698184657, 0.4788911159153099, 0.48252152161555245, 0.47875117074261914, 0.4831545936452734, 0.48351980007446566, 0.4830932214453414, 0.4860331981747716, 0.4818286878747625, 0.47865532495238045, 0.47505668244204363, 0.4794800017510091, 0.4758342997626857, 0.48424356513553196, 0.47431974618642536, 0.4781919417975543, 0.4792778493763806, 0.4803265555126889, 0.47719202734328603, 0.4769660559323457, 0.478737505527588, 0.47257931561799377, 0.4746479878912459, 0.4795153846253862, 0.4768563619068077, 0.48182241372518, 0.4791128492212152, 0.47746602971632557, 0.47844285318801355, 0.47947832861462153, 0.481898005123253, 0.4797853413107875, 0.4812554151446254, 0.47936119653799153, 0.4761035315267317, 0.4790380740308905, 0.47544617910642883, 0.47620275222861375, 0.47716897439670275, 0.4750024095490888, 0.4769970355807124, 0.4761829533734479, 0.47773697041534446, 0.48200226223862563, 0.47862446209689874, 0.4790725283794575, 0.4753567179939052, 0.4757826746226073], 'loss': [0.16211630208542532, 0.09857513063858822, 0.0856730275158352, 0.07692476253052846, 0.07333452642459687, 0.06924016260944103, 0.06535535923837216, 0.06513127241465533, 0.0621032241489931, 0.060976606246237, 0.05923774581312429, 0.0572636851537236, 0.056213739774324166, 0.05433825586748217, 0.053290985955578074, 0.054128411955079926, 0.05280884065201802, 0.050639011891529556, 0.04967565683154982, 0.04959091088441234, 0.04954216189686741, 0.047203730513433316, 0.0463661017347749, 0.04647819994725952, 0.045214851488840635, 0.04552813853757978, 0.04480456229213926, 0.04335285391242048, 0.043541860162481474, 0.042744760486513655, 0.04439676759201537, 0.042574993935390305, 0.04177485551616743, 0.04234804375388419, 0.040787426233442996, 0.04066411481532708, 0.040328171864576726, 0.039672943061173555, 0.040224775638847333, 0.03947885107346228, 0.039193918440699325, 0.04006892572027654, 0.03819401697876809, 0.03850439965202728, 0.0388248468009595, 0.03796938414116086, 0.03723124549257347, 0.037665318872472854, 0.037066224850130024, 0.03700160434924037, 0.03619394314534308, 0.03636398251325651, 0.03696880903435347, 0.03581469577877463, 0.03615400868327676, 0.036040136606328016, 0.03585846368683443, 0.035223151537457933, 0.034918233082450525, 0.03482318444067649, 0.034310206683860735, 0.034413867737972795, 0.03440033330303413, 0.035136888825048014, 0.033738443304051235, 0.033547513782467196, 0.03451785474729935, 0.03423392744293199, 0.03361255949197909, 0.034878631890421785, 0.03359228876816054, 0.033382013622718906, 0.03369954337953242, 0.03339785171322547, 0.03324134723813668, 0.03296153134303989, 0.032654233826023386, 0.03337247640180191, 0.03358235696684133, 0.03282183289251648, 0.032673264648541825], 'acc': [0.9818120621687328, 0.9895459858586043, 0.9909371290197222, 0.9917910650536171, 0.9922715279686815, 0.9926606758754447, 0.9929982298298886, 0.9931919673883107, 0.9934153666489314, 0.9935611837077206, 0.9936874660224992, 0.9938075851314856, 0.9939477222850229, 0.9940549348635973, 0.9941796817656026, 0.9942361956005679, 0.9943224732576571, 0.994448304373108, 0.9945360056960872, 0.9945576395597029, 0.9946025037953198, 0.9947056665275825, 0.9947518080153183, 0.994838612188302, 0.9948967940148441, 0.9949022172239196, 0.9949703871253998, 0.9950464641046588, 0.9950634458732411, 0.9951282465437482, 0.9950803052386846, 0.9951949114156491, 0.9952051431275528, 0.9952268434224768, 0.9952843254857211, 0.9953265651227042, 0.9953410997026557, 0.9953631841790509, 0.995388786396627, 0.9954450978422065, 0.9954382978191937, 0.9954255470646184, 0.9955056173204581, 0.9955374217019763, 0.9955528176259967, 0.9955616800771896, 0.9955619393882355, 0.9956006160214412, 0.9956356337824123, 0.9956618326973153, 0.9956872885011202, 0.9956833003142638, 0.9956913164635097, 0.9957086289392926, 0.9957486131699697, 0.9957662691169977, 0.9957995884804928, 0.9958256877541095, 0.9958172602681052, 0.9958212276457469, 0.9958374689559423, 0.9958443560491902, 0.9958376619643419, 0.9958685953899699, 0.995886798155027, 0.9958876297213707, 0.9958880958001005, 0.9958996052976926, 0.9959081939974026, 0.9959224343307435, 0.9959127757747729, 0.9959578418586377, 0.9959564546117116, 0.9959600827753123, 0.9959399435499132, 0.9959740845765487, 0.9959833437613431, 0.9959776627019158, 0.9959759096608571, 0.9959753257200065, 0.9959696505411975], 'mDice': [0.6855093715679921, 0.8083346347610619, 0.8333856289807573, 0.8504184242956444, 0.8573376643312083, 0.865313830785161, 0.8728978595461508, 0.8732408441657437, 0.8791785082266012, 0.8813530475881546, 0.8847621986770803, 0.8886465105967168, 0.890666866999687, 0.8943619565557778, 0.8963907883323327, 0.8946862551125817, 0.8972754133753488, 0.901547432698853, 0.9034282554628703, 0.9035817013113213, 0.9036564296291788, 0.9082735314398567, 0.9099240210819045, 0.909656785846203, 0.9121511297298212, 0.9115180543006984, 0.9129259145969013, 0.9157923478814096, 0.9154059212401998, 0.9169631932858141, 0.9136851814960829, 0.917264463995973, 0.9188558125441418, 0.9177097627642034, 0.9207874704803934, 0.9210136190617806, 0.9216704935310349, 0.9229728804944879, 0.9218569105257723, 0.9233210969679243, 0.9238922031366059, 0.922149137427834, 0.9258487230974124, 0.9252141917760724, 0.9245674828901479, 0.9262698132509989, 0.9277425430992748, 0.9268543721910402, 0.928033367663094, 0.9281521793940917, 0.9297527089496468, 0.9294095324874719, 0.9281944152166197, 0.9304924860135932, 0.929795459530617, 0.9300123289171669, 0.930356326560945, 0.9316097894219563, 0.9322303223251184, 0.9324125159877374, 0.933425179955074, 0.9332175215239238, 0.9332430511419754, 0.9317570251580464, 0.9345464027614399, 0.9349232208962652, 0.9329851855778193, 0.9335454684024038, 0.9347813960620542, 0.9322514838093418, 0.9348183900872818, 0.9352168262394623, 0.9345826256969858, 0.9351851374646927, 0.9354968699460771, 0.9360477409330448, 0.936658694890785, 0.9352215979978868, 0.934800885721325, 0.9363211676602318, 0.9366179846497265], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:03,  1.19it/s]predicting test subjects:  40%|████      | 2/5 [00:01<00:01,  1.51it/s]predicting test subjects:  60%|██████    | 3/5 [00:01<00:01,  1.78it/s]predicting test subjects:  80%|████████  | 4/5 [00:01<00:00,  2.03it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  2.39it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]
predicting train subjects:   0%|          | 0/247 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/247 [00:00<00:40,  6.02it/s]predicting train subjects:   1%|          | 2/247 [00:00<00:39,  6.20it/s]predicting train subjects:   1%|          | 3/247 [00:00<00:38,  6.37it/s]predicting train subjects:   2%|▏         | 4/247 [00:00<00:40,  6.07it/s]predicting train subjects:   2%|▏         | 5/247 [00:00<00:39,  6.12it/s]predicting train subjects:   2%|▏         | 6/247 [00:00<00:38,  6.25it/s]predicting train subjects:   3%|▎         | 7/247 [00:01<00:37,  6.34it/s]predicting train subjects:   3%|▎         | 8/247 [00:01<00:37,  6.39it/s]predicting train subjects:   4%|▎         | 9/247 [00:01<00:37,  6.42it/s]predicting train subjects:   4%|▍         | 10/247 [00:01<00:38,  6.23it/s]predicting train subjects:   4%|▍         | 11/247 [00:01<00:37,  6.26it/s]predicting train subjects:   5%|▍         | 12/247 [00:01<00:36,  6.36it/s]predicting train subjects:   5%|▌         | 13/247 [00:02<00:36,  6.41it/s]predicting train subjects:   6%|▌         | 14/247 [00:02<00:38,  6.08it/s]predicting train subjects:   6%|▌         | 15/247 [00:02<00:37,  6.21it/s]predicting train subjects:   6%|▋         | 16/247 [00:02<00:36,  6.25it/s]predicting train subjects:   7%|▋         | 17/247 [00:02<00:36,  6.30it/s]predicting train subjects:   7%|▋         | 18/247 [00:02<00:35,  6.37it/s]predicting train subjects:   8%|▊         | 19/247 [00:03<00:35,  6.38it/s]predicting train subjects:   8%|▊         | 20/247 [00:03<00:35,  6.39it/s]predicting train subjects:   9%|▊         | 21/247 [00:03<00:35,  6.39it/s]predicting train subjects:   9%|▉         | 22/247 [00:03<00:35,  6.42it/s]predicting train subjects:   9%|▉         | 23/247 [00:03<00:34,  6.51it/s]predicting train subjects:  10%|▉         | 24/247 [00:03<00:33,  6.59it/s]predicting train subjects:  10%|█         | 25/247 [00:03<00:33,  6.57it/s]predicting train subjects:  11%|█         | 26/247 [00:04<00:33,  6.64it/s]predicting train subjects:  11%|█         | 27/247 [00:04<00:32,  6.68it/s]predicting train subjects:  11%|█▏        | 28/247 [00:04<00:32,  6.70it/s]predicting train subjects:  12%|█▏        | 29/247 [00:04<00:32,  6.73it/s]predicting train subjects:  12%|█▏        | 30/247 [00:04<00:32,  6.62it/s]predicting train subjects:  13%|█▎        | 31/247 [00:04<00:32,  6.63it/s]predicting train subjects:  13%|█▎        | 32/247 [00:04<00:32,  6.63it/s]predicting train subjects:  13%|█▎        | 33/247 [00:05<00:31,  6.71it/s]predicting train subjects:  14%|█▍        | 34/247 [00:05<00:31,  6.74it/s]predicting train subjects:  14%|█▍        | 35/247 [00:05<00:31,  6.71it/s]predicting train subjects:  15%|█▍        | 36/247 [00:05<00:31,  6.68it/s]predicting train subjects:  15%|█▍        | 37/247 [00:05<00:31,  6.57it/s]predicting train subjects:  15%|█▌        | 38/247 [00:05<00:31,  6.64it/s]predicting train subjects:  16%|█▌        | 39/247 [00:06<00:30,  6.71it/s]predicting train subjects:  16%|█▌        | 40/247 [00:06<00:30,  6.79it/s]predicting train subjects:  17%|█▋        | 41/247 [00:06<00:30,  6.80it/s]predicting train subjects:  17%|█▋        | 42/247 [00:06<00:30,  6.82it/s]predicting train subjects:  17%|█▋        | 43/247 [00:06<00:30,  6.78it/s]predicting train subjects:  18%|█▊        | 44/247 [00:06<00:30,  6.76it/s]predicting train subjects:  18%|█▊        | 45/247 [00:06<00:29,  6.81it/s]predicting train subjects:  19%|█▊        | 46/247 [00:07<00:29,  6.83it/s]predicting train subjects:  19%|█▉        | 47/247 [00:07<00:29,  6.85it/s]predicting train subjects:  19%|█▉        | 48/247 [00:07<00:29,  6.84it/s]predicting train subjects:  20%|█▉        | 49/247 [00:07<00:29,  6.82it/s]predicting train subjects:  20%|██        | 50/247 [00:07<00:28,  6.82it/s]predicting train subjects:  21%|██        | 51/247 [00:07<00:28,  6.78it/s]predicting train subjects:  21%|██        | 52/247 [00:07<00:28,  6.78it/s]predicting train subjects:  21%|██▏       | 53/247 [00:08<00:28,  6.78it/s]predicting train subjects:  22%|██▏       | 54/247 [00:08<00:28,  6.79it/s]predicting train subjects:  22%|██▏       | 55/247 [00:08<00:28,  6.82it/s]predicting train subjects:  23%|██▎       | 56/247 [00:08<00:28,  6.81it/s]predicting train subjects:  23%|██▎       | 57/247 [00:08<00:27,  6.81it/s]predicting train subjects:  23%|██▎       | 58/247 [00:08<00:27,  6.79it/s]predicting train subjects:  24%|██▍       | 59/247 [00:08<00:28,  6.67it/s]predicting train subjects:  24%|██▍       | 60/247 [00:09<00:28,  6.54it/s]predicting train subjects:  25%|██▍       | 61/247 [00:09<00:29,  6.41it/s]predicting train subjects:  25%|██▌       | 62/247 [00:09<00:30,  6.08it/s]predicting train subjects:  26%|██▌       | 63/247 [00:09<00:29,  6.14it/s]predicting train subjects:  26%|██▌       | 64/247 [00:09<00:30,  5.94it/s]predicting train subjects:  26%|██▋       | 65/247 [00:09<00:29,  6.09it/s]predicting train subjects:  27%|██▋       | 66/247 [00:10<00:29,  6.18it/s]predicting train subjects:  27%|██▋       | 67/247 [00:10<00:29,  6.10it/s]predicting train subjects:  28%|██▊       | 68/247 [00:10<00:29,  6.17it/s]predicting train subjects:  28%|██▊       | 69/247 [00:10<00:28,  6.21it/s]predicting train subjects:  28%|██▊       | 70/247 [00:10<00:28,  6.24it/s]predicting train subjects:  29%|██▊       | 71/247 [00:10<00:28,  6.27it/s]predicting train subjects:  29%|██▉       | 72/247 [00:11<00:27,  6.30it/s]predicting train subjects:  30%|██▉       | 73/247 [00:11<00:27,  6.33it/s]predicting train subjects:  30%|██▉       | 74/247 [00:11<00:27,  6.36it/s]predicting train subjects:  30%|███       | 75/247 [00:11<00:27,  6.33it/s]predicting train subjects:  31%|███       | 76/247 [00:11<00:26,  6.38it/s]predicting train subjects:  31%|███       | 77/247 [00:12<00:34,  4.87it/s]predicting train subjects:  32%|███▏      | 78/247 [00:12<00:38,  4.39it/s]predicting train subjects:  32%|███▏      | 79/247 [00:12<00:35,  4.75it/s]predicting train subjects:  32%|███▏      | 80/247 [00:12<00:38,  4.38it/s]predicting train subjects:  33%|███▎      | 81/247 [00:12<00:33,  4.89it/s]predicting train subjects:  33%|███▎      | 82/247 [00:13<00:31,  5.29it/s]predicting train subjects:  34%|███▎      | 83/247 [00:13<00:29,  5.59it/s]predicting train subjects:  34%|███▍      | 84/247 [00:13<00:27,  5.86it/s]predicting train subjects:  34%|███▍      | 85/247 [00:13<00:26,  6.08it/s]predicting train subjects:  35%|███▍      | 86/247 [00:13<00:25,  6.22it/s]predicting train subjects:  35%|███▌      | 87/247 [00:13<00:25,  6.30it/s]predicting train subjects:  36%|███▌      | 88/247 [00:13<00:24,  6.38it/s]predicting train subjects:  36%|███▌      | 89/247 [00:14<00:24,  6.46it/s]predicting train subjects:  36%|███▋      | 90/247 [00:14<00:24,  6.50it/s]predicting train subjects:  37%|███▋      | 91/247 [00:14<00:24,  6.35it/s]predicting train subjects:  37%|███▋      | 92/247 [00:14<00:24,  6.45it/s]predicting train subjects:  38%|███▊      | 93/247 [00:14<00:23,  6.54it/s]predicting train subjects:  38%|███▊      | 94/247 [00:14<00:23,  6.58it/s]predicting train subjects:  38%|███▊      | 95/247 [00:15<00:22,  6.63it/s]predicting train subjects:  39%|███▉      | 96/247 [00:15<00:22,  6.59it/s]predicting train subjects:  39%|███▉      | 97/247 [00:15<00:22,  6.59it/s]predicting train subjects:  40%|███▉      | 98/247 [00:15<00:22,  6.59it/s]predicting train subjects:  40%|████      | 99/247 [00:15<00:22,  6.62it/s]predicting train subjects:  40%|████      | 100/247 [00:15<00:23,  6.28it/s]predicting train subjects:  41%|████      | 101/247 [00:15<00:24,  6.04it/s]predicting train subjects:  41%|████▏     | 102/247 [00:16<00:24,  5.91it/s]predicting train subjects:  42%|████▏     | 103/247 [00:16<00:24,  5.83it/s]predicting train subjects:  42%|████▏     | 104/247 [00:16<00:24,  5.75it/s]predicting train subjects:  43%|████▎     | 105/247 [00:16<00:24,  5.68it/s]predicting train subjects:  43%|████▎     | 106/247 [00:16<00:24,  5.66it/s]predicting train subjects:  43%|████▎     | 107/247 [00:17<00:24,  5.62it/s]predicting train subjects:  44%|████▎     | 108/247 [00:17<00:24,  5.62it/s]predicting train subjects:  44%|████▍     | 109/247 [00:17<00:24,  5.65it/s]predicting train subjects:  45%|████▍     | 110/247 [00:17<00:24,  5.66it/s]predicting train subjects:  45%|████▍     | 111/247 [00:17<00:24,  5.63it/s]predicting train subjects:  45%|████▌     | 112/247 [00:17<00:24,  5.62it/s]predicting train subjects:  46%|████▌     | 113/247 [00:18<00:23,  5.60it/s]predicting train subjects:  46%|████▌     | 114/247 [00:18<00:23,  5.60it/s]predicting train subjects:  47%|████▋     | 115/247 [00:18<00:23,  5.62it/s]predicting train subjects:  47%|████▋     | 116/247 [00:18<00:23,  5.66it/s]predicting train subjects:  47%|████▋     | 117/247 [00:18<00:23,  5.63it/s]predicting train subjects:  48%|████▊     | 118/247 [00:19<00:22,  5.79it/s]predicting train subjects:  48%|████▊     | 119/247 [00:19<00:21,  5.90it/s]predicting train subjects:  49%|████▊     | 120/247 [00:19<00:21,  6.03it/s]predicting train subjects:  49%|████▉     | 121/247 [00:19<00:20,  6.06it/s]predicting train subjects:  49%|████▉     | 122/247 [00:19<00:20,  6.05it/s]predicting train subjects:  50%|████▉     | 123/247 [00:19<00:21,  5.90it/s]predicting train subjects:  50%|█████     | 124/247 [00:19<00:20,  5.96it/s]predicting train subjects:  51%|█████     | 125/247 [00:20<00:20,  5.82it/s]predicting train subjects:  51%|█████     | 126/247 [00:20<00:21,  5.58it/s]predicting train subjects:  51%|█████▏    | 127/247 [00:20<00:22,  5.32it/s]predicting train subjects:  52%|█████▏    | 128/247 [00:20<00:22,  5.21it/s]predicting train subjects:  52%|█████▏    | 129/247 [00:20<00:22,  5.19it/s]predicting train subjects:  53%|█████▎    | 130/247 [00:21<00:23,  5.03it/s]predicting train subjects:  53%|█████▎    | 131/247 [00:21<00:22,  5.10it/s]predicting train subjects:  53%|█████▎    | 132/247 [00:21<00:21,  5.40it/s]predicting train subjects:  54%|█████▍    | 133/247 [00:21<00:21,  5.35it/s]predicting train subjects:  54%|█████▍    | 134/247 [00:21<00:21,  5.18it/s]predicting train subjects:  55%|█████▍    | 135/247 [00:22<00:21,  5.11it/s]predicting train subjects:  55%|█████▌    | 136/247 [00:22<00:20,  5.34it/s]predicting train subjects:  55%|█████▌    | 137/247 [00:22<00:19,  5.69it/s]predicting train subjects:  56%|█████▌    | 138/247 [00:22<00:18,  6.02it/s]predicting train subjects:  56%|█████▋    | 139/247 [00:22<00:17,  6.23it/s]predicting train subjects:  57%|█████▋    | 140/247 [00:22<00:16,  6.41it/s]predicting train subjects:  57%|█████▋    | 141/247 [00:23<00:15,  6.76it/s]predicting train subjects:  57%|█████▋    | 142/247 [00:23<00:16,  6.28it/s]predicting train subjects:  58%|█████▊    | 143/247 [00:23<00:16,  6.18it/s]predicting train subjects:  58%|█████▊    | 144/247 [00:23<00:17,  6.05it/s]predicting train subjects:  59%|█████▊    | 145/247 [00:23<00:16,  6.26it/s]predicting train subjects:  59%|█████▉    | 146/247 [00:23<00:16,  6.18it/s]predicting train subjects:  60%|█████▉    | 147/247 [00:24<00:16,  6.25it/s]predicting train subjects:  60%|█████▉    | 148/247 [00:24<00:14,  6.62it/s]predicting train subjects:  60%|██████    | 149/247 [00:24<00:14,  6.92it/s]predicting train subjects:  61%|██████    | 150/247 [00:24<00:13,  6.97it/s]predicting train subjects:  61%|██████    | 151/247 [00:24<00:14,  6.71it/s]predicting train subjects:  62%|██████▏   | 152/247 [00:24<00:14,  6.57it/s]predicting train subjects:  62%|██████▏   | 153/247 [00:24<00:15,  6.26it/s]predicting train subjects:  62%|██████▏   | 154/247 [00:25<00:15,  6.02it/s]predicting train subjects:  63%|██████▎   | 155/247 [00:25<00:14,  6.26it/s]predicting train subjects:  63%|██████▎   | 156/247 [00:25<00:15,  5.92it/s]predicting train subjects:  64%|██████▎   | 157/247 [00:25<00:14,  6.00it/s]predicting train subjects:  64%|██████▍   | 158/247 [00:25<00:15,  5.80it/s]predicting train subjects:  64%|██████▍   | 159/247 [00:25<00:15,  5.67it/s]predicting train subjects:  65%|██████▍   | 160/247 [00:26<00:15,  5.69it/s]predicting train subjects:  65%|██████▌   | 161/247 [00:26<00:15,  5.58it/s]predicting train subjects:  66%|██████▌   | 162/247 [00:26<00:15,  5.50it/s]predicting train subjects:  66%|██████▌   | 163/247 [00:26<00:14,  5.77it/s]predicting train subjects:  66%|██████▋   | 164/247 [00:26<00:14,  5.91it/s]predicting train subjects:  67%|██████▋   | 165/247 [00:27<00:14,  5.59it/s]predicting train subjects:  67%|██████▋   | 166/247 [00:27<00:14,  5.47it/s]predicting train subjects:  68%|██████▊   | 167/247 [00:27<00:14,  5.41it/s]predicting train subjects:  68%|██████▊   | 168/247 [00:27<00:14,  5.58it/s]predicting train subjects:  68%|██████▊   | 169/247 [00:27<00:14,  5.52it/s]predicting train subjects:  69%|██████▉   | 170/247 [00:27<00:14,  5.45it/s]predicting train subjects:  69%|██████▉   | 171/247 [00:28<00:13,  5.81it/s]predicting train subjects:  70%|██████▉   | 172/247 [00:28<00:13,  5.67it/s]predicting train subjects:  70%|███████   | 173/247 [00:28<00:16,  4.43it/s]predicting train subjects:  70%|███████   | 174/247 [00:28<00:15,  4.61it/s]predicting train subjects:  71%|███████   | 175/247 [00:29<00:17,  4.05it/s]predicting train subjects:  71%|███████▏  | 176/247 [00:29<00:16,  4.32it/s]predicting train subjects:  72%|███████▏  | 177/247 [00:29<00:14,  4.82it/s]predicting train subjects:  72%|███████▏  | 178/247 [00:29<00:13,  5.26it/s]predicting train subjects:  72%|███████▏  | 179/247 [00:29<00:13,  5.20it/s]predicting train subjects:  73%|███████▎  | 180/247 [00:30<00:13,  5.15it/s]predicting train subjects:  73%|███████▎  | 181/247 [00:30<00:12,  5.14it/s]predicting train subjects:  74%|███████▎  | 182/247 [00:30<00:12,  5.21it/s]predicting train subjects:  74%|███████▍  | 183/247 [00:30<00:12,  5.23it/s]predicting train subjects:  74%|███████▍  | 184/247 [00:30<00:11,  5.60it/s]predicting train subjects:  75%|███████▍  | 185/247 [00:30<00:11,  5.63it/s]predicting train subjects:  75%|███████▌  | 186/247 [00:31<00:10,  5.61it/s]predicting train subjects:  76%|███████▌  | 187/247 [00:31<00:10,  5.49it/s]predicting train subjects:  76%|███████▌  | 188/247 [00:31<00:10,  5.42it/s]predicting train subjects:  77%|███████▋  | 189/247 [00:31<00:10,  5.34it/s]predicting train subjects:  77%|███████▋  | 190/247 [00:31<00:10,  5.70it/s]predicting train subjects:  77%|███████▋  | 191/247 [00:31<00:09,  5.98it/s]predicting train subjects:  78%|███████▊  | 192/247 [00:32<00:09,  5.73it/s]predicting train subjects:  78%|███████▊  | 193/247 [00:32<00:09,  5.52it/s]predicting train subjects:  79%|███████▊  | 194/247 [00:32<00:09,  5.55it/s]predicting train subjects:  79%|███████▉  | 195/247 [00:32<00:09,  5.37it/s]predicting train subjects:  79%|███████▉  | 196/247 [00:32<00:09,  5.44it/s]predicting train subjects:  80%|███████▉  | 197/247 [00:33<00:08,  5.60it/s]predicting train subjects:  80%|████████  | 198/247 [00:33<00:08,  5.51it/s]predicting train subjects:  81%|████████  | 199/247 [00:33<00:08,  5.49it/s]predicting train subjects:  81%|████████  | 200/247 [00:33<00:08,  5.82it/s]predicting train subjects:  81%|████████▏ | 201/247 [00:33<00:07,  6.18it/s]predicting train subjects:  82%|████████▏ | 202/247 [00:33<00:07,  5.68it/s]predicting train subjects:  82%|████████▏ | 203/247 [00:34<00:07,  5.70it/s]predicting train subjects:  83%|████████▎ | 204/247 [00:34<00:07,  5.88it/s]predicting train subjects:  83%|████████▎ | 205/247 [00:34<00:07,  5.95it/s]predicting train subjects:  83%|████████▎ | 206/247 [00:34<00:07,  5.86it/s]predicting train subjects:  84%|████████▍ | 207/247 [00:34<00:06,  5.85it/s]predicting train subjects:  84%|████████▍ | 208/247 [00:34<00:06,  6.13it/s]predicting train subjects:  85%|████████▍ | 209/247 [00:35<00:06,  6.02it/s]predicting train subjects:  85%|████████▌ | 210/247 [00:35<00:06,  5.90it/s]predicting train subjects:  85%|████████▌ | 211/247 [00:35<00:06,  5.86it/s]predicting train subjects:  86%|████████▌ | 212/247 [00:35<00:06,  5.67it/s]predicting train subjects:  86%|████████▌ | 213/247 [00:35<00:06,  5.34it/s]predicting train subjects:  87%|████████▋ | 214/247 [00:36<00:06,  5.34it/s]predicting train subjects:  87%|████████▋ | 215/247 [00:36<00:05,  5.48it/s]predicting train subjects:  87%|████████▋ | 216/247 [00:36<00:05,  5.65it/s]predicting train subjects:  88%|████████▊ | 217/247 [00:36<00:05,  5.96it/s]predicting train subjects:  88%|████████▊ | 218/247 [00:36<00:04,  5.82it/s]predicting train subjects:  89%|████████▊ | 219/247 [00:36<00:04,  5.70it/s]predicting train subjects:  89%|████████▉ | 220/247 [00:37<00:04,  5.59it/s]predicting train subjects:  89%|████████▉ | 221/247 [00:37<00:04,  5.30it/s]predicting train subjects:  90%|████████▉ | 222/247 [00:37<00:04,  5.30it/s]predicting train subjects:  90%|█████████ | 223/247 [00:37<00:04,  5.63it/s]predicting train subjects:  91%|█████████ | 224/247 [00:37<00:03,  5.88it/s]predicting train subjects:  91%|█████████ | 225/247 [00:37<00:03,  5.76it/s]predicting train subjects:  91%|█████████▏| 226/247 [00:38<00:03,  5.54it/s]predicting train subjects:  92%|█████████▏| 227/247 [00:38<00:03,  5.42it/s]predicting train subjects:  92%|█████████▏| 228/247 [00:38<00:03,  5.55it/s]predicting train subjects:  93%|█████████▎| 229/247 [00:38<00:03,  5.54it/s]predicting train subjects:  93%|█████████▎| 230/247 [00:38<00:03,  5.66it/s]predicting train subjects:  94%|█████████▎| 231/247 [00:39<00:03,  5.30it/s]predicting train subjects:  94%|█████████▍| 232/247 [00:39<00:03,  4.76it/s]predicting train subjects:  94%|█████████▍| 233/247 [00:39<00:02,  4.75it/s]predicting train subjects:  95%|█████████▍| 234/247 [00:39<00:02,  4.97it/s]predicting train subjects:  95%|█████████▌| 235/247 [00:39<00:02,  5.23it/s]predicting train subjects:  96%|█████████▌| 236/247 [00:40<00:02,  5.01it/s]predicting train subjects:  96%|█████████▌| 237/247 [00:40<00:02,  4.82it/s]predicting train subjects:  96%|█████████▋| 238/247 [00:40<00:01,  4.82it/s]predicting train subjects:  97%|█████████▋| 239/247 [00:40<00:01,  4.79it/s]predicting train subjects:  97%|█████████▋| 240/247 [00:40<00:01,  5.08it/s]predicting train subjects:  98%|█████████▊| 241/247 [00:41<00:01,  5.04it/s]predicting train subjects:  98%|█████████▊| 242/247 [00:41<00:01,  4.94it/s]predicting train subjects:  98%|█████████▊| 243/247 [00:41<00:00,  4.86it/s]predicting train subjects:  99%|█████████▉| 244/247 [00:41<00:00,  4.81it/s]predicting train subjects:  99%|█████████▉| 245/247 [00:42<00:00,  4.75it/s]predicting train subjects: 100%|█████████▉| 246/247 [00:42<00:00,  4.70it/s]predicting train subjects: 100%|██████████| 247/247 [00:42<00:00,  5.00it/s]predicting train subjects: 100%|██████████| 247/247 [00:42<00:00,  5.83it/s]
predicting test subjects sagittal:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects sagittal:  20%|██        | 1/5 [00:00<00:00,  4.23it/s]predicting test subjects sagittal:  40%|████      | 2/5 [00:00<00:00,  4.29it/s]predicting test subjects sagittal:  60%|██████    | 3/5 [00:00<00:00,  4.58it/s]predicting test subjects sagittal:  80%|████████  | 4/5 [00:00<00:00,  4.77it/s]predicting test subjects sagittal: 100%|██████████| 5/5 [00:01<00:00,  4.59it/s]predicting test subjects sagittal: 100%|██████████| 5/5 [00:01<00:00,  4.66it/s]
predicting train subjects sagittal:   0%|          | 0/247 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/247 [00:00<00:54,  4.54it/s]predicting train subjects sagittal:   1%|          | 2/247 [00:00<00:48,  5.04it/s]predicting train subjects sagittal:   1%|          | 3/247 [00:00<00:46,  5.28it/s]predicting train subjects sagittal:   2%|▏         | 4/247 [00:00<00:45,  5.39it/s]predicting train subjects sagittal:   2%|▏         | 5/247 [00:00<00:45,  5.29it/s]predicting train subjects sagittal:   2%|▏         | 6/247 [00:01<00:45,  5.25it/s]predicting train subjects sagittal:   3%|▎         | 7/247 [00:01<00:45,  5.33it/s]predicting train subjects sagittal:   3%|▎         | 8/247 [00:01<00:46,  5.11it/s]predicting train subjects sagittal:   4%|▎         | 9/247 [00:01<00:44,  5.39it/s]predicting train subjects sagittal:   4%|▍         | 10/247 [00:01<00:47,  5.01it/s]predicting train subjects sagittal:   4%|▍         | 11/247 [00:02<00:46,  5.05it/s]predicting train subjects sagittal:   5%|▍         | 12/247 [00:02<00:46,  5.05it/s]predicting train subjects sagittal:   5%|▌         | 13/247 [00:02<00:46,  5.07it/s]predicting train subjects sagittal:   6%|▌         | 14/247 [00:02<00:45,  5.08it/s]predicting train subjects sagittal:   6%|▌         | 15/247 [00:02<00:45,  5.11it/s]predicting train subjects sagittal:   6%|▋         | 16/247 [00:03<00:43,  5.33it/s]predicting train subjects sagittal:   7%|▋         | 17/247 [00:03<00:41,  5.58it/s]predicting train subjects sagittal:   7%|▋         | 18/247 [00:03<00:43,  5.23it/s]predicting train subjects sagittal:   8%|▊         | 19/247 [00:03<00:42,  5.40it/s]predicting train subjects sagittal:   8%|▊         | 20/247 [00:03<00:41,  5.43it/s]predicting train subjects sagittal:   9%|▊         | 21/247 [00:03<00:40,  5.61it/s]predicting train subjects sagittal:   9%|▉         | 22/247 [00:04<00:38,  5.84it/s]predicting train subjects sagittal:   9%|▉         | 23/247 [00:04<00:38,  5.77it/s]predicting train subjects sagittal:  10%|▉         | 24/247 [00:04<00:39,  5.66it/s]predicting train subjects sagittal:  10%|█         | 25/247 [00:04<00:38,  5.77it/s]predicting train subjects sagittal:  11%|█         | 26/247 [00:04<00:38,  5.74it/s]predicting train subjects sagittal:  11%|█         | 27/247 [00:04<00:39,  5.51it/s]predicting train subjects sagittal:  11%|█▏        | 28/247 [00:05<00:39,  5.49it/s]predicting train subjects sagittal:  12%|█▏        | 29/247 [00:05<00:39,  5.53it/s]predicting train subjects sagittal:  12%|█▏        | 30/247 [00:05<00:36,  5.87it/s]predicting train subjects sagittal:  13%|█▎        | 31/247 [00:05<00:37,  5.80it/s]predicting train subjects sagittal:  13%|█▎        | 32/247 [00:05<00:37,  5.73it/s]predicting train subjects sagittal:  13%|█▎        | 33/247 [00:06<00:37,  5.65it/s]predicting train subjects sagittal:  14%|█▍        | 34/247 [00:06<00:38,  5.60it/s]predicting train subjects sagittal:  14%|█▍        | 35/247 [00:06<00:36,  5.73it/s]predicting train subjects sagittal:  15%|█▍        | 36/247 [00:06<00:36,  5.73it/s]predicting train subjects sagittal:  15%|█▍        | 37/247 [00:06<00:37,  5.55it/s]predicting train subjects sagittal:  15%|█▌        | 38/247 [00:06<00:35,  5.86it/s]predicting train subjects sagittal:  16%|█▌        | 39/247 [00:07<00:35,  5.88it/s]predicting train subjects sagittal:  16%|█▌        | 40/247 [00:07<00:35,  5.83it/s]predicting train subjects sagittal:  17%|█▋        | 41/247 [00:07<00:36,  5.68it/s]predicting train subjects sagittal:  17%|█▋        | 42/247 [00:07<00:36,  5.58it/s]predicting train subjects sagittal:  17%|█▋        | 43/247 [00:07<00:37,  5.47it/s]predicting train subjects sagittal:  18%|█▊        | 44/247 [00:07<00:36,  5.63it/s]predicting train subjects sagittal:  18%|█▊        | 45/247 [00:08<00:36,  5.46it/s]predicting train subjects sagittal:  19%|█▊        | 46/247 [00:08<00:35,  5.70it/s]predicting train subjects sagittal:  19%|█▉        | 47/247 [00:08<00:35,  5.59it/s]predicting train subjects sagittal:  19%|█▉        | 48/247 [00:08<00:36,  5.53it/s]predicting train subjects sagittal:  20%|█▉        | 49/247 [00:08<00:35,  5.56it/s]predicting train subjects sagittal:  20%|██        | 50/247 [00:09<00:35,  5.54it/s]predicting train subjects sagittal:  21%|██        | 51/247 [00:09<00:36,  5.35it/s]predicting train subjects sagittal:  21%|██        | 52/247 [00:09<00:34,  5.73it/s]predicting train subjects sagittal:  21%|██▏       | 53/247 [00:09<00:33,  5.73it/s]predicting train subjects sagittal:  22%|██▏       | 54/247 [00:09<00:35,  5.49it/s]predicting train subjects sagittal:  22%|██▏       | 55/247 [00:09<00:33,  5.69it/s]predicting train subjects sagittal:  23%|██▎       | 56/247 [00:10<00:34,  5.58it/s]predicting train subjects sagittal:  23%|██▎       | 57/247 [00:10<00:34,  5.53it/s]predicting train subjects sagittal:  23%|██▎       | 58/247 [00:10<00:32,  5.76it/s]predicting train subjects sagittal:  24%|██▍       | 59/247 [00:10<00:32,  5.84it/s]predicting train subjects sagittal:  24%|██▍       | 60/247 [00:10<00:33,  5.58it/s]predicting train subjects sagittal:  25%|██▍       | 61/247 [00:11<00:33,  5.50it/s]predicting train subjects sagittal:  25%|██▌       | 62/247 [00:11<00:34,  5.32it/s]predicting train subjects sagittal:  26%|██▌       | 63/247 [00:11<00:33,  5.45it/s]predicting train subjects sagittal:  26%|██▌       | 64/247 [00:11<00:33,  5.48it/s]predicting train subjects sagittal:  26%|██▋       | 65/247 [00:11<00:32,  5.64it/s]predicting train subjects sagittal:  27%|██▋       | 66/247 [00:11<00:32,  5.53it/s]predicting train subjects sagittal:  27%|██▋       | 67/247 [00:12<00:33,  5.33it/s]predicting train subjects sagittal:  28%|██▊       | 68/247 [00:12<00:33,  5.28it/s]predicting train subjects sagittal:  28%|██▊       | 69/247 [00:12<00:34,  5.18it/s]predicting train subjects sagittal:  28%|██▊       | 70/247 [00:12<00:32,  5.45it/s]predicting train subjects sagittal:  29%|██▊       | 71/247 [00:12<00:32,  5.47it/s]predicting train subjects sagittal:  29%|██▉       | 72/247 [00:13<00:32,  5.42it/s]predicting train subjects sagittal:  30%|██▉       | 73/247 [00:13<00:34,  5.06it/s]predicting train subjects sagittal:  30%|██▉       | 74/247 [00:13<00:35,  4.93it/s]predicting train subjects sagittal:  30%|███       | 75/247 [00:13<00:34,  5.03it/s]predicting train subjects sagittal:  31%|███       | 76/247 [00:13<00:34,  5.00it/s]predicting train subjects sagittal:  31%|███       | 77/247 [00:14<00:33,  5.12it/s]predicting train subjects sagittal:  32%|███▏      | 78/247 [00:14<00:32,  5.23it/s]predicting train subjects sagittal:  32%|███▏      | 79/247 [00:14<00:32,  5.24it/s]predicting train subjects sagittal:  32%|███▏      | 80/247 [00:14<00:29,  5.64it/s]predicting train subjects sagittal:  33%|███▎      | 81/247 [00:14<00:29,  5.56it/s]predicting train subjects sagittal:  33%|███▎      | 82/247 [00:14<00:30,  5.38it/s]predicting train subjects sagittal:  34%|███▎      | 83/247 [00:15<00:30,  5.33it/s]predicting train subjects sagittal:  34%|███▍      | 84/247 [00:15<00:32,  5.07it/s]predicting train subjects sagittal:  34%|███▍      | 85/247 [00:15<00:31,  5.09it/s]predicting train subjects sagittal:  35%|███▍      | 86/247 [00:15<00:29,  5.43it/s]predicting train subjects sagittal:  35%|███▌      | 87/247 [00:15<00:29,  5.46it/s]predicting train subjects sagittal:  36%|███▌      | 88/247 [00:16<00:29,  5.35it/s]predicting train subjects sagittal:  36%|███▌      | 89/247 [00:16<00:29,  5.31it/s]predicting train subjects sagittal:  36%|███▋      | 90/247 [00:16<00:29,  5.26it/s]predicting train subjects sagittal:  37%|███▋      | 91/247 [00:16<00:29,  5.25it/s]predicting train subjects sagittal:  37%|███▋      | 92/247 [00:16<00:30,  5.11it/s]predicting train subjects sagittal:  38%|███▊      | 93/247 [00:17<00:30,  5.05it/s]predicting train subjects sagittal:  38%|███▊      | 94/247 [00:17<00:29,  5.22it/s]predicting train subjects sagittal:  38%|███▊      | 95/247 [00:17<00:27,  5.50it/s]predicting train subjects sagittal:  39%|███▉      | 96/247 [00:17<00:27,  5.49it/s]predicting train subjects sagittal:  39%|███▉      | 97/247 [00:17<00:27,  5.44it/s]predicting train subjects sagittal:  40%|███▉      | 98/247 [00:18<00:28,  5.26it/s]predicting train subjects sagittal:  40%|████      | 99/247 [00:18<00:27,  5.31it/s]predicting train subjects sagittal:  40%|████      | 100/247 [00:18<00:29,  4.95it/s]predicting train subjects sagittal:  41%|████      | 101/247 [00:18<00:30,  4.85it/s]predicting train subjects sagittal:  41%|████▏     | 102/247 [00:18<00:30,  4.77it/s]predicting train subjects sagittal:  42%|████▏     | 103/247 [00:19<00:29,  4.93it/s]predicting train subjects sagittal:  42%|████▏     | 104/247 [00:19<00:29,  4.90it/s]predicting train subjects sagittal:  43%|████▎     | 105/247 [00:19<00:29,  4.78it/s]predicting train subjects sagittal:  43%|████▎     | 106/247 [00:19<00:29,  4.75it/s]predicting train subjects sagittal:  43%|████▎     | 107/247 [00:19<00:29,  4.67it/s]predicting train subjects sagittal:  44%|████▎     | 108/247 [00:20<00:29,  4.76it/s]predicting train subjects sagittal:  44%|████▍     | 109/247 [00:20<00:27,  5.01it/s]predicting train subjects sagittal:  45%|████▍     | 110/247 [00:20<00:29,  4.71it/s]predicting train subjects sagittal:  45%|████▍     | 111/247 [00:20<00:29,  4.69it/s]predicting train subjects sagittal:  45%|████▌     | 112/247 [00:20<00:29,  4.58it/s]predicting train subjects sagittal:  46%|████▌     | 113/247 [00:21<00:28,  4.75it/s]predicting train subjects sagittal:  46%|████▌     | 114/247 [00:21<00:27,  4.78it/s]predicting train subjects sagittal:  47%|████▋     | 115/247 [00:21<00:28,  4.65it/s]predicting train subjects sagittal:  47%|████▋     | 116/247 [00:21<00:28,  4.54it/s]predicting train subjects sagittal:  47%|████▋     | 117/247 [00:22<00:29,  4.48it/s]predicting train subjects sagittal:  48%|████▊     | 118/247 [00:22<00:26,  4.80it/s]predicting train subjects sagittal:  48%|████▊     | 119/247 [00:22<00:25,  5.11it/s]predicting train subjects sagittal:  49%|████▊     | 120/247 [00:22<00:24,  5.10it/s]predicting train subjects sagittal:  49%|████▉     | 121/247 [00:22<00:25,  5.01it/s]predicting train subjects sagittal:  49%|████▉     | 122/247 [00:23<00:25,  4.98it/s]predicting train subjects sagittal:  50%|████▉     | 123/247 [00:23<00:24,  5.06it/s]predicting train subjects sagittal:  50%|█████     | 124/247 [00:23<00:23,  5.31it/s]predicting train subjects sagittal:  51%|█████     | 125/247 [00:23<00:22,  5.44it/s]predicting train subjects sagittal:  51%|█████     | 126/247 [00:23<00:22,  5.38it/s]predicting train subjects sagittal:  51%|█████▏    | 127/247 [00:23<00:22,  5.27it/s]predicting train subjects sagittal:  52%|█████▏    | 128/247 [00:24<00:23,  5.04it/s]predicting train subjects sagittal:  52%|█████▏    | 129/247 [00:24<00:23,  4.93it/s]predicting train subjects sagittal:  53%|█████▎    | 130/247 [00:24<00:23,  5.04it/s]predicting train subjects sagittal:  53%|█████▎    | 131/247 [00:24<00:21,  5.32it/s]predicting train subjects sagittal:  53%|█████▎    | 132/247 [00:24<00:21,  5.29it/s]predicting train subjects sagittal:  54%|█████▍    | 133/247 [00:25<00:23,  4.89it/s]predicting train subjects sagittal:  54%|█████▍    | 134/247 [00:25<00:23,  4.85it/s]predicting train subjects sagittal:  55%|█████▍    | 135/247 [00:25<00:22,  4.98it/s]predicting train subjects sagittal:  55%|█████▌    | 136/247 [00:25<00:20,  5.47it/s]predicting train subjects sagittal:  55%|█████▌    | 137/247 [00:25<00:18,  5.97it/s]predicting train subjects sagittal:  56%|█████▌    | 138/247 [00:25<00:17,  6.24it/s]predicting train subjects sagittal:  56%|█████▋    | 139/247 [00:26<00:17,  6.11it/s]predicting train subjects sagittal:  57%|█████▋    | 140/247 [00:26<00:17,  6.08it/s]predicting train subjects sagittal:  57%|█████▋    | 141/247 [00:26<00:17,  5.95it/s]predicting train subjects sagittal:  57%|█████▋    | 142/247 [00:26<00:16,  6.37it/s]predicting train subjects sagittal:  58%|█████▊    | 143/247 [00:26<00:15,  6.57it/s]predicting train subjects sagittal:  58%|█████▊    | 144/247 [00:26<00:16,  6.38it/s]predicting train subjects sagittal:  59%|█████▊    | 145/247 [00:27<00:16,  6.24it/s]predicting train subjects sagittal:  59%|█████▉    | 146/247 [00:27<00:16,  6.05it/s]predicting train subjects sagittal:  60%|█████▉    | 147/247 [00:27<00:16,  6.11it/s]predicting train subjects sagittal:  60%|█████▉    | 148/247 [00:27<00:16,  6.15it/s]predicting train subjects sagittal:  60%|██████    | 149/247 [00:27<00:15,  6.24it/s]predicting train subjects sagittal:  61%|██████    | 150/247 [00:27<00:14,  6.50it/s]predicting train subjects sagittal:  61%|██████    | 151/247 [00:28<00:14,  6.41it/s]predicting train subjects sagittal:  62%|██████▏   | 152/247 [00:28<00:14,  6.41it/s]predicting train subjects sagittal:  62%|██████▏   | 153/247 [00:28<00:14,  6.30it/s]predicting train subjects sagittal:  62%|██████▏   | 154/247 [00:28<00:15,  5.96it/s]predicting train subjects sagittal:  63%|██████▎   | 155/247 [00:28<00:15,  5.76it/s]predicting train subjects sagittal:  63%|██████▎   | 156/247 [00:28<00:15,  5.80it/s]predicting train subjects sagittal:  64%|██████▎   | 157/247 [00:29<00:15,  5.78it/s]predicting train subjects sagittal:  64%|██████▍   | 158/247 [00:29<00:14,  5.97it/s]predicting train subjects sagittal:  64%|██████▍   | 159/247 [00:29<00:14,  6.18it/s]predicting train subjects sagittal:  65%|██████▍   | 160/247 [00:29<00:14,  6.01it/s]predicting train subjects sagittal:  65%|██████▌   | 161/247 [00:29<00:15,  5.73it/s]predicting train subjects sagittal:  66%|██████▌   | 162/247 [00:29<00:14,  5.69it/s]predicting train subjects sagittal:  66%|██████▌   | 163/247 [00:30<00:15,  5.59it/s]predicting train subjects sagittal:  66%|██████▋   | 164/247 [00:30<00:14,  5.59it/s]predicting train subjects sagittal:  67%|██████▋   | 165/247 [00:30<00:14,  5.66it/s]predicting train subjects sagittal:  67%|██████▋   | 166/247 [00:30<00:14,  5.52it/s]predicting train subjects sagittal:  68%|██████▊   | 167/247 [00:30<00:13,  5.79it/s]predicting train subjects sagittal:  68%|██████▊   | 168/247 [00:30<00:13,  5.91it/s]predicting train subjects sagittal:  68%|██████▊   | 169/247 [00:31<00:13,  5.98it/s]predicting train subjects sagittal:  69%|██████▉   | 170/247 [00:31<00:13,  5.80it/s]predicting train subjects sagittal:  69%|██████▉   | 171/247 [00:31<00:13,  5.78it/s]predicting train subjects sagittal:  70%|██████▉   | 172/247 [00:31<00:13,  5.65it/s]predicting train subjects sagittal:  70%|███████   | 173/247 [00:31<00:12,  5.91it/s]predicting train subjects sagittal:  70%|███████   | 174/247 [00:32<00:12,  6.03it/s]predicting train subjects sagittal:  71%|███████   | 175/247 [00:32<00:12,  5.67it/s]predicting train subjects sagittal:  71%|███████▏  | 176/247 [00:32<00:12,  5.65it/s]predicting train subjects sagittal:  72%|███████▏  | 177/247 [00:32<00:12,  5.51it/s]predicting train subjects sagittal:  72%|███████▏  | 178/247 [00:32<00:12,  5.52it/s]predicting train subjects sagittal:  72%|███████▏  | 179/247 [00:32<00:12,  5.50it/s]predicting train subjects sagittal:  73%|███████▎  | 180/247 [00:33<00:11,  5.65it/s]predicting train subjects sagittal:  73%|███████▎  | 181/247 [00:33<00:11,  5.85it/s]predicting train subjects sagittal:  74%|███████▎  | 182/247 [00:33<00:11,  5.73it/s]predicting train subjects sagittal:  74%|███████▍  | 183/247 [00:33<00:11,  5.47it/s]predicting train subjects sagittal:  74%|███████▍  | 184/247 [00:33<00:11,  5.56it/s]predicting train subjects sagittal:  75%|███████▍  | 185/247 [00:34<00:11,  5.36it/s]predicting train subjects sagittal:  75%|███████▌  | 186/247 [00:34<00:11,  5.54it/s]predicting train subjects sagittal:  76%|███████▌  | 187/247 [00:34<00:10,  5.54it/s]predicting train subjects sagittal:  76%|███████▌  | 188/247 [00:34<00:11,  5.36it/s]predicting train subjects sagittal:  77%|███████▋  | 189/247 [00:34<00:10,  5.45it/s]predicting train subjects sagittal:  77%|███████▋  | 190/247 [00:34<00:10,  5.43it/s]predicting train subjects sagittal:  77%|███████▋  | 191/247 [00:35<00:10,  5.27it/s]predicting train subjects sagittal:  78%|███████▊  | 192/247 [00:35<00:10,  5.42it/s]predicting train subjects sagittal:  78%|███████▊  | 193/247 [00:35<00:09,  5.71it/s]predicting train subjects sagittal:  79%|███████▊  | 194/247 [00:35<00:09,  5.78it/s]predicting train subjects sagittal:  79%|███████▉  | 195/247 [00:35<00:08,  5.79it/s]predicting train subjects sagittal:  79%|███████▉  | 196/247 [00:35<00:08,  5.78it/s]predicting train subjects sagittal:  80%|███████▉  | 197/247 [00:36<00:08,  5.91it/s]predicting train subjects sagittal:  80%|████████  | 198/247 [00:36<00:08,  6.02it/s]predicting train subjects sagittal:  81%|████████  | 199/247 [00:36<00:07,  6.13it/s]predicting train subjects sagittal:  81%|████████  | 200/247 [00:36<00:07,  6.34it/s]predicting train subjects sagittal:  81%|████████▏ | 201/247 [00:36<00:07,  6.49it/s]predicting train subjects sagittal:  82%|████████▏ | 202/247 [00:36<00:07,  6.42it/s]predicting train subjects sagittal:  82%|████████▏ | 203/247 [00:37<00:06,  6.44it/s]predicting train subjects sagittal:  83%|████████▎ | 204/247 [00:37<00:06,  6.39it/s]predicting train subjects sagittal:  83%|████████▎ | 205/247 [00:37<00:06,  6.10it/s]predicting train subjects sagittal:  83%|████████▎ | 206/247 [00:37<00:06,  6.01it/s]predicting train subjects sagittal:  84%|████████▍ | 207/247 [00:37<00:06,  6.02it/s]predicting train subjects sagittal:  84%|████████▍ | 208/247 [00:37<00:06,  6.13it/s]predicting train subjects sagittal:  85%|████████▍ | 209/247 [00:38<00:06,  6.21it/s]predicting train subjects sagittal:  85%|████████▌ | 210/247 [00:38<00:05,  6.32it/s]predicting train subjects sagittal:  85%|████████▌ | 211/247 [00:38<00:05,  6.11it/s]predicting train subjects sagittal:  86%|████████▌ | 212/247 [00:38<00:05,  5.98it/s]predicting train subjects sagittal:  86%|████████▌ | 213/247 [00:38<00:05,  5.91it/s]predicting train subjects sagittal:  87%|████████▋ | 214/247 [00:38<00:05,  6.07it/s]predicting train subjects sagittal:  87%|████████▋ | 215/247 [00:39<00:05,  6.25it/s]predicting train subjects sagittal:  87%|████████▋ | 216/247 [00:39<00:05,  6.15it/s]predicting train subjects sagittal:  88%|████████▊ | 217/247 [00:39<00:05,  5.90it/s]predicting train subjects sagittal:  88%|████████▊ | 218/247 [00:39<00:04,  6.00it/s]predicting train subjects sagittal:  89%|████████▊ | 219/247 [00:39<00:04,  5.90it/s]predicting train subjects sagittal:  89%|████████▉ | 220/247 [00:39<00:04,  5.75it/s]predicting train subjects sagittal:  89%|████████▉ | 221/247 [00:40<00:04,  5.78it/s]predicting train subjects sagittal:  90%|████████▉ | 222/247 [00:40<00:04,  5.96it/s]predicting train subjects sagittal:  90%|█████████ | 223/247 [00:40<00:03,  6.20it/s]predicting train subjects sagittal:  91%|█████████ | 224/247 [00:40<00:03,  6.25it/s]predicting train subjects sagittal:  91%|█████████ | 225/247 [00:40<00:03,  6.40it/s]predicting train subjects sagittal:  91%|█████████▏| 226/247 [00:40<00:03,  6.50it/s]predicting train subjects sagittal:  92%|█████████▏| 227/247 [00:40<00:03,  6.59it/s]predicting train subjects sagittal:  92%|█████████▏| 228/247 [00:41<00:02,  6.66it/s]predicting train subjects sagittal:  93%|█████████▎| 229/247 [00:41<00:02,  6.51it/s]predicting train subjects sagittal:  93%|█████████▎| 230/247 [00:41<00:02,  5.88it/s]predicting train subjects sagittal:  94%|█████████▎| 231/247 [00:41<00:02,  5.51it/s]predicting train subjects sagittal:  94%|█████████▍| 232/247 [00:41<00:02,  5.21it/s]predicting train subjects sagittal:  94%|█████████▍| 233/247 [00:42<00:02,  5.03it/s]predicting train subjects sagittal:  95%|█████████▍| 234/247 [00:42<00:02,  5.12it/s]predicting train subjects sagittal:  95%|█████████▌| 235/247 [00:42<00:02,  5.34it/s]predicting train subjects sagittal:  96%|█████████▌| 236/247 [00:42<00:02,  5.33it/s]predicting train subjects sagittal:  96%|█████████▌| 237/247 [00:42<00:01,  5.23it/s]predicting train subjects sagittal:  96%|█████████▋| 238/247 [00:43<00:01,  5.22it/s]predicting train subjects sagittal:  97%|█████████▋| 239/247 [00:43<00:01,  5.14it/s]predicting train subjects sagittal:  97%|█████████▋| 240/247 [00:43<00:01,  5.13it/s]predicting train subjects sagittal:  98%|█████████▊| 241/247 [00:43<00:01,  5.32it/s]predicting train subjects sagittal:  98%|█████████▊| 242/247 [00:43<00:00,  5.44it/s]predicting train subjects sagittal:  98%|█████████▊| 243/247 [00:43<00:00,  5.52it/s]predicting train subjects sagittal:  99%|█████████▉| 244/247 [00:44<00:00,  5.26it/s]predicting train subjects sagittal:  99%|█████████▉| 245/247 [00:44<00:00,  5.31it/s]predicting train subjects sagittal: 100%|█████████▉| 246/247 [00:44<00:00,  5.30it/s]predicting train subjects sagittal: 100%|██████████| 247/247 [00:44<00:00,  5.39it/s]predicting train subjects sagittal: 100%|██████████| 247/247 [00:44<00:00,  5.52it/s]
saving BB  test1-THALAMUS:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 5/5 [00:00<00:00, 69.56it/s]
saving BB  train1-THALAMUS:   0%|          | 0/247 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 8/247 [00:00<00:03, 73.20it/s]saving BB  train1-THALAMUS:   6%|▌         | 15/247 [00:00<00:03, 71.46it/s]saving BB  train1-THALAMUS:   9%|▉         | 23/247 [00:00<00:03, 73.52it/s]saving BB  train1-THALAMUS:  13%|█▎        | 32/247 [00:00<00:02, 77.77it/s]saving BB  train1-THALAMUS:  17%|█▋        | 41/247 [00:00<00:02, 78.59it/s]saving BB  train1-THALAMUS:  19%|█▉        | 48/247 [00:00<00:02, 74.10it/s]saving BB  train1-THALAMUS:  23%|██▎       | 56/247 [00:00<00:02, 74.94it/s]saving BB  train1-THALAMUS:  26%|██▌       | 63/247 [00:00<00:02, 71.44it/s]saving BB  train1-THALAMUS:  28%|██▊       | 70/247 [00:00<00:02, 68.25it/s]saving BB  train1-THALAMUS:  31%|███       | 77/247 [00:01<00:02, 67.24it/s]saving BB  train1-THALAMUS:  34%|███▍      | 84/247 [00:01<00:02, 65.57it/s]saving BB  train1-THALAMUS:  37%|███▋      | 91/247 [00:01<00:02, 63.49it/s]saving BB  train1-THALAMUS:  40%|████      | 99/247 [00:01<00:02, 67.64it/s]saving BB  train1-THALAMUS:  43%|████▎     | 107/247 [00:01<00:01, 70.00it/s]saving BB  train1-THALAMUS:  47%|████▋     | 115/247 [00:01<00:01, 69.19it/s]saving BB  train1-THALAMUS:  49%|████▉     | 122/247 [00:01<00:01, 68.01it/s]saving BB  train1-THALAMUS:  53%|█████▎    | 130/247 [00:01<00:01, 70.37it/s]saving BB  train1-THALAMUS:  56%|█████▌    | 138/247 [00:01<00:01, 72.52it/s]saving BB  train1-THALAMUS:  60%|█████▉    | 147/247 [00:02<00:01, 76.51it/s]saving BB  train1-THALAMUS:  63%|██████▎   | 155/247 [00:02<00:01, 68.33it/s]saving BB  train1-THALAMUS:  66%|██████▌   | 163/247 [00:02<00:01, 65.63it/s]saving BB  train1-THALAMUS:  70%|██████▉   | 172/247 [00:02<00:01, 71.06it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 180/247 [00:02<00:00, 70.42it/s]saving BB  train1-THALAMUS:  76%|███████▌  | 188/247 [00:02<00:00, 69.12it/s]saving BB  train1-THALAMUS:  79%|███████▉  | 196/247 [00:02<00:00, 69.74it/s]saving BB  train1-THALAMUS:  83%|████████▎ | 204/247 [00:02<00:00, 69.71it/s]saving BB  train1-THALAMUS:  86%|████████▌ | 212/247 [00:02<00:00, 70.16it/s]saving BB  train1-THALAMUS:  89%|████████▉ | 221/247 [00:03<00:00, 71.66it/s]saving BB  train1-THALAMUS:  93%|█████████▎| 230/247 [00:03<00:00, 70.87it/s]saving BB  train1-THALAMUS:  96%|█████████▋| 238/247 [00:03<00:00, 68.06it/s]saving BB  train1-THALAMUS: 100%|█████████▉| 246/247 [00:03<00:00, 68.64it/s]saving BB  train1-THALAMUS: 100%|██████████| 247/247 [00:03<00:00, 70.52it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 5/5 [00:00<00:00, 70.05it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/247 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   4%|▎         | 9/247 [00:00<00:03, 75.43it/s]saving BB  train1-THALAMUS Sagittal:   6%|▋         | 16/247 [00:00<00:03, 70.92it/s]saving BB  train1-THALAMUS Sagittal:   9%|▉         | 23/247 [00:00<00:03, 70.10it/s]saving BB  train1-THALAMUS Sagittal:  12%|█▏        | 29/247 [00:00<00:03, 65.96it/s]saving BB  train1-THALAMUS Sagittal:  15%|█▍        | 37/247 [00:00<00:03, 69.13it/s]saving BB  train1-THALAMUS Sagittal:  18%|█▊        | 45/247 [00:00<00:02, 71.45it/s]saving BB  train1-THALAMUS Sagittal:  22%|██▏       | 55/247 [00:00<00:02, 77.07it/s]saving BB  train1-THALAMUS Sagittal:  26%|██▌       | 64/247 [00:00<00:02, 78.61it/s]saving BB  train1-THALAMUS Sagittal:  30%|██▉       | 73/247 [00:00<00:02, 79.18it/s]saving BB  train1-THALAMUS Sagittal:  33%|███▎      | 81/247 [00:01<00:02, 75.52it/s]saving BB  train1-THALAMUS Sagittal:  36%|███▌      | 89/247 [00:01<00:02, 73.19it/s]saving BB  train1-THALAMUS Sagittal:  39%|███▉      | 97/247 [00:01<00:02, 70.83it/s]saving BB  train1-THALAMUS Sagittal:  43%|████▎     | 105/247 [00:01<00:02, 66.88it/s]saving BB  train1-THALAMUS Sagittal:  45%|████▌     | 112/247 [00:01<00:02, 66.24it/s]saving BB  train1-THALAMUS Sagittal:  48%|████▊     | 119/247 [00:01<00:01, 64.62it/s]saving BB  train1-THALAMUS Sagittal:  51%|█████     | 126/247 [00:01<00:01, 64.75it/s]saving BB  train1-THALAMUS Sagittal:  54%|█████▍    | 134/247 [00:01<00:01, 66.75it/s]saving BB  train1-THALAMUS Sagittal:  58%|█████▊    | 143/247 [00:02<00:01, 71.63it/s]saving BB  train1-THALAMUS Sagittal:  62%|██████▏   | 152/247 [00:02<00:01, 75.15it/s]saving BB  train1-THALAMUS Sagittal:  66%|██████▌   | 162/247 [00:02<00:01, 80.01it/s]saving BB  train1-THALAMUS Sagittal:  70%|██████▉   | 172/247 [00:02<00:00, 82.67it/s]saving BB  train1-THALAMUS Sagittal:  73%|███████▎  | 181/247 [00:02<00:00, 81.74it/s]saving BB  train1-THALAMUS Sagittal:  77%|███████▋  | 190/247 [00:02<00:00, 81.47it/s]saving BB  train1-THALAMUS Sagittal:  81%|████████  | 199/247 [00:02<00:00, 81.67it/s]saving BB  train1-THALAMUS Sagittal:  84%|████████▍ | 208/247 [00:02<00:00, 82.29it/s]saving BB  train1-THALAMUS Sagittal:  88%|████████▊ | 217/247 [00:02<00:00, 83.48it/s]saving BB  train1-THALAMUS Sagittal:  91%|█████████▏| 226/247 [00:03<00:00, 77.00it/s]saving BB  train1-THALAMUS Sagittal:  95%|█████████▍| 234/247 [00:03<00:00, 72.47it/s]saving BB  train1-THALAMUS Sagittal:  98%|█████████▊| 242/247 [00:03<00:00, 72.49it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 247/247 [00:03<00:00, 74.37it/s]
Loading train:   0%|          | 0/247 [00:00<?, ?it/s]Loading train:   0%|          | 1/247 [00:01<04:12,  1.03s/it]Loading train:   1%|          | 2/247 [00:01<03:53,  1.05it/s]Loading train:   1%|          | 3/247 [00:02<03:43,  1.09it/s]Loading train:   2%|▏         | 4/247 [00:03<03:53,  1.04it/s]Loading train:   2%|▏         | 5/247 [00:04<03:28,  1.16it/s]Loading train:   2%|▏         | 6/247 [00:04<03:04,  1.31it/s]Loading train:   3%|▎         | 7/247 [00:05<02:47,  1.43it/s]Loading train:   3%|▎         | 8/247 [00:05<02:36,  1.53it/s]Loading train:   4%|▎         | 9/247 [00:06<02:27,  1.61it/s]Loading train:   4%|▍         | 10/247 [00:07<02:19,  1.70it/s]Loading train:   4%|▍         | 11/247 [00:07<02:14,  1.76it/s]Loading train:   5%|▍         | 12/247 [00:08<02:15,  1.74it/s]Loading train:   5%|▌         | 13/247 [00:08<02:23,  1.63it/s]Loading train:   6%|▌         | 14/247 [00:09<02:21,  1.65it/s]Loading train:   6%|▌         | 15/247 [00:10<02:26,  1.59it/s]Loading train:   6%|▋         | 16/247 [00:10<02:27,  1.57it/s]Loading train:   7%|▋         | 17/247 [00:11<02:26,  1.57it/s]Loading train:   7%|▋         | 18/247 [00:12<02:25,  1.57it/s]Loading train:   8%|▊         | 19/247 [00:12<02:24,  1.58it/s]Loading train:   8%|▊         | 20/247 [00:13<02:21,  1.60it/s]Loading train:   9%|▊         | 21/247 [00:13<02:19,  1.62it/s]Loading train:   9%|▉         | 22/247 [00:14<02:22,  1.58it/s]Loading train:   9%|▉         | 23/247 [00:15<02:27,  1.52it/s]Loading train:  10%|▉         | 24/247 [00:15<02:19,  1.60it/s]Loading train:  10%|█         | 25/247 [00:16<02:14,  1.65it/s]Loading train:  11%|█         | 26/247 [00:16<02:12,  1.67it/s]Loading train:  11%|█         | 27/247 [00:17<02:10,  1.69it/s]Loading train:  11%|█▏        | 28/247 [00:18<02:14,  1.63it/s]Loading train:  12%|█▏        | 29/247 [00:18<02:09,  1.69it/s]Loading train:  12%|█▏        | 30/247 [00:19<02:07,  1.71it/s]Loading train:  13%|█▎        | 31/247 [00:19<02:06,  1.71it/s]Loading train:  13%|█▎        | 32/247 [00:20<02:03,  1.74it/s]Loading train:  13%|█▎        | 33/247 [00:21<02:07,  1.68it/s]Loading train:  14%|█▍        | 34/247 [00:21<02:06,  1.68it/s]Loading train:  14%|█▍        | 35/247 [00:22<01:59,  1.78it/s]Loading train:  15%|█▍        | 36/247 [00:22<01:58,  1.77it/s]Loading train:  15%|█▍        | 37/247 [00:23<01:55,  1.83it/s]Loading train:  15%|█▌        | 38/247 [00:23<01:56,  1.79it/s]Loading train:  16%|█▌        | 39/247 [00:24<01:54,  1.81it/s]Loading train:  16%|█▌        | 40/247 [00:24<01:56,  1.77it/s]Loading train:  17%|█▋        | 41/247 [00:25<01:55,  1.78it/s]Loading train:  17%|█▋        | 42/247 [00:26<01:54,  1.79it/s]Loading train:  17%|█▋        | 43/247 [00:26<01:52,  1.81it/s]Loading train:  18%|█▊        | 44/247 [00:27<01:51,  1.83it/s]Loading train:  18%|█▊        | 45/247 [00:27<01:52,  1.79it/s]Loading train:  19%|█▊        | 46/247 [00:28<01:54,  1.76it/s]Loading train:  19%|█▉        | 47/247 [00:28<01:52,  1.78it/s]Loading train:  19%|█▉        | 48/247 [00:29<01:56,  1.70it/s]Loading train:  20%|█▉        | 49/247 [00:30<01:52,  1.76it/s]Loading train:  20%|██        | 50/247 [00:30<01:47,  1.83it/s]Loading train:  21%|██        | 51/247 [00:31<01:48,  1.81it/s]Loading train:  21%|██        | 52/247 [00:31<01:50,  1.76it/s]Loading train:  21%|██▏       | 53/247 [00:32<01:47,  1.81it/s]Loading train:  22%|██▏       | 54/247 [00:32<01:45,  1.82it/s]Loading train:  22%|██▏       | 55/247 [00:33<01:47,  1.79it/s]Loading train:  23%|██▎       | 56/247 [00:33<01:49,  1.74it/s]Loading train:  23%|██▎       | 57/247 [00:34<01:48,  1.75it/s]Loading train:  23%|██▎       | 58/247 [00:35<01:48,  1.75it/s]Loading train:  24%|██▍       | 59/247 [00:35<01:50,  1.70it/s]Loading train:  24%|██▍       | 60/247 [00:36<01:52,  1.66it/s]Loading train:  25%|██▍       | 61/247 [00:37<01:55,  1.60it/s]Loading train:  25%|██▌       | 62/247 [00:37<01:57,  1.58it/s]Loading train:  26%|██▌       | 63/247 [00:38<01:54,  1.61it/s]Loading train:  26%|██▌       | 64/247 [00:38<01:54,  1.60it/s]Loading train:  26%|██▋       | 65/247 [00:39<01:57,  1.55it/s]Loading train:  27%|██▋       | 66/247 [00:40<01:54,  1.59it/s]Loading train:  27%|██▋       | 67/247 [00:40<01:55,  1.56it/s]Loading train:  28%|██▊       | 68/247 [00:41<01:58,  1.50it/s]Loading train:  28%|██▊       | 69/247 [00:42<01:55,  1.54it/s]Loading train:  28%|██▊       | 70/247 [00:42<01:51,  1.59it/s]Loading train:  29%|██▊       | 71/247 [00:43<01:49,  1.61it/s]Loading train:  29%|██▉       | 72/247 [00:44<01:50,  1.59it/s]Loading train:  30%|██▉       | 73/247 [00:44<01:50,  1.57it/s]Loading train:  30%|██▉       | 74/247 [00:45<01:53,  1.52it/s]Loading train:  30%|███       | 75/247 [00:46<01:55,  1.49it/s]Loading train:  31%|███       | 76/247 [00:46<01:51,  1.54it/s]Loading train:  31%|███       | 77/247 [00:47<02:02,  1.38it/s]Loading train:  32%|███▏      | 78/247 [00:48<02:14,  1.26it/s]Loading train:  32%|███▏      | 79/247 [00:49<02:20,  1.20it/s]Loading train:  32%|███▏      | 80/247 [00:50<02:15,  1.23it/s]Loading train:  33%|███▎      | 81/247 [00:51<02:13,  1.24it/s]Loading train:  33%|███▎      | 82/247 [00:51<02:02,  1.35it/s]Loading train:  34%|███▎      | 83/247 [00:52<01:50,  1.48it/s]Loading train:  34%|███▍      | 84/247 [00:52<01:43,  1.57it/s]Loading train:  34%|███▍      | 85/247 [00:53<01:40,  1.62it/s]Loading train:  35%|███▍      | 86/247 [00:53<01:36,  1.67it/s]Loading train:  35%|███▌      | 87/247 [00:54<01:37,  1.65it/s]Loading train:  36%|███▌      | 88/247 [00:54<01:33,  1.70it/s]Loading train:  36%|███▌      | 89/247 [00:55<01:31,  1.73it/s]Loading train:  36%|███▋      | 90/247 [00:56<01:29,  1.76it/s]Loading train:  37%|███▋      | 91/247 [00:56<01:28,  1.76it/s]Loading train:  37%|███▋      | 92/247 [00:57<01:27,  1.78it/s]Loading train:  38%|███▊      | 93/247 [00:57<01:26,  1.78it/s]Loading train:  38%|███▊      | 94/247 [00:58<01:26,  1.76it/s]Loading train:  38%|███▊      | 95/247 [00:58<01:25,  1.77it/s]Loading train:  39%|███▉      | 96/247 [00:59<01:23,  1.80it/s]Loading train:  39%|███▉      | 97/247 [00:59<01:23,  1.79it/s]Loading train:  40%|███▉      | 98/247 [01:00<01:22,  1.80it/s]Loading train:  40%|████      | 99/247 [01:01<01:21,  1.81it/s]Loading train:  40%|████      | 100/247 [01:01<01:22,  1.77it/s]Loading train:  41%|████      | 101/247 [01:02<01:24,  1.74it/s]Loading train:  41%|████▏     | 102/247 [01:02<01:25,  1.70it/s]Loading train:  42%|████▏     | 103/247 [01:03<01:26,  1.66it/s]Loading train:  42%|████▏     | 104/247 [01:04<01:26,  1.66it/s]Loading train:  43%|████▎     | 105/247 [01:04<01:25,  1.67it/s]Loading train:  43%|████▎     | 106/247 [01:05<01:24,  1.68it/s]Loading train:  43%|████▎     | 107/247 [01:05<01:22,  1.71it/s]Loading train:  44%|████▎     | 108/247 [01:06<01:21,  1.71it/s]Loading train:  44%|████▍     | 109/247 [01:07<01:20,  1.72it/s]Loading train:  45%|████▍     | 110/247 [01:07<01:19,  1.73it/s]Loading train:  45%|████▍     | 111/247 [01:08<01:19,  1.71it/s]Loading train:  45%|████▌     | 112/247 [01:08<01:19,  1.69it/s]Loading train:  46%|████▌     | 113/247 [01:09<01:19,  1.69it/s]Loading train:  46%|████▌     | 114/247 [01:10<01:21,  1.63it/s]Loading train:  47%|████▋     | 115/247 [01:10<01:21,  1.62it/s]Loading train:  47%|████▋     | 116/247 [01:11<01:18,  1.67it/s]Loading train:  47%|████▋     | 117/247 [01:11<01:17,  1.68it/s]Loading train:  48%|████▊     | 118/247 [01:12<01:18,  1.64it/s]Loading train:  48%|████▊     | 119/247 [01:13<01:19,  1.62it/s]Loading train:  49%|████▊     | 120/247 [01:13<01:19,  1.60it/s]Loading train:  49%|████▉     | 121/247 [01:14<01:18,  1.60it/s]Loading train:  49%|████▉     | 122/247 [01:15<01:18,  1.59it/s]Loading train:  50%|████▉     | 123/247 [01:15<01:17,  1.60it/s]Loading train:  50%|█████     | 124/247 [01:16<01:17,  1.59it/s]Loading train:  51%|█████     | 125/247 [01:16<01:16,  1.59it/s]Loading train:  51%|█████     | 126/247 [01:17<01:16,  1.59it/s]Loading train:  51%|█████▏    | 127/247 [01:18<01:14,  1.60it/s]Loading train:  52%|█████▏    | 128/247 [01:18<01:13,  1.61it/s]Loading train:  52%|█████▏    | 129/247 [01:19<01:13,  1.61it/s]Loading train:  53%|█████▎    | 130/247 [01:19<01:11,  1.63it/s]Loading train:  53%|█████▎    | 131/247 [01:20<01:10,  1.66it/s]Loading train:  53%|█████▎    | 132/247 [01:21<01:09,  1.67it/s]Loading train:  54%|█████▍    | 133/247 [01:21<01:09,  1.65it/s]Loading train:  54%|█████▍    | 134/247 [01:22<01:07,  1.66it/s]Loading train:  55%|█████▍    | 135/247 [01:22<01:06,  1.68it/s]Loading train:  55%|█████▌    | 136/247 [01:23<01:02,  1.76it/s]Loading train:  55%|█████▌    | 137/247 [01:23<00:59,  1.84it/s]Loading train:  56%|█████▌    | 138/247 [01:24<00:56,  1.92it/s]Loading train:  56%|█████▋    | 139/247 [01:24<00:55,  1.94it/s]Loading train:  57%|█████▋    | 140/247 [01:25<00:54,  1.97it/s]Loading train:  57%|█████▋    | 141/247 [01:25<00:53,  1.99it/s]Loading train:  57%|█████▋    | 142/247 [01:26<00:52,  2.01it/s]Loading train:  58%|█████▊    | 143/247 [01:26<00:51,  2.00it/s]Loading train:  58%|█████▊    | 144/247 [01:27<00:51,  1.98it/s]Loading train:  59%|█████▊    | 145/247 [01:27<00:50,  2.00it/s]Loading train:  59%|█████▉    | 146/247 [01:28<00:50,  2.01it/s]Loading train:  60%|█████▉    | 147/247 [01:28<00:49,  2.03it/s]Loading train:  60%|█████▉    | 148/247 [01:29<00:49,  2.02it/s]Loading train:  60%|██████    | 149/247 [01:29<00:48,  2.03it/s]Loading train:  61%|██████    | 150/247 [01:30<00:47,  2.04it/s]Loading train:  61%|██████    | 151/247 [01:30<00:47,  2.04it/s]Loading train:  62%|██████▏   | 152/247 [01:31<00:46,  2.04it/s]Loading train:  62%|██████▏   | 153/247 [01:31<00:47,  2.00it/s]Loading train:  62%|██████▏   | 154/247 [01:32<00:46,  2.01it/s]Loading train:  63%|██████▎   | 155/247 [01:32<00:46,  2.00it/s]Loading train:  63%|██████▎   | 156/247 [01:33<00:45,  1.99it/s]Loading train:  64%|██████▎   | 157/247 [01:33<00:44,  2.00it/s]Loading train:  64%|██████▍   | 158/247 [01:34<00:43,  2.02it/s]Loading train:  64%|██████▍   | 159/247 [01:34<00:42,  2.05it/s]Loading train:  65%|██████▍   | 160/247 [01:35<00:41,  2.08it/s]Loading train:  65%|██████▌   | 161/247 [01:35<00:41,  2.08it/s]Loading train:  66%|██████▌   | 162/247 [01:36<00:42,  2.02it/s]Loading train:  66%|██████▌   | 163/247 [01:36<00:41,  2.01it/s]Loading train:  66%|██████▋   | 164/247 [01:37<00:41,  2.01it/s]Loading train:  67%|██████▋   | 165/247 [01:37<00:41,  1.99it/s]Loading train:  67%|██████▋   | 166/247 [01:38<00:40,  2.01it/s]Loading train:  68%|██████▊   | 167/247 [01:38<00:39,  2.02it/s]Loading train:  68%|██████▊   | 168/247 [01:39<00:38,  2.04it/s]Loading train:  68%|██████▊   | 169/247 [01:39<00:38,  2.05it/s]Loading train:  69%|██████▉   | 170/247 [01:40<00:37,  2.06it/s]Loading train:  69%|██████▉   | 171/247 [01:40<00:37,  2.05it/s]Loading train:  70%|██████▉   | 172/247 [01:41<00:45,  1.65it/s]Loading train:  70%|███████   | 173/247 [01:42<00:48,  1.53it/s]Loading train:  70%|███████   | 174/247 [01:43<00:50,  1.45it/s]Loading train:  71%|███████   | 175/247 [01:43<00:54,  1.33it/s]Loading train:  71%|███████▏  | 176/247 [01:44<00:48,  1.46it/s]Loading train:  72%|███████▏  | 177/247 [01:45<00:44,  1.57it/s]Loading train:  72%|███████▏  | 178/247 [01:45<00:41,  1.65it/s]Loading train:  72%|███████▏  | 179/247 [01:46<00:39,  1.71it/s]Loading train:  73%|███████▎  | 180/247 [01:46<00:38,  1.76it/s]Loading train:  73%|███████▎  | 181/247 [01:47<00:36,  1.81it/s]Loading train:  74%|███████▎  | 182/247 [01:47<00:36,  1.78it/s]Loading train:  74%|███████▍  | 183/247 [01:48<00:35,  1.79it/s]Loading train:  74%|███████▍  | 184/247 [01:48<00:35,  1.80it/s]Loading train:  75%|███████▍  | 185/247 [01:49<00:34,  1.78it/s]Loading train:  75%|███████▌  | 186/247 [01:49<00:33,  1.82it/s]Loading train:  76%|███████▌  | 187/247 [01:50<00:33,  1.81it/s]Loading train:  76%|███████▌  | 188/247 [01:51<00:32,  1.83it/s]Loading train:  77%|███████▋  | 189/247 [01:51<00:31,  1.83it/s]Loading train:  77%|███████▋  | 190/247 [01:52<00:31,  1.83it/s]Loading train:  77%|███████▋  | 191/247 [01:52<00:30,  1.84it/s]Loading train:  78%|███████▊  | 192/247 [01:53<00:30,  1.83it/s]Loading train:  78%|███████▊  | 193/247 [01:53<00:29,  1.84it/s]Loading train:  79%|███████▊  | 194/247 [01:54<00:28,  1.83it/s]Loading train:  79%|███████▉  | 195/247 [01:54<00:27,  1.86it/s]Loading train:  79%|███████▉  | 196/247 [01:55<00:27,  1.86it/s]Loading train:  80%|███████▉  | 197/247 [01:55<00:27,  1.83it/s]Loading train:  80%|████████  | 198/247 [01:56<00:27,  1.80it/s]Loading train:  81%|████████  | 199/247 [01:57<00:26,  1.84it/s]Loading train:  81%|████████  | 200/247 [01:57<00:25,  1.86it/s]Loading train:  81%|████████▏ | 201/247 [01:58<00:24,  1.88it/s]Loading train:  82%|████████▏ | 202/247 [01:58<00:23,  1.89it/s]Loading train:  82%|████████▏ | 203/247 [01:59<00:23,  1.90it/s]Loading train:  83%|████████▎ | 204/247 [01:59<00:22,  1.91it/s]Loading train:  83%|████████▎ | 205/247 [02:00<00:21,  1.91it/s]Loading train:  83%|████████▎ | 206/247 [02:00<00:21,  1.91it/s]Loading train:  84%|████████▍ | 207/247 [02:01<00:20,  1.92it/s]Loading train:  84%|████████▍ | 208/247 [02:01<00:20,  1.93it/s]Loading train:  85%|████████▍ | 209/247 [02:02<00:19,  1.92it/s]Loading train:  85%|████████▌ | 210/247 [02:02<00:19,  1.91it/s]Loading train:  85%|████████▌ | 211/247 [02:03<00:18,  1.90it/s]Loading train:  86%|████████▌ | 212/247 [02:03<00:18,  1.85it/s]Loading train:  86%|████████▌ | 213/247 [02:04<00:18,  1.87it/s]Loading train:  87%|████████▋ | 214/247 [02:05<00:23,  1.42it/s]Loading train:  87%|████████▋ | 215/247 [02:07<00:35,  1.12s/it]Loading train:  87%|████████▋ | 216/247 [02:10<00:51,  1.65s/it]Loading train:  88%|████████▊ | 217/247 [02:12<00:56,  1.87s/it]Loading train:  88%|████████▊ | 218/247 [02:15<00:58,  2.01s/it]Loading train:  89%|████████▊ | 219/247 [02:17<00:59,  2.14s/it]Loading train:  89%|████████▉ | 220/247 [02:20<01:01,  2.27s/it]Loading train:  89%|████████▉ | 221/247 [02:22<01:01,  2.35s/it]Loading train:  90%|████████▉ | 222/247 [02:25<01:01,  2.46s/it]Loading train:  90%|█████████ | 223/247 [02:27<00:59,  2.48s/it]Loading train:  91%|█████████ | 224/247 [02:30<00:57,  2.50s/it]Loading train:  91%|█████████ | 225/247 [02:33<00:55,  2.52s/it]Loading train:  91%|█████████▏| 226/247 [02:35<00:53,  2.55s/it]Loading train:  92%|█████████▏| 227/247 [02:38<00:50,  2.52s/it]Loading train:  92%|█████████▏| 228/247 [02:40<00:47,  2.52s/it]Loading train:  93%|█████████▎| 229/247 [02:43<00:45,  2.50s/it]Loading train:  93%|█████████▎| 230/247 [02:46<00:48,  2.87s/it]Loading train:  94%|█████████▎| 231/247 [02:50<00:51,  3.22s/it]Loading train:  94%|█████████▍| 232/247 [02:55<00:52,  3.48s/it]Loading train:  94%|█████████▍| 233/247 [02:58<00:49,  3.57s/it]Loading train:  95%|█████████▍| 234/247 [03:02<00:46,  3.60s/it]Loading train:  95%|█████████▌| 235/247 [03:06<00:43,  3.62s/it]Loading train:  96%|█████████▌| 236/247 [03:09<00:40,  3.67s/it]Loading train:  96%|█████████▌| 237/247 [03:13<00:36,  3.65s/it]Loading train:  96%|█████████▋| 238/247 [03:16<00:32,  3.56s/it]Loading train:  97%|█████████▋| 239/247 [03:20<00:28,  3.54s/it]Loading train:  97%|█████████▋| 240/247 [03:24<00:25,  3.67s/it]Loading train:  98%|█████████▊| 241/247 [03:28<00:22,  3.76s/it]Loading train:  98%|█████████▊| 242/247 [03:32<00:19,  3.82s/it]Loading train:  98%|█████████▊| 243/247 [03:36<00:15,  3.90s/it]Loading train:  99%|█████████▉| 244/247 [03:40<00:11,  3.87s/it]Loading train:  99%|█████████▉| 245/247 [03:43<00:07,  3.82s/it]Loading train: 100%|█████████▉| 246/247 [03:47<00:03,  3.78s/it]Loading train: 100%|██████████| 247/247 [03:51<00:00,  3.88s/it]Loading train: 100%|██████████| 247/247 [03:51<00:00,  1.07it/s]
concatenating: train:   0%|          | 0/247 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/247 [00:00<00:05, 47.98it/s]concatenating: train:   5%|▍         | 12/247 [00:00<00:04, 49.37it/s]concatenating: train:   7%|▋         | 17/247 [00:00<00:04, 49.32it/s]concatenating: train:   9%|▉         | 23/247 [00:00<00:04, 51.43it/s]concatenating: train:  12%|█▏        | 29/247 [00:00<00:04, 52.65it/s]concatenating: train:  14%|█▍        | 35/247 [00:00<00:04, 52.79it/s]concatenating: train:  17%|█▋        | 41/247 [00:00<00:03, 53.95it/s]concatenating: train:  19%|█▉        | 47/247 [00:00<00:03, 54.62it/s]concatenating: train:  21%|██▏       | 53/247 [00:01<00:03, 52.31it/s]concatenating: train:  24%|██▍       | 59/247 [00:01<00:03, 51.79it/s]concatenating: train:  26%|██▋       | 65/247 [00:01<00:03, 49.93it/s]concatenating: train:  28%|██▊       | 70/247 [00:01<00:03, 49.80it/s]concatenating: train:  30%|███       | 75/247 [00:01<00:03, 47.38it/s]concatenating: train:  33%|███▎      | 81/247 [00:01<00:03, 48.44it/s]concatenating: train:  35%|███▌      | 87/247 [00:01<00:03, 50.18it/s]concatenating: train:  38%|███▊      | 93/247 [00:01<00:02, 52.61it/s]concatenating: train:  40%|████      | 99/247 [00:01<00:02, 53.11it/s]concatenating: train:  43%|████▎     | 105/247 [00:02<00:02, 52.47it/s]concatenating: train:  45%|████▍     | 111/247 [00:02<00:02, 50.93it/s]concatenating: train:  47%|████▋     | 117/247 [00:02<00:02, 50.71it/s]concatenating: train:  50%|████▉     | 123/247 [00:02<00:02, 48.97it/s]concatenating: train:  52%|█████▏    | 128/247 [00:02<00:02, 48.24it/s]concatenating: train:  54%|█████▍    | 134/247 [00:02<00:02, 49.36it/s]concatenating: train:  57%|█████▋    | 140/247 [00:02<00:02, 51.94it/s]concatenating: train:  59%|█████▉    | 146/247 [00:02<00:01, 50.64it/s]concatenating: train:  62%|██████▏   | 152/247 [00:02<00:01, 49.64it/s]concatenating: train:  64%|██████▎   | 157/247 [00:03<00:01, 48.76it/s]concatenating: train:  66%|██████▌   | 163/247 [00:03<00:01, 51.42it/s]concatenating: train:  69%|██████▉   | 170/247 [00:03<00:01, 53.64it/s]concatenating: train:  71%|███████▏  | 176/247 [00:03<00:01, 50.57it/s]concatenating: train:  74%|███████▎  | 182/247 [00:03<00:01, 51.41it/s]concatenating: train:  76%|███████▌  | 188/247 [00:03<00:01, 51.63it/s]concatenating: train:  79%|███████▊  | 194/247 [00:03<00:01, 52.77it/s]concatenating: train:  81%|████████  | 200/247 [00:03<00:00, 49.13it/s]concatenating: train:  83%|████████▎ | 205/247 [00:04<00:00, 47.53it/s]concatenating: train:  85%|████████▌ | 210/247 [00:04<00:00, 45.61it/s]concatenating: train:  87%|████████▋ | 215/247 [00:04<00:00, 45.38it/s]concatenating: train:  89%|████████▉ | 220/247 [00:04<00:00, 44.99it/s]concatenating: train:  91%|█████████ | 225/247 [00:04<00:00, 43.73it/s]concatenating: train:  93%|█████████▎| 230/247 [00:04<00:00, 44.40it/s]concatenating: train:  95%|█████████▌| 235/247 [00:04<00:00, 45.10it/s]concatenating: train:  97%|█████████▋| 240/247 [00:04<00:00, 41.91it/s]concatenating: train:  99%|█████████▉| 245/247 [00:04<00:00, 42.38it/s]concatenating: train: 100%|██████████| 247/247 [00:05<00:00, 49.02it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:14<00:59, 14.83s/it]Loading test:  40%|████      | 2/5 [00:26<00:41, 13.94s/it]Loading test:  60%|██████    | 3/5 [00:35<00:24, 12.44s/it]Loading test:  80%|████████  | 4/5 [00:42<00:10, 10.78s/it]Loading test: 100%|██████████| 5/5 [00:54<00:00, 11.14s/it]Loading test: 100%|██████████| 5/5 [00:54<00:00, 10.90s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 52.47it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 0  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 0  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 0  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 88, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 88, 52, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 88, 52, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 88, 52, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 88, 52, 40)   14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 88, 52, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 88, 52, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 44, 26, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 44, 26, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 44, 26, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 44, 26, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 44, 26, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 44, 26, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 44, 26, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 44, 26, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 44, 26, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 22, 13, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 22, 13, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 22, 13, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 22, 13, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 22, 13, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 22, 13, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 22, 13, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 22, 13, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 22, 13, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 22, 13, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 44, 26, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 44, 26, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 44, 26, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 44, 26, 80)   320         conv2d_7[0][0]                   2020-01-22 04:42:18.732653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 04:42:18.732768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 04:42:18.732786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 04:42:18.732798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 04:42:18.734310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
activation_7 (Activation)       (None, 44, 26, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 44, 26, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 44, 26, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 44, 26, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 44, 26, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 44, 26, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 88, 52, 40)   44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 88, 52, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 88, 52, 40)   28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 88, 52, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 88, 52, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 88, 52, 40)   14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 88, 52, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 88, 52, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 88, 52, 120)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 88, 52, 120)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 88, 52, 13)   1573        dropout_5[0][0]                  
==================================================================================================
Total params: 889,253
Trainable params: 887,653
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0
------------------------------------------------------------------
class_weights [6.22937750e-02 3.14210400e-02 7.86368548e-02 9.57788368e-03
 2.85391838e-02 7.22310037e-03 8.59163269e-02 1.15049767e-01
 8.99935258e-02 1.30525869e-02 2.93810629e-01 1.84261706e-01
 2.23620757e-04]
Train on 9433 samples, validate on 198 samples
Epoch 1/300
 - 29s - loss: 0.6920 - acc: 0.9119 - mDice: 0.2536 - val_loss: 0.5479 - val_acc: 0.9386 - val_mDice: 0.1842

Epoch 00001: val_mDice improved from -inf to 0.18423, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/300
 - 25s - loss: 0.5957 - acc: 0.9317 - mDice: 0.3570 - val_loss: 0.3669 - val_acc: 0.9422 - val_mDice: 0.1936

Epoch 00002: val_mDice improved from 0.18423 to 0.19360, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 3/300
 - 25s - loss: 0.5672 - acc: 0.9359 - mDice: 0.3877 - val_loss: 0.3059 - val_acc: 0.9446 - val_mDice: 0.2237

Epoch 00003: val_mDice improved from 0.19360 to 0.22373, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/300
 - 25s - loss: 0.5178 - acc: 0.9397 - mDice: 0.4411 - val_loss: 0.2242 - val_acc: 0.9464 - val_mDice: 0.2278

Epoch 00004: val_mDice improved from 0.22373 to 0.22782, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 5/300
 - 24s - loss: 0.5050 - acc: 0.9416 - mDice: 0.4549 - val_loss: 0.2203 - val_acc: 0.9475 - val_mDice: 0.2323

Epoch 00005: val_mDice improved from 0.22782 to 0.23226, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 6/300
 - 24s - loss: 0.4986 - acc: 0.9427 - mDice: 0.4619 - val_loss: 0.1851 - val_acc: 0.9479 - val_mDice: 0.2311

Epoch 00006: val_mDice did not improve from 0.23226
Epoch 7/300
 - 24s - loss: 0.4911 - acc: 0.9440 - mDice: 0.4699 - val_loss: 0.2053 - val_acc: 0.9478 - val_mDice: 0.2316

Epoch 00007: val_mDice did not improve from 0.23226
Epoch 8/300
 - 24s - loss: 0.4836 - acc: 0.9449 - mDice: 0.4781 - val_loss: 0.2157 - val_acc: 0.9478 - val_mDice: 0.2290

Epoch 00008: val_mDice did not improve from 0.23226
Epoch 9/300
 - 24s - loss: 0.4801 - acc: 0.9456 - mDice: 0.4817 - val_loss: 0.1953 - val_acc: 0.9501 - val_mDice: 0.2399

Epoch 00009: val_mDice improved from 0.23226 to 0.23985, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 10/300
 - 24s - loss: 0.4739 - acc: 0.9462 - mDice: 0.4885 - val_loss: 0.2840 - val_acc: 0.9493 - val_mDice: 0.2439

Epoch 00010: val_mDice improved from 0.23985 to 0.24388, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 11/300
 - 25s - loss: 0.4619 - acc: 0.9466 - mDice: 0.5014 - val_loss: 0.2116 - val_acc: 0.9497 - val_mDice: 0.2471

Epoch 00011: val_mDice improved from 0.24388 to 0.24713, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 12/300
 - 24s - loss: 0.4526 - acc: 0.9471 - mDice: 0.5116 - val_loss: 0.2196 - val_acc: 0.9473 - val_mDice: 0.2405

Epoch 00012: val_mDice did not improve from 0.24713
Epoch 13/300
 - 24s - loss: 0.4465 - acc: 0.9475 - mDice: 0.5181 - val_loss: 0.2291 - val_acc: 0.9490 - val_mDice: 0.2457

Epoch 00013: val_mDice did not improve from 0.24713
Epoch 14/300
 - 24s - loss: 0.4287 - acc: 0.9474 - mDice: 0.5374 - val_loss: 0.2296 - val_acc: 0.9494 - val_mDice: 0.2617

Epoch 00014: val_mDice improved from 0.24713 to 0.26173, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 15/300
 - 24s - loss: 0.4001 - acc: 0.9481 - mDice: 0.5684 - val_loss: 0.2287 - val_acc: 0.9493 - val_mDice: 0.2699

Epoch 00015: val_mDice improved from 0.26173 to 0.26988, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 16/300
 - 24s - loss: 0.3719 - acc: 0.9488 - mDice: 0.5989 - val_loss: 0.1129 - val_acc: 0.9505 - val_mDice: 0.2799

Epoch 00016: val_mDice improved from 0.26988 to 0.27994, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 17/300
 - 24s - loss: 0.3586 - acc: 0.9495 - mDice: 0.6133 - val_loss: 0.1483 - val_acc: 0.9506 - val_mDice: 0.2796

Epoch 00017: val_mDice did not improve from 0.27994
Epoch 18/300
 - 24s - loss: 0.3488 - acc: 0.9501 - mDice: 0.6239 - val_loss: 0.1370 - val_acc: 0.9516 - val_mDice: 0.2856

Epoch 00018: val_mDice improved from 0.27994 to 0.28564, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 19/300
 - 24s - loss: 0.3476 - acc: 0.9504 - mDice: 0.6251 - val_loss: 0.0977 - val_acc: 0.9508 - val_mDice: 0.2838

Epoch 00019: val_mDice did not improve from 0.28564
Epoch 20/300
 - 24s - loss: 0.3427 - acc: 0.9507 - mDice: 0.6304 - val_loss: 0.0768 - val_acc: 0.9511 - val_mDice: 0.2851

Epoch 00020: val_mDice did not improve from 0.28564
Epoch 21/300
 - 24s - loss: 0.3385 - acc: 0.9511 - mDice: 0.6350 - val_loss: 0.0700 - val_acc: 0.9516 - val_mDice: 0.2879

Epoch 00021: val_mDice improved from 0.28564 to 0.28788, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 22/300
 - 24s - loss: 0.3331 - acc: 0.9514 - mDice: 0.6408 - val_loss: 0.0914 - val_acc: 0.9507 - val_mDice: 0.2887

Epoch 00022: val_mDice improved from 0.28788 to 0.28870, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 23/300
 - 23s - loss: 0.3278 - acc: 0.9517 - mDice: 0.6466 - val_loss: 0.1005 - val_acc: 0.9503 - val_mDice: 0.2860

Epoch 00023: val_mDice did not improve from 0.28870
Epoch 24/300
 - 24s - loss: 0.3267 - acc: 0.9519 - mDice: 0.6477 - val_loss: 0.0690 - val_acc: 0.9507 - val_mDice: 0.2882

Epoch 00024: val_mDice did not improve from 0.28870
Epoch 25/300
 - 24s - loss: 0.3244 - acc: 0.9523 - mDice: 0.6502 - val_loss: 0.0137 - val_acc: 0.9517 - val_mDice: 0.2869

Epoch 00025: val_mDice did not improve from 0.28870
Epoch 26/300
 - 23s - loss: 0.3200 - acc: 0.9526 - mDice: 0.6549 - val_loss: 0.0504 - val_acc: 0.9510 - val_mDice: 0.2865

Epoch 00026: val_mDice did not improve from 0.28870
Epoch 27/300
 - 24s - loss: 0.3159 - acc: 0.9529 - mDice: 0.6594 - val_loss: 0.0795 - val_acc: 0.9500 - val_mDice: 0.2822

Epoch 00027: val_mDice did not improve from 0.28870
Epoch 28/300
 - 24s - loss: 0.3163 - acc: 0.9530 - mDice: 0.6590 - val_loss: 0.0907 - val_acc: 0.9502 - val_mDice: 0.2819

Epoch 00028: val_mDice did not improve from 0.28870
Epoch 29/300
 - 24s - loss: 0.3113 - acc: 0.9533 - mDice: 0.6643 - val_loss: 0.0845 - val_acc: 0.9511 - val_mDice: 0.2891

Epoch 00029: val_mDice improved from 0.28870 to 0.28908, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 30/300
 - 24s - loss: 0.3104 - acc: 0.9535 - mDice: 0.6653 - val_loss: 0.0888 - val_acc: 0.9509 - val_mDice: 0.2853

Epoch 00030: val_mDice did not improve from 0.28908
Epoch 31/300
 - 23s - loss: 0.3109 - acc: 0.9536 - mDice: 0.6648 - val_loss: 0.0886 - val_acc: 0.9505 - val_mDice: 0.2841

Epoch 00031: val_mDice did not improve from 0.28908
Epoch 32/300
 - 24s - loss: 0.3070 - acc: 0.9538 - mDice: 0.6690 - val_loss: 0.0988 - val_acc: 0.9502 - val_mDice: 0.2825

Epoch 00032: val_mDice did not improve from 0.28908
Epoch 33/300
 - 23s - loss: 0.3056 - acc: 0.9540 - mDice: 0.6706 - val_loss: 0.1475 - val_acc: 0.9504 - val_mDice: 0.2818

Epoch 00033: val_mDice did not improve from 0.28908
Epoch 34/300
 - 24s - loss: 0.3060 - acc: 0.9541 - mDice: 0.6701 - val_loss: 0.0759 - val_acc: 0.9506 - val_mDice: 0.2850

Epoch 00034: val_mDice did not improve from 0.28908
Epoch 35/300
 - 24s - loss: 0.2998 - acc: 0.9544 - mDice: 0.6768 - val_loss: 0.0710 - val_acc: 0.9515 - val_mDice: 0.2867

Epoch 00035: val_mDice did not improve from 0.28908
Epoch 36/300
 - 24s - loss: 0.3001 - acc: 0.9544 - mDice: 0.6765 - val_loss: 0.0754 - val_acc: 0.9508 - val_mDice: 0.2853

Epoch 00036: val_mDice did not improve from 0.28908
Epoch 37/300
 - 24s - loss: 0.3009 - acc: 0.9547 - mDice: 0.6756 - val_loss: 0.1677 - val_acc: 0.9510 - val_mDice: 0.2828

Epoch 00037: val_mDice did not improve from 0.28908
Epoch 38/300
 - 24s - loss: 0.2943 - acc: 0.9548 - mDice: 0.6827 - val_loss: 0.0741 - val_acc: 0.9513 - val_mDice: 0.2866

Epoch 00038: val_mDice did not improve from 0.28908
Epoch 39/300
 - 24s - loss: 0.2958 - acc: 0.9549 - mDice: 0.6811 - val_loss: 0.1213 - val_acc: 0.9511 - val_mDice: 0.2856

Epoch 00039: val_mDice did not improve from 0.28908
Epoch 40/300
 - 24s - loss: 0.2904 - acc: 0.9551 - mDice: 0.6869 - val_loss: 0.1139 - val_acc: 0.9490 - val_mDice: 0.2795

Epoch 00040: val_mDice did not improve from 0.28908
Epoch 41/300
 - 24s - loss: 0.2926 - acc: 0.9552 - mDice: 0.6846 - val_loss: 0.1483 - val_acc: 0.9507 - val_mDice: 0.2818

Epoch 00041: val_mDice did not improve from 0.28908
Epoch 42/300
 - 24s - loss: 0.2903 - acc: 0.9553 - mDice: 0.6870 - val_loss: 0.1435 - val_acc: 0.9504 - val_mDice: 0.2820

Epoch 00042: val_mDice did not improve from 0.28908
Epoch 43/300
 - 25s - loss: 0.2904 - acc: 0.9554 - mDice: 0.6869 - val_loss: 0.0895 - val_acc: 0.9506 - val_mDice: 0.2846

Epoch 00043: val_mDice did not improve from 0.28908
Epoch 44/300
 - 25s - loss: 0.2887 - acc: 0.9555 - mDice: 0.6888 - val_loss: 0.1008 - val_acc: 0.9506 - val_mDice: 0.2832

Epoch 00044: val_mDice did not improve from 0.28908

Epoch 00044: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 45/300
 - 25s - loss: 0.2868 - acc: 0.9559 - mDice: 0.6908 - val_loss: 0.1325 - val_acc: 0.9508 - val_mDice: 0.2826

Epoch 00045: val_mDice did not improve from 0.28908
Epoch 46/300
 - 24s - loss: 0.2856 - acc: 0.9559 - mDice: 0.6921 - val_loss: 0.1251 - val_acc: 0.9507 - val_mDice: 0.2837

Epoch 00046: val_mDice did not improve from 0.28908
Epoch 47/300
 - 25s - loss: 0.2813 - acc: 0.9559 - mDice: 0.6968 - val_loss: 0.1518 - val_acc: 0.9506 - val_mDice: 0.2809

Epoch 00047: val_mDice did not improve from 0.28908
Epoch 48/300
 - 24s - loss: 0.2842 - acc: 0.9561 - mDice: 0.6937 - val_loss: 0.1306 - val_acc: 0.9509 - val_mDice: 0.2823

Epoch 00048: val_mDice did not improve from 0.28908
Epoch 49/300
 - 25s - loss: 0.2857 - acc: 0.9563 - mDice: 0.6920 - val_loss: 0.1487 - val_acc: 0.9512 - val_mDice: 0.2871

Epoch 00049: val_mDice did not improve from 0.28908
Epoch 50/300
 - 24s - loss: 0.2811 - acc: 0.9562 - mDice: 0.6970 - val_loss: 0.1513 - val_acc: 0.9511 - val_mDice: 0.2847

Epoch 00050: val_mDice did not improve from 0.28908
Epoch 51/300
 - 24s - loss: 0.2823 - acc: 0.9563 - mDice: 0.6957 - val_loss: 0.1510 - val_acc: 0.9510 - val_mDice: 0.2847

Epoch 00051: val_mDice did not improve from 0.28908
Epoch 52/300
 - 24s - loss: 0.2825 - acc: 0.9564 - mDice: 0.6955 - val_loss: 0.1646 - val_acc: 0.9507 - val_mDice: 0.2845

Epoch 00052: val_mDice did not improve from 0.28908
Epoch 53/300
 - 24s - loss: 0.2787 - acc: 0.9564 - mDice: 0.6996 - val_loss: 0.1666 - val_acc: 0.9495 - val_mDice: 0.2782

Epoch 00053: val_mDice did not improve from 0.28908
Epoch 54/300
 - 24s - loss: 0.2789 - acc: 0.9564 - mDice: 0.6994 - val_loss: 0.1718 - val_acc: 0.9506 - val_mDice: 0.2821

Epoch 00054: val_mDice did not improve from 0.28908
Epoch 55/300
 - 24s - loss: 0.2794 - acc: 0.9566 - mDice: 0.6988 - val_loss: 0.1307 - val_acc: 0.9500 - val_mDice: 0.2806

Epoch 00055: val_mDice did not improve from 0.28908
Epoch 56/300
 - 24s - loss: 0.2772 - acc: 0.9566 - mDice: 0.7011 - val_loss: 0.1356 - val_acc: 0.9499 - val_mDice: 0.2817

Epoch 00056: val_mDice did not improve from 0.28908
Epoch 57/300
 - 23s - loss: 0.2784 - acc: 0.9566 - mDice: 0.6999 - val_loss: 0.1359 - val_acc: 0.9503 - val_mDice: 0.2822

Epoch 00057: val_mDice did not improve from 0.28908
Epoch 58/300
 - 24s - loss: 0.2794 - acc: 0.9566 - mDice: 0.6988 - val_loss: 0.1761 - val_acc: 0.9510 - val_mDice: 0.2835

Epoch 00058: val_mDice did not improve from 0.28908
Epoch 59/300
 - 23s - loss: 0.2766 - acc: 0.9568 - mDice: 0.7019 - val_loss: 0.1869 - val_acc: 0.9508 - val_mDice: 0.2803

Epoch 00059: val_mDice did not improve from 0.28908

Epoch 00059: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 60/300
 - 23s - loss: 0.2791 - acc: 0.9568 - mDice: 0.6992 - val_loss: 0.1742 - val_acc: 0.9508 - val_mDice: 0.2803

Epoch 00060: val_mDice did not improve from 0.28908
Epoch 61/300
 - 24s - loss: 0.2774 - acc: 0.9569 - mDice: 0.7010 - val_loss: 0.1516 - val_acc: 0.9503 - val_mDice: 0.2827

Epoch 00061: val_mDice did not improve from 0.28908
Epoch 62/300
 - 23s - loss: 0.2757 - acc: 0.9570 - mDice: 0.7028 - val_loss: 0.1481 - val_acc: 0.9509 - val_mDice: 0.2818

Epoch 00062: val_mDice did not improve from 0.28908
Epoch 63/300
 - 23s - loss: 0.2726 - acc: 0.9570 - mDice: 0.7061 - val_loss: 0.1498 - val_acc: 0.9505 - val_mDice: 0.2823

Epoch 00063: val_mDice did not improve from 0.28908
Epoch 64/300
 - 24s - loss: 0.2740 - acc: 0.9570 - mDice: 0.7047 - val_loss: 0.1560 - val_acc: 0.9499 - val_mDice: 0.2793

Epoch 00064: val_mDice did not improve from 0.28908
Epoch 65/300
 - 23s - loss: 0.2741 - acc: 0.9570 - mDice: 0.7046 - val_loss: 0.1859 - val_acc: 0.9504 - val_mDice: 0.2798

Epoch 00065: val_mDice did not improve from 0.28908
Epoch 66/300
 - 23s - loss: 0.2736 - acc: 0.9571 - mDice: 0.7051 - val_loss: 0.1585 - val_acc: 0.9503 - val_mDice: 0.2800

Epoch 00066: val_mDice did not improve from 0.28908
Epoch 67/300
 - 24s - loss: 0.2744 - acc: 0.9570 - mDice: 0.7042 - val_loss: 0.1870 - val_acc: 0.9506 - val_mDice: 0.2834

Epoch 00067: val_mDice did not improve from 0.28908
Epoch 68/300
 - 23s - loss: 0.2716 - acc: 0.9571 - mDice: 0.7072 - val_loss: 0.1686 - val_acc: 0.9505 - val_mDice: 0.2832

Epoch 00068: val_mDice did not improve from 0.28908
Epoch 69/300
 - 24s - loss: 0.2725 - acc: 0.9571 - mDice: 0.7063 - val_loss: 0.1741 - val_acc: 0.9507 - val_mDice: 0.2851

Epoch 00069: val_mDice did not improve from 0.28908
Restoring model weights from the end of the best epoch
Epoch 00069: early stopping
{'val_loss': [0.5478776386289885, 0.3668667179993307, 0.30586855922534006, 0.22416210801086642, 0.22031302103358838, 0.18511521816253662, 0.20528840017740171, 0.21571583166304562, 0.1952682715473753, 0.2839679134765057, 0.2115518848101298, 0.21956249050363297, 0.22909611644166888, 0.22958642924486689, 0.22865345782478047, 0.11288604142633502, 0.14827247005134744, 0.13696525878075397, 0.09770490522637512, 0.0768060554947817, 0.06998793778451856, 0.09137617966931577, 0.10047352088220192, 0.06904515689897417, 0.013741258494179658, 0.050383967770771546, 0.07952474505461828, 0.0907018528310488, 0.08449745189511415, 0.08878320278693931, 0.08862156518782029, 0.09880123125633808, 0.14754555185297222, 0.07585903172465888, 0.07102562855891507, 0.075420115782757, 0.16772471885714266, 0.0741183723727561, 0.12131874733651528, 0.11385233700275421, 0.14834195213636966, 0.14354883179520117, 0.0895470647950365, 0.10083586918284194, 0.132479762522071, 0.12509173542411642, 0.15184647282303282, 0.1306035690611661, 0.14866108221538138, 0.15134966712106357, 0.15101207834621422, 0.16463434969009172, 0.16662415378077916, 0.17181359607763966, 0.13065298748287288, 0.13555186459644122, 0.13591909291918833, 0.17612167517419416, 0.18685368228365073, 0.17418440122797033, 0.15163479143320913, 0.14811786723257314, 0.14980367176008946, 0.15595048094977332, 0.18589495575864506, 0.1584929899133817, 0.18701248369508922, 0.168563534496258, 0.1741408591694904], 'val_acc': [0.9386125249092026, 0.9421785624340328, 0.9446265546962468, 0.946369286739465, 0.9474564236823959, 0.9478681105555911, 0.9478360980448096, 0.9477698766823971, 0.9500512149598863, 0.9493194607773212, 0.9497223135196802, 0.9473261929521657, 0.9490402214454882, 0.949417694048448, 0.9492962890201144, 0.9505081357377948, 0.9506328587580208, 0.9516239720161515, 0.9508116546303335, 0.9510831598079565, 0.951577623685201, 0.9507277752413894, 0.9503403817764436, 0.9506803094738662, 0.9516681234041849, 0.9510401246523617, 0.9499607044036942, 0.9501527457526235, 0.951051156328182, 0.9508524976595484, 0.9505478745759136, 0.9502200731123337, 0.9504253707750879, 0.950576561869997, 0.9515058807652406, 0.9508381443794327, 0.9509661721460747, 0.9512531378052451, 0.9510677095615503, 0.9490082227822506, 0.95068693943698, 0.9503723870624196, 0.9505809816447172, 0.9506196150876055, 0.9508337282171153, 0.9507134231654081, 0.9506141013569303, 0.9508547000210694, 0.9511880146132575, 0.9511195951037936, 0.9510246623646129, 0.9506990837328362, 0.9494949531073522, 0.9505677397805031, 0.9500280419985453, 0.949866898132093, 0.9503381745983855, 0.950989351730154, 0.9507730247998478, 0.9508072414783516, 0.9502984345561326, 0.9508845035475914, 0.9505280117795925, 0.9498977907980332, 0.9504154312490213, 0.950334860820963, 0.950608572574577, 0.9504816489990311, 0.9506836238533559], 'val_mDice': [0.18422971735473234, 0.19359828889219446, 0.22372726313393526, 0.22782159619259112, 0.23226460946178196, 0.23109655176298788, 0.23162196892680545, 0.22901462899013, 0.23985426263375717, 0.24388418196126668, 0.24713148338475613, 0.2404943210533773, 0.2457133498080451, 0.2617279945148362, 0.26988215955218886, 0.2799424852686699, 0.2796084936520066, 0.28563812367542823, 0.2837686317436623, 0.28505845525951096, 0.2878832693653877, 0.28870465756967817, 0.2859522828249016, 0.2881531593474475, 0.2869199768762396, 0.28645697318845326, 0.2821984216570854, 0.28186310590668157, 0.28907523718145156, 0.2852667804920312, 0.2841032656154247, 0.2824872543263917, 0.28177878087518193, 0.2849925154387349, 0.2867276490938784, 0.28525779915578436, 0.28284087114863926, 0.28660540531078976, 0.28560439294034784, 0.27953622885274165, 0.28175383444988367, 0.2819596230983734, 0.28455921736630524, 0.2831785703698794, 0.28263282489897024, 0.2837136229782393, 0.2809125253497952, 0.28227281833838935, 0.2870579068407868, 0.28469832542568746, 0.2847168423009641, 0.28451167498574115, 0.27819106592373416, 0.28213116263199334, 0.28062271281625284, 0.2816948619003248, 0.28221152075613387, 0.2834754653952338, 0.28027015773936953, 0.2802610010510743, 0.2827144901860844, 0.28179001672701404, 0.2822839457428817, 0.2793425718190694, 0.27979524110001747, 0.2799794268276956, 0.28337166082076354, 0.28323896108853697, 0.28510746224360034], 'loss': [0.6920487267473375, 0.5956882488104542, 0.5672235526361711, 0.5178348543622974, 0.5050291509639923, 0.4985607877195191, 0.49108389351383874, 0.4835587241459307, 0.48013398588354533, 0.4738815203964628, 0.4619447884258354, 0.45255955518852753, 0.446533679342922, 0.42871056843521765, 0.4001008276910073, 0.37189791827594615, 0.3585752763930654, 0.3487648125684591, 0.34763163729726504, 0.3427392428909293, 0.33851767046623693, 0.3331184422939488, 0.32779308404154633, 0.32673812902682686, 0.3243809534221189, 0.32003872370613756, 0.315947877896953, 0.31626933815938524, 0.3113341251170788, 0.3104209080994047, 0.3109153149951418, 0.3070371612143469, 0.3055551853340933, 0.3060174675757668, 0.29983858837626914, 0.30008273287781423, 0.30085829828243726, 0.2943196934473599, 0.29576474547601006, 0.2904248508620487, 0.2925570270523064, 0.29033338617295307, 0.2904115201881477, 0.2886759591332602, 0.28680490061117864, 0.28558466198284377, 0.2812529692629116, 0.284174074269069, 0.285663922208624, 0.2811035533497547, 0.2822676709793183, 0.2824830077118602, 0.27867969370755236, 0.27890112254281674, 0.2794098031239028, 0.2772472796454784, 0.27835434969669814, 0.27937924722263874, 0.27658764837112454, 0.2790791757833688, 0.27740565324643507, 0.27568860298803977, 0.2726183912855917, 0.27395655583326856, 0.2740666015421385, 0.27359233182257336, 0.27443665249292337, 0.2716081523603338, 0.27251964437232395], 'acc': [0.9118557143764886, 0.9317268357604985, 0.9359421323178038, 0.939713832462098, 0.9415734707645707, 0.9427166307994875, 0.9439597329809799, 0.9448682613568936, 0.9455548293464424, 0.9461549624941274, 0.9466076867079717, 0.9471111916843634, 0.9475447323837147, 0.9473984806949357, 0.9481222100163346, 0.9488333344307884, 0.9495154548194317, 0.9500923511282606, 0.9504406642878434, 0.9507198439831827, 0.9510890076411411, 0.9513615394882138, 0.9517459684940837, 0.9519334786405597, 0.952255542206509, 0.952605197206732, 0.9528569730963697, 0.9529737100979815, 0.9532528454989754, 0.9535225275262194, 0.9536289568166268, 0.9538060888485326, 0.9540085201360483, 0.9540869390283349, 0.9543510626586462, 0.9544353896520379, 0.9546973595092977, 0.9548323507350917, 0.9548873718313381, 0.9551160276776519, 0.9552282462389993, 0.955338219279236, 0.9554217348330779, 0.9555193135888254, 0.9558616940674165, 0.9559212552392823, 0.9559204221075089, 0.9561210446060494, 0.956317984456287, 0.9562080579472285, 0.9563260002605334, 0.956433725915632, 0.9564361584018476, 0.9564293458872984, 0.9565587331517483, 0.9566042088102488, 0.9566264031080733, 0.9565682313758925, 0.9567732344904091, 0.9568316124724796, 0.9569080645884132, 0.9569594237428575, 0.957008722325619, 0.9570071702732409, 0.957030407807158, 0.9570910579880462, 0.9569669309582928, 0.9571049109971701, 0.9571133909315949], 'mDice': [0.25357630884822396, 0.3569900552165302, 0.3876995298650906, 0.44106773994045545, 0.4548856257526667, 0.4618526766254201, 0.4699160495912562, 0.4780515533331203, 0.4817333739698344, 0.4884969758371982, 0.501407292615719, 0.5115612983754163, 0.5180708616640236, 0.5373851214033492, 0.5683535798842285, 0.5988723671532354, 0.6132827495942247, 0.6238959764645686, 0.6251085704386228, 0.6304099592704555, 0.6349659098757927, 0.6408100467900715, 0.6465699090364185, 0.6477114137518409, 0.6502408591709927, 0.6549265721841872, 0.6593543179345057, 0.6590015388149276, 0.6643471125958391, 0.6653236192792771, 0.6647831953546218, 0.6689715471498208, 0.670570873260359, 0.6700729003396656, 0.6767506597783398, 0.676485657009532, 0.6756314147944215, 0.6827160799590525, 0.6811497091305517, 0.6869219093740802, 0.6846169486482335, 0.6870188404845746, 0.6869231809630041, 0.6888072594919703, 0.6908245144170427, 0.6921461200102458, 0.6968384598492692, 0.693663523149278, 0.6920390940636324, 0.6969885698701409, 0.695722444229587, 0.6954806522158895, 0.6996016036626277, 0.6993674899062536, 0.6988065774615415, 0.701145791829258, 0.6999433758174654, 0.6988451989555389, 0.7018601770594607, 0.6991573135660472, 0.7009676367433051, 0.7028214507248044, 0.706146126294912, 0.7046941718234498, 0.704576488954321, 0.7050894322974627, 0.7041793250777265, 0.7072383741754673, 0.7062505414152808], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:01<00:06,  1.69s/it]predicting test subjects:  40%|████      | 2/5 [00:02<00:04,  1.50s/it]predicting test subjects:  60%|██████    | 3/5 [00:03<00:02,  1.35s/it]predicting test subjects:  80%|████████  | 4/5 [00:04<00:01,  1.19s/it]predicting test subjects: 100%|██████████| 5/5 [00:05<00:00,  1.14s/it]predicting test subjects: 100%|██████████| 5/5 [00:05<00:00,  1.12s/it]
Loading train:   0%|          | 0/247 [00:00<?, ?it/s]Loading train:   0%|          | 1/247 [00:00<01:16,  3.23it/s]Loading train:   1%|          | 2/247 [00:00<01:13,  3.32it/s]Loading train:   1%|          | 3/247 [00:00<01:11,  3.43it/s]Loading train:   2%|▏         | 4/247 [00:01<01:11,  3.39it/s]Loading train:   2%|▏         | 5/247 [00:01<01:10,  3.44it/s]Loading train:   2%|▏         | 6/247 [00:01<01:09,  3.47it/s]Loading train:   3%|▎         | 7/247 [00:02<01:08,  3.48it/s]Loading train:   3%|▎         | 8/247 [00:02<01:08,  3.48it/s]Loading train:   4%|▎         | 9/247 [00:02<01:08,  3.49it/s]Loading train:   4%|▍         | 10/247 [00:02<01:07,  3.49it/s]Loading train:   4%|▍         | 11/247 [00:03<01:07,  3.48it/s]Loading train:   5%|▍         | 12/247 [00:03<01:06,  3.53it/s]Loading train:   5%|▌         | 13/247 [00:03<01:05,  3.56it/s]Loading train:   6%|▌         | 14/247 [00:03<01:05,  3.56it/s]Loading train:   6%|▌         | 15/247 [00:04<01:04,  3.57it/s]Loading train:   6%|▋         | 16/247 [00:04<01:04,  3.56it/s]Loading train:   7%|▋         | 17/247 [00:04<01:04,  3.56it/s]Loading train:   7%|▋         | 18/247 [00:05<01:04,  3.55it/s]Loading train:   8%|▊         | 19/247 [00:05<01:05,  3.49it/s]Loading train:   8%|▊         | 20/247 [00:05<01:05,  3.49it/s]Loading train:   9%|▊         | 21/247 [00:05<01:05,  3.47it/s]Loading train:   9%|▉         | 22/247 [00:06<01:04,  3.48it/s]Loading train:   9%|▉         | 23/247 [00:06<01:01,  3.61it/s]Loading train:  10%|▉         | 24/247 [00:06<01:00,  3.71it/s]Loading train:  10%|█         | 25/247 [00:07<00:59,  3.76it/s]Loading train:  11%|█         | 26/247 [00:07<00:57,  3.83it/s]Loading train:  11%|█         | 27/247 [00:07<00:57,  3.84it/s]Loading train:  11%|█▏        | 28/247 [00:07<00:56,  3.89it/s]Loading train:  12%|█▏        | 29/247 [00:08<00:55,  3.94it/s]Loading train:  12%|█▏        | 30/247 [00:08<00:54,  3.96it/s]Loading train:  13%|█▎        | 31/247 [00:08<00:54,  3.99it/s]Loading train:  13%|█▎        | 32/247 [00:08<00:53,  4.01it/s]Loading train:  13%|█▎        | 33/247 [00:09<00:53,  4.02it/s]Loading train:  14%|█▍        | 34/247 [00:09<00:52,  4.02it/s]Loading train:  14%|█▍        | 35/247 [00:09<00:52,  4.02it/s]Loading train:  15%|█▍        | 36/247 [00:09<00:52,  4.02it/s]Loading train:  15%|█▍        | 37/247 [00:10<00:52,  4.01it/s]Loading train:  15%|█▌        | 38/247 [00:10<00:51,  4.04it/s]Loading train:  16%|█▌        | 39/247 [00:10<00:52,  3.94it/s]Loading train:  16%|█▌        | 40/247 [00:10<00:52,  3.97it/s]Loading train:  17%|█▋        | 41/247 [00:11<00:52,  3.94it/s]Loading train:  17%|█▋        | 42/247 [00:11<00:53,  3.87it/s]Loading train:  17%|█▋        | 43/247 [00:11<00:52,  3.86it/s]Loading train:  18%|█▊        | 44/247 [00:11<00:53,  3.82it/s]Loading train:  18%|█▊        | 45/247 [00:12<00:52,  3.84it/s]Loading train:  19%|█▊        | 46/247 [00:12<00:53,  3.75it/s]Loading train:  19%|█▉        | 47/247 [00:12<00:53,  3.76it/s]Loading train:  19%|█▉        | 48/247 [00:12<00:52,  3.77it/s]Loading train:  20%|█▉        | 49/247 [00:13<00:52,  3.75it/s]Loading train:  20%|██        | 50/247 [00:13<00:52,  3.76it/s]Loading train:  21%|██        | 51/247 [00:13<00:52,  3.72it/s]Loading train:  21%|██        | 52/247 [00:14<00:53,  3.64it/s]Loading train:  21%|██▏       | 53/247 [00:14<00:52,  3.70it/s]Loading train:  22%|██▏       | 54/247 [00:14<00:51,  3.76it/s]Loading train:  22%|██▏       | 55/247 [00:14<00:50,  3.79it/s]Loading train:  23%|██▎       | 56/247 [00:15<00:50,  3.76it/s]Loading train:  23%|██▎       | 57/247 [00:15<00:50,  3.79it/s]Loading train:  23%|██▎       | 58/247 [00:15<00:49,  3.83it/s]Loading train:  24%|██▍       | 59/247 [00:15<00:50,  3.69it/s]Loading train:  24%|██▍       | 60/247 [00:16<00:51,  3.65it/s]Loading train:  25%|██▍       | 61/247 [00:16<00:52,  3.56it/s]Loading train:  25%|██▌       | 62/247 [00:16<00:51,  3.57it/s]Loading train:  26%|██▌       | 63/247 [00:16<00:51,  3.57it/s]Loading train:  26%|██▌       | 64/247 [00:17<00:52,  3.50it/s]Loading train:  26%|██▋       | 65/247 [00:17<00:52,  3.47it/s]Loading train:  27%|██▋       | 66/247 [00:17<00:52,  3.45it/s]Loading train:  27%|██▋       | 67/247 [00:18<00:51,  3.49it/s]Loading train:  28%|██▊       | 68/247 [00:18<00:51,  3.47it/s]Loading train:  28%|██▊       | 69/247 [00:18<00:50,  3.50it/s]Loading train:  28%|██▊       | 70/247 [00:19<00:51,  3.41it/s]Loading train:  29%|██▊       | 71/247 [00:19<00:50,  3.45it/s]Loading train:  29%|██▉       | 72/247 [00:19<00:50,  3.48it/s]Loading train:  30%|██▉       | 73/247 [00:19<00:50,  3.46it/s]Loading train:  30%|██▉       | 74/247 [00:20<00:49,  3.51it/s]Loading train:  30%|███       | 75/247 [00:20<00:48,  3.53it/s]Loading train:  31%|███       | 76/247 [00:20<00:48,  3.50it/s]Loading train:  31%|███       | 77/247 [00:21<00:49,  3.40it/s]Loading train:  32%|███▏      | 78/247 [00:21<00:51,  3.25it/s]Loading train:  32%|███▏      | 79/247 [00:21<00:52,  3.20it/s]Loading train:  32%|███▏      | 80/247 [00:22<00:50,  3.30it/s]Loading train:  33%|███▎      | 81/247 [00:22<00:50,  3.32it/s]Loading train:  33%|███▎      | 82/247 [00:22<00:49,  3.33it/s]Loading train:  34%|███▎      | 83/247 [00:22<00:48,  3.36it/s]Loading train:  34%|███▍      | 84/247 [00:23<00:50,  3.24it/s]Loading train:  34%|███▍      | 85/247 [00:23<00:49,  3.29it/s]Loading train:  35%|███▍      | 86/247 [00:23<00:49,  3.26it/s]Loading train:  35%|███▌      | 87/247 [00:24<00:49,  3.26it/s]Loading train:  36%|███▌      | 88/247 [00:24<00:48,  3.29it/s]Loading train:  36%|███▌      | 89/247 [00:24<00:48,  3.25it/s]Loading train:  36%|███▋      | 90/247 [00:25<00:49,  3.18it/s]Loading train:  37%|███▋      | 91/247 [00:25<00:49,  3.16it/s]Loading train:  37%|███▋      | 92/247 [00:25<00:48,  3.22it/s]Loading train:  38%|███▊      | 93/247 [00:26<00:47,  3.26it/s]Loading train:  38%|███▊      | 94/247 [00:26<00:46,  3.30it/s]Loading train:  38%|███▊      | 95/247 [00:26<00:46,  3.29it/s]Loading train:  39%|███▉      | 96/247 [00:26<00:45,  3.30it/s]Loading train:  39%|███▉      | 97/247 [00:27<00:45,  3.29it/s]Loading train:  40%|███▉      | 98/247 [00:27<00:45,  3.30it/s]Loading train:  40%|████      | 99/247 [00:27<00:44,  3.31it/s]Loading train:  40%|████      | 100/247 [00:28<00:44,  3.27it/s]Loading train:  41%|████      | 101/247 [00:28<00:45,  3.24it/s]Loading train:  41%|████▏     | 102/247 [00:28<00:45,  3.16it/s]Loading train:  42%|████▏     | 103/247 [00:29<00:45,  3.16it/s]Loading train:  42%|████▏     | 104/247 [00:29<00:45,  3.14it/s]Loading train:  43%|████▎     | 105/247 [00:29<00:45,  3.13it/s]Loading train:  43%|████▎     | 106/247 [00:30<00:45,  3.09it/s]Loading train:  43%|████▎     | 107/247 [00:30<00:44,  3.11it/s]Loading train:  44%|████▎     | 108/247 [00:30<00:44,  3.15it/s]Loading train:  44%|████▍     | 109/247 [00:31<00:43,  3.16it/s]Loading train:  45%|████▍     | 110/247 [00:31<00:43,  3.16it/s]Loading train:  45%|████▍     | 111/247 [00:31<00:43,  3.16it/s]Loading train:  45%|████▌     | 112/247 [00:31<00:42,  3.14it/s]Loading train:  46%|████▌     | 113/247 [00:32<00:42,  3.13it/s]Loading train:  46%|████▌     | 114/247 [00:32<00:42,  3.15it/s]Loading train:  47%|████▋     | 115/247 [00:32<00:41,  3.19it/s]Loading train:  47%|████▋     | 116/247 [00:33<00:40,  3.22it/s]Loading train:  47%|████▋     | 117/247 [00:33<00:40,  3.24it/s]Loading train:  48%|████▊     | 118/247 [00:33<00:38,  3.31it/s]Loading train:  48%|████▊     | 119/247 [00:34<00:38,  3.34it/s]Loading train:  49%|████▊     | 120/247 [00:34<00:37,  3.38it/s]Loading train:  49%|████▉     | 121/247 [00:34<00:37,  3.39it/s]Loading train:  49%|████▉     | 122/247 [00:34<00:36,  3.40it/s]Loading train:  50%|████▉     | 123/247 [00:35<00:36,  3.43it/s]Loading train:  50%|█████     | 124/247 [00:35<00:35,  3.42it/s]Loading train:  51%|█████     | 125/247 [00:35<00:35,  3.44it/s]Loading train:  51%|█████     | 126/247 [00:36<00:35,  3.43it/s]Loading train:  51%|█████▏    | 127/247 [00:36<00:34,  3.43it/s]Loading train:  52%|█████▏    | 128/247 [00:36<00:34,  3.42it/s]Loading train:  52%|█████▏    | 129/247 [00:37<00:34,  3.42it/s]Loading train:  53%|█████▎    | 130/247 [00:37<00:34,  3.35it/s]Loading train:  53%|█████▎    | 131/247 [00:37<00:34,  3.36it/s]Loading train:  53%|█████▎    | 132/247 [00:37<00:34,  3.35it/s]Loading train:  54%|█████▍    | 133/247 [00:38<00:33,  3.37it/s]Loading train:  54%|█████▍    | 134/247 [00:38<00:33,  3.38it/s]Loading train:  55%|█████▍    | 135/247 [00:38<00:33,  3.39it/s]Loading train:  55%|█████▌    | 136/247 [00:39<00:31,  3.51it/s]Loading train:  55%|█████▌    | 137/247 [00:39<00:30,  3.63it/s]Loading train:  56%|█████▌    | 138/247 [00:39<00:29,  3.73it/s]Loading train:  56%|█████▋    | 139/247 [00:39<00:28,  3.75it/s]Loading train:  57%|█████▋    | 140/247 [00:40<00:28,  3.80it/s]Loading train:  57%|█████▋    | 141/247 [00:40<00:27,  3.82it/s]Loading train:  57%|█████▋    | 142/247 [00:40<00:27,  3.82it/s]Loading train:  58%|█████▊    | 143/247 [00:40<00:27,  3.81it/s]Loading train:  58%|█████▊    | 144/247 [00:41<00:26,  3.84it/s]Loading train:  59%|█████▊    | 145/247 [00:41<00:26,  3.85it/s]Loading train:  59%|█████▉    | 146/247 [00:41<00:26,  3.86it/s]Loading train:  60%|█████▉    | 147/247 [00:41<00:25,  3.87it/s]Loading train:  60%|█████▉    | 148/247 [00:42<00:25,  3.87it/s]Loading train:  60%|██████    | 149/247 [00:42<00:25,  3.85it/s]Loading train:  61%|██████    | 150/247 [00:42<00:25,  3.86it/s]Loading train:  61%|██████    | 151/247 [00:42<00:24,  3.87it/s]Loading train:  62%|██████▏   | 152/247 [00:43<00:24,  3.85it/s]Loading train:  62%|██████▏   | 153/247 [00:43<00:24,  3.82it/s]Loading train:  62%|██████▏   | 154/247 [00:43<00:24,  3.75it/s]Loading train:  63%|██████▎   | 155/247 [00:44<00:24,  3.69it/s]Loading train:  63%|██████▎   | 156/247 [00:44<00:24,  3.67it/s]Loading train:  64%|██████▎   | 157/247 [00:44<00:24,  3.60it/s]Loading train:  64%|██████▍   | 158/247 [00:44<00:24,  3.56it/s]Loading train:  64%|██████▍   | 159/247 [00:45<00:24,  3.54it/s]Loading train:  65%|██████▍   | 160/247 [00:45<00:24,  3.54it/s]Loading train:  65%|██████▌   | 161/247 [00:45<00:24,  3.54it/s]Loading train:  66%|██████▌   | 162/247 [00:45<00:23,  3.56it/s]Loading train:  66%|██████▌   | 163/247 [00:46<00:23,  3.60it/s]Loading train:  66%|██████▋   | 164/247 [00:46<00:22,  3.63it/s]Loading train:  67%|██████▋   | 165/247 [00:46<00:22,  3.62it/s]Loading train:  67%|██████▋   | 166/247 [00:47<00:23,  3.49it/s]Loading train:  68%|██████▊   | 167/247 [00:47<00:22,  3.50it/s]Loading train:  68%|██████▊   | 168/247 [00:47<00:22,  3.49it/s]Loading train:  68%|██████▊   | 169/247 [00:47<00:22,  3.52it/s]Loading train:  69%|██████▉   | 170/247 [00:48<00:21,  3.56it/s]Loading train:  69%|██████▉   | 171/247 [00:48<00:21,  3.60it/s]Loading train:  70%|██████▉   | 172/247 [00:48<00:21,  3.57it/s]Loading train:  70%|███████   | 173/247 [00:49<00:20,  3.63it/s]Loading train:  70%|███████   | 174/247 [00:49<00:20,  3.62it/s]Loading train:  71%|███████   | 175/247 [00:49<00:20,  3.45it/s]Loading train:  71%|███████▏  | 176/247 [00:49<00:20,  3.53it/s]Loading train:  72%|███████▏  | 177/247 [00:50<00:19,  3.53it/s]Loading train:  72%|███████▏  | 178/247 [00:50<00:19,  3.51it/s]Loading train:  72%|███████▏  | 179/247 [00:50<00:19,  3.57it/s]Loading train:  73%|███████▎  | 180/247 [00:51<00:18,  3.61it/s]Loading train:  73%|███████▎  | 181/247 [00:51<00:18,  3.64it/s]Loading train:  74%|███████▎  | 182/247 [00:51<00:17,  3.62it/s]Loading train:  74%|███████▍  | 183/247 [00:51<00:17,  3.67it/s]Loading train:  74%|███████▍  | 184/247 [00:52<00:17,  3.67it/s]Loading train:  75%|███████▍  | 185/247 [00:52<00:16,  3.67it/s]Loading train:  75%|███████▌  | 186/247 [00:52<00:16,  3.67it/s]Loading train:  76%|███████▌  | 187/247 [00:52<00:16,  3.65it/s]Loading train:  76%|███████▌  | 188/247 [00:53<00:16,  3.64it/s]Loading train:  77%|███████▋  | 189/247 [00:53<00:15,  3.65it/s]Loading train:  77%|███████▋  | 190/247 [00:53<00:15,  3.64it/s]Loading train:  77%|███████▋  | 191/247 [00:54<00:15,  3.65it/s]Loading train:  78%|███████▊  | 192/247 [00:54<00:15,  3.64it/s]Loading train:  78%|███████▊  | 193/247 [00:54<00:14,  3.64it/s]Loading train:  79%|███████▊  | 194/247 [00:54<00:14,  3.71it/s]Loading train:  79%|███████▉  | 195/247 [00:55<00:13,  3.75it/s]Loading train:  79%|███████▉  | 196/247 [00:55<00:13,  3.80it/s]Loading train:  80%|███████▉  | 197/247 [00:55<00:13,  3.82it/s]Loading train:  80%|████████  | 198/247 [00:55<00:12,  3.84it/s]Loading train:  81%|████████  | 199/247 [00:56<00:12,  3.87it/s]Loading train:  81%|████████  | 200/247 [00:56<00:12,  3.87it/s]Loading train:  81%|████████▏ | 201/247 [00:56<00:11,  3.88it/s]Loading train:  82%|████████▏ | 202/247 [00:56<00:11,  3.89it/s]Loading train:  82%|████████▏ | 203/247 [00:57<00:11,  3.89it/s]Loading train:  83%|████████▎ | 204/247 [00:57<00:11,  3.83it/s]Loading train:  83%|████████▎ | 205/247 [00:57<00:10,  3.84it/s]Loading train:  83%|████████▎ | 206/247 [00:57<00:10,  3.83it/s]Loading train:  84%|████████▍ | 207/247 [00:58<00:10,  3.88it/s]Loading train:  84%|████████▍ | 208/247 [00:58<00:10,  3.89it/s]Loading train:  85%|████████▍ | 209/247 [00:58<00:09,  3.88it/s]Loading train:  85%|████████▌ | 210/247 [00:58<00:09,  3.88it/s]Loading train:  85%|████████▌ | 211/247 [00:59<00:09,  3.83it/s]Loading train:  86%|████████▌ | 212/247 [00:59<00:09,  3.78it/s]Loading train:  86%|████████▌ | 213/247 [00:59<00:09,  3.74it/s]Loading train:  87%|████████▋ | 214/247 [01:00<00:08,  3.69it/s]Loading train:  87%|████████▋ | 215/247 [01:00<00:08,  3.68it/s]Loading train:  87%|████████▋ | 216/247 [01:00<00:08,  3.64it/s]Loading train:  88%|████████▊ | 217/247 [01:00<00:08,  3.66it/s]Loading train:  88%|████████▊ | 218/247 [01:01<00:07,  3.66it/s]Loading train:  89%|████████▊ | 219/247 [01:01<00:07,  3.66it/s]Loading train:  89%|████████▉ | 220/247 [01:01<00:07,  3.67it/s]Loading train:  89%|████████▉ | 221/247 [01:01<00:07,  3.63it/s]Loading train:  90%|████████▉ | 222/247 [01:02<00:06,  3.64it/s]Loading train:  90%|█████████ | 223/247 [01:02<00:06,  3.65it/s]Loading train:  91%|█████████ | 224/247 [01:02<00:06,  3.65it/s]Loading train:  91%|█████████ | 225/247 [01:03<00:06,  3.65it/s]Loading train:  91%|█████████▏| 226/247 [01:03<00:05,  3.65it/s]Loading train:  92%|█████████▏| 227/247 [01:03<00:05,  3.68it/s]Loading train:  92%|█████████▏| 228/247 [01:03<00:05,  3.70it/s]Loading train:  93%|█████████▎| 229/247 [01:04<00:04,  3.72it/s]Loading train:  93%|█████████▎| 230/247 [01:04<00:04,  3.56it/s]Loading train:  94%|█████████▎| 231/247 [01:04<00:04,  3.48it/s]Loading train:  94%|█████████▍| 232/247 [01:05<00:04,  3.41it/s]Loading train:  94%|█████████▍| 233/247 [01:05<00:04,  3.37it/s]Loading train:  95%|█████████▍| 234/247 [01:05<00:03,  3.34it/s]Loading train:  95%|█████████▌| 235/247 [01:05<00:03,  3.31it/s]Loading train:  96%|█████████▌| 236/247 [01:06<00:03,  3.30it/s]Loading train:  96%|█████████▌| 237/247 [01:06<00:03,  3.29it/s]Loading train:  96%|█████████▋| 238/247 [01:06<00:02,  3.28it/s]Loading train:  97%|█████████▋| 239/247 [01:07<00:02,  3.28it/s]Loading train:  97%|█████████▋| 240/247 [01:07<00:02,  3.27it/s]Loading train:  98%|█████████▊| 241/247 [01:07<00:01,  3.28it/s]Loading train:  98%|█████████▊| 242/247 [01:08<00:01,  3.24it/s]Loading train:  98%|█████████▊| 243/247 [01:08<00:01,  3.23it/s]Loading train:  99%|█████████▉| 244/247 [01:08<00:00,  3.22it/s]Loading train:  99%|█████████▉| 245/247 [01:09<00:00,  3.20it/s]Loading train: 100%|█████████▉| 246/247 [01:09<00:00,  3.19it/s]Loading train: 100%|██████████| 247/247 [01:09<00:00,  3.19it/s]Loading train: 100%|██████████| 247/247 [01:09<00:00,  3.54it/s]
concatenating: train:   0%|          | 0/247 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/247 [00:00<00:05, 46.89it/s]concatenating: train:   4%|▍         | 10/247 [00:00<00:04, 47.61it/s]concatenating: train:   6%|▋         | 16/247 [00:00<00:04, 48.60it/s]concatenating: train:   8%|▊         | 20/247 [00:00<00:05, 45.27it/s]concatenating: train:  11%|█         | 26/247 [00:00<00:04, 47.00it/s]concatenating: train:  13%|█▎        | 33/247 [00:00<00:04, 50.68it/s]concatenating: train:  16%|█▌        | 39/247 [00:00<00:04, 50.31it/s]concatenating: train:  18%|█▊        | 45/247 [00:00<00:03, 51.13it/s]concatenating: train:  20%|██        | 50/247 [00:00<00:03, 50.36it/s]concatenating: train:  22%|██▏       | 55/247 [00:01<00:03, 49.58it/s]concatenating: train:  24%|██▍       | 60/247 [00:01<00:03, 49.22it/s]concatenating: train:  26%|██▋       | 65/247 [00:01<00:03, 49.11it/s]concatenating: train:  28%|██▊       | 70/247 [00:01<00:03, 48.04it/s]concatenating: train:  30%|███       | 75/247 [00:01<00:03, 48.54it/s]concatenating: train:  32%|███▏      | 80/247 [00:01<00:03, 47.80it/s]concatenating: train:  34%|███▍      | 85/247 [00:01<00:03, 48.22it/s]concatenating: train:  36%|███▋      | 90/247 [00:01<00:03, 46.91it/s]concatenating: train:  38%|███▊      | 95/247 [00:01<00:03, 44.96it/s]concatenating: train:  40%|████      | 100/247 [00:02<00:03, 44.02it/s]concatenating: train:  43%|████▎     | 105/247 [00:02<00:03, 43.48it/s]concatenating: train:  45%|████▍     | 110/247 [00:02<00:03, 43.79it/s]concatenating: train:  47%|████▋     | 115/247 [00:02<00:03, 43.87it/s]concatenating: train:  49%|████▉     | 121/247 [00:02<00:02, 45.33it/s]concatenating: train:  51%|█████     | 126/247 [00:02<00:02, 45.47it/s]concatenating: train:  53%|█████▎    | 131/247 [00:02<00:02, 44.30it/s]concatenating: train:  55%|█████▌    | 136/247 [00:02<00:02, 44.11it/s]concatenating: train:  57%|█████▋    | 142/247 [00:03<00:02, 46.38it/s]concatenating: train:  60%|█████▉    | 147/247 [00:03<00:02, 46.58it/s]concatenating: train:  62%|██████▏   | 152/247 [00:03<00:02, 46.71it/s]concatenating: train:  64%|██████▎   | 157/247 [00:03<00:01, 46.49it/s]concatenating: train:  66%|██████▌   | 162/247 [00:03<00:01, 46.23it/s]concatenating: train:  68%|██████▊   | 168/247 [00:03<00:01, 48.23it/s]concatenating: train:  70%|███████   | 173/247 [00:03<00:01, 47.80it/s]concatenating: train:  72%|███████▏  | 178/247 [00:03<00:01, 47.03it/s]concatenating: train:  74%|███████▍  | 184/247 [00:03<00:01, 49.76it/s]concatenating: train:  77%|███████▋  | 190/247 [00:03<00:01, 48.92it/s]concatenating: train:  79%|███████▉  | 195/247 [00:04<00:01, 48.73it/s]concatenating: train:  81%|████████▏ | 201/247 [00:04<00:00, 49.32it/s]concatenating: train:  83%|████████▎ | 206/247 [00:04<00:00, 48.61it/s]concatenating: train:  86%|████████▌ | 212/247 [00:04<00:00, 48.94it/s]concatenating: train:  88%|████████▊ | 217/247 [00:04<00:00, 48.53it/s]concatenating: train:  90%|████████▉ | 222/247 [00:04<00:00, 48.46it/s]concatenating: train:  92%|█████████▏| 228/247 [00:04<00:00, 49.38it/s]concatenating: train:  95%|█████████▌| 235/247 [00:04<00:00, 53.14it/s]concatenating: train:  98%|█████████▊| 241/247 [00:04<00:00, 54.78it/s]concatenating: train: 100%|██████████| 247/247 [00:05<00:00, 50.69it/s]concatenating: train: 100%|██████████| 247/247 [00:05<00:00, 48.28it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:00<00:01,  2.86it/s]Loading test:  40%|████      | 2/5 [00:00<00:01,  2.95it/s]Loading test:  60%|██████    | 3/5 [00:00<00:00,  3.05it/s]Loading test:  80%|████████  | 4/5 [00:01<00:00,  3.20it/s]Loading test: 100%|██████████| 5/5 [00:01<00:00,  3.13it/s]Loading test: 100%|██████████| 5/5 [00:01<00:00,  3.17it/s]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 268.34it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 76, 108, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 76, 108, 30)  300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 76, 108, 30)  120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 76, 108, 30)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 76, 108, 30)  8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 76, 108, 30)  120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 76, 108, 30)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 38, 54, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 38, 54, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 38, 54, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 38, 54, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 38, 54, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 38, 54, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 38, 54, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 38, 54, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 38, 54, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 19, 27, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 19, 27, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 19, 27, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 19, 27, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 19, 27, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 19, 27, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 19, 27, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 19, 27, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 19, 27, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 19, 27, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 38, 54, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 38, 54, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 38, 54, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 38, 54, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 38, 54, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 38, 54, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 38, 54, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 38, 54, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 38, 54, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2020-01-22 05:13:00.989769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 05:13:00.989861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 05:13:00.989874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 05:13:00.989882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 05:13:00.990183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 38, 54, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 76, 108, 30)  25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 76, 108, 60)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 76, 108, 30)  16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 76, 108, 30)  120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 76, 108, 30)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 76, 108, 30)  8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 76, 108, 30)  120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 76, 108, 30)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 76, 108, 90)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 76, 108, 90)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 76, 108, 2)   182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 499,142
Non-trainable params: 1,200
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1
------------------------------------------------------------------
class_weights [0.97426102 0.02573898]
Train on 25213 samples, validate on 542 samples
Epoch 1/300
 - 67s - loss: 0.1348 - acc: 0.9861 - mDice: 0.7380 - val_loss: 0.0793 - val_acc: 0.9906 - val_mDice: 0.4481

Epoch 00001: val_mDice improved from -inf to 0.44807, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300
 - 63s - loss: 0.0757 - acc: 0.9919 - mDice: 0.8529 - val_loss: 0.0453 - val_acc: 0.9921 - val_mDice: 0.4712

Epoch 00002: val_mDice improved from 0.44807 to 0.47118, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 3/300
 - 63s - loss: 0.0662 - acc: 0.9928 - mDice: 0.8712 - val_loss: 0.0189 - val_acc: 0.9929 - val_mDice: 0.4908

Epoch 00003: val_mDice improved from 0.47118 to 0.49083, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 4/300
 - 63s - loss: 0.0610 - acc: 0.9933 - mDice: 0.8814 - val_loss: 0.0391 - val_acc: 0.9931 - val_mDice: 0.4967

Epoch 00004: val_mDice improved from 0.49083 to 0.49666, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 5/300
 - 63s - loss: 0.0574 - acc: 0.9936 - mDice: 0.8884 - val_loss: 0.0428 - val_acc: 0.9938 - val_mDice: 0.5015

Epoch 00005: val_mDice improved from 0.49666 to 0.50154, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 6/300
 - 63s - loss: 0.0555 - acc: 0.9939 - mDice: 0.8922 - val_loss: 0.0535 - val_acc: 0.9935 - val_mDice: 0.4993

Epoch 00006: val_mDice did not improve from 0.50154
Epoch 7/300
 - 63s - loss: 0.0533 - acc: 0.9941 - mDice: 0.8964 - val_loss: 0.0197 - val_acc: 0.9935 - val_mDice: 0.5052

Epoch 00007: val_mDice improved from 0.50154 to 0.50516, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 8/300
 - 63s - loss: 0.0517 - acc: 0.9942 - mDice: 0.8995 - val_loss: 0.0653 - val_acc: 0.9938 - val_mDice: 0.5011

Epoch 00008: val_mDice did not improve from 0.50516
Epoch 9/300
 - 63s - loss: 0.0503 - acc: 0.9944 - mDice: 0.9023 - val_loss: 0.0393 - val_acc: 0.9934 - val_mDice: 0.5038

Epoch 00009: val_mDice did not improve from 0.50516
Epoch 10/300
 - 63s - loss: 0.0491 - acc: 0.9945 - mDice: 0.9045 - val_loss: 0.0301 - val_acc: 0.9937 - val_mDice: 0.5041

Epoch 00010: val_mDice did not improve from 0.50516
Epoch 11/300
 - 63s - loss: 0.0479 - acc: 0.9946 - mDice: 0.9069 - val_loss: 0.0390 - val_acc: 0.9937 - val_mDice: 0.5004

Epoch 00011: val_mDice did not improve from 0.50516
Epoch 12/300
 - 63s - loss: 0.0468 - acc: 0.9947 - mDice: 0.9090 - val_loss: 0.0460 - val_acc: 0.9939 - val_mDice: 0.5022

Epoch 00012: val_mDice did not improve from 0.50516
Epoch 13/300
 - 63s - loss: 0.0461 - acc: 0.9948 - mDice: 0.9104 - val_loss: 0.0179 - val_acc: 0.9940 - val_mDice: 0.5043

Epoch 00013: val_mDice did not improve from 0.50516
Epoch 14/300
 - 63s - loss: 0.0455 - acc: 0.9949 - mDice: 0.9115 - val_loss: 0.0045 - val_acc: 0.9939 - val_mDice: 0.5043

Epoch 00014: val_mDice did not improve from 0.50516
Epoch 15/300
 - 63s - loss: 0.0445 - acc: 0.9949 - mDice: 0.9136 - val_loss: -7.7400e-03 - val_acc: 0.9937 - val_mDice: 0.4994

Epoch 00015: val_mDice did not improve from 0.50516
Epoch 16/300
 - 63s - loss: 0.0442 - acc: 0.9950 - mDice: 0.9140 - val_loss: -2.5294e-02 - val_acc: 0.9939 - val_mDice: 0.4980

Epoch 00016: val_mDice did not improve from 0.50516
Epoch 17/300
 - 63s - loss: 0.0441 - acc: 0.9951 - mDice: 0.9142 - val_loss: -4.7777e-02 - val_acc: 0.9937 - val_mDice: 0.5004

Epoch 00017: val_mDice did not improve from 0.50516
Epoch 18/300
 - 63s - loss: 0.0431 - acc: 0.9951 - mDice: 0.9163 - val_loss: -4.6960e-02 - val_acc: 0.9938 - val_mDice: 0.4993

Epoch 00018: val_mDice did not improve from 0.50516
Epoch 19/300
 - 63s - loss: 0.0425 - acc: 0.9952 - mDice: 0.9174 - val_loss: -5.5704e-02 - val_acc: 0.9938 - val_mDice: 0.5015

Epoch 00019: val_mDice did not improve from 0.50516
Epoch 20/300
 - 63s - loss: 0.0417 - acc: 0.9952 - mDice: 0.9191 - val_loss: -5.8891e-02 - val_acc: 0.9937 - val_mDice: 0.5009

Epoch 00020: val_mDice did not improve from 0.50516
Epoch 21/300
 - 63s - loss: 0.0415 - acc: 0.9953 - mDice: 0.9195 - val_loss: -5.9623e-02 - val_acc: 0.9939 - val_mDice: 0.4983

Epoch 00021: val_mDice did not improve from 0.50516
Epoch 22/300
 - 63s - loss: 0.0408 - acc: 0.9953 - mDice: 0.9208 - val_loss: -6.2305e-02 - val_acc: 0.9938 - val_mDice: 0.5011

Epoch 00022: val_mDice did not improve from 0.50516

Epoch 00022: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 23/300
 - 63s - loss: 0.0408 - acc: 0.9953 - mDice: 0.9208 - val_loss: -6.1681e-02 - val_acc: 0.9939 - val_mDice: 0.5015

Epoch 00023: val_mDice did not improve from 0.50516
Epoch 24/300
 - 63s - loss: 0.0403 - acc: 0.9954 - mDice: 0.9218 - val_loss: -8.2776e-02 - val_acc: 0.9938 - val_mDice: 0.5028

Epoch 00024: val_mDice did not improve from 0.50516
Epoch 25/300
 - 63s - loss: 0.0402 - acc: 0.9954 - mDice: 0.9219 - val_loss: -6.5988e-02 - val_acc: 0.9939 - val_mDice: 0.5042

Epoch 00025: val_mDice did not improve from 0.50516
Epoch 26/300
 - 63s - loss: 0.0399 - acc: 0.9954 - mDice: 0.9225 - val_loss: -6.4333e-02 - val_acc: 0.9939 - val_mDice: 0.5023

Epoch 00026: val_mDice did not improve from 0.50516
Epoch 27/300
 - 63s - loss: 0.0400 - acc: 0.9954 - mDice: 0.9223 - val_loss: -4.6224e-02 - val_acc: 0.9939 - val_mDice: 0.5038

Epoch 00027: val_mDice did not improve from 0.50516
Epoch 28/300
 - 63s - loss: 0.0396 - acc: 0.9954 - mDice: 0.9231 - val_loss: -4.7215e-02 - val_acc: 0.9938 - val_mDice: 0.5028

Epoch 00028: val_mDice did not improve from 0.50516
Epoch 29/300
 - 64s - loss: 0.0394 - acc: 0.9955 - mDice: 0.9234 - val_loss: -4.1247e-02 - val_acc: 0.9938 - val_mDice: 0.5053

Epoch 00029: val_mDice improved from 0.50516 to 0.50534, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 30/300
 - 63s - loss: 0.0389 - acc: 0.9955 - mDice: 0.9244 - val_loss: -6.4488e-02 - val_acc: 0.9937 - val_mDice: 0.5041

Epoch 00030: val_mDice did not improve from 0.50534
Epoch 31/300
 - 63s - loss: 0.0393 - acc: 0.9955 - mDice: 0.9238 - val_loss: -6.3230e-02 - val_acc: 0.9940 - val_mDice: 0.5050

Epoch 00031: val_mDice did not improve from 0.50534
Epoch 32/300
 - 63s - loss: 0.0389 - acc: 0.9955 - mDice: 0.9245 - val_loss: -6.4940e-02 - val_acc: 0.9938 - val_mDice: 0.5040

Epoch 00032: val_mDice did not improve from 0.50534
Epoch 33/300
 - 63s - loss: 0.0385 - acc: 0.9955 - mDice: 0.9253 - val_loss: -6.2556e-02 - val_acc: 0.9939 - val_mDice: 0.5032

Epoch 00033: val_mDice did not improve from 0.50534
Epoch 34/300
 - 63s - loss: 0.0385 - acc: 0.9955 - mDice: 0.9253 - val_loss: -6.4975e-02 - val_acc: 0.9939 - val_mDice: 0.5020

Epoch 00034: val_mDice did not improve from 0.50534
Epoch 35/300
 - 63s - loss: 0.0384 - acc: 0.9955 - mDice: 0.9255 - val_loss: -6.7110e-02 - val_acc: 0.9938 - val_mDice: 0.5049

Epoch 00035: val_mDice did not improve from 0.50534
Epoch 36/300
 - 64s - loss: 0.0383 - acc: 0.9956 - mDice: 0.9257 - val_loss: -7.6394e-02 - val_acc: 0.9939 - val_mDice: 0.5055

Epoch 00036: val_mDice improved from 0.50534 to 0.50553, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 37/300
 - 64s - loss: 0.0386 - acc: 0.9956 - mDice: 0.9249 - val_loss: -6.6610e-02 - val_acc: 0.9939 - val_mDice: 0.5041

Epoch 00037: val_mDice did not improve from 0.50553

Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 38/300
 - 64s - loss: 0.0379 - acc: 0.9956 - mDice: 0.9263 - val_loss: -6.7936e-02 - val_acc: 0.9938 - val_mDice: 0.5066

Epoch 00038: val_mDice improved from 0.50553 to 0.50661, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 39/300
 - 66s - loss: 0.0379 - acc: 0.9956 - mDice: 0.9263 - val_loss: -8.3275e-02 - val_acc: 0.9939 - val_mDice: 0.5052

Epoch 00039: val_mDice did not improve from 0.50661
Epoch 40/300
 - 64s - loss: 0.0378 - acc: 0.9956 - mDice: 0.9267 - val_loss: -6.7652e-02 - val_acc: 0.9940 - val_mDice: 0.5061

Epoch 00040: val_mDice did not improve from 0.50661
Epoch 41/300
 - 63s - loss: 0.0375 - acc: 0.9956 - mDice: 0.9271 - val_loss: -8.5331e-02 - val_acc: 0.9938 - val_mDice: 0.5033

Epoch 00041: val_mDice did not improve from 0.50661
Epoch 42/300
 - 63s - loss: 0.0376 - acc: 0.9956 - mDice: 0.9269 - val_loss: -6.8284e-02 - val_acc: 0.9939 - val_mDice: 0.5065

Epoch 00042: val_mDice did not improve from 0.50661
Epoch 43/300
 - 63s - loss: 0.0374 - acc: 0.9956 - mDice: 0.9273 - val_loss: -8.5188e-02 - val_acc: 0.9938 - val_mDice: 0.5035

Epoch 00043: val_mDice did not improve from 0.50661
Epoch 44/300
 - 63s - loss: 0.0374 - acc: 0.9956 - mDice: 0.9274 - val_loss: -8.6062e-02 - val_acc: 0.9938 - val_mDice: 0.5051

Epoch 00044: val_mDice did not improve from 0.50661
Epoch 45/300
 - 63s - loss: 0.0373 - acc: 0.9956 - mDice: 0.9276 - val_loss: -8.6653e-02 - val_acc: 0.9939 - val_mDice: 0.5077

Epoch 00045: val_mDice improved from 0.50661 to 0.50766, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 46/300
 - 63s - loss: 0.0372 - acc: 0.9956 - mDice: 0.9277 - val_loss: -8.5352e-02 - val_acc: 0.9939 - val_mDice: 0.5061

Epoch 00046: val_mDice did not improve from 0.50766
Epoch 47/300
 - 63s - loss: 0.0372 - acc: 0.9957 - mDice: 0.9278 - val_loss: -8.5523e-02 - val_acc: 0.9938 - val_mDice: 0.5049

Epoch 00047: val_mDice did not improve from 0.50766
Epoch 48/300
 - 64s - loss: 0.0374 - acc: 0.9957 - mDice: 0.9274 - val_loss: -8.5832e-02 - val_acc: 0.9938 - val_mDice: 0.5064

Epoch 00048: val_mDice did not improve from 0.50766
Epoch 49/300
 - 64s - loss: 0.0373 - acc: 0.9957 - mDice: 0.9275 - val_loss: -8.4081e-02 - val_acc: 0.9938 - val_mDice: 0.5025

Epoch 00049: val_mDice did not improve from 0.50766
Epoch 50/300
 - 63s - loss: 0.0375 - acc: 0.9957 - mDice: 0.9272 - val_loss: -8.4905e-02 - val_acc: 0.9938 - val_mDice: 0.5036

Epoch 00050: val_mDice did not improve from 0.50766
Epoch 51/300
 - 63s - loss: 0.0370 - acc: 0.9957 - mDice: 0.9281 - val_loss: -8.5588e-02 - val_acc: 0.9939 - val_mDice: 0.5048

Epoch 00051: val_mDice did not improve from 0.50766
Epoch 52/300
 - 63s - loss: 0.0367 - acc: 0.9957 - mDice: 0.9288 - val_loss: -6.8713e-02 - val_acc: 0.9938 - val_mDice: 0.5074

Epoch 00052: val_mDice did not improve from 0.50766
Epoch 53/300
 - 63s - loss: 0.0368 - acc: 0.9957 - mDice: 0.9285 - val_loss: -8.7125e-02 - val_acc: 0.9939 - val_mDice: 0.5069

Epoch 00053: val_mDice did not improve from 0.50766
Epoch 54/300
 - 63s - loss: 0.0368 - acc: 0.9957 - mDice: 0.9285 - val_loss: -7.4271e-02 - val_acc: 0.9939 - val_mDice: 0.5054

Epoch 00054: val_mDice did not improve from 0.50766
Epoch 55/300
 - 63s - loss: 0.0366 - acc: 0.9957 - mDice: 0.9289 - val_loss: -6.9433e-02 - val_acc: 0.9938 - val_mDice: 0.5060

Epoch 00055: val_mDice did not improve from 0.50766
Epoch 56/300
 - 63s - loss: 0.0367 - acc: 0.9957 - mDice: 0.9288 - val_loss: -7.5395e-02 - val_acc: 0.9939 - val_mDice: 0.5082

Epoch 00056: val_mDice improved from 0.50766 to 0.50819, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 57/300
 - 64s - loss: 0.0370 - acc: 0.9957 - mDice: 0.9280 - val_loss: -7.3510e-02 - val_acc: 0.9939 - val_mDice: 0.5074

Epoch 00057: val_mDice did not improve from 0.50819
Epoch 58/300
 - 64s - loss: 0.0363 - acc: 0.9957 - mDice: 0.9295 - val_loss: -7.6292e-02 - val_acc: 0.9938 - val_mDice: 0.5056

Epoch 00058: val_mDice did not improve from 0.50819
Epoch 59/300
 - 65s - loss: 0.0363 - acc: 0.9957 - mDice: 0.9295 - val_loss: -6.9717e-02 - val_acc: 0.9938 - val_mDice: 0.5032

Epoch 00059: val_mDice did not improve from 0.50819
Epoch 60/300
 - 65s - loss: 0.0365 - acc: 0.9957 - mDice: 0.9292 - val_loss: -7.2442e-02 - val_acc: 0.9938 - val_mDice: 0.5071

Epoch 00060: val_mDice did not improve from 0.50819

Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 61/300
 - 67s - loss: 0.0362 - acc: 0.9957 - mDice: 0.9296 - val_loss: -7.1445e-02 - val_acc: 0.9940 - val_mDice: 0.5052

Epoch 00061: val_mDice did not improve from 0.50819
Epoch 62/300
 - 67s - loss: 0.0362 - acc: 0.9957 - mDice: 0.9297 - val_loss: -6.8293e-02 - val_acc: 0.9938 - val_mDice: 0.5057

Epoch 00062: val_mDice did not improve from 0.50819
Epoch 63/300
 - 65s - loss: 0.0363 - acc: 0.9957 - mDice: 0.9295 - val_loss: -6.8174e-02 - val_acc: 0.9938 - val_mDice: 0.5062

Epoch 00063: val_mDice did not improve from 0.50819
Epoch 64/300
 - 65s - loss: 0.0363 - acc: 0.9958 - mDice: 0.9296 - val_loss: -6.7742e-02 - val_acc: 0.9939 - val_mDice: 0.5049

Epoch 00064: val_mDice did not improve from 0.50819
Epoch 65/300
 - 65s - loss: 0.0361 - acc: 0.9957 - mDice: 0.9300 - val_loss: -6.8560e-02 - val_acc: 0.9939 - val_mDice: 0.5070

Epoch 00065: val_mDice did not improve from 0.50819
Epoch 66/300
 - 63s - loss: 0.0361 - acc: 0.9958 - mDice: 0.9299 - val_loss: -6.8413e-02 - val_acc: 0.9939 - val_mDice: 0.5064

Epoch 00066: val_mDice did not improve from 0.50819
Epoch 67/300
 - 64s - loss: 0.0360 - acc: 0.9957 - mDice: 0.9300 - val_loss: -6.8924e-02 - val_acc: 0.9939 - val_mDice: 0.5074

Epoch 00067: val_mDice did not improve from 0.50819
Epoch 68/300
 - 64s - loss: 0.0364 - acc: 0.9957 - mDice: 0.9294 - val_loss: -7.1962e-02 - val_acc: 0.9938 - val_mDice: 0.5070

Epoch 00068: val_mDice did not improve from 0.50819
Epoch 69/300
 - 63s - loss: 0.0361 - acc: 0.9958 - mDice: 0.9300 - val_loss: -7.1762e-02 - val_acc: 0.9939 - val_mDice: 0.5072

Epoch 00069: val_mDice did not improve from 0.50819
Epoch 70/300
 - 63s - loss: 0.0362 - acc: 0.9958 - mDice: 0.9297 - val_loss: -6.7630e-02 - val_acc: 0.9939 - val_mDice: 0.5048

Epoch 00070: val_mDice did not improve from 0.50819
Epoch 71/300
 - 63s - loss: 0.0358 - acc: 0.9958 - mDice: 0.9305 - val_loss: -6.8269e-02 - val_acc: 0.9938 - val_mDice: 0.5051

Epoch 00071: val_mDice did not improve from 0.50819
Epoch 72/300
 - 64s - loss: 0.0360 - acc: 0.9957 - mDice: 0.9301 - val_loss: -6.8775e-02 - val_acc: 0.9938 - val_mDice: 0.5071

Epoch 00072: val_mDice did not improve from 0.50819
Epoch 73/300
 - 65s - loss: 0.0360 - acc: 0.9958 - mDice: 0.9300 - val_loss: -6.8627e-02 - val_acc: 0.9939 - val_mDice: 0.5070

Epoch 00073: val_mDice did not improve from 0.50819
Epoch 74/300
 - 66s - loss: 0.0358 - acc: 0.9958 - mDice: 0.9304 - val_loss: -6.9260e-02 - val_acc: 0.9939 - val_mDice: 0.5079

Epoch 00074: val_mDice did not improve from 0.50819
Epoch 75/300
 - 65s - loss: 0.0358 - acc: 0.9958 - mDice: 0.9305 - val_loss: -6.9951e-02 - val_acc: 0.9938 - val_mDice: 0.5038

Epoch 00075: val_mDice did not improve from 0.50819

Epoch 00075: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.
Epoch 76/300
 - 63s - loss: 0.0357 - acc: 0.9958 - mDice: 0.9306 - val_loss: -6.7646e-02 - val_acc: 0.9938 - val_mDice: 0.5037

Epoch 00076: val_mDice did not improve from 0.50819
Epoch 77/300
 - 64s - loss: 0.0357 - acc: 0.9958 - mDice: 0.9307 - val_loss: -6.6392e-02 - val_acc: 0.9938 - val_mDice: 0.5012

Epoch 00077: val_mDice did not improve from 0.50819
Epoch 78/300
 - 64s - loss: 0.0356 - acc: 0.9958 - mDice: 0.9309 - val_loss: -6.6971e-02 - val_acc: 0.9938 - val_mDice: 0.5021

Epoch 00078: val_mDice did not improve from 0.50819
Epoch 79/300
 - 63s - loss: 0.0357 - acc: 0.9958 - mDice: 0.9307 - val_loss: -6.8165e-02 - val_acc: 0.9938 - val_mDice: 0.5060

Epoch 00079: val_mDice did not improve from 0.50819
Epoch 80/300
 - 63s - loss: 0.0356 - acc: 0.9958 - mDice: 0.9308 - val_loss: -6.8200e-02 - val_acc: 0.9938 - val_mDice: 0.5061

Epoch 00080: val_mDice did not improve from 0.50819
Epoch 81/300
 - 63s - loss: 0.0361 - acc: 0.9958 - mDice: 0.9298 - val_loss: -6.8546e-02 - val_acc: 0.9938 - val_mDice: 0.5057

Epoch 00081: val_mDice did not improve from 0.50819
Epoch 82/300
 - 63s - loss: 0.0358 - acc: 0.9958 - mDice: 0.9306 - val_loss: -6.7494e-02 - val_acc: 0.9938 - val_mDice: 0.5050

Epoch 00082: val_mDice did not improve from 0.50819
Epoch 83/300
 - 63s - loss: 0.0360 - acc: 0.9958 - mDice: 0.9301 - val_loss: -6.8006e-02 - val_acc: 0.9938 - val_mDice: 0.5050

Epoch 00083: val_mDice did not improve from 0.50819
Epoch 84/300
 - 63s - loss: 0.0359 - acc: 0.9958 - mDice: 0.9303 - val_loss: -6.7323e-02 - val_acc: 0.9937 - val_mDice: 0.5042

Epoch 00084: val_mDice did not improve from 0.50819
Epoch 85/300
 - 64s - loss: 0.0359 - acc: 0.9958 - mDice: 0.9303 - val_loss: -6.7862e-02 - val_acc: 0.9938 - val_mDice: 0.5047

Epoch 00085: val_mDice did not improve from 0.50819
Epoch 86/300
 - 64s - loss: 0.0355 - acc: 0.9958 - mDice: 0.9311 - val_loss: -6.7713e-02 - val_acc: 0.9938 - val_mDice: 0.5052

Epoch 00086: val_mDice did not improve from 0.50819
Epoch 87/300
 - 63s - loss: 0.0354 - acc: 0.9958 - mDice: 0.9312 - val_loss: -6.7356e-02 - val_acc: 0.9939 - val_mDice: 0.5057

Epoch 00087: val_mDice did not improve from 0.50819
Epoch 88/300
 - 63s - loss: 0.0357 - acc: 0.9958 - mDice: 0.9306 - val_loss: -6.7569e-02 - val_acc: 0.9938 - val_mDice: 0.5046

Epoch 00088: val_mDice did not improve from 0.50819
Epoch 89/300
 - 63s - loss: 0.0356 - acc: 0.9958 - mDice: 0.9309 - val_loss: -6.8071e-02 - val_acc: 0.9939 - val_mDice: 0.5057

Epoch 00089: val_mDice did not improve from 0.50819
Epoch 90/300
 - 63s - loss: 0.0357 - acc: 0.9958 - mDice: 0.9307 - val_loss: -6.8237e-02 - val_acc: 0.9939 - val_mDice: 0.5060

Epoch 00090: val_mDice did not improve from 0.50819

Epoch 00090: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.
Epoch 91/300
 - 63s - loss: 0.0358 - acc: 0.9958 - mDice: 0.9306 - val_loss: -6.7909e-02 - val_acc: 0.9938 - val_mDice: 0.5056

Epoch 00091: val_mDice did not improve from 0.50819
Epoch 92/300
 - 64s - loss: 0.0353 - acc: 0.9958 - mDice: 0.9314 - val_loss: -6.7974e-02 - val_acc: 0.9938 - val_mDice: 0.5060

Epoch 00092: val_mDice did not improve from 0.50819
Epoch 93/300
 - 63s - loss: 0.0354 - acc: 0.9958 - mDice: 0.9313 - val_loss: -6.8447e-02 - val_acc: 0.9939 - val_mDice: 0.5071

Epoch 00093: val_mDice did not improve from 0.50819
Epoch 94/300
 - 64s - loss: 0.0357 - acc: 0.9958 - mDice: 0.9308 - val_loss: -6.7818e-02 - val_acc: 0.9938 - val_mDice: 0.5054

Epoch 00094: val_mDice did not improve from 0.50819
Epoch 95/300
 - 65s - loss: 0.0357 - acc: 0.9958 - mDice: 0.9307 - val_loss: -6.8043e-02 - val_acc: 0.9938 - val_mDice: 0.5057

Epoch 00095: val_mDice did not improve from 0.50819
Epoch 96/300
 - 66s - loss: 0.0355 - acc: 0.9958 - mDice: 0.9312 - val_loss: -6.8255e-02 - val_acc: 0.9938 - val_mDice: 0.5066

Epoch 00096: val_mDice did not improve from 0.50819
Restoring model weights from the end of the best epoch
Epoch 00096: early stopping
{'val_loss': [0.07932874615341975, 0.04525587080919435, 0.018875902346799294, 0.03908040998606664, 0.04279778787790629, 0.053465563927629335, 0.019660766384258482, 0.0652958753199155, 0.039278306369411986, 0.030103488134502046, 0.03904438587902217, 0.04598816313756788, 0.017890910628317026, 0.004515634444147019, -0.007739959937619987, -0.025294292676932697, -0.04777702702917296, -0.0469603796121819, -0.055703578097573946, -0.05889090802176852, -0.05962340924132794, -0.0623046494882925, -0.061681220314819436, -0.08277608861122623, -0.06598817161198471, -0.06433261691754155, -0.04622402237350211, -0.04721524143021045, -0.04124717865262964, -0.06448761849385785, -0.06323045577400285, -0.0649404771543517, -0.06255556774975189, -0.06497545378124582, -0.06711001020625948, -0.07639441182802524, -0.06660972832533706, -0.06793579915572796, -0.08327504514108285, -0.0676523530890141, -0.08533058430985771, -0.06828375721549636, -0.08518835630034169, -0.08606229208382293, -0.08665340424023871, -0.08535184032142822, -0.0855233372907357, -0.0858320092088182, -0.08408091670912571, -0.08490502020529715, -0.0855880927335732, -0.06871276824456739, -0.08712492165640272, -0.07427095580584889, -0.06943319168807835, -0.07539542024865802, -0.07351029097923933, -0.07629195259835887, -0.06971726857978039, -0.0724424244183016, -0.07144537260171673, -0.06829287840321495, -0.06817431890876531, -0.06774225520361833, -0.06855991628447142, -0.0684125722554337, -0.06892395660116224, -0.071961981701455, -0.0717618892113661, -0.06763038033708875, -0.06826862841294701, -0.06877500616096482, -0.06862679679235409, -0.06925986932652463, -0.06995111151265042, -0.06764570227738236, -0.06639180667835848, -0.06697119076863427, -0.06816458366673811, -0.06819998890821344, -0.06854569914706078, -0.06749385243412313, -0.06800589014903206, -0.06732339194779906, -0.0678619541420268, -0.06771266622626913, -0.06735553075576621, -0.06756926591875809, -0.06807122143102308, -0.06823695011134517, -0.06790894214957402, -0.06797430738531796, -0.06844727354076076, -0.06781760078074747, -0.06804334856157374, -0.06825505337262065], 'val_acc': [0.9905669854135971, 0.9920952825968555, 0.9929409152467312, 0.9931443397409362, 0.9937986887689007, 0.9934972542238412, 0.9935428869680285, 0.9937721658016923, 0.9934032966290013, 0.9936764079706255, 0.9936723610131943, 0.9939277152293723, 0.9940358360755047, 0.9938886005060259, 0.9937224871997904, 0.993909506120365, 0.9937009107582684, 0.9937526084400191, 0.9938312832719726, 0.9936982120535031, 0.9939481688601505, 0.9938204906523448, 0.9939281639137831, 0.9937860981124793, 0.9938755611651938, 0.9938942222577619, 0.9938996141687091, 0.9938303837037175, 0.993827237414258, 0.9936847229285434, 0.9939870559421412, 0.9937863290529849, 0.9938676992905536, 0.993875789906266, 0.9937894720432943, 0.9938667997222985, 0.9939216436935087, 0.9938456664666039, 0.9938557827604653, 0.9939753648539751, 0.9938211658783944, 0.993874893637161, 0.9938034098526648, 0.9938413973664005, 0.9939454723548186, 0.9938605027445128, 0.9937993584963668, 0.9937732886124361, 0.9937573284240666, 0.9938079076939403, 0.9938602773025906, 0.9938306069462062, 0.9939382791079279, 0.9938939968158398, 0.9938488149554967, 0.9938524121288004, 0.9939142261044126, 0.9938315076141779, 0.9938360021563034, 0.9938389241035575, 0.9939803124793781, 0.9938463427923703, 0.9938031822113094, 0.9938760142484714, 0.9938841070636172, 0.9938888270476648, 0.9939418751815149, 0.9938458930082427, 0.9938908472272303, 0.9938631992498447, 0.9938474645033973, 0.9938047581053308, 0.9939160252409228, 0.9938789394948755, 0.9938382499772245, 0.9938117270100161, 0.9937838546904251, 0.9938445447555767, 0.9938398236718125, 0.9937705888079541, 0.993790147269344, 0.9937645205712407, 0.9937791336066608, 0.9937420445614635, 0.9938195932835231, 0.9938249873939036, 0.99385556171741, 0.9938168956784744, 0.9938809574750077, 0.9938519568460894, 0.993822963915188, 0.9938375758508915, 0.9939020863318355, 0.9937946429111861, 0.9938144224156313, 0.993840051313168], 'val_mDice': [0.4480721855581639, 0.4711796368106252, 0.490831983485345, 0.4966619753749608, 0.5015387900201157, 0.4992704538841529, 0.5051643712054319, 0.5011243364907718, 0.5037694850531012, 0.5041124361027651, 0.5003539316108746, 0.5021974584603222, 0.5042509877593755, 0.5043279206400868, 0.49942097884372594, 0.4980358558183285, 0.5003725621863045, 0.49931444351413595, 0.5014551227493039, 0.5008906279982676, 0.4982711675324359, 0.501052075756432, 0.5015151088583073, 0.5027714729089139, 0.5042183008801013, 0.5022756582146641, 0.5038086613605823, 0.5027554093471752, 0.5053362327308233, 0.5040595056386011, 0.5049598827353263, 0.5040399598239532, 0.5031666440057578, 0.501957243219073, 0.5048794258124714, 0.5055279886810542, 0.504079659046722, 0.506612561175744, 0.5051632088048872, 0.5061038663246535, 0.5033421359708828, 0.5064980696708073, 0.5034784486043057, 0.5051239656896169, 0.5076559597275794, 0.5060567522620802, 0.5048656480118797, 0.5064234236509597, 0.5024974562255219, 0.5036050114244552, 0.5047863303955191, 0.5074253782794924, 0.5069230793806899, 0.5054445677467818, 0.5059541464951646, 0.5081904533824357, 0.5074055608348212, 0.505589621203412, 0.5031750430920028, 0.5071295522895686, 0.5051952669211419, 0.5057413887053838, 0.5061665974639878, 0.5048926746493336, 0.5070208535423139, 0.5064278456117834, 0.5073685361230506, 0.5069678252033641, 0.507233369394422, 0.5047527604437402, 0.5050636168115693, 0.507110444712023, 0.5070406236991671, 0.5079085030678894, 0.5037752277736735, 0.503689963714223, 0.5011743307113647, 0.5020809772929582, 0.5059948448105492, 0.5061448522159534, 0.5056773681482266, 0.5049560045844075, 0.5049693529456304, 0.504170038163442, 0.5046726213390097, 0.5052338875967638, 0.5057369496989514, 0.5046409126577343, 0.5057122236688198, 0.5060346270619284, 0.5056140041219352, 0.5059737148540047, 0.507146514869704, 0.5054379966646103, 0.505701302382339, 0.5065605731687862], 'loss': [0.1347605563120713, 0.07566904227585056, 0.066245406298112, 0.061031289247666876, 0.057447134238656224, 0.05547239725414718, 0.05329350479044975, 0.051742753505763524, 0.050264710239448146, 0.04914678780670296, 0.0479249656578656, 0.04683589810422103, 0.04612016417102377, 0.04553549365080917, 0.04450205810347671, 0.044240381016222706, 0.044146079991298125, 0.04307430441105413, 0.042538480592261924, 0.041677320611778076, 0.041466751478487426, 0.040789607844187996, 0.04079243382136494, 0.04026457769004667, 0.04020717038174195, 0.039887234288456655, 0.04001234228183195, 0.039577706557270914, 0.03943744808351517, 0.03894191303654305, 0.03925290689175526, 0.03889887430811327, 0.03848793521113905, 0.038485722964051006, 0.03835822990782432, 0.03827860897641641, 0.0386466487739199, 0.03793595499522966, 0.037941028269404, 0.03777297304970021, 0.03754654655905522, 0.03763148906787612, 0.03744560123028931, 0.037408046392769796, 0.03727392581091402, 0.037215943408684726, 0.03719555039447997, 0.0373600470900597, 0.037309296544699916, 0.037470726523032266, 0.03703383523559351, 0.036656536062314206, 0.03683466256924861, 0.03682238022168011, 0.03662275079718587, 0.03669067260903472, 0.03704763805892719, 0.03629807692127661, 0.03629592432197715, 0.03648046598299982, 0.03624565690863408, 0.03620741910881115, 0.03633478263551362, 0.03627094286122544, 0.03607378264891559, 0.03610907288581666, 0.036042463237846954, 0.036377145448264536, 0.03605876131803926, 0.03619709554615674, 0.035796816925950874, 0.03600624067690307, 0.036043422574049064, 0.03583192963141156, 0.035824992485740094, 0.03574427102263905, 0.03570800463527196, 0.03559806605827065, 0.035700057439923605, 0.03563837679560681, 0.0361482867125843, 0.035752512424462035, 0.036018591168634775, 0.03588131705828492, 0.03590610686243254, 0.03551167426628269, 0.03544041483664094, 0.03573964535894528, 0.035621834310484636, 0.035690126050323775, 0.035765879251272975, 0.03534919723625465, 0.03538197683227987, 0.035657574121204925, 0.03567389258950487, 0.03545822899724937], 'acc': [0.9860824759564509, 0.9918891769349493, 0.992811600732079, 0.9933083284564566, 0.9936347289545526, 0.9938806558702133, 0.9940637929249571, 0.9942491681384855, 0.9943902323163072, 0.9945026372438175, 0.9946182597275932, 0.9947160381651056, 0.9947935494442127, 0.9948675829188919, 0.9949393120391864, 0.9950074968213948, 0.9950586502707964, 0.9951329148851629, 0.9951518663466249, 0.9952176611440419, 0.9952555731898123, 0.995292602362024, 0.9953293608984631, 0.9953681136692492, 0.9953797056414628, 0.9953904918268691, 0.9954179432019263, 0.9954238326907068, 0.9954520916917978, 0.9954888832893942, 0.995478005457089, 0.9954945897515137, 0.9955283038388192, 0.995518049863577, 0.9955413602323566, 0.9955636068947495, 0.9955777552757336, 0.9955775283818696, 0.9955728414539721, 0.9956034287406665, 0.9956034628112705, 0.9956123830921018, 0.9956119185857978, 0.9956406501569472, 0.9956377557631511, 0.9956481065223602, 0.995650696491101, 0.9956584953568642, 0.9956559152178189, 0.995660718409407, 0.9956686043169112, 0.995684406048898, 0.9956855317594342, 0.9956868073361843, 0.995699065971208, 0.9957144614894912, 0.9956945333893881, 0.9957110590035093, 0.9957225449908963, 0.9957187186105245, 0.9957349056722492, 0.995735998151417, 0.9957355537938612, 0.9957545003972125, 0.9957311468917196, 0.9957580566453531, 0.9957476437732483, 0.9957410669173592, 0.9957543219875298, 0.9957563317843792, 0.9957675707906193, 0.9957457444954622, 0.9957504413783496, 0.9957758145222078, 0.9957676000598497, 0.995767836773953, 0.9957784433794746, 0.9957817836532711, 0.995777637152955, 0.9957788634299286, 0.995786721016184, 0.9957799754550133, 0.9957869916945095, 0.9957948538811947, 0.9957944277622394, 0.995804493259335, 0.9957883685374541, 0.9957964630056599, 0.9957884606926225, 0.9957847008222686, 0.9957816285483354, 0.9958061901300262, 0.9958023725817234, 0.9957933312476482, 0.9957928774031833, 0.9958076538941457], 'mDice': [0.7379601706010069, 0.8528912096161178, 0.8712278811629479, 0.8813831157891038, 0.8883758526676182, 0.8921908673168065, 0.8964489439736325, 0.89945029733447, 0.9023283652192053, 0.9045053024141456, 0.9068888511925514, 0.9090108257872306, 0.9103998358169382, 0.9115297718716269, 0.9135570739761716, 0.9140436388001071, 0.9142034563904113, 0.9163073568665059, 0.9173689007957674, 0.91905339844417, 0.9194563372564567, 0.9207893142841849, 0.9207630924650306, 0.9218018979918067, 0.9219061320546609, 0.9225401228126594, 0.9222755552387831, 0.9231389054689244, 0.9234087185836891, 0.924377682496199, 0.9237578844698343, 0.9244577701271528, 0.9252646747255951, 0.9252729986290124, 0.9255176623129615, 0.9256652103309917, 0.9249225742345156, 0.9263394775665988, 0.926326408608167, 0.9266532646031947, 0.927101565356276, 0.9269274981702111, 0.9272980160149229, 0.9273619118560639, 0.9276299086272568, 0.9277400116968131, 0.9277803116186473, 0.9274471473420565, 0.9275476389575619, 0.9272241412810793, 0.9280943813559149, 0.9288380769188646, 0.9284836413632144, 0.9285082016163124, 0.9289022482397045, 0.9287575805364056, 0.9280454233387464, 0.929539872883417, 0.9295404296299951, 0.9291710676651597, 0.9296345444657587, 0.9297060144064385, 0.9294514178979147, 0.9295748698570971, 0.9299765253693015, 0.9298949421518302, 0.9300328747536716, 0.9293635645616448, 0.9299948964011914, 0.9297164269901359, 0.9305138326251975, 0.930100288431964, 0.930024298384525, 0.9304414232582952, 0.9304530185685392, 0.930615057321333, 0.9306801445272438, 0.9308996073021343, 0.9307015761854213, 0.9308205525409753, 0.9297992004278783, 0.9305935509309923, 0.9300563285986538, 0.9303316649018345, 0.9302760682068063, 0.9310639014003136, 0.9312111279698067, 0.930612340347867, 0.9308504783704848, 0.9307113053192251, 0.9305632879509644, 0.9313837023784294, 0.9313227551788055, 0.9307780881878007, 0.9307463411814312, 0.9311678181493438], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:02,  1.36it/s]predicting test subjects:  40%|████      | 2/5 [00:00<00:01,  1.72it/s]predicting test subjects:  60%|██████    | 3/5 [00:01<00:00,  2.14it/s]predicting test subjects:  80%|████████  | 4/5 [00:01<00:00,  2.69it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  3.00it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  3.22it/s]
predicting train subjects:   0%|          | 0/247 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/247 [00:00<00:48,  5.09it/s]predicting train subjects:   1%|          | 2/247 [00:00<00:46,  5.25it/s]predicting train subjects:   1%|          | 3/247 [00:00<00:48,  5.06it/s]predicting train subjects:   2%|▏         | 4/247 [00:00<00:45,  5.31it/s]predicting train subjects:   2%|▏         | 5/247 [00:00<00:43,  5.59it/s]predicting train subjects:   2%|▏         | 6/247 [00:01<00:41,  5.83it/s]predicting train subjects:   3%|▎         | 7/247 [00:01<00:40,  5.96it/s]predicting train subjects:   3%|▎         | 8/247 [00:01<00:39,  6.11it/s]predicting train subjects:   4%|▎         | 9/247 [00:01<00:38,  6.21it/s]predicting train subjects:   4%|▍         | 10/247 [00:01<00:37,  6.32it/s]predicting train subjects:   4%|▍         | 11/247 [00:01<00:36,  6.44it/s]predicting train subjects:   5%|▍         | 12/247 [00:01<00:37,  6.33it/s]predicting train subjects:   5%|▌         | 13/247 [00:02<00:36,  6.44it/s]predicting train subjects:   6%|▌         | 14/247 [00:02<00:36,  6.41it/s]predicting train subjects:   6%|▌         | 15/247 [00:02<00:35,  6.45it/s]predicting train subjects:   6%|▋         | 16/247 [00:02<00:35,  6.51it/s]predicting train subjects:   7%|▋         | 17/247 [00:02<00:35,  6.49it/s]predicting train subjects:   7%|▋         | 18/247 [00:02<00:35,  6.50it/s]predicting train subjects:   8%|▊         | 19/247 [00:03<00:35,  6.50it/s]predicting train subjects:   8%|▊         | 20/247 [00:03<00:34,  6.51it/s]predicting train subjects:   9%|▊         | 21/247 [00:03<00:34,  6.52it/s]predicting train subjects:   9%|▉         | 22/247 [00:03<00:34,  6.48it/s]predicting train subjects:   9%|▉         | 23/247 [00:03<00:33,  6.68it/s]predicting train subjects:  10%|▉         | 24/247 [00:03<00:32,  6.88it/s]predicting train subjects:  10%|█         | 25/247 [00:03<00:31,  7.04it/s]predicting train subjects:  11%|█         | 26/247 [00:04<00:30,  7.14it/s]predicting train subjects:  11%|█         | 27/247 [00:04<00:31,  7.06it/s]predicting train subjects:  11%|█▏        | 28/247 [00:04<00:30,  7.13it/s]predicting train subjects:  12%|█▏        | 29/247 [00:04<00:30,  7.25it/s]predicting train subjects:  12%|█▏        | 30/247 [00:04<00:29,  7.29it/s]predicting train subjects:  13%|█▎        | 31/247 [00:04<00:29,  7.37it/s]predicting train subjects:  13%|█▎        | 32/247 [00:04<00:28,  7.41it/s]predicting train subjects:  13%|█▎        | 33/247 [00:05<00:28,  7.41it/s]predicting train subjects:  14%|█▍        | 34/247 [00:05<00:28,  7.48it/s]predicting train subjects:  14%|█▍        | 35/247 [00:05<00:28,  7.46it/s]predicting train subjects:  15%|█▍        | 36/247 [00:05<00:28,  7.43it/s]predicting train subjects:  15%|█▍        | 37/247 [00:05<00:28,  7.41it/s]predicting train subjects:  15%|█▌        | 38/247 [00:05<00:28,  7.40it/s]predicting train subjects:  16%|█▌        | 39/247 [00:05<00:28,  7.40it/s]predicting train subjects:  16%|█▌        | 40/247 [00:05<00:28,  7.34it/s]predicting train subjects:  17%|█▋        | 41/247 [00:06<00:28,  7.20it/s]predicting train subjects:  17%|█▋        | 42/247 [00:06<00:28,  7.24it/s]predicting train subjects:  17%|█▋        | 43/247 [00:06<00:27,  7.34it/s]predicting train subjects:  18%|█▊        | 44/247 [00:06<00:27,  7.36it/s]predicting train subjects:  18%|█▊        | 45/247 [00:06<00:27,  7.28it/s]predicting train subjects:  19%|█▊        | 46/247 [00:06<00:27,  7.25it/s]predicting train subjects:  19%|█▉        | 47/247 [00:06<00:27,  7.18it/s]predicting train subjects:  19%|█▉        | 48/247 [00:07<00:28,  7.10it/s]predicting train subjects:  20%|█▉        | 49/247 [00:07<00:27,  7.08it/s]predicting train subjects:  20%|██        | 50/247 [00:07<00:27,  7.06it/s]predicting train subjects:  21%|██        | 51/247 [00:07<00:27,  7.06it/s]predicting train subjects:  21%|██        | 52/247 [00:07<00:27,  7.09it/s]predicting train subjects:  21%|██▏       | 53/247 [00:07<00:27,  7.06it/s]predicting train subjects:  22%|██▏       | 54/247 [00:07<00:27,  7.02it/s]predicting train subjects:  22%|██▏       | 55/247 [00:08<00:27,  7.11it/s]predicting train subjects:  23%|██▎       | 56/247 [00:08<00:27,  7.06it/s]predicting train subjects:  23%|██▎       | 57/247 [00:08<00:26,  7.07it/s]predicting train subjects:  23%|██▎       | 58/247 [00:08<00:26,  7.04it/s]predicting train subjects:  24%|██▍       | 59/247 [00:08<00:27,  6.86it/s]predicting train subjects:  24%|██▍       | 60/247 [00:08<00:27,  6.70it/s]predicting train subjects:  25%|██▍       | 61/247 [00:08<00:28,  6.62it/s]predicting train subjects:  25%|██▌       | 62/247 [00:09<00:28,  6.57it/s]predicting train subjects:  26%|██▌       | 63/247 [00:09<00:28,  6.55it/s]predicting train subjects:  26%|██▌       | 64/247 [00:09<00:28,  6.52it/s]predicting train subjects:  26%|██▋       | 65/247 [00:09<00:27,  6.54it/s]predicting train subjects:  27%|██▋       | 66/247 [00:09<00:27,  6.47it/s]predicting train subjects:  27%|██▋       | 67/247 [00:09<00:27,  6.48it/s]predicting train subjects:  28%|██▊       | 68/247 [00:10<00:28,  6.39it/s]predicting train subjects:  28%|██▊       | 69/247 [00:10<00:27,  6.36it/s]predicting train subjects:  28%|██▊       | 70/247 [00:10<00:27,  6.38it/s]predicting train subjects:  29%|██▊       | 71/247 [00:10<00:27,  6.43it/s]predicting train subjects:  29%|██▉       | 72/247 [00:10<00:27,  6.46it/s]predicting train subjects:  30%|██▉       | 73/247 [00:10<00:26,  6.47it/s]predicting train subjects:  30%|██▉       | 74/247 [00:10<00:26,  6.43it/s]predicting train subjects:  30%|███       | 75/247 [00:11<00:26,  6.43it/s]predicting train subjects:  31%|███       | 76/247 [00:11<00:26,  6.35it/s]predicting train subjects:  31%|███       | 77/247 [00:11<00:29,  5.75it/s]predicting train subjects:  32%|███▏      | 78/247 [00:11<00:32,  5.19it/s]predicting train subjects:  32%|███▏      | 79/247 [00:11<00:33,  5.03it/s]predicting train subjects:  32%|███▏      | 80/247 [00:12<00:30,  5.45it/s]predicting train subjects:  33%|███▎      | 81/247 [00:12<00:30,  5.42it/s]predicting train subjects:  33%|███▎      | 82/247 [00:12<00:29,  5.53it/s]predicting train subjects:  34%|███▎      | 83/247 [00:12<00:29,  5.65it/s]predicting train subjects:  34%|███▍      | 84/247 [00:12<00:28,  5.78it/s]predicting train subjects:  34%|███▍      | 85/247 [00:12<00:27,  5.91it/s]predicting train subjects:  35%|███▍      | 86/247 [00:13<00:26,  6.02it/s]predicting train subjects:  35%|███▌      | 87/247 [00:13<00:26,  6.09it/s]predicting train subjects:  36%|███▌      | 88/247 [00:13<00:25,  6.14it/s]predicting train subjects:  36%|███▌      | 89/247 [00:13<00:25,  6.16it/s]predicting train subjects:  36%|███▋      | 90/247 [00:13<00:25,  6.16it/s]predicting train subjects:  37%|███▋      | 91/247 [00:13<00:25,  6.18it/s]predicting train subjects:  37%|███▋      | 92/247 [00:14<00:24,  6.20it/s]predicting train subjects:  38%|███▊      | 93/247 [00:14<00:24,  6.22it/s]predicting train subjects:  38%|███▊      | 94/247 [00:14<00:26,  5.73it/s]predicting train subjects:  38%|███▊      | 95/247 [00:14<00:26,  5.83it/s]predicting train subjects:  39%|███▉      | 96/247 [00:14<00:26,  5.78it/s]predicting train subjects:  39%|███▉      | 97/247 [00:14<00:25,  5.86it/s]predicting train subjects:  40%|███▉      | 98/247 [00:15<00:25,  5.82it/s]predicting train subjects:  40%|████      | 99/247 [00:15<00:25,  5.91it/s]predicting train subjects:  40%|████      | 100/247 [00:15<00:25,  5.79it/s]predicting train subjects:  41%|████      | 101/247 [00:15<00:25,  5.68it/s]predicting train subjects:  41%|████▏     | 102/247 [00:15<00:26,  5.50it/s]predicting train subjects:  42%|████▏     | 103/247 [00:16<00:26,  5.47it/s]predicting train subjects:  42%|████▏     | 104/247 [00:16<00:26,  5.40it/s]predicting train subjects:  43%|████▎     | 105/247 [00:16<00:25,  5.48it/s]predicting train subjects:  43%|████▎     | 106/247 [00:16<00:25,  5.50it/s]predicting train subjects:  43%|████▎     | 107/247 [00:16<00:25,  5.56it/s]predicting train subjects:  44%|████▎     | 108/247 [00:16<00:24,  5.61it/s]predicting train subjects:  44%|████▍     | 109/247 [00:17<00:24,  5.66it/s]predicting train subjects:  45%|████▍     | 110/247 [00:17<00:24,  5.65it/s]predicting train subjects:  45%|████▍     | 111/247 [00:17<00:24,  5.63it/s]predicting train subjects:  45%|████▌     | 112/247 [00:17<00:24,  5.61it/s]predicting train subjects:  46%|████▌     | 113/247 [00:17<00:23,  5.67it/s]predicting train subjects:  46%|████▌     | 114/247 [00:18<00:23,  5.71it/s]predicting train subjects:  47%|████▋     | 115/247 [00:18<00:22,  5.78it/s]predicting train subjects:  47%|████▋     | 116/247 [00:18<00:22,  5.82it/s]predicting train subjects:  47%|████▋     | 117/247 [00:18<00:22,  5.82it/s]predicting train subjects:  48%|████▊     | 118/247 [00:18<00:22,  5.84it/s]predicting train subjects:  48%|████▊     | 119/247 [00:18<00:21,  5.88it/s]predicting train subjects:  49%|████▊     | 120/247 [00:19<00:21,  5.92it/s]predicting train subjects:  49%|████▉     | 121/247 [00:19<00:21,  5.90it/s]predicting train subjects:  49%|████▉     | 122/247 [00:19<00:21,  5.94it/s]predicting train subjects:  50%|████▉     | 123/247 [00:19<00:20,  5.93it/s]predicting train subjects:  50%|█████     | 124/247 [00:19<00:20,  5.96it/s]predicting train subjects:  51%|█████     | 125/247 [00:19<00:20,  5.95it/s]predicting train subjects:  51%|█████     | 126/247 [00:20<00:20,  5.91it/s]predicting train subjects:  51%|█████▏    | 127/247 [00:20<00:20,  5.88it/s]predicting train subjects:  52%|█████▏    | 128/247 [00:20<00:20,  5.90it/s]predicting train subjects:  52%|█████▏    | 129/247 [00:20<00:19,  5.94it/s]predicting train subjects:  53%|█████▎    | 130/247 [00:20<00:19,  5.93it/s]predicting train subjects:  53%|█████▎    | 131/247 [00:20<00:19,  5.90it/s]predicting train subjects:  53%|█████▎    | 132/247 [00:21<00:19,  5.92it/s]predicting train subjects:  54%|█████▍    | 133/247 [00:21<00:19,  5.92it/s]predicting train subjects:  54%|█████▍    | 134/247 [00:21<00:19,  5.93it/s]predicting train subjects:  55%|█████▍    | 135/247 [00:21<00:18,  5.92it/s]predicting train subjects:  55%|█████▌    | 136/247 [00:21<00:17,  6.23it/s]predicting train subjects:  55%|█████▌    | 137/247 [00:21<00:17,  6.47it/s]predicting train subjects:  56%|█████▌    | 138/247 [00:21<00:16,  6.67it/s]predicting train subjects:  56%|█████▋    | 139/247 [00:22<00:15,  6.78it/s]predicting train subjects:  57%|█████▋    | 140/247 [00:22<00:15,  6.88it/s]predicting train subjects:  57%|█████▋    | 141/247 [00:22<00:15,  6.90it/s]predicting train subjects:  57%|█████▋    | 142/247 [00:22<00:15,  6.95it/s]predicting train subjects:  58%|█████▊    | 143/247 [00:22<00:14,  6.99it/s]predicting train subjects:  58%|█████▊    | 144/247 [00:22<00:14,  7.00it/s]predicting train subjects:  59%|█████▊    | 145/247 [00:22<00:14,  7.01it/s]predicting train subjects:  59%|█████▉    | 146/247 [00:23<00:14,  7.02it/s]predicting train subjects:  60%|█████▉    | 147/247 [00:23<00:14,  7.05it/s]predicting train subjects:  60%|█████▉    | 148/247 [00:23<00:13,  7.08it/s]predicting train subjects:  60%|██████    | 149/247 [00:23<00:13,  7.11it/s]predicting train subjects:  61%|██████    | 150/247 [00:23<00:13,  7.13it/s]predicting train subjects:  61%|██████    | 151/247 [00:23<00:13,  7.12it/s]predicting train subjects:  62%|██████▏   | 152/247 [00:23<00:13,  7.11it/s]predicting train subjects:  62%|██████▏   | 153/247 [00:24<00:13,  7.10it/s]predicting train subjects:  62%|██████▏   | 154/247 [00:24<00:13,  6.72it/s]predicting train subjects:  63%|██████▎   | 155/247 [00:24<00:13,  6.59it/s]predicting train subjects:  63%|██████▎   | 156/247 [00:24<00:14,  6.49it/s]predicting train subjects:  64%|██████▎   | 157/247 [00:24<00:13,  6.56it/s]predicting train subjects:  64%|██████▍   | 158/247 [00:24<00:13,  6.55it/s]predicting train subjects:  64%|██████▍   | 159/247 [00:25<00:13,  6.49it/s]predicting train subjects:  65%|██████▍   | 160/247 [00:25<00:13,  6.35it/s]predicting train subjects:  65%|██████▌   | 161/247 [00:25<00:13,  6.35it/s]predicting train subjects:  66%|██████▌   | 162/247 [00:25<00:13,  6.42it/s]predicting train subjects:  66%|██████▌   | 163/247 [00:25<00:12,  6.48it/s]predicting train subjects:  66%|██████▋   | 164/247 [00:25<00:12,  6.56it/s]predicting train subjects:  67%|██████▋   | 165/247 [00:25<00:12,  6.61it/s]predicting train subjects:  67%|██████▋   | 166/247 [00:26<00:12,  6.61it/s]predicting train subjects:  68%|██████▊   | 167/247 [00:26<00:12,  6.64it/s]predicting train subjects:  68%|██████▊   | 168/247 [00:26<00:11,  6.68it/s]predicting train subjects:  68%|██████▊   | 169/247 [00:26<00:11,  6.75it/s]predicting train subjects:  69%|██████▉   | 170/247 [00:26<00:11,  6.77it/s]predicting train subjects:  69%|██████▉   | 171/247 [00:26<00:11,  6.78it/s]predicting train subjects:  70%|██████▉   | 172/247 [00:26<00:11,  6.81it/s]predicting train subjects:  70%|███████   | 173/247 [00:27<00:12,  5.93it/s]predicting train subjects:  70%|███████   | 174/247 [00:27<00:11,  6.20it/s]predicting train subjects:  71%|███████   | 175/247 [00:27<00:13,  5.53it/s]predicting train subjects:  71%|███████▏  | 176/247 [00:27<00:12,  5.82it/s]predicting train subjects:  72%|███████▏  | 177/247 [00:27<00:11,  5.94it/s]predicting train subjects:  72%|███████▏  | 178/247 [00:28<00:11,  6.07it/s]predicting train subjects:  72%|███████▏  | 179/247 [00:28<00:10,  6.27it/s]predicting train subjects:  73%|███████▎  | 180/247 [00:28<00:10,  6.45it/s]predicting train subjects:  73%|███████▎  | 181/247 [00:28<00:09,  6.63it/s]predicting train subjects:  74%|███████▎  | 182/247 [00:28<00:09,  6.77it/s]predicting train subjects:  74%|███████▍  | 183/247 [00:28<00:09,  6.79it/s]predicting train subjects:  74%|███████▍  | 184/247 [00:28<00:09,  6.81it/s]predicting train subjects:  75%|███████▍  | 185/247 [00:29<00:09,  6.78it/s]predicting train subjects:  75%|███████▌  | 186/247 [00:29<00:08,  6.78it/s]predicting train subjects:  76%|███████▌  | 187/247 [00:29<00:08,  6.79it/s]predicting train subjects:  76%|███████▌  | 188/247 [00:29<00:08,  6.72it/s]predicting train subjects:  77%|███████▋  | 189/247 [00:29<00:08,  6.70it/s]predicting train subjects:  77%|███████▋  | 190/247 [00:29<00:08,  6.72it/s]predicting train subjects:  77%|███████▋  | 191/247 [00:29<00:08,  6.72it/s]predicting train subjects:  78%|███████▊  | 192/247 [00:30<00:08,  6.28it/s]predicting train subjects:  78%|███████▊  | 193/247 [00:30<00:09,  5.99it/s]predicting train subjects:  79%|███████▊  | 194/247 [00:30<00:08,  6.34it/s]predicting train subjects:  79%|███████▉  | 195/247 [00:30<00:07,  6.60it/s]predicting train subjects:  79%|███████▉  | 196/247 [00:30<00:07,  6.83it/s]predicting train subjects:  80%|███████▉  | 197/247 [00:30<00:07,  6.84it/s]predicting train subjects:  80%|████████  | 198/247 [00:31<00:07,  6.98it/s]predicting train subjects:  81%|████████  | 199/247 [00:31<00:07,  6.74it/s]predicting train subjects:  81%|████████  | 200/247 [00:31<00:06,  6.91it/s]predicting train subjects:  81%|████████▏ | 201/247 [00:31<00:06,  7.02it/s]predicting train subjects:  82%|████████▏ | 202/247 [00:31<00:06,  6.98it/s]predicting train subjects:  82%|████████▏ | 203/247 [00:31<00:06,  7.09it/s]predicting train subjects:  83%|████████▎ | 204/247 [00:31<00:06,  7.14it/s]predicting train subjects:  83%|████████▎ | 205/247 [00:31<00:05,  7.19it/s]predicting train subjects:  83%|████████▎ | 206/247 [00:32<00:05,  7.23it/s]predicting train subjects:  84%|████████▍ | 207/247 [00:32<00:05,  7.27it/s]predicting train subjects:  84%|████████▍ | 208/247 [00:32<00:05,  7.29it/s]predicting train subjects:  85%|████████▍ | 209/247 [00:32<00:05,  7.32it/s]predicting train subjects:  85%|████████▌ | 210/247 [00:32<00:05,  7.34it/s]predicting train subjects:  85%|████████▌ | 211/247 [00:32<00:04,  7.35it/s]predicting train subjects:  86%|████████▌ | 212/247 [00:32<00:04,  7.21it/s]predicting train subjects:  86%|████████▌ | 213/247 [00:33<00:04,  7.12it/s]predicting train subjects:  87%|████████▋ | 214/247 [00:33<00:04,  7.04it/s]predicting train subjects:  87%|████████▋ | 215/247 [00:33<00:04,  6.98it/s]predicting train subjects:  87%|████████▋ | 216/247 [00:33<00:04,  6.88it/s]predicting train subjects:  88%|████████▊ | 217/247 [00:33<00:04,  6.87it/s]predicting train subjects:  88%|████████▊ | 218/247 [00:33<00:04,  6.81it/s]predicting train subjects:  89%|████████▊ | 219/247 [00:33<00:04,  6.82it/s]predicting train subjects:  89%|████████▉ | 220/247 [00:34<00:03,  6.80it/s]predicting train subjects:  89%|████████▉ | 221/247 [00:34<00:03,  6.78it/s]predicting train subjects:  90%|████████▉ | 222/247 [00:34<00:03,  6.82it/s]predicting train subjects:  90%|█████████ | 223/247 [00:34<00:03,  6.87it/s]predicting train subjects:  91%|█████████ | 224/247 [00:34<00:03,  6.93it/s]predicting train subjects:  91%|█████████ | 225/247 [00:34<00:03,  6.95it/s]predicting train subjects:  91%|█████████▏| 226/247 [00:34<00:03,  6.96it/s]predicting train subjects:  92%|█████████▏| 227/247 [00:35<00:02,  6.96it/s]predicting train subjects:  92%|█████████▏| 228/247 [00:35<00:02,  6.98it/s]predicting train subjects:  93%|█████████▎| 229/247 [00:35<00:02,  6.90it/s]predicting train subjects:  93%|█████████▎| 230/247 [00:35<00:02,  6.60it/s]predicting train subjects:  94%|█████████▎| 231/247 [00:35<00:02,  6.36it/s]predicting train subjects:  94%|█████████▍| 232/247 [00:35<00:02,  6.15it/s]predicting train subjects:  94%|█████████▍| 233/247 [00:36<00:02,  6.05it/s]predicting train subjects:  95%|█████████▍| 234/247 [00:36<00:02,  5.94it/s]predicting train subjects:  95%|█████████▌| 235/247 [00:36<00:02,  5.92it/s]predicting train subjects:  96%|█████████▌| 236/247 [00:36<00:01,  5.95it/s]predicting train subjects:  96%|█████████▌| 237/247 [00:36<00:01,  5.97it/s]predicting train subjects:  96%|█████████▋| 238/247 [00:36<00:01,  6.01it/s]predicting train subjects:  97%|█████████▋| 239/247 [00:37<00:01,  6.03it/s]predicting train subjects:  97%|█████████▋| 240/247 [00:37<00:01,  5.98it/s]predicting train subjects:  98%|█████████▊| 241/247 [00:37<00:01,  5.91it/s]predicting train subjects:  98%|█████████▊| 242/247 [00:37<00:00,  5.89it/s]predicting train subjects:  98%|█████████▊| 243/247 [00:37<00:00,  5.85it/s]predicting train subjects:  99%|█████████▉| 244/247 [00:37<00:00,  5.85it/s]predicting train subjects:  99%|█████████▉| 245/247 [00:38<00:00,  5.88it/s]predicting train subjects: 100%|█████████▉| 246/247 [00:38<00:00,  5.88it/s]predicting train subjects: 100%|██████████| 247/247 [00:38<00:00,  5.81it/s]predicting train subjects: 100%|██████████| 247/247 [00:38<00:00,  6.42it/s]
saving BB  test1-THALAMUS:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 5/5 [00:00<00:00, 71.69it/s]
saving BB  train1-THALAMUS:   0%|          | 0/247 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 7/247 [00:00<00:03, 69.16it/s]saving BB  train1-THALAMUS:   6%|▌         | 15/247 [00:00<00:03, 70.61it/s]saving BB  train1-THALAMUS:  10%|▉         | 24/247 [00:00<00:03, 73.39it/s]saving BB  train1-THALAMUS:  13%|█▎        | 33/247 [00:00<00:02, 76.91it/s]saving BB  train1-THALAMUS:  17%|█▋        | 42/247 [00:00<00:02, 79.42it/s]saving BB  train1-THALAMUS:  21%|██        | 51/247 [00:00<00:02, 81.57it/s]saving BB  train1-THALAMUS:  24%|██▍       | 60/247 [00:00<00:02, 83.07it/s]saving BB  train1-THALAMUS:  28%|██▊       | 68/247 [00:00<00:02, 78.61it/s]saving BB  train1-THALAMUS:  31%|███       | 76/247 [00:00<00:02, 76.71it/s]saving BB  train1-THALAMUS:  34%|███▍      | 84/247 [00:01<00:02, 77.02it/s]saving BB  train1-THALAMUS:  37%|███▋      | 92/247 [00:01<00:01, 77.77it/s]saving BB  train1-THALAMUS:  40%|████      | 100/247 [00:01<00:01, 77.98it/s]saving BB  train1-THALAMUS:  44%|████▎     | 108/247 [00:01<00:01, 75.78it/s]saving BB  train1-THALAMUS:  47%|████▋     | 116/247 [00:01<00:01, 74.61it/s]saving BB  train1-THALAMUS:  50%|█████     | 124/247 [00:01<00:01, 73.22it/s]saving BB  train1-THALAMUS:  53%|█████▎    | 132/247 [00:01<00:01, 72.24it/s]saving BB  train1-THALAMUS:  57%|█████▋    | 140/247 [00:01<00:01, 74.01it/s]saving BB  train1-THALAMUS:  60%|██████    | 149/247 [00:01<00:01, 76.77it/s]saving BB  train1-THALAMUS:  64%|██████▍   | 158/247 [00:02<00:01, 78.93it/s]saving BB  train1-THALAMUS:  68%|██████▊   | 167/247 [00:02<00:00, 81.78it/s]saving BB  train1-THALAMUS:  71%|███████▏  | 176/247 [00:02<00:00, 80.09it/s]saving BB  train1-THALAMUS:  75%|███████▍  | 185/247 [00:02<00:00, 76.51it/s]saving BB  train1-THALAMUS:  78%|███████▊  | 193/247 [00:02<00:00, 75.57it/s]saving BB  train1-THALAMUS:  81%|████████▏ | 201/247 [00:02<00:00, 75.02it/s]saving BB  train1-THALAMUS:  85%|████████▍ | 209/247 [00:02<00:00, 75.88it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 217/247 [00:02<00:00, 75.96it/s]saving BB  train1-THALAMUS:  91%|█████████ | 225/247 [00:02<00:00, 75.34it/s]saving BB  train1-THALAMUS:  94%|█████████▍| 233/247 [00:03<00:00, 74.48it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 241/247 [00:03<00:00, 74.21it/s]saving BB  train1-THALAMUS: 100%|██████████| 247/247 [00:03<00:00, 76.86it/s]
Loading train:   0%|          | 0/247 [00:00<?, ?it/s]Loading train:   0%|          | 1/247 [00:00<04:01,  1.02it/s]Loading train:   1%|          | 2/247 [00:01<03:49,  1.07it/s]Loading train:   1%|          | 3/247 [00:02<03:38,  1.11it/s]Loading train:   2%|▏         | 4/247 [00:03<03:42,  1.09it/s]Loading train:   2%|▏         | 5/247 [00:04<03:26,  1.17it/s]Loading train:   2%|▏         | 6/247 [00:04<03:11,  1.26it/s]Loading train:   3%|▎         | 7/247 [00:05<03:04,  1.30it/s]Loading train:   3%|▎         | 8/247 [00:06<02:55,  1.36it/s]Loading train:   4%|▎         | 9/247 [00:06<02:48,  1.41it/s]Loading train:   4%|▍         | 10/247 [00:07<02:41,  1.47it/s]Loading train:   4%|▍         | 11/247 [00:08<02:38,  1.49it/s]Loading train:   5%|▍         | 12/247 [00:08<02:36,  1.50it/s]Loading train:   5%|▌         | 13/247 [00:09<02:35,  1.51it/s]Loading train:   6%|▌         | 14/247 [00:10<02:35,  1.50it/s]Loading train:   6%|▌         | 15/247 [00:10<02:34,  1.50it/s]Loading train:   6%|▋         | 16/247 [00:11<02:33,  1.51it/s]Loading train:   7%|▋         | 17/247 [00:12<02:30,  1.53it/s]Loading train:   7%|▋         | 18/247 [00:12<02:27,  1.56it/s]Loading train:   8%|▊         | 19/247 [00:13<02:28,  1.54it/s]Loading train:   8%|▊         | 20/247 [00:14<02:27,  1.54it/s]Loading train:   9%|▊         | 21/247 [00:14<02:26,  1.54it/s]Loading train:   9%|▉         | 22/247 [00:15<02:25,  1.54it/s]Loading train:   9%|▉         | 23/247 [00:15<02:20,  1.59it/s]Loading train:  10%|▉         | 24/247 [00:16<02:15,  1.64it/s]Loading train:  10%|█         | 25/247 [00:17<02:11,  1.68it/s]Loading train:  11%|█         | 26/247 [00:17<02:07,  1.73it/s]Loading train:  11%|█         | 27/247 [00:18<02:04,  1.76it/s]Loading train:  11%|█▏        | 28/247 [00:18<02:02,  1.79it/s]Loading train:  12%|█▏        | 29/247 [00:19<02:00,  1.81it/s]Loading train:  12%|█▏        | 30/247 [00:19<01:59,  1.82it/s]Loading train:  13%|█▎        | 31/247 [00:20<01:58,  1.82it/s]Loading train:  13%|█▎        | 32/247 [00:20<01:57,  1.82it/s]Loading train:  13%|█▎        | 33/247 [00:21<02:00,  1.77it/s]Loading train:  14%|█▍        | 34/247 [00:22<02:02,  1.74it/s]Loading train:  14%|█▍        | 35/247 [00:22<02:01,  1.74it/s]Loading train:  15%|█▍        | 36/247 [00:23<02:01,  1.74it/s]Loading train:  15%|█▍        | 37/247 [00:23<02:00,  1.74it/s]Loading train:  15%|█▌        | 38/247 [00:24<02:00,  1.74it/s]Loading train:  16%|█▌        | 39/247 [00:24<01:57,  1.77it/s]Loading train:  16%|█▌        | 40/247 [00:25<01:56,  1.77it/s]Loading train:  17%|█▋        | 41/247 [00:26<01:58,  1.74it/s]Loading train:  17%|█▋        | 42/247 [00:26<01:58,  1.73it/s]Loading train:  17%|█▋        | 43/247 [00:27<01:57,  1.73it/s]Loading train:  18%|█▊        | 44/247 [00:27<01:56,  1.74it/s]Loading train:  18%|█▊        | 45/247 [00:28<01:56,  1.74it/s]Loading train:  19%|█▊        | 46/247 [00:28<01:55,  1.74it/s]Loading train:  19%|█▉        | 47/247 [00:29<01:55,  1.74it/s]Loading train:  19%|█▉        | 48/247 [00:30<01:54,  1.73it/s]Loading train:  20%|█▉        | 49/247 [00:30<01:53,  1.74it/s]Loading train:  20%|██        | 50/247 [00:31<01:53,  1.74it/s]Loading train:  21%|██        | 51/247 [00:31<01:54,  1.72it/s]Loading train:  21%|██        | 52/247 [00:32<01:53,  1.72it/s]Loading train:  21%|██▏       | 53/247 [00:33<01:52,  1.72it/s]Loading train:  22%|██▏       | 54/247 [00:33<01:51,  1.73it/s]Loading train:  22%|██▏       | 55/247 [00:34<01:50,  1.73it/s]Loading train:  23%|██▎       | 56/247 [00:34<01:49,  1.74it/s]Loading train:  23%|██▎       | 57/247 [00:35<01:48,  1.75it/s]Loading train:  23%|██▎       | 58/247 [00:35<01:47,  1.75it/s]Loading train:  24%|██▍       | 59/247 [00:36<01:52,  1.67it/s]Loading train:  24%|██▍       | 60/247 [00:37<01:55,  1.62it/s]Loading train:  25%|██▍       | 61/247 [00:37<01:55,  1.61it/s]Loading train:  25%|██▌       | 62/247 [00:38<01:55,  1.60it/s]Loading train:  26%|██▌       | 63/247 [00:39<01:57,  1.57it/s]Loading train:  26%|██▌       | 64/247 [00:39<02:00,  1.52it/s]Loading train:  26%|██▋       | 65/247 [00:40<01:59,  1.53it/s]Loading train:  27%|██▋       | 66/247 [00:41<01:57,  1.54it/s]Loading train:  27%|██▋       | 67/247 [00:41<01:55,  1.56it/s]Loading train:  28%|██▊       | 68/247 [00:42<01:53,  1.58it/s]Loading train:  28%|██▊       | 69/247 [00:42<01:51,  1.59it/s]Loading train:  28%|██▊       | 70/247 [00:43<01:51,  1.59it/s]Loading train:  29%|██▊       | 71/247 [00:44<01:52,  1.56it/s]Loading train:  29%|██▉       | 72/247 [00:44<01:55,  1.52it/s]Loading train:  30%|██▉       | 73/247 [00:45<01:54,  1.52it/s]Loading train:  30%|██▉       | 74/247 [00:46<01:53,  1.52it/s]Loading train:  30%|███       | 75/247 [00:46<01:51,  1.54it/s]Loading train:  31%|███       | 76/247 [00:47<01:51,  1.53it/s]Loading train:  31%|███       | 77/247 [00:48<02:07,  1.33it/s]Loading train:  32%|███▏      | 78/247 [00:49<02:20,  1.20it/s]Loading train:  32%|███▏      | 79/247 [00:50<02:27,  1.14it/s]Loading train:  32%|███▏      | 80/247 [00:51<02:22,  1.17it/s]Loading train:  33%|███▎      | 81/247 [00:52<02:21,  1.17it/s]Loading train:  33%|███▎      | 82/247 [00:52<02:11,  1.25it/s]Loading train:  34%|███▎      | 83/247 [00:53<02:04,  1.32it/s]Loading train:  34%|███▍      | 84/247 [00:54<02:00,  1.36it/s]Loading train:  34%|███▍      | 85/247 [00:54<01:57,  1.38it/s]Loading train:  35%|███▍      | 86/247 [00:55<01:56,  1.38it/s]Loading train:  35%|███▌      | 87/247 [00:56<01:57,  1.36it/s]Loading train:  36%|███▌      | 88/247 [00:57<01:53,  1.40it/s]Loading train:  36%|███▌      | 89/247 [00:57<01:51,  1.42it/s]Loading train:  36%|███▋      | 90/247 [00:58<01:48,  1.44it/s]Loading train:  37%|███▋      | 91/247 [00:59<01:47,  1.45it/s]Loading train:  37%|███▋      | 92/247 [00:59<01:46,  1.46it/s]Loading train:  38%|███▊      | 93/247 [01:00<01:45,  1.47it/s]Loading train:  38%|███▊      | 94/247 [01:01<01:44,  1.46it/s]Loading train:  38%|███▊      | 95/247 [01:01<01:46,  1.43it/s]Loading train:  39%|███▉      | 96/247 [01:02<01:47,  1.40it/s]Loading train:  39%|███▉      | 97/247 [01:03<01:45,  1.42it/s]Loading train:  40%|███▉      | 98/247 [01:04<01:45,  1.42it/s]Loading train:  40%|████      | 99/247 [01:04<01:42,  1.44it/s]Loading train:  40%|████      | 100/247 [01:05<01:40,  1.46it/s]Loading train:  41%|████      | 101/247 [01:06<01:39,  1.47it/s]Loading train:  41%|████▏     | 102/247 [01:06<01:37,  1.48it/s]Loading train:  42%|████▏     | 103/247 [01:07<01:38,  1.46it/s]Loading train:  42%|████▏     | 104/247 [01:08<01:38,  1.45it/s]Loading train:  43%|████▎     | 105/247 [01:08<01:37,  1.46it/s]Loading train:  43%|████▎     | 106/247 [01:09<01:35,  1.47it/s]Loading train:  43%|████▎     | 107/247 [01:10<01:35,  1.46it/s]Loading train:  44%|████▎     | 108/247 [01:10<01:34,  1.47it/s]Loading train:  44%|████▍     | 109/247 [01:11<01:35,  1.45it/s]Loading train:  45%|████▍     | 110/247 [01:12<01:34,  1.45it/s]Loading train:  45%|████▍     | 111/247 [01:12<01:34,  1.43it/s]Loading train:  45%|████▌     | 112/247 [01:13<01:33,  1.45it/s]Loading train:  46%|████▌     | 113/247 [01:14<01:31,  1.47it/s]Loading train:  46%|████▌     | 114/247 [01:14<01:29,  1.48it/s]Loading train:  47%|████▋     | 115/247 [01:15<01:28,  1.48it/s]Loading train:  47%|████▋     | 116/247 [01:16<01:28,  1.47it/s]Loading train:  47%|████▋     | 117/247 [01:16<01:29,  1.46it/s]Loading train:  48%|████▊     | 118/247 [01:17<01:29,  1.44it/s]Loading train:  48%|████▊     | 119/247 [01:18<01:28,  1.44it/s]Loading train:  49%|████▊     | 120/247 [01:19<01:28,  1.44it/s]Loading train:  49%|████▉     | 121/247 [01:19<01:27,  1.45it/s]Loading train:  49%|████▉     | 122/247 [01:20<01:26,  1.45it/s]Loading train:  50%|████▉     | 123/247 [01:21<01:28,  1.41it/s]Loading train:  50%|█████     | 124/247 [01:21<01:26,  1.42it/s]Loading train:  51%|█████     | 125/247 [01:22<01:25,  1.42it/s]Loading train:  51%|█████     | 126/247 [01:23<01:23,  1.45it/s]Loading train:  51%|█████▏    | 127/247 [01:23<01:21,  1.46it/s]Loading train:  52%|█████▏    | 128/247 [01:24<01:20,  1.48it/s]Loading train:  52%|█████▏    | 129/247 [01:25<01:19,  1.49it/s]Loading train:  53%|█████▎    | 130/247 [01:25<01:19,  1.48it/s]Loading train:  53%|█████▎    | 131/247 [01:26<01:19,  1.45it/s]Loading train:  53%|█████▎    | 132/247 [01:27<01:20,  1.43it/s]Loading train:  54%|█████▍    | 133/247 [01:28<01:19,  1.43it/s]Loading train:  54%|█████▍    | 134/247 [01:28<01:17,  1.45it/s]Loading train:  55%|█████▍    | 135/247 [01:29<01:15,  1.48it/s]Loading train:  55%|█████▌    | 136/247 [01:29<01:12,  1.54it/s]Loading train:  55%|█████▌    | 137/247 [01:30<01:09,  1.58it/s]Loading train:  56%|█████▌    | 138/247 [01:31<01:07,  1.60it/s]Loading train:  56%|█████▋    | 139/247 [01:31<01:05,  1.64it/s]Loading train:  57%|█████▋    | 140/247 [01:32<01:06,  1.62it/s]Loading train:  57%|█████▋    | 141/247 [01:33<01:05,  1.63it/s]Loading train:  57%|█████▋    | 142/247 [01:33<01:03,  1.66it/s]Loading train:  58%|█████▊    | 143/247 [01:34<01:02,  1.68it/s]Loading train:  58%|█████▊    | 144/247 [01:34<01:00,  1.69it/s]Loading train:  59%|█████▊    | 145/247 [01:35<01:00,  1.70it/s]Loading train:  59%|█████▉    | 146/247 [01:35<00:59,  1.70it/s]Loading train:  60%|█████▉    | 147/247 [01:36<00:59,  1.68it/s]Loading train:  60%|█████▉    | 148/247 [01:37<00:59,  1.67it/s]Loading train:  60%|██████    | 149/247 [01:37<00:58,  1.66it/s]Loading train:  61%|██████    | 150/247 [01:38<00:59,  1.63it/s]Loading train:  61%|██████    | 151/247 [01:39<00:59,  1.60it/s]Loading train:  62%|██████▏   | 152/247 [01:39<00:58,  1.62it/s]Loading train:  62%|██████▏   | 153/247 [01:40<00:57,  1.64it/s]Loading train:  62%|██████▏   | 154/247 [01:40<00:55,  1.67it/s]Loading train:  63%|██████▎   | 155/247 [01:41<00:53,  1.71it/s]Loading train:  63%|██████▎   | 156/247 [01:41<00:52,  1.74it/s]Loading train:  64%|██████▎   | 157/247 [01:42<00:51,  1.74it/s]Loading train:  64%|██████▍   | 158/247 [01:43<00:51,  1.74it/s]Loading train:  64%|██████▍   | 159/247 [01:43<00:51,  1.72it/s]Loading train:  65%|██████▍   | 160/247 [01:44<00:51,  1.70it/s]Loading train:  65%|██████▌   | 161/247 [01:44<00:49,  1.73it/s]Loading train:  66%|██████▌   | 162/247 [01:45<00:48,  1.75it/s]Loading train:  66%|██████▌   | 163/247 [01:45<00:47,  1.78it/s]Loading train:  66%|██████▋   | 164/247 [01:46<00:46,  1.80it/s]Loading train:  67%|██████▋   | 165/247 [01:46<00:45,  1.81it/s]Loading train:  67%|██████▋   | 166/247 [01:47<00:44,  1.82it/s]Loading train:  68%|██████▊   | 167/247 [01:48<00:44,  1.81it/s]Loading train:  68%|██████▊   | 168/247 [01:48<00:44,  1.78it/s]Loading train:  68%|██████▊   | 169/247 [01:49<00:43,  1.79it/s]Loading train:  69%|██████▉   | 170/247 [01:49<00:42,  1.82it/s]Loading train:  69%|██████▉   | 171/247 [01:50<00:42,  1.78it/s]Loading train:  70%|██████▉   | 172/247 [01:51<00:49,  1.50it/s]Loading train:  70%|███████   | 173/247 [01:52<00:53,  1.38it/s]Loading train:  70%|███████   | 174/247 [01:52<00:55,  1.31it/s]Loading train:  71%|███████   | 175/247 [01:53<01:00,  1.18it/s]Loading train:  71%|███████▏  | 176/247 [01:54<00:55,  1.29it/s]Loading train:  72%|███████▏  | 177/247 [01:55<00:51,  1.36it/s]Loading train:  72%|███████▏  | 178/247 [01:55<00:48,  1.42it/s]Loading train:  72%|███████▏  | 179/247 [01:56<00:47,  1.45it/s]Loading train:  73%|███████▎  | 180/247 [01:57<00:45,  1.48it/s]Loading train:  73%|███████▎  | 181/247 [01:57<00:44,  1.48it/s]Loading train:  74%|███████▎  | 182/247 [01:58<00:43,  1.50it/s]Loading train:  74%|███████▍  | 183/247 [01:59<00:42,  1.51it/s]Loading train:  74%|███████▍  | 184/247 [01:59<00:40,  1.54it/s]Loading train:  75%|███████▍  | 185/247 [02:00<00:40,  1.54it/s]Loading train:  75%|███████▌  | 186/247 [02:01<00:38,  1.57it/s]Loading train:  76%|███████▌  | 187/247 [02:01<00:38,  1.57it/s]Loading train:  76%|███████▌  | 188/247 [02:02<00:37,  1.56it/s]Loading train:  77%|███████▋  | 189/247 [02:02<00:36,  1.58it/s]Loading train:  77%|███████▋  | 190/247 [02:03<00:36,  1.58it/s]Loading train:  77%|███████▋  | 191/247 [02:04<00:35,  1.59it/s]Loading train:  78%|███████▊  | 192/247 [02:04<00:34,  1.58it/s]Loading train:  78%|███████▊  | 193/247 [02:05<00:34,  1.58it/s]Loading train:  79%|███████▊  | 194/247 [02:06<00:32,  1.61it/s]Loading train:  79%|███████▉  | 195/247 [02:06<00:32,  1.62it/s]Loading train:  79%|███████▉  | 196/247 [02:07<00:31,  1.64it/s]Loading train:  80%|███████▉  | 197/247 [02:07<00:30,  1.66it/s]Loading train:  80%|████████  | 198/247 [02:08<00:29,  1.67it/s]Loading train:  81%|████████  | 199/247 [02:09<00:28,  1.66it/s]Loading train:  81%|████████  | 200/247 [02:09<00:28,  1.65it/s]Loading train:  81%|████████▏ | 201/247 [02:10<00:28,  1.60it/s]Loading train:  82%|████████▏ | 202/247 [02:10<00:27,  1.62it/s]Loading train:  82%|████████▏ | 203/247 [02:11<00:27,  1.62it/s]Loading train:  83%|████████▎ | 204/247 [02:12<00:26,  1.60it/s]Loading train:  83%|████████▎ | 205/247 [02:12<00:26,  1.59it/s]Loading train:  83%|████████▎ | 206/247 [02:13<00:25,  1.62it/s]Loading train:  84%|████████▍ | 207/247 [02:14<00:24,  1.62it/s]Loading train:  84%|████████▍ | 208/247 [02:14<00:24,  1.61it/s]Loading train:  85%|████████▍ | 209/247 [02:15<00:23,  1.61it/s]Loading train:  85%|████████▌ | 210/247 [02:15<00:22,  1.62it/s]Loading train:  85%|████████▌ | 211/247 [02:16<00:21,  1.64it/s]Loading train:  86%|████████▌ | 212/247 [02:17<00:21,  1.65it/s]Loading train:  86%|████████▌ | 213/247 [02:17<00:20,  1.66it/s]Loading train:  87%|████████▋ | 214/247 [02:18<00:19,  1.65it/s]Loading train:  87%|████████▋ | 215/247 [02:18<00:19,  1.65it/s]Loading train:  87%|████████▋ | 216/247 [02:19<00:19,  1.63it/s]Loading train:  88%|████████▊ | 217/247 [02:20<00:18,  1.62it/s]Loading train:  88%|████████▊ | 218/247 [02:20<00:18,  1.61it/s]Loading train:  89%|████████▊ | 219/247 [02:21<00:17,  1.63it/s]Loading train:  89%|████████▉ | 220/247 [02:22<00:16,  1.61it/s]Loading train:  89%|████████▉ | 221/247 [02:22<00:15,  1.63it/s]Loading train:  90%|████████▉ | 222/247 [02:23<00:21,  1.19it/s]Loading train:  90%|█████████ | 223/247 [02:27<00:40,  1.69s/it]Loading train:  91%|█████████ | 224/247 [02:32<00:59,  2.60s/it]Loading train:  91%|█████████ | 225/247 [02:36<01:09,  3.17s/it]Loading train:  91%|█████████▏| 226/247 [02:41<01:12,  3.46s/it]Loading train:  92%|█████████▏| 227/247 [02:45<01:13,  3.68s/it]Loading train:  92%|█████████▏| 228/247 [02:49<01:12,  3.79s/it]Loading train:  93%|█████████▎| 229/247 [02:53<01:07,  3.78s/it]Loading train:  93%|█████████▎| 230/247 [02:59<01:17,  4.54s/it]Loading train:  94%|█████████▎| 231/247 [03:05<01:20,  5.03s/it]Loading train:  94%|█████████▍| 232/247 [03:12<01:22,  5.47s/it]Loading train:  94%|█████████▍| 233/247 [03:18<01:20,  5.74s/it]Loading train:  95%|█████████▍| 234/247 [03:24<01:14,  5.74s/it]Loading train:  95%|█████████▌| 235/247 [03:30<01:10,  5.91s/it]Loading train:  96%|█████████▌| 236/247 [03:36<01:06,  6.00s/it]Loading train:  96%|█████████▌| 237/247 [03:42<01:00,  6.01s/it]Loading train:  96%|█████████▋| 238/247 [03:48<00:53,  5.94s/it]Loading train:  97%|█████████▋| 239/247 [03:54<00:48,  6.00s/it]Loading train:  97%|█████████▋| 240/247 [04:00<00:41,  5.96s/it]Loading train:  98%|█████████▊| 241/247 [04:06<00:35,  5.98s/it]Loading train:  98%|█████████▊| 242/247 [04:12<00:29,  5.98s/it]Loading train:  98%|█████████▊| 243/247 [04:18<00:24,  6.05s/it]Loading train:  99%|█████████▉| 244/247 [04:24<00:18,  6.04s/it]Loading train:  99%|█████████▉| 245/247 [04:30<00:11,  5.98s/it]Loading train: 100%|█████████▉| 246/247 [04:36<00:05,  5.96s/it]Loading train: 100%|██████████| 247/247 [04:42<00:00,  5.99s/it]Loading train: 100%|██████████| 247/247 [04:42<00:00,  1.14s/it]
concatenating: train:   0%|          | 0/247 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/247 [00:00<00:04, 59.11it/s]concatenating: train:   5%|▍         | 12/247 [00:00<00:03, 59.04it/s]concatenating: train:   7%|▋         | 18/247 [00:00<00:03, 57.46it/s]concatenating: train:  10%|▉         | 24/247 [00:00<00:03, 57.81it/s]concatenating: train:  13%|█▎        | 31/247 [00:00<00:03, 59.36it/s]concatenating: train:  15%|█▌        | 38/247 [00:00<00:03, 61.52it/s]concatenating: train:  18%|█▊        | 44/247 [00:00<00:03, 60.89it/s]concatenating: train:  21%|██        | 51/247 [00:00<00:03, 62.15it/s]concatenating: train:  23%|██▎       | 57/247 [00:00<00:03, 59.63it/s]concatenating: train:  26%|██▌       | 63/247 [00:01<00:03, 59.67it/s]concatenating: train:  28%|██▊       | 69/247 [00:01<00:03, 57.68it/s]concatenating: train:  31%|███       | 76/247 [00:01<00:02, 59.36it/s]concatenating: train:  33%|███▎      | 82/247 [00:01<00:02, 57.35it/s]concatenating: train:  36%|███▌      | 88/247 [00:01<00:02, 56.94it/s]concatenating: train:  38%|███▊      | 94/247 [00:01<00:02, 57.79it/s]concatenating: train:  40%|████      | 100/247 [00:01<00:02, 56.86it/s]concatenating: train:  43%|████▎     | 106/247 [00:01<00:02, 55.11it/s]concatenating: train:  45%|████▌     | 112/247 [00:01<00:02, 53.07it/s]concatenating: train:  48%|████▊     | 119/247 [00:02<00:02, 54.78it/s]concatenating: train:  51%|█████     | 125/247 [00:02<00:02, 53.56it/s]concatenating: train:  53%|█████▎    | 132/247 [00:02<00:02, 56.41it/s]concatenating: train:  56%|█████▋    | 139/247 [00:02<00:01, 59.65it/s]concatenating: train:  59%|█████▉    | 146/247 [00:02<00:01, 61.13it/s]concatenating: train:  62%|██████▏   | 153/247 [00:02<00:01, 57.11it/s]concatenating: train:  65%|██████▍   | 160/247 [00:02<00:01, 60.36it/s]concatenating: train:  68%|██████▊   | 167/247 [00:02<00:01, 61.53it/s]concatenating: train:  70%|███████   | 174/247 [00:02<00:01, 62.41it/s]concatenating: train:  73%|███████▎  | 181/247 [00:03<00:01, 59.68it/s]concatenating: train:  76%|███████▌  | 188/247 [00:03<00:00, 60.84it/s]concatenating: train:  79%|███████▉  | 195/247 [00:03<00:00, 63.20it/s]concatenating: train:  82%|████████▏ | 202/247 [00:03<00:00, 62.71it/s]concatenating: train:  85%|████████▍ | 209/247 [00:03<00:00, 59.10it/s]concatenating: train:  87%|████████▋ | 215/247 [00:03<00:00, 57.92it/s]concatenating: train:  89%|████████▉ | 221/247 [00:03<00:00, 57.59it/s]concatenating: train:  92%|█████████▏| 227/247 [00:03<00:00, 57.40it/s]concatenating: train:  94%|█████████▍| 233/247 [00:03<00:00, 53.13it/s]concatenating: train:  97%|█████████▋| 239/247 [00:04<00:00, 51.39it/s]concatenating: train:  99%|█████████▉| 245/247 [00:04<00:00, 49.77it/s]concatenating: train: 100%|██████████| 247/247 [00:04<00:00, 57.59it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:17<01:09, 17.37s/it]Loading test:  40%|████      | 2/5 [00:32<00:50, 16.67s/it]Loading test:  60%|██████    | 3/5 [00:43<00:30, 15.07s/it]Loading test:  80%|████████  | 4/5 [00:49<00:12, 12.21s/it]Loading test: 100%|██████████| 5/5 [01:03<00:00, 12.84s/it]Loading test: 100%|██████████| 5/5 [01:03<00:00, 12.72s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 56.22it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 48, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 48, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 48, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 48, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 48, 52, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 48, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 48, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 24, 26, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 24, 26, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 24, 26, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 24, 26, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 24, 26, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 24, 26, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 24, 26, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 24, 26, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 24, 26, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 12, 13, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 12, 13, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 12, 13, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 12, 13, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 12, 13, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 12, 13, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 12, 13, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 12, 13, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 12, 13, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 12, 13, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 24, 26, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 24, 26, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 24, 26, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 24, 26, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 24, 26, 60)   0           batch_normalization_7[0][0]      2020-01-22 07:04:47.690411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 07:04:47.690492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 07:04:47.690504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 07:04:47.690512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 07:04:47.690797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 24, 26, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 24, 26, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 24, 26, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 24, 26, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24, 26, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 48, 52, 30)   25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 48, 52, 60)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 48, 52, 30)   16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 48, 52, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 48, 52, 30)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 48, 52, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 48, 52, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 48, 52, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 48, 52, 90)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 48, 52, 90)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 48, 52, 13)   1183        dropout_5[0][0]                  
==================================================================================================
Total params: 501,343
Trainable params: 500,143
Non-trainable params: 1,200
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1
------------------------------------------------------------------
class_weights [6.23741938e-02 3.14616033e-02 7.87383719e-02 9.59024836e-03
 2.85760268e-02 7.23242511e-03 8.58324214e-02 1.15198292e-01
 9.01097040e-02 1.30694373e-02 2.94189927e-01 1.83374918e-01
 2.52430462e-04]
Train on 15555 samples, validate on 310 samples
Epoch 1/300
 - 35s - loss: 0.6604 - acc: 0.9090 - mDice: 0.2879 - val_loss: 0.5374 - val_acc: 0.9299 - val_mDice: 0.1981

Epoch 00001: val_mDice improved from -inf to 0.19808, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300
 - 32s - loss: 0.4917 - acc: 0.9303 - mDice: 0.4698 - val_loss: 0.2111 - val_acc: 0.9346 - val_mDice: 0.2235

Epoch 00002: val_mDice improved from 0.19808 to 0.22353, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300
 - 31s - loss: 0.4407 - acc: 0.9352 - mDice: 0.5249 - val_loss: 0.0510 - val_acc: 0.9392 - val_mDice: 0.2365

Epoch 00003: val_mDice improved from 0.22353 to 0.23652, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 4/300
 - 32s - loss: 0.4190 - acc: 0.9378 - mDice: 0.5484 - val_loss: -2.0123e-02 - val_acc: 0.9413 - val_mDice: 0.2378

Epoch 00004: val_mDice improved from 0.23652 to 0.23781, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 5/300
 - 31s - loss: 0.4059 - acc: 0.9395 - mDice: 0.5624 - val_loss: -4.9078e-02 - val_acc: 0.9429 - val_mDice: 0.2403

Epoch 00005: val_mDice improved from 0.23781 to 0.24032, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 6/300
 - 32s - loss: 0.3961 - acc: 0.9408 - mDice: 0.5731 - val_loss: -2.8489e-02 - val_acc: 0.9425 - val_mDice: 0.2449

Epoch 00006: val_mDice improved from 0.24032 to 0.24495, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 7/300
 - 31s - loss: 0.3897 - acc: 0.9418 - mDice: 0.5800 - val_loss: -6.7171e-02 - val_acc: 0.9434 - val_mDice: 0.2446

Epoch 00007: val_mDice did not improve from 0.24495
Epoch 8/300
 - 32s - loss: 0.3789 - acc: 0.9427 - mDice: 0.5916 - val_loss: -8.8273e-02 - val_acc: 0.9445 - val_mDice: 0.2468

Epoch 00008: val_mDice improved from 0.24495 to 0.24678, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 9/300
 - 31s - loss: 0.3749 - acc: 0.9432 - mDice: 0.5960 - val_loss: -1.0336e-01 - val_acc: 0.9442 - val_mDice: 0.2487

Epoch 00009: val_mDice improved from 0.24678 to 0.24869, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 10/300
 - 31s - loss: 0.3720 - acc: 0.9439 - mDice: 0.5991 - val_loss: -8.4259e-02 - val_acc: 0.9451 - val_mDice: 0.2489

Epoch 00010: val_mDice improved from 0.24869 to 0.24888, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 11/300
 - 32s - loss: 0.3648 - acc: 0.9445 - mDice: 0.6069 - val_loss: -8.2219e-02 - val_acc: 0.9461 - val_mDice: 0.2477

Epoch 00011: val_mDice did not improve from 0.24888
Epoch 12/300
 - 31s - loss: 0.3615 - acc: 0.9449 - mDice: 0.6104 - val_loss: -1.0697e-01 - val_acc: 0.9463 - val_mDice: 0.2490

Epoch 00012: val_mDice improved from 0.24888 to 0.24897, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 13/300
 - 31s - loss: 0.3581 - acc: 0.9455 - mDice: 0.6141 - val_loss: -7.6902e-02 - val_acc: 0.9462 - val_mDice: 0.2475

Epoch 00013: val_mDice did not improve from 0.24897
Epoch 14/300
 - 31s - loss: 0.3519 - acc: 0.9458 - mDice: 0.6208 - val_loss: -9.0683e-02 - val_acc: 0.9462 - val_mDice: 0.2496

Epoch 00014: val_mDice improved from 0.24897 to 0.24956, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 15/300
 - 31s - loss: 0.3533 - acc: 0.9462 - mDice: 0.6193 - val_loss: -8.9963e-02 - val_acc: 0.9463 - val_mDice: 0.2484

Epoch 00015: val_mDice did not improve from 0.24956
Epoch 16/300
 - 31s - loss: 0.3495 - acc: 0.9466 - mDice: 0.6233 - val_loss: -8.2279e-02 - val_acc: 0.9466 - val_mDice: 0.2481

Epoch 00016: val_mDice did not improve from 0.24956
Epoch 17/300
 - 31s - loss: 0.3456 - acc: 0.9469 - mDice: 0.6276 - val_loss: -7.3980e-02 - val_acc: 0.9457 - val_mDice: 0.2499

Epoch 00017: val_mDice improved from 0.24956 to 0.24986, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 18/300
 - 31s - loss: 0.3399 - acc: 0.9473 - mDice: 0.6338 - val_loss: -1.0763e-01 - val_acc: 0.9467 - val_mDice: 0.2509

Epoch 00018: val_mDice improved from 0.24986 to 0.25093, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 19/300
 - 31s - loss: 0.3386 - acc: 0.9476 - mDice: 0.6351 - val_loss: -7.2980e-02 - val_acc: 0.9472 - val_mDice: 0.2491

Epoch 00019: val_mDice did not improve from 0.25093
Epoch 20/300
 - 31s - loss: 0.3390 - acc: 0.9477 - mDice: 0.6348 - val_loss: -1.0528e-01 - val_acc: 0.9468 - val_mDice: 0.2486

Epoch 00020: val_mDice did not improve from 0.25093
Epoch 21/300
 - 31s - loss: 0.3355 - acc: 0.9480 - mDice: 0.6385 - val_loss: -7.8511e-02 - val_acc: 0.9477 - val_mDice: 0.2510

Epoch 00021: val_mDice improved from 0.25093 to 0.25100, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 22/300
 - 31s - loss: 0.3349 - acc: 0.9481 - mDice: 0.6391 - val_loss: -8.9676e-02 - val_acc: 0.9478 - val_mDice: 0.2512

Epoch 00022: val_mDice improved from 0.25100 to 0.25116, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 23/300
 - 31s - loss: 0.3356 - acc: 0.9484 - mDice: 0.6383 - val_loss: -8.3458e-02 - val_acc: 0.9478 - val_mDice: 0.2512

Epoch 00023: val_mDice improved from 0.25116 to 0.25117, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 24/300
 - 32s - loss: 0.3333 - acc: 0.9486 - mDice: 0.6408 - val_loss: -9.6171e-02 - val_acc: 0.9467 - val_mDice: 0.2499

Epoch 00024: val_mDice did not improve from 0.25117
Epoch 25/300
 - 31s - loss: 0.3304 - acc: 0.9489 - mDice: 0.6440 - val_loss: -9.5254e-02 - val_acc: 0.9481 - val_mDice: 0.2523

Epoch 00025: val_mDice improved from 0.25117 to 0.25228, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 26/300
 - 32s - loss: 0.3270 - acc: 0.9490 - mDice: 0.6477 - val_loss: -8.7598e-02 - val_acc: 0.9469 - val_mDice: 0.2486

Epoch 00026: val_mDice did not improve from 0.25228
Epoch 27/300
 - 31s - loss: 0.3233 - acc: 0.9492 - mDice: 0.6517 - val_loss: -1.1384e-01 - val_acc: 0.9479 - val_mDice: 0.2490

Epoch 00027: val_mDice did not improve from 0.25228
Epoch 28/300
 - 31s - loss: 0.3244 - acc: 0.9494 - mDice: 0.6505 - val_loss: -8.8827e-02 - val_acc: 0.9486 - val_mDice: 0.2483

Epoch 00028: val_mDice did not improve from 0.25228
Epoch 29/300
 - 30s - loss: 0.3228 - acc: 0.9496 - mDice: 0.6522 - val_loss: -6.0949e-02 - val_acc: 0.9483 - val_mDice: 0.2463

Epoch 00029: val_mDice did not improve from 0.25228
Epoch 30/300
 - 31s - loss: 0.3221 - acc: 0.9498 - mDice: 0.6530 - val_loss: -1.0303e-01 - val_acc: 0.9473 - val_mDice: 0.2490

Epoch 00030: val_mDice did not improve from 0.25228
Epoch 31/300
 - 31s - loss: 0.3199 - acc: 0.9500 - mDice: 0.6553 - val_loss: -1.2100e-01 - val_acc: 0.9482 - val_mDice: 0.2460

Epoch 00031: val_mDice did not improve from 0.25228
Epoch 32/300
 - 31s - loss: 0.3190 - acc: 0.9502 - mDice: 0.6563 - val_loss: -7.4929e-02 - val_acc: 0.9482 - val_mDice: 0.2488

Epoch 00032: val_mDice did not improve from 0.25228
Epoch 33/300
 - 31s - loss: 0.3159 - acc: 0.9503 - mDice: 0.6596 - val_loss: -1.1284e-01 - val_acc: 0.9484 - val_mDice: 0.2491

Epoch 00033: val_mDice did not improve from 0.25228
Epoch 34/300
 - 31s - loss: 0.3150 - acc: 0.9504 - mDice: 0.6606 - val_loss: -6.8527e-02 - val_acc: 0.9479 - val_mDice: 0.2478

Epoch 00034: val_mDice did not improve from 0.25228
Epoch 35/300
 - 31s - loss: 0.3143 - acc: 0.9507 - mDice: 0.6614 - val_loss: -1.1256e-01 - val_acc: 0.9490 - val_mDice: 0.2479

Epoch 00035: val_mDice did not improve from 0.25228
Epoch 36/300
 - 31s - loss: 0.3107 - acc: 0.9508 - mDice: 0.6652 - val_loss: -1.0879e-01 - val_acc: 0.9482 - val_mDice: 0.2494

Epoch 00036: val_mDice did not improve from 0.25228
Epoch 37/300
 - 31s - loss: 0.3134 - acc: 0.9509 - mDice: 0.6623 - val_loss: -1.0543e-01 - val_acc: 0.9495 - val_mDice: 0.2471

Epoch 00037: val_mDice did not improve from 0.25228
Epoch 38/300
 - 31s - loss: 0.3120 - acc: 0.9510 - mDice: 0.6639 - val_loss: -1.0244e-01 - val_acc: 0.9485 - val_mDice: 0.2469

Epoch 00038: val_mDice did not improve from 0.25228
Epoch 39/300
 - 31s - loss: 0.3100 - acc: 0.9512 - mDice: 0.6660 - val_loss: -9.7326e-02 - val_acc: 0.9489 - val_mDice: 0.2473

Epoch 00039: val_mDice did not improve from 0.25228
Epoch 40/300
 - 31s - loss: 0.3088 - acc: 0.9513 - mDice: 0.6673 - val_loss: -7.9529e-02 - val_acc: 0.9478 - val_mDice: 0.2435

Epoch 00040: val_mDice did not improve from 0.25228

Epoch 00040: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 41/300
 - 31s - loss: 0.3075 - acc: 0.9515 - mDice: 0.6687 - val_loss: -6.7883e-02 - val_acc: 0.9484 - val_mDice: 0.2435

Epoch 00041: val_mDice did not improve from 0.25228
Epoch 42/300
 - 31s - loss: 0.3076 - acc: 0.9516 - mDice: 0.6686 - val_loss: -7.7278e-02 - val_acc: 0.9493 - val_mDice: 0.2494

Epoch 00042: val_mDice did not improve from 0.25228
Epoch 43/300
 - 31s - loss: 0.3042 - acc: 0.9518 - mDice: 0.6722 - val_loss: -9.1256e-02 - val_acc: 0.9491 - val_mDice: 0.2457

Epoch 00043: val_mDice did not improve from 0.25228
Epoch 44/300
 - 31s - loss: 0.3053 - acc: 0.9518 - mDice: 0.6711 - val_loss: -1.0847e-01 - val_acc: 0.9487 - val_mDice: 0.2456

Epoch 00044: val_mDice did not improve from 0.25228
Epoch 45/300
 - 32s - loss: 0.3045 - acc: 0.9518 - mDice: 0.6719 - val_loss: -9.3984e-02 - val_acc: 0.9487 - val_mDice: 0.2454

Epoch 00045: val_mDice did not improve from 0.25228
Epoch 46/300
 - 31s - loss: 0.3068 - acc: 0.9518 - mDice: 0.6695 - val_loss: -1.0890e-01 - val_acc: 0.9492 - val_mDice: 0.2462

Epoch 00046: val_mDice did not improve from 0.25228
Epoch 47/300
 - 31s - loss: 0.3037 - acc: 0.9519 - mDice: 0.6728 - val_loss: -7.6828e-02 - val_acc: 0.9485 - val_mDice: 0.2453

Epoch 00047: val_mDice did not improve from 0.25228
Epoch 48/300
 - 31s - loss: 0.3041 - acc: 0.9520 - mDice: 0.6724 - val_loss: -9.4846e-02 - val_acc: 0.9496 - val_mDice: 0.2454

Epoch 00048: val_mDice did not improve from 0.25228
Epoch 49/300
 - 31s - loss: 0.3032 - acc: 0.9521 - mDice: 0.6734 - val_loss: -7.3950e-02 - val_acc: 0.9479 - val_mDice: 0.2445

Epoch 00049: val_mDice did not improve from 0.25228
Epoch 50/300
 - 31s - loss: 0.3008 - acc: 0.9521 - mDice: 0.6759 - val_loss: -4.3661e-02 - val_acc: 0.9493 - val_mDice: 0.2450

Epoch 00050: val_mDice did not improve from 0.25228
Epoch 51/300
 - 31s - loss: 0.3016 - acc: 0.9522 - mDice: 0.6751 - val_loss: -7.6249e-02 - val_acc: 0.9490 - val_mDice: 0.2481

Epoch 00051: val_mDice did not improve from 0.25228
Epoch 52/300
 - 31s - loss: 0.2989 - acc: 0.9523 - mDice: 0.6780 - val_loss: -9.4783e-02 - val_acc: 0.9494 - val_mDice: 0.2458

Epoch 00052: val_mDice did not improve from 0.25228
Epoch 53/300
 - 31s - loss: 0.2984 - acc: 0.9523 - mDice: 0.6785 - val_loss: -7.3942e-02 - val_acc: 0.9494 - val_mDice: 0.2466

Epoch 00053: val_mDice did not improve from 0.25228
Epoch 54/300
 - 30s - loss: 0.3009 - acc: 0.9523 - mDice: 0.6758 - val_loss: -9.0759e-02 - val_acc: 0.9492 - val_mDice: 0.2448

Epoch 00054: val_mDice did not improve from 0.25228
Epoch 55/300
 - 31s - loss: 0.2948 - acc: 0.9525 - mDice: 0.6824 - val_loss: -1.2453e-01 - val_acc: 0.9486 - val_mDice: 0.2476

Epoch 00055: val_mDice did not improve from 0.25228

Epoch 00055: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 56/300
 - 31s - loss: 0.2984 - acc: 0.9525 - mDice: 0.6786 - val_loss: -1.0342e-01 - val_acc: 0.9495 - val_mDice: 0.2470

Epoch 00056: val_mDice did not improve from 0.25228
Epoch 57/300
 - 31s - loss: 0.2955 - acc: 0.9525 - mDice: 0.6817 - val_loss: -9.7480e-02 - val_acc: 0.9489 - val_mDice: 0.2473

Epoch 00057: val_mDice did not improve from 0.25228
Epoch 58/300
 - 31s - loss: 0.2962 - acc: 0.9526 - mDice: 0.6810 - val_loss: -1.0147e-01 - val_acc: 0.9492 - val_mDice: 0.2471

Epoch 00058: val_mDice did not improve from 0.25228
Epoch 59/300
 - 31s - loss: 0.2962 - acc: 0.9526 - mDice: 0.6809 - val_loss: -1.0234e-01 - val_acc: 0.9484 - val_mDice: 0.2463

Epoch 00059: val_mDice did not improve from 0.25228
Epoch 60/300
 - 30s - loss: 0.2981 - acc: 0.9526 - mDice: 0.6788 - val_loss: -8.4669e-02 - val_acc: 0.9486 - val_mDice: 0.2440

Epoch 00060: val_mDice did not improve from 0.25228
Epoch 61/300
 - 31s - loss: 0.2983 - acc: 0.9527 - mDice: 0.6787 - val_loss: -8.6437e-02 - val_acc: 0.9486 - val_mDice: 0.2445

Epoch 00061: val_mDice did not improve from 0.25228
Epoch 62/300
 - 30s - loss: 0.2966 - acc: 0.9527 - mDice: 0.6804 - val_loss: -1.0815e-01 - val_acc: 0.9492 - val_mDice: 0.2468

Epoch 00062: val_mDice did not improve from 0.25228
Epoch 63/300
 - 32s - loss: 0.2936 - acc: 0.9528 - mDice: 0.6837 - val_loss: -9.4201e-02 - val_acc: 0.9491 - val_mDice: 0.2454

Epoch 00063: val_mDice did not improve from 0.25228
Epoch 64/300
 - 31s - loss: 0.2958 - acc: 0.9527 - mDice: 0.6814 - val_loss: -1.1134e-01 - val_acc: 0.9493 - val_mDice: 0.2454

Epoch 00064: val_mDice did not improve from 0.25228
Epoch 65/300
 - 31s - loss: 0.2966 - acc: 0.9527 - mDice: 0.6804 - val_loss: -9.7882e-02 - val_acc: 0.9493 - val_mDice: 0.2455

Epoch 00065: val_mDice did not improve from 0.25228
Restoring model weights from the end of the best epoch
Epoch 00065: early stopping
{'val_loss': [0.5373821585409103, 0.21107048466951855, 0.05096553772267315, -0.02012306474329483, -0.04907779502231748, -0.028489169638393628, -0.0671713218663729, -0.08827293898549772, -0.10335752101156921, -0.08425925503815374, -0.08221872223000373, -0.10697376458237189, -0.07690238030326943, -0.09068340165897333, -0.08996258753975253, -0.08227883796057393, -0.07398026927483208, -0.10762569572656386, -0.07298049704742528, -0.10528226103633642, -0.0785107693124202, -0.08967564334424453, -0.08345827015657578, -0.09617144532591826, -0.09525394423173801, -0.08759815215824111, -0.1138439897838379, -0.08882678182975899, -0.060949336767436996, -0.10303495824337006, -0.12099723438281686, -0.07492907623189592, -0.11283654926697753, -0.06852664078225291, -0.11255961273526473, -0.10879477427431172, -0.10543175142318491, -0.10243655963530464, -0.09732580516532424, -0.07952944823210278, -0.067883413442741, -0.07727769239535255, -0.09125554713330442, -0.10846615955233574, -0.09398443759757004, -0.10890253753431382, -0.07682766717287802, -0.09484586284135378, -0.07394981501443733, -0.04366054504569019, -0.07624895457598951, -0.09478307639308754, -0.07394203096778403, -0.09075864218175411, -0.12453208882136331, -0.1034155657755271, -0.09748000697600806, -0.1014738816903123, -0.10233973544992266, -0.08466852528433647, -0.08643682750933353, -0.10814695168406732, -0.09420149513114724, -0.11133937222973234, -0.09788217734275086], 'val_acc': [0.929889113672318, 0.9346451067155407, 0.9392124164489007, 0.9412531025948063, 0.9428970160022858, 0.9424989723390148, 0.9434062319417154, 0.9444944185595359, 0.944237232208252, 0.945135441518599, 0.946107316401697, 0.9462649899144326, 0.946174523522777, 0.9462223437524611, 0.9463166844460272, 0.9466488303676728, 0.9456950503010904, 0.9467405926796698, 0.9472433309401235, 0.9468026276557676, 0.9477318582996246, 0.9477913014350399, 0.9477964735800221, 0.9467018150514172, 0.9480665806801089, 0.9468995563445552, 0.9479218375298285, 0.9486378200592533, 0.9483405678502975, 0.9473467276942346, 0.9482281304174854, 0.9481660954413875, 0.948398724679024, 0.9478662629281321, 0.9489505790895031, 0.9481880684052745, 0.9495101801810726, 0.9484568891986724, 0.9489311883526463, 0.9478339514424724, 0.9484491290584687, 0.9492620479676032, 0.9491276337254432, 0.9486520405738584, 0.9486597891776792, 0.9492103515132781, 0.9484814463123199, 0.9495567121813374, 0.9479257156771999, 0.9493408779944142, 0.9489647976813778, 0.9493925725260088, 0.9493796556226669, 0.9491857905541697, 0.9485563981917596, 0.9495463775050256, 0.9488626910794166, 0.9491741638029775, 0.9483806356306999, 0.9486339399891515, 0.9486158528635579, 0.9492025952185353, 0.9490681867445668, 0.9492685121874656, 0.9493331293905934], 'val_mDice': [0.1980806392046713, 0.22353471431039995, 0.2365213818848133, 0.2378070811110158, 0.2403212357913294, 0.24494570445629857, 0.24461693436868728, 0.24678075601977686, 0.24868623503754217, 0.24887793787544774, 0.24765599567082622, 0.24896836352925147, 0.24751614779233932, 0.24955709050259284, 0.248410978144215, 0.24807620553239698, 0.24985592860367992, 0.2509251321035047, 0.2490772131469942, 0.2486433868686999, 0.25099825786967433, 0.25116375737613245, 0.2511676308368483, 0.24992503754554257, 0.25228016198642794, 0.248563859491579, 0.2490138333410986, 0.24833187472916418, 0.24634279767351766, 0.24900081465321203, 0.24600000463185773, 0.24880323763335904, 0.24907407489034436, 0.24781145588044198, 0.24786317949333497, 0.2494175021927203, 0.24706096346339873, 0.2469334628793501, 0.2472723187698472, 0.24348475928268126, 0.24348691009706067, 0.24940604380061548, 0.24570824818745737, 0.24558940662011022, 0.2454211166308772, 0.2462091992699331, 0.24530858474393044, 0.2453712018266801, 0.2445266886103538, 0.24497121512409178, 0.24808650439785374, 0.24581299626058148, 0.24663436184487036, 0.24475465330385393, 0.2476329827500928, 0.2469886398363498, 0.24726199286599312, 0.24706689256333536, 0.24633039486023686, 0.24398050817751116, 0.24445518586904771, 0.24677006131218326, 0.24537954587609537, 0.24542053356286017, 0.24552211393752404], 'loss': [0.6604009731066491, 0.4916547744824395, 0.44072315684088326, 0.41896701968394456, 0.4059286204632821, 0.39606870312814885, 0.3896869474459752, 0.3789150928596483, 0.374891920694773, 0.3719762829445214, 0.36475876456516343, 0.3615073538198229, 0.35809004490722685, 0.3518952369613304, 0.35327081856946785, 0.3495216003111113, 0.34560834878665847, 0.3398855084661731, 0.3386492191959677, 0.3389508330776009, 0.33545416944167195, 0.33493529326737176, 0.3356310730289317, 0.333331569401791, 0.3303725905680496, 0.326992996949469, 0.32331986685574765, 0.32436124346755163, 0.3227773789269715, 0.32205146450545774, 0.31988483366572334, 0.31903636501518257, 0.31591288663497197, 0.3150232722907923, 0.31428646527851023, 0.31074897415651, 0.31340450858885893, 0.3119754979116899, 0.3100309580114255, 0.30877127599693194, 0.30747709437912585, 0.3075649712542482, 0.304244516344938, 0.3052768240171167, 0.30450849491313736, 0.3067708361267622, 0.30370174504023817, 0.3040726021607564, 0.3031541000451738, 0.3008141524344922, 0.3015986999125007, 0.29887104295948547, 0.29842640480133775, 0.30092519982494903, 0.2948227270417411, 0.2983625265648418, 0.2955132093662543, 0.296174608063215, 0.29623958152308416, 0.29813510030076956, 0.2982866313249802, 0.2966416951679639, 0.29362707863624976, 0.295790851748438, 0.2966297227100175], 'acc': [0.9089610943530618, 0.930263039920921, 0.9352264043526878, 0.9378173456664102, 0.9394849178991069, 0.9408274972526945, 0.9417882109906888, 0.9426506393134345, 0.943219237831849, 0.9438943898551374, 0.9444871226026865, 0.9449212476970374, 0.9455032130024971, 0.9458023231729209, 0.9462023195213384, 0.9465670825890198, 0.9469030232568985, 0.9472690477742141, 0.9476071001057148, 0.9477386648192815, 0.9480368464332576, 0.9481066196572547, 0.948428085976113, 0.9486481480681218, 0.9488990936429506, 0.9490058280692841, 0.9491717256138616, 0.9493997471005309, 0.9495908077297315, 0.9498419851323333, 0.9500155572299596, 0.9501904159908853, 0.950343152193807, 0.9504444784997016, 0.9506544195467619, 0.9508348166731844, 0.9509204053794483, 0.9510036243077299, 0.9511637519464275, 0.9512917602878412, 0.9515126480890677, 0.9515838387195645, 0.9517645200553411, 0.9517706249236141, 0.951776858006057, 0.9518250220843505, 0.9519356724074782, 0.9519547316009537, 0.9520982973294333, 0.95212894842886, 0.9522142015404733, 0.9523042206670723, 0.9523046065932568, 0.9522599699373837, 0.9524535047245424, 0.9525058670888488, 0.9524689589386698, 0.952591867337308, 0.9526061883351306, 0.9526254027190986, 0.9526718161528381, 0.952693091064203, 0.9527701540454643, 0.9527170170506876, 0.9527409971047736], 'mDice': [0.28786024352345807, 0.4698429580230645, 0.5248720195277332, 0.5483615513075513, 0.5624319096732009, 0.5730845211825161, 0.5799597598509327, 0.5916102492169374, 0.5959507908249392, 0.5990898431362887, 0.6068945707940703, 0.6104082494293981, 0.6140893035592545, 0.6207897530917147, 0.6192809449289053, 0.6233301977661406, 0.6275657568664575, 0.6337579871365895, 0.6350860078593076, 0.6347555294561065, 0.6385225645984648, 0.6390906524826805, 0.6383364350801111, 0.6408280012156181, 0.6440162345628117, 0.6476777168245264, 0.6516600701250244, 0.6505087437602111, 0.6522272391090895, 0.6530141194067519, 0.6553487829248532, 0.6562546065902832, 0.6596314763187481, 0.6605863425364566, 0.6613817275892458, 0.66520963344341, 0.6623408165115208, 0.6638830128454848, 0.6659819970595342, 0.6673425207832395, 0.6687367304043539, 0.6686446575327573, 0.6722333809640468, 0.6711072128078095, 0.6719499186031, 0.6694963429948212, 0.6728229317125574, 0.6724183741239672, 0.6734068571601562, 0.6759383821939586, 0.675090681219055, 0.6780432631102684, 0.6785180972956723, 0.6758133163026045, 0.6824136534924294, 0.6785848981235925, 0.6816725565263674, 0.6809544148302584, 0.6808789161883225, 0.6788268425157314, 0.6786583788101407, 0.6804438918704429, 0.68370544682435, 0.6813618102973391, 0.6804491687303799], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:01<00:06,  1.70s/it]predicting test subjects:  40%|████      | 2/5 [00:02<00:04,  1.53s/it]predicting test subjects:  60%|██████    | 3/5 [00:03<00:02,  1.39s/it]predicting test subjects:  80%|████████  | 4/5 [00:04<00:01,  1.26s/it]predicting test subjects: 100%|██████████| 5/5 [00:06<00:00,  1.23s/it]predicting test subjects: 100%|██████████| 5/5 [00:06<00:00,  1.20s/it]
Loading train:   0%|          | 0/247 [00:00<?, ?it/s]Loading train:   0%|          | 1/247 [00:00<01:02,  3.97it/s]Loading train:   1%|          | 2/247 [00:00<01:01,  3.96it/s]Loading train:   1%|          | 3/247 [00:00<01:00,  4.06it/s]Loading train:   2%|▏         | 4/247 [00:00<00:59,  4.12it/s]Loading train:   2%|▏         | 5/247 [00:01<00:59,  4.05it/s]Loading train:   2%|▏         | 6/247 [00:01<00:59,  4.07it/s]Loading train:   3%|▎         | 7/247 [00:01<00:57,  4.17it/s]Loading train:   3%|▎         | 8/247 [00:01<00:56,  4.27it/s]Loading train:   4%|▎         | 9/247 [00:02<00:54,  4.35it/s]Loading train:   4%|▍         | 10/247 [00:02<00:53,  4.41it/s]Loading train:   4%|▍         | 11/247 [00:02<00:52,  4.47it/s]Loading train:   5%|▍         | 12/247 [00:02<00:52,  4.48it/s]Loading train:   5%|▌         | 13/247 [00:03<00:52,  4.48it/s]Loading train:   6%|▌         | 14/247 [00:03<00:51,  4.49it/s]Loading train:   6%|▌         | 15/247 [00:03<00:51,  4.48it/s]Loading train:   6%|▋         | 16/247 [00:03<00:51,  4.50it/s]Loading train:   7%|▋         | 17/247 [00:03<00:51,  4.50it/s]Loading train:   7%|▋         | 18/247 [00:04<00:51,  4.47it/s]Loading train:   8%|▊         | 19/247 [00:04<00:50,  4.49it/s]Loading train:   8%|▊         | 20/247 [00:04<00:50,  4.51it/s]Loading train:   9%|▊         | 21/247 [00:04<00:50,  4.45it/s]Loading train:   9%|▉         | 22/247 [00:05<00:50,  4.44it/s]Loading train:   9%|▉         | 23/247 [00:05<00:49,  4.48it/s]Loading train:  10%|▉         | 24/247 [00:05<00:49,  4.48it/s]Loading train:  10%|█         | 25/247 [00:05<00:49,  4.52it/s]Loading train:  11%|█         | 26/247 [00:05<00:48,  4.56it/s]Loading train:  11%|█         | 27/247 [00:06<00:48,  4.54it/s]Loading train:  11%|█▏        | 28/247 [00:06<00:48,  4.56it/s]Loading train:  12%|█▏        | 29/247 [00:06<00:48,  4.53it/s]Loading train:  12%|█▏        | 30/247 [00:06<00:47,  4.57it/s]Loading train:  13%|█▎        | 31/247 [00:07<00:50,  4.29it/s]Loading train:  13%|█▎        | 32/247 [00:07<00:49,  4.34it/s]Loading train:  13%|█▎        | 33/247 [00:07<00:48,  4.40it/s]Loading train:  14%|█▍        | 34/247 [00:07<00:48,  4.43it/s]Loading train:  14%|█▍        | 35/247 [00:07<00:47,  4.47it/s]Loading train:  15%|█▍        | 36/247 [00:08<00:46,  4.52it/s]Loading train:  15%|█▍        | 37/247 [00:08<00:46,  4.52it/s]Loading train:  15%|█▌        | 38/247 [00:08<00:45,  4.56it/s]Loading train:  16%|█▌        | 39/247 [00:08<00:46,  4.48it/s]Loading train:  16%|█▌        | 40/247 [00:09<00:45,  4.51it/s]Loading train:  17%|█▋        | 41/247 [00:09<00:45,  4.49it/s]Loading train:  17%|█▋        | 42/247 [00:09<00:45,  4.48it/s]Loading train:  17%|█▋        | 43/247 [00:09<00:45,  4.46it/s]Loading train:  18%|█▊        | 44/247 [00:09<00:45,  4.50it/s]Loading train:  18%|█▊        | 45/247 [00:10<00:44,  4.50it/s]Loading train:  19%|█▊        | 46/247 [00:10<00:44,  4.49it/s]Loading train:  19%|█▉        | 47/247 [00:10<00:44,  4.52it/s]Loading train:  19%|█▉        | 48/247 [00:10<00:44,  4.51it/s]Loading train:  20%|█▉        | 49/247 [00:11<00:43,  4.55it/s]Loading train:  20%|██        | 50/247 [00:11<00:43,  4.53it/s]Loading train:  21%|██        | 51/247 [00:11<00:43,  4.51it/s]Loading train:  21%|██        | 52/247 [00:11<00:43,  4.44it/s]Loading train:  21%|██▏       | 53/247 [00:11<00:43,  4.43it/s]Loading train:  22%|██▏       | 54/247 [00:12<00:43,  4.48it/s]Loading train:  22%|██▏       | 55/247 [00:12<00:42,  4.53it/s]Loading train:  23%|██▎       | 56/247 [00:12<00:41,  4.56it/s]Loading train:  23%|██▎       | 57/247 [00:12<00:42,  4.52it/s]Loading train:  23%|██▎       | 58/247 [00:13<00:42,  4.50it/s]Loading train:  24%|██▍       | 59/247 [00:13<00:42,  4.46it/s]Loading train:  24%|██▍       | 60/247 [00:13<00:45,  4.09it/s]Loading train:  25%|██▍       | 61/247 [00:13<00:44,  4.20it/s]Loading train:  25%|██▌       | 62/247 [00:14<00:43,  4.27it/s]Loading train:  26%|██▌       | 63/247 [00:14<00:42,  4.33it/s]Loading train:  26%|██▌       | 64/247 [00:14<00:42,  4.32it/s]Loading train:  26%|██▋       | 65/247 [00:14<00:42,  4.32it/s]Loading train:  27%|██▋       | 66/247 [00:14<00:41,  4.31it/s]Loading train:  27%|██▋       | 67/247 [00:15<00:41,  4.31it/s]Loading train:  28%|██▊       | 68/247 [00:15<00:41,  4.31it/s]Loading train:  28%|██▊       | 69/247 [00:15<00:41,  4.31it/s]Loading train:  28%|██▊       | 70/247 [00:15<00:40,  4.35it/s]Loading train:  29%|██▊       | 71/247 [00:16<00:40,  4.37it/s]Loading train:  29%|██▉       | 72/247 [00:16<00:39,  4.40it/s]Loading train:  30%|██▉       | 73/247 [00:16<00:39,  4.38it/s]Loading train:  30%|██▉       | 74/247 [00:16<00:39,  4.39it/s]Loading train:  30%|███       | 75/247 [00:16<00:39,  4.41it/s]Loading train:  31%|███       | 76/247 [00:17<00:38,  4.39it/s]Loading train:  31%|███       | 77/247 [00:17<00:39,  4.33it/s]Loading train:  32%|███▏      | 78/247 [00:17<00:41,  4.11it/s]Loading train:  32%|███▏      | 79/247 [00:17<00:41,  4.08it/s]Loading train:  32%|███▏      | 80/247 [00:18<00:38,  4.32it/s]Loading train:  33%|███▎      | 81/247 [00:18<00:38,  4.33it/s]Loading train:  33%|███▎      | 82/247 [00:18<00:37,  4.41it/s]Loading train:  34%|███▎      | 83/247 [00:18<00:37,  4.41it/s]Loading train:  34%|███▍      | 84/247 [00:19<00:36,  4.47it/s]Loading train:  34%|███▍      | 85/247 [00:19<00:36,  4.46it/s]Loading train:  35%|███▍      | 86/247 [00:19<00:35,  4.50it/s]Loading train:  35%|███▌      | 87/247 [00:19<00:35,  4.51it/s]Loading train:  36%|███▌      | 88/247 [00:19<00:35,  4.53it/s]Loading train:  36%|███▌      | 89/247 [00:20<00:34,  4.55it/s]Loading train:  36%|███▋      | 90/247 [00:20<00:34,  4.57it/s]Loading train:  37%|███▋      | 91/247 [00:20<00:34,  4.54it/s]Loading train:  37%|███▋      | 92/247 [00:20<00:34,  4.56it/s]Loading train:  38%|███▊      | 93/247 [00:21<00:33,  4.57it/s]Loading train:  38%|███▊      | 94/247 [00:21<00:33,  4.58it/s]Loading train:  38%|███▊      | 95/247 [00:21<00:33,  4.59it/s]Loading train:  39%|███▉      | 96/247 [00:21<00:33,  4.56it/s]Loading train:  39%|███▉      | 97/247 [00:21<00:33,  4.52it/s]Loading train:  40%|███▉      | 98/247 [00:22<00:33,  4.50it/s]Loading train:  40%|████      | 99/247 [00:22<00:33,  4.42it/s]Loading train:  40%|████      | 100/247 [00:22<00:34,  4.26it/s]Loading train:  41%|████      | 101/247 [00:22<00:34,  4.18it/s]Loading train:  41%|████▏     | 102/247 [00:23<00:34,  4.15it/s]Loading train:  42%|████▏     | 103/247 [00:23<00:35,  4.09it/s]Loading train:  42%|████▏     | 104/247 [00:23<00:35,  4.05it/s]Loading train:  43%|████▎     | 105/247 [00:23<00:35,  4.04it/s]Loading train:  43%|████▎     | 106/247 [00:24<00:35,  4.00it/s]Loading train:  43%|████▎     | 107/247 [00:24<00:35,  3.99it/s]Loading train:  44%|████▎     | 108/247 [00:24<00:35,  3.94it/s]Loading train:  44%|████▍     | 109/247 [00:24<00:35,  3.93it/s]Loading train:  45%|████▍     | 110/247 [00:25<00:34,  3.95it/s]Loading train:  45%|████▍     | 111/247 [00:25<00:34,  3.97it/s]Loading train:  45%|████▌     | 112/247 [00:25<00:33,  4.00it/s]Loading train:  46%|████▌     | 113/247 [00:25<00:33,  4.00it/s]Loading train:  46%|████▌     | 114/247 [00:26<00:33,  4.00it/s]Loading train:  47%|████▋     | 115/247 [00:26<00:33,  3.99it/s]Loading train:  47%|████▋     | 116/247 [00:26<00:33,  3.95it/s]Loading train:  47%|████▋     | 117/247 [00:26<00:32,  3.96it/s]Loading train:  48%|████▊     | 118/247 [00:27<00:31,  4.08it/s]Loading train:  48%|████▊     | 119/247 [00:27<00:30,  4.19it/s]Loading train:  49%|████▊     | 120/247 [00:27<00:29,  4.25it/s]Loading train:  49%|████▉     | 121/247 [00:27<00:29,  4.28it/s]Loading train:  49%|████▉     | 122/247 [00:28<00:29,  4.26it/s]Loading train:  50%|████▉     | 123/247 [00:28<00:28,  4.29it/s]Loading train:  50%|█████     | 124/247 [00:28<00:28,  4.31it/s]Loading train:  51%|█████     | 125/247 [00:28<00:28,  4.34it/s]Loading train:  51%|█████     | 126/247 [00:28<00:28,  4.32it/s]Loading train:  51%|█████▏    | 127/247 [00:29<00:27,  4.32it/s]Loading train:  52%|█████▏    | 128/247 [00:29<00:27,  4.34it/s]Loading train:  52%|█████▏    | 129/247 [00:29<00:27,  4.34it/s]Loading train:  53%|█████▎    | 130/247 [00:29<00:27,  4.31it/s]Loading train:  53%|█████▎    | 131/247 [00:30<00:26,  4.33it/s]Loading train:  53%|█████▎    | 132/247 [00:30<00:26,  4.36it/s]Loading train:  54%|█████▍    | 133/247 [00:30<00:26,  4.34it/s]Loading train:  54%|█████▍    | 134/247 [00:30<00:26,  4.28it/s]Loading train:  55%|█████▍    | 135/247 [00:31<00:26,  4.30it/s]Loading train:  55%|█████▌    | 136/247 [00:31<00:24,  4.52it/s]Loading train:  55%|█████▌    | 137/247 [00:31<00:23,  4.71it/s]Loading train:  56%|█████▌    | 138/247 [00:31<00:22,  4.84it/s]Loading train:  56%|█████▋    | 139/247 [00:31<00:21,  4.93it/s]Loading train:  57%|█████▋    | 140/247 [00:32<00:21,  5.00it/s]Loading train:  57%|█████▋    | 141/247 [00:32<00:20,  5.10it/s]Loading train:  57%|█████▋    | 142/247 [00:32<00:20,  5.15it/s]Loading train:  58%|█████▊    | 143/247 [00:32<00:20,  5.14it/s]Loading train:  58%|█████▊    | 144/247 [00:32<00:20,  5.13it/s]Loading train:  59%|█████▊    | 145/247 [00:32<00:19,  5.17it/s]Loading train:  59%|█████▉    | 146/247 [00:33<00:19,  5.17it/s]Loading train:  60%|█████▉    | 147/247 [00:33<00:19,  5.18it/s]Loading train:  60%|█████▉    | 148/247 [00:33<00:19,  5.15it/s]Loading train:  60%|██████    | 149/247 [00:33<00:19,  5.13it/s]Loading train:  61%|██████    | 150/247 [00:33<00:19,  5.02it/s]Loading train:  61%|██████    | 151/247 [00:34<00:19,  4.92it/s]Loading train:  62%|██████▏   | 152/247 [00:34<00:19,  4.91it/s]Loading train:  62%|██████▏   | 153/247 [00:34<00:18,  4.98it/s]Loading train:  62%|██████▏   | 154/247 [00:34<00:19,  4.85it/s]Loading train:  63%|██████▎   | 155/247 [00:35<00:19,  4.76it/s]Loading train:  63%|██████▎   | 156/247 [00:35<00:19,  4.67it/s]Loading train:  64%|██████▎   | 157/247 [00:35<00:19,  4.54it/s]Loading train:  64%|██████▍   | 158/247 [00:35<00:19,  4.50it/s]Loading train:  64%|██████▍   | 159/247 [00:35<00:19,  4.52it/s]Loading train:  65%|██████▍   | 160/247 [00:36<00:18,  4.59it/s]Loading train:  65%|██████▌   | 161/247 [00:36<00:18,  4.62it/s]Loading train:  66%|██████▌   | 162/247 [00:36<00:18,  4.63it/s]Loading train:  66%|██████▌   | 163/247 [00:36<00:18,  4.59it/s]Loading train:  66%|██████▋   | 164/247 [00:37<00:17,  4.61it/s]Loading train:  67%|██████▋   | 165/247 [00:37<00:17,  4.57it/s]Loading train:  67%|██████▋   | 166/247 [00:37<00:17,  4.56it/s]Loading train:  68%|██████▊   | 167/247 [00:37<00:17,  4.57it/s]Loading train:  68%|██████▊   | 168/247 [00:37<00:17,  4.58it/s]Loading train:  68%|██████▊   | 169/247 [00:38<00:17,  4.58it/s]Loading train:  69%|██████▉   | 170/247 [00:38<00:16,  4.58it/s]Loading train:  69%|██████▉   | 171/247 [00:38<00:16,  4.60it/s]Loading train:  70%|██████▉   | 172/247 [00:38<00:16,  4.46it/s]Loading train:  70%|███████   | 173/247 [00:38<00:16,  4.48it/s]Loading train:  70%|███████   | 174/247 [00:39<00:18,  3.92it/s]Loading train:  71%|███████   | 175/247 [00:39<00:19,  3.78it/s]Loading train:  71%|███████▏  | 176/247 [00:39<00:18,  3.91it/s]Loading train:  72%|███████▏  | 177/247 [00:40<00:17,  4.05it/s]Loading train:  72%|███████▏  | 178/247 [00:40<00:16,  4.15it/s]Loading train:  72%|███████▏  | 179/247 [00:40<00:16,  4.11it/s]Loading train:  73%|███████▎  | 180/247 [00:40<00:16,  4.11it/s]Loading train:  73%|███████▎  | 181/247 [00:41<00:15,  4.22it/s]Loading train:  74%|███████▎  | 182/247 [00:41<00:15,  4.32it/s]Loading train:  74%|███████▍  | 183/247 [00:41<00:14,  4.35it/s]Loading train:  74%|███████▍  | 184/247 [00:41<00:14,  4.36it/s]Loading train:  75%|███████▍  | 185/247 [00:41<00:14,  4.42it/s]Loading train:  75%|███████▌  | 186/247 [00:42<00:13,  4.51it/s]Loading train:  76%|███████▌  | 187/247 [00:42<00:13,  4.52it/s]Loading train:  76%|███████▌  | 188/247 [00:42<00:13,  4.53it/s]Loading train:  77%|███████▋  | 189/247 [00:42<00:12,  4.56it/s]Loading train:  77%|███████▋  | 190/247 [00:42<00:12,  4.56it/s]Loading train:  77%|███████▋  | 191/247 [00:43<00:12,  4.57it/s]Loading train:  78%|███████▊  | 192/247 [00:43<00:12,  4.58it/s]Loading train:  78%|███████▊  | 193/247 [00:43<00:11,  4.55it/s]Loading train:  79%|███████▊  | 194/247 [00:43<00:11,  4.63it/s]Loading train:  79%|███████▉  | 195/247 [00:44<00:11,  4.67it/s]Loading train:  79%|███████▉  | 196/247 [00:44<00:10,  4.71it/s]Loading train:  80%|███████▉  | 197/247 [00:44<00:10,  4.72it/s]Loading train:  80%|████████  | 198/247 [00:44<00:10,  4.73it/s]Loading train:  81%|████████  | 199/247 [00:44<00:10,  4.71it/s]Loading train:  81%|████████  | 200/247 [00:45<00:10,  4.69it/s]Loading train:  81%|████████▏ | 201/247 [00:45<00:09,  4.71it/s]Loading train:  82%|████████▏ | 202/247 [00:45<00:09,  4.73it/s]Loading train:  82%|████████▏ | 203/247 [00:45<00:09,  4.71it/s]Loading train:  83%|████████▎ | 204/247 [00:45<00:09,  4.71it/s]Loading train:  83%|████████▎ | 205/247 [00:46<00:08,  4.71it/s]Loading train:  83%|████████▎ | 206/247 [00:46<00:08,  4.67it/s]Loading train:  84%|████████▍ | 207/247 [00:46<00:08,  4.69it/s]Loading train:  84%|████████▍ | 208/247 [00:46<00:08,  4.70it/s]Loading train:  85%|████████▍ | 209/247 [00:47<00:08,  4.66it/s]Loading train:  85%|████████▌ | 210/247 [00:47<00:07,  4.64it/s]Loading train:  85%|████████▌ | 211/247 [00:47<00:07,  4.64it/s]Loading train:  86%|████████▌ | 212/247 [00:47<00:07,  4.59it/s]Loading train:  86%|████████▌ | 213/247 [00:47<00:07,  4.45it/s]Loading train:  87%|████████▋ | 214/247 [00:48<00:07,  4.38it/s]Loading train:  87%|████████▋ | 215/247 [00:48<00:07,  4.39it/s]Loading train:  87%|████████▋ | 216/247 [00:48<00:07,  4.38it/s]Loading train:  88%|████████▊ | 217/247 [00:48<00:06,  4.40it/s]Loading train:  88%|████████▊ | 218/247 [00:49<00:06,  4.44it/s]Loading train:  89%|████████▊ | 219/247 [00:49<00:06,  4.42it/s]Loading train:  89%|████████▉ | 220/247 [00:49<00:06,  4.43it/s]Loading train:  89%|████████▉ | 221/247 [00:49<00:05,  4.46it/s]Loading train:  90%|████████▉ | 222/247 [00:49<00:05,  4.48it/s]Loading train:  90%|█████████ | 223/247 [00:50<00:05,  4.44it/s]Loading train:  91%|█████████ | 224/247 [00:50<00:05,  4.43it/s]Loading train:  91%|█████████ | 225/247 [00:50<00:04,  4.44it/s]Loading train:  91%|█████████▏| 226/247 [00:50<00:04,  4.46it/s]Loading train:  92%|█████████▏| 227/247 [00:51<00:04,  4.45it/s]Loading train:  92%|█████████▏| 228/247 [00:51<00:04,  4.41it/s]Loading train:  93%|█████████▎| 229/247 [00:51<00:04,  4.20it/s]Loading train:  93%|█████████▎| 230/247 [00:51<00:04,  4.10it/s]Loading train:  94%|█████████▎| 231/247 [00:52<00:03,  4.04it/s]Loading train:  94%|█████████▍| 232/247 [00:52<00:03,  3.99it/s]Loading train:  94%|█████████▍| 233/247 [00:52<00:03,  3.95it/s]Loading train:  95%|█████████▍| 234/247 [00:52<00:03,  3.93it/s]Loading train:  95%|█████████▌| 235/247 [00:53<00:03,  3.96it/s]Loading train:  96%|█████████▌| 236/247 [00:53<00:02,  4.00it/s]Loading train:  96%|█████████▌| 237/247 [00:53<00:02,  4.03it/s]Loading train:  96%|█████████▋| 238/247 [00:53<00:02,  4.07it/s]Loading train:  97%|█████████▋| 239/247 [00:54<00:01,  4.11it/s]Loading train:  97%|█████████▋| 240/247 [00:54<00:01,  4.13it/s]Loading train:  98%|█████████▊| 241/247 [00:54<00:01,  4.16it/s]Loading train:  98%|█████████▊| 242/247 [00:54<00:01,  4.14it/s]Loading train:  98%|█████████▊| 243/247 [00:55<00:00,  4.15it/s]Loading train:  99%|█████████▉| 244/247 [00:55<00:00,  4.16it/s]Loading train:  99%|█████████▉| 245/247 [00:55<00:00,  4.17it/s]Loading train: 100%|█████████▉| 246/247 [00:55<00:00,  4.15it/s]Loading train: 100%|██████████| 247/247 [00:56<00:00,  4.16it/s]Loading train: 100%|██████████| 247/247 [00:56<00:00,  4.41it/s]
concatenating: train:   0%|          | 0/247 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/247 [00:00<00:05, 47.41it/s]concatenating: train:   4%|▍         | 11/247 [00:00<00:04, 49.29it/s]concatenating: train:   7%|▋         | 17/247 [00:00<00:04, 50.70it/s]concatenating: train:   9%|▉         | 23/247 [00:00<00:04, 51.77it/s]concatenating: train:  12%|█▏        | 29/247 [00:00<00:04, 52.90it/s]concatenating: train:  14%|█▍        | 35/247 [00:00<00:03, 53.13it/s]concatenating: train:  17%|█▋        | 41/247 [00:00<00:03, 54.34it/s]concatenating: train:  19%|█▉        | 47/247 [00:00<00:03, 53.12it/s]concatenating: train:  21%|██▏       | 53/247 [00:00<00:03, 53.33it/s]concatenating: train:  24%|██▍       | 59/247 [00:01<00:03, 53.80it/s]concatenating: train:  26%|██▋       | 65/247 [00:01<00:03, 55.49it/s]concatenating: train:  29%|██▊       | 71/247 [00:01<00:03, 53.38it/s]concatenating: train:  31%|███       | 77/247 [00:01<00:03, 54.34it/s]concatenating: train:  34%|███▎      | 83/247 [00:01<00:03, 54.00it/s]concatenating: train:  36%|███▌      | 89/247 [00:01<00:02, 52.80it/s]concatenating: train:  38%|███▊      | 95/247 [00:01<00:02, 51.57it/s]concatenating: train:  41%|████      | 101/247 [00:01<00:02, 52.91it/s]concatenating: train:  43%|████▎     | 107/247 [00:02<00:02, 51.38it/s]concatenating: train:  46%|████▌     | 113/247 [00:02<00:02, 51.15it/s]concatenating: train:  48%|████▊     | 119/247 [00:02<00:02, 49.57it/s]concatenating: train:  50%|█████     | 124/247 [00:02<00:02, 49.40it/s]concatenating: train:  52%|█████▏    | 129/247 [00:02<00:02, 48.50it/s]concatenating: train:  55%|█████▍    | 135/247 [00:02<00:02, 49.84it/s]concatenating: train:  57%|█████▋    | 141/247 [00:02<00:02, 50.90it/s]concatenating: train:  60%|█████▉    | 148/247 [00:02<00:01, 54.07it/s]concatenating: train:  62%|██████▏   | 154/247 [00:02<00:01, 53.89it/s]concatenating: train:  65%|██████▍   | 160/247 [00:03<00:01, 52.97it/s]concatenating: train:  67%|██████▋   | 166/247 [00:03<00:01, 54.18it/s]concatenating: train:  70%|██████▉   | 172/247 [00:03<00:01, 52.51it/s]concatenating: train:  72%|███████▏  | 178/247 [00:03<00:01, 51.28it/s]concatenating: train:  74%|███████▍  | 184/247 [00:03<00:01, 52.66it/s]concatenating: train:  77%|███████▋  | 190/247 [00:03<00:01, 53.17it/s]concatenating: train:  79%|███████▉  | 196/247 [00:03<00:00, 51.24it/s]concatenating: train:  82%|████████▏ | 202/247 [00:03<00:00, 50.96it/s]concatenating: train:  84%|████████▍ | 208/247 [00:03<00:00, 50.97it/s]concatenating: train:  87%|████████▋ | 214/247 [00:04<00:00, 50.54it/s]concatenating: train:  89%|████████▉ | 220/247 [00:04<00:00, 49.62it/s]concatenating: train:  91%|█████████ | 225/247 [00:04<00:00, 49.38it/s]concatenating: train:  93%|█████████▎| 230/247 [00:04<00:00, 48.48it/s]concatenating: train:  95%|█████████▌| 235/247 [00:04<00:00, 47.63it/s]concatenating: train:  98%|█████████▊| 242/247 [00:04<00:00, 51.49it/s]concatenating: train: 100%|██████████| 247/247 [00:04<00:00, 52.31it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:00<00:01,  3.54it/s]Loading test:  40%|████      | 2/5 [00:00<00:00,  3.65it/s]Loading test:  60%|██████    | 3/5 [00:00<00:00,  3.90it/s]Loading test:  80%|████████  | 4/5 [00:00<00:00,  4.05it/s]Loading test: 100%|██████████| 5/5 [00:01<00:00,  3.97it/s]Loading test: 100%|██████████| 5/5 [00:01<00:00,  4.03it/s]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 298.26it/s]
Loading trainS:   0%|          | 0/247 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/247 [00:00<00:57,  4.25it/s]Loading trainS:   1%|          | 2/247 [00:00<00:57,  4.28it/s]Loading trainS:   1%|          | 3/247 [00:00<00:56,  4.30it/s]Loading trainS:   2%|▏         | 4/247 [00:00<00:56,  4.28it/s]Loading trainS:   2%|▏         | 5/247 [00:01<00:55,  4.34it/s]Loading trainS:   2%|▏         | 6/247 [00:01<00:55,  4.38it/s]Loading trainS:   3%|▎         | 7/247 [00:01<00:54,  4.41it/s]Loading trainS:   3%|▎         | 8/247 [00:01<00:54,  4.40it/s]Loading trainS:   4%|▎         | 9/247 [00:02<00:54,  4.39it/s]Loading trainS:   4%|▍         | 10/247 [00:02<00:53,  4.42it/s]Loading trainS:   4%|▍         | 11/247 [00:02<00:54,  4.34it/s]Loading trainS:   5%|▍         | 12/247 [00:02<00:53,  4.40it/s]Loading trainS:   5%|▌         | 13/247 [00:02<00:52,  4.44it/s]Loading trainS:   6%|▌         | 14/247 [00:03<00:52,  4.42it/s]Loading trainS:   6%|▌         | 15/247 [00:03<00:52,  4.45it/s]Loading trainS:   6%|▋         | 16/247 [00:03<00:51,  4.48it/s]Loading trainS:   7%|▋         | 17/247 [00:03<00:51,  4.48it/s]Loading trainS:   7%|▋         | 18/247 [00:04<00:51,  4.46it/s]Loading trainS:   8%|▊         | 19/247 [00:04<00:51,  4.46it/s]Loading trainS:   8%|▊         | 20/247 [00:04<00:50,  4.46it/s]Loading trainS:   9%|▊         | 21/247 [00:04<00:51,  4.38it/s]Loading trainS:   9%|▉         | 22/247 [00:05<00:52,  4.32it/s]Loading trainS:   9%|▉         | 23/247 [00:05<00:51,  4.31it/s]Loading trainS:  10%|▉         | 24/247 [00:05<00:52,  4.23it/s]Loading trainS:  10%|█         | 25/247 [00:05<00:51,  4.32it/s]Loading trainS:  11%|█         | 26/247 [00:05<00:50,  4.38it/s]Loading trainS:  11%|█         | 27/247 [00:06<00:49,  4.43it/s]Loading trainS:  11%|█▏        | 28/247 [00:06<00:49,  4.45it/s]Loading trainS:  12%|█▏        | 29/247 [00:06<00:48,  4.47it/s]Loading trainS:  12%|█▏        | 30/247 [00:06<00:48,  4.49it/s]Loading trainS:  13%|█▎        | 31/247 [00:07<00:47,  4.50it/s]Loading trainS:  13%|█▎        | 32/247 [00:07<00:47,  4.51it/s]Loading trainS:  13%|█▎        | 33/247 [00:07<00:47,  4.51it/s]Loading trainS:  14%|█▍        | 34/247 [00:07<00:47,  4.53it/s]Loading trainS:  14%|█▍        | 35/247 [00:07<00:46,  4.52it/s]Loading trainS:  15%|█▍        | 36/247 [00:08<00:46,  4.52it/s]Loading trainS:  15%|█▍        | 37/247 [00:08<00:46,  4.53it/s]Loading trainS:  15%|█▌        | 38/247 [00:08<00:46,  4.51it/s]Loading trainS:  16%|█▌        | 39/247 [00:08<00:45,  4.53it/s]Loading trainS:  16%|█▌        | 40/247 [00:09<00:45,  4.53it/s]Loading trainS:  17%|█▋        | 41/247 [00:09<00:45,  4.54it/s]Loading trainS:  17%|█▋        | 42/247 [00:09<00:45,  4.53it/s]Loading trainS:  17%|█▋        | 43/247 [00:09<00:44,  4.54it/s]Loading trainS:  18%|█▊        | 44/247 [00:09<00:44,  4.53it/s]Loading trainS:  18%|█▊        | 45/247 [00:10<00:44,  4.53it/s]Loading trainS:  19%|█▊        | 46/247 [00:10<00:44,  4.54it/s]Loading trainS:  19%|█▉        | 47/247 [00:10<00:44,  4.53it/s]Loading trainS:  19%|█▉        | 48/247 [00:10<00:43,  4.54it/s]Loading trainS:  20%|█▉        | 49/247 [00:11<00:43,  4.54it/s]Loading trainS:  20%|██        | 50/247 [00:11<00:43,  4.50it/s]Loading trainS:  21%|██        | 51/247 [00:11<00:43,  4.47it/s]Loading trainS:  21%|██        | 52/247 [00:11<00:43,  4.51it/s]Loading trainS:  21%|██▏       | 53/247 [00:11<00:42,  4.55it/s]Loading trainS:  22%|██▏       | 54/247 [00:12<00:42,  4.57it/s]Loading trainS:  22%|██▏       | 55/247 [00:12<00:42,  4.56it/s]Loading trainS:  23%|██▎       | 56/247 [00:12<00:42,  4.55it/s]Loading trainS:  23%|██▎       | 57/247 [00:12<00:41,  4.59it/s]Loading trainS:  23%|██▎       | 58/247 [00:12<00:41,  4.57it/s]Loading trainS:  24%|██▍       | 59/247 [00:13<00:41,  4.54it/s]Loading trainS:  24%|██▍       | 60/247 [00:13<00:42,  4.45it/s]Loading trainS:  25%|██▍       | 61/247 [00:13<00:41,  4.43it/s]Loading trainS:  25%|██▌       | 62/247 [00:13<00:42,  4.32it/s]Loading trainS:  26%|██▌       | 63/247 [00:14<00:42,  4.29it/s]Loading trainS:  26%|██▌       | 64/247 [00:14<00:42,  4.32it/s]Loading trainS:  26%|██▋       | 65/247 [00:14<00:42,  4.31it/s]Loading trainS:  27%|██▋       | 66/247 [00:14<00:41,  4.38it/s]Loading trainS:  27%|██▋       | 67/247 [00:15<00:41,  4.38it/s]Loading trainS:  28%|██▊       | 68/247 [00:15<00:41,  4.36it/s]Loading trainS:  28%|██▊       | 69/247 [00:15<00:40,  4.37it/s]Loading trainS:  28%|██▊       | 70/247 [00:15<00:39,  4.44it/s]Loading trainS:  29%|██▊       | 71/247 [00:15<00:39,  4.49it/s]Loading trainS:  29%|██▉       | 72/247 [00:16<00:38,  4.53it/s]Loading trainS:  30%|██▉       | 73/247 [00:16<00:38,  4.57it/s]Loading trainS:  30%|██▉       | 74/247 [00:16<00:37,  4.59it/s]Loading trainS:  30%|███       | 75/247 [00:16<00:37,  4.60it/s]Loading trainS:  31%|███       | 76/247 [00:17<00:37,  4.59it/s]Loading trainS:  31%|███       | 77/247 [00:17<00:37,  4.55it/s]Loading trainS:  32%|███▏      | 78/247 [00:17<00:39,  4.27it/s]Loading trainS:  32%|███▏      | 79/247 [00:17<00:39,  4.25it/s]Loading trainS:  32%|███▏      | 80/247 [00:17<00:37,  4.45it/s]Loading trainS:  33%|███▎      | 81/247 [00:18<00:37,  4.43it/s]Loading trainS:  33%|███▎      | 82/247 [00:18<00:36,  4.50it/s]Loading trainS:  34%|███▎      | 83/247 [00:18<00:36,  4.55it/s]Loading trainS:  34%|███▍      | 84/247 [00:18<00:35,  4.59it/s]Loading trainS:  34%|███▍      | 85/247 [00:19<00:35,  4.61it/s]Loading trainS:  35%|███▍      | 86/247 [00:19<00:34,  4.64it/s]Loading trainS:  35%|███▌      | 87/247 [00:19<00:34,  4.68it/s]Loading trainS:  36%|███▌      | 88/247 [00:19<00:33,  4.69it/s]Loading trainS:  36%|███▌      | 89/247 [00:19<00:33,  4.68it/s]Loading trainS:  36%|███▋      | 90/247 [00:20<00:33,  4.67it/s]Loading trainS:  37%|███▋      | 91/247 [00:20<00:33,  4.65it/s]Loading trainS:  37%|███▋      | 92/247 [00:20<00:33,  4.58it/s]Loading trainS:  38%|███▊      | 93/247 [00:20<00:33,  4.59it/s]Loading trainS:  38%|███▊      | 94/247 [00:21<00:34,  4.45it/s]Loading trainS:  38%|███▊      | 95/247 [00:21<00:34,  4.43it/s]Loading trainS:  39%|███▉      | 96/247 [00:21<00:33,  4.47it/s]Loading trainS:  39%|███▉      | 97/247 [00:21<00:33,  4.51it/s]Loading trainS:  40%|███▉      | 98/247 [00:21<00:32,  4.52it/s]Loading trainS:  40%|████      | 99/247 [00:22<00:32,  4.55it/s]Loading trainS:  40%|████      | 100/247 [00:22<00:34,  4.30it/s]Loading trainS:  41%|████      | 101/247 [00:22<00:35,  4.10it/s]Loading trainS:  41%|████▏     | 102/247 [00:22<00:36,  4.01it/s]Loading trainS:  42%|████▏     | 103/247 [00:23<00:36,  3.97it/s]Loading trainS:  42%|████▏     | 104/247 [00:23<00:36,  3.94it/s]Loading trainS:  43%|████▎     | 105/247 [00:23<00:36,  3.91it/s]Loading trainS:  43%|████▎     | 106/247 [00:23<00:36,  3.90it/s]Loading trainS:  43%|████▎     | 107/247 [00:24<00:36,  3.88it/s]Loading trainS:  44%|████▎     | 108/247 [00:24<00:35,  3.87it/s]Loading trainS:  44%|████▍     | 109/247 [00:24<00:35,  3.85it/s]Loading trainS:  45%|████▍     | 110/247 [00:24<00:35,  3.82it/s]Loading trainS:  45%|████▍     | 111/247 [00:25<00:35,  3.78it/s]Loading trainS:  45%|████▌     | 112/247 [00:25<00:36,  3.71it/s]Loading trainS:  46%|████▌     | 113/247 [00:25<00:37,  3.59it/s]Loading trainS:  46%|████▌     | 114/247 [00:26<00:36,  3.67it/s]Loading trainS:  47%|████▋     | 115/247 [00:26<00:35,  3.72it/s]Loading trainS:  47%|████▋     | 116/247 [00:26<00:34,  3.77it/s]Loading trainS:  47%|████▋     | 117/247 [00:26<00:34,  3.81it/s]Loading trainS:  48%|████▊     | 118/247 [00:27<00:32,  3.96it/s]Loading trainS:  48%|████▊     | 119/247 [00:27<00:31,  4.08it/s]Loading trainS:  49%|████▊     | 120/247 [00:27<00:30,  4.17it/s]Loading trainS:  49%|████▉     | 121/247 [00:27<00:29,  4.23it/s]Loading trainS:  49%|████▉     | 122/247 [00:28<00:29,  4.24it/s]Loading trainS:  50%|████▉     | 123/247 [00:28<00:28,  4.30it/s]Loading trainS:  50%|█████     | 124/247 [00:28<00:28,  4.33it/s]Loading trainS:  51%|█████     | 125/247 [00:28<00:27,  4.37it/s]Loading trainS:  51%|█████     | 126/247 [00:28<00:27,  4.38it/s]Loading trainS:  51%|█████▏    | 127/247 [00:29<00:27,  4.38it/s]Loading trainS:  52%|█████▏    | 128/247 [00:29<00:27,  4.37it/s]Loading trainS:  52%|█████▏    | 129/247 [00:29<00:26,  4.39it/s]Loading trainS:  53%|█████▎    | 130/247 [00:29<00:26,  4.39it/s]Loading trainS:  53%|█████▎    | 131/247 [00:30<00:26,  4.35it/s]Loading trainS:  53%|█████▎    | 132/247 [00:30<00:27,  4.23it/s]Loading trainS:  54%|█████▍    | 133/247 [00:30<00:26,  4.29it/s]Loading trainS:  54%|█████▍    | 134/247 [00:30<00:26,  4.33it/s]Loading trainS:  55%|█████▍    | 135/247 [00:31<00:25,  4.36it/s]Loading trainS:  55%|█████▌    | 136/247 [00:31<00:24,  4.61it/s]Loading trainS:  55%|█████▌    | 137/247 [00:31<00:23,  4.77it/s]Loading trainS:  56%|█████▌    | 138/247 [00:31<00:22,  4.89it/s]Loading trainS:  56%|█████▋    | 139/247 [00:31<00:21,  4.97it/s]Loading trainS:  57%|█████▋    | 140/247 [00:31<00:21,  4.97it/s]Loading trainS:  57%|█████▋    | 141/247 [00:32<00:21,  5.01it/s]Loading trainS:  57%|█████▋    | 142/247 [00:32<00:20,  5.05it/s]Loading trainS:  58%|█████▊    | 143/247 [00:32<00:20,  5.04it/s]Loading trainS:  58%|█████▊    | 144/247 [00:32<00:20,  5.07it/s]Loading trainS:  59%|█████▊    | 145/247 [00:32<00:20,  5.09it/s]Loading trainS:  59%|█████▉    | 146/247 [00:33<00:19,  5.11it/s]Loading trainS:  60%|█████▉    | 147/247 [00:33<00:19,  5.13it/s]Loading trainS:  60%|█████▉    | 148/247 [00:33<00:19,  5.11it/s]Loading trainS:  60%|██████    | 149/247 [00:33<00:19,  5.11it/s]Loading trainS:  61%|██████    | 150/247 [00:33<00:18,  5.12it/s]Loading trainS:  61%|██████    | 151/247 [00:34<00:18,  5.11it/s]Loading trainS:  62%|██████▏   | 152/247 [00:34<00:18,  5.08it/s]Loading trainS:  62%|██████▏   | 153/247 [00:34<00:18,  5.07it/s]Loading trainS:  62%|██████▏   | 154/247 [00:34<00:18,  4.92it/s]Loading trainS:  63%|██████▎   | 155/247 [00:34<00:19,  4.78it/s]Loading trainS:  63%|██████▎   | 156/247 [00:35<00:19,  4.70it/s]Loading trainS:  64%|██████▎   | 157/247 [00:35<00:19,  4.66it/s]Loading trainS:  64%|██████▍   | 158/247 [00:35<00:19,  4.60it/s]Loading trainS:  64%|██████▍   | 159/247 [00:35<00:19,  4.59it/s]Loading trainS:  65%|██████▍   | 160/247 [00:36<00:19,  4.57it/s]Loading trainS:  65%|██████▌   | 161/247 [00:36<00:18,  4.54it/s]Loading trainS:  66%|██████▌   | 162/247 [00:36<00:18,  4.57it/s]Loading trainS:  66%|██████▌   | 163/247 [00:36<00:18,  4.62it/s]Loading trainS:  66%|██████▋   | 164/247 [00:36<00:17,  4.64it/s]Loading trainS:  67%|██████▋   | 165/247 [00:37<00:17,  4.65it/s]Loading trainS:  67%|██████▋   | 166/247 [00:37<00:17,  4.61it/s]Loading trainS:  68%|██████▊   | 167/247 [00:37<00:17,  4.49it/s]Loading trainS:  68%|██████▊   | 168/247 [00:37<00:17,  4.54it/s]Loading trainS:  68%|██████▊   | 169/247 [00:38<00:17,  4.57it/s]Loading trainS:  69%|██████▉   | 170/247 [00:38<00:16,  4.60it/s]Loading trainS:  69%|██████▉   | 171/247 [00:38<00:16,  4.62it/s]Loading trainS:  70%|██████▉   | 172/247 [00:38<00:16,  4.54it/s]Loading trainS:  70%|███████   | 173/247 [00:38<00:16,  4.58it/s]Loading trainS:  70%|███████   | 174/247 [00:39<00:16,  4.52it/s]Loading trainS:  71%|███████   | 175/247 [00:39<00:16,  4.34it/s]Loading trainS:  71%|███████▏  | 176/247 [00:39<00:15,  4.44it/s]Loading trainS:  72%|███████▏  | 177/247 [00:39<00:15,  4.52it/s]Loading trainS:  72%|███████▏  | 178/247 [00:40<00:15,  4.56it/s]Loading trainS:  72%|███████▏  | 179/247 [00:40<00:14,  4.60it/s]Loading trainS:  73%|███████▎  | 180/247 [00:40<00:14,  4.63it/s]Loading trainS:  73%|███████▎  | 181/247 [00:40<00:14,  4.66it/s]Loading trainS:  74%|███████▎  | 182/247 [00:40<00:13,  4.65it/s]Loading trainS:  74%|███████▍  | 183/247 [00:41<00:13,  4.66it/s]Loading trainS:  74%|███████▍  | 184/247 [00:41<00:13,  4.68it/s]Loading trainS:  75%|███████▍  | 185/247 [00:41<00:13,  4.67it/s]Loading trainS:  75%|███████▌  | 186/247 [00:41<00:13,  4.68it/s]Loading trainS:  76%|███████▌  | 187/247 [00:41<00:12,  4.68it/s]Loading trainS:  76%|███████▌  | 188/247 [00:42<00:12,  4.67it/s]Loading trainS:  77%|███████▋  | 189/247 [00:42<00:12,  4.68it/s]Loading trainS:  77%|███████▋  | 190/247 [00:42<00:12,  4.66it/s]Loading trainS:  77%|███████▋  | 191/247 [00:42<00:11,  4.68it/s]Loading trainS:  78%|███████▊  | 192/247 [00:43<00:11,  4.67it/s]Loading trainS:  78%|███████▊  | 193/247 [00:43<00:11,  4.67it/s]Loading trainS:  79%|███████▊  | 194/247 [00:43<00:11,  4.73it/s]Loading trainS:  79%|███████▉  | 195/247 [00:43<00:11,  4.72it/s]Loading trainS:  79%|███████▉  | 196/247 [00:43<00:10,  4.70it/s]Loading trainS:  80%|███████▉  | 197/247 [00:44<00:10,  4.73it/s]Loading trainS:  80%|████████  | 198/247 [00:44<00:10,  4.75it/s]Loading trainS:  81%|████████  | 199/247 [00:44<00:10,  4.72it/s]Loading trainS:  81%|████████  | 200/247 [00:44<00:09,  4.75it/s]Loading trainS:  81%|████████▏ | 201/247 [00:44<00:09,  4.69it/s]Loading trainS:  82%|████████▏ | 202/247 [00:45<00:09,  4.63it/s]Loading trainS:  82%|████████▏ | 203/247 [00:45<00:09,  4.45it/s]Loading trainS:  83%|████████▎ | 204/247 [00:45<00:09,  4.51it/s]Loading trainS:  83%|████████▎ | 205/247 [00:45<00:09,  4.58it/s]Loading trainS:  83%|████████▎ | 206/247 [00:46<00:08,  4.64it/s]Loading trainS:  84%|████████▍ | 207/247 [00:46<00:08,  4.68it/s]Loading trainS:  84%|████████▍ | 208/247 [00:46<00:08,  4.71it/s]Loading trainS:  85%|████████▍ | 209/247 [00:46<00:08,  4.74it/s]Loading trainS:  85%|████████▌ | 210/247 [00:46<00:07,  4.77it/s]Loading trainS:  85%|████████▌ | 211/247 [00:47<00:07,  4.78it/s]Loading trainS:  86%|████████▌ | 212/247 [00:47<00:07,  4.72it/s]Loading trainS:  86%|████████▌ | 213/247 [00:47<00:07,  4.68it/s]Loading trainS:  87%|████████▋ | 214/247 [00:47<00:07,  4.65it/s]Loading trainS:  87%|████████▋ | 215/247 [00:47<00:06,  4.63it/s]Loading trainS:  87%|████████▋ | 216/247 [00:48<00:06,  4.62it/s]Loading trainS:  88%|████████▊ | 217/247 [00:48<00:06,  4.58it/s]Loading trainS:  88%|████████▊ | 218/247 [00:48<00:06,  4.57it/s]Loading trainS:  89%|████████▊ | 219/247 [00:48<00:06,  4.56it/s]Loading trainS:  89%|████████▉ | 220/247 [00:49<00:05,  4.52it/s]Loading trainS:  89%|████████▉ | 221/247 [00:49<00:05,  4.52it/s]Loading trainS:  90%|████████▉ | 222/247 [00:49<00:05,  4.50it/s]Loading trainS:  90%|█████████ | 223/247 [00:49<00:05,  4.49it/s]Loading trainS:  91%|█████████ | 224/247 [00:49<00:05,  4.51it/s]Loading trainS:  91%|█████████ | 225/247 [00:50<00:04,  4.47it/s]Loading trainS:  91%|█████████▏| 226/247 [00:50<00:04,  4.44it/s]Loading trainS:  92%|█████████▏| 227/247 [00:50<00:04,  4.44it/s]Loading trainS:  92%|█████████▏| 228/247 [00:50<00:04,  4.41it/s]Loading trainS:  93%|█████████▎| 229/247 [00:51<00:04,  4.25it/s]Loading trainS:  93%|█████████▎| 230/247 [00:51<00:04,  4.22it/s]Loading trainS:  94%|█████████▎| 231/247 [00:51<00:03,  4.20it/s]Loading trainS:  94%|█████████▍| 232/247 [00:51<00:03,  4.19it/s]Loading trainS:  94%|█████████▍| 233/247 [00:52<00:03,  4.16it/s]Loading trainS:  95%|█████████▍| 234/247 [00:52<00:03,  4.17it/s]Loading trainS:  95%|█████████▌| 235/247 [00:52<00:02,  4.18it/s]Loading trainS:  96%|█████████▌| 236/247 [00:52<00:02,  4.19it/s]Loading trainS:  96%|█████████▌| 237/247 [00:53<00:02,  4.19it/s]Loading trainS:  96%|█████████▋| 238/247 [00:53<00:02,  4.19it/s]Loading trainS:  97%|█████████▋| 239/247 [00:53<00:01,  4.19it/s]Loading trainS:  97%|█████████▋| 240/247 [00:53<00:01,  4.20it/s]Loading trainS:  98%|█████████▊| 241/247 [00:53<00:01,  4.20it/s]Loading trainS:  98%|█████████▊| 242/247 [00:54<00:01,  4.20it/s]Loading trainS:  98%|█████████▊| 243/247 [00:54<00:01,  2.59it/s]Loading trainS:  99%|█████████▉| 244/247 [00:55<00:01,  2.55it/s]Loading trainS:  99%|█████████▉| 245/247 [00:56<00:00,  2.02it/s]Loading trainS: 100%|█████████▉| 246/247 [00:56<00:00,  1.86it/s]Loading trainS: 100%|██████████| 247/247 [00:57<00:00,  1.69it/s]Loading trainS: 100%|██████████| 247/247 [00:57<00:00,  4.30it/s]
Loading testS:   0%|          | 0/5 [00:00<?, ?it/s]Loading testS:  20%|██        | 1/5 [00:00<00:03,  1.32it/s]Loading testS:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]Loading testS:  60%|██████    | 3/5 [00:01<00:01,  1.50it/s]Loading testS:  80%|████████  | 4/5 [00:02<00:00,  1.53it/s]Loading testS: 100%|██████████| 5/5 [00:03<00:00,  1.48it/s]Loading testS: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               2020-01-22 07:42:24.945556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 07:42:24.945641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 07:42:24.945655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 07:42:24.945662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 07:42:24.947078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97300229 0.02699771]
Train on 15751 samples, validate on 333 samples
Epoch 1/300
 - 45s - loss: 0.2667 - acc: 0.9759 - mDice: 0.4794 - val_loss: 0.3314 - val_acc: 0.9841 - val_mDice: 0.3423

Epoch 00001: val_mDice improved from -inf to 0.34227, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 42s - loss: 0.1326 - acc: 0.9861 - mDice: 0.7422 - val_loss: 0.3098 - val_acc: 0.9871 - val_mDice: 0.3847

Epoch 00002: val_mDice improved from 0.34227 to 0.38473, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 41s - loss: 0.1121 - acc: 0.9882 - mDice: 0.7820 - val_loss: 0.2983 - val_acc: 0.9888 - val_mDice: 0.4018

Epoch 00003: val_mDice improved from 0.38473 to 0.40184, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300
 - 41s - loss: 0.1025 - acc: 0.9892 - mDice: 0.8007 - val_loss: 0.2949 - val_acc: 0.9898 - val_mDice: 0.4101

Epoch 00004: val_mDice improved from 0.40184 to 0.41009, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300
 - 41s - loss: 0.0977 - acc: 0.9898 - mDice: 0.8098 - val_loss: 0.2924 - val_acc: 0.9902 - val_mDice: 0.4156

Epoch 00005: val_mDice improved from 0.41009 to 0.41562, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300
 - 41s - loss: 0.0915 - acc: 0.9903 - mDice: 0.8220 - val_loss: 0.2861 - val_acc: 0.9909 - val_mDice: 0.4172

Epoch 00006: val_mDice improved from 0.41562 to 0.41723, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/300
 - 41s - loss: 0.0883 - acc: 0.9907 - mDice: 0.8282 - val_loss: 0.2787 - val_acc: 0.9909 - val_mDice: 0.4228

Epoch 00007: val_mDice improved from 0.41723 to 0.42280, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/300
 - 41s - loss: 0.0861 - acc: 0.9910 - mDice: 0.8326 - val_loss: 0.2806 - val_acc: 0.9909 - val_mDice: 0.4247

Epoch 00008: val_mDice improved from 0.42280 to 0.42471, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 9/300
 - 40s - loss: 0.0849 - acc: 0.9912 - mDice: 0.8348 - val_loss: 0.2747 - val_acc: 0.9910 - val_mDice: 0.4254

Epoch 00009: val_mDice improved from 0.42471 to 0.42537, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 10/300
 - 45s - loss: 0.0826 - acc: 0.9914 - mDice: 0.8392 - val_loss: 0.2814 - val_acc: 0.9903 - val_mDice: 0.4243

Epoch 00010: val_mDice did not improve from 0.42537
Epoch 11/300
 - 42s - loss: 0.0807 - acc: 0.9916 - mDice: 0.8429 - val_loss: 0.2824 - val_acc: 0.9914 - val_mDice: 0.4292

Epoch 00011: val_mDice improved from 0.42537 to 0.42923, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 12/300
 - 40s - loss: 0.0777 - acc: 0.9918 - mDice: 0.8488 - val_loss: 0.2817 - val_acc: 0.9911 - val_mDice: 0.4281

Epoch 00012: val_mDice did not improve from 0.42923
Epoch 13/300
 - 41s - loss: 0.0767 - acc: 0.9919 - mDice: 0.8507 - val_loss: 0.2821 - val_acc: 0.9910 - val_mDice: 0.4293

Epoch 00013: val_mDice improved from 0.42923 to 0.42927, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 14/300
 - 41s - loss: 0.0751 - acc: 0.9920 - mDice: 0.8539 - val_loss: 0.2773 - val_acc: 0.9914 - val_mDice: 0.4321

Epoch 00014: val_mDice improved from 0.42927 to 0.43210, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 15/300
 - 41s - loss: 0.0759 - acc: 0.9921 - mDice: 0.8522 - val_loss: 0.2516 - val_acc: 0.9918 - val_mDice: 0.4334

Epoch 00015: val_mDice improved from 0.43210 to 0.43343, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 16/300
 - 41s - loss: 0.0741 - acc: 0.9923 - mDice: 0.8557 - val_loss: 0.2438 - val_acc: 0.9912 - val_mDice: 0.4313

Epoch 00016: val_mDice did not improve from 0.43343
Epoch 17/300
 - 45s - loss: 0.0715 - acc: 0.9924 - mDice: 0.8610 - val_loss: 0.2498 - val_acc: 0.9919 - val_mDice: 0.4362

Epoch 00017: val_mDice improved from 0.43343 to 0.43621, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 18/300
 - 43s - loss: 0.0709 - acc: 0.9925 - mDice: 0.8620 - val_loss: 0.2773 - val_acc: 0.9915 - val_mDice: 0.4357

Epoch 00018: val_mDice did not improve from 0.43621
Epoch 19/300
 - 42s - loss: 0.0698 - acc: 0.9926 - mDice: 0.8643 - val_loss: 0.2671 - val_acc: 0.9916 - val_mDice: 0.4365

Epoch 00019: val_mDice improved from 0.43621 to 0.43651, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 20/300
 - 42s - loss: 0.0701 - acc: 0.9926 - mDice: 0.8635 - val_loss: 0.2242 - val_acc: 0.9922 - val_mDice: 0.4384

Epoch 00020: val_mDice improved from 0.43651 to 0.43843, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 21/300
 - 41s - loss: 0.0713 - acc: 0.9927 - mDice: 0.8612 - val_loss: 0.2100 - val_acc: 0.9921 - val_mDice: 0.4415

Epoch 00021: val_mDice improved from 0.43843 to 0.44151, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 22/300
 - 40s - loss: 0.0674 - acc: 0.9928 - mDice: 0.8689 - val_loss: 0.2056 - val_acc: 0.9920 - val_mDice: 0.4423

Epoch 00022: val_mDice improved from 0.44151 to 0.44226, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 23/300
 - 41s - loss: 0.0670 - acc: 0.9929 - mDice: 0.8696 - val_loss: 0.2044 - val_acc: 0.9919 - val_mDice: 0.4407

Epoch 00023: val_mDice did not improve from 0.44226
Epoch 24/300
 - 41s - loss: 0.0670 - acc: 0.9929 - mDice: 0.8697 - val_loss: 0.1934 - val_acc: 0.9920 - val_mDice: 0.4432

Epoch 00024: val_mDice improved from 0.44226 to 0.44321, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 25/300
 - 41s - loss: 0.0676 - acc: 0.9930 - mDice: 0.8683 - val_loss: 0.1796 - val_acc: 0.9920 - val_mDice: 0.4426

Epoch 00025: val_mDice did not improve from 0.44321
Epoch 26/300
 - 41s - loss: 0.0662 - acc: 0.9930 - mDice: 0.8712 - val_loss: 0.1730 - val_acc: 0.9923 - val_mDice: 0.4429

Epoch 00026: val_mDice did not improve from 0.44321
Epoch 27/300
 - 40s - loss: 0.0657 - acc: 0.9931 - mDice: 0.8722 - val_loss: 0.1587 - val_acc: 0.9922 - val_mDice: 0.4435

Epoch 00027: val_mDice improved from 0.44321 to 0.44347, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 28/300
 - 41s - loss: 0.0639 - acc: 0.9931 - mDice: 0.8757 - val_loss: 0.1652 - val_acc: 0.9922 - val_mDice: 0.4440

Epoch 00028: val_mDice improved from 0.44347 to 0.44400, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 29/300
 - 40s - loss: 0.0649 - acc: 0.9932 - mDice: 0.8737 - val_loss: 0.1818 - val_acc: 0.9923 - val_mDice: 0.4440

Epoch 00029: val_mDice improved from 0.44400 to 0.44404, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 30/300
 - 40s - loss: 0.0645 - acc: 0.9933 - mDice: 0.8746 - val_loss: 0.1489 - val_acc: 0.9921 - val_mDice: 0.4431

Epoch 00030: val_mDice did not improve from 0.44404
Epoch 31/300
 - 41s - loss: 0.0648 - acc: 0.9933 - mDice: 0.8738 - val_loss: 0.1464 - val_acc: 0.9918 - val_mDice: 0.4417

Epoch 00031: val_mDice did not improve from 0.44404
Epoch 32/300
 - 40s - loss: 0.0626 - acc: 0.9934 - mDice: 0.8782 - val_loss: 0.1167 - val_acc: 0.9919 - val_mDice: 0.4468

Epoch 00032: val_mDice improved from 0.44404 to 0.44679, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 33/300
 - 41s - loss: 0.0632 - acc: 0.9933 - mDice: 0.8769 - val_loss: 0.1274 - val_acc: 0.9923 - val_mDice: 0.4462

Epoch 00033: val_mDice did not improve from 0.44679
Epoch 34/300
 - 40s - loss: 0.0635 - acc: 0.9934 - mDice: 0.8764 - val_loss: 0.1448 - val_acc: 0.9923 - val_mDice: 0.4500

Epoch 00034: val_mDice improved from 0.44679 to 0.44998, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 35/300
 - 41s - loss: 0.0634 - acc: 0.9934 - mDice: 0.8765 - val_loss: 0.1393 - val_acc: 0.9920 - val_mDice: 0.4455

Epoch 00035: val_mDice did not improve from 0.44998
Epoch 36/300
 - 41s - loss: 0.0618 - acc: 0.9935 - mDice: 0.8798 - val_loss: 0.1420 - val_acc: 0.9923 - val_mDice: 0.4466

Epoch 00036: val_mDice did not improve from 0.44998
Epoch 37/300
 - 41s - loss: 0.0622 - acc: 0.9935 - mDice: 0.8790 - val_loss: 0.1173 - val_acc: 0.9921 - val_mDice: 0.4451

Epoch 00037: val_mDice did not improve from 0.44998
Epoch 38/300
 - 40s - loss: 0.0610 - acc: 0.9936 - mDice: 0.8813 - val_loss: 0.1118 - val_acc: 0.9924 - val_mDice: 0.4494

Epoch 00038: val_mDice did not improve from 0.44998
Epoch 39/300
 - 41s - loss: 0.0604 - acc: 0.9936 - mDice: 0.8825 - val_loss: 0.1032 - val_acc: 0.9923 - val_mDice: 0.4485

Epoch 00039: val_mDice did not improve from 0.44998
Epoch 40/300
 - 40s - loss: 0.0620 - acc: 0.9936 - mDice: 0.8792 - val_loss: 0.1132 - val_acc: 0.9920 - val_mDice: 0.4467

Epoch 00040: val_mDice did not improve from 0.44998
Epoch 41/300
 - 41s - loss: 0.0608 - acc: 0.9936 - mDice: 0.8817 - val_loss: 0.1056 - val_acc: 0.9924 - val_mDice: 0.4498

Epoch 00041: val_mDice did not improve from 0.44998
Epoch 42/300
 - 41s - loss: 0.0602 - acc: 0.9936 - mDice: 0.8829 - val_loss: 0.1316 - val_acc: 0.9920 - val_mDice: 0.4485

Epoch 00042: val_mDice did not improve from 0.44998
Epoch 43/300
 - 40s - loss: 0.0612 - acc: 0.9937 - mDice: 0.8809 - val_loss: 0.0954 - val_acc: 0.9923 - val_mDice: 0.4494

Epoch 00043: val_mDice did not improve from 0.44998
Epoch 44/300
 - 40s - loss: 0.0586 - acc: 0.9937 - mDice: 0.8860 - val_loss: 0.1029 - val_acc: 0.9921 - val_mDice: 0.4506

Epoch 00044: val_mDice improved from 0.44998 to 0.45064, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 45/300
 - 40s - loss: 0.0584 - acc: 0.9938 - mDice: 0.8863 - val_loss: 0.0973 - val_acc: 0.9923 - val_mDice: 0.4481

Epoch 00045: val_mDice did not improve from 0.45064
Epoch 46/300
 - 40s - loss: 0.0598 - acc: 0.9938 - mDice: 0.8836 - val_loss: 0.0753 - val_acc: 0.9924 - val_mDice: 0.4492

Epoch 00046: val_mDice did not improve from 0.45064
Epoch 47/300
 - 40s - loss: 0.0585 - acc: 0.9938 - mDice: 0.8861 - val_loss: 0.0937 - val_acc: 0.9924 - val_mDice: 0.4493

Epoch 00047: val_mDice did not improve from 0.45064
Epoch 48/300
 - 40s - loss: 0.0569 - acc: 0.9939 - mDice: 0.8893 - val_loss: 0.0905 - val_acc: 0.9925 - val_mDice: 0.4506

Epoch 00048: val_mDice did not improve from 0.45064
Epoch 49/300
 - 40s - loss: 0.0582 - acc: 0.9939 - mDice: 0.8867 - val_loss: 0.0957 - val_acc: 0.9926 - val_mDice: 0.4534

Epoch 00049: val_mDice improved from 0.45064 to 0.45343, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 50/300
 - 41s - loss: 0.0572 - acc: 0.9939 - mDice: 0.8887 - val_loss: 0.0654 - val_acc: 0.9925 - val_mDice: 0.4545

Epoch 00050: val_mDice improved from 0.45343 to 0.45455, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 51/300
 - 41s - loss: 0.0590 - acc: 0.9939 - mDice: 0.8852 - val_loss: 0.0663 - val_acc: 0.9926 - val_mDice: 0.4527

Epoch 00051: val_mDice did not improve from 0.45455
Epoch 52/300
 - 41s - loss: 0.0567 - acc: 0.9939 - mDice: 0.8898 - val_loss: 0.0571 - val_acc: 0.9928 - val_mDice: 0.4626

Epoch 00052: val_mDice improved from 0.45455 to 0.46261, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 53/300
 - 41s - loss: 0.0563 - acc: 0.9939 - mDice: 0.8906 - val_loss: 0.0572 - val_acc: 0.9926 - val_mDice: 0.4590

Epoch 00053: val_mDice did not improve from 0.46261
Epoch 54/300
 - 41s - loss: 0.0567 - acc: 0.9940 - mDice: 0.8897 - val_loss: 0.0573 - val_acc: 0.9926 - val_mDice: 0.4589

Epoch 00054: val_mDice did not improve from 0.46261
Epoch 55/300
 - 42s - loss: 0.0558 - acc: 0.9940 - mDice: 0.8914 - val_loss: 0.0590 - val_acc: 0.9924 - val_mDice: 0.4571

Epoch 00055: val_mDice did not improve from 0.46261
Epoch 56/300
 - 40s - loss: 0.0557 - acc: 0.9940 - mDice: 0.8917 - val_loss: 0.0592 - val_acc: 0.9925 - val_mDice: 0.4594

Epoch 00056: val_mDice did not improve from 0.46261
Epoch 57/300
 - 42s - loss: 0.0543 - acc: 0.9941 - mDice: 0.8945 - val_loss: 0.0577 - val_acc: 0.9926 - val_mDice: 0.4560

Epoch 00057: val_mDice did not improve from 0.46261
Epoch 58/300
 - 42s - loss: 0.0555 - acc: 0.9941 - mDice: 0.8920 - val_loss: 0.0592 - val_acc: 0.9924 - val_mDice: 0.4603

Epoch 00058: val_mDice did not improve from 0.46261
Epoch 59/300
 - 43s - loss: 0.0566 - acc: 0.9941 - mDice: 0.8898 - val_loss: 0.0544 - val_acc: 0.9925 - val_mDice: 0.4638

Epoch 00059: val_mDice improved from 0.46261 to 0.46378, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 60/300
 - 43s - loss: 0.0546 - acc: 0.9941 - mDice: 0.8938 - val_loss: 0.0597 - val_acc: 0.9923 - val_mDice: 0.4598

Epoch 00060: val_mDice did not improve from 0.46378
Epoch 61/300
 - 41s - loss: 0.0545 - acc: 0.9941 - mDice: 0.8939 - val_loss: 0.0584 - val_acc: 0.9926 - val_mDice: 0.4615

Epoch 00061: val_mDice did not improve from 0.46378
Epoch 62/300
 - 41s - loss: 0.0540 - acc: 0.9942 - mDice: 0.8951 - val_loss: 0.0585 - val_acc: 0.9922 - val_mDice: 0.4637

Epoch 00062: val_mDice did not improve from 0.46378
Epoch 63/300
 - 41s - loss: 0.0550 - acc: 0.9942 - mDice: 0.8930 - val_loss: 0.0577 - val_acc: 0.9924 - val_mDice: 0.4628

Epoch 00063: val_mDice did not improve from 0.46378
Epoch 64/300
 - 40s - loss: 0.0537 - acc: 0.9942 - mDice: 0.8956 - val_loss: 0.0533 - val_acc: 0.9924 - val_mDice: 0.4675

Epoch 00064: val_mDice improved from 0.46378 to 0.46754, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 65/300
 - 41s - loss: 0.0542 - acc: 0.9942 - mDice: 0.8946 - val_loss: 0.0532 - val_acc: 0.9924 - val_mDice: 0.4633

Epoch 00065: val_mDice did not improve from 0.46754
Epoch 66/300
 - 41s - loss: 0.0535 - acc: 0.9942 - mDice: 0.8959 - val_loss: 0.0546 - val_acc: 0.9924 - val_mDice: 0.4622

Epoch 00066: val_mDice did not improve from 0.46754
Epoch 67/300
 - 41s - loss: 0.0534 - acc: 0.9942 - mDice: 0.8961 - val_loss: 0.0812 - val_acc: 0.9926 - val_mDice: 0.4701

Epoch 00067: val_mDice improved from 0.46754 to 0.47007, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 68/300
 - 41s - loss: 0.0532 - acc: 0.9942 - mDice: 0.8966 - val_loss: 0.0531 - val_acc: 0.9925 - val_mDice: 0.4656

Epoch 00068: val_mDice did not improve from 0.47007
Epoch 69/300
 - 41s - loss: 0.0528 - acc: 0.9943 - mDice: 0.8974 - val_loss: 0.0511 - val_acc: 0.9925 - val_mDice: 0.4662

Epoch 00069: val_mDice did not improve from 0.47007
Epoch 70/300
 - 40s - loss: 0.0532 - acc: 0.9943 - mDice: 0.8966 - val_loss: 0.0520 - val_acc: 0.9925 - val_mDice: 0.4673

Epoch 00070: val_mDice did not improve from 0.47007
Epoch 71/300
 - 41s - loss: 0.0526 - acc: 0.9943 - mDice: 0.8977 - val_loss: 0.0539 - val_acc: 0.9928 - val_mDice: 0.4657

Epoch 00071: val_mDice did not improve from 0.47007
Epoch 72/300
 - 40s - loss: 0.0518 - acc: 0.9943 - mDice: 0.8993 - val_loss: 0.0527 - val_acc: 0.9925 - val_mDice: 0.4699

Epoch 00072: val_mDice did not improve from 0.47007
Epoch 73/300
 - 41s - loss: 0.0520 - acc: 0.9944 - mDice: 0.8989 - val_loss: 0.0516 - val_acc: 0.9930 - val_mDice: 0.4722

Epoch 00073: val_mDice improved from 0.47007 to 0.47216, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 74/300
 - 41s - loss: 0.0520 - acc: 0.9944 - mDice: 0.8989 - val_loss: 0.0555 - val_acc: 0.9922 - val_mDice: 0.4675

Epoch 00074: val_mDice did not improve from 0.47216
Epoch 75/300
 - 41s - loss: 0.0523 - acc: 0.9944 - mDice: 0.8982 - val_loss: 0.0593 - val_acc: 0.9927 - val_mDice: 0.4633

Epoch 00075: val_mDice did not improve from 0.47216
Epoch 76/300
 - 41s - loss: 0.0515 - acc: 0.9944 - mDice: 0.8999 - val_loss: 0.0567 - val_acc: 0.9926 - val_mDice: 0.4689

Epoch 00076: val_mDice did not improve from 0.47216
Epoch 77/300
 - 41s - loss: 0.0503 - acc: 0.9944 - mDice: 0.9023 - val_loss: 0.0547 - val_acc: 0.9929 - val_mDice: 0.4680

Epoch 00077: val_mDice did not improve from 0.47216
Epoch 78/300
 - 41s - loss: 0.0511 - acc: 0.9944 - mDice: 0.9006 - val_loss: 0.0546 - val_acc: 0.9925 - val_mDice: 0.4707

Epoch 00078: val_mDice did not improve from 0.47216
Epoch 79/300
 - 41s - loss: 0.0501 - acc: 0.9944 - mDice: 0.9027 - val_loss: 0.0571 - val_acc: 0.9922 - val_mDice: 0.4688

Epoch 00079: val_mDice did not improve from 0.47216
Epoch 80/300
 - 41s - loss: 0.0506 - acc: 0.9944 - mDice: 0.9016 - val_loss: 0.0562 - val_acc: 0.9927 - val_mDice: 0.4714

Epoch 00080: val_mDice did not improve from 0.47216
Epoch 81/300
 - 41s - loss: 0.0509 - acc: 0.9945 - mDice: 0.9010 - val_loss: 0.0562 - val_acc: 0.9926 - val_mDice: 0.4703

Epoch 00081: val_mDice did not improve from 0.47216
Epoch 82/300
 - 42s - loss: 0.0508 - acc: 0.9945 - mDice: 0.9012 - val_loss: 0.0539 - val_acc: 0.9927 - val_mDice: 0.4729

Epoch 00082: val_mDice improved from 0.47216 to 0.47293, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 83/300
 - 42s - loss: 0.0501 - acc: 0.9945 - mDice: 0.9025 - val_loss: 0.0570 - val_acc: 0.9927 - val_mDice: 0.4717

Epoch 00083: val_mDice did not improve from 0.47293
Epoch 84/300
 - 42s - loss: 0.0501 - acc: 0.9945 - mDice: 0.9025 - val_loss: 0.0560 - val_acc: 0.9927 - val_mDice: 0.4709

Epoch 00084: val_mDice did not improve from 0.47293
Epoch 85/300
 - 43s - loss: 0.0495 - acc: 0.9945 - mDice: 0.9038 - val_loss: 0.0508 - val_acc: 0.9927 - val_mDice: 0.4736

Epoch 00085: val_mDice improved from 0.47293 to 0.47357, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 86/300
 - 41s - loss: 0.0511 - acc: 0.9945 - mDice: 0.9007 - val_loss: 0.0534 - val_acc: 0.9926 - val_mDice: 0.4717

Epoch 00086: val_mDice did not improve from 0.47357
Epoch 87/300
 - 41s - loss: 0.0493 - acc: 0.9946 - mDice: 0.9041 - val_loss: 0.0520 - val_acc: 0.9925 - val_mDice: 0.4714

Epoch 00087: val_mDice did not improve from 0.47357
Epoch 88/300
 - 41s - loss: 0.0494 - acc: 0.9946 - mDice: 0.9040 - val_loss: 0.0507 - val_acc: 0.9929 - val_mDice: 0.4751

Epoch 00088: val_mDice improved from 0.47357 to 0.47506, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 89/300
 - 41s - loss: 0.0490 - acc: 0.9946 - mDice: 0.9048 - val_loss: 0.0499 - val_acc: 0.9927 - val_mDice: 0.4736

Epoch 00089: val_mDice did not improve from 0.47506
Epoch 90/300
 - 41s - loss: 0.0488 - acc: 0.9946 - mDice: 0.9052 - val_loss: 0.0532 - val_acc: 0.9927 - val_mDice: 0.4723

Epoch 00090: val_mDice did not improve from 0.47506
Epoch 91/300
 - 41s - loss: 0.0494 - acc: 0.9946 - mDice: 0.9040 - val_loss: 0.0520 - val_acc: 0.9928 - val_mDice: 0.4785

Epoch 00091: val_mDice improved from 0.47506 to 0.47852, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 92/300
 - 40s - loss: 0.0490 - acc: 0.9946 - mDice: 0.9047 - val_loss: 0.0531 - val_acc: 0.9927 - val_mDice: 0.4764

Epoch 00092: val_mDice did not improve from 0.47852
Epoch 93/300
 - 40s - loss: 0.0493 - acc: 0.9946 - mDice: 0.9041 - val_loss: 0.0525 - val_acc: 0.9928 - val_mDice: 0.4758

Epoch 00093: val_mDice did not improve from 0.47852
Epoch 94/300
 - 40s - loss: 0.0486 - acc: 0.9946 - mDice: 0.9056 - val_loss: 0.0535 - val_acc: 0.9926 - val_mDice: 0.4756

Epoch 00094: val_mDice did not improve from 0.47852
Epoch 95/300
 - 40s - loss: 0.0493 - acc: 0.9947 - mDice: 0.9041 - val_loss: 0.0501 - val_acc: 0.9928 - val_mDice: 0.4783

Epoch 00095: val_mDice did not improve from 0.47852
Epoch 96/300
 - 41s - loss: 0.0479 - acc: 0.9947 - mDice: 0.9068 - val_loss: 0.0550 - val_acc: 0.9925 - val_mDice: 0.4699

Epoch 00096: val_mDice did not improve from 0.47852
Epoch 97/300
 - 40s - loss: 0.0481 - acc: 0.9947 - mDice: 0.9065 - val_loss: 0.0537 - val_acc: 0.9928 - val_mDice: 0.4693

Epoch 00097: val_mDice did not improve from 0.47852
Epoch 98/300
 - 41s - loss: 0.0492 - acc: 0.9947 - mDice: 0.9044 - val_loss: 0.0519 - val_acc: 0.9928 - val_mDice: 0.4732

Epoch 00098: val_mDice did not improve from 0.47852
Epoch 99/300
 - 40s - loss: 0.0475 - acc: 0.9947 - mDice: 0.9077 - val_loss: 0.0489 - val_acc: 0.9926 - val_mDice: 0.4771

Epoch 00099: val_mDice did not improve from 0.47852
Epoch 100/300
 - 40s - loss: 0.0467 - acc: 0.9947 - mDice: 0.9093 - val_loss: 0.0535 - val_acc: 0.9929 - val_mDice: 0.4752

Epoch 00100: val_mDice did not improve from 0.47852
Epoch 101/300
 - 40s - loss: 0.0486 - acc: 0.9947 - mDice: 0.9055 - val_loss: 0.0512 - val_acc: 0.9927 - val_mDice: 0.4751

Epoch 00101: val_mDice did not improve from 0.47852
Epoch 102/300
 - 40s - loss: 0.0478 - acc: 0.9947 - mDice: 0.9070 - val_loss: 0.0513 - val_acc: 0.9928 - val_mDice: 0.4762

Epoch 00102: val_mDice did not improve from 0.47852
Epoch 103/300
 - 40s - loss: 0.0476 - acc: 0.9947 - mDice: 0.9076 - val_loss: 0.0559 - val_acc: 0.9927 - val_mDice: 0.4732

Epoch 00103: val_mDice did not improve from 0.47852
Epoch 104/300
 - 41s - loss: 0.0475 - acc: 0.9947 - mDice: 0.9077 - val_loss: 0.0587 - val_acc: 0.9923 - val_mDice: 0.4717

Epoch 00104: val_mDice did not improve from 0.47852
Epoch 105/300
 - 40s - loss: 0.0472 - acc: 0.9947 - mDice: 0.9082 - val_loss: 0.0544 - val_acc: 0.9924 - val_mDice: 0.4750

Epoch 00105: val_mDice did not improve from 0.47852
Epoch 106/300
 - 40s - loss: 0.0474 - acc: 0.9948 - mDice: 0.9079 - val_loss: 0.0578 - val_acc: 0.9925 - val_mDice: 0.4730

Epoch 00106: val_mDice did not improve from 0.47852

Epoch 00106: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 107/300
 - 40s - loss: 0.0463 - acc: 0.9948 - mDice: 0.9100 - val_loss: 0.0584 - val_acc: 0.9926 - val_mDice: 0.4729

Epoch 00107: val_mDice did not improve from 0.47852
Epoch 108/300
 - 40s - loss: 0.0469 - acc: 0.9948 - mDice: 0.9088 - val_loss: 0.0536 - val_acc: 0.9929 - val_mDice: 0.4794

Epoch 00108: val_mDice improved from 0.47852 to 0.47943, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 109/300
 - 40s - loss: 0.0469 - acc: 0.9948 - mDice: 0.9088 - val_loss: 0.0566 - val_acc: 0.9926 - val_mDice: 0.4709

Epoch 00109: val_mDice did not improve from 0.47943
Epoch 110/300
 - 40s - loss: 0.0465 - acc: 0.9948 - mDice: 0.9096 - val_loss: 0.0568 - val_acc: 0.9927 - val_mDice: 0.4738

Epoch 00110: val_mDice did not improve from 0.47943
Epoch 111/300
 - 41s - loss: 0.0462 - acc: 0.9949 - mDice: 0.9101 - val_loss: 0.0606 - val_acc: 0.9925 - val_mDice: 0.4697

Epoch 00111: val_mDice did not improve from 0.47943
Epoch 112/300
 - 41s - loss: 0.0470 - acc: 0.9948 - mDice: 0.9086 - val_loss: 0.0553 - val_acc: 0.9929 - val_mDice: 0.4784

Epoch 00112: val_mDice did not improve from 0.47943
Epoch 113/300
 - 40s - loss: 0.0458 - acc: 0.9949 - mDice: 0.9110 - val_loss: 0.0571 - val_acc: 0.9928 - val_mDice: 0.4767

Epoch 00113: val_mDice did not improve from 0.47943
Epoch 114/300
 - 41s - loss: 0.0473 - acc: 0.9948 - mDice: 0.9079 - val_loss: 0.0570 - val_acc: 0.9926 - val_mDice: 0.4755

Epoch 00114: val_mDice did not improve from 0.47943
Epoch 115/300
 - 41s - loss: 0.0465 - acc: 0.9949 - mDice: 0.9096 - val_loss: 0.0572 - val_acc: 0.9929 - val_mDice: 0.4752

Epoch 00115: val_mDice did not improve from 0.47943
Epoch 116/300
 - 40s - loss: 0.0461 - acc: 0.9949 - mDice: 0.9103 - val_loss: 0.0537 - val_acc: 0.9927 - val_mDice: 0.4757

Epoch 00116: val_mDice did not improve from 0.47943
Epoch 117/300
 - 41s - loss: 0.0464 - acc: 0.9948 - mDice: 0.9099 - val_loss: 0.0566 - val_acc: 0.9926 - val_mDice: 0.4778

Epoch 00117: val_mDice did not improve from 0.47943
Epoch 118/300
 - 41s - loss: 0.0473 - acc: 0.9949 - mDice: 0.9081 - val_loss: 0.0553 - val_acc: 0.9929 - val_mDice: 0.4794

Epoch 00118: val_mDice improved from 0.47943 to 0.47944, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 119/300
 - 41s - loss: 0.0460 - acc: 0.9949 - mDice: 0.9106 - val_loss: 0.0576 - val_acc: 0.9927 - val_mDice: 0.4772

Epoch 00119: val_mDice did not improve from 0.47944
Epoch 120/300
 - 41s - loss: 0.0458 - acc: 0.9949 - mDice: 0.9111 - val_loss: 0.0568 - val_acc: 0.9927 - val_mDice: 0.4766

Epoch 00120: val_mDice did not improve from 0.47944
Epoch 121/300
 - 41s - loss: 0.0457 - acc: 0.9949 - mDice: 0.9111 - val_loss: 0.0562 - val_acc: 0.9928 - val_mDice: 0.4784

Epoch 00121: val_mDice did not improve from 0.47944

Epoch 00121: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 122/300
 - 40s - loss: 0.0460 - acc: 0.9949 - mDice: 0.9105 - val_loss: 0.0556 - val_acc: 0.9928 - val_mDice: 0.4802

Epoch 00122: val_mDice improved from 0.47944 to 0.48019, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 123/300
 - 41s - loss: 0.0456 - acc: 0.9949 - mDice: 0.9114 - val_loss: 0.0554 - val_acc: 0.9927 - val_mDice: 0.4817

Epoch 00123: val_mDice improved from 0.48019 to 0.48175, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 124/300
 - 40s - loss: 0.0454 - acc: 0.9949 - mDice: 0.9118 - val_loss: 0.0559 - val_acc: 0.9928 - val_mDice: 0.4787

Epoch 00124: val_mDice did not improve from 0.48175
Epoch 125/300
 - 40s - loss: 0.0479 - acc: 0.9949 - mDice: 0.9069 - val_loss: 0.0539 - val_acc: 0.9928 - val_mDice: 0.4798

Epoch 00125: val_mDice did not improve from 0.48175
Epoch 126/300
 - 41s - loss: 0.0459 - acc: 0.9949 - mDice: 0.9109 - val_loss: 0.0559 - val_acc: 0.9927 - val_mDice: 0.4777

Epoch 00126: val_mDice did not improve from 0.48175
Epoch 127/300
 - 41s - loss: 0.0453 - acc: 0.9949 - mDice: 0.9120 - val_loss: 0.0574 - val_acc: 0.9927 - val_mDice: 0.4761

Epoch 00127: val_mDice did not improve from 0.48175
Epoch 128/300
 - 41s - loss: 0.0458 - acc: 0.9949 - mDice: 0.9110 - val_loss: 0.0553 - val_acc: 0.9927 - val_mDice: 0.4781

Epoch 00128: val_mDice did not improve from 0.48175
Epoch 129/300
 - 40s - loss: 0.0454 - acc: 0.9949 - mDice: 0.9117 - val_loss: 0.0575 - val_acc: 0.9927 - val_mDice: 0.4781

Epoch 00129: val_mDice did not improve from 0.48175
Epoch 130/300
 - 41s - loss: 0.0458 - acc: 0.9949 - mDice: 0.9109 - val_loss: 0.0597 - val_acc: 0.9926 - val_mDice: 0.4779

Epoch 00130: val_mDice did not improve from 0.48175
Epoch 131/300
 - 41s - loss: 0.0459 - acc: 0.9950 - mDice: 0.9107 - val_loss: 0.0599 - val_acc: 0.9927 - val_mDice: 0.4781

Epoch 00131: val_mDice did not improve from 0.48175
Epoch 132/300
 - 40s - loss: 0.0449 - acc: 0.9949 - mDice: 0.9127 - val_loss: 0.0569 - val_acc: 0.9928 - val_mDice: 0.4793

Epoch 00132: val_mDice did not improve from 0.48175
Epoch 133/300
 - 40s - loss: 0.0446 - acc: 0.9949 - mDice: 0.9134 - val_loss: 0.0565 - val_acc: 0.9928 - val_mDice: 0.4796

Epoch 00133: val_mDice did not improve from 0.48175
Epoch 134/300
 - 41s - loss: 0.0459 - acc: 0.9950 - mDice: 0.9108 - val_loss: 0.0561 - val_acc: 0.9928 - val_mDice: 0.4789

Epoch 00134: val_mDice did not improve from 0.48175
Epoch 135/300
 - 40s - loss: 0.0457 - acc: 0.9949 - mDice: 0.9111 - val_loss: 0.0587 - val_acc: 0.9927 - val_mDice: 0.4786

Epoch 00135: val_mDice did not improve from 0.48175
Epoch 136/300
 - 41s - loss: 0.0453 - acc: 0.9949 - mDice: 0.9119 - val_loss: 0.0577 - val_acc: 0.9927 - val_mDice: 0.4792

Epoch 00136: val_mDice did not improve from 0.48175
Epoch 137/300
 - 40s - loss: 0.0458 - acc: 0.9950 - mDice: 0.9109 - val_loss: 0.0591 - val_acc: 0.9927 - val_mDice: 0.4759

Epoch 00137: val_mDice did not improve from 0.48175
Epoch 138/300
 - 40s - loss: 0.0447 - acc: 0.9950 - mDice: 0.9132 - val_loss: 0.0584 - val_acc: 0.9926 - val_mDice: 0.4753

Epoch 00138: val_mDice did not improve from 0.48175

Epoch 00138: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 139/300
 - 41s - loss: 0.0455 - acc: 0.9949 - mDice: 0.9116 - val_loss: 0.0572 - val_acc: 0.9927 - val_mDice: 0.4781

Epoch 00139: val_mDice did not improve from 0.48175
Epoch 140/300
 - 41s - loss: 0.0455 - acc: 0.9950 - mDice: 0.9115 - val_loss: 0.0570 - val_acc: 0.9927 - val_mDice: 0.4766

Epoch 00140: val_mDice did not improve from 0.48175
Epoch 141/300
 - 40s - loss: 0.0445 - acc: 0.9950 - mDice: 0.9136 - val_loss: 0.0582 - val_acc: 0.9926 - val_mDice: 0.4776

Epoch 00141: val_mDice did not improve from 0.48175
Epoch 142/300
 - 41s - loss: 0.0455 - acc: 0.9949 - mDice: 0.9115 - val_loss: 0.0597 - val_acc: 0.9926 - val_mDice: 0.4778

Epoch 00142: val_mDice did not improve from 0.48175
Epoch 143/300
 - 40s - loss: 0.0448 - acc: 0.9950 - mDice: 0.9130 - val_loss: 0.0581 - val_acc: 0.9927 - val_mDice: 0.4775

Epoch 00143: val_mDice did not improve from 0.48175
Epoch 144/300
 - 40s - loss: 0.0454 - acc: 0.9950 - mDice: 0.9117 - val_loss: 0.0580 - val_acc: 0.9927 - val_mDice: 0.4780

Epoch 00144: val_mDice did not improve from 0.48175
Epoch 145/300
 - 40s - loss: 0.0452 - acc: 0.9950 - mDice: 0.9122 - val_loss: 0.0576 - val_acc: 0.9927 - val_mDice: 0.4780

Epoch 00145: val_mDice did not improve from 0.48175
Epoch 146/300
 - 40s - loss: 0.0447 - acc: 0.9950 - mDice: 0.9132 - val_loss: 0.0569 - val_acc: 0.9927 - val_mDice: 0.4777

Epoch 00146: val_mDice did not improve from 0.48175
Epoch 147/300
 - 40s - loss: 0.0447 - acc: 0.9950 - mDice: 0.9131 - val_loss: 0.0573 - val_acc: 0.9926 - val_mDice: 0.4769

Epoch 00147: val_mDice did not improve from 0.48175
Epoch 148/300
 - 41s - loss: 0.0450 - acc: 0.9950 - mDice: 0.9126 - val_loss: 0.0566 - val_acc: 0.9927 - val_mDice: 0.4781

Epoch 00148: val_mDice did not improve from 0.48175
Epoch 149/300
 - 41s - loss: 0.0456 - acc: 0.9950 - mDice: 0.9114 - val_loss: 0.0565 - val_acc: 0.9927 - val_mDice: 0.4780

Epoch 00149: val_mDice did not improve from 0.48175
Epoch 150/300
 - 41s - loss: 0.0448 - acc: 0.9950 - mDice: 0.9130 - val_loss: 0.0571 - val_acc: 0.9927 - val_mDice: 0.4787

Epoch 00150: val_mDice did not improve from 0.48175
Epoch 151/300
 - 40s - loss: 0.0454 - acc: 0.9949 - mDice: 0.9118 - val_loss: 0.0555 - val_acc: 0.9927 - val_mDice: 0.4799

Epoch 00151: val_mDice did not improve from 0.48175
Epoch 152/300
 - 40s - loss: 0.0446 - acc: 0.9949 - mDice: 0.9134 - val_loss: 0.0571 - val_acc: 0.9927 - val_mDice: 0.4786

Epoch 00152: val_mDice did not improve from 0.48175
Epoch 153/300
 - 41s - loss: 0.0458 - acc: 0.9950 - mDice: 0.9109 - val_loss: 0.0556 - val_acc: 0.9927 - val_mDice: 0.4789

Epoch 00153: val_mDice did not improve from 0.48175

Epoch 00153: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.
Epoch 154/300
 - 40s - loss: 0.0451 - acc: 0.9949 - mDice: 0.9125 - val_loss: 0.0574 - val_acc: 0.9927 - val_mDice: 0.4803

Epoch 00154: val_mDice did not improve from 0.48175
Epoch 155/300
 - 41s - loss: 0.0452 - acc: 0.9950 - mDice: 0.9121 - val_loss: 0.0563 - val_acc: 0.9927 - val_mDice: 0.4787

Epoch 00155: val_mDice did not improve from 0.48175
Epoch 156/300
 - 41s - loss: 0.0446 - acc: 0.9950 - mDice: 0.9134 - val_loss: 0.0580 - val_acc: 0.9927 - val_mDice: 0.4772

Epoch 00156: val_mDice did not improve from 0.48175
Epoch 157/300
 - 41s - loss: 0.0450 - acc: 0.9950 - mDice: 0.9126 - val_loss: 0.0570 - val_acc: 0.9926 - val_mDice: 0.4780

Epoch 00157: val_mDice did not improve from 0.48175
Epoch 158/300
 - 40s - loss: 0.0447 - acc: 0.9950 - mDice: 0.9131 - val_loss: 0.0564 - val_acc: 0.9927 - val_mDice: 0.4788

Epoch 00158: val_mDice did not improve from 0.48175
Epoch 159/300
 - 40s - loss: 0.0448 - acc: 0.9950 - mDice: 0.9129 - val_loss: 0.0574 - val_acc: 0.9927 - val_mDice: 0.4770

Epoch 00159: val_mDice did not improve from 0.48175
Epoch 160/300
 - 40s - loss: 0.0446 - acc: 0.9950 - mDice: 0.9133 - val_loss: 0.0578 - val_acc: 0.9927 - val_mDice: 0.4790

Epoch 00160: val_mDice did not improve from 0.48175
Epoch 161/300
 - 40s - loss: 0.0454 - acc: 0.9950 - mDice: 0.9117 - val_loss: 0.0572 - val_acc: 0.9927 - val_mDice: 0.4790

Epoch 00161: val_mDice did not improve from 0.48175
Epoch 162/300
 - 40s - loss: 0.0450 - acc: 0.9950 - mDice: 0.9125 - val_loss: 0.0567 - val_acc: 0.9927 - val_mDice: 0.4790

Epoch 00162: val_mDice did not improve from 0.48175
Epoch 163/300
 - 41s - loss: 0.0455 - acc: 0.9950 - mDice: 0.9116 - val_loss: 0.0571 - val_acc: 0.9927 - val_mDice: 0.4788

Epoch 00163: val_mDice did not improve from 0.48175
Restoring model weights from the end of the best epoch
Epoch 00163: early stopping
{'val_loss': [0.3314191762123022, 0.3097828531766439, 0.2982770043450433, 0.29492608907523454, 0.2924339415074827, 0.28605704269072674, 0.278672689924369, 0.2806222735940515, 0.27468701570599646, 0.2814485870592587, 0.2823554875703903, 0.2816866216985313, 0.28209669019903866, 0.27732146435134764, 0.2516297183416269, 0.24379996798775933, 0.24976079438899731, 0.2772594904219424, 0.26712555825352313, 0.2241621002689138, 0.21002472413552775, 0.205646608684872, 0.20438531291735423, 0.1934344981078271, 0.1795571031155171, 0.173000919165554, 0.15873068512917998, 0.16519996814720622, 0.1818211610252793, 0.14886210617181417, 0.14636703072725474, 0.11671852931246027, 0.1274123710465503, 0.1448291701686991, 0.13934311745045064, 0.14197359102087337, 0.11725031998422411, 0.11181556449101136, 0.10322002073129018, 0.1131752236290379, 0.1055815947932882, 0.13161376740660397, 0.0953945213699484, 0.10286151096448526, 0.09725313049715918, 0.07531018971322893, 0.09367167077085993, 0.09045388426508631, 0.0956972817758898, 0.06535662652493955, 0.06628263985609507, 0.05708533256977528, 0.057188017993001966, 0.05727588597897653, 0.058997739900697815, 0.05917577969061362, 0.057664096937165246, 0.059245917129445005, 0.054370596646904586, 0.05969997206786731, 0.058374058332171165, 0.05849692082261896, 0.0576684475840033, 0.053296438208571426, 0.05317168805871282, 0.05456984101294039, 0.08123798194053294, 0.05306197183805185, 0.051081997048747434, 0.05195944695859342, 0.053857948552738795, 0.05270123463851196, 0.05155839240765787, 0.055536414230908, 0.05927156905333201, 0.05668752947011151, 0.054686228121007166, 0.054595725493388134, 0.057055193919677276, 0.05622420967878164, 0.05616301915667078, 0.053876515354838096, 0.05700080888765352, 0.056033387108966035, 0.05082057953417838, 0.05341757856331788, 0.05203965449476385, 0.05070207713244555, 0.049873187556281104, 0.053166171675687796, 0.0520484732972967, 0.05311909653582014, 0.05250768081561939, 0.053533314316122384, 0.05011356409426566, 0.054966304529536596, 0.0536811834162062, 0.05194621325076163, 0.04888433280649844, 0.05351332474399258, 0.051209918058312334, 0.05131601660817235, 0.05590400001308223, 0.05872949147904599, 0.054388680928820245, 0.05783559619127451, 0.058405184530997065, 0.05357655216086734, 0.056578358551403425, 0.05678517803236529, 0.06061398244655884, 0.05526936063179383, 0.05711480509769451, 0.056955818344164896, 0.05716742888406232, 0.05367559046895654, 0.05662899019123914, 0.05530303721134369, 0.057617508031584476, 0.0568292133353494, 0.05619906551308102, 0.05557062707326792, 0.055357337176978764, 0.055896368828621715, 0.053869501069501356, 0.0559182883472414, 0.057391306525236135, 0.05526301090244774, 0.0575132327484297, 0.059749054881903504, 0.059934330729392915, 0.0568996612374131, 0.056482633178656524, 0.05608819917336599, 0.0586832154978503, 0.0576915608959513, 0.05905326699709391, 0.058408016512343834, 0.057241979169773985, 0.057013543070973574, 0.05817652339333886, 0.05969803603561791, 0.05809161420520004, 0.0580052801438638, 0.05756028654339077, 0.05687736865278479, 0.057274392148753905, 0.05663508673508962, 0.05648250143986207, 0.05710604616829583, 0.055468651625487184, 0.057055583721524604, 0.055609174184613044, 0.057427415573919144, 0.05631507534880538, 0.0579533343379562, 0.056960369373585014, 0.05643820185382087, 0.057362584469912645, 0.057801931693747234, 0.05715027062205581, 0.05665064390536185, 0.0570695581468376], 'val_acc': [0.9841009568165731, 0.9871111596310819, 0.9887562370873071, 0.9897908018873977, 0.9902006867411617, 0.9909161948584937, 0.9908824081535454, 0.9908972663564367, 0.9909545512528749, 0.9903481053160476, 0.991370203258755, 0.9911316951831898, 0.9910147161455126, 0.9913946536926178, 0.9918129333146699, 0.9912155945379812, 0.9918997072958732, 0.9914672800728509, 0.9915598140464531, 0.9921554684996963, 0.9920948221160842, 0.9920346625932344, 0.9919167205736086, 0.9919651453201477, 0.9919557983094865, 0.9923374022807445, 0.9922108363818836, 0.9922144395095092, 0.9923457934691742, 0.9921209550834633, 0.9917947225742512, 0.9919313460856944, 0.9923184684088996, 0.9923072043839876, 0.9920176385759233, 0.9922506282995412, 0.9921329332901551, 0.9924347243151507, 0.992320625273673, 0.9919802433735615, 0.9924093144791978, 0.9919970239604916, 0.9922520674026765, 0.9921494758164918, 0.9923211085545766, 0.9924356872970993, 0.9923640077894514, 0.9925313250080601, 0.9925842961749515, 0.9925138266594918, 0.9925783052816763, 0.9928151344035839, 0.9926360770388767, 0.9926133019788129, 0.9924179473080793, 0.9924670826565396, 0.9926240916724678, 0.9924462317704439, 0.9924984869656263, 0.992306957373748, 0.992598440196063, 0.9922384066624684, 0.9924308884967197, 0.9924452687884953, 0.9923798289742913, 0.9924280156602373, 0.992597244523309, 0.9925102306915833, 0.9925214982963539, 0.9925392329155862, 0.9927662335358582, 0.9925332456021696, 0.9929536785091366, 0.9922393642746292, 0.9927108620738124, 0.9926003607901724, 0.9928772163104724, 0.992542830673424, 0.9921916608695869, 0.992716613116565, 0.9925910137795113, 0.9927355559380563, 0.9926533337469932, 0.9926895314866716, 0.9927393899665581, 0.9926200124236556, 0.9925449911180559, 0.9929105304740928, 0.9927089432696322, 0.9926696364228074, 0.9928285588731279, 0.9926797083548239, 0.9927808554680856, 0.9925576978259616, 0.9927820547206981, 0.9924654108625991, 0.9927897263575602, 0.9927595230910156, 0.992618576900379, 0.9928597179619042, 0.9927005610308489, 0.9927750990555451, 0.9927161387853078, 0.9923016914018281, 0.9924241762619477, 0.9925442697765591, 0.9925548160398329, 0.9929035873384447, 0.9925898145268988, 0.992689048205768, 0.9924682836990815, 0.99289375704688, 0.9927621560769754, 0.9925718328974269, 0.992854443040338, 0.9927271611697681, 0.992640394348282, 0.9928942349579957, 0.9927089432696322, 0.992708941479703, 0.9928355109584224, 0.9928455739407926, 0.9926557286723598, 0.9928117764962686, 0.9927784569628604, 0.9926799392557001, 0.9926598061312426, 0.9926912068604707, 0.9926854468680717, 0.9926351158468573, 0.9927449011587882, 0.9927818112903171, 0.9928127341084294, 0.9927734254716752, 0.9927235669917889, 0.9927379437037058, 0.9927429805646788, 0.9926272061494019, 0.9927060686432205, 0.9926576492664693, 0.9925962815413604, 0.9926432618149766, 0.9927067846149296, 0.9926674741882462, 0.9926502139002711, 0.9926684300104777, 0.9926312818183555, 0.9927003158105386, 0.9927240484827632, 0.9927372295219261, 0.992729794155728, 0.9926749041846564, 0.9926878489531554, 0.9926928822342698, 0.9926813783588352, 0.9926823323911375, 0.9925974790040437, 0.9926873710420396, 0.9926528558358774, 0.9926749059745857, 0.9926808986577902, 0.9926940814868824, 0.99269480103845], 'val_mDice': [0.3422667010052433, 0.38472954530615766, 0.4018392733804487, 0.41008969326994427, 0.41561523804794326, 0.41722728593931074, 0.4227976760528036, 0.4247058176244023, 0.425368565338912, 0.4243233142136636, 0.4292311584270621, 0.4280762857683028, 0.42927453333595, 0.43210052423669953, 0.43342887997314594, 0.43127366909427134, 0.43620755946401, 0.4357332725075655, 0.43650638010097914, 0.438429945045139, 0.4415072021899658, 0.44225501047600446, 0.4407101896432088, 0.4432117052980401, 0.44259692679296403, 0.4429132301170189, 0.44346807776270686, 0.44400278691414957, 0.4440372448425758, 0.44309138267247883, 0.4417264255675484, 0.4467855863743001, 0.4462082751162417, 0.4499819677871269, 0.4454566319067557, 0.44658700804094653, 0.44510349318071885, 0.4493707087304857, 0.4484978300315124, 0.44668282034183254, 0.44980465407747017, 0.4485427880663283, 0.44939497640013526, 0.45063823690867283, 0.448097207281627, 0.44921879156953415, 0.44934129031995934, 0.4506219683898607, 0.453434519850098, 0.4545493978816826, 0.4527048272145045, 0.46260678284877055, 0.4590361742107002, 0.45892139544358124, 0.4570998233538848, 0.4593635272156369, 0.45596488111011974, 0.4603273374540312, 0.4637799588767616, 0.45977639722394514, 0.46147965006642155, 0.463731449108582, 0.4628416672125235, 0.46754399487922144, 0.4633226131533717, 0.4622123397148407, 0.4700726279625352, 0.4656021229855649, 0.46615629314302326, 0.4673407323009617, 0.4657003847328392, 0.4698853387413496, 0.47215932644414144, 0.4675376643840113, 0.46327044655968835, 0.4688698195402391, 0.4679758341104975, 0.4706776531927744, 0.46878155980919645, 0.47142113483612375, 0.4703175542211771, 0.4729312888696563, 0.47165524969850203, 0.4709469818981807, 0.47357036117034057, 0.4716821589417684, 0.4714091680944001, 0.4750633746714428, 0.4735677061564804, 0.47228976508829346, 0.47852028477321695, 0.4764384217000938, 0.47577627856206073, 0.47562656313559887, 0.47831760912395277, 0.4698600024714362, 0.4693150556652056, 0.473202296592821, 0.47714105480023333, 0.4752145834728955, 0.4750651082893772, 0.4762384782974572, 0.4731516937977654, 0.4717297375470668, 0.4749836831718233, 0.4730246224090689, 0.47289671578761444, 0.47943279681679746, 0.47085523072563046, 0.473761692956141, 0.46970589511608984, 0.4784291174043325, 0.4767078135270819, 0.47545731918866674, 0.47520459333330284, 0.4757100353885102, 0.47778318144588006, 0.4794387403573539, 0.47716359658217106, 0.47655933785501187, 0.4784032192945413, 0.4801896806560211, 0.48174609111303146, 0.4786926187021104, 0.4797944012335739, 0.47766468576477233, 0.47613292213741903, 0.47809982065435824, 0.47811539426960564, 0.4778702345511711, 0.4780958161630609, 0.4792850903254953, 0.47956579887831174, 0.47891684089068537, 0.4785542938209421, 0.4791604147038168, 0.4758824329045755, 0.4752977174677391, 0.4780975921364786, 0.47659490170801905, 0.477558920361303, 0.4777787183061824, 0.4774504529966696, 0.47795933374838606, 0.47804887729554296, 0.47773228456229866, 0.4768665321311122, 0.4781090917048959, 0.4779973757864387, 0.47874234812529953, 0.4799003065381949, 0.478577529829745, 0.4788902356336872, 0.48026381706533666, 0.47872217886664487, 0.4771665992164934, 0.4780388740940137, 0.47877021107330425, 0.477003473774516, 0.47899302369257707, 0.4790149671906555, 0.4789905707343473, 0.4788344958657259], 'loss': [0.2666520886149197, 0.13255677855788287, 0.11208335151332392, 0.10245836685895239, 0.09774787005780425, 0.0914876546352888, 0.08831587822450455, 0.08606292606043396, 0.08485113542580648, 0.08259190902900078, 0.0807457246847981, 0.07770226804705001, 0.07674905246049289, 0.07510031123523386, 0.07590957189444837, 0.07412903868246136, 0.07147031746230938, 0.07093286985318234, 0.06979219840996667, 0.07013108465839708, 0.07129127895194048, 0.06741501468888132, 0.06703432481403647, 0.06695050537079132, 0.06763664668275851, 0.0661944596616134, 0.06567770093972702, 0.06393619480076143, 0.06488568219549883, 0.06445079268398651, 0.06480901290383326, 0.0625958379425932, 0.06324227569474242, 0.06349792577003223, 0.06343508883719444, 0.06175712092411979, 0.06218657044938735, 0.06102196824175238, 0.06039669428785357, 0.06203480217466051, 0.060780162710355176, 0.060159930013889786, 0.061184506833791716, 0.058590813199759784, 0.058445861040278164, 0.05980836372872427, 0.058522145327413115, 0.05694498585395288, 0.058210237283652475, 0.05723258799507114, 0.05896255066300807, 0.056667471508526905, 0.05625353662935592, 0.056695823778872126, 0.05583069007435751, 0.05569499364660457, 0.05426630719481071, 0.055499707596331244, 0.05662294162995368, 0.05458646003090914, 0.054525268429022396, 0.05395173720821987, 0.05496314914595414, 0.053709241846055654, 0.05419191178287993, 0.05351093168784971, 0.053426809581974016, 0.053164367955507955, 0.05275846187420992, 0.05318133371794809, 0.05259012754122709, 0.051796855778779224, 0.0520059575318049, 0.05200709216663266, 0.05233229448129908, 0.05148473100453042, 0.05027048503203692, 0.05110965459984482, 0.05005385975736745, 0.05060678937875478, 0.050922668319445974, 0.05079505323292785, 0.05013153973582979, 0.05011845489004954, 0.049468765339543495, 0.051057574008390734, 0.04934912126076864, 0.049382205967161134, 0.048971186360021096, 0.048762042691507655, 0.049374965052931326, 0.04901386637546457, 0.04931928767820259, 0.04855134952171258, 0.0493292360996475, 0.047949671289306404, 0.04811492647627302, 0.04915827128552049, 0.04748690240338192, 0.04669212869830105, 0.04859290943074761, 0.04783923532684889, 0.04755570857716049, 0.047498568933158895, 0.047234230612423385, 0.04735888698828969, 0.046334894976762124, 0.046925525849080905, 0.04690839245117897, 0.046495104983991065, 0.04623667106319023, 0.04701658149808787, 0.045781202075534544, 0.04733368919282616, 0.04650762023475312, 0.04613552724101885, 0.04636970449611048, 0.047257013139774075, 0.04598850062344386, 0.045751730849172084, 0.045739552962932126, 0.04602564302454085, 0.045607726110646736, 0.04541022394427904, 0.04785696331712679, 0.045861305376740304, 0.045277312679878376, 0.045794967438264784, 0.045420365829171395, 0.04584050671133675, 0.04592981531153663, 0.04493372543343559, 0.04459741649555937, 0.045874583748407, 0.04574175055167325, 0.045312694127192896, 0.0458063884560142, 0.04468400712149778, 0.045472379433988394, 0.045528318160382934, 0.04448767553116888, 0.04553802386914864, 0.044777773659832175, 0.04544665179160442, 0.04517827561379054, 0.04467773907022335, 0.04470506718965358, 0.044956179753983715, 0.04557768925319633, 0.04475376712852838, 0.04536142297532322, 0.044561038169926834, 0.04581923685666607, 0.04505012083769556, 0.0452218557422437, 0.044589338263029764, 0.04496311145232644, 0.044741124955368576, 0.044805716908392045, 0.04461438427875249, 0.045417849258516454, 0.0450334355947472, 0.045460547124890706], 'acc': [0.9758962016772879, 0.9860640124558389, 0.9882236594026426, 0.9892390771984153, 0.9898247300394073, 0.9903317833164746, 0.9907254360644115, 0.9909564679361178, 0.9912292559512735, 0.991444789067608, 0.9915760180721752, 0.9917902540240271, 0.9919318507250646, 0.9920436942791546, 0.992142104094433, 0.9922907751716513, 0.9923761452154828, 0.9924652356547738, 0.992554249608186, 0.9926392089131597, 0.9926921012938783, 0.9927970828892548, 0.9928835823220924, 0.992924408555886, 0.9929605821156666, 0.9930326649555516, 0.9930905985322107, 0.9931141792059051, 0.993175092329866, 0.9932757879119397, 0.9932633870806393, 0.9933520662512872, 0.993322870824596, 0.9933703408817376, 0.9934327137083502, 0.9934813070884546, 0.9934646701269335, 0.9935568114642711, 0.9935659080011551, 0.9935857316901378, 0.9936104978608992, 0.9936450298843168, 0.9936732823194364, 0.9937314133319876, 0.9937732421317242, 0.9937672063998741, 0.9938414990618133, 0.9938551449607678, 0.9938545435142929, 0.9938877915383959, 0.9938612734845355, 0.9939139574022567, 0.9939254091835107, 0.9939794309797245, 0.9940039085574426, 0.994040335051923, 0.9940728280863818, 0.9940725856262966, 0.9940751695260571, 0.9941226138545478, 0.9941389617613554, 0.9941901965494285, 0.9941807657141556, 0.9941807606433521, 0.9941937390278047, 0.9942488749211467, 0.9942436200097647, 0.9942352091703753, 0.9942935216192987, 0.9942812527651167, 0.9943138330817245, 0.9943211098775645, 0.994373083822266, 0.994365265854396, 0.9943794296186058, 0.9943924071478334, 0.994405269524414, 0.994444290026204, 0.9944474932791869, 0.9944497385401021, 0.9944700648932826, 0.9944811175379796, 0.9945245778497821, 0.9945425929724092, 0.9945429021476163, 0.9945442806038884, 0.9945773836408568, 0.9945709726771251, 0.9945850709030934, 0.9945934071827522, 0.9946106115368978, 0.994618339460166, 0.9945978312240799, 0.9946267524262654, 0.9946647555281205, 0.9946910465802777, 0.9947151984572675, 0.9946653832330317, 0.9947086364895523, 0.9947487727572945, 0.9947247067017598, 0.9947262467123139, 0.9947345727179195, 0.9947476837303697, 0.9947465172980098, 0.994800473450025, 0.9948197653176906, 0.9948132790979389, 0.9948163197143639, 0.9948469540269237, 0.9948541951114417, 0.9948303983382714, 0.9948729710627053, 0.9948470705759336, 0.9948504959565936, 0.9948539579719159, 0.9948487319981709, 0.9948592580739948, 0.9948911439138067, 0.9948630481059646, 0.9949045330673905, 0.9949026890735264, 0.9949304400591704, 0.9949166653882995, 0.9949196247734817, 0.9949025716465865, 0.9949084905645339, 0.9949281846755694, 0.9949356128159693, 0.9949304192158981, 0.9949628514499106, 0.9949341583619775, 0.9949413553872689, 0.9949515056257422, 0.9949465291960702, 0.994947050883349, 0.9949671697952612, 0.9949592333825064, 0.9949415822527414, 0.9949653860266477, 0.9949598370843535, 0.9949222754337743, 0.9949884787301216, 0.9949526251115448, 0.9949833310019001, 0.9949707368746022, 0.9949599382166076, 0.9949619451573323, 0.9949719532203177, 0.9949703725979264, 0.9949388972585689, 0.9949485656534117, 0.9949574900819312, 0.9949432453929973, 0.994991155073655, 0.9949710772201135, 0.9949826214754137, 0.9949692468493032, 0.9949887178639111, 0.9949555750097837, 0.9949716644872598, 0.9949861113271761, 0.994960485314666], 'mDice': [0.4794464124066139, 0.7421847423939019, 0.7819800093208759, 0.800687483288781, 0.8097882724414021, 0.8220416598932917, 0.8281710511197639, 0.8325573788394921, 0.8348410383878602, 0.8392436380680601, 0.8428645918235795, 0.8488343909373655, 0.8506682523270465, 0.8539072227358069, 0.8522351320454272, 0.8557181156863046, 0.8609868736594964, 0.8620168828922079, 0.8642543836270133, 0.8635278228636238, 0.8611821260751025, 0.868878237130672, 0.8695892047001299, 0.869739765731916, 0.8683464763376392, 0.8711944551112503, 0.8721948679472535, 0.8756654002380238, 0.8737376763304197, 0.8745545578227683, 0.8738414129299892, 0.8782213189449668, 0.8769449431045826, 0.8764107433712341, 0.8764991473659393, 0.8798294550721774, 0.8789813557020144, 0.8812648795391034, 0.8825063786962742, 0.8792161320498927, 0.8817182726016652, 0.8829387660037468, 0.8808755529063326, 0.8860273483756776, 0.8863002109808374, 0.8835731209664033, 0.8861117335931092, 0.8892533467033734, 0.8867256756242582, 0.8886630069775457, 0.8852199114706318, 0.8897702197967466, 0.8905958413177124, 0.8896895293552448, 0.8914029915266602, 0.8916591289315736, 0.8944984885838121, 0.8920316757451814, 0.8897832617458267, 0.8938302522454593, 0.8939458895374155, 0.8950669674229881, 0.8930464620015006, 0.8955526736619156, 0.8945807589746742, 0.8959175441032152, 0.8960841963466527, 0.8966138042591388, 0.8973947039505817, 0.8965538953831896, 0.8977180704465935, 0.8993050024060022, 0.8988549607317453, 0.8988545667307959, 0.8982016818915902, 0.8998885731736135, 0.9023089179713933, 0.9006119073283997, 0.9027169549300946, 0.9016110895659337, 0.9009721475770365, 0.9012165439218939, 0.9025257279037195, 0.902538056729612, 0.9038400903564583, 0.9006638755121577, 0.9040615647703903, 0.9039943746616481, 0.9048081381677726, 0.9052280821457234, 0.9039894726252028, 0.9047072771450624, 0.9041071843930946, 0.9056268244627945, 0.9040544668231217, 0.9068004578554428, 0.9064569231268187, 0.9043954632848522, 0.9077130248613429, 0.9092804101768142, 0.9054963257787917, 0.9070020364350622, 0.9075628742400627, 0.907671177158673, 0.908194140179726, 0.9079199202529484, 0.9099583273372622, 0.9087775917208771, 0.9088108562059337, 0.9096182486071116, 0.9101327706629053, 0.9085876924292594, 0.9110364035783242, 0.9079418716405703, 0.9095933375911224, 0.9103400403867032, 0.9098646978364431, 0.9080899465651987, 0.9106106022018666, 0.9110980403310857, 0.9111002045399453, 0.9105298839049236, 0.9113532807647319, 0.9117564822475416, 0.9068617275855448, 0.9108565293692852, 0.9120230594178426, 0.9109784061834967, 0.9117246257654107, 0.910881715304414, 0.9106953338835854, 0.9126948189541663, 0.9133697314441388, 0.9108079565046484, 0.9110761505597342, 0.9119319733449613, 0.9109325581986173, 0.9131792415763361, 0.9116127213181621, 0.9114929161543458, 0.9135749537077394, 0.9114875565157354, 0.9129818434125241, 0.9116624632295552, 0.9121805465446186, 0.9131923130403041, 0.9131420444256994, 0.9126345519905703, 0.9113913849724617, 0.913036047496078, 0.9118345097340248, 0.9134319717477567, 0.9109104302722487, 0.912452538379313, 0.9120920490355017, 0.9133634591441663, 0.9126083517600136, 0.9130642079911938, 0.9129207680569127, 0.913324201409192, 0.9117081733735642, 0.9124739041147395, 0.9116307964976436], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:02,  1.60it/s]predicting test subjects:  40%|████      | 2/5 [00:00<00:01,  2.03it/s]predicting test subjects:  60%|██████    | 3/5 [00:00<00:00,  2.49it/s]predicting test subjects:  80%|████████  | 4/5 [00:01<00:00,  2.94it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  3.46it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  3.67it/s]
predicting train subjects:   0%|          | 0/247 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/247 [00:00<00:30,  8.05it/s]predicting train subjects:   1%|          | 2/247 [00:00<00:30,  8.09it/s]predicting train subjects:   1%|          | 3/247 [00:00<00:29,  8.22it/s]predicting train subjects:   2%|▏         | 4/247 [00:00<00:29,  8.14it/s]predicting train subjects:   2%|▏         | 5/247 [00:00<00:30,  7.91it/s]predicting train subjects:   2%|▏         | 6/247 [00:00<00:31,  7.74it/s]predicting train subjects:   3%|▎         | 7/247 [00:00<00:31,  7.70it/s]predicting train subjects:   3%|▎         | 8/247 [00:01<00:30,  7.82it/s]predicting train subjects:   4%|▎         | 9/247 [00:01<00:29,  8.02it/s]predicting train subjects:   4%|▍         | 10/247 [00:01<00:29,  8.09it/s]predicting train subjects:   4%|▍         | 11/247 [00:01<00:28,  8.21it/s]predicting train subjects:   5%|▍         | 12/247 [00:01<00:28,  8.31it/s]predicting train subjects:   5%|▌         | 13/247 [00:01<00:27,  8.36it/s]predicting train subjects:   6%|▌         | 14/247 [00:01<00:27,  8.34it/s]predicting train subjects:   6%|▌         | 15/247 [00:01<00:27,  8.41it/s]predicting train subjects:   6%|▋         | 16/247 [00:01<00:27,  8.43it/s]predicting train subjects:   7%|▋         | 17/247 [00:02<00:27,  8.48it/s]predicting train subjects:   7%|▋         | 18/247 [00:02<00:27,  8.47it/s]predicting train subjects:   8%|▊         | 19/247 [00:02<00:27,  8.15it/s]predicting train subjects:   8%|▊         | 20/247 [00:02<00:27,  8.15it/s]predicting train subjects:   9%|▊         | 21/247 [00:02<00:27,  8.23it/s]predicting train subjects:   9%|▉         | 22/247 [00:02<00:27,  8.32it/s]predicting train subjects:   9%|▉         | 23/247 [00:02<00:26,  8.37it/s]predicting train subjects:  10%|▉         | 24/247 [00:02<00:26,  8.54it/s]predicting train subjects:  10%|█         | 25/247 [00:03<00:25,  8.62it/s]predicting train subjects:  11%|█         | 26/247 [00:03<00:25,  8.74it/s]predicting train subjects:  11%|█         | 27/247 [00:03<00:25,  8.78it/s]predicting train subjects:  11%|█▏        | 28/247 [00:03<00:24,  8.79it/s]predicting train subjects:  12%|█▏        | 29/247 [00:03<00:24,  8.86it/s]predicting train subjects:  12%|█▏        | 30/247 [00:03<00:24,  8.88it/s]predicting train subjects:  13%|█▎        | 31/247 [00:03<00:24,  8.94it/s]predicting train subjects:  13%|█▎        | 32/247 [00:03<00:23,  9.01it/s]predicting train subjects:  13%|█▎        | 33/247 [00:03<00:23,  8.99it/s]predicting train subjects:  14%|█▍        | 34/247 [00:04<00:23,  8.97it/s]predicting train subjects:  14%|█▍        | 35/247 [00:04<00:23,  8.96it/s]predicting train subjects:  15%|█▍        | 36/247 [00:04<00:23,  8.99it/s]predicting train subjects:  15%|█▍        | 37/247 [00:04<00:23,  8.92it/s]predicting train subjects:  15%|█▌        | 38/247 [00:04<00:23,  8.84it/s]predicting train subjects:  16%|█▌        | 39/247 [00:04<00:24,  8.45it/s]predicting train subjects:  16%|█▌        | 40/247 [00:04<00:24,  8.56it/s]predicting train subjects:  17%|█▋        | 41/247 [00:04<00:23,  8.62it/s]predicting train subjects:  17%|█▋        | 42/247 [00:04<00:23,  8.67it/s]predicting train subjects:  17%|█▋        | 43/247 [00:05<00:23,  8.66it/s]predicting train subjects:  18%|█▊        | 44/247 [00:05<00:23,  8.64it/s]predicting train subjects:  18%|█▊        | 45/247 [00:05<00:23,  8.65it/s]predicting train subjects:  19%|█▊        | 46/247 [00:05<00:23,  8.58it/s]predicting train subjects:  19%|█▉        | 47/247 [00:05<00:23,  8.62it/s]predicting train subjects:  19%|█▉        | 48/247 [00:05<00:22,  8.73it/s]predicting train subjects:  20%|█▉        | 49/247 [00:05<00:22,  8.70it/s]predicting train subjects:  20%|██        | 50/247 [00:05<00:22,  8.70it/s]predicting train subjects:  21%|██        | 51/247 [00:05<00:22,  8.61it/s]predicting train subjects:  21%|██        | 52/247 [00:06<00:22,  8.71it/s]predicting train subjects:  21%|██▏       | 53/247 [00:06<00:22,  8.79it/s]predicting train subjects:  22%|██▏       | 54/247 [00:06<00:21,  8.82it/s]predicting train subjects:  22%|██▏       | 55/247 [00:06<00:21,  8.83it/s]predicting train subjects:  23%|██▎       | 56/247 [00:06<00:21,  8.85it/s]predicting train subjects:  23%|██▎       | 57/247 [00:06<00:21,  8.88it/s]predicting train subjects:  23%|██▎       | 58/247 [00:06<00:21,  8.80it/s]predicting train subjects:  24%|██▍       | 59/247 [00:06<00:21,  8.57it/s]predicting train subjects:  24%|██▍       | 60/247 [00:07<00:22,  8.43it/s]predicting train subjects:  25%|██▍       | 61/247 [00:07<00:24,  7.71it/s]predicting train subjects:  25%|██▌       | 62/247 [00:07<00:23,  7.79it/s]predicting train subjects:  26%|██▌       | 63/247 [00:07<00:23,  7.83it/s]predicting train subjects:  26%|██▌       | 64/247 [00:07<00:23,  7.91it/s]predicting train subjects:  26%|██▋       | 65/247 [00:07<00:22,  8.01it/s]predicting train subjects:  27%|██▋       | 66/247 [00:07<00:22,  8.08it/s]predicting train subjects:  27%|██▋       | 67/247 [00:07<00:22,  8.03it/s]predicting train subjects:  28%|██▊       | 68/247 [00:08<00:22,  8.04it/s]predicting train subjects:  28%|██▊       | 69/247 [00:08<00:22,  8.04it/s]predicting train subjects:  28%|██▊       | 70/247 [00:08<00:21,  8.06it/s]predicting train subjects:  29%|██▊       | 71/247 [00:08<00:22,  7.96it/s]predicting train subjects:  29%|██▉       | 72/247 [00:08<00:21,  8.00it/s]predicting train subjects:  30%|██▉       | 73/247 [00:08<00:22,  7.71it/s]predicting train subjects:  30%|██▉       | 74/247 [00:08<00:22,  7.72it/s]predicting train subjects:  30%|███       | 75/247 [00:08<00:21,  7.82it/s]predicting train subjects:  31%|███       | 76/247 [00:09<00:21,  7.89it/s]predicting train subjects:  31%|███       | 77/247 [00:09<00:25,  6.74it/s]predicting train subjects:  32%|███▏      | 78/247 [00:09<00:26,  6.29it/s]predicting train subjects:  32%|███▏      | 79/247 [00:09<00:25,  6.63it/s]predicting train subjects:  32%|███▏      | 80/247 [00:09<00:25,  6.44it/s]predicting train subjects:  33%|███▎      | 81/247 [00:09<00:24,  6.78it/s]predicting train subjects:  33%|███▎      | 82/247 [00:10<00:23,  7.11it/s]predicting train subjects:  34%|███▎      | 83/247 [00:10<00:21,  7.49it/s]predicting train subjects:  34%|███▍      | 84/247 [00:10<00:21,  7.75it/s]predicting train subjects:  34%|███▍      | 85/247 [00:10<00:20,  7.93it/s]predicting train subjects:  35%|███▍      | 86/247 [00:10<00:19,  8.08it/s]predicting train subjects:  35%|███▌      | 87/247 [00:10<00:19,  8.12it/s]predicting train subjects:  36%|███▌      | 88/247 [00:10<00:20,  7.93it/s]predicting train subjects:  36%|███▌      | 89/247 [00:10<00:20,  7.79it/s]predicting train subjects:  36%|███▋      | 90/247 [00:10<00:19,  7.95it/s]predicting train subjects:  37%|███▋      | 91/247 [00:11<00:19,  8.02it/s]predicting train subjects:  37%|███▋      | 92/247 [00:11<00:19,  8.06it/s]predicting train subjects:  38%|███▊      | 93/247 [00:11<00:18,  8.15it/s]predicting train subjects:  38%|███▊      | 94/247 [00:11<00:19,  7.87it/s]predicting train subjects:  38%|███▊      | 95/247 [00:11<00:19,  7.92it/s]predicting train subjects:  39%|███▉      | 96/247 [00:11<00:18,  8.12it/s]predicting train subjects:  39%|███▉      | 97/247 [00:11<00:18,  8.23it/s]predicting train subjects:  40%|███▉      | 98/247 [00:11<00:17,  8.32it/s]predicting train subjects:  40%|████      | 99/247 [00:12<00:17,  8.31it/s]predicting train subjects:  40%|████      | 100/247 [00:12<00:18,  7.79it/s]predicting train subjects:  41%|████      | 101/247 [00:12<00:19,  7.59it/s]predicting train subjects:  41%|████▏     | 102/247 [00:12<00:19,  7.46it/s]predicting train subjects:  42%|████▏     | 103/247 [00:12<00:19,  7.38it/s]predicting train subjects:  42%|████▏     | 104/247 [00:12<00:20,  7.08it/s]predicting train subjects:  43%|████▎     | 105/247 [00:12<00:20,  7.06it/s]predicting train subjects:  43%|████▎     | 106/247 [00:13<00:19,  7.05it/s]predicting train subjects:  43%|████▎     | 107/247 [00:13<00:19,  7.09it/s]predicting train subjects:  44%|████▎     | 108/247 [00:13<00:19,  7.10it/s]predicting train subjects:  44%|████▍     | 109/247 [00:13<00:19,  7.13it/s]predicting train subjects:  45%|████▍     | 110/247 [00:13<00:19,  7.15it/s]predicting train subjects:  45%|████▍     | 111/247 [00:13<00:19,  7.13it/s]predicting train subjects:  45%|████▌     | 112/247 [00:13<00:18,  7.13it/s]predicting train subjects:  46%|████▌     | 113/247 [00:14<00:18,  7.06it/s]predicting train subjects:  46%|████▌     | 114/247 [00:14<00:19,  6.97it/s]predicting train subjects:  47%|████▋     | 115/247 [00:14<00:19,  6.89it/s]predicting train subjects:  47%|████▋     | 116/247 [00:14<00:18,  6.90it/s]predicting train subjects:  47%|████▋     | 117/247 [00:14<00:18,  6.94it/s]predicting train subjects:  48%|████▊     | 118/247 [00:14<00:18,  7.09it/s]predicting train subjects:  48%|████▊     | 119/247 [00:14<00:17,  7.21it/s]predicting train subjects:  49%|████▊     | 120/247 [00:15<00:17,  7.37it/s]predicting train subjects:  49%|████▉     | 121/247 [00:15<00:16,  7.49it/s]predicting train subjects:  49%|████▉     | 122/247 [00:15<00:16,  7.43it/s]predicting train subjects:  50%|████▉     | 123/247 [00:15<00:16,  7.48it/s]predicting train subjects:  50%|█████     | 124/247 [00:15<00:16,  7.48it/s]predicting train subjects:  51%|█████     | 125/247 [00:15<00:16,  7.46it/s]predicting train subjects:  51%|█████     | 126/247 [00:15<00:16,  7.49it/s]predicting train subjects:  51%|█████▏    | 127/247 [00:15<00:15,  7.51it/s]predicting train subjects:  52%|█████▏    | 128/247 [00:16<00:16,  7.40it/s]predicting train subjects:  52%|█████▏    | 129/247 [00:16<00:15,  7.43it/s]predicting train subjects:  53%|█████▎    | 130/247 [00:16<00:15,  7.38it/s]predicting train subjects:  53%|█████▎    | 131/247 [00:16<00:15,  7.41it/s]predicting train subjects:  53%|█████▎    | 132/247 [00:16<00:15,  7.47it/s]predicting train subjects:  54%|█████▍    | 133/247 [00:16<00:15,  7.54it/s]predicting train subjects:  54%|█████▍    | 134/247 [00:16<00:14,  7.63it/s]predicting train subjects:  55%|█████▍    | 135/247 [00:17<00:14,  7.65it/s]predicting train subjects:  55%|█████▌    | 136/247 [00:17<00:13,  8.11it/s]predicting train subjects:  55%|█████▌    | 137/247 [00:17<00:13,  8.44it/s]predicting train subjects:  56%|█████▌    | 138/247 [00:17<00:12,  8.70it/s]predicting train subjects:  56%|█████▋    | 139/247 [00:17<00:12,  8.93it/s]predicting train subjects:  57%|█████▋    | 140/247 [00:17<00:11,  9.07it/s]predicting train subjects:  57%|█████▋    | 141/247 [00:17<00:11,  9.09it/s]predicting train subjects:  57%|█████▋    | 142/247 [00:17<00:11,  9.22it/s]predicting train subjects:  58%|█████▊    | 143/247 [00:17<00:11,  9.34it/s]predicting train subjects:  58%|█████▊    | 144/247 [00:18<00:10,  9.43it/s]predicting train subjects:  59%|█████▊    | 145/247 [00:18<00:10,  9.42it/s]predicting train subjects:  59%|█████▉    | 146/247 [00:18<00:11,  8.65it/s]predicting train subjects:  60%|█████▉    | 147/247 [00:18<00:11,  8.79it/s]predicting train subjects:  60%|█████▉    | 148/247 [00:18<00:11,  8.94it/s]predicting train subjects:  60%|██████    | 149/247 [00:18<00:10,  9.09it/s]predicting train subjects:  61%|██████    | 150/247 [00:18<00:10,  9.20it/s]predicting train subjects:  61%|██████    | 151/247 [00:18<00:10,  9.26it/s]predicting train subjects:  62%|██████▏   | 152/247 [00:18<00:10,  9.29it/s]predicting train subjects:  62%|██████▏   | 153/247 [00:18<00:10,  9.33it/s]predicting train subjects:  62%|██████▏   | 154/247 [00:19<00:10,  9.08it/s]predicting train subjects:  63%|██████▎   | 155/247 [00:19<00:10,  8.83it/s]predicting train subjects:  63%|██████▎   | 156/247 [00:19<00:10,  8.68it/s]predicting train subjects:  64%|██████▎   | 157/247 [00:19<00:10,  8.64it/s]predicting train subjects:  64%|██████▍   | 158/247 [00:19<00:10,  8.59it/s]predicting train subjects:  64%|██████▍   | 159/247 [00:19<00:10,  8.54it/s]predicting train subjects:  65%|██████▍   | 160/247 [00:19<00:10,  8.52it/s]predicting train subjects:  65%|██████▌   | 161/247 [00:19<00:10,  8.52it/s]predicting train subjects:  66%|██████▌   | 162/247 [00:20<00:09,  8.52it/s]predicting train subjects:  66%|██████▌   | 163/247 [00:20<00:09,  8.54it/s]predicting train subjects:  66%|██████▋   | 164/247 [00:20<00:09,  8.53it/s]predicting train subjects:  67%|██████▋   | 165/247 [00:20<00:09,  8.48it/s]predicting train subjects:  67%|██████▋   | 166/247 [00:20<00:09,  8.48it/s]predicting train subjects:  68%|██████▊   | 167/247 [00:20<00:09,  8.51it/s]predicting train subjects:  68%|██████▊   | 168/247 [00:20<00:09,  8.52it/s]predicting train subjects:  68%|██████▊   | 169/247 [00:20<00:09,  8.52it/s]predicting train subjects:  69%|██████▉   | 170/247 [00:20<00:09,  8.52it/s]predicting train subjects:  69%|██████▉   | 171/247 [00:21<00:08,  8.52it/s]predicting train subjects:  70%|██████▉   | 172/247 [00:21<00:08,  8.48it/s]predicting train subjects:  70%|███████   | 173/247 [00:21<00:10,  7.11it/s]predicting train subjects:  70%|███████   | 174/247 [00:21<00:09,  7.35it/s]predicting train subjects:  71%|███████   | 175/247 [00:21<00:10,  6.91it/s]predicting train subjects:  71%|███████▏  | 176/247 [00:21<00:09,  7.32it/s]predicting train subjects:  72%|███████▏  | 177/247 [00:21<00:09,  7.60it/s]predicting train subjects:  72%|███████▏  | 178/247 [00:22<00:08,  7.83it/s]predicting train subjects:  72%|███████▏  | 179/247 [00:22<00:08,  8.02it/s]predicting train subjects:  73%|███████▎  | 180/247 [00:22<00:08,  8.15it/s]predicting train subjects:  73%|███████▎  | 181/247 [00:22<00:08,  8.23it/s]predicting train subjects:  74%|███████▎  | 182/247 [00:22<00:07,  8.22it/s]predicting train subjects:  74%|███████▍  | 183/247 [00:22<00:07,  8.14it/s]predicting train subjects:  74%|███████▍  | 184/247 [00:22<00:07,  8.24it/s]predicting train subjects:  75%|███████▍  | 185/247 [00:22<00:07,  8.22it/s]predicting train subjects:  75%|███████▌  | 186/247 [00:23<00:07,  8.20it/s]predicting train subjects:  76%|███████▌  | 187/247 [00:23<00:07,  8.23it/s]predicting train subjects:  76%|███████▌  | 188/247 [00:23<00:07,  8.27it/s]predicting train subjects:  77%|███████▋  | 189/247 [00:23<00:06,  8.29it/s]predicting train subjects:  77%|███████▋  | 190/247 [00:23<00:07,  8.09it/s]predicting train subjects:  77%|███████▋  | 191/247 [00:23<00:06,  8.18it/s]predicting train subjects:  78%|███████▊  | 192/247 [00:23<00:06,  7.97it/s]predicting train subjects:  78%|███████▊  | 193/247 [00:23<00:06,  8.09it/s]predicting train subjects:  79%|███████▊  | 194/247 [00:24<00:06,  8.37it/s]predicting train subjects:  79%|███████▉  | 195/247 [00:24<00:06,  8.55it/s]predicting train subjects:  79%|███████▉  | 196/247 [00:24<00:05,  8.66it/s]predicting train subjects:  80%|███████▉  | 197/247 [00:24<00:05,  8.77it/s]predicting train subjects:  80%|████████  | 198/247 [00:24<00:05,  8.87it/s]predicting train subjects:  81%|████████  | 199/247 [00:24<00:05,  8.91it/s]predicting train subjects:  81%|████████  | 200/247 [00:24<00:05,  8.83it/s]predicting train subjects:  81%|████████▏ | 201/247 [00:24<00:05,  8.87it/s]predicting train subjects:  82%|████████▏ | 202/247 [00:24<00:05,  8.92it/s]predicting train subjects:  82%|████████▏ | 203/247 [00:25<00:04,  8.88it/s]predicting train subjects:  83%|████████▎ | 204/247 [00:25<00:04,  8.84it/s]predicting train subjects:  83%|████████▎ | 205/247 [00:25<00:04,  8.79it/s]predicting train subjects:  83%|████████▎ | 206/247 [00:25<00:04,  8.84it/s]predicting train subjects:  84%|████████▍ | 207/247 [00:25<00:04,  8.90it/s]predicting train subjects:  84%|████████▍ | 208/247 [00:25<00:04,  8.94it/s]predicting train subjects:  85%|████████▍ | 209/247 [00:25<00:04,  8.96it/s]predicting train subjects:  85%|████████▌ | 210/247 [00:25<00:04,  9.01it/s]predicting train subjects:  85%|████████▌ | 211/247 [00:25<00:04,  8.99it/s]predicting train subjects:  86%|████████▌ | 212/247 [00:26<00:04,  8.65it/s]predicting train subjects:  86%|████████▌ | 213/247 [00:26<00:03,  8.62it/s]predicting train subjects:  87%|████████▋ | 214/247 [00:26<00:03,  8.56it/s]predicting train subjects:  87%|████████▋ | 215/247 [00:26<00:03,  8.50it/s]predicting train subjects:  87%|████████▋ | 216/247 [00:26<00:03,  8.46it/s]predicting train subjects:  88%|████████▊ | 217/247 [00:26<00:03,  8.42it/s]predicting train subjects:  88%|████████▊ | 218/247 [00:26<00:03,  8.49it/s]predicting train subjects:  89%|████████▊ | 219/247 [00:26<00:03,  8.43it/s]predicting train subjects:  89%|████████▉ | 220/247 [00:26<00:03,  8.33it/s]predicting train subjects:  89%|████████▉ | 221/247 [00:27<00:03,  8.39it/s]predicting train subjects:  90%|████████▉ | 222/247 [00:27<00:02,  8.41it/s]predicting train subjects:  90%|█████████ | 223/247 [00:27<00:02,  8.49it/s]predicting train subjects:  91%|█████████ | 224/247 [00:27<00:02,  8.54it/s]predicting train subjects:  91%|█████████ | 225/247 [00:27<00:02,  8.41it/s]predicting train subjects:  91%|█████████▏| 226/247 [00:27<00:02,  8.42it/s]predicting train subjects:  92%|█████████▏| 227/247 [00:27<00:02,  8.49it/s]predicting train subjects:  92%|█████████▏| 228/247 [00:27<00:02,  8.53it/s]predicting train subjects:  93%|█████████▎| 229/247 [00:28<00:02,  8.59it/s]predicting train subjects:  93%|█████████▎| 230/247 [00:28<00:02,  7.97it/s]predicting train subjects:  94%|█████████▎| 231/247 [00:28<00:02,  7.80it/s]predicting train subjects:  94%|█████████▍| 232/247 [00:28<00:01,  7.70it/s]predicting train subjects:  94%|█████████▍| 233/247 [00:28<00:01,  7.63it/s]predicting train subjects:  95%|█████████▍| 234/247 [00:28<00:01,  7.57it/s]predicting train subjects:  95%|█████████▌| 235/247 [00:28<00:01,  7.52it/s]predicting train subjects:  96%|█████████▌| 236/247 [00:28<00:01,  7.48it/s]predicting train subjects:  96%|█████████▌| 237/247 [00:29<00:01,  7.51it/s]predicting train subjects:  96%|█████████▋| 238/247 [00:29<00:01,  7.50it/s]predicting train subjects:  97%|█████████▋| 239/247 [00:29<00:01,  7.55it/s]predicting train subjects:  97%|█████████▋| 240/247 [00:29<00:01,  6.88it/s]predicting train subjects:  98%|█████████▊| 241/247 [00:29<00:00,  7.02it/s]predicting train subjects:  98%|█████████▊| 242/247 [00:29<00:00,  7.15it/s]predicting train subjects:  98%|█████████▊| 243/247 [00:29<00:00,  7.24it/s]predicting train subjects:  99%|█████████▉| 244/247 [00:30<00:00,  7.31it/s]predicting train subjects:  99%|█████████▉| 245/247 [00:30<00:00,  7.38it/s]predicting train subjects: 100%|█████████▉| 246/247 [00:30<00:00,  7.47it/s]predicting train subjects: 100%|██████████| 247/247 [00:30<00:00,  7.51it/s]predicting train subjects: 100%|██████████| 247/247 [00:30<00:00,  8.10it/s]
predicting test subjects sagittal:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects sagittal:  20%|██        | 1/5 [00:00<00:00,  7.03it/s]predicting test subjects sagittal:  40%|████      | 2/5 [00:00<00:00,  7.00it/s]predicting test subjects sagittal:  60%|██████    | 3/5 [00:00<00:00,  7.40it/s]predicting test subjects sagittal:  80%|████████  | 4/5 [00:00<00:00,  7.75it/s]predicting test subjects sagittal: 100%|██████████| 5/5 [00:00<00:00,  7.42it/s]predicting test subjects sagittal: 100%|██████████| 5/5 [00:00<00:00,  7.50it/s]
predicting train subjects sagittal:   0%|          | 0/247 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/247 [00:00<00:29,  8.39it/s]predicting train subjects sagittal:   1%|          | 2/247 [00:00<00:28,  8.48it/s]predicting train subjects sagittal:   1%|          | 3/247 [00:00<00:28,  8.56it/s]predicting train subjects sagittal:   2%|▏         | 4/247 [00:00<00:28,  8.48it/s]predicting train subjects sagittal:   2%|▏         | 5/247 [00:00<00:28,  8.49it/s]predicting train subjects sagittal:   2%|▏         | 6/247 [00:00<00:28,  8.49it/s]predicting train subjects sagittal:   3%|▎         | 7/247 [00:00<00:28,  8.48it/s]predicting train subjects sagittal:   3%|▎         | 8/247 [00:00<00:28,  8.45it/s]predicting train subjects sagittal:   4%|▎         | 9/247 [00:01<00:28,  8.47it/s]predicting train subjects sagittal:   4%|▍         | 10/247 [00:01<00:27,  8.47it/s]predicting train subjects sagittal:   4%|▍         | 11/247 [00:01<00:27,  8.45it/s]predicting train subjects sagittal:   5%|▍         | 12/247 [00:01<00:27,  8.48it/s]predicting train subjects sagittal:   5%|▌         | 13/247 [00:01<00:27,  8.49it/s]predicting train subjects sagittal:   6%|▌         | 14/247 [00:01<00:27,  8.50it/s]predicting train subjects sagittal:   6%|▌         | 15/247 [00:01<00:27,  8.41it/s]predicting train subjects sagittal:   6%|▋         | 16/247 [00:01<00:27,  8.38it/s]predicting train subjects sagittal:   7%|▋         | 17/247 [00:02<00:27,  8.37it/s]predicting train subjects sagittal:   7%|▋         | 18/247 [00:02<00:27,  8.36it/s]predicting train subjects sagittal:   8%|▊         | 19/247 [00:02<00:27,  8.35it/s]predicting train subjects sagittal:   8%|▊         | 20/247 [00:02<00:28,  7.90it/s]predicting train subjects sagittal:   9%|▊         | 21/247 [00:02<00:28,  7.94it/s]predicting train subjects sagittal:   9%|▉         | 22/247 [00:02<00:27,  8.07it/s]predicting train subjects sagittal:   9%|▉         | 23/247 [00:02<00:27,  8.08it/s]predicting train subjects sagittal:  10%|▉         | 24/247 [00:02<00:27,  7.99it/s]predicting train subjects sagittal:  10%|█         | 25/247 [00:03<00:27,  8.09it/s]predicting train subjects sagittal:  11%|█         | 26/247 [00:03<00:26,  8.27it/s]predicting train subjects sagittal:  11%|█         | 27/247 [00:03<00:26,  8.37it/s]predicting train subjects sagittal:  11%|█▏        | 28/247 [00:03<00:25,  8.47it/s]predicting train subjects sagittal:  12%|█▏        | 29/247 [00:03<00:25,  8.54it/s]predicting train subjects sagittal:  12%|█▏        | 30/247 [00:03<00:25,  8.50it/s]predicting train subjects sagittal:  13%|█▎        | 31/247 [00:03<00:25,  8.57it/s]predicting train subjects sagittal:  13%|█▎        | 32/247 [00:03<00:24,  8.63it/s]predicting train subjects sagittal:  13%|█▎        | 33/247 [00:03<00:24,  8.65it/s]predicting train subjects sagittal:  14%|█▍        | 34/247 [00:04<00:24,  8.64it/s]predicting train subjects sagittal:  14%|█▍        | 35/247 [00:04<00:24,  8.67it/s]predicting train subjects sagittal:  15%|█▍        | 36/247 [00:04<00:24,  8.65it/s]predicting train subjects sagittal:  15%|█▍        | 37/247 [00:04<00:24,  8.67it/s]predicting train subjects sagittal:  15%|█▌        | 38/247 [00:04<00:23,  8.71it/s]predicting train subjects sagittal:  16%|█▌        | 39/247 [00:04<00:23,  8.72it/s]predicting train subjects sagittal:  16%|█▌        | 40/247 [00:04<00:23,  8.66it/s]predicting train subjects sagittal:  17%|█▋        | 41/247 [00:04<00:23,  8.64it/s]predicting train subjects sagittal:  17%|█▋        | 42/247 [00:04<00:23,  8.70it/s]predicting train subjects sagittal:  17%|█▋        | 43/247 [00:05<00:23,  8.72it/s]predicting train subjects sagittal:  18%|█▊        | 44/247 [00:05<00:23,  8.75it/s]predicting train subjects sagittal:  18%|█▊        | 45/247 [00:05<00:23,  8.75it/s]predicting train subjects sagittal:  19%|█▊        | 46/247 [00:05<00:22,  8.76it/s]predicting train subjects sagittal:  19%|█▉        | 47/247 [00:05<00:22,  8.77it/s]predicting train subjects sagittal:  19%|█▉        | 48/247 [00:05<00:22,  8.75it/s]predicting train subjects sagittal:  20%|█▉        | 49/247 [00:05<00:22,  8.74it/s]predicting train subjects sagittal:  20%|██        | 50/247 [00:05<00:22,  8.72it/s]predicting train subjects sagittal:  21%|██        | 51/247 [00:05<00:22,  8.72it/s]predicting train subjects sagittal:  21%|██        | 52/247 [00:06<00:22,  8.78it/s]predicting train subjects sagittal:  21%|██▏       | 53/247 [00:06<00:21,  8.82it/s]predicting train subjects sagittal:  22%|██▏       | 54/247 [00:06<00:21,  8.82it/s]predicting train subjects sagittal:  22%|██▏       | 55/247 [00:06<00:21,  8.88it/s]predicting train subjects sagittal:  23%|██▎       | 56/247 [00:06<00:21,  8.94it/s]predicting train subjects sagittal:  23%|██▎       | 57/247 [00:06<00:21,  8.90it/s]predicting train subjects sagittal:  23%|██▎       | 58/247 [00:06<00:21,  8.91it/s]predicting train subjects sagittal:  24%|██▍       | 59/247 [00:06<00:21,  8.70it/s]predicting train subjects sagittal:  24%|██▍       | 60/247 [00:07<00:21,  8.56it/s]predicting train subjects sagittal:  25%|██▍       | 61/247 [00:07<00:21,  8.46it/s]predicting train subjects sagittal:  25%|██▌       | 62/247 [00:07<00:22,  8.40it/s]predicting train subjects sagittal:  26%|██▌       | 63/247 [00:07<00:22,  8.33it/s]predicting train subjects sagittal:  26%|██▌       | 64/247 [00:07<00:22,  8.26it/s]predicting train subjects sagittal:  26%|██▋       | 65/247 [00:07<00:21,  8.29it/s]predicting train subjects sagittal:  27%|██▋       | 66/247 [00:07<00:21,  8.29it/s]predicting train subjects sagittal:  27%|██▋       | 67/247 [00:07<00:21,  8.32it/s]predicting train subjects sagittal:  28%|██▊       | 68/247 [00:07<00:21,  8.31it/s]predicting train subjects sagittal:  28%|██▊       | 69/247 [00:08<00:21,  8.34it/s]predicting train subjects sagittal:  28%|██▊       | 70/247 [00:08<00:21,  8.36it/s]predicting train subjects sagittal:  29%|██▊       | 71/247 [00:08<00:21,  8.34it/s]predicting train subjects sagittal:  29%|██▉       | 72/247 [00:08<00:21,  8.33it/s]predicting train subjects sagittal:  30%|██▉       | 73/247 [00:08<00:20,  8.35it/s]predicting train subjects sagittal:  30%|██▉       | 74/247 [00:08<00:20,  8.34it/s]predicting train subjects sagittal:  30%|███       | 75/247 [00:08<00:20,  8.33it/s]predicting train subjects sagittal:  31%|███       | 76/247 [00:08<00:20,  8.33it/s]predicting train subjects sagittal:  31%|███       | 77/247 [00:09<00:20,  8.44it/s]predicting train subjects sagittal:  32%|███▏      | 78/247 [00:09<00:21,  8.03it/s]predicting train subjects sagittal:  32%|███▏      | 79/247 [00:09<00:21,  8.00it/s]predicting train subjects sagittal:  32%|███▏      | 80/247 [00:09<00:19,  8.46it/s]predicting train subjects sagittal:  33%|███▎      | 81/247 [00:09<00:19,  8.58it/s]predicting train subjects sagittal:  33%|███▎      | 82/247 [00:09<00:19,  8.57it/s]predicting train subjects sagittal:  34%|███▎      | 83/247 [00:09<00:19,  8.52it/s]predicting train subjects sagittal:  34%|███▍      | 84/247 [00:09<00:20,  8.13it/s]predicting train subjects sagittal:  34%|███▍      | 85/247 [00:10<00:19,  8.29it/s]predicting train subjects sagittal:  35%|███▍      | 86/247 [00:10<00:19,  8.39it/s]predicting train subjects sagittal:  35%|███▌      | 87/247 [00:10<00:18,  8.43it/s]predicting train subjects sagittal:  36%|███▌      | 88/247 [00:10<00:18,  8.43it/s]predicting train subjects sagittal:  36%|███▌      | 89/247 [00:10<00:19,  8.30it/s]predicting train subjects sagittal:  36%|███▋      | 90/247 [00:10<00:18,  8.36it/s]predicting train subjects sagittal:  37%|███▋      | 91/247 [00:10<00:18,  8.39it/s]predicting train subjects sagittal:  37%|███▋      | 92/247 [00:10<00:18,  8.44it/s]predicting train subjects sagittal:  38%|███▊      | 93/247 [00:10<00:18,  8.40it/s]predicting train subjects sagittal:  38%|███▊      | 94/247 [00:11<00:18,  8.43it/s]predicting train subjects sagittal:  38%|███▊      | 95/247 [00:11<00:18,  8.40it/s]predicting train subjects sagittal:  39%|███▉      | 96/247 [00:11<00:18,  8.39it/s]predicting train subjects sagittal:  39%|███▉      | 97/247 [00:11<00:17,  8.38it/s]predicting train subjects sagittal:  40%|███▉      | 98/247 [00:11<00:17,  8.40it/s]predicting train subjects sagittal:  40%|████      | 99/247 [00:11<00:17,  8.40it/s]predicting train subjects sagittal:  40%|████      | 100/247 [00:11<00:18,  7.93it/s]predicting train subjects sagittal:  41%|████      | 101/247 [00:11<00:19,  7.67it/s]predicting train subjects sagittal:  41%|████▏     | 102/247 [00:12<00:19,  7.47it/s]predicting train subjects sagittal:  42%|████▏     | 103/247 [00:12<00:19,  7.31it/s]predicting train subjects sagittal:  42%|████▏     | 104/247 [00:12<00:19,  7.24it/s]predicting train subjects sagittal:  43%|████▎     | 105/247 [00:12<00:19,  7.17it/s]predicting train subjects sagittal:  43%|████▎     | 106/247 [00:12<00:19,  7.12it/s]predicting train subjects sagittal:  43%|████▎     | 107/247 [00:12<00:19,  7.08it/s]predicting train subjects sagittal:  44%|████▎     | 108/247 [00:12<00:19,  7.11it/s]predicting train subjects sagittal:  44%|████▍     | 109/247 [00:13<00:19,  7.10it/s]predicting train subjects sagittal:  45%|████▍     | 110/247 [00:13<00:19,  7.11it/s]predicting train subjects sagittal:  45%|████▍     | 111/247 [00:13<00:19,  7.05it/s]predicting train subjects sagittal:  45%|████▌     | 112/247 [00:13<00:19,  6.97it/s]predicting train subjects sagittal:  46%|████▌     | 113/247 [00:13<00:19,  7.00it/s]predicting train subjects sagittal:  46%|████▌     | 114/247 [00:13<00:18,  7.04it/s]predicting train subjects sagittal:  47%|████▋     | 115/247 [00:13<00:18,  6.96it/s]predicting train subjects sagittal:  47%|████▋     | 116/247 [00:14<00:18,  6.93it/s]predicting train subjects sagittal:  47%|████▋     | 117/247 [00:14<00:18,  6.94it/s]predicting train subjects sagittal:  48%|████▊     | 118/247 [00:14<00:18,  7.14it/s]predicting train subjects sagittal:  48%|████▊     | 119/247 [00:14<00:17,  7.32it/s]predicting train subjects sagittal:  49%|████▊     | 120/247 [00:14<00:17,  7.44it/s]predicting train subjects sagittal:  49%|████▉     | 121/247 [00:14<00:16,  7.47it/s]predicting train subjects sagittal:  49%|████▉     | 122/247 [00:14<00:17,  7.23it/s]predicting train subjects sagittal:  50%|████▉     | 123/247 [00:15<00:16,  7.31it/s]predicting train subjects sagittal:  50%|█████     | 124/247 [00:15<00:16,  7.42it/s]predicting train subjects sagittal:  51%|█████     | 125/247 [00:15<00:16,  7.51it/s]predicting train subjects sagittal:  51%|█████     | 126/247 [00:15<00:16,  7.49it/s]predicting train subjects sagittal:  51%|█████▏    | 127/247 [00:15<00:15,  7.51it/s]predicting train subjects sagittal:  52%|█████▏    | 128/247 [00:15<00:15,  7.55it/s]predicting train subjects sagittal:  52%|█████▏    | 129/247 [00:15<00:15,  7.48it/s]predicting train subjects sagittal:  53%|█████▎    | 130/247 [00:15<00:15,  7.57it/s]predicting train subjects sagittal:  53%|█████▎    | 131/247 [00:16<00:15,  7.63it/s]predicting train subjects sagittal:  53%|█████▎    | 132/247 [00:16<00:14,  7.70it/s]predicting train subjects sagittal:  54%|█████▍    | 133/247 [00:16<00:14,  7.70it/s]predicting train subjects sagittal:  54%|█████▍    | 134/247 [00:16<00:14,  7.63it/s]predicting train subjects sagittal:  55%|█████▍    | 135/247 [00:16<00:14,  7.65it/s]predicting train subjects sagittal:  55%|█████▌    | 136/247 [00:16<00:13,  8.13it/s]predicting train subjects sagittal:  55%|█████▌    | 137/247 [00:16<00:12,  8.50it/s]predicting train subjects sagittal:  56%|█████▌    | 138/247 [00:16<00:12,  8.76it/s]predicting train subjects sagittal:  56%|█████▋    | 139/247 [00:17<00:12,  8.98it/s]predicting train subjects sagittal:  57%|█████▋    | 140/247 [00:17<00:11,  9.23it/s]predicting train subjects sagittal:  57%|█████▋    | 141/247 [00:17<00:11,  9.17it/s]predicting train subjects sagittal:  57%|█████▋    | 142/247 [00:17<00:11,  9.38it/s]predicting train subjects sagittal:  58%|█████▊    | 143/247 [00:17<00:10,  9.48it/s]predicting train subjects sagittal:  58%|█████▊    | 144/247 [00:17<00:10,  9.46it/s]predicting train subjects sagittal:  59%|█████▊    | 145/247 [00:17<00:10,  9.49it/s]predicting train subjects sagittal:  59%|█████▉    | 146/247 [00:17<00:10,  9.46it/s]predicting train subjects sagittal:  60%|█████▉    | 147/247 [00:17<00:10,  9.47it/s]predicting train subjects sagittal:  60%|█████▉    | 148/247 [00:18<00:10,  9.42it/s]predicting train subjects sagittal:  60%|██████    | 149/247 [00:18<00:10,  9.41it/s]predicting train subjects sagittal:  61%|██████    | 150/247 [00:18<00:10,  9.41it/s]predicting train subjects sagittal:  61%|██████    | 151/247 [00:18<00:10,  9.26it/s]predicting train subjects sagittal:  62%|██████▏   | 152/247 [00:18<00:10,  9.30it/s]predicting train subjects sagittal:  62%|██████▏   | 153/247 [00:18<00:10,  9.32it/s]predicting train subjects sagittal:  62%|██████▏   | 154/247 [00:18<00:10,  9.08it/s]predicting train subjects sagittal:  63%|██████▎   | 155/247 [00:18<00:10,  8.92it/s]predicting train subjects sagittal:  63%|██████▎   | 156/247 [00:18<00:10,  8.84it/s]predicting train subjects sagittal:  64%|██████▎   | 157/247 [00:19<00:10,  8.79it/s]predicting train subjects sagittal:  64%|██████▍   | 158/247 [00:19<00:10,  8.70it/s]predicting train subjects sagittal:  64%|██████▍   | 159/247 [00:19<00:10,  8.63it/s]predicting train subjects sagittal:  65%|██████▍   | 160/247 [00:19<00:10,  8.59it/s]predicting train subjects sagittal:  65%|██████▌   | 161/247 [00:19<00:10,  8.56it/s]predicting train subjects sagittal:  66%|██████▌   | 162/247 [00:19<00:09,  8.51it/s]predicting train subjects sagittal:  66%|██████▌   | 163/247 [00:19<00:09,  8.50it/s]predicting train subjects sagittal:  66%|██████▋   | 164/247 [00:19<00:09,  8.54it/s]predicting train subjects sagittal:  67%|██████▋   | 165/247 [00:19<00:09,  8.62it/s]predicting train subjects sagittal:  67%|██████▋   | 166/247 [00:20<00:09,  8.65it/s]predicting train subjects sagittal:  68%|██████▊   | 167/247 [00:20<00:09,  8.68it/s]predicting train subjects sagittal:  68%|██████▊   | 168/247 [00:20<00:09,  8.72it/s]predicting train subjects sagittal:  68%|██████▊   | 169/247 [00:20<00:08,  8.73it/s]predicting train subjects sagittal:  69%|██████▉   | 170/247 [00:20<00:08,  8.75it/s]predicting train subjects sagittal:  69%|██████▉   | 171/247 [00:20<00:08,  8.79it/s]predicting train subjects sagittal:  70%|██████▉   | 172/247 [00:20<00:08,  8.67it/s]predicting train subjects sagittal:  70%|███████   | 173/247 [00:20<00:08,  8.80it/s]predicting train subjects sagittal:  70%|███████   | 174/247 [00:20<00:08,  8.84it/s]predicting train subjects sagittal:  71%|███████   | 175/247 [00:21<00:08,  8.36it/s]predicting train subjects sagittal:  71%|███████▏  | 176/247 [00:21<00:08,  8.41it/s]predicting train subjects sagittal:  72%|███████▏  | 177/247 [00:21<00:08,  8.46it/s]predicting train subjects sagittal:  72%|███████▏  | 178/247 [00:21<00:08,  8.52it/s]predicting train subjects sagittal:  72%|███████▏  | 179/247 [00:21<00:07,  8.52it/s]predicting train subjects sagittal:  73%|███████▎  | 180/247 [00:21<00:07,  8.55it/s]predicting train subjects sagittal:  73%|███████▎  | 181/247 [00:21<00:07,  8.61it/s]predicting train subjects sagittal:  74%|███████▎  | 182/247 [00:21<00:07,  8.65it/s]predicting train subjects sagittal:  74%|███████▍  | 183/247 [00:22<00:07,  8.67it/s]predicting train subjects sagittal:  74%|███████▍  | 184/247 [00:22<00:07,  8.70it/s]predicting train subjects sagittal:  75%|███████▍  | 185/247 [00:22<00:07,  8.73it/s]predicting train subjects sagittal:  75%|███████▌  | 186/247 [00:22<00:06,  8.78it/s]predicting train subjects sagittal:  76%|███████▌  | 187/247 [00:22<00:06,  8.79it/s]predicting train subjects sagittal:  76%|███████▌  | 188/247 [00:22<00:06,  8.77it/s]predicting train subjects sagittal:  77%|███████▋  | 189/247 [00:22<00:06,  8.68it/s]predicting train subjects sagittal:  77%|███████▋  | 190/247 [00:22<00:06,  8.67it/s]predicting train subjects sagittal:  77%|███████▋  | 191/247 [00:22<00:06,  8.67it/s]predicting train subjects sagittal:  78%|███████▊  | 192/247 [00:23<00:06,  8.64it/s]predicting train subjects sagittal:  78%|███████▊  | 193/247 [00:23<00:06,  8.55it/s]predicting train subjects sagittal:  79%|███████▊  | 194/247 [00:23<00:06,  8.74it/s]predicting train subjects sagittal:  79%|███████▉  | 195/247 [00:23<00:05,  8.82it/s]predicting train subjects sagittal:  79%|███████▉  | 196/247 [00:23<00:05,  8.95it/s]predicting train subjects sagittal:  80%|███████▉  | 197/247 [00:23<00:05,  8.97it/s]predicting train subjects sagittal:  80%|████████  | 198/247 [00:23<00:05,  9.00it/s]predicting train subjects sagittal:  81%|████████  | 199/247 [00:23<00:05,  9.00it/s]predicting train subjects sagittal:  81%|████████  | 200/247 [00:23<00:05,  9.03it/s]predicting train subjects sagittal:  81%|████████▏ | 201/247 [00:24<00:05,  9.08it/s]predicting train subjects sagittal:  82%|████████▏ | 202/247 [00:24<00:04,  9.06it/s]predicting train subjects sagittal:  82%|████████▏ | 203/247 [00:24<00:04,  9.06it/s]predicting train subjects sagittal:  83%|████████▎ | 204/247 [00:24<00:04,  9.10it/s]predicting train subjects sagittal:  83%|████████▎ | 205/247 [00:24<00:04,  9.04it/s]predicting train subjects sagittal:  83%|████████▎ | 206/247 [00:24<00:04,  8.93it/s]predicting train subjects sagittal:  84%|████████▍ | 207/247 [00:24<00:04,  8.89it/s]predicting train subjects sagittal:  84%|████████▍ | 208/247 [00:24<00:04,  8.90it/s]predicting train subjects sagittal:  85%|████████▍ | 209/247 [00:24<00:04,  8.92it/s]predicting train subjects sagittal:  85%|████████▌ | 210/247 [00:25<00:04,  8.89it/s]predicting train subjects sagittal:  85%|████████▌ | 211/247 [00:25<00:04,  8.91it/s]predicting train subjects sagittal:  86%|████████▌ | 212/247 [00:25<00:04,  8.70it/s]predicting train subjects sagittal:  86%|████████▌ | 213/247 [00:25<00:03,  8.70it/s]predicting train subjects sagittal:  87%|████████▋ | 214/247 [00:25<00:03,  8.66it/s]predicting train subjects sagittal:  87%|████████▋ | 215/247 [00:25<00:03,  8.63it/s]predicting train subjects sagittal:  87%|████████▋ | 216/247 [00:25<00:03,  8.58it/s]predicting train subjects sagittal:  88%|████████▊ | 217/247 [00:25<00:03,  8.61it/s]predicting train subjects sagittal:  88%|████████▊ | 218/247 [00:25<00:03,  8.65it/s]predicting train subjects sagittal:  89%|████████▊ | 219/247 [00:26<00:03,  8.58it/s]predicting train subjects sagittal:  89%|████████▉ | 220/247 [00:26<00:03,  8.55it/s]predicting train subjects sagittal:  89%|████████▉ | 221/247 [00:26<00:03,  8.42it/s]predicting train subjects sagittal:  90%|████████▉ | 222/247 [00:26<00:02,  8.41it/s]predicting train subjects sagittal:  90%|█████████ | 223/247 [00:26<00:02,  8.41it/s]predicting train subjects sagittal:  91%|█████████ | 224/247 [00:26<00:02,  8.44it/s]predicting train subjects sagittal:  91%|█████████ | 225/247 [00:26<00:02,  8.46it/s]predicting train subjects sagittal:  91%|█████████▏| 226/247 [00:26<00:02,  8.39it/s]predicting train subjects sagittal:  92%|█████████▏| 227/247 [00:27<00:02,  8.43it/s]predicting train subjects sagittal:  92%|█████████▏| 228/247 [00:27<00:02,  8.41it/s]predicting train subjects sagittal:  93%|█████████▎| 229/247 [00:27<00:02,  8.42it/s]predicting train subjects sagittal:  93%|█████████▎| 230/247 [00:27<00:02,  8.03it/s]predicting train subjects sagittal:  94%|█████████▎| 231/247 [00:27<00:02,  7.85it/s]predicting train subjects sagittal:  94%|█████████▍| 232/247 [00:27<00:01,  7.69it/s]predicting train subjects sagittal:  94%|█████████▍| 233/247 [00:27<00:01,  7.61it/s]predicting train subjects sagittal:  95%|█████████▍| 234/247 [00:27<00:01,  7.56it/s]predicting train subjects sagittal:  95%|█████████▌| 235/247 [00:28<00:01,  7.53it/s]predicting train subjects sagittal:  96%|█████████▌| 236/247 [00:28<00:01,  7.45it/s]predicting train subjects sagittal:  96%|█████████▌| 237/247 [00:28<00:01,  7.42it/s]predicting train subjects sagittal:  96%|█████████▋| 238/247 [00:28<00:01,  7.37it/s]predicting train subjects sagittal:  97%|█████████▋| 239/247 [00:28<00:01,  7.25it/s]predicting train subjects sagittal:  97%|█████████▋| 240/247 [00:28<00:00,  7.22it/s]predicting train subjects sagittal:  98%|█████████▊| 241/247 [00:28<00:00,  7.21it/s]predicting train subjects sagittal:  98%|█████████▊| 242/247 [00:29<00:00,  7.21it/s]predicting train subjects sagittal:  98%|█████████▊| 243/247 [00:29<00:00,  7.17it/s]predicting train subjects sagittal:  99%|█████████▉| 244/247 [00:29<00:00,  7.17it/s]predicting train subjects sagittal:  99%|█████████▉| 245/247 [00:29<00:00,  7.16it/s]predicting train subjects sagittal: 100%|█████████▉| 246/247 [00:29<00:00,  7.18it/s]predicting train subjects sagittal: 100%|██████████| 247/247 [00:29<00:00,  7.03it/s]predicting train subjects sagittal: 100%|██████████| 247/247 [00:29<00:00,  8.29it/s]
saving BB  test1-THALAMUS:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 5/5 [00:00<00:00, 69.69it/s]
saving BB  train1-THALAMUS:   0%|          | 0/247 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   4%|▎         | 9/247 [00:00<00:02, 83.29it/s]saving BB  train1-THALAMUS:   7%|▋         | 18/247 [00:00<00:02, 82.81it/s]saving BB  train1-THALAMUS:  11%|█         | 27/247 [00:00<00:02, 82.53it/s]saving BB  train1-THALAMUS:  15%|█▍        | 36/247 [00:00<00:02, 84.53it/s]saving BB  train1-THALAMUS:  18%|█▊        | 44/247 [00:00<00:02, 81.74it/s]saving BB  train1-THALAMUS:  22%|██▏       | 54/247 [00:00<00:02, 84.63it/s]saving BB  train1-THALAMUS:  25%|██▌       | 62/247 [00:00<00:02, 81.23it/s]saving BB  train1-THALAMUS:  28%|██▊       | 70/247 [00:00<00:02, 78.45it/s]saving BB  train1-THALAMUS:  32%|███▏      | 78/247 [00:00<00:02, 75.72it/s]saving BB  train1-THALAMUS:  35%|███▍      | 86/247 [00:01<00:02, 76.91it/s]saving BB  train1-THALAMUS:  38%|███▊      | 95/247 [00:01<00:01, 78.12it/s]saving BB  train1-THALAMUS:  42%|████▏     | 103/247 [00:01<00:01, 76.80it/s]saving BB  train1-THALAMUS:  45%|████▍     | 111/247 [00:01<00:01, 76.66it/s]saving BB  train1-THALAMUS:  48%|████▊     | 119/247 [00:01<00:01, 76.63it/s]saving BB  train1-THALAMUS:  51%|█████▏    | 127/247 [00:01<00:01, 77.04it/s]saving BB  train1-THALAMUS:  55%|█████▍    | 135/247 [00:01<00:01, 76.59it/s]saving BB  train1-THALAMUS:  58%|█████▊    | 144/247 [00:01<00:01, 80.12it/s]saving BB  train1-THALAMUS:  62%|██████▏   | 153/247 [00:01<00:01, 81.24it/s]saving BB  train1-THALAMUS:  66%|██████▌   | 163/247 [00:02<00:01, 83.95it/s]saving BB  train1-THALAMUS:  70%|███████   | 173/247 [00:02<00:00, 85.62it/s]saving BB  train1-THALAMUS:  74%|███████▎  | 182/247 [00:02<00:00, 83.76it/s]saving BB  train1-THALAMUS:  77%|███████▋  | 191/247 [00:02<00:00, 80.04it/s]saving BB  train1-THALAMUS:  81%|████████  | 200/247 [00:02<00:00, 82.01it/s]saving BB  train1-THALAMUS:  85%|████████▍ | 209/247 [00:02<00:00, 83.51it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 218/247 [00:02<00:00, 85.21it/s]saving BB  train1-THALAMUS:  92%|█████████▏| 227/247 [00:02<00:00, 84.86it/s]saving BB  train1-THALAMUS:  96%|█████████▌| 236/247 [00:02<00:00, 82.02it/s]saving BB  train1-THALAMUS:  99%|█████████▉| 245/247 [00:03<00:00, 79.27it/s]saving BB  train1-THALAMUS: 100%|██████████| 247/247 [00:03<00:00, 80.65it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 5/5 [00:00<00:00, 74.36it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/247 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   4%|▎         | 9/247 [00:00<00:02, 81.21it/s]saving BB  train1-THALAMUS Sagittal:   7%|▋         | 18/247 [00:00<00:02, 81.34it/s]saving BB  train1-THALAMUS Sagittal:  11%|█         | 27/247 [00:00<00:02, 81.75it/s]saving BB  train1-THALAMUS Sagittal:  15%|█▍        | 37/247 [00:00<00:02, 84.48it/s]saving BB  train1-THALAMUS Sagittal:  19%|█▉        | 47/247 [00:00<00:02, 86.91it/s]saving BB  train1-THALAMUS Sagittal:  23%|██▎       | 57/247 [00:00<00:02, 88.26it/s]saving BB  train1-THALAMUS Sagittal:  26%|██▋       | 65/247 [00:00<00:02, 84.25it/s]saving BB  train1-THALAMUS Sagittal:  30%|██▉       | 73/247 [00:00<00:02, 82.88it/s]saving BB  train1-THALAMUS Sagittal:  33%|███▎      | 82/247 [00:00<00:01, 82.58it/s]saving BB  train1-THALAMUS Sagittal:  37%|███▋      | 91/247 [00:01<00:01, 82.43it/s]saving BB  train1-THALAMUS Sagittal:  40%|████      | 100/247 [00:01<00:01, 82.00it/s]saving BB  train1-THALAMUS Sagittal:  44%|████▍     | 109/247 [00:01<00:01, 80.37it/s]saving BB  train1-THALAMUS Sagittal:  47%|████▋     | 117/247 [00:01<00:01, 78.70it/s]saving BB  train1-THALAMUS Sagittal:  51%|█████     | 125/247 [00:01<00:01, 78.33it/s]saving BB  train1-THALAMUS Sagittal:  54%|█████▍    | 133/247 [00:01<00:01, 76.55it/s]saving BB  train1-THALAMUS Sagittal:  57%|█████▋    | 142/247 [00:01<00:01, 79.00it/s]saving BB  train1-THALAMUS Sagittal:  62%|██████▏   | 152/247 [00:01<00:01, 82.30it/s]saving BB  train1-THALAMUS Sagittal:  66%|██████▌   | 162/247 [00:01<00:01, 84.35it/s]saving BB  train1-THALAMUS Sagittal:  69%|██████▉   | 171/247 [00:02<00:00, 85.89it/s]saving BB  train1-THALAMUS Sagittal:  73%|███████▎  | 180/247 [00:02<00:00, 83.48it/s]saving BB  train1-THALAMUS Sagittal:  77%|███████▋  | 189/247 [00:02<00:00, 80.33it/s]saving BB  train1-THALAMUS Sagittal:  80%|████████  | 198/247 [00:02<00:00, 79.29it/s]saving BB  train1-THALAMUS Sagittal:  84%|████████▍ | 207/247 [00:02<00:00, 81.74it/s]saving BB  train1-THALAMUS Sagittal:  87%|████████▋ | 216/247 [00:02<00:00, 82.02it/s]saving BB  train1-THALAMUS Sagittal:  91%|█████████ | 225/247 [00:02<00:00, 83.96it/s]saving BB  train1-THALAMUS Sagittal:  95%|█████████▍| 234/247 [00:02<00:00, 82.63it/s]saving BB  train1-THALAMUS Sagittal:  98%|█████████▊| 243/247 [00:02<00:00, 77.07it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 247/247 [00:03<00:00, 81.70it/s]
Loading train:   0%|          | 0/247 [00:00<?, ?it/s]Loading train:   0%|          | 1/247 [00:00<03:54,  1.05it/s]Loading train:   1%|          | 2/247 [00:01<03:39,  1.12it/s]Loading train:   1%|          | 3/247 [00:02<03:26,  1.18it/s]Loading train:   2%|▏         | 4/247 [00:03<03:27,  1.17it/s]Loading train:   2%|▏         | 5/247 [00:03<03:06,  1.30it/s]Loading train:   2%|▏         | 6/247 [00:04<02:50,  1.41it/s]Loading train:   3%|▎         | 7/247 [00:05<02:38,  1.51it/s]Loading train:   3%|▎         | 8/247 [00:05<02:30,  1.58it/s]Loading train:   4%|▎         | 9/247 [00:06<02:25,  1.64it/s]Loading train:   4%|▍         | 10/247 [00:06<02:19,  1.70it/s]Loading train:   4%|▍         | 11/247 [00:07<02:15,  1.75it/s]Loading train:   5%|▍         | 12/247 [00:07<02:11,  1.79it/s]Loading train:   5%|▌         | 13/247 [00:08<02:08,  1.83it/s]Loading train:   6%|▌         | 14/247 [00:08<02:06,  1.84it/s]Loading train:   6%|▌         | 15/247 [00:09<02:05,  1.85it/s]Loading train:   6%|▋         | 16/247 [00:09<02:05,  1.85it/s]Loading train:   7%|▋         | 17/247 [00:10<02:05,  1.83it/s]Loading train:   7%|▋         | 18/247 [00:10<02:06,  1.81it/s]Loading train:   8%|▊         | 19/247 [00:11<02:04,  1.83it/s]Loading train:   8%|▊         | 20/247 [00:12<02:02,  1.85it/s]Loading train:   9%|▊         | 21/247 [00:12<02:01,  1.86it/s]Loading train:   9%|▉         | 22/247 [00:13<02:01,  1.84it/s]Loading train:   9%|▉         | 23/247 [00:13<02:01,  1.84it/s]Loading train:  10%|▉         | 24/247 [00:14<02:00,  1.85it/s]Loading train:  10%|█         | 25/247 [00:14<01:58,  1.88it/s]Loading train:  11%|█         | 26/247 [00:15<01:54,  1.94it/s]Loading train:  11%|█         | 27/247 [00:15<01:52,  1.96it/s]Loading train:  11%|█▏        | 28/247 [00:16<01:48,  2.01it/s]Loading train:  12%|█▏        | 29/247 [00:16<01:48,  2.01it/s]Loading train:  12%|█▏        | 30/247 [00:17<01:45,  2.06it/s]Loading train:  13%|█▎        | 31/247 [00:17<01:44,  2.07it/s]Loading train:  13%|█▎        | 32/247 [00:18<01:44,  2.06it/s]Loading train:  13%|█▎        | 33/247 [00:18<01:45,  2.03it/s]Loading train:  14%|█▍        | 34/247 [00:19<01:43,  2.05it/s]Loading train:  14%|█▍        | 35/247 [00:19<01:45,  2.02it/s]Loading train:  15%|█▍        | 36/247 [00:20<01:45,  2.01it/s]Loading train:  15%|█▍        | 37/247 [00:20<01:47,  1.95it/s]Loading train:  15%|█▌        | 38/247 [00:21<01:46,  1.96it/s]Loading train:  16%|█▌        | 39/247 [00:21<01:47,  1.94it/s]Loading train:  16%|█▌        | 40/247 [00:22<01:47,  1.93it/s]Loading train:  17%|█▋        | 41/247 [00:22<01:44,  1.98it/s]Loading train:  17%|█▋        | 42/247 [00:23<01:41,  2.02it/s]Loading train:  17%|█▋        | 43/247 [00:23<01:38,  2.06it/s]Loading train:  18%|█▊        | 44/247 [00:24<01:38,  2.05it/s]Loading train:  18%|█▊        | 45/247 [00:24<01:37,  2.08it/s]Loading train:  19%|█▊        | 46/247 [00:25<01:35,  2.11it/s]Loading train:  19%|█▉        | 47/247 [00:25<01:33,  2.15it/s]Loading train:  19%|█▉        | 48/247 [00:25<01:32,  2.15it/s]Loading train:  20%|█▉        | 49/247 [00:26<01:32,  2.15it/s]Loading train:  20%|██        | 50/247 [00:26<01:31,  2.15it/s]Loading train:  21%|██        | 51/247 [00:27<01:30,  2.17it/s]Loading train:  21%|██        | 52/247 [00:27<01:28,  2.21it/s]Loading train:  21%|██▏       | 53/247 [00:28<01:27,  2.22it/s]Loading train:  22%|██▏       | 54/247 [00:28<01:27,  2.21it/s]Loading train:  22%|██▏       | 55/247 [00:29<01:27,  2.19it/s]Loading train:  23%|██▎       | 56/247 [00:29<01:28,  2.17it/s]Loading train:  23%|██▎       | 57/247 [00:30<01:26,  2.20it/s]Loading train:  23%|██▎       | 58/247 [00:30<01:25,  2.21it/s]Loading train:  24%|██▍       | 59/247 [00:30<01:29,  2.09it/s]Loading train:  24%|██▍       | 60/247 [00:31<01:33,  2.01it/s]Loading train:  25%|██▍       | 61/247 [00:32<01:33,  1.99it/s]Loading train:  25%|██▌       | 62/247 [00:32<01:33,  1.97it/s]Loading train:  26%|██▌       | 63/247 [00:33<01:35,  1.93it/s]Loading train:  26%|██▌       | 64/247 [00:33<01:35,  1.91it/s]Loading train:  26%|██▋       | 65/247 [00:34<01:36,  1.89it/s]Loading train:  27%|██▋       | 66/247 [00:34<01:35,  1.89it/s]Loading train:  27%|██▋       | 67/247 [00:35<01:35,  1.88it/s]Loading train:  28%|██▊       | 68/247 [00:35<01:35,  1.87it/s]Loading train:  28%|██▊       | 69/247 [00:36<01:34,  1.88it/s]Loading train:  28%|██▊       | 70/247 [00:36<01:34,  1.87it/s]Loading train:  29%|██▊       | 71/247 [00:37<01:34,  1.85it/s]Loading train:  29%|██▉       | 72/247 [00:37<01:33,  1.87it/s]Loading train:  30%|██▉       | 73/247 [00:38<01:33,  1.86it/s]Loading train:  30%|██▉       | 74/247 [00:39<01:32,  1.87it/s]Loading train:  30%|███       | 75/247 [00:39<01:32,  1.86it/s]Loading train:  31%|███       | 76/247 [00:40<01:32,  1.84it/s]Loading train:  31%|███       | 77/247 [00:41<01:51,  1.52it/s]Loading train:  32%|███▏      | 78/247 [00:42<02:06,  1.33it/s]Loading train:  32%|███▏      | 79/247 [00:42<02:08,  1.30it/s]Loading train:  32%|███▏      | 80/247 [00:43<02:01,  1.37it/s]Loading train:  33%|███▎      | 81/247 [00:44<02:00,  1.37it/s]Loading train:  33%|███▎      | 82/247 [00:44<01:50,  1.49it/s]Loading train:  34%|███▎      | 83/247 [00:45<01:42,  1.60it/s]Loading train:  34%|███▍      | 84/247 [00:45<01:38,  1.66it/s]Loading train:  34%|███▍      | 85/247 [00:46<01:35,  1.70it/s]Loading train:  35%|███▍      | 86/247 [00:46<01:32,  1.74it/s]Loading train:  35%|███▌      | 87/247 [00:47<01:30,  1.77it/s]Loading train:  36%|███▌      | 88/247 [00:47<01:29,  1.78it/s]Loading train:  36%|███▌      | 89/247 [00:48<01:28,  1.79it/s]Loading train:  36%|███▋      | 90/247 [00:49<01:27,  1.79it/s]Loading train:  37%|███▋      | 91/247 [00:49<01:25,  1.82it/s]Loading train:  37%|███▋      | 92/247 [00:50<01:24,  1.83it/s]Loading train:  38%|███▊      | 93/247 [00:50<01:24,  1.83it/s]Loading train:  38%|███▊      | 94/247 [00:51<01:22,  1.86it/s]Loading train:  38%|███▊      | 95/247 [00:51<01:20,  1.89it/s]Loading train:  39%|███▉      | 96/247 [00:52<01:21,  1.86it/s]Loading train:  39%|███▉      | 97/247 [00:52<01:21,  1.83it/s]Loading train:  40%|███▉      | 98/247 [00:53<01:21,  1.83it/s]Loading train:  40%|████      | 99/247 [00:53<01:22,  1.79it/s]Loading train:  40%|████      | 100/247 [00:54<01:26,  1.70it/s]Loading train:  41%|████      | 101/247 [00:55<01:24,  1.72it/s]Loading train:  41%|████▏     | 102/247 [00:55<01:25,  1.69it/s]Loading train:  42%|████▏     | 103/247 [00:56<01:25,  1.69it/s]Loading train:  42%|████▏     | 104/247 [00:57<01:24,  1.69it/s]Loading train:  43%|████▎     | 105/247 [00:57<01:25,  1.66it/s]Loading train:  43%|████▎     | 106/247 [00:58<01:24,  1.66it/s]Loading train:  43%|████▎     | 107/247 [00:58<01:24,  1.67it/s]Loading train:  44%|████▎     | 108/247 [00:59<01:23,  1.66it/s]Loading train:  44%|████▍     | 109/247 [01:00<01:24,  1.63it/s]Loading train:  45%|████▍     | 110/247 [01:00<01:23,  1.64it/s]Loading train:  45%|████▍     | 111/247 [01:01<01:23,  1.63it/s]Loading train:  45%|████▌     | 112/247 [01:01<01:22,  1.64it/s]Loading train:  46%|████▌     | 113/247 [01:02<01:21,  1.65it/s]Loading train:  46%|████▌     | 114/247 [01:03<01:20,  1.66it/s]Loading train:  47%|████▋     | 115/247 [01:03<01:20,  1.64it/s]Loading train:  47%|████▋     | 116/247 [01:04<01:20,  1.64it/s]Loading train:  47%|████▋     | 117/247 [01:04<01:18,  1.65it/s]Loading train:  48%|████▊     | 118/247 [01:05<01:18,  1.65it/s]Loading train:  48%|████▊     | 119/247 [01:06<01:17,  1.66it/s]Loading train:  49%|████▊     | 120/247 [01:06<01:14,  1.71it/s]Loading train:  49%|████▉     | 121/247 [01:07<01:12,  1.74it/s]Loading train:  49%|████▉     | 122/247 [01:07<01:11,  1.75it/s]Loading train:  50%|████▉     | 123/247 [01:08<01:13,  1.70it/s]Loading train:  50%|█████     | 124/247 [01:09<01:13,  1.68it/s]Loading train:  51%|█████     | 125/247 [01:09<01:11,  1.70it/s]Loading train:  51%|█████     | 126/247 [01:10<01:10,  1.72it/s]Loading train:  51%|█████▏    | 127/247 [01:10<01:09,  1.73it/s]Loading train:  52%|█████▏    | 128/247 [01:11<01:08,  1.74it/s]Loading train:  52%|█████▏    | 129/247 [01:11<01:07,  1.74it/s]Loading train:  53%|█████▎    | 130/247 [01:12<01:07,  1.74it/s]Loading train:  53%|█████▎    | 131/247 [01:13<01:06,  1.74it/s]Loading train:  53%|█████▎    | 132/247 [01:13<01:07,  1.70it/s]Loading train:  54%|█████▍    | 133/247 [01:14<01:08,  1.67it/s]Loading train:  54%|█████▍    | 134/247 [01:14<01:08,  1.65it/s]Loading train:  55%|█████▍    | 135/247 [01:15<01:08,  1.64it/s]Loading train:  55%|█████▌    | 136/247 [01:16<01:04,  1.72it/s]Loading train:  55%|█████▌    | 137/247 [01:16<01:00,  1.82it/s]Loading train:  56%|█████▌    | 138/247 [01:16<00:57,  1.90it/s]Loading train:  56%|█████▋    | 139/247 [01:17<00:54,  1.97it/s]Loading train:  57%|█████▋    | 140/247 [01:17<00:53,  2.00it/s]Loading train:  57%|█████▋    | 141/247 [01:18<00:52,  2.03it/s]Loading train:  57%|█████▋    | 142/247 [01:18<00:51,  2.04it/s]Loading train:  58%|█████▊    | 143/247 [01:19<00:51,  2.04it/s]Loading train:  58%|█████▊    | 144/247 [01:19<00:50,  2.06it/s]Loading train:  59%|█████▊    | 145/247 [01:20<00:49,  2.06it/s]Loading train:  59%|█████▉    | 146/247 [01:20<00:50,  2.00it/s]Loading train:  60%|█████▉    | 147/247 [01:21<00:49,  2.01it/s]Loading train:  60%|█████▉    | 148/247 [01:21<00:48,  2.03it/s]Loading train:  60%|██████    | 149/247 [01:22<00:48,  2.03it/s]Loading train:  61%|██████    | 150/247 [01:22<00:47,  2.04it/s]Loading train:  61%|██████    | 151/247 [01:23<00:47,  2.04it/s]Loading train:  62%|██████▏   | 152/247 [01:23<00:46,  2.05it/s]Loading train:  62%|██████▏   | 153/247 [01:24<00:45,  2.08it/s]Loading train:  62%|██████▏   | 154/247 [01:24<00:46,  1.98it/s]Loading train:  63%|██████▎   | 155/247 [01:25<00:47,  1.95it/s]Loading train:  63%|██████▎   | 156/247 [01:25<00:47,  1.92it/s]Loading train:  64%|██████▎   | 157/247 [01:26<00:47,  1.90it/s]Loading train:  64%|██████▍   | 158/247 [01:26<00:47,  1.87it/s]Loading train:  64%|██████▍   | 159/247 [01:27<00:51,  1.72it/s]Loading train:  65%|██████▍   | 160/247 [01:29<01:13,  1.18it/s]Loading train:  65%|██████▌   | 161/247 [01:31<01:50,  1.28s/it]Loading train:  66%|██████▌   | 162/247 [01:34<02:44,  1.93s/it]Loading train:  66%|██████▌   | 163/247 [01:38<03:24,  2.44s/it]Loading train:  66%|██████▋   | 164/247 [01:43<04:25,  3.20s/it]Loading train:  67%|██████▋   | 165/247 [01:48<04:55,  3.61s/it]Loading train:  67%|██████▋   | 166/247 [01:52<05:06,  3.78s/it]Loading train:  68%|██████▊   | 167/247 [01:56<05:07,  3.85s/it]Loading train:  68%|██████▊   | 168/247 [02:00<05:06,  3.88s/it]Loading train:  68%|██████▊   | 169/247 [02:03<04:59,  3.84s/it]Loading train:  69%|██████▉   | 170/247 [02:07<04:55,  3.84s/it]Loading train:  69%|██████▉   | 171/247 [02:11<04:45,  3.76s/it]Loading train:  70%|██████▉   | 172/247 [02:21<06:59,  5.60s/it]Loading train:  70%|███████   | 173/247 [02:27<07:12,  5.84s/it]Loading train:  70%|███████   | 174/247 [02:34<07:33,  6.22s/it]Loading train:  71%|███████   | 175/247 [02:46<09:20,  7.78s/it]Loading train:  71%|███████▏  | 176/247 [02:49<07:35,  6.42s/it]Loading train:  72%|███████▏  | 177/247 [02:51<06:04,  5.20s/it]Loading train:  72%|███████▏  | 178/247 [02:54<04:57,  4.32s/it]Loading train:  72%|███████▏  | 179/247 [02:56<04:15,  3.76s/it]Loading train:  73%|███████▎  | 180/247 [02:58<03:42,  3.32s/it]Loading train:  73%|███████▎  | 181/247 [03:01<03:20,  3.03s/it]Loading train:  74%|███████▎  | 182/247 [03:03<03:02,  2.80s/it]Loading train:  74%|███████▍  | 183/247 [03:05<02:51,  2.68s/it]Loading train:  74%|███████▍  | 184/247 [03:08<02:44,  2.61s/it]Loading train:  75%|███████▍  | 185/247 [03:10<02:38,  2.55s/it]Loading train:  75%|███████▌  | 186/247 [03:12<02:31,  2.48s/it]Loading train:  76%|███████▌  | 187/247 [03:15<02:25,  2.42s/it]Loading train:  76%|███████▌  | 188/247 [03:17<02:20,  2.38s/it]Loading train:  77%|███████▋  | 189/247 [03:19<02:17,  2.37s/it]Loading train:  77%|███████▋  | 190/247 [03:22<02:16,  2.39s/it]Loading train:  77%|███████▋  | 191/247 [03:24<02:13,  2.39s/it]Loading train:  78%|███████▊  | 192/247 [03:27<02:14,  2.44s/it]Loading train:  78%|███████▊  | 193/247 [03:29<02:10,  2.42s/it]Loading train:  79%|███████▊  | 194/247 [03:30<01:49,  2.07s/it]Loading train:  79%|███████▉  | 195/247 [03:31<01:28,  1.70s/it]Loading train:  79%|███████▉  | 196/247 [03:32<01:13,  1.44s/it]Loading train:  80%|███████▉  | 197/247 [03:33<01:02,  1.25s/it]Loading train:  80%|████████  | 198/247 [03:34<00:55,  1.12s/it]Loading train:  81%|████████  | 199/247 [03:35<00:49,  1.03s/it]Loading train:  81%|████████  | 200/247 [03:36<00:47,  1.02s/it]Loading train:  81%|████████▏ | 201/247 [03:36<00:44,  1.04it/s]Loading train:  82%|████████▏ | 202/247 [03:37<00:40,  1.10it/s]Loading train:  82%|████████▏ | 203/247 [03:38<00:39,  1.11it/s]Loading train:  83%|████████▎ | 204/247 [03:39<00:37,  1.15it/s]Loading train:  83%|████████▎ | 205/247 [03:40<00:35,  1.18it/s]Loading train:  83%|████████▎ | 206/247 [03:40<00:35,  1.17it/s]Loading train:  84%|████████▍ | 207/247 [03:41<00:33,  1.18it/s]Loading train:  84%|████████▍ | 208/247 [03:42<00:32,  1.21it/s]Loading train:  85%|████████▍ | 209/247 [03:43<00:30,  1.23it/s]Loading train:  85%|████████▌ | 210/247 [03:44<00:30,  1.20it/s]Loading train:  85%|████████▌ | 211/247 [03:45<00:30,  1.16it/s]Loading train:  86%|████████▌ | 212/247 [03:47<00:41,  1.19s/it]Loading train:  86%|████████▌ | 213/247 [03:49<00:51,  1.53s/it]Loading train:  87%|████████▋ | 214/247 [03:51<00:57,  1.73s/it]Loading train:  87%|████████▋ | 215/247 [03:53<01:00,  1.88s/it]Loading train:  87%|████████▋ | 216/247 [03:56<01:02,  2.00s/it]Loading train:  88%|████████▊ | 217/247 [03:58<01:01,  2.05s/it]Loading train:  88%|████████▊ | 218/247 [04:00<01:00,  2.10s/it]Loading train:  89%|████████▊ | 219/247 [04:02<01:00,  2.15s/it]Loading train:  89%|████████▉ | 220/247 [04:04<00:58,  2.15s/it]Loading train:  89%|████████▉ | 221/247 [04:07<00:56,  2.16s/it]Loading train:  90%|████████▉ | 222/247 [04:10<01:01,  2.45s/it]Loading train:  90%|█████████ | 223/247 [04:14<01:09,  2.91s/it]Loading train:  91%|█████████ | 224/247 [04:18<01:15,  3.28s/it]Loading train:  91%|█████████ | 225/247 [04:22<01:16,  3.46s/it]Loading train:  91%|█████████▏| 226/247 [04:26<01:16,  3.62s/it]Loading train:  92%|█████████▏| 227/247 [04:30<01:14,  3.71s/it]Loading train:  92%|█████████▏| 228/247 [04:34<01:13,  3.88s/it]Loading train:  93%|█████████▎| 229/247 [04:38<01:10,  3.92s/it]Loading train:  93%|█████████▎| 230/247 [04:44<01:16,  4.48s/it]Loading train:  94%|█████████▎| 231/247 [04:50<01:18,  4.94s/it]Loading train:  94%|█████████▍| 232/247 [04:55<01:17,  5.17s/it]Loading train:  94%|█████████▍| 233/247 [05:01<01:15,  5.36s/it]Loading train:  95%|█████████▍| 234/247 [05:07<01:10,  5.41s/it]Loading train:  95%|█████████▌| 235/247 [05:12<01:04,  5.40s/it]Loading train:  96%|█████████▌| 236/247 [05:18<01:00,  5.53s/it]Loading train:  96%|█████████▌| 237/247 [05:23<00:53,  5.35s/it]Loading train:  96%|█████████▋| 238/247 [05:28<00:47,  5.29s/it]Loading train:  97%|█████████▋| 239/247 [05:33<00:42,  5.29s/it]Loading train:  97%|█████████▋| 240/247 [05:39<00:37,  5.31s/it]Loading train:  98%|█████████▊| 241/247 [05:44<00:32,  5.36s/it]Loading train:  98%|█████████▊| 242/247 [05:49<00:26,  5.30s/it]Loading train:  98%|█████████▊| 243/247 [05:55<00:21,  5.30s/it]Loading train:  99%|█████████▉| 244/247 [06:00<00:15,  5.25s/it]Loading train:  99%|█████████▉| 245/247 [06:05<00:10,  5.27s/it]Loading train: 100%|█████████▉| 246/247 [06:10<00:05,  5.28s/it]Loading train: 100%|██████████| 247/247 [06:16<00:00,  5.36s/it]Loading train: 100%|██████████| 247/247 [06:16<00:00,  1.52s/it]
concatenating: train:   0%|          | 0/247 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/247 [00:00<00:04, 52.31it/s]concatenating: train:   5%|▍         | 12/247 [00:00<00:04, 51.81it/s]concatenating: train:   7%|▋         | 17/247 [00:00<00:04, 50.95it/s]concatenating: train:   9%|▉         | 23/247 [00:00<00:04, 52.49it/s]concatenating: train:  11%|█▏        | 28/247 [00:00<00:04, 51.48it/s]concatenating: train:  14%|█▍        | 34/247 [00:00<00:04, 50.81it/s]concatenating: train:  16%|█▌        | 39/247 [00:00<00:04, 49.53it/s]concatenating: train:  19%|█▊        | 46/247 [00:00<00:03, 52.10it/s]concatenating: train:  21%|██▏       | 53/247 [00:00<00:03, 55.06it/s]concatenating: train:  24%|██▍       | 60/247 [00:01<00:03, 57.58it/s]concatenating: train:  27%|██▋       | 66/247 [00:01<00:03, 55.68it/s]concatenating: train:  29%|██▉       | 72/247 [00:01<00:03, 53.74it/s]concatenating: train:  32%|███▏      | 78/247 [00:01<00:03, 51.27it/s]concatenating: train:  34%|███▍      | 84/247 [00:01<00:03, 52.59it/s]concatenating: train:  36%|███▋      | 90/247 [00:01<00:02, 53.80it/s]concatenating: train:  39%|███▉      | 96/247 [00:01<00:02, 53.00it/s]concatenating: train:  41%|████▏     | 102/247 [00:01<00:02, 51.58it/s]concatenating: train:  44%|████▎     | 108/247 [00:02<00:02, 50.79it/s]concatenating: train:  46%|████▌     | 114/247 [00:02<00:02, 50.66it/s]concatenating: train:  49%|████▊     | 120/247 [00:02<00:02, 51.71it/s]concatenating: train:  51%|█████▏    | 127/247 [00:02<00:02, 55.92it/s]concatenating: train:  54%|█████▍    | 134/247 [00:02<00:01, 58.84it/s]concatenating: train:  57%|█████▋    | 141/247 [00:02<00:01, 59.65it/s]concatenating: train:  60%|█████▉    | 148/247 [00:02<00:01, 58.83it/s]concatenating: train:  63%|██████▎   | 155/247 [00:02<00:01, 60.63it/s]concatenating: train:  66%|██████▌   | 162/247 [00:02<00:01, 57.97it/s]concatenating: train:  68%|██████▊   | 168/247 [00:03<00:01, 57.56it/s]concatenating: train:  70%|███████   | 174/247 [00:03<00:01, 55.38it/s]concatenating: train:  73%|███████▎  | 180/247 [00:03<00:01, 51.42it/s]concatenating: train:  75%|███████▌  | 186/247 [00:03<00:01, 48.69it/s]concatenating: train:  77%|███████▋  | 191/247 [00:03<00:01, 46.09it/s]concatenating: train:  79%|███████▉  | 196/247 [00:03<00:01, 45.43it/s]concatenating: train:  81%|████████▏ | 201/247 [00:03<00:01, 45.73it/s]concatenating: train:  84%|████████▍ | 207/247 [00:03<00:00, 47.65it/s]concatenating: train:  86%|████████▌ | 212/247 [00:04<00:00, 47.59it/s]concatenating: train:  88%|████████▊ | 218/247 [00:04<00:00, 48.13it/s]concatenating: train:  91%|█████████ | 224/247 [00:04<00:00, 48.90it/s]concatenating: train:  93%|█████████▎| 229/247 [00:04<00:00, 48.88it/s]concatenating: train:  95%|█████████▌| 235/247 [00:04<00:00, 50.81it/s]concatenating: train:  98%|█████████▊| 241/247 [00:04<00:00, 48.79it/s]concatenating: train: 100%|█████████▉| 246/247 [00:04<00:00, 47.80it/s]concatenating: train: 100%|██████████| 247/247 [00:04<00:00, 52.19it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:13<00:52, 13.10s/it]Loading test:  40%|████      | 2/5 [00:24<00:37, 12.66s/it]Loading test:  60%|██████    | 3/5 [00:31<00:21, 10.85s/it]Loading test:  80%|████████  | 4/5 [00:37<00:09,  9.36s/it]Loading test: 100%|██████████| 5/5 [00:45<00:00,  9.18s/it]Loading test: 100%|██████████| 5/5 [00:45<00:00,  9.20s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 56.73it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 92, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 92, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 92, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 92, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 92, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 92, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 92, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 46, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 46, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 46, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 46, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 46, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 46, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 46, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 46, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 46, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 23, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 23, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 23, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 23, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 23, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 23, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 23, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 23, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 23, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 23, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 46, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 46, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 46, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 46, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 46, 40)   0           batch_normalization_7[0][0]      2020-01-22 09:44:37.320717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 09:44:37.320808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 09:44:37.320821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 09:44:37.320829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 09:44:37.321104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 46, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 46, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 46, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 46, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 46, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 92, 20)   11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 92, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 92, 20)   7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 92, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 92, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 92, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 92, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 92, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 92, 60)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 92, 60)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 92, 13)   793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2
------------------------------------------------------------------
class_weights [6.22971902e-02 3.14227626e-02 7.86411660e-02 9.57840878e-03
 2.85407485e-02 7.22349637e-03 8.63067899e-02 1.15056075e-01
 8.99984596e-02 1.30533025e-02 2.93826737e-01 1.83830287e-01
 2.24577009e-04]
Train on 8989 samples, validate on 184 samples
Epoch 1/300
 - 24s - loss: 0.7441 - acc: 0.8835 - mDice: 0.1986 - val_loss: 0.4886 - val_acc: 0.8851 - val_mDice: 0.1901

Epoch 00001: val_mDice improved from -inf to 0.19009, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/300
 - 20s - loss: 0.6558 - acc: 0.9067 - mDice: 0.2933 - val_loss: 0.4373 - val_acc: 0.9045 - val_mDice: 0.2176

Epoch 00002: val_mDice improved from 0.19009 to 0.21757, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 3/300
 - 19s - loss: 0.6188 - acc: 0.9150 - mDice: 0.3331 - val_loss: 0.3726 - val_acc: 0.9113 - val_mDice: 0.2232

Epoch 00003: val_mDice improved from 0.21757 to 0.22319, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/300
 - 19s - loss: 0.5948 - acc: 0.9201 - mDice: 0.3589 - val_loss: 0.3547 - val_acc: 0.9154 - val_mDice: 0.2287

Epoch 00004: val_mDice improved from 0.22319 to 0.22866, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/300
 - 19s - loss: 0.5681 - acc: 0.9225 - mDice: 0.3877 - val_loss: 0.3773 - val_acc: 0.9177 - val_mDice: 0.2466

Epoch 00005: val_mDice improved from 0.22866 to 0.24659, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 6/300
 - 19s - loss: 0.5458 - acc: 0.9243 - mDice: 0.4118 - val_loss: 0.3582 - val_acc: 0.9237 - val_mDice: 0.2567

Epoch 00006: val_mDice improved from 0.24659 to 0.25666, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 7/300
 - 19s - loss: 0.5258 - acc: 0.9267 - mDice: 0.4334 - val_loss: 0.3088 - val_acc: 0.9291 - val_mDice: 0.2666

Epoch 00007: val_mDice improved from 0.25666 to 0.26661, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 8/300
 - 19s - loss: 0.5138 - acc: 0.9281 - mDice: 0.4464 - val_loss: 0.3132 - val_acc: 0.9238 - val_mDice: 0.2668

Epoch 00008: val_mDice improved from 0.26661 to 0.26682, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 9/300
 - 19s - loss: 0.5050 - acc: 0.9292 - mDice: 0.4559 - val_loss: 0.2959 - val_acc: 0.9281 - val_mDice: 0.2713

Epoch 00009: val_mDice improved from 0.26682 to 0.27132, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 10/300
 - 20s - loss: 0.4969 - acc: 0.9303 - mDice: 0.4645 - val_loss: 0.2924 - val_acc: 0.9284 - val_mDice: 0.2776

Epoch 00010: val_mDice improved from 0.27132 to 0.27764, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 11/300
 - 20s - loss: 0.4891 - acc: 0.9315 - mDice: 0.4730 - val_loss: 0.2608 - val_acc: 0.9342 - val_mDice: 0.2813

Epoch 00011: val_mDice improved from 0.27764 to 0.28135, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 12/300
 - 20s - loss: 0.4826 - acc: 0.9322 - mDice: 0.4801 - val_loss: 0.2677 - val_acc: 0.9334 - val_mDice: 0.2827

Epoch 00012: val_mDice improved from 0.28135 to 0.28271, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 13/300
 - 20s - loss: 0.4755 - acc: 0.9333 - mDice: 0.4877 - val_loss: 0.2983 - val_acc: 0.9348 - val_mDice: 0.2860

Epoch 00013: val_mDice improved from 0.28271 to 0.28603, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 14/300
 - 20s - loss: 0.4744 - acc: 0.9339 - mDice: 0.4888 - val_loss: 0.2590 - val_acc: 0.9313 - val_mDice: 0.2874

Epoch 00014: val_mDice improved from 0.28603 to 0.28740, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 15/300
 - 20s - loss: 0.4706 - acc: 0.9343 - mDice: 0.4929 - val_loss: 0.2621 - val_acc: 0.9325 - val_mDice: 0.2886

Epoch 00015: val_mDice improved from 0.28740 to 0.28856, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 16/300
 - 19s - loss: 0.4628 - acc: 0.9350 - mDice: 0.5013 - val_loss: 0.3141 - val_acc: 0.9279 - val_mDice: 0.2884

Epoch 00016: val_mDice did not improve from 0.28856
Epoch 17/300
 - 20s - loss: 0.4521 - acc: 0.9354 - mDice: 0.5129 - val_loss: 0.3079 - val_acc: 0.9328 - val_mDice: 0.2979

Epoch 00017: val_mDice improved from 0.28856 to 0.29790, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 18/300
 - 19s - loss: 0.4360 - acc: 0.9358 - mDice: 0.5304 - val_loss: 0.2935 - val_acc: 0.9325 - val_mDice: 0.3040

Epoch 00018: val_mDice improved from 0.29790 to 0.30402, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 19/300
 - 19s - loss: 0.4250 - acc: 0.9362 - mDice: 0.5422 - val_loss: 0.2573 - val_acc: 0.9341 - val_mDice: 0.3105

Epoch 00019: val_mDice improved from 0.30402 to 0.31050, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 20/300
 - 19s - loss: 0.4176 - acc: 0.9368 - mDice: 0.5502 - val_loss: 0.2580 - val_acc: 0.9359 - val_mDice: 0.3131

Epoch 00020: val_mDice improved from 0.31050 to 0.31306, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 21/300
 - 20s - loss: 0.4122 - acc: 0.9373 - mDice: 0.5561 - val_loss: 0.2333 - val_acc: 0.9359 - val_mDice: 0.3131

Epoch 00021: val_mDice improved from 0.31306 to 0.31315, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 22/300
 - 19s - loss: 0.4105 - acc: 0.9377 - mDice: 0.5579 - val_loss: 0.2406 - val_acc: 0.9385 - val_mDice: 0.3158

Epoch 00022: val_mDice improved from 0.31315 to 0.31580, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 23/300
 - 19s - loss: 0.4083 - acc: 0.9381 - mDice: 0.5602 - val_loss: 0.2183 - val_acc: 0.9389 - val_mDice: 0.3143

Epoch 00023: val_mDice did not improve from 0.31580
Epoch 24/300
 - 18s - loss: 0.4049 - acc: 0.9385 - mDice: 0.5639 - val_loss: 0.2283 - val_acc: 0.9380 - val_mDice: 0.3175

Epoch 00024: val_mDice improved from 0.31580 to 0.31755, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 25/300
 - 19s - loss: 0.4030 - acc: 0.9387 - mDice: 0.5660 - val_loss: 0.2067 - val_acc: 0.9394 - val_mDice: 0.3166

Epoch 00025: val_mDice did not improve from 0.31755
Epoch 26/300
 - 19s - loss: 0.3998 - acc: 0.9389 - mDice: 0.5694 - val_loss: 0.2329 - val_acc: 0.9369 - val_mDice: 0.3153

Epoch 00026: val_mDice did not improve from 0.31755
Epoch 27/300
 - 19s - loss: 0.3961 - acc: 0.9392 - mDice: 0.5734 - val_loss: 0.2485 - val_acc: 0.9337 - val_mDice: 0.3150

Epoch 00027: val_mDice did not improve from 0.31755
Epoch 28/300
 - 19s - loss: 0.3959 - acc: 0.9396 - mDice: 0.5737 - val_loss: 0.2135 - val_acc: 0.9386 - val_mDice: 0.3133

Epoch 00028: val_mDice did not improve from 0.31755
Epoch 29/300
 - 20s - loss: 0.3906 - acc: 0.9401 - mDice: 0.5794 - val_loss: 0.2226 - val_acc: 0.9360 - val_mDice: 0.3176

Epoch 00029: val_mDice improved from 0.31755 to 0.31759, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 30/300
 - 18s - loss: 0.3883 - acc: 0.9404 - mDice: 0.5818 - val_loss: 0.2600 - val_acc: 0.9351 - val_mDice: 0.3127

Epoch 00030: val_mDice did not improve from 0.31759
Epoch 31/300
 - 19s - loss: 0.3901 - acc: 0.9403 - mDice: 0.5800 - val_loss: 0.1866 - val_acc: 0.9384 - val_mDice: 0.3203

Epoch 00031: val_mDice improved from 0.31759 to 0.32035, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 32/300
 - 20s - loss: 0.3862 - acc: 0.9406 - mDice: 0.5842 - val_loss: 0.2305 - val_acc: 0.9379 - val_mDice: 0.3150

Epoch 00032: val_mDice did not improve from 0.32035
Epoch 33/300
 - 19s - loss: 0.3854 - acc: 0.9408 - mDice: 0.5849 - val_loss: 0.1909 - val_acc: 0.9382 - val_mDice: 0.3201

Epoch 00033: val_mDice did not improve from 0.32035
Epoch 34/300
 - 20s - loss: 0.3834 - acc: 0.9409 - mDice: 0.5871 - val_loss: 0.1973 - val_acc: 0.9374 - val_mDice: 0.3191

Epoch 00034: val_mDice did not improve from 0.32035
Epoch 35/300
 - 18s - loss: 0.3818 - acc: 0.9413 - mDice: 0.5889 - val_loss: 0.2036 - val_acc: 0.9375 - val_mDice: 0.3189

Epoch 00035: val_mDice did not improve from 0.32035
Epoch 36/300
 - 19s - loss: 0.3776 - acc: 0.9416 - mDice: 0.5934 - val_loss: 0.2114 - val_acc: 0.9378 - val_mDice: 0.3189

Epoch 00036: val_mDice did not improve from 0.32035
Epoch 37/300
 - 19s - loss: 0.3760 - acc: 0.9416 - mDice: 0.5952 - val_loss: 0.1976 - val_acc: 0.9373 - val_mDice: 0.3175

Epoch 00037: val_mDice did not improve from 0.32035
Epoch 38/300
 - 19s - loss: 0.3772 - acc: 0.9419 - mDice: 0.5939 - val_loss: 0.2062 - val_acc: 0.9373 - val_mDice: 0.3170

Epoch 00038: val_mDice did not improve from 0.32035
Epoch 39/300
 - 20s - loss: 0.3737 - acc: 0.9421 - mDice: 0.5976 - val_loss: 0.2168 - val_acc: 0.9394 - val_mDice: 0.3224

Epoch 00039: val_mDice improved from 0.32035 to 0.32237, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 40/300
 - 20s - loss: 0.3716 - acc: 0.9424 - mDice: 0.5999 - val_loss: 0.1791 - val_acc: 0.9400 - val_mDice: 0.3222

Epoch 00040: val_mDice did not improve from 0.32237
Epoch 41/300
 - 19s - loss: 0.3713 - acc: 0.9424 - mDice: 0.6002 - val_loss: 0.2389 - val_acc: 0.9340 - val_mDice: 0.3137

Epoch 00041: val_mDice did not improve from 0.32237
Epoch 42/300
 - 20s - loss: 0.3717 - acc: 0.9425 - mDice: 0.5997 - val_loss: 0.1665 - val_acc: 0.9410 - val_mDice: 0.3241

Epoch 00042: val_mDice improved from 0.32237 to 0.32405, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 43/300
 - 20s - loss: 0.3703 - acc: 0.9428 - mDice: 0.6012 - val_loss: 0.1950 - val_acc: 0.9379 - val_mDice: 0.3201

Epoch 00043: val_mDice did not improve from 0.32405
Epoch 44/300
 - 20s - loss: 0.3679 - acc: 0.9429 - mDice: 0.6039 - val_loss: 0.2264 - val_acc: 0.9400 - val_mDice: 0.3199

Epoch 00044: val_mDice did not improve from 0.32405
Epoch 45/300
 - 21s - loss: 0.3696 - acc: 0.9428 - mDice: 0.6020 - val_loss: 0.2385 - val_acc: 0.9374 - val_mDice: 0.3180

Epoch 00045: val_mDice did not improve from 0.32405
Epoch 46/300
 - 20s - loss: 0.3668 - acc: 0.9433 - mDice: 0.6050 - val_loss: 0.1814 - val_acc: 0.9400 - val_mDice: 0.3231

Epoch 00046: val_mDice did not improve from 0.32405
Epoch 47/300
 - 21s - loss: 0.3634 - acc: 0.9434 - mDice: 0.6087 - val_loss: 0.1811 - val_acc: 0.9411 - val_mDice: 0.3221

Epoch 00047: val_mDice did not improve from 0.32405
Epoch 48/300
 - 20s - loss: 0.3641 - acc: 0.9435 - mDice: 0.6080 - val_loss: 0.1415 - val_acc: 0.9418 - val_mDice: 0.3229

Epoch 00048: val_mDice did not improve from 0.32405
Epoch 49/300
 - 19s - loss: 0.3615 - acc: 0.9436 - mDice: 0.6107 - val_loss: 0.1772 - val_acc: 0.9407 - val_mDice: 0.3211

Epoch 00049: val_mDice did not improve from 0.32405
Epoch 50/300
 - 19s - loss: 0.3620 - acc: 0.9438 - mDice: 0.6102 - val_loss: 0.1518 - val_acc: 0.9410 - val_mDice: 0.3215

Epoch 00050: val_mDice did not improve from 0.32405
Epoch 51/300
 - 20s - loss: 0.3606 - acc: 0.9440 - mDice: 0.6117 - val_loss: 0.1890 - val_acc: 0.9368 - val_mDice: 0.3176

Epoch 00051: val_mDice did not improve from 0.32405
Epoch 52/300
 - 20s - loss: 0.3603 - acc: 0.9440 - mDice: 0.6120 - val_loss: 0.1228 - val_acc: 0.9400 - val_mDice: 0.3204

Epoch 00052: val_mDice did not improve from 0.32405
Epoch 53/300
 - 20s - loss: 0.3604 - acc: 0.9441 - mDice: 0.6119 - val_loss: 0.1853 - val_acc: 0.9418 - val_mDice: 0.3197

Epoch 00053: val_mDice did not improve from 0.32405
Epoch 54/300
 - 20s - loss: 0.3583 - acc: 0.9443 - mDice: 0.6142 - val_loss: 0.1425 - val_acc: 0.9417 - val_mDice: 0.3217

Epoch 00054: val_mDice did not improve from 0.32405
Epoch 55/300
 - 20s - loss: 0.3579 - acc: 0.9442 - mDice: 0.6147 - val_loss: 0.1868 - val_acc: 0.9405 - val_mDice: 0.3177

Epoch 00055: val_mDice did not improve from 0.32405
Epoch 56/300
 - 20s - loss: 0.3543 - acc: 0.9443 - mDice: 0.6185 - val_loss: 0.1472 - val_acc: 0.9415 - val_mDice: 0.3195

Epoch 00056: val_mDice did not improve from 0.32405
Epoch 57/300
 - 20s - loss: 0.3559 - acc: 0.9445 - mDice: 0.6168 - val_loss: 0.1903 - val_acc: 0.9423 - val_mDice: 0.3234

Epoch 00057: val_mDice did not improve from 0.32405

Epoch 00057: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 58/300
 - 18s - loss: 0.3533 - acc: 0.9449 - mDice: 0.6196 - val_loss: 0.1487 - val_acc: 0.9417 - val_mDice: 0.3231

Epoch 00058: val_mDice did not improve from 0.32405
Epoch 59/300
 - 20s - loss: 0.3516 - acc: 0.9448 - mDice: 0.6214 - val_loss: 0.1654 - val_acc: 0.9423 - val_mDice: 0.3218

Epoch 00059: val_mDice did not improve from 0.32405
Epoch 60/300
 - 19s - loss: 0.3498 - acc: 0.9451 - mDice: 0.6234 - val_loss: 0.1855 - val_acc: 0.9415 - val_mDice: 0.3219

Epoch 00060: val_mDice did not improve from 0.32405
Epoch 61/300
 - 20s - loss: 0.3526 - acc: 0.9449 - mDice: 0.6203 - val_loss: 0.1435 - val_acc: 0.9447 - val_mDice: 0.3236

Epoch 00061: val_mDice did not improve from 0.32405
Epoch 62/300
 - 20s - loss: 0.3486 - acc: 0.9451 - mDice: 0.6246 - val_loss: 0.1992 - val_acc: 0.9398 - val_mDice: 0.3209

Epoch 00062: val_mDice did not improve from 0.32405
Epoch 63/300
 - 20s - loss: 0.3488 - acc: 0.9451 - mDice: 0.6244 - val_loss: 0.2019 - val_acc: 0.9417 - val_mDice: 0.3215

Epoch 00063: val_mDice did not improve from 0.32405
Epoch 64/300
 - 19s - loss: 0.3506 - acc: 0.9452 - mDice: 0.6224 - val_loss: 0.1588 - val_acc: 0.9414 - val_mDice: 0.3209

Epoch 00064: val_mDice did not improve from 0.32405
Epoch 65/300
 - 19s - loss: 0.3481 - acc: 0.9452 - mDice: 0.6251 - val_loss: 0.1524 - val_acc: 0.9428 - val_mDice: 0.3233

Epoch 00065: val_mDice did not improve from 0.32405
Epoch 66/300
 - 18s - loss: 0.3480 - acc: 0.9453 - mDice: 0.6253 - val_loss: 0.1761 - val_acc: 0.9405 - val_mDice: 0.3219

Epoch 00066: val_mDice did not improve from 0.32405
Epoch 67/300
 - 19s - loss: 0.3484 - acc: 0.9454 - mDice: 0.6248 - val_loss: 0.1864 - val_acc: 0.9403 - val_mDice: 0.3221

Epoch 00067: val_mDice did not improve from 0.32405
Epoch 68/300
 - 19s - loss: 0.3439 - acc: 0.9454 - mDice: 0.6297 - val_loss: 0.1374 - val_acc: 0.9426 - val_mDice: 0.3244

Epoch 00068: val_mDice improved from 0.32405 to 0.32442, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 69/300
 - 19s - loss: 0.3478 - acc: 0.9454 - mDice: 0.6254 - val_loss: 0.1443 - val_acc: 0.9410 - val_mDice: 0.3201

Epoch 00069: val_mDice did not improve from 0.32442
Epoch 70/300
 - 19s - loss: 0.3486 - acc: 0.9454 - mDice: 0.6245 - val_loss: 0.1909 - val_acc: 0.9416 - val_mDice: 0.3207

Epoch 00070: val_mDice did not improve from 0.32442
Epoch 71/300
 - 18s - loss: 0.3463 - acc: 0.9455 - mDice: 0.6271 - val_loss: 0.1591 - val_acc: 0.9409 - val_mDice: 0.3221

Epoch 00071: val_mDice did not improve from 0.32442
Epoch 72/300
 - 18s - loss: 0.3493 - acc: 0.9455 - mDice: 0.6238 - val_loss: 0.1569 - val_acc: 0.9423 - val_mDice: 0.3223

Epoch 00072: val_mDice did not improve from 0.32442

Epoch 00072: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 73/300
 - 18s - loss: 0.3468 - acc: 0.9457 - mDice: 0.6266 - val_loss: 0.1620 - val_acc: 0.9426 - val_mDice: 0.3237

Epoch 00073: val_mDice did not improve from 0.32442
Epoch 74/300
 - 18s - loss: 0.3452 - acc: 0.9458 - mDice: 0.6282 - val_loss: 0.1490 - val_acc: 0.9429 - val_mDice: 0.3242

Epoch 00074: val_mDice did not improve from 0.32442
Epoch 75/300
 - 18s - loss: 0.3434 - acc: 0.9457 - mDice: 0.6303 - val_loss: 0.1712 - val_acc: 0.9411 - val_mDice: 0.3229

Epoch 00075: val_mDice did not improve from 0.32442
Epoch 76/300
 - 18s - loss: 0.3473 - acc: 0.9458 - mDice: 0.6260 - val_loss: 0.2009 - val_acc: 0.9417 - val_mDice: 0.3226

Epoch 00076: val_mDice did not improve from 0.32442
Epoch 77/300
 - 18s - loss: 0.3437 - acc: 0.9458 - mDice: 0.6299 - val_loss: 0.1832 - val_acc: 0.9411 - val_mDice: 0.3221

Epoch 00077: val_mDice did not improve from 0.32442
Epoch 78/300
 - 18s - loss: 0.3423 - acc: 0.9460 - mDice: 0.6315 - val_loss: 0.1587 - val_acc: 0.9418 - val_mDice: 0.3235

Epoch 00078: val_mDice did not improve from 0.32442
Epoch 79/300
 - 18s - loss: 0.3409 - acc: 0.9460 - mDice: 0.6330 - val_loss: 0.1881 - val_acc: 0.9411 - val_mDice: 0.3228

Epoch 00079: val_mDice did not improve from 0.32442
Epoch 80/300
 - 18s - loss: 0.3441 - acc: 0.9459 - mDice: 0.6295 - val_loss: 0.1666 - val_acc: 0.9424 - val_mDice: 0.3227

Epoch 00080: val_mDice did not improve from 0.32442
Epoch 81/300
 - 18s - loss: 0.3438 - acc: 0.9459 - mDice: 0.6298 - val_loss: 0.1582 - val_acc: 0.9426 - val_mDice: 0.3231

Epoch 00081: val_mDice did not improve from 0.32442
Epoch 82/300
 - 17s - loss: 0.3424 - acc: 0.9460 - mDice: 0.6313 - val_loss: 0.1729 - val_acc: 0.9421 - val_mDice: 0.3234

Epoch 00082: val_mDice did not improve from 0.32442
Epoch 83/300
 - 17s - loss: 0.3421 - acc: 0.9460 - mDice: 0.6316 - val_loss: 0.1661 - val_acc: 0.9420 - val_mDice: 0.3237

Epoch 00083: val_mDice did not improve from 0.32442
Epoch 84/300
 - 18s - loss: 0.3402 - acc: 0.9462 - mDice: 0.6337 - val_loss: 0.1664 - val_acc: 0.9430 - val_mDice: 0.3225

Epoch 00084: val_mDice did not improve from 0.32442
Epoch 85/300
 - 18s - loss: 0.3410 - acc: 0.9461 - mDice: 0.6328 - val_loss: 0.1929 - val_acc: 0.9420 - val_mDice: 0.3219

Epoch 00085: val_mDice did not improve from 0.32442
Epoch 86/300
 - 18s - loss: 0.3421 - acc: 0.9460 - mDice: 0.6315 - val_loss: 0.1770 - val_acc: 0.9406 - val_mDice: 0.3217

Epoch 00086: val_mDice did not improve from 0.32442
Epoch 87/300
 - 18s - loss: 0.3430 - acc: 0.9459 - mDice: 0.6306 - val_loss: 0.1880 - val_acc: 0.9409 - val_mDice: 0.3219

Epoch 00087: val_mDice did not improve from 0.32442

Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 88/300
 - 18s - loss: 0.3446 - acc: 0.9461 - mDice: 0.6290 - val_loss: 0.1745 - val_acc: 0.9426 - val_mDice: 0.3239

Epoch 00088: val_mDice did not improve from 0.32442
Epoch 89/300
 - 18s - loss: 0.3423 - acc: 0.9462 - mDice: 0.6314 - val_loss: 0.1765 - val_acc: 0.9418 - val_mDice: 0.3218

Epoch 00089: val_mDice did not improve from 0.32442
Epoch 90/300
 - 18s - loss: 0.3443 - acc: 0.9462 - mDice: 0.6292 - val_loss: 0.1505 - val_acc: 0.9425 - val_mDice: 0.3229

Epoch 00090: val_mDice did not improve from 0.32442
Epoch 91/300
 - 18s - loss: 0.3404 - acc: 0.9462 - mDice: 0.6335 - val_loss: 0.1805 - val_acc: 0.9418 - val_mDice: 0.3233

Epoch 00091: val_mDice did not improve from 0.32442
Epoch 92/300
 - 18s - loss: 0.3413 - acc: 0.9463 - mDice: 0.6325 - val_loss: 0.1541 - val_acc: 0.9415 - val_mDice: 0.3236

Epoch 00092: val_mDice did not improve from 0.32442
Epoch 93/300
 - 18s - loss: 0.3403 - acc: 0.9462 - mDice: 0.6336 - val_loss: 0.1657 - val_acc: 0.9422 - val_mDice: 0.3246

Epoch 00093: val_mDice improved from 0.32442 to 0.32459, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 94/300
 - 18s - loss: 0.3415 - acc: 0.9463 - mDice: 0.6323 - val_loss: 0.1739 - val_acc: 0.9425 - val_mDice: 0.3245

Epoch 00094: val_mDice did not improve from 0.32459
Epoch 95/300
 - 18s - loss: 0.3389 - acc: 0.9463 - mDice: 0.6351 - val_loss: 0.1595 - val_acc: 0.9418 - val_mDice: 0.3232

Epoch 00095: val_mDice did not improve from 0.32459
Epoch 96/300
 - 18s - loss: 0.3421 - acc: 0.9462 - mDice: 0.6316 - val_loss: 0.1734 - val_acc: 0.9419 - val_mDice: 0.3228

Epoch 00096: val_mDice did not improve from 0.32459
Epoch 97/300
 - 19s - loss: 0.3390 - acc: 0.9465 - mDice: 0.6350 - val_loss: 0.1610 - val_acc: 0.9422 - val_mDice: 0.3234

Epoch 00097: val_mDice did not improve from 0.32459
Epoch 98/300
 - 18s - loss: 0.3394 - acc: 0.9464 - mDice: 0.6345 - val_loss: 0.1600 - val_acc: 0.9422 - val_mDice: 0.3220

Epoch 00098: val_mDice did not improve from 0.32459
Epoch 99/300
 - 18s - loss: 0.3389 - acc: 0.9463 - mDice: 0.6351 - val_loss: 0.1668 - val_acc: 0.9417 - val_mDice: 0.3231

Epoch 00099: val_mDice did not improve from 0.32459
Epoch 100/300
 - 18s - loss: 0.3405 - acc: 0.9464 - mDice: 0.6334 - val_loss: 0.1782 - val_acc: 0.9414 - val_mDice: 0.3235

Epoch 00100: val_mDice did not improve from 0.32459
Epoch 101/300
 - 18s - loss: 0.3417 - acc: 0.9463 - mDice: 0.6320 - val_loss: 0.1634 - val_acc: 0.9427 - val_mDice: 0.3229

Epoch 00101: val_mDice did not improve from 0.32459
Epoch 102/300
 - 18s - loss: 0.3414 - acc: 0.9463 - mDice: 0.6323 - val_loss: 0.1666 - val_acc: 0.9423 - val_mDice: 0.3235

Epoch 00102: val_mDice did not improve from 0.32459

Epoch 00102: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.
Epoch 103/300
 - 18s - loss: 0.3411 - acc: 0.9465 - mDice: 0.6327 - val_loss: 0.1571 - val_acc: 0.9421 - val_mDice: 0.3230

Epoch 00103: val_mDice did not improve from 0.32459
Epoch 104/300
 - 18s - loss: 0.3389 - acc: 0.9463 - mDice: 0.6351 - val_loss: 0.1663 - val_acc: 0.9424 - val_mDice: 0.3238

Epoch 00104: val_mDice did not improve from 0.32459
Epoch 105/300
 - 18s - loss: 0.3398 - acc: 0.9464 - mDice: 0.6341 - val_loss: 0.1690 - val_acc: 0.9420 - val_mDice: 0.3238

Epoch 00105: val_mDice did not improve from 0.32459
Epoch 106/300
 - 18s - loss: 0.3423 - acc: 0.9463 - mDice: 0.6314 - val_loss: 0.1582 - val_acc: 0.9426 - val_mDice: 0.3241

Epoch 00106: val_mDice did not improve from 0.32459
Epoch 107/300
 - 19s - loss: 0.3401 - acc: 0.9463 - mDice: 0.6338 - val_loss: 0.1585 - val_acc: 0.9426 - val_mDice: 0.3242

Epoch 00107: val_mDice did not improve from 0.32459
Epoch 108/300
 - 19s - loss: 0.3395 - acc: 0.9465 - mDice: 0.6344 - val_loss: 0.1584 - val_acc: 0.9423 - val_mDice: 0.3235

Epoch 00108: val_mDice did not improve from 0.32459
Epoch 109/300
 - 18s - loss: 0.3408 - acc: 0.9464 - mDice: 0.6330 - val_loss: 0.1509 - val_acc: 0.9421 - val_mDice: 0.3237

Epoch 00109: val_mDice did not improve from 0.32459
Epoch 110/300
 - 18s - loss: 0.3394 - acc: 0.9465 - mDice: 0.6346 - val_loss: 0.1646 - val_acc: 0.9420 - val_mDice: 0.3241

Epoch 00110: val_mDice did not improve from 0.32459
Epoch 111/300
 - 18s - loss: 0.3387 - acc: 0.9465 - mDice: 0.6353 - val_loss: 0.1665 - val_acc: 0.9421 - val_mDice: 0.3237

Epoch 00111: val_mDice did not improve from 0.32459
Epoch 112/300
 - 19s - loss: 0.3408 - acc: 0.9464 - mDice: 0.6331 - val_loss: 0.1652 - val_acc: 0.9424 - val_mDice: 0.3237

Epoch 00112: val_mDice did not improve from 0.32459
Epoch 113/300
 - 18s - loss: 0.3387 - acc: 0.9464 - mDice: 0.6353 - val_loss: 0.1594 - val_acc: 0.9425 - val_mDice: 0.3239

Epoch 00113: val_mDice did not improve from 0.32459
Epoch 114/300
 - 18s - loss: 0.3375 - acc: 0.9465 - mDice: 0.6367 - val_loss: 0.1629 - val_acc: 0.9425 - val_mDice: 0.3230

Epoch 00114: val_mDice did not improve from 0.32459
Epoch 115/300
 - 19s - loss: 0.3398 - acc: 0.9465 - mDice: 0.6341 - val_loss: 0.1531 - val_acc: 0.9426 - val_mDice: 0.3238

Epoch 00115: val_mDice did not improve from 0.32459
Epoch 116/300
 - 18s - loss: 0.3391 - acc: 0.9466 - mDice: 0.6348 - val_loss: 0.1658 - val_acc: 0.9425 - val_mDice: 0.3237

Epoch 00116: val_mDice did not improve from 0.32459
Epoch 117/300
 - 18s - loss: 0.3419 - acc: 0.9466 - mDice: 0.6318 - val_loss: 0.1591 - val_acc: 0.9425 - val_mDice: 0.3228

Epoch 00117: val_mDice did not improve from 0.32459

Epoch 00117: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.
Epoch 118/300
 - 18s - loss: 0.3390 - acc: 0.9465 - mDice: 0.6350 - val_loss: 0.1588 - val_acc: 0.9428 - val_mDice: 0.3231

Epoch 00118: val_mDice did not improve from 0.32459
Epoch 119/300
 - 18s - loss: 0.3386 - acc: 0.9465 - mDice: 0.6354 - val_loss: 0.1587 - val_acc: 0.9428 - val_mDice: 0.3236

Epoch 00119: val_mDice did not improve from 0.32459
Epoch 120/300
 - 18s - loss: 0.3396 - acc: 0.9464 - mDice: 0.6343 - val_loss: 0.1578 - val_acc: 0.9428 - val_mDice: 0.3241

Epoch 00120: val_mDice did not improve from 0.32459
Epoch 121/300
 - 18s - loss: 0.3401 - acc: 0.9465 - mDice: 0.6338 - val_loss: 0.1579 - val_acc: 0.9427 - val_mDice: 0.3241

Epoch 00121: val_mDice did not improve from 0.32459
Epoch 122/300
 - 18s - loss: 0.3382 - acc: 0.9465 - mDice: 0.6358 - val_loss: 0.1646 - val_acc: 0.9425 - val_mDice: 0.3242

Epoch 00122: val_mDice did not improve from 0.32459
Epoch 123/300
 - 18s - loss: 0.3373 - acc: 0.9465 - mDice: 0.6368 - val_loss: 0.1612 - val_acc: 0.9428 - val_mDice: 0.3242

Epoch 00123: val_mDice did not improve from 0.32459
Epoch 124/300
 - 18s - loss: 0.3373 - acc: 0.9466 - mDice: 0.6368 - val_loss: 0.1639 - val_acc: 0.9423 - val_mDice: 0.3239

Epoch 00124: val_mDice did not improve from 0.32459
Epoch 125/300
 - 17s - loss: 0.3402 - acc: 0.9465 - mDice: 0.6337 - val_loss: 0.1593 - val_acc: 0.9422 - val_mDice: 0.3236

Epoch 00125: val_mDice did not improve from 0.32459
Epoch 126/300
 - 18s - loss: 0.3406 - acc: 0.9465 - mDice: 0.6333 - val_loss: 0.1585 - val_acc: 0.9424 - val_mDice: 0.3236

Epoch 00126: val_mDice did not improve from 0.32459
Epoch 127/300
 - 18s - loss: 0.3400 - acc: 0.9465 - mDice: 0.6339 - val_loss: 0.1557 - val_acc: 0.9430 - val_mDice: 0.3234

Epoch 00127: val_mDice did not improve from 0.32459
Epoch 128/300
 - 18s - loss: 0.3391 - acc: 0.9465 - mDice: 0.6349 - val_loss: 0.1598 - val_acc: 0.9424 - val_mDice: 0.3233

Epoch 00128: val_mDice did not improve from 0.32459
Epoch 129/300
 - 18s - loss: 0.3394 - acc: 0.9466 - mDice: 0.6345 - val_loss: 0.1617 - val_acc: 0.9424 - val_mDice: 0.3236

Epoch 00129: val_mDice did not improve from 0.32459
Epoch 130/300
 - 18s - loss: 0.3380 - acc: 0.9465 - mDice: 0.6360 - val_loss: 0.1635 - val_acc: 0.9425 - val_mDice: 0.3235

Epoch 00130: val_mDice did not improve from 0.32459
Epoch 131/300
 - 18s - loss: 0.3394 - acc: 0.9467 - mDice: 0.6345 - val_loss: 0.1650 - val_acc: 0.9424 - val_mDice: 0.3235

Epoch 00131: val_mDice did not improve from 0.32459
Epoch 132/300
 - 18s - loss: 0.3402 - acc: 0.9466 - mDice: 0.6337 - val_loss: 0.1650 - val_acc: 0.9425 - val_mDice: 0.3240

Epoch 00132: val_mDice did not improve from 0.32459

Epoch 00132: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.
Epoch 133/300
 - 18s - loss: 0.3369 - acc: 0.9466 - mDice: 0.6373 - val_loss: 0.1582 - val_acc: 0.9424 - val_mDice: 0.3241

Epoch 00133: val_mDice did not improve from 0.32459
Restoring model weights from the end of the best epoch
Epoch 00133: early stopping
{'val_loss': [0.4885592348873615, 0.43729176785310975, 0.37264647302420245, 0.35466162653882866, 0.37726935261415073, 0.35822617413435615, 0.3088416124329619, 0.3132406692020595, 0.29593449525292154, 0.2924379422126905, 0.2607868743412521, 0.2676568885293344, 0.29833857565308397, 0.2589938555155759, 0.2621089093508604, 0.3140501579391244, 0.30790136705922044, 0.2934864567309294, 0.25729432135172514, 0.25798737852955644, 0.23330516096852397, 0.2406267160144837, 0.2183272813892235, 0.22826671725867884, 0.20665894061043535, 0.23285194443862722, 0.24852961724947975, 0.21352916781831047, 0.2225752452428898, 0.26003881234108756, 0.18659048893934357, 0.2305338133490928, 0.1908583881981347, 0.19731249188518393, 0.20356831576108284, 0.2113634637840416, 0.19759627298006546, 0.20615461262185936, 0.21679153813697075, 0.17911555626146172, 0.23890396752192275, 0.16650101435168282, 0.19501140276375023, 0.22644742043769878, 0.23845763306092957, 0.18142605433240533, 0.18107091914862394, 0.14149376055549667, 0.17717315523844698, 0.15178814455461892, 0.18901802368624054, 0.12281999117492334, 0.1852909428998828, 0.1424940681165975, 0.18684943473857382, 0.14715290077678536, 0.19029150121723828, 0.1486620783765355, 0.1654166324550043, 0.18546393654390197, 0.14353609716762666, 0.19917772686027962, 0.2019178928280978, 0.158849633116599, 0.1523995587764227, 0.17605912693492745, 0.1864052329759073, 0.1374152885021075, 0.1442733805667123, 0.19089684307413257, 0.15908892298846142, 0.15690310501857943, 0.16203697896360056, 0.14899337724984987, 0.17118089563091812, 0.20088513296745394, 0.18321779368283309, 0.1587477772415656, 0.18812086323843055, 0.16657944511541206, 0.15822445692332543, 0.17285408828731463, 0.16612129233291614, 0.1664187393920577, 0.19294862123206258, 0.17698457849252483, 0.18800594499739615, 0.17453428072368968, 0.17650532495716345, 0.15054256983263337, 0.18047446514601292, 0.15412915147759992, 0.1656675085870792, 0.17392414241381313, 0.15949447770886446, 0.17343619688535514, 0.16096610535422098, 0.1600362994181721, 0.16679216307871367, 0.17817175145382466, 0.1633812404723595, 0.16661617145671145, 0.1571006738661748, 0.16632083262843284, 0.1689799870364368, 0.15819753744922901, 0.15847236574020074, 0.15837720641866326, 0.15086678705056725, 0.16457058009489076, 0.16648531039042966, 0.16519402012066997, 0.1594158393006934, 0.1629264839398472, 0.1531272020150462, 0.16580773193551146, 0.15905840431704468, 0.1588255877647063, 0.15868438721593955, 0.15777074005049857, 0.15785044455981773, 0.1646308170064636, 0.16122170462799462, 0.16387931544981574, 0.15934839269713216, 0.15845088583781666, 0.155711079886912, 0.15980843021331922, 0.16167082682089962, 0.1634706752779691, 0.16495393937372643, 0.16502016280656276, 0.15819193368129755], 'val_acc': [0.8850925117731094, 0.9044664327217184, 0.9112633129824763, 0.9154314178487529, 0.9177159848420516, 0.9237176503824152, 0.9290876673615497, 0.9238403368255367, 0.9281152292438175, 0.9284128741077755, 0.9342168601958648, 0.9333512050950009, 0.9348314611808114, 0.9313108946966089, 0.932476458342179, 0.9279380136209986, 0.9328411292770634, 0.9325230290060458, 0.9340691858011744, 0.9358856988989789, 0.9359322779852411, 0.9385065306787905, 0.9388870972654094, 0.9379748747400616, 0.93937899561032, 0.9369319826364517, 0.9336851971304935, 0.9385644618583762, 0.9360151977642722, 0.9351290976223738, 0.9384224628624709, 0.9379066967445872, 0.9381816199292308, 0.9373829870120339, 0.9374591112136841, 0.9378067319807799, 0.9372614304656568, 0.9372523459403411, 0.9394369365080543, 0.9399731489627258, 0.9339737477509872, 0.9409728587969489, 0.9379362334375796, 0.9400379074656445, 0.9374091171700022, 0.9400231261616168, 0.9410796385744343, 0.9418078401814336, 0.9406740762617277, 0.9410478310740512, 0.9367797595003376, 0.939977690577507, 0.9418146538993587, 0.9417203677737195, 0.9405275291722753, 0.941520429175833, 0.942288378658502, 0.9416930947614752, 0.9423099665538125, 0.9414999685857607, 0.944695634038552, 0.9398038756588231, 0.9416806024053822, 0.9414227306842804, 0.9427791425715322, 0.9404684506032778, 0.9402730510286663, 0.9426303272661956, 0.9410364699104558, 0.9416317518638528, 0.9409365038508954, 0.942269068049348, 0.9426144193048063, 0.9428927470808444, 0.9410750950160234, 0.9417180950226991, 0.9411159922247347, 0.9417794333851855, 0.941104637539905, 0.9423826757980429, 0.9426439579414285, 0.9421247833448908, 0.9420259517172108, 0.9430313440768615, 0.9420145963845046, 0.9406161353639935, 0.9409478546484656, 0.942579202030016, 0.9417692111886066, 0.9424712813418844, 0.941822605288547, 0.9414942931869755, 0.9421815911064977, 0.9425133071515871, 0.9418112421813218, 0.9418635018493818, 0.9422452119381531, 0.9422440788020259, 0.9417021935400756, 0.9414408990870351, 0.9426575886166614, 0.942323597876922, 0.9421372828276261, 0.9424360634192176, 0.9419907292594081, 0.942586020283077, 0.9425678473451863, 0.9423076957464218, 0.942130473644837, 0.9419986910146215, 0.9421361509872519, 0.942362226869749, 0.9424928653499355, 0.9425155818462372, 0.9426030659157297, 0.942528080681096, 0.9424826450969862, 0.9428166325973428, 0.9427893705990004, 0.9427995908519496, 0.9427484759817952, 0.942543986050979, 0.9427677814079367, 0.9423111042250758, 0.942153191436892, 0.9423724380524262, 0.9429938546989275, 0.9424065319092377, 0.9423838050469108, 0.942474685933279, 0.9423769952162452, 0.9424610520186631, 0.9423588255177373], 'val_mDice': [0.1900857514296861, 0.2175737478233793, 0.22318748170105013, 0.22865651304955067, 0.2465891581757561, 0.25666231613444246, 0.26660745015934756, 0.2668243590337427, 0.27132277121848386, 0.27763835113981494, 0.2813487392242836, 0.28271160246399435, 0.28602509265360626, 0.2873987107659164, 0.28856284049865993, 0.2884127846311616, 0.29790046430476336, 0.30401778549117886, 0.3105011343470086, 0.31305846774383733, 0.3131484136228328, 0.31579558539163805, 0.3142640703236279, 0.3175467030791681, 0.31663273886332044, 0.3153108105711315, 0.31497468540202017, 0.31328154517256696, 0.3175881140784401, 0.31273365684825444, 0.32034796848893166, 0.314979559701422, 0.3200520275403624, 0.31911627709379664, 0.3189228497364599, 0.3189148298748162, 0.3174657039344311, 0.31701213931259903, 0.32237332273760566, 0.3222381610261357, 0.3137144958195479, 0.324053795967737, 0.3201322026712739, 0.3198837617655163, 0.31803124337254657, 0.3231302842740779, 0.3221163137856385, 0.32286576420554647, 0.32109772393722896, 0.3214537504169604, 0.31757608817323396, 0.3204399733196782, 0.3197161258076844, 0.32174596218797175, 0.31766305831463443, 0.31954407732447854, 0.3234282588262273, 0.32309116617493006, 0.32178533749411936, 0.3219004385173321, 0.3235895915928742, 0.32087582054183533, 0.32145683167745237, 0.3208794119241445, 0.32333701091778017, 0.3219486176967621, 0.32210156548282376, 0.32442428904545045, 0.3201279140522946, 0.32072137802353373, 0.32214724171258835, 0.32226168680126255, 0.32369281699799973, 0.32419207967493846, 0.32292445740945963, 0.32255667873212823, 0.32209461943610856, 0.32349734202675196, 0.3227668537798783, 0.3227404179861364, 0.3231286892829382, 0.3234272963650849, 0.32372587129635655, 0.3224756568670273, 0.3218676699730365, 0.3216848888474962, 0.32194007941238256, 0.32392903012426005, 0.32182045070373494, 0.32287658366334177, 0.3232534072645333, 0.32362657161834446, 0.3245893243862235, 0.3245370902283036, 0.32316617449016677, 0.3228209849608981, 0.3234058405556109, 0.32200006108083157, 0.3231174007220113, 0.3235464347526431, 0.32288107203076716, 0.32346022250535694, 0.32295176058845676, 0.32383862364551297, 0.32381212444085145, 0.32408990488266165, 0.32418954429095204, 0.32345412817338237, 0.3237338568205419, 0.3241207129119531, 0.3236892876093802, 0.3236751958484883, 0.3239304688151764, 0.323041512509403, 0.32375328360206407, 0.3237425248422053, 0.3228481409504362, 0.32307619739161886, 0.3236369493053011, 0.32412421071658964, 0.32408262975513935, 0.3242473806376043, 0.3241713510259338, 0.3239268351589208, 0.323644383162584, 0.32360334220625786, 0.3234153157505004, 0.323257604451931, 0.32364876657400443, 0.3235282098631496, 0.3235174155510638, 0.32397396753177693, 0.32407130114734173], 'loss': [0.744111726819534, 0.6558416552360098, 0.6187802665370737, 0.5948238276987058, 0.5680993662181268, 0.5458013954158885, 0.5258126910197773, 0.5137529089700421, 0.5049893381943333, 0.49694878706373485, 0.48912427681069814, 0.48255900807064517, 0.47549884791951885, 0.4743943097879444, 0.4706151631653196, 0.46279946586581516, 0.45209244425085393, 0.43598735495607743, 0.42503555771346774, 0.41763940690913526, 0.4121905859207096, 0.4105016909434435, 0.40833745835978275, 0.4049445281159773, 0.4029972041153194, 0.3998494184094099, 0.39614235690417443, 0.3959045465921637, 0.39058028734596306, 0.3883167480464931, 0.3900518025674522, 0.38616754415833865, 0.3854319158498484, 0.38339775780480884, 0.38178556971852884, 0.37755763960078265, 0.3759581821555489, 0.37715580587671416, 0.37366145001143125, 0.37159603337833647, 0.37126598574054537, 0.37173148305706005, 0.3702943911905455, 0.3678696359015503, 0.3695929533734525, 0.36680729867407047, 0.36338838768795767, 0.36407368199580475, 0.36153944871929306, 0.36203082696404043, 0.36061741018642746, 0.36030148045480126, 0.3603640768098041, 0.3583029837381032, 0.35785880640094414, 0.3542787035085267, 0.35588209722421105, 0.35325866652842636, 0.35161939794142766, 0.3497899501159786, 0.35259629759325417, 0.34863101516036255, 0.348795299217584, 0.3506352862469385, 0.34807151082572846, 0.3480054712200048, 0.34842373945448074, 0.34387144025646016, 0.3478298016146964, 0.34863970110673426, 0.3463275243010878, 0.34931473881055974, 0.34677620749476756, 0.3452400576552449, 0.34336332618255266, 0.34734587957681595, 0.34366872159720446, 0.342265780947606, 0.3408666911963587, 0.34411138137650177, 0.3438102955222196, 0.3423688508489795, 0.3421272087089211, 0.34017764830737823, 0.3410034228026343, 0.3421429836063075, 0.34296843453633374, 0.34456723267688066, 0.3423059330340394, 0.34428762495086723, 0.34035888121952695, 0.34129855931899294, 0.34032056912231107, 0.34148396852483526, 0.3389341856205552, 0.3421324403045413, 0.33896563884041997, 0.33944870139178873, 0.33893075194808664, 0.3404996357514206, 0.34172599185214636, 0.3414477505007818, 0.3411244449579407, 0.338919469161484, 0.33980834276554966, 0.3422948388186561, 0.34012836603795293, 0.3395177960621262, 0.3408015410220322, 0.33936512783221134, 0.3387130022029058, 0.34076342819158484, 0.3387220558295036, 0.3374561975610735, 0.33983409113660645, 0.33906527266564446, 0.34193516219533554, 0.338994556810636, 0.33859787290068966, 0.33955174460070514, 0.3400885377388826, 0.338219833842824, 0.33734245743697416, 0.3372847322780307, 0.3401668523274058, 0.34058753758029026, 0.3399817175754571, 0.3391006550211731, 0.33943343065726317, 0.33802786517533145, 0.3394250968185836, 0.3401880767264797, 0.33688313958840876], 'acc': [0.8835439803948033, 0.9066889076506631, 0.9150352752743156, 0.9200690203281143, 0.9225149666630449, 0.9242510407755273, 0.9267053817307569, 0.9281161787685238, 0.929187699477709, 0.9303387722630617, 0.931518585627012, 0.9322000431568873, 0.9332572395948219, 0.9338879583210552, 0.9342924146461784, 0.934971314547311, 0.9353847701526454, 0.9357939482477884, 0.9362039848315011, 0.9368316567518353, 0.9373027132872759, 0.9377367947520397, 0.938059956044214, 0.9384613663060923, 0.9387066968358098, 0.9389387715816339, 0.9391895430426747, 0.9396195761658005, 0.9400604954350975, 0.9404366539561426, 0.9402731307715406, 0.9406380550934829, 0.9408474345005107, 0.9408789205596237, 0.9412546812433703, 0.9416284669173653, 0.9415819829331713, 0.9418790984477333, 0.9421181027424297, 0.9424100561061867, 0.9424257752149783, 0.9425303034143727, 0.9427571914023347, 0.9428879492725012, 0.9428433016736617, 0.9432509669287874, 0.9434412776833714, 0.9434795301792579, 0.9435734301391069, 0.9438066205706264, 0.9439984654767559, 0.9440428588424565, 0.9440682280818219, 0.9442600746257696, 0.9442184028695509, 0.9443406954545281, 0.9444877762645433, 0.9448763740806694, 0.9448052645790549, 0.9450879379259836, 0.9449405546766033, 0.9450911710453002, 0.9450712892857738, 0.945164096416442, 0.9452299271373598, 0.945327501293549, 0.9454006586181983, 0.9454310744128883, 0.9454453052224114, 0.9454074493039635, 0.9455444857002379, 0.9455370202068758, 0.9456719162135517, 0.9457935124274316, 0.945709587648695, 0.945776978414599, 0.9458424153828172, 0.945988845216484, 0.9459619171124224, 0.9459055729192971, 0.94588627242607, 0.9460316555745902, 0.9459584989329177, 0.9461963409072022, 0.9460778612822098, 0.9460340742407631, 0.9459232923345049, 0.9460572579212297, 0.9462430802119933, 0.946176411849875, 0.946244546629481, 0.9463262836744024, 0.9462351977646132, 0.9463132387314136, 0.9463389104764525, 0.9461745753716887, 0.9464709708032598, 0.9464260440393131, 0.9463299113885356, 0.946371653113406, 0.9462951235609917, 0.9463287023206828, 0.9464647614169485, 0.9463456316378321, 0.9463769764599963, 0.9463015420641885, 0.946337422429156, 0.9464764350221027, 0.9464300424639674, 0.9464863410567876, 0.9464532044416859, 0.9464486223569979, 0.9464490416981193, 0.9464833886541956, 0.9464809224448807, 0.9466134929089383, 0.9465637294690991, 0.9465220369251887, 0.9464698766412798, 0.9464422511913869, 0.9465094097982274, 0.946498061104364, 0.9465126638710067, 0.9465723807361636, 0.9465246634948027, 0.9464903409402274, 0.9464742487404421, 0.9464507621896056, 0.946575031183442, 0.9465180593677962, 0.9466845357971072, 0.9465614517680104, 0.9465924254420178], 'mDice': [0.19864641660852392, 0.2933251572581602, 0.3331356110847597, 0.3588850908416545, 0.3877292396176417, 0.41183569933524955, 0.43341290791303383, 0.4464265646101941, 0.45587281710858313, 0.4645380968511244, 0.4729717207214885, 0.4800508447676907, 0.4876537821106948, 0.48883049996520644, 0.4929008159041736, 0.5013345013352811, 0.5129245052428887, 0.5303586095070889, 0.5422030277711847, 0.5502052775796866, 0.5560788815922514, 0.5579096339265872, 0.5602278403199307, 0.5638990903447078, 0.5660042131897975, 0.5694046256665274, 0.5734118000692382, 0.5736503226696842, 0.5794019012207687, 0.5818403414683183, 0.5799635055134381, 0.5841538621567953, 0.584944762467728, 0.5871493639100154, 0.5888734569053838, 0.593436563289501, 0.595179381035289, 0.5938571936257464, 0.5976367449121224, 0.5998633100891262, 0.600223269653819, 0.5997073583852756, 0.6012392659741125, 0.6038647555299695, 0.6019977063209146, 0.6049842652522334, 0.6087056988735646, 0.6079663756970821, 0.610706374236508, 0.6101526373325751, 0.6116711230083864, 0.6120136073320219, 0.6119421942903224, 0.6141741040259822, 0.6146544436085462, 0.618522060853725, 0.6167718698031806, 0.6196000550888393, 0.6213736475037844, 0.6233524736698722, 0.620317614427304, 0.6246092750559608, 0.624430202240275, 0.6224344753028156, 0.6250871831350404, 0.6252745477730711, 0.6248216392042003, 0.6297409592673144, 0.6254164849124398, 0.6245476510410964, 0.6270704787012759, 0.6238450685814181, 0.626552491607649, 0.628233105942171, 0.6302755744521696, 0.625972426491991, 0.6299498779567626, 0.631465856205941, 0.6329865040735615, 0.6294673047217979, 0.6297922573759049, 0.6313465109708537, 0.6316149088959498, 0.6337149153032642, 0.6328163970390699, 0.6314707107320618, 0.6306429482206565, 0.6289709033882781, 0.6314052276927486, 0.6291583140112028, 0.6335199959406215, 0.632499179402187, 0.6335582734215232, 0.6322873304041147, 0.6350589784065096, 0.6315957159085751, 0.6350216704277457, 0.6344985313218724, 0.6350652207547399, 0.6333507133618996, 0.6320384724384978, 0.6323335739075375, 0.6326876856127229, 0.6350767396954888, 0.6341005305796182, 0.6313584125112248, 0.633764688324671, 0.6344064816721475, 0.6330168019851306, 0.634577246831036, 0.6352879698635381, 0.6330677968785473, 0.6352796977347959, 0.6366523469113736, 0.6340725638203606, 0.6348406433037993, 0.6317983173610557, 0.6349897129488729, 0.6354164663047039, 0.634340679178727, 0.6337981434920855, 0.6358245635923839, 0.6367570949430951, 0.6368137880246384, 0.6337120453883549, 0.6332594520563756, 0.6339141426846056, 0.6348721325258244, 0.6345042773780522, 0.6360024156699338, 0.6345126304097058, 0.6336890105703062, 0.6372736481435175], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 1.5625e-06]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:01<00:06,  1.59s/it]predicting test subjects:  40%|████      | 2/5 [00:02<00:04,  1.43s/it]predicting test subjects:  60%|██████    | 3/5 [00:03<00:02,  1.26s/it]predicting test subjects:  80%|████████  | 4/5 [00:04<00:01,  1.13s/it]predicting test subjects: 100%|██████████| 5/5 [00:05<00:00,  1.12s/it]predicting test subjects: 100%|██████████| 5/5 [00:05<00:00,  1.09s/it]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM00_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_b’: File exists
mkdir: cannot create directory ‘sd0’: File exists
mkdir: cannot create directory ‘sd1’: File exists
mkdir: cannot create directory ‘sd2’: File exists
cp: cannot stat '/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_b/sd0/vimp*': No such file or directory
cp: cannot stat '/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_b/sd1/vimp*': No such file or directory
cp: cannot stat '/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_b/sd2/vimp*': No such file or directory

  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [00:00<00:01,  2.81it/s] 40%|████      | 2/5 [00:00<00:01,  2.87it/s] 60%|██████    | 3/5 [00:00<00:00,  3.04it/s] 80%|████████  | 4/5 [00:01<00:00,  3.24it/s]100%|██████████| 5/5 [00:01<00:00,  3.14it/s]100%|██████████| 5/5 [00:01<00:00,  3.18it/s]

CrossVal ['b']
Error in label values min 0.0 max 2.0      1-THALAMUS
2020-01-22 10:27:08.293246: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-22 10:27:13.930758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-22 10:27:13.930824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 10:27:14.348468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 10:27:14.348535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 10:27:14.348547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 10:27:14.348992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
Loading train:   0%|          | 0/247 [00:00<?, ?it/s]Loading train:   0%|          | 1/247 [00:00<01:18,  3.12it/s]Loading train:   1%|          | 2/247 [00:00<01:12,  3.36it/s]Loading train:   1%|          | 3/247 [00:00<01:08,  3.57it/s]Loading train:   2%|▏         | 4/247 [00:01<01:06,  3.67it/s]Loading train:   2%|▏         | 5/247 [00:01<01:02,  3.87it/s]Loading train:   2%|▏         | 6/247 [00:01<00:59,  4.04it/s]Loading train:   3%|▎         | 7/247 [00:01<00:57,  4.18it/s]Loading train:   3%|▎         | 8/247 [00:01<00:55,  4.28it/s]Loading train:   4%|▎         | 9/247 [00:02<00:55,  4.32it/s]Loading train:   4%|▍         | 10/247 [00:02<00:54,  4.35it/s]Loading train:   4%|▍         | 11/247 [00:02<00:54,  4.36it/s]Loading train:   5%|▍         | 12/247 [00:02<00:53,  4.40it/s]Loading train:   5%|▌         | 13/247 [00:03<00:53,  4.40it/s]Loading train:   6%|▌         | 14/247 [00:03<00:52,  4.43it/s]Loading train:   6%|▌         | 15/247 [00:03<00:51,  4.48it/s]Loading train:   6%|▋         | 16/247 [00:03<00:51,  4.46it/s]Loading train:   7%|▋         | 17/247 [00:03<00:51,  4.44it/s]Loading train:   7%|▋         | 18/247 [00:04<00:51,  4.42it/s]Loading train:   8%|▊         | 19/247 [00:04<00:51,  4.43it/s]Loading train:   8%|▊         | 20/247 [00:04<00:51,  4.41it/s]Loading train:   9%|▊         | 21/247 [00:04<00:51,  4.43it/s]Loading train:   9%|▉         | 22/247 [00:05<00:51,  4.40it/s]Loading train:   9%|▉         | 23/247 [00:05<00:50,  4.44it/s]Loading train:  10%|▉         | 24/247 [00:05<00:50,  4.44it/s]Loading train:  10%|█         | 25/247 [00:05<00:49,  4.45it/s]Loading train:  11%|█         | 26/247 [00:05<00:49,  4.47it/s]Loading train:  11%|█         | 27/247 [00:06<00:49,  4.47it/s]Loading train:  11%|█▏        | 28/247 [00:06<00:48,  4.48it/s]Loading train:  12%|█▏        | 29/247 [00:06<00:48,  4.46it/s]Loading train:  12%|█▏        | 30/247 [00:06<00:48,  4.49it/s]Loading train:  13%|█▎        | 31/247 [00:07<00:48,  4.49it/s]Loading train:  13%|█▎        | 32/247 [00:07<00:47,  4.49it/s]Loading train:  13%|█▎        | 33/247 [00:07<00:47,  4.50it/s]Loading train:  14%|█▍        | 34/247 [00:07<00:47,  4.52it/s]Loading train:  14%|█▍        | 35/247 [00:07<00:46,  4.51it/s]Loading train:  15%|█▍        | 36/247 [00:08<00:46,  4.51it/s]Loading train:  15%|█▍        | 37/247 [00:08<00:46,  4.51it/s]Loading train:  15%|█▌        | 38/247 [00:08<00:46,  4.51it/s]Loading train:  16%|█▌        | 39/247 [00:08<00:45,  4.53it/s]Loading train:  16%|█▌        | 40/247 [00:09<00:45,  4.53it/s]Loading train:  17%|█▋        | 41/247 [00:09<00:45,  4.52it/s]Loading train:  17%|█▋        | 42/247 [00:09<00:45,  4.52it/s]Loading train:  17%|█▋        | 43/247 [00:09<00:45,  4.51it/s]Loading train:  18%|█▊        | 44/247 [00:09<00:45,  4.51it/s]Loading train:  18%|█▊        | 45/247 [00:10<00:44,  4.54it/s]Loading train:  19%|█▊        | 46/247 [00:10<00:43,  4.58it/s]Loading train:  19%|█▉        | 47/247 [00:10<00:43,  4.60it/s]Loading train:  19%|█▉        | 48/247 [00:10<00:42,  4.63it/s]Loading train:  20%|█▉        | 49/247 [00:11<00:42,  4.64it/s]Loading train:  20%|██        | 50/247 [00:11<00:42,  4.65it/s]Loading train:  21%|██        | 51/247 [00:11<00:42,  4.62it/s]Loading train:  21%|██        | 52/247 [00:11<00:42,  4.64it/s]Loading train:  21%|██▏       | 53/247 [00:11<00:41,  4.66it/s]Loading train:  22%|██▏       | 54/247 [00:12<00:41,  4.66it/s]Loading train:  22%|██▏       | 55/247 [00:12<00:41,  4.66it/s]Loading train:  23%|██▎       | 56/247 [00:12<00:41,  4.65it/s]Loading train:  23%|██▎       | 57/247 [00:12<00:40,  4.67it/s]Loading train:  23%|██▎       | 58/247 [00:12<00:40,  4.67it/s]Loading train:  24%|██▍       | 59/247 [00:13<00:40,  4.65it/s]Loading train:  24%|██▍       | 60/247 [00:13<00:40,  4.62it/s]Loading train:  25%|██▍       | 61/247 [00:13<00:40,  4.61it/s]Loading train:  25%|██▌       | 62/247 [00:13<00:40,  4.60it/s]Loading train:  26%|██▌       | 63/247 [00:14<00:40,  4.56it/s]Loading train:  26%|██▌       | 64/247 [00:14<00:40,  4.55it/s]Loading train:  26%|██▋       | 65/247 [00:14<00:40,  4.55it/s]Loading train:  27%|██▋       | 66/247 [00:14<00:39,  4.55it/s]Loading train:  27%|██▋       | 67/247 [00:14<00:39,  4.56it/s]Loading train:  28%|██▊       | 68/247 [00:15<00:39,  4.55it/s]Loading train:  28%|██▊       | 69/247 [00:15<00:39,  4.56it/s]Loading train:  28%|██▊       | 70/247 [00:15<00:38,  4.55it/s]Loading train:  29%|██▊       | 71/247 [00:15<00:38,  4.57it/s]Loading train:  29%|██▉       | 72/247 [00:16<00:38,  4.57it/s]Loading train:  30%|██▉       | 73/247 [00:16<00:37,  4.59it/s]Loading train:  30%|██▉       | 74/247 [00:16<00:37,  4.60it/s]Loading train:  30%|███       | 75/247 [00:16<00:37,  4.61it/s]Loading train:  31%|███       | 76/247 [00:16<00:37,  4.57it/s]Loading train:  31%|███       | 77/247 [00:17<00:40,  4.16it/s]Loading train:  32%|███▏      | 78/247 [00:17<00:43,  3.93it/s]Loading train:  32%|███▏      | 79/247 [00:17<00:41,  4.06it/s]Loading train:  32%|███▏      | 80/247 [00:17<00:40,  4.15it/s]Loading train:  33%|███▎      | 81/247 [00:18<00:41,  4.03it/s]Loading train:  33%|███▎      | 82/247 [00:18<00:41,  3.99it/s]Loading train:  34%|███▎      | 83/247 [00:18<00:41,  3.96it/s]Loading train:  34%|███▍      | 84/247 [00:19<00:41,  3.91it/s]Loading train:  34%|███▍      | 85/247 [00:19<00:41,  3.91it/s]Loading train:  35%|███▍      | 86/247 [00:19<00:41,  3.90it/s]Loading train:  35%|███▌      | 87/247 [00:19<00:41,  3.89it/s]Loading train:  36%|███▌      | 88/247 [00:20<00:40,  3.89it/s]Loading train:  36%|███▌      | 89/247 [00:20<00:40,  3.88it/s]Loading train:  36%|███▋      | 90/247 [00:20<00:40,  3.89it/s]Loading train:  37%|███▋      | 91/247 [00:20<00:39,  3.91it/s]Loading train:  37%|███▋      | 92/247 [00:21<00:39,  3.91it/s]Loading train:  38%|███▊      | 93/247 [00:21<00:39,  3.90it/s]Loading train:  38%|███▊      | 94/247 [00:21<00:39,  3.89it/s]Loading train:  38%|███▊      | 95/247 [00:21<00:38,  3.91it/s]Loading train:  39%|███▉      | 96/247 [00:22<00:38,  3.91it/s]Loading train:  39%|███▉      | 97/247 [00:22<00:38,  3.92it/s]Loading train:  40%|███▉      | 98/247 [00:22<00:37,  3.92it/s]Loading train:  40%|████      | 99/247 [00:22<00:37,  3.93it/s]Loading train:  40%|████      | 100/247 [00:23<00:36,  3.98it/s]Loading train:  41%|████      | 101/247 [00:23<00:36,  3.98it/s]Loading train:  41%|████▏     | 102/247 [00:23<00:36,  4.00it/s]Loading train:  42%|████▏     | 103/247 [00:23<00:35,  4.03it/s]Loading train:  42%|████▏     | 104/247 [00:24<00:35,  4.05it/s]Loading train:  43%|████▎     | 105/247 [00:24<00:35,  4.02it/s]Loading train:  43%|████▎     | 106/247 [00:24<00:35,  4.01it/s]Loading train:  43%|████▎     | 107/247 [00:24<00:34,  4.01it/s]Loading train:  44%|████▎     | 108/247 [00:25<00:34,  4.04it/s]Loading train:  44%|████▍     | 109/247 [00:25<00:34,  4.05it/s]Loading train:  45%|████▍     | 110/247 [00:25<00:33,  4.06it/s]Loading train:  45%|████▍     | 111/247 [00:25<00:33,  4.07it/s]Loading train:  45%|████▌     | 112/247 [00:26<00:33,  4.04it/s]Loading train:  46%|████▌     | 113/247 [00:26<00:33,  3.99it/s]Loading train:  46%|████▌     | 114/247 [00:26<00:33,  3.98it/s]Loading train:  47%|████▋     | 115/247 [00:26<00:32,  4.00it/s]Loading train:  47%|████▋     | 116/247 [00:27<00:32,  4.02it/s]Loading train:  47%|████▋     | 117/247 [00:27<00:32,  3.98it/s]Loading train:  48%|████▊     | 118/247 [00:27<00:30,  4.19it/s]Loading train:  48%|████▊     | 119/247 [00:27<00:29,  4.35it/s]Loading train:  49%|████▊     | 120/247 [00:27<00:28,  4.49it/s]Loading train:  49%|████▉     | 121/247 [00:28<00:27,  4.59it/s]Loading train:  49%|████▉     | 122/247 [00:28<00:26,  4.68it/s]Loading train:  50%|████▉     | 123/247 [00:28<00:26,  4.76it/s]Loading train:  50%|█████     | 124/247 [00:28<00:25,  4.80it/s]Loading train:  51%|█████     | 125/247 [00:28<00:25,  4.83it/s]Loading train:  51%|█████     | 126/247 [00:29<00:24,  4.86it/s]Loading train:  51%|█████▏    | 127/247 [00:29<00:24,  4.87it/s]Loading train:  52%|█████▏    | 128/247 [00:29<00:24,  4.85it/s]Loading train:  52%|█████▏    | 129/247 [00:29<00:24,  4.87it/s]Loading train:  53%|█████▎    | 130/247 [00:29<00:24,  4.87it/s]Loading train:  53%|█████▎    | 131/247 [00:30<00:23,  4.87it/s]Loading train:  53%|█████▎    | 132/247 [00:30<00:23,  4.88it/s]Loading train:  54%|█████▍    | 133/247 [00:30<00:23,  4.90it/s]Loading train:  54%|█████▍    | 134/247 [00:30<00:23,  4.87it/s]Loading train:  55%|█████▍    | 135/247 [00:31<00:22,  4.88it/s]Loading train:  55%|█████▌    | 136/247 [00:31<00:22,  4.84it/s]Loading train:  55%|█████▌    | 137/247 [00:31<00:23,  4.76it/s]Loading train:  56%|█████▌    | 138/247 [00:31<00:23,  4.69it/s]Loading train:  56%|█████▋    | 139/247 [00:31<00:23,  4.62it/s]Loading train:  57%|█████▋    | 140/247 [00:32<00:23,  4.59it/s]Loading train:  57%|█████▋    | 141/247 [00:32<00:23,  4.56it/s]Loading train:  57%|█████▋    | 142/247 [00:32<00:23,  4.51it/s]Loading train:  58%|█████▊    | 143/247 [00:32<00:23,  4.52it/s]Loading train:  58%|█████▊    | 144/247 [00:32<00:22,  4.53it/s]Loading train:  59%|█████▊    | 145/247 [00:33<00:22,  4.49it/s]Loading train:  59%|█████▉    | 146/247 [00:33<00:22,  4.48it/s]Loading train:  60%|█████▉    | 147/247 [00:33<00:22,  4.54it/s]Loading train:  60%|█████▉    | 148/247 [00:33<00:21,  4.60it/s]Loading train:  60%|██████    | 149/247 [00:34<00:21,  4.64it/s]Loading train:  61%|██████    | 150/247 [00:34<00:20,  4.67it/s]Loading train:  61%|██████    | 151/247 [00:34<00:20,  4.66it/s]Loading train:  62%|██████▏   | 152/247 [00:34<00:20,  4.68it/s]Loading train:  62%|██████▏   | 153/247 [00:34<00:20,  4.68it/s]Loading train:  62%|██████▏   | 154/247 [00:35<00:20,  4.50it/s]Loading train:  63%|██████▎   | 155/247 [00:35<00:20,  4.39it/s]Loading train:  63%|██████▎   | 156/247 [00:35<00:21,  4.33it/s]Loading train:  64%|██████▎   | 157/247 [00:35<00:21,  4.27it/s]Loading train:  64%|██████▍   | 158/247 [00:36<00:21,  4.24it/s]Loading train:  64%|██████▍   | 159/247 [00:36<00:21,  4.18it/s]Loading train:  65%|██████▍   | 160/247 [00:36<00:21,  4.14it/s]Loading train:  65%|██████▌   | 161/247 [00:36<00:20,  4.14it/s]Loading train:  66%|██████▌   | 162/247 [00:37<00:20,  4.15it/s]Loading train:  66%|██████▌   | 163/247 [00:37<00:20,  4.13it/s]Loading train:  66%|██████▋   | 164/247 [00:37<00:20,  4.06it/s]Loading train:  67%|██████▋   | 165/247 [00:37<00:20,  4.07it/s]Loading train:  67%|██████▋   | 166/247 [00:38<00:19,  4.06it/s]Loading train:  68%|██████▊   | 167/247 [00:38<00:19,  4.08it/s]Loading train:  68%|██████▊   | 168/247 [00:38<00:19,  4.11it/s]Loading train:  68%|██████▊   | 169/247 [00:38<00:18,  4.13it/s]Loading train:  69%|██████▉   | 170/247 [00:39<00:18,  4.11it/s]Loading train:  69%|██████▉   | 171/247 [00:39<00:18,  4.06it/s]Loading train:  70%|██████▉   | 172/247 [00:39<00:18,  4.15it/s]Loading train:  70%|███████   | 173/247 [00:39<00:17,  4.27it/s]Loading train:  70%|███████   | 174/247 [00:40<00:16,  4.31it/s]Loading train:  71%|███████   | 175/247 [00:40<00:17,  4.12it/s]Loading train:  71%|███████▏  | 176/247 [00:40<00:16,  4.26it/s]Loading train:  72%|███████▏  | 177/247 [00:40<00:16,  4.37it/s]Loading train:  72%|███████▏  | 178/247 [00:40<00:15,  4.46it/s]Loading train:  72%|███████▏  | 179/247 [00:41<00:15,  4.52it/s]Loading train:  73%|███████▎  | 180/247 [00:41<00:14,  4.58it/s]Loading train:  73%|███████▎  | 181/247 [00:41<00:14,  4.57it/s]Loading train:  74%|███████▎  | 182/247 [00:41<00:14,  4.49it/s]Loading train:  74%|███████▍  | 183/247 [00:42<00:14,  4.50it/s]Loading train:  74%|███████▍  | 184/247 [00:42<00:14,  4.46it/s]Loading train:  75%|███████▍  | 185/247 [00:42<00:13,  4.48it/s]Loading train:  75%|███████▌  | 186/247 [00:42<00:13,  4.51it/s]Loading train:  76%|███████▌  | 187/247 [00:42<00:13,  4.49it/s]Loading train:  76%|███████▌  | 188/247 [00:43<00:13,  4.50it/s]Loading train:  77%|███████▋  | 189/247 [00:43<00:12,  4.55it/s]Loading train:  77%|███████▋  | 190/247 [00:43<00:12,  4.59it/s]Loading train:  77%|███████▋  | 191/247 [00:43<00:12,  4.62it/s]Loading train:  78%|███████▊  | 192/247 [00:43<00:11,  4.60it/s]Loading train:  78%|███████▊  | 193/247 [00:44<00:11,  4.55it/s]Loading train:  79%|███████▊  | 194/247 [00:44<00:11,  4.59it/s]Loading train:  79%|███████▉  | 195/247 [00:44<00:11,  4.65it/s]Loading train:  79%|███████▉  | 196/247 [00:44<00:10,  4.69it/s]Loading train:  80%|███████▉  | 197/247 [00:45<00:10,  4.74it/s]Loading train:  80%|████████  | 198/247 [00:45<00:10,  4.78it/s]Loading train:  81%|████████  | 199/247 [00:45<00:09,  4.81it/s]Loading train:  81%|████████  | 200/247 [00:45<00:09,  4.82it/s]Loading train:  81%|████████▏ | 201/247 [00:45<00:09,  4.82it/s]Loading train:  82%|████████▏ | 202/247 [00:46<00:09,  4.83it/s]Loading train:  82%|████████▏ | 203/247 [00:46<00:09,  4.79it/s]Loading train:  83%|████████▎ | 204/247 [00:46<00:08,  4.81it/s]Loading train:  83%|████████▎ | 205/247 [00:46<00:08,  4.78it/s]Loading train:  83%|████████▎ | 206/247 [00:46<00:08,  4.80it/s]Loading train:  84%|████████▍ | 207/247 [00:47<00:08,  4.81it/s]Loading train:  84%|████████▍ | 208/247 [00:47<00:08,  4.81it/s]Loading train:  85%|████████▍ | 209/247 [00:47<00:07,  4.81it/s]Loading train:  85%|████████▌ | 210/247 [00:47<00:07,  4.80it/s]Loading train:  85%|████████▌ | 211/247 [00:47<00:07,  4.81it/s]Loading train:  86%|████████▌ | 212/247 [00:48<00:07,  4.73it/s]Loading train:  86%|████████▌ | 213/247 [00:48<00:07,  4.69it/s]Loading train:  87%|████████▋ | 214/247 [00:48<00:07,  4.64it/s]Loading train:  87%|████████▋ | 215/247 [00:48<00:06,  4.58it/s]Loading train:  87%|████████▋ | 216/247 [00:49<00:06,  4.59it/s]Loading train:  88%|████████▊ | 217/247 [00:49<00:06,  4.60it/s]Loading train:  88%|████████▊ | 218/247 [00:49<00:06,  4.59it/s]Loading train:  89%|████████▊ | 219/247 [00:49<00:06,  4.57it/s]Loading train:  89%|████████▉ | 220/247 [00:49<00:05,  4.53it/s]Loading train:  89%|████████▉ | 221/247 [00:50<00:05,  4.52it/s]Loading train:  90%|████████▉ | 222/247 [00:50<00:05,  4.51it/s]Loading train:  90%|█████████ | 223/247 [00:50<00:05,  4.52it/s]Loading train:  91%|█████████ | 224/247 [00:50<00:05,  4.55it/s]Loading train:  91%|█████████ | 225/247 [00:51<00:04,  4.55it/s]Loading train:  91%|█████████▏| 226/247 [00:51<00:04,  4.58it/s]Loading train:  92%|█████████▏| 227/247 [00:51<00:04,  4.58it/s]Loading train:  92%|█████████▏| 228/247 [00:51<00:04,  4.59it/s]Loading train:  93%|█████████▎| 229/247 [00:51<00:03,  4.57it/s]Loading train:  93%|█████████▎| 230/247 [00:52<00:03,  4.45it/s]Loading train:  94%|█████████▎| 231/247 [00:52<00:03,  4.35it/s]Loading train:  94%|█████████▍| 232/247 [00:52<00:03,  4.30it/s]Loading train:  94%|█████████▍| 233/247 [00:52<00:03,  4.26it/s]Loading train:  95%|█████████▍| 234/247 [00:53<00:03,  4.23it/s]Loading train:  95%|█████████▌| 235/247 [00:53<00:02,  4.21it/s]Loading train:  96%|█████████▌| 236/247 [00:53<00:02,  4.17it/s]Loading train:  96%|█████████▌| 237/247 [00:53<00:02,  4.14it/s]Loading train:  96%|█████████▋| 238/247 [00:54<00:02,  4.16it/s]Loading train:  97%|█████████▋| 239/247 [00:54<00:01,  4.13it/s]Loading train:  97%|█████████▋| 240/247 [00:54<00:01,  4.14it/s]Loading train:  98%|█████████▊| 241/247 [00:54<00:01,  4.14it/s]Loading train:  98%|█████████▊| 242/247 [00:55<00:01,  4.14it/s]Loading train:  98%|█████████▊| 243/247 [00:55<00:00,  4.11it/s]Loading train:  99%|█████████▉| 244/247 [00:55<00:00,  4.11it/s]Loading train:  99%|█████████▉| 245/247 [00:55<00:00,  4.12it/s]Loading train: 100%|█████████▉| 246/247 [00:56<00:00,  4.13it/s]Loading train: 100%|██████████| 247/247 [00:56<00:00,  4.14it/s]Loading train: 100%|██████████| 247/247 [00:56<00:00,  4.39it/s]
concatenating: train:   0%|          | 0/247 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/247 [00:00<00:04, 50.57it/s]concatenating: train:   5%|▍         | 12/247 [00:00<00:04, 52.50it/s]concatenating: train:   7%|▋         | 18/247 [00:00<00:04, 53.44it/s]concatenating: train:  10%|▉         | 24/247 [00:00<00:04, 54.09it/s]concatenating: train:  12%|█▏        | 30/247 [00:00<00:03, 54.99it/s]concatenating: train:  15%|█▍        | 36/247 [00:00<00:03, 55.49it/s]concatenating: train:  17%|█▋        | 42/247 [00:00<00:03, 55.62it/s]concatenating: train:  19%|█▉        | 48/247 [00:00<00:03, 55.34it/s]concatenating: train:  22%|██▏       | 54/247 [00:00<00:03, 55.74it/s]concatenating: train:  24%|██▍       | 60/247 [00:01<00:03, 55.84it/s]concatenating: train:  27%|██▋       | 66/247 [00:01<00:03, 55.68it/s]concatenating: train:  29%|██▉       | 72/247 [00:01<00:03, 55.92it/s]concatenating: train:  32%|███▏      | 78/247 [00:01<00:03, 54.31it/s]concatenating: train:  34%|███▍      | 84/247 [00:01<00:03, 53.70it/s]concatenating: train:  36%|███▋      | 90/247 [00:01<00:03, 52.24it/s]concatenating: train:  39%|███▉      | 96/247 [00:01<00:02, 51.77it/s]concatenating: train:  41%|████▏     | 102/247 [00:01<00:02, 51.86it/s]concatenating: train:  44%|████▎     | 108/247 [00:01<00:02, 52.19it/s]concatenating: train:  46%|████▌     | 114/247 [00:02<00:02, 51.64it/s]concatenating: train:  49%|████▊     | 120/247 [00:02<00:02, 52.85it/s]concatenating: train:  51%|█████     | 126/247 [00:02<00:02, 54.66it/s]concatenating: train:  54%|█████▍    | 133/247 [00:02<00:02, 56.34it/s]concatenating: train:  56%|█████▋    | 139/247 [00:02<00:01, 57.33it/s]concatenating: train:  59%|█████▊    | 145/247 [00:02<00:01, 57.54it/s]concatenating: train:  61%|██████    | 151/247 [00:02<00:01, 57.46it/s]concatenating: train:  64%|██████▎   | 157/247 [00:02<00:01, 56.34it/s]concatenating: train:  66%|██████▌   | 163/247 [00:02<00:01, 55.34it/s]concatenating: train:  68%|██████▊   | 169/247 [00:03<00:01, 54.70it/s]concatenating: train:  71%|███████   | 175/247 [00:03<00:01, 54.20it/s]concatenating: train:  73%|███████▎  | 181/247 [00:03<00:01, 55.07it/s]concatenating: train:  76%|███████▌  | 187/247 [00:03<00:01, 56.13it/s]concatenating: train:  78%|███████▊  | 193/247 [00:03<00:00, 56.70it/s]concatenating: train:  81%|████████  | 199/247 [00:03<00:00, 57.48it/s]concatenating: train:  83%|████████▎ | 205/247 [00:03<00:00, 57.78it/s]concatenating: train:  85%|████████▌ | 211/247 [00:03<00:00, 58.05it/s]concatenating: train:  88%|████████▊ | 217/247 [00:03<00:00, 57.59it/s]concatenating: train:  90%|█████████ | 223/247 [00:04<00:00, 57.37it/s]concatenating: train:  93%|█████████▎| 229/247 [00:04<00:00, 56.90it/s]concatenating: train:  95%|█████████▌| 235/247 [00:04<00:00, 54.42it/s]concatenating: train:  98%|█████████▊| 241/247 [00:04<00:00, 53.45it/s]concatenating: train: 100%|██████████| 247/247 [00:04<00:00, 52.90it/s]concatenating: train: 100%|██████████| 247/247 [00:04<00:00, 55.05it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:00<00:00,  4.00it/s]Loading test:  40%|████      | 2/5 [00:00<00:00,  3.88it/s]Loading test:  60%|██████    | 3/5 [00:00<00:00,  3.93it/s]Loading test:  80%|████████  | 4/5 [00:00<00:00,  4.17it/s]Loading test: 100%|██████████| 5/5 [00:01<00:00,  4.15it/s]Loading test: 100%|██████████| 5/5 [00:01<00:00,  4.09it/s]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 64.83it/s]
Loading trainS:   0%|          | 0/247 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/247 [00:00<00:59,  4.14it/s]Loading trainS:   1%|          | 2/247 [00:00<00:59,  4.14it/s]Loading trainS:   1%|          | 3/247 [00:00<00:58,  4.15it/s]Loading trainS:   2%|▏         | 4/247 [00:00<00:58,  4.17it/s]Loading trainS:   2%|▏         | 5/247 [00:01<00:56,  4.26it/s]Loading trainS:   2%|▏         | 6/247 [00:01<00:55,  4.36it/s]Loading trainS:   3%|▎         | 7/247 [00:01<00:54,  4.41it/s]Loading trainS:   3%|▎         | 8/247 [00:01<00:55,  4.34it/s]Loading trainS:   4%|▎         | 9/247 [00:02<00:54,  4.40it/s]Loading trainS:   4%|▍         | 10/247 [00:02<00:53,  4.43it/s]Loading trainS:   4%|▍         | 11/247 [00:02<00:52,  4.46it/s]Loading trainS:   5%|▍         | 12/247 [00:02<00:52,  4.48it/s]Loading trainS:   5%|▌         | 13/247 [00:02<00:52,  4.50it/s]Loading trainS:   6%|▌         | 14/247 [00:03<00:51,  4.53it/s]Loading trainS:   6%|▌         | 15/247 [00:03<00:51,  4.54it/s]Loading trainS:   6%|▋         | 16/247 [00:03<00:51,  4.53it/s]Loading trainS:   7%|▋         | 17/247 [00:03<00:50,  4.52it/s]Loading trainS:   7%|▋         | 18/247 [00:04<00:50,  4.52it/s]Loading trainS:   8%|▊         | 19/247 [00:04<00:50,  4.55it/s]Loading trainS:   8%|▊         | 20/247 [00:04<00:50,  4.54it/s]Loading trainS:   9%|▊         | 21/247 [00:04<00:49,  4.56it/s]Loading trainS:   9%|▉         | 22/247 [00:04<00:49,  4.55it/s]Loading trainS:   9%|▉         | 23/247 [00:05<00:49,  4.57it/s]Loading trainS:  10%|▉         | 24/247 [00:05<00:48,  4.60it/s]Loading trainS:  10%|█         | 25/247 [00:05<00:48,  4.60it/s]Loading trainS:  11%|█         | 26/247 [00:05<00:47,  4.62it/s]Loading trainS:  11%|█         | 27/247 [00:06<00:47,  4.64it/s]Loading trainS:  11%|█▏        | 28/247 [00:06<00:47,  4.63it/s]Loading trainS:  12%|█▏        | 29/247 [00:06<00:47,  4.63it/s]Loading trainS:  12%|█▏        | 30/247 [00:06<00:46,  4.64it/s]Loading trainS:  13%|█▎        | 31/247 [00:06<00:46,  4.61it/s]Loading trainS:  13%|█▎        | 32/247 [00:07<00:46,  4.62it/s]Loading trainS:  13%|█▎        | 33/247 [00:07<00:46,  4.55it/s]Loading trainS:  14%|█▍        | 34/247 [00:07<00:46,  4.59it/s]Loading trainS:  14%|█▍        | 35/247 [00:07<00:46,  4.60it/s]Loading trainS:  15%|█▍        | 36/247 [00:07<00:45,  4.61it/s]Loading trainS:  15%|█▍        | 37/247 [00:08<00:45,  4.63it/s]Loading trainS:  15%|█▌        | 38/247 [00:08<00:45,  4.62it/s]Loading trainS:  16%|█▌        | 39/247 [00:08<00:44,  4.64it/s]Loading trainS:  16%|█▌        | 40/247 [00:08<00:44,  4.60it/s]Loading trainS:  17%|█▋        | 41/247 [00:09<00:44,  4.61it/s]Loading trainS:  17%|█▋        | 42/247 [00:09<00:44,  4.62it/s]Loading trainS:  17%|█▋        | 43/247 [00:09<00:44,  4.63it/s]Loading trainS:  18%|█▊        | 44/247 [00:09<00:44,  4.55it/s]Loading trainS:  18%|█▊        | 45/247 [00:09<00:44,  4.52it/s]Loading trainS:  19%|█▊        | 46/247 [00:10<00:43,  4.57it/s]Loading trainS:  19%|█▉        | 47/247 [00:10<00:43,  4.61it/s]Loading trainS:  19%|█▉        | 48/247 [00:10<00:42,  4.64it/s]Loading trainS:  20%|█▉        | 49/247 [00:10<00:42,  4.66it/s]Loading trainS:  20%|██        | 50/247 [00:11<00:42,  4.68it/s]Loading trainS:  21%|██        | 51/247 [00:11<00:41,  4.70it/s]Loading trainS:  21%|██        | 52/247 [00:11<00:41,  4.71it/s]Loading trainS:  21%|██▏       | 53/247 [00:11<00:41,  4.68it/s]Loading trainS:  22%|██▏       | 54/247 [00:11<00:41,  4.69it/s]Loading trainS:  22%|██▏       | 55/247 [00:12<00:41,  4.65it/s]Loading trainS:  23%|██▎       | 56/247 [00:12<00:40,  4.66it/s]Loading trainS:  23%|██▎       | 57/247 [00:12<00:40,  4.67it/s]Loading trainS:  23%|██▎       | 58/247 [00:12<00:40,  4.68it/s]Loading trainS:  24%|██▍       | 59/247 [00:12<00:40,  4.64it/s]Loading trainS:  24%|██▍       | 60/247 [00:13<00:40,  4.60it/s]Loading trainS:  25%|██▍       | 61/247 [00:13<00:40,  4.56it/s]Loading trainS:  25%|██▌       | 62/247 [00:13<00:40,  4.53it/s]Loading trainS:  26%|██▌       | 63/247 [00:13<00:40,  4.49it/s]Loading trainS:  26%|██▌       | 64/247 [00:14<00:40,  4.47it/s]Loading trainS:  26%|██▋       | 65/247 [00:14<00:41,  4.43it/s]Loading trainS:  27%|██▋       | 66/247 [00:14<00:40,  4.47it/s]Loading trainS:  27%|██▋       | 67/247 [00:14<00:39,  4.50it/s]Loading trainS:  28%|██▊       | 68/247 [00:14<00:39,  4.50it/s]Loading trainS:  28%|██▊       | 69/247 [00:15<00:39,  4.52it/s]Loading trainS:  28%|██▊       | 70/247 [00:15<00:39,  4.48it/s]Loading trainS:  29%|██▊       | 71/247 [00:15<00:40,  4.39it/s]Loading trainS:  29%|██▉       | 72/247 [00:15<00:40,  4.34it/s]Loading trainS:  30%|██▉       | 73/247 [00:16<00:40,  4.33it/s]Loading trainS:  30%|██▉       | 74/247 [00:16<00:40,  4.30it/s]Loading trainS:  30%|███       | 75/247 [00:16<00:39,  4.34it/s]Loading trainS:  31%|███       | 76/247 [00:16<00:38,  4.40it/s]Loading trainS:  31%|███       | 77/247 [00:17<00:42,  4.03it/s]Loading trainS:  32%|███▏      | 78/247 [00:17<00:43,  3.87it/s]Loading trainS:  32%|███▏      | 79/247 [00:17<00:41,  4.01it/s]Loading trainS:  32%|███▏      | 80/247 [00:17<00:41,  4.02it/s]Loading trainS:  33%|███▎      | 81/247 [00:18<00:42,  3.91it/s]Loading trainS:  33%|███▎      | 82/247 [00:18<00:42,  3.92it/s]Loading trainS:  34%|███▎      | 83/247 [00:18<00:41,  3.91it/s]Loading trainS:  34%|███▍      | 84/247 [00:18<00:41,  3.92it/s]Loading trainS:  34%|███▍      | 85/247 [00:19<00:41,  3.94it/s]Loading trainS:  35%|███▍      | 86/247 [00:19<00:41,  3.87it/s]Loading trainS:  35%|███▌      | 87/247 [00:19<00:41,  3.88it/s]Loading trainS:  36%|███▌      | 88/247 [00:19<00:41,  3.85it/s]Loading trainS:  36%|███▌      | 89/247 [00:20<00:41,  3.79it/s]Loading trainS:  36%|███▋      | 90/247 [00:20<00:41,  3.79it/s]Loading trainS:  37%|███▋      | 91/247 [00:20<00:40,  3.84it/s]Loading trainS:  37%|███▋      | 92/247 [00:20<00:39,  3.88it/s]Loading trainS:  38%|███▊      | 93/247 [00:21<00:39,  3.90it/s]Loading trainS:  38%|███▊      | 94/247 [00:21<00:41,  3.73it/s]Loading trainS:  38%|███▊      | 95/247 [00:21<00:40,  3.75it/s]Loading trainS:  39%|███▉      | 96/247 [00:22<00:39,  3.81it/s]Loading trainS:  39%|███▉      | 97/247 [00:22<00:39,  3.83it/s]Loading trainS:  40%|███▉      | 98/247 [00:22<00:38,  3.85it/s]Loading trainS:  40%|████      | 99/247 [00:22<00:38,  3.86it/s]Loading trainS:  40%|████      | 100/247 [00:23<00:37,  3.92it/s]Loading trainS:  41%|████      | 101/247 [00:23<00:36,  3.96it/s]Loading trainS:  41%|████▏     | 102/247 [00:23<00:36,  3.92it/s]Loading trainS:  42%|████▏     | 103/247 [00:23<00:36,  3.97it/s]Loading trainS:  42%|████▏     | 104/247 [00:24<00:35,  4.00it/s]Loading trainS:  43%|████▎     | 105/247 [00:24<00:35,  3.97it/s]Loading trainS:  43%|████▎     | 106/247 [00:24<00:35,  3.96it/s]Loading trainS:  43%|████▎     | 107/247 [00:24<00:34,  4.00it/s]Loading trainS:  44%|████▎     | 108/247 [00:25<00:34,  4.06it/s]Loading trainS:  44%|████▍     | 109/247 [00:25<00:33,  4.08it/s]Loading trainS:  45%|████▍     | 110/247 [00:25<00:33,  4.10it/s]Loading trainS:  45%|████▍     | 111/247 [00:25<00:33,  4.08it/s]Loading trainS:  45%|████▌     | 112/247 [00:26<00:33,  4.04it/s]Loading trainS:  46%|████▌     | 113/247 [00:26<00:33,  3.99it/s]Loading trainS:  46%|████▌     | 114/247 [00:26<00:32,  4.04it/s]Loading trainS:  47%|████▋     | 115/247 [00:26<00:32,  4.06it/s]Loading trainS:  47%|████▋     | 116/247 [00:26<00:31,  4.10it/s]Loading trainS:  47%|████▋     | 117/247 [00:27<00:31,  4.13it/s]Loading trainS:  48%|████▊     | 118/247 [00:27<00:29,  4.36it/s]Loading trainS:  48%|████▊     | 119/247 [00:27<00:28,  4.54it/s]Loading trainS:  49%|████▊     | 120/247 [00:27<00:27,  4.65it/s]Loading trainS:  49%|████▉     | 121/247 [00:28<00:26,  4.73it/s]Loading trainS:  49%|████▉     | 122/247 [00:28<00:26,  4.78it/s]Loading trainS:  50%|████▉     | 123/247 [00:28<00:25,  4.79it/s]Loading trainS:  50%|█████     | 124/247 [00:28<00:25,  4.83it/s]Loading trainS:  51%|█████     | 125/247 [00:28<00:25,  4.86it/s]Loading trainS:  51%|█████     | 126/247 [00:29<00:25,  4.84it/s]Loading trainS:  51%|█████▏    | 127/247 [00:29<00:24,  4.84it/s]Loading trainS:  52%|█████▏    | 128/247 [00:29<00:24,  4.78it/s]Loading trainS:  52%|█████▏    | 129/247 [00:29<00:24,  4.81it/s]Loading trainS:  53%|█████▎    | 130/247 [00:29<00:24,  4.85it/s]Loading trainS:  53%|█████▎    | 131/247 [00:30<00:23,  4.86it/s]Loading trainS:  53%|█████▎    | 132/247 [00:30<00:23,  4.87it/s]Loading trainS:  54%|█████▍    | 133/247 [00:30<00:23,  4.86it/s]Loading trainS:  54%|█████▍    | 134/247 [00:30<00:23,  4.84it/s]Loading trainS:  55%|█████▍    | 135/247 [00:30<00:23,  4.82it/s]Loading trainS:  55%|█████▌    | 136/247 [00:31<00:23,  4.66it/s]Loading trainS:  55%|█████▌    | 137/247 [00:31<00:23,  4.65it/s]Loading trainS:  56%|█████▌    | 138/247 [00:31<00:23,  4.65it/s]Loading trainS:  56%|█████▋    | 139/247 [00:31<00:23,  4.64it/s]Loading trainS:  57%|█████▋    | 140/247 [00:32<00:23,  4.65it/s]Loading trainS:  57%|█████▋    | 141/247 [00:32<00:22,  4.64it/s]Loading trainS:  57%|█████▋    | 142/247 [00:32<00:22,  4.61it/s]Loading trainS:  58%|█████▊    | 143/247 [00:32<00:22,  4.60it/s]Loading trainS:  58%|█████▊    | 144/247 [00:32<00:22,  4.62it/s]Loading trainS:  59%|█████▊    | 145/247 [00:33<00:22,  4.62it/s]Loading trainS:  59%|█████▉    | 146/247 [00:33<00:21,  4.61it/s]Loading trainS:  60%|█████▉    | 147/247 [00:33<00:21,  4.61it/s]Loading trainS:  60%|█████▉    | 148/247 [00:33<00:21,  4.62it/s]Loading trainS:  60%|██████    | 149/247 [00:33<00:21,  4.59it/s]Loading trainS:  61%|██████    | 150/247 [00:34<00:20,  4.63it/s]Loading trainS:  61%|██████    | 151/247 [00:34<00:20,  4.64it/s]Loading trainS:  62%|██████▏   | 152/247 [00:34<00:20,  4.67it/s]Loading trainS:  62%|██████▏   | 153/247 [00:34<00:20,  4.65it/s]Loading trainS:  62%|██████▏   | 154/247 [00:35<00:20,  4.47it/s]Loading trainS:  63%|██████▎   | 155/247 [00:35<00:21,  4.38it/s]Loading trainS:  63%|██████▎   | 156/247 [00:35<00:21,  4.31it/s]Loading trainS:  64%|██████▎   | 157/247 [00:35<00:21,  4.27it/s]Loading trainS:  64%|██████▍   | 158/247 [00:36<00:20,  4.24it/s]Loading trainS:  64%|██████▍   | 159/247 [00:36<00:20,  4.20it/s]Loading trainS:  65%|██████▍   | 160/247 [00:36<00:20,  4.20it/s]Loading trainS:  65%|██████▌   | 161/247 [00:36<00:20,  4.20it/s]Loading trainS:  66%|██████▌   | 162/247 [00:36<00:20,  4.21it/s]Loading trainS:  66%|██████▌   | 163/247 [00:37<00:20,  4.16it/s]Loading trainS:  66%|██████▋   | 164/247 [00:37<00:20,  4.12it/s]Loading trainS:  67%|██████▋   | 165/247 [00:37<00:19,  4.12it/s]Loading trainS:  67%|██████▋   | 166/247 [00:37<00:19,  4.10it/s]Loading trainS:  68%|██████▊   | 167/247 [00:38<00:19,  4.11it/s]Loading trainS:  68%|██████▊   | 168/247 [00:38<00:19,  4.11it/s]Loading trainS:  68%|██████▊   | 169/247 [00:38<00:19,  4.10it/s]Loading trainS:  69%|██████▉   | 170/247 [00:38<00:18,  4.13it/s]Loading trainS:  69%|██████▉   | 171/247 [00:39<00:18,  4.12it/s]Loading trainS:  70%|██████▉   | 172/247 [00:39<00:17,  4.19it/s]Loading trainS:  70%|███████   | 173/247 [00:39<00:17,  4.27it/s]Loading trainS:  70%|███████   | 174/247 [00:39<00:17,  4.17it/s]Loading trainS:  71%|███████   | 175/247 [00:40<00:18,  3.99it/s]Loading trainS:  71%|███████▏  | 176/247 [00:40<00:17,  4.08it/s]Loading trainS:  72%|███████▏  | 177/247 [00:40<00:16,  4.19it/s]Loading trainS:  72%|███████▏  | 178/247 [00:40<00:16,  4.27it/s]Loading trainS:  72%|███████▏  | 179/247 [00:41<00:15,  4.32it/s]Loading trainS:  73%|███████▎  | 180/247 [00:41<00:15,  4.38it/s]Loading trainS:  73%|███████▎  | 181/247 [00:41<00:14,  4.43it/s]Loading trainS:  74%|███████▎  | 182/247 [00:41<00:14,  4.45it/s]Loading trainS:  74%|███████▍  | 183/247 [00:41<00:14,  4.40it/s]Loading trainS:  74%|███████▍  | 184/247 [00:42<00:14,  4.36it/s]Loading trainS:  75%|███████▍  | 185/247 [00:42<00:14,  4.34it/s]Loading trainS:  75%|███████▌  | 186/247 [00:42<00:13,  4.44it/s]Loading trainS:  76%|███████▌  | 187/247 [00:42<00:13,  4.50it/s]Loading trainS:  76%|███████▌  | 188/247 [00:43<00:12,  4.55it/s]Loading trainS:  77%|███████▋  | 189/247 [00:43<00:12,  4.61it/s]Loading trainS:  77%|███████▋  | 190/247 [00:43<00:12,  4.64it/s]Loading trainS:  77%|███████▋  | 191/247 [00:43<00:12,  4.63it/s]Loading trainS:  78%|███████▊  | 192/247 [00:43<00:12,  4.58it/s]Loading trainS:  78%|███████▊  | 193/247 [00:44<00:11,  4.54it/s]Loading trainS:  79%|███████▊  | 194/247 [00:44<00:11,  4.55it/s]Loading trainS:  79%|███████▉  | 195/247 [00:44<00:11,  4.57it/s]Loading trainS:  79%|███████▉  | 196/247 [00:44<00:11,  4.55it/s]Loading trainS:  80%|███████▉  | 197/247 [00:45<00:10,  4.63it/s]Loading trainS:  80%|████████  | 198/247 [00:45<00:10,  4.70it/s]Loading trainS:  81%|████████  | 199/247 [00:45<00:10,  4.74it/s]Loading trainS:  81%|████████  | 200/247 [00:45<00:09,  4.76it/s]Loading trainS:  81%|████████▏ | 201/247 [00:45<00:09,  4.80it/s]Loading trainS:  82%|████████▏ | 202/247 [00:46<00:09,  4.81it/s]Loading trainS:  82%|████████▏ | 203/247 [00:46<00:09,  4.79it/s]Loading trainS:  83%|████████▎ | 204/247 [00:46<00:08,  4.78it/s]Loading trainS:  83%|████████▎ | 205/247 [00:46<00:08,  4.75it/s]Loading trainS:  83%|████████▎ | 206/247 [00:46<00:08,  4.76it/s]Loading trainS:  84%|████████▍ | 207/247 [00:47<00:08,  4.77it/s]Loading trainS:  84%|████████▍ | 208/247 [00:47<00:08,  4.77it/s]Loading trainS:  85%|████████▍ | 209/247 [00:47<00:08,  4.75it/s]Loading trainS:  85%|████████▌ | 210/247 [00:47<00:07,  4.75it/s]Loading trainS:  85%|████████▌ | 211/247 [00:47<00:07,  4.69it/s]Loading trainS:  86%|████████▌ | 212/247 [00:48<00:07,  4.65it/s]Loading trainS:  86%|████████▌ | 213/247 [00:48<00:07,  4.64it/s]Loading trainS:  87%|████████▋ | 214/247 [00:48<00:07,  4.64it/s]Loading trainS:  87%|████████▋ | 215/247 [00:48<00:06,  4.61it/s]Loading trainS:  87%|████████▋ | 216/247 [00:49<00:06,  4.61it/s]Loading trainS:  88%|████████▊ | 217/247 [00:49<00:06,  4.58it/s]Loading trainS:  88%|████████▊ | 218/247 [00:49<00:06,  4.57it/s]Loading trainS:  89%|████████▊ | 219/247 [00:49<00:06,  4.56it/s]Loading trainS:  89%|████████▉ | 220/247 [00:49<00:05,  4.55it/s]Loading trainS:  89%|████████▉ | 221/247 [00:50<00:05,  4.54it/s]Loading trainS:  90%|████████▉ | 222/247 [00:50<00:05,  4.49it/s]Loading trainS:  90%|█████████ | 223/247 [00:50<00:05,  4.47it/s]Loading trainS:  91%|█████████ | 224/247 [00:50<00:05,  4.48it/s]Loading trainS:  91%|█████████ | 225/247 [00:51<00:04,  4.42it/s]Loading trainS:  91%|█████████▏| 226/247 [00:51<00:04,  4.45it/s]Loading trainS:  92%|█████████▏| 227/247 [00:51<00:04,  4.49it/s]Loading trainS:  92%|█████████▏| 228/247 [00:51<00:04,  4.54it/s]Loading trainS:  93%|█████████▎| 229/247 [00:51<00:03,  4.57it/s]Loading trainS:  93%|█████████▎| 230/247 [00:52<00:03,  4.45it/s]Loading trainS:  94%|█████████▎| 231/247 [00:52<00:03,  4.36it/s]Loading trainS:  94%|█████████▍| 232/247 [00:52<00:03,  4.31it/s]Loading trainS:  94%|█████████▍| 233/247 [00:52<00:03,  4.25it/s]Loading trainS:  95%|█████████▍| 234/247 [00:53<00:03,  4.19it/s]Loading trainS:  95%|█████████▌| 235/247 [00:53<00:02,  4.18it/s]Loading trainS:  96%|█████████▌| 236/247 [00:53<00:02,  4.17it/s]Loading trainS:  96%|█████████▌| 237/247 [00:53<00:02,  4.17it/s]Loading trainS:  96%|█████████▋| 238/247 [00:54<00:02,  4.17it/s]Loading trainS:  97%|█████████▋| 239/247 [00:54<00:01,  4.14it/s]Loading trainS:  97%|█████████▋| 240/247 [00:54<00:01,  4.13it/s]Loading trainS:  98%|█████████▊| 241/247 [00:54<00:01,  4.17it/s]Loading trainS:  98%|█████████▊| 242/247 [00:55<00:01,  4.19it/s]Loading trainS:  98%|█████████▊| 243/247 [00:55<00:00,  4.19it/s]Loading trainS:  99%|█████████▉| 244/247 [00:55<00:00,  4.21it/s]Loading trainS:  99%|█████████▉| 245/247 [00:55<00:00,  4.18it/s]Loading trainS: 100%|█████████▉| 246/247 [00:56<00:00,  4.17it/s]Loading trainS: 100%|██████████| 247/247 [00:56<00:00,  4.20it/s]Loading trainS: 100%|██████████| 247/247 [00:56<00:00,  4.39it/s]
Loading testS:   0%|          | 0/5 [00:00<?, ?it/s]Loading testS:  20%|██        | 1/5 [00:00<00:00,  4.35it/s]Loading testS:  40%|████      | 2/5 [00:00<00:00,  4.09it/s]Loading testS:  60%|██████    | 3/5 [00:00<00:00,  4.02it/s]Loading testS:  80%|████████  | 4/5 [00:00<00:00,  4.25it/s]Loading testS: 100%|██████████| 5/5 [00:01<00:00,  4.20it/s]Loading testS: 100%|██████████| 5/5 [00:01<00:00,  4.12it/s]----------+++ 
CrossVal ['c']
CrossVal ['c']
(0/5) test vimp2_ANON972_CSFn2
(1/5) test vimp2_H_CSFn2
(2/5) test vimp2_I_CSFn2
(3/5) test vimp2_K_CSFn2
(4/5) test vimp2_ANON765_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97331515 0.02668485]
Train on 16036 samples, validate on 318 samples
Epoch 1/300
 - 76s - loss: 0.1718 - acc: 0.9806 - mDice: 0.6668 - val_loss: 0.2311 - val_acc: 0.9900 - val_mDice: 0.4223

Epoch 00001: val_mDice improved from -inf to 0.42235, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 72s - loss: 0.1061 - acc: 0.9890 - mDice: 0.7936 - val_loss: 0.2208 - val_acc: 0.9912 - val_mDice: 0.4360

Epoch 00002: val_mDice improved from 0.42235 to 0.43599, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 72s - loss: 0.0896 - acc: 0.9906 - mDice: 0.8257 - val_loss: 0.2202 - val_acc: 0.9921 - val_mDice: 0.4501

Epoch 00003: val_mDice improved from 0.43599 to 0.45012, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300
 - 74s - loss: 0.0820 - acc: 0.9916 - mDice: 0.8404 - val_loss: 0.1542 - val_acc: 0.9923 - val_mDice: 0.4493

Epoch 00004: val_mDice did not improve from 0.45012
Epoch 5/300
 - 74s - loss: 0.0762 - acc: 0.9921 - mDice: 0.8517 - val_loss: 0.1433 - val_acc: 0.9927 - val_mDice: 0.4579

Epoch 00005: val_mDice improved from 0.45012 to 0.45786, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300
 - 74s - loss: 0.0704 - acc: 0.9926 - mDice: 0.8629 - val_loss: 0.1189 - val_acc: 0.9930 - val_mDice: 0.4607

Epoch 00006: val_mDice improved from 0.45786 to 0.46074, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/300
 - 74s - loss: 0.0679 - acc: 0.9930 - mDice: 0.8679 - val_loss: 0.0906 - val_acc: 0.9932 - val_mDice: 0.4613

Epoch 00007: val_mDice improved from 0.46074 to 0.46133, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/300
 - 75s - loss: 0.0646 - acc: 0.9932 - mDice: 0.8744 - val_loss: 0.1653 - val_acc: 0.9932 - val_mDice: 0.4663

Epoch 00008: val_mDice improved from 0.46133 to 0.46629, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 9/300
 - 75s - loss: 0.0628 - acc: 0.9934 - mDice: 0.8778 - val_loss: 0.0853 - val_acc: 0.9932 - val_mDice: 0.4710

Epoch 00009: val_mDice improved from 0.46629 to 0.47100, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 10/300
 - 75s - loss: 0.0601 - acc: 0.9937 - mDice: 0.8830 - val_loss: 0.1154 - val_acc: 0.9933 - val_mDice: 0.4731

Epoch 00010: val_mDice improved from 0.47100 to 0.47313, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 11/300
 - 75s - loss: 0.0584 - acc: 0.9938 - mDice: 0.8864 - val_loss: 0.0841 - val_acc: 0.9936 - val_mDice: 0.4763

Epoch 00011: val_mDice improved from 0.47313 to 0.47629, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 12/300
 - 75s - loss: 0.0570 - acc: 0.9939 - mDice: 0.8891 - val_loss: 0.0683 - val_acc: 0.9936 - val_mDice: 0.4770

Epoch 00012: val_mDice improved from 0.47629 to 0.47696, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 13/300
 - 75s - loss: 0.0544 - acc: 0.9941 - mDice: 0.8941 - val_loss: 0.0793 - val_acc: 0.9936 - val_mDice: 0.4788

Epoch 00013: val_mDice improved from 0.47696 to 0.47884, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 14/300
 - 75s - loss: 0.0530 - acc: 0.9942 - mDice: 0.8970 - val_loss: 0.0183 - val_acc: 0.9934 - val_mDice: 0.4797

Epoch 00014: val_mDice improved from 0.47884 to 0.47967, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 15/300
 - 74s - loss: 0.0530 - acc: 0.9943 - mDice: 0.8968 - val_loss: -4.1389e-03 - val_acc: 0.9936 - val_mDice: 0.4794

Epoch 00015: val_mDice did not improve from 0.47967
Epoch 16/300
 - 75s - loss: 0.0518 - acc: 0.9944 - mDice: 0.8993 - val_loss: 0.0425 - val_acc: 0.9936 - val_mDice: 0.4802

Epoch 00016: val_mDice improved from 0.47967 to 0.48020, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 17/300
 - 75s - loss: 0.0511 - acc: 0.9945 - mDice: 0.9007 - val_loss: 0.0094 - val_acc: 0.9938 - val_mDice: 0.4892

Epoch 00017: val_mDice improved from 0.48020 to 0.48918, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 18/300
 - 75s - loss: 0.0499 - acc: 0.9946 - mDice: 0.9030 - val_loss: 0.0427 - val_acc: 0.9936 - val_mDice: 0.4869

Epoch 00018: val_mDice did not improve from 0.48918
Epoch 19/300
 - 74s - loss: 0.0504 - acc: 0.9946 - mDice: 0.9020 - val_loss: 0.0210 - val_acc: 0.9937 - val_mDice: 0.4874

Epoch 00019: val_mDice did not improve from 0.48918
Epoch 20/300
 - 75s - loss: 0.0491 - acc: 0.9947 - mDice: 0.9045 - val_loss: 0.0374 - val_acc: 0.9937 - val_mDice: 0.4951

Epoch 00020: val_mDice improved from 0.48918 to 0.49514, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 21/300
 - 75s - loss: 0.0476 - acc: 0.9948 - mDice: 0.9074 - val_loss: 0.0745 - val_acc: 0.9935 - val_mDice: 0.4884

Epoch 00021: val_mDice did not improve from 0.49514
Epoch 22/300
 - 74s - loss: 0.0472 - acc: 0.9949 - mDice: 0.9083 - val_loss: 0.1034 - val_acc: 0.9936 - val_mDice: 0.4899

Epoch 00022: val_mDice did not improve from 0.49514
Epoch 23/300
 - 74s - loss: 0.0473 - acc: 0.9949 - mDice: 0.9080 - val_loss: 0.0497 - val_acc: 0.9935 - val_mDice: 0.4941

Epoch 00023: val_mDice did not improve from 0.49514
Epoch 24/300
 - 75s - loss: 0.0458 - acc: 0.9950 - mDice: 0.9110 - val_loss: 0.0740 - val_acc: 0.9935 - val_mDice: 0.4884

Epoch 00024: val_mDice did not improve from 0.49514
Epoch 25/300
 - 74s - loss: 0.0463 - acc: 0.9950 - mDice: 0.9100 - val_loss: 0.0415 - val_acc: 0.9936 - val_mDice: 0.4928

Epoch 00025: val_mDice did not improve from 0.49514
Epoch 26/300
 - 75s - loss: 0.0446 - acc: 0.9950 - mDice: 0.9132 - val_loss: 0.0709 - val_acc: 0.9937 - val_mDice: 0.4969

Epoch 00026: val_mDice improved from 0.49514 to 0.49686, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 27/300
 - 74s - loss: 0.0438 - acc: 0.9951 - mDice: 0.9148 - val_loss: 0.0694 - val_acc: 0.9938 - val_mDice: 0.4998

Epoch 00027: val_mDice improved from 0.49686 to 0.49982, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 28/300
 - 75s - loss: 0.0444 - acc: 0.9951 - mDice: 0.9137 - val_loss: 0.0717 - val_acc: 0.9939 - val_mDice: 0.4951

Epoch 00028: val_mDice did not improve from 0.49982
Epoch 29/300
 - 74s - loss: 0.0430 - acc: 0.9952 - mDice: 0.9165 - val_loss: 0.0391 - val_acc: 0.9940 - val_mDice: 0.4974

Epoch 00029: val_mDice did not improve from 0.49982
Epoch 30/300
 - 75s - loss: 0.0430 - acc: 0.9952 - mDice: 0.9165 - val_loss: 0.0671 - val_acc: 0.9938 - val_mDice: 0.4979

Epoch 00030: val_mDice did not improve from 0.49982
Epoch 31/300
 - 75s - loss: 0.0426 - acc: 0.9953 - mDice: 0.9171 - val_loss: 0.0471 - val_acc: 0.9936 - val_mDice: 0.4896

Epoch 00031: val_mDice did not improve from 0.49982
Epoch 32/300
 - 75s - loss: 0.0420 - acc: 0.9953 - mDice: 0.9184 - val_loss: 0.0989 - val_acc: 0.9938 - val_mDice: 0.5038

Epoch 00032: val_mDice improved from 0.49982 to 0.50382, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 33/300
 - 75s - loss: 0.0422 - acc: 0.9953 - mDice: 0.9180 - val_loss: 0.0406 - val_acc: 0.9938 - val_mDice: 0.4956

Epoch 00033: val_mDice did not improve from 0.50382
Epoch 34/300
 - 74s - loss: 0.0413 - acc: 0.9954 - mDice: 0.9197 - val_loss: 0.0469 - val_acc: 0.9935 - val_mDice: 0.4836

Epoch 00034: val_mDice did not improve from 0.50382
Epoch 35/300
 - 75s - loss: 0.0409 - acc: 0.9954 - mDice: 0.9204 - val_loss: 0.0401 - val_acc: 0.9938 - val_mDice: 0.4952

Epoch 00035: val_mDice did not improve from 0.50382
Epoch 36/300
 - 75s - loss: 0.0416 - acc: 0.9954 - mDice: 0.9191 - val_loss: 0.0429 - val_acc: 0.9940 - val_mDice: 0.4924

Epoch 00036: val_mDice did not improve from 0.50382
Epoch 37/300
 - 74s - loss: 0.0404 - acc: 0.9955 - mDice: 0.9214 - val_loss: 0.0433 - val_acc: 0.9939 - val_mDice: 0.4933

Epoch 00037: val_mDice did not improve from 0.50382
Epoch 38/300
 - 75s - loss: 0.0406 - acc: 0.9955 - mDice: 0.9211 - val_loss: 0.0433 - val_acc: 0.9938 - val_mDice: 0.4998

Epoch 00038: val_mDice did not improve from 0.50382
Epoch 39/300
 - 75s - loss: 0.0404 - acc: 0.9955 - mDice: 0.9215 - val_loss: 0.0442 - val_acc: 0.9938 - val_mDice: 0.5056

Epoch 00039: val_mDice improved from 0.50382 to 0.50560, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 40/300
 - 75s - loss: 0.0402 - acc: 0.9955 - mDice: 0.9219 - val_loss: 0.0117 - val_acc: 0.9939 - val_mDice: 0.4958

Epoch 00040: val_mDice did not improve from 0.50560
Epoch 41/300
 - 74s - loss: 0.0397 - acc: 0.9956 - mDice: 0.9228 - val_loss: 0.0114 - val_acc: 0.9937 - val_mDice: 0.4936

Epoch 00041: val_mDice did not improve from 0.50560
Epoch 42/300
 - 74s - loss: 0.0398 - acc: 0.9956 - mDice: 0.9225 - val_loss: 0.0306 - val_acc: 0.9936 - val_mDice: 0.4931

Epoch 00042: val_mDice did not improve from 0.50560
Epoch 43/300
 - 75s - loss: 0.0386 - acc: 0.9956 - mDice: 0.9251 - val_loss: 0.0135 - val_acc: 0.9939 - val_mDice: 0.4935

Epoch 00043: val_mDice did not improve from 0.50560
Epoch 44/300
 - 74s - loss: 0.0384 - acc: 0.9956 - mDice: 0.9253 - val_loss: 0.0186 - val_acc: 0.9939 - val_mDice: 0.4941

Epoch 00044: val_mDice did not improve from 0.50560
Epoch 45/300
 - 74s - loss: 0.0383 - acc: 0.9957 - mDice: 0.9257 - val_loss: 0.0102 - val_acc: 0.9939 - val_mDice: 0.5030

Epoch 00045: val_mDice did not improve from 0.50560
Epoch 46/300
 - 74s - loss: 0.0380 - acc: 0.9957 - mDice: 0.9262 - val_loss: 0.0117 - val_acc: 0.9939 - val_mDice: 0.4919

Epoch 00046: val_mDice did not improve from 0.50560
Epoch 47/300
 - 75s - loss: 0.0373 - acc: 0.9957 - mDice: 0.9275 - val_loss: 0.0207 - val_acc: 0.9937 - val_mDice: 0.4895

Epoch 00047: val_mDice did not improve from 0.50560
Epoch 48/300
 - 75s - loss: 0.0369 - acc: 0.9957 - mDice: 0.9284 - val_loss: 0.0258 - val_acc: 0.9939 - val_mDice: 0.4948

Epoch 00048: val_mDice did not improve from 0.50560
Epoch 49/300
 - 74s - loss: 0.0372 - acc: 0.9957 - mDice: 0.9277 - val_loss: 0.0369 - val_acc: 0.9937 - val_mDice: 0.5003

Epoch 00049: val_mDice did not improve from 0.50560
Epoch 50/300
 - 74s - loss: 0.0373 - acc: 0.9958 - mDice: 0.9276 - val_loss: 0.0227 - val_acc: 0.9936 - val_mDice: 0.4850

Epoch 00050: val_mDice did not improve from 0.50560
Epoch 51/300
 - 75s - loss: 0.0369 - acc: 0.9957 - mDice: 0.9284 - val_loss: 0.0172 - val_acc: 0.9938 - val_mDice: 0.4933

Epoch 00051: val_mDice did not improve from 0.50560
Epoch 52/300
 - 75s - loss: 0.0371 - acc: 0.9958 - mDice: 0.9279 - val_loss: 0.0270 - val_acc: 0.9938 - val_mDice: 0.4911

Epoch 00052: val_mDice did not improve from 0.50560
Epoch 53/300
 - 74s - loss: 0.0370 - acc: 0.9958 - mDice: 0.9281 - val_loss: 0.0181 - val_acc: 0.9937 - val_mDice: 0.4905

Epoch 00053: val_mDice did not improve from 0.50560
Epoch 54/300
 - 74s - loss: 0.0362 - acc: 0.9958 - mDice: 0.9296 - val_loss: 0.0243 - val_acc: 0.9939 - val_mDice: 0.4999

Epoch 00054: val_mDice did not improve from 0.50560

Epoch 00054: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 55/300
 - 75s - loss: 0.0358 - acc: 0.9959 - mDice: 0.9304 - val_loss: 0.0317 - val_acc: 0.9938 - val_mDice: 0.4920

Epoch 00055: val_mDice did not improve from 0.50560
Epoch 56/300
 - 75s - loss: 0.0351 - acc: 0.9959 - mDice: 0.9318 - val_loss: 0.0398 - val_acc: 0.9939 - val_mDice: 0.5020

Epoch 00056: val_mDice did not improve from 0.50560
Epoch 57/300
 - 75s - loss: 0.0354 - acc: 0.9959 - mDice: 0.9313 - val_loss: 0.0298 - val_acc: 0.9939 - val_mDice: 0.4972

Epoch 00057: val_mDice did not improve from 0.50560
Epoch 58/300
 - 75s - loss: 0.0356 - acc: 0.9959 - mDice: 0.9308 - val_loss: 0.0391 - val_acc: 0.9939 - val_mDice: 0.4970

Epoch 00058: val_mDice did not improve from 0.50560
Epoch 59/300
 - 74s - loss: 0.0344 - acc: 0.9959 - mDice: 0.9332 - val_loss: 0.0300 - val_acc: 0.9940 - val_mDice: 0.4989

Epoch 00059: val_mDice did not improve from 0.50560
Epoch 60/300
 - 74s - loss: 0.0353 - acc: 0.9959 - mDice: 0.9313 - val_loss: 0.0374 - val_acc: 0.9940 - val_mDice: 0.5007

Epoch 00060: val_mDice did not improve from 0.50560
Epoch 61/300
 - 74s - loss: 0.0355 - acc: 0.9959 - mDice: 0.9310 - val_loss: 0.0462 - val_acc: 0.9938 - val_mDice: 0.4986

Epoch 00061: val_mDice did not improve from 0.50560
Epoch 62/300
 - 74s - loss: 0.0351 - acc: 0.9959 - mDice: 0.9318 - val_loss: 0.0566 - val_acc: 0.9938 - val_mDice: 0.5004

Epoch 00062: val_mDice did not improve from 0.50560
Epoch 63/300
 - 74s - loss: 0.0351 - acc: 0.9959 - mDice: 0.9319 - val_loss: 0.0465 - val_acc: 0.9939 - val_mDice: 0.5010

Epoch 00063: val_mDice did not improve from 0.50560
Epoch 64/300
 - 75s - loss: 0.0339 - acc: 0.9960 - mDice: 0.9341 - val_loss: 0.0578 - val_acc: 0.9937 - val_mDice: 0.4971

Epoch 00064: val_mDice did not improve from 0.50560
Epoch 65/300
 - 75s - loss: 0.0340 - acc: 0.9959 - mDice: 0.9341 - val_loss: 0.0433 - val_acc: 0.9939 - val_mDice: 0.5019

Epoch 00065: val_mDice did not improve from 0.50560
Epoch 66/300
 - 75s - loss: 0.0341 - acc: 0.9959 - mDice: 0.9339 - val_loss: 0.0473 - val_acc: 0.9940 - val_mDice: 0.5037

Epoch 00066: val_mDice did not improve from 0.50560
Epoch 67/300
 - 74s - loss: 0.0333 - acc: 0.9960 - mDice: 0.9354 - val_loss: 0.0509 - val_acc: 0.9939 - val_mDice: 0.5023

Epoch 00067: val_mDice did not improve from 0.50560
Epoch 68/300
 - 74s - loss: 0.0343 - acc: 0.9960 - mDice: 0.9334 - val_loss: 0.0386 - val_acc: 0.9938 - val_mDice: 0.4966

Epoch 00068: val_mDice did not improve from 0.50560
Epoch 69/300
 - 74s - loss: 0.0339 - acc: 0.9960 - mDice: 0.9342 - val_loss: 0.0424 - val_acc: 0.9938 - val_mDice: 0.4960

Epoch 00069: val_mDice did not improve from 0.50560

Epoch 00069: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 70/300
 - 75s - loss: 0.0343 - acc: 0.9960 - mDice: 0.9334 - val_loss: 0.0453 - val_acc: 0.9940 - val_mDice: 0.5053

Epoch 00070: val_mDice did not improve from 0.50560
Epoch 71/300
 - 75s - loss: 0.0337 - acc: 0.9960 - mDice: 0.9346 - val_loss: 0.0461 - val_acc: 0.9938 - val_mDice: 0.4947

Epoch 00071: val_mDice did not improve from 0.50560
Epoch 72/300
 - 75s - loss: 0.0338 - acc: 0.9960 - mDice: 0.9343 - val_loss: 0.0588 - val_acc: 0.9939 - val_mDice: 0.5008

Epoch 00072: val_mDice did not improve from 0.50560
Epoch 73/300
 - 75s - loss: 0.0331 - acc: 0.9960 - mDice: 0.9358 - val_loss: 0.0542 - val_acc: 0.9939 - val_mDice: 0.4950

Epoch 00073: val_mDice did not improve from 0.50560
Epoch 74/300
 - 74s - loss: 0.0331 - acc: 0.9960 - mDice: 0.9357 - val_loss: 0.0487 - val_acc: 0.9939 - val_mDice: 0.4969

Epoch 00074: val_mDice did not improve from 0.50560
Epoch 75/300
 - 74s - loss: 0.0335 - acc: 0.9960 - mDice: 0.9349 - val_loss: 0.0489 - val_acc: 0.9938 - val_mDice: 0.4956

Epoch 00075: val_mDice did not improve from 0.50560
Epoch 76/300
 - 74s - loss: 0.0332 - acc: 0.9960 - mDice: 0.9356 - val_loss: 0.0505 - val_acc: 0.9939 - val_mDice: 0.4980

Epoch 00076: val_mDice did not improve from 0.50560
Epoch 77/300
 - 74s - loss: 0.0329 - acc: 0.9960 - mDice: 0.9361 - val_loss: 0.0435 - val_acc: 0.9939 - val_mDice: 0.4942

Epoch 00077: val_mDice did not improve from 0.50560
Epoch 78/300
 - 74s - loss: 0.0334 - acc: 0.9961 - mDice: 0.9352 - val_loss: 0.0608 - val_acc: 0.9938 - val_mDice: 0.5003

Epoch 00078: val_mDice did not improve from 0.50560
Epoch 79/300
 - 75s - loss: 0.0337 - acc: 0.9960 - mDice: 0.9345 - val_loss: 0.0528 - val_acc: 0.9938 - val_mDice: 0.4967

Epoch 00079: val_mDice did not improve from 0.50560
Restoring model weights from the end of the best epoch
Epoch 00079: early stopping
{'val_loss': [0.2311256653014219, 0.22082820872090897, 0.22018815741036674, 0.15417772803291585, 0.14328359559459505, 0.11886886586933015, 0.09056862720153616, 0.1653071435742408, 0.0852810763431795, 0.11544106610166202, 0.08405140196939684, 0.06833385498081364, 0.07926088416913771, 0.018256163119145158, -0.004138948688717009, 0.04252658292370023, 0.009368369086358533, 0.04273649094239721, 0.021009203398002767, 0.03737315690180041, 0.07445978312372412, 0.1034232700395884, 0.049731374396093236, 0.07401868813442734, 0.041520811289361435, 0.07087771908479666, 0.06937767405929805, 0.0717393917107732, 0.039097964201333386, 0.0671053613131901, 0.04712047629386374, 0.09886912828159032, 0.04058340941585085, 0.04693583698954972, 0.040059726568138075, 0.042878991543496926, 0.043275952714044344, 0.04329025576699455, 0.04416900315562134, 0.011666640592446117, 0.011396353302886651, 0.030638721753966133, 0.013545499798261895, 0.01857489801990161, 0.01024538203605316, 0.01168316790142899, 0.02074019287554723, 0.02579082764169705, 0.03694776693979899, 0.022709816459964657, 0.017221220071960544, 0.026968153205307776, 0.018124681036427337, 0.024336976590771345, 0.03165746545829113, 0.03980870830750315, 0.029765646834418458, 0.03906447655937207, 0.030002593103819673, 0.037403740269957846, 0.04616765064076058, 0.05661155700496158, 0.046495553832384025, 0.05781895315872049, 0.04332884269877799, 0.047286970294871424, 0.05091234291874388, 0.0386403499628013, 0.04238379652957496, 0.04525878580298814, 0.04614972918288513, 0.05875276586732025, 0.054176708109348826, 0.048698462358435744, 0.04891058305899302, 0.050460500021775566, 0.043531304635341814, 0.06081594222184247, 0.05281824121872584], 'val_acc': [0.9900424274258643, 0.991166205526148, 0.9920856502820861, 0.9923158273007135, 0.9926639765313586, 0.9929900420536785, 0.9932139437153654, 0.9931878376306977, 0.9931654952607065, 0.9932714247103757, 0.9935608371248785, 0.9935656054964606, 0.9935708799452152, 0.9933999364481032, 0.9935974883583357, 0.9935824241278306, 0.9937752020434014, 0.9936449302817291, 0.9936825786746523, 0.9937445693795786, 0.9935013486904168, 0.993555319009337, 0.9935495422321295, 0.9935304668714415, 0.9936351386256188, 0.993706926609735, 0.9938138513445104, 0.9938577807174539, 0.9939544227138255, 0.9938025583261214, 0.9936210828007392, 0.9938058234610647, 0.9937649118075581, 0.9935379943007943, 0.9938429751486149, 0.9939539203853727, 0.993870586344281, 0.9938346867291432, 0.9938254048989253, 0.9938848914590271, 0.9936547125660399, 0.9935681190130845, 0.9938542756644435, 0.9938510086551402, 0.9939137640989052, 0.9938914179801941, 0.9937202345650151, 0.9938864021931054, 0.9937380466071315, 0.9935959813729772, 0.9938173657693203, 0.9937646625176916, 0.9936587368167421, 0.9939177714803684, 0.9937706829616858, 0.9939260580254801, 0.9939022199162897, 0.9938655630597528, 0.993986299952621, 0.9940309790695239, 0.9938397100136714, 0.993763655986426, 0.9938766105369952, 0.9937061731170558, 0.9938500077469544, 0.9939617083507514, 0.9939268096437994, 0.9938098383399675, 0.9938289174493754, 0.9939709845578896, 0.9938078309005162, 0.9938984505785337, 0.993880124961805, 0.9939192840888066, 0.9938206346529834, 0.9938783668122202, 0.9938999556895322, 0.9937952783122752, 0.9937915202206785], 'val_mDice': [0.4223469506311213, 0.43598959720313213, 0.45011925698498134, 0.4492999531151171, 0.4578560543885022, 0.46073707224766036, 0.4613319048716587, 0.46628973862660006, 0.4709975229306791, 0.4731271413258136, 0.4762888991467233, 0.4769609341655606, 0.47883803446619017, 0.4796729462935865, 0.4794134485459178, 0.4802026830426177, 0.48918235320715037, 0.4869422236890916, 0.4874325604558741, 0.4951352059091435, 0.4883589410289784, 0.48993971585383955, 0.4940680037223318, 0.48841876335114054, 0.49275940553572195, 0.4968636338862608, 0.4998188862963667, 0.4950781522681878, 0.4973965935971377, 0.4979245734852065, 0.4895561452041257, 0.5038185281761037, 0.49561633160279234, 0.4835970416290205, 0.4951856608660716, 0.4923702102833949, 0.4933246965490797, 0.4998098161516699, 0.5056013241878845, 0.49582773760992027, 0.4936314805311227, 0.4930548740070571, 0.49350052369090747, 0.49410569053963294, 0.5029835261451373, 0.4919281906489306, 0.4894979916653543, 0.4947703924475226, 0.5002784117493989, 0.4849719523258929, 0.4932636706709112, 0.49111312477843566, 0.49051319348549693, 0.49986615589579697, 0.49202661235002604, 0.5020061252439547, 0.49722830876239443, 0.496969499256251, 0.49891493394502306, 0.5006549055190206, 0.4985926778249021, 0.5003641801154088, 0.5009898665191242, 0.4970649376792728, 0.5019441888954654, 0.5037060972748313, 0.5023297276230728, 0.49657345005551223, 0.4960055836716538, 0.50528911697977, 0.4946979191505684, 0.5008209974698301, 0.4950330296308739, 0.4968739122902072, 0.49561910565544226, 0.4979982951058532, 0.4942178065484425, 0.500275102098408, 0.49666469412967096], 'loss': [0.17180960336976528, 0.10608786924647613, 0.08961241064794374, 0.08198366975321172, 0.07618106527444013, 0.07044157190222816, 0.06787842059130234, 0.06455661080746052, 0.06279999516527086, 0.06012636652538294, 0.058404786645600434, 0.05701097065848167, 0.05442714920217749, 0.05298046118178609, 0.05301968988805199, 0.05177241992358705, 0.051065504458010655, 0.049860241156871495, 0.05037689820790594, 0.04907877044833548, 0.04759947170500418, 0.04716079570603924, 0.047284165932059315, 0.04575185008657398, 0.0462587921900861, 0.04464926841246217, 0.04383663269190546, 0.04438374004747006, 0.04297824632808525, 0.04297475651534723, 0.042632919368927664, 0.04196726755757348, 0.042178981279339625, 0.04128720029263046, 0.04093523329947619, 0.0416011774118448, 0.040447884120082406, 0.040551775886616105, 0.04036417568553621, 0.0401793158057697, 0.039700319733583116, 0.03984719948798054, 0.038553029926943046, 0.03842102906841624, 0.03825118762540175, 0.03799028388354867, 0.03733211446411296, 0.03688043184405582, 0.0372058428511726, 0.037262135446183134, 0.03686176830599301, 0.037112604161358795, 0.03697956838856674, 0.03623806680470192, 0.035828183527992205, 0.03513636841280974, 0.03539037783080784, 0.035602929258761365, 0.034424817926919676, 0.0353407119027945, 0.03549836922867187, 0.035107621665227205, 0.03505927125718208, 0.033923357094195035, 0.03396239342438845, 0.03405218680980914, 0.03329681238146993, 0.034286828203803736, 0.033900786233969714, 0.034257631190446405, 0.0336750415533625, 0.03384630249924242, 0.03308111777955173, 0.03312384963303202, 0.033524118186374, 0.03318219996633807, 0.03289388742202177, 0.0333596283407781, 0.03370675701442637], 'acc': [0.9805859613147987, 0.9890131417073702, 0.9906164643011595, 0.9915519392633116, 0.9921478705086236, 0.9926014763959836, 0.9929926436599813, 0.9932250045918265, 0.993442435853646, 0.9936792622423731, 0.9938208554920397, 0.9939494028921644, 0.9941195081626724, 0.9942243768779915, 0.994344286897528, 0.9943932125799732, 0.994511869593611, 0.9946398537826229, 0.9946400043627781, 0.9947295657533991, 0.9948056147829438, 0.9948672867454653, 0.9948793271141595, 0.9949850730466022, 0.995007570833897, 0.9950434704111175, 0.9950963310482913, 0.9951359446163516, 0.9951577265829361, 0.9952088863170483, 0.9952686573463534, 0.995274108889157, 0.9953182895720585, 0.995366627370428, 0.9954050001984495, 0.9954328350248881, 0.9954792559964605, 0.9954871451075638, 0.9954859556239511, 0.9955381709207886, 0.9955599930599224, 0.9955582210297417, 0.995596997226138, 0.9956039752366092, 0.9956502426396168, 0.9956693859670073, 0.9956770716729858, 0.9957126814711269, 0.995727669333365, 0.9957644186967862, 0.9957452561008452, 0.9957903075259831, 0.9957966297961554, 0.9958130558713412, 0.9958596822105759, 0.9958757787058258, 0.9958698596850867, 0.99589155755991, 0.9959120554991213, 0.9959050420960692, 0.9959182526203545, 0.9959411698777528, 0.9959355943308236, 0.9959604777163418, 0.9959385162965114, 0.9959377440677137, 0.9959612895155464, 0.9959606569242704, 0.995990458689119, 0.9960111850558091, 0.9959873511818585, 0.9960180293163714, 0.9960110694742405, 0.9960136286157953, 0.9960334643175193, 0.9960317721641043, 0.996021209430088, 0.9960518771497708, 0.996048502239333], 'mDice': [0.6667568015382647, 0.7935884122270951, 0.8256751563363874, 0.8404261381714371, 0.8517099705042239, 0.86294095778973, 0.8678594374808789, 0.8743710882412087, 0.8777648901634697, 0.8829878110315306, 0.8863566893834252, 0.8890735755004797, 0.894145665408281, 0.8969817824622704, 0.8968409117646692, 0.8993059073445485, 0.9006621439756078, 0.9030064729293941, 0.9019654915015517, 0.904513848867908, 0.907430542328614, 0.9082745882111257, 0.9080247336282609, 0.9110288370775148, 0.9100027695859451, 0.9132006876217097, 0.914795907341124, 0.9136823185416939, 0.9164820024481168, 0.9164570991677458, 0.9171122811394578, 0.9184336047108503, 0.9179861689296319, 0.919747272307036, 0.9204299939811661, 0.9190817132246527, 0.9213639688094435, 0.9211465644253611, 0.9215255184723986, 0.9218651710357354, 0.9228119844044346, 0.9225140571088736, 0.9250794450640233, 0.9253497302725476, 0.9256623270848232, 0.9261682518411973, 0.9274826117750117, 0.928367353382923, 0.9277061208830568, 0.9275758361416465, 0.9283803091899032, 0.9278541207146276, 0.9281231353343291, 0.9295965306596525, 0.9303863376609462, 0.9317599147201387, 0.9312511703771199, 0.9308118091536338, 0.933163574090412, 0.9313317410170244, 0.9310163373838907, 0.9317832256075632, 0.9318800474430862, 0.934136814333965, 0.9340651047676913, 0.9338836990544016, 0.9353828237267259, 0.9334054710203449, 0.9341677723159276, 0.9334404791277167, 0.93461398791125, 0.9342617929858976, 0.9357853207572734, 0.9357061273113811, 0.9348904585847239, 0.9355736843931256, 0.9361497493389688, 0.9352146602681998, 0.9345232854834457], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:03,  1.11it/s]predicting test subjects:  40%|████      | 2/5 [00:01<00:02,  1.41it/s]predicting test subjects:  60%|██████    | 3/5 [00:01<00:01,  1.78it/s]predicting test subjects:  80%|████████  | 4/5 [00:01<00:00,  2.09it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  2.64it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  2.77it/s]
predicting train subjects:   0%|          | 0/247 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/247 [00:00<00:37,  6.56it/s]predicting train subjects:   1%|          | 2/247 [00:00<00:36,  6.67it/s]predicting train subjects:   1%|          | 3/247 [00:00<00:36,  6.72it/s]predicting train subjects:   2%|▏         | 4/247 [00:00<00:36,  6.63it/s]predicting train subjects:   2%|▏         | 5/247 [00:00<00:36,  6.62it/s]predicting train subjects:   2%|▏         | 6/247 [00:00<00:36,  6.60it/s]predicting train subjects:   3%|▎         | 7/247 [00:01<00:36,  6.61it/s]predicting train subjects:   3%|▎         | 8/247 [00:01<00:35,  6.64it/s]predicting train subjects:   4%|▎         | 9/247 [00:01<00:35,  6.61it/s]predicting train subjects:   4%|▍         | 10/247 [00:01<00:35,  6.64it/s]predicting train subjects:   4%|▍         | 11/247 [00:01<00:35,  6.68it/s]predicting train subjects:   5%|▍         | 12/247 [00:01<00:35,  6.69it/s]predicting train subjects:   5%|▌         | 13/247 [00:01<00:35,  6.61it/s]predicting train subjects:   6%|▌         | 14/247 [00:02<00:35,  6.65it/s]predicting train subjects:   6%|▌         | 15/247 [00:02<00:34,  6.66it/s]predicting train subjects:   6%|▋         | 16/247 [00:02<00:34,  6.67it/s]predicting train subjects:   7%|▋         | 17/247 [00:02<00:34,  6.65it/s]predicting train subjects:   7%|▋         | 18/247 [00:02<00:34,  6.68it/s]predicting train subjects:   8%|▊         | 19/247 [00:02<00:34,  6.66it/s]predicting train subjects:   8%|▊         | 20/247 [00:03<00:34,  6.63it/s]predicting train subjects:   9%|▊         | 21/247 [00:03<00:34,  6.60it/s]predicting train subjects:   9%|▉         | 22/247 [00:03<00:34,  6.60it/s]predicting train subjects:   9%|▉         | 23/247 [00:03<00:33,  6.66it/s]predicting train subjects:  10%|▉         | 24/247 [00:03<00:33,  6.68it/s]predicting train subjects:  10%|█         | 25/247 [00:03<00:32,  6.75it/s]predicting train subjects:  11%|█         | 26/247 [00:03<00:32,  6.73it/s]predicting train subjects:  11%|█         | 27/247 [00:04<00:32,  6.75it/s]predicting train subjects:  11%|█▏        | 28/247 [00:04<00:32,  6.72it/s]predicting train subjects:  12%|█▏        | 29/247 [00:04<00:32,  6.72it/s]predicting train subjects:  12%|█▏        | 30/247 [00:04<00:32,  6.72it/s]predicting train subjects:  13%|█▎        | 31/247 [00:04<00:32,  6.72it/s]predicting train subjects:  13%|█▎        | 32/247 [00:04<00:31,  6.75it/s]predicting train subjects:  13%|█▎        | 33/247 [00:04<00:31,  6.78it/s]predicting train subjects:  14%|█▍        | 34/247 [00:05<00:31,  6.80it/s]predicting train subjects:  14%|█▍        | 35/247 [00:05<00:31,  6.83it/s]predicting train subjects:  15%|█▍        | 36/247 [00:05<00:31,  6.80it/s]predicting train subjects:  15%|█▍        | 37/247 [00:05<00:31,  6.74it/s]predicting train subjects:  15%|█▌        | 38/247 [00:05<00:30,  6.77it/s]predicting train subjects:  16%|█▌        | 39/247 [00:05<00:30,  6.76it/s]predicting train subjects:  16%|█▌        | 40/247 [00:05<00:30,  6.79it/s]predicting train subjects:  17%|█▋        | 41/247 [00:06<00:30,  6.82it/s]predicting train subjects:  17%|█▋        | 42/247 [00:06<00:29,  6.86it/s]predicting train subjects:  17%|█▋        | 43/247 [00:06<00:29,  6.89it/s]predicting train subjects:  18%|█▊        | 44/247 [00:06<00:29,  6.82it/s]predicting train subjects:  18%|█▊        | 45/247 [00:06<00:29,  6.77it/s]predicting train subjects:  19%|█▊        | 46/247 [00:06<00:29,  6.77it/s]predicting train subjects:  19%|█▉        | 47/247 [00:06<00:29,  6.80it/s]predicting train subjects:  19%|█▉        | 48/247 [00:07<00:29,  6.83it/s]predicting train subjects:  20%|█▉        | 49/247 [00:07<00:29,  6.79it/s]predicting train subjects:  20%|██        | 50/247 [00:07<00:29,  6.75it/s]predicting train subjects:  21%|██        | 51/247 [00:07<00:29,  6.74it/s]predicting train subjects:  21%|██        | 52/247 [00:07<00:29,  6.69it/s]predicting train subjects:  21%|██▏       | 53/247 [00:07<00:29,  6.69it/s]predicting train subjects:  22%|██▏       | 54/247 [00:08<00:28,  6.72it/s]predicting train subjects:  22%|██▏       | 55/247 [00:08<00:28,  6.67it/s]predicting train subjects:  23%|██▎       | 56/247 [00:08<00:28,  6.65it/s]predicting train subjects:  23%|██▎       | 57/247 [00:08<00:28,  6.66it/s]predicting train subjects:  23%|██▎       | 58/247 [00:08<00:28,  6.69it/s]predicting train subjects:  24%|██▍       | 59/247 [00:08<00:28,  6.59it/s]predicting train subjects:  24%|██▍       | 60/247 [00:08<00:29,  6.34it/s]predicting train subjects:  25%|██▍       | 61/247 [00:09<00:29,  6.38it/s]predicting train subjects:  25%|██▌       | 62/247 [00:09<00:29,  6.37it/s]predicting train subjects:  26%|██▌       | 63/247 [00:09<00:28,  6.38it/s]predicting train subjects:  26%|██▌       | 64/247 [00:09<00:28,  6.43it/s]predicting train subjects:  26%|██▋       | 65/247 [00:09<00:28,  6.40it/s]predicting train subjects:  27%|██▋       | 66/247 [00:09<00:28,  6.42it/s]predicting train subjects:  27%|██▋       | 67/247 [00:10<00:27,  6.47it/s]predicting train subjects:  28%|██▊       | 68/247 [00:10<00:27,  6.50it/s]predicting train subjects:  28%|██▊       | 69/247 [00:10<00:27,  6.49it/s]predicting train subjects:  28%|██▊       | 70/247 [00:10<00:27,  6.47it/s]predicting train subjects:  29%|██▊       | 71/247 [00:10<00:27,  6.44it/s]predicting train subjects:  29%|██▉       | 72/247 [00:10<00:27,  6.47it/s]predicting train subjects:  30%|██▉       | 73/247 [00:10<00:26,  6.46it/s]predicting train subjects:  30%|██▉       | 74/247 [00:11<00:26,  6.46it/s]predicting train subjects:  30%|███       | 75/247 [00:11<00:26,  6.42it/s]predicting train subjects:  31%|███       | 76/247 [00:11<00:26,  6.44it/s]predicting train subjects:  31%|███       | 77/247 [00:11<00:27,  6.18it/s]predicting train subjects:  32%|███▏      | 78/247 [00:11<00:28,  5.95it/s]predicting train subjects:  32%|███▏      | 79/247 [00:12<00:35,  4.72it/s]predicting train subjects:  32%|███▏      | 80/247 [00:12<00:40,  4.08it/s]predicting train subjects:  33%|███▎      | 81/247 [00:12<00:40,  4.14it/s]predicting train subjects:  33%|███▎      | 82/247 [00:12<00:36,  4.49it/s]predicting train subjects:  34%|███▎      | 83/247 [00:13<00:34,  4.77it/s]predicting train subjects:  34%|███▍      | 84/247 [00:13<00:32,  4.98it/s]predicting train subjects:  34%|███▍      | 85/247 [00:13<00:31,  5.16it/s]predicting train subjects:  35%|███▍      | 86/247 [00:13<00:30,  5.31it/s]predicting train subjects:  35%|███▌      | 87/247 [00:13<00:29,  5.44it/s]predicting train subjects:  36%|███▌      | 88/247 [00:13<00:28,  5.51it/s]predicting train subjects:  36%|███▌      | 89/247 [00:14<00:28,  5.51it/s]predicting train subjects:  36%|███▋      | 90/247 [00:14<00:28,  5.54it/s]predicting train subjects:  37%|███▋      | 91/247 [00:14<00:28,  5.56it/s]predicting train subjects:  37%|███▋      | 92/247 [00:14<00:27,  5.61it/s]predicting train subjects:  38%|███▊      | 93/247 [00:14<00:27,  5.61it/s]predicting train subjects:  38%|███▊      | 94/247 [00:14<00:27,  5.61it/s]predicting train subjects:  38%|███▊      | 95/247 [00:15<00:27,  5.61it/s]predicting train subjects:  39%|███▉      | 96/247 [00:15<00:27,  5.53it/s]predicting train subjects:  39%|███▉      | 97/247 [00:15<00:27,  5.51it/s]predicting train subjects:  40%|███▉      | 98/247 [00:15<00:27,  5.49it/s]predicting train subjects:  40%|████      | 99/247 [00:15<00:27,  5.47it/s]predicting train subjects:  40%|████      | 100/247 [00:16<00:26,  5.57it/s]predicting train subjects:  41%|████      | 101/247 [00:16<00:25,  5.64it/s]predicting train subjects:  41%|████▏     | 102/247 [00:16<00:25,  5.69it/s]predicting train subjects:  42%|████▏     | 103/247 [00:16<00:25,  5.72it/s]predicting train subjects:  42%|████▏     | 104/247 [00:16<00:24,  5.73it/s]predicting train subjects:  43%|████▎     | 105/247 [00:16<00:24,  5.75it/s]predicting train subjects:  43%|████▎     | 106/247 [00:17<00:24,  5.70it/s]predicting train subjects:  43%|████▎     | 107/247 [00:17<00:24,  5.77it/s]predicting train subjects:  44%|████▎     | 108/247 [00:17<00:24,  5.78it/s]predicting train subjects:  44%|████▍     | 109/247 [00:17<00:23,  5.78it/s]predicting train subjects:  45%|████▍     | 110/247 [00:17<00:23,  5.80it/s]predicting train subjects:  45%|████▍     | 111/247 [00:17<00:23,  5.81it/s]predicting train subjects:  45%|████▌     | 112/247 [00:18<00:23,  5.79it/s]predicting train subjects:  46%|████▌     | 113/247 [00:18<00:23,  5.77it/s]predicting train subjects:  46%|████▌     | 114/247 [00:18<00:23,  5.72it/s]predicting train subjects:  47%|████▋     | 115/247 [00:18<00:23,  5.71it/s]predicting train subjects:  47%|████▋     | 116/247 [00:18<00:22,  5.71it/s]predicting train subjects:  47%|████▋     | 117/247 [00:19<00:22,  5.74it/s]predicting train subjects:  48%|████▊     | 118/247 [00:19<00:21,  6.05it/s]predicting train subjects:  48%|████▊     | 119/247 [00:19<00:20,  6.28it/s]predicting train subjects:  49%|████▊     | 120/247 [00:19<00:19,  6.49it/s]predicting train subjects:  49%|████▉     | 121/247 [00:19<00:19,  6.63it/s]predicting train subjects:  49%|████▉     | 122/247 [00:19<00:18,  6.76it/s]predicting train subjects:  50%|████▉     | 123/247 [00:19<00:18,  6.87it/s]predicting train subjects:  50%|█████     | 124/247 [00:20<00:17,  6.89it/s]predicting train subjects:  51%|█████     | 125/247 [00:20<00:17,  6.93it/s]predicting train subjects:  51%|█████     | 126/247 [00:20<00:17,  6.96it/s]predicting train subjects:  51%|█████▏    | 127/247 [00:20<00:17,  6.98it/s]predicting train subjects:  52%|█████▏    | 128/247 [00:20<00:17,  6.95it/s]predicting train subjects:  52%|█████▏    | 129/247 [00:20<00:16,  6.97it/s]predicting train subjects:  53%|█████▎    | 130/247 [00:20<00:16,  6.91it/s]predicting train subjects:  53%|█████▎    | 131/247 [00:21<00:16,  6.94it/s]predicting train subjects:  53%|█████▎    | 132/247 [00:21<00:16,  6.97it/s]predicting train subjects:  54%|█████▍    | 133/247 [00:21<00:16,  6.96it/s]predicting train subjects:  54%|█████▍    | 134/247 [00:21<00:16,  6.98it/s]predicting train subjects:  55%|█████▍    | 135/247 [00:21<00:15,  7.00it/s]predicting train subjects:  55%|█████▌    | 136/247 [00:21<00:15,  6.99it/s]predicting train subjects:  55%|█████▌    | 137/247 [00:21<00:15,  6.93it/s]predicting train subjects:  56%|█████▌    | 138/247 [00:22<00:15,  6.96it/s]predicting train subjects:  56%|█████▋    | 139/247 [00:22<00:15,  6.95it/s]predicting train subjects:  57%|█████▋    | 140/247 [00:22<00:15,  6.95it/s]predicting train subjects:  57%|█████▋    | 141/247 [00:22<00:15,  6.94it/s]predicting train subjects:  57%|█████▋    | 142/247 [00:22<00:15,  6.90it/s]predicting train subjects:  58%|█████▊    | 143/247 [00:22<00:15,  6.87it/s]predicting train subjects:  58%|█████▊    | 144/247 [00:22<00:15,  6.78it/s]predicting train subjects:  59%|█████▊    | 145/247 [00:23<00:15,  6.76it/s]predicting train subjects:  59%|█████▉    | 146/247 [00:23<00:14,  6.76it/s]predicting train subjects:  60%|█████▉    | 147/247 [00:23<00:14,  6.80it/s]predicting train subjects:  60%|█████▉    | 148/247 [00:23<00:14,  6.88it/s]predicting train subjects:  60%|██████    | 149/247 [00:23<00:14,  6.94it/s]predicting train subjects:  61%|██████    | 150/247 [00:23<00:13,  6.96it/s]predicting train subjects:  61%|██████    | 151/247 [00:23<00:13,  6.95it/s]predicting train subjects:  62%|██████▏   | 152/247 [00:24<00:13,  6.96it/s]predicting train subjects:  62%|██████▏   | 153/247 [00:24<00:13,  7.01it/s]predicting train subjects:  62%|██████▏   | 154/247 [00:24<00:13,  6.68it/s]predicting train subjects:  63%|██████▎   | 155/247 [00:24<00:14,  6.48it/s]predicting train subjects:  63%|██████▎   | 156/247 [00:24<00:14,  6.32it/s]predicting train subjects:  64%|██████▎   | 157/247 [00:24<00:14,  6.22it/s]predicting train subjects:  64%|██████▍   | 158/247 [00:25<00:14,  5.96it/s]predicting train subjects:  64%|██████▍   | 159/247 [00:25<00:14,  5.98it/s]predicting train subjects:  65%|██████▍   | 160/247 [00:25<00:14,  5.99it/s]predicting train subjects:  65%|██████▌   | 161/247 [00:25<00:14,  5.97it/s]predicting train subjects:  66%|██████▌   | 162/247 [00:25<00:14,  5.98it/s]predicting train subjects:  66%|██████▌   | 163/247 [00:25<00:14,  5.96it/s]predicting train subjects:  66%|██████▋   | 164/247 [00:26<00:13,  5.95it/s]predicting train subjects:  67%|██████▋   | 165/247 [00:26<00:13,  5.91it/s]predicting train subjects:  67%|██████▋   | 166/247 [00:26<00:13,  5.85it/s]predicting train subjects:  68%|██████▊   | 167/247 [00:26<00:13,  5.90it/s]predicting train subjects:  68%|██████▊   | 168/247 [00:26<00:13,  5.93it/s]predicting train subjects:  68%|██████▊   | 169/247 [00:26<00:13,  5.89it/s]predicting train subjects:  69%|██████▉   | 170/247 [00:27<00:13,  5.86it/s]predicting train subjects:  69%|██████▉   | 171/247 [00:27<00:12,  5.86it/s]predicting train subjects:  70%|██████▉   | 172/247 [00:27<00:12,  6.08it/s]predicting train subjects:  70%|███████   | 173/247 [00:27<00:15,  4.82it/s]predicting train subjects:  70%|███████   | 174/247 [00:27<00:13,  5.30it/s]predicting train subjects:  71%|███████   | 175/247 [00:28<00:14,  5.01it/s]predicting train subjects:  71%|███████▏  | 176/247 [00:28<00:13,  5.41it/s]predicting train subjects:  72%|███████▏  | 177/247 [00:28<00:12,  5.78it/s]predicting train subjects:  72%|███████▏  | 178/247 [00:28<00:11,  6.06it/s]predicting train subjects:  72%|███████▏  | 179/247 [00:28<00:10,  6.27it/s]predicting train subjects:  73%|███████▎  | 180/247 [00:28<00:10,  6.47it/s]predicting train subjects:  73%|███████▎  | 181/247 [00:28<00:10,  6.55it/s]predicting train subjects:  74%|███████▎  | 182/247 [00:29<00:09,  6.64it/s]predicting train subjects:  74%|███████▍  | 183/247 [00:29<00:09,  6.74it/s]predicting train subjects:  74%|███████▍  | 184/247 [00:29<00:09,  6.75it/s]predicting train subjects:  75%|███████▍  | 185/247 [00:29<00:09,  6.76it/s]predicting train subjects:  75%|███████▌  | 186/247 [00:29<00:08,  6.81it/s]predicting train subjects:  76%|███████▌  | 187/247 [00:29<00:08,  6.81it/s]predicting train subjects:  76%|███████▌  | 188/247 [00:29<00:08,  6.82it/s]predicting train subjects:  77%|███████▋  | 189/247 [00:30<00:08,  6.81it/s]predicting train subjects:  77%|███████▋  | 190/247 [00:30<00:08,  6.77it/s]predicting train subjects:  77%|███████▋  | 191/247 [00:30<00:08,  6.72it/s]predicting train subjects:  78%|███████▊  | 192/247 [00:30<00:08,  6.76it/s]predicting train subjects:  78%|███████▊  | 193/247 [00:30<00:07,  6.80it/s]predicting train subjects:  79%|███████▊  | 194/247 [00:30<00:07,  6.91it/s]predicting train subjects:  79%|███████▉  | 195/247 [00:31<00:07,  6.99it/s]predicting train subjects:  79%|███████▉  | 196/247 [00:31<00:07,  7.05it/s]predicting train subjects:  80%|███████▉  | 197/247 [00:31<00:07,  7.06it/s]predicting train subjects:  80%|████████  | 198/247 [00:31<00:06,  7.08it/s]predicting train subjects:  81%|████████  | 199/247 [00:31<00:06,  7.07it/s]predicting train subjects:  81%|████████  | 200/247 [00:31<00:06,  7.07it/s]predicting train subjects:  81%|████████▏ | 201/247 [00:31<00:06,  7.11it/s]predicting train subjects:  82%|████████▏ | 202/247 [00:31<00:06,  7.02it/s]predicting train subjects:  82%|████████▏ | 203/247 [00:32<00:06,  7.05it/s]predicting train subjects:  83%|████████▎ | 204/247 [00:32<00:06,  7.10it/s]predicting train subjects:  83%|████████▎ | 205/247 [00:32<00:05,  7.10it/s]predicting train subjects:  83%|████████▎ | 206/247 [00:32<00:05,  7.12it/s]predicting train subjects:  84%|████████▍ | 207/247 [00:32<00:05,  7.12it/s]predicting train subjects:  84%|████████▍ | 208/247 [00:32<00:05,  7.16it/s]predicting train subjects:  85%|████████▍ | 209/247 [00:32<00:05,  7.19it/s]predicting train subjects:  85%|████████▌ | 210/247 [00:33<00:05,  7.18it/s]predicting train subjects:  85%|████████▌ | 211/247 [00:33<00:04,  7.21it/s]predicting train subjects:  86%|████████▌ | 212/247 [00:33<00:04,  7.15it/s]predicting train subjects:  86%|████████▌ | 213/247 [00:33<00:04,  7.13it/s]predicting train subjects:  87%|████████▋ | 214/247 [00:33<00:04,  7.11it/s]predicting train subjects:  87%|████████▋ | 215/247 [00:33<00:04,  7.09it/s]predicting train subjects:  87%|████████▋ | 216/247 [00:33<00:04,  7.05it/s]predicting train subjects:  88%|████████▊ | 217/247 [00:34<00:04,  7.02it/s]predicting train subjects:  88%|████████▊ | 218/247 [00:34<00:04,  6.98it/s]predicting train subjects:  89%|████████▊ | 219/247 [00:34<00:04,  6.95it/s]predicting train subjects:  89%|████████▉ | 220/247 [00:34<00:03,  6.94it/s]predicting train subjects:  89%|████████▉ | 221/247 [00:34<00:03,  6.93it/s]predicting train subjects:  90%|████████▉ | 222/247 [00:34<00:03,  6.96it/s]predicting train subjects:  90%|█████████ | 223/247 [00:34<00:03,  6.98it/s]predicting train subjects:  91%|█████████ | 224/247 [00:35<00:03,  6.92it/s]predicting train subjects:  91%|█████████ | 225/247 [00:35<00:03,  6.93it/s]predicting train subjects:  91%|█████████▏| 226/247 [00:35<00:03,  6.97it/s]predicting train subjects:  92%|█████████▏| 227/247 [00:35<00:02,  6.99it/s]predicting train subjects:  92%|█████████▏| 228/247 [00:35<00:02,  6.95it/s]predicting train subjects:  93%|█████████▎| 229/247 [00:35<00:02,  6.89it/s]predicting train subjects:  93%|█████████▎| 230/247 [00:35<00:02,  6.60it/s]predicting train subjects:  94%|█████████▎| 231/247 [00:36<00:02,  6.45it/s]predicting train subjects:  94%|█████████▍| 232/247 [00:36<00:02,  6.29it/s]predicting train subjects:  94%|█████████▍| 233/247 [00:36<00:02,  6.24it/s]predicting train subjects:  95%|█████████▍| 234/247 [00:36<00:02,  6.21it/s]predicting train subjects:  95%|█████████▌| 235/247 [00:36<00:01,  6.15it/s]predicting train subjects:  96%|█████████▌| 236/247 [00:36<00:01,  6.06it/s]predicting train subjects:  96%|█████████▌| 237/247 [00:37<00:01,  5.98it/s]predicting train subjects:  96%|█████████▋| 238/247 [00:37<00:01,  5.98it/s]predicting train subjects:  97%|█████████▋| 239/247 [00:37<00:01,  5.97it/s]predicting train subjects:  97%|█████████▋| 240/247 [00:37<00:01,  5.95it/s]predicting train subjects:  98%|█████████▊| 241/247 [00:37<00:01,  5.88it/s]predicting train subjects:  98%|█████████▊| 242/247 [00:38<00:00,  5.71it/s]predicting train subjects:  98%|█████████▊| 243/247 [00:38<00:00,  5.77it/s]predicting train subjects:  99%|█████████▉| 244/247 [00:38<00:00,  5.80it/s]predicting train subjects:  99%|█████████▉| 245/247 [00:38<00:00,  5.82it/s]predicting train subjects: 100%|█████████▉| 246/247 [00:38<00:00,  5.87it/s]predicting train subjects: 100%|██████████| 247/247 [00:38<00:00,  5.93it/s]predicting train subjects: 100%|██████████| 247/247 [00:38<00:00,  6.35it/s]
predicting test subjects sagittal:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects sagittal:  20%|██        | 1/5 [00:00<00:00,  6.59it/s]predicting test subjects sagittal:  40%|████      | 2/5 [00:00<00:00,  6.27it/s]predicting test subjects sagittal:  60%|██████    | 3/5 [00:00<00:00,  6.26it/s]predicting test subjects sagittal:  80%|████████  | 4/5 [00:00<00:00,  6.59it/s]predicting test subjects sagittal: 100%|██████████| 5/5 [00:00<00:00,  6.70it/s]predicting test subjects sagittal: 100%|██████████| 5/5 [00:00<00:00,  6.52it/s]
predicting train subjects sagittal:   0%|          | 0/247 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/247 [00:00<00:37,  6.55it/s]predicting train subjects sagittal:   1%|          | 2/247 [00:00<00:36,  6.68it/s]predicting train subjects sagittal:   1%|          | 3/247 [00:00<00:35,  6.81it/s]predicting train subjects sagittal:   2%|▏         | 4/247 [00:00<00:36,  6.71it/s]predicting train subjects sagittal:   2%|▏         | 5/247 [00:00<00:35,  6.72it/s]predicting train subjects sagittal:   2%|▏         | 6/247 [00:00<00:36,  6.66it/s]predicting train subjects sagittal:   3%|▎         | 7/247 [00:01<00:36,  6.61it/s]predicting train subjects sagittal:   3%|▎         | 8/247 [00:01<00:36,  6.59it/s]predicting train subjects sagittal:   4%|▎         | 9/247 [00:01<00:36,  6.60it/s]predicting train subjects sagittal:   4%|▍         | 10/247 [00:01<00:35,  6.61it/s]predicting train subjects sagittal:   4%|▍         | 11/247 [00:01<00:35,  6.67it/s]predicting train subjects sagittal:   5%|▍         | 12/247 [00:01<00:34,  6.73it/s]predicting train subjects sagittal:   5%|▌         | 13/247 [00:01<00:34,  6.71it/s]predicting train subjects sagittal:   6%|▌         | 14/247 [00:02<00:34,  6.68it/s]predicting train subjects sagittal:   6%|▌         | 15/247 [00:02<00:34,  6.73it/s]predicting train subjects sagittal:   6%|▋         | 16/247 [00:02<00:34,  6.74it/s]predicting train subjects sagittal:   7%|▋         | 17/247 [00:02<00:34,  6.72it/s]predicting train subjects sagittal:   7%|▋         | 18/247 [00:02<00:34,  6.68it/s]predicting train subjects sagittal:   8%|▊         | 19/247 [00:02<00:34,  6.65it/s]predicting train subjects sagittal:   8%|▊         | 20/247 [00:02<00:34,  6.61it/s]predicting train subjects sagittal:   9%|▊         | 21/247 [00:03<00:34,  6.58it/s]predicting train subjects sagittal:   9%|▉         | 22/247 [00:03<00:34,  6.60it/s]predicting train subjects sagittal:   9%|▉         | 23/247 [00:03<00:33,  6.68it/s]predicting train subjects sagittal:  10%|▉         | 24/247 [00:03<00:33,  6.75it/s]predicting train subjects sagittal:  10%|█         | 25/247 [00:03<00:32,  6.81it/s]predicting train subjects sagittal:  11%|█         | 26/247 [00:03<00:32,  6.85it/s]predicting train subjects sagittal:  11%|█         | 27/247 [00:04<00:31,  6.88it/s]predicting train subjects sagittal:  11%|█▏        | 28/247 [00:04<00:31,  6.91it/s]predicting train subjects sagittal:  12%|█▏        | 29/247 [00:04<00:31,  6.95it/s]predicting train subjects sagittal:  12%|█▏        | 30/247 [00:04<00:31,  6.94it/s]predicting train subjects sagittal:  13%|█▎        | 31/247 [00:04<00:31,  6.94it/s]predicting train subjects sagittal:  13%|█▎        | 32/247 [00:04<00:30,  7.00it/s]predicting train subjects sagittal:  13%|█▎        | 33/247 [00:04<00:30,  7.02it/s]predicting train subjects sagittal:  14%|█▍        | 34/247 [00:05<00:30,  7.05it/s]predicting train subjects sagittal:  14%|█▍        | 35/247 [00:05<00:30,  7.06it/s]predicting train subjects sagittal:  15%|█▍        | 36/247 [00:05<00:29,  7.05it/s]predicting train subjects sagittal:  15%|█▍        | 37/247 [00:05<00:29,  7.04it/s]predicting train subjects sagittal:  15%|█▌        | 38/247 [00:05<00:29,  7.04it/s]predicting train subjects sagittal:  16%|█▌        | 39/247 [00:05<00:29,  7.03it/s]predicting train subjects sagittal:  16%|█▌        | 40/247 [00:05<00:29,  6.98it/s]predicting train subjects sagittal:  17%|█▋        | 41/247 [00:06<00:29,  7.01it/s]predicting train subjects sagittal:  17%|█▋        | 42/247 [00:06<00:29,  7.05it/s]predicting train subjects sagittal:  17%|█▋        | 43/247 [00:06<00:29,  7.01it/s]predicting train subjects sagittal:  18%|█▊        | 44/247 [00:06<00:28,  7.03it/s]predicting train subjects sagittal:  18%|█▊        | 45/247 [00:06<00:28,  7.06it/s]predicting train subjects sagittal:  19%|█▊        | 46/247 [00:06<00:28,  7.00it/s]predicting train subjects sagittal:  19%|█▉        | 47/247 [00:06<00:28,  7.01it/s]predicting train subjects sagittal:  19%|█▉        | 48/247 [00:07<00:28,  7.05it/s]predicting train subjects sagittal:  20%|█▉        | 49/247 [00:07<00:28,  7.06it/s]predicting train subjects sagittal:  20%|██        | 50/247 [00:07<00:28,  7.01it/s]predicting train subjects sagittal:  21%|██        | 51/247 [00:07<00:28,  7.00it/s]predicting train subjects sagittal:  21%|██        | 52/247 [00:07<00:27,  6.99it/s]predicting train subjects sagittal:  21%|██▏       | 53/247 [00:07<00:27,  6.95it/s]predicting train subjects sagittal:  22%|██▏       | 54/247 [00:07<00:27,  6.92it/s]predicting train subjects sagittal:  22%|██▏       | 55/247 [00:08<00:27,  6.90it/s]predicting train subjects sagittal:  23%|██▎       | 56/247 [00:08<00:27,  6.89it/s]predicting train subjects sagittal:  23%|██▎       | 57/247 [00:08<00:27,  6.85it/s]predicting train subjects sagittal:  23%|██▎       | 58/247 [00:08<00:27,  6.83it/s]predicting train subjects sagittal:  24%|██▍       | 59/247 [00:08<00:28,  6.68it/s]predicting train subjects sagittal:  24%|██▍       | 60/247 [00:08<00:28,  6.55it/s]predicting train subjects sagittal:  25%|██▍       | 61/247 [00:08<00:28,  6.51it/s]predicting train subjects sagittal:  25%|██▌       | 62/247 [00:09<00:28,  6.50it/s]predicting train subjects sagittal:  26%|██▌       | 63/247 [00:09<00:28,  6.44it/s]predicting train subjects sagittal:  26%|██▌       | 64/247 [00:09<00:28,  6.40it/s]predicting train subjects sagittal:  26%|██▋       | 65/247 [00:09<00:28,  6.43it/s]predicting train subjects sagittal:  27%|██▋       | 66/247 [00:09<00:27,  6.49it/s]predicting train subjects sagittal:  27%|██▋       | 67/247 [00:09<00:27,  6.55it/s]predicting train subjects sagittal:  28%|██▊       | 68/247 [00:10<00:27,  6.53it/s]predicting train subjects sagittal:  28%|██▊       | 69/247 [00:10<00:27,  6.48it/s]predicting train subjects sagittal:  28%|██▊       | 70/247 [00:10<00:27,  6.53it/s]predicting train subjects sagittal:  29%|██▊       | 71/247 [00:10<00:26,  6.59it/s]predicting train subjects sagittal:  29%|██▉       | 72/247 [00:10<00:26,  6.62it/s]predicting train subjects sagittal:  30%|██▉       | 73/247 [00:10<00:26,  6.65it/s]predicting train subjects sagittal:  30%|██▉       | 74/247 [00:10<00:26,  6.64it/s]predicting train subjects sagittal:  30%|███       | 75/247 [00:11<00:25,  6.64it/s]predicting train subjects sagittal:  31%|███       | 76/247 [00:11<00:25,  6.63it/s]predicting train subjects sagittal:  31%|███       | 77/247 [00:11<00:27,  6.21it/s]predicting train subjects sagittal:  32%|███▏      | 78/247 [00:11<00:27,  6.13it/s]predicting train subjects sagittal:  32%|███▏      | 79/247 [00:11<00:26,  6.34it/s]predicting train subjects sagittal:  32%|███▏      | 80/247 [00:11<00:25,  6.53it/s]predicting train subjects sagittal:  33%|███▎      | 81/247 [00:12<00:26,  6.34it/s]predicting train subjects sagittal:  33%|███▎      | 82/247 [00:12<00:26,  6.11it/s]predicting train subjects sagittal:  34%|███▎      | 83/247 [00:12<00:27,  6.01it/s]predicting train subjects sagittal:  34%|███▍      | 84/247 [00:12<00:27,  5.97it/s]predicting train subjects sagittal:  34%|███▍      | 85/247 [00:12<00:27,  5.88it/s]predicting train subjects sagittal:  35%|███▍      | 86/247 [00:12<00:27,  5.84it/s]predicting train subjects sagittal:  35%|███▌      | 87/247 [00:13<00:27,  5.82it/s]predicting train subjects sagittal:  36%|███▌      | 88/247 [00:13<00:27,  5.77it/s]predicting train subjects sagittal:  36%|███▌      | 89/247 [00:13<00:27,  5.73it/s]predicting train subjects sagittal:  36%|███▋      | 90/247 [00:13<00:27,  5.71it/s]predicting train subjects sagittal:  37%|███▋      | 91/247 [00:13<00:27,  5.65it/s]predicting train subjects sagittal:  37%|███▋      | 92/247 [00:13<00:27,  5.66it/s]predicting train subjects sagittal:  38%|███▊      | 93/247 [00:14<00:27,  5.67it/s]predicting train subjects sagittal:  38%|███▊      | 94/247 [00:14<00:26,  5.68it/s]predicting train subjects sagittal:  38%|███▊      | 95/247 [00:14<00:26,  5.70it/s]predicting train subjects sagittal:  39%|███▉      | 96/247 [00:14<00:26,  5.72it/s]predicting train subjects sagittal:  39%|███▉      | 97/247 [00:14<00:26,  5.72it/s]predicting train subjects sagittal:  40%|███▉      | 98/247 [00:15<00:26,  5.72it/s]predicting train subjects sagittal:  40%|████      | 99/247 [00:15<00:25,  5.71it/s]predicting train subjects sagittal:  40%|████      | 100/247 [00:15<00:25,  5.72it/s]predicting train subjects sagittal:  41%|████      | 101/247 [00:15<00:25,  5.80it/s]predicting train subjects sagittal:  41%|████▏     | 102/247 [00:15<00:24,  5.83it/s]predicting train subjects sagittal:  42%|████▏     | 103/247 [00:15<00:24,  5.83it/s]predicting train subjects sagittal:  42%|████▏     | 104/247 [00:16<00:24,  5.83it/s]predicting train subjects sagittal:  43%|████▎     | 105/247 [00:16<00:24,  5.86it/s]predicting train subjects sagittal:  43%|████▎     | 106/247 [00:16<00:24,  5.86it/s]predicting train subjects sagittal:  43%|████▎     | 107/247 [00:16<00:23,  5.90it/s]predicting train subjects sagittal:  44%|████▎     | 108/247 [00:16<00:23,  5.94it/s]predicting train subjects sagittal:  44%|████▍     | 109/247 [00:16<00:23,  5.92it/s]predicting train subjects sagittal:  45%|████▍     | 110/247 [00:17<00:23,  5.96it/s]predicting train subjects sagittal:  45%|████▍     | 111/247 [00:17<00:22,  5.94it/s]predicting train subjects sagittal:  45%|████▌     | 112/247 [00:17<00:22,  5.96it/s]predicting train subjects sagittal:  46%|████▌     | 113/247 [00:17<00:22,  5.95it/s]predicting train subjects sagittal:  46%|████▌     | 114/247 [00:17<00:22,  5.85it/s]predicting train subjects sagittal:  47%|████▋     | 115/247 [00:17<00:22,  5.90it/s]predicting train subjects sagittal:  47%|████▋     | 116/247 [00:18<00:22,  5.91it/s]predicting train subjects sagittal:  47%|████▋     | 117/247 [00:18<00:22,  5.77it/s]predicting train subjects sagittal:  48%|████▊     | 118/247 [00:18<00:21,  6.05it/s]predicting train subjects sagittal:  48%|████▊     | 119/247 [00:18<00:20,  6.31it/s]predicting train subjects sagittal:  49%|████▊     | 120/247 [00:18<00:19,  6.53it/s]predicting train subjects sagittal:  49%|████▉     | 121/247 [00:18<00:18,  6.71it/s]predicting train subjects sagittal:  49%|████▉     | 122/247 [00:18<00:18,  6.83it/s]predicting train subjects sagittal:  50%|████▉     | 123/247 [00:19<00:18,  6.88it/s]predicting train subjects sagittal:  50%|█████     | 124/247 [00:19<00:17,  6.92it/s]predicting train subjects sagittal:  51%|█████     | 125/247 [00:19<00:17,  6.95it/s]predicting train subjects sagittal:  51%|█████     | 126/247 [00:19<00:17,  6.93it/s]predicting train subjects sagittal:  51%|█████▏    | 127/247 [00:19<00:17,  6.91it/s]predicting train subjects sagittal:  52%|█████▏    | 128/247 [00:19<00:17,  6.93it/s]predicting train subjects sagittal:  52%|█████▏    | 129/247 [00:19<00:16,  6.98it/s]predicting train subjects sagittal:  53%|█████▎    | 130/247 [00:20<00:16,  7.01it/s]predicting train subjects sagittal:  53%|█████▎    | 131/247 [00:20<00:16,  7.03it/s]predicting train subjects sagittal:  53%|█████▎    | 132/247 [00:20<00:16,  7.04it/s]predicting train subjects sagittal:  54%|█████▍    | 133/247 [00:20<00:16,  7.05it/s]predicting train subjects sagittal:  54%|█████▍    | 134/247 [00:20<00:16,  7.06it/s]predicting train subjects sagittal:  55%|█████▍    | 135/247 [00:20<00:15,  7.06it/s]predicting train subjects sagittal:  55%|█████▌    | 136/247 [00:20<00:15,  6.99it/s]predicting train subjects sagittal:  55%|█████▌    | 137/247 [00:21<00:15,  6.99it/s]predicting train subjects sagittal:  56%|█████▌    | 138/247 [00:21<00:15,  6.99it/s]predicting train subjects sagittal:  56%|█████▋    | 139/247 [00:21<00:15,  6.96it/s]predicting train subjects sagittal:  57%|█████▋    | 140/247 [00:21<00:15,  6.97it/s]predicting train subjects sagittal:  57%|█████▋    | 141/247 [00:21<00:15,  7.00it/s]predicting train subjects sagittal:  57%|█████▋    | 142/247 [00:21<00:14,  7.01it/s]predicting train subjects sagittal:  58%|█████▊    | 143/247 [00:21<00:14,  7.00it/s]predicting train subjects sagittal:  58%|█████▊    | 144/247 [00:22<00:14,  6.98it/s]predicting train subjects sagittal:  59%|█████▊    | 145/247 [00:22<00:14,  6.95it/s]predicting train subjects sagittal:  59%|█████▉    | 146/247 [00:22<00:14,  6.95it/s]predicting train subjects sagittal:  60%|█████▉    | 147/247 [00:22<00:14,  6.95it/s]predicting train subjects sagittal:  60%|█████▉    | 148/247 [00:22<00:14,  6.97it/s]predicting train subjects sagittal:  60%|██████    | 149/247 [00:22<00:14,  6.99it/s]predicting train subjects sagittal:  61%|██████    | 150/247 [00:22<00:13,  7.01it/s]predicting train subjects sagittal:  61%|██████    | 151/247 [00:23<00:13,  7.00it/s]predicting train subjects sagittal:  62%|██████▏   | 152/247 [00:23<00:13,  6.99it/s]predicting train subjects sagittal:  62%|██████▏   | 153/247 [00:23<00:13,  7.01it/s]predicting train subjects sagittal:  62%|██████▏   | 154/247 [00:23<00:14,  6.62it/s]predicting train subjects sagittal:  63%|██████▎   | 155/247 [00:23<00:14,  6.38it/s]predicting train subjects sagittal:  63%|██████▎   | 156/247 [00:23<00:14,  6.25it/s]predicting train subjects sagittal:  64%|██████▎   | 157/247 [00:24<00:14,  6.10it/s]predicting train subjects sagittal:  64%|██████▍   | 158/247 [00:24<00:14,  6.05it/s]predicting train subjects sagittal:  64%|██████▍   | 159/247 [00:24<00:14,  6.01it/s]predicting train subjects sagittal:  65%|██████▍   | 160/247 [00:24<00:14,  5.94it/s]predicting train subjects sagittal:  65%|██████▌   | 161/247 [00:24<00:14,  5.95it/s]predicting train subjects sagittal:  66%|██████▌   | 162/247 [00:24<00:14,  5.96it/s]predicting train subjects sagittal:  66%|██████▌   | 163/247 [00:25<00:14,  5.94it/s]predicting train subjects sagittal:  66%|██████▋   | 164/247 [00:25<00:13,  5.93it/s]predicting train subjects sagittal:  67%|██████▋   | 165/247 [00:25<00:13,  5.95it/s]predicting train subjects sagittal:  67%|██████▋   | 166/247 [00:25<00:13,  5.94it/s]predicting train subjects sagittal:  68%|██████▊   | 167/247 [00:25<00:13,  5.90it/s]predicting train subjects sagittal:  68%|██████▊   | 168/247 [00:25<00:13,  5.90it/s]predicting train subjects sagittal:  68%|██████▊   | 169/247 [00:26<00:13,  5.87it/s]predicting train subjects sagittal:  69%|██████▉   | 170/247 [00:26<00:13,  5.83it/s]predicting train subjects sagittal:  69%|██████▉   | 171/247 [00:26<00:13,  5.84it/s]predicting train subjects sagittal:  70%|██████▉   | 172/247 [00:26<00:12,  6.10it/s]predicting train subjects sagittal:  70%|███████   | 173/247 [00:26<00:11,  6.38it/s]predicting train subjects sagittal:  70%|███████   | 174/247 [00:26<00:11,  6.52it/s]predicting train subjects sagittal:  71%|███████   | 175/247 [00:27<00:11,  6.22it/s]predicting train subjects sagittal:  71%|███████▏  | 176/247 [00:27<00:11,  6.22it/s]predicting train subjects sagittal:  72%|███████▏  | 177/247 [00:27<00:11,  6.33it/s]predicting train subjects sagittal:  72%|███████▏  | 178/247 [00:27<00:10,  6.36it/s]predicting train subjects sagittal:  72%|███████▏  | 179/247 [00:27<00:10,  6.41it/s]predicting train subjects sagittal:  73%|███████▎  | 180/247 [00:27<00:10,  6.52it/s]predicting train subjects sagittal:  73%|███████▎  | 181/247 [00:27<00:10,  6.57it/s]predicting train subjects sagittal:  74%|███████▎  | 182/247 [00:28<00:09,  6.57it/s]predicting train subjects sagittal:  74%|███████▍  | 183/247 [00:28<00:09,  6.63it/s]predicting train subjects sagittal:  74%|███████▍  | 184/247 [00:28<00:09,  6.67it/s]predicting train subjects sagittal:  75%|███████▍  | 185/247 [00:28<00:09,  6.71it/s]predicting train subjects sagittal:  75%|███████▌  | 186/247 [00:28<00:09,  6.76it/s]predicting train subjects sagittal:  76%|███████▌  | 187/247 [00:28<00:08,  6.76it/s]predicting train subjects sagittal:  76%|███████▌  | 188/247 [00:29<00:08,  6.77it/s]predicting train subjects sagittal:  77%|███████▋  | 189/247 [00:29<00:08,  6.74it/s]predicting train subjects sagittal:  77%|███████▋  | 190/247 [00:29<00:08,  6.75it/s]predicting train subjects sagittal:  77%|███████▋  | 191/247 [00:29<00:08,  6.77it/s]predicting train subjects sagittal:  78%|███████▊  | 192/247 [00:29<00:08,  6.81it/s]predicting train subjects sagittal:  78%|███████▊  | 193/247 [00:29<00:07,  6.83it/s]predicting train subjects sagittal:  79%|███████▊  | 194/247 [00:29<00:07,  6.96it/s]predicting train subjects sagittal:  79%|███████▉  | 195/247 [00:30<00:07,  7.01it/s]predicting train subjects sagittal:  79%|███████▉  | 196/247 [00:30<00:07,  7.08it/s]predicting train subjects sagittal:  80%|███████▉  | 197/247 [00:30<00:07,  7.14it/s]predicting train subjects sagittal:  80%|████████  | 198/247 [00:30<00:06,  7.14it/s]predicting train subjects sagittal:  81%|████████  | 199/247 [00:30<00:06,  7.17it/s]predicting train subjects sagittal:  81%|████████  | 200/247 [00:30<00:06,  7.20it/s]predicting train subjects sagittal:  81%|████████▏ | 201/247 [00:30<00:06,  7.21it/s]predicting train subjects sagittal:  82%|████████▏ | 202/247 [00:30<00:06,  7.18it/s]predicting train subjects sagittal:  82%|████████▏ | 203/247 [00:31<00:06,  7.20it/s]predicting train subjects sagittal:  83%|████████▎ | 204/247 [00:31<00:05,  7.20it/s]predicting train subjects sagittal:  83%|████████▎ | 205/247 [00:31<00:05,  7.13it/s]predicting train subjects sagittal:  83%|████████▎ | 206/247 [00:31<00:05,  7.17it/s]predicting train subjects sagittal:  84%|████████▍ | 207/247 [00:31<00:05,  7.21it/s]predicting train subjects sagittal:  84%|████████▍ | 208/247 [00:31<00:05,  7.19it/s]predicting train subjects sagittal:  85%|████████▍ | 209/247 [00:31<00:05,  7.20it/s]predicting train subjects sagittal:  85%|████████▌ | 210/247 [00:32<00:05,  7.19it/s]predicting train subjects sagittal:  85%|████████▌ | 211/247 [00:32<00:04,  7.22it/s]predicting train subjects sagittal:  86%|████████▌ | 212/247 [00:32<00:04,  7.04it/s]predicting train subjects sagittal:  86%|████████▌ | 213/247 [00:32<00:04,  6.99it/s]predicting train subjects sagittal:  87%|████████▋ | 214/247 [00:32<00:04,  6.94it/s]predicting train subjects sagittal:  87%|████████▋ | 215/247 [00:32<00:04,  6.91it/s]predicting train subjects sagittal:  87%|████████▋ | 216/247 [00:32<00:04,  6.84it/s]predicting train subjects sagittal:  88%|████████▊ | 217/247 [00:33<00:04,  6.81it/s]predicting train subjects sagittal:  88%|████████▊ | 218/247 [00:33<00:04,  6.82it/s]predicting train subjects sagittal:  89%|████████▊ | 219/247 [00:33<00:04,  6.86it/s]predicting train subjects sagittal:  89%|████████▉ | 220/247 [00:33<00:03,  6.86it/s]predicting train subjects sagittal:  89%|████████▉ | 221/247 [00:33<00:03,  6.88it/s]predicting train subjects sagittal:  90%|████████▉ | 222/247 [00:33<00:03,  6.86it/s]predicting train subjects sagittal:  90%|█████████ | 223/247 [00:33<00:03,  6.85it/s]predicting train subjects sagittal:  91%|█████████ | 224/247 [00:34<00:03,  6.85it/s]predicting train subjects sagittal:  91%|█████████ | 225/247 [00:34<00:03,  6.86it/s]predicting train subjects sagittal:  91%|█████████▏| 226/247 [00:34<00:03,  6.86it/s]predicting train subjects sagittal:  92%|█████████▏| 227/247 [00:34<00:02,  6.87it/s]predicting train subjects sagittal:  92%|█████████▏| 228/247 [00:34<00:02,  6.88it/s]predicting train subjects sagittal:  93%|█████████▎| 229/247 [00:34<00:02,  6.85it/s]predicting train subjects sagittal:  93%|█████████▎| 230/247 [00:35<00:02,  6.55it/s]predicting train subjects sagittal:  94%|█████████▎| 231/247 [00:35<00:02,  6.35it/s]predicting train subjects sagittal:  94%|█████████▍| 232/247 [00:35<00:02,  6.24it/s]predicting train subjects sagittal:  94%|█████████▍| 233/247 [00:35<00:02,  6.18it/s]predicting train subjects sagittal:  95%|█████████▍| 234/247 [00:35<00:02,  6.13it/s]predicting train subjects sagittal:  95%|█████████▌| 235/247 [00:35<00:01,  6.07it/s]predicting train subjects sagittal:  96%|█████████▌| 236/247 [00:36<00:01,  6.04it/s]predicting train subjects sagittal:  96%|█████████▌| 237/247 [00:36<00:01,  6.03it/s]predicting train subjects sagittal:  96%|█████████▋| 238/247 [00:36<00:01,  6.04it/s]predicting train subjects sagittal:  97%|█████████▋| 239/247 [00:36<00:01,  5.87it/s]predicting train subjects sagittal:  97%|█████████▋| 240/247 [00:36<00:01,  5.92it/s]predicting train subjects sagittal:  98%|█████████▊| 241/247 [00:36<00:01,  5.80it/s]predicting train subjects sagittal:  98%|█████████▊| 242/247 [00:37<00:00,  5.85it/s]predicting train subjects sagittal:  98%|█████████▊| 243/247 [00:37<00:00,  5.92it/s]predicting train subjects sagittal:  99%|█████████▉| 244/247 [00:37<00:00,  5.95it/s]predicting train subjects sagittal:  99%|█████████▉| 245/247 [00:37<00:00,  5.95it/s]predicting train subjects sagittal: 100%|█████████▉| 246/247 [00:37<00:00,  6.00it/s]predicting train subjects sagittal: 100%|██████████| 247/247 [00:37<00:00,  5.99it/s]predicting train subjects sagittal: 100%|██████████| 247/247 [00:37<00:00,  6.52it/s]
saving BB  test1-THALAMUS:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 5/5 [00:00<00:00, 73.26it/s]
saving BB  train1-THALAMUS:   0%|          | 0/247 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 8/247 [00:00<00:03, 77.95it/s]saving BB  train1-THALAMUS:   7%|▋         | 17/247 [00:00<00:02, 77.76it/s]saving BB  train1-THALAMUS:  11%|█         | 26/247 [00:00<00:02, 79.18it/s]saving BB  train1-THALAMUS:  14%|█▍        | 35/247 [00:00<00:02, 81.46it/s]saving BB  train1-THALAMUS:  18%|█▊        | 44/247 [00:00<00:02, 81.92it/s]saving BB  train1-THALAMUS:  21%|██        | 52/247 [00:00<00:02, 80.99it/s]saving BB  train1-THALAMUS:  25%|██▍       | 61/247 [00:00<00:02, 81.91it/s]saving BB  train1-THALAMUS:  28%|██▊       | 69/247 [00:00<00:02, 80.87it/s]saving BB  train1-THALAMUS:  31%|███       | 77/247 [00:00<00:02, 77.48it/s]saving BB  train1-THALAMUS:  34%|███▍      | 85/247 [00:01<00:02, 75.51it/s]saving BB  train1-THALAMUS:  38%|███▊      | 93/247 [00:01<00:02, 73.24it/s]saving BB  train1-THALAMUS:  41%|████      | 101/247 [00:01<00:02, 71.56it/s]saving BB  train1-THALAMUS:  44%|████▍     | 109/247 [00:01<00:01, 72.04it/s]saving BB  train1-THALAMUS:  47%|████▋     | 117/247 [00:01<00:01, 71.97it/s]saving BB  train1-THALAMUS:  51%|█████     | 125/247 [00:01<00:01, 73.93it/s]saving BB  train1-THALAMUS:  54%|█████▍    | 133/247 [00:01<00:01, 75.03it/s]saving BB  train1-THALAMUS:  57%|█████▋    | 142/247 [00:01<00:01, 76.90it/s]saving BB  train1-THALAMUS:  61%|██████    | 151/247 [00:01<00:01, 78.20it/s]saving BB  train1-THALAMUS:  64%|██████▍   | 159/247 [00:02<00:01, 77.29it/s]saving BB  train1-THALAMUS:  68%|██████▊   | 167/247 [00:02<00:01, 75.36it/s]saving BB  train1-THALAMUS:  71%|███████   | 175/247 [00:02<00:00, 75.35it/s]saving BB  train1-THALAMUS:  74%|███████▍  | 183/247 [00:02<00:00, 76.04it/s]saving BB  train1-THALAMUS:  77%|███████▋  | 191/247 [00:02<00:00, 75.34it/s]saving BB  train1-THALAMUS:  81%|████████  | 200/247 [00:02<00:00, 77.06it/s]saving BB  train1-THALAMUS:  85%|████████▍ | 209/247 [00:02<00:00, 78.63it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 218/247 [00:02<00:00, 81.24it/s]saving BB  train1-THALAMUS:  92%|█████████▏| 227/247 [00:02<00:00, 83.21it/s]saving BB  train1-THALAMUS:  96%|█████████▌| 236/247 [00:03<00:00, 80.91it/s]saving BB  train1-THALAMUS:  99%|█████████▉| 245/247 [00:03<00:00, 78.89it/s]saving BB  train1-THALAMUS: 100%|██████████| 247/247 [00:03<00:00, 77.57it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 5/5 [00:00<00:00, 81.00it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/247 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   4%|▎         | 9/247 [00:00<00:02, 82.15it/s]saving BB  train1-THALAMUS Sagittal:   7%|▋         | 18/247 [00:00<00:02, 82.47it/s]saving BB  train1-THALAMUS Sagittal:  11%|█         | 27/247 [00:00<00:02, 82.92it/s]saving BB  train1-THALAMUS Sagittal:  15%|█▍        | 36/247 [00:00<00:02, 84.71it/s]saving BB  train1-THALAMUS Sagittal:  18%|█▊        | 45/247 [00:00<00:02, 85.90it/s]saving BB  train1-THALAMUS Sagittal:  22%|██▏       | 55/247 [00:00<00:02, 87.84it/s]saving BB  train1-THALAMUS Sagittal:  26%|██▌       | 64/247 [00:00<00:02, 86.94it/s]saving BB  train1-THALAMUS Sagittal:  30%|██▉       | 73/247 [00:00<00:02, 84.97it/s]saving BB  train1-THALAMUS Sagittal:  33%|███▎      | 82/247 [00:00<00:01, 82.89it/s]saving BB  train1-THALAMUS Sagittal:  36%|███▋      | 90/247 [00:01<00:02, 77.47it/s]saving BB  train1-THALAMUS Sagittal:  40%|███▉      | 98/247 [00:01<00:02, 74.13it/s]saving BB  train1-THALAMUS Sagittal:  43%|████▎     | 106/247 [00:01<00:01, 73.43it/s]saving BB  train1-THALAMUS Sagittal:  46%|████▌     | 114/247 [00:01<00:01, 74.67it/s]saving BB  train1-THALAMUS Sagittal:  49%|████▉     | 122/247 [00:01<00:01, 76.08it/s]saving BB  train1-THALAMUS Sagittal:  53%|█████▎    | 131/247 [00:01<00:01, 77.71it/s]saving BB  train1-THALAMUS Sagittal:  57%|█████▋    | 140/247 [00:01<00:01, 80.36it/s]saving BB  train1-THALAMUS Sagittal:  61%|██████    | 150/247 [00:01<00:01, 83.12it/s]saving BB  train1-THALAMUS Sagittal:  64%|██████▍   | 159/247 [00:01<00:01, 82.18it/s]saving BB  train1-THALAMUS Sagittal:  68%|██████▊   | 168/247 [00:02<00:00, 80.93it/s]saving BB  train1-THALAMUS Sagittal:  72%|███████▏  | 177/247 [00:02<00:00, 80.15it/s]saving BB  train1-THALAMUS Sagittal:  75%|███████▌  | 186/247 [00:02<00:00, 79.75it/s]saving BB  train1-THALAMUS Sagittal:  79%|███████▊  | 194/247 [00:02<00:00, 79.77it/s]saving BB  train1-THALAMUS Sagittal:  82%|████████▏ | 203/247 [00:02<00:00, 81.40it/s]saving BB  train1-THALAMUS Sagittal:  86%|████████▌ | 212/247 [00:02<00:00, 82.60it/s]saving BB  train1-THALAMUS Sagittal:  90%|████████▉ | 222/247 [00:02<00:00, 84.75it/s]saving BB  train1-THALAMUS Sagittal:  94%|█████████▎| 231/247 [00:02<00:00, 84.45it/s]saving BB  train1-THALAMUS Sagittal:  97%|█████████▋| 240/247 [00:02<00:00, 82.50it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 247/247 [00:03<00:00, 81.38it/s]
Loading train:   0%|          | 0/247 [00:00<?, ?it/s]Loading train:   0%|          | 1/247 [00:00<03:15,  1.26it/s]Loading train:   1%|          | 2/247 [00:01<03:09,  1.29it/s]Loading train:   1%|          | 3/247 [00:02<03:01,  1.34it/s]Loading train:   2%|▏         | 4/247 [00:03<03:06,  1.30it/s]Loading train:   2%|▏         | 5/247 [00:03<02:46,  1.45it/s]Loading train:   2%|▏         | 6/247 [00:04<02:32,  1.58it/s]Loading train:   3%|▎         | 7/247 [00:04<02:21,  1.70it/s]Loading train:   3%|▎         | 8/247 [00:05<02:13,  1.79it/s]Loading train:   4%|▎         | 9/247 [00:05<02:09,  1.83it/s]Loading train:   4%|▍         | 10/247 [00:06<02:09,  1.83it/s]Loading train:   4%|▍         | 11/247 [00:06<02:05,  1.88it/s]Loading train:   5%|▍         | 12/247 [00:07<02:03,  1.90it/s]Loading train:   5%|▌         | 13/247 [00:07<02:03,  1.90it/s]Loading train:   6%|▌         | 14/247 [00:08<02:01,  1.92it/s]Loading train:   6%|▌         | 15/247 [00:08<01:59,  1.94it/s]Loading train:   6%|▋         | 16/247 [00:09<02:01,  1.90it/s]Loading train:   7%|▋         | 17/247 [00:09<01:58,  1.94it/s]Loading train:   7%|▋         | 18/247 [00:10<01:59,  1.92it/s]Loading train:   8%|▊         | 19/247 [00:10<01:56,  1.95it/s]Loading train:   8%|▊         | 20/247 [00:11<01:58,  1.92it/s]Loading train:   9%|▊         | 21/247 [00:11<01:56,  1.94it/s]Loading train:   9%|▉         | 22/247 [00:12<01:55,  1.94it/s]Loading train:   9%|▉         | 23/247 [00:12<01:52,  2.00it/s]Loading train:  10%|▉         | 24/247 [00:13<01:49,  2.04it/s]Loading train:  10%|█         | 25/247 [00:13<01:47,  2.07it/s]Loading train:  11%|█         | 26/247 [00:14<01:45,  2.10it/s]Loading train:  11%|█         | 27/247 [00:14<01:42,  2.14it/s]Loading train:  11%|█▏        | 28/247 [00:15<01:42,  2.13it/s]Loading train:  12%|█▏        | 29/247 [00:15<01:43,  2.11it/s]Loading train:  12%|█▏        | 30/247 [00:15<01:41,  2.15it/s]Loading train:  13%|█▎        | 31/247 [00:16<01:41,  2.13it/s]Loading train:  13%|█▎        | 32/247 [00:16<01:40,  2.14it/s]Loading train:  13%|█▎        | 33/247 [00:17<01:38,  2.16it/s]Loading train:  14%|█▍        | 34/247 [00:17<01:39,  2.15it/s]Loading train:  14%|█▍        | 35/247 [00:18<01:38,  2.16it/s]Loading train:  15%|█▍        | 36/247 [00:18<01:36,  2.18it/s]Loading train:  15%|█▍        | 37/247 [00:19<01:36,  2.18it/s]Loading train:  15%|█▌        | 38/247 [00:19<01:35,  2.20it/s]Loading train:  16%|█▌        | 39/247 [00:20<01:35,  2.19it/s]Loading train:  16%|█▌        | 40/247 [00:20<01:35,  2.17it/s]Loading train:  17%|█▋        | 41/247 [00:21<01:35,  2.16it/s]Loading train:  17%|█▋        | 42/247 [00:21<01:36,  2.13it/s]Loading train:  17%|█▋        | 43/247 [00:21<01:35,  2.13it/s]Loading train:  18%|█▊        | 44/247 [00:22<01:35,  2.12it/s]Loading train:  18%|█▊        | 45/247 [00:22<01:36,  2.09it/s]Loading train:  19%|█▊        | 46/247 [00:23<01:34,  2.12it/s]Loading train:  19%|█▉        | 47/247 [00:23<01:34,  2.12it/s]Loading train:  19%|█▉        | 48/247 [00:24<01:32,  2.15it/s]Loading train:  20%|█▉        | 49/247 [00:24<01:31,  2.17it/s]Loading train:  20%|██        | 50/247 [00:25<01:31,  2.16it/s]Loading train:  21%|██        | 51/247 [00:25<01:31,  2.14it/s]Loading train:  21%|██        | 52/247 [00:26<01:31,  2.14it/s]Loading train:  21%|██▏       | 53/247 [00:26<01:31,  2.12it/s]Loading train:  22%|██▏       | 54/247 [00:27<01:31,  2.12it/s]Loading train:  22%|██▏       | 55/247 [00:27<01:30,  2.13it/s]Loading train:  23%|██▎       | 56/247 [00:28<01:29,  2.12it/s]Loading train:  23%|██▎       | 57/247 [00:28<01:28,  2.14it/s]Loading train:  23%|██▎       | 58/247 [00:29<01:29,  2.12it/s]Loading train:  24%|██▍       | 59/247 [00:29<01:34,  1.99it/s]Loading train:  24%|██▍       | 60/247 [00:30<01:38,  1.90it/s]Loading train:  25%|██▍       | 61/247 [00:30<01:38,  1.88it/s]Loading train:  25%|██▌       | 62/247 [00:31<01:38,  1.88it/s]Loading train:  26%|██▌       | 63/247 [00:31<01:38,  1.88it/s]Loading train:  26%|██▌       | 64/247 [00:32<01:38,  1.87it/s]Loading train:  26%|██▋       | 65/247 [00:32<01:37,  1.86it/s]Loading train:  27%|██▋       | 66/247 [00:33<01:37,  1.86it/s]Loading train:  27%|██▋       | 67/247 [00:33<01:37,  1.85it/s]Loading train:  28%|██▊       | 68/247 [00:34<01:36,  1.85it/s]Loading train:  28%|██▊       | 69/247 [00:35<01:36,  1.84it/s]Loading train:  28%|██▊       | 70/247 [00:35<01:35,  1.85it/s]Loading train:  29%|██▊       | 71/247 [00:36<01:35,  1.84it/s]Loading train:  29%|██▉       | 72/247 [00:36<01:36,  1.81it/s]Loading train:  30%|██▉       | 73/247 [00:37<01:35,  1.82it/s]Loading train:  30%|██▉       | 74/247 [00:37<01:34,  1.83it/s]Loading train:  30%|███       | 75/247 [00:38<01:34,  1.83it/s]Loading train:  31%|███       | 76/247 [00:38<01:33,  1.83it/s]Loading train:  31%|███       | 77/247 [00:39<01:55,  1.47it/s]Loading train:  32%|███▏      | 78/247 [00:40<02:06,  1.34it/s]Loading train:  32%|███▏      | 79/247 [00:41<02:07,  1.31it/s]Loading train:  32%|███▏      | 80/247 [00:42<02:06,  1.32it/s]Loading train:  33%|███▎      | 81/247 [00:43<02:13,  1.25it/s]Loading train:  33%|███▎      | 82/247 [00:43<02:02,  1.35it/s]Loading train:  34%|███▎      | 83/247 [00:44<01:53,  1.45it/s]Loading train:  34%|███▍      | 84/247 [00:44<01:47,  1.52it/s]Loading train:  34%|███▍      | 85/247 [00:45<01:44,  1.55it/s]Loading train:  35%|███▍      | 86/247 [00:46<01:41,  1.59it/s]Loading train:  35%|███▌      | 87/247 [00:46<01:40,  1.59it/s]Loading train:  36%|███▌      | 88/247 [00:47<01:38,  1.61it/s]Loading train:  36%|███▌      | 89/247 [00:48<01:36,  1.64it/s]Loading train:  36%|███▋      | 90/247 [00:48<01:34,  1.66it/s]Loading train:  37%|███▋      | 91/247 [00:49<01:34,  1.66it/s]Loading train:  37%|███▋      | 92/247 [00:49<01:32,  1.68it/s]Loading train:  38%|███▊      | 93/247 [00:50<01:30,  1.70it/s]Loading train:  38%|███▊      | 94/247 [00:50<01:29,  1.72it/s]Loading train:  38%|███▊      | 95/247 [00:51<01:28,  1.71it/s]Loading train:  39%|███▉      | 96/247 [00:52<01:27,  1.73it/s]Loading train:  39%|███▉      | 97/247 [00:52<01:27,  1.72it/s]Loading train:  40%|███▉      | 98/247 [00:53<01:26,  1.73it/s]Loading train:  40%|████      | 99/247 [00:53<01:24,  1.74it/s]Loading train:  40%|████      | 100/247 [00:54<01:23,  1.77it/s]Loading train:  41%|████      | 101/247 [00:54<01:22,  1.77it/s]Loading train:  41%|████▏     | 102/247 [00:55<01:21,  1.77it/s]Loading train:  42%|████▏     | 103/247 [00:56<01:20,  1.79it/s]Loading train:  42%|████▏     | 104/247 [00:56<01:19,  1.80it/s]Loading train:  43%|████▎     | 105/247 [00:57<01:18,  1.82it/s]Loading train:  43%|████▎     | 106/247 [00:57<01:19,  1.77it/s]Loading train:  43%|████▎     | 107/247 [00:58<01:18,  1.77it/s]Loading train:  44%|████▎     | 108/247 [00:58<01:17,  1.79it/s]Loading train:  44%|████▍     | 109/247 [00:59<01:18,  1.77it/s]Loading train:  45%|████▍     | 110/247 [00:59<01:16,  1.79it/s]Loading train:  45%|████▍     | 111/247 [01:00<01:16,  1.79it/s]Loading train:  45%|████▌     | 112/247 [01:01<01:15,  1.78it/s]Loading train:  46%|████▌     | 113/247 [01:01<01:15,  1.78it/s]Loading train:  46%|████▌     | 114/247 [01:02<01:15,  1.76it/s]Loading train:  47%|████▋     | 115/247 [01:02<01:14,  1.78it/s]Loading train:  47%|████▋     | 116/247 [01:03<01:13,  1.78it/s]Loading train:  47%|████▋     | 117/247 [01:03<01:13,  1.78it/s]Loading train:  48%|████▊     | 118/247 [01:04<01:11,  1.80it/s]Loading train:  48%|████▊     | 119/247 [01:04<01:11,  1.78it/s]Loading train:  49%|████▊     | 120/247 [01:05<01:10,  1.80it/s]Loading train:  49%|████▉     | 121/247 [01:06<01:09,  1.80it/s]Loading train:  49%|████▉     | 122/247 [01:06<01:08,  1.82it/s]Loading train:  50%|████▉     | 123/247 [01:07<01:06,  1.86it/s]Loading train:  50%|█████     | 124/247 [01:07<01:05,  1.89it/s]Loading train:  51%|█████     | 125/247 [01:08<01:03,  1.91it/s]Loading train:  51%|█████     | 126/247 [01:08<01:03,  1.89it/s]Loading train:  51%|█████▏    | 127/247 [01:09<01:03,  1.89it/s]Loading train:  52%|█████▏    | 128/247 [01:09<01:02,  1.89it/s]Loading train:  52%|█████▏    | 129/247 [01:10<01:01,  1.92it/s]Loading train:  53%|█████▎    | 130/247 [01:10<01:01,  1.91it/s]Loading train:  53%|█████▎    | 131/247 [01:11<01:00,  1.90it/s]Loading train:  53%|█████▎    | 132/247 [01:11<01:00,  1.90it/s]Loading train:  54%|█████▍    | 133/247 [01:12<00:59,  1.91it/s]Loading train:  54%|█████▍    | 134/247 [01:12<01:00,  1.88it/s]Loading train:  55%|█████▍    | 135/247 [01:13<01:00,  1.86it/s]Loading train:  55%|█████▌    | 136/247 [01:13<00:57,  1.92it/s]Loading train:  55%|█████▌    | 137/247 [01:14<00:56,  1.96it/s]Loading train:  56%|█████▌    | 138/247 [01:14<00:54,  2.00it/s]Loading train:  56%|█████▋    | 139/247 [01:15<00:53,  2.00it/s]Loading train:  57%|█████▋    | 140/247 [01:15<00:52,  2.02it/s]Loading train:  57%|█████▋    | 141/247 [01:16<00:51,  2.06it/s]Loading train:  57%|█████▋    | 142/247 [01:16<00:50,  2.08it/s]Loading train:  58%|█████▊    | 143/247 [01:17<00:49,  2.08it/s]Loading train:  58%|█████▊    | 144/247 [01:17<00:50,  2.04it/s]Loading train:  59%|█████▊    | 145/247 [01:18<00:50,  2.02it/s]Loading train:  59%|█████▉    | 146/247 [01:18<00:49,  2.03it/s]Loading train:  60%|█████▉    | 147/247 [01:19<00:49,  2.01it/s]Loading train:  60%|█████▉    | 148/247 [01:19<00:49,  2.00it/s]Loading train:  60%|██████    | 149/247 [01:20<00:48,  2.02it/s]Loading train:  61%|██████    | 150/247 [01:20<00:47,  2.03it/s]Loading train:  61%|██████    | 151/247 [01:21<00:46,  2.05it/s]Loading train:  62%|██████▏   | 152/247 [01:21<00:46,  2.06it/s]Loading train:  62%|██████▏   | 153/247 [01:22<00:46,  2.04it/s]Loading train:  62%|██████▏   | 154/247 [01:22<00:47,  1.97it/s]Loading train:  63%|██████▎   | 155/247 [01:23<00:47,  1.92it/s]Loading train:  63%|██████▎   | 156/247 [01:23<00:47,  1.90it/s]Loading train:  64%|██████▎   | 157/247 [01:24<00:49,  1.84it/s]Loading train:  64%|██████▍   | 158/247 [01:25<00:48,  1.82it/s]Loading train:  64%|██████▍   | 159/247 [01:25<00:47,  1.84it/s]Loading train:  65%|██████▍   | 160/247 [01:26<00:47,  1.84it/s]Loading train:  65%|██████▌   | 161/247 [01:26<00:47,  1.82it/s]Loading train:  66%|██████▌   | 162/247 [01:27<00:46,  1.84it/s]Loading train:  66%|██████▌   | 163/247 [01:27<00:45,  1.84it/s]Loading train:  66%|██████▋   | 164/247 [01:28<00:45,  1.84it/s]Loading train:  67%|██████▋   | 165/247 [01:28<00:45,  1.82it/s]Loading train:  67%|██████▋   | 166/247 [01:29<00:44,  1.83it/s]Loading train:  68%|██████▊   | 167/247 [01:29<00:43,  1.83it/s]Loading train:  68%|██████▊   | 168/247 [01:30<00:43,  1.81it/s]Loading train:  68%|██████▊   | 169/247 [01:31<00:42,  1.84it/s]Loading train:  69%|██████▉   | 170/247 [01:31<00:41,  1.87it/s]Loading train:  69%|██████▉   | 171/247 [01:32<00:40,  1.87it/s]Loading train:  70%|██████▉   | 172/247 [01:32<00:45,  1.64it/s]Loading train:  70%|███████   | 173/247 [01:33<00:47,  1.55it/s]Loading train:  70%|███████   | 174/247 [01:34<00:48,  1.50it/s]Loading train:  71%|███████   | 175/247 [01:35<00:53,  1.35it/s]Loading train:  71%|███████▏  | 176/247 [01:35<00:47,  1.49it/s]Loading train:  72%|███████▏  | 177/247 [01:36<00:44,  1.58it/s]Loading train:  72%|███████▏  | 178/247 [01:36<00:42,  1.64it/s]Loading train:  72%|███████▏  | 179/247 [01:37<00:40,  1.69it/s]Loading train:  73%|███████▎  | 180/247 [01:37<00:38,  1.72it/s]Loading train:  73%|███████▎  | 181/247 [01:38<00:37,  1.75it/s]Loading train:  74%|███████▎  | 182/247 [01:39<00:36,  1.79it/s]Loading train:  74%|███████▍  | 183/247 [01:39<00:35,  1.78it/s]Loading train:  74%|███████▍  | 184/247 [01:40<00:35,  1.79it/s]Loading train:  75%|███████▍  | 185/247 [01:40<00:34,  1.81it/s]Loading train:  75%|███████▌  | 186/247 [01:41<00:33,  1.84it/s]Loading train:  76%|███████▌  | 187/247 [01:41<00:32,  1.82it/s]Loading train:  76%|███████▌  | 188/247 [01:42<00:32,  1.84it/s]Loading train:  77%|███████▋  | 189/247 [01:42<00:31,  1.85it/s]Loading train:  77%|███████▋  | 190/247 [01:43<00:30,  1.87it/s]Loading train:  77%|███████▋  | 191/247 [01:43<00:30,  1.84it/s]Loading train:  78%|███████▊  | 192/247 [01:44<00:29,  1.86it/s]Loading train:  78%|███████▊  | 193/247 [01:44<00:28,  1.88it/s]Loading train:  79%|███████▊  | 194/247 [01:45<00:27,  1.92it/s]Loading train:  79%|███████▉  | 195/247 [01:45<00:26,  1.94it/s]Loading train:  79%|███████▉  | 196/247 [01:46<00:26,  1.95it/s]Loading train:  80%|███████▉  | 197/247 [01:46<00:25,  1.95it/s]Loading train:  80%|████████  | 198/247 [01:47<00:25,  1.93it/s]Loading train:  81%|████████  | 199/247 [01:47<00:24,  1.96it/s]Loading train:  81%|████████  | 200/247 [01:48<00:24,  1.93it/s]Loading train:  81%|████████▏ | 201/247 [01:49<00:23,  1.94it/s]Loading train:  82%|████████▏ | 202/247 [01:49<00:23,  1.95it/s]Loading train:  82%|████████▏ | 203/247 [01:50<00:22,  1.94it/s]Loading train:  83%|████████▎ | 204/247 [01:50<00:22,  1.92it/s]Loading train:  83%|████████▎ | 205/247 [01:51<00:21,  1.96it/s]Loading train:  83%|████████▎ | 206/247 [01:51<00:20,  1.96it/s]Loading train:  84%|████████▍ | 207/247 [01:52<00:20,  1.97it/s]Loading train:  84%|████████▍ | 208/247 [01:52<00:19,  1.98it/s]Loading train:  85%|████████▍ | 209/247 [01:53<00:19,  1.93it/s]Loading train:  85%|████████▌ | 210/247 [01:53<00:19,  1.90it/s]Loading train:  85%|████████▌ | 211/247 [01:54<00:19,  1.89it/s]Loading train:  86%|████████▌ | 212/247 [01:54<00:18,  1.91it/s]Loading train:  86%|████████▌ | 213/247 [01:55<00:17,  1.91it/s]Loading train:  87%|████████▋ | 214/247 [01:55<00:17,  1.94it/s]Loading train:  87%|████████▋ | 215/247 [01:56<00:16,  1.94it/s]Loading train:  87%|████████▋ | 216/247 [01:56<00:15,  1.94it/s]Loading train:  88%|████████▊ | 217/247 [01:57<00:15,  1.95it/s]Loading train:  88%|████████▊ | 218/247 [01:57<00:14,  1.95it/s]Loading train:  89%|████████▊ | 219/247 [01:58<00:14,  1.95it/s]Loading train:  89%|████████▉ | 220/247 [01:58<00:13,  1.96it/s]Loading train:  89%|████████▉ | 221/247 [01:59<00:13,  1.95it/s]Loading train:  90%|████████▉ | 222/247 [01:59<00:12,  1.95it/s]Loading train:  90%|█████████ | 223/247 [02:00<00:12,  1.93it/s]Loading train:  91%|█████████ | 224/247 [02:00<00:11,  1.94it/s]Loading train:  91%|█████████ | 225/247 [02:01<00:11,  1.94it/s]Loading train:  91%|█████████▏| 226/247 [02:01<00:10,  1.95it/s]Loading train:  92%|█████████▏| 227/247 [02:02<00:10,  1.96it/s]Loading train:  92%|█████████▏| 228/247 [02:02<00:09,  1.93it/s]Loading train:  93%|█████████▎| 229/247 [02:03<00:09,  1.91it/s]Loading train:  93%|█████████▎| 230/247 [02:04<00:09,  1.81it/s]Loading train:  94%|█████████▎| 231/247 [02:04<00:08,  1.79it/s]Loading train:  94%|█████████▍| 232/247 [02:05<00:08,  1.78it/s]Loading train:  94%|█████████▍| 233/247 [02:05<00:08,  1.74it/s]Loading train:  95%|█████████▍| 234/247 [02:06<00:07,  1.71it/s]Loading train:  95%|█████████▌| 235/247 [02:07<00:07,  1.69it/s]Loading train:  96%|█████████▌| 236/247 [02:07<00:06,  1.68it/s]Loading train:  96%|█████████▌| 237/247 [02:08<00:05,  1.68it/s]Loading train:  96%|█████████▋| 238/247 [02:08<00:05,  1.68it/s]Loading train:  97%|█████████▋| 239/247 [02:09<00:04,  1.68it/s]Loading train:  97%|█████████▋| 240/247 [02:10<00:04,  1.67it/s]Loading train:  98%|█████████▊| 241/247 [02:10<00:03,  1.67it/s]Loading train:  98%|█████████▊| 242/247 [02:11<00:02,  1.68it/s]Loading train:  98%|█████████▊| 243/247 [02:11<00:02,  1.70it/s]Loading train:  99%|█████████▉| 244/247 [02:12<00:01,  1.69it/s]Loading train:  99%|█████████▉| 245/247 [02:13<00:01,  1.69it/s]Loading train: 100%|█████████▉| 246/247 [02:13<00:00,  1.69it/s]Loading train: 100%|██████████| 247/247 [02:14<00:00,  1.69it/s]Loading train: 100%|██████████| 247/247 [02:14<00:00,  1.84it/s]
concatenating: train:   0%|          | 0/247 [00:00<?, ?it/s]concatenating: train:   3%|▎         | 7/247 [00:00<00:03, 62.25it/s]concatenating: train:   5%|▌         | 13/247 [00:00<00:03, 60.82it/s]concatenating: train:   8%|▊         | 20/247 [00:00<00:03, 61.30it/s]concatenating: train:  11%|█         | 27/247 [00:00<00:03, 61.65it/s]concatenating: train:  14%|█▍        | 34/247 [00:00<00:03, 62.16it/s]concatenating: train:  17%|█▋        | 41/247 [00:00<00:03, 62.57it/s]concatenating: train:  19%|█▉        | 48/247 [00:00<00:03, 62.99it/s]concatenating: train:  22%|██▏       | 55/247 [00:00<00:03, 63.17it/s]concatenating: train:  25%|██▍       | 61/247 [00:00<00:03, 61.69it/s]concatenating: train:  27%|██▋       | 67/247 [00:01<00:03, 59.31it/s]concatenating: train:  30%|██▉       | 73/247 [00:01<00:02, 58.73it/s]concatenating: train:  32%|███▏      | 79/247 [00:01<00:02, 57.74it/s]concatenating: train:  34%|███▍      | 85/247 [00:01<00:02, 58.23it/s]concatenating: train:  37%|███▋      | 91/247 [00:01<00:02, 56.37it/s]concatenating: train:  39%|███▉      | 97/247 [00:01<00:02, 56.17it/s]concatenating: train:  42%|████▏     | 104/247 [00:01<00:02, 57.19it/s]concatenating: train:  45%|████▍     | 111/247 [00:01<00:02, 58.13it/s]concatenating: train:  47%|████▋     | 117/247 [00:01<00:02, 57.66it/s]concatenating: train:  50%|████▉     | 123/247 [00:02<00:02, 55.92it/s]concatenating: train:  52%|█████▏    | 129/247 [00:02<00:02, 55.69it/s]concatenating: train:  55%|█████▍    | 135/247 [00:02<00:02, 54.77it/s]concatenating: train:  57%|█████▋    | 142/247 [00:02<00:01, 56.75it/s]concatenating: train:  60%|█████▉    | 148/247 [00:02<00:01, 57.55it/s]concatenating: train:  63%|██████▎   | 155/247 [00:02<00:01, 58.58it/s]concatenating: train:  66%|██████▌   | 162/247 [00:02<00:01, 59.88it/s]concatenating: train:  68%|██████▊   | 169/247 [00:02<00:01, 61.34it/s]concatenating: train:  71%|███████▏  | 176/247 [00:02<00:01, 61.87it/s]concatenating: train:  74%|███████▍  | 183/247 [00:03<00:01, 60.16it/s]concatenating: train:  77%|███████▋  | 190/247 [00:03<00:00, 58.94it/s]concatenating: train:  79%|███████▉  | 196/247 [00:03<00:00, 58.29it/s]concatenating: train:  82%|████████▏ | 202/247 [00:03<00:00, 56.67it/s]concatenating: train:  84%|████████▍ | 208/247 [00:03<00:00, 56.84it/s]concatenating: train:  87%|████████▋ | 214/247 [00:03<00:00, 53.10it/s]concatenating: train:  89%|████████▉ | 220/247 [00:03<00:00, 52.17it/s]concatenating: train:  91%|█████████▏| 226/247 [00:03<00:00, 51.11it/s]concatenating: train:  94%|█████████▍| 232/247 [00:04<00:00, 49.44it/s]concatenating: train:  96%|█████████▌| 237/247 [00:04<00:00, 47.52it/s]concatenating: train:  98%|█████████▊| 242/247 [00:04<00:00, 47.88it/s]concatenating: train: 100%|██████████| 247/247 [00:04<00:00, 56.89it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:00<00:03,  1.17it/s]Loading test:  40%|████      | 2/5 [00:01<00:02,  1.15it/s]Loading test:  60%|██████    | 3/5 [00:02<00:01,  1.17it/s]Loading test:  80%|████████  | 4/5 [00:03<00:00,  1.24it/s]Loading test: 100%|██████████| 5/5 [00:04<00:00,  1.25it/s]Loading test: 100%|██████████| 5/5 [00:04<00:00,  1.23it/s]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 70.58it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 0  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 0  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 0  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 84, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 84, 52, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 84, 52, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 84, 52, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 84, 52, 40)   14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 84, 52, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 84, 52, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 42, 26, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 42, 26, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 42, 26, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 42, 26, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 42, 26, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 42, 26, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 42, 26, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 42, 26, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 42, 26, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 21, 13, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 21, 13, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 21, 13, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 21, 13, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 21, 13, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 21, 13, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 21, 13, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 21, 13, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 21, 13, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 21, 13, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 42, 26, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 42, 26, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 42, 26, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 42, 26, 80)   320         conv2d_7[0][0]                   2020-01-22 12:12:30.023202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 12:12:30.023294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 12:12:30.023309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 12:12:30.023317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 12:12:30.023585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
activation_7 (Activation)       (None, 42, 26, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 42, 26, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 42, 26, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 42, 26, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 42, 26, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 42, 26, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 84, 52, 40)   44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 84, 52, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 84, 52, 40)   28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 84, 52, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 84, 52, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 84, 52, 40)   14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 84, 52, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 84, 52, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 84, 52, 120)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 84, 52, 120)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 84, 52, 13)   1573        dropout_5[0][0]                  
==================================================================================================
Total params: 889,253
Trainable params: 887,653
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0
------------------------------------------------------------------
class_weights [6.52845920e-02 3.19646075e-02 7.71324108e-02 9.61661259e-03
 2.75331668e-02 7.05168399e-03 8.86226817e-02 1.14504684e-01
 8.20487847e-02 1.27672626e-02 2.89799896e-01 1.93445059e-01
 2.28558183e-04]
Train on 9728 samples, validate on 191 samples
Epoch 1/300
 - 27s - loss: 0.6909 - acc: 0.9096 - mDice: 0.2549 - val_loss: 0.5177 - val_acc: 0.9360 - val_mDice: 0.1891

Epoch 00001: val_mDice improved from -inf to 0.18909, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/300
 - 23s - loss: 0.5713 - acc: 0.9309 - mDice: 0.3835 - val_loss: 0.3109 - val_acc: 0.9410 - val_mDice: 0.2226

Epoch 00002: val_mDice improved from 0.18909 to 0.22265, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 3/300
 - 23s - loss: 0.5310 - acc: 0.9361 - mDice: 0.4270 - val_loss: 0.2815 - val_acc: 0.9423 - val_mDice: 0.2241

Epoch 00003: val_mDice improved from 0.22265 to 0.22413, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/300
 - 23s - loss: 0.5170 - acc: 0.9388 - mDice: 0.4420 - val_loss: 0.2429 - val_acc: 0.9444 - val_mDice: 0.2340

Epoch 00004: val_mDice improved from 0.22413 to 0.23404, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 5/300
 - 23s - loss: 0.5043 - acc: 0.9406 - mDice: 0.4557 - val_loss: 0.2410 - val_acc: 0.9461 - val_mDice: 0.2366

Epoch 00005: val_mDice improved from 0.23404 to 0.23659, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 6/300
 - 23s - loss: 0.4977 - acc: 0.9419 - mDice: 0.4628 - val_loss: 0.2338 - val_acc: 0.9473 - val_mDice: 0.2352

Epoch 00006: val_mDice did not improve from 0.23659
Epoch 7/300
 - 23s - loss: 0.4864 - acc: 0.9432 - mDice: 0.4750 - val_loss: 0.2686 - val_acc: 0.9475 - val_mDice: 0.2399

Epoch 00007: val_mDice improved from 0.23659 to 0.23992, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 8/300
 - 23s - loss: 0.4803 - acc: 0.9442 - mDice: 0.4816 - val_loss: 0.2247 - val_acc: 0.9473 - val_mDice: 0.2363

Epoch 00008: val_mDice did not improve from 0.23992
Epoch 9/300
 - 23s - loss: 0.4765 - acc: 0.9448 - mDice: 0.4857 - val_loss: 0.2398 - val_acc: 0.9477 - val_mDice: 0.2409

Epoch 00009: val_mDice improved from 0.23992 to 0.24091, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 10/300
 - 23s - loss: 0.4741 - acc: 0.9456 - mDice: 0.4883 - val_loss: 0.2552 - val_acc: 0.9482 - val_mDice: 0.2380

Epoch 00010: val_mDice did not improve from 0.24091
Epoch 11/300
 - 23s - loss: 0.4698 - acc: 0.9461 - mDice: 0.4929 - val_loss: 0.2640 - val_acc: 0.9489 - val_mDice: 0.2421

Epoch 00011: val_mDice improved from 0.24091 to 0.24208, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 12/300
 - 23s - loss: 0.4591 - acc: 0.9466 - mDice: 0.5045 - val_loss: 0.2638 - val_acc: 0.9483 - val_mDice: 0.2493

Epoch 00012: val_mDice improved from 0.24208 to 0.24925, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 13/300
 - 23s - loss: 0.4435 - acc: 0.9467 - mDice: 0.5214 - val_loss: 0.2316 - val_acc: 0.9495 - val_mDice: 0.2703

Epoch 00013: val_mDice improved from 0.24925 to 0.27027, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 14/300
 - 23s - loss: 0.4155 - acc: 0.9473 - mDice: 0.5517 - val_loss: 0.1629 - val_acc: 0.9495 - val_mDice: 0.2757

Epoch 00014: val_mDice improved from 0.27027 to 0.27568, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 15/300
 - 23s - loss: 0.4057 - acc: 0.9479 - mDice: 0.5623 - val_loss: 0.1922 - val_acc: 0.9497 - val_mDice: 0.2731

Epoch 00015: val_mDice did not improve from 0.27568
Epoch 16/300
 - 23s - loss: 0.3985 - acc: 0.9483 - mDice: 0.5701 - val_loss: 0.1791 - val_acc: 0.9502 - val_mDice: 0.2799

Epoch 00016: val_mDice improved from 0.27568 to 0.27987, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 17/300
 - 23s - loss: 0.3948 - acc: 0.9485 - mDice: 0.5741 - val_loss: 0.1685 - val_acc: 0.9505 - val_mDice: 0.2763

Epoch 00017: val_mDice did not improve from 0.27987
Epoch 18/300
 - 23s - loss: 0.3844 - acc: 0.9490 - mDice: 0.5853 - val_loss: 0.1646 - val_acc: 0.9508 - val_mDice: 0.2839

Epoch 00018: val_mDice improved from 0.27987 to 0.28387, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 19/300
 - 23s - loss: 0.3837 - acc: 0.9494 - mDice: 0.5861 - val_loss: 0.1867 - val_acc: 0.9506 - val_mDice: 0.2806

Epoch 00019: val_mDice did not improve from 0.28387
Epoch 20/300
 - 23s - loss: 0.3814 - acc: 0.9496 - mDice: 0.5886 - val_loss: 0.2055 - val_acc: 0.9510 - val_mDice: 0.2809

Epoch 00020: val_mDice did not improve from 0.28387
Epoch 21/300
 - 23s - loss: 0.3731 - acc: 0.9500 - mDice: 0.5975 - val_loss: 0.1935 - val_acc: 0.9511 - val_mDice: 0.2879

Epoch 00021: val_mDice improved from 0.28387 to 0.28791, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 22/300
 - 23s - loss: 0.3743 - acc: 0.9501 - mDice: 0.5963 - val_loss: 0.1605 - val_acc: 0.9505 - val_mDice: 0.2807

Epoch 00022: val_mDice did not improve from 0.28791
Epoch 23/300
 - 23s - loss: 0.3698 - acc: 0.9503 - mDice: 0.6011 - val_loss: 0.2014 - val_acc: 0.9518 - val_mDice: 0.2837

Epoch 00023: val_mDice did not improve from 0.28791
Epoch 24/300
 - 23s - loss: 0.3662 - acc: 0.9506 - mDice: 0.6050 - val_loss: 0.2177 - val_acc: 0.9510 - val_mDice: 0.2838

Epoch 00024: val_mDice did not improve from 0.28791
Epoch 25/300
 - 23s - loss: 0.3673 - acc: 0.9506 - mDice: 0.6038 - val_loss: 0.2163 - val_acc: 0.9509 - val_mDice: 0.2819

Epoch 00025: val_mDice did not improve from 0.28791
Epoch 26/300
 - 23s - loss: 0.3644 - acc: 0.9511 - mDice: 0.6069 - val_loss: 0.1477 - val_acc: 0.9511 - val_mDice: 0.2810

Epoch 00026: val_mDice did not improve from 0.28791
Epoch 27/300
 - 23s - loss: 0.3620 - acc: 0.9512 - mDice: 0.6096 - val_loss: 0.1781 - val_acc: 0.9520 - val_mDice: 0.2813

Epoch 00027: val_mDice did not improve from 0.28791
Epoch 28/300
 - 23s - loss: 0.3600 - acc: 0.9513 - mDice: 0.6117 - val_loss: 0.1728 - val_acc: 0.9512 - val_mDice: 0.2826

Epoch 00028: val_mDice did not improve from 0.28791
Epoch 29/300
 - 23s - loss: 0.3572 - acc: 0.9517 - mDice: 0.6147 - val_loss: 0.1906 - val_acc: 0.9513 - val_mDice: 0.2795

Epoch 00029: val_mDice did not improve from 0.28791
Epoch 30/300
 - 23s - loss: 0.3570 - acc: 0.9518 - mDice: 0.6149 - val_loss: 0.2478 - val_acc: 0.9515 - val_mDice: 0.2792

Epoch 00030: val_mDice did not improve from 0.28791
Epoch 31/300
 - 23s - loss: 0.3546 - acc: 0.9518 - mDice: 0.6175 - val_loss: 0.1325 - val_acc: 0.9513 - val_mDice: 0.2827

Epoch 00031: val_mDice did not improve from 0.28791
Epoch 32/300
 - 23s - loss: 0.3529 - acc: 0.9521 - mDice: 0.6194 - val_loss: 0.2319 - val_acc: 0.9514 - val_mDice: 0.2788

Epoch 00032: val_mDice did not improve from 0.28791
Epoch 33/300
 - 23s - loss: 0.3540 - acc: 0.9521 - mDice: 0.6182 - val_loss: 0.1493 - val_acc: 0.9511 - val_mDice: 0.2791

Epoch 00033: val_mDice did not improve from 0.28791
Epoch 34/300
 - 23s - loss: 0.3487 - acc: 0.9524 - mDice: 0.6239 - val_loss: 0.1634 - val_acc: 0.9511 - val_mDice: 0.2771

Epoch 00034: val_mDice did not improve from 0.28791
Epoch 35/300
 - 23s - loss: 0.3514 - acc: 0.9525 - mDice: 0.6210 - val_loss: 0.2222 - val_acc: 0.9522 - val_mDice: 0.2844

Epoch 00035: val_mDice did not improve from 0.28791
Epoch 36/300
 - 23s - loss: 0.3506 - acc: 0.9526 - mDice: 0.6219 - val_loss: 0.2057 - val_acc: 0.9513 - val_mDice: 0.2742

Epoch 00036: val_mDice did not improve from 0.28791

Epoch 00036: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 37/300
 - 23s - loss: 0.3434 - acc: 0.9530 - mDice: 0.6296 - val_loss: 0.2216 - val_acc: 0.9519 - val_mDice: 0.2797

Epoch 00037: val_mDice did not improve from 0.28791
Epoch 38/300
 - 23s - loss: 0.3454 - acc: 0.9530 - mDice: 0.6275 - val_loss: 0.1906 - val_acc: 0.9520 - val_mDice: 0.2807

Epoch 00038: val_mDice did not improve from 0.28791
Epoch 39/300
 - 23s - loss: 0.3431 - acc: 0.9533 - mDice: 0.6300 - val_loss: 0.1724 - val_acc: 0.9517 - val_mDice: 0.2782

Epoch 00039: val_mDice did not improve from 0.28791
Epoch 40/300
 - 24s - loss: 0.3396 - acc: 0.9533 - mDice: 0.6338 - val_loss: 0.2355 - val_acc: 0.9518 - val_mDice: 0.2782

Epoch 00040: val_mDice did not improve from 0.28791
Epoch 41/300
 - 23s - loss: 0.3429 - acc: 0.9533 - mDice: 0.6302 - val_loss: 0.2162 - val_acc: 0.9517 - val_mDice: 0.2797

Epoch 00041: val_mDice did not improve from 0.28791
Epoch 42/300
 - 23s - loss: 0.3375 - acc: 0.9535 - mDice: 0.6360 - val_loss: 0.1766 - val_acc: 0.9523 - val_mDice: 0.2805

Epoch 00042: val_mDice did not improve from 0.28791
Epoch 43/300
 - 23s - loss: 0.3393 - acc: 0.9535 - mDice: 0.6341 - val_loss: 0.1984 - val_acc: 0.9518 - val_mDice: 0.2776

Epoch 00043: val_mDice did not improve from 0.28791
Epoch 44/300
 - 23s - loss: 0.3396 - acc: 0.9535 - mDice: 0.6337 - val_loss: 0.1808 - val_acc: 0.9521 - val_mDice: 0.2781

Epoch 00044: val_mDice did not improve from 0.28791
Epoch 45/300
 - 23s - loss: 0.3361 - acc: 0.9537 - mDice: 0.6375 - val_loss: 0.2162 - val_acc: 0.9518 - val_mDice: 0.2791

Epoch 00045: val_mDice did not improve from 0.28791
Epoch 46/300
 - 23s - loss: 0.3370 - acc: 0.9537 - mDice: 0.6366 - val_loss: 0.1993 - val_acc: 0.9517 - val_mDice: 0.2764

Epoch 00046: val_mDice did not improve from 0.28791
Epoch 47/300
 - 23s - loss: 0.3397 - acc: 0.9538 - mDice: 0.6336 - val_loss: 0.1982 - val_acc: 0.9517 - val_mDice: 0.2805

Epoch 00047: val_mDice did not improve from 0.28791
Epoch 48/300
 - 23s - loss: 0.3349 - acc: 0.9538 - mDice: 0.6388 - val_loss: 0.2137 - val_acc: 0.9516 - val_mDice: 0.2790

Epoch 00048: val_mDice did not improve from 0.28791
Epoch 49/300
 - 23s - loss: 0.3377 - acc: 0.9540 - mDice: 0.6358 - val_loss: 0.1642 - val_acc: 0.9523 - val_mDice: 0.2828

Epoch 00049: val_mDice did not improve from 0.28791
Epoch 50/300
 - 23s - loss: 0.3352 - acc: 0.9540 - mDice: 0.6384 - val_loss: 0.1791 - val_acc: 0.9518 - val_mDice: 0.2789

Epoch 00050: val_mDice did not improve from 0.28791
Epoch 51/300
 - 23s - loss: 0.3332 - acc: 0.9540 - mDice: 0.6407 - val_loss: 0.1929 - val_acc: 0.9521 - val_mDice: 0.2793

Epoch 00051: val_mDice did not improve from 0.28791

Epoch 00051: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 52/300
 - 23s - loss: 0.3362 - acc: 0.9541 - mDice: 0.6373 - val_loss: 0.1955 - val_acc: 0.9519 - val_mDice: 0.2779

Epoch 00052: val_mDice did not improve from 0.28791
Epoch 53/300
 - 23s - loss: 0.3340 - acc: 0.9542 - mDice: 0.6398 - val_loss: 0.1878 - val_acc: 0.9520 - val_mDice: 0.2788

Epoch 00053: val_mDice did not improve from 0.28791
Epoch 54/300
 - 23s - loss: 0.3320 - acc: 0.9542 - mDice: 0.6419 - val_loss: 0.2111 - val_acc: 0.9520 - val_mDice: 0.2795

Epoch 00054: val_mDice did not improve from 0.28791
Epoch 55/300
 - 23s - loss: 0.3324 - acc: 0.9542 - mDice: 0.6415 - val_loss: 0.2064 - val_acc: 0.9519 - val_mDice: 0.2778

Epoch 00055: val_mDice did not improve from 0.28791
Epoch 56/300
 - 23s - loss: 0.3317 - acc: 0.9543 - mDice: 0.6423 - val_loss: 0.2078 - val_acc: 0.9522 - val_mDice: 0.2802

Epoch 00056: val_mDice did not improve from 0.28791
Epoch 57/300
 - 23s - loss: 0.3342 - acc: 0.9543 - mDice: 0.6395 - val_loss: 0.2052 - val_acc: 0.9522 - val_mDice: 0.2766

Epoch 00057: val_mDice did not improve from 0.28791
Epoch 58/300
 - 23s - loss: 0.3309 - acc: 0.9544 - mDice: 0.6432 - val_loss: 0.1980 - val_acc: 0.9522 - val_mDice: 0.2759

Epoch 00058: val_mDice did not improve from 0.28791
Epoch 59/300
 - 23s - loss: 0.3332 - acc: 0.9544 - mDice: 0.6406 - val_loss: 0.2049 - val_acc: 0.9523 - val_mDice: 0.2776

Epoch 00059: val_mDice did not improve from 0.28791
Epoch 60/300
 - 23s - loss: 0.3325 - acc: 0.9544 - mDice: 0.6413 - val_loss: 0.2390 - val_acc: 0.9522 - val_mDice: 0.2783

Epoch 00060: val_mDice did not improve from 0.28791
Epoch 61/300
 - 23s - loss: 0.3288 - acc: 0.9544 - mDice: 0.6454 - val_loss: 0.2405 - val_acc: 0.9521 - val_mDice: 0.2778

predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:01<00:05,  1.41s/it]predicting test subjects:  40%|████      | 2/5 [00:02<00:03,  1.30s/it]predicting test subjects:  60%|██████    | 3/5 [00:03<00:02,  1.22s/it]predicting test subjects:  80%|████████  | 4/5 [00:04<00:01,  1.09s/it]predicting test subjects: 100%|██████████| 5/5 [00:05<00:00,  1.02s/it]predicting test subjects: 100%|██████████| 5/5 [00:05<00:00,  1.03s/it]
Epoch 00061: val_mDice did not improve from 0.28791
Restoring model weights from the end of the best epoch
Epoch 00061: early stopping
{'val_loss': [0.5177149789807685, 0.3109384652480717, 0.28147586059624924, 0.24290112693701427, 0.24095202488971007, 0.23382725237980875, 0.2686340220233533, 0.2246645120660048, 0.2397774285905024, 0.25517796502687545, 0.2639573006573772, 0.2637759922998738, 0.23157825551107916, 0.1629420099146079, 0.19220033401126013, 0.17912383334642928, 0.1684592551507875, 0.1645707828797283, 0.18666464388760598, 0.20553447680635603, 0.1934558491159172, 0.1605406402566358, 0.2013528363447617, 0.21773899912210035, 0.21629769222430534, 0.14766268788090864, 0.17811090967261978, 0.17284613102674484, 0.19058933670764194, 0.24781750703805872, 0.13252814988854356, 0.23187448644123151, 0.1493405450369519, 0.1634362667642963, 0.22216870671342961, 0.20567448366999003, 0.22164031925747726, 0.19058748595026462, 0.172437146462071, 0.2355178787485155, 0.2161855545919358, 0.1766042758501013, 0.19838019399976856, 0.18079100424394556, 0.216234253573168, 0.19925449517950494, 0.19818460542942684, 0.21374256623157967, 0.1641500780251638, 0.17914825260483158, 0.19287804633144934, 0.19554835573540932, 0.18779893252628013, 0.2110994426699365, 0.20644989139668604, 0.20776632342351045, 0.20516026918215588, 0.19803545070100206, 0.2049396241483576, 0.23895475285917678, 0.24052409312799963], 'val_acc': [0.9360161027359089, 0.9409652324247111, 0.9422633404507063, 0.9444268673502337, 0.9460569982129242, 0.9472831874617731, 0.9474785689908173, 0.94731435663413, 0.9477470613899032, 0.9481713746854772, 0.9488677753827959, 0.948294831196051, 0.9495018470973868, 0.9495306164806426, 0.949739179686102, 0.950188661120949, 0.9504715397719937, 0.9507939634522843, 0.9506345537944614, 0.9510241060356819, 0.9510660570953529, 0.9505398635464813, 0.9518188013456255, 0.9509725683022544, 0.9508706849282949, 0.9511032148181456, 0.9519578408820467, 0.9511619519188766, 0.9513465390779585, 0.951456810791455, 0.9512878113392136, 0.9514484224519181, 0.9511319810807393, 0.95105767499714, 0.9522095394384175, 0.9512590357146338, 0.9519026972236434, 0.9519710209981309, 0.9516677737860155, 0.9518403713615777, 0.9516893434899015, 0.9523150240563597, 0.9517516759053575, 0.9521424171188115, 0.951803213638785, 0.9516761590048904, 0.9517384879876182, 0.9516342017038955, 0.952328205732775, 0.9517660527953302, 0.9520741120682961, 0.9518703453203771, 0.9519877945565428, 0.9520165639397986, 0.9518859205445694, 0.9521795779622663, 0.9521640058587358, 0.952191570666448, 0.9522778694542291, 0.9522191417154842, 0.9520717029172089], 'val_mDice': [0.18909417083058175, 0.22264524221577042, 0.22413262863356725, 0.23403936811767534, 0.2365943841011508, 0.23518635024650308, 0.2399243719555099, 0.2362757018055396, 0.2409146084615155, 0.2379526515096348, 0.24208220090053936, 0.24925344198191274, 0.27026698027410745, 0.2756814371930991, 0.27306355009372324, 0.2798744854736703, 0.2762842392890241, 0.28387160669446615, 0.28064425137498616, 0.2809080683397061, 0.28790830044534194, 0.28067065198071967, 0.2836827547606373, 0.28383776106681496, 0.2819156699389687, 0.2809868880708492, 0.2812657769554885, 0.28257432876457095, 0.2795448841731898, 0.2791561480826108, 0.2827372035274955, 0.27879517496178285, 0.2790769438273932, 0.27706325005408355, 0.28440480668058726, 0.274194664485187, 0.2797093546688839, 0.2807207606460439, 0.2781971684457212, 0.2781501127069673, 0.27970701734537223, 0.2805442345543009, 0.277594124535488, 0.2780817155559782, 0.279093811107258, 0.2763950198655325, 0.28053690430839123, 0.2789954575880719, 0.2827831236332286, 0.27893525806709824, 0.27932861429165073, 0.2778714800024794, 0.2787562331065282, 0.2794834764830339, 0.2778331784330602, 0.28018797585920757, 0.2765972969744573, 0.2758973404674565, 0.2775673806692927, 0.2782509725485869, 0.2778394229596772], 'loss': [0.6909135587047786, 0.5712765556343488, 0.5309735121401516, 0.5170307752835613, 0.5042859323531095, 0.49768091599138353, 0.4863984294689113, 0.48027715528702464, 0.47650762665771734, 0.4740921607559645, 0.46984379911354107, 0.4591301578942588, 0.44351922848114844, 0.41545744182076305, 0.4057417267038928, 0.3985228528954873, 0.3947904713047472, 0.38443040095980424, 0.38369017531237515, 0.38137028969848824, 0.37314248687930796, 0.3742754355158755, 0.36980391050517364, 0.36617047169956524, 0.36728048268215435, 0.36439338934264687, 0.361961245126287, 0.3599652242234075, 0.35722356703205915, 0.3570460312583141, 0.3546082369958688, 0.35290711012203246, 0.3539946195792015, 0.3487302446554072, 0.35136749322746735, 0.3505841175792739, 0.34338134376785573, 0.3453503062850551, 0.3430825303290284, 0.3395618492810938, 0.34287166008787034, 0.33747017340685587, 0.33927390934580837, 0.3396270914910067, 0.33607526581832453, 0.33696242517754926, 0.3396709498054789, 0.33487145487819553, 0.33767219829238265, 0.33522733860330556, 0.3331702060164197, 0.336238452478459, 0.33397952736796516, 0.33203739126666304, 0.3323576624113086, 0.331669133505784, 0.33420562045035984, 0.3308538399777669, 0.3332151921012586, 0.3325375105732268, 0.32879406959431146], 'acc': [0.9096494480648911, 0.9309456454540946, 0.9360980859001804, 0.9387785258555883, 0.9406383394410736, 0.9419236195175663, 0.9432218898773977, 0.9442007595085, 0.944788118738583, 0.9455540046635035, 0.9461147001282754, 0.946649508648797, 0.9467335471050128, 0.9473052565130944, 0.9479306690321353, 0.9482740522362292, 0.9484775733496797, 0.949032974098564, 0.9493772973380002, 0.9496406426555232, 0.9499906144399, 0.9501166617938954, 0.9502992151109012, 0.9505793627743658, 0.950598283487029, 0.9510990137136296, 0.9511833131117257, 0.9513488261176175, 0.9516600618678096, 0.9517716830841413, 0.9518342832912152, 0.9520745405829266, 0.9521236565748328, 0.9523971901324234, 0.9524843606877288, 0.9526428599278197, 0.952953768073042, 0.9530133789552278, 0.9533236027254086, 0.9532555184008455, 0.953346994644227, 0.9534616757331318, 0.9534616762723186, 0.9534628997850967, 0.9536557594240692, 0.9536836241175862, 0.9537767470863304, 0.9538027292460596, 0.9539584529610645, 0.9539512985042835, 0.9540484933378665, 0.9541372174862772, 0.9541908495961443, 0.9541718344522738, 0.9542203383201635, 0.9543185450357238, 0.9543420552080008, 0.9543591173929408, 0.9544367082417011, 0.9544149874954632, 0.9543965132907033], 'mDice': [0.25492798916131604, 0.38347227407594847, 0.4269521600738364, 0.4419728890735362, 0.455732284803066, 0.4628479050527523, 0.47503702297475875, 0.48163474204116746, 0.485701855311881, 0.48829907979693654, 0.49288615720395584, 0.5044646450244871, 0.5213740883781085, 0.5517470581506035, 0.5622524132608975, 0.5700553161533255, 0.5740992398963212, 0.5853063988370674, 0.5860982673414248, 0.5886035789521166, 0.597505410171212, 0.5962745860513103, 0.601116536417976, 0.6050396311390949, 0.6038307681140539, 0.6069415100470283, 0.6095796889613235, 0.6117384729257441, 0.6146941986486414, 0.6148874176157589, 0.617526642080568, 0.6193575498611225, 0.6181826268347275, 0.6238709139977704, 0.6210085365731318, 0.6218565706267806, 0.6296408186531871, 0.627509523088137, 0.6299519407575166, 0.6337713371735605, 0.6301842573233635, 0.6360251739020705, 0.6340753400673795, 0.6336918698047826, 0.6375339477580335, 0.6365702837141917, 0.6336374606716594, 0.6388370945167384, 0.6357951979796206, 0.6384403172261571, 0.6406659024988154, 0.6373421908080529, 0.6397851957437141, 0.6418847479722708, 0.641536787936562, 0.6422799153645572, 0.6395348246319612, 0.6431640106817021, 0.6406016920778999, 0.6413385382559347, 0.6453917563837757], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05]}
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM
Loading train:   0%|          | 0/247 [00:00<?, ?it/s]Loading train:   0%|          | 1/247 [00:00<01:14,  3.30it/s]Loading train:   1%|          | 2/247 [00:00<01:12,  3.38it/s]Loading train:   1%|          | 3/247 [00:00<01:09,  3.50it/s]Loading train:   2%|▏         | 4/247 [00:01<01:10,  3.47it/s]Loading train:   2%|▏         | 5/247 [00:01<01:09,  3.47it/s]Loading train:   2%|▏         | 6/247 [00:01<01:08,  3.51it/s]Loading train:   3%|▎         | 7/247 [00:01<01:07,  3.55it/s]Loading train:   3%|▎         | 8/247 [00:02<01:07,  3.56it/s]Loading train:   4%|▎         | 9/247 [00:02<01:06,  3.58it/s]Loading train:   4%|▍         | 10/247 [00:02<01:05,  3.59it/s]Loading train:   4%|▍         | 11/247 [00:03<01:05,  3.60it/s]Loading train:   5%|▍         | 12/247 [00:03<01:05,  3.61it/s]Loading train:   5%|▌         | 13/247 [00:03<01:04,  3.64it/s]Loading train:   6%|▌         | 14/247 [00:03<01:03,  3.66it/s]Loading train:   6%|▌         | 15/247 [00:04<01:03,  3.66it/s]Loading train:   6%|▋         | 16/247 [00:04<01:03,  3.65it/s]Loading train:   7%|▋         | 17/247 [00:04<01:03,  3.65it/s]Loading train:   7%|▋         | 18/247 [00:05<01:03,  3.63it/s]Loading train:   8%|▊         | 19/247 [00:05<01:02,  3.66it/s]Loading train:   8%|▊         | 20/247 [00:05<01:02,  3.64it/s]Loading train:   9%|▊         | 21/247 [00:05<01:02,  3.63it/s]Loading train:   9%|▉         | 22/247 [00:06<01:01,  3.63it/s]Loading train:   9%|▉         | 23/247 [00:06<00:59,  3.76it/s]Loading train:  10%|▉         | 24/247 [00:06<00:57,  3.87it/s]Loading train:  10%|█         | 25/247 [00:06<00:56,  3.94it/s]Loading train:  11%|█         | 26/247 [00:07<00:56,  3.94it/s]Loading train:  11%|█         | 27/247 [00:07<00:55,  3.99it/s]Loading train:  11%|█▏        | 28/247 [00:07<00:54,  4.03it/s]Loading train:  12%|█▏        | 29/247 [00:07<00:53,  4.06it/s]Loading train:  12%|█▏        | 30/247 [00:08<00:53,  4.04it/s]Loading train:  13%|█▎        | 31/247 [00:08<00:53,  4.07it/s]Loading train:  13%|█▎        | 32/247 [00:08<00:52,  4.08it/s]Loading train:  13%|█▎        | 33/247 [00:08<00:52,  4.09it/s]Loading train:  14%|█▍        | 34/247 [00:09<00:51,  4.10it/s]Loading train:  14%|█▍        | 35/247 [00:09<00:51,  4.11it/s]Loading train:  15%|█▍        | 36/247 [00:09<00:51,  4.11it/s]Loading train:  15%|█▍        | 37/247 [00:09<00:50,  4.12it/s]Loading train:  15%|█▌        | 38/247 [00:10<00:50,  4.11it/s]Loading train:  16%|█▌        | 39/247 [00:10<00:50,  4.10it/s]Loading train:  16%|█▌        | 40/247 [00:10<00:50,  4.10it/s]Loading train:  17%|█▋        | 41/247 [00:10<00:50,  4.06it/s]Loading train:  17%|█▋        | 42/247 [00:10<00:50,  4.04it/s]Loading train:  17%|█▋        | 43/247 [00:11<00:50,  4.02it/s]Loading train:  18%|█▊        | 44/247 [00:11<00:50,  4.01it/s]Loading train:  18%|█▊        | 45/247 [00:11<00:50,  3.99it/s]Loading train:  19%|█▊        | 46/247 [00:12<00:51,  3.93it/s]Loading train:  19%|█▉        | 47/247 [00:12<00:50,  3.94it/s]Loading train:  19%|█▉        | 48/247 [00:12<00:50,  3.96it/s]Loading train:  20%|█▉        | 49/247 [00:12<00:49,  3.96it/s]Loading train:  20%|██        | 50/247 [00:13<00:49,  3.96it/s]Loading train:  21%|██        | 51/247 [00:13<00:49,  3.95it/s]Loading train:  21%|██        | 52/247 [00:13<00:49,  3.96it/s]Loading train:  21%|██▏       | 53/247 [00:13<00:48,  3.98it/s]Loading train:  22%|██▏       | 54/247 [00:14<00:48,  3.97it/s]Loading train:  22%|██▏       | 55/247 [00:14<00:48,  3.96it/s]Loading train:  23%|██▎       | 56/247 [00:14<00:48,  3.96it/s]Loading train:  23%|██▎       | 57/247 [00:14<00:48,  3.94it/s]Loading train:  23%|██▎       | 58/247 [00:15<00:47,  3.94it/s]Loading train:  24%|██▍       | 59/247 [00:15<00:48,  3.86it/s]Loading train:  24%|██▍       | 60/247 [00:15<00:49,  3.81it/s]Loading train:  25%|██▍       | 61/247 [00:15<00:49,  3.77it/s]Loading train:  25%|██▌       | 62/247 [00:16<00:49,  3.74it/s]Loading train:  26%|██▌       | 63/247 [00:16<00:49,  3.72it/s]Loading train:  26%|██▌       | 64/247 [00:16<00:49,  3.70it/s]Loading train:  26%|██▋       | 65/247 [00:16<00:49,  3.69it/s]Loading train:  27%|██▋       | 66/247 [00:17<00:49,  3.68it/s]Loading train:  27%|██▋       | 67/247 [00:17<00:49,  3.67it/s]Loading train:  28%|██▊       | 68/247 [00:17<00:48,  3.67it/s]Loading train:  28%|██▊       | 69/247 [00:18<00:48,  3.67it/s]Loading train:  28%|██▊       | 70/247 [00:18<00:48,  3.68it/s]Loading train:  29%|██▊       | 71/247 [00:18<00:48,  3.67it/s]Loading train:  29%|██▉       | 72/247 [00:18<00:47,  3.66it/s]Loading train:  30%|██▉       | 73/247 [00:19<00:47,  3.66it/s]Loading train:  30%|██▉       | 74/247 [00:19<00:47,  3.66it/s]Loading train:  30%|███       | 75/247 [00:19<00:46,  3.68it/s]Loading train:  31%|███       | 76/247 [00:19<00:46,  3.68it/s]Loading train:  31%|███       | 77/247 [00:20<00:49,  3.45it/s]Loading train:  32%|███▏      | 78/247 [00:20<00:49,  3.39it/s]Loading train:  32%|███▏      | 79/247 [00:20<00:49,  3.42it/s]Loading train:  32%|███▏      | 80/247 [00:21<00:47,  3.51it/s]Loading train:  33%|███▎      | 81/247 [00:21<00:49,  3.36it/s]Loading train:  33%|███▎      | 82/247 [00:21<00:49,  3.32it/s]Loading train:  34%|███▎      | 83/247 [00:22<00:50,  3.28it/s]Loading train:  34%|███▍      | 84/247 [00:22<00:49,  3.26it/s]Loading train:  34%|███▍      | 85/247 [00:22<00:49,  3.24it/s]Loading train:  35%|███▍      | 86/247 [00:23<00:49,  3.23it/s]Loading train:  35%|███▌      | 87/247 [00:23<00:49,  3.22it/s]Loading train:  36%|███▌      | 88/247 [00:23<00:49,  3.22it/s]Loading train:  36%|███▌      | 89/247 [00:23<00:49,  3.22it/s]Loading train:  36%|███▋      | 90/247 [00:24<00:48,  3.22it/s]Loading train:  37%|███▋      | 91/247 [00:24<00:48,  3.22it/s]Loading train:  37%|███▋      | 92/247 [00:24<00:48,  3.22it/s]Loading train:  38%|███▊      | 93/247 [00:25<00:47,  3.22it/s]Loading train:  38%|███▊      | 94/247 [00:25<00:47,  3.21it/s]Loading train:  38%|███▊      | 95/247 [00:25<00:47,  3.22it/s]Loading train:  39%|███▉      | 96/247 [00:26<00:46,  3.22it/s]Loading train:  39%|███▉      | 97/247 [00:26<00:46,  3.22it/s]Loading train:  40%|███▉      | 98/247 [00:26<00:46,  3.22it/s]Loading train:  40%|████      | 99/247 [00:27<00:45,  3.23it/s]Loading train:  40%|████      | 100/247 [00:27<00:44,  3.28it/s]Loading train:  41%|████      | 101/247 [00:27<00:44,  3.31it/s]Loading train:  41%|████▏     | 102/247 [00:27<00:43,  3.34it/s]Loading train:  42%|████▏     | 103/247 [00:28<00:42,  3.37it/s]Loading train:  42%|████▏     | 104/247 [00:28<00:42,  3.35it/s]Loading train:  43%|████▎     | 105/247 [00:28<00:42,  3.35it/s]Loading train:  43%|████▎     | 106/247 [00:29<00:42,  3.34it/s]Loading train:  43%|████▎     | 107/247 [00:29<00:41,  3.34it/s]Loading train:  44%|████▎     | 108/247 [00:29<00:41,  3.33it/s]Loading train:  44%|████▍     | 109/247 [00:30<00:41,  3.32it/s]Loading train:  45%|████▍     | 110/247 [00:30<00:40,  3.34it/s]Loading train:  45%|████▍     | 111/247 [00:30<00:40,  3.36it/s]Loading train:  45%|████▌     | 112/247 [00:30<00:39,  3.38it/s]Loading train:  46%|████▌     | 113/247 [00:31<00:39,  3.40it/s]Loading train:  46%|████▌     | 114/247 [00:31<00:39,  3.40it/s]Loading train:  47%|████▋     | 115/247 [00:31<00:38,  3.40it/s]Loading train:  47%|████▋     | 116/247 [00:32<00:38,  3.40it/s]Loading train:  47%|████▋     | 117/247 [00:32<00:38,  3.41it/s]Loading train:  48%|████▊     | 118/247 [00:32<00:36,  3.49it/s]Loading train:  48%|████▊     | 119/247 [00:32<00:36,  3.54it/s]Loading train:  49%|████▊     | 120/247 [00:33<00:35,  3.58it/s]Loading train:  49%|████▉     | 121/247 [00:33<00:34,  3.60it/s]Loading train:  49%|████▉     | 122/247 [00:33<00:34,  3.64it/s]Loading train:  50%|████▉     | 123/247 [00:34<00:33,  3.66it/s]Loading train:  50%|█████     | 124/247 [00:34<00:33,  3.64it/s]Loading train:  51%|█████     | 125/247 [00:34<00:33,  3.62it/s]Loading train:  51%|█████     | 126/247 [00:34<00:33,  3.63it/s]Loading train:  51%|█████▏    | 127/247 [00:35<00:32,  3.65it/s]Loading train:  52%|█████▏    | 128/247 [00:35<00:32,  3.62it/s]Loading train:  52%|█████▏    | 129/247 [00:35<00:32,  3.65it/s]Loading train:  53%|█████▎    | 130/247 [00:35<00:32,  3.66it/s]Loading train:  53%|█████▎    | 131/247 [00:36<00:31,  3.66it/s]Loading train:  53%|█████▎    | 132/247 [00:36<00:31,  3.66it/s]Loading train:  54%|█████▍    | 133/247 [00:36<00:30,  3.68it/s]Loading train:  54%|█████▍    | 134/247 [00:37<00:30,  3.68it/s]Loading train:  55%|█████▍    | 135/247 [00:37<00:30,  3.68it/s]Loading train:  55%|█████▌    | 136/247 [00:37<00:29,  3.72it/s]Loading train:  55%|█████▌    | 137/247 [00:37<00:29,  3.76it/s]Loading train:  56%|█████▌    | 138/247 [00:38<00:28,  3.78it/s]Loading train:  56%|█████▋    | 139/247 [00:38<00:28,  3.81it/s]Loading train:  57%|█████▋    | 140/247 [00:38<00:27,  3.83it/s]Loading train:  57%|█████▋    | 141/247 [00:38<00:27,  3.84it/s]Loading train:  57%|█████▋    | 142/247 [00:39<00:27,  3.85it/s]Loading train:  58%|█████▊    | 143/247 [00:39<00:27,  3.85it/s]Loading train:  58%|█████▊    | 144/247 [00:39<00:26,  3.86it/s]Loading train:  59%|█████▊    | 145/247 [00:39<00:26,  3.82it/s]Loading train:  59%|█████▉    | 146/247 [00:40<00:26,  3.82it/s]Loading train:  60%|█████▉    | 147/247 [00:40<00:26,  3.83it/s]Loading train:  60%|█████▉    | 148/247 [00:40<00:25,  3.84it/s]Loading train:  60%|██████    | 149/247 [00:40<00:25,  3.85it/s]Loading train:  61%|██████    | 150/247 [00:41<00:25,  3.85it/s]Loading train:  61%|██████    | 151/247 [00:41<00:24,  3.87it/s]Loading train:  62%|██████▏   | 152/247 [00:41<00:24,  3.86it/s]Loading train:  62%|██████▏   | 153/247 [00:41<00:24,  3.86it/s]Loading train:  62%|██████▏   | 154/247 [00:42<00:25,  3.58it/s]Loading train:  63%|██████▎   | 155/247 [00:42<00:26,  3.45it/s]Loading train:  63%|██████▎   | 156/247 [00:42<00:27,  3.37it/s]Loading train:  64%|██████▎   | 157/247 [00:43<00:27,  3.32it/s]Loading train:  64%|██████▍   | 158/247 [00:43<00:27,  3.29it/s]Loading train:  64%|██████▍   | 159/247 [00:43<00:26,  3.28it/s]Loading train:  65%|██████▍   | 160/247 [00:44<00:26,  3.26it/s]Loading train:  65%|██████▌   | 161/247 [00:44<00:26,  3.24it/s]Loading train:  66%|██████▌   | 162/247 [00:44<00:26,  3.26it/s]Loading train:  66%|██████▌   | 163/247 [00:45<00:26,  3.23it/s]Loading train:  66%|██████▋   | 164/247 [00:45<00:25,  3.22it/s]Loading train:  67%|██████▋   | 165/247 [00:45<00:25,  3.18it/s]Loading train:  67%|██████▋   | 166/247 [00:46<00:25,  3.15it/s]Loading train:  68%|██████▊   | 167/247 [00:46<00:25,  3.14it/s]Loading train:  68%|██████▊   | 168/247 [00:46<00:25,  3.12it/s]Loading train:  68%|██████▊   | 169/247 [00:47<00:24,  3.12it/s]Loading train:  69%|██████▉   | 170/247 [00:47<00:24,  3.11it/s]Loading train:  69%|██████▉   | 171/247 [00:47<00:24,  3.09it/s]Loading train:  70%|██████▉   | 172/247 [00:47<00:23,  3.19it/s]Loading train:  70%|███████   | 173/247 [00:48<00:22,  3.33it/s]Loading train:  70%|███████   | 174/247 [00:48<00:21,  3.38it/s]Loading train:  71%|███████   | 175/247 [00:48<00:21,  3.28it/s]Loading train:  71%|███████▏  | 176/247 [00:49<00:21,  3.32it/s]Loading train:  72%|███████▏  | 177/247 [00:49<00:20,  3.41it/s]Loading train:  72%|███████▏  | 178/247 [00:49<00:19,  3.52it/s]Loading train:  72%|███████▏  | 179/247 [00:49<00:18,  3.59it/s]Loading train:  73%|███████▎  | 180/247 [00:50<00:18,  3.62it/s]Loading train:  73%|███████▎  | 181/247 [00:50<00:17,  3.67it/s]Loading train:  74%|███████▎  | 182/247 [00:50<00:17,  3.69it/s]Loading train:  74%|███████▍  | 183/247 [00:51<00:17,  3.72it/s]Loading train:  74%|███████▍  | 184/247 [00:51<00:16,  3.72it/s]Loading train:  75%|███████▍  | 185/247 [00:51<00:16,  3.73it/s]Loading train:  75%|███████▌  | 186/247 [00:51<00:16,  3.70it/s]Loading train:  76%|███████▌  | 187/247 [00:52<00:16,  3.70it/s]Loading train:  76%|███████▌  | 188/247 [00:52<00:15,  3.73it/s]Loading train:  77%|███████▋  | 189/247 [00:52<00:15,  3.75it/s]Loading train:  77%|███████▋  | 190/247 [00:52<00:15,  3.75it/s]Loading train:  77%|███████▋  | 191/247 [00:53<00:14,  3.76it/s]Loading train:  78%|███████▊  | 192/247 [00:53<00:14,  3.78it/s]Loading train:  78%|███████▊  | 193/247 [00:53<00:14,  3.80it/s]Loading train:  79%|███████▊  | 194/247 [00:53<00:13,  3.88it/s]Loading train:  79%|███████▉  | 195/247 [00:54<00:13,  3.93it/s]Loading train:  79%|███████▉  | 196/247 [00:54<00:12,  3.99it/s]Loading train:  80%|███████▉  | 197/247 [00:54<00:12,  4.03it/s]Loading train:  80%|████████  | 198/247 [00:54<00:12,  4.06it/s]Loading train:  81%|████████  | 199/247 [00:55<00:11,  4.09it/s]Loading train:  81%|████████  | 200/247 [00:55<00:11,  4.09it/s]Loading train:  81%|████████▏ | 201/247 [00:55<00:11,  4.09it/s]Loading train:  82%|████████▏ | 202/247 [00:55<00:10,  4.11it/s]Loading train:  82%|████████▏ | 203/247 [00:56<00:10,  4.10it/s]Loading train:  83%|████████▎ | 204/247 [00:56<00:10,  4.09it/s]Loading train:  83%|████████▎ | 205/247 [00:56<00:10,  4.08it/s]Loading train:  83%|████████▎ | 206/247 [00:56<00:10,  4.07it/s]Loading train:  84%|████████▍ | 207/247 [00:57<00:09,  4.07it/s]Loading train:  84%|████████▍ | 208/247 [00:57<00:09,  4.07it/s]Loading train:  85%|████████▍ | 209/247 [00:57<00:09,  4.09it/s]Loading train:  85%|████████▌ | 210/247 [00:57<00:09,  4.10it/s]Loading train:  85%|████████▌ | 211/247 [00:58<00:08,  4.11it/s]Loading train:  86%|████████▌ | 212/247 [00:58<00:08,  4.04it/s]Loading train:  86%|████████▌ | 213/247 [00:58<00:08,  3.99it/s]Loading train:  87%|████████▋ | 214/247 [00:58<00:08,  3.95it/s]Loading train:  87%|████████▋ | 215/247 [00:59<00:08,  3.93it/s]Loading train:  87%|████████▋ | 216/247 [00:59<00:07,  3.88it/s]Loading train:  88%|████████▊ | 217/247 [00:59<00:07,  3.89it/s]Loading train:  88%|████████▊ | 218/247 [00:59<00:07,  3.86it/s]Loading train:  89%|████████▊ | 219/247 [01:00<00:07,  3.84it/s]Loading train:  89%|████████▉ | 220/247 [01:00<00:07,  3.85it/s]Loading train:  89%|████████▉ | 221/247 [01:00<00:06,  3.86it/s]Loading train:  90%|████████▉ | 222/247 [01:00<00:06,  3.86it/s]Loading train:  90%|█████████ | 223/247 [01:01<00:06,  3.87it/s]Loading train:  91%|█████████ | 224/247 [01:01<00:05,  3.87it/s]Loading train:  91%|█████████ | 225/247 [01:01<00:05,  3.87it/s]Loading train:  91%|█████████▏| 226/247 [01:01<00:05,  3.86it/s]Loading train:  92%|█████████▏| 227/247 [01:02<00:05,  3.83it/s]Loading train:  92%|█████████▏| 228/247 [01:02<00:04,  3.83it/s]Loading train:  93%|█████████▎| 229/247 [01:02<00:04,  3.82it/s]Loading train:  93%|█████████▎| 230/247 [01:03<00:04,  3.68it/s]Loading train:  94%|█████████▎| 231/247 [01:03<00:04,  3.58it/s]Loading train:  94%|█████████▍| 232/247 [01:03<00:04,  3.53it/s]Loading train:  94%|█████████▍| 233/247 [01:03<00:04,  3.47it/s]Loading train:  95%|█████████▍| 234/247 [01:04<00:03,  3.43it/s]Loading train:  95%|█████████▌| 235/247 [01:04<00:03,  3.40it/s]Loading train:  96%|█████████▌| 236/247 [01:04<00:03,  3.38it/s]Loading train:  96%|█████████▌| 237/247 [01:05<00:02,  3.38it/s]Loading train:  96%|█████████▋| 238/247 [01:05<00:02,  3.36it/s]Loading train:  97%|█████████▋| 239/247 [01:05<00:02,  3.35it/s]Loading train:  97%|█████████▋| 240/247 [01:06<00:02,  3.34it/s]Loading train:  98%|█████████▊| 241/247 [01:06<00:01,  3.31it/s]Loading train:  98%|█████████▊| 242/247 [01:06<00:01,  3.31it/s]Loading train:  98%|█████████▊| 243/247 [01:06<00:01,  3.32it/s]Loading train:  99%|█████████▉| 244/247 [01:07<00:00,  3.29it/s]Loading train:  99%|█████████▉| 245/247 [01:07<00:00,  3.29it/s]Loading train: 100%|█████████▉| 246/247 [01:07<00:00,  3.29it/s]Loading train: 100%|██████████| 247/247 [01:08<00:00,  3.26it/s]Loading train: 100%|██████████| 247/247 [01:08<00:00,  3.62it/s]
concatenating: train:   0%|          | 0/247 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/247 [00:00<00:04, 57.22it/s]concatenating: train:   5%|▍         | 12/247 [00:00<00:04, 56.12it/s]concatenating: train:   7%|▋         | 18/247 [00:00<00:04, 54.35it/s]concatenating: train:  10%|▉         | 24/247 [00:00<00:04, 54.83it/s]concatenating: train:  13%|█▎        | 31/247 [00:00<00:03, 57.44it/s]concatenating: train:  15%|█▌        | 38/247 [00:00<00:03, 59.61it/s]concatenating: train:  18%|█▊        | 45/247 [00:00<00:03, 60.29it/s]concatenating: train:  21%|██        | 51/247 [00:00<00:03, 60.09it/s]concatenating: train:  23%|██▎       | 57/247 [00:00<00:03, 59.48it/s]concatenating: train:  26%|██▌       | 63/247 [00:01<00:03, 56.53it/s]concatenating: train:  28%|██▊       | 69/247 [00:01<00:03, 55.41it/s]concatenating: train:  30%|███       | 75/247 [00:01<00:03, 54.67it/s]concatenating: train:  33%|███▎      | 81/247 [00:01<00:03, 52.99it/s]concatenating: train:  35%|███▌      | 87/247 [00:01<00:03, 51.75it/s]concatenating: train:  38%|███▊      | 93/247 [00:01<00:03, 51.11it/s]concatenating: train:  40%|████      | 99/247 [00:01<00:02, 50.82it/s]concatenating: train:  43%|████▎     | 105/247 [00:01<00:02, 51.33it/s]concatenating: train:  45%|████▍     | 111/247 [00:02<00:02, 52.18it/s]concatenating: train:  47%|████▋     | 117/247 [00:02<00:02, 52.49it/s]concatenating: train:  50%|████▉     | 123/247 [00:02<00:02, 54.04it/s]concatenating: train:  52%|█████▏    | 129/247 [00:02<00:02, 55.35it/s]concatenating: train:  55%|█████▍    | 135/247 [00:02<00:02, 55.40it/s]concatenating: train:  57%|█████▋    | 141/247 [00:02<00:01, 55.74it/s]concatenating: train:  60%|█████▉    | 147/247 [00:02<00:01, 56.40it/s]concatenating: train:  62%|██████▏   | 154/247 [00:02<00:01, 57.34it/s]concatenating: train:  65%|██████▍   | 160/247 [00:02<00:01, 56.16it/s]concatenating: train:  67%|██████▋   | 166/247 [00:03<00:01, 53.74it/s]concatenating: train:  70%|██████▉   | 172/247 [00:03<00:01, 53.03it/s]concatenating: train:  72%|███████▏  | 179/247 [00:03<00:01, 55.85it/s]concatenating: train:  75%|███████▌  | 186/247 [00:03<00:01, 58.00it/s]concatenating: train:  78%|███████▊  | 192/247 [00:03<00:00, 58.04it/s]concatenating: train:  81%|████████  | 199/247 [00:03<00:00, 59.93it/s]concatenating: train:  83%|████████▎ | 206/247 [00:03<00:00, 61.86it/s]concatenating: train:  86%|████████▌ | 213/247 [00:03<00:00, 64.08it/s]concatenating: train:  89%|████████▉ | 220/247 [00:03<00:00, 62.27it/s]concatenating: train:  92%|█████████▏| 227/247 [00:03<00:00, 63.10it/s]concatenating: train:  95%|█████████▍| 234/247 [00:04<00:00, 60.63it/s]concatenating: train:  98%|█████████▊| 241/247 [00:04<00:00, 58.02it/s]concatenating: train: 100%|██████████| 247/247 [00:04<00:00, 57.03it/s]concatenating: train: 100%|██████████| 247/247 [00:04<00:00, 56.59it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:00<00:01,  3.24it/s]Loading test:  40%|████      | 2/5 [00:00<00:00,  3.21it/s]Loading test:  60%|██████    | 3/5 [00:00<00:00,  3.25it/s]Loading test:  80%|████████  | 4/5 [00:01<00:00,  3.41it/s]Loading test: 100%|██████████| 5/5 [00:01<00:00,  3.46it/s]Loading test: 100%|██████████| 5/5 [00:01<00:00,  3.41it/s]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 346.92it/s] 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 76, 108, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 76, 108, 30)  300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 76, 108, 30)  120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 76, 108, 30)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 76, 108, 30)  8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 76, 108, 30)  120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 76, 108, 30)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 38, 54, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 38, 54, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 38, 54, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 38, 54, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 38, 54, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 38, 54, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 38, 54, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 38, 54, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 38, 54, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 19, 27, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 19, 27, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 19, 27, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 19, 27, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 19, 27, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 19, 27, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 19, 27, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 19, 27, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 19, 27, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 19, 27, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 38, 54, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 38, 54, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 38, 54, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 38, 54, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 38, 54, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 38, 54, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 38, 54, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 38, 54, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 38, 54, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 38, 54, 210)  0           concatenate_4[0][0]              2020-01-22 12:37:46.983639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 12:37:46.983749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 12:37:46.983765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 12:37:46.983774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 12:37:46.984055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 76, 108, 30)  25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 76, 108, 60)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 76, 108, 30)  16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 76, 108, 30)  120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 76, 108, 30)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 76, 108, 30)  8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 76, 108, 30)  120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 76, 108, 30)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 76, 108, 90)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 76, 108, 90)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 76, 108, 2)   182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 499,142
Non-trainable params: 1,200
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1
------------------------------------------------------------------
class_weights [0.97436874 0.02563126]
Train on 25479 samples, validate on 528 samples
Epoch 1/300
 - 67s - loss: 0.1366 - acc: 0.9858 - mDice: 0.7344 - val_loss: 0.1446 - val_acc: 0.9922 - val_mDice: 0.4766

Epoch 00001: val_mDice improved from -inf to 0.47664, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300
 - 63s - loss: 0.0746 - acc: 0.9920 - mDice: 0.8549 - val_loss: 0.0796 - val_acc: 0.9928 - val_mDice: 0.4798

Epoch 00002: val_mDice improved from 0.47664 to 0.47976, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 3/300
 - 63s - loss: 0.0651 - acc: 0.9929 - mDice: 0.8734 - val_loss: 0.0300 - val_acc: 0.9929 - val_mDice: 0.4860

Epoch 00003: val_mDice improved from 0.47976 to 0.48598, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 4/300
 - 63s - loss: 0.0605 - acc: 0.9934 - mDice: 0.8823 - val_loss: 0.0303 - val_acc: 0.9931 - val_mDice: 0.4872

Epoch 00004: val_mDice improved from 0.48598 to 0.48725, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 5/300
 - 63s - loss: 0.0570 - acc: 0.9938 - mDice: 0.8892 - val_loss: 0.0443 - val_acc: 0.9933 - val_mDice: 0.4919

Epoch 00005: val_mDice improved from 0.48725 to 0.49188, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 6/300
 - 63s - loss: 0.0547 - acc: 0.9940 - mDice: 0.8936 - val_loss: 0.0728 - val_acc: 0.9933 - val_mDice: 0.4958

Epoch 00006: val_mDice improved from 0.49188 to 0.49582, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 7/300
 - 63s - loss: 0.0531 - acc: 0.9942 - mDice: 0.8967 - val_loss: 0.0887 - val_acc: 0.9936 - val_mDice: 0.4968

Epoch 00007: val_mDice improved from 0.49582 to 0.49675, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 8/300
 - 63s - loss: 0.0509 - acc: 0.9944 - mDice: 0.9011 - val_loss: 0.0737 - val_acc: 0.9937 - val_mDice: 0.4989

Epoch 00008: val_mDice improved from 0.49675 to 0.49886, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 9/300
 - 64s - loss: 0.0496 - acc: 0.9945 - mDice: 0.9037 - val_loss: 0.0543 - val_acc: 0.9936 - val_mDice: 0.4972

Epoch 00009: val_mDice did not improve from 0.49886
Epoch 10/300
 - 63s - loss: 0.0483 - acc: 0.9946 - mDice: 0.9062 - val_loss: 0.0511 - val_acc: 0.9938 - val_mDice: 0.5005

Epoch 00010: val_mDice improved from 0.49886 to 0.50046, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 11/300
 - 63s - loss: 0.0476 - acc: 0.9947 - mDice: 0.9075 - val_loss: 0.0634 - val_acc: 0.9938 - val_mDice: 0.5027

Epoch 00011: val_mDice improved from 0.50046 to 0.50270, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 12/300
 - 63s - loss: 0.0468 - acc: 0.9948 - mDice: 0.9091 - val_loss: 0.0680 - val_acc: 0.9938 - val_mDice: 0.5040

Epoch 00012: val_mDice improved from 0.50270 to 0.50397, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 13/300
 - 63s - loss: 0.0454 - acc: 0.9949 - mDice: 0.9118 - val_loss: 0.0361 - val_acc: 0.9939 - val_mDice: 0.5015

Epoch 00013: val_mDice did not improve from 0.50397
Epoch 14/300
 - 63s - loss: 0.0451 - acc: 0.9949 - mDice: 0.9123 - val_loss: 0.0739 - val_acc: 0.9941 - val_mDice: 0.5028

Epoch 00014: val_mDice did not improve from 0.50397
Epoch 15/300
 - 63s - loss: 0.0447 - acc: 0.9950 - mDice: 0.9131 - val_loss: 0.0874 - val_acc: 0.9941 - val_mDice: 0.5047

Epoch 00015: val_mDice improved from 0.50397 to 0.50468, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 16/300
 - 63s - loss: 0.0433 - acc: 0.9951 - mDice: 0.9159 - val_loss: 0.0600 - val_acc: 0.9941 - val_mDice: 0.5063

Epoch 00016: val_mDice improved from 0.50468 to 0.50629, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 17/300
 - 63s - loss: 0.0431 - acc: 0.9951 - mDice: 0.9163 - val_loss: 0.0123 - val_acc: 0.9940 - val_mDice: 0.5045

Epoch 00017: val_mDice did not improve from 0.50629
Epoch 18/300
 - 63s - loss: 0.0426 - acc: 0.9952 - mDice: 0.9172 - val_loss: 0.0129 - val_acc: 0.9941 - val_mDice: 0.5144

Epoch 00018: val_mDice improved from 0.50629 to 0.51439, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 19/300
 - 63s - loss: 0.0422 - acc: 0.9952 - mDice: 0.9180 - val_loss: 0.0418 - val_acc: 0.9940 - val_mDice: 0.5148

Epoch 00019: val_mDice improved from 0.51439 to 0.51480, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 20/300
 - 63s - loss: 0.0416 - acc: 0.9953 - mDice: 0.9191 - val_loss: 0.0468 - val_acc: 0.9941 - val_mDice: 0.5046

Epoch 00020: val_mDice did not improve from 0.51480
Epoch 21/300
 - 63s - loss: 0.0412 - acc: 0.9953 - mDice: 0.9199 - val_loss: 0.0465 - val_acc: 0.9942 - val_mDice: 0.5111

Epoch 00021: val_mDice did not improve from 0.51480
Epoch 22/300
 - 63s - loss: 0.0406 - acc: 0.9953 - mDice: 0.9211 - val_loss: 0.0166 - val_acc: 0.9943 - val_mDice: 0.5073

Epoch 00022: val_mDice did not improve from 0.51480
Epoch 23/300
 - 63s - loss: 0.0403 - acc: 0.9954 - mDice: 0.9218 - val_loss: 0.0193 - val_acc: 0.9941 - val_mDice: 0.5170

Epoch 00023: val_mDice improved from 0.51480 to 0.51696, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 24/300
 - 63s - loss: 0.0399 - acc: 0.9954 - mDice: 0.9226 - val_loss: -3.9027e-03 - val_acc: 0.9942 - val_mDice: 0.5072

Epoch 00024: val_mDice did not improve from 0.51696
Epoch 25/300
 - 63s - loss: 0.0397 - acc: 0.9954 - mDice: 0.9229 - val_loss: -1.4847e-02 - val_acc: 0.9943 - val_mDice: 0.5150

Epoch 00025: val_mDice did not improve from 0.51696
Epoch 26/300
 - 63s - loss: 0.0399 - acc: 0.9955 - mDice: 0.9225 - val_loss: -3.1505e-04 - val_acc: 0.9944 - val_mDice: 0.5139

Epoch 00026: val_mDice did not improve from 0.51696
Epoch 27/300
 - 63s - loss: 0.0391 - acc: 0.9955 - mDice: 0.9240 - val_loss: -2.1262e-02 - val_acc: 0.9943 - val_mDice: 0.5146

Epoch 00027: val_mDice did not improve from 0.51696
Epoch 28/300
 - 63s - loss: 0.0386 - acc: 0.9955 - mDice: 0.9250 - val_loss: 0.0418 - val_acc: 0.9942 - val_mDice: 0.5117

Epoch 00028: val_mDice did not improve from 0.51696
Epoch 29/300
 - 63s - loss: 0.0384 - acc: 0.9956 - mDice: 0.9255 - val_loss: -1.3948e-03 - val_acc: 0.9943 - val_mDice: 0.5148

Epoch 00029: val_mDice did not improve from 0.51696
Epoch 30/300
 - 63s - loss: 0.0379 - acc: 0.9956 - mDice: 0.9264 - val_loss: 0.0017 - val_acc: 0.9943 - val_mDice: 0.5120

Epoch 00030: val_mDice did not improve from 0.51696
Epoch 31/300
 - 63s - loss: 0.0375 - acc: 0.9956 - mDice: 0.9271 - val_loss: -2.7995e-02 - val_acc: 0.9944 - val_mDice: 0.5065

Epoch 00031: val_mDice did not improve from 0.51696
Epoch 32/300
 - 63s - loss: 0.0376 - acc: 0.9956 - mDice: 0.9269 - val_loss: -3.9645e-02 - val_acc: 0.9943 - val_mDice: 0.5152

Epoch 00032: val_mDice did not improve from 0.51696
Epoch 33/300
 - 63s - loss: 0.0374 - acc: 0.9957 - mDice: 0.9275 - val_loss: -2.5259e-02 - val_acc: 0.9943 - val_mDice: 0.5154

Epoch 00033: val_mDice did not improve from 0.51696
Epoch 34/300
 - 63s - loss: 0.0375 - acc: 0.9957 - mDice: 0.9272 - val_loss: 0.0029 - val_acc: 0.9943 - val_mDice: 0.5065

Epoch 00034: val_mDice did not improve from 0.51696
Epoch 35/300
 - 63s - loss: 0.0369 - acc: 0.9957 - mDice: 0.9283 - val_loss: -1.6898e-02 - val_acc: 0.9942 - val_mDice: 0.5113

Epoch 00035: val_mDice did not improve from 0.51696
Epoch 36/300
 - 63s - loss: 0.0366 - acc: 0.9957 - mDice: 0.9290 - val_loss: 0.0042 - val_acc: 0.9944 - val_mDice: 0.5112

Epoch 00036: val_mDice did not improve from 0.51696
Epoch 37/300
 - 63s - loss: 0.0361 - acc: 0.9957 - mDice: 0.9299 - val_loss: -2.4693e-02 - val_acc: 0.9944 - val_mDice: 0.5052

Epoch 00037: val_mDice did not improve from 0.51696
Epoch 38/300
 - 63s - loss: 0.0362 - acc: 0.9958 - mDice: 0.9297 - val_loss: 0.0037 - val_acc: 0.9943 - val_mDice: 0.5062

Epoch 00038: val_mDice did not improve from 0.51696

Epoch 00038: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 39/300
 - 63s - loss: 0.0357 - acc: 0.9958 - mDice: 0.9307 - val_loss: 0.0053 - val_acc: 0.9943 - val_mDice: 0.5056

Epoch 00039: val_mDice did not improve from 0.51696
Epoch 40/300
 - 63s - loss: 0.0355 - acc: 0.9958 - mDice: 0.9311 - val_loss: -2.7144e-02 - val_acc: 0.9945 - val_mDice: 0.5078

Epoch 00040: val_mDice did not improve from 0.51696
Epoch 41/300
 - 63s - loss: 0.0354 - acc: 0.9958 - mDice: 0.9313 - val_loss: -5.1020e-02 - val_acc: 0.9944 - val_mDice: 0.5072

Epoch 00041: val_mDice did not improve from 0.51696
Epoch 42/300
 - 63s - loss: 0.0356 - acc: 0.9958 - mDice: 0.9309 - val_loss: -1.1577e-02 - val_acc: 0.9945 - val_mDice: 0.5076

Epoch 00042: val_mDice did not improve from 0.51696
Epoch 43/300
 - 63s - loss: 0.0351 - acc: 0.9958 - mDice: 0.9319 - val_loss: -2.1653e-02 - val_acc: 0.9945 - val_mDice: 0.5070

Epoch 00043: val_mDice did not improve from 0.51696
Epoch 44/300
 - 63s - loss: 0.0351 - acc: 0.9958 - mDice: 0.9318 - val_loss: -2.3805e-02 - val_acc: 0.9944 - val_mDice: 0.5074

Epoch 00044: val_mDice did not improve from 0.51696
Epoch 45/300
 - 64s - loss: 0.0355 - acc: 0.9958 - mDice: 0.9310 - val_loss: -1.5240e-02 - val_acc: 0.9944 - val_mDice: 0.5100

Epoch 00045: val_mDice did not improve from 0.51696
Epoch 46/300
 - 63s - loss: 0.0352 - acc: 0.9959 - mDice: 0.9316 - val_loss: -2.0384e-02 - val_acc: 0.9944 - val_mDice: 0.5083

Epoch 00046: val_mDice did not improve from 0.51696
Epoch 47/300
 - 63s - loss: 0.0347 - acc: 0.9959 - mDice: 0.9327 - val_loss: -3.4570e-02 - val_acc: 0.9944 - val_mDice: 0.5072

Epoch 00047: val_mDice did not improve from 0.51696
Epoch 48/300
 - 63s - loss: 0.0346 - acc: 0.9959 - mDice: 0.9328 - val_loss: -3.5164e-02 - val_acc: 0.9944 - val_mDice: 0.5070

Epoch 00048: val_mDice did not improve from 0.51696
Epoch 49/300
 - 63s - loss: 0.0347 - acc: 0.9959 - mDice: 0.9327 - val_loss: -3.1501e-02 - val_acc: 0.9943 - val_mDice: 0.5075

Epoch 00049: val_mDice did not improve from 0.51696
Epoch 50/300
 - 63s - loss: 0.0347 - acc: 0.9959 - mDice: 0.9325 - val_loss: 0.0060 - val_acc: 0.9944 - val_mDice: 0.5064

Epoch 00050: val_mDice did not improve from 0.51696
Epoch 51/300
 - 63s - loss: 0.0344 - acc: 0.9959 - mDice: 0.9332 - val_loss: -3.4765e-02 - val_acc: 0.9944 - val_mDice: 0.5059

Epoch 00051: val_mDice did not improve from 0.51696
Epoch 52/300
 - 63s - loss: 0.0347 - acc: 0.9959 - mDice: 0.9327 - val_loss: -3.3244e-02 - val_acc: 0.9944 - val_mDice: 0.5063

Epoch 00052: val_mDice did not improve from 0.51696
Epoch 53/300
 - 63s - loss: 0.0342 - acc: 0.9959 - mDice: 0.9337 - val_loss: 0.0061 - val_acc: 0.9943 - val_mDice: 0.5075

Epoch 00053: val_mDice did not improve from 0.51696

Epoch 00053: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 54/300
 - 63s - loss: 0.0345 - acc: 0.9959 - mDice: 0.9331 - val_loss: -3.1476e-02 - val_acc: 0.9944 - val_mDice: 0.5079

Epoch 00054: val_mDice did not improve from 0.51696
Epoch 55/300
 - 63s - loss: 0.0341 - acc: 0.9959 - mDice: 0.9337 - val_loss: -3.0913e-02 - val_acc: 0.9944 - val_mDice: 0.5080

Epoch 00055: val_mDice did not improve from 0.51696
Epoch 56/300
 - 63s - loss: 0.0342 - acc: 0.9959 - mDice: 0.9335 - val_loss: -4.9394e-02 - val_acc: 0.9944 - val_mDice: 0.5071

Epoch 00056: val_mDice did not improve from 0.51696
Epoch 57/300
 - 64s - loss: 0.0340 - acc: 0.9959 - mDice: 0.9341 - val_loss: -5.0770e-02 - val_acc: 0.9944 - val_mDice: 0.5072

Epoch 00057: val_mDice did not improve from 0.51696
Epoch 58/300
 - 64s - loss: 0.0339 - acc: 0.9960 - mDice: 0.9342 - val_loss: -5.0386e-02 - val_acc: 0.9944 - val_mDice: 0.5075

Epoch 00058: val_mDice did not improve from 0.51696
Epoch 59/300
 - 63s - loss: 0.0342 - acc: 0.9960 - mDice: 0.9337 - val_loss: -4.5852e-02 - val_acc: 0.9944 - val_mDice: 0.5067

Epoch 00059: val_mDice did not improve from 0.51696
Epoch 60/300
 - 63s - loss: 0.0342 - acc: 0.9960 - mDice: 0.9337 - val_loss: -4.7243e-02 - val_acc: 0.9944 - val_mDice: 0.5067

Epoch 00060: val_mDice did not improve from 0.51696
Epoch 61/300
 - 63s - loss: 0.0338 - acc: 0.9960 - mDice: 0.9345 - val_loss: -3.7367e-02 - val_acc: 0.9944 - val_mDice: 0.5067

Epoch 00061: val_mDice did not improve from 0.51696
Epoch 62/300
 - 63s - loss: 0.0336 - acc: 0.9960 - mDice: 0.9347 - val_loss: -3.7371e-02 - val_acc: 0.9944 - val_mDice: 0.5073

Epoch 00062: val_mDice did not improve from 0.51696
Epoch 63/300
 - 63s - loss: 0.0337 - acc: 0.9960 - mDice: 0.9347 - val_loss: -4.9900e-02 - val_acc: 0.9944 - val_mDice: 0.5075

Epoch 00063: val_mDice did not improve from 0.51696
Restoring model weights from the end of the best epoch
Epoch 00063: early stopping
{'val_loss': [0.14461494395227142, 0.07956038294077823, 0.029982124201276085, 0.0302571455073176, 0.0442765462353374, 0.07276335113089193, 0.08869848104024475, 0.07367599281397733, 0.05427114304267999, 0.05111215653067285, 0.0634188379132838, 0.06802539310107629, 0.0361048114401373, 0.07391690431783597, 0.08744626105621908, 0.059981890954077244, 0.012310985870885126, 0.012910378955756172, 0.04179053677415306, 0.046781989971570896, 0.04653421507188768, 0.016649129574723316, 0.019298392940651287, -0.0039026676818276896, -0.01484678965061903, -0.00031504565567681283, -0.021261930381032555, 0.041806097255285946, -0.001394781538031318, 0.0016822146646904223, -0.027994646267457443, -0.039645084010606464, -0.025258739704661297, 0.0028773543114463487, -0.016897624471422398, 0.0042293961300994415, -0.024692517056158096, 0.003685815993583564, 0.005263454591234525, -0.027144223987830406, -0.05101974263335719, -0.01157670050407901, -0.021653086285699497, -0.023804617819912506, -0.015239521409526016, -0.020384180161989097, -0.034570393999191845, -0.03516426882847692, -0.031500924971293316, 0.005984657867388291, -0.03476469870656729, -0.033243934539231385, 0.006056526675820351, -0.03147643581597191, -0.030912786016635822, -0.04939405769674164, -0.05076967857100747, -0.05038568216630004, -0.045851995067840275, -0.047243371182544666, -0.03736659765920856, -0.03737136950208382, -0.049900488843294705], 'val_acc': [0.9922198012019648, 0.9928178931727554, 0.9929284168915316, 0.9931356197956837, 0.9933349860436989, 0.9932892959226262, 0.9936144121668555, 0.9937078650250579, 0.9936044938636549, 0.9937595551212629, 0.9938264669794025, 0.9938412371909979, 0.9939256906509399, 0.9940708257032164, 0.994143968956037, 0.9940890536615343, 0.9940385217919494, 0.9941236638661587, 0.9939702236742685, 0.9941402809186415, 0.9942355727156004, 0.9942598062934298, 0.9941045158740246, 0.9941974991198742, 0.994264646913066, 0.99440056017854, 0.9942918776562719, 0.9941765088023562, 0.9943167998483686, 0.9942787251237667, 0.9943830219633651, 0.9943027216376681, 0.9942563451600798, 0.9943377969391418, 0.9942484960863085, 0.994388326550975, 0.9944035596016682, 0.9943045662208037, 0.9942870280056288, 0.9944743920456279, 0.9943587962876667, 0.9944577817664002, 0.9944550160205725, 0.9944455537832144, 0.9943844059651549, 0.9944220167217832, 0.9944257081458063, 0.9944476309147748, 0.9943354861302809, 0.9943641020041524, 0.9943712523037737, 0.9943758682771162, 0.9943230301141739, 0.9944476275281473, 0.9944291704080321, 0.9944044773777326, 0.9944277852773666, 0.9944210921724638, 0.9944153224880045, 0.9943786385384473, 0.9943862528060422, 0.9944102493199435, 0.9944157841982264], 'val_mDice': [0.47663777671528607, 0.4797581989668666, 0.4859802836376527, 0.4872478108342952, 0.4918831152968691, 0.49581760103633726, 0.49675477171944354, 0.49885850954356453, 0.4972294138225599, 0.5004574615818692, 0.5026989784613555, 0.5039690615553268, 0.501465449014674, 0.5027749273163831, 0.5046799577374684, 0.5062881675329126, 0.5045050949874249, 0.5143862006000497, 0.5147972573159318, 0.5046329292927749, 0.5111126557952075, 0.5073433027244832, 0.516957017931795, 0.5072192976314743, 0.515013321331053, 0.5138765382368795, 0.5146235136569224, 0.5116853683791829, 0.5148391061547127, 0.5120182677285012, 0.5064678518807121, 0.515161351085584, 0.5154437366449698, 0.5064993190106896, 0.5112832776142044, 0.5112108278042411, 0.5052282735139508, 0.506183110841393, 0.5055868060853285, 0.5077664389794496, 0.5072366407781179, 0.5076228349875574, 0.5070052871200125, 0.5073533692934361, 0.5099535128358659, 0.508293814236612, 0.5071567037550456, 0.5070446541095996, 0.5074573342876307, 0.5064049615284123, 0.5059036873993207, 0.506263104396508, 0.5074999686048339, 0.5079139811242543, 0.5079838214883101, 0.5070586627746734, 0.5071588226592487, 0.507466773360923, 0.5066721889141429, 0.506721641902726, 0.506683291133855, 0.5072898896875355, 0.5074831730653815], 'loss': [0.136614631251313, 0.07462406511574199, 0.0651224266048026, 0.0605375779047506, 0.05701077427305528, 0.05473936672555913, 0.053116658667182945, 0.050909348930826534, 0.04957775602481902, 0.0482592658843073, 0.04758965323348239, 0.04678898965541822, 0.04541203424429855, 0.04512527612809544, 0.044731233146890675, 0.043297143092245846, 0.04310485903163304, 0.04263789124543852, 0.042226605906563804, 0.041629328985145736, 0.04122351019857555, 0.0406423355015365, 0.040277570500689745, 0.039863926017156666, 0.039689199312444697, 0.039863167800176805, 0.039119197843989865, 0.03864212010095696, 0.0383772204448301, 0.037895324621078356, 0.037539886067475894, 0.03764673698930521, 0.03735390496177236, 0.03748852211048167, 0.0369369141385237, 0.03656981107536321, 0.03611905019831361, 0.036217158749992595, 0.035690324585176424, 0.03547086922942462, 0.035393129020532814, 0.03560139940584973, 0.035104539958726734, 0.0351330503523689, 0.035512924693205224, 0.03524566190672929, 0.034675607181492776, 0.034639258836716424, 0.034664866518780604, 0.034749699061639015, 0.034436113205139084, 0.03465760200823901, 0.034163044194860014, 0.03446755923807137, 0.034140431381847705, 0.034232067017419744, 0.03396856987994284, 0.03389114178894051, 0.034161683504030954, 0.034163393888777066, 0.03375595128327085, 0.033628317704497, 0.03365065603939507], 'acc': [0.9858008756654042, 0.9920380956229519, 0.9929365536739801, 0.9934200008443538, 0.9937594665702103, 0.9939801982786401, 0.994174453958976, 0.994358222899311, 0.9944977190445972, 0.9946044651313565, 0.9946969855738901, 0.9947876227468602, 0.9948729856608731, 0.9949181383467108, 0.9949954679563546, 0.9950733707615427, 0.9951295985022325, 0.9951602735639746, 0.9952071193969578, 0.9952549647534444, 0.995299965237948, 0.9953465382309563, 0.9953649436582075, 0.9954019672357193, 0.9954469301453649, 0.9954712924204374, 0.9955070688808828, 0.9955282803256272, 0.9955522942484077, 0.9955904953726523, 0.995597204073215, 0.9956347207254076, 0.9956546407198062, 0.9956779752633295, 0.9956889252885553, 0.9957032901904166, 0.9957433707149171, 0.9957667476477085, 0.9957942227828989, 0.9958064120859837, 0.9957968485711428, 0.9958283883224893, 0.9958449481743425, 0.9958421355247535, 0.995845377606637, 0.9958746173307975, 0.9958640596262023, 0.995865450967373, 0.9958806141677, 0.9958928975061829, 0.9959110493417428, 0.9958993247649308, 0.9959370660311181, 0.9959375253745154, 0.9959261972590076, 0.9959375680023986, 0.9959463618876744, 0.9959501676649467, 0.9959590430606678, 0.9959596311018135, 0.995955624558019, 0.9959636764345956, 0.995957211177565], 'mDice': [0.7344089629385809, 0.854904593330869, 0.8734120822569476, 0.8823157007680957, 0.8891854074165629, 0.8936064956307884, 0.896744622491846, 0.9010614349509976, 0.9036550625858691, 0.9062301041759059, 0.9075187178093016, 0.9090679028592008, 0.9117799600035594, 0.9123222437027115, 0.9130732312279746, 0.9158995604490746, 0.9162503945883161, 0.9171641063613455, 0.9179635822499275, 0.9191322165267463, 0.9199238195164414, 0.9210598800525668, 0.9217743834832937, 0.9225833919720268, 0.922910444019419, 0.9225458890869889, 0.9240170030870104, 0.9249563290700981, 0.9254734853526815, 0.9264204426520876, 0.9271214151898042, 0.9268893329559332, 0.9274662075763623, 0.9271813038204523, 0.9282760290617826, 0.9290022841076858, 0.9298818303551609, 0.9296784304113682, 0.9307151935651616, 0.9311461319536564, 0.931300070321449, 0.9308722085773844, 0.9318553685646023, 0.9318030479377366, 0.9310365439378334, 0.931563977300805, 0.9327003374766544, 0.9327736061967483, 0.9327125925228741, 0.9325393796285255, 0.9331579878391147, 0.9327136081482209, 0.93369311502503, 0.9330792733579276, 0.9337378639343736, 0.9335496224139277, 0.9340737172336976, 0.9342220691146934, 0.9336819430690596, 0.9336747929797151, 0.9344891623072518, 0.9347439181009741, 0.9346950272197387], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:02,  1.51it/s]predicting test subjects:  40%|████      | 2/5 [00:00<00:01,  1.88it/s]predicting test subjects:  60%|██████    | 3/5 [00:01<00:00,  2.31it/s]predicting test subjects:  80%|████████  | 4/5 [00:01<00:00,  2.83it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  3.33it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  3.48it/s]
predicting train subjects:   0%|          | 0/247 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/247 [00:00<00:35,  6.90it/s]predicting train subjects:   1%|          | 2/247 [00:00<00:34,  7.08it/s]predicting train subjects:   1%|          | 3/247 [00:00<00:39,  6.23it/s]predicting train subjects:   2%|▏         | 4/247 [00:00<00:41,  5.90it/s]predicting train subjects:   2%|▏         | 5/247 [00:00<00:39,  6.17it/s]predicting train subjects:   2%|▏         | 6/247 [00:00<00:37,  6.36it/s]predicting train subjects:   3%|▎         | 7/247 [00:01<00:36,  6.52it/s]predicting train subjects:   3%|▎         | 8/247 [00:01<00:36,  6.61it/s]predicting train subjects:   4%|▎         | 9/247 [00:01<00:35,  6.70it/s]predicting train subjects:   4%|▍         | 10/247 [00:01<00:35,  6.73it/s]predicting train subjects:   4%|▍         | 11/247 [00:01<00:35,  6.71it/s]predicting train subjects:   5%|▍         | 12/247 [00:01<00:35,  6.65it/s]predicting train subjects:   5%|▌         | 13/247 [00:01<00:35,  6.67it/s]predicting train subjects:   6%|▌         | 14/247 [00:02<00:34,  6.75it/s]predicting train subjects:   6%|▌         | 15/247 [00:02<00:34,  6.78it/s]predicting train subjects:   6%|▋         | 16/247 [00:02<00:34,  6.75it/s]predicting train subjects:   7%|▋         | 17/247 [00:02<00:34,  6.72it/s]predicting train subjects:   7%|▋         | 18/247 [00:02<00:33,  6.75it/s]predicting train subjects:   8%|▊         | 19/247 [00:02<00:33,  6.79it/s]predicting train subjects:   8%|▊         | 20/247 [00:03<00:33,  6.83it/s]predicting train subjects:   9%|▊         | 21/247 [00:03<00:32,  6.90it/s]predicting train subjects:   9%|▉         | 22/247 [00:03<00:32,  6.91it/s]predicting train subjects:   9%|▉         | 23/247 [00:03<00:31,  7.21it/s]predicting train subjects:  10%|▉         | 24/247 [00:03<00:30,  7.40it/s]predicting train subjects:  10%|█         | 25/247 [00:03<00:29,  7.55it/s]predicting train subjects:  11%|█         | 26/247 [00:03<00:28,  7.67it/s]predicting train subjects:  11%|█         | 27/247 [00:03<00:28,  7.80it/s]predicting train subjects:  11%|█▏        | 28/247 [00:04<00:27,  7.88it/s]predicting train subjects:  12%|█▏        | 29/247 [00:04<00:27,  7.94it/s]predicting train subjects:  12%|█▏        | 30/247 [00:04<00:27,  7.99it/s]predicting train subjects:  13%|█▎        | 31/247 [00:04<00:26,  8.02it/s]predicting train subjects:  13%|█▎        | 32/247 [00:04<00:26,  8.03it/s]predicting train subjects:  13%|█▎        | 33/247 [00:04<00:26,  8.00it/s]predicting train subjects:  14%|█▍        | 34/247 [00:04<00:26,  7.96it/s]predicting train subjects:  14%|█▍        | 35/247 [00:04<00:26,  7.91it/s]predicting train subjects:  15%|█▍        | 36/247 [00:05<00:26,  7.90it/s]predicting train subjects:  15%|█▍        | 37/247 [00:05<00:26,  7.90it/s]predicting train subjects:  15%|█▌        | 38/247 [00:05<00:26,  7.86it/s]predicting train subjects:  16%|█▌        | 39/247 [00:05<00:26,  7.89it/s]predicting train subjects:  16%|█▌        | 40/247 [00:05<00:26,  7.92it/s]predicting train subjects:  17%|█▋        | 41/247 [00:05<00:26,  7.86it/s]predicting train subjects:  17%|█▋        | 42/247 [00:05<00:26,  7.80it/s]predicting train subjects:  17%|█▋        | 43/247 [00:05<00:26,  7.76it/s]predicting train subjects:  18%|█▊        | 44/247 [00:06<00:26,  7.76it/s]predicting train subjects:  18%|█▊        | 45/247 [00:06<00:26,  7.70it/s]predicting train subjects:  19%|█▊        | 46/247 [00:06<00:26,  7.69it/s]predicting train subjects:  19%|█▉        | 47/247 [00:06<00:26,  7.62it/s]predicting train subjects:  19%|█▉        | 48/247 [00:06<00:26,  7.64it/s]predicting train subjects:  20%|█▉        | 49/247 [00:06<00:26,  7.58it/s]predicting train subjects:  20%|██        | 50/247 [00:06<00:25,  7.62it/s]predicting train subjects:  21%|██        | 51/247 [00:07<00:25,  7.66it/s]predicting train subjects:  21%|██        | 52/247 [00:07<00:25,  7.69it/s]predicting train subjects:  21%|██▏       | 53/247 [00:07<00:28,  6.80it/s]predicting train subjects:  22%|██▏       | 54/247 [00:07<00:27,  7.04it/s]predicting train subjects:  22%|██▏       | 55/247 [00:07<00:26,  7.20it/s]predicting train subjects:  23%|██▎       | 56/247 [00:07<00:26,  7.33it/s]predicting train subjects:  23%|██▎       | 57/247 [00:07<00:25,  7.46it/s]predicting train subjects:  23%|██▎       | 58/247 [00:07<00:25,  7.53it/s]predicting train subjects:  24%|██▍       | 59/247 [00:08<00:25,  7.29it/s]predicting train subjects:  24%|██▍       | 60/247 [00:08<00:26,  7.17it/s]predicting train subjects:  25%|██▍       | 61/247 [00:08<00:26,  7.11it/s]predicting train subjects:  25%|██▌       | 62/247 [00:08<00:26,  7.08it/s]predicting train subjects:  26%|██▌       | 63/247 [00:08<00:26,  7.04it/s]predicting train subjects:  26%|██▌       | 64/247 [00:08<00:26,  7.02it/s]predicting train subjects:  26%|██▋       | 65/247 [00:08<00:25,  7.00it/s]predicting train subjects:  27%|██▋       | 66/247 [00:09<00:26,  6.96it/s]predicting train subjects:  27%|██▋       | 67/247 [00:09<00:25,  6.95it/s]predicting train subjects:  28%|██▊       | 68/247 [00:09<00:25,  6.96it/s]predicting train subjects:  28%|██▊       | 69/247 [00:09<00:25,  6.92it/s]predicting train subjects:  28%|██▊       | 70/247 [00:09<00:25,  6.91it/s]predicting train subjects:  29%|██▊       | 71/247 [00:09<00:25,  6.92it/s]predicting train subjects:  29%|██▉       | 72/247 [00:09<00:25,  6.93it/s]predicting train subjects:  30%|██▉       | 73/247 [00:10<00:25,  6.94it/s]predicting train subjects:  30%|██▉       | 74/247 [00:10<00:24,  6.95it/s]predicting train subjects:  30%|███       | 75/247 [00:10<00:24,  6.95it/s]predicting train subjects:  31%|███       | 76/247 [00:10<00:24,  6.93it/s]predicting train subjects:  31%|███       | 77/247 [00:10<00:28,  5.89it/s]predicting train subjects:  32%|███▏      | 78/247 [00:11<00:30,  5.50it/s]predicting train subjects:  32%|███▏      | 79/247 [00:11<00:28,  5.91it/s]predicting train subjects:  32%|███▏      | 80/247 [00:11<00:28,  5.95it/s]predicting train subjects:  33%|███▎      | 81/247 [00:11<00:30,  5.41it/s]predicting train subjects:  33%|███▎      | 82/247 [00:11<00:29,  5.60it/s]predicting train subjects:  34%|███▎      | 83/247 [00:11<00:28,  5.70it/s]predicting train subjects:  34%|███▍      | 84/247 [00:12<00:28,  5.80it/s]predicting train subjects:  34%|███▍      | 85/247 [00:12<00:27,  5.89it/s]predicting train subjects:  35%|███▍      | 86/247 [00:12<00:27,  5.95it/s]predicting train subjects:  35%|███▌      | 87/247 [00:12<00:26,  5.96it/s]predicting train subjects:  36%|███▌      | 88/247 [00:12<00:26,  5.98it/s]predicting train subjects:  36%|███▌      | 89/247 [00:12<00:26,  6.00it/s]predicting train subjects:  36%|███▋      | 90/247 [00:13<00:26,  6.02it/s]predicting train subjects:  37%|███▋      | 91/247 [00:13<00:25,  6.03it/s]predicting train subjects:  37%|███▋      | 92/247 [00:13<00:25,  6.03it/s]predicting train subjects:  38%|███▊      | 93/247 [00:13<00:25,  6.01it/s]predicting train subjects:  38%|███▊      | 94/247 [00:13<00:25,  6.02it/s]predicting train subjects:  38%|███▊      | 95/247 [00:13<00:25,  6.02it/s]predicting train subjects:  39%|███▉      | 96/247 [00:14<00:25,  5.99it/s]predicting train subjects:  39%|███▉      | 97/247 [00:14<00:24,  6.01it/s]predicting train subjects:  40%|███▉      | 98/247 [00:14<00:24,  5.98it/s]predicting train subjects:  40%|████      | 99/247 [00:14<00:24,  5.93it/s]predicting train subjects:  40%|████      | 100/247 [00:14<00:24,  6.03it/s]predicting train subjects:  41%|████      | 101/247 [00:14<00:24,  6.04it/s]predicting train subjects:  41%|████▏     | 102/247 [00:15<00:23,  6.10it/s]predicting train subjects:  42%|████▏     | 103/247 [00:15<00:23,  6.21it/s]predicting train subjects:  42%|████▏     | 104/247 [00:15<00:22,  6.22it/s]predicting train subjects:  43%|████▎     | 105/247 [00:15<00:22,  6.23it/s]predicting train subjects:  43%|████▎     | 106/247 [00:15<00:22,  6.24it/s]predicting train subjects:  43%|████▎     | 107/247 [00:15<00:22,  6.24it/s]predicting train subjects:  44%|████▎     | 108/247 [00:15<00:22,  6.24it/s]predicting train subjects:  44%|████▍     | 109/247 [00:16<00:22,  6.27it/s]predicting train subjects:  45%|████▍     | 110/247 [00:16<00:21,  6.29it/s]predicting train subjects:  45%|████▍     | 111/247 [00:16<00:21,  6.27it/s]predicting train subjects:  45%|████▌     | 112/247 [00:16<00:21,  6.29it/s]predicting train subjects:  46%|████▌     | 113/247 [00:16<00:21,  6.26it/s]predicting train subjects:  46%|████▌     | 114/247 [00:16<00:21,  6.11it/s]predicting train subjects:  47%|████▋     | 115/247 [00:17<00:21,  6.14it/s]predicting train subjects:  47%|████▋     | 116/247 [00:17<00:21,  6.20it/s]predicting train subjects:  47%|████▋     | 117/247 [00:17<00:20,  6.21it/s]predicting train subjects:  48%|████▊     | 118/247 [00:17<00:20,  6.43it/s]predicting train subjects:  48%|████▊     | 119/247 [00:17<00:19,  6.60it/s]predicting train subjects:  49%|████▊     | 120/247 [00:17<00:18,  6.71it/s]predicting train subjects:  49%|████▉     | 121/247 [00:17<00:18,  6.82it/s]predicting train subjects:  49%|████▉     | 122/247 [00:18<00:18,  6.90it/s]predicting train subjects:  50%|████▉     | 123/247 [00:18<00:17,  6.91it/s]predicting train subjects:  50%|█████     | 124/247 [00:18<00:17,  6.94it/s]predicting train subjects:  51%|█████     | 125/247 [00:18<00:17,  6.98it/s]predicting train subjects:  51%|█████     | 126/247 [00:18<00:17,  7.02it/s]predicting train subjects:  51%|█████▏    | 127/247 [00:18<00:16,  7.06it/s]predicting train subjects:  52%|█████▏    | 128/247 [00:18<00:16,  7.09it/s]predicting train subjects:  52%|█████▏    | 129/247 [00:19<00:16,  7.08it/s]predicting train subjects:  53%|█████▎    | 130/247 [00:19<00:16,  7.02it/s]predicting train subjects:  53%|█████▎    | 131/247 [00:19<00:16,  7.02it/s]predicting train subjects:  53%|█████▎    | 132/247 [00:19<00:16,  7.01it/s]predicting train subjects:  54%|█████▍    | 133/247 [00:19<00:16,  7.01it/s]predicting train subjects:  54%|█████▍    | 134/247 [00:19<00:16,  7.02it/s]predicting train subjects:  55%|█████▍    | 135/247 [00:19<00:15,  7.06it/s]predicting train subjects:  55%|█████▌    | 136/247 [00:20<00:15,  7.18it/s]predicting train subjects:  55%|█████▌    | 137/247 [00:20<00:15,  7.24it/s]predicting train subjects:  56%|█████▌    | 138/247 [00:20<00:14,  7.30it/s]predicting train subjects:  56%|█████▋    | 139/247 [00:20<00:14,  7.36it/s]predicting train subjects:  57%|█████▋    | 140/247 [00:20<00:14,  7.34it/s]predicting train subjects:  57%|█████▋    | 141/247 [00:20<00:14,  7.30it/s]predicting train subjects:  57%|█████▋    | 142/247 [00:20<00:14,  7.37it/s]predicting train subjects:  58%|█████▊    | 143/247 [00:21<00:14,  7.36it/s]predicting train subjects:  58%|█████▊    | 144/247 [00:21<00:13,  7.38it/s]predicting train subjects:  59%|█████▊    | 145/247 [00:21<00:13,  7.44it/s]predicting train subjects:  59%|█████▉    | 146/247 [00:21<00:13,  7.47it/s]predicting train subjects:  60%|█████▉    | 147/247 [00:21<00:13,  7.47it/s]predicting train subjects:  60%|█████▉    | 148/247 [00:21<00:13,  7.32it/s]predicting train subjects:  60%|██████    | 149/247 [00:21<00:13,  7.36it/s]predicting train subjects:  61%|██████    | 150/247 [00:22<00:13,  7.41it/s]predicting train subjects:  61%|██████    | 151/247 [00:22<00:12,  7.45it/s]predicting train subjects:  62%|██████▏   | 152/247 [00:22<00:12,  7.50it/s]predicting train subjects:  62%|██████▏   | 153/247 [00:22<00:12,  7.54it/s]predicting train subjects:  62%|██████▏   | 154/247 [00:22<00:13,  7.02it/s]predicting train subjects:  63%|██████▎   | 155/247 [00:22<00:13,  6.73it/s]predicting train subjects:  63%|██████▎   | 156/247 [00:22<00:13,  6.57it/s]predicting train subjects:  64%|██████▎   | 157/247 [00:23<00:13,  6.47it/s]predicting train subjects:  64%|██████▍   | 158/247 [00:23<00:13,  6.41it/s]predicting train subjects:  64%|██████▍   | 159/247 [00:23<00:13,  6.35it/s]predicting train subjects:  65%|██████▍   | 160/247 [00:23<00:13,  6.28it/s]predicting train subjects:  65%|██████▌   | 161/247 [00:23<00:13,  6.18it/s]predicting train subjects:  66%|██████▌   | 162/247 [00:23<00:13,  6.20it/s]predicting train subjects:  66%|██████▌   | 163/247 [00:24<00:13,  6.19it/s]predicting train subjects:  66%|██████▋   | 164/247 [00:24<00:13,  6.15it/s]predicting train subjects:  67%|██████▋   | 165/247 [00:24<00:13,  6.10it/s]predicting train subjects:  67%|██████▋   | 166/247 [00:24<00:13,  6.09it/s]predicting train subjects:  68%|██████▊   | 167/247 [00:24<00:13,  6.10it/s]predicting train subjects:  68%|██████▊   | 168/247 [00:24<00:13,  6.03it/s]predicting train subjects:  68%|██████▊   | 169/247 [00:25<00:12,  6.03it/s]predicting train subjects:  69%|██████▉   | 170/247 [00:25<00:12,  6.09it/s]predicting train subjects:  69%|██████▉   | 171/247 [00:25<00:12,  6.15it/s]predicting train subjects:  70%|██████▉   | 172/247 [00:25<00:12,  6.00it/s]predicting train subjects:  70%|███████   | 173/247 [00:25<00:13,  5.58it/s]predicting train subjects:  70%|███████   | 174/247 [00:25<00:12,  6.04it/s]predicting train subjects:  71%|███████   | 175/247 [00:26<00:12,  5.61it/s]predicting train subjects:  71%|███████▏  | 176/247 [00:26<00:11,  6.01it/s]predicting train subjects:  72%|███████▏  | 177/247 [00:26<00:11,  6.28it/s]predicting train subjects:  72%|███████▏  | 178/247 [00:26<00:10,  6.52it/s]predicting train subjects:  72%|███████▏  | 179/247 [00:26<00:10,  6.72it/s]predicting train subjects:  73%|███████▎  | 180/247 [00:26<00:09,  6.90it/s]predicting train subjects:  73%|███████▎  | 181/247 [00:26<00:09,  6.98it/s]predicting train subjects:  74%|███████▎  | 182/247 [00:27<00:09,  7.02it/s]predicting train subjects:  74%|███████▍  | 183/247 [00:27<00:09,  7.11it/s]predicting train subjects:  74%|███████▍  | 184/247 [00:27<00:08,  7.20it/s]predicting train subjects:  75%|███████▍  | 185/247 [00:27<00:08,  7.27it/s]predicting train subjects:  75%|███████▌  | 186/247 [00:27<00:08,  7.32it/s]predicting train subjects:  76%|███████▌  | 187/247 [00:27<00:08,  7.33it/s]predicting train subjects:  76%|███████▌  | 188/247 [00:27<00:08,  7.36it/s]predicting train subjects:  77%|███████▋  | 189/247 [00:27<00:07,  7.34it/s]predicting train subjects:  77%|███████▋  | 190/247 [00:28<00:07,  7.32it/s]predicting train subjects:  77%|███████▋  | 191/247 [00:28<00:07,  7.33it/s]predicting train subjects:  78%|███████▊  | 192/247 [00:28<00:07,  7.32it/s]predicting train subjects:  78%|███████▊  | 193/247 [00:28<00:07,  7.30it/s]predicting train subjects:  79%|███████▊  | 194/247 [00:28<00:07,  7.52it/s]predicting train subjects:  79%|███████▉  | 195/247 [00:28<00:06,  7.69it/s]predicting train subjects:  79%|███████▉  | 196/247 [00:28<00:06,  7.73it/s]predicting train subjects:  80%|███████▉  | 197/247 [00:29<00:06,  7.79it/s]predicting train subjects:  80%|████████  | 198/247 [00:29<00:06,  7.83it/s]predicting train subjects:  81%|████████  | 199/247 [00:29<00:06,  7.86it/s]predicting train subjects:  81%|████████  | 200/247 [00:29<00:06,  7.80it/s]predicting train subjects:  81%|████████▏ | 201/247 [00:29<00:05,  7.83it/s]predicting train subjects:  82%|████████▏ | 202/247 [00:29<00:05,  7.93it/s]predicting train subjects:  82%|████████▏ | 203/247 [00:29<00:05,  7.95it/s]predicting train subjects:  83%|████████▎ | 204/247 [00:29<00:05,  7.91it/s]predicting train subjects:  83%|████████▎ | 205/247 [00:30<00:05,  7.95it/s]predicting train subjects:  83%|████████▎ | 206/247 [00:30<00:05,  8.01it/s]predicting train subjects:  84%|████████▍ | 207/247 [00:30<00:05,  7.98it/s]predicting train subjects:  84%|████████▍ | 208/247 [00:30<00:04,  8.00it/s]predicting train subjects:  85%|████████▍ | 209/247 [00:30<00:04,  7.88it/s]predicting train subjects:  85%|████████▌ | 210/247 [00:30<00:04,  7.75it/s]predicting train subjects:  85%|████████▌ | 211/247 [00:30<00:04,  7.77it/s]predicting train subjects:  86%|████████▌ | 212/247 [00:30<00:04,  7.65it/s]predicting train subjects:  86%|████████▌ | 213/247 [00:31<00:04,  7.57it/s]predicting train subjects:  87%|████████▋ | 214/247 [00:31<00:04,  7.57it/s]predicting train subjects:  87%|████████▋ | 215/247 [00:31<00:04,  7.52it/s]predicting train subjects:  87%|████████▋ | 216/247 [00:31<00:04,  7.48it/s]predicting train subjects:  88%|████████▊ | 217/247 [00:31<00:04,  7.40it/s]predicting train subjects:  88%|████████▊ | 218/247 [00:31<00:03,  7.36it/s]predicting train subjects:  89%|████████▊ | 219/247 [00:31<00:03,  7.39it/s]predicting train subjects:  89%|████████▉ | 220/247 [00:32<00:03,  7.36it/s]predicting train subjects:  89%|████████▉ | 221/247 [00:32<00:03,  7.37it/s]predicting train subjects:  90%|████████▉ | 222/247 [00:32<00:03,  7.36it/s]predicting train subjects:  90%|█████████ | 223/247 [00:32<00:03,  7.38it/s]predicting train subjects:  91%|█████████ | 224/247 [00:32<00:03,  7.44it/s]predicting train subjects:  91%|█████████ | 225/247 [00:32<00:02,  7.43it/s]predicting train subjects:  91%|█████████▏| 226/247 [00:32<00:02,  7.41it/s]predicting train subjects:  92%|█████████▏| 227/247 [00:32<00:02,  7.41it/s]predicting train subjects:  92%|█████████▏| 228/247 [00:33<00:02,  7.38it/s]predicting train subjects:  93%|█████████▎| 229/247 [00:33<00:02,  7.34it/s]predicting train subjects:  93%|█████████▎| 230/247 [00:33<00:02,  7.06it/s]predicting train subjects:  94%|█████████▎| 231/247 [00:33<00:02,  6.88it/s]predicting train subjects:  94%|█████████▍| 232/247 [00:33<00:02,  6.70it/s]predicting train subjects:  94%|█████████▍| 233/247 [00:33<00:02,  6.56it/s]predicting train subjects:  95%|█████████▍| 234/247 [00:34<00:01,  6.51it/s]predicting train subjects:  95%|█████████▌| 235/247 [00:34<00:01,  6.49it/s]predicting train subjects:  96%|█████████▌| 236/247 [00:34<00:01,  6.43it/s]predicting train subjects:  96%|█████████▌| 237/247 [00:34<00:01,  6.41it/s]predicting train subjects:  96%|█████████▋| 238/247 [00:34<00:01,  6.45it/s]predicting train subjects:  97%|█████████▋| 239/247 [00:34<00:01,  6.46it/s]predicting train subjects:  97%|█████████▋| 240/247 [00:34<00:01,  6.39it/s]predicting train subjects:  98%|█████████▊| 241/247 [00:35<00:00,  6.41it/s]predicting train subjects:  98%|█████████▊| 242/247 [00:35<00:00,  6.40it/s]predicting train subjects:  98%|█████████▊| 243/247 [00:35<00:00,  6.42it/s]predicting train subjects:  99%|█████████▉| 244/247 [00:35<00:00,  6.43it/s]predicting train subjects:  99%|█████████▉| 245/247 [00:35<00:00,  6.41it/s]predicting train subjects: 100%|█████████▉| 246/247 [00:35<00:00,  6.41it/s]predicting train subjects: 100%|██████████| 247/247 [00:36<00:00,  6.38it/s]predicting train subjects: 100%|██████████| 247/247 [00:36<00:00,  6.85it/s]
saving BB  test1-THALAMUS:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 5/5 [00:00<00:00, 77.96it/s]
saving BB  train1-THALAMUS:   0%|          | 0/247 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   4%|▎         | 9/247 [00:00<00:02, 81.03it/s]saving BB  train1-THALAMUS:   7%|▋         | 18/247 [00:00<00:02, 81.83it/s]saving BB  train1-THALAMUS:  11%|█         | 27/247 [00:00<00:02, 83.07it/s]saving BB  train1-THALAMUS:  15%|█▍        | 37/247 [00:00<00:02, 85.95it/s]saving BB  train1-THALAMUS:  19%|█▉        | 47/247 [00:00<00:02, 87.84it/s]saving BB  train1-THALAMUS:  23%|██▎       | 56/247 [00:00<00:02, 88.41it/s]saving BB  train1-THALAMUS:  26%|██▋       | 65/247 [00:00<00:02, 87.33it/s]saving BB  train1-THALAMUS:  30%|██▉       | 74/247 [00:00<00:02, 85.28it/s]saving BB  train1-THALAMUS:  34%|███▎      | 83/247 [00:00<00:01, 82.58it/s]saving BB  train1-THALAMUS:  37%|███▋      | 91/247 [00:01<00:01, 78.77it/s]saving BB  train1-THALAMUS:  40%|████      | 99/247 [00:01<00:01, 76.89it/s]saving BB  train1-THALAMUS:  43%|████▎     | 107/247 [00:01<00:01, 77.54it/s]saving BB  train1-THALAMUS:  47%|████▋     | 115/247 [00:01<00:01, 78.22it/s]saving BB  train1-THALAMUS:  50%|█████     | 124/247 [00:01<00:01, 79.65it/s]saving BB  train1-THALAMUS:  54%|█████▍    | 133/247 [00:01<00:01, 80.32it/s]saving BB  train1-THALAMUS:  57%|█████▋    | 142/247 [00:01<00:01, 81.81it/s]saving BB  train1-THALAMUS:  61%|██████    | 151/247 [00:01<00:01, 83.38it/s]saving BB  train1-THALAMUS:  65%|██████▍   | 160/247 [00:01<00:01, 82.21it/s]saving BB  train1-THALAMUS:  68%|██████▊   | 169/247 [00:02<00:00, 80.44it/s]saving BB  train1-THALAMUS:  72%|███████▏  | 178/247 [00:02<00:00, 81.44it/s]saving BB  train1-THALAMUS:  76%|███████▌  | 187/247 [00:02<00:00, 81.12it/s]saving BB  train1-THALAMUS:  79%|███████▉  | 196/247 [00:02<00:00, 81.93it/s]saving BB  train1-THALAMUS:  83%|████████▎ | 205/247 [00:02<00:00, 83.91it/s]saving BB  train1-THALAMUS:  87%|████████▋ | 214/247 [00:02<00:00, 85.16it/s]saving BB  train1-THALAMUS:  90%|█████████ | 223/247 [00:02<00:00, 85.61it/s]saving BB  train1-THALAMUS:  94%|█████████▍| 232/247 [00:02<00:00, 84.64it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 241/247 [00:02<00:00, 82.86it/s]saving BB  train1-THALAMUS: 100%|██████████| 247/247 [00:02<00:00, 82.68it/s]
Loading train:   0%|          | 0/247 [00:00<?, ?it/s]Loading train:   0%|          | 1/247 [00:00<03:34,  1.15it/s]Loading train:   1%|          | 2/247 [00:01<03:27,  1.18it/s]Loading train:   1%|          | 3/247 [00:02<03:20,  1.22it/s]Loading train:   2%|▏         | 4/247 [00:03<03:25,  1.19it/s]Loading train:   2%|▏         | 5/247 [00:03<03:08,  1.29it/s]Loading train:   2%|▏         | 6/247 [00:04<02:54,  1.38it/s]Loading train:   3%|▎         | 7/247 [00:05<02:47,  1.43it/s]Loading train:   3%|▎         | 8/247 [00:05<02:40,  1.49it/s]Loading train:   4%|▎         | 9/247 [00:06<02:35,  1.53it/s]Loading train:   4%|▍         | 10/247 [00:07<02:32,  1.55it/s]Loading train:   4%|▍         | 11/247 [00:07<02:29,  1.58it/s]Loading train:   5%|▍         | 12/247 [00:08<02:27,  1.59it/s]Loading train:   5%|▌         | 13/247 [00:08<02:25,  1.60it/s]Loading train:   6%|▌         | 14/247 [00:09<02:24,  1.61it/s]Loading train:   6%|▌         | 15/247 [00:10<02:23,  1.62it/s]Loading train:   6%|▋         | 16/247 [00:10<02:21,  1.63it/s]Loading train:   7%|▋         | 17/247 [00:11<02:21,  1.62it/s]Loading train:   7%|▋         | 18/247 [00:11<02:20,  1.63it/s]Loading train:   8%|▊         | 19/247 [00:12<02:20,  1.63it/s]Loading train:   8%|▊         | 20/247 [00:13<02:19,  1.63it/s]Loading train:   9%|▊         | 21/247 [00:13<02:18,  1.63it/s]Loading train:   9%|▉         | 22/247 [00:14<02:17,  1.63it/s]Loading train:   9%|▉         | 23/247 [00:14<02:12,  1.69it/s]Loading train:  10%|▉         | 24/247 [00:15<02:08,  1.74it/s]Loading train:  10%|█         | 25/247 [00:15<02:04,  1.78it/s]Loading train:  11%|█         | 26/247 [00:16<02:02,  1.81it/s]Loading train:  11%|█         | 27/247 [00:17<02:00,  1.83it/s]Loading train:  11%|█▏        | 28/247 [00:17<02:00,  1.82it/s]Loading train:  12%|█▏        | 29/247 [00:18<01:57,  1.86it/s]Loading train:  12%|█▏        | 30/247 [00:18<01:56,  1.86it/s]Loading train:  13%|█▎        | 31/247 [00:19<01:55,  1.87it/s]Loading train:  13%|█▎        | 32/247 [00:19<01:54,  1.87it/s]Loading train:  13%|█▎        | 33/247 [00:20<01:54,  1.87it/s]Loading train:  14%|█▍        | 34/247 [00:20<01:52,  1.89it/s]Loading train:  14%|█▍        | 35/247 [00:21<01:52,  1.89it/s]Loading train:  15%|█▍        | 36/247 [00:21<01:51,  1.89it/s]Loading train:  15%|█▍        | 37/247 [00:22<01:50,  1.90it/s]Loading train:  15%|█▌        | 38/247 [00:22<01:49,  1.91it/s]Loading train:  16%|█▌        | 39/247 [00:23<01:48,  1.92it/s]Loading train:  16%|█▌        | 40/247 [00:23<01:47,  1.93it/s]Loading train:  17%|█▋        | 41/247 [00:24<01:49,  1.88it/s]Loading train:  17%|█▋        | 42/247 [00:25<01:50,  1.85it/s]Loading train:  17%|█▋        | 43/247 [00:25<01:51,  1.83it/s]Loading train:  18%|█▊        | 44/247 [00:26<01:53,  1.79it/s]Loading train:  18%|█▊        | 45/247 [00:26<01:53,  1.77it/s]Loading train:  19%|█▊        | 46/247 [00:27<01:52,  1.78it/s]Loading train:  19%|█▉        | 47/247 [00:27<01:52,  1.78it/s]Loading train:  19%|█▉        | 48/247 [00:28<01:51,  1.79it/s]Loading train:  20%|█▉        | 49/247 [00:28<01:50,  1.78it/s]Loading train:  20%|██        | 50/247 [00:29<01:50,  1.78it/s]Loading train:  21%|██        | 51/247 [00:30<01:49,  1.79it/s]Loading train:  21%|██        | 52/247 [00:30<01:49,  1.78it/s]Loading train:  21%|██▏       | 53/247 [00:31<01:48,  1.79it/s]Loading train:  22%|██▏       | 54/247 [00:31<01:47,  1.79it/s]Loading train:  22%|██▏       | 55/247 [00:32<01:48,  1.76it/s]Loading train:  23%|██▎       | 56/247 [00:32<01:48,  1.77it/s]Loading train:  23%|██▎       | 57/247 [00:33<01:47,  1.77it/s]Loading train:  23%|██▎       | 58/247 [00:34<01:45,  1.79it/s]Loading train:  24%|██▍       | 59/247 [00:34<01:48,  1.73it/s]Loading train:  24%|██▍       | 60/247 [00:35<01:49,  1.70it/s]Loading train:  25%|██▍       | 61/247 [00:35<01:50,  1.68it/s]Loading train:  25%|██▌       | 62/247 [00:36<01:50,  1.68it/s]Loading train:  26%|██▌       | 63/247 [00:37<01:50,  1.67it/s]Loading train:  26%|██▌       | 64/247 [00:37<01:51,  1.65it/s]Loading train:  26%|██▋       | 65/247 [00:38<01:50,  1.65it/s]Loading train:  27%|██▋       | 66/247 [00:38<01:50,  1.64it/s]Loading train:  27%|██▋       | 67/247 [00:39<01:49,  1.64it/s]Loading train:  28%|██▊       | 68/247 [00:40<01:50,  1.63it/s]Loading train:  28%|██▊       | 69/247 [00:40<01:49,  1.63it/s]Loading train:  28%|██▊       | 70/247 [00:41<01:48,  1.64it/s]Loading train:  29%|██▊       | 71/247 [00:42<01:48,  1.62it/s]Loading train:  29%|██▉       | 72/247 [00:42<01:48,  1.62it/s]Loading train:  30%|██▉       | 73/247 [00:43<01:48,  1.61it/s]Loading train:  30%|██▉       | 74/247 [00:43<01:47,  1.61it/s]Loading train:  30%|███       | 75/247 [00:44<01:45,  1.63it/s]Loading train:  31%|███       | 76/247 [00:45<01:45,  1.63it/s]Loading train:  31%|███       | 77/247 [00:46<02:07,  1.34it/s]Loading train:  32%|███▏      | 78/247 [00:47<02:15,  1.25it/s]Loading train:  32%|███▏      | 79/247 [00:47<02:17,  1.23it/s]Loading train:  32%|███▏      | 80/247 [00:48<02:14,  1.24it/s]Loading train:  33%|███▎      | 81/247 [00:49<02:19,  1.19it/s]Loading train:  33%|███▎      | 82/247 [00:50<02:12,  1.24it/s]Loading train:  34%|███▎      | 83/247 [00:51<02:08,  1.27it/s]Loading train:  34%|███▍      | 84/247 [00:51<02:04,  1.31it/s]Loading train:  34%|███▍      | 85/247 [00:52<02:01,  1.33it/s]Loading train:  35%|███▍      | 86/247 [00:53<01:58,  1.36it/s]Loading train:  35%|███▌      | 87/247 [00:53<01:55,  1.39it/s]Loading train:  36%|███▌      | 88/247 [00:54<01:53,  1.40it/s]Loading train:  36%|███▌      | 89/247 [00:55<01:52,  1.41it/s]Loading train:  36%|███▋      | 90/247 [00:56<01:50,  1.42it/s]Loading train:  37%|███▋      | 91/247 [00:56<01:50,  1.41it/s]Loading train:  37%|███▋      | 92/247 [00:57<01:50,  1.40it/s]Loading train:  38%|███▊      | 93/247 [00:58<01:51,  1.38it/s]Loading train:  38%|███▊      | 94/247 [00:58<01:51,  1.37it/s]Loading train:  38%|███▊      | 95/247 [00:59<01:50,  1.37it/s]Loading train:  39%|███▉      | 96/247 [01:00<01:49,  1.38it/s]Loading train:  39%|███▉      | 97/247 [01:01<01:48,  1.38it/s]Loading train:  40%|███▉      | 98/247 [01:01<01:49,  1.36it/s]Loading train:  40%|████      | 99/247 [01:02<01:49,  1.35it/s]Loading train:  40%|████      | 100/247 [01:03<01:43,  1.42it/s]Loading train:  41%|████      | 101/247 [01:03<01:38,  1.48it/s]Loading train:  41%|████▏     | 102/247 [01:04<01:35,  1.52it/s]Loading train:  42%|████▏     | 103/247 [01:05<01:33,  1.54it/s]Loading train:  42%|████▏     | 104/247 [01:05<01:32,  1.55it/s]Loading train:  43%|████▎     | 105/247 [01:06<01:30,  1.56it/s]Loading train:  43%|████▎     | 106/247 [01:06<01:29,  1.58it/s]Loading train:  43%|████▎     | 107/247 [01:07<01:28,  1.59it/s]Loading train:  44%|████▎     | 108/247 [01:08<01:27,  1.59it/s]Loading train:  44%|████▍     | 109/247 [01:08<01:26,  1.59it/s]Loading train:  45%|████▍     | 110/247 [01:09<01:25,  1.60it/s]Loading train:  45%|████▍     | 111/247 [01:10<01:25,  1.59it/s]Loading train:  45%|████▌     | 112/247 [01:10<01:25,  1.58it/s]Loading train:  46%|████▌     | 113/247 [01:11<01:24,  1.59it/s]Loading train:  46%|████▌     | 114/247 [01:11<01:23,  1.60it/s]Loading train:  47%|████▋     | 115/247 [01:12<01:22,  1.60it/s]Loading train:  47%|████▋     | 116/247 [01:13<01:21,  1.60it/s]Loading train:  47%|████▋     | 117/247 [01:13<01:20,  1.61it/s]Loading train:  48%|████▊     | 118/247 [01:14<01:20,  1.61it/s]Loading train:  48%|████▊     | 119/247 [01:15<01:18,  1.63it/s]Loading train:  49%|████▊     | 120/247 [01:15<01:18,  1.62it/s]Loading train:  49%|████▉     | 121/247 [01:16<01:17,  1.62it/s]Loading train:  49%|████▉     | 122/247 [01:16<01:17,  1.61it/s]Loading train:  50%|████▉     | 123/247 [01:17<01:17,  1.61it/s]Loading train:  50%|█████     | 124/247 [01:18<01:15,  1.62it/s]Loading train:  51%|█████     | 125/247 [01:18<01:14,  1.63it/s]Loading train:  51%|█████     | 126/247 [01:19<01:13,  1.65it/s]Loading train:  51%|█████▏    | 127/247 [01:19<01:12,  1.65it/s]Loading train:  52%|█████▏    | 128/247 [01:20<01:11,  1.66it/s]Loading train:  52%|█████▏    | 129/247 [01:21<01:11,  1.64it/s]Loading train:  53%|█████▎    | 130/247 [01:21<01:11,  1.64it/s]Loading train:  53%|█████▎    | 131/247 [01:22<01:10,  1.64it/s]Loading train:  53%|█████▎    | 132/247 [01:23<01:09,  1.64it/s]Loading train:  54%|█████▍    | 133/247 [01:23<01:10,  1.62it/s]Loading train:  54%|█████▍    | 134/247 [01:24<01:09,  1.62it/s]Loading train:  55%|█████▍    | 135/247 [01:24<01:08,  1.63it/s]Loading train:  55%|█████▌    | 136/247 [01:25<01:06,  1.66it/s]Loading train:  55%|█████▌    | 137/247 [01:26<01:04,  1.71it/s]Loading train:  56%|█████▌    | 138/247 [01:26<01:02,  1.74it/s]Loading train:  56%|█████▋    | 139/247 [01:27<01:00,  1.77it/s]Loading train:  57%|█████▋    | 140/247 [01:27<00:59,  1.79it/s]Loading train:  57%|█████▋    | 141/247 [01:28<00:58,  1.80it/s]Loading train:  57%|█████▋    | 142/247 [01:28<00:58,  1.80it/s]Loading train:  58%|█████▊    | 143/247 [01:29<00:58,  1.78it/s]Loading train:  58%|█████▊    | 144/247 [01:29<00:57,  1.79it/s]Loading train:  59%|█████▊    | 145/247 [01:30<00:56,  1.80it/s]Loading train:  59%|█████▉    | 146/247 [01:30<00:55,  1.81it/s]Loading train:  60%|█████▉    | 147/247 [01:31<00:55,  1.82it/s]Loading train:  60%|█████▉    | 148/247 [01:32<00:54,  1.80it/s]Loading train:  60%|██████    | 149/247 [01:32<00:53,  1.82it/s]Loading train:  61%|██████    | 150/247 [01:33<00:53,  1.82it/s]Loading train:  61%|██████    | 151/247 [01:33<00:52,  1.82it/s]Loading train:  62%|██████▏   | 152/247 [01:34<00:52,  1.80it/s]Loading train:  62%|██████▏   | 153/247 [01:34<00:52,  1.80it/s]Loading train:  62%|██████▏   | 154/247 [01:35<00:53,  1.75it/s]Loading train:  63%|██████▎   | 155/247 [01:36<00:53,  1.71it/s]Loading train:  63%|██████▎   | 156/247 [01:36<00:54,  1.68it/s]Loading train:  64%|██████▎   | 157/247 [01:37<00:54,  1.66it/s]Loading train:  64%|██████▍   | 158/247 [01:37<00:53,  1.66it/s]Loading train:  64%|██████▍   | 159/247 [01:38<00:53,  1.66it/s]Loading train:  65%|██████▍   | 160/247 [01:39<00:52,  1.65it/s]Loading train:  65%|██████▌   | 161/247 [01:39<00:52,  1.64it/s]Loading train:  66%|██████▌   | 162/247 [01:40<00:51,  1.66it/s]Loading train:  66%|██████▌   | 163/247 [01:40<00:50,  1.65it/s]Loading train:  66%|██████▋   | 164/247 [01:41<00:50,  1.64it/s]Loading train:  67%|██████▋   | 165/247 [01:42<00:50,  1.63it/s]Loading train:  67%|██████▋   | 166/247 [01:42<00:49,  1.62it/s]Loading train:  68%|██████▊   | 167/247 [01:43<00:50,  1.60it/s]Loading train:  68%|██████▊   | 168/247 [01:44<00:49,  1.60it/s]Loading train:  68%|██████▊   | 169/247 [01:44<00:48,  1.61it/s]Loading train:  69%|██████▉   | 170/247 [01:45<00:47,  1.61it/s]Loading train:  69%|██████▉   | 171/247 [01:45<00:46,  1.62it/s]Loading train:  70%|██████▉   | 172/247 [01:46<00:51,  1.44it/s]Loading train:  70%|███████   | 173/247 [01:47<00:53,  1.39it/s]Loading train:  70%|███████   | 174/247 [01:48<00:53,  1.36it/s]Loading train:  71%|███████   | 175/247 [01:49<00:57,  1.25it/s]Loading train:  71%|███████▏  | 176/247 [01:49<00:52,  1.36it/s]Loading train:  72%|███████▏  | 177/247 [01:50<00:48,  1.46it/s]Loading train:  72%|███████▏  | 178/247 [01:51<00:45,  1.52it/s]Loading train:  72%|███████▏  | 179/247 [01:51<00:44,  1.54it/s]Loading train:  73%|███████▎  | 180/247 [01:52<00:42,  1.58it/s]Loading train:  73%|███████▎  | 181/247 [01:52<00:41,  1.61it/s]Loading train:  74%|███████▎  | 182/247 [01:53<00:39,  1.64it/s]Loading train:  74%|███████▍  | 183/247 [01:54<00:38,  1.66it/s]Loading train:  74%|███████▍  | 184/247 [01:54<00:37,  1.66it/s]Loading train:  75%|███████▍  | 185/247 [01:55<00:37,  1.66it/s]Loading train:  75%|███████▌  | 186/247 [01:55<00:36,  1.66it/s]Loading train:  76%|███████▌  | 187/247 [01:56<00:35,  1.68it/s]Loading train:  76%|███████▌  | 188/247 [01:56<00:35,  1.68it/s]Loading train:  77%|███████▋  | 189/247 [01:57<00:34,  1.67it/s]Loading train:  77%|███████▋  | 190/247 [01:58<00:34,  1.67it/s]Loading train:  77%|███████▋  | 191/247 [01:58<00:33,  1.68it/s]Loading train:  78%|███████▊  | 192/247 [01:59<00:32,  1.69it/s]Loading train:  78%|███████▊  | 193/247 [01:59<00:31,  1.69it/s]Loading train:  79%|███████▊  | 194/247 [02:00<00:31,  1.71it/s]Loading train:  79%|███████▉  | 195/247 [02:01<00:30,  1.72it/s]Loading train:  79%|███████▉  | 196/247 [02:01<00:29,  1.73it/s]Loading train:  80%|███████▉  | 197/247 [02:02<00:28,  1.75it/s]Loading train:  80%|████████  | 198/247 [02:02<00:27,  1.75it/s]Loading train:  81%|████████  | 199/247 [02:03<00:27,  1.75it/s]Loading train:  81%|████████  | 200/247 [02:03<00:26,  1.74it/s]Loading train:  81%|████████▏ | 201/247 [02:04<00:26,  1.73it/s]Loading train:  82%|████████▏ | 202/247 [02:05<00:25,  1.74it/s]Loading train:  82%|████████▏ | 203/247 [02:05<00:25,  1.74it/s]Loading train:  83%|████████▎ | 204/247 [02:06<00:24,  1.75it/s]Loading train:  83%|████████▎ | 205/247 [02:06<00:23,  1.76it/s]Loading train:  83%|████████▎ | 206/247 [02:07<00:23,  1.78it/s]Loading train:  84%|████████▍ | 207/247 [02:07<00:22,  1.79it/s]Loading train:  84%|████████▍ | 208/247 [02:08<00:21,  1.78it/s]Loading train:  85%|████████▍ | 209/247 [02:09<00:21,  1.78it/s]Loading train:  85%|████████▌ | 210/247 [02:09<00:20,  1.78it/s]Loading train:  85%|████████▌ | 211/247 [02:10<00:20,  1.76it/s]Loading train:  86%|████████▌ | 212/247 [02:10<00:19,  1.77it/s]Loading train:  86%|████████▌ | 213/247 [02:11<00:19,  1.76it/s]Loading train:  87%|████████▋ | 214/247 [02:11<00:18,  1.76it/s]Loading train:  87%|████████▋ | 215/247 [02:12<00:18,  1.76it/s]Loading train:  87%|████████▋ | 216/247 [02:13<00:17,  1.75it/s]Loading train:  88%|████████▊ | 217/247 [02:13<00:17,  1.72it/s]Loading train:  88%|████████▊ | 218/247 [02:14<00:16,  1.71it/s]Loading train:  89%|████████▊ | 219/247 [02:14<00:16,  1.70it/s]Loading train:  89%|████████▉ | 220/247 [02:15<00:15,  1.70it/s]Loading train:  89%|████████▉ | 221/247 [02:16<00:15,  1.68it/s]Loading train:  90%|████████▉ | 222/247 [02:16<00:14,  1.68it/s]Loading train:  90%|█████████ | 223/247 [02:17<00:14,  1.69it/s]Loading train:  91%|█████████ | 224/247 [02:17<00:13,  1.67it/s]Loading train:  91%|█████████ | 225/247 [02:18<00:13,  1.65it/s]Loading train:  91%|█████████▏| 226/247 [02:19<00:12,  1.65it/s]Loading train:  92%|█████████▏| 227/247 [02:19<00:12,  1.66it/s]Loading train:  92%|█████████▏| 228/247 [02:20<00:11,  1.66it/s]Loading train:  93%|█████████▎| 229/247 [02:20<00:10,  1.67it/s]Loading train:  93%|█████████▎| 230/247 [02:21<00:10,  1.57it/s]Loading train:  94%|█████████▎| 231/247 [02:22<00:10,  1.52it/s]Loading train:  94%|█████████▍| 232/247 [02:22<00:10,  1.48it/s]Loading train:  94%|█████████▍| 233/247 [02:23<00:09,  1.47it/s]Loading train:  95%|█████████▍| 234/247 [02:24<00:08,  1.46it/s]Loading train:  95%|█████████▌| 235/247 [02:25<00:08,  1.46it/s]Loading train:  96%|█████████▌| 236/247 [02:25<00:07,  1.44it/s]Loading train:  96%|█████████▌| 237/247 [02:26<00:06,  1.44it/s]Loading train:  96%|█████████▋| 238/247 [02:27<00:06,  1.44it/s]Loading train:  97%|█████████▋| 239/247 [02:27<00:05,  1.44it/s]Loading train:  97%|█████████▋| 240/247 [02:28<00:04,  1.44it/s]Loading train:  98%|█████████▊| 241/247 [02:29<00:04,  1.44it/s]Loading train:  98%|█████████▊| 242/247 [02:29<00:03,  1.44it/s]Loading train:  98%|█████████▊| 243/247 [02:30<00:02,  1.43it/s]Loading train:  99%|█████████▉| 244/247 [02:31<00:02,  1.41it/s]Loading train:  99%|█████████▉| 245/247 [02:32<00:01,  1.41it/s]Loading train: 100%|█████████▉| 246/247 [02:32<00:00,  1.41it/s]Loading train: 100%|██████████| 247/247 [02:33<00:00,  1.41it/s]Loading train: 100%|██████████| 247/247 [02:33<00:00,  1.61it/s]
concatenating: train:   0%|          | 0/247 [00:00<?, ?it/s]concatenating: train:   3%|▎         | 8/247 [00:00<00:03, 70.56it/s]concatenating: train:   6%|▌         | 15/247 [00:00<00:03, 69.46it/s]concatenating: train:   9%|▉         | 22/247 [00:00<00:03, 69.37it/s]concatenating: train:  12%|█▏        | 30/247 [00:00<00:03, 70.91it/s]concatenating: train:  15%|█▌        | 38/247 [00:00<00:02, 72.69it/s]concatenating: train:  19%|█▊        | 46/247 [00:00<00:02, 73.76it/s]concatenating: train:  22%|██▏       | 54/247 [00:00<00:02, 72.33it/s]concatenating: train:  25%|██▌       | 62/247 [00:00<00:02, 73.00it/s]concatenating: train:  28%|██▊       | 70/247 [00:00<00:02, 72.68it/s]concatenating: train:  32%|███▏      | 78/247 [00:01<00:02, 72.21it/s]concatenating: train:  35%|███▍      | 86/247 [00:01<00:02, 69.39it/s]concatenating: train:  38%|███▊      | 93/247 [00:01<00:02, 65.99it/s]concatenating: train:  40%|████      | 100/247 [00:01<00:02, 64.41it/s]concatenating: train:  44%|████▎     | 108/247 [00:01<00:02, 67.81it/s]concatenating: train:  47%|████▋     | 116/247 [00:01<00:01, 69.33it/s]concatenating: train:  50%|████▉     | 123/247 [00:01<00:01, 68.50it/s]concatenating: train:  53%|█████▎    | 131/247 [00:01<00:01, 68.93it/s]concatenating: train:  56%|█████▋    | 139/247 [00:01<00:01, 70.30it/s]concatenating: train:  60%|█████▉    | 148/247 [00:02<00:01, 73.30it/s]concatenating: train:  63%|██████▎   | 156/247 [00:02<00:01, 73.64it/s]concatenating: train:  67%|██████▋   | 165/247 [00:02<00:01, 75.68it/s]concatenating: train:  70%|███████   | 173/247 [00:02<00:00, 75.88it/s]concatenating: train:  73%|███████▎  | 181/247 [00:02<00:00, 72.61it/s]concatenating: train:  77%|███████▋  | 189/247 [00:02<00:00, 73.73it/s]concatenating: train:  80%|███████▉  | 197/247 [00:02<00:00, 74.27it/s]concatenating: train:  83%|████████▎ | 205/247 [00:02<00:00, 74.53it/s]concatenating: train:  86%|████████▌ | 213/247 [00:02<00:00, 75.12it/s]concatenating: train:  89%|████████▉ | 221/247 [00:03<00:00, 69.39it/s]concatenating: train:  93%|█████████▎| 229/247 [00:03<00:00, 63.04it/s]concatenating: train:  96%|█████████▌| 236/247 [00:03<00:00, 64.09it/s]concatenating: train:  99%|█████████▉| 244/247 [00:03<00:00, 66.07it/s]concatenating: train: 100%|██████████| 247/247 [00:03<00:00, 70.49it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:12<00:48, 12.20s/it]Loading test:  40%|████      | 2/5 [00:26<00:38, 12.75s/it]Loading test:  60%|██████    | 3/5 [00:31<00:21, 10.54s/it]Loading test:  80%|████████  | 4/5 [00:35<00:08,  8.49s/it]Loading test: 100%|██████████| 5/5 [00:45<00:00,  8.93s/it]Loading test: 100%|██████████| 5/5 [00:45<00:00,  9.06s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 52.83it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 48, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 48, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 48, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 48, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 48, 52, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 48, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 48, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 24, 26, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 24, 26, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 24, 26, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 24, 26, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 24, 26, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 24, 26, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 24, 26, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 24, 26, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 24, 26, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 12, 13, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 12, 13, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 12, 13, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 12, 13, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 12, 13, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 12, 13, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 12, 13, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 12, 13, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 12, 13, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 12, 13, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 24, 26, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 24, 26, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 24, 26, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 24, 26, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 24, 26, 60)   0           batch_normalization_7[0][0]      2020-01-22 13:49:31.607887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 13:49:31.607991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 13:49:31.608006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 13:49:31.608015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 13:49:31.608287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 24, 26, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 24, 26, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 24, 26, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 24, 26, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24, 26, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 48, 52, 30)   25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 48, 52, 60)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 48, 52, 30)   16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 48, 52, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 48, 52, 30)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 48, 52, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 48, 52, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 48, 52, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 48, 52, 90)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 48, 52, 90)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 48, 52, 13)   1183        dropout_5[0][0]                  
==================================================================================================
Total params: 501,343
Trainable params: 500,143
Non-trainable params: 1,200
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1
------------------------------------------------------------------
class_weights [6.53189770e-02 3.19814431e-02 7.71730360e-02 9.62167760e-03
 2.75476684e-02 7.05539808e-03 8.86693588e-02 1.14564993e-01
 8.20919994e-02 1.27739871e-02 2.89952532e-01 1.92996529e-01
 2.52399887e-04]
Train on 15605 samples, validate on 322 samples
Epoch 1/300
 - 34s - loss: 0.6561 - acc: 0.9077 - mDice: 0.2926 - val_loss: 0.5084 - val_acc: 0.9366 - val_mDice: 0.1863

Epoch 00001: val_mDice improved from -inf to 0.18628, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300
 - 30s - loss: 0.4978 - acc: 0.9308 - mDice: 0.4632 - val_loss: 0.2068 - val_acc: 0.9437 - val_mDice: 0.2100

Epoch 00002: val_mDice improved from 0.18628 to 0.21000, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300
 - 30s - loss: 0.4368 - acc: 0.9354 - mDice: 0.5291 - val_loss: 0.0788 - val_acc: 0.9466 - val_mDice: 0.2169

Epoch 00003: val_mDice improved from 0.21000 to 0.21686, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 4/300
 - 30s - loss: 0.4166 - acc: 0.9381 - mDice: 0.5509 - val_loss: -4.5750e-03 - val_acc: 0.9479 - val_mDice: 0.2159

Epoch 00004: val_mDice did not improve from 0.21686
Epoch 5/300
 - 30s - loss: 0.4002 - acc: 0.9400 - mDice: 0.5687 - val_loss: -6.0757e-03 - val_acc: 0.9481 - val_mDice: 0.2166

Epoch 00005: val_mDice did not improve from 0.21686
Epoch 6/300
 - 30s - loss: 0.3916 - acc: 0.9412 - mDice: 0.5779 - val_loss: -2.9972e-02 - val_acc: 0.9491 - val_mDice: 0.2204

Epoch 00006: val_mDice improved from 0.21686 to 0.22044, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 7/300
 - 30s - loss: 0.3865 - acc: 0.9419 - mDice: 0.5835 - val_loss: -3.4279e-02 - val_acc: 0.9500 - val_mDice: 0.2226

Epoch 00007: val_mDice improved from 0.22044 to 0.22259, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 8/300
 - 30s - loss: 0.3747 - acc: 0.9429 - mDice: 0.5962 - val_loss: -2.3843e-02 - val_acc: 0.9499 - val_mDice: 0.2228

Epoch 00008: val_mDice improved from 0.22259 to 0.22283, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 9/300
 - 30s - loss: 0.3693 - acc: 0.9436 - mDice: 0.6021 - val_loss: -4.5093e-02 - val_acc: 0.9503 - val_mDice: 0.2235

Epoch 00009: val_mDice improved from 0.22283 to 0.22348, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 10/300
 - 30s - loss: 0.3677 - acc: 0.9441 - mDice: 0.6038 - val_loss: -8.3577e-02 - val_acc: 0.9508 - val_mDice: 0.2242

Epoch 00010: val_mDice improved from 0.22348 to 0.22423, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 11/300
 - 30s - loss: 0.3614 - acc: 0.9447 - mDice: 0.6106 - val_loss: -3.8908e-02 - val_acc: 0.9505 - val_mDice: 0.2232

Epoch 00011: val_mDice did not improve from 0.22423
Epoch 12/300
 - 30s - loss: 0.3567 - acc: 0.9450 - mDice: 0.6156 - val_loss: -9.5345e-02 - val_acc: 0.9504 - val_mDice: 0.2233

Epoch 00012: val_mDice did not improve from 0.22423
Epoch 13/300
 - 30s - loss: 0.3538 - acc: 0.9455 - mDice: 0.6188 - val_loss: -6.8428e-02 - val_acc: 0.9515 - val_mDice: 0.2207

Epoch 00013: val_mDice did not improve from 0.22423
Epoch 14/300
 - 30s - loss: 0.3512 - acc: 0.9460 - mDice: 0.6216 - val_loss: -7.6436e-02 - val_acc: 0.9515 - val_mDice: 0.2225

Epoch 00014: val_mDice did not improve from 0.22423
Epoch 15/300
 - 30s - loss: 0.3476 - acc: 0.9463 - mDice: 0.6254 - val_loss: -6.8918e-02 - val_acc: 0.9513 - val_mDice: 0.2240

Epoch 00015: val_mDice did not improve from 0.22423
Epoch 16/300
 - 30s - loss: 0.3434 - acc: 0.9467 - mDice: 0.6300 - val_loss: -1.0670e-01 - val_acc: 0.9515 - val_mDice: 0.2229

Epoch 00016: val_mDice did not improve from 0.22423
Epoch 17/300
 - 30s - loss: 0.3412 - acc: 0.9470 - mDice: 0.6323 - val_loss: -9.2984e-02 - val_acc: 0.9512 - val_mDice: 0.2226

Epoch 00017: val_mDice did not improve from 0.22423
Epoch 18/300
 - 30s - loss: 0.3367 - acc: 0.9473 - mDice: 0.6372 - val_loss: -1.0380e-01 - val_acc: 0.9522 - val_mDice: 0.2190

Epoch 00018: val_mDice did not improve from 0.22423
Epoch 19/300
 - 30s - loss: 0.3397 - acc: 0.9475 - mDice: 0.6340 - val_loss: -8.2175e-02 - val_acc: 0.9517 - val_mDice: 0.2219

Epoch 00019: val_mDice did not improve from 0.22423
Epoch 20/300
 - 30s - loss: 0.3324 - acc: 0.9479 - mDice: 0.6418 - val_loss: -9.5650e-02 - val_acc: 0.9516 - val_mDice: 0.2211

Epoch 00020: val_mDice did not improve from 0.22423
Epoch 21/300
 - 30s - loss: 0.3315 - acc: 0.9482 - mDice: 0.6428 - val_loss: -6.9035e-02 - val_acc: 0.9510 - val_mDice: 0.2209

Epoch 00021: val_mDice did not improve from 0.22423
Epoch 22/300
 - 30s - loss: 0.3295 - acc: 0.9483 - mDice: 0.6450 - val_loss: -7.7586e-02 - val_acc: 0.9517 - val_mDice: 0.2201

Epoch 00022: val_mDice did not improve from 0.22423
Epoch 23/300
 - 30s - loss: 0.3282 - acc: 0.9486 - mDice: 0.6463 - val_loss: -8.9656e-02 - val_acc: 0.9523 - val_mDice: 0.2211

Epoch 00023: val_mDice did not improve from 0.22423
Epoch 24/300
 - 30s - loss: 0.3248 - acc: 0.9488 - mDice: 0.6501 - val_loss: -1.1087e-01 - val_acc: 0.9522 - val_mDice: 0.2200

Epoch 00024: val_mDice did not improve from 0.22423
Epoch 25/300
 - 30s - loss: 0.3262 - acc: 0.9490 - mDice: 0.6486 - val_loss: -1.1913e-01 - val_acc: 0.9519 - val_mDice: 0.2203

Epoch 00025: val_mDice did not improve from 0.22423

Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 26/300
 - 30s - loss: 0.3207 - acc: 0.9493 - mDice: 0.6545 - val_loss: -1.1325e-01 - val_acc: 0.9527 - val_mDice: 0.2195

Epoch 00026: val_mDice did not improve from 0.22423
Epoch 27/300
 - 30s - loss: 0.3222 - acc: 0.9495 - mDice: 0.6529 - val_loss: -1.2280e-01 - val_acc: 0.9525 - val_mDice: 0.2209

Epoch 00027: val_mDice did not improve from 0.22423
Epoch 28/300
 - 30s - loss: 0.3183 - acc: 0.9495 - mDice: 0.6571 - val_loss: -1.0292e-01 - val_acc: 0.9523 - val_mDice: 0.2181

Epoch 00028: val_mDice did not improve from 0.22423
Epoch 29/300
 - 30s - loss: 0.3192 - acc: 0.9497 - mDice: 0.6561 - val_loss: -1.2256e-01 - val_acc: 0.9526 - val_mDice: 0.2196

Epoch 00029: val_mDice did not improve from 0.22423
Epoch 30/300
 - 30s - loss: 0.3193 - acc: 0.9498 - mDice: 0.6560 - val_loss: -1.1445e-01 - val_acc: 0.9520 - val_mDice: 0.2190

Epoch 00030: val_mDice did not improve from 0.22423
Epoch 31/300
 - 30s - loss: 0.3176 - acc: 0.9499 - mDice: 0.6579 - val_loss: -1.0669e-01 - val_acc: 0.9529 - val_mDice: 0.2209

Epoch 00031: val_mDice did not improve from 0.22423
Epoch 32/300
 - 30s - loss: 0.3152 - acc: 0.9500 - mDice: 0.6604 - val_loss: -1.2605e-01 - val_acc: 0.9525 - val_mDice: 0.2205

Epoch 00032: val_mDice did not improve from 0.22423
Epoch 33/300
 - 30s - loss: 0.3151 - acc: 0.9501 - mDice: 0.6606 - val_loss: -1.3187e-01 - val_acc: 0.9526 - val_mDice: 0.2191

Epoch 00033: val_mDice did not improve from 0.22423
Epoch 34/300
 - 30s - loss: 0.3147 - acc: 0.9502 - mDice: 0.6610 - val_loss: -1.2702e-01 - val_acc: 0.9524 - val_mDice: 0.2168

Epoch 00034: val_mDice did not improve from 0.22423
Epoch 35/300
 - 30s - loss: 0.3133 - acc: 0.9502 - mDice: 0.6625 - val_loss: -1.1846e-01 - val_acc: 0.9525 - val_mDice: 0.2182

Epoch 00035: val_mDice did not improve from 0.22423
Epoch 36/300
 - 29s - loss: 0.3136 - acc: 0.9502 - mDice: 0.6621 - val_loss: -1.2972e-01 - val_acc: 0.9528 - val_mDice: 0.2195

Epoch 00036: val_mDice did not improve from 0.22423
Epoch 37/300
 - 30s - loss: 0.3092 - acc: 0.9504 - mDice: 0.6670 - val_loss: -1.1966e-01 - val_acc: 0.9525 - val_mDice: 0.2196

Epoch 00037: val_mDice did not improve from 0.22423
Epoch 38/300
 - 30s - loss: 0.3086 - acc: 0.9506 - mDice: 0.6675 - val_loss: -1.2139e-01 - val_acc: 0.9524 - val_mDice: 0.2188

Epoch 00038: val_mDice did not improve from 0.22423
Epoch 39/300
 - 31s - loss: 0.3105 - acc: 0.9506 - mDice: 0.6655 - val_loss: -1.2815e-01 - val_acc: 0.9528 - val_mDice: 0.2180

Epoch 00039: val_mDice did not improve from 0.22423
Epoch 40/300
 - 30s - loss: 0.3106 - acc: 0.9506 - mDice: 0.6654 - val_loss: -1.1841e-01 - val_acc: 0.9523 - val_mDice: 0.2182

Epoch 00040: val_mDice did not improve from 0.22423

Epoch 00040: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 41/300
 - 30s - loss: 0.3087 - acc: 0.9508 - mDice: 0.6675 - val_loss: -1.2237e-01 - val_acc: 0.9526 - val_mDice: 0.2192

Epoch 00041: val_mDice did not improve from 0.22423
Epoch 42/300
 - 30s - loss: 0.3053 - acc: 0.9509 - mDice: 0.6712 - val_loss: -1.3698e-01 - val_acc: 0.9526 - val_mDice: 0.2180

Epoch 00042: val_mDice did not improve from 0.22423
Epoch 43/300
 - 30s - loss: 0.3097 - acc: 0.9509 - mDice: 0.6663 - val_loss: -1.3511e-01 - val_acc: 0.9525 - val_mDice: 0.2193

Epoch 00043: val_mDice did not improve from 0.22423
Epoch 44/300
 - 30s - loss: 0.3089 - acc: 0.9509 - mDice: 0.6672 - val_loss: -1.3784e-01 - val_acc: 0.9530 - val_mDice: 0.2192

Epoch 00044: val_mDice did not improve from 0.22423
Epoch 45/300
 - 30s - loss: 0.3077 - acc: 0.9509 - mDice: 0.6685 - val_loss: -1.5011e-01 - val_acc: 0.9525 - val_mDice: 0.2182

Epoch 00045: val_mDice did not improve from 0.22423
Epoch 46/300
 - 30s - loss: 0.3060 - acc: 0.9510 - mDice: 0.6703 - val_loss: -1.4422e-01 - val_acc: 0.9531 - val_mDice: 0.2190

Epoch 00046: val_mDice did not improve from 0.22423
Epoch 47/300
 - 30s - loss: 0.3096 - acc: 0.9511 - mDice: 0.6665 - val_loss: -1.4524e-01 - val_acc: 0.9527 - val_mDice: 0.2175

Epoch 00047: val_mDice did not improve from 0.22423
Epoch 48/300
 - 30s - loss: 0.3055 - acc: 0.9511 - mDice: 0.6710 - val_loss: -1.4164e-01 - val_acc: 0.9525 - val_mDice: 0.2183

Epoch 00048: val_mDice did not improve from 0.22423
Epoch 49/300
 - 30s - loss: 0.3072 - acc: 0.9511 - mDice: 0.6691 - val_loss: -1.3943e-01 - val_acc: 0.9528 - val_mDice: 0.2177

Epoch 00049: val_mDice did not improve from 0.22423
Epoch 50/300
 - 30s - loss: 0.3063 - acc: 0.9512 - mDice: 0.6701 - val_loss: -1.6370e-01 - val_acc: 0.9531 - val_mDice: 0.2172

predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:01<00:05,  1.45s/it]predicting test subjects:  40%|████      | 2/5 [00:02<00:04,  1.36s/it]predicting test subjects:  60%|██████    | 3/5 [00:03<00:02,  1.28s/it]predicting test subjects:  80%|████████  | 4/5 [00:04<00:01,  1.14s/it]predicting test subjects: 100%|██████████| 5/5 [00:05<00:00,  1.07s/it]predicting test subjects: 100%|██████████| 5/5 [00:05<00:00,  1.08s/it]
Loading train:   0%|          | 0/247 [00:00<?, ?it/s]Loading train:   0%|          | 1/247 [00:00<00:57,  4.26it/s]Loading train:   1%|          | 2/247 [00:00<00:57,  4.29it/s]Loading train:   1%|          | 3/247 [00:00<00:56,  4.34it/s]Loading train:   2%|▏         | 4/247 [00:00<00:56,  4.32it/s]Loading train:   2%|▏         | 5/247 [00:01<00:54,  4.41it/s]Loading train:   2%|▏         | 6/247 [00:01<00:53,  4.47it/s]Loading train:   3%|▎         | 7/247 [00:01<00:53,  4.48it/s]Loading train:   3%|▎         | 8/247 [00:01<00:53,  4.47it/s]Loading train:   4%|▎         | 9/247 [00:02<00:52,  4.50it/s]Loading train:   4%|▍         | 10/247 [00:02<00:52,  4.54it/s]Loading train:   4%|▍         | 11/247 [00:02<00:51,  4.55it/s]Loading train:   5%|▍         | 12/247 [00:02<00:52,  4.46it/s]Loading train:   5%|▌         | 13/247 [00:02<00:52,  4.43it/s]Loading train:   6%|▌         | 14/247 [00:03<00:52,  4.43it/s]Loading train:   6%|▌         | 15/247 [00:03<00:52,  4.46it/s]Loading train:   6%|▋         | 16/247 [00:03<00:51,  4.51it/s]Loading train:   7%|▋         | 17/247 [00:03<00:50,  4.55it/s]Loading train:   7%|▋         | 18/247 [00:04<00:49,  4.60it/s]Loading train:   8%|▊         | 19/247 [00:04<00:49,  4.63it/s]Loading train:   8%|▊         | 20/247 [00:04<00:48,  4.65it/s]Loading train:   9%|▊         | 21/247 [00:04<00:48,  4.66it/s]Loading train:   9%|▉         | 22/247 [00:04<00:48,  4.67it/s]Loading train:   9%|▉         | 23/247 [00:05<00:47,  4.69it/s]Loading train:  10%|▉         | 24/247 [00:05<00:47,  4.71it/s]Loading train:  10%|█         | 25/247 [00:05<00:47,  4.71it/s]Loading train:  11%|█         | 26/247 [00:05<00:46,  4.73it/s]Loading train:  11%|█         | 27/247 [00:05<00:46,  4.75it/s]Loading train:  11%|█▏        | 28/247 [00:06<00:46,  4.71it/s]Loading train:  12%|█▏        | 29/247 [00:06<00:46,  4.71it/s]Loading train:  12%|█▏        | 30/247 [00:06<00:46,  4.71it/s]Loading train:  13%|█▎        | 31/247 [00:06<00:46,  4.70it/s]Loading train:  13%|█▎        | 32/247 [00:06<00:45,  4.70it/s]Loading train:  13%|█▎        | 33/247 [00:07<00:45,  4.69it/s]Loading train:  14%|█▍        | 34/247 [00:07<00:45,  4.70it/s]Loading train:  14%|█▍        | 35/247 [00:07<00:45,  4.65it/s]Loading train:  15%|█▍        | 36/247 [00:07<00:45,  4.65it/s]Loading train:  15%|█▍        | 37/247 [00:08<00:45,  4.64it/s]Loading train:  15%|█▌        | 38/247 [00:08<00:44,  4.66it/s]Loading train:  16%|█▌        | 39/247 [00:08<00:44,  4.67it/s]Loading train:  16%|█▌        | 40/247 [00:08<00:44,  4.70it/s]Loading train:  17%|█▋        | 41/247 [00:08<00:43,  4.69it/s]Loading train:  17%|█▋        | 42/247 [00:09<00:44,  4.61it/s]Loading train:  17%|█▋        | 43/247 [00:09<00:45,  4.52it/s]Loading train:  18%|█▊        | 44/247 [00:09<00:45,  4.44it/s]Loading train:  18%|█▊        | 45/247 [00:09<00:44,  4.51it/s]Loading train:  19%|█▊        | 46/247 [00:10<00:44,  4.56it/s]Loading train:  19%|█▉        | 47/247 [00:10<00:44,  4.54it/s]Loading train:  19%|█▉        | 48/247 [00:10<00:43,  4.54it/s]Loading train:  20%|█▉        | 49/247 [00:10<00:43,  4.50it/s]Loading train:  20%|██        | 50/247 [00:10<00:43,  4.49it/s]Loading train:  21%|██        | 51/247 [00:11<00:43,  4.51it/s]Loading train:  21%|██        | 52/247 [00:11<00:43,  4.53it/s]Loading train:  21%|██▏       | 53/247 [00:11<00:42,  4.58it/s]Loading train:  22%|██▏       | 54/247 [00:11<00:41,  4.63it/s]Loading train:  22%|██▏       | 55/247 [00:11<00:41,  4.67it/s]Loading train:  23%|██▎       | 56/247 [00:12<00:40,  4.71it/s]Loading train:  23%|██▎       | 57/247 [00:12<00:40,  4.68it/s]Loading train:  23%|██▎       | 58/247 [00:12<00:40,  4.68it/s]Loading train:  24%|██▍       | 59/247 [00:12<00:40,  4.66it/s]Loading train:  24%|██▍       | 60/247 [00:13<00:40,  4.63it/s]Loading train:  25%|██▍       | 61/247 [00:13<00:40,  4.62it/s]Loading train:  25%|██▌       | 62/247 [00:13<00:40,  4.62it/s]Loading train:  26%|██▌       | 63/247 [00:13<00:39,  4.60it/s]Loading train:  26%|██▌       | 64/247 [00:13<00:39,  4.59it/s]Loading train:  26%|██▋       | 65/247 [00:14<00:40,  4.54it/s]Loading train:  27%|██▋       | 66/247 [00:14<00:40,  4.52it/s]Loading train:  27%|██▋       | 67/247 [00:14<00:39,  4.55it/s]Loading train:  28%|██▊       | 68/247 [00:14<00:39,  4.53it/s]Loading train:  28%|██▊       | 69/247 [00:15<00:39,  4.56it/s]Loading train:  28%|██▊       | 70/247 [00:15<00:38,  4.60it/s]Loading train:  29%|██▊       | 71/247 [00:15<00:38,  4.59it/s]Loading train:  29%|██▉       | 72/247 [00:15<00:38,  4.57it/s]Loading train:  30%|██▉       | 73/247 [00:15<00:37,  4.59it/s]Loading train:  30%|██▉       | 74/247 [00:16<00:37,  4.58it/s]Loading train:  30%|███       | 75/247 [00:16<00:37,  4.60it/s]Loading train:  31%|███       | 76/247 [00:16<00:37,  4.61it/s]Loading train:  31%|███       | 77/247 [00:16<00:39,  4.32it/s]Loading train:  32%|███▏      | 78/247 [00:17<00:40,  4.20it/s]Loading train:  32%|███▏      | 79/247 [00:17<00:38,  4.34it/s]Loading train:  32%|███▏      | 80/247 [00:17<00:38,  4.37it/s]Loading train:  33%|███▎      | 81/247 [00:17<00:39,  4.21it/s]Loading train:  33%|███▎      | 82/247 [00:18<00:40,  4.12it/s]Loading train:  34%|███▎      | 83/247 [00:18<00:40,  4.08it/s]Loading train:  34%|███▍      | 84/247 [00:18<00:40,  4.03it/s]Loading train:  34%|███▍      | 85/247 [00:18<00:40,  3.99it/s]Loading train:  35%|███▍      | 86/247 [00:19<00:40,  3.98it/s]Loading train:  35%|███▌      | 87/247 [00:19<00:40,  3.97it/s]Loading train:  36%|███▌      | 88/247 [00:19<00:40,  3.95it/s]Loading train:  36%|███▌      | 89/247 [00:19<00:40,  3.94it/s]Loading train:  36%|███▋      | 90/247 [00:20<00:39,  3.96it/s]Loading train:  37%|███▋      | 91/247 [00:20<00:39,  4.00it/s]Loading train:  37%|███▋      | 92/247 [00:20<00:38,  4.02it/s]Loading train:  38%|███▊      | 93/247 [00:20<00:38,  4.04it/s]Loading train:  38%|███▊      | 94/247 [00:21<00:38,  4.01it/s]Loading train:  38%|███▊      | 95/247 [00:21<00:37,  4.01it/s]Loading train:  39%|███▉      | 96/247 [00:21<00:37,  4.00it/s]Loading train:  39%|███▉      | 97/247 [00:21<00:37,  4.01it/s]Loading train:  40%|███▉      | 98/247 [00:22<00:37,  3.99it/s]Loading train:  40%|████      | 99/247 [00:22<00:37,  4.00it/s]Loading train:  40%|████      | 100/247 [00:22<00:36,  4.06it/s]Loading train:  41%|████      | 101/247 [00:22<00:35,  4.11it/s]Loading train:  41%|████▏     | 102/247 [00:23<00:35,  4.13it/s]Loading train:  42%|████▏     | 103/247 [00:23<00:34,  4.15it/s]Loading train:  42%|████▏     | 104/247 [00:23<00:34,  4.16it/s]Loading train:  43%|████▎     | 105/247 [00:23<00:34,  4.16it/s]Loading train:  43%|████▎     | 106/247 [00:23<00:33,  4.18it/s]Loading train:  43%|████▎     | 107/247 [00:24<00:33,  4.21it/s]Loading train:  44%|████▎     | 108/247 [00:24<00:32,  4.23it/s]Loading train:  44%|████▍     | 109/247 [00:24<00:32,  4.23it/s]Loading train:  45%|████▍     | 110/247 [00:24<00:32,  4.22it/s]Loading train:  45%|████▍     | 111/247 [00:25<00:32,  4.20it/s]Loading train:  45%|████▌     | 112/247 [00:25<00:32,  4.19it/s]Loading train:  46%|████▌     | 113/247 [00:25<00:31,  4.21it/s]Loading train:  46%|████▌     | 114/247 [00:25<00:31,  4.22it/s]Loading train:  47%|████▋     | 115/247 [00:26<00:31,  4.19it/s]Loading train:  47%|████▋     | 116/247 [00:26<00:31,  4.17it/s]Loading train:  47%|████▋     | 117/247 [00:26<00:31,  4.15it/s]Loading train:  48%|████▊     | 118/247 [00:26<00:29,  4.35it/s]Loading train:  48%|████▊     | 119/247 [00:26<00:28,  4.53it/s]Loading train:  49%|████▊     | 120/247 [00:27<00:27,  4.59it/s]Loading train:  49%|████▉     | 121/247 [00:27<00:26,  4.69it/s]Loading train:  49%|████▉     | 122/247 [00:27<00:26,  4.77it/s]Loading train:  50%|████▉     | 123/247 [00:27<00:25,  4.85it/s]Loading train:  50%|█████     | 124/247 [00:28<00:25,  4.89it/s]Loading train:  51%|█████     | 125/247 [00:28<00:24,  4.92it/s]Loading train:  51%|█████     | 126/247 [00:28<00:24,  4.94it/s]Loading train:  51%|█████▏    | 127/247 [00:28<00:24,  4.95it/s]Loading train:  52%|█████▏    | 128/247 [00:28<00:24,  4.93it/s]Loading train:  52%|█████▏    | 129/247 [00:29<00:23,  4.95it/s]Loading train:  53%|█████▎    | 130/247 [00:29<00:23,  4.97it/s]Loading train:  53%|█████▎    | 131/247 [00:29<00:23,  4.95it/s]Loading train:  53%|█████▎    | 132/247 [00:29<00:23,  4.95it/s]Loading train:  54%|█████▍    | 133/247 [00:29<00:23,  4.96it/s]Loading train:  54%|█████▍    | 134/247 [00:30<00:22,  4.97it/s]Loading train:  55%|█████▍    | 135/247 [00:30<00:22,  4.98it/s]Loading train:  55%|█████▌    | 136/247 [00:30<00:22,  4.95it/s]Loading train:  55%|█████▌    | 137/247 [00:30<00:22,  4.90it/s]Loading train:  56%|█████▌    | 138/247 [00:30<00:22,  4.81it/s]Loading train:  56%|█████▋    | 139/247 [00:31<00:22,  4.81it/s]Loading train:  57%|█████▋    | 140/247 [00:31<00:22,  4.79it/s]Loading train:  57%|█████▋    | 141/247 [00:31<00:22,  4.79it/s]Loading train:  57%|█████▋    | 142/247 [00:31<00:21,  4.79it/s]Loading train:  58%|█████▊    | 143/247 [00:31<00:21,  4.80it/s]Loading train:  58%|█████▊    | 144/247 [00:32<00:21,  4.77it/s]Loading train:  59%|█████▊    | 145/247 [00:32<00:21,  4.77it/s]Loading train:  59%|█████▉    | 146/247 [00:32<00:21,  4.79it/s]Loading train:  60%|█████▉    | 147/247 [00:32<00:20,  4.79it/s]Loading train:  60%|█████▉    | 148/247 [00:32<00:20,  4.78it/s]Loading train:  60%|██████    | 149/247 [00:33<00:20,  4.79it/s]Loading train:  61%|██████    | 150/247 [00:33<00:20,  4.78it/s]Loading train:  61%|██████    | 151/247 [00:33<00:20,  4.79it/s]Loading train:  62%|██████▏   | 152/247 [00:33<00:19,  4.81it/s]Loading train:  62%|██████▏   | 153/247 [00:33<00:19,  4.81it/s]Loading train:  62%|██████▏   | 154/247 [00:34<00:20,  4.60it/s]Loading train:  63%|██████▎   | 155/247 [00:34<00:20,  4.49it/s]Loading train:  63%|██████▎   | 156/247 [00:34<00:22,  3.98it/s]Loading train:  64%|██████▎   | 157/247 [00:35<00:22,  4.05it/s]Loading train:  64%|██████▍   | 158/247 [00:35<00:21,  4.09it/s]Loading train:  64%|██████▍   | 159/247 [00:35<00:21,  4.12it/s]Loading train:  65%|██████▍   | 160/247 [00:35<00:20,  4.16it/s]Loading train:  65%|██████▌   | 161/247 [00:35<00:20,  4.20it/s]Loading train:  66%|██████▌   | 162/247 [00:36<00:20,  4.22it/s]Loading train:  66%|██████▌   | 163/247 [00:36<00:19,  4.22it/s]Loading train:  66%|██████▋   | 164/247 [00:36<00:19,  4.22it/s]Loading train:  67%|██████▋   | 165/247 [00:36<00:19,  4.20it/s]Loading train:  67%|██████▋   | 166/247 [00:37<00:19,  4.19it/s]Loading train:  68%|██████▊   | 167/247 [00:37<00:19,  4.20it/s]Loading train:  68%|██████▊   | 168/247 [00:37<00:18,  4.22it/s]Loading train:  68%|██████▊   | 169/247 [00:37<00:18,  4.20it/s]Loading train:  69%|██████▉   | 170/247 [00:38<00:18,  4.23it/s]Loading train:  69%|██████▉   | 171/247 [00:38<00:17,  4.23it/s]Loading train:  70%|██████▉   | 172/247 [00:38<00:17,  4.29it/s]Loading train:  70%|███████   | 173/247 [00:38<00:16,  4.38it/s]Loading train:  70%|███████   | 174/247 [00:38<00:16,  4.42it/s]Loading train:  71%|███████   | 175/247 [00:39<00:16,  4.30it/s]Loading train:  71%|███████▏  | 176/247 [00:39<00:16,  4.42it/s]Loading train:  72%|███████▏  | 177/247 [00:39<00:15,  4.51it/s]Loading train:  72%|███████▏  | 178/247 [00:39<00:15,  4.58it/s]Loading train:  72%|███████▏  | 179/247 [00:40<00:14,  4.63it/s]Loading train:  73%|███████▎  | 180/247 [00:40<00:14,  4.64it/s]Loading train:  73%|███████▎  | 181/247 [00:40<00:14,  4.68it/s]Loading train:  74%|███████▎  | 182/247 [00:40<00:13,  4.71it/s]Loading train:  74%|███████▍  | 183/247 [00:40<00:13,  4.71it/s]Loading train:  74%|███████▍  | 184/247 [00:41<00:13,  4.73it/s]Loading train:  75%|███████▍  | 185/247 [00:41<00:13,  4.72it/s]Loading train:  75%|███████▌  | 186/247 [00:41<00:12,  4.74it/s]Loading train:  76%|███████▌  | 187/247 [00:41<00:12,  4.71it/s]Loading train:  76%|███████▌  | 188/247 [00:41<00:12,  4.68it/s]Loading train:  77%|███████▋  | 189/247 [00:42<00:12,  4.69it/s]Loading train:  77%|███████▋  | 190/247 [00:42<00:12,  4.74it/s]Loading train:  77%|███████▋  | 191/247 [00:42<00:11,  4.76it/s]Loading train:  78%|███████▊  | 192/247 [00:42<00:11,  4.71it/s]Loading train:  78%|███████▊  | 193/247 [00:43<00:11,  4.71it/s]Loading train:  79%|███████▊  | 194/247 [00:43<00:11,  4.72it/s]Loading train:  79%|███████▉  | 195/247 [00:43<00:10,  4.79it/s]Loading train:  79%|███████▉  | 196/247 [00:43<00:10,  4.84it/s]Loading train:  80%|███████▉  | 197/247 [00:43<00:10,  4.89it/s]Loading train:  80%|████████  | 198/247 [00:44<00:09,  4.92it/s]Loading train:  81%|████████  | 199/247 [00:44<00:09,  4.93it/s]Loading train:  81%|████████  | 200/247 [00:44<00:09,  4.93it/s]Loading train:  81%|████████▏ | 201/247 [00:44<00:09,  4.95it/s]Loading train:  82%|████████▏ | 202/247 [00:44<00:09,  4.92it/s]Loading train:  82%|████████▏ | 203/247 [00:45<00:08,  4.91it/s]Loading train:  83%|████████▎ | 204/247 [00:45<00:08,  4.91it/s]Loading train:  83%|████████▎ | 205/247 [00:45<00:08,  4.92it/s]Loading train:  83%|████████▎ | 206/247 [00:45<00:08,  4.92it/s]Loading train:  84%|████████▍ | 207/247 [00:45<00:08,  4.92it/s]Loading train:  84%|████████▍ | 208/247 [00:46<00:07,  4.96it/s]Loading train:  85%|████████▍ | 209/247 [00:46<00:07,  4.94it/s]Loading train:  85%|████████▌ | 210/247 [00:46<00:07,  4.90it/s]Loading train:  85%|████████▌ | 211/247 [00:46<00:07,  4.92it/s]Loading train:  86%|████████▌ | 212/247 [00:46<00:07,  4.86it/s]Loading train:  86%|████████▌ | 213/247 [00:47<00:07,  4.79it/s]Loading train:  87%|████████▋ | 214/247 [00:47<00:06,  4.73it/s]Loading train:  87%|████████▋ | 215/247 [00:47<00:06,  4.72it/s]Loading train:  87%|████████▋ | 216/247 [00:47<00:06,  4.69it/s]Loading train:  88%|████████▊ | 217/247 [00:47<00:06,  4.66it/s]Loading train:  88%|████████▊ | 218/247 [00:48<00:06,  4.67it/s]Loading train:  89%|████████▊ | 219/247 [00:48<00:06,  4.61it/s]Loading train:  89%|████████▉ | 220/247 [00:48<00:05,  4.62it/s]Loading train:  89%|████████▉ | 221/247 [00:48<00:05,  4.62it/s]Loading train:  90%|████████▉ | 222/247 [00:49<00:05,  4.63it/s]Loading train:  90%|█████████ | 223/247 [00:49<00:05,  4.65it/s]Loading train:  91%|█████████ | 224/247 [00:49<00:04,  4.64it/s]Loading train:  91%|█████████ | 225/247 [00:49<00:04,  4.69it/s]Loading train:  91%|█████████▏| 226/247 [00:49<00:04,  4.69it/s]Loading train:  92%|█████████▏| 227/247 [00:50<00:04,  4.69it/s]Loading train:  92%|█████████▏| 228/247 [00:50<00:04,  4.71it/s]Loading train:  93%|█████████▎| 229/247 [00:50<00:03,  4.72it/s]Loading train:  93%|█████████▎| 230/247 [00:50<00:03,  4.60it/s]Loading train:  94%|█████████▎| 231/247 [00:51<00:03,  4.53it/s]Loading train:  94%|█████████▍| 232/247 [00:51<00:03,  4.49it/s]Loading train:  94%|█████████▍| 233/247 [00:51<00:03,  4.45it/s]Loading train:  95%|█████████▍| 234/247 [00:51<00:02,  4.42it/s]Loading train:  95%|█████████▌| 235/247 [00:51<00:02,  4.37it/s]Loading train:  96%|█████████▌| 236/247 [00:52<00:02,  4.34it/s]Loading train:  96%|█████████▌| 237/247 [00:52<00:02,  4.29it/s]Loading train:  96%|█████████▋| 238/247 [00:52<00:02,  4.27it/s]Loading train:  97%|█████████▋| 239/247 [00:52<00:01,  4.28it/s]Loading train:  97%|█████████▋| 240/247 [00:53<00:01,  4.29it/s]Loading train:  98%|█████████▊| 241/247 [00:53<00:01,  4.30it/s]Loading train:  98%|█████████▊| 242/247 [00:53<00:01,  4.30it/s]Loading train:  98%|█████████▊| 243/247 [00:53<00:00,  4.27it/s]Loading train:  99%|█████████▉| 244/247 [00:54<00:00,  4.28it/s]Loading train:  99%|█████████▉| 245/247 [00:54<00:00,  4.29it/s]Loading train: 100%|█████████▉| 246/247 [00:54<00:00,  4.30it/s]Loading train: 100%|██████████| 247/247 [00:54<00:00,  4.30it/s]Loading train: 100%|██████████| 247/247 [00:54<00:00,  4.51it/s]
concatenating: train:   0%|          | 0/247 [00:00<?, ?it/s]concatenating: train:   3%|▎         | 7/247 [00:00<00:03, 63.69it/s]concatenating: train:   6%|▌         | 14/247 [00:00<00:03, 64.61it/s]concatenating: train:   9%|▊         | 21/247 [00:00<00:03, 64.39it/s]concatenating: train:  11%|█▏        | 28/247 [00:00<00:03, 64.93it/s]concatenating: train:  14%|█▍        | 35/247 [00:00<00:03, 64.21it/s]concatenating: train:  17%|█▋        | 42/247 [00:00<00:03, 64.79it/s]concatenating: train:  20%|█▉        | 49/247 [00:00<00:03, 65.24it/s]concatenating: train:  23%|██▎       | 56/247 [00:00<00:02, 65.14it/s]concatenating: train:  26%|██▌       | 63/247 [00:00<00:02, 64.16it/s]concatenating: train:  28%|██▊       | 70/247 [00:01<00:02, 64.07it/s]concatenating: train:  31%|███       | 77/247 [00:01<00:02, 64.64it/s]concatenating: train:  34%|███▍      | 84/247 [00:01<00:02, 63.88it/s]concatenating: train:  37%|███▋      | 91/247 [00:01<00:02, 61.84it/s]concatenating: train:  40%|███▉      | 98/247 [00:01<00:02, 61.00it/s]concatenating: train:  43%|████▎     | 105/247 [00:01<00:02, 61.36it/s]concatenating: train:  45%|████▌     | 112/247 [00:01<00:02, 61.88it/s]concatenating: train:  48%|████▊     | 119/247 [00:01<00:02, 63.30it/s]concatenating: train:  51%|█████▏    | 127/247 [00:01<00:01, 65.10it/s]concatenating: train:  55%|█████▍    | 135/247 [00:02<00:01, 66.93it/s]concatenating: train:  57%|█████▋    | 142/247 [00:02<00:01, 67.09it/s]concatenating: train:  60%|██████    | 149/247 [00:02<00:01, 66.98it/s]concatenating: train:  63%|██████▎   | 156/247 [00:02<00:01, 66.97it/s]concatenating: train:  66%|██████▌   | 163/247 [00:02<00:01, 66.30it/s]concatenating: train:  69%|██████▉   | 170/247 [00:02<00:01, 64.76it/s]concatenating: train:  72%|███████▏  | 177/247 [00:02<00:01, 65.43it/s]concatenating: train:  75%|███████▍  | 185/247 [00:02<00:00, 67.40it/s]concatenating: train:  78%|███████▊  | 192/247 [00:02<00:00, 67.30it/s]concatenating: train:  81%|████████  | 199/247 [00:03<00:00, 67.60it/s]concatenating: train:  83%|████████▎ | 206/247 [00:03<00:00, 67.90it/s]concatenating: train:  86%|████████▌ | 213/247 [00:03<00:00, 68.13it/s]concatenating: train:  89%|████████▉ | 220/247 [00:03<00:00, 68.16it/s]concatenating: train:  92%|█████████▏| 227/247 [00:03<00:00, 68.28it/s]concatenating: train:  95%|█████████▍| 234/247 [00:03<00:00, 67.38it/s]concatenating: train:  98%|█████████▊| 241/247 [00:03<00:00, 64.95it/s]concatenating: train: 100%|██████████| 247/247 [00:03<00:00, 65.16it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:00<00:00,  4.38it/s]Loading test:  40%|████      | 2/5 [00:00<00:00,  4.18it/s]Loading test:  60%|██████    | 3/5 [00:00<00:00,  4.22it/s]Loading test:  80%|████████  | 4/5 [00:00<00:00,  4.39it/s]Loading test: 100%|██████████| 5/5 [00:01<00:00,  4.39it/s]Loading test: 100%|██████████| 5/5 [00:01<00:00,  4.31it/s]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 303.11it/s]
Loading trainS:   0%|          | 0/247 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/247 [00:00<00:55,  4.44it/s]Loading trainS:   1%|          | 2/247 [00:00<00:54,  4.46it/s]Loading trainS:   1%|          | 3/247 [00:00<00:54,  4.49it/s]Loading trainS:   2%|▏         | 4/247 [00:00<00:54,  4.44it/s]Loading trainS:   2%|▏         | 5/247 [00:01<00:53,  4.50it/s]Loading trainS:   2%|▏         | 6/247 [00:01<00:53,  4.52it/s]Loading trainS:   3%|▎         | 7/247 [00:01<00:52,  4.54it/s]Loading trainS:   3%|▎         | 8/247 [00:01<00:52,  4.58it/s]Loading trainS:   4%|▎         | 9/247 [00:01<00:51,  4.61it/s]Loading trainS:   4%|▍         | 10/247 [00:02<00:51,  4.63it/s]Loading trainS:   4%|▍         | 11/247 [00:02<00:50,  4.64it/s]Loading trainS:   5%|▍         | 12/247 [00:02<00:50,  4.66it/s]Loading trainS:   5%|▌         | 13/247 [00:02<00:50,  4.64it/s]Loading trainS:   6%|▌         | 14/247 [00:03<00:50,  4.64it/s]Loading trainS:   6%|▌         | 15/247 [00:03<00:50,  4.63it/s]Loading trainS:   6%|▋         | 16/247 [00:03<00:49,  4.65it/s]Loading trainS:   7%|▋         | 17/247 [00:03<00:49,  4.62it/s]Loading trainS:   7%|▋         | 18/247 [00:03<00:49,  4.64it/s]Loading trainS:   8%|▊         | 19/247 [00:04<00:49,  4.64it/s]Loading trainS:   8%|▊         | 20/247 [00:04<00:50,  4.47it/s]Loading trainS:   9%|▊         | 21/247 [00:04<00:50,  4.51it/s]Loading trainS:   9%|▉         | 22/247 [00:04<00:49,  4.56it/s]Loading trainS:   9%|▉         | 23/247 [00:05<00:48,  4.62it/s]Loading trainS:  10%|▉         | 24/247 [00:05<00:47,  4.67it/s]Loading trainS:  10%|█         | 25/247 [00:05<00:47,  4.69it/s]Loading trainS:  11%|█         | 26/247 [00:05<00:46,  4.71it/s]Loading trainS:  11%|█         | 27/247 [00:05<00:46,  4.71it/s]Loading trainS:  11%|█▏        | 28/247 [00:06<00:46,  4.74it/s]Loading trainS:  12%|█▏        | 29/247 [00:06<00:45,  4.77it/s]Loading trainS:  12%|█▏        | 30/247 [00:06<00:45,  4.78it/s]Loading trainS:  13%|█▎        | 31/247 [00:06<00:44,  4.80it/s]Loading trainS:  13%|█▎        | 32/247 [00:06<00:44,  4.79it/s]Loading trainS:  13%|█▎        | 33/247 [00:07<00:44,  4.80it/s]Loading trainS:  14%|█▍        | 34/247 [00:07<00:44,  4.80it/s]Loading trainS:  14%|█▍        | 35/247 [00:07<00:44,  4.77it/s]Loading trainS:  15%|█▍        | 36/247 [00:07<00:44,  4.76it/s]Loading trainS:  15%|█▍        | 37/247 [00:07<00:43,  4.77it/s]Loading trainS:  15%|█▌        | 38/247 [00:08<00:43,  4.78it/s]Loading trainS:  16%|█▌        | 39/247 [00:08<00:43,  4.77it/s]Loading trainS:  16%|█▌        | 40/247 [00:08<00:43,  4.76it/s]Loading trainS:  17%|█▋        | 41/247 [00:08<00:43,  4.76it/s]Loading trainS:  17%|█▋        | 42/247 [00:08<00:43,  4.75it/s]Loading trainS:  17%|█▋        | 43/247 [00:09<00:42,  4.77it/s]Loading trainS:  18%|█▊        | 44/247 [00:09<00:42,  4.77it/s]Loading trainS:  18%|█▊        | 45/247 [00:09<00:42,  4.76it/s]Loading trainS:  19%|█▊        | 46/247 [00:09<00:42,  4.76it/s]Loading trainS:  19%|█▉        | 47/247 [00:10<00:42,  4.74it/s]Loading trainS:  19%|█▉        | 48/247 [00:10<00:41,  4.77it/s]Loading trainS:  20%|█▉        | 49/247 [00:10<00:41,  4.76it/s]Loading trainS:  20%|██        | 50/247 [00:10<00:41,  4.75it/s]Loading trainS:  21%|██        | 51/247 [00:10<00:41,  4.76it/s]Loading trainS:  21%|██        | 52/247 [00:11<00:41,  4.74it/s]Loading trainS:  21%|██▏       | 53/247 [00:11<00:41,  4.72it/s]Loading trainS:  22%|██▏       | 54/247 [00:11<00:42,  4.59it/s]Loading trainS:  22%|██▏       | 55/247 [00:11<00:41,  4.59it/s]Loading trainS:  23%|██▎       | 56/247 [00:11<00:41,  4.61it/s]Loading trainS:  23%|██▎       | 57/247 [00:12<00:41,  4.63it/s]Loading trainS:  23%|██▎       | 58/247 [00:12<00:40,  4.68it/s]Loading trainS:  24%|██▍       | 59/247 [00:12<00:40,  4.68it/s]Loading trainS:  24%|██▍       | 60/247 [00:12<00:40,  4.66it/s]Loading trainS:  25%|██▍       | 61/247 [00:13<00:40,  4.64it/s]Loading trainS:  25%|██▌       | 62/247 [00:13<00:39,  4.65it/s]Loading trainS:  26%|██▌       | 63/247 [00:13<00:39,  4.64it/s]Loading trainS:  26%|██▌       | 64/247 [00:13<00:39,  4.64it/s]Loading trainS:  26%|██▋       | 65/247 [00:13<00:39,  4.64it/s]Loading trainS:  27%|██▋       | 66/247 [00:14<00:38,  4.65it/s]Loading trainS:  27%|██▋       | 67/247 [00:14<00:38,  4.66it/s]Loading trainS:  28%|██▊       | 68/247 [00:14<00:38,  4.63it/s]Loading trainS:  28%|██▊       | 69/247 [00:14<00:38,  4.64it/s]Loading trainS:  28%|██▊       | 70/247 [00:14<00:38,  4.59it/s]Loading trainS:  29%|██▊       | 71/247 [00:15<00:38,  4.57it/s]Loading trainS:  29%|██▉       | 72/247 [00:15<00:38,  4.56it/s]Loading trainS:  30%|██▉       | 73/247 [00:15<00:39,  4.44it/s]Loading trainS:  30%|██▉       | 74/247 [00:15<00:38,  4.44it/s]Loading trainS:  30%|███       | 75/247 [00:16<00:38,  4.50it/s]Loading trainS:  31%|███       | 76/247 [00:16<00:37,  4.56it/s]Loading trainS:  31%|███       | 77/247 [00:16<00:39,  4.27it/s]Loading trainS:  32%|███▏      | 78/247 [00:16<00:40,  4.19it/s]Loading trainS:  32%|███▏      | 79/247 [00:17<00:38,  4.36it/s]Loading trainS:  32%|███▏      | 80/247 [00:17<00:37,  4.45it/s]Loading trainS:  33%|███▎      | 81/247 [00:17<00:38,  4.31it/s]Loading trainS:  33%|███▎      | 82/247 [00:17<00:38,  4.25it/s]Loading trainS:  34%|███▎      | 83/247 [00:18<00:39,  4.17it/s]Loading trainS:  34%|███▍      | 84/247 [00:18<00:39,  4.13it/s]Loading trainS:  34%|███▍      | 85/247 [00:18<00:39,  4.11it/s]Loading trainS:  35%|███▍      | 86/247 [00:18<00:39,  4.11it/s]Loading trainS:  35%|███▌      | 87/247 [00:18<00:39,  4.10it/s]Loading trainS:  36%|███▌      | 88/247 [00:19<00:38,  4.08it/s]Loading trainS:  36%|███▌      | 89/247 [00:19<00:39,  4.05it/s]Loading trainS:  36%|███▋      | 90/247 [00:19<00:39,  4.02it/s]Loading trainS:  37%|███▋      | 91/247 [00:19<00:38,  4.01it/s]Loading trainS:  37%|███▋      | 92/247 [00:20<00:38,  4.01it/s]Loading trainS:  38%|███▊      | 93/247 [00:20<00:39,  3.93it/s]Loading trainS:  38%|███▊      | 94/247 [00:20<00:38,  3.95it/s]Loading trainS:  38%|███▊      | 95/247 [00:21<00:38,  3.95it/s]Loading trainS:  39%|███▉      | 96/247 [00:21<00:37,  3.97it/s]Loading trainS:  39%|███▉      | 97/247 [00:21<00:37,  3.97it/s]Loading trainS:  40%|███▉      | 98/247 [00:21<00:37,  3.95it/s]Loading trainS:  40%|████      | 99/247 [00:22<00:37,  3.96it/s]Loading trainS:  40%|████      | 100/247 [00:22<00:36,  4.03it/s]Loading trainS:  41%|████      | 101/247 [00:22<00:35,  4.06it/s]Loading trainS:  41%|████▏     | 102/247 [00:22<00:35,  4.09it/s]Loading trainS:  42%|████▏     | 103/247 [00:22<00:35,  4.11it/s]Loading trainS:  42%|████▏     | 104/247 [00:23<00:34,  4.14it/s]Loading trainS:  43%|████▎     | 105/247 [00:23<00:34,  4.17it/s]Loading trainS:  43%|████▎     | 106/247 [00:23<00:33,  4.17it/s]Loading trainS:  43%|████▎     | 107/247 [00:23<00:33,  4.18it/s]Loading trainS:  44%|████▎     | 108/247 [00:24<00:33,  4.20it/s]Loading trainS:  44%|████▍     | 109/247 [00:24<00:32,  4.22it/s]Loading trainS:  45%|████▍     | 110/247 [00:24<00:32,  4.18it/s]Loading trainS:  45%|████▍     | 111/247 [00:24<00:32,  4.16it/s]Loading trainS:  45%|████▌     | 112/247 [00:25<00:32,  4.18it/s]Loading trainS:  46%|████▌     | 113/247 [00:25<00:31,  4.21it/s]Loading trainS:  46%|████▌     | 114/247 [00:25<00:32,  4.13it/s]Loading trainS:  47%|████▋     | 115/247 [00:25<00:31,  4.16it/s]Loading trainS:  47%|████▋     | 116/247 [00:26<00:31,  4.19it/s]Loading trainS:  47%|████▋     | 117/247 [00:26<00:30,  4.23it/s]Loading trainS:  48%|████▊     | 118/247 [00:26<00:29,  4.44it/s]Loading trainS:  48%|████▊     | 119/247 [00:26<00:27,  4.63it/s]Loading trainS:  49%|████▊     | 120/247 [00:26<00:26,  4.76it/s]Loading trainS:  49%|████▉     | 121/247 [00:27<00:25,  4.85it/s]Loading trainS:  49%|████▉     | 122/247 [00:27<00:25,  4.94it/s]Loading trainS:  50%|████▉     | 123/247 [00:27<00:24,  5.01it/s]Loading trainS:  50%|█████     | 124/247 [00:27<00:24,  5.05it/s]Loading trainS:  51%|█████     | 125/247 [00:27<00:24,  5.05it/s]Loading trainS:  51%|█████     | 126/247 [00:28<00:24,  4.97it/s]Loading trainS:  51%|█████▏    | 127/247 [00:28<00:24,  4.98it/s]Loading trainS:  52%|█████▏    | 128/247 [00:28<00:24,  4.96it/s]Loading trainS:  52%|█████▏    | 129/247 [00:28<00:23,  4.95it/s]Loading trainS:  53%|█████▎    | 130/247 [00:28<00:23,  4.97it/s]Loading trainS:  53%|█████▎    | 131/247 [00:29<00:23,  5.00it/s]Loading trainS:  53%|█████▎    | 132/247 [00:29<00:22,  5.01it/s]Loading trainS:  54%|█████▍    | 133/247 [00:29<00:22,  5.00it/s]Loading trainS:  54%|█████▍    | 134/247 [00:29<00:22,  5.02it/s]Loading trainS:  55%|█████▍    | 135/247 [00:29<00:22,  5.02it/s]Loading trainS:  55%|█████▌    | 136/247 [00:30<00:22,  4.95it/s]Loading trainS:  55%|█████▌    | 137/247 [00:30<00:22,  4.94it/s]Loading trainS:  56%|█████▌    | 138/247 [00:30<00:22,  4.92it/s]Loading trainS:  56%|█████▋    | 139/247 [00:30<00:22,  4.91it/s]Loading trainS:  57%|█████▋    | 140/247 [00:30<00:22,  4.86it/s]Loading trainS:  57%|█████▋    | 141/247 [00:31<00:21,  4.84it/s]Loading trainS:  57%|█████▋    | 142/247 [00:31<00:21,  4.84it/s]Loading trainS:  58%|█████▊    | 143/247 [00:31<00:21,  4.78it/s]Loading trainS:  58%|█████▊    | 144/247 [00:31<00:21,  4.78it/s]Loading trainS:  59%|█████▊    | 145/247 [00:31<00:21,  4.79it/s]Loading trainS:  59%|█████▉    | 146/247 [00:32<00:21,  4.79it/s]Loading trainS:  60%|█████▉    | 147/247 [00:32<00:20,  4.79it/s]Loading trainS:  60%|█████▉    | 148/247 [00:32<00:21,  4.66it/s]Loading trainS:  60%|██████    | 149/247 [00:32<00:20,  4.70it/s]Loading trainS:  61%|██████    | 150/247 [00:33<00:20,  4.70it/s]Loading trainS:  61%|██████    | 151/247 [00:33<00:20,  4.73it/s]Loading trainS:  62%|██████▏   | 152/247 [00:33<00:20,  4.72it/s]Loading trainS:  62%|██████▏   | 153/247 [00:33<00:19,  4.71it/s]Loading trainS:  62%|██████▏   | 154/247 [00:33<00:20,  4.54it/s]Loading trainS:  63%|██████▎   | 155/247 [00:34<00:20,  4.41it/s]Loading trainS:  63%|██████▎   | 156/247 [00:34<00:20,  4.38it/s]Loading trainS:  64%|██████▎   | 157/247 [00:34<00:20,  4.33it/s]Loading trainS:  64%|██████▍   | 158/247 [00:34<00:20,  4.31it/s]Loading trainS:  64%|██████▍   | 159/247 [00:35<00:20,  4.28it/s]Loading trainS:  65%|██████▍   | 160/247 [00:35<00:20,  4.25it/s]Loading trainS:  65%|██████▌   | 161/247 [00:35<00:20,  4.24it/s]Loading trainS:  66%|██████▌   | 162/247 [00:35<00:20,  4.25it/s]Loading trainS:  66%|██████▌   | 163/247 [00:36<00:19,  4.23it/s]Loading trainS:  66%|██████▋   | 164/247 [00:36<00:19,  4.20it/s]Loading trainS:  67%|██████▋   | 165/247 [00:36<00:19,  4.22it/s]Loading trainS:  67%|██████▋   | 166/247 [00:36<00:19,  4.23it/s]Loading trainS:  68%|██████▊   | 167/247 [00:36<00:18,  4.24it/s]Loading trainS:  68%|██████▊   | 168/247 [00:37<00:18,  4.25it/s]Loading trainS:  68%|██████▊   | 169/247 [00:37<00:18,  4.24it/s]Loading trainS:  69%|██████▉   | 170/247 [00:37<00:18,  4.25it/s]Loading trainS:  69%|██████▉   | 171/247 [00:37<00:17,  4.27it/s]Loading trainS:  70%|██████▉   | 172/247 [00:38<00:17,  4.32it/s]Loading trainS:  70%|███████   | 173/247 [00:38<00:16,  4.45it/s]Loading trainS:  70%|███████   | 174/247 [00:38<00:16,  4.45it/s]Loading trainS:  71%|███████   | 175/247 [00:38<00:16,  4.27it/s]Loading trainS:  71%|███████▏  | 176/247 [00:39<00:16,  4.38it/s]Loading trainS:  72%|███████▏  | 177/247 [00:39<00:15,  4.49it/s]Loading trainS:  72%|███████▏  | 178/247 [00:39<00:14,  4.60it/s]Loading trainS:  72%|███████▏  | 179/247 [00:39<00:14,  4.69it/s]Loading trainS:  73%|███████▎  | 180/247 [00:39<00:14,  4.73it/s]Loading trainS:  73%|███████▎  | 181/247 [00:40<00:13,  4.74it/s]Loading trainS:  74%|███████▎  | 182/247 [00:40<00:13,  4.75it/s]Loading trainS:  74%|███████▍  | 183/247 [00:40<00:13,  4.78it/s]Loading trainS:  74%|███████▍  | 184/247 [00:40<00:13,  4.76it/s]Loading trainS:  75%|███████▍  | 185/247 [00:40<00:12,  4.77it/s]Loading trainS:  75%|███████▌  | 186/247 [00:41<00:12,  4.77it/s]Loading trainS:  76%|███████▌  | 187/247 [00:41<00:12,  4.79it/s]Loading trainS:  76%|███████▌  | 188/247 [00:41<00:12,  4.77it/s]Loading trainS:  77%|███████▋  | 189/247 [00:41<00:12,  4.77it/s]Loading trainS:  77%|███████▋  | 190/247 [00:41<00:11,  4.78it/s]Loading trainS:  77%|███████▋  | 191/247 [00:42<00:11,  4.78it/s]Loading trainS:  78%|███████▊  | 192/247 [00:42<00:11,  4.79it/s]Loading trainS:  78%|███████▊  | 193/247 [00:42<00:11,  4.80it/s]Loading trainS:  79%|███████▊  | 194/247 [00:42<00:10,  4.85it/s]Loading trainS:  79%|███████▉  | 195/247 [00:42<00:10,  4.90it/s]Loading trainS:  79%|███████▉  | 196/247 [00:43<00:10,  4.93it/s]Loading trainS:  80%|███████▉  | 197/247 [00:43<00:10,  4.92it/s]Loading trainS:  80%|████████  | 198/247 [00:43<00:09,  4.94it/s]Loading trainS:  81%|████████  | 199/247 [00:43<00:09,  4.95it/s]Loading trainS:  81%|████████  | 200/247 [00:43<00:09,  4.98it/s]Loading trainS:  81%|████████▏ | 201/247 [00:44<00:09,  4.97it/s]Loading trainS:  82%|████████▏ | 202/247 [00:44<00:09,  4.97it/s]Loading trainS:  82%|████████▏ | 203/247 [00:44<00:08,  4.96it/s]Loading trainS:  83%|████████▎ | 204/247 [00:44<00:08,  4.95it/s]Loading trainS:  83%|████████▎ | 205/247 [00:45<00:08,  4.94it/s]Loading trainS:  83%|████████▎ | 206/247 [00:45<00:08,  4.95it/s]Loading trainS:  84%|████████▍ | 207/247 [00:45<00:08,  4.96it/s]Loading trainS:  84%|████████▍ | 208/247 [00:45<00:07,  4.98it/s]Loading trainS:  85%|████████▍ | 209/247 [00:45<00:07,  4.96it/s]Loading trainS:  85%|████████▌ | 210/247 [00:46<00:07,  4.97it/s]Loading trainS:  85%|████████▌ | 211/247 [00:46<00:07,  4.98it/s]Loading trainS:  86%|████████▌ | 212/247 [00:46<00:07,  4.91it/s]Loading trainS:  86%|████████▌ | 213/247 [00:46<00:07,  4.83it/s]Loading trainS:  87%|████████▋ | 214/247 [00:46<00:06,  4.81it/s]Loading trainS:  87%|████████▋ | 215/247 [00:47<00:06,  4.80it/s]Loading trainS:  87%|████████▋ | 216/247 [00:47<00:06,  4.75it/s]Loading trainS:  88%|████████▊ | 217/247 [00:47<00:06,  4.63it/s]Loading trainS:  88%|████████▊ | 218/247 [00:47<00:06,  4.53it/s]Loading trainS:  89%|████████▊ | 219/247 [00:47<00:06,  4.58it/s]Loading trainS:  89%|████████▉ | 220/247 [00:48<00:05,  4.59it/s]Loading trainS:  89%|████████▉ | 221/247 [00:48<00:05,  4.65it/s]Loading trainS:  90%|████████▉ | 222/247 [00:48<00:05,  4.67it/s]Loading trainS:  90%|█████████ | 223/247 [00:48<00:05,  4.71it/s]Loading trainS:  91%|█████████ | 224/247 [00:49<00:04,  4.71it/s]Loading trainS:  91%|█████████ | 225/247 [00:49<00:04,  4.70it/s]Loading trainS:  91%|█████████▏| 226/247 [00:49<00:04,  4.75it/s]Loading trainS:  92%|█████████▏| 227/247 [00:49<00:04,  4.74it/s]Loading trainS:  92%|█████████▏| 228/247 [00:49<00:03,  4.77it/s]Loading trainS:  93%|█████████▎| 229/247 [00:50<00:03,  4.81it/s]Loading trainS:  93%|█████████▎| 230/247 [00:50<00:03,  4.70it/s]Loading trainS:  94%|█████████▎| 231/247 [00:50<00:03,  4.60it/s]Loading trainS:  94%|█████████▍| 232/247 [00:50<00:03,  4.55it/s]Loading trainS:  94%|█████████▍| 233/247 [00:50<00:03,  4.52it/s]Loading trainS:  95%|█████████▍| 234/247 [00:51<00:02,  4.48it/s]Loading trainS:  95%|█████████▌| 235/247 [00:51<00:02,  4.47it/s]Loading trainS:  96%|█████████▌| 236/247 [00:51<00:02,  4.46it/s]Loading trainS:  96%|█████████▌| 237/247 [00:51<00:02,  4.41it/s]Loading trainS:  96%|█████████▋| 238/247 [00:52<00:02,  4.38it/s]Loading trainS:  97%|█████████▋| 239/247 [00:52<00:01,  4.35it/s]Loading trainS:  97%|█████████▋| 240/247 [00:52<00:01,  4.32it/s]Loading trainS:  98%|█████████▊| 241/247 [00:52<00:01,  4.30it/s]Loading trainS:  98%|█████████▊| 242/247 [00:53<00:01,  4.28it/s]Loading trainS:  98%|█████████▊| 243/247 [00:53<00:00,  4.27it/s]Loading trainS:  99%|█████████▉| 244/247 [00:53<00:00,  4.26it/s]Loading trainS:  99%|█████████▉| 245/247 [00:53<00:00,  4.26it/s]Loading trainS: 100%|█████████▉| 246/247 [00:53<00:00,  4.22it/s]Loading trainS: 100%|██████████| 247/247 [00:54<00:00,  4.21it/s]Loading trainS: 100%|██████████| 247/247 [00:54<00:00,  4.56it/s]
Loading testS:   0%|          | 0/5 [00:00<?, ?it/s]Loading testS:  20%|██        | 1/5 [00:00<00:00,  4.22it/s]Loading testS:  40%|████      | 2/5 [00:00<00:00,  4.04it/s]Loading testS:  60%|██████    | 3/5 [00:00<00:00,  4.06it/s]Loading testS:  80%|████████  | 4/5 [00:00<00:00,  4.32it/s]Loading testS: 100%|██████████| 5/5 [00:01<00:00,  4.31it/s]Loading testS: 100%|██████████| 5/5 [00:01<00:00,  4.22it/s]
Epoch 00050: val_mDice did not improve from 0.22423
Restoring model weights from the end of the best epoch
Epoch 00050: early stopping
{'val_loss': [0.5084079693979728, 0.20679662796605078, 0.07881434863398534, -0.004574986605656258, -0.006075719581130221, -0.0299716640888534, -0.03427912383947684, -0.023842929683116652, -0.0450927135765969, -0.08357681552677051, -0.038908304756518845, -0.0953451642862041, -0.06842842892243278, -0.07643582279684011, -0.068917711812031, -0.10670384762791327, -0.0929839672702972, -0.10379936187730535, -0.08217512510184731, -0.09564987940841024, -0.06903518627611746, -0.07758603521235148, -0.08965597337296938, -0.1108673474044967, -0.11913044851631945, -0.11324786676545136, -0.12280426261605266, -0.10292187593993007, -0.1225610084315438, -0.11444533780587386, -0.10668659481214163, -0.12605010134512154, -0.13186844251326846, -0.12702107971450255, -0.11845648931541844, -0.1297182259117311, -0.1196551203282307, -0.1213929858000845, -0.12814668586234684, -0.1184056475498245, -0.12237257648205893, -0.13697536169082758, -0.135110771960307, -0.13783933468718337, -0.1501117394253704, -0.14421872608508243, -0.14524244525186394, -0.14164468852994003, -0.13943068743885859, -0.1637024628345548], 'val_acc': [0.9366178501466786, 0.943719888696019, 0.9466102260240117, 0.9478631637851644, 0.9480759264519496, 0.9491223267146519, 0.949967152213458, 0.9499211122530588, 0.9502744770938565, 0.9508318860338342, 0.9504859976886962, 0.9504001466383846, 0.9514564964341821, 0.9514515133377928, 0.9512997195587395, 0.9515211915377504, 0.9511678322501804, 0.9522092438632657, 0.9516941412635471, 0.9516157574535157, 0.9509973722215025, 0.9517140477340414, 0.9522590192948809, 0.9521744103165146, 0.9518944609979665, 0.952654683071634, 0.9525402180896783, 0.9522975920149999, 0.9525688320213224, 0.952025109936732, 0.9529333884671608, 0.9525427040846451, 0.9526098925874841, 0.9523859309113544, 0.9524618250242671, 0.952848792446326, 0.9525028855904288, 0.9523921523775373, 0.9528487794887945, 0.952275192145235, 0.9526260617356863, 0.9526285569860328, 0.9524506260149227, 0.9529844004174938, 0.9524991556724406, 0.95310135879872, 0.9526907660946342, 0.9524705380386447, 0.9528338542636137, 0.9531150456541073], 'val_mDice': [0.18628179124608543, 0.2100009364741189, 0.2168649128458885, 0.21588009475551037, 0.21658601829354068, 0.22044356148805677, 0.22258954816816015, 0.2228295059168931, 0.22348257591542992, 0.22423369173677812, 0.22320135745295086, 0.22332822742021602, 0.22065328931586342, 0.22254452988597917, 0.2240230155796368, 0.22290960636846027, 0.22257138888195435, 0.21903487327306168, 0.2219184661596458, 0.22105294054154284, 0.22091079121513396, 0.22009354559068353, 0.22107731770747197, 0.219965455764384, 0.2202816703212187, 0.21949620304270562, 0.22091720577167429, 0.21812013090119597, 0.21955233321797032, 0.21901618955390795, 0.22086383774876595, 0.22052335697486533, 0.21911082338102117, 0.2168458060401937, 0.21823215709062097, 0.21953853205864474, 0.21963940262331727, 0.21881113881650177, 0.21801082070292152, 0.2181529549893385, 0.21923900703373161, 0.21801680835507672, 0.21933144745156632, 0.21915445856240964, 0.21817959231897172, 0.21898402618417828, 0.2174509368744326, 0.2182878137375257, 0.2177286226742016, 0.21717263617037985], 'loss': [0.6561325097343778, 0.49782377817584167, 0.43684913455313196, 0.4166238515356419, 0.4001951034884161, 0.39160730164809626, 0.38645244117388167, 0.3746949264213314, 0.36926417723560667, 0.36769065523483707, 0.3613753300834565, 0.35672348522864944, 0.35379654961646806, 0.35118116567777313, 0.34764411169838655, 0.3433694941842453, 0.34123161360645327, 0.33669899158163047, 0.3396785463403716, 0.33243299819689004, 0.33150951752904056, 0.329466451488218, 0.32824470427929303, 0.3247921180931355, 0.32616121576635243, 0.32070802507855806, 0.32222418852319934, 0.31833786138049003, 0.31918111459752835, 0.31929573453950866, 0.3175697151627612, 0.3152327298812475, 0.315081667294681, 0.3146750572131865, 0.31330658060470784, 0.3136329519389647, 0.30916302013534724, 0.30862418562890015, 0.31050488322218883, 0.31057716929465673, 0.3086537732809868, 0.3052952390704847, 0.3097299597882873, 0.30891262580667767, 0.3077200189667304, 0.30603381560728055, 0.3096191228982998, 0.30546038154988897, 0.30719325490694555, 0.30629019464525187], 'acc': [0.9076977496688009, 0.9308042498553116, 0.9353653702816876, 0.9381050311461048, 0.9399528831520435, 0.9411630982850003, 0.9418844839809263, 0.942888974455907, 0.9435867655739422, 0.9441084078740172, 0.9447280725619992, 0.9450477129257249, 0.9454855290562325, 0.9459570815684665, 0.9463313823516305, 0.9466858861233247, 0.9469760270005654, 0.947315307218881, 0.947537719821441, 0.9478755363551747, 0.9481907610288227, 0.9483355107654252, 0.9486119160091445, 0.9487824673570145, 0.9489895782383616, 0.9492918887951175, 0.94948405734013, 0.9495427990639603, 0.9496910662003261, 0.9498207449760241, 0.9499445447399234, 0.9499920927649391, 0.9500652113735427, 0.9502272388179575, 0.9502485497033737, 0.9502371499378763, 0.9504143762993377, 0.9506155569100372, 0.9505935293365388, 0.9505979960753879, 0.9508100889768787, 0.9508512176905103, 0.9508882906909785, 0.9509173282939858, 0.9509373281254901, 0.9509960703267688, 0.9510634901698527, 0.9510983806395905, 0.9510565062281184, 0.9512375836684054], 'mDice': [0.2925778658907217, 0.46317829539744526, 0.5290945012460188, 0.5509154463054965, 0.5686625767235418, 0.5779238520372146, 0.5834939046304831, 0.5961960308905482, 0.6020531923447767, 0.6037500185324419, 0.6105712566647687, 0.6156087590119503, 0.6187629303997887, 0.6215807336983258, 0.6253987402917482, 0.6300192133656911, 0.632327579374842, 0.6372289025451234, 0.6339976043798189, 0.6418383499629525, 0.6428142007976264, 0.6450286574258288, 0.6463436097030798, 0.6500863360121096, 0.648593170354092, 0.6544952059319217, 0.6528536649573045, 0.657053607123588, 0.656130651978299, 0.6560073290563325, 0.6578726872863849, 0.6604078450060853, 0.6605752189737679, 0.6610020979097813, 0.6624768545940036, 0.6621223147272185, 0.6669656159556168, 0.667544791567444, 0.6655134603175854, 0.6654336501947774, 0.6675115683757888, 0.67115006976615, 0.6663427346411976, 0.6672246151152271, 0.6685158763509346, 0.6703386817441388, 0.6664594546872352, 0.6709626063212719, 0.6690815904323049, 0.6700542170204669], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05]}
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________2020-01-22 14:17:20.600958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 14:17:20.601038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 14:17:20.601052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 14:17:20.601061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 14:17:20.601331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

batch_normalization_10 (BatchNo (None, 108, 116, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97331515 0.02668485]
Train on 16036 samples, validate on 318 samples
Epoch 1/300
 - 44s - loss: 0.3655 - acc: 0.9728 - mDice: 0.2832 - val_loss: 0.3127 - val_acc: 0.9863 - val_mDice: 0.3809

Epoch 00001: val_mDice improved from -inf to 0.38086, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 41s - loss: 0.1479 - acc: 0.9845 - mDice: 0.7124 - val_loss: 0.2947 - val_acc: 0.9898 - val_mDice: 0.4156

Epoch 00002: val_mDice improved from 0.38086 to 0.41561, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 41s - loss: 0.1228 - acc: 0.9873 - mDice: 0.7610 - val_loss: 0.2909 - val_acc: 0.9900 - val_mDice: 0.4232

Epoch 00003: val_mDice improved from 0.41561 to 0.42324, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300
 - 40s - loss: 0.1117 - acc: 0.9885 - mDice: 0.7826 - val_loss: 0.2864 - val_acc: 0.9910 - val_mDice: 0.4308

Epoch 00004: val_mDice improved from 0.42324 to 0.43080, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300
 - 40s - loss: 0.1043 - acc: 0.9893 - mDice: 0.7970 - val_loss: 0.2850 - val_acc: 0.9912 - val_mDice: 0.4333

Epoch 00005: val_mDice improved from 0.43080 to 0.43334, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300
 - 40s - loss: 0.0981 - acc: 0.9899 - mDice: 0.8091 - val_loss: 0.2839 - val_acc: 0.9915 - val_mDice: 0.4353

Epoch 00006: val_mDice improved from 0.43334 to 0.43532, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/300
 - 40s - loss: 0.0951 - acc: 0.9903 - mDice: 0.8148 - val_loss: 0.2829 - val_acc: 0.9916 - val_mDice: 0.4366

Epoch 00007: val_mDice improved from 0.43532 to 0.43657, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/300
 - 40s - loss: 0.0905 - acc: 0.9907 - mDice: 0.8238 - val_loss: 0.2808 - val_acc: 0.9920 - val_mDice: 0.4391

Epoch 00008: val_mDice improved from 0.43657 to 0.43909, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 9/300
 - 40s - loss: 0.0882 - acc: 0.9909 - mDice: 0.8283 - val_loss: 0.2807 - val_acc: 0.9922 - val_mDice: 0.4404

Epoch 00009: val_mDice improved from 0.43909 to 0.44045, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 10/300
 - 40s - loss: 0.0853 - acc: 0.9912 - mDice: 0.8339 - val_loss: 0.2788 - val_acc: 0.9922 - val_mDice: 0.4416

Epoch 00010: val_mDice improved from 0.44045 to 0.44157, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 11/300
 - 40s - loss: 0.0849 - acc: 0.9914 - mDice: 0.8347 - val_loss: 0.2735 - val_acc: 0.9925 - val_mDice: 0.4441

Epoch 00011: val_mDice improved from 0.44157 to 0.44414, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 12/300
 - 40s - loss: 0.0816 - acc: 0.9916 - mDice: 0.8411 - val_loss: 0.2695 - val_acc: 0.9924 - val_mDice: 0.4441

Epoch 00012: val_mDice did not improve from 0.44414
Epoch 13/300
 - 40s - loss: 0.0802 - acc: 0.9917 - mDice: 0.8439 - val_loss: 0.2650 - val_acc: 0.9925 - val_mDice: 0.4439

Epoch 00013: val_mDice did not improve from 0.44414
Epoch 14/300
 - 40s - loss: 0.0776 - acc: 0.9919 - mDice: 0.8489 - val_loss: 0.2627 - val_acc: 0.9928 - val_mDice: 0.4461

Epoch 00014: val_mDice improved from 0.44414 to 0.44613, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 15/300
 - 40s - loss: 0.0762 - acc: 0.9920 - mDice: 0.8516 - val_loss: 0.2530 - val_acc: 0.9928 - val_mDice: 0.4478

Epoch 00015: val_mDice improved from 0.44613 to 0.44778, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 16/300
 - 40s - loss: 0.0758 - acc: 0.9922 - mDice: 0.8525 - val_loss: 0.2523 - val_acc: 0.9927 - val_mDice: 0.4464

Epoch 00016: val_mDice did not improve from 0.44778
Epoch 17/300
 - 41s - loss: 0.0762 - acc: 0.9922 - mDice: 0.8515 - val_loss: 0.1827 - val_acc: 0.9927 - val_mDice: 0.4448

Epoch 00017: val_mDice did not improve from 0.44778
Epoch 18/300
 - 41s - loss: 0.0743 - acc: 0.9923 - mDice: 0.8554 - val_loss: 0.2145 - val_acc: 0.9929 - val_mDice: 0.4484

Epoch 00018: val_mDice improved from 0.44778 to 0.44838, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 19/300
 - 40s - loss: 0.0741 - acc: 0.9924 - mDice: 0.8558 - val_loss: 0.1958 - val_acc: 0.9930 - val_mDice: 0.4489

Epoch 00019: val_mDice improved from 0.44838 to 0.44890, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 20/300
 - 40s - loss: 0.0730 - acc: 0.9925 - mDice: 0.8580 - val_loss: 0.2025 - val_acc: 0.9930 - val_mDice: 0.4489

Epoch 00020: val_mDice improved from 0.44890 to 0.44892, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 21/300
 - 40s - loss: 0.0713 - acc: 0.9926 - mDice: 0.8612 - val_loss: 0.1647 - val_acc: 0.9931 - val_mDice: 0.4495

Epoch 00021: val_mDice improved from 0.44892 to 0.44947, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 22/300
 - 40s - loss: 0.0698 - acc: 0.9927 - mDice: 0.8642 - val_loss: 0.1619 - val_acc: 0.9932 - val_mDice: 0.4497

Epoch 00022: val_mDice improved from 0.44947 to 0.44970, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 23/300
 - 40s - loss: 0.0694 - acc: 0.9927 - mDice: 0.8650 - val_loss: 0.1323 - val_acc: 0.9931 - val_mDice: 0.4511

Epoch 00023: val_mDice improved from 0.44970 to 0.45109, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 24/300
 - 40s - loss: 0.0700 - acc: 0.9928 - mDice: 0.8638 - val_loss: 0.1400 - val_acc: 0.9931 - val_mDice: 0.4500

Epoch 00024: val_mDice did not improve from 0.45109
Epoch 25/300
 - 40s - loss: 0.0683 - acc: 0.9929 - mDice: 0.8671 - val_loss: 0.1120 - val_acc: 0.9932 - val_mDice: 0.4503

Epoch 00025: val_mDice did not improve from 0.45109
Epoch 26/300
 - 40s - loss: 0.0676 - acc: 0.9929 - mDice: 0.8685 - val_loss: 0.1566 - val_acc: 0.9931 - val_mDice: 0.4492

Epoch 00026: val_mDice did not improve from 0.45109
Epoch 27/300
 - 40s - loss: 0.0672 - acc: 0.9930 - mDice: 0.8691 - val_loss: 0.0790 - val_acc: 0.9931 - val_mDice: 0.4495

Epoch 00027: val_mDice did not improve from 0.45109
Epoch 28/300
 - 40s - loss: 0.0670 - acc: 0.9930 - mDice: 0.8695 - val_loss: 0.0842 - val_acc: 0.9932 - val_mDice: 0.4513

Epoch 00028: val_mDice improved from 0.45109 to 0.45130, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 29/300
 - 41s - loss: 0.0656 - acc: 0.9931 - mDice: 0.8724 - val_loss: 0.0516 - val_acc: 0.9932 - val_mDice: 0.4516

Epoch 00029: val_mDice improved from 0.45130 to 0.45157, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 30/300
 - 40s - loss: 0.0662 - acc: 0.9932 - mDice: 0.8711 - val_loss: 0.0469 - val_acc: 0.9933 - val_mDice: 0.4521

Epoch 00030: val_mDice improved from 0.45157 to 0.45215, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 31/300
 - 40s - loss: 0.0653 - acc: 0.9932 - mDice: 0.8729 - val_loss: 0.0459 - val_acc: 0.9934 - val_mDice: 0.4535

Epoch 00031: val_mDice improved from 0.45215 to 0.45352, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 32/300
 - 41s - loss: 0.0636 - acc: 0.9933 - mDice: 0.8764 - val_loss: 0.0414 - val_acc: 0.9934 - val_mDice: 0.4547

Epoch 00032: val_mDice improved from 0.45352 to 0.45469, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 33/300
 - 41s - loss: 0.0643 - acc: 0.9933 - mDice: 0.8749 - val_loss: 0.0416 - val_acc: 0.9934 - val_mDice: 0.4556

Epoch 00033: val_mDice improved from 0.45469 to 0.45559, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 34/300
 - 41s - loss: 0.0635 - acc: 0.9933 - mDice: 0.8764 - val_loss: 0.0373 - val_acc: 0.9934 - val_mDice: 0.4538

Epoch 00034: val_mDice did not improve from 0.45559
Epoch 35/300
 - 41s - loss: 0.0634 - acc: 0.9934 - mDice: 0.8767 - val_loss: 0.0385 - val_acc: 0.9934 - val_mDice: 0.4566

Epoch 00035: val_mDice improved from 0.45559 to 0.45663, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 36/300
 - 40s - loss: 0.0636 - acc: 0.9934 - mDice: 0.8762 - val_loss: 0.0380 - val_acc: 0.9934 - val_mDice: 0.4561

Epoch 00036: val_mDice did not improve from 0.45663
Epoch 37/300
 - 40s - loss: 0.0627 - acc: 0.9935 - mDice: 0.8779 - val_loss: 0.0108 - val_acc: 0.9933 - val_mDice: 0.4549

Epoch 00037: val_mDice did not improve from 0.45663
Epoch 38/300
 - 41s - loss: 0.0616 - acc: 0.9935 - mDice: 0.8801 - val_loss: 0.0683 - val_acc: 0.9930 - val_mDice: 0.4559

Epoch 00038: val_mDice did not improve from 0.45663
Epoch 39/300
 - 40s - loss: 0.0621 - acc: 0.9935 - mDice: 0.8791 - val_loss: 0.0170 - val_acc: 0.9935 - val_mDice: 0.4583

Epoch 00039: val_mDice improved from 0.45663 to 0.45829, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 40/300
 - 41s - loss: 0.0621 - acc: 0.9936 - mDice: 0.8791 - val_loss: 0.0669 - val_acc: 0.9935 - val_mDice: 0.4598

Epoch 00040: val_mDice improved from 0.45829 to 0.45978, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 41/300
 - 40s - loss: 0.0610 - acc: 0.9936 - mDice: 0.8813 - val_loss: -1.2037e-03 - val_acc: 0.9934 - val_mDice: 0.4594

Epoch 00041: val_mDice did not improve from 0.45978
Epoch 42/300
 - 40s - loss: 0.0610 - acc: 0.9936 - mDice: 0.8812 - val_loss: 0.0024 - val_acc: 0.9934 - val_mDice: 0.4569

Epoch 00042: val_mDice did not improve from 0.45978
Epoch 43/300
 - 40s - loss: 0.0607 - acc: 0.9937 - mDice: 0.8819 - val_loss: -2.9155e-04 - val_acc: 0.9931 - val_mDice: 0.4528

Epoch 00043: val_mDice did not improve from 0.45978
Epoch 44/300
 - 40s - loss: 0.0605 - acc: 0.9937 - mDice: 0.8822 - val_loss: -3.5659e-03 - val_acc: 0.9934 - val_mDice: 0.4576

Epoch 00044: val_mDice did not improve from 0.45978
Epoch 45/300
 - 40s - loss: 0.0592 - acc: 0.9937 - mDice: 0.8847 - val_loss: -3.8069e-03 - val_acc: 0.9934 - val_mDice: 0.4589

Epoch 00045: val_mDice did not improve from 0.45978
Epoch 46/300
 - 40s - loss: 0.0601 - acc: 0.9937 - mDice: 0.8830 - val_loss: -2.9690e-03 - val_acc: 0.9935 - val_mDice: 0.4574

Epoch 00046: val_mDice did not improve from 0.45978
Epoch 47/300
 - 41s - loss: 0.0594 - acc: 0.9938 - mDice: 0.8843 - val_loss: -6.6488e-03 - val_acc: 0.9938 - val_mDice: 0.4648

Epoch 00047: val_mDice improved from 0.45978 to 0.46480, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 48/300
 - 41s - loss: 0.0586 - acc: 0.9938 - mDice: 0.8859 - val_loss: -4.8446e-03 - val_acc: 0.9936 - val_mDice: 0.4606

Epoch 00048: val_mDice did not improve from 0.46480
Epoch 49/300
 - 41s - loss: 0.0596 - acc: 0.9938 - mDice: 0.8839 - val_loss: -6.2374e-03 - val_acc: 0.9934 - val_mDice: 0.4631

Epoch 00049: val_mDice did not improve from 0.46480
Epoch 50/300
 - 40s - loss: 0.0571 - acc: 0.9939 - mDice: 0.8890 - val_loss: -8.4151e-03 - val_acc: 0.9938 - val_mDice: 0.4673

Epoch 00050: val_mDice improved from 0.46480 to 0.46731, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 51/300
 - 41s - loss: 0.0587 - acc: 0.9939 - mDice: 0.8857 - val_loss: -8.0336e-03 - val_acc: 0.9934 - val_mDice: 0.4664

Epoch 00051: val_mDice did not improve from 0.46731
Epoch 52/300
 - 40s - loss: 0.0569 - acc: 0.9940 - mDice: 0.8894 - val_loss: -7.3708e-03 - val_acc: 0.9935 - val_mDice: 0.4650

Epoch 00052: val_mDice did not improve from 0.46731
Epoch 53/300
 - 40s - loss: 0.0575 - acc: 0.9940 - mDice: 0.8880 - val_loss: -1.2184e-02 - val_acc: 0.9938 - val_mDice: 0.4745

Epoch 00053: val_mDice improved from 0.46731 to 0.47448, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 54/300
 - 41s - loss: 0.0573 - acc: 0.9940 - mDice: 0.8885 - val_loss: -1.0563e-02 - val_acc: 0.9936 - val_mDice: 0.4712

Epoch 00054: val_mDice did not improve from 0.47448
Epoch 55/300
 - 40s - loss: 0.0568 - acc: 0.9940 - mDice: 0.8895 - val_loss: -9.0825e-03 - val_acc: 0.9937 - val_mDice: 0.4683

Epoch 00055: val_mDice did not improve from 0.47448
Epoch 56/300
 - 41s - loss: 0.0569 - acc: 0.9940 - mDice: 0.8892 - val_loss: 0.0167 - val_acc: 0.9937 - val_mDice: 0.4767

Epoch 00056: val_mDice improved from 0.47448 to 0.47668, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 57/300
 - 43s - loss: 0.0560 - acc: 0.9941 - mDice: 0.8910 - val_loss: -8.3906e-03 - val_acc: 0.9934 - val_mDice: 0.4700

Epoch 00057: val_mDice did not improve from 0.47668
Epoch 58/300
 - 42s - loss: 0.0553 - acc: 0.9941 - mDice: 0.8924 - val_loss: -1.1976e-02 - val_acc: 0.9937 - val_mDice: 0.4743

Epoch 00058: val_mDice did not improve from 0.47668
Epoch 59/300
 - 40s - loss: 0.0553 - acc: 0.9941 - mDice: 0.8923 - val_loss: -1.0449e-02 - val_acc: 0.9936 - val_mDice: 0.4718

Epoch 00059: val_mDice did not improve from 0.47668
Epoch 60/300
 - 40s - loss: 0.0555 - acc: 0.9941 - mDice: 0.8919 - val_loss: -1.3170e-02 - val_acc: 0.9937 - val_mDice: 0.4770

Epoch 00060: val_mDice improved from 0.47668 to 0.47698, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 61/300
 - 40s - loss: 0.0551 - acc: 0.9942 - mDice: 0.8927 - val_loss: -1.4457e-02 - val_acc: 0.9938 - val_mDice: 0.4793

Epoch 00061: val_mDice improved from 0.47698 to 0.47931, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 62/300
 - 41s - loss: 0.0550 - acc: 0.9942 - mDice: 0.8929 - val_loss: -1.3496e-02 - val_acc: 0.9938 - val_mDice: 0.4773

Epoch 00062: val_mDice did not improve from 0.47931
Epoch 63/300
 - 41s - loss: 0.0547 - acc: 0.9942 - mDice: 0.8936 - val_loss: -1.2961e-02 - val_acc: 0.9937 - val_mDice: 0.4760

Epoch 00063: val_mDice did not improve from 0.47931
Epoch 64/300
 - 41s - loss: 0.0545 - acc: 0.9942 - mDice: 0.8939 - val_loss: -1.3348e-02 - val_acc: 0.9937 - val_mDice: 0.4768

Epoch 00064: val_mDice did not improve from 0.47931
Epoch 65/300
 - 40s - loss: 0.0546 - acc: 0.9943 - mDice: 0.8937 - val_loss: -1.1957e-02 - val_acc: 0.9936 - val_mDice: 0.4740

Epoch 00065: val_mDice did not improve from 0.47931
Epoch 66/300
 - 40s - loss: 0.0546 - acc: 0.9942 - mDice: 0.8937 - val_loss: -1.2862e-02 - val_acc: 0.9936 - val_mDice: 0.4763

Epoch 00066: val_mDice did not improve from 0.47931
Epoch 67/300
 - 41s - loss: 0.0544 - acc: 0.9942 - mDice: 0.8942 - val_loss: -1.1487e-02 - val_acc: 0.9936 - val_mDice: 0.4738

Epoch 00067: val_mDice did not improve from 0.47931
Epoch 68/300
 - 41s - loss: 0.0535 - acc: 0.9943 - mDice: 0.8960 - val_loss: -1.0925e-02 - val_acc: 0.9934 - val_mDice: 0.4723

Epoch 00068: val_mDice did not improve from 0.47931
Epoch 69/300
 - 40s - loss: 0.0528 - acc: 0.9943 - mDice: 0.8974 - val_loss: -9.5645e-03 - val_acc: 0.9933 - val_mDice: 0.4695

Epoch 00069: val_mDice did not improve from 0.47931
Epoch 70/300
 - 40s - loss: 0.0544 - acc: 0.9943 - mDice: 0.8942 - val_loss: -1.3415e-02 - val_acc: 0.9934 - val_mDice: 0.4771

Epoch 00070: val_mDice did not improve from 0.47931
Epoch 71/300
 - 40s - loss: 0.0533 - acc: 0.9943 - mDice: 0.8963 - val_loss: -1.4159e-02 - val_acc: 0.9936 - val_mDice: 0.4784

Epoch 00071: val_mDice did not improve from 0.47931
Epoch 72/300
 - 40s - loss: 0.0545 - acc: 0.9943 - mDice: 0.8939 - val_loss: -1.0437e-02 - val_acc: 0.9933 - val_mDice: 0.4711

Epoch 00072: val_mDice did not improve from 0.47931
Epoch 73/300
 - 40s - loss: 0.0524 - acc: 0.9944 - mDice: 0.8981 - val_loss: -1.5891e-02 - val_acc: 0.9938 - val_mDice: 0.4840

Epoch 00073: val_mDice improved from 0.47931 to 0.48396, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 74/300
 - 40s - loss: 0.0526 - acc: 0.9944 - mDice: 0.8976 - val_loss: -1.8104e-02 - val_acc: 0.9939 - val_mDice: 0.4870

Epoch 00074: val_mDice improved from 0.48396 to 0.48699, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 75/300
 - 40s - loss: 0.0524 - acc: 0.9944 - mDice: 0.8981 - val_loss: -1.6359e-02 - val_acc: 0.9939 - val_mDice: 0.4830

Epoch 00075: val_mDice did not improve from 0.48699
Epoch 76/300
 - 40s - loss: 0.0523 - acc: 0.9944 - mDice: 0.8982 - val_loss: -1.8183e-02 - val_acc: 0.9939 - val_mDice: 0.4877

Epoch 00076: val_mDice improved from 0.48699 to 0.48771, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 77/300
 - 41s - loss: 0.0530 - acc: 0.9944 - mDice: 0.8970 - val_loss: -1.9683e-02 - val_acc: 0.9939 - val_mDice: 0.4915

Epoch 00077: val_mDice improved from 0.48771 to 0.49153, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 78/300
 - 41s - loss: 0.0521 - acc: 0.9944 - mDice: 0.8987 - val_loss: -2.0710e-02 - val_acc: 0.9939 - val_mDice: 0.4920

Epoch 00078: val_mDice improved from 0.49153 to 0.49197, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 79/300
 - 41s - loss: 0.0523 - acc: 0.9945 - mDice: 0.8982 - val_loss: -1.6831e-02 - val_acc: 0.9937 - val_mDice: 0.4840

Epoch 00079: val_mDice did not improve from 0.49197
Epoch 80/300
 - 40s - loss: 0.0515 - acc: 0.9945 - mDice: 0.8998 - val_loss: -1.8394e-02 - val_acc: 0.9937 - val_mDice: 0.4873

Epoch 00080: val_mDice did not improve from 0.49197
Epoch 81/300
 - 40s - loss: 0.0522 - acc: 0.9945 - mDice: 0.8985 - val_loss: -1.8079e-02 - val_acc: 0.9936 - val_mDice: 0.4870

Epoch 00081: val_mDice did not improve from 0.49197
Epoch 82/300
 - 40s - loss: 0.0517 - acc: 0.9945 - mDice: 0.8995 - val_loss: -1.8833e-02 - val_acc: 0.9937 - val_mDice: 0.4882

Epoch 00082: val_mDice did not improve from 0.49197
Epoch 83/300
 - 40s - loss: 0.0516 - acc: 0.9945 - mDice: 0.8996 - val_loss: -1.4236e-02 - val_acc: 0.9936 - val_mDice: 0.4792

Epoch 00083: val_mDice did not improve from 0.49197
Epoch 84/300
 - 40s - loss: 0.0506 - acc: 0.9945 - mDice: 0.9016 - val_loss: -2.0674e-02 - val_acc: 0.9938 - val_mDice: 0.4918

Epoch 00084: val_mDice did not improve from 0.49197
Epoch 85/300
 - 40s - loss: 0.0512 - acc: 0.9945 - mDice: 0.9005 - val_loss: -1.9171e-02 - val_acc: 0.9936 - val_mDice: 0.4890

Epoch 00085: val_mDice did not improve from 0.49197
Epoch 86/300
 - 40s - loss: 0.0514 - acc: 0.9946 - mDice: 0.9001 - val_loss: -1.8947e-02 - val_acc: 0.9937 - val_mDice: 0.4885

Epoch 00086: val_mDice did not improve from 0.49197
Epoch 87/300
 - 40s - loss: 0.0491 - acc: 0.9946 - mDice: 0.9045 - val_loss: -2.0686e-02 - val_acc: 0.9940 - val_mDice: 0.4919

Epoch 00087: val_mDice did not improve from 0.49197
Epoch 88/300
 - 40s - loss: 0.0497 - acc: 0.9946 - mDice: 0.9034 - val_loss: -1.7625e-02 - val_acc: 0.9936 - val_mDice: 0.4859

Epoch 00088: val_mDice did not improve from 0.49197
Epoch 89/300
 - 40s - loss: 0.0505 - acc: 0.9946 - mDice: 0.9018 - val_loss: -1.6765e-02 - val_acc: 0.9937 - val_mDice: 0.4838

Epoch 00089: val_mDice did not improve from 0.49197
Epoch 90/300
 - 40s - loss: 0.0504 - acc: 0.9946 - mDice: 0.9020 - val_loss: -2.1656e-02 - val_acc: 0.9939 - val_mDice: 0.4937

Epoch 00090: val_mDice improved from 0.49197 to 0.49373, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 91/300
 - 41s - loss: 0.0490 - acc: 0.9947 - mDice: 0.9047 - val_loss: -2.0656e-02 - val_acc: 0.9936 - val_mDice: 0.4919

Epoch 00091: val_mDice did not improve from 0.49373
Epoch 92/300
 - 41s - loss: 0.0495 - acc: 0.9946 - mDice: 0.9037 - val_loss: -2.0616e-02 - val_acc: 0.9937 - val_mDice: 0.4918

Epoch 00092: val_mDice did not improve from 0.49373
Epoch 93/300
 - 41s - loss: 0.0498 - acc: 0.9947 - mDice: 0.9031 - val_loss: -1.8253e-02 - val_acc: 0.9934 - val_mDice: 0.4873

Epoch 00093: val_mDice did not improve from 0.49373
Epoch 94/300
 - 40s - loss: 0.0496 - acc: 0.9947 - mDice: 0.9035 - val_loss: -1.7857e-02 - val_acc: 0.9936 - val_mDice: 0.4862

Epoch 00094: val_mDice did not improve from 0.49373
Epoch 95/300
 - 40s - loss: 0.0487 - acc: 0.9947 - mDice: 0.9054 - val_loss: -2.0562e-02 - val_acc: 0.9937 - val_mDice: 0.4916

Epoch 00095: val_mDice did not improve from 0.49373
Epoch 96/300
 - 40s - loss: 0.0494 - acc: 0.9947 - mDice: 0.9039 - val_loss: -1.9707e-02 - val_acc: 0.9936 - val_mDice: 0.4903

Epoch 00096: val_mDice did not improve from 0.49373
Epoch 97/300
 - 40s - loss: 0.0494 - acc: 0.9947 - mDice: 0.9038 - val_loss: -2.0495e-02 - val_acc: 0.9937 - val_mDice: 0.4921

Epoch 00097: val_mDice did not improve from 0.49373
Epoch 98/300
 - 40s - loss: 0.0489 - acc: 0.9947 - mDice: 0.9049 - val_loss: -1.3561e-02 - val_acc: 0.9932 - val_mDice: 0.4800

Epoch 00098: val_mDice did not improve from 0.49373
Epoch 99/300
 - 40s - loss: 0.0482 - acc: 0.9948 - mDice: 0.9063 - val_loss: -2.1583e-02 - val_acc: 0.9939 - val_mDice: 0.4935

Epoch 00099: val_mDice did not improve from 0.49373
Epoch 100/300
 - 40s - loss: 0.0484 - acc: 0.9947 - mDice: 0.9058 - val_loss: -1.8209e-02 - val_acc: 0.9939 - val_mDice: 0.4895

Epoch 00100: val_mDice did not improve from 0.49373
Epoch 101/300
 - 40s - loss: 0.0481 - acc: 0.9948 - mDice: 0.9065 - val_loss: -2.4396e-02 - val_acc: 0.9938 - val_mDice: 0.4995

Epoch 00101: val_mDice improved from 0.49373 to 0.49947, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 102/300
 - 40s - loss: 0.0483 - acc: 0.9948 - mDice: 0.9061 - val_loss: -2.3380e-02 - val_acc: 0.9940 - val_mDice: 0.4980

Epoch 00102: val_mDice did not improve from 0.49947
Epoch 103/300
 - 40s - loss: 0.0477 - acc: 0.9948 - mDice: 0.9073 - val_loss: -2.3841e-02 - val_acc: 0.9939 - val_mDice: 0.4988

Epoch 00103: val_mDice did not improve from 0.49947
Epoch 104/300
 - 40s - loss: 0.0481 - acc: 0.9948 - mDice: 0.9065 - val_loss: -1.6124e-02 - val_acc: 0.9935 - val_mDice: 0.4850

Epoch 00104: val_mDice did not improve from 0.49947
Epoch 105/300
 - 41s - loss: 0.0486 - acc: 0.9948 - mDice: 0.9054 - val_loss: -1.7464e-02 - val_acc: 0.9935 - val_mDice: 0.4884

Epoch 00105: val_mDice did not improve from 0.49947
Epoch 106/300
 - 41s - loss: 0.0482 - acc: 0.9948 - mDice: 0.9063 - val_loss: -2.1173e-02 - val_acc: 0.9938 - val_mDice: 0.4972

Epoch 00106: val_mDice did not improve from 0.49947
Epoch 107/300
 - 41s - loss: 0.0474 - acc: 0.9948 - mDice: 0.9079 - val_loss: -2.1232e-02 - val_acc: 0.9938 - val_mDice: 0.4970

Epoch 00107: val_mDice did not improve from 0.49947
Epoch 108/300
 - 41s - loss: 0.0478 - acc: 0.9948 - mDice: 0.9071 - val_loss: -1.7052e-02 - val_acc: 0.9937 - val_mDice: 0.4922

Epoch 00108: val_mDice did not improve from 0.49947
Epoch 109/300
 - 41s - loss: 0.0476 - acc: 0.9949 - mDice: 0.9074 - val_loss: -1.8551e-02 - val_acc: 0.9940 - val_mDice: 0.4957

Epoch 00109: val_mDice did not improve from 0.49947
Epoch 110/300
 - 41s - loss: 0.0477 - acc: 0.9948 - mDice: 0.9072 - val_loss: -1.7383e-02 - val_acc: 0.9937 - val_mDice: 0.4936

Epoch 00110: val_mDice did not improve from 0.49947
Epoch 111/300
 - 40s - loss: 0.0489 - acc: 0.9948 - mDice: 0.9048 - val_loss: -1.6083e-02 - val_acc: 0.9937 - val_mDice: 0.4971

Epoch 00111: val_mDice did not improve from 0.49947
Epoch 112/300
 - 40s - loss: 0.0474 - acc: 0.9948 - mDice: 0.9079 - val_loss: -1.9832e-02 - val_acc: 0.9936 - val_mDice: 0.4948

Epoch 00112: val_mDice did not improve from 0.49947
Epoch 113/300
 - 41s - loss: 0.0481 - acc: 0.9949 - mDice: 0.9065 - val_loss: -2.2585e-02 - val_acc: 0.9939 - val_mDice: 0.5001

Epoch 00113: val_mDice improved from 0.49947 to 0.50011, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 114/300
 - 40s - loss: 0.0470 - acc: 0.9949 - mDice: 0.9085 - val_loss: -2.0185e-02 - val_acc: 0.9939 - val_mDice: 0.4981

Epoch 00114: val_mDice did not improve from 0.50011
Epoch 115/300
 - 40s - loss: 0.0465 - acc: 0.9949 - mDice: 0.9096 - val_loss: -1.9912e-02 - val_acc: 0.9937 - val_mDice: 0.4915

Epoch 00115: val_mDice did not improve from 0.50011
Epoch 116/300
 - 40s - loss: 0.0474 - acc: 0.9949 - mDice: 0.9078 - val_loss: -2.1783e-02 - val_acc: 0.9938 - val_mDice: 0.4965

Epoch 00116: val_mDice did not improve from 0.50011

Epoch 00116: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 117/300
 - 40s - loss: 0.0467 - acc: 0.9949 - mDice: 0.9093 - val_loss: -2.0790e-02 - val_acc: 0.9937 - val_mDice: 0.4947

Epoch 00117: val_mDice did not improve from 0.50011
Epoch 118/300
 - 40s - loss: 0.0459 - acc: 0.9949 - mDice: 0.9107 - val_loss: -1.9369e-02 - val_acc: 0.9936 - val_mDice: 0.4928

Epoch 00118: val_mDice did not improve from 0.50011
Epoch 119/300
 - 41s - loss: 0.0469 - acc: 0.9949 - mDice: 0.9088 - val_loss: -1.6870e-02 - val_acc: 0.9936 - val_mDice: 0.4896

Epoch 00119: val_mDice did not improve from 0.50011
Epoch 120/300
 - 40s - loss: 0.0457 - acc: 0.9950 - mDice: 0.9111 - val_loss: -2.2101e-02 - val_acc: 0.9940 - val_mDice: 0.4976

Epoch 00120: val_mDice did not improve from 0.50011
Epoch 121/300
 - 41s - loss: 0.0463 - acc: 0.9949 - mDice: 0.9100 - val_loss: -2.0548e-02 - val_acc: 0.9940 - val_mDice: 0.5005

Epoch 00121: val_mDice improved from 0.50011 to 0.50049, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_wBiasCorrection_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 122/300
 - 40s - loss: 0.0463 - acc: 0.9950 - mDice: 0.9100 - val_loss: -2.0171e-02 - val_acc: 0.9938 - val_mDice: 0.4980

Epoch 00122: val_mDice did not improve from 0.50049
Epoch 123/300
 - 40s - loss: 0.0459 - acc: 0.9950 - mDice: 0.9107 - val_loss: -2.1035e-02 - val_acc: 0.9938 - val_mDice: 0.4976

Epoch 00123: val_mDice did not improve from 0.50049
Epoch 124/300
 - 40s - loss: 0.0465 - acc: 0.9950 - mDice: 0.9096 - val_loss: -2.0499e-02 - val_acc: 0.9939 - val_mDice: 0.4965

Epoch 00124: val_mDice did not improve from 0.50049
Epoch 125/300
 - 40s - loss: 0.0453 - acc: 0.9950 - mDice: 0.9119 - val_loss: -1.9607e-02 - val_acc: 0.9938 - val_mDice: 0.4961

Epoch 00125: val_mDice did not improve from 0.50049
Epoch 126/300
 - 40s - loss: 0.0456 - acc: 0.9950 - mDice: 0.9114 - val_loss: -1.8889e-02 - val_acc: 0.9938 - val_mDice: 0.4982

Epoch 00126: val_mDice did not improve from 0.50049
Epoch 127/300
 - 40s - loss: 0.0460 - acc: 0.9950 - mDice: 0.9106 - val_loss: -1.5368e-02 - val_acc: 0.9938 - val_mDice: 0.4966

Epoch 00127: val_mDice did not improve from 0.50049
Epoch 128/300
 - 40s - loss: 0.0459 - acc: 0.9950 - mDice: 0.9107 - val_loss: -1.6747e-02 - val_acc: 0.9936 - val_mDice: 0.4945

Epoch 00128: val_mDice did not improve from 0.50049
Epoch 129/300
 - 40s - loss: 0.0453 - acc: 0.9950 - mDice: 0.9119 - val_loss: -1.9878e-02 - val_acc: 0.9938 - val_mDice: 0.4976

Epoch 00129: val_mDice did not improve from 0.50049
Epoch 130/300
 - 40s - loss: 0.0454 - acc: 0.9950 - mDice: 0.9118 - val_loss: -1.7942e-02 - val_acc: 0.9936 - val_mDice: 0.4939

Epoch 00130: val_mDice did not improve from 0.50049
Epoch 131/300
 - 40s - loss: 0.0455 - acc: 0.9950 - mDice: 0.9116 - val_loss: -2.1030e-02 - val_acc: 0.9939 - val_mDice: 0.4990

Epoch 00131: val_mDice did not improve from 0.50049
Epoch 132/300
 - 40s - loss: 0.0455 - acc: 0.9950 - mDice: 0.9114 - val_loss: -1.5231e-02 - val_acc: 0.9937 - val_mDice: 0.4955

Epoch 00132: val_mDice did not improve from 0.50049
Epoch 133/300
 - 41s - loss: 0.0458 - acc: 0.9950 - mDice: 0.9109 - val_loss: -1.6304e-02 - val_acc: 0.9936 - val_mDice: 0.4960

Epoch 00133: val_mDice did not improve from 0.50049
Epoch 134/300
 - 41s - loss: 0.0458 - acc: 0.9950 - mDice: 0.9110 - val_loss: -1.9125e-02 - val_acc: 0.9939 - val_mDice: 0.4996

Epoch 00134: val_mDice did not improve from 0.50049
Epoch 135/300
 - 41s - loss: 0.0457 - acc: 0.9950 - mDice: 0.9110 - val_loss: -1.8465e-02 - val_acc: 0.9939 - val_mDice: 0.4987

Epoch 00135: val_mDice did not improve from 0.50049
Epoch 136/300
 - 40s - loss: 0.0449 - acc: 0.9950 - mDice: 0.9126 - val_loss: -1.4519e-02 - val_acc: 0.9936 - val_mDice: 0.4923

Epoch 00136: val_mDice did not improve from 0.50049

Epoch 00136: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 137/300
 - 40s - loss: 0.0444 - acc: 0.9950 - mDice: 0.9138 - val_loss: -9.8275e-03 - val_acc: 0.9937 - val_mDice: 0.4966

Epoch 00137: val_mDice did not improve from 0.50049
Epoch 138/300
 - 40s - loss: 0.0448 - acc: 0.9951 - mDice: 0.9130 - val_loss: 0.0095 - val_acc: 0.9937 - val_mDice: 0.4933

Epoch 00138: val_mDice did not improve from 0.50049
Epoch 139/300
 - 40s - loss: 0.0448 - acc: 0.9950 - mDice: 0.9130 - val_loss: -1.6752e-02 - val_acc: 0.9937 - val_mDice: 0.4953

Epoch 00139: val_mDice did not improve from 0.50049
Epoch 140/300
 - 40s - loss: 0.0442 - acc: 0.9951 - mDice: 0.9142 - val_loss: -1.8287e-02 - val_acc: 0.9938 - val_mDice: 0.4967

Epoch 00140: val_mDice did not improve from 0.50049
Epoch 141/300
 - 40s - loss: 0.0446 - acc: 0.9951 - mDice: 0.9133 - val_loss: 0.0037 - val_acc: 0.9938 - val_mDice: 0.4988

Epoch 00141: val_mDice did not improve from 0.50049
Epoch 142/300
 - 40s - loss: 0.0448 - acc: 0.9951 - mDice: 0.9128 - val_loss: -8.6205e-03 - val_acc: 0.9937 - val_mDice: 0.4946

Epoch 00142: val_mDice did not improve from 0.50049
Epoch 143/300
 - 40s - loss: 0.0454 - acc: 0.9950 - mDice: 0.9116 - val_loss: -1.4628e-02 - val_acc: 0.9938 - val_mDice: 0.4975

Epoch 00143: val_mDice did not improve from 0.50049
Epoch 144/300
 - 40s - loss: 0.0448 - acc: 0.9951 - mDice: 0.9128 - val_loss: -1.6561e-02 - val_acc: 0.9937 - val_mDice: 0.4948

Epoch 00144: val_mDice did not improve from 0.50049
Epoch 145/300
 - 40s - loss: 0.0449 - acc: 0.9951 - mDice: 0.9127 - val_loss: -1.3290e-02 - val_acc: 0.9937 - val_mDice: 0.4965

Epoch 00145: val_mDice did not improve from 0.50049
Epoch 146/300
 - 40s - loss: 0.0451 - acc: 0.9951 - mDice: 0.9124 - val_loss: -1.0302e-02 - val_acc: 0.9936 - val_mDice: 0.4891

Epoch 00146: val_mDice did not improve from 0.50049
Epoch 147/300
 - 40s - loss: 0.0450 - acc: 0.9951 - mDice: 0.9126 - val_loss: -1.2117e-02 - val_acc: 0.9939 - val_mDice: 0.4976

Epoch 00147: val_mDice did not improve from 0.50049
Epoch 148/300
 - 41s - loss: 0.0450 - acc: 0.9951 - mDice: 0.9124 - val_loss: -1.0064e-02 - val_acc: 0.9938 - val_mDice: 0.4965

Epoch 00148: val_mDice did not improve from 0.50049
Epoch 149/300
 - 40s - loss: 0.0454 - acc: 0.9951 - mDice: 0.9116 - val_loss: -9.3312e-03 - val_acc: 0.9937 - val_mDice: 0.4965

Epoch 00149: val_mDice did not improve from 0.50049
Epoch 150/300
 - 41s - loss: 0.0437 - acc: 0.9951 - mDice: 0.9151 - val_loss: -8.1686e-03 - val_acc: 0.9937 - val_mDice: 0.4937

Epoch 00150: val_mDice did not improve from 0.50049
Epoch 151/300
 - 41s - loss: 0.0447 - acc: 0.9951 - mDice: 0.9131 - val_loss: -1.1712e-02 - val_acc: 0.9937 - val_mDice: 0.4942

Epoch 00151: val_mDice did not improve from 0.50049

Epoch 00151: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 152/300
 - 40s - loss: 0.0454 - acc: 0.9951 - mDice: 0.9117 - val_loss: -1.1174e-02 - val_acc: 0.9937 - val_mDice: 0.4941

Epoch 00152: val_mDice did not improve from 0.50049
Epoch 153/300
 - 40s - loss: 0.0447 - acc: 0.9951 - mDice: 0.9131 - val_loss: -1.3471e-02 - val_acc: 0.9937 - val_mDice: 0.4949

Epoch 00153: val_mDice did not improve from 0.50049
Epoch 154/300
 - 40s - loss: 0.0442 - acc: 0.9951 - mDice: 0.9140 - val_loss: -1.4140e-02 - val_acc: 0.9938 - val_mDice: 0.4948

Epoch 00154: val_mDice did not improve from 0.50049
Epoch 155/300
 - 41s - loss: 0.0444 - acc: 0.9951 - mDice: 0.9137 - val_loss: -8.8464e-03 - val_acc: 0.9938 - val_mDice: 0.4957

Epoch 00155: val_mDice did not improve from 0.50049
Epoch 156/300
 - 40s - loss: 0.0452 - acc: 0.9951 - mDice: 0.9121 - val_loss: -1.4803e-02 - val_acc: 0.9936 - val_mDice: 0.4946

Epoch 00156: val_mDice did not improve from 0.50049
Epoch 157/300
 - 40s - loss: 0.0441 - acc: 0.9951 - mDice: 0.9144 - val_loss: -2.7727e-03 - val_acc: 0.9935 - val_mDice: 0.4924

Epoch 00157: val_mDice did not improve from 0.50049
Epoch 158/300
 - 40s - loss: 0.0448 - acc: 0.9951 - mDice: 0.9129 - val_loss: -1.2194e-02 - val_acc: 0.9938 - val_mDice: 0.4946

Epoch 00158: val_mDice did not improve from 0.50049
Epoch 159/300
 - 40s - loss: 0.0450 - acc: 0.9951 - mDice: 0.9124 - val_loss: -1.3813e-02 - val_acc: 0.9938 - val_mDice: 0.4960

Epoch 00159: val_mDice did not improve from 0.50049
Epoch 160/300
 - 40s - loss: 0.0447 - acc: 0.9951 - mDice: 0.9132 - val_loss: -1.4895e-02 - val_acc: 0.9936 - val_mDice: 0.4947

Epoch 00160: val_mDice did not improve from 0.50049
Epoch 161/300
 - 40s - loss: 0.0445 - acc: 0.9951 - mDice: 0.9136 - val_loss: -1.2274e-02 - val_acc: 0.9936 - val_mDice: 0.4924

Epoch 00161: val_mDice did not improve from 0.50049
Restoring model weights from the end of the best epoch
Epoch 00161: early stopping
{'val_loss': [0.31272181985700653, 0.2946694605286766, 0.29089178327681886, 0.2864108611390276, 0.28500856192044494, 0.2839474731927398, 0.2829476874204552, 0.280817430208689, 0.28070225173010016, 0.27876245436615915, 0.273454523592625, 0.2694578137037889, 0.26499123126268387, 0.26272010259658285, 0.25302147040577055, 0.25234975618948724, 0.18270024380781366, 0.21454365532728112, 0.19582415414306353, 0.2025173027185524, 0.16471598944011726, 0.16189995314340172, 0.13233455828150864, 0.13996726705593132, 0.11195135786660812, 0.15657305661237464, 0.07896362760531828, 0.08422500063788216, 0.05155801773071289, 0.04689097671576266, 0.04589778559762727, 0.04137085134503227, 0.041643813158731044, 0.037291671073286785, 0.03852668207771373, 0.038046620177023066, 0.010821597428066926, 0.06830060032178771, 0.016987689308025553, 0.06694348957741035, -0.0012037328110550934, 0.002425817573595347, -0.0002915518088910565, -0.0035659176467349694, -0.0038068579427851072, -0.002969035937351251, -0.0066488200107460505, -0.004844567638898046, -0.006237388360050489, -0.008415137962350305, -0.008033615519415657, -0.007370795187710217, -0.012184192502648575, -0.010562991945998473, -0.00908254382182967, 0.01667621816104313, -0.008390592516593213, -0.011976497540683867, -0.01044875608300263, -0.013169836932383243, -0.014456745213682547, -0.01349556895921815, -0.012960532129560626, -0.013348387612861657, -0.011957245240421415, -0.012861650647982111, -0.011486960985000778, -0.010924866036424096, -0.009564466680745658, -0.013414914503037554, -0.014159193719333073, -0.010437123290022964, -0.015891001846805308, -0.018103820207358907, -0.016358520109323586, -0.0181832787750652, -0.01968340589752737, -0.02070960913251781, -0.0168311430035897, -0.01839369366753776, -0.018079157707826147, -0.0188333032266149, -0.014235793560181023, -0.020673829523272486, -0.019171254299346754, -0.01894657384112196, -0.020685730584012636, -0.01762543571820049, -0.016765386428473132, -0.021655968197111814, -0.02065569026477682, -0.020615558016974973, -0.018253359826480818, -0.017857362779806246, -0.020561594258314407, -0.019707039139180812, -0.02049521563000649, -0.013560522089964189, -0.021583209903735034, -0.01820850302025957, -0.02439580496939473, -0.023380240929201716, -0.02384120394598763, -0.016124194663650584, -0.017464037445731134, -0.021172552551113586, -0.021231979928301566, -0.01705220641580018, -0.018551238021760615, -0.01738292583317127, -0.016082761563220114, -0.019831810919743665, -0.022584837750069, -0.020185038915970042, -0.019911718021773692, -0.021783354147425237, -0.02078951839006172, -0.019369218053307925, -0.016869632796671405, -0.022101213722109044, -0.020548346282551123, -0.020171072451198625, -0.021035140462266573, -0.020499306139331194, -0.01960674040722397, -0.018888715303169108, -0.015368393710199392, -0.016746557686688764, -0.01987770420012984, -0.017941600730958976, -0.021030184982707666, -0.01523147443742872, -0.016303703844922145, -0.01912541022090792, -0.018464950645494763, -0.01451850246708348, -0.009827526063664155, 0.009518092908199478, -0.016752197401328658, -0.01828662948038593, 0.003671513600919232, -0.008620530179461593, -0.01462754026149054, -0.016561408462764333, -0.013290251529066818, -0.010302324480605576, -0.012117427287611572, -0.010064098445124596, -0.00933124415529599, -0.00816862257021778, -0.011711960318703321, -0.011174469389630563, -0.01347099745985847, -0.01414043034585017, -0.00884638839172867, -0.01480329378783328, -0.0027726746205263917, -0.012193610458254064, -0.013813380831442538, -0.01489519604347037, -0.012274132631484818], 'val_acc': [0.9862835572200751, 0.9897703310978487, 0.9900241036834957, 0.9910437067349752, 0.991213138373393, 0.9914872462644517, 0.991570327266957, 0.9920141153365561, 0.9921549329967618, 0.9922081498230029, 0.9924614144571172, 0.9924365623192217, 0.9924890275271434, 0.9927739189855708, 0.9928276426387284, 0.9927465645772107, 0.9926782853948245, 0.9928645349148685, 0.9930337165136757, 0.9930108736895915, 0.9930565612121198, 0.9931609799277108, 0.9930831658765205, 0.9930972254501199, 0.9931577166671273, 0.9930786449204451, 0.9931037519712868, 0.9931943660262246, 0.9931906004371883, 0.9932877391389331, 0.9933883997629274, 0.9933517485294702, 0.9934247904603586, 0.9933871458161552, 0.9933642992433512, 0.9934074713748956, 0.9933482303559406, 0.9930475230486888, 0.9935264463694591, 0.9935076240473574, 0.9934001913610494, 0.9933861355361698, 0.9931464199000185, 0.9934142509346489, 0.9934498956368404, 0.9934790119435053, 0.9937571332139788, 0.993639404668748, 0.993401197892315, 0.9937629024937468, 0.9934378416283326, 0.993545023150414, 0.9938060708765714, 0.9936180707043821, 0.9936632465266582, 0.9937096894162256, 0.9934157560456474, 0.9937320317862168, 0.9935688743801236, 0.9937373006118918, 0.9938414681632564, 0.9937641526917992, 0.9937360429163998, 0.9937149601162605, 0.9935711311094416, 0.9935924669481674, 0.993558831559787, 0.9934175104465125, 0.9932669093773799, 0.9934084760318013, 0.9936496967789512, 0.9933171084841842, 0.9937804764921561, 0.993905226389567, 0.9938906663618747, 0.9938502514137412, 0.9939084877757907, 0.9938841379663479, 0.9936780577185769, 0.9936911107609107, 0.993601752527105, 0.9937388057228904, 0.993562342235877, 0.9937787127194915, 0.9935716334378945, 0.9936707758303708, 0.9940344953686936, 0.9936102883620832, 0.9937433210558861, 0.9939315798897413, 0.993648943286272, 0.9937292671053665, 0.9933582731762772, 0.9935977357738422, 0.9936619944542459, 0.9936333804760339, 0.9936963833353054, 0.9932385409403147, 0.9938560244422289, 0.9938803742516715, 0.9937528634221299, 0.9940103985978372, 0.9939378608697615, 0.9934875440297637, 0.9934852835517259, 0.9937621490010675, 0.9937573825038454, 0.9936903610169513, 0.9939777678663626, 0.9936710307433171, 0.993685339606783, 0.9936436725862371, 0.9939019631289836, 0.9938560263165888, 0.9937011573299672, 0.9937797211251169, 0.9937458364468701, 0.9936210790520195, 0.9936223367475113, 0.9939546738780519, 0.9940048786079358, 0.9938459816218922, 0.9937932708728239, 0.9938793695947659, 0.9938033193162402, 0.9938331834925046, 0.9937789676324377, 0.9936273581576798, 0.9938266569713377, 0.9936401562870674, 0.993884893333387, 0.993696134045439, 0.9936409135284664, 0.9938728449479589, 0.9939285715421041, 0.9936404074512938, 0.9937375480273984, 0.9936863442636886, 0.9937182233768439, 0.9938389583953522, 0.993823397459474, 0.9936562195513983, 0.9938023127849747, 0.9937375480273984, 0.993726001970423, 0.9935914585425419, 0.9938565305194015, 0.993791512723239, 0.9937430680172998, 0.9936843368242372, 0.9936941247316277, 0.9936732912213547, 0.9937420689834738, 0.9938168690639472, 0.993826651348258, 0.993605265077555, 0.9935334808421585, 0.993806076499651, 0.9937694308892736, 0.9936401544127075, 0.9936178214145157], 'val_mDice': [0.38086479018554625, 0.415606734038304, 0.4232396604786294, 0.4307956662424301, 0.4333418983462905, 0.43531665473392905, 0.43657470452142716, 0.43908531912354026, 0.44044842088413905, 0.4415701193819932, 0.44413901232887953, 0.4441131705292046, 0.4439339629795501, 0.44612569137004887, 0.4477839208779503, 0.44641499636145743, 0.4447909798355977, 0.4483808095438785, 0.4488955255405664, 0.44892077570368977, 0.4494681720564131, 0.4497009774569432, 0.4510924946413342, 0.4500192007946037, 0.45027325527326384, 0.44923336341226106, 0.44950509118311227, 0.45130232222400996, 0.4515736507333862, 0.4521458951225138, 0.4535168420828904, 0.4546921110346638, 0.4555853008169611, 0.45376167346116847, 0.45663209151734346, 0.4560932154187335, 0.45490871819099354, 0.4558742746898214, 0.458293843182659, 0.4597771864787594, 0.45942246541008175, 0.45686526733608884, 0.4528282770868891, 0.45756592075919783, 0.4589100923604797, 0.4573592416548399, 0.46480416413982334, 0.4606296363764908, 0.4631250600020091, 0.46731069133435404, 0.46635345432837055, 0.46500454053470175, 0.47447702607268805, 0.47124832341412326, 0.4683295325756823, 0.476676151273573, 0.46995223300683797, 0.474279938652268, 0.47180733707903316, 0.47697657145231775, 0.4793118877605822, 0.47729967173727805, 0.4760405719983128, 0.4767993609157373, 0.47400761946286046, 0.4763121334777313, 0.47382305557810284, 0.4722536753558513, 0.46954943687192296, 0.4771195141211996, 0.47841604119576747, 0.4711299017071724, 0.48395836690686783, 0.4869927784556863, 0.4830486337953019, 0.4877112904247248, 0.49153413811007385, 0.4919720722256966, 0.4839628342093912, 0.4872801281371207, 0.48698187483556615, 0.4882363811041574, 0.479198398548852, 0.4918118821656179, 0.4889614894142691, 0.48852410706334143, 0.49191834416779334, 0.4859070671991732, 0.4838332313599077, 0.4937338112097866, 0.49193054999945296, 0.4917658532752931, 0.48729086400202987, 0.4862121694117972, 0.491598165072735, 0.4902698043382393, 0.4920584502272636, 0.47997560562952507, 0.4935154431271103, 0.48950316864739424, 0.4994695151002152, 0.49803539876293085, 0.49881977105290637, 0.4850270261742034, 0.4883511486293385, 0.49719003433326503, 0.49700733531945906, 0.4921564300480129, 0.4956547128704359, 0.49362626653047476, 0.4971123177487895, 0.4948492432540318, 0.5001120761318026, 0.49807333065278875, 0.49153571468104357, 0.4964678518989551, 0.4946841847784114, 0.49283221645175285, 0.4896030771844792, 0.49756686689343843, 0.5004943284036228, 0.49795509415602535, 0.49763342963074736, 0.4965054309405621, 0.4961011839925118, 0.49818441178063927, 0.4965722326587581, 0.4944896726113445, 0.4976420657439802, 0.49391577406874243, 0.49901326122523854, 0.49546300802590715, 0.4959604078493778, 0.49956736137282176, 0.49869254934337903, 0.4923137966191994, 0.49656620424873427, 0.4933435929083974, 0.4953349990852224, 0.4967079110115579, 0.49878333677660747, 0.49457517153811903, 0.4975063695847613, 0.49475527737500535, 0.4964856231737437, 0.48913590235155335, 0.4975536109516456, 0.49649266608106263, 0.4964836724336792, 0.49369934641715113, 0.4942364030101764, 0.49409695485103056, 0.4948788909417278, 0.4947506088130879, 0.49572313919007405, 0.4946432281402672, 0.49239618129700236, 0.49457109527392956, 0.4960187299633926, 0.49465074432346057, 0.4923785974582036], 'loss': [0.36550098808515274, 0.14788314413086945, 0.1228335587743156, 0.1117039105310631, 0.1042984111578826, 0.09807937221318655, 0.0951363261213889, 0.09049148351998512, 0.08821165999808821, 0.08532709302090297, 0.0848721673688439, 0.08164605414497195, 0.08017539576041666, 0.07764212139662616, 0.07624770903875774, 0.07579100142395921, 0.07623914762937418, 0.07429352996094502, 0.07407689272770117, 0.072964276234745, 0.0712824830584215, 0.06980662497146584, 0.06939000483945172, 0.06996674767290455, 0.06827477767810794, 0.06757183900034675, 0.06724877840702033, 0.06703448186873141, 0.06558130951642425, 0.06622441270221169, 0.06530984161975943, 0.06355095907384156, 0.06428098859138606, 0.06350263147379104, 0.0633601334143916, 0.06360013927473603, 0.06272699996776995, 0.06162955052825423, 0.06208654773071033, 0.06209245860208654, 0.06098861134399437, 0.061028232178647314, 0.06067469380526479, 0.06053015051781759, 0.05924115222468731, 0.060096708349149465, 0.059420673902840886, 0.05862666242313403, 0.05962307892607822, 0.057050059636161346, 0.05871791618089332, 0.056871262004277924, 0.0575203216281815, 0.05727967783905348, 0.05677147034577046, 0.05691486299614057, 0.05601196972865599, 0.05532519632881197, 0.055337423412698435, 0.055532457660553786, 0.05514282114891139, 0.05501417053157655, 0.054658838132167285, 0.05452429993279291, 0.05459177276980523, 0.05462419430858389, 0.05435191636152861, 0.053471799135156095, 0.052781145031923785, 0.05436926002988913, 0.05329150214222608, 0.054491798925181224, 0.052388647415559764, 0.05261963237440327, 0.05239950528474751, 0.052305082180280585, 0.05295330649076867, 0.052088639236397505, 0.052323226752602925, 0.051497902018667364, 0.05217758372489221, 0.05166347400208874, 0.05160524286134607, 0.050602545991008434, 0.05115526661102122, 0.05135726439470393, 0.04912187008340033, 0.04966524850329011, 0.0504749764837687, 0.050383880525366506, 0.04902747088249231, 0.04949580489600439, 0.04979466326510588, 0.04958166437208222, 0.04865619308278281, 0.049400141052427834, 0.04943979548996123, 0.048917007307090137, 0.04817766516780015, 0.048443916280925585, 0.04806123560363737, 0.048286457291213496, 0.04768111623487261, 0.04809914760330604, 0.048616387957303296, 0.04815645767647627, 0.047377530158302025, 0.047763448155604266, 0.04762102959663174, 0.04771984305096166, 0.04890337554809012, 0.047356947850418316, 0.04805804636393529, 0.047045045684447874, 0.046475463606051065, 0.04741029436152961, 0.04665125324303713, 0.0459419008809957, 0.04686218507664107, 0.04574438155600632, 0.04626204762268643, 0.04629130066128763, 0.04592092246482515, 0.04646480951461003, 0.0452903314995614, 0.04558189911779456, 0.045976464206157225, 0.045939520230931695, 0.04530480872104667, 0.045384662798713886, 0.04545154673971717, 0.04554391874803751, 0.04579461751290615, 0.045773463709808164, 0.0457459713796274, 0.044945007567841026, 0.04438370799547747, 0.04476603161079921, 0.04477440220521971, 0.0441587347175808, 0.04457562474590134, 0.044838573928596255, 0.04544696187469007, 0.044838599587474035, 0.044910673256503525, 0.045058575648155616, 0.04495737889359258, 0.04502282969618296, 0.04543400358201471, 0.04369703931726997, 0.04467066310496662, 0.045393792547640525, 0.044684037035593635, 0.0442255109048642, 0.04436444678388203, 0.04520943742008482, 0.04406482651904058, 0.04477439760830998, 0.04501970996209052, 0.0446656267986865, 0.044461612186138394], 'acc': [0.972808705991746, 0.9844831434099655, 0.9872878065992989, 0.9885198040164953, 0.9893289014314232, 0.9898540118993087, 0.9902639735753056, 0.9906849065053549, 0.9909078195508979, 0.9911762830785279, 0.9913972399275807, 0.9915766732134882, 0.9917304817727451, 0.991890805756014, 0.9920306121152248, 0.9921550984304128, 0.9922414744887514, 0.9923026304451359, 0.9923789120576839, 0.9924627898205661, 0.9926085051648127, 0.9926636966647397, 0.9927373109880188, 0.9927735976088699, 0.9928657339231187, 0.9929443162244048, 0.99298417618757, 0.9930073326441022, 0.993111608197309, 0.9931594989236735, 0.9932121615229654, 0.9932821623239472, 0.9932869998675447, 0.9933246906914596, 0.993397768089735, 0.9934302017512717, 0.9934781267848743, 0.9935120839182353, 0.9935246668690962, 0.99358399073719, 0.9935986891400044, 0.9935948611064039, 0.9936762408340146, 0.9936833284641032, 0.9937087142666726, 0.993731034823444, 0.9938013384053165, 0.9938164650724843, 0.9938499050474607, 0.9938732354115833, 0.9939019261125497, 0.9939545444506723, 0.993967720742058, 0.9940091541474168, 0.9940060877047682, 0.9940398707892955, 0.994069487956671, 0.9940652917614302, 0.9941124996139272, 0.9941224696690318, 0.9941534009931925, 0.9941701758202248, 0.9941989758137189, 0.9941759441121316, 0.9942607828270738, 0.9942455812192195, 0.9942285327520058, 0.9942662977211848, 0.994286591563923, 0.9942958202853289, 0.9943266371950362, 0.994322386688055, 0.9943663634603296, 0.9943966177833677, 0.9943935459957779, 0.9944143670028688, 0.9943623356790607, 0.9944435616596943, 0.994459883103379, 0.9945096398535795, 0.9944872796594962, 0.9945180862733152, 0.9944938603601838, 0.9945488783019116, 0.9945489623936729, 0.9945639204981916, 0.9945983951913653, 0.9946175391803688, 0.9946001569256577, 0.9946404158712236, 0.9946538819342546, 0.9945952100969664, 0.99465773861793, 0.9947046577692568, 0.9947183512487623, 0.9946955835382134, 0.9946861908259788, 0.994707375653339, 0.9947580082156647, 0.9947409993634613, 0.9947507101851945, 0.9947860772010839, 0.9947794561866038, 0.9947561758597553, 0.994790596010739, 0.9947721741835237, 0.9948092575425904, 0.994798267451151, 0.994851069048653, 0.9948401090345884, 0.9948458535158595, 0.9948492673722608, 0.994867978086493, 0.9948786103047466, 0.994898979608541, 0.9948849228960405, 0.9949373968018056, 0.9949133898379886, 0.9949235784855827, 0.9949530560972091, 0.9949196015593119, 0.9949583874717559, 0.9949677097853891, 0.9949623497310319, 0.9950048991809554, 0.9949717914992718, 0.9949857589757838, 0.9949958791650123, 0.9949873862465173, 0.9950049178994008, 0.9950120708805036, 0.9950116578927211, 0.9949990941063369, 0.9950204982175349, 0.9950088259881796, 0.9950307966928941, 0.9949991641927164, 0.9950864964380143, 0.9950396616460677, 0.9950645002574806, 0.9950654310726947, 0.9950545042805257, 0.9950388404133017, 0.99506339002619, 0.9950677449416567, 0.9950621057981028, 0.9950768051896198, 0.99507288296165, 0.9951046298465458, 0.995073942560958, 0.9950730462611301, 0.9950828881194153, 0.9950703438245974, 0.9950785125601348, 0.9951001250274046, 0.9950873823527476, 0.9950893188420897, 0.9950804189795358, 0.9950993134809509, 0.9950878758194381, 0.9950936836817521], 'mDice': [0.2832173312154832, 0.7123677038486006, 0.7609667938670575, 0.7825683231456018, 0.7969518237513723, 0.8091118542981527, 0.8147759895138728, 0.8238478185243754, 0.82828318656712, 0.8339113325544154, 0.8347040121795516, 0.841061694097205, 0.8439210823491733, 0.8488996363652215, 0.8516157730748511, 0.8524669894332229, 0.8515236641259151, 0.8553875108191183, 0.8557797639969208, 0.857957078062694, 0.8612442968980175, 0.8641668566987734, 0.8649610746492215, 0.8637883625750505, 0.8671229914328825, 0.8684890322295773, 0.8691129891769149, 0.8695270051294068, 0.8723817978074728, 0.8710670387597713, 0.8728678782588326, 0.8763504763112103, 0.8748852709853296, 0.8764197147024126, 0.8766745479661767, 0.876172947877925, 0.8778895656904635, 0.8800718051502906, 0.8791479625151848, 0.8791101508425169, 0.8813045538774182, 0.8812303124188138, 0.8818909794089913, 0.8821787375870355, 0.8847430189455847, 0.8830153499005439, 0.8843355974236846, 0.8859130087975077, 0.8839039564780865, 0.8890389755948661, 0.8856879090465165, 0.8893525809668357, 0.8880444622732631, 0.8885051513763309, 0.8895198600314953, 0.8892218903170904, 0.8910082508371964, 0.8923857408956931, 0.8923356915717486, 0.8919388257221678, 0.8927022738553664, 0.8929495169558365, 0.8936447517964482, 0.893920085881415, 0.8937457142639628, 0.8936938474768793, 0.89423976272129, 0.895979690307184, 0.8973511850285274, 0.8941728570495041, 0.8963122774602231, 0.8939104410746953, 0.8980913354042088, 0.8976199348115897, 0.8980582940382207, 0.8982384102388818, 0.8969627838460817, 0.8986548727889794, 0.8981739464588788, 0.8998004642117471, 0.8984523008196741, 0.8994655590002623, 0.8995911240815581, 0.9015665046813096, 0.900458998037194, 0.9000507368429488, 0.9045009187188758, 0.9034067686544032, 0.9017972004748314, 0.9019571086547453, 0.9046630825513793, 0.9037492844225254, 0.9031307729410394, 0.9035293410234125, 0.9053700202850666, 0.9038933907110853, 0.9038180281971214, 0.9048517925591015, 0.906306599988608, 0.9057818342397028, 0.9065436111513621, 0.9060717352305186, 0.9072877801164899, 0.9064637333429731, 0.9054065824291495, 0.9063399398221313, 0.9078818484056306, 0.9071094680714114, 0.9073686670613247, 0.9071756278983105, 0.9048083558823588, 0.9078925706875476, 0.9064787861064771, 0.9085040563037926, 0.9096293728965468, 0.9077713406124625, 0.9092597271489752, 0.9106902665228402, 0.90884540154768, 0.9110645830257705, 0.9100479273165951, 0.9099639772997141, 0.9107036936845111, 0.9096217721075295, 0.9119450721279229, 0.9113791024753297, 0.9105831720819975, 0.9106533881104938, 0.9119260112754304, 0.9117587723802467, 0.9116204430535715, 0.9114365163557724, 0.9109422876691248, 0.9109720764637559, 0.9110324261758832, 0.9126180360169445, 0.9137547242653343, 0.9129539009236611, 0.9129541966126548, 0.9141752334396629, 0.9133416546546304, 0.9128251258062069, 0.911611058840091, 0.9128163316677146, 0.912671781479792, 0.9123736566314081, 0.91257089678357, 0.9124409837574399, 0.911610387180594, 0.9150915607293367, 0.9131461941904485, 0.9117040323769158, 0.9131237560022231, 0.914037570903711, 0.9137469670233747, 0.9120605581172416, 0.9143535805836851, 0.9129324548372041, 0.9124376524633002, 0.9131516014051783, 0.913558645018261], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:02,  1.53it/s]predicting test subjects:  40%|████      | 2/5 [00:00<00:01,  1.96it/s]predicting test subjects:  60%|██████    | 3/5 [00:00<00:00,  2.48it/s]predicting test subjects:  80%|████████  | 4/5 [00:01<00:00,  3.04it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  3.80it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  4.00it/s]
predicting train subjects:   0%|          | 0/247 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/247 [00:00<00:28,  8.54it/s]predicting train subjects:   1%|          | 2/247 [00:00<00:27,  8.75it/s]predicting train subjects:   1%|          | 3/247 [00:00<00:27,  8.93it/s]predicting train subjects:   2%|▏         | 4/247 [00:00<00:27,  8.86it/s]predicting train subjects:   2%|▏         | 5/247 [00:00<00:27,  8.78it/s]predicting train subjects:   2%|▏         | 6/247 [00:00<00:27,  8.66it/s]predicting train subjects:   3%|▎         | 7/247 [00:00<00:27,  8.66it/s]predicting train subjects:   3%|▎         | 8/247 [00:00<00:27,  8.66it/s]predicting train subjects:   4%|▎         | 9/247 [00:01<00:27,  8.74it/s]predicting train subjects:   4%|▍         | 10/247 [00:01<00:27,  8.69it/s]predicting train subjects:   4%|▍         | 11/247 [00:01<00:27,  8.73it/s]predicting train subjects:   5%|▍         | 12/247 [00:01<00:26,  8.75it/s]predicting train subjects:   5%|▌         | 13/247 [00:01<00:27,  8.66it/s]predicting train subjects:   6%|▌         | 14/247 [00:01<00:26,  8.66it/s]predicting train subjects:   6%|▌         | 15/247 [00:01<00:26,  8.70it/s]predicting train subjects:   6%|▋         | 16/247 [00:01<00:26,  8.72it/s]predicting train subjects:   7%|▋         | 17/247 [00:01<00:26,  8.71it/s]predicting train subjects:   7%|▋         | 18/247 [00:02<00:26,  8.67it/s]predicting train subjects:   8%|▊         | 19/247 [00:02<00:26,  8.66it/s]predicting train subjects:   8%|▊         | 20/247 [00:02<00:26,  8.66it/s]predicting train subjects:   9%|▊         | 21/247 [00:02<00:25,  8.75it/s]predicting train subjects:   9%|▉         | 22/247 [00:02<00:25,  8.80it/s]predicting train subjects:   9%|▉         | 23/247 [00:02<00:25,  8.96it/s]predicting train subjects:  10%|▉         | 24/247 [00:02<00:24,  8.95it/s]predicting train subjects:  10%|█         | 25/247 [00:02<00:24,  9.07it/s]predicting train subjects:  11%|█         | 26/247 [00:02<00:24,  9.14it/s]predicting train subjects:  11%|█         | 27/247 [00:03<00:23,  9.20it/s]predicting train subjects:  11%|█▏        | 28/247 [00:03<00:23,  9.27it/s]predicting train subjects:  12%|█▏        | 29/247 [00:03<00:23,  9.20it/s]predicting train subjects:  12%|█▏        | 30/247 [00:03<00:23,  9.19it/s]predicting train subjects:  13%|█▎        | 31/247 [00:03<00:23,  9.11it/s]predicting train subjects:  13%|█▎        | 32/247 [00:03<00:23,  9.14it/s]predicting train subjects:  13%|█▎        | 33/247 [00:03<00:23,  9.14it/s]predicting train subjects:  14%|█▍        | 34/247 [00:03<00:23,  9.13it/s]predicting train subjects:  14%|█▍        | 35/247 [00:03<00:23,  9.10it/s]predicting train subjects:  15%|█▍        | 36/247 [00:04<00:23,  9.12it/s]predicting train subjects:  15%|█▍        | 37/247 [00:04<00:22,  9.19it/s]predicting train subjects:  15%|█▌        | 38/247 [00:04<00:22,  9.25it/s]predicting train subjects:  16%|█▌        | 39/247 [00:04<00:22,  9.06it/s]predicting train subjects:  16%|█▌        | 40/247 [00:04<00:22,  9.10it/s]predicting train subjects:  17%|█▋        | 41/247 [00:04<00:22,  9.18it/s]predicting train subjects:  17%|█▋        | 42/247 [00:04<00:22,  9.21it/s]predicting train subjects:  17%|█▋        | 43/247 [00:04<00:22,  9.25it/s]predicting train subjects:  18%|█▊        | 44/247 [00:04<00:21,  9.26it/s]predicting train subjects:  18%|█▊        | 45/247 [00:05<00:21,  9.29it/s]predicting train subjects:  19%|█▊        | 46/247 [00:05<00:21,  9.31it/s]predicting train subjects:  19%|█▉        | 47/247 [00:05<00:21,  9.33it/s]predicting train subjects:  19%|█▉        | 48/247 [00:05<00:22,  8.97it/s]predicting train subjects:  20%|█▉        | 49/247 [00:05<00:21,  9.04it/s]predicting train subjects:  20%|██        | 50/247 [00:05<00:22,  8.93it/s]predicting train subjects:  21%|██        | 51/247 [00:05<00:21,  9.03it/s]predicting train subjects:  21%|██        | 52/247 [00:05<00:21,  9.13it/s]predicting train subjects:  21%|██▏       | 53/247 [00:05<00:21,  9.07it/s]predicting train subjects:  22%|██▏       | 54/247 [00:06<00:21,  9.02it/s]predicting train subjects:  22%|██▏       | 55/247 [00:06<00:21,  9.01it/s]predicting train subjects:  23%|██▎       | 56/247 [00:06<00:20,  9.10it/s]predicting train subjects:  23%|██▎       | 57/247 [00:06<00:20,  9.13it/s]predicting train subjects:  23%|██▎       | 58/247 [00:06<00:20,  9.18it/s]predicting train subjects:  24%|██▍       | 59/247 [00:06<00:21,  8.61it/s]predicting train subjects:  24%|██▍       | 60/247 [00:06<00:22,  8.42it/s]predicting train subjects:  25%|██▍       | 61/247 [00:06<00:22,  8.44it/s]predicting train subjects:  25%|██▌       | 62/247 [00:06<00:21,  8.48it/s]predicting train subjects:  26%|██▌       | 63/247 [00:07<00:21,  8.48it/s]predicting train subjects:  26%|██▌       | 64/247 [00:07<00:21,  8.42it/s]predicting train subjects:  26%|██▋       | 65/247 [00:07<00:21,  8.38it/s]predicting train subjects:  27%|██▋       | 66/247 [00:07<00:23,  7.67it/s]predicting train subjects:  27%|██▋       | 67/247 [00:07<00:22,  7.91it/s]predicting train subjects:  28%|██▊       | 68/247 [00:07<00:22,  7.97it/s]predicting train subjects:  28%|██▊       | 69/247 [00:07<00:21,  8.11it/s]predicting train subjects:  28%|██▊       | 70/247 [00:07<00:21,  8.19it/s]predicting train subjects:  29%|██▊       | 71/247 [00:08<00:21,  8.28it/s]predicting train subjects:  29%|██▉       | 72/247 [00:08<00:21,  8.31it/s]predicting train subjects:  30%|██▉       | 73/247 [00:08<00:20,  8.34it/s]predicting train subjects:  30%|██▉       | 74/247 [00:08<00:20,  8.33it/s]predicting train subjects:  30%|███       | 75/247 [00:08<00:20,  8.38it/s]predicting train subjects:  31%|███       | 76/247 [00:08<00:20,  8.45it/s]predicting train subjects:  31%|███       | 77/247 [00:08<00:21,  8.07it/s]predicting train subjects:  32%|███▏      | 78/247 [00:08<00:21,  7.92it/s]predicting train subjects:  32%|███▏      | 79/247 [00:09<00:23,  7.07it/s]predicting train subjects:  32%|███▏      | 80/247 [00:09<00:26,  6.39it/s]predicting train subjects:  33%|███▎      | 81/247 [00:09<00:26,  6.30it/s]predicting train subjects:  33%|███▎      | 82/247 [00:09<00:25,  6.36it/s]predicting train subjects:  34%|███▎      | 83/247 [00:09<00:24,  6.56it/s]predicting train subjects:  34%|███▍      | 84/247 [00:09<00:24,  6.70it/s]predicting train subjects:  34%|███▍      | 85/247 [00:10<00:23,  6.84it/s]predicting train subjects:  35%|███▍      | 86/247 [00:10<00:23,  6.99it/s]predicting train subjects:  35%|███▌      | 87/247 [00:10<00:22,  7.04it/s]predicting train subjects:  36%|███▌      | 88/247 [00:10<00:22,  7.08it/s]predicting train subjects:  36%|███▌      | 89/247 [00:10<00:22,  7.16it/s]predicting train subjects:  36%|███▋      | 90/247 [00:10<00:21,  7.22it/s]predicting train subjects:  37%|███▋      | 91/247 [00:10<00:21,  7.24it/s]predicting train subjects:  37%|███▋      | 92/247 [00:10<00:21,  7.17it/s]predicting train subjects:  38%|███▊      | 93/247 [00:11<00:21,  7.24it/s]predicting train subjects:  38%|███▊      | 94/247 [00:11<00:21,  7.25it/s]predicting train subjects:  38%|███▊      | 95/247 [00:11<00:20,  7.30it/s]predicting train subjects:  39%|███▉      | 96/247 [00:11<00:20,  7.26it/s]predicting train subjects:  39%|███▉      | 97/247 [00:11<00:20,  7.27it/s]predicting train subjects:  40%|███▉      | 98/247 [00:11<00:20,  7.30it/s]predicting train subjects:  40%|████      | 99/247 [00:11<00:20,  7.32it/s]predicting train subjects:  40%|████      | 100/247 [00:12<00:20,  7.35it/s]predicting train subjects:  41%|████      | 101/247 [00:12<00:19,  7.44it/s]predicting train subjects:  41%|████▏     | 102/247 [00:12<00:19,  7.53it/s]predicting train subjects:  42%|████▏     | 103/247 [00:12<00:18,  7.59it/s]predicting train subjects:  42%|████▏     | 104/247 [00:12<00:19,  7.52it/s]predicting train subjects:  43%|████▎     | 105/247 [00:12<00:18,  7.55it/s]predicting train subjects:  43%|████▎     | 106/247 [00:12<00:18,  7.60it/s]predicting train subjects:  43%|████▎     | 107/247 [00:12<00:18,  7.52it/s]predicting train subjects:  44%|████▎     | 108/247 [00:13<00:18,  7.56it/s]predicting train subjects:  44%|████▍     | 109/247 [00:13<00:18,  7.50it/s]predicting train subjects:  45%|████▍     | 110/247 [00:13<00:18,  7.54it/s]predicting train subjects:  45%|████▍     | 111/247 [00:13<00:17,  7.58it/s]predicting train subjects:  45%|████▌     | 112/247 [00:13<00:18,  7.49it/s]predicting train subjects:  46%|████▌     | 113/247 [00:13<00:17,  7.48it/s]predicting train subjects:  46%|████▌     | 114/247 [00:13<00:17,  7.52it/s]predicting train subjects:  47%|████▋     | 115/247 [00:14<00:17,  7.56it/s]predicting train subjects:  47%|████▋     | 116/247 [00:14<00:17,  7.54it/s]predicting train subjects:  47%|████▋     | 117/247 [00:14<00:17,  7.52it/s]predicting train subjects:  48%|████▊     | 118/247 [00:14<00:16,  7.93it/s]predicting train subjects:  48%|████▊     | 119/247 [00:14<00:15,  8.26it/s]predicting train subjects:  49%|████▊     | 120/247 [00:14<00:14,  8.51it/s]predicting train subjects:  49%|████▉     | 121/247 [00:14<00:14,  8.64it/s]predicting train subjects:  49%|████▉     | 122/247 [00:14<00:14,  8.81it/s]predicting train subjects:  50%|████▉     | 123/247 [00:14<00:13,  8.91it/s]predicting train subjects:  50%|█████     | 124/247 [00:15<00:13,  8.99it/s]predicting train subjects:  51%|█████     | 125/247 [00:15<00:13,  8.80it/s]predicting train subjects:  51%|█████     | 126/247 [00:15<00:13,  8.75it/s]predicting train subjects:  51%|█████▏    | 127/247 [00:15<00:13,  8.64it/s]predicting train subjects:  52%|█████▏    | 128/247 [00:15<00:13,  8.71it/s]predicting train subjects:  52%|█████▏    | 129/247 [00:15<00:13,  8.77it/s]predicting train subjects:  53%|█████▎    | 130/247 [00:15<00:13,  8.81it/s]predicting train subjects:  53%|█████▎    | 131/247 [00:15<00:13,  8.67it/s]predicting train subjects:  53%|█████▎    | 132/247 [00:16<00:13,  8.69it/s]predicting train subjects:  54%|█████▍    | 133/247 [00:16<00:13,  8.70it/s]predicting train subjects:  54%|█████▍    | 134/247 [00:16<00:13,  8.66it/s]predicting train subjects:  55%|█████▍    | 135/247 [00:16<00:12,  8.71it/s]predicting train subjects:  55%|█████▌    | 136/247 [00:16<00:12,  8.63it/s]predicting train subjects:  55%|█████▌    | 137/247 [00:16<00:12,  8.67it/s]predicting train subjects:  56%|█████▌    | 138/247 [00:16<00:12,  8.68it/s]predicting train subjects:  56%|█████▋    | 139/247 [00:16<00:12,  8.72it/s]predicting train subjects:  57%|█████▋    | 140/247 [00:16<00:12,  8.83it/s]predicting train subjects:  57%|█████▋    | 141/247 [00:17<00:12,  8.44it/s]predicting train subjects:  57%|█████▋    | 142/247 [00:17<00:12,  8.60it/s]predicting train subjects:  58%|█████▊    | 143/247 [00:17<00:11,  8.77it/s]predicting train subjects:  58%|█████▊    | 144/247 [00:17<00:11,  8.91it/s]predicting train subjects:  59%|█████▊    | 145/247 [00:17<00:11,  8.95it/s]predicting train subjects:  59%|█████▉    | 146/247 [00:17<00:11,  8.96it/s]predicting train subjects:  60%|█████▉    | 147/247 [00:17<00:11,  9.04it/s]predicting train subjects:  60%|█████▉    | 148/247 [00:17<00:10,  9.08it/s]predicting train subjects:  60%|██████    | 149/247 [00:17<00:10,  9.10it/s]predicting train subjects:  61%|██████    | 150/247 [00:18<00:10,  9.14it/s]predicting train subjects:  61%|██████    | 151/247 [00:18<00:10,  9.18it/s]predicting train subjects:  62%|██████▏   | 152/247 [00:18<00:10,  9.18it/s]predicting train subjects:  62%|██████▏   | 153/247 [00:18<00:10,  9.18it/s]predicting train subjects:  62%|██████▏   | 154/247 [00:18<00:10,  8.61it/s]predicting train subjects:  63%|██████▎   | 155/247 [00:18<00:11,  8.26it/s]predicting train subjects:  63%|██████▎   | 156/247 [00:18<00:11,  7.99it/s]predicting train subjects:  64%|██████▎   | 157/247 [00:18<00:11,  7.83it/s]predicting train subjects:  64%|██████▍   | 158/247 [00:19<00:11,  7.71it/s]predicting train subjects:  64%|██████▍   | 159/247 [00:19<00:11,  7.56it/s]predicting train subjects:  65%|██████▍   | 160/247 [00:19<00:11,  7.45it/s]predicting train subjects:  65%|██████▌   | 161/247 [00:19<00:11,  7.49it/s]predicting train subjects:  66%|██████▌   | 162/247 [00:19<00:11,  7.52it/s]predicting train subjects:  66%|██████▌   | 163/247 [00:19<00:11,  7.55it/s]predicting train subjects:  66%|██████▋   | 164/247 [00:19<00:10,  7.55it/s]predicting train subjects:  67%|██████▋   | 165/247 [00:19<00:10,  7.56it/s]predicting train subjects:  67%|██████▋   | 166/247 [00:20<00:10,  7.53it/s]predicting train subjects:  68%|██████▊   | 167/247 [00:20<00:10,  7.52it/s]predicting train subjects:  68%|██████▊   | 168/247 [00:20<00:10,  7.49it/s]predicting train subjects:  68%|██████▊   | 169/247 [00:20<00:10,  7.49it/s]predicting train subjects:  69%|██████▉   | 170/247 [00:20<00:10,  7.51it/s]predicting train subjects:  69%|██████▉   | 171/247 [00:20<00:10,  7.52it/s]predicting train subjects:  70%|██████▉   | 172/247 [00:20<00:09,  7.76it/s]predicting train subjects:  70%|███████   | 173/247 [00:21<00:10,  6.82it/s]predicting train subjects:  70%|███████   | 174/247 [00:21<00:09,  7.31it/s]predicting train subjects:  71%|███████   | 175/247 [00:21<00:10,  6.98it/s]predicting train subjects:  71%|███████▏  | 176/247 [00:21<00:09,  7.45it/s]predicting train subjects:  72%|███████▏  | 177/247 [00:21<00:08,  7.84it/s]predicting train subjects:  72%|███████▏  | 178/247 [00:21<00:08,  8.14it/s]predicting train subjects:  72%|███████▏  | 179/247 [00:21<00:08,  8.35it/s]predicting train subjects:  73%|███████▎  | 180/247 [00:21<00:07,  8.48it/s]predicting train subjects:  73%|███████▎  | 181/247 [00:22<00:07,  8.54it/s]predicting train subjects:  74%|███████▎  | 182/247 [00:22<00:07,  8.67it/s]predicting train subjects:  74%|███████▍  | 183/247 [00:22<00:07,  8.76it/s]predicting train subjects:  74%|███████▍  | 184/247 [00:22<00:07,  8.82it/s]predicting train subjects:  75%|███████▍  | 185/247 [00:22<00:07,  8.74it/s]predicting train subjects:  75%|███████▌  | 186/247 [00:22<00:06,  8.79it/s]predicting train subjects:  76%|███████▌  | 187/247 [00:22<00:06,  8.84it/s]predicting train subjects:  76%|███████▌  | 188/247 [00:22<00:06,  8.88it/s]predicting train subjects:  77%|███████▋  | 189/247 [00:22<00:06,  8.88it/s]predicting train subjects:  77%|███████▋  | 190/247 [00:23<00:06,  8.88it/s]predicting train subjects:  77%|███████▋  | 191/247 [00:23<00:06,  8.91it/s]predicting train subjects:  78%|███████▊  | 192/247 [00:23<00:06,  8.94it/s]predicting train subjects:  78%|███████▊  | 193/247 [00:23<00:06,  8.94it/s]predicting train subjects:  79%|███████▊  | 194/247 [00:23<00:05,  9.09it/s]predicting train subjects:  79%|███████▉  | 195/247 [00:23<00:05,  9.05it/s]predicting train subjects:  79%|███████▉  | 196/247 [00:23<00:05,  9.09it/s]predicting train subjects:  80%|███████▉  | 197/247 [00:23<00:05,  9.24it/s]predicting train subjects:  80%|████████  | 198/247 [00:23<00:05,  9.30it/s]predicting train subjects:  81%|████████  | 199/247 [00:24<00:05,  9.41it/s]predicting train subjects:  81%|████████  | 200/247 [00:24<00:05,  9.31it/s]predicting train subjects:  81%|████████▏ | 201/247 [00:24<00:04,  9.26it/s]predicting train subjects:  82%|████████▏ | 202/247 [00:24<00:04,  9.36it/s]predicting train subjects:  82%|████████▏ | 203/247 [00:24<00:04,  9.39it/s]predicting train subjects:  83%|████████▎ | 204/247 [00:24<00:04,  9.37it/s]predicting train subjects:  83%|████████▎ | 205/247 [00:24<00:04,  9.34it/s]predicting train subjects:  83%|████████▎ | 206/247 [00:24<00:04,  9.38it/s]predicting train subjects:  84%|████████▍ | 207/247 [00:24<00:04,  9.36it/s]predicting train subjects:  84%|████████▍ | 208/247 [00:24<00:04,  9.39it/s]predicting train subjects:  85%|████████▍ | 209/247 [00:25<00:04,  9.41it/s]predicting train subjects:  85%|████████▌ | 210/247 [00:25<00:04,  9.10it/s]predicting train subjects:  85%|████████▌ | 211/247 [00:25<00:03,  9.05it/s]predicting train subjects:  86%|████████▌ | 212/247 [00:25<00:03,  9.04it/s]predicting train subjects:  86%|████████▌ | 213/247 [00:25<00:03,  8.99it/s]predicting train subjects:  87%|████████▋ | 214/247 [00:25<00:03,  8.93it/s]predicting train subjects:  87%|████████▋ | 215/247 [00:25<00:03,  8.87it/s]predicting train subjects:  87%|████████▋ | 216/247 [00:25<00:03,  8.88it/s]predicting train subjects:  88%|████████▊ | 217/247 [00:25<00:03,  8.88it/s]predicting train subjects:  88%|████████▊ | 218/247 [00:26<00:03,  8.97it/s]predicting train subjects:  89%|████████▊ | 219/247 [00:26<00:03,  9.08it/s]predicting train subjects:  89%|████████▉ | 220/247 [00:26<00:02,  9.16it/s]predicting train subjects:  89%|████████▉ | 221/247 [00:26<00:02,  9.21it/s]predicting train subjects:  90%|████████▉ | 222/247 [00:26<00:02,  9.25it/s]predicting train subjects:  90%|█████████ | 223/247 [00:26<00:02,  9.22it/s]predicting train subjects:  91%|█████████ | 224/247 [00:26<00:02,  9.20it/s]predicting train subjects:  91%|█████████ | 225/247 [00:26<00:02,  9.16it/s]predicting train subjects:  91%|█████████▏| 226/247 [00:26<00:02,  9.03it/s]predicting train subjects:  92%|█████████▏| 227/247 [00:27<00:02,  9.03it/s]predicting train subjects:  92%|█████████▏| 228/247 [00:27<00:02,  9.06it/s]predicting train subjects:  93%|█████████▎| 229/247 [00:27<00:01,  9.10it/s]predicting train subjects:  93%|█████████▎| 230/247 [00:27<00:01,  8.55it/s]predicting train subjects:  94%|█████████▎| 231/247 [00:27<00:01,  8.27it/s]predicting train subjects:  94%|█████████▍| 232/247 [00:27<00:01,  8.11it/s]predicting train subjects:  94%|█████████▍| 233/247 [00:27<00:01,  8.00it/s]predicting train subjects:  95%|█████████▍| 234/247 [00:27<00:01,  7.91it/s]predicting train subjects:  95%|█████████▌| 235/247 [00:28<00:01,  7.87it/s]predicting train subjects:  96%|█████████▌| 236/247 [00:28<00:01,  7.84it/s]predicting train subjects:  96%|█████████▌| 237/247 [00:28<00:01,  7.79it/s]predicting train subjects:  96%|█████████▋| 238/247 [00:28<00:01,  7.77it/s]predicting train subjects:  97%|█████████▋| 239/247 [00:28<00:01,  7.72it/s]predicting train subjects:  97%|█████████▋| 240/247 [00:28<00:00,  7.75it/s]predicting train subjects:  98%|█████████▊| 241/247 [00:28<00:00,  7.78it/s]predicting train subjects:  98%|█████████▊| 242/247 [00:28<00:00,  7.75it/s]predicting train subjects:  98%|█████████▊| 243/247 [00:29<00:00,  7.61it/s]predicting train subjects:  99%|█████████▉| 244/247 [00:29<00:00,  7.61it/s]predicting train subjects:  99%|█████████▉| 245/247 [00:29<00:00,  7.65it/s]predicting train subjects: 100%|█████████▉| 246/247 [00:29<00:00,  7.69it/s]predicting train subjects: 100%|██████████| 247/247 [00:29<00:00,  7.71it/s]predicting train subjects: 100%|██████████| 247/247 [00:29<00:00,  8.33it/s]
predicting test subjects sagittal:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects sagittal:  20%|██        | 1/5 [00:00<00:00,  8.60it/s]predicting test subjects sagittal:  40%|████      | 2/5 [00:00<00:00,  8.21it/s]predicting test subjects sagittal:  60%|██████    | 3/5 [00:00<00:00,  8.10it/s]predicting test subjects sagittal:  80%|████████  | 4/5 [00:00<00:00,  8.58it/s]predicting test subjects sagittal: 100%|██████████| 5/5 [00:00<00:00,  8.63it/s]predicting test subjects sagittal: 100%|██████████| 5/5 [00:00<00:00,  8.42it/s]
predicting train subjects sagittal:   0%|          | 0/247 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/247 [00:00<00:30,  8.12it/s]predicting train subjects sagittal:   1%|          | 2/247 [00:00<00:29,  8.27it/s]predicting train subjects sagittal:   1%|          | 3/247 [00:00<00:28,  8.55it/s]predicting train subjects sagittal:   2%|▏         | 4/247 [00:00<00:28,  8.52it/s]predicting train subjects sagittal:   2%|▏         | 5/247 [00:00<00:28,  8.46it/s]predicting train subjects sagittal:   2%|▏         | 6/247 [00:00<00:28,  8.48it/s]predicting train subjects sagittal:   3%|▎         | 7/247 [00:00<00:28,  8.55it/s]predicting train subjects sagittal:   3%|▎         | 8/247 [00:00<00:27,  8.57it/s]predicting train subjects sagittal:   4%|▎         | 9/247 [00:01<00:27,  8.56it/s]predicting train subjects sagittal:   4%|▍         | 10/247 [00:01<00:27,  8.57it/s]predicting train subjects sagittal:   4%|▍         | 11/247 [00:01<00:27,  8.61it/s]predicting train subjects sagittal:   5%|▍         | 12/247 [00:01<00:27,  8.61it/s]predicting train subjects sagittal:   5%|▌         | 13/247 [00:01<00:27,  8.58it/s]predicting train subjects sagittal:   6%|▌         | 14/247 [00:01<00:27,  8.56it/s]predicting train subjects sagittal:   6%|▌         | 15/247 [00:01<00:27,  8.49it/s]predicting train subjects sagittal:   6%|▋         | 16/247 [00:01<00:27,  8.46it/s]predicting train subjects sagittal:   7%|▋         | 17/247 [00:01<00:27,  8.47it/s]predicting train subjects sagittal:   7%|▋         | 18/247 [00:02<00:26,  8.50it/s]predicting train subjects sagittal:   8%|▊         | 19/247 [00:02<00:26,  8.46it/s]predicting train subjects sagittal:   8%|▊         | 20/247 [00:02<00:26,  8.47it/s]predicting train subjects sagittal:   9%|▊         | 21/247 [00:02<00:26,  8.52it/s]predicting train subjects sagittal:   9%|▉         | 22/247 [00:02<00:26,  8.56it/s]predicting train subjects sagittal:   9%|▉         | 23/247 [00:02<00:25,  8.74it/s]predicting train subjects sagittal:  10%|▉         | 24/247 [00:02<00:25,  8.87it/s]predicting train subjects sagittal:  10%|█         | 25/247 [00:02<00:24,  8.98it/s]predicting train subjects sagittal:  11%|█         | 26/247 [00:03<00:24,  9.08it/s]predicting train subjects sagittal:  11%|█         | 27/247 [00:03<00:24,  9.12it/s]predicting train subjects sagittal:  11%|█▏        | 28/247 [00:03<00:23,  9.16it/s]predicting train subjects sagittal:  12%|█▏        | 29/247 [00:03<00:23,  9.17it/s]predicting train subjects sagittal:  12%|█▏        | 30/247 [00:03<00:23,  9.10it/s]predicting train subjects sagittal:  13%|█▎        | 31/247 [00:03<00:23,  9.14it/s]predicting train subjects sagittal:  13%|█▎        | 32/247 [00:03<00:23,  9.19it/s]predicting train subjects sagittal:  13%|█▎        | 33/247 [00:03<00:23,  9.23it/s]predicting train subjects sagittal:  14%|█▍        | 34/247 [00:03<00:23,  9.13it/s]predicting train subjects sagittal:  14%|█▍        | 35/247 [00:03<00:23,  9.12it/s]predicting train subjects sagittal:  15%|█▍        | 36/247 [00:04<00:23,  9.14it/s]predicting train subjects sagittal:  15%|█▍        | 37/247 [00:04<00:22,  9.18it/s]predicting train subjects sagittal:  15%|█▌        | 38/247 [00:04<00:22,  9.20it/s]predicting train subjects sagittal:  16%|█▌        | 39/247 [00:04<00:22,  9.23it/s]predicting train subjects sagittal:  16%|█▌        | 40/247 [00:04<00:22,  9.28it/s]predicting train subjects sagittal:  17%|█▋        | 41/247 [00:04<00:22,  9.29it/s]predicting train subjects sagittal:  17%|█▋        | 42/247 [00:04<00:22,  9.28it/s]predicting train subjects sagittal:  17%|█▋        | 43/247 [00:04<00:21,  9.28it/s]predicting train subjects sagittal:  18%|█▊        | 44/247 [00:04<00:21,  9.24it/s]predicting train subjects sagittal:  18%|█▊        | 45/247 [00:05<00:22,  9.17it/s]predicting train subjects sagittal:  19%|█▊        | 46/247 [00:05<00:21,  9.14it/s]predicting train subjects sagittal:  19%|█▉        | 47/247 [00:05<00:21,  9.23it/s]predicting train subjects sagittal:  19%|█▉        | 48/247 [00:05<00:21,  9.29it/s]predicting train subjects sagittal:  20%|█▉        | 49/247 [00:05<00:22,  8.90it/s]predicting train subjects sagittal:  20%|██        | 50/247 [00:05<00:21,  8.96it/s]predicting train subjects sagittal:  21%|██        | 51/247 [00:05<00:21,  9.00it/s]predicting train subjects sagittal:  21%|██        | 52/247 [00:05<00:21,  9.03it/s]predicting train subjects sagittal:  21%|██▏       | 53/247 [00:05<00:21,  9.07it/s]predicting train subjects sagittal:  22%|██▏       | 54/247 [00:06<00:21,  9.11it/s]predicting train subjects sagittal:  22%|██▏       | 55/247 [00:06<00:21,  9.12it/s]predicting train subjects sagittal:  23%|██▎       | 56/247 [00:06<00:21,  9.01it/s]predicting train subjects sagittal:  23%|██▎       | 57/247 [00:06<00:20,  9.05it/s]predicting train subjects sagittal:  23%|██▎       | 58/247 [00:06<00:27,  6.95it/s]predicting train subjects sagittal:  24%|██▍       | 59/247 [00:06<00:26,  7.17it/s]predicting train subjects sagittal:  24%|██▍       | 60/247 [00:06<00:25,  7.36it/s]predicting train subjects sagittal:  25%|██▍       | 61/247 [00:06<00:24,  7.57it/s]predicting train subjects sagittal:  25%|██▌       | 62/247 [00:07<00:24,  7.61it/s]predicting train subjects sagittal:  26%|██▌       | 63/247 [00:07<00:23,  7.81it/s]predicting train subjects sagittal:  26%|██▌       | 64/247 [00:07<00:22,  8.04it/s]predicting train subjects sagittal:  26%|██▋       | 65/247 [00:07<00:22,  8.23it/s]predicting train subjects sagittal:  27%|██▋       | 66/247 [00:07<00:21,  8.36it/s]predicting train subjects sagittal:  27%|██▋       | 67/247 [00:07<00:21,  8.43it/s]predicting train subjects sagittal:  28%|██▊       | 68/247 [00:07<00:21,  8.48it/s]predicting train subjects sagittal:  28%|██▊       | 69/247 [00:07<00:20,  8.53it/s]predicting train subjects sagittal:  28%|██▊       | 70/247 [00:08<00:20,  8.58it/s]predicting train subjects sagittal:  29%|██▊       | 71/247 [00:08<00:20,  8.61it/s]predicting train subjects sagittal:  29%|██▉       | 72/247 [00:08<00:20,  8.55it/s]predicting train subjects sagittal:  30%|██▉       | 73/247 [00:08<00:20,  8.51it/s]predicting train subjects sagittal:  30%|██▉       | 74/247 [00:08<00:20,  8.51it/s]predicting train subjects sagittal:  30%|███       | 75/247 [00:08<00:20,  8.54it/s]predicting train subjects sagittal:  31%|███       | 76/247 [00:08<00:20,  8.50it/s]predicting train subjects sagittal:  31%|███       | 77/247 [00:08<00:21,  8.06it/s]predicting train subjects sagittal:  32%|███▏      | 78/247 [00:09<00:21,  7.90it/s]predicting train subjects sagittal:  32%|███▏      | 79/247 [00:09<00:20,  8.22it/s]predicting train subjects sagittal:  32%|███▏      | 80/247 [00:09<00:19,  8.48it/s]predicting train subjects sagittal:  33%|███▎      | 81/247 [00:09<00:20,  8.16it/s]predicting train subjects sagittal:  33%|███▎      | 82/247 [00:09<00:21,  7.85it/s]predicting train subjects sagittal:  34%|███▎      | 83/247 [00:09<00:21,  7.67it/s]predicting train subjects sagittal:  34%|███▍      | 84/247 [00:09<00:21,  7.56it/s]predicting train subjects sagittal:  34%|███▍      | 85/247 [00:09<00:21,  7.48it/s]predicting train subjects sagittal:  35%|███▍      | 86/247 [00:10<00:21,  7.40it/s]predicting train subjects sagittal:  35%|███▌      | 87/247 [00:10<00:21,  7.32it/s]predicting train subjects sagittal:  36%|███▌      | 88/247 [00:10<00:21,  7.35it/s]predicting train subjects sagittal:  36%|███▌      | 89/247 [00:10<00:21,  7.21it/s]predicting train subjects sagittal:  36%|███▋      | 90/247 [00:10<00:21,  7.26it/s]predicting train subjects sagittal:  37%|███▋      | 91/247 [00:10<00:21,  7.30it/s]predicting train subjects sagittal:  37%|███▋      | 92/247 [00:10<00:21,  7.33it/s]predicting train subjects sagittal:  38%|███▊      | 93/247 [00:11<00:21,  7.25it/s]predicting train subjects sagittal:  38%|███▊      | 94/247 [00:11<00:21,  7.27it/s]predicting train subjects sagittal:  38%|███▊      | 95/247 [00:11<00:20,  7.32it/s]predicting train subjects sagittal:  39%|███▉      | 96/247 [00:11<00:20,  7.25it/s]predicting train subjects sagittal:  39%|███▉      | 97/247 [00:11<00:20,  7.25it/s]predicting train subjects sagittal:  40%|███▉      | 98/247 [00:11<00:20,  7.28it/s]predicting train subjects sagittal:  40%|████      | 99/247 [00:11<00:20,  7.32it/s]predicting train subjects sagittal:  40%|████      | 100/247 [00:11<00:19,  7.39it/s]predicting train subjects sagittal:  41%|████      | 101/247 [00:12<00:19,  7.39it/s]predicting train subjects sagittal:  41%|████▏     | 102/247 [00:12<00:19,  7.43it/s]predicting train subjects sagittal:  42%|████▏     | 103/247 [00:12<00:19,  7.45it/s]predicting train subjects sagittal:  42%|████▏     | 104/247 [00:12<00:19,  7.50it/s]predicting train subjects sagittal:  43%|████▎     | 105/247 [00:12<00:18,  7.56it/s]predicting train subjects sagittal:  43%|████▎     | 106/247 [00:12<00:18,  7.59it/s]predicting train subjects sagittal:  43%|████▎     | 107/247 [00:12<00:18,  7.58it/s]predicting train subjects sagittal:  44%|████▎     | 108/247 [00:13<00:18,  7.61it/s]predicting train subjects sagittal:  44%|████▍     | 109/247 [00:13<00:18,  7.60it/s]predicting train subjects sagittal:  45%|████▍     | 110/247 [00:13<00:18,  7.56it/s]predicting train subjects sagittal:  45%|████▍     | 111/247 [00:13<00:17,  7.60it/s]predicting train subjects sagittal:  45%|████▌     | 112/247 [00:13<00:17,  7.66it/s]predicting train subjects sagittal:  46%|████▌     | 113/247 [00:13<00:17,  7.61it/s]predicting train subjects sagittal:  46%|████▌     | 114/247 [00:13<00:17,  7.61it/s]predicting train subjects sagittal:  47%|████▋     | 115/247 [00:13<00:17,  7.64it/s]predicting train subjects sagittal:  47%|████▋     | 116/247 [00:14<00:17,  7.67it/s]predicting train subjects sagittal:  47%|████▋     | 117/247 [00:14<00:16,  7.68it/s]predicting train subjects sagittal:  48%|████▊     | 118/247 [00:14<00:15,  8.11it/s]predicting train subjects sagittal:  48%|████▊     | 119/247 [00:14<00:15,  8.46it/s]predicting train subjects sagittal:  49%|████▊     | 120/247 [00:14<00:14,  8.72it/s]predicting train subjects sagittal:  49%|████▉     | 121/247 [00:14<00:14,  8.90it/s]predicting train subjects sagittal:  49%|████▉     | 122/247 [00:14<00:13,  9.02it/s]predicting train subjects sagittal:  50%|████▉     | 123/247 [00:14<00:13,  9.06it/s]predicting train subjects sagittal:  50%|█████     | 124/247 [00:14<00:13,  9.07it/s]predicting train subjects sagittal:  51%|█████     | 125/247 [00:15<00:13,  9.13it/s]predicting train subjects sagittal:  51%|█████     | 126/247 [00:15<00:13,  9.21it/s]predicting train subjects sagittal:  51%|█████▏    | 127/247 [00:15<00:13,  9.20it/s]predicting train subjects sagittal:  52%|█████▏    | 128/247 [00:15<00:13,  9.15it/s]predicting train subjects sagittal:  52%|█████▏    | 129/247 [00:15<00:12,  9.19it/s]predicting train subjects sagittal:  53%|█████▎    | 130/247 [00:15<00:12,  9.20it/s]predicting train subjects sagittal:  53%|█████▎    | 131/247 [00:15<00:12,  9.24it/s]predicting train subjects sagittal:  53%|█████▎    | 132/247 [00:15<00:12,  9.28it/s]predicting train subjects sagittal:  54%|█████▍    | 133/247 [00:15<00:12,  9.29it/s]predicting train subjects sagittal:  54%|█████▍    | 134/247 [00:16<00:12,  9.28it/s]predicting train subjects sagittal:  55%|█████▍    | 135/247 [00:16<00:12,  9.29it/s]predicting train subjects sagittal:  55%|█████▌    | 136/247 [00:16<00:12,  9.23it/s]predicting train subjects sagittal:  55%|█████▌    | 137/247 [00:16<00:11,  9.18it/s]predicting train subjects sagittal:  56%|█████▌    | 138/247 [00:16<00:11,  9.18it/s]predicting train subjects sagittal:  56%|█████▋    | 139/247 [00:16<00:11,  9.24it/s]predicting train subjects sagittal:  57%|█████▋    | 140/247 [00:16<00:11,  9.28it/s]predicting train subjects sagittal:  57%|█████▋    | 141/247 [00:16<00:11,  9.21it/s]predicting train subjects sagittal:  57%|█████▋    | 142/247 [00:16<00:11,  9.14it/s]predicting train subjects sagittal:  58%|█████▊    | 143/247 [00:17<00:11,  9.07it/s]predicting train subjects sagittal:  58%|█████▊    | 144/247 [00:17<00:11,  9.12it/s]predicting train subjects sagittal:  59%|█████▊    | 145/247 [00:17<00:11,  9.17it/s]predicting train subjects sagittal:  59%|█████▉    | 146/247 [00:17<00:10,  9.21it/s]predicting train subjects sagittal:  60%|█████▉    | 147/247 [00:17<00:10,  9.24it/s]predicting train subjects sagittal:  60%|█████▉    | 148/247 [00:17<00:10,  9.25it/s]predicting train subjects sagittal:  60%|██████    | 149/247 [00:17<00:10,  9.25it/s]predicting train subjects sagittal:  61%|██████    | 150/247 [00:17<00:10,  9.27it/s]predicting train subjects sagittal:  61%|██████    | 151/247 [00:17<00:10,  9.26it/s]predicting train subjects sagittal:  62%|██████▏   | 152/247 [00:18<00:10,  9.20it/s]predicting train subjects sagittal:  62%|██████▏   | 153/247 [00:18<00:10,  9.11it/s]predicting train subjects sagittal:  62%|██████▏   | 154/247 [00:18<00:10,  8.56it/s]predicting train subjects sagittal:  63%|██████▎   | 155/247 [00:18<00:11,  8.25it/s]predicting train subjects sagittal:  63%|██████▎   | 156/247 [00:18<00:11,  7.96it/s]predicting train subjects sagittal:  64%|██████▎   | 157/247 [00:18<00:11,  7.86it/s]predicting train subjects sagittal:  64%|██████▍   | 158/247 [00:18<00:11,  7.82it/s]predicting train subjects sagittal:  64%|██████▍   | 159/247 [00:18<00:11,  7.80it/s]predicting train subjects sagittal:  65%|██████▍   | 160/247 [00:19<00:11,  7.69it/s]predicting train subjects sagittal:  65%|██████▌   | 161/247 [00:19<00:11,  7.72it/s]predicting train subjects sagittal:  66%|██████▌   | 162/247 [00:19<00:10,  7.74it/s]predicting train subjects sagittal:  66%|██████▌   | 163/247 [00:19<00:10,  7.73it/s]predicting train subjects sagittal:  66%|██████▋   | 164/247 [00:19<00:10,  7.69it/s]predicting train subjects sagittal:  67%|██████▋   | 165/247 [00:19<00:10,  7.63it/s]predicting train subjects sagittal:  67%|██████▋   | 166/247 [00:19<00:10,  7.59it/s]predicting train subjects sagittal:  68%|██████▊   | 167/247 [00:19<00:10,  7.62it/s]predicting train subjects sagittal:  68%|██████▊   | 168/247 [00:20<00:10,  7.61it/s]predicting train subjects sagittal:  68%|██████▊   | 169/247 [00:20<00:10,  7.54it/s]predicting train subjects sagittal:  69%|██████▉   | 170/247 [00:20<00:10,  7.52it/s]predicting train subjects sagittal:  69%|██████▉   | 171/247 [00:20<00:10,  7.54it/s]predicting train subjects sagittal:  70%|██████▉   | 172/247 [00:20<00:09,  7.79it/s]predicting train subjects sagittal:  70%|███████   | 173/247 [00:20<00:08,  8.24it/s]predicting train subjects sagittal:  70%|███████   | 174/247 [00:20<00:08,  8.46it/s]predicting train subjects sagittal:  71%|███████   | 175/247 [00:20<00:08,  8.27it/s]predicting train subjects sagittal:  71%|███████▏  | 176/247 [00:21<00:08,  8.31it/s]predicting train subjects sagittal:  72%|███████▏  | 177/247 [00:21<00:08,  8.50it/s]predicting train subjects sagittal:  72%|███████▏  | 178/247 [00:21<00:07,  8.64it/s]predicting train subjects sagittal:  72%|███████▏  | 179/247 [00:21<00:07,  8.66it/s]predicting train subjects sagittal:  73%|███████▎  | 180/247 [00:21<00:07,  8.66it/s]predicting train subjects sagittal:  73%|███████▎  | 181/247 [00:21<00:07,  8.60it/s]predicting train subjects sagittal:  74%|███████▎  | 182/247 [00:21<00:07,  8.66it/s]predicting train subjects sagittal:  74%|███████▍  | 183/247 [00:21<00:07,  8.64it/s]predicting train subjects sagittal:  74%|███████▍  | 184/247 [00:22<00:07,  8.45it/s]predicting train subjects sagittal:  75%|███████▍  | 185/247 [00:22<00:07,  8.59it/s]predicting train subjects sagittal:  75%|███████▌  | 186/247 [00:22<00:07,  8.69it/s]predicting train subjects sagittal:  76%|███████▌  | 187/247 [00:22<00:06,  8.79it/s]predicting train subjects sagittal:  76%|███████▌  | 188/247 [00:22<00:06,  8.80it/s]predicting train subjects sagittal:  77%|███████▋  | 189/247 [00:22<00:06,  8.73it/s]predicting train subjects sagittal:  77%|███████▋  | 190/247 [00:22<00:06,  8.70it/s]predicting train subjects sagittal:  77%|███████▋  | 191/247 [00:22<00:06,  8.73it/s]predicting train subjects sagittal:  78%|███████▊  | 192/247 [00:22<00:06,  8.76it/s]predicting train subjects sagittal:  78%|███████▊  | 193/247 [00:23<00:06,  8.68it/s]predicting train subjects sagittal:  79%|███████▊  | 194/247 [00:23<00:06,  8.83it/s]predicting train subjects sagittal:  79%|███████▉  | 195/247 [00:23<00:05,  8.99it/s]predicting train subjects sagittal:  79%|███████▉  | 196/247 [00:23<00:05,  9.13it/s]predicting train subjects sagittal:  80%|███████▉  | 197/247 [00:23<00:05,  9.26it/s]predicting train subjects sagittal:  80%|████████  | 198/247 [00:23<00:05,  9.23it/s]predicting train subjects sagittal:  81%|████████  | 199/247 [00:23<00:05,  9.22it/s]predicting train subjects sagittal:  81%|████████  | 200/247 [00:23<00:05,  9.21it/s]predicting train subjects sagittal:  81%|████████▏ | 201/247 [00:23<00:04,  9.24it/s]predicting train subjects sagittal:  82%|████████▏ | 202/247 [00:24<00:04,  9.15it/s]predicting train subjects sagittal:  82%|████████▏ | 203/247 [00:24<00:04,  9.16it/s]predicting train subjects sagittal:  83%|████████▎ | 204/247 [00:24<00:04,  9.23it/s]predicting train subjects sagittal:  83%|████████▎ | 205/247 [00:24<00:04,  9.26it/s]predicting train subjects sagittal:  83%|████████▎ | 206/247 [00:24<00:04,  9.31it/s]predicting train subjects sagittal:  84%|████████▍ | 207/247 [00:24<00:04,  9.33it/s]predicting train subjects sagittal:  84%|████████▍ | 208/247 [00:24<00:04,  9.29it/s]predicting train subjects sagittal:  85%|████████▍ | 209/247 [00:24<00:04,  9.28it/s]predicting train subjects sagittal:  85%|████████▌ | 210/247 [00:24<00:03,  9.28it/s]predicting train subjects sagittal:  85%|████████▌ | 211/247 [00:24<00:03,  9.36it/s]predicting train subjects sagittal:  86%|████████▌ | 212/247 [00:25<00:03,  9.28it/s]predicting train subjects sagittal:  86%|████████▌ | 213/247 [00:25<00:03,  9.19it/s]predicting train subjects sagittal:  87%|████████▋ | 214/247 [00:25<00:03,  9.10it/s]predicting train subjects sagittal:  87%|████████▋ | 215/247 [00:25<00:03,  9.12it/s]predicting train subjects sagittal:  87%|████████▋ | 216/247 [00:25<00:03,  9.15it/s]predicting train subjects sagittal:  88%|████████▊ | 217/247 [00:25<00:03,  9.17it/s]predicting train subjects sagittal:  88%|████████▊ | 218/247 [00:25<00:03,  9.12it/s]predicting train subjects sagittal:  89%|████████▊ | 219/247 [00:25<00:03,  9.12it/s]predicting train subjects sagittal:  89%|████████▉ | 220/247 [00:25<00:02,  9.15it/s]predicting train subjects sagittal:  89%|████████▉ | 221/247 [00:26<00:02,  9.16it/s]predicting train subjects sagittal:  90%|████████▉ | 222/247 [00:26<00:02,  9.18it/s]predicting train subjects sagittal:  90%|█████████ | 223/247 [00:26<00:02,  9.19it/s]predicting train subjects sagittal:  91%|█████████ | 224/247 [00:26<00:02,  9.16it/s]predicting train subjects sagittal:  91%|█████████ | 225/247 [00:26<00:02,  9.15it/s]predicting train subjects sagittal:  91%|█████████▏| 226/247 [00:26<00:02,  9.15it/s]predicting train subjects sagittal:  92%|█████████▏| 227/247 [00:26<00:02,  9.08it/s]predicting train subjects sagittal:  92%|█████████▏| 228/247 [00:26<00:02,  9.09it/s]predicting train subjects sagittal:  93%|█████████▎| 229/247 [00:26<00:01,  9.03it/s]predicting train subjects sagittal:  93%|█████████▎| 230/247 [00:27<00:01,  8.58it/s]predicting train subjects sagittal:  94%|█████████▎| 231/247 [00:27<00:01,  8.36it/s]predicting train subjects sagittal:  94%|█████████▍| 232/247 [00:27<00:01,  8.22it/s]predicting train subjects sagittal:  94%|█████████▍| 233/247 [00:27<00:01,  8.12it/s]predicting train subjects sagittal:  95%|█████████▍| 234/247 [00:27<00:01,  8.06it/s]predicting train subjects sagittal:  95%|█████████▌| 235/247 [00:27<00:01,  8.01it/s]predicting train subjects sagittal:  96%|█████████▌| 236/247 [00:27<00:01,  7.89it/s]predicting train subjects sagittal:  96%|█████████▌| 237/247 [00:27<00:01,  7.85it/s]predicting train subjects sagittal:  96%|█████████▋| 238/247 [00:28<00:01,  7.80it/s]predicting train subjects sagittal:  97%|█████████▋| 239/247 [00:28<00:01,  7.64it/s]predicting train subjects sagittal:  97%|█████████▋| 240/247 [00:28<00:00,  7.70it/s]predicting train subjects sagittal:  98%|█████████▊| 241/247 [00:28<00:00,  7.72it/s]predicting train subjects sagittal:  98%|█████████▊| 242/247 [00:28<00:00,  7.65it/s]predicting train subjects sagittal:  98%|█████████▊| 243/247 [00:28<00:00,  7.64it/s]predicting train subjects sagittal:  99%|█████████▉| 244/247 [00:28<00:00,  7.67it/s]predicting train subjects sagittal:  99%|█████████▉| 245/247 [00:29<00:00,  7.72it/s]predicting train subjects sagittal: 100%|█████████▉| 246/247 [00:29<00:00,  7.75it/s]predicting train subjects sagittal: 100%|██████████| 247/247 [00:29<00:00,  7.77it/s]predicting train subjects sagittal: 100%|██████████| 247/247 [00:29<00:00,  8.44it/s]
saving BB  test1-THALAMUS:   0%|          | 0/8 [00:00<?, ?it/s]saving BB  test1-THALAMUS:   0%|          | 0/8 [00:00<?, ?it/s]

Traceback (most recent call last):
  File "main.py", line 1906, in <module>
    
  File "main.py", line 1899, in Run_Csfn_with_Best_WMn_architecture
    #     # crossVal = '_CV_' + UserInfoB['CrossVal'].index[0] if UserInfoB['CrossVal'].Mode else ''
  File "main.py", line 230, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 104, in Loop_Over_Nuclei
    Run_Main(UserI)
  File "main.py", line 224, in Run_Main
    Loop_slicing_orientations(UserInfoB, InitValues)
  File "main.py", line 222, in Loop_slicing_orientations
    subRun(UserInfoB)
  File "main.py", line 216, in subRun
    else: normal_run(params)
  File "main.py", line 203, in normal_run
    choosingModel.check_Run(params, Data)              
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 57, in check_Run
    save_BoundingBox_Hierarchy(params, prediction)
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 535, in save_BoundingBox_Hierarchy
    loop_Subjects(PRED.Test, 'test')
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 527, in loop_Subjects
    save_BoundingBox(PRED[sj] , Subjects[sj] , mode , params.directories.Test.Result)
KeyError: 'vimp2_964_08092013_TG'
2020-01-22 16:08:52.188105: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-22 16:08:56.210165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-22 16:08:56.210234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 16:08:56.625787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 16:08:56.625855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 16:08:56.625868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 16:08:56.626296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
Loading train:   0%|          | 0/26 [00:00<?, ?it/s]Loading train:   4%|▍         | 1/26 [00:00<00:09,  2.57it/s]Loading train:   8%|▊         | 2/26 [00:00<00:08,  2.81it/s]Loading train:  12%|█▏        | 3/26 [00:00<00:07,  3.13it/s]Loading train:  15%|█▌        | 4/26 [00:01<00:06,  3.28it/s]Loading train:  19%|█▉        | 5/26 [00:01<00:05,  3.50it/s]Loading train:  23%|██▎       | 6/26 [00:01<00:05,  3.67it/s]Loading train:  27%|██▋       | 7/26 [00:01<00:05,  3.77it/s]Loading train:  31%|███       | 8/26 [00:02<00:04,  3.68it/s]Loading train:  35%|███▍      | 9/26 [00:02<00:04,  3.82it/s]Loading train:  38%|███▊      | 10/26 [00:02<00:04,  3.72it/s]Loading train:  42%|████▏     | 11/26 [00:02<00:03,  3.90it/s]Loading train:  46%|████▌     | 12/26 [00:03<00:03,  4.04it/s]Loading train:  50%|█████     | 13/26 [00:03<00:03,  3.86it/s]Loading train:  54%|█████▍    | 14/26 [00:03<00:03,  3.92it/s]Loading train:  58%|█████▊    | 15/26 [00:03<00:02,  3.96it/s]Loading train:  62%|██████▏   | 16/26 [00:04<00:02,  4.09it/s]Loading train:  65%|██████▌   | 17/26 [00:04<00:02,  4.13it/s]Loading train:  69%|██████▉   | 18/26 [00:04<00:01,  4.26it/s]Loading train:  73%|███████▎  | 19/26 [00:04<00:01,  4.19it/s]Loading train:  77%|███████▋  | 20/26 [00:05<00:01,  4.36it/s]Loading train:  81%|████████  | 21/26 [00:05<00:01,  4.05it/s]Loading train:  85%|████████▍ | 22/26 [00:05<00:00,  4.05it/s]Loading train:  88%|████████▊ | 23/26 [00:05<00:00,  4.13it/s]Loading train:  92%|█████████▏| 24/26 [00:06<00:00,  4.00it/s]Loading train:  96%|█████████▌| 25/26 [00:06<00:00,  4.01it/s]Loading train: 100%|██████████| 26/26 [00:06<00:00,  4.08it/s]Loading train: 100%|██████████| 26/26 [00:06<00:00,  3.94it/s]
concatenating: train:   0%|          | 0/26 [00:00<?, ?it/s]concatenating: train:  19%|█▉        | 5/26 [00:00<00:00, 44.81it/s]concatenating: train:  42%|████▏     | 11/26 [00:00<00:00, 48.08it/s]concatenating: train:  65%|██████▌   | 17/26 [00:00<00:00, 49.65it/s]concatenating: train:  88%|████████▊ | 23/26 [00:00<00:00, 51.85it/s]concatenating: train: 100%|██████████| 26/26 [00:00<00:00, 54.14it/s]
Loading test:   0%|          | 0/8 [00:00<?, ?it/s]Loading test:  12%|█▎        | 1/8 [00:00<00:01,  3.70it/s]Loading test:  25%|██▌       | 2/8 [00:00<00:01,  3.83it/s]Loading test:  38%|███▊      | 3/8 [00:00<00:01,  3.96it/s]Loading test:  50%|█████     | 4/8 [00:00<00:00,  4.14it/s]Loading test:  62%|██████▎   | 5/8 [00:01<00:00,  4.16it/s]Loading test:  75%|███████▌  | 6/8 [00:01<00:00,  4.14it/s]Loading test:  88%|████████▊ | 7/8 [00:01<00:00,  4.30it/s]Loading test: 100%|██████████| 8/8 [00:01<00:00,  4.24it/s]Loading test: 100%|██████████| 8/8 [00:01<00:00,  4.22it/s]
concatenating: validation:   0%|          | 0/8 [00:00<?, ?it/s]concatenating: validation:  88%|████████▊ | 7/8 [00:00<00:00, 66.36it/s]concatenating: validation: 100%|██████████| 8/8 [00:00<00:00, 66.15it/s]
Loading trainS:   0%|          | 0/26 [00:00<?, ?it/s]Loading trainS:   4%|▍         | 1/26 [00:00<00:06,  4.10it/s]Loading trainS:   8%|▊         | 2/26 [00:00<00:06,  3.89it/s]Loading trainS:  12%|█▏        | 3/26 [00:00<00:05,  3.89it/s]Loading trainS:  15%|█▌        | 4/26 [00:01<00:05,  3.83it/s]Loading trainS:  19%|█▉        | 5/26 [00:01<00:05,  3.91it/s]Loading trainS:  23%|██▎       | 6/26 [00:01<00:05,  3.90it/s]Loading trainS:  27%|██▋       | 7/26 [00:01<00:04,  3.91it/s]Loading trainS:  31%|███       | 8/26 [00:02<00:04,  3.77it/s]Loading trainS:  35%|███▍      | 9/26 [00:02<00:04,  3.86it/s]Loading trainS:  38%|███▊      | 10/26 [00:02<00:04,  3.82it/s]Loading trainS:  42%|████▏     | 11/26 [00:02<00:03,  4.00it/s]Loading trainS:  46%|████▌     | 12/26 [00:03<00:03,  4.14it/s]Loading trainS:  50%|█████     | 13/26 [00:03<00:03,  3.97it/s]Loading trainS:  54%|█████▍    | 14/26 [00:03<00:02,  4.06it/s]Loading trainS:  58%|█████▊    | 15/26 [00:03<00:02,  4.07it/s]Loading trainS:  62%|██████▏   | 16/26 [00:04<00:02,  4.18it/s]Loading trainS:  65%|██████▌   | 17/26 [00:04<00:02,  4.22it/s]Loading trainS:  69%|██████▉   | 18/26 [00:04<00:01,  4.34it/s]Loading trainS:  73%|███████▎  | 19/26 [00:04<00:01,  4.34it/s]Loading trainS:  77%|███████▋  | 20/26 [00:04<00:01,  4.46it/s]Loading trainS:  81%|████████  | 21/26 [00:05<00:01,  4.11it/s]Loading trainS:  85%|████████▍ | 22/26 [00:05<00:00,  4.06it/s]Loading trainS:  88%|████████▊ | 23/26 [00:05<00:00,  4.15it/s]Loading trainS:  92%|█████████▏| 24/26 [00:05<00:00,  4.02it/s]Loading trainS:  96%|█████████▌| 25/26 [00:06<00:00,  4.05it/s]Loading trainS: 100%|██████████| 26/26 [00:06<00:00,  4.12it/s]Loading trainS: 100%|██████████| 26/26 [00:06<00:00,  4.04it/s]
Loading testS:   0%|          | 0/8 [00:00<?, ?it/s]Loading testS:  12%|█▎        | 1/8 [00:00<00:01,  3.80it/s]Loading testS:  25%|██▌       | 2/8 [00:00<00:01,  3.94it/s]Loading testS:  38%|███▊      | 3/8 [00:00<00:01,  4.08it/s]Loading testS:  50%|█████     | 4/8 [00:00<00:00,  4.26it/s]Loading testS:  62%|██████▎   | 5/8 [00:01<00:00,  4.25it/s]Loading testS:  75%|███████▌  | 6/8 [00:01<00:00,  4.24it/s]Loading testS:  88%|████████▊ | 7/8 [00:01<00:00,  4.39it/s]Loading testS: 100%|██████████| 8/8 [00:01<00:00,  4.37it/s]Loading testS: 100%|██████████| 8/8 [00:01<00:00,  4.34it/s]----------+++ 
CrossVal ['d']
CrossVal ['d']
(0/8) test vimp2_L_ET_CSFn2
(1/8) test vimp2_ANON972_MS_CSFn2
(2/8) test vimp2_M_ET_CSFn2
(3/8) test vimp2_ctrl_921_07122013_MP
(4/8) test vimp2_ANON967_MS_CSFn2
(5/8) test vimp2_N_ET_CSFn2
(6/8) test vimp2_K_ET_CSFn2
(7/8) test vimp2_ANON988_MS_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.0001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 112, 120, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 112, 120, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 112, 120, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 112, 120, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 112, 120, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 112, 120, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 112, 120, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 56, 60, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 56, 60, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 56, 60, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 56, 60, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 56, 60, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 56, 60, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 56, 60, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 56, 60, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 60, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 28, 30, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 28, 30, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 28, 30, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 28, 30, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 28, 30, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 28, 30, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 28, 30, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 28, 30, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 30, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 28, 30, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 56, 60, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 56, 60, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 56, 60, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 56, 60, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 56, 60, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 56, 60, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 56, 60, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 56, 60, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 56, 60, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 56, 60, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 112, 120, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 112, 120, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 112, 120, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 112, 120, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 112, 120, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 112, 120, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 112, 120, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 112, 120, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 112, 120, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 112, 120, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 112, 120, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97530947 0.02469053]
Train on 1677 samples, validate on 496 samples
Epoch 1/300
 - 19s - loss: 0.3675 - acc: 0.9512 - mDice: 0.2915 - val_loss: 0.3477 - val_acc: 0.9795 - val_mDice: 0.3154

Epoch 00001: val_mDice improved from -inf to 0.31542, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 8s - loss: 0.2214 - acc: 0.9780 - mDice: 0.5689 - val_loss: 0.3002 - val_acc: 0.9852 - val_mDice: 0.3827

Epoch 00002: val_mDice improved from 0.31542 to 0.38270, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 8s - loss: 0.1837 - acc: 0.9833 - mDice: 0.6415 - val_loss: 0.2895 - val_acc: 0.9866 - val_mDice: 0.3881

Epoch 00003: val_mDice improved from 0.38270 to 0.38812, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300
 - 8s - loss: 0.1609 - acc: 0.9854 - mDice: 0.6858 - val_loss: 0.2941 - val_acc: 0.9844 - val_mDice: 0.3994

Epoch 00004: val_mDice improved from 0.38812 to 0.39942, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300
 - 9s - loss: 0.1460 - acc: 0.9866 - mDice: 0.7151 - val_loss: 0.2743 - val_acc: 0.9883 - val_mDice: 0.4191

Epoch 00005: val_mDice improved from 0.39942 to 0.41910, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300
 - 8s - loss: 0.1425 - acc: 0.9872 - mDice: 0.7217 - val_loss: 0.2197 - val_acc: 0.9881 - val_mDice: 0.4076

Epoch 00006: val_mDice did not improve from 0.41910
Epoch 7/300
 - 8s - loss: 0.1362 - acc: 0.9877 - mDice: 0.7341 - val_loss: 0.2496 - val_acc: 0.9889 - val_mDice: 0.4267

Epoch 00007: val_mDice improved from 0.41910 to 0.42668, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/300
 - 8s - loss: 0.1278 - acc: 0.9884 - mDice: 0.7505 - val_loss: 0.2317 - val_acc: 0.9893 - val_mDice: 0.4291

Epoch 00008: val_mDice improved from 0.42668 to 0.42912, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 9/300
 - 8s - loss: 0.1288 - acc: 0.9887 - mDice: 0.7483 - val_loss: 0.2148 - val_acc: 0.9890 - val_mDice: 0.4207

Epoch 00009: val_mDice did not improve from 0.42912
Epoch 10/300
 - 8s - loss: 0.1235 - acc: 0.9890 - mDice: 0.7588 - val_loss: 0.2207 - val_acc: 0.9893 - val_mDice: 0.4365

Epoch 00010: val_mDice improved from 0.42912 to 0.43651, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 11/300
 - 8s - loss: 0.1210 - acc: 0.9893 - mDice: 0.7637 - val_loss: 0.2034 - val_acc: 0.9900 - val_mDice: 0.4419

Epoch 00011: val_mDice improved from 0.43651 to 0.44191, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 12/300
 - 8s - loss: 0.1149 - acc: 0.9896 - mDice: 0.7758 - val_loss: 0.2463 - val_acc: 0.9891 - val_mDice: 0.4365

Epoch 00012: val_mDice did not improve from 0.44191
Epoch 13/300
 - 8s - loss: 0.1138 - acc: 0.9902 - mDice: 0.7775 - val_loss: 0.2266 - val_acc: 0.9893 - val_mDice: 0.4372

Epoch 00013: val_mDice did not improve from 0.44191
Epoch 14/300
 - 8s - loss: 0.1079 - acc: 0.9902 - mDice: 0.7893 - val_loss: 0.2146 - val_acc: 0.9896 - val_mDice: 0.4221

Epoch 00014: val_mDice did not improve from 0.44191
Epoch 15/300
 - 8s - loss: 0.1075 - acc: 0.9903 - mDice: 0.7902 - val_loss: 0.1852 - val_acc: 0.9906 - val_mDice: 0.4449

Epoch 00015: val_mDice improved from 0.44191 to 0.44491, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 16/300
 - 8s - loss: 0.1070 - acc: 0.9904 - mDice: 0.7911 - val_loss: 0.2123 - val_acc: 0.9903 - val_mDice: 0.4333

Epoch 00016: val_mDice did not improve from 0.44491
Epoch 17/300
 - 8s - loss: 0.1043 - acc: 0.9907 - mDice: 0.7964 - val_loss: 0.2074 - val_acc: 0.9906 - val_mDice: 0.4430

Epoch 00017: val_mDice did not improve from 0.44491
Epoch 18/300
 - 8s - loss: 0.1011 - acc: 0.9907 - mDice: 0.8027 - val_loss: 0.2323 - val_acc: 0.9908 - val_mDice: 0.4442

Epoch 00018: val_mDice did not improve from 0.44491
Epoch 19/300
 - 8s - loss: 0.1032 - acc: 0.9910 - mDice: 0.7984 - val_loss: 0.1927 - val_acc: 0.9907 - val_mDice: 0.4468

Epoch 00019: val_mDice improved from 0.44491 to 0.44676, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 20/300
 - 8s - loss: 0.1031 - acc: 0.9911 - mDice: 0.7985 - val_loss: 0.2307 - val_acc: 0.9909 - val_mDice: 0.4429

Epoch 00020: val_mDice did not improve from 0.44676
Epoch 21/300
 - 8s - loss: 0.1028 - acc: 0.9912 - mDice: 0.7990 - val_loss: 0.2136 - val_acc: 0.9910 - val_mDice: 0.4459

Epoch 00021: val_mDice did not improve from 0.44676
Epoch 22/300
 - 9s - loss: 0.1006 - acc: 0.9913 - mDice: 0.8033 - val_loss: 0.2448 - val_acc: 0.9903 - val_mDice: 0.4497

Epoch 00022: val_mDice improved from 0.44676 to 0.44969, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 23/300
 - 8s - loss: 0.0953 - acc: 0.9915 - mDice: 0.8138 - val_loss: 0.2715 - val_acc: 0.9817 - val_mDice: 0.4015

Epoch 00023: val_mDice did not improve from 0.44969
Epoch 24/300
 - 8s - loss: 0.0947 - acc: 0.9914 - mDice: 0.8150 - val_loss: 0.1936 - val_acc: 0.9907 - val_mDice: 0.4357

Epoch 00024: val_mDice did not improve from 0.44969
Epoch 25/300
 - 8s - loss: 0.0989 - acc: 0.9916 - mDice: 0.8066 - val_loss: 0.2033 - val_acc: 0.9913 - val_mDice: 0.4570

Epoch 00025: val_mDice improved from 0.44969 to 0.45704, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 26/300
 - 8s - loss: 0.0914 - acc: 0.9918 - mDice: 0.8214 - val_loss: 0.2339 - val_acc: 0.9909 - val_mDice: 0.4575

Epoch 00026: val_mDice improved from 0.45704 to 0.45753, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 27/300
 - 8s - loss: 0.0981 - acc: 0.9917 - mDice: 0.8081 - val_loss: 0.2064 - val_acc: 0.9913 - val_mDice: 0.4454

Epoch 00027: val_mDice did not improve from 0.45753
Epoch 28/300
 - 9s - loss: 0.0959 - acc: 0.9919 - mDice: 0.8124 - val_loss: 0.1827 - val_acc: 0.9916 - val_mDice: 0.4506

Epoch 00028: val_mDice did not improve from 0.45753
Epoch 29/300
 - 8s - loss: 0.0922 - acc: 0.9919 - mDice: 0.8197 - val_loss: 0.1903 - val_acc: 0.9916 - val_mDice: 0.4486

Epoch 00029: val_mDice did not improve from 0.45753
Epoch 30/300
 - 8s - loss: 0.0880 - acc: 0.9922 - mDice: 0.8282 - val_loss: 0.2392 - val_acc: 0.9908 - val_mDice: 0.4569

Epoch 00030: val_mDice did not improve from 0.45753
Epoch 31/300
 - 8s - loss: 0.0863 - acc: 0.9924 - mDice: 0.8313 - val_loss: 0.1886 - val_acc: 0.9918 - val_mDice: 0.4529

Epoch 00031: val_mDice did not improve from 0.45753
Epoch 32/300
 - 8s - loss: 0.0860 - acc: 0.9923 - mDice: 0.8320 - val_loss: 0.2409 - val_acc: 0.9904 - val_mDice: 0.4547

Epoch 00032: val_mDice did not improve from 0.45753
Epoch 33/300
 - 8s - loss: 0.0879 - acc: 0.9923 - mDice: 0.8283 - val_loss: 0.1704 - val_acc: 0.9918 - val_mDice: 0.4471

Epoch 00033: val_mDice did not improve from 0.45753
Epoch 34/300
 - 9s - loss: 0.0884 - acc: 0.9923 - mDice: 0.8271 - val_loss: 0.2410 - val_acc: 0.9918 - val_mDice: 0.4566

Epoch 00034: val_mDice did not improve from 0.45753
Epoch 35/300
 - 9s - loss: 0.0913 - acc: 0.9923 - mDice: 0.8213 - val_loss: 0.1827 - val_acc: 0.9922 - val_mDice: 0.4723

Epoch 00035: val_mDice improved from 0.45753 to 0.47232, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 36/300
 - 8s - loss: 0.0882 - acc: 0.9925 - mDice: 0.8275 - val_loss: 0.2115 - val_acc: 0.9913 - val_mDice: 0.4662

Epoch 00036: val_mDice did not improve from 0.47232
Epoch 37/300
 - 9s - loss: 0.0902 - acc: 0.9925 - mDice: 0.8235 - val_loss: 0.1649 - val_acc: 0.9913 - val_mDice: 0.4364

Epoch 00037: val_mDice did not improve from 0.47232
Epoch 38/300
 - 9s - loss: 0.0833 - acc: 0.9926 - mDice: 0.8373 - val_loss: 0.1491 - val_acc: 0.9924 - val_mDice: 0.4627

Epoch 00038: val_mDice did not improve from 0.47232
Epoch 39/300
 - 8s - loss: 0.0820 - acc: 0.9926 - mDice: 0.8398 - val_loss: 0.1842 - val_acc: 0.9922 - val_mDice: 0.4612

Epoch 00039: val_mDice did not improve from 0.47232
Epoch 40/300
 - 9s - loss: 0.0853 - acc: 0.9927 - mDice: 0.8332 - val_loss: 0.1467 - val_acc: 0.9923 - val_mDice: 0.4568

Epoch 00040: val_mDice did not improve from 0.47232
Epoch 41/300
 - 9s - loss: 0.0817 - acc: 0.9926 - mDice: 0.8405 - val_loss: 0.1601 - val_acc: 0.9925 - val_mDice: 0.4676

Epoch 00041: val_mDice did not improve from 0.47232
Epoch 42/300
 - 9s - loss: 0.0832 - acc: 0.9929 - mDice: 0.8373 - val_loss: 0.1762 - val_acc: 0.9922 - val_mDice: 0.4725

Epoch 00042: val_mDice improved from 0.47232 to 0.47245, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 43/300
 - 8s - loss: 0.0828 - acc: 0.9928 - mDice: 0.8381 - val_loss: 0.1736 - val_acc: 0.9919 - val_mDice: 0.4701

Epoch 00043: val_mDice did not improve from 0.47245
Epoch 44/300
 - 8s - loss: 0.0812 - acc: 0.9929 - mDice: 0.8412 - val_loss: 0.1592 - val_acc: 0.9926 - val_mDice: 0.4649

Epoch 00044: val_mDice did not improve from 0.47245
Epoch 45/300
 - 9s - loss: 0.0836 - acc: 0.9927 - mDice: 0.8366 - val_loss: 0.1564 - val_acc: 0.9924 - val_mDice: 0.4705

Epoch 00045: val_mDice did not improve from 0.47245
Epoch 46/300
 - 9s - loss: 0.0782 - acc: 0.9931 - mDice: 0.8472 - val_loss: 0.1554 - val_acc: 0.9924 - val_mDice: 0.4727

Epoch 00046: val_mDice improved from 0.47245 to 0.47273, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 47/300
 - 8s - loss: 0.0808 - acc: 0.9930 - mDice: 0.8420 - val_loss: 0.1962 - val_acc: 0.9917 - val_mDice: 0.4692

Epoch 00047: val_mDice did not improve from 0.47273
Epoch 48/300
 - 8s - loss: 0.0797 - acc: 0.9929 - mDice: 0.8443 - val_loss: 0.1779 - val_acc: 0.9919 - val_mDice: 0.4706

Epoch 00048: val_mDice did not improve from 0.47273
Epoch 49/300
 - 9s - loss: 0.0838 - acc: 0.9931 - mDice: 0.8360 - val_loss: 0.1422 - val_acc: 0.9928 - val_mDice: 0.4666

Epoch 00049: val_mDice did not improve from 0.47273
Epoch 50/300
 - 9s - loss: 0.0787 - acc: 0.9932 - mDice: 0.8462 - val_loss: 0.1657 - val_acc: 0.9924 - val_mDice: 0.4758

Epoch 00050: val_mDice improved from 0.47273 to 0.47578, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 51/300
 - 9s - loss: 0.0757 - acc: 0.9933 - mDice: 0.8520 - val_loss: 0.1745 - val_acc: 0.9924 - val_mDice: 0.4722

Epoch 00051: val_mDice did not improve from 0.47578
Epoch 52/300
 - 8s - loss: 0.0775 - acc: 0.9933 - mDice: 0.8485 - val_loss: 0.1989 - val_acc: 0.9919 - val_mDice: 0.4708

Epoch 00052: val_mDice did not improve from 0.47578
Epoch 53/300
 - 8s - loss: 0.0768 - acc: 0.9933 - mDice: 0.8499 - val_loss: 0.1501 - val_acc: 0.9920 - val_mDice: 0.4463

Epoch 00053: val_mDice did not improve from 0.47578
Epoch 54/300
 - 8s - loss: 0.0824 - acc: 0.9932 - mDice: 0.8388 - val_loss: 0.1615 - val_acc: 0.9928 - val_mDice: 0.4678

Epoch 00054: val_mDice did not improve from 0.47578
Epoch 55/300
 - 9s - loss: 0.0771 - acc: 0.9934 - mDice: 0.8492 - val_loss: 0.1875 - val_acc: 0.9929 - val_mDice: 0.4698

Epoch 00055: val_mDice did not improve from 0.47578
Epoch 56/300
 - 9s - loss: 0.0742 - acc: 0.9934 - mDice: 0.8551 - val_loss: 0.1727 - val_acc: 0.9927 - val_mDice: 0.4619

Epoch 00056: val_mDice did not improve from 0.47578
Epoch 57/300
 - 9s - loss: 0.0788 - acc: 0.9933 - mDice: 0.8460 - val_loss: 0.1620 - val_acc: 0.9931 - val_mDice: 0.4760

Epoch 00057: val_mDice improved from 0.47578 to 0.47598, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 58/300
 - 8s - loss: 0.0762 - acc: 0.9934 - mDice: 0.8510 - val_loss: 0.1884 - val_acc: 0.9927 - val_mDice: 0.4758

Epoch 00058: val_mDice did not improve from 0.47598
Epoch 59/300
 - 8s - loss: 0.0793 - acc: 0.9933 - mDice: 0.8448 - val_loss: 0.2041 - val_acc: 0.9923 - val_mDice: 0.4745

Epoch 00059: val_mDice did not improve from 0.47598
Epoch 60/300
 - 9s - loss: 0.0763 - acc: 0.9935 - mDice: 0.8509 - val_loss: 0.1502 - val_acc: 0.9930 - val_mDice: 0.4788

Epoch 00060: val_mDice improved from 0.47598 to 0.47877, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 61/300
 - 8s - loss: 0.0757 - acc: 0.9934 - mDice: 0.8521 - val_loss: 0.1561 - val_acc: 0.9929 - val_mDice: 0.4788

Epoch 00061: val_mDice did not improve from 0.47877
Epoch 62/300
 - 8s - loss: 0.0775 - acc: 0.9936 - mDice: 0.8484 - val_loss: 0.1431 - val_acc: 0.9931 - val_mDice: 0.4688

Epoch 00062: val_mDice did not improve from 0.47877
Epoch 63/300
 - 9s - loss: 0.0754 - acc: 0.9936 - mDice: 0.8526 - val_loss: 0.1721 - val_acc: 0.9923 - val_mDice: 0.4786

Epoch 00063: val_mDice did not improve from 0.47877
Epoch 64/300
 - 8s - loss: 0.0788 - acc: 0.9934 - mDice: 0.8458 - val_loss: 0.1227 - val_acc: 0.9932 - val_mDice: 0.4789

Epoch 00064: val_mDice improved from 0.47877 to 0.47888, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 65/300
 - 9s - loss: 0.0808 - acc: 0.9934 - mDice: 0.8419 - val_loss: 0.2419 - val_acc: 0.9896 - val_mDice: 0.4498

Epoch 00065: val_mDice did not improve from 0.47888
Epoch 66/300
 - 8s - loss: 0.0775 - acc: 0.9934 - mDice: 0.8485 - val_loss: 0.1475 - val_acc: 0.9932 - val_mDice: 0.4757

Epoch 00066: val_mDice did not improve from 0.47888
Epoch 67/300
 - 8s - loss: 0.0740 - acc: 0.9936 - mDice: 0.8553 - val_loss: 0.1607 - val_acc: 0.9931 - val_mDice: 0.4697

Epoch 00067: val_mDice did not improve from 0.47888
Epoch 68/300
 - 9s - loss: 0.0735 - acc: 0.9937 - mDice: 0.8563 - val_loss: 0.1704 - val_acc: 0.9932 - val_mDice: 0.4786

Epoch 00068: val_mDice did not improve from 0.47888
Epoch 69/300
 - 8s - loss: 0.0716 - acc: 0.9938 - mDice: 0.8600 - val_loss: 0.1373 - val_acc: 0.9932 - val_mDice: 0.4812

Epoch 00069: val_mDice improved from 0.47888 to 0.48119, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 70/300
 - 8s - loss: 0.0747 - acc: 0.9938 - mDice: 0.8538 - val_loss: 0.2022 - val_acc: 0.9927 - val_mDice: 0.4800

Epoch 00070: val_mDice did not improve from 0.48119
Epoch 71/300
 - 8s - loss: 0.0746 - acc: 0.9938 - mDice: 0.8541 - val_loss: 0.2097 - val_acc: 0.9922 - val_mDice: 0.4762

Epoch 00071: val_mDice did not improve from 0.48119
Epoch 72/300
 - 9s - loss: 0.0692 - acc: 0.9938 - mDice: 0.8649 - val_loss: 0.1294 - val_acc: 0.9931 - val_mDice: 0.4777

Epoch 00072: val_mDice did not improve from 0.48119
Epoch 73/300
 - 8s - loss: 0.0708 - acc: 0.9938 - mDice: 0.8616 - val_loss: 0.1336 - val_acc: 0.9934 - val_mDice: 0.4807

Epoch 00073: val_mDice did not improve from 0.48119
Epoch 74/300
 - 8s - loss: 0.0687 - acc: 0.9939 - mDice: 0.8657 - val_loss: 0.1338 - val_acc: 0.9932 - val_mDice: 0.4807

Epoch 00074: val_mDice did not improve from 0.48119
Epoch 75/300
 - 8s - loss: 0.0691 - acc: 0.9939 - mDice: 0.8649 - val_loss: 0.1406 - val_acc: 0.9930 - val_mDice: 0.4804

Epoch 00075: val_mDice did not improve from 0.48119
Epoch 76/300
 - 9s - loss: 0.0761 - acc: 0.9937 - mDice: 0.8512 - val_loss: 0.1697 - val_acc: 0.9929 - val_mDice: 0.4799

Epoch 00076: val_mDice did not improve from 0.48119
Epoch 77/300
 - 9s - loss: 0.0734 - acc: 0.9938 - mDice: 0.8565 - val_loss: 0.2066 - val_acc: 0.9928 - val_mDice: 0.4810

Epoch 00077: val_mDice did not improve from 0.48119
Epoch 78/300
 - 8s - loss: 0.0715 - acc: 0.9939 - mDice: 0.8602 - val_loss: 0.1647 - val_acc: 0.9930 - val_mDice: 0.4819

Epoch 00078: val_mDice improved from 0.48119 to 0.48195, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 79/300
 - 8s - loss: 0.0763 - acc: 0.9940 - mDice: 0.8506 - val_loss: 0.1407 - val_acc: 0.9933 - val_mDice: 0.4835

Epoch 00079: val_mDice improved from 0.48195 to 0.48349, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 80/300
 - 8s - loss: 0.0691 - acc: 0.9939 - mDice: 0.8649 - val_loss: 0.1178 - val_acc: 0.9935 - val_mDice: 0.4793

Epoch 00080: val_mDice did not improve from 0.48349
Epoch 81/300
 - 8s - loss: 0.0700 - acc: 0.9939 - mDice: 0.8631 - val_loss: 0.1804 - val_acc: 0.9932 - val_mDice: 0.4836

Epoch 00081: val_mDice improved from 0.48349 to 0.48362, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 82/300
 - 8s - loss: 0.0692 - acc: 0.9940 - mDice: 0.8647 - val_loss: 0.0862 - val_acc: 0.9935 - val_mDice: 0.4779

Epoch 00082: val_mDice did not improve from 0.48362
Epoch 83/300
 - 9s - loss: 0.0692 - acc: 0.9939 - mDice: 0.8647 - val_loss: 0.1727 - val_acc: 0.9931 - val_mDice: 0.4839

Epoch 00083: val_mDice improved from 0.48362 to 0.48392, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 84/300
 - 8s - loss: 0.0697 - acc: 0.9940 - mDice: 0.8637 - val_loss: 0.0856 - val_acc: 0.9936 - val_mDice: 0.4816

Epoch 00084: val_mDice did not improve from 0.48392
Epoch 85/300
 - 8s - loss: 0.0692 - acc: 0.9940 - mDice: 0.8646 - val_loss: 0.1796 - val_acc: 0.9930 - val_mDice: 0.4819

Epoch 00085: val_mDice did not improve from 0.48392
Epoch 86/300
 - 8s - loss: 0.0728 - acc: 0.9941 - mDice: 0.8575 - val_loss: 0.1509 - val_acc: 0.9933 - val_mDice: 0.4851

Epoch 00086: val_mDice improved from 0.48392 to 0.48509, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 87/300
 - 8s - loss: 0.0675 - acc: 0.9941 - mDice: 0.8680 - val_loss: 0.1991 - val_acc: 0.9929 - val_mDice: 0.4837

Epoch 00087: val_mDice did not improve from 0.48509
Epoch 88/300
 - 8s - loss: 0.0676 - acc: 0.9941 - mDice: 0.8679 - val_loss: 0.1817 - val_acc: 0.9928 - val_mDice: 0.4847

Epoch 00088: val_mDice did not improve from 0.48509
Epoch 89/300
 - 8s - loss: 0.0676 - acc: 0.9941 - mDice: 0.8679 - val_loss: 0.1806 - val_acc: 0.9928 - val_mDice: 0.4828

Epoch 00089: val_mDice did not improve from 0.48509
Epoch 90/300
 - 8s - loss: 0.0677 - acc: 0.9941 - mDice: 0.8677 - val_loss: 0.1426 - val_acc: 0.9935 - val_mDice: 0.4851

Epoch 00090: val_mDice improved from 0.48509 to 0.48510, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 91/300
 - 8s - loss: 0.0657 - acc: 0.9942 - mDice: 0.8717 - val_loss: 0.1260 - val_acc: 0.9934 - val_mDice: 0.4859

Epoch 00091: val_mDice improved from 0.48510 to 0.48594, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 92/300
 - 8s - loss: 0.0698 - acc: 0.9942 - mDice: 0.8634 - val_loss: 0.1175 - val_acc: 0.9936 - val_mDice: 0.4852

Epoch 00092: val_mDice did not improve from 0.48594
Epoch 93/300
 - 9s - loss: 0.0642 - acc: 0.9942 - mDice: 0.8747 - val_loss: 0.1316 - val_acc: 0.9937 - val_mDice: 0.4851

Epoch 00093: val_mDice did not improve from 0.48594
Epoch 94/300
 - 9s - loss: 0.0653 - acc: 0.9943 - mDice: 0.8724 - val_loss: 0.1062 - val_acc: 0.9937 - val_mDice: 0.4854

Epoch 00094: val_mDice did not improve from 0.48594
Epoch 95/300
 - 9s - loss: 0.0714 - acc: 0.9942 - mDice: 0.8602 - val_loss: 0.1633 - val_acc: 0.9926 - val_mDice: 0.4855

Epoch 00095: val_mDice did not improve from 0.48594
Epoch 96/300
 - 8s - loss: 0.0651 - acc: 0.9942 - mDice: 0.8729 - val_loss: 0.1030 - val_acc: 0.9937 - val_mDice: 0.4848

Epoch 00096: val_mDice did not improve from 0.48594
Epoch 97/300
 - 8s - loss: 0.0635 - acc: 0.9942 - mDice: 0.8760 - val_loss: 0.1224 - val_acc: 0.9936 - val_mDice: 0.4865

Epoch 00097: val_mDice improved from 0.48594 to 0.48651, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 98/300
 - 9s - loss: 0.0638 - acc: 0.9943 - mDice: 0.8754 - val_loss: 0.1695 - val_acc: 0.9933 - val_mDice: 0.4868

Epoch 00098: val_mDice improved from 0.48651 to 0.48681, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 99/300
 - 8s - loss: 0.0658 - acc: 0.9943 - mDice: 0.8713 - val_loss: 0.0998 - val_acc: 0.9936 - val_mDice: 0.4872

Epoch 00099: val_mDice improved from 0.48681 to 0.48722, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 100/300
 - 8s - loss: 0.0682 - acc: 0.9942 - mDice: 0.8665 - val_loss: 0.1039 - val_acc: 0.9937 - val_mDice: 0.4811

Epoch 00100: val_mDice did not improve from 0.48722
Epoch 101/300
 - 8s - loss: 0.0662 - acc: 0.9944 - mDice: 0.8705 - val_loss: 0.1617 - val_acc: 0.9930 - val_mDice: 0.4870

Epoch 00101: val_mDice did not improve from 0.48722
Epoch 102/300
 - 9s - loss: 0.0662 - acc: 0.9943 - mDice: 0.8706 - val_loss: 0.1260 - val_acc: 0.9935 - val_mDice: 0.4880

Epoch 00102: val_mDice improved from 0.48722 to 0.48800, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 103/300
 - 9s - loss: 0.0679 - acc: 0.9944 - mDice: 0.8672 - val_loss: 0.0800 - val_acc: 0.9938 - val_mDice: 0.4846

Epoch 00103: val_mDice did not improve from 0.48800
Epoch 104/300
 - 9s - loss: 0.0635 - acc: 0.9944 - mDice: 0.8759 - val_loss: 0.1202 - val_acc: 0.9933 - val_mDice: 0.4866

Epoch 00104: val_mDice did not improve from 0.48800
Epoch 105/300
 - 8s - loss: 0.0634 - acc: 0.9944 - mDice: 0.8761 - val_loss: 0.1155 - val_acc: 0.9937 - val_mDice: 0.4840

Epoch 00105: val_mDice did not improve from 0.48800
Epoch 106/300
 - 9s - loss: 0.0694 - acc: 0.9944 - mDice: 0.8641 - val_loss: 0.1603 - val_acc: 0.9934 - val_mDice: 0.4888

Epoch 00106: val_mDice improved from 0.48800 to 0.48877, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 107/300
 - 9s - loss: 0.0638 - acc: 0.9944 - mDice: 0.8752 - val_loss: 0.1199 - val_acc: 0.9935 - val_mDice: 0.4857

Epoch 00107: val_mDice did not improve from 0.48877
Epoch 108/300
 - 8s - loss: 0.0620 - acc: 0.9945 - mDice: 0.8789 - val_loss: 0.0981 - val_acc: 0.9937 - val_mDice: 0.4863

Epoch 00108: val_mDice did not improve from 0.48877
Epoch 109/300
 - 9s - loss: 0.0637 - acc: 0.9945 - mDice: 0.8755 - val_loss: 0.1170 - val_acc: 0.9937 - val_mDice: 0.4876

Epoch 00109: val_mDice did not improve from 0.48877
Epoch 110/300
 - 9s - loss: 0.0636 - acc: 0.9944 - mDice: 0.8756 - val_loss: 0.1204 - val_acc: 0.9936 - val_mDice: 0.4921

Epoch 00110: val_mDice improved from 0.48877 to 0.49208, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 111/300
 - 8s - loss: 0.0657 - acc: 0.9945 - mDice: 0.8714 - val_loss: 0.1214 - val_acc: 0.9938 - val_mDice: 0.4850

Epoch 00111: val_mDice did not improve from 0.49208
Epoch 112/300
 - 9s - loss: 0.0667 - acc: 0.9945 - mDice: 0.8695 - val_loss: 0.1150 - val_acc: 0.9933 - val_mDice: 0.4931

Epoch 00112: val_mDice improved from 0.49208 to 0.49315, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 113/300
 - 8s - loss: 0.0634 - acc: 0.9946 - mDice: 0.8760 - val_loss: 0.1386 - val_acc: 0.9934 - val_mDice: 0.4925

Epoch 00113: val_mDice did not improve from 0.49315
Epoch 114/300
 - 8s - loss: 0.0635 - acc: 0.9945 - mDice: 0.8759 - val_loss: 0.1768 - val_acc: 0.9930 - val_mDice: 0.4917

Epoch 00114: val_mDice did not improve from 0.49315
Epoch 115/300
 - 9s - loss: 0.0610 - acc: 0.9945 - mDice: 0.8807 - val_loss: 0.1304 - val_acc: 0.9938 - val_mDice: 0.4903

Epoch 00115: val_mDice did not improve from 0.49315
Epoch 116/300
 - 8s - loss: 0.0599 - acc: 0.9946 - mDice: 0.8830 - val_loss: 0.1208 - val_acc: 0.9937 - val_mDice: 0.4904

Epoch 00116: val_mDice did not improve from 0.49315
Epoch 117/300
 - 8s - loss: 0.0611 - acc: 0.9945 - mDice: 0.8805 - val_loss: 0.1193 - val_acc: 0.9934 - val_mDice: 0.4947

Epoch 00117: val_mDice improved from 0.49315 to 0.49472, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 118/300
 - 9s - loss: 0.0636 - acc: 0.9946 - mDice: 0.8756 - val_loss: 0.1340 - val_acc: 0.9938 - val_mDice: 0.4944

Epoch 00118: val_mDice did not improve from 0.49472
Epoch 119/300
 - 9s - loss: 0.0620 - acc: 0.9946 - mDice: 0.8788 - val_loss: 0.1235 - val_acc: 0.9938 - val_mDice: 0.4864

Epoch 00119: val_mDice did not improve from 0.49472
Epoch 120/300
 - 8s - loss: 0.0648 - acc: 0.9946 - mDice: 0.8732 - val_loss: 0.1455 - val_acc: 0.9937 - val_mDice: 0.4888

Epoch 00120: val_mDice did not improve from 0.49472
Epoch 121/300
 - 8s - loss: 0.0609 - acc: 0.9946 - mDice: 0.8810 - val_loss: 0.1288 - val_acc: 0.9938 - val_mDice: 0.4876

Epoch 00121: val_mDice did not improve from 0.49472
Epoch 122/300
 - 8s - loss: 0.0602 - acc: 0.9947 - mDice: 0.8823 - val_loss: 0.1287 - val_acc: 0.9939 - val_mDice: 0.4910

Epoch 00122: val_mDice did not improve from 0.49472
Epoch 123/300
 - 9s - loss: 0.0602 - acc: 0.9947 - mDice: 0.8823 - val_loss: 0.1449 - val_acc: 0.9933 - val_mDice: 0.4939

Epoch 00123: val_mDice did not improve from 0.49472
Epoch 124/300
 - 9s - loss: 0.0623 - acc: 0.9947 - mDice: 0.8781 - val_loss: 0.1537 - val_acc: 0.9939 - val_mDice: 0.4946

Epoch 00124: val_mDice did not improve from 0.49472
Epoch 125/300
 - 8s - loss: 0.0593 - acc: 0.9947 - mDice: 0.8841 - val_loss: 0.1881 - val_acc: 0.9928 - val_mDice: 0.4926

Epoch 00125: val_mDice did not improve from 0.49472
Epoch 126/300
 - 9s - loss: 0.0639 - acc: 0.9946 - mDice: 0.8750 - val_loss: 0.1527 - val_acc: 0.9939 - val_mDice: 0.4979

Epoch 00126: val_mDice improved from 0.49472 to 0.49794, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 127/300
 - 8s - loss: 0.0608 - acc: 0.9947 - mDice: 0.8811 - val_loss: 0.1090 - val_acc: 0.9936 - val_mDice: 0.4838

Epoch 00127: val_mDice did not improve from 0.49794
Epoch 128/300
 - 9s - loss: 0.0638 - acc: 0.9947 - mDice: 0.8751 - val_loss: 0.2412 - val_acc: 0.9918 - val_mDice: 0.4817

Epoch 00128: val_mDice did not improve from 0.49794
Epoch 129/300
 - 9s - loss: 0.0604 - acc: 0.9946 - mDice: 0.8819 - val_loss: 0.1340 - val_acc: 0.9938 - val_mDice: 0.4972

Epoch 00129: val_mDice did not improve from 0.49794
Epoch 130/300
 - 8s - loss: 0.0589 - acc: 0.9948 - mDice: 0.8850 - val_loss: 0.1509 - val_acc: 0.9938 - val_mDice: 0.4973

Epoch 00130: val_mDice did not improve from 0.49794
Epoch 131/300
 - 8s - loss: 0.0618 - acc: 0.9947 - mDice: 0.8792 - val_loss: 0.1542 - val_acc: 0.9938 - val_mDice: 0.5005

Epoch 00131: val_mDice improved from 0.49794 to 0.50055, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 132/300
 - 9s - loss: 0.0611 - acc: 0.9948 - mDice: 0.8805 - val_loss: 0.1631 - val_acc: 0.9935 - val_mDice: 0.4993

Epoch 00132: val_mDice did not improve from 0.50055
Epoch 133/300
 - 8s - loss: 0.0613 - acc: 0.9947 - mDice: 0.8801 - val_loss: 0.1749 - val_acc: 0.9930 - val_mDice: 0.4960

Epoch 00133: val_mDice did not improve from 0.50055
Epoch 134/300
 - 9s - loss: 0.0603 - acc: 0.9948 - mDice: 0.8820 - val_loss: 0.1388 - val_acc: 0.9939 - val_mDice: 0.4989

Epoch 00134: val_mDice did not improve from 0.50055
Epoch 135/300
 - 8s - loss: 0.0596 - acc: 0.9947 - mDice: 0.8835 - val_loss: 0.1401 - val_acc: 0.9938 - val_mDice: 0.4990

Epoch 00135: val_mDice did not improve from 0.50055
Epoch 136/300
 - 9s - loss: 0.0590 - acc: 0.9948 - mDice: 0.8847 - val_loss: 0.1685 - val_acc: 0.9936 - val_mDice: 0.4983

Epoch 00136: val_mDice did not improve from 0.50055
Epoch 137/300
 - 9s - loss: 0.0596 - acc: 0.9948 - mDice: 0.8835 - val_loss: 0.1534 - val_acc: 0.9934 - val_mDice: 0.4993

Epoch 00137: val_mDice did not improve from 0.50055
Epoch 138/300
 - 9s - loss: 0.0623 - acc: 0.9948 - mDice: 0.8780 - val_loss: 0.1365 - val_acc: 0.9939 - val_mDice: 0.4936

Epoch 00138: val_mDice did not improve from 0.50055
Epoch 139/300
 - 8s - loss: 0.0570 - acc: 0.9948 - mDice: 0.8887 - val_loss: 0.1610 - val_acc: 0.9936 - val_mDice: 0.4998

Epoch 00139: val_mDice did not improve from 0.50055
Epoch 140/300
 - 9s - loss: 0.0591 - acc: 0.9948 - mDice: 0.8845 - val_loss: 0.1636 - val_acc: 0.9939 - val_mDice: 0.4952

Epoch 00140: val_mDice did not improve from 0.50055
Epoch 141/300
 - 9s - loss: 0.0601 - acc: 0.9948 - mDice: 0.8824 - val_loss: 0.1509 - val_acc: 0.9939 - val_mDice: 0.4965

Epoch 00141: val_mDice did not improve from 0.50055
Epoch 142/300
 - 9s - loss: 0.0598 - acc: 0.9949 - mDice: 0.8830 - val_loss: 0.1025 - val_acc: 0.9940 - val_mDice: 0.5001

Epoch 00142: val_mDice did not improve from 0.50055
Epoch 143/300
 - 9s - loss: 0.0583 - acc: 0.9948 - mDice: 0.8861 - val_loss: 0.1475 - val_acc: 0.9938 - val_mDice: 0.5029

Epoch 00143: val_mDice improved from 0.50055 to 0.50292, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 144/300
 - 9s - loss: 0.0579 - acc: 0.9949 - mDice: 0.8868 - val_loss: 0.1384 - val_acc: 0.9940 - val_mDice: 0.5013

Epoch 00144: val_mDice did not improve from 0.50292
Epoch 145/300
 - 9s - loss: 0.0579 - acc: 0.9949 - mDice: 0.8868 - val_loss: 0.1426 - val_acc: 0.9941 - val_mDice: 0.5011

Epoch 00145: val_mDice did not improve from 0.50292
Epoch 146/300
 - 9s - loss: 0.0585 - acc: 0.9949 - mDice: 0.8857 - val_loss: 0.1440 - val_acc: 0.9940 - val_mDice: 0.5008

Epoch 00146: val_mDice did not improve from 0.50292
Epoch 147/300
 - 9s - loss: 0.0573 - acc: 0.9949 - mDice: 0.8880 - val_loss: 0.1465 - val_acc: 0.9939 - val_mDice: 0.5012

Epoch 00147: val_mDice did not improve from 0.50292
Epoch 148/300
 - 9s - loss: 0.0623 - acc: 0.9949 - mDice: 0.8780 - val_loss: 0.1520 - val_acc: 0.9934 - val_mDice: 0.4993

Epoch 00148: val_mDice did not improve from 0.50292
Epoch 149/300
 - 9s - loss: 0.0566 - acc: 0.9949 - mDice: 0.8893 - val_loss: 0.1429 - val_acc: 0.9940 - val_mDice: 0.5029

Epoch 00149: val_mDice did not improve from 0.50292
Epoch 150/300
 - 9s - loss: 0.0574 - acc: 0.9950 - mDice: 0.8878 - val_loss: 0.1203 - val_acc: 0.9939 - val_mDice: 0.5001

Epoch 00150: val_mDice did not improve from 0.50292
Epoch 151/300
 - 8s - loss: 0.0572 - acc: 0.9951 - mDice: 0.8881 - val_loss: 0.0942 - val_acc: 0.9940 - val_mDice: 0.4995

Epoch 00151: val_mDice did not improve from 0.50292
Epoch 152/300
 - 8s - loss: 0.0579 - acc: 0.9949 - mDice: 0.8869 - val_loss: 0.1293 - val_acc: 0.9940 - val_mDice: 0.5037

Epoch 00152: val_mDice improved from 0.50292 to 0.50371, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 153/300
 - 9s - loss: 0.0579 - acc: 0.9950 - mDice: 0.8868 - val_loss: 0.1111 - val_acc: 0.9941 - val_mDice: 0.5029

Epoch 00153: val_mDice did not improve from 0.50371
Epoch 154/300
 - 9s - loss: 0.0584 - acc: 0.9950 - mDice: 0.8857 - val_loss: 0.1315 - val_acc: 0.9939 - val_mDice: 0.5043

Epoch 00154: val_mDice improved from 0.50371 to 0.50426, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 155/300
 - 8s - loss: 0.0582 - acc: 0.9949 - mDice: 0.8862 - val_loss: 0.1483 - val_acc: 0.9937 - val_mDice: 0.5058

Epoch 00155: val_mDice improved from 0.50426 to 0.50580, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 156/300
 - 8s - loss: 0.0578 - acc: 0.9950 - mDice: 0.8870 - val_loss: 0.1260 - val_acc: 0.9936 - val_mDice: 0.5035

Epoch 00156: val_mDice did not improve from 0.50580
Epoch 157/300
 - 8s - loss: 0.0590 - acc: 0.9951 - mDice: 0.8846 - val_loss: 0.1543 - val_acc: 0.9937 - val_mDice: 0.5052

Epoch 00157: val_mDice did not improve from 0.50580
Epoch 158/300
 - 9s - loss: 0.0578 - acc: 0.9950 - mDice: 0.8869 - val_loss: 0.1638 - val_acc: 0.9938 - val_mDice: 0.5044

Epoch 00158: val_mDice did not improve from 0.50580
Epoch 159/300
 - 9s - loss: 0.0551 - acc: 0.9950 - mDice: 0.8924 - val_loss: 0.1363 - val_acc: 0.9940 - val_mDice: 0.5055

Epoch 00159: val_mDice did not improve from 0.50580
Epoch 160/300
 - 9s - loss: 0.0580 - acc: 0.9950 - mDice: 0.8867 - val_loss: 0.1078 - val_acc: 0.9940 - val_mDice: 0.4984

Epoch 00160: val_mDice did not improve from 0.50580
Epoch 161/300
 - 8s - loss: 0.0591 - acc: 0.9951 - mDice: 0.8842 - val_loss: 0.1100 - val_acc: 0.9939 - val_mDice: 0.4974

Epoch 00161: val_mDice did not improve from 0.50580
Epoch 162/300
 - 8s - loss: 0.0575 - acc: 0.9950 - mDice: 0.8876 - val_loss: 0.1424 - val_acc: 0.9938 - val_mDice: 0.5051

Epoch 00162: val_mDice did not improve from 0.50580
Epoch 163/300
 - 8s - loss: 0.0568 - acc: 0.9950 - mDice: 0.8890 - val_loss: 0.1368 - val_acc: 0.9941 - val_mDice: 0.5044

Epoch 00163: val_mDice did not improve from 0.50580
Epoch 164/300
 - 9s - loss: 0.0577 - acc: 0.9950 - mDice: 0.8872 - val_loss: 0.1015 - val_acc: 0.9936 - val_mDice: 0.4937

Epoch 00164: val_mDice did not improve from 0.50580
Epoch 165/300
 - 9s - loss: 0.0549 - acc: 0.9951 - mDice: 0.8927 - val_loss: 0.1382 - val_acc: 0.9940 - val_mDice: 0.5074

Epoch 00165: val_mDice improved from 0.50580 to 0.50743, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 166/300
 - 9s - loss: 0.0566 - acc: 0.9951 - mDice: 0.8893 - val_loss: 0.1084 - val_acc: 0.9941 - val_mDice: 0.5076

Epoch 00166: val_mDice improved from 0.50743 to 0.50762, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 167/300
 - 9s - loss: 0.0541 - acc: 0.9952 - mDice: 0.8943 - val_loss: 0.1872 - val_acc: 0.9934 - val_mDice: 0.5042

Epoch 00167: val_mDice did not improve from 0.50762
Epoch 168/300
 - 8s - loss: 0.0543 - acc: 0.9951 - mDice: 0.8939 - val_loss: 0.1271 - val_acc: 0.9939 - val_mDice: 0.5062

Epoch 00168: val_mDice did not improve from 0.50762
Epoch 169/300
 - 8s - loss: 0.0554 - acc: 0.9951 - mDice: 0.8916 - val_loss: 0.1094 - val_acc: 0.9941 - val_mDice: 0.5066

Epoch 00169: val_mDice did not improve from 0.50762
Epoch 170/300
 - 9s - loss: 0.0552 - acc: 0.9951 - mDice: 0.8921 - val_loss: 0.1286 - val_acc: 0.9939 - val_mDice: 0.5089

Epoch 00170: val_mDice improved from 0.50762 to 0.50887, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 171/300
 - 9s - loss: 0.0559 - acc: 0.9951 - mDice: 0.8906 - val_loss: 0.1324 - val_acc: 0.9938 - val_mDice: 0.5082

Epoch 00171: val_mDice did not improve from 0.50887
Epoch 172/300
 - 9s - loss: 0.0571 - acc: 0.9951 - mDice: 0.8884 - val_loss: 0.1129 - val_acc: 0.9940 - val_mDice: 0.5047

Epoch 00172: val_mDice did not improve from 0.50887
Epoch 173/300
 - 9s - loss: 0.0566 - acc: 0.9951 - mDice: 0.8893 - val_loss: 0.1105 - val_acc: 0.9941 - val_mDice: 0.5079

Epoch 00173: val_mDice did not improve from 0.50887
Epoch 174/300
 - 9s - loss: 0.0555 - acc: 0.9951 - mDice: 0.8915 - val_loss: 0.0997 - val_acc: 0.9941 - val_mDice: 0.5068

Epoch 00174: val_mDice did not improve from 0.50887
Epoch 175/300
 - 9s - loss: 0.0561 - acc: 0.9952 - mDice: 0.8903 - val_loss: 0.1618 - val_acc: 0.9940 - val_mDice: 0.5081

Epoch 00175: val_mDice did not improve from 0.50887
Epoch 176/300
 - 9s - loss: 0.0589 - acc: 0.9952 - mDice: 0.8846 - val_loss: 0.1138 - val_acc: 0.9940 - val_mDice: 0.5010

Epoch 00176: val_mDice did not improve from 0.50887
Epoch 177/300
 - 9s - loss: 0.0577 - acc: 0.9952 - mDice: 0.8871 - val_loss: 0.1696 - val_acc: 0.9932 - val_mDice: 0.5024

Epoch 00177: val_mDice did not improve from 0.50887
Epoch 178/300
 - 8s - loss: 0.0569 - acc: 0.9951 - mDice: 0.8888 - val_loss: 0.1307 - val_acc: 0.9940 - val_mDice: 0.5088

Epoch 00178: val_mDice did not improve from 0.50887
Epoch 179/300
 - 9s - loss: 0.0534 - acc: 0.9952 - mDice: 0.8956 - val_loss: 0.2412 - val_acc: 0.9917 - val_mDice: 0.4816

Epoch 00179: val_mDice did not improve from 0.50887
Epoch 180/300
 - 9s - loss: 0.0567 - acc: 0.9952 - mDice: 0.8892 - val_loss: 0.1221 - val_acc: 0.9940 - val_mDice: 0.5088

Epoch 00180: val_mDice did not improve from 0.50887
Epoch 181/300
 - 9s - loss: 0.0549 - acc: 0.9952 - mDice: 0.8927 - val_loss: 0.1376 - val_acc: 0.9941 - val_mDice: 0.5086

Epoch 00181: val_mDice did not improve from 0.50887
Epoch 182/300
 - 9s - loss: 0.0533 - acc: 0.9953 - mDice: 0.8959 - val_loss: 0.1461 - val_acc: 0.9940 - val_mDice: 0.5095

Epoch 00182: val_mDice improved from 0.50887 to 0.50951, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 183/300
 - 9s - loss: 0.0541 - acc: 0.9952 - mDice: 0.8942 - val_loss: 0.1470 - val_acc: 0.9939 - val_mDice: 0.5082

Epoch 00183: val_mDice did not improve from 0.50951
Epoch 184/300
 - 9s - loss: 0.0533 - acc: 0.9953 - mDice: 0.8957 - val_loss: 0.1484 - val_acc: 0.9938 - val_mDice: 0.5054

Epoch 00184: val_mDice did not improve from 0.50951
Epoch 185/300
 - 9s - loss: 0.0525 - acc: 0.9953 - mDice: 0.8975 - val_loss: 0.1479 - val_acc: 0.9939 - val_mDice: 0.5084

Epoch 00185: val_mDice did not improve from 0.50951

Epoch 00185: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 186/300
 - 8s - loss: 0.0553 - acc: 0.9952 - mDice: 0.8919 - val_loss: 0.1477 - val_acc: 0.9940 - val_mDice: 0.5078

Epoch 00186: val_mDice did not improve from 0.50951
Epoch 187/300
 - 9s - loss: 0.0532 - acc: 0.9953 - mDice: 0.8959 - val_loss: 0.1475 - val_acc: 0.9941 - val_mDice: 0.5086

Epoch 00187: val_mDice did not improve from 0.50951
Epoch 188/300
 - 8s - loss: 0.0546 - acc: 0.9952 - mDice: 0.8932 - val_loss: 0.1462 - val_acc: 0.9941 - val_mDice: 0.5091

Epoch 00188: val_mDice did not improve from 0.50951
Epoch 189/300
 - 9s - loss: 0.0549 - acc: 0.9952 - mDice: 0.8926 - val_loss: 0.1460 - val_acc: 0.9939 - val_mDice: 0.5097

Epoch 00189: val_mDice improved from 0.50951 to 0.50975, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 190/300
 - 8s - loss: 0.0569 - acc: 0.9953 - mDice: 0.8887 - val_loss: 0.1472 - val_acc: 0.9937 - val_mDice: 0.5079

Epoch 00190: val_mDice did not improve from 0.50975
Epoch 191/300
 - 9s - loss: 0.0560 - acc: 0.9953 - mDice: 0.8904 - val_loss: 0.1277 - val_acc: 0.9941 - val_mDice: 0.5078

Epoch 00191: val_mDice did not improve from 0.50975
Epoch 192/300
 - 8s - loss: 0.0524 - acc: 0.9953 - mDice: 0.8977 - val_loss: 0.1389 - val_acc: 0.9942 - val_mDice: 0.5086

Epoch 00192: val_mDice did not improve from 0.50975
Epoch 193/300
 - 8s - loss: 0.0542 - acc: 0.9954 - mDice: 0.8939 - val_loss: 0.1492 - val_acc: 0.9940 - val_mDice: 0.5102

Epoch 00193: val_mDice improved from 0.50975 to 0.51018, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 194/300
 - 8s - loss: 0.0522 - acc: 0.9953 - mDice: 0.8980 - val_loss: 0.1247 - val_acc: 0.9941 - val_mDice: 0.5041

Epoch 00194: val_mDice did not improve from 0.51018
Epoch 195/300
 - 8s - loss: 0.0523 - acc: 0.9953 - mDice: 0.8978 - val_loss: 0.1479 - val_acc: 0.9942 - val_mDice: 0.5085

Epoch 00195: val_mDice did not improve from 0.51018
Epoch 196/300
 - 8s - loss: 0.0526 - acc: 0.9953 - mDice: 0.8971 - val_loss: 0.1280 - val_acc: 0.9941 - val_mDice: 0.5075

Epoch 00196: val_mDice did not improve from 0.51018
Epoch 197/300
 - 8s - loss: 0.0529 - acc: 0.9954 - mDice: 0.8967 - val_loss: 0.1481 - val_acc: 0.9942 - val_mDice: 0.5070

Epoch 00197: val_mDice did not improve from 0.51018
Epoch 198/300
 - 8s - loss: 0.0548 - acc: 0.9953 - mDice: 0.8928 - val_loss: 0.1477 - val_acc: 0.9940 - val_mDice: 0.5089

Epoch 00198: val_mDice did not improve from 0.51018
Epoch 199/300
 - 8s - loss: 0.0524 - acc: 0.9954 - mDice: 0.8976 - val_loss: 0.1479 - val_acc: 0.9942 - val_mDice: 0.5088

Epoch 00199: val_mDice did not improve from 0.51018
Epoch 200/300
 - 8s - loss: 0.0520 - acc: 0.9954 - mDice: 0.8984 - val_loss: 0.1278 - val_acc: 0.9940 - val_mDice: 0.5078

Epoch 00200: val_mDice did not improve from 0.51018
Epoch 201/300
 - 8s - loss: 0.0538 - acc: 0.9953 - mDice: 0.8948 - val_loss: 0.1291 - val_acc: 0.9940 - val_mDice: 0.5084

Epoch 00201: val_mDice did not improve from 0.51018
Epoch 202/300
 - 9s - loss: 0.0543 - acc: 0.9953 - mDice: 0.8938 - val_loss: 0.1464 - val_acc: 0.9941 - val_mDice: 0.5094

Epoch 00202: val_mDice did not improve from 0.51018
Epoch 203/300
 - 9s - loss: 0.0522 - acc: 0.9954 - mDice: 0.8980 - val_loss: 0.1283 - val_acc: 0.9941 - val_mDice: 0.5093

Epoch 00203: val_mDice did not improve from 0.51018
Epoch 204/300
 - 9s - loss: 0.0558 - acc: 0.9954 - mDice: 0.8907 - val_loss: 0.1515 - val_acc: 0.9939 - val_mDice: 0.5092

Epoch 00204: val_mDice did not improve from 0.51018
Epoch 205/300
 - 9s - loss: 0.0504 - acc: 0.9955 - mDice: 0.9015 - val_loss: 0.1270 - val_acc: 0.9941 - val_mDice: 0.5068

Epoch 00205: val_mDice did not improve from 0.51018
Epoch 206/300
 - 9s - loss: 0.0520 - acc: 0.9953 - mDice: 0.8985 - val_loss: 0.1268 - val_acc: 0.9942 - val_mDice: 0.5097

Epoch 00206: val_mDice did not improve from 0.51018
Epoch 207/300
 - 9s - loss: 0.0563 - acc: 0.9953 - mDice: 0.8898 - val_loss: 0.1274 - val_acc: 0.9941 - val_mDice: 0.5086

Epoch 00207: val_mDice did not improve from 0.51018
Epoch 208/300
 - 9s - loss: 0.0538 - acc: 0.9953 - mDice: 0.8948 - val_loss: 0.1289 - val_acc: 0.9941 - val_mDice: 0.5092

Epoch 00208: val_mDice did not improve from 0.51018

Epoch 00208: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 209/300
 - 9s - loss: 0.0546 - acc: 0.9953 - mDice: 0.8932 - val_loss: 0.1468 - val_acc: 0.9939 - val_mDice: 0.5089

Epoch 00209: val_mDice did not improve from 0.51018
Epoch 210/300
 - 8s - loss: 0.0528 - acc: 0.9954 - mDice: 0.8967 - val_loss: 0.1241 - val_acc: 0.9941 - val_mDice: 0.5095

Epoch 00210: val_mDice did not improve from 0.51018
Epoch 211/300
 - 8s - loss: 0.0534 - acc: 0.9954 - mDice: 0.8956 - val_loss: 0.1257 - val_acc: 0.9941 - val_mDice: 0.5085

Epoch 00211: val_mDice did not improve from 0.51018
Epoch 212/300
 - 8s - loss: 0.0532 - acc: 0.9954 - mDice: 0.8960 - val_loss: 0.1246 - val_acc: 0.9941 - val_mDice: 0.5090

Epoch 00212: val_mDice did not improve from 0.51018
Epoch 213/300
 - 8s - loss: 0.0542 - acc: 0.9954 - mDice: 0.8940 - val_loss: 0.1254 - val_acc: 0.9941 - val_mDice: 0.5093

Epoch 00213: val_mDice did not improve from 0.51018
Epoch 214/300
 - 8s - loss: 0.0561 - acc: 0.9953 - mDice: 0.8902 - val_loss: 0.1917 - val_acc: 0.9930 - val_mDice: 0.4997

Epoch 00214: val_mDice did not improve from 0.51018
Epoch 215/300
 - 9s - loss: 0.0559 - acc: 0.9953 - mDice: 0.8906 - val_loss: 0.1346 - val_acc: 0.9940 - val_mDice: 0.5101

Epoch 00215: val_mDice did not improve from 0.51018
Epoch 216/300
 - 9s - loss: 0.0513 - acc: 0.9954 - mDice: 0.8998 - val_loss: 0.1508 - val_acc: 0.9940 - val_mDice: 0.5101

Epoch 00216: val_mDice did not improve from 0.51018
Epoch 217/300
 - 9s - loss: 0.0528 - acc: 0.9954 - mDice: 0.8967 - val_loss: 0.1277 - val_acc: 0.9941 - val_mDice: 0.5106

Epoch 00217: val_mDice improved from 0.51018 to 0.51057, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 218/300
 - 9s - loss: 0.0517 - acc: 0.9954 - mDice: 0.8990 - val_loss: 0.1276 - val_acc: 0.9941 - val_mDice: 0.5096

Epoch 00218: val_mDice did not improve from 0.51057
Epoch 219/300
 - 8s - loss: 0.0527 - acc: 0.9954 - mDice: 0.8970 - val_loss: 0.1484 - val_acc: 0.9941 - val_mDice: 0.5103

Epoch 00219: val_mDice did not improve from 0.51057
Epoch 220/300
 - 9s - loss: 0.0508 - acc: 0.9954 - mDice: 0.9008 - val_loss: 0.1467 - val_acc: 0.9941 - val_mDice: 0.5107

Epoch 00220: val_mDice improved from 0.51057 to 0.51065, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_lr0.0001_CSFn2_Init_Main_WITH_NEW_CASES_wBiasCorrection_CV_d/1-THALAMUS/sd2/best_model_weights.h5
Epoch 221/300
 - 8s - loss: 0.0508 - acc: 0.9954 - mDice: 0.9009 - val_loss: 0.1295 - val_acc: 0.9942 - val_mDice: 0.5104

Epoch 00221: val_mDice did not improve from 0.51065
Epoch 222/300
 - 8s - loss: 0.0531 - acc: 0.9954 - mDice: 0.8961 - val_loss: 0.1480 - val_acc: 0.9940 - val_mDice: 0.5100

Epoch 00222: val_mDice did not improve from 0.51065
Epoch 223/300
 - 8s - loss: 0.0538 - acc: 0.9954 - mDice: 0.8947 - val_loss: 0.1512 - val_acc: 0.9941 - val_mDice: 0.5106

Epoch 00223: val_mDice did not improve from 0.51065

Epoch 00223: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 224/300
 - 8s - loss: 0.0515 - acc: 0.9954 - mDice: 0.8994 - val_loss: 0.1351 - val_acc: 0.9941 - val_mDice: 0.5104

Epoch 00224: val_mDice did not improve from 0.51065
Epoch 225/300
 - 8s - loss: 0.0521 - acc: 0.9955 - mDice: 0.8981 - val_loss: 0.1258 - val_acc: 0.9941 - val_mDice: 0.5105

Epoch 00225: val_mDice did not improve from 0.51065
Epoch 226/300
 - 8s - loss: 0.0515 - acc: 0.9954 - mDice: 0.8994 - val_loss: 0.1257 - val_acc: 0.9940 - val_mDice: 0.5100

Epoch 00226: val_mDice did not improve from 0.51065
Epoch 227/300
 - 9s - loss: 0.0514 - acc: 0.9955 - mDice: 0.8996 - val_loss: 0.1327 - val_acc: 0.9940 - val_mDice: 0.5103

Epoch 00227: val_mDice did not improve from 0.51065
Epoch 228/300
 - 8s - loss: 0.0521 - acc: 0.9955 - mDice: 0.8981 - val_loss: 0.1381 - val_acc: 0.9940 - val_mDice: 0.5104

Epoch 00228: val_mDice did not improve from 0.51065
Epoch 229/300
 - 9s - loss: 0.0548 - acc: 0.9955 - mDice: 0.8927 - val_loss: 0.1456 - val_acc: 0.9941 - val_mDice: 0.5105

Epoch 00229: val_mDice did not improve from 0.51065
Epoch 230/300
 - 8s - loss: 0.0524 - acc: 0.9955 - mDice: 0.8976 - val_loss: 0.1410 - val_acc: 0.9941 - val_mDice: 0.5100

Epoch 00230: val_mDice did not improve from 0.51065
Epoch 231/300
 - 9s - loss: 0.0539 - acc: 0.9954 - mDice: 0.8945 - val_loss: 0.1467 - val_acc: 0.9941 - val_mDice: 0.5102

Epoch 00231: val_mDice did not improve from 0.51065
Epoch 232/300
 - 9s - loss: 0.0528 - acc: 0.9954 - mDice: 0.8968 - val_loss: 0.1270 - val_acc: 0.9941 - val_mDice: 0.5097

Epoch 00232: val_mDice did not improve from 0.51065
Epoch 233/300
 - 8s - loss: 0.0518 - acc: 0.9954 - mDice: 0.8989 - val_loss: 0.1271 - val_acc: 0.9941 - val_mDice: 0.5097

Epoch 00233: val_mDice did not improve from 0.51065
Epoch 234/300
 - 8s - loss: 0.0554 - acc: 0.9955 - mDice: 0.8915 - val_loss: 0.1265 - val_acc: 0.9941 - val_mDice: 0.5094

Epoch 00234: val_mDice did not improve from 0.51065
Epoch 235/300
 - 8s - loss: 0.0523 - acc: 0.9955 - mDice: 0.8976 - val_loss: 0.1267 - val_acc: 0.9940 - val_mDice: 0.5101

Epoch 00235: val_mDice did not improve from 0.51065
Epoch 236/300
 - 9s - loss: 0.0517 - acc: 0.9954 - mDice: 0.8990 - val_loss: 0.1261 - val_acc: 0.9940 - val_mDice: 0.5098

Epoch 00236: val_mDice did not improve from 0.51065
Epoch 237/300
 - 9s - loss: 0.0534 - acc: 0.9955 - mDice: 0.8955 - val_loss: 0.1283 - val_acc: 0.9941 - val_mDice: 0.5088

Epoch 00237: val_mDice did not improve from 0.51065
Epoch 238/300
 - 9s - loss: 0.0499 - acc: 0.9955 - mDice: 0.9026 - val_loss: 0.1447 - val_acc: 0.9941 - val_mDice: 0.5094

Epoch 00238: val_mDice did not improve from 0.51065

Epoch 00238: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.
Epoch 239/300
 - 9s - loss: 0.0517 - acc: 0.9954 - mDice: 0.8990 - val_loss: 0.1469 - val_acc: 0.9941 - val_mDice: 0.5091

Epoch 00239: val_mDice did not improve from 0.51065
Epoch 240/300
 - 9s - loss: 0.0510 - acc: 0.9955 - mDice: 0.9003 - val_loss: 0.1481 - val_acc: 0.9941 - val_mDice: 0.5094

Epoch 00240: val_mDice did not improve from 0.51065
Epoch 241/300
 - 9s - loss: 0.0530 - acc: 0.9955 - mDice: 0.8962 - val_loss: 0.1396 - val_acc: 0.9941 - val_mDice: 0.5091

Epoch 00241: val_mDice did not improve from 0.51065
Epoch 242/300
 - 9s - loss: 0.0537 - acc: 0.9954 - mDice: 0.8950 - val_loss: 0.1484 - val_acc: 0.9941 - val_mDice: 0.5098

Epoch 00242: val_mDice did not improve from 0.51065
Epoch 243/300
 - 9s - loss: 0.0530 - acc: 0.9954 - mDice: 0.8964 - val_loss: 0.1479 - val_acc: 0.9941 - val_mDice: 0.5099

Epoch 00243: val_mDice did not improve from 0.51065
Epoch 244/300
 - 9s - loss: 0.0537 - acc: 0.9955 - mDice: 0.8949 - val_loss: 0.1483 - val_acc: 0.9941 - val_mDice: 0.5106

Epoch 00244: val_mDice did not improve from 0.51065
Epoch 245/300
 - 9s - loss: 0.0531 - acc: 0.9954 - mDice: 0.8961 - val_loss: 0.1295 - val_acc: 0.9942 - val_mDice: 0.5099

Epoch 00245: val_mDice did not improve from 0.51065
Epoch 246/300
 - 9s - loss: 0.0535 - acc: 0.9955 - mDice: 0.8953 - val_loss: 0.1398 - val_acc: 0.9941 - val_mDice: 0.5103

Epoch 00246: val_mDice did not improve from 0.51065
Epoch 247/300
 - 9s - loss: 0.0523 - acc: 0.9955 - mDice: 0.8977 - val_loss: 0.1475 - val_acc: 0.9941 - val_mDice: 0.5100

Epoch 00247: val_mDice did not improve from 0.51065
Epoch 248/300
 - 9s - loss: 0.0537 - acc: 0.9955 - mDice: 0.8949 - val_loss: 0.1487 - val_acc: 0.9941 - val_mDice: 0.5099

Epoch 00248: val_mDice did not improve from 0.51065
Epoch 249/300
 - 9s - loss: 0.0520 - acc: 0.9954 - mDice: 0.8983 - val_loss: 0.1430 - val_acc: 0.9941 - val_mDice: 0.5099

Epoch 00249: val_mDice did not improve from 0.51065
Epoch 250/300
 - 9s - loss: 0.0506 - acc: 0.9954 - mDice: 0.9012 - val_loss: 0.1294 - val_acc: 0.9941 - val_mDice: 0.5092

Epoch 00250: val_mDice did not improve from 0.51065
Epoch 251/300
 - 9s - loss: 0.0520 - acc: 0.9955 - mDice: 0.8984 - val_loss: 0.1479 - val_acc: 0.9941 - val_mDice: 0.5098

Epoch 00251: val_mDice did not improve from 0.51065
Epoch 252/300
 - 9s - loss: 0.0555 - acc: 0.9954 - mDice: 0.8913 - val_loss: 0.1438 - val_acc: 0.9941 - val_mDice: 0.5094

Epoch 00252: val_mDice did not improve from 0.51065
Epoch 253/300
 - 9s - loss: 0.0526 - acc: 0.9955 - mDice: 0.8972 - val_loss: 0.1481 - val_acc: 0.9941 - val_mDice: 0.5099

Epoch 00253: val_mDice did not improve from 0.51065

Epoch 00253: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.
Epoch 254/300
 - 8s - loss: 0.0494 - acc: 0.9955 - mDice: 0.9035 - val_loss: 0.1478 - val_acc: 0.9941 - val_mDice: 0.5099

Epoch 00254: val_mDice did not improve from 0.51065
Epoch 255/300
 - 8s - loss: 0.0517 - acc: 0.9955 - mDice: 0.8989 - val_loss: 0.1479 - val_acc: 0.9941 - val_mDice: 0.5099

Epoch 00255: val_mDice did not improve from 0.51065
Epoch 256/300
 - 8s - loss: 0.0498 - acc: 0.9955 - mDice: 0.9028 - val_loss: 0.1422 - val_acc: 0.9941 - val_mDice: 0.5098

Epoch 00256: val_mDice did not improve from 0.51065
Epoch 257/300
 - 8s - loss: 0.0498 - acc: 0.9955 - mDice: 0.9026 - val_loss: 0.1315 - val_acc: 0.9941 - val_mDice: 0.5097

Epoch 00257: val_mDice did not improve from 0.51065
Epoch 258/300
 - 9s - loss: 0.0552 - acc: 0.9955 - mDice: 0.8919 - val_loss: 0.1480 - val_acc: 0.9941 - val_mDice: 0.5098

Epoch 00258: val_mDice did not improve from 0.51065
Epoch 259/300
 - 9s - loss: 0.0518 - acc: 0.9955 - mDice: 0.8988 - val_loss: 0.1485 - val_acc: 0.9941 - val_mDice: 0.5101

Epoch 00259: val_mDice did not improve from 0.51065
Epoch 260/300
 - 8s - loss: 0.0514 - acc: 0.9955 - mDice: 0.8995 - val_loss: 0.1483 - val_acc: 0.9941 - val_mDice: 0.5100

Epoch 00260: val_mDice did not improve from 0.51065
Restoring model weights from the end of the best epoch
Epoch 00260: early stopping
{'val_loss': [0.34772059772043457, 0.30015069925256316, 0.2895010138050683, 0.2941462729847239, 0.27432025978041275, 0.2196870949419756, 0.2495765733502565, 0.2316699913973289, 0.21479379863388115, 0.2207471608454662, 0.2033811021716364, 0.24632258247584105, 0.22657238133251667, 0.21460416243081132, 0.18518163720446248, 0.21232775854127062, 0.207449457818462, 0.2322726937671823, 0.1927435652563168, 0.2307316282944333, 0.21355261052808455, 0.24481858868872927, 0.27150179499820354, 0.1936469074759272, 0.20332481847294875, 0.23388062904198323, 0.2063712215411567, 0.18270143334783853, 0.19034388628337653, 0.2392351357266307, 0.18862760379429785, 0.24094379152501783, 0.17035753466188908, 0.24102124235322397, 0.18270919120479975, 0.21146529573466508, 0.16485965825737484, 0.14907934636838974, 0.1842208961325307, 0.14673052355647087, 0.1600960324848852, 0.17623317025361523, 0.17358898224249, 0.15924831539873155, 0.1563926340050755, 0.15541173415558954, 0.19624754107527195, 0.17785050044016493, 0.14217482332981402, 0.16567394164420904, 0.17448098636082104, 0.19887299607357672, 0.1500935257082024, 0.16146911638638667, 0.1875289250165224, 0.17272903112274024, 0.1619758808144158, 0.18839493321795617, 0.20411170613501342, 0.15021014228583343, 0.1561045038003114, 0.1430931791903511, 0.17208300735200605, 0.12272790786359579, 0.24190742979126592, 0.1474587875869005, 0.16069219003040944, 0.17041452289108308, 0.1373174132839326, 0.20220881724549877, 0.20967914137027918, 0.12942840438336134, 0.13364115252249664, 0.13378581777215004, 0.14058856831322755, 0.16974806352969138, 0.20658954965972132, 0.16465725773765194, 0.14068777934317628, 0.117767337379196, 0.18036188010967547, 0.08616075863040262, 0.17267484229899221, 0.0855500333672089, 0.17955301693009754, 0.1508794175160508, 0.19907555804257432, 0.18172903749490937, 0.1805729901778602, 0.1426064257900561, 0.12600719090551138, 0.1175152430791528, 0.13164748518817848, 0.10619530826807022, 0.16332100737359254, 0.10302963267050443, 0.12244241270086458, 0.16954477837369328, 0.09980472603872899, 0.10388704746841423, 0.16171778933776002, 0.12600608500501803, 0.08001240198650668, 0.12024396186273906, 0.11551096095072647, 0.1602541182430521, 0.11993215064848622, 0.0980962890110189, 0.1170125447453991, 0.12039463318163349, 0.12141508693175931, 0.1149893027820414, 0.1385815338862519, 0.17681539752670833, 0.13040782069607126, 0.12083471938967705, 0.1193112078453264, 0.13402925945457914, 0.12352056789302057, 0.14554410211501584, 0.1287965747798162, 0.12869308429259446, 0.14486574234380836, 0.15370715601790336, 0.18812388683398884, 0.15270678191295556, 0.1090287908370937, 0.24117189450489898, 0.1339541988507394, 0.1509220753946612, 0.15417883620266953, 0.16310599049733532, 0.1749385693260739, 0.13883684163973217, 0.14010018789239467, 0.1685011180778665, 0.15343885775655508, 0.13651359667100252, 0.16099407851335504, 0.16357976288324402, 0.15092517477610418, 0.10250568558131495, 0.14754173442000343, 0.13835130625915143, 0.14263831178146985, 0.1440416459114321, 0.14645130947352417, 0.1520410183997404, 0.14293329492812196, 0.12028258567255351, 0.09423265807450779, 0.12931211169568763, 0.11108020149291523, 0.13154262256237767, 0.14829611099295079, 0.12598883036163547, 0.15433350472801155, 0.16376260274480428, 0.13628632124633558, 0.10778781221879105, 0.11001731809829512, 0.1424141165409838, 0.13679233568930818, 0.10148681393794474, 0.1381908931559132, 0.108423855906773, 0.18720318103629735, 0.12712152975220833, 0.10941123328502139, 0.12864317276304768, 0.13239087338649458, 0.11291875636144992, 0.11050826548448493, 0.0996841911767279, 0.1617810209431956, 0.11380649928844744, 0.16957396731501626, 0.13070729080467455, 0.2412163354155998, 0.12210181187237462, 0.13760635485091516, 0.14608017343186563, 0.14704789260342235, 0.1484313121726436, 0.14786571432505885, 0.14769167846609507, 0.14751005172729492, 0.14621491168415354, 0.14603534659310693, 0.1472216373489749, 0.12768227775250712, 0.1388709167799642, 0.14921347340268473, 0.12473279222725861, 0.14790978535048424, 0.1280193030834198, 0.14807917862649886, 0.1476673852772482, 0.14786413035565807, 0.12781110508066992, 0.12909325945281214, 0.14639003340515397, 0.12825518810460645, 0.15145051509382262, 0.1270121201392143, 0.126841300287314, 0.12738020943417663, 0.12888705622284644, 0.14675180220435705, 0.1240568541050438, 0.12570572719578782, 0.1246085002597782, 0.1253668089367209, 0.19169549436699, 0.13461152103639418, 0.15083601238626626, 0.12770507484674454, 0.12760919750097297, 0.14837260853739515, 0.14665519491198561, 0.1294902368418632, 0.14797414010090212, 0.1512338282961038, 0.1350628623378373, 0.12577408558178332, 0.1257410050640183, 0.1327066840303521, 0.138113988953973, 0.14558399027033198, 0.14102549304164225, 0.1466620717077486, 0.12698581264985184, 0.12711281894195464, 0.12652910885310942, 0.12669717298159677, 0.12611312791705132, 0.1283188048870333, 0.14466101219577174, 0.1469104971075731, 0.14813109135795985, 0.13961280161334622, 0.14836129942728626, 0.1479079301559156, 0.1482576114996787, 0.12954011520430928, 0.13975046130438004, 0.14747682054557146, 0.1486631468118679, 0.14298301983264186, 0.1294313540922538, 0.14790866202524594, 0.1437982089457012, 0.14811064353993825, 0.14784593964296003, 0.14790545638290145, 0.14216348739160645, 0.13147650238487027, 0.1480312586852139, 0.14850425984590285, 0.14828212213732542], 'val_acc': [0.9795238382873996, 0.9852167043474412, 0.9866393972789088, 0.984364653546964, 0.9882719514350737, 0.9881486378369793, 0.9888880409059986, 0.989263062996249, 0.9889745921377213, 0.9893121144944622, 0.989956856735291, 0.9891016524164907, 0.9892933681126563, 0.9895792858254525, 0.9905747473239899, 0.990265274480466, 0.9906445027839753, 0.9907709559605967, 0.9907456040382385, 0.9908735701634038, 0.9910285258966107, 0.9903476293529233, 0.9816803797598808, 0.9907492060334452, 0.9913495468997187, 0.9908672655301709, 0.9912812909291636, 0.9915513124677443, 0.9915765084085926, 0.990800062254552, 0.9917853239082521, 0.9903977342670963, 0.9917613215023472, 0.9917728778335356, 0.992229052609013, 0.9912916441117564, 0.9912785952610355, 0.9923748626343666, 0.9921828467519053, 0.9923154593956086, 0.9924654655398861, 0.9921505996777166, 0.991918679927626, 0.9926049738161026, 0.9923796603275884, 0.9923762104684307, 0.9917370232362901, 0.9918709798685966, 0.9927998401464955, 0.9924437146513693, 0.9923561092826628, 0.9918688780838444, 0.9919911401887094, 0.992750036860666, 0.9929187949145993, 0.9926699303811596, 0.9930637065441378, 0.9926585273877266, 0.9923277598234915, 0.9930428530900709, 0.9929025942279447, 0.9930674549072019, 0.9923178611263153, 0.9932132678166512, 0.9895653328107249, 0.9932240694761276, 0.9931439608335495, 0.9931928654832225, 0.993214015278124, 0.9927458364155984, 0.9921727971203865, 0.9931288089963698, 0.9933595239154754, 0.9931954128607627, 0.9929687481734061, 0.9929051464123111, 0.9928172367715067, 0.9929852514497696, 0.993303270830262, 0.9934966369502006, 0.9931535603057954, 0.9934703839882728, 0.9931436580034995, 0.9935642894237272, 0.9930004054500211, 0.9933130246016287, 0.9929315486261922, 0.9928344891917321, 0.9928278920631255, 0.993460480243929, 0.993399579438471, 0.9935642899044098, 0.9936968950975326, 0.9937492493660219, 0.9926324287249196, 0.9936982482191055, 0.9936122908707588, 0.993320075734969, 0.9936367405518409, 0.9937199981462571, 0.9929938037549296, 0.9934961831858081, 0.9937704006029714, 0.9932563179923642, 0.9937312506860302, 0.99339913144227, 0.993536988093007, 0.993671695791906, 0.9937301982314356, 0.9936026947632912, 0.9938223056735531, 0.9933383731592086, 0.9933574262165255, 0.992961847974408, 0.9938109036414854, 0.9937226918916549, 0.9933526261198905, 0.9938098495045016, 0.9938104556452844, 0.9936878981609498, 0.9937696553045704, 0.9938500566828635, 0.9933029735280622, 0.9938826097115394, 0.9928193383159176, 0.9938755568958098, 0.9936142419615099, 0.9917974702773555, 0.9938187034380052, 0.9937511997357491, 0.9937732544156813, 0.9934991809629625, 0.9930157058181301, 0.9938836578399904, 0.993758398678995, 0.9936421405403845, 0.9934003295436982, 0.9938691075771086, 0.9935609892972054, 0.9939495103974496, 0.993926859911411, 0.9939793665562907, 0.9937902047268806, 0.9939771145582199, 0.9940539194691566, 0.9940354689475028, 0.993948912908954, 0.9933863765289707, 0.993981157099047, 0.9939280647423959, 0.9940026142904835, 0.9939687172732046, 0.99410222590931, 0.9938833588554014, 0.9937169958026179, 0.9935765872078557, 0.9936827988393845, 0.993776255557614, 0.9939838638228755, 0.993966911829287, 0.9939391646654375, 0.9937535983420187, 0.9940596227684328, 0.9936043423029685, 0.9940161166652557, 0.9940593192173589, 0.9934012286605374, 0.9939198150269447, 0.99411257308337, 0.9939247588476827, 0.9937992014231221, 0.9940276681896179, 0.9940524190183608, 0.9941352230887259, 0.993978314101696, 0.9939592644091575, 0.9931759151720232, 0.9940318693557093, 0.9917107709953862, 0.9940339675353419, 0.9941283267351889, 0.9939706589906446, 0.9939348135263689, 0.9938442086981188, 0.9938619045480606, 0.9939751658708819, 0.9941397249217956, 0.9940962241061272, 0.9939169549653607, 0.993698698138037, 0.9941278732111377, 0.9941713718637344, 0.9940201659356395, 0.9940761219589941, 0.994163575431993, 0.9941094217281188, 0.9941746755953758, 0.9940054697855827, 0.9941712269379247, 0.9939780180012027, 0.9939862708410909, 0.9941328235210911, 0.9940969742113545, 0.9939400616192049, 0.9941314747256618, 0.9941641789290213, 0.9941395711033575, 0.9941313281174629, 0.9938850100001981, 0.9940669688005601, 0.9940696704772211, 0.9941370251678652, 0.9940881214795574, 0.992952549649823, 0.9940291698421201, 0.9940029192836054, 0.9940998244189447, 0.9941347731697944, 0.9940603692685405, 0.9941364221515194, 0.9941832262181467, 0.994043871520027, 0.9941158734502331, 0.9941002726554871, 0.9940902177364596, 0.9940371148047908, 0.9940230187869841, 0.9940366694523443, 0.9940750743112257, 0.9941026693390261, 0.9940603692685405, 0.9941320736562053, 0.9940978702037565, 0.9941364274390282, 0.9940158169596426, 0.9940357715372117, 0.9941337269640738, 0.9940905215278748, 0.994115874171257, 0.9940669700022666, 0.9941478241835872, 0.9940699718652233, 0.9940954713571456, 0.9940636674723318, 0.9941539747580406, 0.9941158756133048, 0.9940941201583031, 0.9940866203077378, 0.9940884216658531, 0.9941406283647783, 0.9940966692182326, 0.9941365745279097, 0.9940519700607946, 0.9940609696411318, 0.9940989245810816, 0.9941077765918547, 0.9941143725187548, 0.9941154228102776, 0.9941070228815079, 0.9940720722079277], 'val_mDice': [0.3154189627917028, 0.38269628770649516, 0.38812033020921305, 0.3994223878027932, 0.4191003481466924, 0.4076276386096593, 0.4266766173344466, 0.4291218725003062, 0.42071592633522326, 0.4365126346989024, 0.4419064753118061, 0.43650291349378323, 0.4371539163853853, 0.4220568230413201, 0.44491172527834294, 0.4333129163170534, 0.44301949621688935, 0.44424160323556394, 0.44675888103102485, 0.44286497688341525, 0.44587531818016884, 0.44968883056313763, 0.4015454424605254, 0.4357141131655343, 0.4570373207811386, 0.4575277774805023, 0.4454357765855328, 0.4506301650056435, 0.4485684181112916, 0.4568923324827225, 0.4528764331893575, 0.45465335550327457, 0.4470942115351077, 0.45663985333615736, 0.472315471138685, 0.46622569883061993, 0.4363569566198895, 0.4626878876719744, 0.4612121025040265, 0.45675959137658917, 0.46762642120161363, 0.47245067874750785, 0.4700765806821085, 0.46486141100045175, 0.4704903042124164, 0.4727336644164978, 0.4692150237819841, 0.470575233620982, 0.4666052458267058, 0.4757790009100591, 0.4721817993108303, 0.4708128650822947, 0.4462511744350195, 0.4678257122155159, 0.4698360149538325, 0.46190438431597525, 0.47597740145940937, 0.47575104440892896, 0.47447913656792334, 0.4787743458103749, 0.4787542040069257, 0.46875753289749544, 0.4785784213773666, 0.4788849280486184, 0.44976155153445657, 0.475734292920078, 0.46973596417134805, 0.47860920008632446, 0.4811917912335165, 0.47997363512554475, 0.4761609687439857, 0.47774089800734676, 0.4807177690729018, 0.48071062673003445, 0.4803868106776668, 0.4799128059417971, 0.48095425771128747, 0.48194835743596476, 0.48348973595326944, 0.47928104717885295, 0.4836169023427271, 0.47786917657621447, 0.4839171979936861, 0.48157829611051467, 0.4819457407199567, 0.4850891382703871, 0.48373618374991273, 0.484668106111067, 0.4827906666329509, 0.48509958771001926, 0.4859438430397741, 0.4851935209045488, 0.4851290932105434, 0.4853980020890313, 0.4855335955958713, 0.4847568399723499, 0.48650555312633614, 0.48680618781814233, 0.4872169495830613, 0.48106500638588784, 0.4869660391517344, 0.48799904724283216, 0.4845578848354341, 0.48661932791608176, 0.48400436077387154, 0.48877255408555487, 0.4856955215110808, 0.4862848711453517, 0.48763141517032355, 0.4920765731461166, 0.48496958469190904, 0.4931497179542578, 0.4925122142521427, 0.49171938712994057, 0.49030597470330084, 0.4903565405894614, 0.49472410945341955, 0.49435198141802705, 0.4864303297026415, 0.4887625704249067, 0.4875737997973638, 0.49104634154167387, 0.4939344433707095, 0.4946395879491202, 0.4926204947274058, 0.49793647346837866, 0.4837607699746807, 0.4816596229530631, 0.49715840821004204, 0.49734900953368316, 0.5005476898663948, 0.4992838559912578, 0.4960325468451746, 0.49886184669430217, 0.49897619523108006, 0.49828987072912917, 0.49927057367899724, 0.49355222963758055, 0.499808840874222, 0.49522419760544456, 0.49647896430425104, 0.5001340722364764, 0.5029203929007053, 0.5012504110533383, 0.5011348186000701, 0.5008048395956716, 0.501189848948871, 0.49926411060075604, 0.5028972561441122, 0.5001284185075953, 0.49949448615793257, 0.5037081818426808, 0.5028790782295889, 0.5042638293197078, 0.5057993999892666, 0.5034555723109553, 0.5052464719741575, 0.5044444560283615, 0.5055050305541484, 0.49843628860769734, 0.4973794465824481, 0.5050908693022305, 0.5044455486800401, 0.4936735584370552, 0.5074341000328141, 0.5076168189125676, 0.5042147660447706, 0.5062219470499023, 0.5066037580611245, 0.5088676782625337, 0.5081910171335743, 0.5047254751045858, 0.5079236921043165, 0.5067594884143721, 0.5080575453898599, 0.501007160111781, 0.5024213536131766, 0.5087565540546372, 0.4815918741927993, 0.5087838664410576, 0.5085766784125759, 0.5095123457572153, 0.5081931832096269, 0.5054416014782844, 0.5084000029150517, 0.5078284172040801, 0.5085783950503795, 0.5090668112520249, 0.5097484198068419, 0.5078573999626022, 0.5078279701692443, 0.5086083535946184, 0.5101789708339399, 0.5041478502173578, 0.5084839180832909, 0.5075088984543278, 0.5069746900229685, 0.5089110370364881, 0.5087621948651729, 0.5078296886096078, 0.5083853396917543, 0.5094385591726149, 0.5093285219082909, 0.5092065799380502, 0.506844499538983, 0.5096577318204988, 0.5085587471483215, 0.5092043339485123, 0.5089475244524018, 0.5094811524594983, 0.5084774925583794, 0.5089823607235185, 0.5093340701874225, 0.4997264689976169, 0.5100703545876087, 0.5101400054991245, 0.5105719312785133, 0.5096492267424061, 0.5103283958329309, 0.5106511015084482, 0.5104147348432772, 0.5099739548900435, 0.5106441123831656, 0.5104463715707103, 0.5105209098227562, 0.5100287052412187, 0.5103243292579728, 0.5103548039351741, 0.5104624576145603, 0.5100356973707676, 0.5102403990684017, 0.5096710755700066, 0.509669273611038, 0.5094465889757679, 0.510059465323725, 0.5098468251526356, 0.5087998575500904, 0.5094128781028332, 0.509132337305815, 0.5094161323241649, 0.5091157904075038, 0.509813963285377, 0.5098974897015479, 0.5106451891122326, 0.5099147942758375, 0.5103170409077599, 0.5100306177571896, 0.5098763024134021, 0.5098765217248471, 0.5091753211473266, 0.5098238232876023, 0.5093841729385238, 0.5098750658573643, 0.5099325663139743, 0.5098727285381286, 0.5098315730931298, 0.5097000006466142, 0.5097610323179153, 0.5100785255912812, 0.51003503162534], 'loss': [0.36750954210082903, 0.22144187563895895, 0.18369224506719664, 0.1609398605269766, 0.14599765105018606, 0.14251152267396344, 0.13615945292571222, 0.12779935441809037, 0.12882554360493778, 0.12346878208855326, 0.12095128902115847, 0.11485038705716619, 0.113822667804515, 0.1079198509614668, 0.10747444358658208, 0.1069692883398395, 0.10426627457159647, 0.10108815654712414, 0.10315211719711692, 0.10305462196495964, 0.10278481202389984, 0.10063278132613102, 0.09533205216102963, 0.0947163040279347, 0.09890604026273524, 0.09144655635972498, 0.09812646342766874, 0.0959235920515811, 0.09224889425202076, 0.0879650623068329, 0.08632273007946208, 0.08600290726534866, 0.0878613709283856, 0.08843280869932067, 0.09133467115528468, 0.08820999846902004, 0.09019998636054509, 0.08327327487400478, 0.08203934605767063, 0.08529778988004513, 0.08165041746024085, 0.08322338382260887, 0.0828205838835019, 0.08122533913446311, 0.08357273780595852, 0.07816405530372544, 0.08082915323613724, 0.0796941008305507, 0.08381928536390078, 0.07865044576786215, 0.07573749425337296, 0.0775161400641724, 0.07677415809463588, 0.08237344600575697, 0.07711565496314475, 0.07416746700065933, 0.07876659612536217, 0.07621213220632993, 0.07932557724971606, 0.07625167293070893, 0.07567805980349411, 0.07746611199673259, 0.07536019561684551, 0.07884077948601956, 0.08075151987458243, 0.07748327065274768, 0.074033386805403, 0.07345872061836911, 0.07158287387112457, 0.0747328580367686, 0.07456656982128153, 0.0691523358234708, 0.07079377807743434, 0.06874318467092144, 0.0691236601070627, 0.07605917556305762, 0.07335985471040775, 0.0714827783285825, 0.07625109563288976, 0.06913824438089122, 0.07003116390857231, 0.06919803364190162, 0.06923500105925237, 0.06966987005659847, 0.06923438088838445, 0.0727896910465112, 0.06754052699429688, 0.06756284157648354, 0.06757995611990784, 0.06765597392280115, 0.06565442998528694, 0.06977282545363256, 0.06417865007582632, 0.0652779541114433, 0.07141280459483322, 0.06505544170888322, 0.06349770440434016, 0.06375017960333156, 0.06584189869530085, 0.06822637076276929, 0.06618532533971198, 0.06620722919809158, 0.06787231247305657, 0.06348799325005952, 0.06337518546648088, 0.06941947757700191, 0.06381588639357151, 0.06198443220034197, 0.0636948086387996, 0.06362726193723325, 0.06574766393850869, 0.0666534737969838, 0.06338852077953575, 0.06345050494515903, 0.061049370491653945, 0.0599083812306449, 0.061144909690943376, 0.06359303926134934, 0.06198069813106482, 0.06478719860226467, 0.06087084335803133, 0.060195367981012764, 0.06019614969219703, 0.06234321939177505, 0.05931918286609024, 0.06385813047470759, 0.06081894646994614, 0.06383729683869498, 0.06042950731448354, 0.058865061173072226, 0.06178662674051705, 0.061075388301179465, 0.061329985667213344, 0.06033505831684608, 0.05959805090937788, 0.05899581681885887, 0.059579921083288245, 0.06234574236184554, 0.057004145607041264, 0.059095365336841246, 0.060128999222014444, 0.05981056340656326, 0.05830055538322788, 0.057898523449258, 0.05791979655845576, 0.05846121974764797, 0.057318509993413834, 0.06233231414089476, 0.05664902226194144, 0.05740977440764666, 0.05722615711022219, 0.057853423658765064, 0.057914706305797856, 0.05844497620241942, 0.05821388647999874, 0.05779643188862127, 0.058968938935990124, 0.057834214613632424, 0.055060829072035564, 0.057951959813003676, 0.059146901753663876, 0.057470272366817184, 0.05679494185183257, 0.057679834887252084, 0.05489366366064257, 0.05660309786055866, 0.05408353524223707, 0.054275259624369174, 0.055447671195902, 0.055193491591075096, 0.05594410784164922, 0.05706452757351158, 0.0565993803273942, 0.055505595026125704, 0.056105517046431665, 0.05892835641874042, 0.05767421131163889, 0.05685791931268923, 0.053418848794631185, 0.05666150046944547, 0.05488248937138505, 0.05328562910811277, 0.054129431653961085, 0.053347889056288206, 0.052472452766922306, 0.055281043034902125, 0.053242821155308116, 0.05462986675574798, 0.05494576196320084, 0.056857570765909866, 0.055982389724460756, 0.05237009406942801, 0.054240481256016965, 0.05221880564536093, 0.05232719140494943, 0.05264299252716359, 0.052866579817033764, 0.05479085524485946, 0.052401036290661805, 0.0520188970736755, 0.05377830788004448, 0.054300684266383265, 0.05218387915395738, 0.05581666749541647, 0.050431317545931185, 0.051981098483414895, 0.05631506673160876, 0.05379935367322779, 0.054623819021291514, 0.0528056565394484, 0.05335869501940751, 0.05317317737520772, 0.05416218262025837, 0.056123187197911005, 0.05589437764637799, 0.05129113724953942, 0.05282324651166237, 0.05166752034877124, 0.05269969457768228, 0.05076514168480854, 0.05076203608111249, 0.053110841899381624, 0.0538276014721813, 0.05147248881518592, 0.052099106163586185, 0.05148686133613882, 0.0513564289842394, 0.05210316975556034, 0.0547929925529745, 0.052356038871235126, 0.053901257858648854, 0.05277589358918071, 0.05175170406352642, 0.05539997564445455, 0.052348352191202256, 0.05166141922515137, 0.05343667035239327, 0.04988973156355219, 0.05167433306725877, 0.051004850000729504, 0.053041587424335124, 0.05369448818190864, 0.0529707984851099, 0.053731721785357116, 0.053105866830833764, 0.05354487907516864, 0.05230123591017851, 0.05370950558459965, 0.05201479446944838, 0.05056638965026637, 0.051955126319247195, 0.055525287532635796, 0.05257832055903647, 0.04940486442104435, 0.05169385250437878, 0.049786795319441746, 0.04984535056526773, 0.05522128456251637, 0.05175174387104112, 0.051393686854277754], 'acc': [0.9512137682862984, 0.9780262385583308, 0.9833311157419913, 0.9854002496850726, 0.9866039048153941, 0.9872458598126097, 0.9877413606529941, 0.9884282625993945, 0.9887129257104904, 0.9890167139865986, 0.9893278656932806, 0.9895811580303672, 0.9901510633808129, 0.9902479188315699, 0.9902702798143909, 0.9904334203572805, 0.9906558804591639, 0.9906633338831548, 0.9910166360611139, 0.9911418395449024, 0.9911957893545029, 0.9912662907732905, 0.9914954949434698, 0.9914421215608992, 0.9915845419369642, 0.9917960435987301, 0.9917203974496345, 0.9918564290039299, 0.9919406819997547, 0.9921688681993729, 0.9923531702038214, 0.9923015257469159, 0.9923462488854579, 0.9923462027513917, 0.9923233995141909, 0.9924815692003098, 0.9925059709224803, 0.9926088172620297, 0.992630822713985, 0.9927098415547634, 0.9926436904575687, 0.9928572753674229, 0.9927900157375711, 0.992903684247585, 0.9927473341558544, 0.9931236585478876, 0.9930315102291164, 0.9929480501492249, 0.9930745009943781, 0.9932338696330804, 0.9932756189725622, 0.9932556980126801, 0.9932619534081907, 0.9931978879759691, 0.9933949249019635, 0.993351931719817, 0.9932587627648027, 0.9933753606103045, 0.993320784791993, 0.9934536669321122, 0.9934321933014448, 0.9935786519010791, 0.9935566894554566, 0.9933557928354881, 0.9933964336067704, 0.9933699030355818, 0.9935592611275333, 0.9937090507631297, 0.9938162417019416, 0.9937747574733992, 0.9937973409873357, 0.9937937013009651, 0.9937892208585586, 0.9938729433126228, 0.9939159773685566, 0.9936882414203637, 0.9937867362298778, 0.9939256513765616, 0.9939619882091142, 0.993892774634512, 0.9939410468973287, 0.9939979693330891, 0.9939161582439516, 0.994036040138331, 0.9940495714234254, 0.9940548961761102, 0.9941178517250625, 0.9941095113398848, 0.9941327618784896, 0.9941058283628419, 0.9941734907949831, 0.9942085400676898, 0.9941643081965301, 0.9942734079767567, 0.9941811220377204, 0.9941938542224995, 0.9941815215345643, 0.994341290629756, 0.9942854302016197, 0.9942212329070058, 0.9943654700340653, 0.9942569458989352, 0.9943688412685088, 0.9943554867358881, 0.9944242103226354, 0.9943755420816748, 0.9944262507421031, 0.9944612580036648, 0.994479803187405, 0.994439164797465, 0.9944576670459572, 0.9944814470955923, 0.994557935529048, 0.9945023897224476, 0.9945333575875402, 0.9945908124843522, 0.9945412550785745, 0.9946057196877581, 0.9945944086312537, 0.9946144186249075, 0.9946022147427163, 0.9947027949328642, 0.9947094524198156, 0.9946843399202532, 0.9946985390474914, 0.9946477811963486, 0.994681986407574, 0.994669035707973, 0.994616414580803, 0.9947620755373614, 0.99468948873794, 0.9947758721466496, 0.9947005791470773, 0.9947996077682549, 0.9947081236663003, 0.9947764501020037, 0.9948318197603516, 0.9948412244634111, 0.9948273355504481, 0.9948153150671639, 0.9948201050485396, 0.9948509894244786, 0.9948415345710591, 0.9948776075366571, 0.9949056488741134, 0.994909107507377, 0.9948846165737227, 0.9949111934566895, 0.9948817362867798, 0.9949685608835965, 0.9950852936481391, 0.9949033405003124, 0.9949826692710266, 0.995039639155923, 0.9949369254962418, 0.9949934500701099, 0.9950733580401631, 0.9949956699788179, 0.9950448290251094, 0.9950360007490796, 0.9950732719919549, 0.9950080053679774, 0.9950450961619669, 0.9949802290703758, 0.9950826760197083, 0.9950513046770204, 0.9951683933989377, 0.9951478078312723, 0.9951298806591978, 0.9950883546683074, 0.9951361843568624, 0.9951170608268014, 0.9950906161261088, 0.9951277097279498, 0.9951829005723634, 0.9952485665154728, 0.9951855629487057, 0.9951473181277567, 0.9952205262087468, 0.9951628031088045, 0.9952171097287978, 0.9952829966746986, 0.9952330822452734, 0.9953033602799272, 0.9953199553930411, 0.995233392068582, 0.9953033597112484, 0.9952226127267382, 0.9952387146887214, 0.9952892056518036, 0.9953349042877102, 0.9953247907620921, 0.9953559796655185, 0.995278070601382, 0.9953269195400254, 0.995326433710634, 0.9953523423249478, 0.9953232816663183, 0.9953620160841558, 0.9953586848704816, 0.9953404977411192, 0.9953154726048347, 0.9953579323307418, 0.9954057636187058, 0.9954708930844698, 0.9953135625905201, 0.995349414659954, 0.9953470624268019, 0.9953228398384476, 0.9954306526985919, 0.995435531855909, 0.9954282997901339, 0.9954052300558391, 0.9953378776958036, 0.995340052110209, 0.9953661873785598, 0.9954295862837071, 0.9954079337680205, 0.9954041630367332, 0.9954227971476178, 0.9953778093246456, 0.9954149442623255, 0.9954089106515397, 0.9954125931309886, 0.995475150853591, 0.9954298080328894, 0.9954634838041694, 0.9954800338139475, 0.9954791006120671, 0.9954634407622941, 0.9954405005445919, 0.9954362867059066, 0.9953950685479489, 0.9954505290527321, 0.9955089174002214, 0.9954437394545913, 0.9954677863212021, 0.9954758176294621, 0.9954241277848814, 0.995450306059565, 0.9954773697315686, 0.9953822459077011, 0.9954355758929722, 0.9954677862856597, 0.9954478653257776, 0.9954527007659134, 0.9954608643630749, 0.9954920546881983, 0.99544320919717, 0.9954129950091749, 0.9955063440221082, 0.9954242190933682, 0.9954752392475991, 0.9955311006353808, 0.995487488553008, 0.9954723142127445, 0.995481143732759, 0.9954676549208612, 0.9955046579961486, 0.995491480144917], 'mDice': [0.29147501521149677, 0.5688501176517068, 0.6414573787133067, 0.6858442222007973, 0.7150775567008116, 0.721711067977091, 0.7341493916390691, 0.7504986306706851, 0.7482952245588308, 0.7588355312356928, 0.7636895719474879, 0.7757644819303432, 0.7775220450482592, 0.7892700881210195, 0.790153644485849, 0.7910711893597456, 0.7963553244469062, 0.8027104611686816, 0.7983941156832434, 0.7985233951517707, 0.7990432725325183, 0.8033102117057009, 0.8137810666360668, 0.815043627895908, 0.806587078419198, 0.8213953039585016, 0.8080779622371664, 0.8124040248462254, 0.8197122689294332, 0.8281545769834205, 0.8313460441736075, 0.8320063517146261, 0.8282640397051082, 0.8271175853982479, 0.821331476180697, 0.82749692748257, 0.823503748226703, 0.8373017007208468, 0.8397578045805792, 0.8332032987988983, 0.8405302997565227, 0.8372724157187325, 0.8381122762007449, 0.8412400018834755, 0.8366286763138336, 0.8472477842396036, 0.8419678876210909, 0.8442790634445868, 0.8359596856875912, 0.8462132460653746, 0.8520136297140082, 0.8484672902415178, 0.8499498121711422, 0.8387893299729556, 0.8491978293987847, 0.8551205974572034, 0.8459688305641543, 0.8510145277015922, 0.844810333034078, 0.8508918844096776, 0.8520513501775784, 0.8484028089451662, 0.8526171640974465, 0.8457613854303059, 0.8419213569725846, 0.8484729437083095, 0.8552735893230177, 0.8563455888778237, 0.8600417591662513, 0.8537626885273101, 0.8540846222059743, 0.8649110798332473, 0.8616289981220475, 0.8656926107818909, 0.8649041304363689, 0.8511517260090451, 0.8564994310828, 0.8601823605521491, 0.8506270675660601, 0.8648871254124809, 0.863073334533257, 0.8647115521830466, 0.8646773395967683, 0.8637497234571954, 0.864609936873118, 0.8574976051713856, 0.8679615080747565, 0.8679205102798266, 0.8678742401928295, 0.8677376500893436, 0.8717048919549213, 0.8634499962228972, 0.8746596619211355, 0.8724051507441999, 0.8601846075807145, 0.872889097524528, 0.8760136041743598, 0.8754259330144437, 0.8712711451518514, 0.8665321622850217, 0.8705394955426365, 0.8705516537883556, 0.867167629616987, 0.8759402128366324, 0.8761281055499914, 0.8640686775789279, 0.8752463410920022, 0.8788915659674734, 0.8754621776144065, 0.8756165128061298, 0.8713674954736809, 0.8695445977724515, 0.8760360282449831, 0.8759376717665825, 0.8807231393966493, 0.8829758049265999, 0.8805271079658253, 0.8756009731963765, 0.87883103636666, 0.8732062999079128, 0.8810448976379104, 0.8823473701963271, 0.8823395405913224, 0.8780559917326548, 0.8841009398763494, 0.8750478206252369, 0.8811075207230301, 0.8750804360949289, 0.8819220520658158, 0.8849702971850255, 0.8791663292695456, 0.8805447552413121, 0.8800742818119274, 0.8820140017024377, 0.883535740698244, 0.8847025937101425, 0.8835043238967959, 0.877971029132338, 0.8886596614147271, 0.8844838013020597, 0.8824150545936862, 0.8830333837065302, 0.8860569229848199, 0.8868428358167285, 0.886790858682736, 0.8857028260427212, 0.8880039320933228, 0.8779617518133256, 0.8893423511929361, 0.8877762440207748, 0.8880854091192187, 0.8869186503658283, 0.8867589463843571, 0.8856711737371727, 0.8861823810945327, 0.8869910643224426, 0.8846035673281375, 0.8869113017564454, 0.8924347100047463, 0.8866546386114245, 0.8842466571676496, 0.8876325788068572, 0.8889660951134415, 0.8872284099113109, 0.8927479466300104, 0.8893483258743832, 0.8943247026677777, 0.8939495645752249, 0.891617434751298, 0.8921478895396083, 0.8906190417036103, 0.8883893917413005, 0.8893306853519569, 0.8914988195533616, 0.8902723690407172, 0.8845943053634521, 0.8871334061852935, 0.8887876173038176, 0.8956255494091009, 0.8891689395648635, 0.8927016113937505, 0.895860622664186, 0.8941970133113235, 0.8957256464355391, 0.8974692578392507, 0.89189299150244, 0.8959361267146712, 0.893205538774148, 0.8925636619702648, 0.888715632841853, 0.8904390016622891, 0.8976669990028309, 0.8939143692586986, 0.8979960187034516, 0.8977553561534211, 0.8971224346897327, 0.8966632841880221, 0.8928297289297846, 0.8975892570947136, 0.8983549162541107, 0.8948465500051377, 0.8938140157225876, 0.8980244207609673, 0.890735192407649, 0.9014736665240625, 0.8984511698650048, 0.8897682209774078, 0.8947984139408892, 0.8931629326845395, 0.8967448817382772, 0.8956382047511597, 0.8960102137953452, 0.8940448157475971, 0.8901572458948063, 0.8906140273784955, 0.8998044874601302, 0.8967117403473396, 0.8990326739167911, 0.8969684846001718, 0.900828555857204, 0.9008568944004084, 0.8961415784516786, 0.8947121370883662, 0.8994179836823675, 0.8981340664372817, 0.8993822690199441, 0.8996265939324117, 0.8981234341699876, 0.892745861782513, 0.8976271636939575, 0.8945485123154943, 0.8968003961609173, 0.8988672695703, 0.8915459335580432, 0.8976185552122767, 0.8990252102921247, 0.8954634574743418, 0.9025529646489616, 0.8990096101180812, 0.9003374791600268, 0.8962493202625177, 0.8949913564151998, 0.8964095752581857, 0.8948705076247223, 0.8961357507381541, 0.8952539431316624, 0.8977389354469809, 0.8949043721262727, 0.8983158573339436, 0.9012340617876502, 0.8984075642868956, 0.8913103867017874, 0.897176379701106, 0.903492047911822, 0.8989378433179201, 0.9027606695268434, 0.9026362912315091, 0.8918943166128567, 0.8988113092679528, 0.8995363771311214], 'lr': [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 2.5e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 1.25e-05, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 6.25e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06, 3.125e-06]}
predicting test subjects:   0%|          | 0/8 [00:00<?, ?it/s]predicting test subjects:  12%|█▎        | 1/8 [00:00<00:05,  1.25it/s]predicting test subjects:  25%|██▌       | 2/8 [00:01<00:03,  1.51it/s]predicting test subjects:  38%|███▊      | 3/8 [00:01<00:02,  1.79it/s]predicting test subjects:  50%|█████     | 4/8 [00:01<00:01,  2.08it/s]predicting test subjects:  62%|██████▎   | 5/8 [00:01<00:01,  2.63it/s]predicting test subjects:  75%|███████▌  | 6/8 [00:02<00:00,  3.22it/s]predicting test subjects:  88%|████████▊ | 7/8 [00:02<00:00,  3.34it/s]predicting test subjects: 100%|██████████| 8/8 [00:02<00:00,  3.93it/s]predicting test subjects: 100%|██████████| 8/8 [00:02<00:00,  3.24it/s]
predicting train subjects:   0%|          | 0/26 [00:00<?, ?it/s]predicting train subjects:   4%|▍         | 1/26 [00:00<00:03,  6.64it/s]predicting train subjects:   8%|▊         | 2/26 [00:00<00:03,  6.34it/s]predicting train subjects:  12%|█▏        | 3/26 [00:00<00:03,  6.44it/s]predicting train subjects:  15%|█▌        | 4/26 [00:00<00:03,  5.59it/s]predicting train subjects:  19%|█▉        | 5/26 [00:00<00:03,  5.90it/s]predicting train subjects:  23%|██▎       | 6/26 [00:01<00:03,  5.55it/s]predicting train subjects:  27%|██▋       | 7/26 [00:01<00:03,  5.77it/s]predicting train subjects:  31%|███       | 8/26 [00:01<00:03,  5.14it/s]predicting train subjects:  35%|███▍      | 9/26 [00:01<00:03,  5.46it/s]predicting train subjects:  38%|███▊      | 10/26 [00:01<00:02,  5.50it/s]predicting train subjects:  42%|████▏     | 11/26 [00:01<00:02,  5.89it/s]predicting train subjects:  46%|████▌     | 12/26 [00:02<00:02,  6.12it/s]predicting train subjects:  50%|█████     | 13/26 [00:02<00:02,  5.98it/s]predicting train subjects:  54%|█████▍    | 14/26 [00:02<00:02,  4.65it/s]predicting train subjects:  58%|█████▊    | 15/26 [00:02<00:02,  4.99it/s]predicting train subjects:  62%|██████▏   | 16/26 [00:02<00:01,  5.43it/s]predicting train subjects:  65%|██████▌   | 17/26 [00:03<00:01,  5.67it/s]predicting train subjects:  69%|██████▉   | 18/26 [00:03<00:01,  4.19it/s]predicting train subjects:  73%|███████▎  | 19/26 [00:03<00:01,  4.69it/s]predicting train subjects:  77%|███████▋  | 20/26 [00:03<00:01,  3.97it/s]predicting train subjects:  81%|████████  | 21/26 [00:04<00:01,  3.87it/s]predicting train subjects:  85%|████████▍ | 22/26 [00:04<00:00,  4.34it/s]predicting train subjects:  88%|████████▊ | 23/26 [00:04<00:00,  4.82it/s]predicting train subjects:  92%|█████████▏| 24/26 [00:04<00:00,  5.06it/s]predicting train subjects:  96%|█████████▌| 25/26 [00:04<00:00,  5.38it/s]predicting train subjects: 100%|██████████| 26/26 [00:05<00:00,  5.68it/s]predicting train subjects: 100%|██████████| 26/26 [00:05<00:00,  5.18it/s]
predicting test subjects sagittal:   0%|          | 0/8 [00:00<?, ?it/s]predicting test subjects sagittal:  12%|█▎        | 1/8 [00:00<00:01,  5.49it/s]predicting test subjects sagittal:  25%|██▌       | 2/8 [00:00<00:01,  5.77it/s]predicting test subjects sagittal:  38%|███▊      | 3/8 [00:00<00:00,  6.12it/s]predicting test subjects sagittal:  50%|█████     | 4/8 [00:00<00:00,  6.45it/s]predicting test subjects sagittal:  62%|██████▎   | 5/8 [00:00<00:00,  6.50it/s]predicting test subjects sagittal:  75%|███████▌  | 6/8 [00:00<00:00,  6.57it/s]predicting test subjects sagittal:  88%|████████▊ | 7/8 [00:01<00:00,  6.82it/s]predicting test subjects sagittal: 100%|██████████| 8/8 [00:01<00:00,  6.82it/s]predicting test subjects sagittal: 100%|██████████| 8/8 [00:01<00:00,  6.72it/s]
predicting train subjects sagittal:   0%|          | 0/26 [00:00<?, ?it/s]predicting train subjects sagittal:   4%|▍         | 1/26 [00:00<00:03,  6.77it/s]predicting train subjects sagittal:   8%|▊         | 2/26 [00:00<00:03,  6.35it/s]predicting train subjects sagittal:  12%|█▏        | 3/26 [00:00<00:03,  6.37it/s]predicting train subjects sagittal:  15%|█▌        | 4/26 [00:00<00:03,  6.27it/s]predicting train subjects sagittal:  19%|█▉        | 5/26 [00:00<00:03,  6.43it/s]predicting train subjects sagittal:  23%|██▎       | 6/26 [00:00<00:03,  6.38it/s]predicting train subjects sagittal:  27%|██▋       | 7/26 [00:01<00:02,  6.37it/s]predicting train subjects sagittal:  31%|███       | 8/26 [00:01<00:02,  6.12it/s]predicting train subjects sagittal:  35%|███▍      | 9/26 [00:01<00:02,  6.24it/s]predicting train subjects sagittal:  38%|███▊      | 10/26 [00:01<00:02,  5.86it/s]predicting train subjects sagittal:  42%|████▏     | 11/26 [00:01<00:02,  6.15it/s]predicting train subjects sagittal:  46%|████▌     | 12/26 [00:01<00:02,  6.38it/s]predicting train subjects sagittal:  50%|█████     | 13/26 [00:02<00:02,  6.20it/s]predicting train subjects sagittal:  54%|█████▍    | 14/26 [00:02<00:01,  6.39it/s]predicting train subjects sagittal:  58%|█████▊    | 15/26 [00:02<00:01,  6.45it/s]predicting train subjects sagittal:  62%|██████▏   | 16/26 [00:02<00:01,  6.70it/s]predicting train subjects sagittal:  65%|██████▌   | 17/26 [00:02<00:01,  6.74it/s]predicting train subjects sagittal:  69%|██████▉   | 18/26 [00:02<00:01,  6.97it/s]predicting train subjects sagittal:  73%|███████▎  | 19/26 [00:02<00:01,  6.94it/s]predicting train subjects sagittal:  77%|███████▋  | 20/26 [00:03<00:00,  7.14it/s]predicting train subjects sagittal:  81%|████████  | 21/26 [00:03<00:00,  6.53it/s]predicting train subjects sagittal:  85%|████████▍ | 22/26 [00:03<00:00,  6.38it/s]predicting train subjects sagittal:  88%|████████▊ | 23/26 [00:03<00:00,  6.47it/s]predicting train subjects sagittal:  92%|█████████▏| 24/26 [00:03<00:00,  6.22it/s]predicting train subjects sagittal:  96%|█████████▌| 25/26 [00:03<00:00,  6.16it/s]predicting train subjects sagittal: 100%|██████████| 26/26 [00:04<00:00,  6.29it/s]predicting train subjects sagittal: 100%|██████████| 26/26 [00:04<00:00,  6.38it/s]
saving BB  test1-THALAMUS: 0it [00:00, ?it/s]saving BB  test1-THALAMUS: 0it [00:00, ?it/s]
saving BB  train1-THALAMUS:   0%|          | 0/16 [00:00<?, ?it/s]saving BB  train1-THALAMUS:  50%|█████     | 8/16 [00:00<00:00, 76.08it/s]saving BB  train1-THALAMUS:  50%|█████     | 8/16 [00:00<00:00, 41.83it/s]

Traceback (most recent call last):
  File "main.py", line 1956, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1933, in Run_Csfn_with_Best_WMn_architecture
    predict_Thalamus_For_SD0(UserInfoB)
  File "main.py", line 1883, in predict_Thalamus_For_SD0
    Run(UserI, IV)
  File "main.py", line 230, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 104, in Loop_Over_Nuclei
    Run_Main(UserI)
  File "main.py", line 224, in Run_Main
    Loop_slicing_orientations(UserInfoB, InitValues)
  File "main.py", line 222, in Loop_slicing_orientations
    subRun(UserInfoB)
  File "main.py", line 216, in subRun
    else: normal_run(params)
  File "main.py", line 203, in normal_run
    choosingModel.check_Run(params, Data)              
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 57, in check_Run
    save_BoundingBox_Hierarchy(params, prediction)
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 536, in save_BoundingBox_Hierarchy
    loop_Subjects(PRED.Train, 'train')
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 527, in loop_Subjects
    save_BoundingBox(PRED[sj] , Subjects[sj] , mode , params.directories.Test.Result)
KeyError: 'vimp2_ctrl_921_07122013_MP'
