2020-09-30 20:51:44.643565: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-09-30 20:51:45.080837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:09:00.0
totalMemory: 15.90GiB freeMemory: 15.64GiB
2020-09-30 20:51:45.080933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-09-30 20:51:45.711943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 20:51:45.712044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-09-30 20:51:45.712069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-09-30 20:51:45.712273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
Loading train:   0%|          | 0/59 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/59 [00:01<01:34,  1.64s/it]Loading train:   3%|▎         | 2/59 [00:01<01:08,  1.20s/it]Loading train:   5%|▌         | 3/59 [00:02<00:50,  1.12it/s]Loading train:   7%|▋         | 4/59 [00:02<00:38,  1.44it/s]Loading train:   8%|▊         | 5/59 [00:02<00:28,  1.90it/s]Loading train:  10%|█         | 6/59 [00:02<00:23,  2.24it/s]Loading train:  12%|█▏        | 7/59 [00:02<00:20,  2.60it/s]Loading train:  14%|█▎        | 8/59 [00:02<00:15,  3.25it/s]Loading train:  15%|█▌        | 9/59 [00:03<00:14,  3.35it/s]Loading train:  17%|█▋        | 10/59 [00:03<00:14,  3.48it/s]Loading train:  19%|█▊        | 11/59 [00:03<00:12,  3.86it/s]Loading train:  20%|██        | 12/59 [00:03<00:10,  4.38it/s]Loading train:  22%|██▏       | 13/59 [00:04<00:09,  4.99it/s]Loading train:  24%|██▎       | 14/59 [00:04<00:08,  5.09it/s]Loading train:  25%|██▌       | 15/59 [00:04<00:08,  5.32it/s]Loading train:  27%|██▋       | 16/59 [00:04<00:08,  5.13it/s]Loading train:  29%|██▉       | 17/59 [00:04<00:07,  5.29it/s]Loading train:  31%|███       | 18/59 [00:04<00:07,  5.14it/s]Loading train:  32%|███▏      | 19/59 [00:05<00:08,  4.95it/s]Loading train:  34%|███▍      | 20/59 [00:05<00:07,  4.90it/s]Loading train:  36%|███▌      | 21/59 [00:05<00:07,  5.41it/s]Loading train:  37%|███▋      | 22/59 [00:05<00:06,  5.37it/s]Loading train:  39%|███▉      | 23/59 [00:06<00:10,  3.44it/s]Loading train:  41%|████      | 24/59 [00:06<00:08,  4.06it/s]Loading train:  42%|████▏     | 25/59 [00:06<00:07,  4.70it/s]Loading train:  44%|████▍     | 26/59 [00:06<00:06,  5.08it/s]Loading train:  46%|████▌     | 27/59 [00:06<00:05,  5.93it/s]Loading train:  47%|████▋     | 28/59 [00:06<00:05,  6.12it/s]Loading train:  49%|████▉     | 29/59 [00:07<00:05,  5.41it/s]Loading train:  51%|█████     | 30/59 [00:07<00:04,  6.20it/s]Loading train:  53%|█████▎    | 31/59 [00:07<00:04,  6.34it/s]Loading train:  54%|█████▍    | 32/59 [00:07<00:04,  6.45it/s]Loading train:  56%|█████▌    | 33/59 [00:07<00:04,  6.08it/s]Loading train:  58%|█████▊    | 34/59 [00:07<00:03,  6.67it/s]Loading train:  59%|█████▉    | 35/59 [00:08<00:03,  6.98it/s]Loading train:  61%|██████    | 36/59 [00:08<00:03,  7.00it/s]Loading train:  63%|██████▎   | 37/59 [00:08<00:03,  7.03it/s]Loading train:  64%|██████▍   | 38/59 [00:08<00:03,  6.86it/s]Loading train:  66%|██████▌   | 39/59 [00:08<00:02,  7.39it/s]Loading train:  68%|██████▊   | 40/59 [00:08<00:02,  7.06it/s]Loading train:  69%|██████▉   | 41/59 [00:08<00:02,  6.30it/s]Loading train:  71%|███████   | 42/59 [00:09<00:02,  6.64it/s]Loading train:  73%|███████▎  | 43/59 [00:09<00:02,  6.93it/s]Loading train:  75%|███████▍  | 44/59 [00:09<00:02,  6.75it/s]Loading train:  76%|███████▋  | 45/59 [00:09<00:02,  6.78it/s]Loading train:  78%|███████▊  | 46/59 [00:09<00:02,  6.01it/s]Loading train:  80%|███████▉  | 47/59 [00:09<00:02,  5.93it/s]Loading train:  81%|████████▏ | 48/59 [00:10<00:01,  5.64it/s]Loading train:  83%|████████▎ | 49/59 [00:10<00:01,  5.56it/s]Loading train:  85%|████████▍ | 50/59 [00:10<00:01,  5.87it/s]Loading train:  86%|████████▋ | 51/59 [00:10<00:01,  6.41it/s]Loading train:  88%|████████▊ | 52/59 [00:10<00:01,  6.38it/s]Loading train:  90%|████████▉ | 53/59 [00:10<00:00,  7.00it/s]Loading train:  92%|█████████▏| 54/59 [00:10<00:00,  6.34it/s]Loading train:  93%|█████████▎| 55/59 [00:11<00:00,  5.28it/s]Loading train:  95%|█████████▍| 56/59 [00:11<00:00,  5.89it/s]Loading train:  97%|█████████▋| 57/59 [00:11<00:00,  5.84it/s]Loading train:  98%|█████████▊| 58/59 [00:11<00:00,  6.27it/s]Loading train: 100%|██████████| 59/59 [00:11<00:00,  6.17it/s]Loading train: 100%|██████████| 59/59 [00:11<00:00,  4.98it/s]
concatenating: train:   0%|          | 0/59 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 4/59 [00:00<00:01, 34.35it/s]concatenating: train:  12%|█▏        | 7/59 [00:00<00:01, 30.84it/s]concatenating: train:  17%|█▋        | 10/59 [00:00<00:01, 29.48it/s]concatenating: train:  24%|██▎       | 14/59 [00:00<00:01, 29.78it/s]concatenating: train:  29%|██▉       | 17/59 [00:00<00:01, 28.16it/s]concatenating: train:  34%|███▍      | 20/59 [00:00<00:01, 26.38it/s]concatenating: train:  39%|███▉      | 23/59 [00:00<00:01, 22.72it/s]concatenating: train:  44%|████▍     | 26/59 [00:01<00:01, 23.98it/s]concatenating: train:  51%|█████     | 30/59 [00:01<00:01, 26.29it/s]concatenating: train:  58%|█████▊    | 34/59 [00:01<00:00, 27.66it/s]concatenating: train:  64%|██████▍   | 38/59 [00:01<00:00, 28.59it/s]concatenating: train:  71%|███████   | 42/59 [00:01<00:00, 29.88it/s]concatenating: train:  78%|███████▊  | 46/59 [00:01<00:00, 29.48it/s]concatenating: train:  83%|████████▎ | 49/59 [00:01<00:00, 29.04it/s]concatenating: train:  88%|████████▊ | 52/59 [00:01<00:00, 29.16it/s]concatenating: train:  93%|█████████▎| 55/59 [00:01<00:00, 29.38it/s]concatenating: train:  98%|█████████▊| 58/59 [00:02<00:00, 29.21it/s]concatenating: train: 100%|██████████| 59/59 [00:02<00:00, 28.19it/s]
Loading test:   0%|          | 0/23 [00:00<?, ?it/s]Loading test:   4%|▍         | 1/23 [00:00<00:08,  2.63it/s]Loading test:   9%|▊         | 2/23 [00:00<00:06,  3.34it/s]Loading test:  13%|█▎        | 3/23 [00:00<00:05,  4.00it/s]Loading test:  17%|█▋        | 4/23 [00:00<00:03,  4.80it/s]Loading test:  22%|██▏       | 5/23 [00:00<00:03,  4.81it/s]Loading test:  26%|██▌       | 6/23 [00:01<00:03,  4.59it/s]Loading test:  30%|███       | 7/23 [00:01<00:03,  5.07it/s]Loading test:  35%|███▍      | 8/23 [00:01<00:03,  4.45it/s]Loading test:  39%|███▉      | 9/23 [00:02<00:06,  2.16it/s]Loading test:  43%|████▎     | 10/23 [00:03<00:07,  1.76it/s]Loading test:  48%|████▊     | 11/23 [00:04<00:07,  1.68it/s]Loading test:  52%|█████▏    | 12/23 [00:04<00:05,  2.11it/s]Loading test:  57%|█████▋    | 13/23 [00:04<00:04,  2.43it/s]Loading test:  61%|██████    | 14/23 [00:04<00:03,  2.62it/s]Loading test:  65%|██████▌   | 15/23 [00:05<00:02,  2.80it/s]Loading test:  70%|██████▉   | 16/23 [00:05<00:02,  2.94it/s]Loading test:  74%|███████▍  | 17/23 [00:05<00:01,  3.22it/s]Loading test:  78%|███████▊  | 18/23 [00:06<00:01,  3.07it/s]Loading test:  83%|████████▎ | 19/23 [00:06<00:01,  3.46it/s]Loading test:  87%|████████▋ | 20/23 [00:07<00:01,  1.76it/s]Loading test:  91%|█████████▏| 21/23 [00:08<00:01,  1.53it/s]Loading test:  96%|█████████▌| 22/23 [00:08<00:00,  2.01it/s]Loading test: 100%|██████████| 23/23 [00:08<00:00,  2.13it/s]Loading test: 100%|██████████| 23/23 [00:08<00:00,  2.58it/s]
concatenating: validation:   0%|          | 0/23 [00:00<?, ?it/s]concatenating: validation:  17%|█▋        | 4/23 [00:00<00:00, 29.70it/s]concatenating: validation:  35%|███▍      | 8/23 [00:00<00:00, 30.43it/s]concatenating: validation:  48%|████▊     | 11/23 [00:00<00:00, 28.26it/s]concatenating: validation:  61%|██████    | 14/23 [00:00<00:00, 28.59it/s]concatenating: validation:  78%|███████▊  | 18/23 [00:00<00:00, 29.05it/s]concatenating: validation:  91%|█████████▏| 21/23 [00:00<00:00, 27.34it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 28.11it/s]
Loading trainS:   0%|          | 0/59 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/59 [00:00<00:09,  6.44it/s]Loading trainS:   3%|▎         | 2/59 [00:00<00:08,  6.76it/s]Loading trainS:   5%|▌         | 3/59 [00:00<00:09,  6.14it/s]Loading trainS:   7%|▋         | 4/59 [00:00<00:13,  3.95it/s]Loading trainS:   8%|▊         | 5/59 [00:01<00:11,  4.65it/s]Loading trainS:  10%|█         | 6/59 [00:01<00:15,  3.50it/s]Loading trainS:  12%|█▏        | 7/59 [00:01<00:15,  3.42it/s]Loading trainS:  14%|█▎        | 8/59 [00:02<00:13,  3.75it/s]Loading trainS:  15%|█▌        | 9/59 [00:02<00:16,  3.04it/s]Loading trainS:  17%|█▋        | 10/59 [00:02<00:16,  2.97it/s]Loading trainS:  19%|█▊        | 11/59 [00:03<00:17,  2.68it/s]Loading trainS:  20%|██        | 12/59 [00:03<00:14,  3.24it/s]Loading trainS:  22%|██▏       | 13/59 [00:03<00:11,  3.88it/s]Loading trainS:  24%|██▎       | 14/59 [00:04<00:14,  3.10it/s]Loading trainS:  25%|██▌       | 15/59 [00:04<00:11,  3.75it/s]Loading trainS:  27%|██▋       | 16/59 [00:04<00:13,  3.26it/s]Loading trainS:  29%|██▉       | 17/59 [00:04<00:12,  3.36it/s]Loading trainS:  31%|███       | 18/59 [00:05<00:12,  3.24it/s]Loading trainS:  32%|███▏      | 19/59 [00:05<00:12,  3.11it/s]Loading trainS:  34%|███▍      | 20/59 [00:06<00:14,  2.77it/s]Loading trainS:  37%|███▋      | 22/59 [00:06<00:11,  3.26it/s]Loading trainS:  39%|███▉      | 23/59 [00:08<00:25,  1.40it/s]Loading trainS:  41%|████      | 24/59 [00:08<00:18,  1.85it/s]Loading trainS:  42%|████▏     | 25/59 [00:08<00:14,  2.41it/s]Loading trainS:  44%|████▍     | 26/59 [00:08<00:11,  2.75it/s]Loading trainS:  46%|████▌     | 27/59 [00:08<00:09,  3.39it/s]Loading trainS:  47%|████▋     | 28/59 [00:09<00:09,  3.11it/s]Loading trainS:  49%|████▉     | 29/59 [00:09<00:10,  2.88it/s]Loading trainS:  51%|█████     | 30/59 [00:09<00:08,  3.57it/s]Loading trainS:  53%|█████▎    | 31/59 [00:09<00:07,  3.99it/s]Loading trainS:  54%|█████▍    | 32/59 [00:09<00:05,  4.64it/s]Loading trainS:  56%|█████▌    | 33/59 [00:10<00:06,  3.94it/s]Loading trainS:  58%|█████▊    | 34/59 [00:10<00:06,  4.15it/s]Loading trainS:  59%|█████▉    | 35/59 [00:10<00:05,  4.18it/s]Loading trainS:  61%|██████    | 36/59 [00:10<00:04,  4.84it/s]Loading trainS:  63%|██████▎   | 37/59 [00:11<00:05,  4.05it/s]Loading trainS:  64%|██████▍   | 38/59 [00:11<00:05,  3.93it/s]Loading trainS:  66%|██████▌   | 39/59 [00:11<00:04,  4.41it/s]Loading trainS:  68%|██████▊   | 40/59 [00:11<00:04,  3.89it/s]Loading trainS:  69%|██████▉   | 41/59 [00:12<00:04,  3.73it/s]Loading trainS:  71%|███████   | 42/59 [00:12<00:03,  4.43it/s]Loading trainS:  73%|███████▎  | 43/59 [00:12<00:03,  4.57it/s]Loading trainS:  75%|███████▍  | 44/59 [00:12<00:03,  4.41it/s]Loading trainS:  76%|███████▋  | 45/59 [00:12<00:02,  4.85it/s]Loading trainS:  78%|███████▊  | 46/59 [00:13<00:03,  3.93it/s]Loading trainS:  80%|███████▉  | 47/59 [00:13<00:02,  4.23it/s]Loading trainS:  81%|████████▏ | 48/59 [00:14<00:03,  3.27it/s]Loading trainS:  83%|████████▎ | 49/59 [00:14<00:03,  3.07it/s]Loading trainS:  85%|████████▍ | 50/59 [00:14<00:02,  3.60it/s]Loading trainS:  86%|████████▋ | 51/59 [00:14<00:01,  4.41it/s]Loading trainS:  88%|████████▊ | 52/59 [00:14<00:01,  3.88it/s]Loading trainS:  90%|████████▉ | 53/59 [00:15<00:01,  4.52it/s]Loading trainS:  92%|█████████▏| 54/59 [00:15<00:01,  3.61it/s]Loading trainS:  93%|█████████▎| 55/59 [00:15<00:01,  3.21it/s]Loading trainS:  95%|█████████▍| 56/59 [00:16<00:00,  3.56it/s]Loading trainS:  97%|█████████▋| 57/59 [00:16<00:00,  3.22it/s]Loading trainS:  98%|█████████▊| 58/59 [00:16<00:00,  3.91it/s]Loading trainS: 100%|██████████| 59/59 [00:16<00:00,  4.05it/s]Loading trainS: 100%|██████████| 59/59 [00:16<00:00,  3.50it/s]
Loading testS:   0%|          | 0/23 [00:00<?, ?it/s]Loading testS:   4%|▍         | 1/23 [00:00<00:08,  2.57it/s]Loading testS:   9%|▊         | 2/23 [00:00<00:06,  3.27it/s]Loading testS:  17%|█▋        | 4/23 [00:00<00:04,  3.89it/s]Loading testS:  22%|██▏       | 5/23 [00:01<00:04,  3.82it/s]Loading testS:  26%|██▌       | 6/23 [00:01<00:05,  3.37it/s]Loading testS:  30%|███       | 7/23 [00:01<00:03,  4.16it/s]Loading testS:  35%|███▍      | 8/23 [00:01<00:03,  4.08it/s]Loading testS:  39%|███▉      | 9/23 [00:03<00:08,  1.72it/s]Loading testS:  43%|████▎     | 10/23 [00:03<00:07,  1.72it/s]Loading testS:  48%|████▊     | 11/23 [00:04<00:06,  1.89it/s]Loading testS:  52%|█████▏    | 12/23 [00:04<00:04,  2.43it/s]Loading testS:  57%|█████▋    | 13/23 [00:04<00:03,  2.83it/s]Loading testS:  61%|██████    | 14/23 [00:04<00:02,  3.09it/s]Loading testS:  65%|██████▌   | 15/23 [00:05<00:02,  2.93it/s]Loading testS:  70%|██████▉   | 16/23 [00:05<00:02,  2.83it/s]Loading testS:  74%|███████▍  | 17/23 [00:05<00:01,  3.28it/s]Loading testS:  78%|███████▊  | 18/23 [00:06<00:01,  3.22it/s]Loading testS:  83%|████████▎ | 19/23 [00:06<00:01,  3.46it/s]Loading testS:  87%|████████▋ | 20/23 [00:07<00:01,  1.68it/s]Loading testS:  91%|█████████▏| 21/23 [00:08<00:01,  1.53it/s]Loading testS:  96%|█████████▌| 22/23 [00:08<00:00,  2.02it/s]Loading testS: 100%|██████████| 23/23 [00:08<00:00,  2.43it/s]Loading testS: 100%|██████████| 23/23 [00:08<00:00,  2.64it/s]
 1-THALAMUS SD: [2] GPU: 3 

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 168, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 168, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 168, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 168, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 168, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 168, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 168, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 84, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 84, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 84, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 84, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 84, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 84, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 84, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 84, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 84, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 42, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 42, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 42, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 42, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 42, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 42, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 42, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 42, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 42, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 42, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 84, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 84, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 84, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 84, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 84, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 84, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 84, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 84, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 84, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 84, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 168, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 168, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 168, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 168, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 168, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 168, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 168, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 168, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 168, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 168, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 168, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialization succesfull
class_weights [0.98181702 0.01818298]
Train on 3939 samples, validate on 1622 samples
Epoch 1/30
 - 38s - loss: 0.0668 - acc: 0.9081 - mDice: 0.9156 - val_loss: 0.1502 - val_acc: 0.9329 - val_mDice: 0.7350

Epoch 00001: val_mDice improved from -inf to 0.73500, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM40/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/30
 - 28s - loss: 0.0618 - acc: 0.9084 - mDice: 0.9254 - val_loss: 0.1204 - val_acc: 0.9329 - val_mDice: 0.7427

Epoch 00002: val_mDice improved from 0.73500 to 0.74266, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM40/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/30
 - 28s - loss: 0.0604 - acc: 0.9085 - mDice: 0.9280 - val_loss: 0.1474 - val_acc: 0.9328 - val_mDice: 0.7378

Epoch 00003: val_mDice did not improve from 0.74266
Epoch 4/30
 - 28s - loss: 0.0598 - acc: 0.9085 - mDice: 0.9294 - val_loss: 0.1283 - val_acc: 0.9330 - val_mDice: 0.7387

Epoch 00004: val_mDice did not improve from 0.74266
Epoch 5/30
 - 28s - loss: 0.0588 - acc: 0.9085 - mDice: 0.9312 - val_loss: 0.1411 - val_acc: 0.9329 - val_mDice: 0.7468

Epoch 00005: val_mDice improved from 0.74266 to 0.74681, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM40/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/30
 - 28s - loss: 0.0580 - acc: 0.9086 - mDice: 0.9329 - val_loss: 0.1403 - val_acc: 0.9330 - val_mDice: 0.7497

Epoch 00006: val_mDice improved from 0.74681 to 0.74968, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM40/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/30
 - 27s - loss: 0.0584 - acc: 0.9086 - mDice: 0.9321 - val_loss: 0.1240 - val_acc: 0.9329 - val_mDice: 0.7366

Epoch 00007: val_mDice did not improve from 0.74968
Epoch 8/30
 - 28s - loss: 0.0571 - acc: 0.9086 - mDice: 0.9347 - val_loss: 0.1134 - val_acc: 0.9328 - val_mDice: 0.7381

Epoch 00008: val_mDice did not improve from 0.74968
Epoch 9/30
 - 28s - loss: 0.0565 - acc: 0.9087 - mDice: 0.9357 - val_loss: 0.0930 - val_acc: 0.9329 - val_mDice: 0.7377

Epoch 00009: val_mDice did not improve from 0.74968
Epoch 10/30
 - 28s - loss: 0.0568 - acc: 0.9087 - mDice: 0.9351 - val_loss: 0.1054 - val_acc: 0.9327 - val_mDice: 0.7288

Epoch 00010: val_mDice did not improve from 0.74968
Epoch 11/30
 - 28s - loss: 0.0560 - acc: 0.9087 - mDice: 0.9368 - val_loss: 0.1084 - val_acc: 0.9329 - val_mDice: 0.7447

Epoch 00011: val_mDice did not improve from 0.74968
Epoch 12/30
 - 28s - loss: 0.0550 - acc: 0.9087 - mDice: 0.9388 - val_loss: 0.1394 - val_acc: 0.9329 - val_mDice: 0.7391

Epoch 00012: val_mDice did not improve from 0.74968
Epoch 13/30
 - 27s - loss: 0.0551 - acc: 0.9087 - mDice: 0.9387 - val_loss: 0.1178 - val_acc: 0.9329 - val_mDice: 0.7387

Epoch 00013: val_mDice did not improve from 0.74968
Epoch 14/30
 - 28s - loss: 0.0548 - acc: 0.9088 - mDice: 0.9390 - val_loss: 0.1125 - val_acc: 0.9329 - val_mDice: 0.7344

Epoch 00014: val_mDice did not improve from 0.74968
Epoch 15/30
 - 28s - loss: 0.0549 - acc: 0.9088 - mDice: 0.9390 - val_loss: 0.0990 - val_acc: 0.9329 - val_mDice: 0.7424

Epoch 00015: val_mDice did not improve from 0.74968
Epoch 16/30
 - 28s - loss: 0.0541 - acc: 0.9088 - mDice: 0.9406 - val_loss: 0.1169 - val_acc: 0.9328 - val_mDice: 0.7440

Epoch 00016: val_mDice did not improve from 0.74968
Epoch 17/30
 - 28s - loss: 0.0534 - acc: 0.9088 - mDice: 0.9418 - val_loss: 0.1015 - val_acc: 0.9329 - val_mDice: 0.7375

Epoch 00017: val_mDice did not improve from 0.74968
Epoch 18/30
 - 28s - loss: 0.0538 - acc: 0.9088 - mDice: 0.9412 - val_loss: 0.0986 - val_acc: 0.9328 - val_mDice: 0.7332

Epoch 00018: val_mDice did not improve from 0.74968
Epoch 19/30
 - 28s - loss: 0.0536 - acc: 0.9088 - mDice: 0.9415 - val_loss: 0.1123 - val_acc: 0.9329 - val_mDice: 0.7324

Epoch 00019: val_mDice did not improve from 0.74968
Epoch 20/30
 - 28s - loss: 0.0529 - acc: 0.9088 - mDice: 0.9428 - val_loss: 0.1420 - val_acc: 0.9329 - val_mDice: 0.7423

Epoch 00020: val_mDice did not improve from 0.74968
Epoch 21/30
 - 28s - loss: 0.0526 - acc: 0.9088 - mDice: 0.9434 - val_loss: 0.1410 - val_acc: 0.9329 - val_mDice: 0.7396

Epoch 00021: val_mDice did not improve from 0.74968

Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 22/30
 - 28s - loss: 0.0515 - acc: 0.9089 - mDice: 0.9456 - val_loss: 0.1380 - val_acc: 0.9329 - val_mDice: 0.7352

Epoch 00022: val_mDice did not improve from 0.74968
Epoch 23/30
 - 28s - loss: 0.0512 - acc: 0.9089 - mDice: 0.9463 - val_loss: 0.1295 - val_acc: 0.9329 - val_mDice: 0.7380

Epoch 00023: val_mDice did not improve from 0.74968
Epoch 24/30
 - 28s - loss: 0.0510 - acc: 0.9089 - mDice: 0.9467 - val_loss: 0.1345 - val_acc: 0.9329 - val_mDice: 0.7374

Epoch 00024: val_mDice did not improve from 0.74968
Epoch 25/30
 - 28s - loss: 0.0512 - acc: 0.9089 - mDice: 0.9462 - val_loss: 0.1345 - val_acc: 0.9329 - val_mDice: 0.7369

Epoch 00025: val_mDice did not improve from 0.74968
Epoch 26/30
 - 28s - loss: 0.0506 - acc: 0.9089 - mDice: 0.9474 - val_loss: 0.1458 - val_acc: 0.9328 - val_mDice: 0.7349

Epoch 00026: val_mDice did not improve from 0.74968
Epoch 27/30
 - 28s - loss: 0.0504 - acc: 0.9089 - mDice: 0.9478 - val_loss: 0.1379 - val_acc: 0.9328 - val_mDice: 0.7352

Epoch 00027: val_mDice did not improve from 0.74968
Epoch 28/30
 - 28s - loss: 0.0505 - acc: 0.9090 - mDice: 0.9477 - val_loss: 0.1480 - val_acc: 0.9329 - val_mDice: 0.7351

Epoch 00028: val_mDice did not improve from 0.74968
Epoch 29/30
 - 28s - loss: 0.0505 - acc: 0.9090 - mDice: 0.9477 - val_loss: 0.1370 - val_acc: 0.9329 - val_mDice: 0.7366

Epoch 00029: val_mDice did not improve from 0.74968
Epoch 30/30
 - 27s - loss: 0.0502 - acc: 0.9090 - mDice: 0.9483 - val_loss: 0.1351 - val_acc: 0.9329 - val_mDice: 0.7366

predicting test subjects:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects:   4%|▍         | 1/23 [00:01<00:31,  1.41s/it]predicting test subjects:   9%|▊         | 2/23 [00:01<00:24,  1.14s/it]predicting test subjects:  13%|█▎        | 3/23 [00:02<00:19,  1.04it/s]predicting test subjects:  17%|█▋        | 4/23 [00:02<00:14,  1.30it/s]predicting test subjects:  22%|██▏       | 5/23 [00:03<00:11,  1.61it/s]predicting test subjects:  26%|██▌       | 6/23 [00:03<00:11,  1.53it/s]predicting test subjects:  30%|███       | 7/23 [00:04<00:09,  1.61it/s]predicting test subjects:  35%|███▍      | 8/23 [00:05<00:09,  1.57it/s]predicting test subjects:  39%|███▉      | 9/23 [00:05<00:07,  1.84it/s]predicting test subjects:  43%|████▎     | 10/23 [00:05<00:07,  1.78it/s]predicting test subjects:  48%|████▊     | 11/23 [00:06<00:06,  1.82it/s]predicting test subjects:  52%|█████▏    | 12/23 [00:06<00:05,  2.03it/s]predicting test subjects:  57%|█████▋    | 13/23 [00:07<00:04,  2.19it/s]predicting test subjects:  61%|██████    | 14/23 [00:07<00:03,  2.43it/s]predicting test subjects:  65%|██████▌   | 15/23 [00:08<00:03,  2.28it/s]predicting test subjects:  70%|██████▉   | 16/23 [00:08<00:03,  2.20it/s]predicting test subjects:  74%|███████▍  | 17/23 [00:08<00:02,  2.28it/s]predicting test subjects:  78%|███████▊  | 18/23 [00:09<00:02,  2.47it/s]predicting test subjects:  83%|████████▎ | 19/23 [00:09<00:01,  2.54it/s]predicting test subjects:  87%|████████▋ | 20/23 [00:10<00:01,  2.45it/s]predicting test subjects:  91%|█████████▏| 21/23 [00:10<00:00,  2.16it/s]predicting test subjects:  96%|█████████▌| 22/23 [00:10<00:00,  2.55it/s]predicting test subjects: 100%|██████████| 23/23 [00:11<00:00,  2.33it/s]predicting test subjects: 100%|██████████| 23/23 [00:11<00:00,  2.02it/s]
predicting train subjects:   0%|          | 0/59 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/59 [00:00<00:11,  4.99it/s]predicting train subjects:   3%|▎         | 2/59 [00:00<00:12,  4.55it/s]predicting train subjects:   5%|▌         | 3/59 [00:00<00:12,  4.56it/s]predicting train subjects:   7%|▋         | 4/59 [00:00<00:12,  4.44it/s]predicting train subjects:   8%|▊         | 5/59 [00:01<00:11,  4.56it/s]predicting train subjects:  10%|█         | 6/59 [00:01<00:12,  4.28it/s]predicting train subjects:  12%|█▏        | 7/59 [00:01<00:12,  4.08it/s]predicting train subjects:  14%|█▎        | 8/59 [00:01<00:11,  4.31it/s]predicting train subjects:  15%|█▌        | 9/59 [00:02<00:15,  3.33it/s]predicting train subjects:  17%|█▋        | 10/59 [00:02<00:14,  3.33it/s]predicting train subjects:  19%|█▊        | 11/59 [00:02<00:14,  3.37it/s]predicting train subjects:  20%|██        | 12/59 [00:03<00:13,  3.38it/s]predicting train subjects:  22%|██▏       | 13/59 [00:03<00:17,  2.70it/s]predicting train subjects:  24%|██▎       | 14/59 [00:03<00:14,  3.03it/s]predicting train subjects:  25%|██▌       | 15/59 [00:04<00:13,  3.34it/s]predicting train subjects:  27%|██▋       | 16/59 [00:04<00:12,  3.45it/s]predicting train subjects:  29%|██▉       | 17/59 [00:04<00:14,  2.86it/s]predicting train subjects:  31%|███       | 18/59 [00:05<00:14,  2.92it/s]predicting train subjects:  32%|███▏      | 19/59 [00:05<00:15,  2.61it/s]predicting train subjects:  34%|███▍      | 20/59 [00:06<00:13,  2.81it/s]predicting train subjects:  36%|███▌      | 21/59 [00:06<00:12,  2.95it/s]predicting train subjects:  37%|███▋      | 22/59 [00:06<00:11,  3.19it/s]predicting train subjects:  39%|███▉      | 23/59 [00:07<00:14,  2.52it/s]predicting train subjects:  41%|████      | 24/59 [00:07<00:15,  2.30it/s]predicting train subjects:  42%|████▏     | 25/59 [00:07<00:12,  2.69it/s]predicting train subjects:  44%|████▍     | 26/59 [00:08<00:11,  2.80it/s]predicting train subjects:  46%|████▌     | 27/59 [00:08<00:10,  2.94it/s]predicting train subjects:  47%|████▋     | 28/59 [00:08<00:09,  3.27it/s]predicting train subjects:  49%|████▉     | 29/59 [00:09<00:08,  3.46it/s]predicting train subjects:  51%|█████     | 30/59 [00:09<00:08,  3.42it/s]predicting train subjects:  53%|█████▎    | 31/59 [00:09<00:08,  3.31it/s]predicting train subjects:  54%|█████▍    | 32/59 [00:09<00:07,  3.40it/s]predicting train subjects:  56%|█████▌    | 33/59 [00:10<00:07,  3.41it/s]predicting train subjects:  58%|█████▊    | 34/59 [00:10<00:06,  3.80it/s]predicting train subjects:  59%|█████▉    | 35/59 [00:10<00:06,  3.86it/s]predicting train subjects:  61%|██████    | 36/59 [00:10<00:05,  4.12it/s]predicting train subjects:  63%|██████▎   | 37/59 [00:11<00:06,  3.58it/s]predicting train subjects:  64%|██████▍   | 38/59 [00:11<00:05,  4.00it/s]predicting train subjects:  66%|██████▌   | 39/59 [00:11<00:04,  4.18it/s]predicting train subjects:  68%|██████▊   | 40/59 [00:11<00:04,  4.06it/s]predicting train subjects:  69%|██████▉   | 41/59 [00:12<00:04,  4.12it/s]predicting train subjects:  71%|███████   | 42/59 [00:12<00:04,  3.76it/s]predicting train subjects:  73%|███████▎  | 43/59 [00:12<00:03,  4.02it/s]predicting train subjects:  75%|███████▍  | 44/59 [00:12<00:03,  3.94it/s]predicting train subjects:  76%|███████▋  | 45/59 [00:13<00:03,  4.26it/s]predicting train subjects:  78%|███████▊  | 46/59 [00:13<00:03,  4.11it/s]predicting train subjects:  80%|███████▉  | 47/59 [00:13<00:02,  4.12it/s]predicting train subjects:  81%|████████▏ | 48/59 [00:13<00:02,  3.72it/s]predicting train subjects:  83%|████████▎ | 49/59 [00:14<00:02,  3.52it/s]predicting train subjects:  85%|████████▍ | 50/59 [00:14<00:02,  3.78it/s]predicting train subjects:  86%|████████▋ | 51/59 [00:14<00:02,  3.73it/s]predicting train subjects:  88%|████████▊ | 52/59 [00:15<00:01,  3.59it/s]predicting train subjects:  90%|████████▉ | 53/59 [00:15<00:02,  2.85it/s]predicting train subjects:  92%|█████████▏| 54/59 [00:15<00:01,  2.98it/s]predicting train subjects:  93%|█████████▎| 55/59 [00:16<00:01,  3.17it/s]predicting train subjects:  95%|█████████▍| 56/59 [00:16<00:00,  3.28it/s]predicting train subjects:  97%|█████████▋| 57/59 [00:16<00:00,  3.09it/s]predicting train subjects:  98%|█████████▊| 58/59 [00:17<00:00,  3.44it/s]predicting train subjects: 100%|██████████| 59/59 [00:17<00:00,  3.68it/s]predicting train subjects: 100%|██████████| 59/59 [00:17<00:00,  3.41it/s]
predicting test subjects sagittal:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects sagittal:   4%|▍         | 1/23 [00:00<00:09,  2.23it/s]predicting test subjects sagittal:   9%|▊         | 2/23 [00:00<00:07,  2.71it/s]predicting test subjects sagittal:  13%|█▎        | 3/23 [00:00<00:06,  3.10it/s]predicting test subjects sagittal:  17%|█▋        | 4/23 [00:01<00:06,  3.10it/s]predicting test subjects sagittal:  22%|██▏       | 5/23 [00:01<00:05,  3.03it/s]predicting test subjects sagittal:  26%|██▌       | 6/23 [00:01<00:05,  3.03it/s]predicting test subjects sagittal:  30%|███       | 7/23 [00:02<00:04,  3.49it/s]predicting test subjects sagittal:  35%|███▍      | 8/23 [00:02<00:04,  3.31it/s]predicting test subjects sagittal:  39%|███▉      | 9/23 [00:02<00:04,  3.20it/s]predicting test subjects sagittal:  43%|████▎     | 10/23 [00:03<00:04,  3.03it/s]predicting test subjects sagittal:  48%|████▊     | 11/23 [00:03<00:03,  3.08it/s]predicting test subjects sagittal:  52%|█████▏    | 12/23 [00:03<00:03,  3.17it/s]predicting test subjects sagittal:  57%|█████▋    | 13/23 [00:03<00:02,  3.45it/s]predicting test subjects sagittal:  61%|██████    | 14/23 [00:04<00:02,  3.32it/s]predicting test subjects sagittal:  65%|██████▌   | 15/23 [00:04<00:02,  3.17it/s]predicting test subjects sagittal:  70%|██████▉   | 16/23 [00:04<00:02,  3.17it/s]predicting test subjects sagittal:  74%|███████▍  | 17/23 [00:05<00:01,  3.17it/s]predicting test subjects sagittal:  78%|███████▊  | 18/23 [00:05<00:01,  3.39it/s]predicting test subjects sagittal:  83%|████████▎ | 19/23 [00:05<00:01,  3.26it/s]predicting test subjects sagittal:  87%|████████▋ | 20/23 [00:06<00:01,  2.89it/s]predicting test subjects sagittal:  91%|█████████▏| 21/23 [00:06<00:00,  2.97it/s]predicting test subjects sagittal:  96%|█████████▌| 22/23 [00:06<00:00,  3.25it/s]predicting test subjects sagittal: 100%|██████████| 23/23 [00:07<00:00,  3.26it/s]predicting test subjects sagittal: 100%|██████████| 23/23 [00:07<00:00,  3.24it/s]
predicting train subjects sagittal:   0%|          | 0/59 [00:00<?, ?it/s]predicting train subjects sagittal:   2%|▏         | 1/59 [00:00<00:37,  1.56it/s]predicting train subjects sagittal:   3%|▎         | 2/59 [00:00<00:28,  1.97it/s]predicting train subjects sagittal:   5%|▌         | 3/59 [00:01<00:23,  2.35it/s]predicting train subjects sagittal:   7%|▋         | 4/59 [00:01<00:20,  2.70it/s]predicting train subjects sagittal:   8%|▊         | 5/59 [00:01<00:17,  3.01it/s]predicting train subjects sagittal:  10%|█         | 6/59 [00:01<00:17,  3.03it/s]predicting train subjects sagittal:  12%|█▏        | 7/59 [00:02<00:16,  3.14it/s]predicting train subjects sagittal:  14%|█▎        | 8/59 [00:02<00:14,  3.61it/s]predicting train subjects sagittal:  15%|█▌        | 9/59 [00:02<00:14,  3.44it/s]predicting train subjects sagittal:  17%|█▋        | 10/59 [00:02<00:14,  3.33it/s]predicting train subjects sagittal:  19%|█▊        | 11/59 [00:03<00:14,  3.33it/s]predicting train subjects sagittal:  20%|██        | 12/59 [00:03<00:14,  3.24it/s]predicting train subjects sagittal:  22%|██▏       | 13/59 [00:03<00:12,  3.55it/s]predicting train subjects sagittal:  24%|██▎       | 14/59 [00:04<00:12,  3.67it/s]predicting train subjects sagittal:  25%|██▌       | 15/59 [00:04<00:11,  3.90it/s]predicting train subjects sagittal:  27%|██▋       | 16/59 [00:04<00:11,  3.77it/s]predicting train subjects sagittal:  29%|██▉       | 17/59 [00:04<00:11,  3.56it/s]predicting train subjects sagittal:  31%|███       | 18/59 [00:05<00:11,  3.65it/s]predicting train subjects sagittal:  32%|███▏      | 19/59 [00:05<00:11,  3.44it/s]predicting train subjects sagittal:  34%|███▍      | 20/59 [00:05<00:10,  3.56it/s]predicting train subjects sagittal:  36%|███▌      | 21/59 [00:06<00:10,  3.56it/s]predicting train subjects sagittal:  37%|███▋      | 22/59 [00:06<00:10,  3.66it/s]predicting train subjects sagittal:  39%|███▉      | 23/59 [00:06<00:12,  2.83it/s]predicting train subjects sagittal:  41%|████      | 24/59 [00:07<00:11,  3.02it/s]predicting train subjects sagittal:  42%|████▏     | 25/59 [00:07<00:09,  3.45it/s]predicting train subjects sagittal:  44%|████▍     | 26/59 [00:07<00:09,  3.46it/s]predicting train subjects sagittal:  46%|████▌     | 27/59 [00:07<00:10,  3.16it/s]predicting train subjects sagittal:  47%|████▋     | 28/59 [00:08<00:08,  3.52it/s]predicting train subjects sagittal:  49%|████▉     | 29/59 [00:08<00:08,  3.72it/s]predicting train subjects sagittal:  51%|█████     | 30/59 [00:08<00:07,  3.79it/s]predicting train subjects sagittal:  53%|█████▎    | 31/59 [00:08<00:07,  3.86it/s]predicting train subjects sagittal:  54%|█████▍    | 32/59 [00:09<00:07,  3.83it/s]predicting train subjects sagittal:  56%|█████▌    | 33/59 [00:09<00:07,  3.63it/s]predicting train subjects sagittal:  58%|█████▊    | 34/59 [00:09<00:06,  4.17it/s]predicting train subjects sagittal:  59%|█████▉    | 35/59 [00:09<00:05,  4.21it/s]predicting train subjects sagittal:  61%|██████    | 36/59 [00:10<00:05,  4.45it/s]predicting train subjects sagittal:  63%|██████▎   | 37/59 [00:10<00:05,  4.04it/s]predicting train subjects sagittal:  64%|██████▍   | 38/59 [00:10<00:04,  4.44it/s]predicting train subjects sagittal:  66%|██████▌   | 39/59 [00:10<00:04,  4.54it/s]predicting train subjects sagittal:  68%|██████▊   | 40/59 [00:11<00:04,  4.46it/s]predicting train subjects sagittal:  69%|██████▉   | 41/59 [00:11<00:04,  4.22it/s]predicting train subjects sagittal:  71%|███████   | 42/59 [00:11<00:04,  3.93it/s]predicting train subjects sagittal:  73%|███████▎  | 43/59 [00:11<00:03,  4.12it/s]predicting train subjects sagittal:  75%|███████▍  | 44/59 [00:12<00:03,  4.20it/s]predicting train subjects sagittal:  76%|███████▋  | 45/59 [00:12<00:03,  4.44it/s]predicting train subjects sagittal:  78%|███████▊  | 46/59 [00:12<00:03,  4.12it/s]predicting train subjects sagittal:  80%|███████▉  | 47/59 [00:12<00:02,  4.13it/s]predicting train subjects sagittal:  81%|████████▏ | 48/59 [00:13<00:02,  3.88it/s]predicting train subjects sagittal:  83%|████████▎ | 49/59 [00:13<00:02,  3.47it/s]predicting train subjects sagittal:  85%|████████▍ | 50/59 [00:13<00:02,  3.56it/s]predicting train subjects sagittal:  86%|████████▋ | 51/59 [00:13<00:02,  3.53it/s]predicting train subjects sagittal:  88%|████████▊ | 52/59 [00:14<00:02,  3.47it/s]predicting train subjects sagittal:  90%|████████▉ | 53/59 [00:14<00:01,  3.89it/s]predicting train subjects sagittal:  92%|█████████▏| 54/59 [00:14<00:01,  3.56it/s]predicting train subjects sagittal:  93%|█████████▎| 55/59 [00:15<00:01,  3.50it/s]predicting train subjects sagittal:  95%|█████████▍| 56/59 [00:15<00:00,  3.27it/s]predicting train subjects sagittal:  97%|█████████▋| 57/59 [00:15<00:00,  3.17it/s]predicting train subjects sagittal:  98%|█████████▊| 58/59 [00:15<00:00,  3.53it/s]predicting train subjects sagittal: 100%|██████████| 59/59 [00:16<00:00,  3.92it/s]predicting train subjects sagittal: 100%|██████████| 59/59 [00:16<00:00,  3.66it/s]
saving BB  test1-THALAMUS:   0%|          | 0/23 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  22%|██▏       | 5/23 [00:00<00:00, 46.73it/s]saving BB  test1-THALAMUS:  43%|████▎     | 10/23 [00:00<00:00, 44.82it/s]saving BB  test1-THALAMUS:  65%|██████▌   | 15/23 [00:00<00:00, 43.02it/s]saving BB  test1-THALAMUS:  87%|████████▋ | 20/23 [00:00<00:00, 39.92it/s]saving BB  test1-THALAMUS: 100%|██████████| 23/23 [00:00<00:00, 39.64it/s]
saving BB  train1-THALAMUS:   0%|          | 0/59 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   8%|▊         | 5/59 [00:00<00:01, 42.16it/s]saving BB  train1-THALAMUS:  17%|█▋        | 10/59 [00:00<00:01, 41.94it/s]saving BB  train1-THALAMUS:  24%|██▎       | 14/59 [00:00<00:01, 41.24it/s]saving BB  train1-THALAMUS:  32%|███▏      | 19/59 [00:00<00:00, 41.24it/s]saving BB  train1-THALAMUS:  39%|███▉      | 23/59 [00:00<00:00, 40.26it/s]saving BB  train1-THALAMUS:  47%|████▋     | 28/59 [00:00<00:00, 42.52it/s]saving BB  train1-THALAMUS:  58%|█████▊    | 34/59 [00:00<00:00, 44.77it/s]saving BB  train1-THALAMUS:  69%|██████▉   | 41/59 [00:00<00:00, 47.84it/s]saving BB  train1-THALAMUS:  78%|███████▊  | 46/59 [00:01<00:00, 47.94it/s]saving BB  train1-THALAMUS:  86%|████████▋ | 51/59 [00:01<00:00, 48.13it/s]saving BB  train1-THALAMUS:  95%|█████████▍| 56/59 [00:01<00:00, 48.25it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 45.93it/s]
saving BB  test1-THALAMUS:   0%|          | 0/23 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  26%|██▌       | 6/23 [00:00<00:00, 55.73it/s]saving BB  test1-THALAMUS:  48%|████▊     | 11/23 [00:00<00:00, 53.15it/s]saving BB  test1-THALAMUS:  65%|██████▌   | 15/23 [00:00<00:00, 47.29it/s]saving BB  test1-THALAMUS:  87%|████████▋ | 20/23 [00:00<00:00, 45.61it/s]saving BB  test1-THALAMUS: 100%|██████████| 23/23 [00:00<00:00, 46.18it/s]
saving BB  train1-THALAMUS:   0%|          | 0/59 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   8%|▊         | 5/59 [00:00<00:01, 41.95it/s]saving BB  train1-THALAMUS:  15%|█▌        | 9/59 [00:00<00:01, 40.12it/s]saving BB  train1-THALAMUS:  22%|██▏       | 13/59 [00:00<00:01, 38.40it/s]saving BB  train1-THALAMUS:  31%|███       | 18/59 [00:00<00:01, 39.08it/s]saving BB  train1-THALAMUS:  39%|███▉      | 23/59 [00:00<00:00, 38.92it/s]saving BB  train1-THALAMUS:  49%|████▉     | 29/59 [00:00<00:00, 42.17it/s]saving BB  train1-THALAMUS:  59%|█████▉    | 35/59 [00:00<00:00, 45.74it/s]saving BB  train1-THALAMUS:  69%|██████▉   | 41/59 [00:00<00:00, 47.91it/s]saving BB  train1-THALAMUS:  78%|███████▊  | 46/59 [00:01<00:00, 48.38it/s]saving BB  train1-THALAMUS:  86%|████████▋ | 51/59 [00:01<00:00, 46.20it/s]saving BB  train1-THALAMUS:  95%|█████████▍| 56/59 [00:01<00:00, 46.74it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 44.89it/s]
Loading train:   0%|          | 0/59 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/59 [00:00<00:40,  1.43it/s]Loading train:   3%|▎         | 2/59 [00:01<00:36,  1.58it/s]Loading train:   5%|▌         | 3/59 [00:01<00:35,  1.58it/s]Loading train:   7%|▋         | 4/59 [00:02<00:33,  1.65it/s]Loading train:   8%|▊         | 5/59 [00:02<00:32,  1.67it/s]Loading train:  10%|█         | 6/59 [00:03<00:30,  1.74it/s]Loading train:  12%|█▏        | 7/59 [00:04<00:32,  1.62it/s]Loading train:  14%|█▎        | 8/59 [00:04<00:29,  1.73it/s]Loading train:  15%|█▌        | 9/59 [00:05<00:28,  1.74it/s]Loading train:  17%|█▋        | 10/59 [00:05<00:29,  1.68it/s]Loading train:  19%|█▊        | 11/59 [00:06<00:30,  1.56it/s]Loading train:  20%|██        | 12/59 [00:07<00:30,  1.55it/s]Loading train:  22%|██▏       | 13/59 [00:07<00:27,  1.64it/s]Loading train:  24%|██▎       | 14/59 [00:08<00:27,  1.64it/s]Loading train:  25%|██▌       | 15/59 [00:08<00:24,  1.78it/s]Loading train:  27%|██▋       | 16/59 [00:09<00:25,  1.70it/s]Loading train:  29%|██▉       | 17/59 [00:10<00:23,  1.77it/s]Loading train:  31%|███       | 18/59 [00:10<00:24,  1.67it/s]Loading train:  32%|███▏      | 19/59 [00:11<00:29,  1.38it/s]Loading train:  34%|███▍      | 20/59 [00:12<00:28,  1.38it/s]Loading train:  36%|███▌      | 21/59 [00:12<00:23,  1.63it/s]Loading train:  37%|███▋      | 22/59 [00:13<00:21,  1.72it/s]Loading train:  39%|███▉      | 23/59 [00:14<00:30,  1.18it/s]Loading train:  41%|████      | 24/59 [00:15<00:27,  1.27it/s]Loading train:  42%|████▏     | 25/59 [00:15<00:23,  1.42it/s]Loading train:  44%|████▍     | 26/59 [00:16<00:19,  1.68it/s]Loading train:  46%|████▌     | 27/59 [00:16<00:16,  1.92it/s]Loading train:  47%|████▋     | 28/59 [00:16<00:14,  2.13it/s]Loading train:  49%|████▉     | 29/59 [00:17<00:13,  2.27it/s]Loading train:  51%|█████     | 30/59 [00:18<00:18,  1.56it/s]Loading train:  53%|█████▎    | 31/59 [00:18<00:14,  1.88it/s]Loading train:  54%|█████▍    | 32/59 [00:19<00:12,  2.12it/s]Loading train:  56%|█████▌    | 33/59 [00:19<00:11,  2.33it/s]Loading train:  58%|█████▊    | 34/59 [00:19<00:09,  2.67it/s]Loading train:  59%|█████▉    | 35/59 [00:19<00:08,  2.97it/s]Loading train:  61%|██████    | 36/59 [00:20<00:07,  2.93it/s]Loading train:  63%|██████▎   | 37/59 [00:20<00:07,  2.98it/s]Loading train:  64%|██████▍   | 38/59 [00:20<00:06,  3.19it/s]Loading train:  66%|██████▌   | 39/59 [00:21<00:06,  3.31it/s]Loading train:  68%|██████▊   | 40/59 [00:21<00:05,  3.25it/s]Loading train:  69%|██████▉   | 41/59 [00:21<00:06,  2.97it/s]Loading train:  71%|███████   | 42/59 [00:22<00:08,  2.02it/s]Loading train:  73%|███████▎  | 43/59 [00:23<00:07,  2.24it/s]Loading train:  75%|███████▍  | 44/59 [00:23<00:06,  2.45it/s]Loading train:  76%|███████▋  | 45/59 [00:23<00:05,  2.67it/s]Loading train:  78%|███████▊  | 46/59 [00:23<00:04,  2.69it/s]Loading train:  80%|███████▉  | 47/59 [00:24<00:04,  2.88it/s]Loading train:  81%|████████▏ | 48/59 [00:24<00:03,  2.87it/s]Loading train:  83%|████████▎ | 49/59 [00:24<00:03,  3.05it/s]Loading train:  85%|████████▍ | 50/59 [00:25<00:02,  3.03it/s]Loading train:  86%|████████▋ | 51/59 [00:25<00:02,  3.10it/s]Loading train:  88%|████████▊ | 52/59 [00:25<00:02,  3.11it/s]Loading train:  90%|████████▉ | 53/59 [00:26<00:01,  3.15it/s]Loading train:  92%|█████████▏| 54/59 [00:26<00:01,  3.05it/s]Loading train:  93%|█████████▎| 55/59 [00:26<00:01,  2.99it/s]Loading train:  95%|█████████▍| 56/59 [00:27<00:01,  2.93it/s]Loading train:  97%|█████████▋| 57/59 [00:27<00:00,  3.04it/s]Loading train:  98%|█████████▊| 58/59 [00:27<00:00,  3.09it/s]Loading train: 100%|██████████| 59/59 [00:28<00:00,  3.17it/s]Loading train: 100%|██████████| 59/59 [00:28<00:00,  2.10it/s]
concatenating: train:   0%|          | 0/59 [00:00<?, ?it/s]concatenating: train:  10%|█         | 6/59 [00:00<00:00, 56.17it/s]concatenating: train:  20%|██        | 12/59 [00:00<00:00, 55.53it/s]concatenating: train:  31%|███       | 18/59 [00:00<00:00, 55.94it/s]concatenating: train:  41%|████      | 24/59 [00:00<00:00, 56.77it/s]concatenating: train:  49%|████▉     | 29/59 [00:00<00:00, 52.70it/s]concatenating: train:  59%|█████▉    | 35/59 [00:00<00:00, 54.16it/s]concatenating: train:  71%|███████   | 42/59 [00:00<00:00, 55.83it/s]concatenating: train:  81%|████████▏ | 48/59 [00:00<00:00, 55.36it/s]concatenating: train:  92%|█████████▏| 54/59 [00:00<00:00, 55.04it/s]concatenating: train: 100%|██████████| 59/59 [00:01<00:00, 55.35it/s]
Loading test:   0%|          | 0/23 [00:00<?, ?it/s]Loading test:   4%|▍         | 1/23 [00:00<00:08,  2.60it/s]Loading test:   9%|▊         | 2/23 [00:00<00:07,  2.74it/s]Loading test:  13%|█▎        | 3/23 [00:01<00:06,  2.90it/s]Loading test:  17%|█▋        | 4/23 [00:01<00:06,  2.92it/s]Loading test:  22%|██▏       | 5/23 [00:01<00:05,  3.10it/s]Loading test:  26%|██▌       | 6/23 [00:01<00:05,  3.01it/s]Loading test:  30%|███       | 7/23 [00:02<00:05,  3.07it/s]Loading test:  35%|███▍      | 8/23 [00:02<00:04,  3.02it/s]Loading test:  39%|███▉      | 9/23 [00:03<00:05,  2.64it/s]Loading test:  43%|████▎     | 10/23 [00:03<00:04,  2.73it/s]Loading test:  48%|████▊     | 11/23 [00:04<00:06,  1.92it/s]Loading test:  52%|█████▏    | 12/23 [00:05<00:07,  1.47it/s]Loading test:  57%|█████▋    | 13/23 [00:05<00:06,  1.63it/s]Loading test:  61%|██████    | 14/23 [00:06<00:04,  1.95it/s]Loading test:  65%|██████▌   | 15/23 [00:06<00:03,  2.22it/s]Loading test:  70%|██████▉   | 16/23 [00:06<00:02,  2.47it/s]Loading test:  74%|███████▍  | 17/23 [00:07<00:02,  2.31it/s]Loading test:  78%|███████▊  | 18/23 [00:08<00:02,  1.67it/s]Loading test:  83%|████████▎ | 19/23 [00:08<00:02,  1.70it/s]Loading test:  87%|████████▋ | 20/23 [00:09<00:02,  1.29it/s]Loading test:  91%|█████████▏| 21/23 [00:10<00:01,  1.36it/s]Loading test:  96%|█████████▌| 22/23 [00:11<00:00,  1.46it/s]Loading test: 100%|██████████| 23/23 [00:12<00:00,  1.20it/s]Loading test: 100%|██████████| 23/23 [00:12<00:00,  1.86it/s]
concatenating: validation:   0%|          | 0/23 [00:00<?, ?it/s]concatenating: validation:  26%|██▌       | 6/23 [00:00<00:00, 55.04it/s]concatenating: validation:  52%|█████▏    | 12/23 [00:00<00:00, 55.48it/s]concatenating: validation:  78%|███████▊  | 18/23 [00:00<00:00, 56.16it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 56.14it/s]
Epoch 00030: val_mDice did not improve from 0.74968
{'val_loss': [0.15024844338398533, 0.12038130984720849, 0.1474284405662746, 0.1283446659555638, 0.14111803150041036, 0.14034016842410685, 0.12396394931762933, 0.11335405428996069, 0.09297823481050431, 0.10540593452705702, 0.10843648313189846, 0.13938559566046835, 0.11782505501488869, 0.11245845725849436, 0.09903810489302646, 0.11687897102110335, 0.10153654012852033, 0.09863268470095353, 0.11230911982375508, 0.14199865043953375, 0.14101608135219568, 0.13799092061496104, 0.12953189296967152, 0.1344666914594085, 0.13445408908712292, 0.1458300710064342, 0.1378682376277138, 0.14802565364082115, 0.13697163520883837, 0.13513441200100537], 'val_acc': [0.9329309060747026, 0.9329021150469045, 0.9328120135496636, 0.9329695627368922, 0.9329208431055749, 0.9330021813971193, 0.932894996664821, 0.9328035391802852, 0.9328532399731412, 0.9327474773738593, 0.9328562418864775, 0.9328964518706984, 0.9328955346500242, 0.9329133818681672, 0.9328797776366867, 0.9328386228710743, 0.9329319192066734, 0.9328458702373151, 0.9328829471191848, 0.932907079283318, 0.9329428949867606, 0.932903443473482, 0.9328773765469891, 0.9328905726184681, 0.9329091386201203, 0.9328468128138494, 0.9328116376214786, 0.9328698149885702, 0.9328815503420282, 0.9328848421941115], 'val_mDice': [0.7350036570780759, 0.7426606982822372, 0.7377673207951491, 0.7387382402079026, 0.746807535942621, 0.7496784972865249, 0.7366055829402257, 0.7380597301910696, 0.7376934255389486, 0.7287999756092208, 0.7446526306992541, 0.739051841359196, 0.738734798701124, 0.734352071375647, 0.7424000053495426, 0.7440060198894999, 0.7375118471268661, 0.7331535682247541, 0.7323917500334809, 0.7422594556127906, 0.7396440525365816, 0.7351993556271058, 0.7380499219023491, 0.7374487354267402, 0.7368732042621322, 0.7349450630853561, 0.7352305149473479, 0.7351349812327125, 0.7365694942478775, 0.7365906688714292], 'loss': [0.06676405855799938, 0.06178604494227041, 0.060445671925223335, 0.0597586524215302, 0.05881806418174837, 0.05796168309899568, 0.05837969994349781, 0.0570722040518131, 0.05650463197252629, 0.05682938438694802, 0.056011297404539105, 0.05498735208322506, 0.05505023779232631, 0.05483560784459568, 0.054875075561756956, 0.05405360725279174, 0.05343428328463074, 0.05376223483820136, 0.053586772521737866, 0.05292106369298919, 0.05262659199008121, 0.05154673275676104, 0.051152188572053774, 0.05095846514073566, 0.051222340940793594, 0.05062106728372189, 0.05041913603065764, 0.050472512472384046, 0.0504875738749682, 0.05018206297647653], 'acc': [0.9081285029354662, 0.9084048477275991, 0.9084551980816855, 0.9084845056355862, 0.9085356785156122, 0.9085641930054275, 0.9085750561675677, 0.9086117163379956, 0.9086723311294241, 0.9086584054325159, 0.9087007674931209, 0.9087179934170441, 0.908704704168818, 0.9087706347140119, 0.9087530996902972, 0.908772795961181, 0.9088056511036422, 0.9087798562289555, 0.9088033957056479, 0.9088430526777704, 0.9088471558651484, 0.9089071177746264, 0.9089311921496657, 0.9089373526279017, 0.908922960943424, 0.9089476072143619, 0.9089303584109711, 0.9089625634406235, 0.9089558654310861, 0.9089619118297545], 'mDice': [0.9155968572924542, 0.9253862763032238, 0.9280181751392855, 0.9293913271530657, 0.9312146628347863, 0.9329139397331349, 0.932080952974103, 0.9346638109424327, 0.9357472579622668, 0.9351216625005165, 0.9367543463024218, 0.9387944666193536, 0.9386811481117748, 0.9390400876003826, 0.9390040051365116, 0.9406485621564187, 0.941842559903792, 0.9411952821062616, 0.9415267194760999, 0.9428380884837305, 0.9434492419624909, 0.9455609176327777, 0.9463397533677501, 0.9466804296391117, 0.9462044456943881, 0.9473864605475816, 0.9478129721831839, 0.9476833947723545, 0.9476535962632114, 0.9482690539418153], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005]}

 MultiClass_24567891011121314 SD: [0] GPU: 3 

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 84, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 84, 52, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 84, 52, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 84, 52, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 84, 52, 40)   14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 84, 52, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 84, 52, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 42, 26, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 42, 26, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 42, 26, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 42, 26, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 42, 26, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 42, 26, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 42, 26, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 42, 26, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 42, 26, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 21, 13, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 21, 13, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 21, 13, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________2020-09-30 21:09:16.961690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-09-30 21:09:16.961876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 21:09:16.961892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-09-30 21:09:16.961901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-09-30 21:09:16.962039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

batch_normalization_5 (BatchNor (None, 21, 13, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 21, 13, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 21, 13, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 21, 13, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 21, 13, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 21, 13, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 21, 13, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 42, 26, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 42, 26, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 42, 26, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 42, 26, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 42, 26, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 42, 26, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 42, 26, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 42, 26, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 42, 26, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 42, 26, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 84, 52, 40)   44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 84, 52, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 84, 52, 40)   28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 84, 52, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 84, 52, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 84, 52, 40)   14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 84, 52, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 84, 52, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 84, 52, 120)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 84, 52, 120)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 84, 52, 13)   1573        dropout_5[0][0]                  
==================================================================================================
Total params: 889,253
Trainable params: 887,653
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialization succesfull
class_weights [0.06393493 0.02956725 0.08776437 0.01010358 0.02893323 0.00645468
 0.07982461 0.1164499  0.07824979 0.01317497 0.30405397 0.18117079
 0.00031793]
Train on 2250 samples, validate on 878 samples
Epoch 1/30
 - 12s - loss: 0.2700 - acc: 0.7189 - mDice: 0.7221 - val_loss: 0.3070 - val_acc: 0.7674 - val_mDice: 0.6828

Epoch 00001: val_mDice improved from -inf to 0.68277, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM40/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/30
 - 6s - loss: 0.2525 - acc: 0.7205 - mDice: 0.7409 - val_loss: 0.3050 - val_acc: 0.7675 - val_mDice: 0.6835

Epoch 00002: val_mDice improved from 0.68277 to 0.68352, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM40/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 3/30
 - 6s - loss: 0.2435 - acc: 0.7218 - mDice: 0.7507 - val_loss: 0.2976 - val_acc: 0.7681 - val_mDice: 0.6852

Epoch 00003: val_mDice improved from 0.68352 to 0.68518, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM40/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/30
 - 5s - loss: 0.2358 - acc: 0.7223 - mDice: 0.7590 - val_loss: 0.2915 - val_acc: 0.7683 - val_mDice: 0.6916

Epoch 00004: val_mDice improved from 0.68518 to 0.69157, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM40/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 5/30
 - 5s - loss: 0.2328 - acc: 0.7225 - mDice: 0.7622 - val_loss: 0.2925 - val_acc: 0.7681 - val_mDice: 0.6880

Epoch 00005: val_mDice did not improve from 0.69157
Epoch 6/30
 - 5s - loss: 0.2327 - acc: 0.7227 - mDice: 0.7624 - val_loss: 0.3018 - val_acc: 0.7679 - val_mDice: 0.6877

Epoch 00006: val_mDice did not improve from 0.69157
Epoch 7/30
 - 5s - loss: 0.2261 - acc: 0.7231 - mDice: 0.7695 - val_loss: 0.2903 - val_acc: 0.7678 - val_mDice: 0.6853

Epoch 00007: val_mDice did not improve from 0.69157
Epoch 8/30
 - 5s - loss: 0.2267 - acc: 0.7233 - mDice: 0.7689 - val_loss: 0.3040 - val_acc: 0.7676 - val_mDice: 0.6859

Epoch 00008: val_mDice did not improve from 0.69157
Epoch 9/30
 - 5s - loss: 0.2207 - acc: 0.7235 - mDice: 0.7754 - val_loss: 0.2978 - val_acc: 0.7681 - val_mDice: 0.6901

Epoch 00009: val_mDice did not improve from 0.69157
Epoch 10/30
 - 5s - loss: 0.2175 - acc: 0.7238 - mDice: 0.7788 - val_loss: 0.3033 - val_acc: 0.7674 - val_mDice: 0.6851

Epoch 00010: val_mDice did not improve from 0.69157
Epoch 11/30
 - 5s - loss: 0.2166 - acc: 0.7238 - mDice: 0.7798 - val_loss: 0.3057 - val_acc: 0.7681 - val_mDice: 0.6818

Epoch 00011: val_mDice did not improve from 0.69157
Epoch 12/30
 - 6s - loss: 0.2152 - acc: 0.7240 - mDice: 0.7813 - val_loss: 0.3018 - val_acc: 0.7679 - val_mDice: 0.6868

Epoch 00012: val_mDice did not improve from 0.69157
Epoch 13/30
 - 5s - loss: 0.2134 - acc: 0.7241 - mDice: 0.7833 - val_loss: 0.3024 - val_acc: 0.7680 - val_mDice: 0.6865

Epoch 00013: val_mDice did not improve from 0.69157
Epoch 14/30
 - 6s - loss: 0.2108 - acc: 0.7246 - mDice: 0.7859 - val_loss: 0.3022 - val_acc: 0.7677 - val_mDice: 0.6832

Epoch 00014: val_mDice did not improve from 0.69157
Epoch 15/30
 - 6s - loss: 0.2187 - acc: 0.7239 - mDice: 0.7775 - val_loss: 0.3052 - val_acc: 0.7682 - val_mDice: 0.6840

Epoch 00015: val_mDice did not improve from 0.69157
Epoch 16/30
 - 5s - loss: 0.2089 - acc: 0.7246 - mDice: 0.7882 - val_loss: 0.3060 - val_acc: 0.7678 - val_mDice: 0.6831

Epoch 00016: val_mDice did not improve from 0.69157
Epoch 17/30
 - 5s - loss: 0.2075 - acc: 0.7246 - mDice: 0.7896 - val_loss: 0.3036 - val_acc: 0.7678 - val_mDice: 0.6862

Epoch 00017: val_mDice did not improve from 0.69157
Epoch 18/30
 - 5s - loss: 0.2068 - acc: 0.7250 - mDice: 0.7904 - val_loss: 0.3036 - val_acc: 0.7679 - val_mDice: 0.6861

Epoch 00018: val_mDice did not improve from 0.69157
Epoch 19/30
 - 5s - loss: 0.2085 - acc: 0.7251 - mDice: 0.7885 - val_loss: 0.3027 - val_acc: 0.7675 - val_mDice: 0.6834

Epoch 00019: val_mDice did not improve from 0.69157

Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 20/30
 - 5s - loss: 0.1998 - acc: 0.7254 - mDice: 0.7979 - val_loss: 0.3005 - val_acc: 0.7679 - val_mDice: 0.6871

Epoch 00020: val_mDice did not improve from 0.69157
Epoch 21/30
 - 5s - loss: 0.1962 - acc: 0.7255 - mDice: 0.8018 - val_loss: 0.3060 - val_acc: 0.7679 - val_mDice: 0.6821

Epoch 00021: val_mDice did not improve from 0.69157
Epoch 22/30
 - 5s - loss: 0.1959 - acc: 0.7259 - mDice: 0.8021 - val_loss: 0.3028 - val_acc: 0.7680 - val_mDice: 0.6844

Epoch 00022: val_mDice did not improve from 0.69157
Epoch 23/30
 - 5s - loss: 0.1920 - acc: 0.7259 - mDice: 0.8063 - val_loss: 0.3025 - val_acc: 0.7681 - val_mDice: 0.6864

Epoch 00023: val_mDice did not improve from 0.69157
Epoch 24/30
 - 5s - loss: 0.1940 - acc: 0.7258 - mDice: 0.8042 - val_loss: 0.3001 - val_acc: 0.7680 - val_mDice: 0.6863

Epoch 00024: val_mDice did not improve from 0.69157
Epoch 25/30
 - 5s - loss: 0.1916 - acc: 0.7260 - mDice: 0.8069 - val_loss: 0.3034 - val_acc: 0.7680 - val_mDice: 0.6852

Epoch 00025: val_mDice did not improve from 0.69157
Epoch 26/30
 - 5s - loss: 0.1889 - acc: 0.7262 - mDice: 0.8098 - val_loss: 0.2974 - val_acc: 0.7680 - val_mDice: 0.6851

Epoch 00026: val_mDice did not improve from 0.69157
Epoch 27/30
 - 5s - loss: 0.1913 - acc: 0.7261 - mDice: 0.8071 - val_loss: 0.3001 - val_acc: 0.7684 - val_mDice: 0.6869

Epoch 00027: val_mDice did not improve from 0.69157
Epoch 28/30
 - 5s - loss: 0.1878 - acc: 0.7262 - mDice: 0.8109 - val_loss: 0.3031 - val_acc: 0.7681 - val_mDice: 0.6842

Epoch 00028: val_mDice did not improve from 0.69157
Epoch 29/30
 - 5s - loss: 0.1874 - acc: 0.7263 - mDice: 0.8113 - val_loss: 0.3034 - val_acc: 0.7681 - val_mDice: 0.6839

Epoch 00029: val_mDice did not improve from 0.69157
Epoch 30/30
 - 5s - loss: 0.1895 - acc: 0.7263 - mDice: 0.8091 - val_loss: 0.3040 - val_acc: 0.7681 - val_mDice: 0.6852

predicting test subjects:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects:   4%|▍         | 1/23 [00:01<00:30,  1.40s/it]predicting test subjects:   9%|▊         | 2/23 [00:01<00:23,  1.11s/it]predicting test subjects:  13%|█▎        | 3/23 [00:02<00:18,  1.06it/s]predicting test subjects:  17%|█▋        | 4/23 [00:02<00:15,  1.21it/s]predicting test subjects:  22%|██▏       | 5/23 [00:03<00:13,  1.38it/s]predicting test subjects:  26%|██▌       | 6/23 [00:03<00:11,  1.50it/s]predicting test subjects:  30%|███       | 7/23 [00:04<00:09,  1.69it/s]predicting test subjects:  35%|███▍      | 8/23 [00:04<00:08,  1.77it/s]predicting test subjects:  39%|███▉      | 9/23 [00:05<00:08,  1.68it/s]predicting test subjects:  43%|████▎     | 10/23 [00:06<00:07,  1.63it/s]predicting test subjects:  48%|████▊     | 11/23 [00:06<00:07,  1.56it/s]predicting test subjects:  52%|█████▏    | 12/23 [00:07<00:07,  1.57it/s]predicting test subjects:  57%|█████▋    | 13/23 [00:08<00:06,  1.64it/s]predicting test subjects:  61%|██████    | 14/23 [00:08<00:05,  1.69it/s]predicting test subjects:  65%|██████▌   | 15/23 [00:09<00:04,  1.68it/s]predicting test subjects:  70%|██████▉   | 16/23 [00:09<00:04,  1.66it/s]predicting test subjects:  74%|███████▍  | 17/23 [00:10<00:03,  1.75it/s]predicting test subjects:  78%|███████▊  | 18/23 [00:10<00:02,  1.74it/s]predicting test subjects:  83%|████████▎ | 19/23 [00:11<00:02,  1.75it/s]predicting test subjects:  87%|████████▋ | 20/23 [00:12<00:01,  1.51it/s]predicting test subjects:  91%|█████████▏| 21/23 [00:13<00:01,  1.46it/s]predicting test subjects:  96%|█████████▌| 22/23 [00:13<00:00,  1.53it/s]predicting test subjects: 100%|██████████| 23/23 [00:14<00:00,  1.53it/s]predicting test subjects: 100%|██████████| 23/23 [00:14<00:00,  1.60it/s]
Loading train:   0%|          | 0/59 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/59 [00:00<00:10,  5.68it/s]Loading train:   3%|▎         | 2/59 [00:00<00:08,  6.43it/s]Loading train:   5%|▌         | 3/59 [00:00<00:09,  6.13it/s]Loading train:   7%|▋         | 4/59 [00:00<00:09,  6.00it/s]Loading train:   8%|▊         | 5/59 [00:00<00:09,  5.93it/s]Loading train:  10%|█         | 6/59 [00:00<00:08,  5.99it/s]Loading train:  12%|█▏        | 7/59 [00:01<00:08,  5.83it/s]Loading train:  14%|█▎        | 8/59 [00:01<00:08,  6.28it/s]Loading train:  15%|█▌        | 9/59 [00:01<00:08,  6.18it/s]Loading train:  17%|█▋        | 10/59 [00:01<00:08,  6.10it/s]Loading train:  19%|█▊        | 11/59 [00:01<00:08,  5.85it/s]Loading train:  20%|██        | 12/59 [00:01<00:07,  6.07it/s]Loading train:  22%|██▏       | 13/59 [00:02<00:07,  6.19it/s]Loading train:  24%|██▎       | 14/59 [00:02<00:07,  6.28it/s]Loading train:  25%|██▌       | 15/59 [00:02<00:06,  6.53it/s]Loading train:  27%|██▋       | 16/59 [00:02<00:06,  6.35it/s]Loading train:  29%|██▉       | 17/59 [00:02<00:06,  6.38it/s]Loading train:  31%|███       | 18/59 [00:02<00:06,  6.13it/s]Loading train:  32%|███▏      | 19/59 [00:03<00:06,  6.07it/s]Loading train:  34%|███▍      | 20/59 [00:03<00:06,  6.15it/s]Loading train:  36%|███▌      | 21/59 [00:03<00:05,  6.61it/s]Loading train:  37%|███▋      | 22/59 [00:03<00:05,  6.59it/s]Loading train:  39%|███▉      | 23/59 [00:03<00:07,  5.14it/s]Loading train:  41%|████      | 24/59 [00:03<00:06,  5.60it/s]Loading train:  42%|████▏     | 25/59 [00:04<00:05,  6.34it/s]Loading train:  44%|████▍     | 26/59 [00:04<00:04,  6.83it/s]Loading train:  46%|████▌     | 27/59 [00:04<00:04,  7.02it/s]Loading train:  47%|████▋     | 28/59 [00:04<00:04,  7.40it/s]Loading train:  49%|████▉     | 29/59 [00:04<00:04,  7.32it/s]Loading train:  51%|█████     | 30/59 [00:04<00:03,  7.60it/s]Loading train:  53%|█████▎    | 31/59 [00:04<00:03,  7.73it/s]Loading train:  54%|█████▍    | 32/59 [00:04<00:03,  7.79it/s]Loading train:  56%|█████▌    | 33/59 [00:05<00:03,  7.74it/s]Loading train:  59%|█████▉    | 35/59 [00:05<00:02,  8.81it/s]Loading train:  61%|██████    | 36/59 [00:05<00:02,  7.84it/s]Loading train:  63%|██████▎   | 37/59 [00:05<00:02,  7.69it/s]Loading train:  66%|██████▌   | 39/59 [00:05<00:02,  8.93it/s]Loading train:  68%|██████▊   | 40/59 [00:05<00:02,  8.20it/s]Loading train:  69%|██████▉   | 41/59 [00:05<00:02,  7.73it/s]Loading train:  71%|███████   | 42/59 [00:06<00:02,  7.69it/s]Loading train:  73%|███████▎  | 43/59 [00:06<00:02,  7.62it/s]Loading train:  75%|███████▍  | 44/59 [00:06<00:02,  7.47it/s]Loading train:  76%|███████▋  | 45/59 [00:06<00:01,  7.66it/s]Loading train:  78%|███████▊  | 46/59 [00:06<00:01,  7.44it/s]Loading train:  80%|███████▉  | 47/59 [00:06<00:01,  7.44it/s]Loading train:  81%|████████▏ | 48/59 [00:06<00:01,  7.45it/s]Loading train:  83%|████████▎ | 49/59 [00:07<00:01,  7.75it/s]Loading train:  85%|████████▍ | 50/59 [00:07<00:01,  8.01it/s]Loading train:  86%|████████▋ | 51/59 [00:07<00:00,  8.29it/s]Loading train:  88%|████████▊ | 52/59 [00:07<00:00,  8.47it/s]Loading train:  90%|████████▉ | 53/59 [00:07<00:00,  8.74it/s]Loading train:  92%|█████████▏| 54/59 [00:07<00:00,  8.83it/s]Loading train:  93%|█████████▎| 55/59 [00:07<00:00,  8.11it/s]Loading train:  95%|█████████▍| 56/59 [00:07<00:00,  8.36it/s]Loading train:  97%|█████████▋| 57/59 [00:07<00:00,  8.30it/s]Loading train:  98%|█████████▊| 58/59 [00:08<00:00,  8.22it/s]Loading train: 100%|██████████| 59/59 [00:08<00:00,  8.08it/s]Loading train: 100%|██████████| 59/59 [00:08<00:00,  7.20it/s]
concatenating: train:   0%|          | 0/59 [00:00<?, ?it/s]concatenating: train:   5%|▌         | 3/59 [00:00<00:02, 23.50it/s]concatenating: train:  10%|█         | 6/59 [00:00<00:02, 23.58it/s]concatenating: train:  15%|█▌        | 9/59 [00:00<00:02, 23.22it/s]concatenating: train:  20%|██        | 12/59 [00:00<00:02, 23.28it/s]concatenating: train:  25%|██▌       | 15/59 [00:00<00:01, 23.22it/s]concatenating: train:  31%|███       | 18/59 [00:00<00:01, 22.14it/s]concatenating: train:  36%|███▌      | 21/59 [00:00<00:01, 22.28it/s]concatenating: train:  41%|████      | 24/59 [00:01<00:01, 22.22it/s]concatenating: train:  46%|████▌     | 27/59 [00:01<00:01, 22.73it/s]concatenating: train:  51%|█████     | 30/59 [00:01<00:01, 23.50it/s]concatenating: train:  56%|█████▌    | 33/59 [00:01<00:01, 24.14it/s]concatenating: train:  63%|██████▎   | 37/59 [00:01<00:00, 26.64it/s]concatenating: train:  69%|██████▉   | 41/59 [00:01<00:00, 28.93it/s]concatenating: train:  75%|███████▍  | 44/59 [00:01<00:00, 28.31it/s]concatenating: train:  80%|███████▉  | 47/59 [00:01<00:00, 28.10it/s]concatenating: train:  85%|████████▍ | 50/59 [00:01<00:00, 27.68it/s]concatenating: train:  92%|█████████▏| 54/59 [00:02<00:00, 28.62it/s]concatenating: train:  98%|█████████▊| 58/59 [00:02<00:00, 29.23it/s]concatenating: train: 100%|██████████| 59/59 [00:02<00:00, 26.01it/s]
Loading test:   0%|          | 0/23 [00:00<?, ?it/s]Loading test:   4%|▍         | 1/23 [00:00<00:03,  5.98it/s]Loading test:   9%|▊         | 2/23 [00:00<00:03,  6.67it/s]Loading test:  13%|█▎        | 3/23 [00:00<00:02,  7.09it/s]Loading test:  17%|█▋        | 4/23 [00:00<00:02,  7.55it/s]Loading test:  22%|██▏       | 5/23 [00:00<00:02,  7.59it/s]Loading test:  26%|██▌       | 6/23 [00:00<00:02,  7.88it/s]Loading test:  35%|███▍      | 8/23 [00:00<00:01,  8.17it/s]Loading test:  39%|███▉      | 9/23 [00:01<00:02,  6.82it/s]Loading test:  43%|████▎     | 10/23 [00:01<00:02,  6.32it/s]Loading test:  48%|████▊     | 11/23 [00:01<00:02,  5.78it/s]Loading test:  52%|█████▏    | 12/23 [00:01<00:01,  6.06it/s]Loading test:  57%|█████▋    | 13/23 [00:01<00:01,  6.19it/s]Loading test:  61%|██████    | 14/23 [00:02<00:01,  6.58it/s]Loading test:  65%|██████▌   | 15/23 [00:02<00:01,  6.66it/s]Loading test:  70%|██████▉   | 16/23 [00:02<00:01,  6.42it/s]Loading test:  74%|███████▍  | 17/23 [00:02<00:00,  6.55it/s]Loading test:  78%|███████▊  | 18/23 [00:02<00:00,  6.48it/s]Loading test:  83%|████████▎ | 19/23 [00:02<00:00,  6.31it/s]Loading test:  87%|████████▋ | 20/23 [00:03<00:00,  5.51it/s]Loading test:  91%|█████████▏| 21/23 [00:03<00:00,  5.59it/s]Loading test:  96%|█████████▌| 22/23 [00:03<00:00,  5.88it/s]Loading test: 100%|██████████| 23/23 [00:03<00:00,  6.00it/s]Loading test: 100%|██████████| 23/23 [00:03<00:00,  6.56it/s]
concatenating: validation:   0%|          | 0/23 [00:00<?, ?it/s]concatenating: validation:  13%|█▎        | 3/23 [00:00<00:00, 22.59it/s]concatenating: validation:  26%|██▌       | 6/23 [00:00<00:00, 22.98it/s]concatenating: validation:  39%|███▉      | 9/23 [00:00<00:00, 23.77it/s]concatenating: validation:  52%|█████▏    | 12/23 [00:00<00:00, 23.63it/s]concatenating: validation:  65%|██████▌   | 15/23 [00:00<00:00, 24.13it/s]concatenating: validation:  78%|███████▊  | 18/23 [00:00<00:00, 24.59it/s]concatenating: validation:  91%|█████████▏| 21/23 [00:00<00:00, 24.21it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 24.68it/s]
Epoch 00030: val_mDice did not improve from 0.69157
{'val_loss': [0.306980049114564, 0.30495392587163184, 0.29756052411906, 0.29154561880916563, 0.29247609500461397, 0.30175790811861297, 0.2902935571578206, 0.30402649586048647, 0.2978130824405675, 0.3032819025201515, 0.3056623938075502, 0.30176556381929437, 0.30238088581855316, 0.30217833447021886, 0.3051794438468025, 0.30601547722935946, 0.30360865613322596, 0.3036355670193605, 0.30274061654993506, 0.300483918543014, 0.3060320703681889, 0.30279954575460427, 0.3025103724287963, 0.3001266083847689, 0.3033702926103509, 0.29736844122681366, 0.3000596899853209, 0.3031326188177617, 0.30335498752246415, 0.3040222241829631], 'val_acc': [0.7673752762733668, 0.7675267735619208, 0.768053228396218, 0.7682946804308403, 0.7681392755758246, 0.7679218130529877, 0.767820636476634, 0.7676297688945822, 0.7681390106134914, 0.7674352454704685, 0.7680722580955349, 0.7679186781745294, 0.767958311054321, 0.7676647073315597, 0.7681752544464989, 0.7678391493534444, 0.7677684874360816, 0.7679270209239665, 0.7674928754907534, 0.7678866057841273, 0.7678946947725596, 0.768031587090199, 0.7681356198695363, 0.7679997773539775, 0.7679502306875172, 0.7679948201890958, 0.7683606479732756, 0.7681272765091177, 0.7681004168096599, 0.7681014648466827], 'val_mDice': [0.682773933052202, 0.683515427867481, 0.6851823864873829, 0.6915731370313293, 0.6879560982719369, 0.6877461610731068, 0.6853152015214629, 0.6858884853762754, 0.6900720491767744, 0.6851422640077074, 0.6818152651156815, 0.6868189858412689, 0.6864919067786875, 0.6831591199905291, 0.6839539551246139, 0.6831264824421911, 0.6862046136128332, 0.6860982345285611, 0.6833675893401231, 0.6871288983588338, 0.682138356912652, 0.6844414888047412, 0.6864291787962165, 0.6862691545812307, 0.6852190208054892, 0.6850549789115888, 0.6868616768873905, 0.6842100062239957, 0.6838696392630662, 0.6851693527573605], 'loss': [0.26999536355336506, 0.2525458417336146, 0.2434698764483134, 0.2358293447891871, 0.23283219635486602, 0.232711318731308, 0.22611088156700135, 0.22667178471883137, 0.22066239575544994, 0.21745329697926838, 0.216623149116834, 0.21516944706439972, 0.21335522731145223, 0.21084412515163423, 0.21869032422701518, 0.20887693842252095, 0.2074782117207845, 0.2067517743508021, 0.2085097680489222, 0.1998131936788559, 0.19620476961135863, 0.19588652948538463, 0.1920421423514684, 0.19403946657975515, 0.19155034442742666, 0.18885120848814646, 0.19128336290518444, 0.18784596979618073, 0.18742151618003844, 0.1895475721359253], 'acc': [0.7188748455047608, 0.7205397836367289, 0.7217762517929077, 0.7223175644874573, 0.7225115974744161, 0.722721611658732, 0.723148656686147, 0.7233181715011596, 0.7235400915145874, 0.723784290154775, 0.7237809324264526, 0.7239906374613444, 0.7240935118993124, 0.7245567758878072, 0.7239446449279785, 0.7246132469177247, 0.7246375687917074, 0.7249857568740845, 0.7251203735669454, 0.725357961654663, 0.7255424308776856, 0.7258776958783467, 0.7259475978215536, 0.7258439151446024, 0.7259725244839986, 0.7261525265375773, 0.7261063281695048, 0.7262309741973877, 0.7262517301241557, 0.7262654638290406], 'mDice': [0.7221033438046773, 0.7409312319755554, 0.7507292000452678, 0.7590169016520182, 0.7622306672732035, 0.7623707485198975, 0.7695141688982645, 0.7689088519414266, 0.7753835368156433, 0.7788498695691427, 0.7797651839256287, 0.7813291732470194, 0.7832894571622213, 0.7859499128659566, 0.7775191648801167, 0.7881548579533895, 0.7896181178092957, 0.7904010327657064, 0.7885249050458273, 0.7979358434677124, 0.8018427880605062, 0.8021120858192444, 0.8063214381535848, 0.8041660602887472, 0.8068592754999796, 0.8097598497072855, 0.8070973467826843, 0.8108584396044414, 0.8112860735257467, 0.8090762797991434], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005]}

 1-THALAMUS SD: [1] GPU: 3 

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 112, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 112, 116, 30) 300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 112, 116, 30) 120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 112, 116, 30) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 112, 116, 30) 8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 112, 116, 30) 120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 112, 116, 30) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 56, 58, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 56, 58, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 56, 58, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 56, 58, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 56, 58, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 56, 58, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 56, 58, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 56, 58, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 58, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 28, 29, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 28, 29, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 28, 29, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 28, 29, 120)  480         conv2d_5[0][0]                   2020-09-30 21:12:59.915684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-09-30 21:12:59.915806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 21:12:59.915825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-09-30 21:12:59.915838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-09-30 21:12:59.915995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
activation_5 (Activation)       (None, 28, 29, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 28, 29, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 28, 29, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 28, 29, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 29, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 28, 29, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 56, 58, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 56, 58, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 56, 58, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 56, 58, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 56, 58, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 56, 58, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 56, 58, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 56, 58, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 56, 58, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 56, 58, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 112, 116, 30) 25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 112, 116, 60) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 112, 116, 30) 16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 112, 116, 30) 120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 112, 116, 30) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 112, 116, 30) 8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 112, 116, 30) 120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 112, 116, 30) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 112, 116, 90) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 112, 116, 90) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 112, 116, 2)  182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 499,142
Non-trainable params: 1,200
__________________________________________________________________________________________________
 --- initialization failed
class_weights [0.98268488 0.01731512]
Train on 6178 samples, validate on 2524 samples
Epoch 1/30
 - 26s - loss: 0.1389 - acc: 0.9054 - mDice: 0.7787 - val_loss: 0.1997 - val_acc: 0.9341 - val_mDice: 0.6382

Epoch 00001: val_mDice improved from -inf to 0.63822, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM30/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/30
 - 21s - loss: 0.0765 - acc: 0.9115 - mDice: 0.8945 - val_loss: 0.1917 - val_acc: 0.9346 - val_mDice: 0.6538

Epoch 00002: val_mDice improved from 0.63822 to 0.65379, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM30/1-THALAMUS/sd1/best_model_weights.h5
Epoch 3/30
 - 21s - loss: 0.0700 - acc: 0.9118 - mDice: 0.9071 - val_loss: 0.2012 - val_acc: 0.9344 - val_mDice: 0.6349

Epoch 00003: val_mDice did not improve from 0.65379
Epoch 4/30
 - 21s - loss: 0.0673 - acc: 0.9120 - mDice: 0.9125 - val_loss: 0.1887 - val_acc: 0.9348 - val_mDice: 0.6596

Epoch 00004: val_mDice improved from 0.65379 to 0.65964, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM30/1-THALAMUS/sd1/best_model_weights.h5
Epoch 5/30
 - 21s - loss: 0.0648 - acc: 0.9121 - mDice: 0.9173 - val_loss: 0.1833 - val_acc: 0.9351 - val_mDice: 0.6704

Epoch 00005: val_mDice improved from 0.65964 to 0.67042, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM30/1-THALAMUS/sd1/best_model_weights.h5
Epoch 6/30
 - 21s - loss: 0.0632 - acc: 0.9122 - mDice: 0.9204 - val_loss: 0.1872 - val_acc: 0.9349 - val_mDice: 0.6626

Epoch 00006: val_mDice did not improve from 0.67042
Epoch 7/30
 - 21s - loss: 0.0623 - acc: 0.9122 - mDice: 0.9222 - val_loss: 0.1797 - val_acc: 0.9350 - val_mDice: 0.6776

Epoch 00007: val_mDice improved from 0.67042 to 0.67760, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM30/1-THALAMUS/sd1/best_model_weights.h5
Epoch 8/30
 - 20s - loss: 0.0616 - acc: 0.9122 - mDice: 0.9237 - val_loss: 0.1772 - val_acc: 0.9352 - val_mDice: 0.6824

Epoch 00008: val_mDice improved from 0.67760 to 0.68242, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM30/1-THALAMUS/sd1/best_model_weights.h5
Epoch 9/30
 - 21s - loss: 0.0602 - acc: 0.9123 - mDice: 0.9262 - val_loss: 0.1779 - val_acc: 0.9351 - val_mDice: 0.6810

Epoch 00009: val_mDice did not improve from 0.68242
Epoch 10/30
 - 21s - loss: 0.0599 - acc: 0.9123 - mDice: 0.9268 - val_loss: 0.1837 - val_acc: 0.9352 - val_mDice: 0.6694

Epoch 00010: val_mDice did not improve from 0.68242
Epoch 11/30
 - 21s - loss: 0.0592 - acc: 0.9124 - mDice: 0.9284 - val_loss: 0.1807 - val_acc: 0.9351 - val_mDice: 0.6755

Epoch 00011: val_mDice did not improve from 0.68242
Epoch 12/30
 - 21s - loss: 0.0587 - acc: 0.9124 - mDice: 0.9292 - val_loss: 0.1809 - val_acc: 0.9352 - val_mDice: 0.6750

Epoch 00012: val_mDice did not improve from 0.68242
Epoch 13/30
 - 21s - loss: 0.0591 - acc: 0.9124 - mDice: 0.9285 - val_loss: 0.1818 - val_acc: 0.9352 - val_mDice: 0.6732

Epoch 00013: val_mDice did not improve from 0.68242
Epoch 14/30
 - 21s - loss: 0.0578 - acc: 0.9125 - mDice: 0.9309 - val_loss: 0.1848 - val_acc: 0.9348 - val_mDice: 0.6675

Epoch 00014: val_mDice did not improve from 0.68242
Epoch 15/30
 - 21s - loss: 0.0573 - acc: 0.9125 - mDice: 0.9321 - val_loss: 0.1743 - val_acc: 0.9352 - val_mDice: 0.6882

Epoch 00015: val_mDice improved from 0.68242 to 0.68816, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM30/1-THALAMUS/sd1/best_model_weights.h5
Epoch 16/30
 - 21s - loss: 0.0572 - acc: 0.9125 - mDice: 0.9322 - val_loss: 0.1747 - val_acc: 0.9353 - val_mDice: 0.6875

Epoch 00016: val_mDice did not improve from 0.68816
Epoch 17/30
 - 21s - loss: 0.0567 - acc: 0.9125 - mDice: 0.9332 - val_loss: 0.1769 - val_acc: 0.9352 - val_mDice: 0.6831

Epoch 00017: val_mDice did not improve from 0.68816
Epoch 18/30
 - 21s - loss: 0.0564 - acc: 0.9125 - mDice: 0.9338 - val_loss: 0.1797 - val_acc: 0.9352 - val_mDice: 0.6775

Epoch 00018: val_mDice did not improve from 0.68816
Epoch 19/30
 - 21s - loss: 0.0558 - acc: 0.9126 - mDice: 0.9350 - val_loss: 0.1762 - val_acc: 0.9352 - val_mDice: 0.6844

Epoch 00019: val_mDice did not improve from 0.68816
Epoch 20/30
 - 21s - loss: 0.0557 - acc: 0.9125 - mDice: 0.9351 - val_loss: 0.1811 - val_acc: 0.9351 - val_mDice: 0.6746

Epoch 00020: val_mDice did not improve from 0.68816
Epoch 21/30
 - 21s - loss: 0.0551 - acc: 0.9126 - mDice: 0.9363 - val_loss: 0.1818 - val_acc: 0.9350 - val_mDice: 0.6733

Epoch 00021: val_mDice did not improve from 0.68816
Epoch 22/30
 - 21s - loss: 0.0551 - acc: 0.9126 - mDice: 0.9363 - val_loss: 0.1826 - val_acc: 0.9350 - val_mDice: 0.6717

Epoch 00022: val_mDice did not improve from 0.68816
Epoch 23/30
 - 21s - loss: 0.0550 - acc: 0.9126 - mDice: 0.9366 - val_loss: 0.1772 - val_acc: 0.9352 - val_mDice: 0.6824

Epoch 00023: val_mDice did not improve from 0.68816
Epoch 24/30
 - 21s - loss: 0.0549 - acc: 0.9126 - mDice: 0.9368 - val_loss: 0.1814 - val_acc: 0.9350 - val_mDice: 0.6742

Epoch 00024: val_mDice did not improve from 0.68816
Epoch 25/30
 - 21s - loss: 0.0541 - acc: 0.9126 - mDice: 0.9383 - val_loss: 0.1757 - val_acc: 0.9353 - val_mDice: 0.6855

Epoch 00025: val_mDice did not improve from 0.68816
Epoch 26/30
 - 21s - loss: 0.0541 - acc: 0.9126 - mDice: 0.9383 - val_loss: 0.1754 - val_acc: 0.9353 - val_mDice: 0.6859

Epoch 00026: val_mDice did not improve from 0.68816
Epoch 27/30
 - 21s - loss: 0.0537 - acc: 0.9127 - mDice: 0.9390 - val_loss: 0.1806 - val_acc: 0.9351 - val_mDice: 0.6756

Epoch 00027: val_mDice did not improve from 0.68816
Epoch 28/30
 - 21s - loss: 0.0537 - acc: 0.9127 - mDice: 0.9392 - val_loss: 0.1789 - val_acc: 0.9352 - val_mDice: 0.6789

Epoch 00028: val_mDice did not improve from 0.68816
Epoch 29/30
 - 21s - loss: 0.0534 - acc: 0.9127 - mDice: 0.9396 - val_loss: 0.1790 - val_acc: 0.9352 - val_mDice: 0.6787

Epoch 00029: val_mDice did not improve from 0.68816
Epoch 30/30
 - 21s - loss: 0.0533 - acc: 0.9127 - mDice: 0.9399 - val_loss: 0.1776 - val_acc: 0.9352 - val_mDice: 0.6815

predicting test subjects:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects:   4%|▍         | 1/23 [00:00<00:20,  1.05it/s]predicting test subjects:   9%|▊         | 2/23 [00:01<00:14,  1.41it/s]predicting test subjects:  13%|█▎        | 3/23 [00:01<00:11,  1.80it/s]predicting test subjects:  17%|█▋        | 4/23 [00:01<00:08,  2.12it/s]predicting test subjects:  22%|██▏       | 5/23 [00:01<00:07,  2.46it/s]predicting test subjects:  26%|██▌       | 6/23 [00:02<00:05,  2.90it/s]predicting test subjects:  30%|███       | 7/23 [00:02<00:05,  3.09it/s]predicting test subjects:  35%|███▍      | 8/23 [00:02<00:04,  3.10it/s]predicting test subjects:  39%|███▉      | 9/23 [00:02<00:04,  3.02it/s]predicting test subjects:  43%|████▎     | 10/23 [00:03<00:04,  2.99it/s]predicting test subjects:  48%|████▊     | 11/23 [00:03<00:04,  2.81it/s]predicting test subjects:  52%|█████▏    | 12/23 [00:04<00:03,  2.90it/s]predicting test subjects:  57%|█████▋    | 13/23 [00:04<00:03,  2.97it/s]predicting test subjects:  61%|██████    | 14/23 [00:04<00:03,  2.98it/s]predicting test subjects:  65%|██████▌   | 15/23 [00:04<00:02,  3.23it/s]predicting test subjects:  70%|██████▉   | 16/23 [00:05<00:02,  3.10it/s]predicting test subjects:  74%|███████▍  | 17/23 [00:05<00:01,  3.31it/s]predicting test subjects:  78%|███████▊  | 18/23 [00:05<00:01,  3.15it/s]predicting test subjects:  83%|████████▎ | 19/23 [00:06<00:01,  3.10it/s]predicting test subjects:  87%|████████▋ | 20/23 [00:06<00:00,  3.15it/s]predicting test subjects:  91%|█████████▏| 21/23 [00:06<00:00,  3.40it/s]predicting test subjects:  96%|█████████▌| 22/23 [00:06<00:00,  3.69it/s]predicting test subjects: 100%|██████████| 23/23 [00:07<00:00,  3.64it/s]predicting test subjects: 100%|██████████| 23/23 [00:07<00:00,  3.16it/s]
predicting train subjects:   0%|          | 0/59 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/59 [00:00<00:11,  5.15it/s]predicting train subjects:   3%|▎         | 2/59 [00:00<00:11,  4.95it/s]predicting train subjects:   5%|▌         | 3/59 [00:00<00:11,  4.72it/s]predicting train subjects:   7%|▋         | 4/59 [00:00<00:11,  4.59it/s]predicting train subjects:   8%|▊         | 5/59 [00:01<00:14,  3.76it/s]predicting train subjects:  10%|█         | 6/59 [00:01<00:14,  3.62it/s]predicting train subjects:  12%|█▏        | 7/59 [00:01<00:13,  3.75it/s]predicting train subjects:  14%|█▎        | 8/59 [00:02<00:12,  3.93it/s]predicting train subjects:  15%|█▌        | 9/59 [00:02<00:12,  3.95it/s]predicting train subjects:  17%|█▋        | 10/59 [00:02<00:12,  3.90it/s]predicting train subjects:  19%|█▊        | 11/59 [00:02<00:12,  3.74it/s]predicting train subjects:  20%|██        | 12/59 [00:03<00:11,  3.99it/s]predicting train subjects:  22%|██▏       | 13/59 [00:03<00:10,  4.31it/s]predicting train subjects:  24%|██▎       | 14/59 [00:03<00:10,  4.31it/s]predicting train subjects:  25%|██▌       | 15/59 [00:03<00:10,  4.29it/s]predicting train subjects:  27%|██▋       | 16/59 [00:03<00:09,  4.44it/s]predicting train subjects:  29%|██▉       | 17/59 [00:04<00:09,  4.46it/s]predicting train subjects:  31%|███       | 18/59 [00:04<00:09,  4.33it/s]predicting train subjects:  32%|███▏      | 19/59 [00:04<00:10,  3.88it/s]predicting train subjects:  34%|███▍      | 20/59 [00:05<00:12,  3.24it/s]predicting train subjects:  36%|███▌      | 21/59 [00:05<00:11,  3.20it/s]predicting train subjects:  37%|███▋      | 22/59 [00:05<00:10,  3.43it/s]predicting train subjects:  39%|███▉      | 23/59 [00:06<00:11,  3.12it/s]predicting train subjects:  41%|████      | 24/59 [00:06<00:10,  3.32it/s]predicting train subjects:  42%|████▏     | 25/59 [00:06<00:08,  3.79it/s]predicting train subjects:  44%|████▍     | 26/59 [00:06<00:07,  4.17it/s]predicting train subjects:  46%|████▌     | 27/59 [00:06<00:07,  4.37it/s]predicting train subjects:  47%|████▋     | 28/59 [00:07<00:06,  4.73it/s]predicting train subjects:  49%|████▉     | 29/59 [00:07<00:05,  5.17it/s]predicting train subjects:  51%|█████     | 30/59 [00:07<00:05,  5.01it/s]predicting train subjects:  53%|█████▎    | 31/59 [00:07<00:05,  5.22it/s]predicting train subjects:  54%|█████▍    | 32/59 [00:07<00:05,  5.29it/s]predicting train subjects:  56%|█████▌    | 33/59 [00:08<00:05,  4.51it/s]predicting train subjects:  58%|█████▊    | 34/59 [00:08<00:05,  4.71it/s]predicting train subjects:  61%|██████    | 36/59 [00:08<00:04,  5.05it/s]predicting train subjects:  63%|██████▎   | 37/59 [00:08<00:04,  5.04it/s]predicting train subjects:  64%|██████▍   | 38/59 [00:08<00:03,  5.83it/s]predicting train subjects:  66%|██████▌   | 39/59 [00:09<00:03,  5.50it/s]predicting train subjects:  68%|██████▊   | 40/59 [00:09<00:04,  4.40it/s]predicting train subjects:  69%|██████▉   | 41/59 [00:09<00:04,  4.48it/s]predicting train subjects:  71%|███████   | 42/59 [00:09<00:03,  4.62it/s]predicting train subjects:  73%|███████▎  | 43/59 [00:10<00:03,  4.78it/s]predicting train subjects:  75%|███████▍  | 44/59 [00:10<00:02,  5.04it/s]predicting train subjects:  76%|███████▋  | 45/59 [00:10<00:02,  5.29it/s]predicting train subjects:  78%|███████▊  | 46/59 [00:10<00:02,  4.92it/s]predicting train subjects:  80%|███████▉  | 47/59 [00:10<00:02,  4.59it/s]predicting train subjects:  81%|████████▏ | 48/59 [00:11<00:02,  4.79it/s]predicting train subjects:  83%|████████▎ | 49/59 [00:11<00:02,  4.44it/s]predicting train subjects:  85%|████████▍ | 50/59 [00:11<00:01,  4.70it/s]predicting train subjects:  86%|████████▋ | 51/59 [00:11<00:01,  4.93it/s]predicting train subjects:  88%|████████▊ | 52/59 [00:11<00:01,  5.20it/s]predicting train subjects:  90%|████████▉ | 53/59 [00:12<00:01,  5.32it/s]predicting train subjects:  92%|█████████▏| 54/59 [00:12<00:00,  5.45it/s]predicting train subjects:  93%|█████████▎| 55/59 [00:12<00:00,  4.74it/s]predicting train subjects:  95%|█████████▍| 56/59 [00:12<00:00,  5.02it/s]predicting train subjects:  97%|█████████▋| 57/59 [00:12<00:00,  5.01it/s]predicting train subjects:  98%|█████████▊| 58/59 [00:13<00:00,  5.17it/s]predicting train subjects: 100%|██████████| 59/59 [00:13<00:00,  5.19it/s]predicting train subjects: 100%|██████████| 59/59 [00:13<00:00,  4.46it/s]
saving BB  test1-THALAMUS:   0%|          | 0/23 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  26%|██▌       | 6/23 [00:00<00:00, 51.85it/s]saving BB  test1-THALAMUS:  43%|████▎     | 10/23 [00:00<00:00, 46.26it/s]saving BB  test1-THALAMUS:  65%|██████▌   | 15/23 [00:00<00:00, 45.82it/s]saving BB  test1-THALAMUS:  87%|████████▋ | 20/23 [00:00<00:00, 43.73it/s]saving BB  test1-THALAMUS: 100%|██████████| 23/23 [00:00<00:00, 42.80it/s]
saving BB  train1-THALAMUS:   0%|          | 0/59 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   8%|▊         | 5/59 [00:00<00:01, 48.55it/s]saving BB  train1-THALAMUS:  17%|█▋        | 10/59 [00:00<00:01, 46.30it/s]saving BB  train1-THALAMUS:  25%|██▌       | 15/59 [00:00<00:00, 46.90it/s]saving BB  train1-THALAMUS:  34%|███▍      | 20/59 [00:00<00:00, 44.99it/s]saving BB  train1-THALAMUS:  41%|████      | 24/59 [00:00<00:00, 40.73it/s]saving BB  train1-THALAMUS:  49%|████▉     | 29/59 [00:00<00:00, 42.27it/s]saving BB  train1-THALAMUS:  59%|█████▉    | 35/59 [00:00<00:00, 45.22it/s]saving BB  train1-THALAMUS:  69%|██████▉   | 41/59 [00:00<00:00, 48.55it/s]saving BB  train1-THALAMUS:  80%|███████▉  | 47/59 [00:01<00:00, 49.74it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 52/59 [00:01<00:00, 47.59it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 58/59 [00:01<00:00, 48.88it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 47.18it/s]
Loading train:   0%|          | 0/59 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/59 [00:00<00:32,  1.78it/s]Loading train:   3%|▎         | 2/59 [00:00<00:29,  1.93it/s]Loading train:   5%|▌         | 3/59 [00:01<00:29,  1.88it/s]Loading train:   7%|▋         | 4/59 [00:02<00:28,  1.91it/s]Loading train:   8%|▊         | 5/59 [00:02<00:28,  1.89it/s]Loading train:  10%|█         | 6/59 [00:03<00:27,  1.94it/s]Loading train:  12%|█▏        | 7/59 [00:03<00:29,  1.76it/s]Loading train:  14%|█▎        | 8/59 [00:04<00:26,  1.89it/s]Loading train:  15%|█▌        | 9/59 [00:04<00:27,  1.79it/s]Loading train:  17%|█▋        | 10/59 [00:05<00:27,  1.81it/s]Loading train:  19%|█▊        | 11/59 [00:06<00:28,  1.70it/s]Loading train:  20%|██        | 12/59 [00:06<00:27,  1.74it/s]Loading train:  22%|██▏       | 13/59 [00:07<00:24,  1.86it/s]Loading train:  24%|██▎       | 14/59 [00:07<00:24,  1.82it/s]Loading train:  25%|██▌       | 15/59 [00:08<00:22,  1.93it/s]Loading train:  27%|██▋       | 16/59 [00:08<00:23,  1.86it/s]Loading train:  29%|██▉       | 17/59 [00:09<00:21,  1.97it/s]Loading train:  31%|███       | 18/59 [00:09<00:21,  1.87it/s]Loading train:  32%|███▏      | 19/59 [00:10<00:21,  1.90it/s]Loading train:  34%|███▍      | 20/59 [00:10<00:21,  1.77it/s]Loading train:  36%|███▌      | 21/59 [00:11<00:18,  2.02it/s]Loading train:  37%|███▋      | 22/59 [00:11<00:17,  2.10it/s]Loading train:  39%|███▉      | 23/59 [00:12<00:20,  1.75it/s]Loading train:  41%|████      | 24/59 [00:12<00:16,  2.08it/s]Loading train:  42%|████▏     | 25/59 [00:13<00:16,  2.06it/s]Loading train:  44%|████▍     | 26/59 [00:13<00:14,  2.31it/s]Loading train:  46%|████▌     | 27/59 [00:13<00:12,  2.54it/s]Loading train:  47%|████▋     | 28/59 [00:14<00:11,  2.75it/s]Loading train:  49%|████▉     | 29/59 [00:14<00:10,  2.85it/s]Loading train:  51%|█████     | 30/59 [00:14<00:09,  3.01it/s]Loading train:  53%|█████▎    | 31/59 [00:14<00:08,  3.17it/s]Loading train:  54%|█████▍    | 32/59 [00:15<00:08,  3.18it/s]Loading train:  56%|█████▌    | 33/59 [00:15<00:08,  3.15it/s]Loading train:  58%|█████▊    | 34/59 [00:15<00:07,  3.45it/s]Loading train:  59%|█████▉    | 35/59 [00:16<00:06,  3.66it/s]Loading train:  61%|██████    | 36/59 [00:16<00:06,  3.31it/s]Loading train:  63%|██████▎   | 37/59 [00:16<00:06,  3.20it/s]Loading train:  64%|██████▍   | 38/59 [00:16<00:05,  3.54it/s]Loading train:  66%|██████▌   | 39/59 [00:17<00:05,  3.83it/s]Loading train:  68%|██████▊   | 40/59 [00:17<00:05,  3.71it/s]Loading train:  69%|██████▉   | 41/59 [00:17<00:05,  3.35it/s]Loading train:  71%|███████   | 42/59 [00:18<00:07,  2.24it/s]Loading train:  73%|███████▎  | 43/59 [00:18<00:06,  2.38it/s]Loading train:  75%|███████▍  | 44/59 [00:19<00:05,  2.62it/s]Loading train:  76%|███████▋  | 45/59 [00:19<00:04,  2.92it/s]Loading train:  78%|███████▊  | 46/59 [00:19<00:04,  2.95it/s]Loading train:  80%|███████▉  | 47/59 [00:20<00:03,  3.26it/s]Loading train:  81%|████████▏ | 48/59 [00:20<00:03,  3.20it/s]Loading train:  83%|████████▎ | 49/59 [00:20<00:02,  3.43it/s]Loading train:  85%|████████▍ | 50/59 [00:20<00:02,  3.31it/s]Loading train:  86%|████████▋ | 51/59 [00:21<00:02,  3.44it/s]Loading train:  88%|████████▊ | 52/59 [00:21<00:02,  3.42it/s]Loading train:  90%|████████▉ | 53/59 [00:21<00:01,  3.48it/s]Loading train:  92%|█████████▏| 54/59 [00:22<00:01,  3.42it/s]Loading train:  93%|█████████▎| 55/59 [00:22<00:01,  3.53it/s]Loading train:  95%|█████████▍| 56/59 [00:22<00:00,  3.50it/s]Loading train:  97%|█████████▋| 57/59 [00:22<00:00,  3.62it/s]Loading train:  98%|█████████▊| 58/59 [00:23<00:00,  3.51it/s]Loading train: 100%|██████████| 59/59 [00:23<00:00,  3.69it/s]Loading train: 100%|██████████| 59/59 [00:23<00:00,  2.51it/s]
concatenating: train:   0%|          | 0/59 [00:00<?, ?it/s]concatenating: train:  10%|█         | 6/59 [00:00<00:00, 55.04it/s]concatenating: train:  20%|██        | 12/59 [00:00<00:00, 55.91it/s]concatenating: train:  31%|███       | 18/59 [00:00<00:00, 54.06it/s]concatenating: train:  42%|████▏     | 25/59 [00:00<00:00, 55.84it/s]concatenating: train:  53%|█████▎    | 31/59 [00:00<00:00, 53.93it/s]concatenating: train:  64%|██████▍   | 38/59 [00:00<00:00, 57.41it/s]concatenating: train:  76%|███████▋  | 45/59 [00:00<00:00, 58.88it/s]concatenating: train:  88%|████████▊ | 52/59 [00:00<00:00, 60.53it/s]concatenating: train: 100%|██████████| 59/59 [00:01<00:00, 60.72it/s]concatenating: train: 100%|██████████| 59/59 [00:01<00:00, 58.61it/s]
Loading test:   0%|          | 0/23 [00:00<?, ?it/s]Loading test:   4%|▍         | 1/23 [00:00<00:08,  2.70it/s]Loading test:   9%|▊         | 2/23 [00:00<00:07,  2.90it/s]Loading test:  13%|█▎        | 3/23 [00:01<00:06,  2.90it/s]Loading test:  17%|█▋        | 4/23 [00:01<00:06,  3.02it/s]Loading test:  22%|██▏       | 5/23 [00:01<00:05,  3.26it/s]Loading test:  26%|██▌       | 6/23 [00:01<00:04,  3.40it/s]Loading test:  30%|███       | 7/23 [00:02<00:04,  3.48it/s]Loading test:  35%|███▍      | 8/23 [00:02<00:04,  3.44it/s]Loading test:  39%|███▉      | 9/23 [00:02<00:04,  3.29it/s]Loading test:  43%|████▎     | 10/23 [00:02<00:03,  3.40it/s]Loading test:  48%|████▊     | 11/23 [00:03<00:05,  2.27it/s]Loading test:  52%|█████▏    | 12/23 [00:04<00:04,  2.42it/s]Loading test:  57%|█████▋    | 13/23 [00:04<00:03,  2.60it/s]Loading test:  61%|██████    | 14/23 [00:04<00:03,  2.86it/s]Loading test:  65%|██████▌   | 15/23 [00:05<00:02,  2.91it/s]Loading test:  70%|██████▉   | 16/23 [00:05<00:02,  3.14it/s]Loading test:  74%|███████▍  | 17/23 [00:05<00:02,  2.82it/s]Loading test:  78%|███████▊  | 18/23 [00:06<00:01,  2.59it/s]Loading test:  83%|████████▎ | 19/23 [00:06<00:01,  2.43it/s]Loading test:  87%|████████▋ | 20/23 [00:07<00:01,  1.66it/s]Loading test:  91%|█████████▏| 21/23 [00:08<00:01,  1.60it/s]Loading test:  96%|█████████▌| 22/23 [00:08<00:00,  1.74it/s]Loading test: 100%|██████████| 23/23 [00:09<00:00,  1.71it/s]Loading test: 100%|██████████| 23/23 [00:09<00:00,  2.43it/s]
concatenating: validation:   0%|          | 0/23 [00:00<?, ?it/s]concatenating: validation:  26%|██▌       | 6/23 [00:00<00:00, 57.95it/s]concatenating: validation:  57%|█████▋    | 13/23 [00:00<00:00, 59.56it/s]concatenating: validation:  87%|████████▋ | 20/23 [00:00<00:00, 61.32it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 62.41it/s]
Epoch 00030: val_mDice did not improve from 0.68816

Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
{'val_loss': [0.19967578597095237, 0.19174128197865611, 0.2012228097961463, 0.18871799185754, 0.18325546600526751, 0.18719858898177577, 0.17966828741747207, 0.17720385278957393, 0.17794432893702422, 0.1837073087798529, 0.1806816230491158, 0.1809288077410635, 0.18180100156345572, 0.18475256844906346, 0.17432335553699646, 0.17465388006493804, 0.1768652225229092, 0.17966694082244167, 0.17618994986883435, 0.18110909006852072, 0.1817802705531925, 0.18261276244255234, 0.17718485448503551, 0.18135888709519915, 0.17565384791151467, 0.175445747583892, 0.1806065949910983, 0.1789320841307638, 0.17902081198222852, 0.17764102589049732], 'val_acc': [0.9341060118233157, 0.9345568553109177, 0.9343644333811456, 0.9348142375658884, 0.935102633836719, 0.9349105732187794, 0.9350096208357773, 0.9352064118049035, 0.9351262962194705, 0.9351529518227948, 0.9351380395624792, 0.9352102785499651, 0.9351748784918414, 0.9348204585054974, 0.9352284560774095, 0.9352744128171313, 0.9352049771843923, 0.9351805475453379, 0.9352019577510006, 0.9351027556909056, 0.9350042252457463, 0.9349902920696512, 0.9352269017280754, 0.9350407914200978, 0.9352753295573493, 0.9352981665905229, 0.9351377965625606, 0.9351727446266664, 0.9351784122632536, 0.9351685633277742], 'val_mDice': [0.6382221898050071, 0.6537908419598804, 0.6348762181995163, 0.6596448768782854, 0.6704212082435405, 0.6625823470623035, 0.6776044349894118, 0.6824235458145906, 0.6809884796909516, 0.6694450935116928, 0.6754807234201079, 0.6749854543572961, 0.6732366910286537, 0.6675154956061999, 0.6881597398492076, 0.6874831345546598, 0.6830780771947815, 0.6775004339965651, 0.6844296485851948, 0.6746465968142893, 0.6733457338537261, 0.6717211817643247, 0.6823695416779223, 0.674164754275669, 0.685477300581065, 0.6858863663253053, 0.6756351409382619, 0.6789403247894764, 0.6787424143728477, 0.681512790205182], 'loss': [0.13887582742722146, 0.07651858727362365, 0.07004605552531941, 0.06725548653854024, 0.0648381361932614, 0.06324032661419117, 0.06232026975641709, 0.06156943077097899, 0.06024149904318827, 0.059947334147939096, 0.0591507533275124, 0.058749975851869925, 0.05909529049884119, 0.05784980760264374, 0.057265108752140685, 0.05719160885776506, 0.05667868859561835, 0.05638373537756703, 0.05578656548645628, 0.055735565677367205, 0.055148329437454374, 0.05514721924667607, 0.054971400784631884, 0.05489354099452438, 0.054096833055952906, 0.05410880061917492, 0.05373626708858952, 0.05366634725849001, 0.05343421043658689, 0.05332393694262591], 'acc': [0.9054472409200808, 0.9114801662567944, 0.9118347930537725, 0.9119604272184807, 0.9120851136712667, 0.9121626575774998, 0.9122093043191489, 0.9122475898933472, 0.91232383932331, 0.9123375172289339, 0.9123799257503817, 0.9123925114959218, 0.9123930721925122, 0.9124583176865706, 0.9124700924500403, 0.9124770801636192, 0.9125012995433406, 0.9125238886038514, 0.9125538037293318, 0.912546413409436, 0.9125851106203733, 0.9125753555841221, 0.9126071626409121, 0.9125876022836235, 0.9126335768955894, 0.9126336757671335, 0.9126682360413855, 0.9126653090268904, 0.9126698921397497, 0.912679973660491], 'mDice': [0.7786895457634689, 0.8944786168989451, 0.9070567535173164, 0.912538938188136, 0.9172546143437635, 0.9204014745797491, 0.9221781437240322, 0.9236540521723277, 0.9262436071658605, 0.9268251755894408, 0.9283802179929005, 0.9291744656105881, 0.9284876991182749, 0.9309424437840573, 0.9321142075593589, 0.9322398866253325, 0.9332374311759272, 0.9338118872545267, 0.9350379375857263, 0.9351226745125699, 0.936265833191055, 0.9362862099718222, 0.9365766610131367, 0.9367635875853492, 0.9383243819817481, 0.9382918371896445, 0.9390314060841305, 0.9391731526443355, 0.9396371474425049, 0.9398580657389991], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]}

 MultiClass_24567891011121314 SD: [1] GPU: 3 

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 48, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 48, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 48, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 48, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 48, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 48, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 48, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 24, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 24, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 24, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 24, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 24, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 24, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 24, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 24, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 24, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 12, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 12, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 12, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________2020-09-30 21:24:41.222523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-09-30 21:24:41.222702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 21:24:41.222734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-09-30 21:24:41.222756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-09-30 21:24:41.222957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

batch_normalization_5 (BatchNor (None, 13, 12, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 12, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 12, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 12, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 12, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 12, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 12, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 24, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 24, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 24, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 24, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 24, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 24, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 24, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 24, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 24, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 24, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 48, 30)   25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 48, 60)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 48, 30)   16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 48, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 48, 30)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 48, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 48, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 48, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 48, 90)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 48, 90)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 48, 13)   1183        dropout_5[0][0]                  
==================================================================================================
Total params: 501,343
Trainable params: 500,143
Non-trainable params: 1,200
__________________________________________________________________________________________________
 --- initialization succesfull
class_weights [0.06398555 0.02959066 0.08783386 0.01011158 0.02895614 0.00645979
 0.07963164 0.11654211 0.07831174 0.0131854  0.30429472 0.18073491
 0.00036191]
Train on 3663 samples, validate on 1462 samples
Epoch 1/30
 - 11s - loss: 0.2710 - acc: 0.6894 - mDice: 0.7230 - val_loss: 0.5009 - val_acc: 0.7455 - val_mDice: 0.4721

Epoch 00001: val_mDice improved from -inf to 0.47212, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM30/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/30
 - 6s - loss: 0.2555 - acc: 0.6911 - mDice: 0.7398 - val_loss: 0.4924 - val_acc: 0.7463 - val_mDice: 0.4823

Epoch 00002: val_mDice improved from 0.47212 to 0.48230, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM30/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/30
 - 6s - loss: 0.2476 - acc: 0.6914 - mDice: 0.7484 - val_loss: 0.5040 - val_acc: 0.7451 - val_mDice: 0.4690

Epoch 00003: val_mDice did not improve from 0.48230
Epoch 4/30
 - 6s - loss: 0.2429 - acc: 0.6918 - mDice: 0.7534 - val_loss: 0.5013 - val_acc: 0.7451 - val_mDice: 0.4739

Epoch 00004: val_mDice did not improve from 0.48230
Epoch 5/30
 - 6s - loss: 0.2395 - acc: 0.6923 - mDice: 0.7571 - val_loss: 0.4946 - val_acc: 0.7451 - val_mDice: 0.4790

Epoch 00005: val_mDice did not improve from 0.48230
Epoch 6/30
 - 6s - loss: 0.2394 - acc: 0.6924 - mDice: 0.7572 - val_loss: 0.4946 - val_acc: 0.7457 - val_mDice: 0.4786

Epoch 00006: val_mDice did not improve from 0.48230
Epoch 7/30
 - 6s - loss: 0.2394 - acc: 0.6928 - mDice: 0.7572 - val_loss: 0.4903 - val_acc: 0.7457 - val_mDice: 0.4774

Epoch 00007: val_mDice did not improve from 0.48230
Epoch 8/30
 - 6s - loss: 0.2357 - acc: 0.6927 - mDice: 0.7613 - val_loss: 0.4543 - val_acc: 0.7456 - val_mDice: 0.4676

Epoch 00008: val_mDice did not improve from 0.48230
Epoch 9/30
 - 6s - loss: 0.2348 - acc: 0.6927 - mDice: 0.7622 - val_loss: 0.4985 - val_acc: 0.7455 - val_mDice: 0.4749

Epoch 00009: val_mDice did not improve from 0.48230
Epoch 10/30
 - 6s - loss: 0.2287 - acc: 0.6934 - mDice: 0.7688 - val_loss: 0.4951 - val_acc: 0.7465 - val_mDice: 0.4807

Epoch 00010: val_mDice did not improve from 0.48230
Epoch 11/30
 - 6s - loss: 0.2249 - acc: 0.6935 - mDice: 0.7729 - val_loss: 0.4907 - val_acc: 0.7459 - val_mDice: 0.4794

Epoch 00011: val_mDice did not improve from 0.48230
Epoch 12/30
 - 6s - loss: 0.2321 - acc: 0.6930 - mDice: 0.7651 - val_loss: 0.4986 - val_acc: 0.7460 - val_mDice: 0.4768

Epoch 00012: val_mDice did not improve from 0.48230
Epoch 13/30
 - 6s - loss: 0.2233 - acc: 0.6937 - mDice: 0.7746 - val_loss: 0.4974 - val_acc: 0.7455 - val_mDice: 0.4765

Epoch 00013: val_mDice did not improve from 0.48230
Epoch 14/30
 - 6s - loss: 0.2254 - acc: 0.6939 - mDice: 0.7724 - val_loss: 0.4890 - val_acc: 0.7458 - val_mDice: 0.4826

Epoch 00014: val_mDice improved from 0.48230 to 0.48258, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM30/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 15/30
 - 5s - loss: 0.2171 - acc: 0.6940 - mDice: 0.7813 - val_loss: 0.4926 - val_acc: 0.7458 - val_mDice: 0.4769

Epoch 00015: val_mDice did not improve from 0.48258
Epoch 16/30
 - 6s - loss: 0.2194 - acc: 0.6941 - mDice: 0.7788 - val_loss: 0.4879 - val_acc: 0.7459 - val_mDice: 0.4676

Epoch 00016: val_mDice did not improve from 0.48258
Epoch 17/30
 - 5s - loss: 0.2214 - acc: 0.6937 - mDice: 0.7767 - val_loss: 0.4910 - val_acc: 0.7459 - val_mDice: 0.4804

Epoch 00017: val_mDice did not improve from 0.48258

Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 18/30
 - 6s - loss: 0.2106 - acc: 0.6949 - mDice: 0.7883 - val_loss: 0.4921 - val_acc: 0.7464 - val_mDice: 0.4798

Epoch 00018: val_mDice did not improve from 0.48258
Epoch 19/30
 - 6s - loss: 0.2086 - acc: 0.6950 - mDice: 0.7905 - val_loss: 0.4939 - val_acc: 0.7459 - val_mDice: 0.4790

Epoch 00019: val_mDice did not improve from 0.48258
Epoch 20/30
 - 6s - loss: 0.2131 - acc: 0.6950 - mDice: 0.7857 - val_loss: 0.4942 - val_acc: 0.7467 - val_mDice: 0.4800

Epoch 00020: val_mDice did not improve from 0.48258
Epoch 21/30
 - 6s - loss: 0.2100 - acc: 0.6952 - mDice: 0.7890 - val_loss: 0.4908 - val_acc: 0.7464 - val_mDice: 0.4780

Epoch 00021: val_mDice did not improve from 0.48258
Epoch 22/30
 - 6s - loss: 0.2101 - acc: 0.6954 - mDice: 0.7889 - val_loss: 0.4940 - val_acc: 0.7466 - val_mDice: 0.4793

Epoch 00022: val_mDice did not improve from 0.48258
Epoch 23/30
 - 6s - loss: 0.2067 - acc: 0.6954 - mDice: 0.7926 - val_loss: 0.4898 - val_acc: 0.7468 - val_mDice: 0.4819

Epoch 00023: val_mDice did not improve from 0.48258
Epoch 24/30
 - 6s - loss: 0.2059 - acc: 0.6955 - mDice: 0.7935 - val_loss: 0.4936 - val_acc: 0.7466 - val_mDice: 0.4816

Epoch 00024: val_mDice did not improve from 0.48258
Epoch 25/30
 - 6s - loss: 0.2064 - acc: 0.6956 - mDice: 0.7929 - val_loss: 0.4932 - val_acc: 0.7452 - val_mDice: 0.4737

Epoch 00025: val_mDice did not improve from 0.48258
Epoch 26/30
 - 6s - loss: 0.2128 - acc: 0.6946 - mDice: 0.7859 - val_loss: 0.4926 - val_acc: 0.7465 - val_mDice: 0.4806

Epoch 00026: val_mDice did not improve from 0.48258
Epoch 27/30
 - 6s - loss: 0.2056 - acc: 0.6956 - mDice: 0.7938 - val_loss: 0.4848 - val_acc: 0.7460 - val_mDice: 0.4819

Epoch 00027: val_mDice did not improve from 0.48258
Epoch 28/30
 - 6s - loss: 0.2062 - acc: 0.6953 - mDice: 0.7931 - val_loss: 0.4882 - val_acc: 0.7468 - val_mDice: 0.4851

Epoch 00028: val_mDice improved from 0.48258 to 0.48511, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM30/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 29/30
 - 6s - loss: 0.2014 - acc: 0.6957 - mDice: 0.7983 - val_loss: 0.4988 - val_acc: 0.7461 - val_mDice: 0.4750

Epoch 00029: val_mDice did not improve from 0.48511
Epoch 30/30
 - 6s - loss: 0.1999 - acc: 0.6959 - mDice: 0.7999 - val_loss: 0.4850 - val_acc: 0.7469 - val_mDice: 0.4820

predicting test subjects:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects:   4%|▍         | 1/23 [00:01<00:26,  1.19s/it]predicting test subjects:   9%|▊         | 2/23 [00:01<00:20,  1.02it/s]predicting test subjects:  13%|█▎        | 3/23 [00:02<00:16,  1.19it/s]predicting test subjects:  17%|█▋        | 4/23 [00:02<00:14,  1.33it/s]predicting test subjects:  22%|██▏       | 5/23 [00:03<00:12,  1.50it/s]predicting test subjects:  26%|██▌       | 6/23 [00:03<00:10,  1.60it/s]predicting test subjects:  30%|███       | 7/23 [00:04<00:10,  1.55it/s]predicting test subjects:  35%|███▍      | 8/23 [00:04<00:09,  1.60it/s]predicting test subjects:  39%|███▉      | 9/23 [00:05<00:08,  1.56it/s]predicting test subjects:  43%|████▎     | 10/23 [00:06<00:08,  1.58it/s]predicting test subjects:  48%|████▊     | 11/23 [00:07<00:07,  1.53it/s]predicting test subjects:  52%|█████▏    | 12/23 [00:07<00:07,  1.56it/s]predicting test subjects:  57%|█████▋    | 13/23 [00:08<00:06,  1.66it/s]predicting test subjects:  61%|██████    | 14/23 [00:08<00:05,  1.75it/s]predicting test subjects:  65%|██████▌   | 15/23 [00:09<00:04,  1.76it/s]predicting test subjects:  70%|██████▉   | 16/23 [00:09<00:03,  1.78it/s]predicting test subjects:  74%|███████▍  | 17/23 [00:10<00:03,  1.83it/s]predicting test subjects:  78%|███████▊  | 18/23 [00:10<00:02,  1.77it/s]predicting test subjects:  83%|████████▎ | 19/23 [00:11<00:02,  1.74it/s]predicting test subjects:  87%|████████▋ | 20/23 [00:12<00:02,  1.49it/s]predicting test subjects:  91%|█████████▏| 21/23 [00:13<00:01,  1.45it/s]predicting test subjects:  96%|█████████▌| 22/23 [00:13<00:00,  1.52it/s]predicting test subjects: 100%|██████████| 23/23 [00:14<00:00,  1.50it/s]predicting test subjects: 100%|██████████| 23/23 [00:14<00:00,  1.60it/s]
Loading train:   0%|          | 0/59 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/59 [00:00<00:07,  7.71it/s]Loading train:   3%|▎         | 2/59 [00:00<00:07,  8.00it/s]Loading train:   5%|▌         | 3/59 [00:00<00:07,  7.89it/s]Loading train:   7%|▋         | 4/59 [00:00<00:07,  7.77it/s]Loading train:   8%|▊         | 5/59 [00:00<00:06,  7.80it/s]Loading train:  10%|█         | 6/59 [00:00<00:07,  7.45it/s]Loading train:  12%|█▏        | 7/59 [00:00<00:07,  7.18it/s]Loading train:  14%|█▎        | 8/59 [00:01<00:06,  7.50it/s]Loading train:  15%|█▌        | 9/59 [00:01<00:07,  7.12it/s]Loading train:  17%|█▋        | 10/59 [00:01<00:06,  7.11it/s]Loading train:  19%|█▊        | 11/59 [00:01<00:07,  6.61it/s]Loading train:  20%|██        | 12/59 [00:01<00:07,  6.61it/s]Loading train:  22%|██▏       | 13/59 [00:01<00:06,  6.83it/s]Loading train:  24%|██▎       | 14/59 [00:01<00:06,  6.79it/s]Loading train:  25%|██▌       | 15/59 [00:02<00:06,  6.80it/s]Loading train:  27%|██▋       | 16/59 [00:02<00:06,  6.68it/s]Loading train:  29%|██▉       | 17/59 [00:02<00:06,  6.71it/s]Loading train:  31%|███       | 18/59 [00:02<00:06,  6.40it/s]Loading train:  32%|███▏      | 19/59 [00:02<00:06,  6.13it/s]Loading train:  34%|███▍      | 20/59 [00:02<00:06,  5.94it/s]Loading train:  36%|███▌      | 21/59 [00:03<00:05,  6.42it/s]Loading train:  37%|███▋      | 22/59 [00:03<00:05,  6.51it/s]Loading train:  39%|███▉      | 23/59 [00:03<00:06,  5.40it/s]Loading train:  41%|████      | 24/59 [00:03<00:05,  6.03it/s]Loading train:  42%|████▏     | 25/59 [00:03<00:05,  6.58it/s]Loading train:  44%|████▍     | 26/59 [00:03<00:04,  7.13it/s]Loading train:  46%|████▌     | 27/59 [00:03<00:04,  7.50it/s]Loading train:  47%|████▋     | 28/59 [00:04<00:03,  7.83it/s]Loading train:  49%|████▉     | 29/59 [00:04<00:03,  7.83it/s]Loading train:  51%|█████     | 30/59 [00:04<00:03,  8.09it/s]Loading train:  53%|█████▎    | 31/59 [00:04<00:03,  8.14it/s]Loading train:  54%|█████▍    | 32/59 [00:04<00:03,  8.60it/s]Loading train:  56%|█████▌    | 33/59 [00:04<00:03,  8.65it/s]Loading train:  59%|█████▉    | 35/59 [00:04<00:02,  9.25it/s]Loading train:  61%|██████    | 36/59 [00:04<00:02,  9.45it/s]Loading train:  63%|██████▎   | 37/59 [00:05<00:02,  9.04it/s]Loading train:  66%|██████▌   | 39/59 [00:05<00:02,  9.56it/s]Loading train:  68%|██████▊   | 40/59 [00:05<00:02,  9.15it/s]Loading train:  69%|██████▉   | 41/59 [00:05<00:02,  8.82it/s]Loading train:  71%|███████   | 42/59 [00:05<00:01,  8.88it/s]Loading train:  75%|███████▍  | 44/59 [00:05<00:01,  9.20it/s]Loading train:  76%|███████▋  | 45/59 [00:05<00:01,  9.26it/s]Loading train:  78%|███████▊  | 46/59 [00:06<00:01,  8.69it/s]Loading train:  80%|███████▉  | 47/59 [00:06<00:01,  8.37it/s]Loading train:  81%|████████▏ | 48/59 [00:06<00:01,  8.34it/s]Loading train:  83%|████████▎ | 49/59 [00:06<00:01,  8.07it/s]Loading train:  85%|████████▍ | 50/59 [00:06<00:01,  8.16it/s]Loading train:  86%|████████▋ | 51/59 [00:06<00:00,  8.59it/s]Loading train:  88%|████████▊ | 52/59 [00:06<00:00,  8.59it/s]Loading train:  90%|████████▉ | 53/59 [00:06<00:00,  8.89it/s]Loading train:  92%|█████████▏| 54/59 [00:06<00:00,  8.66it/s]Loading train:  93%|█████████▎| 55/59 [00:07<00:00,  8.15it/s]Loading train:  95%|█████████▍| 56/59 [00:07<00:00,  8.17it/s]Loading train:  97%|█████████▋| 57/59 [00:07<00:00,  8.08it/s]Loading train:  98%|█████████▊| 58/59 [00:07<00:00,  8.17it/s]Loading train: 100%|██████████| 59/59 [00:07<00:00,  8.22it/s]Loading train: 100%|██████████| 59/59 [00:07<00:00,  7.77it/s]
concatenating: train:   0%|          | 0/59 [00:00<?, ?it/s]concatenating: train:   5%|▌         | 3/59 [00:00<00:01, 28.25it/s]concatenating: train:  12%|█▏        | 7/59 [00:00<00:01, 29.28it/s]concatenating: train:  17%|█▋        | 10/59 [00:00<00:01, 26.41it/s]concatenating: train:  24%|██▎       | 14/59 [00:00<00:01, 27.59it/s]concatenating: train:  29%|██▉       | 17/59 [00:00<00:01, 27.88it/s]concatenating: train:  34%|███▍      | 20/59 [00:00<00:01, 27.55it/s]concatenating: train:  39%|███▉      | 23/59 [00:00<00:01, 26.90it/s]concatenating: train:  46%|████▌     | 27/59 [00:00<00:01, 28.31it/s]concatenating: train:  53%|█████▎    | 31/59 [00:01<00:00, 29.62it/s]concatenating: train:  59%|█████▉    | 35/59 [00:01<00:00, 30.65it/s]concatenating: train:  66%|██████▌   | 39/59 [00:01<00:00, 31.55it/s]concatenating: train:  73%|███████▎  | 43/59 [00:01<00:00, 31.99it/s]concatenating: train:  80%|███████▉  | 47/59 [00:01<00:00, 31.80it/s]concatenating: train:  86%|████████▋ | 51/59 [00:01<00:00, 31.86it/s]concatenating: train:  93%|█████████▎| 55/59 [00:01<00:00, 31.89it/s]concatenating: train: 100%|██████████| 59/59 [00:01<00:00, 31.35it/s]concatenating: train: 100%|██████████| 59/59 [00:01<00:00, 30.26it/s]
Loading test:   0%|          | 0/23 [00:00<?, ?it/s]Loading test:   4%|▍         | 1/23 [00:00<00:03,  6.59it/s]Loading test:   9%|▊         | 2/23 [00:00<00:02,  7.22it/s]Loading test:  13%|█▎        | 3/23 [00:00<00:02,  7.69it/s]Loading test:  17%|█▋        | 4/23 [00:00<00:02,  7.90it/s]Loading test:  22%|██▏       | 5/23 [00:00<00:02,  8.10it/s]Loading test:  26%|██▌       | 6/23 [00:00<00:02,  8.30it/s]Loading test:  30%|███       | 7/23 [00:00<00:01,  8.68it/s]Loading test:  35%|███▍      | 8/23 [00:00<00:01,  8.86it/s]Loading test:  39%|███▉      | 9/23 [00:01<00:01,  7.45it/s]Loading test:  43%|████▎     | 10/23 [00:01<00:01,  7.11it/s]Loading test:  48%|████▊     | 11/23 [00:01<00:01,  6.80it/s]Loading test:  52%|█████▏    | 12/23 [00:01<00:01,  7.01it/s]Loading test:  57%|█████▋    | 13/23 [00:01<00:01,  7.29it/s]Loading test:  61%|██████    | 14/23 [00:01<00:01,  7.34it/s]Loading test:  65%|██████▌   | 15/23 [00:01<00:01,  7.25it/s]Loading test:  70%|██████▉   | 16/23 [00:02<00:00,  7.27it/s]Loading test:  74%|███████▍  | 17/23 [00:02<00:00,  7.10it/s]Loading test:  78%|███████▊  | 18/23 [00:02<00:00,  6.85it/s]Loading test:  83%|████████▎ | 19/23 [00:02<00:00,  7.12it/s]Loading test:  87%|████████▋ | 20/23 [00:02<00:00,  6.26it/s]Loading test:  91%|█████████▏| 21/23 [00:02<00:00,  6.11it/s]Loading test:  96%|█████████▌| 22/23 [00:03<00:00,  6.51it/s]Loading test: 100%|██████████| 23/23 [00:03<00:00,  6.49it/s]Loading test: 100%|██████████| 23/23 [00:03<00:00,  7.20it/s]
concatenating: validation:   0%|          | 0/23 [00:00<?, ?it/s]concatenating: validation:  17%|█▋        | 4/23 [00:00<00:00, 33.67it/s]concatenating: validation:  35%|███▍      | 8/23 [00:00<00:00, 35.24it/s]concatenating: validation:  48%|████▊     | 11/23 [00:00<00:00, 32.12it/s]concatenating: validation:  65%|██████▌   | 15/23 [00:00<00:00, 31.81it/s]concatenating: validation:  83%|████████▎ | 19/23 [00:00<00:00, 31.70it/s]concatenating: validation:  96%|█████████▌| 22/23 [00:00<00:00, 30.17it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 31.10it/s]
Loading trainS:   0%|          | 0/59 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/59 [00:00<00:07,  7.36it/s]Loading trainS:   3%|▎         | 2/59 [00:00<00:07,  7.52it/s]Loading trainS:   5%|▌         | 3/59 [00:00<00:07,  7.57it/s]Loading trainS:   7%|▋         | 4/59 [00:00<00:07,  7.54it/s]Loading trainS:   8%|▊         | 5/59 [00:00<00:07,  7.60it/s]Loading trainS:  10%|█         | 6/59 [00:00<00:07,  7.30it/s]Loading trainS:  12%|█▏        | 7/59 [00:00<00:07,  6.59it/s]Loading trainS:  14%|█▎        | 8/59 [00:01<00:07,  7.16it/s]Loading trainS:  15%|█▌        | 9/59 [00:01<00:07,  6.90it/s]Loading trainS:  17%|█▋        | 10/59 [00:01<00:07,  6.75it/s]Loading trainS:  19%|█▊        | 11/59 [00:01<00:07,  6.50it/s]Loading trainS:  20%|██        | 12/59 [00:01<00:07,  6.65it/s]Loading trainS:  22%|██▏       | 13/59 [00:01<00:06,  6.99it/s]Loading trainS:  24%|██▎       | 14/59 [00:01<00:06,  6.99it/s]Loading trainS:  25%|██▌       | 15/59 [00:02<00:06,  6.85it/s]Loading trainS:  27%|██▋       | 16/59 [00:02<00:06,  6.72it/s]Loading trainS:  29%|██▉       | 17/59 [00:02<00:06,  6.55it/s]Loading trainS:  31%|███       | 18/59 [00:02<00:06,  6.22it/s]Loading trainS:  32%|███▏      | 19/59 [00:02<00:06,  6.01it/s]Loading trainS:  34%|███▍      | 20/59 [00:02<00:06,  5.90it/s]Loading trainS:  36%|███▌      | 21/59 [00:03<00:05,  6.50it/s]Loading trainS:  37%|███▋      | 22/59 [00:03<00:05,  6.85it/s]Loading trainS:  39%|███▉      | 23/59 [00:03<00:06,  5.56it/s]Loading trainS:  41%|████      | 24/59 [00:03<00:05,  6.16it/s]Loading trainS:  42%|████▏     | 25/59 [00:03<00:05,  6.54it/s]Loading trainS:  44%|████▍     | 26/59 [00:03<00:04,  6.94it/s]Loading trainS:  46%|████▌     | 27/59 [00:04<00:04,  7.29it/s]Loading trainS:  47%|████▋     | 28/59 [00:04<00:03,  7.75it/s]Loading trainS:  49%|████▉     | 29/59 [00:04<00:03,  7.76it/s]Loading trainS:  53%|█████▎    | 31/59 [00:04<00:03,  8.29it/s]Loading trainS:  54%|█████▍    | 32/59 [00:04<00:03,  8.54it/s]Loading trainS:  56%|█████▌    | 33/59 [00:04<00:03,  8.47it/s]Loading trainS:  59%|█████▉    | 35/59 [00:04<00:02,  8.98it/s]Loading trainS:  61%|██████    | 36/59 [00:04<00:02,  9.08it/s]Loading trainS:  63%|██████▎   | 37/59 [00:05<00:02,  8.93it/s]Loading trainS:  66%|██████▌   | 39/59 [00:05<00:02,  9.34it/s]Loading trainS:  68%|██████▊   | 40/59 [00:05<00:02,  8.82it/s]Loading trainS:  69%|██████▉   | 41/59 [00:05<00:02,  8.24it/s]Loading trainS:  71%|███████   | 42/59 [00:05<00:02,  8.30it/s]Loading trainS:  73%|███████▎  | 43/59 [00:05<00:01,  8.46it/s]Loading trainS:  75%|███████▍  | 44/59 [00:05<00:01,  8.58it/s]Loading trainS:  76%|███████▋  | 45/59 [00:06<00:01,  8.64it/s]Loading trainS:  78%|███████▊  | 46/59 [00:06<00:01,  8.28it/s]Loading trainS:  80%|███████▉  | 47/59 [00:06<00:01,  8.51it/s]Loading trainS:  81%|████████▏ | 48/59 [00:06<00:01,  8.27it/s]Loading trainS:  83%|████████▎ | 49/59 [00:06<00:01,  8.11it/s]Loading trainS:  85%|████████▍ | 50/59 [00:06<00:01,  8.20it/s]Loading trainS:  88%|████████▊ | 52/59 [00:06<00:00,  8.53it/s]Loading trainS:  92%|█████████▏| 54/59 [00:07<00:00,  8.88it/s]Loading trainS:  93%|█████████▎| 55/59 [00:07<00:00,  8.10it/s]Loading trainS:  95%|█████████▍| 56/59 [00:07<00:00,  8.05it/s]Loading trainS:  97%|█████████▋| 57/59 [00:07<00:00,  7.77it/s]Loading trainS:  98%|█████████▊| 58/59 [00:07<00:00,  7.83it/s]Loading trainS: 100%|██████████| 59/59 [00:07<00:00,  8.03it/s]Loading trainS: 100%|██████████| 59/59 [00:07<00:00,  7.67it/s]
Loading testS:   0%|          | 0/23 [00:00<?, ?it/s]Loading testS:   4%|▍         | 1/23 [00:00<00:03,  6.86it/s]Loading testS:   9%|▊         | 2/23 [00:00<00:02,  7.56it/s]Loading testS:  13%|█▎        | 3/23 [00:00<00:02,  7.89it/s]Loading testS:  17%|█▋        | 4/23 [00:00<00:02,  8.27it/s]Loading testS:  26%|██▌       | 6/23 [00:00<00:02,  8.49it/s]Loading testS:  30%|███       | 7/23 [00:00<00:01,  8.87it/s]Loading testS:  35%|███▍      | 8/23 [00:00<00:01,  8.91it/s]Loading testS:  39%|███▉      | 9/23 [00:01<00:01,  7.07it/s]Loading testS:  43%|████▎     | 10/23 [00:01<00:01,  6.68it/s]Loading testS:  48%|████▊     | 11/23 [00:01<00:01,  6.22it/s]Loading testS:  57%|█████▋    | 13/23 [00:01<00:01,  6.93it/s]Loading testS:  61%|██████    | 14/23 [00:01<00:01,  7.18it/s]Loading testS:  65%|██████▌   | 15/23 [00:01<00:01,  7.29it/s]Loading testS:  70%|██████▉   | 16/23 [00:02<00:00,  7.21it/s]Loading testS:  74%|███████▍  | 17/23 [00:02<00:00,  7.00it/s]Loading testS:  78%|███████▊  | 18/23 [00:02<00:00,  6.86it/s]Loading testS:  83%|████████▎ | 19/23 [00:02<00:00,  6.92it/s]Loading testS:  87%|████████▋ | 20/23 [00:02<00:00,  5.65it/s]Loading testS:  91%|█████████▏| 21/23 [00:02<00:00,  5.42it/s]Loading testS:  96%|█████████▌| 22/23 [00:03<00:00,  5.84it/s]Loading testS: 100%|██████████| 23/23 [00:03<00:00,  5.89it/s]Loading testS: 100%|██████████| 23/23 [00:03<00:00,  6.99it/s]
Epoch 00030: val_mDice did not improve from 0.48511
{'val_loss': [0.5008662275348251, 0.4924434779769908, 0.5039903707853257, 0.5013156634349014, 0.49463571255412536, 0.4946315501359191, 0.4902542250182495, 0.4543292976192182, 0.4984941383882358, 0.49514173083364066, 0.4907154438841359, 0.4986158870721157, 0.49742620131548715, 0.4890364516123864, 0.49260722698004233, 0.4879163581108427, 0.49095911819044447, 0.4920588488781012, 0.4938818357793153, 0.49420246632741677, 0.49077409554767215, 0.4940339385973934, 0.4898170914434702, 0.4936223222521196, 0.49322201248395947, 0.492583046574273, 0.48482060505818736, 0.4882225085218041, 0.49876751129995783, 0.48502246511618513], 'val_acc': [0.7455022436567925, 0.7463383246544448, 0.7451084518114617, 0.7451490122626158, 0.7451095459766166, 0.7457475049061912, 0.7456510448178579, 0.745649947880394, 0.7454830623969497, 0.7465348103467155, 0.7458998726428615, 0.7460015352952986, 0.7454600398175204, 0.7457502439679264, 0.7458335488382345, 0.7458883579183375, 0.7459286399562773, 0.7463758693021887, 0.745936867048411, 0.7467118319628503, 0.7464040975849541, 0.7465542714874657, 0.7467808960115209, 0.7465879768540856, 0.7451553132726458, 0.7464786349259388, 0.7460404492598715, 0.7467545929547048, 0.746117727762208, 0.7469376428141966], 'val_mDice': [0.47212326037410834, 0.48230484980075694, 0.4689609717817215, 0.4738704823518093, 0.47899571994750184, 0.47860145911309365, 0.4773769890316208, 0.4676197328580551, 0.47494731996356215, 0.48065801700717287, 0.47940248895115467, 0.47684319323456237, 0.47647820561920406, 0.48258431522856005, 0.4769493971330849, 0.46762443783345203, 0.48039228755537366, 0.4797963501245966, 0.47895813616127714, 0.47999360337955355, 0.4780166322001028, 0.47927222010538123, 0.48189195688869985, 0.4816252712756599, 0.47365701467654936, 0.480599992211638, 0.48187463195477237, 0.4851052095723707, 0.47499095223507704, 0.4820372955988272], 'loss': [0.27103570977456254, 0.25551447274741534, 0.247605411831616, 0.24289625961208422, 0.2395037770222485, 0.2393915196552714, 0.23937992562435378, 0.23567196648693006, 0.23483257729142148, 0.22867140347209747, 0.2249122680600406, 0.2321281174867491, 0.22330804380308303, 0.22539249421337607, 0.2170842160073389, 0.2193886553479647, 0.2214398111455466, 0.21058705519887877, 0.2085959515934787, 0.21310360140238113, 0.20996283657427794, 0.21009683945840935, 0.20665138986733583, 0.20585584562396925, 0.20641767595460628, 0.21284056957094127, 0.205590283792591, 0.2062167911183922, 0.20144072958526799, 0.1998731505909872], 'acc': [0.6893795020847805, 0.6911349701256561, 0.6913667365627929, 0.691820321026442, 0.6922977413436021, 0.6924330362230905, 0.6927699130104762, 0.6926990346670346, 0.6926983785033715, 0.6933783647664544, 0.6935399082427529, 0.6929514734204142, 0.6937444433142765, 0.6939223956417393, 0.6939999470937261, 0.6941330542728414, 0.6936579292958921, 0.694947130117721, 0.6950403208814616, 0.6949544608056009, 0.6952015377770282, 0.6954310069213042, 0.6954496037169229, 0.6954798990834647, 0.6955966010726348, 0.6945526198126272, 0.6955654317487174, 0.6953201014228183, 0.6957367106991454, 0.6958631508961551], 'mDice': [0.7229998779043031, 0.7397539116728999, 0.7483726383524777, 0.7534147012829292, 0.7571058226329787, 0.7572079506396857, 0.7572180623162681, 0.761269228952425, 0.7621823600039533, 0.7688057677214698, 0.7728950430339326, 0.7650890844850439, 0.7746108147963259, 0.7723803432333382, 0.7813481607464471, 0.7788204448228973, 0.776658259338282, 0.7883393453048156, 0.7905403642162351, 0.7856720220739972, 0.7890285656502948, 0.7888799609706404, 0.7926302654981418, 0.7934776704688232, 0.7928688188345094, 0.7859287441118539, 0.7937644819174507, 0.793086947621526, 0.7982556485036276, 0.7999282035546456], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005]}

 1-THALAMUS SD: [2] GPU: 3 

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 168, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 168, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 168, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 168, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 168, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 168, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 168, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 84, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 84, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 84, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 84, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 84, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 84, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 84, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 84, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 84, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 42, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 42, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 42, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 42, 80)   320         conv2d_5[0][0]                   2020-09-30 21:28:37.184749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-09-30 21:28:37.184946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 21:28:37.184990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-09-30 21:28:37.185010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-09-30 21:28:37.185212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 42, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 42, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 42, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 42, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 42, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 42, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 84, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 84, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 84, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 84, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 84, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 84, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 84, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 84, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 84, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 84, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 168, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 168, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 168, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 168, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 168, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 168, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 168, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 168, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 168, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 168, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 168, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
 --- initialization succesfull
class_weights [0.98181702 0.01818298]
Train on 3939 samples, validate on 1622 samples
Epoch 1/30
 - 19s - loss: 0.0686 - acc: 0.9080 - mDice: 0.9119 - val_loss: 0.1439 - val_acc: 0.9329 - val_mDice: 0.7360

Epoch 00001: val_mDice improved from -inf to 0.73600, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM20/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/30
 - 15s - loss: 0.0636 - acc: 0.9083 - mDice: 0.9217 - val_loss: 0.1424 - val_acc: 0.9330 - val_mDice: 0.7421

Epoch 00002: val_mDice improved from 0.73600 to 0.74211, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM20/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/30
 - 15s - loss: 0.0627 - acc: 0.9084 - mDice: 0.9235 - val_loss: 0.1497 - val_acc: 0.9329 - val_mDice: 0.7333

Epoch 00003: val_mDice did not improve from 0.74211
Epoch 4/30
 - 15s - loss: 0.0620 - acc: 0.9084 - mDice: 0.9249 - val_loss: 0.1154 - val_acc: 0.9329 - val_mDice: 0.7408

Epoch 00004: val_mDice did not improve from 0.74211
Epoch 5/30
 - 15s - loss: 0.0614 - acc: 0.9084 - mDice: 0.9261 - val_loss: 0.1357 - val_acc: 0.9328 - val_mDice: 0.7358

Epoch 00005: val_mDice did not improve from 0.74211
Epoch 6/30
 - 15s - loss: 0.0614 - acc: 0.9084 - mDice: 0.9262 - val_loss: 0.1249 - val_acc: 0.9330 - val_mDice: 0.7372

Epoch 00006: val_mDice did not improve from 0.74211
Epoch 7/30
 - 15s - loss: 0.0603 - acc: 0.9085 - mDice: 0.9284 - val_loss: 0.1300 - val_acc: 0.9328 - val_mDice: 0.7303

Epoch 00007: val_mDice did not improve from 0.74211
Epoch 8/30
 - 14s - loss: 0.0596 - acc: 0.9085 - mDice: 0.9297 - val_loss: 0.1347 - val_acc: 0.9329 - val_mDice: 0.7383

Epoch 00008: val_mDice did not improve from 0.74211
Epoch 9/30
 - 15s - loss: 0.0604 - acc: 0.9085 - mDice: 0.9281 - val_loss: 0.1092 - val_acc: 0.9329 - val_mDice: 0.7391

Epoch 00009: val_mDice did not improve from 0.74211
Epoch 10/30
 - 15s - loss: 0.0594 - acc: 0.9085 - mDice: 0.9301 - val_loss: 0.0934 - val_acc: 0.9329 - val_mDice: 0.7431

Epoch 00010: val_mDice improved from 0.74211 to 0.74312, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM20/1-THALAMUS/sd2/best_model_weights.h5
Epoch 11/30
 - 15s - loss: 0.0590 - acc: 0.9085 - mDice: 0.9309 - val_loss: 0.1201 - val_acc: 0.9329 - val_mDice: 0.7395

Epoch 00011: val_mDice did not improve from 0.74312
Epoch 12/30
 - 15s - loss: 0.0595 - acc: 0.9085 - mDice: 0.9298 - val_loss: 0.1394 - val_acc: 0.9329 - val_mDice: 0.7373

Epoch 00012: val_mDice did not improve from 0.74312
Epoch 13/30
 - 14s - loss: 0.0590 - acc: 0.9086 - mDice: 0.9308 - val_loss: 0.1302 - val_acc: 0.9329 - val_mDice: 0.7364

Epoch 00013: val_mDice did not improve from 0.74312
Epoch 14/30
 - 15s - loss: 0.0588 - acc: 0.9086 - mDice: 0.9312 - val_loss: 0.1259 - val_acc: 0.9329 - val_mDice: 0.7395

Epoch 00014: val_mDice did not improve from 0.74312
Epoch 15/30
 - 15s - loss: 0.0583 - acc: 0.9086 - mDice: 0.9323 - val_loss: 0.1123 - val_acc: 0.9329 - val_mDice: 0.7407

Epoch 00015: val_mDice did not improve from 0.74312
Epoch 16/30
 - 15s - loss: 0.0578 - acc: 0.9086 - mDice: 0.9332 - val_loss: 0.1144 - val_acc: 0.9329 - val_mDice: 0.7372

Epoch 00016: val_mDice did not improve from 0.74312
Epoch 17/30
 - 15s - loss: 0.0581 - acc: 0.9086 - mDice: 0.9327 - val_loss: 0.1280 - val_acc: 0.9329 - val_mDice: 0.7332

Epoch 00017: val_mDice did not improve from 0.74312
Epoch 18/30
 - 15s - loss: 0.0579 - acc: 0.9086 - mDice: 0.9330 - val_loss: 0.1410 - val_acc: 0.9329 - val_mDice: 0.7377

Epoch 00018: val_mDice did not improve from 0.74312
Epoch 19/30
 - 15s - loss: 0.0580 - acc: 0.9086 - mDice: 0.9328 - val_loss: 0.1067 - val_acc: 0.9329 - val_mDice: 0.7358

Epoch 00019: val_mDice did not improve from 0.74312
Epoch 20/30
 - 14s - loss: 0.0576 - acc: 0.9086 - mDice: 0.9335 - val_loss: 0.1145 - val_acc: 0.9329 - val_mDice: 0.7348

Epoch 00020: val_mDice did not improve from 0.74312
Epoch 21/30
 - 15s - loss: 0.0572 - acc: 0.9086 - mDice: 0.9344 - val_loss: 0.1287 - val_acc: 0.9329 - val_mDice: 0.7399

Epoch 00021: val_mDice did not improve from 0.74312
Epoch 22/30
 - 15s - loss: 0.0572 - acc: 0.9086 - mDice: 0.9344 - val_loss: 0.0987 - val_acc: 0.9329 - val_mDice: 0.7380

Epoch 00022: val_mDice did not improve from 0.74312
Epoch 23/30
 - 15s - loss: 0.0573 - acc: 0.9086 - mDice: 0.9342 - val_loss: 0.0933 - val_acc: 0.9329 - val_mDice: 0.7306

Epoch 00023: val_mDice did not improve from 0.74312
Epoch 24/30
 - 15s - loss: 0.0570 - acc: 0.9086 - mDice: 0.9348 - val_loss: 0.1105 - val_acc: 0.9328 - val_mDice: 0.7356

Epoch 00024: val_mDice did not improve from 0.74312
Epoch 25/30
 - 14s - loss: 0.0569 - acc: 0.9086 - mDice: 0.9351 - val_loss: 0.0975 - val_acc: 0.9329 - val_mDice: 0.7410

Epoch 00025: val_mDice did not improve from 0.74312

Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 26/30
 - 15s - loss: 0.0566 - acc: 0.9087 - mDice: 0.9355 - val_loss: 0.0858 - val_acc: 0.9328 - val_mDice: 0.7329

Epoch 00026: val_mDice did not improve from 0.74312
Epoch 27/30
 - 15s - loss: 0.0564 - acc: 0.9087 - mDice: 0.9360 - val_loss: 0.0802 - val_acc: 0.9329 - val_mDice: 0.7385

Epoch 00027: val_mDice did not improve from 0.74312
Epoch 28/30
 - 15s - loss: 0.0559 - acc: 0.9087 - mDice: 0.9371 - val_loss: 0.1116 - val_acc: 0.9329 - val_mDice: 0.7396

Epoch 00028: val_mDice did not improve from 0.74312
Epoch 29/30
 - 14s - loss: 0.0560 - acc: 0.9087 - mDice: 0.9368 - val_loss: 0.0939 - val_acc: 0.9329 - val_mDice: 0.7373

Epoch 00029: val_mDice did not improve from 0.74312
Epoch 30/30
 - 14s - loss: 0.0555 - acc: 0.9087 - mDice: 0.9377 - val_loss: 0.0965 - val_acc: 0.9330 - val_mDice: 0.7318

predicting test subjects:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects:   4%|▍         | 1/23 [00:00<00:19,  1.13it/s]predicting test subjects:   9%|▊         | 2/23 [00:01<00:14,  1.43it/s]predicting test subjects:  13%|█▎        | 3/23 [00:01<00:11,  1.72it/s]predicting test subjects:  17%|█▋        | 4/23 [00:01<00:08,  2.20it/s]predicting test subjects:  22%|██▏       | 5/23 [00:01<00:06,  2.66it/s]predicting test subjects:  26%|██▌       | 6/23 [00:02<00:06,  2.83it/s]predicting test subjects:  30%|███       | 7/23 [00:02<00:05,  3.13it/s]predicting test subjects:  35%|███▍      | 8/23 [00:02<00:04,  3.18it/s]predicting test subjects:  39%|███▉      | 9/23 [00:02<00:04,  3.13it/s]predicting test subjects:  43%|████▎     | 10/23 [00:03<00:04,  3.00it/s]predicting test subjects:  48%|████▊     | 11/23 [00:03<00:03,  3.02it/s]predicting test subjects:  52%|█████▏    | 12/23 [00:03<00:03,  3.21it/s]predicting test subjects:  57%|█████▋    | 13/23 [00:04<00:02,  3.46it/s]predicting test subjects:  61%|██████    | 14/23 [00:04<00:02,  3.69it/s]predicting test subjects:  65%|██████▌   | 15/23 [00:04<00:02,  3.67it/s]predicting test subjects:  70%|██████▉   | 16/23 [00:05<00:02,  3.37it/s]predicting test subjects:  74%|███████▍  | 17/23 [00:05<00:02,  2.76it/s]predicting test subjects:  78%|███████▊  | 18/23 [00:05<00:01,  3.20it/s]predicting test subjects:  83%|████████▎ | 19/23 [00:05<00:01,  3.55it/s]predicting test subjects:  87%|████████▋ | 20/23 [00:06<00:00,  3.30it/s]predicting test subjects:  91%|█████████▏| 21/23 [00:06<00:00,  3.12it/s]predicting test subjects:  96%|█████████▌| 22/23 [00:06<00:00,  3.47it/s]predicting test subjects: 100%|██████████| 23/23 [00:07<00:00,  3.31it/s]predicting test subjects: 100%|██████████| 23/23 [00:07<00:00,  3.19it/s]
predicting train subjects:   0%|          | 0/59 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/59 [00:00<00:09,  6.23it/s]predicting train subjects:   3%|▎         | 2/59 [00:00<00:08,  6.37it/s]predicting train subjects:   5%|▌         | 3/59 [00:00<00:09,  6.21it/s]predicting train subjects:   7%|▋         | 4/59 [00:00<00:09,  5.79it/s]predicting train subjects:   8%|▊         | 5/59 [00:00<00:09,  5.77it/s]predicting train subjects:  10%|█         | 6/59 [00:01<00:09,  5.47it/s]predicting train subjects:  12%|█▏        | 7/59 [00:01<00:10,  4.80it/s]predicting train subjects:  14%|█▎        | 8/59 [00:01<00:09,  5.15it/s]predicting train subjects:  15%|█▌        | 9/59 [00:01<00:11,  4.49it/s]predicting train subjects:  17%|█▋        | 10/59 [00:02<00:10,  4.47it/s]predicting train subjects:  19%|█▊        | 11/59 [00:02<00:10,  4.39it/s]predicting train subjects:  20%|██        | 12/59 [00:02<00:09,  4.70it/s]predicting train subjects:  22%|██▏       | 13/59 [00:02<00:11,  4.11it/s]predicting train subjects:  24%|██▎       | 14/59 [00:02<00:10,  4.37it/s]predicting train subjects:  25%|██▌       | 15/59 [00:03<00:09,  4.72it/s]predicting train subjects:  27%|██▋       | 16/59 [00:03<00:08,  4.83it/s]predicting train subjects:  29%|██▉       | 17/59 [00:03<00:10,  4.18it/s]predicting train subjects:  31%|███       | 18/59 [00:03<00:09,  4.30it/s]predicting train subjects:  32%|███▏      | 19/59 [00:04<00:10,  3.97it/s]predicting train subjects:  34%|███▍      | 20/59 [00:04<00:09,  4.12it/s]predicting train subjects:  36%|███▌      | 21/59 [00:04<00:09,  4.13it/s]predicting train subjects:  37%|███▋      | 22/59 [00:04<00:08,  4.28it/s]predicting train subjects:  39%|███▉      | 23/59 [00:05<00:10,  3.55it/s]predicting train subjects:  41%|████      | 24/59 [00:05<00:09,  3.61it/s]predicting train subjects:  42%|████▏     | 25/59 [00:05<00:08,  4.20it/s]predicting train subjects:  44%|████▍     | 26/59 [00:05<00:07,  4.67it/s]predicting train subjects:  46%|████▌     | 27/59 [00:05<00:06,  4.65it/s]predicting train subjects:  47%|████▋     | 28/59 [00:06<00:06,  5.12it/s]predicting train subjects:  49%|████▉     | 29/59 [00:06<00:05,  5.38it/s]predicting train subjects:  51%|█████     | 30/59 [00:06<00:05,  5.25it/s]predicting train subjects:  53%|█████▎    | 31/59 [00:06<00:05,  5.10it/s]predicting train subjects:  54%|█████▍    | 32/59 [00:06<00:04,  5.65it/s]predicting train subjects:  56%|█████▌    | 33/59 [00:07<00:04,  5.76it/s]predicting train subjects:  59%|█████▉    | 35/59 [00:07<00:03,  6.47it/s]predicting train subjects:  61%|██████    | 36/59 [00:07<00:03,  6.39it/s]predicting train subjects:  63%|██████▎   | 37/59 [00:07<00:03,  6.13it/s]predicting train subjects:  64%|██████▍   | 38/59 [00:07<00:03,  6.85it/s]predicting train subjects:  66%|██████▌   | 39/59 [00:07<00:03,  6.60it/s]predicting train subjects:  68%|██████▊   | 40/59 [00:08<00:03,  6.32it/s]predicting train subjects:  69%|██████▉   | 41/59 [00:08<00:03,  5.92it/s]predicting train subjects:  71%|███████   | 42/59 [00:08<00:02,  5.97it/s]predicting train subjects:  73%|███████▎  | 43/59 [00:08<00:02,  5.63it/s]predicting train subjects:  75%|███████▍  | 44/59 [00:08<00:02,  5.81it/s]predicting train subjects:  76%|███████▋  | 45/59 [00:08<00:02,  5.93it/s]predicting train subjects:  78%|███████▊  | 46/59 [00:09<00:02,  5.77it/s]predicting train subjects:  80%|███████▉  | 47/59 [00:09<00:02,  5.57it/s]predicting train subjects:  81%|████████▏ | 48/59 [00:09<00:02,  5.49it/s]predicting train subjects:  83%|████████▎ | 49/59 [00:09<00:01,  5.66it/s]predicting train subjects:  85%|████████▍ | 50/59 [00:09<00:01,  5.73it/s]predicting train subjects:  86%|████████▋ | 51/59 [00:09<00:01,  6.04it/s]predicting train subjects:  88%|████████▊ | 52/59 [00:10<00:01,  6.06it/s]predicting train subjects:  90%|████████▉ | 53/59 [00:10<00:01,  5.17it/s]predicting train subjects:  92%|█████████▏| 54/59 [00:10<00:00,  5.37it/s]predicting train subjects:  93%|█████████▎| 55/59 [00:10<00:00,  5.30it/s]predicting train subjects:  95%|█████████▍| 56/59 [00:10<00:00,  5.64it/s]predicting train subjects:  97%|█████████▋| 57/59 [00:11<00:00,  5.61it/s]predicting train subjects:  98%|█████████▊| 58/59 [00:11<00:00,  5.47it/s]predicting train subjects: 100%|██████████| 59/59 [00:11<00:00,  5.51it/s]predicting train subjects: 100%|██████████| 59/59 [00:11<00:00,  5.17it/s]
predicting test subjects sagittal:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects sagittal:   4%|▍         | 1/23 [00:00<00:05,  4.29it/s]predicting test subjects sagittal:   9%|▊         | 2/23 [00:00<00:04,  4.88it/s]predicting test subjects sagittal:  13%|█▎        | 3/23 [00:00<00:03,  5.54it/s]predicting test subjects sagittal:  17%|█▋        | 4/23 [00:00<00:03,  5.38it/s]predicting test subjects sagittal:  22%|██▏       | 5/23 [00:00<00:03,  5.14it/s]predicting test subjects sagittal:  26%|██▌       | 6/23 [00:01<00:03,  5.39it/s]predicting test subjects sagittal:  30%|███       | 7/23 [00:01<00:02,  6.09it/s]predicting test subjects sagittal:  35%|███▍      | 8/23 [00:01<00:02,  6.19it/s]predicting test subjects sagittal:  39%|███▉      | 9/23 [00:01<00:02,  4.74it/s]predicting test subjects sagittal:  43%|████▎     | 10/23 [00:02<00:03,  4.04it/s]predicting test subjects sagittal:  48%|████▊     | 11/23 [00:02<00:02,  4.06it/s]predicting test subjects sagittal:  52%|█████▏    | 12/23 [00:02<00:02,  4.29it/s]predicting test subjects sagittal:  57%|█████▋    | 13/23 [00:02<00:02,  4.61it/s]predicting test subjects sagittal:  61%|██████    | 14/23 [00:02<00:02,  4.42it/s]predicting test subjects sagittal:  65%|██████▌   | 15/23 [00:03<00:01,  4.66it/s]predicting test subjects sagittal:  70%|██████▉   | 16/23 [00:03<00:01,  4.47it/s]predicting test subjects sagittal:  74%|███████▍  | 17/23 [00:03<00:01,  4.74it/s]predicting test subjects sagittal:  78%|███████▊  | 18/23 [00:03<00:01,  4.76it/s]predicting test subjects sagittal:  83%|████████▎ | 19/23 [00:03<00:00,  4.68it/s]predicting test subjects sagittal:  87%|████████▋ | 20/23 [00:04<00:00,  3.88it/s]predicting test subjects sagittal:  91%|█████████▏| 21/23 [00:04<00:00,  3.95it/s]predicting test subjects sagittal:  96%|█████████▌| 22/23 [00:04<00:00,  4.17it/s]predicting test subjects sagittal: 100%|██████████| 23/23 [00:04<00:00,  4.10it/s]predicting test subjects sagittal: 100%|██████████| 23/23 [00:04<00:00,  4.62it/s]
predicting train subjects sagittal:   0%|          | 0/59 [00:00<?, ?it/s]predicting train subjects sagittal:   2%|▏         | 1/59 [00:00<00:10,  5.76it/s]predicting train subjects sagittal:   3%|▎         | 2/59 [00:00<00:09,  5.91it/s]predicting train subjects sagittal:   5%|▌         | 3/59 [00:00<00:09,  5.85it/s]predicting train subjects sagittal:   7%|▋         | 4/59 [00:00<00:09,  5.59it/s]predicting train subjects sagittal:   8%|▊         | 5/59 [00:00<00:09,  5.57it/s]predicting train subjects sagittal:  10%|█         | 6/59 [00:01<00:10,  4.87it/s]predicting train subjects sagittal:  12%|█▏        | 7/59 [00:01<00:11,  4.70it/s]predicting train subjects sagittal:  14%|█▎        | 8/59 [00:01<00:09,  5.13it/s]predicting train subjects sagittal:  15%|█▌        | 9/59 [00:01<00:10,  4.87it/s]predicting train subjects sagittal:  17%|█▋        | 10/59 [00:02<00:11,  4.41it/s]predicting train subjects sagittal:  19%|█▊        | 11/59 [00:02<00:11,  4.02it/s]predicting train subjects sagittal:  20%|██        | 12/59 [00:02<00:10,  4.31it/s]predicting train subjects sagittal:  22%|██▏       | 13/59 [00:02<00:09,  4.67it/s]predicting train subjects sagittal:  24%|██▎       | 14/59 [00:02<00:10,  4.36it/s]predicting train subjects sagittal:  25%|██▌       | 15/59 [00:03<00:09,  4.41it/s]predicting train subjects sagittal:  27%|██▋       | 16/59 [00:03<00:09,  4.46it/s]predicting train subjects sagittal:  29%|██▉       | 17/59 [00:03<00:09,  4.50it/s]predicting train subjects sagittal:  31%|███       | 18/59 [00:03<00:09,  4.47it/s]predicting train subjects sagittal:  32%|███▏      | 19/59 [00:04<00:10,  4.00it/s]predicting train subjects sagittal:  34%|███▍      | 20/59 [00:04<00:09,  4.10it/s]predicting train subjects sagittal:  36%|███▌      | 21/59 [00:04<00:09,  3.88it/s]predicting train subjects sagittal:  37%|███▋      | 22/59 [00:04<00:09,  4.07it/s]predicting train subjects sagittal:  39%|███▉      | 23/59 [00:05<00:11,  3.07it/s]predicting train subjects sagittal:  41%|████      | 24/59 [00:05<00:10,  3.34it/s]predicting train subjects sagittal:  42%|████▏     | 25/59 [00:05<00:08,  3.91it/s]predicting train subjects sagittal:  44%|████▍     | 26/59 [00:05<00:07,  4.39it/s]predicting train subjects sagittal:  46%|████▌     | 27/59 [00:06<00:07,  4.43it/s]predicting train subjects sagittal:  47%|████▋     | 28/59 [00:06<00:06,  4.77it/s]predicting train subjects sagittal:  49%|████▉     | 29/59 [00:06<00:06,  4.94it/s]predicting train subjects sagittal:  51%|█████     | 30/59 [00:06<00:05,  4.87it/s]predicting train subjects sagittal:  53%|█████▎    | 31/59 [00:07<00:06,  4.49it/s]predicting train subjects sagittal:  54%|█████▍    | 32/59 [00:07<00:05,  4.86it/s]predicting train subjects sagittal:  56%|█████▌    | 33/59 [00:07<00:05,  5.19it/s]predicting train subjects sagittal:  58%|█████▊    | 34/59 [00:07<00:04,  5.74it/s]predicting train subjects sagittal:  59%|█████▉    | 35/59 [00:07<00:03,  6.21it/s]predicting train subjects sagittal:  61%|██████    | 36/59 [00:07<00:03,  6.21it/s]predicting train subjects sagittal:  63%|██████▎   | 37/59 [00:07<00:03,  6.04it/s]predicting train subjects sagittal:  64%|██████▍   | 38/59 [00:08<00:03,  6.68it/s]predicting train subjects sagittal:  66%|██████▌   | 39/59 [00:08<00:03,  6.24it/s]predicting train subjects sagittal:  68%|██████▊   | 40/59 [00:08<00:03,  6.04it/s]predicting train subjects sagittal:  69%|██████▉   | 41/59 [00:08<00:03,  5.78it/s]predicting train subjects sagittal:  71%|███████   | 42/59 [00:08<00:02,  5.86it/s]predicting train subjects sagittal:  73%|███████▎  | 43/59 [00:09<00:03,  5.28it/s]predicting train subjects sagittal:  75%|███████▍  | 44/59 [00:09<00:02,  5.48it/s]predicting train subjects sagittal:  76%|███████▋  | 45/59 [00:09<00:02,  5.76it/s]predicting train subjects sagittal:  78%|███████▊  | 46/59 [00:09<00:02,  5.42it/s]predicting train subjects sagittal:  80%|███████▉  | 47/59 [00:09<00:02,  5.45it/s]predicting train subjects sagittal:  81%|████████▏ | 48/59 [00:09<00:01,  5.89it/s]predicting train subjects sagittal:  83%|████████▎ | 49/59 [00:10<00:01,  5.79it/s]predicting train subjects sagittal:  85%|████████▍ | 50/59 [00:10<00:01,  5.91it/s]predicting train subjects sagittal:  86%|████████▋ | 51/59 [00:10<00:01,  6.11it/s]predicting train subjects sagittal:  88%|████████▊ | 52/59 [00:10<00:01,  5.84it/s]predicting train subjects sagittal:  90%|████████▉ | 53/59 [00:10<00:00,  6.24it/s]predicting train subjects sagittal:  92%|█████████▏| 54/59 [00:10<00:00,  6.14it/s]predicting train subjects sagittal:  93%|█████████▎| 55/59 [00:11<00:00,  5.60it/s]predicting train subjects sagittal:  95%|█████████▍| 56/59 [00:11<00:00,  5.70it/s]predicting train subjects sagittal:  97%|█████████▋| 57/59 [00:11<00:00,  5.57it/s]predicting train subjects sagittal:  98%|█████████▊| 58/59 [00:11<00:00,  5.80it/s]predicting train subjects sagittal: 100%|██████████| 59/59 [00:11<00:00,  5.47it/s]predicting train subjects sagittal: 100%|██████████| 59/59 [00:11<00:00,  5.01it/s]
saving BB  test1-THALAMUS:   0%|          | 0/23 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  26%|██▌       | 6/23 [00:00<00:00, 54.11it/s]saving BB  test1-THALAMUS:  43%|████▎     | 10/23 [00:00<00:00, 46.34it/s]saving BB  test1-THALAMUS:  65%|██████▌   | 15/23 [00:00<00:00, 46.58it/s]saving BB  test1-THALAMUS:  87%|████████▋ | 20/23 [00:00<00:00, 45.66it/s]saving BB  test1-THALAMUS: 100%|██████████| 23/23 [00:00<00:00, 43.05it/s]
saving BB  train1-THALAMUS:   0%|          | 0/59 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   8%|▊         | 5/59 [00:00<00:01, 46.76it/s]saving BB  train1-THALAMUS:  15%|█▌        | 9/59 [00:00<00:01, 44.43it/s]saving BB  train1-THALAMUS:  24%|██▎       | 14/59 [00:00<00:01, 43.91it/s]saving BB  train1-THALAMUS:  32%|███▏      | 19/59 [00:00<00:00, 44.14it/s]saving BB  train1-THALAMUS:  39%|███▉      | 23/59 [00:00<00:00, 41.29it/s]saving BB  train1-THALAMUS:  49%|████▉     | 29/59 [00:00<00:00, 43.43it/s]saving BB  train1-THALAMUS:  61%|██████    | 36/59 [00:00<00:00, 47.08it/s]saving BB  train1-THALAMUS:  71%|███████   | 42/59 [00:00<00:00, 49.89it/s]saving BB  train1-THALAMUS:  81%|████████▏ | 48/59 [00:01<00:00, 50.67it/s]saving BB  train1-THALAMUS:  90%|████████▉ | 53/59 [00:01<00:00, 50.25it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 50.44it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 48.12it/s]
saving BB  test1-THALAMUS:   0%|          | 0/23 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  22%|██▏       | 5/23 [00:00<00:00, 48.02it/s]saving BB  test1-THALAMUS:  43%|████▎     | 10/23 [00:00<00:00, 46.03it/s]saving BB  test1-THALAMUS:  65%|██████▌   | 15/23 [00:00<00:00, 45.93it/s]saving BB  test1-THALAMUS:  83%|████████▎ | 19/23 [00:00<00:00, 43.17it/s]saving BB  test1-THALAMUS: 100%|██████████| 23/23 [00:00<00:00, 40.31it/s]saving BB  test1-THALAMUS: 100%|██████████| 23/23 [00:00<00:00, 41.56it/s]
saving BB  train1-THALAMUS:   0%|          | 0/59 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   8%|▊         | 5/59 [00:00<00:01, 47.54it/s]saving BB  train1-THALAMUS:  17%|█▋        | 10/59 [00:00<00:01, 45.40it/s]saving BB  train1-THALAMUS:  25%|██▌       | 15/59 [00:00<00:00, 44.98it/s]saving BB  train1-THALAMUS:  34%|███▍      | 20/59 [00:00<00:00, 43.81it/s]saving BB  train1-THALAMUS:  41%|████      | 24/59 [00:00<00:00, 41.70it/s]saving BB  train1-THALAMUS:  51%|█████     | 30/59 [00:00<00:00, 44.78it/s]saving BB  train1-THALAMUS:  63%|██████▎   | 37/59 [00:00<00:00, 48.30it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 43/59 [00:00<00:00, 49.58it/s]saving BB  train1-THALAMUS:  81%|████████▏ | 48/59 [00:01<00:00, 49.24it/s]saving BB  train1-THALAMUS:  92%|█████████▏| 54/59 [00:01<00:00, 50.81it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 48.64it/s]
Loading train:   0%|          | 0/59 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/59 [00:00<00:32,  1.81it/s]Loading train:   3%|▎         | 2/59 [00:00<00:29,  1.92it/s]Loading train:   5%|▌         | 3/59 [00:01<00:29,  1.88it/s]Loading train:   7%|▋         | 4/59 [00:02<00:28,  1.91it/s]Loading train:   8%|▊         | 5/59 [00:02<00:28,  1.93it/s]Loading train:  10%|█         | 6/59 [00:03<00:26,  1.98it/s]Loading train:  12%|█▏        | 7/59 [00:03<00:27,  1.86it/s]Loading train:  14%|█▎        | 8/59 [00:04<00:25,  2.02it/s]Loading train:  15%|█▌        | 9/59 [00:04<00:25,  1.98it/s]Loading train:  17%|█▋        | 10/59 [00:05<00:25,  1.93it/s]Loading train:  19%|█▊        | 11/59 [00:05<00:26,  1.83it/s]Loading train:  20%|██        | 12/59 [00:06<00:26,  1.81it/s]Loading train:  22%|██▏       | 13/59 [00:06<00:24,  1.88it/s]Loading train:  24%|██▎       | 14/59 [00:07<00:24,  1.85it/s]Loading train:  25%|██▌       | 15/59 [00:07<00:22,  1.95it/s]Loading train:  27%|██▋       | 16/59 [00:08<00:22,  1.87it/s]Loading train:  29%|██▉       | 17/59 [00:08<00:21,  1.93it/s]Loading train:  31%|███       | 18/59 [00:09<00:22,  1.85it/s]Loading train:  32%|███▏      | 19/59 [00:09<00:21,  1.88it/s]Loading train:  34%|███▍      | 20/59 [00:10<00:21,  1.81it/s]Loading train:  36%|███▌      | 21/59 [00:10<00:18,  2.11it/s]Loading train:  37%|███▋      | 22/59 [00:11<00:17,  2.15it/s]Loading train:  39%|███▉      | 23/59 [00:12<00:20,  1.73it/s]Loading train:  41%|████      | 24/59 [00:12<00:17,  2.05it/s]Loading train:  42%|████▏     | 25/59 [00:12<00:16,  2.04it/s]Loading train:  44%|████▍     | 26/59 [00:13<00:14,  2.30it/s]Loading train:  46%|████▌     | 27/59 [00:13<00:12,  2.56it/s]Loading train:  47%|████▋     | 28/59 [00:13<00:11,  2.77it/s]Loading train:  49%|████▉     | 29/59 [00:14<00:10,  2.96it/s]Loading train:  51%|█████     | 30/59 [00:14<00:09,  3.16it/s]Loading train:  53%|█████▎    | 31/59 [00:14<00:08,  3.23it/s]Loading train:  54%|█████▍    | 32/59 [00:14<00:08,  3.26it/s]Loading train:  56%|█████▌    | 33/59 [00:15<00:07,  3.36it/s]Loading train:  58%|█████▊    | 34/59 [00:15<00:06,  3.68it/s]Loading train:  59%|█████▉    | 35/59 [00:15<00:05,  4.12it/s]Loading train:  61%|██████    | 36/59 [00:15<00:05,  3.83it/s]Loading train:  63%|██████▎   | 37/59 [00:16<00:06,  3.61it/s]Loading train:  64%|██████▍   | 38/59 [00:16<00:05,  3.99it/s]Loading train:  66%|██████▌   | 39/59 [00:16<00:04,  4.21it/s]Loading train:  68%|██████▊   | 40/59 [00:16<00:04,  3.89it/s]Loading train:  69%|██████▉   | 41/59 [00:17<00:04,  3.75it/s]Loading train:  71%|███████   | 42/59 [00:18<00:08,  2.00it/s]Loading train:  73%|███████▎  | 43/59 [00:18<00:07,  2.27it/s]Loading train:  75%|███████▍  | 44/59 [00:18<00:05,  2.56it/s]Loading train:  76%|███████▋  | 45/59 [00:19<00:04,  2.91it/s]Loading train:  78%|███████▊  | 46/59 [00:19<00:04,  2.97it/s]Loading train:  80%|███████▉  | 47/59 [00:19<00:03,  3.18it/s]Loading train:  81%|████████▏ | 48/59 [00:19<00:03,  3.29it/s]Loading train:  83%|████████▎ | 49/59 [00:20<00:02,  3.55it/s]Loading train:  85%|████████▍ | 50/59 [00:20<00:02,  3.53it/s]Loading train:  86%|████████▋ | 51/59 [00:20<00:02,  3.72it/s]Loading train:  88%|████████▊ | 52/59 [00:20<00:01,  3.66it/s]Loading train:  90%|████████▉ | 53/59 [00:21<00:01,  3.68it/s]Loading train:  92%|█████████▏| 54/59 [00:21<00:01,  3.63it/s]Loading train:  93%|█████████▎| 55/59 [00:21<00:01,  3.70it/s]Loading train:  95%|█████████▍| 56/59 [00:22<00:00,  3.64it/s]Loading train:  97%|█████████▋| 57/59 [00:22<00:00,  3.63it/s]Loading train:  98%|█████████▊| 58/59 [00:22<00:00,  3.66it/s]Loading train: 100%|██████████| 59/59 [00:22<00:00,  3.82it/s]Loading train: 100%|██████████| 59/59 [00:22<00:00,  2.58it/s]
concatenating: train:   0%|          | 0/59 [00:00<?, ?it/s]concatenating: train:  10%|█         | 6/59 [00:00<00:00, 58.70it/s]concatenating: train:  20%|██        | 12/59 [00:00<00:00, 58.26it/s]concatenating: train:  31%|███       | 18/59 [00:00<00:00, 57.80it/s]concatenating: train:  41%|████      | 24/59 [00:00<00:00, 57.67it/s]concatenating: train:  53%|█████▎    | 31/59 [00:00<00:00, 58.99it/s]concatenating: train:  64%|██████▍   | 38/59 [00:00<00:00, 60.46it/s]concatenating: train:  75%|███████▍  | 44/59 [00:00<00:00, 59.94it/s]concatenating: train:  86%|████████▋ | 51/59 [00:00<00:00, 60.75it/s]concatenating: train:  97%|█████████▋| 57/59 [00:00<00:00, 60.00it/s]concatenating: train: 100%|██████████| 59/59 [00:00<00:00, 59.85it/s]
Loading test:   0%|          | 0/23 [00:00<?, ?it/s]Loading test:   4%|▍         | 1/23 [00:00<00:07,  2.86it/s]Loading test:   9%|▊         | 2/23 [00:00<00:06,  3.05it/s]Loading test:  13%|█▎        | 3/23 [00:00<00:06,  3.27it/s]Loading test:  17%|█▋        | 4/23 [00:01<00:05,  3.25it/s]Loading test:  22%|██▏       | 5/23 [00:01<00:05,  3.47it/s]Loading test:  26%|██▌       | 6/23 [00:01<00:04,  3.46it/s]Loading test:  30%|███       | 7/23 [00:01<00:04,  3.52it/s]Loading test:  35%|███▍      | 8/23 [00:02<00:04,  3.43it/s]Loading test:  39%|███▉      | 9/23 [00:02<00:04,  3.19it/s]Loading test:  43%|████▎     | 10/23 [00:02<00:03,  3.35it/s]Loading test:  48%|████▊     | 11/23 [00:03<00:03,  3.16it/s]Loading test:  52%|█████▏    | 12/23 [00:03<00:03,  3.19it/s]Loading test:  57%|█████▋    | 13/23 [00:03<00:03,  3.26it/s]Loading test:  61%|██████    | 14/23 [00:04<00:02,  3.43it/s]Loading test:  65%|██████▌   | 15/23 [00:04<00:02,  3.39it/s]Loading test:  70%|██████▉   | 16/23 [00:04<00:01,  3.63it/s]Loading test:  74%|███████▍  | 17/23 [00:05<00:01,  3.01it/s]Loading test:  78%|███████▊  | 18/23 [00:05<00:01,  2.78it/s]Loading test:  83%|████████▎ | 19/23 [00:06<00:01,  2.54it/s]Loading test:  87%|████████▋ | 20/23 [00:07<00:01,  1.63it/s]Loading test:  91%|█████████▏| 21/23 [00:07<00:01,  1.64it/s]Loading test:  96%|█████████▌| 22/23 [00:08<00:00,  1.73it/s]Loading test: 100%|██████████| 23/23 [00:08<00:00,  1.74it/s]Loading test: 100%|██████████| 23/23 [00:08<00:00,  2.60it/s]
concatenating: validation:   0%|          | 0/23 [00:00<?, ?it/s]concatenating: validation:  26%|██▌       | 6/23 [00:00<00:00, 59.81it/s]concatenating: validation:  52%|█████▏    | 12/23 [00:00<00:00, 58.33it/s]concatenating: validation:  83%|████████▎ | 19/23 [00:00<00:00, 59.24it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 58.87it/s]
Epoch 00030: val_mDice did not improve from 0.74312
{'val_loss': [0.1438523245936331, 0.14243918443247952, 0.149668546290161, 0.11539373623549423, 0.13567624644418827, 0.12493469545436259, 0.13004779350357049, 0.13474762500361362, 0.10918260184283174, 0.09339067975631037, 0.120073252248448, 0.13940949719651854, 0.13021018076802887, 0.12588679848833678, 0.11229053877854465, 0.11442801868944956, 0.12797081040082967, 0.1410153226699547, 0.10665091955356769, 0.1144506834520879, 0.12871869685617535, 0.09870798852749435, 0.09328368588197981, 0.11050913155391243, 0.0975459251235872, 0.0857600416395579, 0.08018238571932959, 0.11158500784036557, 0.09393729463348259, 0.09651313312856984], 'val_acc': [0.9329294453566818, 0.9329605470750246, 0.9329490961481699, 0.9328762697085793, 0.9328174904154203, 0.932962157723348, 0.9328018282109506, 0.9329136618850558, 0.9328874020335413, 0.9329165094583749, 0.9329478294575994, 0.932924923194175, 0.93292179339911, 0.9328878507220201, 0.9329315664294909, 0.9329274852384621, 0.9328648143720097, 0.9329354282372094, 0.9328974969731012, 0.9329451395315835, 0.9329376738844611, 0.9328873700631091, 0.9328768043864964, 0.9328270760329231, 0.9329392261040637, 0.9328469715635814, 0.9328667160615086, 0.9329120765925923, 0.9329396042371061, 0.9329657670748954], 'val_mDice': [0.7359992197350714, 0.7421059464133183, 0.73330613531341, 0.7407549692445077, 0.7358056249880174, 0.7371787699318403, 0.7302535438287714, 0.7382558083431675, 0.7391451532537929, 0.7431235701246452, 0.7395488271546458, 0.7372870311857592, 0.7364101197143371, 0.7395280179201307, 0.7407394740569936, 0.7371695343354481, 0.7331693729181324, 0.7377484775206898, 0.7357509197201005, 0.7348143966218547, 0.739916420912625, 0.7379625355491802, 0.7306433554495307, 0.73555511294987, 0.7410444069364361, 0.732919858577954, 0.7385441741725813, 0.7395887782776547, 0.7372821325438531, 0.7318499741292617], 'loss': [0.0686415331856676, 0.06360821120628217, 0.06274255483162285, 0.06202608638039456, 0.061396631716501594, 0.061371041241939615, 0.0602586879554543, 0.059571027801133075, 0.060414473734007296, 0.0594249554687752, 0.05899426927797139, 0.059505499371661545, 0.059048830038567854, 0.05879920635369563, 0.05826329579754177, 0.05780537801554252, 0.058077044348088455, 0.05792064873128562, 0.057994739844067264, 0.05762449819353457, 0.05718997906204044, 0.0572032093014412, 0.05732068249222348, 0.05700766456349606, 0.05687313106390509, 0.05661944723251841, 0.05635818696076508, 0.055851267927353926, 0.055984945223019125, 0.0555313467604916], 'acc': [0.9080465728784461, 0.9083065159522389, 0.9083650876162893, 0.9083936685655104, 0.908426561568043, 0.9084256753064437, 0.908473625177658, 0.9085141430805407, 0.9085001375778704, 0.9085182335571034, 0.908534465722246, 0.908518939506708, 0.9085784849554005, 0.9085650906613648, 0.908554463511828, 0.9085965280485625, 0.9085782368221268, 0.9085956677533431, 0.9086236887003571, 0.9086232727389252, 0.908629277191903, 0.9086239864058105, 0.9086338990713383, 0.9086334685378645, 0.9086498983107899, 0.9086732051341659, 0.9086882502907392, 0.9087021764416052, 0.9087051863172811, 0.9087327636114442], 'mDice': [0.9118530255460775, 0.9217499322956605, 0.9235164278895457, 0.9248846557788601, 0.9261471055794198, 0.9261599974784793, 0.9283727047920953, 0.9297354071083999, 0.9280579226305715, 0.9300539950696185, 0.9308951675483315, 0.9298465790770386, 0.9307678078570079, 0.9312473936836319, 0.932314841917972, 0.9332162389660826, 0.9326698015141106, 0.9329855714421733, 0.9328149695116785, 0.9335360855249984, 0.9343987499496361, 0.9344267902352478, 0.934175215689898, 0.9347974853214197, 0.935050901028371, 0.9355451687546241, 0.9360405155837036, 0.9370918575353688, 0.9367916196335024, 0.9377001942974506], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005]}

 MultiClass_24567891011121314 SD: [2] GPU: 3 

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 84, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 84, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 84, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 84, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 84, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 84, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 84, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 42, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 42, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 42, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 42, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 42, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 42, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 42, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 42, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 42, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 21, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 21, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 21, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 21, 80)   320         conv2d_5[0][0]                   2020-09-30 21:37:30.844157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-09-30 21:37:30.844290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 21:37:30.844307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-09-30 21:37:30.844317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-09-30 21:37:30.844531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 21, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 21, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 21, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 21, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 21, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 21, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 42, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 42, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 42, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 42, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 42, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 42, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 42, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 42, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 42, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 42, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 84, 20)   11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 84, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 84, 20)   7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 84, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 84, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 84, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 84, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 84, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 84, 60)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 84, 60)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 84, 13)   793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
 --- initialization succesfull
class_weights [0.06368803 0.02945306 0.08742544 0.01006456 0.02882149 0.00642976
 0.08176825 0.11640658 0.0779476  0.01312409 0.30287977 0.18166814
 0.00032323]
Train on 2220 samples, validate on 887 samples
Epoch 1/30
 - 10s - loss: 0.2768 - acc: 0.7142 - mDice: 0.7151 - val_loss: 0.2710 - val_acc: 0.7697 - val_mDice: 0.6867

Epoch 00001: val_mDice improved from -inf to 0.68668, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM20/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/30
 - 4s - loss: 0.2575 - acc: 0.7162 - mDice: 0.7359 - val_loss: 0.2665 - val_acc: 0.7705 - val_mDice: 0.6941

Epoch 00002: val_mDice improved from 0.68668 to 0.69410, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM20/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 3/30
 - 4s - loss: 0.2524 - acc: 0.7168 - mDice: 0.7415 - val_loss: 0.2562 - val_acc: 0.7707 - val_mDice: 0.6965

Epoch 00003: val_mDice improved from 0.69410 to 0.69645, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM20/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/30
 - 4s - loss: 0.2488 - acc: 0.7172 - mDice: 0.7453 - val_loss: 0.2598 - val_acc: 0.7707 - val_mDice: 0.6981

Epoch 00004: val_mDice improved from 0.69645 to 0.69813, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM20/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/30
 - 4s - loss: 0.2443 - acc: 0.7175 - mDice: 0.7501 - val_loss: 0.2740 - val_acc: 0.7703 - val_mDice: 0.6917

Epoch 00005: val_mDice did not improve from 0.69813
Epoch 6/30
 - 4s - loss: 0.2423 - acc: 0.7177 - mDice: 0.7523 - val_loss: 0.2522 - val_acc: 0.7711 - val_mDice: 0.7008

Epoch 00006: val_mDice improved from 0.69813 to 0.70078, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM20/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 7/30
 - 4s - loss: 0.2412 - acc: 0.7178 - mDice: 0.7536 - val_loss: 0.2588 - val_acc: 0.7711 - val_mDice: 0.6992

Epoch 00007: val_mDice did not improve from 0.70078
Epoch 8/30
 - 4s - loss: 0.2399 - acc: 0.7178 - mDice: 0.7550 - val_loss: 0.2603 - val_acc: 0.7708 - val_mDice: 0.6976

Epoch 00008: val_mDice did not improve from 0.70078
Epoch 9/30
 - 4s - loss: 0.2364 - acc: 0.7182 - mDice: 0.7587 - val_loss: 0.2592 - val_acc: 0.7711 - val_mDice: 0.6988

Epoch 00009: val_mDice did not improve from 0.70078
Epoch 10/30
 - 4s - loss: 0.2382 - acc: 0.7182 - mDice: 0.7568 - val_loss: 0.2539 - val_acc: 0.7709 - val_mDice: 0.6989

Epoch 00010: val_mDice did not improve from 0.70078
Epoch 11/30
 - 4s - loss: 0.2354 - acc: 0.7183 - mDice: 0.7599 - val_loss: 0.2575 - val_acc: 0.7711 - val_mDice: 0.7005

Epoch 00011: val_mDice did not improve from 0.70078
Epoch 12/30
 - 4s - loss: 0.2333 - acc: 0.7186 - mDice: 0.7621 - val_loss: 0.2571 - val_acc: 0.7714 - val_mDice: 0.7011

Epoch 00012: val_mDice improved from 0.70078 to 0.70113, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM20/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 13/30
 - 4s - loss: 0.2341 - acc: 0.7187 - mDice: 0.7612 - val_loss: 0.2552 - val_acc: 0.7711 - val_mDice: 0.6975

Epoch 00013: val_mDice did not improve from 0.70113
Epoch 14/30
 - 4s - loss: 0.2328 - acc: 0.7186 - mDice: 0.7626 - val_loss: 0.2556 - val_acc: 0.7712 - val_mDice: 0.6971

Epoch 00014: val_mDice did not improve from 0.70113
Epoch 15/30
 - 4s - loss: 0.2317 - acc: 0.7187 - mDice: 0.7638 - val_loss: 0.2539 - val_acc: 0.7711 - val_mDice: 0.6989

Epoch 00015: val_mDice did not improve from 0.70113
Epoch 16/30
 - 4s - loss: 0.2280 - acc: 0.7188 - mDice: 0.7678 - val_loss: 0.2522 - val_acc: 0.7712 - val_mDice: 0.7009

Epoch 00016: val_mDice did not improve from 0.70113
Epoch 17/30
 - 4s - loss: 0.2269 - acc: 0.7191 - mDice: 0.7690 - val_loss: 0.2494 - val_acc: 0.7713 - val_mDice: 0.7038

Epoch 00017: val_mDice improved from 0.70113 to 0.70384, saving model to /array/hdd/msmajdi/experiments/exp6/models/GE_Siemens/FM20/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 18/30
 - 4s - loss: 0.2278 - acc: 0.7190 - mDice: 0.7680 - val_loss: 0.2607 - val_acc: 0.7709 - val_mDice: 0.7004

Epoch 00018: val_mDice did not improve from 0.70384
Epoch 19/30
 - 4s - loss: 0.2272 - acc: 0.7191 - mDice: 0.7687 - val_loss: 0.2534 - val_acc: 0.7711 - val_mDice: 0.6995

Epoch 00019: val_mDice did not improve from 0.70384
Epoch 20/30
 - 4s - loss: 0.2257 - acc: 0.7191 - mDice: 0.7703 - val_loss: 0.2540 - val_acc: 0.7712 - val_mDice: 0.6988

Epoch 00020: val_mDice did not improve from 0.70384
Epoch 21/30
 - 4s - loss: 0.2259 - acc: 0.7193 - mDice: 0.7700 - val_loss: 0.2583 - val_acc: 0.7705 - val_mDice: 0.6942

Epoch 00021: val_mDice did not improve from 0.70384
Epoch 22/30
 - 4s - loss: 0.2255 - acc: 0.7192 - mDice: 0.7705 - val_loss: 0.2658 - val_acc: 0.7707 - val_mDice: 0.6972

Epoch 00022: val_mDice did not improve from 0.70384
Epoch 23/30
 - 4s - loss: 0.2233 - acc: 0.7194 - mDice: 0.7729 - val_loss: 0.2522 - val_acc: 0.7712 - val_mDice: 0.7005

Epoch 00023: val_mDice did not improve from 0.70384
Epoch 24/30
 - 4s - loss: 0.2224 - acc: 0.7194 - mDice: 0.7739 - val_loss: 0.2549 - val_acc: 0.7709 - val_mDice: 0.6979

Epoch 00024: val_mDice did not improve from 0.70384
Epoch 25/30
 - 4s - loss: 0.2241 - acc: 0.7193 - mDice: 0.7721 - val_loss: 0.2561 - val_acc: 0.7709 - val_mDice: 0.6965

Epoch 00025: val_mDice did not improve from 0.70384
Epoch 26/30
 - 4s - loss: 0.2218 - acc: 0.7195 - mDice: 0.7745 - val_loss: 0.2540 - val_acc: 0.7710 - val_mDice: 0.6988

Epoch 00026: val_mDice did not improve from 0.70384
Epoch 27/30
 - 4s - loss: 0.2211 - acc: 0.7196 - mDice: 0.7753 - val_loss: 0.2534 - val_acc: 0.7711 - val_mDice: 0.6995

Epoch 00027: val_mDice did not improve from 0.70384
Epoch 28/30
 - 4s - loss: 0.2217 - acc: 0.7195 - mDice: 0.7746 - val_loss: 0.2533 - val_acc: 0.7711 - val_mDice: 0.6996

Epoch 00028: val_mDice did not improve from 0.70384
Epoch 29/30
 - 4s - loss: 0.2202 - acc: 0.7197 - mDice: 0.7762 - val_loss: 0.2551 - val_acc: 0.7713 - val_mDice: 0.6977

Epoch 00029: val_mDice did not improve from 0.70384
Epoch 30/30
 - 4s - loss: 0.2197 - acc: 0.7198 - mDice: 0.7768 - val_loss: 0.2540 - val_acc: 0.7713 - val_mDice: 0.6988

predicting test subjects:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects:   4%|▍         | 1/23 [00:01<00:29,  1.32s/it]predicting test subjects:   9%|▊         | 2/23 [00:01<00:22,  1.08s/it]predicting test subjects:  13%|█▎        | 3/23 [00:02<00:18,  1.07it/s]predicting test subjects:  17%|█▋        | 4/23 [00:03<00:15,  1.21it/s]predicting test subjects:  22%|██▏       | 5/23 [00:03<00:13,  1.36it/s]predicting test subjects:  26%|██▌       | 6/23 [00:04<00:11,  1.42it/s]predicting test subjects:  30%|███       | 7/23 [00:04<00:10,  1.57it/s]predicting test subjects:  35%|███▍      | 8/23 [00:05<00:09,  1.52it/s]predicting test subjects:  39%|███▉      | 9/23 [00:06<00:09,  1.42it/s]predicting test subjects:  43%|████▎     | 10/23 [00:06<00:09,  1.41it/s]predicting test subjects:  48%|████▊     | 11/23 [00:07<00:08,  1.38it/s]predicting test subjects:  52%|█████▏    | 12/23 [00:08<00:07,  1.43it/s]predicting test subjects:  57%|█████▋    | 13/23 [00:08<00:06,  1.51it/s]predicting test subjects:  61%|██████    | 14/23 [00:09<00:05,  1.58it/s]predicting test subjects:  65%|██████▌   | 15/23 [00:10<00:05,  1.58it/s]predicting test subjects:  70%|██████▉   | 16/23 [00:10<00:04,  1.54it/s]predicting test subjects:  74%|███████▍  | 17/23 [00:11<00:03,  1.61it/s]predicting test subjects:  78%|███████▊  | 18/23 [00:11<00:03,  1.56it/s]predicting test subjects:  83%|████████▎ | 19/23 [00:12<00:02,  1.52it/s]predicting test subjects:  87%|████████▋ | 20/23 [00:13<00:02,  1.29it/s]predicting test subjects:  91%|█████████▏| 21/23 [00:14<00:01,  1.28it/s]predicting test subjects:  96%|█████████▌| 22/23 [00:15<00:00,  1.37it/s]predicting test subjects: 100%|██████████| 23/23 [00:15<00:00,  1.38it/s]predicting test subjects: 100%|██████████| 23/23 [00:15<00:00,  1.45it/s]
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:03<01:26,  3.95s/it]  9%|▊         | 2/23 [00:06<01:13,  3.49s/it] 13%|█▎        | 3/23 [00:09<01:05,  3.28s/it] 17%|█▋        | 4/23 [00:12<01:03,  3.37s/it] 22%|██▏       | 5/23 [00:15<00:58,  3.24s/it] 26%|██▌       | 6/23 [00:18<00:54,  3.20s/it] 30%|███       | 7/23 [00:21<00:47,  2.99s/it] 35%|███▍      | 8/23 [00:24<00:44,  2.93s/it] 39%|███▉      | 9/23 [00:29<00:50,  3.60s/it] 43%|████▎     | 10/23 [00:33<00:49,  3.79s/it] 48%|████▊     | 11/23 [00:37<00:48,  4.00s/it] 52%|█████▏    | 12/23 [00:41<00:43,  3.98s/it] 57%|█████▋    | 13/23 [00:44<00:37,  3.71s/it] 61%|██████    | 14/23 [00:48<00:32,  3.56s/it] 65%|██████▌   | 15/23 [00:51<00:28,  3.59s/it] 70%|██████▉   | 16/23 [00:55<00:25,  3.70s/it] 74%|███████▍  | 17/23 [00:59<00:22,  3.73s/it] 78%|███████▊  | 18/23 [01:04<00:19,  3.94s/it] 83%|████████▎ | 19/23 [01:08<00:16,  4.00s/it] 87%|████████▋ | 20/23 [01:15<00:15,  5.12s/it] 91%|█████████▏| 21/23 [01:20<00:10,  5.09s/it] 96%|█████████▌| 22/23 [01:24<00:04,  4.65s/it]100%|██████████| 23/23 [01:29<00:00,  4.75s/it]100%|██████████| 23/23 [01:29<00:00,  3.89s/it]    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 
    Flipping: PProcessed.nii.gz   ----------- 

Loading train:   0%|          | 0/59 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/59 [00:00<00:07,  7.68it/s]Loading train:   3%|▎         | 2/59 [00:00<00:07,  7.53it/s]Loading train:   5%|▌         | 3/59 [00:00<00:07,  7.24it/s]Loading train:   7%|▋         | 4/59 [00:00<00:07,  7.21it/s]Loading train:   8%|▊         | 5/59 [00:00<00:07,  7.39it/s]Loading train:  10%|█         | 6/59 [00:00<00:07,  7.02it/s]Loading train:  12%|█▏        | 7/59 [00:01<00:07,  6.57it/s]Loading train:  14%|█▎        | 8/59 [00:01<00:07,  7.01it/s]Loading train:  15%|█▌        | 9/59 [00:01<00:07,  6.60it/s]Loading train:  17%|█▋        | 10/59 [00:01<00:07,  6.46it/s]Loading train:  19%|█▊        | 11/59 [00:01<00:07,  6.25it/s]Loading train:  20%|██        | 12/59 [00:01<00:07,  6.33it/s]Loading train:  22%|██▏       | 13/59 [00:01<00:06,  6.58it/s]Loading train:  24%|██▎       | 14/59 [00:02<00:06,  6.69it/s]Loading train:  25%|██▌       | 15/59 [00:02<00:06,  6.83it/s]Loading train:  27%|██▋       | 16/59 [00:02<00:06,  6.59it/s]Loading train:  29%|██▉       | 17/59 [00:02<00:06,  6.45it/s]Loading train:  31%|███       | 18/59 [00:02<00:06,  6.36it/s]Loading train:  32%|███▏      | 19/59 [00:02<00:06,  6.00it/s]Loading train:  34%|███▍      | 20/59 [00:03<00:06,  5.93it/s]Loading train:  36%|███▌      | 21/59 [00:03<00:05,  6.53it/s]Loading train:  37%|███▋      | 22/59 [00:03<00:05,  6.39it/s]Loading train:  39%|███▉      | 23/59 [00:03<00:06,  5.31it/s]Loading train:  41%|████      | 24/59 [00:03<00:06,  5.80it/s]Loading train:  42%|████▏     | 25/59 [00:03<00:05,  6.22it/s]Loading train:  44%|████▍     | 26/59 [00:04<00:04,  6.69it/s]Loading train:  46%|████▌     | 27/59 [00:04<00:04,  7.10it/s]Loading train:  47%|████▋     | 28/59 [00:04<00:04,  7.29it/s]Loading train:  49%|████▉     | 29/59 [00:04<00:04,  7.25it/s]Loading train:  51%|█████     | 30/59 [00:04<00:03,  7.81it/s]Loading train:  53%|█████▎    | 31/59 [00:04<00:03,  8.10it/s]Loading train:  54%|█████▍    | 32/59 [00:04<00:03,  8.33it/s]Loading train:  56%|█████▌    | 33/59 [00:04<00:03,  7.95it/s]Loading train:  58%|█████▊    | 34/59 [00:04<00:02,  8.44it/s]Loading train:  59%|█████▉    | 35/59 [00:05<00:02,  8.61it/s]Loading train:  61%|██████    | 36/59 [00:05<00:02,  8.66it/s]Loading train:  63%|██████▎   | 37/59 [00:05<00:02,  8.36it/s]Loading train:  66%|██████▌   | 39/59 [00:05<00:02,  8.82it/s]Loading train:  68%|██████▊   | 40/59 [00:05<00:02,  8.30it/s]Loading train:  69%|██████▉   | 41/59 [00:05<00:02,  7.30it/s]Loading train:  71%|███████   | 42/59 [00:05<00:02,  7.20it/s]Loading train:  73%|███████▎  | 43/59 [00:06<00:02,  7.31it/s]Loading train:  75%|███████▍  | 44/59 [00:06<00:02,  7.25it/s]Loading train:  76%|███████▋  | 45/59 [00:06<00:01,  7.29it/s]Loading train:  78%|███████▊  | 46/59 [00:06<00:01,  7.25it/s]Loading train:  80%|███████▉  | 47/59 [00:06<00:01,  7.45it/s]Loading train:  81%|████████▏ | 48/59 [00:06<00:01,  7.23it/s]Loading train:  83%|████████▎ | 49/59 [00:06<00:01,  6.94it/s]Loading train:  85%|████████▍ | 50/59 [00:07<00:01,  6.56it/s]Loading train:  86%|████████▋ | 51/59 [00:07<00:01,  7.19it/s]Loading train:  88%|████████▊ | 52/59 [00:07<00:00,  7.39it/s]Loading train:  90%|████████▉ | 53/59 [00:07<00:00,  7.64it/s]Loading train:  92%|█████████▏| 54/59 [00:07<00:00,  7.30it/s]Loading train:  93%|█████████▎| 55/59 [00:07<00:00,  7.33it/s]Loading train:  95%|█████████▍| 56/59 [00:07<00:00,  7.89it/s]Loading train:  97%|█████████▋| 57/59 [00:08<00:00,  7.89it/s]Loading train:  98%|█████████▊| 58/59 [00:08<00:00,  8.20it/s]Loading train: 100%|██████████| 59/59 [00:08<00:00,  8.59it/s]Loading train: 100%|██████████| 59/59 [00:08<00:00,  7.18it/s]
concatenating: train:   0%|          | 0/59 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 4/59 [00:00<00:01, 38.31it/s]concatenating: train:  14%|█▎        | 8/59 [00:00<00:01, 38.66it/s]concatenating: train:  20%|██        | 12/59 [00:00<00:01, 36.27it/s]concatenating: train:  27%|██▋       | 16/59 [00:00<00:01, 36.58it/s]concatenating: train:  32%|███▏      | 19/59 [00:00<00:01, 33.49it/s]concatenating: train:  37%|███▋      | 22/59 [00:00<00:01, 31.67it/s]concatenating: train:  42%|████▏     | 25/59 [00:00<00:01, 31.06it/s]concatenating: train:  49%|████▉     | 29/59 [00:00<00:00, 33.13it/s]concatenating: train:  56%|█████▌    | 33/59 [00:00<00:00, 34.01it/s]concatenating: train:  63%|██████▎   | 37/59 [00:01<00:00, 35.47it/s]concatenating: train:  69%|██████▉   | 41/59 [00:01<00:00, 36.13it/s]concatenating: train:  76%|███████▋  | 45/59 [00:01<00:00, 36.11it/s]concatenating: train:  83%|████████▎ | 49/59 [00:01<00:00, 33.40it/s]concatenating: train:  90%|████████▉ | 53/59 [00:01<00:00, 33.73it/s]concatenating: train:  97%|█████████▋| 57/59 [00:01<00:00, 33.51it/s]concatenating: train: 100%|██████████| 59/59 [00:01<00:00, 34.23it/s]
Loading test:   0%|          | 0/23 [00:00<?, ?it/s]Loading test:   4%|▍         | 1/23 [00:00<00:02,  7.71it/s]Loading test:  13%|█▎        | 3/23 [00:00<00:02,  8.07it/s]Loading test:  17%|█▋        | 4/23 [00:00<00:02,  8.14it/s]Loading test:  26%|██▌       | 6/23 [00:00<00:01,  8.59it/s]Loading test:  30%|███       | 7/23 [00:00<00:01,  8.96it/s]Loading test:  39%|███▉      | 9/23 [00:01<00:01,  8.58it/s]Loading test:  43%|████▎     | 10/23 [00:01<00:01,  8.46it/s]Loading test:  48%|████▊     | 11/23 [00:01<00:01,  7.78it/s]Loading test:  52%|█████▏    | 12/23 [00:01<00:01,  7.90it/s]Loading test:  57%|█████▋    | 13/23 [00:01<00:01,  7.94it/s]Loading test:  61%|██████    | 14/23 [00:01<00:01,  7.84it/s]Loading test:  65%|██████▌   | 15/23 [00:01<00:01,  7.84it/s]Loading test:  70%|██████▉   | 16/23 [00:01<00:00,  7.40it/s]Loading test:  74%|███████▍  | 17/23 [00:02<00:00,  7.16it/s]Loading test:  78%|███████▊  | 18/23 [00:02<00:00,  7.00it/s]Loading test:  83%|████████▎ | 19/23 [00:02<00:00,  6.75it/s]Loading test:  87%|████████▋ | 20/23 [00:02<00:00,  5.99it/s]Loading test:  91%|█████████▏| 21/23 [00:02<00:00,  5.86it/s]Loading test:  96%|█████████▌| 22/23 [00:02<00:00,  6.32it/s]Loading test: 100%|██████████| 23/23 [00:03<00:00,  6.59it/s]Loading test: 100%|██████████| 23/23 [00:03<00:00,  7.46it/s]
concatenating: validation:   0%|          | 0/23 [00:00<?, ?it/s]concatenating: validation:  17%|█▋        | 4/23 [00:00<00:00, 35.03it/s]concatenating: validation:  35%|███▍      | 8/23 [00:00<00:00, 35.94it/s]concatenating: validation:  48%|████▊     | 11/23 [00:00<00:00, 33.63it/s]concatenating: validation:  65%|██████▌   | 15/23 [00:00<00:00, 34.12it/s]concatenating: validation:  83%|████████▎ | 19/23 [00:00<00:00, 34.30it/s]concatenating: validation:  96%|█████████▌| 22/23 [00:00<00:00, 32.46it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 33.19it/s]
Loading trainS:   0%|          | 0/59 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/59 [00:00<00:06,  8.45it/s]Loading trainS:   3%|▎         | 2/59 [00:00<00:06,  8.74it/s]Loading trainS:   5%|▌         | 3/59 [00:00<00:06,  9.06it/s]Loading trainS:   7%|▋         | 4/59 [00:00<00:06,  8.38it/s]Loading trainS:   8%|▊         | 5/59 [00:00<00:06,  8.16it/s]Loading trainS:  10%|█         | 6/59 [00:00<00:06,  7.86it/s]Loading trainS:  12%|█▏        | 7/59 [00:00<00:07,  7.01it/s]Loading trainS:  14%|█▎        | 8/59 [00:01<00:06,  7.43it/s]Loading trainS:  15%|█▌        | 9/59 [00:01<00:06,  7.46it/s]Loading trainS:  17%|█▋        | 10/59 [00:01<00:06,  7.26it/s]Loading trainS:  19%|█▊        | 11/59 [00:01<00:06,  7.06it/s]Loading trainS:  20%|██        | 12/59 [00:01<00:06,  7.20it/s]Loading trainS:  22%|██▏       | 13/59 [00:01<00:06,  7.23it/s]Loading trainS:  24%|██▎       | 14/59 [00:01<00:06,  7.31it/s]Loading trainS:  25%|██▌       | 15/59 [00:02<00:06,  6.97it/s]Loading trainS:  27%|██▋       | 16/59 [00:02<00:06,  6.88it/s]Loading trainS:  29%|██▉       | 17/59 [00:02<00:06,  6.84it/s]Loading trainS:  31%|███       | 18/59 [00:02<00:06,  6.68it/s]Loading trainS:  32%|███▏      | 19/59 [00:02<00:06,  6.47it/s]Loading trainS:  34%|███▍      | 20/59 [00:02<00:06,  6.47it/s]Loading trainS:  36%|███▌      | 21/59 [00:02<00:05,  7.05it/s]Loading trainS:  37%|███▋      | 22/59 [00:03<00:05,  7.12it/s]Loading trainS:  39%|███▉      | 23/59 [00:03<00:06,  5.89it/s]Loading trainS:  41%|████      | 24/59 [00:03<00:05,  6.53it/s]Loading trainS:  42%|████▏     | 25/59 [00:03<00:04,  6.90it/s]Loading trainS:  44%|████▍     | 26/59 [00:03<00:04,  7.27it/s]Loading trainS:  46%|████▌     | 27/59 [00:03<00:04,  7.37it/s]Loading trainS:  47%|████▋     | 28/59 [00:03<00:04,  7.58it/s]Loading trainS:  49%|████▉     | 29/59 [00:04<00:04,  7.38it/s]Loading trainS:  51%|█████     | 30/59 [00:04<00:03,  7.75it/s]Loading trainS:  53%|█████▎    | 31/59 [00:04<00:03,  8.01it/s]Loading trainS:  54%|█████▍    | 32/59 [00:04<00:03,  7.66it/s]Loading trainS:  56%|█████▌    | 33/59 [00:04<00:03,  7.67it/s]Loading trainS:  58%|█████▊    | 34/59 [00:04<00:03,  8.15it/s]Loading trainS:  59%|█████▉    | 35/59 [00:04<00:02,  8.47it/s]Loading trainS:  61%|██████    | 36/59 [00:04<00:02,  8.56it/s]Loading trainS:  63%|██████▎   | 37/59 [00:04<00:02,  8.44it/s]Loading trainS:  66%|██████▌   | 39/59 [00:05<00:02,  8.78it/s]Loading trainS:  68%|██████▊   | 40/59 [00:05<00:02,  8.35it/s]Loading trainS:  69%|██████▉   | 41/59 [00:05<00:02,  7.95it/s]Loading trainS:  71%|███████   | 42/59 [00:05<00:02,  8.04it/s]Loading trainS:  73%|███████▎  | 43/59 [00:05<00:01,  8.18it/s]Loading trainS:  75%|███████▍  | 44/59 [00:05<00:01,  7.94it/s]Loading trainS:  76%|███████▋  | 45/59 [00:05<00:01,  8.15it/s]Loading trainS:  78%|███████▊  | 46/59 [00:06<00:01,  7.91it/s]Loading trainS:  80%|███████▉  | 47/59 [00:06<00:01,  7.97it/s]Loading trainS:  81%|████████▏ | 48/59 [00:06<00:01,  7.80it/s]Loading trainS:  83%|████████▎ | 49/59 [00:06<00:01,  7.55it/s]Loading trainS:  85%|████████▍ | 50/59 [00:06<00:01,  7.62it/s]Loading trainS:  86%|████████▋ | 51/59 [00:06<00:00,  8.08it/s]Loading trainS:  88%|████████▊ | 52/59 [00:06<00:00,  8.18it/s]Loading trainS:  90%|████████▉ | 53/59 [00:06<00:00,  8.51it/s]Loading trainS:  92%|█████████▏| 54/59 [00:07<00:00,  7.82it/s]Loading trainS:  93%|█████████▎| 55/59 [00:07<00:00,  7.20it/s]Loading trainS:  95%|█████████▍| 56/59 [00:07<00:00,  7.33it/s]Loading trainS:  97%|█████████▋| 57/59 [00:07<00:00,  6.95it/s]Loading trainS: 100%|██████████| 59/59 [00:07<00:00,  7.89it/s]Loading trainS: 100%|██████████| 59/59 [00:07<00:00,  7.62it/s]
Loading testS:   0%|          | 0/23 [00:00<?, ?it/s]Loading testS:   4%|▍         | 1/23 [00:00<00:03,  6.84it/s]Loading testS:   9%|▊         | 2/23 [00:00<00:02,  7.41it/s]Loading testS:  13%|█▎        | 3/23 [00:00<00:02,  7.62it/s]Loading testS:  17%|█▋        | 4/23 [00:00<00:02,  7.96it/s]Loading testS:  22%|██▏       | 5/23 [00:00<00:02,  8.18it/s]Loading testS:  26%|██▌       | 6/23 [00:00<00:02,  8.39it/s]Loading testS:  35%|███▍      | 8/23 [00:00<00:01,  8.81it/s]Loading testS:  39%|███▉      | 9/23 [00:01<00:01,  7.21it/s]Loading testS:  43%|████▎     | 10/23 [00:01<00:01,  6.91it/s]Loading testS:  48%|████▊     | 11/23 [00:01<00:01,  6.82it/s]Loading testS:  52%|█████▏    | 12/23 [00:01<00:01,  6.93it/s]Loading testS:  57%|█████▋    | 13/23 [00:01<00:01,  7.25it/s]Loading testS:  61%|██████    | 14/23 [00:01<00:01,  7.53it/s]Loading testS:  65%|██████▌   | 15/23 [00:01<00:01,  7.51it/s]Loading testS:  70%|██████▉   | 16/23 [00:02<00:00,  7.47it/s]Loading testS:  74%|███████▍  | 17/23 [00:02<00:00,  7.31it/s]Loading testS:  78%|███████▊  | 18/23 [00:02<00:00,  7.15it/s]Loading testS:  83%|████████▎ | 19/23 [00:02<00:00,  7.13it/s]Loading testS:  87%|████████▋ | 20/23 [00:02<00:00,  5.94it/s]Loading testS:  91%|█████████▏| 21/23 [00:02<00:00,  5.96it/s]Loading testS:  96%|█████████▌| 22/23 [00:03<00:00,  6.49it/s]Loading testS: 100%|██████████| 23/23 [00:03<00:00,  6.55it/s]Loading testS: 100%|██████████| 23/23 [00:03<00:00,  7.23it/s]
Epoch 00030: val_mDice did not improve from 0.70384
{'val_loss': [0.27100327600049867, 0.26650222543660623, 0.2561791984883075, 0.2597932390356279, 0.2739972312049573, 0.25224144496401646, 0.2588106047492285, 0.2603015942496942, 0.2591892779713419, 0.25392111213594304, 0.25745469624892703, 0.25705875739404827, 0.25516469034092265, 0.2556068679861716, 0.2538823476134723, 0.2521643468263856, 0.24935280020099884, 0.26069407580418275, 0.2533579171207873, 0.25404803599351694, 0.25826108788288166, 0.26575397026411174, 0.25219885141430204, 0.2548549595754069, 0.2561354497374112, 0.25401871260822556, 0.25336840293520335, 0.2532834320790066, 0.2550532529225075, 0.25397981202656145], 'val_acc': [0.7696501858944017, 0.7704944516182484, 0.7706616941877256, 0.7707053135992306, 0.7703413855129057, 0.7711376412409403, 0.7711118287338614, 0.7708436625651603, 0.7710989268481933, 0.7709324415668156, 0.7710756935019638, 0.7713962611243582, 0.7711097627978986, 0.7711729979165646, 0.7711422873793218, 0.7712001020368083, 0.7712682360256135, 0.7708808285475046, 0.7711453791602757, 0.7712439759272196, 0.7705045072309329, 0.7706903390241986, 0.771174545285398, 0.7709159317521931, 0.7709419946054998, 0.7710103931865133, 0.771130405257224, 0.7711257632851197, 0.7713490211842697, 0.7712555878460743], 'val_mDice': [0.686683565142996, 0.6941026294648043, 0.6964509126970706, 0.6981290485436914, 0.6916519548605368, 0.700780955887701, 0.6992370920694667, 0.6976285445784233, 0.6988285951979937, 0.6988931450123448, 0.7005161824798907, 0.7011280044358909, 0.6975463473877482, 0.6970698187020613, 0.6989455732444496, 0.7008699970960348, 0.7038364395616826, 0.7003894016197059, 0.6994997376519367, 0.698756831071288, 0.694214981532285, 0.6972117075589491, 0.7005420704167483, 0.697898493947413, 0.6965075401335182, 0.6987926865242245, 0.6994917026501497, 0.6995893275993115, 0.6976620993055109, 0.698821757933736], 'loss': [0.27676811713624644, 0.25748388549766027, 0.25235254357795456, 0.2487663989131515, 0.24434835302668648, 0.24234135710709803, 0.24116430210100637, 0.23986206042605476, 0.23644518429363096, 0.2381511773612048, 0.23535957851925413, 0.23328646413377813, 0.23409504987098076, 0.23280724721985893, 0.23166861626747492, 0.22799750679248088, 0.2268885327754794, 0.22782875335699804, 0.2271930063898499, 0.2256586604021691, 0.22592221059509227, 0.22551474619556117, 0.22326611626792597, 0.22235328502751686, 0.22405368975690892, 0.22184047505662247, 0.22106788891392784, 0.22168842119139595, 0.2202229054795729, 0.21965716114720782], 'acc': [0.7141606251935702, 0.7162149814335076, 0.7167990787609203, 0.7172445783743987, 0.7174965199586507, 0.7177151369081961, 0.7177643308768401, 0.7178354883516157, 0.718207042764973, 0.7182281887209093, 0.7183322423213238, 0.7185978889465332, 0.7187001890427357, 0.7186410974811863, 0.7186893635504955, 0.718840750488075, 0.7190825786139514, 0.719030191769471, 0.7190724715993211, 0.7191169173330874, 0.7193024416227598, 0.7192491252680082, 0.7194398029430492, 0.7193983471071398, 0.719289756304509, 0.7195020906023077, 0.7196055256031655, 0.7195358139437598, 0.7197327807142928, 0.7198102313119012], 'mDice': [0.715106650784209, 0.7359351378840369, 0.7414579020964133, 0.7453322748880129, 0.7501476278176179, 0.7522757166140789, 0.7535814377101692, 0.7550288615999995, 0.7587092220783234, 0.7568357747954291, 0.7598520823427148, 0.7620912137869242, 0.7612159316604202, 0.7626274316697508, 0.7638188858289976, 0.76782018510071, 0.7689715680238363, 0.7679884216269931, 0.7686626178187292, 0.7703287061807271, 0.7700374416402869, 0.7704628754306484, 0.7729415023649061, 0.7739185408965962, 0.7720895836482177, 0.7744873882950963, 0.7753103866770461, 0.7746057252626162, 0.7762256449944264, 0.776833634118776], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]}
Flip L-R the image & its nuclei

 1-THALAMUS SD: [2] GPU: 3 

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 168, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 168, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 168, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 168, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 168, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 168, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 168, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 84, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 84, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 84, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 84, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 84, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 84, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 84, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 84, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 84, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 42, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 42, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 42, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 42, 160)  640         conv2d_5[0][0]                   2020-09-30 21:43:57.205700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-09-30 21:43:57.206662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 21:43:57.206718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-09-30 21:43:57.206744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-09-30 21:43:57.206943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

predicting test subjects:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects:   4%|▍         | 1/23 [00:00<00:15,  1.38it/s]predicting test subjects:   9%|▊         | 2/23 [00:00<00:11,  1.78it/s]predicting test subjects:  13%|█▎        | 3/23 [00:01<00:09,  2.14it/s]predicting test subjects:  17%|█▋        | 4/23 [00:01<00:07,  2.56it/s]predicting test subjects:  22%|██▏       | 5/23 [00:01<00:06,  2.86it/s]predicting test subjects:  26%|██▌       | 6/23 [00:01<00:05,  2.90it/s]predicting test subjects:  30%|███       | 7/23 [00:02<00:05,  3.10it/s]predicting test subjects:  35%|███▍      | 8/23 [00:02<00:04,  3.50it/s]predicting test subjects:  39%|███▉      | 9/23 [00:02<00:04,  3.03it/s]predicting test subjects:  43%|████▎     | 10/23 [00:03<00:04,  2.88it/s]predicting test subjects:  48%|████▊     | 11/23 [00:03<00:03,  3.12it/s]predicting test subjects:  52%|█████▏    | 12/23 [00:03<00:03,  3.07it/s]predicting test subjects:  57%|█████▋    | 13/23 [00:04<00:03,  3.13it/s]predicting test subjects:  61%|██████    | 14/23 [00:04<00:02,  3.01it/s]predicting test subjects:  65%|██████▌   | 15/23 [00:04<00:02,  3.26it/s]predicting test subjects:  70%|██████▉   | 16/23 [00:05<00:02,  3.23it/s]predicting test subjects:  74%|███████▍  | 17/23 [00:05<00:01,  3.45it/s]predicting test subjects:  78%|███████▊  | 18/23 [00:05<00:01,  3.27it/s]predicting test subjects:  83%|████████▎ | 19/23 [00:05<00:01,  3.37it/s]predicting test subjects:  87%|████████▋ | 20/23 [00:06<00:01,  2.78it/s]predicting test subjects:  91%|█████████▏| 21/23 [00:06<00:00,  2.80it/s]predicting test subjects:  96%|█████████▌| 22/23 [00:07<00:00,  3.06it/s]predicting test subjects: 100%|██████████| 23/23 [00:07<00:00,  3.11it/s]predicting test subjects: 100%|██████████| 23/23 [00:07<00:00,  3.13it/s]
predicting train subjects:   0%|          | 0/59 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/59 [00:00<00:20,  2.78it/s]predicting train subjects:   3%|▎         | 2/59 [00:00<00:17,  3.24it/s]predicting train subjects:   5%|▌         | 3/59 [00:00<00:15,  3.53it/s]predicting train subjects:   7%|▋         | 4/59 [00:01<00:15,  3.67it/s]predicting train subjects:   8%|▊         | 5/59 [00:01<00:13,  3.92it/s]predicting train subjects:  10%|█         | 6/59 [00:01<00:13,  3.95it/s]predicting train subjects:  12%|█▏        | 7/59 [00:01<00:14,  3.57it/s]predicting train subjects:  14%|█▎        | 8/59 [00:02<00:13,  3.88it/s]predicting train subjects:  15%|█▌        | 9/59 [00:02<00:14,  3.54it/s]predicting train subjects:  17%|█▋        | 10/59 [00:02<00:15,  3.13it/s]predicting train subjects:  19%|█▊        | 11/59 [00:03<00:14,  3.23it/s]predicting train subjects:  20%|██        | 12/59 [00:03<00:13,  3.60it/s]predicting train subjects:  22%|██▏       | 13/59 [00:03<00:11,  3.88it/s]predicting train subjects:  24%|██▎       | 14/59 [00:03<00:11,  3.89it/s]predicting train subjects:  25%|██▌       | 15/59 [00:04<00:11,  3.79it/s]predicting train subjects:  27%|██▋       | 16/59 [00:04<00:11,  3.82it/s]predicting train subjects:  29%|██▉       | 17/59 [00:04<00:12,  3.47it/s]predicting train subjects:  31%|███       | 18/59 [00:04<00:11,  3.49it/s]predicting train subjects:  32%|███▏      | 19/59 [00:05<00:12,  3.21it/s]predicting train subjects:  34%|███▍      | 20/59 [00:05<00:11,  3.31it/s]predicting train subjects:  36%|███▌      | 21/59 [00:05<00:11,  3.38it/s]predicting train subjects:  37%|███▋      | 22/59 [00:06<00:11,  3.25it/s]predicting train subjects:  39%|███▉      | 23/59 [00:06<00:13,  2.65it/s]predicting train subjects:  41%|████      | 24/59 [00:06<00:12,  2.87it/s]predicting train subjects:  42%|████▏     | 25/59 [00:07<00:10,  3.33it/s]predicting train subjects:  44%|████▍     | 26/59 [00:07<00:09,  3.32it/s]predicting train subjects:  46%|████▌     | 27/59 [00:07<00:10,  3.18it/s]predicting train subjects:  47%|████▋     | 28/59 [00:08<00:09,  3.27it/s]predicting train subjects:  49%|████▉     | 29/59 [00:08<00:09,  3.25it/s]predicting train subjects:  51%|█████     | 30/59 [00:08<00:08,  3.42it/s]predicting train subjects:  53%|█████▎    | 31/59 [00:08<00:07,  3.54it/s]predicting train subjects:  54%|█████▍    | 32/59 [00:09<00:07,  3.62it/s]predicting train subjects:  56%|█████▌    | 33/59 [00:09<00:07,  3.53it/s]predicting train subjects:  58%|█████▊    | 34/59 [00:09<00:06,  4.04it/s]predicting train subjects:  59%|█████▉    | 35/59 [00:09<00:05,  4.07it/s]predicting train subjects:  61%|██████    | 36/59 [00:10<00:05,  4.30it/s]predicting train subjects:  63%|██████▎   | 37/59 [00:10<00:04,  4.57it/s]predicting train subjects:  64%|██████▍   | 38/59 [00:10<00:04,  4.69it/s]predicting train subjects:  66%|██████▌   | 39/59 [00:10<00:04,  4.99it/s]predicting train subjects:  68%|██████▊   | 40/59 [00:10<00:04,  4.30it/s]predicting train subjects:  69%|██████▉   | 41/59 [00:11<00:05,  3.58it/s]predicting train subjects:  71%|███████   | 42/59 [00:11<00:04,  3.62it/s]predicting train subjects:  73%|███████▎  | 43/59 [00:11<00:04,  3.99it/s]predicting train subjects:  75%|███████▍  | 44/59 [00:12<00:03,  4.23it/s]predicting train subjects:  76%|███████▋  | 45/59 [00:12<00:03,  3.98it/s]predicting train subjects:  78%|███████▊  | 46/59 [00:12<00:03,  3.63it/s]predicting train subjects:  80%|███████▉  | 47/59 [00:12<00:03,  3.38it/s]predicting train subjects:  81%|████████▏ | 48/59 [00:13<00:03,  3.54it/s]predicting train subjects:  83%|████████▎ | 49/59 [00:13<00:02,  3.37it/s]predicting train subjects:  85%|████████▍ | 50/59 [00:13<00:02,  3.28it/s]predicting train subjects:  86%|████████▋ | 51/59 [00:14<00:02,  3.37it/s]predicting train subjects:  88%|████████▊ | 52/59 [00:14<00:01,  3.76it/s]predicting train subjects:  90%|████████▉ | 53/59 [00:14<00:01,  4.14it/s]predicting train subjects:  92%|█████████▏| 54/59 [00:14<00:01,  4.26it/s]predicting train subjects:  93%|█████████▎| 55/59 [00:15<00:01,  3.84it/s]predicting train subjects:  95%|█████████▍| 56/59 [00:15<00:00,  4.20it/s]predicting train subjects:  97%|█████████▋| 57/59 [00:15<00:00,  3.93it/s]predicting train subjects:  98%|█████████▊| 58/59 [00:15<00:00,  4.19it/s]predicting train subjects: 100%|██████████| 59/59 [00:16<00:00,  4.12it/s]predicting train subjects: 100%|██████████| 59/59 [00:16<00:00,  3.68it/s]
predicting test subjects sagittal:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects sagittal:   4%|▍         | 1/23 [00:00<00:07,  2.93it/s]predicting test subjects sagittal:   9%|▊         | 2/23 [00:00<00:06,  3.39it/s]predicting test subjects sagittal:  13%|█▎        | 3/23 [00:00<00:05,  3.62it/s]predicting test subjects sagittal:  17%|█▋        | 4/23 [00:00<00:04,  4.04it/s]predicting test subjects sagittal:  22%|██▏       | 5/23 [00:01<00:04,  4.01it/s]predicting test subjects sagittal:  26%|██▌       | 6/23 [00:01<00:04,  3.87it/s]predicting test subjects sagittal:  30%|███       | 7/23 [00:01<00:04,  3.89it/s]predicting test subjects sagittal:  35%|███▍      | 8/23 [00:01<00:03,  4.10it/s]predicting test subjects sagittal:  39%|███▉      | 9/23 [00:02<00:04,  3.22it/s]predicting test subjects sagittal:  43%|████▎     | 10/23 [00:02<00:04,  3.08it/s]predicting test subjects sagittal:  48%|████▊     | 11/23 [00:03<00:03,  3.26it/s]predicting test subjects sagittal:  52%|█████▏    | 12/23 [00:03<00:03,  3.19it/s]predicting test subjects sagittal:  57%|█████▋    | 13/23 [00:03<00:03,  3.20it/s]predicting test subjects sagittal:  61%|██████    | 14/23 [00:03<00:02,  3.24it/s]predicting test subjects sagittal:  65%|██████▌   | 15/23 [00:04<00:02,  3.40it/s]predicting test subjects sagittal:  70%|██████▉   | 16/23 [00:04<00:02,  3.32it/s]predicting test subjects sagittal:  74%|███████▍  | 17/23 [00:04<00:01,  3.61it/s]predicting test subjects sagittal:  78%|███████▊  | 18/23 [00:05<00:01,  3.49it/s]predicting test subjects sagittal:  83%|████████▎ | 19/23 [00:05<00:01,  3.59it/s]predicting test subjects sagittal:  87%|████████▋ | 20/23 [00:05<00:01,  2.83it/s]predicting test subjects sagittal:  91%|█████████▏| 21/23 [00:06<00:00,  2.96it/s]predicting test subjects sagittal:  96%|█████████▌| 22/23 [00:06<00:00,  2.94it/s]predicting test subjects sagittal: 100%|██████████| 23/23 [00:06<00:00,  2.79it/s]predicting test subjects sagittal: 100%|██████████| 23/23 [00:06<00:00,  3.33it/s]
predicting train subjects sagittal:   0%|          | 0/59 [00:00<?, ?it/s]predicting train subjects sagittal:   2%|▏         | 1/59 [00:00<00:16,  3.46it/s]predicting train subjects sagittal:   3%|▎         | 2/59 [00:00<00:14,  3.92it/s]predicting train subjects sagittal:   5%|▌         | 3/59 [00:00<00:13,  4.10it/s]predicting train subjects sagittal:   7%|▋         | 4/59 [00:00<00:13,  4.15it/s]predicting train subjects sagittal:   8%|▊         | 5/59 [00:01<00:12,  4.31it/s]predicting train subjects sagittal:  10%|█         | 6/59 [00:01<00:13,  4.07it/s]predicting train subjects sagittal:  12%|█▏        | 7/59 [00:01<00:13,  3.73it/s]predicting train subjects sagittal:  14%|█▎        | 8/59 [00:01<00:12,  4.08it/s]predicting train subjects sagittal:  15%|█▌        | 9/59 [00:02<00:14,  3.48it/s]predicting train subjects sagittal:  17%|█▋        | 10/59 [00:02<00:16,  3.06it/s]predicting train subjects sagittal:  19%|█▊        | 11/59 [00:03<00:15,  3.13it/s]predicting train subjects sagittal:  20%|██        | 12/59 [00:03<00:13,  3.41it/s]predicting train subjects sagittal:  22%|██▏       | 13/59 [00:03<00:12,  3.63it/s]predicting train subjects sagittal:  24%|██▎       | 14/59 [00:03<00:12,  3.75it/s]predicting train subjects sagittal:  25%|██▌       | 15/59 [00:04<00:12,  3.48it/s]predicting train subjects sagittal:  27%|██▋       | 16/59 [00:04<00:12,  3.45it/s]predicting train subjects sagittal:  29%|██▉       | 17/59 [00:04<00:12,  3.23it/s]predicting train subjects sagittal:  31%|███       | 18/59 [00:05<00:12,  3.26it/s]predicting train subjects sagittal:  32%|███▏      | 19/59 [00:05<00:12,  3.09it/s]predicting train subjects sagittal:  34%|███▍      | 20/59 [00:05<00:11,  3.34it/s]predicting train subjects sagittal:  36%|███▌      | 21/59 [00:05<00:11,  3.42it/s]predicting train subjects sagittal:  37%|███▋      | 22/59 [00:06<00:10,  3.44it/s]predicting train subjects sagittal:  39%|███▉      | 23/59 [00:06<00:12,  2.80it/s]predicting train subjects sagittal:  41%|████      | 24/59 [00:06<00:11,  2.97it/s]predicting train subjects sagittal:  42%|████▏     | 25/59 [00:07<00:09,  3.47it/s]predicting train subjects sagittal:  44%|████▍     | 26/59 [00:07<00:10,  3.21it/s]predicting train subjects sagittal:  46%|████▌     | 27/59 [00:07<00:10,  3.11it/s]predicting train subjects sagittal:  47%|████▋     | 28/59 [00:08<00:09,  3.18it/s]predicting train subjects sagittal:  49%|████▉     | 29/59 [00:08<00:08,  3.39it/s]predicting train subjects sagittal:  51%|█████     | 30/59 [00:08<00:08,  3.48it/s]predicting train subjects sagittal:  53%|█████▎    | 31/59 [00:08<00:07,  3.57it/s]predicting train subjects sagittal:  54%|█████▍    | 32/59 [00:09<00:07,  3.56it/s]predicting train subjects sagittal:  56%|█████▌    | 33/59 [00:09<00:07,  3.66it/s]predicting train subjects sagittal:  58%|█████▊    | 34/59 [00:09<00:05,  4.22it/s]predicting train subjects sagittal:  59%|█████▉    | 35/59 [00:09<00:05,  4.34it/s]predicting train subjects sagittal:  61%|██████    | 36/59 [00:10<00:05,  4.54it/s]predicting train subjects sagittal:  63%|██████▎   | 37/59 [00:10<00:04,  4.58it/s]predicting train subjects sagittal:  64%|██████▍   | 38/59 [00:10<00:04,  4.72it/s]predicting train subjects sagittal:  66%|██████▌   | 39/59 [00:10<00:04,  4.96it/s]predicting train subjects sagittal:  68%|██████▊   | 40/59 [00:10<00:04,  4.20it/s]predicting train subjects sagittal:  69%|██████▉   | 41/59 [00:11<00:04,  3.72it/s]predicting train subjects sagittal:  71%|███████   | 42/59 [00:11<00:04,  3.56it/s]predicting train subjects sagittal:  73%|███████▎  | 43/59 [00:11<00:04,  3.91it/s]predicting train subjects sagittal:  75%|███████▍  | 44/59 [00:12<00:03,  4.13it/s]predicting train subjects sagittal:  76%|███████▋  | 45/59 [00:12<00:03,  4.04it/s]predicting train subjects sagittal:  78%|███████▊  | 46/59 [00:12<00:03,  3.63it/s]predicting train subjects sagittal:  80%|███████▉  | 47/59 [00:12<00:03,  3.68it/s]predicting train subjects sagittal:  81%|████████▏ | 48/59 [00:13<00:02,  3.88it/s]predicting train subjects sagittal:  83%|████████▎ | 49/59 [00:13<00:02,  3.68it/s]predicting train subjects sagittal:  85%|████████▍ | 50/59 [00:13<00:02,  3.61it/s]predicting train subjects sagittal:  86%|████████▋ | 51/59 [00:14<00:02,  3.51it/s]predicting train subjects sagittal:  88%|████████▊ | 52/59 [00:14<00:01,  3.79it/s]predicting train subjects sagittal:  90%|████████▉ | 53/59 [00:14<00:01,  4.18it/s]predicting train subjects sagittal:  92%|█████████▏| 54/59 [00:14<00:01,  4.36it/s]predicting train subjects sagittal:  93%|█████████▎| 55/59 [00:14<00:01,  3.87it/s]predicting train subjects sagittal:  95%|█████████▍| 56/59 [00:15<00:00,  4.12it/s]predicting train subjects sagittal:  97%|█████████▋| 57/59 [00:15<00:00,  3.77it/s]predicting train subjects sagittal:  98%|█████████▊| 58/59 [00:15<00:00,  4.08it/s]predicting train subjects sagittal: 100%|██████████| 59/59 [00:15<00:00,  4.26it/s]predicting train subjects sagittal: 100%|██████████| 59/59 [00:15<00:00,  3.72it/s]
saving BB  test1-THALAMUS:   0%|          | 0/23 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  22%|██▏       | 5/23 [00:00<00:00, 47.74it/s]saving BB  test1-THALAMUS:  43%|████▎     | 10/23 [00:00<00:00, 46.85it/s]saving BB  test1-THALAMUS:  65%|██████▌   | 15/23 [00:00<00:00, 45.64it/s]saving BB  test1-THALAMUS:  87%|████████▋ | 20/23 [00:00<00:00, 44.80it/s]saving BB  test1-THALAMUS: 100%|██████████| 23/23 [00:00<00:00, 44.21it/s]
saving BB  train1-THALAMUS:   0%|          | 0/59 [00:00<?, ?it/s]saving BB  train1-THALAMUS:  10%|█         | 6/59 [00:00<00:01, 45.37it/s]saving BB  train1-THALAMUS:  19%|█▊        | 11/59 [00:00<00:01, 44.72it/s]saving BB  train1-THALAMUS:  27%|██▋       | 16/59 [00:00<00:00, 45.47it/s]saving BB  train1-THALAMUS:  36%|███▌      | 21/59 [00:00<00:00, 45.78it/s]saving BB  train1-THALAMUS:  44%|████▍     | 26/59 [00:00<00:00, 46.52it/s]saving BB  train1-THALAMUS:  54%|█████▍    | 32/59 [00:00<00:00, 48.39it/s]saving BB  train1-THALAMUS:  64%|██████▍   | 38/59 [00:00<00:00, 50.03it/s]saving BB  train1-THALAMUS:  75%|███████▍  | 44/59 [00:00<00:00, 50.39it/s]saving BB  train1-THALAMUS:  83%|████████▎ | 49/59 [00:01<00:00, 49.95it/s]saving BB  train1-THALAMUS:  93%|█████████▎| 55/59 [00:01<00:00, 49.93it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 48.96it/s]
saving BB  test1-THALAMUS:   0%|          | 0/23 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  26%|██▌       | 6/23 [00:00<00:00, 55.84it/s]saving BB  test1-THALAMUS:  43%|████▎     | 10/23 [00:00<00:00, 49.45it/s]saving BB  test1-THALAMUS:  65%|██████▌   | 15/23 [00:00<00:00, 47.18it/s]saving BB  test1-THALAMUS:  87%|████████▋ | 20/23 [00:00<00:00, 46.06it/s]saving BB  test1-THALAMUS: 100%|██████████| 23/23 [00:00<00:00, 44.60it/s]
saving BB  train1-THALAMUS:   0%|          | 0/59 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   7%|▋         | 4/59 [00:00<00:01, 39.61it/s]saving BB  train1-THALAMUS:  14%|█▎        | 8/59 [00:00<00:01, 39.48it/s]saving BB  train1-THALAMUS:  20%|██        | 12/59 [00:00<00:01, 36.29it/s]saving BB  train1-THALAMUS:  29%|██▉       | 17/59 [00:00<00:01, 37.16it/s]saving BB  train1-THALAMUS:  37%|███▋      | 22/59 [00:00<00:00, 38.83it/s]saving BB  train1-THALAMUS:  44%|████▍     | 26/59 [00:00<00:00, 38.48it/s]saving BB  train1-THALAMUS:  53%|█████▎    | 31/59 [00:00<00:00, 40.59it/s]saving BB  train1-THALAMUS:  63%|██████▎   | 37/59 [00:00<00:00, 44.00it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 43/59 [00:01<00:00, 46.16it/s]saving BB  train1-THALAMUS:  81%|████████▏ | 48/59 [00:01<00:00, 44.31it/s]saving BB  train1-THALAMUS:  90%|████████▉ | 53/59 [00:01<00:00, 44.16it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 45.16it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 42.79it/s]
Loading train:   0%|          | 0/59 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/59 [00:00<00:34,  1.68it/s]Loading train:   3%|▎         | 2/59 [00:01<00:31,  1.80it/s]Loading train:   5%|▌         | 3/59 [00:01<00:31,  1.77it/s]Loading train:   7%|▋         | 4/59 [00:02<00:30,  1.82it/s]Loading train:   8%|▊         | 5/59 [00:02<00:30,  1.78it/s]Loading train:  10%|█         | 6/59 [00:03<00:35,  1.50it/s]Loading train:  12%|█▏        | 7/59 [00:04<00:41,  1.26it/s]Loading train:  14%|█▎        | 8/59 [00:05<00:34,  1.48it/s]Loading train:  15%|█▌        | 9/59 [00:05<00:31,  1.57it/s]Loading train:  17%|█▋        | 10/59 [00:06<00:30,  1.61it/s]Loading train:  19%|█▊        | 11/59 [00:06<00:30,  1.59it/s]Loading train:  20%|██        | 12/59 [00:07<00:35,  1.32it/s]Loading train:  22%|██▏       | 13/59 [00:08<00:30,  1.49it/s]Loading train:  24%|██▎       | 14/59 [00:09<00:29,  1.53it/s]Loading train:  25%|██▌       | 15/59 [00:09<00:31,  1.40it/s]Loading train:  27%|██▋       | 16/59 [00:10<00:31,  1.37it/s]Loading train:  29%|██▉       | 17/59 [00:11<00:27,  1.52it/s]Loading train:  31%|███       | 18/59 [00:11<00:26,  1.53it/s]Loading train:  32%|███▏      | 19/59 [00:12<00:29,  1.34it/s]Loading train:  34%|███▍      | 20/59 [00:13<00:27,  1.41it/s]Loading train:  36%|███▌      | 21/59 [00:13<00:22,  1.68it/s]Loading train:  37%|███▋      | 22/59 [00:14<00:20,  1.77it/s]Loading train:  39%|███▉      | 23/59 [00:15<00:28,  1.25it/s]Loading train:  41%|████      | 24/59 [00:15<00:22,  1.56it/s]Loading train:  42%|████▏     | 25/59 [00:16<00:20,  1.69it/s]Loading train:  44%|████▍     | 26/59 [00:16<00:16,  1.96it/s]Loading train:  46%|████▌     | 27/59 [00:17<00:14,  2.15it/s]Loading train:  47%|████▋     | 28/59 [00:17<00:12,  2.41it/s]Loading train:  49%|████▉     | 29/59 [00:17<00:11,  2.69it/s]Loading train:  51%|█████     | 30/59 [00:17<00:10,  2.82it/s]Loading train:  53%|█████▎    | 31/59 [00:18<00:09,  2.90it/s]Loading train:  54%|█████▍    | 32/59 [00:18<00:09,  2.93it/s]Loading train:  56%|█████▌    | 33/59 [00:19<00:14,  1.85it/s]Loading train:  58%|█████▊    | 34/59 [00:19<00:11,  2.21it/s]Loading train:  59%|█████▉    | 35/59 [00:20<00:09,  2.56it/s]Loading train:  61%|██████    | 36/59 [00:20<00:08,  2.61it/s]Loading train:  63%|██████▎   | 37/59 [00:20<00:08,  2.68it/s]Loading train:  64%|██████▍   | 38/59 [00:21<00:07,  2.97it/s]Loading train:  66%|██████▌   | 39/59 [00:21<00:06,  3.22it/s]Loading train:  68%|██████▊   | 40/59 [00:21<00:06,  3.12it/s]Loading train:  69%|██████▉   | 41/59 [00:21<00:06,  2.98it/s]Loading train:  71%|███████   | 42/59 [00:22<00:07,  2.17it/s]Loading train:  73%|███████▎  | 43/59 [00:23<00:06,  2.36it/s]Loading train:  75%|███████▍  | 44/59 [00:23<00:05,  2.59it/s]Loading train:  76%|███████▋  | 45/59 [00:23<00:04,  2.94it/s]Loading train:  78%|███████▊  | 46/59 [00:23<00:04,  2.92it/s]Loading train:  80%|███████▉  | 47/59 [00:24<00:03,  3.07it/s]Loading train:  81%|████████▏ | 48/59 [00:24<00:03,  3.01it/s]Loading train:  83%|████████▎ | 49/59 [00:24<00:03,  3.12it/s]Loading train:  85%|████████▍ | 50/59 [00:25<00:02,  3.10it/s]Loading train:  86%|████████▋ | 51/59 [00:25<00:02,  3.24it/s]Loading train:  88%|████████▊ | 52/59 [00:25<00:02,  3.14it/s]Loading train:  90%|████████▉ | 53/59 [00:26<00:01,  3.18it/s]Loading train:  92%|█████████▏| 54/59 [00:26<00:01,  3.09it/s]Loading train:  93%|█████████▎| 55/59 [00:26<00:01,  3.15it/s]Loading train:  95%|█████████▍| 56/59 [00:27<00:00,  3.10it/s]Loading train:  97%|█████████▋| 57/59 [00:27<00:00,  3.22it/s]Loading train:  98%|█████████▊| 58/59 [00:27<00:00,  3.24it/s]Loading train: 100%|██████████| 59/59 [00:27<00:00,  3.48it/s]Loading train: 100%|██████████| 59/59 [00:27<00:00,  2.11it/s]
concatenating: train:   0%|          | 0/59 [00:00<?, ?it/s]concatenating: train:  12%|█▏        | 7/59 [00:00<00:00, 60.16it/s]concatenating: train:  22%|██▏       | 13/59 [00:00<00:00, 58.87it/s]concatenating: train:  32%|███▏      | 19/59 [00:00<00:00, 58.46it/s]concatenating: train:  42%|████▏     | 25/59 [00:00<00:00, 57.88it/s]concatenating: train:  53%|█████▎    | 31/59 [00:00<00:00, 57.88it/s]concatenating: train:  63%|██████▎   | 37/59 [00:00<00:00, 57.63it/s]concatenating: train:  73%|███████▎  | 43/59 [00:00<00:00, 57.48it/s]concatenating: train:  83%|████████▎ | 49/59 [00:00<00:00, 56.80it/s]concatenating: train:  93%|█████████▎| 55/59 [00:00<00:00, 56.20it/s]concatenating: train: 100%|██████████| 59/59 [00:01<00:00, 57.39it/s]
Loading test:   0%|          | 0/23 [00:00<?, ?it/s]Loading test:   4%|▍         | 1/23 [00:00<00:07,  2.78it/s]Loading test:   9%|▊         | 2/23 [00:00<00:07,  2.90it/s]Loading test:  13%|█▎        | 3/23 [00:00<00:06,  2.97it/s]Loading test:  17%|█▋        | 4/23 [00:01<00:09,  2.07it/s]Loading test:  22%|██▏       | 5/23 [00:02<00:07,  2.34it/s]Loading test:  26%|██▌       | 6/23 [00:02<00:06,  2.52it/s]Loading test:  30%|███       | 7/23 [00:02<00:05,  2.71it/s]Loading test:  35%|███▍      | 8/23 [00:03<00:05,  2.76it/s]Loading test:  39%|███▉      | 9/23 [00:03<00:05,  2.63it/s]Loading test:  43%|████▎     | 10/23 [00:03<00:04,  2.76it/s]Loading test:  48%|████▊     | 11/23 [00:04<00:06,  1.94it/s]Loading test:  52%|█████▏    | 12/23 [00:05<00:06,  1.71it/s]Loading test:  57%|█████▋    | 13/23 [00:05<00:05,  1.95it/s]Loading test:  61%|██████    | 14/23 [00:06<00:03,  2.27it/s]Loading test:  65%|██████▌   | 15/23 [00:06<00:03,  2.45it/s]Loading test:  70%|██████▉   | 16/23 [00:06<00:02,  2.63it/s]Loading test:  74%|███████▍  | 17/23 [00:07<00:02,  2.45it/s]Loading test:  78%|███████▊  | 18/23 [00:08<00:02,  1.75it/s]Loading test:  83%|████████▎ | 19/23 [00:08<00:02,  1.81it/s]Loading test:  87%|████████▋ | 20/23 [00:10<00:02,  1.24it/s]Loading test:  91%|█████████▏| 21/23 [00:10<00:01,  1.32it/s]Loading test:  96%|█████████▌| 22/23 [00:11<00:00,  1.45it/s]Loading test: 100%|██████████| 23/23 [00:12<00:00,  1.21it/s]Loading test: 100%|██████████| 23/23 [00:12<00:00,  1.86it/s]
concatenating: validation:   0%|          | 0/23 [00:00<?, ?it/s]concatenating: validation:  30%|███       | 7/23 [00:00<00:00, 62.50it/s]concatenating: validation:  57%|█████▋    | 13/23 [00:00<00:00, 60.30it/s]concatenating: validation:  87%|████████▋ | 20/23 [00:00<00:00, 60.30it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 59.44it/s]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 42, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 42, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 42, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 42, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 42, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 42, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 84, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 84, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 84, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 84, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 84, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 84, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 84, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 84, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 84, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 84, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 168, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 168, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 168, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 168, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 168, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 168, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 168, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 168, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 168, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 168, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 168, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________

 MultiClass_24567891011121314 SD: [0] GPU: 3 

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 84, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 84, 52, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 84, 52, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 84, 52, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 84, 52, 40)   14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 84, 52, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 84, 52, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 42, 26, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 42, 26, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 42, 26, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 42, 26, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 42, 26, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 42, 26, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 42, 26, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 42, 26, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 42, 26, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 21, 13, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 21, 13, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 21, 13, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 21, 13, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 21, 13, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 21, 13, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 21, 13, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 21, 13, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 21, 13, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 21, 13, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 42, 26, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 42, 26, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 42, 26, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 42, 26, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 42, 26, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 42, 26, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 42, 26, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 42, 26, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 42, 26, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 42, 26, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 84, 52, 40)   44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 84, 52, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 84, 52, 40)   28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 84, 52, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 84, 52, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 84, 52, 40)   14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 84, 52, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 84, 52, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 84, 52, 120)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 84, 52, 120)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 84, 52, 13)   1573        dropout_5[0][0]                  
==================================================================================================
Total params: 889,253
Trainable params: 887,653
Non-trainable params: 1,6002020-09-30 21:45:33.253183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-09-30 21:45:33.253348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 21:45:33.253375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-09-30 21:45:33.253392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-09-30 21:45:33.253583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

predicting test subjects:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects:   4%|▍         | 1/23 [00:01<00:23,  1.05s/it]predicting test subjects:   9%|▊         | 2/23 [00:01<00:18,  1.15it/s]predicting test subjects:  13%|█▎        | 3/23 [00:01<00:14,  1.34it/s]predicting test subjects:  17%|█▋        | 4/23 [00:02<00:12,  1.47it/s]predicting test subjects:  22%|██▏       | 5/23 [00:02<00:11,  1.60it/s]predicting test subjects:  26%|██▌       | 6/23 [00:03<00:10,  1.68it/s]predicting test subjects:  30%|███       | 7/23 [00:03<00:08,  1.78it/s]predicting test subjects:  35%|███▍      | 8/23 [00:04<00:08,  1.85it/s]predicting test subjects:  39%|███▉      | 9/23 [00:05<00:08,  1.67it/s]predicting test subjects:  43%|████▎     | 10/23 [00:05<00:08,  1.57it/s]predicting test subjects:  48%|████▊     | 11/23 [00:06<00:07,  1.55it/s]predicting test subjects:  52%|█████▏    | 12/23 [00:07<00:07,  1.57it/s]predicting test subjects:  57%|█████▋    | 13/23 [00:07<00:05,  1.67it/s]predicting test subjects:  61%|██████    | 14/23 [00:08<00:05,  1.74it/s]predicting test subjects:  65%|██████▌   | 15/23 [00:08<00:04,  1.77it/s]predicting test subjects:  70%|██████▉   | 16/23 [00:09<00:03,  1.75it/s]predicting test subjects:  74%|███████▍  | 17/23 [00:09<00:03,  1.78it/s]predicting test subjects:  78%|███████▊  | 18/23 [00:10<00:02,  1.75it/s]predicting test subjects:  83%|████████▎ | 19/23 [00:11<00:02,  1.75it/s]predicting test subjects:  87%|████████▋ | 20/23 [00:11<00:02,  1.48it/s]predicting test subjects:  91%|█████████▏| 21/23 [00:12<00:01,  1.49it/s]predicting test subjects:  96%|█████████▌| 22/23 [00:13<00:00,  1.55it/s]predicting test subjects: 100%|██████████| 23/23 [00:13<00:00,  1.53it/s]predicting test subjects: 100%|██████████| 23/23 [00:13<00:00,  1.65it/s]
Loading train:   0%|          | 0/59 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/59 [00:00<00:07,  7.57it/s]Loading train:   3%|▎         | 2/59 [00:00<00:07,  7.67it/s]Loading train:   5%|▌         | 3/59 [00:00<00:07,  7.33it/s]Loading train:   7%|▋         | 4/59 [00:00<00:07,  7.26it/s]Loading train:   8%|▊         | 5/59 [00:00<00:07,  7.42it/s]Loading train:  10%|█         | 6/59 [00:00<00:07,  7.11it/s]Loading train:  12%|█▏        | 7/59 [00:01<00:07,  6.57it/s]Loading train:  14%|█▎        | 8/59 [00:01<00:07,  6.79it/s]Loading train:  15%|█▌        | 9/59 [00:01<00:07,  6.46it/s]Loading train:  17%|█▋        | 10/59 [00:01<00:08,  6.06it/s]Loading train:  19%|█▊        | 11/59 [00:01<00:08,  5.95it/s]Loading train:  20%|██        | 12/59 [00:01<00:07,  6.15it/s]Loading train:  22%|██▏       | 13/59 [00:01<00:07,  6.23it/s]Loading train:  24%|██▎       | 14/59 [00:02<00:07,  6.27it/s]Loading train:  25%|██▌       | 15/59 [00:02<00:06,  6.36it/s]Loading train:  27%|██▋       | 16/59 [00:02<00:06,  6.15it/s]Loading train:  29%|██▉       | 17/59 [00:02<00:06,  6.16it/s]Loading train:  31%|███       | 18/59 [00:02<00:06,  6.21it/s]Loading train:  32%|███▏      | 19/59 [00:02<00:06,  5.92it/s]Loading train:  34%|███▍      | 20/59 [00:03<00:06,  5.84it/s]Loading train:  36%|███▌      | 21/59 [00:03<00:06,  6.30it/s]Loading train:  37%|███▋      | 22/59 [00:03<00:05,  6.39it/s]Loading train:  39%|███▉      | 23/59 [00:03<00:06,  5.84it/s]Loading train:  42%|████▏     | 25/59 [00:03<00:05,  6.58it/s]Loading train:  44%|████▍     | 26/59 [00:03<00:04,  6.93it/s]Loading train:  46%|████▌     | 27/59 [00:04<00:04,  7.07it/s]Loading train:  47%|████▋     | 28/59 [00:04<00:04,  7.26it/s]Loading train:  49%|████▉     | 29/59 [00:04<00:03,  7.63it/s]Loading train:  51%|█████     | 30/59 [00:04<00:03,  8.06it/s]Loading train:  53%|█████▎    | 31/59 [00:04<00:03,  8.11it/s]Loading train:  54%|█████▍    | 32/59 [00:04<00:03,  7.86it/s]Loading train:  56%|█████▌    | 33/59 [00:04<00:03,  8.02it/s]Loading train:  59%|█████▉    | 35/59 [00:04<00:02,  9.28it/s]Loading train:  63%|██████▎   | 37/59 [00:05<00:02,  8.88it/s]Loading train:  66%|██████▌   | 39/59 [00:05<00:01, 10.15it/s]Loading train:  69%|██████▉   | 41/59 [00:05<00:02,  8.89it/s]Loading train:  71%|███████   | 42/59 [00:05<00:02,  8.33it/s]Loading train:  73%|███████▎  | 43/59 [00:05<00:01,  8.05it/s]Loading train:  75%|███████▍  | 44/59 [00:06<00:01,  8.47it/s]Loading train:  76%|███████▋  | 45/59 [00:06<00:01,  8.59it/s]Loading train:  78%|███████▊  | 46/59 [00:06<00:01,  8.00it/s]Loading train:  80%|███████▉  | 47/59 [00:06<00:01,  8.03it/s]Loading train:  81%|████████▏ | 48/59 [00:06<00:01,  7.88it/s]Loading train:  83%|████████▎ | 49/59 [00:06<00:01,  7.85it/s]Loading train:  85%|████████▍ | 50/59 [00:06<00:01,  7.84it/s]Loading train:  86%|████████▋ | 51/59 [00:06<00:01,  7.88it/s]Loading train:  88%|████████▊ | 52/59 [00:07<00:00,  7.93it/s]Loading train:  90%|████████▉ | 53/59 [00:07<00:00,  8.18it/s]Loading train:  92%|█████████▏| 54/59 [00:07<00:00,  7.88it/s]Loading train:  93%|█████████▎| 55/59 [00:07<00:00,  7.13it/s]Loading train:  95%|█████████▍| 56/59 [00:07<00:00,  7.38it/s]Loading train:  97%|█████████▋| 57/59 [00:07<00:00,  7.51it/s]Loading train:  98%|█████████▊| 58/59 [00:07<00:00,  7.67it/s]Loading train: 100%|██████████| 59/59 [00:07<00:00,  7.73it/s]Loading train: 100%|██████████| 59/59 [00:07<00:00,  7.40it/s]
concatenating: train:   0%|          | 0/59 [00:00<?, ?it/s]concatenating: train:   5%|▌         | 3/59 [00:00<00:02, 25.14it/s]concatenating: train:  10%|█         | 6/59 [00:00<00:02, 25.04it/s]concatenating: train:  15%|█▌        | 9/59 [00:00<00:02, 24.54it/s]concatenating: train:  20%|██        | 12/59 [00:00<00:01, 23.81it/s]concatenating: train:  25%|██▌       | 15/59 [00:00<00:01, 24.03it/s]concatenating: train:  31%|███       | 18/59 [00:00<00:01, 23.61it/s]concatenating: train:  36%|███▌      | 21/59 [00:00<00:01, 23.93it/s]concatenating: train:  41%|████      | 24/59 [00:01<00:01, 23.16it/s]concatenating: train:  46%|████▌     | 27/59 [00:01<00:01, 24.13it/s]concatenating: train:  51%|█████     | 30/59 [00:01<00:01, 24.80it/s]concatenating: train:  56%|█████▌    | 33/59 [00:01<00:01, 25.07it/s]concatenating: train:  63%|██████▎   | 37/59 [00:01<00:00, 27.66it/s]concatenating: train:  69%|██████▉   | 41/59 [00:01<00:00, 29.44it/s]concatenating: train:  76%|███████▋  | 45/59 [00:01<00:00, 29.03it/s]concatenating: train:  81%|████████▏ | 48/59 [00:01<00:00, 28.17it/s]concatenating: train:  86%|████████▋ | 51/59 [00:01<00:00, 27.67it/s]concatenating: train:  92%|█████████▏| 54/59 [00:02<00:00, 27.98it/s]concatenating: train:  97%|█████████▋| 57/59 [00:02<00:00, 28.11it/s]concatenating: train: 100%|██████████| 59/59 [00:02<00:00, 26.31it/s]
Loading test:   0%|          | 0/23 [00:00<?, ?it/s]Loading test:   4%|▍         | 1/23 [00:00<00:03,  6.12it/s]Loading test:   9%|▊         | 2/23 [00:00<00:03,  6.47it/s]Loading test:  13%|█▎        | 3/23 [00:00<00:02,  6.89it/s]Loading test:  17%|█▋        | 4/23 [00:00<00:02,  7.29it/s]Loading test:  22%|██▏       | 5/23 [00:00<00:02,  7.46it/s]Loading test:  26%|██▌       | 6/23 [00:00<00:02,  7.46it/s]Loading test:  30%|███       | 7/23 [00:00<00:02,  7.89it/s]Loading test:  35%|███▍      | 8/23 [00:01<00:01,  8.14it/s]Loading test:  39%|███▉      | 9/23 [00:01<00:02,  6.99it/s]Loading test:  43%|████▎     | 10/23 [00:01<00:01,  6.57it/s]Loading test:  48%|████▊     | 11/23 [00:01<00:01,  6.38it/s]Loading test:  52%|█████▏    | 12/23 [00:01<00:01,  6.30it/s]Loading test:  57%|█████▋    | 13/23 [00:01<00:01,  6.65it/s]Loading test:  61%|██████    | 14/23 [00:01<00:01,  7.03it/s]Loading test:  65%|██████▌   | 15/23 [00:02<00:01,  7.01it/s]Loading test:  70%|██████▉   | 16/23 [00:02<00:01,  6.90it/s]Loading test:  74%|███████▍  | 17/23 [00:02<00:00,  6.64it/s]Loading test:  78%|███████▊  | 18/23 [00:02<00:00,  6.43it/s]Loading test:  83%|████████▎ | 19/23 [00:02<00:00,  6.31it/s]Loading test:  87%|████████▋ | 20/23 [00:03<00:00,  5.39it/s]Loading test:  91%|█████████▏| 21/23 [00:03<00:00,  5.42it/s]Loading test:  96%|█████████▌| 22/23 [00:03<00:00,  5.84it/s]Loading test: 100%|██████████| 23/23 [00:03<00:00,  5.85it/s]Loading test: 100%|██████████| 23/23 [00:03<00:00,  6.56it/s]
concatenating: validation:   0%|          | 0/23 [00:00<?, ?it/s]concatenating: validation:  17%|█▋        | 4/23 [00:00<00:00, 37.19it/s]concatenating: validation:  35%|███▍      | 8/23 [00:00<00:00, 37.83it/s]concatenating: validation:  48%|████▊     | 11/23 [00:00<00:00, 33.77it/s]concatenating: validation:  65%|██████▌   | 15/23 [00:00<00:00, 33.81it/s]concatenating: validation:  83%|████████▎ | 19/23 [00:00<00:00, 34.46it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 33.60it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 34.04it/s]
__________________________________________________________________________________________________

 1-THALAMUS SD: [1] GPU: 3 

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 112, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 112, 116, 30) 300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 112, 116, 30) 120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 112, 116, 30) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 112, 116, 30) 8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 112, 116, 30) 120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 112, 116, 30) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 56, 58, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 56, 58, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 56, 58, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 56, 58, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 56, 58, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 56, 58, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 56, 58, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 56, 58, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 58, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 28, 29, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 28, 29, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 28, 29, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 28, 29, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 28, 29, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 28, 29, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 28, 29, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 28, 29, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 29, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 28, 29, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 56, 58, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 56, 58, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 56, 58, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 56, 58, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 56, 58, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 56, 58, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 56, 58, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 56, 58, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 56, 58, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 56, 58, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 112, 116, 30) 25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 112, 116, 60) 0           conv2d_transpose_2[0][0]         2020-09-30 21:46:05.671799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-09-30 21:46:05.673180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 21:46:05.673261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-09-30 21:46:05.673284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-09-30 21:46:05.673503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

predicting test subjects:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects:   4%|▍         | 1/23 [00:00<00:15,  1.42it/s]predicting test subjects:   9%|▊         | 2/23 [00:00<00:11,  1.84it/s]predicting test subjects:  13%|█▎        | 3/23 [00:01<00:08,  2.28it/s]predicting test subjects:  17%|█▋        | 4/23 [00:01<00:06,  2.77it/s]predicting test subjects:  22%|██▏       | 5/23 [00:01<00:06,  2.83it/s]predicting test subjects:  26%|██▌       | 6/23 [00:01<00:05,  3.28it/s]predicting test subjects:  30%|███       | 7/23 [00:01<00:04,  3.80it/s]predicting test subjects:  35%|███▍      | 8/23 [00:02<00:04,  3.66it/s]predicting test subjects:  39%|███▉      | 9/23 [00:02<00:04,  3.36it/s]predicting test subjects:  43%|████▎     | 10/23 [00:02<00:04,  3.15it/s]predicting test subjects:  48%|████▊     | 11/23 [00:03<00:03,  3.39it/s]predicting test subjects:  52%|█████▏    | 12/23 [00:03<00:03,  3.31it/s]predicting test subjects:  57%|█████▋    | 13/23 [00:03<00:02,  3.35it/s]predicting test subjects:  61%|██████    | 14/23 [00:04<00:02,  3.42it/s]predicting test subjects:  65%|██████▌   | 15/23 [00:04<00:02,  3.38it/s]predicting test subjects:  70%|██████▉   | 16/23 [00:04<00:02,  3.43it/s]predicting test subjects:  74%|███████▍  | 17/23 [00:04<00:01,  3.73it/s]predicting test subjects:  78%|███████▊  | 18/23 [00:05<00:01,  3.90it/s]predicting test subjects:  83%|████████▎ | 19/23 [00:05<00:00,  4.05it/s]predicting test subjects:  87%|████████▋ | 20/23 [00:05<00:00,  3.26it/s]predicting test subjects:  91%|█████████▏| 21/23 [00:06<00:00,  3.44it/s]predicting test subjects:  96%|█████████▌| 22/23 [00:06<00:00,  3.78it/s]predicting test subjects: 100%|██████████| 23/23 [00:06<00:00,  3.82it/s]predicting test subjects: 100%|██████████| 23/23 [00:06<00:00,  3.54it/s]
predicting train subjects:   0%|          | 0/59 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/59 [00:00<00:11,  5.22it/s]predicting train subjects:   3%|▎         | 2/59 [00:00<00:10,  5.41it/s]predicting train subjects:   5%|▌         | 3/59 [00:00<00:11,  4.99it/s]predicting train subjects:   7%|▋         | 4/59 [00:00<00:11,  4.70it/s]predicting train subjects:   8%|▊         | 5/59 [00:01<00:11,  4.57it/s]predicting train subjects:  10%|█         | 6/59 [00:01<00:11,  4.47it/s]predicting train subjects:  12%|█▏        | 7/59 [00:01<00:13,  3.94it/s]predicting train subjects:  14%|█▎        | 8/59 [00:01<00:11,  4.36it/s]predicting train subjects:  15%|█▌        | 9/59 [00:02<00:11,  4.35it/s]predicting train subjects:  17%|█▋        | 10/59 [00:02<00:12,  4.08it/s]predicting train subjects:  19%|█▊        | 11/59 [00:02<00:12,  3.85it/s]predicting train subjects:  20%|██        | 12/59 [00:02<00:12,  3.62it/s]predicting train subjects:  22%|██▏       | 13/59 [00:03<00:11,  3.97it/s]predicting train subjects:  24%|██▎       | 14/59 [00:03<00:10,  4.12it/s]predicting train subjects:  25%|██▌       | 15/59 [00:03<00:11,  3.97it/s]predicting train subjects:  27%|██▋       | 16/59 [00:03<00:12,  3.58it/s]predicting train subjects:  29%|██▉       | 17/59 [00:04<00:10,  3.89it/s]predicting train subjects:  31%|███       | 18/59 [00:04<00:10,  3.92it/s]predicting train subjects:  32%|███▏      | 19/59 [00:04<00:11,  3.61it/s]predicting train subjects:  34%|███▍      | 20/59 [00:05<00:11,  3.30it/s]predicting train subjects:  36%|███▌      | 21/59 [00:05<00:11,  3.44it/s]predicting train subjects:  37%|███▋      | 22/59 [00:05<00:09,  3.73it/s]predicting train subjects:  39%|███▉      | 23/59 [00:05<00:10,  3.37it/s]predicting train subjects:  41%|████      | 24/59 [00:06<00:10,  3.40it/s]predicting train subjects:  42%|████▏     | 25/59 [00:06<00:08,  3.85it/s]predicting train subjects:  44%|████▍     | 26/59 [00:06<00:07,  4.21it/s]predicting train subjects:  46%|████▌     | 27/59 [00:06<00:07,  4.08it/s]predicting train subjects:  47%|████▋     | 28/59 [00:07<00:06,  4.43it/s]predicting train subjects:  49%|████▉     | 29/59 [00:07<00:07,  4.08it/s]predicting train subjects:  51%|█████     | 30/59 [00:07<00:06,  4.50it/s]predicting train subjects:  53%|█████▎    | 31/59 [00:07<00:06,  4.38it/s]predicting train subjects:  54%|█████▍    | 32/59 [00:08<00:06,  4.12it/s]predicting train subjects:  56%|█████▌    | 33/59 [00:08<00:05,  4.45it/s]predicting train subjects:  59%|█████▉    | 35/59 [00:08<00:04,  5.38it/s]predicting train subjects:  61%|██████    | 36/59 [00:08<00:05,  4.52it/s]predicting train subjects:  63%|██████▎   | 37/59 [00:08<00:04,  4.61it/s]predicting train subjects:  66%|██████▌   | 39/59 [00:09<00:03,  5.41it/s]predicting train subjects:  68%|██████▊   | 40/59 [00:09<00:04,  4.67it/s]predicting train subjects:  69%|██████▉   | 41/59 [00:09<00:04,  3.90it/s]predicting train subjects:  71%|███████   | 42/59 [00:10<00:04,  3.68it/s]predicting train subjects:  73%|███████▎  | 43/59 [00:10<00:04,  3.45it/s]predicting train subjects:  75%|███████▍  | 44/59 [00:10<00:03,  3.91it/s]predicting train subjects:  76%|███████▋  | 45/59 [00:10<00:03,  3.96it/s]predicting train subjects:  78%|███████▊  | 46/59 [00:11<00:03,  4.15it/s]predicting train subjects:  80%|███████▉  | 47/59 [00:11<00:03,  3.74it/s]predicting train subjects:  81%|████████▏ | 48/59 [00:11<00:02,  3.92it/s]predicting train subjects:  83%|████████▎ | 49/59 [00:11<00:02,  4.29it/s]predicting train subjects:  85%|████████▍ | 50/59 [00:11<00:02,  4.43it/s]predicting train subjects:  86%|████████▋ | 51/59 [00:12<00:01,  4.68it/s]predicting train subjects:  88%|████████▊ | 52/59 [00:12<00:01,  4.64it/s]predicting train subjects:  90%|████████▉ | 53/59 [00:12<00:01,  4.82it/s]predicting train subjects:  92%|█████████▏| 54/59 [00:12<00:00,  5.06it/s]predicting train subjects:  93%|█████████▎| 55/59 [00:13<00:00,  4.18it/s]predicting train subjects:  95%|█████████▍| 56/59 [00:13<00:00,  4.13it/s]predicting train subjects:  97%|█████████▋| 57/59 [00:13<00:00,  4.00it/s]predicting train subjects:  98%|█████████▊| 58/59 [00:13<00:00,  4.58it/s]predicting train subjects: 100%|██████████| 59/59 [00:13<00:00,  4.69it/s]predicting train subjects: 100%|██████████| 59/59 [00:13<00:00,  4.23it/s]
saving BB  test1-THALAMUS:   0%|          | 0/23 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  22%|██▏       | 5/23 [00:00<00:00, 48.02it/s]saving BB  test1-THALAMUS:  43%|████▎     | 10/23 [00:00<00:00, 44.41it/s]saving BB  test1-THALAMUS:  65%|██████▌   | 15/23 [00:00<00:00, 43.97it/s]saving BB  test1-THALAMUS:  87%|████████▋ | 20/23 [00:00<00:00, 43.31it/s]saving BB  test1-THALAMUS: 100%|██████████| 23/23 [00:00<00:00, 41.59it/s]
saving BB  train1-THALAMUS:   0%|          | 0/59 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   7%|▋         | 4/59 [00:00<00:01, 39.92it/s]saving BB  train1-THALAMUS:  15%|█▌        | 9/59 [00:00<00:01, 40.83it/s]saving BB  train1-THALAMUS:  22%|██▏       | 13/59 [00:00<00:01, 38.66it/s]saving BB  train1-THALAMUS:  32%|███▏      | 19/59 [00:00<00:00, 42.07it/s]saving BB  train1-THALAMUS:  41%|████      | 24/59 [00:00<00:00, 41.15it/s]saving BB  train1-THALAMUS:  49%|████▉     | 29/59 [00:00<00:00, 41.92it/s]saving BB  train1-THALAMUS:  58%|█████▊    | 34/59 [00:00<00:00, 43.42it/s]saving BB  train1-THALAMUS:  68%|██████▊   | 40/59 [00:00<00:00, 46.49it/s]saving BB  train1-THALAMUS:  78%|███████▊  | 46/59 [00:01<00:00, 47.50it/s]saving BB  train1-THALAMUS:  86%|████████▋ | 51/59 [00:01<00:00, 47.83it/s]saving BB  train1-THALAMUS:  95%|█████████▍| 56/59 [00:01<00:00, 48.35it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 45.63it/s]
Loading train:   0%|          | 0/59 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/59 [00:00<00:34,  1.67it/s]Loading train:   3%|▎         | 2/59 [00:01<00:31,  1.80it/s]Loading train:   5%|▌         | 3/59 [00:01<00:31,  1.78it/s]Loading train:   7%|▋         | 4/59 [00:02<00:29,  1.84it/s]Loading train:   8%|▊         | 5/59 [00:02<00:29,  1.82it/s]Loading train:  10%|█         | 6/59 [00:03<00:28,  1.86it/s]Loading train:  12%|█▏        | 7/59 [00:04<00:37,  1.40it/s]Loading train:  14%|█▎        | 8/59 [00:04<00:32,  1.59it/s]Loading train:  15%|█▌        | 9/59 [00:05<00:30,  1.62it/s]Loading train:  17%|█▋        | 10/59 [00:05<00:29,  1.66it/s]Loading train:  19%|█▊        | 11/59 [00:06<00:30,  1.59it/s]Loading train:  20%|██        | 12/59 [00:07<00:29,  1.60it/s]Loading train:  22%|██▏       | 13/59 [00:07<00:26,  1.72it/s]Loading train:  24%|██▎       | 14/59 [00:08<00:26,  1.72it/s]Loading train:  25%|██▌       | 15/59 [00:08<00:23,  1.84it/s]Loading train:  27%|██▋       | 16/59 [00:09<00:24,  1.77it/s]Loading train:  29%|██▉       | 17/59 [00:09<00:22,  1.85it/s]Loading train:  31%|███       | 18/59 [00:10<00:23,  1.77it/s]Loading train:  32%|███▏      | 19/59 [00:11<00:26,  1.50it/s]Loading train:  34%|███▍      | 20/59 [00:12<00:25,  1.51it/s]Loading train:  36%|███▌      | 21/59 [00:12<00:21,  1.79it/s]Loading train:  37%|███▋      | 22/59 [00:12<00:20,  1.82it/s]Loading train:  39%|███▉      | 23/59 [00:13<00:23,  1.55it/s]Loading train:  41%|████      | 24/59 [00:14<00:18,  1.86it/s]Loading train:  42%|████▏     | 25/59 [00:14<00:17,  1.90it/s]Loading train:  44%|████▍     | 26/59 [00:14<00:15,  2.15it/s]Loading train:  46%|████▌     | 27/59 [00:15<00:13,  2.39it/s]Loading train:  47%|████▋     | 28/59 [00:15<00:12,  2.55it/s]Loading train:  49%|████▉     | 29/59 [00:15<00:11,  2.69it/s]Loading train:  51%|█████     | 30/59 [00:16<00:09,  2.91it/s]Loading train:  53%|█████▎    | 31/59 [00:16<00:09,  3.01it/s]Loading train:  54%|█████▍    | 32/59 [00:16<00:08,  3.02it/s]Loading train:  56%|█████▌    | 33/59 [00:17<00:08,  3.06it/s]Loading train:  58%|█████▊    | 34/59 [00:17<00:07,  3.36it/s]Loading train:  59%|█████▉    | 35/59 [00:17<00:06,  3.68it/s]Loading train:  61%|██████    | 36/59 [00:17<00:06,  3.38it/s]Loading train:  63%|██████▎   | 37/59 [00:18<00:06,  3.30it/s]Loading train:  64%|██████▍   | 38/59 [00:18<00:05,  3.64it/s]Loading train:  66%|██████▌   | 39/59 [00:18<00:05,  3.88it/s]Loading train:  68%|██████▊   | 40/59 [00:18<00:05,  3.50it/s]Loading train:  69%|██████▉   | 41/59 [00:19<00:05,  3.21it/s]Loading train:  71%|███████   | 42/59 [00:19<00:05,  3.08it/s]Loading train:  73%|███████▎  | 43/59 [00:19<00:05,  3.04it/s]Loading train:  75%|███████▍  | 44/59 [00:20<00:04,  3.22it/s]Loading train:  76%|███████▋  | 45/59 [00:20<00:04,  3.49it/s]Loading train:  78%|███████▊  | 46/59 [00:20<00:03,  3.28it/s]Loading train:  80%|███████▉  | 47/59 [00:21<00:03,  3.40it/s]Loading train:  81%|████████▏ | 48/59 [00:21<00:03,  3.45it/s]Loading train:  83%|████████▎ | 49/59 [00:21<00:02,  3.53it/s]Loading train:  85%|████████▍ | 50/59 [00:21<00:02,  3.44it/s]Loading train:  86%|████████▋ | 51/59 [00:22<00:02,  3.52it/s]Loading train:  88%|████████▊ | 52/59 [00:22<00:02,  3.44it/s]Loading train:  90%|████████▉ | 53/59 [00:22<00:01,  3.37it/s]Loading train:  92%|█████████▏| 54/59 [00:23<00:01,  3.31it/s]Loading train:  93%|█████████▎| 55/59 [00:23<00:01,  3.41it/s]Loading train:  95%|█████████▍| 56/59 [00:23<00:00,  3.36it/s]Loading train:  97%|█████████▋| 57/59 [00:24<00:00,  3.47it/s]Loading train:  98%|█████████▊| 58/59 [00:24<00:00,  3.43it/s]Loading train: 100%|██████████| 59/59 [00:24<00:00,  3.59it/s]Loading train: 100%|██████████| 59/59 [00:24<00:00,  2.40it/s]
concatenating: train:   0%|          | 0/59 [00:00<?, ?it/s]concatenating: train:  10%|█         | 6/59 [00:00<00:01, 52.82it/s]concatenating: train:  20%|██        | 12/59 [00:00<00:00, 53.01it/s]concatenating: train:  31%|███       | 18/59 [00:00<00:00, 53.25it/s]concatenating: train:  41%|████      | 24/59 [00:00<00:00, 54.50it/s]concatenating: train:  51%|█████     | 30/59 [00:00<00:00, 54.27it/s]concatenating: train:  63%|██████▎   | 37/59 [00:00<00:00, 56.91it/s]concatenating: train:  73%|███████▎  | 43/59 [00:00<00:00, 55.85it/s]concatenating: train:  85%|████████▍ | 50/59 [00:00<00:00, 57.23it/s]concatenating: train:  95%|█████████▍| 56/59 [00:00<00:00, 57.46it/s]concatenating: train: 100%|██████████| 59/59 [00:01<00:00, 56.53it/s]
Loading test:   0%|          | 0/23 [00:00<?, ?it/s]Loading test:   4%|▍         | 1/23 [00:00<00:08,  2.61it/s]Loading test:   9%|▊         | 2/23 [00:00<00:07,  2.71it/s]Loading test:  13%|█▎        | 3/23 [00:01<00:07,  2.84it/s]Loading test:  17%|█▋        | 4/23 [00:01<00:06,  2.96it/s]Loading test:  22%|██▏       | 5/23 [00:01<00:05,  3.11it/s]Loading test:  26%|██▌       | 6/23 [00:01<00:05,  3.10it/s]Loading test:  30%|███       | 7/23 [00:02<00:05,  3.18it/s]Loading test:  35%|███▍      | 8/23 [00:02<00:04,  3.20it/s]Loading test:  39%|███▉      | 9/23 [00:02<00:04,  2.92it/s]Loading test:  43%|████▎     | 10/23 [00:03<00:04,  3.10it/s]Loading test:  48%|████▊     | 11/23 [00:04<00:05,  2.17it/s]Loading test:  52%|█████▏    | 12/23 [00:04<00:05,  1.85it/s]Loading test:  57%|█████▋    | 13/23 [00:05<00:04,  2.09it/s]Loading test:  61%|██████    | 14/23 [00:05<00:03,  2.40it/s]Loading test:  65%|██████▌   | 15/23 [00:05<00:03,  2.57it/s]Loading test:  70%|██████▉   | 16/23 [00:05<00:02,  2.83it/s]Loading test:  74%|███████▍  | 17/23 [00:06<00:02,  2.54it/s]Loading test:  78%|███████▊  | 18/23 [00:06<00:02,  2.32it/s]Loading test:  83%|████████▎ | 19/23 [00:07<00:01,  2.21it/s]Loading test:  87%|████████▋ | 20/23 [00:08<00:01,  1.53it/s]Loading test:  91%|█████████▏| 21/23 [00:09<00:01,  1.54it/s]Loading test:  96%|█████████▌| 22/23 [00:09<00:00,  1.66it/s]Loading test: 100%|██████████| 23/23 [00:10<00:00,  1.34it/s]Loading test: 100%|██████████| 23/23 [00:10<00:00,  2.13it/s]
concatenating: validation:   0%|          | 0/23 [00:00<?, ?it/s]concatenating: validation:  26%|██▌       | 6/23 [00:00<00:00, 56.43it/s]concatenating: validation:  52%|█████▏    | 12/23 [00:00<00:00, 56.57it/s]concatenating: validation:  78%|███████▊  | 18/23 [00:00<00:00, 57.54it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 59.79it/s]
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 112, 116, 30) 16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 112, 116, 30) 120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 112, 116, 30) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 112, 116, 30) 8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 112, 116, 30) 120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 112, 116, 30) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 112, 116, 90) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 112, 116, 90) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 112, 116, 2)  182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 499,142
Non-trainable params: 1,200
__________________________________________________________________________________________________

 MultiClass_24567891011121314 SD: [1] GPU: 3 

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 52, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 26, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 26, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 26, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 26, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 26, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 26, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 26, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 26, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 13, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 13, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 13, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 13, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 13, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 13, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 13, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 13, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 13, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 26, 150)  0           conv2d_transpose_1[0][0]         2020-09-30 21:47:09.220615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-09-30 21:47:09.220784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 21:47:09.220809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-09-30 21:47:09.220829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-09-30 21:47:09.221004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

predicting test subjects:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects:   4%|▍         | 1/23 [00:00<00:21,  1.01it/s]predicting test subjects:   9%|▊         | 2/23 [00:01<00:17,  1.21it/s]predicting test subjects:  13%|█▎        | 3/23 [00:01<00:14,  1.37it/s]predicting test subjects:  17%|█▋        | 4/23 [00:02<00:13,  1.46it/s]predicting test subjects:  22%|██▏       | 5/23 [00:05<00:25,  1.44s/it]predicting test subjects:  26%|██▌       | 6/23 [00:06<00:19,  1.17s/it]predicting test subjects:  30%|███       | 7/23 [00:06<00:15,  1.06it/s]predicting test subjects:  35%|███▍      | 8/23 [00:07<00:11,  1.25it/s]predicting test subjects:  39%|███▉      | 9/23 [00:07<00:10,  1.30it/s]predicting test subjects:  43%|████▎     | 10/23 [00:08<00:09,  1.36it/s]predicting test subjects:  48%|████▊     | 11/23 [00:09<00:08,  1.45it/s]predicting test subjects:  52%|█████▏    | 12/23 [00:09<00:07,  1.53it/s]predicting test subjects:  57%|█████▋    | 13/23 [00:10<00:06,  1.66it/s]predicting test subjects:  61%|██████    | 14/23 [00:10<00:05,  1.79it/s]predicting test subjects:  65%|██████▌   | 15/23 [00:11<00:04,  1.77it/s]predicting test subjects:  70%|██████▉   | 16/23 [00:11<00:03,  1.79it/s]predicting test subjects:  74%|███████▍  | 17/23 [00:12<00:03,  1.80it/s]predicting test subjects:  78%|███████▊  | 18/23 [00:12<00:02,  1.77it/s]predicting test subjects:  83%|████████▎ | 19/23 [00:13<00:02,  1.78it/s]predicting test subjects:  87%|████████▋ | 20/23 [00:14<00:01,  1.55it/s]predicting test subjects:  91%|█████████▏| 21/23 [00:14<00:01,  1.53it/s]predicting test subjects:  96%|█████████▌| 22/23 [00:15<00:00,  1.66it/s]predicting test subjects: 100%|██████████| 23/23 [00:16<00:00,  1.61it/s]predicting test subjects: 100%|██████████| 23/23 [00:16<00:00,  1.43it/s]
Loading train:   0%|          | 0/59 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/59 [00:00<00:06,  8.34it/s]Loading train:   3%|▎         | 2/59 [00:00<00:06,  8.41it/s]Loading train:   5%|▌         | 3/59 [00:00<00:06,  8.08it/s]Loading train:   7%|▋         | 4/59 [00:00<00:07,  7.72it/s]Loading train:   8%|▊         | 5/59 [00:00<00:06,  7.84it/s]Loading train:  10%|█         | 6/59 [00:00<00:06,  7.68it/s]Loading train:  12%|█▏        | 7/59 [00:00<00:06,  7.47it/s]Loading train:  14%|█▎        | 8/59 [00:01<00:06,  7.81it/s]Loading train:  15%|█▌        | 9/59 [00:01<00:06,  7.35it/s]Loading train:  17%|█▋        | 10/59 [00:01<00:06,  7.10it/s]Loading train:  19%|█▊        | 11/59 [00:01<00:07,  6.80it/s]Loading train:  20%|██        | 12/59 [00:01<00:06,  7.05it/s]Loading train:  22%|██▏       | 13/59 [00:01<00:06,  7.44it/s]Loading train:  24%|██▎       | 14/59 [00:01<00:06,  7.47it/s]Loading train:  25%|██▌       | 15/59 [00:02<00:05,  7.49it/s]Loading train:  27%|██▋       | 16/59 [00:02<00:05,  7.41it/s]Loading train:  29%|██▉       | 17/59 [00:02<00:05,  7.27it/s]Loading train:  31%|███       | 18/59 [00:02<00:05,  7.06it/s]Loading train:  32%|███▏      | 19/59 [00:02<00:05,  6.78it/s]Loading train:  34%|███▍      | 20/59 [00:02<00:05,  6.55it/s]Loading train:  36%|███▌      | 21/59 [00:02<00:05,  7.21it/s]Loading train:  37%|███▋      | 22/59 [00:03<00:05,  7.19it/s]Loading train:  39%|███▉      | 23/59 [00:03<00:05,  6.09it/s]Loading train:  41%|████      | 24/59 [00:03<00:05,  6.82it/s]Loading train:  42%|████▏     | 25/59 [00:03<00:04,  7.41it/s]Loading train:  44%|████▍     | 26/59 [00:03<00:04,  7.89it/s]Loading train:  46%|████▌     | 27/59 [00:03<00:03,  8.35it/s]Loading train:  47%|████▋     | 28/59 [00:03<00:03,  8.72it/s]Loading train:  49%|████▉     | 29/59 [00:03<00:03,  8.86it/s]Loading train:  53%|█████▎    | 31/59 [00:04<00:03,  9.23it/s]Loading train:  56%|█████▌    | 33/59 [00:04<00:02,  9.49it/s]Loading train:  59%|█████▉    | 35/59 [00:04<00:02, 10.07it/s]Loading train:  63%|██████▎   | 37/59 [00:04<00:02, 10.12it/s]Loading train:  66%|██████▌   | 39/59 [00:04<00:01, 10.62it/s]Loading train:  69%|██████▉   | 41/59 [00:05<00:01,  9.87it/s]Loading train:  73%|███████▎  | 43/59 [00:05<00:01,  9.82it/s]Loading train:  75%|███████▍  | 44/59 [00:05<00:01,  9.80it/s]Loading train:  78%|███████▊  | 46/59 [00:05<00:01,  9.68it/s]Loading train:  80%|███████▉  | 47/59 [00:05<00:01,  9.45it/s]Loading train:  81%|████████▏ | 48/59 [00:05<00:01,  9.20it/s]Loading train:  83%|████████▎ | 49/59 [00:05<00:01,  9.11it/s]Loading train:  85%|████████▍ | 50/59 [00:06<00:00,  9.16it/s]Loading train:  88%|████████▊ | 52/59 [00:06<00:00,  9.73it/s]Loading train:  90%|████████▉ | 53/59 [00:06<00:00,  9.73it/s]Loading train:  92%|█████████▏| 54/59 [00:06<00:00,  9.19it/s]Loading train:  93%|█████████▎| 55/59 [00:06<00:00,  9.38it/s]Loading train:  95%|█████████▍| 56/59 [00:06<00:00,  9.30it/s]Loading train:  97%|█████████▋| 57/59 [00:06<00:00,  8.92it/s]Loading train:  98%|█████████▊| 58/59 [00:06<00:00,  8.90it/s]Loading train: 100%|██████████| 59/59 [00:06<00:00,  9.11it/s]Loading train: 100%|██████████| 59/59 [00:06<00:00,  8.48it/s]
concatenating: train:   0%|          | 0/59 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 4/59 [00:00<00:01, 34.45it/s]concatenating: train:  12%|█▏        | 7/59 [00:00<00:01, 32.82it/s]concatenating: train:  19%|█▊        | 11/59 [00:00<00:01, 31.78it/s]concatenating: train:  25%|██▌       | 15/59 [00:00<00:01, 31.87it/s]concatenating: train:  31%|███       | 18/59 [00:00<00:01, 30.36it/s]concatenating: train:  36%|███▌      | 21/59 [00:00<00:01, 29.34it/s]concatenating: train:  41%|████      | 24/59 [00:00<00:01, 27.98it/s]concatenating: train:  47%|████▋     | 28/59 [00:00<00:01, 29.26it/s]concatenating: train:  54%|█████▍    | 32/59 [00:01<00:00, 30.61it/s]concatenating: train:  61%|██████    | 36/59 [00:01<00:00, 31.86it/s]concatenating: train:  68%|██████▊   | 40/59 [00:01<00:00, 32.17it/s]concatenating: train:  75%|███████▍  | 44/59 [00:01<00:00, 32.43it/s]concatenating: train:  81%|████████▏ | 48/59 [00:01<00:00, 32.05it/s]concatenating: train:  88%|████████▊ | 52/59 [00:01<00:00, 31.69it/s]concatenating: train:  95%|█████████▍| 56/59 [00:01<00:00, 32.09it/s]concatenating: train: 100%|██████████| 59/59 [00:01<00:00, 31.18it/s]
Loading test:   0%|          | 0/23 [00:00<?, ?it/s]Loading test:   4%|▍         | 1/23 [00:00<00:02,  8.12it/s]Loading test:   9%|▊         | 2/23 [00:00<00:02,  8.50it/s]Loading test:  13%|█▎        | 3/23 [00:00<00:02,  8.80it/s]Loading test:  17%|█▋        | 4/23 [00:00<00:02,  8.81it/s]Loading test:  22%|██▏       | 5/23 [00:00<00:01,  9.09it/s]Loading test:  26%|██▌       | 6/23 [00:00<00:01,  8.75it/s]Loading test:  30%|███       | 7/23 [00:00<00:01,  9.04it/s]Loading test:  35%|███▍      | 8/23 [00:00<00:01,  8.99it/s]Loading test:  39%|███▉      | 9/23 [00:01<00:01,  7.59it/s]Loading test:  43%|████▎     | 10/23 [00:01<00:01,  7.26it/s]Loading test:  48%|████▊     | 11/23 [00:01<00:01,  7.52it/s]Loading test:  57%|█████▋    | 13/23 [00:01<00:01,  8.47it/s]Loading test:  65%|██████▌   | 15/23 [00:01<00:00,  8.66it/s]Loading test:  70%|██████▉   | 16/23 [00:01<00:00,  8.45it/s]Loading test:  74%|███████▍  | 17/23 [00:01<00:00,  8.06it/s]Loading test:  78%|███████▊  | 18/23 [00:02<00:00,  7.85it/s]Loading test:  83%|████████▎ | 19/23 [00:02<00:00,  7.59it/s]Loading test:  87%|████████▋ | 20/23 [00:02<00:00,  6.47it/s]Loading test:  91%|█████████▏| 21/23 [00:02<00:00,  6.39it/s]Loading test:  96%|█████████▌| 22/23 [00:02<00:00,  6.92it/s]Loading test: 100%|██████████| 23/23 [00:02<00:00,  6.93it/s]Loading test: 100%|██████████| 23/23 [00:02<00:00,  7.95it/s]
concatenating: validation:   0%|          | 0/23 [00:00<?, ?it/s]concatenating: validation:  17%|█▋        | 4/23 [00:00<00:00, 32.22it/s]concatenating: validation:  35%|███▍      | 8/23 [00:00<00:00, 33.09it/s]concatenating: validation:  48%|████▊     | 11/23 [00:00<00:00, 30.45it/s]concatenating: validation:  65%|██████▌   | 15/23 [00:00<00:00, 30.52it/s]concatenating: validation:  83%|████████▎ | 19/23 [00:00<00:00, 30.91it/s]concatenating: validation:  96%|█████████▌| 22/23 [00:00<00:00, 28.95it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 30.02it/s]
Loading trainS:   0%|          | 0/59 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/59 [00:00<00:07,  7.34it/s]Loading trainS:   3%|▎         | 2/59 [00:00<00:07,  7.69it/s]Loading trainS:   5%|▌         | 3/59 [00:00<00:07,  7.44it/s]Loading trainS:   7%|▋         | 4/59 [00:00<00:07,  7.44it/s]Loading trainS:   8%|▊         | 5/59 [00:00<00:07,  7.53it/s]Loading trainS:  10%|█         | 6/59 [00:00<00:07,  7.53it/s]Loading trainS:  12%|█▏        | 7/59 [00:00<00:07,  6.95it/s]Loading trainS:  14%|█▎        | 8/59 [00:01<00:06,  7.37it/s]Loading trainS:  15%|█▌        | 9/59 [00:01<00:07,  7.04it/s]Loading trainS:  17%|█▋        | 10/59 [00:01<00:07,  6.86it/s]Loading trainS:  19%|█▊        | 11/59 [00:01<00:07,  6.60it/s]Loading trainS:  20%|██        | 12/59 [00:01<00:06,  7.04it/s]Loading trainS:  22%|██▏       | 13/59 [00:01<00:06,  7.21it/s]Loading trainS:  24%|██▎       | 14/59 [00:01<00:06,  7.19it/s]Loading trainS:  25%|██▌       | 15/59 [00:02<00:06,  7.31it/s]Loading trainS:  27%|██▋       | 16/59 [00:02<00:06,  7.07it/s]Loading trainS:  29%|██▉       | 17/59 [00:02<00:05,  7.32it/s]Loading trainS:  31%|███       | 18/59 [00:02<00:06,  6.77it/s]Loading trainS:  32%|███▏      | 19/59 [00:02<00:06,  6.29it/s]Loading trainS:  34%|███▍      | 20/59 [00:02<00:06,  6.17it/s]Loading trainS:  36%|███▌      | 21/59 [00:03<00:05,  6.68it/s]Loading trainS:  37%|███▋      | 22/59 [00:03<00:05,  6.66it/s]Loading trainS:  39%|███▉      | 23/59 [00:03<00:06,  5.71it/s]Loading trainS:  41%|████      | 24/59 [00:03<00:05,  6.38it/s]Loading trainS:  42%|████▏     | 25/59 [00:03<00:04,  6.86it/s]Loading trainS:  44%|████▍     | 26/59 [00:03<00:04,  7.23it/s]Loading trainS:  46%|████▌     | 27/59 [00:03<00:04,  7.73it/s]Loading trainS:  47%|████▋     | 28/59 [00:03<00:03,  8.01it/s]Loading trainS:  49%|████▉     | 29/59 [00:04<00:03,  8.13it/s]Loading trainS:  51%|█████     | 30/59 [00:04<00:03,  8.50it/s]Loading trainS:  53%|█████▎    | 31/59 [00:04<00:03,  8.26it/s]Loading trainS:  56%|█████▌    | 33/59 [00:04<00:02,  9.29it/s]Loading trainS:  58%|█████▊    | 34/59 [00:04<00:02,  9.26it/s]Loading trainS:  59%|█████▉    | 35/59 [00:04<00:02,  9.36it/s]Loading trainS:  61%|██████    | 36/59 [00:04<00:02,  9.44it/s]Loading trainS:  63%|██████▎   | 37/59 [00:04<00:02,  9.20it/s]Loading trainS:  64%|██████▍   | 38/59 [00:05<00:02,  9.13it/s]Loading trainS:  68%|██████▊   | 40/59 [00:05<00:02,  9.16it/s]Loading trainS:  69%|██████▉   | 41/59 [00:05<00:02,  8.40it/s]Loading trainS:  71%|███████   | 42/59 [00:05<00:02,  8.45it/s]Loading trainS:  73%|███████▎  | 43/59 [00:05<00:01,  8.75it/s]Loading trainS:  75%|███████▍  | 44/59 [00:05<00:01,  8.85it/s]Loading trainS:  76%|███████▋  | 45/59 [00:05<00:01,  8.83it/s]Loading trainS:  78%|███████▊  | 46/59 [00:05<00:01,  8.07it/s]Loading trainS:  80%|███████▉  | 47/59 [00:06<00:01,  7.48it/s]Loading trainS:  83%|████████▎ | 49/59 [00:06<00:01,  8.44it/s]Loading trainS:  86%|████████▋ | 51/59 [00:06<00:00,  9.12it/s]Loading trainS:  88%|████████▊ | 52/59 [00:06<00:00,  9.16it/s]Loading trainS:  92%|█████████▏| 54/59 [00:06<00:00,  9.36it/s]Loading trainS:  93%|█████████▎| 55/59 [00:06<00:00,  9.47it/s]Loading trainS:  95%|█████████▍| 56/59 [00:06<00:00,  9.42it/s]Loading trainS:  97%|█████████▋| 57/59 [00:07<00:00,  9.51it/s]Loading trainS:  98%|█████████▊| 58/59 [00:07<00:00,  9.23it/s]Loading trainS: 100%|██████████| 59/59 [00:07<00:00,  9.37it/s]Loading trainS: 100%|██████████| 59/59 [00:07<00:00,  8.07it/s]
Loading testS:   0%|          | 0/23 [00:00<?, ?it/s]Loading testS:   4%|▍         | 1/23 [00:00<00:03,  7.02it/s]Loading testS:  13%|█▎        | 3/23 [00:00<00:02,  7.98it/s]Loading testS:  17%|█▋        | 4/23 [00:00<00:02,  8.39it/s]Loading testS:  22%|██▏       | 5/23 [00:00<00:02,  8.63it/s]Loading testS:  26%|██▌       | 6/23 [00:00<00:01,  8.52it/s]Loading testS:  35%|███▍      | 8/23 [00:00<00:01,  8.99it/s]Loading testS:  39%|███▉      | 9/23 [00:01<00:01,  7.95it/s]Loading testS:  43%|████▎     | 10/23 [00:01<00:01,  7.75it/s]Loading testS:  48%|████▊     | 11/23 [00:01<00:01,  7.82it/s]Loading testS:  52%|█████▏    | 12/23 [00:01<00:01,  7.67it/s]Loading testS:  57%|█████▋    | 13/23 [00:01<00:01,  7.64it/s]Loading testS:  61%|██████    | 14/23 [00:01<00:01,  7.82it/s]Loading testS:  65%|██████▌   | 15/23 [00:01<00:01,  7.70it/s]Loading testS:  70%|██████▉   | 16/23 [00:01<00:00,  7.77it/s]Loading testS:  74%|███████▍  | 17/23 [00:02<00:00,  7.45it/s]Loading testS:  78%|███████▊  | 18/23 [00:02<00:00,  7.33it/s]Loading testS:  83%|████████▎ | 19/23 [00:02<00:00,  7.16it/s]Loading testS:  87%|████████▋ | 20/23 [00:02<00:00,  5.79it/s]Loading testS:  91%|█████████▏| 21/23 [00:02<00:00,  5.85it/s]Loading testS:  96%|█████████▌| 22/23 [00:02<00:00,  6.35it/s]Loading testS: 100%|██████████| 23/23 [00:03<00:00,  6.49it/s]Loading testS: 100%|██████████| 23/23 [00:03<00:00,  7.57it/s]
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 26, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 26, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 26, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 26, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 26, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 26, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 26, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 26, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 30)   25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 52, 60)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 52, 30)   16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 52, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 52, 30)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 52, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 52, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 52, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 52, 90)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 52, 90)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 52, 13)   1183        dropout_5[0][0]                  
==================================================================================================
Total params: 501,343
Trainable params: 500,143
Non-trainable params: 1,200
__________________________________________________________________________________________________

 1-THALAMUS SD: [2] GPU: 3 

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 168, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 168, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 168, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 168, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 168, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 168, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 168, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 84, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 84, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 84, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 84, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 84, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 84, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 84, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 84, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 84, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 42, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________2020-09-30 21:47:53.098237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-09-30 21:47:53.098440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 21:47:53.098473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-09-30 21:47:53.098493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-09-30 21:47:53.098702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

predicting test subjects:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects:   4%|▍         | 1/23 [00:00<00:13,  1.67it/s]predicting test subjects:   9%|▊         | 2/23 [00:00<00:09,  2.16it/s]predicting test subjects:  13%|█▎        | 3/23 [00:00<00:07,  2.52it/s]predicting test subjects:  17%|█▋        | 4/23 [00:01<00:06,  3.09it/s]predicting test subjects:  22%|██▏       | 5/23 [00:01<00:05,  3.40it/s]predicting test subjects:  26%|██▌       | 6/23 [00:01<00:04,  3.53it/s]predicting test subjects:  30%|███       | 7/23 [00:01<00:04,  3.82it/s]predicting test subjects:  35%|███▍      | 8/23 [00:01<00:03,  4.35it/s]predicting test subjects:  39%|███▉      | 9/23 [00:02<00:03,  4.01it/s]predicting test subjects:  43%|████▎     | 10/23 [00:02<00:03,  4.09it/s]predicting test subjects:  48%|████▊     | 11/23 [00:02<00:02,  4.23it/s]predicting test subjects:  52%|█████▏    | 12/23 [00:02<00:02,  4.36it/s]predicting test subjects:  57%|█████▋    | 13/23 [00:03<00:02,  4.60it/s]predicting test subjects:  61%|██████    | 14/23 [00:03<00:01,  4.94it/s]predicting test subjects:  65%|██████▌   | 15/23 [00:03<00:01,  5.08it/s]predicting test subjects:  70%|██████▉   | 16/23 [00:03<00:01,  4.62it/s]predicting test subjects:  74%|███████▍  | 17/23 [00:03<00:01,  5.05it/s]predicting test subjects:  78%|███████▊  | 18/23 [00:04<00:01,  4.63it/s]predicting test subjects:  83%|████████▎ | 19/23 [00:04<00:00,  4.60it/s]predicting test subjects:  87%|████████▋ | 20/23 [00:04<00:00,  3.97it/s]predicting test subjects:  91%|█████████▏| 21/23 [00:04<00:00,  4.06it/s]predicting test subjects:  96%|█████████▌| 22/23 [00:05<00:00,  4.39it/s]predicting test subjects: 100%|██████████| 23/23 [00:05<00:00,  4.41it/s]predicting test subjects: 100%|██████████| 23/23 [00:05<00:00,  4.29it/s]
predicting train subjects:   0%|          | 0/59 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/59 [00:00<00:08,  6.58it/s]predicting train subjects:   3%|▎         | 2/59 [00:00<00:08,  6.55it/s]predicting train subjects:   5%|▌         | 3/59 [00:00<00:08,  6.29it/s]predicting train subjects:   7%|▋         | 4/59 [00:00<00:09,  5.86it/s]predicting train subjects:   8%|▊         | 5/59 [00:00<00:09,  5.61it/s]predicting train subjects:  10%|█         | 6/59 [00:01<00:09,  5.59it/s]predicting train subjects:  12%|█▏        | 7/59 [00:01<00:09,  5.70it/s]predicting train subjects:  14%|█▎        | 8/59 [00:01<00:08,  6.25it/s]predicting train subjects:  15%|█▌        | 9/59 [00:01<00:08,  5.69it/s]predicting train subjects:  17%|█▋        | 10/59 [00:01<00:09,  5.27it/s]predicting train subjects:  19%|█▊        | 11/59 [00:02<00:09,  5.01it/s]predicting train subjects:  20%|██        | 12/59 [00:02<00:08,  5.24it/s]predicting train subjects:  22%|██▏       | 13/59 [00:02<00:08,  5.69it/s]predicting train subjects:  24%|██▎       | 14/59 [00:02<00:08,  5.61it/s]predicting train subjects:  25%|██▌       | 15/59 [00:02<00:08,  5.11it/s]predicting train subjects:  27%|██▋       | 16/59 [00:02<00:08,  4.92it/s]predicting train subjects:  29%|██▉       | 17/59 [00:03<00:09,  4.37it/s]predicting train subjects:  31%|███       | 18/59 [00:03<00:09,  4.43it/s]predicting train subjects:  32%|███▏      | 19/59 [00:03<00:08,  4.48it/s]predicting train subjects:  34%|███▍      | 20/59 [00:03<00:08,  4.85it/s]predicting train subjects:  36%|███▌      | 21/59 [00:04<00:08,  4.74it/s]predicting train subjects:  37%|███▋      | 22/59 [00:04<00:07,  4.90it/s]predicting train subjects:  39%|███▉      | 23/59 [00:04<00:09,  3.79it/s]predicting train subjects:  41%|████      | 24/59 [00:04<00:08,  3.89it/s]predicting train subjects:  42%|████▏     | 25/59 [00:05<00:07,  4.36it/s]predicting train subjects:  44%|████▍     | 26/59 [00:05<00:07,  4.23it/s]predicting train subjects:  46%|████▌     | 27/59 [00:05<00:07,  4.08it/s]predicting train subjects:  47%|████▋     | 28/59 [00:05<00:06,  4.55it/s]predicting train subjects:  49%|████▉     | 29/59 [00:06<00:06,  4.32it/s]predicting train subjects:  51%|█████     | 30/59 [00:06<00:06,  4.81it/s]predicting train subjects:  53%|█████▎    | 31/59 [00:06<00:05,  4.67it/s]predicting train subjects:  54%|█████▍    | 32/59 [00:06<00:05,  5.12it/s]predicting train subjects:  56%|█████▌    | 33/59 [00:06<00:04,  5.53it/s]predicting train subjects:  58%|█████▊    | 34/59 [00:06<00:04,  6.19it/s]predicting train subjects:  59%|█████▉    | 35/59 [00:06<00:04,  5.98it/s]predicting train subjects:  61%|██████    | 36/59 [00:07<00:03,  6.10it/s]predicting train subjects:  63%|██████▎   | 37/59 [00:07<00:03,  6.11it/s]predicting train subjects:  64%|██████▍   | 38/59 [00:07<00:03,  6.68it/s]predicting train subjects:  66%|██████▌   | 39/59 [00:07<00:03,  6.53it/s]predicting train subjects:  68%|██████▊   | 40/59 [00:07<00:03,  5.03it/s]predicting train subjects:  69%|██████▉   | 41/59 [00:08<00:04,  4.39it/s]predicting train subjects:  71%|███████   | 42/59 [00:08<00:04,  4.11it/s]predicting train subjects:  73%|███████▎  | 43/59 [00:08<00:03,  4.07it/s]predicting train subjects:  75%|███████▍  | 44/59 [00:08<00:03,  4.55it/s]predicting train subjects:  76%|███████▋  | 45/59 [00:09<00:02,  4.86it/s]predicting train subjects:  78%|███████▊  | 46/59 [00:09<00:02,  4.97it/s]predicting train subjects:  80%|███████▉  | 47/59 [00:09<00:02,  5.09it/s]predicting train subjects:  81%|████████▏ | 48/59 [00:09<00:02,  5.26it/s]predicting train subjects:  83%|████████▎ | 49/59 [00:09<00:01,  5.12it/s]predicting train subjects:  85%|████████▍ | 50/59 [00:09<00:01,  5.25it/s]predicting train subjects:  86%|████████▋ | 51/59 [00:10<00:01,  5.44it/s]predicting train subjects:  88%|████████▊ | 52/59 [00:10<00:01,  4.98it/s]predicting train subjects:  90%|████████▉ | 53/59 [00:10<00:01,  5.30it/s]predicting train subjects:  92%|█████████▏| 54/59 [00:10<00:00,  5.29it/s]predicting train subjects:  93%|█████████▎| 55/59 [00:11<00:00,  4.71it/s]predicting train subjects:  95%|█████████▍| 56/59 [00:11<00:00,  5.07it/s]predicting train subjects:  97%|█████████▋| 57/59 [00:11<00:00,  5.09it/s]predicting train subjects:  98%|█████████▊| 58/59 [00:11<00:00,  5.53it/s]predicting train subjects: 100%|██████████| 59/59 [00:11<00:00,  5.63it/s]predicting train subjects: 100%|██████████| 59/59 [00:11<00:00,  5.05it/s]
predicting test subjects sagittal:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects sagittal:   4%|▍         | 1/23 [00:00<00:04,  5.14it/s]predicting test subjects sagittal:   9%|▊         | 2/23 [00:00<00:03,  5.61it/s]predicting test subjects sagittal:  13%|█▎        | 3/23 [00:00<00:04,  4.74it/s]predicting test subjects sagittal:  17%|█▋        | 4/23 [00:00<00:03,  5.06it/s]predicting test subjects sagittal:  22%|██▏       | 5/23 [00:01<00:03,  4.73it/s]predicting test subjects sagittal:  26%|██▌       | 6/23 [00:01<00:03,  4.36it/s]predicting test subjects sagittal:  30%|███       | 7/23 [00:01<00:03,  4.48it/s]predicting test subjects sagittal:  35%|███▍      | 8/23 [00:01<00:03,  4.92it/s]predicting test subjects sagittal:  39%|███▉      | 9/23 [00:01<00:03,  4.55it/s]predicting test subjects sagittal:  43%|████▎     | 10/23 [00:02<00:02,  4.49it/s]predicting test subjects sagittal:  48%|████▊     | 11/23 [00:02<00:02,  4.21it/s]predicting test subjects sagittal:  52%|█████▏    | 12/23 [00:02<00:02,  4.34it/s]predicting test subjects sagittal:  57%|█████▋    | 13/23 [00:02<00:02,  4.72it/s]predicting test subjects sagittal:  61%|██████    | 14/23 [00:02<00:01,  4.94it/s]predicting test subjects sagittal:  65%|██████▌   | 15/23 [00:03<00:01,  5.06it/s]predicting test subjects sagittal:  70%|██████▉   | 16/23 [00:03<00:01,  4.50it/s]predicting test subjects sagittal:  74%|███████▍  | 17/23 [00:03<00:01,  4.81it/s]predicting test subjects sagittal:  78%|███████▊  | 18/23 [00:03<00:01,  4.21it/s]predicting test subjects sagittal:  83%|████████▎ | 19/23 [00:04<00:00,  4.37it/s]predicting test subjects sagittal:  87%|████████▋ | 20/23 [00:04<00:00,  3.70it/s]predicting test subjects sagittal:  91%|█████████▏| 21/23 [00:04<00:00,  3.60it/s]predicting test subjects sagittal:  96%|█████████▌| 22/23 [00:04<00:00,  4.04it/s]predicting test subjects sagittal: 100%|██████████| 23/23 [00:05<00:00,  4.09it/s]predicting test subjects sagittal: 100%|██████████| 23/23 [00:05<00:00,  4.40it/s]
predicting train subjects sagittal:   0%|          | 0/59 [00:00<?, ?it/s]predicting train subjects sagittal:   2%|▏         | 1/59 [00:00<00:11,  5.05it/s]predicting train subjects sagittal:   3%|▎         | 2/59 [00:00<00:10,  5.47it/s]predicting train subjects sagittal:   5%|▌         | 3/59 [00:00<00:09,  5.80it/s]predicting train subjects sagittal:   7%|▋         | 4/59 [00:00<00:10,  5.39it/s]predicting train subjects sagittal:   8%|▊         | 5/59 [00:00<00:09,  5.47it/s]predicting train subjects sagittal:  10%|█         | 6/59 [00:01<00:10,  5.20it/s]predicting train subjects sagittal:  12%|█▏        | 7/59 [00:01<00:11,  4.70it/s]predicting train subjects sagittal:  14%|█▎        | 8/59 [00:01<00:09,  5.20it/s]predicting train subjects sagittal:  15%|█▌        | 9/59 [00:01<00:10,  4.92it/s]predicting train subjects sagittal:  17%|█▋        | 10/59 [00:01<00:09,  5.02it/s]predicting train subjects sagittal:  19%|█▊        | 11/59 [00:02<00:10,  4.70it/s]predicting train subjects sagittal:  20%|██        | 12/59 [00:02<00:08,  5.24it/s]predicting train subjects sagittal:  22%|██▏       | 13/59 [00:02<00:08,  5.59it/s]predicting train subjects sagittal:  24%|██▎       | 14/59 [00:02<00:08,  5.60it/s]predicting train subjects sagittal:  25%|██▌       | 15/59 [00:02<00:08,  4.89it/s]predicting train subjects sagittal:  27%|██▋       | 16/59 [00:03<00:08,  4.92it/s]predicting train subjects sagittal:  29%|██▉       | 17/59 [00:03<00:09,  4.23it/s]predicting train subjects sagittal:  31%|███       | 18/59 [00:03<00:09,  4.27it/s]predicting train subjects sagittal:  32%|███▏      | 19/59 [00:03<00:09,  4.27it/s]predicting train subjects sagittal:  34%|███▍      | 20/59 [00:04<00:08,  4.34it/s]predicting train subjects sagittal:  36%|███▌      | 21/59 [00:04<00:08,  4.26it/s]predicting train subjects sagittal:  37%|███▋      | 22/59 [00:04<00:08,  4.47it/s]predicting train subjects sagittal:  39%|███▉      | 23/59 [00:04<00:10,  3.52it/s]predicting train subjects sagittal:  41%|████      | 24/59 [00:05<00:09,  3.72it/s]predicting train subjects sagittal:  42%|████▏     | 25/59 [00:05<00:07,  4.34it/s]predicting train subjects sagittal:  44%|████▍     | 26/59 [00:05<00:07,  4.21it/s]predicting train subjects sagittal:  46%|████▌     | 27/59 [00:05<00:07,  4.19it/s]predicting train subjects sagittal:  47%|████▋     | 28/59 [00:05<00:06,  4.69it/s]predicting train subjects sagittal:  49%|████▉     | 29/59 [00:06<00:06,  4.37it/s]predicting train subjects sagittal:  51%|█████     | 30/59 [00:06<00:05,  4.93it/s]predicting train subjects sagittal:  53%|█████▎    | 31/59 [00:06<00:06,  4.59it/s]predicting train subjects sagittal:  54%|█████▍    | 32/59 [00:06<00:05,  5.07it/s]predicting train subjects sagittal:  56%|█████▌    | 33/59 [00:06<00:04,  5.39it/s]predicting train subjects sagittal:  58%|█████▊    | 34/59 [00:07<00:04,  6.19it/s]predicting train subjects sagittal:  59%|█████▉    | 35/59 [00:07<00:03,  6.01it/s]predicting train subjects sagittal:  61%|██████    | 36/59 [00:07<00:03,  6.30it/s]predicting train subjects sagittal:  63%|██████▎   | 37/59 [00:07<00:03,  6.34it/s]predicting train subjects sagittal:  64%|██████▍   | 38/59 [00:07<00:03,  6.85it/s]predicting train subjects sagittal:  66%|██████▌   | 39/59 [00:07<00:03,  6.54it/s]predicting train subjects sagittal:  68%|██████▊   | 40/59 [00:08<00:03,  5.36it/s]predicting train subjects sagittal:  69%|██████▉   | 41/59 [00:08<00:03,  4.63it/s]predicting train subjects sagittal:  71%|███████   | 42/59 [00:08<00:04,  4.08it/s]predicting train subjects sagittal:  73%|███████▎  | 43/59 [00:08<00:03,  4.04it/s]predicting train subjects sagittal:  75%|███████▍  | 44/59 [00:09<00:03,  4.51it/s]predicting train subjects sagittal:  76%|███████▋  | 45/59 [00:09<00:02,  4.94it/s]predicting train subjects sagittal:  78%|███████▊  | 46/59 [00:09<00:02,  5.11it/s]predicting train subjects sagittal:  80%|███████▉  | 47/59 [00:09<00:02,  5.39it/s]predicting train subjects sagittal:  81%|████████▏ | 48/59 [00:09<00:02,  5.42it/s]predicting train subjects sagittal:  83%|████████▎ | 49/59 [00:09<00:01,  5.49it/s]predicting train subjects sagittal:  85%|████████▍ | 50/59 [00:10<00:01,  5.69it/s]predicting train subjects sagittal:  86%|████████▋ | 51/59 [00:10<00:01,  6.08it/s]predicting train subjects sagittal:  88%|████████▊ | 52/59 [00:10<00:01,  5.22it/s]predicting train subjects sagittal:  90%|████████▉ | 53/59 [00:10<00:01,  5.72it/s]predicting train subjects sagittal:  92%|█████████▏| 54/59 [00:10<00:00,  5.68it/s]predicting train subjects sagittal:  93%|█████████▎| 55/59 [00:11<00:00,  4.82it/s]predicting train subjects sagittal:  95%|█████████▍| 56/59 [00:11<00:00,  5.20it/s]predicting train subjects sagittal:  97%|█████████▋| 57/59 [00:11<00:00,  5.31it/s]predicting train subjects sagittal:  98%|█████████▊| 58/59 [00:11<00:00,  5.64it/s]predicting train subjects sagittal: 100%|██████████| 59/59 [00:11<00:00,  5.76it/s]predicting train subjects sagittal: 100%|██████████| 59/59 [00:11<00:00,  5.02it/s]
saving BB  test1-THALAMUS:   0%|          | 0/23 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  17%|█▋        | 4/23 [00:00<00:00, 37.74it/s]saving BB  test1-THALAMUS:  39%|███▉      | 9/23 [00:00<00:00, 40.43it/s]saving BB  test1-THALAMUS:  61%|██████    | 14/23 [00:00<00:00, 42.29it/s]saving BB  test1-THALAMUS:  83%|████████▎ | 19/23 [00:00<00:00, 40.65it/s]saving BB  test1-THALAMUS: 100%|██████████| 23/23 [00:00<00:00, 37.78it/s]saving BB  test1-THALAMUS: 100%|██████████| 23/23 [00:00<00:00, 40.17it/s]
saving BB  train1-THALAMUS:   0%|          | 0/59 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   8%|▊         | 5/59 [00:00<00:01, 49.86it/s]saving BB  train1-THALAMUS:  17%|█▋        | 10/59 [00:00<00:00, 49.29it/s]saving BB  train1-THALAMUS:  27%|██▋       | 16/59 [00:00<00:00, 50.46it/s]saving BB  train1-THALAMUS:  36%|███▌      | 21/59 [00:00<00:00, 49.46it/s]saving BB  train1-THALAMUS:  44%|████▍     | 26/59 [00:00<00:00, 48.15it/s]saving BB  train1-THALAMUS:  54%|█████▍    | 32/59 [00:00<00:00, 49.96it/s]saving BB  train1-THALAMUS:  66%|██████▌   | 39/59 [00:00<00:00, 53.58it/s]saving BB  train1-THALAMUS:  76%|███████▋  | 45/59 [00:00<00:00, 54.65it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 52/59 [00:00<00:00, 55.39it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 58.77it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 54.59it/s]
saving BB  test1-THALAMUS:   0%|          | 0/23 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  22%|██▏       | 5/23 [00:00<00:00, 49.17it/s]saving BB  test1-THALAMUS:  43%|████▎     | 10/23 [00:00<00:00, 48.28it/s]saving BB  test1-THALAMUS:  65%|██████▌   | 15/23 [00:00<00:00, 46.68it/s]saving BB  test1-THALAMUS:  87%|████████▋ | 20/23 [00:00<00:00, 44.83it/s]saving BB  test1-THALAMUS: 100%|██████████| 23/23 [00:00<00:00, 44.44it/s]
saving BB  train1-THALAMUS:   0%|          | 0/59 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   7%|▋         | 4/59 [00:00<00:01, 33.83it/s]saving BB  train1-THALAMUS:  15%|█▌        | 9/59 [00:00<00:01, 36.54it/s]saving BB  train1-THALAMUS:  25%|██▌       | 15/59 [00:00<00:01, 40.44it/s]saving BB  train1-THALAMUS:  34%|███▍      | 20/59 [00:00<00:00, 41.35it/s]saving BB  train1-THALAMUS:  41%|████      | 24/59 [00:00<00:00, 40.42it/s]saving BB  train1-THALAMUS:  49%|████▉     | 29/59 [00:00<00:00, 42.11it/s]saving BB  train1-THALAMUS:  59%|█████▉    | 35/59 [00:00<00:00, 44.89it/s]saving BB  train1-THALAMUS:  69%|██████▉   | 41/59 [00:00<00:00, 46.38it/s]saving BB  train1-THALAMUS:  80%|███████▉  | 47/59 [00:01<00:00, 48.24it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 52/59 [00:01<00:00, 47.65it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 58/59 [00:01<00:00, 48.83it/s]saving BB  train1-THALAMUS: 100%|██████████| 59/59 [00:01<00:00, 47.07it/s]
Loading train:   0%|          | 0/59 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/59 [00:00<00:29,  1.97it/s]Loading train:   3%|▎         | 2/59 [00:00<00:27,  2.11it/s]Loading train:   5%|▌         | 3/59 [00:01<00:28,  2.00it/s]Loading train:   7%|▋         | 4/59 [00:01<00:27,  1.97it/s]Loading train:   8%|▊         | 5/59 [00:02<00:27,  1.97it/s]Loading train:  10%|█         | 6/59 [00:03<00:27,  1.96it/s]Loading train:  12%|█▏        | 7/59 [00:03<00:28,  1.80it/s]Loading train:  14%|█▎        | 8/59 [00:04<00:26,  1.94it/s]Loading train:  15%|█▌        | 9/59 [00:04<00:26,  1.90it/s]Loading train:  17%|█▋        | 10/59 [00:05<00:26,  1.87it/s]Loading train:  19%|█▊        | 11/59 [00:05<00:27,  1.75it/s]Loading train:  20%|██        | 12/59 [00:06<00:26,  1.77it/s]Loading train:  22%|██▏       | 13/59 [00:06<00:24,  1.84it/s]Loading train:  24%|██▎       | 14/59 [00:07<00:24,  1.83it/s]Loading train:  25%|██▌       | 15/59 [00:07<00:22,  1.97it/s]Loading train:  27%|██▋       | 16/59 [00:08<00:22,  1.88it/s]Loading train:  29%|██▉       | 17/59 [00:08<00:21,  1.93it/s]Loading train:  31%|███       | 18/59 [00:09<00:22,  1.84it/s]Loading train:  32%|███▏      | 19/59 [00:10<00:21,  1.89it/s]Loading train:  34%|███▍      | 20/59 [00:10<00:21,  1.78it/s]Loading train:  36%|███▌      | 21/59 [00:10<00:18,  2.05it/s]Loading train:  37%|███▋      | 22/59 [00:11<00:17,  2.13it/s]Loading train:  39%|███▉      | 23/59 [00:12<00:19,  1.82it/s]Loading train:  41%|████      | 24/59 [00:12<00:16,  2.13it/s]Loading train:  42%|████▏     | 25/59 [00:12<00:16,  2.08it/s]Loading train:  44%|████▍     | 26/59 [00:13<00:13,  2.36it/s]Loading train:  46%|████▌     | 27/59 [00:13<00:12,  2.55it/s]Loading train:  47%|████▋     | 28/59 [00:13<00:11,  2.75it/s]Loading train:  49%|████▉     | 29/59 [00:14<00:10,  2.93it/s]Loading train:  51%|█████     | 30/59 [00:14<00:09,  3.15it/s]Loading train:  53%|█████▎    | 31/59 [00:14<00:08,  3.31it/s]Loading train:  54%|█████▍    | 32/59 [00:14<00:08,  3.37it/s]Loading train:  56%|█████▌    | 33/59 [00:15<00:07,  3.33it/s]Loading train:  58%|█████▊    | 34/59 [00:15<00:07,  3.57it/s]Loading train:  59%|█████▉    | 35/59 [00:15<00:06,  3.85it/s]Loading train:  61%|██████    | 36/59 [00:16<00:06,  3.63it/s]Loading train:  63%|██████▎   | 37/59 [00:16<00:06,  3.31it/s]Loading train:  64%|██████▍   | 38/59 [00:16<00:05,  3.61it/s]Loading train:  66%|██████▌   | 39/59 [00:16<00:05,  3.86it/s]Loading train:  68%|██████▊   | 40/59 [00:17<00:05,  3.59it/s]Loading train:  69%|██████▉   | 41/59 [00:17<00:05,  3.43it/s]Loading train:  71%|███████   | 42/59 [00:17<00:05,  3.39it/s]Loading train:  73%|███████▎  | 43/59 [00:18<00:04,  3.29it/s]Loading train:  75%|███████▍  | 44/59 [00:18<00:04,  3.35it/s]Loading train:  76%|███████▋  | 45/59 [00:18<00:03,  3.56it/s]Loading train:  78%|███████▊  | 46/59 [00:18<00:03,  3.44it/s]Loading train:  80%|███████▉  | 47/59 [00:19<00:03,  3.56it/s]Loading train:  81%|████████▏ | 48/59 [00:19<00:03,  3.53it/s]Loading train:  83%|████████▎ | 49/59 [00:19<00:02,  3.68it/s]Loading train:  85%|████████▍ | 50/59 [00:20<00:02,  3.58it/s]Loading train:  86%|████████▋ | 51/59 [00:20<00:02,  3.75it/s]Loading train:  88%|████████▊ | 52/59 [00:20<00:01,  3.69it/s]Loading train:  90%|████████▉ | 53/59 [00:20<00:01,  3.78it/s]Loading train:  92%|█████████▏| 54/59 [00:21<00:01,  3.67it/s]Loading train:  93%|█████████▎| 55/59 [00:21<00:01,  3.70it/s]Loading train:  95%|█████████▍| 56/59 [00:21<00:00,  3.65it/s]Loading train:  97%|█████████▋| 57/59 [00:21<00:00,  3.67it/s]Loading train:  98%|█████████▊| 58/59 [00:22<00:00,  3.83it/s]Loading train: 100%|██████████| 59/59 [00:22<00:00,  3.90it/s]Loading train: 100%|██████████| 59/59 [00:22<00:00,  2.64it/s]
concatenating: train:   0%|          | 0/59 [00:00<?, ?it/s]concatenating: train:  10%|█         | 6/59 [00:00<00:00, 55.89it/s]concatenating: train:  20%|██        | 12/59 [00:00<00:00, 54.39it/s]concatenating: train:  31%|███       | 18/59 [00:00<00:00, 53.96it/s]concatenating: train:  41%|████      | 24/59 [00:00<00:00, 54.09it/s]concatenating: train:  51%|█████     | 30/59 [00:00<00:00, 55.01it/s]concatenating: train:  61%|██████    | 36/59 [00:00<00:00, 56.20it/s]concatenating: train:  71%|███████   | 42/59 [00:00<00:00, 56.16it/s]concatenating: train:  81%|████████▏ | 48/59 [00:00<00:00, 56.78it/s]concatenating: train:  92%|█████████▏| 54/59 [00:00<00:00, 56.54it/s]concatenating: train: 100%|██████████| 59/59 [00:01<00:00, 55.74it/s]
Loading test:   0%|          | 0/23 [00:00<?, ?it/s]Loading test:   4%|▍         | 1/23 [00:00<00:07,  3.04it/s]Loading test:   9%|▊         | 2/23 [00:00<00:06,  3.28it/s]Loading test:  13%|█▎        | 3/23 [00:00<00:05,  3.40it/s]Loading test:  17%|█▋        | 4/23 [00:01<00:05,  3.22it/s]Loading test:  22%|██▏       | 5/23 [00:01<00:05,  3.53it/s]Loading test:  26%|██▌       | 6/23 [00:01<00:04,  3.57it/s]Loading test:  30%|███       | 7/23 [00:01<00:04,  3.57it/s]Loading test:  35%|███▍      | 8/23 [00:02<00:04,  3.49it/s]Loading test:  39%|███▉      | 9/23 [00:02<00:04,  3.17it/s]Loading test:  43%|████▎     | 10/23 [00:02<00:03,  3.32it/s]Loading test:  48%|████▊     | 11/23 [00:03<00:05,  2.28it/s]Loading test:  52%|█████▏    | 12/23 [00:04<00:05,  1.96it/s]Loading test:  57%|█████▋    | 13/23 [00:04<00:04,  2.23it/s]Loading test:  61%|██████    | 14/23 [00:04<00:03,  2.51it/s]Loading test:  65%|██████▌   | 15/23 [00:05<00:02,  2.71it/s]Loading test:  70%|██████▉   | 16/23 [00:05<00:02,  3.05it/s]Loading test:  74%|███████▍  | 17/23 [00:05<00:02,  2.74it/s]Loading test:  78%|███████▊  | 18/23 [00:06<00:01,  2.54it/s]Loading test:  83%|████████▎ | 19/23 [00:06<00:01,  2.37it/s]Loading test:  87%|████████▋ | 20/23 [00:07<00:01,  1.61it/s]Loading test:  91%|█████████▏| 21/23 [00:08<00:01,  1.65it/s]Loading test:  96%|█████████▌| 22/23 [00:09<00:00,  1.74it/s]Loading test: 100%|██████████| 23/23 [00:10<00:00,  1.41it/s]Loading test: 100%|██████████| 23/23 [00:10<00:00,  2.29it/s]
concatenating: validation:   0%|          | 0/23 [00:00<?, ?it/s]concatenating: validation:  30%|███       | 7/23 [00:00<00:00, 68.65it/s]concatenating: validation:  61%|██████    | 14/23 [00:00<00:00, 67.57it/s]concatenating: validation:  91%|█████████▏| 21/23 [00:00<00:00, 65.88it/s]concatenating: validation: 100%|██████████| 23/23 [00:00<00:00, 64.70it/s]
dropout_2 (Dropout)             (None, 29, 42, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 42, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 42, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 42, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 42, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 42, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 42, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 42, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 42, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 84, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 84, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 84, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 84, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 84, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 84, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 84, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 84, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 84, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 84, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 168, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 168, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 168, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 168, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 168, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 168, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 168, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 168, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 168, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 168, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 168, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________

 MultiClass_24567891011121314 SD: [2] GPU: 3 

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 84, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 84, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 84, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 84, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 84, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 84, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 84, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 42, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 42, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 42, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 42, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 42, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 42, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 42, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 42, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 42, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 21, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 21, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 21, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 21, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 21, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 21, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 21, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 21, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 21, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 21, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 42, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 42, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 42, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 42, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 42, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 42, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 42, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 42, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 42, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 42, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 84, 20)   11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 84, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 84, 20)   7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 84, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 84, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 84, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 84, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 84, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 84, 60)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________2020-09-30 21:49:09.147834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-09-30 21:49:09.149491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 21:49:09.149527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-09-30 21:49:09.149544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-09-30 21:49:09.149697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15153 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

predicting test subjects:   0%|          | 0/23 [00:00<?, ?it/s]predicting test subjects:   4%|▍         | 1/23 [00:01<00:23,  1.05s/it]predicting test subjects:   9%|▊         | 2/23 [00:01<00:18,  1.13it/s]predicting test subjects:  13%|█▎        | 3/23 [00:02<00:16,  1.25it/s]predicting test subjects:  17%|█▋        | 4/23 [00:02<00:13,  1.39it/s]predicting test subjects:  22%|██▏       | 5/23 [00:03<00:12,  1.50it/s]predicting test subjects:  26%|██▌       | 6/23 [00:03<00:10,  1.55it/s]predicting test subjects:  30%|███       | 7/23 [00:04<00:09,  1.66it/s]predicting test subjects:  35%|███▍      | 8/23 [00:04<00:08,  1.68it/s]predicting test subjects:  39%|███▉      | 9/23 [00:05<00:09,  1.48it/s]predicting test subjects:  43%|████▎     | 10/23 [00:06<00:09,  1.43it/s]predicting test subjects:  48%|████▊     | 11/23 [00:07<00:08,  1.41it/s]predicting test subjects:  52%|█████▏    | 12/23 [00:07<00:07,  1.47it/s]predicting test subjects:  57%|█████▋    | 13/23 [00:08<00:06,  1.53it/s]predicting test subjects:  61%|██████    | 14/23 [00:09<00:05,  1.52it/s]predicting test subjects:  65%|██████▌   | 15/23 [00:09<00:05,  1.54it/s]predicting test subjects:  70%|██████▉   | 16/23 [00:10<00:04,  1.52it/s]predicting test subjects:  74%|███████▍  | 17/23 [00:11<00:03,  1.56it/s]predicting test subjects:  78%|███████▊  | 18/23 [00:11<00:03,  1.50it/s]predicting test subjects:  83%|████████▎ | 19/23 [00:12<00:02,  1.53it/s]predicting test subjects:  87%|████████▋ | 20/23 [00:13<00:02,  1.30it/s]predicting test subjects:  91%|█████████▏| 21/23 [00:14<00:01,  1.29it/s]predicting test subjects:  96%|█████████▌| 22/23 [00:14<00:00,  1.40it/s]predicting test subjects: 100%|██████████| 23/23 [00:15<00:00,  1.33it/s]predicting test subjects: 100%|██████████| 23/23 [00:15<00:00,  1.47it/s]
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:04<01:32,  4.19s/it]  9%|▊         | 2/23 [00:06<01:16,  3.63s/it] 13%|█▎        | 3/23 [00:09<01:06,  3.35s/it] 17%|█▋        | 4/23 [00:12<01:01,  3.23s/it] 22%|██▏       | 5/23 [00:15<00:56,  3.14s/it] 26%|██▌       | 6/23 [00:18<00:52,  3.10s/it] 30%|███       | 7/23 [00:20<00:45,  2.87s/it] 35%|███▍      | 8/23 [00:23<00:42,  2.83s/it] 39%|███▉      | 9/23 [00:28<00:49,  3.52s/it] 43%|████▎     | 10/23 [00:32<00:49,  3.78s/it] 48%|████▊     | 11/23 [00:37<00:50,  4.17s/it] 52%|█████▏    | 12/23 [00:41<00:45,  4.12s/it] 57%|█████▋    | 13/23 [00:44<00:38,  3.85s/it] 61%|██████    | 14/23 [00:48<00:33,  3.70s/it] 65%|██████▌   | 15/23 [00:52<00:29,  3.72s/it] 70%|██████▉   | 16/23 [00:55<00:26,  3.77s/it] 74%|███████▍  | 17/23 [00:59<00:22,  3.79s/it] 78%|███████▊  | 18/23 [01:04<00:20,  4.05s/it] 83%|████████▎ | 19/23 [01:08<00:16,  4.14s/it] 87%|████████▋ | 20/23 [01:16<00:15,  5.30s/it] 91%|█████████▏| 21/23 [01:22<00:10,  5.32s/it] 96%|█████████▌| 22/23 [01:25<00:04,  4.84s/it]100%|██████████| 23/23 [01:31<00:00,  4.96s/it]100%|██████████| 23/23 [01:31<00:00,  3.96s/it]    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: flipped_PProcessed.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 
    Flipping: 1-THALAMUS.nii.gz   ----------- 
    Flipping: 10-MGN.nii.gz   ----------- 
    Flipping: 11-CM.nii.gz   ----------- 
    Flipping: 12-MD-Pf.nii.gz   ----------- 
    Flipping: 13-Hb.nii.gz   ----------- 
    Flipping: 14-MTT.nii.gz   ----------- 
    Flipping: 2-AV.nii.gz   ----------- 
    Flipping: 4-VA.nii.gz   ----------- 
    Flipping: 5-VLa.nii.gz   ----------- 
    Flipping: 6-VLP.nii.gz   ----------- 
    Flipping: 7-VPL.nii.gz   ----------- 
    Flipping: 8-Pul.nii.gz   ----------- 
    Flipping: 9-LGN.nii.gz   ----------- 


dropout_5 (Dropout)             (None, 52, 84, 60)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 84, 13)   793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
Reverse Flip L-R the flipped image & its nuclei
