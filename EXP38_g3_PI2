2019-08-16 21:24:15.521002: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-16 21:24:15.844144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:09:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-08-16 21:24:15.844778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-16 21:24:16.271361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-16 21:24:16.271431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-16 21:24:16.271445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-16 21:24:16.271918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<04:22,  1.01it/s]Loading train:   1%|          | 2/266 [00:01<04:03,  1.09it/s]Loading train:   1%|          | 3/266 [00:02<03:50,  1.14it/s]Loading train:   2%|▏         | 4/266 [00:03<03:28,  1.26it/s]Loading train:   2%|▏         | 5/266 [00:03<03:24,  1.28it/s]Loading train:   2%|▏         | 6/266 [00:04<03:22,  1.29it/s]Loading train:   3%|▎         | 7/266 [00:05<03:22,  1.28it/s]Loading train:   3%|▎         | 8/266 [00:05<03:03,  1.40it/s]Loading train:   3%|▎         | 9/266 [00:06<02:55,  1.47it/s]Loading train:   4%|▍         | 10/266 [00:07<02:59,  1.43it/s]Loading train:   4%|▍         | 11/266 [00:08<03:03,  1.39it/s]Loading train:   5%|▍         | 12/266 [00:08<03:09,  1.34it/s]Loading train:   5%|▍         | 13/266 [00:09<03:09,  1.33it/s]Loading train:   5%|▌         | 14/266 [00:10<03:14,  1.29it/s]Loading train:   6%|▌         | 15/266 [00:11<03:15,  1.28it/s]Loading train:   6%|▌         | 16/266 [00:12<03:14,  1.28it/s]Loading train:   6%|▋         | 17/266 [00:12<03:01,  1.37it/s]Loading train:   7%|▋         | 18/266 [00:13<02:52,  1.44it/s]Loading train:   7%|▋         | 19/266 [00:14<03:11,  1.29it/s]Loading train:   8%|▊         | 20/266 [00:14<03:05,  1.33it/s]Loading train:   8%|▊         | 21/266 [00:15<03:09,  1.29it/s]Loading train:   8%|▊         | 22/266 [00:16<03:20,  1.22it/s]Loading train:   9%|▊         | 23/266 [00:17<03:08,  1.29it/s]Loading train:   9%|▉         | 24/266 [00:18<03:09,  1.27it/s]Loading train:   9%|▉         | 25/266 [00:18<02:59,  1.35it/s]Loading train:  10%|▉         | 26/266 [00:19<02:51,  1.40it/s]Loading train:  10%|█         | 27/266 [00:20<03:03,  1.31it/s]Loading train:  11%|█         | 28/266 [00:21<03:31,  1.12it/s]Loading train:  11%|█         | 29/266 [00:22<03:40,  1.08it/s]Loading train:  11%|█▏        | 30/266 [00:23<03:39,  1.08it/s]Loading train:  12%|█▏        | 31/266 [00:24<03:14,  1.21it/s]Loading train:  12%|█▏        | 32/266 [00:24<03:01,  1.29it/s]Loading train:  12%|█▏        | 33/266 [00:25<03:01,  1.28it/s]Loading train:  13%|█▎        | 34/266 [00:26<03:01,  1.28it/s]Loading train:  13%|█▎        | 35/266 [00:27<03:05,  1.24it/s]Loading train:  14%|█▎        | 36/266 [00:28<03:19,  1.15it/s]Loading train:  14%|█▍        | 37/266 [00:29<03:27,  1.11it/s]Loading train:  14%|█▍        | 38/266 [00:30<03:36,  1.05it/s]Loading train:  15%|█▍        | 39/266 [00:31<03:44,  1.01it/s]Loading train:  15%|█▌        | 40/266 [00:32<03:50,  1.02s/it]Loading train:  15%|█▌        | 41/266 [00:33<03:48,  1.01s/it]Loading train:  16%|█▌        | 42/266 [00:34<03:23,  1.10it/s]Loading train:  16%|█▌        | 43/266 [00:35<03:30,  1.06it/s]Loading train:  17%|█▋        | 44/266 [00:35<03:18,  1.12it/s]Loading train:  17%|█▋        | 45/266 [00:36<02:50,  1.29it/s]Loading train:  17%|█▋        | 46/266 [00:37<02:58,  1.24it/s]Loading train:  18%|█▊        | 47/266 [00:38<03:25,  1.07it/s]Loading train:  18%|█▊        | 48/266 [00:39<03:25,  1.06it/s]Loading train:  18%|█▊        | 49/266 [00:40<03:23,  1.07it/s]Loading train:  19%|█▉        | 50/266 [00:41<03:16,  1.10it/s]Loading train:  19%|█▉        | 51/266 [00:41<03:05,  1.16it/s]Loading train:  20%|█▉        | 52/266 [00:42<02:54,  1.23it/s]Loading train:  20%|█▉        | 53/266 [00:43<02:40,  1.33it/s]Loading train:  20%|██        | 54/266 [00:43<02:35,  1.37it/s]Loading train:  21%|██        | 55/266 [00:44<02:40,  1.31it/s]Loading train:  21%|██        | 56/266 [00:45<02:23,  1.46it/s]Loading train:  21%|██▏       | 57/266 [00:46<02:29,  1.40it/s]Loading train:  22%|██▏       | 58/266 [00:46<02:22,  1.46it/s]Loading train:  22%|██▏       | 59/266 [00:47<02:46,  1.24it/s]Loading train:  23%|██▎       | 60/266 [00:48<02:38,  1.30it/s]Loading train:  23%|██▎       | 61/266 [00:48<02:19,  1.47it/s]Loading train:  23%|██▎       | 62/266 [00:49<02:12,  1.54it/s]Loading train:  24%|██▎       | 63/266 [00:50<02:25,  1.40it/s]Loading train:  24%|██▍       | 64/266 [00:51<02:47,  1.20it/s]Loading train:  24%|██▍       | 65/266 [00:52<02:50,  1.18it/s]Loading train:  25%|██▍       | 66/266 [00:53<03:09,  1.05it/s]Loading train:  25%|██▌       | 67/266 [00:54<03:09,  1.05it/s]Loading train:  26%|██▌       | 68/266 [00:55<03:04,  1.07it/s]Loading train:  26%|██▌       | 69/266 [00:56<02:59,  1.09it/s]Loading train:  26%|██▋       | 70/266 [00:57<03:00,  1.09it/s]Loading train:  27%|██▋       | 71/266 [00:58<03:16,  1.01s/it]Loading train:  27%|██▋       | 72/266 [00:59<03:09,  1.02it/s]Loading train:  27%|██▋       | 73/266 [01:00<03:11,  1.01it/s]Loading train:  28%|██▊       | 74/266 [01:00<02:47,  1.15it/s]Loading train:  28%|██▊       | 75/266 [01:01<02:47,  1.14it/s]Loading train:  29%|██▊       | 76/266 [01:02<02:36,  1.21it/s]Loading train:  29%|██▉       | 77/266 [01:03<02:44,  1.15it/s]Loading train:  29%|██▉       | 78/266 [01:04<02:51,  1.10it/s]Loading train:  30%|██▉       | 79/266 [01:05<02:53,  1.08it/s]Loading train:  30%|███       | 80/266 [01:06<03:00,  1.03it/s]Loading train:  30%|███       | 81/266 [01:07<03:01,  1.02it/s]Loading train:  31%|███       | 82/266 [01:08<02:53,  1.06it/s]Loading train:  31%|███       | 83/266 [01:09<02:41,  1.13it/s]Loading train:  32%|███▏      | 84/266 [01:10<02:43,  1.12it/s]Loading train:  32%|███▏      | 85/266 [01:10<02:40,  1.13it/s]Loading train:  32%|███▏      | 86/266 [01:11<02:34,  1.16it/s]Loading train:  33%|███▎      | 87/266 [01:12<02:38,  1.13it/s]Loading train:  33%|███▎      | 88/266 [01:13<02:38,  1.12it/s]Loading train:  33%|███▎      | 89/266 [01:14<02:41,  1.09it/s]Loading train:  34%|███▍      | 90/266 [01:15<02:36,  1.12it/s]Loading train:  34%|███▍      | 91/266 [01:16<02:26,  1.19it/s]Loading train:  35%|███▍      | 92/266 [01:16<02:25,  1.19it/s]Loading train:  35%|███▍      | 93/266 [01:18<02:36,  1.10it/s]Loading train:  35%|███▌      | 94/266 [01:19<02:42,  1.06it/s]Loading train:  36%|███▌      | 95/266 [01:20<02:48,  1.01it/s]Loading train:  36%|███▌      | 96/266 [01:20<02:34,  1.10it/s]Loading train:  36%|███▋      | 97/266 [01:21<02:34,  1.09it/s]Loading train:  37%|███▋      | 98/266 [01:22<02:20,  1.20it/s]Loading train:  37%|███▋      | 99/266 [01:23<02:08,  1.30it/s]Loading train:  38%|███▊      | 100/266 [01:23<01:58,  1.40it/s]Loading train:  38%|███▊      | 101/266 [01:24<01:50,  1.49it/s]Loading train:  38%|███▊      | 102/266 [01:24<01:54,  1.43it/s]Loading train:  39%|███▊      | 103/266 [01:25<02:06,  1.29it/s]Loading train:  39%|███▉      | 104/266 [01:27<02:19,  1.16it/s]Loading train:  39%|███▉      | 105/266 [01:28<02:34,  1.04it/s]Loading train:  40%|███▉      | 106/266 [01:29<02:40,  1.00s/it]Loading train:  40%|████      | 107/266 [01:30<02:46,  1.05s/it]Loading train:  41%|████      | 108/266 [01:31<02:43,  1.04s/it]Loading train:  41%|████      | 109/266 [01:32<02:38,  1.01s/it]Loading train:  41%|████▏     | 110/266 [01:33<02:33,  1.01it/s]Loading train:  42%|████▏     | 111/266 [01:34<02:43,  1.05s/it]Loading train:  42%|████▏     | 112/266 [01:35<02:36,  1.02s/it]Loading train:  42%|████▏     | 113/266 [01:36<02:40,  1.05s/it]Loading train:  43%|████▎     | 114/266 [01:37<02:34,  1.02s/it]Loading train:  43%|████▎     | 115/266 [01:38<02:36,  1.03s/it]Loading train:  44%|████▎     | 116/266 [01:39<02:38,  1.06s/it]Loading train:  44%|████▍     | 117/266 [01:40<02:40,  1.08s/it]Loading train:  44%|████▍     | 118/266 [01:41<02:33,  1.04s/it]Loading train:  45%|████▍     | 119/266 [01:42<02:34,  1.05s/it]Loading train:  45%|████▌     | 120/266 [01:44<02:39,  1.09s/it]Loading train:  45%|████▌     | 121/266 [01:45<02:33,  1.06s/it]Loading train:  46%|████▌     | 122/266 [01:46<02:30,  1.04s/it]Loading train:  46%|████▌     | 123/266 [01:47<02:30,  1.05s/it]Loading train:  47%|████▋     | 124/266 [01:47<02:13,  1.07it/s]Loading train:  47%|████▋     | 125/266 [01:48<02:16,  1.03it/s]Loading train:  47%|████▋     | 126/266 [01:49<02:22,  1.02s/it]Loading train:  48%|████▊     | 127/266 [01:50<02:16,  1.02it/s]Loading train:  48%|████▊     | 128/266 [01:51<02:13,  1.04it/s]Loading train:  48%|████▊     | 129/266 [01:52<02:04,  1.10it/s]Loading train:  49%|████▉     | 130/266 [01:53<01:55,  1.18it/s]Loading train:  49%|████▉     | 131/266 [01:54<01:50,  1.22it/s]Loading train:  50%|████▉     | 132/266 [01:54<01:53,  1.18it/s]Loading train:  50%|█████     | 133/266 [01:55<01:44,  1.28it/s]Loading train:  50%|█████     | 134/266 [01:56<01:46,  1.23it/s]Loading train:  51%|█████     | 135/266 [01:57<01:48,  1.21it/s]Loading train:  51%|█████     | 136/266 [01:58<01:50,  1.17it/s]Loading train:  52%|█████▏    | 137/266 [01:59<01:45,  1.22it/s]Loading train:  52%|█████▏    | 138/266 [01:59<01:35,  1.35it/s]Loading train:  52%|█████▏    | 139/266 [02:00<01:45,  1.20it/s]Loading train:  53%|█████▎    | 140/266 [02:01<01:46,  1.18it/s]Loading train:  53%|█████▎    | 141/266 [02:01<01:29,  1.39it/s]Loading train:  53%|█████▎    | 142/266 [02:02<01:34,  1.31it/s]Loading train:  54%|█████▍    | 143/266 [02:03<01:45,  1.17it/s]Loading train:  54%|█████▍    | 144/266 [02:04<01:48,  1.13it/s]Loading train:  55%|█████▍    | 145/266 [02:05<01:58,  1.02it/s]Loading train:  55%|█████▍    | 146/266 [02:06<01:51,  1.07it/s]Loading train:  55%|█████▌    | 147/266 [02:07<01:58,  1.01it/s]Loading train:  56%|█████▌    | 148/266 [02:08<01:57,  1.00it/s]Loading train:  56%|█████▌    | 149/266 [02:09<01:56,  1.00it/s]Loading train:  56%|█████▋    | 150/266 [02:10<01:56,  1.00s/it]Loading train:  57%|█████▋    | 151/266 [02:11<01:51,  1.03it/s]Loading train:  57%|█████▋    | 152/266 [02:12<01:45,  1.08it/s]Loading train:  58%|█████▊    | 153/266 [02:13<01:54,  1.01s/it]Loading train:  58%|█████▊    | 154/266 [02:14<01:42,  1.09it/s]Loading train:  58%|█████▊    | 155/266 [02:15<01:35,  1.16it/s]Loading train:  59%|█████▊    | 156/266 [02:16<01:35,  1.15it/s]Loading train:  59%|█████▉    | 157/266 [02:17<01:38,  1.11it/s]Loading train:  59%|█████▉    | 158/266 [02:18<01:43,  1.05it/s]Loading train:  60%|█████▉    | 159/266 [02:19<01:41,  1.05it/s]Loading train:  60%|██████    | 160/266 [02:20<01:36,  1.10it/s]Loading train:  61%|██████    | 161/266 [02:21<01:43,  1.02it/s]Loading train:  61%|██████    | 162/266 [02:22<01:45,  1.02s/it]Loading train:  61%|██████▏   | 163/266 [02:23<01:41,  1.01it/s]Loading train:  62%|██████▏   | 164/266 [02:24<01:47,  1.05s/it]Loading train:  62%|██████▏   | 165/266 [02:25<01:43,  1.03s/it]Loading train:  62%|██████▏   | 166/266 [02:26<01:49,  1.09s/it]Loading train:  63%|██████▎   | 167/266 [02:27<01:48,  1.09s/it]Loading train:  63%|██████▎   | 168/266 [02:28<01:43,  1.06s/it]Loading train:  64%|██████▎   | 169/266 [02:29<01:42,  1.06s/it]Loading train:  64%|██████▍   | 170/266 [02:30<01:37,  1.02s/it]Loading train:  64%|██████▍   | 171/266 [02:31<01:32,  1.03it/s]Loading train:  65%|██████▍   | 172/266 [02:32<01:41,  1.08s/it]Loading train:  65%|██████▌   | 173/266 [02:33<01:30,  1.02it/s]Loading train:  65%|██████▌   | 174/266 [02:34<01:30,  1.01it/s]Loading train:  66%|██████▌   | 175/266 [02:35<01:29,  1.02it/s]Loading train:  66%|██████▌   | 176/266 [02:36<01:29,  1.01it/s]Loading train:  67%|██████▋   | 177/266 [02:37<01:31,  1.03s/it]Loading train:  67%|██████▋   | 178/266 [02:38<01:30,  1.03s/it]Loading train:  67%|██████▋   | 179/266 [02:39<01:32,  1.07s/it]Loading train:  68%|██████▊   | 180/266 [02:40<01:24,  1.01it/s]Loading train:  68%|██████▊   | 181/266 [02:41<01:18,  1.08it/s]Loading train:  68%|██████▊   | 182/266 [02:42<01:19,  1.06it/s]Loading train:  69%|██████▉   | 183/266 [02:43<01:18,  1.06it/s]Loading train:  69%|██████▉   | 184/266 [02:44<01:18,  1.05it/s]Loading train:  70%|██████▉   | 185/266 [02:45<01:17,  1.05it/s]Loading train:  70%|██████▉   | 186/266 [02:46<01:14,  1.07it/s]Loading train:  70%|███████   | 187/266 [02:46<01:02,  1.26it/s]Loading train:  71%|███████   | 188/266 [02:47<01:06,  1.17it/s]Loading train:  71%|███████   | 189/266 [02:48<01:11,  1.08it/s]Loading train:  71%|███████▏  | 190/266 [02:49<01:12,  1.05it/s]Loading train:  72%|███████▏  | 191/266 [02:50<01:16,  1.01s/it]Loading train:  72%|███████▏  | 192/266 [02:51<01:14,  1.01s/it]Loading train:  73%|███████▎  | 193/266 [02:52<01:13,  1.00s/it]Loading train:  73%|███████▎  | 194/266 [02:54<01:14,  1.04s/it]Loading train:  73%|███████▎  | 195/266 [02:54<01:09,  1.02it/s]Loading train:  74%|███████▎  | 196/266 [02:55<01:02,  1.11it/s]Loading train:  74%|███████▍  | 197/266 [02:56<01:01,  1.13it/s]Loading train:  74%|███████▍  | 198/266 [02:57<00:58,  1.16it/s]Loading train:  75%|███████▍  | 199/266 [02:58<00:59,  1.12it/s]Loading train:  75%|███████▌  | 200/266 [02:59<01:00,  1.08it/s]Loading train:  76%|███████▌  | 201/266 [03:00<00:59,  1.08it/s]Loading train:  76%|███████▌  | 202/266 [03:01<01:00,  1.05it/s]Loading train:  76%|███████▋  | 203/266 [03:02<00:57,  1.09it/s]Loading train:  77%|███████▋  | 204/266 [03:02<00:54,  1.13it/s]Loading train:  77%|███████▋  | 205/266 [03:03<00:53,  1.15it/s]Loading train:  77%|███████▋  | 206/266 [03:04<00:48,  1.24it/s]Loading train:  78%|███████▊  | 207/266 [03:04<00:44,  1.32it/s]Loading train:  78%|███████▊  | 208/266 [03:05<00:44,  1.30it/s]Loading train:  79%|███████▊  | 209/266 [03:06<00:44,  1.27it/s]Loading train:  79%|███████▉  | 210/266 [03:07<00:42,  1.30it/s]Loading train:  79%|███████▉  | 211/266 [03:08<00:44,  1.23it/s]Loading train:  80%|███████▉  | 212/266 [03:09<00:46,  1.17it/s]Loading train:  80%|████████  | 213/266 [03:10<00:49,  1.08it/s]Loading train:  80%|████████  | 214/266 [03:11<00:50,  1.04it/s]Loading train:  81%|████████  | 215/266 [03:12<00:50,  1.01it/s]Loading train:  81%|████████  | 216/266 [03:12<00:41,  1.22it/s]Loading train:  82%|████████▏ | 217/266 [03:13<00:43,  1.14it/s]Loading train:  82%|████████▏ | 218/266 [03:14<00:45,  1.07it/s]Loading train:  82%|████████▏ | 219/266 [03:15<00:44,  1.05it/s]Loading train:  83%|████████▎ | 220/266 [03:17<00:51,  1.11s/it]Loading train:  83%|████████▎ | 221/266 [03:18<00:47,  1.06s/it]Loading train:  83%|████████▎ | 222/266 [03:19<00:47,  1.07s/it]Loading train:  84%|████████▍ | 223/266 [03:20<00:47,  1.10s/it]Loading train:  84%|████████▍ | 224/266 [03:21<00:48,  1.15s/it]Loading train:  85%|████████▍ | 225/266 [03:23<00:47,  1.16s/it]Loading train:  85%|████████▍ | 226/266 [03:24<00:46,  1.15s/it]Loading train:  85%|████████▌ | 227/266 [03:25<00:44,  1.14s/it]Loading train:  86%|████████▌ | 228/266 [03:25<00:34,  1.09it/s]Loading train:  86%|████████▌ | 229/266 [03:26<00:31,  1.17it/s]Loading train:  86%|████████▋ | 230/266 [03:27<00:29,  1.22it/s]Loading train:  87%|████████▋ | 231/266 [03:28<00:32,  1.09it/s]Loading train:  87%|████████▋ | 232/266 [03:29<00:32,  1.05it/s]Loading train:  88%|████████▊ | 233/266 [03:30<00:33,  1.01s/it]Loading train:  88%|████████▊ | 234/266 [03:31<00:34,  1.07s/it]Loading train:  88%|████████▊ | 235/266 [03:32<00:33,  1.07s/it]Loading train:  89%|████████▊ | 236/266 [03:33<00:32,  1.10s/it]Loading train:  89%|████████▉ | 237/266 [03:34<00:28,  1.01it/s]Loading train:  89%|████████▉ | 238/266 [03:35<00:28,  1.02s/it]Loading train:  90%|████████▉ | 239/266 [03:36<00:24,  1.10it/s]Loading train:  90%|█████████ | 240/266 [03:37<00:24,  1.06it/s]Loading train:  91%|█████████ | 241/266 [03:38<00:23,  1.08it/s]Loading train:  91%|█████████ | 242/266 [03:39<00:21,  1.10it/s]Loading train:  91%|█████████▏| 243/266 [03:39<00:20,  1.11it/s]Loading train:  92%|█████████▏| 244/266 [03:40<00:17,  1.23it/s]Loading train:  92%|█████████▏| 245/266 [03:41<00:18,  1.13it/s]Loading train:  92%|█████████▏| 246/266 [03:42<00:17,  1.11it/s]Loading train:  93%|█████████▎| 247/266 [03:43<00:16,  1.13it/s]Loading train:  93%|█████████▎| 248/266 [03:44<00:14,  1.23it/s]Loading train:  94%|█████████▎| 249/266 [03:44<00:13,  1.27it/s]Loading train:  94%|█████████▍| 250/266 [03:45<00:11,  1.34it/s]Loading train:  94%|█████████▍| 251/266 [03:46<00:11,  1.27it/s]Loading train:  95%|█████████▍| 252/266 [03:46<00:09,  1.41it/s]Loading train:  95%|█████████▌| 253/266 [03:47<00:09,  1.44it/s]Loading train:  95%|█████████▌| 254/266 [03:48<00:08,  1.34it/s]Loading train:  96%|█████████▌| 255/266 [03:49<00:08,  1.25it/s]Loading train:  96%|█████████▌| 256/266 [03:50<00:09,  1.07it/s]Loading train:  97%|█████████▋| 257/266 [03:51<00:08,  1.01it/s]Loading train:  97%|█████████▋| 258/266 [03:52<00:08,  1.01s/it]Loading train:  97%|█████████▋| 259/266 [03:53<00:07,  1.01s/it]Loading train:  98%|█████████▊| 260/266 [03:54<00:06,  1.06s/it]Loading train:  98%|█████████▊| 261/266 [03:56<00:05,  1.07s/it]Loading train:  98%|█████████▊| 262/266 [03:57<00:04,  1.04s/it]Loading train:  99%|█████████▉| 263/266 [03:57<00:02,  1.03it/s]Loading train:  99%|█████████▉| 264/266 [03:59<00:02,  1.08s/it]Loading train: 100%|█████████▉| 265/266 [04:00<00:01,  1.11s/it]Loading train: 100%|██████████| 266/266 [04:01<00:00,  1.10s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 4/266 [00:00<00:09, 27.85it/s]concatenating: train:   2%|▏         | 6/266 [00:00<00:14, 18.12it/s]concatenating: train:   3%|▎         | 7/266 [00:00<00:18, 13.64it/s]concatenating: train:   3%|▎         | 8/266 [00:00<00:21, 12.12it/s]concatenating: train:   4%|▍         | 10/266 [00:00<00:20, 12.70it/s]concatenating: train:   5%|▍         | 12/266 [00:01<00:25,  9.87it/s]concatenating: train:   5%|▌         | 14/266 [00:01<00:21, 11.63it/s]concatenating: train:   7%|▋         | 18/266 [00:01<00:16, 14.71it/s]concatenating: train:  10%|█         | 27/266 [00:01<00:12, 19.63it/s]concatenating: train:  28%|██▊       | 75/266 [00:01<00:06, 27.55it/s]concatenating: train:  36%|███▌      | 95/266 [00:01<00:04, 36.54it/s]concatenating: train:  42%|████▏     | 112/266 [00:02<00:04, 31.66it/s]concatenating: train:  49%|████▉     | 131/266 [00:02<00:03, 41.67it/s]concatenating: train:  55%|█████▍    | 145/266 [00:03<00:04, 24.56it/s]concatenating: train:  58%|█████▊    | 155/266 [00:03<00:04, 24.91it/s]concatenating: train:  61%|██████▏   | 163/266 [00:04<00:06, 14.96it/s]concatenating: train:  64%|██████▎   | 169/266 [00:05<00:05, 17.81it/s]concatenating: train:  65%|██████▌   | 174/266 [00:05<00:05, 18.30it/s]concatenating: train:  67%|██████▋   | 178/266 [00:05<00:06, 12.65it/s]concatenating: train:  68%|██████▊   | 182/266 [00:06<00:06, 13.69it/s]concatenating: train:  70%|██████▉   | 185/266 [00:06<00:05, 16.14it/s]concatenating: train:  71%|███████   | 189/266 [00:06<00:04, 18.88it/s]concatenating: train:  72%|███████▏  | 192/266 [00:06<00:03, 18.57it/s]concatenating: train:  73%|███████▎  | 195/266 [00:06<00:05, 12.87it/s]concatenating: train:  74%|███████▍  | 197/266 [00:07<00:06, 10.77it/s]concatenating: train:  76%|███████▌  | 201/266 [00:07<00:04, 13.46it/s]concatenating: train:  77%|███████▋  | 205/266 [00:07<00:03, 16.22it/s]concatenating: train:  78%|███████▊  | 208/266 [00:07<00:03, 17.90it/s]concatenating: train:  79%|███████▉  | 211/266 [00:07<00:03, 17.15it/s]concatenating: train:  80%|████████  | 214/266 [00:08<00:03, 14.88it/s]concatenating: train:  81%|████████  | 216/266 [00:08<00:04, 11.62it/s]concatenating: train:  82%|████████▏ | 218/266 [00:08<00:03, 12.13it/s]concatenating: train:  83%|████████▎ | 220/266 [00:08<00:03, 12.61it/s]concatenating: train:  83%|████████▎ | 222/266 [00:08<00:03, 12.57it/s]concatenating: train:  84%|████████▍ | 224/266 [00:08<00:03, 11.76it/s]concatenating: train:  85%|████████▍ | 226/266 [00:09<00:03, 10.34it/s]concatenating: train:  86%|████████▌ | 228/266 [00:09<00:03, 10.43it/s]concatenating: train:  86%|████████▋ | 230/266 [00:09<00:03, 10.76it/s]concatenating: train:  87%|████████▋ | 232/266 [00:09<00:02, 11.70it/s]concatenating: train:  89%|████████▊ | 236/266 [00:09<00:02, 14.59it/s]concatenating: train:  90%|█████████ | 240/266 [00:09<00:01, 17.46it/s]concatenating: train:  92%|█████████▏| 244/266 [00:10<00:01, 18.67it/s]concatenating: train:  93%|█████████▎| 247/266 [00:10<00:01, 14.81it/s]concatenating: train:  94%|█████████▎| 249/266 [00:10<00:01, 12.19it/s]concatenating: train:  94%|█████████▍| 251/266 [00:10<00:01, 12.28it/s]concatenating: train:  98%|█████████▊| 261/266 [00:10<00:00, 16.65it/s]concatenating: train: 100%|██████████| 266/266 [00:10<00:00, 24.40it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:00<00:03,  1.12it/s]Loading test:  40%|████      | 2/5 [00:01<00:02,  1.08it/s]Loading test:  60%|██████    | 3/5 [00:03<00:02,  1.03s/it]Loading test:  80%|████████  | 4/5 [00:04<00:01,  1.01s/it]Loading test: 100%|██████████| 5/5 [00:05<00:00,  1.03it/s]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation:  60%|██████    | 3/5 [00:00<00:00, 25.03it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 26.08it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:00<04:24,  1.00it/s]Loading trainS:   1%|          | 2/266 [00:01<04:07,  1.07it/s]Loading trainS:   1%|          | 3/266 [00:02<04:14,  1.03it/s]Loading trainS:   2%|▏         | 4/266 [00:03<04:05,  1.07it/s]Loading trainS:   2%|▏         | 5/266 [00:04<04:32,  1.04s/it]Loading trainS:   2%|▏         | 6/266 [00:05<04:25,  1.02s/it]Loading trainS:   3%|▎         | 7/266 [00:06<04:25,  1.02s/it]Loading trainS:   3%|▎         | 8/266 [00:08<04:26,  1.03s/it]Loading trainS:   3%|▎         | 9/266 [00:09<04:31,  1.06s/it]Loading trainS:   4%|▍         | 10/266 [00:09<04:04,  1.05it/s]Loading trainS:   4%|▍         | 11/266 [00:10<03:32,  1.20it/s]Loading trainS:   5%|▍         | 12/266 [00:11<03:20,  1.26it/s]Loading trainS:   5%|▍         | 13/266 [00:11<03:21,  1.25it/s]Loading trainS:   5%|▌         | 14/266 [00:12<02:56,  1.43it/s]Loading trainS:   6%|▌         | 15/266 [00:12<02:45,  1.52it/s]Loading trainS:   6%|▌         | 16/266 [00:14<03:20,  1.25it/s]Loading trainS:   6%|▋         | 17/266 [00:15<03:46,  1.10it/s]Loading trainS:   7%|▋         | 18/266 [00:16<04:05,  1.01it/s]Loading trainS:   7%|▋         | 19/266 [00:17<04:07,  1.00s/it]Loading trainS:   8%|▊         | 20/266 [00:18<04:12,  1.03s/it]Loading trainS:   8%|▊         | 21/266 [00:19<04:05,  1.00s/it]Loading trainS:   8%|▊         | 22/266 [00:20<04:27,  1.09s/it]Loading trainS:   9%|▊         | 23/266 [00:22<04:34,  1.13s/it]Loading trainS:   9%|▉         | 24/266 [00:22<04:12,  1.05s/it]Loading trainS:   9%|▉         | 25/266 [00:23<04:03,  1.01s/it]Loading trainS:  10%|▉         | 26/266 [00:24<04:13,  1.05s/it]Loading trainS:  10%|█         | 27/266 [00:25<04:07,  1.04s/it]Loading trainS:  11%|█         | 28/266 [00:26<03:42,  1.07it/s]Loading trainS:  11%|█         | 29/266 [00:27<03:44,  1.06it/s]Loading trainS:  11%|█▏        | 30/266 [00:28<03:05,  1.27it/s]Loading trainS:  12%|█▏        | 31/266 [00:29<03:20,  1.17it/s]Loading trainS:  12%|█▏        | 32/266 [00:29<03:11,  1.22it/s]Loading trainS:  12%|█▏        | 33/266 [00:30<03:00,  1.29it/s]Loading trainS:  13%|█▎        | 34/266 [00:31<02:49,  1.37it/s]Loading trainS:  13%|█▎        | 35/266 [00:32<03:13,  1.19it/s]Loading trainS:  14%|█▎        | 36/266 [00:33<03:24,  1.13it/s]Loading trainS:  14%|█▍        | 37/266 [00:33<02:56,  1.30it/s]Loading trainS:  14%|█▍        | 38/266 [00:34<02:54,  1.31it/s]Loading trainS:  15%|█▍        | 39/266 [00:35<02:58,  1.27it/s]Loading trainS:  15%|█▌        | 40/266 [00:36<03:02,  1.24it/s]Loading trainS:  15%|█▌        | 41/266 [00:36<02:58,  1.26it/s]Loading trainS:  16%|█▌        | 42/266 [00:37<02:56,  1.27it/s]Loading trainS:  16%|█▌        | 43/266 [00:38<03:13,  1.15it/s]Loading trainS:  17%|█▋        | 44/266 [00:39<03:22,  1.09it/s]Loading trainS:  17%|█▋        | 45/266 [00:40<03:33,  1.03it/s]Loading trainS:  17%|█▋        | 46/266 [00:41<03:29,  1.05it/s]Loading trainS:  18%|█▊        | 47/266 [00:42<03:39,  1.00s/it]Loading trainS:  18%|█▊        | 48/266 [00:43<03:43,  1.03s/it]Loading trainS:  18%|█▊        | 49/266 [00:44<03:34,  1.01it/s]Loading trainS:  19%|█▉        | 50/266 [00:45<03:37,  1.01s/it]Loading trainS:  19%|█▉        | 51/266 [00:47<03:48,  1.06s/it]Loading trainS:  20%|█▉        | 52/266 [00:48<03:50,  1.08s/it]Loading trainS:  20%|█▉        | 53/266 [00:49<03:35,  1.01s/it]Loading trainS:  20%|██        | 54/266 [00:49<03:23,  1.04it/s]Loading trainS:  21%|██        | 55/266 [00:50<03:18,  1.07it/s]Loading trainS:  21%|██        | 56/266 [00:51<03:12,  1.09it/s]Loading trainS:  21%|██▏       | 57/266 [00:52<02:39,  1.31it/s]Loading trainS:  22%|██▏       | 58/266 [00:53<02:53,  1.20it/s]Loading trainS:  22%|██▏       | 59/266 [00:54<03:07,  1.10it/s]Loading trainS:  23%|██▎       | 60/266 [00:55<03:16,  1.05it/s]Loading trainS:  23%|██▎       | 61/266 [00:56<03:18,  1.03it/s]Loading trainS:  23%|██▎       | 62/266 [00:56<03:06,  1.09it/s]Loading trainS:  24%|██▎       | 63/266 [00:57<02:49,  1.20it/s]Loading trainS:  24%|██▍       | 64/266 [00:58<02:46,  1.22it/s]Loading trainS:  24%|██▍       | 65/266 [00:59<02:45,  1.21it/s]Loading trainS:  25%|██▍       | 66/266 [01:00<03:03,  1.09it/s]Loading trainS:  25%|██▌       | 67/266 [01:01<03:17,  1.01it/s]Loading trainS:  26%|██▌       | 68/266 [01:02<03:17,  1.00it/s]Loading trainS:  26%|██▌       | 69/266 [01:03<03:11,  1.03it/s]Loading trainS:  26%|██▋       | 70/266 [01:04<03:12,  1.02it/s]Loading trainS:  27%|██▋       | 71/266 [01:05<03:17,  1.01s/it]Loading trainS:  27%|██▋       | 72/266 [01:06<03:10,  1.02it/s]Loading trainS:  27%|██▋       | 73/266 [01:07<03:12,  1.00it/s]Loading trainS:  28%|██▊       | 74/266 [01:08<03:14,  1.01s/it]Loading trainS:  28%|██▊       | 75/266 [01:09<02:49,  1.13it/s]Loading trainS:  29%|██▊       | 76/266 [01:10<02:47,  1.13it/s]Loading trainS:  29%|██▉       | 77/266 [01:10<02:29,  1.27it/s]Loading trainS:  29%|██▉       | 78/266 [01:11<02:37,  1.19it/s]Loading trainS:  30%|██▉       | 79/266 [01:12<02:48,  1.11it/s]Loading trainS:  30%|███       | 80/266 [01:13<02:58,  1.04it/s]Loading trainS:  30%|███       | 81/266 [01:14<03:11,  1.04s/it]Loading trainS:  31%|███       | 82/266 [01:15<03:14,  1.06s/it]Loading trainS:  31%|███       | 83/266 [01:17<03:14,  1.06s/it]Loading trainS:  32%|███▏      | 84/266 [01:17<03:05,  1.02s/it]Loading trainS:  32%|███▏      | 85/266 [01:18<02:40,  1.13it/s]Loading trainS:  32%|███▏      | 86/266 [01:19<02:33,  1.17it/s]Loading trainS:  33%|███▎      | 87/266 [01:20<02:36,  1.15it/s]Loading trainS:  33%|███▎      | 88/266 [01:21<02:31,  1.17it/s]Loading trainS:  33%|███▎      | 89/266 [01:21<02:22,  1.24it/s]Loading trainS:  34%|███▍      | 90/266 [01:22<02:13,  1.31it/s]Loading trainS:  34%|███▍      | 91/266 [01:23<02:04,  1.41it/s]Loading trainS:  35%|███▍      | 92/266 [01:23<02:14,  1.29it/s]Loading trainS:  35%|███▍      | 93/266 [01:24<02:21,  1.22it/s]Loading trainS:  35%|███▌      | 94/266 [01:25<02:27,  1.16it/s]Loading trainS:  36%|███▌      | 95/266 [01:26<02:28,  1.15it/s]Loading trainS:  36%|███▌      | 96/266 [01:27<02:30,  1.13it/s]Loading trainS:  36%|███▋      | 97/266 [01:28<02:39,  1.06it/s]Loading trainS:  37%|███▋      | 98/266 [01:29<02:44,  1.02it/s]Loading trainS:  37%|███▋      | 99/266 [01:30<02:46,  1.00it/s]Loading trainS:  38%|███▊      | 100/266 [01:31<02:43,  1.02it/s]Loading trainS:  38%|███▊      | 101/266 [01:32<02:43,  1.01it/s]Loading trainS:  38%|███▊      | 102/266 [01:33<02:50,  1.04s/it]Loading trainS:  39%|███▊      | 103/266 [01:34<02:49,  1.04s/it]Loading trainS:  39%|███▉      | 104/266 [01:35<02:45,  1.02s/it]Loading trainS:  39%|███▉      | 105/266 [01:36<02:41,  1.00s/it]Loading trainS:  40%|███▉      | 106/266 [01:37<02:39,  1.00it/s]Loading trainS:  40%|████      | 107/266 [01:38<02:34,  1.03it/s]Loading trainS:  41%|████      | 108/266 [01:39<02:37,  1.00it/s]Loading trainS:  41%|████      | 109/266 [01:41<02:45,  1.06s/it]Loading trainS:  41%|████▏     | 110/266 [01:42<02:42,  1.04s/it]Loading trainS:  42%|████▏     | 111/266 [01:42<02:33,  1.01it/s]Loading trainS:  42%|████▏     | 112/266 [01:43<02:29,  1.03it/s]Loading trainS:  42%|████▏     | 113/266 [01:45<02:38,  1.03s/it]Loading trainS:  43%|████▎     | 114/266 [01:46<02:37,  1.04s/it]Loading trainS:  43%|████▎     | 115/266 [01:46<02:24,  1.05it/s]Loading trainS:  44%|████▎     | 116/266 [01:47<01:58,  1.27it/s]Loading trainS:  44%|████▍     | 117/266 [01:47<01:48,  1.37it/s]Loading trainS:  44%|████▍     | 118/266 [01:48<01:34,  1.57it/s]Loading trainS:  45%|████▍     | 119/266 [01:49<01:40,  1.46it/s]Loading trainS:  45%|████▌     | 120/266 [01:49<01:50,  1.32it/s]Loading trainS:  45%|████▌     | 121/266 [01:51<02:12,  1.09it/s]Loading trainS:  46%|████▌     | 122/266 [01:52<02:05,  1.15it/s]Loading trainS:  46%|████▌     | 123/266 [01:52<01:59,  1.19it/s]Loading trainS:  47%|████▋     | 124/266 [01:54<02:17,  1.03it/s]Loading trainS:  47%|████▋     | 125/266 [01:55<02:24,  1.03s/it]Loading trainS:  47%|████▋     | 126/266 [01:55<02:07,  1.10it/s]Loading trainS:  48%|████▊     | 127/266 [01:56<02:08,  1.08it/s]Loading trainS:  48%|████▊     | 128/266 [01:58<02:26,  1.06s/it]Loading trainS:  48%|████▊     | 129/266 [01:58<02:13,  1.02it/s]Loading trainS:  49%|████▉     | 130/266 [01:59<02:07,  1.07it/s]Loading trainS:  49%|████▉     | 131/266 [02:00<02:14,  1.00it/s]Loading trainS:  50%|████▉     | 132/266 [02:02<02:21,  1.06s/it]Loading trainS:  50%|█████     | 133/266 [02:03<02:26,  1.10s/it]Loading trainS:  50%|█████     | 134/266 [02:04<02:33,  1.16s/it]Loading trainS:  51%|█████     | 135/266 [02:05<02:28,  1.13s/it]Loading trainS:  51%|█████     | 136/266 [02:06<02:14,  1.04s/it]Loading trainS:  52%|█████▏    | 137/266 [02:07<02:07,  1.01it/s]Loading trainS:  52%|█████▏    | 138/266 [02:08<02:08,  1.00s/it]Loading trainS:  52%|█████▏    | 139/266 [02:09<02:12,  1.04s/it]Loading trainS:  53%|█████▎    | 140/266 [02:10<02:12,  1.05s/it]Loading trainS:  53%|█████▎    | 141/266 [02:11<02:14,  1.07s/it]Loading trainS:  53%|█████▎    | 142/266 [02:12<02:11,  1.06s/it]Loading trainS:  54%|█████▍    | 143/266 [02:13<01:58,  1.04it/s]Loading trainS:  54%|█████▍    | 144/266 [02:14<02:02,  1.01s/it]Loading trainS:  55%|█████▍    | 145/266 [02:15<02:00,  1.01it/s]Loading trainS:  55%|█████▍    | 146/266 [02:16<02:04,  1.04s/it]Loading trainS:  55%|█████▌    | 147/266 [02:17<01:58,  1.00it/s]Loading trainS:  56%|█████▌    | 148/266 [02:18<02:05,  1.06s/it]Loading trainS:  56%|█████▌    | 149/266 [02:19<01:50,  1.06it/s]Loading trainS:  56%|█████▋    | 150/266 [02:20<01:49,  1.06it/s]Loading trainS:  57%|█████▋    | 151/266 [02:21<01:41,  1.13it/s]Loading trainS:  57%|█████▋    | 152/266 [02:22<01:40,  1.13it/s]Loading trainS:  58%|█████▊    | 153/266 [02:23<01:49,  1.03it/s]Loading trainS:  58%|█████▊    | 154/266 [02:24<01:56,  1.04s/it]Loading trainS:  58%|█████▊    | 155/266 [02:25<01:57,  1.06s/it]Loading trainS:  59%|█████▊    | 156/266 [02:26<01:40,  1.10it/s]Loading trainS:  59%|█████▉    | 157/266 [02:27<01:43,  1.06it/s]Loading trainS:  59%|█████▉    | 158/266 [02:28<01:44,  1.04it/s]Loading trainS:  60%|█████▉    | 159/266 [02:29<01:48,  1.01s/it]Loading trainS:  60%|██████    | 160/266 [02:30<01:42,  1.04it/s]Loading trainS:  61%|██████    | 161/266 [02:30<01:32,  1.14it/s]Loading trainS:  61%|██████    | 162/266 [02:31<01:27,  1.19it/s]Loading trainS:  61%|██████▏   | 163/266 [02:32<01:20,  1.28it/s]Loading trainS:  62%|██████▏   | 164/266 [02:32<01:12,  1.40it/s]Loading trainS:  62%|██████▏   | 165/266 [02:33<01:15,  1.34it/s]Loading trainS:  62%|██████▏   | 166/266 [02:34<01:27,  1.14it/s]Loading trainS:  63%|██████▎   | 167/266 [02:35<01:17,  1.28it/s]Loading trainS:  63%|██████▎   | 168/266 [02:36<01:12,  1.35it/s]Loading trainS:  64%|██████▎   | 169/266 [02:37<01:20,  1.21it/s]Loading trainS:  64%|██████▍   | 170/266 [02:38<01:26,  1.11it/s]Loading trainS:  64%|██████▍   | 171/266 [02:38<01:19,  1.20it/s]Loading trainS:  65%|██████▍   | 172/266 [02:39<01:18,  1.20it/s]Loading trainS:  65%|██████▌   | 173/266 [02:40<01:27,  1.06it/s]Loading trainS:  65%|██████▌   | 174/266 [02:41<01:28,  1.04it/s]Loading trainS:  66%|██████▌   | 175/266 [02:42<01:30,  1.00it/s]Loading trainS:  66%|██████▌   | 176/266 [02:44<01:35,  1.06s/it]Loading trainS:  67%|██████▋   | 177/266 [02:45<01:34,  1.07s/it]Loading trainS:  67%|██████▋   | 178/266 [02:46<01:28,  1.01s/it]Loading trainS:  67%|██████▋   | 179/266 [02:47<01:28,  1.02s/it]Loading trainS:  68%|██████▊   | 180/266 [02:48<01:27,  1.01s/it]Loading trainS:  68%|██████▊   | 181/266 [02:49<01:25,  1.01s/it]Loading trainS:  68%|██████▊   | 182/266 [02:50<01:30,  1.08s/it]Loading trainS:  69%|██████▉   | 183/266 [02:51<01:24,  1.02s/it]Loading trainS:  69%|██████▉   | 184/266 [02:52<01:26,  1.05s/it]Loading trainS:  70%|██████▉   | 185/266 [02:53<01:19,  1.02it/s]Loading trainS:  70%|██████▉   | 186/266 [02:54<01:19,  1.01it/s]Loading trainS:  70%|███████   | 187/266 [02:55<01:21,  1.03s/it]Loading trainS:  71%|███████   | 188/266 [02:56<01:22,  1.06s/it]Loading trainS:  71%|███████   | 189/266 [02:57<01:21,  1.06s/it]Loading trainS:  71%|███████▏  | 190/266 [02:58<01:20,  1.06s/it]Loading trainS:  72%|███████▏  | 191/266 [02:59<01:19,  1.05s/it]Loading trainS:  72%|███████▏  | 192/266 [03:00<01:13,  1.00it/s]Loading trainS:  73%|███████▎  | 193/266 [03:01<01:12,  1.00it/s]Loading trainS:  73%|███████▎  | 194/266 [03:02<01:15,  1.05s/it]Loading trainS:  73%|███████▎  | 195/266 [03:03<01:13,  1.04s/it]Loading trainS:  74%|███████▎  | 196/266 [03:04<01:13,  1.05s/it]Loading trainS:  74%|███████▍  | 197/266 [03:05<01:11,  1.04s/it]Loading trainS:  74%|███████▍  | 198/266 [03:06<01:10,  1.04s/it]Loading trainS:  75%|███████▍  | 199/266 [03:08<01:15,  1.12s/it]Loading trainS:  75%|███████▌  | 200/266 [03:09<01:13,  1.12s/it]Loading trainS:  76%|███████▌  | 201/266 [03:10<01:12,  1.12s/it]Loading trainS:  76%|███████▌  | 202/266 [03:11<01:13,  1.16s/it]Loading trainS:  76%|███████▋  | 203/266 [03:12<01:10,  1.11s/it]Loading trainS:  77%|███████▋  | 204/266 [03:13<01:09,  1.12s/it]Loading trainS:  77%|███████▋  | 205/266 [03:14<01:00,  1.01it/s]Loading trainS:  77%|███████▋  | 206/266 [03:15<00:59,  1.01it/s]Loading trainS:  78%|███████▊  | 207/266 [03:16<00:53,  1.11it/s]Loading trainS:  78%|███████▊  | 208/266 [03:17<00:52,  1.10it/s]Loading trainS:  79%|███████▊  | 209/266 [03:18<00:53,  1.07it/s]Loading trainS:  79%|███████▉  | 210/266 [03:18<00:50,  1.10it/s]Loading trainS:  79%|███████▉  | 211/266 [03:19<00:46,  1.18it/s]Loading trainS:  80%|███████▉  | 212/266 [03:20<00:42,  1.26it/s]Loading trainS:  80%|████████  | 213/266 [03:20<00:39,  1.36it/s]Loading trainS:  80%|████████  | 214/266 [03:21<00:42,  1.22it/s]Loading trainS:  81%|████████  | 215/266 [03:22<00:41,  1.23it/s]Loading trainS:  81%|████████  | 216/266 [03:23<00:38,  1.30it/s]Loading trainS:  82%|████████▏ | 217/266 [03:24<00:38,  1.28it/s]Loading trainS:  82%|████████▏ | 218/266 [03:24<00:35,  1.35it/s]Loading trainS:  82%|████████▏ | 219/266 [03:25<00:33,  1.42it/s]Loading trainS:  83%|████████▎ | 220/266 [03:25<00:28,  1.62it/s]Loading trainS:  83%|████████▎ | 221/266 [03:26<00:29,  1.54it/s]Loading trainS:  83%|████████▎ | 222/266 [03:26<00:25,  1.75it/s]Loading trainS:  84%|████████▍ | 223/266 [03:27<00:27,  1.56it/s]Loading trainS:  84%|████████▍ | 224/266 [03:28<00:28,  1.49it/s]Loading trainS:  85%|████████▍ | 225/266 [03:29<00:29,  1.39it/s]Loading trainS:  85%|████████▍ | 226/266 [03:30<00:29,  1.36it/s]Loading trainS:  85%|████████▌ | 227/266 [03:30<00:30,  1.28it/s]Loading trainS:  86%|████████▌ | 228/266 [03:31<00:29,  1.30it/s]Loading trainS:  86%|████████▌ | 229/266 [03:32<00:28,  1.31it/s]Loading trainS:  86%|████████▋ | 230/266 [03:33<00:27,  1.31it/s]Loading trainS:  87%|████████▋ | 231/266 [03:33<00:25,  1.37it/s]Loading trainS:  87%|████████▋ | 232/266 [03:34<00:24,  1.38it/s]Loading trainS:  88%|████████▊ | 233/266 [03:35<00:25,  1.31it/s]Loading trainS:  88%|████████▊ | 234/266 [03:36<00:22,  1.42it/s]Loading trainS:  88%|████████▊ | 235/266 [03:36<00:23,  1.31it/s]Loading trainS:  89%|████████▊ | 236/266 [03:37<00:22,  1.35it/s]Loading trainS:  89%|████████▉ | 237/266 [03:38<00:20,  1.43it/s]Loading trainS:  89%|████████▉ | 238/266 [03:38<00:19,  1.47it/s]Loading trainS:  90%|████████▉ | 239/266 [03:39<00:18,  1.43it/s]Loading trainS:  90%|█████████ | 240/266 [03:40<00:17,  1.48it/s]Loading trainS:  91%|█████████ | 241/266 [03:40<00:17,  1.45it/s]Loading trainS:  91%|█████████ | 242/266 [03:41<00:17,  1.37it/s]Loading trainS:  91%|█████████▏| 243/266 [03:42<00:17,  1.32it/s]Loading trainS:  92%|█████████▏| 244/266 [03:43<00:16,  1.32it/s]Loading trainS:  92%|█████████▏| 245/266 [03:44<00:15,  1.37it/s]Loading trainS:  92%|█████████▏| 246/266 [03:44<00:14,  1.38it/s]Loading trainS:  93%|█████████▎| 247/266 [03:45<00:13,  1.40it/s]Loading trainS:  93%|█████████▎| 248/266 [03:46<00:12,  1.41it/s]Loading trainS:  94%|█████████▎| 249/266 [03:46<00:12,  1.34it/s]Loading trainS:  94%|█████████▍| 250/266 [03:47<00:10,  1.53it/s]Loading trainS:  94%|█████████▍| 251/266 [03:47<00:08,  1.71it/s]Loading trainS:  95%|█████████▍| 252/266 [03:48<00:07,  1.79it/s]Loading trainS:  95%|█████████▌| 253/266 [03:48<00:07,  1.83it/s]Loading trainS:  95%|█████████▌| 254/266 [03:49<00:06,  1.93it/s]Loading trainS:  96%|█████████▌| 255/266 [03:49<00:05,  1.99it/s]Loading trainS:  96%|█████████▌| 256/266 [03:50<00:05,  1.97it/s]Loading trainS:  97%|█████████▋| 257/266 [03:50<00:04,  1.95it/s]Loading trainS:  97%|█████████▋| 258/266 [03:51<00:03,  2.01it/s]Loading trainS:  97%|█████████▋| 259/266 [03:51<00:03,  2.07it/s]Loading trainS:  98%|█████████▊| 260/266 [03:52<00:02,  2.13it/s]Loading trainS:  98%|█████████▊| 261/266 [03:52<00:02,  2.11it/s]Loading trainS:  98%|█████████▊| 262/266 [03:53<00:01,  2.01it/s]Loading trainS:  99%|█████████▉| 263/266 [03:53<00:01,  2.04it/s]Loading trainS:  99%|█████████▉| 264/266 [03:54<00:00,  2.06it/s]Loading trainS: 100%|█████████▉| 265/266 [03:54<00:00,  2.06it/s]Loading trainS: 100%|██████████| 266/266 [03:55<00:00,  2.07it/s]
Loading testS:   0%|          | 0/5 [00:00<?, ?it/s]Loading testS:  20%|██        | 1/5 [00:00<00:01,  2.28it/s]Loading testS:  40%|████      | 2/5 [00:00<00:01,  2.33it/s]Loading testS:  60%|██████    | 3/5 [00:01<00:00,  2.29it/s]Loading testS:  80%|████████  | 4/5 [00:01<00:00,  2.26it/s]Loading testS: 100%|██████████| 5/5 [00:02<00:00,  2.22it/s]
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:06,  6.29it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:05,  7.50it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:05,  7.15it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:03,  9.19it/s]loading the weights for Res Unet:  23%|██▎       | 10/44 [00:00<00:03, 10.06it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:03,  8.64it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:02, 11.07it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02, 11.42it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:01<00:02,  9.18it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:01<00:01, 11.41it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01, 11.91it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01, 12.07it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:02<00:01,  9.57it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:02<00:00, 11.83it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:02<00:00, 12.21it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:02<00:00, 12.31it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:03<00:00,  9.61it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:03<00:00, 13.51it/s]---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 3  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 3  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 22,982
Non-trainable params: 200,180
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 320 samples
Epoch 1/300
 - 34s - loss: 0.5530 - acc: 0.9098 - mDice: 0.4595 - val_loss: 0.8921 - val_acc: 0.9842 - val_mDice: 0.6273

Epoch 00001: val_mDice improved from -inf to 0.62728, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 30s - loss: 0.1810 - acc: 0.9829 - mDice: 0.7077 - val_loss: 0.8642 - val_acc: 0.9851 - val_mDice: 0.6693

Epoch 00002: val_mDice improved from 0.62728 to 0.66927, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 30s - loss: 0.1526 - acc: 0.9852 - mDice: 0.7455 - val_loss: 0.8311 - val_acc: 0.9873 - val_mDice: 0.7011

Epoch 00003: val_mDice improved from 0.66927 to 0.70114, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 30s - loss: 0.1354 - acc: 0.9866 - mDice: 0.7699 - val_loss: 0.8275 - val_acc: 0.9878 - val_mDice: 0.7046

Epoch 00004: val_mDice improved from 0.70114 to 0.70464, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 30s - loss: 0.1251 - acc: 0.9874 - mDice: 0.7850 - val_loss: 0.8242 - val_acc: 0.9882 - val_mDice: 0.7164

Epoch 00005: val_mDice improved from 0.70464 to 0.71640, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 30s - loss: 0.1184 - acc: 0.9881 - mDice: 0.7953 - val_loss: 0.8080 - val_acc: 0.9891 - val_mDice: 0.7252

Epoch 00006: val_mDice improved from 0.71640 to 0.72519, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 30s - loss: 0.1141 - acc: 0.9884 - mDice: 0.8019 - val_loss: 0.7978 - val_acc: 0.9893 - val_mDice: 0.7341

Epoch 00007: val_mDice improved from 0.72519 to 0.73408, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 30s - loss: 0.1095 - acc: 0.9888 - mDice: 0.8089 - val_loss: 0.7943 - val_acc: 0.9896 - val_mDice: 0.7385

Epoch 00008: val_mDice improved from 0.73408 to 0.73851, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 31s - loss: 0.1071 - acc: 0.9890 - mDice: 0.8127 - val_loss: 0.7667 - val_acc: 0.9898 - val_mDice: 0.7284

Epoch 00009: val_mDice did not improve from 0.73851
Epoch 10/300
 - 31s - loss: 0.1059 - acc: 0.9891 - mDice: 0.8146 - val_loss: 0.8221 - val_acc: 0.9890 - val_mDice: 0.7439

Epoch 00010: val_mDice improved from 0.73851 to 0.74390, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 31s - loss: 0.1035 - acc: 0.9893 - mDice: 0.8184 - val_loss: 0.7773 - val_acc: 0.9900 - val_mDice: 0.7402

Epoch 00011: val_mDice did not improve from 0.74390
Epoch 12/300
 - 31s - loss: 0.1018 - acc: 0.9895 - mDice: 0.8211 - val_loss: 0.7640 - val_acc: 0.9901 - val_mDice: 0.7382

Epoch 00012: val_mDice did not improve from 0.74390
Epoch 13/300
 - 31s - loss: 0.0995 - acc: 0.9897 - mDice: 0.8247 - val_loss: 0.7959 - val_acc: 0.9902 - val_mDice: 0.7599

Epoch 00013: val_mDice improved from 0.74390 to 0.75988, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 31s - loss: 0.1002 - acc: 0.9896 - mDice: 0.8236 - val_loss: 0.7883 - val_acc: 0.9902 - val_mDice: 0.7555

Epoch 00014: val_mDice did not improve from 0.75988
Epoch 15/300
 - 31s - loss: 0.0981 - acc: 0.9898 - mDice: 0.8268 - val_loss: 0.7953 - val_acc: 0.9903 - val_mDice: 0.7569

Epoch 00015: val_mDice did not improve from 0.75988
Epoch 16/300
 - 31s - loss: 0.0973 - acc: 0.9899 - mDice: 0.8280 - val_loss: 0.7564 - val_acc: 0.9902 - val_mDice: 0.7411

Epoch 00016: val_mDice did not improve from 0.75988
Epoch 17/300
 - 31s - loss: 0.0957 - acc: 0.9900 - mDice: 0.8308 - val_loss: 0.7826 - val_acc: 0.9903 - val_mDice: 0.7519

Epoch 00017: val_mDice did not improve from 0.75988
Epoch 18/300
 - 31s - loss: 0.0948 - acc: 0.9901 - mDice: 0.8321 - val_loss: 0.7656 - val_acc: 0.9906 - val_mDice: 0.7562

Epoch 00018: val_mDice did not improve from 0.75988
Epoch 19/300
 - 31s - loss: 0.0942 - acc: 0.9901 - mDice: 0.8330 - val_loss: 0.7777 - val_acc: 0.9907 - val_mDice: 0.7677

Epoch 00019: val_mDice improved from 0.75988 to 0.76768, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 31s - loss: 0.0931 - acc: 0.9902 - mDice: 0.8348 - val_loss: 0.7784 - val_acc: 0.9906 - val_mDice: 0.7562

Epoch 00020: val_mDice did not improve from 0.76768
Epoch 21/300
 - 31s - loss: 0.0931 - acc: 0.9902 - mDice: 0.8348 - val_loss: 0.7409 - val_acc: 0.9908 - val_mDice: 0.7603

Epoch 00021: val_mDice did not improve from 0.76768
Epoch 22/300
 - 31s - loss: 0.0929 - acc: 0.9903 - mDice: 0.8352 - val_loss: 0.7765 - val_acc: 0.9907 - val_mDice: 0.7663

Epoch 00022: val_mDice did not improve from 0.76768
Epoch 23/300
 - 31s - loss: 0.0922 - acc: 0.9903 - mDice: 0.8362 - val_loss: 0.7623 - val_acc: 0.9909 - val_mDice: 0.7665

Epoch 00023: val_mDice did not improve from 0.76768
Epoch 24/300
 - 31s - loss: 0.0918 - acc: 0.9904 - mDice: 0.8370 - val_loss: 0.7971 - val_acc: 0.9902 - val_mDice: 0.7606

Epoch 00024: val_mDice did not improve from 0.76768
Epoch 25/300
 - 31s - loss: 0.0913 - acc: 0.9904 - mDice: 0.8378 - val_loss: 0.7724 - val_acc: 0.9909 - val_mDice: 0.7694

Epoch 00025: val_mDice improved from 0.76768 to 0.76935, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 31s - loss: 0.0895 - acc: 0.9905 - mDice: 0.8406 - val_loss: 0.7532 - val_acc: 0.9908 - val_mDice: 0.7618

Epoch 00026: val_mDice did not improve from 0.76935
Epoch 27/300
 - 31s - loss: 0.0904 - acc: 0.9905 - mDice: 0.8391 - val_loss: 0.7893 - val_acc: 0.9903 - val_mDice: 0.7578

Epoch 00027: val_mDice did not improve from 0.76935
Epoch 28/300
 - 31s - loss: 0.0895 - acc: 0.9906 - mDice: 0.8406 - val_loss: 0.7741 - val_acc: 0.9907 - val_mDice: 0.7655

Epoch 00028: val_mDice did not improve from 0.76935
Epoch 29/300
 - 31s - loss: 0.0894 - acc: 0.9906 - mDice: 0.8408 - val_loss: 0.7536 - val_acc: 0.9910 - val_mDice: 0.7671

Epoch 00029: val_mDice did not improve from 0.76935
Epoch 30/300
 - 31s - loss: 0.0880 - acc: 0.9907 - mDice: 0.8430 - val_loss: 0.7908 - val_acc: 0.9906 - val_mDice: 0.7684

Epoch 00030: val_mDice did not improve from 0.76935
Epoch 31/300
 - 31s - loss: 0.0881 - acc: 0.9907 - mDice: 0.8430 - val_loss: 0.7796 - val_acc: 0.9906 - val_mDice: 0.7645

Epoch 00031: val_mDice did not improve from 0.76935
Epoch 32/300
 - 31s - loss: 0.0879 - acc: 0.9907 - mDice: 0.8433 - val_loss: 0.7626 - val_acc: 0.9910 - val_mDice: 0.7709

Epoch 00032: val_mDice improved from 0.76935 to 0.77095, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 31s - loss: 0.0877 - acc: 0.9907 - mDice: 0.8435 - val_loss: 0.7169 - val_acc: 0.9908 - val_mDice: 0.7517

Epoch 00033: val_mDice did not improve from 0.77095
Epoch 34/300
 - 31s - loss: 0.0873 - acc: 0.9907 - mDice: 0.8441 - val_loss: 0.7442 - val_acc: 0.9909 - val_mDice: 0.7555

Epoch 00034: val_mDice did not improve from 0.77095
Epoch 35/300
 - 31s - loss: 0.0877 - acc: 0.9907 - mDice: 0.8435 - val_loss: 0.7524 - val_acc: 0.9910 - val_mDice: 0.7653

Epoch 00035: val_mDice did not improve from 0.77095
Epoch 36/300
 - 31s - loss: 0.0863 - acc: 0.9908 - mDice: 0.8458 - val_loss: 0.7502 - val_acc: 0.9911 - val_mDice: 0.7669

Epoch 00036: val_mDice did not improve from 0.77095
Epoch 37/300
 - 30s - loss: 0.0869 - acc: 0.9908 - mDice: 0.8448 - val_loss: 0.7352 - val_acc: 0.9909 - val_mDice: 0.7584

Epoch 00037: val_mDice did not improve from 0.77095
Epoch 38/300
 - 31s - loss: 0.0855 - acc: 0.9909 - mDice: 0.8471 - val_loss: 0.7383 - val_acc: 0.9911 - val_mDice: 0.7595

Epoch 00038: val_mDice did not improve from 0.77095
Epoch 39/300
 - 31s - loss: 0.0857 - acc: 0.9909 - mDice: 0.8469 - val_loss: 0.7471 - val_acc: 0.9908 - val_mDice: 0.7513

Epoch 00039: val_mDice did not improve from 0.77095
Epoch 40/300
 - 31s - loss: 0.0851 - acc: 0.9909 - mDice: 0.8479 - val_loss: 0.7654 - val_acc: 0.9910 - val_mDice: 0.7686

Epoch 00040: val_mDice did not improve from 0.77095
Epoch 41/300
 - 31s - loss: 0.0848 - acc: 0.9910 - mDice: 0.8483 - val_loss: 0.7546 - val_acc: 0.9912 - val_mDice: 0.7709

Epoch 00041: val_mDice did not improve from 0.77095
Epoch 42/300
 - 31s - loss: 0.0845 - acc: 0.9910 - mDice: 0.8489 - val_loss: 0.7762 - val_acc: 0.9907 - val_mDice: 0.7685

Epoch 00042: val_mDice did not improve from 0.77095
Epoch 43/300
 - 31s - loss: 0.0848 - acc: 0.9910 - mDice: 0.8484 - val_loss: 0.7375 - val_acc: 0.9911 - val_mDice: 0.7612

Epoch 00043: val_mDice did not improve from 0.77095
Epoch 44/300
 - 31s - loss: 0.0839 - acc: 0.9911 - mDice: 0.8498 - val_loss: 0.7527 - val_acc: 0.9913 - val_mDice: 0.7696

Epoch 00044: val_mDice did not improve from 0.77095
Epoch 45/300
 - 31s - loss: 0.0837 - acc: 0.9911 - mDice: 0.8501 - val_loss: 0.7320 - val_acc: 0.9910 - val_mDice: 0.7507

Epoch 00045: val_mDice did not improve from 0.77095
Epoch 46/300
 - 31s - loss: 0.0841 - acc: 0.9911 - mDice: 0.8495 - val_loss: 0.7520 - val_acc: 0.9911 - val_mDice: 0.7643

Epoch 00046: val_mDice did not improve from 0.77095
Epoch 47/300
 - 31s - loss: 0.0834 - acc: 0.9911 - mDice: 0.8507 - val_loss: 0.7574 - val_acc: 0.9911 - val_mDice: 0.7712

Epoch 00047: val_mDice improved from 0.77095 to 0.77117, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 31s - loss: 0.0828 - acc: 0.9911 - mDice: 0.8516 - val_loss: 0.7446 - val_acc: 0.9913 - val_mDice: 0.7693

Epoch 00048: val_mDice did not improve from 0.77117
Epoch 49/300
 - 31s - loss: 0.0834 - acc: 0.9911 - mDice: 0.8506 - val_loss: 0.7633 - val_acc: 0.9910 - val_mDice: 0.7651

Epoch 00049: val_mDice did not improve from 0.77117
Epoch 50/300
 - 31s - loss: 0.0826 - acc: 0.9912 - mDice: 0.8520 - val_loss: 0.7442 - val_acc: 0.9912 - val_mDice: 0.7684

Epoch 00050: val_mDice did not improve from 0.77117
Epoch 51/300
 - 31s - loss: 0.0821 - acc: 0.9912 - mDice: 0.8528 - val_loss: 0.7571 - val_acc: 0.9911 - val_mDice: 0.7709

Epoch 00051: val_mDice did not improve from 0.77117
Epoch 52/300
 - 31s - loss: 0.0819 - acc: 0.9913 - mDice: 0.8532 - val_loss: 0.7360 - val_acc: 0.9909 - val_mDice: 0.7593

Epoch 00052: val_mDice did not improve from 0.77117
Epoch 53/300
 - 31s - loss: 0.0819 - acc: 0.9912 - mDice: 0.8531 - val_loss: 0.7593 - val_acc: 0.9912 - val_mDice: 0.7748

Epoch 00053: val_mDice improved from 0.77117 to 0.77482, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 31s - loss: 0.0810 - acc: 0.9913 - mDice: 0.8545 - val_loss: 0.7717 - val_acc: 0.9911 - val_mDice: 0.7699

Epoch 00054: val_mDice did not improve from 0.77482
Epoch 55/300
 - 31s - loss: 0.0819 - acc: 0.9913 - mDice: 0.8532 - val_loss: 0.7424 - val_acc: 0.9914 - val_mDice: 0.7745

Epoch 00055: val_mDice did not improve from 0.77482
Epoch 56/300
 - 31s - loss: 0.0815 - acc: 0.9913 - mDice: 0.8539 - val_loss: 0.7630 - val_acc: 0.9911 - val_mDice: 0.7658

Epoch 00056: val_mDice did not improve from 0.77482
Epoch 57/300
 - 31s - loss: 0.0806 - acc: 0.9914 - mDice: 0.8552 - val_loss: 0.7488 - val_acc: 0.9912 - val_mDice: 0.7678

Epoch 00057: val_mDice did not improve from 0.77482
Epoch 58/300
 - 31s - loss: 0.0805 - acc: 0.9914 - mDice: 0.8555 - val_loss: 0.7496 - val_acc: 0.9913 - val_mDice: 0.7682

Epoch 00058: val_mDice did not improve from 0.77482
Epoch 59/300
 - 31s - loss: 0.0808 - acc: 0.9913 - mDice: 0.8550 - val_loss: 0.7820 - val_acc: 0.9910 - val_mDice: 0.7716

Epoch 00059: val_mDice did not improve from 0.77482
Epoch 60/300
 - 31s - loss: 0.0802 - acc: 0.9914 - mDice: 0.8559 - val_loss: 0.7617 - val_acc: 0.9911 - val_mDice: 0.7632

Epoch 00060: val_mDice did not improve from 0.77482
Epoch 61/300
 - 31s - loss: 0.0803 - acc: 0.9914 - mDice: 0.8558 - val_loss: 0.7454 - val_acc: 0.9913 - val_mDice: 0.7720

Epoch 00061: val_mDice did not improve from 0.77482
Epoch 62/300
 - 31s - loss: 0.0802 - acc: 0.9914 - mDice: 0.8559 - val_loss: 0.7567 - val_acc: 0.9912 - val_mDice: 0.7698

Epoch 00062: val_mDice did not improve from 0.77482
Epoch 63/300
 - 31s - loss: 0.0797 - acc: 0.9914 - mDice: 0.8567 - val_loss: 0.7532 - val_acc: 0.9915 - val_mDice: 0.7739

Epoch 00063: val_mDice did not improve from 0.77482
Epoch 64/300
 - 31s - loss: 0.0795 - acc: 0.9915 - mDice: 0.8570 - val_loss: 0.7495 - val_acc: 0.9912 - val_mDice: 0.7606

Epoch 00064: val_mDice did not improve from 0.77482
Epoch 65/300
 - 31s - loss: 0.0788 - acc: 0.9915 - mDice: 0.8583 - val_loss: 0.7641 - val_acc: 0.9914 - val_mDice: 0.7750

Epoch 00065: val_mDice improved from 0.77482 to 0.77501, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 66/300
 - 31s - loss: 0.0792 - acc: 0.9915 - mDice: 0.8576 - val_loss: 0.7737 - val_acc: 0.9911 - val_mDice: 0.7747

Epoch 00066: val_mDice did not improve from 0.77501
Epoch 67/300
 - 31s - loss: 0.0788 - acc: 0.9915 - mDice: 0.8582 - val_loss: 0.7258 - val_acc: 0.9912 - val_mDice: 0.7588

Epoch 00067: val_mDice did not improve from 0.77501
Epoch 68/300
 - 31s - loss: 0.0790 - acc: 0.9915 - mDice: 0.8580 - val_loss: 0.7831 - val_acc: 0.9908 - val_mDice: 0.7676

Epoch 00068: val_mDice did not improve from 0.77501
Epoch 69/300
 - 31s - loss: 0.0789 - acc: 0.9915 - mDice: 0.8582 - val_loss: 0.7529 - val_acc: 0.9913 - val_mDice: 0.7662

Epoch 00069: val_mDice did not improve from 0.77501
Epoch 70/300
 - 31s - loss: 0.0785 - acc: 0.9915 - mDice: 0.8588 - val_loss: 0.7401 - val_acc: 0.9914 - val_mDice: 0.7719

Epoch 00070: val_mDice did not improve from 0.77501
Epoch 71/300
 - 31s - loss: 0.0788 - acc: 0.9915 - mDice: 0.8583 - val_loss: 0.7314 - val_acc: 0.9912 - val_mDice: 0.7605

Epoch 00071: val_mDice did not improve from 0.77501
Epoch 72/300
 - 31s - loss: 0.0781 - acc: 0.9916 - mDice: 0.8594 - val_loss: 0.7451 - val_acc: 0.9912 - val_mDice: 0.7687

Epoch 00072: val_mDice did not improve from 0.77501
Epoch 73/300
 - 31s - loss: 0.0777 - acc: 0.9916 - mDice: 0.8601 - val_loss: 0.7559 - val_acc: 0.9914 - val_mDice: 0.7774

Epoch 00073: val_mDice improved from 0.77501 to 0.77743, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 31s - loss: 0.0781 - acc: 0.9916 - mDice: 0.8595 - val_loss: 0.7397 - val_acc: 0.9913 - val_mDice: 0.7635

Epoch 00074: val_mDice did not improve from 0.77743
Epoch 75/300
 - 31s - loss: 0.0776 - acc: 0.9916 - mDice: 0.8603 - val_loss: 0.7296 - val_acc: 0.9911 - val_mDice: 0.7555

Epoch 00075: val_mDice did not improve from 0.77743
Epoch 76/300
 - 31s - loss: 0.0782 - acc: 0.9916 - mDice: 0.8592 - val_loss: 0.7701 - val_acc: 0.9912 - val_mDice: 0.7725

Epoch 00076: val_mDice did not improve from 0.77743
Epoch 77/300
 - 31s - loss: 0.0776 - acc: 0.9916 - mDice: 0.8602 - val_loss: 0.7481 - val_acc: 0.9911 - val_mDice: 0.7616

Epoch 00077: val_mDice did not improve from 0.77743
Epoch 78/300
 - 31s - loss: 0.0781 - acc: 0.9916 - mDice: 0.8595 - val_loss: 0.7950 - val_acc: 0.9905 - val_mDice: 0.7586

Epoch 00078: val_mDice did not improve from 0.77743
Epoch 79/300
 - 31s - loss: 0.0777 - acc: 0.9916 - mDice: 0.8601 - val_loss: 0.7437 - val_acc: 0.9912 - val_mDice: 0.7653

Epoch 00079: val_mDice did not improve from 0.77743
Epoch 80/300
 - 31s - loss: 0.0774 - acc: 0.9916 - mDice: 0.8606 - val_loss: 0.7538 - val_acc: 0.9914 - val_mDice: 0.7731

Epoch 00080: val_mDice did not improve from 0.77743
Epoch 81/300
 - 31s - loss: 0.0783 - acc: 0.9915 - mDice: 0.8592 - val_loss: 0.7517 - val_acc: 0.9913 - val_mDice: 0.7727

Epoch 00081: val_mDice did not improve from 0.77743
Epoch 82/300
 - 31s - loss: 0.0776 - acc: 0.9916 - mDice: 0.8602 - val_loss: 0.7710 - val_acc: 0.9913 - val_mDice: 0.7788

Epoch 00082: val_mDice improved from 0.77743 to 0.77879, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 83/300
 - 31s - loss: 0.0769 - acc: 0.9917 - mDice: 0.8615 - val_loss: 0.7642 - val_acc: 0.9910 - val_mDice: 0.7650

Epoch 00083: val_mDice did not improve from 0.77879
Epoch 84/300
 - 31s - loss: 0.0765 - acc: 0.9917 - mDice: 0.8621 - val_loss: 0.7669 - val_acc: 0.9913 - val_mDice: 0.7768

Epoch 00084: val_mDice did not improve from 0.77879
Epoch 85/300
 - 31s - loss: 0.0773 - acc: 0.9917 - mDice: 0.8608 - val_loss: 0.7564 - val_acc: 0.9912 - val_mDice: 0.7684

Epoch 00085: val_mDice did not improve from 0.77879
Epoch 86/300
 - 31s - loss: 0.0764 - acc: 0.9918 - mDice: 0.8623 - val_loss: 0.7448 - val_acc: 0.9914 - val_mDice: 0.7705

Epoch 00086: val_mDice did not improve from 0.77879
Epoch 87/300
 - 31s - loss: 0.0766 - acc: 0.9917 - mDice: 0.8620 - val_loss: 0.7780 - val_acc: 0.9911 - val_mDice: 0.7691

Epoch 00087: val_mDice did not improve from 0.77879
Epoch 88/300
 - 31s - loss: 0.0766 - acc: 0.9918 - mDice: 0.8621 - val_loss: 0.7517 - val_acc: 0.9914 - val_mDice: 0.7740

Epoch 00088: val_mDice did not improve from 0.77879
Epoch 89/300
 - 31s - loss: 0.0764 - acc: 0.9917 - mDice: 0.8623 - val_loss: 0.7560 - val_acc: 0.9912 - val_mDice: 0.7685

Epoch 00089: val_mDice did not improve from 0.77879
Epoch 90/300
 - 31s - loss: 0.0761 - acc: 0.9918 - mDice: 0.8628 - val_loss: 0.7534 - val_acc: 0.9915 - val_mDice: 0.7766

Epoch 00090: val_mDice did not improve from 0.77879
Epoch 91/300
 - 31s - loss: 0.0760 - acc: 0.9918 - mDice: 0.8630 - val_loss: 0.7655 - val_acc: 0.9913 - val_mDice: 0.7766

Epoch 00091: val_mDice did not improve from 0.77879
Epoch 92/300
 - 31s - loss: 0.0759 - acc: 0.9918 - mDice: 0.8631 - val_loss: 0.7622 - val_acc: 0.9913 - val_mDice: 0.7694

Epoch 00092: val_mDice did not improve from 0.77879
Epoch 93/300
 - 31s - loss: 0.0762 - acc: 0.9918 - mDice: 0.8626 - val_loss: 0.7535 - val_acc: 0.9914 - val_mDice: 0.7768

Epoch 00093: val_mDice did not improve from 0.77879
Epoch 94/300
 - 31s - loss: 0.0762 - acc: 0.9918 - mDice: 0.8627 - val_loss: 0.7892 - val_acc: 0.9908 - val_mDice: 0.7691

Epoch 00094: val_mDice did not improve from 0.77879
Epoch 95/300
 - 31s - loss: 0.0765 - acc: 0.9917 - mDice: 0.8621 - val_loss: 0.7449 - val_acc: 0.9915 - val_mDice: 0.7745

Epoch 00095: val_mDice did not improve from 0.77879
Epoch 96/300
 - 31s - loss: 0.0755 - acc: 0.9918 - mDice: 0.8638 - val_loss: 0.7559 - val_acc: 0.9916 - val_mDice: 0.7777

Epoch 00096: val_mDice did not improve from 0.77879
Epoch 97/300
 - 31s - loss: 0.0759 - acc: 0.9918 - mDice: 0.8631 - val_loss: 0.7414 - val_acc: 0.9914 - val_mDice: 0.7706

Epoch 00097: val_mDice did not improve from 0.77879
Epoch 98/300
 - 31s - loss: 0.0762 - acc: 0.9918 - mDice: 0.8627 - val_loss: 0.7584 - val_acc: 0.9914 - val_mDice: 0.7761

Epoch 00098: val_mDice did not improve from 0.77879
Epoch 99/300
 - 31s - loss: 0.0759 - acc: 0.9918 - mDice: 0.8631 - val_loss: 0.7704 - val_acc: 0.9912 - val_mDice: 0.7749

Epoch 00099: val_mDice did not improve from 0.77879
Epoch 100/300
 - 32s - loss: 0.0756 - acc: 0.9918 - mDice: 0.8636 - val_loss: 0.7478 - val_acc: 0.9913 - val_mDice: 0.7689

Epoch 00100: val_mDice did not improve from 0.77879
Epoch 101/300
 - 33s - loss: 0.0759 - acc: 0.9918 - mDice: 0.8632 - val_loss: 0.7496 - val_acc: 0.9914 - val_mDice: 0.7695

Epoch 00101: val_mDice did not improve from 0.77879
Epoch 102/300
 - 33s - loss: 0.0761 - acc: 0.9918 - mDice: 0.8628 - val_loss: 0.7526 - val_acc: 0.9913 - val_mDice: 0.7707

Epoch 00102: val_mDice did not improve from 0.77879
Epoch 103/300
 - 32s - loss: 0.0751 - acc: 0.9919 - mDice: 0.8644 - val_loss: 0.7762 - val_acc: 0.9911 - val_mDice: 0.7745

Epoch 00103: val_mDice did not improve from 0.77879
Epoch 104/300
 - 32s - loss: 0.0748 - acc: 0.9919 - mDice: 0.8649 - val_loss: 0.7925 - val_acc: 0.9906 - val_mDice: 0.7736

Epoch 00104: val_mDice did not improve from 0.77879
Epoch 105/300
 - 32s - loss: 0.0746 - acc: 0.9919 - mDice: 0.8653 - val_loss: 0.7598 - val_acc: 0.9916 - val_mDice: 0.7831

Epoch 00105: val_mDice improved from 0.77879 to 0.78308, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 106/300
 - 32s - loss: 0.0751 - acc: 0.9919 - mDice: 0.8645 - val_loss: 0.7810 - val_acc: 0.9906 - val_mDice: 0.7595

Epoch 00106: val_mDice did not improve from 0.78308
Epoch 107/300
 - 31s - loss: 0.0750 - acc: 0.9919 - mDice: 0.8647 - val_loss: 0.7533 - val_acc: 0.9915 - val_mDice: 0.7789

Epoch 00107: val_mDice did not improve from 0.78308
Epoch 108/300
 - 31s - loss: 0.0745 - acc: 0.9919 - mDice: 0.8655 - val_loss: 0.7621 - val_acc: 0.9913 - val_mDice: 0.7758

Epoch 00108: val_mDice did not improve from 0.78308
Epoch 109/300
 - 31s - loss: 0.0747 - acc: 0.9919 - mDice: 0.8651 - val_loss: 0.7568 - val_acc: 0.9915 - val_mDice: 0.7792

Epoch 00109: val_mDice did not improve from 0.78308
Epoch 110/300
 - 31s - loss: 0.0750 - acc: 0.9919 - mDice: 0.8647 - val_loss: 0.7758 - val_acc: 0.9912 - val_mDice: 0.7715

Epoch 00110: val_mDice did not improve from 0.78308
Epoch 111/300
 - 31s - loss: 0.0743 - acc: 0.9919 - mDice: 0.8659 - val_loss: 0.7560 - val_acc: 0.9915 - val_mDice: 0.7792

Epoch 00111: val_mDice did not improve from 0.78308
Epoch 112/300
 - 31s - loss: 0.0751 - acc: 0.9919 - mDice: 0.8645 - val_loss: 0.7632 - val_acc: 0.9913 - val_mDice: 0.7738

Epoch 00112: val_mDice did not improve from 0.78308
Epoch 113/300
 - 31s - loss: 0.0737 - acc: 0.9920 - mDice: 0.8668 - val_loss: 0.7540 - val_acc: 0.9914 - val_mDice: 0.7745

Epoch 00113: val_mDice did not improve from 0.78308
Epoch 114/300
 - 31s - loss: 0.0745 - acc: 0.9919 - mDice: 0.8655 - val_loss: 0.7584 - val_acc: 0.9913 - val_mDice: 0.7762

Epoch 00114: val_mDice did not improve from 0.78308
Epoch 115/300
 - 31s - loss: 0.0750 - acc: 0.9919 - mDice: 0.8646 - val_loss: 0.7488 - val_acc: 0.9914 - val_mDice: 0.7796

Epoch 00115: val_mDice did not improve from 0.78308
Epoch 116/300
 - 31s - loss: 0.0746 - acc: 0.9919 - mDice: 0.8653 - val_loss: 0.7634 - val_acc: 0.9912 - val_mDice: 0.7752

Epoch 00116: val_mDice did not improve from 0.78308
Epoch 117/300
 - 32s - loss: 0.0753 - acc: 0.9919 - mDice: 0.8643 - val_loss: 0.7523 - val_acc: 0.9918 - val_mDice: 0.7851

Epoch 00117: val_mDice improved from 0.78308 to 0.78513, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 118/300
 - 33s - loss: 0.0743 - acc: 0.9919 - mDice: 0.8658 - val_loss: 0.7682 - val_acc: 0.9914 - val_mDice: 0.7778

Epoch 00118: val_mDice did not improve from 0.78513
Epoch 119/300
 - 33s - loss: 0.0743 - acc: 0.9919 - mDice: 0.8657 - val_loss: 0.7409 - val_acc: 0.9915 - val_mDice: 0.7726

Epoch 00119: val_mDice did not improve from 0.78513
Epoch 120/300
 - 32s - loss: 0.0738 - acc: 0.9920 - mDice: 0.8666 - val_loss: 0.7419 - val_acc: 0.9915 - val_mDice: 0.7736

Epoch 00120: val_mDice did not improve from 0.78513
Epoch 121/300
 - 32s - loss: 0.0743 - acc: 0.9919 - mDice: 0.8659 - val_loss: 0.7248 - val_acc: 0.9917 - val_mDice: 0.7753

Epoch 00121: val_mDice did not improve from 0.78513
Epoch 122/300
 - 31s - loss: 0.0740 - acc: 0.9920 - mDice: 0.8663 - val_loss: 0.7477 - val_acc: 0.9916 - val_mDice: 0.7814

Epoch 00122: val_mDice did not improve from 0.78513
Epoch 123/300
 - 31s - loss: 0.0746 - acc: 0.9919 - mDice: 0.8655 - val_loss: 0.7359 - val_acc: 0.9909 - val_mDice: 0.7451

Epoch 00123: val_mDice did not improve from 0.78513
Epoch 124/300
 - 31s - loss: 0.0739 - acc: 0.9920 - mDice: 0.8664 - val_loss: 0.7323 - val_acc: 0.9914 - val_mDice: 0.7708

Epoch 00124: val_mDice did not improve from 0.78513
Epoch 125/300
 - 31s - loss: 0.0737 - acc: 0.9920 - mDice: 0.8668 - val_loss: 0.7605 - val_acc: 0.9914 - val_mDice: 0.7827

Epoch 00125: val_mDice did not improve from 0.78513
Epoch 126/300
 - 31s - loss: 0.0736 - acc: 0.9920 - mDice: 0.8671 - val_loss: 0.7504 - val_acc: 0.9915 - val_mDice: 0.7804

Epoch 00126: val_mDice did not improve from 0.78513
Epoch 127/300
 - 31s - loss: 0.0732 - acc: 0.9920 - mDice: 0.8676 - val_loss: 0.7427 - val_acc: 0.9913 - val_mDice: 0.7702

Epoch 00127: val_mDice did not improve from 0.78513
Epoch 128/300
 - 31s - loss: 0.0741 - acc: 0.9920 - mDice: 0.8661 - val_loss: 0.7331 - val_acc: 0.9915 - val_mDice: 0.7745

Epoch 00128: val_mDice did not improve from 0.78513
Epoch 129/300
 - 31s - loss: 0.0740 - acc: 0.9920 - mDice: 0.8664 - val_loss: 0.7467 - val_acc: 0.9914 - val_mDice: 0.7746

Epoch 00129: val_mDice did not improve from 0.78513
Epoch 130/300
 - 32s - loss: 0.0736 - acc: 0.9920 - mDice: 0.8670 - val_loss: 0.7605 - val_acc: 0.9914 - val_mDice: 0.7810

Epoch 00130: val_mDice did not improve from 0.78513
Epoch 131/300
 - 32s - loss: 0.0729 - acc: 0.9920 - mDice: 0.8682 - val_loss: 0.7553 - val_acc: 0.9914 - val_mDice: 0.7798

Epoch 00131: val_mDice did not improve from 0.78513
Epoch 132/300
 - 32s - loss: 0.0737 - acc: 0.9920 - mDice: 0.8668 - val_loss: 0.7646 - val_acc: 0.9914 - val_mDice: 0.7824

Epoch 00132: val_mDice did not improve from 0.78513
Epoch 133/300
 - 32s - loss: 0.0736 - acc: 0.9920 - mDice: 0.8671 - val_loss: 0.7425 - val_acc: 0.9916 - val_mDice: 0.7749

Epoch 00133: val_mDice did not improve from 0.78513
Epoch 134/300
 - 32s - loss: 0.0733 - acc: 0.9920 - mDice: 0.8675 - val_loss: 0.7704 - val_acc: 0.9913 - val_mDice: 0.7785

Epoch 00134: val_mDice did not improve from 0.78513
Epoch 135/300
 - 32s - loss: 0.0734 - acc: 0.9920 - mDice: 0.8672 - val_loss: 0.7622 - val_acc: 0.9916 - val_mDice: 0.7835

Epoch 00135: val_mDice did not improve from 0.78513
Epoch 136/300
 - 32s - loss: 0.0736 - acc: 0.9920 - mDice: 0.8669 - val_loss: 0.7399 - val_acc: 0.9916 - val_mDice: 0.7783

Epoch 00136: val_mDice did not improve from 0.78513
Epoch 137/300
 - 32s - loss: 0.0738 - acc: 0.9920 - mDice: 0.8667 - val_loss: 0.7697 - val_acc: 0.9914 - val_mDice: 0.7816

Epoch 00137: val_mDice did not improve from 0.78513
Epoch 138/300
 - 32s - loss: 0.0736 - acc: 0.9920 - mDice: 0.8670 - val_loss: 0.7648 - val_acc: 0.9913 - val_mDice: 0.7794

Epoch 00138: val_mDice did not improve from 0.78513
Epoch 139/300
 - 32s - loss: 0.0735 - acc: 0.9920 - mDice: 0.8672 - val_loss: 0.7772 - val_acc: 0.9912 - val_mDice: 0.7792

Epoch 00139: val_mDice did not improve from 0.78513
Epoch 140/300
 - 32s - loss: 0.0731 - acc: 0.9920 - mDice: 0.8679 - val_loss: 0.7509 - val_acc: 0.9915 - val_mDice: 0.7797

Epoch 00140: val_mDice did not improve from 0.78513
Epoch 141/300
 - 32s - loss: 0.0730 - acc: 0.9921 - mDice: 0.8681 - val_loss: 0.7491 - val_acc: 0.9913 - val_mDice: 0.7717

Epoch 00141: val_mDice did not improve from 0.78513
Epoch 142/300
 - 32s - loss: 0.0739 - acc: 0.9920 - mDice: 0.8666 - val_loss: 0.7947 - val_acc: 0.9905 - val_mDice: 0.7638

Epoch 00142: val_mDice did not improve from 0.78513
Epoch 143/300
 - 32s - loss: 0.0733 - acc: 0.9920 - mDice: 0.8675 - val_loss: 0.7564 - val_acc: 0.9916 - val_mDice: 0.7825

Epoch 00143: val_mDice did not improve from 0.78513
Epoch 144/300
 - 32s - loss: 0.0730 - acc: 0.9921 - mDice: 0.8680 - val_loss: 0.7675 - val_acc: 0.9914 - val_mDice: 0.7835

Epoch 00144: val_mDice did not improve from 0.78513
Epoch 145/300
 - 32s - loss: 0.0732 - acc: 0.9920 - mDice: 0.8676 - val_loss: 0.7591 - val_acc: 0.9913 - val_mDice: 0.7796

Epoch 00145: val_mDice did not improve from 0.78513
Epoch 146/300
 - 32s - loss: 0.0722 - acc: 0.9921 - mDice: 0.8694 - val_loss: 0.7527 - val_acc: 0.9916 - val_mDice: 0.7794

Epoch 00146: val_mDice did not improve from 0.78513
Epoch 147/300
 - 32s - loss: 0.0721 - acc: 0.9921 - mDice: 0.8695 - val_loss: 0.7542 - val_acc: 0.9913 - val_mDice: 0.7742

Epoch 00147: val_mDice did not improve from 0.78513
Epoch 148/300
 - 32s - loss: 0.0725 - acc: 0.9921 - mDice: 0.8688 - val_loss: 0.7493 - val_acc: 0.9915 - val_mDice: 0.7783

Epoch 00148: val_mDice did not improve from 0.78513
Epoch 149/300
 - 32s - loss: 0.0727 - acc: 0.9921 - mDice: 0.8686 - val_loss: 0.7448 - val_acc: 0.9917 - val_mDice: 0.7852

Epoch 00149: val_mDice improved from 0.78513 to 0.78524, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 150/300
 - 31s - loss: 0.0721 - acc: 0.9921 - mDice: 0.8695 - val_loss: 0.7355 - val_acc: 0.9914 - val_mDice: 0.7719

Epoch 00150: val_mDice did not improve from 0.78524
Epoch 151/300
 - 31s - loss: 0.0729 - acc: 0.9921 - mDice: 0.8682 - val_loss: 0.7516 - val_acc: 0.9913 - val_mDice: 0.7738

Epoch 00151: val_mDice did not improve from 0.78524
Epoch 152/300
 - 31s - loss: 0.0739 - acc: 0.9921 - mDice: 0.8666 - val_loss: 0.7278 - val_acc: 0.9913 - val_mDice: 0.7616

Epoch 00152: val_mDice did not improve from 0.78524
Epoch 153/300
 - 31s - loss: 0.0722 - acc: 0.9921 - mDice: 0.8693 - val_loss: 0.7496 - val_acc: 0.9916 - val_mDice: 0.7839

Epoch 00153: val_mDice did not improve from 0.78524
Epoch 154/300
 - 31s - loss: 0.0728 - acc: 0.9921 - mDice: 0.8683 - val_loss: 0.7774 - val_acc: 0.9908 - val_mDice: 0.7660

Epoch 00154: val_mDice did not improve from 0.78524
Epoch 155/300
 - 31s - loss: 0.0723 - acc: 0.9921 - mDice: 0.8692 - val_loss: 0.7375 - val_acc: 0.9918 - val_mDice: 0.7839

Epoch 00155: val_mDice did not improve from 0.78524
Epoch 156/300
 - 30s - loss: 0.0727 - acc: 0.9921 - mDice: 0.8686 - val_loss: 0.7294 - val_acc: 0.9906 - val_mDice: 0.7327

Epoch 00156: val_mDice did not improve from 0.78524
Epoch 157/300
 - 30s - loss: 0.0734 - acc: 0.9920 - mDice: 0.8674 - val_loss: 0.7549 - val_acc: 0.9915 - val_mDice: 0.7812

Epoch 00157: val_mDice did not improve from 0.78524
Epoch 158/300
 - 30s - loss: 0.0723 - acc: 0.9921 - mDice: 0.8691 - val_loss: 0.7563 - val_acc: 0.9916 - val_mDice: 0.7845

Epoch 00158: val_mDice did not improve from 0.78524
Epoch 159/300
 - 30s - loss: 0.0720 - acc: 0.9921 - mDice: 0.8696 - val_loss: 0.7754 - val_acc: 0.9912 - val_mDice: 0.7826

Epoch 00159: val_mDice did not improve from 0.78524
Epoch 160/300
 - 30s - loss: 0.0727 - acc: 0.9921 - mDice: 0.8685 - val_loss: 0.7844 - val_acc: 0.9909 - val_mDice: 0.7769

Epoch 00160: val_mDice did not improve from 0.78524
Epoch 161/300
 - 31s - loss: 0.0718 - acc: 0.9921 - mDice: 0.8700 - val_loss: 0.7346 - val_acc: 0.9917 - val_mDice: 0.7816

Epoch 00161: val_mDice did not improve from 0.78524
Epoch 162/300
 - 31s - loss: 0.0722 - acc: 0.9921 - mDice: 0.8693 - val_loss: 0.7697 - val_acc: 0.9915 - val_mDice: 0.7867

Epoch 00162: val_mDice improved from 0.78524 to 0.78669, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 163/300
 - 32s - loss: 0.0720 - acc: 0.9921 - mDice: 0.8697 - val_loss: 0.7567 - val_acc: 0.9918 - val_mDice: 0.7858

Epoch 00163: val_mDice did not improve from 0.78669
Epoch 164/300
 - 32s - loss: 0.0718 - acc: 0.9922 - mDice: 0.8700 - val_loss: 0.7310 - val_acc: 0.9917 - val_mDice: 0.7821

Epoch 00164: val_mDice did not improve from 0.78669
Epoch 165/300
 - 33s - loss: 0.0717 - acc: 0.9921 - mDice: 0.8702 - val_loss: 0.7605 - val_acc: 0.9914 - val_mDice: 0.7828

Epoch 00165: val_mDice did not improve from 0.78669
Epoch 166/300
 - 32s - loss: 0.0718 - acc: 0.9922 - mDice: 0.8700 - val_loss: 0.7732 - val_acc: 0.9908 - val_mDice: 0.7644

Epoch 00166: val_mDice did not improve from 0.78669
Epoch 167/300
 - 32s - loss: 0.0719 - acc: 0.9921 - mDice: 0.8698 - val_loss: 0.7256 - val_acc: 0.9918 - val_mDice: 0.7801

Epoch 00167: val_mDice did not improve from 0.78669
Epoch 168/300
 - 33s - loss: 0.0714 - acc: 0.9922 - mDice: 0.8707 - val_loss: 0.7426 - val_acc: 0.9918 - val_mDice: 0.7864

Epoch 00168: val_mDice did not improve from 0.78669
Epoch 169/300
 - 33s - loss: 0.0720 - acc: 0.9921 - mDice: 0.8697 - val_loss: 0.7688 - val_acc: 0.9917 - val_mDice: 0.7866

Epoch 00169: val_mDice did not improve from 0.78669
Epoch 170/300
 - 33s - loss: 0.0720 - acc: 0.9921 - mDice: 0.8697 - val_loss: 0.7472 - val_acc: 0.9916 - val_mDice: 0.7822

Epoch 00170: val_mDice did not improve from 0.78669
Epoch 171/300
 - 32s - loss: 0.0718 - acc: 0.9921 - mDice: 0.8700 - val_loss: 0.7522 - val_acc: 0.9916 - val_mDice: 0.7827

Epoch 00171: val_mDice did not improve from 0.78669
Epoch 172/300
 - 32s - loss: 0.0715 - acc: 0.9922 - mDice: 0.8706 - val_loss: 0.7449 - val_acc: 0.9914 - val_mDice: 0.7793

Epoch 00172: val_mDice did not improve from 0.78669
Epoch 173/300
 - 32s - loss: 0.0719 - acc: 0.9922 - mDice: 0.8698 - val_loss: 0.7486 - val_acc: 0.9915 - val_mDice: 0.7779

Epoch 00173: val_mDice did not improve from 0.78669
Epoch 174/300
 - 32s - loss: 0.0721 - acc: 0.9921 - mDice: 0.8695 - val_loss: 0.7490 - val_acc: 0.9916 - val_mDice: 0.7851

Epoch 00174: val_mDice did not improve from 0.78669
Epoch 175/300
 - 32s - loss: 0.0719 - acc: 0.9922 - mDice: 0.8699 - val_loss: 0.7356 - val_acc: 0.9915 - val_mDice: 0.7763

Epoch 00175: val_mDice did not improve from 0.78669
Epoch 176/300
 - 32s - loss: 0.0720 - acc: 0.9922 - mDice: 0.8697 - val_loss: 0.7460 - val_acc: 0.9915 - val_mDice: 0.7788

Epoch 00176: val_mDice did not improve from 0.78669
Epoch 177/300
 - 32s - loss: 0.0719 - acc: 0.9922 - mDice: 0.8698 - val_loss: 0.7383 - val_acc: 0.9917 - val_mDice: 0.7832

Epoch 00177: val_mDice did not improve from 0.78669
Epoch 178/300
 - 33s - loss: 0.0721 - acc: 0.9921 - mDice: 0.8696 - val_loss: 0.7471 - val_acc: 0.9917 - val_mDice: 0.7800

Epoch 00178: val_mDice did not improve from 0.78669
Epoch 179/300
 - 33s - loss: 0.0714 - acc: 0.9922 - mDice: 0.8706 - val_loss: 0.7663 - val_acc: 0.9912 - val_mDice: 0.7781

Epoch 00179: val_mDice did not improve from 0.78669
Epoch 180/300
 - 32s - loss: 0.0716 - acc: 0.9922 - mDice: 0.8704 - val_loss: 0.7540 - val_acc: 0.9916 - val_mDice: 0.7849

Epoch 00180: val_mDice did not improve from 0.78669
Epoch 181/300
 - 32s - loss: 0.0710 - acc: 0.9922 - mDice: 0.8714 - val_loss: 0.7407 - val_acc: 0.9918 - val_mDice: 0.7856

Epoch 00181: val_mDice did not improve from 0.78669
Epoch 182/300
 - 32s - loss: 0.0715 - acc: 0.9922 - mDice: 0.8704 - val_loss: 0.7584 - val_acc: 0.9913 - val_mDice: 0.7791

Epoch 00182: val_mDice did not improve from 0.78669
Epoch 183/300
 - 33s - loss: 0.0708 - acc: 0.9922 - mDice: 0.8718 - val_loss: 0.7854 - val_acc: 0.9907 - val_mDice: 0.7699

Epoch 00183: val_mDice did not improve from 0.78669
Epoch 184/300
 - 33s - loss: 0.0719 - acc: 0.9922 - mDice: 0.8698 - val_loss: 0.7578 - val_acc: 0.9916 - val_mDice: 0.7876

Epoch 00184: val_mDice improved from 0.78669 to 0.78761, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 185/300
 - 33s - loss: 0.0711 - acc: 0.9922 - mDice: 0.8712 - val_loss: 0.7555 - val_acc: 0.9915 - val_mDice: 0.7791

Epoch 00185: val_mDice did not improve from 0.78761
Epoch 186/300
 - 32s - loss: 0.0710 - acc: 0.9922 - mDice: 0.8713 - val_loss: 0.7261 - val_acc: 0.9915 - val_mDice: 0.7686

Epoch 00186: val_mDice did not improve from 0.78761
Epoch 187/300
 - 32s - loss: 0.0714 - acc: 0.9922 - mDice: 0.8708 - val_loss: 0.7340 - val_acc: 0.9913 - val_mDice: 0.7627

Epoch 00187: val_mDice did not improve from 0.78761
Epoch 188/300
 - 32s - loss: 0.0723 - acc: 0.9921 - mDice: 0.8693 - val_loss: 0.7672 - val_acc: 0.9914 - val_mDice: 0.7849

Epoch 00188: val_mDice did not improve from 0.78761
Epoch 189/300
 - 32s - loss: 0.0713 - acc: 0.9922 - mDice: 0.8709 - val_loss: 0.7631 - val_acc: 0.9913 - val_mDice: 0.7804

Epoch 00189: val_mDice did not improve from 0.78761
Epoch 190/300
 - 33s - loss: 0.0715 - acc: 0.9922 - mDice: 0.8705 - val_loss: 0.7629 - val_acc: 0.9912 - val_mDice: 0.7764

Epoch 00190: val_mDice did not improve from 0.78761
Epoch 191/300
 - 33s - loss: 0.0713 - acc: 0.9922 - mDice: 0.8709 - val_loss: 0.7525 - val_acc: 0.9915 - val_mDice: 0.7789

Epoch 00191: val_mDice did not improve from 0.78761
Epoch 192/300
 - 33s - loss: 0.0711 - acc: 0.9922 - mDice: 0.8713 - val_loss: 0.7235 - val_acc: 0.9915 - val_mDice: 0.7787

Epoch 00192: val_mDice did not improve from 0.78761
Epoch 193/300
 - 32s - loss: 0.0714 - acc: 0.9922 - mDice: 0.8707 - val_loss: 0.7328 - val_acc: 0.9916 - val_mDice: 0.7744

Epoch 00193: val_mDice did not improve from 0.78761
Epoch 194/300
 - 32s - loss: 0.0710 - acc: 0.9922 - mDice: 0.8714 - val_loss: 0.7634 - val_acc: 0.9913 - val_mDice: 0.7791

Epoch 00194: val_mDice did not improve from 0.78761
Epoch 195/300
 - 32s - loss: 0.0720 - acc: 0.9921 - mDice: 0.8697 - val_loss: 0.7468 - val_acc: 0.9915 - val_mDice: 0.7801

Epoch 00195: val_mDice did not improve from 0.78761
Epoch 196/300
 - 32s - loss: 0.0709 - acc: 0.9922 - mDice: 0.8716 - val_loss: 0.7399 - val_acc: 0.9914 - val_mDice: 0.7778

Epoch 00196: val_mDice did not improve from 0.78761
Epoch 197/300
 - 32s - loss: 0.0709 - acc: 0.9922 - mDice: 0.8715 - val_loss: 0.7340 - val_acc: 0.9916 - val_mDice: 0.7809

Epoch 00197: val_mDice did not improve from 0.78761
Epoch 198/300
 - 31s - loss: 0.0711 - acc: 0.9922 - mDice: 0.8711 - val_loss: 0.7522 - val_acc: 0.9916 - val_mDice: 0.7823

Epoch 00198: val_mDice did not improve from 0.78761
Epoch 199/300
 - 31s - loss: 0.0709 - acc: 0.9922 - mDice: 0.8716 - val_loss: 0.7361 - val_acc: 0.9915 - val_mDice: 0.7780

Epoch 00199: val_mDice did not improve from 0.78761
Epoch 200/300
 - 31s - loss: 0.0709 - acc: 0.9922 - mDice: 0.8716 - val_loss: 0.7196 - val_acc: 0.9919 - val_mDice: 0.7868

Epoch 00200: val_mDice did not improve from 0.78761
Epoch 201/300
 - 30s - loss: 0.0712 - acc: 0.9922 - mDice: 0.8711 - val_loss: 0.7605 - val_acc: 0.9915 - val_mDice: 0.7848

Epoch 00201: val_mDice did not improve from 0.78761
Epoch 202/300
 - 31s - loss: 0.0710 - acc: 0.9922 - mDice: 0.8714 - val_loss: 0.7362 - val_acc: 0.9910 - val_mDice: 0.7475

Epoch 00202: val_mDice did not improve from 0.78761
Epoch 203/300
 - 31s - loss: 0.0703 - acc: 0.9923 - mDice: 0.8725 - val_loss: 0.7348 - val_acc: 0.9914 - val_mDice: 0.7738

Epoch 00203: val_mDice did not improve from 0.78761
Epoch 204/300
 - 31s - loss: 0.0704 - acc: 0.9922 - mDice: 0.8723 - val_loss: 0.7602 - val_acc: 0.9916 - val_mDice: 0.7872

Epoch 00204: val_mDice did not improve from 0.78761
Epoch 205/300
 - 31s - loss: 0.0705 - acc: 0.9923 - mDice: 0.8723 - val_loss: 0.7838 - val_acc: 0.9907 - val_mDice: 0.7722

Epoch 00205: val_mDice did not improve from 0.78761
Epoch 206/300
 - 31s - loss: 0.0706 - acc: 0.9922 - mDice: 0.8720 - val_loss: 0.7725 - val_acc: 0.9911 - val_mDice: 0.7805

Epoch 00206: val_mDice did not improve from 0.78761
Epoch 207/300
 - 31s - loss: 0.0703 - acc: 0.9923 - mDice: 0.8726 - val_loss: 0.7622 - val_acc: 0.9915 - val_mDice: 0.7869

Epoch 00207: val_mDice did not improve from 0.78761
Epoch 208/300
 - 31s - loss: 0.0707 - acc: 0.9923 - mDice: 0.8718 - val_loss: 0.7604 - val_acc: 0.9912 - val_mDice: 0.7725

Epoch 00208: val_mDice did not improve from 0.78761
Epoch 209/300
 - 31s - loss: 0.0708 - acc: 0.9923 - mDice: 0.8718 - val_loss: 0.7481 - val_acc: 0.9917 - val_mDice: 0.7859

Epoch 00209: val_mDice did not improve from 0.78761
Epoch 210/300
 - 31s - loss: 0.0704 - acc: 0.9923 - mDice: 0.8724 - val_loss: 0.7294 - val_acc: 0.9917 - val_mDice: 0.7804

Epoch 00210: val_mDice did not improve from 0.78761
Epoch 211/300
 - 31s - loss: 0.0708 - acc: 0.9923 - mDice: 0.8717 - val_loss: 0.7767 - val_acc: 0.9911 - val_mDice: 0.7827

Epoch 00211: val_mDice did not improve from 0.78761
Epoch 212/300
 - 30s - loss: 0.0716 - acc: 0.9922 - mDice: 0.8703 - val_loss: 0.7353 - val_acc: 0.9916 - val_mDice: 0.7825

Epoch 00212: val_mDice did not improve from 0.78761
Epoch 213/300
 - 31s - loss: 0.0708 - acc: 0.9922 - mDice: 0.8718 - val_loss: 0.7669 - val_acc: 0.9912 - val_mDice: 0.7739

Epoch 00213: val_mDice did not improve from 0.78761
Epoch 214/300
 - 31s - loss: 0.0706 - acc: 0.9923 - mDice: 0.8721 - val_loss: 0.7524 - val_acc: 0.9914 - val_mDice: 0.7773

Epoch 00214: val_mDice did not improve from 0.78761
Epoch 215/300
 - 31s - loss: 0.0702 - acc: 0.9923 - mDice: 0.8728 - val_loss: 0.7523 - val_acc: 0.9914 - val_mDice: 0.7782

Epoch 00215: val_mDice did not improve from 0.78761
Epoch 216/300
 - 30s - loss: 0.0707 - acc: 0.9922 - mDice: 0.8719 - val_loss: 0.7307 - val_acc: 0.9915 - val_mDice: 0.7731

Epoch 00216: val_mDice did not improve from 0.78761
Epoch 217/300
 - 30s - loss: 0.0707 - acc: 0.9922 - mDice: 0.8720 - val_loss: 0.7248 - val_acc: 0.9918 - val_mDice: 0.7803

Epoch 00217: val_mDice did not improve from 0.78761
Epoch 218/300
 - 31s - loss: 0.0703 - acc: 0.9923 - mDice: 0.8726 - val_loss: 0.7495 - val_acc: 0.9915 - val_mDice: 0.7855

Epoch 00218: val_mDice did not improve from 0.78761
Epoch 219/300
 - 31s - loss: 0.0705 - acc: 0.9923 - mDice: 0.8723 - val_loss: 0.7488 - val_acc: 0.9916 - val_mDice: 0.7849

Epoch 00219: val_mDice did not improve from 0.78761
Epoch 220/300
 - 31s - loss: 0.0701 - acc: 0.9923 - mDice: 0.8729 - val_loss: 0.7434 - val_acc: 0.9919 - val_mDice: 0.7896

Epoch 00220: val_mDice improved from 0.78761 to 0.78962, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 221/300
 - 31s - loss: 0.0700 - acc: 0.9923 - mDice: 0.8731 - val_loss: 0.7362 - val_acc: 0.9914 - val_mDice: 0.7777

Epoch 00221: val_mDice did not improve from 0.78962
Epoch 222/300
 - 32s - loss: 0.0700 - acc: 0.9923 - mDice: 0.8731 - val_loss: 0.7390 - val_acc: 0.9918 - val_mDice: 0.7851

Epoch 00222: val_mDice did not improve from 0.78962
Epoch 223/300
 - 31s - loss: 0.0701 - acc: 0.9923 - mDice: 0.8729 - val_loss: 0.7266 - val_acc: 0.9917 - val_mDice: 0.7806

Epoch 00223: val_mDice did not improve from 0.78962
Epoch 224/300
 - 31s - loss: 0.0704 - acc: 0.9923 - mDice: 0.8724 - val_loss: 0.7717 - val_acc: 0.9909 - val_mDice: 0.7711

Epoch 00224: val_mDice did not improve from 0.78962
Epoch 225/300
 - 31s - loss: 0.0704 - acc: 0.9923 - mDice: 0.8724 - val_loss: 0.7405 - val_acc: 0.9918 - val_mDice: 0.7897

Epoch 00225: val_mDice improved from 0.78962 to 0.78968, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 226/300
 - 31s - loss: 0.0701 - acc: 0.9923 - mDice: 0.8729 - val_loss: 0.7412 - val_acc: 0.9916 - val_mDice: 0.7818

Epoch 00226: val_mDice did not improve from 0.78968
Epoch 227/300
 - 31s - loss: 0.0700 - acc: 0.9923 - mDice: 0.8731 - val_loss: 0.7287 - val_acc: 0.9914 - val_mDice: 0.7733

Epoch 00227: val_mDice did not improve from 0.78968
Epoch 228/300
 - 31s - loss: 0.0695 - acc: 0.9924 - mDice: 0.8739 - val_loss: 0.7178 - val_acc: 0.9911 - val_mDice: 0.7578

Epoch 00228: val_mDice did not improve from 0.78968
Epoch 229/300
 - 31s - loss: 0.0701 - acc: 0.9923 - mDice: 0.8729 - val_loss: 0.7156 - val_acc: 0.9914 - val_mDice: 0.7694

Epoch 00229: val_mDice did not improve from 0.78968
Epoch 230/300
 - 30s - loss: 0.0700 - acc: 0.9923 - mDice: 0.8730 - val_loss: 0.7322 - val_acc: 0.9917 - val_mDice: 0.7782

Epoch 00230: val_mDice did not improve from 0.78968
Epoch 231/300
 - 31s - loss: 0.0703 - acc: 0.9923 - mDice: 0.8725 - val_loss: 0.7473 - val_acc: 0.9915 - val_mDice: 0.7834

Epoch 00231: val_mDice did not improve from 0.78968
Epoch 232/300
 - 31s - loss: 0.0702 - acc: 0.9923 - mDice: 0.8727 - val_loss: 0.8150 - val_acc: 0.9894 - val_mDice: 0.7572

Epoch 00232: val_mDice did not improve from 0.78968
Epoch 233/300
 - 31s - loss: 0.0700 - acc: 0.9923 - mDice: 0.8730 - val_loss: 0.7277 - val_acc: 0.9915 - val_mDice: 0.7791

Epoch 00233: val_mDice did not improve from 0.78968
Epoch 234/300
 - 31s - loss: 0.0701 - acc: 0.9923 - mDice: 0.8730 - val_loss: 0.7522 - val_acc: 0.9916 - val_mDice: 0.7852

Epoch 00234: val_mDice did not improve from 0.78968
Epoch 235/300
 - 31s - loss: 0.0695 - acc: 0.9923 - mDice: 0.8739 - val_loss: 0.7203 - val_acc: 0.9917 - val_mDice: 0.7821

Epoch 00235: val_mDice did not improve from 0.78968
Epoch 236/300
 - 31s - loss: 0.0705 - acc: 0.9923 - mDice: 0.8723 - val_loss: 0.7328 - val_acc: 0.9919 - val_mDice: 0.7852

Epoch 00236: val_mDice did not improve from 0.78968
Epoch 237/300
 - 31s - loss: 0.0699 - acc: 0.9923 - mDice: 0.8732 - val_loss: 0.7310 - val_acc: 0.9915 - val_mDice: 0.7775

Epoch 00237: val_mDice did not improve from 0.78968
Epoch 238/300
 - 31s - loss: 0.0704 - acc: 0.9923 - mDice: 0.8724 - val_loss: 0.7369 - val_acc: 0.9917 - val_mDice: 0.7854

Epoch 00238: val_mDice did not improve from 0.78968
Epoch 239/300
 - 31s - loss: 0.0696 - acc: 0.9923 - mDice: 0.8737 - val_loss: 0.7538 - val_acc: 0.9915 - val_mDice: 0.7836

Epoch 00239: val_mDice did not improve from 0.78968
Epoch 240/300
 - 30s - loss: 0.0697 - acc: 0.9923 - mDice: 0.8736 - val_loss: 0.7387 - val_acc: 0.9916 - val_mDice: 0.7800

Epoch 00240: val_mDice did not improve from 0.78968
Epoch 241/300
 - 31s - loss: 0.0701 - acc: 0.9923 - mDice: 0.8728 - val_loss: 0.7171 - val_acc: 0.9913 - val_mDice: 0.7676

Epoch 00241: val_mDice did not improve from 0.78968
Epoch 242/300
 - 31s - loss: 0.0702 - acc: 0.9923 - mDice: 0.8728 - val_loss: 0.7396 - val_acc: 0.9916 - val_mDice: 0.7790

Epoch 00242: val_mDice did not improve from 0.78968
Epoch 243/300
 - 30s - loss: 0.0695 - acc: 0.9923 - mDice: 0.8740 - val_loss: 0.7458 - val_acc: 0.9916 - val_mDice: 0.7837

Epoch 00243: val_mDice did not improve from 0.78968
Epoch 244/300
 - 31s - loss: 0.0707 - acc: 0.9923 - mDice: 0.8720 - val_loss: 0.7420 - val_acc: 0.9915 - val_mDice: 0.7790

Epoch 00244: val_mDice did not improve from 0.78968
Epoch 245/300
 - 31s - loss: 0.0696 - acc: 0.9923 - mDice: 0.8738 - val_loss: 0.7241 - val_acc: 0.9916 - val_mDice: 0.7805

Epoch 00245: val_mDice did not improve from 0.78968
Epoch 246/300
 - 31s - loss: 0.0700 - acc: 0.9923 - mDice: 0.8731 - val_loss: 0.7615 - val_acc: 0.9916 - val_mDice: 0.7890

Epoch 00246: val_mDice did not improve from 0.78968
Epoch 247/300
 - 31s - loss: 0.0693 - acc: 0.9924 - mDice: 0.8743 - val_loss: 0.7374 - val_acc: 0.9915 - val_mDice: 0.7748

Epoch 00247: val_mDice did not improve from 0.78968
Epoch 248/300
 - 31s - loss: 0.0694 - acc: 0.9924 - mDice: 0.8741 - val_loss: 0.7470 - val_acc: 0.9916 - val_mDice: 0.7823

Epoch 00248: val_mDice did not improve from 0.78968
Epoch 249/300
 - 30s - loss: 0.0696 - acc: 0.9924 - mDice: 0.8738 - val_loss: 0.7418 - val_acc: 0.9916 - val_mDice: 0.7828

Epoch 00249: val_mDice did not improve from 0.78968
Epoch 250/300
 - 30s - loss: 0.0701 - acc: 0.9923 - mDice: 0.8729 - val_loss: 0.7365 - val_acc: 0.9916 - val_mDice: 0.7819

Epoch 00250: val_mDice did not improve from 0.78968
Epoch 251/300
 - 31s - loss: 0.0696 - acc: 0.9923 - mDice: 0.8737 - val_loss: 0.7341 - val_acc: 0.9918 - val_mDice: 0.7845

Epoch 00251: val_mDice did not improve from 0.78968
Epoch 252/300
 - 31s - loss: 0.0694 - acc: 0.9923 - mDice: 0.8741 - val_loss: 0.7419 - val_acc: 0.9916 - val_mDice: 0.7800

Epoch 00252: val_mDice did not improve from 0.78968
Epoch 253/300
 - 30s - loss: 0.0696 - acc: 0.9923 - mDice: 0.8738 - val_loss: 0.7509 - val_acc: 0.9916 - val_mDice: 0.7864

Epoch 00253: val_mDice did not improve from 0.78968
Epoch 254/300
 - 31s - loss: 0.0694 - acc: 0.9924 - mDice: 0.8741 - val_loss: 0.7408 - val_acc: 0.9913 - val_mDice: 0.7764

Epoch 00254: val_mDice did not improve from 0.78968
Epoch 255/300
 - 31s - loss: 0.0694 - acc: 0.9923 - mDice: 0.8741 - val_loss: 0.7324 - val_acc: 0.9916 - val_mDice: 0.7747

Epoch 00255: val_mDice did not improve from 0.78968
Epoch 256/300
 - 31s - loss: 0.0693 - acc: 0.9924 - mDice: 0.8743 - val_loss: 0.7379 - val_acc: 0.9917 - val_mDice: 0.7864

Epoch 00256: val_mDice did not improve from 0.78968
Epoch 257/300
 - 31s - loss: 0.0696 - acc: 0.9923 - mDice: 0.8738 - val_loss: 0.7131 - val_acc: 0.9916 - val_mDice: 0.7749

Epoch 00257: val_mDice did not improve from 0.78968
Epoch 258/300
 - 31s - loss: 0.0698 - acc: 0.9923 - mDice: 0.8735 - val_loss: 0.7252 - val_acc: 0.9914 - val_mDice: 0.7758

Epoch 00258: val_mDice did not improve from 0.78968
Epoch 259/300
 - 31s - loss: 0.0693 - acc: 0.9924 - mDice: 0.8743 - val_loss: 0.7136 - val_acc: 0.9912 - val_mDice: 0.7644

Epoch 00259: val_mDice did not improve from 0.78968
Epoch 260/300
 - 31s - loss: 0.0690 - acc: 0.9924 - mDice: 0.8747 - val_loss: 0.7640 - val_acc: 0.9911 - val_mDice: 0.7785

Epoch 00260: val_mDice did not improve from 0.78968
Epoch 261/300
 - 31s - loss: 0.0690 - acc: 0.9924 - mDice: 0.8747 - val_loss: 0.7429 - val_acc: 0.9918 - val_mDice: 0.7882

Epoch 00261: val_mDice did not improve from 0.78968
Epoch 262/300
 - 30s - loss: 0.0693 - acc: 0.9924 - mDice: 0.8743 - val_loss: 0.7334 - val_acc: 0.9914 - val_mDice: 0.7752

Epoch 00262: val_mDice did not improve from 0.78968
Epoch 263/300
 - 31s - loss: 0.0694 - acc: 0.9924 - mDice: 0.8741 - val_loss: 0.7337 - val_acc: 0.9916 - val_mDice: 0.7807

Epoch 00263: val_mDice did not improve from 0.78968
Epoch 264/300
 - 31s - loss: 0.0696 - acc: 0.9923 - mDice: 0.8738 - val_loss: 0.7417 - val_acc: 0.9915 - val_mDice: 0.7780

Epoch 00264: val_mDice did not improve from 0.78968
Epoch 265/300
 - 30s - loss: 0.0692 - acc: 0.9924 - mDice: 0.8744 - val_loss: 0.7320 - val_acc: 0.9918 - val_mDice: 0.7872

Epoch 00265: val_mDice did not improve from 0.78968
Restoring model weights from the end of the best epoch
Epoch 00265: early stopping
{'val_loss': [0.8920704340562224, 0.8641502908430994, 0.8310637073591352, 0.8274772034492344, 0.8242365822661668, 0.8079631654545665, 0.7978424231987447, 0.7942753192037344, 0.7667496942449361, 0.8221292747184634, 0.7773086160887033, 0.764029924524948, 0.79593435395509, 0.788272246485576, 0.7953326185233891, 0.7563513391651213, 0.7825842297170311, 0.7655751935672015, 0.7776549579575658, 0.7784358030185103, 0.7408537021838129, 0.7764897029846907, 0.7623139335773885, 0.7971018957905471, 0.7723799799568951, 0.7532180428970605, 0.7893459231127053, 0.7741193308029324, 0.7535815490409732, 0.7907909033820033, 0.7795522669330239, 0.7625976535491645, 0.7168652664404362, 0.7442254342604429, 0.7524461185093969, 0.7502331824507564, 0.735232247505337, 0.7383323551621288, 0.7471341034397483, 0.7654449930414557, 0.754561611218378, 0.7762153642252088, 0.7374830024782568, 0.7527047565672547, 0.732039537280798, 0.752016358776018, 0.7574089257977903, 0.7446477543562651, 0.7632947063539177, 0.7441563117317855, 0.7571366003248841, 0.7360001290217042, 0.7592611003201455, 0.7717454482335597, 0.7423532691318542, 0.7629526208620518, 0.7487784659024328, 0.7495635827071965, 0.7819786574691534, 0.7617003477644175, 0.7454415217507631, 0.7567348864395171, 0.7532226126641035, 0.7494681214448065, 0.7641393123194575, 0.7736522741615772, 0.7258445248007774, 0.7830976052209735, 0.7528677231166512, 0.7400840839836746, 0.7313960371538997, 0.7451445830520242, 0.7558978628367186, 0.7396579128690064, 0.7295864732004702, 0.7701199958100915, 0.7480524419806898, 0.7949831089936197, 0.7436570411082357, 0.7538331721443683, 0.7517049917951226, 0.7709680555853993, 0.7641909890808165, 0.7668781967367977, 0.7563518895767629, 0.744772951118648, 0.7779568475671113, 0.7516728930640966, 0.7559566302224994, 0.7534478313755244, 0.765484536299482, 0.7622295410837978, 0.7534571562428027, 0.789156646002084, 0.7448551917914301, 0.7558708291035146, 0.7413670343812555, 0.7583948639221489, 0.7703659990802407, 0.7477934579364955, 0.7496009711176157, 0.7526073448825628, 0.7762119397521019, 0.7924732144456357, 0.7597783575765789, 0.7810019985772669, 0.753313273191452, 0.7621498631779104, 0.7567888947669417, 0.7758220550604165, 0.7559863170608878, 0.7632454733829945, 0.7540047313086689, 0.758440105477348, 0.7488151513971388, 0.763433872256428, 0.7523021495435387, 0.7682242712471634, 0.740914398804307, 0.7419208846986294, 0.7247762125916779, 0.747699337080121, 0.7358512186910957, 0.7322962728794664, 0.7604644002858549, 0.7504343597684056, 0.7426967537030578, 0.7331101589370519, 0.7466910574585199, 0.760470915818587, 0.7552556733135134, 0.764582906384021, 0.7424562857486308, 0.7704134956002235, 0.7622470441274345, 0.7399150608107448, 0.7697286596521735, 0.7648465111851692, 0.7772228911053389, 0.7509005768224597, 0.7491269959136844, 0.794714615913108, 0.7564390429761261, 0.7674753421451896, 0.7590598380193114, 0.7526685877237469, 0.7542090730275959, 0.7492568127345294, 0.7448306009173393, 0.7354572685435414, 0.7515994950663298, 0.7277607426512986, 0.7495621680282056, 0.7773745516315103, 0.737476299284026, 0.7294341733213514, 0.7548841717652977, 0.7562907743267715, 0.7754382218699902, 0.7843576173763722, 0.7345626528840512, 0.7696548630483449, 0.7567366734147072, 0.7310430856887251, 0.7605396660510451, 0.7731788279488683, 0.7256081167142838, 0.7426225563976914, 0.76875289529562, 0.7471772963181138, 0.7521905803587288, 0.7449314792174846, 0.748628199333325, 0.7490288144908845, 0.735635656863451, 0.7460081859026104, 0.7382509554736316, 0.7471259767189622, 0.7662569717504084, 0.7539706870447844, 0.7407082130666822, 0.7584218787960708, 0.7853794309776276, 0.7577663178090006, 0.7554864264093339, 0.7261375575326383, 0.7339698625728488, 0.7672492677811533, 0.7631248238030821, 0.7629327834583819, 0.7524914499372244, 0.7234777852427214, 0.7327617958653718, 0.7633884230162948, 0.7468239804729819, 0.7398575972765684, 0.7339816247113049, 0.7522359972354025, 0.7361241497565061, 0.7195816452149302, 0.7604612307623029, 0.7362426090985537, 0.7347570376005024, 0.7601889378856868, 0.7838488942943513, 0.7724919312167913, 0.7622195351868868, 0.7603746140375733, 0.7480951840989292, 0.7294483287259936, 0.776700638467446, 0.7352751814760268, 0.7668947398196906, 0.7524040048010647, 0.7522731926292181, 0.730737368343398, 0.7248440978582948, 0.7494990304112434, 0.7488114614970982, 0.743404561188072, 0.7362059457227588, 0.738970881793648, 0.7265565861016512, 0.7716624496970326, 0.7404769831337035, 0.7412025076337159, 0.7287031053565443, 0.7178416126407683, 0.715573726920411, 0.7322237836197019, 0.7472734788898379, 0.8150187442079186, 0.7276518067810684, 0.7522033711429685, 0.720254568150267, 0.7328333558980376, 0.7310453986283392, 0.7368974424898624, 0.7537536821328104, 0.7387197136413306, 0.7170521533116698, 0.7396284053102136, 0.745815162314102, 0.7419689502567053, 0.7241150385234505, 0.761508651310578, 0.7374343080446124, 0.747014393331483, 0.7418160520028323, 0.7365193672012538, 0.7341294658835977, 0.7418507328256965, 0.7508942591957748, 0.740768667543307, 0.7324233551044017, 0.7378930863924325, 0.7131203385069966, 0.7251844867132604, 0.7135714334435761, 0.7639916965272278, 0.7428735515568405, 0.733410838060081, 0.7336521788965911, 0.7416873180773109, 0.7319717013742775], 'val_acc': [0.9842465315014124, 0.9851370602846146, 0.9873236436396837, 0.9878250267356634, 0.9881505332887173, 0.9890709761530161, 0.9892962258309126, 0.9896429367363453, 0.9897604305297136, 0.9889886695891619, 0.9899984113872051, 0.9901218730956316, 0.9901670329272747, 0.9901877287775278, 0.9903181660920382, 0.9901884719729424, 0.9903147090226412, 0.9905716180801392, 0.9906893502920866, 0.9906192645430565, 0.9908435177057981, 0.9907118119299412, 0.9908659663051367, 0.9902463555335999, 0.9908983781933784, 0.9907923731952906, 0.9902643039822578, 0.9906713962554932, 0.9909949228167534, 0.990621754899621, 0.9906449634581804, 0.9910044055432081, 0.9908367712050676, 0.9908559862524271, 0.9910006579011679, 0.9911256246268749, 0.990903127938509, 0.9910862147808075, 0.9908018596470356, 0.9910415653139353, 0.9911687765270472, 0.9907417353242636, 0.9911179021000862, 0.9912680611014366, 0.9909587595611811, 0.9910862203687429, 0.9911388456821442, 0.9912892580032349, 0.9910078886896372, 0.9912109430879354, 0.9911106526851654, 0.990938812494278, 0.9912293925881386, 0.9911034349352121, 0.9914274420589209, 0.9910700041800737, 0.9911967273801565, 0.9912593197077513, 0.9909896813333035, 0.9910984355956316, 0.9913403950631618, 0.9912097007036209, 0.9914773497730494, 0.9911520686000586, 0.9914266988635063, 0.9911326076835394, 0.9911737740039825, 0.9908090848475695, 0.9912755321711302, 0.9914287105202675, 0.9912323895841837, 0.9912141803652048, 0.9913942776620388, 0.9912578444927931, 0.9910560324788094, 0.9911695271730423, 0.9910892080515623, 0.9905092604458332, 0.9911623019725084, 0.9913541078567505, 0.9912510942667723, 0.9913299195468426, 0.9910208694636822, 0.9913062304258347, 0.9912436194717884, 0.991352865472436, 0.9910725075751543, 0.9913813099265099, 0.9911550618708134, 0.9914576299488544, 0.9912975113838911, 0.9913082215934992, 0.9914232138544321, 0.9907853864133358, 0.9914641138166189, 0.9915514346212149, 0.991351131349802, 0.9913618452847004, 0.9912254065275192, 0.9913176950067282, 0.9913638345897198, 0.9913481269031763, 0.9910879600793123, 0.9906145241111517, 0.9915643930435181, 0.9906155224889517, 0.9914611205458641, 0.9913369081914425, 0.9915404617786407, 0.9911647886037827, 0.9915177654474974, 0.9912807885557413, 0.9913950376212597, 0.9913052171468735, 0.9914257023483515, 0.9912094473838806, 0.9918031096458435, 0.9913965296000242, 0.9914987944066525, 0.991496043279767, 0.991678636521101, 0.9916287530213594, 0.9908597208559513, 0.9913541115820408, 0.9914112351834774, 0.991539191454649, 0.9913064781576395, 0.9914733525365591, 0.9914159681648016, 0.9913650956004858, 0.9914461653679609, 0.9913755692541599, 0.9915895890444517, 0.9912999793887138, 0.991575364023447, 0.991631492972374, 0.9914329461753368, 0.9912977498024702, 0.9911937285214663, 0.9914878103882074, 0.9913219381123781, 0.9905359447002411, 0.9915651418268681, 0.991401756182313, 0.9913209397345781, 0.9916045423597097, 0.9912862610071898, 0.9914631117135286, 0.9917163047939539, 0.9914252199232578, 0.9913446418941021, 0.9912568386644125, 0.9915681388229132, 0.9908006154000759, 0.9918357972055674, 0.9906145352870226, 0.9914745911955833, 0.9916105382144451, 0.991166278719902, 0.9908517338335514, 0.9917195476591587, 0.9914893191307783, 0.9917574636638165, 0.9916504565626383, 0.9913980253040791, 0.9907724298536777, 0.9917706903070211, 0.9918400309979916, 0.9916931074112654, 0.9916499555110931, 0.9915524199604988, 0.9914274550974369, 0.9914726000279188, 0.9916474707424641, 0.9915242400020361, 0.9914793409407139, 0.9916943609714508, 0.9916871208697557, 0.9912485927343369, 0.9916120301932096, 0.9917931482195854, 0.9913274198770523, 0.9907437339425087, 0.991580355912447, 0.9914905596524477, 0.9914591312408447, 0.9912847597151995, 0.9914312027394772, 0.9913461375981569, 0.9912196602672338, 0.9914870634675026, 0.9915464241057634, 0.9915778636932373, 0.9912910144776106, 0.9914960563182831, 0.9913586229085922, 0.991629496216774, 0.9915586542338133, 0.9914725925773382, 0.9918886702507734, 0.9914561435580254, 0.9910213481634855, 0.9914287086576223, 0.9915796164423227, 0.9906649217009544, 0.9911360964179039, 0.9915367141366005, 0.9911642763763666, 0.9916619304567575, 0.9916504435241222, 0.9911014325916767, 0.9915688820183277, 0.9912473428994417, 0.9913715776056051, 0.9913650825619698, 0.991501783952117, 0.9917629528790712, 0.9915302153676748, 0.9915915839374065, 0.991901895031333, 0.9913528747856617, 0.9917941335588694, 0.991706570610404, 0.9908846635371447, 0.9917641784995794, 0.9915771093219519, 0.9914376735687256, 0.9911403469741344, 0.9914059974253178, 0.9916572030633688, 0.991537457332015, 0.9894062336534262, 0.9914953000843525, 0.9915875904262066, 0.9916763957589865, 0.9919398054480553, 0.9914576355367899, 0.9917357545346022, 0.9914738442748785, 0.9915726277977228, 0.991260077804327, 0.9915990680456161, 0.9916062895208597, 0.9915082659572363, 0.9915569089353085, 0.991580868139863, 0.991520244628191, 0.9915943332016468, 0.991620272397995, 0.9915506709367037, 0.9918385185301304, 0.9916337430477142, 0.9916412141174078, 0.991304237395525, 0.9915691278874874, 0.9916821345686913, 0.9915673937648535, 0.9914010129868984, 0.991191977635026, 0.9911308530718088, 0.9918168298900127, 0.9914498887956142, 0.991627749055624, 0.9915419425815344, 0.9917951244860888], 'val_mDice': [0.6272828904911876, 0.6692719738930464, 0.701138935983181, 0.7046437822282314, 0.7164022512733936, 0.7251948583871126, 0.7340764533728361, 0.7385080121457577, 0.7283508591353893, 0.7438966911286116, 0.7401503808796406, 0.7382481824606657, 0.7598766684532166, 0.7554834242910147, 0.7568865083158016, 0.7410773262381554, 0.7518702652305365, 0.7562145031988621, 0.7676833122968674, 0.7561967335641384, 0.7602816913276911, 0.7662524469196796, 0.7665019668638706, 0.7606376800686121, 0.7693514041602612, 0.761793702840805, 0.7578048948198557, 0.7655202504247427, 0.7670847792178392, 0.7683970965445042, 0.7644603401422501, 0.7709493208676577, 0.7517215982079506, 0.7554639223963022, 0.7652734871953726, 0.7669430412352085, 0.7583602797240019, 0.7594922557473183, 0.751318670809269, 0.7685548346489668, 0.7708689011633396, 0.7684719748795033, 0.761204194277525, 0.7695998530834913, 0.7506506890058517, 0.7643124274909496, 0.7711711712181568, 0.7693372759968042, 0.7651297934353352, 0.7683927193284035, 0.7709080725908279, 0.7593365758657455, 0.7748227566480637, 0.7699108496308327, 0.7745456416159868, 0.7658119965344667, 0.7677748985588551, 0.7682463899254799, 0.7715671882033348, 0.7631739042699337, 0.7720497902482748, 0.7697943691164255, 0.7738972827792168, 0.7606193982064724, 0.7750118151307106, 0.7747144158929586, 0.7587939500808716, 0.7676249090582132, 0.7661635987460613, 0.7718535885214806, 0.7604654785245657, 0.7686676923185587, 0.7774338405579329, 0.7634793687611818, 0.755542078986764, 0.772468838840723, 0.7615720108151436, 0.7585706748068333, 0.7653424143791199, 0.7731248904019594, 0.772666297852993, 0.7787931058555841, 0.7649664115160704, 0.7767514046281576, 0.7683651614934206, 0.7704753801226616, 0.769133698195219, 0.7739529386162758, 0.7685466948896646, 0.776640810072422, 0.7766473107039928, 0.7693699654191732, 0.7768304273486137, 0.7690537348389626, 0.7745183445513248, 0.7777474448084831, 0.7705747056752443, 0.7760978955775499, 0.7749393489211798, 0.7689077686518431, 0.7695285975933075, 0.7706884853541851, 0.7745215762406588, 0.7736052200198174, 0.7830834295600653, 0.759472930803895, 0.7789365202188492, 0.7757960259914398, 0.7792462967336178, 0.7715232670307159, 0.7792453840374947, 0.7737851236015558, 0.7745481189340353, 0.776235731318593, 0.7795997709035873, 0.7751535531133413, 0.7851326372474432, 0.7778374012559652, 0.7725674565881491, 0.7736382633447647, 0.7752895448356867, 0.7814109418541193, 0.7451011147350073, 0.7708419859409332, 0.7826992403715849, 0.7804281450808048, 0.7702371757477522, 0.7745319977402687, 0.7745836116373539, 0.7809904403984547, 0.7798010762780905, 0.7824304606765509, 0.7748706266283989, 0.7784672267735004, 0.7834548037499189, 0.7782571204006672, 0.7816358841955662, 0.7794294599443674, 0.7792247552424669, 0.7797298673540354, 0.7717316877096891, 0.7637943234294653, 0.7825161423534155, 0.7834838703274727, 0.7796326372772455, 0.7793633919209242, 0.7741532474756241, 0.7782682776451111, 0.7852358650416136, 0.7718717027455568, 0.7738153263926506, 0.7615664228796959, 0.783882224932313, 0.7660065311938524, 0.7839230541139841, 0.7326751854270697, 0.7811986841261387, 0.7844792027026415, 0.7826346810907125, 0.7768919598311186, 0.7815648522228003, 0.7866868376731873, 0.785778071731329, 0.7820671238005161, 0.7827754225581884, 0.7643680647015572, 0.7801201939582825, 0.786359841004014, 0.7865691743791103, 0.7821870408952236, 0.7827015407383442, 0.7792994193732738, 0.7779040653258562, 0.7851204555481672, 0.7763075921684504, 0.7788197509944439, 0.7832480408251286, 0.7800133991986513, 0.7780751492828131, 0.7849175110459328, 0.78559136018157, 0.7791328430175781, 0.7699314318597317, 0.7876131497323513, 0.779059985652566, 0.7685847766697407, 0.7626824639737606, 0.784927923232317, 0.7803549338132143, 0.7764443289488554, 0.77892841771245, 0.7787477131932974, 0.7743530999869108, 0.7791138160973787, 0.7801056653261185, 0.7778230868279934, 0.7809281442314386, 0.7822569459676743, 0.7780472561717033, 0.7867624517530203, 0.7847954798489809, 0.747466878965497, 0.7738228794187307, 0.7872287556529045, 0.77220662496984, 0.7804564293473959, 0.7868815120309591, 0.7725479546934366, 0.7858848664909601, 0.7804036233574152, 0.7826521433889866, 0.782531164586544, 0.7739161886274815, 0.7773405406624079, 0.7782236859202385, 0.7730776630342007, 0.7802588120102882, 0.7855488266795874, 0.7849449571222067, 0.7896185107529163, 0.777686508372426, 0.7851430773735046, 0.7805824745446444, 0.771074527874589, 0.7896803505718708, 0.7817746233195066, 0.7732605468481779, 0.7578012254089117, 0.7694292068481445, 0.778224803507328, 0.7833667658269405, 0.7572117354720831, 0.7791272830218077, 0.7851534429937601, 0.7820684742182493, 0.7851750124245882, 0.7775081507861614, 0.7854388933628798, 0.7835877034813166, 0.7800281327217817, 0.7676240894943476, 0.7789602503180504, 0.7837388757616282, 0.7790250238031149, 0.7805466558784246, 0.789034403860569, 0.7747992500662804, 0.7823231443762779, 0.7828270271420479, 0.7818607147783041, 0.7845338899642229, 0.780012235045433, 0.786358555778861, 0.7764452043920755, 0.7747176382690668, 0.7863989192992449, 0.7748556416481733, 0.7757575809955597, 0.7644062209874392, 0.778500447049737, 0.7882322371006012, 0.7752002496272326, 0.7807349134236574, 0.7779527362436056, 0.7872375566512346], 'loss': [0.553027405645826, 0.1810476018276671, 0.15259833885888072, 0.13538880616970092, 0.12510678335285025, 0.11839375721185316, 0.11406753338770376, 0.10949992976257772, 0.10709075069813913, 0.10588243135740301, 0.10345870304813337, 0.10176767094230228, 0.09946041158527617, 0.10015845979378103, 0.09814973078400634, 0.0973473484350356, 0.09565039270428591, 0.0948363176814366, 0.09421527956689126, 0.09307902951351411, 0.0930910855665867, 0.092874978241036, 0.09224380080863977, 0.09175153546605748, 0.0912601005139252, 0.08948949359048324, 0.09044575287747414, 0.08954198310867799, 0.08939473754706333, 0.08801562270898153, 0.08808964248077925, 0.08786936741886245, 0.08769524849858644, 0.08734904381041744, 0.08771777936334572, 0.08630683338173288, 0.08694577307258664, 0.08553497237686263, 0.08570178716758464, 0.08511333772074031, 0.08483431879459143, 0.08446549257864946, 0.0847854787256364, 0.08388053034242332, 0.0837069392837069, 0.08414606252526993, 0.0833502090745435, 0.0828025238230293, 0.08341451352469748, 0.08259791189371203, 0.08210241212874765, 0.0818969259386772, 0.08191754061053948, 0.08104669172509395, 0.0819103830220552, 0.08147223279368342, 0.08064785655802158, 0.08046649619149208, 0.08082091227689728, 0.0802413287019742, 0.08031743961464614, 0.08020193843283317, 0.07972421168768545, 0.07953327252380563, 0.07877713967120313, 0.07916857634292422, 0.07880176499051417, 0.07899057711015266, 0.07886766837188089, 0.07850990866032699, 0.07879656517906958, 0.07812490104976273, 0.07768600408138887, 0.07805293051311055, 0.07760610969948604, 0.07824157212767933, 0.0776109481703235, 0.07806206834896927, 0.07772260520832733, 0.07739339557713203, 0.07826358352241249, 0.07762096416405564, 0.07685529419909277, 0.0764622534867071, 0.07727797054454634, 0.07636381817008403, 0.07656642936790767, 0.07656609533917091, 0.07635168267055381, 0.07610190969141875, 0.0759736666408471, 0.07593109624325516, 0.07624231859439132, 0.07617404656334317, 0.07646816976275804, 0.07551283222087878, 0.07593226582469793, 0.0761623489804089, 0.07592880616400166, 0.07559808658006799, 0.07587814079802101, 0.07609801096282272, 0.07513991111017997, 0.07481479100416426, 0.07458128094667962, 0.07511264618052912, 0.07499394738819246, 0.07449765533314455, 0.07471464837102632, 0.07496807445241029, 0.07426257323951094, 0.07506983384638903, 0.07368161612274382, 0.07446725838569325, 0.07501572821961472, 0.0745580643908867, 0.07526465852743687, 0.07431166812663255, 0.07434556830470551, 0.07383681580933306, 0.07426089044533513, 0.07397685197898426, 0.07455404212089578, 0.07391979850363797, 0.07371163210678255, 0.07355596558033443, 0.07318243659462291, 0.07411796732892126, 0.07395735291968215, 0.07358281372847132, 0.07286088108761919, 0.07369775481957501, 0.07356489485274059, 0.07328397263998393, 0.07344061082402796, 0.07364171740406479, 0.07379784858808834, 0.07358186475069553, 0.07349057601548604, 0.07306413368590413, 0.07295027431011879, 0.07385895533033388, 0.07332230964399622, 0.07301091882681007, 0.07323901230181304, 0.07217560367719546, 0.07210639900649841, 0.07254975462451177, 0.07266427130260174, 0.07210483996579979, 0.07289158638713458, 0.07386478238120248, 0.07221294329673193, 0.07284282894024481, 0.07226652198731795, 0.07267894901216647, 0.07335065633484444, 0.07233547591572372, 0.07203172306638707, 0.07269430494140883, 0.07179594662356864, 0.07220069620209911, 0.07196383295442026, 0.07182928107084194, 0.07167529339124215, 0.0718028224126912, 0.07193041879060903, 0.0713987178871984, 0.07203235949416796, 0.07198187512830047, 0.07184743915851266, 0.07148086638179572, 0.07193105025653004, 0.07213039844111517, 0.07186348415089704, 0.07199845408069225, 0.0719152181462861, 0.07205587561662444, 0.07143223628512559, 0.07158414126732311, 0.0710190146372884, 0.07154620685392284, 0.070765364974429, 0.0719229620250001, 0.07113381812161335, 0.07103404296345449, 0.07135443630670163, 0.07228219776218592, 0.07129993072519517, 0.07150718990068022, 0.07125398280118295, 0.07105755997059707, 0.07139068009879125, 0.07102747070260602, 0.07196847657184298, 0.07087461332118275, 0.07091882776291969, 0.07113618910679685, 0.07089472837392095, 0.07086778761421027, 0.07121143978471275, 0.0709814463917478, 0.0702994560931699, 0.07041857827749556, 0.07047304997258075, 0.07062154565843137, 0.0702955348526883, 0.07072834244128427, 0.07078022852391126, 0.07037589251449733, 0.07079051587303452, 0.07164955441182647, 0.07075378933015346, 0.07059234056409275, 0.07017274623912248, 0.07070951629916979, 0.07065644301885274, 0.07030052639975966, 0.0704734255365514, 0.07010036926667658, 0.06999898422520252, 0.06999142933301468, 0.0701133135138758, 0.07038714831704267, 0.07037292418007486, 0.0701351767050738, 0.06998885760545398, 0.06949698430433761, 0.07012217500175322, 0.07004968457610415, 0.07032332343211764, 0.07020711078304952, 0.0700466724120637, 0.07005371003295606, 0.06953290231206476, 0.07045785951686401, 0.0699320199039252, 0.07041260848745688, 0.06961641576480018, 0.06965543343003333, 0.07013578020340061, 0.0702056709529389, 0.0694809353490442, 0.07069184998054877, 0.06958613074961904, 0.06997145430262917, 0.06927730404364255, 0.06942069791434838, 0.06956215176752556, 0.07008152505644968, 0.069642148928833, 0.06938560745300448, 0.0695863115515002, 0.06942074853622623, 0.06941154475662646, 0.06925136078440369, 0.06959673491894915, 0.06976094864766759, 0.06926952436414, 0.06904528020126169, 0.06904363793817508, 0.06927172217980777, 0.0693593928509263, 0.06957648256024279, 0.06923774025829617], 'acc': [0.9098067033736842, 0.9828771518624949, 0.9851572211090386, 0.9865526162928455, 0.9874367953814931, 0.9880613697673492, 0.9883923170540797, 0.9887896159723742, 0.9890165698989504, 0.9891170468897691, 0.9893409164023618, 0.9894653221269206, 0.9896796119535816, 0.989636106954796, 0.9898311858450313, 0.9898579492446533, 0.9900130480441314, 0.9901017504825926, 0.9901279380779393, 0.9902396209635801, 0.9902325134403658, 0.9902758134066525, 0.9903400371170797, 0.9903656920517475, 0.9903855546573126, 0.9905384759793814, 0.9904837213950912, 0.9905562401648887, 0.9905815993270018, 0.9906731744572664, 0.9907159556477718, 0.9907077680111097, 0.9907258016882811, 0.9907106424757478, 0.9907278080317317, 0.990827594634976, 0.9907975710821966, 0.9909027846512983, 0.9908779589878054, 0.9909301631830427, 0.9909553649991428, 0.9909922403575679, 0.9909715564724348, 0.9910571053771312, 0.9910661931568091, 0.9910542846446228, 0.9911016847276516, 0.9911237384046677, 0.9910836888617224, 0.991189572777055, 0.991184671719869, 0.9912506902800962, 0.9912307460479541, 0.9913015345379189, 0.991258192577608, 0.9912845991388269, 0.991353944368142, 0.9913719561895843, 0.9913357601246657, 0.9914178019951095, 0.9913778199791645, 0.9913914597476278, 0.9914280028172565, 0.9914534412998763, 0.9915181427200289, 0.9914789718660284, 0.9915076872329339, 0.9915101299568988, 0.991521252950919, 0.9915327430048428, 0.9915241522698698, 0.9915576155168803, 0.9916048309134595, 0.9915716332384861, 0.9916201448681546, 0.9915645017612267, 0.9916290038413421, 0.9915786323763982, 0.9916282999843785, 0.9916406695097876, 0.9915465711716286, 0.9916288511974963, 0.9916969122088328, 0.9917062191759873, 0.991671195010268, 0.9917520308959937, 0.9917426458756323, 0.9917584077273427, 0.9917310590360089, 0.9917569643628158, 0.9917619176563021, 0.9917878526226906, 0.9917535240730232, 0.9917773840263134, 0.9917250493892161, 0.991849905448438, 0.9917935707628125, 0.991780469977677, 0.991800436626484, 0.9918087017831174, 0.9917997663702798, 0.9917823029087993, 0.991850809365653, 0.9918699553650792, 0.9919021924752259, 0.9918501420872572, 0.9918805468202926, 0.9918918866333929, 0.9918943649387061, 0.9918585186762525, 0.9919094462988614, 0.9918660079960597, 0.9919589291857664, 0.9919265220981588, 0.9918719824354616, 0.9919119301373581, 0.9918764806990426, 0.9919180739378838, 0.9919186614594893, 0.9919683663731768, 0.9919195047332889, 0.9919656115887375, 0.9919318515234768, 0.9919643464425832, 0.991983316010322, 0.9920063986171064, 0.9920141714866033, 0.9919512603595113, 0.9919806730529773, 0.9919939398266954, 0.9920475988960354, 0.9919991696131579, 0.9919965464686272, 0.992007302776701, 0.991990072249092, 0.9919881662439355, 0.9919990615256367, 0.9920134193129665, 0.9920301803856122, 0.9920400176381239, 0.9920579945984321, 0.9920012248065173, 0.9920409542212049, 0.9920509212646139, 0.992026385431909, 0.9921106837401618, 0.9921141716194884, 0.9920756050666977, 0.9920603174759949, 0.9921432134857767, 0.9920768904065482, 0.9920559952355185, 0.9921019768914349, 0.992083415255752, 0.9921115657046958, 0.9920868176927519, 0.9920483819488534, 0.9921148038774719, 0.9921425164362218, 0.9920659554922943, 0.9921398086180543, 0.9921105367403018, 0.9921227452840015, 0.9921535443966017, 0.9921491530640444, 0.9921590616039051, 0.9921439497316009, 0.9921554673544909, 0.992109443482946, 0.9921220681095866, 0.9921481796807912, 0.992162706531464, 0.9921524535076425, 0.9921327962115817, 0.9921594612050225, 0.9921514205405199, 0.9921552409579477, 0.9921490469986628, 0.9921693441842067, 0.992156288938765, 0.9922066423083288, 0.9921669314045295, 0.9922238320122616, 0.9921730335849883, 0.992212738193839, 0.9922089333371915, 0.9921917951839705, 0.992130726731667, 0.9922123413835514, 0.9921776089668828, 0.9921894006035504, 0.9922015746946989, 0.9921993950497222, 0.9922241558731676, 0.9921468488981988, 0.9922334910187002, 0.9922299604597609, 0.9922062489675343, 0.9922410985505773, 0.9922442667725555, 0.9922015684690018, 0.9922293456669703, 0.992268774870091, 0.9922308582413054, 0.9922573121843042, 0.9922357637929047, 0.9922614196901663, 0.9922704806280175, 0.992250604954721, 0.9922581133463525, 0.9922546551689337, 0.9921506597727894, 0.9922499847049232, 0.9922764120069242, 0.992273725961384, 0.9922460150992924, 0.9922499653976443, 0.9922860719551813, 0.992268356024021, 0.9923021966147619, 0.9923166967898112, 0.9923152230100875, 0.9922883544176508, 0.9922671790279217, 0.992316887348776, 0.9922925144437438, 0.9923098210162702, 0.9923581946072048, 0.9923221538592343, 0.9923170730462955, 0.9922767033058412, 0.9922849335043885, 0.9923233453565682, 0.9923138002409148, 0.9923279796367436, 0.9922854303482643, 0.9923331210515468, 0.992293201209472, 0.9923396736913211, 0.9923492979058769, 0.9923155312617371, 0.9923056850619497, 0.9923481088461908, 0.9922784123601323, 0.9923414035625708, 0.9923147820520584, 0.9923713481480255, 0.9923533809244585, 0.9923713115279068, 0.9923359820952714, 0.9923485302545613, 0.9923406297063334, 0.9923211156913347, 0.9923673432519021, 0.9923443524170188, 0.9923542919330125, 0.9923415869263195, 0.9923375208604706, 0.9923832425398598, 0.9923825944371654, 0.9923868962900417, 0.9923507781329022, 0.9923602793570441, 0.9923356659143412, 0.9923673569775215], 'mDice': [0.4594943986529775, 0.7076775282569445, 0.7454605444086766, 0.7699482736097779, 0.7850402042856612, 0.7952952420336175, 0.8018825727638168, 0.8089164250733837, 0.8126912756194439, 0.8145639740992773, 0.8184349836061623, 0.8210633350219186, 0.8246993748405136, 0.8235659741615742, 0.8267565457762999, 0.8279939739770891, 0.8308120495463908, 0.8320726288158581, 0.8330109605075064, 0.8348126932208209, 0.8348072322248457, 0.8351547844016318, 0.8361670833729805, 0.8370212794693697, 0.8377609044552348, 0.8406105598354084, 0.8391145219492057, 0.8405532947716237, 0.8407659785692404, 0.8430486030662823, 0.8429899014176787, 0.8432998290194359, 0.8435404918758077, 0.8441445101037374, 0.8435176258734243, 0.8458202674411877, 0.8447998333534076, 0.8471046466091666, 0.8468585983516033, 0.8478761805853141, 0.8482500359448902, 0.8488537664264977, 0.8483813360785308, 0.8498306208833025, 0.8501498692055343, 0.8494595705638769, 0.8507387734697687, 0.8516093293687282, 0.8505961888063878, 0.8519738560854745, 0.8527611118652714, 0.8531804306382181, 0.8531165559269402, 0.8544819707084896, 0.8531744470714872, 0.85385186050749, 0.855184547055179, 0.8555312807501518, 0.8549789571936598, 0.8559059376803594, 0.8557660664722065, 0.8559012855748841, 0.8567135970504134, 0.8570448655110973, 0.8582931864589879, 0.8576426083055179, 0.8582227744332614, 0.8579651501323615, 0.8581689441468612, 0.858755449730995, 0.8583056192108878, 0.8594059000810819, 0.8600949356333504, 0.8595253672280683, 0.8602770256971224, 0.8591686304305036, 0.8602479446691905, 0.8595071615789969, 0.8600812765299076, 0.8605913842916905, 0.8591722182880174, 0.860217307473193, 0.8615311462385834, 0.8621444458937665, 0.8608207333111466, 0.8623182600877198, 0.8620495383881022, 0.8620696731722838, 0.8623486974658459, 0.8627888641388447, 0.8629783255942846, 0.8630992964722075, 0.8625558855794816, 0.8626546070414219, 0.8621264747159467, 0.8638284889555038, 0.8630808793918118, 0.8626568984165416, 0.8631138783153846, 0.863606247417711, 0.8631560050251454, 0.8628338202029082, 0.864422052333451, 0.8649085496615155, 0.8653324027226004, 0.8644697197890523, 0.8646946656062682, 0.865453605121667, 0.8651145550865904, 0.864669474641856, 0.8658550147660119, 0.8645226025368066, 0.8668102283116014, 0.8655302734952279, 0.8646191124565942, 0.8653391244464931, 0.8642776284232239, 0.865783076135625, 0.8657217718339678, 0.8665944785277879, 0.8659115235080034, 0.8663292556838373, 0.8654786043498413, 0.8664230182152308, 0.8667839014309963, 0.8670692439138564, 0.8676396268120734, 0.8660905713872709, 0.866412506378305, 0.8670336611805248, 0.8682042008647385, 0.8668304866748334, 0.8670601093064094, 0.867485741663439, 0.8672240366015518, 0.866937490294616, 0.8666749826718031, 0.8670411834986046, 0.8672151419569892, 0.8678928644756579, 0.8680788904889004, 0.8666022243684796, 0.8675352347995452, 0.8679725421878038, 0.8676111676474959, 0.8693519901894132, 0.8694880528151955, 0.8687799879243402, 0.8685745671815651, 0.8695442726612922, 0.8681729381696901, 0.8666362333300499, 0.8693309444797791, 0.8682556110792935, 0.8692333458316746, 0.8685634610571761, 0.8674365427763575, 0.8691266565230357, 0.869646075483231, 0.8685135622396831, 0.8700165270761814, 0.8693388545431436, 0.869724841985994, 0.8699919747131771, 0.8702223864709152, 0.8700136307696646, 0.8697875492440099, 0.8706894879337247, 0.8696705740379292, 0.8697380835522173, 0.8699664218065367, 0.8705736273213437, 0.8698186001268513, 0.8695218108790919, 0.8699257713626234, 0.8696967284618748, 0.8698138931119734, 0.8696049165542884, 0.8706159562866239, 0.8704015183520812, 0.8713760143776755, 0.8704381108602552, 0.8717691265734782, 0.8698411969215014, 0.8711779755458497, 0.8712821172493405, 0.870782170123141, 0.8692531896285705, 0.8708897136057435, 0.8705379908489574, 0.8709295189545464, 0.8712701386894044, 0.8706864292604716, 0.8713558049751544, 0.869739593051792, 0.8716037665923686, 0.8715415416908442, 0.8711291628528226, 0.8715541699185554, 0.8716042091639543, 0.87105870389545, 0.8714165826271507, 0.872542656533073, 0.8723455771857526, 0.8722710953823044, 0.8720110546798813, 0.8725791567568858, 0.8718497655429643, 0.8717876596222, 0.8724247527008925, 0.8717176921694235, 0.870290402406542, 0.8718009037160979, 0.8721097277281965, 0.8727926231918122, 0.8718792819749662, 0.8719942571394699, 0.872563621322494, 0.8722871007841196, 0.8729121913696064, 0.8730802051389731, 0.8730592076282849, 0.8729260614334644, 0.8724274671741157, 0.8724445365330366, 0.8728544307846468, 0.8730823208718705, 0.8739291861717385, 0.8729072983317278, 0.873020297637229, 0.8725123662448336, 0.8727190320370185, 0.8729849623251532, 0.8729805478972675, 0.8738564743845529, 0.8723198691837682, 0.8732258662801317, 0.8723744502518531, 0.8736827028702122, 0.8736474075161705, 0.8728381702192738, 0.872779321612361, 0.873953454680347, 0.8719727866073733, 0.8737929051202447, 0.8731346151146502, 0.8742995202105068, 0.8740772853970209, 0.8738052566541505, 0.8729381627137339, 0.8736865557526772, 0.8740839459788886, 0.8737575818163545, 0.8740524567557195, 0.8740798314301633, 0.8743276439076505, 0.8737587549482279, 0.8735035351722432, 0.8743211693834092, 0.8746955995500413, 0.8747135211577381, 0.8742903320025079, 0.8741423319936478, 0.8737907228229408, 0.8743794367221143]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:03,  1.19it/s]predicting test subjects:  40%|████      | 2/5 [00:01<00:01,  1.54it/s]predicting test subjects:  60%|██████    | 3/5 [00:01<00:01,  1.95it/s]predicting test subjects:  80%|████████  | 4/5 [00:01<00:00,  2.38it/s]predicting test subjects: 100%|██████████| 5/5 [00:01<00:00,  2.79it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:12,  3.68it/s]predicting train subjects:   1%|          | 2/266 [00:00<01:10,  3.72it/s]predicting train subjects:   1%|          | 3/266 [00:00<01:14,  3.52it/s]predicting train subjects:   2%|▏         | 4/266 [00:01<01:13,  3.59it/s]predicting train subjects:   2%|▏         | 5/266 [00:01<01:11,  3.65it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<01:09,  3.75it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<01:07,  3.81it/s]predicting train subjects:   3%|▎         | 8/266 [00:02<01:06,  3.89it/s]predicting train subjects:   3%|▎         | 9/266 [00:02<01:04,  3.96it/s]predicting train subjects:   4%|▍         | 10/266 [00:02<01:04,  3.96it/s]predicting train subjects:   4%|▍         | 11/266 [00:02<01:03,  4.01it/s]predicting train subjects:   5%|▍         | 12/266 [00:03<01:03,  4.02it/s]predicting train subjects:   5%|▍         | 13/266 [00:03<01:07,  3.73it/s]predicting train subjects:   5%|▌         | 14/266 [00:03<01:11,  3.54it/s]predicting train subjects:   6%|▌         | 15/266 [00:04<01:13,  3.41it/s]predicting train subjects:   6%|▌         | 16/266 [00:04<01:14,  3.38it/s]predicting train subjects:   6%|▋         | 17/266 [00:04<01:09,  3.56it/s]predicting train subjects:   7%|▋         | 18/266 [00:04<01:08,  3.61it/s]predicting train subjects:   7%|▋         | 19/266 [00:05<01:05,  3.74it/s]predicting train subjects:   8%|▊         | 20/266 [00:05<01:04,  3.82it/s]predicting train subjects:   8%|▊         | 21/266 [00:05<01:04,  3.78it/s]predicting train subjects:   8%|▊         | 22/266 [00:05<01:04,  3.80it/s]predicting train subjects:   9%|▊         | 23/266 [00:06<01:04,  3.78it/s]predicting train subjects:   9%|▉         | 24/266 [00:06<01:02,  3.90it/s]predicting train subjects:   9%|▉         | 25/266 [00:06<01:02,  3.86it/s]predicting train subjects:  10%|▉         | 26/266 [00:06<01:00,  3.98it/s]predicting train subjects:  10%|█         | 27/266 [00:07<00:58,  4.08it/s]predicting train subjects:  11%|█         | 28/266 [00:07<00:57,  4.17it/s]predicting train subjects:  11%|█         | 29/266 [00:07<00:57,  4.10it/s]predicting train subjects:  11%|█▏        | 30/266 [00:07<00:56,  4.16it/s]predicting train subjects:  12%|█▏        | 31/266 [00:08<00:55,  4.23it/s]predicting train subjects:  12%|█▏        | 32/266 [00:08<00:55,  4.23it/s]predicting train subjects:  12%|█▏        | 33/266 [00:08<01:01,  3.80it/s]predicting train subjects:  13%|█▎        | 34/266 [00:08<00:58,  3.94it/s]predicting train subjects:  13%|█▎        | 35/266 [00:09<00:57,  4.04it/s]predicting train subjects:  14%|█▎        | 36/266 [00:09<00:56,  4.10it/s]predicting train subjects:  14%|█▍        | 37/266 [00:09<00:55,  4.15it/s]predicting train subjects:  14%|█▍        | 38/266 [00:09<00:54,  4.20it/s]predicting train subjects:  15%|█▍        | 39/266 [00:10<00:54,  4.17it/s]predicting train subjects:  15%|█▌        | 40/266 [00:10<00:53,  4.21it/s]predicting train subjects:  15%|█▌        | 41/266 [00:10<00:52,  4.26it/s]predicting train subjects:  16%|█▌        | 42/266 [00:10<00:49,  4.51it/s]predicting train subjects:  16%|█▌        | 43/266 [00:10<00:48,  4.60it/s]predicting train subjects:  17%|█▋        | 44/266 [00:11<00:46,  4.78it/s]predicting train subjects:  17%|█▋        | 45/266 [00:11<00:47,  4.66it/s]predicting train subjects:  17%|█▋        | 46/266 [00:11<00:45,  4.81it/s]predicting train subjects:  18%|█▊        | 47/266 [00:11<00:44,  4.91it/s]predicting train subjects:  18%|█▊        | 48/266 [00:11<00:43,  4.97it/s]predicting train subjects:  18%|█▊        | 49/266 [00:12<00:43,  5.03it/s]predicting train subjects:  19%|█▉        | 50/266 [00:12<00:42,  5.11it/s]predicting train subjects:  19%|█▉        | 51/266 [00:12<00:43,  4.98it/s]predicting train subjects:  20%|█▉        | 52/266 [00:12<00:42,  5.03it/s]predicting train subjects:  20%|█▉        | 53/266 [00:12<00:41,  5.09it/s]predicting train subjects:  20%|██        | 54/266 [00:13<00:41,  5.13it/s]predicting train subjects:  21%|██        | 55/266 [00:13<00:41,  5.12it/s]predicting train subjects:  21%|██        | 56/266 [00:13<00:43,  4.86it/s]predicting train subjects:  21%|██▏       | 57/266 [00:13<00:42,  4.96it/s]predicting train subjects:  22%|██▏       | 58/266 [00:13<00:41,  4.97it/s]predicting train subjects:  22%|██▏       | 59/266 [00:14<00:40,  5.06it/s]predicting train subjects:  23%|██▎       | 60/266 [00:14<00:40,  5.12it/s]predicting train subjects:  23%|██▎       | 61/266 [00:14<00:40,  5.07it/s]predicting train subjects:  23%|██▎       | 62/266 [00:14<00:39,  5.15it/s]predicting train subjects:  24%|██▎       | 63/266 [00:14<00:41,  4.92it/s]predicting train subjects:  24%|██▍       | 64/266 [00:15<00:41,  4.92it/s]predicting train subjects:  24%|██▍       | 65/266 [00:15<00:40,  5.02it/s]predicting train subjects:  25%|██▍       | 66/266 [00:15<00:39,  5.12it/s]predicting train subjects:  25%|██▌       | 67/266 [00:15<00:43,  4.62it/s]predicting train subjects:  26%|██▌       | 68/266 [00:15<00:44,  4.46it/s]predicting train subjects:  26%|██▌       | 69/266 [00:16<00:41,  4.71it/s]predicting train subjects:  26%|██▋       | 70/266 [00:16<00:40,  4.83it/s]predicting train subjects:  27%|██▋       | 71/266 [00:16<00:39,  4.96it/s]predicting train subjects:  27%|██▋       | 72/266 [00:16<00:38,  5.04it/s]predicting train subjects:  27%|██▋       | 73/266 [00:16<00:37,  5.13it/s]predicting train subjects:  28%|██▊       | 74/266 [00:17<00:36,  5.19it/s]predicting train subjects:  28%|██▊       | 75/266 [00:17<00:36,  5.23it/s]predicting train subjects:  29%|██▊       | 76/266 [00:17<00:36,  5.23it/s]predicting train subjects:  29%|██▉       | 77/266 [00:17<00:36,  5.22it/s]predicting train subjects:  29%|██▉       | 78/266 [00:17<00:39,  4.79it/s]predicting train subjects:  30%|██▉       | 79/266 [00:18<00:41,  4.56it/s]predicting train subjects:  30%|███       | 80/266 [00:18<00:42,  4.33it/s]predicting train subjects:  30%|███       | 81/266 [00:18<00:46,  3.98it/s]predicting train subjects:  31%|███       | 82/266 [00:18<00:45,  4.04it/s]predicting train subjects:  31%|███       | 83/266 [00:19<00:44,  4.08it/s]predicting train subjects:  32%|███▏      | 84/266 [00:19<00:44,  4.09it/s]predicting train subjects:  32%|███▏      | 85/266 [00:19<00:44,  4.10it/s]predicting train subjects:  32%|███▏      | 86/266 [00:19<00:44,  4.07it/s]predicting train subjects:  33%|███▎      | 87/266 [00:20<00:43,  4.10it/s]predicting train subjects:  33%|███▎      | 88/266 [00:20<00:44,  4.02it/s]predicting train subjects:  33%|███▎      | 89/266 [00:20<00:43,  4.08it/s]predicting train subjects:  34%|███▍      | 90/266 [00:20<00:42,  4.14it/s]predicting train subjects:  34%|███▍      | 91/266 [00:21<00:42,  4.09it/s]predicting train subjects:  35%|███▍      | 92/266 [00:21<00:41,  4.15it/s]predicting train subjects:  35%|███▍      | 93/266 [00:21<00:42,  4.11it/s]predicting train subjects:  35%|███▌      | 94/266 [00:21<00:44,  3.85it/s]predicting train subjects:  36%|███▌      | 95/266 [00:22<00:46,  3.71it/s]predicting train subjects:  36%|███▌      | 96/266 [00:22<00:48,  3.47it/s]predicting train subjects:  36%|███▋      | 97/266 [00:22<00:47,  3.54it/s]predicting train subjects:  37%|███▋      | 98/266 [00:23<00:46,  3.62it/s]predicting train subjects:  37%|███▋      | 99/266 [00:23<00:46,  3.59it/s]predicting train subjects:  38%|███▊      | 100/266 [00:23<00:42,  3.93it/s]predicting train subjects:  38%|███▊      | 101/266 [00:23<00:39,  4.19it/s]predicting train subjects:  38%|███▊      | 102/266 [00:23<00:37,  4.39it/s]predicting train subjects:  39%|███▊      | 103/266 [00:24<00:35,  4.55it/s]predicting train subjects:  39%|███▉      | 104/266 [00:24<00:34,  4.67it/s]predicting train subjects:  39%|███▉      | 105/266 [00:24<00:35,  4.50it/s]predicting train subjects:  40%|███▉      | 106/266 [00:24<00:35,  4.46it/s]predicting train subjects:  40%|████      | 107/266 [00:25<00:36,  4.38it/s]predicting train subjects:  41%|████      | 108/266 [00:25<00:34,  4.55it/s]predicting train subjects:  41%|████      | 109/266 [00:25<00:34,  4.54it/s]predicting train subjects:  41%|████▏     | 110/266 [00:25<00:34,  4.47it/s]predicting train subjects:  42%|████▏     | 111/266 [00:25<00:33,  4.61it/s]predicting train subjects:  42%|████▏     | 112/266 [00:26<00:32,  4.69it/s]predicting train subjects:  42%|████▏     | 113/266 [00:26<00:32,  4.69it/s]predicting train subjects:  43%|████▎     | 114/266 [00:26<00:33,  4.51it/s]predicting train subjects:  43%|████▎     | 115/266 [00:26<00:33,  4.56it/s]predicting train subjects:  44%|████▎     | 116/266 [00:27<00:32,  4.65it/s]predicting train subjects:  44%|████▍     | 117/266 [00:27<00:31,  4.67it/s]predicting train subjects:  44%|████▍     | 118/266 [00:27<00:31,  4.76it/s]predicting train subjects:  45%|████▍     | 119/266 [00:27<00:33,  4.40it/s]predicting train subjects:  45%|████▌     | 120/266 [00:28<00:36,  4.02it/s]predicting train subjects:  45%|████▌     | 121/266 [00:28<00:38,  3.81it/s]predicting train subjects:  46%|████▌     | 122/266 [00:28<00:36,  3.93it/s]predicting train subjects:  46%|████▌     | 123/266 [00:28<00:37,  3.78it/s]predicting train subjects:  47%|████▋     | 124/266 [00:29<00:38,  3.68it/s]predicting train subjects:  47%|████▋     | 125/266 [00:29<00:39,  3.57it/s]predicting train subjects:  47%|████▋     | 126/266 [00:29<00:37,  3.69it/s]predicting train subjects:  48%|████▊     | 127/266 [00:29<00:36,  3.82it/s]predicting train subjects:  48%|████▊     | 128/266 [00:30<00:35,  3.93it/s]predicting train subjects:  48%|████▊     | 129/266 [00:30<00:34,  4.02it/s]predicting train subjects:  49%|████▉     | 130/266 [00:30<00:33,  4.01it/s]predicting train subjects:  49%|████▉     | 131/266 [00:30<00:33,  4.04it/s]predicting train subjects:  50%|████▉     | 132/266 [00:31<00:33,  4.02it/s]predicting train subjects:  50%|█████     | 133/266 [00:31<00:32,  4.07it/s]predicting train subjects:  50%|█████     | 134/266 [00:31<00:32,  4.12it/s]predicting train subjects:  51%|█████     | 135/266 [00:31<00:32,  4.01it/s]predicting train subjects:  51%|█████     | 136/266 [00:32<00:34,  3.76it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:32<00:34,  3.70it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:32<00:32,  3.94it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:32<00:31,  4.06it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:33<00:30,  4.13it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:33<00:30,  4.11it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:33<00:29,  4.17it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:33<00:29,  4.22it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:34<00:28,  4.25it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:34<00:27,  4.34it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:34<00:27,  4.37it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:34<00:27,  4.34it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:34<00:27,  4.28it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:35<00:27,  4.19it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:35<00:29,  3.99it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:35<00:27,  4.14it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:35<00:26,  4.26it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:36<00:25,  4.36it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:36<00:25,  4.35it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:36<00:24,  4.55it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:36<00:22,  4.90it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:36<00:21,  5.05it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:37<00:20,  5.26it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:37<00:19,  5.42it/s]predicting train subjects:  60%|██████    | 160/266 [00:37<00:19,  5.56it/s]predicting train subjects:  61%|██████    | 161/266 [00:37<00:19,  5.42it/s]predicting train subjects:  61%|██████    | 162/266 [00:37<00:18,  5.56it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:37<00:18,  5.56it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:38<00:18,  5.64it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:38<00:17,  5.74it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:38<00:17,  5.63it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:38<00:17,  5.74it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:38<00:16,  5.77it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:39<00:16,  5.81it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:39<00:16,  5.88it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:39<00:17,  5.47it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:39<00:18,  5.15it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:39<00:19,  4.67it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:40<00:20,  4.39it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:40<00:21,  4.28it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:40<00:20,  4.47it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:40<00:19,  4.63it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:40<00:18,  4.79it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:41<00:18,  4.74it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:41<00:18,  4.74it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:41<00:17,  4.85it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:41<00:17,  4.72it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:42<00:18,  4.45it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:42<00:17,  4.66it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:42<00:16,  4.84it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:42<00:17,  4.52it/s]predicting train subjects:  70%|███████   | 187/266 [00:42<00:18,  4.27it/s]predicting train subjects:  71%|███████   | 188/266 [00:43<00:17,  4.34it/s]predicting train subjects:  71%|███████   | 189/266 [00:43<00:17,  4.50it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:43<00:17,  4.38it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:43<00:16,  4.56it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:44<00:16,  4.38it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:44<00:15,  4.62it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:44<00:17,  4.19it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:44<00:16,  4.31it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:44<00:15,  4.49it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:45<00:14,  4.67it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:45<00:14,  4.80it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:45<00:13,  4.87it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:45<00:13,  4.92it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:45<00:13,  4.96it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:46<00:12,  4.99it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:46<00:13,  4.51it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:46<00:13,  4.59it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:46<00:13,  4.66it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:47<00:12,  4.74it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:47<00:12,  4.82it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:47<00:12,  4.73it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:47<00:11,  4.82it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:47<00:11,  4.89it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:48<00:11,  4.89it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:48<00:10,  4.94it/s]predicting train subjects:  80%|████████  | 213/266 [00:48<00:10,  5.09it/s]predicting train subjects:  80%|████████  | 214/266 [00:48<00:10,  5.14it/s]predicting train subjects:  81%|████████  | 215/266 [00:48<00:10,  4.97it/s]predicting train subjects:  81%|████████  | 216/266 [00:49<00:10,  4.90it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:49<00:09,  4.96it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:49<00:09,  5.04it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:49<00:09,  5.13it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:49<00:08,  5.21it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:50<00:08,  5.13it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:50<00:08,  5.24it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:50<00:08,  5.27it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:50<00:07,  5.27it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:50<00:08,  4.97it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:51<00:07,  5.13it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:51<00:07,  5.26it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:51<00:07,  5.34it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:51<00:06,  5.35it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:51<00:06,  5.34it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:51<00:07,  4.90it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:52<00:06,  4.90it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:52<00:06,  5.03it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:52<00:06,  5.11it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:52<00:05,  5.20it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:52<00:05,  5.25it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:53<00:05,  5.31it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:53<00:05,  5.34it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:53<00:05,  5.36it/s]predicting train subjects:  90%|█████████ | 240/266 [00:53<00:05,  5.12it/s]predicting train subjects:  91%|█████████ | 241/266 [00:53<00:04,  5.15it/s]predicting train subjects:  91%|█████████ | 242/266 [00:54<00:04,  5.08it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:54<00:04,  4.98it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:54<00:04,  5.08it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:54<00:04,  5.00it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:54<00:03,  5.09it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:55<00:03,  5.16it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:55<00:03,  5.09it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:55<00:03,  4.80it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:55<00:03,  4.57it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:55<00:03,  4.52it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:56<00:03,  4.45it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:56<00:03,  4.24it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:56<00:02,  4.23it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:56<00:02,  4.28it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:57<00:02,  4.31it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:57<00:02,  4.20it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:57<00:01,  4.17it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:57<00:01,  4.21it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:58<00:01,  4.24it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:58<00:01,  4.25it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:58<00:00,  4.29it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:58<00:00,  4.31it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:59<00:00,  4.22it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:59<00:00,  4.21it/s]predicting train subjects: 100%|██████████| 266/266 [00:59<00:00,  3.91it/s]
predicting test subjects sagittal:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects sagittal:  20%|██        | 1/5 [00:00<00:01,  3.59it/s]predicting test subjects sagittal:  40%|████      | 2/5 [00:00<00:00,  3.91it/s]predicting test subjects sagittal:  60%|██████    | 3/5 [00:00<00:00,  4.21it/s]predicting test subjects sagittal:  80%|████████  | 4/5 [00:00<00:00,  4.48it/s]predicting test subjects sagittal: 100%|██████████| 5/5 [00:01<00:00,  4.53it/s]
predicting train subjects sagittal:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/266 [00:00<01:12,  3.67it/s]predicting train subjects sagittal:   1%|          | 2/266 [00:00<01:09,  3.80it/s]predicting train subjects sagittal:   1%|          | 3/266 [00:00<01:05,  3.99it/s]predicting train subjects sagittal:   2%|▏         | 4/266 [00:01<01:08,  3.81it/s]predicting train subjects sagittal:   2%|▏         | 5/266 [00:01<01:09,  3.74it/s]predicting train subjects sagittal:   2%|▏         | 6/266 [00:01<01:12,  3.57it/s]predicting train subjects sagittal:   3%|▎         | 7/266 [00:01<01:14,  3.47it/s]predicting train subjects sagittal:   3%|▎         | 8/266 [00:02<01:16,  3.37it/s]predicting train subjects sagittal:   3%|▎         | 9/266 [00:02<01:13,  3.52it/s]predicting train subjects sagittal:   4%|▍         | 10/266 [00:02<01:15,  3.41it/s]predicting train subjects sagittal:   4%|▍         | 11/266 [00:03<01:11,  3.58it/s]predicting train subjects sagittal:   5%|▍         | 12/266 [00:03<01:09,  3.65it/s]predicting train subjects sagittal:   5%|▍         | 13/266 [00:03<01:11,  3.52it/s]predicting train subjects sagittal:   5%|▌         | 14/266 [00:03<01:13,  3.43it/s]predicting train subjects sagittal:   6%|▌         | 15/266 [00:04<01:14,  3.38it/s]predicting train subjects sagittal:   6%|▌         | 16/266 [00:04<01:13,  3.41it/s]predicting train subjects sagittal:   6%|▋         | 17/266 [00:04<01:14,  3.35it/s]predicting train subjects sagittal:   7%|▋         | 18/266 [00:05<01:10,  3.50it/s]predicting train subjects sagittal:   7%|▋         | 19/266 [00:05<01:08,  3.59it/s]predicting train subjects sagittal:   8%|▊         | 20/266 [00:05<01:07,  3.66it/s]predicting train subjects sagittal:   8%|▊         | 21/266 [00:05<01:05,  3.72it/s]predicting train subjects sagittal:   8%|▊         | 22/266 [00:06<01:04,  3.79it/s]predicting train subjects sagittal:   9%|▊         | 23/266 [00:06<01:06,  3.65it/s]predicting train subjects sagittal:   9%|▉         | 24/266 [00:06<01:04,  3.77it/s]predicting train subjects sagittal:   9%|▉         | 25/266 [00:06<01:02,  3.86it/s]predicting train subjects sagittal:  10%|▉         | 26/266 [00:07<01:01,  3.89it/s]predicting train subjects sagittal:  10%|█         | 27/266 [00:07<00:59,  4.01it/s]predicting train subjects sagittal:  11%|█         | 28/266 [00:07<00:58,  4.10it/s]predicting train subjects sagittal:  11%|█         | 29/266 [00:07<00:56,  4.18it/s]predicting train subjects sagittal:  11%|█▏        | 30/266 [00:08<00:58,  4.06it/s]predicting train subjects sagittal:  12%|█▏        | 31/266 [00:08<00:57,  4.11it/s]predicting train subjects sagittal:  12%|█▏        | 32/266 [00:08<00:57,  4.06it/s]predicting train subjects sagittal:  12%|█▏        | 33/266 [00:08<00:56,  4.10it/s]predicting train subjects sagittal:  13%|█▎        | 34/266 [00:09<00:55,  4.15it/s]predicting train subjects sagittal:  13%|█▎        | 35/266 [00:09<00:55,  4.19it/s]predicting train subjects sagittal:  14%|█▎        | 36/266 [00:09<00:54,  4.22it/s]predicting train subjects sagittal:  14%|█▍        | 37/266 [00:09<00:57,  4.01it/s]predicting train subjects sagittal:  14%|█▍        | 38/266 [00:10<00:56,  4.07it/s]predicting train subjects sagittal:  15%|█▍        | 39/266 [00:10<00:55,  4.08it/s]predicting train subjects sagittal:  15%|█▌        | 40/266 [00:10<00:55,  4.06it/s]predicting train subjects sagittal:  15%|█▌        | 41/266 [00:10<00:55,  4.08it/s]predicting train subjects sagittal:  16%|█▌        | 42/266 [00:11<00:54,  4.14it/s]predicting train subjects sagittal:  16%|█▌        | 43/266 [00:11<00:51,  4.30it/s]predicting train subjects sagittal:  17%|█▋        | 44/266 [00:11<00:49,  4.53it/s]predicting train subjects sagittal:  17%|█▋        | 45/266 [00:11<00:47,  4.70it/s]predicting train subjects sagittal:  17%|█▋        | 46/266 [00:11<00:46,  4.72it/s]predicting train subjects sagittal:  18%|█▊        | 47/266 [00:12<00:45,  4.83it/s]predicting train subjects sagittal:  18%|█▊        | 48/266 [00:12<00:44,  4.90it/s]predicting train subjects sagittal:  18%|█▊        | 49/266 [00:12<00:43,  4.96it/s]predicting train subjects sagittal:  19%|█▉        | 50/266 [00:12<00:43,  4.96it/s]predicting train subjects sagittal:  19%|█▉        | 51/266 [00:12<00:44,  4.86it/s]predicting train subjects sagittal:  20%|█▉        | 52/266 [00:13<00:44,  4.78it/s]predicting train subjects sagittal:  20%|█▉        | 53/266 [00:13<00:43,  4.88it/s]predicting train subjects sagittal:  20%|██        | 54/266 [00:13<00:42,  4.94it/s]predicting train subjects sagittal:  21%|██        | 55/266 [00:13<00:44,  4.79it/s]predicting train subjects sagittal:  21%|██        | 56/266 [00:13<00:43,  4.86it/s]predicting train subjects sagittal:  21%|██▏       | 57/266 [00:14<00:43,  4.76it/s]predicting train subjects sagittal:  22%|██▏       | 58/266 [00:14<00:42,  4.86it/s]predicting train subjects sagittal:  22%|██▏       | 59/266 [00:14<00:41,  4.97it/s]predicting train subjects sagittal:  23%|██▎       | 60/266 [00:14<00:40,  5.03it/s]predicting train subjects sagittal:  23%|██▎       | 61/266 [00:14<00:41,  4.98it/s]predicting train subjects sagittal:  23%|██▎       | 62/266 [00:15<00:43,  4.72it/s]predicting train subjects sagittal:  24%|██▎       | 63/266 [00:15<00:45,  4.51it/s]predicting train subjects sagittal:  24%|██▍       | 64/266 [00:15<00:45,  4.42it/s]predicting train subjects sagittal:  24%|██▍       | 65/266 [00:15<00:42,  4.68it/s]predicting train subjects sagittal:  25%|██▍       | 66/266 [00:15<00:41,  4.83it/s]predicting train subjects sagittal:  25%|██▌       | 67/266 [00:16<00:39,  4.98it/s]predicting train subjects sagittal:  26%|██▌       | 68/266 [00:16<00:42,  4.66it/s]predicting train subjects sagittal:  26%|██▌       | 69/266 [00:16<00:44,  4.48it/s]predicting train subjects sagittal:  26%|██▋       | 70/266 [00:16<00:45,  4.35it/s]predicting train subjects sagittal:  27%|██▋       | 71/266 [00:17<00:43,  4.48it/s]predicting train subjects sagittal:  27%|██▋       | 72/266 [00:17<00:41,  4.68it/s]predicting train subjects sagittal:  27%|██▋       | 73/266 [00:17<00:40,  4.76it/s]predicting train subjects sagittal:  28%|██▊       | 74/266 [00:17<00:39,  4.91it/s]predicting train subjects sagittal:  28%|██▊       | 75/266 [00:17<00:37,  5.03it/s]predicting train subjects sagittal:  29%|██▊       | 76/266 [00:18<00:37,  5.13it/s]predicting train subjects sagittal:  29%|██▉       | 77/266 [00:18<00:36,  5.22it/s]predicting train subjects sagittal:  29%|██▉       | 78/266 [00:18<00:40,  4.63it/s]predicting train subjects sagittal:  30%|██▉       | 79/266 [00:18<00:42,  4.44it/s]predicting train subjects sagittal:  30%|███       | 80/266 [00:19<00:43,  4.31it/s]predicting train subjects sagittal:  30%|███       | 81/266 [00:19<00:47,  3.88it/s]predicting train subjects sagittal:  31%|███       | 82/266 [00:19<00:49,  3.70it/s]predicting train subjects sagittal:  31%|███       | 83/266 [00:19<00:50,  3.60it/s]predicting train subjects sagittal:  32%|███▏      | 84/266 [00:20<00:48,  3.74it/s]predicting train subjects sagittal:  32%|███▏      | 85/266 [00:20<00:46,  3.90it/s]predicting train subjects sagittal:  32%|███▏      | 86/266 [00:20<00:44,  4.01it/s]predicting train subjects sagittal:  33%|███▎      | 87/266 [00:20<00:45,  3.97it/s]predicting train subjects sagittal:  33%|███▎      | 88/266 [00:21<00:44,  4.04it/s]predicting train subjects sagittal:  33%|███▎      | 89/266 [00:21<00:42,  4.12it/s]predicting train subjects sagittal:  34%|███▍      | 90/266 [00:21<00:43,  4.04it/s]predicting train subjects sagittal:  34%|███▍      | 91/266 [00:21<00:42,  4.10it/s]predicting train subjects sagittal:  35%|███▍      | 92/266 [00:22<00:41,  4.17it/s]predicting train subjects sagittal:  35%|███▍      | 93/266 [00:22<00:41,  4.20it/s]predicting train subjects sagittal:  35%|███▌      | 94/266 [00:22<00:40,  4.22it/s]predicting train subjects sagittal:  36%|███▌      | 95/266 [00:22<00:40,  4.21it/s]predicting train subjects sagittal:  36%|███▌      | 96/266 [00:22<00:39,  4.36it/s]predicting train subjects sagittal:  36%|███▋      | 97/266 [00:23<00:40,  4.17it/s]predicting train subjects sagittal:  37%|███▋      | 98/266 [00:23<00:44,  3.78it/s]predicting train subjects sagittal:  37%|███▋      | 99/266 [00:23<00:42,  3.94it/s]predicting train subjects sagittal:  38%|███▊      | 100/266 [00:24<00:39,  4.25it/s]predicting train subjects sagittal:  38%|███▊      | 101/266 [00:24<00:37,  4.41it/s]predicting train subjects sagittal:  38%|███▊      | 102/266 [00:24<00:36,  4.47it/s]predicting train subjects sagittal:  39%|███▊      | 103/266 [00:24<00:35,  4.62it/s]predicting train subjects sagittal:  39%|███▉      | 104/266 [00:24<00:34,  4.71it/s]predicting train subjects sagittal:  39%|███▉      | 105/266 [00:25<00:33,  4.81it/s]predicting train subjects sagittal:  40%|███▉      | 106/266 [00:25<00:32,  4.86it/s]predicting train subjects sagittal:  40%|████      | 107/266 [00:25<00:33,  4.80it/s]predicting train subjects sagittal:  41%|████      | 108/266 [00:25<00:32,  4.81it/s]predicting train subjects sagittal:  41%|████      | 109/266 [00:25<00:32,  4.84it/s]predicting train subjects sagittal:  41%|████▏     | 110/266 [00:26<00:32,  4.86it/s]predicting train subjects sagittal:  42%|████▏     | 111/266 [00:26<00:32,  4.76it/s]predicting train subjects sagittal:  42%|████▏     | 112/266 [00:26<00:31,  4.82it/s]predicting train subjects sagittal:  42%|████▏     | 113/266 [00:26<00:31,  4.88it/s]predicting train subjects sagittal:  43%|████▎     | 114/266 [00:26<00:30,  4.92it/s]predicting train subjects sagittal:  43%|████▎     | 115/266 [00:27<00:30,  4.90it/s]predicting train subjects sagittal:  44%|████▎     | 116/266 [00:27<00:32,  4.64it/s]predicting train subjects sagittal:  44%|████▍     | 117/266 [00:27<00:34,  4.35it/s]predicting train subjects sagittal:  44%|████▍     | 118/266 [00:27<00:33,  4.40it/s]predicting train subjects sagittal:  45%|████▍     | 119/266 [00:28<00:34,  4.29it/s]predicting train subjects sagittal:  45%|████▌     | 120/266 [00:28<00:35,  4.15it/s]predicting train subjects sagittal:  45%|████▌     | 121/266 [00:28<00:37,  3.92it/s]predicting train subjects sagittal:  46%|████▌     | 122/266 [00:28<00:38,  3.70it/s]predicting train subjects sagittal:  46%|████▌     | 123/266 [00:29<00:38,  3.72it/s]predicting train subjects sagittal:  47%|████▋     | 124/266 [00:29<00:36,  3.85it/s]predicting train subjects sagittal:  47%|████▋     | 125/266 [00:29<00:37,  3.81it/s]predicting train subjects sagittal:  47%|████▋     | 126/266 [00:29<00:36,  3.85it/s]predicting train subjects sagittal:  48%|████▊     | 127/266 [00:30<00:36,  3.83it/s]predicting train subjects sagittal:  48%|████▊     | 128/266 [00:30<00:35,  3.88it/s]predicting train subjects sagittal:  48%|████▊     | 129/266 [00:30<00:37,  3.67it/s]predicting train subjects sagittal:  49%|████▉     | 130/266 [00:31<00:38,  3.56it/s]predicting train subjects sagittal:  49%|████▉     | 131/266 [00:31<00:37,  3.64it/s]predicting train subjects sagittal:  50%|████▉     | 132/266 [00:31<00:38,  3.52it/s]predicting train subjects sagittal:  50%|█████     | 133/266 [00:31<00:36,  3.68it/s]predicting train subjects sagittal:  50%|█████     | 134/266 [00:32<00:34,  3.83it/s]predicting train subjects sagittal:  51%|█████     | 135/266 [00:32<00:33,  3.90it/s]predicting train subjects sagittal:  51%|█████     | 136/266 [00:32<00:33,  3.91it/s]predicting train subjects sagittal:  52%|█████▏    | 137/266 [00:32<00:34,  3.75it/s]predicting train subjects sagittal:  52%|█████▏    | 138/266 [00:33<00:32,  3.95it/s]predicting train subjects sagittal:  52%|█████▏    | 139/266 [00:33<00:32,  3.94it/s]predicting train subjects sagittal:  53%|█████▎    | 140/266 [00:33<00:30,  4.08it/s]predicting train subjects sagittal:  53%|█████▎    | 141/266 [00:33<00:29,  4.19it/s]predicting train subjects sagittal:  53%|█████▎    | 142/266 [00:34<00:29,  4.27it/s]predicting train subjects sagittal:  54%|█████▍    | 143/266 [00:34<00:28,  4.29it/s]predicting train subjects sagittal:  54%|█████▍    | 144/266 [00:34<00:29,  4.17it/s]predicting train subjects sagittal:  55%|█████▍    | 145/266 [00:34<00:28,  4.25it/s]predicting train subjects sagittal:  55%|█████▍    | 146/266 [00:34<00:28,  4.20it/s]predicting train subjects sagittal:  55%|█████▌    | 147/266 [00:35<00:27,  4.25it/s]predicting train subjects sagittal:  56%|█████▌    | 148/266 [00:35<00:27,  4.31it/s]predicting train subjects sagittal:  56%|█████▌    | 149/266 [00:35<00:26,  4.34it/s]predicting train subjects sagittal:  56%|█████▋    | 150/266 [00:35<00:26,  4.33it/s]predicting train subjects sagittal:  57%|█████▋    | 151/266 [00:36<00:27,  4.21it/s]predicting train subjects sagittal:  57%|█████▋    | 152/266 [00:36<00:27,  4.14it/s]predicting train subjects sagittal:  58%|█████▊    | 153/266 [00:36<00:26,  4.26it/s]predicting train subjects sagittal:  58%|█████▊    | 154/266 [00:36<00:25,  4.32it/s]predicting train subjects sagittal:  58%|█████▊    | 155/266 [00:37<00:24,  4.55it/s]predicting train subjects sagittal:  59%|█████▊    | 156/266 [00:37<00:22,  4.87it/s]predicting train subjects sagittal:  59%|█████▉    | 157/266 [00:37<00:21,  5.08it/s]predicting train subjects sagittal:  59%|█████▉    | 158/266 [00:37<00:20,  5.31it/s]predicting train subjects sagittal:  60%|█████▉    | 159/266 [00:37<00:19,  5.46it/s]predicting train subjects sagittal:  60%|██████    | 160/266 [00:37<00:19,  5.55it/s]predicting train subjects sagittal:  61%|██████    | 161/266 [00:38<00:18,  5.65it/s]predicting train subjects sagittal:  61%|██████    | 162/266 [00:38<00:18,  5.66it/s]predicting train subjects sagittal:  61%|██████▏   | 163/266 [00:38<00:18,  5.57it/s]predicting train subjects sagittal:  62%|██████▏   | 164/266 [00:38<00:18,  5.64it/s]predicting train subjects sagittal:  62%|██████▏   | 165/266 [00:38<00:18,  5.52it/s]predicting train subjects sagittal:  62%|██████▏   | 166/266 [00:38<00:17,  5.62it/s]predicting train subjects sagittal:  63%|██████▎   | 167/266 [00:39<00:17,  5.71it/s]predicting train subjects sagittal:  63%|██████▎   | 168/266 [00:39<00:17,  5.59it/s]predicting train subjects sagittal:  64%|██████▎   | 169/266 [00:39<00:17,  5.70it/s]predicting train subjects sagittal:  64%|██████▍   | 170/266 [00:39<00:16,  5.77it/s]predicting train subjects sagittal:  64%|██████▍   | 171/266 [00:39<00:16,  5.80it/s]predicting train subjects sagittal:  65%|██████▍   | 172/266 [00:40<00:16,  5.78it/s]predicting train subjects sagittal:  65%|██████▌   | 173/266 [00:40<00:17,  5.32it/s]predicting train subjects sagittal:  65%|██████▌   | 174/266 [00:40<00:17,  5.27it/s]predicting train subjects sagittal:  66%|██████▌   | 175/266 [00:40<00:17,  5.24it/s]predicting train subjects sagittal:  66%|██████▌   | 176/266 [00:40<00:17,  5.15it/s]predicting train subjects sagittal:  67%|██████▋   | 177/266 [00:41<00:17,  5.06it/s]predicting train subjects sagittal:  67%|██████▋   | 178/266 [00:41<00:17,  4.96it/s]predicting train subjects sagittal:  67%|██████▋   | 179/266 [00:41<00:18,  4.78it/s]predicting train subjects sagittal:  68%|██████▊   | 180/266 [00:41<00:17,  4.79it/s]predicting train subjects sagittal:  68%|██████▊   | 181/266 [00:41<00:17,  4.90it/s]predicting train subjects sagittal:  68%|██████▊   | 182/266 [00:42<00:18,  4.63it/s]predicting train subjects sagittal:  69%|██████▉   | 183/266 [00:42<00:17,  4.70it/s]predicting train subjects sagittal:  69%|██████▉   | 184/266 [00:42<00:18,  4.41it/s]predicting train subjects sagittal:  70%|██████▉   | 185/266 [00:42<00:17,  4.52it/s]predicting train subjects sagittal:  70%|██████▉   | 186/266 [00:42<00:17,  4.57it/s]predicting train subjects sagittal:  70%|███████   | 187/266 [00:43<00:17,  4.54it/s]predicting train subjects sagittal:  71%|███████   | 188/266 [00:43<00:16,  4.68it/s]predicting train subjects sagittal:  71%|███████   | 189/266 [00:43<00:15,  4.83it/s]predicting train subjects sagittal:  71%|███████▏  | 190/266 [00:43<00:15,  4.93it/s]predicting train subjects sagittal:  72%|███████▏  | 191/266 [00:44<00:15,  4.97it/s]predicting train subjects sagittal:  72%|███████▏  | 192/266 [00:44<00:14,  5.13it/s]predicting train subjects sagittal:  73%|███████▎  | 193/266 [00:44<00:14,  5.13it/s]predicting train subjects sagittal:  73%|███████▎  | 194/266 [00:44<00:14,  4.81it/s]predicting train subjects sagittal:  73%|███████▎  | 195/266 [00:44<00:15,  4.63it/s]predicting train subjects sagittal:  74%|███████▎  | 196/266 [00:45<00:14,  4.72it/s]predicting train subjects sagittal:  74%|███████▍  | 197/266 [00:45<00:14,  4.83it/s]predicting train subjects sagittal:  74%|███████▍  | 198/266 [00:45<00:13,  4.88it/s]predicting train subjects sagittal:  75%|███████▍  | 199/266 [00:45<00:14,  4.76it/s]predicting train subjects sagittal:  75%|███████▌  | 200/266 [00:45<00:13,  4.85it/s]predicting train subjects sagittal:  76%|███████▌  | 201/266 [00:46<00:13,  4.88it/s]predicting train subjects sagittal:  76%|███████▌  | 202/266 [00:46<00:12,  4.95it/s]predicting train subjects sagittal:  76%|███████▋  | 203/266 [00:46<00:13,  4.81it/s]predicting train subjects sagittal:  77%|███████▋  | 204/266 [00:46<00:12,  4.87it/s]predicting train subjects sagittal:  77%|███████▋  | 205/266 [00:46<00:12,  4.74it/s]predicting train subjects sagittal:  77%|███████▋  | 206/266 [00:47<00:12,  4.67it/s]predicting train subjects sagittal:  78%|███████▊  | 207/266 [00:47<00:12,  4.79it/s]predicting train subjects sagittal:  78%|███████▊  | 208/266 [00:47<00:11,  4.86it/s]predicting train subjects sagittal:  79%|███████▊  | 209/266 [00:47<00:11,  4.89it/s]predicting train subjects sagittal:  79%|███████▉  | 210/266 [00:47<00:11,  4.91it/s]predicting train subjects sagittal:  79%|███████▉  | 211/266 [00:48<00:11,  4.90it/s]predicting train subjects sagittal:  80%|███████▉  | 212/266 [00:48<00:11,  4.82it/s]predicting train subjects sagittal:  80%|████████  | 213/266 [00:48<00:10,  4.96it/s]predicting train subjects sagittal:  80%|████████  | 214/266 [00:48<00:10,  5.06it/s]predicting train subjects sagittal:  81%|████████  | 215/266 [00:48<00:09,  5.20it/s]predicting train subjects sagittal:  81%|████████  | 216/266 [00:49<00:09,  5.30it/s]predicting train subjects sagittal:  82%|████████▏ | 217/266 [00:49<00:09,  5.21it/s]predicting train subjects sagittal:  82%|████████▏ | 218/266 [00:49<00:09,  5.31it/s]predicting train subjects sagittal:  82%|████████▏ | 219/266 [00:49<00:08,  5.37it/s]predicting train subjects sagittal:  83%|████████▎ | 220/266 [00:49<00:08,  5.40it/s]predicting train subjects sagittal:  83%|████████▎ | 221/266 [00:50<00:08,  5.46it/s]predicting train subjects sagittal:  83%|████████▎ | 222/266 [00:50<00:08,  5.48it/s]predicting train subjects sagittal:  84%|████████▍ | 223/266 [00:50<00:07,  5.49it/s]predicting train subjects sagittal:  84%|████████▍ | 224/266 [00:50<00:08,  5.19it/s]predicting train subjects sagittal:  85%|████████▍ | 225/266 [00:50<00:07,  5.32it/s]predicting train subjects sagittal:  85%|████████▍ | 226/266 [00:50<00:07,  5.34it/s]predicting train subjects sagittal:  85%|████████▌ | 227/266 [00:51<00:07,  5.02it/s]predicting train subjects sagittal:  86%|████████▌ | 228/266 [00:51<00:07,  5.06it/s]predicting train subjects sagittal:  86%|████████▌ | 229/266 [00:51<00:07,  4.75it/s]predicting train subjects sagittal:  86%|████████▋ | 230/266 [00:51<00:07,  4.58it/s]predicting train subjects sagittal:  87%|████████▋ | 231/266 [00:52<00:07,  4.56it/s]predicting train subjects sagittal:  87%|████████▋ | 232/266 [00:52<00:07,  4.48it/s]predicting train subjects sagittal:  88%|████████▊ | 233/266 [00:52<00:07,  4.46it/s]predicting train subjects sagittal:  88%|████████▊ | 234/266 [00:52<00:06,  4.65it/s]predicting train subjects sagittal:  88%|████████▊ | 235/266 [00:52<00:06,  4.80it/s]predicting train subjects sagittal:  89%|████████▊ | 236/266 [00:53<00:06,  4.62it/s]predicting train subjects sagittal:  89%|████████▉ | 237/266 [00:53<00:06,  4.45it/s]predicting train subjects sagittal:  89%|████████▉ | 238/266 [00:53<00:06,  4.29it/s]predicting train subjects sagittal:  90%|████████▉ | 239/266 [00:53<00:05,  4.52it/s]predicting train subjects sagittal:  90%|█████████ | 240/266 [00:54<00:05,  4.67it/s]predicting train subjects sagittal:  91%|█████████ | 241/266 [00:54<00:05,  4.85it/s]predicting train subjects sagittal:  91%|█████████ | 242/266 [00:54<00:04,  4.94it/s]predicting train subjects sagittal:  91%|█████████▏| 243/266 [00:54<00:04,  4.82it/s]predicting train subjects sagittal:  92%|█████████▏| 244/266 [00:54<00:04,  4.94it/s]predicting train subjects sagittal:  92%|█████████▏| 245/266 [00:55<00:04,  5.01it/s]predicting train subjects sagittal:  92%|█████████▏| 246/266 [00:55<00:04,  4.99it/s]predicting train subjects sagittal:  93%|█████████▎| 247/266 [00:55<00:03,  4.95it/s]predicting train subjects sagittal:  93%|█████████▎| 248/266 [00:55<00:03,  5.04it/s]predicting train subjects sagittal:  94%|█████████▎| 249/266 [00:55<00:03,  4.74it/s]predicting train subjects sagittal:  94%|█████████▍| 250/266 [00:56<00:03,  4.35it/s]predicting train subjects sagittal:  94%|█████████▍| 251/266 [00:56<00:03,  4.31it/s]predicting train subjects sagittal:  95%|█████████▍| 252/266 [00:56<00:03,  4.32it/s]predicting train subjects sagittal:  95%|█████████▌| 253/266 [00:56<00:03,  4.33it/s]predicting train subjects sagittal:  95%|█████████▌| 254/266 [00:57<00:02,  4.14it/s]predicting train subjects sagittal:  96%|█████████▌| 255/266 [00:57<00:02,  4.14it/s]predicting train subjects sagittal:  96%|█████████▌| 256/266 [00:57<00:02,  4.08it/s]predicting train subjects sagittal:  97%|█████████▋| 257/266 [00:57<00:02,  4.14it/s]predicting train subjects sagittal:  97%|█████████▋| 258/266 [00:58<00:01,  4.19it/s]predicting train subjects sagittal:  97%|█████████▋| 259/266 [00:58<00:01,  4.24it/s]predicting train subjects sagittal:  98%|█████████▊| 260/266 [00:58<00:01,  4.25it/s]predicting train subjects sagittal:  98%|█████████▊| 261/266 [00:58<00:01,  4.19it/s]predicting train subjects sagittal:  98%|█████████▊| 262/266 [00:59<00:00,  4.23it/s]predicting train subjects sagittal:  99%|█████████▉| 263/266 [00:59<00:00,  4.28it/s]predicting train subjects sagittal:  99%|█████████▉| 264/266 [00:59<00:00,  4.18it/s]predicting train subjects sagittal: 100%|█████████▉| 265/266 [00:59<00:00,  4.11it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:59<00:00,  4.11it/s]
saving BB  test1-THALAMUS:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 5/5 [00:00<00:00, 66.27it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 7/266 [00:00<00:04, 59.36it/s]saving BB  train1-THALAMUS:   5%|▍         | 13/266 [00:00<00:04, 58.97it/s]saving BB  train1-THALAMUS:   7%|▋         | 19/266 [00:00<00:04, 58.01it/s]saving BB  train1-THALAMUS:   9%|▉         | 25/266 [00:00<00:04, 58.56it/s]saving BB  train1-THALAMUS:  12%|█▏        | 32/266 [00:00<00:03, 60.91it/s]saving BB  train1-THALAMUS:  14%|█▍        | 38/266 [00:00<00:03, 60.48it/s]saving BB  train1-THALAMUS:  17%|█▋        | 45/266 [00:00<00:03, 61.97it/s]saving BB  train1-THALAMUS:  20%|█▉        | 53/266 [00:00<00:03, 64.21it/s]saving BB  train1-THALAMUS:  23%|██▎       | 60/266 [00:00<00:03, 65.83it/s]saving BB  train1-THALAMUS:  26%|██▌       | 68/266 [00:01<00:02, 69.19it/s]saving BB  train1-THALAMUS:  29%|██▊       | 76/266 [00:01<00:02, 71.63it/s]saving BB  train1-THALAMUS:  32%|███▏      | 84/266 [00:01<00:02, 70.94it/s]saving BB  train1-THALAMUS:  35%|███▍      | 92/266 [00:01<00:02, 69.99it/s]saving BB  train1-THALAMUS:  37%|███▋      | 99/266 [00:01<00:02, 69.12it/s]saving BB  train1-THALAMUS:  40%|████      | 107/266 [00:01<00:02, 71.22it/s]saving BB  train1-THALAMUS:  43%|████▎     | 115/266 [00:01<00:02, 72.05it/s]saving BB  train1-THALAMUS:  46%|████▌     | 123/266 [00:01<00:02, 70.83it/s]saving BB  train1-THALAMUS:  49%|████▉     | 131/266 [00:01<00:01, 69.49it/s]saving BB  train1-THALAMUS:  52%|█████▏    | 138/266 [00:02<00:01, 67.78it/s]saving BB  train1-THALAMUS:  55%|█████▍    | 145/266 [00:02<00:01, 67.40it/s]saving BB  train1-THALAMUS:  57%|█████▋    | 152/266 [00:02<00:01, 64.36it/s]saving BB  train1-THALAMUS:  60%|██████    | 160/266 [00:02<00:01, 66.96it/s]saving BB  train1-THALAMUS:  63%|██████▎   | 168/266 [00:02<00:01, 70.25it/s]saving BB  train1-THALAMUS:  67%|██████▋   | 177/266 [00:02<00:01, 73.26it/s]saving BB  train1-THALAMUS:  70%|██████▉   | 186/266 [00:02<00:01, 75.83it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 194/266 [00:02<00:00, 75.60it/s]saving BB  train1-THALAMUS:  76%|███████▌  | 202/266 [00:02<00:00, 74.45it/s]saving BB  train1-THALAMUS:  79%|███████▉  | 210/266 [00:03<00:00, 73.12it/s]saving BB  train1-THALAMUS:  82%|████████▏ | 218/266 [00:03<00:00, 74.02it/s]saving BB  train1-THALAMUS:  85%|████████▍ | 226/266 [00:03<00:00, 75.69it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 234/266 [00:03<00:00, 73.63it/s]saving BB  train1-THALAMUS:  91%|█████████▏| 243/266 [00:03<00:00, 75.55it/s]saving BB  train1-THALAMUS:  94%|█████████▍| 251/266 [00:03<00:00, 74.98it/s]saving BB  train1-THALAMUS:  97%|█████████▋| 259/266 [00:03<00:00, 71.31it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 69.71it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 5/5 [00:00<00:00, 73.16it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   2%|▏         | 6/266 [00:00<00:04, 58.98it/s]saving BB  train1-THALAMUS Sagittal:   5%|▍         | 12/266 [00:00<00:04, 57.87it/s]saving BB  train1-THALAMUS Sagittal:   7%|▋         | 18/266 [00:00<00:04, 57.08it/s]saving BB  train1-THALAMUS Sagittal:   9%|▉         | 24/266 [00:00<00:04, 56.84it/s]saving BB  train1-THALAMUS Sagittal:  12%|█▏        | 31/266 [00:00<00:03, 59.89it/s]saving BB  train1-THALAMUS Sagittal:  14%|█▍        | 38/266 [00:00<00:03, 62.29it/s]saving BB  train1-THALAMUS Sagittal:  17%|█▋        | 45/266 [00:00<00:03, 64.13it/s]saving BB  train1-THALAMUS Sagittal:  20%|█▉        | 53/266 [00:00<00:03, 65.89it/s]saving BB  train1-THALAMUS Sagittal:  23%|██▎       | 61/266 [00:00<00:03, 67.81it/s]saving BB  train1-THALAMUS Sagittal:  26%|██▌       | 69/266 [00:01<00:02, 70.82it/s]saving BB  train1-THALAMUS Sagittal:  29%|██▉       | 78/266 [00:01<00:02, 73.11it/s]saving BB  train1-THALAMUS Sagittal:  32%|███▏      | 86/266 [00:01<00:02, 71.47it/s]saving BB  train1-THALAMUS Sagittal:  35%|███▌      | 94/266 [00:01<00:02, 70.72it/s]saving BB  train1-THALAMUS Sagittal:  38%|███▊      | 102/266 [00:01<00:02, 71.31it/s]saving BB  train1-THALAMUS Sagittal:  41%|████▏     | 110/266 [00:01<00:02, 72.30it/s]saving BB  train1-THALAMUS Sagittal:  44%|████▍     | 118/266 [00:01<00:02, 71.15it/s]saving BB  train1-THALAMUS Sagittal:  47%|████▋     | 126/266 [00:01<00:02, 69.61it/s]saving BB  train1-THALAMUS Sagittal:  50%|█████     | 133/266 [00:01<00:01, 68.87it/s]saving BB  train1-THALAMUS Sagittal:  53%|█████▎    | 140/266 [00:02<00:01, 68.71it/s]saving BB  train1-THALAMUS Sagittal:  55%|█████▌    | 147/266 [00:02<00:01, 68.72it/s]saving BB  train1-THALAMUS Sagittal:  58%|█████▊    | 155/266 [00:02<00:01, 69.80it/s]saving BB  train1-THALAMUS Sagittal:  62%|██████▏   | 164/266 [00:02<00:01, 72.72it/s]saving BB  train1-THALAMUS Sagittal:  65%|██████▍   | 172/266 [00:02<00:01, 74.67it/s]saving BB  train1-THALAMUS Sagittal:  68%|██████▊   | 181/266 [00:02<00:01, 76.05it/s]saving BB  train1-THALAMUS Sagittal:  71%|███████▏  | 190/266 [00:02<00:00, 77.37it/s]saving BB  train1-THALAMUS Sagittal:  74%|███████▍  | 198/266 [00:02<00:00, 75.31it/s]saving BB  train1-THALAMUS Sagittal:  77%|███████▋  | 206/266 [00:02<00:00, 73.19it/s]saving BB  train1-THALAMUS Sagittal:  80%|████████  | 214/266 [00:03<00:00, 72.70it/s]saving BB  train1-THALAMUS Sagittal:  83%|████████▎ | 222/266 [00:03<00:00, 73.80it/s]saving BB  train1-THALAMUS Sagittal:  86%|████████▋ | 230/266 [00:03<00:00, 74.80it/s]saving BB  train1-THALAMUS Sagittal:  90%|████████▉ | 239/266 [00:03<00:00, 76.31it/s]saving BB  train1-THALAMUS Sagittal:  93%|█████████▎| 247/266 [00:03<00:00, 76.48it/s]saving BB  train1-THALAMUS Sagittal:  96%|█████████▌| 255/266 [00:03<00:00, 73.18it/s]saving BB  train1-THALAMUS Sagittal:  99%|█████████▉| 263/266 [00:03<00:00, 70.89it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 70.66it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<07:30,  1.70s/it]Loading train:   1%|          | 2/266 [00:03<07:06,  1.61s/it]Loading train:   1%|          | 3/266 [00:04<06:18,  1.44s/it]Loading train:   2%|▏         | 4/266 [00:05<05:51,  1.34s/it]Loading train:   2%|▏         | 5/266 [00:06<05:51,  1.35s/it]Loading train:   2%|▏         | 6/266 [00:07<05:21,  1.24s/it]Loading train:   3%|▎         | 7/266 [00:08<04:56,  1.14s/it]Loading train:   3%|▎         | 8/266 [00:09<04:39,  1.08s/it]Loading train:   3%|▎         | 9/266 [00:10<04:26,  1.04s/it]Loading train:   4%|▍         | 10/266 [00:11<04:19,  1.01s/it]Loading train:   4%|▍         | 11/266 [00:12<04:15,  1.00s/it]Loading train:   5%|▍         | 12/266 [00:13<04:11,  1.01it/s]Loading train:   5%|▍         | 13/266 [00:14<04:08,  1.02it/s]Loading train:   5%|▌         | 14/266 [00:15<04:08,  1.01it/s]Loading train:   6%|▌         | 15/266 [00:16<04:01,  1.04it/s]Loading train:   6%|▌         | 16/266 [00:17<03:53,  1.07it/s]Loading train:   6%|▋         | 17/266 [00:17<03:45,  1.10it/s]Loading train:   7%|▋         | 18/266 [00:18<03:46,  1.09it/s]Loading train:   7%|▋         | 19/266 [00:19<03:58,  1.04it/s]Loading train:   8%|▊         | 20/266 [00:20<03:48,  1.08it/s]Loading train:   8%|▊         | 21/266 [00:21<03:46,  1.08it/s]Loading train:   8%|▊         | 22/266 [00:22<03:45,  1.08it/s]Loading train:   9%|▊         | 23/266 [00:23<03:50,  1.05it/s]Loading train:   9%|▉         | 24/266 [00:24<03:49,  1.05it/s]Loading train:   9%|▉         | 25/266 [00:25<03:45,  1.07it/s]Loading train:  10%|▉         | 26/266 [00:26<03:36,  1.11it/s]Loading train:  10%|█         | 27/266 [00:27<03:29,  1.14it/s]Loading train:  11%|█         | 28/266 [00:27<03:28,  1.14it/s]Loading train:  11%|█         | 29/266 [00:28<03:29,  1.13it/s]Loading train:  11%|█▏        | 30/266 [00:29<03:25,  1.15it/s]Loading train:  12%|█▏        | 31/266 [00:30<03:24,  1.15it/s]Loading train:  12%|█▏        | 32/266 [00:31<03:18,  1.18it/s]Loading train:  12%|█▏        | 33/266 [00:32<03:15,  1.19it/s]Loading train:  13%|█▎        | 34/266 [00:32<03:12,  1.20it/s]Loading train:  13%|█▎        | 35/266 [00:33<03:10,  1.21it/s]Loading train:  14%|█▎        | 36/266 [00:34<03:09,  1.21it/s]Loading train:  14%|█▍        | 37/266 [00:35<03:17,  1.16it/s]Loading train:  14%|█▍        | 38/266 [00:36<03:15,  1.17it/s]Loading train:  15%|█▍        | 39/266 [00:37<03:11,  1.19it/s]Loading train:  15%|█▌        | 40/266 [00:38<03:11,  1.18it/s]Loading train:  15%|█▌        | 41/266 [00:38<03:08,  1.19it/s]Loading train:  16%|█▌        | 42/266 [00:39<03:03,  1.22it/s]Loading train:  16%|█▌        | 43/266 [00:40<03:02,  1.22it/s]Loading train:  17%|█▋        | 44/266 [00:41<02:55,  1.27it/s]Loading train:  17%|█▋        | 45/266 [00:41<02:45,  1.33it/s]Loading train:  17%|█▋        | 46/266 [00:42<02:38,  1.39it/s]Loading train:  18%|█▊        | 47/266 [00:43<02:35,  1.41it/s]Loading train:  18%|█▊        | 48/266 [00:43<02:36,  1.40it/s]Loading train:  18%|█▊        | 49/266 [00:44<02:37,  1.38it/s]Loading train:  19%|█▉        | 50/266 [00:45<02:41,  1.34it/s]Loading train:  19%|█▉        | 51/266 [00:46<02:41,  1.33it/s]Loading train:  20%|█▉        | 52/266 [00:47<02:44,  1.30it/s]Loading train:  20%|█▉        | 53/266 [00:47<02:38,  1.34it/s]Loading train:  20%|██        | 54/266 [00:48<02:33,  1.39it/s]Loading train:  21%|██        | 55/266 [00:49<02:30,  1.40it/s]Loading train:  21%|██        | 56/266 [00:49<02:26,  1.44it/s]Loading train:  21%|██▏       | 57/266 [00:50<02:31,  1.38it/s]Loading train:  22%|██▏       | 58/266 [00:51<02:32,  1.36it/s]Loading train:  22%|██▏       | 59/266 [00:52<02:37,  1.32it/s]Loading train:  23%|██▎       | 60/266 [00:53<02:52,  1.20it/s]Loading train:  23%|██▎       | 61/266 [00:53<02:46,  1.23it/s]Loading train:  23%|██▎       | 62/266 [00:54<02:37,  1.29it/s]Loading train:  24%|██▎       | 63/266 [00:55<02:34,  1.32it/s]Loading train:  24%|██▍       | 64/266 [00:55<02:27,  1.37it/s]Loading train:  24%|██▍       | 65/266 [00:56<02:20,  1.43it/s]Loading train:  25%|██▍       | 66/266 [00:57<02:20,  1.42it/s]Loading train:  25%|██▌       | 67/266 [00:57<02:15,  1.47it/s]Loading train:  26%|██▌       | 68/266 [00:58<02:12,  1.50it/s]Loading train:  26%|██▌       | 69/266 [00:59<02:10,  1.50it/s]Loading train:  26%|██▋       | 70/266 [00:59<02:11,  1.49it/s]Loading train:  27%|██▋       | 71/266 [01:00<02:11,  1.48it/s]Loading train:  27%|██▋       | 72/266 [01:01<02:09,  1.50it/s]Loading train:  27%|██▋       | 73/266 [01:01<02:08,  1.50it/s]Loading train:  28%|██▊       | 74/266 [01:02<02:06,  1.51it/s]Loading train:  28%|██▊       | 75/266 [01:03<02:08,  1.49it/s]Loading train:  29%|██▊       | 76/266 [01:03<02:08,  1.47it/s]Loading train:  29%|██▉       | 77/266 [01:04<02:06,  1.49it/s]Loading train:  29%|██▉       | 78/266 [01:05<02:18,  1.36it/s]Loading train:  30%|██▉       | 79/266 [01:06<02:27,  1.27it/s]Loading train:  30%|███       | 80/266 [01:07<02:28,  1.25it/s]Loading train:  30%|███       | 81/266 [01:08<02:32,  1.22it/s]Loading train:  31%|███       | 82/266 [01:08<02:35,  1.18it/s]Loading train:  31%|███       | 83/266 [01:09<02:34,  1.19it/s]Loading train:  32%|███▏      | 84/266 [01:10<02:32,  1.19it/s]Loading train:  32%|███▏      | 85/266 [01:11<02:32,  1.19it/s]Loading train:  32%|███▏      | 86/266 [01:12<02:30,  1.20it/s]Loading train:  33%|███▎      | 87/266 [01:13<02:34,  1.16it/s]Loading train:  33%|███▎      | 88/266 [01:14<02:33,  1.16it/s]Loading train:  33%|███▎      | 89/266 [01:15<02:39,  1.11it/s]Loading train:  34%|███▍      | 90/266 [01:15<02:35,  1.13it/s]Loading train:  34%|███▍      | 91/266 [01:16<02:31,  1.16it/s]Loading train:  35%|███▍      | 92/266 [01:17<02:30,  1.16it/s]Loading train:  35%|███▍      | 93/266 [01:18<02:31,  1.14it/s]Loading train:  35%|███▌      | 94/266 [01:19<02:33,  1.12it/s]Loading train:  36%|███▌      | 95/266 [01:20<02:32,  1.12it/s]Loading train:  36%|███▌      | 96/266 [01:21<02:51,  1.01s/it]Loading train:  36%|███▋      | 97/266 [01:23<03:16,  1.16s/it]Loading train:  37%|███▋      | 98/266 [01:24<03:16,  1.17s/it]Loading train:  37%|███▋      | 99/266 [01:25<03:05,  1.11s/it]Loading train:  38%|███▊      | 100/266 [01:26<03:05,  1.12s/it]Loading train:  38%|███▊      | 101/266 [01:27<02:47,  1.01s/it]Loading train:  38%|███▊      | 102/266 [01:27<02:34,  1.06it/s]Loading train:  39%|███▊      | 103/266 [01:28<02:27,  1.11it/s]Loading train:  39%|███▉      | 104/266 [01:29<02:18,  1.17it/s]Loading train:  39%|███▉      | 105/266 [01:30<02:08,  1.25it/s]Loading train:  40%|███▉      | 106/266 [01:31<02:07,  1.25it/s]Loading train:  40%|████      | 107/266 [01:31<02:04,  1.27it/s]Loading train:  41%|████      | 108/266 [01:32<02:00,  1.31it/s]Loading train:  41%|████      | 109/266 [01:33<01:58,  1.32it/s]Loading train:  41%|████▏     | 110/266 [01:33<01:56,  1.34it/s]Loading train:  42%|████▏     | 111/266 [01:34<01:55,  1.34it/s]Loading train:  42%|████▏     | 112/266 [01:35<01:53,  1.35it/s]Loading train:  42%|████▏     | 113/266 [01:36<01:53,  1.35it/s]Loading train:  43%|████▎     | 114/266 [01:36<01:55,  1.32it/s]Loading train:  43%|████▎     | 115/266 [01:37<01:58,  1.27it/s]Loading train:  44%|████▎     | 116/266 [01:38<01:56,  1.29it/s]Loading train:  44%|████▍     | 117/266 [01:39<01:57,  1.26it/s]Loading train:  44%|████▍     | 118/266 [01:40<01:53,  1.30it/s]Loading train:  45%|████▍     | 119/266 [01:41<02:01,  1.21it/s]Loading train:  45%|████▌     | 120/266 [01:41<02:04,  1.18it/s]Loading train:  45%|████▌     | 121/266 [01:42<02:03,  1.17it/s]Loading train:  46%|████▌     | 122/266 [01:43<02:09,  1.11it/s]Loading train:  46%|████▌     | 123/266 [01:44<02:08,  1.11it/s]Loading train:  47%|████▋     | 124/266 [01:45<02:10,  1.09it/s]Loading train:  47%|████▋     | 125/266 [01:46<02:09,  1.09it/s]Loading train:  47%|████▋     | 126/266 [01:47<02:08,  1.09it/s]Loading train:  48%|████▊     | 127/266 [01:48<02:05,  1.11it/s]Loading train:  48%|████▊     | 128/266 [01:49<02:00,  1.14it/s]Loading train:  48%|████▊     | 129/266 [01:50<02:03,  1.11it/s]Loading train:  49%|████▉     | 130/266 [01:51<02:00,  1.13it/s]Loading train:  49%|████▉     | 131/266 [01:51<02:02,  1.10it/s]Loading train:  50%|████▉     | 132/266 [01:52<02:05,  1.07it/s]Loading train:  50%|█████     | 133/266 [01:54<02:08,  1.04it/s]Loading train:  50%|█████     | 134/266 [01:54<02:05,  1.05it/s]Loading train:  51%|█████     | 135/266 [01:55<02:02,  1.07it/s]Loading train:  51%|█████     | 136/266 [01:56<01:59,  1.09it/s]Loading train:  52%|█████▏    | 137/266 [01:57<01:59,  1.08it/s]Loading train:  52%|█████▏    | 138/266 [01:58<01:57,  1.09it/s]Loading train:  52%|█████▏    | 139/266 [01:59<01:52,  1.13it/s]Loading train:  53%|█████▎    | 140/266 [02:00<01:48,  1.17it/s]Loading train:  53%|█████▎    | 141/266 [02:01<01:47,  1.16it/s]Loading train:  53%|█████▎    | 142/266 [02:01<01:46,  1.17it/s]Loading train:  54%|█████▍    | 143/266 [02:02<01:43,  1.19it/s]Loading train:  54%|█████▍    | 144/266 [02:03<01:40,  1.22it/s]Loading train:  55%|█████▍    | 145/266 [02:04<01:40,  1.20it/s]Loading train:  55%|█████▍    | 146/266 [02:05<01:39,  1.20it/s]Loading train:  55%|█████▌    | 147/266 [02:05<01:37,  1.22it/s]Loading train:  56%|█████▌    | 148/266 [02:06<01:39,  1.19it/s]Loading train:  56%|█████▌    | 149/266 [02:07<01:37,  1.20it/s]Loading train:  56%|█████▋    | 150/266 [02:08<01:34,  1.22it/s]Loading train:  57%|█████▋    | 151/266 [02:09<01:31,  1.26it/s]Loading train:  57%|█████▋    | 152/266 [02:09<01:30,  1.26it/s]Loading train:  58%|█████▊    | 153/266 [02:10<01:29,  1.27it/s]Loading train:  58%|█████▊    | 154/266 [02:11<01:29,  1.26it/s]Loading train:  58%|█████▊    | 155/266 [02:12<01:25,  1.29it/s]Loading train:  59%|█████▊    | 156/266 [02:12<01:19,  1.38it/s]Loading train:  59%|█████▉    | 157/266 [02:13<01:17,  1.40it/s]Loading train:  59%|█████▉    | 158/266 [02:14<01:14,  1.46it/s]Loading train:  60%|█████▉    | 159/266 [02:14<01:10,  1.51it/s]Loading train:  60%|██████    | 160/266 [02:15<01:08,  1.56it/s]Loading train:  61%|██████    | 161/266 [02:15<01:04,  1.62it/s]Loading train:  61%|██████    | 162/266 [02:16<01:05,  1.59it/s]Loading train:  61%|██████▏   | 163/266 [02:17<01:08,  1.49it/s]Loading train:  62%|██████▏   | 164/266 [02:18<01:07,  1.52it/s]Loading train:  62%|██████▏   | 165/266 [02:18<01:08,  1.48it/s]Loading train:  62%|██████▏   | 166/266 [02:19<01:06,  1.51it/s]Loading train:  63%|██████▎   | 167/266 [02:19<01:05,  1.52it/s]Loading train:  63%|██████▎   | 168/266 [02:20<01:04,  1.52it/s]Loading train:  64%|██████▎   | 169/266 [02:21<01:03,  1.53it/s]Loading train:  64%|██████▍   | 170/266 [02:21<01:03,  1.51it/s]Loading train:  64%|██████▍   | 171/266 [02:22<01:03,  1.49it/s]Loading train:  65%|██████▍   | 172/266 [02:23<01:00,  1.55it/s]Loading train:  65%|██████▌   | 173/266 [02:23<01:01,  1.52it/s]Loading train:  65%|██████▌   | 174/266 [02:24<01:00,  1.52it/s]Loading train:  66%|██████▌   | 175/266 [02:25<01:02,  1.47it/s]Loading train:  66%|██████▌   | 176/266 [02:25<00:59,  1.50it/s]Loading train:  67%|██████▋   | 177/266 [02:26<01:01,  1.44it/s]Loading train:  67%|██████▋   | 178/266 [02:27<00:59,  1.47it/s]Loading train:  67%|██████▋   | 179/266 [02:28<00:59,  1.46it/s]Loading train:  68%|██████▊   | 180/266 [02:28<00:58,  1.48it/s]Loading train:  68%|██████▊   | 181/266 [02:29<00:58,  1.45it/s]Loading train:  68%|██████▊   | 182/266 [02:30<00:59,  1.42it/s]Loading train:  69%|██████▉   | 183/266 [02:30<00:58,  1.42it/s]Loading train:  69%|██████▉   | 184/266 [02:31<00:56,  1.45it/s]Loading train:  70%|██████▉   | 185/266 [02:32<00:55,  1.46it/s]Loading train:  70%|██████▉   | 186/266 [02:32<00:53,  1.49it/s]Loading train:  70%|███████   | 187/266 [02:33<00:52,  1.50it/s]Loading train:  71%|███████   | 188/266 [02:34<00:52,  1.49it/s]Loading train:  71%|███████   | 189/266 [02:34<00:50,  1.51it/s]Loading train:  71%|███████▏  | 190/266 [02:35<00:49,  1.52it/s]Loading train:  72%|███████▏  | 191/266 [02:36<01:01,  1.22it/s]Loading train:  72%|███████▏  | 192/266 [02:37<01:05,  1.13it/s]Loading train:  73%|███████▎  | 193/266 [02:38<01:06,  1.10it/s]Loading train:  73%|███████▎  | 194/266 [02:39<01:13,  1.02s/it]Loading train:  73%|███████▎  | 195/266 [02:40<01:06,  1.06it/s]Loading train:  74%|███████▎  | 196/266 [02:41<01:01,  1.13it/s]Loading train:  74%|███████▍  | 197/266 [02:42<00:58,  1.19it/s]Loading train:  74%|███████▍  | 198/266 [02:42<00:54,  1.25it/s]Loading train:  75%|███████▍  | 199/266 [02:43<00:52,  1.27it/s]Loading train:  75%|███████▌  | 200/266 [02:44<00:51,  1.28it/s]Loading train:  76%|███████▌  | 201/266 [02:45<00:51,  1.27it/s]Loading train:  76%|███████▌  | 202/266 [02:46<00:51,  1.24it/s]Loading train:  76%|███████▋  | 203/266 [02:46<00:50,  1.24it/s]Loading train:  77%|███████▋  | 204/266 [02:47<00:48,  1.28it/s]Loading train:  77%|███████▋  | 205/266 [02:48<00:45,  1.35it/s]Loading train:  77%|███████▋  | 206/266 [02:48<00:43,  1.39it/s]Loading train:  78%|███████▊  | 207/266 [02:49<00:42,  1.38it/s]Loading train:  78%|███████▊  | 208/266 [02:50<00:41,  1.41it/s]Loading train:  79%|███████▊  | 209/266 [02:51<00:40,  1.42it/s]Loading train:  79%|███████▉  | 210/266 [02:51<00:41,  1.35it/s]Loading train:  79%|███████▉  | 211/266 [02:52<00:41,  1.32it/s]Loading train:  80%|███████▉  | 212/266 [02:53<00:40,  1.32it/s]Loading train:  80%|████████  | 213/266 [02:54<00:38,  1.36it/s]Loading train:  80%|████████  | 214/266 [02:54<00:36,  1.42it/s]Loading train:  81%|████████  | 215/266 [02:55<00:35,  1.45it/s]Loading train:  81%|████████  | 216/266 [02:56<00:33,  1.49it/s]Loading train:  82%|████████▏ | 217/266 [02:56<00:32,  1.50it/s]Loading train:  82%|████████▏ | 218/266 [02:57<00:32,  1.49it/s]Loading train:  82%|████████▏ | 219/266 [02:58<00:31,  1.49it/s]Loading train:  83%|████████▎ | 220/266 [02:58<00:30,  1.50it/s]Loading train:  83%|████████▎ | 221/266 [02:59<00:29,  1.50it/s]Loading train:  83%|████████▎ | 222/266 [02:59<00:28,  1.55it/s]Loading train:  84%|████████▍ | 223/266 [03:00<00:27,  1.57it/s]Loading train:  84%|████████▍ | 224/266 [03:01<00:28,  1.49it/s]Loading train:  85%|████████▍ | 225/266 [03:02<00:28,  1.45it/s]Loading train:  85%|████████▍ | 226/266 [03:02<00:28,  1.38it/s]Loading train:  85%|████████▌ | 227/266 [03:03<00:28,  1.35it/s]Loading train:  86%|████████▌ | 228/266 [03:04<00:27,  1.36it/s]Loading train:  86%|████████▌ | 229/266 [03:04<00:26,  1.41it/s]Loading train:  86%|████████▋ | 230/266 [03:05<00:25,  1.41it/s]Loading train:  87%|████████▋ | 231/266 [03:06<00:24,  1.41it/s]Loading train:  87%|████████▋ | 232/266 [03:07<00:23,  1.44it/s]Loading train:  88%|████████▊ | 233/266 [03:07<00:23,  1.43it/s]Loading train:  88%|████████▊ | 234/266 [03:08<00:22,  1.43it/s]Loading train:  88%|████████▊ | 235/266 [03:09<00:21,  1.45it/s]Loading train:  89%|████████▊ | 236/266 [03:09<00:20,  1.45it/s]Loading train:  89%|████████▉ | 237/266 [03:10<00:20,  1.41it/s]Loading train:  89%|████████▉ | 238/266 [03:11<00:19,  1.44it/s]Loading train:  90%|████████▉ | 239/266 [03:11<00:18,  1.42it/s]Loading train:  90%|█████████ | 240/266 [03:12<00:18,  1.43it/s]Loading train:  91%|█████████ | 241/266 [03:13<00:16,  1.47it/s]Loading train:  91%|█████████ | 242/266 [03:13<00:16,  1.47it/s]Loading train:  91%|█████████▏| 243/266 [03:14<00:15,  1.48it/s]Loading train:  92%|█████████▏| 244/266 [03:15<00:15,  1.46it/s]Loading train:  92%|█████████▏| 245/266 [03:15<00:14,  1.50it/s]Loading train:  92%|█████████▏| 246/266 [03:16<00:13,  1.51it/s]Loading train:  93%|█████████▎| 247/266 [03:17<00:12,  1.51it/s]Loading train:  93%|█████████▎| 248/266 [03:17<00:11,  1.53it/s]Loading train:  94%|█████████▎| 249/266 [03:18<00:12,  1.36it/s]Loading train:  94%|█████████▍| 250/266 [03:19<00:12,  1.31it/s]Loading train:  94%|█████████▍| 251/266 [03:20<00:11,  1.27it/s]Loading train:  95%|█████████▍| 252/266 [03:21<00:11,  1.27it/s]Loading train:  95%|█████████▌| 253/266 [03:22<00:10,  1.24it/s]Loading train:  95%|█████████▌| 254/266 [03:22<00:09,  1.25it/s]Loading train:  96%|█████████▌| 255/266 [03:23<00:09,  1.21it/s]Loading train:  96%|█████████▌| 256/266 [03:24<00:08,  1.22it/s]Loading train:  97%|█████████▋| 257/266 [03:25<00:07,  1.23it/s]Loading train:  97%|█████████▋| 258/266 [03:26<00:06,  1.22it/s]Loading train:  97%|█████████▋| 259/266 [03:27<00:05,  1.24it/s]Loading train:  98%|█████████▊| 260/266 [03:27<00:04,  1.22it/s]Loading train:  98%|█████████▊| 261/266 [03:28<00:04,  1.23it/s]Loading train:  98%|█████████▊| 262/266 [03:29<00:03,  1.24it/s]Loading train:  99%|█████████▉| 263/266 [03:30<00:02,  1.15it/s]Loading train:  99%|█████████▉| 264/266 [03:31<00:01,  1.09it/s]Loading train: 100%|█████████▉| 265/266 [03:32<00:00,  1.10it/s]Loading train: 100%|██████████| 266/266 [03:33<00:00,  1.14it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 19/266 [00:00<00:01, 179.89it/s]concatenating: train:  15%|█▍        | 39/266 [00:00<00:01, 184.45it/s]concatenating: train:  21%|██▏       | 57/266 [00:00<00:01, 182.50it/s]concatenating: train:  29%|██▊       | 76/266 [00:00<00:01, 183.79it/s]concatenating: train:  36%|███▌      | 96/266 [00:00<00:00, 185.60it/s]concatenating: train:  43%|████▎     | 115/266 [00:00<00:00, 184.05it/s]concatenating: train:  50%|█████     | 133/266 [00:00<00:00, 180.02it/s]concatenating: train:  57%|█████▋    | 152/266 [00:00<00:00, 179.69it/s]concatenating: train:  64%|██████▎   | 169/266 [00:00<00:00, 175.86it/s]concatenating: train:  70%|███████   | 187/266 [00:01<00:00, 174.27it/s]concatenating: train:  77%|███████▋  | 204/266 [00:01<00:00, 172.05it/s]concatenating: train:  83%|████████▎ | 222/266 [00:01<00:00, 173.75it/s]concatenating: train:  91%|█████████▏| 243/266 [00:01<00:00, 181.73it/s]concatenating: train: 100%|█████████▉| 265/266 [00:01<00:00, 191.09it/s]concatenating: train: 100%|██████████| 266/266 [00:01<00:00, 183.54it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:01<00:05,  1.29s/it]Loading test:  40%|████      | 2/5 [00:02<00:03,  1.26s/it]Loading test:  60%|██████    | 3/5 [00:03<00:02,  1.20s/it]Loading test:  80%|████████  | 4/5 [00:04<00:01,  1.15s/it]Loading test: 100%|██████████| 5/5 [00:05<00:00,  1.20s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 647.53it/s]2019-08-16 23:56:41.291164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-16 23:56:41.291266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-16 23:56:41.291280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-16 23:56:41.291289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-16 23:56:41.291688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:08,  5.36it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:06,  6.31it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:06,  6.00it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:04,  7.67it/s]loading the weights for Res Unet:  20%|██        | 9/44 [00:00<00:05,  6.77it/s]loading the weights for Res Unet:  25%|██▌       | 11/44 [00:01<00:04,  7.67it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:04,  6.88it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:03,  8.83it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02,  9.37it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:02<00:03,  7.64it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:02,  9.48it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01, 10.10it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01, 10.52it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:02<00:01,  8.24it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:03<00:00, 10.19it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:03<00:00, 10.52it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00, 10.62it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:03<00:00,  8.14it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:03<00:00, 11.44it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 3  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 3  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 96, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 96, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 96, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 96, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 96, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 96, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 96, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 48, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 48, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 48, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 48, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 48, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 48, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 48, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 48, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 48, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 24, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 24, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 24, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 24, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 24, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 24, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 24, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 24, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 24, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 24, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 48, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 48, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 48, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 48, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 48, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 48, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 48, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 48, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 48, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 48, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 96, 20)   11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 96, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 96, 20)   7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 96, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 96, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 96, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 96, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 96, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 96, 60)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 96, 60)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 96, 13)   793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 23,653
Non-trainable params: 200,180
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.30157797e-02 3.26602037e-02 7.63713943e-02 9.48952977e-03
 2.74648600e-02 7.18540645e-03 8.77004830e-02 1.13521573e-01
 8.91305063e-02 1.35420255e-02 2.88978633e-01 1.90714164e-01
 2.25441041e-04]
Train on 9571 samples, validate on 177 samples
Epoch 1/300
 - 13s - loss: 3.0392 - acc: 0.7218 - mDice: 0.0774 - val_loss: 1.9731 - val_acc: 0.9173 - val_mDice: 0.1935

Epoch 00001: val_mDice improved from -inf to 0.19346, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 9s - loss: 1.6201 - acc: 0.8894 - mDice: 0.1984 - val_loss: 1.5668 - val_acc: 0.9168 - val_mDice: 0.2952

Epoch 00002: val_mDice improved from 0.19346 to 0.29517, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 9s - loss: 1.3063 - acc: 0.8906 - mDice: 0.2630 - val_loss: 1.2439 - val_acc: 0.9170 - val_mDice: 0.3445

Epoch 00003: val_mDice improved from 0.29517 to 0.34453, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 9s - loss: 1.1550 - acc: 0.8910 - mDice: 0.3032 - val_loss: 1.1191 - val_acc: 0.9173 - val_mDice: 0.3751

Epoch 00004: val_mDice improved from 0.34453 to 0.37514, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 9s - loss: 1.0923 - acc: 0.8912 - mDice: 0.3261 - val_loss: 1.1259 - val_acc: 0.9178 - val_mDice: 0.3863

Epoch 00005: val_mDice improved from 0.37514 to 0.38633, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 9s - loss: 1.0243 - acc: 0.8922 - mDice: 0.3473 - val_loss: 1.1110 - val_acc: 0.9191 - val_mDice: 0.3911

Epoch 00006: val_mDice improved from 0.38633 to 0.39106, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 9s - loss: 0.9763 - acc: 0.8937 - mDice: 0.3643 - val_loss: 1.0542 - val_acc: 0.9227 - val_mDice: 0.4073

Epoch 00007: val_mDice improved from 0.39106 to 0.40733, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 9s - loss: 0.9446 - acc: 0.8956 - mDice: 0.3766 - val_loss: 1.0425 - val_acc: 0.9266 - val_mDice: 0.4210

Epoch 00008: val_mDice improved from 0.40733 to 0.42101, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 9s - loss: 0.9200 - acc: 0.8985 - mDice: 0.3860 - val_loss: 1.0519 - val_acc: 0.9269 - val_mDice: 0.4143

Epoch 00009: val_mDice did not improve from 0.42101
Epoch 10/300
 - 9s - loss: 0.8904 - acc: 0.9007 - mDice: 0.3973 - val_loss: 1.0160 - val_acc: 0.9244 - val_mDice: 0.4336

Epoch 00010: val_mDice improved from 0.42101 to 0.43364, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 9s - loss: 0.8683 - acc: 0.9016 - mDice: 0.4059 - val_loss: 1.1576 - val_acc: 0.9252 - val_mDice: 0.4406

Epoch 00011: val_mDice improved from 0.43364 to 0.44055, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 9s - loss: 0.8557 - acc: 0.9028 - mDice: 0.4119 - val_loss: 0.9620 - val_acc: 0.9251 - val_mDice: 0.4463

Epoch 00012: val_mDice improved from 0.44055 to 0.44625, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 9s - loss: 0.8374 - acc: 0.9037 - mDice: 0.4190 - val_loss: 1.0803 - val_acc: 0.9215 - val_mDice: 0.4335

Epoch 00013: val_mDice did not improve from 0.44625
Epoch 14/300
 - 9s - loss: 0.8234 - acc: 0.9043 - mDice: 0.4246 - val_loss: 0.9508 - val_acc: 0.9266 - val_mDice: 0.4545

Epoch 00014: val_mDice improved from 0.44625 to 0.45448, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 9s - loss: 0.8080 - acc: 0.9049 - mDice: 0.4307 - val_loss: 0.9708 - val_acc: 0.9266 - val_mDice: 0.4532

Epoch 00015: val_mDice did not improve from 0.45448
Epoch 16/300
 - 9s - loss: 0.8002 - acc: 0.9056 - mDice: 0.4349 - val_loss: 0.9570 - val_acc: 0.9221 - val_mDice: 0.4575

Epoch 00016: val_mDice improved from 0.45448 to 0.45750, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 9s - loss: 0.7854 - acc: 0.9065 - mDice: 0.4409 - val_loss: 0.9903 - val_acc: 0.9266 - val_mDice: 0.4540

Epoch 00017: val_mDice did not improve from 0.45750
Epoch 18/300
 - 9s - loss: 0.7807 - acc: 0.9071 - mDice: 0.4438 - val_loss: 1.1539 - val_acc: 0.9261 - val_mDice: 0.4545

Epoch 00018: val_mDice did not improve from 0.45750
Epoch 19/300
 - 9s - loss: 0.7661 - acc: 0.9074 - mDice: 0.4494 - val_loss: 0.9284 - val_acc: 0.9246 - val_mDice: 0.4582

Epoch 00019: val_mDice improved from 0.45750 to 0.45819, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 9s - loss: 0.7589 - acc: 0.9080 - mDice: 0.4529 - val_loss: 0.9256 - val_acc: 0.9289 - val_mDice: 0.4730

Epoch 00020: val_mDice improved from 0.45819 to 0.47296, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 9s - loss: 0.7541 - acc: 0.9085 - mDice: 0.4553 - val_loss: 0.9312 - val_acc: 0.9244 - val_mDice: 0.4642

Epoch 00021: val_mDice did not improve from 0.47296
Epoch 22/300
 - 9s - loss: 0.7486 - acc: 0.9087 - mDice: 0.4578 - val_loss: 1.0505 - val_acc: 0.9239 - val_mDice: 0.4625

Epoch 00022: val_mDice did not improve from 0.47296
Epoch 23/300
 - 9s - loss: 0.7436 - acc: 0.9093 - mDice: 0.4605 - val_loss: 0.9192 - val_acc: 0.9252 - val_mDice: 0.4648

Epoch 00023: val_mDice did not improve from 0.47296
Epoch 24/300
 - 9s - loss: 0.7348 - acc: 0.9096 - mDice: 0.4644 - val_loss: 0.8953 - val_acc: 0.9295 - val_mDice: 0.4795

Epoch 00024: val_mDice improved from 0.47296 to 0.47953, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 9s - loss: 0.7287 - acc: 0.9100 - mDice: 0.4673 - val_loss: 0.9477 - val_acc: 0.9296 - val_mDice: 0.4724

Epoch 00025: val_mDice did not improve from 0.47953
Epoch 26/300
 - 9s - loss: 0.7216 - acc: 0.9105 - mDice: 0.4705 - val_loss: 0.9359 - val_acc: 0.9302 - val_mDice: 0.4783

Epoch 00026: val_mDice did not improve from 0.47953
Epoch 27/300
 - 9s - loss: 0.7246 - acc: 0.9106 - mDice: 0.4711 - val_loss: 0.9823 - val_acc: 0.9286 - val_mDice: 0.4753

Epoch 00027: val_mDice did not improve from 0.47953
Epoch 28/300
 - 9s - loss: 0.7067 - acc: 0.9112 - mDice: 0.4775 - val_loss: 0.9094 - val_acc: 0.9306 - val_mDice: 0.4851

Epoch 00028: val_mDice improved from 0.47953 to 0.48513, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 9s - loss: 0.7038 - acc: 0.9118 - mDice: 0.4787 - val_loss: 0.9884 - val_acc: 0.9299 - val_mDice: 0.4883

Epoch 00029: val_mDice improved from 0.48513 to 0.48834, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 9s - loss: 0.7055 - acc: 0.9119 - mDice: 0.4785 - val_loss: 1.0543 - val_acc: 0.9299 - val_mDice: 0.4869

Epoch 00030: val_mDice did not improve from 0.48834
Epoch 31/300
 - 9s - loss: 0.6962 - acc: 0.9122 - mDice: 0.4828 - val_loss: 0.9115 - val_acc: 0.9270 - val_mDice: 0.4852

Epoch 00031: val_mDice did not improve from 0.48834
Epoch 32/300
 - 9s - loss: 0.6929 - acc: 0.9123 - mDice: 0.4843 - val_loss: 0.8329 - val_acc: 0.9318 - val_mDice: 0.4970

Epoch 00032: val_mDice improved from 0.48834 to 0.49697, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 9s - loss: 0.6875 - acc: 0.9130 - mDice: 0.4872 - val_loss: 0.9206 - val_acc: 0.9297 - val_mDice: 0.4808

Epoch 00033: val_mDice did not improve from 0.49697
Epoch 34/300
 - 9s - loss: 0.6869 - acc: 0.9131 - mDice: 0.4878 - val_loss: 0.8277 - val_acc: 0.9321 - val_mDice: 0.4951

Epoch 00034: val_mDice did not improve from 0.49697
Epoch 35/300
 - 9s - loss: 0.7166 - acc: 0.9113 - mDice: 0.4746 - val_loss: 0.8613 - val_acc: 0.9276 - val_mDice: 0.4925

Epoch 00035: val_mDice did not improve from 0.49697
Epoch 36/300
 - 9s - loss: 0.6814 - acc: 0.9130 - mDice: 0.4897 - val_loss: 0.8196 - val_acc: 0.9292 - val_mDice: 0.4969

Epoch 00036: val_mDice did not improve from 0.49697
Epoch 37/300
 - 9s - loss: 0.6828 - acc: 0.9133 - mDice: 0.4908 - val_loss: 0.9005 - val_acc: 0.9299 - val_mDice: 0.4877

Epoch 00037: val_mDice did not improve from 0.49697
Epoch 38/300
 - 9s - loss: 0.6749 - acc: 0.9137 - mDice: 0.4933 - val_loss: 0.8723 - val_acc: 0.9314 - val_mDice: 0.4971

Epoch 00038: val_mDice improved from 0.49697 to 0.49713, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 9s - loss: 0.6744 - acc: 0.9140 - mDice: 0.4938 - val_loss: 0.8661 - val_acc: 0.9297 - val_mDice: 0.4881

Epoch 00039: val_mDice did not improve from 0.49713
Epoch 40/300
 - 9s - loss: 0.6717 - acc: 0.9141 - mDice: 0.4945 - val_loss: 0.8796 - val_acc: 0.9320 - val_mDice: 0.4959

Epoch 00040: val_mDice did not improve from 0.49713
Epoch 41/300
 - 9s - loss: 0.6813 - acc: 0.9140 - mDice: 0.4943 - val_loss: 1.0677 - val_acc: 0.9286 - val_mDice: 0.4873

Epoch 00041: val_mDice did not improve from 0.49713
Epoch 42/300
 - 9s - loss: 0.6641 - acc: 0.9147 - mDice: 0.4988 - val_loss: 0.9547 - val_acc: 0.9308 - val_mDice: 0.4956

Epoch 00042: val_mDice did not improve from 0.49713
Epoch 43/300
 - 9s - loss: 0.6658 - acc: 0.9146 - mDice: 0.4985 - val_loss: 0.8570 - val_acc: 0.9318 - val_mDice: 0.4972

Epoch 00043: val_mDice improved from 0.49713 to 0.49722, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 44/300
 - 9s - loss: 0.6630 - acc: 0.9152 - mDice: 0.5003 - val_loss: 0.8371 - val_acc: 0.9316 - val_mDice: 0.4960

Epoch 00044: val_mDice did not improve from 0.49722
Epoch 45/300
 - 9s - loss: 0.6724 - acc: 0.9139 - mDice: 0.4952 - val_loss: 0.8884 - val_acc: 0.9326 - val_mDice: 0.4952

Epoch 00045: val_mDice did not improve from 0.49722
Epoch 46/300
 - 9s - loss: 0.6598 - acc: 0.9152 - mDice: 0.5014 - val_loss: 0.8566 - val_acc: 0.9322 - val_mDice: 0.4970

Epoch 00046: val_mDice did not improve from 0.49722
Epoch 47/300
 - 9s - loss: 0.6555 - acc: 0.9155 - mDice: 0.5035 - val_loss: 0.9220 - val_acc: 0.9303 - val_mDice: 0.5011

Epoch 00047: val_mDice improved from 0.49722 to 0.50111, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 9s - loss: 0.6529 - acc: 0.9154 - mDice: 0.5045 - val_loss: 0.9091 - val_acc: 0.9273 - val_mDice: 0.4911

Epoch 00048: val_mDice did not improve from 0.50111
Epoch 49/300
 - 9s - loss: 0.6520 - acc: 0.9156 - mDice: 0.5056 - val_loss: 0.8634 - val_acc: 0.9292 - val_mDice: 0.4954

Epoch 00049: val_mDice did not improve from 0.50111
Epoch 50/300
 - 9s - loss: 0.6466 - acc: 0.9159 - mDice: 0.5079 - val_loss: 0.8528 - val_acc: 0.9294 - val_mDice: 0.4955

Epoch 00050: val_mDice did not improve from 0.50111
Epoch 51/300
 - 9s - loss: 0.6517 - acc: 0.9160 - mDice: 0.5067 - val_loss: 0.8481 - val_acc: 0.9321 - val_mDice: 0.4955

Epoch 00051: val_mDice did not improve from 0.50111
Epoch 52/300
 - 9s - loss: 0.6462 - acc: 0.9160 - mDice: 0.5081 - val_loss: 0.8485 - val_acc: 0.9299 - val_mDice: 0.4977

Epoch 00052: val_mDice did not improve from 0.50111
Epoch 53/300
 - 9s - loss: 0.6763 - acc: 0.9137 - mDice: 0.4952 - val_loss: 0.8746 - val_acc: 0.9286 - val_mDice: 0.4988

Epoch 00053: val_mDice did not improve from 0.50111
Epoch 54/300
 - 9s - loss: 0.6489 - acc: 0.9160 - mDice: 0.5066 - val_loss: 0.9104 - val_acc: 0.9313 - val_mDice: 0.4985

Epoch 00054: val_mDice did not improve from 0.50111
Epoch 55/300
 - 9s - loss: 0.6451 - acc: 0.9160 - mDice: 0.5090 - val_loss: 0.9078 - val_acc: 0.9305 - val_mDice: 0.4970

Epoch 00055: val_mDice did not improve from 0.50111
Epoch 56/300
 - 9s - loss: 0.6388 - acc: 0.9165 - mDice: 0.5120 - val_loss: 0.8885 - val_acc: 0.9322 - val_mDice: 0.5002

Epoch 00056: val_mDice did not improve from 0.50111
Epoch 57/300
 - 9s - loss: 0.6368 - acc: 0.9167 - mDice: 0.5127 - val_loss: 0.8747 - val_acc: 0.9310 - val_mDice: 0.4956

Epoch 00057: val_mDice did not improve from 0.50111
Epoch 58/300
 - 9s - loss: 0.6380 - acc: 0.9168 - mDice: 0.5125 - val_loss: 0.8877 - val_acc: 0.9333 - val_mDice: 0.4984

Epoch 00058: val_mDice did not improve from 0.50111
Epoch 59/300
 - 9s - loss: 0.6347 - acc: 0.9168 - mDice: 0.5139 - val_loss: 0.9980 - val_acc: 0.9303 - val_mDice: 0.5011

Epoch 00059: val_mDice did not improve from 0.50111
Epoch 60/300
 - 9s - loss: 0.6344 - acc: 0.9171 - mDice: 0.5141 - val_loss: 0.8800 - val_acc: 0.9286 - val_mDice: 0.4965

Epoch 00060: val_mDice did not improve from 0.50111
Epoch 61/300
 - 9s - loss: 0.6306 - acc: 0.9171 - mDice: 0.5161 - val_loss: 0.8633 - val_acc: 0.9321 - val_mDice: 0.4982

Epoch 00061: val_mDice did not improve from 0.50111
Epoch 62/300
 - 9s - loss: 0.6312 - acc: 0.9173 - mDice: 0.5162 - val_loss: 0.8633 - val_acc: 0.9304 - val_mDice: 0.5013

Epoch 00062: val_mDice improved from 0.50111 to 0.50128, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 9s - loss: 0.6291 - acc: 0.9175 - mDice: 0.5171 - val_loss: 0.8886 - val_acc: 0.9336 - val_mDice: 0.5035

Epoch 00063: val_mDice improved from 0.50128 to 0.50345, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 9s - loss: 0.6266 - acc: 0.9176 - mDice: 0.5181 - val_loss: 0.8686 - val_acc: 0.9323 - val_mDice: 0.5104

Epoch 00064: val_mDice improved from 0.50345 to 0.51040, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 65/300
 - 9s - loss: 0.6255 - acc: 0.9178 - mDice: 0.5193 - val_loss: 0.8895 - val_acc: 0.9317 - val_mDice: 0.5031

Epoch 00065: val_mDice did not improve from 0.51040
Epoch 66/300
 - 9s - loss: 0.6234 - acc: 0.9180 - mDice: 0.5202 - val_loss: 0.8700 - val_acc: 0.9315 - val_mDice: 0.4992

Epoch 00066: val_mDice did not improve from 0.51040
Epoch 67/300
 - 9s - loss: 0.6240 - acc: 0.9178 - mDice: 0.5197 - val_loss: 0.8932 - val_acc: 0.9325 - val_mDice: 0.5020

Epoch 00067: val_mDice did not improve from 0.51040
Epoch 68/300
 - 9s - loss: 0.6229 - acc: 0.9179 - mDice: 0.5201 - val_loss: 0.9683 - val_acc: 0.9280 - val_mDice: 0.4954

Epoch 00068: val_mDice did not improve from 0.51040
Epoch 69/300
 - 9s - loss: 0.6246 - acc: 0.9179 - mDice: 0.5195 - val_loss: 0.8769 - val_acc: 0.9321 - val_mDice: 0.4982

Epoch 00069: val_mDice did not improve from 0.51040
Epoch 70/300
 - 9s - loss: 0.6195 - acc: 0.9184 - mDice: 0.5217 - val_loss: 0.9188 - val_acc: 0.9328 - val_mDice: 0.5013

Epoch 00070: val_mDice did not improve from 0.51040
Epoch 71/300
 - 9s - loss: 0.6208 - acc: 0.9183 - mDice: 0.5214 - val_loss: 0.9013 - val_acc: 0.9329 - val_mDice: 0.4970

Epoch 00071: val_mDice did not improve from 0.51040
Epoch 72/300
 - 9s - loss: 0.6165 - acc: 0.9186 - mDice: 0.5236 - val_loss: 0.8539 - val_acc: 0.9315 - val_mDice: 0.5033

Epoch 00072: val_mDice did not improve from 0.51040
Epoch 73/300
 - 9s - loss: 0.6143 - acc: 0.9187 - mDice: 0.5248 - val_loss: 0.9146 - val_acc: 0.9329 - val_mDice: 0.5104

Epoch 00073: val_mDice did not improve from 0.51040
Epoch 74/300
 - 9s - loss: 0.6137 - acc: 0.9189 - mDice: 0.5253 - val_loss: 0.9569 - val_acc: 0.9292 - val_mDice: 0.4953

Epoch 00074: val_mDice did not improve from 0.51040
Epoch 75/300
 - 9s - loss: 0.6161 - acc: 0.9188 - mDice: 0.5241 - val_loss: 0.9012 - val_acc: 0.9299 - val_mDice: 0.4967

Epoch 00075: val_mDice did not improve from 0.51040
Epoch 76/300
 - 9s - loss: 0.6124 - acc: 0.9189 - mDice: 0.5257 - val_loss: 0.8963 - val_acc: 0.9306 - val_mDice: 0.5031

Epoch 00076: val_mDice did not improve from 0.51040
Epoch 77/300
 - 9s - loss: 0.6137 - acc: 0.9190 - mDice: 0.5252 - val_loss: 0.8951 - val_acc: 0.9322 - val_mDice: 0.4976

Epoch 00077: val_mDice did not improve from 0.51040
Epoch 78/300
 - 9s - loss: 0.6096 - acc: 0.9191 - mDice: 0.5273 - val_loss: 0.8665 - val_acc: 0.9328 - val_mDice: 0.5113

Epoch 00078: val_mDice improved from 0.51040 to 0.51130, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 79/300
 - 10s - loss: 0.6113 - acc: 0.9192 - mDice: 0.5271 - val_loss: 0.9012 - val_acc: 0.9331 - val_mDice: 0.5049

Epoch 00079: val_mDice did not improve from 0.51130
Epoch 80/300
 - 9s - loss: 0.6066 - acc: 0.9192 - mDice: 0.5290 - val_loss: 1.0078 - val_acc: 0.9340 - val_mDice: 0.5064

Epoch 00080: val_mDice did not improve from 0.51130
Epoch 81/300
 - 9s - loss: 0.6104 - acc: 0.9191 - mDice: 0.5272 - val_loss: 0.8478 - val_acc: 0.9326 - val_mDice: 0.5083

Epoch 00081: val_mDice did not improve from 0.51130
Epoch 82/300
 - 9s - loss: 0.6062 - acc: 0.9193 - mDice: 0.5293 - val_loss: 1.0159 - val_acc: 0.9322 - val_mDice: 0.4991

Epoch 00082: val_mDice did not improve from 0.51130
Epoch 83/300
 - 9s - loss: 0.6055 - acc: 0.9195 - mDice: 0.5297 - val_loss: 0.8899 - val_acc: 0.9331 - val_mDice: 0.5077

Epoch 00083: val_mDice did not improve from 0.51130
Epoch 84/300
 - 9s - loss: 0.6079 - acc: 0.9194 - mDice: 0.5288 - val_loss: 0.9934 - val_acc: 0.9324 - val_mDice: 0.5020

Epoch 00084: val_mDice did not improve from 0.51130
Epoch 85/300
 - 9s - loss: 0.6074 - acc: 0.9197 - mDice: 0.5289 - val_loss: 0.8709 - val_acc: 0.9319 - val_mDice: 0.5090

Epoch 00085: val_mDice did not improve from 0.51130
Epoch 86/300
 - 9s - loss: 0.6046 - acc: 0.9199 - mDice: 0.5304 - val_loss: 0.9179 - val_acc: 0.9321 - val_mDice: 0.5103

Epoch 00086: val_mDice did not improve from 0.51130
Epoch 87/300
 - 9s - loss: 0.5990 - acc: 0.9197 - mDice: 0.5329 - val_loss: 0.8979 - val_acc: 0.9329 - val_mDice: 0.5061

Epoch 00087: val_mDice did not improve from 0.51130
Epoch 88/300
 - 9s - loss: 0.6006 - acc: 0.9198 - mDice: 0.5322 - val_loss: 0.8925 - val_acc: 0.9310 - val_mDice: 0.5006

Epoch 00088: val_mDice did not improve from 0.51130
Epoch 89/300
 - 9s - loss: 0.6016 - acc: 0.9199 - mDice: 0.5321 - val_loss: 0.9143 - val_acc: 0.9319 - val_mDice: 0.4959

Epoch 00089: val_mDice did not improve from 0.51130
Epoch 90/300
 - 9s - loss: 0.6009 - acc: 0.9200 - mDice: 0.5326 - val_loss: 0.8796 - val_acc: 0.9314 - val_mDice: 0.5005

Epoch 00090: val_mDice did not improve from 0.51130
Epoch 91/300
 - 9s - loss: 0.6011 - acc: 0.9200 - mDice: 0.5322 - val_loss: 1.0928 - val_acc: 0.9307 - val_mDice: 0.4965

Epoch 00091: val_mDice did not improve from 0.51130
Epoch 92/300
 - 9s - loss: 0.6045 - acc: 0.9203 - mDice: 0.5326 - val_loss: 1.0820 - val_acc: 0.9285 - val_mDice: 0.5015

Epoch 00092: val_mDice did not improve from 0.51130
Epoch 93/300
 - 9s - loss: 0.6139 - acc: 0.9196 - mDice: 0.5268 - val_loss: 0.8885 - val_acc: 0.9332 - val_mDice: 0.5085

Epoch 00093: val_mDice did not improve from 0.51130
Epoch 94/300
 - 9s - loss: 0.5993 - acc: 0.9202 - mDice: 0.5333 - val_loss: 1.0707 - val_acc: 0.9336 - val_mDice: 0.5100

Epoch 00094: val_mDice did not improve from 0.51130
Epoch 95/300
 - 9s - loss: 0.5973 - acc: 0.9204 - mDice: 0.5341 - val_loss: 1.0633 - val_acc: 0.9319 - val_mDice: 0.5046

Epoch 00095: val_mDice did not improve from 0.51130
Epoch 96/300
 - 9s - loss: 0.5916 - acc: 0.9206 - mDice: 0.5370 - val_loss: 0.8966 - val_acc: 0.9337 - val_mDice: 0.5116

Epoch 00096: val_mDice improved from 0.51130 to 0.51162, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 97/300
 - 9s - loss: 0.5968 - acc: 0.9205 - mDice: 0.5348 - val_loss: 0.8797 - val_acc: 0.9330 - val_mDice: 0.5064

Epoch 00097: val_mDice did not improve from 0.51162
Epoch 98/300
 - 9s - loss: 0.5969 - acc: 0.9206 - mDice: 0.5350 - val_loss: 1.0869 - val_acc: 0.9320 - val_mDice: 0.5041

Epoch 00098: val_mDice did not improve from 0.51162
Epoch 99/300
 - 9s - loss: 0.5931 - acc: 0.9206 - mDice: 0.5369 - val_loss: 0.8884 - val_acc: 0.9347 - val_mDice: 0.5087

Epoch 00099: val_mDice did not improve from 0.51162
Epoch 100/300
 - 9s - loss: 0.5955 - acc: 0.9207 - mDice: 0.5358 - val_loss: 0.8962 - val_acc: 0.9317 - val_mDice: 0.5048

Epoch 00100: val_mDice did not improve from 0.51162
Epoch 101/300
 - 9s - loss: 0.5916 - acc: 0.9206 - mDice: 0.5369 - val_loss: 0.9106 - val_acc: 0.9316 - val_mDice: 0.5017

Epoch 00101: val_mDice did not improve from 0.51162
Epoch 102/300
 - 9s - loss: 0.5940 - acc: 0.9208 - mDice: 0.5363 - val_loss: 0.8627 - val_acc: 0.9351 - val_mDice: 0.5167

Epoch 00102: val_mDice improved from 0.51162 to 0.51671, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 103/300
 - 9s - loss: 0.5922 - acc: 0.9208 - mDice: 0.5371 - val_loss: 0.9019 - val_acc: 0.9322 - val_mDice: 0.5041

Epoch 00103: val_mDice did not improve from 0.51671
Epoch 104/300
 - 9s - loss: 0.5909 - acc: 0.9209 - mDice: 0.5375 - val_loss: 0.8542 - val_acc: 0.9327 - val_mDice: 0.5168

Epoch 00104: val_mDice improved from 0.51671 to 0.51683, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 105/300
 - 9s - loss: 0.5933 - acc: 0.9210 - mDice: 0.5369 - val_loss: 0.8538 - val_acc: 0.9360 - val_mDice: 0.5183

Epoch 00105: val_mDice improved from 0.51683 to 0.51829, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 106/300
 - 9s - loss: 0.5886 - acc: 0.9210 - mDice: 0.5390 - val_loss: 0.8824 - val_acc: 0.9323 - val_mDice: 0.5071

Epoch 00106: val_mDice did not improve from 0.51829
Epoch 107/300
 - 9s - loss: 0.5907 - acc: 0.9212 - mDice: 0.5379 - val_loss: 0.8757 - val_acc: 0.9336 - val_mDice: 0.5106

Epoch 00107: val_mDice did not improve from 0.51829
Epoch 108/300
 - 9s - loss: 0.5892 - acc: 0.9211 - mDice: 0.5387 - val_loss: 0.8944 - val_acc: 0.9330 - val_mDice: 0.5032

Epoch 00108: val_mDice did not improve from 0.51829
Epoch 109/300
 - 9s - loss: 0.5864 - acc: 0.9212 - mDice: 0.5404 - val_loss: 0.8771 - val_acc: 0.9332 - val_mDice: 0.5087

Epoch 00109: val_mDice did not improve from 0.51829
Epoch 110/300
 - 9s - loss: 0.5861 - acc: 0.9215 - mDice: 0.5405 - val_loss: 0.8796 - val_acc: 0.9325 - val_mDice: 0.5104

Epoch 00110: val_mDice did not improve from 0.51829
Epoch 111/300
 - 9s - loss: 0.5882 - acc: 0.9212 - mDice: 0.5395 - val_loss: 0.8838 - val_acc: 0.9327 - val_mDice: 0.5080

Epoch 00111: val_mDice did not improve from 0.51829
Epoch 112/300
 - 9s - loss: 0.5863 - acc: 0.9214 - mDice: 0.5405 - val_loss: 0.9371 - val_acc: 0.9323 - val_mDice: 0.5006

Epoch 00112: val_mDice did not improve from 0.51829
Epoch 113/300
 - 9s - loss: 0.5864 - acc: 0.9212 - mDice: 0.5403 - val_loss: 0.8829 - val_acc: 0.9341 - val_mDice: 0.4998

Epoch 00113: val_mDice did not improve from 0.51829
Epoch 114/300
 - 9s - loss: 0.5864 - acc: 0.9215 - mDice: 0.5406 - val_loss: 0.9143 - val_acc: 0.9305 - val_mDice: 0.4999

Epoch 00114: val_mDice did not improve from 0.51829
Epoch 115/300
 - 9s - loss: 0.5836 - acc: 0.9216 - mDice: 0.5422 - val_loss: 0.8861 - val_acc: 0.9324 - val_mDice: 0.5060

Epoch 00115: val_mDice did not improve from 0.51829
Epoch 116/300
 - 9s - loss: 0.5823 - acc: 0.9216 - mDice: 0.5426 - val_loss: 0.8685 - val_acc: 0.9350 - val_mDice: 0.5156

Epoch 00116: val_mDice did not improve from 0.51829
Epoch 117/300
 - 9s - loss: 0.5812 - acc: 0.9218 - mDice: 0.5432 - val_loss: 0.8770 - val_acc: 0.9323 - val_mDice: 0.5103

Epoch 00117: val_mDice did not improve from 0.51829
Epoch 118/300
 - 9s - loss: 0.5815 - acc: 0.9217 - mDice: 0.5434 - val_loss: 0.9054 - val_acc: 0.9333 - val_mDice: 0.5007

Epoch 00118: val_mDice did not improve from 0.51829
Epoch 119/300
 - 9s - loss: 0.5820 - acc: 0.9218 - mDice: 0.5427 - val_loss: 0.8613 - val_acc: 0.9338 - val_mDice: 0.5127

Epoch 00119: val_mDice did not improve from 0.51829
Epoch 120/300
 - 9s - loss: 0.5798 - acc: 0.9218 - mDice: 0.5437 - val_loss: 0.8579 - val_acc: 0.9357 - val_mDice: 0.5217

Epoch 00120: val_mDice improved from 0.51829 to 0.52168, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 121/300
 - 9s - loss: 0.5840 - acc: 0.9220 - mDice: 0.5429 - val_loss: 0.8708 - val_acc: 0.9348 - val_mDice: 0.5146

Epoch 00121: val_mDice did not improve from 0.52168
Epoch 122/300
 - 9s - loss: 0.5818 - acc: 0.9216 - mDice: 0.5429 - val_loss: 0.8640 - val_acc: 0.9337 - val_mDice: 0.5161

Epoch 00122: val_mDice did not improve from 0.52168
Epoch 123/300
 - 9s - loss: 0.5795 - acc: 0.9221 - mDice: 0.5443 - val_loss: 1.0895 - val_acc: 0.9327 - val_mDice: 0.5074

Epoch 00123: val_mDice did not improve from 0.52168
Epoch 124/300
 - 9s - loss: 0.5795 - acc: 0.9220 - mDice: 0.5444 - val_loss: 0.8809 - val_acc: 0.9350 - val_mDice: 0.5112

Epoch 00124: val_mDice did not improve from 0.52168
Epoch 125/300
 - 9s - loss: 0.5795 - acc: 0.9221 - mDice: 0.5445 - val_loss: 0.8669 - val_acc: 0.9325 - val_mDice: 0.5127

Epoch 00125: val_mDice did not improve from 0.52168
Epoch 126/300
 - 9s - loss: 0.5788 - acc: 0.9222 - mDice: 0.5453 - val_loss: 1.0867 - val_acc: 0.9320 - val_mDice: 0.5062

Epoch 00126: val_mDice did not improve from 0.52168
Epoch 127/300
 - 9s - loss: 0.5810 - acc: 0.9222 - mDice: 0.5436 - val_loss: 0.8684 - val_acc: 0.9345 - val_mDice: 0.5127

Epoch 00127: val_mDice did not improve from 0.52168
Epoch 128/300
 - 10s - loss: 0.5850 - acc: 0.9223 - mDice: 0.5458 - val_loss: 0.8756 - val_acc: 0.9343 - val_mDice: 0.5103

Epoch 00128: val_mDice did not improve from 0.52168
Epoch 129/300
 - 10s - loss: 0.5779 - acc: 0.9223 - mDice: 0.5457 - val_loss: 0.8610 - val_acc: 0.9348 - val_mDice: 0.5183

Epoch 00129: val_mDice did not improve from 0.52168
Epoch 130/300
 - 10s - loss: 0.5759 - acc: 0.9224 - mDice: 0.5463 - val_loss: 0.8691 - val_acc: 0.9370 - val_mDice: 0.5146

Epoch 00130: val_mDice did not improve from 0.52168
Epoch 131/300
 - 10s - loss: 0.5751 - acc: 0.9225 - mDice: 0.5469 - val_loss: 0.8617 - val_acc: 0.9336 - val_mDice: 0.5166

Epoch 00131: val_mDice did not improve from 0.52168
Epoch 132/300
 - 10s - loss: 0.5752 - acc: 0.9222 - mDice: 0.5466 - val_loss: 0.8740 - val_acc: 0.9330 - val_mDice: 0.5112

Epoch 00132: val_mDice did not improve from 0.52168
Epoch 133/300
 - 10s - loss: 0.5726 - acc: 0.9225 - mDice: 0.5478 - val_loss: 0.8684 - val_acc: 0.9355 - val_mDice: 0.5145

Epoch 00133: val_mDice did not improve from 0.52168
Epoch 134/300
 - 10s - loss: 0.5781 - acc: 0.9224 - mDice: 0.5472 - val_loss: 0.8970 - val_acc: 0.9322 - val_mDice: 0.5060

Epoch 00134: val_mDice did not improve from 0.52168
Epoch 135/300
 - 10s - loss: 0.5732 - acc: 0.9226 - mDice: 0.5479 - val_loss: 1.0776 - val_acc: 0.9294 - val_mDice: 0.5127

Epoch 00135: val_mDice did not improve from 0.52168
Epoch 136/300
 - 9s - loss: 0.5748 - acc: 0.9225 - mDice: 0.5471 - val_loss: 0.8613 - val_acc: 0.9362 - val_mDice: 0.5178

Epoch 00136: val_mDice did not improve from 0.52168
Epoch 137/300
 - 10s - loss: 0.5733 - acc: 0.9227 - mDice: 0.5481 - val_loss: 0.8777 - val_acc: 0.9317 - val_mDice: 0.5076

Epoch 00137: val_mDice did not improve from 0.52168
Epoch 138/300
 - 10s - loss: 0.5729 - acc: 0.9226 - mDice: 0.5484 - val_loss: 0.8496 - val_acc: 0.9348 - val_mDice: 0.5204

Epoch 00138: val_mDice did not improve from 0.52168
Epoch 139/300
 - 10s - loss: 0.5736 - acc: 0.9227 - mDice: 0.5477 - val_loss: 0.8845 - val_acc: 0.9337 - val_mDice: 0.5108

Epoch 00139: val_mDice did not improve from 0.52168
Epoch 140/300
 - 10s - loss: 0.5724 - acc: 0.9229 - mDice: 0.5485 - val_loss: 0.8784 - val_acc: 0.9336 - val_mDice: 0.5123

Epoch 00140: val_mDice did not improve from 0.52168
Epoch 141/300
 - 10s - loss: 0.5713 - acc: 0.9228 - mDice: 0.5489 - val_loss: 0.8879 - val_acc: 0.9341 - val_mDice: 0.5081

Epoch 00141: val_mDice did not improve from 0.52168
Epoch 142/300
 - 10s - loss: 0.5750 - acc: 0.9227 - mDice: 0.5482 - val_loss: 0.8843 - val_acc: 0.9316 - val_mDice: 0.5083

Epoch 00142: val_mDice did not improve from 0.52168
Epoch 143/300
 - 10s - loss: 0.5754 - acc: 0.9228 - mDice: 0.5474 - val_loss: 0.8680 - val_acc: 0.9352 - val_mDice: 0.5161

Epoch 00143: val_mDice did not improve from 0.52168
Epoch 144/300
 - 10s - loss: 0.5702 - acc: 0.9230 - mDice: 0.5497 - val_loss: 0.8649 - val_acc: 0.9326 - val_mDice: 0.5146

Epoch 00144: val_mDice did not improve from 0.52168
Epoch 145/300
 - 10s - loss: 0.5720 - acc: 0.9231 - mDice: 0.5501 - val_loss: 0.8845 - val_acc: 0.9352 - val_mDice: 0.5115

Epoch 00145: val_mDice did not improve from 0.52168
Epoch 146/300
 - 9s - loss: 0.5716 - acc: 0.9231 - mDice: 0.5491 - val_loss: 0.8542 - val_acc: 0.9353 - val_mDice: 0.5240

Epoch 00146: val_mDice improved from 0.52168 to 0.52396, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 147/300
 - 10s - loss: 0.5708 - acc: 0.9230 - mDice: 0.5494 - val_loss: 0.8829 - val_acc: 0.9320 - val_mDice: 0.5098

Epoch 00147: val_mDice did not improve from 0.52396
Epoch 148/300
 - 9s - loss: 0.5667 - acc: 0.9233 - mDice: 0.5513 - val_loss: 0.8753 - val_acc: 0.9310 - val_mDice: 0.5121

Epoch 00148: val_mDice did not improve from 0.52396
Epoch 149/300
 - 10s - loss: 0.5667 - acc: 0.9231 - mDice: 0.5514 - val_loss: 0.8740 - val_acc: 0.9364 - val_mDice: 0.5129

Epoch 00149: val_mDice did not improve from 0.52396
Epoch 150/300
 - 10s - loss: 0.5688 - acc: 0.9231 - mDice: 0.5504 - val_loss: 0.8650 - val_acc: 0.9313 - val_mDice: 0.5123

Epoch 00150: val_mDice did not improve from 0.52396
Epoch 151/300
 - 10s - loss: 0.5652 - acc: 0.9234 - mDice: 0.5522 - val_loss: 0.8393 - val_acc: 0.9352 - val_mDice: 0.5269

Epoch 00151: val_mDice improved from 0.52396 to 0.52690, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 152/300
 - 9s - loss: 0.5644 - acc: 0.9235 - mDice: 0.5526 - val_loss: 0.9282 - val_acc: 0.9342 - val_mDice: 0.5144

Epoch 00152: val_mDice did not improve from 0.52690
Epoch 153/300
 - 10s - loss: 0.5641 - acc: 0.9235 - mDice: 0.5525 - val_loss: 0.8714 - val_acc: 0.9332 - val_mDice: 0.5117

Epoch 00153: val_mDice did not improve from 0.52690
Epoch 154/300
 - 10s - loss: 0.5658 - acc: 0.9235 - mDice: 0.5523 - val_loss: 0.8523 - val_acc: 0.9350 - val_mDice: 0.5239

Epoch 00154: val_mDice did not improve from 0.52690
Epoch 155/300
 - 10s - loss: 0.5655 - acc: 0.9236 - mDice: 0.5521 - val_loss: 0.8609 - val_acc: 0.9348 - val_mDice: 0.5174

Epoch 00155: val_mDice did not improve from 0.52690
Epoch 156/300
 - 9s - loss: 0.5668 - acc: 0.9237 - mDice: 0.5518 - val_loss: 0.8677 - val_acc: 0.9346 - val_mDice: 0.5148

Epoch 00156: val_mDice did not improve from 0.52690
Epoch 157/300
 - 10s - loss: 0.5651 - acc: 0.9237 - mDice: 0.5526 - val_loss: 0.8764 - val_acc: 0.9344 - val_mDice: 0.5124

Epoch 00157: val_mDice did not improve from 0.52690
Epoch 158/300
 - 9s - loss: 0.5695 - acc: 0.9238 - mDice: 0.5531 - val_loss: 1.0956 - val_acc: 0.9327 - val_mDice: 0.5072

Epoch 00158: val_mDice did not improve from 0.52690
Epoch 159/300
 - 10s - loss: 0.5720 - acc: 0.9235 - mDice: 0.5497 - val_loss: 0.8621 - val_acc: 0.9350 - val_mDice: 0.5139

Epoch 00159: val_mDice did not improve from 0.52690
Epoch 160/300
 - 10s - loss: 0.5623 - acc: 0.9237 - mDice: 0.5538 - val_loss: 0.8767 - val_acc: 0.9334 - val_mDice: 0.5134

Epoch 00160: val_mDice did not improve from 0.52690
Epoch 161/300
 - 10s - loss: 0.5642 - acc: 0.9236 - mDice: 0.5532 - val_loss: 0.8875 - val_acc: 0.9318 - val_mDice: 0.5057

Epoch 00161: val_mDice did not improve from 0.52690
Epoch 162/300
 - 10s - loss: 0.5617 - acc: 0.9238 - mDice: 0.5544 - val_loss: 0.8483 - val_acc: 0.9362 - val_mDice: 0.5251

Epoch 00162: val_mDice did not improve from 0.52690
Epoch 163/300
 - 10s - loss: 0.5623 - acc: 0.9239 - mDice: 0.5539 - val_loss: 0.8973 - val_acc: 0.9330 - val_mDice: 0.5052

Epoch 00163: val_mDice did not improve from 0.52690
Epoch 164/300
 - 10s - loss: 0.5568 - acc: 0.9241 - mDice: 0.5567 - val_loss: 0.8462 - val_acc: 0.9364 - val_mDice: 0.5241

Epoch 00164: val_mDice did not improve from 0.52690
Epoch 165/300
 - 10s - loss: 0.5617 - acc: 0.9239 - mDice: 0.5539 - val_loss: 0.8590 - val_acc: 0.9358 - val_mDice: 0.5184

Epoch 00165: val_mDice did not improve from 0.52690
Epoch 166/300
 - 9s - loss: 0.5608 - acc: 0.9242 - mDice: 0.5550 - val_loss: 0.8692 - val_acc: 0.9349 - val_mDice: 0.5184

Epoch 00166: val_mDice did not improve from 0.52690
Epoch 167/300
 - 11s - loss: 0.5666 - acc: 0.9242 - mDice: 0.5551 - val_loss: 0.8526 - val_acc: 0.9325 - val_mDice: 0.5218

Epoch 00167: val_mDice did not improve from 0.52690
Epoch 168/300
 - 9s - loss: 0.5616 - acc: 0.9240 - mDice: 0.5545 - val_loss: 0.8500 - val_acc: 0.9367 - val_mDice: 0.5223

Epoch 00168: val_mDice did not improve from 0.52690
Epoch 169/300
 - 9s - loss: 0.5589 - acc: 0.9243 - mDice: 0.5562 - val_loss: 0.8677 - val_acc: 0.9349 - val_mDice: 0.5187

Epoch 00169: val_mDice did not improve from 0.52690
Epoch 170/300
 - 9s - loss: 0.5599 - acc: 0.9242 - mDice: 0.5557 - val_loss: 0.8692 - val_acc: 0.9350 - val_mDice: 0.5166

Epoch 00170: val_mDice did not improve from 0.52690
Epoch 171/300
 - 9s - loss: 0.5604 - acc: 0.9241 - mDice: 0.5556 - val_loss: 0.8446 - val_acc: 0.9361 - val_mDice: 0.5267

Epoch 00171: val_mDice did not improve from 0.52690
Epoch 172/300
 - 10s - loss: 0.5629 - acc: 0.9242 - mDice: 0.5545 - val_loss: 0.8608 - val_acc: 0.9350 - val_mDice: 0.5184

Epoch 00172: val_mDice did not improve from 0.52690
Epoch 173/300
 - 9s - loss: 0.5632 - acc: 0.9243 - mDice: 0.5540 - val_loss: 0.8968 - val_acc: 0.9373 - val_mDice: 0.5033

Epoch 00173: val_mDice did not improve from 0.52690
Epoch 174/300
 - 9s - loss: 0.5555 - acc: 0.9242 - mDice: 0.5580 - val_loss: 0.8725 - val_acc: 0.9335 - val_mDice: 0.5112

Epoch 00174: val_mDice did not improve from 0.52690
Epoch 175/300
 - 9s - loss: 0.5600 - acc: 0.9243 - mDice: 0.5558 - val_loss: 0.8751 - val_acc: 0.9328 - val_mDice: 0.5132

Epoch 00175: val_mDice did not improve from 0.52690
Epoch 176/300
 - 9s - loss: 0.5576 - acc: 0.9246 - mDice: 0.5568 - val_loss: 0.8688 - val_acc: 0.9332 - val_mDice: 0.5172

Epoch 00176: val_mDice did not improve from 0.52690
Epoch 177/300
 - 9s - loss: 0.5573 - acc: 0.9242 - mDice: 0.5568 - val_loss: 0.8416 - val_acc: 0.9376 - val_mDice: 0.5246

Epoch 00177: val_mDice did not improve from 0.52690
Epoch 178/300
 - 9s - loss: 0.5616 - acc: 0.9245 - mDice: 0.5563 - val_loss: 0.8609 - val_acc: 0.9308 - val_mDice: 0.5126

Epoch 00178: val_mDice did not improve from 0.52690
Epoch 179/300
 - 9s - loss: 0.5573 - acc: 0.9245 - mDice: 0.5570 - val_loss: 0.8701 - val_acc: 0.9340 - val_mDice: 0.5139

Epoch 00179: val_mDice did not improve from 0.52690
Epoch 180/300
 - 9s - loss: 0.5567 - acc: 0.9245 - mDice: 0.5573 - val_loss: 0.8634 - val_acc: 0.9362 - val_mDice: 0.5204

Epoch 00180: val_mDice did not improve from 0.52690
Epoch 181/300
 - 9s - loss: 0.5571 - acc: 0.9244 - mDice: 0.5572 - val_loss: 0.8432 - val_acc: 0.9332 - val_mDice: 0.5263

Epoch 00181: val_mDice did not improve from 0.52690
Epoch 182/300
 - 9s - loss: 0.5537 - acc: 0.9247 - mDice: 0.5589 - val_loss: 0.8543 - val_acc: 0.9361 - val_mDice: 0.5207

Epoch 00182: val_mDice did not improve from 0.52690
Epoch 183/300
 - 9s - loss: 0.5557 - acc: 0.9247 - mDice: 0.5581 - val_loss: 0.8644 - val_acc: 0.9335 - val_mDice: 0.5184

Epoch 00183: val_mDice did not improve from 0.52690
Epoch 184/300
 - 9s - loss: 0.5529 - acc: 0.9245 - mDice: 0.5591 - val_loss: 0.8586 - val_acc: 0.9350 - val_mDice: 0.5200

Epoch 00184: val_mDice did not improve from 0.52690
Epoch 185/300
 - 9s - loss: 0.5539 - acc: 0.9249 - mDice: 0.5588 - val_loss: 0.8514 - val_acc: 0.9354 - val_mDice: 0.5211

Epoch 00185: val_mDice did not improve from 0.52690
Epoch 186/300
 - 9s - loss: 0.5540 - acc: 0.9247 - mDice: 0.5588 - val_loss: 0.8521 - val_acc: 0.9343 - val_mDice: 0.5210

Epoch 00186: val_mDice did not improve from 0.52690
Epoch 187/300
 - 9s - loss: 0.5546 - acc: 0.9248 - mDice: 0.5587 - val_loss: 0.8891 - val_acc: 0.9314 - val_mDice: 0.5172

Epoch 00187: val_mDice did not improve from 0.52690
Epoch 188/300
 - 9s - loss: 0.5547 - acc: 0.9248 - mDice: 0.5585 - val_loss: 0.8507 - val_acc: 0.9363 - val_mDice: 0.5256

Epoch 00188: val_mDice did not improve from 0.52690
Epoch 189/300
 - 9s - loss: 0.5526 - acc: 0.9248 - mDice: 0.5596 - val_loss: 0.8487 - val_acc: 0.9366 - val_mDice: 0.5224

Epoch 00189: val_mDice did not improve from 0.52690
Epoch 190/300
 - 9s - loss: 0.5556 - acc: 0.9245 - mDice: 0.5581 - val_loss: 0.8534 - val_acc: 0.9362 - val_mDice: 0.5218

Epoch 00190: val_mDice did not improve from 0.52690
Epoch 191/300
 - 9s - loss: 0.5532 - acc: 0.9249 - mDice: 0.5592 - val_loss: 0.8617 - val_acc: 0.9349 - val_mDice: 0.5184

Epoch 00191: val_mDice did not improve from 0.52690
Restoring model weights from the end of the best epoch
Epoch 00191: early stopping
{'val_loss': [1.9730769820132499, 1.5667872927283164, 1.2438800072265883, 1.1191289620210896, 1.1259439341766013, 1.1109838526127702, 1.0541746912703003, 1.0424639309866954, 1.0518542743672086, 1.0160337801033494, 1.1575564011342108, 0.9620346578501039, 1.080270507241373, 0.9507698844381645, 0.9708074452513356, 0.9570298861649077, 0.990260943854596, 1.1538929373531017, 0.9284265048086306, 0.9255743585737412, 0.9311908928014464, 1.050493965041166, 0.9191848618836053, 0.8952896568061268, 0.9477228936502489, 0.9359286504950227, 0.9823389322744251, 0.9093850649009316, 0.988445491440552, 1.0543124743100614, 0.9114663466221868, 0.8329376642313381, 0.9206452841139109, 0.8277387390028959, 0.8612878120551675, 0.8195603048734073, 0.9005384862759692, 0.8722747954945106, 0.8660714558962375, 0.8795540810978345, 1.0677446308782546, 0.954673281497201, 0.8570056635107698, 0.8370813003367623, 0.8884273349902051, 0.8566066053627575, 0.9220316989273675, 0.9090709430349748, 0.8633616058166418, 0.8527805252937274, 0.8481405097885993, 0.8484615518548394, 0.8745945038768531, 0.9104083177060057, 0.9077575267371485, 0.8885498875278538, 0.8747177090348497, 0.8877489263728514, 0.9980335518465204, 0.8800081964266502, 0.8632704538140593, 0.863309529541576, 0.8886423185046783, 0.8686213580902014, 0.8894583518895726, 0.8700409642720627, 0.893173173322516, 0.9683420577291715, 0.8769257405383438, 0.9187972161729457, 0.9013125977273715, 0.8538589383249229, 0.9145819433664871, 0.9568794063255612, 0.9011722271051784, 0.8963048895873592, 0.8951079616438871, 0.8664549420782401, 0.9011672970938818, 1.0077613120698659, 0.847812294960022, 1.0159364016042591, 0.8898852901943659, 0.9933618852647684, 0.870946233555422, 0.9178578415832951, 0.8978535448764003, 0.8924971293594878, 0.9142817481089447, 0.8796245358084555, 1.0927601426334705, 1.08198649465701, 0.8884531003607194, 1.07073043362569, 1.063332364384064, 0.8966297908017864, 0.8797171183225125, 1.0869497161800579, 0.8884370300055897, 0.8962316115697225, 0.9106488652148489, 0.8626680555990187, 0.9018547076963436, 0.8542051402862463, 0.8537632462668554, 0.8824342947221745, 0.875672227918765, 0.8943972567380485, 0.8771402236432005, 0.8796223691627805, 0.8837692468179821, 0.9370599574288406, 0.8829403448913057, 0.9142685825541869, 0.8860668179679052, 0.8684538609563968, 0.8770167450446867, 0.9053875329130787, 0.8613308715281514, 0.8579118622224883, 0.8708118025192433, 0.8640181243756396, 1.0895051208593078, 0.8809351954756484, 0.8669362782085009, 1.0867434504341944, 0.8684225600991545, 0.8755635305986567, 0.8610045195972852, 0.8690655851094736, 0.8616581656838541, 0.8739701192931267, 0.8683626948103393, 0.8969996002434337, 1.0775954440488653, 0.8612762474070834, 0.8776821847689353, 0.849564534122661, 0.884525793420393, 0.8784116634541312, 0.8879295270995232, 0.8842890599353165, 0.8680158503311503, 0.8649483205234937, 0.8845099076039373, 0.8542400409946334, 0.8828707030937497, 0.8753007145251258, 0.8740296195455863, 0.8650146958518163, 0.8392998525651835, 0.9282100416172696, 0.8714079688497856, 0.8522869186886286, 0.8608956855569182, 0.867712603450495, 0.8763889714149432, 1.0955902115773346, 0.8620862381606452, 0.8766765742652161, 0.8874676375739319, 0.8482797482592911, 0.8973187854734518, 0.8462134586215693, 0.8590038933996427, 0.8692271716177127, 0.8526224059573675, 0.8500289533097866, 0.8677145491885601, 0.869150058697846, 0.8446486669745149, 0.8607738415400187, 0.8968369051561518, 0.8725499217793092, 0.8751432404006269, 0.8688178298163549, 0.8416209416200886, 0.8609356031579486, 0.8700542739555661, 0.8633683172322936, 0.8432324208782218, 0.8543469111124674, 0.8643768217604039, 0.8585589504511343, 0.851417146833603, 0.8521359842375847, 0.8890859312930349, 0.8506966539695437, 0.8487499730061676, 0.8534488819413266, 0.8616761844710442], 'val_acc': [0.9173140306930757, 0.9167741843536075, 0.917002804535257, 0.9172506591694504, 0.9178380360037593, 0.9190535393811888, 0.9227407951812959, 0.9266114019404696, 0.9269237706890214, 0.9244429597073356, 0.925218212402473, 0.9250733360732343, 0.9215468111011268, 0.9266159093312625, 0.9265649820451682, 0.9221013565521455, 0.9265887481344621, 0.9260760724207776, 0.9245787498641149, 0.9288624565480119, 0.9244395454051131, 0.9238781986263512, 0.9251944382311934, 0.929525660256208, 0.9296173214238915, 0.9302262010547401, 0.9286372409028522, 0.9305974245071411, 0.9299229050086717, 0.9299047939521444, 0.9269961960571634, 0.9317551969134875, 0.929744082318861, 0.9321207767826015, 0.9276050601975393, 0.9291816155115763, 0.9298595301175522, 0.9314066511763017, 0.9297486196803508, 0.9320143686849519, 0.9285840378642756, 0.9308430235938164, 0.9318106689695584, 0.9316499590200219, 0.9326153471644989, 0.9322248788203223, 0.9302986469645959, 0.9273323258437679, 0.9292189586634017, 0.9294170195099998, 0.9320528574582547, 0.9298900850748612, 0.9286258974991276, 0.931302508391903, 0.9304865046409564, 0.9321988625715008, 0.9310388218211589, 0.933317042339993, 0.9302590391730184, 0.9285998802400578, 0.9320890836123019, 0.9304208799270587, 0.9335501715961823, 0.9322826175366418, 0.9317042703008921, 0.9314688797724449, 0.932456892425731, 0.9279932504319875, 0.9320947315733311, 0.9327794559257853, 0.932866592191707, 0.9314722667979656, 0.9329005466342646, 0.9292449786164666, 0.9299455325482255, 0.9306200604654301, 0.9321999947230021, 0.9327828412675588, 0.9330748336463325, 0.934008554550214, 0.9326493039643023, 0.9321943403637342, 0.9330906814101052, 0.9324331125297115, 0.9319193090422678, 0.9321083251365834, 0.9329039511707543, 0.9310263765733794, 0.9318627452446242, 0.9313602322912485, 0.9306506137389922, 0.9285229279496575, 0.9331857578902595, 0.9335682974696833, 0.9319419571235354, 0.9337414412175195, 0.9329967582966648, 0.9320053438008842, 0.9346966373044893, 0.9317054415153245, 0.9315503852515571, 0.935139164076013, 0.9322282958165401, 0.9327092763394286, 0.9360230713240845, 0.9323278753097448, 0.933627138366807, 0.9329763738449011, 0.9332434726973712, 0.9324965271572608, 0.9326572364332986, 0.9323075262166686, 0.9341013569616329, 0.9304672779336487, 0.932445595156675, 0.9350259677838471, 0.9322543255353378, 0.9332910031922119, 0.9337527607120363, 0.9357095694137831, 0.934750973168066, 0.933654300237106, 0.9326821343373444, 0.9350327657440961, 0.9324919988880049, 0.9320030663646547, 0.9345438500582162, 0.9342824261067277, 0.9348494484599701, 0.9370291781964275, 0.9336079015570172, 0.9329797753506461, 0.9354866049383993, 0.9322316666780892, 0.9294373851037968, 0.9362029935007041, 0.9317314517026567, 0.9348313212394714, 0.9337074948569476, 0.9335909283767312, 0.9341104000301684, 0.931577531968133, 0.9352048069743787, 0.9325757491386543, 0.9351799218668102, 0.9352930824635393, 0.9319804438763419, 0.9310365676206384, 0.9364123815870554, 0.9312979935926232, 0.9352070517459158, 0.9342394322998779, 0.9332196921278528, 0.9350067639754991, 0.9347973728584031, 0.9345913896452909, 0.9343786344016339, 0.9326594929910649, 0.9350067562302627, 0.9334041819734088, 0.9318265056206008, 0.9362211176904581, 0.9329922148736857, 0.9363546725046836, 0.9357548345953731, 0.934949050515385, 0.9325429079896312, 0.9367315402138705, 0.9348584864772645, 0.9350474853973604, 0.9361317103865456, 0.9349829756607444, 0.9373189295079075, 0.9334630242175301, 0.9327681320535262, 0.933219689770607, 0.9375599935903387, 0.9307966262607251, 0.9339994855519742, 0.936157761320556, 0.9331936863182628, 0.9361000141855014, 0.9335275225046664, 0.9350497459961196, 0.9353881481677125, 0.9342745020564667, 0.9314088969580872, 0.9363365520191731, 0.9366025217508865, 0.9362324368482255, 0.9349343032486694], 'val_mDice': [0.19345527500082543, 0.2951699321889608, 0.34453106162238256, 0.37514369164483025, 0.386334596717425, 0.3910566472401053, 0.407334057456356, 0.4210063242642893, 0.41433411264150155, 0.4336369559589752, 0.4405500296145509, 0.4462526681396247, 0.4335353179166546, 0.4544830813919757, 0.45318854191882463, 0.4575003968120295, 0.45403561669554415, 0.45449778033515154, 0.45818620953856215, 0.47295607449644705, 0.4642438797627465, 0.46252398386513444, 0.4648362579992262, 0.4795343515563146, 0.4724344277785996, 0.4783112123861151, 0.4752745495340918, 0.4851268233552491, 0.4883374615577655, 0.4869128240030364, 0.4851712453163276, 0.49697014032784154, 0.48076842935745323, 0.49510717358292833, 0.49250356019553493, 0.4968977482978907, 0.4876825930708546, 0.4971316729561757, 0.4881040268698655, 0.4959218926348929, 0.4873123259867652, 0.4955967183166978, 0.49721929313099317, 0.49595132756367916, 0.4951925338324854, 0.49699563495183396, 0.5011100678120629, 0.4911356265262022, 0.495406673116199, 0.495541062058702, 0.4954631241027918, 0.4976645186122528, 0.4988161426479534, 0.4985233299476279, 0.4970496020748117, 0.5001870353343123, 0.4955753849051093, 0.4984270933657716, 0.5010827540677819, 0.4964908267145103, 0.4982241072223685, 0.5012795770235654, 0.5034502091380836, 0.5103993159902971, 0.5030574350707275, 0.4992353687178617, 0.5020368833326351, 0.49541167081412624, 0.49819440087356137, 0.5012943108203047, 0.4969902095821618, 0.5033041195007367, 0.510376450705663, 0.49529313956950344, 0.4967348208535189, 0.5031368604487618, 0.4975765539427935, 0.5112994656724444, 0.5048891336230908, 0.5063622577042229, 0.5083289749204776, 0.49910136131243515, 0.5077250441588924, 0.5020154418244873, 0.5089861410485823, 0.5102641484831686, 0.5060736109307931, 0.5006305262193842, 0.4959233133806347, 0.5005238857646447, 0.49645306498317393, 0.5015264331957715, 0.508450641133691, 0.5099902166485113, 0.5046351871921517, 0.5116154373028857, 0.5064237683506335, 0.5041115058343962, 0.5086922921703361, 0.5047714716970584, 0.5017320399230483, 0.5167145304760691, 0.5041028806718729, 0.5168303476888582, 0.5182893286317082, 0.507106514973829, 0.5106002520033195, 0.5031677178070371, 0.5086797425302408, 0.5103680083980668, 0.5079636708491266, 0.5005856880360404, 0.49977612495422363, 0.49990205946615185, 0.5059790742599358, 0.5156216904268427, 0.5103004682535506, 0.5007497206919611, 0.5126856780995084, 0.5216785851171462, 0.5146032716594847, 0.5161021880510837, 0.5073902101840003, 0.5112195082303495, 0.5127083659172058, 0.506179780946613, 0.5127484060950198, 0.5102892605598364, 0.5183250136294607, 0.5146182603755239, 0.5165986812720864, 0.5111900096559255, 0.5144823762656605, 0.5059804737904651, 0.512663018568761, 0.5178090313733634, 0.5076306829344754, 0.5204041516040004, 0.5107746514896888, 0.5122965217310157, 0.5080941558557716, 0.5082842620079127, 0.5161327928473047, 0.5145958715912986, 0.5114783486403988, 0.5239606570389311, 0.5097643807109467, 0.5120812233558483, 0.5129041156526339, 0.5123050805539061, 0.5268964370091757, 0.5143838753808017, 0.5117024523390215, 0.5239315975857319, 0.5173863600202873, 0.5148103910650911, 0.5124122497052123, 0.507206462534134, 0.5139096157025482, 0.5134030822980202, 0.5057160073080978, 0.5251121622020916, 0.5051591497356609, 0.5241237336632896, 0.5184498176736346, 0.5183668267928948, 0.5218391213039894, 0.5223054919539198, 0.5186500296754352, 0.5166300427442216, 0.526741853878323, 0.5183609552302603, 0.5033197436629042, 0.5111720265641724, 0.5131964303005887, 0.5171502242654057, 0.5246179938989844, 0.5125502900215192, 0.5139136550116674, 0.5204297243538549, 0.5263230123762357, 0.5207258267591228, 0.5184469573241842, 0.5200031706168826, 0.5211365798772392, 0.5209512151567276, 0.5172140261547714, 0.525578017962181, 0.5223741794036607, 0.5218297239750792, 0.5184380031574918], 'loss': [3.039242991906898, 1.620112751110907, 1.306290879292917, 1.1550353471543544, 1.092265926549806, 1.0242592261360925, 0.9763089035038365, 0.944570625928138, 0.9200212383467057, 0.8904236027619243, 0.8682871365694153, 0.8557224091212088, 0.8373777895135175, 0.8233592872422337, 0.8079902249037216, 0.8001675495793984, 0.7854164264105216, 0.7807467885867342, 0.7660518657474878, 0.7589072540827448, 0.754111424090035, 0.748632192094801, 0.743640105822233, 0.7348180821654117, 0.7287123056056419, 0.7216020644227983, 0.7246043081318942, 0.7066748779764894, 0.7038213646022046, 0.705522359930825, 0.6961535081483767, 0.6928859196191496, 0.6875496379958713, 0.6869264687381809, 0.7166324674541357, 0.6814277033897518, 0.6827572585895958, 0.674933639567619, 0.6744470196001433, 0.6716958268497596, 0.6812920472288366, 0.6641422958660993, 0.6657697956722217, 0.663023590418802, 0.6723612279755458, 0.659795171532458, 0.6554782601524304, 0.6529448338342603, 0.6519597363987633, 0.6466152794918363, 0.6516519462211592, 0.6462035089148933, 0.6763108281702622, 0.6489129659213089, 0.6451088759844585, 0.6387636538720681, 0.6367830017083567, 0.6380110253997263, 0.6346595794563444, 0.6344003640376044, 0.6306366138467718, 0.6311987892000319, 0.6290512989088232, 0.626616548880329, 0.625502085065458, 0.6233974580741852, 0.6239724656703791, 0.6228839154465671, 0.624571448626183, 0.6195401779014281, 0.6207985980123851, 0.6165165352031686, 0.6143308004208486, 0.6137112837801307, 0.6160860747369825, 0.6124256418025577, 0.6136804383235246, 0.6095734501181445, 0.6112794777203872, 0.6066074736837483, 0.6103955664899753, 0.6061937928810076, 0.6055350733294081, 0.6078767826808507, 0.6074188682629689, 0.6045524954123254, 0.5990229597685541, 0.6005615197295692, 0.6016427020321146, 0.6009261925857452, 0.6010947082062872, 0.6045399365000634, 0.6139088877875807, 0.5992974856968223, 0.5973078733190027, 0.5915866194816608, 0.5968443350462614, 0.5968924484560824, 0.5930551688021695, 0.5955237788586591, 0.5915818754725665, 0.5939536647367671, 0.5921657509644843, 0.5908525120488355, 0.5933208223757307, 0.588631220066165, 0.5907130275957388, 0.5892140543419744, 0.5864286571943502, 0.5861322938019599, 0.5882162440409794, 0.5863009010496191, 0.586416159250584, 0.5864069504232668, 0.5836219650278419, 0.5823429126952985, 0.581186974120855, 0.5814589614741619, 0.5819946440792473, 0.5798417982942021, 0.58395751891322, 0.5818252858292557, 0.5794613796358584, 0.5795242598955814, 0.579477449249865, 0.578825980963571, 0.5809514576234659, 0.5849863075373029, 0.5779450348820785, 0.5759370919430272, 0.5751212758712043, 0.5752439566035217, 0.5726007544786351, 0.5781383204330609, 0.5732414898559998, 0.5748194165955192, 0.5733387563410703, 0.5728968842664043, 0.5736394588718867, 0.5723519396438156, 0.5713179534959539, 0.5749560601969596, 0.5754473993252865, 0.5701982006049183, 0.5720202797458858, 0.57160262822507, 0.5708481135344433, 0.5667162008973551, 0.5666677429812814, 0.568750670282286, 0.5652164996398952, 0.5644427601315362, 0.5641337777800775, 0.5658441689268273, 0.5655458727957574, 0.5668144031523064, 0.5650547913169303, 0.5694890183573443, 0.5719572805384336, 0.562301376236854, 0.5641795832925725, 0.5616511065264179, 0.5623061437427954, 0.5568172210449336, 0.5617305714438147, 0.5607622762095553, 0.5666216136382648, 0.56160340732871, 0.5588556207057115, 0.5598716065124115, 0.5603584587580233, 0.5628943262136589, 0.5632426054197947, 0.5554924586040699, 0.5599766928329324, 0.557572814024749, 0.5572719525274544, 0.561563989253219, 0.5573172931349298, 0.556725963026911, 0.5571013192325385, 0.5537287879794783, 0.5557097924900284, 0.5529066316760853, 0.553910526551992, 0.5539933810810997, 0.5546200026876438, 0.554692241138157, 0.5526024447739429, 0.5556158542682877, 0.5532035092900955], 'acc': [0.7218401379818677, 0.8893980027029799, 0.8905546959696263, 0.8909918398346949, 0.8911974551460733, 0.89220539894663, 0.8937416974186162, 0.8955855034085454, 0.8984858075063588, 0.9006758536767865, 0.9016383146930794, 0.9028321166993084, 0.9037128709137545, 0.9043280242030218, 0.9048918978433825, 0.9056316692494584, 0.9064985226468848, 0.9071056791986496, 0.9073994927191582, 0.9080293798892554, 0.9085290828302679, 0.9086930051752672, 0.9092925851116633, 0.9096096332276731, 0.9100323345964823, 0.9104909501396093, 0.9105686010294057, 0.9112110027834959, 0.9117516248269261, 0.9118773916985545, 0.9121975578393399, 0.912343693070994, 0.9130047829332253, 0.9131012316619997, 0.9112621992063578, 0.9130349460471175, 0.9132979505364268, 0.9136911831658383, 0.9139976382703944, 0.9140743670537297, 0.914022023644695, 0.9147388107935321, 0.9145658242392348, 0.9151617825535718, 0.9138586409798661, 0.9152195277775533, 0.9154847738897522, 0.9154121459383612, 0.915583187373451, 0.9158640643008611, 0.9160152432276106, 0.9159936029613859, 0.9137302377906268, 0.9160034384439513, 0.9160299350221184, 0.9164809753573211, 0.9166916554754598, 0.9168104316458136, 0.9167884589798485, 0.9170637914733619, 0.9171137919235149, 0.9173354196730309, 0.9175145612990092, 0.9176152114110386, 0.9177660748211521, 0.9180494467363426, 0.9177597771056106, 0.917915305502424, 0.9179368829630021, 0.9184034961902314, 0.918298302685974, 0.9185713134194576, 0.9187111470495293, 0.9188985306632991, 0.918769686813203, 0.9189003719180969, 0.9189975918267241, 0.919126228851007, 0.9191543580521462, 0.9192168118811466, 0.9190603838782988, 0.919301663355797, 0.9195161523857331, 0.9194140352558634, 0.9196564473461649, 0.9198564118245915, 0.9196987689793166, 0.9198216241350893, 0.9198944187565365, 0.9200196846028926, 0.9200494489242833, 0.9202540152225608, 0.9196140423063673, 0.920226034824408, 0.9204157004908401, 0.9206448002253166, 0.9205035024234313, 0.9206253130998174, 0.9206269909602922, 0.9207084490119222, 0.9206043212481794, 0.9207949314654502, 0.9208366643945297, 0.9209318966990591, 0.9209767060648024, 0.9210128115332248, 0.9211682581986246, 0.921083032082221, 0.9212403625620407, 0.9215187298978786, 0.9212085478620436, 0.9214102503349385, 0.9212329721528029, 0.9214608381813615, 0.9215559426949274, 0.9216152596917101, 0.9218169405609572, 0.9216607826317307, 0.9218323422429854, 0.9218252897262573, 0.9219529224580488, 0.9216336165833605, 0.9220601452230199, 0.9220402601893426, 0.9220661721612179, 0.9222301826334064, 0.9221763696282924, 0.9223238191078006, 0.9223339299325621, 0.922359382112302, 0.9225078163669097, 0.9222466125718455, 0.9224508655829784, 0.9224018695996904, 0.9225707334817459, 0.9224842511029493, 0.9227251531362608, 0.9225865132307236, 0.9227061712147736, 0.922856301097284, 0.9227921720919172, 0.9227361834082854, 0.922780907717061, 0.9229879708029037, 0.9230871794864235, 0.9230877437719592, 0.9229562841865688, 0.9232600584459608, 0.9231065401723549, 0.9230649509084672, 0.9233622617786823, 0.923535121943233, 0.9234969480111673, 0.9235163867915764, 0.9235712054530327, 0.9237021208494488, 0.9236844121295075, 0.9238488414222551, 0.9234583080593365, 0.9237187392671036, 0.9236466175099175, 0.9237521237470603, 0.9239385449678319, 0.9241285893737977, 0.9238701053154454, 0.9242100297514143, 0.9241820243368726, 0.9239663834445293, 0.9242708510321742, 0.9241758896108581, 0.9241152570210533, 0.9241573902684037, 0.9242870512644882, 0.9242227539947919, 0.9242531865159492, 0.9245628876230731, 0.9242488975037154, 0.9245426707003211, 0.9245369319706025, 0.9244624021944862, 0.9243796466846922, 0.9246796766003526, 0.9246714520063312, 0.9244546376417054, 0.9249011762101578, 0.9247254471882995, 0.9248156382638749, 0.9247982867919169, 0.9248472194775756, 0.924533377189694, 0.9249361932072168], 'mDice': [0.07737678154546344, 0.19836883274767295, 0.2630145014727356, 0.3032424757036218, 0.32610954666396763, 0.3472559072264882, 0.3643287420459656, 0.376576505678473, 0.38596292717393793, 0.3972711422303642, 0.40594864760044536, 0.4118907689369406, 0.4189552075498444, 0.4246096816472456, 0.43065171275202624, 0.4348748457268629, 0.44086791857222307, 0.4437841363317378, 0.449394736739862, 0.4529496076188981, 0.4552506697662397, 0.4578183021693379, 0.4604610322336582, 0.46443217813426857, 0.46732871590481695, 0.47046584475066594, 0.47113396646012184, 0.4774914541789798, 0.4786891301726239, 0.4784840686002902, 0.4827727436265805, 0.4842972524978688, 0.4872492841450651, 0.48776729848800615, 0.4745611538646388, 0.489748432835296, 0.4907646406912502, 0.4932727088450937, 0.4938098404390313, 0.4944998549077344, 0.49427631189426824, 0.49882590668276927, 0.4985321033444353, 0.5003207531696252, 0.4951505485391582, 0.50138142750114, 0.5034687675822929, 0.5045163045606074, 0.505593483288103, 0.5078687526604035, 0.5067045858518892, 0.5081429732292999, 0.4951946970475105, 0.5065693737521165, 0.5090441814959579, 0.5119638468215064, 0.512702064421493, 0.5124700179531985, 0.5138810482590366, 0.5141443039179969, 0.5161050206789614, 0.5161748056550743, 0.5170791697233103, 0.518136381635894, 0.51925363333643, 0.5201546278459228, 0.5196894998648064, 0.5201423032544596, 0.5195352612101238, 0.521738768302713, 0.5214450389735316, 0.5236351151735902, 0.5248145575857676, 0.5252811632464266, 0.5241315308913531, 0.5256523241905593, 0.5252325679550673, 0.5273288018326564, 0.5270641285814396, 0.5289551447755851, 0.5271856125937324, 0.5293184616805842, 0.5296915684756709, 0.5288375467295533, 0.5289249066561196, 0.5304135192606395, 0.5328558778678122, 0.5322001631289064, 0.5321085257921806, 0.5326256548246113, 0.5321691622742539, 0.5325895239100936, 0.5268003852523591, 0.5333398780956274, 0.534116311726457, 0.5369558140920404, 0.5347816750754409, 0.5350069883798634, 0.5368529518760866, 0.535808662463475, 0.5368533249297943, 0.5362756395337475, 0.5371345700191089, 0.5375338528593709, 0.5368824024058948, 0.5390390159941532, 0.537946788199256, 0.5387110784937876, 0.5403559327486752, 0.5404765492708204, 0.5394743639540241, 0.5405400716676793, 0.5402898483796499, 0.5406412855545788, 0.5421666451699182, 0.5426109667333013, 0.543162894229034, 0.5434394115789372, 0.5426637309806472, 0.5436769008773482, 0.5428626953889155, 0.5428823184947112, 0.5442754349393688, 0.544381932870508, 0.5444622926298023, 0.5452606610501526, 0.5436492986582424, 0.5458118106401922, 0.545698170178164, 0.5462749068259496, 0.5468928530701723, 0.546553722765986, 0.5478431096820295, 0.5471987140868702, 0.5478748198014394, 0.5470741422257819, 0.5480759214948878, 0.5484294245988146, 0.5476553081880728, 0.548515277277052, 0.54885034907538, 0.5482244343235007, 0.547426707152849, 0.5496577625550667, 0.5500982002073066, 0.5491417706940642, 0.5494205104172535, 0.5513278820113371, 0.551373939655652, 0.5504449330306878, 0.5522158026396325, 0.5526312550349042, 0.5524635873248418, 0.5523099707543271, 0.5520618412950771, 0.5517983458817111, 0.5526156419590964, 0.55314169371219, 0.5496826889827301, 0.5538366025187431, 0.5531673351994204, 0.5543905946730372, 0.553886946260761, 0.5567412704786727, 0.5539013050514483, 0.5550323088345215, 0.555108986713211, 0.5545251303214337, 0.5561578428342965, 0.5557194827193565, 0.5556294155362481, 0.5545403853863736, 0.5540071503892611, 0.5579538620847256, 0.5557913851643316, 0.5567942973451672, 0.5568000640120331, 0.5563363707398138, 0.5570259995563142, 0.5573003095415191, 0.5571854081214651, 0.5589323693053648, 0.5581020889966269, 0.5591232143976935, 0.558781493520951, 0.5588462464663193, 0.5586624131557921, 0.5585008685136212, 0.5595763249465034, 0.5580725986660368, 0.5591509150880816]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:02<00:11,  2.87s/it]predicting test subjects:  40%|████      | 2/5 [00:05<00:08,  2.69s/it]predicting test subjects:  60%|██████    | 3/5 [00:07<00:05,  2.51s/it]predicting test subjects:  80%|████████  | 4/5 [00:09<00:02,  2.38s/it]predicting test subjects: 100%|██████████| 5/5 [00:11<00:00,  2.38s/it]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:02<13:12,  2.99s/it]predicting train subjects:   1%|          | 2/266 [00:05<12:24,  2.82s/it]predicting train subjects:   1%|          | 3/266 [00:07<11:22,  2.59s/it]predicting train subjects:   2%|▏         | 4/266 [00:09<10:22,  2.38s/it]predicting train subjects:   2%|▏         | 5/266 [00:11<10:32,  2.42s/it]predicting train subjects:   2%|▏         | 6/266 [00:14<10:39,  2.46s/it]predicting train subjects:   3%|▎         | 7/266 [00:17<11:04,  2.57s/it]predicting train subjects:   3%|▎         | 8/266 [00:20<11:22,  2.64s/it]predicting train subjects:   3%|▎         | 9/266 [00:22<11:25,  2.67s/it]predicting train subjects:   4%|▍         | 10/266 [00:25<11:35,  2.72s/it]predicting train subjects:   4%|▍         | 11/266 [00:28<11:36,  2.73s/it]predicting train subjects:   5%|▍         | 12/266 [00:31<11:40,  2.76s/it]predicting train subjects:   5%|▍         | 13/266 [00:33<11:34,  2.75s/it]predicting train subjects:   5%|▌         | 14/266 [00:36<11:39,  2.78s/it]predicting train subjects:   6%|▌         | 15/266 [00:39<11:28,  2.74s/it]predicting train subjects:   6%|▌         | 16/266 [00:42<11:23,  2.74s/it]predicting train subjects:   6%|▋         | 17/266 [00:44<11:25,  2.75s/it]predicting train subjects:   7%|▋         | 18/266 [00:47<11:21,  2.75s/it]predicting train subjects:   7%|▋         | 19/266 [00:50<11:35,  2.82s/it]predicting train subjects:   8%|▊         | 20/266 [00:53<11:38,  2.84s/it]predicting train subjects:   8%|▊         | 21/266 [00:56<11:27,  2.81s/it]predicting train subjects:   8%|▊         | 22/266 [00:59<11:35,  2.85s/it]predicting train subjects:   9%|▊         | 23/266 [01:02<11:37,  2.87s/it]predicting train subjects:   9%|▉         | 24/266 [01:04<11:12,  2.78s/it]predicting train subjects:   9%|▉         | 25/266 [01:07<10:45,  2.68s/it]predicting train subjects:  10%|▉         | 26/266 [01:09<10:45,  2.69s/it]predicting train subjects:  10%|█         | 27/266 [01:12<10:29,  2.63s/it]predicting train subjects:  11%|█         | 28/266 [01:14<10:22,  2.62s/it]predicting train subjects:  11%|█         | 29/266 [01:17<10:21,  2.62s/it]predicting train subjects:  11%|█▏        | 30/266 [01:20<10:19,  2.62s/it]predicting train subjects:  12%|█▏        | 31/266 [01:22<10:13,  2.61s/it]predicting train subjects:  12%|█▏        | 32/266 [01:25<10:19,  2.65s/it]predicting train subjects:  12%|█▏        | 33/266 [01:28<10:13,  2.63s/it]predicting train subjects:  13%|█▎        | 34/266 [01:30<10:09,  2.63s/it]predicting train subjects:  13%|█▎        | 35/266 [01:33<10:01,  2.61s/it]predicting train subjects:  14%|█▎        | 36/266 [01:35<09:38,  2.52s/it]predicting train subjects:  14%|█▍        | 37/266 [01:38<09:30,  2.49s/it]predicting train subjects:  14%|█▍        | 38/266 [01:40<09:24,  2.48s/it]predicting train subjects:  15%|█▍        | 39/266 [01:43<09:31,  2.52s/it]predicting train subjects:  15%|█▌        | 40/266 [01:45<09:43,  2.58s/it]predicting train subjects:  15%|█▌        | 41/266 [01:48<09:34,  2.56s/it]predicting train subjects:  16%|█▌        | 42/266 [01:50<09:08,  2.45s/it]predicting train subjects:  16%|█▌        | 43/266 [01:52<08:34,  2.31s/it]predicting train subjects:  17%|█▋        | 44/266 [01:54<08:10,  2.21s/it]predicting train subjects:  17%|█▋        | 45/266 [01:56<07:50,  2.13s/it]predicting train subjects:  17%|█▋        | 46/266 [01:58<07:45,  2.12s/it]predicting train subjects:  18%|█▊        | 47/266 [02:00<07:37,  2.09s/it]predicting train subjects:  18%|█▊        | 48/266 [02:02<07:35,  2.09s/it]predicting train subjects:  18%|█▊        | 49/266 [02:04<07:28,  2.07s/it]predicting train subjects:  19%|█▉        | 50/266 [02:06<07:22,  2.05s/it]predicting train subjects:  19%|█▉        | 51/266 [02:08<07:24,  2.07s/it]predicting train subjects:  20%|█▉        | 52/266 [02:10<07:17,  2.04s/it]predicting train subjects:  20%|█▉        | 53/266 [02:12<07:15,  2.05s/it]predicting train subjects:  20%|██        | 54/266 [02:14<07:08,  2.02s/it]predicting train subjects:  21%|██        | 55/266 [02:16<07:03,  2.00s/it]predicting train subjects:  21%|██        | 56/266 [02:18<07:02,  2.01s/it]predicting train subjects:  21%|██▏       | 57/266 [02:20<06:56,  1.99s/it]predicting train subjects:  22%|██▏       | 58/266 [02:22<06:51,  1.98s/it]predicting train subjects:  22%|██▏       | 59/266 [02:24<06:49,  1.98s/it]predicting train subjects:  23%|██▎       | 60/266 [02:26<06:38,  1.93s/it]predicting train subjects:  23%|██▎       | 61/266 [02:28<06:30,  1.90s/it]predicting train subjects:  23%|██▎       | 62/266 [02:30<06:26,  1.89s/it]predicting train subjects:  24%|██▎       | 63/266 [02:32<06:30,  1.92s/it]predicting train subjects:  24%|██▍       | 64/266 [02:34<06:25,  1.91s/it]predicting train subjects:  24%|██▍       | 65/266 [02:35<06:22,  1.90s/it]predicting train subjects:  25%|██▍       | 66/266 [02:37<06:16,  1.88s/it]predicting train subjects:  25%|██▌       | 67/266 [02:39<06:22,  1.92s/it]predicting train subjects:  26%|██▌       | 68/266 [02:41<06:13,  1.89s/it]predicting train subjects:  26%|██▌       | 69/266 [02:43<06:04,  1.85s/it]predicting train subjects:  26%|██▋       | 70/266 [02:45<05:58,  1.83s/it]predicting train subjects:  27%|██▋       | 71/266 [02:46<05:54,  1.82s/it]predicting train subjects:  27%|██▋       | 72/266 [02:48<05:50,  1.81s/it]predicting train subjects:  27%|██▋       | 73/266 [02:50<05:47,  1.80s/it]predicting train subjects:  28%|██▊       | 74/266 [02:52<05:46,  1.80s/it]predicting train subjects:  28%|██▊       | 75/266 [02:54<05:45,  1.81s/it]predicting train subjects:  29%|██▊       | 76/266 [02:56<05:53,  1.86s/it]predicting train subjects:  29%|██▉       | 77/266 [02:57<05:51,  1.86s/it]predicting train subjects:  29%|██▉       | 78/266 [03:00<06:30,  2.08s/it]predicting train subjects:  30%|██▉       | 79/266 [03:03<07:02,  2.26s/it]predicting train subjects:  30%|███       | 80/266 [03:05<07:09,  2.31s/it]predicting train subjects:  30%|███       | 81/266 [03:08<07:21,  2.39s/it]predicting train subjects:  31%|███       | 82/266 [03:10<07:32,  2.46s/it]predicting train subjects:  31%|███       | 83/266 [03:13<07:45,  2.54s/it]predicting train subjects:  32%|███▏      | 84/266 [03:15<07:35,  2.50s/it]predicting train subjects:  32%|███▏      | 85/266 [03:18<07:37,  2.53s/it]predicting train subjects:  32%|███▏      | 86/266 [03:21<07:40,  2.56s/it]predicting train subjects:  33%|███▎      | 87/266 [03:23<07:41,  2.58s/it]predicting train subjects:  33%|███▎      | 88/266 [03:26<07:50,  2.64s/it]predicting train subjects:  33%|███▎      | 89/266 [03:29<07:52,  2.67s/it]predicting train subjects:  34%|███▍      | 90/266 [03:31<07:47,  2.65s/it]predicting train subjects:  34%|███▍      | 91/266 [03:34<07:45,  2.66s/it]predicting train subjects:  35%|███▍      | 92/266 [03:37<07:37,  2.63s/it]predicting train subjects:  35%|███▍      | 93/266 [03:39<07:36,  2.64s/it]predicting train subjects:  35%|███▌      | 94/266 [03:42<07:41,  2.68s/it]predicting train subjects:  36%|███▌      | 95/266 [03:45<07:31,  2.64s/it]predicting train subjects:  36%|███▌      | 96/266 [03:47<07:09,  2.53s/it]predicting train subjects:  36%|███▋      | 97/266 [03:50<07:15,  2.58s/it]predicting train subjects:  37%|███▋      | 98/266 [03:52<07:18,  2.61s/it]predicting train subjects:  37%|███▋      | 99/266 [03:54<06:39,  2.39s/it]predicting train subjects:  38%|███▊      | 100/266 [03:56<06:15,  2.26s/it]predicting train subjects:  38%|███▊      | 101/266 [03:58<06:13,  2.26s/it]predicting train subjects:  38%|███▊      | 102/266 [04:01<06:04,  2.22s/it]predicting train subjects:  39%|███▊      | 103/266 [04:03<06:00,  2.21s/it]predicting train subjects:  39%|███▉      | 104/266 [04:05<05:54,  2.19s/it]predicting train subjects:  39%|███▉      | 105/266 [04:07<05:57,  2.22s/it]predicting train subjects:  40%|███▉      | 106/266 [04:09<05:54,  2.22s/it]predicting train subjects:  40%|████      | 107/266 [04:12<05:57,  2.25s/it]predicting train subjects:  41%|████      | 108/266 [04:14<06:01,  2.28s/it]predicting train subjects:  41%|████      | 109/266 [04:16<05:54,  2.26s/it]predicting train subjects:  41%|████▏     | 110/266 [04:19<05:53,  2.27s/it]predicting train subjects:  42%|████▏     | 111/266 [04:21<05:49,  2.25s/it]predicting train subjects:  42%|████▏     | 112/266 [04:23<05:47,  2.26s/it]predicting train subjects:  42%|████▏     | 113/266 [04:25<05:41,  2.23s/it]predicting train subjects:  43%|████▎     | 114/266 [04:27<05:34,  2.20s/it]predicting train subjects:  43%|████▎     | 115/266 [04:30<05:33,  2.21s/it]predicting train subjects:  44%|████▎     | 116/266 [04:32<05:32,  2.21s/it]predicting train subjects:  44%|████▍     | 117/266 [04:34<05:39,  2.28s/it]predicting train subjects:  44%|████▍     | 118/266 [04:36<05:33,  2.25s/it]predicting train subjects:  45%|████▍     | 119/266 [04:39<05:46,  2.36s/it]predicting train subjects:  45%|████▌     | 120/266 [04:41<05:50,  2.40s/it]predicting train subjects:  45%|████▌     | 121/266 [04:44<05:55,  2.45s/it]predicting train subjects:  46%|████▌     | 122/266 [04:47<05:57,  2.48s/it]predicting train subjects:  46%|████▌     | 123/266 [04:49<05:59,  2.51s/it]predicting train subjects:  47%|████▋     | 124/266 [04:52<05:55,  2.50s/it]predicting train subjects:  47%|████▋     | 125/266 [04:54<05:56,  2.53s/it]predicting train subjects:  47%|████▋     | 126/266 [04:57<05:51,  2.51s/it]predicting train subjects:  48%|████▊     | 127/266 [04:59<05:48,  2.50s/it]predicting train subjects:  48%|████▊     | 128/266 [05:02<05:44,  2.49s/it]predicting train subjects:  48%|████▊     | 129/266 [05:04<05:49,  2.55s/it]predicting train subjects:  49%|████▉     | 130/266 [05:07<05:53,  2.60s/it]predicting train subjects:  49%|████▉     | 131/266 [05:10<05:54,  2.62s/it]predicting train subjects:  50%|████▉     | 132/266 [05:12<05:54,  2.64s/it]predicting train subjects:  50%|█████     | 133/266 [05:15<05:51,  2.64s/it]predicting train subjects:  50%|█████     | 134/266 [05:18<05:48,  2.64s/it]predicting train subjects:  51%|█████     | 135/266 [05:20<05:46,  2.65s/it]predicting train subjects:  51%|█████     | 136/266 [05:23<05:35,  2.58s/it]predicting train subjects:  52%|█████▏    | 137/266 [05:26<05:37,  2.61s/it]predicting train subjects:  52%|█████▏    | 138/266 [05:28<05:27,  2.56s/it]predicting train subjects:  52%|█████▏    | 139/266 [05:30<05:19,  2.51s/it]predicting train subjects:  53%|█████▎    | 140/266 [05:33<05:20,  2.54s/it]predicting train subjects:  53%|█████▎    | 141/266 [05:36<05:19,  2.56s/it]predicting train subjects:  53%|█████▎    | 142/266 [05:38<05:14,  2.53s/it]predicting train subjects:  54%|█████▍    | 143/266 [05:40<05:06,  2.49s/it]predicting train subjects:  54%|█████▍    | 144/266 [05:43<05:03,  2.48s/it]predicting train subjects:  55%|█████▍    | 145/266 [05:45<04:58,  2.47s/it]predicting train subjects:  55%|█████▍    | 146/266 [05:48<04:54,  2.46s/it]predicting train subjects:  55%|█████▌    | 147/266 [05:50<04:55,  2.48s/it]predicting train subjects:  56%|█████▌    | 148/266 [05:53<04:51,  2.47s/it]predicting train subjects:  56%|█████▌    | 149/266 [05:55<04:50,  2.49s/it]predicting train subjects:  56%|█████▋    | 150/266 [05:58<04:47,  2.48s/it]predicting train subjects:  57%|█████▋    | 151/266 [06:00<04:43,  2.46s/it]predicting train subjects:  57%|█████▋    | 152/266 [06:03<04:43,  2.49s/it]predicting train subjects:  58%|█████▊    | 153/266 [06:05<04:39,  2.48s/it]predicting train subjects:  58%|█████▊    | 154/266 [06:08<04:36,  2.47s/it]predicting train subjects:  58%|█████▊    | 155/266 [06:10<04:15,  2.30s/it]predicting train subjects:  59%|█████▊    | 156/266 [06:11<03:56,  2.15s/it]predicting train subjects:  59%|█████▉    | 157/266 [06:13<03:43,  2.05s/it]predicting train subjects:  59%|█████▉    | 158/266 [06:15<03:31,  1.96s/it]predicting train subjects:  60%|█████▉    | 159/266 [06:17<03:26,  1.93s/it]predicting train subjects:  60%|██████    | 160/266 [06:18<03:17,  1.87s/it]predicting train subjects:  61%|██████    | 161/266 [06:20<03:11,  1.82s/it]predicting train subjects:  61%|██████    | 162/266 [06:22<03:15,  1.88s/it]predicting train subjects:  61%|██████▏   | 163/266 [06:24<03:09,  1.84s/it]predicting train subjects:  62%|██████▏   | 164/266 [06:26<03:04,  1.81s/it]predicting train subjects:  62%|██████▏   | 165/266 [06:28<03:03,  1.82s/it]predicting train subjects:  62%|██████▏   | 166/266 [06:29<03:01,  1.82s/it]predicting train subjects:  63%|██████▎   | 167/266 [06:31<02:56,  1.78s/it]predicting train subjects:  63%|██████▎   | 168/266 [06:33<02:51,  1.75s/it]predicting train subjects:  64%|██████▎   | 169/266 [06:34<02:47,  1.73s/it]predicting train subjects:  64%|██████▍   | 170/266 [06:36<02:43,  1.71s/it]predicting train subjects:  64%|██████▍   | 171/266 [06:38<02:46,  1.75s/it]predicting train subjects:  65%|██████▍   | 172/266 [06:40<02:43,  1.73s/it]predicting train subjects:  65%|██████▌   | 173/266 [06:42<02:51,  1.84s/it]predicting train subjects:  65%|██████▌   | 174/266 [06:44<02:56,  1.92s/it]predicting train subjects:  66%|██████▌   | 175/266 [06:46<03:01,  2.00s/it]predicting train subjects:  66%|██████▌   | 176/266 [06:48<02:58,  1.99s/it]predicting train subjects:  67%|██████▋   | 177/266 [06:50<02:54,  1.96s/it]predicting train subjects:  67%|██████▋   | 178/266 [06:52<02:52,  1.96s/it]predicting train subjects:  67%|██████▋   | 179/266 [06:54<02:52,  1.98s/it]predicting train subjects:  68%|██████▊   | 180/266 [06:56<02:50,  1.98s/it]predicting train subjects:  68%|██████▊   | 181/266 [06:58<02:50,  2.00s/it]predicting train subjects:  68%|██████▊   | 182/266 [07:00<02:50,  2.03s/it]predicting train subjects:  69%|██████▉   | 183/266 [07:02<02:49,  2.04s/it]predicting train subjects:  69%|██████▉   | 184/266 [07:04<02:46,  2.03s/it]predicting train subjects:  70%|██████▉   | 185/266 [07:06<02:41,  2.00s/it]predicting train subjects:  70%|██████▉   | 186/266 [07:08<02:38,  1.99s/it]predicting train subjects:  70%|███████   | 187/266 [07:10<02:36,  1.98s/it]predicting train subjects:  71%|███████   | 188/266 [07:12<02:34,  1.98s/it]predicting train subjects:  71%|███████   | 189/266 [07:14<02:30,  1.95s/it]predicting train subjects:  71%|███████▏  | 190/266 [07:16<02:33,  2.02s/it]predicting train subjects:  72%|███████▏  | 191/266 [07:18<02:33,  2.05s/it]predicting train subjects:  72%|███████▏  | 192/266 [07:20<02:28,  2.00s/it]predicting train subjects:  73%|███████▎  | 193/266 [07:22<02:24,  1.98s/it]predicting train subjects:  73%|███████▎  | 194/266 [07:24<02:36,  2.17s/it]predicting train subjects:  73%|███████▎  | 195/266 [07:26<02:31,  2.13s/it]predicting train subjects:  74%|███████▎  | 196/266 [07:29<02:33,  2.19s/it]predicting train subjects:  74%|███████▍  | 197/266 [07:31<02:36,  2.27s/it]predicting train subjects:  74%|███████▍  | 198/266 [07:33<02:32,  2.24s/it]predicting train subjects:  75%|███████▍  | 199/266 [07:36<02:31,  2.26s/it]predicting train subjects:  75%|███████▌  | 200/266 [07:38<02:27,  2.23s/it]predicting train subjects:  76%|███████▌  | 201/266 [07:40<02:24,  2.22s/it]predicting train subjects:  76%|███████▌  | 202/266 [07:42<02:19,  2.19s/it]predicting train subjects:  76%|███████▋  | 203/266 [07:44<02:16,  2.17s/it]predicting train subjects:  77%|███████▋  | 204/266 [07:47<02:16,  2.21s/it]predicting train subjects:  77%|███████▋  | 205/266 [07:49<02:11,  2.16s/it]predicting train subjects:  77%|███████▋  | 206/266 [07:51<02:10,  2.17s/it]predicting train subjects:  78%|███████▊  | 207/266 [07:53<02:07,  2.17s/it]predicting train subjects:  78%|███████▊  | 208/266 [07:55<02:07,  2.20s/it]predicting train subjects:  79%|███████▊  | 209/266 [07:58<02:07,  2.23s/it]predicting train subjects:  79%|███████▉  | 210/266 [08:00<02:03,  2.20s/it]predicting train subjects:  79%|███████▉  | 211/266 [08:02<02:01,  2.20s/it]predicting train subjects:  80%|███████▉  | 212/266 [08:04<01:59,  2.20s/it]predicting train subjects:  80%|████████  | 213/266 [08:06<01:50,  2.09s/it]predicting train subjects:  80%|████████  | 214/266 [08:08<01:50,  2.13s/it]predicting train subjects:  81%|████████  | 215/266 [08:10<01:42,  2.01s/it]predicting train subjects:  81%|████████  | 216/266 [08:12<01:40,  2.01s/it]predicting train subjects:  82%|████████▏ | 217/266 [08:14<01:37,  1.99s/it]predicting train subjects:  82%|████████▏ | 218/266 [08:16<01:32,  1.93s/it]predicting train subjects:  82%|████████▏ | 219/266 [08:18<01:29,  1.91s/it]predicting train subjects:  83%|████████▎ | 220/266 [08:19<01:27,  1.89s/it]predicting train subjects:  83%|████████▎ | 221/266 [08:22<01:31,  2.03s/it]predicting train subjects:  83%|████████▎ | 222/266 [08:23<01:25,  1.94s/it]predicting train subjects:  84%|████████▍ | 223/266 [08:25<01:22,  1.93s/it]predicting train subjects:  84%|████████▍ | 224/266 [08:27<01:18,  1.86s/it]predicting train subjects:  85%|████████▍ | 225/266 [08:29<01:16,  1.87s/it]predicting train subjects:  85%|████████▍ | 226/266 [08:31<01:17,  1.94s/it]predicting train subjects:  85%|████████▌ | 227/266 [08:33<01:20,  2.05s/it]predicting train subjects:  86%|████████▌ | 228/266 [08:35<01:15,  2.00s/it]predicting train subjects:  86%|████████▌ | 229/266 [08:37<01:13,  2.00s/it]predicting train subjects:  86%|████████▋ | 230/266 [08:39<01:11,  1.98s/it]predicting train subjects:  87%|████████▋ | 231/266 [08:42<01:14,  2.12s/it]predicting train subjects:  87%|████████▋ | 232/266 [08:44<01:12,  2.14s/it]predicting train subjects:  88%|████████▊ | 233/266 [08:46<01:14,  2.24s/it]predicting train subjects:  88%|████████▊ | 234/266 [08:48<01:09,  2.16s/it]predicting train subjects:  88%|████████▊ | 235/266 [08:50<01:03,  2.05s/it]predicting train subjects:  89%|████████▊ | 236/266 [08:52<00:58,  1.96s/it]predicting train subjects:  89%|████████▉ | 237/266 [08:54<01:00,  2.07s/it]predicting train subjects:  89%|████████▉ | 238/266 [08:56<00:57,  2.06s/it]predicting train subjects:  90%|████████▉ | 239/266 [08:58<00:55,  2.06s/it]predicting train subjects:  90%|█████████ | 240/266 [09:00<00:53,  2.04s/it]predicting train subjects:  91%|█████████ | 241/266 [09:02<00:49,  1.98s/it]predicting train subjects:  91%|█████████ | 242/266 [09:04<00:47,  1.98s/it]predicting train subjects:  91%|█████████▏| 243/266 [09:06<00:45,  1.99s/it]predicting train subjects:  92%|█████████▏| 244/266 [09:08<00:43,  1.98s/it]predicting train subjects:  92%|█████████▏| 245/266 [09:10<00:42,  2.04s/it]predicting train subjects:  92%|█████████▏| 246/266 [09:12<00:39,  1.98s/it]predicting train subjects:  93%|█████████▎| 247/266 [09:14<00:37,  1.98s/it]predicting train subjects:  93%|█████████▎| 248/266 [09:16<00:35,  1.97s/it]predicting train subjects:  94%|█████████▎| 249/266 [09:19<00:37,  2.18s/it]predicting train subjects:  94%|█████████▍| 250/266 [09:21<00:36,  2.30s/it]predicting train subjects:  94%|█████████▍| 251/266 [09:24<00:36,  2.42s/it]predicting train subjects:  95%|█████████▍| 252/266 [09:27<00:35,  2.56s/it]predicting train subjects:  95%|█████████▌| 253/266 [09:29<00:32,  2.52s/it]predicting train subjects:  95%|█████████▌| 254/266 [09:32<00:30,  2.55s/it]predicting train subjects:  96%|█████████▌| 255/266 [09:35<00:28,  2.58s/it]predicting train subjects:  96%|█████████▌| 256/266 [09:37<00:26,  2.66s/it]predicting train subjects:  97%|█████████▋| 257/266 [09:40<00:23,  2.63s/it]predicting train subjects:  97%|█████████▋| 258/266 [09:43<00:21,  2.63s/it]predicting train subjects:  97%|█████████▋| 259/266 [09:45<00:18,  2.60s/it]predicting train subjects:  98%|█████████▊| 260/266 [09:48<00:15,  2.62s/it]predicting train subjects:  98%|█████████▊| 261/266 [09:50<00:13,  2.62s/it]predicting train subjects:  98%|█████████▊| 262/266 [09:53<00:10,  2.62s/it]predicting train subjects:  99%|█████████▉| 263/266 [09:55<00:07,  2.57s/it]predicting train subjects:  99%|█████████▉| 264/266 [09:58<00:05,  2.57s/it]predicting train subjects: 100%|█████████▉| 265/266 [10:01<00:02,  2.58s/it]predicting train subjects: 100%|██████████| 266/266 [10:03<00:00,  2.60s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<03:04,  1.44it/s]Loading train:   1%|          | 2/266 [00:01<02:46,  1.59it/s]Loading train:   1%|          | 3/266 [00:01<02:28,  1.77it/s]Loading train:   2%|▏         | 4/266 [00:02<02:17,  1.90it/s]Loading train:   2%|▏         | 5/266 [00:02<02:09,  2.01it/s]Loading train:   2%|▏         | 6/266 [00:02<02:03,  2.10it/s]Loading train:   3%|▎         | 7/266 [00:03<01:59,  2.16it/s]Loading train:   3%|▎         | 8/266 [00:03<02:09,  1.99it/s]Loading train:   3%|▎         | 9/266 [00:04<02:02,  2.09it/s]Loading train:   4%|▍         | 10/266 [00:04<02:03,  2.07it/s]Loading train:   4%|▍         | 11/266 [00:05<02:04,  2.05it/s]Loading train:   5%|▍         | 12/266 [00:05<02:00,  2.10it/s]Loading train:   5%|▍         | 13/266 [00:06<01:56,  2.18it/s]Loading train:   5%|▌         | 14/266 [00:06<02:04,  2.02it/s]Loading train:   6%|▌         | 15/266 [00:07<02:01,  2.06it/s]Loading train:   6%|▌         | 16/266 [00:07<01:56,  2.15it/s]Loading train:   6%|▋         | 17/266 [00:08<01:54,  2.17it/s]Loading train:   7%|▋         | 18/266 [00:08<01:53,  2.19it/s]Loading train:   7%|▋         | 19/266 [00:09<01:54,  2.15it/s]Loading train:   8%|▊         | 20/266 [00:09<02:00,  2.03it/s]Loading train:   8%|▊         | 21/266 [00:10<01:58,  2.07it/s]Loading train:   8%|▊         | 22/266 [00:10<01:56,  2.09it/s]Loading train:   9%|▊         | 23/266 [00:11<02:03,  1.97it/s]Loading train:   9%|▉         | 24/266 [00:11<02:02,  1.97it/s]Loading train:   9%|▉         | 25/266 [00:11<01:54,  2.10it/s]Loading train:  10%|▉         | 26/266 [00:12<01:47,  2.22it/s]Loading train:  10%|█         | 27/266 [00:12<01:53,  2.10it/s]Loading train:  11%|█         | 28/266 [00:13<01:49,  2.16it/s]Loading train:  11%|█         | 29/266 [00:13<01:46,  2.22it/s]Loading train:  11%|█▏        | 30/266 [00:14<01:43,  2.27it/s]Loading train:  12%|█▏        | 31/266 [00:14<01:42,  2.29it/s]Loading train:  12%|█▏        | 32/266 [00:15<01:38,  2.37it/s]Loading train:  12%|█▏        | 33/266 [00:15<01:40,  2.32it/s]Loading train:  13%|█▎        | 34/266 [00:15<01:37,  2.38it/s]Loading train:  13%|█▎        | 35/266 [00:16<01:40,  2.29it/s]Loading train:  14%|█▎        | 36/266 [00:16<01:38,  2.34it/s]Loading train:  14%|█▍        | 37/266 [00:17<01:39,  2.30it/s]Loading train:  14%|█▍        | 38/266 [00:17<01:37,  2.33it/s]Loading train:  15%|█▍        | 39/266 [00:17<01:34,  2.41it/s]Loading train:  15%|█▌        | 40/266 [00:18<01:34,  2.40it/s]Loading train:  15%|█▌        | 41/266 [00:18<01:36,  2.33it/s]Loading train:  16%|█▌        | 42/266 [00:19<01:36,  2.33it/s]Loading train:  16%|█▌        | 43/266 [00:19<01:31,  2.44it/s]Loading train:  17%|█▋        | 44/266 [00:20<01:27,  2.54it/s]Loading train:  17%|█▋        | 45/266 [00:20<01:24,  2.61it/s]Loading train:  17%|█▋        | 46/266 [00:20<01:22,  2.66it/s]Loading train:  18%|█▊        | 47/266 [00:21<01:20,  2.71it/s]Loading train:  18%|█▊        | 48/266 [00:21<01:29,  2.44it/s]Loading train:  18%|█▊        | 49/266 [00:22<01:32,  2.35it/s]Loading train:  19%|█▉        | 50/266 [00:22<01:30,  2.39it/s]Loading train:  19%|█▉        | 51/266 [00:22<01:27,  2.47it/s]Loading train:  20%|█▉        | 52/266 [00:23<01:24,  2.54it/s]Loading train:  20%|█▉        | 53/266 [00:23<01:20,  2.63it/s]Loading train:  20%|██        | 54/266 [00:23<01:19,  2.68it/s]Loading train:  21%|██        | 55/266 [00:24<01:28,  2.38it/s]Loading train:  21%|██        | 56/266 [00:24<01:32,  2.27it/s]Loading train:  21%|██▏       | 57/266 [00:25<01:32,  2.26it/s]Loading train:  22%|██▏       | 58/266 [00:25<01:29,  2.33it/s]Loading train:  22%|██▏       | 59/266 [00:26<01:28,  2.34it/s]Loading train:  23%|██▎       | 60/266 [00:26<01:24,  2.44it/s]Loading train:  23%|██▎       | 61/266 [00:26<01:24,  2.41it/s]Loading train:  23%|██▎       | 62/266 [00:27<01:27,  2.32it/s]Loading train:  24%|██▎       | 63/266 [00:27<01:25,  2.39it/s]Loading train:  24%|██▍       | 64/266 [00:28<01:22,  2.46it/s]Loading train:  24%|██▍       | 65/266 [00:28<01:17,  2.60it/s]Loading train:  25%|██▍       | 66/266 [00:28<01:13,  2.71it/s]Loading train:  25%|██▌       | 67/266 [00:29<01:10,  2.82it/s]Loading train:  26%|██▌       | 68/266 [00:29<01:10,  2.79it/s]Loading train:  26%|██▌       | 69/266 [00:30<01:19,  2.49it/s]Loading train:  26%|██▋       | 70/266 [00:30<01:17,  2.52it/s]Loading train:  27%|██▋       | 71/266 [00:30<01:13,  2.64it/s]Loading train:  27%|██▋       | 72/266 [00:31<01:13,  2.65it/s]Loading train:  27%|██▋       | 73/266 [00:31<01:13,  2.63it/s]Loading train:  28%|██▊       | 74/266 [00:31<01:09,  2.75it/s]Loading train:  28%|██▊       | 75/266 [00:32<01:06,  2.85it/s]Loading train:  29%|██▊       | 76/266 [00:32<01:16,  2.48it/s]Loading train:  29%|██▉       | 77/266 [00:33<01:16,  2.49it/s]Loading train:  29%|██▉       | 78/266 [00:33<01:16,  2.47it/s]Loading train:  30%|██▉       | 79/266 [00:33<01:17,  2.42it/s]Loading train:  30%|███       | 80/266 [00:34<01:15,  2.45it/s]Loading train:  30%|███       | 81/266 [00:34<01:14,  2.47it/s]Loading train:  31%|███       | 82/266 [00:35<01:26,  2.14it/s]Loading train:  31%|███       | 83/266 [00:35<01:21,  2.24it/s]Loading train:  32%|███▏      | 84/266 [00:36<01:18,  2.32it/s]Loading train:  32%|███▏      | 85/266 [00:36<01:17,  2.33it/s]Loading train:  32%|███▏      | 86/266 [00:37<01:19,  2.27it/s]Loading train:  33%|███▎      | 87/266 [00:37<01:17,  2.31it/s]Loading train:  33%|███▎      | 88/266 [00:37<01:21,  2.19it/s]Loading train:  33%|███▎      | 89/266 [00:38<01:23,  2.11it/s]Loading train:  34%|███▍      | 90/266 [00:38<01:22,  2.12it/s]Loading train:  34%|███▍      | 91/266 [00:39<01:25,  2.05it/s]Loading train:  35%|███▍      | 92/266 [00:39<01:20,  2.15it/s]Loading train:  35%|███▍      | 93/266 [00:40<01:20,  2.16it/s]Loading train:  35%|███▌      | 94/266 [00:40<01:19,  2.17it/s]Loading train:  36%|███▌      | 95/266 [00:41<01:15,  2.26it/s]Loading train:  36%|███▌      | 96/266 [00:41<01:13,  2.31it/s]Loading train:  36%|███▋      | 97/266 [00:42<01:21,  2.09it/s]Loading train:  37%|███▋      | 98/266 [00:42<01:20,  2.09it/s]Loading train:  37%|███▋      | 99/266 [00:43<01:16,  2.19it/s]Loading train:  38%|███▊      | 100/266 [00:43<01:13,  2.26it/s]Loading train:  38%|███▊      | 101/266 [00:43<01:09,  2.39it/s]Loading train:  38%|███▊      | 102/266 [00:44<01:06,  2.48it/s]Loading train:  39%|███▊      | 103/266 [00:44<01:03,  2.56it/s]Loading train:  39%|███▉      | 104/266 [00:44<01:03,  2.56it/s]Loading train:  39%|███▉      | 105/266 [00:45<01:01,  2.62it/s]Loading train:  40%|███▉      | 106/266 [00:45<01:02,  2.55it/s]Loading train:  40%|████      | 107/266 [00:46<01:07,  2.36it/s]Loading train:  41%|████      | 108/266 [00:46<01:10,  2.26it/s]Loading train:  41%|████      | 109/266 [00:47<01:07,  2.33it/s]Loading train:  41%|████▏     | 110/266 [00:47<01:07,  2.30it/s]Loading train:  42%|████▏     | 111/266 [00:48<01:12,  2.15it/s]Loading train:  42%|████▏     | 112/266 [00:48<01:12,  2.11it/s]Loading train:  42%|████▏     | 113/266 [00:49<01:11,  2.15it/s]Loading train:  43%|████▎     | 114/266 [00:49<01:11,  2.14it/s]Loading train:  43%|████▎     | 115/266 [00:50<01:10,  2.14it/s]Loading train:  44%|████▎     | 116/266 [00:50<01:07,  2.22it/s]Loading train:  44%|████▍     | 117/266 [00:50<01:03,  2.35it/s]Loading train:  44%|████▍     | 118/266 [00:51<01:00,  2.43it/s]Loading train:  45%|████▍     | 119/266 [00:51<01:07,  2.17it/s]Loading train:  45%|████▌     | 120/266 [00:52<01:06,  2.19it/s]Loading train:  45%|████▌     | 121/266 [00:52<01:03,  2.28it/s]Loading train:  46%|████▌     | 122/266 [00:53<01:03,  2.25it/s]Loading train:  46%|████▌     | 123/266 [00:53<01:02,  2.28it/s]Loading train:  47%|████▋     | 124/266 [00:53<00:59,  2.37it/s]Loading train:  47%|████▋     | 125/266 [00:54<01:03,  2.24it/s]Loading train:  47%|████▋     | 126/266 [00:54<01:01,  2.27it/s]Loading train:  48%|████▊     | 127/266 [00:55<00:59,  2.33it/s]Loading train:  48%|████▊     | 128/266 [00:55<00:58,  2.35it/s]Loading train:  48%|████▊     | 129/266 [00:56<00:58,  2.33it/s]Loading train:  49%|████▉     | 130/266 [00:56<00:59,  2.28it/s]Loading train:  49%|████▉     | 131/266 [00:56<00:58,  2.31it/s]Loading train:  50%|████▉     | 132/266 [00:57<00:56,  2.36it/s]Loading train:  50%|█████     | 133/266 [00:57<00:54,  2.42it/s]Loading train:  50%|█████     | 134/266 [00:58<00:54,  2.43it/s]Loading train:  51%|█████     | 135/266 [00:58<00:56,  2.33it/s]Loading train:  51%|█████     | 136/266 [00:59<00:57,  2.28it/s]Loading train:  52%|█████▏    | 137/266 [00:59<00:57,  2.25it/s]Loading train:  52%|█████▏    | 138/266 [00:59<00:55,  2.31it/s]Loading train:  52%|█████▏    | 139/266 [01:00<00:53,  2.37it/s]Loading train:  53%|█████▎    | 140/266 [01:00<00:50,  2.48it/s]Loading train:  53%|█████▎    | 141/266 [01:01<00:48,  2.55it/s]Loading train:  53%|█████▎    | 142/266 [01:01<00:47,  2.61it/s]Loading train:  54%|█████▍    | 143/266 [01:01<00:49,  2.51it/s]Loading train:  54%|█████▍    | 144/266 [01:02<00:48,  2.50it/s]Loading train:  55%|█████▍    | 145/266 [01:02<00:49,  2.45it/s]Loading train:  55%|█████▍    | 146/266 [01:03<00:47,  2.51it/s]Loading train:  55%|█████▌    | 147/266 [01:03<00:47,  2.49it/s]Loading train:  56%|█████▌    | 148/266 [01:03<00:47,  2.50it/s]Loading train:  56%|█████▌    | 149/266 [01:04<00:47,  2.45it/s]Loading train:  56%|█████▋    | 150/266 [01:04<00:51,  2.26it/s]Loading train:  57%|█████▋    | 151/266 [01:05<00:48,  2.37it/s]Loading train:  57%|█████▋    | 152/266 [01:05<00:46,  2.43it/s]Loading train:  58%|█████▊    | 153/266 [01:05<00:45,  2.50it/s]Loading train:  58%|█████▊    | 154/266 [01:06<00:45,  2.47it/s]Loading train:  58%|█████▊    | 155/266 [01:06<00:42,  2.61it/s]Loading train:  59%|█████▊    | 156/266 [01:06<00:39,  2.76it/s]Loading train:  59%|█████▉    | 157/266 [01:07<00:40,  2.67it/s]Loading train:  59%|█████▉    | 158/266 [01:07<00:39,  2.72it/s]Loading train:  60%|█████▉    | 159/266 [01:08<00:38,  2.79it/s]Loading train:  60%|██████    | 160/266 [01:08<00:37,  2.81it/s]Loading train:  61%|██████    | 161/266 [01:08<00:37,  2.79it/s]Loading train:  61%|██████    | 162/266 [01:09<00:37,  2.81it/s]Loading train:  61%|██████▏   | 163/266 [01:09<00:36,  2.82it/s]Loading train:  62%|██████▏   | 164/266 [01:09<00:38,  2.68it/s]Loading train:  62%|██████▏   | 165/266 [01:10<00:36,  2.74it/s]Loading train:  62%|██████▏   | 166/266 [01:10<00:35,  2.81it/s]Loading train:  63%|██████▎   | 167/266 [01:10<00:36,  2.71it/s]Loading train:  63%|██████▎   | 168/266 [01:11<00:35,  2.72it/s]Loading train:  64%|██████▎   | 169/266 [01:11<00:34,  2.85it/s]Loading train:  64%|██████▍   | 170/266 [01:11<00:32,  2.95it/s]Loading train:  64%|██████▍   | 171/266 [01:12<00:36,  2.61it/s]Loading train:  65%|██████▍   | 172/266 [01:12<00:34,  2.72it/s]Loading train:  65%|██████▌   | 173/266 [01:13<00:33,  2.77it/s]Loading train:  65%|██████▌   | 174/266 [01:13<00:34,  2.70it/s]Loading train:  66%|██████▌   | 175/266 [01:13<00:32,  2.78it/s]Loading train:  66%|██████▌   | 176/266 [01:14<00:33,  2.71it/s]Loading train:  67%|██████▋   | 177/266 [01:14<00:36,  2.42it/s]Loading train:  67%|██████▋   | 178/266 [01:15<00:34,  2.55it/s]Loading train:  67%|██████▋   | 179/266 [01:15<00:33,  2.58it/s]Loading train:  68%|██████▊   | 180/266 [01:15<00:34,  2.50it/s]Loading train:  68%|██████▊   | 181/266 [01:16<00:34,  2.44it/s]Loading train:  68%|██████▊   | 182/266 [01:16<00:35,  2.40it/s]Loading train:  69%|██████▉   | 183/266 [01:17<00:39,  2.12it/s]Loading train:  69%|██████▉   | 184/266 [01:17<00:35,  2.29it/s]Loading train:  70%|██████▉   | 185/266 [01:18<00:33,  2.42it/s]Loading train:  70%|██████▉   | 186/266 [01:18<00:31,  2.55it/s]Loading train:  70%|███████   | 187/266 [01:18<00:29,  2.67it/s]Loading train:  71%|███████   | 188/266 [01:19<00:28,  2.75it/s]Loading train:  71%|███████   | 189/266 [01:19<00:32,  2.35it/s]Loading train:  71%|███████▏  | 190/266 [01:20<00:30,  2.49it/s]Loading train:  72%|███████▏  | 191/266 [01:20<00:33,  2.26it/s]Loading train:  72%|███████▏  | 192/266 [01:20<00:30,  2.40it/s]Loading train:  73%|███████▎  | 193/266 [01:21<00:29,  2.47it/s]Loading train:  73%|███████▎  | 194/266 [01:21<00:34,  2.08it/s]Loading train:  73%|███████▎  | 195/266 [01:22<00:31,  2.29it/s]Loading train:  74%|███████▎  | 196/266 [01:22<00:28,  2.46it/s]Loading train:  74%|███████▍  | 197/266 [01:22<00:26,  2.59it/s]Loading train:  74%|███████▍  | 198/266 [01:23<00:25,  2.70it/s]Loading train:  75%|███████▍  | 199/266 [01:23<00:24,  2.73it/s]Loading train:  75%|███████▌  | 200/266 [01:24<00:24,  2.67it/s]Loading train:  76%|███████▌  | 201/266 [01:24<00:24,  2.69it/s]Loading train:  76%|███████▌  | 202/266 [01:24<00:23,  2.78it/s]Loading train:  76%|███████▋  | 203/266 [01:25<00:22,  2.81it/s]Loading train:  77%|███████▋  | 204/266 [01:25<00:21,  2.86it/s]Loading train:  77%|███████▋  | 205/266 [01:25<00:21,  2.85it/s]Loading train:  77%|███████▋  | 206/266 [01:26<00:21,  2.85it/s]Loading train:  78%|███████▊  | 207/266 [01:26<00:23,  2.47it/s]Loading train:  78%|███████▊  | 208/266 [01:27<00:22,  2.59it/s]Loading train:  79%|███████▊  | 209/266 [01:27<00:21,  2.66it/s]Loading train:  79%|███████▉  | 210/266 [01:27<00:20,  2.69it/s]Loading train:  79%|███████▉  | 211/266 [01:28<00:19,  2.77it/s]Loading train:  80%|███████▉  | 212/266 [01:28<00:19,  2.84it/s]Loading train:  80%|████████  | 213/266 [01:28<00:21,  2.44it/s]Loading train:  80%|████████  | 214/266 [01:29<00:20,  2.54it/s]Loading train:  81%|████████  | 215/266 [01:29<00:18,  2.69it/s]Loading train:  81%|████████  | 216/266 [01:29<00:17,  2.81it/s]Loading train:  82%|████████▏ | 217/266 [01:30<00:16,  2.91it/s]Loading train:  82%|████████▏ | 218/266 [01:30<00:16,  2.97it/s]Loading train:  82%|████████▏ | 219/266 [01:30<00:16,  2.90it/s]Loading train:  83%|████████▎ | 220/266 [01:31<00:17,  2.60it/s]Loading train:  83%|████████▎ | 221/266 [01:31<00:16,  2.67it/s]Loading train:  83%|████████▎ | 222/266 [01:32<00:15,  2.82it/s]Loading train:  84%|████████▍ | 223/266 [01:32<00:14,  2.94it/s]Loading train:  84%|████████▍ | 224/266 [01:32<00:13,  3.03it/s]Loading train:  85%|████████▍ | 225/266 [01:32<00:13,  3.12it/s]Loading train:  85%|████████▍ | 226/266 [01:33<00:13,  2.98it/s]Loading train:  85%|████████▌ | 227/266 [01:33<00:14,  2.75it/s]Loading train:  86%|████████▌ | 228/266 [01:34<00:13,  2.88it/s]Loading train:  86%|████████▌ | 229/266 [01:34<00:12,  2.97it/s]Loading train:  86%|████████▋ | 230/266 [01:34<00:11,  3.04it/s]Loading train:  87%|████████▋ | 231/266 [01:35<00:11,  3.01it/s]Loading train:  87%|████████▋ | 232/266 [01:35<00:11,  2.99it/s]Loading train:  88%|████████▊ | 233/266 [01:35<00:10,  3.01it/s]Loading train:  88%|████████▊ | 234/266 [01:36<00:11,  2.87it/s]Loading train:  88%|████████▊ | 235/266 [01:36<00:10,  2.91it/s]Loading train:  89%|████████▊ | 236/266 [01:36<00:10,  2.94it/s]Loading train:  89%|████████▉ | 237/266 [01:37<00:09,  2.96it/s]Loading train:  89%|████████▉ | 238/266 [01:37<00:09,  2.99it/s]Loading train:  90%|████████▉ | 239/266 [01:37<00:09,  2.99it/s]Loading train:  90%|█████████ | 240/266 [01:38<00:10,  2.50it/s]Loading train:  91%|█████████ | 241/266 [01:38<00:10,  2.35it/s]Loading train:  91%|█████████ | 242/266 [01:39<00:10,  2.21it/s]Loading train:  91%|█████████▏| 243/266 [01:39<00:09,  2.40it/s]Loading train:  92%|█████████▏| 244/266 [01:40<00:09,  2.43it/s]Loading train:  92%|█████████▏| 245/266 [01:40<00:08,  2.45it/s]Loading train:  92%|█████████▏| 246/266 [01:40<00:08,  2.36it/s]Loading train:  93%|█████████▎| 247/266 [01:41<00:08,  2.31it/s]Loading train:  93%|█████████▎| 248/266 [01:41<00:07,  2.26it/s]Loading train:  94%|█████████▎| 249/266 [01:42<00:07,  2.26it/s]Loading train:  94%|█████████▍| 250/266 [01:42<00:07,  2.08it/s]Loading train:  94%|█████████▍| 251/266 [01:43<00:08,  1.78it/s]Loading train:  95%|█████████▍| 252/266 [01:44<00:07,  1.87it/s]Loading train:  95%|█████████▌| 253/266 [01:44<00:06,  1.88it/s]Loading train:  95%|█████████▌| 254/266 [01:45<00:05,  2.02it/s]Loading train:  96%|█████████▌| 255/266 [01:45<00:05,  2.05it/s]Loading train:  96%|█████████▌| 256/266 [01:45<00:04,  2.05it/s]Loading train:  97%|█████████▋| 257/266 [01:46<00:04,  2.06it/s]Loading train:  97%|█████████▋| 258/266 [01:47<00:04,  1.97it/s]Loading train:  97%|█████████▋| 259/266 [01:47<00:03,  2.08it/s]Loading train:  98%|█████████▊| 260/266 [01:47<00:02,  2.15it/s]Loading train:  98%|█████████▊| 261/266 [01:48<00:02,  2.29it/s]Loading train:  98%|█████████▊| 262/266 [01:48<00:01,  2.37it/s]Loading train:  99%|█████████▉| 263/266 [01:48<00:01,  2.44it/s]Loading train:  99%|█████████▉| 264/266 [01:49<00:00,  2.47it/s]Loading train: 100%|█████████▉| 265/266 [01:49<00:00,  2.51it/s]Loading train: 100%|██████████| 266/266 [01:50<00:00,  2.46it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   4%|▍         | 10/266 [00:00<00:02, 97.39it/s]concatenating: train:   8%|▊         | 22/266 [00:00<00:02, 102.82it/s]concatenating: train:  15%|█▌        | 40/266 [00:00<00:01, 117.62it/s]concatenating: train:  21%|██▏       | 57/266 [00:00<00:01, 128.77it/s]concatenating: train:  27%|██▋       | 73/266 [00:00<00:01, 136.52it/s]concatenating: train:  33%|███▎      | 88/266 [00:00<00:01, 137.77it/s]concatenating: train:  39%|███▉      | 105/266 [00:00<00:01, 145.57it/s]concatenating: train:  46%|████▌     | 122/266 [00:00<00:00, 149.20it/s]concatenating: train:  52%|█████▏    | 138/266 [00:00<00:00, 149.78it/s]concatenating: train:  59%|█████▊    | 156/266 [00:01<00:00, 156.37it/s]concatenating: train:  66%|██████▌   | 176/266 [00:01<00:00, 165.98it/s]concatenating: train:  73%|███████▎  | 193/266 [00:01<00:00, 167.01it/s]concatenating: train:  80%|████████  | 213/266 [00:01<00:00, 173.94it/s]concatenating: train:  88%|████████▊ | 233/266 [00:01<00:00, 179.68it/s]concatenating: train:  95%|█████████▍| 252/266 [00:01<00:00, 182.25it/s]concatenating: train: 100%|██████████| 266/266 [00:01<00:00, 168.37it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:00<00:01,  2.53it/s]Loading test:  40%|████      | 2/5 [00:00<00:01,  2.55it/s]Loading test:  60%|██████    | 3/5 [00:01<00:00,  2.45it/s]Loading test:  80%|████████  | 4/5 [00:01<00:00,  2.53it/s]Loading test: 100%|██████████| 5/5 [00:02<00:00,  2.48it/s]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 626.95it/s]2019-08-17 00:38:30.874408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-17 00:38:30.874522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-17 00:38:30.874539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-17 00:38:30.874549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-17 00:38:30.874963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:07,  6.02it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:05,  7.01it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:06,  6.33it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:04,  8.00it/s]loading the weights for Res Unet:  20%|██        | 9/44 [00:00<00:05,  6.86it/s]loading the weights for Res Unet:  25%|██▌       | 11/44 [00:01<00:04,  7.66it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:04,  6.71it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:03,  8.66it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02,  9.02it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:02<00:03,  7.38it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:02,  9.26it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01,  9.61it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01,  9.64it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:03<00:01,  7.60it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:03<00:00,  9.45it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:03<00:00,  9.66it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00,  9.53it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:04<00:00,  7.56it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:04<00:00, 10.92it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 3  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 3  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 76, 108, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 76, 108, 30)  300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 76, 108, 30)  120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 76, 108, 30)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 76, 108, 30)  8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 76, 108, 30)  120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 76, 108, 30)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 38, 54, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 38, 54, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 38, 54, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 38, 54, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 38, 54, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 38, 54, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 38, 54, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 38, 54, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 38, 54, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 19, 27, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 19, 27, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 19, 27, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 19, 27, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 19, 27, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 19, 27, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 19, 27, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 19, 27, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 19, 27, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 19, 27, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 38, 54, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 38, 54, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 38, 54, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 38, 54, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 38, 54, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 38, 54, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 38, 54, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 38, 54, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 38, 54, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 38, 54, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 76, 108, 30)  25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 76, 108, 60)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 76, 108, 30)  16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 76, 108, 30)  120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 76, 108, 30)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 76, 108, 30)  8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 76, 108, 30)  120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 76, 108, 30)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 76, 108, 90)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 76, 108, 90)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 76, 108, 2)   182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 50,972
Non-trainable params: 449,370
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97453182 0.02546818]
Train on 27987 samples, validate on 500 samples
Epoch 1/300
 - 50s - loss: 0.4286 - acc: 0.9477 - mDice: 0.4959 - val_loss: 0.1855 - val_acc: 0.9819 - val_mDice: 0.7133

Epoch 00001: val_mDice improved from -inf to 0.71334, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 46s - loss: 0.1839 - acc: 0.9808 - mDice: 0.7040 - val_loss: 0.1216 - val_acc: 0.9890 - val_mDice: 0.7994

Epoch 00002: val_mDice improved from 0.71334 to 0.79942, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 46s - loss: 0.1366 - acc: 0.9843 - mDice: 0.7727 - val_loss: 0.0912 - val_acc: 0.9918 - val_mDice: 0.8453

Epoch 00003: val_mDice improved from 0.79942 to 0.84531, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 46s - loss: 0.1173 - acc: 0.9862 - mDice: 0.8029 - val_loss: 0.0785 - val_acc: 0.9930 - val_mDice: 0.8660

Epoch 00004: val_mDice improved from 0.84531 to 0.86597, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 46s - loss: 0.1069 - acc: 0.9873 - mDice: 0.8196 - val_loss: 0.0793 - val_acc: 0.9937 - val_mDice: 0.8646

Epoch 00005: val_mDice did not improve from 0.86597
Epoch 6/300
 - 47s - loss: 0.1000 - acc: 0.9882 - mDice: 0.8309 - val_loss: 0.0737 - val_acc: 0.9935 - val_mDice: 0.8742

Epoch 00006: val_mDice improved from 0.86597 to 0.87419, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 47s - loss: 0.0948 - acc: 0.9887 - mDice: 0.8396 - val_loss: 0.0950 - val_acc: 0.9943 - val_mDice: 0.8381

Epoch 00007: val_mDice did not improve from 0.87419
Epoch 8/300
 - 48s - loss: 0.0906 - acc: 0.9893 - mDice: 0.8467 - val_loss: 0.3496 - val_acc: 0.9941 - val_mDice: 0.5127

Epoch 00008: val_mDice did not improve from 0.87419
Epoch 9/300
 - 47s - loss: 0.0878 - acc: 0.9896 - mDice: 0.8513 - val_loss: 0.0741 - val_acc: 0.9942 - val_mDice: 0.8737

Epoch 00009: val_mDice did not improve from 0.87419
Epoch 10/300
 - 48s - loss: 0.0840 - acc: 0.9901 - mDice: 0.8579 - val_loss: 0.0782 - val_acc: 0.9943 - val_mDice: 0.8663

Epoch 00010: val_mDice did not improve from 0.87419
Epoch 11/300
 - 48s - loss: 0.0816 - acc: 0.9905 - mDice: 0.8621 - val_loss: 0.0716 - val_acc: 0.9945 - val_mDice: 0.8777

Epoch 00011: val_mDice improved from 0.87419 to 0.87774, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 49s - loss: 0.0792 - acc: 0.9907 - mDice: 0.8661 - val_loss: 0.0738 - val_acc: 0.9944 - val_mDice: 0.8743

Epoch 00012: val_mDice did not improve from 0.87774
Epoch 13/300
 - 48s - loss: 0.0774 - acc: 0.9910 - mDice: 0.8692 - val_loss: 0.0704 - val_acc: 0.9938 - val_mDice: 0.8802

Epoch 00013: val_mDice improved from 0.87774 to 0.88016, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 48s - loss: 0.0759 - acc: 0.9912 - mDice: 0.8718 - val_loss: 0.1605 - val_acc: 0.9942 - val_mDice: 0.7372

Epoch 00014: val_mDice did not improve from 0.88016
Epoch 15/300
 - 48s - loss: 0.0745 - acc: 0.9915 - mDice: 0.8743 - val_loss: 0.0697 - val_acc: 0.9946 - val_mDice: 0.8811

Epoch 00015: val_mDice improved from 0.88016 to 0.88112, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 48s - loss: 0.0694 - acc: 0.9925 - mDice: 0.8766 - val_loss: 0.0682 - val_acc: 0.9941 - val_mDice: 0.8758

Epoch 00016: val_mDice did not improve from 0.88112
Epoch 17/300
 - 48s - loss: 0.0667 - acc: 0.9929 - mDice: 0.8786 - val_loss: 0.0773 - val_acc: 0.9929 - val_mDice: 0.8605

Epoch 00017: val_mDice did not improve from 0.88112
Epoch 18/300
 - 48s - loss: 0.0657 - acc: 0.9930 - mDice: 0.8803 - val_loss: 0.0722 - val_acc: 0.9933 - val_mDice: 0.8691

Epoch 00018: val_mDice did not improve from 0.88112
Epoch 19/300
 - 48s - loss: 0.0644 - acc: 0.9931 - mDice: 0.8824 - val_loss: 0.0679 - val_acc: 0.9938 - val_mDice: 0.8763

Epoch 00019: val_mDice did not improve from 0.88112
Epoch 20/300
 - 48s - loss: 0.0641 - acc: 0.9932 - mDice: 0.8828 - val_loss: 0.0662 - val_acc: 0.9941 - val_mDice: 0.8790

Epoch 00020: val_mDice did not improve from 0.88112
Epoch 21/300
 - 46s - loss: 0.0631 - acc: 0.9933 - mDice: 0.8846 - val_loss: 0.0739 - val_acc: 0.9931 - val_mDice: 0.8663

Epoch 00021: val_mDice did not improve from 0.88112
Epoch 22/300
 - 46s - loss: 0.0623 - acc: 0.9933 - mDice: 0.8859 - val_loss: 0.0706 - val_acc: 0.9934 - val_mDice: 0.8717

Epoch 00022: val_mDice did not improve from 0.88112
Epoch 23/300
 - 45s - loss: 0.0617 - acc: 0.9934 - mDice: 0.8870 - val_loss: 0.0758 - val_acc: 0.9931 - val_mDice: 0.8634

Epoch 00023: val_mDice did not improve from 0.88112
Epoch 24/300
 - 46s - loss: 0.0610 - acc: 0.9934 - mDice: 0.8881 - val_loss: 0.0855 - val_acc: 0.9918 - val_mDice: 0.8474

Epoch 00024: val_mDice did not improve from 0.88112
Epoch 25/300
 - 46s - loss: 0.0607 - acc: 0.9935 - mDice: 0.8888 - val_loss: 0.0639 - val_acc: 0.9942 - val_mDice: 0.8833

Epoch 00025: val_mDice improved from 0.88112 to 0.88329, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 47s - loss: 0.0602 - acc: 0.9935 - mDice: 0.8897 - val_loss: 0.0693 - val_acc: 0.9936 - val_mDice: 0.8742

Epoch 00026: val_mDice did not improve from 0.88329
Epoch 27/300
 - 48s - loss: 0.0596 - acc: 0.9936 - mDice: 0.8906 - val_loss: 0.0662 - val_acc: 0.9941 - val_mDice: 0.8792

Epoch 00027: val_mDice did not improve from 0.88329
Epoch 28/300
 - 48s - loss: 0.0592 - acc: 0.9936 - mDice: 0.8913 - val_loss: 0.0689 - val_acc: 0.9938 - val_mDice: 0.8747

Epoch 00028: val_mDice did not improve from 0.88329
Epoch 29/300
 - 48s - loss: 0.0589 - acc: 0.9937 - mDice: 0.8919 - val_loss: 0.0632 - val_acc: 0.9942 - val_mDice: 0.8844

Epoch 00029: val_mDice improved from 0.88329 to 0.88441, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 48s - loss: 0.0585 - acc: 0.9937 - mDice: 0.8926 - val_loss: 0.0744 - val_acc: 0.9930 - val_mDice: 0.8659

Epoch 00030: val_mDice did not improve from 0.88441
Epoch 31/300
 - 48s - loss: 0.0580 - acc: 0.9937 - mDice: 0.8933 - val_loss: 0.0645 - val_acc: 0.9941 - val_mDice: 0.8822

Epoch 00031: val_mDice did not improve from 0.88441
Epoch 32/300
 - 48s - loss: 0.0579 - acc: 0.9937 - mDice: 0.8936 - val_loss: 0.0657 - val_acc: 0.9939 - val_mDice: 0.8802

Epoch 00032: val_mDice did not improve from 0.88441
Epoch 33/300
 - 48s - loss: 0.0574 - acc: 0.9938 - mDice: 0.8944 - val_loss: 0.0630 - val_acc: 0.9944 - val_mDice: 0.8848

Epoch 00033: val_mDice improved from 0.88441 to 0.88478, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 48s - loss: 0.0569 - acc: 0.9938 - mDice: 0.8953 - val_loss: 0.0702 - val_acc: 0.9935 - val_mDice: 0.8727

Epoch 00034: val_mDice did not improve from 0.88478
Epoch 35/300
 - 49s - loss: 0.0567 - acc: 0.9939 - mDice: 0.8957 - val_loss: 0.0666 - val_acc: 0.9942 - val_mDice: 0.8786

Epoch 00035: val_mDice did not improve from 0.88478
Epoch 36/300
 - 49s - loss: 0.0564 - acc: 0.9939 - mDice: 0.8962 - val_loss: 0.0680 - val_acc: 0.9937 - val_mDice: 0.8764

Epoch 00036: val_mDice did not improve from 0.88478
Epoch 37/300
 - 49s - loss: 0.0560 - acc: 0.9939 - mDice: 0.8970 - val_loss: 0.0704 - val_acc: 0.9935 - val_mDice: 0.8723

Epoch 00037: val_mDice did not improve from 0.88478
Epoch 38/300
 - 49s - loss: 0.0559 - acc: 0.9939 - mDice: 0.8970 - val_loss: 0.0664 - val_acc: 0.9939 - val_mDice: 0.8791

Epoch 00038: val_mDice did not improve from 0.88478
Epoch 39/300
 - 49s - loss: 0.0558 - acc: 0.9939 - mDice: 0.8973 - val_loss: 0.0719 - val_acc: 0.9936 - val_mDice: 0.8697

Epoch 00039: val_mDice did not improve from 0.88478
Epoch 40/300
 - 47s - loss: 0.0554 - acc: 0.9940 - mDice: 0.8979 - val_loss: 0.0686 - val_acc: 0.9938 - val_mDice: 0.8752

Epoch 00040: val_mDice did not improve from 0.88478
Epoch 41/300
 - 46s - loss: 0.0551 - acc: 0.9940 - mDice: 0.8985 - val_loss: 0.0678 - val_acc: 0.9938 - val_mDice: 0.8767

Epoch 00041: val_mDice did not improve from 0.88478
Epoch 42/300
 - 46s - loss: 0.0550 - acc: 0.9940 - mDice: 0.8986 - val_loss: 0.0667 - val_acc: 0.9941 - val_mDice: 0.8784

Epoch 00042: val_mDice did not improve from 0.88478
Epoch 43/300
 - 45s - loss: 0.0546 - acc: 0.9940 - mDice: 0.8993 - val_loss: 0.0676 - val_acc: 0.9938 - val_mDice: 0.8772

Epoch 00043: val_mDice did not improve from 0.88478
Epoch 44/300
 - 45s - loss: 0.0546 - acc: 0.9940 - mDice: 0.8994 - val_loss: 0.0652 - val_acc: 0.9941 - val_mDice: 0.8811

Epoch 00044: val_mDice did not improve from 0.88478
Epoch 45/300
 - 45s - loss: 0.0542 - acc: 0.9941 - mDice: 0.9000 - val_loss: 0.0687 - val_acc: 0.9936 - val_mDice: 0.8751

Epoch 00045: val_mDice did not improve from 0.88478
Epoch 46/300
 - 45s - loss: 0.0540 - acc: 0.9941 - mDice: 0.9004 - val_loss: 0.0664 - val_acc: 0.9940 - val_mDice: 0.8788

Epoch 00046: val_mDice did not improve from 0.88478
Epoch 47/300
 - 46s - loss: 0.0540 - acc: 0.9941 - mDice: 0.9004 - val_loss: 0.0652 - val_acc: 0.9941 - val_mDice: 0.8810

Epoch 00047: val_mDice did not improve from 0.88478
Epoch 48/300
 - 47s - loss: 0.0539 - acc: 0.9941 - mDice: 0.9006 - val_loss: 0.0637 - val_acc: 0.9941 - val_mDice: 0.8837

Epoch 00048: val_mDice did not improve from 0.88478
Epoch 49/300
 - 47s - loss: 0.0535 - acc: 0.9941 - mDice: 0.9012 - val_loss: 0.0692 - val_acc: 0.9936 - val_mDice: 0.8745

Epoch 00049: val_mDice did not improve from 0.88478
Epoch 50/300
 - 47s - loss: 0.0537 - acc: 0.9941 - mDice: 0.9010 - val_loss: 0.0630 - val_acc: 0.9943 - val_mDice: 0.8849

Epoch 00050: val_mDice improved from 0.88478 to 0.88485, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 47s - loss: 0.0532 - acc: 0.9942 - mDice: 0.9018 - val_loss: 0.0642 - val_acc: 0.9940 - val_mDice: 0.8828

Epoch 00051: val_mDice did not improve from 0.88485
Epoch 52/300
 - 47s - loss: 0.0530 - acc: 0.9942 - mDice: 0.9021 - val_loss: 0.0669 - val_acc: 0.9938 - val_mDice: 0.8783

Epoch 00052: val_mDice did not improve from 0.88485
Epoch 53/300
 - 47s - loss: 0.0530 - acc: 0.9942 - mDice: 0.9021 - val_loss: 0.0646 - val_acc: 0.9941 - val_mDice: 0.8820

Epoch 00053: val_mDice did not improve from 0.88485
Epoch 54/300
 - 46s - loss: 0.0529 - acc: 0.9942 - mDice: 0.9022 - val_loss: 0.0642 - val_acc: 0.9943 - val_mDice: 0.8826

Epoch 00054: val_mDice did not improve from 0.88485
Epoch 55/300
 - 47s - loss: 0.0526 - acc: 0.9942 - mDice: 0.9028 - val_loss: 0.0648 - val_acc: 0.9940 - val_mDice: 0.8817

Epoch 00055: val_mDice did not improve from 0.88485
Epoch 56/300
 - 47s - loss: 0.0524 - acc: 0.9942 - mDice: 0.9033 - val_loss: 0.0674 - val_acc: 0.9937 - val_mDice: 0.8775

Epoch 00056: val_mDice did not improve from 0.88485
Epoch 57/300
 - 47s - loss: 0.0523 - acc: 0.9942 - mDice: 0.9034 - val_loss: 0.0646 - val_acc: 0.9941 - val_mDice: 0.8821

Epoch 00057: val_mDice did not improve from 0.88485
Epoch 58/300
 - 47s - loss: 0.0521 - acc: 0.9942 - mDice: 0.9036 - val_loss: 0.0645 - val_acc: 0.9939 - val_mDice: 0.8821

Epoch 00058: val_mDice did not improve from 0.88485
Epoch 59/300
 - 46s - loss: 0.0521 - acc: 0.9943 - mDice: 0.9038 - val_loss: 0.0667 - val_acc: 0.9941 - val_mDice: 0.8783

Epoch 00059: val_mDice did not improve from 0.88485
Epoch 60/300
 - 47s - loss: 0.0520 - acc: 0.9943 - mDice: 0.9038 - val_loss: 0.0689 - val_acc: 0.9934 - val_mDice: 0.8749

Epoch 00060: val_mDice did not improve from 0.88485
Epoch 61/300
 - 46s - loss: 0.0518 - acc: 0.9943 - mDice: 0.9042 - val_loss: 0.0644 - val_acc: 0.9943 - val_mDice: 0.8823

Epoch 00061: val_mDice did not improve from 0.88485
Epoch 62/300
 - 47s - loss: 0.0518 - acc: 0.9943 - mDice: 0.9043 - val_loss: 0.0653 - val_acc: 0.9943 - val_mDice: 0.8807

Epoch 00062: val_mDice did not improve from 0.88485
Epoch 63/300
 - 46s - loss: 0.0517 - acc: 0.9943 - mDice: 0.9045 - val_loss: 0.0737 - val_acc: 0.9930 - val_mDice: 0.8669

Epoch 00063: val_mDice did not improve from 0.88485
Epoch 64/300
 - 47s - loss: 0.0516 - acc: 0.9943 - mDice: 0.9047 - val_loss: 0.0671 - val_acc: 0.9939 - val_mDice: 0.8779

Epoch 00064: val_mDice did not improve from 0.88485
Epoch 65/300
 - 46s - loss: 0.0513 - acc: 0.9943 - mDice: 0.9051 - val_loss: 0.0661 - val_acc: 0.9940 - val_mDice: 0.8796

Epoch 00065: val_mDice did not improve from 0.88485
Epoch 66/300
 - 47s - loss: 0.0511 - acc: 0.9943 - mDice: 0.9054 - val_loss: 0.0653 - val_acc: 0.9939 - val_mDice: 0.8808

Epoch 00066: val_mDice did not improve from 0.88485
Epoch 67/300
 - 47s - loss: 0.0512 - acc: 0.9943 - mDice: 0.9053 - val_loss: 0.0657 - val_acc: 0.9942 - val_mDice: 0.8803

Epoch 00067: val_mDice did not improve from 0.88485
Epoch 68/300
 - 47s - loss: 0.0512 - acc: 0.9943 - mDice: 0.9053 - val_loss: 0.0684 - val_acc: 0.9941 - val_mDice: 0.8756

Epoch 00068: val_mDice did not improve from 0.88485
Epoch 69/300
 - 47s - loss: 0.0513 - acc: 0.9943 - mDice: 0.9052 - val_loss: 0.0692 - val_acc: 0.9935 - val_mDice: 0.8745

Epoch 00069: val_mDice did not improve from 0.88485
Epoch 70/300
 - 48s - loss: 0.0508 - acc: 0.9944 - mDice: 0.9060 - val_loss: 0.0649 - val_acc: 0.9942 - val_mDice: 0.8814

Epoch 00070: val_mDice did not improve from 0.88485
Epoch 71/300
 - 48s - loss: 0.0511 - acc: 0.9943 - mDice: 0.9055 - val_loss: 0.0676 - val_acc: 0.9937 - val_mDice: 0.8772

Epoch 00071: val_mDice did not improve from 0.88485
Epoch 72/300
 - 47s - loss: 0.0505 - acc: 0.9944 - mDice: 0.9065 - val_loss: 0.0658 - val_acc: 0.9942 - val_mDice: 0.8797

Epoch 00072: val_mDice did not improve from 0.88485
Epoch 73/300
 - 47s - loss: 0.0507 - acc: 0.9944 - mDice: 0.9062 - val_loss: 0.0632 - val_acc: 0.9944 - val_mDice: 0.8843

Epoch 00073: val_mDice did not improve from 0.88485
Epoch 74/300
 - 48s - loss: 0.0507 - acc: 0.9944 - mDice: 0.9062 - val_loss: 0.0700 - val_acc: 0.9934 - val_mDice: 0.8732

Epoch 00074: val_mDice did not improve from 0.88485
Epoch 75/300
 - 47s - loss: 0.0504 - acc: 0.9944 - mDice: 0.9067 - val_loss: 0.0630 - val_acc: 0.9943 - val_mDice: 0.8847

Epoch 00075: val_mDice did not improve from 0.88485
Epoch 76/300
 - 48s - loss: 0.0505 - acc: 0.9944 - mDice: 0.9065 - val_loss: 0.0682 - val_acc: 0.9937 - val_mDice: 0.8763

Epoch 00076: val_mDice did not improve from 0.88485
Epoch 77/300
 - 47s - loss: 0.0503 - acc: 0.9944 - mDice: 0.9069 - val_loss: 0.0639 - val_acc: 0.9945 - val_mDice: 0.8830

Epoch 00077: val_mDice did not improve from 0.88485
Epoch 78/300
 - 47s - loss: 0.0501 - acc: 0.9944 - mDice: 0.9073 - val_loss: 0.0650 - val_acc: 0.9943 - val_mDice: 0.8812

Epoch 00078: val_mDice did not improve from 0.88485
Epoch 79/300
 - 48s - loss: 0.0501 - acc: 0.9944 - mDice: 0.9072 - val_loss: 0.0641 - val_acc: 0.9941 - val_mDice: 0.8829

Epoch 00079: val_mDice did not improve from 0.88485
Epoch 80/300
 - 47s - loss: 0.0501 - acc: 0.9944 - mDice: 0.9072 - val_loss: 0.0620 - val_acc: 0.9941 - val_mDice: 0.8866

Epoch 00080: val_mDice improved from 0.88485 to 0.88660, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 81/300
 - 45s - loss: 0.0500 - acc: 0.9944 - mDice: 0.9074 - val_loss: 0.0659 - val_acc: 0.9942 - val_mDice: 0.8796

Epoch 00081: val_mDice did not improve from 0.88660
Epoch 82/300
 - 46s - loss: 0.0501 - acc: 0.9944 - mDice: 0.9072 - val_loss: 0.0640 - val_acc: 0.9944 - val_mDice: 0.8829

Epoch 00082: val_mDice did not improve from 0.88660
Epoch 83/300
 - 45s - loss: 0.0500 - acc: 0.9944 - mDice: 0.9074 - val_loss: 0.0655 - val_acc: 0.9941 - val_mDice: 0.8804

Epoch 00083: val_mDice did not improve from 0.88660
Epoch 84/300
 - 45s - loss: 0.0499 - acc: 0.9944 - mDice: 0.9077 - val_loss: 0.0633 - val_acc: 0.9941 - val_mDice: 0.8842

Epoch 00084: val_mDice did not improve from 0.88660
Epoch 85/300
 - 45s - loss: 0.0497 - acc: 0.9945 - mDice: 0.9080 - val_loss: 0.0620 - val_acc: 0.9945 - val_mDice: 0.8865

Epoch 00085: val_mDice did not improve from 0.88660
Epoch 86/300
 - 46s - loss: 0.0496 - acc: 0.9945 - mDice: 0.9082 - val_loss: 0.0666 - val_acc: 0.9941 - val_mDice: 0.8787

Epoch 00086: val_mDice did not improve from 0.88660
Epoch 87/300
 - 47s - loss: 0.0495 - acc: 0.9945 - mDice: 0.9084 - val_loss: 0.0645 - val_acc: 0.9941 - val_mDice: 0.8822

Epoch 00087: val_mDice did not improve from 0.88660
Epoch 88/300
 - 46s - loss: 0.0493 - acc: 0.9945 - mDice: 0.9087 - val_loss: 0.0655 - val_acc: 0.9938 - val_mDice: 0.8805

Epoch 00088: val_mDice did not improve from 0.88660
Epoch 89/300
 - 47s - loss: 0.0495 - acc: 0.9945 - mDice: 0.9083 - val_loss: 0.0684 - val_acc: 0.9937 - val_mDice: 0.8757

Epoch 00089: val_mDice did not improve from 0.88660
Epoch 90/300
 - 47s - loss: 0.0495 - acc: 0.9945 - mDice: 0.9083 - val_loss: 0.0673 - val_acc: 0.9942 - val_mDice: 0.8773

Epoch 00090: val_mDice did not improve from 0.88660
Epoch 91/300
 - 47s - loss: 0.0494 - acc: 0.9945 - mDice: 0.9085 - val_loss: 0.0657 - val_acc: 0.9940 - val_mDice: 0.8802

Epoch 00091: val_mDice did not improve from 0.88660
Epoch 92/300
 - 47s - loss: 0.0491 - acc: 0.9945 - mDice: 0.9090 - val_loss: 0.0657 - val_acc: 0.9938 - val_mDice: 0.8802

Epoch 00092: val_mDice did not improve from 0.88660
Epoch 93/300
 - 47s - loss: 0.0491 - acc: 0.9945 - mDice: 0.9090 - val_loss: 0.0686 - val_acc: 0.9939 - val_mDice: 0.8754

Epoch 00093: val_mDice did not improve from 0.88660
Epoch 94/300
 - 47s - loss: 0.0491 - acc: 0.9945 - mDice: 0.9090 - val_loss: 0.0709 - val_acc: 0.9934 - val_mDice: 0.8716

Epoch 00094: val_mDice did not improve from 0.88660
Epoch 95/300
 - 48s - loss: 0.0490 - acc: 0.9945 - mDice: 0.9092 - val_loss: 0.0650 - val_acc: 0.9940 - val_mDice: 0.8815

Epoch 00095: val_mDice did not improve from 0.88660
Epoch 96/300
 - 47s - loss: 0.0492 - acc: 0.9945 - mDice: 0.9088 - val_loss: 0.0635 - val_acc: 0.9941 - val_mDice: 0.8839

Epoch 00096: val_mDice did not improve from 0.88660
Epoch 97/300
 - 47s - loss: 0.0487 - acc: 0.9946 - mDice: 0.9098 - val_loss: 0.0638 - val_acc: 0.9942 - val_mDice: 0.8835

Epoch 00097: val_mDice did not improve from 0.88660
Epoch 98/300
 - 48s - loss: 0.0489 - acc: 0.9945 - mDice: 0.9094 - val_loss: 0.0654 - val_acc: 0.9940 - val_mDice: 0.8806

Epoch 00098: val_mDice did not improve from 0.88660
Epoch 99/300
 - 48s - loss: 0.0487 - acc: 0.9945 - mDice: 0.9098 - val_loss: 0.0658 - val_acc: 0.9945 - val_mDice: 0.8799

Epoch 00099: val_mDice did not improve from 0.88660
Epoch 100/300
 - 47s - loss: 0.0488 - acc: 0.9945 - mDice: 0.9096 - val_loss: 0.0672 - val_acc: 0.9938 - val_mDice: 0.8777

Epoch 00100: val_mDice did not improve from 0.88660
Epoch 101/300
 - 47s - loss: 0.0487 - acc: 0.9946 - mDice: 0.9098 - val_loss: 0.0657 - val_acc: 0.9940 - val_mDice: 0.8803

Epoch 00101: val_mDice did not improve from 0.88660
Epoch 102/300
 - 48s - loss: 0.0486 - acc: 0.9946 - mDice: 0.9099 - val_loss: 0.0665 - val_acc: 0.9940 - val_mDice: 0.8788

Epoch 00102: val_mDice did not improve from 0.88660
Epoch 103/300
 - 47s - loss: 0.0484 - acc: 0.9946 - mDice: 0.9104 - val_loss: 0.0654 - val_acc: 0.9942 - val_mDice: 0.8806

Epoch 00103: val_mDice did not improve from 0.88660
Epoch 104/300
 - 47s - loss: 0.0487 - acc: 0.9945 - mDice: 0.9097 - val_loss: 0.0646 - val_acc: 0.9939 - val_mDice: 0.8822

Epoch 00104: val_mDice did not improve from 0.88660
Epoch 105/300
 - 47s - loss: 0.0486 - acc: 0.9946 - mDice: 0.9099 - val_loss: 0.0650 - val_acc: 0.9941 - val_mDice: 0.8814

Epoch 00105: val_mDice did not improve from 0.88660
Epoch 106/300
 - 48s - loss: 0.0484 - acc: 0.9946 - mDice: 0.9103 - val_loss: 0.0627 - val_acc: 0.9943 - val_mDice: 0.8853

Epoch 00106: val_mDice did not improve from 0.88660
Epoch 107/300
 - 47s - loss: 0.0484 - acc: 0.9946 - mDice: 0.9103 - val_loss: 0.0632 - val_acc: 0.9943 - val_mDice: 0.8844

Epoch 00107: val_mDice did not improve from 0.88660
Epoch 108/300
 - 47s - loss: 0.0484 - acc: 0.9946 - mDice: 0.9103 - val_loss: 0.0668 - val_acc: 0.9938 - val_mDice: 0.8783

Epoch 00108: val_mDice did not improve from 0.88660
Epoch 109/300
 - 47s - loss: 0.0485 - acc: 0.9946 - mDice: 0.9102 - val_loss: 0.0661 - val_acc: 0.9940 - val_mDice: 0.8795

Epoch 00109: val_mDice did not improve from 0.88660
Epoch 110/300
 - 47s - loss: 0.0484 - acc: 0.9946 - mDice: 0.9103 - val_loss: 0.0655 - val_acc: 0.9939 - val_mDice: 0.8805

Epoch 00110: val_mDice did not improve from 0.88660
Epoch 111/300
 - 47s - loss: 0.0483 - acc: 0.9946 - mDice: 0.9105 - val_loss: 0.0665 - val_acc: 0.9939 - val_mDice: 0.8788

Epoch 00111: val_mDice did not improve from 0.88660
Epoch 112/300
 - 47s - loss: 0.0483 - acc: 0.9946 - mDice: 0.9105 - val_loss: 0.0666 - val_acc: 0.9942 - val_mDice: 0.8785

Epoch 00112: val_mDice did not improve from 0.88660
Epoch 113/300
 - 47s - loss: 0.0480 - acc: 0.9946 - mDice: 0.9110 - val_loss: 0.0672 - val_acc: 0.9940 - val_mDice: 0.8775

Epoch 00113: val_mDice did not improve from 0.88660
Epoch 114/300
 - 48s - loss: 0.0480 - acc: 0.9946 - mDice: 0.9111 - val_loss: 0.0659 - val_acc: 0.9944 - val_mDice: 0.8796

Epoch 00114: val_mDice did not improve from 0.88660
Epoch 115/300
 - 48s - loss: 0.0481 - acc: 0.9946 - mDice: 0.9108 - val_loss: 0.0648 - val_acc: 0.9943 - val_mDice: 0.8816

Epoch 00115: val_mDice did not improve from 0.88660
Epoch 116/300
 - 47s - loss: 0.0480 - acc: 0.9946 - mDice: 0.9110 - val_loss: 0.0657 - val_acc: 0.9941 - val_mDice: 0.8800

Epoch 00116: val_mDice did not improve from 0.88660
Epoch 117/300
 - 48s - loss: 0.0479 - acc: 0.9946 - mDice: 0.9112 - val_loss: 0.0657 - val_acc: 0.9938 - val_mDice: 0.8802

Epoch 00117: val_mDice did not improve from 0.88660
Epoch 118/300
 - 48s - loss: 0.0480 - acc: 0.9946 - mDice: 0.9110 - val_loss: 0.0649 - val_acc: 0.9941 - val_mDice: 0.8815

Epoch 00118: val_mDice did not improve from 0.88660
Epoch 119/300
 - 47s - loss: 0.0482 - acc: 0.9946 - mDice: 0.9107 - val_loss: 0.0651 - val_acc: 0.9944 - val_mDice: 0.8811

Epoch 00119: val_mDice did not improve from 0.88660
Epoch 120/300
 - 48s - loss: 0.0479 - acc: 0.9946 - mDice: 0.9112 - val_loss: 0.0641 - val_acc: 0.9942 - val_mDice: 0.8828

Epoch 00120: val_mDice did not improve from 0.88660
Restoring model weights from the end of the best epoch
Epoch 00120: early stopping
{'val_loss': [0.18547115996479988, 0.1215570479631424, 0.09117848761379718, 0.07852295823395253, 0.07934794761240482, 0.07374563105404378, 0.09500798434019089, 0.34957580268383026, 0.0740518219769001, 0.07824488915503025, 0.07161254808306694, 0.07377003841102123, 0.07039832249283791, 0.16047310680150986, 0.0696997158229351, 0.06818816736340523, 0.07734533064067364, 0.07217737063765525, 0.06791751757264138, 0.06623921655118466, 0.07390660308301449, 0.07061965726315975, 0.07582780420780182, 0.08550855219364166, 0.06387915052473545, 0.06926160119473934, 0.06620673015713692, 0.0688697662204504, 0.06315171048045158, 0.07438916116952896, 0.06452202387154102, 0.06573553234338761, 0.06297000162303448, 0.07023114301264285, 0.06660263761878013, 0.06800574325025081, 0.07042843326926232, 0.0663995124399662, 0.07193176858127118, 0.0686456836760044, 0.06782431453466416, 0.06669446229934692, 0.06758068539202214, 0.06522427275776863, 0.06874191723763942, 0.06643403209745884, 0.06515648886561394, 0.06370482482016086, 0.0691540151834488, 0.06295900382101535, 0.06418318673968315, 0.0668580785393715, 0.06463447958230972, 0.06422550901770592, 0.0648494005203247, 0.06737296842038631, 0.06461044363677501, 0.06454680562019348, 0.0667480319738388, 0.06891192682087421, 0.06441161781549454, 0.06530873626470565, 0.07372056730091572, 0.06706031672656536, 0.06605241633951664, 0.06530325002968311, 0.06565233692526817, 0.06838486194610596, 0.06917043328285218, 0.0649485345929861, 0.0675556268543005, 0.06583777479827405, 0.0632007945328951, 0.0700293205678463, 0.06295850425958634, 0.06815407574176788, 0.06391080915927887, 0.06500768326222897, 0.06406183429062366, 0.061995621770620346, 0.06594198197126389, 0.06399190798401833, 0.06549355573952198, 0.06332083195447921, 0.06200107783079147, 0.06657046750187874, 0.0644968494772911, 0.06554232873022556, 0.06842509657144547, 0.06733187809586524, 0.06571914702653885, 0.06567738018929958, 0.06860773749649525, 0.0709056306630373, 0.06501856371760369, 0.06350502222776414, 0.06377392783761024, 0.06538310386240483, 0.06575718447566033, 0.06715486906468868, 0.06565061323344708, 0.06649127639830113, 0.0653734713792801, 0.06456036381423473, 0.0649807684123516, 0.06270554214715958, 0.06315668895840645, 0.06682409457862377, 0.06608534380793571, 0.06550549417734146, 0.0665380522608757, 0.06655431054532528, 0.06723029352724552, 0.06590048559010028, 0.0648027915507555, 0.06574428118765355, 0.06572230830788613, 0.06487927921116352, 0.06505327448248863, 0.06412594504654408], 'val_acc': [0.9819295763969421, 0.989002925157547, 0.9918365120887757, 0.9929807484149933, 0.9937161326408386, 0.9935397148132324, 0.9942541301250458, 0.9940940618515015, 0.9941739737987518, 0.9943204164505005, 0.9945085287094116, 0.9943774342536926, 0.9938345491886139, 0.9942168653011322, 0.9945699214935303, 0.9941206037998199, 0.9929030239582062, 0.9933128714561462, 0.9938214004039765, 0.9940640807151795, 0.993057256937027, 0.9934229969978332, 0.9931086719036102, 0.9918048262596131, 0.9941637456417084, 0.9936432778835297, 0.9940679848194123, 0.9937953174114227, 0.9942183375358582, 0.9930190026760102, 0.994108909368515, 0.9938586711883545, 0.9943793833255767, 0.9935170590877533, 0.9942171096801757, 0.9937173426151276, 0.9934970736503601, 0.9939307987689971, 0.9936091661453247, 0.9938255488872528, 0.9937872767448426, 0.9940740764141083, 0.9937650978565216, 0.9941096484661103, 0.9935614109039307, 0.9940372824668884, 0.9941276848316193, 0.9940891742706299, 0.9935748100280761, 0.9942748606204986, 0.9939590513706207, 0.993812620639801, 0.9941388905048371, 0.9942524433135986, 0.9940265536308288, 0.9936593532562256, 0.9940745651721954, 0.9939449310302735, 0.9940760254859924, 0.9934451758861542, 0.994269746541977, 0.9942763209342956, 0.9930302202701569, 0.99390789270401, 0.9940423965454102, 0.993903511762619, 0.9941542446613312, 0.994050931930542, 0.9935482382774353, 0.9941781163215637, 0.9936934649944306, 0.9942309975624084, 0.9943837761878968, 0.9934098482131958, 0.9942724227905273, 0.993706613779068, 0.9944685697555542, 0.9942802131175995, 0.9941206037998199, 0.9941137969493866, 0.9942307412624359, 0.9943742752075195, 0.9941060066223144, 0.9941072106361389, 0.9944822013378143, 0.9941052675247193, 0.9941164672374725, 0.9937597513198853, 0.9937253892421722, 0.9941754400730133, 0.9939712405204773, 0.9938491761684418, 0.9939429819583893, 0.9934100866317749, 0.9939697861671448, 0.9941391289234162, 0.9941593527793884, 0.9940445840358734, 0.9944619834423065, 0.9937838733196258, 0.993982458114624, 0.9939607679843903, 0.9941958963871003, 0.993927139043808, 0.9940567791461945, 0.9943347871303558, 0.994327974319458, 0.9938428282737732, 0.9940380215644836, 0.9939098477363586, 0.9939061880111695, 0.9941573977470398, 0.9939995110034943, 0.9943562328815461, 0.9942750990390777, 0.9941201269626617, 0.9938457667827606, 0.9940955221652985, 0.9943513572216034, 0.9941532611846924], 'val_mDice': [0.7133384436368942, 0.7994240164756775, 0.8453059673309327, 0.8659668385982513, 0.8645759105682373, 0.8741862535476684, 0.8381454825401307, 0.5127254813909531, 0.8736764907836914, 0.8663399338722229, 0.8777385234832764, 0.8742751777172089, 0.8801613092422486, 0.7372436046600341, 0.8811225652694702, 0.8757909953594207, 0.8605151176452637, 0.8690632104873657, 0.876315301656723, 0.8790365338325501, 0.8663230419158936, 0.8717098355293273, 0.8634086370468139, 0.8474427759647369, 0.8832890629768372, 0.8742175757884979, 0.879234117269516, 0.874746811389923, 0.8844078779220581, 0.8658937454223633, 0.8822110116481781, 0.8802244484424591, 0.8847842395305634, 0.8726866483688355, 0.8785525023937225, 0.876380443572998, 0.8723372995853425, 0.8790558516979218, 0.8697346925735474, 0.8751669228076935, 0.8767245411872864, 0.8783594906330109, 0.8772051632404327, 0.8810503661632538, 0.8750515878200531, 0.8787622809410095, 0.8810040712356567, 0.8836692988872528, 0.874508398771286, 0.8848534762859345, 0.8828425168991089, 0.878298431634903, 0.8819989085197448, 0.8825878083705903, 0.881676435470581, 0.8775389850139618, 0.8821341872215271, 0.882125335931778, 0.8782861351966857, 0.8749350249767304, 0.8823159813880921, 0.8807323217391968, 0.8668983519077301, 0.8779160380363464, 0.8795972347259522, 0.880800986289978, 0.8802692651748657, 0.8755972385406494, 0.8744750857353211, 0.8813555002212524, 0.8772473812103272, 0.8797485291957855, 0.8843286097049713, 0.8732373893260956, 0.8847410142421722, 0.8763103544712066, 0.8830194056034089, 0.8812418699264526, 0.8829344809055328, 0.8865955591201782, 0.8795995473861694, 0.8829135060310364, 0.8804453790187836, 0.8841530561447144, 0.8864717066287995, 0.8786957561969757, 0.8821743309497834, 0.8804700314998627, 0.8756976425647736, 0.8773485362529755, 0.8802189588546753, 0.8802459001541137, 0.8753807365894317, 0.8716443002223968, 0.8814588189125061, 0.8839204370975494, 0.8834569394588471, 0.8806455075740814, 0.8798628091812134, 0.8776897192001343, 0.8802677869796753, 0.8788132548332215, 0.8806002199649811, 0.8821847081184387, 0.8814376413822174, 0.885299128293991, 0.8843920111656189, 0.87825146317482, 0.8794587194919586, 0.8805196583271027, 0.8787685930728912, 0.8784985244274139, 0.8775160610675812, 0.879644364118576, 0.8815895736217498, 0.8800211727619172, 0.8802246809005737, 0.8815389573574066, 0.8811008095741272, 0.8828328073024749], 'loss': [0.4285822457486795, 0.18388692468620937, 0.13659621445947884, 0.11729727035778556, 0.10694995081752265, 0.1000453001333807, 0.09479907644341501, 0.0905644017820335, 0.08784304159348105, 0.08398845698082141, 0.0815512112857581, 0.07923901699504425, 0.07739643574662926, 0.07589489460448282, 0.07450308196993892, 0.06943153076779784, 0.06667000414997229, 0.06565957599171668, 0.06437217576810136, 0.06413788551039863, 0.06312345498567498, 0.062343190391100235, 0.06169907516159718, 0.06104511767945268, 0.06066601936178869, 0.06016560302955522, 0.059597498806990855, 0.0592031506696059, 0.05886936275022979, 0.05848549210754656, 0.05804510242811339, 0.057861745551185395, 0.05742779797405685, 0.05690161564811363, 0.05669684474806943, 0.05641983026868171, 0.05596039582440681, 0.05590819092047139, 0.05576299981968462, 0.05542148285482748, 0.05508118775617073, 0.05502650522031844, 0.05462410170417867, 0.0545590640828277, 0.05424337681378502, 0.054015876482194504, 0.053971627221939954, 0.053856243904557954, 0.053518463457382925, 0.05367889193949151, 0.05320211780094899, 0.053046548596983, 0.05300507758055128, 0.05294600422483964, 0.05262110915947996, 0.05236369853504312, 0.052287082288294574, 0.052148917284539416, 0.052082572699921025, 0.05204945551194566, 0.05183038863021959, 0.051771103096018095, 0.05167529703783347, 0.05156407013413573, 0.05134823543896807, 0.05114169437233929, 0.05118954819950059, 0.05123576435579634, 0.05127330886287722, 0.05083349585470848, 0.05108066420790109, 0.050517649069579125, 0.05068496778124453, 0.0506797074855245, 0.05042550189323878, 0.05050965075843284, 0.05028240438816261, 0.05009539016625393, 0.050136677851202904, 0.0501259330004016, 0.050042386994782244, 0.050115184268573774, 0.05001011553023152, 0.04987448784170476, 0.049695444440437664, 0.04956450699528906, 0.049450523160470154, 0.04930910747667987, 0.049524141567469274, 0.049543413258488676, 0.049387596995112304, 0.049146028406797986, 0.049147331106486836, 0.04914603471571228, 0.04898861617405758, 0.04921719463210732, 0.048662654315443095, 0.04887033503006248, 0.04869015512853998, 0.048791176462563625, 0.048690605430641874, 0.04859617456855547, 0.04835634117113239, 0.0487056304624302, 0.04864032559279703, 0.04837833215648685, 0.04838764188908285, 0.04836824091237411, 0.04845941564046302, 0.04837773840189061, 0.048259839484367004, 0.04827379468883946, 0.04797861885353824, 0.04795048464259078, 0.048133741642418716, 0.04800896659678733, 0.04789565790577854, 0.047997924041023085, 0.04815711672053603, 0.047863220063696026], 'acc': [0.9476714321280801, 0.9808311430051804, 0.9843068108615902, 0.9861656010042293, 0.9873276506882881, 0.9881748875229246, 0.9887190462798642, 0.9892846764413968, 0.9896269936025234, 0.990104018608959, 0.9904636047479957, 0.9907473176776326, 0.9909992319733297, 0.9912475235154571, 0.9915295834432619, 0.9925106247615408, 0.9929287338446807, 0.9930277082358423, 0.9931455873176294, 0.9931769702375504, 0.9932697141873738, 0.9933298319044674, 0.9933851928569865, 0.9934471981056114, 0.9934847563240021, 0.9935248541108715, 0.9935843396698815, 0.9936162230733098, 0.9936508225249713, 0.9936849509557565, 0.9937271682141094, 0.993741645547028, 0.9937780443808569, 0.9938245060087192, 0.9938616116241051, 0.9938741972363212, 0.9939064350304345, 0.9939192261356639, 0.9939297157310701, 0.9939627742385687, 0.993989956128907, 0.9940090372204871, 0.9940236806209292, 0.9940295669709379, 0.9940649443774735, 0.9940867242047431, 0.994089818206951, 0.9941044487374582, 0.9941278649845124, 0.9941208814315552, 0.99415366158811, 0.9941635348982812, 0.9941694601285699, 0.9941852839254518, 0.9942059351086263, 0.9942297677938061, 0.9942391928562624, 0.9942443903037648, 0.9942556701042297, 0.9942612288299513, 0.9942816445207904, 0.9942840016548004, 0.99429351323738, 0.9942986176392782, 0.9943141497147958, 0.9943404871240052, 0.9943334862137801, 0.9943354587128646, 0.9943243336157046, 0.9943682216654919, 0.9943389889809481, 0.9943992541496294, 0.9943759307202987, 0.9943734626126501, 0.9943940140437129, 0.9943930433763118, 0.9944153540314264, 0.9944164064441049, 0.9944301796682727, 0.9944273850672216, 0.9944257925922056, 0.9944313813172494, 0.9944405199911756, 0.994445755113533, 0.9944684020779577, 0.9944674965226732, 0.9944763851965697, 0.9944931350281824, 0.9944788921802388, 0.9944713958701626, 0.9944872328138439, 0.9945129385015634, 0.9945122860706167, 0.9944979457152926, 0.9945199860251039, 0.9945051032264086, 0.9945529313440487, 0.9945319523044469, 0.9945469281317085, 0.9945424716244404, 0.9945525346633922, 0.9945562649681309, 0.9945768806274738, 0.9945457967573046, 0.9945574491725209, 0.9945761292047276, 0.9945768612597445, 0.9945748098181022, 0.9945648157056064, 0.9945640973873229, 0.9945877472227374, 0.9945777828902018, 0.9946172548251064, 0.9946120955295177, 0.99459344754117, 0.994608885951333, 0.9946212624393527, 0.994613492955697, 0.9946078766336625, 0.9946213670152941], 'mDice': [0.4958872156051376, 0.7039853170459266, 0.77268980850841, 0.8028736488865209, 0.8195650041535697, 0.8309004431573866, 0.8396158941319318, 0.8467402164373903, 0.8513417174145097, 0.8579073400364883, 0.862071825423186, 0.8660575693096724, 0.8692360327217993, 0.8718442571045191, 0.874264465796499, 0.8766135710690589, 0.8785792527532051, 0.880258836282617, 0.882432015067312, 0.8828191916889012, 0.8845560836072996, 0.8859016438467938, 0.8870002770916282, 0.8881354088968465, 0.8887863059934451, 0.8896509243002547, 0.8906304545288715, 0.891313923384065, 0.8918904642581378, 0.8925561268033929, 0.8933200380867494, 0.8936467343989849, 0.8943937948527271, 0.895313669056926, 0.8956634123579126, 0.8961504835912422, 0.8969561699272697, 0.8970459946828048, 0.8972951692136387, 0.8978938379612119, 0.8984973639793946, 0.8985883782189107, 0.8992978219677237, 0.8994114192607136, 0.8999652031210239, 0.9003649089831088, 0.9004334348567741, 0.9006433714968252, 0.9012350898725741, 0.9009535749970549, 0.9017919063925909, 0.9020682394655758, 0.9021380983305569, 0.902249728282659, 0.9028150310712122, 0.9032675536958318, 0.9033987441302138, 0.9036499436323242, 0.9037611067995754, 0.903824212539446, 0.9042063849137673, 0.9043114134517987, 0.9044864123651408, 0.9046801041103437, 0.905062991229268, 0.9054230950365718, 0.905339620566323, 0.9052592903193534, 0.9051979211475082, 0.9059676619533915, 0.905533002755835, 0.9065259863578328, 0.9062303813144351, 0.9062443893196098, 0.9066913947652049, 0.9065450199799475, 0.9069463459992283, 0.9072773945494711, 0.9071999790030677, 0.9072232106518514, 0.907372364416619, 0.9072422513212299, 0.90742707675892, 0.9076753798143467, 0.9079837621389216, 0.9082188692345736, 0.9084174691508402, 0.9086699371604191, 0.908289818725909, 0.9082624077409326, 0.9085294986012264, 0.9089530684341677, 0.9089532730901987, 0.9089603153085914, 0.9092361252761183, 0.908834685506036, 0.9098139743408462, 0.9094447521133114, 0.9097682337743547, 0.9095916864841055, 0.9097611527438332, 0.9099373742008948, 0.9103586314707344, 0.909746602250563, 0.9098527447695253, 0.9103187076462833, 0.9103048874467465, 0.9103409734387172, 0.9101804539901496, 0.9103256546978646, 0.9105318995663116, 0.9105089912482872, 0.9110300405942031, 0.9110797666257893, 0.9107574671906206, 0.9109800478101616, 0.9111759147639102, 0.9109955702571874, 0.9107111579212349, 0.9112345142674079]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:03,  1.05it/s]predicting test subjects:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]predicting test subjects:  60%|██████    | 3/5 [00:01<00:01,  1.59it/s]predicting test subjects:  80%|████████  | 4/5 [00:01<00:00,  1.86it/s]predicting test subjects: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:50,  2.41it/s]predicting train subjects:   1%|          | 2/266 [00:00<01:46,  2.47it/s]predicting train subjects:   1%|          | 3/266 [00:01<01:36,  2.71it/s]predicting train subjects:   2%|▏         | 4/266 [00:01<01:31,  2.88it/s]predicting train subjects:   2%|▏         | 5/266 [00:01<01:31,  2.84it/s]predicting train subjects:   2%|▏         | 6/266 [00:02<01:27,  2.97it/s]predicting train subjects:   3%|▎         | 7/266 [00:02<01:24,  3.08it/s]predicting train subjects:   3%|▎         | 8/266 [00:02<01:26,  2.97it/s]predicting train subjects:   3%|▎         | 9/266 [00:03<01:27,  2.95it/s]predicting train subjects:   4%|▍         | 10/266 [00:03<01:27,  2.94it/s]predicting train subjects:   4%|▍         | 11/266 [00:03<01:28,  2.87it/s]predicting train subjects:   5%|▍         | 12/266 [00:04<01:24,  3.02it/s]predicting train subjects:   5%|▍         | 13/266 [00:04<01:20,  3.13it/s]predicting train subjects:   5%|▌         | 14/266 [00:04<01:19,  3.19it/s]predicting train subjects:   6%|▌         | 15/266 [00:04<01:17,  3.24it/s]predicting train subjects:   6%|▌         | 16/266 [00:05<01:15,  3.29it/s]predicting train subjects:   6%|▋         | 17/266 [00:05<01:17,  3.20it/s]predicting train subjects:   7%|▋         | 18/266 [00:05<01:21,  3.06it/s]predicting train subjects:   7%|▋         | 19/266 [00:06<01:18,  3.16it/s]predicting train subjects:   8%|▊         | 20/266 [00:06<01:16,  3.20it/s]predicting train subjects:   8%|▊         | 21/266 [00:06<01:16,  3.19it/s]predicting train subjects:   8%|▊         | 22/266 [00:07<01:15,  3.24it/s]predicting train subjects:   9%|▊         | 23/266 [00:07<01:14,  3.25it/s]predicting train subjects:   9%|▉         | 24/266 [00:07<01:16,  3.17it/s]predicting train subjects:   9%|▉         | 25/266 [00:08<01:22,  2.92it/s]predicting train subjects:  10%|▉         | 26/266 [00:08<01:22,  2.92it/s]predicting train subjects:  10%|█         | 27/266 [00:08<01:17,  3.07it/s]predicting train subjects:  11%|█         | 28/266 [00:09<01:18,  3.01it/s]predicting train subjects:  11%|█         | 29/266 [00:09<01:14,  3.17it/s]predicting train subjects:  11%|█▏        | 30/266 [00:09<01:11,  3.28it/s]predicting train subjects:  12%|█▏        | 31/266 [00:10<01:11,  3.28it/s]predicting train subjects:  12%|█▏        | 32/266 [00:10<01:13,  3.20it/s]predicting train subjects:  12%|█▏        | 33/266 [00:10<01:13,  3.15it/s]predicting train subjects:  13%|█▎        | 34/266 [00:11<01:15,  3.09it/s]predicting train subjects:  13%|█▎        | 35/266 [00:11<01:11,  3.23it/s]predicting train subjects:  14%|█▎        | 36/266 [00:11<01:09,  3.32it/s]predicting train subjects:  14%|█▍        | 37/266 [00:11<01:08,  3.34it/s]predicting train subjects:  14%|█▍        | 38/266 [00:12<01:10,  3.24it/s]predicting train subjects:  15%|█▍        | 39/266 [00:12<01:12,  3.13it/s]predicting train subjects:  15%|█▌        | 40/266 [00:12<01:15,  3.00it/s]predicting train subjects:  15%|█▌        | 41/266 [00:13<01:13,  3.06it/s]predicting train subjects:  16%|█▌        | 42/266 [00:13<01:10,  3.17it/s]predicting train subjects:  16%|█▌        | 43/266 [00:13<01:05,  3.41it/s]predicting train subjects:  17%|█▋        | 44/266 [00:14<01:02,  3.53it/s]predicting train subjects:  17%|█▋        | 45/266 [00:14<01:02,  3.54it/s]predicting train subjects:  17%|█▋        | 46/266 [00:14<01:04,  3.42it/s]predicting train subjects:  18%|█▊        | 47/266 [00:14<01:04,  3.41it/s]predicting train subjects:  18%|█▊        | 48/266 [00:15<01:02,  3.47it/s]predicting train subjects:  18%|█▊        | 49/266 [00:15<01:00,  3.62it/s]predicting train subjects:  19%|█▉        | 50/266 [00:15<00:58,  3.71it/s]predicting train subjects:  19%|█▉        | 51/266 [00:15<00:56,  3.81it/s]predicting train subjects:  20%|█▉        | 52/266 [00:16<00:55,  3.87it/s]predicting train subjects:  20%|█▉        | 53/266 [00:16<00:55,  3.85it/s]predicting train subjects:  20%|██        | 54/266 [00:16<00:59,  3.57it/s]predicting train subjects:  21%|██        | 55/266 [00:17<00:56,  3.74it/s]predicting train subjects:  21%|██        | 56/266 [00:17<00:54,  3.88it/s]predicting train subjects:  21%|██▏       | 57/266 [00:17<00:52,  3.98it/s]predicting train subjects:  22%|██▏       | 58/266 [00:17<00:52,  3.98it/s]predicting train subjects:  22%|██▏       | 59/266 [00:17<00:51,  4.03it/s]predicting train subjects:  23%|██▎       | 60/266 [00:18<00:53,  3.85it/s]predicting train subjects:  23%|██▎       | 61/266 [00:18<00:51,  3.98it/s]predicting train subjects:  23%|██▎       | 62/266 [00:18<00:50,  4.05it/s]predicting train subjects:  24%|██▎       | 63/266 [00:19<00:52,  3.85it/s]predicting train subjects:  24%|██▍       | 64/266 [00:19<00:53,  3.78it/s]predicting train subjects:  24%|██▍       | 65/266 [00:19<00:51,  3.88it/s]predicting train subjects:  25%|██▍       | 66/266 [00:19<00:49,  4.02it/s]predicting train subjects:  25%|██▌       | 67/266 [00:19<00:48,  4.10it/s]predicting train subjects:  26%|██▌       | 68/266 [00:20<00:49,  4.04it/s]predicting train subjects:  26%|██▌       | 69/266 [00:20<00:47,  4.16it/s]predicting train subjects:  26%|██▋       | 70/266 [00:20<00:46,  4.23it/s]predicting train subjects:  27%|██▋       | 71/266 [00:21<00:50,  3.89it/s]predicting train subjects:  27%|██▋       | 72/266 [00:21<00:48,  4.01it/s]predicting train subjects:  27%|██▋       | 73/266 [00:21<00:46,  4.12it/s]predicting train subjects:  28%|██▊       | 74/266 [00:21<00:45,  4.24it/s]predicting train subjects:  28%|██▊       | 75/266 [00:21<00:44,  4.29it/s]predicting train subjects:  29%|██▊       | 76/266 [00:22<00:44,  4.28it/s]predicting train subjects:  29%|██▉       | 77/266 [00:22<00:43,  4.32it/s]predicting train subjects:  29%|██▉       | 78/266 [00:22<00:50,  3.69it/s]predicting train subjects:  30%|██▉       | 79/266 [00:23<00:52,  3.54it/s]predicting train subjects:  30%|███       | 80/266 [00:23<00:54,  3.40it/s]predicting train subjects:  30%|███       | 81/266 [00:23<00:55,  3.36it/s]predicting train subjects:  31%|███       | 82/266 [00:24<00:56,  3.24it/s]predicting train subjects:  31%|███       | 83/266 [00:24<00:59,  3.09it/s]predicting train subjects:  32%|███▏      | 84/266 [00:24<00:57,  3.19it/s]predicting train subjects:  32%|███▏      | 85/266 [00:24<00:56,  3.21it/s]predicting train subjects:  32%|███▏      | 86/266 [00:25<00:54,  3.28it/s]predicting train subjects:  33%|███▎      | 87/266 [00:25<00:54,  3.26it/s]predicting train subjects:  33%|███▎      | 88/266 [00:25<00:59,  2.97it/s]predicting train subjects:  33%|███▎      | 89/266 [00:26<00:57,  3.07it/s]predicting train subjects:  34%|███▍      | 90/266 [00:26<00:55,  3.18it/s]predicting train subjects:  34%|███▍      | 91/266 [00:26<00:53,  3.25it/s]predicting train subjects:  35%|███▍      | 92/266 [00:27<00:52,  3.30it/s]predicting train subjects:  35%|███▍      | 93/266 [00:27<00:51,  3.37it/s]predicting train subjects:  35%|███▌      | 94/266 [00:27<00:53,  3.24it/s]predicting train subjects:  36%|███▌      | 95/266 [00:28<00:53,  3.21it/s]predicting train subjects:  36%|███▌      | 96/266 [00:28<00:57,  2.93it/s]predicting train subjects:  36%|███▋      | 97/266 [00:28<00:58,  2.89it/s]predicting train subjects:  37%|███▋      | 98/266 [00:29<00:57,  2.90it/s]predicting train subjects:  37%|███▋      | 99/266 [00:29<00:52,  3.19it/s]predicting train subjects:  38%|███▊      | 100/266 [00:29<00:51,  3.20it/s]predicting train subjects:  38%|███▊      | 101/266 [00:29<00:48,  3.41it/s]predicting train subjects:  38%|███▊      | 102/266 [00:30<00:50,  3.25it/s]predicting train subjects:  39%|███▊      | 103/266 [00:30<00:51,  3.14it/s]predicting train subjects:  39%|███▉      | 104/266 [00:30<00:50,  3.19it/s]predicting train subjects:  39%|███▉      | 105/266 [00:31<00:48,  3.33it/s]predicting train subjects:  40%|███▉      | 106/266 [00:31<00:45,  3.51it/s]predicting train subjects:  40%|████      | 107/266 [00:31<00:43,  3.62it/s]predicting train subjects:  41%|████      | 108/266 [00:31<00:42,  3.72it/s]predicting train subjects:  41%|████      | 109/266 [00:32<00:41,  3.80it/s]predicting train subjects:  41%|████▏     | 110/266 [00:32<00:40,  3.82it/s]predicting train subjects:  42%|████▏     | 111/266 [00:32<00:39,  3.89it/s]predicting train subjects:  42%|████▏     | 112/266 [00:33<00:40,  3.82it/s]predicting train subjects:  42%|████▏     | 113/266 [00:33<00:43,  3.53it/s]predicting train subjects:  43%|████▎     | 114/266 [00:33<00:43,  3.46it/s]predicting train subjects:  43%|████▎     | 115/266 [00:33<00:44,  3.41it/s]predicting train subjects:  44%|████▎     | 116/266 [00:34<00:44,  3.37it/s]predicting train subjects:  44%|████▍     | 117/266 [00:34<00:42,  3.51it/s]predicting train subjects:  44%|████▍     | 118/266 [00:34<00:42,  3.45it/s]predicting train subjects:  45%|████▍     | 119/266 [00:35<00:43,  3.41it/s]predicting train subjects:  45%|████▌     | 120/266 [00:35<00:42,  3.41it/s]predicting train subjects:  45%|████▌     | 121/266 [00:35<00:46,  3.15it/s]predicting train subjects:  46%|████▌     | 122/266 [00:36<00:44,  3.26it/s]predicting train subjects:  46%|████▌     | 123/266 [00:36<00:43,  3.32it/s]predicting train subjects:  47%|████▋     | 124/266 [00:36<00:42,  3.37it/s]predicting train subjects:  47%|████▋     | 125/266 [00:36<00:41,  3.40it/s]predicting train subjects:  47%|████▋     | 126/266 [00:37<00:40,  3.42it/s]predicting train subjects:  48%|████▊     | 127/266 [00:37<00:40,  3.42it/s]predicting train subjects:  48%|████▊     | 128/266 [00:37<00:40,  3.44it/s]predicting train subjects:  48%|████▊     | 129/266 [00:38<00:40,  3.42it/s]predicting train subjects:  49%|████▉     | 130/266 [00:38<00:42,  3.19it/s]predicting train subjects:  49%|████▉     | 131/266 [00:38<00:41,  3.23it/s]predicting train subjects:  50%|████▉     | 132/266 [00:39<00:42,  3.13it/s]predicting train subjects:  50%|█████     | 133/266 [00:39<00:42,  3.14it/s]predicting train subjects:  50%|█████     | 134/266 [00:39<00:42,  3.07it/s]predicting train subjects:  51%|█████     | 135/266 [00:40<00:43,  3.02it/s]predicting train subjects:  51%|█████     | 136/266 [00:40<00:42,  3.07it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:40<00:44,  2.93it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:41<00:41,  3.11it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:41<00:39,  3.24it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:41<00:38,  3.29it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:41<00:38,  3.22it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:42<00:37,  3.32it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:42<00:35,  3.43it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:42<00:34,  3.50it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:43<00:34,  3.47it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:43<00:37,  3.20it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:43<00:36,  3.28it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:44<00:34,  3.37it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:44<00:34,  3.44it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:44<00:34,  3.40it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:44<00:35,  3.22it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:45<00:34,  3.30it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:45<00:34,  3.26it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:45<00:38,  2.93it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:46<00:34,  3.23it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:46<00:30,  3.56it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:46<00:29,  3.65it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:46<00:27,  3.88it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:47<00:26,  4.07it/s]predicting train subjects:  60%|██████    | 160/266 [00:47<00:25,  4.18it/s]predicting train subjects:  61%|██████    | 161/266 [00:47<00:24,  4.36it/s]predicting train subjects:  61%|██████    | 162/266 [00:47<00:23,  4.45it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:47<00:23,  4.43it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:48<00:22,  4.46it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:48<00:22,  4.48it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:48<00:22,  4.54it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:48<00:21,  4.56it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:49<00:21,  4.57it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:49<00:21,  4.58it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:49<00:20,  4.62it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:49<00:20,  4.62it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:49<00:20,  4.64it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:50<00:20,  4.46it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:50<00:21,  4.29it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:50<00:23,  3.92it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:50<00:22,  4.00it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:51<00:21,  4.06it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:51<00:21,  4.12it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:51<00:20,  4.14it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:51<00:20,  4.12it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:52<00:20,  4.16it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:52<00:20,  4.15it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:52<00:19,  4.19it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:52<00:20,  4.10it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:53<00:22,  3.60it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:53<00:21,  3.73it/s]predicting train subjects:  70%|███████   | 187/266 [00:53<00:20,  3.86it/s]predicting train subjects:  71%|███████   | 188/266 [00:53<00:19,  3.95it/s]predicting train subjects:  71%|███████   | 189/266 [00:54<00:19,  4.02it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:54<00:18,  4.02it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:54<00:20,  3.73it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:55<00:20,  3.53it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:55<00:20,  3.50it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:55<00:23,  3.10it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:56<00:21,  3.25it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:56<00:21,  3.33it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:56<00:20,  3.34it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:56<00:20,  3.37it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:57<00:19,  3.47it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:57<00:18,  3.65it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:57<00:17,  3.81it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:57<00:16,  3.79it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:58<00:19,  3.31it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:58<00:18,  3.43it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:58<00:16,  3.61it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:59<00:16,  3.65it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:59<00:15,  3.79it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:59<00:14,  3.88it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:59<00:14,  3.96it/s]predicting train subjects:  79%|███████▉  | 210/266 [01:00<00:13,  4.02it/s]predicting train subjects:  79%|███████▉  | 211/266 [01:00<00:13,  4.05it/s]predicting train subjects:  80%|███████▉  | 212/266 [01:00<00:13,  4.05it/s]predicting train subjects:  80%|████████  | 213/266 [01:00<00:14,  3.63it/s]predicting train subjects:  80%|████████  | 214/266 [01:01<00:13,  3.88it/s]predicting train subjects:  81%|████████  | 215/266 [01:01<00:12,  4.09it/s]predicting train subjects:  81%|████████  | 216/266 [01:01<00:11,  4.22it/s]predicting train subjects:  82%|████████▏ | 217/266 [01:01<00:11,  4.39it/s]predicting train subjects:  82%|████████▏ | 218/266 [01:01<00:10,  4.48it/s]predicting train subjects:  82%|████████▏ | 219/266 [01:02<00:10,  4.56it/s]predicting train subjects:  83%|████████▎ | 220/266 [01:02<00:10,  4.57it/s]predicting train subjects:  83%|████████▎ | 221/266 [01:02<00:09,  4.57it/s]predicting train subjects:  83%|████████▎ | 222/266 [01:02<00:09,  4.61it/s]predicting train subjects:  84%|████████▍ | 223/266 [01:03<00:10,  4.22it/s]predicting train subjects:  84%|████████▍ | 224/266 [01:03<00:09,  4.33it/s]predicting train subjects:  85%|████████▍ | 225/266 [01:03<00:09,  4.40it/s]predicting train subjects:  85%|████████▍ | 226/266 [01:03<00:08,  4.46it/s]predicting train subjects:  85%|████████▌ | 227/266 [01:04<00:09,  4.25it/s]predicting train subjects:  86%|████████▌ | 228/266 [01:04<00:08,  4.31it/s]predicting train subjects:  86%|████████▌ | 229/266 [01:04<00:08,  4.41it/s]predicting train subjects:  86%|████████▋ | 230/266 [01:04<00:08,  4.50it/s]predicting train subjects:  87%|████████▋ | 231/266 [01:04<00:07,  4.40it/s]predicting train subjects:  87%|████████▋ | 232/266 [01:05<00:07,  4.41it/s]predicting train subjects:  88%|████████▊ | 233/266 [01:05<00:07,  4.45it/s]predicting train subjects:  88%|████████▊ | 234/266 [01:05<00:07,  4.44it/s]predicting train subjects:  88%|████████▊ | 235/266 [01:05<00:07,  4.42it/s]predicting train subjects:  89%|████████▊ | 236/266 [01:06<00:07,  4.17it/s]predicting train subjects:  89%|████████▉ | 237/266 [01:06<00:07,  3.98it/s]predicting train subjects:  89%|████████▉ | 238/266 [01:06<00:06,  4.08it/s]predicting train subjects:  90%|████████▉ | 239/266 [01:06<00:06,  4.16it/s]predicting train subjects:  90%|█████████ | 240/266 [01:07<00:06,  4.24it/s]predicting train subjects:  91%|█████████ | 241/266 [01:07<00:05,  4.23it/s]predicting train subjects:  91%|█████████ | 242/266 [01:07<00:05,  4.30it/s]predicting train subjects:  91%|█████████▏| 243/266 [01:07<00:05,  4.34it/s]predicting train subjects:  92%|█████████▏| 244/266 [01:08<00:05,  4.02it/s]predicting train subjects:  92%|█████████▏| 245/266 [01:08<00:06,  3.47it/s]predicting train subjects:  92%|█████████▏| 246/266 [01:08<00:05,  3.71it/s]predicting train subjects:  93%|█████████▎| 247/266 [01:08<00:04,  3.89it/s]predicting train subjects:  93%|█████████▎| 248/266 [01:09<00:04,  4.01it/s]predicting train subjects:  94%|█████████▎| 249/266 [01:09<00:04,  3.56it/s]predicting train subjects:  94%|█████████▍| 250/266 [01:09<00:04,  3.50it/s]predicting train subjects:  94%|█████████▍| 251/266 [01:10<00:04,  3.26it/s]predicting train subjects:  95%|█████████▍| 252/266 [01:10<00:04,  3.18it/s]predicting train subjects:  95%|█████████▌| 253/266 [01:10<00:04,  3.08it/s]predicting train subjects:  95%|█████████▌| 254/266 [01:11<00:03,  3.16it/s]predicting train subjects:  96%|█████████▌| 255/266 [01:11<00:03,  3.25it/s]predicting train subjects:  96%|█████████▌| 256/266 [01:11<00:03,  3.28it/s]predicting train subjects:  97%|█████████▋| 257/266 [01:12<00:02,  3.22it/s]predicting train subjects:  97%|█████████▋| 258/266 [01:12<00:02,  3.33it/s]predicting train subjects:  97%|█████████▋| 259/266 [01:12<00:02,  3.25it/s]predicting train subjects:  98%|█████████▊| 260/266 [01:12<00:01,  3.24it/s]predicting train subjects:  98%|█████████▊| 261/266 [01:13<00:01,  3.12it/s]predicting train subjects:  98%|█████████▊| 262/266 [01:13<00:01,  3.19it/s]predicting train subjects:  99%|█████████▉| 263/266 [01:13<00:00,  3.31it/s]predicting train subjects:  99%|█████████▉| 264/266 [01:14<00:00,  3.39it/s]predicting train subjects: 100%|█████████▉| 265/266 [01:14<00:00,  3.45it/s]predicting train subjects: 100%|██████████| 266/266 [01:14<00:00,  3.49it/s]
saving BB  test1-THALAMUS:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 5/5 [00:00<00:00, 52.91it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   2%|▏         | 6/266 [00:00<00:04, 56.91it/s]saving BB  train1-THALAMUS:   5%|▍         | 12/266 [00:00<00:04, 56.46it/s]saving BB  train1-THALAMUS:   7%|▋         | 18/266 [00:00<00:04, 55.89it/s]saving BB  train1-THALAMUS:   9%|▉         | 24/266 [00:00<00:04, 54.57it/s]saving BB  train1-THALAMUS:  12%|█▏        | 31/266 [00:00<00:04, 57.28it/s]saving BB  train1-THALAMUS:  14%|█▍        | 38/266 [00:00<00:03, 59.97it/s]saving BB  train1-THALAMUS:  17%|█▋        | 46/266 [00:00<00:03, 62.86it/s]saving BB  train1-THALAMUS:  20%|██        | 54/266 [00:00<00:03, 65.61it/s]saving BB  train1-THALAMUS:  23%|██▎       | 62/266 [00:00<00:02, 68.73it/s]saving BB  train1-THALAMUS:  27%|██▋       | 71/266 [00:01<00:02, 72.06it/s]saving BB  train1-THALAMUS:  30%|██▉       | 79/266 [00:01<00:02, 73.52it/s]saving BB  train1-THALAMUS:  33%|███▎      | 87/266 [00:01<00:02, 71.54it/s]saving BB  train1-THALAMUS:  36%|███▌      | 95/266 [00:01<00:02, 70.85it/s]saving BB  train1-THALAMUS:  39%|███▊      | 103/266 [00:01<00:02, 69.49it/s]saving BB  train1-THALAMUS:  42%|████▏     | 111/266 [00:01<00:02, 69.87it/s]saving BB  train1-THALAMUS:  45%|████▍     | 119/266 [00:01<00:02, 70.07it/s]saving BB  train1-THALAMUS:  48%|████▊     | 127/266 [00:01<00:02, 69.11it/s]saving BB  train1-THALAMUS:  50%|█████     | 134/266 [00:01<00:01, 68.51it/s]saving BB  train1-THALAMUS:  53%|█████▎    | 141/266 [00:02<00:01, 68.67it/s]saving BB  train1-THALAMUS:  56%|█████▌    | 148/266 [00:02<00:01, 68.85it/s]saving BB  train1-THALAMUS:  58%|█████▊    | 155/266 [00:02<00:01, 69.14it/s]saving BB  train1-THALAMUS:  61%|██████▏   | 163/266 [00:02<00:01, 71.56it/s]saving BB  train1-THALAMUS:  65%|██████▍   | 172/266 [00:02<00:01, 73.50it/s]saving BB  train1-THALAMUS:  68%|██████▊   | 181/266 [00:02<00:01, 75.43it/s]saving BB  train1-THALAMUS:  71%|███████▏  | 190/266 [00:02<00:00, 77.15it/s]saving BB  train1-THALAMUS:  74%|███████▍  | 198/266 [00:02<00:00, 74.10it/s]saving BB  train1-THALAMUS:  77%|███████▋  | 206/266 [00:02<00:00, 71.81it/s]saving BB  train1-THALAMUS:  80%|████████  | 214/266 [00:03<00:00, 71.20it/s]saving BB  train1-THALAMUS:  83%|████████▎ | 222/266 [00:03<00:00, 73.40it/s]saving BB  train1-THALAMUS:  86%|████████▋ | 230/266 [00:03<00:00, 73.78it/s]saving BB  train1-THALAMUS:  89%|████████▉ | 238/266 [00:03<00:00, 74.34it/s]saving BB  train1-THALAMUS:  92%|█████████▏| 246/266 [00:03<00:00, 75.93it/s]saving BB  train1-THALAMUS:  95%|█████████▌| 254/266 [00:03<00:00, 73.49it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 262/266 [00:03<00:00, 71.80it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 69.96it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:02<08:59,  2.03s/it]Loading train:   1%|          | 2/266 [00:03<08:38,  1.96s/it]Loading train:   1%|          | 3/266 [00:05<08:12,  1.87s/it]Loading train:   2%|▏         | 4/266 [00:06<07:18,  1.67s/it]Loading train:   2%|▏         | 5/266 [00:08<07:06,  1.63s/it]Loading train:   2%|▏         | 6/266 [00:09<06:35,  1.52s/it]Loading train:   3%|▎         | 7/266 [00:10<06:07,  1.42s/it]Loading train:   3%|▎         | 8/266 [00:12<06:03,  1.41s/it]Loading train:   3%|▎         | 9/266 [00:13<05:56,  1.39s/it]Loading train:   4%|▍         | 10/266 [00:14<05:43,  1.34s/it]Loading train:   4%|▍         | 11/266 [00:15<05:28,  1.29s/it]Loading train:   5%|▍         | 12/266 [00:16<05:18,  1.25s/it]Loading train:   5%|▍         | 13/266 [00:18<05:08,  1.22s/it]Loading train:   5%|▌         | 14/266 [00:19<05:03,  1.21s/it]Loading train:   6%|▌         | 15/266 [00:20<05:09,  1.23s/it]Loading train:   6%|▌         | 16/266 [00:21<05:05,  1.22s/it]Loading train:   6%|▋         | 17/266 [00:23<05:16,  1.27s/it]Loading train:   7%|▋         | 18/266 [00:24<05:03,  1.22s/it]Loading train:   7%|▋         | 19/266 [00:25<05:07,  1.24s/it]Loading train:   8%|▊         | 20/266 [00:26<05:15,  1.28s/it]Loading train:   8%|▊         | 21/266 [00:28<05:15,  1.29s/it]Loading train:   8%|▊         | 22/266 [00:29<05:06,  1.26s/it]Loading train:   9%|▊         | 23/266 [00:30<05:19,  1.32s/it]Loading train:   9%|▉         | 24/266 [00:31<04:54,  1.22s/it]Loading train:   9%|▉         | 25/266 [00:32<04:38,  1.15s/it]Loading train:  10%|▉         | 26/266 [00:33<04:28,  1.12s/it]Loading train:  10%|█         | 27/266 [00:35<04:25,  1.11s/it]Loading train:  11%|█         | 28/266 [00:36<04:18,  1.09s/it]Loading train:  11%|█         | 29/266 [00:37<04:18,  1.09s/it]Loading train:  11%|█▏        | 30/266 [00:38<04:16,  1.08s/it]Loading train:  12%|█▏        | 31/266 [00:39<04:10,  1.07s/it]Loading train:  12%|█▏        | 32/266 [00:40<04:02,  1.04s/it]Loading train:  12%|█▏        | 33/266 [00:41<04:01,  1.04s/it]Loading train:  13%|█▎        | 34/266 [00:42<03:56,  1.02s/it]Loading train:  13%|█▎        | 35/266 [00:43<03:53,  1.01s/it]Loading train:  14%|█▎        | 36/266 [00:44<03:53,  1.02s/it]Loading train:  14%|█▍        | 37/266 [00:45<03:56,  1.03s/it]Loading train:  14%|█▍        | 38/266 [00:46<03:52,  1.02s/it]Loading train:  15%|█▍        | 39/266 [00:47<03:45,  1.01it/s]Loading train:  15%|█▌        | 40/266 [00:48<03:50,  1.02s/it]Loading train:  15%|█▌        | 41/266 [00:49<03:45,  1.00s/it]Loading train:  16%|█▌        | 42/266 [00:50<04:00,  1.07s/it]Loading train:  16%|█▌        | 43/266 [00:51<03:51,  1.04s/it]Loading train:  17%|█▋        | 44/266 [00:52<03:54,  1.05s/it]Loading train:  17%|█▋        | 45/266 [00:53<03:50,  1.04s/it]Loading train:  17%|█▋        | 46/266 [00:54<03:40,  1.00s/it]Loading train:  18%|█▊        | 47/266 [00:55<03:45,  1.03s/it]Loading train:  18%|█▊        | 48/266 [00:56<03:47,  1.05s/it]Loading train:  18%|█▊        | 49/266 [00:57<03:43,  1.03s/it]Loading train:  19%|█▉        | 50/266 [00:58<03:43,  1.03s/it]Loading train:  19%|█▉        | 51/266 [00:59<03:37,  1.01s/it]Loading train:  20%|█▉        | 52/266 [01:00<03:28,  1.02it/s]Loading train:  20%|█▉        | 53/266 [01:01<03:25,  1.04it/s]Loading train:  20%|██        | 54/266 [01:02<03:32,  1.00s/it]Loading train:  21%|██        | 55/266 [01:03<03:30,  1.00it/s]Loading train:  21%|██        | 56/266 [01:04<03:27,  1.01it/s]Loading train:  21%|██▏       | 57/266 [01:05<03:28,  1.00it/s]Loading train:  22%|██▏       | 58/266 [01:06<03:33,  1.03s/it]Loading train:  22%|██▏       | 59/266 [01:07<03:25,  1.01it/s]Loading train:  23%|██▎       | 60/266 [01:08<03:33,  1.04s/it]Loading train:  23%|██▎       | 61/266 [01:09<03:29,  1.02s/it]Loading train:  23%|██▎       | 62/266 [01:10<03:17,  1.04it/s]Loading train:  24%|██▎       | 63/266 [01:11<03:10,  1.06it/s]Loading train:  24%|██▍       | 64/266 [01:12<03:11,  1.06it/s]Loading train:  24%|██▍       | 65/266 [01:13<03:14,  1.03it/s]Loading train:  25%|██▍       | 66/266 [01:14<03:09,  1.06it/s]Loading train:  25%|██▌       | 67/266 [01:15<03:09,  1.05it/s]Loading train:  26%|██▌       | 68/266 [01:16<03:08,  1.05it/s]Loading train:  26%|██▌       | 69/266 [01:17<02:59,  1.10it/s]Loading train:  26%|██▋       | 70/266 [01:17<03:02,  1.08it/s]Loading train:  27%|██▋       | 71/266 [01:18<02:58,  1.09it/s]Loading train:  27%|██▋       | 72/266 [01:19<02:57,  1.09it/s]Loading train:  27%|██▋       | 73/266 [01:20<02:55,  1.10it/s]Loading train:  28%|██▊       | 74/266 [01:21<02:48,  1.14it/s]Loading train:  28%|██▊       | 75/266 [01:22<02:48,  1.14it/s]Loading train:  29%|██▊       | 76/266 [01:23<02:43,  1.16it/s]Loading train:  29%|██▉       | 77/266 [01:24<02:48,  1.12it/s]Loading train:  29%|██▉       | 78/266 [01:25<02:56,  1.06it/s]Loading train:  30%|██▉       | 79/266 [01:26<03:05,  1.01it/s]Loading train:  30%|███       | 80/266 [01:27<03:12,  1.03s/it]Loading train:  30%|███       | 81/266 [01:28<03:16,  1.06s/it]Loading train:  31%|███       | 82/266 [01:29<03:15,  1.06s/it]Loading train:  31%|███       | 83/266 [01:30<03:11,  1.05s/it]Loading train:  32%|███▏      | 84/266 [01:31<03:14,  1.07s/it]Loading train:  32%|███▏      | 85/266 [01:32<03:16,  1.08s/it]Loading train:  32%|███▏      | 86/266 [01:33<03:13,  1.08s/it]Loading train:  33%|███▎      | 87/266 [01:34<03:07,  1.05s/it]Loading train:  33%|███▎      | 88/266 [01:36<03:11,  1.08s/it]Loading train:  33%|███▎      | 89/266 [01:37<03:05,  1.05s/it]Loading train:  34%|███▍      | 90/266 [01:38<03:09,  1.08s/it]Loading train:  34%|███▍      | 91/266 [01:39<03:11,  1.10s/it]Loading train:  35%|███▍      | 92/266 [01:40<03:14,  1.12s/it]Loading train:  35%|███▍      | 93/266 [01:41<03:24,  1.18s/it]Loading train:  35%|███▌      | 94/266 [01:42<03:19,  1.16s/it]Loading train:  36%|███▌      | 95/266 [01:44<03:15,  1.14s/it]Loading train:  36%|███▌      | 96/266 [01:45<03:37,  1.28s/it]Loading train:  36%|███▋      | 97/266 [01:47<03:56,  1.40s/it]Loading train:  37%|███▋      | 98/266 [01:48<03:55,  1.40s/it]Loading train:  37%|███▋      | 99/266 [01:50<03:48,  1.37s/it]Loading train:  38%|███▊      | 100/266 [01:51<03:46,  1.37s/it]Loading train:  38%|███▊      | 101/266 [01:52<03:29,  1.27s/it]Loading train:  38%|███▊      | 102/266 [01:53<03:14,  1.19s/it]Loading train:  39%|███▊      | 103/266 [01:54<03:05,  1.14s/it]Loading train:  39%|███▉      | 104/266 [01:55<02:51,  1.06s/it]Loading train:  39%|███▉      | 105/266 [01:56<02:45,  1.03s/it]Loading train:  40%|███▉      | 106/266 [01:57<02:43,  1.02s/it]Loading train:  40%|████      | 107/266 [01:58<02:43,  1.03s/it]Loading train:  41%|████      | 108/266 [01:59<02:45,  1.05s/it]Loading train:  41%|████      | 109/266 [02:00<02:45,  1.06s/it]Loading train:  41%|████▏     | 110/266 [02:01<02:42,  1.04s/it]Loading train:  42%|████▏     | 111/266 [02:02<02:37,  1.01s/it]Loading train:  42%|████▏     | 112/266 [02:03<02:36,  1.02s/it]Loading train:  42%|████▏     | 113/266 [02:04<02:36,  1.03s/it]Loading train:  43%|████▎     | 114/266 [02:05<02:38,  1.04s/it]Loading train:  43%|████▎     | 115/266 [02:06<02:37,  1.04s/it]Loading train:  44%|████▎     | 116/266 [02:07<02:32,  1.01s/it]Loading train:  44%|████▍     | 117/266 [02:08<02:35,  1.04s/it]Loading train:  44%|████▍     | 118/266 [02:09<02:34,  1.04s/it]Loading train:  45%|████▍     | 119/266 [02:10<02:35,  1.06s/it]Loading train:  45%|████▌     | 120/266 [02:11<02:39,  1.09s/it]Loading train:  45%|████▌     | 121/266 [02:13<02:36,  1.08s/it]Loading train:  46%|████▌     | 122/266 [02:14<02:34,  1.07s/it]Loading train:  46%|████▌     | 123/266 [02:15<02:32,  1.07s/it]Loading train:  47%|████▋     | 124/266 [02:16<02:32,  1.07s/it]Loading train:  47%|████▋     | 125/266 [02:17<02:30,  1.07s/it]Loading train:  47%|████▋     | 126/266 [02:18<02:31,  1.08s/it]Loading train:  48%|████▊     | 127/266 [02:19<02:31,  1.09s/it]Loading train:  48%|████▊     | 128/266 [02:20<02:31,  1.10s/it]Loading train:  48%|████▊     | 129/266 [02:21<02:28,  1.08s/it]Loading train:  49%|████▉     | 130/266 [02:22<02:33,  1.13s/it]Loading train:  49%|████▉     | 131/266 [02:23<02:27,  1.10s/it]Loading train:  50%|████▉     | 132/266 [02:25<02:27,  1.10s/it]Loading train:  50%|█████     | 133/266 [02:26<02:26,  1.10s/it]Loading train:  50%|█████     | 134/266 [02:27<02:18,  1.05s/it]Loading train:  51%|█████     | 135/266 [02:28<02:13,  1.02s/it]Loading train:  51%|█████     | 136/266 [02:29<02:10,  1.01s/it]Loading train:  52%|█████▏    | 137/266 [02:30<02:13,  1.03s/it]Loading train:  52%|█████▏    | 138/266 [02:31<02:12,  1.03s/it]Loading train:  52%|█████▏    | 139/266 [02:32<02:11,  1.03s/it]Loading train:  53%|█████▎    | 140/266 [02:33<02:09,  1.02s/it]Loading train:  53%|█████▎    | 141/266 [02:34<02:05,  1.00s/it]Loading train:  53%|█████▎    | 142/266 [02:35<02:02,  1.01it/s]Loading train:  54%|█████▍    | 143/266 [02:36<02:03,  1.01s/it]Loading train:  54%|█████▍    | 144/266 [02:37<02:03,  1.01s/it]Loading train:  55%|█████▍    | 145/266 [02:38<02:00,  1.00it/s]Loading train:  55%|█████▍    | 146/266 [02:39<01:56,  1.03it/s]Loading train:  55%|█████▌    | 147/266 [02:39<01:53,  1.05it/s]Loading train:  56%|█████▌    | 148/266 [02:40<01:51,  1.06it/s]Loading train:  56%|█████▌    | 149/266 [02:41<01:53,  1.03it/s]Loading train:  56%|█████▋    | 150/266 [02:42<01:55,  1.01it/s]Loading train:  57%|█████▋    | 151/266 [02:44<01:59,  1.04s/it]Loading train:  57%|█████▋    | 152/266 [02:45<02:01,  1.07s/it]Loading train:  58%|█████▊    | 153/266 [02:46<02:02,  1.08s/it]Loading train:  58%|█████▊    | 154/266 [02:47<02:01,  1.09s/it]Loading train:  58%|█████▊    | 155/266 [02:48<01:56,  1.05s/it]Loading train:  59%|█████▊    | 156/266 [02:49<01:51,  1.02s/it]Loading train:  59%|█████▉    | 157/266 [02:50<01:44,  1.04it/s]Loading train:  59%|█████▉    | 158/266 [02:51<01:40,  1.07it/s]Loading train:  60%|█████▉    | 159/266 [02:51<01:35,  1.12it/s]Loading train:  60%|██████    | 160/266 [02:52<01:33,  1.13it/s]Loading train:  61%|██████    | 161/266 [02:53<01:34,  1.11it/s]Loading train:  61%|██████    | 162/266 [02:54<01:36,  1.08it/s]Loading train:  61%|██████▏   | 163/266 [02:55<01:34,  1.09it/s]Loading train:  62%|██████▏   | 164/266 [02:56<01:30,  1.12it/s]Loading train:  62%|██████▏   | 165/266 [02:57<01:35,  1.06it/s]Loading train:  62%|██████▏   | 166/266 [02:58<01:33,  1.07it/s]Loading train:  63%|██████▎   | 167/266 [02:59<01:33,  1.06it/s]Loading train:  63%|██████▎   | 168/266 [03:00<01:31,  1.07it/s]Loading train:  64%|██████▎   | 169/266 [03:01<01:34,  1.03it/s]Loading train:  64%|██████▍   | 170/266 [03:02<01:35,  1.00it/s]Loading train:  64%|██████▍   | 171/266 [03:03<01:35,  1.00s/it]Loading train:  65%|██████▍   | 172/266 [03:04<01:34,  1.00s/it]Loading train:  65%|██████▌   | 173/266 [03:05<01:33,  1.01s/it]Loading train:  65%|██████▌   | 174/266 [03:06<01:27,  1.05it/s]Loading train:  66%|██████▌   | 175/266 [03:06<01:22,  1.10it/s]Loading train:  66%|██████▌   | 176/266 [03:07<01:19,  1.14it/s]Loading train:  67%|██████▋   | 177/266 [03:08<01:18,  1.14it/s]Loading train:  67%|██████▋   | 178/266 [03:09<01:14,  1.17it/s]Loading train:  67%|██████▋   | 179/266 [03:10<01:14,  1.17it/s]Loading train:  68%|██████▊   | 180/266 [03:11<01:11,  1.20it/s]Loading train:  68%|██████▊   | 181/266 [03:11<01:08,  1.23it/s]Loading train:  68%|██████▊   | 182/266 [03:12<01:12,  1.16it/s]Loading train:  69%|██████▉   | 183/266 [03:13<01:13,  1.14it/s]Loading train:  69%|██████▉   | 184/266 [03:14<01:14,  1.11it/s]Loading train:  70%|██████▉   | 185/266 [03:15<01:11,  1.13it/s]Loading train:  70%|██████▉   | 186/266 [03:16<01:15,  1.06it/s]Loading train:  70%|███████   | 187/266 [03:17<01:13,  1.07it/s]Loading train:  71%|███████   | 188/266 [03:18<01:14,  1.05it/s]Loading train:  71%|███████   | 189/266 [03:19<01:10,  1.10it/s]Loading train:  71%|███████▏  | 190/266 [03:20<01:08,  1.10it/s]Loading train:  72%|███████▏  | 191/266 [03:21<01:21,  1.08s/it]Loading train:  72%|███████▏  | 192/266 [03:22<01:23,  1.13s/it]Loading train:  73%|███████▎  | 193/266 [03:24<01:26,  1.19s/it]Loading train:  73%|███████▎  | 194/266 [03:26<01:38,  1.37s/it]Loading train:  73%|███████▎  | 195/266 [03:27<01:30,  1.27s/it]Loading train:  74%|███████▎  | 196/266 [03:28<01:23,  1.19s/it]Loading train:  74%|███████▍  | 197/266 [03:29<01:15,  1.09s/it]Loading train:  74%|███████▍  | 198/266 [03:30<01:13,  1.08s/it]Loading train:  75%|███████▍  | 199/266 [03:30<01:08,  1.03s/it]Loading train:  75%|███████▌  | 200/266 [03:31<01:06,  1.00s/it]Loading train:  76%|███████▌  | 201/266 [03:32<01:06,  1.02s/it]Loading train:  76%|███████▌  | 202/266 [03:33<01:03,  1.00it/s]Loading train:  76%|███████▋  | 203/266 [03:35<01:07,  1.07s/it]Loading train:  77%|███████▋  | 204/266 [03:36<01:05,  1.06s/it]Loading train:  77%|███████▋  | 205/266 [03:37<01:05,  1.07s/it]Loading train:  77%|███████▋  | 206/266 [03:38<01:01,  1.03s/it]Loading train:  78%|███████▊  | 207/266 [03:39<00:58,  1.01it/s]Loading train:  78%|███████▊  | 208/266 [03:40<00:56,  1.03it/s]Loading train:  79%|███████▊  | 209/266 [03:40<00:54,  1.04it/s]Loading train:  79%|███████▉  | 210/266 [03:41<00:52,  1.08it/s]Loading train:  79%|███████▉  | 211/266 [03:42<00:52,  1.05it/s]Loading train:  80%|███████▉  | 212/266 [03:43<00:51,  1.05it/s]Loading train:  80%|████████  | 213/266 [03:44<00:48,  1.09it/s]Loading train:  80%|████████  | 214/266 [03:45<00:46,  1.11it/s]Loading train:  81%|████████  | 215/266 [03:46<00:45,  1.13it/s]Loading train:  81%|████████  | 216/266 [03:47<00:43,  1.15it/s]Loading train:  82%|████████▏ | 217/266 [03:47<00:41,  1.17it/s]Loading train:  82%|████████▏ | 218/266 [03:48<00:41,  1.17it/s]Loading train:  82%|████████▏ | 219/266 [03:49<00:40,  1.16it/s]Loading train:  83%|████████▎ | 220/266 [03:50<00:38,  1.18it/s]Loading train:  83%|████████▎ | 221/266 [03:51<00:39,  1.14it/s]Loading train:  83%|████████▎ | 222/266 [03:52<00:39,  1.12it/s]Loading train:  84%|████████▍ | 223/266 [03:53<00:38,  1.12it/s]Loading train:  84%|████████▍ | 224/266 [03:54<00:37,  1.13it/s]Loading train:  85%|████████▍ | 225/266 [03:55<00:37,  1.11it/s]Loading train:  85%|████████▍ | 226/266 [03:56<00:37,  1.07it/s]Loading train:  85%|████████▌ | 227/266 [03:56<00:35,  1.10it/s]Loading train:  86%|████████▌ | 228/266 [03:57<00:33,  1.14it/s]Loading train:  86%|████████▌ | 229/266 [03:58<00:31,  1.18it/s]Loading train:  86%|████████▋ | 230/266 [03:59<00:31,  1.16it/s]Loading train:  87%|████████▋ | 231/266 [04:00<00:31,  1.13it/s]Loading train:  87%|████████▋ | 232/266 [04:01<00:31,  1.07it/s]Loading train:  88%|████████▊ | 233/266 [04:02<00:30,  1.08it/s]Loading train:  88%|████████▊ | 234/266 [04:03<00:30,  1.06it/s]Loading train:  88%|████████▊ | 235/266 [04:04<00:30,  1.03it/s]Loading train:  89%|████████▊ | 236/266 [04:05<00:29,  1.03it/s]Loading train:  89%|████████▉ | 237/266 [04:06<00:26,  1.08it/s]Loading train:  89%|████████▉ | 238/266 [04:07<00:26,  1.07it/s]Loading train:  90%|████████▉ | 239/266 [04:08<00:25,  1.07it/s]Loading train:  90%|█████████ | 240/266 [04:09<00:24,  1.06it/s]Loading train:  91%|█████████ | 241/266 [04:09<00:23,  1.07it/s]Loading train:  91%|█████████ | 242/266 [04:10<00:21,  1.09it/s]Loading train:  91%|█████████▏| 243/266 [04:11<00:21,  1.07it/s]Loading train:  92%|█████████▏| 244/266 [04:12<00:20,  1.07it/s]Loading train:  92%|█████████▏| 245/266 [04:13<00:19,  1.10it/s]Loading train:  92%|█████████▏| 246/266 [04:14<00:17,  1.13it/s]Loading train:  93%|█████████▎| 247/266 [04:15<00:16,  1.17it/s]Loading train:  93%|█████████▎| 248/266 [04:16<00:16,  1.09it/s]Loading train:  94%|█████████▎| 249/266 [04:17<00:16,  1.00it/s]Loading train:  94%|█████████▍| 250/266 [04:18<00:16,  1.04s/it]Loading train:  94%|█████████▍| 251/266 [04:19<00:15,  1.06s/it]Loading train:  95%|█████████▍| 252/266 [04:20<00:15,  1.09s/it]Loading train:  95%|█████████▌| 253/266 [04:21<00:13,  1.06s/it]Loading train:  95%|█████████▌| 254/266 [04:22<00:12,  1.06s/it]Loading train:  96%|█████████▌| 255/266 [04:24<00:11,  1.07s/it]Loading train:  96%|█████████▌| 256/266 [04:24<00:10,  1.03s/it]Loading train:  97%|█████████▋| 257/266 [04:25<00:09,  1.01s/it]Loading train:  97%|█████████▋| 258/266 [04:26<00:08,  1.00s/it]Loading train:  97%|█████████▋| 259/266 [04:27<00:07,  1.00s/it]Loading train:  98%|█████████▊| 260/266 [04:28<00:05,  1.00it/s]Loading train:  98%|█████████▊| 261/266 [04:29<00:05,  1.00s/it]Loading train:  98%|█████████▊| 262/266 [04:30<00:04,  1.00s/it]Loading train:  99%|█████████▉| 263/266 [04:31<00:03,  1.01s/it]Loading train:  99%|█████████▉| 264/266 [04:32<00:02,  1.01s/it]Loading train: 100%|█████████▉| 265/266 [04:33<00:01,  1.02s/it]Loading train: 100%|██████████| 266/266 [04:34<00:00,  1.01s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   1%|          | 2/266 [00:00<00:13, 19.90it/s]concatenating: train:   5%|▍         | 12/266 [00:00<00:09, 26.10it/s]concatenating: train:   9%|▉         | 24/266 [00:00<00:07, 33.96it/s]concatenating: train:  13%|█▎        | 34/266 [00:00<00:05, 42.24it/s]concatenating: train:  17%|█▋        | 45/266 [00:00<00:04, 51.22it/s]concatenating: train:  20%|██        | 54/266 [00:00<00:03, 58.49it/s]concatenating: train:  25%|██▌       | 67/266 [00:00<00:02, 70.01it/s]concatenating: train:  30%|██▉       | 79/266 [00:00<00:02, 78.81it/s]concatenating: train:  35%|███▍      | 93/266 [00:00<00:01, 89.87it/s]concatenating: train:  39%|███▉      | 105/266 [00:01<00:01, 90.16it/s]concatenating: train:  44%|████▎     | 116/266 [00:01<00:01, 91.14it/s]concatenating: train:  50%|████▉     | 132/266 [00:01<00:01, 104.00it/s]concatenating: train:  54%|█████▍    | 144/266 [00:01<00:01, 102.78it/s]concatenating: train:  59%|█████▊    | 156/266 [00:01<00:01, 100.92it/s]concatenating: train:  64%|██████▍   | 170/266 [00:01<00:00, 109.64it/s]concatenating: train:  69%|██████▉   | 183/266 [00:01<00:00, 114.99it/s]concatenating: train:  74%|███████▍  | 198/266 [00:01<00:00, 122.98it/s]concatenating: train:  80%|████████  | 213/266 [00:01<00:00, 129.13it/s]concatenating: train:  85%|████████▌ | 227/266 [00:02<00:00, 129.16it/s]concatenating: train:  92%|█████████▏| 245/266 [00:02<00:00, 139.14it/s]concatenating: train: 100%|██████████| 266/266 [00:02<00:00, 118.44it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:01<00:05,  1.42s/it]Loading test:  40%|████      | 2/5 [00:02<00:04,  1.38s/it]Loading test:  60%|██████    | 3/5 [00:03<00:02,  1.34s/it]Loading test:  80%|████████  | 4/5 [00:05<00:01,  1.28s/it]Loading test: 100%|██████████| 5/5 [00:06<00:00,  1.33s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation:  40%|████      | 2/5 [00:00<00:00, 18.73it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 33.02it/s]2019-08-17 02:19:17.969044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-17 02:19:17.969149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-17 02:19:17.969165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-17 02:19:17.969175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-17 02:19:17.969593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:08,  5.25it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:06,  6.22it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:06,  5.89it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:04,  7.61it/s]loading the weights for Res Unet:  23%|██▎       | 10/44 [00:00<00:04,  8.25it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:04,  7.18it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:02,  9.23it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02,  9.57it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:02<00:02,  7.81it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:01,  9.67it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01, 10.05it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01, 10.09it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:02<00:01,  8.04it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:03<00:00,  9.97it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:03<00:00, 10.24it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00, 10.35it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:03<00:00,  8.12it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:03<00:00, 11.35it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 3  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 3  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 68, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 68, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 68, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 68, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 68, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 68, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 68, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 34, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 34, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 34, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 34, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 34, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 34, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 34, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 34, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 34, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 17, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 17, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 17, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 17, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 17, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 17, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 17, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 17, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 17, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 17, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 34, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 34, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 34, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 34, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 34, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 34, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 34, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 34, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 34, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 34, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 68, 30)   25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 68, 60)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 68, 30)   16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 68, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 68, 30)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 68, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 68, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 68, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 68, 90)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 68, 90)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 68, 13)   1183        dropout_5[0][0]                  
==================================================================================================
Total params: 501,343
Trainable params: 51,973
Non-trainable params: 449,370
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.34828515e-02 3.29022805e-02 7.69374577e-02 9.55986599e-03
 2.76684291e-02 7.23874054e-03 8.42873872e-02 1.14354864e-01
 8.97911400e-02 1.36423988e-02 2.91120537e-01 1.88842793e-01
 1.71254702e-04]
Train on 17436 samples, validate on 314 samples
Epoch 1/300
 - 20s - loss: 1.7727 - acc: 0.8126 - mDice: 0.2259 - val_loss: 1.7512 - val_acc: 0.9396 - val_mDice: 0.3898

Epoch 00001: val_mDice improved from -inf to 0.38979, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 16s - loss: 0.8751 - acc: 0.9217 - mDice: 0.4056 - val_loss: 1.5309 - val_acc: 0.9441 - val_mDice: 0.4574

Epoch 00002: val_mDice improved from 0.38979 to 0.45738, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 15s - loss: 0.7422 - acc: 0.9248 - mDice: 0.4647 - val_loss: 1.4546 - val_acc: 0.9483 - val_mDice: 0.4904

Epoch 00003: val_mDice improved from 0.45738 to 0.49038, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 15s - loss: 0.6851 - acc: 0.9271 - mDice: 0.4957 - val_loss: 1.3597 - val_acc: 0.9500 - val_mDice: 0.4908

Epoch 00004: val_mDice improved from 0.49038 to 0.49081, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 16s - loss: 0.6310 - acc: 0.9287 - mDice: 0.5221 - val_loss: 1.3995 - val_acc: 0.9521 - val_mDice: 0.5049

Epoch 00005: val_mDice improved from 0.49081 to 0.50489, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 15s - loss: 0.6114 - acc: 0.9296 - mDice: 0.5315 - val_loss: 1.3213 - val_acc: 0.9521 - val_mDice: 0.5119

Epoch 00006: val_mDice improved from 0.50489 to 0.51190, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 15s - loss: 0.5871 - acc: 0.9306 - mDice: 0.5530 - val_loss: 1.3604 - val_acc: 0.9522 - val_mDice: 0.5042

Epoch 00007: val_mDice did not improve from 0.51190
Epoch 8/300
 - 16s - loss: 0.5722 - acc: 0.9310 - mDice: 0.5549 - val_loss: 1.3020 - val_acc: 0.9546 - val_mDice: 0.5276

Epoch 00008: val_mDice improved from 0.51190 to 0.52761, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 16s - loss: 0.5306 - acc: 0.9318 - mDice: 0.5753 - val_loss: 1.3046 - val_acc: 0.9546 - val_mDice: 0.5275

Epoch 00009: val_mDice did not improve from 0.52761
Epoch 10/300
 - 15s - loss: 0.5302 - acc: 0.9318 - mDice: 0.5786 - val_loss: 1.1847 - val_acc: 0.9546 - val_mDice: 0.5301

Epoch 00010: val_mDice improved from 0.52761 to 0.53013, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 15s - loss: 0.5431 - acc: 0.9319 - mDice: 0.5777 - val_loss: 1.2956 - val_acc: 0.9537 - val_mDice: 0.5198

Epoch 00011: val_mDice did not improve from 0.53013
Epoch 12/300
 - 16s - loss: 0.5219 - acc: 0.9332 - mDice: 0.5917 - val_loss: 1.2380 - val_acc: 0.9555 - val_mDice: 0.5268

Epoch 00012: val_mDice did not improve from 0.53013
Epoch 13/300
 - 16s - loss: 0.4911 - acc: 0.9342 - mDice: 0.6008 - val_loss: 1.1740 - val_acc: 0.9572 - val_mDice: 0.5416

Epoch 00013: val_mDice improved from 0.53013 to 0.54157, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 16s - loss: 0.5010 - acc: 0.9345 - mDice: 0.5974 - val_loss: 1.2299 - val_acc: 0.9584 - val_mDice: 0.5408

Epoch 00014: val_mDice did not improve from 0.54157
Epoch 15/300
 - 16s - loss: 0.4994 - acc: 0.9353 - mDice: 0.6010 - val_loss: 1.2162 - val_acc: 0.9556 - val_mDice: 0.5346

Epoch 00015: val_mDice did not improve from 0.54157
Epoch 16/300
 - 16s - loss: 0.4849 - acc: 0.9355 - mDice: 0.6095 - val_loss: 1.0783 - val_acc: 0.9581 - val_mDice: 0.5410

Epoch 00016: val_mDice did not improve from 0.54157
Epoch 17/300
 - 17s - loss: 0.4738 - acc: 0.9369 - mDice: 0.6130 - val_loss: 1.2509 - val_acc: 0.9568 - val_mDice: 0.5257

Epoch 00017: val_mDice did not improve from 0.54157
Epoch 18/300
 - 16s - loss: 0.4850 - acc: 0.9403 - mDice: 0.6094 - val_loss: 1.1631 - val_acc: 0.9607 - val_mDice: 0.5323

Epoch 00018: val_mDice did not improve from 0.54157
Epoch 19/300
 - 16s - loss: 0.4609 - acc: 0.9506 - mDice: 0.6196 - val_loss: 1.1716 - val_acc: 0.9624 - val_mDice: 0.5348

Epoch 00019: val_mDice did not improve from 0.54157
Epoch 20/300
 - 16s - loss: 0.4506 - acc: 0.9525 - mDice: 0.6252 - val_loss: 1.1424 - val_acc: 0.9627 - val_mDice: 0.5396

Epoch 00020: val_mDice did not improve from 0.54157
Epoch 21/300
 - 16s - loss: 0.4576 - acc: 0.9528 - mDice: 0.6277 - val_loss: 1.1509 - val_acc: 0.9631 - val_mDice: 0.5395

Epoch 00021: val_mDice did not improve from 0.54157
Epoch 22/300
 - 17s - loss: 0.4511 - acc: 0.9529 - mDice: 0.6279 - val_loss: 1.1515 - val_acc: 0.9621 - val_mDice: 0.5341

Epoch 00022: val_mDice did not improve from 0.54157
Epoch 23/300
 - 16s - loss: 0.4471 - acc: 0.9528 - mDice: 0.6293 - val_loss: 1.1847 - val_acc: 0.9620 - val_mDice: 0.5327

Epoch 00023: val_mDice did not improve from 0.54157
Epoch 24/300
 - 16s - loss: 0.4482 - acc: 0.9532 - mDice: 0.6321 - val_loss: 1.1543 - val_acc: 0.9628 - val_mDice: 0.5395

Epoch 00024: val_mDice did not improve from 0.54157
Epoch 25/300
 - 16s - loss: 0.4325 - acc: 0.9533 - mDice: 0.6363 - val_loss: 1.1264 - val_acc: 0.9625 - val_mDice: 0.5458

Epoch 00025: val_mDice improved from 0.54157 to 0.54579, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 16s - loss: 0.4392 - acc: 0.9534 - mDice: 0.6366 - val_loss: 1.1025 - val_acc: 0.9618 - val_mDice: 0.5326

Epoch 00026: val_mDice did not improve from 0.54579
Epoch 27/300
 - 16s - loss: 0.4462 - acc: 0.9528 - mDice: 0.6309 - val_loss: 1.1299 - val_acc: 0.9609 - val_mDice: 0.5315

Epoch 00027: val_mDice did not improve from 0.54579
Epoch 28/300
 - 17s - loss: 0.4330 - acc: 0.9534 - mDice: 0.6358 - val_loss: 1.1592 - val_acc: 0.9609 - val_mDice: 0.5281

Epoch 00028: val_mDice did not improve from 0.54579
Epoch 29/300
 - 16s - loss: 0.4301 - acc: 0.9535 - mDice: 0.6377 - val_loss: 1.1135 - val_acc: 0.9613 - val_mDice: 0.5315

Epoch 00029: val_mDice did not improve from 0.54579
Epoch 30/300
 - 16s - loss: 0.4411 - acc: 0.9539 - mDice: 0.6414 - val_loss: 1.1694 - val_acc: 0.9610 - val_mDice: 0.5329

Epoch 00030: val_mDice did not improve from 0.54579
Epoch 31/300
 - 16s - loss: 0.4197 - acc: 0.9541 - mDice: 0.6440 - val_loss: 1.0708 - val_acc: 0.9618 - val_mDice: 0.5366

Epoch 00031: val_mDice did not improve from 0.54579
Epoch 32/300
 - 16s - loss: 0.4197 - acc: 0.9542 - mDice: 0.6470 - val_loss: 1.1490 - val_acc: 0.9613 - val_mDice: 0.5389

Epoch 00032: val_mDice did not improve from 0.54579
Epoch 33/300
 - 16s - loss: 0.4202 - acc: 0.9541 - mDice: 0.6485 - val_loss: 1.1238 - val_acc: 0.9595 - val_mDice: 0.5451

Epoch 00033: val_mDice did not improve from 0.54579
Epoch 34/300
 - 16s - loss: 0.4162 - acc: 0.9540 - mDice: 0.6458 - val_loss: 1.1389 - val_acc: 0.9617 - val_mDice: 0.5362

Epoch 00034: val_mDice did not improve from 0.54579
Epoch 35/300
 - 17s - loss: 0.4222 - acc: 0.9545 - mDice: 0.6506 - val_loss: 1.1720 - val_acc: 0.9614 - val_mDice: 0.5135

Epoch 00035: val_mDice did not improve from 0.54579
Epoch 36/300
 - 16s - loss: 0.4153 - acc: 0.9544 - mDice: 0.6508 - val_loss: 1.1396 - val_acc: 0.9619 - val_mDice: 0.5409

Epoch 00036: val_mDice did not improve from 0.54579
Epoch 37/300
 - 16s - loss: 0.4141 - acc: 0.9547 - mDice: 0.6514 - val_loss: 1.1544 - val_acc: 0.9617 - val_mDice: 0.5362

Epoch 00037: val_mDice did not improve from 0.54579
Epoch 38/300
 - 16s - loss: 0.4088 - acc: 0.9542 - mDice: 0.6506 - val_loss: 1.0508 - val_acc: 0.9618 - val_mDice: 0.5433

Epoch 00038: val_mDice did not improve from 0.54579
Epoch 39/300
 - 16s - loss: 0.4272 - acc: 0.9541 - mDice: 0.6439 - val_loss: 1.0147 - val_acc: 0.9621 - val_mDice: 0.5416

Epoch 00039: val_mDice did not improve from 0.54579
Epoch 40/300
 - 16s - loss: 0.4381 - acc: 0.9544 - mDice: 0.6466 - val_loss: 1.0859 - val_acc: 0.9611 - val_mDice: 0.5300

Epoch 00040: val_mDice did not improve from 0.54579
Epoch 41/300
 - 16s - loss: 0.4201 - acc: 0.9547 - mDice: 0.6545 - val_loss: 1.0447 - val_acc: 0.9624 - val_mDice: 0.5470

Epoch 00041: val_mDice improved from 0.54579 to 0.54704, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 17s - loss: 0.3969 - acc: 0.9551 - mDice: 0.6605 - val_loss: 1.0981 - val_acc: 0.9619 - val_mDice: 0.5406

Epoch 00042: val_mDice did not improve from 0.54704
Epoch 43/300
 - 16s - loss: 0.3977 - acc: 0.9552 - mDice: 0.6612 - val_loss: 0.9695 - val_acc: 0.9628 - val_mDice: 0.5458

Epoch 00043: val_mDice did not improve from 0.54704
Epoch 44/300
 - 16s - loss: 0.3916 - acc: 0.9551 - mDice: 0.6639 - val_loss: 1.0189 - val_acc: 0.9623 - val_mDice: 0.5408

Epoch 00044: val_mDice did not improve from 0.54704
Epoch 45/300
 - 16s - loss: 0.4005 - acc: 0.9548 - mDice: 0.6593 - val_loss: 1.1122 - val_acc: 0.9619 - val_mDice: 0.5457

Epoch 00045: val_mDice did not improve from 0.54704
Epoch 46/300
 - 16s - loss: 0.3900 - acc: 0.9552 - mDice: 0.6649 - val_loss: 0.9987 - val_acc: 0.9626 - val_mDice: 0.5469

Epoch 00046: val_mDice did not improve from 0.54704
Epoch 47/300
 - 16s - loss: 0.3879 - acc: 0.9552 - mDice: 0.6665 - val_loss: 1.0197 - val_acc: 0.9625 - val_mDice: 0.5383

Epoch 00047: val_mDice did not improve from 0.54704
Epoch 48/300
 - 16s - loss: 0.4052 - acc: 0.9553 - mDice: 0.6656 - val_loss: 1.1703 - val_acc: 0.9609 - val_mDice: 0.5244

Epoch 00048: val_mDice did not improve from 0.54704
Epoch 49/300
 - 17s - loss: 0.4046 - acc: 0.9549 - mDice: 0.6608 - val_loss: 1.1709 - val_acc: 0.9626 - val_mDice: 0.5498

Epoch 00049: val_mDice improved from 0.54704 to 0.54980, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 16s - loss: 0.4078 - acc: 0.9548 - mDice: 0.6569 - val_loss: 1.2831 - val_acc: 0.9626 - val_mDice: 0.5395

Epoch 00050: val_mDice did not improve from 0.54980
Epoch 51/300
 - 16s - loss: 0.3930 - acc: 0.9553 - mDice: 0.6648 - val_loss: 1.1916 - val_acc: 0.9615 - val_mDice: 0.5285

Epoch 00051: val_mDice did not improve from 0.54980
Epoch 52/300
 - 16s - loss: 0.3862 - acc: 0.9556 - mDice: 0.6669 - val_loss: 1.1325 - val_acc: 0.9622 - val_mDice: 0.5461

Epoch 00052: val_mDice did not improve from 0.54980
Epoch 53/300
 - 16s - loss: 0.4041 - acc: 0.9552 - mDice: 0.6624 - val_loss: 1.0782 - val_acc: 0.9624 - val_mDice: 0.5430

Epoch 00053: val_mDice did not improve from 0.54980
Epoch 54/300
 - 16s - loss: 0.3877 - acc: 0.9556 - mDice: 0.6691 - val_loss: 1.1015 - val_acc: 0.9612 - val_mDice: 0.5280

Epoch 00054: val_mDice did not improve from 0.54980
Epoch 55/300
 - 16s - loss: 0.3843 - acc: 0.9557 - mDice: 0.6688 - val_loss: 1.0503 - val_acc: 0.9622 - val_mDice: 0.5410

Epoch 00055: val_mDice did not improve from 0.54980
Epoch 56/300
 - 17s - loss: 0.3839 - acc: 0.9559 - mDice: 0.6725 - val_loss: 1.3034 - val_acc: 0.9625 - val_mDice: 0.5363

Epoch 00056: val_mDice did not improve from 0.54980
Epoch 57/300
 - 16s - loss: 0.3898 - acc: 0.9557 - mDice: 0.6668 - val_loss: 1.1965 - val_acc: 0.9619 - val_mDice: 0.5357

Epoch 00057: val_mDice did not improve from 0.54980
Epoch 58/300
 - 16s - loss: 0.4041 - acc: 0.9556 - mDice: 0.6630 - val_loss: 1.1327 - val_acc: 0.9623 - val_mDice: 0.5382

Epoch 00058: val_mDice did not improve from 0.54980
Epoch 59/300
 - 16s - loss: 0.3888 - acc: 0.9558 - mDice: 0.6686 - val_loss: 1.1325 - val_acc: 0.9619 - val_mDice: 0.5375

Epoch 00059: val_mDice did not improve from 0.54980
Epoch 60/300
 - 16s - loss: 0.3801 - acc: 0.9562 - mDice: 0.6741 - val_loss: 1.0359 - val_acc: 0.9621 - val_mDice: 0.5456

Epoch 00060: val_mDice did not improve from 0.54980
Epoch 61/300
 - 16s - loss: 0.3896 - acc: 0.9561 - mDice: 0.6709 - val_loss: 0.9674 - val_acc: 0.9629 - val_mDice: 0.5443

Epoch 00061: val_mDice did not improve from 0.54980
Epoch 62/300
 - 16s - loss: 0.3747 - acc: 0.9565 - mDice: 0.6780 - val_loss: 1.0891 - val_acc: 0.9623 - val_mDice: 0.5407

Epoch 00062: val_mDice did not improve from 0.54980
Epoch 63/300
 - 17s - loss: 0.3715 - acc: 0.9565 - mDice: 0.6781 - val_loss: 1.1042 - val_acc: 0.9627 - val_mDice: 0.5411

Epoch 00063: val_mDice did not improve from 0.54980
Epoch 64/300
 - 17s - loss: 0.3741 - acc: 0.9564 - mDice: 0.6758 - val_loss: 1.0621 - val_acc: 0.9630 - val_mDice: 0.5426

Epoch 00064: val_mDice did not improve from 0.54980
Epoch 65/300
 - 16s - loss: 0.3915 - acc: 0.9560 - mDice: 0.6689 - val_loss: 1.0598 - val_acc: 0.9629 - val_mDice: 0.5419

Epoch 00065: val_mDice did not improve from 0.54980
Epoch 66/300
 - 16s - loss: 0.3718 - acc: 0.9565 - mDice: 0.6765 - val_loss: 1.1439 - val_acc: 0.9630 - val_mDice: 0.5468

Epoch 00066: val_mDice did not improve from 0.54980
Epoch 67/300
 - 16s - loss: 0.3815 - acc: 0.9564 - mDice: 0.6747 - val_loss: 1.0896 - val_acc: 0.9630 - val_mDice: 0.5384

Epoch 00067: val_mDice did not improve from 0.54980
Epoch 68/300
 - 16s - loss: 0.3749 - acc: 0.9566 - mDice: 0.6783 - val_loss: 1.1833 - val_acc: 0.9624 - val_mDice: 0.5465

Epoch 00068: val_mDice did not improve from 0.54980
Epoch 69/300
 - 16s - loss: 0.3714 - acc: 0.9566 - mDice: 0.6804 - val_loss: 1.1442 - val_acc: 0.9621 - val_mDice: 0.5336

Epoch 00069: val_mDice did not improve from 0.54980
Epoch 70/300
 - 17s - loss: 0.3645 - acc: 0.9567 - mDice: 0.6808 - val_loss: 1.1529 - val_acc: 0.9625 - val_mDice: 0.5472

Epoch 00070: val_mDice did not improve from 0.54980
Epoch 71/300
 - 17s - loss: 0.3874 - acc: 0.9564 - mDice: 0.6745 - val_loss: 1.0088 - val_acc: 0.9628 - val_mDice: 0.5459

Epoch 00071: val_mDice did not improve from 0.54980
Epoch 72/300
 - 16s - loss: 0.3746 - acc: 0.9568 - mDice: 0.6814 - val_loss: 1.0818 - val_acc: 0.9625 - val_mDice: 0.5460

Epoch 00072: val_mDice did not improve from 0.54980
Epoch 73/300
 - 16s - loss: 0.3820 - acc: 0.9564 - mDice: 0.6749 - val_loss: 1.1123 - val_acc: 0.9626 - val_mDice: 0.5464

Epoch 00073: val_mDice did not improve from 0.54980
Epoch 74/300
 - 16s - loss: 0.3601 - acc: 0.9569 - mDice: 0.6835 - val_loss: 1.0713 - val_acc: 0.9630 - val_mDice: 0.5426

Epoch 00074: val_mDice did not improve from 0.54980
Epoch 75/300
 - 16s - loss: 0.3602 - acc: 0.9569 - mDice: 0.6840 - val_loss: 1.1341 - val_acc: 0.9622 - val_mDice: 0.5368

Epoch 00075: val_mDice did not improve from 0.54980
Epoch 76/300
 - 16s - loss: 0.3664 - acc: 0.9569 - mDice: 0.6830 - val_loss: 0.9941 - val_acc: 0.9628 - val_mDice: 0.5405

Epoch 00076: val_mDice did not improve from 0.54980
Epoch 77/300
 - 17s - loss: 0.3683 - acc: 0.9570 - mDice: 0.6857 - val_loss: 1.2622 - val_acc: 0.9610 - val_mDice: 0.5195

Epoch 00077: val_mDice did not improve from 0.54980
Epoch 78/300
 - 16s - loss: 0.3776 - acc: 0.9564 - mDice: 0.6770 - val_loss: 0.9888 - val_acc: 0.9627 - val_mDice: 0.5365

Epoch 00078: val_mDice did not improve from 0.54980
Epoch 79/300
 - 16s - loss: 0.3721 - acc: 0.9569 - mDice: 0.6817 - val_loss: 1.0659 - val_acc: 0.9629 - val_mDice: 0.5433

Epoch 00079: val_mDice did not improve from 0.54980
Epoch 80/300
 - 16s - loss: 0.3653 - acc: 0.9569 - mDice: 0.6816 - val_loss: 1.1206 - val_acc: 0.9624 - val_mDice: 0.5421

Epoch 00080: val_mDice did not improve from 0.54980
Epoch 81/300
 - 16s - loss: 0.3733 - acc: 0.9571 - mDice: 0.6860 - val_loss: 1.0899 - val_acc: 0.9623 - val_mDice: 0.5411

Epoch 00081: val_mDice did not improve from 0.54980
Epoch 82/300
 - 16s - loss: 0.3684 - acc: 0.9571 - mDice: 0.6851 - val_loss: 1.0841 - val_acc: 0.9619 - val_mDice: 0.5328

Epoch 00082: val_mDice did not improve from 0.54980
Epoch 83/300
 - 16s - loss: 0.3573 - acc: 0.9572 - mDice: 0.6886 - val_loss: 1.0443 - val_acc: 0.9629 - val_mDice: 0.5427

Epoch 00083: val_mDice did not improve from 0.54980
Epoch 84/300
 - 17s - loss: 0.3538 - acc: 0.9573 - mDice: 0.6904 - val_loss: 1.1154 - val_acc: 0.9623 - val_mDice: 0.5450

Epoch 00084: val_mDice did not improve from 0.54980
Epoch 85/300
 - 17s - loss: 0.3508 - acc: 0.9573 - mDice: 0.6903 - val_loss: 1.0145 - val_acc: 0.9621 - val_mDice: 0.5347

Epoch 00085: val_mDice did not improve from 0.54980
Epoch 86/300
 - 16s - loss: 0.3817 - acc: 0.9568 - mDice: 0.6788 - val_loss: 1.1301 - val_acc: 0.9623 - val_mDice: 0.5324

Epoch 00086: val_mDice did not improve from 0.54980
Epoch 87/300
 - 16s - loss: 0.3648 - acc: 0.9573 - mDice: 0.6880 - val_loss: 1.0805 - val_acc: 0.9629 - val_mDice: 0.5457

Epoch 00087: val_mDice did not improve from 0.54980
Epoch 88/300
 - 16s - loss: 0.4102 - acc: 0.9555 - mDice: 0.6653 - val_loss: 1.0898 - val_acc: 0.9614 - val_mDice: 0.5300

Epoch 00088: val_mDice did not improve from 0.54980
Epoch 89/300
 - 16s - loss: 0.3568 - acc: 0.9572 - mDice: 0.6860 - val_loss: 1.0658 - val_acc: 0.9625 - val_mDice: 0.5439

Epoch 00089: val_mDice did not improve from 0.54980
Restoring model weights from the end of the best epoch
Epoch 00089: early stopping
{'val_loss': [1.7511864384268498, 1.5308629991901908, 1.454581911017181, 1.359724515562604, 1.399546515030466, 1.3212544397943338, 1.3604143637760429, 1.3019653031021168, 1.3046456681694953, 1.1847103435522433, 1.29563354686567, 1.238033594219548, 1.1740421630015039, 1.2298947386680894, 1.216226506005427, 1.0783093055342412, 1.2509306365517294, 1.1630516299016915, 1.1715649468883587, 1.1424391493675814, 1.1509313302434934, 1.1514902786844095, 1.1846688150600264, 1.1543040831757199, 1.126433241519199, 1.102462286022818, 1.1299096585079362, 1.1591713470258531, 1.1134960833628467, 1.1693992481869497, 1.0708091744951382, 1.1489884617981638, 1.123776657186496, 1.1388864405215926, 1.171983516519996, 1.1395859211493449, 1.154431585673314, 1.050753755554272, 1.014674657089695, 1.0858889267702772, 1.0446824416233476, 1.0980610927199101, 0.9694918190977376, 1.0188921316034476, 1.1122467832960141, 0.9986634681558912, 1.0197291191975781, 1.1702949389530595, 1.170866891457017, 1.2830911066122115, 1.1915756596881113, 1.1325167895881993, 1.0782360637643535, 1.101462319398382, 1.050281630010362, 1.3034187887504602, 1.1965129760799893, 1.1326666986866363, 1.1324734099351677, 1.0358700199871307, 0.9674115864334593, 1.089090991551709, 1.104231990636534, 1.0620824630093422, 1.059840744657881, 1.143918183578807, 1.0896197730188917, 1.1832942021120885, 1.1442146175985883, 1.1529120730746323, 1.0088471891773734, 1.0818368336956972, 1.1122993995809252, 1.0712658958450245, 1.1340871722834884, 0.9941236321713515, 1.2622003912166426, 0.9888194108464915, 1.0658628073087923, 1.1206095235742581, 1.0898680032058885, 1.084079934912882, 1.0443454920106632, 1.1153899529936966, 1.0144726802968675, 1.1301468586086467, 1.0805064800438609, 1.0898235869255795, 1.0658036684914], 'val_acc': [0.9395625018010474, 0.9440666725681086, 0.9483375758122486, 0.9499659572437311, 0.9520897128779418, 0.9520914968411633, 0.9521842648269264, 0.9545574913358992, 0.954564696284616, 0.9546448554203009, 0.9536631373083515, 0.9555248037265365, 0.9572387276941045, 0.9583807754668461, 0.9556049484356194, 0.958101576300943, 0.956771302754712, 0.9607485764345546, 0.9624346217532067, 0.9626994049473173, 0.9631173067791446, 0.962086063661393, 0.9619779939864092, 0.9628399105588342, 0.9624823567214286, 0.9618221563138779, 0.9609016973501557, 0.9608557672257636, 0.9613412166856656, 0.960970132593896, 0.9617987373831925, 0.9612862817041433, 0.9595318132904684, 0.9616528377411472, 0.9613979511959537, 0.9619185332280056, 0.9617447116572386, 0.9617933422137218, 0.9620761415760988, 0.9610962286876266, 0.9623985825830205, 0.9619176383990391, 0.9628209855146469, 0.9622787897753867, 0.9619239473798472, 0.9625994354296642, 0.9624760549539214, 0.9608989942605328, 0.9625985182015, 0.9626462668370289, 0.9615078355856003, 0.9622319633034384, 0.9623832710229667, 0.9611565838953492, 0.9621986367140606, 0.9624823426744741, 0.9619059236186325, 0.9622706926552353, 0.9618636145713223, 0.9621346942178762, 0.9628750393345098, 0.9623292278332315, 0.962697605798199, 0.9630443529718241, 0.9629443826948761, 0.9629885151887395, 0.9629903124396209, 0.9623850906730458, 0.9620914740167605, 0.9625426857334793, 0.9628200800555526, 0.9624616260741167, 0.9626147340817056, 0.9630056224810849, 0.9622229584463083, 0.9628110816524287, 0.960980950647099, 0.962689501844394, 0.9629434620498851, 0.9623724689149553, 0.9623148251490988, 0.9618645056038145, 0.9628579274863954, 0.9623319354786235, 0.9620752471267797, 0.9623274214708122, 0.9629227758213214, 0.9614330898424622, 0.9625453971753455], 'val_mDice': [0.3897931493201833, 0.45738484601306306, 0.4903841440085393, 0.4908087050459187, 0.5048903957673698, 0.5118988536915202, 0.5041891620227485, 0.5276127066581872, 0.5275359049344518, 0.5301321266563075, 0.5198060422186639, 0.526843975778598, 0.5415656186972454, 0.5408213894078686, 0.534556807985731, 0.5410383199430575, 0.5257129582819665, 0.532281532314173, 0.5347752907093922, 0.5396164163092899, 0.5394686529780649, 0.5341291419070238, 0.532680128790011, 0.5394660559049838, 0.5457891045482295, 0.5326341098281229, 0.5314575246754726, 0.5281092802620238, 0.5314538178929857, 0.5329489159356257, 0.5366020065963648, 0.5388871640156788, 0.5450769562250489, 0.5361731555431511, 0.513542037291132, 0.5408884594394903, 0.5362274377209366, 0.5433034716518061, 0.5415743335037474, 0.530032837561741, 0.5470415306319097, 0.540637055969542, 0.545805304862891, 0.5408084895580437, 0.5456630371178791, 0.5468916864531814, 0.5383269890288639, 0.5244236254388359, 0.54980262012998, 0.5394664382099346, 0.5284529039814214, 0.5461459123784569, 0.5429703270553783, 0.52796399118794, 0.540990722597025, 0.536253747856541, 0.5356783874475273, 0.5382325497402507, 0.5375459104966206, 0.5455502622826084, 0.5442717713155564, 0.540688350511964, 0.5410881101325818, 0.5425776789902123, 0.541945886080432, 0.5468244600068232, 0.5384471266512658, 0.546468916402501, 0.5336003593958107, 0.5472217969074371, 0.5459485274211616, 0.545970090825087, 0.5464012241287596, 0.5426172587522275, 0.5367917126151407, 0.5405084050385056, 0.5195035609849699, 0.5364577065987192, 0.5432792440721184, 0.5421173987305088, 0.5411103020428093, 0.5328145399215115, 0.5427022190989962, 0.5450144421522785, 0.5346786749970381, 0.5323567081978366, 0.5457446442288199, 0.5299881249666214, 0.5438577423619616], 'loss': [1.7726829848751122, 0.8751475210172317, 0.7421892205286146, 0.6851395615937377, 0.630965249380898, 0.6114318748971523, 0.5871376861438042, 0.5722023630482692, 0.5305595107549209, 0.5302435504026877, 0.5431421830357968, 0.5219391946140083, 0.4910917611167529, 0.5009645124891806, 0.499379719423575, 0.48488153345638474, 0.4738067407062716, 0.48497160007768225, 0.46087534522266393, 0.4505521519881756, 0.45764932925888746, 0.4511325460770831, 0.447090115667374, 0.44823173100411223, 0.43250577004438806, 0.43919965898971486, 0.4462098554990302, 0.43299409572781156, 0.43005001670752513, 0.44113589863925934, 0.41966972723141754, 0.419747701558697, 0.4201965150133845, 0.41622993772211464, 0.4221522492959319, 0.4152608468981614, 0.41408945054301266, 0.4088190231377391, 0.42720808039895164, 0.43810703695490427, 0.4201133825529275, 0.39687361146884653, 0.39767379837589656, 0.39157147044762475, 0.400467109607704, 0.3899987105978386, 0.387923940496588, 0.40520530121059534, 0.4045859165122187, 0.4078402809420975, 0.3930435827987734, 0.3861842814471421, 0.4041151801950206, 0.38772656828896607, 0.38430867370976085, 0.3838510825720695, 0.3897967302736325, 0.4040686691004247, 0.38877381002870165, 0.3801070718651492, 0.38962688570340914, 0.3746872204012979, 0.3714893911342387, 0.3740742081764724, 0.39146499769229304, 0.3717517574826466, 0.381473414830643, 0.37490687696560177, 0.3714481909789468, 0.36449340912939676, 0.38741472972303553, 0.3746305355739364, 0.38203881485428276, 0.36006603358357453, 0.3602098770290921, 0.36643787289696783, 0.36828266185723085, 0.3775885027362601, 0.37207900133324917, 0.3653085350026284, 0.37328628336625735, 0.36840932538857235, 0.35734048051266915, 0.3538213465691816, 0.35080532779535534, 0.38174812351106696, 0.3647964546496673, 0.4101725113237361, 0.3567889522736323], 'acc': [0.812610148057174, 0.9216926100809767, 0.9247931216077045, 0.9270699196970499, 0.9286590026083493, 0.929623570955519, 0.930588216375774, 0.9309932050299661, 0.9318307851413216, 0.9318205522243843, 0.9319021213038249, 0.9331945001969618, 0.934218931270592, 0.9345156208422513, 0.9352926210570592, 0.9355391590084935, 0.9369290865227892, 0.9402565084949878, 0.9505888340013184, 0.9524583078546518, 0.9527714280298364, 0.9528865723482484, 0.9528284565479171, 0.953209066157167, 0.9532994098634888, 0.9534324277948694, 0.9528370705471861, 0.9534422562849251, 0.9534691165663501, 0.9538942482745492, 0.9540750479638016, 0.9542398093202897, 0.9540998145278152, 0.9539604223589472, 0.9545286940267584, 0.9543861081414771, 0.9547156100564114, 0.9542461340239793, 0.9540625749432349, 0.9543729059040943, 0.9547319593069121, 0.9550573738254684, 0.9551913309641512, 0.9551497454502682, 0.954818411906068, 0.9551726615833673, 0.9552184845649696, 0.9552750100739972, 0.9548737851748583, 0.9548369641013035, 0.9553120551948346, 0.9555903677410914, 0.9552338603276347, 0.9556179074115452, 0.9556621240076326, 0.9559084498901437, 0.9556506563761716, 0.9555905787707961, 0.9558008828822777, 0.9562010016701696, 0.9560720595123298, 0.9564745160425221, 0.9564770622848507, 0.9563546513274985, 0.9559890128625973, 0.9564842954295084, 0.9564126529915669, 0.9565734703364682, 0.956580915556886, 0.9566777785878905, 0.9563529334652996, 0.9567651232739987, 0.956397505234786, 0.9568723822698376, 0.9568615159082643, 0.9569046285535853, 0.9569845748729842, 0.956360249043222, 0.9568506312099556, 0.9569079347790641, 0.9571020349186751, 0.9570660610063876, 0.9572152980398383, 0.9573027231696549, 0.9572581169418742, 0.956827308263875, 0.9573109608077762, 0.9555479678286684, 0.9571500452293883], 'mDice': [0.22586428831028255, 0.40563528524828385, 0.46465328714187937, 0.4957008964187045, 0.5221102791974019, 0.5315087578642065, 0.5530451141959294, 0.5548539841823223, 0.575260400156768, 0.5785587853071585, 0.5776848875962148, 0.5917021059803964, 0.6008469521055858, 0.5974209618817286, 0.6010112041971994, 0.6095084392592348, 0.6129818247064152, 0.6093880444334362, 0.6195779473348396, 0.6251968443940664, 0.62772229296758, 0.6279279211261289, 0.6292754705891912, 0.6321356124863162, 0.6363009282018811, 0.6365610291840806, 0.6308654846606875, 0.6358313517276233, 0.6377007604315347, 0.6414321725809894, 0.644037666541498, 0.6469751498613863, 0.6484871982817093, 0.6458387787967244, 0.6506174304639016, 0.6508428888136052, 0.6513632851802146, 0.6506283817747422, 0.6439158404795067, 0.6466327916799157, 0.6545244757668035, 0.6605387446762198, 0.6611524219700545, 0.6638960924326868, 0.6593051870964787, 0.6649472297441251, 0.6664771327159634, 0.6656345766081289, 0.660826072865983, 0.6569083195660852, 0.6648440505829829, 0.666929731752709, 0.6624297552164554, 0.6690836818728542, 0.66882080660142, 0.6725442687711412, 0.6667971747420364, 0.6630217316268808, 0.668566819525662, 0.6741050433390565, 0.6708747716938911, 0.6780125184271363, 0.6780758585697961, 0.675849265893902, 0.668937452973289, 0.6765190789420059, 0.6747296642042896, 0.6783034494845357, 0.6804314632841304, 0.6807868140748565, 0.6745480629289662, 0.681360252278473, 0.6749210635315089, 0.6834737743073905, 0.684028914076496, 0.6829641665387356, 0.6856977238201662, 0.6770212805976723, 0.6816855148045015, 0.6815639543927134, 0.6859620565612209, 0.6851031930245891, 0.6885803840690371, 0.6903566813545507, 0.6902767298458967, 0.6788166597174461, 0.6880025753503984, 0.6652509073279653, 0.6859741795292741]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:02<00:11,  2.89s/it]predicting test subjects:  40%|████      | 2/5 [00:05<00:08,  2.70s/it]predicting test subjects:  60%|██████    | 3/5 [00:07<00:04,  2.49s/it]predicting test subjects:  80%|████████  | 4/5 [00:09<00:02,  2.33s/it]predicting test subjects: 100%|██████████| 5/5 [00:11<00:00,  2.35s/it]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:02<12:46,  2.89s/it]predicting train subjects:   1%|          | 2/266 [00:05<12:23,  2.82s/it]predicting train subjects:   1%|          | 3/266 [00:07<11:33,  2.64s/it]predicting train subjects:   2%|▏         | 4/266 [00:09<10:38,  2.44s/it]predicting train subjects:   2%|▏         | 5/266 [00:12<10:45,  2.47s/it]predicting train subjects:   2%|▏         | 6/266 [00:15<11:11,  2.58s/it]predicting train subjects:   3%|▎         | 7/266 [00:17<11:21,  2.63s/it]predicting train subjects:   3%|▎         | 8/266 [00:20<11:41,  2.72s/it]predicting train subjects:   3%|▎         | 9/266 [00:23<11:47,  2.75s/it]predicting train subjects:   4%|▍         | 10/266 [00:26<11:47,  2.76s/it]predicting train subjects:   4%|▍         | 11/266 [00:29<11:42,  2.75s/it]predicting train subjects:   5%|▍         | 12/266 [00:31<11:41,  2.76s/it]predicting train subjects:   5%|▍         | 13/266 [00:34<11:44,  2.78s/it]predicting train subjects:   5%|▌         | 14/266 [00:37<11:41,  2.78s/it]predicting train subjects:   6%|▌         | 15/266 [00:40<11:37,  2.78s/it]predicting train subjects:   6%|▌         | 16/266 [00:43<11:46,  2.83s/it]predicting train subjects:   6%|▋         | 17/266 [00:46<11:41,  2.82s/it]predicting train subjects:   7%|▋         | 18/266 [00:48<11:34,  2.80s/it]predicting train subjects:   7%|▋         | 19/266 [00:51<11:33,  2.81s/it]predicting train subjects:   8%|▊         | 20/266 [00:54<11:27,  2.80s/it]predicting train subjects:   8%|▊         | 21/266 [00:57<11:21,  2.78s/it]predicting train subjects:   8%|▊         | 22/266 [00:59<11:21,  2.79s/it]predicting train subjects:   9%|▊         | 23/266 [01:02<11:17,  2.79s/it]predicting train subjects:   9%|▉         | 24/266 [01:05<10:53,  2.70s/it]predicting train subjects:   9%|▉         | 25/266 [01:07<10:44,  2.68s/it]predicting train subjects:  10%|▉         | 26/266 [01:10<10:31,  2.63s/it]predicting train subjects:  10%|█         | 27/266 [01:12<10:22,  2.60s/it]predicting train subjects:  11%|█         | 28/266 [01:15<10:10,  2.56s/it]predicting train subjects:  11%|█         | 29/266 [01:17<10:08,  2.57s/it]predicting train subjects:  11%|█▏        | 30/266 [01:20<09:57,  2.53s/it]predicting train subjects:  12%|█▏        | 31/266 [01:22<09:53,  2.53s/it]predicting train subjects:  12%|█▏        | 32/266 [01:25<09:46,  2.50s/it]predicting train subjects:  12%|█▏        | 33/266 [01:27<09:44,  2.51s/it]predicting train subjects:  13%|█▎        | 34/266 [01:30<09:38,  2.49s/it]predicting train subjects:  13%|█▎        | 35/266 [01:32<09:36,  2.49s/it]predicting train subjects:  14%|█▎        | 36/266 [01:35<09:30,  2.48s/it]predicting train subjects:  14%|█▍        | 37/266 [01:37<09:27,  2.48s/it]predicting train subjects:  14%|█▍        | 38/266 [01:40<09:25,  2.48s/it]predicting train subjects:  15%|█▍        | 39/266 [01:42<09:21,  2.47s/it]predicting train subjects:  15%|█▌        | 40/266 [01:45<09:23,  2.49s/it]predicting train subjects:  15%|█▌        | 41/266 [01:47<09:18,  2.48s/it]predicting train subjects:  16%|█▌        | 42/266 [01:49<08:53,  2.38s/it]predicting train subjects:  16%|█▌        | 43/266 [01:52<08:35,  2.31s/it]predicting train subjects:  17%|█▋        | 44/266 [01:54<08:27,  2.29s/it]predicting train subjects:  17%|█▋        | 45/266 [01:56<08:18,  2.25s/it]predicting train subjects:  17%|█▋        | 46/266 [01:58<08:09,  2.22s/it]predicting train subjects:  18%|█▊        | 47/266 [02:00<08:06,  2.22s/it]predicting train subjects:  18%|█▊        | 48/266 [02:02<08:01,  2.21s/it]predicting train subjects:  18%|█▊        | 49/266 [02:05<07:52,  2.18s/it]predicting train subjects:  19%|█▉        | 50/266 [02:07<07:48,  2.17s/it]predicting train subjects:  19%|█▉        | 51/266 [02:09<07:42,  2.15s/it]predicting train subjects:  20%|█▉        | 52/266 [02:11<07:37,  2.14s/it]predicting train subjects:  20%|█▉        | 53/266 [02:13<07:35,  2.14s/it]predicting train subjects:  20%|██        | 54/266 [02:15<07:33,  2.14s/it]predicting train subjects:  21%|██        | 55/266 [02:17<07:28,  2.12s/it]predicting train subjects:  21%|██        | 56/266 [02:19<07:25,  2.12s/it]predicting train subjects:  21%|██▏       | 57/266 [02:22<07:22,  2.12s/it]predicting train subjects:  22%|██▏       | 58/266 [02:24<07:24,  2.14s/it]predicting train subjects:  22%|██▏       | 59/266 [02:26<07:21,  2.13s/it]predicting train subjects:  23%|██▎       | 60/266 [02:28<07:10,  2.09s/it]predicting train subjects:  23%|██▎       | 61/266 [02:30<07:00,  2.05s/it]predicting train subjects:  23%|██▎       | 62/266 [02:32<06:55,  2.04s/it]predicting train subjects:  24%|██▎       | 63/266 [02:34<06:51,  2.03s/it]predicting train subjects:  24%|██▍       | 64/266 [02:36<06:46,  2.01s/it]predicting train subjects:  24%|██▍       | 65/266 [02:38<06:41,  2.00s/it]predicting train subjects:  25%|██▍       | 66/266 [02:40<06:38,  1.99s/it]predicting train subjects:  25%|██▌       | 67/266 [02:42<06:35,  1.99s/it]predicting train subjects:  26%|██▌       | 68/266 [02:44<06:33,  1.99s/it]predicting train subjects:  26%|██▌       | 69/266 [02:46<06:30,  1.98s/it]predicting train subjects:  26%|██▋       | 70/266 [02:48<06:28,  1.98s/it]predicting train subjects:  27%|██▋       | 71/266 [02:50<06:24,  1.97s/it]predicting train subjects:  27%|██▋       | 72/266 [02:52<06:21,  1.97s/it]predicting train subjects:  27%|██▋       | 73/266 [02:54<06:23,  1.99s/it]predicting train subjects:  28%|██▊       | 74/266 [02:56<06:23,  2.00s/it]predicting train subjects:  28%|██▊       | 75/266 [02:58<06:21,  1.99s/it]predicting train subjects:  29%|██▊       | 76/266 [03:00<06:22,  2.01s/it]predicting train subjects:  29%|██▉       | 77/266 [03:02<06:23,  2.03s/it]predicting train subjects:  29%|██▉       | 78/266 [03:04<06:48,  2.17s/it]predicting train subjects:  30%|██▉       | 79/266 [03:07<07:08,  2.29s/it]predicting train subjects:  30%|███       | 80/266 [03:09<07:23,  2.38s/it]predicting train subjects:  30%|███       | 81/266 [03:12<07:32,  2.44s/it]predicting train subjects:  31%|███       | 82/266 [03:15<07:38,  2.49s/it]predicting train subjects:  31%|███       | 83/266 [03:17<07:36,  2.49s/it]predicting train subjects:  32%|███▏      | 84/266 [03:20<07:40,  2.53s/it]predicting train subjects:  32%|███▏      | 85/266 [03:22<07:37,  2.53s/it]predicting train subjects:  32%|███▏      | 86/266 [03:25<07:36,  2.53s/it]predicting train subjects:  33%|███▎      | 87/266 [03:27<07:35,  2.55s/it]predicting train subjects:  33%|███▎      | 88/266 [03:30<07:33,  2.55s/it]predicting train subjects:  33%|███▎      | 89/266 [03:32<07:35,  2.57s/it]predicting train subjects:  34%|███▍      | 90/266 [03:35<07:32,  2.57s/it]predicting train subjects:  34%|███▍      | 91/266 [03:38<07:27,  2.56s/it]predicting train subjects:  35%|███▍      | 92/266 [03:40<07:21,  2.54s/it]predicting train subjects:  35%|███▍      | 93/266 [03:43<07:19,  2.54s/it]predicting train subjects:  35%|███▌      | 94/266 [03:45<07:20,  2.56s/it]predicting train subjects:  36%|███▌      | 95/266 [03:48<07:15,  2.55s/it]predicting train subjects:  36%|███▌      | 96/266 [03:50<06:57,  2.45s/it]predicting train subjects:  36%|███▋      | 97/266 [03:53<06:58,  2.48s/it]predicting train subjects:  37%|███▋      | 98/266 [03:55<06:56,  2.48s/it]predicting train subjects:  37%|███▋      | 99/266 [03:57<06:25,  2.31s/it]predicting train subjects:  38%|███▊      | 100/266 [03:59<06:11,  2.24s/it]predicting train subjects:  38%|███▊      | 101/266 [04:01<06:09,  2.24s/it]predicting train subjects:  38%|███▊      | 102/266 [04:04<06:13,  2.28s/it]predicting train subjects:  39%|███▊      | 103/266 [04:06<06:10,  2.27s/it]predicting train subjects:  39%|███▉      | 104/266 [04:08<06:04,  2.25s/it]predicting train subjects:  39%|███▉      | 105/266 [04:10<06:01,  2.25s/it]predicting train subjects:  40%|███▉      | 106/266 [04:13<06:00,  2.25s/it]predicting train subjects:  40%|████      | 107/266 [04:15<05:57,  2.25s/it]predicting train subjects:  41%|████      | 108/266 [04:17<05:54,  2.25s/it]predicting train subjects:  41%|████      | 109/266 [04:19<05:50,  2.23s/it]predicting train subjects:  41%|████▏     | 110/266 [04:21<05:49,  2.24s/it]predicting train subjects:  42%|████▏     | 111/266 [04:24<05:47,  2.24s/it]predicting train subjects:  42%|████▏     | 112/266 [04:26<05:47,  2.26s/it]predicting train subjects:  42%|████▏     | 113/266 [04:28<05:47,  2.27s/it]predicting train subjects:  43%|████▎     | 114/266 [04:31<05:45,  2.28s/it]predicting train subjects:  43%|████▎     | 115/266 [04:33<05:46,  2.30s/it]predicting train subjects:  44%|████▎     | 116/266 [04:35<05:46,  2.31s/it]predicting train subjects:  44%|████▍     | 117/266 [04:38<05:43,  2.30s/it]predicting train subjects:  44%|████▍     | 118/266 [04:40<05:36,  2.27s/it]predicting train subjects:  45%|████▍     | 119/266 [04:42<05:48,  2.37s/it]predicting train subjects:  45%|████▌     | 120/266 [04:45<05:56,  2.44s/it]predicting train subjects:  45%|████▌     | 121/266 [04:48<06:00,  2.48s/it]predicting train subjects:  46%|████▌     | 122/266 [04:50<06:00,  2.50s/it]predicting train subjects:  46%|████▌     | 123/266 [04:53<05:57,  2.50s/it]predicting train subjects:  47%|████▋     | 124/266 [04:55<05:56,  2.51s/it]predicting train subjects:  47%|████▋     | 125/266 [04:58<05:55,  2.52s/it]predicting train subjects:  47%|████▋     | 126/266 [05:00<05:54,  2.53s/it]predicting train subjects:  48%|████▊     | 127/266 [05:03<05:53,  2.54s/it]predicting train subjects:  48%|████▊     | 128/266 [05:05<05:51,  2.55s/it]predicting train subjects:  48%|████▊     | 129/266 [05:08<05:49,  2.55s/it]predicting train subjects:  49%|████▉     | 130/266 [05:11<05:47,  2.55s/it]predicting train subjects:  49%|████▉     | 131/266 [05:13<05:47,  2.58s/it]predicting train subjects:  50%|████▉     | 132/266 [05:16<05:45,  2.57s/it]predicting train subjects:  50%|█████     | 133/266 [05:18<05:43,  2.59s/it]predicting train subjects:  50%|█████     | 134/266 [05:21<05:40,  2.58s/it]predicting train subjects:  51%|█████     | 135/266 [05:23<05:36,  2.57s/it]predicting train subjects:  51%|█████     | 136/266 [05:26<05:33,  2.57s/it]predicting train subjects:  52%|█████▏    | 137/266 [05:29<05:29,  2.55s/it]predicting train subjects:  52%|█████▏    | 138/266 [05:31<05:23,  2.53s/it]predicting train subjects:  52%|█████▏    | 139/266 [05:33<05:20,  2.52s/it]predicting train subjects:  53%|█████▎    | 140/266 [05:36<05:16,  2.51s/it]predicting train subjects:  53%|█████▎    | 141/266 [05:38<05:09,  2.48s/it]predicting train subjects:  53%|█████▎    | 142/266 [05:41<05:06,  2.47s/it]predicting train subjects:  54%|█████▍    | 143/266 [05:43<05:02,  2.46s/it]predicting train subjects:  54%|█████▍    | 144/266 [05:46<04:59,  2.46s/it]predicting train subjects:  55%|█████▍    | 145/266 [05:48<04:56,  2.45s/it]predicting train subjects:  55%|█████▍    | 146/266 [05:51<04:54,  2.45s/it]predicting train subjects:  55%|█████▌    | 147/266 [05:53<04:52,  2.46s/it]predicting train subjects:  56%|█████▌    | 148/266 [05:56<04:48,  2.45s/it]predicting train subjects:  56%|█████▌    | 149/266 [05:58<04:49,  2.47s/it]predicting train subjects:  56%|█████▋    | 150/266 [06:01<04:47,  2.48s/it]predicting train subjects:  57%|█████▋    | 151/266 [06:03<04:43,  2.46s/it]predicting train subjects:  57%|█████▋    | 152/266 [06:05<04:39,  2.45s/it]predicting train subjects:  58%|█████▊    | 153/266 [06:08<04:36,  2.45s/it]predicting train subjects:  58%|█████▊    | 154/266 [06:10<04:34,  2.45s/it]predicting train subjects:  58%|█████▊    | 155/266 [06:12<04:12,  2.27s/it]predicting train subjects:  59%|█████▊    | 156/266 [06:14<03:57,  2.15s/it]predicting train subjects:  59%|█████▉    | 157/266 [06:16<03:48,  2.09s/it]predicting train subjects:  59%|█████▉    | 158/266 [06:18<03:39,  2.03s/it]predicting train subjects:  60%|█████▉    | 159/266 [06:20<03:32,  1.98s/it]predicting train subjects:  60%|██████    | 160/266 [06:22<03:26,  1.94s/it]predicting train subjects:  61%|██████    | 161/266 [06:23<03:21,  1.92s/it]predicting train subjects:  61%|██████    | 162/266 [06:25<03:16,  1.89s/it]predicting train subjects:  61%|██████▏   | 163/266 [06:27<03:13,  1.88s/it]predicting train subjects:  62%|██████▏   | 164/266 [06:29<03:11,  1.88s/it]predicting train subjects:  62%|██████▏   | 165/266 [06:31<03:12,  1.90s/it]predicting train subjects:  62%|██████▏   | 166/266 [06:33<03:09,  1.89s/it]predicting train subjects:  63%|██████▎   | 167/266 [06:35<03:05,  1.88s/it]predicting train subjects:  63%|██████▎   | 168/266 [06:37<03:02,  1.87s/it]predicting train subjects:  64%|██████▎   | 169/266 [06:38<03:00,  1.86s/it]predicting train subjects:  64%|██████▍   | 170/266 [06:40<02:59,  1.87s/it]predicting train subjects:  64%|██████▍   | 171/266 [06:42<02:57,  1.86s/it]predicting train subjects:  65%|██████▍   | 172/266 [06:44<02:54,  1.86s/it]predicting train subjects:  65%|██████▌   | 173/266 [06:46<02:59,  1.93s/it]predicting train subjects:  65%|██████▌   | 174/266 [06:48<03:02,  1.98s/it]predicting train subjects:  66%|██████▌   | 175/266 [06:50<03:01,  1.99s/it]predicting train subjects:  66%|██████▌   | 176/266 [06:52<02:59,  2.00s/it]predicting train subjects:  67%|██████▋   | 177/266 [06:54<02:59,  2.01s/it]predicting train subjects:  67%|██████▋   | 178/266 [06:56<02:59,  2.04s/it]predicting train subjects:  67%|██████▋   | 179/266 [06:58<02:59,  2.06s/it]predicting train subjects:  68%|██████▊   | 180/266 [07:00<02:56,  2.05s/it]predicting train subjects:  68%|██████▊   | 181/266 [07:03<02:55,  2.07s/it]predicting train subjects:  68%|██████▊   | 182/266 [07:05<02:52,  2.06s/it]predicting train subjects:  69%|██████▉   | 183/266 [07:07<02:50,  2.05s/it]predicting train subjects:  69%|██████▉   | 184/266 [07:09<02:47,  2.05s/it]predicting train subjects:  70%|██████▉   | 185/266 [07:11<02:45,  2.04s/it]predicting train subjects:  70%|██████▉   | 186/266 [07:13<02:45,  2.07s/it]predicting train subjects:  70%|███████   | 187/266 [07:15<02:42,  2.06s/it]predicting train subjects:  71%|███████   | 188/266 [07:17<02:39,  2.04s/it]predicting train subjects:  71%|███████   | 189/266 [07:19<02:37,  2.04s/it]predicting train subjects:  71%|███████▏  | 190/266 [07:21<02:34,  2.03s/it]predicting train subjects:  72%|███████▏  | 191/266 [07:23<02:36,  2.08s/it]predicting train subjects:  72%|███████▏  | 192/266 [07:25<02:32,  2.06s/it]predicting train subjects:  73%|███████▎  | 193/266 [07:27<02:28,  2.03s/it]predicting train subjects:  73%|███████▎  | 194/266 [07:30<02:36,  2.18s/it]predicting train subjects:  73%|███████▎  | 195/266 [07:32<02:35,  2.19s/it]predicting train subjects:  74%|███████▎  | 196/266 [07:34<02:33,  2.19s/it]predicting train subjects:  74%|███████▍  | 197/266 [07:36<02:30,  2.18s/it]predicting train subjects:  74%|███████▍  | 198/266 [07:38<02:27,  2.17s/it]predicting train subjects:  75%|███████▍  | 199/266 [07:40<02:24,  2.16s/it]predicting train subjects:  75%|███████▌  | 200/266 [07:43<02:22,  2.15s/it]predicting train subjects:  76%|███████▌  | 201/266 [07:45<02:20,  2.16s/it]predicting train subjects:  76%|███████▌  | 202/266 [07:47<02:19,  2.19s/it]predicting train subjects:  76%|███████▋  | 203/266 [07:49<02:20,  2.23s/it]predicting train subjects:  77%|███████▋  | 204/266 [07:52<02:17,  2.22s/it]predicting train subjects:  77%|███████▋  | 205/266 [07:54<02:16,  2.23s/it]predicting train subjects:  77%|███████▋  | 206/266 [07:56<02:13,  2.22s/it]predicting train subjects:  78%|███████▊  | 207/266 [07:58<02:09,  2.20s/it]predicting train subjects:  78%|███████▊  | 208/266 [08:00<02:07,  2.20s/it]predicting train subjects:  79%|███████▊  | 209/266 [08:03<02:06,  2.22s/it]predicting train subjects:  79%|███████▉  | 210/266 [08:05<02:02,  2.19s/it]predicting train subjects:  79%|███████▉  | 211/266 [08:07<02:01,  2.20s/it]predicting train subjects:  80%|███████▉  | 212/266 [08:09<01:59,  2.22s/it]predicting train subjects:  80%|████████  | 213/266 [08:11<01:52,  2.13s/it]predicting train subjects:  80%|████████  | 214/266 [08:13<01:48,  2.08s/it]predicting train subjects:  81%|████████  | 215/266 [08:15<01:44,  2.04s/it]predicting train subjects:  81%|████████  | 216/266 [08:17<01:39,  2.00s/it]predicting train subjects:  82%|████████▏ | 217/266 [08:19<01:36,  1.98s/it]predicting train subjects:  82%|████████▏ | 218/266 [08:21<01:33,  1.96s/it]predicting train subjects:  82%|████████▏ | 219/266 [08:23<01:32,  1.96s/it]predicting train subjects:  83%|████████▎ | 220/266 [08:25<01:28,  1.93s/it]predicting train subjects:  83%|████████▎ | 221/266 [08:27<01:26,  1.93s/it]predicting train subjects:  83%|████████▎ | 222/266 [08:28<01:24,  1.92s/it]predicting train subjects:  84%|████████▍ | 223/266 [08:30<01:22,  1.91s/it]predicting train subjects:  84%|████████▍ | 224/266 [08:32<01:20,  1.92s/it]predicting train subjects:  85%|████████▍ | 225/266 [08:34<01:18,  1.92s/it]predicting train subjects:  85%|████████▍ | 226/266 [08:36<01:17,  1.93s/it]predicting train subjects:  85%|████████▌ | 227/266 [08:38<01:16,  1.95s/it]predicting train subjects:  86%|████████▌ | 228/266 [08:40<01:13,  1.94s/it]predicting train subjects:  86%|████████▌ | 229/266 [08:42<01:11,  1.94s/it]predicting train subjects:  86%|████████▋ | 230/266 [08:44<01:09,  1.94s/it]predicting train subjects:  87%|████████▋ | 231/266 [08:46<01:07,  1.94s/it]predicting train subjects:  87%|████████▋ | 232/266 [08:48<01:06,  1.94s/it]predicting train subjects:  88%|████████▊ | 233/266 [08:50<01:03,  1.94s/it]predicting train subjects:  88%|████████▊ | 234/266 [08:52<01:03,  1.97s/it]predicting train subjects:  88%|████████▊ | 235/266 [08:54<01:01,  1.97s/it]predicting train subjects:  89%|████████▊ | 236/266 [08:56<00:59,  1.98s/it]predicting train subjects:  89%|████████▉ | 237/266 [08:58<00:57,  1.97s/it]predicting train subjects:  89%|████████▉ | 238/266 [09:00<00:55,  1.99s/it]predicting train subjects:  90%|████████▉ | 239/266 [09:02<00:53,  2.00s/it]predicting train subjects:  90%|█████████ | 240/266 [09:04<00:52,  2.01s/it]predicting train subjects:  91%|█████████ | 241/266 [09:06<00:50,  2.02s/it]predicting train subjects:  91%|█████████ | 242/266 [09:08<00:48,  2.01s/it]predicting train subjects:  91%|█████████▏| 243/266 [09:10<00:46,  2.01s/it]predicting train subjects:  92%|█████████▏| 244/266 [09:12<00:44,  2.02s/it]predicting train subjects:  92%|█████████▏| 245/266 [09:14<00:42,  2.01s/it]predicting train subjects:  92%|█████████▏| 246/266 [09:16<00:40,  2.01s/it]predicting train subjects:  93%|█████████▎| 247/266 [09:18<00:37,  1.98s/it]predicting train subjects:  93%|█████████▎| 248/266 [09:20<00:35,  1.97s/it]predicting train subjects:  94%|█████████▎| 249/266 [09:22<00:36,  2.15s/it]predicting train subjects:  94%|█████████▍| 250/266 [09:25<00:36,  2.27s/it]predicting train subjects:  94%|█████████▍| 251/266 [09:27<00:35,  2.34s/it]predicting train subjects:  95%|█████████▍| 252/266 [09:30<00:33,  2.39s/it]predicting train subjects:  95%|█████████▌| 253/266 [09:32<00:31,  2.41s/it]predicting train subjects:  95%|█████████▌| 254/266 [09:35<00:29,  2.43s/it]predicting train subjects:  96%|█████████▌| 255/266 [09:37<00:27,  2.47s/it]predicting train subjects:  96%|█████████▌| 256/266 [09:40<00:25,  2.51s/it]predicting train subjects:  97%|█████████▋| 257/266 [09:43<00:22,  2.52s/it]predicting train subjects:  97%|█████████▋| 258/266 [09:45<00:20,  2.51s/it]predicting train subjects:  97%|█████████▋| 259/266 [09:48<00:17,  2.52s/it]predicting train subjects:  98%|█████████▊| 260/266 [09:50<00:15,  2.50s/it]predicting train subjects:  98%|█████████▊| 261/266 [09:53<00:12,  2.51s/it]predicting train subjects:  98%|█████████▊| 262/266 [09:55<00:10,  2.50s/it]predicting train subjects:  99%|█████████▉| 263/266 [09:58<00:07,  2.54s/it]predicting train subjects:  99%|█████████▉| 264/266 [10:00<00:05,  2.53s/it]predicting train subjects: 100%|█████████▉| 265/266 [10:03<00:02,  2.52s/it]predicting train subjects: 100%|██████████| 266/266 [10:05<00:00,  2.53s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<07:28,  1.69s/it]Loading train:   1%|          | 2/266 [00:03<07:14,  1.65s/it]Loading train:   1%|          | 3/266 [00:04<06:46,  1.55s/it]Loading train:   2%|▏         | 4/266 [00:05<06:19,  1.45s/it]Loading train:   2%|▏         | 5/266 [00:07<06:26,  1.48s/it]Loading train:   2%|▏         | 6/266 [00:08<05:48,  1.34s/it]Loading train:   3%|▎         | 7/266 [00:09<05:23,  1.25s/it]Loading train:   3%|▎         | 8/266 [00:10<04:56,  1.15s/it]Loading train:   3%|▎         | 9/266 [00:11<04:42,  1.10s/it]Loading train:   4%|▍         | 10/266 [00:12<04:35,  1.08s/it]Loading train:   4%|▍         | 11/266 [00:13<04:32,  1.07s/it]Loading train:   5%|▍         | 12/266 [00:14<04:29,  1.06s/it]Loading train:   5%|▍         | 13/266 [00:15<04:26,  1.05s/it]Loading train:   5%|▌         | 14/266 [00:16<04:18,  1.02s/it]Loading train:   6%|▌         | 15/266 [00:17<04:16,  1.02s/it]Loading train:   6%|▌         | 16/266 [00:18<04:12,  1.01s/it]Loading train:   6%|▋         | 17/266 [00:19<04:07,  1.00it/s]Loading train:   7%|▋         | 18/266 [00:20<04:12,  1.02s/it]Loading train:   7%|▋         | 19/266 [00:21<04:15,  1.03s/it]Loading train:   8%|▊         | 20/266 [00:22<04:06,  1.00s/it]Loading train:   8%|▊         | 21/266 [00:23<04:08,  1.01s/it]Loading train:   8%|▊         | 22/266 [00:24<04:02,  1.00it/s]Loading train:   9%|▊         | 23/266 [00:25<04:01,  1.01it/s]Loading train:   9%|▉         | 24/266 [00:26<04:13,  1.05s/it]Loading train:   9%|▉         | 25/266 [00:27<04:02,  1.00s/it]Loading train:  10%|▉         | 26/266 [00:28<03:53,  1.03it/s]Loading train:  10%|█         | 27/266 [00:29<03:52,  1.03it/s]Loading train:  11%|█         | 28/266 [00:30<03:44,  1.06it/s]Loading train:  11%|█         | 29/266 [00:31<03:39,  1.08it/s]Loading train:  11%|█▏        | 30/266 [00:31<03:32,  1.11it/s]Loading train:  12%|█▏        | 31/266 [00:32<03:29,  1.12it/s]Loading train:  12%|█▏        | 32/266 [00:33<03:25,  1.14it/s]Loading train:  12%|█▏        | 33/266 [00:34<03:25,  1.13it/s]Loading train:  13%|█▎        | 34/266 [00:35<03:23,  1.14it/s]Loading train:  13%|█▎        | 35/266 [00:36<03:22,  1.14it/s]Loading train:  14%|█▎        | 36/266 [00:37<03:22,  1.14it/s]Loading train:  14%|█▍        | 37/266 [00:38<03:23,  1.12it/s]Loading train:  14%|█▍        | 38/266 [00:38<03:23,  1.12it/s]Loading train:  15%|█▍        | 39/266 [00:39<03:22,  1.12it/s]Loading train:  15%|█▌        | 40/266 [00:40<03:20,  1.13it/s]Loading train:  15%|█▌        | 41/266 [00:41<03:24,  1.10it/s]Loading train:  16%|█▌        | 42/266 [00:42<03:16,  1.14it/s]Loading train:  16%|█▌        | 43/266 [00:43<03:07,  1.19it/s]Loading train:  17%|█▋        | 44/266 [00:44<03:03,  1.21it/s]Loading train:  17%|█▋        | 45/266 [00:44<02:58,  1.24it/s]Loading train:  17%|█▋        | 46/266 [00:45<02:51,  1.28it/s]Loading train:  18%|█▊        | 47/266 [00:46<02:51,  1.28it/s]Loading train:  18%|█▊        | 48/266 [00:47<02:54,  1.25it/s]Loading train:  18%|█▊        | 49/266 [00:47<02:49,  1.28it/s]Loading train:  19%|█▉        | 50/266 [00:48<02:48,  1.28it/s]Loading train:  19%|█▉        | 51/266 [00:49<02:46,  1.29it/s]Loading train:  20%|█▉        | 52/266 [00:50<02:43,  1.31it/s]Loading train:  20%|█▉        | 53/266 [00:50<02:41,  1.32it/s]Loading train:  20%|██        | 54/266 [00:51<02:40,  1.32it/s]Loading train:  21%|██        | 55/266 [00:52<02:39,  1.32it/s]Loading train:  21%|██        | 56/266 [00:53<02:44,  1.28it/s]Loading train:  21%|██▏       | 57/266 [00:54<02:45,  1.26it/s]Loading train:  22%|██▏       | 58/266 [00:54<02:41,  1.28it/s]Loading train:  22%|██▏       | 59/266 [00:55<02:40,  1.29it/s]Loading train:  23%|██▎       | 60/266 [00:56<02:42,  1.27it/s]Loading train:  23%|██▎       | 61/266 [00:57<02:36,  1.31it/s]Loading train:  23%|██▎       | 62/266 [00:57<02:38,  1.29it/s]Loading train:  24%|██▎       | 63/266 [00:58<02:34,  1.31it/s]Loading train:  24%|██▍       | 64/266 [00:59<02:29,  1.35it/s]Loading train:  24%|██▍       | 65/266 [01:00<02:32,  1.32it/s]Loading train:  25%|██▍       | 66/266 [01:00<02:28,  1.35it/s]Loading train:  25%|██▌       | 67/266 [01:01<02:27,  1.35it/s]Loading train:  26%|██▌       | 68/266 [01:02<02:29,  1.33it/s]Loading train:  26%|██▌       | 69/266 [01:03<02:29,  1.32it/s]Loading train:  26%|██▋       | 70/266 [01:03<02:26,  1.34it/s]Loading train:  27%|██▋       | 71/266 [01:04<02:27,  1.32it/s]Loading train:  27%|██▋       | 72/266 [01:05<02:30,  1.29it/s]Loading train:  27%|██▋       | 73/266 [01:06<02:32,  1.26it/s]Loading train:  28%|██▊       | 74/266 [01:07<02:30,  1.28it/s]Loading train:  28%|██▊       | 75/266 [01:07<02:25,  1.31it/s]Loading train:  29%|██▊       | 76/266 [01:08<02:24,  1.32it/s]Loading train:  29%|██▉       | 77/266 [01:09<02:25,  1.30it/s]Loading train:  29%|██▉       | 78/266 [01:10<02:41,  1.16it/s]Loading train:  30%|██▉       | 79/266 [01:11<02:47,  1.12it/s]Loading train:  30%|███       | 80/266 [01:12<02:47,  1.11it/s]Loading train:  30%|███       | 81/266 [01:13<02:49,  1.09it/s]Loading train:  31%|███       | 82/266 [01:14<02:47,  1.10it/s]Loading train:  31%|███       | 83/266 [01:15<02:47,  1.10it/s]Loading train:  32%|███▏      | 84/266 [01:16<02:48,  1.08it/s]Loading train:  32%|███▏      | 85/266 [01:16<02:48,  1.08it/s]Loading train:  32%|███▏      | 86/266 [01:17<02:45,  1.08it/s]Loading train:  33%|███▎      | 87/266 [01:18<02:44,  1.09it/s]Loading train:  33%|███▎      | 88/266 [01:19<02:43,  1.09it/s]Loading train:  33%|███▎      | 89/266 [01:20<02:41,  1.10it/s]Loading train:  34%|███▍      | 90/266 [01:21<02:41,  1.09it/s]Loading train:  34%|███▍      | 91/266 [01:22<02:37,  1.11it/s]Loading train:  35%|███▍      | 92/266 [01:23<02:36,  1.11it/s]Loading train:  35%|███▍      | 93/266 [01:24<02:34,  1.12it/s]Loading train:  35%|███▌      | 94/266 [01:24<02:30,  1.14it/s]Loading train:  36%|███▌      | 95/266 [01:25<02:35,  1.10it/s]Loading train:  36%|███▌      | 96/266 [01:27<02:52,  1.01s/it]Loading train:  36%|███▋      | 97/266 [01:28<03:15,  1.16s/it]Loading train:  37%|███▋      | 98/266 [01:30<03:36,  1.29s/it]Loading train:  37%|███▋      | 99/266 [01:31<03:29,  1.25s/it]Loading train:  38%|███▊      | 100/266 [01:32<03:35,  1.30s/it]Loading train:  38%|███▊      | 101/266 [01:33<03:16,  1.19s/it]Loading train:  38%|███▊      | 102/266 [01:34<02:59,  1.09s/it]Loading train:  39%|███▊      | 103/266 [01:35<02:43,  1.00s/it]Loading train:  39%|███▉      | 104/266 [01:36<02:32,  1.07it/s]Loading train:  39%|███▉      | 105/266 [01:37<02:26,  1.10it/s]Loading train:  40%|███▉      | 106/266 [01:38<02:26,  1.09it/s]Loading train:  40%|████      | 107/266 [01:38<02:19,  1.14it/s]Loading train:  41%|████      | 108/266 [01:39<02:15,  1.16it/s]Loading train:  41%|████      | 109/266 [01:40<02:12,  1.18it/s]Loading train:  41%|████▏     | 110/266 [01:41<02:11,  1.19it/s]Loading train:  42%|████▏     | 111/266 [01:42<02:13,  1.16it/s]Loading train:  42%|████▏     | 112/266 [01:43<02:11,  1.17it/s]Loading train:  42%|████▏     | 113/266 [01:43<02:13,  1.15it/s]Loading train:  43%|████▎     | 114/266 [01:44<02:08,  1.19it/s]Loading train:  43%|████▎     | 115/266 [01:45<02:05,  1.20it/s]Loading train:  44%|████▎     | 116/266 [01:46<02:05,  1.19it/s]Loading train:  44%|████▍     | 117/266 [01:47<01:59,  1.24it/s]Loading train:  44%|████▍     | 118/266 [01:47<01:57,  1.26it/s]Loading train:  45%|████▍     | 119/266 [01:48<02:10,  1.13it/s]Loading train:  45%|████▌     | 120/266 [01:49<02:15,  1.08it/s]Loading train:  45%|████▌     | 121/266 [01:50<02:12,  1.10it/s]Loading train:  46%|████▌     | 122/266 [01:51<02:09,  1.11it/s]Loading train:  46%|████▌     | 123/266 [01:52<02:07,  1.12it/s]Loading train:  47%|████▋     | 124/266 [01:53<02:04,  1.14it/s]Loading train:  47%|████▋     | 125/266 [01:54<02:02,  1.15it/s]Loading train:  47%|████▋     | 126/266 [01:55<02:00,  1.16it/s]Loading train:  48%|████▊     | 127/266 [01:56<02:00,  1.15it/s]Loading train:  48%|████▊     | 128/266 [01:56<02:01,  1.13it/s]Loading train:  48%|████▊     | 129/266 [01:57<02:04,  1.10it/s]Loading train:  49%|████▉     | 130/266 [01:58<02:04,  1.09it/s]Loading train:  49%|████▉     | 131/266 [01:59<02:05,  1.08it/s]Loading train:  50%|████▉     | 132/266 [02:00<02:06,  1.06it/s]Loading train:  50%|█████     | 133/266 [02:01<02:04,  1.07it/s]Loading train:  50%|█████     | 134/266 [02:02<02:03,  1.07it/s]Loading train:  51%|█████     | 135/266 [02:03<02:02,  1.07it/s]Loading train:  51%|█████     | 136/266 [02:04<02:00,  1.08it/s]Loading train:  52%|█████▏    | 137/266 [02:05<02:11,  1.02s/it]Loading train:  52%|█████▏    | 138/266 [02:06<02:08,  1.01s/it]Loading train:  52%|█████▏    | 139/266 [02:07<02:05,  1.01it/s]Loading train:  53%|█████▎    | 140/266 [02:08<02:03,  1.02it/s]Loading train:  53%|█████▎    | 141/266 [02:09<02:05,  1.00s/it]Loading train:  53%|█████▎    | 142/266 [02:10<02:01,  1.02it/s]Loading train:  54%|█████▍    | 143/266 [02:11<01:57,  1.05it/s]Loading train:  54%|█████▍    | 144/266 [02:12<01:54,  1.06it/s]Loading train:  55%|█████▍    | 145/266 [02:13<01:54,  1.06it/s]Loading train:  55%|█████▍    | 146/266 [02:14<01:54,  1.05it/s]Loading train:  55%|█████▌    | 147/266 [02:15<01:52,  1.06it/s]Loading train:  56%|█████▌    | 148/266 [02:16<01:49,  1.08it/s]Loading train:  56%|█████▌    | 149/266 [02:17<01:47,  1.09it/s]Loading train:  56%|█████▋    | 150/266 [02:17<01:45,  1.10it/s]Loading train:  57%|█████▋    | 151/266 [02:18<01:44,  1.10it/s]Loading train:  57%|█████▋    | 152/266 [02:19<01:44,  1.09it/s]Loading train:  58%|█████▊    | 153/266 [02:20<01:46,  1.06it/s]Loading train:  58%|█████▊    | 154/266 [02:21<01:44,  1.08it/s]Loading train:  58%|█████▊    | 155/266 [02:22<01:39,  1.12it/s]Loading train:  59%|█████▊    | 156/266 [02:23<01:33,  1.18it/s]Loading train:  59%|█████▉    | 157/266 [02:23<01:28,  1.24it/s]Loading train:  59%|█████▉    | 158/266 [02:24<01:23,  1.30it/s]Loading train:  60%|█████▉    | 159/266 [02:25<01:18,  1.36it/s]Loading train:  60%|██████    | 160/266 [02:25<01:15,  1.41it/s]Loading train:  61%|██████    | 161/266 [02:26<01:13,  1.43it/s]Loading train:  61%|██████    | 162/266 [02:27<01:14,  1.40it/s]Loading train:  61%|██████▏   | 163/266 [02:28<01:15,  1.37it/s]Loading train:  62%|██████▏   | 164/266 [02:28<01:14,  1.37it/s]Loading train:  62%|██████▏   | 165/266 [02:29<01:13,  1.38it/s]Loading train:  62%|██████▏   | 166/266 [02:30<01:10,  1.42it/s]Loading train:  63%|██████▎   | 167/266 [02:30<01:09,  1.42it/s]Loading train:  63%|██████▎   | 168/266 [02:31<01:09,  1.40it/s]Loading train:  64%|██████▎   | 169/266 [02:32<01:10,  1.37it/s]Loading train:  64%|██████▍   | 170/266 [02:33<01:08,  1.40it/s]Loading train:  64%|██████▍   | 171/266 [02:33<01:09,  1.38it/s]Loading train:  65%|██████▍   | 172/266 [02:34<01:07,  1.39it/s]Loading train:  65%|██████▌   | 173/266 [02:35<01:10,  1.31it/s]Loading train:  65%|██████▌   | 174/266 [02:36<01:10,  1.30it/s]Loading train:  66%|██████▌   | 175/266 [02:36<01:09,  1.31it/s]Loading train:  66%|██████▌   | 176/266 [02:37<01:07,  1.34it/s]Loading train:  67%|██████▋   | 177/266 [02:38<01:07,  1.32it/s]Loading train:  67%|██████▋   | 178/266 [02:39<01:09,  1.26it/s]Loading train:  67%|██████▋   | 179/266 [02:40<01:06,  1.30it/s]Loading train:  68%|██████▊   | 180/266 [02:40<01:03,  1.34it/s]Loading train:  68%|██████▊   | 181/266 [02:41<01:03,  1.34it/s]Loading train:  68%|██████▊   | 182/266 [02:42<01:05,  1.28it/s]Loading train:  69%|██████▉   | 183/266 [02:43<01:02,  1.32it/s]Loading train:  69%|██████▉   | 184/266 [02:43<01:02,  1.32it/s]Loading train:  70%|██████▉   | 185/266 [02:44<01:01,  1.32it/s]Loading train:  70%|██████▉   | 186/266 [02:45<01:00,  1.32it/s]Loading train:  70%|███████   | 187/266 [02:45<00:58,  1.36it/s]Loading train:  71%|███████   | 188/266 [02:46<00:56,  1.38it/s]Loading train:  71%|███████   | 189/266 [02:47<00:55,  1.40it/s]Loading train:  71%|███████▏  | 190/266 [02:48<00:55,  1.38it/s]Loading train:  72%|███████▏  | 191/266 [02:49<01:06,  1.13it/s]Loading train:  72%|███████▏  | 192/266 [02:50<01:12,  1.02it/s]Loading train:  73%|███████▎  | 193/266 [02:51<01:17,  1.06s/it]Loading train:  73%|███████▎  | 194/266 [02:53<01:22,  1.14s/it]Loading train:  73%|███████▎  | 195/266 [02:54<01:15,  1.06s/it]Loading train:  74%|███████▎  | 196/266 [02:54<01:08,  1.02it/s]Loading train:  74%|███████▍  | 197/266 [02:55<01:04,  1.08it/s]Loading train:  74%|███████▍  | 198/266 [02:56<01:00,  1.13it/s]Loading train:  75%|███████▍  | 199/266 [02:57<00:56,  1.18it/s]Loading train:  75%|███████▌  | 200/266 [02:57<00:54,  1.22it/s]Loading train:  76%|███████▌  | 201/266 [02:58<00:53,  1.22it/s]Loading train:  76%|███████▌  | 202/266 [02:59<00:51,  1.24it/s]Loading train:  76%|███████▋  | 203/266 [03:00<00:49,  1.26it/s]Loading train:  77%|███████▋  | 204/266 [03:01<00:48,  1.28it/s]Loading train:  77%|███████▋  | 205/266 [03:01<00:47,  1.29it/s]Loading train:  77%|███████▋  | 206/266 [03:02<00:46,  1.29it/s]Loading train:  78%|███████▊  | 207/266 [03:03<00:46,  1.27it/s]Loading train:  78%|███████▊  | 208/266 [03:04<00:45,  1.26it/s]Loading train:  79%|███████▊  | 209/266 [03:05<00:45,  1.24it/s]Loading train:  79%|███████▉  | 210/266 [03:05<00:45,  1.23it/s]Loading train:  79%|███████▉  | 211/266 [03:06<00:44,  1.23it/s]Loading train:  80%|███████▉  | 212/266 [03:07<00:43,  1.25it/s]Loading train:  80%|████████  | 213/266 [03:08<00:43,  1.21it/s]Loading train:  80%|████████  | 214/266 [03:09<00:42,  1.22it/s]Loading train:  81%|████████  | 215/266 [03:09<00:40,  1.26it/s]Loading train:  81%|████████  | 216/266 [03:10<00:41,  1.21it/s]Loading train:  82%|████████▏ | 217/266 [03:11<00:40,  1.21it/s]Loading train:  82%|████████▏ | 218/266 [03:12<00:38,  1.24it/s]Loading train:  82%|████████▏ | 219/266 [03:13<00:37,  1.25it/s]Loading train:  83%|████████▎ | 220/266 [03:14<00:37,  1.22it/s]Loading train:  83%|████████▎ | 221/266 [03:14<00:36,  1.24it/s]Loading train:  83%|████████▎ | 222/266 [03:15<00:34,  1.28it/s]Loading train:  84%|████████▍ | 223/266 [03:16<00:33,  1.29it/s]Loading train:  84%|████████▍ | 224/266 [03:17<00:32,  1.30it/s]Loading train:  85%|████████▍ | 225/266 [03:17<00:31,  1.28it/s]Loading train:  85%|████████▍ | 226/266 [03:18<00:30,  1.32it/s]Loading train:  85%|████████▌ | 227/266 [03:19<00:29,  1.32it/s]Loading train:  86%|████████▌ | 228/266 [03:20<00:29,  1.28it/s]Loading train:  86%|████████▌ | 229/266 [03:20<00:28,  1.30it/s]Loading train:  86%|████████▋ | 230/266 [03:21<00:27,  1.30it/s]Loading train:  87%|████████▋ | 231/266 [03:22<00:28,  1.24it/s]Loading train:  87%|████████▋ | 232/266 [03:23<00:26,  1.30it/s]Loading train:  88%|████████▊ | 233/266 [03:24<00:26,  1.27it/s]Loading train:  88%|████████▊ | 234/266 [03:24<00:24,  1.30it/s]Loading train:  88%|████████▊ | 235/266 [03:25<00:23,  1.34it/s]Loading train:  89%|████████▊ | 236/266 [03:26<00:21,  1.37it/s]Loading train:  89%|████████▉ | 237/266 [03:26<00:20,  1.39it/s]Loading train:  89%|████████▉ | 238/266 [03:27<00:20,  1.36it/s]Loading train:  90%|████████▉ | 239/266 [03:28<00:20,  1.34it/s]Loading train:  90%|█████████ | 240/266 [03:29<00:19,  1.34it/s]Loading train:  91%|█████████ | 241/266 [03:29<00:18,  1.36it/s]Loading train:  91%|█████████ | 242/266 [03:30<00:17,  1.34it/s]Loading train:  91%|█████████▏| 243/266 [03:31<00:17,  1.35it/s]Loading train:  92%|█████████▏| 244/266 [03:32<00:16,  1.36it/s]Loading train:  92%|█████████▏| 245/266 [03:32<00:15,  1.40it/s]Loading train:  92%|█████████▏| 246/266 [03:33<00:14,  1.42it/s]Loading train:  93%|█████████▎| 247/266 [03:34<00:13,  1.40it/s]Loading train:  93%|█████████▎| 248/266 [03:34<00:13,  1.37it/s]Loading train:  94%|█████████▎| 249/266 [03:35<00:13,  1.26it/s]Loading train:  94%|█████████▍| 250/266 [03:36<00:13,  1.22it/s]Loading train:  94%|█████████▍| 251/266 [03:37<00:12,  1.17it/s]Loading train:  95%|█████████▍| 252/266 [03:38<00:11,  1.17it/s]Loading train:  95%|█████████▌| 253/266 [03:39<00:10,  1.18it/s]Loading train:  95%|█████████▌| 254/266 [03:40<00:10,  1.14it/s]Loading train:  96%|█████████▌| 255/266 [03:41<00:09,  1.14it/s]Loading train:  96%|█████████▌| 256/266 [03:42<00:09,  1.09it/s]Loading train:  97%|█████████▋| 257/266 [03:43<00:08,  1.12it/s]Loading train:  97%|█████████▋| 258/266 [03:43<00:07,  1.13it/s]Loading train:  97%|█████████▋| 259/266 [03:44<00:06,  1.15it/s]Loading train:  98%|█████████▊| 260/266 [03:45<00:05,  1.14it/s]Loading train:  98%|█████████▊| 261/266 [03:46<00:04,  1.13it/s]Loading train:  98%|█████████▊| 262/266 [03:47<00:03,  1.14it/s]Loading train:  99%|█████████▉| 263/266 [03:48<00:02,  1.14it/s]Loading train:  99%|█████████▉| 264/266 [03:49<00:01,  1.17it/s]Loading train: 100%|█████████▉| 265/266 [03:49<00:00,  1.18it/s]Loading train: 100%|██████████| 266/266 [03:50<00:00,  1.20it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   3%|▎         | 7/266 [00:00<00:03, 66.97it/s]concatenating: train:   6%|▋         | 17/266 [00:00<00:03, 73.59it/s]concatenating: train:  10%|█         | 27/266 [00:00<00:03, 79.54it/s]concatenating: train:  14%|█▍        | 37/266 [00:00<00:02, 84.54it/s]concatenating: train:  17%|█▋        | 45/266 [00:00<00:02, 79.27it/s]concatenating: train:  20%|██        | 54/266 [00:00<00:02, 80.76it/s]concatenating: train:  24%|██▎       | 63/266 [00:00<00:02, 82.33it/s]concatenating: train:  27%|██▋       | 73/266 [00:00<00:02, 86.07it/s]concatenating: train:  31%|███       | 82/266 [00:00<00:02, 86.74it/s]concatenating: train:  36%|███▌      | 95/266 [00:01<00:01, 94.47it/s]concatenating: train:  39%|███▉      | 105/266 [00:01<00:01, 94.70it/s]concatenating: train:  44%|████▍     | 117/266 [00:01<00:01, 98.46it/s]concatenating: train:  48%|████▊     | 127/266 [00:01<00:01, 97.94it/s]concatenating: train:  53%|█████▎    | 141/266 [00:01<00:01, 105.64it/s]concatenating: train:  58%|█████▊    | 154/266 [00:01<00:01, 108.94it/s]concatenating: train:  62%|██████▏   | 166/266 [00:01<00:00, 107.22it/s]concatenating: train:  67%|██████▋   | 177/266 [00:01<00:00, 101.67it/s]concatenating: train:  71%|███████   | 189/266 [00:01<00:00, 106.06it/s]concatenating: train:  75%|███████▌  | 200/266 [00:02<00:00, 101.35it/s]concatenating: train:  80%|████████  | 213/266 [00:02<00:00, 105.45it/s]concatenating: train:  84%|████████▍ | 224/266 [00:02<00:00, 104.40it/s]concatenating: train:  88%|████████▊ | 235/266 [00:02<00:00, 103.78it/s]concatenating: train:  92%|█████████▏| 246/266 [00:02<00:00, 92.36it/s] concatenating: train: 100%|██████████| 266/266 [00:02<00:00, 101.52it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:01<00:04,  1.23s/it]Loading test:  40%|████      | 2/5 [00:02<00:03,  1.21s/it]Loading test:  60%|██████    | 3/5 [00:03<00:02,  1.16s/it]Loading test:  80%|████████  | 4/5 [00:04<00:01,  1.14s/it]Loading test: 100%|██████████| 5/5 [00:05<00:00,  1.21s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 309.17it/s]2019-08-17 02:58:12.928608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-17 02:58:12.928715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-17 02:58:12.928732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-17 02:58:12.928742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-17 02:58:12.929145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:08,  5.35it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:06,  6.31it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:06,  5.96it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:04,  7.61it/s]loading the weights for Res Unet:  20%|██        | 9/44 [00:00<00:05,  6.73it/s]loading the weights for Res Unet:  25%|██▌       | 11/44 [00:01<00:04,  7.65it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:04,  6.89it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:03,  8.90it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02,  9.38it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:02<00:03,  7.63it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:01,  9.51it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01,  9.89it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01, 10.02it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:02<00:01,  7.94it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:03<00:00,  9.79it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:03<00:00, 10.25it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00, 10.53it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:03<00:00,  8.14it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:03<00:00, 11.34it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 3  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 3  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 3  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 96, 64, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 96, 64, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 96, 64, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 96, 64, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 96, 64, 40)   14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 96, 64, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 96, 64, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 48, 32, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 48, 32, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 48, 32, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 48, 32, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 48, 32, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 48, 32, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 48, 32, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 48, 32, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 48, 32, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 24, 16, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 24, 16, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 24, 16, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 24, 16, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 24, 16, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 24, 16, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 24, 16, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 24, 16, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 24, 16, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 24, 16, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 48, 32, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 48, 32, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 48, 32, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 48, 32, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 48, 32, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 48, 32, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 48, 32, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 48, 32, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 48, 32, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 48, 32, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 96, 64, 40)   44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 96, 64, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 96, 64, 40)   28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 96, 64, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 96, 64, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 96, 64, 40)   14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 96, 64, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 96, 64, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 96, 64, 120)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 96, 64, 120)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 96, 64, 13)   1573        dropout_5[0][0]                  
==================================================================================================
Total params: 889,253
Trainable params: 91,293
Non-trainable params: 797,960
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.33628492e-02 3.28400850e-02 7.67920220e-02 9.54179487e-03
 2.76161271e-02 7.22498122e-03 8.47429414e-02 1.14138698e-01
 8.96214069e-02 1.36166104e-02 2.90570229e-01 1.89766573e-01
 1.65682294e-04]
Train on 10325 samples, validate on 192 samples
Epoch 1/300
 - 24s - loss: 2.3280 - acc: 0.7736 - mDice: 0.1446 - val_loss: 1.1624 - val_acc: 0.9395 - val_mDice: 0.3421

Epoch 00001: val_mDice improved from -inf to 0.34210, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 19s - loss: 1.1730 - acc: 0.9196 - mDice: 0.3057 - val_loss: 0.9151 - val_acc: 0.9429 - val_mDice: 0.4169

Epoch 00002: val_mDice improved from 0.34210 to 0.41685, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 20s - loss: 0.9409 - acc: 0.9222 - mDice: 0.3793 - val_loss: 0.7881 - val_acc: 0.9454 - val_mDice: 0.4630

Epoch 00003: val_mDice improved from 0.41685 to 0.46298, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 19s - loss: 0.8275 - acc: 0.9240 - mDice: 0.4236 - val_loss: 0.6977 - val_acc: 0.9476 - val_mDice: 0.4918

Epoch 00004: val_mDice improved from 0.46298 to 0.49177, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 19s - loss: 0.7609 - acc: 0.9261 - mDice: 0.4539 - val_loss: 0.6432 - val_acc: 0.9484 - val_mDice: 0.5167

Epoch 00005: val_mDice improved from 0.49177 to 0.51667, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 19s - loss: 0.7223 - acc: 0.9281 - mDice: 0.4736 - val_loss: 0.6186 - val_acc: 0.9488 - val_mDice: 0.5273

Epoch 00006: val_mDice improved from 0.51667 to 0.52727, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 19s - loss: 0.6909 - acc: 0.9296 - mDice: 0.4892 - val_loss: 0.6253 - val_acc: 0.9527 - val_mDice: 0.5257

Epoch 00007: val_mDice did not improve from 0.52727
Epoch 8/300
 - 19s - loss: 0.6558 - acc: 0.9312 - mDice: 0.5051 - val_loss: 0.5929 - val_acc: 0.9551 - val_mDice: 0.5421

Epoch 00008: val_mDice improved from 0.52727 to 0.54209, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 20s - loss: 0.6368 - acc: 0.9331 - mDice: 0.5176 - val_loss: 0.6014 - val_acc: 0.9558 - val_mDice: 0.5409

Epoch 00009: val_mDice did not improve from 0.54209
Epoch 10/300
 - 20s - loss: 0.6162 - acc: 0.9355 - mDice: 0.5255 - val_loss: 0.6016 - val_acc: 0.9580 - val_mDice: 0.5471

Epoch 00010: val_mDice improved from 0.54209 to 0.54709, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 20s - loss: 0.6083 - acc: 0.9395 - mDice: 0.5301 - val_loss: 0.5710 - val_acc: 0.9591 - val_mDice: 0.5543

Epoch 00011: val_mDice improved from 0.54709 to 0.55433, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 20s - loss: 0.5835 - acc: 0.9443 - mDice: 0.5426 - val_loss: 0.5733 - val_acc: 0.9580 - val_mDice: 0.5520

Epoch 00012: val_mDice did not improve from 0.55433
Epoch 13/300
 - 21s - loss: 0.5703 - acc: 0.9463 - mDice: 0.5497 - val_loss: 0.5661 - val_acc: 0.9551 - val_mDice: 0.5549

Epoch 00013: val_mDice improved from 0.55433 to 0.55491, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 20s - loss: 0.5608 - acc: 0.9471 - mDice: 0.5555 - val_loss: 0.5596 - val_acc: 0.9561 - val_mDice: 0.5614

Epoch 00014: val_mDice improved from 0.55491 to 0.56142, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 20s - loss: 0.5555 - acc: 0.9474 - mDice: 0.5582 - val_loss: 0.5592 - val_acc: 0.9570 - val_mDice: 0.5603

Epoch 00015: val_mDice did not improve from 0.56142
Epoch 16/300
 - 20s - loss: 0.5430 - acc: 0.9481 - mDice: 0.5652 - val_loss: 0.5554 - val_acc: 0.9580 - val_mDice: 0.5674

Epoch 00016: val_mDice improved from 0.56142 to 0.56744, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 21s - loss: 0.5337 - acc: 0.9486 - mDice: 0.5704 - val_loss: 0.5261 - val_acc: 0.9590 - val_mDice: 0.5809

Epoch 00017: val_mDice improved from 0.56744 to 0.58090, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 21s - loss: 0.5284 - acc: 0.9491 - mDice: 0.5756 - val_loss: 0.5549 - val_acc: 0.9570 - val_mDice: 0.5647

Epoch 00018: val_mDice did not improve from 0.58090
Epoch 19/300
 - 20s - loss: 0.5336 - acc: 0.9491 - mDice: 0.5741 - val_loss: 0.5476 - val_acc: 0.9557 - val_mDice: 0.5689

Epoch 00019: val_mDice did not improve from 0.58090
Epoch 20/300
 - 20s - loss: 0.5130 - acc: 0.9497 - mDice: 0.5830 - val_loss: 0.5288 - val_acc: 0.9577 - val_mDice: 0.5776

Epoch 00020: val_mDice did not improve from 0.58090
Epoch 21/300
 - 21s - loss: 0.5087 - acc: 0.9500 - mDice: 0.5855 - val_loss: 0.5279 - val_acc: 0.9582 - val_mDice: 0.5789

Epoch 00021: val_mDice did not improve from 0.58090
Epoch 22/300
 - 21s - loss: 0.5259 - acc: 0.9494 - mDice: 0.5785 - val_loss: 0.5343 - val_acc: 0.9570 - val_mDice: 0.5758

Epoch 00022: val_mDice did not improve from 0.58090
Epoch 23/300
 - 21s - loss: 0.5091 - acc: 0.9503 - mDice: 0.5884 - val_loss: 0.5401 - val_acc: 0.9588 - val_mDice: 0.5737

Epoch 00023: val_mDice did not improve from 0.58090
Epoch 24/300
 - 20s - loss: 0.4993 - acc: 0.9507 - mDice: 0.5915 - val_loss: 0.5185 - val_acc: 0.9585 - val_mDice: 0.5848

Epoch 00024: val_mDice improved from 0.58090 to 0.58481, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 21s - loss: 0.5120 - acc: 0.9504 - mDice: 0.5890 - val_loss: 0.5630 - val_acc: 0.9564 - val_mDice: 0.5610

Epoch 00025: val_mDice did not improve from 0.58481
Epoch 26/300
 - 21s - loss: 0.4904 - acc: 0.9507 - mDice: 0.5961 - val_loss: 0.5354 - val_acc: 0.9580 - val_mDice: 0.5778

Epoch 00026: val_mDice did not improve from 0.58481
Epoch 27/300
 - 21s - loss: 0.4858 - acc: 0.9511 - mDice: 0.5992 - val_loss: 0.5238 - val_acc: 0.9576 - val_mDice: 0.5796

Epoch 00027: val_mDice did not improve from 0.58481
Epoch 28/300
 - 21s - loss: 0.4896 - acc: 0.9512 - mDice: 0.5984 - val_loss: 0.5190 - val_acc: 0.9584 - val_mDice: 0.5834

Epoch 00028: val_mDice did not improve from 0.58481
Epoch 29/300
 - 21s - loss: 0.4790 - acc: 0.9516 - mDice: 0.6031 - val_loss: 0.5376 - val_acc: 0.9572 - val_mDice: 0.5759

Epoch 00029: val_mDice did not improve from 0.58481
Epoch 30/300
 - 21s - loss: 0.4736 - acc: 0.9517 - mDice: 0.6067 - val_loss: 0.5219 - val_acc: 0.9588 - val_mDice: 0.5814

Epoch 00030: val_mDice did not improve from 0.58481
Epoch 31/300
 - 21s - loss: 0.4686 - acc: 0.9519 - mDice: 0.6097 - val_loss: 0.5159 - val_acc: 0.9599 - val_mDice: 0.5849

Epoch 00031: val_mDice improved from 0.58481 to 0.58489, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 21s - loss: 0.4670 - acc: 0.9521 - mDice: 0.6110 - val_loss: 0.5243 - val_acc: 0.9583 - val_mDice: 0.5827

Epoch 00032: val_mDice did not improve from 0.58489
Epoch 33/300
 - 21s - loss: 0.4697 - acc: 0.9523 - mDice: 0.6133 - val_loss: 0.5198 - val_acc: 0.9591 - val_mDice: 0.5847

Epoch 00033: val_mDice did not improve from 0.58489
Epoch 34/300
 - 21s - loss: 0.4601 - acc: 0.9523 - mDice: 0.6152 - val_loss: 0.5170 - val_acc: 0.9591 - val_mDice: 0.5878

Epoch 00034: val_mDice improved from 0.58489 to 0.58776, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 20s - loss: 0.4645 - acc: 0.9524 - mDice: 0.6127 - val_loss: 0.5175 - val_acc: 0.9596 - val_mDice: 0.5846

Epoch 00035: val_mDice did not improve from 0.58776
Epoch 36/300
 - 20s - loss: 0.4576 - acc: 0.9525 - mDice: 0.6168 - val_loss: 0.5273 - val_acc: 0.9584 - val_mDice: 0.5779

Epoch 00036: val_mDice did not improve from 0.58776
Epoch 37/300
 - 20s - loss: 0.4607 - acc: 0.9527 - mDice: 0.6186 - val_loss: 0.5218 - val_acc: 0.9594 - val_mDice: 0.5860

Epoch 00037: val_mDice did not improve from 0.58776
Epoch 38/300
 - 20s - loss: 0.4604 - acc: 0.9528 - mDice: 0.6189 - val_loss: 0.5257 - val_acc: 0.9582 - val_mDice: 0.5823

Epoch 00038: val_mDice did not improve from 0.58776
Epoch 39/300
 - 21s - loss: 0.4487 - acc: 0.9529 - mDice: 0.6225 - val_loss: 0.5282 - val_acc: 0.9585 - val_mDice: 0.5819

Epoch 00039: val_mDice did not improve from 0.58776
Epoch 40/300
 - 21s - loss: 0.4446 - acc: 0.9531 - mDice: 0.6250 - val_loss: 0.5313 - val_acc: 0.9589 - val_mDice: 0.5775

Epoch 00040: val_mDice did not improve from 0.58776
Epoch 41/300
 - 21s - loss: 0.4413 - acc: 0.9533 - mDice: 0.6271 - val_loss: 0.5262 - val_acc: 0.9592 - val_mDice: 0.5849

Epoch 00041: val_mDice did not improve from 0.58776
Epoch 42/300
 - 20s - loss: 0.4408 - acc: 0.9534 - mDice: 0.6274 - val_loss: 0.5301 - val_acc: 0.9596 - val_mDice: 0.5802

Epoch 00042: val_mDice did not improve from 0.58776
Epoch 43/300
 - 20s - loss: 0.4462 - acc: 0.9535 - mDice: 0.6283 - val_loss: 0.5301 - val_acc: 0.9589 - val_mDice: 0.5802

Epoch 00043: val_mDice did not improve from 0.58776
Epoch 44/300
 - 20s - loss: 0.4395 - acc: 0.9536 - mDice: 0.6287 - val_loss: 0.5253 - val_acc: 0.9594 - val_mDice: 0.5819

Epoch 00044: val_mDice did not improve from 0.58776
Epoch 45/300
 - 21s - loss: 0.4346 - acc: 0.9537 - mDice: 0.6314 - val_loss: 0.5146 - val_acc: 0.9597 - val_mDice: 0.5882

Epoch 00045: val_mDice improved from 0.58776 to 0.58820, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 21s - loss: 0.4337 - acc: 0.9539 - mDice: 0.6322 - val_loss: 0.5286 - val_acc: 0.9596 - val_mDice: 0.5827

Epoch 00046: val_mDice did not improve from 0.58820
Epoch 47/300
 - 21s - loss: 0.4302 - acc: 0.9539 - mDice: 0.6342 - val_loss: 0.5042 - val_acc: 0.9609 - val_mDice: 0.5935

Epoch 00047: val_mDice improved from 0.58820 to 0.59348, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 21s - loss: 0.4346 - acc: 0.9539 - mDice: 0.6347 - val_loss: 0.5558 - val_acc: 0.9594 - val_mDice: 0.5685

Epoch 00048: val_mDice did not improve from 0.59348
Epoch 49/300
 - 20s - loss: 0.4350 - acc: 0.9538 - mDice: 0.6312 - val_loss: 0.5246 - val_acc: 0.9590 - val_mDice: 0.5813

Epoch 00049: val_mDice did not improve from 0.59348
Epoch 50/300
 - 21s - loss: 0.4482 - acc: 0.9532 - mDice: 0.6260 - val_loss: 0.5318 - val_acc: 0.9582 - val_mDice: 0.5778

Epoch 00050: val_mDice did not improve from 0.59348
Epoch 51/300
 - 21s - loss: 0.4282 - acc: 0.9537 - mDice: 0.6353 - val_loss: 0.5258 - val_acc: 0.9599 - val_mDice: 0.5866

Epoch 00051: val_mDice did not improve from 0.59348
Epoch 52/300
 - 21s - loss: 0.4265 - acc: 0.9540 - mDice: 0.6368 - val_loss: 0.5340 - val_acc: 0.9578 - val_mDice: 0.5792

Epoch 00052: val_mDice did not improve from 0.59348
Epoch 53/300
 - 21s - loss: 0.4272 - acc: 0.9542 - mDice: 0.6385 - val_loss: 0.5264 - val_acc: 0.9582 - val_mDice: 0.5809

Epoch 00053: val_mDice did not improve from 0.59348
Epoch 54/300
 - 21s - loss: 0.4239 - acc: 0.9543 - mDice: 0.6386 - val_loss: 0.5222 - val_acc: 0.9586 - val_mDice: 0.5855

Epoch 00054: val_mDice did not improve from 0.59348
Epoch 55/300
 - 21s - loss: 0.4208 - acc: 0.9545 - mDice: 0.6407 - val_loss: 0.5239 - val_acc: 0.9594 - val_mDice: 0.5846

Epoch 00055: val_mDice did not improve from 0.59348
Epoch 56/300
 - 21s - loss: 0.4375 - acc: 0.9537 - mDice: 0.6325 - val_loss: 0.5498 - val_acc: 0.9590 - val_mDice: 0.5708

Epoch 00056: val_mDice did not improve from 0.59348
Epoch 57/300
 - 21s - loss: 0.4330 - acc: 0.9535 - mDice: 0.6321 - val_loss: 0.5461 - val_acc: 0.9590 - val_mDice: 0.5748

Epoch 00057: val_mDice did not improve from 0.59348
Epoch 58/300
 - 21s - loss: 0.4253 - acc: 0.9541 - mDice: 0.6379 - val_loss: 0.5209 - val_acc: 0.9584 - val_mDice: 0.5845

Epoch 00058: val_mDice did not improve from 0.59348
Epoch 59/300
 - 21s - loss: 0.4177 - acc: 0.9545 - mDice: 0.6425 - val_loss: 0.5195 - val_acc: 0.9601 - val_mDice: 0.5890

Epoch 00059: val_mDice did not improve from 0.59348
Epoch 60/300
 - 21s - loss: 0.4168 - acc: 0.9546 - mDice: 0.6443 - val_loss: 0.5101 - val_acc: 0.9592 - val_mDice: 0.5901

Epoch 00060: val_mDice did not improve from 0.59348
Epoch 61/300
 - 20s - loss: 0.4153 - acc: 0.9546 - mDice: 0.6439 - val_loss: 0.5262 - val_acc: 0.9581 - val_mDice: 0.5821

Epoch 00061: val_mDice did not improve from 0.59348
Epoch 62/300
 - 20s - loss: 0.4193 - acc: 0.9547 - mDice: 0.6458 - val_loss: 0.5228 - val_acc: 0.9593 - val_mDice: 0.5854

Epoch 00062: val_mDice did not improve from 0.59348
Epoch 63/300
 - 21s - loss: 0.4089 - acc: 0.9548 - mDice: 0.6485 - val_loss: 0.5007 - val_acc: 0.9607 - val_mDice: 0.5965

Epoch 00063: val_mDice improved from 0.59348 to 0.59645, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 21s - loss: 0.4090 - acc: 0.9551 - mDice: 0.6486 - val_loss: 0.5266 - val_acc: 0.9594 - val_mDice: 0.5804

Epoch 00064: val_mDice did not improve from 0.59645
Epoch 65/300
 - 21s - loss: 0.4102 - acc: 0.9550 - mDice: 0.6479 - val_loss: 0.5187 - val_acc: 0.9589 - val_mDice: 0.5877

Epoch 00065: val_mDice did not improve from 0.59645
Epoch 66/300
 - 21s - loss: 0.4060 - acc: 0.9551 - mDice: 0.6503 - val_loss: 0.5383 - val_acc: 0.9579 - val_mDice: 0.5798

Epoch 00066: val_mDice did not improve from 0.59645
Epoch 67/300
 - 21s - loss: 0.4064 - acc: 0.9550 - mDice: 0.6500 - val_loss: 0.5082 - val_acc: 0.9598 - val_mDice: 0.5920

Epoch 00067: val_mDice did not improve from 0.59645
Epoch 68/300
 - 21s - loss: 0.4027 - acc: 0.9552 - mDice: 0.6524 - val_loss: 0.5195 - val_acc: 0.9592 - val_mDice: 0.5881

Epoch 00068: val_mDice did not improve from 0.59645
Epoch 69/300
 - 20s - loss: 0.4014 - acc: 0.9553 - mDice: 0.6537 - val_loss: 0.5262 - val_acc: 0.9596 - val_mDice: 0.5830

Epoch 00069: val_mDice did not improve from 0.59645
Epoch 70/300
 - 21s - loss: 0.4019 - acc: 0.9553 - mDice: 0.6535 - val_loss: 0.5339 - val_acc: 0.9599 - val_mDice: 0.5784

Epoch 00070: val_mDice did not improve from 0.59645
Epoch 71/300
 - 21s - loss: 0.4150 - acc: 0.9549 - mDice: 0.6478 - val_loss: 0.5280 - val_acc: 0.9597 - val_mDice: 0.5826

Epoch 00071: val_mDice did not improve from 0.59645
Epoch 72/300
 - 20s - loss: 0.4104 - acc: 0.9552 - mDice: 0.6521 - val_loss: 0.5225 - val_acc: 0.9595 - val_mDice: 0.5852

Epoch 00072: val_mDice did not improve from 0.59645
Epoch 73/300
 - 20s - loss: 0.3996 - acc: 0.9554 - mDice: 0.6547 - val_loss: 0.5064 - val_acc: 0.9594 - val_mDice: 0.5937

Epoch 00073: val_mDice did not improve from 0.59645
Epoch 74/300
 - 20s - loss: 0.4114 - acc: 0.9551 - mDice: 0.6503 - val_loss: 0.5270 - val_acc: 0.9605 - val_mDice: 0.5830

Epoch 00074: val_mDice did not improve from 0.59645
Epoch 75/300
 - 21s - loss: 0.3977 - acc: 0.9555 - mDice: 0.6560 - val_loss: 0.5149 - val_acc: 0.9600 - val_mDice: 0.5882

Epoch 00075: val_mDice did not improve from 0.59645
Epoch 76/300
 - 21s - loss: 0.3965 - acc: 0.9556 - mDice: 0.6570 - val_loss: 0.5206 - val_acc: 0.9591 - val_mDice: 0.5869

Epoch 00076: val_mDice did not improve from 0.59645
Epoch 77/300
 - 21s - loss: 0.4016 - acc: 0.9556 - mDice: 0.6577 - val_loss: 0.5318 - val_acc: 0.9584 - val_mDice: 0.5828

Epoch 00077: val_mDice did not improve from 0.59645
Epoch 78/300
 - 21s - loss: 0.3928 - acc: 0.9557 - mDice: 0.6594 - val_loss: 0.5157 - val_acc: 0.9608 - val_mDice: 0.5934

Epoch 00078: val_mDice did not improve from 0.59645
Epoch 79/300
 - 21s - loss: 0.3964 - acc: 0.9556 - mDice: 0.6573 - val_loss: 0.5233 - val_acc: 0.9597 - val_mDice: 0.5868

Epoch 00079: val_mDice did not improve from 0.59645
Epoch 80/300
 - 21s - loss: 0.3910 - acc: 0.9557 - mDice: 0.6605 - val_loss: 0.5138 - val_acc: 0.9592 - val_mDice: 0.5881

Epoch 00080: val_mDice did not improve from 0.59645
Epoch 81/300
 - 20s - loss: 0.3937 - acc: 0.9557 - mDice: 0.6590 - val_loss: 0.5286 - val_acc: 0.9585 - val_mDice: 0.5813

Epoch 00081: val_mDice did not improve from 0.59645
Epoch 82/300
 - 21s - loss: 0.3904 - acc: 0.9559 - mDice: 0.6610 - val_loss: 0.5267 - val_acc: 0.9608 - val_mDice: 0.5863

Epoch 00082: val_mDice did not improve from 0.59645
Epoch 83/300
 - 21s - loss: 0.3902 - acc: 0.9559 - mDice: 0.6613 - val_loss: 0.5085 - val_acc: 0.9593 - val_mDice: 0.5933

Epoch 00083: val_mDice did not improve from 0.59645
Epoch 84/300
 - 20s - loss: 0.3872 - acc: 0.9560 - mDice: 0.6631 - val_loss: 0.5076 - val_acc: 0.9603 - val_mDice: 0.5916

Epoch 00084: val_mDice did not improve from 0.59645
Epoch 85/300
 - 20s - loss: 0.3873 - acc: 0.9559 - mDice: 0.6632 - val_loss: 0.5364 - val_acc: 0.9595 - val_mDice: 0.5808

Epoch 00085: val_mDice did not improve from 0.59645
Epoch 86/300
 - 20s - loss: 0.4051 - acc: 0.9555 - mDice: 0.6551 - val_loss: 0.5280 - val_acc: 0.9600 - val_mDice: 0.5846

Epoch 00086: val_mDice did not improve from 0.59645
Epoch 87/300
 - 21s - loss: 0.3976 - acc: 0.9557 - mDice: 0.6606 - val_loss: 0.5179 - val_acc: 0.9605 - val_mDice: 0.5896

Epoch 00087: val_mDice did not improve from 0.59645
Epoch 88/300
 - 21s - loss: 0.3886 - acc: 0.9560 - mDice: 0.6625 - val_loss: 0.5151 - val_acc: 0.9602 - val_mDice: 0.5903

Epoch 00088: val_mDice did not improve from 0.59645
Epoch 89/300
 - 21s - loss: 0.3860 - acc: 0.9561 - mDice: 0.6640 - val_loss: 0.5187 - val_acc: 0.9601 - val_mDice: 0.5872

Epoch 00089: val_mDice did not improve from 0.59645
Epoch 90/300
 - 21s - loss: 0.3836 - acc: 0.9562 - mDice: 0.6656 - val_loss: 0.5246 - val_acc: 0.9582 - val_mDice: 0.5842

Epoch 00090: val_mDice did not improve from 0.59645
Epoch 91/300
 - 20s - loss: 0.3820 - acc: 0.9562 - mDice: 0.6667 - val_loss: 0.5222 - val_acc: 0.9595 - val_mDice: 0.5853

Epoch 00091: val_mDice did not improve from 0.59645
Epoch 92/300
 - 20s - loss: 0.3807 - acc: 0.9563 - mDice: 0.6677 - val_loss: 0.5225 - val_acc: 0.9601 - val_mDice: 0.5873

Epoch 00092: val_mDice did not improve from 0.59645
Epoch 93/300
 - 20s - loss: 0.3806 - acc: 0.9563 - mDice: 0.6678 - val_loss: 0.5081 - val_acc: 0.9612 - val_mDice: 0.5951

Epoch 00093: val_mDice did not improve from 0.59645
Epoch 94/300
 - 21s - loss: 0.3794 - acc: 0.9564 - mDice: 0.6685 - val_loss: 0.5192 - val_acc: 0.9599 - val_mDice: 0.5865

Epoch 00094: val_mDice did not improve from 0.59645
Epoch 95/300
 - 21s - loss: 0.3791 - acc: 0.9563 - mDice: 0.6687 - val_loss: 0.5233 - val_acc: 0.9611 - val_mDice: 0.5873

Epoch 00095: val_mDice did not improve from 0.59645
Epoch 96/300
 - 21s - loss: 0.3784 - acc: 0.9564 - mDice: 0.6694 - val_loss: 0.5139 - val_acc: 0.9608 - val_mDice: 0.5897

Epoch 00096: val_mDice did not improve from 0.59645
Epoch 97/300
 - 20s - loss: 0.3769 - acc: 0.9564 - mDice: 0.6702 - val_loss: 0.5119 - val_acc: 0.9597 - val_mDice: 0.5891

Epoch 00097: val_mDice did not improve from 0.59645
Epoch 98/300
 - 20s - loss: 0.3757 - acc: 0.9566 - mDice: 0.6711 - val_loss: 0.5139 - val_acc: 0.9599 - val_mDice: 0.5884

Epoch 00098: val_mDice did not improve from 0.59645
Epoch 99/300
 - 20s - loss: 0.3762 - acc: 0.9565 - mDice: 0.6708 - val_loss: 0.5193 - val_acc: 0.9596 - val_mDice: 0.5876

Epoch 00099: val_mDice did not improve from 0.59645
Epoch 100/300
 - 21s - loss: 0.3775 - acc: 0.9565 - mDice: 0.6701 - val_loss: 0.5188 - val_acc: 0.9609 - val_mDice: 0.5897

Epoch 00100: val_mDice did not improve from 0.59645
Epoch 101/300
 - 21s - loss: 0.3755 - acc: 0.9565 - mDice: 0.6712 - val_loss: 0.5215 - val_acc: 0.9594 - val_mDice: 0.5863

Epoch 00101: val_mDice did not improve from 0.59645
Epoch 102/300
 - 20s - loss: 0.3743 - acc: 0.9565 - mDice: 0.6723 - val_loss: 0.5129 - val_acc: 0.9596 - val_mDice: 0.5884

Epoch 00102: val_mDice did not improve from 0.59645
Epoch 103/300
 - 20s - loss: 0.3756 - acc: 0.9565 - mDice: 0.6714 - val_loss: 0.4953 - val_acc: 0.9600 - val_mDice: 0.5991

Epoch 00103: val_mDice improved from 0.59645 to 0.59907, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute2_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 104/300
 - 20s - loss: 0.3705 - acc: 0.9568 - mDice: 0.6749 - val_loss: 0.5247 - val_acc: 0.9616 - val_mDice: 0.5880

Epoch 00104: val_mDice did not improve from 0.59907
Epoch 105/300
 - 21s - loss: 0.3732 - acc: 0.9567 - mDice: 0.6730 - val_loss: 0.5181 - val_acc: 0.9602 - val_mDice: 0.5890

Epoch 00105: val_mDice did not improve from 0.59907
Epoch 106/300
 - 22s - loss: 0.3745 - acc: 0.9566 - mDice: 0.6721 - val_loss: 0.5139 - val_acc: 0.9603 - val_mDice: 0.5904

Epoch 00106: val_mDice did not improve from 0.59907
Epoch 107/300
 - 20s - loss: 0.3751 - acc: 0.9567 - mDice: 0.6717 - val_loss: 0.5195 - val_acc: 0.9595 - val_mDice: 0.5863

Epoch 00107: val_mDice did not improve from 0.59907
Epoch 108/300
 - 21s - loss: 0.3705 - acc: 0.9568 - mDice: 0.6749 - val_loss: 0.5115 - val_acc: 0.9615 - val_mDice: 0.5921

Epoch 00108: val_mDice did not improve from 0.59907
Epoch 109/300
 - 21s - loss: 0.3718 - acc: 0.9567 - mDice: 0.6740 - val_loss: 0.5167 - val_acc: 0.9597 - val_mDice: 0.5901

Epoch 00109: val_mDice did not improve from 0.59907
Epoch 110/300
 - 21s - loss: 0.3677 - acc: 0.9568 - mDice: 0.6766 - val_loss: 0.5078 - val_acc: 0.9604 - val_mDice: 0.5946

Epoch 00110: val_mDice did not improve from 0.59907
Epoch 111/300
 - 21s - loss: 0.3684 - acc: 0.9569 - mDice: 0.6763 - val_loss: 0.5198 - val_acc: 0.9598 - val_mDice: 0.5877

Epoch 00111: val_mDice did not improve from 0.59907
Epoch 112/300
 - 21s - loss: 0.3672 - acc: 0.9569 - mDice: 0.6770 - val_loss: 0.5106 - val_acc: 0.9599 - val_mDice: 0.5903

Epoch 00112: val_mDice did not improve from 0.59907
Epoch 113/300
 - 21s - loss: 0.3661 - acc: 0.9571 - mDice: 0.6780 - val_loss: 0.5137 - val_acc: 0.9610 - val_mDice: 0.5905

Epoch 00113: val_mDice did not improve from 0.59907
Epoch 114/300
 - 20s - loss: 0.3650 - acc: 0.9571 - mDice: 0.6787 - val_loss: 0.5095 - val_acc: 0.9605 - val_mDice: 0.5927

Epoch 00114: val_mDice did not improve from 0.59907
Epoch 115/300
 - 21s - loss: 0.3671 - acc: 0.9569 - mDice: 0.6774 - val_loss: 0.5362 - val_acc: 0.9604 - val_mDice: 0.5840

Epoch 00115: val_mDice did not improve from 0.59907
Epoch 116/300
 - 21s - loss: 0.3636 - acc: 0.9569 - mDice: 0.6797 - val_loss: 0.5154 - val_acc: 0.9604 - val_mDice: 0.5890

Epoch 00116: val_mDice did not improve from 0.59907
Epoch 117/300
 - 20s - loss: 0.3630 - acc: 0.9570 - mDice: 0.6801 - val_loss: 0.5144 - val_acc: 0.9609 - val_mDice: 0.5912

Epoch 00117: val_mDice did not improve from 0.59907
Epoch 118/300
 - 21s - loss: 0.3621 - acc: 0.9572 - mDice: 0.6808 - val_loss: 0.5156 - val_acc: 0.9607 - val_mDice: 0.5900

Epoch 00118: val_mDice did not improve from 0.59907
Epoch 119/300
 - 21s - loss: 0.3631 - acc: 0.9571 - mDice: 0.6805 - val_loss: 0.5410 - val_acc: 0.9589 - val_mDice: 0.5787

Epoch 00119: val_mDice did not improve from 0.59907
Epoch 120/300
 - 20s - loss: 0.3600 - acc: 0.9572 - mDice: 0.6822 - val_loss: 0.4998 - val_acc: 0.9607 - val_mDice: 0.5950

Epoch 00120: val_mDice did not improve from 0.59907
Epoch 121/300
 - 20s - loss: 0.3578 - acc: 0.9573 - mDice: 0.6837 - val_loss: 0.5183 - val_acc: 0.9608 - val_mDice: 0.5890

Epoch 00121: val_mDice did not improve from 0.59907
Epoch 122/300
 - 20s - loss: 0.3600 - acc: 0.9572 - mDice: 0.6824 - val_loss: 0.5146 - val_acc: 0.9598 - val_mDice: 0.5898

Epoch 00122: val_mDice did not improve from 0.59907
Epoch 123/300
 - 20s - loss: 0.3587 - acc: 0.9573 - mDice: 0.6832 - val_loss: 0.5048 - val_acc: 0.9601 - val_mDice: 0.5931

Epoch 00123: val_mDice did not improve from 0.59907
Epoch 124/300
 - 21s - loss: 0.3599 - acc: 0.9573 - mDice: 0.6826 - val_loss: 0.5177 - val_acc: 0.9596 - val_mDice: 0.5890

Epoch 00124: val_mDice did not improve from 0.59907
Epoch 125/300
 - 21s - loss: 0.3579 - acc: 0.9573 - mDice: 0.6837 - val_loss: 0.5126 - val_acc: 0.9596 - val_mDice: 0.5904

Epoch 00125: val_mDice did not improve from 0.59907
Epoch 126/300
 - 20s - loss: 0.3630 - acc: 0.9573 - mDice: 0.6838 - val_loss: 0.5221 - val_acc: 0.9596 - val_mDice: 0.5888

Epoch 00126: val_mDice did not improve from 0.59907
Epoch 127/300
 - 20s - loss: 0.3649 - acc: 0.9573 - mDice: 0.6831 - val_loss: 0.5355 - val_acc: 0.9591 - val_mDice: 0.5808

Epoch 00127: val_mDice did not improve from 0.59907
Epoch 128/300
 - 20s - loss: 0.3590 - acc: 0.9572 - mDice: 0.6828 - val_loss: 0.5105 - val_acc: 0.9600 - val_mDice: 0.5927

Epoch 00128: val_mDice did not improve from 0.59907
Epoch 129/300
 - 20s - loss: 0.3565 - acc: 0.9573 - mDice: 0.6847 - val_loss: 0.5167 - val_acc: 0.9606 - val_mDice: 0.5872

Epoch 00129: val_mDice did not improve from 0.59907
Epoch 130/300
 - 21s - loss: 0.3563 - acc: 0.9574 - mDice: 0.6850 - val_loss: 0.5178 - val_acc: 0.9615 - val_mDice: 0.5919

Epoch 00130: val_mDice did not improve from 0.59907
Epoch 131/300
 - 21s - loss: 0.3548 - acc: 0.9574 - mDice: 0.6862 - val_loss: 0.5128 - val_acc: 0.9603 - val_mDice: 0.5911

Epoch 00131: val_mDice did not improve from 0.59907
Epoch 132/300
 - 21s - loss: 0.3564 - acc: 0.9574 - mDice: 0.6850 - val_loss: 0.5113 - val_acc: 0.9614 - val_mDice: 0.5925

Epoch 00132: val_mDice did not improve from 0.59907
Epoch 133/300
 - 20s - loss: 0.3541 - acc: 0.9573 - mDice: 0.6864 - val_loss: 0.5054 - val_acc: 0.9608 - val_mDice: 0.5952

Epoch 00133: val_mDice did not improve from 0.59907
Epoch 134/300
 - 21s - loss: 0.3608 - acc: 0.9575 - mDice: 0.6861 - val_loss: 0.5153 - val_acc: 0.9596 - val_mDice: 0.5898

Epoch 00134: val_mDice did not improve from 0.59907
Epoch 135/300
 - 21s - loss: 0.3605 - acc: 0.9576 - mDice: 0.6867 - val_loss: 0.4992 - val_acc: 0.9612 - val_mDice: 0.5968

Epoch 00135: val_mDice did not improve from 0.59907
Epoch 136/300
 - 21s - loss: 0.3537 - acc: 0.9575 - mDice: 0.6869 - val_loss: 0.5078 - val_acc: 0.9602 - val_mDice: 0.5945

Epoch 00136: val_mDice did not improve from 0.59907
Epoch 137/300
 - 21s - loss: 0.3520 - acc: 0.9577 - mDice: 0.6878 - val_loss: 0.5156 - val_acc: 0.9601 - val_mDice: 0.5907

Epoch 00137: val_mDice did not improve from 0.59907
Epoch 138/300
 - 21s - loss: 0.3580 - acc: 0.9577 - mDice: 0.6883 - val_loss: 0.4994 - val_acc: 0.9614 - val_mDice: 0.5980

Epoch 00138: val_mDice did not improve from 0.59907
Epoch 139/300
 - 21s - loss: 0.3527 - acc: 0.9576 - mDice: 0.6876 - val_loss: 0.5025 - val_acc: 0.9605 - val_mDice: 0.5950

Epoch 00139: val_mDice did not improve from 0.59907
Epoch 140/300
 - 21s - loss: 0.3524 - acc: 0.9576 - mDice: 0.6880 - val_loss: 0.5157 - val_acc: 0.9608 - val_mDice: 0.5900

Epoch 00140: val_mDice did not improve from 0.59907
Epoch 141/300
 - 21s - loss: 0.3518 - acc: 0.9576 - mDice: 0.6885 - val_loss: 0.5177 - val_acc: 0.9600 - val_mDice: 0.5884

Epoch 00141: val_mDice did not improve from 0.59907
Epoch 142/300
 - 21s - loss: 0.3722 - acc: 0.9566 - mDice: 0.6750 - val_loss: 0.5042 - val_acc: 0.9606 - val_mDice: 0.5967

Epoch 00142: val_mDice did not improve from 0.59907
Epoch 143/300
 - 21s - loss: 0.3536 - acc: 0.9574 - mDice: 0.6870 - val_loss: 0.5121 - val_acc: 0.9607 - val_mDice: 0.5933

Epoch 00143: val_mDice did not improve from 0.59907
Restoring model weights from the end of the best epoch
Epoch 00143: early stopping
{'val_loss': [1.1623867843300104, 0.9151297758022944, 0.7880954295396805, 0.6976554275800785, 0.6431707292795181, 0.6186158439765373, 0.6252566886444887, 0.5929158659030994, 0.601394529764851, 0.6015862971544266, 0.5710118397449454, 0.5732769457002481, 0.5661042348171273, 0.5595809835940599, 0.5592163940891623, 0.5554145127534866, 0.526146691913406, 0.5549229932948947, 0.5476212070013086, 0.5288479883844653, 0.5278912199040254, 0.5342618953436613, 0.540053358922402, 0.5185493814448515, 0.5630265368769566, 0.5354179764787356, 0.5238343722497424, 0.5190074415877461, 0.537584642569224, 0.5219137681027254, 0.5159255107864738, 0.5242646771172682, 0.5197569889326891, 0.5169687708839774, 0.5174882064263026, 0.5273390430957079, 0.5217805414771041, 0.5257084410016736, 0.5282387360930443, 0.5313388829429945, 0.5262338391815623, 0.5300805550068617, 0.5301258374626437, 0.5253112316131592, 0.5146104314674934, 0.5286303960407773, 0.5041908898080388, 0.5557805579155684, 0.5246362860004107, 0.5318365364025036, 0.5258144618322452, 0.5340230933700999, 0.5264415393273035, 0.5221782106285294, 0.5238710890213648, 0.5498148317759236, 0.546142720306913, 0.5208916359891494, 0.5194753538817167, 0.5101083203529319, 0.5261837805931767, 0.5228475701684753, 0.5007147553066412, 0.5266069903348883, 0.518720788260301, 0.5382718971620003, 0.508245180050532, 0.5194925516843796, 0.5261970699454347, 0.5339472088962793, 0.5280131030206879, 0.5224931736787161, 0.5063984598964453, 0.5269684189309677, 0.5148869551097354, 0.5206263534103831, 0.5317955060551564, 0.5156722053264579, 0.523262372550865, 0.5138116739690304, 0.5285672936588526, 0.5266975595926245, 0.5085241630052527, 0.507603251375258, 0.5363859233135978, 0.5280144934852918, 0.5179356380055348, 0.5151073864350716, 0.5187323034430543, 0.524643691877524, 0.5221581030637026, 0.5225476141398152, 0.5081495977938175, 0.5192331299185753, 0.5232603450616201, 0.5138948550447822, 0.511891974757115, 0.5139493336901069, 0.5193018279969692, 0.5187711144487063, 0.5214643608778715, 0.5128538754458228, 0.4953372282907367, 0.5247066291049123, 0.5181198374678692, 0.5139251441384355, 0.5195337068289518, 0.5114930653944612, 0.5166788861776391, 0.507834161631763, 0.5197579301893711, 0.5106130217512449, 0.5137117328122258, 0.5094637867684165, 0.5361574093500773, 0.5154174795995156, 0.5143979589144388, 0.5155950939903656, 0.5410212700565656, 0.49977618145445984, 0.5183187266811728, 0.5146007717897495, 0.5047687139982978, 0.5177350388839841, 0.5125778059785565, 0.5221034558489919, 0.5354648310070237, 0.5104631669819355, 0.5166542107860247, 0.5177932291602095, 0.5128041704495748, 0.5113398333390554, 0.5053943215558926, 0.5152656783660253, 0.499195186731716, 0.5078164053459963, 0.5156110761066278, 0.4993518168727557, 0.5025075258066257, 0.5157263965035478, 0.5176677359268069, 0.5042468595008055, 0.5120782650386294], 'val_acc': [0.9395099158088366, 0.9429134658227364, 0.9454006620993217, 0.9475614745169878, 0.9484049615760645, 0.9488177858293056, 0.95265281945467, 0.9551315264155468, 0.9557512147972981, 0.9580273013561964, 0.9591462947428226, 0.9579789899289608, 0.9550586423526207, 0.9561352251718441, 0.9569515716284513, 0.9579747430980206, 0.9590496439486742, 0.9570287056267262, 0.9557257698227962, 0.9576806034892797, 0.958190931007266, 0.9570202318330606, 0.9587783819685379, 0.9584765962014595, 0.9563785009086132, 0.957975598052144, 0.9576314433167378, 0.9584477798392376, 0.9572067217280468, 0.9588114513705174, 0.959933798139294, 0.9582740068435669, 0.9591098291178545, 0.9590589764217535, 0.9595786264787117, 0.9584104760239521, 0.9594031473000845, 0.9581722610940536, 0.9584850743412971, 0.9588631478448709, 0.9591598653544983, 0.959591339652737, 0.9588648496816555, 0.9593751622984806, 0.9596549098690351, 0.9595650515208641, 0.9609358062346777, 0.9594251780460278, 0.9589733512451252, 0.9582044904430708, 0.9599253485600153, 0.9577882538239161, 0.9581722722699245, 0.9585834083457788, 0.9594175523767868, 0.9590089401851097, 0.9589555418739716, 0.9584045354276896, 0.9601075984537601, 0.9592090093841156, 0.9580908864736557, 0.9593361858278513, 0.9607009794563055, 0.9593717809766531, 0.9589216566334168, 0.9578662520895401, 0.9597939401865005, 0.959226818755269, 0.9595743759224812, 0.9599456874032816, 0.959718489398559, 0.9594633355736732, 0.9594022861371437, 0.960489897057414, 0.9600245133042336, 0.9591462953637043, 0.958360451584061, 0.9608485028147697, 0.9596752567837635, 0.9592446206758419, 0.9584638886153698, 0.9607611733178297, 0.9592903958012661, 0.9602923815449079, 0.9594565505782763, 0.9599965251982212, 0.960549246519804, 0.9601847330729166, 0.9600957271953424, 0.9581637966136137, 0.9595090945561727, 0.9601109785338243, 0.9612341889490684, 0.9599405707170566, 0.9610561865071455, 0.9608103471497694, 0.9596650792906681, 0.9598846503843864, 0.9596498093257347, 0.9609383599211773, 0.9593590671817461, 0.9595930203795433, 0.9599728037913641, 0.9616411123424768, 0.9602449182420969, 0.9603178389370441, 0.9595192850877842, 0.9615266633530458, 0.959690518056353, 0.9604280175020298, 0.9598024239142736, 0.9598566566904386, 0.9610434547066689, 0.9604585375636816, 0.9604034355531136, 0.9603703785687685, 0.9609010517597198, 0.9606891199946404, 0.9588690834740797, 0.9606535118073225, 0.9607577938586473, 0.959765978778402, 0.9601364141951004, 0.9595981302360693, 0.959583718329668, 0.9596438904603323, 0.9591301965216795, 0.9600312927116951, 0.960621302947402, 0.9615410814682642, 0.9602974758793911, 0.9613545866062244, 0.9608281447241703, 0.9596150505046049, 0.9611867293715477, 0.9602423713852962, 0.9600524753332138, 0.961374074841539, 0.9604754994312922, 0.9607687977453073, 0.9600067200760046, 0.9605941728999218, 0.9606925037999948], 'val_mDice': [0.34210158449908096, 0.4168501499419411, 0.4629780075823267, 0.4917747260381778, 0.5166717836012443, 0.5272746176148454, 0.5257072253152728, 0.5420859158039093, 0.540923017077148, 0.547091405838728, 0.5543346193929514, 0.5519555748129884, 0.5549137741327286, 0.5614183365056912, 0.5603092045833667, 0.5674375873059034, 0.5808952494213978, 0.5647481475025415, 0.5689338246981303, 0.5775888257970413, 0.5789272965242466, 0.5757532405356566, 0.5736643175284067, 0.584810525799791, 0.5609592342128357, 0.5778445604567727, 0.579617282996575, 0.5834283518294493, 0.5759362274159988, 0.581373260046045, 0.584894726673762, 0.5826930413022637, 0.5846530279765526, 0.5877646865944067, 0.5846324178079764, 0.5778990083684524, 0.5859738209595283, 0.5822749361395836, 0.5819381050144633, 0.5775444942216078, 0.5849090097472072, 0.5801553962131342, 0.5802385779097676, 0.5819319256891807, 0.5882014610494176, 0.5827444655199846, 0.5934820851931969, 0.568518973266085, 0.5812793482715884, 0.5777871869504452, 0.586629287339747, 0.5791628124813238, 0.5808983438958725, 0.5855009102572998, 0.5846166725580891, 0.570835916015009, 0.5747628208870689, 0.5845009656623006, 0.5889890187730392, 0.590105797474583, 0.5820752118403713, 0.5853959148128828, 0.5964542701840401, 0.5803818510224422, 0.587698843019704, 0.5798312661548456, 0.5920490249991417, 0.5880764558290442, 0.5829702361176411, 0.5784103277449807, 0.5826402064412832, 0.5851955146839222, 0.5937060912450155, 0.5830289845665296, 0.5882287776718537, 0.5868623672674099, 0.5827837167307734, 0.5934057512010137, 0.5868131776029865, 0.5880702535311381, 0.5812839906041821, 0.5863220713411769, 0.5932772544523081, 0.5916034212956826, 0.5808127981921037, 0.5845581808437904, 0.5895629720762372, 0.5902864790211121, 0.5871716377635797, 0.584239388195177, 0.585301049053669, 0.5873228392253319, 0.5950531853983799, 0.5864967455466589, 0.5872546931107839, 0.589690929899613, 0.5891071843604246, 0.5884067788720131, 0.5875720477973422, 0.5897454066822926, 0.586337024345994, 0.5884031982471546, 0.5990662587185701, 0.588018569474419, 0.5890105403959751, 0.5904459832236171, 0.5862977771709362, 0.5921078510582447, 0.5901028960943222, 0.5945804237077633, 0.5876794128368298, 0.5902976387490829, 0.5904549658298492, 0.5926648881286383, 0.5840011121084293, 0.5890225985397896, 0.5912355842689673, 0.5900228979686896, 0.5787096340209246, 0.5950156940768162, 0.5890147338310877, 0.5898325722664595, 0.5931225766738256, 0.5889895626654228, 0.5904196488360564, 0.5888385487099489, 0.5807845511784157, 0.5926509642352661, 0.5872259053091208, 0.5919434844205776, 0.5911334001769623, 0.5924718677997589, 0.5952268969267607, 0.5897609436263641, 0.596782802293698, 0.5945331007242203, 0.5907174814492464, 0.5980402690668901, 0.594961978495121, 0.5899684273948272, 0.5883555101851622, 0.5967170707881451, 0.5932833639283975], 'loss': [2.3279995476651134, 1.1730049669598264, 0.9409075054649002, 0.827474096785446, 0.7609383823219281, 0.7223093404319615, 0.6909188688811609, 0.6558494155112536, 0.636832721874154, 0.6162430902370241, 0.6083247725091893, 0.5834948966058634, 0.570344524002537, 0.560834213117133, 0.5555071532004682, 0.5430164706620408, 0.5336613041725343, 0.5284449969019208, 0.5336274107778333, 0.5129552734677497, 0.5086926934794133, 0.5259389064791128, 0.5090924047528976, 0.4993142782920209, 0.511974034240113, 0.4904266436365557, 0.48584861494149767, 0.4895866305429768, 0.4790093878568229, 0.47362143768236653, 0.4686085234687057, 0.46701666178772583, 0.4696677858979592, 0.46012266893075104, 0.46447447079434523, 0.4575509248576499, 0.4607144473684325, 0.4604018189860891, 0.4487217879757177, 0.444607676302261, 0.44125076219186943, 0.4408296361818152, 0.4462345755562078, 0.4395010666177579, 0.43461869040066625, 0.43373463308262766, 0.4302088891767128, 0.43459871021656377, 0.4349973476683545, 0.4481714836910331, 0.4282342360758608, 0.42650042824248713, 0.42717355172224253, 0.42393571109517725, 0.42081331434607794, 0.4374611160512698, 0.4330157970978042, 0.4252855119635926, 0.41774881665412217, 0.41682060954068534, 0.415282012691798, 0.4192836721930608, 0.40888898496766357, 0.40897051438003706, 0.4102213185000939, 0.4059537442053779, 0.40636574197335046, 0.40272103188690206, 0.4013843829348936, 0.4019056448203311, 0.4150289642031487, 0.41036626177030383, 0.39963059065994283, 0.41141732834441896, 0.39774695406814464, 0.39649954618611, 0.40159086186718423, 0.3927798872998494, 0.39643079475398213, 0.3909765709254701, 0.393658037335763, 0.39042989133922584, 0.3902477084435793, 0.3872155999588909, 0.38732727423995805, 0.405107286161141, 0.39758472155427815, 0.3885821196415234, 0.38596458527424143, 0.38356508931582545, 0.38200606142348875, 0.3807117802611852, 0.38060127361057455, 0.379448127371347, 0.37910083001231454, 0.378386606243563, 0.3769114016764966, 0.37568768584699375, 0.37621746888749535, 0.37750505780480964, 0.3754976824034213, 0.37425593183923866, 0.37558142195025024, 0.3705236823737766, 0.3732442121961792, 0.374477244173932, 0.3751156920382243, 0.37054816516201955, 0.3717795417037484, 0.36768724330978303, 0.3684010992760231, 0.36719042741068914, 0.3661129687513624, 0.3649829450011542, 0.36712731922509884, 0.3635639438617605, 0.3629620245524815, 0.3620866846663034, 0.36310431981779473, 0.35998300165298774, 0.35781654260926327, 0.359950868255001, 0.35865769367529754, 0.3598914360740283, 0.3579033805152117, 0.3630096720436872, 0.3648979702093988, 0.3589514029949687, 0.35651357064235584, 0.3562508594181578, 0.3547809650765204, 0.35637967753929894, 0.35407942591221514, 0.36084315274587264, 0.36047066564132746, 0.3536931773507855, 0.35204039438296175, 0.35800972429372496, 0.35269170299569286, 0.3523546454404226, 0.35175341165671914, 0.3721925202062574, 0.3535857211446647], 'acc': [0.7735685105942929, 0.9195865975742479, 0.9221690884513948, 0.9240085088600547, 0.9261161347278383, 0.9280872007259157, 0.9296247762860167, 0.9311738597278734, 0.9331012909406611, 0.9354599038856082, 0.9395005903578844, 0.9442645123738065, 0.9463176888934637, 0.9470578284875533, 0.9474481517408431, 0.9481413481310551, 0.9486221101035794, 0.9491035632302051, 0.9491026820051179, 0.949739567592704, 0.950028470295682, 0.9494106535761466, 0.9503131130705734, 0.9507002419190026, 0.9503875823632857, 0.9507477505732391, 0.951104202224325, 0.9511948105209388, 0.9515618686237289, 0.951687790579715, 0.9519300615239086, 0.9521401940188743, 0.9523213194011199, 0.9523475331775213, 0.9523882186153033, 0.9524889487331196, 0.9527059051661457, 0.9528164580716925, 0.9529411810073668, 0.9531135876588613, 0.953321021203556, 0.9534113020354265, 0.9534831517545131, 0.953592803662972, 0.9537304840422716, 0.9538973437094516, 0.9539198980781703, 0.9539281632074721, 0.9538169790411111, 0.9532292140309516, 0.9536862992489886, 0.9539580505350311, 0.9542123018685034, 0.9543451897745848, 0.9544741401660818, 0.9537094091099053, 0.9534630703291074, 0.9540846002881233, 0.9544679573026754, 0.9546100045520515, 0.9545751996248166, 0.9547375037941459, 0.9548375417476127, 0.9550797787763304, 0.9550365555950285, 0.9550937936611961, 0.9550064327353138, 0.9551747084818509, 0.9552700942422807, 0.955302651655876, 0.9548564719230153, 0.9551867053237435, 0.9554469006690794, 0.9550650701395824, 0.955513030218443, 0.955551666872842, 0.9556248111528577, 0.955680882238014, 0.9556136044693917, 0.9557391452731578, 0.9557197701555765, 0.9558762753558216, 0.9558866924581458, 0.9560499904230778, 0.9559145323012124, 0.9554955332966174, 0.9557183999703525, 0.9560130403059158, 0.9560596854865695, 0.9562208993094308, 0.9561759246463637, 0.9562710455774395, 0.9563315282722362, 0.9563579351503682, 0.9563248620772189, 0.956383787979514, 0.9564448385781295, 0.956585216175846, 0.956485621577025, 0.9564897322770181, 0.9565265600098247, 0.9565172751071089, 0.9565455062048775, 0.9568452852401549, 0.9566783170434522, 0.9566128839880733, 0.9566689364557982, 0.9568094246035338, 0.9567172843376603, 0.9567893708877933, 0.9568577846083745, 0.956946520915043, 0.957094965777732, 0.9570591996714798, 0.9569222111678874, 0.9569462013879642, 0.957035507618948, 0.9571680940380974, 0.9570593881549327, 0.9572360834544277, 0.9572906121791996, 0.957174024628092, 0.9572571762826194, 0.9572591768627305, 0.9572563721250391, 0.9572824601976981, 0.957342014618705, 0.9572442957621798, 0.9573292166499768, 0.957396779164275, 0.9574476642123724, 0.9573920519242275, 0.9573385803520535, 0.9575143887979355, 0.9575769585212264, 0.9575221476485597, 0.9576741253492619, 0.9577225201355054, 0.9575987751201048, 0.9575791949221355, 0.9576166054238419, 0.9565923889381833, 0.9574379238320321], 'mDice': [0.14459347469195472, 0.3056800641823046, 0.37925141587961675, 0.42356205159757676, 0.45391486725853375, 0.4736059421344184, 0.48922501508029265, 0.5050928734116635, 0.5175850290363118, 0.5255369997197722, 0.5300847727796356, 0.5425953040977367, 0.5497239214744752, 0.5554992840884673, 0.5581869228988814, 0.5651889886463526, 0.5704399758620643, 0.5756200728058526, 0.5740907883528648, 0.5829836883498739, 0.5854546400305722, 0.578470348734544, 0.5884066222654993, 0.5915076832217108, 0.5889621965821661, 0.5960855449371707, 0.5992070520183942, 0.5983656174334141, 0.6031434430625767, 0.6066650528884685, 0.6097260748214353, 0.6109657364087878, 0.6133011657446984, 0.6152178190522275, 0.6127412287432691, 0.6168270734551455, 0.618642751443184, 0.6189141336999852, 0.6224833054346265, 0.6249611810679586, 0.6270913493258036, 0.6274231390860698, 0.6282942759788642, 0.6287392024266518, 0.6313721790152081, 0.6321705719749229, 0.6342039766380919, 0.6346600485771678, 0.6311910596944518, 0.6260482834845997, 0.6352625787979754, 0.6367691717771294, 0.6385334467483779, 0.6386206867331166, 0.6407281792481355, 0.6325191002492466, 0.6321423512682788, 0.6378998684248105, 0.6425117146016321, 0.644296354589393, 0.6439356665345716, 0.6458212661108151, 0.6484565550132179, 0.6486027806492175, 0.6478769349128224, 0.6502532694876627, 0.6500356145401556, 0.65235192637178, 0.6536625604941251, 0.6534772773631837, 0.6477876441242042, 0.6520738653640192, 0.6546857665872459, 0.6502519375475498, 0.6559914528024687, 0.6570401861361672, 0.6577345692793913, 0.6593635393978609, 0.6572764517319982, 0.6604920339930721, 0.6590063665738695, 0.6609906881831171, 0.6612597995462487, 0.6630671584288664, 0.663218938842524, 0.6550840223095319, 0.660560407592367, 0.6625237335015729, 0.6640024289091909, 0.6655874334582405, 0.6667313043199498, 0.6676738299700019, 0.667780835461097, 0.6685423827921796, 0.6687381406095935, 0.6693528437441255, 0.6702469281365161, 0.6710839843057258, 0.6708063868575754, 0.6700564661845745, 0.6711842577913483, 0.6723310104582558, 0.6713508138067786, 0.6749077305089475, 0.6730179171873929, 0.6720691220812302, 0.6717426903888619, 0.6748666148497464, 0.6739585402514109, 0.6765930226870945, 0.6762844399158949, 0.6770092247473414, 0.6779878034141392, 0.6787010439948943, 0.6774151703347305, 0.6797020563779097, 0.6800780309314589, 0.6807956157238663, 0.6805184288694552, 0.6821888878616813, 0.6836766796885622, 0.6823797575209389, 0.6832017363128016, 0.6825826612569518, 0.6836695313165032, 0.6838013861716227, 0.6830583994094165, 0.6828495590219198, 0.6847191868625022, 0.6849808037714, 0.6861699779154891, 0.6849505159525837, 0.6864462621852792, 0.6860917415514985, 0.6866662564346923, 0.6868567548998908, 0.6878289423035074, 0.6883236412274636, 0.687609475115021, 0.6879903822776481, 0.6885384165345901, 0.6749531121577247, 0.686998371206242]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:02<00:11,  2.91s/it]predicting test subjects:  40%|████      | 2/5 [00:05<00:08,  2.67s/it]predicting test subjects:  60%|██████    | 3/5 [00:06<00:04,  2.43s/it]predicting test subjects:  80%|████████  | 4/5 [00:08<00:02,  2.27s/it]predicting test subjects: 100%|██████████| 5/5 [00:11<00:00,  2.30s/it]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:02<12:40,  2.87s/it]predicting train subjects:   1%|          | 2/266 [00:05<12:15,  2.78s/it]predicting train subjects:   1%|          | 3/266 [00:07<11:11,  2.56s/it]predicting train subjects:   2%|▏         | 4/266 [00:09<10:20,  2.37s/it]predicting train subjects:   2%|▏         | 5/266 [00:11<10:31,  2.42s/it]predicting train subjects:   2%|▏         | 6/266 [00:14<10:52,  2.51s/it]predicting train subjects:   3%|▎         | 7/266 [00:17<11:03,  2.56s/it]predicting train subjects:   3%|▎         | 8/266 [00:20<11:10,  2.60s/it]predicting train subjects:   3%|▎         | 9/266 [00:22<11:17,  2.64s/it]predicting train subjects:   4%|▍         | 10/266 [00:25<11:33,  2.71s/it]predicting train subjects:   4%|▍         | 11/266 [00:28<11:31,  2.71s/it]predicting train subjects:   5%|▍         | 12/266 [00:31<11:26,  2.70s/it]predicting train subjects:   5%|▍         | 13/266 [00:33<11:23,  2.70s/it]predicting train subjects:   5%|▌         | 14/266 [00:36<11:23,  2.71s/it]predicting train subjects:   6%|▌         | 15/266 [00:39<11:20,  2.71s/it]predicting train subjects:   6%|▌         | 16/266 [00:41<11:09,  2.68s/it]predicting train subjects:   6%|▋         | 17/266 [00:44<11:02,  2.66s/it]predicting train subjects:   7%|▋         | 18/266 [00:47<11:17,  2.73s/it]predicting train subjects:   7%|▋         | 19/266 [00:50<11:30,  2.80s/it]predicting train subjects:   8%|▊         | 20/266 [00:52<11:16,  2.75s/it]predicting train subjects:   8%|▊         | 21/266 [00:55<11:10,  2.73s/it]predicting train subjects:   8%|▊         | 22/266 [00:58<11:07,  2.74s/it]predicting train subjects:   9%|▊         | 23/266 [01:00<10:53,  2.69s/it]predicting train subjects:   9%|▉         | 24/266 [01:03<10:34,  2.62s/it]predicting train subjects:   9%|▉         | 25/266 [01:05<10:18,  2.57s/it]predicting train subjects:  10%|▉         | 26/266 [01:08<10:10,  2.54s/it]predicting train subjects:  10%|█         | 27/266 [01:10<10:04,  2.53s/it]predicting train subjects:  11%|█         | 28/266 [01:13<09:53,  2.49s/it]predicting train subjects:  11%|█         | 29/266 [01:15<09:49,  2.49s/it]predicting train subjects:  11%|█▏        | 30/266 [01:18<09:39,  2.46s/it]predicting train subjects:  12%|█▏        | 31/266 [01:20<09:40,  2.47s/it]predicting train subjects:  12%|█▏        | 32/266 [01:22<09:33,  2.45s/it]predicting train subjects:  12%|█▏        | 33/266 [01:25<09:30,  2.45s/it]predicting train subjects:  13%|█▎        | 34/266 [01:27<09:30,  2.46s/it]predicting train subjects:  13%|█▎        | 35/266 [01:30<09:30,  2.47s/it]predicting train subjects:  14%|█▎        | 36/266 [01:32<09:26,  2.46s/it]predicting train subjects:  14%|█▍        | 37/266 [01:35<09:25,  2.47s/it]predicting train subjects:  14%|█▍        | 38/266 [01:37<09:28,  2.49s/it]predicting train subjects:  15%|█▍        | 39/266 [01:40<09:20,  2.47s/it]predicting train subjects:  15%|█▌        | 40/266 [01:42<09:18,  2.47s/it]predicting train subjects:  15%|█▌        | 41/266 [01:45<09:17,  2.48s/it]predicting train subjects:  16%|█▌        | 42/266 [01:47<08:44,  2.34s/it]predicting train subjects:  16%|█▌        | 43/266 [01:49<08:21,  2.25s/it]predicting train subjects:  17%|█▋        | 44/266 [01:51<08:06,  2.19s/it]predicting train subjects:  17%|█▋        | 45/266 [01:53<07:52,  2.14s/it]predicting train subjects:  17%|█▋        | 46/266 [01:55<07:41,  2.10s/it]predicting train subjects:  18%|█▊        | 47/266 [01:57<07:33,  2.07s/it]predicting train subjects:  18%|█▊        | 48/266 [01:59<07:29,  2.06s/it]predicting train subjects:  18%|█▊        | 49/266 [02:01<07:20,  2.03s/it]predicting train subjects:  19%|█▉        | 50/266 [02:03<07:17,  2.03s/it]predicting train subjects:  19%|█▉        | 51/266 [02:05<07:12,  2.01s/it]predicting train subjects:  20%|█▉        | 52/266 [02:07<07:10,  2.01s/it]predicting train subjects:  20%|█▉        | 53/266 [02:09<07:10,  2.02s/it]predicting train subjects:  20%|██        | 54/266 [02:11<07:11,  2.04s/it]predicting train subjects:  21%|██        | 55/266 [02:13<07:07,  2.03s/it]predicting train subjects:  21%|██        | 56/266 [02:15<07:05,  2.03s/it]predicting train subjects:  21%|██▏       | 57/266 [02:17<07:03,  2.03s/it]predicting train subjects:  22%|██▏       | 58/266 [02:19<07:01,  2.03s/it]predicting train subjects:  22%|██▏       | 59/266 [02:21<07:00,  2.03s/it]predicting train subjects:  23%|██▎       | 60/266 [02:23<06:50,  1.99s/it]predicting train subjects:  23%|██▎       | 61/266 [02:25<06:41,  1.96s/it]predicting train subjects:  23%|██▎       | 62/266 [02:27<06:36,  1.94s/it]predicting train subjects:  24%|██▎       | 63/266 [02:29<06:32,  1.93s/it]predicting train subjects:  24%|██▍       | 64/266 [02:31<06:29,  1.93s/it]predicting train subjects:  24%|██▍       | 65/266 [02:32<06:23,  1.91s/it]predicting train subjects:  25%|██▍       | 66/266 [02:34<06:20,  1.90s/it]predicting train subjects:  25%|██▌       | 67/266 [02:36<06:15,  1.89s/it]predicting train subjects:  26%|██▌       | 68/266 [02:38<06:12,  1.88s/it]predicting train subjects:  26%|██▌       | 69/266 [02:40<06:13,  1.89s/it]predicting train subjects:  26%|██▋       | 70/266 [02:42<06:12,  1.90s/it]predicting train subjects:  27%|██▋       | 71/266 [02:44<06:12,  1.91s/it]predicting train subjects:  27%|██▋       | 72/266 [02:46<06:08,  1.90s/it]predicting train subjects:  27%|██▋       | 73/266 [02:48<06:06,  1.90s/it]predicting train subjects:  28%|██▊       | 74/266 [02:50<06:05,  1.90s/it]predicting train subjects:  28%|██▊       | 75/266 [02:51<06:03,  1.90s/it]predicting train subjects:  29%|██▊       | 76/266 [02:53<06:01,  1.90s/it]predicting train subjects:  29%|██▉       | 77/266 [02:55<05:57,  1.89s/it]predicting train subjects:  29%|██▉       | 78/266 [02:58<06:28,  2.07s/it]predicting train subjects:  30%|██▉       | 79/266 [03:00<06:48,  2.19s/it]predicting train subjects:  30%|███       | 80/266 [03:03<07:04,  2.28s/it]predicting train subjects:  30%|███       | 81/266 [03:05<07:11,  2.33s/it]predicting train subjects:  31%|███       | 82/266 [03:08<07:17,  2.38s/it]predicting train subjects:  31%|███       | 83/266 [03:10<07:20,  2.41s/it]predicting train subjects:  32%|███▏      | 84/266 [03:13<07:22,  2.43s/it]predicting train subjects:  32%|███▏      | 85/266 [03:15<07:21,  2.44s/it]predicting train subjects:  32%|███▏      | 86/266 [03:18<07:22,  2.46s/it]predicting train subjects:  33%|███▎      | 87/266 [03:20<07:19,  2.46s/it]predicting train subjects:  33%|███▎      | 88/266 [03:22<07:18,  2.46s/it]predicting train subjects:  33%|███▎      | 89/266 [03:25<07:15,  2.46s/it]predicting train subjects:  34%|███▍      | 90/266 [03:27<07:12,  2.46s/it]predicting train subjects:  34%|███▍      | 91/266 [03:30<07:08,  2.45s/it]predicting train subjects:  35%|███▍      | 92/266 [03:32<07:06,  2.45s/it]predicting train subjects:  35%|███▍      | 93/266 [03:35<07:05,  2.46s/it]predicting train subjects:  35%|███▌      | 94/266 [03:37<07:04,  2.47s/it]predicting train subjects:  36%|███▌      | 95/266 [03:40<07:00,  2.46s/it]predicting train subjects:  36%|███▌      | 96/266 [03:42<06:42,  2.37s/it]predicting train subjects:  36%|███▋      | 97/266 [03:44<06:45,  2.40s/it]predicting train subjects:  37%|███▋      | 98/266 [03:47<06:42,  2.40s/it]predicting train subjects:  37%|███▋      | 99/266 [03:48<06:10,  2.22s/it]predicting train subjects:  38%|███▊      | 100/266 [03:50<05:57,  2.15s/it]predicting train subjects:  38%|███▊      | 101/266 [03:53<05:56,  2.16s/it]predicting train subjects:  38%|███▊      | 102/266 [03:55<05:52,  2.15s/it]predicting train subjects:  39%|███▊      | 103/266 [03:57<05:50,  2.15s/it]predicting train subjects:  39%|███▉      | 104/266 [03:59<05:46,  2.14s/it]predicting train subjects:  39%|███▉      | 105/266 [04:01<05:43,  2.13s/it]predicting train subjects:  40%|███▉      | 106/266 [04:03<05:40,  2.13s/it]predicting train subjects:  40%|████      | 107/266 [04:05<05:38,  2.13s/it]predicting train subjects:  41%|████      | 108/266 [04:08<05:38,  2.14s/it]predicting train subjects:  41%|████      | 109/266 [04:10<05:34,  2.13s/it]predicting train subjects:  41%|████▏     | 110/266 [04:12<05:31,  2.12s/it]predicting train subjects:  42%|████▏     | 111/266 [04:14<05:29,  2.13s/it]predicting train subjects:  42%|████▏     | 112/266 [04:16<05:27,  2.13s/it]predicting train subjects:  42%|████▏     | 113/266 [04:18<05:26,  2.13s/it]predicting train subjects:  43%|████▎     | 114/266 [04:20<05:23,  2.13s/it]predicting train subjects:  43%|████▎     | 115/266 [04:22<05:20,  2.12s/it]predicting train subjects:  44%|████▎     | 116/266 [04:25<05:17,  2.12s/it]predicting train subjects:  44%|████▍     | 117/266 [04:27<05:14,  2.11s/it]predicting train subjects:  44%|████▍     | 118/266 [04:29<05:11,  2.10s/it]predicting train subjects:  45%|████▍     | 119/266 [04:31<05:25,  2.22s/it]predicting train subjects:  45%|████▌     | 120/266 [04:34<05:34,  2.29s/it]predicting train subjects:  45%|████▌     | 121/266 [04:36<05:40,  2.35s/it]predicting train subjects:  46%|████▌     | 122/266 [04:39<05:42,  2.38s/it]predicting train subjects:  46%|████▌     | 123/266 [04:41<05:42,  2.40s/it]predicting train subjects:  47%|████▋     | 124/266 [04:44<05:44,  2.42s/it]predicting train subjects:  47%|████▋     | 125/266 [04:46<05:41,  2.42s/it]predicting train subjects:  47%|████▋     | 126/266 [04:48<05:42,  2.45s/it]predicting train subjects:  48%|████▊     | 127/266 [04:51<05:41,  2.46s/it]predicting train subjects:  48%|████▊     | 128/266 [04:53<05:39,  2.46s/it]predicting train subjects:  48%|████▊     | 129/266 [04:56<05:34,  2.44s/it]predicting train subjects:  49%|████▉     | 130/266 [04:58<05:32,  2.45s/it]predicting train subjects:  49%|████▉     | 131/266 [05:01<05:30,  2.45s/it]predicting train subjects:  50%|████▉     | 132/266 [05:03<05:29,  2.46s/it]predicting train subjects:  50%|█████     | 133/266 [05:06<05:27,  2.46s/it]predicting train subjects:  50%|█████     | 134/266 [05:08<05:24,  2.46s/it]predicting train subjects:  51%|█████     | 135/266 [05:11<05:23,  2.47s/it]predicting train subjects:  51%|█████     | 136/266 [05:13<05:21,  2.48s/it]predicting train subjects:  52%|█████▏    | 137/266 [05:16<05:17,  2.46s/it]predicting train subjects:  52%|█████▏    | 138/266 [05:18<05:14,  2.46s/it]predicting train subjects:  52%|█████▏    | 139/266 [05:20<05:09,  2.44s/it]predicting train subjects:  53%|█████▎    | 140/266 [05:23<05:06,  2.43s/it]predicting train subjects:  53%|█████▎    | 141/266 [05:25<05:05,  2.44s/it]predicting train subjects:  53%|█████▎    | 142/266 [05:28<05:01,  2.43s/it]predicting train subjects:  54%|█████▍    | 143/266 [05:30<04:57,  2.42s/it]predicting train subjects:  54%|█████▍    | 144/266 [05:33<04:57,  2.44s/it]predicting train subjects:  55%|█████▍    | 145/266 [05:35<04:53,  2.42s/it]predicting train subjects:  55%|█████▍    | 146/266 [05:37<04:48,  2.40s/it]predicting train subjects:  55%|█████▌    | 147/266 [05:40<04:43,  2.38s/it]predicting train subjects:  56%|█████▌    | 148/266 [05:42<04:39,  2.37s/it]predicting train subjects:  56%|█████▌    | 149/266 [05:44<04:37,  2.37s/it]predicting train subjects:  56%|█████▋    | 150/266 [05:47<04:36,  2.38s/it]predicting train subjects:  57%|█████▋    | 151/266 [05:49<04:32,  2.37s/it]predicting train subjects:  57%|█████▋    | 152/266 [05:51<04:32,  2.39s/it]predicting train subjects:  58%|█████▊    | 153/266 [05:54<04:30,  2.40s/it]predicting train subjects:  58%|█████▊    | 154/266 [05:56<04:28,  2.39s/it]predicting train subjects:  58%|█████▊    | 155/266 [05:58<04:04,  2.20s/it]predicting train subjects:  59%|█████▊    | 156/266 [06:00<03:49,  2.09s/it]predicting train subjects:  59%|█████▉    | 157/266 [06:02<03:37,  1.99s/it]predicting train subjects:  59%|█████▉    | 158/266 [06:03<03:28,  1.93s/it]predicting train subjects:  60%|█████▉    | 159/266 [06:05<03:20,  1.87s/it]predicting train subjects:  60%|██████    | 160/266 [06:07<03:16,  1.86s/it]predicting train subjects:  61%|██████    | 161/266 [06:09<03:12,  1.83s/it]predicting train subjects:  61%|██████    | 162/266 [06:10<03:06,  1.80s/it]predicting train subjects:  61%|██████▏   | 163/266 [06:12<03:03,  1.78s/it]predicting train subjects:  62%|██████▏   | 164/266 [06:14<02:59,  1.76s/it]predicting train subjects:  62%|██████▏   | 165/266 [06:16<02:56,  1.75s/it]predicting train subjects:  62%|██████▏   | 166/266 [06:17<02:54,  1.75s/it]predicting train subjects:  63%|██████▎   | 167/266 [06:19<02:51,  1.73s/it]predicting train subjects:  63%|██████▎   | 168/266 [06:21<02:48,  1.72s/it]predicting train subjects:  64%|██████▎   | 169/266 [06:22<02:46,  1.72s/it]predicting train subjects:  64%|██████▍   | 170/266 [06:24<02:45,  1.72s/it]predicting train subjects:  64%|██████▍   | 171/266 [06:26<02:43,  1.72s/it]predicting train subjects:  65%|██████▍   | 172/266 [06:28<02:42,  1.73s/it]predicting train subjects:  65%|██████▌   | 173/266 [06:30<02:48,  1.81s/it]predicting train subjects:  65%|██████▌   | 174/266 [06:32<02:53,  1.88s/it]predicting train subjects:  66%|██████▌   | 175/266 [06:34<02:54,  1.91s/it]predicting train subjects:  66%|██████▌   | 176/266 [06:36<02:55,  1.95s/it]predicting train subjects:  67%|██████▋   | 177/266 [06:38<02:54,  1.96s/it]predicting train subjects:  67%|██████▋   | 178/266 [06:40<02:53,  1.97s/it]predicting train subjects:  67%|██████▋   | 179/266 [06:42<02:49,  1.95s/it]predicting train subjects:  68%|██████▊   | 180/266 [06:44<02:48,  1.96s/it]predicting train subjects:  68%|██████▊   | 181/266 [06:46<02:48,  1.98s/it]predicting train subjects:  68%|██████▊   | 182/266 [06:48<02:46,  1.98s/it]predicting train subjects:  69%|██████▉   | 183/266 [06:50<02:43,  1.97s/it]predicting train subjects:  69%|██████▉   | 184/266 [06:52<02:41,  1.97s/it]predicting train subjects:  70%|██████▉   | 185/266 [06:54<02:40,  1.98s/it]predicting train subjects:  70%|██████▉   | 186/266 [06:56<02:38,  1.98s/it]predicting train subjects:  70%|███████   | 187/266 [06:57<02:36,  1.98s/it]predicting train subjects:  71%|███████   | 188/266 [06:59<02:33,  1.97s/it]predicting train subjects:  71%|███████   | 189/266 [07:01<02:32,  1.98s/it]predicting train subjects:  71%|███████▏  | 190/266 [07:03<02:29,  1.97s/it]predicting train subjects:  72%|███████▏  | 191/266 [07:05<02:29,  2.00s/it]predicting train subjects:  72%|███████▏  | 192/266 [07:07<02:23,  1.94s/it]predicting train subjects:  73%|███████▎  | 193/266 [07:09<02:20,  1.93s/it]predicting train subjects:  73%|███████▎  | 194/266 [07:12<02:28,  2.06s/it]predicting train subjects:  73%|███████▎  | 195/266 [07:14<02:27,  2.07s/it]predicting train subjects:  74%|███████▎  | 196/266 [07:16<02:25,  2.07s/it]predicting train subjects:  74%|███████▍  | 197/266 [07:18<02:24,  2.09s/it]predicting train subjects:  74%|███████▍  | 198/266 [07:20<02:21,  2.08s/it]predicting train subjects:  75%|███████▍  | 199/266 [07:22<02:20,  2.09s/it]predicting train subjects:  75%|███████▌  | 200/266 [07:24<02:17,  2.08s/it]predicting train subjects:  76%|███████▌  | 201/266 [07:26<02:15,  2.09s/it]predicting train subjects:  76%|███████▌  | 202/266 [07:28<02:14,  2.10s/it]predicting train subjects:  76%|███████▋  | 203/266 [07:30<02:11,  2.09s/it]predicting train subjects:  77%|███████▋  | 204/266 [07:32<02:09,  2.09s/it]predicting train subjects:  77%|███████▋  | 205/266 [07:35<02:07,  2.09s/it]predicting train subjects:  77%|███████▋  | 206/266 [07:37<02:06,  2.10s/it]predicting train subjects:  78%|███████▊  | 207/266 [07:39<02:03,  2.10s/it]predicting train subjects:  78%|███████▊  | 208/266 [07:41<02:02,  2.11s/it]predicting train subjects:  79%|███████▊  | 209/266 [07:43<02:00,  2.11s/it]predicting train subjects:  79%|███████▉  | 210/266 [07:45<01:59,  2.13s/it]predicting train subjects:  79%|███████▉  | 211/266 [07:47<01:56,  2.12s/it]predicting train subjects:  80%|███████▉  | 212/266 [07:49<01:55,  2.14s/it]predicting train subjects:  80%|████████  | 213/266 [07:51<01:48,  2.05s/it]predicting train subjects:  80%|████████  | 214/266 [07:53<01:43,  1.98s/it]predicting train subjects:  81%|████████  | 215/266 [07:55<01:38,  1.94s/it]predicting train subjects:  81%|████████  | 216/266 [07:57<01:35,  1.92s/it]predicting train subjects:  82%|████████▏ | 217/266 [07:59<01:33,  1.90s/it]predicting train subjects:  82%|████████▏ | 218/266 [08:01<01:30,  1.88s/it]predicting train subjects:  82%|████████▏ | 219/266 [08:02<01:28,  1.88s/it]predicting train subjects:  83%|████████▎ | 220/266 [08:04<01:26,  1.88s/it]predicting train subjects:  83%|████████▎ | 221/266 [08:06<01:25,  1.89s/it]predicting train subjects:  83%|████████▎ | 222/266 [08:08<01:24,  1.91s/it]predicting train subjects:  84%|████████▍ | 223/266 [08:10<01:21,  1.89s/it]predicting train subjects:  84%|████████▍ | 224/266 [08:12<01:19,  1.90s/it]predicting train subjects:  85%|████████▍ | 225/266 [08:14<01:17,  1.88s/it]predicting train subjects:  85%|████████▍ | 226/266 [08:16<01:15,  1.89s/it]predicting train subjects:  85%|████████▌ | 227/266 [08:18<01:13,  1.89s/it]predicting train subjects:  86%|████████▌ | 228/266 [08:19<01:11,  1.89s/it]predicting train subjects:  86%|████████▌ | 229/266 [08:21<01:09,  1.89s/it]predicting train subjects:  86%|████████▋ | 230/266 [08:23<01:07,  1.88s/it]predicting train subjects:  87%|████████▋ | 231/266 [08:25<01:06,  1.90s/it]predicting train subjects:  87%|████████▋ | 232/266 [08:27<01:04,  1.90s/it]predicting train subjects:  88%|████████▊ | 233/266 [08:29<01:02,  1.90s/it]predicting train subjects:  88%|████████▊ | 234/266 [08:31<01:00,  1.90s/it]predicting train subjects:  88%|████████▊ | 235/266 [08:33<00:58,  1.89s/it]predicting train subjects:  89%|████████▊ | 236/266 [08:35<00:56,  1.88s/it]predicting train subjects:  89%|████████▉ | 237/266 [08:36<00:54,  1.86s/it]predicting train subjects:  89%|████████▉ | 238/266 [08:38<00:52,  1.86s/it]predicting train subjects:  90%|████████▉ | 239/266 [08:40<00:49,  1.85s/it]predicting train subjects:  90%|█████████ | 240/266 [08:42<00:48,  1.85s/it]predicting train subjects:  91%|█████████ | 241/266 [08:44<00:45,  1.83s/it]predicting train subjects:  91%|█████████ | 242/266 [08:46<00:45,  1.89s/it]predicting train subjects:  91%|█████████▏| 243/266 [08:48<00:43,  1.89s/it]predicting train subjects:  92%|█████████▏| 244/266 [08:50<00:41,  1.90s/it]predicting train subjects:  92%|█████████▏| 245/266 [08:51<00:39,  1.90s/it]predicting train subjects:  92%|█████████▏| 246/266 [08:53<00:38,  1.91s/it]predicting train subjects:  93%|█████████▎| 247/266 [08:55<00:35,  1.89s/it]predicting train subjects:  93%|█████████▎| 248/266 [08:57<00:34,  1.92s/it]predicting train subjects:  94%|█████████▎| 249/266 [09:00<00:35,  2.06s/it]predicting train subjects:  94%|█████████▍| 250/266 [09:02<00:34,  2.16s/it]predicting train subjects:  94%|█████████▍| 251/266 [09:04<00:33,  2.23s/it]predicting train subjects:  95%|█████████▍| 252/266 [09:07<00:32,  2.29s/it]predicting train subjects:  95%|█████████▌| 253/266 [09:09<00:30,  2.31s/it]predicting train subjects:  95%|█████████▌| 254/266 [09:12<00:28,  2.35s/it]predicting train subjects:  96%|█████████▌| 255/266 [09:14<00:26,  2.38s/it]predicting train subjects:  96%|█████████▌| 256/266 [09:16<00:23,  2.39s/it]predicting train subjects:  97%|█████████▋| 257/266 [09:19<00:21,  2.40s/it]predicting train subjects:  97%|█████████▋| 258/266 [09:21<00:19,  2.43s/it]predicting train subjects:  97%|█████████▋| 259/266 [09:24<00:16,  2.41s/it]predicting train subjects:  98%|█████████▊| 260/266 [09:26<00:14,  2.41s/it]predicting train subjects:  98%|█████████▊| 261/266 [09:29<00:12,  2.41s/it]predicting train subjects:  98%|█████████▊| 262/266 [09:31<00:09,  2.40s/it]predicting train subjects:  99%|█████████▉| 263/266 [09:33<00:07,  2.40s/it]predicting train subjects:  99%|█████████▉| 264/266 [09:36<00:04,  2.39s/it]predicting train subjects: 100%|█████████▉| 265/266 [09:38<00:02,  2.37s/it]predicting train subjects: 100%|██████████| 266/266 [09:40<00:00,  2.37s/it]

