2020-01-20 22:18:19.959694: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:23.240504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:23.240566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:23.653890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:23.653962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:23.653973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:23.654425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['a']
TypeExperiment 9
CrossVal ['a']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:26.910031: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:29.410564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:29.410636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:29.832980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:29.833054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:29.833068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:29.833544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['b']
TypeExperiment 9
CrossVal ['b']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:34.570343: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:39.131720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:39.131759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:39.557867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:39.557927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:39.557940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:39.558423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['c']
TypeExperiment 9
CrossVal ['c']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:43.053303: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:45.594057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:45.594119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:46.013993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:46.014055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:46.014067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:46.014521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['d']
TypeExperiment 9
CrossVal ['d']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:21:38.885964: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:21:42.138959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:05:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:21:42.139021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:21:42.544223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:21:42.544295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:21:42.544308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:21:42.544753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:44,  2.53it/s]Loading train:   1%|          | 2/266 [00:00<01:34,  2.79it/s]Loading train:   1%|          | 3/266 [00:00<01:23,  3.16it/s]Loading train:   2%|▏         | 4/266 [00:01<01:15,  3.46it/s]Loading train:   2%|▏         | 5/266 [00:01<01:12,  3.58it/s]Loading train:   2%|▏         | 6/266 [00:01<01:10,  3.71it/s]Loading train:   3%|▎         | 7/266 [00:01<01:08,  3.77it/s]Loading train:   3%|▎         | 8/266 [00:02<01:07,  3.84it/s]Loading train:   3%|▎         | 9/266 [00:02<01:06,  3.87it/s]Loading train:   4%|▍         | 10/266 [00:02<01:05,  3.90it/s]Loading train:   4%|▍         | 11/266 [00:02<01:04,  3.93it/s]Loading train:   5%|▍         | 12/266 [00:03<01:04,  3.95it/s]Loading train:   5%|▍         | 13/266 [00:03<01:03,  3.96it/s]Loading train:   5%|▌         | 14/266 [00:03<01:03,  3.95it/s]Loading train:   6%|▌         | 15/266 [00:03<01:03,  3.96it/s]Loading train:   6%|▌         | 16/266 [00:04<01:03,  3.95it/s]Loading train:   6%|▋         | 17/266 [00:04<01:02,  3.97it/s]Loading train:   7%|▋         | 18/266 [00:04<01:02,  3.98it/s]Loading train:   7%|▋         | 19/266 [00:04<01:01,  3.99it/s]Loading train:   8%|▊         | 20/266 [00:05<01:01,  3.98it/s]Loading train:   8%|▊         | 21/266 [00:05<01:01,  3.97it/s]Loading train:   8%|▊         | 22/266 [00:05<01:01,  3.98it/s]Loading train:   9%|▊         | 23/266 [00:05<01:00,  3.99it/s]Loading train:   9%|▉         | 24/266 [00:06<00:59,  4.06it/s]Loading train:   9%|▉         | 25/266 [00:06<00:58,  4.09it/s]Loading train:  10%|▉         | 26/266 [00:06<00:58,  4.12it/s]Loading train:  10%|█         | 27/266 [00:06<00:57,  4.12it/s]Loading train:  11%|█         | 28/266 [00:07<00:58,  4.10it/s]Loading train:  11%|█         | 29/266 [00:07<00:57,  4.11it/s]Loading train:  11%|█▏        | 30/266 [00:07<00:57,  4.13it/s]Loading train:  12%|█▏        | 31/266 [00:07<00:56,  4.15it/s]Loading train:  12%|█▏        | 32/266 [00:08<00:56,  4.15it/s]Loading train:  12%|█▏        | 33/266 [00:08<00:56,  4.14it/s]Loading train:  13%|█▎        | 34/266 [00:08<00:55,  4.15it/s]Loading train:  13%|█▎        | 35/266 [00:08<00:55,  4.16it/s]Loading train:  14%|█▎        | 36/266 [00:09<00:55,  4.12it/s]Loading train:  14%|█▍        | 37/266 [00:09<00:55,  4.12it/s]Loading train:  14%|█▍        | 38/266 [00:09<00:55,  4.13it/s]Loading train:  15%|█▍        | 39/266 [00:09<00:54,  4.13it/s]Loading train:  15%|█▌        | 40/266 [00:09<00:54,  4.14it/s]Loading train:  15%|█▌        | 41/266 [00:10<00:54,  4.10it/s]Loading train:  16%|█▌        | 42/266 [00:10<00:53,  4.18it/s]Loading train:  16%|█▌        | 43/266 [00:10<00:51,  4.35it/s]Loading train:  17%|█▋        | 44/266 [00:10<00:52,  4.26it/s]Loading train:  17%|█▋        | 45/266 [00:11<00:49,  4.44it/s]Loading train:  17%|█▋        | 46/266 [00:11<00:47,  4.59it/s]Loading train:  18%|█▊        | 47/266 [00:11<00:46,  4.71it/s]Loading train:  18%|█▊        | 48/266 [00:11<00:45,  4.78it/s]Loading train:  18%|█▊        | 49/266 [00:11<00:44,  4.83it/s]Loading train:  19%|█▉        | 50/266 [00:12<00:44,  4.85it/s]Loading train:  19%|█▉        | 51/266 [00:12<00:44,  4.87it/s]Loading train:  20%|█▉        | 52/266 [00:12<00:43,  4.88it/s]Loading train:  20%|█▉        | 53/266 [00:12<00:43,  4.91it/s]Loading train:  20%|██        | 54/266 [00:12<00:43,  4.92it/s]Loading train:  21%|██        | 55/266 [00:13<00:42,  4.94it/s]Loading train:  21%|██        | 56/266 [00:13<00:42,  4.92it/s]Loading train:  21%|██▏       | 57/266 [00:13<00:42,  4.94it/s]Loading train:  22%|██▏       | 58/266 [00:13<00:42,  4.95it/s]Loading train:  22%|██▏       | 59/266 [00:13<00:41,  4.95it/s]Loading train:  23%|██▎       | 60/266 [00:14<00:42,  4.89it/s]Loading train:  23%|██▎       | 61/266 [00:14<00:42,  4.84it/s]Loading train:  23%|██▎       | 62/266 [00:14<00:42,  4.80it/s]Loading train:  24%|██▎       | 63/266 [00:14<00:42,  4.76it/s]Loading train:  24%|██▍       | 64/266 [00:15<00:42,  4.73it/s]Loading train:  24%|██▍       | 65/266 [00:15<00:42,  4.72it/s]Loading train:  25%|██▍       | 66/266 [00:15<00:42,  4.71it/s]Loading train:  25%|██▌       | 67/266 [00:15<00:42,  4.71it/s]Loading train:  26%|██▌       | 68/266 [00:15<00:41,  4.72it/s]Loading train:  26%|██▌       | 69/266 [00:16<00:41,  4.73it/s]Loading train:  26%|██▋       | 70/266 [00:16<00:41,  4.73it/s]Loading train:  27%|██▋       | 71/266 [00:16<00:41,  4.75it/s]Loading train:  27%|██▋       | 72/266 [00:16<00:40,  4.74it/s]Loading train:  27%|██▋       | 73/266 [00:16<00:40,  4.75it/s]Loading train:  28%|██▊       | 74/266 [00:17<00:40,  4.74it/s]Loading train:  28%|██▊       | 75/266 [00:17<00:40,  4.73it/s]Loading train:  29%|██▊       | 76/266 [00:17<00:40,  4.71it/s]Loading train:  29%|██▉       | 77/266 [00:17<00:40,  4.70it/s]Loading train:  29%|██▉       | 78/266 [00:18<00:41,  4.53it/s]Loading train:  30%|██▉       | 79/266 [00:18<00:42,  4.43it/s]Loading train:  30%|███       | 80/266 [00:18<00:42,  4.37it/s]Loading train:  30%|███       | 81/266 [00:18<00:42,  4.32it/s]Loading train:  31%|███       | 82/266 [00:18<00:42,  4.30it/s]Loading train:  31%|███       | 83/266 [00:19<00:42,  4.27it/s]Loading train:  32%|███▏      | 84/266 [00:19<00:42,  4.25it/s]Loading train:  32%|███▏      | 85/266 [00:19<00:42,  4.23it/s]Loading train:  32%|███▏      | 86/266 [00:19<00:42,  4.21it/s]Loading train:  33%|███▎      | 87/266 [00:20<00:42,  4.21it/s]Loading train:  33%|███▎      | 88/266 [00:20<00:42,  4.22it/s]Loading train:  33%|███▎      | 89/266 [00:20<00:41,  4.22it/s]Loading train:  34%|███▍      | 90/266 [00:20<00:41,  4.22it/s]Loading train:  34%|███▍      | 91/266 [00:21<00:41,  4.22it/s]Loading train:  35%|███▍      | 92/266 [00:21<00:41,  4.22it/s]Loading train:  35%|███▍      | 93/266 [00:21<00:41,  4.20it/s]Loading train:  35%|███▌      | 94/266 [00:21<00:40,  4.21it/s]Loading train:  36%|███▌      | 95/266 [00:22<00:40,  4.22it/s]Loading train:  36%|███▌      | 96/266 [00:22<00:39,  4.27it/s]Loading train:  36%|███▋      | 97/266 [00:22<00:41,  4.07it/s]Loading train:  37%|███▋      | 98/266 [00:22<00:40,  4.12it/s]Loading train:  37%|███▋      | 99/266 [00:22<00:38,  4.35it/s]Loading train:  38%|███▊      | 100/266 [00:23<00:38,  4.35it/s]Loading train:  38%|███▊      | 101/266 [00:23<00:37,  4.45it/s]Loading train:  38%|███▊      | 102/266 [00:23<00:36,  4.52it/s]Loading train:  39%|███▊      | 103/266 [00:23<00:35,  4.58it/s]Loading train:  39%|███▉      | 104/266 [00:24<00:35,  4.62it/s]Loading train:  39%|███▉      | 105/266 [00:24<00:34,  4.65it/s]Loading train:  40%|███▉      | 106/266 [00:24<00:34,  4.67it/s]Loading train:  40%|████      | 107/266 [00:24<00:33,  4.69it/s]Loading train:  41%|████      | 108/266 [00:24<00:33,  4.70it/s]Loading train:  41%|████      | 109/266 [00:25<00:33,  4.68it/s]Loading train:  41%|████▏     | 110/266 [00:25<00:33,  4.65it/s]Loading train:  42%|████▏     | 111/266 [00:25<00:33,  4.66it/s]Loading train:  42%|████▏     | 112/266 [00:25<00:32,  4.69it/s]Loading train:  42%|████▏     | 113/266 [00:25<00:32,  4.71it/s]Loading train:  43%|████▎     | 114/266 [00:26<00:32,  4.71it/s]Loading train:  43%|████▎     | 115/266 [00:26<00:32,  4.71it/s]Loading train:  44%|████▎     | 116/266 [00:26<00:31,  4.71it/s]Loading train:  44%|████▍     | 117/266 [00:26<00:31,  4.72it/s]Loading train:  44%|████▍     | 118/266 [00:27<00:31,  4.67it/s]Loading train:  45%|████▍     | 119/266 [00:27<00:33,  4.45it/s]Loading train:  45%|████▌     | 120/266 [00:27<00:33,  4.31it/s]Loading train:  45%|████▌     | 121/266 [00:27<00:34,  4.21it/s]Loading train:  46%|████▌     | 122/266 [00:28<00:34,  4.15it/s]Loading train:  46%|████▌     | 123/266 [00:28<00:34,  4.10it/s]Loading train:  47%|████▋     | 124/266 [00:28<00:34,  4.07it/s]Loading train:  47%|████▋     | 125/266 [00:28<00:34,  4.03it/s]Loading train:  47%|████▋     | 126/266 [00:29<00:34,  4.03it/s]Loading train:  48%|████▊     | 127/266 [00:29<00:34,  4.04it/s]Loading train:  48%|████▊     | 128/266 [00:29<00:34,  4.01it/s]Loading train:  48%|████▊     | 129/266 [00:29<00:34,  4.02it/s]Loading train:  49%|████▉     | 130/266 [00:30<00:33,  4.01it/s]Loading train:  49%|████▉     | 131/266 [00:30<00:33,  4.00it/s]Loading train:  50%|████▉     | 132/266 [00:30<00:33,  4.00it/s]Loading train:  50%|█████     | 133/266 [00:30<00:33,  4.01it/s]Loading train:  50%|█████     | 134/266 [00:31<00:33,  4.00it/s]Loading train:  51%|█████     | 135/266 [00:31<00:32,  3.99it/s]Loading train:  51%|█████     | 136/266 [00:31<00:32,  3.99it/s]Loading train:  52%|█████▏    | 137/266 [00:31<00:35,  3.60it/s]Loading train:  52%|█████▏    | 138/266 [00:32<00:48,  2.62it/s]Loading train:  52%|█████▏    | 139/266 [00:33<00:55,  2.31it/s]Loading train:  53%|█████▎    | 140/266 [00:33<00:58,  2.14it/s]Loading train:  53%|█████▎    | 141/266 [00:34<01:01,  2.02it/s]Loading train:  53%|█████▎    | 142/266 [00:34<01:10,  1.77it/s]Loading train:  54%|█████▍    | 143/266 [00:35<01:09,  1.78it/s]Loading train:  54%|█████▍    | 144/266 [00:35<01:04,  1.90it/s]Loading train:  55%|█████▍    | 145/266 [00:36<01:05,  1.84it/s]Loading train:  55%|█████▍    | 146/266 [00:36<01:03,  1.90it/s]Loading train:  55%|█████▌    | 147/266 [00:37<00:58,  2.02it/s]Loading train:  56%|█████▌    | 148/266 [00:37<00:52,  2.24it/s]Loading train:  56%|█████▌    | 149/266 [00:38<00:53,  2.20it/s]Loading train:  56%|█████▋    | 150/266 [00:38<00:51,  2.24it/s]Loading train:  57%|█████▋    | 151/266 [00:39<00:50,  2.28it/s]Loading train:  57%|█████▋    | 152/266 [00:39<00:48,  2.34it/s]Loading train:  58%|█████▊    | 153/266 [00:39<00:45,  2.50it/s]Loading train:  58%|█████▊    | 154/266 [00:39<00:38,  2.89it/s]Loading train:  58%|█████▊    | 155/266 [00:40<00:35,  3.15it/s]Loading train:  59%|█████▊    | 156/266 [00:40<00:32,  3.37it/s]Loading train:  59%|█████▉    | 157/266 [00:40<00:32,  3.31it/s]Loading train:  59%|█████▉    | 158/266 [00:41<00:32,  3.29it/s]Loading train:  60%|█████▉    | 159/266 [00:41<00:33,  3.23it/s]Loading train:  60%|██████    | 160/266 [00:41<00:34,  3.07it/s]Loading train:  61%|██████    | 161/266 [00:42<00:37,  2.77it/s]Loading train:  61%|██████    | 162/266 [00:42<00:36,  2.81it/s]Loading train:  61%|██████▏   | 163/266 [00:42<00:36,  2.80it/s]Loading train:  62%|██████▏   | 164/266 [00:43<00:35,  2.88it/s]Loading train:  62%|██████▏   | 165/266 [00:43<00:32,  3.10it/s]Loading train:  62%|██████▏   | 166/266 [00:43<00:30,  3.24it/s]Loading train:  63%|██████▎   | 167/266 [00:44<00:27,  3.58it/s]Loading train:  63%|██████▎   | 168/266 [00:44<00:27,  3.54it/s]Loading train:  64%|██████▎   | 169/266 [00:44<00:27,  3.56it/s]Loading train:  64%|██████▍   | 170/266 [00:44<00:27,  3.52it/s]Loading train:  64%|██████▍   | 171/266 [00:45<00:26,  3.61it/s]Loading train:  65%|██████▍   | 172/266 [00:45<00:26,  3.55it/s]Loading train:  65%|██████▌   | 173/266 [00:46<00:35,  2.64it/s]Loading train:  65%|██████▌   | 174/266 [00:46<00:39,  2.33it/s]Loading train:  66%|██████▌   | 175/266 [00:47<00:46,  1.95it/s]Loading train:  66%|██████▌   | 176/266 [00:47<00:49,  1.80it/s]Loading train:  67%|██████▋   | 177/266 [00:48<00:55,  1.60it/s]Loading train:  67%|██████▋   | 178/266 [00:49<00:58,  1.50it/s]Loading train:  67%|██████▋   | 179/266 [00:50<00:55,  1.57it/s]Loading train:  68%|██████▊   | 180/266 [00:50<00:54,  1.57it/s]Loading train:  68%|██████▊   | 181/266 [00:51<00:52,  1.61it/s]Loading train:  68%|██████▊   | 182/266 [00:51<00:51,  1.62it/s]Loading train:  69%|██████▉   | 183/266 [00:52<00:51,  1.61it/s]Loading train:  69%|██████▉   | 184/266 [00:53<00:53,  1.54it/s]Loading train:  70%|██████▉   | 185/266 [00:53<00:53,  1.50it/s]Loading train:  70%|██████▉   | 186/266 [00:54<00:51,  1.56it/s]Loading train:  70%|███████   | 187/266 [00:55<00:53,  1.48it/s]Loading train:  71%|███████   | 188/266 [00:55<00:52,  1.49it/s]Loading train:  71%|███████   | 189/266 [00:56<00:54,  1.41it/s]Loading train:  71%|███████▏  | 190/266 [00:57<00:49,  1.52it/s]Loading train:  72%|███████▏  | 191/266 [00:57<00:50,  1.49it/s]Loading train:  72%|███████▏  | 192/266 [00:58<00:48,  1.52it/s]Loading train:  73%|███████▎  | 193/266 [00:59<00:46,  1.58it/s]Loading train:  73%|███████▎  | 194/266 [00:59<00:46,  1.53it/s]Loading train:  73%|███████▎  | 195/266 [01:00<00:43,  1.62it/s]Loading train:  74%|███████▎  | 196/266 [01:01<00:42,  1.64it/s]Loading train:  74%|███████▍  | 197/266 [01:01<00:41,  1.67it/s]Loading train:  74%|███████▍  | 198/266 [01:02<00:39,  1.71it/s]Loading train:  75%|███████▍  | 199/266 [01:02<00:36,  1.82it/s]Loading train:  75%|███████▌  | 200/266 [01:03<00:34,  1.92it/s]Loading train:  76%|███████▌  | 201/266 [01:03<00:38,  1.70it/s]Loading train:  76%|███████▌  | 202/266 [01:04<00:38,  1.68it/s]Loading train:  76%|███████▋  | 203/266 [01:04<00:35,  1.77it/s]Loading train:  77%|███████▋  | 204/266 [01:05<00:36,  1.68it/s]Loading train:  77%|███████▋  | 205/266 [01:06<00:35,  1.70it/s]Loading train:  77%|███████▋  | 206/266 [01:06<00:34,  1.73it/s]Loading train:  78%|███████▊  | 207/266 [01:07<00:33,  1.75it/s]Loading train:  78%|███████▊  | 208/266 [01:07<00:30,  1.87it/s]Loading train:  79%|███████▊  | 209/266 [01:08<00:31,  1.80it/s]Loading train:  79%|███████▉  | 210/266 [01:08<00:32,  1.71it/s]Loading train:  79%|███████▉  | 211/266 [01:09<00:31,  1.74it/s]Loading train:  80%|███████▉  | 212/266 [01:10<00:34,  1.57it/s]Loading train:  80%|████████  | 213/266 [01:10<00:32,  1.62it/s]Loading train:  80%|████████  | 214/266 [01:11<00:32,  1.62it/s]Loading train:  81%|████████  | 215/266 [01:12<00:30,  1.67it/s]Loading train:  81%|████████  | 216/266 [01:12<00:30,  1.64it/s]Loading train:  82%|████████▏ | 217/266 [01:13<00:28,  1.72it/s]Loading train:  82%|████████▏ | 218/266 [01:13<00:28,  1.69it/s]Loading train:  82%|████████▏ | 219/266 [01:14<00:27,  1.72it/s]Loading train:  83%|████████▎ | 220/266 [01:14<00:26,  1.77it/s]Loading train:  83%|████████▎ | 221/266 [01:15<00:24,  1.80it/s]Loading train:  83%|████████▎ | 222/266 [01:16<00:24,  1.77it/s]Loading train:  84%|████████▍ | 223/266 [01:16<00:25,  1.69it/s]Loading train:  84%|████████▍ | 224/266 [01:17<00:26,  1.59it/s]Loading train:  85%|████████▍ | 225/266 [01:17<00:24,  1.65it/s]Loading train:  85%|████████▍ | 226/266 [01:18<00:24,  1.65it/s]Loading train:  85%|████████▌ | 227/266 [01:19<00:24,  1.60it/s]Loading train:  86%|████████▌ | 228/266 [01:19<00:23,  1.59it/s]Loading train:  86%|████████▌ | 229/266 [01:20<00:21,  1.69it/s]Loading train:  86%|████████▋ | 230/266 [01:20<00:20,  1.75it/s]Loading train:  87%|████████▋ | 231/266 [01:21<00:19,  1.77it/s]Loading train:  87%|████████▋ | 232/266 [01:21<00:19,  1.79it/s]Loading train:  88%|████████▊ | 233/266 [01:22<00:18,  1.74it/s]Loading train:  88%|████████▊ | 234/266 [01:23<00:19,  1.68it/s]Loading train:  88%|████████▊ | 235/266 [01:23<00:19,  1.61it/s]Loading train:  89%|████████▊ | 236/266 [01:24<00:18,  1.64it/s]Loading train:  89%|████████▉ | 237/266 [01:25<00:17,  1.67it/s]Loading train:  89%|████████▉ | 238/266 [01:25<00:16,  1.68it/s]Loading train:  90%|████████▉ | 239/266 [01:26<00:15,  1.72it/s]Loading train:  90%|█████████ | 240/266 [01:26<00:15,  1.68it/s]Loading train:  91%|█████████ | 241/266 [01:27<00:14,  1.69it/s]Loading train:  91%|█████████ | 242/266 [01:28<00:14,  1.66it/s]Loading train:  91%|█████████▏| 243/266 [01:28<00:13,  1.65it/s]Loading train:  92%|█████████▏| 244/266 [01:29<00:13,  1.66it/s]Loading train:  92%|█████████▏| 245/266 [01:29<00:12,  1.63it/s]Loading train:  92%|█████████▏| 246/266 [01:30<00:12,  1.59it/s]Loading train:  93%|█████████▎| 247/266 [01:31<00:12,  1.51it/s]Loading train:  93%|█████████▎| 248/266 [01:31<00:11,  1.53it/s]Loading train:  94%|█████████▎| 249/266 [01:32<00:11,  1.53it/s]Loading train:  94%|█████████▍| 250/266 [01:33<00:11,  1.42it/s]Loading train:  94%|█████████▍| 251/266 [01:34<00:10,  1.43it/s]Loading train:  95%|█████████▍| 252/266 [01:34<00:09,  1.45it/s]Loading train:  95%|█████████▌| 253/266 [01:35<00:08,  1.45it/s]Loading train:  95%|█████████▌| 254/266 [01:36<00:08,  1.43it/s]Loading train:  96%|█████████▌| 255/266 [01:36<00:07,  1.44it/s]Loading train:  96%|█████████▌| 256/266 [01:37<00:06,  1.47it/s]Loading train:  97%|█████████▋| 257/266 [01:38<00:06,  1.45it/s]Loading train:  97%|█████████▋| 258/266 [01:38<00:05,  1.47it/s]Loading train:  97%|█████████▋| 259/266 [01:39<00:04,  1.48it/s]Loading train:  98%|█████████▊| 260/266 [01:40<00:03,  1.53it/s]Loading train:  98%|█████████▊| 261/266 [01:40<00:03,  1.54it/s]Loading train:  98%|█████████▊| 262/266 [01:41<00:02,  1.53it/s]Loading train:  99%|█████████▉| 263/266 [01:42<00:01,  1.51it/s]Loading train:  99%|█████████▉| 264/266 [01:42<00:01,  1.53it/s]Loading train: 100%|█████████▉| 265/266 [01:43<00:00,  1.50it/s]Loading train: 100%|██████████| 266/266 [01:44<00:00,  1.50it/s]Loading train: 100%|██████████| 266/266 [01:44<00:00,  2.55it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   1%|          | 3/266 [00:00<00:11, 23.47it/s]concatenating: train:   3%|▎         | 8/266 [00:00<00:09, 27.58it/s]concatenating: train:   5%|▍         | 13/266 [00:00<00:07, 31.70it/s]concatenating: train:   7%|▋         | 18/266 [00:00<00:07, 34.60it/s]concatenating: train:   9%|▊         | 23/266 [00:00<00:06, 37.98it/s]concatenating: train:  11%|█         | 28/266 [00:00<00:05, 39.91it/s]concatenating: train:  12%|█▏        | 33/266 [00:00<00:05, 41.84it/s]concatenating: train:  15%|█▍        | 39/266 [00:00<00:05, 44.82it/s]concatenating: train:  17%|█▋        | 46/266 [00:00<00:04, 49.15it/s]concatenating: train:  20%|█▉        | 53/266 [00:01<00:03, 53.30it/s]concatenating: train:  23%|██▎       | 60/266 [00:01<00:03, 56.60it/s]concatenating: train:  25%|██▌       | 67/266 [00:01<00:03, 58.18it/s]concatenating: train:  28%|██▊       | 74/266 [00:01<00:03, 59.21it/s]concatenating: train:  30%|███       | 81/266 [00:01<00:03, 59.11it/s]concatenating: train:  33%|███▎      | 87/266 [00:01<00:03, 58.63it/s]concatenating: train:  35%|███▍      | 93/266 [00:01<00:02, 58.26it/s]concatenating: train:  37%|███▋      | 99/266 [00:01<00:02, 58.75it/s]concatenating: train:  40%|███▉      | 106/266 [00:01<00:02, 60.00it/s]concatenating: train:  42%|████▏     | 113/266 [00:02<00:02, 60.82it/s]concatenating: train:  45%|████▌     | 120/266 [00:02<00:02, 60.59it/s]concatenating: train:  48%|████▊     | 127/266 [00:02<00:02, 58.37it/s]concatenating: train:  50%|█████     | 133/266 [00:02<00:02, 57.27it/s]concatenating: train:  52%|█████▏    | 139/266 [00:02<00:02, 56.09it/s]concatenating: train:  55%|█████▍    | 145/266 [00:02<00:02, 54.55it/s]concatenating: train:  57%|█████▋    | 151/266 [00:02<00:02, 54.99it/s]concatenating: train:  59%|█████▉    | 158/266 [00:02<00:01, 57.62it/s]concatenating: train:  62%|██████▏   | 165/266 [00:02<00:01, 59.08it/s]concatenating: train:  65%|██████▍   | 172/266 [00:03<00:01, 61.85it/s]concatenating: train:  67%|██████▋   | 179/266 [00:03<00:01, 57.96it/s]concatenating: train:  70%|██████▉   | 185/266 [00:03<00:01, 55.38it/s]concatenating: train:  72%|███████▏  | 191/266 [00:03<00:01, 53.78it/s]concatenating: train:  74%|███████▍  | 197/266 [00:03<00:01, 52.30it/s]concatenating: train:  76%|███████▋  | 203/266 [00:03<00:01, 51.55it/s]concatenating: train:  79%|███████▊  | 209/266 [00:03<00:01, 50.08it/s]concatenating: train:  81%|████████  | 215/266 [00:03<00:01, 49.55it/s]concatenating: train:  83%|████████▎ | 220/266 [00:04<00:00, 49.11it/s]concatenating: train:  85%|████████▍ | 226/266 [00:04<00:00, 49.37it/s]concatenating: train:  87%|████████▋ | 231/266 [00:04<00:00, 48.54it/s]concatenating: train:  89%|████████▊ | 236/266 [00:04<00:00, 46.59it/s]concatenating: train:  91%|█████████ | 241/266 [00:04<00:00, 46.75it/s]concatenating: train:  92%|█████████▏| 246/266 [00:04<00:00, 47.18it/s]concatenating: train:  94%|█████████▍| 251/266 [00:04<00:00, 45.13it/s]concatenating: train:  96%|█████████▌| 256/266 [00:04<00:00, 42.73it/s]concatenating: train:  98%|█████████▊| 261/266 [00:04<00:00, 43.51it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 43.80it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 52.18it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:01,  1.62it/s]Loading test:  50%|█████     | 2/4 [00:01<00:01,  1.64it/s]Loading test:  75%|███████▌  | 3/4 [00:01<00:00,  1.68it/s]Loading test: 100%|██████████| 4/4 [00:02<00:00,  1.69it/s]Loading test: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 66.46it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:00<02:44,  1.61it/s]Loading trainS:   1%|          | 2/266 [00:01<02:50,  1.55it/s]Loading trainS:   1%|          | 3/266 [00:01<02:43,  1.61it/s]Loading trainS:   2%|▏         | 4/266 [00:02<02:33,  1.71it/s]Loading trainS:   2%|▏         | 5/266 [00:02<02:31,  1.72it/s]Loading trainS:   2%|▏         | 6/266 [00:03<02:36,  1.66it/s]Loading trainS:   3%|▎         | 7/266 [00:04<02:30,  1.72it/s]Loading trainS:   3%|▎         | 8/266 [00:04<02:32,  1.69it/s]Loading trainS:   3%|▎         | 9/266 [00:05<02:41,  1.59it/s]Loading trainS:   4%|▍         | 10/266 [00:06<02:44,  1.55it/s]Loading trainS:   4%|▍         | 11/266 [00:06<02:39,  1.60it/s]Loading trainS:   5%|▍         | 12/266 [00:07<02:34,  1.64it/s]Loading trainS:   5%|▍         | 13/266 [00:07<02:34,  1.64it/s]Loading trainS:   5%|▌         | 14/266 [00:08<02:31,  1.66it/s]Loading trainS:   6%|▌         | 15/266 [00:09<02:37,  1.59it/s]Loading trainS:   6%|▌         | 16/266 [00:09<02:37,  1.59it/s]Loading trainS:   6%|▋         | 17/266 [00:10<02:37,  1.58it/s]Loading trainS:   7%|▋         | 18/266 [00:11<02:39,  1.55it/s]Loading trainS:   7%|▋         | 19/266 [00:11<02:32,  1.62it/s]Loading trainS:   8%|▊         | 20/266 [00:12<02:34,  1.59it/s]Loading trainS:   8%|▊         | 21/266 [00:13<02:40,  1.53it/s]Loading trainS:   8%|▊         | 22/266 [00:13<02:37,  1.55it/s]Loading trainS:   9%|▊         | 23/266 [00:14<02:37,  1.54it/s]Loading trainS:   9%|▉         | 24/266 [00:14<02:29,  1.62it/s]Loading trainS:   9%|▉         | 25/266 [00:15<02:28,  1.62it/s]Loading trainS:  10%|▉         | 26/266 [00:16<02:32,  1.57it/s]Loading trainS:  10%|█         | 27/266 [00:16<02:29,  1.60it/s]Loading trainS:  11%|█         | 28/266 [00:17<02:27,  1.61it/s]Loading trainS:  11%|█         | 29/266 [00:17<02:23,  1.65it/s]Loading trainS:  11%|█▏        | 30/266 [00:18<02:23,  1.64it/s]Loading trainS:  12%|█▏        | 31/266 [00:19<02:28,  1.58it/s]Loading trainS:  12%|█▏        | 32/266 [00:19<02:24,  1.62it/s]Loading trainS:  12%|█▏        | 33/266 [00:20<02:27,  1.58it/s]Loading trainS:  13%|█▎        | 34/266 [00:21<02:23,  1.62it/s]Loading trainS:  13%|█▎        | 35/266 [00:21<02:23,  1.61it/s]Loading trainS:  14%|█▎        | 36/266 [00:22<02:20,  1.63it/s]Loading trainS:  14%|█▍        | 37/266 [00:22<02:16,  1.68it/s]Loading trainS:  14%|█▍        | 38/266 [00:23<02:17,  1.65it/s]Loading trainS:  15%|█▍        | 39/266 [00:24<02:17,  1.65it/s]Loading trainS:  15%|█▌        | 40/266 [00:24<02:22,  1.59it/s]Loading trainS:  15%|█▌        | 41/266 [00:25<02:15,  1.66it/s]Loading trainS:  16%|█▌        | 42/266 [00:25<02:12,  1.69it/s]Loading trainS:  16%|█▌        | 43/266 [00:26<02:02,  1.83it/s]Loading trainS:  17%|█▋        | 44/266 [00:26<01:58,  1.87it/s]Loading trainS:  17%|█▋        | 45/266 [00:27<02:00,  1.83it/s]Loading trainS:  17%|█▋        | 46/266 [00:27<01:55,  1.91it/s]Loading trainS:  18%|█▊        | 47/266 [00:28<01:49,  2.00it/s]Loading trainS:  18%|█▊        | 48/266 [00:28<01:44,  2.09it/s]Loading trainS:  18%|█▊        | 49/266 [00:29<01:46,  2.05it/s]Loading trainS:  19%|█▉        | 50/266 [00:29<01:46,  2.03it/s]Loading trainS:  19%|█▉        | 51/266 [00:30<01:47,  2.01it/s]Loading trainS:  20%|█▉        | 52/266 [00:30<01:42,  2.08it/s]Loading trainS:  20%|█▉        | 53/266 [00:31<01:40,  2.11it/s]Loading trainS:  20%|██        | 54/266 [00:31<01:46,  1.99it/s]Loading trainS:  21%|██        | 55/266 [00:32<01:44,  2.02it/s]Loading trainS:  21%|██        | 56/266 [00:32<01:42,  2.05it/s]Loading trainS:  21%|██▏       | 57/266 [00:33<01:47,  1.94it/s]Loading trainS:  22%|██▏       | 58/266 [00:33<01:46,  1.95it/s]Loading trainS:  22%|██▏       | 59/266 [00:34<01:49,  1.90it/s]Loading trainS:  23%|██▎       | 60/266 [00:34<01:48,  1.89it/s]Loading trainS:  23%|██▎       | 61/266 [00:35<01:48,  1.89it/s]Loading trainS:  23%|██▎       | 62/266 [00:36<01:50,  1.84it/s]Loading trainS:  24%|██▎       | 63/266 [00:36<01:51,  1.83it/s]Loading trainS:  24%|██▍       | 64/266 [00:37<01:47,  1.88it/s]Loading trainS:  24%|██▍       | 65/266 [00:37<01:42,  1.97it/s]Loading trainS:  25%|██▍       | 66/266 [00:38<01:53,  1.76it/s]Loading trainS:  25%|██▌       | 67/266 [00:38<01:48,  1.83it/s]Loading trainS:  26%|██▌       | 68/266 [00:39<01:48,  1.83it/s]Loading trainS:  26%|██▌       | 69/266 [00:39<01:52,  1.76it/s]Loading trainS:  26%|██▋       | 70/266 [00:40<01:49,  1.78it/s]Loading trainS:  27%|██▋       | 71/266 [00:40<01:46,  1.83it/s]Loading trainS:  27%|██▋       | 72/266 [00:41<01:40,  1.93it/s]Loading trainS:  27%|██▋       | 73/266 [00:41<01:41,  1.90it/s]Loading trainS:  28%|██▊       | 74/266 [00:42<01:38,  1.94it/s]Loading trainS:  28%|██▊       | 75/266 [00:42<01:37,  1.95it/s]Loading trainS:  29%|██▊       | 76/266 [00:43<01:39,  1.90it/s]Loading trainS:  29%|██▉       | 77/266 [00:44<01:42,  1.84it/s]Loading trainS:  29%|██▉       | 78/266 [00:44<01:48,  1.73it/s]Loading trainS:  30%|██▉       | 79/266 [00:45<01:51,  1.67it/s]Loading trainS:  30%|███       | 80/266 [00:45<01:46,  1.74it/s]Loading trainS:  30%|███       | 81/266 [00:46<01:50,  1.67it/s]Loading trainS:  31%|███       | 82/266 [00:47<01:49,  1.68it/s]Loading trainS:  31%|███       | 83/266 [00:47<01:48,  1.68it/s]Loading trainS:  32%|███▏      | 84/266 [00:48<01:49,  1.66it/s]Loading trainS:  32%|███▏      | 85/266 [00:48<01:50,  1.63it/s]Loading trainS:  32%|███▏      | 86/266 [00:49<01:49,  1.64it/s]Loading trainS:  33%|███▎      | 87/266 [00:50<01:49,  1.63it/s]Loading trainS:  33%|███▎      | 88/266 [00:50<01:50,  1.61it/s]Loading trainS:  33%|███▎      | 89/266 [00:51<01:45,  1.68it/s]Loading trainS:  34%|███▍      | 90/266 [00:52<01:46,  1.65it/s]Loading trainS:  34%|███▍      | 91/266 [00:52<01:48,  1.61it/s]Loading trainS:  35%|███▍      | 92/266 [00:53<01:47,  1.61it/s]Loading trainS:  35%|███▍      | 93/266 [00:53<01:50,  1.57it/s]Loading trainS:  35%|███▌      | 94/266 [00:54<01:48,  1.58it/s]Loading trainS:  36%|███▌      | 95/266 [00:55<01:52,  1.53it/s]Loading trainS:  36%|███▌      | 96/266 [00:55<01:47,  1.58it/s]Loading trainS:  36%|███▋      | 97/266 [00:56<01:52,  1.51it/s]Loading trainS:  37%|███▋      | 98/266 [00:57<01:52,  1.49it/s]Loading trainS:  37%|███▋      | 99/266 [00:57<01:40,  1.67it/s]Loading trainS:  38%|███▊      | 100/266 [00:58<01:36,  1.72it/s]Loading trainS:  38%|███▊      | 101/266 [00:58<01:31,  1.80it/s]Loading trainS:  38%|███▊      | 102/266 [00:59<01:27,  1.88it/s]Loading trainS:  39%|███▊      | 103/266 [00:59<01:26,  1.88it/s]Loading trainS:  39%|███▉      | 104/266 [01:00<01:26,  1.88it/s]Loading trainS:  39%|███▉      | 105/266 [01:00<01:26,  1.85it/s]Loading trainS:  40%|███▉      | 106/266 [01:01<01:26,  1.86it/s]Loading trainS:  40%|████      | 107/266 [01:01<01:23,  1.89it/s]Loading trainS:  41%|████      | 108/266 [01:02<01:24,  1.87it/s]Loading trainS:  41%|████      | 109/266 [01:02<01:20,  1.94it/s]Loading trainS:  41%|████▏     | 110/266 [01:03<01:20,  1.95it/s]Loading trainS:  42%|████▏     | 111/266 [01:03<01:18,  1.96it/s]Loading trainS:  42%|████▏     | 112/266 [01:04<01:19,  1.93it/s]Loading trainS:  42%|████▏     | 113/266 [01:04<01:19,  1.93it/s]Loading trainS:  43%|████▎     | 114/266 [01:05<01:23,  1.82it/s]Loading trainS:  43%|████▎     | 115/266 [01:06<01:22,  1.84it/s]Loading trainS:  44%|████▎     | 116/266 [01:06<01:17,  1.93it/s]Loading trainS:  44%|████▍     | 117/266 [01:07<01:21,  1.82it/s]Loading trainS:  44%|████▍     | 118/266 [01:07<01:20,  1.84it/s]Loading trainS:  45%|████▍     | 119/266 [01:08<01:24,  1.75it/s]Loading trainS:  45%|████▌     | 120/266 [01:09<01:26,  1.68it/s]Loading trainS:  45%|████▌     | 121/266 [01:09<01:26,  1.67it/s]Loading trainS:  46%|████▌     | 122/266 [01:10<01:26,  1.66it/s]Loading trainS:  46%|████▌     | 123/266 [01:10<01:28,  1.62it/s]Loading trainS:  47%|████▋     | 124/266 [01:11<01:32,  1.53it/s]Loading trainS:  47%|████▋     | 125/266 [01:12<01:32,  1.53it/s]Loading trainS:  47%|████▋     | 126/266 [01:12<01:29,  1.57it/s]Loading trainS:  48%|████▊     | 127/266 [01:13<01:28,  1.58it/s]Loading trainS:  48%|████▊     | 128/266 [01:14<01:27,  1.58it/s]Loading trainS:  48%|████▊     | 129/266 [01:14<01:26,  1.58it/s]Loading trainS:  49%|████▉     | 130/266 [01:15<01:27,  1.55it/s]Loading trainS:  49%|████▉     | 131/266 [01:16<01:26,  1.55it/s]Loading trainS:  50%|████▉     | 132/266 [01:16<01:24,  1.58it/s]Loading trainS:  50%|█████     | 133/266 [01:17<01:25,  1.56it/s]Loading trainS:  50%|█████     | 134/266 [01:17<01:24,  1.57it/s]Loading trainS:  51%|█████     | 135/266 [01:18<01:22,  1.59it/s]Loading trainS:  51%|█████     | 136/266 [01:19<01:18,  1.66it/s]Loading trainS:  52%|█████▏    | 137/266 [01:19<01:16,  1.70it/s]Loading trainS:  52%|█████▏    | 138/266 [01:20<01:13,  1.75it/s]Loading trainS:  52%|█████▏    | 139/266 [01:20<01:11,  1.78it/s]Loading trainS:  53%|█████▎    | 140/266 [01:21<01:11,  1.77it/s]Loading trainS:  53%|█████▎    | 141/266 [01:21<01:11,  1.76it/s]Loading trainS:  53%|█████▎    | 142/266 [01:22<01:09,  1.78it/s]Loading trainS:  54%|█████▍    | 143/266 [01:23<01:08,  1.79it/s]Loading trainS:  54%|█████▍    | 144/266 [01:23<01:08,  1.78it/s]Loading trainS:  55%|█████▍    | 145/266 [01:24<01:05,  1.86it/s]Loading trainS:  55%|█████▍    | 146/266 [01:24<01:04,  1.86it/s]Loading trainS:  55%|█████▌    | 147/266 [01:25<01:05,  1.80it/s]Loading trainS:  56%|█████▌    | 148/266 [01:25<01:07,  1.76it/s]Loading trainS:  56%|█████▌    | 149/266 [01:26<01:10,  1.66it/s]Loading trainS:  56%|█████▋    | 150/266 [01:27<01:09,  1.67it/s]Loading trainS:  57%|█████▋    | 151/266 [01:27<01:05,  1.74it/s]Loading trainS:  57%|█████▋    | 152/266 [01:28<01:04,  1.77it/s]Loading trainS:  58%|█████▊    | 153/266 [01:28<01:05,  1.73it/s]Loading trainS:  58%|█████▊    | 154/266 [01:29<01:03,  1.76it/s]Loading trainS:  58%|█████▊    | 155/266 [01:29<00:58,  1.89it/s]Loading trainS:  59%|█████▊    | 156/266 [01:30<00:56,  1.95it/s]Loading trainS:  59%|█████▉    | 157/266 [01:30<00:51,  2.10it/s]Loading trainS:  59%|█████▉    | 158/266 [01:31<00:51,  2.10it/s]Loading trainS:  60%|█████▉    | 159/266 [01:31<00:51,  2.09it/s]Loading trainS:  60%|██████    | 160/266 [01:32<00:53,  1.99it/s]Loading trainS:  61%|██████    | 161/266 [01:32<00:53,  1.95it/s]Loading trainS:  61%|██████    | 162/266 [01:33<00:52,  1.97it/s]Loading trainS:  61%|██████▏   | 163/266 [01:33<00:49,  2.09it/s]Loading trainS:  62%|██████▏   | 164/266 [01:34<00:49,  2.08it/s]Loading trainS:  62%|██████▏   | 165/266 [01:34<00:48,  2.07it/s]Loading trainS:  62%|██████▏   | 166/266 [01:34<00:47,  2.11it/s]Loading trainS:  63%|██████▎   | 167/266 [01:35<00:46,  2.12it/s]Loading trainS:  63%|██████▎   | 168/266 [01:35<00:45,  2.17it/s]Loading trainS:  64%|██████▎   | 169/266 [01:36<00:42,  2.30it/s]Loading trainS:  64%|██████▍   | 170/266 [01:36<00:44,  2.13it/s]Loading trainS:  64%|██████▍   | 171/266 [01:37<00:46,  2.04it/s]Loading trainS:  65%|██████▍   | 172/266 [01:37<00:47,  1.98it/s]Loading trainS:  65%|██████▌   | 173/266 [01:38<00:49,  1.89it/s]Loading trainS:  65%|██████▌   | 174/266 [01:38<00:47,  1.95it/s]Loading trainS:  66%|██████▌   | 175/266 [01:39<00:44,  2.03it/s]Loading trainS:  66%|██████▌   | 176/266 [01:39<00:44,  2.01it/s]Loading trainS:  67%|██████▋   | 177/266 [01:40<00:45,  1.94it/s]Loading trainS:  67%|██████▋   | 178/266 [01:41<00:47,  1.86it/s]Loading trainS:  67%|██████▋   | 179/266 [01:41<00:46,  1.89it/s]Loading trainS:  68%|██████▊   | 180/266 [01:42<00:47,  1.81it/s]Loading trainS:  68%|██████▊   | 181/266 [01:42<00:48,  1.76it/s]Loading trainS:  68%|██████▊   | 182/266 [01:43<00:49,  1.70it/s]Loading trainS:  69%|██████▉   | 183/266 [01:44<00:49,  1.68it/s]Loading trainS:  69%|██████▉   | 184/266 [01:44<00:49,  1.67it/s]Loading trainS:  70%|██████▉   | 185/266 [01:45<00:50,  1.60it/s]Loading trainS:  70%|██████▉   | 186/266 [01:45<00:50,  1.59it/s]Loading trainS:  70%|███████   | 187/266 [01:46<00:48,  1.63it/s]Loading trainS:  71%|███████   | 188/266 [01:47<00:45,  1.73it/s]Loading trainS:  71%|███████   | 189/266 [01:47<00:43,  1.77it/s]Loading trainS:  71%|███████▏  | 190/266 [01:48<00:40,  1.87it/s]Loading trainS:  72%|███████▏  | 191/266 [01:48<00:34,  2.16it/s]Loading trainS:  72%|███████▏  | 192/266 [01:48<00:29,  2.47it/s]Loading trainS:  73%|███████▎  | 193/266 [01:48<00:26,  2.75it/s]Loading trainS:  73%|███████▎  | 194/266 [01:49<00:25,  2.78it/s]Loading trainS:  73%|███████▎  | 195/266 [01:49<00:26,  2.66it/s]Loading trainS:  74%|███████▎  | 196/266 [01:49<00:25,  2.69it/s]Loading trainS:  74%|███████▍  | 197/266 [01:50<00:28,  2.43it/s]Loading trainS:  74%|███████▍  | 198/266 [01:50<00:29,  2.32it/s]Loading trainS:  75%|███████▍  | 199/266 [01:51<00:28,  2.33it/s]Loading trainS:  75%|███████▌  | 200/266 [01:51<00:28,  2.31it/s]Loading trainS:  76%|███████▌  | 201/266 [01:52<00:29,  2.24it/s]Loading trainS:  76%|███████▌  | 202/266 [01:52<00:28,  2.22it/s]Loading trainS:  76%|███████▋  | 203/266 [01:53<00:28,  2.23it/s]Loading trainS:  77%|███████▋  | 204/266 [01:53<00:27,  2.25it/s]Loading trainS:  77%|███████▋  | 205/266 [01:54<00:26,  2.31it/s]Loading trainS:  77%|███████▋  | 206/266 [01:54<00:27,  2.16it/s]Loading trainS:  78%|███████▊  | 207/266 [01:55<00:27,  2.17it/s]Loading trainS:  78%|███████▊  | 208/266 [01:55<00:25,  2.24it/s]Loading trainS:  79%|███████▊  | 209/266 [01:55<00:23,  2.46it/s]Loading trainS:  79%|███████▉  | 210/266 [01:56<00:21,  2.57it/s]Loading trainS:  79%|███████▉  | 211/266 [01:56<00:24,  2.28it/s]Loading trainS:  80%|███████▉  | 212/266 [01:57<00:23,  2.32it/s]Loading trainS:  80%|████████  | 213/266 [01:57<00:22,  2.33it/s]Loading trainS:  80%|████████  | 214/266 [01:57<00:21,  2.42it/s]Loading trainS:  81%|████████  | 215/266 [01:58<00:21,  2.38it/s]Loading trainS:  81%|████████  | 216/266 [01:58<00:21,  2.31it/s]Loading trainS:  82%|████████▏ | 217/266 [01:59<00:20,  2.36it/s]Loading trainS:  82%|████████▏ | 218/266 [01:59<00:20,  2.33it/s]Loading trainS:  82%|████████▏ | 219/266 [02:00<00:23,  2.01it/s]Loading trainS:  83%|████████▎ | 220/266 [02:00<00:21,  2.16it/s]Loading trainS:  83%|████████▎ | 221/266 [02:01<00:19,  2.33it/s]Loading trainS:  83%|████████▎ | 222/266 [02:01<00:18,  2.33it/s]Loading trainS:  84%|████████▍ | 223/266 [02:01<00:17,  2.48it/s]Loading trainS:  84%|████████▍ | 224/266 [02:02<00:19,  2.18it/s]Loading trainS:  85%|████████▍ | 225/266 [02:02<00:18,  2.26it/s]Loading trainS:  85%|████████▍ | 226/266 [02:03<00:16,  2.43it/s]Loading trainS:  85%|████████▌ | 227/266 [02:03<00:15,  2.50it/s]Loading trainS:  86%|████████▌ | 228/266 [02:03<00:14,  2.60it/s]Loading trainS:  86%|████████▌ | 229/266 [02:04<00:14,  2.47it/s]Loading trainS:  86%|████████▋ | 230/266 [02:04<00:14,  2.51it/s]Loading trainS:  87%|████████▋ | 231/266 [02:05<00:15,  2.32it/s]Loading trainS:  87%|████████▋ | 232/266 [02:05<00:13,  2.45it/s]Loading trainS:  88%|████████▊ | 233/266 [02:06<00:14,  2.21it/s]Loading trainS:  88%|████████▊ | 234/266 [02:06<00:14,  2.19it/s]Loading trainS:  88%|████████▊ | 235/266 [02:06<00:13,  2.31it/s]Loading trainS:  89%|████████▊ | 236/266 [02:07<00:12,  2.32it/s]Loading trainS:  89%|████████▉ | 237/266 [02:07<00:12,  2.23it/s]Loading trainS:  89%|████████▉ | 238/266 [02:08<00:12,  2.26it/s]Loading trainS:  90%|████████▉ | 239/266 [02:08<00:13,  2.05it/s]Loading trainS:  90%|█████████ | 240/266 [02:09<00:13,  1.99it/s]Loading trainS:  91%|█████████ | 241/266 [02:09<00:11,  2.14it/s]Loading trainS:  91%|█████████ | 242/266 [02:10<00:11,  2.01it/s]Loading trainS:  91%|█████████▏| 243/266 [02:10<00:10,  2.11it/s]Loading trainS:  92%|█████████▏| 244/266 [02:11<00:09,  2.23it/s]Loading trainS:  92%|█████████▏| 245/266 [02:11<00:09,  2.28it/s]Loading trainS:  92%|█████████▏| 246/266 [02:12<00:08,  2.31it/s]Loading trainS:  93%|█████████▎| 247/266 [02:12<00:07,  2.46it/s]Loading trainS:  93%|█████████▎| 248/266 [02:12<00:07,  2.49it/s]Loading trainS:  94%|█████████▎| 249/266 [02:13<00:06,  2.44it/s]Loading trainS:  94%|█████████▍| 250/266 [02:13<00:07,  2.28it/s]Loading trainS:  94%|█████████▍| 251/266 [02:14<00:06,  2.23it/s]Loading trainS:  95%|█████████▍| 252/266 [02:14<00:06,  2.12it/s]Loading trainS:  95%|█████████▌| 253/266 [02:15<00:06,  2.02it/s]Loading trainS:  95%|█████████▌| 254/266 [02:15<00:05,  2.09it/s]Loading trainS:  96%|█████████▌| 255/266 [02:16<00:05,  2.08it/s]Loading trainS:  96%|█████████▌| 256/266 [02:16<00:04,  2.06it/s]Loading trainS:  97%|█████████▋| 257/266 [02:17<00:04,  2.06it/s]Loading trainS:  97%|█████████▋| 258/266 [02:17<00:04,  1.92it/s]Loading trainS:  97%|█████████▋| 259/266 [02:18<00:03,  1.92it/s]Loading trainS:  98%|█████████▊| 260/266 [02:18<00:03,  1.83it/s]Loading trainS:  98%|█████████▊| 261/266 [02:19<00:02,  1.82it/s]Loading trainS:  98%|█████████▊| 262/266 [02:19<00:02,  1.92it/s]Loading trainS:  99%|█████████▉| 263/266 [02:20<00:01,  2.18it/s]Loading trainS:  99%|█████████▉| 264/266 [02:20<00:00,  2.39it/s]Loading trainS: 100%|█████████▉| 265/266 [02:20<00:00,  2.63it/s]Loading trainS: 100%|██████████| 266/266 [02:21<00:00,  2.60it/s]Loading trainS: 100%|██████████| 266/266 [02:21<00:00,  1.88it/s]
Loading testS:   0%|          | 0/4 [00:00<?, ?it/s]Loading testS:  25%|██▌       | 1/4 [00:00<00:01,  2.20it/s]Loading testS:  50%|█████     | 2/4 [00:00<00:00,  2.24it/s]Loading testS:  75%|███████▌  | 3/4 [00:01<00:00,  2.07it/s]Loading testS: 100%|██████████| 4/4 [00:01<00:00,  2.04it/s]Loading testS: 100%|██████████| 4/4 [00:01<00:00,  2.04it/s]----------+++ 
CrossVal ['a']
TypeExperiment 9
CrossVal ['a']
(0/4) test vimp2_A_CSFn2
(1/4) test vimp2_ANON967_CSFn2
(2/4) test vimp2_B_CSFn2
(3/4) test vimp2_E_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 1  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 1  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from WMn   /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 256 samples
Epoch 1/300
 - 82s - loss: 0.0957 - acc: 0.9896 - mDice: 0.8140 - val_loss: 0.0999 - val_acc: 0.9919 - val_mDice: 0.4486

Epoch 00001: val_mDice improved from -inf to 0.44859, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 77s - loss: 0.0640 - acc: 0.9931 - mDice: 0.8755 - val_loss: 0.0716 - val_acc: 0.9908 - val_mDice: 0.4716

Epoch 00002: val_mDice improved from 0.44859 to 0.47159, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 77s - loss: 0.0555 - acc: 0.9939 - mDice: 0.8920 - val_loss: 0.3076 - val_acc: 0.9800 - val_mDice: 0.3962

Epoch 00003: val_mDice did not improve from 0.47159
Epoch 4/300
 - 77s - loss: 0.0520 - acc: 0.9943 - mDice: 0.8989 - val_loss: 0.1942 - val_acc: 0.9893 - val_mDice: 0.4613

Epoch 00004: val_mDice did not improve from 0.47159
Epoch 5/300
 - 79s - loss: 0.0482 - acc: 0.9946 - mDice: 0.9064 - val_loss: 0.1078 - val_acc: 0.9934 - val_mDice: 0.5066

Epoch 00005: val_mDice improved from 0.47159 to 0.50661, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300
 - 79s - loss: 0.0458 - acc: 0.9948 - mDice: 0.9109 - val_loss: 0.0353 - val_acc: 0.9936 - val_mDice: 0.4954

Epoch 00006: val_mDice did not improve from 0.50661
Epoch 7/300
 - 80s - loss: 0.0459 - acc: 0.9949 - mDice: 0.9108 - val_loss: 0.0431 - val_acc: 0.9940 - val_mDice: 0.4795

Epoch 00007: val_mDice did not improve from 0.50661
Epoch 8/300
 - 79s - loss: 0.0427 - acc: 0.9952 - mDice: 0.9170 - val_loss: 0.0366 - val_acc: 0.9942 - val_mDice: 0.4925

Epoch 00008: val_mDice did not improve from 0.50661
Epoch 9/300
 - 80s - loss: 0.0419 - acc: 0.9952 - mDice: 0.9186 - val_loss: 0.0717 - val_acc: 0.9936 - val_mDice: 0.5009

Epoch 00009: val_mDice did not improve from 0.50661
Epoch 10/300
 - 80s - loss: 0.0398 - acc: 0.9954 - mDice: 0.9226 - val_loss: -2.1922e-02 - val_acc: 0.9933 - val_mDice: 0.4701

Epoch 00010: val_mDice did not improve from 0.50661
Epoch 11/300
 - 79s - loss: 0.0401 - acc: 0.9954 - mDice: 0.9220 - val_loss: 0.1019 - val_acc: 0.9915 - val_mDice: 0.4887

Epoch 00011: val_mDice did not improve from 0.50661
Epoch 12/300
 - 79s - loss: 0.0383 - acc: 0.9956 - mDice: 0.9256 - val_loss: 0.0764 - val_acc: 0.9940 - val_mDice: 0.5037

Epoch 00012: val_mDice did not improve from 0.50661
Epoch 13/300
 - 79s - loss: 0.0374 - acc: 0.9956 - mDice: 0.9274 - val_loss: 0.0270 - val_acc: 0.9939 - val_mDice: 0.5121

Epoch 00013: val_mDice improved from 0.50661 to 0.51206, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Thalamus_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 14/300
 - 79s - loss: 0.0363 - acc: 0.9957 - mDice: 0.9296 - val_loss: 0.0711 - val_acc: 0.9940 - val_mDice: 0.5017

Epoch 00014: val_mDice did not improve from 0.51206
Epoch 15/300
 - 80s - loss: 0.0368 - acc: 0.9956 - mDice: 0.9286 - val_loss: 0.0389 - val_acc: 0.9938 - val_mDice: 0.4940

Epoch 00015: val_mDice did not improve from 0.51206
Epoch 16/300
 - 79s - loss: 0.0359 - acc: 0.9957 - mDice: 0.9304 - val_loss: 0.1132 - val_acc: 0.9939 - val_mDice: 0.4960

Epoch 00016: val_mDice did not improve from 0.51206
Epoch 17/300
 - 79s - loss: 0.0346 - acc: 0.9958 - mDice: 0.9329 - val_loss: -9.0228e-04 - val_acc: 0.9942 - val_mDice: 0.4899

Epoch 00017: val_mDice did not improve from 0.51206
Epoch 18/300
 - 79s - loss: 0.0346 - acc: 0.9959 - mDice: 0.9327 - val_loss: 0.1643 - val_acc: 0.9933 - val_mDice: 0.5059

Epoch 00018: val_mDice did not improve from 0.51206
Epoch 19/300
 - 80s - loss: 0.0343 - acc: 0.9959 - mDice: 0.9334 - val_loss: 0.0037 - val_acc: 0.9937 - val_mDice: 0.4803

Epoch 00019: val_mDice did not improve from 0.51206
Epoch 20/300
 - 79s - loss: 0.0333 - acc: 0.9959 - mDice: 0.9355 - val_loss: 0.2233 - val_acc: 0.9919 - val_mDice: 0.4802

Epoch 00020: val_mDice did not improve from 0.51206
Epoch 21/300
 - 79s - loss: 0.0335 - acc: 0.9960 - mDice: 0.9349 - val_loss: 0.0491 - val_acc: 0.9925 - val_mDice: 0.4704

Epoch 00021: val_mDice did not improve from 0.51206
Epoch 22/300
 - 79s - loss: 0.0322 - acc: 0.9960 - mDice: 0.9374 - val_loss: -4.4497e-02 - val_acc: 0.9941 - val_mDice: 0.4986

Epoch 00022: val_mDice did not improve from 0.51206
Epoch 23/300
 - 80s - loss: 0.0328 - acc: 0.9960 - mDice: 0.9363 - val_loss: -6.6090e-04 - val_acc: 0.9942 - val_mDice: 0.4889

Epoch 00023: val_mDice did not improve from 0.51206
Epoch 24/300
 - 80s - loss: 0.0333 - acc: 0.9960 - mDice: 0.9354 - val_loss: 0.0363 - val_acc: 0.9940 - val_mDice: 0.4932

Epoch 00024: val_mDice did not improve from 0.51206
Epoch 25/300
 - 79s - loss: 0.0327 - acc: 0.9961 - mDice: 0.9365 - val_loss: -1.0214e-02 - val_acc: 0.9939 - val_mDice: 0.5080

Epoch 00025: val_mDice did not improve from 0.51206
Epoch 26/300
 - 80s - loss: 0.0322 - acc: 0.9960 - mDice: 0.9375 - val_loss: 0.0043 - val_acc: 0.9939 - val_mDice: 0.4791

Epoch 00026: val_mDice did not improve from 0.51206
Epoch 27/300
 - 80s - loss: 0.0324 - acc: 0.9961 - mDice: 0.9371 - val_loss: -5.2688e-03 - val_acc: 0.9943 - val_mDice: 0.4985

Epoch 00027: val_mDice did not improve from 0.51206
Epoch 28/300
 - 80s - loss: 0.0311 - acc: 0.9961 - mDice: 0.9396 - val_loss: -7.6872e-03 - val_acc: 0.9941 - val_mDice: 0.5032

Epoch 00028: val_mDice did not improve from 0.51206

Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 29/300
 - 81s - loss: 0.0297 - acc: 0.9963 - mDice: 0.9423 - val_loss: -2.5447e-03 - val_acc: 0.9941 - val_mDice: 0.4947

Epoch 00029: val_mDice did not improve from 0.51206
Epoch 30/300
 - 80s - loss: 0.0288 - acc: 0.9963 - mDice: 0.9441 - val_loss: 0.0723 - val_acc: 0.9936 - val_mDice: 0.4997

Epoch 00030: val_mDice did not improve from 0.51206
Epoch 31/300
 - 81s - loss: 0.0293 - acc: 0.9963 - mDice: 0.9432 - val_loss: 0.0335 - val_acc: 0.9937 - val_mDice: 0.4991

Epoch 00031: val_mDice did not improve from 0.51206
Epoch 32/300
 - 80s - loss: 0.0289 - acc: 0.9963 - mDice: 0.9440 - val_loss: -4.7593e-03 - val_acc: 0.9943 - val_mDice: 0.4972

Epoch 00032: val_mDice did not improve from 0.51206
Epoch 33/300
 - 80s - loss: 0.0285 - acc: 0.9964 - mDice: 0.9448 - val_loss: 0.0324 - val_acc: 0.9935 - val_mDice: 0.5021

Epoch 00033: val_mDice did not improve from 0.51206
Epoch 34/300
 - 79s - loss: 0.0293 - acc: 0.9964 - mDice: 0.9431 - val_loss: -3.2453e-03 - val_acc: 0.9942 - val_mDice: 0.4942

Epoch 00034: val_mDice did not improve from 0.51206
Epoch 35/300
 - 80s - loss: 0.0282 - acc: 0.9964 - mDice: 0.9454 - val_loss: -4.0119e-02 - val_acc: 0.9942 - val_mDice: 0.4897

Epoch 00035: val_mDice did not improve from 0.51206
Epoch 36/300
 - 80s - loss: 0.0282 - acc: 0.9964 - mDice: 0.9453 - val_loss: 0.0384 - val_acc: 0.9926 - val_mDice: 0.4901

Epoch 00036: val_mDice did not improve from 0.51206
Epoch 37/300
 - 79s - loss: 0.0278 - acc: 0.9964 - mDice: 0.9461 - val_loss: -4.7261e-03 - val_acc: 0.9939 - val_mDice: 0.4985
