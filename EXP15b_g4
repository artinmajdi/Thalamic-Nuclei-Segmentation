*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
(0/456) train vimp2_845_05312013_VZ
(1/456) train vimp2_823_05202013_AJ
(2/456) train vimp2_915_07112013_LC
(3/456) train vimp2_901_07052013_AS
(4/456) train vimp2_ctrl_911_07082013_TTO
(5/456) train vimp2_ctrl_925_07152013_LS
(6/456) train vimp2_869_06142013_BL
(7/456) train vimp2_ANON724_03272013
(8/456) train vimp2_819_05172013_DS
(9/456) train vimp2_ctrl_918_07112013_TQ
(10/456) train vimp2_ctrl_902_07052013_SI
(11/456) train vimp2_ANON606_20130110
(12/456) train vimp2_943_07242013_PA
(13/456) train vimp2_824_05212013_JS
(14/456) train vimp2_ANON624_20130117
(15/456) train vimp2_ctrl_920_07122013_SW
(16/456) train vimp2_884_06272013_TS
(17/456) train vimp2_668_02282013_CD
(18/456) train vimp2_964_08092013_TG
(19/456) train vimp2_ctrl_921_07122013_MP
(20/456) train vimp2_972_08152013_DC
(21/456) train vimp2_988_08302013_CB
(22/456) train vimp2_ANON702_03152013
(23/456) train vimp2_ANON714_03222013
(24/456) train vimp2_972_08152013_DC_Aug0_Rot_-1_sd0
(25/456) train vimp2_972_08152013_DC_Aug0_Rot_1_sd2
(26/456) train vimp2_972_08152013_DC_Aug0_Rot_-7_sd1
(27/456) train vimp2_972_08152013_DC_Aug1_Rot_2_sd0
(28/456) train vimp2_972_08152013_DC_Aug1_Rot_5_sd1
(29/456) train vimp2_972_08152013_DC_Aug1_Rot_6_sd2
(30/456) train vimp2_972_08152013_DC_Aug2_Rot_-3_sd1
(31/456) train vimp2_972_08152013_DC_Aug2_Rot_-3_sd2
(32/456) train vimp2_972_08152013_DC_Aug2_Rot_-5_sd0
(33/456) train vimp2_972_08152013_DC_Aug3_Rot_2_sd1
(34/456) train vimp2_972_08152013_DC_Aug3_Rot_-3_sd0
(35/456) train vimp2_972_08152013_DC_Aug3_Rot_-6_sd2
(36/456) train vimp2_972_08152013_DC_Aug4_Rot_-2_sd2
(37/456) train vimp2_972_08152013_DC_Aug4_Rot_3_sd0
(38/456) train vimp2_972_08152013_DC_Aug4_Rot_6_sd1
(39/456) train vimp2_972_08152013_DC_Aug5_Rot_-1_sd2
(40/456) train vimp2_972_08152013_DC_Aug5_Rot_7_sd0
(41/456) train vimp2_972_08152013_DC_Aug5_Rot_7_sd1
(42/456) train vimp2_988_08302013_CB_Aug0_Rot_-3_sd2
(43/456) train vimp2_988_08302013_CB_Aug0_Rot_4_sd0
(44/456) train vimp2_988_08302013_CB_Aug0_Rot_-5_sd1
(45/456) train vimp2_988_08302013_CB_Aug1_Rot_-2_sd2
(46/456) train vimp2_988_08302013_CB_Aug1_Rot_-3_sd0
(47/456) train vimp2_988_08302013_CB_Aug1_Rot_-6_sd1
(48/456) train vimp2_988_08302013_CB_Aug2_Rot_-2_sd0
(49/456) train vimp2_988_08302013_CB_Aug2_Rot_4_sd1
(50/456) train vimp2_988_08302013_CB_Aug2_Rot_-5_sd2
(51/456) train vimp2_988_08302013_CB_Aug3_Rot_2_sd2
(52/456) train vimp2_988_08302013_CB_Aug3_Rot_-3_sd0
(53/456) train vimp2_988_08302013_CB_Aug3_Rot_3_sd1
(54/456) train vimp2_988_08302013_CB_Aug4_Rot_-2_sd0
(55/456) train vimp2_988_08302013_CB_Aug4_Rot_-6_sd1
(56/456) train vimp2_988_08302013_CB_Aug4_Rot_7_sd2
(57/456) train vimp2_988_08302013_CB_Aug5_Rot_-1_sd0
(58/456) train vimp2_988_08302013_CB_Aug5_Rot_-6_sd1
(59/456) train vimp2_988_08302013_CB_Aug5_Rot_7_sd2
(60/456) train vimp2_ANON702_03152013_Aug0_Rot_-2_sd1
(61/456) train vimp2_ANON702_03152013_Aug0_Rot_-3_sd2
(62/456) train vimp2_ANON702_03152013_Aug0_Rot_-4_sd0
(63/456) train vimp2_ANON702_03152013_Aug1_Rot_-4_sd1
(64/456) train vimp2_ANON702_03152013_Aug1_Rot_-5_sd2
(65/456) train vimp2_ANON702_03152013_Aug1_Rot_-7_sd0
(66/456) train vimp2_ANON702_03152013_Aug2_Rot_6_sd0
(67/456) train vimp2_ANON702_03152013_Aug2_Rot_6_sd2
(68/456) train vimp2_ANON702_03152013_Aug2_Rot_-7_sd1
(69/456) train vimp2_ANON702_03152013_Aug3_Rot_-1_sd2
(70/456) train vimp2_ANON702_03152013_Aug3_Rot_-3_sd0
(71/456) train vimp2_ANON702_03152013_Aug3_Rot_-6_sd1
(72/456) train vimp2_ANON702_03152013_Aug4_Rot_-2_sd0
(73/456) train vimp2_ANON702_03152013_Aug4_Rot_-3_sd2
(74/456) train vimp2_ANON702_03152013_Aug4_Rot_-7_sd1
(75/456) train vimp2_ANON702_03152013_Aug5_Rot_3_sd0
(76/456) train vimp2_ANON702_03152013_Aug5_Rot_3_sd2
(77/456) train vimp2_ANON702_03152013_Aug5_Rot_4_sd1
(78/456) train vimp2_ANON714_03222013_Aug0_Rot_-1_sd2
(79/456) train vimp2_ANON714_03222013_Aug0_Rot_-2_sd1
(80/456) train vimp2_ANON714_03222013_Aug0_Rot_4_sd0
(81/456) train vimp2_ANON714_03222013_Aug1_Rot_0_sd0
(82/456) train vimp2_ANON714_03222013_Aug1_Rot_1_sd1
(83/456) train vimp2_ANON714_03222013_Aug1_Rot_-6_sd2
(84/456) train vimp2_ANON714_03222013_Aug2_Rot_-2_sd0
(85/456) train vimp2_ANON714_03222013_Aug2_Rot_4_sd1
(86/456) train vimp2_ANON714_03222013_Aug2_Rot_6_sd2
(87/456) train vimp2_ANON714_03222013_Aug3_Rot_2_sd0
(88/456) train vimp2_ANON714_03222013_Aug3_Rot_-3_sd2
(89/456) train vimp2_ANON714_03222013_Aug3_Rot_-7_sd1
(90/456) train vimp2_ANON714_03222013_Aug4_Rot_1_sd0
(91/456) train vimp2_ANON714_03222013_Aug4_Rot_-2_sd1
(92/456) train vimp2_ANON714_03222013_Aug4_Rot_4_sd2
(93/456) train vimp2_ANON714_03222013_Aug5_Rot_1_sd0
(94/456) train vimp2_ANON714_03222013_Aug5_Rot_6_sd1
(95/456) train vimp2_ANON714_03222013_Aug5_Rot_-7_sd2
(96/456) train vimp2_668_02282013_CD_Aug0_Rot_7_sd0
(97/456) train vimp2_668_02282013_CD_Aug1_Rot_-1_sd0
(98/456) train vimp2_668_02282013_CD_Aug2_Rot_-4_sd0
(99/456) train vimp2_668_02282013_CD_Aug3_Rot_3_sd0
(100/456) train vimp2_668_02282013_CD_Aug4_Rot_-5_sd0
(101/456) train vimp2_668_02282013_CD_Aug5_Rot_-6_sd0
(102/456) train vimp2_819_05172013_DS_Aug0_Rot_1_sd0
(103/456) train vimp2_819_05172013_DS_Aug1_Rot_5_sd0
(104/456) train vimp2_819_05172013_DS_Aug2_Rot_-4_sd0
(105/456) train vimp2_819_05172013_DS_Aug3_Rot_2_sd0
(106/456) train vimp2_819_05172013_DS_Aug4_Rot_5_sd0
(107/456) train vimp2_819_05172013_DS_Aug5_Rot_4_sd0
(108/456) train vimp2_823_05202013_AJ_Aug0_Rot_-7_sd0
(109/456) train vimp2_823_05202013_AJ_Aug1_Rot_7_sd0
(110/456) train vimp2_823_05202013_AJ_Aug2_Rot_3_sd0
(111/456) train vimp2_823_05202013_AJ_Aug3_Rot_-5_sd0
(112/456) train vimp2_823_05202013_AJ_Aug4_Rot_-3_sd0
(113/456) train vimp2_823_05202013_AJ_Aug5_Rot_1_sd0
(114/456) train vimp2_824_05212013_JS_Aug0_Rot_-2_sd0
(115/456) train vimp2_824_05212013_JS_Aug1_Rot_6_sd0
(116/456) train vimp2_824_05212013_JS_Aug2_Rot_1_sd0
(117/456) train vimp2_824_05212013_JS_Aug3_Rot_-4_sd0
(118/456) train vimp2_824_05212013_JS_Aug4_Rot_7_sd0
(119/456) train vimp2_824_05212013_JS_Aug5_Rot_2_sd0
(120/456) train vimp2_845_05312013_VZ_Aug0_Rot_1_sd0
(121/456) train vimp2_845_05312013_VZ_Aug1_Rot_5_sd0
(122/456) train vimp2_845_05312013_VZ_Aug2_Rot_-6_sd0
(123/456) train vimp2_845_05312013_VZ_Aug3_Rot_3_sd0
(124/456) train vimp2_845_05312013_VZ_Aug4_Rot_-2_sd0
(125/456) train vimp2_845_05312013_VZ_Aug5_Rot_5_sd0
(126/456) train vimp2_869_06142013_BL_Aug0_Rot_-2_sd0
(127/456) train vimp2_869_06142013_BL_Aug1_Rot_-4_sd0
(128/456) train vimp2_869_06142013_BL_Aug2_Rot_-1_sd0
(129/456) train vimp2_869_06142013_BL_Aug3_Rot_3_sd0
(130/456) train vimp2_869_06142013_BL_Aug4_Rot_3_sd0
(131/456) train vimp2_869_06142013_BL_Aug5_Rot_1_sd0
(132/456) train vimp2_884_06272013_TS_Aug0_Rot_-7_sd0
(133/456) train vimp2_884_06272013_TS_Aug1_Rot_7_sd0
(134/456) train vimp2_884_06272013_TS_Aug2_Rot_-5_sd0
(135/456) train vimp2_884_06272013_TS_Aug3_Rot_-2_sd0
(136/456) train vimp2_884_06272013_TS_Aug4_Rot_6_sd0
(137/456) train vimp2_884_06272013_TS_Aug5_Rot_-1_sd0
(138/456) train vimp2_901_07052013_AS_Aug0_Rot_-4_sd0
(139/456) train vimp2_901_07052013_AS_Aug1_Rot_-2_sd0
(140/456) train vimp2_901_07052013_AS_Aug2_Rot_1_sd0
(141/456) train vimp2_901_07052013_AS_Aug3_Rot_2_sd0
(142/456) train vimp2_901_07052013_AS_Aug4_Rot_-7_sd0
(143/456) train vimp2_901_07052013_AS_Aug5_Rot_-5_sd0
(144/456) train vimp2_915_07112013_LC_Aug0_Rot_-2_sd0
(145/456) train vimp2_915_07112013_LC_Aug1_Rot_4_sd0
(146/456) train vimp2_915_07112013_LC_Aug2_Rot_-2_sd0
(147/456) train vimp2_915_07112013_LC_Aug3_Rot_-1_sd0
(148/456) train vimp2_915_07112013_LC_Aug4_Rot_1_sd0
(149/456) train vimp2_915_07112013_LC_Aug5_Rot_4_sd0
(150/456) train vimp2_943_07242013_PA_Aug0_Rot_-5_sd0
(151/456) train vimp2_943_07242013_PA_Aug1_Rot_-7_sd0
(152/456) train vimp2_943_07242013_PA_Aug2_Rot_-4_sd0
(153/456) train vimp2_943_07242013_PA_Aug3_Rot_6_sd0
(154/456) train vimp2_943_07242013_PA_Aug4_Rot_6_sd0
(155/456) train vimp2_943_07242013_PA_Aug5_Rot_5_sd0
(156/456) train vimp2_964_08092013_TG_Aug0_Rot_5_sd0
(157/456) train vimp2_964_08092013_TG_Aug1_Rot_-4_sd0
(158/456) train vimp2_964_08092013_TG_Aug2_Rot_7_sd0
(159/456) train vimp2_964_08092013_TG_Aug3_Rot_-1_sd0
(160/456) train vimp2_964_08092013_TG_Aug4_Rot_-1_sd0
(161/456) train vimp2_964_08092013_TG_Aug5_Rot_2_sd0
(162/456) train vimp2_ANON606_20130110_Aug0_Rot_1_sd0
(163/456) train vimp2_ANON606_20130110_Aug1_Rot_2_sd0
(164/456) train vimp2_ANON606_20130110_Aug2_Rot_-5_sd0
(165/456) train vimp2_ANON606_20130110_Aug3_Rot_-5_sd0
(166/456) train vimp2_ANON606_20130110_Aug4_Rot_3_sd0
(167/456) train vimp2_ANON606_20130110_Aug5_Rot_-1_sd0
(168/456) train vimp2_ANON624_20130117_Aug0_Rot_1_sd0
(169/456) train vimp2_ANON624_20130117_Aug1_Rot_-2_sd0
(170/456) train vimp2_ANON624_20130117_Aug2_Rot_5_sd0
(171/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd0
(172/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd0
(173/456) train vimp2_ANON624_20130117_Aug5_Rot_3_sd0
(174/456) train vimp2_ANON724_03272013_Aug0_Rot_-4_sd0
(175/456) train vimp2_ANON724_03272013_Aug1_Rot_-3_sd0
(176/456) train vimp2_ANON724_03272013_Aug2_Rot_-4_sd0
(177/456) train vimp2_ANON724_03272013_Aug3_Rot_5_sd0
(178/456) train vimp2_ANON724_03272013_Aug4_Rot_5_sd0
(179/456) train vimp2_ANON724_03272013_Aug5_Rot_-6_sd0
(180/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_5_sd0
(181/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_7_sd0
(182/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_4_sd0
(183/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_-4_sd0
(184/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_3_sd0
(185/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_-7_sd0
(186/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_6_sd0
(187/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_1_sd0
(188/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_4_sd0
(189/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_-2_sd0
(190/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_6_sd0
(191/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_-2_sd0
(192/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_5_sd0
(193/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_-3_sd0
(194/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_-4_sd0
(195/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_1_sd0
(196/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_3_sd0
(197/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_7_sd0
(198/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_7_sd0
(199/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_-6_sd0
(200/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_-4_sd0
(201/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_1_sd0
(202/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-2_sd0
(203/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_4_sd0
(204/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_-1_sd0
(205/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_-3_sd0
(206/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_6_sd0
(207/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_3_sd0
(208/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_1_sd0
(209/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_-5_sd0
(210/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_-5_sd0
(211/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_-2_sd0
(212/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_5_sd0
(213/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_3_sd0
(214/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_4_sd0
(215/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-7_sd0
(216/456) train vimp2_668_02282013_CD_Aug0_Rot_-5_sd1
(217/456) train vimp2_668_02282013_CD_Aug1_Rot_-6_sd1
(218/456) train vimp2_668_02282013_CD_Aug2_Rot_3_sd1
(219/456) train vimp2_668_02282013_CD_Aug3_Rot_4_sd1
(220/456) train vimp2_668_02282013_CD_Aug4_Rot_-6_sd1
(221/456) train vimp2_668_02282013_CD_Aug5_Rot_-2_sd1
(222/456) train vimp2_819_05172013_DS_Aug0_Rot_-1_sd1
(223/456) train vimp2_819_05172013_DS_Aug1_Rot_3_sd1
(224/456) train vimp2_819_05172013_DS_Aug2_Rot_6_sd1
(225/456) train vimp2_819_05172013_DS_Aug3_Rot_3_sd1
(226/456) train vimp2_819_05172013_DS_Aug4_Rot_-7_sd1
(227/456) train vimp2_819_05172013_DS_Aug5_Rot_-5_sd1
(228/456) train vimp2_823_05202013_AJ_Aug0_Rot_-7_sd1
(229/456) train vimp2_823_05202013_AJ_Aug1_Rot_3_sd1
(230/456) train vimp2_823_05202013_AJ_Aug2_Rot_6_sd1
(231/456) train vimp2_823_05202013_AJ_Aug3_Rot_-7_sd1
(232/456) train vimp2_823_05202013_AJ_Aug4_Rot_-4_sd1
(233/456) train vimp2_823_05202013_AJ_Aug5_Rot_-4_sd1
(234/456) train vimp2_824_05212013_JS_Aug0_Rot_7_sd1
(235/456) train vimp2_824_05212013_JS_Aug1_Rot_3_sd1
(236/456) train vimp2_824_05212013_JS_Aug2_Rot_3_sd1
(237/456) train vimp2_824_05212013_JS_Aug3_Rot_-4_sd1
(238/456) train vimp2_824_05212013_JS_Aug4_Rot_4_sd1
(239/456) train vimp2_824_05212013_JS_Aug5_Rot_6_sd1
(240/456) train vimp2_845_05312013_VZ_Aug0_Rot_5_sd1
(241/456) train vimp2_845_05312013_VZ_Aug1_Rot_6_sd1
(242/456) train vimp2_845_05312013_VZ_Aug2_Rot_1_sd1
(243/456) train vimp2_845_05312013_VZ_Aug3_Rot_-6_sd1
(244/456) train vimp2_845_05312013_VZ_Aug4_Rot_-2_sd1
(245/456) train vimp2_845_05312013_VZ_Aug5_Rot_-6_sd1
(246/456) train vimp2_869_06142013_BL_Aug0_Rot_-4_sd1
(247/456) train vimp2_869_06142013_BL_Aug1_Rot_-3_sd1
(248/456) train vimp2_869_06142013_BL_Aug2_Rot_-4_sd1
(249/456) train vimp2_869_06142013_BL_Aug3_Rot_1_sd1
(250/456) train vimp2_869_06142013_BL_Aug4_Rot_7_sd1
(251/456) train vimp2_869_06142013_BL_Aug5_Rot_5_sd1
(252/456) train vimp2_884_06272013_TS_Aug0_Rot_5_sd1
(253/456) train vimp2_884_06272013_TS_Aug1_Rot_-1_sd1
(254/456) train vimp2_884_06272013_TS_Aug2_Rot_-1_sd1
(255/456) train vimp2_884_06272013_TS_Aug3_Rot_3_sd1
(256/456) train vimp2_884_06272013_TS_Aug4_Rot_4_sd1
(257/456) train vimp2_884_06272013_TS_Aug5_Rot_3_sd1
(258/456) train vimp2_901_07052013_AS_Aug0_Rot_3_sd1
(259/456) train vimp2_901_07052013_AS_Aug1_Rot_5_sd1
(260/456) train vimp2_901_07052013_AS_Aug2_Rot_-5_sd1
(261/456) train vimp2_901_07052013_AS_Aug3_Rot_7_sd1
(262/456) train vimp2_901_07052013_AS_Aug4_Rot_7_sd1
(263/456) train vimp2_901_07052013_AS_Aug5_Rot_6_sd1
(264/456) train vimp2_915_07112013_LC_Aug0_Rot_-6_sd1
(265/456) train vimp2_915_07112013_LC_Aug1_Rot_-5_sd1
(266/456) train vimp2_915_07112013_LC_Aug2_Rot_7_sd1
(267/456) train vimp2_915_07112013_LC_Aug3_Rot_1_sd1
(268/456) train vimp2_915_07112013_LC_Aug4_Rot_6_sd1
(269/456) train vimp2_915_07112013_LC_Aug5_Rot_-7_sd1
(270/456) train vimp2_943_07242013_PA_Aug0_Rot_1_sd1
(271/456) train vimp2_943_07242013_PA_Aug1_Rot_6_sd1
(272/456) train vimp2_943_07242013_PA_Aug2_Rot_3_sd1
(273/456) train vimp2_943_07242013_PA_Aug3_Rot_6_sd1
(274/456) train vimp2_943_07242013_PA_Aug4_Rot_3_sd1
(275/456) train vimp2_943_07242013_PA_Aug5_Rot_3_sd1
(276/456) train vimp2_964_08092013_TG_Aug0_Rot_6_sd1
(277/456) train vimp2_964_08092013_TG_Aug1_Rot_-4_sd1
(278/456) train vimp2_964_08092013_TG_Aug2_Rot_5_sd1
(279/456) train vimp2_964_08092013_TG_Aug3_Rot_0_sd1
(280/456) train vimp2_964_08092013_TG_Aug4_Rot_3_sd1
(281/456) train vimp2_964_08092013_TG_Aug5_Rot_-1_sd1
(282/456) train vimp2_ANON606_20130110_Aug0_Rot_-6_sd1
(283/456) train vimp2_ANON606_20130110_Aug1_Rot_7_sd1
(284/456) train vimp2_ANON606_20130110_Aug2_Rot_1_sd1
(285/456) train vimp2_ANON606_20130110_Aug3_Rot_1_sd1
(286/456) train vimp2_ANON606_20130110_Aug4_Rot_4_sd1
(287/456) train vimp2_ANON606_20130110_Aug5_Rot_4_sd1
(288/456) train vimp2_ANON624_20130117_Aug0_Rot_7_sd1
(289/456) train vimp2_ANON624_20130117_Aug1_Rot_6_sd1
(290/456) train vimp2_ANON624_20130117_Aug2_Rot_-1_sd1
(291/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd1
(292/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd1
(293/456) train vimp2_ANON624_20130117_Aug5_Rot_4_sd1
(294/456) train vimp2_ANON724_03272013_Aug0_Rot_-2_sd1
(295/456) train vimp2_ANON724_03272013_Aug1_Rot_4_sd1
(296/456) train vimp2_ANON724_03272013_Aug2_Rot_-1_sd1
(297/456) train vimp2_ANON724_03272013_Aug3_Rot_-6_sd1
(298/456) train vimp2_ANON724_03272013_Aug4_Rot_-5_sd1
(299/456) train vimp2_ANON724_03272013_Aug5_Rot_-4_sd1
(300/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_-2_sd1
(301/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_7_sd1
(302/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_2_sd1
(303/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_-4_sd1
(304/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_-1_sd1
(305/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_-4_sd1
(306/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_7_sd1
(307/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_-1_sd1
(308/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_-5_sd1
(309/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_-3_sd1
(310/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_6_sd1
(311/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_6_sd1
(312/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_1_sd1
(313/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_-2_sd1
(314/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_3_sd1
(315/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_-1_sd1
(316/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_7_sd1
(317/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_6_sd1
(318/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_-3_sd1
(319/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_7_sd1
(320/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_-4_sd1
(321/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_-1_sd1
(322/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-5_sd1
(323/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_-5_sd1
(324/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_4_sd1
(325/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_3_sd1
(326/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_-1_sd1
(327/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_-2_sd1
(328/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_2_sd1
(329/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_4_sd1
(330/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_-2_sd1
(331/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_4_sd1
(332/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_-2_sd1
(333/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_2_sd1
(334/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_3_sd1
(335/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-4_sd1
(336/456) train vimp2_668_02282013_CD_Aug0_Rot_-3_sd2
(337/456) train vimp2_668_02282013_CD_Aug1_Rot_6_sd2
(338/456) train vimp2_668_02282013_CD_Aug2_Rot_-6_sd2
(339/456) train vimp2_668_02282013_CD_Aug3_Rot_7_sd2
(340/456) train vimp2_668_02282013_CD_Aug4_Rot_1_sd2
(341/456) train vimp2_668_02282013_CD_Aug5_Rot_7_sd2
(342/456) train vimp2_819_05172013_DS_Aug0_Rot_3_sd2
(343/456) train vimp2_819_05172013_DS_Aug1_Rot_-1_sd2
(344/456) train vimp2_819_05172013_DS_Aug2_Rot_-2_sd2
(345/456) train vimp2_819_05172013_DS_Aug3_Rot_-1_sd2
(346/456) train vimp2_819_05172013_DS_Aug4_Rot_7_sd2
(347/456) train vimp2_819_05172013_DS_Aug5_Rot_3_sd2
(348/456) train vimp2_823_05202013_AJ_Aug0_Rot_-6_sd2
(349/456) train vimp2_823_05202013_AJ_Aug1_Rot_-7_sd2
(350/456) train vimp2_823_05202013_AJ_Aug2_Rot_7_sd2
(351/456) train vimp2_823_05202013_AJ_Aug3_Rot_-2_sd2
(352/456) train vimp2_823_05202013_AJ_Aug4_Rot_1_sd2
(353/456) train vimp2_823_05202013_AJ_Aug5_Rot_1_sd2
(354/456) train vimp2_824_05212013_JS_Aug0_Rot_-5_sd2
(355/456) train vimp2_824_05212013_JS_Aug1_Rot_3_sd2
(356/456) train vimp2_824_05212013_JS_Aug2_Rot_5_sd2
(357/456) train vimp2_824_05212013_JS_Aug3_Rot_2_sd2
(358/456) train vimp2_824_05212013_JS_Aug4_Rot_5_sd2
(359/456) train vimp2_824_05212013_JS_Aug5_Rot_-7_sd2
(360/456) train vimp2_845_05312013_VZ_Aug0_Rot_-5_sd2
(361/456) train vimp2_845_05312013_VZ_Aug1_Rot_3_sd2
(362/456) train vimp2_845_05312013_VZ_Aug2_Rot_-3_sd2
(363/456) train vimp2_845_05312013_VZ_Aug3_Rot_-1_sd2
(364/456) train vimp2_845_05312013_VZ_Aug4_Rot_4_sd2
(365/456) train vimp2_845_05312013_VZ_Aug5_Rot_-2_sd2
(366/456) train vimp2_869_06142013_BL_Aug0_Rot_3_sd2
(367/456) train vimp2_869_06142013_BL_Aug1_Rot_7_sd2
(368/456) train vimp2_869_06142013_BL_Aug2_Rot_4_sd2
(369/456) train vimp2_869_06142013_BL_Aug3_Rot_1_sd2
(370/456) train vimp2_869_06142013_BL_Aug4_Rot_1_sd2
(371/456) train vimp2_869_06142013_BL_Aug5_Rot_1_sd2
(372/456) train vimp2_884_06272013_TS_Aug0_Rot_3_sd2
(373/456) train vimp2_884_06272013_TS_Aug1_Rot_-5_sd2
(374/456) train vimp2_884_06272013_TS_Aug2_Rot_-7_sd2
(375/456) train vimp2_884_06272013_TS_Aug3_Rot_-6_sd2
(376/456) train vimp2_884_06272013_TS_Aug4_Rot_-2_sd2
(377/456) train vimp2_884_06272013_TS_Aug5_Rot_-6_sd2
(378/456) train vimp2_901_07052013_AS_Aug0_Rot_2_sd2
(379/456) train vimp2_901_07052013_AS_Aug1_Rot_-3_sd2
(380/456) train vimp2_901_07052013_AS_Aug2_Rot_-6_sd2
(381/456) train vimp2_901_07052013_AS_Aug3_Rot_6_sd2
(382/456) train vimp2_901_07052013_AS_Aug4_Rot_-5_sd2
(383/456) train vimp2_901_07052013_AS_Aug5_Rot_1_sd2
(384/456) train vimp2_915_07112013_LC_Aug0_Rot_1_sd2
(385/456) train vimp2_915_07112013_LC_Aug1_Rot_7_sd2
(386/456) train vimp2_915_07112013_LC_Aug2_Rot_3_sd2
(387/456) train vimp2_915_07112013_LC_Aug3_Rot_6_sd2
(388/456) train vimp2_915_07112013_LC_Aug4_Rot_4_sd2
(389/456) train vimp2_915_07112013_LC_Aug5_Rot_4_sd2
(390/456) train vimp2_943_07242013_PA_Aug0_Rot_3_sd2
(391/456) train vimp2_943_07242013_PA_Aug1_Rot_-4_sd2
(392/456) train vimp2_943_07242013_PA_Aug2_Rot_3_sd2
(393/456) train vimp2_943_07242013_PA_Aug3_Rot_2_sd2
(394/456) train vimp2_943_07242013_PA_Aug4_Rot_-1_sd2
(395/456) train vimp2_943_07242013_PA_Aug5_Rot_-3_sd2
(396/456) train vimp2_964_08092013_TG_Aug0_Rot_2_sd2
(397/456) train vimp2_964_08092013_TG_Aug1_Rot_-6_sd2
(398/456) train vimp2_964_08092013_TG_Aug2_Rot_5_sd2
(399/456) train vimp2_964_08092013_TG_Aug3_Rot_-7_sd2
(400/456) train vimp2_964_08092013_TG_Aug4_Rot_-6_sd2
(401/456) train vimp2_964_08092013_TG_Aug5_Rot_5_sd2
(402/456) train vimp2_ANON606_20130110_Aug0_Rot_-6_sd2
(403/456) train vimp2_ANON606_20130110_Aug1_Rot_-5_sd2
(404/456) train vimp2_ANON606_20130110_Aug2_Rot_-3_sd2
(405/456) train vimp2_ANON606_20130110_Aug3_Rot_3_sd2
(406/456) train vimp2_ANON606_20130110_Aug4_Rot_-2_sd2
(407/456) train vimp2_ANON606_20130110_Aug5_Rot_-3_sd2
(408/456) train vimp2_ANON624_20130117_Aug0_Rot_2_sd2
(409/456) train vimp2_ANON624_20130117_Aug1_Rot_-2_sd2
(410/456) train vimp2_ANON624_20130117_Aug2_Rot_-6_sd2
(411/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd2
(412/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd2
(413/456) train vimp2_ANON624_20130117_Aug5_Rot_-7_sd2
(414/456) train vimp2_ANON724_03272013_Aug0_Rot_-5_sd2
(415/456) train vimp2_ANON724_03272013_Aug1_Rot_6_sd2
(416/456) train vimp2_ANON724_03272013_Aug2_Rot_-3_sd2
(417/456) train vimp2_ANON724_03272013_Aug3_Rot_-6_sd2
(418/456) train vimp2_ANON724_03272013_Aug4_Rot_-1_sd2
(419/456) train vimp2_ANON724_03272013_Aug5_Rot_-7_sd2
(420/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_-4_sd2
(421/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_-3_sd2
(422/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_-5_sd2
(423/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_5_sd2
(424/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_-2_sd2
(425/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_5_sd2
(426/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_-3_sd2
(427/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_7_sd2
(428/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_3_sd2
(429/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_5_sd2
(430/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_7_sd2
(431/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_2_sd2
(432/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_-7_sd2
(433/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_5_sd2
(434/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_5_sd2
(435/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_4_sd2
(436/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_5_sd2
(437/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_-4_sd2
(438/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_7_sd2
(439/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_-2_sd2
(440/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_1_sd2
(441/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_-7_sd2
(442/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-3_sd2
(443/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_7_sd2
(444/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_1_sd2
(445/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_-7_sd2
(446/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_6_sd2
(447/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_-2_sd2
(448/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_-5_sd2
(449/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_6_sd2
(450/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_2_sd2
(451/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_5_sd2
(452/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_2_sd2
(453/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_5_sd2
(454/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_-4_sd2
(455/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-3_sd22019-07-06 16:59:47.519386: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-06 16:59:51.047893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:84:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-07-06 16:59:51.047978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 16:59:51.457781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 16:59:51.457852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 16:59:51.457866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 16:59:51.458368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a’: File exists
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<07:01,  1.48s/it]Loading train:   1%|          | 2/285 [00:03<07:27,  1.58s/it]Loading train:   1%|          | 3/285 [00:04<07:07,  1.52s/it]Loading train:   1%|▏         | 4/285 [00:07<08:17,  1.77s/it]Loading train:   2%|▏         | 5/285 [00:08<08:09,  1.75s/it]Loading train:   2%|▏         | 6/285 [00:10<08:20,  1.79s/it]Loading train:   2%|▏         | 7/285 [00:12<08:52,  1.91s/it]Loading train:   3%|▎         | 8/285 [00:15<09:25,  2.04s/it]Loading train:   3%|▎         | 9/285 [00:17<09:32,  2.07s/it]Loading train:   4%|▎         | 10/285 [00:19<09:30,  2.08s/it]Loading train:   4%|▍         | 11/285 [00:21<09:40,  2.12s/it]Loading train:   4%|▍         | 12/285 [00:23<08:59,  1.97s/it]Loading train:   5%|▍         | 13/285 [00:24<08:31,  1.88s/it]Loading train:   5%|▍         | 14/285 [00:26<08:13,  1.82s/it]Loading train:   5%|▌         | 15/285 [00:28<08:20,  1.85s/it]Loading train:   6%|▌         | 16/285 [00:30<08:12,  1.83s/it]Loading train:   6%|▌         | 17/285 [00:31<07:35,  1.70s/it]Loading train:   6%|▋         | 18/285 [00:33<07:31,  1.69s/it]Loading train:   7%|▋         | 19/285 [00:35<07:53,  1.78s/it]Loading train:   7%|▋         | 20/285 [00:37<08:30,  1.93s/it]Loading train:   7%|▋         | 21/285 [00:39<08:24,  1.91s/it]Loading train:   8%|▊         | 22/285 [00:41<08:12,  1.87s/it]Loading train:   8%|▊         | 23/285 [00:42<07:42,  1.76s/it]Loading train:   8%|▊         | 24/285 [00:44<07:01,  1.62s/it]Loading train:   9%|▉         | 25/285 [00:45<07:01,  1.62s/it]Loading train:   9%|▉         | 26/285 [00:47<06:41,  1.55s/it]Loading train:   9%|▉         | 27/285 [00:48<06:44,  1.57s/it]Loading train:  10%|▉         | 28/285 [00:50<06:46,  1.58s/it]Loading train:  10%|█         | 29/285 [00:51<06:21,  1.49s/it]Loading train:  11%|█         | 30/285 [00:52<06:09,  1.45s/it]Loading train:  11%|█         | 31/285 [00:54<05:41,  1.34s/it]Loading train:  11%|█         | 32/285 [00:55<05:53,  1.40s/it]Loading train:  12%|█▏        | 33/285 [00:57<06:10,  1.47s/it]Loading train:  12%|█▏        | 34/285 [00:58<06:05,  1.46s/it]Loading train:  12%|█▏        | 35/285 [01:00<06:30,  1.56s/it]Loading train:  13%|█▎        | 36/285 [01:01<06:18,  1.52s/it]Loading train:  13%|█▎        | 37/285 [01:03<06:20,  1.53s/it]Loading train:  13%|█▎        | 38/285 [01:05<06:26,  1.56s/it]Loading train:  14%|█▎        | 39/285 [01:06<06:15,  1.53s/it]Loading train:  14%|█▍        | 40/285 [01:08<06:30,  1.59s/it]Loading train:  14%|█▍        | 41/285 [01:09<06:13,  1.53s/it]Loading train:  15%|█▍        | 42/285 [01:10<05:52,  1.45s/it]Loading train:  15%|█▌        | 43/285 [01:12<05:38,  1.40s/it]Loading train:  15%|█▌        | 44/285 [01:14<06:11,  1.54s/it]Loading train:  16%|█▌        | 45/285 [01:16<06:46,  1.69s/it]Loading train:  16%|█▌        | 46/285 [01:17<06:54,  1.74s/it]Loading train:  16%|█▋        | 47/285 [01:19<06:31,  1.65s/it]Loading train:  17%|█▋        | 48/285 [01:20<05:50,  1.48s/it]Loading train:  17%|█▋        | 49/285 [01:21<05:29,  1.39s/it]Loading train:  18%|█▊        | 50/285 [01:22<05:07,  1.31s/it]Loading train:  18%|█▊        | 51/285 [01:23<04:56,  1.27s/it]Loading train:  18%|█▊        | 52/285 [01:25<04:43,  1.22s/it]Loading train:  19%|█▊        | 53/285 [01:26<05:02,  1.30s/it]Loading train:  19%|█▉        | 54/285 [01:27<05:05,  1.32s/it]Loading train:  19%|█▉        | 55/285 [01:29<05:13,  1.36s/it]Loading train:  20%|█▉        | 56/285 [01:30<05:03,  1.33s/it]Loading train:  20%|██        | 57/285 [01:32<05:23,  1.42s/it]Loading train:  20%|██        | 58/285 [01:33<05:36,  1.48s/it]Loading train:  21%|██        | 59/285 [01:35<05:47,  1.54s/it]Loading train:  21%|██        | 60/285 [01:36<05:36,  1.50s/it]Loading train:  21%|██▏       | 61/285 [01:38<05:15,  1.41s/it]Loading train:  22%|██▏       | 62/285 [01:39<05:10,  1.39s/it]Loading train:  22%|██▏       | 63/285 [01:40<05:03,  1.37s/it]Loading train:  22%|██▏       | 64/285 [01:42<05:39,  1.53s/it]Loading train:  23%|██▎       | 65/285 [01:44<06:10,  1.68s/it]Loading train:  23%|██▎       | 66/285 [01:46<06:20,  1.74s/it]Loading train:  24%|██▎       | 67/285 [01:48<06:01,  1.66s/it]Loading train:  24%|██▍       | 68/285 [01:49<05:29,  1.52s/it]Loading train:  24%|██▍       | 69/285 [01:50<04:52,  1.36s/it]Loading train:  25%|██▍       | 70/285 [01:51<04:38,  1.30s/it]Loading train:  25%|██▍       | 71/285 [01:52<04:40,  1.31s/it]Loading train:  25%|██▌       | 72/285 [01:54<04:45,  1.34s/it]Loading train:  26%|██▌       | 73/285 [01:55<04:39,  1.32s/it]Loading train:  26%|██▌       | 74/285 [01:57<05:09,  1.47s/it]Loading train:  26%|██▋       | 75/285 [01:58<05:03,  1.44s/it]Loading train:  27%|██▋       | 76/285 [01:59<04:55,  1.41s/it]Loading train:  27%|██▋       | 77/285 [02:01<05:26,  1.57s/it]Loading train:  27%|██▋       | 78/285 [02:03<05:31,  1.60s/it]Loading train:  28%|██▊       | 79/285 [02:04<05:01,  1.46s/it]Loading train:  28%|██▊       | 80/285 [02:05<04:40,  1.37s/it]Loading train:  28%|██▊       | 81/285 [02:07<04:54,  1.44s/it]Loading train:  29%|██▉       | 82/285 [02:09<04:59,  1.47s/it]Loading train:  29%|██▉       | 83/285 [02:10<04:58,  1.48s/it]Loading train:  29%|██▉       | 84/285 [02:11<04:44,  1.42s/it]Loading train:  30%|██▉       | 85/285 [02:13<04:38,  1.39s/it]Loading train:  30%|███       | 86/285 [02:14<04:57,  1.50s/it]Loading train:  31%|███       | 87/285 [02:16<04:57,  1.50s/it]Loading train:  31%|███       | 88/285 [02:18<05:23,  1.64s/it]Loading train:  31%|███       | 89/285 [02:20<05:28,  1.68s/it]Loading train:  32%|███▏      | 90/285 [02:22<05:53,  1.82s/it]Loading train:  32%|███▏      | 91/285 [02:24<06:06,  1.89s/it]Loading train:  32%|███▏      | 92/285 [02:26<06:23,  1.99s/it]Loading train:  33%|███▎      | 93/285 [02:28<06:18,  1.97s/it]Loading train:  33%|███▎      | 94/285 [02:30<06:36,  2.08s/it]Loading train:  33%|███▎      | 95/285 [02:32<05:59,  1.89s/it]Loading train:  34%|███▎      | 96/285 [02:33<05:33,  1.76s/it]Loading train:  34%|███▍      | 97/285 [02:35<05:05,  1.63s/it]Loading train:  34%|███▍      | 98/285 [02:36<05:14,  1.68s/it]Loading train:  35%|███▍      | 99/285 [02:38<05:01,  1.62s/it]Loading train:  35%|███▌      | 100/285 [02:40<05:07,  1.66s/it]Loading train:  35%|███▌      | 101/285 [02:42<05:27,  1.78s/it]Loading train:  36%|███▌      | 102/285 [02:43<05:27,  1.79s/it]Loading train:  36%|███▌      | 103/285 [02:45<05:35,  1.84s/it]Loading train:  36%|███▋      | 104/285 [02:47<05:16,  1.75s/it]Loading train:  37%|███▋      | 105/285 [02:48<04:41,  1.56s/it]Loading train:  37%|███▋      | 106/285 [02:50<05:01,  1.68s/it]Loading train:  38%|███▊      | 107/285 [02:52<05:27,  1.84s/it]Loading train:  38%|███▊      | 108/285 [02:55<05:54,  2.00s/it]Loading train:  38%|███▊      | 109/285 [02:57<06:11,  2.11s/it]Loading train:  39%|███▊      | 110/285 [02:59<06:20,  2.17s/it]Loading train:  39%|███▉      | 111/285 [03:01<06:05,  2.10s/it]Loading train:  39%|███▉      | 112/285 [03:03<05:35,  1.94s/it]Loading train:  40%|███▉      | 113/285 [03:05<05:23,  1.88s/it]Loading train:  40%|████      | 114/285 [03:07<05:31,  1.94s/it]Loading train:  40%|████      | 115/285 [03:09<05:42,  2.01s/it]Loading train:  41%|████      | 116/285 [03:11<05:40,  2.02s/it]Loading train:  41%|████      | 117/285 [03:13<05:53,  2.10s/it]Loading train:  41%|████▏     | 118/285 [03:15<05:19,  1.91s/it]Loading train:  42%|████▏     | 119/285 [03:17<05:32,  2.01s/it]Loading train:  42%|████▏     | 120/285 [03:18<05:09,  1.88s/it]Loading train:  42%|████▏     | 121/285 [03:20<05:16,  1.93s/it]Loading train:  43%|████▎     | 122/285 [03:22<05:17,  1.95s/it]Loading train:  43%|████▎     | 123/285 [03:24<05:13,  1.94s/it]Loading train:  44%|████▎     | 124/285 [03:26<05:12,  1.94s/it]Loading train:  44%|████▍     | 125/285 [03:28<04:55,  1.85s/it]Loading train:  44%|████▍     | 126/285 [03:30<04:56,  1.86s/it]Loading train:  45%|████▍     | 127/285 [03:32<05:05,  1.93s/it]Loading train:  45%|████▍     | 128/285 [03:34<04:50,  1.85s/it]Loading train:  45%|████▌     | 129/285 [03:35<04:13,  1.63s/it]Loading train:  46%|████▌     | 130/285 [03:36<04:05,  1.58s/it]Loading train:  46%|████▌     | 131/285 [03:38<04:03,  1.58s/it]Loading train:  46%|████▋     | 132/285 [03:40<04:21,  1.71s/it]Loading train:  47%|████▋     | 133/285 [03:42<04:32,  1.79s/it]Loading train:  47%|████▋     | 134/285 [03:43<04:24,  1.75s/it]Loading train:  47%|████▋     | 135/285 [03:45<04:01,  1.61s/it]Loading train:  48%|████▊     | 136/285 [03:46<03:58,  1.60s/it]Loading train:  48%|████▊     | 137/285 [03:48<03:55,  1.59s/it]Loading train:  48%|████▊     | 138/285 [03:50<04:19,  1.76s/it]Loading train:  49%|████▉     | 139/285 [03:52<04:10,  1.72s/it]Loading train:  49%|████▉     | 140/285 [03:53<04:13,  1.75s/it]Loading train:  49%|████▉     | 141/285 [03:55<04:09,  1.73s/it]Loading train:  50%|████▉     | 142/285 [03:57<04:13,  1.78s/it]Loading train:  50%|█████     | 143/285 [03:58<03:45,  1.59s/it]Loading train:  51%|█████     | 144/285 [04:00<03:52,  1.65s/it]Loading train:  51%|█████     | 145/285 [04:02<03:51,  1.65s/it]Loading train:  51%|█████     | 146/285 [04:04<04:00,  1.73s/it]Loading train:  52%|█████▏    | 147/285 [04:05<04:08,  1.80s/it]Loading train:  52%|█████▏    | 148/285 [04:07<03:55,  1.72s/it]Loading train:  52%|█████▏    | 149/285 [04:08<03:35,  1.59s/it]Loading train:  53%|█████▎    | 150/285 [04:10<03:25,  1.52s/it]Loading train:  53%|█████▎    | 151/285 [04:11<03:29,  1.57s/it]Loading train:  53%|█████▎    | 152/285 [04:13<03:26,  1.56s/it]Loading train:  54%|█████▎    | 153/285 [04:15<03:31,  1.60s/it]Loading train:  54%|█████▍    | 154/285 [04:16<03:40,  1.69s/it]Loading train:  54%|█████▍    | 155/285 [04:18<03:52,  1.79s/it]Loading train:  55%|█████▍    | 156/285 [04:20<03:27,  1.61s/it]Loading train:  55%|█████▌    | 157/285 [04:21<03:05,  1.45s/it]Loading train:  55%|█████▌    | 158/285 [04:22<02:57,  1.40s/it]Loading train:  56%|█████▌    | 159/285 [04:24<03:08,  1.50s/it]Loading train:  56%|█████▌    | 160/285 [04:26<03:19,  1.59s/it]Loading train:  56%|█████▋    | 161/285 [04:27<03:02,  1.47s/it]Loading train:  57%|█████▋    | 162/285 [04:28<03:02,  1.48s/it]Loading train:  57%|█████▋    | 163/285 [04:29<02:39,  1.31s/it]Loading train:  58%|█████▊    | 164/285 [04:30<02:32,  1.26s/it]Loading train:  58%|█████▊    | 165/285 [04:31<02:15,  1.13s/it]Loading train:  58%|█████▊    | 166/285 [04:32<02:03,  1.03s/it]Loading train:  59%|█████▊    | 167/285 [04:33<01:53,  1.04it/s]Loading train:  59%|█████▉    | 168/285 [04:34<01:49,  1.07it/s]Loading train:  59%|█████▉    | 169/285 [04:34<01:43,  1.12it/s]Loading train:  60%|█████▉    | 170/285 [04:35<01:42,  1.12it/s]Loading train:  60%|██████    | 171/285 [04:36<01:47,  1.06it/s]Loading train:  60%|██████    | 172/285 [04:37<01:50,  1.02it/s]Loading train:  61%|██████    | 173/285 [04:38<01:45,  1.06it/s]Loading train:  61%|██████    | 174/285 [04:39<01:41,  1.09it/s]Loading train:  61%|██████▏   | 175/285 [04:40<01:43,  1.06it/s]Loading train:  62%|██████▏   | 176/285 [04:41<01:41,  1.07it/s]Loading train:  62%|██████▏   | 177/285 [04:42<01:42,  1.05it/s]Loading train:  62%|██████▏   | 178/285 [04:43<01:46,  1.00it/s]Loading train:  63%|██████▎   | 179/285 [04:44<01:52,  1.06s/it]Loading train:  63%|██████▎   | 180/285 [04:46<01:54,  1.09s/it]Loading train:  64%|██████▎   | 181/285 [04:46<01:48,  1.05s/it]Loading train:  64%|██████▍   | 182/285 [04:47<01:47,  1.04s/it]Loading train:  64%|██████▍   | 183/285 [04:49<01:51,  1.10s/it]Loading train:  65%|██████▍   | 184/285 [04:50<01:49,  1.08s/it]Loading train:  65%|██████▍   | 185/285 [04:51<01:44,  1.05s/it]Loading train:  65%|██████▌   | 186/285 [04:52<01:45,  1.07s/it]Loading train:  66%|██████▌   | 187/285 [04:53<01:44,  1.06s/it]Loading train:  66%|██████▌   | 188/285 [04:54<01:42,  1.06s/it]Loading train:  66%|██████▋   | 189/285 [04:55<01:34,  1.02it/s]Loading train:  67%|██████▋   | 190/285 [04:56<01:32,  1.03it/s]Loading train:  67%|██████▋   | 191/285 [04:57<01:37,  1.04s/it]Loading train:  67%|██████▋   | 192/285 [04:58<01:32,  1.01it/s]Loading train:  68%|██████▊   | 193/285 [04:59<01:27,  1.05it/s]Loading train:  68%|██████▊   | 194/285 [04:59<01:24,  1.07it/s]Loading train:  68%|██████▊   | 195/285 [05:00<01:24,  1.07it/s]Loading train:  69%|██████▉   | 196/285 [05:02<01:31,  1.03s/it]Loading train:  69%|██████▉   | 197/285 [05:03<01:26,  1.02it/s]Loading train:  69%|██████▉   | 198/285 [05:04<01:26,  1.01it/s]Loading train:  70%|██████▉   | 199/285 [05:05<01:26,  1.00s/it]Loading train:  70%|███████   | 200/285 [05:05<01:18,  1.08it/s]Loading train:  71%|███████   | 201/285 [05:06<01:14,  1.12it/s]Loading train:  71%|███████   | 202/285 [05:07<01:19,  1.04it/s]Loading train:  71%|███████   | 203/285 [05:08<01:21,  1.01it/s]Loading train:  72%|███████▏  | 204/285 [05:09<01:21,  1.01s/it]Loading train:  72%|███████▏  | 205/285 [05:10<01:19,  1.00it/s]Loading train:  72%|███████▏  | 206/285 [05:11<01:20,  1.02s/it]Loading train:  73%|███████▎  | 207/285 [05:13<01:21,  1.04s/it]Loading train:  73%|███████▎  | 208/285 [05:14<01:19,  1.04s/it]Loading train:  73%|███████▎  | 209/285 [05:15<01:17,  1.02s/it]Loading train:  74%|███████▎  | 210/285 [05:15<01:14,  1.01it/s]Loading train:  74%|███████▍  | 211/285 [05:17<01:15,  1.03s/it]Loading train:  74%|███████▍  | 212/285 [05:18<01:21,  1.12s/it]Loading train:  75%|███████▍  | 213/285 [05:19<01:23,  1.16s/it]Loading train:  75%|███████▌  | 214/285 [05:20<01:24,  1.18s/it]Loading train:  75%|███████▌  | 215/285 [05:21<01:18,  1.13s/it]Loading train:  76%|███████▌  | 216/285 [05:22<01:13,  1.07s/it]Loading train:  76%|███████▌  | 217/285 [05:23<01:09,  1.02s/it]Loading train:  76%|███████▋  | 218/285 [05:24<01:08,  1.02s/it]Loading train:  77%|███████▋  | 219/285 [05:25<01:10,  1.06s/it]Loading train:  77%|███████▋  | 220/285 [05:27<01:09,  1.08s/it]Loading train:  78%|███████▊  | 221/285 [05:28<01:08,  1.07s/it]Loading train:  78%|███████▊  | 222/285 [05:28<01:04,  1.02s/it]Loading train:  78%|███████▊  | 223/285 [05:29<01:00,  1.02it/s]Loading train:  79%|███████▊  | 224/285 [05:30<01:00,  1.01it/s]Loading train:  79%|███████▉  | 225/285 [05:31<00:58,  1.02it/s]Loading train:  79%|███████▉  | 226/285 [05:32<01:00,  1.02s/it]Loading train:  80%|███████▉  | 227/285 [05:34<00:59,  1.03s/it]Loading train:  80%|████████  | 228/285 [05:35<00:58,  1.03s/it]Loading train:  80%|████████  | 229/285 [05:36<00:58,  1.04s/it]Loading train:  81%|████████  | 230/285 [05:37<00:57,  1.05s/it]Loading train:  81%|████████  | 231/285 [05:38<00:59,  1.11s/it]Loading train:  81%|████████▏ | 232/285 [05:39<01:03,  1.20s/it]Loading train:  82%|████████▏ | 233/285 [05:40<01:01,  1.19s/it]Loading train:  82%|████████▏ | 234/285 [05:42<01:06,  1.30s/it]Loading train:  82%|████████▏ | 235/285 [05:43<01:05,  1.30s/it]Loading train:  83%|████████▎ | 236/285 [05:45<01:04,  1.32s/it]Loading train:  83%|████████▎ | 237/285 [05:46<01:04,  1.35s/it]Loading train:  84%|████████▎ | 238/285 [05:47<01:01,  1.32s/it]Loading train:  84%|████████▍ | 239/285 [05:49<00:59,  1.29s/it]Loading train:  84%|████████▍ | 240/285 [05:50<00:55,  1.24s/it]Loading train:  85%|████████▍ | 241/285 [05:51<00:56,  1.29s/it]Loading train:  85%|████████▍ | 242/285 [05:52<00:52,  1.23s/it]Loading train:  85%|████████▌ | 243/285 [05:53<00:50,  1.21s/it]Loading train:  86%|████████▌ | 244/285 [05:55<00:51,  1.25s/it]Loading train:  86%|████████▌ | 245/285 [05:56<00:48,  1.22s/it]Loading train:  86%|████████▋ | 246/285 [05:57<00:48,  1.25s/it]Loading train:  87%|████████▋ | 247/285 [05:59<00:48,  1.27s/it]Loading train:  87%|████████▋ | 248/285 [06:00<00:47,  1.28s/it]Loading train:  87%|████████▋ | 249/285 [06:02<00:50,  1.40s/it]Loading train:  88%|████████▊ | 250/285 [06:02<00:44,  1.28s/it]Loading train:  88%|████████▊ | 251/285 [06:04<00:41,  1.22s/it]Loading train:  88%|████████▊ | 252/285 [06:05<00:40,  1.22s/it]Loading train:  89%|████████▉ | 253/285 [06:06<00:37,  1.16s/it]Loading train:  89%|████████▉ | 254/285 [06:07<00:34,  1.11s/it]Loading train:  89%|████████▉ | 255/285 [06:08<00:31,  1.05s/it]Loading train:  90%|████████▉ | 256/285 [06:09<00:29,  1.00s/it]Loading train:  90%|█████████ | 257/285 [06:10<00:27,  1.03it/s]Loading train:  91%|█████████ | 258/285 [06:11<00:26,  1.01it/s]Loading train:  91%|█████████ | 259/285 [06:11<00:24,  1.05it/s]Loading train:  91%|█████████ | 260/285 [06:13<00:25,  1.00s/it]Loading train:  92%|█████████▏| 261/285 [06:13<00:23,  1.04it/s]Loading train:  92%|█████████▏| 262/285 [06:14<00:22,  1.04it/s]Loading train:  92%|█████████▏| 263/285 [06:15<00:21,  1.01it/s]Loading train:  93%|█████████▎| 264/285 [06:16<00:21,  1.01s/it]Loading train:  93%|█████████▎| 265/285 [06:17<00:18,  1.06it/s]Loading train:  93%|█████████▎| 266/285 [06:18<00:17,  1.09it/s]Loading train:  94%|█████████▎| 267/285 [06:19<00:16,  1.08it/s]Loading train:  94%|█████████▍| 268/285 [06:20<00:16,  1.00it/s]Loading train:  94%|█████████▍| 269/285 [06:21<00:16,  1.01s/it]Loading train:  95%|█████████▍| 270/285 [06:22<00:15,  1.05s/it]Loading train:  95%|█████████▌| 271/285 [06:24<00:15,  1.11s/it]Loading train:  95%|█████████▌| 272/285 [06:25<00:14,  1.15s/it]Loading train:  96%|█████████▌| 273/285 [06:26<00:13,  1.16s/it]Loading train:  96%|█████████▌| 274/285 [06:27<00:12,  1.13s/it]Loading train:  96%|█████████▋| 275/285 [06:28<00:11,  1.11s/it]Loading train:  97%|█████████▋| 276/285 [06:29<00:10,  1.12s/it]Loading train:  97%|█████████▋| 277/285 [06:30<00:08,  1.07s/it]Loading train:  98%|█████████▊| 278/285 [06:31<00:07,  1.10s/it]Loading train:  98%|█████████▊| 279/285 [06:32<00:06,  1.07s/it]Loading train:  98%|█████████▊| 280/285 [06:34<00:05,  1.07s/it]Loading train:  99%|█████████▊| 281/285 [06:35<00:04,  1.13s/it]Loading train:  99%|█████████▉| 282/285 [06:36<00:03,  1.20s/it]Loading train:  99%|█████████▉| 283/285 [06:37<00:02,  1.18s/it]Loading train: 100%|█████████▉| 284/285 [06:39<00:01,  1.20s/it]Loading train: 100%|██████████| 285/285 [06:40<00:00,  1.18s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/285 [00:00<00:06, 45.07it/s]concatenating: train:   3%|▎         | 9/285 [00:00<00:07, 39.32it/s]concatenating: train:  10%|▉         | 28/285 [00:00<00:04, 51.56it/s]concatenating: train:  16%|█▋        | 47/285 [00:00<00:03, 65.74it/s]concatenating: train:  20%|██        | 58/285 [00:00<00:03, 66.62it/s]concatenating: train:  24%|██▍       | 69/285 [00:00<00:02, 75.54it/s]concatenating: train:  28%|██▊       | 81/285 [00:00<00:02, 83.30it/s]concatenating: train:  33%|███▎      | 93/285 [00:00<00:02, 90.39it/s]concatenating: train:  42%|████▏     | 121/285 [00:01<00:01, 113.26it/s]concatenating: train:  49%|████▉     | 141/285 [00:01<00:01, 129.83it/s]concatenating: train:  55%|█████▌    | 158/285 [00:01<00:01, 122.46it/s]concatenating: train:  61%|██████    | 174/285 [00:01<00:00, 125.54it/s]concatenating: train:  66%|██████▋   | 189/285 [00:01<00:00, 127.96it/s]concatenating: train:  72%|███████▏  | 204/285 [00:01<00:00, 128.34it/s]concatenating: train:  79%|███████▉  | 225/285 [00:01<00:00, 143.04it/s]concatenating: train:  85%|████████▍ | 241/285 [00:01<00:00, 137.09it/s]concatenating: train:  90%|████████▉ | 256/285 [00:01<00:00, 134.53it/s]concatenating: train:  95%|█████████▌| 271/285 [00:02<00:00, 100.84it/s]concatenating: train:  99%|█████████▉| 283/285 [00:02<00:00, 96.64it/s] concatenating: train: 100%|██████████| 285/285 [00:02<00:00, 120.00it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.64s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.54s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 67.35it/s]
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:10,  3.86it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:08,  4.62it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.35it/s]loading the weights for Unet:  20%|██        | 8/40 [00:02<00:08,  3.92it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:02<00:14,  2.11it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:03<00:12,  2.35it/s]loading the weights for Unet:  30%|███       | 12/40 [00:03<00:10,  2.70it/s]loading the weights for Unet:  40%|████      | 16/40 [00:04<00:06,  3.60it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:04<00:06,  3.59it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:04<00:04,  4.31it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:04<00:04,  4.16it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:05<00:03,  5.00it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:06<00:04,  3.08it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:07<00:07,  1.92it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:08<00:05,  2.17it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:08<00:04,  2.46it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:08<00:02,  3.23it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:08<00:01,  3.93it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:09<00:01,  3.90it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:09<00:00,  4.66it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:09<00:00,  4.36it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:09<00:00,  4.16it/s]
(0/4) test vimp2_ctrl_991_08302013_JF
(1/4) test vimp2_967_08132013_KW
(2/4) test vimp2_765_04162013_AW
(3/4) test vimp2_ANON695_03132013
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 80, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 80, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 80, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 80, 10)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 80, 10)   910         dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 80, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 80, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 80, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 80, 10)   910         dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 80, 10)   40          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 80, 10)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 80, 10)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 80, 20)   1820        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 80, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 80, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 80, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 80, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 80, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 40, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 40, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 40, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 40, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 40, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 40, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 40, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 20, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 20, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 20, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 20, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 20, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 20, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 20, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 20, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 40, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 40, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 40, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 40, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 40, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 40, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 10)   1810        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 10)   40          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 10)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 10)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 13)   143         dropout_9[0][0]                  
==================================================================================================
Total params: 189,493
Trainable params: 45,933
Non-trainable params: 143,560
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 24s - loss: 209.6170 - acc: 0.4094 - mDice: 0.0184 - val_loss: 152.6250 - val_acc: 0.8714 - val_mDice: 0.0168

Epoch 00001: val_mDice improved from -inf to 0.01682, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 10s - loss: 127.9470 - acc: 0.7341 - mDice: 0.0179 - val_loss: 96.5018 - val_acc: 0.9041 - val_mDice: 0.0172

Epoch 00002: val_mDice improved from 0.01682 to 0.01720, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 10s - loss: 76.1967 - acc: 0.8366 - mDice: 0.0177 - val_loss: 49.6091 - val_acc: 0.9047 - val_mDice: 0.0148

Epoch 00003: val_mDice did not improve from 0.01720
Epoch 4/300
 - 11s - loss: 47.4045 - acc: 0.8657 - mDice: 0.0176 - val_loss: 19.1519 - val_acc: 0.9047 - val_mDice: 0.0130

Epoch 00004: val_mDice did not improve from 0.01720
Epoch 5/300
 - 11s - loss: 31.7768 - acc: 0.8682 - mDice: 0.0179 - val_loss: 12.9675 - val_acc: 0.9047 - val_mDice: 0.0125

Epoch 00005: val_mDice did not improve from 0.01720
Epoch 6/300
 - 10s - loss: 23.0755 - acc: 0.8688 - mDice: 0.0182 - val_loss: 10.6634 - val_acc: 0.9047 - val_mDice: 0.0141

Epoch 00006: val_mDice did not improve from 0.01720
Epoch 7/300
 - 10s - loss: 18.1506 - acc: 0.8692 - mDice: 0.0188 - val_loss: 9.6083 - val_acc: 0.9047 - val_mDice: 0.0144

Epoch 00007: val_mDice did not improve from 0.01720
Epoch 8/300
 - 10s - loss: 14.8935 - acc: 0.8692 - mDice: 0.0199 - val_loss: 8.2508 - val_acc: 0.9047 - val_mDice: 0.0150

Epoch 00008: val_mDice did not improve from 0.01720
Epoch 9/300
 - 10s - loss: 12.7464 - acc: 0.8692 - mDice: 0.0213 - val_loss: 7.5634 - val_acc: 0.9047 - val_mDice: 0.0114

Epoch 00009: val_mDice did not improve from 0.01720
Epoch 10/300
 - 10s - loss: 11.2914 - acc: 0.8692 - mDice: 0.0231 - val_loss: 7.0876 - val_acc: 0.9047 - val_mDice: 0.0126

Epoch 00010: val_mDice did not improve from 0.01720
Epoch 11/300
 - 10s - loss: 10.2422 - acc: 0.8692 - mDice: 0.0251 - val_loss: 6.7001 - val_acc: 0.9047 - val_mDice: 0.0130

Epoch 00011: val_mDice did not improve from 0.01720
Epoch 12/300
 - 10s - loss: 9.4232 - acc: 0.8692 - mDice: 0.0276 - val_loss: 6.2017 - val_acc: 0.9047 - val_mDice: 0.0174

Epoch 00012: val_mDice improved from 0.01720 to 0.01744, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 10s - loss: 8.7908 - acc: 0.8692 - mDice: 0.0306 - val_loss: 5.8688 - val_acc: 0.9047 - val_mDice: 0.0229

Epoch 00013: val_mDice improved from 0.01744 to 0.02293, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 11s - loss: 8.2952 - acc: 0.8692 - mDice: 0.0337 - val_loss: 5.5530 - val_acc: 0.9047 - val_mDice: 0.0310

Epoch 00014: val_mDice improved from 0.02293 to 0.03102, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 10s - loss: 7.8807 - acc: 0.8692 - mDice: 0.0370 - val_loss: 5.7398 - val_acc: 0.9047 - val_mDice: 0.0293

Epoch 00015: val_mDice did not improve from 0.03102
Epoch 16/300
 - 11s - loss: 7.4086 - acc: 0.8692 - mDice: 0.0407 - val_loss: 5.5074 - val_acc: 0.9047 - val_mDice: 0.0354

Epoch 00016: val_mDice improved from 0.03102 to 0.03542, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 11s - loss: 7.0528 - acc: 0.8692 - mDice: 0.0444 - val_loss: 5.6463 - val_acc: 0.9047 - val_mDice: 0.0368

Epoch 00017: val_mDice improved from 0.03542 to 0.03681, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 11s - loss: 6.7777 - acc: 0.8692 - mDice: 0.0486 - val_loss: 5.7078 - val_acc: 0.9047 - val_mDice: 0.0369

Epoch 00018: val_mDice improved from 0.03681 to 0.03689, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 12s - loss: 6.5567 - acc: 0.8692 - mDice: 0.0522 - val_loss: 5.0485 - val_acc: 0.9047 - val_mDice: 0.0491

Epoch 00019: val_mDice improved from 0.03689 to 0.04908, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 11s - loss: 6.3581 - acc: 0.8692 - mDice: 0.0559 - val_loss: 5.0018 - val_acc: 0.9047 - val_mDice: 0.0566

Epoch 00020: val_mDice improved from 0.04908 to 0.05659, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 11s - loss: 6.1905 - acc: 0.8692 - mDice: 0.0593 - val_loss: 5.2094 - val_acc: 0.9047 - val_mDice: 0.0558

Epoch 00021: val_mDice did not improve from 0.05659
Epoch 22/300
 - 11s - loss: 6.0323 - acc: 0.8692 - mDice: 0.0629 - val_loss: 4.7761 - val_acc: 0.9047 - val_mDice: 0.0694

Epoch 00022: val_mDice improved from 0.05659 to 0.06941, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 11s - loss: 5.8912 - acc: 0.8692 - mDice: 0.0666 - val_loss: 4.6836 - val_acc: 0.9047 - val_mDice: 0.0774

Epoch 00023: val_mDice improved from 0.06941 to 0.07744, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 11s - loss: 5.7741 - acc: 0.8692 - mDice: 0.0699 - val_loss: 5.6061 - val_acc: 0.9047 - val_mDice: 0.0561

Epoch 00024: val_mDice did not improve from 0.07744
Epoch 25/300
 - 10s - loss: 5.6385 - acc: 0.8692 - mDice: 0.0741 - val_loss: 4.6815 - val_acc: 0.9047 - val_mDice: 0.0812

Epoch 00025: val_mDice improved from 0.07744 to 0.08118, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 10s - loss: 5.5210 - acc: 0.8692 - mDice: 0.0781 - val_loss: 4.7376 - val_acc: 0.9047 - val_mDice: 0.0856

Epoch 00026: val_mDice improved from 0.08118 to 0.08562, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 10s - loss: 5.4218 - acc: 0.8693 - mDice: 0.0823 - val_loss: 4.8504 - val_acc: 0.9047 - val_mDice: 0.0901

Epoch 00027: val_mDice improved from 0.08562 to 0.09010, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 10s - loss: 5.3022 - acc: 0.8694 - mDice: 0.0877 - val_loss: 3.9969 - val_acc: 0.9047 - val_mDice: 0.1141

Epoch 00028: val_mDice improved from 0.09010 to 0.11412, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 10s - loss: 5.2022 - acc: 0.8697 - mDice: 0.0937 - val_loss: 4.9837 - val_acc: 0.9048 - val_mDice: 0.0966

Epoch 00029: val_mDice did not improve from 0.11412
Epoch 30/300
 - 10s - loss: 5.1019 - acc: 0.8700 - mDice: 0.0997 - val_loss: 4.0405 - val_acc: 0.9049 - val_mDice: 0.1333

Epoch 00030: val_mDice improved from 0.11412 to 0.13332, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 10s - loss: 5.0065 - acc: 0.8703 - mDice: 0.1051 - val_loss: 4.1722 - val_acc: 0.9049 - val_mDice: 0.1322

Epoch 00031: val_mDice did not improve from 0.13332
Epoch 32/300
 - 10s - loss: 4.9112 - acc: 0.8705 - mDice: 0.1101 - val_loss: 4.1501 - val_acc: 0.9054 - val_mDice: 0.1368

Epoch 00032: val_mDice improved from 0.13332 to 0.13680, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 11s - loss: 4.8333 - acc: 0.8709 - mDice: 0.1150 - val_loss: 3.8301 - val_acc: 0.9052 - val_mDice: 0.1539

Epoch 00033: val_mDice improved from 0.13680 to 0.15386, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 10s - loss: 4.7534 - acc: 0.8714 - mDice: 0.1201 - val_loss: 3.8676 - val_acc: 0.9057 - val_mDice: 0.1575

Epoch 00034: val_mDice improved from 0.15386 to 0.15755, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 10s - loss: 4.6768 - acc: 0.8717 - mDice: 0.1253 - val_loss: 5.2277 - val_acc: 0.9053 - val_mDice: 0.1134

Epoch 00035: val_mDice did not improve from 0.15755
Epoch 36/300
 - 10s - loss: 4.6074 - acc: 0.8720 - mDice: 0.1310 - val_loss: 4.1002 - val_acc: 0.9058 - val_mDice: 0.1577

Epoch 00036: val_mDice improved from 0.15755 to 0.15771, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 10s - loss: 4.5326 - acc: 0.8726 - mDice: 0.1372 - val_loss: 3.8523 - val_acc: 0.9065 - val_mDice: 0.1721

Epoch 00037: val_mDice improved from 0.15771 to 0.17211, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 10s - loss: 4.4741 - acc: 0.8730 - mDice: 0.1417 - val_loss: 3.6693 - val_acc: 0.9063 - val_mDice: 0.1827

Epoch 00038: val_mDice improved from 0.17211 to 0.18266, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 10s - loss: 4.3548 - acc: 0.8735 - mDice: 0.1487 - val_loss: 4.0277 - val_acc: 0.9094 - val_mDice: 0.1747

Epoch 00039: val_mDice did not improve from 0.18266
Epoch 40/300
 - 10s - loss: 4.2810 - acc: 0.8746 - mDice: 0.1550 - val_loss: 3.9701 - val_acc: 0.9086 - val_mDice: 0.1831

Epoch 00040: val_mDice improved from 0.18266 to 0.18305, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 10s - loss: 4.2297 - acc: 0.8753 - mDice: 0.1602 - val_loss: 3.9745 - val_acc: 0.9100 - val_mDice: 0.1898

Epoch 00041: val_mDice improved from 0.18305 to 0.18978, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 10s - loss: 4.1668 - acc: 0.8764 - mDice: 0.1668 - val_loss: 3.6579 - val_acc: 0.9155 - val_mDice: 0.2063

Epoch 00042: val_mDice improved from 0.18978 to 0.20630, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 11s - loss: 4.1170 - acc: 0.8777 - mDice: 0.1728 - val_loss: 3.6808 - val_acc: 0.9157 - val_mDice: 0.2068

Epoch 00043: val_mDice improved from 0.20630 to 0.20685, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 44/300
 - 11s - loss: 4.0643 - acc: 0.8789 - mDice: 0.1789 - val_loss: 3.9878 - val_acc: 0.9175 - val_mDice: 0.2026

Epoch 00044: val_mDice did not improve from 0.20685
Epoch 45/300
 - 11s - loss: 4.0056 - acc: 0.8804 - mDice: 0.1865 - val_loss: 3.4738 - val_acc: 0.9177 - val_mDice: 0.2275

Epoch 00045: val_mDice improved from 0.20685 to 0.22753, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 11s - loss: 3.9592 - acc: 0.8813 - mDice: 0.1928 - val_loss: 3.6695 - val_acc: 0.9169 - val_mDice: 0.2238

Epoch 00046: val_mDice did not improve from 0.22753
Epoch 47/300
 - 11s - loss: 3.9277 - acc: 0.8822 - mDice: 0.1970 - val_loss: 3.4724 - val_acc: 0.9191 - val_mDice: 0.2382

Epoch 00047: val_mDice improved from 0.22753 to 0.23817, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 11s - loss: 3.8751 - acc: 0.8829 - mDice: 0.2026 - val_loss: 4.3223 - val_acc: 0.9206 - val_mDice: 0.2073

Epoch 00048: val_mDice did not improve from 0.23817
Epoch 49/300
 - 11s - loss: 3.8079 - acc: 0.8841 - mDice: 0.2085 - val_loss: 3.8944 - val_acc: 0.9209 - val_mDice: 0.2234

Epoch 00049: val_mDice did not improve from 0.23817
Epoch 50/300
 - 11s - loss: 3.7644 - acc: 0.8848 - mDice: 0.2136 - val_loss: 3.5101 - val_acc: 0.9215 - val_mDice: 0.2470

Epoch 00050: val_mDice improved from 0.23817 to 0.24697, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 11s - loss: 3.7228 - acc: 0.8856 - mDice: 0.2179 - val_loss: 3.4764 - val_acc: 0.9210 - val_mDice: 0.2572

Epoch 00051: val_mDice improved from 0.24697 to 0.25717, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 12s - loss: 3.6860 - acc: 0.8862 - mDice: 0.2216 - val_loss: 4.4339 - val_acc: 0.9190 - val_mDice: 0.2145

Epoch 00052: val_mDice did not improve from 0.25717
Epoch 53/300
 - 11s - loss: 3.6536 - acc: 0.8860 - mDice: 0.2247 - val_loss: 3.6390 - val_acc: 0.9191 - val_mDice: 0.2536

Epoch 00053: val_mDice did not improve from 0.25717
Epoch 54/300
 - 11s - loss: 3.6203 - acc: 0.8858 - mDice: 0.2281 - val_loss: 4.0893 - val_acc: 0.9189 - val_mDice: 0.2413

Epoch 00054: val_mDice did not improve from 0.25717
Epoch 55/300
 - 11s - loss: 3.5728 - acc: 0.8863 - mDice: 0.2330 - val_loss: 4.2331 - val_acc: 0.9188 - val_mDice: 0.2343

Epoch 00055: val_mDice did not improve from 0.25717
Epoch 56/300
 - 11s - loss: 3.5392 - acc: 0.8868 - mDice: 0.2370 - val_loss: 3.5291 - val_acc: 0.9181 - val_mDice: 0.2643

Epoch 00056: val_mDice improved from 0.25717 to 0.26426, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 57/300
 - 11s - loss: 3.5109 - acc: 0.8877 - mDice: 0.2398 - val_loss: 4.2599 - val_acc: 0.9175 - val_mDice: 0.2343

Epoch 00057: val_mDice did not improve from 0.26426
Epoch 58/300
 - 11s - loss: 3.4646 - acc: 0.8884 - mDice: 0.2445 - val_loss: 3.6335 - val_acc: 0.9191 - val_mDice: 0.2720

Epoch 00058: val_mDice improved from 0.26426 to 0.27199, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 11s - loss: 3.4211 - acc: 0.8887 - mDice: 0.2491 - val_loss: 4.0376 - val_acc: 0.9182 - val_mDice: 0.2589

Epoch 00059: val_mDice did not improve from 0.27199
Epoch 60/300
 - 11s - loss: 3.3825 - acc: 0.8890 - mDice: 0.2532 - val_loss: 3.4969 - val_acc: 0.9180 - val_mDice: 0.2810

Epoch 00060: val_mDice improved from 0.27199 to 0.28099, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 61/300
 - 11s - loss: 3.3510 - acc: 0.8894 - mDice: 0.2572 - val_loss: 3.8392 - val_acc: 0.9210 - val_mDice: 0.2797

Epoch 00061: val_mDice did not improve from 0.28099
Epoch 62/300
 - 11s - loss: 3.3234 - acc: 0.8900 - mDice: 0.2604 - val_loss: 3.8626 - val_acc: 0.9189 - val_mDice: 0.2709

Epoch 00062: val_mDice did not improve from 0.28099
Epoch 63/300
 - 11s - loss: 3.2954 - acc: 0.8905 - mDice: 0.2643 - val_loss: 3.2000 - val_acc: 0.9168 - val_mDice: 0.2988

Epoch 00063: val_mDice improved from 0.28099 to 0.29879, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 11s - loss: 3.2625 - acc: 0.8912 - mDice: 0.2682 - val_loss: 3.5602 - val_acc: 0.9212 - val_mDice: 0.2843

Epoch 00064: val_mDice did not improve from 0.29879
Epoch 65/300
 - 11s - loss: 3.2335 - acc: 0.8918 - mDice: 0.2725 - val_loss: 4.0287 - val_acc: 0.9189 - val_mDice: 0.2683

Epoch 00065: val_mDice did not improve from 0.29879
Epoch 66/300
 - 10s - loss: 3.2030 - acc: 0.8922 - mDice: 0.2755 - val_loss: 3.7747 - val_acc: 0.9208 - val_mDice: 0.2869

Epoch 00066: val_mDice did not improve from 0.29879
Epoch 67/300
 - 11s - loss: 3.1768 - acc: 0.8924 - mDice: 0.2784 - val_loss: 3.6752 - val_acc: 0.9228 - val_mDice: 0.2933

Epoch 00067: val_mDice did not improve from 0.29879
Epoch 68/300
 - 10s - loss: 3.1547 - acc: 0.8927 - mDice: 0.2810 - val_loss: 3.7444 - val_acc: 0.9212 - val_mDice: 0.2830

Epoch 00068: val_mDice did not improve from 0.29879
Epoch 69/300
 - 11s - loss: 3.1288 - acc: 0.8928 - mDice: 0.2843 - val_loss: 4.1980 - val_acc: 0.9234 - val_mDice: 0.2789

Epoch 00069: val_mDice did not improve from 0.29879
Epoch 70/300
 - 11s - loss: 3.1084 - acc: 0.8929 - mDice: 0.2876 - val_loss: 3.7511 - val_acc: 0.9223 - val_mDice: 0.2950

Epoch 00070: val_mDice did not improve from 0.29879
Epoch 71/300
 - 11s - loss: 3.0862 - acc: 0.8930 - mDice: 0.2906 - val_loss: 4.3597 - val_acc: 0.9206 - val_mDice: 0.2634

Epoch 00071: val_mDice did not improve from 0.29879
Epoch 72/300
 - 11s - loss: 3.0598 - acc: 0.8932 - mDice: 0.2934 - val_loss: 3.4253 - val_acc: 0.9225 - val_mDice: 0.3135

Epoch 00072: val_mDice improved from 0.29879 to 0.31347, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 73/300
 - 11s - loss: 3.0391 - acc: 0.8932 - mDice: 0.2960 - val_loss: 3.9564 - val_acc: 0.9206 - val_mDice: 0.2924

Epoch 00073: val_mDice did not improve from 0.31347
Epoch 74/300
 - 11s - loss: 3.0146 - acc: 0.8937 - mDice: 0.2995 - val_loss: 3.6802 - val_acc: 0.9236 - val_mDice: 0.3130

Epoch 00074: val_mDice did not improve from 0.31347
Epoch 75/300
 - 11s - loss: 2.9938 - acc: 0.8942 - mDice: 0.3023 - val_loss: 3.5555 - val_acc: 0.9227 - val_mDice: 0.3151

Epoch 00075: val_mDice improved from 0.31347 to 0.31514, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 76/300
 - 11s - loss: 2.9804 - acc: 0.8945 - mDice: 0.3038 - val_loss: 3.5155 - val_acc: 0.9257 - val_mDice: 0.3209

Epoch 00076: val_mDice improved from 0.31514 to 0.32094, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 77/300
 - 11s - loss: 2.9629 - acc: 0.8948 - mDice: 0.3062 - val_loss: 3.6573 - val_acc: 0.9242 - val_mDice: 0.3144

Epoch 00077: val_mDice did not improve from 0.32094
Epoch 78/300
 - 11s - loss: 2.9503 - acc: 0.8953 - mDice: 0.3088 - val_loss: 4.1007 - val_acc: 0.9236 - val_mDice: 0.2968

Epoch 00078: val_mDice did not improve from 0.32094
Epoch 79/300
 - 10s - loss: 2.9288 - acc: 0.8957 - mDice: 0.3111 - val_loss: 3.5043 - val_acc: 0.9223 - val_mDice: 0.3199

Epoch 00079: val_mDice did not improve from 0.32094
Epoch 80/300
 - 12s - loss: 2.9121 - acc: 0.8963 - mDice: 0.3133 - val_loss: 3.5871 - val_acc: 0.9300 - val_mDice: 0.3225

Epoch 00080: val_mDice improved from 0.32094 to 0.32250, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 81/300
 - 10s - loss: 2.8985 - acc: 0.8967 - mDice: 0.3155 - val_loss: 3.4983 - val_acc: 0.9247 - val_mDice: 0.3166

Epoch 00081: val_mDice did not improve from 0.32250
Epoch 82/300
 - 11s - loss: 2.8785 - acc: 0.8971 - mDice: 0.3185 - val_loss: 3.7761 - val_acc: 0.9279 - val_mDice: 0.3181

Epoch 00082: val_mDice did not improve from 0.32250
Epoch 83/300
 - 11s - loss: 2.8648 - acc: 0.8977 - mDice: 0.3209 - val_loss: 3.6637 - val_acc: 0.9255 - val_mDice: 0.3183

Epoch 00083: val_mDice did not improve from 0.32250
Epoch 84/300
 - 10s - loss: 2.8518 - acc: 0.8980 - mDice: 0.3226 - val_loss: 3.6478 - val_acc: 0.9273 - val_mDice: 0.3164

Epoch 00084: val_mDice did not improve from 0.32250
Epoch 85/300
 - 11s - loss: 2.8382 - acc: 0.8983 - mDice: 0.3240 - val_loss: 3.7782 - val_acc: 0.9255 - val_mDice: 0.3168

Epoch 00085: val_mDice did not improve from 0.32250
Epoch 86/300
 - 10s - loss: 2.8111 - acc: 0.8990 - mDice: 0.3286 - val_loss: 3.6510 - val_acc: 0.9289 - val_mDice: 0.3298

Epoch 00086: val_mDice improved from 0.32250 to 0.32980, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 87/300
 - 10s - loss: 2.7909 - acc: 0.8998 - mDice: 0.3327 - val_loss: 4.1706 - val_acc: 0.9268 - val_mDice: 0.3050

Epoch 00087: val_mDice did not improve from 0.32980
Epoch 88/300
 - 11s - loss: 2.7881 - acc: 0.8995 - mDice: 0.3325 - val_loss: 3.4186 - val_acc: 0.9300 - val_mDice: 0.3379

Epoch 00088: val_mDice improved from 0.32980 to 0.33795, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 89/300
 - 11s - loss: 2.7666 - acc: 0.9000 - mDice: 0.3365 - val_loss: 3.5764 - val_acc: 0.9276 - val_mDice: 0.3232

Epoch 00089: val_mDice did not improve from 0.33795
Epoch 90/300
 - 10s - loss: 2.7524 - acc: 0.9003 - mDice: 0.3391 - val_loss: 3.4327 - val_acc: 0.9307 - val_mDice: 0.3363

Epoch 00090: val_mDice did not improve from 0.33795
Epoch 91/300
 - 10s - loss: 2.7420 - acc: 0.9003 - mDice: 0.3406 - val_loss: 3.7404 - val_acc: 0.9293 - val_mDice: 0.3211

Epoch 00091: val_mDice did not improve from 0.33795
Epoch 92/300
 - 11s - loss: 2.7216 - acc: 0.9010 - mDice: 0.3448 - val_loss: 3.5767 - val_acc: 0.9294 - val_mDice: 0.3281

Epoch 00092: val_mDice did not improve from 0.33795
Epoch 93/300
 - 10s - loss: 2.7092 - acc: 0.9013 - mDice: 0.3471 - val_loss: 3.7937 - val_acc: 0.9279 - val_mDice: 0.3240

Epoch 00093: val_mDice did not improve from 0.33795
Epoch 94/300
 - 11s - loss: 2.6939 - acc: 0.9019 - mDice: 0.3498 - val_loss: 3.6205 - val_acc: 0.9315 - val_mDice: 0.3353

Epoch 00094: val_mDice did not improve from 0.33795
Epoch 95/300
 - 10s - loss: 2.6861 - acc: 0.9020 - mDice: 0.3501 - val_loss: 3.3264 - val_acc: 0.9321 - val_mDice: 0.3503

Epoch 00095: val_mDice improved from 0.33795 to 0.35028, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 96/300
 - 10s - loss: 2.6731 - acc: 0.9024 - mDice: 0.3538 - val_loss: 3.7875 - val_acc: 0.9314 - val_mDice: 0.3369

Epoch 00096: val_mDice did not improve from 0.35028
Epoch 97/300
 - 11s - loss: 2.6773 - acc: 0.9025 - mDice: 0.3528 - val_loss: 3.6035 - val_acc: 0.9321 - val_mDice: 0.3438

Epoch 00097: val_mDice did not improve from 0.35028
Epoch 98/300
 - 10s - loss: 2.6623 - acc: 0.9027 - mDice: 0.3560 - val_loss: 3.3412 - val_acc: 0.9334 - val_mDice: 0.3557

Epoch 00098: val_mDice improved from 0.35028 to 0.35571, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 99/300
 - 10s - loss: 2.6516 - acc: 0.9033 - mDice: 0.3579 - val_loss: 3.3777 - val_acc: 0.9321 - val_mDice: 0.3500

Epoch 00099: val_mDice did not improve from 0.35571
Epoch 100/300
 - 11s - loss: 2.6395 - acc: 0.9034 - mDice: 0.3595 - val_loss: 3.5033 - val_acc: 0.9328 - val_mDice: 0.3477

Epoch 00100: val_mDice did not improve from 0.35571
Epoch 101/300
 - 10s - loss: 2.6264 - acc: 0.9037 - mDice: 0.3622 - val_loss: 3.5502 - val_acc: 0.9332 - val_mDice: 0.3416

Epoch 00101: val_mDice did not improve from 0.35571
Epoch 102/300
 - 10s - loss: 2.6256 - acc: 0.9037 - mDice: 0.3615 - val_loss: 3.5787 - val_acc: 0.9334 - val_mDice: 0.3433

Epoch 00102: val_mDice did not improve from 0.35571
Epoch 103/300
 - 10s - loss: 2.6089 - acc: 0.9043 - mDice: 0.3649 - val_loss: 3.5983 - val_acc: 0.9323 - val_mDice: 0.3412

Epoch 00103: val_mDice did not improve from 0.35571
Epoch 104/300
 - 11s - loss: 2.6018 - acc: 0.9043 - mDice: 0.3660 - val_loss: 3.3636 - val_acc: 0.9335 - val_mDice: 0.3526

Epoch 00104: val_mDice did not improve from 0.35571
Epoch 105/300
 - 10s - loss: 2.5988 - acc: 0.9045 - mDice: 0.3665 - val_loss: 3.7441 - val_acc: 0.9342 - val_mDice: 0.3448

Epoch 00105: val_mDice did not improve from 0.35571
Epoch 106/300
 - 11s - loss: 2.5847 - acc: 0.9050 - mDice: 0.3700 - val_loss: 3.6960 - val_acc: 0.9325 - val_mDice: 0.3475

Epoch 00106: val_mDice did not improve from 0.35571
Epoch 107/300
 - 11s - loss: 2.5689 - acc: 0.9055 - mDice: 0.3726 - val_loss: 3.7525 - val_acc: 0.9368 - val_mDice: 0.3488

Epoch 00107: val_mDice did not improve from 0.35571
Epoch 108/300
 - 10s - loss: 2.5656 - acc: 0.9056 - mDice: 0.3729 - val_loss: 3.3828 - val_acc: 0.9338 - val_mDice: 0.3515

Epoch 00108: val_mDice did not improve from 0.35571
Epoch 109/300
 - 11s - loss: 2.5689 - acc: 0.9057 - mDice: 0.3724 - val_loss: 3.9849 - val_acc: 0.9326 - val_mDice: 0.3300

Epoch 00109: val_mDice did not improve from 0.35571
Epoch 110/300
 - 10s - loss: 2.5535 - acc: 0.9061 - mDice: 0.3749 - val_loss: 3.8487 - val_acc: 0.9359 - val_mDice: 0.3418

Epoch 00110: val_mDice did not improve from 0.35571
Epoch 111/300
 - 10s - loss: 2.5442 - acc: 0.9063 - mDice: 0.3768 - val_loss: 3.8568 - val_acc: 0.9326 - val_mDice: 0.3350

Epoch 00111: val_mDice did not improve from 0.35571
Epoch 112/300
 - 10s - loss: 2.5430 - acc: 0.9064 - mDice: 0.3781 - val_loss: 3.2322 - val_acc: 0.9339 - val_mDice: 0.3550

Epoch 00112: val_mDice did not improve from 0.35571
Epoch 113/300
 - 10s - loss: 2.5325 - acc: 0.9065 - mDice: 0.3789 - val_loss: 3.4171 - val_acc: 0.9357 - val_mDice: 0.3646

Epoch 00113: val_mDice improved from 0.35571 to 0.36460, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 114/300
 - 11s - loss: 2.5229 - acc: 0.9070 - mDice: 0.3811 - val_loss: 3.9575 - val_acc: 0.9363 - val_mDice: 0.3381

Epoch 00114: val_mDice did not improve from 0.36460
Epoch 115/300
 - 10s - loss: 2.5145 - acc: 0.9069 - mDice: 0.3827 - val_loss: 3.6384 - val_acc: 0.9364 - val_mDice: 0.3553

Epoch 00115: val_mDice did not improve from 0.36460
Epoch 116/300
 - 10s - loss: 2.5091 - acc: 0.9072 - mDice: 0.3842 - val_loss: 3.5010 - val_acc: 0.9357 - val_mDice: 0.3549

Epoch 00116: val_mDice did not improve from 0.36460
Epoch 117/300
 - 11s - loss: 2.4956 - acc: 0.9074 - mDice: 0.3863 - val_loss: 3.3579 - val_acc: 0.9369 - val_mDice: 0.3706

Epoch 00117: val_mDice improved from 0.36460 to 0.37059, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 118/300
 - 10s - loss: 2.4913 - acc: 0.9077 - mDice: 0.3876 - val_loss: 3.2408 - val_acc: 0.9342 - val_mDice: 0.3680

Epoch 00118: val_mDice did not improve from 0.37059
Epoch 119/300
 - 11s - loss: 2.4919 - acc: 0.9078 - mDice: 0.3878 - val_loss: 3.8062 - val_acc: 0.9375 - val_mDice: 0.3543

Epoch 00119: val_mDice did not improve from 0.37059
Epoch 120/300
 - 10s - loss: 2.4836 - acc: 0.9078 - mDice: 0.3888 - val_loss: 3.3017 - val_acc: 0.9376 - val_mDice: 0.3679

Epoch 00120: val_mDice did not improve from 0.37059
Epoch 121/300
 - 10s - loss: 2.4854 - acc: 0.9079 - mDice: 0.3886 - val_loss: 4.1253 - val_acc: 0.9333 - val_mDice: 0.3311

Epoch 00121: val_mDice did not improve from 0.37059
Epoch 122/300
 - 11s - loss: 2.4776 - acc: 0.9079 - mDice: 0.3904 - val_loss: 3.7877 - val_acc: 0.9370 - val_mDice: 0.3464

Epoch 00122: val_mDice did not improve from 0.37059
Epoch 123/300
 - 10s - loss: 2.4672 - acc: 0.9085 - mDice: 0.3929 - val_loss: 3.5372 - val_acc: 0.9358 - val_mDice: 0.3581

Epoch 00123: val_mDice did not improve from 0.37059
Epoch 124/300
 - 11s - loss: 2.4638 - acc: 0.9081 - mDice: 0.3924 - val_loss: 3.5363 - val_acc: 0.9383 - val_mDice: 0.3655

Epoch 00124: val_mDice did not improve from 0.37059
Epoch 125/300
 - 10s - loss: 2.4527 - acc: 0.9085 - mDice: 0.3953 - val_loss: 4.1361 - val_acc: 0.9361 - val_mDice: 0.3409

Epoch 00125: val_mDice did not improve from 0.37059
Epoch 126/300
 - 11s - loss: 2.4555 - acc: 0.9085 - mDice: 0.3945 - val_loss: 3.8084 - val_acc: 0.9361 - val_mDice: 0.3497

Epoch 00126: val_mDice did not improve from 0.37059
Epoch 127/300
 - 10s - loss: 2.4457 - acc: 0.9086 - mDice: 0.3964 - val_loss: 3.3218 - val_acc: 0.9373 - val_mDice: 0.3626

Epoch 00127: val_mDice did not improve from 0.37059
Epoch 128/300
 - 10s - loss: 2.4485 - acc: 0.9088 - mDice: 0.3960 - val_loss: 4.1371 - val_acc: 0.9365 - val_mDice: 0.3412

Epoch 00128: val_mDice did not improve from 0.37059
Epoch 129/300
 - 11s - loss: 2.4318 - acc: 0.9091 - mDice: 0.3989 - val_loss: 3.5705 - val_acc: 0.9392 - val_mDice: 0.3671

Epoch 00129: val_mDice did not improve from 0.37059
Epoch 130/300
 - 10s - loss: 2.4352 - acc: 0.9092 - mDice: 0.3986 - val_loss: 3.5596 - val_acc: 0.9373 - val_mDice: 0.3655

Epoch 00130: val_mDice did not improve from 0.37059
Epoch 131/300
 - 11s - loss: 2.4274 - acc: 0.9092 - mDice: 0.3996 - val_loss: 3.7301 - val_acc: 0.9352 - val_mDice: 0.3481

Epoch 00131: val_mDice did not improve from 0.37059
Epoch 132/300
 - 10s - loss: 2.4357 - acc: 0.9092 - mDice: 0.3992 - val_loss: 3.1356 - val_acc: 0.9375 - val_mDice: 0.3690

Epoch 00132: val_mDice did not improve from 0.37059
Epoch 133/300
 - 10s - loss: 2.4224 - acc: 0.9096 - mDice: 0.4021 - val_loss: 3.4044 - val_acc: 0.9369 - val_mDice: 0.3720

Epoch 00133: val_mDice improved from 0.37059 to 0.37198, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 134/300
 - 10s - loss: 2.4264 - acc: 0.9094 - mDice: 0.4012 - val_loss: 3.3750 - val_acc: 0.9387 - val_mDice: 0.3706

Epoch 00134: val_mDice did not improve from 0.37198
Epoch 135/300
 - 11s - loss: 2.4185 - acc: 0.9095 - mDice: 0.4018 - val_loss: 3.2183 - val_acc: 0.9388 - val_mDice: 0.3790

Epoch 00135: val_mDice improved from 0.37198 to 0.37902, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 136/300
 - 10s - loss: 2.4166 - acc: 0.9097 - mDice: 0.4033 - val_loss: 3.5315 - val_acc: 0.9375 - val_mDice: 0.3687

Epoch 00136: val_mDice did not improve from 0.37902
Epoch 137/300
 - 10s - loss: 2.4073 - acc: 0.9097 - mDice: 0.4048 - val_loss: 3.5347 - val_acc: 0.9390 - val_mDice: 0.3643

Epoch 00137: val_mDice did not improve from 0.37902
Epoch 138/300
 - 11s - loss: 2.4065 - acc: 0.9099 - mDice: 0.4047 - val_loss: 3.4292 - val_acc: 0.9382 - val_mDice: 0.3660

Epoch 00138: val_mDice did not improve from 0.37902
Epoch 139/300
 - 10s - loss: 2.4035 - acc: 0.9098 - mDice: 0.4055 - val_loss: 3.4468 - val_acc: 0.9368 - val_mDice: 0.3631

Epoch 00139: val_mDice did not improve from 0.37902
Epoch 140/300
 - 11s - loss: 2.4083 - acc: 0.9098 - mDice: 0.4045 - val_loss: 3.4075 - val_acc: 0.9378 - val_mDice: 0.3679

Epoch 00140: val_mDice did not improve from 0.37902
Epoch 141/300
 - 11s - loss: 2.3944 - acc: 0.9101 - mDice: 0.4074 - val_loss: 3.1670 - val_acc: 0.9380 - val_mDice: 0.3748

Epoch 00141: val_mDice did not improve from 0.37902
Epoch 142/300
 - 10s - loss: 2.3932 - acc: 0.9101 - mDice: 0.4076 - val_loss: 3.0965 - val_acc: 0.9386 - val_mDice: 0.3840

Epoch 00142: val_mDice improved from 0.37902 to 0.38397, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 143/300
 - 11s - loss: 2.3948 - acc: 0.9101 - mDice: 0.4075 - val_loss: 3.5978 - val_acc: 0.9366 - val_mDice: 0.3617

Epoch 00143: val_mDice did not improve from 0.38397
Epoch 144/300
 - 11s - loss: 2.3880 - acc: 0.9102 - mDice: 0.4083 - val_loss: 3.4208 - val_acc: 0.9381 - val_mDice: 0.3621

Epoch 00144: val_mDice did not improve from 0.38397
Epoch 145/300
 - 10s - loss: 2.3857 - acc: 0.9102 - mDice: 0.4086 - val_loss: 3.6858 - val_acc: 0.9365 - val_mDice: 0.3543

Epoch 00145: val_mDice did not improve from 0.38397
Epoch 146/300
 - 11s - loss: 2.3844 - acc: 0.9103 - mDice: 0.4089 - val_loss: 3.5229 - val_acc: 0.9400 - val_mDice: 0.3769

Epoch 00146: val_mDice did not improve from 0.38397
Epoch 147/300
 - 11s - loss: 2.3760 - acc: 0.9103 - mDice: 0.4111 - val_loss: 3.5728 - val_acc: 0.9371 - val_mDice: 0.3579

Epoch 00147: val_mDice did not improve from 0.38397
Epoch 148/300
 - 10s - loss: 2.3742 - acc: 0.9106 - mDice: 0.4114 - val_loss: 3.3225 - val_acc: 0.9400 - val_mDice: 0.3741

Epoch 00148: val_mDice did not improve from 0.38397
Epoch 149/300
 - 11s - loss: 2.3761 - acc: 0.9106 - mDice: 0.4118 - val_loss: 3.3822 - val_acc: 0.9375 - val_mDice: 0.3673

Epoch 00149: val_mDice did not improve from 0.38397
Epoch 150/300
 - 11s - loss: 2.3693 - acc: 0.9106 - mDice: 0.4122 - val_loss: 3.1628 - val_acc: 0.9378 - val_mDice: 0.3748

Epoch 00150: val_mDice did not improve from 0.38397
Epoch 151/300
 - 10s - loss: 2.3762 - acc: 0.9106 - mDice: 0.4111 - val_loss: 3.6826 - val_acc: 0.9368 - val_mDice: 0.3563

Epoch 00151: val_mDice did not improve from 0.38397
Epoch 152/300
 - 11s - loss: 2.3637 - acc: 0.9109 - mDice: 0.4139 - val_loss: 3.3062 - val_acc: 0.9393 - val_mDice: 0.3796

Epoch 00152: val_mDice did not improve from 0.38397
Epoch 153/300
 - 11s - loss: 2.3622 - acc: 0.9108 - mDice: 0.4138 - val_loss: 3.3593 - val_acc: 0.9376 - val_mDice: 0.3727

Epoch 00153: val_mDice did not improve from 0.38397
Epoch 154/300
 - 10s - loss: 2.3554 - acc: 0.9111 - mDice: 0.4143 - val_loss: 3.7162 - val_acc: 0.9382 - val_mDice: 0.3683

Epoch 00154: val_mDice did not improve from 0.38397
Epoch 155/300
 - 11s - loss: 2.3538 - acc: 0.9111 - mDice: 0.4165 - val_loss: 3.4395 - val_acc: 0.9390 - val_mDice: 0.3694

Epoch 00155: val_mDice did not improve from 0.38397
Epoch 156/300
 - 11s - loss: 2.3554 - acc: 0.9109 - mDice: 0.4149 - val_loss: 3.4756 - val_acc: 0.9373 - val_mDice: 0.3638

Epoch 00156: val_mDice did not improve from 0.38397
Epoch 157/300
 - 10s - loss: 2.3546 - acc: 0.9110 - mDice: 0.4157 - val_loss: 3.7483 - val_acc: 0.9358 - val_mDice: 0.3556

Epoch 00157: val_mDice did not improve from 0.38397
Epoch 158/300
 - 11s - loss: 2.3517 - acc: 0.9111 - mDice: 0.4162 - val_loss: 3.8919 - val_acc: 0.9380 - val_mDice: 0.3591

Epoch 00158: val_mDice did not improve from 0.38397
Epoch 159/300
 - 10s - loss: 2.3435 - acc: 0.9111 - mDice: 0.4168 - val_loss: 3.7355 - val_acc: 0.9370 - val_mDice: 0.3618

Epoch 00159: val_mDice did not improve from 0.38397
Epoch 160/300
 - 10s - loss: 2.3475 - acc: 0.9112 - mDice: 0.4175 - val_loss: 3.6046 - val_acc: 0.9374 - val_mDice: 0.3607

Epoch 00160: val_mDice did not improve from 0.38397
Epoch 161/300
 - 11s - loss: 2.3473 - acc: 0.9112 - mDice: 0.4167 - val_loss: 3.7159 - val_acc: 0.9380 - val_mDice: 0.3600

Epoch 00161: val_mDice did not improve from 0.38397
Epoch 162/300
 - 11s - loss: 2.3403 - acc: 0.9114 - mDice: 0.4185 - val_loss: 3.3900 - val_acc: 0.9382 - val_mDice: 0.3772

Epoch 00162: val_mDice did not improve from 0.38397
Epoch 163/300
 - 10s - loss: 2.3368 - acc: 0.9115 - mDice: 0.4182 - val_loss: 3.4512 - val_acc: 0.9378 - val_mDice: 0.3795

Epoch 00163: val_mDice did not improve from 0.38397
Epoch 164/300
 - 11s - loss: 2.3373 - acc: 0.9116 - mDice: 0.4190 - val_loss: 3.3895 - val_acc: 0.9387 - val_mDice: 0.3831

Epoch 00164: val_mDice did not improve from 0.38397
Epoch 165/300
 - 10s - loss: 2.3304 - acc: 0.9118 - mDice: 0.4200 - val_loss: 3.2867 - val_acc: 0.9402 - val_mDice: 0.3811

Epoch 00165: val_mDice did not improve from 0.38397
Epoch 166/300
 - 10s - loss: 2.3257 - acc: 0.9118 - mDice: 0.4215 - val_loss: 3.3279 - val_acc: 0.9375 - val_mDice: 0.3806

Epoch 00166: val_mDice did not improve from 0.38397
Epoch 167/300
 - 11s - loss: 2.3282 - acc: 0.9115 - mDice: 0.4203 - val_loss: 3.5861 - val_acc: 0.9338 - val_mDice: 0.3577

Epoch 00167: val_mDice did not improve from 0.38397
Epoch 168/300
 - 11s - loss: 2.3227 - acc: 0.9119 - mDice: 0.4217 - val_loss: 3.1905 - val_acc: 0.9385 - val_mDice: 0.3829

Epoch 00168: val_mDice did not improve from 0.38397
Epoch 169/300
 - 10s - loss: 2.3260 - acc: 0.9117 - mDice: 0.4210 - val_loss: 3.8426 - val_acc: 0.9351 - val_mDice: 0.3530

Epoch 00169: val_mDice did not improve from 0.38397
Epoch 170/300
 - 11s - loss: 2.3232 - acc: 0.9118 - mDice: 0.4214 - val_loss: 3.6797 - val_acc: 0.9396 - val_mDice: 0.3735

Epoch 00170: val_mDice did not improve from 0.38397
Epoch 171/300
 - 11s - loss: 2.3235 - acc: 0.9119 - mDice: 0.4221 - val_loss: 3.5857 - val_acc: 0.9378 - val_mDice: 0.3694

Epoch 00171: val_mDice did not improve from 0.38397
Epoch 172/300
 - 10s - loss: 2.3115 - acc: 0.9120 - mDice: 0.4235 - val_loss: 3.3380 - val_acc: 0.9380 - val_mDice: 0.3794

Epoch 00172: val_mDice did not improve from 0.38397
Restoring model weights from the end of the best epoch
Epoch 00172: early stopping
{'val_loss': [152.62503069923036, 96.50183827536446, 49.60914620899019, 19.151949371610367, 12.967497011025747, 10.66341443146978, 9.608314911524454, 8.250791170057797, 7.563417241686866, 7.087607196044354, 6.700092454396543, 6.201665974443867, 5.86880165107903, 5.553027892219169, 5.73984983740818, 5.507366503810599, 5.646331466113527, 5.707810302309337, 5.048500410591562, 5.001767720406254, 5.209445705016454, 4.77611022823978, 4.683622680959248, 5.606080678602059, 4.681531782040284, 4.737599419075108, 4.850397405525048, 3.9969350469875193, 4.983650907741061, 4.040515508635768, 4.172203906102195, 4.150072910601184, 3.830069118046335, 3.867603327369406, 5.227735779691665, 4.100213174885583, 3.852340537389474, 3.6692767515335056, 4.027712458966389, 3.970136784150132, 3.9745315301808573, 3.657909143849143, 3.6808149127200958, 3.987789243681445, 3.4737582332676364, 3.669512788986876, 3.4723750868191323, 4.322311639830115, 3.8943604048164118, 3.510147114373034, 3.4764244521613277, 4.433860238774547, 3.6390481620051323, 4.089334877828757, 4.233107886676278, 3.5290855598148134, 4.259936332192627, 3.6335101975128055, 4.037592392392634, 3.4969277138422643, 3.839158552299653, 3.862553222814486, 3.200015610039589, 3.5601643427452516, 4.028697793305453, 3.7746894180198156, 3.675166137738242, 3.744391948406008, 4.197987478847305, 3.7511083049300527, 4.359679934125216, 3.425279051464583, 3.9563565217180265, 3.68017202850786, 3.5555024347489788, 3.5155375747098807, 3.6573156978092376, 4.100691091535347, 3.5043352334316644, 3.587059732038705, 3.4983105621788475, 3.7760514718524756, 3.663717226568787, 3.647795798633957, 3.778155850135677, 3.6510219670211277, 4.170624916840877, 3.4185965106645155, 3.576421672788759, 3.4326823867547014, 3.740434591902331, 3.5766900709076297, 3.793727088054376, 3.620522398279891, 3.326436483656012, 3.7875465682840774, 3.6034505721403374, 3.3412255487360416, 3.377662977358947, 3.5033288153908435, 3.5501933889685287, 3.578736954967358, 3.5982726048234674, 3.3635950330423103, 3.7441010766086125, 3.6959627569387004, 3.752481623597088, 3.3827560816758444, 3.9849317012177337, 3.848682820131736, 3.8568168610023954, 3.2321732458658516, 3.4171302767381784, 3.957478822652428, 3.638404626997986, 3.5010358342191292, 3.357888494862155, 3.240848353088257, 3.8061782434538363, 3.3017030263186564, 4.12534405798873, 3.787748982442454, 3.5372222124909363, 3.536327517170104, 4.136092493399268, 3.8084189356526448, 3.3218152513727546, 4.137128016889273, 3.570535803967643, 3.5596129549445497, 3.7301282409489867, 3.135597028680855, 3.404416058123821, 3.3749937432418977, 3.21831470847662, 3.5315012730258917, 3.534731020663111, 3.429188179606128, 3.446814821412166, 3.407454803275565, 3.1669683185450377, 3.0964538188252066, 3.5978235422544893, 3.420827859968302, 3.6857946403324604, 3.5228597359022213, 3.572839362502453, 3.3225189851197814, 3.382202249973835, 3.162845029584354, 3.6826210358579243, 3.3061531916305067, 3.359324592032603, 3.716169038388346, 3.4394692211367546, 3.47564963193699, 3.7482654268276834, 3.891912712981658, 3.7354848266446163, 3.6046143012298715, 3.715913926162535, 3.3899505854628624, 3.451168602909006, 3.389547946264169, 3.2867061557869115, 3.32790437032513, 3.5861189157391586, 3.1904925857670605, 3.8426455771328794, 3.6796740169991695, 3.5857004653884186, 3.3380088792404248], 'val_acc': [0.8713759127117339, 0.9040888349215189, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047481758253915, 0.9047504464785258, 0.9048992877914792, 0.9049290333475385, 0.9054441310110546, 0.9051579520815894, 0.9056890777179173, 0.9052678715615046, 0.905817312853677, 0.9064926618621463, 0.9063347038768587, 0.9094391181355431, 0.9086332292783827, 0.9100182907921928, 0.9155242471467882, 0.9156639320509774, 0.9175114489737011, 0.9177243652797881, 0.9169322394189381, 0.9190521836280823, 0.9205792347590128, 0.9208997175807044, 0.9214903769038972, 0.9209706783294678, 0.919038440499987, 0.9190796670459566, 0.9188850607190814, 0.9188232421875, 0.9181455913044158, 0.9174542256764003, 0.9191414713859558, 0.9181753510520572, 0.9180402840886798, 0.9210210470926194, 0.9189400445847284, 0.9167948961257935, 0.9211973207337516, 0.9189262901033673, 0.9208127288591295, 0.9228090672265916, 0.9211538519178119, 0.9234203440802438, 0.9222527345021566, 0.9205952570551917, 0.9224839976855687, 0.9206044106256395, 0.9235943215233939, 0.9226625533331008, 0.9257211798713321, 0.9242170112473624, 0.9235828831082299, 0.9223351847557795, 0.9299977137928918, 0.9246588803472973, 0.9278685961450849, 0.9254555957657951, 0.9272916430518741, 0.925478492464338, 0.9289377303350539, 0.9267902970314026, 0.9299656635239011, 0.9276419281959534, 0.9306639461290269, 0.9293429737999326, 0.9293681383132935, 0.9279349786894662, 0.931469806603023, 0.9320902086439586, 0.9313804705937704, 0.9321451726413909, 0.9334043292772203, 0.932074194862729, 0.9327976249513172, 0.9331616305169605, 0.9334019961811247, 0.9322641974403745, 0.9335050497736249, 0.9342353571028936, 0.9325412114461263, 0.9368040192694891, 0.9338140856652033, 0.9326419347808474, 0.9358539410999843, 0.9326076110204061, 0.9339102676936558, 0.9357257116408575, 0.9362523130008152, 0.9364377487273443, 0.9357211334364754, 0.9369207961218697, 0.9342399324689593, 0.9375366398266384, 0.9375503943079994, 0.9333470832733881, 0.9369963464282808, 0.9357532206035796, 0.938253192674546, 0.9361378068015689, 0.9361057451793126, 0.9372504353523254, 0.936536158834185, 0.939237634340922, 0.9372527230353582, 0.9351922898065477, 0.9375045725277492, 0.9369207790919712, 0.9387133802686419, 0.9387637149719965, 0.9374748127801078, 0.9390201455070859, 0.9381822319257827, 0.9368383827663603, 0.9378044747170948, 0.9380494611603873, 0.938596628961109, 0.9366300503412882, 0.9380700616609483, 0.9364606028511411, 0.9400068776948112, 0.937065030847277, 0.9400411815870375, 0.9375480782418024, 0.9378205339113871, 0.9367788547561282, 0.939281131540026, 0.9375572176206679, 0.9381708048638844, 0.938969768228985, 0.9373328912825811, 0.935769206001645, 0.9380311171213785, 0.9370100583348956, 0.9373878013520014, 0.9379670137450808, 0.9382417656126476, 0.9378090727896917, 0.938736271290552, 0.9401831712041583, 0.93747250522886, 0.9337637083871024, 0.9384889943259103, 0.9350549550283522, 0.9396222801435561, 0.9378411031904674, 0.9380403161048889], 'val_mDice': [0.016823523467229234, 0.01719915330232609, 0.014782156706565902, 0.012978772067331843, 0.012524084448592649, 0.014080498921906664, 0.014442508674359747, 0.015040477161251363, 0.011376520330529837, 0.012605835617120777, 0.013037452341190406, 0.01743632066063583, 0.022934284050106293, 0.03102192991147084, 0.0293492367934613, 0.03542222235617893, 0.036805687860275306, 0.03688841635760452, 0.04907867494260981, 0.05659130235601749, 0.05577035376890784, 0.0694142336779762, 0.07743959683215335, 0.0560958706995561, 0.08118400942268116, 0.0856217494057048, 0.0900966568096053, 0.1141222764161371, 0.0965989852058036, 0.13331502276871884, 0.13221529516435804, 0.13680162546890123, 0.15386081163194917, 0.1575476936995983, 0.11338251252614316, 0.15771368704736233, 0.1721086340202462, 0.18266281059810094, 0.1746713425964117, 0.18305418454110622, 0.1897805534480583, 0.20629592643429837, 0.20684620857770955, 0.2025556182932286, 0.22752773140867552, 0.2238210491126492, 0.23816940893552133, 0.20731554388822543, 0.2234248085213559, 0.24696794147824958, 0.25716752412595917, 0.2144731410912105, 0.2536377140453884, 0.24130142276130973, 0.23430876886206015, 0.2642596931684585, 0.23433054451431548, 0.2719901595264673, 0.25886289464930695, 0.28098676956835245, 0.27971559204161167, 0.2708586861512491, 0.29879217133635566, 0.28425059219201404, 0.2683394297602631, 0.2869336378006708, 0.2932936112795557, 0.2829990504043443, 0.2789299988320896, 0.29500441528147175, 0.2633528173679397, 0.31346612193045165, 0.29240810471986023, 0.31299575684326036, 0.3151371052399987, 0.3209400651532979, 0.3144116613659121, 0.2967895833509309, 0.31990954945130007, 0.32250177212769077, 0.31655505964798586, 0.3180839034418265, 0.3183173680944102, 0.3164381930338485, 0.31677853546681856, 0.3297961153799579, 0.3050363778713204, 0.33794592285440084, 0.32321990627263275, 0.33633983933499884, 0.3211109580560809, 0.3280989278462671, 0.32403591124429587, 0.33531245181248304, 0.3502768755313896, 0.33691823296248913, 0.3438429825362705, 0.35571106691800414, 0.3500301382903542, 0.34770220589070094, 0.3416480358157839, 0.34326655896646635, 0.34121109261399224, 0.3526210981820311, 0.34483414409416063, 0.3474512048775241, 0.3487543304051672, 0.35152540586534, 0.3300490432551929, 0.3418279569596052, 0.33495214616968516, 0.35501317644403096, 0.3646019796530406, 0.3381133242732003, 0.3552925022585051, 0.35489058592135, 0.370588452156101, 0.36798090434500147, 0.354283331671641, 0.3679241312756425, 0.33113111227395986, 0.34636236461145536, 0.358085104397365, 0.3654704757389568, 0.3409173465555623, 0.34969927104456083, 0.36258668044493314, 0.34124974356520743, 0.36711333461460616, 0.3654561875654118, 0.34813900886192206, 0.36901365131849334, 0.3719823117412272, 0.37060111530479933, 0.37902271800807547, 0.36866617229368004, 0.3643135630658695, 0.3660255404455321, 0.363064139017037, 0.3679055754272711, 0.37477139312596547, 0.38397285838921863, 0.36167614676413085, 0.3620980623222533, 0.3543117064095679, 0.3768989463292417, 0.3578952419615927, 0.3740567674949056, 0.36732173143398195, 0.374798446893692, 0.35630709323145093, 0.3796309530735016, 0.3727317956231889, 0.36825067709599224, 0.36938383820511045, 0.3638070821762085, 0.3556315323249215, 0.35907689446494695, 0.3618342182820752, 0.36074996633189066, 0.35997817683077993, 0.3771731888077089, 0.3794571324473336, 0.3830755452315013, 0.3810573530693849, 0.3806448064389683, 0.35769328545956386, 0.38294310175946783, 0.35298959644777433, 0.37350858960832867, 0.36943724378943443, 0.3794166087394669], 'loss': [209.61704875298128, 127.94704782762852, 76.19671575187004, 47.404460665392286, 31.77679847517247, 23.075458521647146, 18.15062065797434, 14.89353727554342, 12.746427402161716, 11.291383360308766, 10.242177211735, 9.423233951611259, 8.790809082897592, 8.295227540764294, 7.880685027541511, 7.408608182032306, 7.0527871659387165, 6.777746084730635, 6.556731742081533, 6.358142206520388, 6.190546200320007, 6.032326008649789, 5.891221654743778, 5.774053087578313, 5.638507675430507, 5.52104342160759, 5.421767714902893, 5.302162401428428, 5.2022252195438075, 5.10192752068319, 5.00652160197755, 4.911177741876797, 4.83326555043028, 4.753448707464, 4.67680191722522, 4.607363196358829, 4.532574634550163, 4.474058273740878, 4.354809517021098, 4.280972302017447, 4.229656113487966, 4.166809419365731, 4.117041276235306, 4.064302448410967, 4.005596012928501, 3.959177084596074, 3.9277209133363677, 3.8751242355227538, 3.80791401881484, 3.7643768609500885, 3.722753908906756, 3.6860036172456816, 3.6536295373062417, 3.6202612583729628, 3.5727925743697257, 3.5392195689372525, 3.5108759045164226, 3.46460806126077, 3.421145770644921, 3.3825312261064777, 3.35100641635794, 3.3233709313723767, 3.295378906730974, 3.2624846992636085, 3.2334961394296577, 3.203007021137853, 3.1768074162047286, 3.1546856770516363, 3.128835508123353, 3.1084350344531217, 3.08623974211119, 3.0598423560046726, 3.0390571453954207, 3.0146482467007862, 2.993785468971704, 2.980402653217775, 2.9629313637807186, 2.9502590968012696, 2.928794447893533, 2.912138634250186, 2.898530782659504, 2.8784968684996075, 2.864793582943471, 2.8517564055040343, 2.838174560016259, 2.81110517643586, 2.7908915957205784, 2.788129605423262, 2.766577531083921, 2.752407358863438, 2.741995827641955, 2.7216159923471466, 2.709157561736459, 2.6939377521351626, 2.686091081287582, 2.6730860080796215, 2.6773204946646647, 2.662316417013815, 2.651562420968401, 2.6394808547722484, 2.626376550942688, 2.6256342159430828, 2.6088522470868436, 2.601827864061322, 2.59880675119496, 2.584729127601688, 2.5689373962079856, 2.5656117670610046, 2.568866256507394, 2.5534504987794997, 2.544168851038659, 2.5430436568244565, 2.5324827225561752, 2.5228871470222085, 2.514524092282637, 2.5090843714430355, 2.4956298184711994, 2.491283808182907, 2.491858436809883, 2.483634765860155, 2.485398683815488, 2.4775938428342283, 2.4672476546438484, 2.46381611724752, 2.452727060040367, 2.455490394021956, 2.445715939100018, 2.4484639223862503, 2.4317931176518943, 2.435202523856258, 2.4274156429828744, 2.4356538213377217, 2.4223869942920113, 2.426372982275746, 2.418472660881765, 2.4166090655745123, 2.4072976280136653, 2.4065332731714317, 2.4034922141111354, 2.4083397714117716, 2.3944156524738367, 2.3932042037542094, 2.3947895321975627, 2.3880409802318607, 2.385699572442744, 2.3843625005968785, 2.3760085471544694, 2.374201837798546, 2.3760752347891376, 2.369316410209394, 2.3762102255086712, 2.3637132009422017, 2.3622262248741035, 2.355431199487473, 2.3538211345764535, 2.3554496559674787, 2.3546480834196823, 2.351737952195589, 2.343474289551392, 2.3475432500964257, 2.34728677302214, 2.340270750849422, 2.3368291436120545, 2.3373288087861397, 2.330366124140635, 2.325685505074574, 2.3281749809365158, 2.322718191289341, 2.326040840181101, 2.3231656717063607, 2.3234655682190732, 2.3114975580756365], 'acc': [0.4093525706328407, 0.7340889662268627, 0.8365799847861717, 0.8657209548918939, 0.8681864589424015, 0.8688211163643447, 0.869226829070725, 0.8692395280086399, 0.8692396425640098, 0.869239643942949, 0.8692396453333793, 0.8692396459539019, 0.8692396445404893, 0.869239641713664, 0.8692396437131258, 0.8692396430236562, 0.869239643942949, 0.8692396429432181, 0.8692396431040943, 0.8692396471949472, 0.869239644528998, 0.8692391358613278, 0.8692348943019571, 0.8692245362166704, 0.8692139695208726, 0.8692425423006757, 0.8692508347925708, 0.8694241144465462, 0.8697105927896656, 0.8699908958985347, 0.8703011921864612, 0.8705289703309939, 0.8709126270435577, 0.8714239370714889, 0.8717018357931086, 0.8719734104814524, 0.8725669814691347, 0.8730273589431515, 0.8734986054338838, 0.8745528067585496, 0.8752973177356252, 0.8764065735391139, 0.8776997236564759, 0.8789106395999797, 0.8803561205622179, 0.8812522263064512, 0.8821775924270039, 0.8829446041172632, 0.884149190672611, 0.8848471463542465, 0.8856309838830011, 0.8861664883750745, 0.8859673917971527, 0.8858129739853282, 0.8863433092596996, 0.8868279701853984, 0.8876714712970859, 0.888353094076499, 0.8887392076909116, 0.8889500716069495, 0.8894166362315491, 0.8899519041781023, 0.8905118754173442, 0.8911753261574252, 0.8918405930293142, 0.8922013812771141, 0.8924192853899666, 0.8926691488192815, 0.8927696244850402, 0.8929436445603923, 0.892987878940366, 0.893168088694356, 0.89323340934068, 0.8937018040933199, 0.8941634550176422, 0.8944640890559595, 0.894828370489871, 0.8952739865926503, 0.8957124945628797, 0.896293202863623, 0.8967420392834384, 0.8970831054378985, 0.8977213704289557, 0.8979806200902265, 0.8983157106463519, 0.898983730811569, 0.8997659677137224, 0.8995376983565909, 0.9000003008408562, 0.9003118920790476, 0.900312772451257, 0.900985797947258, 0.901317250241382, 0.9018725860502671, 0.9019891172561293, 0.9023813677932662, 0.9024653673654626, 0.9027398397680282, 0.9033491389586421, 0.9034241049740617, 0.9037064063955829, 0.9036994265043737, 0.9043471957530869, 0.9043205750570008, 0.9045258999008744, 0.9050199929082556, 0.9055383221961089, 0.9056014205670803, 0.9056878278474714, 0.9061329588739312, 0.9063061452311819, 0.906410581761756, 0.9064743272343972, 0.9069979880755638, 0.9068884070102985, 0.9072228715740143, 0.9074233566057666, 0.9076886241873129, 0.9078422989692674, 0.9078432263288283, 0.9078648715711454, 0.9079017557917658, 0.9085005853409157, 0.9080888485549984, 0.9085150706025708, 0.9084629080119518, 0.9085541350638818, 0.9088210538331314, 0.9090999514123068, 0.9092149509040519, 0.9091786851261493, 0.9092455640311689, 0.909600947745301, 0.9094027583139904, 0.9094638189231589, 0.9096779701841666, 0.9097365968048584, 0.9099085782220971, 0.9097661890458666, 0.90978384440819, 0.9100746961832277, 0.9101138769350737, 0.9100696220433802, 0.91017764861905, 0.9102361800814494, 0.9103112117458636, 0.9103494905451026, 0.9105788494824949, 0.9105981703390155, 0.9106135593458984, 0.9106004873245053, 0.9109054303523179, 0.9107623655188858, 0.9110622335105405, 0.9111207636859299, 0.9109281614057954, 0.9110378104872342, 0.9111102652820935, 0.9110551212296818, 0.9111730629409097, 0.9111881507185352, 0.911441652363887, 0.9114731635021432, 0.9115530132488959, 0.9117711524593823, 0.9118126072197813, 0.9115142917642249, 0.9119013321981141, 0.9117236521999248, 0.9117631584963221, 0.9119372249499943, 0.9119876475607197], 'mDice': [0.018352299002908617, 0.017870582449130524, 0.017697330412720817, 0.017628916256820646, 0.017885652469682607, 0.01820846137427137, 0.018755810055181496, 0.0198679237348371, 0.021290391118716178, 0.02306698559845703, 0.02514814691381439, 0.027557806446731767, 0.03062213371547863, 0.03368878814370067, 0.037008670117926544, 0.040730663166039016, 0.0444332575186705, 0.04859388118072215, 0.052166845357174446, 0.055875766113851484, 0.05930921199787369, 0.06285150295275356, 0.0665669609683544, 0.069850758924658, 0.07409848955088046, 0.07813062301077593, 0.08232957153007063, 0.08767399992790208, 0.09370025391405848, 0.09965138560071579, 0.10514137183330595, 0.11007889302446361, 0.1149809963416566, 0.12013193089619109, 0.12534801678917692, 0.13104555962263606, 0.1371600004044116, 0.14172812988642255, 0.1486938120526157, 0.15500439349536793, 0.1601587880495451, 0.16679319173402127, 0.17282310639855855, 0.17885182134581595, 0.18650261501321816, 0.192769569380908, 0.19700097128194446, 0.20256332625640894, 0.20854971230213917, 0.2135840449916712, 0.2178987354982421, 0.22161737993339456, 0.2247419202600069, 0.22812675827611772, 0.2330025988252999, 0.2369886367955786, 0.23978395637391228, 0.24452074517030717, 0.2490619401068087, 0.25316878729802417, 0.25723831722427015, 0.26040926506713746, 0.26427141916712793, 0.26821850480683074, 0.27252636493366994, 0.2755465877522663, 0.2784494403264921, 0.28103212283391676, 0.28432456032949904, 0.2876446080479016, 0.29061254286136334, 0.29335328630912, 0.2960266027580182, 0.2994732556429032, 0.30232133701262687, 0.3038332112692261, 0.30622724859797706, 0.3087694445498904, 0.31114670417661355, 0.3132835413785897, 0.315489875217704, 0.3185156740985074, 0.32086209960013246, 0.32259799599509625, 0.32401843404916986, 0.3286296062750326, 0.33270322446283496, 0.3324508033427104, 0.3365152302690616, 0.33912003871699115, 0.3405766147327993, 0.34482387438522777, 0.3470754364632172, 0.3497555544950012, 0.35007743091281723, 0.3538287239415305, 0.35283964350087027, 0.35595698534281744, 0.35790101395812546, 0.3594978389044545, 0.362235522313869, 0.36145236502154293, 0.3648939169174861, 0.365987214987876, 0.36652686760470155, 0.3699754381687719, 0.3725611456952298, 0.37285796458238074, 0.3724323945949136, 0.3749276177488312, 0.37676644396501535, 0.3781244269691958, 0.37893877722106045, 0.38110220535701933, 0.38274410671527753, 0.38421760687645123, 0.3863036533311679, 0.38763618286528845, 0.387794857123281, 0.3887999527298556, 0.3886312858801438, 0.39040148379440964, 0.39288504724215745, 0.39243287581204217, 0.3952924137816899, 0.3944948796353084, 0.39637363618891525, 0.3959647935319779, 0.39887803160365154, 0.39862044793644325, 0.3996462288081175, 0.39921147859645806, 0.4020856629912739, 0.4012211922772339, 0.40175711226091004, 0.4032811687037231, 0.404814163866773, 0.40472524569906526, 0.4054757474313151, 0.4045472449883574, 0.40738951358947767, 0.40759029086440063, 0.4075000274611135, 0.40829704325277727, 0.4086201408820872, 0.4088503837424573, 0.4110798126462569, 0.4114342932529203, 0.41184633059011666, 0.41216876709279654, 0.4110838156319179, 0.41394019304430507, 0.41376510110806014, 0.414301694760967, 0.41645873275500905, 0.4149157709760061, 0.4156622587198096, 0.41618082618492747, 0.41684644504252577, 0.4174710232006784, 0.4166938775774369, 0.4184717501384205, 0.41820481162161316, 0.41902677687533174, 0.41995826417571985, 0.4214943106553218, 0.4202930106750361, 0.42171394838196524, 0.4210009567995441, 0.4213557999925115, 0.42212148405164795, 0.42350359799109466]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.34s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.08s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.87s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:32,  1.38s/it]predicting train subjects:   1%|          | 2/285 [00:03<06:51,  1.46s/it]predicting train subjects:   1%|          | 3/285 [00:04<06:42,  1.43s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:14,  1.55s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<06:55,  1.48s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:22,  1.59s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<07:45,  1.67s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<07:55,  1.72s/it]predicting train subjects:   3%|▎         | 9/285 [00:14<07:38,  1.66s/it]predicting train subjects:   4%|▎         | 10/285 [00:16<07:54,  1.72s/it]predicting train subjects:   4%|▍         | 11/285 [00:18<08:05,  1.77s/it]predicting train subjects:   4%|▍         | 12/285 [00:20<08:16,  1.82s/it]predicting train subjects:   5%|▍         | 13/285 [00:22<08:20,  1.84s/it]predicting train subjects:   5%|▍         | 14/285 [00:24<08:21,  1.85s/it]predicting train subjects:   5%|▌         | 15/285 [00:25<08:25,  1.87s/it]predicting train subjects:   6%|▌         | 16/285 [00:27<08:31,  1.90s/it]predicting train subjects:   6%|▌         | 17/285 [00:29<08:38,  1.94s/it]predicting train subjects:   6%|▋         | 18/285 [00:31<08:34,  1.93s/it]predicting train subjects:   7%|▋         | 19/285 [00:33<08:25,  1.90s/it]predicting train subjects:   7%|▋         | 20/285 [00:35<08:20,  1.89s/it]predicting train subjects:   7%|▋         | 21/285 [00:37<08:23,  1.91s/it]predicting train subjects:   8%|▊         | 22/285 [00:39<08:22,  1.91s/it]predicting train subjects:   8%|▊         | 23/285 [00:41<08:27,  1.94s/it]predicting train subjects:   8%|▊         | 24/285 [00:43<08:24,  1.93s/it]predicting train subjects:   9%|▉         | 25/285 [00:45<08:29,  1.96s/it]predicting train subjects:   9%|▉         | 26/285 [00:47<08:28,  1.96s/it]predicting train subjects:   9%|▉         | 27/285 [00:49<08:18,  1.93s/it]predicting train subjects:  10%|▉         | 28/285 [00:51<08:15,  1.93s/it]predicting train subjects:  10%|█         | 29/285 [00:52<08:04,  1.89s/it]predicting train subjects:  11%|█         | 30/285 [00:54<08:00,  1.88s/it]predicting train subjects:  11%|█         | 31/285 [00:56<07:54,  1.87s/it]predicting train subjects:  11%|█         | 32/285 [00:58<07:42,  1.83s/it]predicting train subjects:  12%|█▏        | 33/285 [01:00<07:38,  1.82s/it]predicting train subjects:  12%|█▏        | 34/285 [01:01<07:32,  1.80s/it]predicting train subjects:  12%|█▏        | 35/285 [01:03<07:30,  1.80s/it]predicting train subjects:  13%|█▎        | 36/285 [01:05<07:28,  1.80s/it]predicting train subjects:  13%|█▎        | 37/285 [01:07<07:20,  1.78s/it]predicting train subjects:  13%|█▎        | 38/285 [01:08<07:17,  1.77s/it]predicting train subjects:  14%|█▎        | 39/285 [01:10<07:20,  1.79s/it]predicting train subjects:  14%|█▍        | 40/285 [01:12<07:18,  1.79s/it]predicting train subjects:  14%|█▍        | 41/285 [01:14<07:14,  1.78s/it]predicting train subjects:  15%|█▍        | 42/285 [01:16<07:20,  1.81s/it]predicting train subjects:  15%|█▌        | 43/285 [01:17<07:12,  1.79s/it]predicting train subjects:  15%|█▌        | 44/285 [01:19<07:09,  1.78s/it]predicting train subjects:  16%|█▌        | 45/285 [01:21<07:09,  1.79s/it]predicting train subjects:  16%|█▌        | 46/285 [01:23<06:52,  1.72s/it]predicting train subjects:  16%|█▋        | 47/285 [01:24<06:33,  1.65s/it]predicting train subjects:  17%|█▋        | 48/285 [01:26<06:21,  1.61s/it]predicting train subjects:  17%|█▋        | 49/285 [01:27<06:16,  1.60s/it]predicting train subjects:  18%|█▊        | 50/285 [01:29<06:08,  1.57s/it]predicting train subjects:  18%|█▊        | 51/285 [01:30<06:03,  1.55s/it]predicting train subjects:  18%|█▊        | 52/285 [01:32<06:06,  1.57s/it]predicting train subjects:  19%|█▊        | 53/285 [01:33<06:09,  1.59s/it]predicting train subjects:  19%|█▉        | 54/285 [01:35<06:08,  1.59s/it]predicting train subjects:  19%|█▉        | 55/285 [01:37<06:02,  1.58s/it]predicting train subjects:  20%|█▉        | 56/285 [01:38<05:57,  1.56s/it]predicting train subjects:  20%|██        | 57/285 [01:40<05:51,  1.54s/it]predicting train subjects:  20%|██        | 58/285 [01:41<05:59,  1.58s/it]predicting train subjects:  21%|██        | 59/285 [01:43<05:50,  1.55s/it]predicting train subjects:  21%|██        | 60/285 [01:44<05:50,  1.56s/it]predicting train subjects:  21%|██▏       | 61/285 [01:46<05:46,  1.55s/it]predicting train subjects:  22%|██▏       | 62/285 [01:47<05:45,  1.55s/it]predicting train subjects:  22%|██▏       | 63/285 [01:49<05:40,  1.54s/it]predicting train subjects:  22%|██▏       | 64/285 [01:51<05:43,  1.55s/it]predicting train subjects:  23%|██▎       | 65/285 [01:52<06:02,  1.65s/it]predicting train subjects:  23%|██▎       | 66/285 [01:54<06:03,  1.66s/it]predicting train subjects:  24%|██▎       | 67/285 [01:56<06:03,  1.67s/it]predicting train subjects:  24%|██▍       | 68/285 [01:57<05:57,  1.65s/it]predicting train subjects:  24%|██▍       | 69/285 [01:59<05:53,  1.63s/it]predicting train subjects:  25%|██▍       | 70/285 [02:01<05:46,  1.61s/it]predicting train subjects:  25%|██▍       | 71/285 [02:02<05:45,  1.62s/it]predicting train subjects:  25%|██▌       | 72/285 [02:04<05:44,  1.62s/it]predicting train subjects:  26%|██▌       | 73/285 [02:05<05:43,  1.62s/it]predicting train subjects:  26%|██▌       | 74/285 [02:07<05:38,  1.61s/it]predicting train subjects:  26%|██▋       | 75/285 [02:09<05:38,  1.61s/it]predicting train subjects:  27%|██▋       | 76/285 [02:10<05:32,  1.59s/it]predicting train subjects:  27%|██▋       | 77/285 [02:12<05:30,  1.59s/it]predicting train subjects:  27%|██▋       | 78/285 [02:13<05:35,  1.62s/it]predicting train subjects:  28%|██▊       | 79/285 [02:15<05:33,  1.62s/it]predicting train subjects:  28%|██▊       | 80/285 [02:17<05:32,  1.62s/it]predicting train subjects:  28%|██▊       | 81/285 [02:18<05:31,  1.63s/it]predicting train subjects:  29%|██▉       | 82/285 [02:20<05:28,  1.62s/it]predicting train subjects:  29%|██▉       | 83/285 [02:22<05:26,  1.62s/it]predicting train subjects:  29%|██▉       | 84/285 [02:23<05:24,  1.61s/it]predicting train subjects:  30%|██▉       | 85/285 [02:25<05:32,  1.66s/it]predicting train subjects:  30%|███       | 86/285 [02:27<05:42,  1.72s/it]predicting train subjects:  31%|███       | 87/285 [02:29<05:47,  1.76s/it]predicting train subjects:  31%|███       | 88/285 [02:30<05:52,  1.79s/it]predicting train subjects:  31%|███       | 89/285 [02:32<05:51,  1.79s/it]predicting train subjects:  32%|███▏      | 90/285 [02:34<05:51,  1.81s/it]predicting train subjects:  32%|███▏      | 91/285 [02:36<05:47,  1.79s/it]predicting train subjects:  32%|███▏      | 92/285 [02:38<05:53,  1.83s/it]predicting train subjects:  33%|███▎      | 93/285 [02:40<05:52,  1.83s/it]predicting train subjects:  33%|███▎      | 94/285 [02:41<05:49,  1.83s/it]predicting train subjects:  33%|███▎      | 95/285 [02:43<05:50,  1.84s/it]predicting train subjects:  34%|███▎      | 96/285 [02:45<05:49,  1.85s/it]predicting train subjects:  34%|███▍      | 97/285 [02:47<05:42,  1.82s/it]predicting train subjects:  34%|███▍      | 98/285 [02:49<05:36,  1.80s/it]predicting train subjects:  35%|███▍      | 99/285 [02:50<05:32,  1.79s/it]predicting train subjects:  35%|███▌      | 100/285 [02:52<05:30,  1.78s/it]predicting train subjects:  35%|███▌      | 101/285 [02:54<05:42,  1.86s/it]predicting train subjects:  36%|███▌      | 102/285 [02:56<05:38,  1.85s/it]predicting train subjects:  36%|███▌      | 103/285 [02:58<05:29,  1.81s/it]predicting train subjects:  36%|███▋      | 104/285 [02:59<05:20,  1.77s/it]predicting train subjects:  37%|███▋      | 105/285 [03:01<05:18,  1.77s/it]predicting train subjects:  37%|███▋      | 106/285 [03:03<05:12,  1.75s/it]predicting train subjects:  38%|███▊      | 107/285 [03:05<05:10,  1.75s/it]predicting train subjects:  38%|███▊      | 108/285 [03:06<05:07,  1.73s/it]predicting train subjects:  38%|███▊      | 109/285 [03:08<05:07,  1.75s/it]predicting train subjects:  39%|███▊      | 110/285 [03:10<05:02,  1.73s/it]predicting train subjects:  39%|███▉      | 111/285 [03:12<04:59,  1.72s/it]predicting train subjects:  39%|███▉      | 112/285 [03:13<04:57,  1.72s/it]predicting train subjects:  40%|███▉      | 113/285 [03:15<05:01,  1.75s/it]predicting train subjects:  40%|████      | 114/285 [03:17<05:02,  1.77s/it]predicting train subjects:  40%|████      | 115/285 [03:19<05:05,  1.80s/it]predicting train subjects:  41%|████      | 116/285 [03:21<05:07,  1.82s/it]predicting train subjects:  41%|████      | 117/285 [03:23<05:15,  1.88s/it]predicting train subjects:  41%|████▏     | 118/285 [03:25<05:18,  1.91s/it]predicting train subjects:  42%|████▏     | 119/285 [03:26<05:06,  1.85s/it]predicting train subjects:  42%|████▏     | 120/285 [03:28<05:05,  1.85s/it]predicting train subjects:  42%|████▏     | 121/285 [03:30<04:50,  1.77s/it]predicting train subjects:  43%|████▎     | 122/285 [03:31<04:36,  1.70s/it]predicting train subjects:  43%|████▎     | 123/285 [03:33<04:22,  1.62s/it]predicting train subjects:  44%|████▎     | 124/285 [03:34<04:25,  1.65s/it]predicting train subjects:  44%|████▍     | 125/285 [03:36<04:24,  1.65s/it]predicting train subjects:  44%|████▍     | 126/285 [03:38<04:22,  1.65s/it]predicting train subjects:  45%|████▍     | 127/285 [03:39<04:14,  1.61s/it]predicting train subjects:  45%|████▍     | 128/285 [03:41<04:12,  1.61s/it]predicting train subjects:  45%|████▌     | 129/285 [03:43<04:10,  1.61s/it]predicting train subjects:  46%|████▌     | 130/285 [03:44<04:09,  1.61s/it]predicting train subjects:  46%|████▌     | 131/285 [03:46<04:06,  1.60s/it]predicting train subjects:  46%|████▋     | 132/285 [03:47<04:04,  1.60s/it]predicting train subjects:  47%|████▋     | 133/285 [03:49<04:01,  1.59s/it]predicting train subjects:  47%|████▋     | 134/285 [03:50<03:58,  1.58s/it]predicting train subjects:  47%|████▋     | 135/285 [03:52<03:59,  1.60s/it]predicting train subjects:  48%|████▊     | 136/285 [03:54<03:58,  1.60s/it]predicting train subjects:  48%|████▊     | 137/285 [03:55<03:55,  1.59s/it]predicting train subjects:  48%|████▊     | 138/285 [03:57<03:51,  1.58s/it]predicting train subjects:  49%|████▉     | 139/285 [03:58<03:52,  1.59s/it]predicting train subjects:  49%|████▉     | 140/285 [04:00<03:50,  1.59s/it]predicting train subjects:  49%|████▉     | 141/285 [04:02<03:52,  1.61s/it]predicting train subjects:  50%|████▉     | 142/285 [04:03<03:51,  1.62s/it]predicting train subjects:  50%|█████     | 143/285 [04:05<03:41,  1.56s/it]predicting train subjects:  51%|█████     | 144/285 [04:06<03:34,  1.52s/it]predicting train subjects:  51%|█████     | 145/285 [04:08<03:29,  1.49s/it]predicting train subjects:  51%|█████     | 146/285 [04:09<03:26,  1.48s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:10<03:23,  1.47s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:12<03:20,  1.47s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:13<03:18,  1.46s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:15<03:16,  1.45s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:16<03:13,  1.45s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:18<03:11,  1.44s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:19<03:08,  1.43s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:21<03:11,  1.46s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:22<03:10,  1.47s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:24<03:08,  1.46s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:25<03:04,  1.44s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:26<03:06,  1.47s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:28<03:04,  1.46s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:29<03:02,  1.46s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:31<02:59,  1.44s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:32<03:00,  1.47s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:34<03:00,  1.48s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:35<02:57,  1.47s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:37<02:56,  1.47s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:38<02:52,  1.45s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:40<02:50,  1.45s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:41<02:48,  1.44s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:42<02:45,  1.42s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:44<02:42,  1.41s/it]predicting train subjects:  60%|██████    | 171/285 [04:45<02:43,  1.43s/it]predicting train subjects:  60%|██████    | 172/285 [04:47<02:39,  1.41s/it]predicting train subjects:  61%|██████    | 173/285 [04:48<02:39,  1.42s/it]predicting train subjects:  61%|██████    | 174/285 [04:49<02:36,  1.41s/it]predicting train subjects:  61%|██████▏   | 175/285 [04:51<02:36,  1.42s/it]predicting train subjects:  62%|██████▏   | 176/285 [04:52<02:34,  1.42s/it]predicting train subjects:  62%|██████▏   | 177/285 [04:54<02:35,  1.44s/it]predicting train subjects:  62%|██████▏   | 178/285 [04:55<02:32,  1.42s/it]predicting train subjects:  63%|██████▎   | 179/285 [04:57<02:31,  1.43s/it]predicting train subjects:  63%|██████▎   | 180/285 [04:58<02:26,  1.40s/it]predicting train subjects:  64%|██████▎   | 181/285 [04:59<02:22,  1.37s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:01<02:20,  1.36s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:02<02:22,  1.40s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:03<02:20,  1.39s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:05<02:20,  1.40s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:07<02:25,  1.47s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:08<02:29,  1.53s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:10<02:26,  1.51s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:11<02:23,  1.49s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:13<02:20,  1.48s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:14<02:16,  1.46s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:15<02:17,  1.48s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:17<02:15,  1.47s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:18<02:10,  1.43s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:20<02:08,  1.43s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:21<02:14,  1.51s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:23<02:20,  1.59s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:25<02:20,  1.62s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:27<02:21,  1.64s/it]predicting train subjects:  70%|███████   | 200/285 [05:28<02:19,  1.64s/it]predicting train subjects:  71%|███████   | 201/285 [05:30<02:17,  1.64s/it]predicting train subjects:  71%|███████   | 202/285 [05:32<02:18,  1.67s/it]predicting train subjects:  71%|███████   | 203/285 [05:33<02:17,  1.68s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:35<02:16,  1.68s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:37<02:15,  1.69s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:38<02:11,  1.67s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:40<02:12,  1.69s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:42<02:11,  1.71s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:43<02:09,  1.70s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:45<02:07,  1.71s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:47<02:05,  1.70s/it]predicting train subjects:  74%|███████▍  | 212/285 [05:49<02:03,  1.69s/it]predicting train subjects:  75%|███████▍  | 213/285 [05:50<02:00,  1.67s/it]predicting train subjects:  75%|███████▌  | 214/285 [05:52<01:54,  1.61s/it]predicting train subjects:  75%|███████▌  | 215/285 [05:53<01:50,  1.58s/it]predicting train subjects:  76%|███████▌  | 216/285 [05:55<01:49,  1.59s/it]predicting train subjects:  76%|███████▌  | 217/285 [05:56<01:49,  1.61s/it]predicting train subjects:  76%|███████▋  | 218/285 [05:58<01:47,  1.61s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:00<01:44,  1.59s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:01<01:41,  1.56s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:03<01:44,  1.63s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:05<01:43,  1.64s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:06<01:40,  1.62s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:08<01:38,  1.61s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:09<01:35,  1.59s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:11<01:32,  1.56s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:12<01:30,  1.56s/it]predicting train subjects:  80%|████████  | 228/285 [06:14<01:28,  1.55s/it]predicting train subjects:  80%|████████  | 229/285 [06:15<01:26,  1.55s/it]predicting train subjects:  81%|████████  | 230/285 [06:17<01:25,  1.55s/it]predicting train subjects:  81%|████████  | 231/285 [06:18<01:23,  1.55s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:20<01:29,  1.68s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:22<01:32,  1.78s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:24<01:33,  1.84s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:26<01:33,  1.87s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:28<01:33,  1.92s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:30<01:33,  1.94s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:32<01:30,  1.93s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:34<01:29,  1.94s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:36<01:27,  1.94s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:38<01:23,  1.90s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:40<01:22,  1.91s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:42<01:19,  1.89s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:44<01:16,  1.88s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:46<01:15,  1.89s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:47<01:13,  1.88s/it]predicting train subjects:  87%|████████▋ | 247/285 [06:49<01:12,  1.90s/it]predicting train subjects:  87%|████████▋ | 248/285 [06:51<01:09,  1.89s/it]predicting train subjects:  87%|████████▋ | 249/285 [06:53<01:07,  1.87s/it]predicting train subjects:  88%|████████▊ | 250/285 [06:55<01:01,  1.75s/it]predicting train subjects:  88%|████████▊ | 251/285 [06:56<00:55,  1.64s/it]predicting train subjects:  88%|████████▊ | 252/285 [06:57<00:52,  1.59s/it]predicting train subjects:  89%|████████▉ | 253/285 [06:59<00:48,  1.52s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:00<00:45,  1.48s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:01<00:43,  1.45s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:03<00:41,  1.45s/it]predicting train subjects:  90%|█████████ | 257/285 [07:04<00:40,  1.44s/it]predicting train subjects:  91%|█████████ | 258/285 [07:06<00:38,  1.43s/it]predicting train subjects:  91%|█████████ | 259/285 [07:07<00:36,  1.42s/it]predicting train subjects:  91%|█████████ | 260/285 [07:09<00:35,  1.40s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:10<00:33,  1.41s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:11<00:32,  1.40s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:13<00:31,  1.41s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:14<00:29,  1.40s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:15<00:27,  1.39s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:17<00:26,  1.40s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:18<00:25,  1.41s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:20<00:26,  1.55s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:22<00:26,  1.63s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:24<00:25,  1.70s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:26<00:24,  1.75s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:28<00:23,  1.78s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:29<00:21,  1.81s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:31<00:19,  1.82s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:33<00:18,  1.84s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:35<00:16,  1.87s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:37<00:14,  1.87s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:39<00:13,  1.87s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:41<00:11,  1.88s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:43<00:09,  1.85s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:45<00:07,  1.89s/it]predicting train subjects:  99%|█████████▉| 282/285 [07:46<00:05,  1.87s/it]predicting train subjects:  99%|█████████▉| 283/285 [07:48<00:03,  1.86s/it]predicting train subjects: 100%|█████████▉| 284/285 [07:50<00:01,  1.87s/it]predicting train subjects: 100%|██████████| 285/285 [07:52<00:00,  1.86s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:28,  1.37s/it]Loading train:   1%|          | 2/285 [00:02<06:47,  1.44s/it]Loading train:   1%|          | 3/285 [00:04<06:40,  1.42s/it]Loading train:   1%|▏         | 4/285 [00:06<07:14,  1.55s/it]Loading train:   2%|▏         | 5/285 [00:07<06:55,  1.48s/it]Loading train:   2%|▏         | 6/285 [00:09<07:07,  1.53s/it]Loading train:   2%|▏         | 7/285 [00:11<07:33,  1.63s/it]Loading train:   3%|▎         | 8/285 [00:12<07:40,  1.66s/it]Loading train:   3%|▎         | 9/285 [00:14<07:28,  1.62s/it]Loading train:   4%|▎         | 10/285 [00:15<07:04,  1.54s/it]Loading train:   4%|▍         | 11/285 [00:16<06:34,  1.44s/it]Loading train:   4%|▍         | 12/285 [00:18<06:13,  1.37s/it]Loading train:   5%|▍         | 13/285 [00:19<06:21,  1.40s/it]Loading train:   5%|▍         | 14/285 [00:20<06:15,  1.39s/it]Loading train:   5%|▌         | 15/285 [00:22<06:08,  1.36s/it]Loading train:   6%|▌         | 16/285 [00:23<05:50,  1.30s/it]Loading train:   6%|▌         | 17/285 [00:24<05:43,  1.28s/it]Loading train:   6%|▋         | 18/285 [00:25<05:43,  1.29s/it]Loading train:   7%|▋         | 19/285 [00:27<05:45,  1.30s/it]Loading train:   7%|▋         | 20/285 [00:28<05:37,  1.27s/it]Loading train:   7%|▋         | 21/285 [00:29<05:27,  1.24s/it]Loading train:   8%|▊         | 22/285 [00:31<05:45,  1.32s/it]Loading train:   8%|▊         | 23/285 [00:32<05:51,  1.34s/it]Loading train:   8%|▊         | 24/285 [00:33<05:48,  1.34s/it]Loading train:   9%|▉         | 25/285 [00:35<05:39,  1.31s/it]Loading train:   9%|▉         | 26/285 [00:36<05:38,  1.31s/it]Loading train:   9%|▉         | 27/285 [00:37<05:37,  1.31s/it]Loading train:  10%|▉         | 28/285 [00:39<05:45,  1.34s/it]Loading train:  10%|█         | 29/285 [00:40<05:20,  1.25s/it]Loading train:  11%|█         | 30/285 [00:41<05:13,  1.23s/it]Loading train:  11%|█         | 31/285 [00:42<05:10,  1.22s/it]Loading train:  11%|█         | 32/285 [00:43<04:53,  1.16s/it]Loading train:  12%|█▏        | 33/285 [00:44<04:48,  1.14s/it]Loading train:  12%|█▏        | 34/285 [00:45<04:55,  1.18s/it]Loading train:  12%|█▏        | 35/285 [00:46<04:44,  1.14s/it]Loading train:  13%|█▎        | 36/285 [00:47<04:36,  1.11s/it]Loading train:  13%|█▎        | 37/285 [00:49<04:48,  1.16s/it]Loading train:  13%|█▎        | 38/285 [00:50<05:18,  1.29s/it]Loading train:  14%|█▎        | 39/285 [00:52<05:12,  1.27s/it]Loading train:  14%|█▍        | 40/285 [00:53<05:25,  1.33s/it]Loading train:  14%|█▍        | 41/285 [00:54<05:26,  1.34s/it]Loading train:  15%|█▍        | 42/285 [00:56<05:26,  1.34s/it]Loading train:  15%|█▌        | 43/285 [00:57<05:28,  1.36s/it]Loading train:  15%|█▌        | 44/285 [00:59<05:59,  1.49s/it]Loading train:  16%|█▌        | 45/285 [01:00<05:58,  1.49s/it]Loading train:  16%|█▌        | 46/285 [01:02<06:09,  1.54s/it]Loading train:  16%|█▋        | 47/285 [01:04<06:07,  1.55s/it]Loading train:  17%|█▋        | 48/285 [01:05<06:05,  1.54s/it]Loading train:  17%|█▋        | 49/285 [01:07<05:51,  1.49s/it]Loading train:  18%|█▊        | 50/285 [01:08<05:35,  1.43s/it]Loading train:  18%|█▊        | 51/285 [01:09<05:23,  1.38s/it]Loading train:  18%|█▊        | 52/285 [01:11<05:20,  1.38s/it]Loading train:  19%|█▊        | 53/285 [01:12<05:21,  1.38s/it]Loading train:  19%|█▉        | 54/285 [01:13<05:22,  1.39s/it]Loading train:  19%|█▉        | 55/285 [01:15<05:22,  1.40s/it]Loading train:  20%|█▉        | 56/285 [01:16<05:11,  1.36s/it]Loading train:  20%|██        | 57/285 [01:17<05:13,  1.37s/it]Loading train:  20%|██        | 58/285 [01:19<05:20,  1.41s/it]Loading train:  21%|██        | 59/285 [01:21<05:32,  1.47s/it]Loading train:  21%|██        | 60/285 [01:22<05:26,  1.45s/it]Loading train:  21%|██▏       | 61/285 [01:23<05:20,  1.43s/it]Loading train:  22%|██▏       | 62/285 [01:25<05:11,  1.40s/it]Loading train:  22%|██▏       | 63/285 [01:26<05:13,  1.41s/it]Loading train:  22%|██▏       | 64/285 [01:28<05:28,  1.49s/it]Loading train:  23%|██▎       | 65/285 [01:29<05:31,  1.51s/it]Loading train:  23%|██▎       | 66/285 [01:31<05:29,  1.51s/it]Loading train:  24%|██▎       | 67/285 [01:32<04:58,  1.37s/it]Loading train:  24%|██▍       | 68/285 [01:33<04:31,  1.25s/it]Loading train:  24%|██▍       | 69/285 [01:34<04:15,  1.18s/it]Loading train:  25%|██▍       | 70/285 [01:35<04:01,  1.12s/it]Loading train:  25%|██▍       | 71/285 [01:36<03:50,  1.08s/it]Loading train:  25%|██▌       | 72/285 [01:37<03:43,  1.05s/it]Loading train:  26%|██▌       | 73/285 [01:38<03:41,  1.04s/it]Loading train:  26%|██▌       | 74/285 [01:39<03:41,  1.05s/it]Loading train:  26%|██▋       | 75/285 [01:40<03:44,  1.07s/it]Loading train:  27%|██▋       | 76/285 [01:41<03:46,  1.08s/it]Loading train:  27%|██▋       | 77/285 [01:42<03:48,  1.10s/it]Loading train:  27%|██▋       | 78/285 [01:43<03:41,  1.07s/it]Loading train:  28%|██▊       | 79/285 [01:44<03:42,  1.08s/it]Loading train:  28%|██▊       | 80/285 [01:45<03:36,  1.06s/it]Loading train:  28%|██▊       | 81/285 [01:46<03:31,  1.04s/it]Loading train:  29%|██▉       | 82/285 [01:47<03:28,  1.03s/it]Loading train:  29%|██▉       | 83/285 [01:48<03:26,  1.02s/it]Loading train:  29%|██▉       | 84/285 [01:49<03:27,  1.03s/it]Loading train:  30%|██▉       | 85/285 [01:50<03:28,  1.04s/it]Loading train:  30%|███       | 86/285 [01:52<03:41,  1.11s/it]Loading train:  31%|███       | 87/285 [01:53<03:39,  1.11s/it]Loading train:  31%|███       | 88/285 [01:54<03:36,  1.10s/it]Loading train:  31%|███       | 89/285 [01:55<03:36,  1.10s/it]Loading train:  32%|███▏      | 90/285 [01:56<03:33,  1.10s/it]Loading train:  32%|███▏      | 91/285 [01:57<03:41,  1.14s/it]Loading train:  32%|███▏      | 92/285 [01:59<03:40,  1.14s/it]Loading train:  33%|███▎      | 93/285 [02:00<03:33,  1.11s/it]Loading train:  33%|███▎      | 94/285 [02:01<03:42,  1.17s/it]Loading train:  33%|███▎      | 95/285 [02:02<03:44,  1.18s/it]Loading train:  34%|███▎      | 96/285 [02:03<03:35,  1.14s/it]Loading train:  34%|███▍      | 97/285 [02:04<03:42,  1.19s/it]Loading train:  34%|███▍      | 98/285 [02:06<03:38,  1.17s/it]Loading train:  35%|███▍      | 99/285 [02:07<03:33,  1.15s/it]Loading train:  35%|███▌      | 100/285 [02:08<03:37,  1.17s/it]Loading train:  35%|███▌      | 101/285 [02:09<03:31,  1.15s/it]Loading train:  36%|███▌      | 102/285 [02:10<03:32,  1.16s/it]Loading train:  36%|███▌      | 103/285 [02:11<03:36,  1.19s/it]Loading train:  36%|███▋      | 104/285 [02:13<03:31,  1.17s/it]Loading train:  37%|███▋      | 105/285 [02:14<03:23,  1.13s/it]Loading train:  37%|███▋      | 106/285 [02:15<03:27,  1.16s/it]Loading train:  38%|███▊      | 107/285 [02:16<03:22,  1.14s/it]Loading train:  38%|███▊      | 108/285 [02:17<03:13,  1.09s/it]Loading train:  38%|███▊      | 109/285 [02:18<03:15,  1.11s/it]Loading train:  39%|███▊      | 110/285 [02:19<03:13,  1.11s/it]Loading train:  39%|███▉      | 111/285 [02:20<03:12,  1.10s/it]Loading train:  39%|███▉      | 112/285 [02:21<03:16,  1.13s/it]Loading train:  40%|███▉      | 113/285 [02:23<03:20,  1.17s/it]Loading train:  40%|████      | 114/285 [02:24<03:13,  1.13s/it]Loading train:  40%|████      | 115/285 [02:25<03:09,  1.12s/it]Loading train:  41%|████      | 116/285 [02:26<03:07,  1.11s/it]Loading train:  41%|████      | 117/285 [02:27<03:00,  1.08s/it]Loading train:  41%|████▏     | 118/285 [02:28<02:53,  1.04s/it]Loading train:  42%|████▏     | 119/285 [02:29<02:53,  1.05s/it]Loading train:  42%|████▏     | 120/285 [02:30<02:54,  1.06s/it]Loading train:  42%|████▏     | 121/285 [02:31<03:12,  1.17s/it]Loading train:  43%|████▎     | 122/285 [02:33<03:13,  1.19s/it]Loading train:  43%|████▎     | 123/285 [02:34<03:20,  1.24s/it]Loading train:  44%|████▎     | 124/285 [02:35<03:04,  1.15s/it]Loading train:  44%|████▍     | 125/285 [02:36<02:57,  1.11s/it]Loading train:  44%|████▍     | 126/285 [02:37<02:49,  1.07s/it]Loading train:  45%|████▍     | 127/285 [02:38<02:48,  1.07s/it]Loading train:  45%|████▍     | 128/285 [02:39<02:42,  1.03s/it]Loading train:  45%|████▌     | 129/285 [02:40<02:45,  1.06s/it]Loading train:  46%|████▌     | 130/285 [02:41<02:40,  1.04s/it]Loading train:  46%|████▌     | 131/285 [02:42<02:43,  1.06s/it]Loading train:  46%|████▋     | 132/285 [02:43<02:43,  1.07s/it]Loading train:  47%|████▋     | 133/285 [02:44<02:38,  1.04s/it]Loading train:  47%|████▋     | 134/285 [02:45<02:33,  1.02s/it]Loading train:  47%|████▋     | 135/285 [02:46<02:38,  1.06s/it]Loading train:  48%|████▊     | 136/285 [02:47<02:33,  1.03s/it]Loading train:  48%|████▊     | 137/285 [02:48<02:30,  1.02s/it]Loading train:  48%|████▊     | 138/285 [02:49<02:27,  1.01s/it]Loading train:  49%|████▉     | 139/285 [02:50<02:27,  1.01s/it]Loading train:  49%|████▉     | 140/285 [02:51<02:26,  1.01s/it]Loading train:  49%|████▉     | 141/285 [02:52<02:21,  1.01it/s]Loading train:  50%|████▉     | 142/285 [02:53<02:21,  1.01it/s]Loading train:  50%|█████     | 143/285 [02:54<02:19,  1.02it/s]Loading train:  51%|█████     | 144/285 [02:55<02:19,  1.01it/s]Loading train:  51%|█████     | 145/285 [02:56<02:15,  1.03it/s]Loading train:  51%|█████     | 146/285 [02:57<02:15,  1.03it/s]Loading train:  52%|█████▏    | 147/285 [02:58<02:14,  1.02it/s]Loading train:  52%|█████▏    | 148/285 [02:59<02:09,  1.06it/s]Loading train:  52%|█████▏    | 149/285 [03:00<02:15,  1.01it/s]Loading train:  53%|█████▎    | 150/285 [03:01<02:06,  1.07it/s]Loading train:  53%|█████▎    | 151/285 [03:02<02:09,  1.04it/s]Loading train:  53%|█████▎    | 152/285 [03:03<02:06,  1.05it/s]Loading train:  54%|█████▎    | 153/285 [03:04<02:00,  1.09it/s]Loading train:  54%|█████▍    | 154/285 [03:05<01:59,  1.10it/s]Loading train:  54%|█████▍    | 155/285 [03:06<02:03,  1.05it/s]Loading train:  55%|█████▍    | 156/285 [03:07<02:09,  1.00s/it]Loading train:  55%|█████▌    | 157/285 [03:08<02:02,  1.04it/s]Loading train:  55%|█████▌    | 158/285 [03:09<02:03,  1.03it/s]Loading train:  56%|█████▌    | 159/285 [03:10<02:01,  1.04it/s]Loading train:  56%|█████▌    | 160/285 [03:10<01:59,  1.05it/s]Loading train:  56%|█████▋    | 161/285 [03:11<01:59,  1.03it/s]Loading train:  57%|█████▋    | 162/285 [03:12<01:54,  1.08it/s]Loading train:  57%|█████▋    | 163/285 [03:13<01:54,  1.07it/s]Loading train:  58%|█████▊    | 164/285 [03:14<01:53,  1.06it/s]Loading train:  58%|█████▊    | 165/285 [03:15<01:49,  1.09it/s]Loading train:  58%|█████▊    | 166/285 [03:16<01:49,  1.08it/s]Loading train:  59%|█████▊    | 167/285 [03:17<01:47,  1.10it/s]Loading train:  59%|█████▉    | 168/285 [03:18<01:50,  1.06it/s]Loading train:  59%|█████▉    | 169/285 [03:19<01:48,  1.07it/s]Loading train:  60%|█████▉    | 170/285 [03:20<01:49,  1.05it/s]Loading train:  60%|██████    | 171/285 [03:21<01:48,  1.05it/s]Loading train:  60%|██████    | 172/285 [03:22<01:45,  1.07it/s]Loading train:  61%|██████    | 173/285 [03:23<01:46,  1.05it/s]Loading train:  61%|██████    | 174/285 [03:24<01:45,  1.05it/s]Loading train:  61%|██████▏   | 175/285 [03:25<01:46,  1.04it/s]Loading train:  62%|██████▏   | 176/285 [03:25<01:42,  1.07it/s]Loading train:  62%|██████▏   | 177/285 [03:26<01:36,  1.12it/s]Loading train:  62%|██████▏   | 178/285 [03:27<01:34,  1.13it/s]Loading train:  63%|██████▎   | 179/285 [03:28<01:33,  1.13it/s]Loading train:  63%|██████▎   | 180/285 [03:29<01:37,  1.08it/s]Loading train:  64%|██████▎   | 181/285 [03:30<01:35,  1.09it/s]Loading train:  64%|██████▍   | 182/285 [03:31<01:37,  1.06it/s]Loading train:  64%|██████▍   | 183/285 [03:32<01:35,  1.07it/s]Loading train:  65%|██████▍   | 184/285 [03:33<01:31,  1.10it/s]Loading train:  65%|██████▍   | 185/285 [03:34<01:29,  1.12it/s]Loading train:  65%|██████▌   | 186/285 [03:34<01:28,  1.11it/s]Loading train:  66%|██████▌   | 187/285 [03:35<01:29,  1.09it/s]Loading train:  66%|██████▌   | 188/285 [03:36<01:27,  1.11it/s]Loading train:  66%|██████▋   | 189/285 [03:37<01:25,  1.12it/s]Loading train:  67%|██████▋   | 190/285 [03:38<01:25,  1.11it/s]Loading train:  67%|██████▋   | 191/285 [03:39<01:26,  1.08it/s]Loading train:  67%|██████▋   | 192/285 [03:40<01:25,  1.09it/s]Loading train:  68%|██████▊   | 193/285 [03:41<01:24,  1.09it/s]Loading train:  68%|██████▊   | 194/285 [03:42<01:26,  1.05it/s]Loading train:  68%|██████▊   | 195/285 [03:43<01:23,  1.07it/s]Loading train:  69%|██████▉   | 196/285 [03:44<01:26,  1.03it/s]Loading train:  69%|██████▉   | 197/285 [03:45<01:24,  1.04it/s]Loading train:  69%|██████▉   | 198/285 [03:46<01:23,  1.04it/s]Loading train:  70%|██████▉   | 199/285 [03:47<01:24,  1.02it/s]Loading train:  70%|███████   | 200/285 [03:48<01:21,  1.04it/s]Loading train:  71%|███████   | 201/285 [03:49<01:22,  1.02it/s]Loading train:  71%|███████   | 202/285 [03:50<01:19,  1.04it/s]Loading train:  71%|███████   | 203/285 [03:51<01:20,  1.01it/s]Loading train:  72%|███████▏  | 204/285 [03:52<01:19,  1.01it/s]Loading train:  72%|███████▏  | 205/285 [03:53<01:25,  1.07s/it]Loading train:  72%|███████▏  | 206/285 [03:54<01:20,  1.01s/it]Loading train:  73%|███████▎  | 207/285 [03:55<01:16,  1.02it/s]Loading train:  73%|███████▎  | 208/285 [03:56<01:15,  1.01it/s]Loading train:  73%|███████▎  | 209/285 [03:57<01:18,  1.03s/it]Loading train:  74%|███████▎  | 210/285 [03:58<01:18,  1.04s/it]Loading train:  74%|███████▍  | 211/285 [03:59<01:16,  1.03s/it]Loading train:  74%|███████▍  | 212/285 [04:00<01:13,  1.01s/it]Loading train:  75%|███████▍  | 213/285 [04:01<01:10,  1.03it/s]Loading train:  75%|███████▌  | 214/285 [04:02<01:10,  1.01it/s]Loading train:  75%|███████▌  | 215/285 [04:03<01:06,  1.05it/s]Loading train:  76%|███████▌  | 216/285 [04:04<01:05,  1.05it/s]Loading train:  76%|███████▌  | 217/285 [04:05<01:03,  1.07it/s]Loading train:  76%|███████▋  | 218/285 [04:05<01:03,  1.06it/s]Loading train:  77%|███████▋  | 219/285 [04:06<01:01,  1.08it/s]Loading train:  77%|███████▋  | 220/285 [04:07<00:58,  1.11it/s]Loading train:  78%|███████▊  | 221/285 [04:08<00:58,  1.10it/s]Loading train:  78%|███████▊  | 222/285 [04:09<00:55,  1.13it/s]Loading train:  78%|███████▊  | 223/285 [04:10<00:53,  1.15it/s]Loading train:  79%|███████▊  | 224/285 [04:11<00:52,  1.17it/s]Loading train:  79%|███████▉  | 225/285 [04:11<00:49,  1.20it/s]Loading train:  79%|███████▉  | 226/285 [04:13<01:03,  1.08s/it]Loading train:  80%|███████▉  | 227/285 [04:14<01:00,  1.04s/it]Loading train:  80%|████████  | 228/285 [04:15<01:03,  1.12s/it]Loading train:  80%|████████  | 229/285 [04:17<01:08,  1.22s/it]Loading train:  81%|████████  | 230/285 [04:18<01:05,  1.18s/it]Loading train:  81%|████████  | 231/285 [04:19<01:06,  1.23s/it]Loading train:  81%|████████▏ | 232/285 [04:21<01:15,  1.42s/it]Loading train:  82%|████████▏ | 233/285 [04:23<01:17,  1.49s/it]Loading train:  82%|████████▏ | 234/285 [04:24<01:10,  1.38s/it]Loading train:  82%|████████▏ | 235/285 [04:25<01:11,  1.44s/it]Loading train:  83%|████████▎ | 236/285 [04:26<01:05,  1.33s/it]Loading train:  83%|████████▎ | 237/285 [04:28<01:13,  1.53s/it]Loading train:  84%|████████▎ | 238/285 [04:30<01:08,  1.46s/it]Loading train:  84%|████████▍ | 239/285 [04:32<01:13,  1.60s/it]Loading train:  84%|████████▍ | 240/285 [04:33<01:12,  1.61s/it]Loading train:  85%|████████▍ | 241/285 [04:35<01:12,  1.64s/it]Loading train:  85%|████████▍ | 242/285 [04:37<01:08,  1.59s/it]Loading train:  85%|████████▌ | 243/285 [04:38<01:00,  1.45s/it]Loading train:  86%|████████▌ | 244/285 [04:39<00:59,  1.46s/it]Loading train:  86%|████████▌ | 245/285 [04:41<00:58,  1.46s/it]Loading train:  86%|████████▋ | 246/285 [04:42<00:59,  1.53s/it]Loading train:  87%|████████▋ | 247/285 [04:44<00:59,  1.56s/it]Loading train:  87%|████████▋ | 248/285 [04:46<01:06,  1.79s/it]Loading train:  87%|████████▋ | 249/285 [04:48<01:08,  1.89s/it]Loading train:  88%|████████▊ | 250/285 [04:50<01:01,  1.76s/it]Loading train:  88%|████████▊ | 251/285 [04:51<00:53,  1.58s/it]Loading train:  88%|████████▊ | 252/285 [04:52<00:48,  1.46s/it]Loading train:  89%|████████▉ | 253/285 [04:53<00:44,  1.40s/it]Loading train:  89%|████████▉ | 254/285 [04:55<00:44,  1.44s/it]Loading train:  89%|████████▉ | 255/285 [04:56<00:40,  1.34s/it]Loading train:  90%|████████▉ | 256/285 [04:57<00:36,  1.27s/it]Loading train:  90%|█████████ | 257/285 [04:59<00:41,  1.49s/it]Loading train:  91%|█████████ | 258/285 [05:01<00:41,  1.52s/it]Loading train:  91%|█████████ | 259/285 [05:03<00:42,  1.65s/it]Loading train:  91%|█████████ | 260/285 [05:05<00:42,  1.72s/it]Loading train:  92%|█████████▏| 261/285 [05:06<00:40,  1.69s/it]Loading train:  92%|█████████▏| 262/285 [05:08<00:39,  1.71s/it]Loading train:  92%|█████████▏| 263/285 [05:10<00:37,  1.70s/it]Loading train:  93%|█████████▎| 264/285 [05:11<00:34,  1.66s/it]Loading train:  93%|█████████▎| 265/285 [05:13<00:32,  1.60s/it]Loading train:  93%|█████████▎| 266/285 [05:14<00:30,  1.60s/it]Loading train:  94%|█████████▎| 267/285 [05:16<00:30,  1.70s/it]Loading train:  94%|█████████▍| 268/285 [05:19<00:32,  1.91s/it]Loading train:  94%|█████████▍| 269/285 [05:21<00:32,  2.00s/it]Loading train:  95%|█████████▍| 270/285 [05:23<00:30,  2.04s/it]Loading train:  95%|█████████▌| 271/285 [05:26<00:31,  2.24s/it]Loading train:  95%|█████████▌| 272/285 [05:28<00:29,  2.29s/it]Loading train:  96%|█████████▌| 273/285 [05:31<00:28,  2.37s/it]Loading train:  96%|█████████▌| 274/285 [05:33<00:26,  2.42s/it]Loading train:  96%|█████████▋| 275/285 [05:35<00:23,  2.34s/it]Loading train:  97%|█████████▋| 276/285 [05:38<00:21,  2.36s/it]Loading train:  97%|█████████▋| 277/285 [05:40<00:19,  2.38s/it]Loading train:  98%|█████████▊| 278/285 [05:42<00:16,  2.37s/it]Loading train:  98%|█████████▊| 279/285 [05:44<00:13,  2.25s/it]Loading train:  98%|█████████▊| 280/285 [05:46<00:09,  1.97s/it]Loading train:  99%|█████████▊| 281/285 [05:47<00:07,  1.79s/it]Loading train:  99%|█████████▉| 282/285 [05:49<00:05,  1.70s/it]Loading train:  99%|█████████▉| 283/285 [05:50<00:03,  1.71s/it]Loading train: 100%|█████████▉| 284/285 [05:52<00:01,  1.74s/it]Loading train: 100%|██████████| 285/285 [05:54<00:00,  1.73s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   3%|▎         | 8/285 [00:00<00:03, 76.25it/s]concatenating: train:   9%|▉         | 27/285 [00:00<00:02, 91.87it/s]concatenating: train:  12%|█▏        | 34/285 [00:00<00:03, 77.18it/s]concatenating: train:  14%|█▍        | 41/285 [00:00<00:05, 43.28it/s]concatenating: train:  16%|█▋        | 47/285 [00:00<00:07, 31.08it/s]concatenating: train:  18%|█▊        | 52/285 [00:01<00:08, 26.40it/s]concatenating: train:  20%|██        | 58/285 [00:01<00:07, 31.11it/s]concatenating: train:  22%|██▏       | 63/285 [00:01<00:08, 27.45it/s]concatenating: train:  24%|██▍       | 68/285 [00:01<00:07, 30.62it/s]concatenating: train:  29%|██▉       | 84/285 [00:01<00:04, 40.34it/s]concatenating: train:  39%|███▊      | 110/285 [00:01<00:03, 53.96it/s]concatenating: train:  48%|████▊     | 138/285 [00:02<00:02, 71.11it/s]concatenating: train:  57%|█████▋    | 162/285 [00:02<00:01, 89.82it/s]concatenating: train:  64%|██████▎   | 181/285 [00:02<00:01, 82.44it/s]concatenating: train:  69%|██████▉   | 197/285 [00:02<00:01, 58.17it/s]concatenating: train:  73%|███████▎  | 209/285 [00:03<00:01, 42.49it/s]concatenating: train:  77%|███████▋  | 219/285 [00:03<00:02, 32.66it/s]concatenating: train:  79%|███████▉  | 226/285 [00:03<00:01, 34.51it/s]concatenating: train:  82%|████████▏ | 233/285 [00:04<00:01, 32.00it/s]concatenating: train:  84%|████████▍ | 239/285 [00:04<00:01, 34.16it/s]concatenating: train:  86%|████████▌ | 244/285 [00:04<00:01, 31.47it/s]concatenating: train:  93%|█████████▎| 265/285 [00:04<00:00, 42.20it/s]concatenating: train: 100%|██████████| 285/285 [00:04<00:00, 60.10it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:02<00:04,  2.22s/it]Loading test:  67%|██████▋   | 2/3 [00:04<00:02,  2.12s/it]Loading test: 100%|██████████| 3/3 [00:05<00:00,  1.99s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 48.98it/s]2019-07-06 17:52:27.533470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 17:52:27.533657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 17:52:27.533683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 17:52:27.533699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 17:52:27.534317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:09,  3.92it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:08,  4.36it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.36it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:06,  5.32it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:06,  5.13it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.34it/s]loading the weights for Unet:  30%|███       | 12/40 [00:02<00:06,  4.45it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:04,  5.75it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.46it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.70it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:03<00:04,  4.39it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:03,  5.12it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  5.66it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:04<00:03,  4.60it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:04<00:02,  5.37it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  5.12it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  5.99it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:05<00:01,  5.91it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:05<00:00,  5.11it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  5.16it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:06<00:00,  3.92it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:06<00:00,  6.44it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 52, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 52, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 52, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 52, 10)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 52, 10)   910         dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 52, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 52, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 52, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 52, 10)   910         dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 52, 10)   40          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 52, 10)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 52, 10)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 52, 20)   1820        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 52, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 52, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 52, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 52, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 52, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 26, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 26, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 26, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 26, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 26, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 26, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 26, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 13, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 13, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 13, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 13, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 13, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 13, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 13, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 13, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 26, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 26, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 26, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 26, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 26, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 26, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 26, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 26, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 52, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 52, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 52, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 52, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 52, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 52, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 52, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 52, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 52, 10)   1810        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 52, 10)   40          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 52, 10)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 52, 10)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 52, 13)   143         dropout_9[0][0]                  
==================================================================================================
Total params: 189,493
Trainable params: 45,933
Non-trainable params: 143,560
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.49841486e-02 3.19966680e-02 7.50970181e-02 9.33357939e-03
 2.71292049e-02 7.07427267e-03 8.46489586e-02 1.12779077e-01
 8.61338510e-02 1.32649165e-02 2.94521391e-01 1.92807035e-01
 2.29878984e-04]
Train on 18361 samples, validate on 179 samples
Epoch 1/300
 - 24s - loss: 208.2165 - acc: 0.2718 - mDice: 0.0150 - val_loss: 117.5181 - val_acc: 0.9136 - val_mDice: 0.0131

Epoch 00001: val_mDice improved from -inf to 0.01307, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 15s - loss: 106.7201 - acc: 0.8259 - mDice: 0.0151 - val_loss: 48.4659 - val_acc: 0.9136 - val_mDice: 0.0132

Epoch 00002: val_mDice improved from 0.01307 to 0.01324, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 16s - loss: 52.2694 - acc: 0.8860 - mDice: 0.0162 - val_loss: 22.2463 - val_acc: 0.9136 - val_mDice: 0.0161

Epoch 00003: val_mDice improved from 0.01324 to 0.01610, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 17s - loss: 28.0721 - acc: 0.8863 - mDice: 0.0176 - val_loss: 16.9125 - val_acc: 0.9136 - val_mDice: 0.0243

Epoch 00004: val_mDice improved from 0.01610 to 0.02428, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 14s - loss: 18.0419 - acc: 0.8863 - mDice: 0.0190 - val_loss: 9.2193 - val_acc: 0.9136 - val_mDice: 0.0245

Epoch 00005: val_mDice improved from 0.02428 to 0.02451, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 14s - loss: 13.3995 - acc: 0.8863 - mDice: 0.0211 - val_loss: 6.3767 - val_acc: 0.9136 - val_mDice: 0.0264

Epoch 00006: val_mDice improved from 0.02451 to 0.02639, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 14s - loss: 10.8915 - acc: 0.8863 - mDice: 0.0246 - val_loss: 5.4864 - val_acc: 0.9136 - val_mDice: 0.0326

Epoch 00007: val_mDice improved from 0.02639 to 0.03265, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 13s - loss: 9.2992 - acc: 0.8863 - mDice: 0.0297 - val_loss: 4.7017 - val_acc: 0.9136 - val_mDice: 0.0397

Epoch 00008: val_mDice improved from 0.03265 to 0.03972, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 14s - loss: 8.1876 - acc: 0.8863 - mDice: 0.0355 - val_loss: 4.2556 - val_acc: 0.9136 - val_mDice: 0.0509

Epoch 00009: val_mDice improved from 0.03972 to 0.05092, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 16s - loss: 7.3844 - acc: 0.8864 - mDice: 0.0416 - val_loss: 4.0959 - val_acc: 0.9136 - val_mDice: 0.0571

Epoch 00010: val_mDice improved from 0.05092 to 0.05706, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 18s - loss: 6.7897 - acc: 0.8872 - mDice: 0.0475 - val_loss: 3.9365 - val_acc: 0.9136 - val_mDice: 0.0682

Epoch 00011: val_mDice improved from 0.05706 to 0.06815, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 16s - loss: 6.3249 - acc: 0.8882 - mDice: 0.0530 - val_loss: 3.6691 - val_acc: 0.9144 - val_mDice: 0.0773

Epoch 00012: val_mDice improved from 0.06815 to 0.07734, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 14s - loss: 5.9593 - acc: 0.8893 - mDice: 0.0583 - val_loss: 3.5209 - val_acc: 0.9145 - val_mDice: 0.0828

Epoch 00013: val_mDice improved from 0.07734 to 0.08282, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 14s - loss: 5.6657 - acc: 0.8894 - mDice: 0.0635 - val_loss: 3.4876 - val_acc: 0.9174 - val_mDice: 0.0903

Epoch 00014: val_mDice improved from 0.08282 to 0.09026, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 14s - loss: 5.4059 - acc: 0.8905 - mDice: 0.0689 - val_loss: 3.3500 - val_acc: 0.9174 - val_mDice: 0.0995

Epoch 00015: val_mDice improved from 0.09026 to 0.09954, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 13s - loss: 5.1968 - acc: 0.8915 - mDice: 0.0746 - val_loss: 3.3027 - val_acc: 0.9151 - val_mDice: 0.1069

Epoch 00016: val_mDice improved from 0.09954 to 0.10689, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 14s - loss: 4.9722 - acc: 0.8922 - mDice: 0.0809 - val_loss: 3.2028 - val_acc: 0.9187 - val_mDice: 0.1148

Epoch 00017: val_mDice improved from 0.10689 to 0.11479, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 13s - loss: 4.7263 - acc: 0.8930 - mDice: 0.0910 - val_loss: 3.0415 - val_acc: 0.9194 - val_mDice: 0.1342

Epoch 00018: val_mDice improved from 0.11479 to 0.13422, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 14s - loss: 4.5285 - acc: 0.8948 - mDice: 0.1008 - val_loss: 3.0470 - val_acc: 0.9214 - val_mDice: 0.1444

Epoch 00019: val_mDice improved from 0.13422 to 0.14438, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 14s - loss: 4.3556 - acc: 0.8962 - mDice: 0.1091 - val_loss: 2.9908 - val_acc: 0.9205 - val_mDice: 0.1543

Epoch 00020: val_mDice improved from 0.14438 to 0.15427, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 13s - loss: 4.2007 - acc: 0.8972 - mDice: 0.1174 - val_loss: 2.8575 - val_acc: 0.9216 - val_mDice: 0.1635

Epoch 00021: val_mDice improved from 0.15427 to 0.16355, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 14s - loss: 4.0655 - acc: 0.8980 - mDice: 0.1256 - val_loss: 2.8499 - val_acc: 0.9216 - val_mDice: 0.1718

Epoch 00022: val_mDice improved from 0.16355 to 0.17176, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 14s - loss: 3.9402 - acc: 0.8995 - mDice: 0.1347 - val_loss: 2.8514 - val_acc: 0.9231 - val_mDice: 0.1805

Epoch 00023: val_mDice improved from 0.17176 to 0.18046, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 13s - loss: 3.8277 - acc: 0.9014 - mDice: 0.1450 - val_loss: 2.8334 - val_acc: 0.9276 - val_mDice: 0.1937

Epoch 00024: val_mDice improved from 0.18046 to 0.19370, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 14s - loss: 3.7233 - acc: 0.9033 - mDice: 0.1545 - val_loss: 2.7434 - val_acc: 0.9290 - val_mDice: 0.2021

Epoch 00025: val_mDice improved from 0.19370 to 0.20215, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 14s - loss: 3.6194 - acc: 0.9049 - mDice: 0.1645 - val_loss: 2.6978 - val_acc: 0.9306 - val_mDice: 0.2125

Epoch 00026: val_mDice improved from 0.20215 to 0.21248, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 13s - loss: 3.5248 - acc: 0.9061 - mDice: 0.1734 - val_loss: 2.6219 - val_acc: 0.9335 - val_mDice: 0.2238

Epoch 00027: val_mDice improved from 0.21248 to 0.22377, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 14s - loss: 3.4370 - acc: 0.9071 - mDice: 0.1833 - val_loss: 2.5304 - val_acc: 0.9343 - val_mDice: 0.2379

Epoch 00028: val_mDice improved from 0.22377 to 0.23792, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 14s - loss: 3.3449 - acc: 0.9080 - mDice: 0.1942 - val_loss: 2.4931 - val_acc: 0.9358 - val_mDice: 0.2458

Epoch 00029: val_mDice improved from 0.23792 to 0.24581, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 13s - loss: 3.2689 - acc: 0.9092 - mDice: 0.2029 - val_loss: 2.4920 - val_acc: 0.9377 - val_mDice: 0.2483

Epoch 00030: val_mDice improved from 0.24581 to 0.24829, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 14s - loss: 3.1927 - acc: 0.9104 - mDice: 0.2113 - val_loss: 2.5750 - val_acc: 0.9335 - val_mDice: 0.2584

Epoch 00031: val_mDice improved from 0.24829 to 0.25842, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 14s - loss: 3.1227 - acc: 0.9116 - mDice: 0.2190 - val_loss: 2.3688 - val_acc: 0.9386 - val_mDice: 0.2740

Epoch 00032: val_mDice improved from 0.25842 to 0.27397, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 14s - loss: 3.0542 - acc: 0.9128 - mDice: 0.2271 - val_loss: 2.4689 - val_acc: 0.9377 - val_mDice: 0.2777

Epoch 00033: val_mDice improved from 0.27397 to 0.27771, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 14s - loss: 2.9945 - acc: 0.9140 - mDice: 0.2347 - val_loss: 2.3530 - val_acc: 0.9401 - val_mDice: 0.2946

Epoch 00034: val_mDice improved from 0.27771 to 0.29457, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 13s - loss: 2.9330 - acc: 0.9151 - mDice: 0.2422 - val_loss: 2.3082 - val_acc: 0.9386 - val_mDice: 0.3057

Epoch 00035: val_mDice improved from 0.29457 to 0.30568, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 13s - loss: 2.8705 - acc: 0.9159 - mDice: 0.2495 - val_loss: 2.3764 - val_acc: 0.9426 - val_mDice: 0.3053

Epoch 00036: val_mDice did not improve from 0.30568
Epoch 37/300
 - 13s - loss: 2.8080 - acc: 0.9170 - mDice: 0.2594 - val_loss: 2.3513 - val_acc: 0.9390 - val_mDice: 0.3186

Epoch 00037: val_mDice improved from 0.30568 to 0.31858, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 13s - loss: 2.7455 - acc: 0.9178 - mDice: 0.2707 - val_loss: 2.2758 - val_acc: 0.9425 - val_mDice: 0.3309

Epoch 00038: val_mDice improved from 0.31858 to 0.33091, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 13s - loss: 2.6948 - acc: 0.9183 - mDice: 0.2813 - val_loss: 2.2388 - val_acc: 0.9410 - val_mDice: 0.3391

Epoch 00039: val_mDice improved from 0.33091 to 0.33914, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 13s - loss: 2.6464 - acc: 0.9190 - mDice: 0.2898 - val_loss: 2.3151 - val_acc: 0.9420 - val_mDice: 0.3393

Epoch 00040: val_mDice improved from 0.33914 to 0.33928, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 13s - loss: 2.6058 - acc: 0.9197 - mDice: 0.2971 - val_loss: 2.1980 - val_acc: 0.9447 - val_mDice: 0.3532

Epoch 00041: val_mDice improved from 0.33928 to 0.35324, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 13s - loss: 2.5741 - acc: 0.9199 - mDice: 0.3027 - val_loss: 2.2231 - val_acc: 0.9424 - val_mDice: 0.3587

Epoch 00042: val_mDice improved from 0.35324 to 0.35868, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 13s - loss: 2.5322 - acc: 0.9204 - mDice: 0.3094 - val_loss: 2.5213 - val_acc: 0.9427 - val_mDice: 0.3494

Epoch 00043: val_mDice did not improve from 0.35868
Epoch 44/300
 - 13s - loss: 2.4972 - acc: 0.9203 - mDice: 0.3151 - val_loss: 2.5618 - val_acc: 0.9414 - val_mDice: 0.3354

Epoch 00044: val_mDice did not improve from 0.35868
Epoch 45/300
 - 14s - loss: 2.4665 - acc: 0.9203 - mDice: 0.3197 - val_loss: 2.2947 - val_acc: 0.9447 - val_mDice: 0.3761

Epoch 00045: val_mDice improved from 0.35868 to 0.37608, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 13s - loss: 2.4329 - acc: 0.9206 - mDice: 0.3257 - val_loss: 2.2389 - val_acc: 0.9443 - val_mDice: 0.3761

Epoch 00046: val_mDice improved from 0.37608 to 0.37614, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 14s - loss: 2.4084 - acc: 0.9209 - mDice: 0.3300 - val_loss: 2.2638 - val_acc: 0.9446 - val_mDice: 0.3730

Epoch 00047: val_mDice did not improve from 0.37614
Epoch 48/300
 - 13s - loss: 2.3734 - acc: 0.9214 - mDice: 0.3367 - val_loss: 2.3194 - val_acc: 0.9446 - val_mDice: 0.3743

Epoch 00048: val_mDice did not improve from 0.37614
Epoch 49/300
 - 14s - loss: 2.3374 - acc: 0.9221 - mDice: 0.3446 - val_loss: 2.2521 - val_acc: 0.9440 - val_mDice: 0.3847

Epoch 00049: val_mDice improved from 0.37614 to 0.38468, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 14s - loss: 2.3117 - acc: 0.9227 - mDice: 0.3505 - val_loss: 2.2054 - val_acc: 0.9459 - val_mDice: 0.3963

Epoch 00050: val_mDice improved from 0.38468 to 0.39626, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 14s - loss: 2.2874 - acc: 0.9231 - mDice: 0.3561 - val_loss: 2.1860 - val_acc: 0.9459 - val_mDice: 0.3971

Epoch 00051: val_mDice improved from 0.39626 to 0.39710, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 14s - loss: 2.2641 - acc: 0.9230 - mDice: 0.3601 - val_loss: 2.1988 - val_acc: 0.9463 - val_mDice: 0.4016

Epoch 00052: val_mDice improved from 0.39710 to 0.40163, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 14s - loss: 2.2385 - acc: 0.9230 - mDice: 0.3660 - val_loss: 2.2591 - val_acc: 0.9452 - val_mDice: 0.4014

Epoch 00053: val_mDice did not improve from 0.40163
Epoch 54/300
 - 14s - loss: 2.2092 - acc: 0.9228 - mDice: 0.3721 - val_loss: 2.5459 - val_acc: 0.9466 - val_mDice: 0.4007

Epoch 00054: val_mDice did not improve from 0.40163
Epoch 55/300
 - 14s - loss: 2.1838 - acc: 0.9228 - mDice: 0.3784 - val_loss: 2.1914 - val_acc: 0.9466 - val_mDice: 0.4190

Epoch 00055: val_mDice improved from 0.40163 to 0.41899, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 14s - loss: 2.1605 - acc: 0.9231 - mDice: 0.3836 - val_loss: 2.2817 - val_acc: 0.9457 - val_mDice: 0.4129

Epoch 00056: val_mDice did not improve from 0.41899
Epoch 57/300
 - 14s - loss: 2.1414 - acc: 0.9232 - mDice: 0.3874 - val_loss: 2.3392 - val_acc: 0.9464 - val_mDice: 0.4131

Epoch 00057: val_mDice did not improve from 0.41899
Epoch 58/300
 - 14s - loss: 2.1227 - acc: 0.9236 - mDice: 0.3913 - val_loss: 2.0913 - val_acc: 0.9454 - val_mDice: 0.4234

Epoch 00058: val_mDice improved from 0.41899 to 0.42342, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 14s - loss: 2.1071 - acc: 0.9238 - mDice: 0.3954 - val_loss: 2.2915 - val_acc: 0.9444 - val_mDice: 0.4119

Epoch 00059: val_mDice did not improve from 0.42342
Epoch 60/300
 - 14s - loss: 2.0917 - acc: 0.9241 - mDice: 0.3979 - val_loss: 2.2414 - val_acc: 0.9460 - val_mDice: 0.4189

Epoch 00060: val_mDice did not improve from 0.42342
Epoch 61/300
 - 14s - loss: 2.0770 - acc: 0.9243 - mDice: 0.4017 - val_loss: 2.0641 - val_acc: 0.9455 - val_mDice: 0.4277

Epoch 00061: val_mDice improved from 0.42342 to 0.42769, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 62/300
 - 13s - loss: 2.0599 - acc: 0.9244 - mDice: 0.4054 - val_loss: 2.1634 - val_acc: 0.9466 - val_mDice: 0.4341

Epoch 00062: val_mDice improved from 0.42769 to 0.43408, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 15s - loss: 2.0411 - acc: 0.9247 - mDice: 0.4100 - val_loss: 2.1845 - val_acc: 0.9457 - val_mDice: 0.4301

Epoch 00063: val_mDice did not improve from 0.43408
Epoch 64/300
 - 13s - loss: 2.0289 - acc: 0.9247 - mDice: 0.4132 - val_loss: 2.1320 - val_acc: 0.9480 - val_mDice: 0.4454

Epoch 00064: val_mDice improved from 0.43408 to 0.44544, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 65/300
 - 14s - loss: 2.0120 - acc: 0.9252 - mDice: 0.4163 - val_loss: 2.2396 - val_acc: 0.9461 - val_mDice: 0.4351

Epoch 00065: val_mDice did not improve from 0.44544
Epoch 66/300
 - 14s - loss: 2.0080 - acc: 0.9251 - mDice: 0.4179 - val_loss: 2.2554 - val_acc: 0.9462 - val_mDice: 0.4317

Epoch 00066: val_mDice did not improve from 0.44544
Epoch 67/300
 - 13s - loss: 1.9942 - acc: 0.9252 - mDice: 0.4207 - val_loss: 2.2408 - val_acc: 0.9459 - val_mDice: 0.4335

Epoch 00067: val_mDice did not improve from 0.44544
Epoch 68/300
 - 14s - loss: 1.9809 - acc: 0.9255 - mDice: 0.4237 - val_loss: 2.2938 - val_acc: 0.9471 - val_mDice: 0.4325

Epoch 00068: val_mDice did not improve from 0.44544
Epoch 69/300
 - 13s - loss: 1.9720 - acc: 0.9255 - mDice: 0.4267 - val_loss: 2.2139 - val_acc: 0.9450 - val_mDice: 0.4282

Epoch 00069: val_mDice did not improve from 0.44544
Epoch 70/300
 - 14s - loss: 1.9677 - acc: 0.9254 - mDice: 0.4277 - val_loss: 2.3019 - val_acc: 0.9450 - val_mDice: 0.4300

Epoch 00070: val_mDice did not improve from 0.44544
Epoch 71/300
 - 13s - loss: 1.9525 - acc: 0.9257 - mDice: 0.4316 - val_loss: 2.2472 - val_acc: 0.9457 - val_mDice: 0.4295

Epoch 00071: val_mDice did not improve from 0.44544
Epoch 72/300
 - 14s - loss: 1.9422 - acc: 0.9259 - mDice: 0.4345 - val_loss: 2.0717 - val_acc: 0.9464 - val_mDice: 0.4545

Epoch 00072: val_mDice improved from 0.44544 to 0.45445, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 73/300
 - 13s - loss: 1.9355 - acc: 0.9259 - mDice: 0.4363 - val_loss: 2.3691 - val_acc: 0.9466 - val_mDice: 0.4332

Epoch 00073: val_mDice did not improve from 0.45445
Epoch 74/300
 - 13s - loss: 1.9242 - acc: 0.9260 - mDice: 0.4389 - val_loss: 2.2499 - val_acc: 0.9467 - val_mDice: 0.4488

Epoch 00074: val_mDice did not improve from 0.45445
Epoch 75/300
 - 13s - loss: 1.9207 - acc: 0.9262 - mDice: 0.4406 - val_loss: 2.2208 - val_acc: 0.9466 - val_mDice: 0.4458

Epoch 00075: val_mDice did not improve from 0.45445
Epoch 76/300
 - 13s - loss: 1.9154 - acc: 0.9263 - mDice: 0.4418 - val_loss: 2.2523 - val_acc: 0.9452 - val_mDice: 0.4400

Epoch 00076: val_mDice did not improve from 0.45445
Epoch 77/300
 - 13s - loss: 1.9091 - acc: 0.9264 - mDice: 0.4437 - val_loss: 2.2611 - val_acc: 0.9454 - val_mDice: 0.4441

Epoch 00077: val_mDice did not improve from 0.45445
Epoch 78/300
 - 13s - loss: 1.8977 - acc: 0.9266 - mDice: 0.4462 - val_loss: 2.3587 - val_acc: 0.9455 - val_mDice: 0.4342

Epoch 00078: val_mDice did not improve from 0.45445
Epoch 79/300
 - 13s - loss: 1.8949 - acc: 0.9268 - mDice: 0.4480 - val_loss: 2.2191 - val_acc: 0.9488 - val_mDice: 0.4606

Epoch 00079: val_mDice improved from 0.45445 to 0.46065, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 80/300
 - 13s - loss: 1.8898 - acc: 0.9269 - mDice: 0.4484 - val_loss: 2.3277 - val_acc: 0.9464 - val_mDice: 0.4423

Epoch 00080: val_mDice did not improve from 0.46065
Epoch 81/300
 - 13s - loss: 1.8814 - acc: 0.9269 - mDice: 0.4505 - val_loss: 2.3691 - val_acc: 0.9456 - val_mDice: 0.4373

Epoch 00081: val_mDice did not improve from 0.46065
Epoch 82/300
 - 13s - loss: 1.8821 - acc: 0.9270 - mDice: 0.4510 - val_loss: 2.3304 - val_acc: 0.9466 - val_mDice: 0.4476

Epoch 00082: val_mDice did not improve from 0.46065
Epoch 83/300
 - 13s - loss: 1.8724 - acc: 0.9272 - mDice: 0.4535 - val_loss: 2.3703 - val_acc: 0.9453 - val_mDice: 0.4385

Epoch 00083: val_mDice did not improve from 0.46065
Epoch 84/300
 - 13s - loss: 1.8682 - acc: 0.9272 - mDice: 0.4543 - val_loss: 2.2443 - val_acc: 0.9452 - val_mDice: 0.4419

Epoch 00084: val_mDice did not improve from 0.46065
Epoch 85/300
 - 13s - loss: 1.8687 - acc: 0.9273 - mDice: 0.4547 - val_loss: 2.2147 - val_acc: 0.9463 - val_mDice: 0.4544

Epoch 00085: val_mDice did not improve from 0.46065
Epoch 86/300
 - 13s - loss: 1.8649 - acc: 0.9274 - mDice: 0.4557 - val_loss: 2.3173 - val_acc: 0.9464 - val_mDice: 0.4423

Epoch 00086: val_mDice did not improve from 0.46065
Epoch 87/300
 - 13s - loss: 1.8595 - acc: 0.9276 - mDice: 0.4575 - val_loss: 2.1927 - val_acc: 0.9478 - val_mDice: 0.4591

Epoch 00087: val_mDice did not improve from 0.46065
Epoch 88/300
 - 13s - loss: 1.8541 - acc: 0.9278 - mDice: 0.4591 - val_loss: 2.2877 - val_acc: 0.9475 - val_mDice: 0.4531

Epoch 00088: val_mDice did not improve from 0.46065
Epoch 89/300
 - 13s - loss: 1.8521 - acc: 0.9277 - mDice: 0.4588 - val_loss: 2.2714 - val_acc: 0.9458 - val_mDice: 0.4478

Epoch 00089: val_mDice did not improve from 0.46065
Epoch 90/300
 - 13s - loss: 1.8431 - acc: 0.9279 - mDice: 0.4617 - val_loss: 2.2606 - val_acc: 0.9454 - val_mDice: 0.4457

Epoch 00090: val_mDice did not improve from 0.46065
Epoch 91/300
 - 13s - loss: 1.8444 - acc: 0.9280 - mDice: 0.4614 - val_loss: 2.3463 - val_acc: 0.9450 - val_mDice: 0.4432

Epoch 00091: val_mDice did not improve from 0.46065
Epoch 92/300
 - 13s - loss: 1.8376 - acc: 0.9280 - mDice: 0.4632 - val_loss: 2.4339 - val_acc: 0.9464 - val_mDice: 0.4494

Epoch 00092: val_mDice did not improve from 0.46065
Epoch 93/300
 - 14s - loss: 1.8384 - acc: 0.9281 - mDice: 0.4633 - val_loss: 2.2620 - val_acc: 0.9438 - val_mDice: 0.4407

Epoch 00093: val_mDice did not improve from 0.46065
Epoch 94/300
 - 13s - loss: 1.8310 - acc: 0.9282 - mDice: 0.4654 - val_loss: 2.3313 - val_acc: 0.9469 - val_mDice: 0.4498

Epoch 00094: val_mDice did not improve from 0.46065
Epoch 95/300
 - 13s - loss: 1.8318 - acc: 0.9283 - mDice: 0.4648 - val_loss: 2.3285 - val_acc: 0.9462 - val_mDice: 0.4519

Epoch 00095: val_mDice did not improve from 0.46065
Epoch 96/300
 - 13s - loss: 1.8284 - acc: 0.9284 - mDice: 0.4662 - val_loss: 2.3232 - val_acc: 0.9486 - val_mDice: 0.4696

Epoch 00096: val_mDice improved from 0.46065 to 0.46957, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 97/300
 - 14s - loss: 1.8276 - acc: 0.9285 - mDice: 0.4663 - val_loss: 2.3374 - val_acc: 0.9467 - val_mDice: 0.4496

Epoch 00097: val_mDice did not improve from 0.46957
Epoch 98/300
 - 13s - loss: 1.8219 - acc: 0.9286 - mDice: 0.4683 - val_loss: 2.2274 - val_acc: 0.9453 - val_mDice: 0.4574

Epoch 00098: val_mDice did not improve from 0.46957
Epoch 99/300
 - 13s - loss: 1.8211 - acc: 0.9286 - mDice: 0.4678 - val_loss: 2.2817 - val_acc: 0.9458 - val_mDice: 0.4478

Epoch 00099: val_mDice did not improve from 0.46957
Epoch 100/300
 - 13s - loss: 1.8210 - acc: 0.9287 - mDice: 0.4682 - val_loss: 2.2799 - val_acc: 0.9454 - val_mDice: 0.4498

Epoch 00100: val_mDice did not improve from 0.46957
Epoch 101/300
 - 13s - loss: 1.8102 - acc: 0.9290 - mDice: 0.4711 - val_loss: 2.3152 - val_acc: 0.9459 - val_mDice: 0.4531

Epoch 00101: val_mDice did not improve from 0.46957
Epoch 102/300
 - 13s - loss: 1.8096 - acc: 0.9289 - mDice: 0.4712 - val_loss: 2.3352 - val_acc: 0.9464 - val_mDice: 0.4578

Epoch 00102: val_mDice did not improve from 0.46957
Epoch 103/300
 - 13s - loss: 1.8089 - acc: 0.9290 - mDice: 0.4717 - val_loss: 2.2426 - val_acc: 0.9458 - val_mDice: 0.4563

Epoch 00103: val_mDice did not improve from 0.46957
Epoch 104/300
 - 13s - loss: 1.8088 - acc: 0.9291 - mDice: 0.4714 - val_loss: 2.3617 - val_acc: 0.9447 - val_mDice: 0.4419

Epoch 00104: val_mDice did not improve from 0.46957
Epoch 105/300
 - 14s - loss: 1.7971 - acc: 0.9293 - mDice: 0.4745 - val_loss: 2.2567 - val_acc: 0.9445 - val_mDice: 0.4476

Epoch 00105: val_mDice did not improve from 0.46957
Epoch 106/300
 - 13s - loss: 1.8028 - acc: 0.9291 - mDice: 0.4728 - val_loss: 2.3149 - val_acc: 0.9469 - val_mDice: 0.4607

Epoch 00106: val_mDice did not improve from 0.46957
Epoch 107/300
 - 13s - loss: 1.8063 - acc: 0.9292 - mDice: 0.4723 - val_loss: 2.2925 - val_acc: 0.9456 - val_mDice: 0.4542

Epoch 00107: val_mDice did not improve from 0.46957
Epoch 108/300
 - 13s - loss: 1.7957 - acc: 0.9294 - mDice: 0.4750 - val_loss: 2.1588 - val_acc: 0.9446 - val_mDice: 0.4641

Epoch 00108: val_mDice did not improve from 0.46957
Epoch 109/300
 - 14s - loss: 1.7914 - acc: 0.9296 - mDice: 0.4760 - val_loss: 2.2595 - val_acc: 0.9455 - val_mDice: 0.4533

Epoch 00109: val_mDice did not improve from 0.46957
Epoch 110/300
 - 13s - loss: 1.7893 - acc: 0.9297 - mDice: 0.4770 - val_loss: 2.5118 - val_acc: 0.9437 - val_mDice: 0.4332

Epoch 00110: val_mDice did not improve from 0.46957
Epoch 111/300
 - 13s - loss: 1.7882 - acc: 0.9296 - mDice: 0.4770 - val_loss: 2.5482 - val_acc: 0.9424 - val_mDice: 0.4313

Epoch 00111: val_mDice did not improve from 0.46957
Epoch 112/300
 - 13s - loss: 1.7902 - acc: 0.9296 - mDice: 0.4769 - val_loss: 2.2881 - val_acc: 0.9470 - val_mDice: 0.4610

Epoch 00112: val_mDice did not improve from 0.46957
Epoch 113/300
 - 13s - loss: 1.7843 - acc: 0.9296 - mDice: 0.4781 - val_loss: 2.2980 - val_acc: 0.9448 - val_mDice: 0.4504

Epoch 00113: val_mDice did not improve from 0.46957
Epoch 114/300
 - 13s - loss: 1.7809 - acc: 0.9298 - mDice: 0.4788 - val_loss: 2.5277 - val_acc: 0.9440 - val_mDice: 0.4341

Epoch 00114: val_mDice did not improve from 0.46957
Epoch 115/300
 - 13s - loss: 1.7870 - acc: 0.9297 - mDice: 0.4776 - val_loss: 2.3826 - val_acc: 0.9466 - val_mDice: 0.4577

Epoch 00115: val_mDice did not improve from 0.46957
Epoch 116/300
 - 13s - loss: 1.7770 - acc: 0.9297 - mDice: 0.4801 - val_loss: 2.3617 - val_acc: 0.9468 - val_mDice: 0.4583

Epoch 00116: val_mDice did not improve from 0.46957
Epoch 117/300
 - 13s - loss: 1.7746 - acc: 0.9299 - mDice: 0.4812 - val_loss: 2.3369 - val_acc: 0.9434 - val_mDice: 0.4410

Epoch 00117: val_mDice did not improve from 0.46957
Epoch 118/300
 - 13s - loss: 1.7728 - acc: 0.9299 - mDice: 0.4813 - val_loss: 2.3072 - val_acc: 0.9469 - val_mDice: 0.4608

Epoch 00118: val_mDice did not improve from 0.46957
Epoch 119/300
 - 13s - loss: 1.7756 - acc: 0.9299 - mDice: 0.4809 - val_loss: 2.5154 - val_acc: 0.9447 - val_mDice: 0.4397

Epoch 00119: val_mDice did not improve from 0.46957
Epoch 120/300
 - 13s - loss: 1.7663 - acc: 0.9300 - mDice: 0.4832 - val_loss: 2.4024 - val_acc: 0.9451 - val_mDice: 0.4491

Epoch 00120: val_mDice did not improve from 0.46957
Epoch 121/300
 - 13s - loss: 1.7696 - acc: 0.9299 - mDice: 0.4821 - val_loss: 2.4854 - val_acc: 0.9478 - val_mDice: 0.4592

Epoch 00121: val_mDice did not improve from 0.46957
Epoch 122/300
 - 13s - loss: 1.7650 - acc: 0.9300 - mDice: 0.4838 - val_loss: 2.4257 - val_acc: 0.9479 - val_mDice: 0.4615

Epoch 00122: val_mDice did not improve from 0.46957
Epoch 123/300
 - 13s - loss: 1.7632 - acc: 0.9300 - mDice: 0.4840 - val_loss: 2.2764 - val_acc: 0.9440 - val_mDice: 0.4583

Epoch 00123: val_mDice did not improve from 0.46957
Epoch 124/300
 - 13s - loss: 1.7651 - acc: 0.9301 - mDice: 0.4848 - val_loss: 2.3643 - val_acc: 0.9462 - val_mDice: 0.4500

Epoch 00124: val_mDice did not improve from 0.46957
Epoch 125/300
 - 13s - loss: 1.7581 - acc: 0.9302 - mDice: 0.4856 - val_loss: 2.3769 - val_acc: 0.9451 - val_mDice: 0.4497

Epoch 00125: val_mDice did not improve from 0.46957
Epoch 126/300
 - 13s - loss: 1.7638 - acc: 0.9301 - mDice: 0.4847 - val_loss: 2.3699 - val_acc: 0.9474 - val_mDice: 0.4671

Epoch 00126: val_mDice did not improve from 0.46957
Restoring model weights from the end of the best epoch
Epoch 00126: early stopping
{'val_loss': [117.51808558629212, 48.46590216865753, 22.24633585540942, 16.912510062063205, 9.219293594360352, 6.376720873337218, 5.486382934634246, 4.701686811180754, 4.255627425689271, 4.095895968335967, 3.936486056397081, 3.66914772854171, 3.52087088936534, 3.4875964519031886, 3.350030539422062, 3.3027187672407265, 3.202764678933767, 3.041454102073968, 3.0469520238524708, 2.9907534815079675, 2.857497255229417, 2.8498641991748492, 2.851408237851532, 2.8333610156394915, 2.743402976563523, 2.6977784620317, 2.6219224743336937, 2.530419767901884, 2.4931223259291837, 2.491971759156808, 2.5750172151533586, 2.368802424915676, 2.4689213363817952, 2.3530383789339546, 2.3082015208025886, 2.376420082326708, 2.351270904754127, 2.2758407872482387, 2.2387748950020563, 2.3151095020038457, 2.198012611719483, 2.223112513899137, 2.521312905423468, 2.561820761451508, 2.2946771922724207, 2.238910316754986, 2.263830410035629, 2.3193598965692788, 2.2521314660930103, 2.2053821952649333, 2.186004442875612, 2.19884141064223, 2.2591345363489075, 2.5458855602328336, 2.191383689475459, 2.281691680407391, 2.3391989982327934, 2.091297468659598, 2.2915430921416045, 2.2414037115747036, 2.064069212481962, 2.163405085409154, 2.184453302255556, 2.1319678282604535, 2.23962063629534, 2.2553804693275326, 2.2407589211810235, 2.2938475182602525, 2.2139141612878723, 2.3018779408332355, 2.247220165902676, 2.0716722930609848, 2.3691252783024113, 2.2498927396102992, 2.2208042770790652, 2.2523353859033, 2.2610704192902125, 2.3587314776202155, 2.2190778029031595, 2.327734699462379, 2.3690915121046525, 2.3303784085385626, 2.370334044515088, 2.2442825346685655, 2.21466420882241, 2.317292182805152, 2.1926661616597096, 2.2877131494063905, 2.2714429327895522, 2.2606468959893595, 2.34630122397865, 2.433856343423854, 2.2619875303193844, 2.3313213646744884, 2.328484924145917, 2.323230728756782, 2.3373749788912983, 2.2274285681420865, 2.2816766254062757, 2.279860781557733, 2.3151529248200315, 2.3352126862083735, 2.2425728936435125, 2.361695598623606, 2.2567035206203356, 2.3148638562783184, 2.292546234983306, 2.1588210886417154, 2.2595332015160077, 2.5118011935463165, 2.548156730289566, 2.28809882808664, 2.2979908042779846, 2.527720619180349, 2.3826390045315193, 2.361744803423322, 2.3368926314668284, 2.3072466530613394, 2.5153868398186883, 2.4023516710910053, 2.4854005235533476, 2.425685022130359, 2.2763817430208517, 2.364305934426505, 2.376885661865746, 2.36990751101318], 'val_acc': [0.9136041049850719, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9144160434520444, 0.9145090473430783, 0.9174448711246086, 0.917399439065816, 0.91510404055345, 0.9186824363703169, 0.9193910946393146, 0.9213517824364774, 0.9204819558719017, 0.9215976346138469, 0.9216038235072983, 0.9230831252130051, 0.927624303868363, 0.9290168135525794, 0.9305994037809319, 0.9334918899909078, 0.9343224314338002, 0.9358099582474991, 0.9376590917896293, 0.9334959937207525, 0.9386115194032978, 0.9376632178295924, 0.9400990718569835, 0.9386404353147112, 0.9425927906728989, 0.9389544925210196, 0.942501926222327, 0.9410205540044347, 0.9420184383179222, 0.944710492088808, 0.9424399340618922, 0.9426630688779181, 0.9414130952105176, 0.9447208276674068, 0.9442600904230299, 0.9445782983103278, 0.9446237466854757, 0.9440245664985486, 0.945869549692676, 0.9458612999436575, 0.9463199590171516, 0.9451671052911428, 0.9466071248720478, 0.9465719937612225, 0.9456856746913335, 0.9463901766185654, 0.945429493595102, 0.9444088739389814, 0.9460348387004277, 0.9454501471039969, 0.9465988724591345, 0.9456939520782599, 0.9480471747547554, 0.9460947573517954, 0.946245572087485, 0.9458778267466156, 0.9471071186012396, 0.9450390162414679, 0.9450059359966043, 0.9457063431846363, 0.9463654050613914, 0.946592673243091, 0.946685630515967, 0.9465658128594553, 0.9452311236765132, 0.9453675030996014, 0.945501786703504, 0.9488343473253303, 0.9463963821613589, 0.9455617100166875, 0.9465802724800962, 0.945286928275444, 0.9452290704796434, 0.9462662592280511, 0.9464459962019042, 0.9478095706614702, 0.9474790166210196, 0.9457993290943807, 0.9453778147031475, 0.945022495432273, 0.9464211813564407, 0.9437849385112358, 0.9469418182719354, 0.9462414640288114, 0.9486132630422794, 0.9467393496183044, 0.9452704221176702, 0.9458137807233373, 0.9454108836264584, 0.9458530362091917, 0.9464439050445344, 0.9458199809383414, 0.9446753756294037, 0.9445080457452956, 0.9469252951318325, 0.9455658590327428, 0.9446402541751968, 0.9455018023538856, 0.94373532979848, 0.9423779495601547, 0.9469976155451556, 0.944836511625258, 0.9440018657865471, 0.9465885551948121, 0.9467558834139861, 0.9433572538738144, 0.9469232525905418, 0.9447146401059028, 0.945144377607207, 0.9477558588848434, 0.9478508913317206, 0.9439729179083968, 0.9461980761762437, 0.945105123786287, 0.9473798391539291], 'val_mDice': [0.013066517137115895, 0.01324071853770224, 0.016101758250727335, 0.024283354071764972, 0.024510128090584744, 0.026390435559123588, 0.032645282403217346, 0.03972302810106863, 0.05092492757895805, 0.05705537671982909, 0.06815184873409111, 0.07734098943085643, 0.08281902627572, 0.09025610766930287, 0.09953666624077205, 0.1068948149764338, 0.1147932041041012, 0.1342178749638563, 0.14438216146810096, 0.1542678807534319, 0.16354973264579667, 0.17175989502302094, 0.18045952832898615, 0.19370223207180726, 0.20214764340962776, 0.21248338042690768, 0.2237715609579779, 0.23792422092493687, 0.24581250040557798, 0.2482930305283829, 0.2584177928263915, 0.2739692834835479, 0.27770649287953725, 0.2945722587947739, 0.3056756617636654, 0.30532508505789263, 0.3185790723928526, 0.3309070224868519, 0.3391365307336413, 0.33928232952203163, 0.35324471389781165, 0.3586831592314736, 0.3493949627743087, 0.33539254771930543, 0.376082768986345, 0.3761425812484166, 0.3730098262179497, 0.3742944967813332, 0.38467595630517887, 0.39626335864626494, 0.39710322388723573, 0.40162845830011634, 0.40143855740238166, 0.4006766362230205, 0.41899134010576006, 0.41294962300934607, 0.4130737098901631, 0.42341687909051695, 0.4118563905774548, 0.41886274974439397, 0.4276890461671286, 0.43408105293465726, 0.4300919300018076, 0.44543860211718683, 0.43509899570955246, 0.4316636333918438, 0.4335451648887975, 0.4324596890524113, 0.42817618860212786, 0.4300430523616642, 0.42953810212332444, 0.4544518020565949, 0.4331690209537911, 0.4487667559911419, 0.44580796144528095, 0.43997800167046447, 0.44410210947750667, 0.43421890033023985, 0.46064965901428095, 0.4422769381680302, 0.4373003692267327, 0.4475871539315698, 0.438517565167816, 0.44186238686465684, 0.4544142751054391, 0.4423231520799285, 0.45910338732783357, 0.45312713344669875, 0.4477672536945876, 0.4457221866985939, 0.4431934330050506, 0.449426056619463, 0.4407238936957034, 0.44978072989586343, 0.45193010002541145, 0.46957017989131994, 0.44957833649725887, 0.45743122330591, 0.44777060420819503, 0.44979905965607925, 0.45306097628684017, 0.4577706352958466, 0.4562820144538773, 0.44190392447583504, 0.4476420679571908, 0.46071239456784124, 0.45419248389132194, 0.4640814612031649, 0.4533293015797045, 0.43321844535832965, 0.4313400093070622, 0.46100612675677466, 0.4504343947218783, 0.4340989704571623, 0.4576940499870471, 0.45826133869213764, 0.44102099987381665, 0.4607524738631435, 0.43971306385274705, 0.4490655420878746, 0.4591812418159826, 0.4614770888616253, 0.4583199620246887, 0.4500273076515624, 0.44971826256320463, 0.4671349711924292], 'loss': [208.2164572599465, 106.72009574940415, 52.26940155600639, 28.0721455638909, 18.041870989575376, 13.399534231956395, 10.891536199373265, 9.299179680979424, 8.187649400371475, 7.38440626380841, 6.789738511657424, 6.324902418242659, 5.959331375731537, 5.665735407843257, 5.405877456887863, 5.196773834396282, 4.972169194733887, 4.7263033158521655, 4.528549184181625, 4.355586415691239, 4.200679878959741, 4.065463639326289, 3.9401964932496716, 3.8277209982309235, 3.7232991143980745, 3.6193530821708038, 3.524793386056814, 3.4370262299505403, 3.3449309631199315, 3.268943850786739, 3.192749052166932, 3.122663027943755, 3.054200199335841, 2.9945090987260197, 2.9330355058820876, 2.870517424414941, 2.8080074711047476, 2.7455002294179627, 2.6947873949154055, 2.646398947328649, 2.6057755729158902, 2.574061305244649, 2.5321570157446405, 2.4971853290324337, 2.466473570953468, 2.4328543957876527, 2.40840219506043, 2.3733569402709245, 2.337381100279334, 2.311748661602006, 2.2874167889890624, 2.26408076188928, 2.2384755919944563, 2.2092396766750793, 2.183828362711673, 2.160517498419881, 2.141425450872306, 2.122744038112672, 2.1071436403299053, 2.0917204930679607, 2.0770468169012837, 2.059908729234258, 2.041127574715854, 2.0288684426176737, 2.0119513974367136, 2.007987842834578, 1.9942438660231, 1.9808869052703302, 1.9719970995067972, 1.9676755064810603, 1.952503958063171, 1.9422207914544805, 1.935539360386855, 1.9242007288042804, 1.9206552731433106, 1.9153750388621376, 1.909083876881969, 1.8976922708816968, 1.894945981838716, 1.889784760117031, 1.8813821834959785, 1.8821292894220671, 1.8723843296228246, 1.868162195021976, 1.8687493468180154, 1.8648550213568156, 1.859519843422544, 1.8541331673998127, 1.852109982780262, 1.8430739918550332, 1.8443892500147465, 1.8376265401628813, 1.8384415105721328, 1.8310263990985822, 1.8318402999094463, 1.8283954594988532, 1.827581330014693, 1.8219465072245504, 1.8211364940155383, 1.8209748839339972, 1.8102168979925772, 1.8096474666532092, 1.8088593234891732, 1.808840082229453, 1.7970829006179174, 1.802804494071621, 1.806315942813963, 1.7957422523922886, 1.791385004368621, 1.7893071329663128, 1.7881541858278558, 1.790186373504845, 1.784326323567701, 1.780925815157768, 1.7870325467846349, 1.7769537539362914, 1.7746168640884243, 1.7727745712131309, 1.7755998179294608, 1.7663007510246636, 1.7696050934838052, 1.7650304796012566, 1.7632345225412234, 1.7651189619319305, 1.7580613402147502, 1.7638250459130635], 'acc': [0.27181342321449903, 0.8258772381711166, 0.886020934014959, 0.8862617507355385, 0.8862687218430112, 0.8862687382301495, 0.8862687385645147, 0.8862687214437208, 0.8862700696851387, 0.8864093282391423, 0.887236204686738, 0.8881985201606316, 0.8892844229357142, 0.889373204938714, 0.8904839417434623, 0.8915216049320343, 0.8921948619757815, 0.8930073004520496, 0.8947977610815, 0.8962465148074429, 0.8971581695990185, 0.8979824102072277, 0.8994831903510205, 0.9014158119180676, 0.90334208574789, 0.9049240601107573, 0.9061132879189939, 0.9070749961143679, 0.9080013340286008, 0.9091630499578887, 0.9103787237831362, 0.911573067507929, 0.9127594382374267, 0.9139729982289566, 0.9150899355139233, 0.9159174612077335, 0.9169939377814179, 0.9177761788682723, 0.9183222421938334, 0.9190185433554432, 0.9196601992053123, 0.9199242988259151, 0.9203872352434357, 0.9203236277284825, 0.9203478977263516, 0.9205960664807059, 0.9208714626017339, 0.9213944420005026, 0.9221216400985942, 0.9227082691845255, 0.9230541034069698, 0.9230072339481858, 0.9230160946407373, 0.9227782817689638, 0.9228042448263635, 0.9230873949951962, 0.9231709032807693, 0.9235890878064266, 0.9238219260357867, 0.9240762960819252, 0.9242836754537993, 0.9243549569150045, 0.9247132976663754, 0.9247250222937745, 0.9251501328135802, 0.9250782668561329, 0.9252372449613411, 0.9254812632379147, 0.925506762858732, 0.9254138271738894, 0.9256894492186283, 0.925867379383677, 0.9258573088509072, 0.9260285751251255, 0.9261746416488742, 0.9263266317887631, 0.9264452058851742, 0.926624911905705, 0.926829853778402, 0.9268792414547058, 0.9269460892373457, 0.9270329820967586, 0.9272064436079986, 0.9272488999046029, 0.9273148691054945, 0.9273562801604268, 0.927601965997131, 0.9278042510920416, 0.9277196336134048, 0.9279053215334913, 0.9280499595292324, 0.9280036940540106, 0.9281003137123325, 0.9282479714213695, 0.9283364945153747, 0.9284169006358044, 0.9284788962283378, 0.9285723737178107, 0.9286491934084878, 0.928712863439984, 0.9289790151373869, 0.928948078609475, 0.9290055842332334, 0.9291362646527102, 0.9293411458624804, 0.9290951544549897, 0.9292472244367302, 0.929426161904369, 0.9295969863296575, 0.9296526777465763, 0.9296156576169186, 0.9295963013810365, 0.9296160182410884, 0.929833971595759, 0.929703879584272, 0.9297184818132882, 0.929887348783866, 0.9299379029530311, 0.9299001387732785, 0.9300032853820499, 0.9299251746231484, 0.9299908150067602, 0.9300392174128645, 0.9300944246982711, 0.9302408196140391, 0.9301251446354799], 'mDice': [0.01496437686064795, 0.015068788907175834, 0.0161873763654884, 0.017605753891430945, 0.019045149541554982, 0.021137450057354817, 0.02462949980707491, 0.029722619625649923, 0.03545594862150209, 0.04159021012663166, 0.0475431611533908, 0.053043587923043437, 0.058306977271313024, 0.06353280538486425, 0.068888816682837, 0.07457606613442865, 0.08088194284733978, 0.09104186230807333, 0.10082030929347073, 0.10911636786467109, 0.11741017574455268, 0.12560136285111131, 0.13474483701987475, 0.14502138257623984, 0.15452662672581519, 0.16449110817642398, 0.1733556347558535, 0.18329138151441457, 0.1942427225861763, 0.20289443413482552, 0.2112587518901985, 0.21904982489608132, 0.22712105228246332, 0.23472158564723827, 0.2421632793209897, 0.24945752455851758, 0.2594366981166295, 0.2706883746024265, 0.28127484721923457, 0.28983750076999293, 0.29713049328416125, 0.3027131989655895, 0.30944947668958234, 0.31506413972382324, 0.31970870333665546, 0.325667255591297, 0.32999370676565715, 0.3367467753414261, 0.34456748076756727, 0.35053817160826456, 0.3561341414202416, 0.36013091416876014, 0.3660130297901986, 0.37207788613508563, 0.37843934150473446, 0.38355218632296145, 0.38736576446033333, 0.39129024895958947, 0.3953931881584027, 0.39787348181156995, 0.4017023323755574, 0.4054319216537486, 0.4099666818957736, 0.41316766842869423, 0.4162636555122216, 0.4178889837797921, 0.42067498455126295, 0.42374402585550824, 0.4267360838752486, 0.42768370218314894, 0.4315888148352417, 0.434508460627669, 0.43634385086081073, 0.43886489210406143, 0.44061382648004865, 0.4417519917656125, 0.44368541093311775, 0.4462368754602765, 0.4479813766738544, 0.44839510996839943, 0.45049476416098916, 0.451039004992, 0.45347934961643593, 0.45429509963859926, 0.45468027023219654, 0.4557487242969512, 0.4574691278210822, 0.4591010924414296, 0.45879820572557967, 0.46169538710165514, 0.4613704484607989, 0.4632437572335244, 0.4633246009145698, 0.46544615684216023, 0.46483046407241474, 0.4661918742910176, 0.46629671122921196, 0.46827283632611794, 0.46778375487215645, 0.4682324572778412, 0.4710909188731902, 0.4711921781173271, 0.4716566093629186, 0.4714426018626762, 0.47446792331904564, 0.4728069803526599, 0.47228290553147756, 0.47498042752206354, 0.4760326325494112, 0.47704813769424503, 0.47700019414749134, 0.47685024289158745, 0.4780799496421224, 0.4787663951963324, 0.47758316460027656, 0.4801237330730948, 0.48121721230426595, 0.48131331871533783, 0.48092028881221754, 0.483245265397181, 0.48205908250785345, 0.4838298657031226, 0.48401030329998057, 0.48476905534096154, 0.4856205514658186, 0.4846822635166318]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.71s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.43s/it]predicting test subjects: 100%|██████████| 3/3 [00:06<00:00,  2.20s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:50,  1.66s/it]predicting train subjects:   1%|          | 2/285 [00:03<08:04,  1.71s/it]predicting train subjects:   1%|          | 3/285 [00:05<07:49,  1.66s/it]predicting train subjects:   1%|▏         | 4/285 [00:07<08:13,  1.76s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<07:59,  1.71s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<08:41,  1.87s/it]predicting train subjects:   2%|▏         | 7/285 [00:13<09:15,  2.00s/it]predicting train subjects:   3%|▎         | 8/285 [00:15<09:17,  2.01s/it]predicting train subjects:   3%|▎         | 9/285 [00:17<09:03,  1.97s/it]predicting train subjects:   4%|▎         | 10/285 [00:19<09:29,  2.07s/it]predicting train subjects:   4%|▍         | 11/285 [00:21<09:36,  2.10s/it]predicting train subjects:   4%|▍         | 12/285 [00:23<09:49,  2.16s/it]predicting train subjects:   5%|▍         | 13/285 [00:26<09:51,  2.18s/it]predicting train subjects:   5%|▍         | 14/285 [00:28<09:53,  2.19s/it]predicting train subjects:   5%|▌         | 15/285 [00:30<09:50,  2.19s/it]predicting train subjects:   6%|▌         | 16/285 [00:32<09:52,  2.20s/it]predicting train subjects:   6%|▌         | 17/285 [00:34<09:56,  2.22s/it]predicting train subjects:   6%|▋         | 18/285 [00:37<09:59,  2.25s/it]predicting train subjects:   7%|▋         | 19/285 [00:39<09:57,  2.25s/it]predicting train subjects:   7%|▋         | 20/285 [00:41<09:59,  2.26s/it]predicting train subjects:   7%|▋         | 21/285 [00:44<09:57,  2.26s/it]predicting train subjects:   8%|▊         | 22/285 [00:46<09:50,  2.24s/it]predicting train subjects:   8%|▊         | 23/285 [00:48<09:50,  2.25s/it]predicting train subjects:   8%|▊         | 24/285 [00:50<09:44,  2.24s/it]predicting train subjects:   9%|▉         | 25/285 [00:52<09:38,  2.22s/it]predicting train subjects:   9%|▉         | 26/285 [00:55<09:39,  2.24s/it]predicting train subjects:   9%|▉         | 27/285 [00:57<09:39,  2.25s/it]predicting train subjects:  10%|▉         | 28/285 [00:59<09:23,  2.19s/it]predicting train subjects:  10%|█         | 29/285 [01:01<09:08,  2.14s/it]predicting train subjects:  11%|█         | 30/285 [01:03<08:51,  2.09s/it]predicting train subjects:  11%|█         | 31/285 [01:05<08:43,  2.06s/it]predicting train subjects:  11%|█         | 32/285 [01:07<08:36,  2.04s/it]predicting train subjects:  12%|█▏        | 33/285 [01:09<08:32,  2.03s/it]predicting train subjects:  12%|█▏        | 34/285 [01:11<08:30,  2.03s/it]predicting train subjects:  12%|█▏        | 35/285 [01:13<08:24,  2.02s/it]predicting train subjects:  13%|█▎        | 36/285 [01:15<08:30,  2.05s/it]predicting train subjects:  13%|█▎        | 37/285 [01:17<08:22,  2.03s/it]predicting train subjects:  13%|█▎        | 38/285 [01:19<08:19,  2.02s/it]predicting train subjects:  14%|█▎        | 39/285 [01:21<08:15,  2.01s/it]predicting train subjects:  14%|█▍        | 40/285 [01:23<08:13,  2.02s/it]predicting train subjects:  14%|█▍        | 41/285 [01:25<08:14,  2.03s/it]predicting train subjects:  15%|█▍        | 42/285 [01:27<08:13,  2.03s/it]predicting train subjects:  15%|█▌        | 43/285 [01:29<08:11,  2.03s/it]predicting train subjects:  15%|█▌        | 44/285 [01:31<08:10,  2.04s/it]predicting train subjects:  16%|█▌        | 45/285 [01:33<08:08,  2.04s/it]predicting train subjects:  16%|█▌        | 46/285 [01:35<07:49,  1.96s/it]predicting train subjects:  16%|█▋        | 47/285 [01:37<07:32,  1.90s/it]predicting train subjects:  17%|█▋        | 48/285 [01:39<07:19,  1.85s/it]predicting train subjects:  17%|█▋        | 49/285 [01:40<07:12,  1.83s/it]predicting train subjects:  18%|█▊        | 50/285 [01:42<07:06,  1.82s/it]predicting train subjects:  18%|█▊        | 51/285 [01:44<06:57,  1.79s/it]predicting train subjects:  18%|█▊        | 52/285 [01:46<06:53,  1.78s/it]predicting train subjects:  19%|█▊        | 53/285 [01:47<06:49,  1.77s/it]predicting train subjects:  19%|█▉        | 54/285 [01:49<06:49,  1.77s/it]predicting train subjects:  19%|█▉        | 55/285 [01:51<06:50,  1.79s/it]predicting train subjects:  20%|█▉        | 56/285 [01:53<06:49,  1.79s/it]predicting train subjects:  20%|██        | 57/285 [01:55<06:44,  1.77s/it]predicting train subjects:  20%|██        | 58/285 [01:56<06:44,  1.78s/it]predicting train subjects:  21%|██        | 59/285 [01:58<06:48,  1.81s/it]predicting train subjects:  21%|██        | 60/285 [02:00<06:41,  1.78s/it]predicting train subjects:  21%|██▏       | 61/285 [02:02<06:40,  1.79s/it]predicting train subjects:  22%|██▏       | 62/285 [02:04<06:49,  1.84s/it]predicting train subjects:  22%|██▏       | 63/285 [02:05<06:39,  1.80s/it]predicting train subjects:  22%|██▏       | 64/285 [02:07<06:46,  1.84s/it]predicting train subjects:  23%|██▎       | 65/285 [02:09<06:59,  1.91s/it]predicting train subjects:  23%|██▎       | 66/285 [02:12<07:09,  1.96s/it]predicting train subjects:  24%|██▎       | 67/285 [02:13<07:02,  1.94s/it]predicting train subjects:  24%|██▍       | 68/285 [02:15<06:50,  1.89s/it]predicting train subjects:  24%|██▍       | 69/285 [02:17<06:48,  1.89s/it]predicting train subjects:  25%|██▍       | 70/285 [02:19<06:41,  1.87s/it]predicting train subjects:  25%|██▍       | 71/285 [02:21<06:34,  1.84s/it]predicting train subjects:  25%|██▌       | 72/285 [02:22<06:27,  1.82s/it]predicting train subjects:  26%|██▌       | 73/285 [02:24<06:19,  1.79s/it]predicting train subjects:  26%|██▌       | 74/285 [02:26<06:15,  1.78s/it]predicting train subjects:  26%|██▋       | 75/285 [02:28<06:11,  1.77s/it]predicting train subjects:  27%|██▋       | 76/285 [02:30<06:16,  1.80s/it]predicting train subjects:  27%|██▋       | 77/285 [02:31<06:11,  1.79s/it]predicting train subjects:  27%|██▋       | 78/285 [02:33<06:15,  1.82s/it]predicting train subjects:  28%|██▊       | 79/285 [02:35<06:10,  1.80s/it]predicting train subjects:  28%|██▊       | 80/285 [02:37<06:13,  1.82s/it]predicting train subjects:  28%|██▊       | 81/285 [02:39<06:13,  1.83s/it]predicting train subjects:  29%|██▉       | 82/285 [02:41<06:11,  1.83s/it]predicting train subjects:  29%|██▉       | 83/285 [02:42<06:09,  1.83s/it]predicting train subjects:  29%|██▉       | 84/285 [02:44<06:07,  1.83s/it]predicting train subjects:  30%|██▉       | 85/285 [02:46<06:15,  1.88s/it]predicting train subjects:  30%|███       | 86/285 [02:48<06:21,  1.92s/it]predicting train subjects:  31%|███       | 87/285 [02:50<06:22,  1.93s/it]predicting train subjects:  31%|███       | 88/285 [02:52<06:19,  1.93s/it]predicting train subjects:  31%|███       | 89/285 [02:54<06:22,  1.95s/it]predicting train subjects:  32%|███▏      | 90/285 [02:56<06:22,  1.96s/it]predicting train subjects:  32%|███▏      | 91/285 [02:58<06:23,  1.97s/it]predicting train subjects:  32%|███▏      | 92/285 [03:00<06:22,  1.98s/it]predicting train subjects:  33%|███▎      | 93/285 [03:02<06:28,  2.03s/it]predicting train subjects:  33%|███▎      | 94/285 [03:04<06:27,  2.03s/it]predicting train subjects:  33%|███▎      | 95/285 [03:06<06:22,  2.02s/it]predicting train subjects:  34%|███▎      | 96/285 [03:08<06:21,  2.02s/it]predicting train subjects:  34%|███▍      | 97/285 [03:10<06:19,  2.02s/it]predicting train subjects:  34%|███▍      | 98/285 [03:12<06:15,  2.01s/it]predicting train subjects:  35%|███▍      | 99/285 [03:14<06:12,  2.00s/it]predicting train subjects:  35%|███▌      | 100/285 [03:16<06:14,  2.03s/it]predicting train subjects:  35%|███▌      | 101/285 [03:18<06:18,  2.06s/it]predicting train subjects:  36%|███▌      | 102/285 [03:20<06:12,  2.04s/it]predicting train subjects:  36%|███▌      | 103/285 [03:22<06:09,  2.03s/it]predicting train subjects:  36%|███▋      | 104/285 [03:24<06:05,  2.02s/it]predicting train subjects:  37%|███▋      | 105/285 [03:26<05:59,  2.00s/it]predicting train subjects:  37%|███▋      | 106/285 [03:28<05:56,  1.99s/it]predicting train subjects:  38%|███▊      | 107/285 [03:30<05:54,  1.99s/it]predicting train subjects:  38%|███▊      | 108/285 [03:32<06:01,  2.04s/it]predicting train subjects:  38%|███▊      | 109/285 [03:34<05:54,  2.01s/it]predicting train subjects:  39%|███▊      | 110/285 [03:37<05:57,  2.04s/it]predicting train subjects:  39%|███▉      | 111/285 [03:39<05:57,  2.06s/it]predicting train subjects:  39%|███▉      | 112/285 [03:41<05:54,  2.05s/it]predicting train subjects:  40%|███▉      | 113/285 [03:43<05:43,  2.00s/it]predicting train subjects:  40%|████      | 114/285 [03:45<05:40,  1.99s/it]predicting train subjects:  40%|████      | 115/285 [03:47<05:38,  1.99s/it]predicting train subjects:  41%|████      | 116/285 [03:49<05:38,  2.00s/it]predicting train subjects:  41%|████      | 117/285 [03:51<05:35,  2.00s/it]predicting train subjects:  41%|████▏     | 118/285 [03:53<05:34,  2.00s/it]predicting train subjects:  42%|████▏     | 119/285 [03:54<05:27,  1.97s/it]predicting train subjects:  42%|████▏     | 120/285 [03:56<05:21,  1.95s/it]predicting train subjects:  42%|████▏     | 121/285 [03:58<05:07,  1.88s/it]predicting train subjects:  43%|████▎     | 122/285 [04:00<04:51,  1.79s/it]predicting train subjects:  43%|████▎     | 123/285 [04:01<04:35,  1.70s/it]predicting train subjects:  44%|████▎     | 124/285 [04:03<04:33,  1.70s/it]predicting train subjects:  44%|████▍     | 125/285 [04:05<04:33,  1.71s/it]predicting train subjects:  44%|████▍     | 126/285 [04:06<04:30,  1.70s/it]predicting train subjects:  45%|████▍     | 127/285 [04:08<04:31,  1.72s/it]predicting train subjects:  45%|████▍     | 128/285 [04:10<04:29,  1.72s/it]predicting train subjects:  45%|████▌     | 129/285 [04:12<04:32,  1.74s/it]predicting train subjects:  46%|████▌     | 130/285 [04:13<04:32,  1.76s/it]predicting train subjects:  46%|████▌     | 131/285 [04:15<04:27,  1.74s/it]predicting train subjects:  46%|████▋     | 132/285 [04:17<04:27,  1.75s/it]predicting train subjects:  47%|████▋     | 133/285 [04:18<04:24,  1.74s/it]predicting train subjects:  47%|████▋     | 134/285 [04:20<04:20,  1.73s/it]predicting train subjects:  47%|████▋     | 135/285 [04:22<04:17,  1.72s/it]predicting train subjects:  48%|████▊     | 136/285 [04:24<04:18,  1.74s/it]predicting train subjects:  48%|████▊     | 137/285 [04:25<04:15,  1.73s/it]predicting train subjects:  48%|████▊     | 138/285 [04:27<04:16,  1.74s/it]predicting train subjects:  49%|████▉     | 139/285 [04:29<04:14,  1.75s/it]predicting train subjects:  49%|████▉     | 140/285 [04:31<04:16,  1.77s/it]predicting train subjects:  49%|████▉     | 141/285 [04:32<04:15,  1.77s/it]predicting train subjects:  50%|████▉     | 142/285 [04:34<04:09,  1.75s/it]predicting train subjects:  50%|█████     | 143/285 [04:36<04:07,  1.74s/it]predicting train subjects:  51%|█████     | 144/285 [04:38<04:01,  1.71s/it]predicting train subjects:  51%|█████     | 145/285 [04:39<03:56,  1.69s/it]predicting train subjects:  51%|█████     | 146/285 [04:41<03:52,  1.67s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:42<03:50,  1.67s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:44<03:41,  1.62s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:46<03:38,  1.60s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:47<03:40,  1.63s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:49<03:37,  1.62s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:50<03:32,  1.60s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:52<03:31,  1.60s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:54<03:28,  1.60s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:55<03:24,  1.58s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:57<03:27,  1.61s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:58<03:25,  1.61s/it]predicting train subjects:  55%|█████▌    | 158/285 [05:00<03:24,  1.61s/it]predicting train subjects:  56%|█████▌    | 159/285 [05:02<03:23,  1.62s/it]predicting train subjects:  56%|█████▌    | 160/285 [05:03<03:20,  1.60s/it]predicting train subjects:  56%|█████▋    | 161/285 [05:05<03:21,  1.62s/it]predicting train subjects:  57%|█████▋    | 162/285 [05:07<03:21,  1.64s/it]predicting train subjects:  57%|█████▋    | 163/285 [05:08<03:17,  1.62s/it]predicting train subjects:  58%|█████▊    | 164/285 [05:10<03:16,  1.62s/it]predicting train subjects:  58%|█████▊    | 165/285 [05:11<03:15,  1.63s/it]predicting train subjects:  58%|█████▊    | 166/285 [05:13<03:13,  1.63s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:15<03:11,  1.62s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:16<03:09,  1.62s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:18<03:08,  1.62s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:19<03:03,  1.59s/it]predicting train subjects:  60%|██████    | 171/285 [05:21<03:04,  1.62s/it]predicting train subjects:  60%|██████    | 172/285 [05:23<02:59,  1.59s/it]predicting train subjects:  61%|██████    | 173/285 [05:24<02:56,  1.57s/it]predicting train subjects:  61%|██████    | 174/285 [05:26<02:53,  1.57s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:27<02:52,  1.56s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:29<02:54,  1.60s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:30<02:48,  1.56s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:32<02:46,  1.56s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:34<02:45,  1.56s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:35<02:40,  1.53s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:37<02:38,  1.52s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:38<02:39,  1.55s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:40<02:36,  1.53s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:41<02:34,  1.53s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:43<02:32,  1.52s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:44<02:30,  1.52s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:46<02:28,  1.51s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:47<02:26,  1.51s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:49<02:28,  1.54s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:50<02:27,  1.55s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:52<02:23,  1.52s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:53<02:22,  1.53s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:55<02:19,  1.51s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:56<02:17,  1.51s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:58<02:17,  1.52s/it]predicting train subjects:  69%|██████▉   | 196/285 [06:00<02:22,  1.60s/it]predicting train subjects:  69%|██████▉   | 197/285 [06:01<02:25,  1.65s/it]predicting train subjects:  69%|██████▉   | 198/285 [06:03<02:26,  1.69s/it]predicting train subjects:  70%|██████▉   | 199/285 [06:05<02:26,  1.71s/it]predicting train subjects:  70%|███████   | 200/285 [06:07<02:26,  1.73s/it]predicting train subjects:  71%|███████   | 201/285 [06:09<02:31,  1.80s/it]predicting train subjects:  71%|███████   | 202/285 [06:10<02:28,  1.79s/it]predicting train subjects:  71%|███████   | 203/285 [06:12<02:27,  1.80s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:14<02:26,  1.81s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:16<02:25,  1.82s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:18<02:23,  1.82s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:20<02:20,  1.80s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:21<02:19,  1.81s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:23<02:17,  1.81s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:25<02:16,  1.82s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:27<02:15,  1.83s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:29<02:17,  1.88s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:31<02:14,  1.87s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:32<02:09,  1.82s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:34<02:02,  1.75s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:36<01:57,  1.70s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:37<01:52,  1.66s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:39<01:50,  1.64s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:40<01:47,  1.63s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:42<01:43,  1.59s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:43<01:41,  1.59s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:45<01:39,  1.59s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:47<01:39,  1.61s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:48<01:36,  1.58s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:50<01:34,  1.57s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:51<01:32,  1.57s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:53<01:30,  1.56s/it]predicting train subjects:  80%|████████  | 228/285 [06:54<01:29,  1.57s/it]predicting train subjects:  80%|████████  | 229/285 [06:56<01:27,  1.57s/it]predicting train subjects:  81%|████████  | 230/285 [06:58<01:27,  1.59s/it]predicting train subjects:  81%|████████  | 231/285 [06:59<01:26,  1.60s/it]predicting train subjects:  81%|████████▏ | 232/285 [07:01<01:29,  1.69s/it]predicting train subjects:  82%|████████▏ | 233/285 [07:03<01:31,  1.76s/it]predicting train subjects:  82%|████████▏ | 234/285 [07:05<01:31,  1.80s/it]predicting train subjects:  82%|████████▏ | 235/285 [07:07<01:32,  1.86s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:09<01:32,  1.89s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:11<01:32,  1.92s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:13<01:30,  1.93s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:15<01:28,  1.92s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:17<01:26,  1.92s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:19<01:24,  1.91s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:20<01:22,  1.91s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:23<01:22,  1.96s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:25<01:20,  1.96s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:26<01:18,  1.96s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:28<01:16,  1.96s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:30<01:14,  1.95s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:32<01:11,  1.93s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:34<01:09,  1.93s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:36<01:04,  1.84s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:37<00:59,  1.74s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:39<00:55,  1.67s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:40<00:52,  1.64s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:42<00:50,  1.63s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:43<00:47,  1.58s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:45<00:45,  1.57s/it]predicting train subjects:  90%|█████████ | 257/285 [07:47<00:43,  1.55s/it]predicting train subjects:  91%|█████████ | 258/285 [07:48<00:41,  1.55s/it]predicting train subjects:  91%|█████████ | 259/285 [07:50<00:40,  1.54s/it]predicting train subjects:  91%|█████████ | 260/285 [07:51<00:39,  1.58s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:53<00:37,  1.58s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:54<00:35,  1.55s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:56<00:34,  1.56s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:57<00:32,  1.55s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:59<00:31,  1.55s/it]predicting train subjects:  93%|█████████▎| 266/285 [08:01<00:29,  1.57s/it]predicting train subjects:  94%|█████████▎| 267/285 [08:02<00:28,  1.58s/it]predicting train subjects:  94%|█████████▍| 268/285 [08:04<00:28,  1.70s/it]predicting train subjects:  94%|█████████▍| 269/285 [08:06<00:28,  1.79s/it]predicting train subjects:  95%|█████████▍| 270/285 [08:08<00:27,  1.83s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:10<00:25,  1.83s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:12<00:24,  1.87s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:14<00:23,  1.96s/it]predicting train subjects:  96%|█████████▌| 274/285 [08:16<00:21,  1.96s/it]predicting train subjects:  96%|█████████▋| 275/285 [08:18<00:19,  1.97s/it]predicting train subjects:  97%|█████████▋| 276/285 [08:20<00:17,  1.95s/it]predicting train subjects:  97%|█████████▋| 277/285 [08:22<00:15,  1.96s/it]predicting train subjects:  98%|█████████▊| 278/285 [08:24<00:13,  1.97s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:26<00:12,  2.00s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:28<00:10,  2.01s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:30<00:08,  2.00s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:32<00:06,  2.01s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:34<00:04,  2.01s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:36<00:02,  2.02s/it]predicting train subjects: 100%|██████████| 285/285 [08:38<00:00,  2.00s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:44,  1.42s/it]Loading train:   1%|          | 2/285 [00:02<06:49,  1.45s/it]Loading train:   1%|          | 3/285 [00:04<06:27,  1.37s/it]Loading train:   1%|▏         | 4/285 [00:05<06:43,  1.43s/it]Loading train:   2%|▏         | 5/285 [00:06<06:16,  1.34s/it]Loading train:   2%|▏         | 6/285 [00:08<06:30,  1.40s/it]Loading train:   2%|▏         | 7/285 [00:09<06:40,  1.44s/it]Loading train:   3%|▎         | 8/285 [00:11<06:48,  1.48s/it]Loading train:   3%|▎         | 9/285 [00:12<06:41,  1.45s/it]Loading train:   4%|▎         | 10/285 [00:14<06:20,  1.38s/it]Loading train:   4%|▍         | 11/285 [00:14<05:38,  1.24s/it]Loading train:   4%|▍         | 12/285 [00:16<05:22,  1.18s/it]Loading train:   5%|▍         | 13/285 [00:17<05:35,  1.23s/it]Loading train:   5%|▍         | 14/285 [00:18<05:20,  1.18s/it]Loading train:   5%|▌         | 15/285 [00:19<05:13,  1.16s/it]Loading train:   6%|▌         | 16/285 [00:20<05:00,  1.12s/it]Loading train:   6%|▌         | 17/285 [00:21<05:02,  1.13s/it]Loading train:   6%|▋         | 18/285 [00:22<04:59,  1.12s/it]Loading train:   7%|▋         | 19/285 [00:23<04:48,  1.08s/it]Loading train:   7%|▋         | 20/285 [00:24<04:48,  1.09s/it]Loading train:   7%|▋         | 21/285 [00:25<04:33,  1.04s/it]Loading train:   8%|▊         | 22/285 [00:26<04:32,  1.04s/it]Loading train:   8%|▊         | 23/285 [00:27<04:22,  1.00s/it]Loading train:   8%|▊         | 24/285 [00:28<04:29,  1.03s/it]Loading train:   9%|▉         | 25/285 [00:29<04:32,  1.05s/it]Loading train:   9%|▉         | 26/285 [00:31<04:38,  1.07s/it]Loading train:   9%|▉         | 27/285 [00:32<04:36,  1.07s/it]Loading train:  10%|▉         | 28/285 [00:33<05:08,  1.20s/it]Loading train:  10%|█         | 29/285 [00:34<05:08,  1.21s/it]Loading train:  11%|█         | 30/285 [00:36<05:00,  1.18s/it]Loading train:  11%|█         | 31/285 [00:37<04:46,  1.13s/it]Loading train:  11%|█         | 32/285 [00:38<04:49,  1.14s/it]Loading train:  12%|█▏        | 33/285 [00:39<04:43,  1.12s/it]Loading train:  12%|█▏        | 34/285 [00:40<04:43,  1.13s/it]Loading train:  12%|█▏        | 35/285 [00:41<04:31,  1.09s/it]Loading train:  13%|█▎        | 36/285 [00:42<04:20,  1.05s/it]Loading train:  13%|█▎        | 37/285 [00:43<04:29,  1.08s/it]Loading train:  13%|█▎        | 38/285 [00:44<04:12,  1.02s/it]Loading train:  14%|█▎        | 39/285 [00:45<04:22,  1.07s/it]Loading train:  14%|█▍        | 40/285 [00:46<04:20,  1.06s/it]Loading train:  14%|█▍        | 41/285 [00:47<04:13,  1.04s/it]Loading train:  15%|█▍        | 42/285 [00:48<04:22,  1.08s/it]Loading train:  15%|█▌        | 43/285 [00:49<04:12,  1.05s/it]Loading train:  15%|█▌        | 44/285 [00:50<04:20,  1.08s/it]Loading train:  16%|█▌        | 45/285 [00:51<04:14,  1.06s/it]Loading train:  16%|█▌        | 46/285 [00:53<04:16,  1.07s/it]Loading train:  16%|█▋        | 47/285 [00:53<03:57,  1.00it/s]Loading train:  17%|█▋        | 48/285 [00:54<03:58,  1.01s/it]Loading train:  17%|█▋        | 49/285 [00:55<03:50,  1.02it/s]Loading train:  18%|█▊        | 50/285 [00:56<03:40,  1.06it/s]Loading train:  18%|█▊        | 51/285 [00:57<03:52,  1.01it/s]Loading train:  18%|█▊        | 52/285 [00:58<03:50,  1.01it/s]Loading train:  19%|█▊        | 53/285 [00:59<04:02,  1.05s/it]Loading train:  19%|█▉        | 54/285 [01:00<03:43,  1.03it/s]Loading train:  19%|█▉        | 55/285 [01:01<03:42,  1.03it/s]Loading train:  20%|█▉        | 56/285 [01:02<03:32,  1.08it/s]Loading train:  20%|██        | 57/285 [01:03<03:39,  1.04it/s]Loading train:  20%|██        | 58/285 [01:04<03:32,  1.07it/s]Loading train:  21%|██        | 59/285 [01:05<03:26,  1.10it/s]Loading train:  21%|██        | 60/285 [01:06<03:36,  1.04it/s]Loading train:  21%|██▏       | 61/285 [01:07<03:31,  1.06it/s]Loading train:  22%|██▏       | 62/285 [01:08<03:21,  1.11it/s]Loading train:  22%|██▏       | 63/285 [01:09<03:36,  1.02it/s]Loading train:  22%|██▏       | 64/285 [01:10<03:58,  1.08s/it]Loading train:  23%|██▎       | 65/285 [01:12<04:36,  1.26s/it]Loading train:  23%|██▎       | 66/285 [01:13<04:26,  1.22s/it]Loading train:  24%|██▎       | 67/285 [01:14<04:06,  1.13s/it]Loading train:  24%|██▍       | 68/285 [01:15<03:42,  1.02s/it]Loading train:  24%|██▍       | 69/285 [01:16<03:43,  1.03s/it]Loading train:  25%|██▍       | 70/285 [01:17<03:33,  1.01it/s]Loading train:  25%|██▍       | 71/285 [01:18<03:33,  1.00it/s]Loading train:  25%|██▌       | 72/285 [01:18<03:25,  1.04it/s]Loading train:  26%|██▌       | 73/285 [01:19<03:13,  1.10it/s]Loading train:  26%|██▌       | 74/285 [01:20<03:12,  1.10it/s]Loading train:  26%|██▋       | 75/285 [01:21<03:03,  1.14it/s]Loading train:  27%|██▋       | 76/285 [01:22<03:04,  1.13it/s]Loading train:  27%|██▋       | 77/285 [01:23<03:07,  1.11it/s]Loading train:  27%|██▋       | 78/285 [01:24<03:09,  1.09it/s]Loading train:  28%|██▊       | 79/285 [01:25<03:06,  1.11it/s]Loading train:  28%|██▊       | 80/285 [01:25<03:06,  1.10it/s]Loading train:  28%|██▊       | 81/285 [01:26<03:06,  1.10it/s]Loading train:  29%|██▉       | 82/285 [01:27<03:00,  1.12it/s]Loading train:  29%|██▉       | 83/285 [01:28<02:52,  1.17it/s]Loading train:  29%|██▉       | 84/285 [01:29<03:08,  1.06it/s]Loading train:  30%|██▉       | 85/285 [01:30<03:16,  1.02it/s]Loading train:  30%|███       | 86/285 [01:31<03:24,  1.03s/it]Loading train:  31%|███       | 87/285 [01:33<03:30,  1.06s/it]Loading train:  31%|███       | 88/285 [01:34<03:25,  1.04s/it]Loading train:  31%|███       | 89/285 [01:36<04:23,  1.34s/it]Loading train:  32%|███▏      | 90/285 [01:37<04:02,  1.24s/it]Loading train:  32%|███▏      | 91/285 [01:38<03:45,  1.16s/it]Loading train:  32%|███▏      | 92/285 [01:39<03:53,  1.21s/it]Loading train:  33%|███▎      | 93/285 [01:40<03:33,  1.11s/it]Loading train:  33%|███▎      | 94/285 [01:41<03:31,  1.11s/it]Loading train:  33%|███▎      | 95/285 [01:42<03:25,  1.08s/it]Loading train:  34%|███▎      | 96/285 [01:43<03:23,  1.07s/it]Loading train:  34%|███▍      | 97/285 [01:44<03:39,  1.17s/it]Loading train:  34%|███▍      | 98/285 [01:45<03:19,  1.07s/it]Loading train:  35%|███▍      | 99/285 [01:46<03:25,  1.10s/it]Loading train:  35%|███▌      | 100/285 [01:47<03:19,  1.08s/it]Loading train:  35%|███▌      | 101/285 [01:48<03:15,  1.06s/it]Loading train:  36%|███▌      | 102/285 [01:49<03:14,  1.07s/it]Loading train:  36%|███▌      | 103/285 [01:50<03:10,  1.04s/it]Loading train:  36%|███▋      | 104/285 [01:51<03:03,  1.02s/it]Loading train:  37%|███▋      | 105/285 [01:52<03:00,  1.00s/it]Loading train:  37%|███▋      | 106/285 [01:53<02:56,  1.02it/s]Loading train:  38%|███▊      | 107/285 [01:55<03:10,  1.07s/it]Loading train:  38%|███▊      | 108/285 [01:56<03:22,  1.15s/it]Loading train:  38%|███▊      | 109/285 [01:57<03:20,  1.14s/it]Loading train:  39%|███▊      | 110/285 [01:58<03:24,  1.17s/it]Loading train:  39%|███▉      | 111/285 [02:00<03:29,  1.21s/it]Loading train:  39%|███▉      | 112/285 [02:01<03:24,  1.18s/it]Loading train:  40%|███▉      | 113/285 [02:02<03:16,  1.14s/it]Loading train:  40%|████      | 114/285 [02:03<03:15,  1.15s/it]Loading train:  40%|████      | 115/285 [02:04<03:15,  1.15s/it]Loading train:  41%|████      | 116/285 [02:05<03:09,  1.12s/it]Loading train:  41%|████      | 117/285 [02:06<03:08,  1.12s/it]Loading train:  41%|████▏     | 118/285 [02:07<03:07,  1.12s/it]Loading train:  42%|████▏     | 119/285 [02:08<03:00,  1.09s/it]Loading train:  42%|████▏     | 120/285 [02:09<02:55,  1.07s/it]Loading train:  42%|████▏     | 121/285 [02:11<03:15,  1.19s/it]Loading train:  43%|████▎     | 122/285 [02:12<03:13,  1.19s/it]Loading train:  43%|████▎     | 123/285 [02:13<03:20,  1.24s/it]Loading train:  44%|████▎     | 124/285 [02:14<03:11,  1.19s/it]Loading train:  44%|████▍     | 125/285 [02:16<03:07,  1.17s/it]Loading train:  44%|████▍     | 126/285 [02:16<02:52,  1.09s/it]Loading train:  45%|████▍     | 127/285 [02:18<03:03,  1.16s/it]Loading train:  45%|████▍     | 128/285 [02:19<02:56,  1.12s/it]Loading train:  45%|████▌     | 129/285 [02:20<02:43,  1.05s/it]Loading train:  46%|████▌     | 130/285 [02:21<02:49,  1.09s/it]Loading train:  46%|████▌     | 131/285 [02:22<02:34,  1.00s/it]Loading train:  46%|████▋     | 132/285 [02:23<02:27,  1.04it/s]Loading train:  47%|████▋     | 133/285 [02:24<02:30,  1.01it/s]Loading train:  47%|████▋     | 134/285 [02:25<02:36,  1.03s/it]Loading train:  47%|████▋     | 135/285 [02:26<02:42,  1.08s/it]Loading train:  48%|████▊     | 136/285 [02:27<02:36,  1.05s/it]Loading train:  48%|████▊     | 137/285 [02:28<02:24,  1.03it/s]Loading train:  48%|████▊     | 138/285 [02:29<02:25,  1.01it/s]Loading train:  49%|████▉     | 139/285 [02:30<02:20,  1.04it/s]Loading train:  49%|████▉     | 140/285 [02:31<02:14,  1.08it/s]Loading train:  49%|████▉     | 141/285 [02:32<02:18,  1.04it/s]Loading train:  50%|████▉     | 142/285 [02:32<02:15,  1.06it/s]Loading train:  50%|█████     | 143/285 [02:33<02:12,  1.07it/s]Loading train:  51%|█████     | 144/285 [02:35<02:20,  1.00it/s]Loading train:  51%|█████     | 145/285 [02:35<02:13,  1.05it/s]Loading train:  51%|█████     | 146/285 [02:36<02:14,  1.03it/s]Loading train:  52%|█████▏    | 147/285 [02:37<02:18,  1.01s/it]Loading train:  52%|█████▏    | 148/285 [02:38<02:14,  1.02it/s]Loading train:  52%|█████▏    | 149/285 [02:39<02:05,  1.09it/s]Loading train:  53%|█████▎    | 150/285 [02:40<02:07,  1.06it/s]Loading train:  53%|█████▎    | 151/285 [02:41<02:06,  1.06it/s]Loading train:  53%|█████▎    | 152/285 [02:42<02:01,  1.10it/s]Loading train:  54%|█████▎    | 153/285 [02:43<02:00,  1.09it/s]Loading train:  54%|█████▍    | 154/285 [02:44<01:58,  1.11it/s]Loading train:  54%|█████▍    | 155/285 [02:45<02:07,  1.02it/s]Loading train:  55%|█████▍    | 156/285 [02:46<02:00,  1.07it/s]Loading train:  55%|█████▌    | 157/285 [02:47<02:00,  1.06it/s]Loading train:  55%|█████▌    | 158/285 [02:48<02:10,  1.03s/it]Loading train:  56%|█████▌    | 159/285 [02:49<02:07,  1.01s/it]Loading train:  56%|█████▌    | 160/285 [02:50<02:13,  1.07s/it]Loading train:  56%|█████▋    | 161/285 [02:51<02:08,  1.03s/it]Loading train:  57%|█████▋    | 162/285 [02:52<01:59,  1.03it/s]Loading train:  57%|█████▋    | 163/285 [02:53<01:54,  1.07it/s]Loading train:  58%|█████▊    | 164/285 [02:54<01:47,  1.13it/s]Loading train:  58%|█████▊    | 165/285 [02:55<01:55,  1.04it/s]Loading train:  58%|█████▊    | 166/285 [02:56<01:59,  1.01s/it]Loading train:  59%|█████▊    | 167/285 [02:57<01:57,  1.00it/s]Loading train:  59%|█████▉    | 168/285 [02:58<01:54,  1.03it/s]Loading train:  59%|█████▉    | 169/285 [02:59<01:57,  1.01s/it]Loading train:  60%|█████▉    | 170/285 [03:00<01:52,  1.02it/s]Loading train:  60%|██████    | 171/285 [03:01<01:49,  1.04it/s]Loading train:  60%|██████    | 172/285 [03:02<01:52,  1.00it/s]Loading train:  61%|██████    | 173/285 [03:03<01:53,  1.01s/it]Loading train:  61%|██████    | 174/285 [03:04<01:48,  1.02it/s]Loading train:  61%|██████▏   | 175/285 [03:04<01:42,  1.07it/s]Loading train:  62%|██████▏   | 176/285 [03:05<01:43,  1.05it/s]Loading train:  62%|██████▏   | 177/285 [03:06<01:40,  1.07it/s]Loading train:  62%|██████▏   | 178/285 [03:07<01:43,  1.03it/s]Loading train:  63%|██████▎   | 179/285 [03:08<01:34,  1.12it/s]Loading train:  63%|██████▎   | 180/285 [03:09<01:37,  1.08it/s]Loading train:  64%|██████▎   | 181/285 [03:10<01:36,  1.08it/s]Loading train:  64%|██████▍   | 182/285 [03:11<01:34,  1.09it/s]Loading train:  64%|██████▍   | 183/285 [03:12<01:35,  1.06it/s]Loading train:  65%|██████▍   | 184/285 [03:13<01:33,  1.08it/s]Loading train:  65%|██████▍   | 185/285 [03:14<01:33,  1.07it/s]Loading train:  65%|██████▌   | 186/285 [03:15<01:34,  1.05it/s]Loading train:  66%|██████▌   | 187/285 [03:16<01:29,  1.09it/s]Loading train:  66%|██████▌   | 188/285 [03:17<01:28,  1.10it/s]Loading train:  66%|██████▋   | 189/285 [03:17<01:27,  1.10it/s]Loading train:  67%|██████▋   | 190/285 [03:18<01:26,  1.09it/s]Loading train:  67%|██████▋   | 191/285 [03:19<01:21,  1.16it/s]Loading train:  67%|██████▋   | 192/285 [03:20<01:17,  1.19it/s]Loading train:  68%|██████▊   | 193/285 [03:21<01:22,  1.12it/s]Loading train:  68%|██████▊   | 194/285 [03:22<01:16,  1.20it/s]Loading train:  68%|██████▊   | 195/285 [03:23<01:17,  1.16it/s]Loading train:  69%|██████▉   | 196/285 [03:24<01:27,  1.02it/s]Loading train:  69%|██████▉   | 197/285 [03:25<01:26,  1.02it/s]Loading train:  69%|██████▉   | 198/285 [03:26<01:21,  1.06it/s]Loading train:  70%|██████▉   | 199/285 [03:27<01:27,  1.01s/it]Loading train:  70%|███████   | 200/285 [03:28<01:26,  1.02s/it]Loading train:  71%|███████   | 201/285 [03:29<01:31,  1.09s/it]Loading train:  71%|███████   | 202/285 [03:30<01:27,  1.05s/it]Loading train:  71%|███████   | 203/285 [03:31<01:24,  1.03s/it]Loading train:  72%|███████▏  | 204/285 [03:32<01:24,  1.04s/it]Loading train:  72%|███████▏  | 205/285 [03:33<01:20,  1.00s/it]Loading train:  72%|███████▏  | 206/285 [03:34<01:23,  1.05s/it]Loading train:  73%|███████▎  | 207/285 [03:35<01:18,  1.00s/it]Loading train:  73%|███████▎  | 208/285 [03:36<01:18,  1.02s/it]Loading train:  73%|███████▎  | 209/285 [03:37<01:21,  1.07s/it]Loading train:  74%|███████▎  | 210/285 [03:39<01:24,  1.13s/it]Loading train:  74%|███████▍  | 211/285 [03:40<01:20,  1.09s/it]Loading train:  74%|███████▍  | 212/285 [03:41<01:21,  1.11s/it]Loading train:  75%|███████▍  | 213/285 [03:42<01:14,  1.03s/it]Loading train:  75%|███████▌  | 214/285 [03:43<01:13,  1.04s/it]Loading train:  75%|███████▌  | 215/285 [03:43<01:05,  1.06it/s]Loading train:  76%|███████▌  | 216/285 [03:44<01:05,  1.05it/s]Loading train:  76%|███████▌  | 217/285 [03:45<01:03,  1.08it/s]Loading train:  76%|███████▋  | 218/285 [03:46<01:00,  1.11it/s]Loading train:  77%|███████▋  | 219/285 [03:47<00:59,  1.11it/s]Loading train:  77%|███████▋  | 220/285 [03:48<00:57,  1.12it/s]Loading train:  78%|███████▊  | 221/285 [03:49<00:59,  1.07it/s]Loading train:  78%|███████▊  | 222/285 [03:50<00:57,  1.09it/s]Loading train:  78%|███████▊  | 223/285 [03:51<00:54,  1.13it/s]Loading train:  79%|███████▊  | 224/285 [03:51<00:54,  1.11it/s]Loading train:  79%|███████▉  | 225/285 [03:53<00:56,  1.06it/s]Loading train:  79%|███████▉  | 226/285 [03:54<00:57,  1.02it/s]Loading train:  80%|███████▉  | 227/285 [03:54<00:55,  1.04it/s]Loading train:  80%|████████  | 228/285 [03:56<00:57,  1.02s/it]Loading train:  80%|████████  | 229/285 [03:57<00:59,  1.06s/it]Loading train:  81%|████████  | 230/285 [03:58<00:56,  1.03s/it]Loading train:  81%|████████  | 231/285 [03:59<00:54,  1.00s/it]Loading train:  81%|████████▏ | 232/285 [04:00<00:59,  1.12s/it]Loading train:  82%|████████▏ | 233/285 [04:01<00:58,  1.12s/it]Loading train:  82%|████████▏ | 234/285 [04:02<00:56,  1.11s/it]Loading train:  82%|████████▏ | 235/285 [04:04<00:58,  1.17s/it]Loading train:  83%|████████▎ | 236/285 [04:05<00:58,  1.19s/it]Loading train:  83%|████████▎ | 237/285 [04:06<00:54,  1.14s/it]Loading train:  84%|████████▎ | 238/285 [04:07<00:53,  1.14s/it]Loading train:  84%|████████▍ | 239/285 [04:08<00:52,  1.14s/it]Loading train:  84%|████████▍ | 240/285 [04:09<00:51,  1.15s/it]Loading train:  85%|████████▍ | 241/285 [04:11<00:51,  1.18s/it]Loading train:  85%|████████▍ | 242/285 [04:12<00:51,  1.21s/it]Loading train:  85%|████████▌ | 243/285 [04:13<00:50,  1.19s/it]Loading train:  86%|████████▌ | 244/285 [04:14<00:49,  1.20s/it]Loading train:  86%|████████▌ | 245/285 [04:15<00:46,  1.17s/it]Loading train:  86%|████████▋ | 246/285 [04:17<00:48,  1.23s/it]Loading train:  87%|████████▋ | 247/285 [04:18<00:44,  1.18s/it]Loading train:  87%|████████▋ | 248/285 [04:19<00:43,  1.17s/it]Loading train:  87%|████████▋ | 249/285 [04:20<00:43,  1.19s/it]Loading train:  88%|████████▊ | 250/285 [04:21<00:37,  1.08s/it]Loading train:  88%|████████▊ | 251/285 [04:22<00:34,  1.02s/it]Loading train:  88%|████████▊ | 252/285 [04:23<00:31,  1.04it/s]Loading train:  89%|████████▉ | 253/285 [04:24<00:33,  1.04s/it]Loading train:  89%|████████▉ | 254/285 [04:25<00:32,  1.03s/it]Loading train:  89%|████████▉ | 255/285 [04:26<00:30,  1.01s/it]Loading train:  90%|████████▉ | 256/285 [04:27<00:28,  1.01it/s]Loading train:  90%|█████████ | 257/285 [04:28<00:27,  1.02it/s]Loading train:  91%|█████████ | 258/285 [04:29<00:26,  1.03it/s]Loading train:  91%|█████████ | 259/285 [04:30<00:24,  1.06it/s]Loading train:  91%|█████████ | 260/285 [04:30<00:22,  1.13it/s]Loading train:  92%|█████████▏| 261/285 [04:31<00:20,  1.16it/s]Loading train:  92%|█████████▏| 262/285 [04:32<00:19,  1.15it/s]Loading train:  92%|█████████▏| 263/285 [04:33<00:18,  1.18it/s]Loading train:  93%|█████████▎| 264/285 [04:34<00:18,  1.16it/s]Loading train:  93%|█████████▎| 265/285 [04:35<00:17,  1.17it/s]Loading train:  93%|█████████▎| 266/285 [04:35<00:16,  1.16it/s]Loading train:  94%|█████████▎| 267/285 [04:36<00:15,  1.13it/s]Loading train:  94%|█████████▍| 268/285 [04:38<00:17,  1.00s/it]Loading train:  94%|█████████▍| 269/285 [04:39<00:16,  1.04s/it]Loading train:  95%|█████████▍| 270/285 [04:40<00:15,  1.02s/it]Loading train:  95%|█████████▌| 271/285 [04:41<00:14,  1.03s/it]Loading train:  95%|█████████▌| 272/285 [04:42<00:13,  1.07s/it]Loading train:  96%|█████████▌| 273/285 [04:43<00:12,  1.06s/it]Loading train:  96%|█████████▌| 274/285 [04:44<00:11,  1.09s/it]Loading train:  96%|█████████▋| 275/285 [04:46<00:12,  1.21s/it]Loading train:  97%|█████████▋| 276/285 [04:47<00:10,  1.19s/it]Loading train:  97%|█████████▋| 277/285 [04:48<00:09,  1.24s/it]Loading train:  98%|█████████▊| 278/285 [04:49<00:08,  1.18s/it]Loading train:  98%|█████████▊| 279/285 [04:50<00:07,  1.19s/it]Loading train:  98%|█████████▊| 280/285 [04:51<00:05,  1.13s/it]Loading train:  99%|█████████▊| 281/285 [04:52<00:04,  1.10s/it]Loading train:  99%|█████████▉| 282/285 [04:54<00:03,  1.08s/it]Loading train:  99%|█████████▉| 283/285 [04:55<00:02,  1.10s/it]Loading train: 100%|█████████▉| 284/285 [04:56<00:01,  1.10s/it]Loading train: 100%|██████████| 285/285 [04:57<00:00,  1.09s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 19/285 [00:00<00:01, 178.71it/s]concatenating: train:  16%|█▌        | 45/285 [00:00<00:01, 195.79it/s]concatenating: train:  21%|██▏       | 61/285 [00:00<00:01, 182.35it/s]concatenating: train:  32%|███▏      | 92/285 [00:00<00:00, 207.17it/s]concatenating: train:  42%|████▏     | 120/285 [00:00<00:00, 224.12it/s]concatenating: train:  49%|████▉     | 141/285 [00:00<00:00, 146.77it/s]concatenating: train:  58%|█████▊    | 165/285 [00:00<00:00, 165.18it/s]concatenating: train:  68%|██████▊   | 195/285 [00:00<00:00, 189.80it/s]concatenating: train:  76%|███████▋  | 218/285 [00:01<00:00, 200.14it/s]concatenating: train:  86%|████████▌ | 244/285 [00:01<00:00, 214.16it/s]concatenating: train:  94%|█████████▍| 269/285 [00:01<00:00, 223.10it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 214.19it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.54s/it]Loading test:  67%|██████▋   | 2/3 [00:03<00:01,  1.62s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.56s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 615.54it/s]2019-07-06 18:35:44.726401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 18:35:44.726507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 18:35:44.726522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 18:35:44.726531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 18:35:44.726996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:10,  3.71it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:08,  4.40it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.18it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:05,  5.37it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:06,  4.63it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.23it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:06,  4.41it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:04,  5.62it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.13it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.74it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:04,  4.60it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:03,  5.64it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.17it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  4.70it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:04<00:02,  5.21it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  4.72it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  5.73it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  6.04it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:05<00:01,  4.85it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  5.18it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  4.65it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  6.91it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 80, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 80, 52, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 80, 52, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 80, 52, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 80, 52, 10)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 80, 52, 10)   910         dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 80, 52, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 80, 52, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 80, 52, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 80, 52, 10)   910         dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 80, 52, 10)   40          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 80, 52, 10)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 80, 52, 10)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 80, 52, 10)   910         dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 80, 52, 10)   40          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 80, 52, 10)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 80, 52, 10)   910         activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 80, 52, 10)   40          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 80, 52, 10)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 40, 26, 10)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 40, 26, 10)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 40, 26, 20)   1820        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 40, 26, 20)   80          conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 40, 26, 20)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 40, 26, 20)   3620        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 40, 26, 20)   80          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 40, 26, 20)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 20, 13, 20)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 20, 13, 20)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 20, 13, 40)   7240        dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 20, 13, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 20, 13, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 13, 40)   14440       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 20, 13, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 20, 13, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 20, 13, 40)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 40, 26, 20)   3220        dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 40, 26, 40)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 40, 26, 20)   7220        concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 40, 26, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 40, 26, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 40, 26, 20)   3620        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 40, 26, 20)   80          conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 40, 26, 20)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 40, 26, 20)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 80, 52, 10)   810         dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 80, 52, 20)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 80, 52, 10)   1810        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 80, 52, 10)   40          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 80, 52, 10)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 80, 52, 10)   910         activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 80, 52, 10)   40          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 80, 52, 10)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 80, 52, 10)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 80, 52, 10)   910         dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 80, 52, 10)   40          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 80, 52, 10)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 80, 52, 10)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 80, 52, 13)   143         dropout_9[0][0]                  
==================================================================================================
Total params: 50,463
Trainable params: 14,243
Non-trainable params: 36,220
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.48913484e-02 3.19509754e-02 7.49897764e-02 9.32025064e-03
 2.70904632e-02 7.06417031e-03 8.46180096e-02 1.12618024e-01
 8.60108482e-02 1.32459736e-02 2.94100802e-01 1.93843398e-01
 2.55960049e-04]
Train on 10843 samples, validate on 104 samples
Epoch 1/300
 - 18s - loss: 374.7830 - acc: 0.3243 - mDice: 0.0173 - val_loss: 277.9264 - val_acc: 0.8305 - val_mDice: 0.0166

Epoch 00001: val_mDice improved from -inf to 0.01660, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 9s - loss: 237.8316 - acc: 0.7981 - mDice: 0.0198 - val_loss: 179.0308 - val_acc: 0.9005 - val_mDice: 0.0177

Epoch 00002: val_mDice improved from 0.01660 to 0.01772, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 9s - loss: 140.5049 - acc: 0.8560 - mDice: 0.0212 - val_loss: 110.1344 - val_acc: 0.9034 - val_mDice: 0.0204

Epoch 00003: val_mDice improved from 0.01772 to 0.02039, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 8s - loss: 83.2322 - acc: 0.8695 - mDice: 0.0216 - val_loss: 64.7283 - val_acc: 0.9034 - val_mDice: 0.0213

Epoch 00004: val_mDice improved from 0.02039 to 0.02132, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 8s - loss: 53.1355 - acc: 0.8746 - mDice: 0.0220 - val_loss: 35.5969 - val_acc: 0.9034 - val_mDice: 0.0207

Epoch 00005: val_mDice did not improve from 0.02132
Epoch 6/300
 - 8s - loss: 37.7282 - acc: 0.8747 - mDice: 0.0214 - val_loss: 23.3061 - val_acc: 0.9034 - val_mDice: 0.0204

Epoch 00006: val_mDice did not improve from 0.02132
Epoch 7/300
 - 9s - loss: 29.4262 - acc: 0.8748 - mDice: 0.0211 - val_loss: 16.7381 - val_acc: 0.9034 - val_mDice: 0.0210

Epoch 00007: val_mDice did not improve from 0.02132
Epoch 8/300
 - 9s - loss: 24.4972 - acc: 0.8748 - mDice: 0.0213 - val_loss: 13.1065 - val_acc: 0.9034 - val_mDice: 0.0214

Epoch 00008: val_mDice improved from 0.02132 to 0.02141, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 8s - loss: 21.3085 - acc: 0.8748 - mDice: 0.0218 - val_loss: 11.4378 - val_acc: 0.9034 - val_mDice: 0.0221

Epoch 00009: val_mDice improved from 0.02141 to 0.02205, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 9s - loss: 19.1142 - acc: 0.8748 - mDice: 0.0225 - val_loss: 10.5700 - val_acc: 0.9034 - val_mDice: 0.0230

Epoch 00010: val_mDice improved from 0.02205 to 0.02302, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 9s - loss: 17.4925 - acc: 0.8748 - mDice: 0.0234 - val_loss: 9.7848 - val_acc: 0.9034 - val_mDice: 0.0204

Epoch 00011: val_mDice did not improve from 0.02302
Epoch 12/300
 - 9s - loss: 16.1292 - acc: 0.8748 - mDice: 0.0244 - val_loss: 9.4379 - val_acc: 0.9034 - val_mDice: 0.0223

Epoch 00012: val_mDice did not improve from 0.02302
Epoch 13/300
 - 9s - loss: 15.1126 - acc: 0.8748 - mDice: 0.0257 - val_loss: 9.0839 - val_acc: 0.9034 - val_mDice: 0.0260

Epoch 00013: val_mDice improved from 0.02302 to 0.02604, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 9s - loss: 14.3351 - acc: 0.8748 - mDice: 0.0272 - val_loss: 8.8057 - val_acc: 0.9034 - val_mDice: 0.0280

Epoch 00014: val_mDice improved from 0.02604 to 0.02801, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 8s - loss: 13.7256 - acc: 0.8748 - mDice: 0.0285 - val_loss: 8.6142 - val_acc: 0.9034 - val_mDice: 0.0295

Epoch 00015: val_mDice improved from 0.02801 to 0.02949, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 8s - loss: 13.2274 - acc: 0.8748 - mDice: 0.0297 - val_loss: 8.4766 - val_acc: 0.9034 - val_mDice: 0.0310

Epoch 00016: val_mDice improved from 0.02949 to 0.03098, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 9s - loss: 12.7839 - acc: 0.8748 - mDice: 0.0310 - val_loss: 8.4241 - val_acc: 0.9034 - val_mDice: 0.0310

Epoch 00017: val_mDice improved from 0.03098 to 0.03104, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 8s - loss: 12.3626 - acc: 0.8748 - mDice: 0.0327 - val_loss: 8.2882 - val_acc: 0.9034 - val_mDice: 0.0320

Epoch 00018: val_mDice improved from 0.03104 to 0.03197, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 8s - loss: 11.9604 - acc: 0.8748 - mDice: 0.0342 - val_loss: 8.0656 - val_acc: 0.9034 - val_mDice: 0.0359

Epoch 00019: val_mDice improved from 0.03197 to 0.03585, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 9s - loss: 11.6036 - acc: 0.8748 - mDice: 0.0360 - val_loss: 8.0608 - val_acc: 0.9034 - val_mDice: 0.0373

Epoch 00020: val_mDice improved from 0.03585 to 0.03734, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 9s - loss: 11.2764 - acc: 0.8748 - mDice: 0.0378 - val_loss: 8.0851 - val_acc: 0.9034 - val_mDice: 0.0386

Epoch 00021: val_mDice improved from 0.03734 to 0.03865, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 8s - loss: 10.9714 - acc: 0.8748 - mDice: 0.0396 - val_loss: 8.1507 - val_acc: 0.9034 - val_mDice: 0.0385

Epoch 00022: val_mDice did not improve from 0.03865
Epoch 23/300
 - 9s - loss: 10.6952 - acc: 0.8746 - mDice: 0.0414 - val_loss: 7.7701 - val_acc: 0.9034 - val_mDice: 0.0453

Epoch 00023: val_mDice improved from 0.03865 to 0.04533, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 9s - loss: 10.4367 - acc: 0.8744 - mDice: 0.0432 - val_loss: 8.4062 - val_acc: 0.9034 - val_mDice: 0.0347

Epoch 00024: val_mDice did not improve from 0.04533
Epoch 25/300
 - 9s - loss: 10.2098 - acc: 0.8742 - mDice: 0.0448 - val_loss: 7.7779 - val_acc: 0.9034 - val_mDice: 0.0445

Epoch 00025: val_mDice did not improve from 0.04533
Epoch 26/300
 - 9s - loss: 10.0037 - acc: 0.8740 - mDice: 0.0462 - val_loss: 7.4864 - val_acc: 0.9034 - val_mDice: 0.0514

Epoch 00026: val_mDice improved from 0.04533 to 0.05139, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 9s - loss: 9.7925 - acc: 0.8739 - mDice: 0.0479 - val_loss: 7.4371 - val_acc: 0.9034 - val_mDice: 0.0523

Epoch 00027: val_mDice improved from 0.05139 to 0.05232, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 9s - loss: 9.6120 - acc: 0.8738 - mDice: 0.0496 - val_loss: 7.3577 - val_acc: 0.9034 - val_mDice: 0.0564

Epoch 00028: val_mDice improved from 0.05232 to 0.05637, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 9s - loss: 9.4348 - acc: 0.8736 - mDice: 0.0516 - val_loss: 7.2292 - val_acc: 0.9034 - val_mDice: 0.0611

Epoch 00029: val_mDice improved from 0.05637 to 0.06113, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 8s - loss: 9.2546 - acc: 0.8735 - mDice: 0.0542 - val_loss: 7.2885 - val_acc: 0.9034 - val_mDice: 0.0596

Epoch 00030: val_mDice did not improve from 0.06113
Epoch 31/300
 - 9s - loss: 9.0756 - acc: 0.8733 - mDice: 0.0571 - val_loss: 6.9990 - val_acc: 0.9033 - val_mDice: 0.0671

Epoch 00031: val_mDice improved from 0.06113 to 0.06710, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 9s - loss: 8.9342 - acc: 0.8735 - mDice: 0.0600 - val_loss: 7.0842 - val_acc: 0.9033 - val_mDice: 0.0636

Epoch 00032: val_mDice did not improve from 0.06710
Epoch 33/300
 - 8s - loss: 8.7739 - acc: 0.8736 - mDice: 0.0631 - val_loss: 6.9199 - val_acc: 0.9032 - val_mDice: 0.0734

Epoch 00033: val_mDice improved from 0.06710 to 0.07345, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 9s - loss: 8.6383 - acc: 0.8739 - mDice: 0.0660 - val_loss: 6.9993 - val_acc: 0.9032 - val_mDice: 0.0699

Epoch 00034: val_mDice did not improve from 0.07345
Epoch 35/300
 - 9s - loss: 8.4961 - acc: 0.8744 - mDice: 0.0688 - val_loss: 6.9396 - val_acc: 0.9033 - val_mDice: 0.0739

Epoch 00035: val_mDice improved from 0.07345 to 0.07391, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 9s - loss: 8.3802 - acc: 0.8751 - mDice: 0.0711 - val_loss: 6.7350 - val_acc: 0.9033 - val_mDice: 0.0800

Epoch 00036: val_mDice improved from 0.07391 to 0.08003, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 9s - loss: 8.2422 - acc: 0.8758 - mDice: 0.0740 - val_loss: 6.9690 - val_acc: 0.9030 - val_mDice: 0.0712

Epoch 00037: val_mDice did not improve from 0.08003
Epoch 38/300
 - 9s - loss: 8.1348 - acc: 0.8767 - mDice: 0.0764 - val_loss: 6.8893 - val_acc: 0.9024 - val_mDice: 0.0889

Epoch 00038: val_mDice improved from 0.08003 to 0.08889, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 9s - loss: 8.0129 - acc: 0.8776 - mDice: 0.0795 - val_loss: 6.5416 - val_acc: 0.9016 - val_mDice: 0.0909

Epoch 00039: val_mDice improved from 0.08889 to 0.09088, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 9s - loss: 7.8592 - acc: 0.8785 - mDice: 0.0829 - val_loss: 6.3790 - val_acc: 0.9036 - val_mDice: 0.1045

Epoch 00040: val_mDice improved from 0.09088 to 0.10445, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 9s - loss: 7.7353 - acc: 0.8790 - mDice: 0.0859 - val_loss: 6.5050 - val_acc: 0.8999 - val_mDice: 0.0936

Epoch 00041: val_mDice did not improve from 0.10445
Epoch 42/300
 - 9s - loss: 7.6461 - acc: 0.8794 - mDice: 0.0882 - val_loss: 6.4348 - val_acc: 0.9001 - val_mDice: 0.1022

Epoch 00042: val_mDice did not improve from 0.10445
Epoch 43/300
 - 9s - loss: 7.5527 - acc: 0.8797 - mDice: 0.0906 - val_loss: 6.2513 - val_acc: 0.9034 - val_mDice: 0.1081

Epoch 00043: val_mDice improved from 0.10445 to 0.10812, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 44/300
 - 9s - loss: 7.4654 - acc: 0.8801 - mDice: 0.0930 - val_loss: 6.2250 - val_acc: 0.9024 - val_mDice: 0.1101

Epoch 00044: val_mDice improved from 0.10812 to 0.11009, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 8s - loss: 7.3757 - acc: 0.8804 - mDice: 0.0950 - val_loss: 6.2912 - val_acc: 0.9019 - val_mDice: 0.1136

Epoch 00045: val_mDice improved from 0.11009 to 0.11358, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 9s - loss: 7.2869 - acc: 0.8808 - mDice: 0.0978 - val_loss: 6.1856 - val_acc: 0.8986 - val_mDice: 0.1091

Epoch 00046: val_mDice did not improve from 0.11358
Epoch 47/300
 - 9s - loss: 7.2133 - acc: 0.8809 - mDice: 0.0996 - val_loss: 6.0311 - val_acc: 0.9094 - val_mDice: 0.1193

Epoch 00047: val_mDice improved from 0.11358 to 0.11927, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 9s - loss: 7.1455 - acc: 0.8813 - mDice: 0.1022 - val_loss: 6.1182 - val_acc: 0.9082 - val_mDice: 0.1143

Epoch 00048: val_mDice did not improve from 0.11927
Epoch 49/300
 - 9s - loss: 7.0718 - acc: 0.8815 - mDice: 0.1046 - val_loss: 6.0528 - val_acc: 0.9141 - val_mDice: 0.1228

Epoch 00049: val_mDice improved from 0.11927 to 0.12279, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 9s - loss: 6.9906 - acc: 0.8821 - mDice: 0.1078 - val_loss: 6.0055 - val_acc: 0.9139 - val_mDice: 0.1244

Epoch 00050: val_mDice improved from 0.12279 to 0.12444, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 8s - loss: 6.9100 - acc: 0.8827 - mDice: 0.1108 - val_loss: 5.8945 - val_acc: 0.9151 - val_mDice: 0.1299

Epoch 00051: val_mDice improved from 0.12444 to 0.12990, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 9s - loss: 6.8516 - acc: 0.8832 - mDice: 0.1130 - val_loss: 5.8449 - val_acc: 0.9121 - val_mDice: 0.1344

Epoch 00052: val_mDice improved from 0.12990 to 0.13439, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 8s - loss: 6.7912 - acc: 0.8838 - mDice: 0.1150 - val_loss: 5.8387 - val_acc: 0.9134 - val_mDice: 0.1347

Epoch 00053: val_mDice improved from 0.13439 to 0.13466, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 9s - loss: 6.7320 - acc: 0.8843 - mDice: 0.1174 - val_loss: 5.7660 - val_acc: 0.9147 - val_mDice: 0.1373

Epoch 00054: val_mDice improved from 0.13466 to 0.13730, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 9s - loss: 6.6647 - acc: 0.8849 - mDice: 0.1196 - val_loss: 5.6416 - val_acc: 0.9136 - val_mDice: 0.1420

Epoch 00055: val_mDice improved from 0.13730 to 0.14197, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 9s - loss: 6.6103 - acc: 0.8854 - mDice: 0.1216 - val_loss: 5.7119 - val_acc: 0.9140 - val_mDice: 0.1411

Epoch 00056: val_mDice did not improve from 0.14197
Epoch 57/300
 - 8s - loss: 6.5592 - acc: 0.8857 - mDice: 0.1233 - val_loss: 5.7123 - val_acc: 0.9144 - val_mDice: 0.1387

Epoch 00057: val_mDice did not improve from 0.14197
Epoch 58/300
 - 9s - loss: 6.4972 - acc: 0.8860 - mDice: 0.1252 - val_loss: 5.6781 - val_acc: 0.9127 - val_mDice: 0.1412

Epoch 00058: val_mDice did not improve from 0.14197
Epoch 59/300
 - 9s - loss: 6.4543 - acc: 0.8863 - mDice: 0.1265 - val_loss: 5.7029 - val_acc: 0.9127 - val_mDice: 0.1465

Epoch 00059: val_mDice improved from 0.14197 to 0.14648, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 60/300
 - 9s - loss: 6.3934 - acc: 0.8866 - mDice: 0.1288 - val_loss: 5.6670 - val_acc: 0.9119 - val_mDice: 0.1442

Epoch 00060: val_mDice did not improve from 0.14648
Epoch 61/300
 - 9s - loss: 6.3385 - acc: 0.8870 - mDice: 0.1307 - val_loss: 5.7211 - val_acc: 0.9118 - val_mDice: 0.1411

Epoch 00061: val_mDice did not improve from 0.14648
Epoch 62/300
 - 9s - loss: 6.2993 - acc: 0.8869 - mDice: 0.1317 - val_loss: 5.5216 - val_acc: 0.9134 - val_mDice: 0.1539

Epoch 00062: val_mDice improved from 0.14648 to 0.15387, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 9s - loss: 6.2367 - acc: 0.8877 - mDice: 0.1339 - val_loss: 5.5127 - val_acc: 0.9099 - val_mDice: 0.1509

Epoch 00063: val_mDice did not improve from 0.15387
Epoch 64/300
 - 9s - loss: 6.1864 - acc: 0.8884 - mDice: 0.1366 - val_loss: 5.5904 - val_acc: 0.9154 - val_mDice: 0.1588

Epoch 00064: val_mDice improved from 0.15387 to 0.15881, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 65/300
 - 9s - loss: 6.1340 - acc: 0.8887 - mDice: 0.1387 - val_loss: 5.3856 - val_acc: 0.9135 - val_mDice: 0.1626

Epoch 00065: val_mDice improved from 0.15881 to 0.16256, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 66/300
 - 9s - loss: 6.0793 - acc: 0.8895 - mDice: 0.1415 - val_loss: 5.3641 - val_acc: 0.9126 - val_mDice: 0.1656

Epoch 00066: val_mDice improved from 0.16256 to 0.16557, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 67/300
 - 10s - loss: 6.0259 - acc: 0.8900 - mDice: 0.1441 - val_loss: 5.3047 - val_acc: 0.9190 - val_mDice: 0.1707

Epoch 00067: val_mDice improved from 0.16557 to 0.17070, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 68/300
 - 9s - loss: 5.9786 - acc: 0.8904 - mDice: 0.1471 - val_loss: 5.3557 - val_acc: 0.9175 - val_mDice: 0.1680

Epoch 00068: val_mDice did not improve from 0.17070
Epoch 69/300
 - 10s - loss: 5.9137 - acc: 0.8910 - mDice: 0.1506 - val_loss: 5.2518 - val_acc: 0.9175 - val_mDice: 0.1724

Epoch 00069: val_mDice improved from 0.17070 to 0.17243, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 70/300
 - 10s - loss: 5.8651 - acc: 0.8914 - mDice: 0.1531 - val_loss: 5.3113 - val_acc: 0.9175 - val_mDice: 0.1767

Epoch 00070: val_mDice improved from 0.17243 to 0.17671, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 71/300
 - 12s - loss: 5.8233 - acc: 0.8921 - mDice: 0.1564 - val_loss: 5.1272 - val_acc: 0.9186 - val_mDice: 0.1811

Epoch 00071: val_mDice improved from 0.17671 to 0.18107, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 72/300
 - 13s - loss: 5.7782 - acc: 0.8926 - mDice: 0.1592 - val_loss: 5.1534 - val_acc: 0.9171 - val_mDice: 0.1846

Epoch 00072: val_mDice improved from 0.18107 to 0.18455, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 73/300
 - 12s - loss: 5.7397 - acc: 0.8928 - mDice: 0.1617 - val_loss: 5.1496 - val_acc: 0.9199 - val_mDice: 0.1863

Epoch 00073: val_mDice improved from 0.18455 to 0.18626, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 14s - loss: 5.6937 - acc: 0.8933 - mDice: 0.1650 - val_loss: 5.1844 - val_acc: 0.9192 - val_mDice: 0.1860

Epoch 00074: val_mDice did not improve from 0.18626
Epoch 75/300
 - 12s - loss: 5.6551 - acc: 0.8939 - mDice: 0.1683 - val_loss: 5.0207 - val_acc: 0.9205 - val_mDice: 0.1984

Epoch 00075: val_mDice improved from 0.18626 to 0.19836, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 76/300
 - 13s - loss: 5.6253 - acc: 0.8939 - mDice: 0.1702 - val_loss: 5.0373 - val_acc: 0.9205 - val_mDice: 0.1976

Epoch 00076: val_mDice did not improve from 0.19836
Epoch 77/300
 - 12s - loss: 5.5642 - acc: 0.8940 - mDice: 0.1731 - val_loss: 5.2232 - val_acc: 0.9209 - val_mDice: 0.1951

Epoch 00077: val_mDice did not improve from 0.19836
Epoch 78/300
 - 13s - loss: 5.5198 - acc: 0.8945 - mDice: 0.1764 - val_loss: 4.8601 - val_acc: 0.9200 - val_mDice: 0.2046

Epoch 00078: val_mDice improved from 0.19836 to 0.20463, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 79/300
 - 12s - loss: 5.4944 - acc: 0.8945 - mDice: 0.1781 - val_loss: 5.2364 - val_acc: 0.9204 - val_mDice: 0.1979

Epoch 00079: val_mDice did not improve from 0.20463
Epoch 80/300
 - 13s - loss: 5.4491 - acc: 0.8946 - mDice: 0.1807 - val_loss: 4.9369 - val_acc: 0.9192 - val_mDice: 0.2000

Epoch 00080: val_mDice did not improve from 0.20463
Epoch 81/300
 - 12s - loss: 5.4246 - acc: 0.8946 - mDice: 0.1826 - val_loss: 4.9267 - val_acc: 0.9212 - val_mDice: 0.2104

Epoch 00081: val_mDice improved from 0.20463 to 0.21040, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 82/300
 - 13s - loss: 5.3736 - acc: 0.8949 - mDice: 0.1859 - val_loss: 4.9256 - val_acc: 0.9193 - val_mDice: 0.2093

Epoch 00082: val_mDice did not improve from 0.21040
Epoch 83/300
 - 12s - loss: 5.3690 - acc: 0.8948 - mDice: 0.1877 - val_loss: 5.1412 - val_acc: 0.9204 - val_mDice: 0.2124

Epoch 00083: val_mDice improved from 0.21040 to 0.21243, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 84/300
 - 11s - loss: 5.3099 - acc: 0.8949 - mDice: 0.1923 - val_loss: 5.0170 - val_acc: 0.9210 - val_mDice: 0.2164

Epoch 00084: val_mDice improved from 0.21243 to 0.21644, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 85/300
 - 12s - loss: 5.2810 - acc: 0.8950 - mDice: 0.1945 - val_loss: 4.9023 - val_acc: 0.9190 - val_mDice: 0.2155

Epoch 00085: val_mDice did not improve from 0.21644
Epoch 86/300
 - 12s - loss: 5.2568 - acc: 0.8951 - mDice: 0.1969 - val_loss: 4.9687 - val_acc: 0.9206 - val_mDice: 0.2158

Epoch 00086: val_mDice did not improve from 0.21644
Epoch 87/300
 - 12s - loss: 5.2212 - acc: 0.8953 - mDice: 0.1999 - val_loss: 4.8676 - val_acc: 0.9212 - val_mDice: 0.2169

Epoch 00087: val_mDice improved from 0.21644 to 0.21692, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 88/300
 - 12s - loss: 5.1934 - acc: 0.8953 - mDice: 0.2012 - val_loss: 4.9125 - val_acc: 0.9193 - val_mDice: 0.2176

Epoch 00088: val_mDice improved from 0.21692 to 0.21758, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 89/300
 - 11s - loss: 5.1604 - acc: 0.8955 - mDice: 0.2036 - val_loss: 4.7528 - val_acc: 0.9207 - val_mDice: 0.2274

Epoch 00089: val_mDice improved from 0.21758 to 0.22745, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 90/300
 - 12s - loss: 5.1411 - acc: 0.8956 - mDice: 0.2054 - val_loss: 5.0660 - val_acc: 0.9202 - val_mDice: 0.2180

Epoch 00090: val_mDice did not improve from 0.22745
Epoch 91/300
 - 12s - loss: 5.1350 - acc: 0.8956 - mDice: 0.2058 - val_loss: 4.8065 - val_acc: 0.9196 - val_mDice: 0.2233

Epoch 00091: val_mDice did not improve from 0.22745
Epoch 92/300
 - 12s - loss: 5.1074 - acc: 0.8958 - mDice: 0.2080 - val_loss: 4.6484 - val_acc: 0.9198 - val_mDice: 0.2287

Epoch 00092: val_mDice improved from 0.22745 to 0.22870, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 93/300
 - 13s - loss: 5.0903 - acc: 0.8957 - mDice: 0.2090 - val_loss: 4.6198 - val_acc: 0.9204 - val_mDice: 0.2284

Epoch 00093: val_mDice did not improve from 0.22870
Epoch 94/300
 - 12s - loss: 5.0564 - acc: 0.8960 - mDice: 0.2111 - val_loss: 4.8424 - val_acc: 0.9180 - val_mDice: 0.2194

Epoch 00094: val_mDice did not improve from 0.22870
Epoch 95/300
 - 12s - loss: 5.0358 - acc: 0.8960 - mDice: 0.2127 - val_loss: 4.8168 - val_acc: 0.9211 - val_mDice: 0.2278

Epoch 00095: val_mDice did not improve from 0.22870
Epoch 96/300
 - 12s - loss: 5.0208 - acc: 0.8959 - mDice: 0.2140 - val_loss: 4.8469 - val_acc: 0.9212 - val_mDice: 0.2332

Epoch 00096: val_mDice improved from 0.22870 to 0.23317, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 97/300
 - 13s - loss: 5.0068 - acc: 0.8959 - mDice: 0.2147 - val_loss: 4.6585 - val_acc: 0.9210 - val_mDice: 0.2385

Epoch 00097: val_mDice improved from 0.23317 to 0.23852, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 98/300
 - 12s - loss: 4.9850 - acc: 0.8961 - mDice: 0.2175 - val_loss: 5.1447 - val_acc: 0.9215 - val_mDice: 0.2271

Epoch 00098: val_mDice did not improve from 0.23852
Epoch 99/300
 - 11s - loss: 4.9667 - acc: 0.8960 - mDice: 0.2185 - val_loss: 4.7329 - val_acc: 0.9213 - val_mDice: 0.2421

Epoch 00099: val_mDice improved from 0.23852 to 0.24213, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 100/300
 - 13s - loss: 4.9304 - acc: 0.8963 - mDice: 0.2208 - val_loss: 5.0331 - val_acc: 0.9199 - val_mDice: 0.2328

Epoch 00100: val_mDice did not improve from 0.24213
Epoch 101/300
 - 11s - loss: 4.9349 - acc: 0.8961 - mDice: 0.2210 - val_loss: 5.0561 - val_acc: 0.9204 - val_mDice: 0.2363

Epoch 00101: val_mDice did not improve from 0.24213
Epoch 102/300
 - 13s - loss: 4.9042 - acc: 0.8962 - mDice: 0.2229 - val_loss: 4.8545 - val_acc: 0.9206 - val_mDice: 0.2387

Epoch 00102: val_mDice did not improve from 0.24213
Epoch 103/300
 - 12s - loss: 4.8936 - acc: 0.8963 - mDice: 0.2245 - val_loss: 4.8842 - val_acc: 0.9204 - val_mDice: 0.2311

Epoch 00103: val_mDice did not improve from 0.24213
Epoch 104/300
 - 11s - loss: 4.8772 - acc: 0.8963 - mDice: 0.2251 - val_loss: 4.8122 - val_acc: 0.9209 - val_mDice: 0.2379

Epoch 00104: val_mDice did not improve from 0.24213
Epoch 105/300
 - 9s - loss: 4.8653 - acc: 0.8964 - mDice: 0.2267 - val_loss: 4.7522 - val_acc: 0.9200 - val_mDice: 0.2427

Epoch 00105: val_mDice improved from 0.24213 to 0.24274, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 106/300
 - 10s - loss: 4.8459 - acc: 0.8962 - mDice: 0.2285 - val_loss: 4.5561 - val_acc: 0.9211 - val_mDice: 0.2477

Epoch 00106: val_mDice improved from 0.24274 to 0.24767, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 107/300
 - 9s - loss: 4.8222 - acc: 0.8963 - mDice: 0.2303 - val_loss: 4.8649 - val_acc: 0.9214 - val_mDice: 0.2414

Epoch 00107: val_mDice did not improve from 0.24767
Epoch 108/300
 - 10s - loss: 4.8100 - acc: 0.8962 - mDice: 0.2311 - val_loss: 4.5771 - val_acc: 0.9213 - val_mDice: 0.2508

Epoch 00108: val_mDice improved from 0.24767 to 0.25083, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 109/300
 - 9s - loss: 4.7856 - acc: 0.8963 - mDice: 0.2338 - val_loss: 4.7571 - val_acc: 0.9197 - val_mDice: 0.2397

Epoch 00109: val_mDice did not improve from 0.25083
Epoch 110/300
 - 10s - loss: 4.7699 - acc: 0.8962 - mDice: 0.2359 - val_loss: 4.6052 - val_acc: 0.9206 - val_mDice: 0.2476

Epoch 00110: val_mDice did not improve from 0.25083
Epoch 111/300
 - 9s - loss: 4.7386 - acc: 0.8962 - mDice: 0.2389 - val_loss: 4.6735 - val_acc: 0.9192 - val_mDice: 0.2513

Epoch 00111: val_mDice improved from 0.25083 to 0.25131, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 112/300
 - 10s - loss: 4.7372 - acc: 0.8962 - mDice: 0.2397 - val_loss: 4.5678 - val_acc: 0.9200 - val_mDice: 0.2481

Epoch 00112: val_mDice did not improve from 0.25131
Epoch 113/300
 - 9s - loss: 4.7171 - acc: 0.8964 - mDice: 0.2422 - val_loss: 4.6249 - val_acc: 0.9219 - val_mDice: 0.2578

Epoch 00113: val_mDice improved from 0.25131 to 0.25781, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 114/300
 - 10s - loss: 4.7036 - acc: 0.8963 - mDice: 0.2430 - val_loss: 4.7608 - val_acc: 0.9201 - val_mDice: 0.2491

Epoch 00114: val_mDice did not improve from 0.25781
Epoch 115/300
 - 9s - loss: 4.6853 - acc: 0.8962 - mDice: 0.2451 - val_loss: 4.4144 - val_acc: 0.9214 - val_mDice: 0.2602

Epoch 00115: val_mDice improved from 0.25781 to 0.26016, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 116/300
 - 10s - loss: 4.6534 - acc: 0.8963 - mDice: 0.2478 - val_loss: 4.3001 - val_acc: 0.9200 - val_mDice: 0.2660

Epoch 00116: val_mDice improved from 0.26016 to 0.26598, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 117/300
 - 9s - loss: 4.6492 - acc: 0.8962 - mDice: 0.2488 - val_loss: 5.0476 - val_acc: 0.9207 - val_mDice: 0.2454

Epoch 00117: val_mDice did not improve from 0.26598
Epoch 118/300
 - 10s - loss: 4.6466 - acc: 0.8961 - mDice: 0.2496 - val_loss: 4.4445 - val_acc: 0.9212 - val_mDice: 0.2593

Epoch 00118: val_mDice did not improve from 0.26598
Epoch 119/300
 - 9s - loss: 4.6296 - acc: 0.8960 - mDice: 0.2510 - val_loss: 4.5895 - val_acc: 0.9211 - val_mDice: 0.2616

Epoch 00119: val_mDice did not improve from 0.26598
Epoch 120/300
 - 9s - loss: 4.6183 - acc: 0.8962 - mDice: 0.2528 - val_loss: 4.5378 - val_acc: 0.9221 - val_mDice: 0.2679

Epoch 00120: val_mDice improved from 0.26598 to 0.26794, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 121/300
 - 9s - loss: 4.5949 - acc: 0.8963 - mDice: 0.2552 - val_loss: 4.4078 - val_acc: 0.9211 - val_mDice: 0.2717

Epoch 00121: val_mDice improved from 0.26794 to 0.27170, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 122/300
 - 10s - loss: 4.5958 - acc: 0.8963 - mDice: 0.2563 - val_loss: 4.3999 - val_acc: 0.9216 - val_mDice: 0.2782

Epoch 00122: val_mDice improved from 0.27170 to 0.27815, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 123/300
 - 9s - loss: 4.5752 - acc: 0.8963 - mDice: 0.2575 - val_loss: 4.5772 - val_acc: 0.9219 - val_mDice: 0.2658

Epoch 00123: val_mDice did not improve from 0.27815
Epoch 124/300
 - 10s - loss: 4.5803 - acc: 0.8961 - mDice: 0.2579 - val_loss: 4.3855 - val_acc: 0.9216 - val_mDice: 0.2694

Epoch 00124: val_mDice did not improve from 0.27815
Epoch 125/300
 - 9s - loss: 4.5546 - acc: 0.8963 - mDice: 0.2611 - val_loss: 4.2952 - val_acc: 0.9221 - val_mDice: 0.2812

Epoch 00125: val_mDice improved from 0.27815 to 0.28118, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 126/300
 - 10s - loss: 4.5560 - acc: 0.8962 - mDice: 0.2609 - val_loss: 4.6721 - val_acc: 0.9229 - val_mDice: 0.2655

Epoch 00126: val_mDice did not improve from 0.28118
Epoch 127/300
 - 9s - loss: 4.5540 - acc: 0.8962 - mDice: 0.2612 - val_loss: 4.3369 - val_acc: 0.9219 - val_mDice: 0.2699

Epoch 00127: val_mDice did not improve from 0.28118
Epoch 128/300
 - 10s - loss: 4.5287 - acc: 0.8965 - mDice: 0.2643 - val_loss: 4.6661 - val_acc: 0.9227 - val_mDice: 0.2708

Epoch 00128: val_mDice did not improve from 0.28118
Epoch 129/300
 - 9s - loss: 4.5296 - acc: 0.8963 - mDice: 0.2643 - val_loss: 4.4033 - val_acc: 0.9206 - val_mDice: 0.2759

Epoch 00129: val_mDice did not improve from 0.28118
Epoch 130/300
 - 10s - loss: 4.5147 - acc: 0.8964 - mDice: 0.2656 - val_loss: 4.2947 - val_acc: 0.9209 - val_mDice: 0.2735

Epoch 00130: val_mDice did not improve from 0.28118
Epoch 131/300
 - 9s - loss: 4.4962 - acc: 0.8966 - mDice: 0.2675 - val_loss: 4.3001 - val_acc: 0.9214 - val_mDice: 0.2787

Epoch 00131: val_mDice did not improve from 0.28118
Epoch 132/300
 - 10s - loss: 4.5119 - acc: 0.8964 - mDice: 0.2673 - val_loss: 4.3135 - val_acc: 0.9227 - val_mDice: 0.2787

Epoch 00132: val_mDice did not improve from 0.28118
Epoch 133/300
 - 9s - loss: 4.4912 - acc: 0.8965 - mDice: 0.2692 - val_loss: 4.4200 - val_acc: 0.9235 - val_mDice: 0.2825

Epoch 00133: val_mDice improved from 0.28118 to 0.28250, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 134/300
 - 10s - loss: 4.4877 - acc: 0.8965 - mDice: 0.2701 - val_loss: 4.3867 - val_acc: 0.9247 - val_mDice: 0.2817

Epoch 00134: val_mDice did not improve from 0.28250
Epoch 135/300
 - 9s - loss: 4.4792 - acc: 0.8969 - mDice: 0.2713 - val_loss: 4.3729 - val_acc: 0.9217 - val_mDice: 0.2783

Epoch 00135: val_mDice did not improve from 0.28250
Epoch 136/300
 - 9s - loss: 4.4600 - acc: 0.8970 - mDice: 0.2728 - val_loss: 4.3950 - val_acc: 0.9236 - val_mDice: 0.2814

Epoch 00136: val_mDice did not improve from 0.28250
Epoch 137/300
 - 9s - loss: 4.4644 - acc: 0.8970 - mDice: 0.2733 - val_loss: 4.2141 - val_acc: 0.9234 - val_mDice: 0.2848

Epoch 00137: val_mDice improved from 0.28250 to 0.28479, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 138/300
 - 9s - loss: 4.4447 - acc: 0.8973 - mDice: 0.2758 - val_loss: 4.3809 - val_acc: 0.9228 - val_mDice: 0.2790

Epoch 00138: val_mDice did not improve from 0.28479
Epoch 139/300
 - 9s - loss: 4.4412 - acc: 0.8974 - mDice: 0.2767 - val_loss: 4.3803 - val_acc: 0.9218 - val_mDice: 0.2812

Epoch 00139: val_mDice did not improve from 0.28479
Epoch 140/300
 - 8s - loss: 4.4328 - acc: 0.8972 - mDice: 0.2772 - val_loss: 4.2297 - val_acc: 0.9239 - val_mDice: 0.2960

Epoch 00140: val_mDice improved from 0.28479 to 0.29601, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 141/300
 - 9s - loss: 4.4341 - acc: 0.8974 - mDice: 0.2782 - val_loss: 4.2024 - val_acc: 0.9228 - val_mDice: 0.2873

Epoch 00141: val_mDice did not improve from 0.29601
Epoch 142/300
 - 9s - loss: 4.4193 - acc: 0.8975 - mDice: 0.2793 - val_loss: 4.3222 - val_acc: 0.9226 - val_mDice: 0.2893

Epoch 00142: val_mDice did not improve from 0.29601
Epoch 143/300
 - 8s - loss: 4.4300 - acc: 0.8972 - mDice: 0.2786 - val_loss: 4.3239 - val_acc: 0.9249 - val_mDice: 0.2907

Epoch 00143: val_mDice did not improve from 0.29601
Epoch 144/300
 - 9s - loss: 4.4059 - acc: 0.8974 - mDice: 0.2805 - val_loss: 4.2039 - val_acc: 0.9266 - val_mDice: 0.3004

Epoch 00144: val_mDice improved from 0.29601 to 0.30035, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 145/300
 - 9s - loss: 4.4049 - acc: 0.8975 - mDice: 0.2814 - val_loss: 4.3599 - val_acc: 0.9248 - val_mDice: 0.2895

Epoch 00145: val_mDice did not improve from 0.30035
Epoch 146/300
 - 8s - loss: 4.4117 - acc: 0.8974 - mDice: 0.2802 - val_loss: 4.3983 - val_acc: 0.9246 - val_mDice: 0.2887

Epoch 00146: val_mDice did not improve from 0.30035
Epoch 147/300
 - 9s - loss: 4.3888 - acc: 0.8976 - mDice: 0.2831 - val_loss: 4.3075 - val_acc: 0.9228 - val_mDice: 0.2926

Epoch 00147: val_mDice did not improve from 0.30035
Epoch 148/300
 - 9s - loss: 4.3820 - acc: 0.8978 - mDice: 0.2846 - val_loss: 4.1332 - val_acc: 0.9234 - val_mDice: 0.3050

Epoch 00148: val_mDice improved from 0.30035 to 0.30501, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 149/300
 - 8s - loss: 4.3742 - acc: 0.8977 - mDice: 0.2851 - val_loss: 4.3655 - val_acc: 0.9243 - val_mDice: 0.2897

Epoch 00149: val_mDice did not improve from 0.30501
Epoch 150/300
 - 8s - loss: 4.3912 - acc: 0.8976 - mDice: 0.2830 - val_loss: 4.5026 - val_acc: 0.9225 - val_mDice: 0.2743

Epoch 00150: val_mDice did not improve from 0.30501
Epoch 151/300
 - 8s - loss: 4.3732 - acc: 0.8979 - mDice: 0.2851 - val_loss: 4.3261 - val_acc: 0.9243 - val_mDice: 0.2888

Epoch 00151: val_mDice did not improve from 0.30501
Epoch 152/300
 - 9s - loss: 4.3746 - acc: 0.8979 - mDice: 0.2858 - val_loss: 4.2648 - val_acc: 0.9253 - val_mDice: 0.2911

Epoch 00152: val_mDice did not improve from 0.30501
Epoch 153/300
 - 8s - loss: 4.3608 - acc: 0.8978 - mDice: 0.2868 - val_loss: 4.3346 - val_acc: 0.9249 - val_mDice: 0.2901

Epoch 00153: val_mDice did not improve from 0.30501
Epoch 154/300
 - 8s - loss: 4.3529 - acc: 0.8979 - mDice: 0.2882 - val_loss: 4.1381 - val_acc: 0.9249 - val_mDice: 0.2981

Epoch 00154: val_mDice did not improve from 0.30501
Epoch 155/300
 - 9s - loss: 4.3467 - acc: 0.8978 - mDice: 0.2888 - val_loss: 4.1665 - val_acc: 0.9259 - val_mDice: 0.3000

Epoch 00155: val_mDice did not improve from 0.30501
Epoch 156/300
 - 9s - loss: 4.3470 - acc: 0.8976 - mDice: 0.2887 - val_loss: 4.3045 - val_acc: 0.9227 - val_mDice: 0.2965

Epoch 00156: val_mDice did not improve from 0.30501
Epoch 157/300
 - 8s - loss: 4.3344 - acc: 0.8980 - mDice: 0.2898 - val_loss: 4.3127 - val_acc: 0.9231 - val_mDice: 0.2955

Epoch 00157: val_mDice did not improve from 0.30501
Epoch 158/300
 - 8s - loss: 4.3464 - acc: 0.8977 - mDice: 0.2892 - val_loss: 4.3459 - val_acc: 0.9251 - val_mDice: 0.2949

Epoch 00158: val_mDice did not improve from 0.30501
Epoch 159/300
 - 8s - loss: 4.3345 - acc: 0.8978 - mDice: 0.2908 - val_loss: 4.5102 - val_acc: 0.9232 - val_mDice: 0.2821

Epoch 00159: val_mDice did not improve from 0.30501
Epoch 160/300
 - 8s - loss: 4.3238 - acc: 0.8981 - mDice: 0.2920 - val_loss: 4.4966 - val_acc: 0.9256 - val_mDice: 0.2938

Epoch 00160: val_mDice did not improve from 0.30501
Epoch 161/300
 - 9s - loss: 4.3225 - acc: 0.8980 - mDice: 0.2917 - val_loss: 4.2528 - val_acc: 0.9243 - val_mDice: 0.2933

Epoch 00161: val_mDice did not improve from 0.30501
Epoch 162/300
 - 8s - loss: 4.3142 - acc: 0.8978 - mDice: 0.2927 - val_loss: 4.5805 - val_acc: 0.9255 - val_mDice: 0.2940

Epoch 00162: val_mDice did not improve from 0.30501
Epoch 163/300
 - 8s - loss: 4.3233 - acc: 0.8978 - mDice: 0.2915 - val_loss: 4.2098 - val_acc: 0.9209 - val_mDice: 0.2855

Epoch 00163: val_mDice did not improve from 0.30501
Epoch 164/300
 - 8s - loss: 4.3190 - acc: 0.8978 - mDice: 0.2921 - val_loss: 4.2572 - val_acc: 0.9231 - val_mDice: 0.2916

Epoch 00164: val_mDice did not improve from 0.30501
Epoch 165/300
 - 8s - loss: 4.3110 - acc: 0.8979 - mDice: 0.2930 - val_loss: 4.2138 - val_acc: 0.9232 - val_mDice: 0.2941

Epoch 00165: val_mDice did not improve from 0.30501
Epoch 166/300
 - 9s - loss: 4.3018 - acc: 0.8979 - mDice: 0.2944 - val_loss: 4.2714 - val_acc: 0.9250 - val_mDice: 0.3006

Epoch 00166: val_mDice did not improve from 0.30501
Epoch 167/300
 - 8s - loss: 4.3080 - acc: 0.8978 - mDice: 0.2940 - val_loss: 4.3176 - val_acc: 0.9230 - val_mDice: 0.2972

Epoch 00167: val_mDice did not improve from 0.30501
Epoch 168/300
 - 8s - loss: 4.2924 - acc: 0.8979 - mDice: 0.2950 - val_loss: 4.3394 - val_acc: 0.9246 - val_mDice: 0.3004

Epoch 00168: val_mDice did not improve from 0.30501
Epoch 169/300
 - 8s - loss: 4.2977 - acc: 0.8978 - mDice: 0.2950 - val_loss: 4.2091 - val_acc: 0.9248 - val_mDice: 0.3092

Epoch 00169: val_mDice improved from 0.30501 to 0.30918, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 170/300
 - 8s - loss: 4.2946 - acc: 0.8978 - mDice: 0.2956 - val_loss: 4.2882 - val_acc: 0.9247 - val_mDice: 0.3040

Epoch 00170: val_mDice did not improve from 0.30918
Epoch 171/300
 - 8s - loss: 4.2976 - acc: 0.8979 - mDice: 0.2962 - val_loss: 4.2514 - val_acc: 0.9218 - val_mDice: 0.2919

Epoch 00171: val_mDice did not improve from 0.30918
Epoch 172/300
 - 8s - loss: 4.2801 - acc: 0.8979 - mDice: 0.2966 - val_loss: 4.0571 - val_acc: 0.9233 - val_mDice: 0.3066

Epoch 00172: val_mDice did not improve from 0.30918
Epoch 173/300
 - 9s - loss: 4.2794 - acc: 0.8978 - mDice: 0.2973 - val_loss: 3.9834 - val_acc: 0.9253 - val_mDice: 0.3177

Epoch 00173: val_mDice improved from 0.30918 to 0.31774, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 174/300
 - 9s - loss: 4.2824 - acc: 0.8976 - mDice: 0.2971 - val_loss: 4.3575 - val_acc: 0.9225 - val_mDice: 0.3030

Epoch 00174: val_mDice did not improve from 0.31774
Epoch 175/300
 - 8s - loss: 4.2734 - acc: 0.8978 - mDice: 0.2986 - val_loss: 4.4156 - val_acc: 0.9226 - val_mDice: 0.2988

Epoch 00175: val_mDice did not improve from 0.31774
Epoch 176/300
 - 8s - loss: 4.2726 - acc: 0.8979 - mDice: 0.2991 - val_loss: 4.2186 - val_acc: 0.9223 - val_mDice: 0.3007

Epoch 00176: val_mDice did not improve from 0.31774
Epoch 177/300
 - 8s - loss: 4.2672 - acc: 0.8978 - mDice: 0.2992 - val_loss: 4.5241 - val_acc: 0.9214 - val_mDice: 0.3001

Epoch 00177: val_mDice did not improve from 0.31774
Epoch 178/300
 - 8s - loss: 4.2743 - acc: 0.8976 - mDice: 0.2980 - val_loss: 4.0765 - val_acc: 0.9223 - val_mDice: 0.3061

Epoch 00178: val_mDice did not improve from 0.31774
Epoch 179/300
 - 8s - loss: 4.2552 - acc: 0.8978 - mDice: 0.3003 - val_loss: 4.2082 - val_acc: 0.9232 - val_mDice: 0.3049

Epoch 00179: val_mDice did not improve from 0.31774
Epoch 180/300
 - 8s - loss: 4.2525 - acc: 0.8976 - mDice: 0.3004 - val_loss: 4.2031 - val_acc: 0.9219 - val_mDice: 0.3101

Epoch 00180: val_mDice did not improve from 0.31774
Epoch 181/300
 - 8s - loss: 4.2536 - acc: 0.8978 - mDice: 0.3006 - val_loss: 4.5173 - val_acc: 0.9238 - val_mDice: 0.3040

Epoch 00181: val_mDice did not improve from 0.31774
Epoch 182/300
 - 8s - loss: 4.2548 - acc: 0.8978 - mDice: 0.3005 - val_loss: 4.3738 - val_acc: 0.9221 - val_mDice: 0.3005

Epoch 00182: val_mDice did not improve from 0.31774
Epoch 183/300
 - 8s - loss: 4.2480 - acc: 0.8975 - mDice: 0.3010 - val_loss: 4.1939 - val_acc: 0.9222 - val_mDice: 0.3087

Epoch 00183: val_mDice did not improve from 0.31774
Epoch 184/300
 - 9s - loss: 4.2467 - acc: 0.8977 - mDice: 0.3023 - val_loss: 4.2155 - val_acc: 0.9226 - val_mDice: 0.3111

Epoch 00184: val_mDice did not improve from 0.31774
Epoch 185/300
 - 8s - loss: 4.2485 - acc: 0.8977 - mDice: 0.3020 - val_loss: 4.2076 - val_acc: 0.9228 - val_mDice: 0.3066

Epoch 00185: val_mDice did not improve from 0.31774
Epoch 186/300
 - 9s - loss: 4.2443 - acc: 0.8978 - mDice: 0.3021 - val_loss: 4.2126 - val_acc: 0.9215 - val_mDice: 0.3056

Epoch 00186: val_mDice did not improve from 0.31774
Epoch 187/300
 - 9s - loss: 4.2555 - acc: 0.8975 - mDice: 0.3000 - val_loss: 4.2200 - val_acc: 0.9237 - val_mDice: 0.3017

Epoch 00187: val_mDice did not improve from 0.31774
Epoch 188/300
 - 8s - loss: 4.2337 - acc: 0.8976 - mDice: 0.3033 - val_loss: 4.1023 - val_acc: 0.9223 - val_mDice: 0.3133

Epoch 00188: val_mDice did not improve from 0.31774
Epoch 189/300
 - 8s - loss: 4.2338 - acc: 0.8975 - mDice: 0.3026 - val_loss: 4.1043 - val_acc: 0.9219 - val_mDice: 0.3141

Epoch 00189: val_mDice did not improve from 0.31774
Epoch 190/300
 - 8s - loss: 4.2333 - acc: 0.8975 - mDice: 0.3032 - val_loss: 4.2977 - val_acc: 0.9238 - val_mDice: 0.3020

Epoch 00190: val_mDice did not improve from 0.31774
Epoch 191/300
 - 9s - loss: 4.2229 - acc: 0.8977 - mDice: 0.3045 - val_loss: 4.4016 - val_acc: 0.9224 - val_mDice: 0.2998

Epoch 00191: val_mDice did not improve from 0.31774
Epoch 192/300
 - 8s - loss: 4.2175 - acc: 0.8976 - mDice: 0.3045 - val_loss: 4.3637 - val_acc: 0.9218 - val_mDice: 0.3063

Epoch 00192: val_mDice did not improve from 0.31774
Epoch 193/300
 - 8s - loss: 4.2170 - acc: 0.8977 - mDice: 0.3057 - val_loss: 4.1686 - val_acc: 0.9222 - val_mDice: 0.3118

Epoch 00193: val_mDice did not improve from 0.31774
Epoch 194/300
 - 8s - loss: 4.2133 - acc: 0.8976 - mDice: 0.3059 - val_loss: 4.3006 - val_acc: 0.9232 - val_mDice: 0.3020

Epoch 00194: val_mDice did not improve from 0.31774
Epoch 195/300
 - 8s - loss: 4.2167 - acc: 0.8977 - mDice: 0.3060 - val_loss: 4.4008 - val_acc: 0.9216 - val_mDice: 0.2987

Epoch 00195: val_mDice did not improve from 0.31774
Epoch 196/300
 - 8s - loss: 4.2113 - acc: 0.8977 - mDice: 0.3062 - val_loss: 4.3775 - val_acc: 0.9227 - val_mDice: 0.2975

Epoch 00196: val_mDice did not improve from 0.31774
Epoch 197/300
 - 8s - loss: 4.2019 - acc: 0.8977 - mDice: 0.3073 - val_loss: 4.1860 - val_acc: 0.9222 - val_mDice: 0.3085

Epoch 00197: val_mDice did not improve from 0.31774
Epoch 198/300
 - 8s - loss: 4.1969 - acc: 0.8975 - mDice: 0.3078 - val_loss: 4.1366 - val_acc: 0.9221 - val_mDice: 0.3119

Epoch 00198: val_mDice did not improve from 0.31774
Epoch 199/300
 - 9s - loss: 4.1975 - acc: 0.8976 - mDice: 0.3079 - val_loss: 4.3194 - val_acc: 0.9226 - val_mDice: 0.3141

Epoch 00199: val_mDice did not improve from 0.31774
Epoch 200/300
 - 8s - loss: 4.2038 - acc: 0.8975 - mDice: 0.3072 - val_loss: 4.1992 - val_acc: 0.9231 - val_mDice: 0.3144

Epoch 00200: val_mDice did not improve from 0.31774
Epoch 201/300
 - 8s - loss: 4.1951 - acc: 0.8974 - mDice: 0.3080 - val_loss: 4.2780 - val_acc: 0.9229 - val_mDice: 0.3058

Epoch 00201: val_mDice did not improve from 0.31774
Epoch 202/300
 - 8s - loss: 4.1955 - acc: 0.8976 - mDice: 0.3081 - val_loss: 4.2857 - val_acc: 0.9210 - val_mDice: 0.3059

Epoch 00202: val_mDice did not improve from 0.31774
Epoch 203/300
 - 8s - loss: 4.1980 - acc: 0.8976 - mDice: 0.3080 - val_loss: 4.2045 - val_acc: 0.9218 - val_mDice: 0.3133

Epoch 00203: val_mDice did not improve from 0.31774
Restoring model weights from the end of the best epoch
Epoch 00203: early stopping
{'val_loss': [277.92644728147064, 179.03083990170404, 110.13441988138052, 64.72831566517169, 35.59685566792121, 23.30605991070087, 16.7380912899971, 13.106495334551884, 11.437763067392202, 10.569994791195942, 9.784796623083261, 9.437851263926579, 9.083919036846895, 8.805695519997524, 8.614222533427752, 8.476622001482891, 8.42407774925232, 8.288167971831102, 8.065573187974783, 8.060848976557072, 8.085132520932417, 8.150706690091353, 7.770116652433689, 8.406175285577774, 7.777898742602422, 7.486420035362244, 7.4371235852058115, 7.357726692007138, 7.229178052682143, 7.288534808617372, 6.998959322388355, 7.084233420399519, 6.9198737396643715, 6.999271393968509, 6.939619444883787, 6.734988436102867, 6.968983010603831, 6.889322182306876, 6.541627822014002, 6.379027072053689, 6.504986492487101, 6.4348215976586705, 6.251274882600858, 6.225037290499761, 6.291161685035779, 6.185564656670277, 6.031078800559044, 6.11818804190709, 6.052828239706846, 6.005532834392327, 5.894494578242302, 5.8448974536015434, 5.838746117857786, 5.766023138394723, 5.6415999462971325, 5.711860557015125, 5.712331531139521, 5.6780597028824, 5.702946035907819, 5.667048970094094, 5.721099008734409, 5.521560114163619, 5.512662346546467, 5.590408073021815, 5.385634305385443, 5.364129785161752, 5.304683421666805, 5.3556634325247545, 5.251822794859226, 5.311295029062491, 5.127209933904501, 5.1533809911746244, 5.149625005630346, 5.184415294573857, 5.020744140331562, 5.0373428372236395, 5.223164340624442, 4.8600734655673685, 5.236388663832958, 4.93693106334943, 4.926744090823027, 4.925596872201333, 5.141198527354461, 5.017045680146951, 4.902304116349954, 4.968722254037857, 4.867597437821901, 4.912495303612489, 4.752802353638869, 5.065979965604269, 4.806460773715606, 4.648360292498882, 4.619813937407273, 4.842445059464528, 4.8167875202802515, 4.846885341864366, 4.658452718303754, 5.144729271531105, 4.732861920044972, 5.033089245741184, 5.056116810211768, 4.8545429179301625, 4.884151365894538, 4.812230700483689, 4.752232612325595, 4.55605047597335, 4.864873498678207, 4.577094487272776, 4.757115546327371, 4.605169203418952, 4.67352527150741, 4.567803792082346, 4.624914849033723, 4.760752174716729, 4.414354471059946, 4.300092297104689, 5.047633579144111, 4.444504662201955, 4.589465112640307, 4.53777533196486, 4.40780461522249, 4.399933443619655, 4.577230591040391, 4.385487951911413, 4.295205340935634, 4.672137338381547, 4.336855213229473, 4.666111194170439, 4.403321361312499, 4.294706898239943, 4.3001205657537165, 4.313530896718685, 4.420018349702541, 4.386702664769613, 4.372921412953963, 4.39496251138357, 4.214100987865375, 4.380918578459666, 4.380252081614274, 4.229689124685067, 4.202409590666111, 4.322193627174084, 4.323898002505302, 4.203903101957762, 4.359895903330583, 4.398328530100676, 4.307460720722492, 4.133185210136267, 4.365504287756407, 4.502566064779575, 4.326121820853307, 4.264798359229014, 4.334588303015782, 4.138103104554689, 4.16647247167734, 4.304507339229951, 4.312748787494806, 4.345937514534364, 4.510231231267635, 4.496583438836611, 4.252777984509101, 4.580503592124352, 4.209800536815937, 4.2571978981678305, 4.213822183700708, 4.271426443870251, 4.31761126105602, 4.339377719622392, 4.209131953807978, 4.288192691711279, 4.251404849382547, 4.057149190169114, 3.983420592087966, 4.357501360086294, 4.415552714696298, 4.218568850022096, 4.524094994251545, 4.076457420220742, 4.208224688584988, 4.2031071713337536, 4.51727088368856, 4.373789752905186, 4.193948404147075, 4.215450206628213, 4.2075817493292, 4.212601326979124, 4.219970955298497, 4.10233262181282, 4.104315212139716, 4.297671840741084, 4.401611598638388, 4.363650915714411, 4.168613665379011, 4.300571019832905, 4.400781062933115, 4.3775351528938, 4.185962548622718, 4.136595345460451, 4.319407185682883, 4.199173374817922, 4.27799946298966, 4.2856537241202135, 4.20448423578189], 'val_acc': [0.8304641155096201, 0.9005362460246453, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.9033862077272855, 0.9033838579287896, 0.9033815654424521, 0.9033792729561145, 0.9033746306712811, 0.9033630536152766, 0.9033584503027109, 0.9033561188441056, 0.9032636949649224, 0.9033006613071148, 0.9031897233082697, 0.9032452014776376, 0.9032983298485096, 0.9032636559926547, 0.9030233071400569, 0.9024200439453125, 0.9015786601946905, 0.9035918827240284, 0.8999236959677476, 0.9001363790952243, 0.9033977274711316, 0.9024454492789048, 0.9018675891252664, 0.898620089659324, 0.9093957887246058, 0.9081985193949479, 0.9140648383360642, 0.9139237931141486, 0.915148879473026, 0.9120585322380066, 0.9133991392759176, 0.9146750019146845, 0.9136464595794678, 0.9139954906243545, 0.9143745532402625, 0.9126872076438024, 0.9127311454369471, 0.9119291007518768, 0.9117580239589398, 0.9134199435894306, 0.9098511727956625, 0.9154100693189181, 0.9134800067314734, 0.912555469916417, 0.9190273468311017, 0.9174902920539563, 0.9174718191990485, 0.9174718191990485, 0.9185881798083966, 0.9170534587823428, 0.9198525249958038, 0.9191961105053241, 0.9205343860846299, 0.920506660754864, 0.9209065529016348, 0.9199657967457404, 0.9203795240475581, 0.9191753016068385, 0.9211515188217163, 0.9192931560369638, 0.920409578543443, 0.9210082407181079, 0.9190112031423129, 0.9206476509571075, 0.9212463108392862, 0.9192839517043188, 0.9207401000536405, 0.9201876842058622, 0.9196306329507095, 0.9197646815043229, 0.9204234274534079, 0.9179641466874343, 0.9210567680689005, 0.9211816099973825, 0.9209735645697668, 0.9214843740830054, 0.9213225589348719, 0.919928789138794, 0.920400314606153, 0.9206337653673612, 0.9204350274342757, 0.9208996181304638, 0.9199866125216851, 0.9210914235848647, 0.9213595871741955, 0.9212994667199942, 0.9196745524039636, 0.9206153108523443, 0.9191868259356573, 0.9200490300471966, 0.9218911574437068, 0.9200975551055028, 0.9214127499323624, 0.9199611705083114, 0.92071700325379, 0.9212254767234509, 0.9210775723824134, 0.9221223363509545, 0.9210868386121897, 0.9215560532533206, 0.9218657291852511, 0.9216438554800473, 0.9221338927745819, 0.9228666103803195, 0.9219212256945096, 0.9227255628659174, 0.9205783078303704, 0.9208672161285694, 0.9214057968213007, 0.9226863063298739, 0.9235091346960801, 0.9247156725480006, 0.9216901063919067, 0.9236085346111884, 0.9233820530084463, 0.922769518998953, 0.921835734294011, 0.9239067022617047, 0.9227880101937515, 0.9226100032146161, 0.9249329773279337, 0.9265786982499636, 0.9248150701706226, 0.9246001472839942, 0.9228087824124557, 0.9233773946762085, 0.9243435447032635, 0.9225106583191798, 0.9242973075463221, 0.9253305334311265, 0.9248844545621139, 0.9248936841121087, 0.9258829790812272, 0.9226562541264755, 0.9230769253694094, 0.9250508248806, 0.923150897026062, 0.9256148017369784, 0.924295037984848, 0.9254692311470325, 0.9209180726454809, 0.9231139283913833, 0.9231578501371237, 0.9249676259664389, 0.9229960212340722, 0.9246139755615821, 0.9247988829245934, 0.9247180361014146, 0.9217756344721868, 0.9233265725465921, 0.9252820083728204, 0.9225476269538586, 0.9226123484281393, 0.9223049260102786, 0.9214127292999854, 0.9223372569450965, 0.9232132870417374, 0.9218980899223914, 0.9237795701393714, 0.9220899687363551, 0.9222471576470596, 0.9225522096340473, 0.9227510255116683, 0.9215213518876296, 0.923689434161553, 0.9222517815920023, 0.9218842318424811, 0.9238211718889383, 0.9223672976860633, 0.9218126122768109, 0.9221685597529778, 0.9231670751021459, 0.9215883704332205, 0.922723263502121, 0.9222194162698892, 0.9220553063429319, 0.9226192465195289, 0.9231485724449158, 0.9229012658962836, 0.9210036328205695, 0.9217964296157544], 'val_mDice': [0.016600592885739528, 0.0177172812131735, 0.02039181820761699, 0.02132478582028013, 0.020687124835183986, 0.020442024231530152, 0.020972035037210353, 0.021406214755888168, 0.022053143869225796, 0.02301598308034814, 0.020428991339241084, 0.0223365814711612, 0.026041162809213765, 0.028012024883467417, 0.029494749346318152, 0.030978653221749343, 0.031039849031143464, 0.03197204962802621, 0.035854047594162136, 0.037338668337235086, 0.0386450497720104, 0.03846884604830008, 0.04533154295327572, 0.03468950930982828, 0.04447417301484025, 0.05138714504070007, 0.05231764597388414, 0.05636590597434686, 0.0611333202284116, 0.059599978992572196, 0.06710317272406358, 0.06363290118483397, 0.073445889668969, 0.06985427983678304, 0.07391478178592828, 0.08003128649523625, 0.07117743102403787, 0.08888509138845481, 0.09088341741321179, 0.10445443734240073, 0.09356157961659707, 0.10220798649466954, 0.10811545749982962, 0.11008829351228017, 0.11358460425757445, 0.10912412075469127, 0.11927482729347852, 0.1143340110205687, 0.12279158801986621, 0.12444267020775722, 0.12989745217447096, 0.13439312964104688, 0.134660820548351, 0.13729941930908424, 0.14196547063497397, 0.14114103394632155, 0.13873307449886432, 0.14117743103550032, 0.1464827676805166, 0.1441763682434192, 0.1411009354946705, 0.1538681800548847, 0.1508890320188724, 0.15881228088759458, 0.1625633595081476, 0.16557144316343161, 0.17069962620735168, 0.16799663451428598, 0.1724259194273215, 0.17671269221374622, 0.18107198952482298, 0.18455289877378023, 0.1862575811548875, 0.18599304327597985, 0.1983550051943614, 0.19757622996201882, 0.19509016865721115, 0.20462666265666485, 0.1979089228866192, 0.2000411254568742, 0.2103959132845585, 0.2093354847568732, 0.212428325930467, 0.21643865681611574, 0.21552920542084253, 0.21579765471128318, 0.21692042539899165, 0.21758447057352617, 0.22744820897395795, 0.21797100669489458, 0.22327690499906355, 0.22870435938239098, 0.22835041611240461, 0.21937774895475462, 0.2278209924697876, 0.23317357381949058, 0.2385183204538547, 0.227076998553597, 0.24212588398502424, 0.23282557926498926, 0.23626402106422645, 0.23866239849191445, 0.23107995465397835, 0.23785990018111008, 0.24274012489387622, 0.2476682592756473, 0.24141438104785407, 0.25083400753255075, 0.23966889269649982, 0.24760317859741357, 0.25130771587674433, 0.2480631904819837, 0.2578147119627549, 0.2490942654414819, 0.26016148623938745, 0.2659808351443364, 0.24536314764275, 0.2593341623074733, 0.2616271065691343, 0.26794279309419483, 0.2716995057864831, 0.27815257442685276, 0.2657770568934771, 0.26937726928064454, 0.28117554176312226, 0.26554919607364214, 0.2698795458731743, 0.2708415479327624, 0.27588107236302817, 0.2734937507372636, 0.27867127037965334, 0.2787286534618873, 0.28249625947612983, 0.2817289519768495, 0.2783421960014563, 0.2814193952542085, 0.2847936640565212, 0.27895394225533193, 0.28119502030313015, 0.2960126090508241, 0.2872988587388626, 0.28928459636293924, 0.29074039997962803, 0.30035334023145527, 0.2894641960469576, 0.2887052587018563, 0.292643895229468, 0.30500515779623616, 0.2896709040953563, 0.27425845941671956, 0.28876854121111906, 0.2911256988747762, 0.29005066176446587, 0.29807773060523546, 0.30001544823440224, 0.296457219009216, 0.2955450743723374, 0.29486678970547825, 0.28214073037871945, 0.2938499078154564, 0.2933342465414451, 0.2939599074709874, 0.2854895075926414, 0.29156136627380663, 0.2940764076148088, 0.3006333551154687, 0.29716331540391994, 0.3004191555082798, 0.30917611431617004, 0.303978969844488, 0.29186182755690354, 0.3065714422040261, 0.31773723819508004, 0.30297963679409945, 0.29881686258774537, 0.30068559314195925, 0.3000620102080015, 0.3061021494750793, 0.30492604122712064, 0.31007252690883785, 0.3039718155677502, 0.3004526742375814, 0.30865049476806933, 0.31107056628053004, 0.3066416834122859, 0.30561587099845594, 0.3016676888442956, 0.3132830554476151, 0.31412141082378536, 0.3020162060856819, 0.2997998153934112, 0.3062632264426121, 0.3117518164217472, 0.3019607591514404, 0.29870525733209574, 0.2974803950637579, 0.3084505067135279, 0.31193682880928886, 0.31408559975142664, 0.3144027188133735, 0.30581314775806206, 0.30590620321723133, 0.31328212555784446], 'loss': [374.7830019701696, 237.83156355045426, 140.5048653140759, 83.23215464513086, 53.13549093288501, 37.72822787622503, 29.42615576053385, 24.49720554744077, 21.30852735147016, 19.114151812738722, 17.492476742744532, 16.129206888770018, 15.11256848711441, 14.335059886025753, 13.725598435972291, 13.22744321334981, 12.783925652646891, 12.3626216393956, 11.960382526237279, 11.603638347439976, 11.276369807267885, 10.9713853111151, 10.695189514888407, 10.436667439551258, 10.20976466852438, 10.003704750789638, 9.792460576025507, 9.611992691601706, 9.434846040563347, 9.254579837434594, 9.075580986291733, 8.934236602568598, 8.773908685254817, 8.638323257981572, 8.496091773718208, 8.380214671900404, 8.24224738743564, 8.134823013339263, 8.012911094353118, 7.859155085301648, 7.7353130173421505, 7.646052269899108, 7.552697284980463, 7.465403948104614, 7.375669779465733, 7.286947951989234, 7.21328983357018, 7.145475888201685, 7.071838392055223, 6.990556164332271, 6.91002394925907, 6.851594572784957, 6.791212753452279, 6.731952636320851, 6.664651752804562, 6.610294958217705, 6.559176533685931, 6.49716531699361, 6.45426990479481, 6.393407687395028, 6.338470988703271, 6.29934425759907, 6.236654060106595, 6.186399322500267, 6.134047657825801, 6.079311063212024, 6.025905626721459, 5.97857473180884, 5.913710711066775, 5.865077196073018, 5.823321322964685, 5.778184057480403, 5.739733617375898, 5.693744666526032, 5.65507183393341, 5.625294939108158, 5.564186860744797, 5.519758680314691, 5.494395605259844, 5.449112808331676, 5.424616457164194, 5.37363765557006, 5.369047590939308, 5.3098705049965265, 5.28102838101449, 5.256801175563239, 5.221182475151154, 5.193422854303995, 5.160401154694649, 5.141141227368421, 5.135018015066152, 5.10735605185865, 5.090282072190219, 5.056405479030878, 5.0357911419650785, 5.020807700239401, 5.006768947970017, 4.984952262458706, 4.966727091137426, 4.930439569144674, 4.934878595286736, 4.904236307951574, 4.893618181942904, 4.877240662951647, 4.865277285198988, 4.845896269533452, 4.822218110351823, 4.809964673430962, 4.785588589429658, 4.769880287873506, 4.738616578574331, 4.737166553595558, 4.7171275252116445, 4.703573468139798, 4.685319239194132, 4.653361773827326, 4.6491691105855875, 4.646643567269967, 4.629583056360266, 4.618316328141848, 4.594934587470722, 4.59579430166936, 4.575233524557872, 4.580335374930836, 4.554629202402683, 4.55603907259586, 4.553957731040171, 4.528718985348492, 4.529646432158978, 4.514728046074835, 4.496215879790713, 4.511924024438722, 4.491204278040095, 4.487700506867512, 4.479246896944571, 4.460006444247283, 4.46444734162228, 4.44465125664721, 4.441181531177174, 4.432841738981261, 4.434084586075594, 4.419260738993839, 4.429994256527433, 4.405915411627074, 4.404926352809631, 4.4116611553494, 4.388813093218055, 4.382018340510173, 4.374186276169179, 4.391242189555989, 4.373171855984914, 4.374598128444032, 4.360822611977382, 4.3529398326865865, 4.346746677601677, 4.34700228653853, 4.334407709274391, 4.346422157493848, 4.334518708459766, 4.323779561175947, 4.322469791824677, 4.314219958478978, 4.323276074942128, 4.319002899210644, 4.310954580445093, 4.301771124229653, 4.308010742605629, 4.292369016013022, 4.297682581633469, 4.294616827915969, 4.297589344489314, 4.280101643226073, 4.279448199291945, 4.2823888585541745, 4.273398083568796, 4.272624410321261, 4.26717360348628, 4.274289058823213, 4.255172741475783, 4.2525005283736546, 4.253592110632267, 4.254835148716527, 4.247985172368477, 4.246668838678466, 4.248503291869663, 4.244305423613702, 4.255505225584316, 4.233737124218599, 4.2338283974603454, 4.233330298160523, 4.222866770351086, 4.217546531868896, 4.217000347105964, 4.213320186434335, 4.216664270172587, 4.211302598497035, 4.201851307339832, 4.196912756487748, 4.197472412076305, 4.203837357598192, 4.195071123902521, 4.1955161694175125, 4.1979991231118134], 'acc': [0.32427217323413077, 0.7980742639614891, 0.8559983768476768, 0.8695350267356539, 0.8745960464262494, 0.8747182487342055, 0.8747641830814618, 0.8747880377915033, 0.8748053490351478, 0.874838515669506, 0.8748462758556367, 0.8748469830361897, 0.874847006942913, 0.874847006860457, 0.8748470057445534, 0.8748470088668847, 0.8748470089603347, 0.8748470105269975, 0.874847005354262, 0.8748467855652265, 0.8748392017028609, 0.8747677103043658, 0.8745985964034345, 0.874368256059485, 0.874207035983762, 0.874019506616829, 0.8738618818781071, 0.8737514119278864, 0.873589483027989, 0.8734720078196724, 0.8733491871142139, 0.8734569783240659, 0.8736290779927774, 0.8739130473231451, 0.874387696309799, 0.875061097877729, 0.8757578607888764, 0.8766730924676421, 0.8776429016893776, 0.8785010177931046, 0.8790315794436835, 0.8793864715344504, 0.8797457509404518, 0.8800889124050578, 0.8803729975970043, 0.8807789408873667, 0.880892492880914, 0.881301521996582, 0.881512421161214, 0.8820675230164771, 0.8827462919378681, 0.8831615278633562, 0.8837865332742244, 0.884325051944238, 0.8849300584878406, 0.8853845755618337, 0.8857012282138621, 0.8860480254133788, 0.8862797409848491, 0.8866202895254114, 0.8869963323268505, 0.8869146112805086, 0.8876700636855265, 0.8883912161048967, 0.8886782006288184, 0.8894691880609259, 0.889957251077559, 0.8904266689994135, 0.8909990455318638, 0.891389606288798, 0.8920887240589099, 0.8926154529433799, 0.8928403362108203, 0.8932811818658938, 0.8938550600088204, 0.8938877189620047, 0.8939601433493715, 0.8944934569556896, 0.8945389041368079, 0.8946197787620128, 0.8945656851144291, 0.8949092025467911, 0.8947781145896989, 0.8949457840441031, 0.8950375195832795, 0.8950976902935257, 0.8953102707730924, 0.8952694580166429, 0.8954710925481839, 0.8955881246603097, 0.8955835789197443, 0.8958192847426567, 0.8956915885333722, 0.8959769320028421, 0.8959743600703909, 0.8959007563268216, 0.895940021050105, 0.8961248037274843, 0.8960140902793992, 0.8962915408471672, 0.8960686709831795, 0.8961741998166404, 0.8962604138152599, 0.8962648724273904, 0.8963777354741398, 0.8961674150786892, 0.8962616113777322, 0.8962004454628651, 0.8963398036784839, 0.8961533573592246, 0.8962366261034118, 0.8962379593113895, 0.8963513096452123, 0.896284869073843, 0.8962401519465533, 0.896300011836415, 0.8961545109177146, 0.8961189258453712, 0.8959696191126577, 0.8961822249886515, 0.8963016525720369, 0.8963354144117923, 0.8963258171899506, 0.8960900635535918, 0.8962977041202553, 0.8962099764121257, 0.8962365164535133, 0.8965206639436649, 0.8962855096354999, 0.8964366847921493, 0.8965581079904943, 0.8964295697852077, 0.8965476222620078, 0.8965349848129911, 0.8968527420941593, 0.8969793715545593, 0.8970008138184132, 0.8973270814258059, 0.8973598941593035, 0.8971898093782513, 0.897393699365382, 0.8975070077456265, 0.8971857088887251, 0.8973822864968981, 0.8975172729759565, 0.8974436943319729, 0.8976144447852619, 0.8977722907642437, 0.8976920169349484, 0.8976309148904462, 0.8978873276352102, 0.8978582192509528, 0.8977526470291805, 0.8979126426288148, 0.8978456690842888, 0.897613381373094, 0.8979969352722872, 0.8976774489836731, 0.8977924866407194, 0.8981106206080708, 0.8979763633198022, 0.8978369370285659, 0.8978033969842738, 0.897793065684754, 0.8978980373579847, 0.8978662431806279, 0.8977866595404664, 0.8979225341743575, 0.8977836426867997, 0.8977955707014679, 0.8978558911956633, 0.8978697087759683, 0.8977639764742493, 0.8976389454110988, 0.8977858571233118, 0.8978886817869947, 0.8977784300921292, 0.8976309153686907, 0.8978185143229334, 0.8976371246741771, 0.8977722685506156, 0.8978065657052763, 0.897545182780785, 0.8977457317415005, 0.8977002186945836, 0.8978313935272458, 0.8974752382610729, 0.8976130033236395, 0.8975417748551235, 0.8975233485818975, 0.8977100843049872, 0.8975634755303716, 0.8976576074478229, 0.8976254646797677, 0.8977155607806288, 0.897670377903755, 0.8976573439901258, 0.8975331040751524, 0.8976194319728994, 0.8974700293274457, 0.8974465756555726, 0.8975892148092204, 0.897619655576896], 'mDice': [0.01727434964686492, 0.019771004355925097, 0.021225847462938575, 0.02155368751010958, 0.021980327650159802, 0.02140982784993928, 0.021086212133202487, 0.02131497410398221, 0.02183852986773859, 0.022536101265827866, 0.02344545455262811, 0.024361209097514786, 0.02566465606976015, 0.027179216327858424, 0.028453188504531386, 0.029672188732443587, 0.031015341260232445, 0.032651606487835355, 0.03421375697837164, 0.03598635752218891, 0.03776913337367581, 0.03961874110971025, 0.041446147064496196, 0.04322889347577968, 0.044750865986896246, 0.046170660321132886, 0.04789377645518379, 0.04956681256660135, 0.05164676076168011, 0.054223529625497466, 0.05713862914705061, 0.05996169056356871, 0.06313343644471975, 0.06595114993508161, 0.0687539411720222, 0.07107601307018496, 0.07404923709206483, 0.07644556200417449, 0.07954007468036782, 0.08285378685620816, 0.08585245953756847, 0.08821584018728758, 0.09057769354563033, 0.09295994437558289, 0.09496622774751712, 0.09775625338311852, 0.09955790891032658, 0.10221285978336457, 0.10458126081473096, 0.10780063828718484, 0.11081061787497214, 0.11301974392551278, 0.1149897402921836, 0.11742829287292926, 0.11960407384348061, 0.12155101555009219, 0.12326883170370641, 0.12523564945054014, 0.1264747885180984, 0.1287835032911821, 0.13071295032199542, 0.13165495548451242, 0.1338625528639141, 0.13655254998314176, 0.1386931274736608, 0.1414843615016341, 0.1441446525163012, 0.14713641577328548, 0.15056922558509894, 0.15313079946047484, 0.15644710119234773, 0.15915371168806447, 0.16165760920285893, 0.1649832547667457, 0.16827769835590273, 0.1702068110231015, 0.17305569335739956, 0.1763614840880428, 0.17813637314231526, 0.18071136807291577, 0.18258422882087685, 0.18593580946209443, 0.1877423016831827, 0.1922626012334478, 0.19449332137944722, 0.19690762531556164, 0.19988151317640807, 0.20115258487582724, 0.20358995452727996, 0.20541384442369515, 0.2057817521805936, 0.2080455091107236, 0.20900887313566777, 0.21108799647286647, 0.21265950857532054, 0.2140133310816093, 0.2147299326687603, 0.21752861799057435, 0.21851425017447948, 0.22080536327145037, 0.2210234687932026, 0.22291255711871497, 0.22445238756867425, 0.22514244295085672, 0.22666410899938477, 0.22851865903473445, 0.23029698156913597, 0.23114079553889885, 0.23383409960138218, 0.23593596805563538, 0.23886620333925182, 0.23965106573995823, 0.24220028870068144, 0.24300043412974584, 0.24511859101127964, 0.24779120155512313, 0.24882068486601996, 0.24957496756905065, 0.25095638237535584, 0.2527864922058271, 0.25524040917832175, 0.25634458030291524, 0.25752307139429825, 0.25789110497894907, 0.26111143146077503, 0.2609385717984859, 0.26117946353882626, 0.2642757514080944, 0.2642921555100536, 0.26561828038299545, 0.2674902224743002, 0.26731943253011686, 0.2691568106393648, 0.27011484864188395, 0.2712717479493228, 0.27276281593449053, 0.273291566522977, 0.2758167921798669, 0.2766782707155691, 0.2772095952924081, 0.27822997849178654, 0.2792557151480529, 0.27858291583000533, 0.2804628444022494, 0.28135316654796544, 0.2801572483431135, 0.2831454205301913, 0.28456581739947046, 0.2851040867419851, 0.2830155885363597, 0.2851145130609725, 0.2857861134257911, 0.286783008544436, 0.28819110470098247, 0.2888389772059655, 0.28867087345067, 0.28983022307630596, 0.2892361534820385, 0.290796778435792, 0.29201921337173947, 0.29167008935926936, 0.2927095861295531, 0.29153781481458946, 0.29209787342300564, 0.2930491171859017, 0.2943620425104864, 0.2939726201951212, 0.29498625296371334, 0.2949601851831797, 0.2955977158287866, 0.2962454258900864, 0.2966032418599595, 0.2972768731913443, 0.297084876922711, 0.29855944403288914, 0.29911222664190484, 0.29924249169154327, 0.29804327527414465, 0.3002915562826319, 0.30044871386177346, 0.3005873237760659, 0.300457944514775, 0.3009634794514304, 0.3022929962343432, 0.3019706807040447, 0.3020930404863399, 0.29999076592012996, 0.30325081934845777, 0.30256630910571997, 0.3032006985568631, 0.3044790709732226, 0.3045208300873988, 0.30571356307731323, 0.30587427758823116, 0.30597961386510236, 0.3061507795114947, 0.30730999724611874, 0.3077765504152674, 0.30793040693163554, 0.30720724210103234, 0.30798509902513455, 0.30805814112561947, 0.3079832727966477]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.43s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.16s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.94s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:26,  1.36s/it]predicting train subjects:   1%|          | 2/285 [00:03<06:55,  1.47s/it]predicting train subjects:   1%|          | 3/285 [00:04<06:57,  1.48s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:24,  1.58s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<07:08,  1.53s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:32,  1.62s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<08:01,  1.73s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<08:09,  1.77s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<07:56,  1.73s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:26,  1.84s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:39,  1.90s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:45,  1.92s/it]predicting train subjects:   5%|▍         | 13/285 [00:23<08:53,  1.96s/it]predicting train subjects:   5%|▍         | 14/285 [00:25<08:52,  1.97s/it]predicting train subjects:   5%|▌         | 15/285 [00:27<08:52,  1.97s/it]predicting train subjects:   6%|▌         | 16/285 [00:29<08:53,  1.98s/it]predicting train subjects:   6%|▌         | 17/285 [00:31<08:48,  1.97s/it]predicting train subjects:   6%|▋         | 18/285 [00:33<08:47,  1.98s/it]predicting train subjects:   7%|▋         | 19/285 [00:35<08:44,  1.97s/it]predicting train subjects:   7%|▋         | 20/285 [00:37<08:48,  2.00s/it]predicting train subjects:   7%|▋         | 21/285 [00:39<08:44,  1.99s/it]predicting train subjects:   8%|▊         | 22/285 [00:41<08:38,  1.97s/it]predicting train subjects:   8%|▊         | 23/285 [00:43<08:40,  1.99s/it]predicting train subjects:   8%|▊         | 24/285 [00:45<08:38,  1.99s/it]predicting train subjects:   9%|▉         | 25/285 [00:47<08:36,  1.99s/it]predicting train subjects:   9%|▉         | 26/285 [00:49<08:33,  1.98s/it]predicting train subjects:   9%|▉         | 27/285 [00:50<08:24,  1.96s/it]predicting train subjects:  10%|▉         | 28/285 [00:52<08:13,  1.92s/it]predicting train subjects:  10%|█         | 29/285 [00:54<08:13,  1.93s/it]predicting train subjects:  11%|█         | 30/285 [00:56<08:05,  1.90s/it]predicting train subjects:  11%|█         | 31/285 [00:58<07:59,  1.89s/it]predicting train subjects:  11%|█         | 32/285 [01:00<07:54,  1.88s/it]predicting train subjects:  12%|█▏        | 33/285 [01:02<07:50,  1.87s/it]predicting train subjects:  12%|█▏        | 34/285 [01:04<07:50,  1.87s/it]predicting train subjects:  12%|█▏        | 35/285 [01:05<07:45,  1.86s/it]predicting train subjects:  13%|█▎        | 36/285 [01:07<07:48,  1.88s/it]predicting train subjects:  13%|█▎        | 37/285 [01:09<07:44,  1.87s/it]predicting train subjects:  13%|█▎        | 38/285 [01:11<07:35,  1.85s/it]predicting train subjects:  14%|█▎        | 39/285 [01:13<07:40,  1.87s/it]predicting train subjects:  14%|█▍        | 40/285 [01:15<07:36,  1.86s/it]predicting train subjects:  14%|█▍        | 41/285 [01:17<07:40,  1.89s/it]predicting train subjects:  15%|█▍        | 42/285 [01:19<07:41,  1.90s/it]predicting train subjects:  15%|█▌        | 43/285 [01:20<07:31,  1.86s/it]predicting train subjects:  15%|█▌        | 44/285 [01:22<07:28,  1.86s/it]predicting train subjects:  16%|█▌        | 45/285 [01:24<07:28,  1.87s/it]predicting train subjects:  16%|█▌        | 46/285 [01:26<07:18,  1.83s/it]predicting train subjects:  16%|█▋        | 47/285 [01:27<06:58,  1.76s/it]predicting train subjects:  17%|█▋        | 48/285 [01:29<06:51,  1.74s/it]predicting train subjects:  17%|█▋        | 49/285 [01:31<06:53,  1.75s/it]predicting train subjects:  18%|█▊        | 50/285 [01:33<06:47,  1.74s/it]predicting train subjects:  18%|█▊        | 51/285 [01:34<06:39,  1.71s/it]predicting train subjects:  18%|█▊        | 52/285 [01:36<06:34,  1.69s/it]predicting train subjects:  19%|█▊        | 53/285 [01:38<06:29,  1.68s/it]predicting train subjects:  19%|█▉        | 54/285 [01:39<06:26,  1.67s/it]predicting train subjects:  19%|█▉        | 55/285 [01:41<06:29,  1.69s/it]predicting train subjects:  20%|█▉        | 56/285 [01:43<06:30,  1.70s/it]predicting train subjects:  20%|██        | 57/285 [01:44<06:33,  1.73s/it]predicting train subjects:  20%|██        | 58/285 [01:46<06:32,  1.73s/it]predicting train subjects:  21%|██        | 59/285 [01:48<06:21,  1.69s/it]predicting train subjects:  21%|██        | 60/285 [01:49<06:19,  1.69s/it]predicting train subjects:  21%|██▏       | 61/285 [01:51<06:12,  1.66s/it]predicting train subjects:  22%|██▏       | 62/285 [01:53<06:15,  1.69s/it]predicting train subjects:  22%|██▏       | 63/285 [01:54<06:10,  1.67s/it]predicting train subjects:  22%|██▏       | 64/285 [01:56<06:16,  1.70s/it]predicting train subjects:  23%|██▎       | 65/285 [01:58<06:34,  1.79s/it]predicting train subjects:  23%|██▎       | 66/285 [02:00<06:35,  1.81s/it]predicting train subjects:  24%|██▎       | 67/285 [02:02<06:23,  1.76s/it]predicting train subjects:  24%|██▍       | 68/285 [02:03<06:14,  1.73s/it]predicting train subjects:  24%|██▍       | 69/285 [02:05<06:14,  1.73s/it]predicting train subjects:  25%|██▍       | 70/285 [02:07<06:10,  1.72s/it]predicting train subjects:  25%|██▍       | 71/285 [02:08<06:04,  1.70s/it]predicting train subjects:  25%|██▌       | 72/285 [02:10<05:59,  1.69s/it]predicting train subjects:  26%|██▌       | 73/285 [02:12<05:55,  1.68s/it]predicting train subjects:  26%|██▌       | 74/285 [02:13<05:52,  1.67s/it]predicting train subjects:  26%|██▋       | 75/285 [02:15<05:53,  1.68s/it]predicting train subjects:  27%|██▋       | 76/285 [02:17<05:51,  1.68s/it]predicting train subjects:  27%|██▋       | 77/285 [02:18<05:46,  1.67s/it]predicting train subjects:  27%|██▋       | 78/285 [02:20<05:52,  1.70s/it]predicting train subjects:  28%|██▊       | 79/285 [02:22<05:59,  1.75s/it]predicting train subjects:  28%|██▊       | 80/285 [02:24<06:02,  1.77s/it]predicting train subjects:  28%|██▊       | 81/285 [02:26<06:01,  1.77s/it]predicting train subjects:  29%|██▉       | 82/285 [02:27<05:53,  1.74s/it]predicting train subjects:  29%|██▉       | 83/285 [02:29<05:48,  1.73s/it]predicting train subjects:  29%|██▉       | 84/285 [02:31<05:43,  1.71s/it]predicting train subjects:  30%|██▉       | 85/285 [02:33<05:48,  1.74s/it]predicting train subjects:  30%|███       | 86/285 [02:34<05:57,  1.80s/it]predicting train subjects:  31%|███       | 87/285 [02:36<05:59,  1.81s/it]predicting train subjects:  31%|███       | 88/285 [02:38<06:01,  1.84s/it]predicting train subjects:  31%|███       | 89/285 [02:40<06:03,  1.85s/it]predicting train subjects:  32%|███▏      | 90/285 [02:42<06:01,  1.85s/it]predicting train subjects:  32%|███▏      | 91/285 [02:44<06:03,  1.87s/it]predicting train subjects:  32%|███▏      | 92/285 [02:46<06:03,  1.88s/it]predicting train subjects:  33%|███▎      | 93/285 [02:48<06:00,  1.88s/it]predicting train subjects:  33%|███▎      | 94/285 [02:50<05:58,  1.88s/it]predicting train subjects:  33%|███▎      | 95/285 [02:51<05:55,  1.87s/it]predicting train subjects:  34%|███▎      | 96/285 [02:53<05:49,  1.85s/it]predicting train subjects:  34%|███▍      | 97/285 [02:55<05:50,  1.86s/it]predicting train subjects:  34%|███▍      | 98/285 [02:57<05:52,  1.89s/it]predicting train subjects:  35%|███▍      | 99/285 [02:59<05:49,  1.88s/it]predicting train subjects:  35%|███▌      | 100/285 [03:01<05:45,  1.87s/it]predicting train subjects:  35%|███▌      | 101/285 [03:03<05:39,  1.84s/it]predicting train subjects:  36%|███▌      | 102/285 [03:04<05:36,  1.84s/it]predicting train subjects:  36%|███▌      | 103/285 [03:06<05:29,  1.81s/it]predicting train subjects:  36%|███▋      | 104/285 [03:08<05:27,  1.81s/it]predicting train subjects:  37%|███▋      | 105/285 [03:10<05:24,  1.80s/it]predicting train subjects:  37%|███▋      | 106/285 [03:12<05:25,  1.82s/it]predicting train subjects:  38%|███▊      | 107/285 [03:13<05:26,  1.83s/it]predicting train subjects:  38%|███▊      | 108/285 [03:15<05:25,  1.84s/it]predicting train subjects:  38%|███▊      | 109/285 [03:17<05:19,  1.81s/it]predicting train subjects:  39%|███▊      | 110/285 [03:19<05:17,  1.81s/it]predicting train subjects:  39%|███▉      | 111/285 [03:21<05:14,  1.81s/it]predicting train subjects:  39%|███▉      | 112/285 [03:22<05:08,  1.78s/it]predicting train subjects:  40%|███▉      | 113/285 [03:24<05:05,  1.78s/it]predicting train subjects:  40%|████      | 114/285 [03:26<05:05,  1.79s/it]predicting train subjects:  40%|████      | 115/285 [03:28<05:08,  1.82s/it]predicting train subjects:  41%|████      | 116/285 [03:30<05:09,  1.83s/it]predicting train subjects:  41%|████      | 117/285 [03:31<05:06,  1.82s/it]predicting train subjects:  41%|████▏     | 118/285 [03:33<05:06,  1.84s/it]predicting train subjects:  42%|████▏     | 119/285 [03:35<05:06,  1.85s/it]predicting train subjects:  42%|████▏     | 120/285 [03:37<05:09,  1.88s/it]predicting train subjects:  42%|████▏     | 121/285 [03:39<05:01,  1.84s/it]predicting train subjects:  43%|████▎     | 122/285 [03:40<04:44,  1.75s/it]predicting train subjects:  43%|████▎     | 123/285 [03:42<04:28,  1.66s/it]predicting train subjects:  44%|████▎     | 124/285 [03:44<04:27,  1.66s/it]predicting train subjects:  44%|████▍     | 125/285 [03:45<04:27,  1.67s/it]predicting train subjects:  44%|████▍     | 126/285 [03:47<04:26,  1.67s/it]predicting train subjects:  45%|████▍     | 127/285 [03:49<04:23,  1.67s/it]predicting train subjects:  45%|████▍     | 128/285 [03:50<04:19,  1.65s/it]predicting train subjects:  45%|████▌     | 129/285 [03:52<04:18,  1.66s/it]predicting train subjects:  46%|████▌     | 130/285 [03:54<04:16,  1.65s/it]predicting train subjects:  46%|████▌     | 131/285 [03:55<04:14,  1.65s/it]predicting train subjects:  46%|████▋     | 132/285 [03:57<04:13,  1.66s/it]predicting train subjects:  47%|████▋     | 133/285 [03:58<04:10,  1.65s/it]predicting train subjects:  47%|████▋     | 134/285 [04:00<04:11,  1.67s/it]predicting train subjects:  47%|████▋     | 135/285 [04:02<04:11,  1.67s/it]predicting train subjects:  48%|████▊     | 136/285 [04:04<04:10,  1.68s/it]predicting train subjects:  48%|████▊     | 137/285 [04:05<04:07,  1.67s/it]predicting train subjects:  48%|████▊     | 138/285 [04:07<04:07,  1.68s/it]predicting train subjects:  49%|████▉     | 139/285 [04:09<04:02,  1.66s/it]predicting train subjects:  49%|████▉     | 140/285 [04:10<03:58,  1.64s/it]predicting train subjects:  49%|████▉     | 141/285 [04:12<03:58,  1.65s/it]predicting train subjects:  50%|████▉     | 142/285 [04:13<03:53,  1.63s/it]predicting train subjects:  50%|█████     | 143/285 [04:15<03:47,  1.60s/it]predicting train subjects:  51%|█████     | 144/285 [04:16<03:42,  1.57s/it]predicting train subjects:  51%|█████     | 145/285 [04:18<03:40,  1.57s/it]predicting train subjects:  51%|█████     | 146/285 [04:19<03:34,  1.54s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:21<03:30,  1.53s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:23<03:31,  1.55s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:24<03:30,  1.55s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:26<03:25,  1.52s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:27<03:23,  1.52s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:29<03:20,  1.51s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:30<03:16,  1.49s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:32<03:16,  1.50s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:33<03:13,  1.49s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:34<03:11,  1.48s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:36<03:11,  1.49s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:37<03:09,  1.49s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:39<03:07,  1.48s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:40<03:04,  1.48s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:42<03:01,  1.46s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:43<02:59,  1.46s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:45<03:01,  1.49s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:46<03:02,  1.51s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:48<03:00,  1.50s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:49<02:57,  1.49s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:51<02:54,  1.48s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:52<02:51,  1.47s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:54<02:50,  1.47s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:55<02:50,  1.48s/it]predicting train subjects:  60%|██████    | 171/285 [04:57<02:49,  1.48s/it]predicting train subjects:  60%|██████    | 172/285 [04:58<02:48,  1.49s/it]predicting train subjects:  61%|██████    | 173/285 [05:00<02:47,  1.50s/it]predicting train subjects:  61%|██████    | 174/285 [05:01<02:44,  1.48s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:03<02:42,  1.48s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:04<02:41,  1.48s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:06<02:39,  1.47s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:07<02:35,  1.45s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:08<02:33,  1.45s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:10<02:31,  1.44s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:11<02:30,  1.45s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:13<02:27,  1.43s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:14<02:24,  1.41s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:16<02:23,  1.42s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:17<02:21,  1.41s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:18<02:18,  1.40s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:20<02:18,  1.41s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:21<02:19,  1.43s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:23<02:17,  1.44s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:24<02:15,  1.43s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:25<02:14,  1.43s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:27<02:14,  1.45s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:28<02:11,  1.43s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:30<02:09,  1.43s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:31<02:08,  1.43s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:33<02:13,  1.50s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:35<02:16,  1.55s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:36<02:17,  1.58s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:38<02:18,  1.61s/it]predicting train subjects:  70%|███████   | 200/285 [05:40<02:18,  1.63s/it]predicting train subjects:  71%|███████   | 201/285 [05:41<02:19,  1.66s/it]predicting train subjects:  71%|███████   | 202/285 [05:43<02:19,  1.68s/it]predicting train subjects:  71%|███████   | 203/285 [05:45<02:17,  1.68s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:46<02:16,  1.69s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:48<02:14,  1.68s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:50<02:12,  1.68s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:51<02:10,  1.67s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:53<02:08,  1.67s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:55<02:07,  1.67s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:56<02:05,  1.67s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:58<02:04,  1.68s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:00<02:04,  1.70s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:02<02:01,  1.69s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:03<01:54,  1.61s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:04<01:50,  1.57s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:06<01:46,  1.55s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:07<01:43,  1.52s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:09<01:39,  1.49s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:10<01:38,  1.49s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:12<01:35,  1.47s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:13<01:32,  1.44s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:15<01:32,  1.47s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:16<01:30,  1.46s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:18<01:29,  1.48s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:19<01:28,  1.48s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:21<01:27,  1.49s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:22<01:26,  1.50s/it]predicting train subjects:  80%|████████  | 228/285 [06:24<01:24,  1.49s/it]predicting train subjects:  80%|████████  | 229/285 [06:25<01:22,  1.47s/it]predicting train subjects:  81%|████████  | 230/285 [06:26<01:20,  1.47s/it]predicting train subjects:  81%|████████  | 231/285 [06:28<01:18,  1.46s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:30<01:23,  1.57s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:32<01:26,  1.67s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:33<01:27,  1.72s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:35<01:28,  1.76s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:37<01:28,  1.80s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:39<01:26,  1.80s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:41<01:24,  1.80s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:43<01:23,  1.82s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:44<01:21,  1.82s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:46<01:19,  1.80s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:48<01:17,  1.80s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:50<01:17,  1.84s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:52<01:15,  1.85s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:54<01:14,  1.85s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:55<01:11,  1.84s/it]predicting train subjects:  87%|████████▋ | 247/285 [06:57<01:09,  1.83s/it]predicting train subjects:  87%|████████▋ | 248/285 [06:59<01:07,  1.83s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:01<01:06,  1.85s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:02<00:59,  1.71s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:04<00:54,  1.60s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:05<00:50,  1.53s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:07<00:48,  1.53s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:08<00:46,  1.49s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:09<00:43,  1.45s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:11<00:42,  1.45s/it]predicting train subjects:  90%|█████████ | 257/285 [07:12<00:40,  1.43s/it]predicting train subjects:  91%|█████████ | 258/285 [07:14<00:38,  1.41s/it]predicting train subjects:  91%|█████████ | 259/285 [07:15<00:36,  1.40s/it]predicting train subjects:  91%|█████████ | 260/285 [07:16<00:35,  1.41s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:18<00:33,  1.41s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:19<00:32,  1.40s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:21<00:30,  1.41s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:22<00:29,  1.41s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:23<00:27,  1.39s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:25<00:27,  1.43s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:26<00:25,  1.41s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:28<00:26,  1.55s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:30<00:26,  1.66s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:32<00:25,  1.72s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:34<00:25,  1.80s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:36<00:23,  1.82s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:38<00:21,  1.83s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:40<00:20,  1.84s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:41<00:18,  1.85s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:43<00:16,  1.87s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:45<00:15,  1.90s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:47<00:13,  1.93s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:49<00:11,  1.92s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:51<00:09,  1.91s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:53<00:07,  1.88s/it]predicting train subjects:  99%|█████████▉| 282/285 [07:55<00:05,  1.87s/it]predicting train subjects:  99%|█████████▉| 283/285 [07:57<00:03,  1.86s/it]predicting train subjects: 100%|█████████▉| 284/285 [07:58<00:01,  1.87s/it]predicting train subjects: 100%|██████████| 285/285 [08:00<00:00,  1.86s/it]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:24,  1.35s/it]Loading train:   1%|          | 2/285 [00:02<06:35,  1.40s/it]Loading train:   1%|          | 3/285 [00:04<06:26,  1.37s/it]Loading train:   1%|▏         | 4/285 [00:05<06:50,  1.46s/it]Loading train:   2%|▏         | 5/285 [00:06<06:20,  1.36s/it]Loading train:   2%|▏         | 6/285 [00:08<06:32,  1.41s/it]Loading train:   2%|▏         | 7/285 [00:10<06:53,  1.49s/it]Loading train:   3%|▎         | 8/285 [00:11<07:08,  1.55s/it]Loading train:   3%|▎         | 9/285 [00:13<06:56,  1.51s/it]Loading train:   4%|▎         | 10/285 [00:14<06:19,  1.38s/it]Loading train:   4%|▍         | 11/285 [00:15<05:48,  1.27s/it]Loading train:   4%|▍         | 12/285 [00:16<05:30,  1.21s/it]Loading train:   5%|▍         | 13/285 [00:17<05:05,  1.12s/it]Loading train:   5%|▍         | 14/285 [00:18<05:03,  1.12s/it]Loading train:   5%|▌         | 15/285 [00:19<04:50,  1.08s/it]Loading train:   6%|▌         | 16/285 [00:20<04:53,  1.09s/it]Loading train:   6%|▌         | 17/285 [00:21<04:38,  1.04s/it]Loading train:   6%|▋         | 18/285 [00:22<04:38,  1.04s/it]Loading train:   7%|▋         | 19/285 [00:23<04:41,  1.06s/it]Loading train:   7%|▋         | 20/285 [00:24<04:33,  1.03s/it]Loading train:   7%|▋         | 21/285 [00:25<04:41,  1.07s/it]Loading train:   8%|▊         | 22/285 [00:26<04:51,  1.11s/it]Loading train:   8%|▊         | 23/285 [00:28<04:46,  1.10s/it]Loading train:   8%|▊         | 24/285 [00:29<04:47,  1.10s/it]Loading train:   9%|▉         | 25/285 [00:30<04:47,  1.10s/it]Loading train:   9%|▉         | 26/285 [00:31<04:33,  1.06s/it]Loading train:   9%|▉         | 27/285 [00:32<04:39,  1.08s/it]Loading train:  10%|▉         | 28/285 [00:33<04:32,  1.06s/it]Loading train:  10%|█         | 29/285 [00:34<04:38,  1.09s/it]Loading train:  11%|█         | 30/285 [00:35<04:23,  1.03s/it]Loading train:  11%|█         | 31/285 [00:36<04:21,  1.03s/it]Loading train:  11%|█         | 32/285 [00:37<04:10,  1.01it/s]Loading train:  12%|█▏        | 33/285 [00:38<04:07,  1.02it/s]Loading train:  12%|█▏        | 34/285 [00:39<04:17,  1.03s/it]Loading train:  12%|█▏        | 35/285 [00:40<04:14,  1.02s/it]Loading train:  13%|█▎        | 36/285 [00:41<04:12,  1.02s/it]Loading train:  13%|█▎        | 37/285 [00:42<04:08,  1.00s/it]Loading train:  13%|█▎        | 38/285 [00:43<04:14,  1.03s/it]Loading train:  14%|█▎        | 39/285 [00:44<04:08,  1.01s/it]Loading train:  14%|█▍        | 40/285 [00:45<03:58,  1.03it/s]Loading train:  14%|█▍        | 41/285 [00:46<03:56,  1.03it/s]Loading train:  15%|█▍        | 42/285 [00:47<03:58,  1.02it/s]Loading train:  15%|█▌        | 43/285 [00:48<03:55,  1.03it/s]Loading train:  15%|█▌        | 44/285 [00:49<04:03,  1.01s/it]Loading train:  16%|█▌        | 45/285 [00:50<04:00,  1.00s/it]Loading train:  16%|█▌        | 46/285 [00:51<03:49,  1.04it/s]Loading train:  16%|█▋        | 47/285 [00:52<03:39,  1.08it/s]Loading train:  17%|█▋        | 48/285 [00:52<03:32,  1.12it/s]Loading train:  17%|█▋        | 49/285 [00:53<03:47,  1.04it/s]Loading train:  18%|█▊        | 50/285 [00:54<03:38,  1.07it/s]Loading train:  18%|█▊        | 51/285 [00:55<03:41,  1.06it/s]Loading train:  18%|█▊        | 52/285 [00:56<03:36,  1.08it/s]Loading train:  19%|█▊        | 53/285 [00:57<03:24,  1.14it/s]Loading train:  19%|█▉        | 54/285 [00:58<03:23,  1.14it/s]Loading train:  19%|█▉        | 55/285 [00:59<03:28,  1.10it/s]Loading train:  20%|█▉        | 56/285 [01:00<03:17,  1.16it/s]Loading train:  20%|██        | 57/285 [01:00<03:09,  1.20it/s]Loading train:  20%|██        | 58/285 [01:01<03:08,  1.21it/s]Loading train:  21%|██        | 59/285 [01:02<03:18,  1.14it/s]Loading train:  21%|██        | 60/285 [01:03<03:24,  1.10it/s]Loading train:  21%|██▏       | 61/285 [01:04<03:18,  1.13it/s]Loading train:  22%|██▏       | 62/285 [01:05<03:21,  1.11it/s]Loading train:  22%|██▏       | 63/285 [01:06<03:15,  1.14it/s]Loading train:  22%|██▏       | 64/285 [01:07<03:52,  1.05s/it]Loading train:  23%|██▎       | 65/285 [01:09<04:31,  1.23s/it]Loading train:  23%|██▎       | 66/285 [01:10<04:40,  1.28s/it]Loading train:  24%|██▎       | 67/285 [01:11<04:12,  1.16s/it]Loading train:  24%|██▍       | 68/285 [01:12<03:53,  1.08s/it]Loading train:  24%|██▍       | 69/285 [01:13<03:33,  1.01it/s]Loading train:  25%|██▍       | 70/285 [01:14<03:18,  1.08it/s]Loading train:  25%|██▍       | 71/285 [01:14<03:15,  1.09it/s]Loading train:  25%|██▌       | 72/285 [01:15<03:04,  1.15it/s]Loading train:  26%|██▌       | 73/285 [01:16<02:59,  1.18it/s]Loading train:  26%|██▌       | 74/285 [01:17<03:01,  1.17it/s]Loading train:  26%|██▋       | 75/285 [01:18<02:56,  1.19it/s]Loading train:  27%|██▋       | 76/285 [01:19<03:00,  1.16it/s]Loading train:  27%|██▋       | 77/285 [01:20<03:06,  1.11it/s]Loading train:  27%|██▋       | 78/285 [01:20<02:58,  1.16it/s]Loading train:  28%|██▊       | 79/285 [01:21<03:01,  1.13it/s]Loading train:  28%|██▊       | 80/285 [01:22<02:57,  1.16it/s]Loading train:  28%|██▊       | 81/285 [01:23<02:51,  1.19it/s]Loading train:  29%|██▉       | 82/285 [01:24<02:57,  1.15it/s]Loading train:  29%|██▉       | 83/285 [01:25<02:55,  1.15it/s]Loading train:  29%|██▉       | 84/285 [01:26<02:51,  1.17it/s]Loading train:  30%|██▉       | 85/285 [01:27<03:11,  1.04it/s]Loading train:  30%|███       | 86/285 [01:28<03:15,  1.02it/s]Loading train:  31%|███       | 87/285 [01:29<03:13,  1.02it/s]Loading train:  31%|███       | 88/285 [01:30<03:25,  1.05s/it]Loading train:  31%|███       | 89/285 [01:31<03:21,  1.03s/it]Loading train:  32%|███▏      | 90/285 [01:32<03:17,  1.01s/it]Loading train:  32%|███▏      | 91/285 [01:33<03:13,  1.00it/s]Loading train:  32%|███▏      | 92/285 [01:34<03:24,  1.06s/it]Loading train:  33%|███▎      | 93/285 [01:35<03:24,  1.07s/it]Loading train:  33%|███▎      | 94/285 [01:36<03:22,  1.06s/it]Loading train:  33%|███▎      | 95/285 [01:37<03:16,  1.03s/it]Loading train:  34%|███▎      | 96/285 [01:39<03:32,  1.13s/it]Loading train:  34%|███▍      | 97/285 [01:40<03:27,  1.10s/it]Loading train:  34%|███▍      | 98/285 [01:41<03:24,  1.09s/it]Loading train:  35%|███▍      | 99/285 [01:42<03:14,  1.05s/it]Loading train:  35%|███▌      | 100/285 [01:43<03:12,  1.04s/it]Loading train:  35%|███▌      | 101/285 [01:44<03:06,  1.01s/it]Loading train:  36%|███▌      | 102/285 [01:45<03:10,  1.04s/it]Loading train:  36%|███▌      | 103/285 [01:46<03:00,  1.01it/s]Loading train:  36%|███▋      | 104/285 [01:46<02:53,  1.04it/s]Loading train:  37%|███▋      | 105/285 [01:47<02:58,  1.01it/s]Loading train:  37%|███▋      | 106/285 [01:48<02:51,  1.04it/s]Loading train:  38%|███▊      | 107/285 [01:49<02:49,  1.05it/s]Loading train:  38%|███▊      | 108/285 [01:50<02:46,  1.06it/s]Loading train:  38%|███▊      | 109/285 [01:51<02:41,  1.09it/s]Loading train:  39%|███▊      | 110/285 [01:52<02:46,  1.05it/s]Loading train:  39%|███▉      | 111/285 [01:53<02:42,  1.07it/s]Loading train:  39%|███▉      | 112/285 [01:54<02:39,  1.08it/s]Loading train:  40%|███▉      | 113/285 [01:55<02:37,  1.09it/s]Loading train:  40%|████      | 114/285 [01:56<02:35,  1.10it/s]Loading train:  40%|████      | 115/285 [01:57<02:43,  1.04it/s]Loading train:  41%|████      | 116/285 [01:58<02:37,  1.07it/s]Loading train:  41%|████      | 117/285 [01:59<02:36,  1.07it/s]Loading train:  41%|████▏     | 118/285 [01:59<02:34,  1.08it/s]Loading train:  42%|████▏     | 119/285 [02:00<02:37,  1.05it/s]Loading train:  42%|████▏     | 120/285 [02:01<02:36,  1.06it/s]Loading train:  42%|████▏     | 121/285 [02:03<02:58,  1.09s/it]Loading train:  43%|████▎     | 122/285 [02:04<03:04,  1.13s/it]Loading train:  43%|████▎     | 123/285 [02:05<03:10,  1.17s/it]Loading train:  44%|████▎     | 124/285 [02:06<03:02,  1.13s/it]Loading train:  44%|████▍     | 125/285 [02:07<02:48,  1.05s/it]Loading train:  44%|████▍     | 126/285 [02:08<02:43,  1.03s/it]Loading train:  45%|████▍     | 127/285 [02:09<02:32,  1.03it/s]Loading train:  45%|████▍     | 128/285 [02:10<02:34,  1.02it/s]Loading train:  45%|████▌     | 129/285 [02:11<02:23,  1.09it/s]Loading train:  46%|████▌     | 130/285 [02:12<02:25,  1.06it/s]Loading train:  46%|████▌     | 131/285 [02:13<02:15,  1.13it/s]Loading train:  46%|████▋     | 132/285 [02:13<02:11,  1.16it/s]Loading train:  47%|████▋     | 133/285 [02:14<02:05,  1.21it/s]Loading train:  47%|████▋     | 134/285 [02:15<01:59,  1.26it/s]Loading train:  47%|████▋     | 135/285 [02:16<02:02,  1.23it/s]Loading train:  48%|████▊     | 136/285 [02:17<02:00,  1.24it/s]Loading train:  48%|████▊     | 137/285 [02:17<01:56,  1.27it/s]Loading train:  48%|████▊     | 138/285 [02:18<02:07,  1.16it/s]Loading train:  49%|████▉     | 139/285 [02:19<02:06,  1.16it/s]Loading train:  49%|████▉     | 140/285 [02:20<02:12,  1.09it/s]Loading train:  49%|████▉     | 141/285 [02:21<02:08,  1.12it/s]Loading train:  50%|████▉     | 142/285 [02:22<02:09,  1.11it/s]Loading train:  50%|█████     | 143/285 [02:23<02:05,  1.13it/s]Loading train:  51%|█████     | 144/285 [02:24<02:03,  1.14it/s]Loading train:  51%|█████     | 145/285 [02:24<01:59,  1.17it/s]Loading train:  51%|█████     | 146/285 [02:25<01:51,  1.24it/s]Loading train:  52%|█████▏    | 147/285 [02:26<01:56,  1.19it/s]Loading train:  52%|█████▏    | 148/285 [02:27<01:50,  1.24it/s]Loading train:  52%|█████▏    | 149/285 [02:28<01:48,  1.26it/s]Loading train:  53%|█████▎    | 150/285 [02:28<01:43,  1.30it/s]Loading train:  53%|█████▎    | 151/285 [02:29<01:42,  1.31it/s]Loading train:  53%|█████▎    | 152/285 [02:30<01:51,  1.19it/s]Loading train:  54%|█████▎    | 153/285 [02:31<01:46,  1.23it/s]Loading train:  54%|█████▍    | 154/285 [02:32<01:53,  1.16it/s]Loading train:  54%|█████▍    | 155/285 [02:32<01:46,  1.22it/s]Loading train:  55%|█████▍    | 156/285 [02:33<01:43,  1.24it/s]Loading train:  55%|█████▌    | 157/285 [02:34<01:44,  1.22it/s]Loading train:  55%|█████▌    | 158/285 [02:35<01:38,  1.28it/s]Loading train:  56%|█████▌    | 159/285 [02:36<01:43,  1.21it/s]Loading train:  56%|█████▌    | 160/285 [02:37<01:43,  1.21it/s]Loading train:  56%|█████▋    | 161/285 [02:37<01:44,  1.18it/s]Loading train:  57%|█████▋    | 162/285 [02:38<01:39,  1.23it/s]Loading train:  57%|█████▋    | 163/285 [02:39<01:46,  1.15it/s]Loading train:  58%|█████▊    | 164/285 [02:40<01:42,  1.18it/s]Loading train:  58%|█████▊    | 165/285 [02:41<01:46,  1.12it/s]Loading train:  58%|█████▊    | 166/285 [02:42<01:40,  1.19it/s]Loading train:  59%|█████▊    | 167/285 [02:43<01:39,  1.19it/s]Loading train:  59%|█████▉    | 168/285 [02:43<01:32,  1.27it/s]Loading train:  59%|█████▉    | 169/285 [02:44<01:37,  1.19it/s]Loading train:  60%|█████▉    | 170/285 [02:45<01:35,  1.21it/s]Loading train:  60%|██████    | 171/285 [02:46<01:39,  1.15it/s]Loading train:  60%|██████    | 172/285 [02:47<01:44,  1.08it/s]Loading train:  61%|██████    | 173/285 [02:48<01:38,  1.13it/s]Loading train:  61%|██████    | 174/285 [02:49<01:36,  1.15it/s]Loading train:  61%|██████▏   | 175/285 [02:49<01:33,  1.18it/s]Loading train:  62%|██████▏   | 176/285 [02:50<01:31,  1.19it/s]Loading train:  62%|██████▏   | 177/285 [02:51<01:28,  1.22it/s]Loading train:  62%|██████▏   | 178/285 [02:52<01:28,  1.22it/s]Loading train:  63%|██████▎   | 179/285 [02:53<01:24,  1.25it/s]Loading train:  63%|██████▎   | 180/285 [02:53<01:21,  1.28it/s]Loading train:  64%|██████▎   | 181/285 [02:54<01:21,  1.28it/s]Loading train:  64%|██████▍   | 182/285 [02:55<01:20,  1.28it/s]Loading train:  64%|██████▍   | 183/285 [02:56<01:18,  1.30it/s]Loading train:  65%|██████▍   | 184/285 [02:56<01:18,  1.28it/s]Loading train:  65%|██████▍   | 185/285 [02:57<01:14,  1.34it/s]Loading train:  65%|██████▌   | 186/285 [02:58<01:13,  1.35it/s]Loading train:  66%|██████▌   | 187/285 [02:59<01:19,  1.23it/s]Loading train:  66%|██████▌   | 188/285 [02:59<01:15,  1.29it/s]Loading train:  66%|██████▋   | 189/285 [03:00<01:11,  1.34it/s]Loading train:  67%|██████▋   | 190/285 [03:01<01:10,  1.35it/s]Loading train:  67%|██████▋   | 191/285 [03:02<01:06,  1.40it/s]Loading train:  67%|██████▋   | 192/285 [03:02<01:06,  1.39it/s]Loading train:  68%|██████▊   | 193/285 [03:03<01:07,  1.37it/s]Loading train:  68%|██████▊   | 194/285 [03:04<01:08,  1.34it/s]Loading train:  68%|██████▊   | 195/285 [03:05<01:08,  1.32it/s]Loading train:  69%|██████▉   | 196/285 [03:05<01:08,  1.29it/s]Loading train:  69%|██████▉   | 197/285 [03:06<01:13,  1.20it/s]Loading train:  69%|██████▉   | 198/285 [03:07<01:12,  1.20it/s]Loading train:  70%|██████▉   | 199/285 [03:08<01:14,  1.15it/s]Loading train:  70%|███████   | 200/285 [03:09<01:10,  1.20it/s]Loading train:  71%|███████   | 201/285 [03:10<01:10,  1.19it/s]Loading train:  71%|███████   | 202/285 [03:11<01:09,  1.20it/s]Loading train:  71%|███████   | 203/285 [03:11<01:07,  1.21it/s]Loading train:  72%|███████▏  | 204/285 [03:12<01:11,  1.13it/s]Loading train:  72%|███████▏  | 205/285 [03:13<01:12,  1.11it/s]Loading train:  72%|███████▏  | 206/285 [03:14<01:14,  1.07it/s]Loading train:  73%|███████▎  | 207/285 [03:15<01:12,  1.07it/s]Loading train:  73%|███████▎  | 208/285 [03:16<01:17,  1.00s/it]Loading train:  73%|███████▎  | 209/285 [03:17<01:13,  1.03it/s]Loading train:  74%|███████▎  | 210/285 [03:18<01:13,  1.02it/s]Loading train:  74%|███████▍  | 211/285 [03:19<01:09,  1.07it/s]Loading train:  74%|███████▍  | 212/285 [03:20<01:11,  1.02it/s]Loading train:  75%|███████▍  | 213/285 [03:21<01:07,  1.06it/s]Loading train:  75%|███████▌  | 214/285 [03:22<01:07,  1.05it/s]Loading train:  75%|███████▌  | 215/285 [03:23<01:02,  1.11it/s]Loading train:  76%|███████▌  | 216/285 [03:24<01:00,  1.14it/s]Loading train:  76%|███████▌  | 217/285 [03:25<00:58,  1.17it/s]Loading train:  76%|███████▋  | 218/285 [03:25<00:56,  1.18it/s]Loading train:  77%|███████▋  | 219/285 [03:26<00:56,  1.16it/s]Loading train:  77%|███████▋  | 220/285 [03:27<00:54,  1.20it/s]Loading train:  78%|███████▊  | 221/285 [03:28<00:53,  1.19it/s]Loading train:  78%|███████▊  | 222/285 [03:29<00:51,  1.23it/s]Loading train:  78%|███████▊  | 223/285 [03:29<00:48,  1.27it/s]Loading train:  79%|███████▊  | 224/285 [03:30<00:50,  1.22it/s]Loading train:  79%|███████▉  | 225/285 [03:31<00:46,  1.28it/s]Loading train:  79%|███████▉  | 226/285 [03:32<00:46,  1.28it/s]Loading train:  80%|███████▉  | 227/285 [03:32<00:44,  1.30it/s]Loading train:  80%|████████  | 228/285 [03:33<00:42,  1.35it/s]Loading train:  80%|████████  | 229/285 [03:34<00:43,  1.29it/s]Loading train:  81%|████████  | 230/285 [03:35<00:41,  1.33it/s]Loading train:  81%|████████  | 231/285 [03:36<00:42,  1.28it/s]Loading train:  81%|████████▏ | 232/285 [03:37<00:45,  1.15it/s]Loading train:  82%|████████▏ | 233/285 [03:38<00:47,  1.10it/s]Loading train:  82%|████████▏ | 234/285 [03:39<00:47,  1.07it/s]Loading train:  82%|████████▏ | 235/285 [03:40<00:47,  1.05it/s]Loading train:  83%|████████▎ | 236/285 [03:41<00:50,  1.04s/it]Loading train:  83%|████████▎ | 237/285 [03:42<00:48,  1.00s/it]Loading train:  84%|████████▎ | 238/285 [03:43<00:46,  1.01it/s]Loading train:  84%|████████▍ | 239/285 [03:44<00:44,  1.04it/s]Loading train:  84%|████████▍ | 240/285 [03:45<00:44,  1.00it/s]Loading train:  85%|████████▍ | 241/285 [03:46<00:42,  1.02it/s]Loading train:  85%|████████▍ | 242/285 [03:47<00:43,  1.02s/it]Loading train:  85%|████████▌ | 243/285 [03:48<00:44,  1.06s/it]Loading train:  86%|████████▌ | 244/285 [03:49<00:42,  1.04s/it]Loading train:  86%|████████▌ | 245/285 [03:50<00:41,  1.05s/it]Loading train:  86%|████████▋ | 246/285 [03:51<00:40,  1.03s/it]Loading train:  87%|████████▋ | 247/285 [03:52<00:39,  1.04s/it]Loading train:  87%|████████▋ | 248/285 [03:53<00:36,  1.01it/s]Loading train:  87%|████████▋ | 249/285 [03:54<00:35,  1.02it/s]Loading train:  88%|████████▊ | 250/285 [03:55<00:32,  1.09it/s]Loading train:  88%|████████▊ | 251/285 [03:55<00:29,  1.15it/s]Loading train:  88%|████████▊ | 252/285 [03:56<00:28,  1.17it/s]Loading train:  89%|████████▉ | 253/285 [03:57<00:26,  1.20it/s]Loading train:  89%|████████▉ | 254/285 [03:58<00:24,  1.24it/s]Loading train:  89%|████████▉ | 255/285 [03:59<00:25,  1.17it/s]Loading train:  90%|████████▉ | 256/285 [03:59<00:23,  1.25it/s]Loading train:  90%|█████████ | 257/285 [04:00<00:21,  1.33it/s]Loading train:  91%|█████████ | 258/285 [04:01<00:22,  1.19it/s]Loading train:  91%|█████████ | 259/285 [04:02<00:20,  1.24it/s]Loading train:  91%|█████████ | 260/285 [04:03<00:20,  1.20it/s]Loading train:  92%|█████████▏| 261/285 [04:03<00:19,  1.23it/s]Loading train:  92%|█████████▏| 262/285 [04:04<00:17,  1.30it/s]Loading train:  92%|█████████▏| 263/285 [04:05<00:16,  1.35it/s]Loading train:  93%|█████████▎| 264/285 [04:06<00:16,  1.27it/s]Loading train:  93%|█████████▎| 265/285 [04:06<00:15,  1.30it/s]Loading train:  93%|█████████▎| 266/285 [04:07<00:15,  1.25it/s]Loading train:  94%|█████████▎| 267/285 [04:08<00:14,  1.27it/s]Loading train:  94%|█████████▍| 268/285 [04:09<00:14,  1.16it/s]Loading train:  94%|█████████▍| 269/285 [04:10<00:15,  1.06it/s]Loading train:  95%|█████████▍| 270/285 [04:11<00:14,  1.04it/s]Loading train:  95%|█████████▌| 271/285 [04:12<00:14,  1.01s/it]Loading train:  95%|█████████▌| 272/285 [04:13<00:13,  1.02s/it]Loading train:  96%|█████████▌| 273/285 [04:14<00:11,  1.01it/s]Loading train:  96%|█████████▌| 274/285 [04:15<00:11,  1.05s/it]Loading train:  96%|█████████▋| 275/285 [04:16<00:10,  1.04s/it]Loading train:  97%|█████████▋| 276/285 [04:18<00:09,  1.06s/it]Loading train:  97%|█████████▋| 277/285 [04:19<00:08,  1.05s/it]Loading train:  98%|█████████▊| 278/285 [04:20<00:07,  1.11s/it]Loading train:  98%|█████████▊| 279/285 [04:21<00:06,  1.09s/it]Loading train:  98%|█████████▊| 280/285 [04:22<00:05,  1.05s/it]Loading train:  99%|█████████▊| 281/285 [04:23<00:04,  1.06s/it]Loading train:  99%|█████████▉| 282/285 [04:24<00:03,  1.02s/it]Loading train:  99%|█████████▉| 283/285 [04:25<00:02,  1.04s/it]Loading train: 100%|█████████▉| 284/285 [04:26<00:01,  1.01s/it]Loading train: 100%|██████████| 285/285 [04:27<00:00,  1.02s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   8%|▊         | 23/285 [00:00<00:01, 228.55it/s]concatenating: train:  14%|█▍        | 41/285 [00:00<00:01, 210.81it/s]concatenating: train:  26%|██▌       | 74/285 [00:00<00:00, 235.46it/s]concatenating: train:  33%|███▎      | 93/285 [00:00<00:00, 217.96it/s]concatenating: train:  42%|████▏     | 119/285 [00:00<00:00, 220.61it/s]concatenating: train:  48%|████▊     | 138/285 [00:00<00:01, 118.74it/s]concatenating: train:  54%|█████▎    | 153/285 [00:00<00:01, 116.09it/s]concatenating: train:  61%|██████    | 173/285 [00:01<00:00, 132.25it/s]concatenating: train:  67%|██████▋   | 192/285 [00:01<00:00, 145.00it/s]concatenating: train:  80%|███████▉  | 227/285 [00:01<00:00, 175.63it/s]concatenating: train:  88%|████████▊ | 251/285 [00:01<00:00, 189.89it/s]concatenating: train:  96%|█████████▋| 275/285 [00:01<00:00, 195.02it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 184.08it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.40s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.38s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.28s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 679.61it/s]2019-07-06 19:21:24.803198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 19:21:24.803294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 19:21:24.803309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 19:21:24.803318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 19:21:24.803766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:10,  3.89it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:08,  4.55it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.25it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:05,  5.44it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:06,  4.77it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.40it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  4.96it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:03,  6.28it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.35it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.80it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:04,  4.83it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:02,  5.86it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.29it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  5.30it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:02,  5.77it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  4.94it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  5.90it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  6.28it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:05<00:00,  5.01it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  5.53it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  4.67it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  7.14it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 4  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 80, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 80, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 80, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 80, 20)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 80, 20)   3620        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 80, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 80, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 80, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 80, 20)   3620        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 80, 20)   80          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 80, 20)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 80, 20)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 80, 20)   3620        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 80, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 80, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 80, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 80, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 80, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 40, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 40, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 40, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 40, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 40, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 40, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 40, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 20, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 20, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 20, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 20, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 20, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 20, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 20, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 20, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 40, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 40, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 40, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 40, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 40, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 40, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 20)   3620        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 20)   80          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 20)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 20)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 13)   273         dropout_9[0][0]                  
==================================================================================================
Total params: 198,913
Trainable params: 55,273
Non-trainable params: 143,640
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 20s - loss: 240.1002 - acc: 0.3272 - mDice: 0.0165 - val_loss: 137.8914 - val_acc: 0.8303 - val_mDice: 0.0133

Epoch 00001: val_mDice improved from -inf to 0.01334, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 11s - loss: 123.9322 - acc: 0.7096 - mDice: 0.0141 - val_loss: 77.3197 - val_acc: 0.7069 - val_mDice: 0.0121

Epoch 00002: val_mDice did not improve from 0.01334
Epoch 3/300
 - 11s - loss: 66.8343 - acc: 0.7998 - mDice: 0.0130 - val_loss: 47.1417 - val_acc: 0.9047 - val_mDice: 0.0120

Epoch 00003: val_mDice did not improve from 0.01334
Epoch 4/300
 - 11s - loss: 40.5964 - acc: 0.8690 - mDice: 0.0122 - val_loss: 22.7057 - val_acc: 0.9047 - val_mDice: 0.0068

Epoch 00004: val_mDice did not improve from 0.01334
Epoch 5/300
 - 11s - loss: 27.7836 - acc: 0.8691 - mDice: 0.0125 - val_loss: 12.7847 - val_acc: 0.9047 - val_mDice: 0.0091

Epoch 00005: val_mDice did not improve from 0.01334
Epoch 6/300
 - 11s - loss: 18.6449 - acc: 0.8692 - mDice: 0.0137 - val_loss: 9.1561 - val_acc: 0.9047 - val_mDice: 0.0102

Epoch 00006: val_mDice did not improve from 0.01334
Epoch 7/300
 - 11s - loss: 14.5824 - acc: 0.8692 - mDice: 0.0153 - val_loss: 8.3770 - val_acc: 0.9047 - val_mDice: 0.0112

Epoch 00007: val_mDice did not improve from 0.01334
Epoch 8/300
 - 11s - loss: 12.2473 - acc: 0.8692 - mDice: 0.0176 - val_loss: 7.0848 - val_acc: 0.9047 - val_mDice: 0.0124

Epoch 00008: val_mDice did not improve from 0.01334
Epoch 9/300
 - 11s - loss: 10.1548 - acc: 0.8692 - mDice: 0.0204 - val_loss: 6.6664 - val_acc: 0.9047 - val_mDice: 0.0208

Epoch 00009: val_mDice improved from 0.01334 to 0.02077, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 11s - loss: 9.0326 - acc: 0.8692 - mDice: 0.0228 - val_loss: 6.4020 - val_acc: 0.9047 - val_mDice: 0.0271

Epoch 00010: val_mDice improved from 0.02077 to 0.02708, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 11s - loss: 8.3590 - acc: 0.8692 - mDice: 0.0252 - val_loss: 5.8915 - val_acc: 0.9047 - val_mDice: 0.0264

Epoch 00011: val_mDice did not improve from 0.02708
Epoch 12/300
 - 11s - loss: 7.8792 - acc: 0.8692 - mDice: 0.0275 - val_loss: 5.7450 - val_acc: 0.9047 - val_mDice: 0.0379

Epoch 00012: val_mDice improved from 0.02708 to 0.03793, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 11s - loss: 7.4828 - acc: 0.8692 - mDice: 0.0301 - val_loss: 5.5320 - val_acc: 0.9047 - val_mDice: 0.0309

Epoch 00013: val_mDice did not improve from 0.03793
Epoch 14/300
 - 11s - loss: 7.1638 - acc: 0.8691 - mDice: 0.0329 - val_loss: 5.5623 - val_acc: 0.9047 - val_mDice: 0.0303

Epoch 00014: val_mDice did not improve from 0.03793
Epoch 15/300
 - 11s - loss: 6.8975 - acc: 0.8690 - mDice: 0.0356 - val_loss: 5.2960 - val_acc: 0.9047 - val_mDice: 0.0378

Epoch 00015: val_mDice did not improve from 0.03793
Epoch 16/300
 - 11s - loss: 6.6492 - acc: 0.8687 - mDice: 0.0396 - val_loss: 5.3338 - val_acc: 0.9047 - val_mDice: 0.0390

Epoch 00016: val_mDice improved from 0.03793 to 0.03902, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 11s - loss: 6.4316 - acc: 0.8684 - mDice: 0.0437 - val_loss: 4.9927 - val_acc: 0.9047 - val_mDice: 0.0549

Epoch 00017: val_mDice improved from 0.03902 to 0.05493, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 11s - loss: 6.2512 - acc: 0.8685 - mDice: 0.0476 - val_loss: 5.0127 - val_acc: 0.9047 - val_mDice: 0.0540

Epoch 00018: val_mDice did not improve from 0.05493
Epoch 19/300
 - 11s - loss: 6.0832 - acc: 0.8688 - mDice: 0.0509 - val_loss: 4.8102 - val_acc: 0.9047 - val_mDice: 0.0650

Epoch 00019: val_mDice improved from 0.05493 to 0.06497, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 11s - loss: 5.9246 - acc: 0.8689 - mDice: 0.0545 - val_loss: 4.6875 - val_acc: 0.9047 - val_mDice: 0.0695

Epoch 00020: val_mDice improved from 0.06497 to 0.06952, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 11s - loss: 5.7976 - acc: 0.8692 - mDice: 0.0575 - val_loss: 4.6250 - val_acc: 0.9047 - val_mDice: 0.0737

Epoch 00021: val_mDice improved from 0.06952 to 0.07372, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 11s - loss: 5.6677 - acc: 0.8691 - mDice: 0.0606 - val_loss: 4.6260 - val_acc: 0.9047 - val_mDice: 0.0736

Epoch 00022: val_mDice did not improve from 0.07372
Epoch 23/300
 - 11s - loss: 5.5011 - acc: 0.8692 - mDice: 0.0646 - val_loss: 4.4929 - val_acc: 0.9047 - val_mDice: 0.0819

Epoch 00023: val_mDice improved from 0.07372 to 0.08190, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 11s - loss: 5.3695 - acc: 0.8690 - mDice: 0.0684 - val_loss: 4.4903 - val_acc: 0.9047 - val_mDice: 0.0841

Epoch 00024: val_mDice improved from 0.08190 to 0.08407, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 11s - loss: 5.2640 - acc: 0.8689 - mDice: 0.0724 - val_loss: 4.8005 - val_acc: 0.9047 - val_mDice: 0.0767

Epoch 00025: val_mDice did not improve from 0.08407
Epoch 26/300
 - 11s - loss: 5.1613 - acc: 0.8690 - mDice: 0.0766 - val_loss: 4.3586 - val_acc: 0.9047 - val_mDice: 0.0915

Epoch 00026: val_mDice improved from 0.08407 to 0.09151, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 11s - loss: 5.0569 - acc: 0.8694 - mDice: 0.0823 - val_loss: 4.2572 - val_acc: 0.9047 - val_mDice: 0.0990

Epoch 00027: val_mDice improved from 0.09151 to 0.09904, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 11s - loss: 4.9646 - acc: 0.8704 - mDice: 0.0881 - val_loss: 4.2051 - val_acc: 0.9051 - val_mDice: 0.1054

Epoch 00028: val_mDice improved from 0.09904 to 0.10540, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 11s - loss: 4.8793 - acc: 0.8720 - mDice: 0.0947 - val_loss: 4.4184 - val_acc: 0.9049 - val_mDice: 0.1024

Epoch 00029: val_mDice did not improve from 0.10540
Epoch 30/300
 - 12s - loss: 4.7805 - acc: 0.8741 - mDice: 0.1024 - val_loss: 4.3025 - val_acc: 0.9066 - val_mDice: 0.1164

Epoch 00030: val_mDice improved from 0.10540 to 0.11640, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 11s - loss: 4.6759 - acc: 0.8768 - mDice: 0.1113 - val_loss: 4.3029 - val_acc: 0.9084 - val_mDice: 0.1236

Epoch 00031: val_mDice improved from 0.11640 to 0.12362, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 11s - loss: 4.5755 - acc: 0.8790 - mDice: 0.1198 - val_loss: 4.6155 - val_acc: 0.9100 - val_mDice: 0.1243

Epoch 00032: val_mDice improved from 0.12362 to 0.12427, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 11s - loss: 4.4864 - acc: 0.8806 - mDice: 0.1278 - val_loss: 4.4283 - val_acc: 0.9113 - val_mDice: 0.1316

Epoch 00033: val_mDice improved from 0.12427 to 0.13157, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 11s - loss: 4.3948 - acc: 0.8816 - mDice: 0.1363 - val_loss: 4.0012 - val_acc: 0.9155 - val_mDice: 0.1533

Epoch 00034: val_mDice improved from 0.13157 to 0.15327, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 11s - loss: 4.2775 - acc: 0.8826 - mDice: 0.1476 - val_loss: 3.8734 - val_acc: 0.9136 - val_mDice: 0.1672

Epoch 00035: val_mDice improved from 0.15327 to 0.16718, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 11s - loss: 4.1635 - acc: 0.8835 - mDice: 0.1592 - val_loss: 3.8683 - val_acc: 0.9162 - val_mDice: 0.1785

Epoch 00036: val_mDice improved from 0.16718 to 0.17851, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 11s - loss: 4.0420 - acc: 0.8844 - mDice: 0.1705 - val_loss: 3.9354 - val_acc: 0.9156 - val_mDice: 0.1894

Epoch 00037: val_mDice improved from 0.17851 to 0.18945, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 12s - loss: 3.9364 - acc: 0.8861 - mDice: 0.1823 - val_loss: 3.8985 - val_acc: 0.9161 - val_mDice: 0.1986

Epoch 00038: val_mDice improved from 0.18945 to 0.19858, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 11s - loss: 3.8283 - acc: 0.8876 - mDice: 0.1937 - val_loss: 3.7584 - val_acc: 0.9166 - val_mDice: 0.2143

Epoch 00039: val_mDice improved from 0.19858 to 0.21427, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 11s - loss: 3.7150 - acc: 0.8896 - mDice: 0.2073 - val_loss: 3.7061 - val_acc: 0.9175 - val_mDice: 0.2225

Epoch 00040: val_mDice improved from 0.21427 to 0.22254, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 11s - loss: 3.6228 - acc: 0.8909 - mDice: 0.2183 - val_loss: 3.6045 - val_acc: 0.9212 - val_mDice: 0.2362

Epoch 00041: val_mDice improved from 0.22254 to 0.23622, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 11s - loss: 3.5233 - acc: 0.8924 - mDice: 0.2298 - val_loss: 3.3305 - val_acc: 0.9196 - val_mDice: 0.2533

Epoch 00042: val_mDice improved from 0.23622 to 0.25328, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 11s - loss: 3.4408 - acc: 0.8941 - mDice: 0.2405 - val_loss: 3.8446 - val_acc: 0.9226 - val_mDice: 0.2365

Epoch 00043: val_mDice did not improve from 0.25328
Epoch 44/300
 - 12s - loss: 3.3685 - acc: 0.8953 - mDice: 0.2490 - val_loss: 3.2857 - val_acc: 0.9230 - val_mDice: 0.2684

Epoch 00044: val_mDice improved from 0.25328 to 0.26844, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 11s - loss: 3.2892 - acc: 0.8971 - mDice: 0.2603 - val_loss: 3.1302 - val_acc: 0.9192 - val_mDice: 0.2801

Epoch 00045: val_mDice improved from 0.26844 to 0.28011, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 11s - loss: 3.2343 - acc: 0.8983 - mDice: 0.2679 - val_loss: 3.2368 - val_acc: 0.9093 - val_mDice: 0.2834

Epoch 00046: val_mDice improved from 0.28011 to 0.28337, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 11s - loss: 3.1709 - acc: 0.8993 - mDice: 0.2753 - val_loss: 3.4310 - val_acc: 0.9267 - val_mDice: 0.2773

Epoch 00047: val_mDice did not improve from 0.28337
Epoch 48/300
 - 11s - loss: 3.1055 - acc: 0.9006 - mDice: 0.2845 - val_loss: 3.4282 - val_acc: 0.9261 - val_mDice: 0.2853

Epoch 00048: val_mDice improved from 0.28337 to 0.28534, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 11s - loss: 3.0453 - acc: 0.9018 - mDice: 0.2921 - val_loss: 3.0715 - val_acc: 0.9169 - val_mDice: 0.3130

Epoch 00049: val_mDice improved from 0.28534 to 0.31303, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 11s - loss: 3.0026 - acc: 0.9028 - mDice: 0.2995 - val_loss: 3.2853 - val_acc: 0.9256 - val_mDice: 0.2956

Epoch 00050: val_mDice did not improve from 0.31303
Epoch 51/300
 - 12s - loss: 2.9677 - acc: 0.9031 - mDice: 0.3044 - val_loss: 3.4029 - val_acc: 0.9287 - val_mDice: 0.3055

Epoch 00051: val_mDice did not improve from 0.31303
Epoch 52/300
 - 12s - loss: 2.9196 - acc: 0.9040 - mDice: 0.3109 - val_loss: 3.1634 - val_acc: 0.9136 - val_mDice: 0.3047

Epoch 00052: val_mDice did not improve from 0.31303
Epoch 53/300
 - 11s - loss: 2.8720 - acc: 0.9053 - mDice: 0.3188 - val_loss: 3.1507 - val_acc: 0.9275 - val_mDice: 0.3207

Epoch 00053: val_mDice improved from 0.31303 to 0.32068, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 11s - loss: 2.8195 - acc: 0.9061 - mDice: 0.3268 - val_loss: 3.1374 - val_acc: 0.9215 - val_mDice: 0.3290

Epoch 00054: val_mDice improved from 0.32068 to 0.32900, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 11s - loss: 2.7853 - acc: 0.9068 - mDice: 0.3328 - val_loss: 2.9298 - val_acc: 0.9173 - val_mDice: 0.3382

Epoch 00055: val_mDice improved from 0.32900 to 0.33823, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 12s - loss: 2.7389 - acc: 0.9077 - mDice: 0.3408 - val_loss: 3.0166 - val_acc: 0.9256 - val_mDice: 0.3483

Epoch 00056: val_mDice improved from 0.33823 to 0.34833, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 57/300
 - 12s - loss: 2.6963 - acc: 0.9087 - mDice: 0.3479 - val_loss: 3.0530 - val_acc: 0.9186 - val_mDice: 0.3430

Epoch 00057: val_mDice did not improve from 0.34833
Epoch 58/300
 - 11s - loss: 2.6662 - acc: 0.9093 - mDice: 0.3537 - val_loss: 3.6841 - val_acc: 0.9301 - val_mDice: 0.3123

Epoch 00058: val_mDice did not improve from 0.34833
Epoch 59/300
 - 11s - loss: 2.6316 - acc: 0.9104 - mDice: 0.3606 - val_loss: 3.3296 - val_acc: 0.9315 - val_mDice: 0.3465

Epoch 00059: val_mDice did not improve from 0.34833
Epoch 60/300
 - 11s - loss: 2.5971 - acc: 0.9111 - mDice: 0.3670 - val_loss: 3.2472 - val_acc: 0.9339 - val_mDice: 0.3490

Epoch 00060: val_mDice improved from 0.34833 to 0.34901, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 61/300
 - 11s - loss: 2.5754 - acc: 0.9117 - mDice: 0.3707 - val_loss: 2.9223 - val_acc: 0.9283 - val_mDice: 0.3752

Epoch 00061: val_mDice improved from 0.34901 to 0.37516, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 62/300
 - 12s - loss: 2.5379 - acc: 0.9123 - mDice: 0.3775 - val_loss: 2.9171 - val_acc: 0.9314 - val_mDice: 0.3817

Epoch 00062: val_mDice improved from 0.37516 to 0.38174, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 12s - loss: 2.5231 - acc: 0.9125 - mDice: 0.3802 - val_loss: 2.9543 - val_acc: 0.9312 - val_mDice: 0.3739

Epoch 00063: val_mDice did not improve from 0.38174
Epoch 64/300
 - 11s - loss: 2.4946 - acc: 0.9133 - mDice: 0.3858 - val_loss: 2.9692 - val_acc: 0.9329 - val_mDice: 0.3776

Epoch 00064: val_mDice did not improve from 0.38174
Epoch 65/300
 - 11s - loss: 2.4747 - acc: 0.9135 - mDice: 0.3890 - val_loss: 2.8602 - val_acc: 0.9248 - val_mDice: 0.3784

Epoch 00065: val_mDice did not improve from 0.38174
Epoch 66/300
 - 11s - loss: 2.4572 - acc: 0.9138 - mDice: 0.3916 - val_loss: 3.0374 - val_acc: 0.9197 - val_mDice: 0.3649

Epoch 00066: val_mDice did not improve from 0.38174
Epoch 67/300
 - 11s - loss: 2.4320 - acc: 0.9143 - mDice: 0.3964 - val_loss: 2.9213 - val_acc: 0.9359 - val_mDice: 0.3841

Epoch 00067: val_mDice improved from 0.38174 to 0.38409, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 68/300
 - 11s - loss: 2.4128 - acc: 0.9149 - mDice: 0.4000 - val_loss: 2.7335 - val_acc: 0.9302 - val_mDice: 0.3965

Epoch 00068: val_mDice improved from 0.38409 to 0.39649, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 69/300
 - 11s - loss: 2.3956 - acc: 0.9154 - mDice: 0.4028 - val_loss: 2.9602 - val_acc: 0.9351 - val_mDice: 0.3912

Epoch 00069: val_mDice did not improve from 0.39649
Epoch 70/300
 - 12s - loss: 2.3772 - acc: 0.9157 - mDice: 0.4070 - val_loss: 2.9029 - val_acc: 0.9335 - val_mDice: 0.3939

Epoch 00070: val_mDice did not improve from 0.39649
Epoch 71/300
 - 11s - loss: 2.3583 - acc: 0.9162 - mDice: 0.4104 - val_loss: 2.7363 - val_acc: 0.9296 - val_mDice: 0.3950

Epoch 00071: val_mDice did not improve from 0.39649
Epoch 72/300
 - 11s - loss: 2.3394 - acc: 0.9165 - mDice: 0.4134 - val_loss: 2.8976 - val_acc: 0.9271 - val_mDice: 0.3970

Epoch 00072: val_mDice improved from 0.39649 to 0.39698, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 73/300
 - 11s - loss: 2.3228 - acc: 0.9172 - mDice: 0.4177 - val_loss: 2.9236 - val_acc: 0.9346 - val_mDice: 0.4020

Epoch 00073: val_mDice improved from 0.39698 to 0.40196, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 11s - loss: 2.3147 - acc: 0.9173 - mDice: 0.4187 - val_loss: 2.9023 - val_acc: 0.9203 - val_mDice: 0.3930

Epoch 00074: val_mDice did not improve from 0.40196
Epoch 75/300
 - 14s - loss: 2.2986 - acc: 0.9178 - mDice: 0.4228 - val_loss: 2.9320 - val_acc: 0.9365 - val_mDice: 0.3988

Epoch 00075: val_mDice did not improve from 0.40196
Epoch 76/300
 - 13s - loss: 2.2869 - acc: 0.9181 - mDice: 0.4248 - val_loss: 2.8710 - val_acc: 0.9274 - val_mDice: 0.3970

Epoch 00076: val_mDice did not improve from 0.40196
Epoch 77/300
 - 13s - loss: 2.2664 - acc: 0.9186 - mDice: 0.4291 - val_loss: 2.7937 - val_acc: 0.9361 - val_mDice: 0.4105

Epoch 00077: val_mDice improved from 0.40196 to 0.41055, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 78/300
 - 14s - loss: 2.2581 - acc: 0.9188 - mDice: 0.4311 - val_loss: 2.8641 - val_acc: 0.9379 - val_mDice: 0.4063

Epoch 00078: val_mDice did not improve from 0.41055
Epoch 79/300
 - 13s - loss: 2.2382 - acc: 0.9193 - mDice: 0.4347 - val_loss: 2.9448 - val_acc: 0.9318 - val_mDice: 0.4072

Epoch 00079: val_mDice did not improve from 0.41055
Epoch 80/300
 - 14s - loss: 2.2190 - acc: 0.9198 - mDice: 0.4380 - val_loss: 2.8575 - val_acc: 0.9305 - val_mDice: 0.4054

Epoch 00080: val_mDice did not improve from 0.41055
Epoch 81/300
 - 14s - loss: 2.2144 - acc: 0.9197 - mDice: 0.4395 - val_loss: 2.8832 - val_acc: 0.9354 - val_mDice: 0.4139

Epoch 00081: val_mDice improved from 0.41055 to 0.41392, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 82/300
 - 14s - loss: 2.2054 - acc: 0.9201 - mDice: 0.4417 - val_loss: 2.9798 - val_acc: 0.9353 - val_mDice: 0.4105

Epoch 00082: val_mDice did not improve from 0.41392
Epoch 83/300
 - 14s - loss: 2.1875 - acc: 0.9205 - mDice: 0.4455 - val_loss: 2.9031 - val_acc: 0.9343 - val_mDice: 0.4144

Epoch 00083: val_mDice improved from 0.41392 to 0.41440, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 84/300
 - 13s - loss: 2.1764 - acc: 0.9207 - mDice: 0.4478 - val_loss: 2.8555 - val_acc: 0.9390 - val_mDice: 0.4200

Epoch 00084: val_mDice improved from 0.41440 to 0.41995, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 85/300
 - 12s - loss: 2.1643 - acc: 0.9210 - mDice: 0.4502 - val_loss: 2.7946 - val_acc: 0.9326 - val_mDice: 0.4191

Epoch 00085: val_mDice did not improve from 0.41995
Epoch 86/300
 - 14s - loss: 2.1525 - acc: 0.9214 - mDice: 0.4523 - val_loss: 2.8768 - val_acc: 0.9350 - val_mDice: 0.4204

Epoch 00086: val_mDice improved from 0.41995 to 0.42042, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 87/300
 - 13s - loss: 2.1436 - acc: 0.9215 - mDice: 0.4547 - val_loss: 2.9522 - val_acc: 0.9396 - val_mDice: 0.4116

Epoch 00087: val_mDice did not improve from 0.42042
Epoch 88/300
 - 12s - loss: 2.1311 - acc: 0.9218 - mDice: 0.4569 - val_loss: 2.8705 - val_acc: 0.9391 - val_mDice: 0.4257

Epoch 00088: val_mDice improved from 0.42042 to 0.42569, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 89/300
 - 12s - loss: 2.1273 - acc: 0.9221 - mDice: 0.4587 - val_loss: 2.8156 - val_acc: 0.9368 - val_mDice: 0.4241

Epoch 00089: val_mDice did not improve from 0.42569
Epoch 90/300
 - 12s - loss: 2.1144 - acc: 0.9222 - mDice: 0.4602 - val_loss: 2.9367 - val_acc: 0.9376 - val_mDice: 0.4147

Epoch 00090: val_mDice did not improve from 0.42569
Epoch 91/300
 - 12s - loss: 2.1072 - acc: 0.9226 - mDice: 0.4625 - val_loss: 2.8612 - val_acc: 0.9384 - val_mDice: 0.4234

Epoch 00091: val_mDice did not improve from 0.42569
Epoch 92/300
 - 12s - loss: 2.0862 - acc: 0.9231 - mDice: 0.4661 - val_loss: 2.9409 - val_acc: 0.9316 - val_mDice: 0.4083

Epoch 00092: val_mDice did not improve from 0.42569
Epoch 93/300
 - 12s - loss: 2.0837 - acc: 0.9233 - mDice: 0.4677 - val_loss: 3.0047 - val_acc: 0.9368 - val_mDice: 0.4160

Epoch 00093: val_mDice did not improve from 0.42569
Epoch 94/300
 - 12s - loss: 2.0686 - acc: 0.9237 - mDice: 0.4710 - val_loss: 2.9047 - val_acc: 0.9398 - val_mDice: 0.4232

Epoch 00094: val_mDice did not improve from 0.42569
Epoch 95/300
 - 13s - loss: 2.0642 - acc: 0.9239 - mDice: 0.4717 - val_loss: 2.8205 - val_acc: 0.9395 - val_mDice: 0.4306

Epoch 00095: val_mDice improved from 0.42569 to 0.43056, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 96/300
 - 12s - loss: 2.0509 - acc: 0.9243 - mDice: 0.4742 - val_loss: 3.0563 - val_acc: 0.9358 - val_mDice: 0.4129

Epoch 00096: val_mDice did not improve from 0.43056
Epoch 97/300
 - 12s - loss: 2.0532 - acc: 0.9244 - mDice: 0.4747 - val_loss: 3.0585 - val_acc: 0.9354 - val_mDice: 0.4239

Epoch 00097: val_mDice did not improve from 0.43056
Epoch 98/300
 - 13s - loss: 2.0463 - acc: 0.9243 - mDice: 0.4762 - val_loss: 2.9710 - val_acc: 0.9374 - val_mDice: 0.4247

Epoch 00098: val_mDice did not improve from 0.43056
Epoch 99/300
 - 12s - loss: 2.0368 - acc: 0.9248 - mDice: 0.4782 - val_loss: 2.8584 - val_acc: 0.9362 - val_mDice: 0.4232

Epoch 00099: val_mDice did not improve from 0.43056
Epoch 100/300
 - 11s - loss: 2.0204 - acc: 0.9250 - mDice: 0.4812 - val_loss: 2.9117 - val_acc: 0.9289 - val_mDice: 0.4228

Epoch 00100: val_mDice did not improve from 0.43056
Epoch 101/300
 - 12s - loss: 2.0112 - acc: 0.9253 - mDice: 0.4835 - val_loss: 3.0596 - val_acc: 0.9391 - val_mDice: 0.4236

Epoch 00101: val_mDice did not improve from 0.43056
Epoch 102/300
 - 11s - loss: 2.0136 - acc: 0.9253 - mDice: 0.4831 - val_loss: 3.0339 - val_acc: 0.9395 - val_mDice: 0.4273

Epoch 00102: val_mDice did not improve from 0.43056
Epoch 103/300
 - 12s - loss: 1.9960 - acc: 0.9257 - mDice: 0.4866 - val_loss: 2.8419 - val_acc: 0.9367 - val_mDice: 0.4276

Epoch 00103: val_mDice did not improve from 0.43056
Epoch 104/300
 - 11s - loss: 1.9911 - acc: 0.9257 - mDice: 0.4879 - val_loss: 3.0123 - val_acc: 0.9352 - val_mDice: 0.4246

Epoch 00104: val_mDice did not improve from 0.43056
Epoch 105/300
 - 11s - loss: 1.9870 - acc: 0.9260 - mDice: 0.4889 - val_loss: 2.9917 - val_acc: 0.9356 - val_mDice: 0.4269

Epoch 00105: val_mDice did not improve from 0.43056
Epoch 106/300
 - 11s - loss: 1.9779 - acc: 0.9261 - mDice: 0.4906 - val_loss: 2.9593 - val_acc: 0.9373 - val_mDice: 0.4351

Epoch 00106: val_mDice improved from 0.43056 to 0.43513, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 107/300
 - 11s - loss: 1.9681 - acc: 0.9265 - mDice: 0.4929 - val_loss: 2.9211 - val_acc: 0.9373 - val_mDice: 0.4290

Epoch 00107: val_mDice did not improve from 0.43513
Epoch 108/300
 - 11s - loss: 1.9579 - acc: 0.9267 - mDice: 0.4953 - val_loss: 3.1221 - val_acc: 0.9346 - val_mDice: 0.4244

Epoch 00108: val_mDice did not improve from 0.43513
Epoch 109/300
 - 11s - loss: 1.9552 - acc: 0.9268 - mDice: 0.4961 - val_loss: 3.1873 - val_acc: 0.9405 - val_mDice: 0.4207

Epoch 00109: val_mDice did not improve from 0.43513
Epoch 110/300
 - 11s - loss: 1.9520 - acc: 0.9266 - mDice: 0.4965 - val_loss: 2.8315 - val_acc: 0.9342 - val_mDice: 0.4326

Epoch 00110: val_mDice did not improve from 0.43513
Epoch 111/300
 - 11s - loss: 1.9457 - acc: 0.9268 - mDice: 0.4981 - val_loss: 2.9428 - val_acc: 0.9330 - val_mDice: 0.4245

Epoch 00111: val_mDice did not improve from 0.43513
Epoch 112/300
 - 11s - loss: 1.9341 - acc: 0.9272 - mDice: 0.5005 - val_loss: 2.9058 - val_acc: 0.9410 - val_mDice: 0.4457

Epoch 00112: val_mDice improved from 0.43513 to 0.44571, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 113/300
 - 11s - loss: 1.9265 - acc: 0.9273 - mDice: 0.5020 - val_loss: 2.9680 - val_acc: 0.9326 - val_mDice: 0.4333

Epoch 00113: val_mDice did not improve from 0.44571
Epoch 114/300
 - 11s - loss: 1.9237 - acc: 0.9275 - mDice: 0.5033 - val_loss: 3.0791 - val_acc: 0.9290 - val_mDice: 0.4250

Epoch 00114: val_mDice did not improve from 0.44571
Epoch 115/300
 - 12s - loss: 1.9221 - acc: 0.9273 - mDice: 0.5041 - val_loss: 2.9449 - val_acc: 0.9315 - val_mDice: 0.4238

Epoch 00115: val_mDice did not improve from 0.44571
Epoch 116/300
 - 11s - loss: 1.9215 - acc: 0.9273 - mDice: 0.5040 - val_loss: 3.0019 - val_acc: 0.9368 - val_mDice: 0.4228

Epoch 00116: val_mDice did not improve from 0.44571
Epoch 117/300
 - 11s - loss: 1.8997 - acc: 0.9277 - mDice: 0.5080 - val_loss: 3.0185 - val_acc: 0.9416 - val_mDice: 0.4339

Epoch 00117: val_mDice did not improve from 0.44571
Epoch 118/300
 - 11s - loss: 1.9055 - acc: 0.9275 - mDice: 0.5071 - val_loss: 3.1603 - val_acc: 0.9405 - val_mDice: 0.4328

Epoch 00118: val_mDice did not improve from 0.44571
Epoch 119/300
 - 11s - loss: 1.8953 - acc: 0.9278 - mDice: 0.5099 - val_loss: 3.0089 - val_acc: 0.9377 - val_mDice: 0.4343

Epoch 00119: val_mDice did not improve from 0.44571
Epoch 120/300
 - 11s - loss: 1.8896 - acc: 0.9280 - mDice: 0.5105 - val_loss: 3.1817 - val_acc: 0.9345 - val_mDice: 0.4306

Epoch 00120: val_mDice did not improve from 0.44571
Epoch 121/300
 - 11s - loss: 1.8816 - acc: 0.9282 - mDice: 0.5129 - val_loss: 3.0701 - val_acc: 0.9404 - val_mDice: 0.4300

Epoch 00121: val_mDice did not improve from 0.44571
Epoch 122/300
 - 11s - loss: 1.8738 - acc: 0.9283 - mDice: 0.5148 - val_loss: 3.0941 - val_acc: 0.9394 - val_mDice: 0.4313

Epoch 00122: val_mDice did not improve from 0.44571
Epoch 123/300
 - 11s - loss: 1.8636 - acc: 0.9285 - mDice: 0.5165 - val_loss: 3.1377 - val_acc: 0.9418 - val_mDice: 0.4357

Epoch 00123: val_mDice did not improve from 0.44571
Epoch 124/300
 - 11s - loss: 1.8672 - acc: 0.9284 - mDice: 0.5166 - val_loss: 3.0903 - val_acc: 0.9341 - val_mDice: 0.4275

Epoch 00124: val_mDice did not improve from 0.44571
Epoch 125/300
 - 11s - loss: 1.8621 - acc: 0.9286 - mDice: 0.5177 - val_loss: 3.0936 - val_acc: 0.9383 - val_mDice: 0.4387

Epoch 00125: val_mDice did not improve from 0.44571
Epoch 126/300
 - 11s - loss: 1.8637 - acc: 0.9284 - mDice: 0.5164 - val_loss: 2.9513 - val_acc: 0.9389 - val_mDice: 0.4449

Epoch 00126: val_mDice did not improve from 0.44571
Epoch 127/300
 - 11s - loss: 1.8498 - acc: 0.9287 - mDice: 0.5197 - val_loss: 3.5254 - val_acc: 0.9404 - val_mDice: 0.4288

Epoch 00127: val_mDice did not improve from 0.44571
Epoch 128/300
 - 11s - loss: 1.8427 - acc: 0.9290 - mDice: 0.5219 - val_loss: 3.0285 - val_acc: 0.9383 - val_mDice: 0.4423

Epoch 00128: val_mDice did not improve from 0.44571
Epoch 129/300
 - 11s - loss: 1.8375 - acc: 0.9290 - mDice: 0.5228 - val_loss: 2.9986 - val_acc: 0.9385 - val_mDice: 0.4379

Epoch 00129: val_mDice did not improve from 0.44571
Epoch 130/300
 - 11s - loss: 1.8396 - acc: 0.9291 - mDice: 0.5232 - val_loss: 3.1967 - val_acc: 0.9401 - val_mDice: 0.4396

Epoch 00130: val_mDice did not improve from 0.44571
Epoch 131/300
 - 11s - loss: 1.8347 - acc: 0.9289 - mDice: 0.5237 - val_loss: 3.1565 - val_acc: 0.9410 - val_mDice: 0.4428

Epoch 00131: val_mDice did not improve from 0.44571
Epoch 132/300
 - 11s - loss: 1.8302 - acc: 0.9290 - mDice: 0.5244 - val_loss: 3.0139 - val_acc: 0.9387 - val_mDice: 0.4432

Epoch 00132: val_mDice did not improve from 0.44571
Epoch 133/300
 - 11s - loss: 1.8280 - acc: 0.9290 - mDice: 0.5256 - val_loss: 3.3976 - val_acc: 0.9405 - val_mDice: 0.4378

Epoch 00133: val_mDice did not improve from 0.44571
Epoch 134/300
 - 11s - loss: 1.8331 - acc: 0.9288 - mDice: 0.5252 - val_loss: 3.0206 - val_acc: 0.9369 - val_mDice: 0.4218

Epoch 00134: val_mDice did not improve from 0.44571
Epoch 135/300
 - 11s - loss: 1.8123 - acc: 0.9292 - mDice: 0.5289 - val_loss: 2.9519 - val_acc: 0.9416 - val_mDice: 0.4471

Epoch 00135: val_mDice improved from 0.44571 to 0.44713, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 136/300
 - 11s - loss: 1.8200 - acc: 0.9291 - mDice: 0.5278 - val_loss: 3.4552 - val_acc: 0.9282 - val_mDice: 0.4019

Epoch 00136: val_mDice did not improve from 0.44713
Epoch 137/300
 - 11s - loss: 1.8059 - acc: 0.9296 - mDice: 0.5313 - val_loss: 3.1292 - val_acc: 0.9311 - val_mDice: 0.4330

Epoch 00137: val_mDice did not improve from 0.44713
Epoch 138/300
 - 12s - loss: 1.8107 - acc: 0.9295 - mDice: 0.5294 - val_loss: 3.2118 - val_acc: 0.9339 - val_mDice: 0.4291

Epoch 00138: val_mDice did not improve from 0.44713
Epoch 139/300
 - 11s - loss: 1.8014 - acc: 0.9296 - mDice: 0.5312 - val_loss: 3.1546 - val_acc: 0.9427 - val_mDice: 0.4427

Epoch 00139: val_mDice did not improve from 0.44713
Epoch 140/300
 - 11s - loss: 1.8013 - acc: 0.9296 - mDice: 0.5317 - val_loss: 3.2609 - val_acc: 0.9365 - val_mDice: 0.4385

Epoch 00140: val_mDice did not improve from 0.44713
Epoch 141/300
 - 11s - loss: 1.7924 - acc: 0.9298 - mDice: 0.5333 - val_loss: 3.1500 - val_acc: 0.9391 - val_mDice: 0.4428

Epoch 00141: val_mDice did not improve from 0.44713
Epoch 142/300
 - 11s - loss: 1.7872 - acc: 0.9299 - mDice: 0.5350 - val_loss: 3.2491 - val_acc: 0.9394 - val_mDice: 0.4400

Epoch 00142: val_mDice did not improve from 0.44713
Epoch 143/300
 - 11s - loss: 1.7849 - acc: 0.9299 - mDice: 0.5352 - val_loss: 3.2806 - val_acc: 0.9396 - val_mDice: 0.4346

Epoch 00143: val_mDice did not improve from 0.44713
Epoch 144/300
 - 11s - loss: 1.7831 - acc: 0.9301 - mDice: 0.5366 - val_loss: 3.4048 - val_acc: 0.9421 - val_mDice: 0.4368

Epoch 00144: val_mDice did not improve from 0.44713
Epoch 145/300
 - 11s - loss: 1.7845 - acc: 0.9297 - mDice: 0.5362 - val_loss: 3.1236 - val_acc: 0.9415 - val_mDice: 0.4437

Epoch 00145: val_mDice did not improve from 0.44713
Epoch 146/300
 - 11s - loss: 1.7720 - acc: 0.9300 - mDice: 0.5392 - val_loss: 3.1318 - val_acc: 0.9429 - val_mDice: 0.4493

Epoch 00146: val_mDice improved from 0.44713 to 0.44932, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 147/300
 - 11s - loss: 1.7751 - acc: 0.9301 - mDice: 0.5388 - val_loss: 3.0925 - val_acc: 0.9397 - val_mDice: 0.4470

Epoch 00147: val_mDice did not improve from 0.44932
Epoch 148/300
 - 11s - loss: 1.7626 - acc: 0.9302 - mDice: 0.5407 - val_loss: 3.0869 - val_acc: 0.9390 - val_mDice: 0.4486

Epoch 00148: val_mDice did not improve from 0.44932
Epoch 149/300
 - 11s - loss: 1.7564 - acc: 0.9304 - mDice: 0.5425 - val_loss: 3.1715 - val_acc: 0.9413 - val_mDice: 0.4393

Epoch 00149: val_mDice did not improve from 0.44932
Epoch 150/300
 - 12s - loss: 1.7617 - acc: 0.9302 - mDice: 0.5415 - val_loss: 3.1921 - val_acc: 0.9390 - val_mDice: 0.4457

Epoch 00150: val_mDice did not improve from 0.44932
Epoch 151/300
 - 11s - loss: 1.7517 - acc: 0.9304 - mDice: 0.5436 - val_loss: 3.1887 - val_acc: 0.9414 - val_mDice: 0.4415

Epoch 00151: val_mDice did not improve from 0.44932
Epoch 152/300
 - 11s - loss: 1.7508 - acc: 0.9303 - mDice: 0.5441 - val_loss: 3.3035 - val_acc: 0.9406 - val_mDice: 0.4382

Epoch 00152: val_mDice did not improve from 0.44932
Epoch 153/300
 - 11s - loss: 1.7466 - acc: 0.9304 - mDice: 0.5446 - val_loss: 3.0563 - val_acc: 0.9387 - val_mDice: 0.4526

Epoch 00153: val_mDice improved from 0.44932 to 0.45257, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 154/300
 - 11s - loss: 1.7450 - acc: 0.9304 - mDice: 0.5449 - val_loss: 3.3062 - val_acc: 0.9335 - val_mDice: 0.4333

Epoch 00154: val_mDice did not improve from 0.45257
Epoch 155/300
 - 11s - loss: 1.7456 - acc: 0.9303 - mDice: 0.5454 - val_loss: 3.2287 - val_acc: 0.9388 - val_mDice: 0.4481

Epoch 00155: val_mDice did not improve from 0.45257
Epoch 156/300
 - 11s - loss: 1.7355 - acc: 0.9307 - mDice: 0.5478 - val_loss: 3.1130 - val_acc: 0.9423 - val_mDice: 0.4512

Epoch 00156: val_mDice did not improve from 0.45257
Epoch 157/300
 - 12s - loss: 1.7298 - acc: 0.9307 - mDice: 0.5490 - val_loss: 3.3459 - val_acc: 0.9321 - val_mDice: 0.4352

Epoch 00157: val_mDice did not improve from 0.45257
Epoch 158/300
 - 11s - loss: 1.7311 - acc: 0.9308 - mDice: 0.5492 - val_loss: 3.3037 - val_acc: 0.9408 - val_mDice: 0.4480

Epoch 00158: val_mDice did not improve from 0.45257
Epoch 159/300
 - 11s - loss: 1.7269 - acc: 0.9309 - mDice: 0.5497 - val_loss: 2.8877 - val_acc: 0.9413 - val_mDice: 0.4592

Epoch 00159: val_mDice improved from 0.45257 to 0.45919, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 160/300
 - 11s - loss: 1.7277 - acc: 0.9308 - mDice: 0.5503 - val_loss: 3.0700 - val_acc: 0.9410 - val_mDice: 0.4486

Epoch 00160: val_mDice did not improve from 0.45919
Epoch 161/300
 - 11s - loss: 1.7278 - acc: 0.9308 - mDice: 0.5500 - val_loss: 3.1825 - val_acc: 0.9351 - val_mDice: 0.4404

Epoch 00161: val_mDice did not improve from 0.45919
Epoch 162/300
 - 11s - loss: 1.7272 - acc: 0.9308 - mDice: 0.5492 - val_loss: 3.1140 - val_acc: 0.9406 - val_mDice: 0.4473

Epoch 00162: val_mDice did not improve from 0.45919
Epoch 163/300
 - 12s - loss: 1.7188 - acc: 0.9311 - mDice: 0.5519 - val_loss: 3.3133 - val_acc: 0.9416 - val_mDice: 0.4501

Epoch 00163: val_mDice did not improve from 0.45919
Epoch 164/300
 - 11s - loss: 1.7092 - acc: 0.9312 - mDice: 0.5539 - val_loss: 3.2766 - val_acc: 0.9389 - val_mDice: 0.4400

Epoch 00164: val_mDice did not improve from 0.45919
Epoch 165/300
 - 12s - loss: 1.7090 - acc: 0.9313 - mDice: 0.5536 - val_loss: 3.1855 - val_acc: 0.9378 - val_mDice: 0.4474

Epoch 00165: val_mDice did not improve from 0.45919
Epoch 166/300
 - 11s - loss: 1.7051 - acc: 0.9313 - mDice: 0.5551 - val_loss: 3.0733 - val_acc: 0.9416 - val_mDice: 0.4550

Epoch 00166: val_mDice did not improve from 0.45919
Epoch 167/300
 - 12s - loss: 1.7098 - acc: 0.9313 - mDice: 0.5541 - val_loss: 3.4910 - val_acc: 0.9395 - val_mDice: 0.4386

Epoch 00167: val_mDice did not improve from 0.45919
Epoch 168/300
 - 11s - loss: 1.6984 - acc: 0.9314 - mDice: 0.5560 - val_loss: 3.1205 - val_acc: 0.9419 - val_mDice: 0.4556

Epoch 00168: val_mDice did not improve from 0.45919
Epoch 169/300
 - 12s - loss: 1.6954 - acc: 0.9315 - mDice: 0.5578 - val_loss: 3.1105 - val_acc: 0.9373 - val_mDice: 0.4437

Epoch 00169: val_mDice did not improve from 0.45919
Epoch 170/300
 - 11s - loss: 1.7001 - acc: 0.9314 - mDice: 0.5561 - val_loss: 3.3218 - val_acc: 0.9385 - val_mDice: 0.4470

Epoch 00170: val_mDice did not improve from 0.45919
Epoch 171/300
 - 11s - loss: 1.7023 - acc: 0.9314 - mDice: 0.5558 - val_loss: 3.1939 - val_acc: 0.9357 - val_mDice: 0.4514

Epoch 00171: val_mDice did not improve from 0.45919
Epoch 172/300
 - 11s - loss: 1.6897 - acc: 0.9318 - mDice: 0.5585 - val_loss: 3.3164 - val_acc: 0.9397 - val_mDice: 0.4437

Epoch 00172: val_mDice did not improve from 0.45919
Epoch 173/300
 - 11s - loss: 1.6868 - acc: 0.9319 - mDice: 0.5597 - val_loss: 3.1654 - val_acc: 0.9409 - val_mDice: 0.4493

Epoch 00173: val_mDice did not improve from 0.45919
Epoch 174/300
 - 12s - loss: 1.6883 - acc: 0.9318 - mDice: 0.5593 - val_loss: 3.4613 - val_acc: 0.9387 - val_mDice: 0.4423

Epoch 00174: val_mDice did not improve from 0.45919
Epoch 175/300
 - 12s - loss: 1.6861 - acc: 0.9318 - mDice: 0.5591 - val_loss: 3.2503 - val_acc: 0.9385 - val_mDice: 0.4450

Epoch 00175: val_mDice did not improve from 0.45919
Epoch 176/300
 - 12s - loss: 1.6875 - acc: 0.9319 - mDice: 0.5592 - val_loss: 3.1546 - val_acc: 0.9405 - val_mDice: 0.4488

Epoch 00176: val_mDice did not improve from 0.45919
Epoch 177/300
 - 12s - loss: 1.6812 - acc: 0.9321 - mDice: 0.5608 - val_loss: 3.1223 - val_acc: 0.9415 - val_mDice: 0.4518

Epoch 00177: val_mDice did not improve from 0.45919
Epoch 178/300
 - 12s - loss: 1.6789 - acc: 0.9320 - mDice: 0.5615 - val_loss: 3.0604 - val_acc: 0.9425 - val_mDice: 0.4571

Epoch 00178: val_mDice did not improve from 0.45919
Epoch 179/300
 - 12s - loss: 1.6787 - acc: 0.9320 - mDice: 0.5618 - val_loss: 3.1873 - val_acc: 0.9415 - val_mDice: 0.4505

Epoch 00179: val_mDice did not improve from 0.45919
Epoch 180/300
 - 12s - loss: 1.6748 - acc: 0.9321 - mDice: 0.5621 - val_loss: 3.2342 - val_acc: 0.9415 - val_mDice: 0.4527

Epoch 00180: val_mDice did not improve from 0.45919
Epoch 181/300
 - 12s - loss: 1.6768 - acc: 0.9322 - mDice: 0.5620 - val_loss: 3.4389 - val_acc: 0.9363 - val_mDice: 0.4397

Epoch 00181: val_mDice did not improve from 0.45919
Epoch 182/300
 - 12s - loss: 1.6661 - acc: 0.9325 - mDice: 0.5646 - val_loss: 3.2939 - val_acc: 0.9412 - val_mDice: 0.4473

Epoch 00182: val_mDice did not improve from 0.45919
Epoch 183/300
 - 12s - loss: 1.6686 - acc: 0.9324 - mDice: 0.5640 - val_loss: 3.2366 - val_acc: 0.9423 - val_mDice: 0.4524

Epoch 00183: val_mDice did not improve from 0.45919
Epoch 184/300
 - 12s - loss: 1.6615 - acc: 0.9324 - mDice: 0.5658 - val_loss: 3.0954 - val_acc: 0.9424 - val_mDice: 0.4572

Epoch 00184: val_mDice did not improve from 0.45919
Epoch 185/300
 - 12s - loss: 1.6555 - acc: 0.9327 - mDice: 0.5667 - val_loss: 3.1568 - val_acc: 0.9385 - val_mDice: 0.4538

Epoch 00185: val_mDice did not improve from 0.45919
Epoch 186/300
 - 12s - loss: 1.6561 - acc: 0.9327 - mDice: 0.5664 - val_loss: 3.2179 - val_acc: 0.9407 - val_mDice: 0.4476

Epoch 00186: val_mDice did not improve from 0.45919
Epoch 187/300
 - 12s - loss: 1.6613 - acc: 0.9325 - mDice: 0.5662 - val_loss: 3.4595 - val_acc: 0.9335 - val_mDice: 0.4448

Epoch 00187: val_mDice did not improve from 0.45919
Epoch 188/300
 - 12s - loss: 1.6605 - acc: 0.9325 - mDice: 0.5659 - val_loss: 3.2749 - val_acc: 0.9352 - val_mDice: 0.4476

Epoch 00188: val_mDice did not improve from 0.45919
Epoch 189/300
 - 12s - loss: 1.6559 - acc: 0.9325 - mDice: 0.5669 - val_loss: 3.2697 - val_acc: 0.9372 - val_mDice: 0.4547

Epoch 00189: val_mDice did not improve from 0.45919
Restoring model weights from the end of the best epoch
Epoch 00189: early stopping
{'val_loss': [137.89136704944428, 77.3197471300761, 47.14165644418625, 22.705657896541414, 12.784678944519587, 9.15607006422111, 8.377040714735077, 7.084767909986632, 6.66635705104896, 6.402024112996601, 5.89148209102097, 5.745034522243908, 5.532014170218082, 5.562325272709131, 5.296030408747139, 5.333750086880865, 4.9927498780723125, 5.012663826523792, 4.810240703679266, 4.6875153452690155, 4.625035688014967, 4.626000254193232, 4.49286682956985, 4.490299059078097, 4.800534511251109, 4.358615039182561, 4.257229147833728, 4.205130060691209, 4.418383474993918, 4.302505926805592, 4.302917079955694, 4.615474333543153, 4.4282607319099565, 4.0012197164109065, 3.873380776272998, 3.868276494022991, 3.9353654370421456, 3.898476340009698, 3.7583899269590066, 3.706069865042255, 3.6045072176715447, 3.330461812870843, 3.8446076100337363, 3.285698010142715, 3.130207271802993, 3.2367832019392933, 3.4310153912131987, 3.4282000433387503, 3.0715488351526714, 3.285348517421101, 3.402926817092867, 3.1633645205625465, 3.1507011766855917, 3.137392841922563, 2.929770223840716, 3.016600161923894, 3.0530351472663737, 3.6840855990137373, 3.3295624977687286, 3.2471726654496575, 2.9222613813000775, 2.917089578939513, 2.954304321846437, 2.969230864086144, 2.8602204841251173, 3.0374030534710204, 2.92125569445835, 2.733481359934168, 2.960183890048592, 2.9029357725576985, 2.736333318897301, 2.8976261080464436, 2.9236410490370224, 2.9022582475362078, 2.9320015905957137, 2.871020705527848, 2.793685745758315, 2.8640528229464377, 2.9448436287098696, 2.85746709980248, 2.8832319793956622, 2.979795261153153, 2.903145600642477, 2.8555386431870007, 2.794571783304924, 2.8767559888462224, 2.9521859416826848, 2.870548162875431, 2.8155799602557505, 2.936685213420008, 2.8612437776422928, 2.9408710394941626, 3.004717766395992, 2.9046607982101187, 2.82053183196556, 3.056329988297962, 3.0585012047418525, 2.970958175714172, 2.858405819961003, 2.9117202764554393, 3.059620526840999, 3.033899829456849, 2.841930448210665, 3.0122765485818186, 2.9917050461683954, 2.959342404579123, 2.9210820175884735, 3.122105352669245, 3.1872646432547342, 2.8315336745498434, 2.9428016446708214, 2.9058169380628636, 2.9679834897674264, 3.0791273794713474, 2.94485217873894, 3.001900791101867, 3.0184692014895735, 3.1602804614674476, 3.0089158933343634, 3.1817262769632397, 3.070050066558733, 3.0940965765954127, 3.1377304382178757, 3.0902586739421603, 3.0936298787682537, 2.9512985377971614, 3.525391803522195, 3.0284948400443508, 2.9985911437709416, 3.1966884625809535, 3.1565131502935575, 3.0139085925849423, 3.3975727103561875, 3.020581120536441, 2.951877431323131, 3.455218863540462, 3.129248582270174, 3.2118468613230755, 3.15457476775295, 3.2608570917287754, 3.149971407084238, 3.24913484809388, 3.2806376578463685, 3.4047643010770634, 3.1235783010100326, 3.1317565825191283, 3.0924727078970697, 3.086924433708191, 3.171497760562315, 3.192076936186779, 3.1887475496512794, 3.30353560103547, 3.0563465205597735, 3.3061833120439026, 3.2287006052654412, 3.1129875588009046, 3.345863594984015, 3.3037259966090677, 2.887678410148337, 3.0700184866076423, 3.1825248070415997, 3.1139881178027107, 3.313302366196045, 3.2766397525334643, 3.185525623992795, 3.073311587662569, 3.490952005476824, 3.12047603251856, 3.1104716632426497, 3.3217656937915656, 3.1939183588450155, 3.316362652661545, 3.1653947855035462, 3.4613168157548424, 3.2502631413351213, 3.1546364640817046, 3.1223203483082, 3.0604298778233074, 3.1872832985772264, 3.2341582542285323, 3.4389356872332946, 3.2939342004025267, 3.236609306674273, 3.0953878505776324, 3.1568096865500723, 3.217921119493743, 3.4595348762703084, 3.274863621574782, 3.269669145789175], 'val_acc': [0.8302976318768093, 0.7069345088232131, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047252989950634, 0.9050526704107013, 0.9048649101030259, 0.906586522147769, 0.9084111650784811, 0.9100320651417687, 0.9112728947684878, 0.9155242925598508, 0.9136309368269784, 0.9162110686302185, 0.9155700774419875, 0.9161126131103152, 0.9166186111313956, 0.9175412200746083, 0.9212248438880557, 0.9196016760099501, 0.9225549328894842, 0.9230059754280817, 0.9192216169266474, 0.9093063189869836, 0.926671238172622, 0.9260554001444862, 0.9168589484123957, 0.925608978385017, 0.9286859120641436, 0.9135805737404596, 0.9274794118744987, 0.9215155584471566, 0.9173466279393151, 0.9256318835985093, 0.9185965969448998, 0.9300847138677325, 0.9315498925390697, 0.9338713515372503, 0.9282508889834086, 0.9313736188979376, 0.9311606997535342, 0.9329395833469573, 0.9247504415966216, 0.9197115273702712, 0.9358905440285092, 0.9301785855066209, 0.9351259101004827, 0.9335393962405977, 0.9296405741146633, 0.9271016234443301, 0.934608538945516, 0.9202587036859422, 0.9365361872173491, 0.9273992890403384, 0.9360600085485549, 0.9378548718634105, 0.9318383591515678, 0.9305174010140556, 0.9354372478666759, 0.9352838879539853, 0.9342651083355858, 0.9389629165331522, 0.9326007224264599, 0.935022893406096, 0.9396337157204038, 0.9391094190733773, 0.9368063097908383, 0.9375755417914617, 0.9383516595477149, 0.9316117224239168, 0.9368314941724142, 0.9398443301518759, 0.9394528553599403, 0.9358035893667311, 0.9353960610571361, 0.9374221421423412, 0.9362408547174363, 0.9288621828669593, 0.9391094389415923, 0.9395123521486918, 0.9366826642127264, 0.9351763044084821, 0.9355585944084894, 0.9372870780172802, 0.9373099917457217, 0.9345741868019104, 0.9405265393711272, 0.9342124348595029, 0.9330036384718758, 0.9409890288398379, 0.9325503933997381, 0.9290475902103242, 0.9315064066932315, 0.936753667536236, 0.9416483356839135, 0.9405380317143032, 0.9376693878855024, 0.9344711530776251, 0.9403594448452904, 0.9393543913250878, 0.9418040286926996, 0.9341437816619873, 0.9383287373043242, 0.9389194193340483, 0.9404029335294452, 0.9382531841595968, 0.9384523715291705, 0.9401373721304394, 0.9410118943169004, 0.9386767659868512, 0.9405356986182076, 0.9369253658113026, 0.9415979697590783, 0.9282028164182391, 0.93106685649781, 0.9339377198900495, 0.9426991967927842, 0.9365086896078927, 0.9390659417424884, 0.9393658439318339, 0.9395581654139927, 0.9420650317555382, 0.9415201459612165, 0.9428617358207703, 0.9397001039414179, 0.9389972771917071, 0.941298056216467, 0.9389766227631342, 0.9414125510624477, 0.9405791958173116, 0.9386721366927737, 0.9335279549871173, 0.9387522708801996, 0.942335185550508, 0.9320833512714931, 0.9408264529137385, 0.9412820679800851, 0.9409752857117426, 0.9350617868559701, 0.9405563047954014, 0.9415819389479501, 0.9389079809188843, 0.9377975974764142, 0.9415956820760455, 0.9395169133231753, 0.9418612718582153, 0.9372893855685279, 0.9384592714763823, 0.9356890860058013, 0.9396977963901701, 0.9409157349949792, 0.9386950475828988, 0.938527953057062, 0.9405471483866373, 0.9414812326431274, 0.9424565264156887, 0.9414743866239276, 0.9415109725225539, 0.9363347121647426, 0.9411607015700567, 0.9422756433486938, 0.9424267411231995, 0.9384661146572658, 0.9407371907007127, 0.9335096109481085, 0.9352037764730907, 0.9372184077898661], 'val_mDice': [0.013342723339086487, 0.012142232508354243, 0.012049423303410765, 0.00684507476121542, 0.009055572556376103, 0.01022093012441127, 0.011178416443899983, 0.012439053522289864, 0.020773181370237752, 0.02708452824680578, 0.02635872110147916, 0.0379254528143931, 0.03090115383799587, 0.03029181675187179, 0.03781325425509186, 0.039015088319068865, 0.054927313700318336, 0.05395768578386023, 0.06496694812639839, 0.06952119547696341, 0.07372484529124838, 0.07357285730540752, 0.08190029584580943, 0.08407054810474317, 0.07674189049395777, 0.09151147643015498, 0.0990445510528627, 0.10540291001754147, 0.10242101567841712, 0.11639835862886339, 0.12362116380106836, 0.12426821967320782, 0.13157141315085547, 0.15326791114750363, 0.16717860838841825, 0.1785062134620689, 0.18944669018189111, 0.19858217718345778, 0.2142666413315705, 0.2225443951430775, 0.2362167001480148, 0.2532750165888241, 0.23649676472303413, 0.26843753847337903, 0.2801108618399927, 0.28336549887344953, 0.2773365488364583, 0.2853404744749978, 0.31303173551956814, 0.2955774711001487, 0.30552157795145396, 0.3047325627080032, 0.3206846018632253, 0.3290018311568669, 0.33822831068010556, 0.34832903033211116, 0.34303942961352213, 0.3122592395881102, 0.3465441432559774, 0.349008366110779, 0.375161784745398, 0.38173969373816535, 0.3739347049877757, 0.3776265914951052, 0.3783547079988888, 0.3648878602044923, 0.3840888440608978, 0.39649499899574686, 0.39121554720969426, 0.3938770563829513, 0.3950268200465611, 0.3969828287760417, 0.40195619456824805, 0.39300324696870076, 0.3988311528449967, 0.39700094645931605, 0.41054590685026987, 0.4062856253946111, 0.4072026856953189, 0.4054104724810237, 0.41391588250796, 0.4105415010736102, 0.4144034820298354, 0.4199531426032384, 0.41909571169387727, 0.42041544687180293, 0.4115757303578513, 0.42569349457820255, 0.42407427568520817, 0.41466963947528884, 0.4234308171130362, 0.4083457126149109, 0.4159979170986584, 0.42319200560450554, 0.43056146640862736, 0.41288535651706515, 0.4239135728705497, 0.42467047540204866, 0.4232435435766265, 0.4227532439288639, 0.4235635752834025, 0.4273346689130579, 0.4276462956553414, 0.4245726842255819, 0.42688650637865067, 0.4351330711728051, 0.4290499259673414, 0.4244063700593653, 0.4206620573642708, 0.4325937965796107, 0.42454295250631513, 0.4457125612312839, 0.43328911899810746, 0.42498992791488055, 0.4237597861460277, 0.4227534106799534, 0.43390124575013206, 0.43280314033230144, 0.43431865530354635, 0.4305818327480838, 0.43002498983627274, 0.43133987699236187, 0.43571903804938, 0.4274705831138861, 0.4386630732388723, 0.4449090212583542, 0.4287504842948346, 0.44226530087845667, 0.4379272304830097, 0.439629463745015, 0.44284512581569807, 0.4432097167841026, 0.4377589747309685, 0.4218252943385215, 0.4471317655628636, 0.4019498780724548, 0.432965201990945, 0.4290592393704823, 0.4427135217757452, 0.4384513483161018, 0.4427837186626026, 0.4400269264976184, 0.43464783596850576, 0.4368081195723443, 0.44372499521289555, 0.4493184634262607, 0.4469563950385366, 0.44861234972874325, 0.439341524881976, 0.4457330070436001, 0.4415093976117316, 0.4381762370467186, 0.4525737546029545, 0.43328689641895746, 0.44809860470039503, 0.4511960365232967, 0.4351883097773507, 0.44798732903741656, 0.45918519138580277, 0.448581657239369, 0.4404220330928053, 0.4473165451061158, 0.45009786387284595, 0.43995322109687895, 0.4473744236997196, 0.4549816939092818, 0.43856553236643475, 0.45558352839379085, 0.4437447828905923, 0.4469684014717738, 0.451389369510469, 0.44365860983019784, 0.449344479434547, 0.44226941359894617, 0.44499056360551287, 0.4488421618228867, 0.45177786903721945, 0.45713217680652934, 0.45046774954313323, 0.45273865333625246, 0.439666228280181, 0.44733041773239773, 0.452398784636032, 0.45722275440182003, 0.4538456488932882, 0.44759140482970644, 0.4448357838250342, 0.4476185659212725, 0.4547302330888453], 'loss': [240.10024272359036, 123.93217131727023, 66.83425565314748, 40.59643098534429, 27.783550871022985, 18.64493150878371, 14.582407297921682, 12.247272550105956, 10.1547565189933, 9.032622322908413, 8.358968813064955, 7.879196811692205, 7.482797337975694, 7.1637521963117665, 6.897470224930966, 6.649185048193975, 6.431639266193361, 6.251235815930371, 6.083194294623012, 5.92462074703763, 5.797624432560289, 5.667722058797218, 5.5011073707821785, 5.3694511024805305, 5.264039176448545, 5.161273383303646, 5.0568968966583165, 4.964616584161906, 4.879326816052859, 4.780456590220398, 4.6758894270409135, 4.575489019690301, 4.486434352404814, 4.394777286436509, 4.27747094541564, 4.16347476317746, 4.04203744115732, 3.9364033698667926, 3.8283305670085706, 3.714962091155426, 3.62283997276465, 3.523256940466603, 3.4407565826300552, 3.3684949309972523, 3.289186088202752, 3.2342738948485192, 3.1708977298743926, 3.105470280760351, 3.0453307331240382, 3.0026390351195635, 2.9677405236014196, 2.919588911526461, 2.8719733374321508, 2.819461941925896, 2.7853245310914847, 2.7388946601874293, 2.696345551615624, 2.666200828065021, 2.631587550385922, 2.59710930072666, 2.5753970524468173, 2.53791130037236, 2.5231028669069375, 2.494623609262095, 2.4747181175668413, 2.457168123798837, 2.432048269220922, 2.4127590069771734, 2.3955529363531816, 2.377171940809963, 2.358287075592278, 2.3394023847644307, 2.3227758412648885, 2.3147456335704266, 2.2986080870592502, 2.2868732760033903, 2.2663701932876585, 2.2581333576058613, 2.2381722265432535, 2.2189576056322107, 2.2143601780570217, 2.20535433977262, 2.1875044812304862, 2.1763815792488965, 2.16433296565355, 2.1524505734972257, 2.1436351406935072, 2.1311062857758594, 2.127297315773119, 2.114410726509734, 2.107172541749806, 2.0862091941964036, 2.083691125916267, 2.0685877263120815, 2.064166662250384, 2.0509408896705286, 2.053210994285817, 2.046347573724169, 2.036771758480157, 2.0203702670888495, 2.0111540434837156, 2.0135785054443476, 1.9959943953658061, 1.9910984360139816, 1.9869932774199395, 1.9779153653875305, 1.9681356785865975, 1.9579460433574225, 1.9552166125483428, 1.9519683023581091, 1.9457027762295172, 1.9341438078158666, 1.9265409881309759, 1.9236863586331279, 1.9221049871705782, 1.9214753383888727, 1.899676360030475, 1.9055148248661453, 1.8952564582306226, 1.8895974437556609, 1.8815768681580054, 1.8738428520425474, 1.8635742276625433, 1.8671935181960633, 1.8620671550271966, 1.8636820357912696, 1.8498413037858488, 1.8427449921456656, 1.8374542578902575, 1.8396024662611317, 1.8347091229866987, 1.8301547356876353, 1.8280466313028418, 1.8330890432543117, 1.8123190110649519, 1.8199565457727769, 1.8058904687034985, 1.8107302318033924, 1.8013626546190495, 1.8013279437788976, 1.792352580302525, 1.78715638356423, 1.7848955752280997, 1.7831435656303933, 1.784462406820522, 1.7720451684546925, 1.775128084293606, 1.762582616261809, 1.7563508325444037, 1.7617469578550482, 1.7517207411412492, 1.750786224127689, 1.7466077609215718, 1.7450084204569887, 1.7455998431608952, 1.735459778243139, 1.729786880953546, 1.7311180869071774, 1.7269462872406456, 1.7276886371327935, 1.7278399159505184, 1.7271791418922047, 1.718821501695101, 1.7092036965358963, 1.7090121234614701, 1.7051300156063627, 1.7097693723085094, 1.69840988387532, 1.6954022691225485, 1.7001234283561257, 1.7023195366145145, 1.6896787087352423, 1.686781823186946, 1.6882816474056668, 1.6860865713751612, 1.687529682745381, 1.681185562417115, 1.6788756142513863, 1.6786664661148598, 1.6747694503647388, 1.6768190132898242, 1.6660560191378604, 1.6685597613433756, 1.6615032418651665, 1.6554786749880783, 1.6560724243070661, 1.6613062800344347, 1.6605014676231398, 1.6558579501559654], 'acc': [0.32721736105218174, 0.7095944592091811, 0.7998359388346292, 0.8690307045778098, 0.8690719725723921, 0.8691718204186467, 0.8691881101600738, 0.869196688080882, 0.8691862296318075, 0.8691840092065317, 0.869176940488659, 0.8691742049721548, 0.8691638038754578, 0.8691251292172621, 0.8689905942141355, 0.8686733211835777, 0.868428352149392, 0.8685085484580266, 0.8687585025870641, 0.8689362060616925, 0.8691769173224813, 0.8691283947979758, 0.8692081790910835, 0.8690278553677727, 0.8689135428060565, 0.8690298222980679, 0.8694235632041297, 0.8703670467695152, 0.8719538955622288, 0.8740655503016528, 0.8767677582319197, 0.8790195033870636, 0.8805792216094491, 0.8815923434290234, 0.8825810129917815, 0.8834934507764004, 0.8843636251332467, 0.8861426410703547, 0.8876002854824709, 0.889603194667352, 0.8909433123369954, 0.8924378706054327, 0.8940665482877191, 0.8953198438576013, 0.8971051653616916, 0.8982842202956033, 0.8993108696806102, 0.9006180374958347, 0.9018287191805592, 0.9028127123584677, 0.9030702656832692, 0.9039794382319093, 0.9053375612616148, 0.9060570262911142, 0.9068451220069474, 0.9076570636485615, 0.908664088971311, 0.9092840746645525, 0.91038582435067, 0.9110952078756027, 0.9116884295090513, 0.9123430839388362, 0.9125060718000245, 0.9132654349974454, 0.9135012086103802, 0.9138295055814669, 0.9143148419460351, 0.9149336416596779, 0.9153573175420737, 0.9156989193454468, 0.916160917980767, 0.9165039518124478, 0.9172163945943238, 0.9173034508379526, 0.9177858668162411, 0.9181310332003002, 0.9185501648921462, 0.9187701339778676, 0.9192927998914547, 0.9198133070298373, 0.919674830949672, 0.9200828894154056, 0.9204994504084494, 0.9207459279216276, 0.9209884223460876, 0.9213734708104447, 0.9215429022819154, 0.9217951253748868, 0.9220711696416812, 0.9222265669254753, 0.9225555087459646, 0.9230897346511843, 0.9233042373233432, 0.9236810823982566, 0.9238837436143204, 0.9243474802530545, 0.924361269253859, 0.9243211368731228, 0.9248384458838985, 0.924972308906995, 0.9253090884567388, 0.9252999355412872, 0.9256695989632114, 0.9257335739372919, 0.9260200453282598, 0.9260711850218168, 0.9264655486108343, 0.9266650584683107, 0.9268044141507779, 0.926632853884538, 0.9268474686184852, 0.9271592912311749, 0.927287823781127, 0.9274627062105503, 0.9273348168782561, 0.9273214434322558, 0.9277253117041021, 0.927545722989832, 0.9278054831803408, 0.9280223521810289, 0.9281921766501574, 0.9282533028278825, 0.9284573731358993, 0.9283713879602733, 0.9285553004186875, 0.9284407614351629, 0.9287153258129699, 0.929007151728079, 0.9290286081821353, 0.9290973115270713, 0.9288808414304971, 0.9289909785414836, 0.9289824966752784, 0.9287538596929694, 0.9292005700069469, 0.9291330228450327, 0.9296134179535228, 0.9294507993262743, 0.9295836001295609, 0.9295613324456025, 0.929830747508396, 0.9298997100019629, 0.9298816129194649, 0.9300925192723551, 0.9297209134767436, 0.9299864856084923, 0.9300778757199769, 0.930226430213587, 0.930383952373665, 0.9301599952925003, 0.9303996850121842, 0.930325327832873, 0.9304195213965972, 0.9303664332275472, 0.9303256555492533, 0.9306599768526825, 0.9306710537100013, 0.9308285051764655, 0.9308708361033096, 0.9307584091604434, 0.93078764731286, 0.9307782916244064, 0.9311445238894986, 0.931227684377222, 0.9312528734356998, 0.9313484082475709, 0.9313144855056168, 0.9313617121031088, 0.9314530554455738, 0.9313836070827972, 0.9314297197272788, 0.9318264245136593, 0.9318540431709907, 0.931761680103024, 0.9317609843477845, 0.9319125067780927, 0.9320759389334753, 0.9320320507361752, 0.931976713885088, 0.9320956337468023, 0.9322299124855791, 0.9324651583095541, 0.9323609270455636, 0.9324467783261982, 0.9326734221094212, 0.9326856825102127, 0.9325309856125953, 0.932528666719573, 0.9325277386015955], 'mDice': [0.016487331628937334, 0.014140372974494645, 0.01302904622273418, 0.012244189742694752, 0.012481608924694562, 0.013687481372939476, 0.015341647354990433, 0.017551845510556493, 0.02042878065664046, 0.022839661025793562, 0.025189401808474735, 0.027530939216794295, 0.03014856903771226, 0.03288251136438406, 0.03563830630571162, 0.0395579738547824, 0.04368627113179708, 0.04758905346252359, 0.05092113597362699, 0.05454277377902416, 0.05752699884670604, 0.060641454630776456, 0.06459272246364686, 0.06839218712281647, 0.07244547690491651, 0.0766329135000993, 0.0822794648486685, 0.08807324134897859, 0.09466500207997251, 0.10240086865133234, 0.11125288968063721, 0.11983353163560784, 0.12777010582822676, 0.13628952911600156, 0.14764443598651739, 0.15921650410376464, 0.1705350283887406, 0.18228433296552668, 0.19368146014898435, 0.20733740244363116, 0.21827870141409578, 0.22980810868756169, 0.24045810137889922, 0.24903912011750154, 0.26027627572265, 0.2678628521133807, 0.27528934375798975, 0.2845278471817464, 0.2921019782444542, 0.299505893954749, 0.30439739857470116, 0.3109281414479494, 0.3188207282458332, 0.32678721070933114, 0.3328439768210573, 0.34083259119471765, 0.34785458601828784, 0.35367771781476814, 0.36060452367589724, 0.36699048600514916, 0.37065639603521777, 0.3774541938824302, 0.38017443590640376, 0.3857960734715323, 0.389019932261041, 0.3915861763773981, 0.39641444119318664, 0.39997554927380624, 0.4027663377844945, 0.40704238613561466, 0.4103757715280377, 0.4133584915224196, 0.41773619384280236, 0.4187228725851444, 0.42283399530934396, 0.42483356512303017, 0.42906220864302763, 0.43113548177180777, 0.4347085388370486, 0.4379515218118815, 0.4394708423628751, 0.44172122264404806, 0.4455241133637205, 0.44777277230940643, 0.4502026597348532, 0.4523412117740656, 0.4546850871000994, 0.45688841302477146, 0.4587292799904601, 0.4602042170981578, 0.4624614190660002, 0.4661491621154247, 0.46769046684623294, 0.47095911019519965, 0.47170000573171683, 0.47418565900196613, 0.47466588912038693, 0.4762250403057571, 0.47824045124025455, 0.4811509315370479, 0.4834923570800062, 0.4830715855709592, 0.4866199692090352, 0.4878922600449223, 0.48891184705517665, 0.4906420024093553, 0.49288194894744686, 0.49533896053115456, 0.4960931920122728, 0.4964803573550667, 0.4980594062761509, 0.5004583337253198, 0.502018583976903, 0.5033170144199889, 0.5040557733017837, 0.5039900947747856, 0.5080281465021843, 0.5070556868862411, 0.509918512361735, 0.51054961737213, 0.5128843870401061, 0.5147625674786815, 0.5164894879909984, 0.5166411158412045, 0.5176503247370811, 0.5164217448128839, 0.5196510230228578, 0.5219158510470496, 0.5227901866400612, 0.5231661641209346, 0.5236879327128521, 0.524429961817789, 0.525559612430082, 0.5251678528527385, 0.5288530707428261, 0.5277778689333164, 0.5312778005508966, 0.5293989284594256, 0.5312166909434236, 0.5317074754046456, 0.5333223686021533, 0.5349899108800121, 0.5351988490730529, 0.5365618135236421, 0.5361575078752796, 0.5391588811013013, 0.538819643316734, 0.5406743043048639, 0.542469056189945, 0.5415018282806296, 0.5435816801672201, 0.544081697418944, 0.5445824502542663, 0.5448704811105016, 0.5454333000084741, 0.5478101787664583, 0.5490353944238634, 0.5492480659912334, 0.5497069884661147, 0.5502735004044461, 0.5500411654635617, 0.5491616231801769, 0.5519362419631088, 0.5539115857636743, 0.5536102771713068, 0.5550618215013566, 0.5540983221171011, 0.5560014901281621, 0.5577908181744272, 0.556099798078594, 0.5557714807336813, 0.5584625841588994, 0.5596834731419146, 0.5593299730931355, 0.5591458636417723, 0.5592349812270636, 0.5607649152945847, 0.5615129430491225, 0.561797929299643, 0.5621289682912303, 0.5620288189973862, 0.5645683562799251, 0.5640282869017237, 0.5657867966691997, 0.566724541710364, 0.5664217217202496, 0.5662172482793723, 0.5659368177450345, 0.5668561897205208]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.97s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.59s/it]predicting test subjects: 100%|██████████| 3/3 [00:06<00:00,  2.30s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:36,  1.61s/it]predicting train subjects:   1%|          | 2/285 [00:03<08:07,  1.72s/it]predicting train subjects:   1%|          | 3/285 [00:05<08:12,  1.75s/it]predicting train subjects:   1%|▏         | 4/285 [00:07<08:45,  1.87s/it]predicting train subjects:   2%|▏         | 5/285 [00:09<08:26,  1.81s/it]predicting train subjects:   2%|▏         | 6/285 [00:11<08:50,  1.90s/it]predicting train subjects:   2%|▏         | 7/285 [00:13<09:30,  2.05s/it]predicting train subjects:   3%|▎         | 8/285 [00:15<09:37,  2.09s/it]predicting train subjects:   3%|▎         | 9/285 [00:17<09:26,  2.05s/it]predicting train subjects:   4%|▎         | 10/285 [00:20<09:35,  2.09s/it]predicting train subjects:   4%|▍         | 11/285 [00:22<09:56,  2.18s/it]predicting train subjects:   4%|▍         | 12/285 [00:24<09:48,  2.16s/it]predicting train subjects:   5%|▍         | 13/285 [00:26<10:02,  2.22s/it]predicting train subjects:   5%|▍         | 14/285 [00:29<10:12,  2.26s/it]predicting train subjects:   5%|▌         | 15/285 [00:31<10:21,  2.30s/it]predicting train subjects:   6%|▌         | 16/285 [00:33<10:19,  2.30s/it]predicting train subjects:   6%|▌         | 17/285 [00:36<10:18,  2.31s/it]predicting train subjects:   6%|▋         | 18/285 [00:38<10:23,  2.34s/it]predicting train subjects:   7%|▋         | 19/285 [00:41<10:19,  2.33s/it]predicting train subjects:   7%|▋         | 20/285 [00:43<10:28,  2.37s/it]predicting train subjects:   7%|▋         | 21/285 [00:45<10:15,  2.33s/it]predicting train subjects:   8%|▊         | 22/285 [00:47<09:54,  2.26s/it]predicting train subjects:   8%|▊         | 23/285 [00:50<10:06,  2.31s/it]predicting train subjects:   8%|▊         | 24/285 [00:52<10:00,  2.30s/it]predicting train subjects:   9%|▉         | 25/285 [00:54<10:01,  2.31s/it]predicting train subjects:   9%|▉         | 26/285 [00:57<09:57,  2.31s/it]predicting train subjects:   9%|▉         | 27/285 [00:59<09:54,  2.30s/it]predicting train subjects:  10%|▉         | 28/285 [01:01<09:44,  2.28s/it]predicting train subjects:  10%|█         | 29/285 [01:03<09:40,  2.27s/it]predicting train subjects:  11%|█         | 30/285 [01:06<09:26,  2.22s/it]predicting train subjects:  11%|█         | 31/285 [01:08<09:25,  2.23s/it]predicting train subjects:  11%|█         | 32/285 [01:10<09:30,  2.25s/it]predicting train subjects:  12%|█▏        | 33/285 [01:12<09:25,  2.24s/it]predicting train subjects:  12%|█▏        | 34/285 [01:15<09:25,  2.25s/it]predicting train subjects:  12%|█▏        | 35/285 [01:17<09:22,  2.25s/it]predicting train subjects:  13%|█▎        | 36/285 [01:19<09:09,  2.21s/it]predicting train subjects:  13%|█▎        | 37/285 [01:21<09:11,  2.22s/it]predicting train subjects:  13%|█▎        | 38/285 [01:24<09:18,  2.26s/it]predicting train subjects:  14%|█▎        | 39/285 [01:26<09:04,  2.21s/it]predicting train subjects:  14%|█▍        | 40/285 [01:28<08:53,  2.18s/it]predicting train subjects:  14%|█▍        | 41/285 [01:30<08:53,  2.19s/it]predicting train subjects:  15%|█▍        | 42/285 [01:32<08:56,  2.21s/it]predicting train subjects:  15%|█▌        | 43/285 [01:34<08:55,  2.21s/it]predicting train subjects:  15%|█▌        | 44/285 [01:37<08:50,  2.20s/it]predicting train subjects:  16%|█▌        | 45/285 [01:39<08:47,  2.20s/it]predicting train subjects:  16%|█▌        | 46/285 [01:41<08:09,  2.05s/it]predicting train subjects:  16%|█▋        | 47/285 [01:42<07:54,  1.99s/it]predicting train subjects:  17%|█▋        | 48/285 [01:44<07:38,  1.93s/it]predicting train subjects:  17%|█▋        | 49/285 [01:46<07:30,  1.91s/it]predicting train subjects:  18%|█▊        | 50/285 [01:48<07:24,  1.89s/it]predicting train subjects:  18%|█▊        | 51/285 [01:50<07:19,  1.88s/it]predicting train subjects:  18%|█▊        | 52/285 [01:51<07:07,  1.83s/it]predicting train subjects:  19%|█▊        | 53/285 [01:53<07:19,  1.89s/it]predicting train subjects:  19%|█▉        | 54/285 [01:55<07:24,  1.92s/it]predicting train subjects:  19%|█▉        | 55/285 [01:57<07:20,  1.92s/it]predicting train subjects:  20%|█▉        | 56/285 [01:59<07:19,  1.92s/it]predicting train subjects:  20%|██        | 57/285 [02:01<07:11,  1.89s/it]predicting train subjects:  20%|██        | 58/285 [02:03<07:00,  1.85s/it]predicting train subjects:  21%|██        | 59/285 [02:05<06:59,  1.86s/it]predicting train subjects:  21%|██        | 60/285 [02:07<07:06,  1.90s/it]predicting train subjects:  21%|██▏       | 61/285 [02:09<07:08,  1.91s/it]predicting train subjects:  22%|██▏       | 62/285 [02:11<07:03,  1.90s/it]predicting train subjects:  22%|██▏       | 63/285 [02:12<07:00,  1.89s/it]predicting train subjects:  22%|██▏       | 64/285 [02:14<07:05,  1.93s/it]predicting train subjects:  23%|██▎       | 65/285 [02:17<07:25,  2.02s/it]predicting train subjects:  23%|██▎       | 66/285 [02:19<07:21,  2.02s/it]predicting train subjects:  24%|██▎       | 67/285 [02:21<07:26,  2.05s/it]predicting train subjects:  24%|██▍       | 68/285 [02:23<07:18,  2.02s/it]predicting train subjects:  24%|██▍       | 69/285 [02:25<07:13,  2.01s/it]predicting train subjects:  25%|██▍       | 70/285 [02:27<07:09,  2.00s/it]predicting train subjects:  25%|██▍       | 71/285 [02:29<07:04,  1.98s/it]predicting train subjects:  25%|██▌       | 72/285 [02:31<07:00,  1.97s/it]predicting train subjects:  26%|██▌       | 73/285 [02:33<07:01,  1.99s/it]predicting train subjects:  26%|██▌       | 74/285 [02:35<07:05,  2.02s/it]predicting train subjects:  26%|██▋       | 75/285 [02:37<06:58,  1.99s/it]predicting train subjects:  27%|██▋       | 76/285 [02:39<06:57,  2.00s/it]predicting train subjects:  27%|██▋       | 77/285 [02:41<06:56,  2.00s/it]predicting train subjects:  27%|██▋       | 78/285 [02:43<06:51,  1.99s/it]predicting train subjects:  28%|██▊       | 79/285 [02:45<06:46,  1.97s/it]predicting train subjects:  28%|██▊       | 80/285 [02:47<06:42,  1.97s/it]predicting train subjects:  28%|██▊       | 81/285 [02:49<06:44,  1.98s/it]predicting train subjects:  29%|██▉       | 82/285 [02:50<06:39,  1.97s/it]predicting train subjects:  29%|██▉       | 83/285 [02:52<06:33,  1.95s/it]predicting train subjects:  29%|██▉       | 84/285 [02:54<06:35,  1.97s/it]predicting train subjects:  30%|██▉       | 85/285 [02:57<06:49,  2.05s/it]predicting train subjects:  30%|███       | 86/285 [02:59<06:48,  2.05s/it]predicting train subjects:  31%|███       | 87/285 [03:01<06:54,  2.09s/it]predicting train subjects:  31%|███       | 88/285 [03:03<07:02,  2.15s/it]predicting train subjects:  31%|███       | 89/285 [03:05<07:06,  2.17s/it]predicting train subjects:  32%|███▏      | 90/285 [03:08<07:09,  2.20s/it]predicting train subjects:  32%|███▏      | 91/285 [03:10<07:13,  2.23s/it]predicting train subjects:  32%|███▏      | 92/285 [03:12<07:08,  2.22s/it]predicting train subjects:  33%|███▎      | 93/285 [03:14<07:05,  2.22s/it]predicting train subjects:  33%|███▎      | 94/285 [03:17<07:07,  2.24s/it]predicting train subjects:  33%|███▎      | 95/285 [03:19<07:05,  2.24s/it]predicting train subjects:  34%|███▎      | 96/285 [03:21<07:05,  2.25s/it]predicting train subjects:  34%|███▍      | 97/285 [03:23<06:57,  2.22s/it]predicting train subjects:  34%|███▍      | 98/285 [03:25<06:52,  2.20s/it]predicting train subjects:  35%|███▍      | 99/285 [03:28<06:49,  2.20s/it]predicting train subjects:  35%|███▌      | 100/285 [03:30<06:50,  2.22s/it]predicting train subjects:  35%|███▌      | 101/285 [03:32<06:38,  2.17s/it]predicting train subjects:  36%|███▌      | 102/285 [03:34<06:45,  2.21s/it]predicting train subjects:  36%|███▌      | 103/285 [03:37<06:43,  2.21s/it]predicting train subjects:  36%|███▋      | 104/285 [03:39<06:37,  2.20s/it]predicting train subjects:  37%|███▋      | 105/285 [03:41<06:37,  2.21s/it]predicting train subjects:  37%|███▋      | 106/285 [03:43<06:30,  2.18s/it]predicting train subjects:  38%|███▊      | 107/285 [03:45<06:29,  2.19s/it]predicting train subjects:  38%|███▊      | 108/285 [03:47<06:20,  2.15s/it]predicting train subjects:  38%|███▊      | 109/285 [03:50<06:24,  2.18s/it]predicting train subjects:  39%|███▊      | 110/285 [03:52<06:17,  2.16s/it]predicting train subjects:  39%|███▉      | 111/285 [03:54<06:16,  2.16s/it]predicting train subjects:  39%|███▉      | 112/285 [03:56<06:05,  2.11s/it]predicting train subjects:  40%|███▉      | 113/285 [03:58<06:14,  2.17s/it]predicting train subjects:  40%|████      | 114/285 [04:00<06:07,  2.15s/it]predicting train subjects:  40%|████      | 115/285 [04:02<06:08,  2.17s/it]predicting train subjects:  41%|████      | 116/285 [04:05<06:01,  2.14s/it]predicting train subjects:  41%|████      | 117/285 [04:07<05:58,  2.13s/it]predicting train subjects:  41%|████▏     | 118/285 [04:09<06:00,  2.16s/it]predicting train subjects:  42%|████▏     | 119/285 [04:11<05:59,  2.16s/it]predicting train subjects:  42%|████▏     | 120/285 [04:13<05:54,  2.15s/it]predicting train subjects:  42%|████▏     | 121/285 [04:15<05:38,  2.07s/it]predicting train subjects:  43%|████▎     | 122/285 [04:17<05:20,  1.97s/it]predicting train subjects:  43%|████▎     | 123/285 [04:18<05:01,  1.86s/it]predicting train subjects:  44%|████▎     | 124/285 [04:20<05:03,  1.88s/it]predicting train subjects:  44%|████▍     | 125/285 [04:22<05:07,  1.92s/it]predicting train subjects:  44%|████▍     | 126/285 [04:24<05:05,  1.92s/it]predicting train subjects:  45%|████▍     | 127/285 [04:26<05:02,  1.91s/it]predicting train subjects:  45%|████▍     | 128/285 [04:28<05:06,  1.95s/it]predicting train subjects:  45%|████▌     | 129/285 [04:30<05:02,  1.94s/it]predicting train subjects:  46%|████▌     | 130/285 [04:32<05:00,  1.94s/it]predicting train subjects:  46%|████▌     | 131/285 [04:34<04:50,  1.89s/it]predicting train subjects:  46%|████▋     | 132/285 [04:36<05:19,  2.09s/it]predicting train subjects:  47%|████▋     | 133/285 [04:38<05:08,  2.03s/it]predicting train subjects:  47%|████▋     | 134/285 [04:40<05:00,  1.99s/it]predicting train subjects:  47%|████▋     | 135/285 [04:42<05:00,  2.01s/it]predicting train subjects:  48%|████▊     | 136/285 [04:44<04:56,  1.99s/it]predicting train subjects:  48%|████▊     | 137/285 [04:46<04:55,  2.00s/it]predicting train subjects:  48%|████▊     | 138/285 [04:48<04:54,  2.00s/it]predicting train subjects:  49%|████▉     | 139/285 [04:50<04:43,  1.94s/it]predicting train subjects:  49%|████▉     | 140/285 [04:52<04:46,  1.97s/it]predicting train subjects:  49%|████▉     | 141/285 [04:54<04:42,  1.97s/it]predicting train subjects:  50%|████▉     | 142/285 [04:56<04:33,  1.92s/it]predicting train subjects:  50%|█████     | 143/285 [04:57<04:21,  1.84s/it]predicting train subjects:  51%|█████     | 144/285 [05:00<04:30,  1.92s/it]predicting train subjects:  51%|█████     | 145/285 [05:01<04:15,  1.83s/it]predicting train subjects:  51%|█████     | 146/285 [05:03<03:57,  1.71s/it]predicting train subjects:  52%|█████▏    | 147/285 [05:04<03:44,  1.63s/it]predicting train subjects:  52%|█████▏    | 148/285 [05:06<03:45,  1.65s/it]predicting train subjects:  52%|█████▏    | 149/285 [05:07<03:42,  1.63s/it]predicting train subjects:  53%|█████▎    | 150/285 [05:09<03:47,  1.68s/it]predicting train subjects:  53%|█████▎    | 151/285 [05:11<03:46,  1.69s/it]predicting train subjects:  53%|█████▎    | 152/285 [05:13<03:50,  1.73s/it]predicting train subjects:  54%|█████▎    | 153/285 [05:14<03:41,  1.68s/it]predicting train subjects:  54%|█████▍    | 154/285 [05:16<03:39,  1.67s/it]predicting train subjects:  54%|█████▍    | 155/285 [05:18<03:39,  1.69s/it]predicting train subjects:  55%|█████▍    | 156/285 [05:19<03:37,  1.69s/it]predicting train subjects:  55%|█████▌    | 157/285 [05:21<03:35,  1.68s/it]predicting train subjects:  55%|█████▌    | 158/285 [05:22<03:22,  1.60s/it]predicting train subjects:  56%|█████▌    | 159/285 [05:24<03:15,  1.55s/it]predicting train subjects:  56%|█████▌    | 160/285 [05:25<03:09,  1.52s/it]predicting train subjects:  56%|█████▋    | 161/285 [05:27<03:03,  1.48s/it]predicting train subjects:  57%|█████▋    | 162/285 [05:28<02:59,  1.46s/it]predicting train subjects:  57%|█████▋    | 163/285 [05:30<03:00,  1.48s/it]predicting train subjects:  58%|█████▊    | 164/285 [05:31<02:57,  1.47s/it]predicting train subjects:  58%|█████▊    | 165/285 [05:32<02:54,  1.45s/it]predicting train subjects:  58%|█████▊    | 166/285 [05:34<02:53,  1.46s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:35<02:51,  1.45s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:37<02:49,  1.45s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:38<02:49,  1.46s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:40<02:48,  1.46s/it]predicting train subjects:  60%|██████    | 171/285 [05:41<02:46,  1.46s/it]predicting train subjects:  60%|██████    | 172/285 [05:43<02:42,  1.44s/it]predicting train subjects:  61%|██████    | 173/285 [05:44<02:41,  1.44s/it]predicting train subjects:  61%|██████    | 174/285 [05:45<02:39,  1.43s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:47<02:37,  1.43s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:48<02:35,  1.42s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:50<02:32,  1.41s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:51<02:30,  1.40s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:52<02:27,  1.39s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:54<02:22,  1.36s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:55<02:20,  1.35s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:56<02:17,  1.33s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:58<02:16,  1.33s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:59<02:16,  1.35s/it]predicting train subjects:  65%|██████▍   | 185/285 [06:00<02:14,  1.35s/it]predicting train subjects:  65%|██████▌   | 186/285 [06:02<02:18,  1.39s/it]predicting train subjects:  66%|██████▌   | 187/285 [06:03<02:16,  1.40s/it]predicting train subjects:  66%|██████▌   | 188/285 [06:05<02:15,  1.40s/it]predicting train subjects:  66%|██████▋   | 189/285 [06:06<02:15,  1.42s/it]predicting train subjects:  67%|██████▋   | 190/285 [06:07<02:13,  1.40s/it]predicting train subjects:  67%|██████▋   | 191/285 [06:09<02:10,  1.38s/it]predicting train subjects:  67%|██████▋   | 192/285 [06:10<02:09,  1.39s/it]predicting train subjects:  68%|██████▊   | 193/285 [06:12<02:08,  1.39s/it]predicting train subjects:  68%|██████▊   | 194/285 [06:13<02:05,  1.38s/it]predicting train subjects:  68%|██████▊   | 195/285 [06:14<02:04,  1.38s/it]predicting train subjects:  69%|██████▉   | 196/285 [06:16<02:10,  1.47s/it]predicting train subjects:  69%|██████▉   | 197/285 [06:18<02:16,  1.55s/it]predicting train subjects:  69%|██████▉   | 198/285 [06:19<02:19,  1.60s/it]predicting train subjects:  70%|██████▉   | 199/285 [06:21<02:20,  1.63s/it]predicting train subjects:  70%|███████   | 200/285 [06:23<02:19,  1.65s/it]predicting train subjects:  71%|███████   | 201/285 [06:24<02:18,  1.64s/it]predicting train subjects:  71%|███████   | 202/285 [06:26<02:17,  1.65s/it]predicting train subjects:  71%|███████   | 203/285 [06:28<02:15,  1.65s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:29<02:13,  1.64s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:31<02:10,  1.63s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:33<02:09,  1.64s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:34<02:09,  1.66s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:36<02:07,  1.66s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:38<02:04,  1.64s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:39<02:03,  1.65s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:41<02:01,  1.64s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:43<02:00,  1.65s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:44<01:58,  1.65s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:46<01:54,  1.61s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:47<01:50,  1.57s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:49<01:46,  1.54s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:50<01:44,  1.54s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:52<01:43,  1.54s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:53<01:40,  1.52s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:55<01:38,  1.51s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:56<01:36,  1.50s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:58<01:34,  1.50s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:59<01:32,  1.49s/it]predicting train subjects:  79%|███████▊  | 224/285 [07:01<01:31,  1.49s/it]predicting train subjects:  79%|███████▉  | 225/285 [07:02<01:29,  1.49s/it]predicting train subjects:  79%|███████▉  | 226/285 [07:04<01:27,  1.48s/it]predicting train subjects:  80%|███████▉  | 227/285 [07:05<01:26,  1.49s/it]predicting train subjects:  80%|████████  | 228/285 [07:07<01:24,  1.49s/it]predicting train subjects:  80%|████████  | 229/285 [07:08<01:23,  1.49s/it]predicting train subjects:  81%|████████  | 230/285 [07:10<01:21,  1.48s/it]predicting train subjects:  81%|████████  | 231/285 [07:11<01:20,  1.49s/it]predicting train subjects:  81%|████████▏ | 232/285 [07:13<01:24,  1.60s/it]predicting train subjects:  82%|████████▏ | 233/285 [07:15<01:28,  1.70s/it]predicting train subjects:  82%|████████▏ | 234/285 [07:17<01:29,  1.75s/it]predicting train subjects:  82%|████████▏ | 235/285 [07:19<01:30,  1.81s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:21<01:29,  1.83s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:23<01:29,  1.86s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:24<01:27,  1.86s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:26<01:26,  1.87s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:28<01:24,  1.87s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:30<01:22,  1.87s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:32<01:20,  1.88s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:34<01:19,  1.88s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:36<01:17,  1.88s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:38<01:15,  1.88s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:39<01:13,  1.88s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:41<01:10,  1.86s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:43<01:08,  1.85s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:45<01:05,  1.81s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:46<00:58,  1.68s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:48<00:55,  1.62s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:49<00:51,  1.55s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:50<00:48,  1.52s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:52<00:46,  1.49s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:53<00:43,  1.46s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:55<00:41,  1.45s/it]predicting train subjects:  90%|█████████ | 257/285 [07:56<00:40,  1.44s/it]predicting train subjects:  91%|█████████ | 258/285 [07:58<00:39,  1.47s/it]predicting train subjects:  91%|█████████ | 259/285 [07:59<00:37,  1.44s/it]predicting train subjects:  91%|█████████ | 260/285 [08:01<00:36,  1.46s/it]predicting train subjects:  92%|█████████▏| 261/285 [08:02<00:34,  1.43s/it]predicting train subjects:  92%|█████████▏| 262/285 [08:03<00:32,  1.40s/it]predicting train subjects:  92%|█████████▏| 263/285 [08:05<00:30,  1.39s/it]predicting train subjects:  93%|█████████▎| 264/285 [08:06<00:29,  1.40s/it]predicting train subjects:  93%|█████████▎| 265/285 [08:07<00:27,  1.39s/it]predicting train subjects:  93%|█████████▎| 266/285 [08:09<00:26,  1.38s/it]predicting train subjects:  94%|█████████▎| 267/285 [08:10<00:24,  1.38s/it]predicting train subjects:  94%|█████████▍| 268/285 [08:12<00:26,  1.53s/it]predicting train subjects:  94%|█████████▍| 269/285 [08:14<00:26,  1.65s/it]predicting train subjects:  95%|█████████▍| 270/285 [08:16<00:25,  1.72s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:18<00:24,  1.77s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:20<00:23,  1.81s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:22<00:21,  1.83s/it]predicting train subjects:  96%|█████████▌| 274/285 [08:23<00:20,  1.85s/it]predicting train subjects:  96%|█████████▋| 275/285 [08:25<00:18,  1.86s/it]predicting train subjects:  97%|█████████▋| 276/285 [08:27<00:16,  1.88s/it]predicting train subjects:  97%|█████████▋| 277/285 [08:29<00:15,  1.91s/it]predicting train subjects:  98%|█████████▊| 278/285 [08:31<00:13,  1.90s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:33<00:11,  1.90s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:35<00:09,  1.90s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:37<00:07,  1.90s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:39<00:05,  1.91s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:41<00:03,  1.91s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:43<00:01,  1.92s/it]predicting train subjects: 100%|██████████| 285/285 [08:44<00:00,  1.89s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:01,  1.27s/it]Loading train:   1%|          | 2/285 [00:02<06:31,  1.38s/it]Loading train:   1%|          | 3/285 [00:04<06:19,  1.35s/it]Loading train:   1%|▏         | 4/285 [00:05<06:48,  1.45s/it]Loading train:   2%|▏         | 5/285 [00:07<06:36,  1.42s/it]Loading train:   2%|▏         | 6/285 [00:09<07:19,  1.58s/it]Loading train:   2%|▏         | 7/285 [00:10<07:37,  1.65s/it]Loading train:   3%|▎         | 8/285 [00:12<07:41,  1.66s/it]Loading train:   3%|▎         | 9/285 [00:14<07:18,  1.59s/it]Loading train:   4%|▎         | 10/285 [00:15<07:08,  1.56s/it]Loading train:   4%|▍         | 11/285 [00:16<06:53,  1.51s/it]Loading train:   4%|▍         | 12/285 [00:18<06:30,  1.43s/it]Loading train:   5%|▍         | 13/285 [00:19<06:13,  1.37s/it]Loading train:   5%|▍         | 14/285 [00:20<06:00,  1.33s/it]Loading train:   5%|▌         | 15/285 [00:21<05:52,  1.30s/it]Loading train:   6%|▌         | 16/285 [00:23<05:54,  1.32s/it]Loading train:   6%|▌         | 17/285 [00:24<05:59,  1.34s/it]Loading train:   6%|▋         | 18/285 [00:25<05:42,  1.28s/it]Loading train:   7%|▋         | 19/285 [00:27<05:37,  1.27s/it]Loading train:   7%|▋         | 20/285 [00:28<05:42,  1.29s/it]Loading train:   7%|▋         | 21/285 [00:29<05:43,  1.30s/it]Loading train:   8%|▊         | 22/285 [00:30<05:35,  1.28s/it]Loading train:   8%|▊         | 23/285 [00:32<05:34,  1.27s/it]Loading train:   8%|▊         | 24/285 [00:33<05:29,  1.26s/it]Loading train:   9%|▉         | 25/285 [00:34<05:25,  1.25s/it]Loading train:   9%|▉         | 26/285 [00:35<05:18,  1.23s/it]Loading train:   9%|▉         | 27/285 [00:37<05:21,  1.25s/it]Loading train:  10%|▉         | 28/285 [00:38<05:11,  1.21s/it]Loading train:  10%|█         | 29/285 [00:39<04:59,  1.17s/it]Loading train:  11%|█         | 30/285 [00:40<04:53,  1.15s/it]Loading train:  11%|█         | 31/285 [00:41<04:45,  1.12s/it]Loading train:  11%|█         | 32/285 [00:42<04:36,  1.09s/it]Loading train:  12%|█▏        | 33/285 [00:43<04:39,  1.11s/it]Loading train:  12%|█▏        | 34/285 [00:44<04:32,  1.09s/it]Loading train:  12%|█▏        | 35/285 [00:45<04:35,  1.10s/it]Loading train:  13%|█▎        | 36/285 [00:46<04:28,  1.08s/it]Loading train:  13%|█▎        | 37/285 [00:47<04:21,  1.06s/it]Loading train:  13%|█▎        | 38/285 [00:48<04:15,  1.03s/it]Loading train:  14%|█▎        | 39/285 [00:49<04:10,  1.02s/it]Loading train:  14%|█▍        | 40/285 [00:50<04:04,  1.00it/s]Loading train:  14%|█▍        | 41/285 [00:51<03:59,  1.02it/s]Loading train:  15%|█▍        | 42/285 [00:52<03:55,  1.03it/s]Loading train:  15%|█▌        | 43/285 [00:53<03:54,  1.03it/s]Loading train:  15%|█▌        | 44/285 [00:54<03:51,  1.04it/s]Loading train:  16%|█▌        | 45/285 [00:55<03:54,  1.02it/s]Loading train:  16%|█▌        | 46/285 [00:56<03:55,  1.01it/s]Loading train:  16%|█▋        | 47/285 [00:57<03:53,  1.02it/s]Loading train:  17%|█▋        | 48/285 [00:58<03:45,  1.05it/s]Loading train:  17%|█▋        | 49/285 [00:59<03:39,  1.08it/s]Loading train:  18%|█▊        | 50/285 [01:00<03:42,  1.06it/s]Loading train:  18%|█▊        | 51/285 [01:01<03:41,  1.06it/s]Loading train:  18%|█▊        | 52/285 [01:02<03:32,  1.09it/s]Loading train:  19%|█▊        | 53/285 [01:02<03:29,  1.11it/s]Loading train:  19%|█▉        | 54/285 [01:03<03:28,  1.11it/s]Loading train:  19%|█▉        | 55/285 [01:04<03:32,  1.08it/s]Loading train:  20%|█▉        | 56/285 [01:05<03:39,  1.04it/s]Loading train:  20%|██        | 57/285 [01:06<03:35,  1.06it/s]Loading train:  20%|██        | 58/285 [01:07<03:32,  1.07it/s]Loading train:  21%|██        | 59/285 [01:08<03:34,  1.05it/s]Loading train:  21%|██        | 60/285 [01:09<03:32,  1.06it/s]Loading train:  21%|██▏       | 61/285 [01:10<03:32,  1.05it/s]Loading train:  22%|██▏       | 62/285 [01:11<03:30,  1.06it/s]Loading train:  22%|██▏       | 63/285 [01:12<03:28,  1.07it/s]Loading train:  22%|██▏       | 64/285 [01:14<04:11,  1.14s/it]Loading train:  23%|██▎       | 65/285 [01:15<04:41,  1.28s/it]Loading train:  23%|██▎       | 66/285 [01:17<04:49,  1.32s/it]Loading train:  24%|██▎       | 67/285 [01:18<04:33,  1.26s/it]Loading train:  24%|██▍       | 68/285 [01:19<04:18,  1.19s/it]Loading train:  24%|██▍       | 69/285 [01:20<04:06,  1.14s/it]Loading train:  25%|██▍       | 70/285 [01:21<03:56,  1.10s/it]Loading train:  25%|██▍       | 71/285 [01:22<03:48,  1.07s/it]Loading train:  25%|██▌       | 72/285 [01:23<03:45,  1.06s/it]Loading train:  26%|██▌       | 73/285 [01:24<03:42,  1.05s/it]Loading train:  26%|██▌       | 74/285 [01:25<03:36,  1.03s/it]Loading train:  26%|██▋       | 75/285 [01:26<03:30,  1.00s/it]Loading train:  27%|██▋       | 76/285 [01:27<03:27,  1.01it/s]Loading train:  27%|██▋       | 77/285 [01:28<03:25,  1.01it/s]Loading train:  27%|██▋       | 78/285 [01:29<03:23,  1.01it/s]Loading train:  28%|██▊       | 79/285 [01:30<03:26,  1.00s/it]Loading train:  28%|██▊       | 80/285 [01:31<03:29,  1.02s/it]Loading train:  28%|██▊       | 81/285 [01:32<03:28,  1.02s/it]Loading train:  29%|██▉       | 82/285 [01:33<03:30,  1.04s/it]Loading train:  29%|██▉       | 83/285 [01:34<03:28,  1.03s/it]Loading train:  29%|██▉       | 84/285 [01:35<03:30,  1.04s/it]Loading train:  30%|██▉       | 85/285 [01:36<03:41,  1.11s/it]Loading train:  30%|███       | 86/285 [01:37<03:37,  1.09s/it]Loading train:  31%|███       | 87/285 [01:38<03:35,  1.09s/it]Loading train:  31%|███       | 88/285 [01:39<03:34,  1.09s/it]Loading train:  31%|███       | 89/285 [01:40<03:30,  1.08s/it]Loading train:  32%|███▏      | 90/285 [01:42<03:28,  1.07s/it]Loading train:  32%|███▏      | 91/285 [01:43<03:29,  1.08s/it]Loading train:  32%|███▏      | 92/285 [01:44<03:29,  1.08s/it]Loading train:  33%|███▎      | 93/285 [01:45<03:30,  1.10s/it]Loading train:  33%|███▎      | 94/285 [01:46<03:29,  1.10s/it]Loading train:  33%|███▎      | 95/285 [01:47<03:31,  1.11s/it]Loading train:  34%|███▎      | 96/285 [01:48<03:27,  1.10s/it]Loading train:  34%|███▍      | 97/285 [01:49<03:27,  1.10s/it]Loading train:  34%|███▍      | 98/285 [01:50<03:23,  1.09s/it]Loading train:  35%|███▍      | 99/285 [01:51<03:24,  1.10s/it]Loading train:  35%|███▌      | 100/285 [01:53<03:20,  1.09s/it]Loading train:  35%|███▌      | 101/285 [01:54<03:19,  1.08s/it]Loading train:  36%|███▌      | 102/285 [01:55<03:18,  1.08s/it]Loading train:  36%|███▌      | 103/285 [01:56<03:19,  1.10s/it]Loading train:  36%|███▋      | 104/285 [01:57<03:20,  1.11s/it]Loading train:  37%|███▋      | 105/285 [01:58<03:17,  1.10s/it]Loading train:  37%|███▋      | 106/285 [01:59<03:14,  1.08s/it]Loading train:  38%|███▊      | 107/285 [02:00<03:12,  1.08s/it]Loading train:  38%|███▊      | 108/285 [02:01<03:14,  1.10s/it]Loading train:  38%|███▊      | 109/285 [02:02<03:17,  1.12s/it]Loading train:  39%|███▊      | 110/285 [02:03<03:11,  1.10s/it]Loading train:  39%|███▉      | 111/285 [02:05<03:09,  1.09s/it]Loading train:  39%|███▉      | 112/285 [02:06<03:05,  1.07s/it]Loading train:  40%|███▉      | 113/285 [02:07<03:03,  1.07s/it]Loading train:  40%|████      | 114/285 [02:08<03:00,  1.06s/it]Loading train:  40%|████      | 115/285 [02:09<03:03,  1.08s/it]Loading train:  41%|████      | 116/285 [02:10<03:01,  1.08s/it]Loading train:  41%|████      | 117/285 [02:11<03:00,  1.08s/it]Loading train:  41%|████▏     | 118/285 [02:12<02:58,  1.07s/it]Loading train:  42%|████▏     | 119/285 [02:13<02:55,  1.05s/it]Loading train:  42%|████▏     | 120/285 [02:14<02:53,  1.05s/it]Loading train:  42%|████▏     | 121/285 [02:16<03:11,  1.17s/it]Loading train:  43%|████▎     | 122/285 [02:17<03:15,  1.20s/it]Loading train:  43%|████▎     | 123/285 [02:18<03:20,  1.24s/it]Loading train:  44%|████▎     | 124/285 [02:19<03:06,  1.16s/it]Loading train:  44%|████▍     | 125/285 [02:20<02:52,  1.08s/it]Loading train:  44%|████▍     | 126/285 [02:21<02:43,  1.03s/it]Loading train:  45%|████▍     | 127/285 [02:22<02:37,  1.00it/s]Loading train:  45%|████▍     | 128/285 [02:23<02:30,  1.04it/s]Loading train:  45%|████▌     | 129/285 [02:24<02:29,  1.04it/s]Loading train:  46%|████▌     | 130/285 [02:25<02:26,  1.06it/s]Loading train:  46%|████▌     | 131/285 [02:26<02:28,  1.03it/s]Loading train:  46%|████▋     | 132/285 [02:27<02:25,  1.05it/s]Loading train:  47%|████▋     | 133/285 [02:27<02:22,  1.07it/s]Loading train:  47%|████▋     | 134/285 [02:28<02:20,  1.08it/s]Loading train:  47%|████▋     | 135/285 [02:29<02:20,  1.07it/s]Loading train:  48%|████▊     | 136/285 [02:30<02:18,  1.08it/s]Loading train:  48%|████▊     | 137/285 [02:31<02:16,  1.08it/s]Loading train:  48%|████▊     | 138/285 [02:32<02:17,  1.07it/s]Loading train:  49%|████▉     | 139/285 [02:33<02:17,  1.06it/s]Loading train:  49%|████▉     | 140/285 [02:34<02:15,  1.07it/s]Loading train:  49%|████▉     | 141/285 [02:35<02:15,  1.06it/s]Loading train:  50%|████▉     | 142/285 [02:36<02:13,  1.08it/s]Loading train:  50%|█████     | 143/285 [02:37<02:11,  1.08it/s]Loading train:  51%|█████     | 144/285 [02:38<02:11,  1.07it/s]Loading train:  51%|█████     | 145/285 [02:39<02:09,  1.08it/s]Loading train:  51%|█████     | 146/285 [02:40<02:09,  1.07it/s]Loading train:  52%|█████▏    | 147/285 [02:40<02:07,  1.08it/s]Loading train:  52%|█████▏    | 148/285 [02:41<02:07,  1.08it/s]Loading train:  52%|█████▏    | 149/285 [02:42<02:04,  1.10it/s]Loading train:  53%|█████▎    | 150/285 [02:43<02:02,  1.10it/s]Loading train:  53%|█████▎    | 151/285 [02:44<02:01,  1.10it/s]Loading train:  53%|█████▎    | 152/285 [02:45<01:58,  1.13it/s]Loading train:  54%|█████▎    | 153/285 [02:46<01:58,  1.11it/s]Loading train:  54%|█████▍    | 154/285 [02:47<01:59,  1.10it/s]Loading train:  54%|█████▍    | 155/285 [02:48<01:58,  1.10it/s]Loading train:  55%|█████▍    | 156/285 [02:49<01:57,  1.10it/s]Loading train:  55%|█████▌    | 157/285 [02:49<01:55,  1.11it/s]Loading train:  55%|█████▌    | 158/285 [02:50<01:55,  1.10it/s]Loading train:  56%|█████▌    | 159/285 [02:51<01:52,  1.12it/s]Loading train:  56%|█████▌    | 160/285 [02:52<01:49,  1.14it/s]Loading train:  56%|█████▋    | 161/285 [02:53<01:47,  1.15it/s]Loading train:  57%|█████▋    | 162/285 [02:54<01:49,  1.12it/s]Loading train:  57%|█████▋    | 163/285 [02:55<01:49,  1.11it/s]Loading train:  58%|█████▊    | 164/285 [02:56<01:47,  1.13it/s]Loading train:  58%|█████▊    | 165/285 [02:57<01:46,  1.13it/s]Loading train:  58%|█████▊    | 166/285 [02:57<01:46,  1.12it/s]Loading train:  59%|█████▊    | 167/285 [02:58<01:44,  1.13it/s]Loading train:  59%|█████▉    | 168/285 [02:59<01:42,  1.14it/s]Loading train:  59%|█████▉    | 169/285 [03:00<01:41,  1.14it/s]Loading train:  60%|█████▉    | 170/285 [03:01<01:39,  1.16it/s]Loading train:  60%|██████    | 171/285 [03:02<01:37,  1.16it/s]Loading train:  60%|██████    | 172/285 [03:03<01:38,  1.14it/s]Loading train:  61%|██████    | 173/285 [03:04<01:37,  1.15it/s]Loading train:  61%|██████    | 174/285 [03:04<01:38,  1.13it/s]Loading train:  61%|██████▏   | 175/285 [03:05<01:36,  1.14it/s]Loading train:  62%|██████▏   | 176/285 [03:06<01:36,  1.13it/s]Loading train:  62%|██████▏   | 177/285 [03:07<01:34,  1.15it/s]Loading train:  62%|██████▏   | 178/285 [03:08<01:36,  1.11it/s]Loading train:  63%|██████▎   | 179/285 [03:09<01:33,  1.13it/s]Loading train:  63%|██████▎   | 180/285 [03:10<01:29,  1.17it/s]Loading train:  64%|██████▎   | 181/285 [03:10<01:28,  1.17it/s]Loading train:  64%|██████▍   | 182/285 [03:11<01:28,  1.17it/s]Loading train:  64%|██████▍   | 183/285 [03:12<01:28,  1.15it/s]Loading train:  65%|██████▍   | 184/285 [03:13<01:27,  1.15it/s]Loading train:  65%|██████▍   | 185/285 [03:14<01:25,  1.17it/s]Loading train:  65%|██████▌   | 186/285 [03:15<01:24,  1.17it/s]Loading train:  66%|██████▌   | 187/285 [03:16<01:21,  1.20it/s]Loading train:  66%|██████▌   | 188/285 [03:16<01:22,  1.17it/s]Loading train:  66%|██████▋   | 189/285 [03:17<01:24,  1.13it/s]Loading train:  67%|██████▋   | 190/285 [03:18<01:25,  1.11it/s]Loading train:  67%|██████▋   | 191/285 [03:19<01:23,  1.13it/s]Loading train:  67%|██████▋   | 192/285 [03:20<01:22,  1.12it/s]Loading train:  68%|██████▊   | 193/285 [03:21<01:25,  1.08it/s]Loading train:  68%|██████▊   | 194/285 [03:22<01:23,  1.09it/s]Loading train:  68%|██████▊   | 195/285 [03:23<01:21,  1.10it/s]Loading train:  69%|██████▉   | 196/285 [03:24<01:21,  1.09it/s]Loading train:  69%|██████▉   | 197/285 [03:25<01:24,  1.04it/s]Loading train:  69%|██████▉   | 198/285 [03:26<01:22,  1.06it/s]Loading train:  70%|██████▉   | 199/285 [03:27<01:19,  1.08it/s]Loading train:  70%|███████   | 200/285 [03:28<01:19,  1.07it/s]Loading train:  71%|███████   | 201/285 [03:29<01:19,  1.05it/s]Loading train:  71%|███████   | 202/285 [03:30<01:19,  1.04it/s]Loading train:  71%|███████   | 203/285 [03:31<01:20,  1.02it/s]Loading train:  72%|███████▏  | 204/285 [03:32<01:20,  1.01it/s]Loading train:  72%|███████▏  | 205/285 [03:33<01:18,  1.02it/s]Loading train:  72%|███████▏  | 206/285 [03:34<01:16,  1.03it/s]Loading train:  73%|███████▎  | 207/285 [03:35<01:15,  1.04it/s]Loading train:  73%|███████▎  | 208/285 [03:36<01:16,  1.01it/s]Loading train:  73%|███████▎  | 209/285 [03:37<01:15,  1.01it/s]Loading train:  74%|███████▎  | 210/285 [03:37<01:12,  1.03it/s]Loading train:  74%|███████▍  | 211/285 [03:38<01:11,  1.03it/s]Loading train:  74%|███████▍  | 212/285 [03:39<01:11,  1.02it/s]Loading train:  75%|███████▍  | 213/285 [03:40<01:08,  1.06it/s]Loading train:  75%|███████▌  | 214/285 [03:41<01:06,  1.07it/s]Loading train:  75%|███████▌  | 215/285 [03:42<01:04,  1.09it/s]Loading train:  76%|███████▌  | 216/285 [03:43<01:03,  1.09it/s]Loading train:  76%|███████▌  | 217/285 [03:44<01:00,  1.12it/s]Loading train:  76%|███████▋  | 218/285 [03:45<01:00,  1.12it/s]Loading train:  77%|███████▋  | 219/285 [03:46<00:58,  1.12it/s]Loading train:  77%|███████▋  | 220/285 [03:46<00:56,  1.16it/s]Loading train:  78%|███████▊  | 221/285 [03:47<00:55,  1.15it/s]Loading train:  78%|███████▊  | 222/285 [03:48<00:54,  1.15it/s]Loading train:  78%|███████▊  | 223/285 [03:49<00:54,  1.13it/s]Loading train:  79%|███████▊  | 224/285 [03:50<00:53,  1.14it/s]Loading train:  79%|███████▉  | 225/285 [03:51<00:52,  1.13it/s]Loading train:  79%|███████▉  | 226/285 [03:52<00:51,  1.15it/s]Loading train:  80%|███████▉  | 227/285 [03:53<00:50,  1.15it/s]Loading train:  80%|████████  | 228/285 [03:53<00:49,  1.16it/s]Loading train:  80%|████████  | 229/285 [03:54<00:47,  1.17it/s]Loading train:  81%|████████  | 230/285 [03:55<00:47,  1.16it/s]Loading train:  81%|████████  | 231/285 [03:56<00:45,  1.18it/s]Loading train:  81%|████████▏ | 232/285 [03:57<00:48,  1.10it/s]Loading train:  82%|████████▏ | 233/285 [03:58<00:47,  1.10it/s]Loading train:  82%|████████▏ | 234/285 [03:59<00:47,  1.07it/s]Loading train:  82%|████████▏ | 235/285 [04:00<00:47,  1.04it/s]Loading train:  83%|████████▎ | 236/285 [04:01<00:48,  1.01it/s]Loading train:  83%|████████▎ | 237/285 [04:02<00:48,  1.01s/it]Loading train:  84%|████████▎ | 238/285 [04:03<00:47,  1.02s/it]Loading train:  84%|████████▍ | 239/285 [04:04<00:47,  1.02s/it]Loading train:  84%|████████▍ | 240/285 [04:05<00:46,  1.03s/it]Loading train:  85%|████████▍ | 241/285 [04:06<00:45,  1.04s/it]Loading train:  85%|████████▍ | 242/285 [04:07<00:44,  1.04s/it]Loading train:  85%|████████▌ | 243/285 [04:08<00:43,  1.03s/it]Loading train:  86%|████████▌ | 244/285 [04:09<00:42,  1.04s/it]Loading train:  86%|████████▌ | 245/285 [04:10<00:41,  1.03s/it]Loading train:  86%|████████▋ | 246/285 [04:11<00:40,  1.03s/it]Loading train:  87%|████████▋ | 247/285 [04:13<00:41,  1.08s/it]Loading train:  87%|████████▋ | 248/285 [04:14<00:39,  1.07s/it]Loading train:  87%|████████▋ | 249/285 [04:15<00:37,  1.05s/it]Loading train:  88%|████████▊ | 250/285 [04:16<00:35,  1.02s/it]Loading train:  88%|████████▊ | 251/285 [04:17<00:33,  1.00it/s]Loading train:  88%|████████▊ | 252/285 [04:17<00:32,  1.03it/s]Loading train:  89%|████████▉ | 253/285 [04:18<00:30,  1.05it/s]Loading train:  89%|████████▉ | 254/285 [04:19<00:29,  1.04it/s]Loading train:  89%|████████▉ | 255/285 [04:20<00:28,  1.05it/s]Loading train:  90%|████████▉ | 256/285 [04:21<00:26,  1.07it/s]Loading train:  90%|█████████ | 257/285 [04:22<00:25,  1.10it/s]Loading train:  91%|█████████ | 258/285 [04:23<00:23,  1.13it/s]Loading train:  91%|█████████ | 259/285 [04:24<00:23,  1.12it/s]Loading train:  91%|█████████ | 260/285 [04:25<00:23,  1.08it/s]Loading train:  92%|█████████▏| 261/285 [04:26<00:27,  1.16s/it]Loading train:  92%|█████████▏| 262/285 [04:27<00:24,  1.08s/it]Loading train:  92%|█████████▏| 263/285 [04:28<00:22,  1.03s/it]Loading train:  93%|█████████▎| 264/285 [04:29<00:20,  1.00it/s]Loading train:  93%|█████████▎| 265/285 [04:30<00:20,  1.00s/it]Loading train:  93%|█████████▎| 266/285 [04:31<00:17,  1.06it/s]Loading train:  94%|█████████▎| 267/285 [04:32<00:16,  1.10it/s]Loading train:  94%|█████████▍| 268/285 [04:33<00:16,  1.01it/s]Loading train:  94%|█████████▍| 269/285 [04:34<00:16,  1.03s/it]Loading train:  95%|█████████▍| 270/285 [04:35<00:15,  1.05s/it]Loading train:  95%|█████████▌| 271/285 [04:36<00:14,  1.04s/it]Loading train:  95%|█████████▌| 272/285 [04:37<00:13,  1.06s/it]Loading train:  96%|█████████▌| 273/285 [04:38<00:12,  1.06s/it]Loading train:  96%|█████████▌| 274/285 [04:39<00:11,  1.07s/it]Loading train:  96%|█████████▋| 275/285 [04:41<00:10,  1.08s/it]Loading train:  97%|█████████▋| 276/285 [04:42<00:09,  1.07s/it]Loading train:  97%|█████████▋| 277/285 [04:43<00:08,  1.09s/it]Loading train:  98%|█████████▊| 278/285 [04:44<00:07,  1.10s/it]Loading train:  98%|█████████▊| 279/285 [04:45<00:06,  1.12s/it]Loading train:  98%|█████████▊| 280/285 [04:46<00:05,  1.13s/it]Loading train:  99%|█████████▊| 281/285 [04:47<00:04,  1.14s/it]Loading train:  99%|█████████▉| 282/285 [04:48<00:03,  1.11s/it]Loading train:  99%|█████████▉| 283/285 [04:50<00:02,  1.11s/it]Loading train: 100%|█████████▉| 284/285 [04:51<00:01,  1.12s/it]Loading train: 100%|██████████| 285/285 [04:52<00:00,  1.10s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   6%|▌         | 17/285 [00:00<00:01, 169.10it/s]concatenating: train:  14%|█▍        | 41/285 [00:00<00:01, 184.78it/s]concatenating: train:  23%|██▎       | 65/285 [00:00<00:01, 198.03it/s]concatenating: train:  31%|███       | 87/285 [00:00<00:00, 203.67it/s]concatenating: train:  40%|███▉      | 113/285 [00:00<00:00, 215.63it/s]concatenating: train:  48%|████▊     | 136/285 [00:00<00:00, 218.91it/s]concatenating: train:  57%|█████▋    | 162/285 [00:00<00:00, 228.99it/s]concatenating: train:  67%|██████▋   | 190/285 [00:00<00:00, 240.47it/s]concatenating: train:  75%|███████▌  | 214/285 [00:00<00:00, 237.50it/s]concatenating: train:  84%|████████▎ | 238/285 [00:01<00:00, 235.22it/s]concatenating: train:  93%|█████████▎| 264/285 [00:01<00:00, 237.56it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 237.90it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.51s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.44s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.39s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 44.34it/s]2019-07-06 20:12:25.165971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 20:12:25.166107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 20:12:25.166124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 20:12:25.166133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 20:12:25.166584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:08,  4.63it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:06,  5.50it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:06,  5.22it/s]loading the weights for Unet:  20%|██        | 8/40 [00:00<00:04,  6.64it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:05,  5.73it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:04,  6.39it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  5.57it/s]loading the weights for Unet:  40%|████      | 16/40 [00:01<00:03,  7.01it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:03,  5.91it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  6.37it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  5.48it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:02<00:02,  6.69it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:02<00:02,  7.30it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  5.88it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:01,  6.65it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:03<00:01,  5.68it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:03<00:01,  6.90it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  7.32it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.70it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:04<00:00,  6.15it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:04<00:00,  5.22it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:04<00:00,  8.21it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 52, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 52, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 52, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 52, 20)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 52, 20)   3620        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 52, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 52, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 52, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 52, 20)   3620        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 52, 20)   80          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 52, 20)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 52, 20)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 52, 20)   3620        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 52, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 52, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 52, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 52, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 52, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 26, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 26, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 26, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 26, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 26, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 26, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 26, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 13, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 13, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 13, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 13, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 13, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 13, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 13, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 13, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 26, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 26, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 26, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 26, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 26, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 26, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 26, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 26, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 52, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 52, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 52, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 52, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 52, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 52, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 52, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 52, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 52, 20)   3620        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 52, 20)   80          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 52, 20)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 52, 20)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 52, 13)   273         dropout_9[0][0]                  
==================================================================================================
Total params: 198,913
Trainable params: 55,273
Non-trainable params: 143,640
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.49841486e-02 3.19966680e-02 7.50970181e-02 9.33357939e-03
 2.71292049e-02 7.07427267e-03 8.46489586e-02 1.12779077e-01
 8.61338510e-02 1.32649165e-02 2.94521391e-01 1.92807035e-01
 2.29878984e-04]
Train on 18361 samples, validate on 179 samples
Epoch 1/300
 - 23s - loss: 136.2772 - acc: 0.5874 - mDice: 0.0131 - val_loss: 37.0412 - val_acc: 0.9136 - val_mDice: 0.0079

Epoch 00001: val_mDice improved from -inf to 0.00785, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 14s - loss: 34.1498 - acc: 0.8860 - mDice: 0.0139 - val_loss: 11.9643 - val_acc: 0.9136 - val_mDice: 0.0104

Epoch 00002: val_mDice improved from 0.00785 to 0.01045, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 14s - loss: 15.8989 - acc: 0.8863 - mDice: 0.0167 - val_loss: 7.1691 - val_acc: 0.9136 - val_mDice: 0.0150

Epoch 00003: val_mDice improved from 0.01045 to 0.01496, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 14s - loss: 10.6852 - acc: 0.8863 - mDice: 0.0206 - val_loss: 5.5937 - val_acc: 0.9136 - val_mDice: 0.0219

Epoch 00004: val_mDice improved from 0.01496 to 0.02189, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 14s - loss: 8.3951 - acc: 0.8863 - mDice: 0.0257 - val_loss: 4.8521 - val_acc: 0.9136 - val_mDice: 0.0287

Epoch 00005: val_mDice improved from 0.02189 to 0.02870, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 14s - loss: 7.0745 - acc: 0.8863 - mDice: 0.0326 - val_loss: 4.3784 - val_acc: 0.9136 - val_mDice: 0.0391

Epoch 00006: val_mDice improved from 0.02870 to 0.03910, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 14s - loss: 6.1634 - acc: 0.8863 - mDice: 0.0425 - val_loss: 4.1284 - val_acc: 0.9136 - val_mDice: 0.0560

Epoch 00007: val_mDice improved from 0.03910 to 0.05599, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 14s - loss: 5.4788 - acc: 0.8863 - mDice: 0.0559 - val_loss: 3.8394 - val_acc: 0.9136 - val_mDice: 0.0743

Epoch 00008: val_mDice improved from 0.05599 to 0.07434, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 14s - loss: 4.9619 - acc: 0.8864 - mDice: 0.0712 - val_loss: 3.4703 - val_acc: 0.9136 - val_mDice: 0.0985

Epoch 00009: val_mDice improved from 0.07434 to 0.09848, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 14s - loss: 4.5493 - acc: 0.8871 - mDice: 0.0877 - val_loss: 3.0320 - val_acc: 0.9145 - val_mDice: 0.1320

Epoch 00010: val_mDice improved from 0.09848 to 0.13199, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 14s - loss: 4.1738 - acc: 0.8879 - mDice: 0.1067 - val_loss: 2.8307 - val_acc: 0.9143 - val_mDice: 0.1599

Epoch 00011: val_mDice improved from 0.13199 to 0.15988, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 14s - loss: 3.8622 - acc: 0.8895 - mDice: 0.1280 - val_loss: 2.6172 - val_acc: 0.9157 - val_mDice: 0.1901

Epoch 00012: val_mDice improved from 0.15988 to 0.19010, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 14s - loss: 3.5876 - acc: 0.8940 - mDice: 0.1553 - val_loss: 2.6398 - val_acc: 0.9215 - val_mDice: 0.2114

Epoch 00013: val_mDice improved from 0.19010 to 0.21135, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 14s - loss: 3.3448 - acc: 0.8979 - mDice: 0.1852 - val_loss: 2.3895 - val_acc: 0.9259 - val_mDice: 0.2543

Epoch 00014: val_mDice improved from 0.21135 to 0.25427, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 14s - loss: 3.1269 - acc: 0.9020 - mDice: 0.2149 - val_loss: 2.2261 - val_acc: 0.9289 - val_mDice: 0.2949

Epoch 00015: val_mDice improved from 0.25427 to 0.29490, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 14s - loss: 2.9425 - acc: 0.9071 - mDice: 0.2427 - val_loss: 2.2000 - val_acc: 0.9367 - val_mDice: 0.3165

Epoch 00016: val_mDice improved from 0.29490 to 0.31650, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 14s - loss: 2.7990 - acc: 0.9130 - mDice: 0.2667 - val_loss: 2.2588 - val_acc: 0.9386 - val_mDice: 0.3317

Epoch 00017: val_mDice improved from 0.31650 to 0.33166, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 14s - loss: 2.6753 - acc: 0.9179 - mDice: 0.2893 - val_loss: 2.4503 - val_acc: 0.9423 - val_mDice: 0.3331

Epoch 00018: val_mDice improved from 0.33166 to 0.33315, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 14s - loss: 2.5604 - acc: 0.9217 - mDice: 0.3104 - val_loss: 2.0010 - val_acc: 0.9454 - val_mDice: 0.3940

Epoch 00019: val_mDice improved from 0.33315 to 0.39400, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 14s - loss: 2.4523 - acc: 0.9245 - mDice: 0.3313 - val_loss: 2.1555 - val_acc: 0.9442 - val_mDice: 0.3834

Epoch 00020: val_mDice did not improve from 0.39400
Epoch 21/300
 - 14s - loss: 2.3634 - acc: 0.9257 - mDice: 0.3494 - val_loss: 2.0619 - val_acc: 0.9436 - val_mDice: 0.4043

Epoch 00021: val_mDice improved from 0.39400 to 0.40430, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 15s - loss: 2.2798 - acc: 0.9266 - mDice: 0.3657 - val_loss: 1.9872 - val_acc: 0.9447 - val_mDice: 0.4196

Epoch 00022: val_mDice improved from 0.40430 to 0.41957, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 14s - loss: 2.2145 - acc: 0.9273 - mDice: 0.3799 - val_loss: 2.0372 - val_acc: 0.9440 - val_mDice: 0.4301

Epoch 00023: val_mDice improved from 0.41957 to 0.43011, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 15s - loss: 2.1523 - acc: 0.9284 - mDice: 0.3938 - val_loss: 2.1997 - val_acc: 0.9454 - val_mDice: 0.4215

Epoch 00024: val_mDice did not improve from 0.43011
Epoch 25/300
 - 15s - loss: 2.0926 - acc: 0.9294 - mDice: 0.4063 - val_loss: 2.0122 - val_acc: 0.9440 - val_mDice: 0.4484

Epoch 00025: val_mDice improved from 0.43011 to 0.44841, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 14s - loss: 2.0467 - acc: 0.9305 - mDice: 0.4175 - val_loss: 2.0578 - val_acc: 0.9449 - val_mDice: 0.4537

Epoch 00026: val_mDice improved from 0.44841 to 0.45369, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 15s - loss: 1.9990 - acc: 0.9313 - mDice: 0.4279 - val_loss: 2.0023 - val_acc: 0.9460 - val_mDice: 0.4625

Epoch 00027: val_mDice improved from 0.45369 to 0.46252, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 14s - loss: 1.9564 - acc: 0.9324 - mDice: 0.4382 - val_loss: 1.9835 - val_acc: 0.9470 - val_mDice: 0.4699

Epoch 00028: val_mDice improved from 0.46252 to 0.46988, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 15s - loss: 1.9127 - acc: 0.9333 - mDice: 0.4482 - val_loss: 1.9947 - val_acc: 0.9460 - val_mDice: 0.4729

Epoch 00029: val_mDice improved from 0.46988 to 0.47287, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 15s - loss: 1.8797 - acc: 0.9339 - mDice: 0.4554 - val_loss: 2.1815 - val_acc: 0.9473 - val_mDice: 0.4478

Epoch 00030: val_mDice did not improve from 0.47287
Epoch 31/300
 - 14s - loss: 1.8419 - acc: 0.9347 - mDice: 0.4640 - val_loss: 1.9672 - val_acc: 0.9473 - val_mDice: 0.4782

Epoch 00031: val_mDice improved from 0.47287 to 0.47820, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 15s - loss: 1.8165 - acc: 0.9352 - mDice: 0.4700 - val_loss: 1.8962 - val_acc: 0.9471 - val_mDice: 0.4805

Epoch 00032: val_mDice improved from 0.47820 to 0.48055, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 14s - loss: 1.7947 - acc: 0.9356 - mDice: 0.4748 - val_loss: 1.9579 - val_acc: 0.9452 - val_mDice: 0.4830

Epoch 00033: val_mDice improved from 0.48055 to 0.48304, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 15s - loss: 1.7633 - acc: 0.9361 - mDice: 0.4822 - val_loss: 1.9746 - val_acc: 0.9484 - val_mDice: 0.4899

Epoch 00034: val_mDice improved from 0.48304 to 0.48987, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 14s - loss: 1.7439 - acc: 0.9365 - mDice: 0.4875 - val_loss: 1.9853 - val_acc: 0.9500 - val_mDice: 0.4834

Epoch 00035: val_mDice did not improve from 0.48987
Epoch 36/300
 - 15s - loss: 1.7198 - acc: 0.9368 - mDice: 0.4925 - val_loss: 2.1421 - val_acc: 0.9491 - val_mDice: 0.4704

Epoch 00036: val_mDice did not improve from 0.48987
Epoch 37/300
 - 15s - loss: 1.7088 - acc: 0.9368 - mDice: 0.4955 - val_loss: 2.1120 - val_acc: 0.9491 - val_mDice: 0.4802

Epoch 00037: val_mDice did not improve from 0.48987
Epoch 38/300
 - 15s - loss: 1.6878 - acc: 0.9373 - mDice: 0.5000 - val_loss: 2.0102 - val_acc: 0.9464 - val_mDice: 0.4896

Epoch 00038: val_mDice did not improve from 0.48987
Epoch 39/300
 - 15s - loss: 1.6640 - acc: 0.9376 - mDice: 0.5060 - val_loss: 2.2365 - val_acc: 0.9442 - val_mDice: 0.4658

Epoch 00039: val_mDice did not improve from 0.48987
Epoch 40/300
 - 15s - loss: 1.6493 - acc: 0.9378 - mDice: 0.5095 - val_loss: 2.0826 - val_acc: 0.9481 - val_mDice: 0.4891

Epoch 00040: val_mDice did not improve from 0.48987
Epoch 41/300
 - 14s - loss: 1.6354 - acc: 0.9379 - mDice: 0.5137 - val_loss: 2.0936 - val_acc: 0.9433 - val_mDice: 0.4778

Epoch 00041: val_mDice did not improve from 0.48987
Epoch 42/300
 - 15s - loss: 1.6209 - acc: 0.9382 - mDice: 0.5165 - val_loss: 2.1655 - val_acc: 0.9465 - val_mDice: 0.4830

Epoch 00042: val_mDice did not improve from 0.48987
Epoch 43/300
 - 14s - loss: 1.5995 - acc: 0.9385 - mDice: 0.5216 - val_loss: 2.0352 - val_acc: 0.9478 - val_mDice: 0.4989

Epoch 00043: val_mDice improved from 0.48987 to 0.49886, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 44/300
 - 15s - loss: 1.5883 - acc: 0.9386 - mDice: 0.5250 - val_loss: 2.1230 - val_acc: 0.9480 - val_mDice: 0.4882

Epoch 00044: val_mDice did not improve from 0.49886
Epoch 45/300
 - 15s - loss: 1.5749 - acc: 0.9390 - mDice: 0.5285 - val_loss: 2.1221 - val_acc: 0.9480 - val_mDice: 0.4948

Epoch 00045: val_mDice did not improve from 0.49886
Epoch 46/300
 - 14s - loss: 1.5625 - acc: 0.9393 - mDice: 0.5313 - val_loss: 2.1033 - val_acc: 0.9439 - val_mDice: 0.4888

Epoch 00046: val_mDice did not improve from 0.49886
Epoch 47/300
 - 14s - loss: 1.5517 - acc: 0.9395 - mDice: 0.5345 - val_loss: 2.1511 - val_acc: 0.9472 - val_mDice: 0.4933

Epoch 00047: val_mDice did not improve from 0.49886
Epoch 48/300
 - 15s - loss: 1.5379 - acc: 0.9399 - mDice: 0.5384 - val_loss: 2.1174 - val_acc: 0.9464 - val_mDice: 0.4989

Epoch 00048: val_mDice improved from 0.49886 to 0.49888, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 14s - loss: 1.5284 - acc: 0.9402 - mDice: 0.5409 - val_loss: 2.1256 - val_acc: 0.9474 - val_mDice: 0.4882

Epoch 00049: val_mDice did not improve from 0.49888
Epoch 50/300
 - 15s - loss: 1.5192 - acc: 0.9403 - mDice: 0.5427 - val_loss: 2.1374 - val_acc: 0.9472 - val_mDice: 0.5051

Epoch 00050: val_mDice improved from 0.49888 to 0.50511, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 14s - loss: 1.5106 - acc: 0.9406 - mDice: 0.5452 - val_loss: 2.2002 - val_acc: 0.9485 - val_mDice: 0.4952

Epoch 00051: val_mDice did not improve from 0.50511
Epoch 52/300
 - 15s - loss: 1.5006 - acc: 0.9407 - mDice: 0.5482 - val_loss: 2.1033 - val_acc: 0.9487 - val_mDice: 0.5070

Epoch 00052: val_mDice improved from 0.50511 to 0.50705, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 15s - loss: 1.4960 - acc: 0.9409 - mDice: 0.5498 - val_loss: 2.2784 - val_acc: 0.9458 - val_mDice: 0.4924

Epoch 00053: val_mDice did not improve from 0.50705
Epoch 54/300
 - 15s - loss: 1.4866 - acc: 0.9409 - mDice: 0.5511 - val_loss: 2.1243 - val_acc: 0.9471 - val_mDice: 0.4935

Epoch 00054: val_mDice did not improve from 0.50705
Epoch 55/300
 - 16s - loss: 1.4764 - acc: 0.9412 - mDice: 0.5543 - val_loss: 2.0497 - val_acc: 0.9480 - val_mDice: 0.5136

Epoch 00055: val_mDice improved from 0.50705 to 0.51364, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 16s - loss: 1.4774 - acc: 0.9412 - mDice: 0.5547 - val_loss: 2.0870 - val_acc: 0.9473 - val_mDice: 0.5122

Epoch 00056: val_mDice did not improve from 0.51364
Epoch 57/300
 - 16s - loss: 1.4648 - acc: 0.9414 - mDice: 0.5576 - val_loss: 2.1657 - val_acc: 0.9479 - val_mDice: 0.5039

Epoch 00057: val_mDice did not improve from 0.51364
Epoch 58/300
 - 16s - loss: 1.4574 - acc: 0.9415 - mDice: 0.5595 - val_loss: 2.1879 - val_acc: 0.9479 - val_mDice: 0.5076

Epoch 00058: val_mDice did not improve from 0.51364
Epoch 59/300
 - 16s - loss: 1.4463 - acc: 0.9416 - mDice: 0.5630 - val_loss: 2.2270 - val_acc: 0.9471 - val_mDice: 0.4951

Epoch 00059: val_mDice did not improve from 0.51364
Epoch 60/300
 - 16s - loss: 1.4373 - acc: 0.9417 - mDice: 0.5660 - val_loss: 2.2206 - val_acc: 0.9469 - val_mDice: 0.4945

Epoch 00060: val_mDice did not improve from 0.51364
Epoch 61/300
 - 16s - loss: 1.4358 - acc: 0.9416 - mDice: 0.5653 - val_loss: 2.2117 - val_acc: 0.9467 - val_mDice: 0.5030

Epoch 00061: val_mDice did not improve from 0.51364
Epoch 62/300
 - 16s - loss: 1.4250 - acc: 0.9418 - mDice: 0.5681 - val_loss: 2.1860 - val_acc: 0.9476 - val_mDice: 0.5077

Epoch 00062: val_mDice did not improve from 0.51364
Epoch 63/300
 - 16s - loss: 1.4191 - acc: 0.9419 - mDice: 0.5705 - val_loss: 2.2137 - val_acc: 0.9475 - val_mDice: 0.5114

Epoch 00063: val_mDice did not improve from 0.51364
Epoch 64/300
 - 16s - loss: 1.4204 - acc: 0.9419 - mDice: 0.5702 - val_loss: 2.1502 - val_acc: 0.9472 - val_mDice: 0.5130

Epoch 00064: val_mDice did not improve from 0.51364
Epoch 65/300
 - 16s - loss: 1.4109 - acc: 0.9421 - mDice: 0.5724 - val_loss: 2.3063 - val_acc: 0.9486 - val_mDice: 0.4973

Epoch 00065: val_mDice did not improve from 0.51364
Epoch 66/300
 - 16s - loss: 1.4032 - acc: 0.9421 - mDice: 0.5748 - val_loss: 2.3490 - val_acc: 0.9482 - val_mDice: 0.4965

Epoch 00066: val_mDice did not improve from 0.51364
Epoch 67/300
 - 16s - loss: 1.4034 - acc: 0.9421 - mDice: 0.5747 - val_loss: 2.3250 - val_acc: 0.9461 - val_mDice: 0.4947

Epoch 00067: val_mDice did not improve from 0.51364
Epoch 68/300
 - 16s - loss: 1.3950 - acc: 0.9423 - mDice: 0.5770 - val_loss: 2.1961 - val_acc: 0.9476 - val_mDice: 0.5118

Epoch 00068: val_mDice did not improve from 0.51364
Epoch 69/300
 - 16s - loss: 1.3859 - acc: 0.9423 - mDice: 0.5796 - val_loss: 2.2867 - val_acc: 0.9493 - val_mDice: 0.5014

Epoch 00069: val_mDice did not improve from 0.51364
Epoch 70/300
 - 16s - loss: 1.3867 - acc: 0.9422 - mDice: 0.5798 - val_loss: 2.3270 - val_acc: 0.9480 - val_mDice: 0.5004

Epoch 00070: val_mDice did not improve from 0.51364
Epoch 71/300
 - 16s - loss: 1.3772 - acc: 0.9423 - mDice: 0.5816 - val_loss: 2.1510 - val_acc: 0.9486 - val_mDice: 0.5164

Epoch 00071: val_mDice improved from 0.51364 to 0.51636, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 72/300
 - 16s - loss: 1.3749 - acc: 0.9423 - mDice: 0.5828 - val_loss: 2.1527 - val_acc: 0.9489 - val_mDice: 0.5202

Epoch 00072: val_mDice improved from 0.51636 to 0.52021, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 73/300
 - 15s - loss: 1.3687 - acc: 0.9425 - mDice: 0.5849 - val_loss: 2.1907 - val_acc: 0.9474 - val_mDice: 0.5095

Epoch 00073: val_mDice did not improve from 0.52021
Epoch 74/300
 - 15s - loss: 1.3674 - acc: 0.9423 - mDice: 0.5850 - val_loss: 2.2796 - val_acc: 0.9487 - val_mDice: 0.5118

Epoch 00074: val_mDice did not improve from 0.52021
Epoch 75/300
 - 14s - loss: 1.3673 - acc: 0.9424 - mDice: 0.5852 - val_loss: 2.1579 - val_acc: 0.9449 - val_mDice: 0.5149

Epoch 00075: val_mDice did not improve from 0.52021
Epoch 76/300
 - 15s - loss: 1.3592 - acc: 0.9425 - mDice: 0.5867 - val_loss: 2.2715 - val_acc: 0.9482 - val_mDice: 0.5120

Epoch 00076: val_mDice did not improve from 0.52021
Epoch 77/300
 - 14s - loss: 1.3541 - acc: 0.9427 - mDice: 0.5883 - val_loss: 2.2577 - val_acc: 0.9474 - val_mDice: 0.5050

Epoch 00077: val_mDice did not improve from 0.52021
Epoch 78/300
 - 14s - loss: 1.3470 - acc: 0.9429 - mDice: 0.5908 - val_loss: 2.2250 - val_acc: 0.9487 - val_mDice: 0.5161

Epoch 00078: val_mDice did not improve from 0.52021
Epoch 79/300
 - 15s - loss: 1.3441 - acc: 0.9430 - mDice: 0.5911 - val_loss: 2.2707 - val_acc: 0.9476 - val_mDice: 0.5161

Epoch 00079: val_mDice did not improve from 0.52021
Epoch 80/300
 - 14s - loss: 1.3428 - acc: 0.9430 - mDice: 0.5920 - val_loss: 2.2237 - val_acc: 0.9481 - val_mDice: 0.5106

Epoch 00080: val_mDice did not improve from 0.52021
Epoch 81/300
 - 14s - loss: 1.3368 - acc: 0.9431 - mDice: 0.5937 - val_loss: 2.2079 - val_acc: 0.9471 - val_mDice: 0.5174

Epoch 00081: val_mDice did not improve from 0.52021
Epoch 82/300
 - 14s - loss: 1.3343 - acc: 0.9432 - mDice: 0.5940 - val_loss: 2.3676 - val_acc: 0.9469 - val_mDice: 0.5028

Epoch 00082: val_mDice did not improve from 0.52021
Epoch 83/300
 - 14s - loss: 1.3365 - acc: 0.9433 - mDice: 0.5937 - val_loss: 2.2461 - val_acc: 0.9477 - val_mDice: 0.5109

Epoch 00083: val_mDice did not improve from 0.52021
Epoch 84/300
 - 14s - loss: 1.3269 - acc: 0.9435 - mDice: 0.5965 - val_loss: 2.4017 - val_acc: 0.9478 - val_mDice: 0.5019

Epoch 00084: val_mDice did not improve from 0.52021
Epoch 85/300
 - 14s - loss: 1.3246 - acc: 0.9435 - mDice: 0.5974 - val_loss: 2.1879 - val_acc: 0.9480 - val_mDice: 0.5104

Epoch 00085: val_mDice did not improve from 0.52021
Epoch 86/300
 - 14s - loss: 1.3243 - acc: 0.9436 - mDice: 0.5972 - val_loss: 2.4877 - val_acc: 0.9461 - val_mDice: 0.4838

Epoch 00086: val_mDice did not improve from 0.52021
Epoch 87/300
 - 14s - loss: 1.3180 - acc: 0.9438 - mDice: 0.5987 - val_loss: 2.3789 - val_acc: 0.9477 - val_mDice: 0.5106

Epoch 00087: val_mDice did not improve from 0.52021
Epoch 88/300
 - 14s - loss: 1.3192 - acc: 0.9438 - mDice: 0.5985 - val_loss: 2.2501 - val_acc: 0.9482 - val_mDice: 0.5154

Epoch 00088: val_mDice did not improve from 0.52021
Epoch 89/300
 - 14s - loss: 1.3141 - acc: 0.9439 - mDice: 0.5998 - val_loss: 2.2127 - val_acc: 0.9484 - val_mDice: 0.5109

Epoch 00089: val_mDice did not improve from 0.52021
Epoch 90/300
 - 14s - loss: 1.3124 - acc: 0.9440 - mDice: 0.6001 - val_loss: 2.3407 - val_acc: 0.9472 - val_mDice: 0.5097

Epoch 00090: val_mDice did not improve from 0.52021
Epoch 91/300
 - 14s - loss: 1.3138 - acc: 0.9441 - mDice: 0.6000 - val_loss: 2.2901 - val_acc: 0.9484 - val_mDice: 0.5174

Epoch 00091: val_mDice did not improve from 0.52021
Epoch 92/300
 - 14s - loss: 1.3096 - acc: 0.9441 - mDice: 0.6012 - val_loss: 2.2928 - val_acc: 0.9478 - val_mDice: 0.5120

Epoch 00092: val_mDice did not improve from 0.52021
Epoch 93/300
 - 14s - loss: 1.3051 - acc: 0.9444 - mDice: 0.6027 - val_loss: 2.3258 - val_acc: 0.9466 - val_mDice: 0.5045

Epoch 00093: val_mDice did not improve from 0.52021
Epoch 94/300
 - 14s - loss: 1.3033 - acc: 0.9444 - mDice: 0.6033 - val_loss: 2.3079 - val_acc: 0.9473 - val_mDice: 0.5164

Epoch 00094: val_mDice did not improve from 0.52021
Epoch 95/300
 - 14s - loss: 1.3030 - acc: 0.9444 - mDice: 0.6032 - val_loss: 2.4100 - val_acc: 0.9471 - val_mDice: 0.5141

Epoch 00095: val_mDice did not improve from 0.52021
Epoch 96/300
 - 14s - loss: 1.3013 - acc: 0.9444 - mDice: 0.6032 - val_loss: 2.2536 - val_acc: 0.9483 - val_mDice: 0.5215

Epoch 00096: val_mDice improved from 0.52021 to 0.52147, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 97/300
 - 14s - loss: 1.3003 - acc: 0.9445 - mDice: 0.6042 - val_loss: 2.2816 - val_acc: 0.9479 - val_mDice: 0.5175

Epoch 00097: val_mDice did not improve from 0.52147
Epoch 98/300
 - 14s - loss: 1.2925 - acc: 0.9447 - mDice: 0.6063 - val_loss: 2.2702 - val_acc: 0.9475 - val_mDice: 0.5158

Epoch 00098: val_mDice did not improve from 0.52147
Epoch 99/300
 - 14s - loss: 1.2947 - acc: 0.9447 - mDice: 0.6051 - val_loss: 2.2981 - val_acc: 0.9485 - val_mDice: 0.5160

Epoch 00099: val_mDice did not improve from 0.52147
Epoch 100/300
 - 14s - loss: 1.2893 - acc: 0.9449 - mDice: 0.6074 - val_loss: 2.3538 - val_acc: 0.9476 - val_mDice: 0.5055

Epoch 00100: val_mDice did not improve from 0.52147
Epoch 101/300
 - 15s - loss: 1.2885 - acc: 0.9450 - mDice: 0.6076 - val_loss: 2.3192 - val_acc: 0.9476 - val_mDice: 0.5205

Epoch 00101: val_mDice did not improve from 0.52147
Epoch 102/300
 - 14s - loss: 1.2896 - acc: 0.9449 - mDice: 0.6069 - val_loss: 2.4777 - val_acc: 0.9485 - val_mDice: 0.5071

Epoch 00102: val_mDice did not improve from 0.52147
Epoch 103/300
 - 14s - loss: 1.2849 - acc: 0.9451 - mDice: 0.6082 - val_loss: 2.5070 - val_acc: 0.9470 - val_mDice: 0.5032

Epoch 00103: val_mDice did not improve from 0.52147
Epoch 104/300
 - 14s - loss: 1.2845 - acc: 0.9453 - mDice: 0.6083 - val_loss: 2.3509 - val_acc: 0.9481 - val_mDice: 0.5069

Epoch 00104: val_mDice did not improve from 0.52147
Epoch 105/300
 - 14s - loss: 1.2811 - acc: 0.9452 - mDice: 0.6094 - val_loss: 2.3460 - val_acc: 0.9482 - val_mDice: 0.5135

Epoch 00105: val_mDice did not improve from 0.52147
Epoch 106/300
 - 14s - loss: 1.2821 - acc: 0.9453 - mDice: 0.6096 - val_loss: 2.1854 - val_acc: 0.9479 - val_mDice: 0.5266

Epoch 00106: val_mDice improved from 0.52147 to 0.52660, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 107/300
 - 14s - loss: 1.2784 - acc: 0.9453 - mDice: 0.6100 - val_loss: 2.3606 - val_acc: 0.9471 - val_mDice: 0.5209

Epoch 00107: val_mDice did not improve from 0.52660
Epoch 108/300
 - 14s - loss: 1.2744 - acc: 0.9456 - mDice: 0.6117 - val_loss: 2.2299 - val_acc: 0.9492 - val_mDice: 0.5218

Epoch 00108: val_mDice did not improve from 0.52660
Epoch 109/300
 - 14s - loss: 1.2715 - acc: 0.9457 - mDice: 0.6127 - val_loss: 2.6431 - val_acc: 0.9482 - val_mDice: 0.4942

Epoch 00109: val_mDice did not improve from 0.52660
Epoch 110/300
 - 14s - loss: 1.2729 - acc: 0.9457 - mDice: 0.6121 - val_loss: 2.3401 - val_acc: 0.9472 - val_mDice: 0.5267

Epoch 00110: val_mDice improved from 0.52660 to 0.52667, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 111/300
 - 15s - loss: 1.2740 - acc: 0.9457 - mDice: 0.6121 - val_loss: 2.2907 - val_acc: 0.9485 - val_mDice: 0.5215

Epoch 00111: val_mDice did not improve from 0.52667
Epoch 112/300
 - 14s - loss: 1.2722 - acc: 0.9457 - mDice: 0.6120 - val_loss: 2.2708 - val_acc: 0.9489 - val_mDice: 0.5146

Epoch 00112: val_mDice did not improve from 0.52667
Epoch 113/300
 - 14s - loss: 1.2653 - acc: 0.9459 - mDice: 0.6142 - val_loss: 2.4140 - val_acc: 0.9470 - val_mDice: 0.4979

Epoch 00113: val_mDice did not improve from 0.52667
Epoch 114/300
 - 15s - loss: 1.2664 - acc: 0.9460 - mDice: 0.6138 - val_loss: 2.4449 - val_acc: 0.9488 - val_mDice: 0.5070

Epoch 00114: val_mDice did not improve from 0.52667
Epoch 115/300
 - 14s - loss: 1.2667 - acc: 0.9460 - mDice: 0.6141 - val_loss: 2.4067 - val_acc: 0.9475 - val_mDice: 0.5104

Epoch 00115: val_mDice did not improve from 0.52667
Epoch 116/300
 - 14s - loss: 1.2595 - acc: 0.9461 - mDice: 0.6159 - val_loss: 2.5013 - val_acc: 0.9461 - val_mDice: 0.5052

Epoch 00116: val_mDice did not improve from 0.52667
Epoch 117/300
 - 14s - loss: 1.2622 - acc: 0.9461 - mDice: 0.6155 - val_loss: 2.3543 - val_acc: 0.9486 - val_mDice: 0.5158

Epoch 00117: val_mDice did not improve from 0.52667
Epoch 118/300
 - 14s - loss: 1.2562 - acc: 0.9463 - mDice: 0.6179 - val_loss: 2.3254 - val_acc: 0.9475 - val_mDice: 0.5157

Epoch 00118: val_mDice did not improve from 0.52667
Epoch 119/300
 - 14s - loss: 1.2569 - acc: 0.9462 - mDice: 0.6169 - val_loss: 2.4054 - val_acc: 0.9470 - val_mDice: 0.5043

Epoch 00119: val_mDice did not improve from 0.52667
Epoch 120/300
 - 14s - loss: 1.2546 - acc: 0.9464 - mDice: 0.6178 - val_loss: 2.3913 - val_acc: 0.9491 - val_mDice: 0.5086

Epoch 00120: val_mDice did not improve from 0.52667
Epoch 121/300
 - 14s - loss: 1.2532 - acc: 0.9464 - mDice: 0.6180 - val_loss: 2.4458 - val_acc: 0.9470 - val_mDice: 0.5014

Epoch 00121: val_mDice did not improve from 0.52667
Epoch 122/300
 - 14s - loss: 1.2507 - acc: 0.9465 - mDice: 0.6187 - val_loss: 2.3921 - val_acc: 0.9477 - val_mDice: 0.5109

Epoch 00122: val_mDice did not improve from 0.52667
Epoch 123/300
 - 14s - loss: 1.2489 - acc: 0.9464 - mDice: 0.6188 - val_loss: 2.4194 - val_acc: 0.9485 - val_mDice: 0.5140

Epoch 00123: val_mDice did not improve from 0.52667
Epoch 124/300
 - 14s - loss: 1.2488 - acc: 0.9465 - mDice: 0.6192 - val_loss: 2.5506 - val_acc: 0.9475 - val_mDice: 0.4977

Epoch 00124: val_mDice did not improve from 0.52667
Epoch 125/300
 - 14s - loss: 1.2467 - acc: 0.9466 - mDice: 0.6199 - val_loss: 2.3455 - val_acc: 0.9481 - val_mDice: 0.5260

Epoch 00125: val_mDice did not improve from 0.52667
Epoch 126/300
 - 15s - loss: 1.2470 - acc: 0.9466 - mDice: 0.6201 - val_loss: 2.3158 - val_acc: 0.9490 - val_mDice: 0.5222

Epoch 00126: val_mDice did not improve from 0.52667
Epoch 127/300
 - 15s - loss: 1.2449 - acc: 0.9466 - mDice: 0.6203 - val_loss: 2.2719 - val_acc: 0.9501 - val_mDice: 0.5320

Epoch 00127: val_mDice improved from 0.52667 to 0.53200, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 128/300
 - 14s - loss: 1.2471 - acc: 0.9466 - mDice: 0.6202 - val_loss: 2.3782 - val_acc: 0.9488 - val_mDice: 0.5115

Epoch 00128: val_mDice did not improve from 0.53200
Epoch 129/300
 - 15s - loss: 1.2423 - acc: 0.9467 - mDice: 0.6218 - val_loss: 2.3642 - val_acc: 0.9497 - val_mDice: 0.5123

Epoch 00129: val_mDice did not improve from 0.53200
Epoch 130/300
 - 15s - loss: 1.2406 - acc: 0.9467 - mDice: 0.6216 - val_loss: 2.3966 - val_acc: 0.9475 - val_mDice: 0.5063

Epoch 00130: val_mDice did not improve from 0.53200
Epoch 131/300
 - 15s - loss: 1.2424 - acc: 0.9467 - mDice: 0.6219 - val_loss: 2.4299 - val_acc: 0.9481 - val_mDice: 0.5211

Epoch 00131: val_mDice did not improve from 0.53200
Epoch 132/300
 - 15s - loss: 1.2399 - acc: 0.9467 - mDice: 0.6220 - val_loss: 2.4105 - val_acc: 0.9478 - val_mDice: 0.5099

Epoch 00132: val_mDice did not improve from 0.53200
Epoch 133/300
 - 16s - loss: 1.2378 - acc: 0.9468 - mDice: 0.6226 - val_loss: 2.4309 - val_acc: 0.9490 - val_mDice: 0.5167

Epoch 00133: val_mDice did not improve from 0.53200
Epoch 134/300
 - 15s - loss: 1.2339 - acc: 0.9469 - mDice: 0.6243 - val_loss: 2.3810 - val_acc: 0.9475 - val_mDice: 0.5279

Epoch 00134: val_mDice did not improve from 0.53200
Epoch 135/300
 - 15s - loss: 1.2318 - acc: 0.9470 - mDice: 0.6244 - val_loss: 2.3862 - val_acc: 0.9488 - val_mDice: 0.5202

Epoch 00135: val_mDice did not improve from 0.53200
Epoch 136/300
 - 14s - loss: 1.2324 - acc: 0.9469 - mDice: 0.6243 - val_loss: 2.3245 - val_acc: 0.9503 - val_mDice: 0.5226

Epoch 00136: val_mDice did not improve from 0.53200
Epoch 137/300
 - 15s - loss: 1.2295 - acc: 0.9470 - mDice: 0.6253 - val_loss: 2.4278 - val_acc: 0.9496 - val_mDice: 0.5171

Epoch 00137: val_mDice did not improve from 0.53200
Epoch 138/300
 - 15s - loss: 1.2280 - acc: 0.9470 - mDice: 0.6261 - val_loss: 2.5002 - val_acc: 0.9463 - val_mDice: 0.5014

Epoch 00138: val_mDice did not improve from 0.53200
Epoch 139/300
 - 15s - loss: 1.2316 - acc: 0.9469 - mDice: 0.6246 - val_loss: 2.3604 - val_acc: 0.9491 - val_mDice: 0.5295

Epoch 00139: val_mDice did not improve from 0.53200
Epoch 140/300
 - 14s - loss: 1.2293 - acc: 0.9471 - mDice: 0.6253 - val_loss: 2.4586 - val_acc: 0.9483 - val_mDice: 0.5101

Epoch 00140: val_mDice did not improve from 0.53200
Epoch 141/300
 - 15s - loss: 1.2267 - acc: 0.9471 - mDice: 0.6260 - val_loss: 2.4818 - val_acc: 0.9493 - val_mDice: 0.5107

Epoch 00141: val_mDice did not improve from 0.53200
Epoch 142/300
 - 15s - loss: 1.2251 - acc: 0.9471 - mDice: 0.6269 - val_loss: 2.4002 - val_acc: 0.9468 - val_mDice: 0.5191

Epoch 00142: val_mDice did not improve from 0.53200
Epoch 143/300
 - 15s - loss: 1.2236 - acc: 0.9472 - mDice: 0.6273 - val_loss: 2.3543 - val_acc: 0.9480 - val_mDice: 0.5213

Epoch 00143: val_mDice did not improve from 0.53200
Epoch 144/300
 - 16s - loss: 1.2276 - acc: 0.9471 - mDice: 0.6259 - val_loss: 2.5595 - val_acc: 0.9496 - val_mDice: 0.5011

Epoch 00144: val_mDice did not improve from 0.53200
Epoch 145/300
 - 15s - loss: 1.2225 - acc: 0.9472 - mDice: 0.6273 - val_loss: 2.4888 - val_acc: 0.9492 - val_mDice: 0.5095

Epoch 00145: val_mDice did not improve from 0.53200
Epoch 146/300
 - 14s - loss: 1.2223 - acc: 0.9473 - mDice: 0.6275 - val_loss: 2.3430 - val_acc: 0.9481 - val_mDice: 0.5195

Epoch 00146: val_mDice did not improve from 0.53200
Epoch 147/300
 - 14s - loss: 1.2227 - acc: 0.9473 - mDice: 0.6273 - val_loss: 2.3148 - val_acc: 0.9491 - val_mDice: 0.5340

Epoch 00147: val_mDice improved from 0.53200 to 0.53395, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 148/300
 - 14s - loss: 1.2206 - acc: 0.9473 - mDice: 0.6279 - val_loss: 2.5003 - val_acc: 0.9472 - val_mDice: 0.5195

Epoch 00148: val_mDice did not improve from 0.53395
Epoch 149/300
 - 14s - loss: 1.2194 - acc: 0.9473 - mDice: 0.6283 - val_loss: 2.4338 - val_acc: 0.9475 - val_mDice: 0.5177

Epoch 00149: val_mDice did not improve from 0.53395
Epoch 150/300
 - 14s - loss: 1.2171 - acc: 0.9474 - mDice: 0.6289 - val_loss: 2.5081 - val_acc: 0.9490 - val_mDice: 0.5056

Epoch 00150: val_mDice did not improve from 0.53395
Epoch 151/300
 - 15s - loss: 1.2207 - acc: 0.9473 - mDice: 0.6282 - val_loss: 2.2946 - val_acc: 0.9466 - val_mDice: 0.5142

Epoch 00151: val_mDice did not improve from 0.53395
Epoch 152/300
 - 14s - loss: 1.2176 - acc: 0.9472 - mDice: 0.6288 - val_loss: 2.5330 - val_acc: 0.9478 - val_mDice: 0.5142

Epoch 00152: val_mDice did not improve from 0.53395
Epoch 153/300
 - 14s - loss: 1.2174 - acc: 0.9474 - mDice: 0.6291 - val_loss: 2.6316 - val_acc: 0.9494 - val_mDice: 0.5076

Epoch 00153: val_mDice did not improve from 0.53395
Epoch 154/300
 - 14s - loss: 1.2156 - acc: 0.9474 - mDice: 0.6297 - val_loss: 2.2986 - val_acc: 0.9500 - val_mDice: 0.5263

Epoch 00154: val_mDice did not improve from 0.53395
Epoch 155/300
 - 14s - loss: 1.2125 - acc: 0.9476 - mDice: 0.6306 - val_loss: 2.3515 - val_acc: 0.9480 - val_mDice: 0.5221

Epoch 00155: val_mDice did not improve from 0.53395
Epoch 156/300
 - 14s - loss: 1.2123 - acc: 0.9475 - mDice: 0.6300 - val_loss: 2.4112 - val_acc: 0.9497 - val_mDice: 0.5183

Epoch 00156: val_mDice did not improve from 0.53395
Epoch 157/300
 - 15s - loss: 1.2095 - acc: 0.9475 - mDice: 0.6311 - val_loss: 2.2999 - val_acc: 0.9493 - val_mDice: 0.5269

Epoch 00157: val_mDice did not improve from 0.53395
Epoch 158/300
 - 15s - loss: 1.2103 - acc: 0.9476 - mDice: 0.6311 - val_loss: 2.3304 - val_acc: 0.9493 - val_mDice: 0.5265

Epoch 00158: val_mDice did not improve from 0.53395
Epoch 159/300
 - 14s - loss: 1.2081 - acc: 0.9476 - mDice: 0.6309 - val_loss: 2.4611 - val_acc: 0.9474 - val_mDice: 0.5079

Epoch 00159: val_mDice did not improve from 0.53395
Epoch 160/300
 - 14s - loss: 1.2132 - acc: 0.9474 - mDice: 0.6304 - val_loss: 2.3995 - val_acc: 0.9475 - val_mDice: 0.5157

Epoch 00160: val_mDice did not improve from 0.53395
Epoch 161/300
 - 14s - loss: 1.2089 - acc: 0.9476 - mDice: 0.6315 - val_loss: 2.4001 - val_acc: 0.9493 - val_mDice: 0.5192

Epoch 00161: val_mDice did not improve from 0.53395
Epoch 162/300
 - 14s - loss: 1.2083 - acc: 0.9476 - mDice: 0.6315 - val_loss: 2.5083 - val_acc: 0.9492 - val_mDice: 0.4969

Epoch 00162: val_mDice did not improve from 0.53395
Epoch 163/300
 - 14s - loss: 1.2046 - acc: 0.9476 - mDice: 0.6324 - val_loss: 2.2609 - val_acc: 0.9492 - val_mDice: 0.5253

Epoch 00163: val_mDice did not improve from 0.53395
Epoch 164/300
 - 14s - loss: 1.2053 - acc: 0.9477 - mDice: 0.6324 - val_loss: 2.4658 - val_acc: 0.9487 - val_mDice: 0.5196

Epoch 00164: val_mDice did not improve from 0.53395
Epoch 165/300
 - 14s - loss: 1.2022 - acc: 0.9478 - mDice: 0.6332 - val_loss: 2.4757 - val_acc: 0.9476 - val_mDice: 0.5171

Epoch 00165: val_mDice did not improve from 0.53395
Epoch 166/300
 - 14s - loss: 1.2010 - acc: 0.9477 - mDice: 0.6333 - val_loss: 2.3840 - val_acc: 0.9503 - val_mDice: 0.5251

Epoch 00166: val_mDice did not improve from 0.53395
Epoch 167/300
 - 14s - loss: 1.2013 - acc: 0.9478 - mDice: 0.6337 - val_loss: 2.5977 - val_acc: 0.9496 - val_mDice: 0.5172

Epoch 00167: val_mDice did not improve from 0.53395
Epoch 168/300
 - 14s - loss: 1.2049 - acc: 0.9477 - mDice: 0.6329 - val_loss: 2.5077 - val_acc: 0.9486 - val_mDice: 0.5085

Epoch 00168: val_mDice did not improve from 0.53395
Epoch 169/300
 - 14s - loss: 1.2013 - acc: 0.9477 - mDice: 0.6336 - val_loss: 2.3675 - val_acc: 0.9476 - val_mDice: 0.5171

Epoch 00169: val_mDice did not improve from 0.53395
Epoch 170/300
 - 14s - loss: 1.2025 - acc: 0.9478 - mDice: 0.6333 - val_loss: 2.4791 - val_acc: 0.9486 - val_mDice: 0.5156

Epoch 00170: val_mDice did not improve from 0.53395
Epoch 171/300
 - 14s - loss: 1.1995 - acc: 0.9478 - mDice: 0.6346 - val_loss: 2.4350 - val_acc: 0.9483 - val_mDice: 0.5155

Epoch 00171: val_mDice did not improve from 0.53395
Epoch 172/300
 - 14s - loss: 1.1960 - acc: 0.9479 - mDice: 0.6350 - val_loss: 2.4697 - val_acc: 0.9485 - val_mDice: 0.5151

Epoch 00172: val_mDice did not improve from 0.53395
Epoch 173/300
 - 14s - loss: 1.1976 - acc: 0.9480 - mDice: 0.6351 - val_loss: 2.4183 - val_acc: 0.9483 - val_mDice: 0.5191

Epoch 00173: val_mDice did not improve from 0.53395
Epoch 174/300
 - 14s - loss: 1.1960 - acc: 0.9479 - mDice: 0.6351 - val_loss: 2.3859 - val_acc: 0.9484 - val_mDice: 0.5148

Epoch 00174: val_mDice did not improve from 0.53395
Epoch 175/300
 - 14s - loss: 1.1944 - acc: 0.9480 - mDice: 0.6358 - val_loss: 2.5818 - val_acc: 0.9496 - val_mDice: 0.5150

Epoch 00175: val_mDice did not improve from 0.53395
Epoch 176/300
 - 14s - loss: 1.1957 - acc: 0.9479 - mDice: 0.6351 - val_loss: 2.3754 - val_acc: 0.9477 - val_mDice: 0.5183

Epoch 00176: val_mDice did not improve from 0.53395
Epoch 177/300
 - 14s - loss: 1.1963 - acc: 0.9479 - mDice: 0.6352 - val_loss: 2.4493 - val_acc: 0.9486 - val_mDice: 0.5184

Epoch 00177: val_mDice did not improve from 0.53395
Restoring model weights from the end of the best epoch
Epoch 00177: early stopping
{'val_loss': [37.041223344856135, 11.964276873199633, 7.169120181206218, 5.593736025208202, 4.852106986765089, 4.378437212725592, 4.128430121437797, 3.839417175207724, 3.470264675896927, 3.0319956347929033, 2.8306859325430245, 2.617227471740552, 2.63979913935315, 2.389539393632772, 2.226149062204627, 2.1999952713204496, 2.2588005412224286, 2.4503479709838354, 2.0010354319098274, 2.1555134464242607, 2.0619473404058533, 1.987175616472127, 2.037185017623049, 2.1996745104230317, 2.0121542674869133, 2.057794817333115, 2.002325187182293, 1.9835490754196763, 1.9946932393079364, 2.1815034544001746, 1.9671815001098802, 1.8962316699534154, 1.957865395359487, 1.9746306355439085, 1.9853251286725093, 2.142142603517245, 2.1119722600755746, 2.010213205934237, 2.236475137358937, 2.082561959101501, 2.093580059499048, 2.1655071873904608, 2.0352055427082423, 2.1229641504127885, 2.1221009899118095, 2.10330773065876, 2.1510679801749117, 2.117391600955132, 2.125634290652568, 2.1374145792849237, 2.2001560613429745, 2.103331087687828, 2.278353351454495, 2.124347704082894, 2.0497173490471012, 2.087007879544903, 2.165728965951078, 2.1878508788913322, 2.227026120244458, 2.2206094424817815, 2.2116782865044793, 2.1860300588874177, 2.2137160154694286, 2.150173795955807, 2.3062719233209195, 2.349033263808522, 2.3249731463426984, 2.1960959021605593, 2.2866854068287257, 2.327040473842088, 2.1509879514491757, 2.1527213917098234, 2.1906505443530375, 2.2795746179932324, 2.1579001389402253, 2.2714500733593987, 2.25774914592338, 2.225034086397906, 2.2706951988475947, 2.2236703694199718, 2.207907033366198, 2.367641682065399, 2.2461003671145305, 2.4017409998611363, 2.1878500970382264, 2.4876771172997674, 2.3788805966936675, 2.25012569054545, 2.212714887864097, 2.340739204896895, 2.2901166217953133, 2.2928161807566383, 2.3258117190952405, 2.30793194264673, 2.4100391878096086, 2.253566650038991, 2.2816123069997607, 2.27018503769816, 2.29809318174863, 2.3538438354790543, 2.3191696851613135, 2.4777124340973753, 2.5070116733039556, 2.350909692615104, 2.3459806349024426, 2.1854271702260277, 2.360615967372276, 2.2298999365481587, 2.6431459461510514, 2.3400766982712558, 2.2906983924311635, 2.2708283549580495, 2.4140357917913513, 2.444947116201816, 2.406688780757968, 2.5012923805407308, 2.354320895072468, 2.3253992096671845, 2.4054373908975273, 2.391276992233106, 2.4458060064795295, 2.392050982853554, 2.4193860392330744, 2.5505874263507695, 2.3454875879447554, 2.3158120416396155, 2.2719182009137544, 2.378159216662359, 2.3642365239851966, 2.396625223106512, 2.429908366176669, 2.4104885868520043, 2.430920555604903, 2.3809715263004407, 2.3861686424170125, 2.324518530062457, 2.427806436016573, 2.5002002249882875, 2.3603624058835333, 2.4586050044224916, 2.4818118311173425, 2.4002289598880533, 2.3542811990450216, 2.55945028273087, 2.4888158643711877, 2.3430437908492276, 2.314811040569284, 2.5003467711656455, 2.4337679620561654, 2.5080814188419107, 2.294567832733666, 2.5329536126312595, 2.631635175736923, 2.2985582018697728, 2.3515135642536524, 2.4111561029316992, 2.2998993290203242, 2.3304078472393184, 2.461148400546452, 2.3995318918920763, 2.40010414709592, 2.5082546505848122, 2.2609495903526606, 2.4657643254242796, 2.475725533575985, 2.3840394619456884, 2.5977374375199473, 2.507704840026088, 2.3675343031323823, 2.4790975634612185, 2.4349969365743287, 2.469731248290845, 2.418327490044706, 2.385867370573502, 2.5818107221379627, 2.375356137419546, 2.4493171382882744], 'val_acc': [0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9144739132353713, 0.914254903793335, 0.9157383258782286, 0.921461273505035, 0.9259301440675831, 0.9289362357315405, 0.9367272943757766, 0.9385888146954542, 0.9422560297577075, 0.9454150236518689, 0.944210520003761, 0.9435638312520928, 0.9447146167968239, 0.9439687838767494, 0.9454253465769678, 0.9439935600957391, 0.9449357117354537, 0.9460059324456327, 0.9470389345504718, 0.9459976737059694, 0.9472992696575613, 0.9472538319379924, 0.9470823350565394, 0.9451877787792483, 0.948408723876463, 0.950020250661413, 0.9490698436119037, 0.9490512732686943, 0.9464212076624012, 0.944150597689538, 0.9481194791847101, 0.9432890604994151, 0.9465017651712428, 0.9478405840570034, 0.9479872427839141, 0.9480099817894024, 0.9438861468650775, 0.947177366504456, 0.9463695434218679, 0.9473798508084686, 0.9472290347408316, 0.9484934453857677, 0.9486545807156483, 0.94581792474459, 0.9470947205021395, 0.9480141088283262, 0.9472703497503057, 0.947921147892595, 0.9479335340041688, 0.947061680215697, 0.946859179928316, 0.9467393815850412, 0.9476091548717221, 0.9475120532446067, 0.9472434750482357, 0.94858641464617, 0.9482166007910361, 0.9460864832947374, 0.9475905662142364, 0.9492888800258743, 0.948040967214041, 0.9485657524796172, 0.9488983743683586, 0.9474211668169032, 0.9486690446651181, 0.9449480811976854, 0.9481731976210738, 0.9473736592511225, 0.9486835319236671, 0.9475554337714638, 0.9480575010097226, 0.9470534208100602, 0.9469005265715402, 0.9476794107666229, 0.9477868419785739, 0.947983122737714, 0.9460885611326335, 0.9477021560988612, 0.9481897360785714, 0.9483756885848231, 0.9472042355457497, 0.9484479923488042, 0.9478426349229653, 0.9466360690873429, 0.9472579623067845, 0.9470885122954512, 0.9483054077159093, 0.9478529608449456, 0.947538923284861, 0.9485079226547113, 0.947619511428492, 0.9476236134934026, 0.948483109807169, 0.9469790475328541, 0.9481442767148577, 0.9481608058487236, 0.9478632917617287, 0.947123663718474, 0.9492144657912867, 0.9481731946241923, 0.9471856325698298, 0.9485223772805497, 0.9489273052641799, 0.9470244945760545, 0.948799228868005, 0.9474500890550667, 0.946096840517481, 0.9486174110593742, 0.9474748662730169, 0.9470430565945929, 0.9491029092053461, 0.9469624767756329, 0.947698013409556, 0.9485161547554272, 0.9475182704419397, 0.9480884964239664, 0.9489582910218053, 0.9500739777554347, 0.9487806052469009, 0.9496752126922821, 0.9474500757355929, 0.9480781671721176, 0.9477971805540543, 0.949036810318185, 0.9475120768866725, 0.9487682124755902, 0.9503363450812228, 0.9495843239336706, 0.9462786573271512, 0.9491401001727781, 0.9483095650566357, 0.9493219223102378, 0.9468137558612078, 0.9480058777265709, 0.9495533281864401, 0.9491793703100535, 0.9480616220548832, 0.9490636780275313, 0.947191828123018, 0.9474790289415328, 0.9489727406528409, 0.9465720140734198, 0.9477538043560263, 0.9493962965864043, 0.9499851348679825, 0.9480327031465882, 0.9496690184710412, 0.9492640648474241, 0.9493405192923945, 0.9473901440977385, 0.9475224034746266, 0.9493322498971524, 0.9492082928812038, 0.9492330950731672, 0.9486938421952658, 0.9475905845285127, 0.9503280850096122, 0.9495615732736428, 0.9485802031096133, 0.9475864341805101, 0.9486484038097233, 0.9482930409175724, 0.9484872401759611, 0.9482599610056957, 0.9484335144138869, 0.9496483622982516, 0.9476546365455543, 0.9486091296766057], 'val_mDice': [0.007852536433652127, 0.010449156706226604, 0.014957067239301165, 0.02188655238744267, 0.028702432778794005, 0.03909994878249461, 0.055994645198130744, 0.07434090992758394, 0.09848286136568592, 0.13198635015407753, 0.15987612923097344, 0.19009534490175087, 0.21135043565121442, 0.2542706158574067, 0.29489624933157554, 0.31649599911114357, 0.33166008088841786, 0.3331497619604931, 0.3940029197565004, 0.3833517617020527, 0.4043010128609961, 0.41956936597158123, 0.430114140057697, 0.4215315249046134, 0.44841447239481536, 0.453690853864787, 0.46252023574360257, 0.4698757527593794, 0.4728680697566304, 0.4477661904676, 0.4781988494556043, 0.4805451138725494, 0.48303912505091234, 0.4898676729069076, 0.4833835083679114, 0.4703714975764632, 0.4801955904041589, 0.4895764359548771, 0.465849306496828, 0.48912404585817004, 0.4777642495139351, 0.4829757280189898, 0.49886313330527793, 0.4881504516028825, 0.49480197682726984, 0.4888480425546955, 0.49329854140068563, 0.49887963247032807, 0.48823686585080023, 0.5051085884011658, 0.4952209268868302, 0.5070465525102349, 0.49236622019853005, 0.4934667164053997, 0.5136428147387904, 0.512163285436577, 0.5038972097402178, 0.5075553082887021, 0.49514178207466725, 0.49452862876087594, 0.5029647192142529, 0.5077408966738418, 0.5113708688227158, 0.5130115208013097, 0.49729127757376135, 0.4965306422230918, 0.4946524747257126, 0.511772578821502, 0.5013671062845092, 0.5004324123846086, 0.5163611010466208, 0.5202062139963971, 0.509542498009165, 0.5117522837729428, 0.5148816265207429, 0.5120234564362958, 0.504992897450591, 0.5161109365231498, 0.5160967730943051, 0.5106477640860574, 0.5173619461126168, 0.5028009994069957, 0.5108997795168914, 0.5019097631204061, 0.5104044557616697, 0.48382244995852425, 0.5105615989122977, 0.515419900084341, 0.5108770793043701, 0.509728985458779, 0.5174044067633219, 0.511961587647486, 0.5045223848779774, 0.5164159216694326, 0.5141356678981355, 0.5214711980446757, 0.5174653057279534, 0.5157501585989691, 0.5159891878759395, 0.5054987723933918, 0.5204527884222275, 0.5071144846564565, 0.5031592439672801, 0.5069285945852375, 0.5135487221472756, 0.5266018982706123, 0.5208983444634763, 0.5217680206844927, 0.4941646694804037, 0.5266679775115498, 0.5214535537378748, 0.5146078858628619, 0.4978981301105222, 0.50704742776615, 0.510418596880396, 0.5052255295508401, 0.5158236869220627, 0.515663593175025, 0.5043347715998495, 0.5086313769138059, 0.5013848829535799, 0.510857533809193, 0.5139874353089147, 0.49773058944574283, 0.525994892559904, 0.5221859586305458, 0.5320018829580125, 0.5115159944782044, 0.5123340365939966, 0.506349818666554, 0.5211434019677466, 0.5099125377958713, 0.5167222742261833, 0.5279169152568839, 0.5202494366874908, 0.5225506218452027, 0.5170770683768076, 0.5013558102053637, 0.5294516429554816, 0.5100744040318708, 0.5107205300357754, 0.5191157869786523, 0.5213227431867375, 0.5011163800122351, 0.5094528770979556, 0.5194579998874131, 0.5339548974063809, 0.5195256844579175, 0.51771973364846, 0.5055994431399766, 0.5141591512957099, 0.5142414040405657, 0.5076197082769938, 0.5263448540724855, 0.5220829484849002, 0.5182654378134445, 0.5269318083145099, 0.5264524431201999, 0.5078711631244788, 0.5156766795579282, 0.5192475815059087, 0.49687361117847806, 0.5253006526211786, 0.5195500967222885, 0.5171191589126374, 0.5250597862557992, 0.5171863942838913, 0.5084579785109898, 0.5171477854584848, 0.5155856659292509, 0.5155453423880998, 0.5151462601549799, 0.5191230056339136, 0.5147673837299453, 0.5149680439320357, 0.5183231192927121, 0.5183713531027959], 'loss': [136.27721876978882, 34.14981712642354, 15.898904872092539, 10.685215949129223, 8.395064922688242, 7.074522399388136, 6.163362165985722, 5.47883399149268, 4.961861106254886, 4.5492712019134025, 4.1737985291529585, 3.862168826306868, 3.587648044626643, 3.3447683499364653, 3.1269307375915116, 2.9425369970978075, 2.798985001308636, 2.6752790649513374, 2.560361029821169, 2.452346617558327, 2.363449656802983, 2.2798184233029666, 2.214456289125277, 2.152310553254609, 2.09264834885062, 2.046747818802652, 1.998979742052664, 1.9564063100408127, 1.9127215389838945, 1.879664141805491, 1.8418989312083274, 1.8164766190014505, 1.7947421999293458, 1.7633004154257055, 1.743857338566009, 1.7197630474376664, 1.7088346122611122, 1.6877506477342867, 1.6640271178310417, 1.6493361623943854, 1.6353712642863518, 1.6209102982588113, 1.5995261250423403, 1.5882719030411931, 1.5748667657897983, 1.5624983365171765, 1.5516682459595066, 1.5378983979923495, 1.5283524360761138, 1.519189687412384, 1.510567492772198, 1.5006167984081504, 1.4960063459709458, 1.4865986944692768, 1.4764114360223441, 1.477396283152446, 1.464774807419732, 1.457377168268207, 1.4462551022074555, 1.437266749251701, 1.435824686912441, 1.4249670480917005, 1.4191408590659889, 1.4204333219112157, 1.4108681169108597, 1.4031671100851952, 1.4034314818725329, 1.3949697109149384, 1.385916199827498, 1.386661504158481, 1.3771805407713846, 1.3749458658286842, 1.3686547942179998, 1.3674394344492717, 1.3673136593320194, 1.3592036109228784, 1.3541048812220853, 1.346957072863151, 1.34414310044605, 1.3427635846032302, 1.33682087501548, 1.3342819078368147, 1.3365392850950442, 1.3268632184224907, 1.3246494584058626, 1.324310674360132, 1.317992531170961, 1.3192036690019093, 1.3141276279725993, 1.3123760795315382, 1.3138314822090067, 1.3095526920249991, 1.3051035678839218, 1.303321894928817, 1.3030155634290543, 1.3012656334029402, 1.3002656965329493, 1.2924856349057305, 1.2946828077135637, 1.2892990816224472, 1.288533227157764, 1.2895996011548114, 1.2849214137765903, 1.2845457552253379, 1.2811281111893404, 1.2820797847296495, 1.2784216451778643, 1.2743981244302927, 1.2715061435959942, 1.272872932842436, 1.2740232681151626, 1.272198287388006, 1.2652544108463473, 1.2663644859489678, 1.2666816534958896, 1.2594825458107195, 1.2622264165074781, 1.2562196620908752, 1.256939202211085, 1.2546493874565656, 1.2532244798995709, 1.2507200711147828, 1.2488713414442707, 1.2488412055942748, 1.2466739509874527, 1.2469878633988007, 1.2449275455562632, 1.247140428935655, 1.2422887732225476, 1.2406218446526196, 1.2424228781676294, 1.239870533526486, 1.237766688841048, 1.2339063330346167, 1.2317958041756938, 1.2324431883854439, 1.2294922083734745, 1.2280153303252581, 1.2315879308426214, 1.2293472738348057, 1.2266619315880276, 1.2250597015730105, 1.2235736317555634, 1.2275555057889476, 1.2224714158374845, 1.2222513191113447, 1.2226764416196014, 1.2205808840379184, 1.2193591383364288, 1.2171067905435116, 1.2206575889147468, 1.2175777542468387, 1.2173757182442215, 1.21557087338209, 1.2125000575874165, 1.2122756554782925, 1.2094990518785083, 1.2103393717981048, 1.2080642751707023, 1.213235326159605, 1.208939127566008, 1.2083231609577019, 1.2046142014716485, 1.2052759089427578, 1.2021945364776114, 1.2010296125127875, 1.2012828058046257, 1.2048548934091041, 1.2012615102638924, 1.2024940203447811, 1.1995313024671501, 1.1960281148487157, 1.1975663343316543, 1.1960391569394677, 1.1944052417464885, 1.195749325554496, 1.1963255241197552], 'acc': [0.5873600235091699, 0.8860088905460479, 0.8862619519973852, 0.8862687397818635, 0.8862687395286549, 0.8862686381056434, 0.8862662437298311, 0.8862583472132586, 0.8863674345232551, 0.8870517894285618, 0.8878901290429988, 0.8894968182619514, 0.8940285472372286, 0.8978700795066389, 0.9019706750882165, 0.9071009577401654, 0.9129964850786549, 0.9178579550774735, 0.9217075857745557, 0.9245080541378856, 0.9256536153743005, 0.9266345977400144, 0.9273063043098643, 0.9283713430278241, 0.9293897287446061, 0.9304737733065007, 0.9312877216556481, 0.9323860959000112, 0.9332996418511035, 0.9338825434441466, 0.9347315566919318, 0.9352143376093403, 0.9356060508685489, 0.936062322226584, 0.936477686810757, 0.9367546344769494, 0.9368146588343835, 0.9372527989697907, 0.9376149495068306, 0.9378181811407704, 0.9379275875779501, 0.9381648773987578, 0.9385005219083591, 0.9386080388935567, 0.9389980804244324, 0.9393187972735907, 0.939515301146465, 0.9399139267265598, 0.9402497516147945, 0.9403208494024088, 0.9405675059141114, 0.9407220122945458, 0.9409336824606955, 0.9409451823914715, 0.9412283389392122, 0.9411527024077133, 0.9413577277970323, 0.9415161017466264, 0.9415935043883242, 0.9416832185266818, 0.9416203953836629, 0.9417872892882491, 0.9418654008846866, 0.9418932967998579, 0.9420564230779996, 0.9421056493403417, 0.9420917126570293, 0.9422987480483266, 0.9423332116448161, 0.942210814248541, 0.9422565130949553, 0.9422661204346124, 0.9424533584844923, 0.9423250930123999, 0.9423711371606202, 0.942476983735468, 0.9426630514813732, 0.9428628378298527, 0.9429842756565967, 0.9430157352809222, 0.9431268172737793, 0.9431994273949407, 0.9432838421184495, 0.9434667888027113, 0.9434956916081971, 0.9436224446948023, 0.9437792075854826, 0.9437606371044431, 0.9439398369740559, 0.9439893493945286, 0.9440635086845893, 0.9440649579854373, 0.9443572764270453, 0.9444193113740315, 0.9443855348037716, 0.9443846480771482, 0.944518652053695, 0.9447071746955036, 0.944679783265917, 0.9448624512000214, 0.944974438825741, 0.9449455347899213, 0.9450896509585379, 0.945284578799682, 0.945190540242251, 0.9453072596087149, 0.9453345122865054, 0.9456060617378182, 0.9457024792642948, 0.9456569820400624, 0.9457116458619981, 0.9456515626984999, 0.9459471626970206, 0.9459947594551795, 0.9460207621851658, 0.9461321469083291, 0.946116173253081, 0.9462935212981525, 0.9461670515713989, 0.9463839999510503, 0.9464226892120301, 0.9464588431704052, 0.9464218865636322, 0.9465233210215972, 0.9466061799311052, 0.9466226380717085, 0.9465747004915872, 0.9465751003598369, 0.9466504561187564, 0.9467485443185302, 0.9466757303612352, 0.9467235468881893, 0.9467842107205848, 0.9468797467696459, 0.9470178981465157, 0.9469127407112151, 0.94700353853999, 0.9470282507016764, 0.9469282052030128, 0.947097056409735, 0.9470823918070111, 0.9471201173821979, 0.9471551238513384, 0.9471373587260805, 0.9472010282803761, 0.9473239537852957, 0.9472637496270631, 0.9472639291486827, 0.9473048785200151, 0.9473736826860985, 0.9472529712562682, 0.947224393534859, 0.947409593453887, 0.9473873787628502, 0.9475665374146549, 0.9474835546569467, 0.947537495685321, 0.9476323023866149, 0.9475563667314088, 0.9474083054209189, 0.9476030979188438, 0.9476384861842849, 0.9476328669507988, 0.9477301104078755, 0.9477554089453906, 0.9477159512289066, 0.9477940797773323, 0.9477241680802723, 0.9476915185153241, 0.9477990930697593, 0.9478262255708738, 0.9479492909702741, 0.947959585904257, 0.9479152968224471, 0.9480108864640729, 0.9479020242402512, 0.947943028668213], 'mDice': [0.013124260899247795, 0.01388693687162474, 0.016667009988806488, 0.020572843910846975, 0.02571484968605907, 0.03259855875713257, 0.04246236916667372, 0.05587448601330309, 0.07123888240549198, 0.087658627103288, 0.10672405986953785, 0.12799043249385328, 0.15534379008347274, 0.18516656489314728, 0.21492611251361204, 0.242742468615281, 0.26674417982654086, 0.28928440308026787, 0.31038053514981967, 0.3313122310187067, 0.34939940012194065, 0.365723815003769, 0.37988375185504797, 0.393805319025693, 0.4063207665064205, 0.4175420035954804, 0.4279008864882491, 0.4381714011224213, 0.44816074873989337, 0.45540552784217403, 0.4640252567066047, 0.46999671443486085, 0.47477554210675876, 0.48219180778281684, 0.4874999654539127, 0.4925225955063816, 0.49553698352281644, 0.4999937214828526, 0.5060414338928637, 0.5095113237129695, 0.51365116539685, 0.5165420171384892, 0.5216371927325758, 0.5250350930456904, 0.5284707878261965, 0.5312996190699687, 0.5344777049195577, 0.5383883792391453, 0.540925950349192, 0.5426820148979421, 0.5452158817096536, 0.5481659027429118, 0.5498006784489365, 0.551111776989391, 0.5543215746395495, 0.5546696683263397, 0.5576163930549101, 0.5594949232331483, 0.5629938861642175, 0.5660279070614899, 0.5652926061662555, 0.5680917732810787, 0.570536414224931, 0.570232733753822, 0.5724273655391338, 0.5748064812735583, 0.5746967635456757, 0.577039544946338, 0.5796246401696584, 0.5797787481037063, 0.5816079210880555, 0.582796753040424, 0.5849003393395158, 0.5850277233666554, 0.58515702213417, 0.5866993251277187, 0.5883089778314364, 0.5908015538363095, 0.5910974122990866, 0.5919954749040826, 0.5936512060625336, 0.5940127446594194, 0.5936981516234736, 0.5964683408889367, 0.5973587050546223, 0.5971530057669634, 0.5987278743316377, 0.5985428750771439, 0.5998397468449301, 0.600084733796804, 0.6000135481880223, 0.601213761819525, 0.6026731964926478, 0.6033158354734285, 0.6032229523488388, 0.6032056495304636, 0.604167272969143, 0.6063000334808298, 0.605143895200392, 0.6074025119696883, 0.6075563416632423, 0.6068890467168433, 0.6082380279841022, 0.6083284300024567, 0.6093594181784994, 0.6095797587542172, 0.6099528091047709, 0.6116759645076016, 0.6126735322771882, 0.6120601023933504, 0.6120875777128794, 0.6119668283407208, 0.6142095632915643, 0.6138150712437545, 0.6141142428268646, 0.6159333306540267, 0.615500548022108, 0.617933475804364, 0.6168539120632737, 0.6178434979752656, 0.6179707861045342, 0.6187136807438335, 0.6188075823186822, 0.6191611463847903, 0.6198519341638292, 0.620069235782401, 0.6203377672409376, 0.6201811752238309, 0.6218325359539154, 0.6216031459413915, 0.621904017897864, 0.6220278475423646, 0.6225947433713669, 0.6242580266762621, 0.6244236142616777, 0.6243438013699389, 0.6253447672502842, 0.6260811376595028, 0.624600721336607, 0.6252824223890108, 0.6259767226771353, 0.6269069757832141, 0.6273242626488322, 0.6258772198855633, 0.6273491019861389, 0.6274841928084713, 0.6272711997834383, 0.6278658723702625, 0.628267261874318, 0.6289000104542463, 0.6281819593594112, 0.6287808629216131, 0.6290584112342086, 0.62970553197721, 0.6305814786976923, 0.6300353871750861, 0.6310785763384853, 0.6310895204018173, 0.6309432307396519, 0.6303773424307393, 0.6315112788331424, 0.6314975682228688, 0.6324000219609001, 0.6324154408784041, 0.6331869332490809, 0.6332905629667918, 0.6336714783549003, 0.6328643997324155, 0.6335939793218994, 0.6332860705730579, 0.6345943995053444, 0.6349796900516255, 0.6350774792348187, 0.635131797086218, 0.6358416126279632, 0.6350766483406818, 0.6351724024185408]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.74s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.46s/it]predicting test subjects: 100%|██████████| 3/3 [00:06<00:00,  2.17s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:29,  1.58s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:49,  1.66s/it]predicting train subjects:   1%|          | 3/285 [00:05<07:44,  1.65s/it]predicting train subjects:   1%|▏         | 4/285 [00:07<08:11,  1.75s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<08:13,  1.76s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<08:38,  1.86s/it]predicting train subjects:   2%|▏         | 7/285 [00:13<09:19,  2.01s/it]predicting train subjects:   3%|▎         | 8/285 [00:15<09:20,  2.02s/it]predicting train subjects:   3%|▎         | 9/285 [00:17<08:53,  1.93s/it]predicting train subjects:   4%|▎         | 10/285 [00:19<09:16,  2.02s/it]predicting train subjects:   4%|▍         | 11/285 [00:21<09:26,  2.07s/it]predicting train subjects:   4%|▍         | 12/285 [00:23<09:27,  2.08s/it]predicting train subjects:   5%|▍         | 13/285 [00:25<09:34,  2.11s/it]predicting train subjects:   5%|▍         | 14/285 [00:27<09:39,  2.14s/it]predicting train subjects:   5%|▌         | 15/285 [00:30<09:48,  2.18s/it]predicting train subjects:   6%|▌         | 16/285 [00:32<09:54,  2.21s/it]predicting train subjects:   6%|▌         | 17/285 [00:34<09:51,  2.21s/it]predicting train subjects:   6%|▋         | 18/285 [00:36<09:42,  2.18s/it]predicting train subjects:   7%|▋         | 19/285 [00:39<09:51,  2.22s/it]predicting train subjects:   7%|▋         | 20/285 [00:41<09:43,  2.20s/it]predicting train subjects:   7%|▋         | 21/285 [00:43<09:49,  2.23s/it]predicting train subjects:   8%|▊         | 22/285 [00:45<09:43,  2.22s/it]predicting train subjects:   8%|▊         | 23/285 [00:47<09:36,  2.20s/it]predicting train subjects:   8%|▊         | 24/285 [00:50<09:29,  2.18s/it]predicting train subjects:   9%|▉         | 25/285 [00:52<09:28,  2.19s/it]predicting train subjects:   9%|▉         | 26/285 [00:54<09:34,  2.22s/it]predicting train subjects:   9%|▉         | 27/285 [00:56<09:40,  2.25s/it]predicting train subjects:  10%|▉         | 28/285 [00:58<09:18,  2.17s/it]predicting train subjects:  10%|█         | 29/285 [01:00<09:01,  2.12s/it]predicting train subjects:  11%|█         | 30/285 [01:02<08:56,  2.10s/it]predicting train subjects:  11%|█         | 31/285 [01:04<08:46,  2.07s/it]predicting train subjects:  11%|█         | 32/285 [01:06<08:34,  2.03s/it]predicting train subjects:  12%|█▏        | 33/285 [01:09<09:21,  2.23s/it]predicting train subjects:  12%|█▏        | 34/285 [01:11<09:03,  2.17s/it]predicting train subjects:  12%|█▏        | 35/285 [01:13<08:45,  2.10s/it]predicting train subjects:  13%|█▎        | 36/285 [01:15<08:36,  2.07s/it]predicting train subjects:  13%|█▎        | 37/285 [01:17<08:28,  2.05s/it]predicting train subjects:  13%|█▎        | 38/285 [01:19<08:19,  2.02s/it]predicting train subjects:  14%|█▎        | 39/285 [01:21<08:04,  1.97s/it]predicting train subjects:  14%|█▍        | 40/285 [01:23<08:01,  1.96s/it]predicting train subjects:  14%|█▍        | 41/285 [01:25<07:56,  1.95s/it]predicting train subjects:  15%|█▍        | 42/285 [01:27<07:54,  1.95s/it]predicting train subjects:  15%|█▌        | 43/285 [01:29<07:53,  1.96s/it]predicting train subjects:  15%|█▌        | 44/285 [01:31<08:03,  2.01s/it]predicting train subjects:  16%|█▌        | 45/285 [01:33<08:01,  2.01s/it]predicting train subjects:  16%|█▌        | 46/285 [01:35<07:44,  1.94s/it]predicting train subjects:  16%|█▋        | 47/285 [01:36<07:29,  1.89s/it]predicting train subjects:  17%|█▋        | 48/285 [01:38<07:17,  1.84s/it]predicting train subjects:  17%|█▋        | 49/285 [01:40<07:12,  1.83s/it]predicting train subjects:  18%|█▊        | 50/285 [01:42<06:58,  1.78s/it]predicting train subjects:  18%|█▊        | 51/285 [01:43<06:55,  1.78s/it]predicting train subjects:  18%|█▊        | 52/285 [01:45<06:50,  1.76s/it]predicting train subjects:  19%|█▊        | 53/285 [01:47<06:53,  1.78s/it]predicting train subjects:  19%|█▉        | 54/285 [01:49<06:50,  1.78s/it]predicting train subjects:  19%|█▉        | 55/285 [01:50<06:49,  1.78s/it]predicting train subjects:  20%|█▉        | 56/285 [01:52<06:46,  1.77s/it]predicting train subjects:  20%|██        | 57/285 [01:54<06:42,  1.76s/it]predicting train subjects:  20%|██        | 58/285 [01:56<06:35,  1.74s/it]predicting train subjects:  21%|██        | 59/285 [01:58<06:56,  1.84s/it]predicting train subjects:  21%|██        | 60/285 [01:59<06:44,  1.80s/it]predicting train subjects:  21%|██▏       | 61/285 [02:01<06:38,  1.78s/it]predicting train subjects:  22%|██▏       | 62/285 [02:03<06:39,  1.79s/it]predicting train subjects:  22%|██▏       | 63/285 [02:05<06:37,  1.79s/it]predicting train subjects:  22%|██▏       | 64/285 [02:07<06:42,  1.82s/it]predicting train subjects:  23%|██▎       | 65/285 [02:09<06:55,  1.89s/it]predicting train subjects:  23%|██▎       | 66/285 [02:11<06:55,  1.90s/it]predicting train subjects:  24%|██▎       | 67/285 [02:12<06:43,  1.85s/it]predicting train subjects:  24%|██▍       | 68/285 [02:14<06:47,  1.88s/it]predicting train subjects:  24%|██▍       | 69/285 [02:16<06:46,  1.88s/it]predicting train subjects:  25%|██▍       | 70/285 [02:18<06:35,  1.84s/it]predicting train subjects:  25%|██▍       | 71/285 [02:20<06:28,  1.82s/it]predicting train subjects:  25%|██▌       | 72/285 [02:22<06:31,  1.84s/it]predicting train subjects:  26%|██▌       | 73/285 [02:23<06:23,  1.81s/it]predicting train subjects:  26%|██▌       | 74/285 [02:25<06:21,  1.81s/it]predicting train subjects:  26%|██▋       | 75/285 [02:27<06:22,  1.82s/it]predicting train subjects:  27%|██▋       | 76/285 [02:29<06:16,  1.80s/it]predicting train subjects:  27%|██▋       | 77/285 [02:30<06:09,  1.78s/it]predicting train subjects:  27%|██▋       | 78/285 [02:32<06:07,  1.78s/it]predicting train subjects:  28%|██▊       | 79/285 [02:34<06:04,  1.77s/it]predicting train subjects:  28%|██▊       | 80/285 [02:36<06:04,  1.78s/it]predicting train subjects:  28%|██▊       | 81/285 [02:37<05:59,  1.76s/it]predicting train subjects:  29%|██▉       | 82/285 [02:39<05:58,  1.77s/it]predicting train subjects:  29%|██▉       | 83/285 [02:41<05:59,  1.78s/it]predicting train subjects:  29%|██▉       | 84/285 [02:43<05:54,  1.77s/it]predicting train subjects:  30%|██▉       | 85/285 [02:45<06:03,  1.82s/it]predicting train subjects:  30%|███       | 86/285 [02:47<06:06,  1.84s/it]predicting train subjects:  31%|███       | 87/285 [02:49<06:13,  1.88s/it]predicting train subjects:  31%|███       | 88/285 [02:51<06:24,  1.95s/it]predicting train subjects:  31%|███       | 89/285 [02:53<06:28,  1.98s/it]predicting train subjects:  32%|███▏      | 90/285 [02:55<06:27,  1.99s/it]predicting train subjects:  32%|███▏      | 91/285 [02:57<06:22,  1.97s/it]predicting train subjects:  32%|███▏      | 92/285 [02:59<06:18,  1.96s/it]predicting train subjects:  33%|███▎      | 93/285 [03:01<06:16,  1.96s/it]predicting train subjects:  33%|███▎      | 94/285 [03:03<06:22,  2.00s/it]predicting train subjects:  33%|███▎      | 95/285 [03:05<06:14,  1.97s/it]predicting train subjects:  34%|███▎      | 96/285 [03:07<06:09,  1.96s/it]predicting train subjects:  34%|███▍      | 97/285 [03:08<06:05,  1.94s/it]predicting train subjects:  34%|███▍      | 98/285 [03:11<06:24,  2.06s/it]predicting train subjects:  35%|███▍      | 99/285 [03:13<06:19,  2.04s/it]predicting train subjects:  35%|███▌      | 100/285 [03:15<06:12,  2.02s/it]predicting train subjects:  35%|███▌      | 101/285 [03:17<06:08,  2.00s/it]predicting train subjects:  36%|███▌      | 102/285 [03:19<06:00,  1.97s/it]predicting train subjects:  36%|███▌      | 103/285 [03:21<05:56,  1.96s/it]predicting train subjects:  36%|███▋      | 104/285 [03:22<05:50,  1.94s/it]predicting train subjects:  37%|███▋      | 105/285 [03:24<05:45,  1.92s/it]predicting train subjects:  37%|███▋      | 106/285 [03:26<05:41,  1.91s/it]predicting train subjects:  38%|███▊      | 107/285 [03:28<05:44,  1.94s/it]predicting train subjects:  38%|███▊      | 108/285 [03:30<05:49,  1.97s/it]predicting train subjects:  38%|███▊      | 109/285 [03:32<05:47,  1.97s/it]predicting train subjects:  39%|███▊      | 110/285 [03:34<05:43,  1.96s/it]predicting train subjects:  39%|███▉      | 111/285 [03:36<05:42,  1.97s/it]predicting train subjects:  39%|███▉      | 112/285 [03:38<05:40,  1.97s/it]predicting train subjects:  40%|███▉      | 113/285 [03:40<05:38,  1.97s/it]predicting train subjects:  40%|████      | 114/285 [03:42<05:35,  1.96s/it]predicting train subjects:  40%|████      | 115/285 [03:44<05:34,  1.97s/it]predicting train subjects:  41%|████      | 116/285 [03:46<05:30,  1.96s/it]predicting train subjects:  41%|████      | 117/285 [03:48<05:27,  1.95s/it]predicting train subjects:  41%|████▏     | 118/285 [03:50<05:23,  1.94s/it]predicting train subjects:  42%|████▏     | 119/285 [03:52<05:24,  1.95s/it]predicting train subjects:  42%|████▏     | 120/285 [03:54<05:21,  1.95s/it]predicting train subjects:  42%|████▏     | 121/285 [03:56<05:25,  1.98s/it]predicting train subjects:  43%|████▎     | 122/285 [03:57<05:03,  1.86s/it]predicting train subjects:  43%|████▎     | 123/285 [03:59<04:46,  1.77s/it]predicting train subjects:  44%|████▎     | 124/285 [04:01<04:44,  1.77s/it]predicting train subjects:  44%|████▍     | 125/285 [04:02<04:40,  1.75s/it]predicting train subjects:  44%|████▍     | 126/285 [04:04<04:32,  1.72s/it]predicting train subjects:  45%|████▍     | 127/285 [04:06<04:31,  1.72s/it]predicting train subjects:  45%|████▍     | 128/285 [04:07<04:26,  1.70s/it]predicting train subjects:  45%|████▌     | 129/285 [04:09<04:27,  1.71s/it]predicting train subjects:  46%|████▌     | 130/285 [04:11<04:25,  1.72s/it]predicting train subjects:  46%|████▌     | 131/285 [04:13<04:38,  1.81s/it]predicting train subjects:  46%|████▋     | 132/285 [04:15<04:35,  1.80s/it]predicting train subjects:  47%|████▋     | 133/285 [04:16<04:34,  1.81s/it]predicting train subjects:  47%|████▋     | 134/285 [04:18<04:29,  1.79s/it]predicting train subjects:  47%|████▋     | 135/285 [04:20<04:27,  1.79s/it]predicting train subjects:  48%|████▊     | 136/285 [04:22<04:25,  1.78s/it]predicting train subjects:  48%|████▊     | 137/285 [04:23<04:20,  1.76s/it]predicting train subjects:  48%|████▊     | 138/285 [04:25<04:14,  1.73s/it]predicting train subjects:  49%|████▉     | 139/285 [04:27<04:17,  1.76s/it]predicting train subjects:  49%|████▉     | 140/285 [04:29<04:10,  1.72s/it]predicting train subjects:  49%|████▉     | 141/285 [04:30<04:13,  1.76s/it]predicting train subjects:  50%|████▉     | 142/285 [04:32<04:05,  1.72s/it]predicting train subjects:  50%|█████     | 143/285 [04:34<04:03,  1.72s/it]predicting train subjects:  51%|█████     | 144/285 [04:36<04:04,  1.73s/it]predicting train subjects:  51%|█████     | 145/285 [04:37<03:59,  1.71s/it]predicting train subjects:  51%|█████     | 146/285 [04:39<03:56,  1.70s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:41<03:56,  1.72s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:42<03:49,  1.68s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:44<03:44,  1.65s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:46<03:48,  1.69s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:47<03:43,  1.67s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:49<03:40,  1.66s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:50<03:35,  1.64s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:52<03:33,  1.63s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:54<03:28,  1.61s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:55<03:27,  1.61s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:57<03:25,  1.61s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:58<03:21,  1.59s/it]predicting train subjects:  56%|█████▌    | 159/285 [05:00<03:31,  1.68s/it]predicting train subjects:  56%|█████▌    | 160/285 [05:02<03:25,  1.64s/it]predicting train subjects:  56%|█████▋    | 161/285 [05:03<03:18,  1.60s/it]predicting train subjects:  57%|█████▋    | 162/285 [05:05<03:15,  1.59s/it]predicting train subjects:  57%|█████▋    | 163/285 [05:06<03:12,  1.58s/it]predicting train subjects:  58%|█████▊    | 164/285 [05:08<03:10,  1.57s/it]predicting train subjects:  58%|█████▊    | 165/285 [05:10<03:07,  1.56s/it]predicting train subjects:  58%|█████▊    | 166/285 [05:11<03:07,  1.58s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:13<03:04,  1.57s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:14<03:11,  1.64s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:16<03:08,  1.62s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:18<03:05,  1.61s/it]predicting train subjects:  60%|██████    | 171/285 [05:19<03:03,  1.61s/it]predicting train subjects:  60%|██████    | 172/285 [05:21<03:08,  1.66s/it]predicting train subjects:  61%|██████    | 173/285 [05:23<03:04,  1.65s/it]predicting train subjects:  61%|██████    | 174/285 [05:24<03:03,  1.65s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:26<02:58,  1.63s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:28<03:00,  1.66s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:29<02:56,  1.63s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:31<02:51,  1.61s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:32<02:50,  1.60s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:34<02:46,  1.59s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:35<02:43,  1.57s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:37<02:41,  1.57s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:39<02:38,  1.55s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:40<02:38,  1.57s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:42<02:36,  1.56s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:43<02:33,  1.55s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:45<02:33,  1.57s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:46<02:31,  1.56s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:48<02:29,  1.55s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:49<02:26,  1.54s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:51<02:24,  1.54s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:52<02:22,  1.53s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:54<02:19,  1.52s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:55<02:17,  1.51s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:57<02:16,  1.52s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:59<02:21,  1.59s/it]predicting train subjects:  69%|██████▉   | 197/285 [06:00<02:24,  1.64s/it]predicting train subjects:  69%|██████▉   | 198/285 [06:02<02:26,  1.68s/it]predicting train subjects:  70%|██████▉   | 199/285 [06:04<02:27,  1.71s/it]predicting train subjects:  70%|███████   | 200/285 [06:06<02:27,  1.74s/it]predicting train subjects:  71%|███████   | 201/285 [06:08<02:29,  1.79s/it]predicting train subjects:  71%|███████   | 202/285 [06:10<02:28,  1.79s/it]predicting train subjects:  71%|███████   | 203/285 [06:11<02:27,  1.79s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:13<02:24,  1.78s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:15<02:21,  1.77s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:17<02:20,  1.77s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:18<02:19,  1.79s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:20<02:18,  1.80s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:22<02:15,  1.79s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:24<02:13,  1.78s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:26<02:11,  1.77s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:27<02:09,  1.78s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:29<02:08,  1.79s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:31<02:02,  1.73s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:32<01:56,  1.66s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:34<01:51,  1.62s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:36<01:53,  1.67s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:37<01:49,  1.63s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:39<01:48,  1.65s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:40<01:45,  1.62s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:42<01:43,  1.62s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:43<01:40,  1.59s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:45<01:37,  1.58s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:47<01:36,  1.58s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:48<01:33,  1.56s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:50<01:32,  1.56s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:51<01:31,  1.58s/it]predicting train subjects:  80%|████████  | 228/285 [06:53<01:30,  1.59s/it]predicting train subjects:  80%|████████  | 229/285 [06:55<01:28,  1.59s/it]predicting train subjects:  81%|████████  | 230/285 [06:56<01:27,  1.60s/it]predicting train subjects:  81%|████████  | 231/285 [06:58<01:24,  1.57s/it]predicting train subjects:  81%|████████▏ | 232/285 [07:00<01:28,  1.67s/it]predicting train subjects:  82%|████████▏ | 233/285 [07:02<01:33,  1.79s/it]predicting train subjects:  82%|████████▏ | 234/285 [07:04<01:36,  1.89s/it]predicting train subjects:  82%|████████▏ | 235/285 [07:06<01:35,  1.90s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:08<01:33,  1.91s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:09<01:31,  1.90s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:12<01:31,  1.95s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:13<01:29,  1.94s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:15<01:27,  1.94s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:17<01:26,  1.96s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:20<01:26,  2.01s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:21<01:22,  1.97s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:23<01:21,  1.99s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:26<01:21,  2.03s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:28<01:20,  2.06s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:30<01:16,  2.02s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:32<01:13,  1.99s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:34<01:12,  2.03s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:35<01:05,  1.88s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:37<01:00,  1.77s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:38<00:55,  1.69s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:40<00:54,  1.69s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:41<00:51,  1.65s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:43<00:49,  1.66s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:45<00:47,  1.64s/it]predicting train subjects:  90%|█████████ | 257/285 [07:46<00:45,  1.62s/it]predicting train subjects:  91%|█████████ | 258/285 [07:48<00:42,  1.59s/it]predicting train subjects:  91%|█████████ | 259/285 [07:49<00:41,  1.59s/it]predicting train subjects:  91%|█████████ | 260/285 [07:51<00:38,  1.56s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:52<00:36,  1.53s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:54<00:35,  1.54s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:55<00:34,  1.55s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:57<00:32,  1.56s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:59<00:30,  1.54s/it]predicting train subjects:  93%|█████████▎| 266/285 [08:00<00:29,  1.56s/it]predicting train subjects:  94%|█████████▎| 267/285 [08:02<00:28,  1.56s/it]predicting train subjects:  94%|█████████▍| 268/285 [08:04<00:28,  1.69s/it]predicting train subjects:  94%|█████████▍| 269/285 [08:06<00:28,  1.80s/it]predicting train subjects:  95%|█████████▍| 270/285 [08:08<00:27,  1.86s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:10<00:26,  1.88s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:12<00:24,  1.91s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:14<00:23,  1.92s/it]predicting train subjects:  96%|█████████▌| 274/285 [08:16<00:21,  1.93s/it]predicting train subjects:  96%|█████████▋| 275/285 [08:18<00:19,  1.93s/it]predicting train subjects:  97%|█████████▋| 276/285 [08:20<00:17,  1.94s/it]predicting train subjects:  97%|█████████▋| 277/285 [08:21<00:15,  1.95s/it]predicting train subjects:  98%|█████████▊| 278/285 [08:23<00:13,  1.94s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:25<00:11,  1.94s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:27<00:09,  1.97s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:29<00:07,  1.97s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:31<00:05,  2.00s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:33<00:04,  2.01s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:35<00:02,  2.01s/it]predicting train subjects: 100%|██████████| 285/285 [08:37<00:00,  2.02s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:10,  1.30s/it]Loading train:   1%|          | 2/285 [00:02<06:33,  1.39s/it]Loading train:   1%|          | 3/285 [00:04<06:16,  1.34s/it]Loading train:   1%|▏         | 4/285 [00:05<06:40,  1.43s/it]Loading train:   2%|▏         | 5/285 [00:06<06:04,  1.30s/it]Loading train:   2%|▏         | 6/285 [00:08<06:20,  1.36s/it]Loading train:   2%|▏         | 7/285 [00:09<06:41,  1.44s/it]Loading train:   3%|▎         | 8/285 [00:11<06:45,  1.46s/it]Loading train:   3%|▎         | 9/285 [00:12<06:30,  1.41s/it]Loading train:   4%|▎         | 10/285 [00:13<06:09,  1.34s/it]Loading train:   4%|▍         | 11/285 [00:14<05:38,  1.24s/it]Loading train:   4%|▍         | 12/285 [00:15<05:09,  1.13s/it]Loading train:   5%|▍         | 13/285 [00:16<04:49,  1.07s/it]Loading train:   5%|▍         | 14/285 [00:17<04:39,  1.03s/it]Loading train:   5%|▌         | 15/285 [00:18<04:34,  1.02s/it]Loading train:   6%|▌         | 16/285 [00:19<04:29,  1.00s/it]Loading train:   6%|▌         | 17/285 [00:20<04:21,  1.03it/s]Loading train:   6%|▋         | 18/285 [00:21<04:19,  1.03it/s]Loading train:   7%|▋         | 19/285 [00:22<04:18,  1.03it/s]Loading train:   7%|▋         | 20/285 [00:23<04:18,  1.02it/s]Loading train:   7%|▋         | 21/285 [00:24<04:10,  1.05it/s]Loading train:   8%|▊         | 22/285 [00:25<04:17,  1.02it/s]Loading train:   8%|▊         | 23/285 [00:26<04:17,  1.02it/s]Loading train:   8%|▊         | 24/285 [00:27<04:19,  1.01it/s]Loading train:   9%|▉         | 25/285 [00:28<04:16,  1.01it/s]Loading train:   9%|▉         | 26/285 [00:29<04:12,  1.03it/s]Loading train:   9%|▉         | 27/285 [00:30<04:11,  1.02it/s]Loading train:  10%|▉         | 28/285 [00:31<04:06,  1.04it/s]Loading train:  10%|█         | 29/285 [00:32<04:03,  1.05it/s]Loading train:  11%|█         | 30/285 [00:33<04:08,  1.03it/s]Loading train:  11%|█         | 31/285 [00:34<04:08,  1.02it/s]Loading train:  11%|█         | 32/285 [00:35<04:04,  1.04it/s]Loading train:  12%|█▏        | 33/285 [00:35<03:56,  1.07it/s]Loading train:  12%|█▏        | 34/285 [00:36<03:50,  1.09it/s]Loading train:  12%|█▏        | 35/285 [00:37<03:44,  1.11it/s]Loading train:  13%|█▎        | 36/285 [00:38<03:42,  1.12it/s]Loading train:  13%|█▎        | 37/285 [00:39<03:36,  1.15it/s]Loading train:  13%|█▎        | 38/285 [00:40<03:40,  1.12it/s]Loading train:  14%|█▎        | 39/285 [00:41<03:40,  1.12it/s]Loading train:  14%|█▍        | 40/285 [00:42<03:38,  1.12it/s]Loading train:  14%|█▍        | 41/285 [00:43<03:43,  1.09it/s]Loading train:  15%|█▍        | 42/285 [00:43<03:43,  1.09it/s]Loading train:  15%|█▌        | 43/285 [00:44<03:44,  1.08it/s]Loading train:  15%|█▌        | 44/285 [00:45<03:35,  1.12it/s]Loading train:  16%|█▌        | 45/285 [00:46<03:34,  1.12it/s]Loading train:  16%|█▌        | 46/285 [00:47<03:35,  1.11it/s]Loading train:  16%|█▋        | 47/285 [00:48<03:23,  1.17it/s]Loading train:  17%|█▋        | 48/285 [00:49<03:16,  1.21it/s]Loading train:  17%|█▋        | 49/285 [00:49<03:20,  1.18it/s]Loading train:  18%|█▊        | 50/285 [00:50<03:22,  1.16it/s]Loading train:  18%|█▊        | 51/285 [00:51<03:17,  1.18it/s]Loading train:  18%|█▊        | 52/285 [00:52<03:16,  1.19it/s]Loading train:  19%|█▊        | 53/285 [00:53<03:20,  1.16it/s]Loading train:  19%|█▉        | 54/285 [00:54<03:25,  1.12it/s]Loading train:  19%|█▉        | 55/285 [00:55<03:19,  1.15it/s]Loading train:  20%|█▉        | 56/285 [00:56<03:16,  1.17it/s]Loading train:  20%|██        | 57/285 [00:56<03:14,  1.17it/s]Loading train:  20%|██        | 58/285 [00:57<03:11,  1.19it/s]Loading train:  21%|██        | 59/285 [00:58<03:09,  1.19it/s]Loading train:  21%|██        | 60/285 [00:59<03:10,  1.18it/s]Loading train:  21%|██▏       | 61/285 [01:00<03:14,  1.15it/s]Loading train:  22%|██▏       | 62/285 [01:01<03:15,  1.14it/s]Loading train:  22%|██▏       | 63/285 [01:01<03:10,  1.17it/s]Loading train:  22%|██▏       | 64/285 [01:03<03:45,  1.02s/it]Loading train:  23%|██▎       | 65/285 [01:04<04:16,  1.17s/it]Loading train:  23%|██▎       | 66/285 [01:06<04:26,  1.22s/it]Loading train:  24%|██▎       | 67/285 [01:07<03:57,  1.09s/it]Loading train:  24%|██▍       | 68/285 [01:07<03:35,  1.01it/s]Loading train:  24%|██▍       | 69/285 [01:08<03:21,  1.07it/s]Loading train:  25%|██▍       | 70/285 [01:09<03:13,  1.11it/s]Loading train:  25%|██▍       | 71/285 [01:10<03:08,  1.14it/s]Loading train:  25%|██▌       | 72/285 [01:10<02:57,  1.20it/s]Loading train:  26%|██▌       | 73/285 [01:11<02:56,  1.20it/s]Loading train:  26%|██▌       | 74/285 [01:12<02:53,  1.22it/s]Loading train:  26%|██▋       | 75/285 [01:13<02:51,  1.22it/s]Loading train:  27%|██▋       | 76/285 [01:14<02:53,  1.21it/s]Loading train:  27%|██▋       | 77/285 [01:15<02:55,  1.18it/s]Loading train:  27%|██▋       | 78/285 [01:15<02:54,  1.19it/s]Loading train:  28%|██▊       | 79/285 [01:16<02:50,  1.21it/s]Loading train:  28%|██▊       | 80/285 [01:17<02:48,  1.22it/s]Loading train:  28%|██▊       | 81/285 [01:18<02:41,  1.26it/s]Loading train:  29%|██▉       | 82/285 [01:19<02:41,  1.26it/s]Loading train:  29%|██▉       | 83/285 [01:20<02:48,  1.20it/s]Loading train:  29%|██▉       | 84/285 [01:20<02:43,  1.23it/s]Loading train:  30%|██▉       | 85/285 [01:21<02:51,  1.16it/s]Loading train:  30%|███       | 86/285 [01:22<02:53,  1.15it/s]Loading train:  31%|███       | 87/285 [01:23<02:56,  1.12it/s]Loading train:  31%|███       | 88/285 [01:24<02:56,  1.11it/s]Loading train:  31%|███       | 89/285 [01:25<02:53,  1.13it/s]Loading train:  32%|███▏      | 90/285 [01:26<02:52,  1.13it/s]Loading train:  32%|███▏      | 91/285 [01:27<02:54,  1.11it/s]Loading train:  32%|███▏      | 92/285 [01:28<02:51,  1.12it/s]Loading train:  33%|███▎      | 93/285 [01:28<02:49,  1.13it/s]Loading train:  33%|███▎      | 94/285 [01:29<02:49,  1.13it/s]Loading train:  33%|███▎      | 95/285 [01:30<02:49,  1.12it/s]Loading train:  34%|███▎      | 96/285 [01:31<02:49,  1.12it/s]Loading train:  34%|███▍      | 97/285 [01:32<02:50,  1.10it/s]Loading train:  34%|███▍      | 98/285 [01:33<02:43,  1.14it/s]Loading train:  35%|███▍      | 99/285 [01:34<02:44,  1.13it/s]Loading train:  35%|███▌      | 100/285 [01:35<02:51,  1.08it/s]Loading train:  35%|███▌      | 101/285 [01:36<02:55,  1.05it/s]Loading train:  36%|███▌      | 102/285 [01:37<02:51,  1.07it/s]Loading train:  36%|███▌      | 103/285 [01:38<02:50,  1.07it/s]Loading train:  36%|███▋      | 104/285 [01:39<02:47,  1.08it/s]Loading train:  37%|███▋      | 105/285 [01:39<02:39,  1.13it/s]Loading train:  37%|███▋      | 106/285 [01:40<02:37,  1.14it/s]Loading train:  38%|███▊      | 107/285 [01:41<02:38,  1.12it/s]Loading train:  38%|███▊      | 108/285 [01:42<02:41,  1.09it/s]Loading train:  38%|███▊      | 109/285 [01:43<02:41,  1.09it/s]Loading train:  39%|███▊      | 110/285 [01:44<02:38,  1.10it/s]Loading train:  39%|███▉      | 111/285 [01:45<02:39,  1.09it/s]Loading train:  39%|███▉      | 112/285 [01:46<02:41,  1.07it/s]Loading train:  40%|███▉      | 113/285 [01:47<02:42,  1.06it/s]Loading train:  40%|████      | 114/285 [01:48<02:41,  1.06it/s]Loading train:  40%|████      | 115/285 [01:49<02:39,  1.06it/s]Loading train:  41%|████      | 116/285 [01:50<02:37,  1.07it/s]Loading train:  41%|████      | 117/285 [01:50<02:35,  1.08it/s]Loading train:  41%|████▏     | 118/285 [01:51<02:38,  1.05it/s]Loading train:  42%|████▏     | 119/285 [01:53<02:42,  1.02it/s]Loading train:  42%|████▏     | 120/285 [01:54<02:42,  1.02it/s]Loading train:  42%|████▏     | 121/285 [01:55<02:57,  1.08s/it]Loading train:  43%|████▎     | 122/285 [01:56<03:05,  1.14s/it]Loading train:  43%|████▎     | 123/285 [01:57<03:07,  1.16s/it]Loading train:  44%|████▎     | 124/285 [01:58<02:57,  1.10s/it]Loading train:  44%|████▍     | 125/285 [01:59<02:43,  1.02s/it]Loading train:  44%|████▍     | 126/285 [02:00<02:29,  1.06it/s]Loading train:  45%|████▍     | 127/285 [02:01<02:22,  1.11it/s]Loading train:  45%|████▍     | 128/285 [02:02<02:18,  1.13it/s]Loading train:  45%|████▌     | 129/285 [02:02<02:15,  1.15it/s]Loading train:  46%|████▌     | 130/285 [02:03<02:12,  1.17it/s]Loading train:  46%|████▌     | 131/285 [02:04<02:10,  1.18it/s]Loading train:  46%|████▋     | 132/285 [02:05<02:08,  1.19it/s]Loading train:  47%|████▋     | 133/285 [02:06<02:04,  1.22it/s]Loading train:  47%|████▋     | 134/285 [02:06<02:02,  1.23it/s]Loading train:  47%|████▋     | 135/285 [02:07<02:04,  1.21it/s]Loading train:  48%|████▊     | 136/285 [02:08<02:03,  1.20it/s]Loading train:  48%|████▊     | 137/285 [02:09<02:02,  1.21it/s]Loading train:  48%|████▊     | 138/285 [02:10<02:04,  1.18it/s]Loading train:  49%|████▉     | 139/285 [02:11<02:03,  1.18it/s]Loading train:  49%|████▉     | 140/285 [02:11<01:57,  1.23it/s]Loading train:  49%|████▉     | 141/285 [02:12<01:59,  1.20it/s]Loading train:  50%|████▉     | 142/285 [02:13<02:03,  1.16it/s]Loading train:  50%|█████     | 143/285 [02:14<01:57,  1.21it/s]Loading train:  51%|█████     | 144/285 [02:15<01:55,  1.22it/s]Loading train:  51%|█████     | 145/285 [02:16<01:54,  1.23it/s]Loading train:  51%|█████     | 146/285 [02:16<01:52,  1.24it/s]Loading train:  52%|█████▏    | 147/285 [02:17<01:48,  1.27it/s]Loading train:  52%|█████▏    | 148/285 [02:18<01:47,  1.27it/s]Loading train:  52%|█████▏    | 149/285 [02:19<01:48,  1.26it/s]Loading train:  53%|█████▎    | 150/285 [02:20<01:49,  1.24it/s]Loading train:  53%|█████▎    | 151/285 [02:20<01:48,  1.24it/s]Loading train:  53%|█████▎    | 152/285 [02:21<01:47,  1.24it/s]Loading train:  54%|█████▎    | 153/285 [02:22<01:46,  1.24it/s]Loading train:  54%|█████▍    | 154/285 [02:23<01:43,  1.26it/s]Loading train:  54%|█████▍    | 155/285 [02:23<01:42,  1.27it/s]Loading train:  55%|█████▍    | 156/285 [02:24<01:42,  1.26it/s]Loading train:  55%|█████▌    | 157/285 [02:25<01:45,  1.21it/s]Loading train:  55%|█████▌    | 158/285 [02:26<01:46,  1.20it/s]Loading train:  56%|█████▌    | 159/285 [02:27<01:44,  1.21it/s]Loading train:  56%|█████▌    | 160/285 [02:28<01:46,  1.17it/s]Loading train:  56%|█████▋    | 161/285 [02:29<01:42,  1.21it/s]Loading train:  57%|█████▋    | 162/285 [02:29<01:37,  1.26it/s]Loading train:  57%|█████▋    | 163/285 [02:30<01:34,  1.29it/s]Loading train:  58%|█████▊    | 164/285 [02:31<01:32,  1.31it/s]Loading train:  58%|█████▊    | 165/285 [02:31<01:30,  1.32it/s]Loading train:  58%|█████▊    | 166/285 [02:32<01:29,  1.33it/s]Loading train:  59%|█████▊    | 167/285 [02:33<01:26,  1.36it/s]Loading train:  59%|█████▉    | 168/285 [02:34<01:25,  1.37it/s]Loading train:  59%|█████▉    | 169/285 [02:34<01:24,  1.37it/s]Loading train:  60%|█████▉    | 170/285 [02:35<01:24,  1.35it/s]Loading train:  60%|██████    | 171/285 [02:36<01:26,  1.32it/s]Loading train:  60%|██████    | 172/285 [02:37<01:26,  1.30it/s]Loading train:  61%|██████    | 173/285 [02:37<01:27,  1.29it/s]Loading train:  61%|██████    | 174/285 [02:38<01:26,  1.29it/s]Loading train:  61%|██████▏   | 175/285 [02:39<01:26,  1.27it/s]Loading train:  62%|██████▏   | 176/285 [02:40<01:26,  1.26it/s]Loading train:  62%|██████▏   | 177/285 [02:41<01:29,  1.21it/s]Loading train:  62%|██████▏   | 178/285 [02:42<01:26,  1.23it/s]Loading train:  63%|██████▎   | 179/285 [02:42<01:24,  1.25it/s]Loading train:  63%|██████▎   | 180/285 [02:43<01:21,  1.28it/s]Loading train:  64%|██████▎   | 181/285 [02:44<01:19,  1.31it/s]Loading train:  64%|██████▍   | 182/285 [02:45<01:16,  1.34it/s]Loading train:  64%|██████▍   | 183/285 [02:45<01:19,  1.28it/s]Loading train:  65%|██████▍   | 184/285 [02:46<01:15,  1.33it/s]Loading train:  65%|██████▍   | 185/285 [02:47<01:15,  1.33it/s]Loading train:  65%|██████▌   | 186/285 [02:48<01:14,  1.33it/s]Loading train:  66%|██████▌   | 187/285 [02:48<01:12,  1.35it/s]Loading train:  66%|██████▌   | 188/285 [02:49<01:12,  1.35it/s]Loading train:  66%|██████▋   | 189/285 [02:50<01:12,  1.33it/s]Loading train:  67%|██████▋   | 190/285 [02:51<01:10,  1.34it/s]Loading train:  67%|██████▋   | 191/285 [02:51<01:10,  1.33it/s]Loading train:  67%|██████▋   | 192/285 [02:52<01:09,  1.35it/s]Loading train:  68%|██████▊   | 193/285 [02:53<01:07,  1.37it/s]Loading train:  68%|██████▊   | 194/285 [02:53<01:04,  1.40it/s]Loading train:  68%|██████▊   | 195/285 [02:54<01:03,  1.42it/s]Loading train:  69%|██████▉   | 196/285 [02:55<01:05,  1.35it/s]Loading train:  69%|██████▉   | 197/285 [02:56<01:06,  1.31it/s]Loading train:  69%|██████▉   | 198/285 [02:57<01:10,  1.23it/s]Loading train:  70%|██████▉   | 199/285 [02:57<01:09,  1.24it/s]Loading train:  70%|███████   | 200/285 [02:58<01:08,  1.24it/s]Loading train:  71%|███████   | 201/285 [02:59<01:07,  1.25it/s]Loading train:  71%|███████   | 202/285 [03:00<01:07,  1.22it/s]Loading train:  71%|███████   | 203/285 [03:01<01:07,  1.21it/s]Loading train:  72%|███████▏  | 204/285 [03:02<01:07,  1.21it/s]Loading train:  72%|███████▏  | 205/285 [03:02<01:04,  1.23it/s]Loading train:  72%|███████▏  | 206/285 [03:03<01:04,  1.22it/s]Loading train:  73%|███████▎  | 207/285 [03:04<01:03,  1.22it/s]Loading train:  73%|███████▎  | 208/285 [03:05<01:02,  1.24it/s]Loading train:  73%|███████▎  | 209/285 [03:06<01:01,  1.24it/s]Loading train:  74%|███████▎  | 210/285 [03:06<01:00,  1.24it/s]Loading train:  74%|███████▍  | 211/285 [03:07<00:59,  1.24it/s]Loading train:  74%|███████▍  | 212/285 [03:08<00:59,  1.22it/s]Loading train:  75%|███████▍  | 213/285 [03:09<00:59,  1.22it/s]Loading train:  75%|███████▌  | 214/285 [03:10<00:56,  1.25it/s]Loading train:  75%|███████▌  | 215/285 [03:10<00:54,  1.29it/s]Loading train:  76%|███████▌  | 216/285 [03:11<00:52,  1.33it/s]Loading train:  76%|███████▌  | 217/285 [03:12<00:52,  1.30it/s]Loading train:  76%|███████▋  | 218/285 [03:13<00:51,  1.29it/s]Loading train:  77%|███████▋  | 219/285 [03:13<00:49,  1.34it/s]Loading train:  77%|███████▋  | 220/285 [03:14<00:48,  1.34it/s]Loading train:  78%|███████▊  | 221/285 [03:15<00:48,  1.32it/s]Loading train:  78%|███████▊  | 222/285 [03:16<00:46,  1.36it/s]Loading train:  78%|███████▊  | 223/285 [03:16<00:46,  1.32it/s]Loading train:  79%|███████▊  | 224/285 [03:17<00:45,  1.34it/s]Loading train:  79%|███████▉  | 225/285 [03:18<00:44,  1.34it/s]Loading train:  79%|███████▉  | 226/285 [03:19<00:45,  1.30it/s]Loading train:  80%|███████▉  | 227/285 [03:19<00:43,  1.33it/s]Loading train:  80%|████████  | 228/285 [03:20<00:43,  1.31it/s]Loading train:  80%|████████  | 229/285 [03:21<00:45,  1.24it/s]Loading train:  81%|████████  | 230/285 [03:22<00:43,  1.27it/s]Loading train:  81%|████████  | 231/285 [03:23<00:41,  1.29it/s]Loading train:  81%|████████▏ | 232/285 [03:24<00:46,  1.13it/s]Loading train:  82%|████████▏ | 233/285 [03:25<00:47,  1.08it/s]Loading train:  82%|████████▏ | 234/285 [03:26<00:47,  1.08it/s]Loading train:  82%|████████▏ | 235/285 [03:27<00:47,  1.06it/s]Loading train:  83%|████████▎ | 236/285 [03:28<00:47,  1.04it/s]Loading train:  83%|████████▎ | 237/285 [03:29<00:45,  1.06it/s]Loading train:  84%|████████▎ | 238/285 [03:29<00:44,  1.06it/s]Loading train:  84%|████████▍ | 239/285 [03:30<00:44,  1.03it/s]Loading train:  84%|████████▍ | 240/285 [03:31<00:42,  1.05it/s]Loading train:  85%|████████▍ | 241/285 [03:32<00:42,  1.04it/s]Loading train:  85%|████████▍ | 242/285 [03:33<00:41,  1.03it/s]Loading train:  85%|████████▌ | 243/285 [03:34<00:41,  1.01it/s]Loading train:  86%|████████▌ | 244/285 [03:35<00:39,  1.04it/s]Loading train:  86%|████████▌ | 245/285 [03:36<00:37,  1.05it/s]Loading train:  86%|████████▋ | 246/285 [03:37<00:38,  1.00it/s]Loading train:  87%|████████▋ | 247/285 [03:38<00:37,  1.02it/s]Loading train:  87%|████████▋ | 248/285 [03:39<00:36,  1.02it/s]Loading train:  87%|████████▋ | 249/285 [03:40<00:35,  1.03it/s]Loading train:  88%|████████▊ | 250/285 [03:41<00:34,  1.02it/s]Loading train:  88%|████████▊ | 251/285 [03:42<00:30,  1.11it/s]Loading train:  88%|████████▊ | 252/285 [03:43<00:28,  1.17it/s]Loading train:  89%|████████▉ | 253/285 [03:43<00:26,  1.20it/s]Loading train:  89%|████████▉ | 254/285 [03:44<00:25,  1.24it/s]Loading train:  89%|████████▉ | 255/285 [03:45<00:23,  1.29it/s]Loading train:  90%|████████▉ | 256/285 [03:46<00:23,  1.26it/s]Loading train:  90%|█████████ | 257/285 [03:46<00:21,  1.31it/s]Loading train:  91%|█████████ | 258/285 [03:47<00:20,  1.33it/s]Loading train:  91%|█████████ | 259/285 [03:48<00:19,  1.32it/s]Loading train:  91%|█████████ | 260/285 [03:49<00:18,  1.35it/s]Loading train:  92%|█████████▏| 261/285 [03:49<00:17,  1.36it/s]Loading train:  92%|█████████▏| 262/285 [03:50<00:17,  1.30it/s]Loading train:  92%|█████████▏| 263/285 [03:51<00:16,  1.30it/s]Loading train:  93%|█████████▎| 264/285 [03:52<00:15,  1.33it/s]Loading train:  93%|█████████▎| 265/285 [03:53<00:15,  1.25it/s]Loading train:  93%|█████████▎| 266/285 [03:53<00:15,  1.26it/s]Loading train:  94%|█████████▎| 267/285 [03:54<00:13,  1.30it/s]Loading train:  94%|█████████▍| 268/285 [03:55<00:14,  1.16it/s]Loading train:  94%|█████████▍| 269/285 [03:56<00:14,  1.14it/s]Loading train:  95%|█████████▍| 270/285 [03:57<00:14,  1.05it/s]Loading train:  95%|█████████▌| 271/285 [03:58<00:13,  1.06it/s]Loading train:  95%|█████████▌| 272/285 [03:59<00:12,  1.05it/s]Loading train:  96%|█████████▌| 273/285 [04:00<00:11,  1.06it/s]Loading train:  96%|█████████▌| 274/285 [04:01<00:10,  1.04it/s]Loading train:  96%|█████████▋| 275/285 [04:02<00:09,  1.04it/s]Loading train:  97%|█████████▋| 276/285 [04:03<00:09,  1.01s/it]Loading train:  97%|█████████▋| 277/285 [04:04<00:07,  1.00it/s]Loading train:  98%|█████████▊| 278/285 [04:05<00:07,  1.01s/it]Loading train:  98%|█████████▊| 279/285 [04:06<00:06,  1.00s/it]Loading train:  98%|█████████▊| 280/285 [04:07<00:05,  1.02s/it]Loading train:  99%|█████████▊| 281/285 [04:08<00:03,  1.01it/s]Loading train:  99%|█████████▉| 282/285 [04:09<00:03,  1.01s/it]Loading train:  99%|█████████▉| 283/285 [04:10<00:01,  1.02it/s]Loading train: 100%|█████████▉| 284/285 [04:11<00:00,  1.02it/s]Loading train: 100%|██████████| 285/285 [04:12<00:00,  1.06it/s]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   8%|▊         | 24/285 [00:00<00:01, 239.64it/s]concatenating: train:  19%|█▊        | 53/285 [00:00<00:00, 251.35it/s]concatenating: train:  29%|██▉       | 82/285 [00:00<00:00, 260.00it/s]concatenating: train:  40%|████      | 115/285 [00:00<00:00, 275.99it/s]concatenating: train:  48%|████▊     | 138/285 [00:00<00:00, 235.73it/s]concatenating: train:  56%|█████▌    | 160/285 [00:00<00:00, 229.42it/s]concatenating: train:  65%|██████▍   | 184/285 [00:00<00:00, 231.62it/s]concatenating: train:  76%|███████▌  | 216/285 [00:00<00:00, 251.04it/s]concatenating: train:  87%|████████▋ | 248/285 [00:00<00:00, 259.94it/s]concatenating: train:  96%|█████████▌| 274/285 [00:01<00:00, 173.04it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 228.20it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.29s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.28s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 469.23it/s]2019-07-06 21:09:04.259567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 21:09:04.259657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 21:09:04.259672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 21:09:04.259681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 21:09:04.260103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:11,  3.43it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:08,  4.23it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.31it/s]loading the weights for Unet:  20%|██        | 8/40 [00:00<00:05,  5.53it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:06,  4.92it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.59it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  5.13it/s]loading the weights for Unet:  40%|████      | 16/40 [00:01<00:03,  6.40it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.08it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.78it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  5.05it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:02<00:02,  6.23it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.70it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  5.16it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:02,  5.87it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:03<00:02,  5.14it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  6.20it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  6.73it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.45it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  5.95it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  4.75it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  7.51it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 80, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 80, 52, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 80, 52, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 80, 52, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 80, 52, 20)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 80, 52, 20)   3620        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 80, 52, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 80, 52, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 80, 52, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 80, 52, 20)   3620        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 80, 52, 20)   80          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 80, 52, 20)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 80, 52, 20)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 80, 52, 10)   1810        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 80, 52, 10)   40          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 80, 52, 10)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 80, 52, 10)   910         activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 80, 52, 10)   40          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 80, 52, 10)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 40, 26, 10)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 40, 26, 10)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 40, 26, 20)   1820        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 40, 26, 20)   80          conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 40, 26, 20)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 40, 26, 20)   3620        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 40, 26, 20)   80          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 40, 26, 20)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 20, 13, 20)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 20, 13, 20)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 20, 13, 40)   7240        dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 20, 13, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 20, 13, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 13, 40)   14440       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 20, 13, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 20, 13, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 20, 13, 40)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 40, 26, 20)   3220        dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 40, 26, 40)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 40, 26, 20)   7220        concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 40, 26, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 40, 26, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 40, 26, 20)   3620        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 40, 26, 20)   80          conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 40, 26, 20)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 40, 26, 20)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 80, 52, 10)   810         dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 80, 52, 20)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 80, 52, 10)   1810        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 80, 52, 10)   40          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 80, 52, 10)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 80, 52, 10)   910         activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 80, 52, 10)   40          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 80, 52, 10)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 80, 52, 10)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 80, 52, 20)   1820        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 80, 52, 20)   80          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 80, 52, 20)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 80, 52, 20)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 80, 52, 13)   273         dropout_9[0][0]                  
==================================================================================================
Total params: 58,083
Trainable params: 21,783
Non-trainable params: 36,300
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.48913484e-02 3.19509754e-02 7.49897764e-02 9.32025064e-03
 2.70904632e-02 7.06417031e-03 8.46180096e-02 1.12618024e-01
 8.60108482e-02 1.32459736e-02 2.94100802e-01 1.93843398e-01
 2.55960049e-04]
Train on 10843 samples, validate on 104 samples
Epoch 1/300
 - 19s - loss: 275.7896 - acc: 0.5792 - mDice: 0.0186 - val_loss: 162.1448 - val_acc: 0.9034 - val_mDice: 0.0204

Epoch 00001: val_mDice improved from -inf to 0.02039, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 10s - loss: 119.0108 - acc: 0.8537 - mDice: 0.0235 - val_loss: 79.5117 - val_acc: 0.9034 - val_mDice: 0.0247

Epoch 00002: val_mDice improved from 0.02039 to 0.02467, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 10s - loss: 59.3327 - acc: 0.8531 - mDice: 0.0266 - val_loss: 37.5659 - val_acc: 0.9034 - val_mDice: 0.0271

Epoch 00003: val_mDice improved from 0.02467 to 0.02707, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 9s - loss: 35.5599 - acc: 0.8677 - mDice: 0.0287 - val_loss: 19.0596 - val_acc: 0.9034 - val_mDice: 0.0300

Epoch 00004: val_mDice improved from 0.02707 to 0.03000, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 10s - loss: 24.7827 - acc: 0.8745 - mDice: 0.0310 - val_loss: 12.4953 - val_acc: 0.9034 - val_mDice: 0.0305

Epoch 00005: val_mDice improved from 0.03000 to 0.03049, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 10s - loss: 19.2490 - acc: 0.8748 - mDice: 0.0336 - val_loss: 10.7527 - val_acc: 0.9034 - val_mDice: 0.0294

Epoch 00006: val_mDice did not improve from 0.03049
Epoch 7/300
 - 10s - loss: 15.8914 - acc: 0.8748 - mDice: 0.0370 - val_loss: 9.9244 - val_acc: 0.9034 - val_mDice: 0.0357

Epoch 00007: val_mDice improved from 0.03049 to 0.03571, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 10s - loss: 13.6992 - acc: 0.8748 - mDice: 0.0413 - val_loss: 9.0491 - val_acc: 0.9034 - val_mDice: 0.0458

Epoch 00008: val_mDice improved from 0.03571 to 0.04581, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 10s - loss: 12.3099 - acc: 0.8748 - mDice: 0.0461 - val_loss: 8.2856 - val_acc: 0.9034 - val_mDice: 0.0564

Epoch 00009: val_mDice improved from 0.04581 to 0.05642, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 10s - loss: 11.3007 - acc: 0.8751 - mDice: 0.0513 - val_loss: 7.5816 - val_acc: 0.9034 - val_mDice: 0.0634

Epoch 00010: val_mDice improved from 0.05642 to 0.06336, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 10s - loss: 10.5487 - acc: 0.8757 - mDice: 0.0558 - val_loss: 7.3010 - val_acc: 0.9034 - val_mDice: 0.0709

Epoch 00011: val_mDice improved from 0.06336 to 0.07093, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 9s - loss: 9.9704 - acc: 0.8769 - mDice: 0.0601 - val_loss: 7.2596 - val_acc: 0.9034 - val_mDice: 0.0699

Epoch 00012: val_mDice did not improve from 0.07093
Epoch 13/300
 - 10s - loss: 9.4674 - acc: 0.8765 - mDice: 0.0632 - val_loss: 7.0412 - val_acc: 0.9034 - val_mDice: 0.0741

Epoch 00013: val_mDice improved from 0.07093 to 0.07408, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 10s - loss: 9.0407 - acc: 0.8754 - mDice: 0.0673 - val_loss: 6.9052 - val_acc: 0.9034 - val_mDice: 0.0815

Epoch 00014: val_mDice improved from 0.07408 to 0.08147, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 9s - loss: 8.6931 - acc: 0.8762 - mDice: 0.0720 - val_loss: 6.5086 - val_acc: 0.9033 - val_mDice: 0.0909

Epoch 00015: val_mDice improved from 0.08147 to 0.09090, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 9s - loss: 8.3496 - acc: 0.8776 - mDice: 0.0779 - val_loss: 6.3033 - val_acc: 0.9049 - val_mDice: 0.1005

Epoch 00016: val_mDice improved from 0.09090 to 0.10053, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 10s - loss: 8.0694 - acc: 0.8790 - mDice: 0.0836 - val_loss: 6.2823 - val_acc: 0.9026 - val_mDice: 0.1057

Epoch 00017: val_mDice improved from 0.10053 to 0.10574, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 10s - loss: 7.7956 - acc: 0.8788 - mDice: 0.0876 - val_loss: 6.0926 - val_acc: 0.9053 - val_mDice: 0.1133

Epoch 00018: val_mDice improved from 0.10574 to 0.11331, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 10s - loss: 7.5440 - acc: 0.8790 - mDice: 0.0938 - val_loss: 5.7687 - val_acc: 0.9057 - val_mDice: 0.1254

Epoch 00019: val_mDice improved from 0.11331 to 0.12539, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 10s - loss: 7.3124 - acc: 0.8797 - mDice: 0.1007 - val_loss: 5.6662 - val_acc: 0.9065 - val_mDice: 0.1315

Epoch 00020: val_mDice improved from 0.12539 to 0.13149, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 10s - loss: 7.1292 - acc: 0.8819 - mDice: 0.1072 - val_loss: 5.4659 - val_acc: 0.9082 - val_mDice: 0.1446

Epoch 00021: val_mDice improved from 0.13149 to 0.14456, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 10s - loss: 6.9235 - acc: 0.8843 - mDice: 0.1151 - val_loss: 5.4614 - val_acc: 0.9057 - val_mDice: 0.1480

Epoch 00022: val_mDice improved from 0.14456 to 0.14801, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 9s - loss: 6.7518 - acc: 0.8873 - mDice: 0.1232 - val_loss: 5.3223 - val_acc: 0.9098 - val_mDice: 0.1620

Epoch 00023: val_mDice improved from 0.14801 to 0.16199, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 10s - loss: 6.5853 - acc: 0.8900 - mDice: 0.1310 - val_loss: 5.3620 - val_acc: 0.9063 - val_mDice: 0.1664

Epoch 00024: val_mDice improved from 0.16199 to 0.16639, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 10s - loss: 6.3999 - acc: 0.8925 - mDice: 0.1411 - val_loss: 5.1313 - val_acc: 0.9082 - val_mDice: 0.1854

Epoch 00025: val_mDice improved from 0.16639 to 0.18543, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 10s - loss: 6.1944 - acc: 0.8944 - mDice: 0.1543 - val_loss: 4.9818 - val_acc: 0.9053 - val_mDice: 0.2001

Epoch 00026: val_mDice improved from 0.18543 to 0.20009, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 10s - loss: 6.0250 - acc: 0.8962 - mDice: 0.1659 - val_loss: 4.8500 - val_acc: 0.9127 - val_mDice: 0.2124

Epoch 00027: val_mDice improved from 0.20009 to 0.21241, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 9s - loss: 5.8524 - acc: 0.8982 - mDice: 0.1769 - val_loss: 4.7703 - val_acc: 0.9061 - val_mDice: 0.2221

Epoch 00028: val_mDice improved from 0.21241 to 0.22213, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 10s - loss: 5.7059 - acc: 0.8993 - mDice: 0.1868 - val_loss: 4.7337 - val_acc: 0.9158 - val_mDice: 0.2311

Epoch 00029: val_mDice improved from 0.22213 to 0.23111, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 9s - loss: 5.5698 - acc: 0.9004 - mDice: 0.1962 - val_loss: 4.5787 - val_acc: 0.9164 - val_mDice: 0.2459

Epoch 00030: val_mDice improved from 0.23111 to 0.24586, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 9s - loss: 5.4630 - acc: 0.9010 - mDice: 0.2044 - val_loss: 4.4757 - val_acc: 0.9109 - val_mDice: 0.2509

Epoch 00031: val_mDice improved from 0.24586 to 0.25089, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 10s - loss: 5.3478 - acc: 0.9018 - mDice: 0.2122 - val_loss: 4.4400 - val_acc: 0.9165 - val_mDice: 0.2580

Epoch 00032: val_mDice improved from 0.25089 to 0.25798, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 9s - loss: 5.2512 - acc: 0.9026 - mDice: 0.2196 - val_loss: 4.3839 - val_acc: 0.9169 - val_mDice: 0.2644

Epoch 00033: val_mDice improved from 0.25798 to 0.26445, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 10s - loss: 5.1672 - acc: 0.9031 - mDice: 0.2250 - val_loss: 4.6608 - val_acc: 0.9139 - val_mDice: 0.2543

Epoch 00034: val_mDice did not improve from 0.26445
Epoch 35/300
 - 9s - loss: 5.0521 - acc: 0.9040 - mDice: 0.2333 - val_loss: 4.4010 - val_acc: 0.9135 - val_mDice: 0.2624

Epoch 00035: val_mDice did not improve from 0.26445
Epoch 36/300
 - 10s - loss: 4.9792 - acc: 0.9043 - mDice: 0.2398 - val_loss: 4.2892 - val_acc: 0.9149 - val_mDice: 0.2758

Epoch 00036: val_mDice improved from 0.26445 to 0.27579, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 10s - loss: 4.9087 - acc: 0.9048 - mDice: 0.2461 - val_loss: 4.4696 - val_acc: 0.9104 - val_mDice: 0.2695

Epoch 00037: val_mDice did not improve from 0.27579
Epoch 38/300
 - 9s - loss: 4.8183 - acc: 0.9057 - mDice: 0.2535 - val_loss: 4.3149 - val_acc: 0.9200 - val_mDice: 0.2848

Epoch 00038: val_mDice improved from 0.27579 to 0.28480, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 10s - loss: 4.7463 - acc: 0.9060 - mDice: 0.2604 - val_loss: 4.1101 - val_acc: 0.9200 - val_mDice: 0.3019

Epoch 00039: val_mDice improved from 0.28480 to 0.30187, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 10s - loss: 4.6634 - acc: 0.9067 - mDice: 0.2685 - val_loss: 4.0918 - val_acc: 0.9182 - val_mDice: 0.3052

Epoch 00040: val_mDice improved from 0.30187 to 0.30518, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 10s - loss: 4.5965 - acc: 0.9070 - mDice: 0.2759 - val_loss: 4.0218 - val_acc: 0.9174 - val_mDice: 0.3156

Epoch 00041: val_mDice improved from 0.30518 to 0.31561, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 10s - loss: 4.5295 - acc: 0.9074 - mDice: 0.2825 - val_loss: 4.2632 - val_acc: 0.9138 - val_mDice: 0.3061

Epoch 00042: val_mDice did not improve from 0.31561
Epoch 43/300
 - 10s - loss: 4.4782 - acc: 0.9076 - mDice: 0.2886 - val_loss: 4.1901 - val_acc: 0.9190 - val_mDice: 0.3117

Epoch 00043: val_mDice did not improve from 0.31561
Epoch 44/300
 - 10s - loss: 4.4398 - acc: 0.9078 - mDice: 0.2932 - val_loss: 4.1797 - val_acc: 0.9142 - val_mDice: 0.3215

Epoch 00044: val_mDice improved from 0.31561 to 0.32152, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 9s - loss: 4.3761 - acc: 0.9085 - mDice: 0.2995 - val_loss: 4.1840 - val_acc: 0.9228 - val_mDice: 0.3259

Epoch 00045: val_mDice improved from 0.32152 to 0.32589, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 9s - loss: 4.3121 - acc: 0.9094 - mDice: 0.3072 - val_loss: 4.0824 - val_acc: 0.9236 - val_mDice: 0.3376

Epoch 00046: val_mDice improved from 0.32589 to 0.33763, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 10s - loss: 4.2873 - acc: 0.9097 - mDice: 0.3107 - val_loss: 3.8882 - val_acc: 0.9213 - val_mDice: 0.3465

Epoch 00047: val_mDice improved from 0.33763 to 0.34651, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 9s - loss: 4.2297 - acc: 0.9102 - mDice: 0.3169 - val_loss: 4.0567 - val_acc: 0.9230 - val_mDice: 0.3433

Epoch 00048: val_mDice did not improve from 0.34651
Epoch 49/300
 - 9s - loss: 4.2004 - acc: 0.9106 - mDice: 0.3217 - val_loss: 4.1683 - val_acc: 0.9227 - val_mDice: 0.3467

Epoch 00049: val_mDice improved from 0.34651 to 0.34671, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 10s - loss: 4.1543 - acc: 0.9112 - mDice: 0.3268 - val_loss: 4.1648 - val_acc: 0.9225 - val_mDice: 0.3462

Epoch 00050: val_mDice did not improve from 0.34671
Epoch 51/300
 - 9s - loss: 4.1157 - acc: 0.9116 - mDice: 0.3315 - val_loss: 4.1336 - val_acc: 0.9240 - val_mDice: 0.3499

Epoch 00051: val_mDice improved from 0.34671 to 0.34992, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 9s - loss: 4.1014 - acc: 0.9118 - mDice: 0.3341 - val_loss: 4.1350 - val_acc: 0.9248 - val_mDice: 0.3572

Epoch 00052: val_mDice improved from 0.34992 to 0.35717, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 10s - loss: 4.0676 - acc: 0.9124 - mDice: 0.3383 - val_loss: 4.0433 - val_acc: 0.9252 - val_mDice: 0.3631

Epoch 00053: val_mDice improved from 0.35717 to 0.36306, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 9s - loss: 4.0239 - acc: 0.9128 - mDice: 0.3431 - val_loss: 3.9552 - val_acc: 0.9278 - val_mDice: 0.3716

Epoch 00054: val_mDice improved from 0.36306 to 0.37158, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 9s - loss: 3.9943 - acc: 0.9132 - mDice: 0.3471 - val_loss: 4.1008 - val_acc: 0.9259 - val_mDice: 0.3678

Epoch 00055: val_mDice did not improve from 0.37158
Epoch 56/300
 - 9s - loss: 3.9584 - acc: 0.9137 - mDice: 0.3520 - val_loss: 4.4287 - val_acc: 0.9213 - val_mDice: 0.3466

Epoch 00056: val_mDice did not improve from 0.37158
Epoch 57/300
 - 10s - loss: 3.9373 - acc: 0.9140 - mDice: 0.3542 - val_loss: 3.9934 - val_acc: 0.9293 - val_mDice: 0.3746

Epoch 00057: val_mDice improved from 0.37158 to 0.37461, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 58/300
 - 10s - loss: 3.9038 - acc: 0.9145 - mDice: 0.3591 - val_loss: 4.1980 - val_acc: 0.9233 - val_mDice: 0.3593

Epoch 00058: val_mDice did not improve from 0.37461
Epoch 59/300
 - 9s - loss: 3.8895 - acc: 0.9148 - mDice: 0.3598 - val_loss: 4.1076 - val_acc: 0.9257 - val_mDice: 0.3697

Epoch 00059: val_mDice did not improve from 0.37461
Epoch 60/300
 - 9s - loss: 3.8685 - acc: 0.9151 - mDice: 0.3630 - val_loss: 4.1409 - val_acc: 0.9241 - val_mDice: 0.3652

Epoch 00060: val_mDice did not improve from 0.37461
Epoch 61/300
 - 10s - loss: 3.8321 - acc: 0.9156 - mDice: 0.3678 - val_loss: 4.3822 - val_acc: 0.9222 - val_mDice: 0.3656

Epoch 00061: val_mDice did not improve from 0.37461
Epoch 62/300
 - 10s - loss: 3.8120 - acc: 0.9160 - mDice: 0.3705 - val_loss: 4.1239 - val_acc: 0.9279 - val_mDice: 0.3779

Epoch 00062: val_mDice improved from 0.37461 to 0.37792, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 9s - loss: 3.8121 - acc: 0.9162 - mDice: 0.3710 - val_loss: 3.8908 - val_acc: 0.9325 - val_mDice: 0.3912

Epoch 00063: val_mDice improved from 0.37792 to 0.39122, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 9s - loss: 3.7853 - acc: 0.9164 - mDice: 0.3743 - val_loss: 4.1925 - val_acc: 0.9301 - val_mDice: 0.3807

Epoch 00064: val_mDice did not improve from 0.39122
Epoch 65/300
 - 10s - loss: 3.7681 - acc: 0.9168 - mDice: 0.3766 - val_loss: 4.2572 - val_acc: 0.9244 - val_mDice: 0.3653

Epoch 00065: val_mDice did not improve from 0.39122
Epoch 66/300
 - 9s - loss: 3.7433 - acc: 0.9171 - mDice: 0.3794 - val_loss: 3.9901 - val_acc: 0.9301 - val_mDice: 0.3861

Epoch 00066: val_mDice did not improve from 0.39122
Epoch 67/300
 - 10s - loss: 3.7216 - acc: 0.9176 - mDice: 0.3815 - val_loss: 4.1104 - val_acc: 0.9288 - val_mDice: 0.3822

Epoch 00067: val_mDice did not improve from 0.39122
Epoch 68/300
 - 10s - loss: 3.7051 - acc: 0.9178 - mDice: 0.3838 - val_loss: 4.1696 - val_acc: 0.9274 - val_mDice: 0.3826

Epoch 00068: val_mDice did not improve from 0.39122
Epoch 69/300
 - 10s - loss: 3.6926 - acc: 0.9181 - mDice: 0.3860 - val_loss: 3.8326 - val_acc: 0.9300 - val_mDice: 0.3981

Epoch 00069: val_mDice improved from 0.39122 to 0.39806, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 70/300
 - 10s - loss: 3.6615 - acc: 0.9185 - mDice: 0.3887 - val_loss: 4.0661 - val_acc: 0.9222 - val_mDice: 0.3783

Epoch 00070: val_mDice did not improve from 0.39806
Epoch 71/300
 - 9s - loss: 3.6614 - acc: 0.9185 - mDice: 0.3895 - val_loss: 3.9218 - val_acc: 0.9296 - val_mDice: 0.3961

Epoch 00071: val_mDice did not improve from 0.39806
Epoch 72/300
 - 10s - loss: 3.6411 - acc: 0.9188 - mDice: 0.3922 - val_loss: 3.9327 - val_acc: 0.9304 - val_mDice: 0.3947

Epoch 00072: val_mDice did not improve from 0.39806
Epoch 73/300
 - 9s - loss: 3.6173 - acc: 0.9190 - mDice: 0.3935 - val_loss: 4.2001 - val_acc: 0.9276 - val_mDice: 0.3882

Epoch 00073: val_mDice did not improve from 0.39806
Epoch 74/300
 - 10s - loss: 3.6099 - acc: 0.9193 - mDice: 0.3955 - val_loss: 3.9862 - val_acc: 0.9319 - val_mDice: 0.3968

Epoch 00074: val_mDice did not improve from 0.39806
Epoch 75/300
 - 10s - loss: 3.6051 - acc: 0.9193 - mDice: 0.3965 - val_loss: 4.1499 - val_acc: 0.9248 - val_mDice: 0.3880

Epoch 00075: val_mDice did not improve from 0.39806
Epoch 76/300
 - 10s - loss: 3.5823 - acc: 0.9198 - mDice: 0.3985 - val_loss: 4.0343 - val_acc: 0.9277 - val_mDice: 0.3922

Epoch 00076: val_mDice did not improve from 0.39806
Epoch 77/300
 - 10s - loss: 3.5657 - acc: 0.9199 - mDice: 0.4013 - val_loss: 3.8824 - val_acc: 0.9274 - val_mDice: 0.4001

Epoch 00077: val_mDice improved from 0.39806 to 0.40006, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 78/300
 - 9s - loss: 3.5511 - acc: 0.9202 - mDice: 0.4018 - val_loss: 3.8705 - val_acc: 0.9304 - val_mDice: 0.4043

Epoch 00078: val_mDice improved from 0.40006 to 0.40427, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 79/300
 - 10s - loss: 3.5344 - acc: 0.9204 - mDice: 0.4048 - val_loss: 3.9416 - val_acc: 0.9328 - val_mDice: 0.4060

Epoch 00079: val_mDice improved from 0.40427 to 0.40601, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 80/300
 - 10s - loss: 3.5308 - acc: 0.9203 - mDice: 0.4041 - val_loss: 3.9924 - val_acc: 0.9333 - val_mDice: 0.4052

Epoch 00080: val_mDice did not improve from 0.40601
Epoch 81/300
 - 10s - loss: 3.5126 - acc: 0.9206 - mDice: 0.4073 - val_loss: 3.7932 - val_acc: 0.9306 - val_mDice: 0.4082

Epoch 00081: val_mDice improved from 0.40601 to 0.40823, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 82/300
 - 10s - loss: 3.4914 - acc: 0.9207 - mDice: 0.4095 - val_loss: 3.9980 - val_acc: 0.9266 - val_mDice: 0.4041

Epoch 00082: val_mDice did not improve from 0.40823
Epoch 83/300
 - 10s - loss: 3.5075 - acc: 0.9205 - mDice: 0.4075 - val_loss: 3.8044 - val_acc: 0.9339 - val_mDice: 0.4201

Epoch 00083: val_mDice improved from 0.40823 to 0.42009, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 84/300
 - 10s - loss: 3.4797 - acc: 0.9207 - mDice: 0.4110 - val_loss: 4.0760 - val_acc: 0.9257 - val_mDice: 0.3941

Epoch 00084: val_mDice did not improve from 0.42009
Epoch 85/300
 - 10s - loss: 3.4724 - acc: 0.9205 - mDice: 0.4118 - val_loss: 4.0256 - val_acc: 0.9288 - val_mDice: 0.3994

Epoch 00085: val_mDice did not improve from 0.42009
Epoch 86/300
 - 10s - loss: 3.4653 - acc: 0.9208 - mDice: 0.4130 - val_loss: 3.7922 - val_acc: 0.9337 - val_mDice: 0.4225

Epoch 00086: val_mDice improved from 0.42009 to 0.42251, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 87/300
 - 9s - loss: 3.4529 - acc: 0.9209 - mDice: 0.4136 - val_loss: 4.0848 - val_acc: 0.9292 - val_mDice: 0.4069

Epoch 00087: val_mDice did not improve from 0.42251
Epoch 88/300
 - 10s - loss: 3.4461 - acc: 0.9210 - mDice: 0.4152 - val_loss: 3.9733 - val_acc: 0.9316 - val_mDice: 0.4107

Epoch 00088: val_mDice did not improve from 0.42251
Epoch 89/300
 - 10s - loss: 3.4283 - acc: 0.9213 - mDice: 0.4174 - val_loss: 3.9160 - val_acc: 0.9327 - val_mDice: 0.4102

Epoch 00089: val_mDice did not improve from 0.42251
Epoch 90/300
 - 10s - loss: 3.4145 - acc: 0.9213 - mDice: 0.4181 - val_loss: 3.7977 - val_acc: 0.9349 - val_mDice: 0.4217

Epoch 00090: val_mDice did not improve from 0.42251
Epoch 91/300
 - 10s - loss: 3.4335 - acc: 0.9211 - mDice: 0.4161 - val_loss: 4.0954 - val_acc: 0.9269 - val_mDice: 0.3911

Epoch 00091: val_mDice did not improve from 0.42251
Epoch 92/300
 - 10s - loss: 3.4096 - acc: 0.9213 - mDice: 0.4192 - val_loss: 3.8471 - val_acc: 0.9362 - val_mDice: 0.4219

Epoch 00092: val_mDice did not improve from 0.42251
Epoch 93/300
 - 10s - loss: 3.4001 - acc: 0.9217 - mDice: 0.4208 - val_loss: 3.7777 - val_acc: 0.9331 - val_mDice: 0.4187

Epoch 00093: val_mDice did not improve from 0.42251
Epoch 94/300
 - 9s - loss: 3.3931 - acc: 0.9215 - mDice: 0.4211 - val_loss: 3.7482 - val_acc: 0.9323 - val_mDice: 0.4184

Epoch 00094: val_mDice did not improve from 0.42251
Epoch 95/300
 - 10s - loss: 3.3833 - acc: 0.9217 - mDice: 0.4224 - val_loss: 3.5443 - val_acc: 0.9334 - val_mDice: 0.4236

Epoch 00095: val_mDice improved from 0.42251 to 0.42357, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 96/300
 - 9s - loss: 3.3713 - acc: 0.9218 - mDice: 0.4239 - val_loss: 3.8159 - val_acc: 0.9358 - val_mDice: 0.4219

Epoch 00096: val_mDice did not improve from 0.42357
Epoch 97/300
 - 10s - loss: 3.3641 - acc: 0.9218 - mDice: 0.4247 - val_loss: 3.9675 - val_acc: 0.9315 - val_mDice: 0.4083

Epoch 00097: val_mDice did not improve from 0.42357
Epoch 98/300
 - 10s - loss: 3.3494 - acc: 0.9223 - mDice: 0.4269 - val_loss: 3.8657 - val_acc: 0.9317 - val_mDice: 0.4146

Epoch 00098: val_mDice did not improve from 0.42357
Epoch 99/300
 - 10s - loss: 3.3565 - acc: 0.9222 - mDice: 0.4263 - val_loss: 4.1172 - val_acc: 0.9336 - val_mDice: 0.4017

Epoch 00099: val_mDice did not improve from 0.42357
Epoch 100/300
 - 10s - loss: 3.3550 - acc: 0.9218 - mDice: 0.4257 - val_loss: 3.9744 - val_acc: 0.9306 - val_mDice: 0.4108

Epoch 00100: val_mDice did not improve from 0.42357
Epoch 101/300
 - 11s - loss: 3.3316 - acc: 0.9225 - mDice: 0.4293 - val_loss: 4.0138 - val_acc: 0.9344 - val_mDice: 0.4195

Epoch 00101: val_mDice did not improve from 0.42357
Epoch 102/300
 - 11s - loss: 3.3238 - acc: 0.9224 - mDice: 0.4298 - val_loss: 3.9588 - val_acc: 0.9317 - val_mDice: 0.4174

Epoch 00102: val_mDice did not improve from 0.42357
Epoch 103/300
 - 11s - loss: 3.3317 - acc: 0.9221 - mDice: 0.4291 - val_loss: 3.9256 - val_acc: 0.9326 - val_mDice: 0.4173

Epoch 00103: val_mDice did not improve from 0.42357
Epoch 104/300
 - 10s - loss: 3.3235 - acc: 0.9224 - mDice: 0.4300 - val_loss: 3.8573 - val_acc: 0.9352 - val_mDice: 0.4329

Epoch 00104: val_mDice improved from 0.42357 to 0.43291, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 105/300
 - 11s - loss: 3.3197 - acc: 0.9225 - mDice: 0.4313 - val_loss: 3.8539 - val_acc: 0.9348 - val_mDice: 0.4284

Epoch 00105: val_mDice did not improve from 0.43291
Epoch 106/300
 - 11s - loss: 3.3029 - acc: 0.9227 - mDice: 0.4329 - val_loss: 3.9037 - val_acc: 0.9332 - val_mDice: 0.4246

Epoch 00106: val_mDice did not improve from 0.43291
Epoch 107/300
 - 11s - loss: 3.3051 - acc: 0.9227 - mDice: 0.4330 - val_loss: 3.8954 - val_acc: 0.9316 - val_mDice: 0.4258

Epoch 00107: val_mDice did not improve from 0.43291
Epoch 108/300
 - 11s - loss: 3.2972 - acc: 0.9228 - mDice: 0.4340 - val_loss: 4.0463 - val_acc: 0.9275 - val_mDice: 0.4100

Epoch 00108: val_mDice did not improve from 0.43291
Epoch 109/300
 - 10s - loss: 3.2838 - acc: 0.9229 - mDice: 0.4362 - val_loss: 3.9549 - val_acc: 0.9316 - val_mDice: 0.4163

Epoch 00109: val_mDice did not improve from 0.43291
Epoch 110/300
 - 11s - loss: 3.2830 - acc: 0.9230 - mDice: 0.4357 - val_loss: 3.8844 - val_acc: 0.9361 - val_mDice: 0.4255

Epoch 00110: val_mDice did not improve from 0.43291
Epoch 111/300
 - 10s - loss: 3.2732 - acc: 0.9231 - mDice: 0.4372 - val_loss: 3.9931 - val_acc: 0.9295 - val_mDice: 0.4151

Epoch 00111: val_mDice did not improve from 0.43291
Epoch 112/300
 - 11s - loss: 3.2751 - acc: 0.9230 - mDice: 0.4366 - val_loss: 3.7974 - val_acc: 0.9334 - val_mDice: 0.4212

Epoch 00112: val_mDice did not improve from 0.43291
Epoch 113/300
 - 10s - loss: 3.2941 - acc: 0.9227 - mDice: 0.4348 - val_loss: 3.8044 - val_acc: 0.9363 - val_mDice: 0.4285

Epoch 00113: val_mDice did not improve from 0.43291
Epoch 114/300
 - 11s - loss: 3.2569 - acc: 0.9232 - mDice: 0.4393 - val_loss: 3.7361 - val_acc: 0.9333 - val_mDice: 0.4293

Epoch 00114: val_mDice did not improve from 0.43291
Epoch 115/300
 - 10s - loss: 3.2531 - acc: 0.9235 - mDice: 0.4399 - val_loss: 3.9712 - val_acc: 0.9300 - val_mDice: 0.4186

Epoch 00115: val_mDice did not improve from 0.43291
Epoch 116/300
 - 11s - loss: 3.2394 - acc: 0.9236 - mDice: 0.4424 - val_loss: 3.7710 - val_acc: 0.9332 - val_mDice: 0.4285

Epoch 00116: val_mDice did not improve from 0.43291
Epoch 117/300
 - 10s - loss: 3.2390 - acc: 0.9235 - mDice: 0.4417 - val_loss: 3.8223 - val_acc: 0.9321 - val_mDice: 0.4276

Epoch 00117: val_mDice did not improve from 0.43291
Epoch 118/300
 - 11s - loss: 3.2284 - acc: 0.9238 - mDice: 0.4430 - val_loss: 3.9645 - val_acc: 0.9276 - val_mDice: 0.4168

Epoch 00118: val_mDice did not improve from 0.43291
Epoch 119/300
 - 10s - loss: 3.2302 - acc: 0.9237 - mDice: 0.4434 - val_loss: 3.8206 - val_acc: 0.9336 - val_mDice: 0.4277

Epoch 00119: val_mDice did not improve from 0.43291
Epoch 120/300
 - 11s - loss: 3.2268 - acc: 0.9237 - mDice: 0.4439 - val_loss: 3.9419 - val_acc: 0.9324 - val_mDice: 0.4170

Epoch 00120: val_mDice did not improve from 0.43291
Epoch 121/300
 - 10s - loss: 3.2165 - acc: 0.9239 - mDice: 0.4450 - val_loss: 3.9709 - val_acc: 0.9326 - val_mDice: 0.4219

Epoch 00121: val_mDice did not improve from 0.43291
Epoch 122/300
 - 11s - loss: 3.2220 - acc: 0.9238 - mDice: 0.4438 - val_loss: 3.7915 - val_acc: 0.9319 - val_mDice: 0.4326

Epoch 00122: val_mDice did not improve from 0.43291
Epoch 123/300
 - 10s - loss: 3.2113 - acc: 0.9241 - mDice: 0.4462 - val_loss: 3.6885 - val_acc: 0.9356 - val_mDice: 0.4375

Epoch 00123: val_mDice improved from 0.43291 to 0.43749, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 124/300
 - 11s - loss: 3.2142 - acc: 0.9238 - mDice: 0.4453 - val_loss: 3.8324 - val_acc: 0.9351 - val_mDice: 0.4360

Epoch 00124: val_mDice did not improve from 0.43749
Epoch 125/300
 - 11s - loss: 3.2085 - acc: 0.9240 - mDice: 0.4465 - val_loss: 3.9073 - val_acc: 0.9335 - val_mDice: 0.4326

Epoch 00125: val_mDice did not improve from 0.43749
Epoch 126/300
 - 11s - loss: 3.1987 - acc: 0.9242 - mDice: 0.4471 - val_loss: 3.9155 - val_acc: 0.9310 - val_mDice: 0.4250

Epoch 00126: val_mDice did not improve from 0.43749
Epoch 127/300
 - 11s - loss: 3.1904 - acc: 0.9242 - mDice: 0.4489 - val_loss: 3.7394 - val_acc: 0.9358 - val_mDice: 0.4355

Epoch 00127: val_mDice did not improve from 0.43749
Epoch 128/300
 - 10s - loss: 3.1818 - acc: 0.9244 - mDice: 0.4496 - val_loss: 3.8953 - val_acc: 0.9330 - val_mDice: 0.4234

Epoch 00128: val_mDice did not improve from 0.43749
Epoch 129/300
 - 11s - loss: 3.1816 - acc: 0.9244 - mDice: 0.4498 - val_loss: 3.7859 - val_acc: 0.9328 - val_mDice: 0.4374

Epoch 00129: val_mDice did not improve from 0.43749
Epoch 130/300
 - 10s - loss: 3.1994 - acc: 0.9241 - mDice: 0.4472 - val_loss: 3.8852 - val_acc: 0.9341 - val_mDice: 0.4254

Epoch 00130: val_mDice did not improve from 0.43749
Epoch 131/300
 - 11s - loss: 3.1818 - acc: 0.9245 - mDice: 0.4501 - val_loss: 3.8620 - val_acc: 0.9336 - val_mDice: 0.4305

Epoch 00131: val_mDice did not improve from 0.43749
Epoch 132/300
 - 10s - loss: 3.1634 - acc: 0.9247 - mDice: 0.4526 - val_loss: 3.7447 - val_acc: 0.9364 - val_mDice: 0.4300

Epoch 00132: val_mDice did not improve from 0.43749
Epoch 133/300
 - 11s - loss: 3.1748 - acc: 0.9246 - mDice: 0.4518 - val_loss: 3.9330 - val_acc: 0.9332 - val_mDice: 0.4187

Epoch 00133: val_mDice did not improve from 0.43749
Epoch 134/300
 - 12s - loss: 3.1534 - acc: 0.9246 - mDice: 0.4534 - val_loss: 4.0280 - val_acc: 0.9315 - val_mDice: 0.4216

Epoch 00134: val_mDice did not improve from 0.43749
Epoch 135/300
 - 13s - loss: 3.1642 - acc: 0.9247 - mDice: 0.4525 - val_loss: 3.6572 - val_acc: 0.9336 - val_mDice: 0.4398

Epoch 00135: val_mDice improved from 0.43749 to 0.43977, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 136/300
 - 11s - loss: 3.1557 - acc: 0.9246 - mDice: 0.4534 - val_loss: 3.9310 - val_acc: 0.9305 - val_mDice: 0.4261

Epoch 00136: val_mDice did not improve from 0.43977
Epoch 137/300
 - 13s - loss: 3.1534 - acc: 0.9248 - mDice: 0.4534 - val_loss: 3.7908 - val_acc: 0.9367 - val_mDice: 0.4356

Epoch 00137: val_mDice did not improve from 0.43977
Epoch 138/300
 - 13s - loss: 3.1532 - acc: 0.9246 - mDice: 0.4540 - val_loss: 3.9088 - val_acc: 0.9318 - val_mDice: 0.4297

Epoch 00138: val_mDice did not improve from 0.43977
Epoch 139/300
 - 12s - loss: 3.1529 - acc: 0.9249 - mDice: 0.4547 - val_loss: 3.6045 - val_acc: 0.9345 - val_mDice: 0.4364

Epoch 00139: val_mDice did not improve from 0.43977
Epoch 140/300
 - 13s - loss: 3.1482 - acc: 0.9248 - mDice: 0.4550 - val_loss: 3.8021 - val_acc: 0.9320 - val_mDice: 0.4281

Epoch 00140: val_mDice did not improve from 0.43977
Epoch 141/300
 - 12s - loss: 3.1405 - acc: 0.9251 - mDice: 0.4558 - val_loss: 3.8649 - val_acc: 0.9340 - val_mDice: 0.4247

Epoch 00141: val_mDice did not improve from 0.43977
Epoch 142/300
 - 13s - loss: 3.1280 - acc: 0.9251 - mDice: 0.4575 - val_loss: 3.8182 - val_acc: 0.9337 - val_mDice: 0.4308

Epoch 00142: val_mDice did not improve from 0.43977
Epoch 143/300
 - 13s - loss: 3.1452 - acc: 0.9248 - mDice: 0.4557 - val_loss: 3.6282 - val_acc: 0.9370 - val_mDice: 0.4388

Epoch 00143: val_mDice did not improve from 0.43977
Epoch 144/300
 - 12s - loss: 3.1265 - acc: 0.9250 - mDice: 0.4571 - val_loss: 3.8378 - val_acc: 0.9352 - val_mDice: 0.4342

Epoch 00144: val_mDice did not improve from 0.43977
Epoch 145/300
 - 14s - loss: 3.1260 - acc: 0.9250 - mDice: 0.4581 - val_loss: 3.9070 - val_acc: 0.9333 - val_mDice: 0.4305

Epoch 00145: val_mDice did not improve from 0.43977
Epoch 146/300
 - 13s - loss: 3.1333 - acc: 0.9251 - mDice: 0.4575 - val_loss: 3.8663 - val_acc: 0.9359 - val_mDice: 0.4347

Epoch 00146: val_mDice did not improve from 0.43977
Epoch 147/300
 - 12s - loss: 3.1337 - acc: 0.9249 - mDice: 0.4573 - val_loss: 3.8020 - val_acc: 0.9351 - val_mDice: 0.4352

Epoch 00147: val_mDice did not improve from 0.43977
Epoch 148/300
 - 13s - loss: 3.1120 - acc: 0.9253 - mDice: 0.4603 - val_loss: 3.7600 - val_acc: 0.9334 - val_mDice: 0.4347

Epoch 00148: val_mDice did not improve from 0.43977
Epoch 149/300
 - 12s - loss: 3.1129 - acc: 0.9255 - mDice: 0.4613 - val_loss: 3.8162 - val_acc: 0.9348 - val_mDice: 0.4274

Epoch 00149: val_mDice did not improve from 0.43977
Epoch 150/300
 - 13s - loss: 3.1091 - acc: 0.9254 - mDice: 0.4608 - val_loss: 3.8557 - val_acc: 0.9345 - val_mDice: 0.4303

Epoch 00150: val_mDice did not improve from 0.43977
Epoch 151/300
 - 12s - loss: 3.1226 - acc: 0.9251 - mDice: 0.4596 - val_loss: 3.9212 - val_acc: 0.9321 - val_mDice: 0.4284

Epoch 00151: val_mDice did not improve from 0.43977
Epoch 152/300
 - 13s - loss: 3.1139 - acc: 0.9253 - mDice: 0.4597 - val_loss: 4.0692 - val_acc: 0.9325 - val_mDice: 0.4246

Epoch 00152: val_mDice did not improve from 0.43977
Epoch 153/300
 - 13s - loss: 3.0995 - acc: 0.9256 - mDice: 0.4627 - val_loss: 3.9580 - val_acc: 0.9354 - val_mDice: 0.4291

Epoch 00153: val_mDice did not improve from 0.43977
Epoch 154/300
 - 12s - loss: 3.1052 - acc: 0.9255 - mDice: 0.4616 - val_loss: 3.9166 - val_acc: 0.9345 - val_mDice: 0.4333

Epoch 00154: val_mDice did not improve from 0.43977
Epoch 155/300
 - 13s - loss: 3.0953 - acc: 0.9257 - mDice: 0.4625 - val_loss: 3.8371 - val_acc: 0.9352 - val_mDice: 0.4341

Epoch 00155: val_mDice did not improve from 0.43977
Epoch 156/300
 - 12s - loss: 3.0991 - acc: 0.9256 - mDice: 0.4626 - val_loss: 3.8429 - val_acc: 0.9324 - val_mDice: 0.4372

Epoch 00156: val_mDice did not improve from 0.43977
Epoch 157/300
 - 11s - loss: 3.0908 - acc: 0.9257 - mDice: 0.4634 - val_loss: 3.8420 - val_acc: 0.9332 - val_mDice: 0.4358

Epoch 00157: val_mDice did not improve from 0.43977
Epoch 158/300
 - 10s - loss: 3.0956 - acc: 0.9257 - mDice: 0.4636 - val_loss: 3.9147 - val_acc: 0.9346 - val_mDice: 0.4246

Epoch 00158: val_mDice did not improve from 0.43977
Epoch 159/300
 - 11s - loss: 3.0879 - acc: 0.9256 - mDice: 0.4635 - val_loss: 3.7217 - val_acc: 0.9353 - val_mDice: 0.4411

Epoch 00159: val_mDice improved from 0.43977 to 0.44110, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 160/300
 - 10s - loss: 3.0756 - acc: 0.9258 - mDice: 0.4659 - val_loss: 3.7093 - val_acc: 0.9367 - val_mDice: 0.4457

Epoch 00160: val_mDice improved from 0.44110 to 0.44572, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 161/300
 - 10s - loss: 3.0702 - acc: 0.9259 - mDice: 0.4665 - val_loss: 3.7264 - val_acc: 0.9379 - val_mDice: 0.4453

Epoch 00161: val_mDice did not improve from 0.44572
Epoch 162/300
 - 11s - loss: 3.0757 - acc: 0.9259 - mDice: 0.4666 - val_loss: 3.9093 - val_acc: 0.9342 - val_mDice: 0.4348

Epoch 00162: val_mDice did not improve from 0.44572
Epoch 163/300
 - 10s - loss: 3.0757 - acc: 0.9260 - mDice: 0.4658 - val_loss: 3.8405 - val_acc: 0.9350 - val_mDice: 0.4365

Epoch 00163: val_mDice did not improve from 0.44572
Epoch 164/300
 - 11s - loss: 3.0736 - acc: 0.9260 - mDice: 0.4670 - val_loss: 3.6481 - val_acc: 0.9374 - val_mDice: 0.4429

Epoch 00164: val_mDice did not improve from 0.44572
Epoch 165/300
 - 9s - loss: 3.0748 - acc: 0.9258 - mDice: 0.4658 - val_loss: 3.8113 - val_acc: 0.9367 - val_mDice: 0.4360

Epoch 00165: val_mDice did not improve from 0.44572
Epoch 166/300
 - 10s - loss: 3.0701 - acc: 0.9261 - mDice: 0.4673 - val_loss: 4.0811 - val_acc: 0.9299 - val_mDice: 0.4216

Epoch 00166: val_mDice did not improve from 0.44572
Epoch 167/300
 - 9s - loss: 3.0698 - acc: 0.9259 - mDice: 0.4668 - val_loss: 3.6752 - val_acc: 0.9369 - val_mDice: 0.4430

Epoch 00167: val_mDice did not improve from 0.44572
Epoch 168/300
 - 9s - loss: 3.0741 - acc: 0.9259 - mDice: 0.4666 - val_loss: 3.7417 - val_acc: 0.9356 - val_mDice: 0.4405

Epoch 00168: val_mDice did not improve from 0.44572
Epoch 169/300
 - 10s - loss: 3.0685 - acc: 0.9261 - mDice: 0.4671 - val_loss: 3.6727 - val_acc: 0.9389 - val_mDice: 0.4480

Epoch 00169: val_mDice improved from 0.44572 to 0.44798, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM20_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 170/300
 - 10s - loss: 3.0524 - acc: 0.9262 - mDice: 0.4700 - val_loss: 3.6680 - val_acc: 0.9346 - val_mDice: 0.4397

Epoch 00170: val_mDice did not improve from 0.44798
Epoch 171/300
 - 9s - loss: 3.0498 - acc: 0.9263 - mDice: 0.4701 - val_loss: 3.8801 - val_acc: 0.9351 - val_mDice: 0.4340

Epoch 00171: val_mDice did not improve from 0.44798
Epoch 172/300
 - 9s - loss: 3.0575 - acc: 0.9261 - mDice: 0.4687 - val_loss: 3.8211 - val_acc: 0.9350 - val_mDice: 0.4341

Epoch 00172: val_mDice did not improve from 0.44798
Epoch 173/300
 - 9s - loss: 3.0587 - acc: 0.9263 - mDice: 0.4689 - val_loss: 3.9376 - val_acc: 0.9335 - val_mDice: 0.4286

Epoch 00173: val_mDice did not improve from 0.44798
Epoch 174/300
 - 10s - loss: 3.0625 - acc: 0.9262 - mDice: 0.4691 - val_loss: 3.7199 - val_acc: 0.9343 - val_mDice: 0.4321

Epoch 00174: val_mDice did not improve from 0.44798
Epoch 175/300
 - 9s - loss: 3.0512 - acc: 0.9264 - mDice: 0.4698 - val_loss: 3.8320 - val_acc: 0.9321 - val_mDice: 0.4345

Epoch 00175: val_mDice did not improve from 0.44798
Epoch 176/300
 - 9s - loss: 3.0466 - acc: 0.9264 - mDice: 0.4711 - val_loss: 3.6087 - val_acc: 0.9372 - val_mDice: 0.4425

Epoch 00176: val_mDice did not improve from 0.44798
Epoch 177/300
 - 9s - loss: 3.0380 - acc: 0.9265 - mDice: 0.4711 - val_loss: 3.9127 - val_acc: 0.9298 - val_mDice: 0.4261

Epoch 00177: val_mDice did not improve from 0.44798
Epoch 178/300
 - 9s - loss: 3.0406 - acc: 0.9265 - mDice: 0.4721 - val_loss: 3.7454 - val_acc: 0.9341 - val_mDice: 0.4410

Epoch 00178: val_mDice did not improve from 0.44798
Epoch 179/300
 - 9s - loss: 3.0327 - acc: 0.9264 - mDice: 0.4723 - val_loss: 3.9668 - val_acc: 0.9318 - val_mDice: 0.4211

Epoch 00179: val_mDice did not improve from 0.44798
Epoch 180/300
 - 9s - loss: 3.0418 - acc: 0.9264 - mDice: 0.4719 - val_loss: 3.7029 - val_acc: 0.9348 - val_mDice: 0.4340

Epoch 00180: val_mDice did not improve from 0.44798
Epoch 181/300
 - 10s - loss: 3.0446 - acc: 0.9265 - mDice: 0.4712 - val_loss: 3.9435 - val_acc: 0.9322 - val_mDice: 0.4257

Epoch 00181: val_mDice did not improve from 0.44798
Epoch 182/300
 - 9s - loss: 3.0371 - acc: 0.9266 - mDice: 0.4722 - val_loss: 3.8177 - val_acc: 0.9343 - val_mDice: 0.4355

Epoch 00182: val_mDice did not improve from 0.44798
Epoch 183/300
 - 9s - loss: 3.0421 - acc: 0.9265 - mDice: 0.4721 - val_loss: 3.7606 - val_acc: 0.9357 - val_mDice: 0.4445

Epoch 00183: val_mDice did not improve from 0.44798
Epoch 184/300
 - 9s - loss: 3.0323 - acc: 0.9267 - mDice: 0.4732 - val_loss: 3.7178 - val_acc: 0.9370 - val_mDice: 0.4401

Epoch 00184: val_mDice did not improve from 0.44798
Epoch 185/300
 - 9s - loss: 3.0315 - acc: 0.9266 - mDice: 0.4731 - val_loss: 3.9203 - val_acc: 0.9328 - val_mDice: 0.4261

Epoch 00185: val_mDice did not improve from 0.44798
Epoch 186/300
 - 9s - loss: 3.0417 - acc: 0.9265 - mDice: 0.4714 - val_loss: 3.9053 - val_acc: 0.9340 - val_mDice: 0.4309

Epoch 00186: val_mDice did not improve from 0.44798
Epoch 187/300
 - 9s - loss: 3.0275 - acc: 0.9267 - mDice: 0.4738 - val_loss: 3.8806 - val_acc: 0.9366 - val_mDice: 0.4361

Epoch 00187: val_mDice did not improve from 0.44798
Epoch 188/300
 - 9s - loss: 3.0266 - acc: 0.9269 - mDice: 0.4748 - val_loss: 4.0034 - val_acc: 0.9344 - val_mDice: 0.4298

Epoch 00188: val_mDice did not improve from 0.44798
Epoch 189/300
 - 9s - loss: 3.0216 - acc: 0.9267 - mDice: 0.4741 - val_loss: 3.8241 - val_acc: 0.9370 - val_mDice: 0.4392

Epoch 00189: val_mDice did not improve from 0.44798
Epoch 190/300
 - 10s - loss: 3.0318 - acc: 0.9267 - mDice: 0.4731 - val_loss: 3.8980 - val_acc: 0.9339 - val_mDice: 0.4371

Epoch 00190: val_mDice did not improve from 0.44798
Epoch 191/300
 - 10s - loss: 3.0301 - acc: 0.9269 - mDice: 0.4740 - val_loss: 3.8971 - val_acc: 0.9324 - val_mDice: 0.4301

Epoch 00191: val_mDice did not improve from 0.44798
Epoch 192/300
 - 9s - loss: 3.0318 - acc: 0.9267 - mDice: 0.4729 - val_loss: 3.9530 - val_acc: 0.9341 - val_mDice: 0.4342

Epoch 00192: val_mDice did not improve from 0.44798
Epoch 193/300
 - 9s - loss: 3.0117 - acc: 0.9269 - mDice: 0.4760 - val_loss: 3.9887 - val_acc: 0.9350 - val_mDice: 0.4312

Epoch 00193: val_mDice did not improve from 0.44798
Epoch 194/300
 - 9s - loss: 3.0258 - acc: 0.9267 - mDice: 0.4740 - val_loss: 3.9718 - val_acc: 0.9345 - val_mDice: 0.4326

Epoch 00194: val_mDice did not improve from 0.44798
Epoch 195/300
 - 9s - loss: 3.0256 - acc: 0.9268 - mDice: 0.4749 - val_loss: 3.8019 - val_acc: 0.9352 - val_mDice: 0.4355

Epoch 00195: val_mDice did not improve from 0.44798
Epoch 196/300
 - 9s - loss: 3.0165 - acc: 0.9268 - mDice: 0.4754 - val_loss: 3.8235 - val_acc: 0.9336 - val_mDice: 0.4407

Epoch 00196: val_mDice did not improve from 0.44798
Epoch 197/300
 - 10s - loss: 3.0253 - acc: 0.9269 - mDice: 0.4750 - val_loss: 3.6848 - val_acc: 0.9343 - val_mDice: 0.4405

Epoch 00197: val_mDice did not improve from 0.44798
Epoch 198/300
 - 10s - loss: 3.0150 - acc: 0.9269 - mDice: 0.4757 - val_loss: 3.7803 - val_acc: 0.9377 - val_mDice: 0.4342

Epoch 00198: val_mDice did not improve from 0.44798
Epoch 199/300
 - 9s - loss: 3.0048 - acc: 0.9272 - mDice: 0.4772 - val_loss: 3.8377 - val_acc: 0.9352 - val_mDice: 0.4348

Epoch 00199: val_mDice did not improve from 0.44798
Restoring model weights from the end of the best epoch
Epoch 00199: early stopping
{'val_loss': [162.14482780603262, 79.51165876938747, 37.56588995915193, 19.05961823234191, 12.495340010294548, 10.752718691642468, 9.92436532332347, 9.049071873609837, 8.285632878541946, 7.581572584234751, 7.300990512737861, 7.259602808035337, 7.041199162602425, 6.90519557434779, 6.508553084272605, 6.303258899312753, 6.282266964132969, 6.092605571334179, 5.768744331139785, 5.6662183037171, 5.46589305309149, 5.461435249218574, 5.322312020338499, 5.362007781863213, 5.131334773623026, 4.981844595991648, 4.850045894200985, 4.77026376242821, 4.733697606967046, 4.578667272741978, 4.475699060238325, 4.440018699719356, 4.383926464961125, 4.6608301894022865, 4.400963519628231, 4.289176697914417, 4.4696078059765005, 4.3149305410110035, 4.110082293932255, 4.091774753653086, 4.0217828911084395, 4.263242861399283, 4.190115255805162, 4.179713001618018, 4.184035911009862, 4.082408984120075, 3.8882408955922494, 4.056651609448286, 4.168272723372166, 4.16478250347651, 4.133609948249964, 4.135026568403611, 4.0432739315124655, 3.9551687997121077, 4.1007671149877405, 4.4286949668939295, 3.9933674312554874, 4.197956406153166, 4.107580030193696, 4.140894732796228, 4.38221559043114, 4.123894831308951, 3.890827034528439, 4.192471764408625, 4.2571986088385945, 3.990096311156566, 4.110408577781457, 4.169610059032073, 3.83263974235608, 4.066106467292859, 3.9218498709110112, 3.9327332182572436, 4.200096572820957, 3.9862334533379626, 4.149924327547733, 4.0342545085228405, 3.8823913553586373, 3.870473106320088, 3.9415518618547, 3.992350079692327, 3.79316018980283, 3.9979992566200404, 3.8043555514170575, 4.076028493734507, 4.0256194816185875, 3.7921993686602664, 4.084760802296492, 3.9732872511331854, 3.9160360923180213, 3.7977456083664527, 4.095439426027811, 3.8471052027665653, 3.777660304537186, 3.748225380594914, 3.5442842428500834, 3.8159204664138646, 3.967535919868029, 3.86565832564464, 4.117248199307001, 3.974406667626821, 4.013809001216521, 3.958753084907165, 3.9255745743329706, 3.857336061505171, 3.8538849227703533, 3.9037118955300403, 3.8954446888886967, 4.046318393487197, 3.954855530307843, 3.884409699302453, 3.9931317132252913, 3.7974251199227114, 3.8043888761447024, 3.7360810408225427, 3.971192623560245, 3.7709522281701746, 3.8223476386987247, 3.9644696437395535, 3.8205869656342726, 3.941912577702449, 3.9708705865419827, 3.7914982632948804, 3.688537602241223, 3.832444734298266, 3.9073392829069724, 3.9155179663346362, 3.739436151889654, 3.8953362496999593, 3.7858712730499415, 3.885207087947772, 3.862001569225238, 3.74469479574607, 3.9329619717139463, 4.027982892898413, 3.657158061861992, 3.9310481754633098, 3.7907615017432432, 3.9087928040669513, 3.6045463795845327, 3.8021010148983736, 3.86485307262494, 3.8182231187820435, 3.628170982003212, 3.8377923415257382, 3.9069784650435815, 3.8662718740793376, 3.802030086517334, 3.759986863686488, 3.816211617909945, 3.855695742827195, 3.921173018904833, 4.069156239812191, 3.958016114739271, 3.9165758043527603, 3.8370810598134995, 3.8428527323099284, 3.8419547711427393, 3.9147057132079053, 3.721724808216095, 3.7092832315426607, 3.7264361553467236, 3.9092631317101993, 3.840520830108569, 3.6480883233822308, 3.8112903386354446, 4.081065156138861, 3.6752041692917166, 3.741675853729248, 3.672652651484196, 3.6679540952810874, 3.880055457353592, 3.821120873093605, 3.9375804410530972, 3.7199061065912247, 3.8320086976656547, 3.608703387471346, 3.9127248984116774, 3.7454331929867086, 3.966783807827876, 3.7028783433712444, 3.9434596472061596, 3.8176692380354953, 3.760571000667719, 3.7177527225934544, 3.92027271940158, 3.9052779984015684, 3.880635820902311, 4.003394677088811, 3.824125648691104, 3.898041408795577, 3.8970992255669374, 3.9529538487012568, 3.9887484094271293, 3.971754169234863, 3.8019493164924474, 3.8234657026254215, 3.6847836065750856, 3.7802510960743976, 3.8377404671448927], 'val_acc': [0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.9033815654424521, 0.9033283980993124, 0.9049324966393985, 0.9025979844423441, 0.9053416435535138, 0.9057437846293817, 0.9065042757070981, 0.9082447359195123, 0.9057114261847276, 0.9097887438077193, 0.9063193454192235, 0.9082447313345395, 0.9053069765751178, 0.9126918224187998, 0.9061205891462473, 0.9158330445106213, 0.9163531087912046, 0.910891246337157, 0.916482503597553, 0.9168985852828393, 0.9139029865081494, 0.9134545991053948, 0.9149338878118075, 0.9103689170800723, 0.9200259034450238, 0.9199889302253723, 0.9181675108579489, 0.9173539349666009, 0.9137527713408837, 0.9189603466254014, 0.9142404817617856, 0.9228388277383951, 0.9235831361550552, 0.9212555633141444, 0.9229659965405097, 0.9227071129358732, 0.9224990996030661, 0.9239552594148196, 0.9248012052132533, 0.9252080023288727, 0.9277620980372796, 0.9259222287398118, 0.921336442232132, 0.9293176508866824, 0.9232549277635721, 0.9256749474085294, 0.9241216824604914, 0.9222263097763062, 0.9278637904387254, 0.9324819766558133, 0.9300942787757287, 0.9243828745988699, 0.9301335765765264, 0.9288345942130456, 0.927417693229822, 0.9300111004939446, 0.92218472636663, 0.9295858053060678, 0.9303624056852781, 0.9276442069273728, 0.9318717420101166, 0.9247942979519184, 0.9276881493054904, 0.9273691910963792, 0.9303947801773365, 0.932835592673375, 0.9332863550919753, 0.9306028278974386, 0.9265717359689566, 0.9339496791362762, 0.9256633726450113, 0.928832260461954, 0.9337208569049835, 0.9291697373756995, 0.9316475849885207, 0.9327015555821933, 0.9349274428991171, 0.9268837548219241, 0.9362310308676499, 0.9331268416001246, 0.9322947355417105, 0.9334319761166205, 0.9358404462154095, 0.9314973033391513, 0.931712290415397, 0.9335659902829391, 0.9306374673659985, 0.9344420112096347, 0.9316891523507925, 0.9326368409853715, 0.9351724065267123, 0.9348442256450653, 0.933193869315661, 0.9315666556358337, 0.9274708536955026, 0.9315597116947174, 0.9361339417787699, 0.9295326540103326, 0.9334319600692162, 0.9363073271054488, 0.9332840511432061, 0.9299879578443674, 0.933219279234226, 0.9321098304711856, 0.9275795381802779, 0.933646905880708, 0.9323664192969983, 0.9325628853761233, 0.9319087473245767, 0.935560730787424, 0.9350568743852469, 0.933489760527244, 0.9309680324334365, 0.9357664355864892, 0.9329927747066205, 0.9328032823709341, 0.9341346140091236, 0.9336422842282516, 0.9364043772220612, 0.9332169684079977, 0.9314834704765906, 0.9336191461636469, 0.9305334641383245, 0.9367303114670974, 0.9317816266646752, 0.9344928814814641, 0.931982730443661, 0.9340375616000249, 0.9337162444224725, 0.9370400263712957, 0.9351678123840919, 0.9333325486916763, 0.9358589167778308, 0.9351400503745446, 0.9333926645609049, 0.9347980068280146, 0.9344744040415838, 0.9321144406612103, 0.9325258984015539, 0.9353735080132117, 0.9345252651434678, 0.9352232905534598, 0.9323641176407154, 0.9331823472793286, 0.9345899361830491, 0.935271815611766, 0.9367095094460708, 0.9379229797766759, 0.9341877538424271, 0.9349944706146534, 0.9374144902596107, 0.9366887120100168, 0.9299232409550593, 0.9369221742336566, 0.9355769019860488, 0.938939984028156, 0.934620025066229, 0.9351308116546044, 0.9349967585160182, 0.933533659348121, 0.9343333886219904, 0.9320566585430732, 0.9372364924504206, 0.9298169589959658, 0.9341161388617295, 0.9317931670408982, 0.9347517444537237, 0.9321745358980619, 0.934335701740705, 0.9357456496128669, 0.9369660913944244, 0.9328286808270675, 0.9340306245363675, 0.9366332292556763, 0.9344119979785039, 0.9370284699476682, 0.9338526290196639, 0.9324126060192401, 0.9341069070192484, 0.9349574973950019, 0.9345044356126052, 0.9352394571671119, 0.9336076424672053, 0.934284868148657, 0.9377311399349799, 0.935237128000993], 'val_mDice': [0.020392809063196182, 0.024670379881102305, 0.027068435572660886, 0.03000440699263261, 0.03049401523402104, 0.02941467337167034, 0.035712937179666296, 0.04581137832540732, 0.05641900332501301, 0.0633597717835353, 0.07093311925060473, 0.06987977808771226, 0.07407509693159507, 0.08147385565993878, 0.09090330709631626, 0.10053467549956761, 0.10573756164656235, 0.11331424690209903, 0.12538636705050102, 0.13148593372450426, 0.1445637961419729, 0.14800744561048654, 0.16198593077178186, 0.16639165179087564, 0.1854324620217085, 0.2000944041288816, 0.21240645174223644, 0.22213010060099456, 0.23111336277081415, 0.24586095546300596, 0.25088923739699215, 0.25797562587719697, 0.2644494932431441, 0.25433885334775996, 0.26238708140758366, 0.27579030050681186, 0.2694653178063723, 0.28479944456082124, 0.3018655553460121, 0.3051750069627395, 0.3156149100798827, 0.30608553152817947, 0.31171228192173517, 0.32152465788217693, 0.32588552511655366, 0.33763307648209423, 0.3465149835325204, 0.34326344012067866, 0.34671490582136005, 0.3462090916358508, 0.3499207275991256, 0.3571674580184313, 0.36305661929341465, 0.37158488768797654, 0.3678395559008305, 0.3465978755400731, 0.3746131555392192, 0.3593077608025991, 0.36970549563948923, 0.3651853415828485, 0.36558273835824084, 0.37792056426405907, 0.3912213613780645, 0.3807143551798967, 0.36528777388425976, 0.3860785525578719, 0.382244443664184, 0.3826198847248004, 0.3980579152703285, 0.3782711968972133, 0.3960558700446899, 0.3946911185406722, 0.388209326336017, 0.39675692984691036, 0.3880114165636209, 0.39221998762625915, 0.40006209909915924, 0.4042698603409987, 0.4060082252209003, 0.40517737429875594, 0.40823476761579514, 0.4041067785941638, 0.42008879551520717, 0.39411957877186626, 0.3994371842306394, 0.42251326258365923, 0.40689336307919943, 0.41072720375198585, 0.4101796757716399, 0.4216567954191795, 0.39110735288033116, 0.42194757667871624, 0.4186537666962697, 0.41840679141191334, 0.42357015208556104, 0.4218960381471194, 0.40827846699036086, 0.41460050298617435, 0.4017297172775635, 0.4108051276550843, 0.4194824409026366, 0.41735681748160947, 0.41726473489632976, 0.43291436995451266, 0.42843968306596464, 0.4245544866873668, 0.4258365791577559, 0.4100197140987103, 0.41626850630228335, 0.425476505779303, 0.415070761854832, 0.42118440052637685, 0.42848800122737885, 0.4293055064403094, 0.4186103848310617, 0.4284567294212488, 0.42758747792014706, 0.4168151705884017, 0.42774963264281934, 0.4169944364290971, 0.4219331810107598, 0.4326199831870886, 0.437489200096864, 0.43596586470420545, 0.4325550551024767, 0.42498590911810213, 0.4355240108875128, 0.4234101462822694, 0.43743403255939484, 0.4254012726820432, 0.43046846699256164, 0.43003006279468536, 0.41869086542954814, 0.4215944383579951, 0.43977044408137983, 0.4261238082097127, 0.43555381378302205, 0.42972087344297993, 0.4363704908352632, 0.4281239171440785, 0.4247433990240097, 0.4308079039821258, 0.43881314992904663, 0.43416461119285, 0.4304899682219212, 0.434730972808141, 0.43516248636520827, 0.4346842384682252, 0.42739663616969037, 0.4303212297650484, 0.42840120282310706, 0.42455657055744755, 0.42912479088856625, 0.4332618713378906, 0.4340755675847714, 0.437169883113641, 0.43583059826722514, 0.42460655592955077, 0.4411018662727796, 0.4457168063292137, 0.4453213249261563, 0.4348104687837454, 0.4365444914079629, 0.44289923917788726, 0.4359742523385928, 0.4216195098482646, 0.4430179475591733, 0.4405055997463373, 0.4479793252853247, 0.43971769167826724, 0.43397411560783017, 0.43414952892523545, 0.4285985959263948, 0.43208683969882816, 0.4344928299005215, 0.4424668097725281, 0.4260736004664348, 0.4410278017704303, 0.4211075517993707, 0.433989371244724, 0.42571358382701874, 0.4355292830329675, 0.44450221153405994, 0.44008900683659774, 0.4261038097051474, 0.4309177089195985, 0.43611185997724533, 0.4297794120816084, 0.43919188557909083, 0.4371226647725472, 0.4300800424355727, 0.4341995343565941, 0.43121288888729536, 0.4326355732404269, 0.4354585334658623, 0.4406684316121615, 0.44051520984906417, 0.434158614048591, 0.4347542008528343], 'loss': [275.7895978670614, 119.01075312990791, 59.33267088322515, 35.559939681037946, 24.78267002668612, 19.249022133157094, 15.891434543282083, 13.699212354761883, 12.30991010592013, 11.300742338451965, 10.54874643140049, 9.970435481180093, 9.467359740960799, 9.040725073185088, 8.69311298827541, 8.349625204010312, 8.069435165034639, 7.795601648454526, 7.543973946762648, 7.312385128442183, 7.1292157931531435, 6.923513590932211, 6.751754869452088, 6.585250680411224, 6.399871392123848, 6.194353668669718, 6.02504028654358, 5.852449534773376, 5.705881961188193, 5.569826179046335, 5.462957112780139, 5.347844113681761, 5.251168926890394, 5.1671802032744765, 5.052129039352784, 4.9792190512354875, 4.908674947067478, 4.818288731001119, 4.746343884049862, 4.663420386544389, 4.5965336571427535, 4.529455718613304, 4.478179774666518, 4.43984120964927, 4.376075474228419, 4.312071819185804, 4.287337715683153, 4.229715946887589, 4.200360053691391, 4.154323823236731, 4.115735994321574, 4.101398040642985, 4.067642171994398, 4.0239177896430505, 3.9943354677738414, 3.958426247616706, 3.9373390157867223, 3.9038428433825585, 3.8894551347610995, 3.868522040597475, 3.8320746720744205, 3.81200486582649, 3.8120563694397966, 3.7853362039040785, 3.7680978335369386, 3.7432942978517603, 3.721615834516715, 3.7051169640417854, 3.6926230723200653, 3.661548295870243, 3.6613543328572207, 3.6411185102643953, 3.6173387814368487, 3.609856343847639, 3.6050556415804835, 3.5823439063153386, 3.5656726557665577, 3.5511259689857013, 3.534381704896422, 3.5308266462550155, 3.512598501638789, 3.491388817296569, 3.5074725272547087, 3.4796761076663416, 3.4724328373231144, 3.4652739099764487, 3.452901373663303, 3.446138328559546, 3.428342401569074, 3.4145320401710193, 3.433474074295721, 3.409649111309208, 3.4000764741653953, 3.3931388873230715, 3.383257439443681, 3.3712849226932864, 3.3641384661467018, 3.3494467985516794, 3.3565262557395346, 3.3550092705521077, 3.3316123024911377, 3.3237702028564255, 3.3317317159525706, 3.3235148628248585, 3.3196908460819534, 3.302894497779664, 3.305070156058537, 3.2972239189940407, 3.28377190782258, 3.28299989615003, 3.2732104486307843, 3.2751059012913037, 3.294117425636514, 3.2569431061675647, 3.2531113243186582, 3.2394226110807196, 3.2390134067187106, 3.2284156402780333, 3.2301597713071857, 3.2268001381611895, 3.2164773416730252, 3.2220443869982764, 3.211347104581115, 3.214155071128902, 3.2084835602724167, 3.1986744079622387, 3.190380555989942, 3.181796408266882, 3.1816456395190307, 3.1993630228915095, 3.1818073968831038, 3.1633597347092763, 3.1748185159095175, 3.153375346810354, 3.164240239445578, 3.155670696526098, 3.1533935233960233, 3.1532134617055507, 3.152902676458235, 3.1482283304309995, 3.1404608609018254, 3.127952586402865, 3.145151341070298, 3.1264914621959847, 3.125960824630978, 3.1333484358533794, 3.133651120190162, 3.111968877372031, 3.1129309146769053, 3.109121810693995, 3.1225587287798344, 3.113912617677606, 3.099516061413698, 3.1051563428884763, 3.0953486890444553, 3.099122404150667, 3.090796094961869, 3.095601178321585, 3.0878989466395663, 3.0755601397500008, 3.0701749374711906, 3.0756967785964644, 3.0756968866137355, 3.073587076916616, 3.0747953055475095, 3.070092897007059, 3.069792274794191, 3.074056278974337, 3.0684833931305042, 3.0524327831053704, 3.0498141135213386, 3.0575116270089584, 3.058744687859471, 3.062450566426377, 3.0511765925523417, 3.0466335172265326, 3.0380171832591527, 3.040638591248666, 3.032726560331167, 3.041756248942471, 3.0445743875398876, 3.037116681057336, 3.042093164287765, 3.0322517335475356, 3.0314815840597555, 3.0417115887537505, 3.0274928017404923, 3.0265998444213733, 3.021613582049328, 3.031837157170557, 3.0300659305394317, 3.0317976450245068, 3.011675260454639, 3.02577727173875, 3.025551201847991, 3.016542507792315, 3.0252558499214603, 3.0149566616330827, 3.0047909543598466], 'acc': [0.5792062325054547, 0.8537086130046079, 0.853092032726495, 0.8676942624718591, 0.8745354592695697, 0.8748466765145078, 0.8748469858726738, 0.8748470098398647, 0.8748468729959998, 0.8751108467958185, 0.8756737304846431, 0.876947865834219, 0.8765371045283062, 0.8754214428617408, 0.8761896399194812, 0.8775640431776771, 0.8789918780887543, 0.8787671878946275, 0.8790007221361241, 0.8796798844217847, 0.8819189632465553, 0.8843484174432343, 0.8872636058701477, 0.8900047589533555, 0.8925293653112248, 0.8944238680079449, 0.8962425651128558, 0.8982380263086154, 0.8993420538059693, 0.9004015372730232, 0.900985369001336, 0.9017648308586944, 0.9026277361458808, 0.9031234477362949, 0.9040130253186869, 0.9043373223062614, 0.904838328661431, 0.9057464405673022, 0.9060173969938121, 0.9066797097450713, 0.9070455780891676, 0.9073522030554123, 0.9076222715304807, 0.9077777289645012, 0.9085215183242401, 0.9093909189015706, 0.9096785898931152, 0.9101603798768204, 0.910605743996037, 0.9111620242235285, 0.9116327487860594, 0.9118325645761122, 0.9123733014921019, 0.912818267217237, 0.913180340175247, 0.9136916806609233, 0.9140112085952801, 0.9144569272914957, 0.914763243476766, 0.9150884079062432, 0.915623514736687, 0.9159670976610473, 0.9161585994249647, 0.9163560225362654, 0.9168168585601465, 0.917084753227489, 0.9175515128250635, 0.9177914972423155, 0.918085646076329, 0.9185243124376086, 0.9185348211163296, 0.9188347991944503, 0.9190214195726851, 0.919295350778348, 0.919318736682439, 0.91978161549157, 0.9199000675113375, 0.9202158739950937, 0.9203699740450574, 0.9203048853277006, 0.9205715635886417, 0.9206996140835811, 0.9205236773115786, 0.9207198088551474, 0.9204627327963862, 0.9208067815143068, 0.9208514756858651, 0.921032489236624, 0.9213310683893892, 0.9212994973377846, 0.9210788669769124, 0.9213432661174111, 0.9216793742960894, 0.9215463355965559, 0.9216757600107152, 0.9217992455968336, 0.9217995566041182, 0.9223105656114504, 0.9221542699084361, 0.9218275337094785, 0.9225053911355109, 0.9223645268238131, 0.9220882720869428, 0.9223516875336688, 0.9224979416543263, 0.9227383508295698, 0.9227438003311274, 0.9227792298476063, 0.9229194293391119, 0.922986845567549, 0.9231139208177002, 0.923044577934948, 0.9226849620757612, 0.9231594828269496, 0.9235370752447086, 0.923561660745185, 0.9235235045869379, 0.9238336804629274, 0.9237434318295861, 0.9236529775016765, 0.9238933637211885, 0.9238210001862995, 0.924068855346596, 0.9238304437597574, 0.9239797348533292, 0.9242490723015889, 0.9242275045782858, 0.9243868558672668, 0.9243854822558824, 0.9241478485802954, 0.9244585499039636, 0.9246546420226026, 0.9245929871220699, 0.9246247838390694, 0.9247173609839565, 0.9246364655798568, 0.9247656243265, 0.9246477264600695, 0.9248586238711388, 0.9248170581710939, 0.9251201140872889, 0.9251062375619866, 0.9248469419379544, 0.9250073596899727, 0.925036290788479, 0.9251193393533392, 0.9249002580920632, 0.925269403443789, 0.925511830876391, 0.925368967189743, 0.9250609674742221, 0.9252909552860796, 0.9255713961676985, 0.9254870445407494, 0.9256634265418028, 0.9256419179339058, 0.9257218397478331, 0.9256750178174384, 0.9256194410899748, 0.9258415110379713, 0.9258804407867791, 0.925931941316234, 0.9259910702122695, 0.9260133944082819, 0.9258040902107325, 0.9260523205949934, 0.9258950073637888, 0.9259256424691024, 0.9261338380466448, 0.9262409631225067, 0.9262928603254627, 0.9260994976026199, 0.9262740856979118, 0.926197378065691, 0.9263560258186712, 0.9264302934424349, 0.9265465508096169, 0.9265150640719515, 0.9264393126811751, 0.9263972629263295, 0.9264891075711953, 0.926648109323983, 0.9265496772307262, 0.9266988771060489, 0.9266088660557332, 0.9265475468772766, 0.9267242146376082, 0.9268815278688128, 0.9266951947450572, 0.9266625628813951, 0.9268507394119568, 0.9266752659983162, 0.9269271511867962, 0.926744011988073, 0.9267617703902373, 0.9268285679408218, 0.926893945347391, 0.9269149204269278, 0.9271999539555433], 'mDice': [0.018648724909897583, 0.023509100718741696, 0.026605908745453332, 0.02873942291773433, 0.030998564349580357, 0.03357321203557546, 0.037041232551248006, 0.04131456673142546, 0.046130624259709985, 0.05131249589670148, 0.055803817570522836, 0.06008616270845989, 0.06320238771470371, 0.06729667371355892, 0.07203500044694369, 0.07787371856897154, 0.08356950687263259, 0.08763414975157811, 0.09378804519592501, 0.10072404753757251, 0.10722903283519916, 0.11511951633829104, 0.12317313606516084, 0.13104873625876817, 0.14107484925742036, 0.15432770768209522, 0.16588476333221935, 0.1768712373753596, 0.18675877204034136, 0.1961966976708925, 0.20437074799846813, 0.21223125073903823, 0.21964434619309617, 0.22501412076911814, 0.23328117468717033, 0.2397942471705404, 0.24612836029205684, 0.253460442372313, 0.26036675704379786, 0.2684981967870087, 0.2758751083874872, 0.2824727144890426, 0.28862161111602985, 0.29321648222949576, 0.29950607704696297, 0.30719673936188713, 0.31069968818771676, 0.3168772622292897, 0.321666413621468, 0.32678263075873054, 0.33145990251483354, 0.33412826631294124, 0.3382977830558908, 0.34314150399556187, 0.3470873966830953, 0.35201216413011016, 0.35417242956856326, 0.3591150584986517, 0.3598340016762335, 0.3629869711824551, 0.3678380665464242, 0.37047502773390545, 0.37100437423691013, 0.3743340003829067, 0.3766191182437335, 0.3793857465763984, 0.3814829564216578, 0.38376221582973774, 0.38598628939819724, 0.3886565374160488, 0.38954835857213077, 0.3922121295418388, 0.3934995375122189, 0.3955356913728599, 0.39645705313894963, 0.3984964138399705, 0.4012795480939281, 0.40177408299971185, 0.404752369179537, 0.40408417404132235, 0.4073141715238095, 0.4094583770011301, 0.4074872403548101, 0.4109728880241042, 0.41177796593266947, 0.4129982925336497, 0.41360590116933493, 0.41519203328299126, 0.41743318441773236, 0.4180703615555187, 0.4160729481056257, 0.41916423370331773, 0.4207577124453774, 0.42112955611800723, 0.42240448175821865, 0.4238778754847522, 0.42467087955808197, 0.4269385129078655, 0.42629688655517645, 0.42567582607797005, 0.42934371615252154, 0.4297595756977211, 0.42910515369331553, 0.4299945620129186, 0.43125767274051535, 0.4329277548327785, 0.4330314090691028, 0.43397618251775644, 0.4361689622129757, 0.43574234307565385, 0.43720469609855295, 0.43662077463070076, 0.43482526643991054, 0.4392962639598755, 0.43988480816839104, 0.4424168880515946, 0.441716278098468, 0.44298518086055827, 0.4434124641116998, 0.4438969469797146, 0.44500810977366645, 0.4437551413208623, 0.44616544359088506, 0.44526487752145194, 0.4465370621065982, 0.44712814810717333, 0.4489314421553745, 0.44960483979290505, 0.44975142681564584, 0.4472070515106319, 0.45008439611687184, 0.4526450579719197, 0.45183717411272223, 0.4534156351393795, 0.4525055782849703, 0.4533854882528913, 0.4534224244728746, 0.45396009975063245, 0.4547184975035327, 0.4549532872492852, 0.45575801455247866, 0.45749692573633555, 0.45573007142995653, 0.45709365312490013, 0.4580912579280099, 0.4575181546756991, 0.45732647424077105, 0.46028925484917455, 0.4612961359363986, 0.4607563183408354, 0.45959095069098427, 0.4597332238474862, 0.4626726447782822, 0.4616464596249615, 0.46252956382575033, 0.4625672010943138, 0.46338762805645045, 0.46362105982996027, 0.4635270628763787, 0.4659280913222224, 0.46649759342438907, 0.46659056395828247, 0.4658238463661036, 0.46704262756426196, 0.4658143256606183, 0.4673284938235812, 0.4668117408095872, 0.4665601553374261, 0.46714256191939696, 0.4700437132848053, 0.470066185241848, 0.4687210484139431, 0.4689498153640304, 0.4690729623750975, 0.4697815931108312, 0.4711186734252252, 0.4711313327443036, 0.47205520305072407, 0.47227918543213576, 0.4719371806868304, 0.4712222965426807, 0.4722488336289137, 0.47211715723055925, 0.47322560861235363, 0.47314337751010704, 0.471371301591127, 0.47375868315418784, 0.47482315303104494, 0.4741350293082526, 0.47310919934760204, 0.4740404971799408, 0.47288927002706443, 0.4760307495685056, 0.47397036247562135, 0.47489312203742035, 0.4754223562986618, 0.4750144776548834, 0.47572505836346973, 0.47718232071928507]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.31s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.08s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.88s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:30,  1.38s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:01,  1.49s/it]predicting train subjects:   1%|          | 3/285 [00:04<06:57,  1.48s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:25,  1.59s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<07:13,  1.55s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:39,  1.65s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<08:03,  1.74s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<08:09,  1.77s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<07:55,  1.72s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:31,  1.86s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:41,  1.90s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:36,  1.89s/it]predicting train subjects:   5%|▍         | 13/285 [00:23<08:41,  1.92s/it]predicting train subjects:   5%|▍         | 14/285 [00:25<08:40,  1.92s/it]predicting train subjects:   5%|▌         | 15/285 [00:27<08:36,  1.91s/it]predicting train subjects:   6%|▌         | 16/285 [00:29<08:42,  1.94s/it]predicting train subjects:   6%|▌         | 17/285 [00:31<08:47,  1.97s/it]predicting train subjects:   6%|▋         | 18/285 [00:33<08:44,  1.96s/it]predicting train subjects:   7%|▋         | 19/285 [00:35<08:47,  1.98s/it]predicting train subjects:   7%|▋         | 20/285 [00:37<08:44,  1.98s/it]predicting train subjects:   7%|▋         | 21/285 [00:38<08:36,  1.96s/it]predicting train subjects:   8%|▊         | 22/285 [00:40<08:30,  1.94s/it]predicting train subjects:   8%|▊         | 23/285 [00:42<08:33,  1.96s/it]predicting train subjects:   8%|▊         | 24/285 [00:44<08:24,  1.93s/it]predicting train subjects:   9%|▉         | 25/285 [00:46<08:31,  1.97s/it]predicting train subjects:   9%|▉         | 26/285 [00:48<08:33,  1.98s/it]predicting train subjects:   9%|▉         | 27/285 [00:50<08:29,  1.97s/it]predicting train subjects:  10%|▉         | 28/285 [00:52<08:17,  1.93s/it]predicting train subjects:  10%|█         | 29/285 [00:54<08:07,  1.90s/it]predicting train subjects:  11%|█         | 30/285 [00:56<07:56,  1.87s/it]predicting train subjects:  11%|█         | 31/285 [00:58<07:57,  1.88s/it]predicting train subjects:  11%|█         | 32/285 [00:59<07:48,  1.85s/it]predicting train subjects:  12%|█▏        | 33/285 [01:01<07:46,  1.85s/it]predicting train subjects:  12%|█▏        | 34/285 [01:03<07:35,  1.81s/it]predicting train subjects:  12%|█▏        | 35/285 [01:05<07:35,  1.82s/it]predicting train subjects:  13%|█▎        | 36/285 [01:07<07:35,  1.83s/it]predicting train subjects:  13%|█▎        | 37/285 [01:08<07:35,  1.84s/it]predicting train subjects:  13%|█▎        | 38/285 [01:10<07:32,  1.83s/it]predicting train subjects:  14%|█▎        | 39/285 [01:12<07:31,  1.84s/it]predicting train subjects:  14%|█▍        | 40/285 [01:14<07:25,  1.82s/it]predicting train subjects:  14%|█▍        | 41/285 [01:16<07:26,  1.83s/it]predicting train subjects:  15%|█▍        | 42/285 [01:18<07:31,  1.86s/it]predicting train subjects:  15%|█▌        | 43/285 [01:19<07:18,  1.81s/it]predicting train subjects:  15%|█▌        | 44/285 [01:21<07:12,  1.80s/it]predicting train subjects:  16%|█▌        | 45/285 [01:23<07:14,  1.81s/it]predicting train subjects:  16%|█▌        | 46/285 [01:25<06:57,  1.75s/it]predicting train subjects:  16%|█▋        | 47/285 [01:26<06:42,  1.69s/it]predicting train subjects:  17%|█▋        | 48/285 [01:28<06:35,  1.67s/it]predicting train subjects:  17%|█▋        | 49/285 [01:29<06:32,  1.66s/it]predicting train subjects:  18%|█▊        | 50/285 [01:31<06:24,  1.64s/it]predicting train subjects:  18%|█▊        | 51/285 [01:33<06:20,  1.63s/it]predicting train subjects:  18%|█▊        | 52/285 [01:34<06:18,  1.62s/it]predicting train subjects:  19%|█▊        | 53/285 [01:36<06:17,  1.63s/it]predicting train subjects:  19%|█▉        | 54/285 [01:38<06:16,  1.63s/it]predicting train subjects:  19%|█▉        | 55/285 [01:39<06:06,  1.59s/it]predicting train subjects:  20%|█▉        | 56/285 [01:41<06:06,  1.60s/it]predicting train subjects:  20%|██        | 57/285 [01:42<06:03,  1.60s/it]predicting train subjects:  20%|██        | 58/285 [01:44<06:02,  1.60s/it]predicting train subjects:  21%|██        | 59/285 [01:45<06:01,  1.60s/it]predicting train subjects:  21%|██        | 60/285 [01:47<06:14,  1.67s/it]predicting train subjects:  21%|██▏       | 61/285 [01:49<06:10,  1.66s/it]predicting train subjects:  22%|██▏       | 62/285 [01:51<06:09,  1.66s/it]predicting train subjects:  22%|██▏       | 63/285 [01:52<06:11,  1.67s/it]predicting train subjects:  22%|██▏       | 64/285 [01:54<06:05,  1.65s/it]predicting train subjects:  23%|██▎       | 65/285 [01:56<06:23,  1.74s/it]predicting train subjects:  23%|██▎       | 66/285 [01:58<06:24,  1.76s/it]predicting train subjects:  24%|██▎       | 67/285 [01:59<06:13,  1.71s/it]predicting train subjects:  24%|██▍       | 68/285 [02:01<06:04,  1.68s/it]predicting train subjects:  24%|██▍       | 69/285 [02:02<05:57,  1.65s/it]predicting train subjects:  25%|██▍       | 70/285 [02:04<05:56,  1.66s/it]predicting train subjects:  25%|██▍       | 71/285 [02:06<05:53,  1.65s/it]predicting train subjects:  25%|██▌       | 72/285 [02:07<05:47,  1.63s/it]predicting train subjects:  26%|██▌       | 73/285 [02:09<05:45,  1.63s/it]predicting train subjects:  26%|██▌       | 74/285 [02:11<05:42,  1.63s/it]predicting train subjects:  26%|██▋       | 75/285 [02:12<05:35,  1.60s/it]predicting train subjects:  27%|██▋       | 76/285 [02:14<05:32,  1.59s/it]predicting train subjects:  27%|██▋       | 77/285 [02:15<05:36,  1.62s/it]predicting train subjects:  27%|██▋       | 78/285 [02:17<05:30,  1.60s/it]predicting train subjects:  28%|██▊       | 79/285 [02:18<05:21,  1.56s/it]predicting train subjects:  28%|██▊       | 80/285 [02:20<05:17,  1.55s/it]predicting train subjects:  28%|██▊       | 81/285 [02:21<05:17,  1.55s/it]predicting train subjects:  29%|██▉       | 82/285 [02:23<05:16,  1.56s/it]predicting train subjects:  29%|██▉       | 83/285 [02:25<05:15,  1.56s/it]predicting train subjects:  29%|██▉       | 84/285 [02:26<05:16,  1.57s/it]predicting train subjects:  30%|██▉       | 85/285 [02:28<05:36,  1.68s/it]predicting train subjects:  30%|███       | 86/285 [02:30<05:40,  1.71s/it]predicting train subjects:  31%|███       | 87/285 [02:32<05:48,  1.76s/it]predicting train subjects:  31%|███       | 88/285 [02:34<05:50,  1.78s/it]predicting train subjects:  31%|███       | 89/285 [02:35<05:52,  1.80s/it]predicting train subjects:  32%|███▏      | 90/285 [02:37<05:51,  1.80s/it]predicting train subjects:  32%|███▏      | 91/285 [02:39<05:50,  1.81s/it]predicting train subjects:  32%|███▏      | 92/285 [02:41<05:55,  1.84s/it]predicting train subjects:  33%|███▎      | 93/285 [02:43<05:51,  1.83s/it]predicting train subjects:  33%|███▎      | 94/285 [02:45<05:47,  1.82s/it]predicting train subjects:  33%|███▎      | 95/285 [02:47<05:53,  1.86s/it]predicting train subjects:  34%|███▎      | 96/285 [02:48<05:49,  1.85s/it]predicting train subjects:  34%|███▍      | 97/285 [02:50<05:45,  1.84s/it]predicting train subjects:  34%|███▍      | 98/285 [02:52<05:42,  1.83s/it]predicting train subjects:  35%|███▍      | 99/285 [02:54<05:37,  1.81s/it]predicting train subjects:  35%|███▌      | 100/285 [02:56<05:35,  1.81s/it]predicting train subjects:  35%|███▌      | 101/285 [02:57<05:32,  1.81s/it]predicting train subjects:  36%|███▌      | 102/285 [02:59<05:33,  1.82s/it]predicting train subjects:  36%|███▌      | 103/285 [03:01<05:32,  1.83s/it]predicting train subjects:  36%|███▋      | 104/285 [03:03<05:30,  1.83s/it]predicting train subjects:  37%|███▋      | 105/285 [03:05<05:29,  1.83s/it]predicting train subjects:  37%|███▋      | 106/285 [03:07<05:26,  1.82s/it]predicting train subjects:  38%|███▊      | 107/285 [03:08<05:27,  1.84s/it]predicting train subjects:  38%|███▊      | 108/285 [03:10<05:23,  1.83s/it]predicting train subjects:  38%|███▊      | 109/285 [03:12<05:19,  1.82s/it]predicting train subjects:  39%|███▊      | 110/285 [03:14<05:22,  1.84s/it]predicting train subjects:  39%|███▉      | 111/285 [03:16<05:20,  1.84s/it]predicting train subjects:  39%|███▉      | 112/285 [03:18<05:27,  1.89s/it]predicting train subjects:  40%|███▉      | 113/285 [03:20<05:22,  1.88s/it]predicting train subjects:  40%|████      | 114/285 [03:21<05:15,  1.84s/it]predicting train subjects:  40%|████      | 115/285 [03:23<05:17,  1.87s/it]predicting train subjects:  41%|████      | 116/285 [03:25<05:12,  1.85s/it]predicting train subjects:  41%|████      | 117/285 [03:27<05:08,  1.84s/it]predicting train subjects:  41%|████▏     | 118/285 [03:29<05:06,  1.84s/it]predicting train subjects:  42%|████▏     | 119/285 [03:31<05:05,  1.84s/it]predicting train subjects:  42%|████▏     | 120/285 [03:32<05:05,  1.85s/it]predicting train subjects:  42%|████▏     | 121/285 [03:34<04:52,  1.78s/it]predicting train subjects:  43%|████▎     | 122/285 [03:36<04:38,  1.71s/it]predicting train subjects:  43%|████▎     | 123/285 [03:37<04:22,  1.62s/it]predicting train subjects:  44%|████▎     | 124/285 [03:39<04:23,  1.64s/it]predicting train subjects:  44%|████▍     | 125/285 [03:40<04:23,  1.65s/it]predicting train subjects:  44%|████▍     | 126/285 [03:42<04:22,  1.65s/it]predicting train subjects:  45%|████▍     | 127/285 [03:44<04:19,  1.64s/it]predicting train subjects:  45%|████▍     | 128/285 [03:45<04:17,  1.64s/it]predicting train subjects:  45%|████▌     | 129/285 [03:47<04:16,  1.64s/it]predicting train subjects:  46%|████▌     | 130/285 [03:49<04:20,  1.68s/it]predicting train subjects:  46%|████▌     | 131/285 [03:50<04:11,  1.64s/it]predicting train subjects:  46%|████▋     | 132/285 [03:52<04:09,  1.63s/it]predicting train subjects:  47%|████▋     | 133/285 [03:54<04:09,  1.64s/it]predicting train subjects:  47%|████▋     | 134/285 [03:55<04:08,  1.65s/it]predicting train subjects:  47%|████▋     | 135/285 [03:57<04:08,  1.66s/it]predicting train subjects:  48%|████▊     | 136/285 [03:58<04:01,  1.62s/it]predicting train subjects:  48%|████▊     | 137/285 [04:00<03:57,  1.60s/it]predicting train subjects:  48%|████▊     | 138/285 [04:02<03:54,  1.60s/it]predicting train subjects:  49%|████▉     | 139/285 [04:03<03:53,  1.60s/it]predicting train subjects:  49%|████▉     | 140/285 [04:05<03:54,  1.62s/it]predicting train subjects:  49%|████▉     | 141/285 [04:06<03:51,  1.61s/it]predicting train subjects:  50%|████▉     | 142/285 [04:08<03:49,  1.61s/it]predicting train subjects:  50%|█████     | 143/285 [04:10<03:44,  1.58s/it]predicting train subjects:  51%|█████     | 144/285 [04:11<03:41,  1.57s/it]predicting train subjects:  51%|█████     | 145/285 [04:13<03:38,  1.56s/it]predicting train subjects:  51%|█████     | 146/285 [04:14<03:32,  1.53s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:16<03:29,  1.52s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:17<03:27,  1.51s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:18<03:22,  1.49s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:20<03:23,  1.51s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:21<03:18,  1.48s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:23<03:16,  1.48s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:24<03:15,  1.48s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:26<03:13,  1.48s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:27<03:14,  1.49s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:29<03:10,  1.48s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:30<03:08,  1.47s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:32<03:04,  1.45s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:33<03:03,  1.45s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:35<03:02,  1.46s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:36<02:59,  1.45s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:38<02:59,  1.46s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:39<02:57,  1.46s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:40<02:56,  1.46s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:42<02:56,  1.47s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:43<02:55,  1.47s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:45<02:52,  1.46s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:46<02:48,  1.44s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:48<02:47,  1.44s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:49<02:44,  1.43s/it]predicting train subjects:  60%|██████    | 171/285 [04:51<02:45,  1.45s/it]predicting train subjects:  60%|██████    | 172/285 [04:52<02:42,  1.43s/it]predicting train subjects:  61%|██████    | 173/285 [04:54<02:42,  1.45s/it]predicting train subjects:  61%|██████    | 174/285 [04:55<02:43,  1.47s/it]predicting train subjects:  61%|██████▏   | 175/285 [04:57<02:47,  1.52s/it]predicting train subjects:  62%|██████▏   | 176/285 [04:58<02:41,  1.48s/it]predicting train subjects:  62%|██████▏   | 177/285 [04:59<02:38,  1.46s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:01<02:33,  1.43s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:02<02:28,  1.40s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:04<02:25,  1.39s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:05<02:22,  1.37s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:06<02:19,  1.35s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:08<02:20,  1.38s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:09<02:18,  1.38s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:10<02:17,  1.38s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:12<02:14,  1.36s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:13<02:13,  1.36s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:14<02:13,  1.38s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:16<02:08,  1.34s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:17<02:07,  1.35s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:18<02:07,  1.36s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:20<02:05,  1.35s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:21<02:04,  1.36s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:23<02:04,  1.37s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:24<02:02,  1.36s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:26<02:09,  1.45s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:27<02:13,  1.52s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:29<02:20,  1.61s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:31<02:19,  1.63s/it]predicting train subjects:  70%|███████   | 200/285 [05:32<02:18,  1.63s/it]predicting train subjects:  71%|███████   | 201/285 [05:34<02:17,  1.63s/it]predicting train subjects:  71%|███████   | 202/285 [05:36<02:17,  1.65s/it]predicting train subjects:  71%|███████   | 203/285 [05:37<02:14,  1.64s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:39<02:12,  1.63s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:41<02:14,  1.68s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:42<02:13,  1.69s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:44<02:13,  1.71s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:46<02:10,  1.69s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:47<02:07,  1.68s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:49<02:08,  1.71s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:51<02:04,  1.68s/it]predicting train subjects:  74%|███████▍  | 212/285 [05:53<02:03,  1.69s/it]predicting train subjects:  75%|███████▍  | 213/285 [05:54<02:01,  1.69s/it]predicting train subjects:  75%|███████▌  | 214/285 [05:56<01:54,  1.62s/it]predicting train subjects:  75%|███████▌  | 215/285 [05:57<01:49,  1.56s/it]predicting train subjects:  76%|███████▌  | 216/285 [05:59<01:45,  1.53s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:00<01:40,  1.48s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:01<01:39,  1.48s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:03<01:37,  1.48s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:04<01:34,  1.46s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:06<01:34,  1.47s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:07<01:31,  1.45s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:09<01:30,  1.46s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:10<01:29,  1.47s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:12<01:27,  1.46s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:13<01:26,  1.46s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:15<01:24,  1.46s/it]predicting train subjects:  80%|████████  | 228/285 [06:16<01:23,  1.46s/it]predicting train subjects:  80%|████████  | 229/285 [06:17<01:21,  1.46s/it]predicting train subjects:  81%|████████  | 230/285 [06:19<01:21,  1.48s/it]predicting train subjects:  81%|████████  | 231/285 [06:20<01:19,  1.48s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:22<01:23,  1.57s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:24<01:25,  1.64s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:26<01:25,  1.67s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:28<01:26,  1.73s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:29<01:25,  1.75s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:31<01:24,  1.77s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:33<01:24,  1.79s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:35<01:22,  1.80s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:37<01:20,  1.78s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:38<01:18,  1.78s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:40<01:17,  1.81s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:42<01:16,  1.82s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:44<01:14,  1.82s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:46<01:14,  1.86s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:48<01:12,  1.87s/it]predicting train subjects:  87%|████████▋ | 247/285 [06:50<01:11,  1.88s/it]predicting train subjects:  87%|████████▋ | 248/285 [06:52<01:08,  1.86s/it]predicting train subjects:  87%|████████▋ | 249/285 [06:53<01:06,  1.84s/it]predicting train subjects:  88%|████████▊ | 250/285 [06:55<01:00,  1.73s/it]predicting train subjects:  88%|████████▊ | 251/285 [06:56<00:54,  1.62s/it]predicting train subjects:  88%|████████▊ | 252/285 [06:58<00:50,  1.54s/it]predicting train subjects:  89%|████████▉ | 253/285 [06:59<00:47,  1.48s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:00<00:45,  1.45s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:02<00:42,  1.43s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:03<00:40,  1.40s/it]predicting train subjects:  90%|█████████ | 257/285 [07:04<00:39,  1.41s/it]predicting train subjects:  91%|█████████ | 258/285 [07:06<00:38,  1.44s/it]predicting train subjects:  91%|█████████ | 259/285 [07:07<00:36,  1.42s/it]predicting train subjects:  91%|█████████ | 260/285 [07:09<00:34,  1.39s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:10<00:32,  1.37s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:11<00:32,  1.39s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:13<00:29,  1.36s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:14<00:28,  1.36s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:15<00:27,  1.39s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:17<00:26,  1.40s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:18<00:25,  1.41s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:20<00:26,  1.53s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:22<00:25,  1.62s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:24<00:24,  1.66s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:26<00:23,  1.71s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:27<00:22,  1.74s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:29<00:21,  1.77s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:31<00:19,  1.77s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:33<00:17,  1.78s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:35<00:16,  1.81s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:36<00:14,  1.80s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:38<00:12,  1.81s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:40<00:10,  1.81s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:42<00:08,  1.80s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:44<00:07,  1.79s/it]predicting train subjects:  99%|█████████▉| 282/285 [07:45<00:05,  1.79s/it]predicting train subjects:  99%|█████████▉| 283/285 [07:47<00:03,  1.80s/it]predicting train subjects: 100%|█████████▉| 284/285 [07:49<00:01,  1.80s/it]predicting train subjects: 100%|██████████| 285/285 [07:51<00:00,  1.79s/it]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<08:06,  1.71s/it]Loading train:   1%|          | 2/285 [00:03<07:52,  1.67s/it]Loading train:   1%|          | 3/285 [00:05<07:55,  1.69s/it]Loading train:   1%|▏         | 4/285 [00:07<08:37,  1.84s/it]Loading train:   2%|▏         | 5/285 [00:08<08:01,  1.72s/it]Loading train:   2%|▏         | 6/285 [00:10<08:37,  1.85s/it]Loading train:   2%|▏         | 7/285 [00:12<08:55,  1.93s/it]Loading train:   3%|▎         | 8/285 [00:14<08:41,  1.88s/it]Loading train:   3%|▎         | 9/285 [00:16<08:33,  1.86s/it]Loading train:   4%|▎         | 10/285 [00:17<07:59,  1.74s/it]Loading train:   4%|▍         | 11/285 [00:19<07:15,  1.59s/it]Loading train:   4%|▍         | 12/285 [00:20<06:44,  1.48s/it]Loading train:   5%|▍         | 13/285 [00:21<06:29,  1.43s/it]Loading train:   5%|▍         | 14/285 [00:23<06:34,  1.46s/it]Loading train:   5%|▌         | 15/285 [00:24<06:49,  1.52s/it]Loading train:   6%|▌         | 16/285 [00:26<06:47,  1.51s/it]Loading train:   6%|▌         | 17/285 [00:27<06:40,  1.49s/it]Loading train:   6%|▋         | 18/285 [00:29<06:39,  1.50s/it]Loading train:   7%|▋         | 19/285 [00:30<06:16,  1.42s/it]Loading train:   7%|▋         | 20/285 [00:31<05:54,  1.34s/it]Loading train:   7%|▋         | 21/285 [00:33<05:59,  1.36s/it]Loading train:   8%|▊         | 22/285 [00:34<06:21,  1.45s/it]Loading train:   8%|▊         | 23/285 [00:36<06:04,  1.39s/it]Loading train:   8%|▊         | 24/285 [00:37<06:22,  1.47s/it]Loading train:   9%|▉         | 25/285 [00:38<05:54,  1.36s/it]Loading train:   9%|▉         | 26/285 [00:40<05:40,  1.31s/it]Loading train:   9%|▉         | 27/285 [00:41<05:33,  1.29s/it]Loading train:  10%|▉         | 28/285 [00:42<05:35,  1.31s/it]Loading train:  10%|█         | 29/285 [00:43<05:29,  1.29s/it]Loading train:  11%|█         | 30/285 [00:45<05:25,  1.28s/it]Loading train:  11%|█         | 31/285 [00:46<05:22,  1.27s/it]Loading train:  11%|█         | 32/285 [00:47<05:26,  1.29s/it]Loading train:  12%|█▏        | 33/285 [00:49<05:34,  1.33s/it]Loading train:  12%|█▏        | 34/285 [00:50<05:41,  1.36s/it]Loading train:  12%|█▏        | 35/285 [00:52<05:53,  1.41s/it]Loading train:  13%|█▎        | 36/285 [00:53<05:37,  1.36s/it]Loading train:  13%|█▎        | 37/285 [00:54<05:27,  1.32s/it]Loading train:  13%|█▎        | 38/285 [00:55<05:23,  1.31s/it]Loading train:  14%|█▎        | 39/285 [00:57<05:14,  1.28s/it]Loading train:  14%|█▍        | 40/285 [00:58<05:02,  1.23s/it]Loading train:  14%|█▍        | 41/285 [00:59<05:07,  1.26s/it]Loading train:  15%|█▍        | 42/285 [01:00<05:17,  1.31s/it]Loading train:  15%|█▌        | 43/285 [01:02<05:37,  1.39s/it]Loading train:  15%|█▌        | 44/285 [01:03<05:24,  1.35s/it]Loading train:  16%|█▌        | 45/285 [01:05<05:22,  1.34s/it]Loading train:  16%|█▌        | 46/285 [01:06<05:36,  1.41s/it]Loading train:  16%|█▋        | 47/285 [01:07<05:11,  1.31s/it]Loading train:  17%|█▋        | 48/285 [01:08<04:43,  1.20s/it]Loading train:  17%|█▋        | 49/285 [01:09<04:40,  1.19s/it]Loading train:  18%|█▊        | 50/285 [01:11<04:40,  1.19s/it]Loading train:  18%|█▊        | 51/285 [01:12<04:47,  1.23s/it]Loading train:  18%|█▊        | 52/285 [01:13<04:27,  1.15s/it]Loading train:  19%|█▊        | 53/285 [01:14<04:20,  1.12s/it]Loading train:  19%|█▉        | 54/285 [01:15<04:13,  1.10s/it]Loading train:  19%|█▉        | 55/285 [01:16<04:05,  1.07s/it]Loading train:  20%|█▉        | 56/285 [01:17<04:04,  1.07s/it]Loading train:  20%|██        | 57/285 [01:18<04:12,  1.11s/it]Loading train:  20%|██        | 58/285 [01:19<04:18,  1.14s/it]Loading train:  21%|██        | 59/285 [01:21<04:47,  1.27s/it]Loading train:  21%|██        | 60/285 [01:22<04:49,  1.29s/it]Loading train:  21%|██▏       | 61/285 [01:23<04:27,  1.20s/it]Loading train:  22%|██▏       | 62/285 [01:25<04:41,  1.26s/it]Loading train:  22%|██▏       | 63/285 [01:26<04:19,  1.17s/it]Loading train:  22%|██▏       | 64/285 [01:27<04:48,  1.30s/it]Loading train:  23%|██▎       | 65/285 [01:29<05:30,  1.50s/it]Loading train:  23%|██▎       | 66/285 [01:31<05:58,  1.64s/it]Loading train:  24%|██▎       | 67/285 [01:33<05:52,  1.62s/it]Loading train:  24%|██▍       | 68/285 [01:34<05:21,  1.48s/it]Loading train:  24%|██▍       | 69/285 [01:35<05:02,  1.40s/it]Loading train:  25%|██▍       | 70/285 [01:36<04:52,  1.36s/it]Loading train:  25%|██▍       | 71/285 [01:37<04:32,  1.27s/it]Loading train:  25%|██▌       | 72/285 [01:39<04:35,  1.30s/it]Loading train:  26%|██▌       | 73/285 [01:40<04:21,  1.23s/it]Loading train:  26%|██▌       | 74/285 [01:41<04:16,  1.22s/it]Loading train:  26%|██▋       | 75/285 [01:42<04:04,  1.17s/it]Loading train:  27%|██▋       | 76/285 [01:44<04:17,  1.23s/it]Loading train:  27%|██▋       | 77/285 [01:45<04:34,  1.32s/it]Loading train:  27%|██▋       | 78/285 [01:46<04:12,  1.22s/it]Loading train:  28%|██▊       | 79/285 [01:47<04:01,  1.17s/it]Loading train:  28%|██▊       | 80/285 [01:48<04:05,  1.20s/it]Loading train:  28%|██▊       | 81/285 [01:50<04:07,  1.22s/it]Loading train:  29%|██▉       | 82/285 [01:51<03:50,  1.13s/it]Loading train:  29%|██▉       | 83/285 [01:52<04:00,  1.19s/it]Loading train:  29%|██▉       | 84/285 [01:53<04:12,  1.26s/it]Loading train:  30%|██▉       | 85/285 [01:55<04:45,  1.43s/it]Loading train:  30%|███       | 86/285 [01:57<04:47,  1.44s/it]Loading train:  31%|███       | 87/285 [01:58<04:48,  1.46s/it]Loading train:  31%|███       | 88/285 [01:59<04:44,  1.45s/it]Loading train:  31%|███       | 89/285 [02:01<04:37,  1.42s/it]Loading train:  32%|███▏      | 90/285 [02:02<04:42,  1.45s/it]Loading train:  32%|███▏      | 91/285 [02:04<04:29,  1.39s/it]Loading train:  32%|███▏      | 92/285 [02:05<04:29,  1.40s/it]Loading train:  33%|███▎      | 93/285 [02:07<04:34,  1.43s/it]Loading train:  33%|███▎      | 94/285 [02:08<04:30,  1.42s/it]Loading train:  33%|███▎      | 95/285 [02:09<04:23,  1.39s/it]Loading train:  34%|███▎      | 96/285 [02:10<04:12,  1.34s/it]Loading train:  34%|███▍      | 97/285 [02:12<04:15,  1.36s/it]Loading train:  34%|███▍      | 98/285 [02:13<03:59,  1.28s/it]Loading train:  35%|███▍      | 99/285 [02:14<04:06,  1.32s/it]Loading train:  35%|███▌      | 100/285 [02:16<03:58,  1.29s/it]Loading train:  35%|███▌      | 101/285 [02:17<04:13,  1.38s/it]Loading train:  36%|███▌      | 102/285 [02:19<04:09,  1.36s/it]Loading train:  36%|███▌      | 103/285 [02:20<04:01,  1.32s/it]Loading train:  36%|███▋      | 104/285 [02:21<03:49,  1.27s/it]Loading train:  37%|███▋      | 105/285 [02:22<03:43,  1.24s/it]Loading train:  37%|███▋      | 106/285 [02:23<03:39,  1.22s/it]Loading train:  38%|███▊      | 107/285 [02:24<03:35,  1.21s/it]Loading train:  38%|███▊      | 108/285 [02:26<03:35,  1.22s/it]Loading train:  38%|███▊      | 109/285 [02:27<03:29,  1.19s/it]Loading train:  39%|███▊      | 110/285 [02:28<03:53,  1.33s/it]Loading train:  39%|███▉      | 111/285 [02:30<03:55,  1.36s/it]Loading train:  39%|███▉      | 112/285 [02:31<03:47,  1.32s/it]Loading train:  40%|███▉      | 113/285 [02:32<03:36,  1.26s/it]Loading train:  40%|████      | 114/285 [02:34<03:38,  1.28s/it]Loading train:  40%|████      | 115/285 [02:35<03:41,  1.30s/it]Loading train:  41%|████      | 116/285 [02:36<03:46,  1.34s/it]Loading train:  41%|████      | 117/285 [02:38<03:51,  1.38s/it]Loading train:  41%|████▏     | 118/285 [02:39<03:53,  1.40s/it]Loading train:  42%|████▏     | 119/285 [02:41<04:03,  1.47s/it]Loading train:  42%|████▏     | 120/285 [02:42<03:59,  1.45s/it]Loading train:  42%|████▏     | 121/285 [02:44<04:07,  1.51s/it]Loading train:  43%|████▎     | 122/285 [02:45<04:00,  1.48s/it]Loading train:  43%|████▎     | 123/285 [02:47<03:55,  1.46s/it]Loading train:  44%|████▎     | 124/285 [02:48<04:07,  1.54s/it]Loading train:  44%|████▍     | 125/285 [02:50<03:46,  1.42s/it]Loading train:  44%|████▍     | 126/285 [02:51<03:29,  1.32s/it]Loading train:  45%|████▍     | 127/285 [02:52<03:27,  1.31s/it]Loading train:  45%|████▍     | 128/285 [02:53<03:24,  1.30s/it]Loading train:  45%|████▌     | 129/285 [02:54<03:14,  1.25s/it]Loading train:  46%|████▌     | 130/285 [02:56<03:09,  1.22s/it]Loading train:  46%|████▌     | 131/285 [02:57<03:28,  1.35s/it]Loading train:  46%|████▋     | 132/285 [02:58<03:19,  1.31s/it]Loading train:  47%|████▋     | 133/285 [03:00<03:12,  1.26s/it]Loading train:  47%|████▋     | 134/285 [03:01<03:12,  1.28s/it]Loading train:  47%|████▋     | 135/285 [03:02<03:10,  1.27s/it]Loading train:  48%|████▊     | 136/285 [03:03<03:07,  1.26s/it]Loading train:  48%|████▊     | 137/285 [03:05<03:05,  1.25s/it]Loading train:  48%|████▊     | 138/285 [03:06<03:11,  1.30s/it]Loading train:  49%|████▉     | 139/285 [03:08<03:18,  1.36s/it]Loading train:  49%|████▉     | 140/285 [03:09<03:15,  1.35s/it]Loading train:  49%|████▉     | 141/285 [03:10<03:14,  1.35s/it]Loading train:  50%|████▉     | 142/285 [03:12<03:20,  1.40s/it]Loading train:  50%|█████     | 143/285 [03:13<03:08,  1.33s/it]Loading train:  51%|█████     | 144/285 [03:14<02:52,  1.22s/it]Loading train:  51%|█████     | 145/285 [03:15<02:41,  1.15s/it]Loading train:  51%|█████     | 146/285 [03:16<02:44,  1.18s/it]Loading train:  52%|█████▏    | 147/285 [03:17<02:42,  1.18s/it]Loading train:  52%|█████▏    | 148/285 [03:18<02:34,  1.12s/it]Loading train:  52%|█████▏    | 149/285 [03:19<02:31,  1.11s/it]Loading train:  53%|█████▎    | 150/285 [03:21<02:33,  1.13s/it]Loading train:  53%|█████▎    | 151/285 [03:22<02:29,  1.12s/it]Loading train:  53%|█████▎    | 152/285 [03:23<02:37,  1.19s/it]Loading train:  54%|█████▎    | 153/285 [03:24<02:31,  1.15s/it]Loading train:  54%|█████▍    | 154/285 [03:25<02:34,  1.18s/it]Loading train:  54%|█████▍    | 155/285 [03:26<02:29,  1.15s/it]Loading train:  55%|█████▍    | 156/285 [03:28<02:37,  1.22s/it]Loading train:  55%|█████▌    | 157/285 [03:29<02:23,  1.12s/it]Loading train:  55%|█████▌    | 158/285 [03:30<02:27,  1.16s/it]Loading train:  56%|█████▌    | 159/285 [03:31<02:26,  1.16s/it]Loading train:  56%|█████▌    | 160/285 [03:32<02:29,  1.20s/it]Loading train:  56%|█████▋    | 161/285 [03:33<02:20,  1.13s/it]Loading train:  57%|█████▋    | 162/285 [03:35<02:29,  1.22s/it]Loading train:  57%|█████▋    | 163/285 [03:36<02:22,  1.17s/it]Loading train:  58%|█████▊    | 164/285 [03:37<02:17,  1.14s/it]Loading train:  58%|█████▊    | 165/285 [03:38<02:11,  1.10s/it]Loading train:  58%|█████▊    | 166/285 [03:39<02:12,  1.11s/it]Loading train:  59%|█████▊    | 167/285 [03:40<02:15,  1.15s/it]Loading train:  59%|█████▉    | 168/285 [03:42<02:20,  1.20s/it]Loading train:  59%|█████▉    | 169/285 [03:43<02:24,  1.24s/it]Loading train:  60%|█████▉    | 170/285 [03:44<02:22,  1.24s/it]Loading train:  60%|██████    | 171/285 [03:45<02:15,  1.19s/it]Loading train:  60%|██████    | 172/285 [03:47<02:23,  1.27s/it]Loading train:  61%|██████    | 173/285 [03:48<02:12,  1.18s/it]Loading train:  61%|██████    | 174/285 [03:49<02:05,  1.13s/it]Loading train:  61%|██████▏   | 175/285 [03:50<02:04,  1.13s/it]Loading train:  62%|██████▏   | 176/285 [03:51<02:05,  1.15s/it]Loading train:  62%|██████▏   | 177/285 [03:52<02:02,  1.13s/it]Loading train:  62%|██████▏   | 178/285 [03:53<02:01,  1.13s/it]Loading train:  63%|██████▎   | 179/285 [03:54<02:05,  1.18s/it]Loading train:  63%|██████▎   | 180/285 [03:56<01:59,  1.14s/it]Loading train:  64%|██████▎   | 181/285 [03:57<01:55,  1.11s/it]Loading train:  64%|██████▍   | 182/285 [03:58<01:54,  1.11s/it]Loading train:  64%|██████▍   | 183/285 [03:59<01:57,  1.16s/it]Loading train:  65%|██████▍   | 184/285 [04:00<01:50,  1.09s/it]Loading train:  65%|██████▍   | 185/285 [04:01<01:47,  1.07s/it]Loading train:  65%|██████▌   | 186/285 [04:02<01:54,  1.16s/it]Loading train:  66%|██████▌   | 187/285 [04:04<01:57,  1.20s/it]Loading train:  66%|██████▌   | 188/285 [04:05<01:54,  1.18s/it]Loading train:  66%|██████▋   | 189/285 [04:06<01:48,  1.13s/it]Loading train:  67%|██████▋   | 190/285 [04:07<01:49,  1.15s/it]Loading train:  67%|██████▋   | 191/285 [04:08<01:51,  1.19s/it]Loading train:  67%|██████▋   | 192/285 [04:09<01:50,  1.18s/it]Loading train:  68%|██████▊   | 193/285 [04:11<01:54,  1.25s/it]Loading train:  68%|██████▊   | 194/285 [04:12<01:42,  1.13s/it]Loading train:  68%|██████▊   | 195/285 [04:13<01:43,  1.15s/it]Loading train:  69%|██████▉   | 196/285 [04:14<01:55,  1.30s/it]Loading train:  69%|██████▉   | 197/285 [04:16<01:50,  1.26s/it]Loading train:  69%|██████▉   | 198/285 [04:17<01:49,  1.26s/it]Loading train:  70%|██████▉   | 199/285 [04:18<01:49,  1.27s/it]Loading train:  70%|███████   | 200/285 [04:19<01:44,  1.23s/it]Loading train:  71%|███████   | 201/285 [04:20<01:41,  1.20s/it]Loading train:  71%|███████   | 202/285 [04:22<01:42,  1.24s/it]Loading train:  71%|███████   | 203/285 [04:23<01:41,  1.24s/it]Loading train:  72%|███████▏  | 204/285 [04:24<01:38,  1.22s/it]Loading train:  72%|███████▏  | 205/285 [04:25<01:35,  1.20s/it]Loading train:  72%|███████▏  | 206/285 [04:27<01:43,  1.31s/it]Loading train:  73%|███████▎  | 207/285 [04:28<01:34,  1.21s/it]Loading train:  73%|███████▎  | 208/285 [04:29<01:34,  1.23s/it]Loading train:  73%|███████▎  | 209/285 [04:30<01:33,  1.23s/it]Loading train:  74%|███████▎  | 210/285 [04:32<01:38,  1.31s/it]Loading train:  74%|███████▍  | 211/285 [04:33<01:41,  1.38s/it]Loading train:  74%|███████▍  | 212/285 [04:35<01:34,  1.29s/it]Loading train:  75%|███████▍  | 213/285 [04:36<01:30,  1.26s/it]Loading train:  75%|███████▌  | 214/285 [04:37<01:31,  1.29s/it]Loading train:  75%|███████▌  | 215/285 [04:38<01:31,  1.30s/it]Loading train:  76%|███████▌  | 216/285 [04:40<01:29,  1.30s/it]Loading train:  76%|███████▌  | 217/285 [04:41<01:21,  1.19s/it]Loading train:  76%|███████▋  | 218/285 [04:42<01:17,  1.16s/it]Loading train:  77%|███████▋  | 219/285 [04:43<01:14,  1.13s/it]Loading train:  77%|███████▋  | 220/285 [04:44<01:14,  1.14s/it]Loading train:  78%|███████▊  | 221/285 [04:45<01:14,  1.17s/it]Loading train:  78%|███████▊  | 222/285 [04:46<01:12,  1.15s/it]Loading train:  78%|███████▊  | 223/285 [04:47<01:10,  1.14s/it]Loading train:  79%|███████▊  | 224/285 [04:48<01:05,  1.07s/it]Loading train:  79%|███████▉  | 225/285 [04:49<01:05,  1.10s/it]Loading train:  79%|███████▉  | 226/285 [04:51<01:09,  1.17s/it]Loading train:  80%|███████▉  | 227/285 [04:52<01:07,  1.17s/it]Loading train:  80%|████████  | 228/285 [04:53<01:07,  1.18s/it]Loading train:  80%|████████  | 229/285 [04:55<01:11,  1.27s/it]Loading train:  81%|████████  | 230/285 [04:56<01:10,  1.28s/it]Loading train:  81%|████████  | 231/285 [04:57<01:04,  1.19s/it]Loading train:  81%|████████▏ | 232/285 [04:58<01:05,  1.25s/it]Loading train:  82%|████████▏ | 233/285 [05:00<01:07,  1.30s/it]Loading train:  82%|████████▏ | 234/285 [05:01<01:06,  1.30s/it]Loading train:  82%|████████▏ | 235/285 [05:02<01:05,  1.30s/it]Loading train:  83%|████████▎ | 236/285 [05:04<01:06,  1.36s/it]Loading train:  83%|████████▎ | 237/285 [05:05<01:06,  1.40s/it]Loading train:  84%|████████▎ | 238/285 [05:07<01:08,  1.45s/it]Loading train:  84%|████████▍ | 239/285 [05:08<01:07,  1.47s/it]Loading train:  84%|████████▍ | 240/285 [05:10<01:06,  1.48s/it]Loading train:  85%|████████▍ | 241/285 [05:11<01:02,  1.42s/it]Loading train:  85%|████████▍ | 242/285 [05:12<00:59,  1.38s/it]Loading train:  85%|████████▌ | 243/285 [05:14<00:57,  1.37s/it]Loading train:  86%|████████▌ | 244/285 [05:15<00:56,  1.39s/it]Loading train:  86%|████████▌ | 245/285 [05:17<00:54,  1.37s/it]Loading train:  86%|████████▋ | 246/285 [05:18<00:52,  1.35s/it]Loading train:  87%|████████▋ | 247/285 [05:19<00:50,  1.32s/it]Loading train:  87%|████████▋ | 248/285 [05:20<00:48,  1.31s/it]Loading train:  87%|████████▋ | 249/285 [05:22<00:47,  1.32s/it]Loading train:  88%|████████▊ | 250/285 [05:23<00:44,  1.28s/it]Loading train:  88%|████████▊ | 251/285 [05:24<00:40,  1.20s/it]Loading train:  88%|████████▊ | 252/285 [05:25<00:39,  1.19s/it]Loading train:  89%|████████▉ | 253/285 [05:26<00:35,  1.11s/it]Loading train:  89%|████████▉ | 254/285 [05:27<00:34,  1.12s/it]Loading train:  89%|████████▉ | 255/285 [05:28<00:32,  1.09s/it]Loading train:  90%|████████▉ | 256/285 [05:29<00:31,  1.09s/it]Loading train:  90%|█████████ | 257/285 [05:30<00:29,  1.05s/it]Loading train:  91%|█████████ | 258/285 [05:31<00:29,  1.09s/it]Loading train:  91%|█████████ | 259/285 [05:33<00:28,  1.10s/it]Loading train:  91%|█████████ | 260/285 [05:34<00:28,  1.15s/it]Loading train:  92%|█████████▏| 261/285 [05:35<00:30,  1.26s/it]Loading train:  92%|█████████▏| 262/285 [05:36<00:28,  1.22s/it]Loading train:  92%|█████████▏| 263/285 [05:38<00:25,  1.17s/it]Loading train:  93%|█████████▎| 264/285 [05:39<00:24,  1.18s/it]Loading train:  93%|█████████▎| 265/285 [05:40<00:22,  1.15s/it]Loading train:  93%|█████████▎| 266/285 [05:41<00:22,  1.17s/it]Loading train:  94%|█████████▎| 267/285 [05:42<00:21,  1.18s/it]Loading train:  94%|█████████▍| 268/285 [05:44<00:22,  1.33s/it]Loading train:  94%|█████████▍| 269/285 [05:45<00:21,  1.34s/it]Loading train:  95%|█████████▍| 270/285 [05:47<00:19,  1.33s/it]Loading train:  95%|█████████▌| 271/285 [05:48<00:18,  1.34s/it]Loading train:  95%|█████████▌| 272/285 [05:49<00:16,  1.30s/it]Loading train:  96%|█████████▌| 273/285 [05:50<00:15,  1.31s/it]Loading train:  96%|█████████▌| 274/285 [05:52<00:14,  1.30s/it]Loading train:  96%|█████████▋| 275/285 [05:53<00:13,  1.37s/it]Loading train:  97%|█████████▋| 276/285 [05:55<00:12,  1.35s/it]Loading train:  97%|█████████▋| 277/285 [05:56<00:11,  1.47s/it]Loading train:  98%|█████████▊| 278/285 [05:58<00:10,  1.50s/it]Loading train:  98%|█████████▊| 279/285 [06:00<00:09,  1.54s/it]Loading train:  98%|█████████▊| 280/285 [06:01<00:07,  1.53s/it]Loading train:  99%|█████████▊| 281/285 [06:02<00:05,  1.50s/it]Loading train:  99%|█████████▉| 282/285 [06:04<00:04,  1.50s/it]Loading train:  99%|█████████▉| 283/285 [06:05<00:02,  1.48s/it]Loading train: 100%|█████████▉| 284/285 [06:07<00:01,  1.39s/it]Loading train: 100%|██████████| 285/285 [06:08<00:00,  1.37s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 3/285 [00:00<00:11, 24.94it/s]concatenating: train:   2%|▏         | 6/285 [00:00<00:11, 24.23it/s]concatenating: train:   3%|▎         | 9/285 [00:00<00:10, 25.41it/s]concatenating: train:   7%|▋         | 20/285 [00:00<00:08, 32.81it/s]concatenating: train:  14%|█▍        | 40/285 [00:00<00:05, 43.70it/s]concatenating: train:  18%|█▊        | 51/285 [00:00<00:04, 50.61it/s]concatenating: train:  22%|██▏       | 63/285 [00:00<00:03, 61.20it/s]concatenating: train:  26%|██▌       | 73/285 [00:01<00:03, 55.88it/s]concatenating: train:  29%|██▉       | 82/285 [00:01<00:03, 51.00it/s]concatenating: train:  31%|███       | 89/285 [00:01<00:03, 49.03it/s]concatenating: train:  34%|███▎      | 96/285 [00:01<00:04, 44.43it/s]concatenating: train:  36%|███▌      | 102/285 [00:01<00:06, 29.81it/s]concatenating: train:  38%|███▊      | 107/285 [00:02<00:05, 32.82it/s]concatenating: train:  39%|███▉      | 112/285 [00:02<00:04, 35.22it/s]concatenating: train:  41%|████      | 117/285 [00:02<00:05, 33.00it/s]concatenating: train:  42%|████▏     | 121/285 [00:02<00:06, 26.30it/s]concatenating: train:  44%|████▍     | 125/285 [00:02<00:06, 26.45it/s]concatenating: train:  45%|████▌     | 129/285 [00:02<00:05, 28.27it/s]concatenating: train:  47%|████▋     | 134/285 [00:02<00:04, 31.49it/s]concatenating: train:  48%|████▊     | 138/285 [00:03<00:04, 30.32it/s]concatenating: train:  51%|█████     | 144/285 [00:03<00:03, 35.33it/s]concatenating: train:  53%|█████▎    | 152/285 [00:03<00:03, 41.86it/s]concatenating: train:  55%|█████▌    | 158/285 [00:03<00:03, 41.89it/s]concatenating: train:  58%|█████▊    | 164/285 [00:03<00:02, 44.55it/s]concatenating: train:  60%|█████▉    | 170/285 [00:03<00:02, 46.91it/s]concatenating: train:  62%|██████▏   | 176/285 [00:03<00:02, 47.64it/s]concatenating: train:  64%|██████▍   | 182/285 [00:03<00:02, 50.29it/s]concatenating: train:  66%|██████▋   | 189/285 [00:04<00:01, 54.89it/s]concatenating: train:  68%|██████▊   | 195/285 [00:04<00:01, 47.72it/s]concatenating: train:  72%|███████▏  | 205/285 [00:04<00:01, 56.25it/s]concatenating: train:  74%|███████▍  | 212/285 [00:04<00:01, 52.58it/s]concatenating: train:  76%|███████▋  | 218/285 [00:04<00:01, 52.46it/s]concatenating: train:  79%|███████▊  | 224/285 [00:04<00:01, 49.65it/s]concatenating: train:  81%|████████  | 230/285 [00:05<00:01, 31.37it/s]concatenating: train:  82%|████████▏ | 235/285 [00:05<00:01, 29.98it/s]concatenating: train:  85%|████████▍ | 241/285 [00:05<00:01, 34.31it/s]concatenating: train:  86%|████████▋ | 246/285 [00:05<00:01, 35.31it/s]concatenating: train:  89%|████████▉ | 253/285 [00:05<00:00, 40.61it/s]concatenating: train:  91%|█████████ | 259/285 [00:05<00:00, 43.41it/s]concatenating: train:  94%|█████████▎| 267/285 [00:05<00:00, 49.87it/s]concatenating: train:  96%|█████████▋| 275/285 [00:05<00:00, 54.02it/s]concatenating: train:  99%|█████████▊| 281/285 [00:06<00:00, 44.68it/s]concatenating: train: 100%|██████████| 285/285 [00:06<00:00, 46.35it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.54s/it]Loading test:  67%|██████▋   | 2/3 [00:03<00:01,  1.53s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 173.50it/s]2019-07-06 21:58:21.243063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 21:58:21.243156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 21:58:21.243171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 21:58:21.243180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 21:58:21.243608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:10,  3.87it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:08,  4.53it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.26it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:05,  5.50it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:06,  4.93it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.70it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  4.99it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:03,  6.22it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.37it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.95it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  5.12it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:02,  6.11it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.53it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  5.45it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:02,  5.94it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  5.01it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  5.97it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  6.52it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.12it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  5.72it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  4.94it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  7.36it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 4  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 80, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 80, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 80, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 80, 30)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 80, 30)   8130        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 80, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 80, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 80, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 80, 30)   8130        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 80, 30)   120         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 80, 30)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 80, 30)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 80, 20)   5420        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 80, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 80, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 80, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 80, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 80, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 40, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 40, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 40, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 40, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 40, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 40, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 40, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 20, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 20, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 20, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 20, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 20, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 20, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 20, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 20, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 40, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 40, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 40, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 40, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 40, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 40, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 30)   5430        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 30)   120         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 30)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 30)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 13)   403         dropout_9[0][0]                  
==================================================================================================
Total params: 211,933
Trainable params: 68,213
Non-trainable params: 143,720
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 20s - loss: 187.7829 - acc: 0.6344 - mDice: 0.0177 - val_loss: 68.3980 - val_acc: 0.9047 - val_mDice: 0.0118

Epoch 00001: val_mDice improved from -inf to 0.01185, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 12s - loss: 65.0254 - acc: 0.8649 - mDice: 0.0177 - val_loss: 30.3099 - val_acc: 0.9047 - val_mDice: 0.0139

Epoch 00002: val_mDice improved from 0.01185 to 0.01386, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 12s - loss: 30.1209 - acc: 0.8686 - mDice: 0.0178 - val_loss: 14.8812 - val_acc: 0.9047 - val_mDice: 0.0142

Epoch 00003: val_mDice improved from 0.01386 to 0.01418, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 12s - loss: 18.4040 - acc: 0.8692 - mDice: 0.0187 - val_loss: 10.4205 - val_acc: 0.9047 - val_mDice: 0.0159

Epoch 00004: val_mDice improved from 0.01418 to 0.01589, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 12s - loss: 13.5802 - acc: 0.8692 - mDice: 0.0201 - val_loss: 8.1347 - val_acc: 0.9047 - val_mDice: 0.0176

Epoch 00005: val_mDice improved from 0.01589 to 0.01760, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 12s - loss: 11.1591 - acc: 0.8692 - mDice: 0.0219 - val_loss: 6.9714 - val_acc: 0.9047 - val_mDice: 0.0175

Epoch 00006: val_mDice did not improve from 0.01760
Epoch 7/300
 - 12s - loss: 9.7245 - acc: 0.8692 - mDice: 0.0243 - val_loss: 6.5340 - val_acc: 0.9047 - val_mDice: 0.0163

Epoch 00007: val_mDice did not improve from 0.01760
Epoch 8/300
 - 12s - loss: 8.7184 - acc: 0.8692 - mDice: 0.0275 - val_loss: 6.2770 - val_acc: 0.9047 - val_mDice: 0.0188

Epoch 00008: val_mDice improved from 0.01760 to 0.01885, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 12s - loss: 7.9415 - acc: 0.8692 - mDice: 0.0319 - val_loss: 6.1940 - val_acc: 0.9047 - val_mDice: 0.0207

Epoch 00009: val_mDice improved from 0.01885 to 0.02066, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 12s - loss: 7.3568 - acc: 0.8692 - mDice: 0.0365 - val_loss: 6.1443 - val_acc: 0.9047 - val_mDice: 0.0233

Epoch 00010: val_mDice improved from 0.02066 to 0.02329, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 12s - loss: 6.8989 - acc: 0.8692 - mDice: 0.0420 - val_loss: 5.9458 - val_acc: 0.9047 - val_mDice: 0.0289

Epoch 00011: val_mDice improved from 0.02329 to 0.02886, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 12s - loss: 6.5144 - acc: 0.8693 - mDice: 0.0488 - val_loss: 6.0206 - val_acc: 0.9047 - val_mDice: 0.0326

Epoch 00012: val_mDice improved from 0.02886 to 0.03258, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 12s - loss: 6.1690 - acc: 0.8694 - mDice: 0.0565 - val_loss: 5.8628 - val_acc: 0.9047 - val_mDice: 0.0395

Epoch 00013: val_mDice improved from 0.03258 to 0.03952, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 12s - loss: 5.8498 - acc: 0.8696 - mDice: 0.0661 - val_loss: 5.3268 - val_acc: 0.9048 - val_mDice: 0.0590

Epoch 00014: val_mDice improved from 0.03952 to 0.05896, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 12s - loss: 5.5590 - acc: 0.8699 - mDice: 0.0770 - val_loss: 4.2666 - val_acc: 0.9056 - val_mDice: 0.1017

Epoch 00015: val_mDice improved from 0.05896 to 0.10173, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 12s - loss: 5.2808 - acc: 0.8707 - mDice: 0.0892 - val_loss: 4.5180 - val_acc: 0.9054 - val_mDice: 0.1025

Epoch 00016: val_mDice improved from 0.10173 to 0.10248, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 12s - loss: 5.0104 - acc: 0.8714 - mDice: 0.1033 - val_loss: 4.3112 - val_acc: 0.9058 - val_mDice: 0.1191

Epoch 00017: val_mDice improved from 0.10248 to 0.11905, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 12s - loss: 4.7626 - acc: 0.8719 - mDice: 0.1174 - val_loss: 4.7985 - val_acc: 0.9057 - val_mDice: 0.1067

Epoch 00018: val_mDice did not improve from 0.11905
Epoch 19/300
 - 11s - loss: 4.5321 - acc: 0.8731 - mDice: 0.1331 - val_loss: 4.1465 - val_acc: 0.9076 - val_mDice: 0.1455

Epoch 00019: val_mDice improved from 0.11905 to 0.14549, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 12s - loss: 4.3253 - acc: 0.8745 - mDice: 0.1486 - val_loss: 4.1029 - val_acc: 0.9083 - val_mDice: 0.1513

Epoch 00020: val_mDice improved from 0.14549 to 0.15134, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 12s - loss: 4.1362 - acc: 0.8767 - mDice: 0.1644 - val_loss: 4.6649 - val_acc: 0.9103 - val_mDice: 0.1423

Epoch 00021: val_mDice did not improve from 0.15134
Epoch 22/300
 - 12s - loss: 3.9629 - acc: 0.8797 - mDice: 0.1815 - val_loss: 3.4876 - val_acc: 0.9182 - val_mDice: 0.2072

Epoch 00022: val_mDice improved from 0.15134 to 0.20719, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 12s - loss: 3.8008 - acc: 0.8827 - mDice: 0.1981 - val_loss: 3.3499 - val_acc: 0.9198 - val_mDice: 0.2266

Epoch 00023: val_mDice improved from 0.20719 to 0.22659, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 12s - loss: 3.6638 - acc: 0.8857 - mDice: 0.2130 - val_loss: 3.3889 - val_acc: 0.9198 - val_mDice: 0.2334

Epoch 00024: val_mDice improved from 0.22659 to 0.23345, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 12s - loss: 3.5415 - acc: 0.8880 - mDice: 0.2274 - val_loss: 3.1979 - val_acc: 0.9249 - val_mDice: 0.2557

Epoch 00025: val_mDice improved from 0.23345 to 0.25566, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 11s - loss: 3.4194 - acc: 0.8902 - mDice: 0.2417 - val_loss: 3.1607 - val_acc: 0.9263 - val_mDice: 0.2670

Epoch 00026: val_mDice improved from 0.25566 to 0.26702, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 11s - loss: 3.3057 - acc: 0.8923 - mDice: 0.2561 - val_loss: 3.3738 - val_acc: 0.9233 - val_mDice: 0.2671

Epoch 00027: val_mDice improved from 0.26702 to 0.26709, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 12s - loss: 3.2147 - acc: 0.8941 - mDice: 0.2692 - val_loss: 3.2724 - val_acc: 0.9244 - val_mDice: 0.2881

Epoch 00028: val_mDice improved from 0.26709 to 0.28809, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 12s - loss: 3.1202 - acc: 0.8960 - mDice: 0.2833 - val_loss: 3.1875 - val_acc: 0.9280 - val_mDice: 0.3011

Epoch 00029: val_mDice improved from 0.28809 to 0.30105, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 12s - loss: 3.0245 - acc: 0.8978 - mDice: 0.2976 - val_loss: 3.0482 - val_acc: 0.9278 - val_mDice: 0.3115

Epoch 00030: val_mDice improved from 0.30105 to 0.31150, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 12s - loss: 2.9487 - acc: 0.8995 - mDice: 0.3093 - val_loss: 3.0233 - val_acc: 0.9304 - val_mDice: 0.3282

Epoch 00031: val_mDice improved from 0.31150 to 0.32821, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 12s - loss: 2.8759 - acc: 0.9015 - mDice: 0.3214 - val_loss: 2.8906 - val_acc: 0.9303 - val_mDice: 0.3395

Epoch 00032: val_mDice improved from 0.32821 to 0.33955, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 12s - loss: 2.8186 - acc: 0.9029 - mDice: 0.3309 - val_loss: 2.9732 - val_acc: 0.9302 - val_mDice: 0.3453

Epoch 00033: val_mDice improved from 0.33955 to 0.34526, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 12s - loss: 2.7533 - acc: 0.9048 - mDice: 0.3414 - val_loss: 3.0627 - val_acc: 0.9302 - val_mDice: 0.3461

Epoch 00034: val_mDice improved from 0.34526 to 0.34611, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 12s - loss: 2.6964 - acc: 0.9064 - mDice: 0.3513 - val_loss: 3.1894 - val_acc: 0.9301 - val_mDice: 0.3395

Epoch 00035: val_mDice did not improve from 0.34611
Epoch 36/300
 - 12s - loss: 2.6490 - acc: 0.9081 - mDice: 0.3605 - val_loss: 3.2092 - val_acc: 0.9308 - val_mDice: 0.3492

Epoch 00036: val_mDice improved from 0.34611 to 0.34916, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 12s - loss: 2.5888 - acc: 0.9096 - mDice: 0.3703 - val_loss: 3.7978 - val_acc: 0.9257 - val_mDice: 0.3135

Epoch 00037: val_mDice did not improve from 0.34916
Epoch 38/300
 - 12s - loss: 2.5463 - acc: 0.9108 - mDice: 0.3788 - val_loss: 3.0925 - val_acc: 0.9331 - val_mDice: 0.3668

Epoch 00038: val_mDice improved from 0.34916 to 0.36676, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 12s - loss: 2.5017 - acc: 0.9120 - mDice: 0.3866 - val_loss: 3.0510 - val_acc: 0.9330 - val_mDice: 0.3807

Epoch 00039: val_mDice improved from 0.36676 to 0.38069, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 12s - loss: 2.4615 - acc: 0.9131 - mDice: 0.3951 - val_loss: 3.6076 - val_acc: 0.9305 - val_mDice: 0.3428

Epoch 00040: val_mDice did not improve from 0.38069
Epoch 41/300
 - 11s - loss: 2.4198 - acc: 0.9141 - mDice: 0.4024 - val_loss: 3.1073 - val_acc: 0.9305 - val_mDice: 0.3834

Epoch 00041: val_mDice improved from 0.38069 to 0.38339, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 12s - loss: 2.3889 - acc: 0.9149 - mDice: 0.4088 - val_loss: 3.4383 - val_acc: 0.9306 - val_mDice: 0.3638

Epoch 00042: val_mDice did not improve from 0.38339
Epoch 43/300
 - 12s - loss: 2.3509 - acc: 0.9158 - mDice: 0.4166 - val_loss: 3.9117 - val_acc: 0.9263 - val_mDice: 0.3351

Epoch 00043: val_mDice did not improve from 0.38339
Epoch 44/300
 - 12s - loss: 2.3168 - acc: 0.9164 - mDice: 0.4234 - val_loss: 3.5983 - val_acc: 0.9344 - val_mDice: 0.3721

Epoch 00044: val_mDice did not improve from 0.38339
Epoch 45/300
 - 12s - loss: 2.2892 - acc: 0.9174 - mDice: 0.4293 - val_loss: 3.1709 - val_acc: 0.9285 - val_mDice: 0.3934

Epoch 00045: val_mDice improved from 0.38339 to 0.39342, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 12s - loss: 2.2581 - acc: 0.9180 - mDice: 0.4356 - val_loss: 3.4333 - val_acc: 0.9342 - val_mDice: 0.3815

Epoch 00046: val_mDice did not improve from 0.39342
Epoch 47/300
 - 12s - loss: 2.2314 - acc: 0.9188 - mDice: 0.4410 - val_loss: 3.2622 - val_acc: 0.9347 - val_mDice: 0.3911

Epoch 00047: val_mDice did not improve from 0.39342
Epoch 48/300
 - 12s - loss: 2.2090 - acc: 0.9192 - mDice: 0.4452 - val_loss: 3.1841 - val_acc: 0.9361 - val_mDice: 0.4021

Epoch 00048: val_mDice improved from 0.39342 to 0.40211, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 12s - loss: 2.1937 - acc: 0.9199 - mDice: 0.4490 - val_loss: 3.2253 - val_acc: 0.9323 - val_mDice: 0.3966

Epoch 00049: val_mDice did not improve from 0.40211
Epoch 50/300
 - 12s - loss: 2.1591 - acc: 0.9205 - mDice: 0.4549 - val_loss: 3.7979 - val_acc: 0.9327 - val_mDice: 0.3721

Epoch 00050: val_mDice did not improve from 0.40211
Epoch 51/300
 - 12s - loss: 2.1445 - acc: 0.9210 - mDice: 0.4586 - val_loss: 3.2238 - val_acc: 0.9292 - val_mDice: 0.4034

Epoch 00051: val_mDice improved from 0.40211 to 0.40339, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 12s - loss: 2.1232 - acc: 0.9214 - mDice: 0.4628 - val_loss: 3.0377 - val_acc: 0.9352 - val_mDice: 0.4190

Epoch 00052: val_mDice improved from 0.40339 to 0.41900, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 12s - loss: 2.1132 - acc: 0.9219 - mDice: 0.4645 - val_loss: 3.4704 - val_acc: 0.9368 - val_mDice: 0.3984

Epoch 00053: val_mDice did not improve from 0.41900
Epoch 54/300
 - 12s - loss: 2.0876 - acc: 0.9224 - mDice: 0.4699 - val_loss: 3.3055 - val_acc: 0.9302 - val_mDice: 0.3957

Epoch 00054: val_mDice did not improve from 0.41900
Epoch 55/300
 - 12s - loss: 2.0707 - acc: 0.9228 - mDice: 0.4741 - val_loss: 3.2720 - val_acc: 0.9321 - val_mDice: 0.4072

Epoch 00055: val_mDice did not improve from 0.41900
Epoch 56/300
 - 11s - loss: 2.0530 - acc: 0.9234 - mDice: 0.4785 - val_loss: 3.1737 - val_acc: 0.9301 - val_mDice: 0.4137

Epoch 00056: val_mDice did not improve from 0.41900
Epoch 57/300
 - 12s - loss: 2.0406 - acc: 0.9237 - mDice: 0.4808 - val_loss: 3.2232 - val_acc: 0.9346 - val_mDice: 0.4160

Epoch 00057: val_mDice did not improve from 0.41900
Epoch 58/300
 - 12s - loss: 2.0262 - acc: 0.9241 - mDice: 0.4841 - val_loss: 3.1864 - val_acc: 0.9346 - val_mDice: 0.4211

Epoch 00058: val_mDice improved from 0.41900 to 0.42114, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 12s - loss: 2.0041 - acc: 0.9246 - mDice: 0.4883 - val_loss: 3.2124 - val_acc: 0.9352 - val_mDice: 0.4181

Epoch 00059: val_mDice did not improve from 0.42114
Epoch 60/300
 - 12s - loss: 1.9888 - acc: 0.9251 - mDice: 0.4920 - val_loss: 3.1681 - val_acc: 0.9374 - val_mDice: 0.4182

Epoch 00060: val_mDice did not improve from 0.42114
Epoch 61/300
 - 12s - loss: 1.9811 - acc: 0.9252 - mDice: 0.4932 - val_loss: 3.2729 - val_acc: 0.9385 - val_mDice: 0.4177

Epoch 00061: val_mDice did not improve from 0.42114
Epoch 62/300
 - 12s - loss: 1.9653 - acc: 0.9258 - mDice: 0.4973 - val_loss: 3.1771 - val_acc: 0.9393 - val_mDice: 0.4332

Epoch 00062: val_mDice improved from 0.42114 to 0.43317, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 12s - loss: 1.9450 - acc: 0.9261 - mDice: 0.5015 - val_loss: 3.0955 - val_acc: 0.9371 - val_mDice: 0.4331

Epoch 00063: val_mDice did not improve from 0.43317
Epoch 64/300
 - 12s - loss: 1.9388 - acc: 0.9264 - mDice: 0.5033 - val_loss: 3.2815 - val_acc: 0.9385 - val_mDice: 0.4326

Epoch 00064: val_mDice did not improve from 0.43317
Epoch 65/300
 - 12s - loss: 1.9315 - acc: 0.9264 - mDice: 0.5051 - val_loss: 3.2224 - val_acc: 0.9321 - val_mDice: 0.4237

Epoch 00065: val_mDice did not improve from 0.43317
Epoch 66/300
 - 12s - loss: 1.9122 - acc: 0.9270 - mDice: 0.5080 - val_loss: 3.0435 - val_acc: 0.9375 - val_mDice: 0.4425

Epoch 00066: val_mDice improved from 0.43317 to 0.44254, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 67/300
 - 12s - loss: 1.9010 - acc: 0.9273 - mDice: 0.5116 - val_loss: 3.2704 - val_acc: 0.9412 - val_mDice: 0.4337

Epoch 00067: val_mDice did not improve from 0.44254
Epoch 68/300
 - 12s - loss: 1.8907 - acc: 0.9274 - mDice: 0.5133 - val_loss: 3.2076 - val_acc: 0.9357 - val_mDice: 0.4302

Epoch 00068: val_mDice did not improve from 0.44254
Epoch 69/300
 - 12s - loss: 1.8835 - acc: 0.9278 - mDice: 0.5156 - val_loss: 3.2499 - val_acc: 0.9252 - val_mDice: 0.4100

Epoch 00069: val_mDice did not improve from 0.44254
Epoch 70/300
 - 12s - loss: 1.8718 - acc: 0.9280 - mDice: 0.5184 - val_loss: 3.1928 - val_acc: 0.9383 - val_mDice: 0.4394

Epoch 00070: val_mDice did not improve from 0.44254
Epoch 71/300
 - 12s - loss: 1.8596 - acc: 0.9282 - mDice: 0.5212 - val_loss: 3.1939 - val_acc: 0.9368 - val_mDice: 0.4409

Epoch 00071: val_mDice did not improve from 0.44254
Epoch 72/300
 - 12s - loss: 1.8515 - acc: 0.9284 - mDice: 0.5228 - val_loss: 3.6087 - val_acc: 0.9382 - val_mDice: 0.4073

Epoch 00072: val_mDice did not improve from 0.44254
Epoch 73/300
 - 12s - loss: 1.8464 - acc: 0.9287 - mDice: 0.5239 - val_loss: 3.4310 - val_acc: 0.9413 - val_mDice: 0.4348

Epoch 00073: val_mDice did not improve from 0.44254
Epoch 74/300
 - 12s - loss: 1.8375 - acc: 0.9289 - mDice: 0.5262 - val_loss: 3.0671 - val_acc: 0.9354 - val_mDice: 0.4441

Epoch 00074: val_mDice improved from 0.44254 to 0.44405, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 75/300
 - 12s - loss: 1.8171 - acc: 0.9293 - mDice: 0.5306 - val_loss: 3.0737 - val_acc: 0.9398 - val_mDice: 0.4548

Epoch 00075: val_mDice improved from 0.44405 to 0.45478, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 76/300
 - 12s - loss: 1.8176 - acc: 0.9293 - mDice: 0.5304 - val_loss: 3.3253 - val_acc: 0.9392 - val_mDice: 0.4353

Epoch 00076: val_mDice did not improve from 0.45478
Epoch 77/300
 - 12s - loss: 1.8031 - acc: 0.9297 - mDice: 0.5338 - val_loss: 3.3414 - val_acc: 0.9359 - val_mDice: 0.4342

Epoch 00077: val_mDice did not improve from 0.45478
Epoch 78/300
 - 12s - loss: 1.7984 - acc: 0.9299 - mDice: 0.5344 - val_loss: 3.0361 - val_acc: 0.9374 - val_mDice: 0.4482

Epoch 00078: val_mDice did not improve from 0.45478
Epoch 79/300
 - 12s - loss: 1.7880 - acc: 0.9302 - mDice: 0.5373 - val_loss: 3.6200 - val_acc: 0.9382 - val_mDice: 0.4204

Epoch 00079: val_mDice did not improve from 0.45478
Epoch 80/300
 - 12s - loss: 1.7818 - acc: 0.9303 - mDice: 0.5389 - val_loss: 3.3288 - val_acc: 0.9394 - val_mDice: 0.4482

Epoch 00080: val_mDice did not improve from 0.45478
Epoch 81/300
 - 12s - loss: 1.7691 - acc: 0.9307 - mDice: 0.5414 - val_loss: 3.1135 - val_acc: 0.9410 - val_mDice: 0.4527

Epoch 00081: val_mDice did not improve from 0.45478
Epoch 82/300
 - 12s - loss: 1.7642 - acc: 0.9307 - mDice: 0.5429 - val_loss: 3.6280 - val_acc: 0.9418 - val_mDice: 0.4307

Epoch 00082: val_mDice did not improve from 0.45478
Epoch 83/300
 - 12s - loss: 1.7556 - acc: 0.9311 - mDice: 0.5447 - val_loss: 3.5308 - val_acc: 0.9406 - val_mDice: 0.4337

Epoch 00083: val_mDice did not improve from 0.45478
Epoch 84/300
 - 12s - loss: 1.7546 - acc: 0.9312 - mDice: 0.5456 - val_loss: 3.2016 - val_acc: 0.9354 - val_mDice: 0.4470

Epoch 00084: val_mDice did not improve from 0.45478
Epoch 85/300
 - 12s - loss: 1.7469 - acc: 0.9315 - mDice: 0.5466 - val_loss: 3.3474 - val_acc: 0.9409 - val_mDice: 0.4441

Epoch 00085: val_mDice did not improve from 0.45478
Epoch 86/300
 - 12s - loss: 1.7436 - acc: 0.9315 - mDice: 0.5480 - val_loss: 3.2804 - val_acc: 0.9411 - val_mDice: 0.4502

Epoch 00086: val_mDice did not improve from 0.45478
Epoch 87/300
 - 12s - loss: 1.7280 - acc: 0.9319 - mDice: 0.5509 - val_loss: 3.2454 - val_acc: 0.9393 - val_mDice: 0.4546

Epoch 00087: val_mDice did not improve from 0.45478
Epoch 88/300
 - 12s - loss: 1.7221 - acc: 0.9321 - mDice: 0.5529 - val_loss: 3.4915 - val_acc: 0.9407 - val_mDice: 0.4412

Epoch 00088: val_mDice did not improve from 0.45478
Epoch 89/300
 - 12s - loss: 1.7200 - acc: 0.9320 - mDice: 0.5539 - val_loss: 3.3304 - val_acc: 0.9373 - val_mDice: 0.4418

Epoch 00089: val_mDice did not improve from 0.45478
Epoch 90/300
 - 12s - loss: 1.7142 - acc: 0.9324 - mDice: 0.5544 - val_loss: 3.2090 - val_acc: 0.9398 - val_mDice: 0.4635

Epoch 00090: val_mDice improved from 0.45478 to 0.46352, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 91/300
 - 12s - loss: 1.7093 - acc: 0.9324 - mDice: 0.5561 - val_loss: 3.2113 - val_acc: 0.9397 - val_mDice: 0.4581

Epoch 00091: val_mDice did not improve from 0.46352
Epoch 92/300
 - 12s - loss: 1.7053 - acc: 0.9327 - mDice: 0.5567 - val_loss: 3.4696 - val_acc: 0.9393 - val_mDice: 0.4385

Epoch 00092: val_mDice did not improve from 0.46352
Epoch 93/300
 - 12s - loss: 1.6892 - acc: 0.9331 - mDice: 0.5602 - val_loss: 3.4881 - val_acc: 0.9413 - val_mDice: 0.4447

Epoch 00093: val_mDice did not improve from 0.46352
Epoch 94/300
 - 12s - loss: 1.6849 - acc: 0.9331 - mDice: 0.5612 - val_loss: 3.4193 - val_acc: 0.9399 - val_mDice: 0.4393

Epoch 00094: val_mDice did not improve from 0.46352
Epoch 95/300
 - 12s - loss: 1.6787 - acc: 0.9334 - mDice: 0.5635 - val_loss: 3.2729 - val_acc: 0.9402 - val_mDice: 0.4553

Epoch 00095: val_mDice did not improve from 0.46352
Epoch 96/300
 - 12s - loss: 1.6775 - acc: 0.9333 - mDice: 0.5630 - val_loss: 3.2692 - val_acc: 0.9389 - val_mDice: 0.4460

Epoch 00096: val_mDice did not improve from 0.46352
Epoch 97/300
 - 12s - loss: 1.6732 - acc: 0.9336 - mDice: 0.5643 - val_loss: 3.2488 - val_acc: 0.9381 - val_mDice: 0.4522

Epoch 00097: val_mDice did not improve from 0.46352
Epoch 98/300
 - 12s - loss: 1.6583 - acc: 0.9337 - mDice: 0.5680 - val_loss: 3.3469 - val_acc: 0.9389 - val_mDice: 0.4498

Epoch 00098: val_mDice did not improve from 0.46352
Epoch 99/300
 - 12s - loss: 1.6551 - acc: 0.9339 - mDice: 0.5687 - val_loss: 3.2382 - val_acc: 0.9417 - val_mDice: 0.4602

Epoch 00099: val_mDice did not improve from 0.46352
Epoch 100/300
 - 12s - loss: 1.6545 - acc: 0.9339 - mDice: 0.5688 - val_loss: 3.5686 - val_acc: 0.9386 - val_mDice: 0.4399

Epoch 00100: val_mDice did not improve from 0.46352
Epoch 101/300
 - 12s - loss: 1.6490 - acc: 0.9341 - mDice: 0.5706 - val_loss: 3.2019 - val_acc: 0.9412 - val_mDice: 0.4626

Epoch 00101: val_mDice did not improve from 0.46352
Epoch 102/300
 - 12s - loss: 1.6428 - acc: 0.9341 - mDice: 0.5715 - val_loss: 3.2983 - val_acc: 0.9406 - val_mDice: 0.4587

Epoch 00102: val_mDice did not improve from 0.46352
Epoch 103/300
 - 12s - loss: 1.6431 - acc: 0.9342 - mDice: 0.5717 - val_loss: 3.6988 - val_acc: 0.9403 - val_mDice: 0.4372

Epoch 00103: val_mDice did not improve from 0.46352
Epoch 104/300
 - 12s - loss: 1.6368 - acc: 0.9344 - mDice: 0.5728 - val_loss: 3.4426 - val_acc: 0.9410 - val_mDice: 0.4469

Epoch 00104: val_mDice did not improve from 0.46352
Epoch 105/300
 - 12s - loss: 1.6311 - acc: 0.9344 - mDice: 0.5749 - val_loss: 3.1891 - val_acc: 0.9383 - val_mDice: 0.4606

Epoch 00105: val_mDice did not improve from 0.46352
Epoch 106/300
 - 12s - loss: 1.6243 - acc: 0.9347 - mDice: 0.5765 - val_loss: 3.4815 - val_acc: 0.9356 - val_mDice: 0.4407

Epoch 00106: val_mDice did not improve from 0.46352
Epoch 107/300
 - 12s - loss: 1.6271 - acc: 0.9343 - mDice: 0.5750 - val_loss: 3.3032 - val_acc: 0.9393 - val_mDice: 0.4626

Epoch 00107: val_mDice did not improve from 0.46352
Epoch 108/300
 - 12s - loss: 1.6192 - acc: 0.9345 - mDice: 0.5773 - val_loss: 3.5643 - val_acc: 0.9422 - val_mDice: 0.4478

Epoch 00108: val_mDice did not improve from 0.46352
Epoch 109/300
 - 12s - loss: 1.6093 - acc: 0.9347 - mDice: 0.5790 - val_loss: 3.4573 - val_acc: 0.9416 - val_mDice: 0.4554

Epoch 00109: val_mDice did not improve from 0.46352
Epoch 110/300
 - 12s - loss: 1.6112 - acc: 0.9346 - mDice: 0.5788 - val_loss: 3.4519 - val_acc: 0.9399 - val_mDice: 0.4533

Epoch 00110: val_mDice did not improve from 0.46352
Epoch 111/300
 - 12s - loss: 1.6016 - acc: 0.9349 - mDice: 0.5810 - val_loss: 3.2354 - val_acc: 0.9409 - val_mDice: 0.4683

Epoch 00111: val_mDice improved from 0.46352 to 0.46832, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 112/300
 - 12s - loss: 1.6086 - acc: 0.9346 - mDice: 0.5800 - val_loss: 3.1967 - val_acc: 0.9389 - val_mDice: 0.4655

Epoch 00112: val_mDice did not improve from 0.46832
Epoch 113/300
 - 12s - loss: 1.6050 - acc: 0.9346 - mDice: 0.5803 - val_loss: 3.3375 - val_acc: 0.9410 - val_mDice: 0.4631

Epoch 00113: val_mDice did not improve from 0.46832
Epoch 114/300
 - 12s - loss: 1.5974 - acc: 0.9348 - mDice: 0.5825 - val_loss: 3.4325 - val_acc: 0.9402 - val_mDice: 0.4467

Epoch 00114: val_mDice did not improve from 0.46832
Epoch 115/300
 - 12s - loss: 1.5821 - acc: 0.9353 - mDice: 0.5859 - val_loss: 3.2664 - val_acc: 0.9402 - val_mDice: 0.4565

Epoch 00115: val_mDice did not improve from 0.46832
Epoch 116/300
 - 12s - loss: 1.5911 - acc: 0.9350 - mDice: 0.5838 - val_loss: 3.4820 - val_acc: 0.9387 - val_mDice: 0.4559

Epoch 00116: val_mDice did not improve from 0.46832
Epoch 117/300
 - 11s - loss: 1.5847 - acc: 0.9353 - mDice: 0.5859 - val_loss: 3.4734 - val_acc: 0.9406 - val_mDice: 0.4585

Epoch 00117: val_mDice did not improve from 0.46832
Epoch 118/300
 - 11s - loss: 1.5765 - acc: 0.9354 - mDice: 0.5875 - val_loss: 3.4953 - val_acc: 0.9418 - val_mDice: 0.4612

Epoch 00118: val_mDice did not improve from 0.46832
Epoch 119/300
 - 11s - loss: 1.5811 - acc: 0.9352 - mDice: 0.5863 - val_loss: 3.3103 - val_acc: 0.9402 - val_mDice: 0.4570

Epoch 00119: val_mDice did not improve from 0.46832
Epoch 120/300
 - 11s - loss: 1.5712 - acc: 0.9354 - mDice: 0.5881 - val_loss: 3.5452 - val_acc: 0.9376 - val_mDice: 0.4499

Epoch 00120: val_mDice did not improve from 0.46832
Epoch 121/300
 - 11s - loss: 1.5614 - acc: 0.9356 - mDice: 0.5911 - val_loss: 3.3420 - val_acc: 0.9358 - val_mDice: 0.4540

Epoch 00121: val_mDice did not improve from 0.46832
Epoch 122/300
 - 12s - loss: 1.5673 - acc: 0.9356 - mDice: 0.5897 - val_loss: 3.3788 - val_acc: 0.9390 - val_mDice: 0.4531

Epoch 00122: val_mDice did not improve from 0.46832
Epoch 123/300
 - 12s - loss: 1.5678 - acc: 0.9354 - mDice: 0.5894 - val_loss: 3.5636 - val_acc: 0.9420 - val_mDice: 0.4614

Epoch 00123: val_mDice did not improve from 0.46832
Epoch 124/300
 - 12s - loss: 1.5634 - acc: 0.9353 - mDice: 0.5900 - val_loss: 3.1702 - val_acc: 0.9388 - val_mDice: 0.4682

Epoch 00124: val_mDice did not improve from 0.46832
Epoch 125/300
 - 12s - loss: 1.5649 - acc: 0.9354 - mDice: 0.5903 - val_loss: 3.3331 - val_acc: 0.9392 - val_mDice: 0.4589

Epoch 00125: val_mDice did not improve from 0.46832
Epoch 126/300
 - 11s - loss: 1.5510 - acc: 0.9358 - mDice: 0.5931 - val_loss: 3.6246 - val_acc: 0.9424 - val_mDice: 0.4523

Epoch 00126: val_mDice did not improve from 0.46832
Epoch 127/300
 - 11s - loss: 1.5473 - acc: 0.9360 - mDice: 0.5950 - val_loss: 3.4076 - val_acc: 0.9396 - val_mDice: 0.4567

Epoch 00127: val_mDice did not improve from 0.46832
Epoch 128/300
 - 12s - loss: 1.5450 - acc: 0.9361 - mDice: 0.5949 - val_loss: 3.3825 - val_acc: 0.9401 - val_mDice: 0.4669

Epoch 00128: val_mDice did not improve from 0.46832
Epoch 129/300
 - 12s - loss: 1.5511 - acc: 0.9358 - mDice: 0.5946 - val_loss: 3.8503 - val_acc: 0.9428 - val_mDice: 0.4385

Epoch 00129: val_mDice did not improve from 0.46832
Epoch 130/300
 - 12s - loss: 1.5399 - acc: 0.9361 - mDice: 0.5959 - val_loss: 3.6277 - val_acc: 0.9395 - val_mDice: 0.4504

Epoch 00130: val_mDice did not improve from 0.46832
Epoch 131/300
 - 12s - loss: 1.5377 - acc: 0.9362 - mDice: 0.5969 - val_loss: 3.6043 - val_acc: 0.9391 - val_mDice: 0.4479

Epoch 00131: val_mDice did not improve from 0.46832
Epoch 132/300
 - 12s - loss: 1.5335 - acc: 0.9362 - mDice: 0.5977 - val_loss: 3.8457 - val_acc: 0.9423 - val_mDice: 0.4384

Epoch 00132: val_mDice did not improve from 0.46832
Epoch 133/300
 - 12s - loss: 1.5288 - acc: 0.9364 - mDice: 0.5986 - val_loss: 3.4373 - val_acc: 0.9407 - val_mDice: 0.4628

Epoch 00133: val_mDice did not improve from 0.46832
Epoch 134/300
 - 12s - loss: 1.5308 - acc: 0.9363 - mDice: 0.5987 - val_loss: 3.5301 - val_acc: 0.9400 - val_mDice: 0.4450

Epoch 00134: val_mDice did not improve from 0.46832
Epoch 135/300
 - 12s - loss: 1.5326 - acc: 0.9363 - mDice: 0.5981 - val_loss: 3.4719 - val_acc: 0.9364 - val_mDice: 0.4499

Epoch 00135: val_mDice did not improve from 0.46832
Epoch 136/300
 - 12s - loss: 1.5194 - acc: 0.9366 - mDice: 0.6012 - val_loss: 3.3793 - val_acc: 0.9420 - val_mDice: 0.4644

Epoch 00136: val_mDice did not improve from 0.46832
Epoch 137/300
 - 12s - loss: 1.5215 - acc: 0.9366 - mDice: 0.6013 - val_loss: 3.4945 - val_acc: 0.9403 - val_mDice: 0.4520

Epoch 00137: val_mDice did not improve from 0.46832
Epoch 138/300
 - 12s - loss: 1.5237 - acc: 0.9365 - mDice: 0.6005 - val_loss: 3.7563 - val_acc: 0.9421 - val_mDice: 0.4489

Epoch 00138: val_mDice did not improve from 0.46832
Epoch 139/300
 - 12s - loss: 1.5206 - acc: 0.9366 - mDice: 0.6017 - val_loss: 3.4096 - val_acc: 0.9419 - val_mDice: 0.4628

Epoch 00139: val_mDice did not improve from 0.46832
Epoch 140/300
 - 12s - loss: 1.5189 - acc: 0.9368 - mDice: 0.6018 - val_loss: 3.3727 - val_acc: 0.9437 - val_mDice: 0.4605

Epoch 00140: val_mDice did not improve from 0.46832
Epoch 141/300
 - 12s - loss: 1.5109 - acc: 0.9368 - mDice: 0.6034 - val_loss: 3.6361 - val_acc: 0.9404 - val_mDice: 0.4508

Epoch 00141: val_mDice did not improve from 0.46832
Restoring model weights from the end of the best epoch
Epoch 00141: early stopping
{'val_loss': [68.39800360089257, 30.30986520222255, 14.881223391918908, 10.420453041791916, 8.134698287362145, 6.971396405782018, 6.533969963235514, 6.277049741574696, 6.194026872515678, 6.1442500234005, 5.945754539221525, 6.020559539220163, 5.862807119886081, 5.3267888476451235, 4.266572141221592, 4.5180095333960795, 4.311159050003404, 4.798534823315484, 4.14653768257371, 4.102856043282719, 4.664905991848736, 3.487568892627245, 3.3498739143390033, 3.3889297304586288, 3.1979426069273833, 3.160686257250962, 3.3738176906481385, 3.2723852521961643, 3.1875366565017473, 3.0481684611045887, 3.0232769060170366, 2.890633692920563, 2.9731848187123737, 3.0626905353385068, 3.1894306359103037, 3.2091503009820976, 3.7977607567750273, 3.092506012169733, 3.050973317807629, 3.607619080470786, 3.1072773583499447, 3.4383223185404423, 3.9116934340092397, 3.598335901829636, 3.170852218532846, 3.43334438503232, 3.262233416638559, 3.1841361922256293, 3.225337884322341, 3.7979331398382783, 3.22383061721034, 3.0376557817327834, 3.4704156391588703, 3.305484872777015, 3.272031274513297, 3.173744176913585, 3.223191614728421, 3.186444505273054, 3.212403764549111, 3.168064457453078, 3.272896087041036, 3.1770897923214805, 3.0954623699099537, 3.2815016487702016, 3.222405822082822, 3.0434887501012002, 3.2704060836916877, 3.2075523156450974, 3.2499467061300362, 3.1927976433764256, 3.193904296738938, 3.6086596831058464, 3.4310310739979504, 3.067133204035816, 3.0737309380035316, 3.325291485015658, 3.3413550995201584, 3.0360875003749417, 3.619971334557271, 3.328831293597995, 3.113505910993332, 3.6279695643821643, 3.5307935111535094, 3.2016048851822103, 3.3474340556588555, 3.28037891215423, 3.2454487057624473, 3.4914622955201637, 3.3303837502109155, 3.2089589852617966, 3.2112506693007337, 3.469587471313952, 3.488099020240562, 3.419282887840555, 3.272930390024114, 3.269241024673517, 3.248756394300255, 3.3468901455136284, 3.2381667906773233, 3.5685922926557914, 3.2018569015365625, 3.298255386939716, 3.698765069224118, 3.442641697535735, 3.189068869266304, 3.4815081634248295, 3.303154682407954, 3.564276195929519, 3.4573246591414013, 3.4519475622634803, 3.235443662608131, 3.1966826442318657, 3.337489786424807, 3.432508554642222, 3.266363829906498, 3.4819703179278543, 3.4733516544192318, 3.495312641707382, 3.3103223393360772, 3.545172898520139, 3.34198856242888, 3.3788470642286397, 3.5635710994801704, 3.170201721601188, 3.3330530493653248, 3.6245544141025414, 3.407632377397801, 3.3824749110887447, 3.850297442814779, 3.627669518946537, 3.6042600743960413, 3.8456846898909482, 3.437260222044729, 3.5301115564292385, 3.4718660537508272, 3.379338426781552, 3.494500576695871, 3.7562590933715305, 3.409552622985627, 3.3727453698049343, 3.6360614643254805], 'val_acc': [0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9048260024615696, 0.9055517599696205, 0.9053846398989359, 0.9058264522325425, 0.9056707989601862, 0.907605338664282, 0.9082989664304824, 0.9102953246661595, 0.9182325800259908, 0.9197847871553331, 0.9197802401724315, 0.9248878217878795, 0.9262614392098927, 0.9233447937738328, 0.9244390924771627, 0.928026556968689, 0.9278227800414676, 0.9303731634503319, 0.9302999348867507, 0.9301510878971645, 0.930240375655038, 0.9301396381287348, 0.9308218899227324, 0.9256570481118702, 0.9331250190734863, 0.9329670525732494, 0.9305288365909031, 0.9305242640631539, 0.9305997973396665, 0.9263003525279817, 0.9344482478641328, 0.9284981886545817, 0.9341987286295209, 0.9347115187417894, 0.9360577095122564, 0.932339764776684, 0.9327037447974795, 0.9292192941620236, 0.9351763100851149, 0.9368338017236619, 0.9301946021261669, 0.9321130854742867, 0.9300801555315653, 0.9346062228793189, 0.9346107840538025, 0.9351579660461062, 0.9373969974971953, 0.9385393943105426, 0.9392948689914885, 0.9371497091792879, 0.9384523544992719, 0.9320673034304664, 0.9375114213852656, 0.9411859001432147, 0.9356776617822193, 0.9252358277638754, 0.9383287401426406, 0.9368452486537752, 0.9381776565597171, 0.9412568722452436, 0.9354029581660316, 0.9398191599618821, 0.9392147631872267, 0.9358539127168202, 0.9374290335746038, 0.9382463239488148, 0.9393818633896964, 0.9410279364812941, 0.9418131964547294, 0.9405929559753055, 0.9353845970971244, 0.9409478221620832, 0.9410943105107262, 0.9392925926617214, 0.940663925239018, 0.9372733575957162, 0.9398259804362342, 0.9396611764317467, 0.939290273757208, 0.9412568921134585, 0.9398672126588368, 0.9401785759698778, 0.9388644524982998, 0.9381433299609593, 0.9388667657261803, 0.9416735569636027, 0.9386378469921294, 0.9412385537510827, 0.9405952152751741, 0.9402861793835958, 0.9410027464230856, 0.9383150140444437, 0.9355768958727518, 0.9393292268117269, 0.9421955119995844, 0.9415728279522487, 0.9398740586780366, 0.9409088747841972, 0.9389240202449617, 0.9410073104358855, 0.9402060310045878, 0.9402197684560504, 0.9387385305904207, 0.9405540085974193, 0.941783396970658, 0.9401579641160511, 0.9375915811175392, 0.9358195917946952, 0.9390086872237069, 0.942010053566524, 0.9388484415553865, 0.9392284523873102, 0.9423649339448839, 0.939569578284309, 0.9401030398550487, 0.9427655793371654, 0.9395352289790199, 0.939127748920804, 0.9423466268039885, 0.940732601143065, 0.9399954336030143, 0.9363827847299122, 0.9419734591529483, 0.9403319727806818, 0.9420649920191083, 0.9418612803731646, 0.9436630038988023, 0.9404372828347343], 'val_mDice': [0.011849800191287483, 0.01386442998912008, 0.014180282391386018, 0.015889887088200168, 0.017602687069614018, 0.017464121925600228, 0.016334821691825277, 0.018846096182685523, 0.020661907425771158, 0.0232924202434896, 0.028863067683275966, 0.032580790775162835, 0.03952360556771358, 0.05895983799183298, 0.10172763979062438, 0.10247947288943189, 0.11905343941457215, 0.10665969149253908, 0.14548615166651352, 0.1513413791322992, 0.1422996650272537, 0.20718873762303874, 0.22658874724237693, 0.2334463249537207, 0.2556583020126536, 0.2670186447157037, 0.2670920693448612, 0.288085566877964, 0.30105311299363774, 0.3114953672602063, 0.32820667406278, 0.3395487701609021, 0.3452592215367726, 0.3461123862791629, 0.3395445481652305, 0.3491632007062435, 0.31345008522094714, 0.36675851171215373, 0.3806881918793633, 0.34280143678188324, 0.3833876389655329, 0.3638179522185099, 0.335134601442232, 0.37212151626036283, 0.3934222108551434, 0.3815470229656923, 0.39110975579491686, 0.4021067874772208, 0.3966101418648447, 0.3720524383797532, 0.4033935792034581, 0.41900334995062577, 0.39842710821401506, 0.3957385745431696, 0.4071898508284773, 0.4137483797967434, 0.41604224787581534, 0.4211398535186336, 0.418066412565254, 0.4181751807530721, 0.41767957238923936, 0.4331704457955701, 0.43312151659102666, 0.4326056703215554, 0.42374039175254957, 0.4425414300390652, 0.43369631185418084, 0.43019886687397957, 0.40999153079021544, 0.4394193500989959, 0.4408571498379821, 0.40732847890328794, 0.43477885407351313, 0.4440507001820065, 0.45477601708400817, 0.43533589442571, 0.4342385764632906, 0.4482179025099391, 0.42036324837023303, 0.448244868822041, 0.45267907601027263, 0.4307470025405997, 0.4336692513454528, 0.4469935723713466, 0.4440856199888956, 0.4501987486368134, 0.45463889961441356, 0.4411857841270311, 0.4418047061633496, 0.4635231897589706, 0.45806005064930233, 0.43854524940252304, 0.44469972806317465, 0.43931458464690615, 0.4552772041587603, 0.4459841301043828, 0.45217547104472205, 0.44981391905319124, 0.4601728795539765, 0.4399264029094151, 0.4625712971956957, 0.45873446691603886, 0.43715046541321845, 0.4469025883646238, 0.4606409055136499, 0.44066202853407177, 0.4625866785645485, 0.44783213557231993, 0.45543511318308966, 0.4533263762437162, 0.46831888155568213, 0.4655172995158604, 0.4630727750204858, 0.4467367313447453, 0.4564926808788663, 0.45593451282807756, 0.45850077732687905, 0.46116002932900474, 0.45697683468461037, 0.4498727552237965, 0.45401421650534585, 0.4530893789515609, 0.4613915905356407, 0.468205967829341, 0.4588674083352089, 0.4522608167358807, 0.4566952380396071, 0.46693299355961027, 0.4384822795788447, 0.45039277541495504, 0.447888574252526, 0.4383606133716447, 0.4628459981509617, 0.44495022687174024, 0.44990967356023337, 0.46443187906628564, 0.45203296769232976, 0.4488949857297398, 0.4627670668775127, 0.4605297084365572, 0.45083137903185116], 'loss': [187.78289752560863, 65.02540783783776, 30.120915158258736, 18.40397974105844, 13.580174503469825, 11.159078086252178, 9.724498435943968, 8.718425730875698, 7.941548602852675, 7.356814087436956, 6.898940864729932, 6.514379886434513, 6.169022962769861, 5.8497870432197505, 5.5589780242130065, 5.280775836007269, 5.010412375362066, 4.7625809191094675, 4.53208689007521, 4.325337907264567, 4.136205226763241, 3.9628808994341016, 3.8008230727647847, 3.6637622174762545, 3.5414773479279282, 3.4194292798307973, 3.305685698514916, 3.21470427834875, 3.1202386752107087, 3.0244936080757583, 2.9487227752807628, 2.875941752422194, 2.8186105385620928, 2.753289856201931, 2.6964326640280407, 2.649028545219314, 2.588755816606191, 2.5463148103138145, 2.501652294853553, 2.461469106506883, 2.4197629524467765, 2.388855780644157, 2.3509153769106814, 2.3167507410141366, 2.289205358004051, 2.258054299714916, 2.231380992963499, 2.209038346103788, 2.193726523524193, 2.159055455217753, 2.1445344126429062, 2.1231528468414242, 2.1131651635020137, 2.0876417809974166, 2.070732580367508, 2.053030736326413, 2.0406045410843148, 2.026187724889715, 2.0041381656399624, 1.9888140236639622, 1.9811327095456452, 1.9652999781082194, 1.9450088890618802, 1.9388396432915933, 1.9315109405072641, 1.9121999055727474, 1.9009796217592414, 1.8906776276493384, 1.8834623018854588, 1.8718322376674463, 1.8596200141369872, 1.8514824162472736, 1.846356091252039, 1.8374835094413662, 1.8170676857882297, 1.8175955205288195, 1.8030666550483136, 1.7983811259936202, 1.787982135557223, 1.7818456518183514, 1.76907417232737, 1.764229723155579, 1.7555562024496183, 1.7546049941590418, 1.7469140783954913, 1.7435677580367355, 1.728018630693155, 1.7221358943265919, 1.7200096469176442, 1.7142376231853043, 1.709305338715229, 1.7053100320675572, 1.6892405912920714, 1.6848707166461787, 1.6786950418249085, 1.677470466179128, 1.6732265984871861, 1.6582543021202638, 1.655146165760078, 1.6545432948505394, 1.6490475442659103, 1.6427526847231886, 1.6430530300576125, 1.6368050253503879, 1.631103783690771, 1.62434024856296, 1.6270912074849673, 1.6191988582897903, 1.6092628288618374, 1.6112445451538986, 1.601585033796232, 1.6086304798414879, 1.6050130702728984, 1.5974266451403014, 1.5821299515686122, 1.591130112714567, 1.5847180224301523, 1.5764750784845831, 1.5811231540121369, 1.5712206042844803, 1.5613569764473179, 1.567340352826204, 1.5678056159505775, 1.5634019255569176, 1.5649464492143315, 1.5510003508596675, 1.5473402829121505, 1.5450285965936226, 1.5511113988978569, 1.5398668849401582, 1.5377461482546844, 1.5334559396119023, 1.5288076432242612, 1.5308056464883801, 1.5326157748549802, 1.5194303031966985, 1.5214546009688472, 1.5236934315199933, 1.5206495841298233, 1.5188758075869289, 1.510941870069605], 'acc': [0.6344223873094026, 0.8648851030544438, 0.8686157654585213, 0.8691872534596532, 0.8692389937386661, 0.8692391827567502, 0.869238927124414, 0.8692373976970377, 0.8692366118740893, 0.8692385287948581, 0.8692407331784751, 0.8692544506468111, 0.8693804345380048, 0.8696234154292147, 0.869940453752838, 0.8706585957132048, 0.8713553947866457, 0.8719054482358716, 0.8730827156969272, 0.8744973068593991, 0.8767411790189197, 0.8797256161947344, 0.8826501577236484, 0.885669588640473, 0.8879726121300145, 0.8901727550954104, 0.892267743482143, 0.8941297876437275, 0.8959531050806724, 0.897834429236214, 0.8995344815213763, 0.9014627913392668, 0.9029459972728165, 0.9048104715043923, 0.9064496248380192, 0.9080566825470484, 0.9096280825328845, 0.9108375819841286, 0.9120100306389027, 0.9130978563030906, 0.9141027053266172, 0.9149217122617382, 0.9157571053491155, 0.9163712252880167, 0.9174474642436261, 0.9180173983412118, 0.9187692321171897, 0.9191818297173628, 0.9198741116389526, 0.9205018848450813, 0.9210070528482136, 0.9213874424977042, 0.9218826929751173, 0.9223783840580623, 0.9228356105527, 0.9234178954865033, 0.9237363931414294, 0.9240844587910169, 0.9246335365373733, 0.9250609646625824, 0.9252083564117186, 0.9257612425122207, 0.926106593892458, 0.9263516586020385, 0.9264192310330126, 0.9270200316033107, 0.9273144245860157, 0.9273934408559812, 0.9278382456369292, 0.9279981560621599, 0.9282301994608344, 0.9284421556115909, 0.9286603156555496, 0.9288635547505377, 0.9292925816752535, 0.9293102159742815, 0.9297488121391607, 0.9299151616011921, 0.9301993653382183, 0.9303126094404249, 0.930683465771117, 0.9307257591563428, 0.9310931964303938, 0.931232689788444, 0.9314878348649662, 0.9314612143872123, 0.9318989023597847, 0.9320951030390879, 0.9320125612698701, 0.9324480744255791, 0.9324291887852367, 0.9326579911368233, 0.933052628284386, 0.9330926237364644, 0.9333586360026077, 0.9332571915056932, 0.9335822934517861, 0.9337437063806062, 0.9339464121593868, 0.9339200933705365, 0.934090311342934, 0.9340557420393946, 0.9341944201341456, 0.9343951825969637, 0.9344006982041803, 0.9346511161508922, 0.93427689131317, 0.9345498101109596, 0.9347260769148519, 0.934628986659619, 0.9349294301515281, 0.9346160312620045, 0.9345913555480989, 0.934761599628411, 0.935273553299173, 0.9350329177881083, 0.9352858606988074, 0.9354469495309624, 0.9352067750911849, 0.9353720616209547, 0.9355657327053334, 0.9355726370076948, 0.9354125403567318, 0.9352603050263741, 0.9354091376175374, 0.9357645921746683, 0.9360421426222231, 0.9360503245806542, 0.935847755638602, 0.936099608153168, 0.9362036060202161, 0.9362449000537659, 0.9364273731065481, 0.9363233581061811, 0.936299977526399, 0.9366122654276315, 0.9366191731428672, 0.9365301458895815, 0.9365805213290969, 0.9367749546565646, 0.9368264652272973], 'mDice': [0.01772912612379873, 0.01766181378480796, 0.01777819059724543, 0.018651147604818882, 0.020060027067745057, 0.02188223178518377, 0.02428519779700281, 0.02752065338612361, 0.03187997290673905, 0.03652392966406686, 0.041962727568203344, 0.048781106590000865, 0.05652610545520818, 0.06607227937372429, 0.07701432061707879, 0.08922038245705573, 0.10330233120932063, 0.1173792038495678, 0.13313008114301977, 0.14864375427471596, 0.16438412735267757, 0.18151964722098884, 0.19808767122042897, 0.2130477893713331, 0.22741844328538702, 0.24170466713393288, 0.2561236115544891, 0.2692404481425597, 0.28329636875043746, 0.29759442316720497, 0.30933802934977184, 0.3213846830014482, 0.33085304807784677, 0.341350846944895, 0.35127163924462124, 0.3605014070716234, 0.3703372584778334, 0.37876301258084216, 0.38658011354921135, 0.3950934270707449, 0.40243037911722696, 0.40884213130299885, 0.41657366423632614, 0.4233730637788175, 0.42929802084902197, 0.43561882987770706, 0.44099828546300474, 0.4452180414386446, 0.4490000357029409, 0.45492332137249236, 0.45856989176948565, 0.462794098426364, 0.46446836939868336, 0.4698993642883584, 0.4740969042730212, 0.47854413410154223, 0.48079103569270143, 0.4841399301688796, 0.4883057231822178, 0.4919987452416285, 0.49324835028197006, 0.49726323401603895, 0.5014843054536636, 0.5033279912237811, 0.5050600230613379, 0.507991844963793, 0.5115563731582403, 0.5133204621594836, 0.5156066080313646, 0.5183748291916348, 0.5211929235702906, 0.5228237167061099, 0.5239186117431666, 0.5262470311090761, 0.5306074513574545, 0.5304445906919115, 0.5338409511436726, 0.5344193027454786, 0.5373127098393298, 0.5388828709747421, 0.5414483486077725, 0.5428987908436885, 0.5446907551458214, 0.545570088439487, 0.5466190615393464, 0.5479762669654855, 0.5509282860058177, 0.5529033097316242, 0.5538842036978275, 0.5544429300422955, 0.5560592632314845, 0.5567094353735964, 0.5601771428149768, 0.5612399176478455, 0.5635077816019266, 0.5630273278195757, 0.5642640294655453, 0.5679970249887006, 0.5687194567598174, 0.5688487099433694, 0.5706162330891462, 0.5715306363791567, 0.5716776858870869, 0.5727658217781101, 0.5748686059858382, 0.576490464595694, 0.5750374432954114, 0.5772583827775684, 0.5789760159048842, 0.5788239222720153, 0.5810133840436993, 0.5799651436661396, 0.5802587728889227, 0.5824637814296195, 0.5859060590842199, 0.5837575797403114, 0.5859464179442203, 0.5875036095249094, 0.586324649795255, 0.5880985318384856, 0.5910928657984765, 0.5897046937082047, 0.5894268813196671, 0.589981563086222, 0.5903203210114559, 0.5930938998444453, 0.5949852336273502, 0.5949363681032223, 0.5946297673377454, 0.595854562234851, 0.5968837639787695, 0.5977051670045229, 0.5986251022651611, 0.5986770488679444, 0.5980708786626301, 0.6012201693698301, 0.6012844363687492, 0.6005008396027702, 0.6017263729447732, 0.6018455074038928, 0.6034485816220132]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.46s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.18s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.97s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:47,  1.43s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:14,  1.54s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:18,  1.55s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<08:05,  1.73s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<07:50,  1.68s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<08:25,  1.81s/it]predicting train subjects:   2%|▏         | 7/285 [00:12<08:49,  1.90s/it]predicting train subjects:   3%|▎         | 8/285 [00:14<09:02,  1.96s/it]predicting train subjects:   3%|▎         | 9/285 [00:16<08:36,  1.87s/it]predicting train subjects:   4%|▎         | 10/285 [00:18<08:52,  1.93s/it]predicting train subjects:   4%|▍         | 11/285 [00:20<09:03,  1.98s/it]predicting train subjects:   4%|▍         | 12/285 [00:22<09:07,  2.01s/it]predicting train subjects:   5%|▍         | 13/285 [00:24<09:06,  2.01s/it]predicting train subjects:   5%|▍         | 14/285 [00:26<09:07,  2.02s/it]predicting train subjects:   5%|▌         | 15/285 [00:29<09:24,  2.09s/it]predicting train subjects:   6%|▌         | 16/285 [00:31<09:20,  2.08s/it]predicting train subjects:   6%|▌         | 17/285 [00:33<09:16,  2.08s/it]predicting train subjects:   6%|▋         | 18/285 [00:35<09:12,  2.07s/it]predicting train subjects:   7%|▋         | 19/285 [00:37<09:08,  2.06s/it]predicting train subjects:   7%|▋         | 20/285 [00:39<09:07,  2.07s/it]predicting train subjects:   7%|▋         | 21/285 [00:41<09:01,  2.05s/it]predicting train subjects:   8%|▊         | 22/285 [00:43<09:04,  2.07s/it]predicting train subjects:   8%|▊         | 23/285 [00:45<09:02,  2.07s/it]predicting train subjects:   8%|▊         | 24/285 [00:47<09:01,  2.07s/it]predicting train subjects:   9%|▉         | 25/285 [00:49<09:03,  2.09s/it]predicting train subjects:   9%|▉         | 26/285 [00:51<08:59,  2.08s/it]predicting train subjects:   9%|▉         | 27/285 [00:53<08:56,  2.08s/it]predicting train subjects:  10%|▉         | 28/285 [00:55<08:54,  2.08s/it]predicting train subjects:  10%|█         | 29/285 [00:58<08:57,  2.10s/it]predicting train subjects:  11%|█         | 30/285 [01:00<08:54,  2.09s/it]predicting train subjects:  11%|█         | 31/285 [01:02<08:46,  2.07s/it]predicting train subjects:  11%|█         | 32/285 [01:04<08:37,  2.04s/it]predicting train subjects:  12%|█▏        | 33/285 [01:06<08:30,  2.03s/it]predicting train subjects:  12%|█▏        | 34/285 [01:08<08:27,  2.02s/it]predicting train subjects:  12%|█▏        | 35/285 [01:10<08:21,  2.00s/it]predicting train subjects:  13%|█▎        | 36/285 [01:12<08:11,  1.98s/it]predicting train subjects:  13%|█▎        | 37/285 [01:13<08:05,  1.96s/it]predicting train subjects:  13%|█▎        | 38/285 [01:15<08:00,  1.95s/it]predicting train subjects:  14%|█▎        | 39/285 [01:18<08:09,  1.99s/it]predicting train subjects:  14%|█▍        | 40/285 [01:19<08:02,  1.97s/it]predicting train subjects:  14%|█▍        | 41/285 [01:21<07:58,  1.96s/it]predicting train subjects:  15%|█▍        | 42/285 [01:23<07:55,  1.96s/it]predicting train subjects:  15%|█▌        | 43/285 [01:25<07:51,  1.95s/it]predicting train subjects:  15%|█▌        | 44/285 [01:27<07:47,  1.94s/it]predicting train subjects:  16%|█▌        | 45/285 [01:29<07:43,  1.93s/it]predicting train subjects:  16%|█▌        | 46/285 [01:31<07:22,  1.85s/it]predicting train subjects:  16%|█▋        | 47/285 [01:32<07:03,  1.78s/it]predicting train subjects:  17%|█▋        | 48/285 [01:34<06:52,  1.74s/it]predicting train subjects:  17%|█▋        | 49/285 [01:36<06:48,  1.73s/it]predicting train subjects:  18%|█▊        | 50/285 [01:37<06:43,  1.72s/it]predicting train subjects:  18%|█▊        | 51/285 [01:39<06:44,  1.73s/it]predicting train subjects:  18%|█▊        | 52/285 [01:41<06:37,  1.71s/it]predicting train subjects:  19%|█▊        | 53/285 [01:42<06:32,  1.69s/it]predicting train subjects:  19%|█▉        | 54/285 [01:44<06:27,  1.68s/it]predicting train subjects:  19%|█▉        | 55/285 [01:46<06:21,  1.66s/it]predicting train subjects:  20%|█▉        | 56/285 [01:47<06:19,  1.66s/it]predicting train subjects:  20%|██        | 57/285 [01:49<06:17,  1.66s/it]predicting train subjects:  20%|██        | 58/285 [01:51<06:15,  1.66s/it]predicting train subjects:  21%|██        | 59/285 [01:52<06:17,  1.67s/it]predicting train subjects:  21%|██        | 60/285 [01:54<06:15,  1.67s/it]predicting train subjects:  21%|██▏       | 61/285 [01:56<06:21,  1.70s/it]predicting train subjects:  22%|██▏       | 62/285 [01:58<06:20,  1.71s/it]predicting train subjects:  22%|██▏       | 63/285 [01:59<06:13,  1.68s/it]predicting train subjects:  22%|██▏       | 64/285 [02:01<06:16,  1.70s/it]predicting train subjects:  23%|██▎       | 65/285 [02:03<06:30,  1.78s/it]predicting train subjects:  23%|██▎       | 66/285 [02:05<06:33,  1.80s/it]predicting train subjects:  24%|██▎       | 67/285 [02:06<06:25,  1.77s/it]predicting train subjects:  24%|██▍       | 68/285 [02:08<06:20,  1.75s/it]predicting train subjects:  24%|██▍       | 69/285 [02:10<06:20,  1.76s/it]predicting train subjects:  25%|██▍       | 70/285 [02:12<06:17,  1.76s/it]predicting train subjects:  25%|██▍       | 71/285 [02:13<06:17,  1.77s/it]predicting train subjects:  25%|██▌       | 72/285 [02:15<06:12,  1.75s/it]predicting train subjects:  26%|██▌       | 73/285 [02:17<06:09,  1.74s/it]predicting train subjects:  26%|██▌       | 74/285 [02:19<06:13,  1.77s/it]predicting train subjects:  26%|██▋       | 75/285 [02:20<06:10,  1.77s/it]predicting train subjects:  27%|██▋       | 76/285 [02:22<06:06,  1.75s/it]predicting train subjects:  27%|██▋       | 77/285 [02:24<06:00,  1.73s/it]predicting train subjects:  27%|██▋       | 78/285 [02:26<06:02,  1.75s/it]predicting train subjects:  28%|██▊       | 79/285 [02:27<05:55,  1.72s/it]predicting train subjects:  28%|██▊       | 80/285 [02:29<05:52,  1.72s/it]predicting train subjects:  28%|██▊       | 81/285 [02:31<05:52,  1.73s/it]predicting train subjects:  29%|██▉       | 82/285 [02:33<05:49,  1.72s/it]predicting train subjects:  29%|██▉       | 83/285 [02:34<05:47,  1.72s/it]predicting train subjects:  29%|██▉       | 84/285 [02:36<05:44,  1.71s/it]predicting train subjects:  30%|██▉       | 85/285 [02:38<05:54,  1.77s/it]predicting train subjects:  30%|███       | 86/285 [02:40<05:59,  1.81s/it]predicting train subjects:  31%|███       | 87/285 [02:42<06:03,  1.84s/it]predicting train subjects:  31%|███       | 88/285 [02:44<06:11,  1.89s/it]predicting train subjects:  31%|███       | 89/285 [02:46<06:14,  1.91s/it]predicting train subjects:  32%|███▏      | 90/285 [02:48<06:14,  1.92s/it]predicting train subjects:  32%|███▏      | 91/285 [02:49<06:11,  1.91s/it]predicting train subjects:  32%|███▏      | 92/285 [02:51<06:15,  1.95s/it]predicting train subjects:  33%|███▎      | 93/285 [02:54<06:27,  2.02s/it]predicting train subjects:  33%|███▎      | 94/285 [02:56<06:28,  2.04s/it]predicting train subjects:  33%|███▎      | 95/285 [02:58<06:20,  2.00s/it]predicting train subjects:  34%|███▎      | 96/285 [03:00<06:17,  2.00s/it]predicting train subjects:  34%|███▍      | 97/285 [03:02<06:14,  1.99s/it]predicting train subjects:  34%|███▍      | 98/285 [03:04<06:12,  1.99s/it]predicting train subjects:  35%|███▍      | 99/285 [03:06<06:07,  1.98s/it]predicting train subjects:  35%|███▌      | 100/285 [03:07<06:02,  1.96s/it]predicting train subjects:  35%|███▌      | 101/285 [03:09<05:59,  1.95s/it]predicting train subjects:  36%|███▌      | 102/285 [03:11<05:55,  1.94s/it]predicting train subjects:  36%|███▌      | 103/285 [03:13<05:47,  1.91s/it]predicting train subjects:  36%|███▋      | 104/285 [03:15<05:42,  1.89s/it]predicting train subjects:  37%|███▋      | 105/285 [03:17<05:38,  1.88s/it]predicting train subjects:  37%|███▋      | 106/285 [03:19<05:33,  1.86s/it]predicting train subjects:  38%|███▊      | 107/285 [03:21<05:34,  1.88s/it]predicting train subjects:  38%|███▊      | 108/285 [03:23<05:37,  1.91s/it]predicting train subjects:  38%|███▊      | 109/285 [03:24<05:32,  1.89s/it]predicting train subjects:  39%|███▊      | 110/285 [03:26<05:29,  1.89s/it]predicting train subjects:  39%|███▉      | 111/285 [03:28<05:28,  1.89s/it]predicting train subjects:  39%|███▉      | 112/285 [03:30<05:29,  1.91s/it]predicting train subjects:  40%|███▉      | 113/285 [03:32<05:26,  1.90s/it]predicting train subjects:  40%|████      | 114/285 [03:34<05:26,  1.91s/it]predicting train subjects:  40%|████      | 115/285 [03:36<05:24,  1.91s/it]predicting train subjects:  41%|████      | 116/285 [03:38<05:21,  1.90s/it]predicting train subjects:  41%|████      | 117/285 [03:40<05:19,  1.90s/it]predicting train subjects:  41%|████▏     | 118/285 [03:42<05:20,  1.92s/it]predicting train subjects:  42%|████▏     | 119/285 [03:43<05:15,  1.90s/it]predicting train subjects:  42%|████▏     | 120/285 [03:45<05:15,  1.91s/it]predicting train subjects:  42%|████▏     | 121/285 [03:47<05:03,  1.85s/it]predicting train subjects:  43%|████▎     | 122/285 [03:49<04:50,  1.78s/it]predicting train subjects:  43%|████▎     | 123/285 [03:50<04:35,  1.70s/it]predicting train subjects:  44%|████▎     | 124/285 [03:52<04:41,  1.75s/it]predicting train subjects:  44%|████▍     | 125/285 [03:54<04:40,  1.75s/it]predicting train subjects:  44%|████▍     | 126/285 [03:56<04:35,  1.73s/it]predicting train subjects:  45%|████▍     | 127/285 [03:57<04:26,  1.69s/it]predicting train subjects:  45%|████▍     | 128/285 [03:59<04:24,  1.69s/it]predicting train subjects:  45%|████▌     | 129/285 [04:00<04:22,  1.68s/it]predicting train subjects:  46%|████▌     | 130/285 [04:02<04:20,  1.68s/it]predicting train subjects:  46%|████▌     | 131/285 [04:04<04:18,  1.68s/it]predicting train subjects:  46%|████▋     | 132/285 [04:06<04:17,  1.68s/it]predicting train subjects:  47%|████▋     | 133/285 [04:07<04:14,  1.68s/it]predicting train subjects:  47%|████▋     | 134/285 [04:09<04:11,  1.67s/it]predicting train subjects:  47%|████▋     | 135/285 [04:11<04:10,  1.67s/it]predicting train subjects:  48%|████▊     | 136/285 [04:12<04:08,  1.67s/it]predicting train subjects:  48%|████▊     | 137/285 [04:14<04:07,  1.67s/it]predicting train subjects:  48%|████▊     | 138/285 [04:16<04:09,  1.70s/it]predicting train subjects:  49%|████▉     | 139/285 [04:17<04:09,  1.71s/it]predicting train subjects:  49%|████▉     | 140/285 [04:19<04:10,  1.73s/it]predicting train subjects:  49%|████▉     | 141/285 [04:21<04:05,  1.71s/it]predicting train subjects:  50%|████▉     | 142/285 [04:22<03:56,  1.66s/it]predicting train subjects:  50%|█████     | 143/285 [04:24<03:51,  1.63s/it]predicting train subjects:  51%|█████     | 144/285 [04:25<03:47,  1.61s/it]predicting train subjects:  51%|█████     | 145/285 [04:27<03:41,  1.58s/it]predicting train subjects:  51%|█████     | 146/285 [04:28<03:34,  1.54s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:30<03:31,  1.53s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:32<03:31,  1.55s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:33<03:33,  1.57s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:35<03:29,  1.55s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:36<03:24,  1.53s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:38<03:20,  1.51s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:39<03:21,  1.53s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:41<03:23,  1.56s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:42<03:21,  1.55s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:44<03:18,  1.54s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:45<03:19,  1.55s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:47<03:17,  1.55s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:49<03:17,  1.57s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:50<03:15,  1.57s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:52<03:12,  1.55s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:53<03:08,  1.53s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:55<03:06,  1.53s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:56<03:02,  1.51s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:58<02:59,  1.50s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:59<03:00,  1.51s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:01<02:56,  1.50s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:02<02:53,  1.48s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:04<02:51,  1.47s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:05<02:49,  1.47s/it]predicting train subjects:  60%|██████    | 171/285 [05:06<02:47,  1.47s/it]predicting train subjects:  60%|██████    | 172/285 [05:08<02:45,  1.46s/it]predicting train subjects:  61%|██████    | 173/285 [05:09<02:42,  1.45s/it]predicting train subjects:  61%|██████    | 174/285 [05:11<02:41,  1.45s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:12<02:40,  1.46s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:14<02:40,  1.47s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:15<02:39,  1.48s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:17<02:37,  1.47s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:18<02:34,  1.46s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:20<02:33,  1.46s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:21<02:30,  1.45s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:22<02:29,  1.45s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:24<02:27,  1.44s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:25<02:25,  1.44s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:27<02:26,  1.46s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:28<02:24,  1.46s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:30<02:22,  1.45s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:31<02:22,  1.47s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:33<02:19,  1.46s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:34<02:16,  1.44s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:35<02:13,  1.42s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:37<02:13,  1.43s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:38<02:15,  1.47s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:40<02:13,  1.46s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:41<02:11,  1.46s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:43<02:15,  1.53s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:45<02:18,  1.58s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:46<02:22,  1.63s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:48<02:21,  1.65s/it]predicting train subjects:  70%|███████   | 200/285 [05:50<02:21,  1.66s/it]predicting train subjects:  71%|███████   | 201/285 [05:52<02:21,  1.68s/it]predicting train subjects:  71%|███████   | 202/285 [05:53<02:21,  1.71s/it]predicting train subjects:  71%|███████   | 203/285 [05:55<02:18,  1.69s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:57<02:15,  1.68s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:58<02:13,  1.67s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:00<02:11,  1.66s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:02<02:11,  1.68s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:03<02:10,  1.69s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:05<02:08,  1.69s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:07<02:06,  1.69s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:08<02:05,  1.69s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:10<02:04,  1.70s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:12<02:00,  1.68s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:13<01:54,  1.62s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:15<01:50,  1.58s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:16<01:47,  1.56s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:18<01:45,  1.54s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:19<01:41,  1.52s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:21<01:39,  1.51s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:22<01:36,  1.49s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:24<01:34,  1.48s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:25<01:32,  1.47s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:27<01:30,  1.47s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:28<01:29,  1.47s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:29<01:27,  1.45s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:31<01:26,  1.46s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:32<01:24,  1.46s/it]predicting train subjects:  80%|████████  | 228/285 [06:34<01:22,  1.45s/it]predicting train subjects:  80%|████████  | 229/285 [06:35<01:21,  1.46s/it]predicting train subjects:  81%|████████  | 230/285 [06:37<01:20,  1.47s/it]predicting train subjects:  81%|████████  | 231/285 [06:38<01:18,  1.46s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:40<01:23,  1.57s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:42<01:26,  1.66s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:44<01:27,  1.72s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:46<01:27,  1.75s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:47<01:27,  1.79s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:49<01:26,  1.80s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:51<01:24,  1.81s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:53<01:24,  1.84s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:55<01:22,  1.84s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:57<01:20,  1.83s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:59<01:18,  1.83s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:00<01:16,  1.82s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:02<01:14,  1.81s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:04<01:12,  1.80s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:06<01:10,  1.81s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:08<01:08,  1.81s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:09<01:06,  1.80s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:11<01:04,  1.80s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:12<00:58,  1.68s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:14<00:53,  1.58s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:15<00:49,  1.51s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:17<00:46,  1.46s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:18<00:44,  1.43s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:19<00:42,  1.42s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:21<00:40,  1.41s/it]predicting train subjects:  90%|█████████ | 257/285 [07:22<00:39,  1.40s/it]predicting train subjects:  91%|█████████ | 258/285 [07:23<00:37,  1.40s/it]predicting train subjects:  91%|█████████ | 259/285 [07:25<00:36,  1.40s/it]predicting train subjects:  91%|█████████ | 260/285 [07:26<00:34,  1.38s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:28<00:32,  1.37s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:29<00:31,  1.37s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:30<00:29,  1.35s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:32<00:27,  1.33s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:33<00:26,  1.33s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:34<00:25,  1.32s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:35<00:23,  1.33s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:37<00:25,  1.49s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:39<00:25,  1.59s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:41<00:25,  1.67s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:43<00:24,  1.75s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:45<00:23,  1.83s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:47<00:21,  1.82s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:49<00:20,  1.82s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:50<00:18,  1.81s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:52<00:16,  1.83s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:54<00:14,  1.84s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:56<00:12,  1.84s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:58<00:10,  1.83s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:00<00:09,  1.84s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:01<00:07,  1.84s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:03<00:05,  1.86s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:05<00:03,  1.84s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:07<00:01,  1.84s/it]predicting train subjects: 100%|██████████| 285/285 [08:09<00:00,  1.82s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:32,  1.38s/it]Loading train:   1%|          | 2/285 [00:02<06:33,  1.39s/it]Loading train:   1%|          | 3/285 [00:04<06:33,  1.39s/it]Loading train:   1%|▏         | 4/285 [00:05<06:49,  1.46s/it]Loading train:   2%|▏         | 5/285 [00:07<06:33,  1.40s/it]Loading train:   2%|▏         | 6/285 [00:08<06:53,  1.48s/it]Loading train:   2%|▏         | 7/285 [00:10<07:38,  1.65s/it]Loading train:   3%|▎         | 8/285 [00:12<07:47,  1.69s/it]Loading train:   3%|▎         | 9/285 [00:13<07:19,  1.59s/it]Loading train:   4%|▎         | 10/285 [00:15<07:11,  1.57s/it]Loading train:   4%|▍         | 11/285 [00:16<06:31,  1.43s/it]Loading train:   4%|▍         | 12/285 [00:17<06:07,  1.35s/it]Loading train:   5%|▍         | 13/285 [00:18<06:01,  1.33s/it]Loading train:   5%|▍         | 14/285 [00:20<05:52,  1.30s/it]Loading train:   5%|▌         | 15/285 [00:21<05:54,  1.31s/it]Loading train:   6%|▌         | 16/285 [00:22<05:39,  1.26s/it]Loading train:   6%|▌         | 17/285 [00:23<05:34,  1.25s/it]Loading train:   6%|▋         | 18/285 [00:25<05:30,  1.24s/it]Loading train:   7%|▋         | 19/285 [00:26<05:42,  1.29s/it]Loading train:   7%|▋         | 20/285 [00:27<05:32,  1.25s/it]Loading train:   7%|▋         | 21/285 [00:28<05:27,  1.24s/it]Loading train:   8%|▊         | 22/285 [00:30<05:36,  1.28s/it]Loading train:   8%|▊         | 23/285 [00:31<05:34,  1.28s/it]Loading train:   8%|▊         | 24/285 [00:33<05:46,  1.33s/it]Loading train:   9%|▉         | 25/285 [00:34<05:37,  1.30s/it]Loading train:   9%|▉         | 26/285 [00:35<05:26,  1.26s/it]Loading train:   9%|▉         | 27/285 [00:36<05:28,  1.27s/it]Loading train:  10%|▉         | 28/285 [00:37<05:13,  1.22s/it]Loading train:  10%|█         | 29/285 [00:38<05:05,  1.19s/it]Loading train:  11%|█         | 30/285 [00:39<04:53,  1.15s/it]Loading train:  11%|█         | 31/285 [00:41<04:42,  1.11s/it]Loading train:  11%|█         | 32/285 [00:42<04:34,  1.08s/it]Loading train:  12%|█▏        | 33/285 [00:43<04:34,  1.09s/it]Loading train:  12%|█▏        | 34/285 [00:44<04:26,  1.06s/it]Loading train:  12%|█▏        | 35/285 [00:45<04:24,  1.06s/it]Loading train:  13%|█▎        | 36/285 [00:46<04:31,  1.09s/it]Loading train:  13%|█▎        | 37/285 [00:47<04:24,  1.07s/it]Loading train:  13%|█▎        | 38/285 [00:48<04:23,  1.07s/it]Loading train:  14%|█▎        | 39/285 [00:49<04:28,  1.09s/it]Loading train:  14%|█▍        | 40/285 [00:50<04:27,  1.09s/it]Loading train:  14%|█▍        | 41/285 [00:51<04:27,  1.10s/it]Loading train:  15%|█▍        | 42/285 [00:52<04:22,  1.08s/it]Loading train:  15%|█▌        | 43/285 [00:53<04:20,  1.07s/it]Loading train:  15%|█▌        | 44/285 [00:54<04:17,  1.07s/it]Loading train:  16%|█▌        | 45/285 [00:55<04:15,  1.06s/it]Loading train:  16%|█▌        | 46/285 [00:57<04:16,  1.07s/it]Loading train:  16%|█▋        | 47/285 [00:58<04:10,  1.05s/it]Loading train:  17%|█▋        | 48/285 [00:58<03:58,  1.01s/it]Loading train:  17%|█▋        | 49/285 [00:59<03:48,  1.03it/s]Loading train:  18%|█▊        | 50/285 [01:01<03:59,  1.02s/it]Loading train:  18%|█▊        | 51/285 [01:01<03:54,  1.00s/it]Loading train:  18%|█▊        | 52/285 [01:03<04:00,  1.03s/it]Loading train:  19%|█▊        | 53/285 [01:03<03:50,  1.01it/s]Loading train:  19%|█▉        | 54/285 [01:04<03:49,  1.01it/s]Loading train:  19%|█▉        | 55/285 [01:05<03:44,  1.02it/s]Loading train:  20%|█▉        | 56/285 [01:06<03:49,  1.00s/it]Loading train:  20%|██        | 57/285 [01:07<03:40,  1.03it/s]Loading train:  20%|██        | 58/285 [01:09<03:56,  1.04s/it]Loading train:  21%|██        | 59/285 [01:10<03:52,  1.03s/it]Loading train:  21%|██        | 60/285 [01:10<03:39,  1.02it/s]Loading train:  21%|██▏       | 61/285 [01:11<03:41,  1.01it/s]Loading train:  22%|██▏       | 62/285 [01:12<03:39,  1.02it/s]Loading train:  22%|██▏       | 63/285 [01:14<03:47,  1.02s/it]Loading train:  22%|██▏       | 64/285 [01:15<04:10,  1.13s/it]Loading train:  23%|██▎       | 65/285 [01:17<04:44,  1.29s/it]Loading train:  23%|██▎       | 66/285 [01:18<04:45,  1.30s/it]Loading train:  24%|██▎       | 67/285 [01:19<04:36,  1.27s/it]Loading train:  24%|██▍       | 68/285 [01:20<04:16,  1.18s/it]Loading train:  24%|██▍       | 69/285 [01:21<04:03,  1.13s/it]Loading train:  25%|██▍       | 70/285 [01:22<03:50,  1.07s/it]Loading train:  25%|██▍       | 71/285 [01:23<03:45,  1.05s/it]Loading train:  25%|██▌       | 72/285 [01:24<03:39,  1.03s/it]Loading train:  26%|██▌       | 73/285 [01:25<03:37,  1.03s/it]Loading train:  26%|██▌       | 74/285 [01:26<03:39,  1.04s/it]Loading train:  26%|██▋       | 75/285 [01:27<03:34,  1.02s/it]Loading train:  27%|██▋       | 76/285 [01:28<03:35,  1.03s/it]Loading train:  27%|██▋       | 77/285 [01:29<03:38,  1.05s/it]Loading train:  27%|██▋       | 78/285 [01:30<03:29,  1.01s/it]Loading train:  28%|██▊       | 79/285 [01:31<03:30,  1.02s/it]Loading train:  28%|██▊       | 80/285 [01:32<03:24,  1.00it/s]Loading train:  28%|██▊       | 81/285 [01:33<03:30,  1.03s/it]Loading train:  29%|██▉       | 82/285 [01:34<03:19,  1.02it/s]Loading train:  29%|██▉       | 83/285 [01:35<03:17,  1.02it/s]Loading train:  29%|██▉       | 84/285 [01:36<03:10,  1.06it/s]Loading train:  30%|██▉       | 85/285 [01:37<03:24,  1.02s/it]Loading train:  30%|███       | 86/285 [01:38<03:23,  1.02s/it]Loading train:  31%|███       | 87/285 [01:39<03:33,  1.08s/it]Loading train:  31%|███       | 88/285 [01:40<03:31,  1.08s/it]Loading train:  31%|███       | 89/285 [01:41<03:28,  1.06s/it]Loading train:  32%|███▏      | 90/285 [01:43<03:33,  1.09s/it]Loading train:  32%|███▏      | 91/285 [01:44<03:32,  1.10s/it]Loading train:  32%|███▏      | 92/285 [01:45<03:36,  1.12s/it]Loading train:  33%|███▎      | 93/285 [01:46<03:38,  1.14s/it]Loading train:  33%|███▎      | 94/285 [01:47<03:36,  1.13s/it]Loading train:  33%|███▎      | 95/285 [01:48<03:40,  1.16s/it]Loading train:  34%|███▎      | 96/285 [01:50<03:34,  1.13s/it]Loading train:  34%|███▍      | 97/285 [01:51<03:35,  1.15s/it]Loading train:  34%|███▍      | 98/285 [01:52<03:26,  1.10s/it]Loading train:  35%|███▍      | 99/285 [01:53<03:20,  1.08s/it]Loading train:  35%|███▌      | 100/285 [01:54<03:23,  1.10s/it]Loading train:  35%|███▌      | 101/285 [01:55<03:28,  1.14s/it]Loading train:  36%|███▌      | 102/285 [01:56<03:26,  1.13s/it]Loading train:  36%|███▌      | 103/285 [01:57<03:24,  1.13s/it]Loading train:  36%|███▋      | 104/285 [01:58<03:21,  1.12s/it]Loading train:  37%|███▋      | 105/285 [02:00<03:26,  1.15s/it]Loading train:  37%|███▋      | 106/285 [02:01<03:22,  1.13s/it]Loading train:  38%|███▊      | 107/285 [02:02<03:19,  1.12s/it]Loading train:  38%|███▊      | 108/285 [02:03<03:11,  1.08s/it]Loading train:  38%|███▊      | 109/285 [02:04<03:07,  1.07s/it]Loading train:  39%|███▊      | 110/285 [02:05<03:09,  1.08s/it]Loading train:  39%|███▉      | 111/285 [02:06<03:00,  1.04s/it]Loading train:  39%|███▉      | 112/285 [02:07<03:09,  1.09s/it]Loading train:  40%|███▉      | 113/285 [02:08<03:07,  1.09s/it]Loading train:  40%|████      | 114/285 [02:09<03:04,  1.08s/it]Loading train:  40%|████      | 115/285 [02:10<03:02,  1.07s/it]Loading train:  41%|████      | 116/285 [02:11<02:57,  1.05s/it]Loading train:  41%|████      | 117/285 [02:12<02:57,  1.06s/it]Loading train:  41%|████▏     | 118/285 [02:13<02:53,  1.04s/it]Loading train:  42%|████▏     | 119/285 [02:14<02:55,  1.06s/it]Loading train:  42%|████▏     | 120/285 [02:16<02:53,  1.05s/it]Loading train:  42%|████▏     | 121/285 [02:17<03:12,  1.18s/it]Loading train:  43%|████▎     | 122/285 [02:18<03:17,  1.21s/it]Loading train:  43%|████▎     | 123/285 [02:20<03:20,  1.24s/it]Loading train:  44%|████▎     | 124/285 [02:20<03:03,  1.14s/it]Loading train:  44%|████▍     | 125/285 [02:21<02:52,  1.08s/it]Loading train:  44%|████▍     | 126/285 [02:22<02:44,  1.03s/it]Loading train:  45%|████▍     | 127/285 [02:23<02:34,  1.02it/s]Loading train:  45%|████▍     | 128/285 [02:24<02:29,  1.05it/s]Loading train:  45%|████▌     | 129/285 [02:25<02:30,  1.04it/s]Loading train:  46%|████▌     | 130/285 [02:26<02:28,  1.05it/s]Loading train:  46%|████▌     | 131/285 [02:27<02:27,  1.04it/s]Loading train:  46%|████▋     | 132/285 [02:28<02:21,  1.08it/s]Loading train:  47%|████▋     | 133/285 [02:29<02:18,  1.10it/s]Loading train:  47%|████▋     | 134/285 [02:30<02:14,  1.13it/s]Loading train:  47%|████▋     | 135/285 [02:30<02:13,  1.13it/s]Loading train:  48%|████▊     | 136/285 [02:31<02:17,  1.08it/s]Loading train:  48%|████▊     | 137/285 [02:32<02:17,  1.08it/s]Loading train:  48%|████▊     | 138/285 [02:33<02:21,  1.04it/s]Loading train:  49%|████▉     | 139/285 [02:34<02:19,  1.05it/s]Loading train:  49%|████▉     | 140/285 [02:35<02:16,  1.07it/s]Loading train:  49%|████▉     | 141/285 [02:36<02:13,  1.08it/s]Loading train:  50%|████▉     | 142/285 [02:37<02:11,  1.08it/s]Loading train:  50%|█████     | 143/285 [02:38<02:07,  1.11it/s]Loading train:  51%|█████     | 144/285 [02:39<02:08,  1.10it/s]Loading train:  51%|█████     | 145/285 [02:40<02:12,  1.06it/s]Loading train:  51%|█████     | 146/285 [02:41<02:09,  1.07it/s]Loading train:  52%|█████▏    | 147/285 [02:42<02:13,  1.04it/s]Loading train:  52%|█████▏    | 148/285 [02:43<02:11,  1.04it/s]Loading train:  52%|█████▏    | 149/285 [02:44<02:06,  1.07it/s]Loading train:  53%|█████▎    | 150/285 [02:45<02:10,  1.03it/s]Loading train:  53%|█████▎    | 151/285 [02:46<02:07,  1.05it/s]Loading train:  53%|█████▎    | 152/285 [02:47<02:07,  1.04it/s]Loading train:  54%|█████▎    | 153/285 [02:47<02:03,  1.07it/s]Loading train:  54%|█████▍    | 154/285 [02:49<02:08,  1.02it/s]Loading train:  54%|█████▍    | 155/285 [02:49<02:02,  1.06it/s]Loading train:  55%|█████▍    | 156/285 [02:50<02:04,  1.03it/s]Loading train:  55%|█████▌    | 157/285 [02:51<02:06,  1.01it/s]Loading train:  55%|█████▌    | 158/285 [02:52<02:03,  1.03it/s]Loading train:  56%|█████▌    | 159/285 [02:53<01:58,  1.06it/s]Loading train:  56%|█████▌    | 160/285 [02:54<01:58,  1.05it/s]Loading train:  56%|█████▋    | 161/285 [02:55<01:55,  1.08it/s]Loading train:  57%|█████▋    | 162/285 [02:56<01:50,  1.11it/s]Loading train:  57%|█████▋    | 163/285 [02:57<01:53,  1.07it/s]Loading train:  58%|█████▊    | 164/285 [02:58<01:49,  1.11it/s]Loading train:  58%|█████▊    | 165/285 [02:59<01:50,  1.08it/s]Loading train:  58%|█████▊    | 166/285 [03:00<01:47,  1.11it/s]Loading train:  59%|█████▊    | 167/285 [03:01<01:49,  1.08it/s]Loading train:  59%|█████▉    | 168/285 [03:01<01:45,  1.11it/s]Loading train:  59%|█████▉    | 169/285 [03:02<01:46,  1.09it/s]Loading train:  60%|█████▉    | 170/285 [03:03<01:42,  1.12it/s]Loading train:  60%|██████    | 171/285 [03:04<01:40,  1.14it/s]Loading train:  60%|██████    | 172/285 [03:05<01:43,  1.09it/s]Loading train:  61%|██████    | 173/285 [03:06<01:41,  1.10it/s]Loading train:  61%|██████    | 174/285 [03:07<01:39,  1.12it/s]Loading train:  61%|██████▏   | 175/285 [03:08<01:36,  1.14it/s]Loading train:  62%|██████▏   | 176/285 [03:09<01:39,  1.10it/s]Loading train:  62%|██████▏   | 177/285 [03:10<01:37,  1.11it/s]Loading train:  62%|██████▏   | 178/285 [03:10<01:37,  1.10it/s]Loading train:  63%|██████▎   | 179/285 [03:11<01:35,  1.11it/s]Loading train:  63%|██████▎   | 180/285 [03:12<01:35,  1.10it/s]Loading train:  64%|██████▎   | 181/285 [03:13<01:34,  1.10it/s]Loading train:  64%|██████▍   | 182/285 [03:14<01:34,  1.09it/s]Loading train:  64%|██████▍   | 183/285 [03:15<01:35,  1.07it/s]Loading train:  65%|██████▍   | 184/285 [03:16<01:30,  1.11it/s]Loading train:  65%|██████▍   | 185/285 [03:17<01:31,  1.09it/s]Loading train:  65%|██████▌   | 186/285 [03:18<01:28,  1.12it/s]Loading train:  66%|██████▌   | 187/285 [03:19<01:25,  1.14it/s]Loading train:  66%|██████▌   | 188/285 [03:19<01:23,  1.16it/s]Loading train:  66%|██████▋   | 189/285 [03:20<01:20,  1.19it/s]Loading train:  67%|██████▋   | 190/285 [03:21<01:22,  1.15it/s]Loading train:  67%|██████▋   | 191/285 [03:22<01:22,  1.14it/s]Loading train:  67%|██████▋   | 192/285 [03:23<01:24,  1.10it/s]Loading train:  68%|██████▊   | 193/285 [03:24<01:23,  1.10it/s]Loading train:  68%|██████▊   | 194/285 [03:25<01:24,  1.08it/s]Loading train:  68%|██████▊   | 195/285 [03:26<01:20,  1.11it/s]Loading train:  69%|██████▉   | 196/285 [03:27<01:23,  1.06it/s]Loading train:  69%|██████▉   | 197/285 [03:28<01:24,  1.05it/s]Loading train:  69%|██████▉   | 198/285 [03:29<01:22,  1.05it/s]Loading train:  70%|██████▉   | 199/285 [03:30<01:22,  1.04it/s]Loading train:  70%|███████   | 200/285 [03:31<01:20,  1.06it/s]Loading train:  71%|███████   | 201/285 [03:32<01:20,  1.05it/s]Loading train:  71%|███████   | 202/285 [03:32<01:18,  1.05it/s]Loading train:  71%|███████   | 203/285 [03:34<01:20,  1.01it/s]Loading train:  72%|███████▏  | 204/285 [03:35<01:21,  1.00s/it]Loading train:  72%|███████▏  | 205/285 [03:35<01:17,  1.03it/s]Loading train:  72%|███████▏  | 206/285 [03:37<01:18,  1.00it/s]Loading train:  73%|███████▎  | 207/285 [03:37<01:16,  1.02it/s]Loading train:  73%|███████▎  | 208/285 [03:39<01:18,  1.01s/it]Loading train:  73%|███████▎  | 209/285 [03:40<01:15,  1.00it/s]Loading train:  74%|███████▎  | 210/285 [03:41<01:14,  1.01it/s]Loading train:  74%|███████▍  | 211/285 [03:41<01:12,  1.03it/s]Loading train:  74%|███████▍  | 212/285 [03:43<01:13,  1.01s/it]Loading train:  75%|███████▍  | 213/285 [03:43<01:11,  1.01it/s]Loading train:  75%|███████▌  | 214/285 [03:44<01:09,  1.02it/s]Loading train:  75%|███████▌  | 215/285 [03:45<01:06,  1.05it/s]Loading train:  76%|███████▌  | 216/285 [03:46<01:05,  1.05it/s]Loading train:  76%|███████▌  | 217/285 [03:47<01:02,  1.09it/s]Loading train:  76%|███████▋  | 218/285 [03:48<01:01,  1.10it/s]Loading train:  77%|███████▋  | 219/285 [03:49<00:57,  1.14it/s]Loading train:  77%|███████▋  | 220/285 [03:50<00:58,  1.12it/s]Loading train:  78%|███████▊  | 221/285 [03:51<00:57,  1.12it/s]Loading train:  78%|███████▊  | 222/285 [03:52<00:56,  1.12it/s]Loading train:  78%|███████▊  | 223/285 [03:52<00:55,  1.12it/s]Loading train:  79%|███████▊  | 224/285 [03:53<00:56,  1.09it/s]Loading train:  79%|███████▉  | 225/285 [03:54<00:54,  1.10it/s]Loading train:  79%|███████▉  | 226/285 [03:55<00:55,  1.07it/s]Loading train:  80%|███████▉  | 227/285 [03:56<00:54,  1.07it/s]Loading train:  80%|████████  | 228/285 [03:57<00:52,  1.08it/s]Loading train:  80%|████████  | 229/285 [03:58<00:50,  1.11it/s]Loading train:  81%|████████  | 230/285 [03:59<00:48,  1.13it/s]Loading train:  81%|████████  | 231/285 [04:00<00:47,  1.14it/s]Loading train:  81%|████████▏ | 232/285 [04:01<00:50,  1.04it/s]Loading train:  82%|████████▏ | 233/285 [04:02<00:50,  1.04it/s]Loading train:  82%|████████▏ | 234/285 [04:03<00:50,  1.00it/s]Loading train:  82%|████████▏ | 235/285 [04:04<00:52,  1.06s/it]Loading train:  83%|████████▎ | 236/285 [04:05<00:51,  1.05s/it]Loading train:  83%|████████▎ | 237/285 [04:06<00:49,  1.03s/it]Loading train:  84%|████████▎ | 238/285 [04:07<00:47,  1.02s/it]Loading train:  84%|████████▍ | 239/285 [04:08<00:47,  1.04s/it]Loading train:  84%|████████▍ | 240/285 [04:09<00:46,  1.04s/it]Loading train:  85%|████████▍ | 241/285 [04:10<00:46,  1.06s/it]Loading train:  85%|████████▍ | 242/285 [04:11<00:44,  1.03s/it]Loading train:  85%|████████▌ | 243/285 [04:12<00:44,  1.06s/it]Loading train:  86%|████████▌ | 244/285 [04:14<00:44,  1.09s/it]Loading train:  86%|████████▌ | 245/285 [04:15<00:42,  1.07s/it]Loading train:  86%|████████▋ | 246/285 [04:16<00:41,  1.06s/it]Loading train:  87%|████████▋ | 247/285 [04:17<00:40,  1.08s/it]Loading train:  87%|████████▋ | 248/285 [04:18<00:39,  1.07s/it]Loading train:  87%|████████▋ | 249/285 [04:19<00:37,  1.04s/it]Loading train:  88%|████████▊ | 250/285 [04:20<00:36,  1.04s/it]Loading train:  88%|████████▊ | 251/285 [04:21<00:34,  1.01s/it]Loading train:  88%|████████▊ | 252/285 [04:22<00:32,  1.01it/s]Loading train:  89%|████████▉ | 253/285 [04:23<00:30,  1.05it/s]Loading train:  89%|████████▉ | 254/285 [04:24<00:29,  1.05it/s]Loading train:  89%|████████▉ | 255/285 [04:24<00:28,  1.07it/s]Loading train:  90%|████████▉ | 256/285 [04:25<00:27,  1.07it/s]Loading train:  90%|█████████ | 257/285 [04:26<00:25,  1.09it/s]Loading train:  91%|█████████ | 258/285 [04:27<00:24,  1.11it/s]Loading train:  91%|█████████ | 259/285 [04:28<00:24,  1.06it/s]Loading train:  91%|█████████ | 260/285 [04:29<00:22,  1.10it/s]Loading train:  92%|█████████▏| 261/285 [04:30<00:22,  1.09it/s]Loading train:  92%|█████████▏| 262/285 [04:31<00:20,  1.12it/s]Loading train:  92%|█████████▏| 263/285 [04:32<00:20,  1.10it/s]Loading train:  93%|█████████▎| 264/285 [04:33<00:19,  1.10it/s]Loading train:  93%|█████████▎| 265/285 [04:33<00:18,  1.10it/s]Loading train:  93%|█████████▎| 266/285 [04:34<00:17,  1.07it/s]Loading train:  94%|█████████▎| 267/285 [04:35<00:16,  1.09it/s]Loading train:  94%|█████████▍| 268/285 [04:37<00:17,  1.02s/it]Loading train:  94%|█████████▍| 269/285 [04:38<00:16,  1.04s/it]Loading train:  95%|█████████▍| 270/285 [04:39<00:15,  1.04s/it]Loading train:  95%|█████████▌| 271/285 [04:40<00:15,  1.09s/it]Loading train:  95%|█████████▌| 272/285 [04:41<00:14,  1.08s/it]Loading train:  96%|█████████▌| 273/285 [04:42<00:12,  1.07s/it]Loading train:  96%|█████████▌| 274/285 [04:43<00:12,  1.09s/it]Loading train:  96%|█████████▋| 275/285 [04:44<00:10,  1.10s/it]Loading train:  97%|█████████▋| 276/285 [04:45<00:09,  1.11s/it]Loading train:  97%|█████████▋| 277/285 [04:47<00:09,  1.16s/it]Loading train:  98%|█████████▊| 278/285 [04:48<00:07,  1.12s/it]Loading train:  98%|█████████▊| 279/285 [04:49<00:06,  1.10s/it]Loading train:  98%|█████████▊| 280/285 [04:50<00:05,  1.09s/it]Loading train:  99%|█████████▊| 281/285 [04:51<00:04,  1.07s/it]Loading train:  99%|█████████▉| 282/285 [04:52<00:03,  1.06s/it]Loading train:  99%|█████████▉| 283/285 [04:53<00:02,  1.07s/it]Loading train: 100%|█████████▉| 284/285 [04:54<00:01,  1.03s/it]Loading train: 100%|██████████| 285/285 [04:55<00:00,  1.04s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 19/285 [00:00<00:01, 181.13it/s]concatenating: train:  14%|█▍        | 40/285 [00:00<00:01, 188.02it/s]concatenating: train:  21%|██        | 60/285 [00:00<00:01, 191.02it/s]concatenating: train:  26%|██▌       | 73/285 [00:00<00:02, 86.80it/s] concatenating: train:  32%|███▏      | 92/285 [00:00<00:01, 102.38it/s]concatenating: train:  40%|███▉      | 113/285 [00:00<00:01, 120.42it/s]concatenating: train:  48%|████▊     | 137/285 [00:00<00:01, 141.12it/s]concatenating: train:  57%|█████▋    | 162/285 [00:01<00:00, 161.67it/s]concatenating: train:  64%|██████▍   | 183/285 [00:01<00:00, 172.33it/s]concatenating: train:  74%|███████▍  | 211/285 [00:01<00:00, 192.68it/s]concatenating: train:  84%|████████▍ | 239/285 [00:01<00:00, 211.31it/s]concatenating: train:  94%|█████████▍| 268/285 [00:01<00:00, 228.14it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 185.45it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.49s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.46s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.37s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 35.51it/s]2019-07-06 22:39:57.697604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 22:39:57.697684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 22:39:57.697698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 22:39:57.697707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 22:39:57.698123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:08,  4.40it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:07,  5.12it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:07,  4.69it/s]loading the weights for Unet:  20%|██        | 8/40 [00:00<00:05,  5.99it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:05,  5.31it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:04,  5.83it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  5.18it/s]loading the weights for Unet:  40%|████      | 16/40 [00:01<00:03,  6.49it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.50it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  6.19it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:04,  4.85it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:02<00:02,  5.95it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.44it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  5.31it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:02,  5.90it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  4.76it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  5.76it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  6.28it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.50it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  6.14it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  5.20it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  7.54it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 52, 30)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 52, 30)   8130        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 52, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 52, 30)   8130        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 52, 30)   120         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 52, 30)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 52, 30)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 52, 20)   5420        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 52, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 52, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 52, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 52, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 52, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 26, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 26, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 26, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 26, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 26, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 26, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 26, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 13, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 13, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 13, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 13, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 13, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 13, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 13, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 13, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 26, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 26, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 26, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 26, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 26, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 26, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 26, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 26, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 52, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 52, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 52, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 52, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 52, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 52, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 52, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 52, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 52, 30)   5430        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 52, 30)   120         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 52, 30)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 52, 30)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 52, 13)   403         dropout_9[0][0]                  
==================================================================================================
Total params: 211,933
Trainable params: 68,213
Non-trainable params: 143,720
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.49841486e-02 3.19966680e-02 7.50970181e-02 9.33357939e-03
 2.71292049e-02 7.07427267e-03 8.46489586e-02 1.12779077e-01
 8.61338510e-02 1.32649165e-02 2.94521391e-01 1.92807035e-01
 2.29878984e-04]
Train on 18361 samples, validate on 179 samples
Epoch 1/300
 - 23s - loss: 112.3504 - acc: 0.7597 - mDice: 0.0139 - val_loss: 20.9506 - val_acc: 0.9136 - val_mDice: 0.0043

Epoch 00001: val_mDice improved from -inf to 0.00432, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 16s - loss: 21.7332 - acc: 0.8847 - mDice: 0.0160 - val_loss: 8.3169 - val_acc: 0.9136 - val_mDice: 0.0098

Epoch 00002: val_mDice improved from 0.00432 to 0.00980, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 15s - loss: 11.1450 - acc: 0.8859 - mDice: 0.0197 - val_loss: 5.9449 - val_acc: 0.9136 - val_mDice: 0.0129

Epoch 00003: val_mDice improved from 0.00980 to 0.01286, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 16s - loss: 8.2174 - acc: 0.8862 - mDice: 0.0252 - val_loss: 4.9965 - val_acc: 0.9136 - val_mDice: 0.0233

Epoch 00004: val_mDice improved from 0.01286 to 0.02334, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 16s - loss: 6.8373 - acc: 0.8862 - mDice: 0.0313 - val_loss: 4.5979 - val_acc: 0.9136 - val_mDice: 0.0409

Epoch 00005: val_mDice improved from 0.02334 to 0.04092, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 15s - loss: 5.9571 - acc: 0.8862 - mDice: 0.0398 - val_loss: 4.1781 - val_acc: 0.9136 - val_mDice: 0.0548

Epoch 00006: val_mDice improved from 0.04092 to 0.05481, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 16s - loss: 5.2732 - acc: 0.8862 - mDice: 0.0531 - val_loss: 3.8163 - val_acc: 0.9136 - val_mDice: 0.0725

Epoch 00007: val_mDice improved from 0.05481 to 0.07245, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 16s - loss: 4.6769 - acc: 0.8867 - mDice: 0.0785 - val_loss: 3.3686 - val_acc: 0.9142 - val_mDice: 0.1130

Epoch 00008: val_mDice improved from 0.07245 to 0.11297, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 16s - loss: 4.1537 - acc: 0.8899 - mDice: 0.1096 - val_loss: 2.8208 - val_acc: 0.9184 - val_mDice: 0.1708

Epoch 00009: val_mDice improved from 0.11297 to 0.17078, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 16s - loss: 3.6723 - acc: 0.8944 - mDice: 0.1519 - val_loss: 2.5389 - val_acc: 0.9229 - val_mDice: 0.2181

Epoch 00010: val_mDice improved from 0.17078 to 0.21811, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 16s - loss: 3.3227 - acc: 0.8980 - mDice: 0.1922 - val_loss: 2.6361 - val_acc: 0.9235 - val_mDice: 0.2398

Epoch 00011: val_mDice improved from 0.21811 to 0.23984, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 16s - loss: 3.0372 - acc: 0.9018 - mDice: 0.2299 - val_loss: 2.2841 - val_acc: 0.9306 - val_mDice: 0.2983

Epoch 00012: val_mDice improved from 0.23984 to 0.29834, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 16s - loss: 2.8032 - acc: 0.9062 - mDice: 0.2656 - val_loss: 2.1497 - val_acc: 0.9336 - val_mDice: 0.3329

Epoch 00013: val_mDice improved from 0.29834 to 0.33286, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 16s - loss: 2.6205 - acc: 0.9129 - mDice: 0.2959 - val_loss: 2.1769 - val_acc: 0.9378 - val_mDice: 0.3421

Epoch 00014: val_mDice improved from 0.33286 to 0.34205, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 16s - loss: 2.4811 - acc: 0.9185 - mDice: 0.3226 - val_loss: 2.6955 - val_acc: 0.9355 - val_mDice: 0.3192

Epoch 00015: val_mDice did not improve from 0.34205
Epoch 16/300
 - 17s - loss: 2.3536 - acc: 0.9222 - mDice: 0.3466 - val_loss: 2.0889 - val_acc: 0.9437 - val_mDice: 0.3906

Epoch 00016: val_mDice improved from 0.34205 to 0.39055, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 17s - loss: 2.2558 - acc: 0.9248 - mDice: 0.3659 - val_loss: 2.0383 - val_acc: 0.9458 - val_mDice: 0.4061

Epoch 00017: val_mDice improved from 0.39055 to 0.40611, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 17s - loss: 2.1741 - acc: 0.9268 - mDice: 0.3828 - val_loss: 1.9502 - val_acc: 0.9398 - val_mDice: 0.4196

Epoch 00018: val_mDice improved from 0.40611 to 0.41958, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 16s - loss: 2.0986 - acc: 0.9287 - mDice: 0.3996 - val_loss: 1.8988 - val_acc: 0.9441 - val_mDice: 0.4389

Epoch 00019: val_mDice improved from 0.41958 to 0.43893, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 16s - loss: 2.0293 - acc: 0.9304 - mDice: 0.4148 - val_loss: 1.9221 - val_acc: 0.9469 - val_mDice: 0.4450

Epoch 00020: val_mDice improved from 0.43893 to 0.44496, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 16s - loss: 1.9742 - acc: 0.9316 - mDice: 0.4281 - val_loss: 1.8390 - val_acc: 0.9471 - val_mDice: 0.4554

Epoch 00021: val_mDice improved from 0.44496 to 0.45543, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 16s - loss: 1.9227 - acc: 0.9326 - mDice: 0.4405 - val_loss: 1.8824 - val_acc: 0.9487 - val_mDice: 0.4635

Epoch 00022: val_mDice improved from 0.45543 to 0.46348, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 17s - loss: 1.8832 - acc: 0.9338 - mDice: 0.4505 - val_loss: 1.8375 - val_acc: 0.9487 - val_mDice: 0.4688

Epoch 00023: val_mDice improved from 0.46348 to 0.46878, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 16s - loss: 1.8418 - acc: 0.9346 - mDice: 0.4613 - val_loss: 1.9393 - val_acc: 0.9456 - val_mDice: 0.4604

Epoch 00024: val_mDice did not improve from 0.46878
Epoch 25/300
 - 16s - loss: 1.7947 - acc: 0.9357 - mDice: 0.4723 - val_loss: 1.8464 - val_acc: 0.9499 - val_mDice: 0.4822

Epoch 00025: val_mDice improved from 0.46878 to 0.48217, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 16s - loss: 1.7731 - acc: 0.9362 - mDice: 0.4781 - val_loss: 1.9013 - val_acc: 0.9492 - val_mDice: 0.4781

Epoch 00026: val_mDice did not improve from 0.48217
Epoch 27/300
 - 16s - loss: 1.7332 - acc: 0.9373 - mDice: 0.4876 - val_loss: 1.9398 - val_acc: 0.9498 - val_mDice: 0.4837

Epoch 00027: val_mDice improved from 0.48217 to 0.48370, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 16s - loss: 1.6991 - acc: 0.9384 - mDice: 0.4965 - val_loss: 1.9288 - val_acc: 0.9496 - val_mDice: 0.4822

Epoch 00028: val_mDice did not improve from 0.48370
Epoch 29/300
 - 15s - loss: 1.6688 - acc: 0.9392 - mDice: 0.5052 - val_loss: 1.8800 - val_acc: 0.9488 - val_mDice: 0.5016

Epoch 00029: val_mDice improved from 0.48370 to 0.50161, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 15s - loss: 1.6411 - acc: 0.9399 - mDice: 0.5124 - val_loss: 1.9025 - val_acc: 0.9486 - val_mDice: 0.4947

Epoch 00030: val_mDice did not improve from 0.50161
Epoch 31/300
 - 16s - loss: 1.6234 - acc: 0.9403 - mDice: 0.5174 - val_loss: 1.8888 - val_acc: 0.9509 - val_mDice: 0.4979

Epoch 00031: val_mDice did not improve from 0.50161
Epoch 32/300
 - 15s - loss: 1.5933 - acc: 0.9410 - mDice: 0.5246 - val_loss: 1.9414 - val_acc: 0.9500 - val_mDice: 0.5003

Epoch 00032: val_mDice did not improve from 0.50161
Epoch 33/300
 - 15s - loss: 1.5802 - acc: 0.9414 - mDice: 0.5291 - val_loss: 1.8506 - val_acc: 0.9494 - val_mDice: 0.5062

Epoch 00033: val_mDice improved from 0.50161 to 0.50618, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 15s - loss: 1.5641 - acc: 0.9418 - mDice: 0.5334 - val_loss: 1.8875 - val_acc: 0.9483 - val_mDice: 0.5022

Epoch 00034: val_mDice did not improve from 0.50618
Epoch 35/300
 - 15s - loss: 1.5452 - acc: 0.9423 - mDice: 0.5382 - val_loss: 1.9897 - val_acc: 0.9500 - val_mDice: 0.4990

Epoch 00035: val_mDice did not improve from 0.50618
Epoch 36/300
 - 15s - loss: 1.5225 - acc: 0.9430 - mDice: 0.5450 - val_loss: 1.9647 - val_acc: 0.9497 - val_mDice: 0.4979

Epoch 00036: val_mDice did not improve from 0.50618
Epoch 37/300
 - 15s - loss: 1.5094 - acc: 0.9433 - mDice: 0.5478 - val_loss: 1.9764 - val_acc: 0.9507 - val_mDice: 0.5048

Epoch 00037: val_mDice did not improve from 0.50618
Epoch 38/300
 - 15s - loss: 1.4931 - acc: 0.9439 - mDice: 0.5528 - val_loss: 1.9340 - val_acc: 0.9498 - val_mDice: 0.5123

Epoch 00038: val_mDice improved from 0.50618 to 0.51233, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 15s - loss: 1.4786 - acc: 0.9444 - mDice: 0.5561 - val_loss: 1.9648 - val_acc: 0.9491 - val_mDice: 0.5044

Epoch 00039: val_mDice did not improve from 0.51233
Epoch 40/300
 - 15s - loss: 1.4626 - acc: 0.9448 - mDice: 0.5607 - val_loss: 1.9241 - val_acc: 0.9494 - val_mDice: 0.5216

Epoch 00040: val_mDice improved from 0.51233 to 0.52163, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 15s - loss: 1.4530 - acc: 0.9452 - mDice: 0.5635 - val_loss: 1.9730 - val_acc: 0.9500 - val_mDice: 0.5147

Epoch 00041: val_mDice did not improve from 0.52163
Epoch 42/300
 - 16s - loss: 1.4353 - acc: 0.9457 - mDice: 0.5677 - val_loss: 2.0535 - val_acc: 0.9499 - val_mDice: 0.5114

Epoch 00042: val_mDice did not improve from 0.52163
Epoch 43/300
 - 15s - loss: 1.4302 - acc: 0.9457 - mDice: 0.5696 - val_loss: 1.9301 - val_acc: 0.9489 - val_mDice: 0.5158

Epoch 00043: val_mDice did not improve from 0.52163
Epoch 44/300
 - 15s - loss: 1.4145 - acc: 0.9461 - mDice: 0.5736 - val_loss: 2.0488 - val_acc: 0.9472 - val_mDice: 0.5048

Epoch 00044: val_mDice did not improve from 0.52163
Epoch 45/300
 - 15s - loss: 1.4030 - acc: 0.9463 - mDice: 0.5764 - val_loss: 1.9800 - val_acc: 0.9500 - val_mDice: 0.5191

Epoch 00045: val_mDice did not improve from 0.52163
Epoch 46/300
 - 15s - loss: 1.3968 - acc: 0.9463 - mDice: 0.5785 - val_loss: 2.0473 - val_acc: 0.9503 - val_mDice: 0.5155

Epoch 00046: val_mDice did not improve from 0.52163
Epoch 47/300
 - 16s - loss: 1.3863 - acc: 0.9465 - mDice: 0.5818 - val_loss: 2.0234 - val_acc: 0.9499 - val_mDice: 0.5232

Epoch 00047: val_mDice improved from 0.52163 to 0.52322, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 15s - loss: 1.3776 - acc: 0.9466 - mDice: 0.5846 - val_loss: 1.9786 - val_acc: 0.9510 - val_mDice: 0.5288

Epoch 00048: val_mDice improved from 0.52322 to 0.52877, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 15s - loss: 1.3643 - acc: 0.9467 - mDice: 0.5879 - val_loss: 2.1236 - val_acc: 0.9494 - val_mDice: 0.5124

Epoch 00049: val_mDice did not improve from 0.52877
Epoch 50/300
 - 15s - loss: 1.3603 - acc: 0.9465 - mDice: 0.5896 - val_loss: 1.9456 - val_acc: 0.9515 - val_mDice: 0.5307

Epoch 00050: val_mDice improved from 0.52877 to 0.53066, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 15s - loss: 1.3508 - acc: 0.9466 - mDice: 0.5922 - val_loss: 2.0592 - val_acc: 0.9511 - val_mDice: 0.5214

Epoch 00051: val_mDice did not improve from 0.53066
Epoch 52/300
 - 15s - loss: 1.3419 - acc: 0.9468 - mDice: 0.5944 - val_loss: 2.2658 - val_acc: 0.9505 - val_mDice: 0.5088

Epoch 00052: val_mDice did not improve from 0.53066
Epoch 53/300
 - 15s - loss: 1.3291 - acc: 0.9470 - mDice: 0.5979 - val_loss: 2.0563 - val_acc: 0.9515 - val_mDice: 0.5259

Epoch 00053: val_mDice did not improve from 0.53066
Epoch 54/300
 - 15s - loss: 1.3270 - acc: 0.9469 - mDice: 0.5982 - val_loss: 2.0543 - val_acc: 0.9514 - val_mDice: 0.5258

Epoch 00054: val_mDice did not improve from 0.53066
Epoch 55/300
 - 15s - loss: 1.3160 - acc: 0.9472 - mDice: 0.6010 - val_loss: 2.0149 - val_acc: 0.9518 - val_mDice: 0.5341

Epoch 00055: val_mDice improved from 0.53066 to 0.53410, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 14s - loss: 1.3076 - acc: 0.9474 - mDice: 0.6039 - val_loss: 2.1778 - val_acc: 0.9507 - val_mDice: 0.5179

Epoch 00056: val_mDice did not improve from 0.53410
Epoch 57/300
 - 15s - loss: 1.3063 - acc: 0.9473 - mDice: 0.6039 - val_loss: 2.0962 - val_acc: 0.9497 - val_mDice: 0.5231

Epoch 00057: val_mDice did not improve from 0.53410
Epoch 58/300
 - 15s - loss: 1.2985 - acc: 0.9475 - mDice: 0.6057 - val_loss: 2.0642 - val_acc: 0.9506 - val_mDice: 0.5332

Epoch 00058: val_mDice did not improve from 0.53410
Epoch 59/300
 - 15s - loss: 1.2897 - acc: 0.9477 - mDice: 0.6082 - val_loss: 1.9771 - val_acc: 0.9518 - val_mDice: 0.5364

Epoch 00059: val_mDice improved from 0.53410 to 0.53639, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 60/300
 - 15s - loss: 1.2878 - acc: 0.9477 - mDice: 0.6090 - val_loss: 2.0003 - val_acc: 0.9505 - val_mDice: 0.5444

Epoch 00060: val_mDice improved from 0.53639 to 0.54436, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 61/300
 - 15s - loss: 1.2796 - acc: 0.9478 - mDice: 0.6112 - val_loss: 2.0598 - val_acc: 0.9490 - val_mDice: 0.5296

Epoch 00061: val_mDice did not improve from 0.54436
Epoch 62/300
 - 15s - loss: 1.2747 - acc: 0.9479 - mDice: 0.6125 - val_loss: 2.0225 - val_acc: 0.9509 - val_mDice: 0.5390

Epoch 00062: val_mDice did not improve from 0.54436
Epoch 63/300
 - 15s - loss: 1.2703 - acc: 0.9480 - mDice: 0.6138 - val_loss: 2.0883 - val_acc: 0.9493 - val_mDice: 0.5296

Epoch 00063: val_mDice did not improve from 0.54436
Epoch 64/300
 - 15s - loss: 1.2676 - acc: 0.9481 - mDice: 0.6149 - val_loss: 2.1619 - val_acc: 0.9490 - val_mDice: 0.5230

Epoch 00064: val_mDice did not improve from 0.54436
Epoch 65/300
 - 15s - loss: 1.2603 - acc: 0.9481 - mDice: 0.6166 - val_loss: 2.1132 - val_acc: 0.9506 - val_mDice: 0.5324

Epoch 00065: val_mDice did not improve from 0.54436
Epoch 66/300
 - 15s - loss: 1.2530 - acc: 0.9484 - mDice: 0.6186 - val_loss: 2.1951 - val_acc: 0.9502 - val_mDice: 0.5269

Epoch 00066: val_mDice did not improve from 0.54436
Epoch 67/300
 - 15s - loss: 1.2513 - acc: 0.9483 - mDice: 0.6192 - val_loss: 2.0741 - val_acc: 0.9494 - val_mDice: 0.5354

Epoch 00067: val_mDice did not improve from 0.54436
Epoch 68/300
 - 15s - loss: 1.2435 - acc: 0.9485 - mDice: 0.6212 - val_loss: 2.1500 - val_acc: 0.9503 - val_mDice: 0.5333

Epoch 00068: val_mDice did not improve from 0.54436
Epoch 69/300
 - 15s - loss: 1.2335 - acc: 0.9487 - mDice: 0.6241 - val_loss: 2.1092 - val_acc: 0.9502 - val_mDice: 0.5329

Epoch 00069: val_mDice did not improve from 0.54436
Epoch 70/300
 - 15s - loss: 1.2385 - acc: 0.9485 - mDice: 0.6224 - val_loss: 2.1884 - val_acc: 0.9514 - val_mDice: 0.5341

Epoch 00070: val_mDice did not improve from 0.54436
Epoch 71/300
 - 15s - loss: 1.2299 - acc: 0.9488 - mDice: 0.6252 - val_loss: 2.0398 - val_acc: 0.9511 - val_mDice: 0.5471

Epoch 00071: val_mDice improved from 0.54436 to 0.54707, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 72/300
 - 15s - loss: 1.2300 - acc: 0.9488 - mDice: 0.6250 - val_loss: 2.1098 - val_acc: 0.9493 - val_mDice: 0.5289

Epoch 00072: val_mDice did not improve from 0.54707
Epoch 73/300
 - 15s - loss: 1.2227 - acc: 0.9489 - mDice: 0.6270 - val_loss: 2.1320 - val_acc: 0.9500 - val_mDice: 0.5377

Epoch 00073: val_mDice did not improve from 0.54707
Epoch 74/300
 - 15s - loss: 1.2192 - acc: 0.9491 - mDice: 0.6281 - val_loss: 2.1150 - val_acc: 0.9510 - val_mDice: 0.5423

Epoch 00074: val_mDice did not improve from 0.54707
Epoch 75/300
 - 15s - loss: 1.2185 - acc: 0.9491 - mDice: 0.6287 - val_loss: 2.1913 - val_acc: 0.9511 - val_mDice: 0.5258

Epoch 00075: val_mDice did not improve from 0.54707
Epoch 76/300
 - 15s - loss: 1.2100 - acc: 0.9493 - mDice: 0.6312 - val_loss: 2.1422 - val_acc: 0.9513 - val_mDice: 0.5363

Epoch 00076: val_mDice did not improve from 0.54707
Epoch 77/300
 - 16s - loss: 1.2051 - acc: 0.9494 - mDice: 0.6324 - val_loss: 2.1554 - val_acc: 0.9499 - val_mDice: 0.5252

Epoch 00077: val_mDice did not improve from 0.54707
Epoch 78/300
 - 15s - loss: 1.2071 - acc: 0.9494 - mDice: 0.6325 - val_loss: 2.1534 - val_acc: 0.9507 - val_mDice: 0.5283

Epoch 00078: val_mDice did not improve from 0.54707
Epoch 79/300
 - 15s - loss: 1.2031 - acc: 0.9495 - mDice: 0.6329 - val_loss: 2.0564 - val_acc: 0.9513 - val_mDice: 0.5450

Epoch 00079: val_mDice did not improve from 0.54707
Epoch 80/300
 - 15s - loss: 1.1959 - acc: 0.9497 - mDice: 0.6347 - val_loss: 2.1673 - val_acc: 0.9517 - val_mDice: 0.5405

Epoch 00080: val_mDice did not improve from 0.54707
Epoch 81/300
 - 15s - loss: 1.1971 - acc: 0.9497 - mDice: 0.6348 - val_loss: 2.0965 - val_acc: 0.9522 - val_mDice: 0.5449

Epoch 00081: val_mDice did not improve from 0.54707
Epoch 82/300
 - 15s - loss: 1.1921 - acc: 0.9497 - mDice: 0.6355 - val_loss: 2.0696 - val_acc: 0.9512 - val_mDice: 0.5517

Epoch 00082: val_mDice improved from 0.54707 to 0.55166, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 83/300
 - 15s - loss: 1.1848 - acc: 0.9500 - mDice: 0.6381 - val_loss: 2.2037 - val_acc: 0.9509 - val_mDice: 0.5333

Epoch 00083: val_mDice did not improve from 0.55166
Epoch 84/300
 - 15s - loss: 1.1860 - acc: 0.9500 - mDice: 0.6375 - val_loss: 2.1913 - val_acc: 0.9510 - val_mDice: 0.5358

Epoch 00084: val_mDice did not improve from 0.55166
Epoch 85/300
 - 15s - loss: 1.1849 - acc: 0.9500 - mDice: 0.6380 - val_loss: 2.1188 - val_acc: 0.9510 - val_mDice: 0.5387

Epoch 00085: val_mDice did not improve from 0.55166
Epoch 86/300
 - 15s - loss: 1.1831 - acc: 0.9501 - mDice: 0.6384 - val_loss: 2.0835 - val_acc: 0.9505 - val_mDice: 0.5475

Epoch 00086: val_mDice did not improve from 0.55166
Epoch 87/300
 - 15s - loss: 1.1750 - acc: 0.9503 - mDice: 0.6406 - val_loss: 2.2076 - val_acc: 0.9518 - val_mDice: 0.5379

Epoch 00087: val_mDice did not improve from 0.55166
Epoch 88/300
 - 15s - loss: 1.1777 - acc: 0.9503 - mDice: 0.6398 - val_loss: 2.1830 - val_acc: 0.9517 - val_mDice: 0.5370

Epoch 00088: val_mDice did not improve from 0.55166
Epoch 89/300
 - 15s - loss: 1.1734 - acc: 0.9504 - mDice: 0.6410 - val_loss: 2.2142 - val_acc: 0.9514 - val_mDice: 0.5397

Epoch 00089: val_mDice did not improve from 0.55166
Epoch 90/300
 - 15s - loss: 1.1675 - acc: 0.9506 - mDice: 0.6434 - val_loss: 2.2671 - val_acc: 0.9519 - val_mDice: 0.5315

Epoch 00090: val_mDice did not improve from 0.55166
Epoch 91/300
 - 15s - loss: 1.1720 - acc: 0.9504 - mDice: 0.6417 - val_loss: 2.0667 - val_acc: 0.9517 - val_mDice: 0.5487

Epoch 00091: val_mDice did not improve from 0.55166
Epoch 92/300
 - 15s - loss: 1.1620 - acc: 0.9506 - mDice: 0.6445 - val_loss: 2.3296 - val_acc: 0.9512 - val_mDice: 0.5337

Epoch 00092: val_mDice did not improve from 0.55166
Epoch 93/300
 - 15s - loss: 1.1622 - acc: 0.9507 - mDice: 0.6448 - val_loss: 2.1678 - val_acc: 0.9503 - val_mDice: 0.5341

Epoch 00093: val_mDice did not improve from 0.55166
Epoch 94/300
 - 15s - loss: 1.1600 - acc: 0.9507 - mDice: 0.6452 - val_loss: 2.2150 - val_acc: 0.9518 - val_mDice: 0.5399

Epoch 00094: val_mDice did not improve from 0.55166
Epoch 95/300
 - 15s - loss: 1.1587 - acc: 0.9508 - mDice: 0.6459 - val_loss: 2.2423 - val_acc: 0.9511 - val_mDice: 0.5304

Epoch 00095: val_mDice did not improve from 0.55166
Epoch 96/300
 - 15s - loss: 1.1519 - acc: 0.9509 - mDice: 0.6474 - val_loss: 2.2388 - val_acc: 0.9512 - val_mDice: 0.5349

Epoch 00096: val_mDice did not improve from 0.55166
Epoch 97/300
 - 15s - loss: 1.1567 - acc: 0.9509 - mDice: 0.6463 - val_loss: 2.3249 - val_acc: 0.9506 - val_mDice: 0.5282

Epoch 00097: val_mDice did not improve from 0.55166
Epoch 98/300
 - 15s - loss: 1.1531 - acc: 0.9510 - mDice: 0.6467 - val_loss: 2.3500 - val_acc: 0.9508 - val_mDice: 0.5308

Epoch 00098: val_mDice did not improve from 0.55166
Epoch 99/300
 - 15s - loss: 1.1456 - acc: 0.9512 - mDice: 0.6494 - val_loss: 2.1059 - val_acc: 0.9526 - val_mDice: 0.5461

Epoch 00099: val_mDice did not improve from 0.55166
Epoch 100/300
 - 15s - loss: 1.1502 - acc: 0.9511 - mDice: 0.6481 - val_loss: 2.1920 - val_acc: 0.9522 - val_mDice: 0.5384

Epoch 00100: val_mDice did not improve from 0.55166
Epoch 101/300
 - 15s - loss: 1.1419 - acc: 0.9513 - mDice: 0.6506 - val_loss: 2.2424 - val_acc: 0.9520 - val_mDice: 0.5422

Epoch 00101: val_mDice did not improve from 0.55166
Epoch 102/300
 - 15s - loss: 1.1415 - acc: 0.9513 - mDice: 0.6510 - val_loss: 2.2268 - val_acc: 0.9491 - val_mDice: 0.5428

Epoch 00102: val_mDice did not improve from 0.55166
Epoch 103/300
 - 15s - loss: 1.1390 - acc: 0.9514 - mDice: 0.6512 - val_loss: 2.3198 - val_acc: 0.9509 - val_mDice: 0.5264

Epoch 00103: val_mDice did not improve from 0.55166
Epoch 104/300
 - 15s - loss: 1.1400 - acc: 0.9513 - mDice: 0.6512 - val_loss: 2.1134 - val_acc: 0.9510 - val_mDice: 0.5469

Epoch 00104: val_mDice did not improve from 0.55166
Epoch 105/300
 - 14s - loss: 1.1358 - acc: 0.9515 - mDice: 0.6524 - val_loss: 2.1742 - val_acc: 0.9513 - val_mDice: 0.5461

Epoch 00105: val_mDice did not improve from 0.55166
Epoch 106/300
 - 15s - loss: 1.1367 - acc: 0.9514 - mDice: 0.6515 - val_loss: 2.1800 - val_acc: 0.9518 - val_mDice: 0.5474

Epoch 00106: val_mDice did not improve from 0.55166
Epoch 107/300
 - 15s - loss: 1.1332 - acc: 0.9515 - mDice: 0.6527 - val_loss: 2.1778 - val_acc: 0.9518 - val_mDice: 0.5482

Epoch 00107: val_mDice did not improve from 0.55166
Epoch 108/300
 - 15s - loss: 1.1328 - acc: 0.9515 - mDice: 0.6530 - val_loss: 2.1599 - val_acc: 0.9517 - val_mDice: 0.5449

Epoch 00108: val_mDice did not improve from 0.55166
Epoch 109/300
 - 15s - loss: 1.1305 - acc: 0.9516 - mDice: 0.6535 - val_loss: 2.1936 - val_acc: 0.9515 - val_mDice: 0.5435

Epoch 00109: val_mDice did not improve from 0.55166
Epoch 110/300
 - 15s - loss: 1.1287 - acc: 0.9517 - mDice: 0.6542 - val_loss: 2.3196 - val_acc: 0.9515 - val_mDice: 0.5338

Epoch 00110: val_mDice did not improve from 0.55166
Epoch 111/300
 - 15s - loss: 1.1309 - acc: 0.9516 - mDice: 0.6539 - val_loss: 2.3697 - val_acc: 0.9513 - val_mDice: 0.5288

Epoch 00111: val_mDice did not improve from 0.55166
Epoch 112/300
 - 15s - loss: 1.1280 - acc: 0.9518 - mDice: 0.6544 - val_loss: 2.2399 - val_acc: 0.9511 - val_mDice: 0.5389

Epoch 00112: val_mDice did not improve from 0.55166
Restoring model weights from the end of the best epoch
Epoch 00112: early stopping
{'val_loss': [20.950586350936465, 8.31693270219771, 5.944939091218917, 4.996476783432774, 4.59789351511268, 4.178099753470394, 3.8162885804416082, 3.368594795631963, 2.8208283099382285, 2.5388621058543968, 2.636090941935278, 2.2840737883605104, 2.149724478162201, 2.176915947951418, 2.695547783174994, 2.088936731136045, 2.0383447834899306, 1.9501876631262582, 1.8987660308123966, 1.9220634492415956, 1.8389782639189138, 1.8824129151232416, 1.8374619330773807, 1.93927329279191, 1.8464048394943748, 1.901345759130723, 1.9398258171933989, 1.9287891168168136, 1.880004363353026, 1.9024915994878588, 1.8888405454891355, 1.9413911164139903, 1.8506194173290742, 1.8875040714967184, 1.9897423296667345, 1.9647497278352024, 1.976380912951251, 1.9339980839351036, 1.9647794342573794, 1.924101997354177, 1.972951139151717, 2.053499937057495, 1.9300748982243032, 2.0488462288286433, 1.9800343180501927, 2.047285928406529, 2.0233704431096933, 1.978644869181031, 2.1236273696302703, 1.9456401150985803, 2.0591624582279993, 2.265763172224247, 2.0563243967194795, 2.0542804835228945, 2.014854580330449, 2.177766620113863, 2.096188156298419, 2.0642281364462227, 1.9770686360044851, 2.0003303535823718, 2.059841471677386, 2.0224531096453107, 2.088282281460043, 2.1619015059657603, 2.113225859636701, 2.195126970387038, 2.0740522698983135, 2.150037604337298, 2.1091722309922374, 2.1883516524757085, 2.039786147005731, 2.109787797128688, 2.13203554846055, 2.1149747704660427, 2.191322333319893, 2.1421608059099935, 2.15540098611203, 2.153443930535343, 2.056415652429591, 2.1673222120913715, 2.0964850740059795, 2.069592258783692, 2.203718763489963, 2.191252673804427, 2.1187968533798305, 2.0834914825482076, 2.20758222201683, 2.18303979575301, 2.214246163821087, 2.26707792548494, 2.066740634055111, 2.3296399556058747, 2.167778281526193, 2.2150303198638577, 2.2422823373165874, 2.2388255356410363, 2.324885750616063, 2.3499642151028084, 2.105897405294067, 2.192018487599975, 2.2423538802056338, 2.2268277240199086, 2.3198470989419095, 2.113365491675265, 2.1741762201213306, 2.179951824955434, 2.1778356323029078, 2.1598932623197244, 2.193641621307288, 2.3196220824172378, 2.369738115278702, 2.2398609622230743], 'val_acc': [0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9141991215045225, 0.9184407248843316, 0.9228537868521067, 0.9235025194770131, 0.930589045226241, 0.9335600627201229, 0.9377871821712516, 0.9355145389807291, 0.9437105452548192, 0.9457724467336133, 0.9398284387988085, 0.9440535123787779, 0.9469335685229169, 0.9470823303947236, 0.9486793602645064, 0.9486917823386591, 0.9456009888116208, 0.949900437999704, 0.949175238609314, 0.9498239622435756, 0.9496380470318502, 0.9487847422754299, 0.9485698901741199, 0.9508590385234555, 0.9500016716605458, 0.9494252338089757, 0.9483488325300163, 0.9499892525832746, 0.9497062114363942, 0.9507144329934146, 0.9497558018348736, 0.9490884615722315, 0.9493549602657723, 0.9500016646678221, 0.9499169674665568, 0.9489335251254076, 0.9471794226982074, 0.9499871993864049, 0.9503156795848016, 0.949916950151241, 0.950966498705262, 0.9494314370208612, 0.9515243375767543, 0.9510615404757707, 0.9505181522342746, 0.9515491101328887, 0.9513962392034477, 0.9518218346814203, 0.9506524668059535, 0.9497082916051982, 0.9505863429447792, 0.9518445547066587, 0.9505346863629431, 0.9490120001345373, 0.9508921077797533, 0.9492599268199345, 0.9490016915278727, 0.9505739578321659, 0.9502413073065561, 0.949425235140923, 0.9502950387294066, 0.9501813953149252, 0.9513652231440198, 0.9510697872279077, 0.9493425824788696, 0.949976859811964, 0.9509685692174474, 0.9510966682567277, 0.9512970624023309, 0.9498694329288419, 0.9506772586753248, 0.9512702063475241, 0.9516710142849544, 0.9522474514705509, 0.9512454284636002, 0.9508817692042729, 0.9510263890527481, 0.95103259792541, 0.9505160877158522, 0.9518011881652491, 0.9516565649869055, 0.9514334215132217, 0.9518672710690419, 0.9516668852481096, 0.9511978945918589, 0.9502578401032773, 0.9517701827614001, 0.9511297122060254, 0.951243359283362, 0.9506442057353824, 0.9508280690821855, 0.9525635528830843, 0.9522371069013074, 0.9520222491392211, 0.949076062141184, 0.9509417035060221, 0.9510161034221756, 0.951328076130851, 0.9518218400092099, 0.9518363046246534, 0.9516772284854058, 0.9515429378887794, 0.9514726906515366, 0.9513073993128771, 0.9510511845849746], 'val_mDice': [0.004323722968949785, 0.009800944290014619, 0.012856567299316049, 0.02333644982531417, 0.040924774992732366, 0.05481168163638541, 0.0724542231699608, 0.11296893989240657, 0.17077589143254904, 0.21811248470285086, 0.23983964723581708, 0.29833602355845146, 0.3328619044919254, 0.34205453971910743, 0.31916944208091863, 0.39055011492201736, 0.406105165541505, 0.4195792963051929, 0.4389313519333994, 0.4449590870787978, 0.45542802224612106, 0.46347700284180027, 0.4687813683595071, 0.46036115105591674, 0.48216769648663826, 0.47814017467658615, 0.48370072362143235, 0.48220036632521857, 0.5016066809939272, 0.494700834071836, 0.49789049239131994, 0.5003231067897221, 0.5061783427632721, 0.5021657472549204, 0.4989935219620859, 0.4979348310878157, 0.5047988373807023, 0.5123329597145485, 0.5044141471718943, 0.5216342831790114, 0.5146604191990538, 0.511409287346142, 0.5157629111625629, 0.5047663443581352, 0.5190685105057402, 0.5155288666986221, 0.5232227627791506, 0.5287719122524368, 0.512356024880649, 0.5306585817363675, 0.5214259666104556, 0.5087889942376973, 0.5258962217322941, 0.5258220367591474, 0.5341035054382666, 0.5178881633881084, 0.5230584527527153, 0.5331558223876207, 0.5363881128460335, 0.5443587689426358, 0.5295746785968376, 0.5390433286155403, 0.5296382066590826, 0.5230368162666619, 0.5323588083909211, 0.5269245032491631, 0.5354427122536984, 0.5333229643006564, 0.5329176468889141, 0.5341215340118834, 0.5470657395250971, 0.5289144209643316, 0.5376801297651322, 0.5422728383008328, 0.525759625701265, 0.5362624336221364, 0.5251657891206901, 0.5282775110372618, 0.5449806478436433, 0.5404800090043904, 0.5449138087267317, 0.5516621666913591, 0.5333140435831507, 0.535807401441329, 0.5387373596596319, 0.5474902900903584, 0.5378802302163407, 0.5370309900638112, 0.5397058858551793, 0.5315377932021071, 0.5487362489354011, 0.5336650040562593, 0.534124121985622, 0.5399151134757356, 0.5303701755054836, 0.5349124996355792, 0.5281694876082117, 0.5308112392212425, 0.5460753410888117, 0.5384286935103006, 0.5421581481422126, 0.5428302817504499, 0.526366897301967, 0.5469357641049604, 0.546066762348793, 0.5474398152788258, 0.5481517308251151, 0.5448857039712661, 0.5434908510586403, 0.533819444352688, 0.5288086576834737, 0.538887286319413], 'loss': [112.35041333081111, 21.733176371778683, 11.14504240925331, 8.217404632849876, 6.837267811239548, 5.957100448729938, 5.273159007076656, 4.676852666983047, 4.153672646141333, 3.6723007487482016, 3.322660232777105, 3.0371775737054714, 2.8032415372848405, 2.6205261509426667, 2.4810541510796225, 2.3535664285698537, 2.2558065570522046, 2.1740705897349573, 2.0985846374232384, 2.02927158697064, 1.9741762301013022, 1.9226885316912226, 1.8831675293255876, 1.8417958458760264, 1.7947349109315163, 1.7731351291982056, 1.733169919141698, 1.6990901007773562, 1.6687625201853082, 1.641078379443642, 1.6233830530330133, 1.5932562313764926, 1.5801552740792992, 1.5641409101182875, 1.5451536576613238, 1.5225486391854504, 1.5094159481007448, 1.493135995415845, 1.478583154433057, 1.4625921416344982, 1.4529905673803234, 1.4352546025741777, 1.4302430359712517, 1.4144573415405421, 1.403007298732309, 1.3967929368321397, 1.3862708138326214, 1.3776085929350943, 1.36432088416397, 1.3602567298091202, 1.3508397291723495, 1.3419450231480394, 1.329094906117777, 1.327030825599347, 1.3159854470706376, 1.3075501132872323, 1.3063013430715846, 1.2985043210808238, 1.2897494013555806, 1.2878310489476645, 1.279594142234025, 1.2746690032165442, 1.2702987737875346, 1.267604232629746, 1.2603213492318128, 1.2529972470031963, 1.2512895700932978, 1.2434757897734519, 1.233526531303184, 1.23852486782075, 1.2299442652179347, 1.2300383030092474, 1.2227317173816912, 1.2192234569376028, 1.218464557586052, 1.2100400517211618, 1.2051050179366043, 1.2071182454867884, 1.2030511147497682, 1.195858189618949, 1.1971418502952633, 1.192126117856484, 1.1847568252566956, 1.185970974467781, 1.1849299763212613, 1.1831468109906378, 1.1750115260021676, 1.1777144570533995, 1.1734159947460978, 1.1674711988302209, 1.1719618638800182, 1.1619568227987964, 1.1621999275216246, 1.1599713297270982, 1.1587461794320615, 1.1518980988970016, 1.1567209405055279, 1.1531422027094511, 1.1455925441886805, 1.1501797148847779, 1.141921191188636, 1.1415361899071599, 1.1390188567733577, 1.1400301296313236, 1.1357656323139096, 1.1367170150019907, 1.1332067130704142, 1.1328271003137207, 1.1305201285954467, 1.1287044881345998, 1.1308889086810952, 1.1280119758592142], 'acc': [0.7597258410600009, 0.884650349045662, 0.8859056067534258, 0.8861962084247944, 0.8862334117151334, 0.8862378230464205, 0.8861870436677226, 0.8866524198308346, 0.8899300446256327, 0.8943884190076395, 0.8979768720134645, 0.9017999336341778, 0.906192464793042, 0.9129330782964075, 0.9184887732343278, 0.9221859739877373, 0.9248028283518882, 0.926790455744473, 0.9286632362056626, 0.9303981024652426, 0.9315519443550866, 0.9326437866594931, 0.9337518233520833, 0.9345570293447237, 0.9357043029007507, 0.9362175573087792, 0.9373466174013376, 0.938385552907716, 0.9391822365658533, 0.9398791638570351, 0.9402967195002889, 0.9409695161686803, 0.9414492911369503, 0.9418098079236944, 0.9423143989762854, 0.9430172648131734, 0.9433348219863834, 0.9438591308209652, 0.9443656732674927, 0.9448429145497971, 0.9451663914988174, 0.9457016758497787, 0.9457327327913438, 0.946119736345445, 0.9463341256468575, 0.9463048418988382, 0.94645600350787, 0.9465925653777697, 0.9466592752980898, 0.946479891654252, 0.9465998359627924, 0.9468071949676091, 0.946960070681698, 0.9468647993459505, 0.9472067688656856, 0.9473880620786005, 0.9473263519105423, 0.9475196709197096, 0.9476672498289455, 0.947679474257571, 0.947823848150296, 0.9478873372448016, 0.94797187327144, 0.9480875048474557, 0.9480672648617738, 0.9483522700624563, 0.9482877740289422, 0.9484710848400921, 0.9487366749274108, 0.948514146061675, 0.9487513575436509, 0.9487993348450529, 0.9489433478896158, 0.9490797479852184, 0.949140355056696, 0.9492591121644601, 0.9493978682975027, 0.9494424985323938, 0.9495489714315895, 0.9496928594690361, 0.9497342733741878, 0.9496642616370243, 0.9500456439644337, 0.9500216769306016, 0.9500307198489104, 0.9500999896460502, 0.9503004164823253, 0.9502817261167734, 0.9503712349029922, 0.9505582744276255, 0.9503879733422205, 0.9506283864483621, 0.9507339902156825, 0.9506664932192908, 0.9507828736466252, 0.9508787279496246, 0.9508796745892879, 0.950969506729325, 0.9511937437432244, 0.9511373869494074, 0.951266758137815, 0.9513177607977652, 0.9513588275832033, 0.9513343549827807, 0.9515186107163206, 0.9514342366717401, 0.9515435059218227, 0.9515249544509025, 0.9516432083034156, 0.9517104801779238, 0.9516462099804713, 0.9517551148731811], 'mDice': [0.013887073294936809, 0.015983435926847023, 0.019714947065280566, 0.025195665600609235, 0.03125636285020469, 0.03979302691309438, 0.05305315184149112, 0.07853206051179289, 0.1096315565150026, 0.15190322591659064, 0.19218010852590872, 0.2299063273143888, 0.2656138999293582, 0.29588212458261914, 0.3225881381885632, 0.34658622920243715, 0.36587260505172864, 0.382786723755328, 0.39956485399116415, 0.41475101854343976, 0.42811700311999623, 0.4405370999311649, 0.45046987321783105, 0.4612631916977093, 0.472337801548905, 0.47810790388778, 0.48762739081085654, 0.49647069486370077, 0.5052200452083272, 0.512431928696738, 0.5174060350383498, 0.5246137355733597, 0.5290943228479733, 0.5334268727532906, 0.5382121245583563, 0.5449534878382255, 0.5477749646005401, 0.5527985343635229, 0.556147334706932, 0.5606712059390083, 0.5635243337188539, 0.5677356078768729, 0.5696156712751179, 0.5735859468341503, 0.5764188897953165, 0.5785434813347785, 0.5817550496528899, 0.5845590041429218, 0.5879470230668116, 0.5896304264469062, 0.5921919699828289, 0.5943890860524678, 0.5979001605882733, 0.5982038424390443, 0.6010211501711822, 0.6038879442335727, 0.6039024948080435, 0.6057166811181247, 0.608196370456611, 0.6090001988388876, 0.6112154951529801, 0.6124996133219682, 0.6137634724367224, 0.6149053913946667, 0.616607045157985, 0.6186041458844684, 0.6191874036575467, 0.6211606795754959, 0.6240550577559377, 0.6224134922073036, 0.6252020246342022, 0.625001632620596, 0.6269758549282464, 0.6280676161312511, 0.6286553754427875, 0.6311594569457785, 0.6324463303322483, 0.6325193338323786, 0.6329067106613386, 0.6346829216254705, 0.6348164955013154, 0.6354691858326945, 0.6380774487493917, 0.6375408612151504, 0.638004477543091, 0.6383890618322774, 0.6406132170859391, 0.6398226031647561, 0.6409608835319854, 0.6433963681298347, 0.6417163285572319, 0.6445292783169164, 0.6448084704756873, 0.645212335752367, 0.6458790008457299, 0.6474120686967209, 0.6463130375741776, 0.6467456487254714, 0.6494441284839121, 0.6481053700868318, 0.6505754716054435, 0.6509579650379657, 0.6511591348011284, 0.6512264021925467, 0.6524172084444873, 0.65154021575517, 0.652684396933538, 0.6529668196406826, 0.6535325593129268, 0.65421232959166, 0.6539169993641316, 0.6544099724258344]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.66s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.33s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  2.09s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:59,  1.48s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:22,  1.56s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:21,  1.57s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:46,  1.66s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<07:52,  1.69s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<08:25,  1.81s/it]predicting train subjects:   2%|▏         | 7/285 [00:12<08:53,  1.92s/it]predicting train subjects:   3%|▎         | 8/285 [00:14<09:01,  1.95s/it]predicting train subjects:   3%|▎         | 9/285 [00:16<08:47,  1.91s/it]predicting train subjects:   4%|▎         | 10/285 [00:18<09:01,  1.97s/it]predicting train subjects:   4%|▍         | 11/285 [00:20<09:21,  2.05s/it]predicting train subjects:   4%|▍         | 12/285 [00:23<09:33,  2.10s/it]predicting train subjects:   5%|▍         | 13/285 [00:25<09:40,  2.13s/it]predicting train subjects:   5%|▍         | 14/285 [00:27<09:45,  2.16s/it]predicting train subjects:   5%|▌         | 15/285 [00:29<09:57,  2.21s/it]predicting train subjects:   6%|▌         | 16/285 [00:32<09:52,  2.20s/it]predicting train subjects:   6%|▌         | 17/285 [00:34<09:53,  2.22s/it]predicting train subjects:   6%|▋         | 18/285 [00:36<09:50,  2.21s/it]predicting train subjects:   7%|▋         | 19/285 [00:38<09:48,  2.21s/it]predicting train subjects:   7%|▋         | 20/285 [00:40<09:45,  2.21s/it]predicting train subjects:   7%|▋         | 21/285 [00:43<09:36,  2.19s/it]predicting train subjects:   8%|▊         | 22/285 [00:45<09:31,  2.17s/it]predicting train subjects:   8%|▊         | 23/285 [00:47<09:28,  2.17s/it]predicting train subjects:   8%|▊         | 24/285 [00:49<09:26,  2.17s/it]predicting train subjects:   9%|▉         | 25/285 [00:51<09:26,  2.18s/it]predicting train subjects:   9%|▉         | 26/285 [00:53<09:20,  2.16s/it]predicting train subjects:   9%|▉         | 27/285 [00:56<09:18,  2.17s/it]predicting train subjects:  10%|▉         | 28/285 [00:57<08:58,  2.09s/it]predicting train subjects:  10%|█         | 29/285 [00:59<08:48,  2.07s/it]predicting train subjects:  11%|█         | 30/285 [01:01<08:40,  2.04s/it]predicting train subjects:  11%|█         | 31/285 [01:03<08:33,  2.02s/it]predicting train subjects:  11%|█         | 32/285 [01:05<08:28,  2.01s/it]predicting train subjects:  12%|█▏        | 33/285 [01:07<08:30,  2.02s/it]predicting train subjects:  12%|█▏        | 34/285 [01:09<08:22,  2.00s/it]predicting train subjects:  12%|█▏        | 35/285 [01:11<08:19,  2.00s/it]predicting train subjects:  13%|█▎        | 36/285 [01:13<08:14,  1.99s/it]predicting train subjects:  13%|█▎        | 37/285 [01:15<08:15,  2.00s/it]predicting train subjects:  13%|█▎        | 38/285 [01:17<08:13,  2.00s/it]predicting train subjects:  14%|█▎        | 39/285 [01:19<08:03,  1.97s/it]predicting train subjects:  14%|█▍        | 40/285 [01:21<08:04,  1.98s/it]predicting train subjects:  14%|█▍        | 41/285 [01:23<08:01,  1.97s/it]predicting train subjects:  15%|█▍        | 42/285 [01:25<07:57,  1.96s/it]predicting train subjects:  15%|█▌        | 43/285 [01:27<08:05,  2.01s/it]predicting train subjects:  15%|█▌        | 44/285 [01:29<07:57,  1.98s/it]predicting train subjects:  16%|█▌        | 45/285 [01:31<07:52,  1.97s/it]predicting train subjects:  16%|█▌        | 46/285 [01:33<07:40,  1.93s/it]predicting train subjects:  16%|█▋        | 47/285 [01:35<07:27,  1.88s/it]predicting train subjects:  17%|█▋        | 48/285 [01:36<07:12,  1.83s/it]predicting train subjects:  17%|█▋        | 49/285 [01:38<07:02,  1.79s/it]predicting train subjects:  18%|█▊        | 50/285 [01:40<06:55,  1.77s/it]predicting train subjects:  18%|█▊        | 51/285 [01:42<06:50,  1.75s/it]predicting train subjects:  18%|█▊        | 52/285 [01:43<06:48,  1.76s/it]predicting train subjects:  19%|█▊        | 53/285 [01:45<06:44,  1.74s/it]predicting train subjects:  19%|█▉        | 54/285 [01:47<06:39,  1.73s/it]predicting train subjects:  19%|█▉        | 55/285 [01:49<06:38,  1.73s/it]predicting train subjects:  20%|█▉        | 56/285 [01:50<06:36,  1.73s/it]predicting train subjects:  20%|██        | 57/285 [01:52<06:37,  1.74s/it]predicting train subjects:  20%|██        | 58/285 [01:54<06:44,  1.78s/it]predicting train subjects:  21%|██        | 59/285 [01:56<06:38,  1.76s/it]predicting train subjects:  21%|██        | 60/285 [01:57<06:30,  1.74s/it]predicting train subjects:  21%|██▏       | 61/285 [01:59<06:32,  1.75s/it]predicting train subjects:  22%|██▏       | 62/285 [02:01<06:30,  1.75s/it]predicting train subjects:  22%|██▏       | 63/285 [02:03<06:23,  1.73s/it]predicting train subjects:  22%|██▏       | 64/285 [02:04<06:31,  1.77s/it]predicting train subjects:  23%|██▎       | 65/285 [02:06<06:41,  1.83s/it]predicting train subjects:  23%|██▎       | 66/285 [02:08<06:47,  1.86s/it]predicting train subjects:  24%|██▎       | 67/285 [02:10<06:43,  1.85s/it]predicting train subjects:  24%|██▍       | 68/285 [02:12<06:39,  1.84s/it]predicting train subjects:  24%|██▍       | 69/285 [02:14<06:31,  1.81s/it]predicting train subjects:  25%|██▍       | 70/285 [02:15<06:25,  1.79s/it]predicting train subjects:  25%|██▍       | 71/285 [02:17<06:22,  1.79s/it]predicting train subjects:  25%|██▌       | 72/285 [02:19<06:19,  1.78s/it]predicting train subjects:  26%|██▌       | 73/285 [02:21<06:14,  1.77s/it]predicting train subjects:  26%|██▌       | 74/285 [02:22<06:14,  1.78s/it]predicting train subjects:  26%|██▋       | 75/285 [02:24<06:12,  1.78s/it]predicting train subjects:  27%|██▋       | 76/285 [02:26<06:12,  1.78s/it]predicting train subjects:  27%|██▋       | 77/285 [02:28<06:10,  1.78s/it]predicting train subjects:  27%|██▋       | 78/285 [02:30<06:19,  1.83s/it]predicting train subjects:  28%|██▊       | 79/285 [02:32<06:23,  1.86s/it]predicting train subjects:  28%|██▊       | 80/285 [02:34<06:17,  1.84s/it]predicting train subjects:  28%|██▊       | 81/285 [02:35<06:15,  1.84s/it]predicting train subjects:  29%|██▉       | 82/285 [02:37<06:15,  1.85s/it]predicting train subjects:  29%|██▉       | 83/285 [02:39<06:16,  1.86s/it]predicting train subjects:  29%|██▉       | 84/285 [02:41<06:10,  1.84s/it]predicting train subjects:  30%|██▉       | 85/285 [02:43<06:14,  1.87s/it]predicting train subjects:  30%|███       | 86/285 [02:45<06:18,  1.90s/it]predicting train subjects:  31%|███       | 87/285 [02:47<06:25,  1.95s/it]predicting train subjects:  31%|███       | 88/285 [02:49<06:26,  1.96s/it]predicting train subjects:  31%|███       | 89/285 [02:51<06:34,  2.01s/it]predicting train subjects:  32%|███▏      | 90/285 [02:53<06:31,  2.01s/it]predicting train subjects:  32%|███▏      | 91/285 [02:55<06:36,  2.04s/it]predicting train subjects:  32%|███▏      | 92/285 [02:57<06:29,  2.02s/it]predicting train subjects:  33%|███▎      | 93/285 [02:59<06:26,  2.01s/it]predicting train subjects:  33%|███▎      | 94/285 [03:01<06:21,  2.00s/it]predicting train subjects:  33%|███▎      | 95/285 [03:03<06:15,  1.97s/it]predicting train subjects:  34%|███▎      | 96/285 [03:05<06:16,  1.99s/it]predicting train subjects:  34%|███▍      | 97/285 [03:07<06:15,  2.00s/it]predicting train subjects:  34%|███▍      | 98/285 [03:09<06:14,  2.00s/it]predicting train subjects:  35%|███▍      | 99/285 [03:11<06:09,  1.98s/it]predicting train subjects:  35%|███▌      | 100/285 [03:13<06:08,  1.99s/it]predicting train subjects:  35%|███▌      | 101/285 [03:15<06:02,  1.97s/it]predicting train subjects:  36%|███▌      | 102/285 [03:17<05:58,  1.96s/it]predicting train subjects:  36%|███▌      | 103/285 [03:19<05:53,  1.95s/it]predicting train subjects:  36%|███▋      | 104/285 [03:21<05:55,  1.96s/it]predicting train subjects:  37%|███▋      | 105/285 [03:23<05:48,  1.94s/it]predicting train subjects:  37%|███▋      | 106/285 [03:25<05:53,  1.98s/it]predicting train subjects:  38%|███▊      | 107/285 [03:27<05:55,  2.00s/it]predicting train subjects:  38%|███▊      | 108/285 [03:29<05:54,  2.00s/it]predicting train subjects:  38%|███▊      | 109/285 [03:31<05:50,  1.99s/it]predicting train subjects:  39%|███▊      | 110/285 [03:33<05:44,  1.97s/it]predicting train subjects:  39%|███▉      | 111/285 [03:35<05:44,  1.98s/it]predicting train subjects:  39%|███▉      | 112/285 [03:37<05:37,  1.95s/it]predicting train subjects:  40%|███▉      | 113/285 [03:39<05:40,  1.98s/it]predicting train subjects:  40%|████      | 114/285 [03:41<05:38,  1.98s/it]predicting train subjects:  40%|████      | 115/285 [03:43<05:36,  1.98s/it]predicting train subjects:  41%|████      | 116/285 [03:44<05:31,  1.96s/it]predicting train subjects:  41%|████      | 117/285 [03:46<05:25,  1.94s/it]predicting train subjects:  41%|████▏     | 118/285 [03:48<05:28,  1.97s/it]predicting train subjects:  42%|████▏     | 119/285 [03:50<05:29,  1.98s/it]predicting train subjects:  42%|████▏     | 120/285 [03:52<05:29,  2.00s/it]predicting train subjects:  42%|████▏     | 121/285 [03:54<05:17,  1.93s/it]predicting train subjects:  43%|████▎     | 122/285 [03:56<05:01,  1.85s/it]predicting train subjects:  43%|████▎     | 123/285 [03:57<04:48,  1.78s/it]predicting train subjects:  44%|████▎     | 124/285 [03:59<04:47,  1.79s/it]predicting train subjects:  44%|████▍     | 125/285 [04:01<04:47,  1.79s/it]predicting train subjects:  44%|████▍     | 126/285 [04:03<04:47,  1.81s/it]predicting train subjects:  45%|████▍     | 127/285 [04:05<04:45,  1.81s/it]predicting train subjects:  45%|████▍     | 128/285 [04:07<04:47,  1.83s/it]predicting train subjects:  45%|████▌     | 129/285 [04:08<04:45,  1.83s/it]predicting train subjects:  46%|████▌     | 130/285 [04:10<04:41,  1.82s/it]predicting train subjects:  46%|████▌     | 131/285 [04:12<04:38,  1.81s/it]predicting train subjects:  46%|████▋     | 132/285 [04:14<04:36,  1.81s/it]predicting train subjects:  47%|████▋     | 133/285 [04:16<04:36,  1.82s/it]predicting train subjects:  47%|████▋     | 134/285 [04:17<04:32,  1.80s/it]predicting train subjects:  47%|████▋     | 135/285 [04:19<04:29,  1.80s/it]predicting train subjects:  48%|████▊     | 136/285 [04:21<04:23,  1.77s/it]predicting train subjects:  48%|████▊     | 137/285 [04:23<04:19,  1.75s/it]predicting train subjects:  48%|████▊     | 138/285 [04:24<04:18,  1.76s/it]predicting train subjects:  49%|████▉     | 139/285 [04:26<04:19,  1.78s/it]predicting train subjects:  49%|████▉     | 140/285 [04:28<04:17,  1.77s/it]predicting train subjects:  49%|████▉     | 141/285 [04:30<04:17,  1.78s/it]predicting train subjects:  50%|████▉     | 142/285 [04:31<04:11,  1.76s/it]predicting train subjects:  50%|█████     | 143/285 [04:33<04:02,  1.71s/it]predicting train subjects:  51%|█████     | 144/285 [04:35<04:00,  1.70s/it]predicting train subjects:  51%|█████     | 145/285 [04:37<03:58,  1.71s/it]predicting train subjects:  51%|█████     | 146/285 [04:38<03:54,  1.69s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:40<03:53,  1.69s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:41<03:50,  1.68s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:43<03:47,  1.67s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:45<03:46,  1.67s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:46<03:42,  1.66s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:48<03:44,  1.69s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:50<03:43,  1.69s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:52<03:38,  1.67s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:53<03:32,  1.63s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:55<03:31,  1.64s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:56<03:33,  1.67s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:58<03:34,  1.69s/it]predicting train subjects:  56%|█████▌    | 159/285 [05:00<03:32,  1.69s/it]predicting train subjects:  56%|█████▌    | 160/285 [05:01<03:26,  1.65s/it]predicting train subjects:  56%|█████▋    | 161/285 [05:03<03:26,  1.66s/it]predicting train subjects:  57%|█████▋    | 162/285 [05:05<03:24,  1.66s/it]predicting train subjects:  57%|█████▋    | 163/285 [05:06<03:18,  1.63s/it]predicting train subjects:  58%|█████▊    | 164/285 [05:08<03:19,  1.65s/it]predicting train subjects:  58%|█████▊    | 165/285 [05:10<03:16,  1.64s/it]predicting train subjects:  58%|█████▊    | 166/285 [05:11<03:15,  1.64s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:13<03:11,  1.62s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:14<03:08,  1.61s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:16<03:08,  1.62s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:18<03:02,  1.58s/it]predicting train subjects:  60%|██████    | 171/285 [05:19<02:59,  1.58s/it]predicting train subjects:  60%|██████    | 172/285 [05:21<03:04,  1.63s/it]predicting train subjects:  61%|██████    | 173/285 [05:23<03:04,  1.65s/it]predicting train subjects:  61%|██████    | 174/285 [05:24<03:05,  1.68s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:26<03:01,  1.65s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:28<02:58,  1.64s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:29<02:58,  1.65s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:31<02:50,  1.59s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:32<02:46,  1.57s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:34<02:44,  1.57s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:35<02:42,  1.56s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:37<02:39,  1.55s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:38<02:37,  1.54s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:40<02:36,  1.55s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:42<02:36,  1.57s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:43<02:36,  1.58s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:45<02:34,  1.58s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:46<02:31,  1.57s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:48<02:31,  1.58s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:49<02:30,  1.58s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:51<02:30,  1.60s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:53<02:26,  1.58s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:54<02:26,  1.59s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:56<02:23,  1.57s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:57<02:21,  1.57s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:59<02:27,  1.66s/it]predicting train subjects:  69%|██████▉   | 197/285 [06:01<02:31,  1.73s/it]predicting train subjects:  69%|██████▉   | 198/285 [06:03<02:35,  1.78s/it]predicting train subjects:  70%|██████▉   | 199/285 [06:05<02:34,  1.79s/it]predicting train subjects:  70%|███████   | 200/285 [06:07<02:34,  1.82s/it]predicting train subjects:  71%|███████   | 201/285 [06:09<02:33,  1.83s/it]predicting train subjects:  71%|███████   | 202/285 [06:10<02:30,  1.81s/it]predicting train subjects:  71%|███████   | 203/285 [06:12<02:28,  1.81s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:14<02:27,  1.82s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:16<02:24,  1.80s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:18<02:23,  1.81s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:19<02:21,  1.81s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:21<02:19,  1.82s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:23<02:17,  1.81s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:25<02:14,  1.79s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:27<02:14,  1.81s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:28<02:13,  1.82s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:30<02:11,  1.82s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:32<02:05,  1.76s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:33<01:59,  1.70s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:35<01:54,  1.66s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:37<01:51,  1.63s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:38<01:47,  1.61s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:40<01:45,  1.60s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:41<01:42,  1.57s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:43<01:39,  1.56s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:44<01:40,  1.59s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:46<01:39,  1.61s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:48<01:38,  1.61s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:49<01:36,  1.61s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:51<01:34,  1.60s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:52<01:31,  1.57s/it]predicting train subjects:  80%|████████  | 228/285 [06:54<01:31,  1.61s/it]predicting train subjects:  80%|████████  | 229/285 [06:56<01:29,  1.60s/it]predicting train subjects:  81%|████████  | 230/285 [06:57<01:27,  1.59s/it]predicting train subjects:  81%|████████  | 231/285 [06:59<01:27,  1.61s/it]predicting train subjects:  81%|████████▏ | 232/285 [07:01<01:31,  1.73s/it]predicting train subjects:  82%|████████▏ | 233/285 [07:03<01:32,  1.78s/it]predicting train subjects:  82%|████████▏ | 234/285 [07:05<01:32,  1.81s/it]predicting train subjects:  82%|████████▏ | 235/285 [07:07<01:32,  1.86s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:09<01:32,  1.88s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:10<01:30,  1.89s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:12<01:29,  1.90s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:14<01:27,  1.91s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:16<01:26,  1.92s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:18<01:24,  1.92s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:20<01:22,  1.92s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:22<01:21,  1.93s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:24<01:19,  1.95s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:26<01:19,  1.99s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:28<01:17,  1.98s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:30<01:15,  1.98s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:32<01:12,  1.95s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:34<01:09,  1.94s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:35<01:03,  1.82s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:37<00:58,  1.71s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:38<00:55,  1.67s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:40<00:52,  1.63s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:42<00:49,  1.60s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:43<00:48,  1.61s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:45<00:46,  1.59s/it]predicting train subjects:  90%|█████████ | 257/285 [07:46<00:44,  1.59s/it]predicting train subjects:  91%|█████████ | 258/285 [07:48<00:43,  1.62s/it]predicting train subjects:  91%|█████████ | 259/285 [07:50<00:41,  1.60s/it]predicting train subjects:  91%|█████████ | 260/285 [07:51<00:39,  1.58s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:53<00:37,  1.55s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:54<00:35,  1.56s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:56<00:33,  1.54s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:57<00:32,  1.55s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:59<00:31,  1.56s/it]predicting train subjects:  93%|█████████▎| 266/285 [08:00<00:29,  1.56s/it]predicting train subjects:  94%|█████████▎| 267/285 [08:02<00:28,  1.59s/it]predicting train subjects:  94%|█████████▍| 268/285 [08:04<00:28,  1.71s/it]predicting train subjects:  94%|█████████▍| 269/285 [08:06<00:29,  1.82s/it]predicting train subjects:  95%|█████████▍| 270/285 [08:08<00:28,  1.89s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:10<00:26,  1.92s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:12<00:25,  1.94s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:14<00:23,  1.95s/it]predicting train subjects:  96%|█████████▌| 274/285 [08:16<00:21,  1.98s/it]predicting train subjects:  96%|█████████▋| 275/285 [08:18<00:20,  2.05s/it]predicting train subjects:  97%|█████████▋| 276/285 [08:20<00:18,  2.03s/it]predicting train subjects:  97%|█████████▋| 277/285 [08:22<00:16,  2.01s/it]predicting train subjects:  98%|█████████▊| 278/285 [08:24<00:14,  2.00s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:26<00:11,  1.99s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:28<00:10,  2.02s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:30<00:08,  2.04s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:32<00:06,  2.05s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:35<00:04,  2.05s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:36<00:02,  2.02s/it]predicting train subjects: 100%|██████████| 285/285 [08:38<00:00,  2.01s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:25,  1.36s/it]Loading train:   1%|          | 2/285 [00:02<06:44,  1.43s/it]Loading train:   1%|          | 3/285 [00:04<06:34,  1.40s/it]Loading train:   1%|▏         | 4/285 [00:05<06:51,  1.47s/it]Loading train:   2%|▏         | 5/285 [00:07<06:27,  1.38s/it]Loading train:   2%|▏         | 6/285 [00:08<06:39,  1.43s/it]Loading train:   2%|▏         | 7/285 [00:10<06:59,  1.51s/it]Loading train:   3%|▎         | 8/285 [00:11<07:03,  1.53s/it]Loading train:   3%|▎         | 9/285 [00:13<06:59,  1.52s/it]Loading train:   4%|▎         | 10/285 [00:14<06:20,  1.38s/it]Loading train:   4%|▍         | 11/285 [00:15<05:59,  1.31s/it]Loading train:   4%|▍         | 12/285 [00:16<05:35,  1.23s/it]Loading train:   5%|▍         | 13/285 [00:17<05:24,  1.19s/it]Loading train:   5%|▍         | 14/285 [00:18<05:06,  1.13s/it]Loading train:   5%|▌         | 15/285 [00:19<04:48,  1.07s/it]Loading train:   6%|▌         | 16/285 [00:20<04:44,  1.06s/it]Loading train:   6%|▌         | 17/285 [00:21<04:39,  1.04s/it]Loading train:   6%|▋         | 18/285 [00:22<04:40,  1.05s/it]Loading train:   7%|▋         | 19/285 [00:23<04:31,  1.02s/it]Loading train:   7%|▋         | 20/285 [00:25<04:52,  1.10s/it]Loading train:   7%|▋         | 21/285 [00:25<04:30,  1.03s/it]Loading train:   8%|▊         | 22/285 [00:26<04:30,  1.03s/it]Loading train:   8%|▊         | 23/285 [00:27<04:24,  1.01s/it]Loading train:   8%|▊         | 24/285 [00:29<04:36,  1.06s/it]Loading train:   9%|▉         | 25/285 [00:30<04:27,  1.03s/it]Loading train:   9%|▉         | 26/285 [00:31<04:35,  1.06s/it]Loading train:   9%|▉         | 27/285 [00:32<04:26,  1.03s/it]Loading train:  10%|▉         | 28/285 [00:33<04:35,  1.07s/it]Loading train:  10%|█         | 29/285 [00:34<04:30,  1.06s/it]Loading train:  11%|█         | 30/285 [00:35<04:19,  1.02s/it]Loading train:  11%|█         | 31/285 [00:36<04:11,  1.01it/s]Loading train:  11%|█         | 32/285 [00:37<04:03,  1.04it/s]Loading train:  12%|█▏        | 33/285 [00:37<03:56,  1.07it/s]Loading train:  12%|█▏        | 34/285 [00:38<03:57,  1.06it/s]Loading train:  12%|█▏        | 35/285 [00:39<03:54,  1.06it/s]Loading train:  13%|█▎        | 36/285 [00:40<03:55,  1.06it/s]Loading train:  13%|█▎        | 37/285 [00:41<03:53,  1.06it/s]Loading train:  13%|█▎        | 38/285 [00:42<04:00,  1.03it/s]Loading train:  14%|█▎        | 39/285 [00:43<03:54,  1.05it/s]Loading train:  14%|█▍        | 40/285 [00:44<03:53,  1.05it/s]Loading train:  14%|█▍        | 41/285 [00:45<04:03,  1.00it/s]Loading train:  15%|█▍        | 42/285 [00:46<03:58,  1.02it/s]Loading train:  15%|█▌        | 43/285 [00:47<03:59,  1.01it/s]Loading train:  15%|█▌        | 44/285 [00:48<04:00,  1.00it/s]Loading train:  16%|█▌        | 45/285 [00:49<03:59,  1.00it/s]Loading train:  16%|█▌        | 46/285 [00:50<04:02,  1.01s/it]Loading train:  16%|█▋        | 47/285 [00:51<03:45,  1.06it/s]Loading train:  17%|█▋        | 48/285 [00:52<03:33,  1.11it/s]Loading train:  17%|█▋        | 49/285 [00:53<03:25,  1.15it/s]Loading train:  18%|█▊        | 50/285 [00:53<03:17,  1.19it/s]Loading train:  18%|█▊        | 51/285 [00:54<03:17,  1.18it/s]Loading train:  18%|█▊        | 52/285 [00:55<03:15,  1.19it/s]Loading train:  19%|█▊        | 53/285 [00:56<03:21,  1.15it/s]Loading train:  19%|█▉        | 54/285 [00:57<03:14,  1.19it/s]Loading train:  19%|█▉        | 55/285 [00:58<03:24,  1.13it/s]Loading train:  20%|█▉        | 56/285 [00:59<03:16,  1.17it/s]Loading train:  20%|██        | 57/285 [00:59<03:12,  1.19it/s]Loading train:  20%|██        | 58/285 [01:00<03:25,  1.10it/s]Loading train:  21%|██        | 59/285 [01:01<03:16,  1.15it/s]Loading train:  21%|██        | 60/285 [01:02<03:24,  1.10it/s]Loading train:  21%|██▏       | 61/285 [01:03<03:26,  1.09it/s]Loading train:  22%|██▏       | 62/285 [01:04<03:20,  1.11it/s]Loading train:  22%|██▏       | 63/285 [01:05<03:21,  1.10it/s]Loading train:  22%|██▏       | 64/285 [01:06<03:46,  1.02s/it]Loading train:  23%|██▎       | 65/285 [01:08<04:20,  1.18s/it]Loading train:  23%|██▎       | 66/285 [01:09<04:28,  1.23s/it]Loading train:  24%|██▎       | 67/285 [01:10<04:04,  1.12s/it]Loading train:  24%|██▍       | 68/285 [01:11<03:47,  1.05s/it]Loading train:  24%|██▍       | 69/285 [01:12<03:26,  1.05it/s]Loading train:  25%|██▍       | 70/285 [01:13<03:26,  1.04it/s]Loading train:  25%|██▍       | 71/285 [01:13<03:21,  1.06it/s]Loading train:  25%|██▌       | 72/285 [01:14<03:19,  1.07it/s]Loading train:  26%|██▌       | 73/285 [01:15<03:17,  1.07it/s]Loading train:  26%|██▌       | 74/285 [01:16<03:10,  1.11it/s]Loading train:  26%|██▋       | 75/285 [01:17<03:07,  1.12it/s]Loading train:  27%|██▋       | 76/285 [01:18<03:09,  1.10it/s]Loading train:  27%|██▋       | 77/285 [01:19<03:07,  1.11it/s]Loading train:  27%|██▋       | 78/285 [01:20<03:02,  1.14it/s]Loading train:  28%|██▊       | 79/285 [01:21<03:02,  1.13it/s]Loading train:  28%|██▊       | 80/285 [01:21<03:03,  1.12it/s]Loading train:  28%|██▊       | 81/285 [01:22<02:56,  1.16it/s]Loading train:  29%|██▉       | 82/285 [01:23<02:57,  1.14it/s]Loading train:  29%|██▉       | 83/285 [01:24<02:54,  1.16it/s]Loading train:  29%|██▉       | 84/285 [01:25<02:57,  1.14it/s]Loading train:  30%|██▉       | 85/285 [01:26<03:20,  1.00s/it]Loading train:  30%|███       | 86/285 [01:27<03:16,  1.01it/s]Loading train:  31%|███       | 87/285 [01:28<03:25,  1.04s/it]Loading train:  31%|███       | 88/285 [01:29<03:17,  1.00s/it]Loading train:  31%|███       | 89/285 [01:30<03:13,  1.02it/s]Loading train:  32%|███▏      | 90/285 [01:31<03:10,  1.02it/s]Loading train:  32%|███▏      | 91/285 [01:32<03:07,  1.04it/s]Loading train:  32%|███▏      | 92/285 [01:33<03:09,  1.02it/s]Loading train:  33%|███▎      | 93/285 [01:34<03:05,  1.04it/s]Loading train:  33%|███▎      | 94/285 [01:35<03:07,  1.02it/s]Loading train:  33%|███▎      | 95/285 [01:36<03:05,  1.03it/s]Loading train:  34%|███▎      | 96/285 [01:37<03:01,  1.04it/s]Loading train:  34%|███▍      | 97/285 [01:38<03:00,  1.04it/s]Loading train:  34%|███▍      | 98/285 [01:39<03:04,  1.01it/s]Loading train:  35%|███▍      | 99/285 [01:40<02:59,  1.04it/s]Loading train:  35%|███▌      | 100/285 [01:41<02:53,  1.07it/s]Loading train:  35%|███▌      | 101/285 [01:42<02:48,  1.09it/s]Loading train:  36%|███▌      | 102/285 [01:43<02:45,  1.10it/s]Loading train:  36%|███▌      | 103/285 [01:44<03:04,  1.01s/it]Loading train:  36%|███▋      | 104/285 [01:45<03:00,  1.00it/s]Loading train:  37%|███▋      | 105/285 [01:46<03:03,  1.02s/it]Loading train:  37%|███▋      | 106/285 [01:47<02:58,  1.00it/s]Loading train:  38%|███▊      | 107/285 [01:48<02:54,  1.02it/s]Loading train:  38%|███▊      | 108/285 [01:49<02:48,  1.05it/s]Loading train:  38%|███▊      | 109/285 [01:50<02:47,  1.05it/s]Loading train:  39%|███▊      | 110/285 [01:50<02:44,  1.06it/s]Loading train:  39%|███▉      | 111/285 [01:51<02:41,  1.08it/s]Loading train:  39%|███▉      | 112/285 [01:52<02:41,  1.07it/s]Loading train:  40%|███▉      | 113/285 [01:53<02:40,  1.07it/s]Loading train:  40%|████      | 114/285 [01:54<02:49,  1.01it/s]Loading train:  40%|████      | 115/285 [01:55<02:49,  1.00it/s]Loading train:  41%|████      | 116/285 [01:56<02:47,  1.01it/s]Loading train:  41%|████      | 117/285 [01:57<02:44,  1.02it/s]Loading train:  41%|████▏     | 118/285 [01:58<02:43,  1.02it/s]Loading train:  42%|████▏     | 119/285 [01:59<02:46,  1.00s/it]Loading train:  42%|████▏     | 120/285 [02:00<02:44,  1.00it/s]Loading train:  42%|████▏     | 121/285 [02:02<02:55,  1.07s/it]Loading train:  43%|████▎     | 122/285 [02:03<03:08,  1.16s/it]Loading train:  43%|████▎     | 123/285 [02:04<03:17,  1.22s/it]Loading train:  44%|████▎     | 124/285 [02:05<03:00,  1.12s/it]Loading train:  44%|████▍     | 125/285 [02:06<02:52,  1.08s/it]Loading train:  44%|████▍     | 126/285 [02:07<02:39,  1.00s/it]Loading train:  45%|████▍     | 127/285 [02:08<02:36,  1.01it/s]Loading train:  45%|████▍     | 128/285 [02:09<02:30,  1.04it/s]Loading train:  45%|████▌     | 129/285 [02:10<02:24,  1.08it/s]Loading train:  46%|████▌     | 130/285 [02:11<02:26,  1.06it/s]Loading train:  46%|████▌     | 131/285 [02:12<02:26,  1.05it/s]Loading train:  46%|████▋     | 132/285 [02:13<02:35,  1.02s/it]Loading train:  47%|████▋     | 133/285 [02:14<02:29,  1.02it/s]Loading train:  47%|████▋     | 134/285 [02:15<02:21,  1.07it/s]Loading train:  47%|████▋     | 135/285 [02:15<02:16,  1.10it/s]Loading train:  48%|████▊     | 136/285 [02:16<02:16,  1.09it/s]Loading train:  48%|████▊     | 137/285 [02:17<02:10,  1.14it/s]Loading train:  48%|████▊     | 138/285 [02:18<02:06,  1.16it/s]Loading train:  49%|████▉     | 139/285 [02:19<02:10,  1.12it/s]Loading train:  49%|████▉     | 140/285 [02:20<02:05,  1.15it/s]Loading train:  49%|████▉     | 141/285 [02:20<02:01,  1.19it/s]Loading train:  50%|████▉     | 142/285 [02:21<02:02,  1.16it/s]Loading train:  50%|█████     | 143/285 [02:22<02:02,  1.16it/s]Loading train:  51%|█████     | 144/285 [02:23<02:05,  1.12it/s]Loading train:  51%|█████     | 145/285 [02:24<02:01,  1.15it/s]Loading train:  51%|█████     | 146/285 [02:25<02:05,  1.11it/s]Loading train:  52%|█████▏    | 147/285 [02:26<02:01,  1.13it/s]Loading train:  52%|█████▏    | 148/285 [02:27<02:00,  1.14it/s]Loading train:  52%|█████▏    | 149/285 [02:28<01:56,  1.17it/s]Loading train:  53%|█████▎    | 150/285 [02:28<01:56,  1.16it/s]Loading train:  53%|█████▎    | 151/285 [02:29<01:56,  1.15it/s]Loading train:  53%|█████▎    | 152/285 [02:30<01:57,  1.13it/s]Loading train:  54%|█████▎    | 153/285 [02:31<01:56,  1.14it/s]Loading train:  54%|█████▍    | 154/285 [02:32<01:51,  1.17it/s]Loading train:  54%|█████▍    | 155/285 [02:33<01:49,  1.19it/s]Loading train:  55%|█████▍    | 156/285 [02:33<01:44,  1.23it/s]Loading train:  55%|█████▌    | 157/285 [02:34<01:44,  1.22it/s]Loading train:  55%|█████▌    | 158/285 [02:35<01:44,  1.22it/s]Loading train:  56%|█████▌    | 159/285 [02:36<01:40,  1.25it/s]Loading train:  56%|█████▌    | 160/285 [02:37<01:49,  1.14it/s]Loading train:  56%|█████▋    | 161/285 [02:38<01:48,  1.14it/s]Loading train:  57%|█████▋    | 162/285 [02:39<01:44,  1.18it/s]Loading train:  57%|█████▋    | 163/285 [02:39<01:42,  1.19it/s]Loading train:  58%|█████▊    | 164/285 [02:40<01:40,  1.21it/s]Loading train:  58%|█████▊    | 165/285 [02:41<01:37,  1.23it/s]Loading train:  58%|█████▊    | 166/285 [02:42<01:40,  1.18it/s]Loading train:  59%|█████▊    | 167/285 [02:43<01:35,  1.24it/s]Loading train:  59%|█████▉    | 168/285 [02:43<01:37,  1.20it/s]Loading train:  59%|█████▉    | 169/285 [02:44<01:33,  1.24it/s]Loading train:  60%|█████▉    | 170/285 [02:45<01:35,  1.21it/s]Loading train:  60%|██████    | 171/285 [02:46<01:39,  1.15it/s]Loading train:  60%|██████    | 172/285 [02:47<01:38,  1.15it/s]Loading train:  61%|██████    | 173/285 [02:48<01:36,  1.17it/s]Loading train:  61%|██████    | 174/285 [02:49<01:32,  1.20it/s]Loading train:  61%|██████▏   | 175/285 [02:49<01:35,  1.15it/s]Loading train:  62%|██████▏   | 176/285 [02:50<01:32,  1.17it/s]Loading train:  62%|██████▏   | 177/285 [02:51<01:34,  1.15it/s]Loading train:  62%|██████▏   | 178/285 [02:52<01:28,  1.21it/s]Loading train:  63%|██████▎   | 179/285 [02:53<01:22,  1.29it/s]Loading train:  63%|██████▎   | 180/285 [02:54<01:27,  1.20it/s]Loading train:  64%|██████▎   | 181/285 [02:54<01:26,  1.21it/s]Loading train:  64%|██████▍   | 182/285 [02:55<01:24,  1.22it/s]Loading train:  64%|██████▍   | 183/285 [02:56<01:24,  1.21it/s]Loading train:  65%|██████▍   | 184/285 [02:57<01:19,  1.27it/s]Loading train:  65%|██████▍   | 185/285 [02:58<01:20,  1.24it/s]Loading train:  65%|██████▌   | 186/285 [02:58<01:20,  1.23it/s]Loading train:  66%|██████▌   | 187/285 [02:59<01:18,  1.25it/s]Loading train:  66%|██████▌   | 188/285 [03:00<01:16,  1.27it/s]Loading train:  66%|██████▋   | 189/285 [03:01<01:16,  1.26it/s]Loading train:  67%|██████▋   | 190/285 [03:02<01:16,  1.24it/s]Loading train:  67%|██████▋   | 191/285 [03:02<01:12,  1.29it/s]Loading train:  67%|██████▋   | 192/285 [03:03<01:12,  1.28it/s]Loading train:  68%|██████▊   | 193/285 [03:04<01:10,  1.31it/s]Loading train:  68%|██████▊   | 194/285 [03:04<01:07,  1.34it/s]Loading train:  68%|██████▊   | 195/285 [03:05<01:12,  1.23it/s]Loading train:  69%|██████▉   | 196/285 [03:06<01:16,  1.16it/s]Loading train:  69%|██████▉   | 197/285 [03:07<01:16,  1.15it/s]Loading train:  69%|██████▉   | 198/285 [03:08<01:15,  1.15it/s]Loading train:  70%|██████▉   | 199/285 [03:09<01:17,  1.10it/s]Loading train:  70%|███████   | 200/285 [03:10<01:16,  1.11it/s]Loading train:  71%|███████   | 201/285 [03:11<01:16,  1.10it/s]Loading train:  71%|███████   | 202/285 [03:12<01:14,  1.11it/s]Loading train:  71%|███████   | 203/285 [03:13<01:17,  1.06it/s]Loading train:  72%|███████▏  | 204/285 [03:14<01:19,  1.02it/s]Loading train:  72%|███████▏  | 205/285 [03:15<01:15,  1.06it/s]Loading train:  72%|███████▏  | 206/285 [03:16<01:17,  1.02it/s]Loading train:  73%|███████▎  | 207/285 [03:17<01:12,  1.07it/s]Loading train:  73%|███████▎  | 208/285 [03:18<01:10,  1.09it/s]Loading train:  73%|███████▎  | 209/285 [03:18<01:07,  1.12it/s]Loading train:  74%|███████▎  | 210/285 [03:19<01:05,  1.14it/s]Loading train:  74%|███████▍  | 211/285 [03:20<01:06,  1.11it/s]Loading train:  74%|███████▍  | 212/285 [03:21<01:04,  1.14it/s]Loading train:  75%|███████▍  | 213/285 [03:22<01:08,  1.05it/s]Loading train:  75%|███████▌  | 214/285 [03:23<01:03,  1.12it/s]Loading train:  75%|███████▌  | 215/285 [03:24<01:00,  1.16it/s]Loading train:  76%|███████▌  | 216/285 [03:25<00:59,  1.16it/s]Loading train:  76%|███████▌  | 217/285 [03:26<00:59,  1.15it/s]Loading train:  76%|███████▋  | 218/285 [03:26<00:58,  1.14it/s]Loading train:  77%|███████▋  | 219/285 [03:27<00:55,  1.19it/s]Loading train:  77%|███████▋  | 220/285 [03:28<00:54,  1.19it/s]Loading train:  78%|███████▊  | 221/285 [03:29<00:54,  1.17it/s]Loading train:  78%|███████▊  | 222/285 [03:30<00:52,  1.19it/s]Loading train:  78%|███████▊  | 223/285 [03:30<00:51,  1.21it/s]Loading train:  79%|███████▊  | 224/285 [03:31<00:49,  1.23it/s]Loading train:  79%|███████▉  | 225/285 [03:32<00:48,  1.24it/s]Loading train:  79%|███████▉  | 226/285 [03:33<00:47,  1.25it/s]Loading train:  80%|███████▉  | 227/285 [03:34<00:47,  1.23it/s]Loading train:  80%|████████  | 228/285 [03:34<00:46,  1.24it/s]Loading train:  80%|████████  | 229/285 [03:35<00:44,  1.25it/s]Loading train:  81%|████████  | 230/285 [03:36<00:44,  1.23it/s]Loading train:  81%|████████  | 231/285 [03:37<00:44,  1.21it/s]Loading train:  81%|████████▏ | 232/285 [03:38<00:46,  1.15it/s]Loading train:  82%|████████▏ | 233/285 [03:39<00:46,  1.11it/s]Loading train:  82%|████████▏ | 234/285 [03:40<00:47,  1.08it/s]Loading train:  82%|████████▏ | 235/285 [03:41<00:47,  1.04it/s]Loading train:  83%|████████▎ | 236/285 [03:42<00:47,  1.03it/s]Loading train:  83%|████████▎ | 237/285 [03:43<00:45,  1.06it/s]Loading train:  84%|████████▎ | 238/285 [03:44<00:45,  1.04it/s]Loading train:  84%|████████▍ | 239/285 [03:45<00:47,  1.02s/it]Loading train:  84%|████████▍ | 240/285 [03:46<00:45,  1.01s/it]Loading train:  85%|████████▍ | 241/285 [03:47<00:43,  1.01it/s]Loading train:  85%|████████▍ | 242/285 [03:48<00:43,  1.01s/it]Loading train:  85%|████████▌ | 243/285 [03:49<00:41,  1.01it/s]Loading train:  86%|████████▌ | 244/285 [03:50<00:41,  1.01s/it]Loading train:  86%|████████▌ | 245/285 [03:51<00:39,  1.01it/s]Loading train:  86%|████████▋ | 246/285 [03:52<00:39,  1.02s/it]Loading train:  87%|████████▋ | 247/285 [03:53<00:38,  1.00s/it]Loading train:  87%|████████▋ | 248/285 [03:54<00:38,  1.04s/it]Loading train:  87%|████████▋ | 249/285 [03:55<00:37,  1.03s/it]Loading train:  88%|████████▊ | 250/285 [03:56<00:33,  1.05it/s]Loading train:  88%|████████▊ | 251/285 [03:57<00:30,  1.13it/s]Loading train:  88%|████████▊ | 252/285 [03:58<00:30,  1.09it/s]Loading train:  89%|████████▉ | 253/285 [03:58<00:27,  1.16it/s]Loading train:  89%|████████▉ | 254/285 [03:59<00:26,  1.19it/s]Loading train:  89%|████████▉ | 255/285 [04:00<00:24,  1.23it/s]Loading train:  90%|████████▉ | 256/285 [04:01<00:22,  1.31it/s]Loading train:  90%|█████████ | 257/285 [04:01<00:21,  1.31it/s]Loading train:  91%|█████████ | 258/285 [04:02<00:21,  1.25it/s]Loading train:  91%|█████████ | 259/285 [04:03<00:20,  1.26it/s]Loading train:  91%|█████████ | 260/285 [04:04<00:20,  1.23it/s]Loading train:  92%|█████████▏| 261/285 [04:05<00:18,  1.27it/s]Loading train:  92%|█████████▏| 262/285 [04:05<00:17,  1.32it/s]Loading train:  92%|█████████▏| 263/285 [04:06<00:17,  1.28it/s]Loading train:  93%|█████████▎| 264/285 [04:07<00:16,  1.28it/s]Loading train:  93%|█████████▎| 265/285 [04:08<00:15,  1.27it/s]Loading train:  93%|█████████▎| 266/285 [04:08<00:15,  1.26it/s]Loading train:  94%|█████████▎| 267/285 [04:09<00:13,  1.31it/s]Loading train:  94%|█████████▍| 268/285 [04:10<00:14,  1.21it/s]Loading train:  94%|█████████▍| 269/285 [04:11<00:13,  1.22it/s]Loading train:  95%|█████████▍| 270/285 [04:12<00:12,  1.18it/s]Loading train:  95%|█████████▌| 271/285 [04:13<00:12,  1.14it/s]Loading train:  95%|█████████▌| 272/285 [04:14<00:12,  1.04it/s]Loading train:  96%|█████████▌| 273/285 [04:15<00:11,  1.02it/s]Loading train:  96%|█████████▌| 274/285 [04:16<00:10,  1.01it/s]Loading train:  96%|█████████▋| 275/285 [04:17<00:10,  1.00s/it]Loading train:  97%|█████████▋| 276/285 [04:18<00:09,  1.02s/it]Loading train:  97%|█████████▋| 277/285 [04:19<00:08,  1.01s/it]Loading train:  98%|█████████▊| 278/285 [04:20<00:06,  1.03it/s]Loading train:  98%|█████████▊| 279/285 [04:21<00:05,  1.00it/s]Loading train:  98%|█████████▊| 280/285 [04:22<00:04,  1.02it/s]Loading train:  99%|█████████▊| 281/285 [04:23<00:03,  1.01it/s]Loading train:  99%|█████████▉| 282/285 [04:24<00:02,  1.00it/s]Loading train:  99%|█████████▉| 283/285 [04:25<00:02,  1.00s/it]Loading train: 100%|█████████▉| 284/285 [04:26<00:00,  1.03it/s]Loading train: 100%|██████████| 285/285 [04:27<00:00,  1.06it/s]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 20/285 [00:00<00:01, 196.91it/s]concatenating: train:  15%|█▍        | 42/285 [00:00<00:01, 201.76it/s]concatenating: train:  24%|██▎       | 67/285 [00:00<00:01, 213.48it/s]concatenating: train:  34%|███▎      | 96/285 [00:00<00:00, 231.21it/s]concatenating: train:  42%|████▏     | 119/285 [00:00<00:00, 226.78it/s]concatenating: train:  51%|█████     | 144/285 [00:00<00:00, 230.56it/s]concatenating: train:  58%|█████▊    | 166/285 [00:00<00:00, 196.55it/s]concatenating: train:  65%|██████▌   | 186/285 [00:00<00:00, 178.34it/s]concatenating: train:  72%|███████▏  | 204/285 [00:01<00:00, 177.24it/s]concatenating: train:  81%|████████▏ | 232/285 [00:01<00:00, 198.42it/s]concatenating: train:  91%|█████████ | 258/285 [00:01<00:00, 213.19it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 220.01it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.36s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.37s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.29s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 412.54it/s]2019-07-06 23:22:27.516949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 23:22:27.517054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 23:22:27.517069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 23:22:27.517079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 23:22:27.517514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:09,  4.17it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:07,  4.95it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.25it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:05,  5.47it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:06,  4.84it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.29it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  4.78it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:03,  6.04it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.17it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.76it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  5.20it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:02,  6.35it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.75it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  5.51it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:01,  6.08it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  5.36it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  6.53it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  7.07it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.85it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  6.33it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  5.46it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  7.61it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 80, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 80, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 80, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 80, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 80, 52, 30)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 80, 52, 30)   8130        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 80, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 80, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 80, 52, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 80, 52, 30)   8130        dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 80, 52, 30)   120         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 80, 52, 30)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 80, 52, 30)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 80, 52, 10)   2710        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 80, 52, 10)   40          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 80, 52, 10)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 80, 52, 10)   910         activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 80, 52, 10)   40          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 80, 52, 10)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 40, 26, 10)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 40, 26, 10)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 40, 26, 20)   1820        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 40, 26, 20)   80          conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 40, 26, 20)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 40, 26, 20)   3620        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 40, 26, 20)   80          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 40, 26, 20)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 20, 13, 20)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 20, 13, 20)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 20, 13, 40)   7240        dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 20, 13, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 20, 13, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 13, 40)   14440       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 20, 13, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 20, 13, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 20, 13, 40)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 40, 26, 20)   3220        dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 40, 26, 40)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 40, 26, 20)   7220        concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 40, 26, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 40, 26, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 40, 26, 20)   3620        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 40, 26, 20)   80          conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 40, 26, 20)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 40, 26, 20)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 80, 52, 10)   810         dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 80, 52, 20)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 80, 52, 10)   1810        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 80, 52, 10)   40          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 80, 52, 10)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 80, 52, 10)   910         activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 80, 52, 10)   40          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 80, 52, 10)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 80, 52, 10)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 80, 52, 30)   2730        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 80, 52, 30)   120         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 80, 52, 30)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 80, 52, 30)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 80, 52, 13)   403         dropout_9[0][0]                  
==================================================================================================
Total params: 69,303
Trainable params: 32,923
Non-trainable params: 36,380
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.48913484e-02 3.19509754e-02 7.49897764e-02 9.32025064e-03
 2.70904632e-02 7.06417031e-03 8.46180096e-02 1.12618024e-01
 8.60108482e-02 1.32459736e-02 2.94100802e-01 1.93843398e-01
 2.55960049e-04]
Train on 10843 samples, validate on 104 samples
Epoch 1/300
 - 20s - loss: 189.3553 - acc: 0.5517 - mDice: 0.0165 - val_loss: 97.3278 - val_acc: 0.9034 - val_mDice: 0.0122

Epoch 00001: val_mDice improved from -inf to 0.01221, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 10s - loss: 58.6183 - acc: 0.8672 - mDice: 0.0175 - val_loss: 22.4203 - val_acc: 0.9034 - val_mDice: 0.0108

Epoch 00002: val_mDice did not improve from 0.01221
Epoch 3/300
 - 11s - loss: 28.5394 - acc: 0.8733 - mDice: 0.0181 - val_loss: 14.1123 - val_acc: 0.9034 - val_mDice: 0.0137

Epoch 00003: val_mDice improved from 0.01221 to 0.01370, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 11s - loss: 19.3200 - acc: 0.8741 - mDice: 0.0192 - val_loss: 11.4104 - val_acc: 0.9034 - val_mDice: 0.0158

Epoch 00004: val_mDice improved from 0.01370 to 0.01581, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 10s - loss: 15.4618 - acc: 0.8745 - mDice: 0.0209 - val_loss: 10.0573 - val_acc: 0.9034 - val_mDice: 0.0184

Epoch 00005: val_mDice improved from 0.01581 to 0.01838, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 11s - loss: 13.4246 - acc: 0.8746 - mDice: 0.0227 - val_loss: 9.3409 - val_acc: 0.9034 - val_mDice: 0.0213

Epoch 00006: val_mDice improved from 0.01838 to 0.02127, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 10s - loss: 12.1522 - acc: 0.8747 - mDice: 0.0251 - val_loss: 8.7781 - val_acc: 0.9034 - val_mDice: 0.0274

Epoch 00007: val_mDice improved from 0.02127 to 0.02742, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 11s - loss: 11.2656 - acc: 0.8748 - mDice: 0.0282 - val_loss: 8.3992 - val_acc: 0.9034 - val_mDice: 0.0344

Epoch 00008: val_mDice improved from 0.02742 to 0.03437, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 10s - loss: 10.5909 - acc: 0.8748 - mDice: 0.0324 - val_loss: 8.1209 - val_acc: 0.9034 - val_mDice: 0.0426

Epoch 00009: val_mDice improved from 0.03437 to 0.04258, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 11s - loss: 10.0512 - acc: 0.8748 - mDice: 0.0375 - val_loss: 7.8548 - val_acc: 0.9034 - val_mDice: 0.0472

Epoch 00010: val_mDice improved from 0.04258 to 0.04720, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 11s - loss: 9.5704 - acc: 0.8748 - mDice: 0.0435 - val_loss: 8.1652 - val_acc: 0.9034 - val_mDice: 0.0449

Epoch 00011: val_mDice did not improve from 0.04720
Epoch 12/300
 - 11s - loss: 9.1076 - acc: 0.8749 - mDice: 0.0502 - val_loss: 7.8716 - val_acc: 0.9034 - val_mDice: 0.0546

Epoch 00012: val_mDice improved from 0.04720 to 0.05462, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 11s - loss: 8.6914 - acc: 0.8749 - mDice: 0.0579 - val_loss: 8.2919 - val_acc: 0.9034 - val_mDice: 0.0608

Epoch 00013: val_mDice improved from 0.05462 to 0.06084, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 11s - loss: 8.3179 - acc: 0.8749 - mDice: 0.0657 - val_loss: 7.6500 - val_acc: 0.9034 - val_mDice: 0.0712

Epoch 00014: val_mDice improved from 0.06084 to 0.07119, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 11s - loss: 7.9804 - acc: 0.8750 - mDice: 0.0734 - val_loss: 7.0690 - val_acc: 0.9034 - val_mDice: 0.0825

Epoch 00015: val_mDice improved from 0.07119 to 0.08252, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 11s - loss: 7.6681 - acc: 0.8750 - mDice: 0.0816 - val_loss: 6.7846 - val_acc: 0.9034 - val_mDice: 0.0916

Epoch 00016: val_mDice improved from 0.08252 to 0.09163, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 11s - loss: 7.3871 - acc: 0.8750 - mDice: 0.0898 - val_loss: 6.2114 - val_acc: 0.9035 - val_mDice: 0.1191

Epoch 00017: val_mDice improved from 0.09163 to 0.11907, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 11s - loss: 7.1338 - acc: 0.8751 - mDice: 0.0985 - val_loss: 7.4615 - val_acc: 0.9034 - val_mDice: 0.0928

Epoch 00018: val_mDice did not improve from 0.11907
Epoch 19/300
 - 11s - loss: 6.9118 - acc: 0.8752 - mDice: 0.1070 - val_loss: 5.7933 - val_acc: 0.9038 - val_mDice: 0.1358

Epoch 00019: val_mDice improved from 0.11907 to 0.13580, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 11s - loss: 6.7153 - acc: 0.8753 - mDice: 0.1159 - val_loss: 6.1367 - val_acc: 0.9037 - val_mDice: 0.1371

Epoch 00020: val_mDice improved from 0.13580 to 0.13706, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 11s - loss: 6.5474 - acc: 0.8755 - mDice: 0.1231 - val_loss: 5.7150 - val_acc: 0.9037 - val_mDice: 0.1462

Epoch 00021: val_mDice improved from 0.13706 to 0.14619, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 11s - loss: 6.4010 - acc: 0.8758 - mDice: 0.1301 - val_loss: 5.5926 - val_acc: 0.9034 - val_mDice: 0.1554

Epoch 00022: val_mDice improved from 0.14619 to 0.15542, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 11s - loss: 6.2484 - acc: 0.8760 - mDice: 0.1380 - val_loss: 5.9904 - val_acc: 0.9040 - val_mDice: 0.1512

Epoch 00023: val_mDice did not improve from 0.15542
Epoch 24/300
 - 11s - loss: 6.1123 - acc: 0.8762 - mDice: 0.1454 - val_loss: 5.7139 - val_acc: 0.9040 - val_mDice: 0.1629

Epoch 00024: val_mDice improved from 0.15542 to 0.16292, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 11s - loss: 5.9846 - acc: 0.8763 - mDice: 0.1531 - val_loss: 5.7742 - val_acc: 0.9038 - val_mDice: 0.1629

Epoch 00025: val_mDice did not improve from 0.16292
Epoch 26/300
 - 10s - loss: 5.8427 - acc: 0.8764 - mDice: 0.1605 - val_loss: 5.4985 - val_acc: 0.9041 - val_mDice: 0.1735

Epoch 00026: val_mDice improved from 0.16292 to 0.17349, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 11s - loss: 5.7128 - acc: 0.8767 - mDice: 0.1688 - val_loss: 5.8197 - val_acc: 0.9046 - val_mDice: 0.1809

Epoch 00027: val_mDice improved from 0.17349 to 0.18091, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 11s - loss: 5.5798 - acc: 0.8779 - mDice: 0.1784 - val_loss: 5.3305 - val_acc: 0.9046 - val_mDice: 0.1969

Epoch 00028: val_mDice improved from 0.18091 to 0.19685, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 11s - loss: 5.4540 - acc: 0.8807 - mDice: 0.1894 - val_loss: 5.1816 - val_acc: 0.9110 - val_mDice: 0.2090

Epoch 00029: val_mDice improved from 0.19685 to 0.20896, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 11s - loss: 5.2890 - acc: 0.8843 - mDice: 0.2025 - val_loss: 5.1461 - val_acc: 0.9093 - val_mDice: 0.2164

Epoch 00030: val_mDice improved from 0.20896 to 0.21639, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 11s - loss: 5.1688 - acc: 0.8875 - mDice: 0.2122 - val_loss: 6.6035 - val_acc: 0.9139 - val_mDice: 0.2011

Epoch 00031: val_mDice did not improve from 0.21639
Epoch 32/300
 - 12s - loss: 5.0410 - acc: 0.8899 - mDice: 0.2234 - val_loss: 5.1083 - val_acc: 0.9162 - val_mDice: 0.2343

Epoch 00032: val_mDice improved from 0.21639 to 0.23428, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 11s - loss: 4.9232 - acc: 0.8924 - mDice: 0.2335 - val_loss: 5.1286 - val_acc: 0.9160 - val_mDice: 0.2330

Epoch 00033: val_mDice did not improve from 0.23428
Epoch 34/300
 - 11s - loss: 4.8402 - acc: 0.8942 - mDice: 0.2408 - val_loss: 5.1425 - val_acc: 0.9132 - val_mDice: 0.2389

Epoch 00034: val_mDice improved from 0.23428 to 0.23891, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 11s - loss: 4.7342 - acc: 0.8967 - mDice: 0.2508 - val_loss: 4.9119 - val_acc: 0.9167 - val_mDice: 0.2522

Epoch 00035: val_mDice improved from 0.23891 to 0.25219, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 11s - loss: 4.6407 - acc: 0.8986 - mDice: 0.2602 - val_loss: 5.0852 - val_acc: 0.9222 - val_mDice: 0.2574

Epoch 00036: val_mDice improved from 0.25219 to 0.25739, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 11s - loss: 4.5664 - acc: 0.9005 - mDice: 0.2674 - val_loss: 4.7751 - val_acc: 0.9209 - val_mDice: 0.2781

Epoch 00037: val_mDice improved from 0.25739 to 0.27807, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 14s - loss: 4.4763 - acc: 0.9023 - mDice: 0.2774 - val_loss: 4.8705 - val_acc: 0.9169 - val_mDice: 0.2709

Epoch 00038: val_mDice did not improve from 0.27807
Epoch 39/300
 - 13s - loss: 4.3980 - acc: 0.9035 - mDice: 0.2862 - val_loss: 4.6769 - val_acc: 0.9206 - val_mDice: 0.2831

Epoch 00039: val_mDice improved from 0.27807 to 0.28306, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 13s - loss: 4.3208 - acc: 0.9048 - mDice: 0.2950 - val_loss: 4.8758 - val_acc: 0.9211 - val_mDice: 0.2844

Epoch 00040: val_mDice improved from 0.28306 to 0.28444, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 13s - loss: 4.2429 - acc: 0.9061 - mDice: 0.3038 - val_loss: 4.7878 - val_acc: 0.9206 - val_mDice: 0.2923

Epoch 00041: val_mDice improved from 0.28444 to 0.29233, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 13s - loss: 4.1904 - acc: 0.9075 - mDice: 0.3103 - val_loss: 4.6807 - val_acc: 0.9202 - val_mDice: 0.3059

Epoch 00042: val_mDice improved from 0.29233 to 0.30593, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 13s - loss: 4.1393 - acc: 0.9084 - mDice: 0.3165 - val_loss: 4.4727 - val_acc: 0.9197 - val_mDice: 0.3176

Epoch 00043: val_mDice improved from 0.30593 to 0.31764, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 44/300
 - 12s - loss: 4.0738 - acc: 0.9097 - mDice: 0.3254 - val_loss: 4.3717 - val_acc: 0.9251 - val_mDice: 0.3220

Epoch 00044: val_mDice improved from 0.31764 to 0.32202, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 13s - loss: 4.0262 - acc: 0.9108 - mDice: 0.3312 - val_loss: 4.5358 - val_acc: 0.9274 - val_mDice: 0.3274

Epoch 00045: val_mDice improved from 0.32202 to 0.32739, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 13s - loss: 3.9540 - acc: 0.9123 - mDice: 0.3408 - val_loss: 4.3903 - val_acc: 0.9265 - val_mDice: 0.3337

Epoch 00046: val_mDice improved from 0.32739 to 0.33372, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 12s - loss: 3.9005 - acc: 0.9133 - mDice: 0.3482 - val_loss: 4.6852 - val_acc: 0.9254 - val_mDice: 0.3256

Epoch 00047: val_mDice did not improve from 0.33372
Epoch 48/300
 - 14s - loss: 3.8580 - acc: 0.9141 - mDice: 0.3538 - val_loss: 4.6151 - val_acc: 0.9301 - val_mDice: 0.3376

Epoch 00048: val_mDice improved from 0.33372 to 0.33759, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 12s - loss: 3.8035 - acc: 0.9154 - mDice: 0.3616 - val_loss: 4.6776 - val_acc: 0.9300 - val_mDice: 0.3377

Epoch 00049: val_mDice improved from 0.33759 to 0.33769, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 13s - loss: 3.7568 - acc: 0.9162 - mDice: 0.3682 - val_loss: 4.4924 - val_acc: 0.9325 - val_mDice: 0.3504

Epoch 00050: val_mDice improved from 0.33769 to 0.35035, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 13s - loss: 3.7359 - acc: 0.9165 - mDice: 0.3717 - val_loss: 4.4014 - val_acc: 0.9320 - val_mDice: 0.3567

Epoch 00051: val_mDice improved from 0.35035 to 0.35672, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 13s - loss: 3.6863 - acc: 0.9176 - mDice: 0.3791 - val_loss: 4.4929 - val_acc: 0.9300 - val_mDice: 0.3533

Epoch 00052: val_mDice did not improve from 0.35672
Epoch 53/300
 - 13s - loss: 3.6360 - acc: 0.9183 - mDice: 0.3855 - val_loss: 4.1344 - val_acc: 0.9309 - val_mDice: 0.3725

Epoch 00053: val_mDice improved from 0.35672 to 0.37248, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 14s - loss: 3.6073 - acc: 0.9188 - mDice: 0.3902 - val_loss: 4.4178 - val_acc: 0.9355 - val_mDice: 0.3723

Epoch 00054: val_mDice did not improve from 0.37248
Epoch 55/300
 - 13s - loss: 3.5698 - acc: 0.9196 - mDice: 0.3953 - val_loss: 4.4276 - val_acc: 0.9360 - val_mDice: 0.3796

Epoch 00055: val_mDice improved from 0.37248 to 0.37957, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 13s - loss: 3.5555 - acc: 0.9198 - mDice: 0.3979 - val_loss: 4.6869 - val_acc: 0.9353 - val_mDice: 0.3669

Epoch 00056: val_mDice did not improve from 0.37957
Epoch 57/300
 - 13s - loss: 3.5191 - acc: 0.9204 - mDice: 0.4031 - val_loss: 4.2840 - val_acc: 0.9338 - val_mDice: 0.3771

Epoch 00057: val_mDice did not improve from 0.37957
Epoch 58/300
 - 13s - loss: 3.4868 - acc: 0.9211 - mDice: 0.4072 - val_loss: 4.3038 - val_acc: 0.9350 - val_mDice: 0.3897

Epoch 00058: val_mDice improved from 0.37957 to 0.38969, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 12s - loss: 3.4551 - acc: 0.9219 - mDice: 0.4129 - val_loss: 4.0769 - val_acc: 0.9358 - val_mDice: 0.3967

Epoch 00059: val_mDice improved from 0.38969 to 0.39668, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 60/300
 - 13s - loss: 3.4191 - acc: 0.9222 - mDice: 0.4175 - val_loss: 4.2550 - val_acc: 0.9338 - val_mDice: 0.3906

Epoch 00060: val_mDice did not improve from 0.39668
Epoch 61/300
 - 14s - loss: 3.4074 - acc: 0.9228 - mDice: 0.4198 - val_loss: 4.1261 - val_acc: 0.9353 - val_mDice: 0.3971

Epoch 00061: val_mDice improved from 0.39668 to 0.39708, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 62/300
 - 14s - loss: 3.3882 - acc: 0.9231 - mDice: 0.4224 - val_loss: 4.1644 - val_acc: 0.9355 - val_mDice: 0.3985

Epoch 00062: val_mDice improved from 0.39708 to 0.39853, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 14s - loss: 3.3538 - acc: 0.9235 - mDice: 0.4272 - val_loss: 4.2001 - val_acc: 0.9341 - val_mDice: 0.3998

Epoch 00063: val_mDice improved from 0.39853 to 0.39984, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 14s - loss: 3.3348 - acc: 0.9240 - mDice: 0.4300 - val_loss: 4.1240 - val_acc: 0.9354 - val_mDice: 0.4035

Epoch 00064: val_mDice improved from 0.39984 to 0.40347, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 65/300
 - 14s - loss: 3.3096 - acc: 0.9243 - mDice: 0.4328 - val_loss: 4.3919 - val_acc: 0.9395 - val_mDice: 0.3974

Epoch 00065: val_mDice did not improve from 0.40347
Epoch 66/300
 - 14s - loss: 3.2820 - acc: 0.9248 - mDice: 0.4378 - val_loss: 4.2706 - val_acc: 0.9374 - val_mDice: 0.3999

Epoch 00066: val_mDice did not improve from 0.40347
Epoch 67/300
 - 16s - loss: 3.2763 - acc: 0.9248 - mDice: 0.4384 - val_loss: 4.1672 - val_acc: 0.9338 - val_mDice: 0.4063

Epoch 00067: val_mDice improved from 0.40347 to 0.40633, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 68/300
 - 14s - loss: 3.2434 - acc: 0.9253 - mDice: 0.4437 - val_loss: 4.2832 - val_acc: 0.9352 - val_mDice: 0.3930

Epoch 00068: val_mDice did not improve from 0.40633
Epoch 69/300
 - 16s - loss: 3.2426 - acc: 0.9255 - mDice: 0.4440 - val_loss: 4.3697 - val_acc: 0.9321 - val_mDice: 0.3999

Epoch 00069: val_mDice did not improve from 0.40633
Epoch 70/300
 - 14s - loss: 3.2284 - acc: 0.9255 - mDice: 0.4457 - val_loss: 4.2082 - val_acc: 0.9329 - val_mDice: 0.4072

Epoch 00070: val_mDice improved from 0.40633 to 0.40720, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 71/300
 - 15s - loss: 3.2109 - acc: 0.9259 - mDice: 0.4481 - val_loss: 4.4230 - val_acc: 0.9271 - val_mDice: 0.3917

Epoch 00071: val_mDice did not improve from 0.40720
Epoch 72/300
 - 14s - loss: 3.1994 - acc: 0.9259 - mDice: 0.4491 - val_loss: 4.3576 - val_acc: 0.9251 - val_mDice: 0.3967

Epoch 00072: val_mDice did not improve from 0.40720
Epoch 73/300
 - 15s - loss: 3.1785 - acc: 0.9263 - mDice: 0.4522 - val_loss: 4.3049 - val_acc: 0.9309 - val_mDice: 0.4081

Epoch 00073: val_mDice improved from 0.40720 to 0.40809, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 13s - loss: 3.1568 - acc: 0.9267 - mDice: 0.4551 - val_loss: 3.9910 - val_acc: 0.9345 - val_mDice: 0.4271

Epoch 00074: val_mDice improved from 0.40809 to 0.42706, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 75/300
 - 15s - loss: 3.1421 - acc: 0.9269 - mDice: 0.4580 - val_loss: 4.0532 - val_acc: 0.9350 - val_mDice: 0.4150

Epoch 00075: val_mDice did not improve from 0.42706
Epoch 76/300
 - 15s - loss: 3.1449 - acc: 0.9268 - mDice: 0.4580 - val_loss: 4.0738 - val_acc: 0.9378 - val_mDice: 0.4131

Epoch 00076: val_mDice did not improve from 0.42706
Epoch 77/300
 - 14s - loss: 3.1309 - acc: 0.9270 - mDice: 0.4595 - val_loss: 4.2242 - val_acc: 0.9353 - val_mDice: 0.4098

Epoch 00077: val_mDice did not improve from 0.42706
Epoch 78/300
 - 15s - loss: 3.1093 - acc: 0.9274 - mDice: 0.4624 - val_loss: 4.1884 - val_acc: 0.9396 - val_mDice: 0.4235

Epoch 00078: val_mDice did not improve from 0.42706
Epoch 79/300
 - 14s - loss: 3.0960 - acc: 0.9276 - mDice: 0.4650 - val_loss: 4.1871 - val_acc: 0.9343 - val_mDice: 0.4146

Epoch 00079: val_mDice did not improve from 0.42706
Epoch 80/300
 - 14s - loss: 3.0979 - acc: 0.9276 - mDice: 0.4642 - val_loss: 4.3203 - val_acc: 0.9279 - val_mDice: 0.4148

Epoch 00080: val_mDice did not improve from 0.42706
Epoch 81/300
 - 14s - loss: 3.0846 - acc: 0.9278 - mDice: 0.4660 - val_loss: 4.2908 - val_acc: 0.9370 - val_mDice: 0.4182

Epoch 00081: val_mDice did not improve from 0.42706
Epoch 82/300
 - 15s - loss: 3.0820 - acc: 0.9278 - mDice: 0.4664 - val_loss: 4.2534 - val_acc: 0.9375 - val_mDice: 0.4138

Epoch 00082: val_mDice did not improve from 0.42706
Epoch 83/300
 - 14s - loss: 3.0638 - acc: 0.9283 - mDice: 0.4693 - val_loss: 4.1149 - val_acc: 0.9386 - val_mDice: 0.4306

Epoch 00083: val_mDice improved from 0.42706 to 0.43062, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 84/300
 - 14s - loss: 3.0421 - acc: 0.9284 - mDice: 0.4728 - val_loss: 4.1827 - val_acc: 0.9362 - val_mDice: 0.4285

Epoch 00084: val_mDice did not improve from 0.43062
Epoch 85/300
 - 14s - loss: 3.0412 - acc: 0.9284 - mDice: 0.4733 - val_loss: 4.2546 - val_acc: 0.9338 - val_mDice: 0.4199

Epoch 00085: val_mDice did not improve from 0.43062
Epoch 86/300
 - 14s - loss: 3.0456 - acc: 0.9285 - mDice: 0.4724 - val_loss: 4.1090 - val_acc: 0.9336 - val_mDice: 0.4264

Epoch 00086: val_mDice did not improve from 0.43062
Epoch 87/300
 - 15s - loss: 3.0177 - acc: 0.9289 - mDice: 0.4761 - val_loss: 4.4866 - val_acc: 0.9385 - val_mDice: 0.4120

Epoch 00087: val_mDice did not improve from 0.43062
Epoch 88/300
 - 14s - loss: 3.0204 - acc: 0.9289 - mDice: 0.4760 - val_loss: 4.1554 - val_acc: 0.9378 - val_mDice: 0.4301

Epoch 00088: val_mDice did not improve from 0.43062
Epoch 89/300
 - 15s - loss: 3.0020 - acc: 0.9291 - mDice: 0.4784 - val_loss: 4.2151 - val_acc: 0.9396 - val_mDice: 0.4227

Epoch 00089: val_mDice did not improve from 0.43062
Epoch 90/300
 - 16s - loss: 2.9989 - acc: 0.9294 - mDice: 0.4798 - val_loss: 4.4640 - val_acc: 0.9376 - val_mDice: 0.4208

Epoch 00090: val_mDice did not improve from 0.43062
Epoch 91/300
 - 15s - loss: 2.9846 - acc: 0.9294 - mDice: 0.4813 - val_loss: 4.3621 - val_acc: 0.9307 - val_mDice: 0.4158

Epoch 00091: val_mDice did not improve from 0.43062
Epoch 92/300
 - 15s - loss: 2.9765 - acc: 0.9296 - mDice: 0.4821 - val_loss: 4.2678 - val_acc: 0.9332 - val_mDice: 0.4309

Epoch 00092: val_mDice improved from 0.43062 to 0.43086, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 93/300
 - 14s - loss: 2.9735 - acc: 0.9298 - mDice: 0.4836 - val_loss: 4.1643 - val_acc: 0.9392 - val_mDice: 0.4342

Epoch 00093: val_mDice improved from 0.43086 to 0.43418, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 94/300
 - 14s - loss: 2.9643 - acc: 0.9298 - mDice: 0.4834 - val_loss: 4.2109 - val_acc: 0.9364 - val_mDice: 0.4315

Epoch 00094: val_mDice did not improve from 0.43418
Epoch 95/300
 - 14s - loss: 2.9588 - acc: 0.9300 - mDice: 0.4850 - val_loss: 4.3450 - val_acc: 0.9316 - val_mDice: 0.4237

Epoch 00095: val_mDice did not improve from 0.43418
Epoch 96/300
 - 13s - loss: 2.9509 - acc: 0.9301 - mDice: 0.4860 - val_loss: 4.4903 - val_acc: 0.9317 - val_mDice: 0.4214

Epoch 00096: val_mDice did not improve from 0.43418
Epoch 97/300
 - 13s - loss: 2.9534 - acc: 0.9300 - mDice: 0.4856 - val_loss: 4.0521 - val_acc: 0.9358 - val_mDice: 0.4340

Epoch 00097: val_mDice did not improve from 0.43418
Epoch 98/300
 - 12s - loss: 2.9534 - acc: 0.9301 - mDice: 0.4856 - val_loss: 4.1623 - val_acc: 0.9366 - val_mDice: 0.4355

Epoch 00098: val_mDice improved from 0.43418 to 0.43555, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 99/300
 - 11s - loss: 2.9377 - acc: 0.9303 - mDice: 0.4879 - val_loss: 4.3546 - val_acc: 0.9283 - val_mDice: 0.4170

Epoch 00099: val_mDice did not improve from 0.43555
Epoch 100/300
 - 12s - loss: 2.9251 - acc: 0.9305 - mDice: 0.4901 - val_loss: 4.2549 - val_acc: 0.9381 - val_mDice: 0.4272

Epoch 00100: val_mDice did not improve from 0.43555
Epoch 101/300
 - 11s - loss: 2.9099 - acc: 0.9308 - mDice: 0.4920 - val_loss: 4.1488 - val_acc: 0.9376 - val_mDice: 0.4375

Epoch 00101: val_mDice improved from 0.43555 to 0.43750, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 102/300
 - 11s - loss: 2.9182 - acc: 0.9307 - mDice: 0.4914 - val_loss: 4.0331 - val_acc: 0.9371 - val_mDice: 0.4361

Epoch 00102: val_mDice did not improve from 0.43750
Epoch 103/300
 - 11s - loss: 2.9147 - acc: 0.9306 - mDice: 0.4913 - val_loss: 4.2899 - val_acc: 0.9373 - val_mDice: 0.4302

Epoch 00103: val_mDice did not improve from 0.43750
Epoch 104/300
 - 12s - loss: 2.9090 - acc: 0.9306 - mDice: 0.4914 - val_loss: 4.4182 - val_acc: 0.9363 - val_mDice: 0.4262

Epoch 00104: val_mDice did not improve from 0.43750
Epoch 105/300
 - 11s - loss: 2.9070 - acc: 0.9308 - mDice: 0.4935 - val_loss: 4.3451 - val_acc: 0.9342 - val_mDice: 0.4338

Epoch 00105: val_mDice did not improve from 0.43750
Epoch 106/300
 - 11s - loss: 2.9080 - acc: 0.9306 - mDice: 0.4918 - val_loss: 4.4964 - val_acc: 0.9369 - val_mDice: 0.4303

Epoch 00106: val_mDice did not improve from 0.43750
Epoch 107/300
 - 11s - loss: 2.8897 - acc: 0.9311 - mDice: 0.4951 - val_loss: 4.0908 - val_acc: 0.9313 - val_mDice: 0.4400

Epoch 00107: val_mDice improved from 0.43750 to 0.44004, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 108/300
 - 11s - loss: 2.8934 - acc: 0.9310 - mDice: 0.4949 - val_loss: 4.0808 - val_acc: 0.9386 - val_mDice: 0.4375

Epoch 00108: val_mDice did not improve from 0.44004
Epoch 109/300
 - 12s - loss: 2.8733 - acc: 0.9314 - mDice: 0.4976 - val_loss: 4.3817 - val_acc: 0.9380 - val_mDice: 0.4306

Epoch 00109: val_mDice did not improve from 0.44004
Epoch 110/300
 - 11s - loss: 2.8787 - acc: 0.9313 - mDice: 0.4966 - val_loss: 4.2116 - val_acc: 0.9364 - val_mDice: 0.4384

Epoch 00110: val_mDice did not improve from 0.44004
Epoch 111/300
 - 11s - loss: 2.8760 - acc: 0.9313 - mDice: 0.4971 - val_loss: 4.5372 - val_acc: 0.9319 - val_mDice: 0.4211

Epoch 00111: val_mDice did not improve from 0.44004
Epoch 112/300
 - 10s - loss: 2.8678 - acc: 0.9313 - mDice: 0.4985 - val_loss: 4.3658 - val_acc: 0.9367 - val_mDice: 0.4361

Epoch 00112: val_mDice did not improve from 0.44004
Epoch 113/300
 - 10s - loss: 2.8593 - acc: 0.9317 - mDice: 0.5002 - val_loss: 4.3680 - val_acc: 0.9332 - val_mDice: 0.4341

Epoch 00113: val_mDice did not improve from 0.44004
Epoch 114/300
 - 10s - loss: 2.8620 - acc: 0.9315 - mDice: 0.4984 - val_loss: 4.2211 - val_acc: 0.9345 - val_mDice: 0.4337

Epoch 00114: val_mDice did not improve from 0.44004
Epoch 115/300
 - 11s - loss: 2.8540 - acc: 0.9316 - mDice: 0.5007 - val_loss: 4.2769 - val_acc: 0.9342 - val_mDice: 0.4367

Epoch 00115: val_mDice did not improve from 0.44004
Epoch 116/300
 - 10s - loss: 2.8486 - acc: 0.9317 - mDice: 0.5014 - val_loss: 4.2869 - val_acc: 0.9371 - val_mDice: 0.4306

Epoch 00116: val_mDice did not improve from 0.44004
Epoch 117/300
 - 10s - loss: 2.8492 - acc: 0.9316 - mDice: 0.5020 - val_loss: 4.2768 - val_acc: 0.9353 - val_mDice: 0.4362

Epoch 00117: val_mDice did not improve from 0.44004
Epoch 118/300
 - 10s - loss: 2.8409 - acc: 0.9318 - mDice: 0.5029 - val_loss: 4.3317 - val_acc: 0.9355 - val_mDice: 0.4408

Epoch 00118: val_mDice improved from 0.44004 to 0.44078, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 119/300
 - 10s - loss: 2.8410 - acc: 0.9316 - mDice: 0.5027 - val_loss: 4.3818 - val_acc: 0.9375 - val_mDice: 0.4343

Epoch 00119: val_mDice did not improve from 0.44078
Epoch 120/300
 - 10s - loss: 2.8239 - acc: 0.9319 - mDice: 0.5052 - val_loss: 4.2548 - val_acc: 0.9314 - val_mDice: 0.4433

Epoch 00120: val_mDice improved from 0.44078 to 0.44330, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 121/300
 - 11s - loss: 2.8408 - acc: 0.9316 - mDice: 0.5030 - val_loss: 4.2923 - val_acc: 0.9345 - val_mDice: 0.4448

Epoch 00121: val_mDice improved from 0.44330 to 0.44479, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 122/300
 - 10s - loss: 2.8288 - acc: 0.9317 - mDice: 0.5044 - val_loss: 4.3895 - val_acc: 0.9381 - val_mDice: 0.4420

Epoch 00122: val_mDice did not improve from 0.44479
Epoch 123/300
 - 10s - loss: 2.8122 - acc: 0.9319 - mDice: 0.5070 - val_loss: 4.3041 - val_acc: 0.9363 - val_mDice: 0.4365

Epoch 00123: val_mDice did not improve from 0.44479
Epoch 124/300
 - 10s - loss: 2.8068 - acc: 0.9321 - mDice: 0.5079 - val_loss: 4.2267 - val_acc: 0.9370 - val_mDice: 0.4436

Epoch 00124: val_mDice did not improve from 0.44479
Epoch 125/300
 - 10s - loss: 2.8013 - acc: 0.9320 - mDice: 0.5087 - val_loss: 4.3039 - val_acc: 0.9322 - val_mDice: 0.4421

Epoch 00125: val_mDice did not improve from 0.44479
Epoch 126/300
 - 10s - loss: 2.8088 - acc: 0.9318 - mDice: 0.5085 - val_loss: 4.2040 - val_acc: 0.9363 - val_mDice: 0.4445

Epoch 00126: val_mDice did not improve from 0.44479
Epoch 127/300
 - 10s - loss: 2.8024 - acc: 0.9319 - mDice: 0.5086 - val_loss: 4.5591 - val_acc: 0.9332 - val_mDice: 0.4296

Epoch 00127: val_mDice did not improve from 0.44479
Epoch 128/300
 - 10s - loss: 2.8071 - acc: 0.9321 - mDice: 0.5082 - val_loss: 4.4056 - val_acc: 0.9366 - val_mDice: 0.4357

Epoch 00128: val_mDice did not improve from 0.44479
Epoch 129/300
 - 10s - loss: 2.7880 - acc: 0.9322 - mDice: 0.5116 - val_loss: 4.2688 - val_acc: 0.9294 - val_mDice: 0.4388

Epoch 00129: val_mDice did not improve from 0.44479
Epoch 130/300
 - 10s - loss: 2.7993 - acc: 0.9319 - mDice: 0.5100 - val_loss: 4.3511 - val_acc: 0.9363 - val_mDice: 0.4380

Epoch 00130: val_mDice did not improve from 0.44479
Epoch 131/300
 - 11s - loss: 2.7861 - acc: 0.9322 - mDice: 0.5113 - val_loss: 4.5906 - val_acc: 0.9347 - val_mDice: 0.4272

Epoch 00131: val_mDice did not improve from 0.44479
Epoch 132/300
 - 10s - loss: 2.7793 - acc: 0.9323 - mDice: 0.5127 - val_loss: 4.3617 - val_acc: 0.9361 - val_mDice: 0.4389

Epoch 00132: val_mDice did not improve from 0.44479
Epoch 133/300
 - 10s - loss: 2.7794 - acc: 0.9323 - mDice: 0.5130 - val_loss: 4.4324 - val_acc: 0.9335 - val_mDice: 0.4386

Epoch 00133: val_mDice did not improve from 0.44479
Epoch 134/300
 - 10s - loss: 2.7780 - acc: 0.9323 - mDice: 0.5128 - val_loss: 4.4659 - val_acc: 0.9338 - val_mDice: 0.4456

Epoch 00134: val_mDice improved from 0.44479 to 0.44564, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 135/300
 - 10s - loss: 2.7827 - acc: 0.9321 - mDice: 0.5124 - val_loss: 5.0835 - val_acc: 0.9382 - val_mDice: 0.4128

Epoch 00135: val_mDice did not improve from 0.44564
Epoch 136/300
 - 10s - loss: 2.7745 - acc: 0.9322 - mDice: 0.5143 - val_loss: 4.2914 - val_acc: 0.9367 - val_mDice: 0.4471

Epoch 00136: val_mDice improved from 0.44564 to 0.44711, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 137/300
 - 10s - loss: 2.7684 - acc: 0.9323 - mDice: 0.5142 - val_loss: 4.1299 - val_acc: 0.9362 - val_mDice: 0.4485

Epoch 00137: val_mDice improved from 0.44711 to 0.44854, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 138/300
 - 10s - loss: 2.7636 - acc: 0.9324 - mDice: 0.5146 - val_loss: 4.3854 - val_acc: 0.9380 - val_mDice: 0.4479

Epoch 00138: val_mDice did not improve from 0.44854
Epoch 139/300
 - 10s - loss: 2.7659 - acc: 0.9323 - mDice: 0.5151 - val_loss: 4.4392 - val_acc: 0.9379 - val_mDice: 0.4436

Epoch 00139: val_mDice did not improve from 0.44854
Epoch 140/300
 - 11s - loss: 2.7577 - acc: 0.9325 - mDice: 0.5158 - val_loss: 4.5264 - val_acc: 0.9382 - val_mDice: 0.4401

Epoch 00140: val_mDice did not improve from 0.44854
Epoch 141/300
 - 10s - loss: 2.7494 - acc: 0.9325 - mDice: 0.5171 - val_loss: 4.4274 - val_acc: 0.9380 - val_mDice: 0.4464

Epoch 00141: val_mDice did not improve from 0.44854
Epoch 142/300
 - 10s - loss: 2.7506 - acc: 0.9327 - mDice: 0.5176 - val_loss: 4.3950 - val_acc: 0.9381 - val_mDice: 0.4441

Epoch 00142: val_mDice did not improve from 0.44854
Epoch 143/300
 - 10s - loss: 2.7544 - acc: 0.9324 - mDice: 0.5170 - val_loss: 4.3399 - val_acc: 0.9366 - val_mDice: 0.4485

Epoch 00143: val_mDice did not improve from 0.44854
Epoch 144/300
 - 10s - loss: 2.7414 - acc: 0.9327 - mDice: 0.5190 - val_loss: 4.3327 - val_acc: 0.9371 - val_mDice: 0.4492

Epoch 00144: val_mDice improved from 0.44854 to 0.44925, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 145/300
 - 10s - loss: 2.7524 - acc: 0.9327 - mDice: 0.5181 - val_loss: 4.2604 - val_acc: 0.9373 - val_mDice: 0.4551

Epoch 00145: val_mDice improved from 0.44925 to 0.45507, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 146/300
 - 10s - loss: 2.7549 - acc: 0.9326 - mDice: 0.5168 - val_loss: 4.4149 - val_acc: 0.9385 - val_mDice: 0.4453

Epoch 00146: val_mDice did not improve from 0.45507
Epoch 147/300
 - 10s - loss: 2.7402 - acc: 0.9329 - mDice: 0.5187 - val_loss: 4.3427 - val_acc: 0.9360 - val_mDice: 0.4558

Epoch 00147: val_mDice improved from 0.45507 to 0.45579, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 148/300
 - 10s - loss: 2.7354 - acc: 0.9329 - mDice: 0.5203 - val_loss: 4.3767 - val_acc: 0.9362 - val_mDice: 0.4447

Epoch 00148: val_mDice did not improve from 0.45579
Epoch 149/300
 - 10s - loss: 2.7445 - acc: 0.9327 - mDice: 0.5184 - val_loss: 4.3572 - val_acc: 0.9367 - val_mDice: 0.4539

Epoch 00149: val_mDice did not improve from 0.45579
Epoch 150/300
 - 11s - loss: 2.7327 - acc: 0.9328 - mDice: 0.5200 - val_loss: 4.4491 - val_acc: 0.9339 - val_mDice: 0.4435

Epoch 00150: val_mDice did not improve from 0.45579
Epoch 151/300
 - 10s - loss: 2.7319 - acc: 0.9329 - mDice: 0.5198 - val_loss: 4.6201 - val_acc: 0.9259 - val_mDice: 0.4386

Epoch 00151: val_mDice did not improve from 0.45579
Epoch 152/300
 - 10s - loss: 2.7200 - acc: 0.9332 - mDice: 0.5222 - val_loss: 4.6259 - val_acc: 0.9290 - val_mDice: 0.4412

Epoch 00152: val_mDice did not improve from 0.45579
Epoch 153/300
 - 10s - loss: 2.7172 - acc: 0.9331 - mDice: 0.5224 - val_loss: 4.4090 - val_acc: 0.9361 - val_mDice: 0.4522

Epoch 00153: val_mDice did not improve from 0.45579
Epoch 154/300
 - 10s - loss: 2.7280 - acc: 0.9329 - mDice: 0.5207 - val_loss: 4.5339 - val_acc: 0.9333 - val_mDice: 0.4354

Epoch 00154: val_mDice did not improve from 0.45579
Epoch 155/300
 - 10s - loss: 2.7303 - acc: 0.9329 - mDice: 0.5204 - val_loss: 4.7312 - val_acc: 0.9385 - val_mDice: 0.4381

Epoch 00155: val_mDice did not improve from 0.45579
Epoch 156/300
 - 10s - loss: 2.7121 - acc: 0.9333 - mDice: 0.5238 - val_loss: 4.4085 - val_acc: 0.9365 - val_mDice: 0.4449

Epoch 00156: val_mDice did not improve from 0.45579
Epoch 157/300
 - 10s - loss: 2.7264 - acc: 0.9330 - mDice: 0.5212 - val_loss: 4.4748 - val_acc: 0.9331 - val_mDice: 0.4488

Epoch 00157: val_mDice did not improve from 0.45579
Epoch 158/300
 - 10s - loss: 2.7108 - acc: 0.9332 - mDice: 0.5235 - val_loss: 4.4666 - val_acc: 0.9339 - val_mDice: 0.4424

Epoch 00158: val_mDice did not improve from 0.45579
Epoch 159/300
 - 10s - loss: 2.7137 - acc: 0.9331 - mDice: 0.5230 - val_loss: 4.7014 - val_acc: 0.9293 - val_mDice: 0.4370

Epoch 00159: val_mDice did not improve from 0.45579
Epoch 160/300
 - 11s - loss: 2.7197 - acc: 0.9332 - mDice: 0.5226 - val_loss: 4.4006 - val_acc: 0.9370 - val_mDice: 0.4463

Epoch 00160: val_mDice did not improve from 0.45579
Epoch 161/300
 - 11s - loss: 2.7207 - acc: 0.9331 - mDice: 0.5223 - val_loss: 4.6062 - val_acc: 0.9278 - val_mDice: 0.4389

Epoch 00161: val_mDice did not improve from 0.45579
Epoch 162/300
 - 10s - loss: 2.7141 - acc: 0.9331 - mDice: 0.5236 - val_loss: 4.4269 - val_acc: 0.9366 - val_mDice: 0.4501

Epoch 00162: val_mDice did not improve from 0.45579
Epoch 163/300
 - 10s - loss: 2.6982 - acc: 0.9334 - mDice: 0.5255 - val_loss: 4.3497 - val_acc: 0.9342 - val_mDice: 0.4532

Epoch 00163: val_mDice did not improve from 0.45579
Epoch 164/300
 - 10s - loss: 2.6992 - acc: 0.9334 - mDice: 0.5250 - val_loss: 4.3480 - val_acc: 0.9386 - val_mDice: 0.4528

Epoch 00164: val_mDice did not improve from 0.45579
Epoch 165/300
 - 10s - loss: 2.6859 - acc: 0.9335 - mDice: 0.5274 - val_loss: 4.4792 - val_acc: 0.9389 - val_mDice: 0.4493

Epoch 00165: val_mDice did not improve from 0.45579
Epoch 166/300
 - 10s - loss: 2.6980 - acc: 0.9335 - mDice: 0.5263 - val_loss: 4.4946 - val_acc: 0.9377 - val_mDice: 0.4498

Epoch 00166: val_mDice did not improve from 0.45579
Epoch 167/300
 - 10s - loss: 2.7026 - acc: 0.9334 - mDice: 0.5249 - val_loss: 4.2969 - val_acc: 0.9361 - val_mDice: 0.4532

Epoch 00167: val_mDice did not improve from 0.45579
Epoch 168/300
 - 10s - loss: 2.6848 - acc: 0.9337 - mDice: 0.5282 - val_loss: 4.3108 - val_acc: 0.9361 - val_mDice: 0.4547

Epoch 00168: val_mDice did not improve from 0.45579
Epoch 169/300
 - 10s - loss: 2.6845 - acc: 0.9337 - mDice: 0.5279 - val_loss: 4.3426 - val_acc: 0.9358 - val_mDice: 0.4492

Epoch 00169: val_mDice did not improve from 0.45579
Epoch 170/300
 - 10s - loss: 2.6835 - acc: 0.9337 - mDice: 0.5279 - val_loss: 4.5080 - val_acc: 0.9393 - val_mDice: 0.4486

Epoch 00170: val_mDice did not improve from 0.45579
Epoch 171/300
 - 10s - loss: 2.6809 - acc: 0.9337 - mDice: 0.5285 - val_loss: 4.2911 - val_acc: 0.9354 - val_mDice: 0.4571

Epoch 00171: val_mDice improved from 0.45579 to 0.45710, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 172/300
 - 11s - loss: 2.6841 - acc: 0.9337 - mDice: 0.5278 - val_loss: 4.6906 - val_acc: 0.9331 - val_mDice: 0.4396

Epoch 00172: val_mDice did not improve from 0.45710
Epoch 173/300
 - 10s - loss: 2.6767 - acc: 0.9337 - mDice: 0.5295 - val_loss: 4.5392 - val_acc: 0.9358 - val_mDice: 0.4511

Epoch 00173: val_mDice did not improve from 0.45710
Epoch 174/300
 - 10s - loss: 2.6744 - acc: 0.9337 - mDice: 0.5298 - val_loss: 4.4725 - val_acc: 0.9359 - val_mDice: 0.4546

Epoch 00174: val_mDice did not improve from 0.45710
Epoch 175/300
 - 10s - loss: 2.6813 - acc: 0.9337 - mDice: 0.5288 - val_loss: 4.4911 - val_acc: 0.9328 - val_mDice: 0.4479

Epoch 00175: val_mDice did not improve from 0.45710
Epoch 176/300
 - 10s - loss: 2.6946 - acc: 0.9335 - mDice: 0.5272 - val_loss: 4.4761 - val_acc: 0.9362 - val_mDice: 0.4544

Epoch 00176: val_mDice did not improve from 0.45710
Epoch 177/300
 - 10s - loss: 2.6756 - acc: 0.9336 - mDice: 0.5291 - val_loss: 4.6790 - val_acc: 0.9396 - val_mDice: 0.4386

Epoch 00177: val_mDice did not improve from 0.45710
Epoch 178/300
 - 10s - loss: 2.6825 - acc: 0.9336 - mDice: 0.5284 - val_loss: 4.3989 - val_acc: 0.9385 - val_mDice: 0.4454

Epoch 00178: val_mDice did not improve from 0.45710
Epoch 179/300
 - 10s - loss: 2.6742 - acc: 0.9336 - mDice: 0.5290 - val_loss: 4.3933 - val_acc: 0.9372 - val_mDice: 0.4546

Epoch 00179: val_mDice did not improve from 0.45710
Epoch 180/300
 - 10s - loss: 2.6718 - acc: 0.9337 - mDice: 0.5300 - val_loss: 4.5726 - val_acc: 0.9382 - val_mDice: 0.4468

Epoch 00180: val_mDice did not improve from 0.45710
Epoch 181/300
 - 10s - loss: 2.6704 - acc: 0.9337 - mDice: 0.5304 - val_loss: 4.4206 - val_acc: 0.9347 - val_mDice: 0.4589

Epoch 00181: val_mDice improved from 0.45710 to 0.45887, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 182/300
 - 10s - loss: 2.6676 - acc: 0.9338 - mDice: 0.5308 - val_loss: 4.5050 - val_acc: 0.9387 - val_mDice: 0.4516

Epoch 00182: val_mDice did not improve from 0.45887
Epoch 183/300
 - 11s - loss: 2.6701 - acc: 0.9338 - mDice: 0.5305 - val_loss: 4.3227 - val_acc: 0.9391 - val_mDice: 0.4576

Epoch 00183: val_mDice did not improve from 0.45887
Epoch 184/300
 - 10s - loss: 2.6726 - acc: 0.9338 - mDice: 0.5304 - val_loss: 4.3859 - val_acc: 0.9394 - val_mDice: 0.4549

Epoch 00184: val_mDice did not improve from 0.45887
Epoch 185/300
 - 10s - loss: 2.6636 - acc: 0.9339 - mDice: 0.5314 - val_loss: 4.5701 - val_acc: 0.9370 - val_mDice: 0.4509

Epoch 00185: val_mDice did not improve from 0.45887
Epoch 186/300
 - 10s - loss: 2.6639 - acc: 0.9338 - mDice: 0.5316 - val_loss: 4.5309 - val_acc: 0.9389 - val_mDice: 0.4484

Epoch 00186: val_mDice did not improve from 0.45887
Epoch 187/300
 - 10s - loss: 2.6676 - acc: 0.9337 - mDice: 0.5315 - val_loss: 4.4415 - val_acc: 0.9371 - val_mDice: 0.4551

Epoch 00187: val_mDice did not improve from 0.45887
Epoch 188/300
 - 10s - loss: 2.6572 - acc: 0.9339 - mDice: 0.5327 - val_loss: 4.4534 - val_acc: 0.9390 - val_mDice: 0.4522

Epoch 00188: val_mDice did not improve from 0.45887
Epoch 189/300
 - 10s - loss: 2.6585 - acc: 0.9339 - mDice: 0.5324 - val_loss: 4.3289 - val_acc: 0.9369 - val_mDice: 0.4654

Epoch 00189: val_mDice improved from 0.45887 to 0.46535, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM30_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 190/300
 - 10s - loss: 2.6592 - acc: 0.9339 - mDice: 0.5323 - val_loss: 4.5792 - val_acc: 0.9332 - val_mDice: 0.4458

Epoch 00190: val_mDice did not improve from 0.46535
Epoch 191/300
 - 10s - loss: 2.6570 - acc: 0.9340 - mDice: 0.5324 - val_loss: 4.6638 - val_acc: 0.9368 - val_mDice: 0.4446

Epoch 00191: val_mDice did not improve from 0.46535
Epoch 192/300
 - 10s - loss: 2.6541 - acc: 0.9340 - mDice: 0.5333 - val_loss: 4.1975 - val_acc: 0.9356 - val_mDice: 0.4627

Epoch 00192: val_mDice did not improve from 0.46535
Epoch 193/300
 - 11s - loss: 2.6456 - acc: 0.9340 - mDice: 0.5343 - val_loss: 4.4545 - val_acc: 0.9362 - val_mDice: 0.4619

Epoch 00193: val_mDice did not improve from 0.46535
Epoch 194/300
 - 10s - loss: 2.6553 - acc: 0.9341 - mDice: 0.5329 - val_loss: 4.5018 - val_acc: 0.9349 - val_mDice: 0.4514

Epoch 00194: val_mDice did not improve from 0.46535
Epoch 195/300
 - 10s - loss: 2.6380 - acc: 0.9342 - mDice: 0.5352 - val_loss: 4.4906 - val_acc: 0.9385 - val_mDice: 0.4459

Epoch 00195: val_mDice did not improve from 0.46535
Epoch 196/300
 - 10s - loss: 2.6552 - acc: 0.9340 - mDice: 0.5332 - val_loss: 4.4264 - val_acc: 0.9338 - val_mDice: 0.4607

Epoch 00196: val_mDice did not improve from 0.46535
Epoch 197/300
 - 10s - loss: 2.6477 - acc: 0.9341 - mDice: 0.5343 - val_loss: 4.3021 - val_acc: 0.9370 - val_mDice: 0.4606

Epoch 00197: val_mDice did not improve from 0.46535
Epoch 198/300
 - 10s - loss: 2.6494 - acc: 0.9341 - mDice: 0.5334 - val_loss: 4.6279 - val_acc: 0.9315 - val_mDice: 0.4389

Epoch 00198: val_mDice did not improve from 0.46535
Epoch 199/300
 - 10s - loss: 2.6522 - acc: 0.9342 - mDice: 0.5338 - val_loss: 4.4853 - val_acc: 0.9332 - val_mDice: 0.4533

Epoch 00199: val_mDice did not improve from 0.46535
Epoch 200/300
 - 10s - loss: 2.6523 - acc: 0.9341 - mDice: 0.5337 - val_loss: 4.3270 - val_acc: 0.9380 - val_mDice: 0.4536

Epoch 00200: val_mDice did not improve from 0.46535
Epoch 201/300
 - 10s - loss: 2.6383 - acc: 0.9342 - mDice: 0.5357 - val_loss: 4.5863 - val_acc: 0.9348 - val_mDice: 0.4528

Epoch 00201: val_mDice did not improve from 0.46535
Epoch 202/300
 - 10s - loss: 2.6430 - acc: 0.9341 - mDice: 0.5352 - val_loss: 4.5513 - val_acc: 0.9387 - val_mDice: 0.4487

Epoch 00202: val_mDice did not improve from 0.46535
Epoch 203/300
 - 10s - loss: 2.6474 - acc: 0.9341 - mDice: 0.5347 - val_loss: 4.4091 - val_acc: 0.9371 - val_mDice: 0.4631

Epoch 00203: val_mDice did not improve from 0.46535
Epoch 204/300
 - 10s - loss: 2.6335 - acc: 0.9343 - mDice: 0.5360 - val_loss: 4.3120 - val_acc: 0.9355 - val_mDice: 0.4597

Epoch 00204: val_mDice did not improve from 0.46535
Epoch 205/300
 - 11s - loss: 2.6323 - acc: 0.9343 - mDice: 0.5370 - val_loss: 4.4252 - val_acc: 0.9356 - val_mDice: 0.4612

Epoch 00205: val_mDice did not improve from 0.46535
Epoch 206/300
 - 10s - loss: 2.6399 - acc: 0.9343 - mDice: 0.5363 - val_loss: 4.5460 - val_acc: 0.9397 - val_mDice: 0.4468

Epoch 00206: val_mDice did not improve from 0.46535
Epoch 207/300
 - 10s - loss: 2.6349 - acc: 0.9344 - mDice: 0.5360 - val_loss: 4.2670 - val_acc: 0.9356 - val_mDice: 0.4636

Epoch 00207: val_mDice did not improve from 0.46535
Epoch 208/300
 - 10s - loss: 2.6297 - acc: 0.9343 - mDice: 0.5370 - val_loss: 4.5472 - val_acc: 0.9372 - val_mDice: 0.4541

Epoch 00208: val_mDice did not improve from 0.46535
Epoch 209/300
 - 10s - loss: 2.6214 - acc: 0.9346 - mDice: 0.5389 - val_loss: 4.5711 - val_acc: 0.9394 - val_mDice: 0.4554

Epoch 00209: val_mDice did not improve from 0.46535
Epoch 210/300
 - 10s - loss: 2.6396 - acc: 0.9342 - mDice: 0.5356 - val_loss: 4.3923 - val_acc: 0.9377 - val_mDice: 0.4616

Epoch 00210: val_mDice did not improve from 0.46535
Epoch 211/300
 - 10s - loss: 2.6267 - acc: 0.9344 - mDice: 0.5376 - val_loss: 4.4762 - val_acc: 0.9348 - val_mDice: 0.4508

Epoch 00211: val_mDice did not improve from 0.46535
Epoch 212/300
 - 10s - loss: 2.6196 - acc: 0.9345 - mDice: 0.5384 - val_loss: 4.5685 - val_acc: 0.9336 - val_mDice: 0.4486

Epoch 00212: val_mDice did not improve from 0.46535
Epoch 213/300
 - 10s - loss: 2.6208 - acc: 0.9346 - mDice: 0.5382 - val_loss: 4.3909 - val_acc: 0.9346 - val_mDice: 0.4537

Epoch 00213: val_mDice did not improve from 0.46535
Epoch 214/300
 - 10s - loss: 2.6307 - acc: 0.9344 - mDice: 0.5368 - val_loss: 4.3432 - val_acc: 0.9366 - val_mDice: 0.4603

Epoch 00214: val_mDice did not improve from 0.46535
Epoch 215/300
 - 10s - loss: 2.6252 - acc: 0.9345 - mDice: 0.5382 - val_loss: 4.5934 - val_acc: 0.9318 - val_mDice: 0.4469

Epoch 00215: val_mDice did not improve from 0.46535
Epoch 216/300
 - 10s - loss: 2.6210 - acc: 0.9345 - mDice: 0.5387 - val_loss: 4.3785 - val_acc: 0.9363 - val_mDice: 0.4619

Epoch 00216: val_mDice did not improve from 0.46535
Epoch 217/300
 - 10s - loss: 2.6212 - acc: 0.9344 - mDice: 0.5381 - val_loss: 4.4370 - val_acc: 0.9338 - val_mDice: 0.4585

Epoch 00217: val_mDice did not improve from 0.46535
Epoch 218/300
 - 10s - loss: 2.6094 - acc: 0.9346 - mDice: 0.5402 - val_loss: 4.3289 - val_acc: 0.9379 - val_mDice: 0.4644

Epoch 00218: val_mDice did not improve from 0.46535
Epoch 219/300
 - 10s - loss: 2.6111 - acc: 0.9347 - mDice: 0.5411 - val_loss: 4.3141 - val_acc: 0.9364 - val_mDice: 0.4642

Epoch 00219: val_mDice did not improve from 0.46535
Restoring model weights from the end of the best epoch
Epoch 00219: early stopping
{'val_loss': [97.32776592327998, 22.42034990970905, 14.112250325771479, 11.410387156101374, 10.057283522991034, 9.340888551794565, 8.778065849955265, 8.399229304148601, 8.120882129439941, 7.854764911990899, 8.165196705323, 7.871593510875335, 8.291886333089609, 7.650030988913316, 7.069007884997588, 6.784645963173646, 6.211368503478857, 7.461468186516028, 5.793312720381296, 6.136740236328198, 5.714962664705056, 5.592571201232763, 5.990382526929562, 5.713946592349273, 5.774190563421983, 5.498511681189904, 5.819665131660608, 5.330469307991175, 5.181571888235899, 5.146121391883264, 6.603469353455764, 5.108254926709028, 5.1286347428193455, 5.142495346757082, 4.911919684364245, 5.085183799266815, 4.775124350419412, 4.87051206941788, 4.676916919075525, 4.8758172897192145, 4.787812267358486, 4.680695643791785, 4.4726594640658455, 4.371723967102858, 4.535836048997366, 4.390311158620394, 4.685212447093083, 4.615066774762594, 4.677594203215379, 4.492359328728456, 4.40136006130622, 4.492863679161439, 4.134448103033579, 4.417813554406166, 4.427583522521532, 4.686896259968098, 4.284037520106022, 4.303803003751314, 4.076893257407042, 4.254982523047007, 4.126103523832101, 4.16441719692487, 4.200059148554619, 4.124049446903742, 4.391923389755762, 4.270554794141879, 4.167243457757509, 4.283215589248217, 4.369687817417658, 4.20816630927416, 4.42303805397107, 4.357621414157061, 4.304891909544285, 3.9910124207918463, 4.053217917107618, 4.073810713222394, 4.224150128089464, 4.188438269954461, 4.187074014773736, 4.320338347783456, 4.290794064219181, 4.253428376064851, 4.114861341623159, 4.182729842571112, 4.254573456369913, 4.1090008020401, 4.486553717118043, 4.155377976023233, 4.21508974983142, 4.464016988873482, 4.362140770715016, 4.2678378980893354, 4.16426364160501, 4.210928091062949, 4.345014911431533, 4.490279295123541, 4.052093517321807, 4.162293814695799, 4.35464186507922, 4.254918169516784, 4.148763754046881, 4.033055812120438, 4.289886510142913, 4.41816499714668, 4.345090123323294, 4.4963526496520405, 4.090793129343253, 4.080761041778785, 4.381720498204231, 4.211615030582134, 4.53716695194061, 4.3658278320844355, 4.368028771418792, 4.221075004109969, 4.276908029730503, 4.286916199785012, 4.276845247699664, 4.331666064377014, 4.381751712698203, 4.254801648167463, 4.292264424837553, 4.389496618738542, 4.3040775542075815, 4.226702856329771, 4.3038620696618, 4.20404920555078, 4.559134452388837, 4.405643022977388, 4.268757461355283, 4.351067078801302, 4.590640528270831, 4.361683411093859, 4.432384641124652, 4.465910508082463, 5.083509705387629, 4.29138610798579, 4.12987694144249, 4.385397349412624, 4.43917580980521, 4.526437368530494, 4.427419224610696, 4.395000657209983, 4.339865347513785, 4.332708981174689, 4.260426178574562, 4.414855083593955, 4.342690772735155, 4.376707201966872, 4.357160082230201, 4.4491166690221196, 4.6201307613116045, 4.625903327877705, 4.409025490283966, 4.533925867997683, 4.731240986631467, 4.40852348621075, 4.474823517295031, 4.466600813544714, 4.701421040755052, 4.400556047375385, 4.60622935455579, 4.426874991792899, 4.349697555486973, 4.3479733272240715, 4.4791820886043405, 4.494630929369193, 4.296861108679038, 4.310835536855918, 4.342616814833421, 4.5080385483228245, 4.291119176607865, 4.690576199155587, 4.539213190858181, 4.472487227274821, 4.491119022552784, 4.476119616856942, 4.679018948513728, 4.398928236502868, 4.393324180291249, 4.572579081241901, 4.420647552380195, 4.504969883423585, 4.32265442609787, 4.385891806620818, 4.570145079722772, 4.530917261655514, 4.441531232916391, 4.453424441126677, 4.328861823448768, 4.579200711387855, 4.663847125493563, 4.197469146205829, 4.4545296258651295, 4.501750049682764, 4.490578238780682, 4.426443971120394, 4.302059997732822, 4.627879660863143, 4.485256666174302, 4.3269774581377325, 4.586292859453422, 4.551338389515877, 4.4091354402212, 4.311968143169697, 4.425151870800899, 4.545967157643575, 4.266990798597153, 4.547201484441757, 4.571111486508296, 4.392260475800588, 4.47621579353626, 4.568532310999357, 4.390941235881585, 4.343157181946131, 4.5933583206855335, 4.37849866770781, 4.437031619823896, 4.328904146185288, 4.314136027716673], 'val_acc': [0.9033862077272855, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.903388500213623, 0.9034878794963543, 0.903388500213623, 0.9037536932871892, 0.903654314004458, 0.9037120846601633, 0.9033862077272855, 0.9040125723068531, 0.9039940604796777, 0.9037583355720227, 0.9040865623033963, 0.9046458716575916, 0.9046274171425746, 0.91102534532547, 0.909340331187615, 0.913863711632215, 0.9162398347487817, 0.9159832505079416, 0.9131818344959846, 0.9167413757397578, 0.9221755014016078, 0.9209018899844243, 0.9168662039133219, 0.9205529047892644, 0.9210590559702653, 0.9206106754449698, 0.9202454411066495, 0.9196976698361911, 0.9251225269757785, 0.9274477729430566, 0.9264584756814517, 0.925367520405696, 0.930101250226681, 0.9300272510601924, 0.9324773435409253, 0.931975781917572, 0.9300064444541931, 0.9308894139069778, 0.9354913830757141, 0.9360161056885352, 0.9353088048788217, 0.9337532497369326, 0.935045295036756, 0.9358450701603522, 0.9337624747019547, 0.9353434649797586, 0.9355168480139512, 0.9341045503432934, 0.9353527174546168, 0.9395108887782464, 0.9374167712835165, 0.9337624769944411, 0.9352001685362595, 0.932068251646482, 0.9329373378020066, 0.9270756542682648, 0.9250716796288123, 0.9309333196053138, 0.9345437357058892, 0.9349667246525104, 0.9378120417778308, 0.9353157121401566, 0.9395709886000707, 0.934310264312304, 0.9279470443725586, 0.936972989485814, 0.9375185003647437, 0.9386487786586468, 0.9362264000452482, 0.933806414787586, 0.9335775489990528, 0.9385077586540809, 0.9377704102259415, 0.939635714659324, 0.9376433262458215, 0.9306536546120276, 0.9332216198627765, 0.9392011738740481, 0.9363558361163506, 0.9315597048172584, 0.93172153372031, 0.9357502506329463, 0.9365523342902844, 0.9282775452503791, 0.9380824909760401, 0.9376132373626416, 0.9371117055416107, 0.9372827571171981, 0.9363466180287875, 0.934210901076977, 0.9368828512155093, 0.9312800535788903, 0.9385701830570514, 0.9379507188613598, 0.9363789673034961, 0.9318602039263799, 0.9366933222000415, 0.9332262667325827, 0.9344882414891169, 0.9342455795178046, 0.9371440685712374, 0.935297218652872, 0.9355237713226905, 0.9375138420325059, 0.9314141090099628, 0.9345252468035772, 0.9380524250177237, 0.9363396717951848, 0.9370169112315545, 0.9321930202154013, 0.9362819194793701, 0.9332308356578534, 0.9366170465946198, 0.9293893300569974, 0.9363096539790814, 0.9346731648995326, 0.9360831219416398, 0.9334643345612746, 0.9337694301055028, 0.9382072641299322, 0.93670254945755, 0.9362356433501611, 0.9379669152773343, 0.9379183420768151, 0.9381610521903405, 0.9380038586946634, 0.9381448534818796, 0.936552325120339, 0.9371348206813519, 0.937349761907871, 0.9384707670945388, 0.9360415270695319, 0.9362472066512475, 0.936697950729957, 0.9339080842641684, 0.9258875571764432, 0.9289755958777207, 0.936106202694086, 0.9333325624465942, 0.9384753933319678, 0.936545369716791, 0.933099091053009, 0.9338826674681443, 0.9292945495018592, 0.9369707015844492, 0.9278430090500758, 0.9366424817305344, 0.9342201099945948, 0.9386209960167224, 0.9388914727247678, 0.9376525374559256, 0.9360946577328902, 0.9361293315887451, 0.9358196396094102, 0.9392543733119965, 0.9353758073770083, 0.9330713519683251, 0.9357664310015165, 0.9359259146910447, 0.9327939909238082, 0.9362333852511185, 0.9395895050122187, 0.9384869658029996, 0.9372434593164004, 0.9382280684434451, 0.9346592930647043, 0.9387296850864704, 0.9390555918216705, 0.9393537594721868, 0.9369845482019278, 0.9388729631900787, 0.9370746818872598, 0.938953853570498, 0.9369499018559089, 0.9332377795989697, 0.9367649669830616, 0.9356323939103347, 0.9362379725162799, 0.934908947119346, 0.9384915691155654, 0.9338179551638089, 0.9370284997499906, 0.9314626569931324, 0.93318461225583, 0.9379923389508174, 0.9348187767542325, 0.9387365808853736, 0.9370585015186896, 0.9355283975601196, 0.9355584451785455, 0.939663453744008, 0.9356370430726272, 0.9371948815309085, 0.9394138363691477, 0.9377010831466088, 0.9348049255517813, 0.9335983831148881, 0.9345899682778579, 0.9365569582352271, 0.931830135675577, 0.9363350432652694, 0.933838780109699, 0.9379229751917032, 0.9364321506940402], 'val_mDice': [0.012213612906634808, 0.010827870645488683, 0.013699367821503144, 0.015810210687609818, 0.018375652161641762, 0.021265021382042996, 0.027421276443279706, 0.0343674890553722, 0.04257601356277099, 0.047204756106321626, 0.044888248380560145, 0.05462266877293587, 0.060842227978775136, 0.07118715546452083, 0.08251773322430941, 0.09162989230110095, 0.1190674124428859, 0.09281352105048987, 0.13580085824315363, 0.137056446992434, 0.1461917353937259, 0.15542301764855018, 0.1512204006027717, 0.16292037112781635, 0.16285406253658807, 0.17349189691818678, 0.18091210493674645, 0.19685033450906092, 0.20895697491673323, 0.21638710510272247, 0.20106290022914225, 0.23428138288167807, 0.2330382878963764, 0.23890605855446595, 0.2521897577322446, 0.25738608550566894, 0.27806900097773624, 0.2709323348334202, 0.2830630815946139, 0.2844410464167595, 0.29232630276909244, 0.30593131797818035, 0.3176380286996181, 0.32201875259096807, 0.3273918912387811, 0.3337224696118098, 0.325579889691793, 0.33759470828450644, 0.33768560049625546, 0.3503518904057833, 0.3567160857984653, 0.35326156650598234, 0.3724806340268025, 0.372309477856526, 0.379572948297629, 0.36693750207240766, 0.3771231882274151, 0.38969480704802734, 0.3966773688220061, 0.39058641344308853, 0.3970785140991211, 0.3985277804044577, 0.39984317868947983, 0.4034680835902691, 0.39744123071432114, 0.39986952107686263, 0.40632628219632, 0.393006281210826, 0.39990119292185855, 0.40720023530033916, 0.39174948861965764, 0.396731052834254, 0.40808559839542097, 0.4270576708591901, 0.41497348707455856, 0.41305137368348926, 0.40982662026698774, 0.4234772760134477, 0.4146297098352359, 0.41477905271145016, 0.4182105975655409, 0.4138082168423213, 0.43062440248636097, 0.428496822141684, 0.4198519289493561, 0.42636223939748913, 0.4120146549091889, 0.43014884969362843, 0.4226887845076047, 0.4207989573478699, 0.41575770252026045, 0.43085616196577364, 0.4341835815172929, 0.4315071621766457, 0.4237327524102651, 0.4213912636041641, 0.4340089639792076, 0.4355464910085385, 0.41695681844766325, 0.42720800179701585, 0.4375029905484273, 0.436077098433788, 0.43022187054157257, 0.4262272102328447, 0.4337582588195801, 0.4302636396426421, 0.4400371972184915, 0.43751972569869113, 0.43061099688594157, 0.4384357101069047, 0.4211369053675578, 0.4361390798137738, 0.4341334250683968, 0.4337087938418755, 0.4367111800954892, 0.43062943850572294, 0.43618601044783223, 0.44078116290844405, 0.43431120394514156, 0.44330231616130245, 0.4447893030368365, 0.4419996469066693, 0.43645056509054625, 0.4435830076153462, 0.44210923119233203, 0.4444660045779668, 0.429572098243695, 0.4357476827616875, 0.43877673149108887, 0.4379609955044893, 0.427245343533846, 0.43893972726968616, 0.4386301436103307, 0.44564334188516325, 0.4127945819726357, 0.44710652931378436, 0.44854030586205995, 0.4478694395377086, 0.44364964589476585, 0.4400953452747602, 0.4463655037375597, 0.4440929115964816, 0.44845241136275804, 0.4492465667426586, 0.45506652244008505, 0.44528253089923125, 0.45578953337210876, 0.44466599793388295, 0.45391098524515444, 0.44345769458092177, 0.43863341154960483, 0.44120006836377657, 0.4522401610246071, 0.4354468394930546, 0.4381039899129134, 0.4449328762980608, 0.44881772737090403, 0.44242733258467454, 0.4370125320095282, 0.44626461084072405, 0.43894721653599006, 0.45005884737922597, 0.453220838537583, 0.45283301174640656, 0.44932990865065503, 0.4498482971237256, 0.45324926995314085, 0.45470665137355143, 0.44921365494911486, 0.448615662753582, 0.4571000664280011, 0.43964686072789705, 0.45105363543217, 0.45456662774086, 0.4479305543578588, 0.4543993060405438, 0.4386107405790916, 0.44538830192043233, 0.45459822794565785, 0.4468119829319991, 0.45886909331266695, 0.45160338139304745, 0.45756514886250865, 0.4549393000511023, 0.450911629658479, 0.4483760664096245, 0.45512759456267726, 0.4521824706059236, 0.4653514807040875, 0.4458407054726894, 0.44463680082788837, 0.4627467365219043, 0.461940170194094, 0.4513778190773267, 0.44590216932388455, 0.4606647247878405, 0.4605647497452222, 0.43886723541296446, 0.45325259692393816, 0.4535639142760864, 0.45278306018847686, 0.44871873551836383, 0.4630967269723232, 0.45969358487771106, 0.4612201234469047, 0.44683529952397716, 0.463566056237771, 0.45413967164663166, 0.4553743411715214, 0.46164162571613604, 0.450831780066857, 0.4485611984362969, 0.45371524359171206, 0.4602860069045654, 0.44689107915529835, 0.46185067296028137, 0.45852005080534863, 0.46440298224870974, 0.46419670375493854], 'loss': [189.35530680285535, 58.618250252388506, 28.539368097691764, 19.319991026704827, 15.461787839618808, 13.4246433358411, 12.152183078525582, 11.265639624603997, 10.590913087517135, 10.05123403078294, 9.570372077775136, 9.107589114891981, 8.69139324808477, 8.317948699272351, 7.980449168497177, 7.668125182469921, 7.38706983381407, 7.133779210687222, 6.911796112666543, 6.715343020316981, 6.547357183600839, 6.401002739294422, 6.248378320933377, 6.112305029944983, 5.984626268071793, 5.842683588766935, 5.712827148422548, 5.579824346749752, 5.454045691921583, 5.288955731507565, 5.168818367293848, 5.041040646109669, 4.923244686548004, 4.840204238144001, 4.734204021512082, 4.640744652132876, 4.566405432850719, 4.476290255897678, 4.3980145589964295, 4.320826357826803, 4.242913343496761, 4.190368858153756, 4.139322003381098, 4.073778602323397, 4.02617113076595, 3.9540322905238523, 3.9004646570361716, 3.857988847969841, 3.803485272236991, 3.756796712619423, 3.735855206884296, 3.686327048270649, 3.635951800198174, 3.6072970679861083, 3.569811387342643, 3.5554900451811613, 3.5190971176705435, 3.4868071080683385, 3.4551057017488276, 3.4191323546678127, 3.407382718918454, 3.388236669292202, 3.353750562014796, 3.33481278029643, 3.3096453524554974, 3.281985874249906, 3.276290786307005, 3.243441300133569, 3.242613889431586, 3.228428781807648, 3.210853182355889, 3.199356443512806, 3.1784982310442604, 3.156836745935338, 3.142061560518746, 3.1448876552890503, 3.130851962453576, 3.10934211805849, 3.0960070664929056, 3.0978866930125792, 3.084615622291945, 3.082014278568246, 3.0637978352733417, 3.0421216665109805, 3.0412363270205742, 3.045563996987051, 3.017720096887263, 3.0203912309820575, 3.0019536186801568, 2.998936277863769, 2.984556369551497, 2.976506046722002, 2.9735345189711886, 2.9642638429706634, 2.958842314719978, 2.9508698906151336, 2.9534238621964946, 2.953358240742355, 2.937733487835298, 2.925114739549168, 2.9098923915120536, 2.918199275974241, 2.9146885732921475, 2.908953221106149, 2.9069903973755578, 2.907971271188411, 2.8897167574399556, 2.8933555902969044, 2.873349080245829, 2.878724640688641, 2.876048889688314, 2.867789440881521, 2.8592661619516226, 2.8620272871533916, 2.8539983445094275, 2.8485752016176744, 2.8491977258855696, 2.840924949000861, 2.840989563074387, 2.823855993218454, 2.840763834881011, 2.8287811348230036, 2.8122452995955896, 2.8068388948952436, 2.801251469278076, 2.8087578656587016, 2.80235557850575, 2.8071081132606466, 2.788048918733383, 2.7993344534128384, 2.7860671323592245, 2.779349152819538, 2.7794095567811254, 2.777953609718362, 2.7826991216025756, 2.7744746554176007, 2.7683984720760813, 2.7635887712941094, 2.765908373729182, 2.7576915354105553, 2.749406321796557, 2.7505789266502925, 2.754354360709823, 2.741357196699943, 2.752382033834112, 2.754891496428944, 2.740247558032434, 2.7353633846357543, 2.74445283329378, 2.7327405342940945, 2.731878684860616, 2.71998418462592, 2.717182787694054, 2.728048010120024, 2.7302848561755804, 2.712105959486194, 2.7263555297835733, 2.710799075147053, 2.7137012641324136, 2.7197281206532575, 2.7206912423196035, 2.714119420658011, 2.6982050409486766, 2.699166529470843, 2.6859094523464435, 2.6979765280975383, 2.7026338360026614, 2.684840877931474, 2.6844527718216735, 2.6834504409193896, 2.6809192950420138, 2.6840761428520072, 2.676708260475682, 2.6743512901775643, 2.6813426596656846, 2.6945654240897317, 2.675624178177776, 2.6824655947975047, 2.6741707649308357, 2.6717707190656093, 2.6704340013291885, 2.667610081394561, 2.6700852096800896, 2.6725966613538743, 2.6636041766282523, 2.663888926579923, 2.6675765527236903, 2.657180248245282, 2.658469159148182, 2.6592411605797053, 2.656982584928857, 2.6541254505794076, 2.6456442081454408, 2.655322846402808, 2.6379737900838878, 2.655160372507679, 2.6476902906760333, 2.649388272615197, 2.652221452043163, 2.65226617798774, 2.6382827566062064, 2.642975897616614, 2.647380091959751, 2.6334639332627146, 2.6323460634922524, 2.6398534362500614, 2.6349186856709514, 2.629684653964241, 2.6214264733186896, 2.6396345941934443, 2.6266911304229015, 2.619595463520199, 2.6207882148318213, 2.6307370783294752, 2.6251741122750905, 2.621044925995598, 2.6212386546688613, 2.609394669895582, 2.6110627977410266], 'acc': [0.5517131088295183, 0.8672378787110018, 0.873312785293875, 0.874070010353012, 0.8744716324523005, 0.8746170189276641, 0.8747198429481906, 0.874794555855976, 0.8748309554115252, 0.8748397600999301, 0.8748453883659485, 0.8748633675466194, 0.8748868919052146, 0.874928501010062, 0.8749547951238836, 0.8750004448992271, 0.8750461774053558, 0.8751197823033082, 0.8752036509638749, 0.8753262673875101, 0.8755016542389132, 0.8757577106586123, 0.8760019971591503, 0.8761834550594301, 0.8763110187620318, 0.876431467023424, 0.876703135599831, 0.8778875644418308, 0.8806565229626453, 0.884272577241922, 0.8874765271840583, 0.8898586176159219, 0.8924354286879002, 0.8941646818908875, 0.8966987963742153, 0.8986408018234701, 0.9005014537709518, 0.9022551531128962, 0.903485677014261, 0.9047619353567202, 0.9061319037995677, 0.9074535864223845, 0.9084023994613615, 0.9097383160724374, 0.9108107464511181, 0.9123475827722656, 0.913323821357879, 0.9141328351409477, 0.9153522498040558, 0.916173276020176, 0.9164732940949208, 0.9175875631632493, 0.9182888073838967, 0.9188389222054401, 0.9195814924267378, 0.9198191922869529, 0.9203722556281835, 0.9210978906433663, 0.9218711192390416, 0.9222076779239503, 0.9227570167142374, 0.9230980508487778, 0.9235199377961807, 0.9240240283055488, 0.924291701270878, 0.9248235944198483, 0.9248019151116808, 0.9252846587697023, 0.9254819691362586, 0.9255022961561458, 0.9259451522297671, 0.9258892657318579, 0.9263464260187072, 0.9266716290265665, 0.9268794484016674, 0.9268382754887007, 0.9269863938390357, 0.9274121157128927, 0.9276233927180909, 0.9275948588208036, 0.9278481236344541, 0.927804494843012, 0.9283483807895276, 0.928373386139119, 0.9284160435117281, 0.9285037453616775, 0.9289168716741314, 0.9288618535801108, 0.9291059853053671, 0.9293848956949452, 0.9294398551573011, 0.9295861520121197, 0.9297743032836808, 0.9298406367832738, 0.9300080380964838, 0.9300786956401259, 0.9299837177962733, 0.9300664336239622, 0.9303197864510746, 0.9304508117196701, 0.9307756580121731, 0.9307210126354566, 0.9306346836108118, 0.9306434862543096, 0.9307874114401476, 0.9306252397405126, 0.9310569937605375, 0.9309886445157436, 0.9313894234261675, 0.9312911265771305, 0.9313229359923435, 0.9312962916387048, 0.9316598951547203, 0.9314634243709558, 0.9316320948183872, 0.931655391439195, 0.9316303393096048, 0.9317873905178454, 0.9316101016169773, 0.9318947805356113, 0.9316076615315615, 0.9317242695728285, 0.9318769558168893, 0.9321090743175173, 0.9319684469731743, 0.9318249230783694, 0.9319167274297696, 0.9320635817141746, 0.9322350168100617, 0.9319428457860235, 0.9321755794683726, 0.9323055809201388, 0.9323148270940952, 0.9322521732014659, 0.9321448519409533, 0.9322116082415836, 0.9323107900406661, 0.9323638429679544, 0.9322909488456337, 0.9324675072693994, 0.9325455469881972, 0.9326872758998166, 0.9324308841813153, 0.9327285994597712, 0.9326779017432594, 0.932562082854736, 0.9328561842546003, 0.9328542810727406, 0.9326993356240936, 0.9327704765299067, 0.9329119216776813, 0.9331508002014691, 0.9331102934279896, 0.9329157304381198, 0.9329152453003978, 0.933314917905078, 0.9329637307737253, 0.9331589558077708, 0.9331147492531096, 0.9331719673153748, 0.9330982322085116, 0.9330634259709263, 0.9333648211916665, 0.9334286681281831, 0.9335318462969379, 0.9334682871426184, 0.9333706108294411, 0.9336886097889462, 0.9336634432819841, 0.9336580809088615, 0.9337200878247622, 0.9336982065050582, 0.9337025092369976, 0.9337408171141597, 0.9337487961053519, 0.9335029165452118, 0.9335991721316852, 0.9335879811191321, 0.9335533297777637, 0.9337085836555733, 0.9336593657042128, 0.9337919418149732, 0.9338478095733975, 0.9338431734281482, 0.9338630159589667, 0.9337840282003411, 0.9337226169021171, 0.9339000600890283, 0.9339164450612113, 0.9338859864665631, 0.9339898056257456, 0.9339897370279072, 0.9340393297909525, 0.9341192030383357, 0.9342217852321045, 0.9340361376964418, 0.9341071011076155, 0.9340585303033925, 0.9342129610676085, 0.9341065943499481, 0.9341967990397747, 0.9341318895156496, 0.9340753139939836, 0.9342747939245338, 0.9342983156940127, 0.934300707702123, 0.9343987622626492, 0.9342897549762197, 0.9345604501878337, 0.9342279033853066, 0.9344282079356453, 0.9345283880341947, 0.9345646810676739, 0.934389986912065, 0.93448760073464, 0.9345283918766412, 0.9344121331626789, 0.9346235636238637, 0.9347350538123262], 'mDice': [0.016505126833034806, 0.017517375164293972, 0.01806322347575818, 0.01922654327619101, 0.020866012957246334, 0.022709766085213492, 0.025094508498949692, 0.028222220786892266, 0.03237376582208325, 0.037525678583449414, 0.04349993444160816, 0.050247135776815124, 0.057871939678490296, 0.06572853399551103, 0.07340929970553382, 0.08161155128117847, 0.08976529204066495, 0.09851769698668833, 0.10704424799884871, 0.11588132149050907, 0.12311040933920671, 0.13009562186898863, 0.13801232162001695, 0.14537995892356906, 0.15305599830010538, 0.16045207629963051, 0.16880285682316956, 0.17835255107525189, 0.1894238585765804, 0.20254152900539596, 0.2122321357921537, 0.22337791293977668, 0.23352292269663452, 0.24080143611110974, 0.25079166106409023, 0.26020585141926905, 0.26743712249216006, 0.2774165720750757, 0.2862111742119612, 0.29497510995700393, 0.30380787437586376, 0.3102529348234711, 0.3164751092182428, 0.32539515310379125, 0.33123513265728216, 0.340774710186291, 0.34816213845547017, 0.3538385540161759, 0.36159929013896513, 0.36817790438831377, 0.3717047464723331, 0.37908568610556176, 0.38551418155494965, 0.39022977997992164, 0.39529671302517216, 0.39791904195674527, 0.4031399979694055, 0.4072009267088431, 0.41294835717170364, 0.4175119402058327, 0.41979850835381516, 0.4223842146563088, 0.4272024008930472, 0.43003842953902094, 0.4327984899021345, 0.4377584533284404, 0.438422760312148, 0.4436947229184931, 0.44403585795710626, 0.4457161629193385, 0.44812904427898753, 0.44912484062286384, 0.45222050666995983, 0.4550946305062645, 0.45804836287067946, 0.45803502113256445, 0.45950648412596284, 0.4624240990220692, 0.46501078066280116, 0.46415922402192183, 0.4660165614773072, 0.4664186935280562, 0.4692945855383744, 0.47283200618251714, 0.4733414682066222, 0.4724226509179069, 0.4761169689130181, 0.4760441212958431, 0.4783674386097784, 0.47977278760952075, 0.48126886960524073, 0.4821297751574194, 0.4836350571462768, 0.48336928452907163, 0.48500035201712727, 0.4860079780634639, 0.48561462955941886, 0.48564382905748116, 0.487934964884804, 0.49009695061236236, 0.49200024327085784, 0.49140357230462284, 0.49131875327622265, 0.4913897604934846, 0.49345303214224107, 0.49175900352250757, 0.4950983866789481, 0.49487668295622556, 0.49759773127676254, 0.49658743336688194, 0.4971152479843406, 0.4985319251923129, 0.5002466091851978, 0.49843319207993336, 0.5007260622963259, 0.5013580432646633, 0.5019516928943774, 0.5028697356058093, 0.502715093122245, 0.5052181435480094, 0.5030398837240114, 0.5044082535772122, 0.5070450843441456, 0.5078990195088421, 0.5086912131777859, 0.5085005078392254, 0.5085532990898647, 0.5082425640032409, 0.5116421785190142, 0.5099601577450338, 0.5113244703650903, 0.5127132933050529, 0.5130004610103247, 0.5127986886014713, 0.5123823640158088, 0.5142867461810261, 0.5142367021311675, 0.5145587018195151, 0.5151185680170137, 0.5158140816713165, 0.5171332047300717, 0.5176270626946414, 0.5169924347809427, 0.5190367556075641, 0.518078163245568, 0.5167915281208967, 0.5186859645917606, 0.5202629585432345, 0.5184369396758808, 0.5199838668055764, 0.5198184957305718, 0.5222163400141656, 0.5224028678878387, 0.5206945703157824, 0.5203524526718148, 0.5237935349739622, 0.521211580641758, 0.5235109895366062, 0.5230151045303728, 0.5226427889239642, 0.5222563744470838, 0.5236236091789411, 0.5254919562383319, 0.5250125664203774, 0.5273536393797003, 0.526255860480833, 0.5248806920260903, 0.5282399317308577, 0.5278678528004594, 0.5279236942719328, 0.5284743758350855, 0.5277777793365003, 0.5295009462582518, 0.5297562283452868, 0.5288219002189644, 0.5271774094493739, 0.5291387790586086, 0.5283679303830041, 0.5290327973771687, 0.5300493339083682, 0.5304039668786199, 0.530831411311869, 0.5305324771863161, 0.5304189443500303, 0.5313567605436832, 0.5316424850035361, 0.5314860701242144, 0.5326743654952788, 0.5324327995622553, 0.5322825168591413, 0.5324079537044248, 0.5332929249576325, 0.5342641851069313, 0.5329239849981364, 0.5352185061032246, 0.533248891241599, 0.5342787827093597, 0.533408353989012, 0.5338098658667074, 0.5337067381994625, 0.5356644494863583, 0.5351680760991361, 0.5346651265599857, 0.5360257556591704, 0.5370471492845348, 0.5362972706366671, 0.5360479560668405, 0.5369833068061356, 0.538855380978101, 0.5356186606628279, 0.5375961992234461, 0.5384044332235136, 0.5381536440667681, 0.5368442349819573, 0.5381892581690966, 0.5387055382488465, 0.538129948177846, 0.5401919622719425, 0.5410642437386048]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.30s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.02s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.83s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:10,  1.31s/it]predicting train subjects:   1%|          | 2/285 [00:02<06:34,  1.39s/it]predicting train subjects:   1%|          | 3/285 [00:04<06:39,  1.42s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:05,  1.52s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<06:51,  1.47s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:15,  1.56s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<07:45,  1.68s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<07:54,  1.71s/it]predicting train subjects:   3%|▎         | 9/285 [00:14<07:41,  1.67s/it]predicting train subjects:   4%|▎         | 10/285 [00:16<07:58,  1.74s/it]predicting train subjects:   4%|▍         | 11/285 [00:18<08:07,  1.78s/it]predicting train subjects:   4%|▍         | 12/285 [00:20<08:09,  1.79s/it]predicting train subjects:   5%|▍         | 13/285 [00:22<08:16,  1.82s/it]predicting train subjects:   5%|▍         | 14/285 [00:24<08:28,  1.88s/it]predicting train subjects:   5%|▌         | 15/285 [00:25<08:29,  1.89s/it]predicting train subjects:   6%|▌         | 16/285 [00:27<08:35,  1.91s/it]predicting train subjects:   6%|▌         | 17/285 [00:29<08:41,  1.94s/it]predicting train subjects:   6%|▋         | 18/285 [00:31<08:32,  1.92s/it]predicting train subjects:   7%|▋         | 19/285 [00:33<08:27,  1.91s/it]predicting train subjects:   7%|▋         | 20/285 [00:35<08:26,  1.91s/it]predicting train subjects:   7%|▋         | 21/285 [00:37<08:18,  1.89s/it]predicting train subjects:   8%|▊         | 22/285 [00:39<08:12,  1.87s/it]predicting train subjects:   8%|▊         | 23/285 [00:41<08:10,  1.87s/it]predicting train subjects:   8%|▊         | 24/285 [00:43<08:13,  1.89s/it]predicting train subjects:   9%|▉         | 25/285 [00:45<08:13,  1.90s/it]predicting train subjects:   9%|▉         | 26/285 [00:46<08:09,  1.89s/it]predicting train subjects:   9%|▉         | 27/285 [00:48<08:06,  1.89s/it]predicting train subjects:  10%|▉         | 28/285 [00:50<07:48,  1.82s/it]predicting train subjects:  10%|█         | 29/285 [00:52<07:41,  1.80s/it]predicting train subjects:  11%|█         | 30/285 [00:53<07:31,  1.77s/it]predicting train subjects:  11%|█         | 31/285 [00:55<07:27,  1.76s/it]predicting train subjects:  11%|█         | 32/285 [00:57<07:25,  1.76s/it]predicting train subjects:  12%|█▏        | 33/285 [00:59<07:22,  1.76s/it]predicting train subjects:  12%|█▏        | 34/285 [01:00<07:17,  1.74s/it]predicting train subjects:  12%|█▏        | 35/285 [01:02<07:12,  1.73s/it]predicting train subjects:  13%|█▎        | 36/285 [01:04<07:07,  1.72s/it]predicting train subjects:  13%|█▎        | 37/285 [01:05<07:03,  1.71s/it]predicting train subjects:  13%|█▎        | 38/285 [01:07<06:58,  1.70s/it]predicting train subjects:  14%|█▎        | 39/285 [01:09<06:55,  1.69s/it]predicting train subjects:  14%|█▍        | 40/285 [01:11<06:56,  1.70s/it]predicting train subjects:  14%|█▍        | 41/285 [01:12<07:03,  1.73s/it]predicting train subjects:  15%|█▍        | 42/285 [01:14<07:03,  1.74s/it]predicting train subjects:  15%|█▌        | 43/285 [01:16<07:04,  1.75s/it]predicting train subjects:  15%|█▌        | 44/285 [01:18<07:01,  1.75s/it]predicting train subjects:  16%|█▌        | 45/285 [01:19<07:02,  1.76s/it]predicting train subjects:  16%|█▌        | 46/285 [01:21<06:49,  1.71s/it]predicting train subjects:  16%|█▋        | 47/285 [01:22<06:28,  1.63s/it]predicting train subjects:  17%|█▋        | 48/285 [01:24<06:18,  1.60s/it]predicting train subjects:  17%|█▋        | 49/285 [01:26<06:16,  1.59s/it]predicting train subjects:  18%|█▊        | 50/285 [01:27<06:09,  1.57s/it]predicting train subjects:  18%|█▊        | 51/285 [01:29<06:05,  1.56s/it]predicting train subjects:  18%|█▊        | 52/285 [01:30<06:04,  1.56s/it]predicting train subjects:  19%|█▊        | 53/285 [01:32<06:04,  1.57s/it]predicting train subjects:  19%|█▉        | 54/285 [01:33<06:00,  1.56s/it]predicting train subjects:  19%|█▉        | 55/285 [01:35<05:59,  1.56s/it]predicting train subjects:  20%|█▉        | 56/285 [01:36<05:56,  1.56s/it]predicting train subjects:  20%|██        | 57/285 [01:38<05:50,  1.54s/it]predicting train subjects:  20%|██        | 58/285 [01:40<05:57,  1.58s/it]predicting train subjects:  21%|██        | 59/285 [01:41<05:52,  1.56s/it]predicting train subjects:  21%|██        | 60/285 [01:43<05:52,  1.57s/it]predicting train subjects:  21%|██▏       | 61/285 [01:44<05:54,  1.58s/it]predicting train subjects:  22%|██▏       | 62/285 [01:46<05:55,  1.59s/it]predicting train subjects:  22%|██▏       | 63/285 [01:47<05:52,  1.59s/it]predicting train subjects:  22%|██▏       | 64/285 [01:49<05:46,  1.57s/it]predicting train subjects:  23%|██▎       | 65/285 [01:51<05:54,  1.61s/it]predicting train subjects:  23%|██▎       | 66/285 [01:52<06:01,  1.65s/it]predicting train subjects:  24%|██▎       | 67/285 [01:54<05:55,  1.63s/it]predicting train subjects:  24%|██▍       | 68/285 [01:56<05:46,  1.60s/it]predicting train subjects:  24%|██▍       | 69/285 [01:57<05:42,  1.58s/it]predicting train subjects:  25%|██▍       | 70/285 [01:59<05:37,  1.57s/it]predicting train subjects:  25%|██▍       | 71/285 [02:00<05:28,  1.54s/it]predicting train subjects:  25%|██▌       | 72/285 [02:02<05:25,  1.53s/it]predicting train subjects:  26%|██▌       | 73/285 [02:03<05:25,  1.54s/it]predicting train subjects:  26%|██▌       | 74/285 [02:05<05:30,  1.57s/it]predicting train subjects:  26%|██▋       | 75/285 [02:06<05:31,  1.58s/it]predicting train subjects:  27%|██▋       | 76/285 [02:08<05:29,  1.58s/it]predicting train subjects:  27%|██▋       | 77/285 [02:10<05:28,  1.58s/it]predicting train subjects:  27%|██▋       | 78/285 [02:11<05:27,  1.58s/it]predicting train subjects:  28%|██▊       | 79/285 [02:13<05:21,  1.56s/it]predicting train subjects:  28%|██▊       | 80/285 [02:14<05:17,  1.55s/it]predicting train subjects:  28%|██▊       | 81/285 [02:16<05:23,  1.59s/it]predicting train subjects:  29%|██▉       | 82/285 [02:17<05:21,  1.58s/it]predicting train subjects:  29%|██▉       | 83/285 [02:19<05:21,  1.59s/it]predicting train subjects:  29%|██▉       | 84/285 [02:21<05:17,  1.58s/it]predicting train subjects:  30%|██▉       | 85/285 [02:23<05:35,  1.68s/it]predicting train subjects:  30%|███       | 86/285 [02:24<05:40,  1.71s/it]predicting train subjects:  31%|███       | 87/285 [02:26<05:43,  1.73s/it]predicting train subjects:  31%|███       | 88/285 [02:28<05:43,  1.75s/it]predicting train subjects:  31%|███       | 89/285 [02:30<05:40,  1.74s/it]predicting train subjects:  32%|███▏      | 90/285 [02:31<05:36,  1.72s/it]predicting train subjects:  32%|███▏      | 91/285 [02:33<05:34,  1.72s/it]predicting train subjects:  32%|███▏      | 92/285 [02:35<05:32,  1.72s/it]predicting train subjects:  33%|███▎      | 93/285 [02:37<05:34,  1.74s/it]predicting train subjects:  33%|███▎      | 94/285 [02:38<05:31,  1.73s/it]predicting train subjects:  33%|███▎      | 95/285 [02:40<05:26,  1.72s/it]predicting train subjects:  34%|███▎      | 96/285 [02:42<05:25,  1.72s/it]predicting train subjects:  34%|███▍      | 97/285 [02:43<05:20,  1.71s/it]predicting train subjects:  34%|███▍      | 98/285 [02:45<05:19,  1.71s/it]predicting train subjects:  35%|███▍      | 99/285 [02:47<05:18,  1.71s/it]predicting train subjects:  35%|███▌      | 100/285 [02:48<05:17,  1.72s/it]predicting train subjects:  35%|███▌      | 101/285 [02:50<05:15,  1.72s/it]predicting train subjects:  36%|███▌      | 102/285 [02:52<05:16,  1.73s/it]predicting train subjects:  36%|███▌      | 103/285 [02:54<05:13,  1.72s/it]predicting train subjects:  36%|███▋      | 104/285 [02:55<05:14,  1.73s/it]predicting train subjects:  37%|███▋      | 105/285 [02:57<05:08,  1.72s/it]predicting train subjects:  37%|███▋      | 106/285 [02:59<05:07,  1.72s/it]predicting train subjects:  38%|███▊      | 107/285 [03:01<05:08,  1.73s/it]predicting train subjects:  38%|███▊      | 108/285 [03:02<05:06,  1.73s/it]predicting train subjects:  38%|███▊      | 109/285 [03:04<05:04,  1.73s/it]predicting train subjects:  39%|███▊      | 110/285 [03:06<05:01,  1.72s/it]predicting train subjects:  39%|███▉      | 111/285 [03:08<05:03,  1.74s/it]predicting train subjects:  39%|███▉      | 112/285 [03:09<05:00,  1.74s/it]predicting train subjects:  40%|███▉      | 113/285 [03:11<04:58,  1.73s/it]predicting train subjects:  40%|████      | 114/285 [03:13<04:56,  1.74s/it]predicting train subjects:  40%|████      | 115/285 [03:14<04:55,  1.74s/it]predicting train subjects:  41%|████      | 116/285 [03:16<04:48,  1.71s/it]predicting train subjects:  41%|████      | 117/285 [03:18<04:45,  1.70s/it]predicting train subjects:  41%|████▏     | 118/285 [03:19<04:42,  1.69s/it]predicting train subjects:  42%|████▏     | 119/285 [03:21<04:39,  1.68s/it]predicting train subjects:  42%|████▏     | 120/285 [03:23<04:36,  1.68s/it]predicting train subjects:  42%|████▏     | 121/285 [03:24<04:27,  1.63s/it]predicting train subjects:  43%|████▎     | 122/285 [03:26<04:15,  1.57s/it]predicting train subjects:  43%|████▎     | 123/285 [03:27<04:04,  1.51s/it]predicting train subjects:  44%|████▎     | 124/285 [03:29<04:04,  1.52s/it]predicting train subjects:  44%|████▍     | 125/285 [03:30<04:05,  1.53s/it]predicting train subjects:  44%|████▍     | 126/285 [03:32<04:07,  1.56s/it]predicting train subjects:  45%|████▍     | 127/285 [03:33<04:03,  1.54s/it]predicting train subjects:  45%|████▍     | 128/285 [03:35<04:01,  1.54s/it]predicting train subjects:  45%|████▌     | 129/285 [03:36<03:58,  1.53s/it]predicting train subjects:  46%|████▌     | 130/285 [03:38<04:01,  1.56s/it]predicting train subjects:  46%|████▌     | 131/285 [03:40<04:02,  1.58s/it]predicting train subjects:  46%|████▋     | 132/285 [03:41<04:00,  1.57s/it]predicting train subjects:  47%|████▋     | 133/285 [03:43<03:58,  1.57s/it]predicting train subjects:  47%|████▋     | 134/285 [03:44<03:56,  1.57s/it]predicting train subjects:  47%|████▋     | 135/285 [03:46<03:54,  1.57s/it]predicting train subjects:  48%|████▊     | 136/285 [03:47<03:54,  1.58s/it]predicting train subjects:  48%|████▊     | 137/285 [03:49<03:53,  1.58s/it]predicting train subjects:  48%|████▊     | 138/285 [03:51<03:49,  1.56s/it]predicting train subjects:  49%|████▉     | 139/285 [03:52<03:48,  1.56s/it]predicting train subjects:  49%|████▉     | 140/285 [03:54<03:46,  1.56s/it]predicting train subjects:  49%|████▉     | 141/285 [03:55<03:44,  1.56s/it]predicting train subjects:  50%|████▉     | 142/285 [03:57<03:36,  1.51s/it]predicting train subjects:  50%|█████     | 143/285 [03:58<03:32,  1.50s/it]predicting train subjects:  51%|█████     | 144/285 [04:00<03:28,  1.48s/it]predicting train subjects:  51%|█████     | 145/285 [04:01<03:21,  1.44s/it]predicting train subjects:  51%|█████     | 146/285 [04:02<03:17,  1.42s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:04<03:15,  1.41s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:05<03:12,  1.41s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:06<03:08,  1.39s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:08<03:09,  1.40s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:09<03:09,  1.42s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:11<03:07,  1.41s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:12<03:04,  1.40s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:13<03:03,  1.40s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:15<03:03,  1.41s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:16<02:59,  1.39s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:18<02:59,  1.40s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:19<03:01,  1.43s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:21<02:59,  1.42s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:22<02:56,  1.42s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:23<02:53,  1.40s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:25<02:51,  1.39s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:26<02:49,  1.39s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:27<02:48,  1.39s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:29<02:47,  1.39s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:30<02:45,  1.39s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:32<02:44,  1.40s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:33<02:40,  1.38s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:34<02:39,  1.37s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:36<02:38,  1.38s/it]predicting train subjects:  60%|██████    | 171/285 [04:37<02:38,  1.39s/it]predicting train subjects:  60%|██████    | 172/285 [04:38<02:35,  1.37s/it]predicting train subjects:  61%|██████    | 173/285 [04:40<02:33,  1.37s/it]predicting train subjects:  61%|██████    | 174/285 [04:41<02:34,  1.39s/it]predicting train subjects:  61%|██████▏   | 175/285 [04:43<02:31,  1.38s/it]predicting train subjects:  62%|██████▏   | 176/285 [04:44<02:31,  1.39s/it]predicting train subjects:  62%|██████▏   | 177/285 [04:46<02:32,  1.41s/it]predicting train subjects:  62%|██████▏   | 178/285 [04:47<02:27,  1.38s/it]predicting train subjects:  63%|██████▎   | 179/285 [04:48<02:24,  1.36s/it]predicting train subjects:  63%|██████▎   | 180/285 [04:49<02:22,  1.36s/it]predicting train subjects:  64%|██████▎   | 181/285 [04:51<02:20,  1.35s/it]predicting train subjects:  64%|██████▍   | 182/285 [04:52<02:17,  1.34s/it]predicting train subjects:  64%|██████▍   | 183/285 [04:53<02:15,  1.33s/it]predicting train subjects:  65%|██████▍   | 184/285 [04:55<02:12,  1.31s/it]predicting train subjects:  65%|██████▍   | 185/285 [04:56<02:10,  1.31s/it]predicting train subjects:  65%|██████▌   | 186/285 [04:57<02:08,  1.30s/it]predicting train subjects:  66%|██████▌   | 187/285 [04:59<02:09,  1.32s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:00<02:09,  1.34s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:01<02:09,  1.35s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:03<02:08,  1.35s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:04<02:05,  1.34s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:05<02:04,  1.34s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:07<02:03,  1.34s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:08<02:01,  1.33s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:09<02:00,  1.34s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:11<02:06,  1.42s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:13<02:12,  1.51s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:14<02:17,  1.58s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:16<02:16,  1.59s/it]predicting train subjects:  70%|███████   | 200/285 [05:18<02:16,  1.61s/it]predicting train subjects:  71%|███████   | 201/285 [05:19<02:14,  1.60s/it]predicting train subjects:  71%|███████   | 202/285 [05:21<02:10,  1.57s/it]predicting train subjects:  71%|███████   | 203/285 [05:22<02:09,  1.58s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:24<02:07,  1.58s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:26<02:06,  1.58s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:27<02:06,  1.60s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:29<02:08,  1.65s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:31<02:07,  1.66s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:32<02:04,  1.64s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:34<02:03,  1.65s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:36<02:01,  1.65s/it]predicting train subjects:  74%|███████▍  | 212/285 [05:37<01:59,  1.63s/it]predicting train subjects:  75%|███████▍  | 213/285 [05:39<01:56,  1.62s/it]predicting train subjects:  75%|███████▌  | 214/285 [05:40<01:49,  1.55s/it]predicting train subjects:  75%|███████▌  | 215/285 [05:42<01:45,  1.51s/it]predicting train subjects:  76%|███████▌  | 216/285 [05:43<01:41,  1.47s/it]predicting train subjects:  76%|███████▌  | 217/285 [05:44<01:38,  1.45s/it]predicting train subjects:  76%|███████▋  | 218/285 [05:46<01:35,  1.43s/it]predicting train subjects:  77%|███████▋  | 219/285 [05:47<01:33,  1.42s/it]predicting train subjects:  77%|███████▋  | 220/285 [05:49<01:30,  1.40s/it]predicting train subjects:  78%|███████▊  | 221/285 [05:50<01:29,  1.40s/it]predicting train subjects:  78%|███████▊  | 222/285 [05:51<01:28,  1.40s/it]predicting train subjects:  78%|███████▊  | 223/285 [05:53<01:27,  1.41s/it]predicting train subjects:  79%|███████▊  | 224/285 [05:54<01:25,  1.40s/it]predicting train subjects:  79%|███████▉  | 225/285 [05:56<01:23,  1.39s/it]predicting train subjects:  79%|███████▉  | 226/285 [05:57<01:21,  1.39s/it]predicting train subjects:  80%|███████▉  | 227/285 [05:58<01:19,  1.37s/it]predicting train subjects:  80%|████████  | 228/285 [06:00<01:18,  1.37s/it]predicting train subjects:  80%|████████  | 229/285 [06:01<01:16,  1.37s/it]predicting train subjects:  81%|████████  | 230/285 [06:02<01:15,  1.38s/it]predicting train subjects:  81%|████████  | 231/285 [06:04<01:15,  1.39s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:06<01:19,  1.50s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:07<01:22,  1.58s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:09<01:24,  1.66s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:11<01:23,  1.68s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:13<01:22,  1.68s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:14<01:22,  1.72s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:16<01:20,  1.72s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:18<01:19,  1.73s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:20<01:18,  1.74s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:21<01:16,  1.74s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:23<01:14,  1.74s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:25<01:13,  1.75s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:27<01:11,  1.74s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:28<01:09,  1.73s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:30<01:07,  1.72s/it]predicting train subjects:  87%|████████▋ | 247/285 [06:32<01:04,  1.70s/it]predicting train subjects:  87%|████████▋ | 248/285 [06:33<01:03,  1.73s/it]predicting train subjects:  87%|████████▋ | 249/285 [06:35<01:02,  1.73s/it]predicting train subjects:  88%|████████▊ | 250/285 [06:37<00:57,  1.63s/it]predicting train subjects:  88%|████████▊ | 251/285 [06:38<00:52,  1.53s/it]predicting train subjects:  88%|████████▊ | 252/285 [06:39<00:48,  1.48s/it]predicting train subjects:  89%|████████▉ | 253/285 [06:41<00:46,  1.45s/it]predicting train subjects:  89%|████████▉ | 254/285 [06:42<00:43,  1.41s/it]predicting train subjects:  89%|████████▉ | 255/285 [06:43<00:41,  1.39s/it]predicting train subjects:  90%|████████▉ | 256/285 [06:45<00:40,  1.40s/it]predicting train subjects:  90%|█████████ | 257/285 [06:46<00:39,  1.41s/it]predicting train subjects:  91%|█████████ | 258/285 [06:48<00:38,  1.43s/it]predicting train subjects:  91%|█████████ | 259/285 [06:49<00:36,  1.40s/it]predicting train subjects:  91%|█████████ | 260/285 [06:50<00:34,  1.39s/it]predicting train subjects:  92%|█████████▏| 261/285 [06:52<00:32,  1.37s/it]predicting train subjects:  92%|█████████▏| 262/285 [06:53<00:31,  1.35s/it]predicting train subjects:  92%|█████████▏| 263/285 [06:54<00:29,  1.36s/it]predicting train subjects:  93%|█████████▎| 264/285 [06:56<00:28,  1.35s/it]predicting train subjects:  93%|█████████▎| 265/285 [06:57<00:26,  1.33s/it]predicting train subjects:  93%|█████████▎| 266/285 [06:58<00:25,  1.35s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:00<00:24,  1.35s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:01<00:25,  1.47s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:03<00:24,  1.55s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:05<00:23,  1.59s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:07<00:22,  1.62s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:08<00:21,  1.66s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:10<00:20,  1.68s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:12<00:18,  1.69s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:14<00:17,  1.74s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:15<00:15,  1.77s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:17<00:14,  1.80s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:19<00:12,  1.79s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:21<00:10,  1.77s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:23<00:08,  1.76s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:24<00:06,  1.74s/it]predicting train subjects:  99%|█████████▉| 282/285 [07:26<00:05,  1.77s/it]predicting train subjects:  99%|█████████▉| 283/285 [07:28<00:03,  1.74s/it]predicting train subjects: 100%|█████████▉| 284/285 [07:30<00:01,  1.78s/it]predicting train subjects: 100%|██████████| 285/285 [07:31<00:00,  1.76s/it]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<05:55,  1.25s/it]Loading train:   1%|          | 2/285 [00:02<06:05,  1.29s/it]Loading train:   1%|          | 3/285 [00:03<05:59,  1.27s/it]Loading train:   1%|▏         | 4/285 [00:05<06:27,  1.38s/it]Loading train:   2%|▏         | 5/285 [00:06<06:12,  1.33s/it]Loading train:   2%|▏         | 6/285 [00:08<06:31,  1.40s/it]Loading train:   2%|▏         | 7/285 [00:09<06:44,  1.45s/it]Loading train:   3%|▎         | 8/285 [00:11<06:55,  1.50s/it]Loading train:   3%|▎         | 9/285 [00:12<06:33,  1.43s/it]Loading train:   4%|▎         | 10/285 [00:13<05:50,  1.27s/it]Loading train:   4%|▍         | 11/285 [00:14<05:17,  1.16s/it]Loading train:   4%|▍         | 12/285 [00:15<04:58,  1.09s/it]Loading train:   5%|▍         | 13/285 [00:16<04:45,  1.05s/it]Loading train:   5%|▍         | 14/285 [00:17<04:35,  1.02s/it]Loading train:   5%|▌         | 15/285 [00:18<04:38,  1.03s/it]Loading train:   6%|▌         | 16/285 [00:19<04:27,  1.01it/s]Loading train:   6%|▌         | 17/285 [00:20<04:25,  1.01it/s]Loading train:   6%|▋         | 18/285 [00:21<04:20,  1.03it/s]Loading train:   7%|▋         | 19/285 [00:22<04:14,  1.05it/s]Loading train:   7%|▋         | 20/285 [00:23<04:11,  1.06it/s]Loading train:   7%|▋         | 21/285 [00:24<04:08,  1.06it/s]Loading train:   8%|▊         | 22/285 [00:24<04:06,  1.07it/s]Loading train:   8%|▊         | 23/285 [00:26<04:15,  1.02it/s]Loading train:   8%|▊         | 24/285 [00:26<04:15,  1.02it/s]Loading train:   9%|▉         | 25/285 [00:27<04:07,  1.05it/s]Loading train:   9%|▉         | 26/285 [00:28<04:05,  1.05it/s]Loading train:   9%|▉         | 27/285 [00:29<04:02,  1.06it/s]Loading train:  10%|▉         | 28/285 [00:30<04:13,  1.01it/s]Loading train:  10%|█         | 29/285 [00:31<04:02,  1.06it/s]Loading train:  11%|█         | 30/285 [00:32<03:57,  1.07it/s]Loading train:  11%|█         | 31/285 [00:33<03:57,  1.07it/s]Loading train:  11%|█         | 32/285 [00:34<03:57,  1.07it/s]Loading train:  12%|█▏        | 33/285 [00:35<03:53,  1.08it/s]Loading train:  12%|█▏        | 34/285 [00:36<03:48,  1.10it/s]Loading train:  12%|█▏        | 35/285 [00:37<03:46,  1.10it/s]Loading train:  13%|█▎        | 36/285 [00:38<03:44,  1.11it/s]Loading train:  13%|█▎        | 37/285 [00:38<03:46,  1.10it/s]Loading train:  13%|█▎        | 38/285 [00:39<03:45,  1.09it/s]Loading train:  14%|█▎        | 39/285 [00:40<03:50,  1.07it/s]Loading train:  14%|█▍        | 40/285 [00:41<03:50,  1.06it/s]Loading train:  14%|█▍        | 41/285 [00:42<03:45,  1.08it/s]Loading train:  15%|█▍        | 42/285 [00:43<03:39,  1.11it/s]Loading train:  15%|█▌        | 43/285 [00:44<03:46,  1.07it/s]Loading train:  15%|█▌        | 44/285 [00:45<03:43,  1.08it/s]Loading train:  16%|█▌        | 45/285 [00:46<03:39,  1.09it/s]Loading train:  16%|█▌        | 46/285 [00:47<03:37,  1.10it/s]Loading train:  16%|█▋        | 47/285 [00:47<03:19,  1.20it/s]Loading train:  17%|█▋        | 48/285 [00:48<03:06,  1.27it/s]Loading train:  17%|█▋        | 49/285 [00:49<03:02,  1.29it/s]Loading train:  18%|█▊        | 50/285 [00:50<02:56,  1.33it/s]Loading train:  18%|█▊        | 51/285 [00:50<02:52,  1.35it/s]Loading train:  18%|█▊        | 52/285 [00:51<02:48,  1.39it/s]Loading train:  19%|█▊        | 53/285 [00:52<02:50,  1.36it/s]Loading train:  19%|█▉        | 54/285 [00:52<02:49,  1.36it/s]Loading train:  19%|█▉        | 55/285 [00:53<02:51,  1.34it/s]Loading train:  20%|█▉        | 56/285 [00:54<02:58,  1.28it/s]Loading train:  20%|██        | 57/285 [00:55<02:57,  1.28it/s]Loading train:  20%|██        | 58/285 [00:56<02:53,  1.31it/s]Loading train:  21%|██        | 59/285 [00:56<02:54,  1.30it/s]Loading train:  21%|██        | 60/285 [00:57<02:45,  1.36it/s]Loading train:  21%|██▏       | 61/285 [00:58<02:46,  1.35it/s]Loading train:  22%|██▏       | 62/285 [00:59<02:47,  1.33it/s]Loading train:  22%|██▏       | 63/285 [00:59<02:46,  1.33it/s]Loading train:  22%|██▏       | 64/285 [01:01<03:24,  1.08it/s]Loading train:  23%|██▎       | 65/285 [01:02<04:03,  1.11s/it]Loading train:  23%|██▎       | 66/285 [01:03<04:08,  1.13s/it]Loading train:  24%|██▎       | 67/285 [01:04<03:52,  1.06s/it]Loading train:  24%|██▍       | 68/285 [01:05<03:34,  1.01it/s]Loading train:  24%|██▍       | 69/285 [01:06<03:21,  1.07it/s]Loading train:  25%|██▍       | 70/285 [01:07<03:08,  1.14it/s]Loading train:  25%|██▍       | 71/285 [01:07<03:04,  1.16it/s]Loading train:  25%|██▌       | 72/285 [01:08<03:02,  1.17it/s]Loading train:  26%|██▌       | 73/285 [01:09<02:59,  1.18it/s]Loading train:  26%|██▌       | 74/285 [01:10<02:57,  1.19it/s]Loading train:  26%|██▋       | 75/285 [01:11<02:52,  1.21it/s]Loading train:  27%|██▋       | 76/285 [01:11<02:45,  1.27it/s]Loading train:  27%|██▋       | 77/285 [01:12<02:55,  1.18it/s]Loading train:  27%|██▋       | 78/285 [01:13<02:49,  1.22it/s]Loading train:  28%|██▊       | 79/285 [01:14<02:46,  1.24it/s]Loading train:  28%|██▊       | 80/285 [01:15<02:41,  1.27it/s]Loading train:  28%|██▊       | 81/285 [01:16<02:41,  1.26it/s]Loading train:  29%|██▉       | 82/285 [01:16<02:39,  1.27it/s]Loading train:  29%|██▉       | 83/285 [01:17<02:38,  1.28it/s]Loading train:  29%|██▉       | 84/285 [01:18<02:38,  1.27it/s]Loading train:  30%|██▉       | 85/285 [01:19<02:50,  1.17it/s]Loading train:  30%|███       | 86/285 [01:20<02:52,  1.15it/s]Loading train:  31%|███       | 87/285 [01:21<02:55,  1.13it/s]Loading train:  31%|███       | 88/285 [01:22<02:56,  1.12it/s]Loading train:  31%|███       | 89/285 [01:23<02:57,  1.10it/s]Loading train:  32%|███▏      | 90/285 [01:23<02:58,  1.09it/s]Loading train:  32%|███▏      | 91/285 [01:24<02:54,  1.11it/s]Loading train:  32%|███▏      | 92/285 [01:25<02:54,  1.11it/s]Loading train:  33%|███▎      | 93/285 [01:26<02:57,  1.08it/s]Loading train:  33%|███▎      | 94/285 [01:27<02:54,  1.09it/s]Loading train:  33%|███▎      | 95/285 [01:28<02:58,  1.06it/s]Loading train:  34%|███▎      | 96/285 [01:29<02:59,  1.05it/s]Loading train:  34%|███▍      | 97/285 [01:30<02:57,  1.06it/s]Loading train:  34%|███▍      | 98/285 [01:31<02:55,  1.06it/s]Loading train:  35%|███▍      | 99/285 [01:32<02:55,  1.06it/s]Loading train:  35%|███▌      | 100/285 [01:33<02:53,  1.07it/s]Loading train:  35%|███▌      | 101/285 [01:34<02:51,  1.07it/s]Loading train:  36%|███▌      | 102/285 [01:35<02:51,  1.07it/s]Loading train:  36%|███▌      | 103/285 [01:36<02:48,  1.08it/s]Loading train:  36%|███▋      | 104/285 [01:36<02:45,  1.09it/s]Loading train:  37%|███▋      | 105/285 [01:37<02:38,  1.14it/s]Loading train:  37%|███▋      | 106/285 [01:38<02:35,  1.15it/s]Loading train:  38%|███▊      | 107/285 [01:39<02:39,  1.12it/s]Loading train:  38%|███▊      | 108/285 [01:40<02:39,  1.11it/s]Loading train:  38%|███▊      | 109/285 [01:41<02:32,  1.15it/s]Loading train:  39%|███▊      | 110/285 [01:42<02:29,  1.17it/s]Loading train:  39%|███▉      | 111/285 [01:42<02:25,  1.19it/s]Loading train:  39%|███▉      | 112/285 [01:43<02:23,  1.21it/s]Loading train:  40%|███▉      | 113/285 [01:44<02:25,  1.18it/s]Loading train:  40%|████      | 114/285 [01:45<02:26,  1.17it/s]Loading train:  40%|████      | 115/285 [01:46<02:24,  1.18it/s]Loading train:  41%|████      | 116/285 [01:47<02:24,  1.17it/s]Loading train:  41%|████      | 117/285 [01:48<02:23,  1.17it/s]Loading train:  41%|████▏     | 118/285 [01:48<02:23,  1.17it/s]Loading train:  42%|████▏     | 119/285 [01:49<02:21,  1.17it/s]Loading train:  42%|████▏     | 120/285 [01:50<02:20,  1.17it/s]Loading train:  42%|████▏     | 121/285 [01:51<02:41,  1.02it/s]Loading train:  43%|████▎     | 122/285 [01:53<02:47,  1.03s/it]Loading train:  43%|████▎     | 123/285 [01:54<02:49,  1.04s/it]Loading train:  44%|████▎     | 124/285 [01:54<02:36,  1.03it/s]Loading train:  44%|████▍     | 125/285 [01:55<02:23,  1.12it/s]Loading train:  44%|████▍     | 126/285 [01:56<02:16,  1.16it/s]Loading train:  45%|████▍     | 127/285 [01:57<02:10,  1.21it/s]Loading train:  45%|████▍     | 128/285 [01:57<02:05,  1.25it/s]Loading train:  45%|████▌     | 129/285 [01:58<02:03,  1.26it/s]Loading train:  46%|████▌     | 130/285 [01:59<02:01,  1.28it/s]Loading train:  46%|████▌     | 131/285 [02:00<01:59,  1.29it/s]Loading train:  46%|████▋     | 132/285 [02:00<01:57,  1.30it/s]Loading train:  47%|████▋     | 133/285 [02:01<01:56,  1.31it/s]Loading train:  47%|████▋     | 134/285 [02:02<01:55,  1.31it/s]Loading train:  47%|████▋     | 135/285 [02:03<01:53,  1.32it/s]Loading train:  48%|████▊     | 136/285 [02:03<01:51,  1.34it/s]Loading train:  48%|████▊     | 137/285 [02:04<01:50,  1.34it/s]Loading train:  48%|████▊     | 138/285 [02:05<01:47,  1.36it/s]Loading train:  49%|████▉     | 139/285 [02:06<01:47,  1.36it/s]Loading train:  49%|████▉     | 140/285 [02:06<01:49,  1.33it/s]Loading train:  49%|████▉     | 141/285 [02:07<01:50,  1.31it/s]Loading train:  50%|████▉     | 142/285 [02:08<01:52,  1.27it/s]Loading train:  50%|█████     | 143/285 [02:09<01:47,  1.32it/s]Loading train:  51%|█████     | 144/285 [02:09<01:41,  1.39it/s]Loading train:  51%|█████     | 145/285 [02:10<01:43,  1.35it/s]Loading train:  51%|█████     | 146/285 [02:11<01:42,  1.36it/s]Loading train:  52%|█████▏    | 147/285 [02:12<01:38,  1.40it/s]Loading train:  52%|█████▏    | 148/285 [02:12<01:36,  1.42it/s]Loading train:  52%|█████▏    | 149/285 [02:13<01:35,  1.42it/s]Loading train:  53%|█████▎    | 150/285 [02:14<01:34,  1.42it/s]Loading train:  53%|█████▎    | 151/285 [02:14<01:33,  1.43it/s]Loading train:  53%|█████▎    | 152/285 [02:15<01:32,  1.44it/s]Loading train:  54%|█████▎    | 153/285 [02:16<01:31,  1.45it/s]Loading train:  54%|█████▍    | 154/285 [02:16<01:31,  1.43it/s]Loading train:  54%|█████▍    | 155/285 [02:17<01:30,  1.44it/s]Loading train:  55%|█████▍    | 156/285 [02:18<01:28,  1.46it/s]Loading train:  55%|█████▌    | 157/285 [02:18<01:30,  1.41it/s]Loading train:  55%|█████▌    | 158/285 [02:19<01:29,  1.42it/s]Loading train:  56%|█████▌    | 159/285 [02:20<01:29,  1.40it/s]Loading train:  56%|█████▌    | 160/285 [02:21<01:29,  1.39it/s]Loading train:  56%|█████▋    | 161/285 [02:21<01:28,  1.39it/s]Loading train:  57%|█████▋    | 162/285 [02:22<01:26,  1.42it/s]Loading train:  57%|█████▋    | 163/285 [02:23<01:25,  1.42it/s]Loading train:  58%|█████▊    | 164/285 [02:23<01:24,  1.44it/s]Loading train:  58%|█████▊    | 165/285 [02:24<01:23,  1.43it/s]Loading train:  58%|█████▊    | 166/285 [02:25<01:22,  1.43it/s]Loading train:  59%|█████▊    | 167/285 [02:26<01:24,  1.40it/s]Loading train:  59%|█████▉    | 168/285 [02:26<01:22,  1.41it/s]Loading train:  59%|█████▉    | 169/285 [02:27<01:19,  1.46it/s]Loading train:  60%|█████▉    | 170/285 [02:28<01:18,  1.46it/s]Loading train:  60%|██████    | 171/285 [02:28<01:19,  1.43it/s]Loading train:  60%|██████    | 172/285 [02:29<01:19,  1.43it/s]Loading train:  61%|██████    | 173/285 [02:30<01:18,  1.43it/s]Loading train:  61%|██████    | 174/285 [02:30<01:16,  1.44it/s]Loading train:  61%|██████▏   | 175/285 [02:31<01:15,  1.45it/s]Loading train:  62%|██████▏   | 176/285 [02:32<01:16,  1.43it/s]Loading train:  62%|██████▏   | 177/285 [02:33<01:16,  1.42it/s]Loading train:  62%|██████▏   | 178/285 [02:33<01:16,  1.40it/s]Loading train:  63%|██████▎   | 179/285 [02:34<01:17,  1.38it/s]Loading train:  63%|██████▎   | 180/285 [02:35<01:15,  1.38it/s]Loading train:  64%|██████▎   | 181/285 [02:35<01:14,  1.39it/s]Loading train:  64%|██████▍   | 182/285 [02:36<01:12,  1.42it/s]Loading train:  64%|██████▍   | 183/285 [02:37<01:11,  1.42it/s]Loading train:  65%|██████▍   | 184/285 [02:37<01:09,  1.44it/s]Loading train:  65%|██████▍   | 185/285 [02:38<01:08,  1.46it/s]Loading train:  65%|██████▌   | 186/285 [02:39<01:07,  1.46it/s]Loading train:  66%|██████▌   | 187/285 [02:39<01:06,  1.48it/s]Loading train:  66%|██████▌   | 188/285 [02:40<01:05,  1.49it/s]Loading train:  66%|██████▋   | 189/285 [02:41<01:03,  1.50it/s]Loading train:  67%|██████▋   | 190/285 [02:41<01:02,  1.51it/s]Loading train:  67%|██████▋   | 191/285 [02:42<01:02,  1.51it/s]Loading train:  67%|██████▋   | 192/285 [02:43<01:01,  1.50it/s]Loading train:  68%|██████▊   | 193/285 [02:43<01:02,  1.46it/s]Loading train:  68%|██████▊   | 194/285 [02:44<01:01,  1.49it/s]Loading train:  68%|██████▊   | 195/285 [02:45<01:01,  1.47it/s]Loading train:  69%|██████▉   | 196/285 [02:46<01:04,  1.38it/s]Loading train:  69%|██████▉   | 197/285 [02:46<01:05,  1.35it/s]Loading train:  69%|██████▉   | 198/285 [02:47<01:04,  1.34it/s]Loading train:  70%|██████▉   | 199/285 [02:48<01:08,  1.25it/s]Loading train:  70%|███████   | 200/285 [02:49<01:07,  1.26it/s]Loading train:  71%|███████   | 201/285 [02:50<01:04,  1.29it/s]Loading train:  71%|███████   | 202/285 [02:50<01:04,  1.30it/s]Loading train:  71%|███████   | 203/285 [02:51<01:01,  1.33it/s]Loading train:  72%|███████▏  | 204/285 [02:52<00:59,  1.36it/s]Loading train:  72%|███████▏  | 205/285 [02:53<00:59,  1.35it/s]Loading train:  72%|███████▏  | 206/285 [02:53<00:59,  1.33it/s]Loading train:  73%|███████▎  | 207/285 [02:54<01:00,  1.29it/s]Loading train:  73%|███████▎  | 208/285 [02:55<00:58,  1.31it/s]Loading train:  73%|███████▎  | 209/285 [02:56<00:57,  1.33it/s]Loading train:  74%|███████▎  | 210/285 [02:56<00:58,  1.28it/s]Loading train:  74%|███████▍  | 211/285 [02:57<00:57,  1.29it/s]Loading train:  74%|███████▍  | 212/285 [02:58<00:57,  1.27it/s]Loading train:  75%|███████▍  | 213/285 [02:59<00:56,  1.28it/s]Loading train:  75%|███████▌  | 214/285 [03:00<00:55,  1.28it/s]Loading train:  75%|███████▌  | 215/285 [03:00<00:53,  1.31it/s]Loading train:  76%|███████▌  | 216/285 [03:01<00:52,  1.32it/s]Loading train:  76%|███████▌  | 217/285 [03:02<00:51,  1.32it/s]Loading train:  76%|███████▋  | 218/285 [03:03<00:50,  1.33it/s]Loading train:  77%|███████▋  | 219/285 [03:03<00:49,  1.33it/s]Loading train:  77%|███████▋  | 220/285 [03:04<00:48,  1.33it/s]Loading train:  78%|███████▊  | 221/285 [03:05<00:47,  1.35it/s]Loading train:  78%|███████▊  | 222/285 [03:05<00:45,  1.39it/s]Loading train:  78%|███████▊  | 223/285 [03:06<00:44,  1.39it/s]Loading train:  79%|███████▊  | 224/285 [03:07<00:42,  1.42it/s]Loading train:  79%|███████▉  | 225/285 [03:08<00:42,  1.41it/s]Loading train:  79%|███████▉  | 226/285 [03:08<00:41,  1.42it/s]Loading train:  80%|███████▉  | 227/285 [03:09<00:40,  1.43it/s]Loading train:  80%|████████  | 228/285 [03:10<00:39,  1.43it/s]Loading train:  80%|████████  | 229/285 [03:10<00:38,  1.44it/s]Loading train:  81%|████████  | 230/285 [03:11<00:37,  1.45it/s]Loading train:  81%|████████  | 231/285 [03:12<00:37,  1.43it/s]Loading train:  81%|████████▏ | 232/285 [03:13<00:40,  1.31it/s]Loading train:  82%|████████▏ | 233/285 [03:14<00:42,  1.22it/s]Loading train:  82%|████████▏ | 234/285 [03:15<00:43,  1.16it/s]Loading train:  82%|████████▏ | 235/285 [03:16<00:44,  1.11it/s]Loading train:  83%|████████▎ | 236/285 [03:16<00:45,  1.09it/s]Loading train:  83%|████████▎ | 237/285 [03:17<00:44,  1.09it/s]Loading train:  84%|████████▎ | 238/285 [03:18<00:42,  1.11it/s]Loading train:  84%|████████▍ | 239/285 [03:19<00:41,  1.12it/s]Loading train:  84%|████████▍ | 240/285 [03:20<00:40,  1.10it/s]Loading train:  85%|████████▍ | 241/285 [03:21<00:39,  1.11it/s]Loading train:  85%|████████▍ | 242/285 [03:22<00:38,  1.11it/s]Loading train:  85%|████████▌ | 243/285 [03:23<00:38,  1.09it/s]Loading train:  86%|████████▌ | 244/285 [03:24<00:38,  1.06it/s]Loading train:  86%|████████▌ | 245/285 [03:25<00:38,  1.04it/s]Loading train:  86%|████████▋ | 246/285 [03:26<00:38,  1.02it/s]Loading train:  87%|████████▋ | 247/285 [03:27<00:36,  1.05it/s]Loading train:  87%|████████▋ | 248/285 [03:28<00:34,  1.06it/s]Loading train:  87%|████████▋ | 249/285 [03:29<00:33,  1.08it/s]Loading train:  88%|████████▊ | 250/285 [03:29<00:29,  1.18it/s]Loading train:  88%|████████▊ | 251/285 [03:30<00:27,  1.25it/s]Loading train:  88%|████████▊ | 252/285 [03:31<00:25,  1.32it/s]Loading train:  89%|████████▉ | 253/285 [03:31<00:23,  1.35it/s]Loading train:  89%|████████▉ | 254/285 [03:32<00:22,  1.37it/s]Loading train:  89%|████████▉ | 255/285 [03:33<00:21,  1.40it/s]Loading train:  90%|████████▉ | 256/285 [03:33<00:20,  1.39it/s]Loading train:  90%|█████████ | 257/285 [03:34<00:19,  1.44it/s]Loading train:  91%|█████████ | 258/285 [03:35<00:18,  1.47it/s]Loading train:  91%|█████████ | 259/285 [03:35<00:17,  1.47it/s]Loading train:  91%|█████████ | 260/285 [03:36<00:17,  1.45it/s]Loading train:  92%|█████████▏| 261/285 [03:37<00:16,  1.42it/s]Loading train:  92%|█████████▏| 262/285 [03:37<00:15,  1.45it/s]Loading train:  92%|█████████▏| 263/285 [03:38<00:15,  1.46it/s]Loading train:  93%|█████████▎| 264/285 [03:39<00:14,  1.43it/s]Loading train:  93%|█████████▎| 265/285 [03:40<00:14,  1.40it/s]Loading train:  93%|█████████▎| 266/285 [03:40<00:13,  1.36it/s]Loading train:  94%|█████████▎| 267/285 [03:41<00:13,  1.38it/s]Loading train:  94%|█████████▍| 268/285 [03:42<00:13,  1.23it/s]Loading train:  94%|█████████▍| 269/285 [03:43<00:13,  1.20it/s]Loading train:  95%|█████████▍| 270/285 [03:44<00:13,  1.11it/s]Loading train:  95%|█████████▌| 271/285 [03:45<00:12,  1.12it/s]Loading train:  95%|█████████▌| 272/285 [03:46<00:11,  1.11it/s]Loading train:  96%|█████████▌| 273/285 [03:47<00:10,  1.14it/s]Loading train:  96%|█████████▌| 274/285 [03:48<00:09,  1.12it/s]Loading train:  96%|█████████▋| 275/285 [03:48<00:08,  1.12it/s]Loading train:  97%|█████████▋| 276/285 [03:49<00:07,  1.13it/s]Loading train:  97%|█████████▋| 277/285 [03:50<00:07,  1.12it/s]Loading train:  98%|█████████▊| 278/285 [03:51<00:06,  1.14it/s]Loading train:  98%|█████████▊| 279/285 [03:52<00:05,  1.13it/s]Loading train:  98%|█████████▊| 280/285 [03:53<00:04,  1.12it/s]Loading train:  99%|█████████▊| 281/285 [03:54<00:03,  1.13it/s]Loading train:  99%|█████████▉| 282/285 [03:55<00:02,  1.13it/s]Loading train:  99%|█████████▉| 283/285 [03:56<00:01,  1.14it/s]Loading train: 100%|█████████▉| 284/285 [03:56<00:00,  1.12it/s]Loading train: 100%|██████████| 285/285 [03:57<00:00,  1.14it/s]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   8%|▊         | 23/285 [00:00<00:01, 220.69it/s]concatenating: train:  17%|█▋        | 48/285 [00:00<00:01, 225.75it/s]concatenating: train:  22%|██▏       | 63/285 [00:00<00:01, 195.82it/s]concatenating: train:  32%|███▏      | 90/285 [00:00<00:00, 212.56it/s]concatenating: train:  42%|████▏     | 121/285 [00:00<00:00, 234.62it/s]concatenating: train:  54%|█████▎    | 153/285 [00:00<00:00, 254.90it/s]concatenating: train:  66%|██████▌   | 188/285 [00:00<00:00, 276.24it/s]concatenating: train:  78%|███████▊  | 223/285 [00:00<00:00, 293.00it/s]concatenating: train:  90%|████████▉ | 256/285 [00:00<00:00, 302.77it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 283.13it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.28s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.26s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.19s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 124.84it/s]2019-07-07 00:17:05.255735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-07 00:17:05.255843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-07 00:17:05.255858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-07 00:17:05.255868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-07 00:17:05.256317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:10,  3.77it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:08,  4.51it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.33it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:05,  5.55it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:06,  4.93it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.55it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  4.93it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:03,  6.23it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.13it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.70it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:03,  5.09it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:02,  6.18it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.75it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  5.51it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:01,  6.17it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  5.38it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  6.54it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  7.03it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:04<00:00,  5.72it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:04<00:00,  6.36it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  5.46it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  7.70it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 4  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 80, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 80, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 80, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 80, 40)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 80, 40)   14440       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 80, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 80, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 80, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 80, 40)   14440       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 80, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 80, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 80, 40)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 80, 20)   7220        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 80, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 80, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 80, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 80, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 80, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 40, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 40, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 40, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 40, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 40, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 40, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 40, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 20, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 20, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 20, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 20, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 20, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 20, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 20, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 20, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 40, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 40, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 40, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 40, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 40, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 40, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 40)   7240        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 40)   160         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 40)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 40)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 13)   533         dropout_9[0][0]                  
==================================================================================================
Total params: 228,553
Trainable params: 84,753
Non-trainable params: 143,800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 22s - loss: 126.8120 - acc: 0.6128 - mDice: 0.0191 - val_loss: 44.7667 - val_acc: 0.9047 - val_mDice: 0.0133

Epoch 00001: val_mDice improved from -inf to 0.01333, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 14s - loss: 32.8807 - acc: 0.8668 - mDice: 0.0187 - val_loss: 15.5994 - val_acc: 0.9047 - val_mDice: 0.0142

Epoch 00002: val_mDice improved from 0.01333 to 0.01422, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 13s - loss: 15.6230 - acc: 0.8690 - mDice: 0.0182 - val_loss: 8.7485 - val_acc: 0.9047 - val_mDice: 0.0137

Epoch 00003: val_mDice did not improve from 0.01422
Epoch 4/300
 - 13s - loss: 10.8538 - acc: 0.8691 - mDice: 0.0201 - val_loss: 7.0941 - val_acc: 0.9047 - val_mDice: 0.0143

Epoch 00004: val_mDice improved from 0.01422 to 0.01432, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 14s - loss: 8.8191 - acc: 0.8692 - mDice: 0.0241 - val_loss: 6.4481 - val_acc: 0.9047 - val_mDice: 0.0165

Epoch 00005: val_mDice improved from 0.01432 to 0.01652, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 14s - loss: 7.6489 - acc: 0.8692 - mDice: 0.0305 - val_loss: 5.7960 - val_acc: 0.9047 - val_mDice: 0.0302

Epoch 00006: val_mDice improved from 0.01652 to 0.03024, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 13s - loss: 6.8570 - acc: 0.8692 - mDice: 0.0389 - val_loss: 5.4746 - val_acc: 0.9048 - val_mDice: 0.0392

Epoch 00007: val_mDice improved from 0.03024 to 0.03923, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 13s - loss: 6.2854 - acc: 0.8692 - mDice: 0.0481 - val_loss: 5.2610 - val_acc: 0.9048 - val_mDice: 0.0516

Epoch 00008: val_mDice improved from 0.03923 to 0.05156, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 13s - loss: 5.7905 - acc: 0.8692 - mDice: 0.0592 - val_loss: 4.8239 - val_acc: 0.9048 - val_mDice: 0.0699

Epoch 00009: val_mDice improved from 0.05156 to 0.06986, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 14s - loss: 5.3357 - acc: 0.8692 - mDice: 0.0734 - val_loss: 4.6760 - val_acc: 0.9048 - val_mDice: 0.0796

Epoch 00010: val_mDice improved from 0.06986 to 0.07955, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 13s - loss: 4.9427 - acc: 0.8693 - mDice: 0.0904 - val_loss: 4.2234 - val_acc: 0.9046 - val_mDice: 0.1090

Epoch 00011: val_mDice improved from 0.07955 to 0.10902, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 13s - loss: 4.6102 - acc: 0.8698 - mDice: 0.1087 - val_loss: 4.0939 - val_acc: 0.9044 - val_mDice: 0.1248

Epoch 00012: val_mDice improved from 0.10902 to 0.12476, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 13s - loss: 4.3255 - acc: 0.8707 - mDice: 0.1286 - val_loss: 3.6850 - val_acc: 0.9042 - val_mDice: 0.1550

Epoch 00013: val_mDice improved from 0.12476 to 0.15496, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 13s - loss: 4.0708 - acc: 0.8724 - mDice: 0.1487 - val_loss: 3.7183 - val_acc: 0.9046 - val_mDice: 0.1642

Epoch 00014: val_mDice improved from 0.15496 to 0.16419, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 13s - loss: 3.8515 - acc: 0.8745 - mDice: 0.1698 - val_loss: 4.0180 - val_acc: 0.9034 - val_mDice: 0.1629

Epoch 00015: val_mDice did not improve from 0.16419
Epoch 16/300
 - 14s - loss: 3.6639 - acc: 0.8771 - mDice: 0.1904 - val_loss: 3.4242 - val_acc: 0.9078 - val_mDice: 0.2079

Epoch 00016: val_mDice improved from 0.16419 to 0.20789, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 13s - loss: 3.4780 - acc: 0.8800 - mDice: 0.2137 - val_loss: 3.1472 - val_acc: 0.9114 - val_mDice: 0.2431

Epoch 00017: val_mDice improved from 0.20789 to 0.24310, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 13s - loss: 3.3169 - acc: 0.8825 - mDice: 0.2364 - val_loss: 3.1243 - val_acc: 0.9133 - val_mDice: 0.2567

Epoch 00018: val_mDice improved from 0.24310 to 0.25671, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 13s - loss: 3.1655 - acc: 0.8859 - mDice: 0.2588 - val_loss: 3.3758 - val_acc: 0.9072 - val_mDice: 0.2566

Epoch 00019: val_mDice did not improve from 0.25671
Epoch 20/300
 - 13s - loss: 3.0409 - acc: 0.8892 - mDice: 0.2791 - val_loss: 3.1511 - val_acc: 0.9192 - val_mDice: 0.2919

Epoch 00020: val_mDice improved from 0.25671 to 0.29191, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 13s - loss: 2.9168 - acc: 0.8935 - mDice: 0.3001 - val_loss: 3.0279 - val_acc: 0.9226 - val_mDice: 0.3030

Epoch 00021: val_mDice improved from 0.29191 to 0.30300, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 13s - loss: 2.8127 - acc: 0.8973 - mDice: 0.3190 - val_loss: 2.9808 - val_acc: 0.9278 - val_mDice: 0.3241

Epoch 00022: val_mDice improved from 0.30300 to 0.32406, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 14s - loss: 2.7286 - acc: 0.9005 - mDice: 0.3350 - val_loss: 2.9871 - val_acc: 0.9287 - val_mDice: 0.3322

Epoch 00023: val_mDice improved from 0.32406 to 0.33219, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 13s - loss: 2.6452 - acc: 0.9033 - mDice: 0.3510 - val_loss: 2.8528 - val_acc: 0.9307 - val_mDice: 0.3582

Epoch 00024: val_mDice improved from 0.33219 to 0.35815, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 13s - loss: 2.5799 - acc: 0.9058 - mDice: 0.3649 - val_loss: 3.7411 - val_acc: 0.9271 - val_mDice: 0.3182

Epoch 00025: val_mDice did not improve from 0.35815
Epoch 26/300
 - 13s - loss: 2.5170 - acc: 0.9077 - mDice: 0.3775 - val_loss: 2.8511 - val_acc: 0.9356 - val_mDice: 0.3811

Epoch 00026: val_mDice improved from 0.35815 to 0.38108, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 13s - loss: 2.4445 - acc: 0.9100 - mDice: 0.3926 - val_loss: 3.0422 - val_acc: 0.9359 - val_mDice: 0.3774

Epoch 00027: val_mDice did not improve from 0.38108
Epoch 28/300
 - 13s - loss: 2.3934 - acc: 0.9117 - mDice: 0.4031 - val_loss: 2.8120 - val_acc: 0.9351 - val_mDice: 0.3922

Epoch 00028: val_mDice improved from 0.38108 to 0.39218, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 14s - loss: 2.3436 - acc: 0.9133 - mDice: 0.4137 - val_loss: 2.8656 - val_acc: 0.9390 - val_mDice: 0.3992

Epoch 00029: val_mDice improved from 0.39218 to 0.39917, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 14s - loss: 2.2949 - acc: 0.9147 - mDice: 0.4234 - val_loss: 2.8036 - val_acc: 0.9385 - val_mDice: 0.4123

Epoch 00030: val_mDice improved from 0.39917 to 0.41228, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 13s - loss: 2.2562 - acc: 0.9160 - mDice: 0.4311 - val_loss: 3.0166 - val_acc: 0.9378 - val_mDice: 0.3924

Epoch 00031: val_mDice did not improve from 0.41228
Epoch 32/300
 - 13s - loss: 2.2106 - acc: 0.9174 - mDice: 0.4415 - val_loss: 2.9399 - val_acc: 0.9361 - val_mDice: 0.3996

Epoch 00032: val_mDice did not improve from 0.41228
Epoch 33/300
 - 13s - loss: 2.1758 - acc: 0.9184 - mDice: 0.4488 - val_loss: 3.8754 - val_acc: 0.9330 - val_mDice: 0.3578

Epoch 00033: val_mDice did not improve from 0.41228
Epoch 34/300
 - 13s - loss: 2.1492 - acc: 0.9195 - mDice: 0.4557 - val_loss: 2.9156 - val_acc: 0.9363 - val_mDice: 0.4047

Epoch 00034: val_mDice did not improve from 0.41228
Epoch 35/300
 - 13s - loss: 2.1127 - acc: 0.9206 - mDice: 0.4631 - val_loss: 2.7646 - val_acc: 0.9352 - val_mDice: 0.4198

Epoch 00035: val_mDice improved from 0.41228 to 0.41976, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 14s - loss: 2.0867 - acc: 0.9216 - mDice: 0.4687 - val_loss: 2.8629 - val_acc: 0.9391 - val_mDice: 0.4241

Epoch 00036: val_mDice improved from 0.41976 to 0.42415, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 14s - loss: 2.0558 - acc: 0.9224 - mDice: 0.4753 - val_loss: 2.7878 - val_acc: 0.9380 - val_mDice: 0.4349

Epoch 00037: val_mDice improved from 0.42415 to 0.43492, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 13s - loss: 2.0300 - acc: 0.9235 - mDice: 0.4812 - val_loss: 3.1910 - val_acc: 0.9404 - val_mDice: 0.4199

Epoch 00038: val_mDice did not improve from 0.43492
Epoch 39/300
 - 14s - loss: 2.0052 - acc: 0.9242 - mDice: 0.4873 - val_loss: 2.7297 - val_acc: 0.9358 - val_mDice: 0.4312

Epoch 00039: val_mDice did not improve from 0.43492
Epoch 40/300
 - 14s - loss: 1.9797 - acc: 0.9251 - mDice: 0.4931 - val_loss: 2.7587 - val_acc: 0.9363 - val_mDice: 0.4292

Epoch 00040: val_mDice did not improve from 0.43492
Epoch 41/300
 - 13s - loss: 1.9503 - acc: 0.9258 - mDice: 0.4990 - val_loss: 2.8256 - val_acc: 0.9396 - val_mDice: 0.4406

Epoch 00041: val_mDice improved from 0.43492 to 0.44060, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 14s - loss: 1.9267 - acc: 0.9267 - mDice: 0.5049 - val_loss: 3.0495 - val_acc: 0.9350 - val_mDice: 0.4115

Epoch 00042: val_mDice did not improve from 0.44060
Epoch 43/300
 - 14s - loss: 1.9172 - acc: 0.9269 - mDice: 0.5073 - val_loss: 3.0778 - val_acc: 0.9392 - val_mDice: 0.4312

Epoch 00043: val_mDice did not improve from 0.44060
Epoch 44/300
 - 14s - loss: 1.8912 - acc: 0.9277 - mDice: 0.5137 - val_loss: 2.8757 - val_acc: 0.9399 - val_mDice: 0.4436

Epoch 00044: val_mDice improved from 0.44060 to 0.44358, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 14s - loss: 1.8727 - acc: 0.9284 - mDice: 0.5171 - val_loss: 3.0994 - val_acc: 0.9411 - val_mDice: 0.4344

Epoch 00045: val_mDice did not improve from 0.44358
Epoch 46/300
 - 13s - loss: 1.8576 - acc: 0.9288 - mDice: 0.5217 - val_loss: 3.0609 - val_acc: 0.9395 - val_mDice: 0.4326

Epoch 00046: val_mDice did not improve from 0.44358
Epoch 47/300
 - 14s - loss: 1.8432 - acc: 0.9292 - mDice: 0.5249 - val_loss: 3.0327 - val_acc: 0.9412 - val_mDice: 0.4412

Epoch 00047: val_mDice did not improve from 0.44358
Epoch 48/300
 - 14s - loss: 1.8217 - acc: 0.9298 - mDice: 0.5298 - val_loss: 3.1598 - val_acc: 0.9412 - val_mDice: 0.4313

Epoch 00048: val_mDice did not improve from 0.44358
Epoch 49/300
 - 14s - loss: 1.8185 - acc: 0.9300 - mDice: 0.5314 - val_loss: 3.1217 - val_acc: 0.9413 - val_mDice: 0.4407

Epoch 00049: val_mDice did not improve from 0.44358
Epoch 50/300
 - 14s - loss: 1.8026 - acc: 0.9304 - mDice: 0.5343 - val_loss: 3.0233 - val_acc: 0.9417 - val_mDice: 0.4383

Epoch 00050: val_mDice did not improve from 0.44358
Epoch 51/300
 - 14s - loss: 1.7837 - acc: 0.9309 - mDice: 0.5389 - val_loss: 3.5434 - val_acc: 0.9417 - val_mDice: 0.4248

Epoch 00051: val_mDice did not improve from 0.44358
Epoch 52/300
 - 14s - loss: 1.7716 - acc: 0.9314 - mDice: 0.5421 - val_loss: 3.2769 - val_acc: 0.9355 - val_mDice: 0.4204

Epoch 00052: val_mDice did not improve from 0.44358
Epoch 53/300
 - 14s - loss: 1.7581 - acc: 0.9318 - mDice: 0.5454 - val_loss: 3.0258 - val_acc: 0.9376 - val_mDice: 0.4306

Epoch 00053: val_mDice did not improve from 0.44358
Epoch 54/300
 - 14s - loss: 1.7503 - acc: 0.9319 - mDice: 0.5467 - val_loss: 2.9226 - val_acc: 0.9402 - val_mDice: 0.4469

Epoch 00054: val_mDice improved from 0.44358 to 0.44694, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 14s - loss: 1.7306 - acc: 0.9327 - mDice: 0.5519 - val_loss: 3.0749 - val_acc: 0.9427 - val_mDice: 0.4496

Epoch 00055: val_mDice improved from 0.44694 to 0.44965, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 14s - loss: 1.7272 - acc: 0.9328 - mDice: 0.5531 - val_loss: 3.1075 - val_acc: 0.9424 - val_mDice: 0.4473

Epoch 00056: val_mDice did not improve from 0.44965
Epoch 57/300
 - 14s - loss: 1.7156 - acc: 0.9334 - mDice: 0.5559 - val_loss: 3.2941 - val_acc: 0.9426 - val_mDice: 0.4377

Epoch 00057: val_mDice did not improve from 0.44965
Epoch 58/300
 - 14s - loss: 1.7130 - acc: 0.9333 - mDice: 0.5565 - val_loss: 3.1797 - val_acc: 0.9408 - val_mDice: 0.4456

Epoch 00058: val_mDice did not improve from 0.44965
Epoch 59/300
 - 14s - loss: 1.7011 - acc: 0.9337 - mDice: 0.5587 - val_loss: 3.0329 - val_acc: 0.9423 - val_mDice: 0.4536

Epoch 00059: val_mDice improved from 0.44965 to 0.45357, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 60/300
 - 14s - loss: 1.6881 - acc: 0.9340 - mDice: 0.5616 - val_loss: 3.4762 - val_acc: 0.9425 - val_mDice: 0.4347

Epoch 00060: val_mDice did not improve from 0.45357
Epoch 61/300
 - 14s - loss: 1.6764 - acc: 0.9343 - mDice: 0.5639 - val_loss: 3.5600 - val_acc: 0.9429 - val_mDice: 0.4404

Epoch 00061: val_mDice did not improve from 0.45357
Epoch 62/300
 - 14s - loss: 1.6557 - acc: 0.9350 - mDice: 0.5696 - val_loss: 3.3264 - val_acc: 0.9430 - val_mDice: 0.4423

Epoch 00062: val_mDice did not improve from 0.45357
Epoch 63/300
 - 14s - loss: 1.6621 - acc: 0.9349 - mDice: 0.5685 - val_loss: 3.0934 - val_acc: 0.9418 - val_mDice: 0.4496

Epoch 00063: val_mDice did not improve from 0.45357
Epoch 64/300
 - 14s - loss: 1.6497 - acc: 0.9351 - mDice: 0.5706 - val_loss: 3.3009 - val_acc: 0.9435 - val_mDice: 0.4520

Epoch 00064: val_mDice did not improve from 0.45357
Epoch 65/300
 - 14s - loss: 1.6326 - acc: 0.9358 - mDice: 0.5756 - val_loss: 3.1139 - val_acc: 0.9445 - val_mDice: 0.4606

Epoch 00065: val_mDice improved from 0.45357 to 0.46059, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 66/300
 - 14s - loss: 1.6237 - acc: 0.9359 - mDice: 0.5772 - val_loss: 3.0726 - val_acc: 0.9368 - val_mDice: 0.4483

Epoch 00066: val_mDice did not improve from 0.46059
Epoch 67/300
 - 14s - loss: 1.6290 - acc: 0.9358 - mDice: 0.5766 - val_loss: 3.3586 - val_acc: 0.9432 - val_mDice: 0.4486

Epoch 00067: val_mDice did not improve from 0.46059
Epoch 68/300
 - 14s - loss: 1.6178 - acc: 0.9362 - mDice: 0.5790 - val_loss: 3.3915 - val_acc: 0.9358 - val_mDice: 0.4231

Epoch 00068: val_mDice did not improve from 0.46059
Epoch 69/300
 - 14s - loss: 1.6071 - acc: 0.9365 - mDice: 0.5818 - val_loss: 3.1440 - val_acc: 0.9426 - val_mDice: 0.4544

Epoch 00069: val_mDice did not improve from 0.46059
Epoch 70/300
 - 14s - loss: 1.6006 - acc: 0.9366 - mDice: 0.5830 - val_loss: 3.3059 - val_acc: 0.9392 - val_mDice: 0.4378

Epoch 00070: val_mDice did not improve from 0.46059
Epoch 71/300
 - 14s - loss: 1.5994 - acc: 0.9367 - mDice: 0.5841 - val_loss: 3.1247 - val_acc: 0.9414 - val_mDice: 0.4641

Epoch 00071: val_mDice improved from 0.46059 to 0.46411, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 72/300
 - 14s - loss: 1.5887 - acc: 0.9368 - mDice: 0.5858 - val_loss: 3.1971 - val_acc: 0.9382 - val_mDice: 0.4520

Epoch 00072: val_mDice did not improve from 0.46411
Epoch 73/300
 - 14s - loss: 1.5796 - acc: 0.9373 - mDice: 0.5880 - val_loss: 3.2862 - val_acc: 0.9429 - val_mDice: 0.4568

Epoch 00073: val_mDice did not improve from 0.46411
Epoch 74/300
 - 14s - loss: 1.5769 - acc: 0.9372 - mDice: 0.5888 - val_loss: 3.7085 - val_acc: 0.9432 - val_mDice: 0.4410

Epoch 00074: val_mDice did not improve from 0.46411
Epoch 75/300
 - 14s - loss: 1.5685 - acc: 0.9374 - mDice: 0.5911 - val_loss: 3.4225 - val_acc: 0.9400 - val_mDice: 0.4496

Epoch 00075: val_mDice did not improve from 0.46411
Epoch 76/300
 - 14s - loss: 1.5575 - acc: 0.9381 - mDice: 0.5947 - val_loss: 3.1873 - val_acc: 0.9422 - val_mDice: 0.4549

Epoch 00076: val_mDice did not improve from 0.46411
Epoch 77/300
 - 14s - loss: 1.5520 - acc: 0.9379 - mDice: 0.5948 - val_loss: 3.1550 - val_acc: 0.9435 - val_mDice: 0.4587

Epoch 00077: val_mDice did not improve from 0.46411
Epoch 78/300
 - 14s - loss: 1.5469 - acc: 0.9382 - mDice: 0.5964 - val_loss: 3.3370 - val_acc: 0.9442 - val_mDice: 0.4510

Epoch 00078: val_mDice did not improve from 0.46411
Epoch 79/300
 - 14s - loss: 1.5377 - acc: 0.9383 - mDice: 0.5986 - val_loss: 3.3653 - val_acc: 0.9431 - val_mDice: 0.4547

Epoch 00079: val_mDice did not improve from 0.46411
Epoch 80/300
 - 13s - loss: 1.5305 - acc: 0.9386 - mDice: 0.6004 - val_loss: 3.5164 - val_acc: 0.9438 - val_mDice: 0.4476

Epoch 00080: val_mDice did not improve from 0.46411
Epoch 81/300
 - 14s - loss: 1.5225 - acc: 0.9388 - mDice: 0.6027 - val_loss: 3.5224 - val_acc: 0.9408 - val_mDice: 0.4398

Epoch 00081: val_mDice did not improve from 0.46411
Epoch 82/300
 - 14s - loss: 1.5231 - acc: 0.9386 - mDice: 0.6024 - val_loss: 3.2679 - val_acc: 0.9421 - val_mDice: 0.4588

Epoch 00082: val_mDice did not improve from 0.46411
Epoch 83/300
 - 14s - loss: 1.5147 - acc: 0.9391 - mDice: 0.6047 - val_loss: 3.3394 - val_acc: 0.9414 - val_mDice: 0.4479

Epoch 00083: val_mDice did not improve from 0.46411
Epoch 84/300
 - 14s - loss: 1.5087 - acc: 0.9392 - mDice: 0.6059 - val_loss: 3.1489 - val_acc: 0.9410 - val_mDice: 0.4606

Epoch 00084: val_mDice did not improve from 0.46411
Epoch 85/300
 - 14s - loss: 1.5055 - acc: 0.9392 - mDice: 0.6070 - val_loss: 3.2450 - val_acc: 0.9389 - val_mDice: 0.4500

Epoch 00085: val_mDice did not improve from 0.46411
Epoch 86/300
 - 14s - loss: 1.5011 - acc: 0.9394 - mDice: 0.6076 - val_loss: 3.2122 - val_acc: 0.9436 - val_mDice: 0.4668

Epoch 00086: val_mDice improved from 0.46411 to 0.46682, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 87/300
 - 14s - loss: 1.4936 - acc: 0.9394 - mDice: 0.6098 - val_loss: 3.4781 - val_acc: 0.9443 - val_mDice: 0.4563

Epoch 00087: val_mDice did not improve from 0.46682
Epoch 88/300
 - 14s - loss: 1.4923 - acc: 0.9395 - mDice: 0.6103 - val_loss: 3.2460 - val_acc: 0.9451 - val_mDice: 0.4691

Epoch 00088: val_mDice improved from 0.46682 to 0.46911, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 89/300
 - 14s - loss: 1.4788 - acc: 0.9398 - mDice: 0.6129 - val_loss: 3.2392 - val_acc: 0.9413 - val_mDice: 0.4602

Epoch 00089: val_mDice did not improve from 0.46911
Epoch 90/300
 - 14s - loss: 1.4796 - acc: 0.9398 - mDice: 0.6128 - val_loss: 3.1927 - val_acc: 0.9400 - val_mDice: 0.4572

Epoch 00090: val_mDice did not improve from 0.46911
Epoch 91/300
 - 14s - loss: 1.4776 - acc: 0.9400 - mDice: 0.6136 - val_loss: 3.4973 - val_acc: 0.9431 - val_mDice: 0.4538

Epoch 00091: val_mDice did not improve from 0.46911
Epoch 92/300
 - 14s - loss: 1.4725 - acc: 0.9400 - mDice: 0.6143 - val_loss: 3.3758 - val_acc: 0.9425 - val_mDice: 0.4558

Epoch 00092: val_mDice did not improve from 0.46911
Epoch 93/300
 - 14s - loss: 1.4679 - acc: 0.9400 - mDice: 0.6160 - val_loss: 3.6062 - val_acc: 0.9421 - val_mDice: 0.4453

Epoch 00093: val_mDice did not improve from 0.46911
Epoch 94/300
 - 14s - loss: 1.4694 - acc: 0.9401 - mDice: 0.6155 - val_loss: 3.3832 - val_acc: 0.9432 - val_mDice: 0.4592

Epoch 00094: val_mDice did not improve from 0.46911
Epoch 95/300
 - 14s - loss: 1.4588 - acc: 0.9405 - mDice: 0.6176 - val_loss: 3.4835 - val_acc: 0.9445 - val_mDice: 0.4612

Epoch 00095: val_mDice did not improve from 0.46911
Epoch 96/300
 - 14s - loss: 1.4463 - acc: 0.9407 - mDice: 0.6209 - val_loss: 3.4055 - val_acc: 0.9438 - val_mDice: 0.4610

Epoch 00096: val_mDice did not improve from 0.46911
Epoch 97/300
 - 14s - loss: 1.4543 - acc: 0.9406 - mDice: 0.6191 - val_loss: 3.7695 - val_acc: 0.9420 - val_mDice: 0.4385

Epoch 00097: val_mDice did not improve from 0.46911
Epoch 98/300
 - 15s - loss: 1.4431 - acc: 0.9410 - mDice: 0.6214 - val_loss: 3.3690 - val_acc: 0.9412 - val_mDice: 0.4597

Epoch 00098: val_mDice did not improve from 0.46911
Epoch 99/300
 - 15s - loss: 1.4427 - acc: 0.9411 - mDice: 0.6219 - val_loss: 3.3820 - val_acc: 0.9435 - val_mDice: 0.4585

Epoch 00099: val_mDice did not improve from 0.46911
Epoch 100/300
 - 16s - loss: 1.4349 - acc: 0.9413 - mDice: 0.6238 - val_loss: 3.1771 - val_acc: 0.9417 - val_mDice: 0.4608

Epoch 00100: val_mDice did not improve from 0.46911
Epoch 101/300
 - 15s - loss: 1.4320 - acc: 0.9414 - mDice: 0.6240 - val_loss: 3.3181 - val_acc: 0.9439 - val_mDice: 0.4658

Epoch 00101: val_mDice did not improve from 0.46911
Epoch 102/300
 - 15s - loss: 1.4197 - acc: 0.9418 - mDice: 0.6271 - val_loss: 3.1863 - val_acc: 0.9424 - val_mDice: 0.4725

Epoch 00102: val_mDice improved from 0.46911 to 0.47251, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 103/300
 - 15s - loss: 1.4206 - acc: 0.9417 - mDice: 0.6278 - val_loss: 3.3576 - val_acc: 0.9392 - val_mDice: 0.4602

Epoch 00103: val_mDice did not improve from 0.47251
Epoch 104/300
 - 16s - loss: 1.4153 - acc: 0.9418 - mDice: 0.6280 - val_loss: 3.2755 - val_acc: 0.9406 - val_mDice: 0.4598

Epoch 00104: val_mDice did not improve from 0.47251
Epoch 105/300
 - 15s - loss: 1.4173 - acc: 0.9418 - mDice: 0.6283 - val_loss: 3.2142 - val_acc: 0.9442 - val_mDice: 0.4698

Epoch 00105: val_mDice did not improve from 0.47251
Epoch 106/300
 - 15s - loss: 1.4119 - acc: 0.9420 - mDice: 0.6296 - val_loss: 3.5491 - val_acc: 0.9403 - val_mDice: 0.4438

Epoch 00106: val_mDice did not improve from 0.47251
Epoch 107/300
 - 16s - loss: 1.4066 - acc: 0.9421 - mDice: 0.6306 - val_loss: 3.7035 - val_acc: 0.9430 - val_mDice: 0.4519

Epoch 00107: val_mDice did not improve from 0.47251
Epoch 108/300
 - 16s - loss: 1.4080 - acc: 0.9420 - mDice: 0.6296 - val_loss: 3.3580 - val_acc: 0.9427 - val_mDice: 0.4630

Epoch 00108: val_mDice did not improve from 0.47251
Epoch 109/300
 - 15s - loss: 1.4029 - acc: 0.9422 - mDice: 0.6317 - val_loss: 3.3951 - val_acc: 0.9424 - val_mDice: 0.4618

Epoch 00109: val_mDice did not improve from 0.47251
Epoch 110/300
 - 15s - loss: 1.4013 - acc: 0.9422 - mDice: 0.6317 - val_loss: 3.3613 - val_acc: 0.9440 - val_mDice: 0.4667

Epoch 00110: val_mDice did not improve from 0.47251
Epoch 111/300
 - 15s - loss: 1.3998 - acc: 0.9424 - mDice: 0.6316 - val_loss: 3.7354 - val_acc: 0.9419 - val_mDice: 0.4503

Epoch 00111: val_mDice did not improve from 0.47251
Epoch 112/300
 - 16s - loss: 1.3894 - acc: 0.9426 - mDice: 0.6348 - val_loss: 3.3402 - val_acc: 0.9414 - val_mDice: 0.4639

Epoch 00112: val_mDice did not improve from 0.47251
Epoch 113/300
 - 19s - loss: 1.3951 - acc: 0.9425 - mDice: 0.6330 - val_loss: 3.4643 - val_acc: 0.9411 - val_mDice: 0.4452

Epoch 00113: val_mDice did not improve from 0.47251
Epoch 114/300
 - 19s - loss: 1.3884 - acc: 0.9425 - mDice: 0.6349 - val_loss: 3.6659 - val_acc: 0.9369 - val_mDice: 0.4343

Epoch 00114: val_mDice did not improve from 0.47251
Epoch 115/300
 - 18s - loss: 1.3863 - acc: 0.9427 - mDice: 0.6359 - val_loss: 3.3041 - val_acc: 0.9442 - val_mDice: 0.4686

Epoch 00115: val_mDice did not improve from 0.47251
Epoch 116/300
 - 18s - loss: 1.3769 - acc: 0.9429 - mDice: 0.6375 - val_loss: 3.3247 - val_acc: 0.9424 - val_mDice: 0.4657

Epoch 00116: val_mDice did not improve from 0.47251
Epoch 117/300
 - 16s - loss: 1.3852 - acc: 0.9429 - mDice: 0.6356 - val_loss: 3.6139 - val_acc: 0.9431 - val_mDice: 0.4577

Epoch 00117: val_mDice did not improve from 0.47251
Epoch 118/300
 - 15s - loss: 1.3711 - acc: 0.9431 - mDice: 0.6393 - val_loss: 3.2799 - val_acc: 0.9422 - val_mDice: 0.4687

Epoch 00118: val_mDice did not improve from 0.47251
Epoch 119/300
 - 15s - loss: 1.3667 - acc: 0.9432 - mDice: 0.6402 - val_loss: 3.6069 - val_acc: 0.9379 - val_mDice: 0.4347

Epoch 00119: val_mDice did not improve from 0.47251
Epoch 120/300
 - 15s - loss: 1.3714 - acc: 0.9431 - mDice: 0.6389 - val_loss: 3.3894 - val_acc: 0.9415 - val_mDice: 0.4648

Epoch 00120: val_mDice did not improve from 0.47251
Epoch 121/300
 - 15s - loss: 1.3693 - acc: 0.9432 - mDice: 0.6397 - val_loss: 3.5315 - val_acc: 0.9402 - val_mDice: 0.4575

Epoch 00121: val_mDice did not improve from 0.47251
Epoch 122/300
 - 15s - loss: 1.3719 - acc: 0.9432 - mDice: 0.6396 - val_loss: 3.4159 - val_acc: 0.9391 - val_mDice: 0.4603

Epoch 00122: val_mDice did not improve from 0.47251
Epoch 123/300
 - 15s - loss: 1.3624 - acc: 0.9432 - mDice: 0.6409 - val_loss: 3.3593 - val_acc: 0.9445 - val_mDice: 0.4671

Epoch 00123: val_mDice did not improve from 0.47251
Epoch 124/300
 - 15s - loss: 1.3599 - acc: 0.9434 - mDice: 0.6420 - val_loss: 3.4474 - val_acc: 0.9440 - val_mDice: 0.4619

Epoch 00124: val_mDice did not improve from 0.47251
Epoch 125/300
 - 15s - loss: 1.3489 - acc: 0.9436 - mDice: 0.6438 - val_loss: 3.5273 - val_acc: 0.9428 - val_mDice: 0.4644

Epoch 00125: val_mDice did not improve from 0.47251
Epoch 126/300
 - 15s - loss: 1.3480 - acc: 0.9438 - mDice: 0.6451 - val_loss: 3.6194 - val_acc: 0.9417 - val_mDice: 0.4521

Epoch 00126: val_mDice did not improve from 0.47251
Epoch 127/300
 - 15s - loss: 1.3483 - acc: 0.9437 - mDice: 0.6448 - val_loss: 3.4885 - val_acc: 0.9413 - val_mDice: 0.4552

Epoch 00127: val_mDice did not improve from 0.47251
Epoch 128/300
 - 15s - loss: 1.3474 - acc: 0.9437 - mDice: 0.6447 - val_loss: 3.6288 - val_acc: 0.9449 - val_mDice: 0.4678

Epoch 00128: val_mDice did not improve from 0.47251
Epoch 129/300
 - 15s - loss: 1.3454 - acc: 0.9439 - mDice: 0.6457 - val_loss: 3.6485 - val_acc: 0.9439 - val_mDice: 0.4585

Epoch 00129: val_mDice did not improve from 0.47251
Epoch 130/300
 - 15s - loss: 1.3414 - acc: 0.9440 - mDice: 0.6463 - val_loss: 3.5228 - val_acc: 0.9430 - val_mDice: 0.4592

Epoch 00130: val_mDice did not improve from 0.47251
Epoch 131/300
 - 15s - loss: 1.3359 - acc: 0.9441 - mDice: 0.6479 - val_loss: 3.9640 - val_acc: 0.9426 - val_mDice: 0.4476

Epoch 00131: val_mDice did not improve from 0.47251
Epoch 132/300
 - 15s - loss: 1.3354 - acc: 0.9440 - mDice: 0.6481 - val_loss: 3.7652 - val_acc: 0.9427 - val_mDice: 0.4494

Epoch 00132: val_mDice did not improve from 0.47251
Restoring model weights from the end of the best epoch
Epoch 00132: early stopping
{'val_loss': [44.766712290900095, 15.599439936024803, 8.748485445976257, 7.09413403840292, 6.448080290995893, 5.7960421198180745, 5.474624803023679, 5.260973800683305, 4.823858939466023, 4.675957896170162, 4.2234203318754835, 4.093929424881935, 3.684986999701886, 3.718299381967102, 4.017990382210839, 3.4242156988808086, 3.1471613006932393, 3.1243082147446417, 3.375755053013563, 3.151077289311659, 3.027888382385884, 2.98080564973255, 2.987099183102449, 2.8528213842461505, 3.741073980705724, 2.851108171178826, 3.0421612553653263, 2.811970013504227, 2.865594885134626, 2.803621627434733, 3.0165887679461214, 2.939919108303175, 3.8753948998637497, 2.9156485743199787, 2.764603915535623, 2.8628756388400993, 2.787793874252765, 3.1910290049743795, 2.729651520006536, 2.75873384815419, 2.825602512496213, 3.0495091283782605, 3.0777621190728888, 2.8757259911813198, 3.0994048112160746, 3.0609278486420712, 3.032684155340706, 3.159765818333697, 3.1216985160440562, 3.0232653228033866, 3.5434311556053304, 3.2769371116799966, 3.0258492899703837, 2.9225687882197753, 3.0748577248305082, 3.107507829110892, 3.294069624239845, 3.1796626485218957, 3.0328903235938576, 3.476240714967606, 3.560043031926311, 3.3264182658333863, 3.093434643754292, 3.300934807818738, 3.113946166820824, 3.0726145006538856, 3.3586180847182514, 3.3914660638907836, 3.1439650111077797, 3.305866675185306, 3.1247181358215, 3.1970620398808802, 3.286150442281117, 3.7085223382205834, 3.4225258307886266, 3.18732342846869, 3.1549766302064417, 3.336992728390864, 3.365330368036493, 3.516438855612207, 3.5224156638252593, 3.26792271767876, 3.339378884461309, 3.1488959842051067, 3.2449764043004032, 3.212236571391778, 3.4781495605018877, 3.2459898743157587, 3.239209124552352, 3.192658720937158, 3.497327022564908, 3.375812880428774, 3.6062363845606646, 3.3831892723572397, 3.4834690662322654, 3.405539557204715, 3.7695253336181245, 3.369023401068435, 3.3820457196395313, 3.1771394162483158, 3.3181272980845753, 3.1863335520472553, 3.357596726023725, 3.2755303850530515, 3.2142035319869007, 3.5490687527871203, 3.7034919041519365, 3.35801535906891, 3.3951142734759268, 3.3612755931292972, 3.735351904561477, 3.3401696033154926, 3.4643395340203176, 3.6658897115405473, 3.3041211844732365, 3.3247295977281675, 3.6138546521376287, 3.2798774056580093, 3.6069460016276156, 3.3894330287973085, 3.5314686775118824, 3.4159058293416384, 3.3592989534095286, 3.447391044703268, 3.5272942410249795, 3.6194426310470416, 3.4884896290798983, 3.6288212494525527, 3.64852593280375, 3.522770263210294, 3.9639653967516053, 3.765163899576735], 'val_acc': [0.904739036446526, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047939294860476, 0.9048191337358384, 0.9048122650101071, 0.9047825194540478, 0.9046474155925569, 0.9043749939827692, 0.9041712596302941, 0.9046016619319007, 0.9033882816632589, 0.9077816009521484, 0.9113782019842238, 0.9132738198552813, 0.9072458829198565, 0.9191735443614778, 0.9226327992620922, 0.9278456880932763, 0.9286859035491943, 0.9307486358143034, 0.9270970821380615, 0.9355906815755934, 0.9358516420636859, 0.935114446140471, 0.9389675089291164, 0.9385393971488589, 0.9378227988878886, 0.9360806118874323, 0.9330402669452486, 0.9362843206950596, 0.9352381030718485, 0.9390888043812343, 0.9380265729767936, 0.9404464023453849, 0.9357990054857164, 0.9362706144650778, 0.9396039361045474, 0.9350434995832897, 0.939178120522272, 0.9399038695153736, 0.941149285861424, 0.9395352743920826, 0.9412431177638826, 0.9411721683683849, 0.9413484391712007, 0.9416964224406651, 0.9416643636567252, 0.9355128010114034, 0.9376007346879869, 0.9402358077821278, 0.9427152077356974, 0.9423924173627581, 0.9426327773502895, 0.9407806595166525, 0.9422641737120492, 0.9425137525513059, 0.9429097714878264, 0.9430127938588461, 0.941810877550216, 0.9435233445394606, 0.9445467165538243, 0.9367582627705165, 0.943228006362915, 0.9358401951335725, 0.9426465176400685, 0.9391529247874305, 0.9414423108100891, 0.9381593295506069, 0.9429349984441485, 0.9432120124499003, 0.940038902418954, 0.9422023778869992, 0.9434867274193537, 0.9441575067383903, 0.9430631597836813, 0.9437843561172485, 0.9407875424339658, 0.9421222266696748, 0.9413736462593079, 0.9409523634683519, 0.9388599083537147, 0.9436400930086771, 0.9442765570822216, 0.9450778393518358, 0.9412889111609686, 0.9400480957258315, 0.943106662659418, 0.9425114393234253, 0.9420878972326007, 0.9432211404755002, 0.9444802914346967, 0.9437706101508367, 0.9419757411593482, 0.9411858774366832, 0.9435462696211678, 0.9417033053579784, 0.9438942301841009, 0.9423740761620658, 0.9391849637031555, 0.940588374932607, 0.9441643499192738, 0.9403319642657325, 0.9430036402883983, 0.9426602834746951, 0.9424404501914978, 0.9439583449136644, 0.9419276402110145, 0.9414056851750329, 0.9411286796842303, 0.9369391288076129, 0.9441643726258051, 0.9423740591321673, 0.9431364422752744, 0.9421726323309398, 0.9379143629755292, 0.9415132914270673, 0.9401534114565168, 0.9391414665040516, 0.9445238283702305, 0.9439835321335566, 0.9427770205906459, 0.9417170229412261, 0.9412522656576974, 0.9448580855414981, 0.9438621572085789, 0.9430242947169712, 0.9425687051954723, 0.9427334950083778], 'val_mDice': [0.013333740211757166, 0.014224891295833956, 0.01374135117623068, 0.014322413131594658, 0.01651532548878874, 0.030244888604751656, 0.03922754723489994, 0.05156164332514718, 0.06985646827767293, 0.07955039501012791, 0.10901917055958793, 0.12476179385114283, 0.15496273068267674, 0.16419070763956933, 0.16293823719024658, 0.2078904973875199, 0.24309564692278704, 0.2567080184373827, 0.25663895567967776, 0.29191291536248865, 0.30299599122788223, 0.324059267262263, 0.332188937901741, 0.3581506609916687, 0.31824285306391265, 0.38108030937257265, 0.3774162699424085, 0.3921826614865235, 0.3991650513240269, 0.41228474126685233, 0.39242052127208027, 0.399604826101235, 0.3578269674575755, 0.40472373534880934, 0.41976027385819525, 0.4241493158042431, 0.4349164152074428, 0.41992279621107237, 0.4312149169189589, 0.4292040169239044, 0.44059923894348596, 0.41154911723874865, 0.43120987926210674, 0.4435821093973659, 0.4344186555771601, 0.4326378437025206, 0.44121028518392924, 0.4313137538376309, 0.4406847669964745, 0.4382940905079955, 0.4247590849796931, 0.42039925569579717, 0.4306202307343483, 0.44693739747717265, 0.4496491939893791, 0.44725890918856576, 0.43766974178808077, 0.4455907570109481, 0.45357195287942886, 0.43467185575337636, 0.44041482288213, 0.442300966098195, 0.44955224437373026, 0.45204911948669524, 0.46059185461629004, 0.4482611465666975, 0.4486440035204093, 0.42308577859685537, 0.4544191960067976, 0.43779631792789414, 0.464105417153665, 0.4520093912170047, 0.4568308024179368, 0.44103424801003366, 0.44957096519924344, 0.4549158981868199, 0.45871799758502413, 0.45099168235347387, 0.45472735308465506, 0.4476082032280309, 0.43976812188824016, 0.4588010538192022, 0.44789129460141774, 0.46061404262270245, 0.4499754866673833, 0.4668238487626825, 0.4563261061197236, 0.46911460374082836, 0.4602234260666938, 0.45716526323840734, 0.45377735509758904, 0.45577429447855267, 0.4453225758458887, 0.45917136736568953, 0.46123005804561434, 0.46101928874850273, 0.4384846995983805, 0.4596643518833887, 0.45845374784299303, 0.4608338971932729, 0.46575463297111647, 0.4725127454314913, 0.46019109923924717, 0.45975636708594503, 0.46976482513405027, 0.44375671002836453, 0.4519204564747356, 0.4629641052867685, 0.4617558331007049, 0.4667206144049054, 0.45031846314668655, 0.4639305532688186, 0.4451502254676251, 0.4342887990531467, 0.46864586713768186, 0.4656608388537452, 0.4576999827155045, 0.46865611541129293, 0.43468803557611646, 0.46483599962223143, 0.4575306542572521, 0.4602599823403926, 0.46710620926959173, 0.461920627171085, 0.464414618909359, 0.4521172071496646, 0.45520003104493734, 0.4678399878598395, 0.45845681038640795, 0.45916472446350826, 0.44758826919964384, 0.44944576262718156], 'loss': [126.81195319289529, 32.88068834664804, 15.622996807925752, 10.853799845870345, 8.819115754012223, 7.648925503623585, 6.856971739435647, 6.285432917685184, 5.790547702407617, 5.335744973067399, 4.942735165145003, 4.610229334758614, 4.32545749433748, 4.070837228220507, 3.851486289756294, 3.6638579702294582, 3.4780060163876305, 3.316907701497825, 3.165462067942181, 3.040940005999437, 2.9167587430761666, 2.8127200811704918, 2.7286095465449947, 2.6451958224611154, 2.5799340851761507, 2.516975180018447, 2.4445458110964133, 2.3934458001215657, 2.3436081674210665, 2.294880415493752, 2.2561964836795365, 2.210590726014206, 2.175765997806863, 2.1492323171708634, 2.112659680813476, 2.0866671236167647, 2.0557717173596397, 2.029960538602321, 2.005178082510987, 1.9797287139677464, 1.9503184787473722, 1.9267149101867367, 1.9171637016522165, 1.891233511598027, 1.8727441814471146, 1.8576115610651458, 1.8432316403180298, 1.8216683292563582, 1.8184872645736085, 1.8026495325742302, 1.7836881340503785, 1.7716077550518874, 1.7580607083805821, 1.7502631783048748, 1.7306018934466831, 1.7271910349436066, 1.7155896384568488, 1.7129716128082157, 1.7010695293318772, 1.6880547351085178, 1.6764006906330688, 1.655693467605176, 1.662053685186455, 1.6496805267709056, 1.6325673945542587, 1.6237383006923065, 1.6289592365182524, 1.6177934940688201, 1.6070587398092573, 1.6006072342729025, 1.5993745237594628, 1.5886599959861987, 1.5796084312751892, 1.576896712364938, 1.5684616561337705, 1.55745481927661, 1.5519835013793524, 1.5468761536434756, 1.5377132199691719, 1.5305171397257706, 1.5225142109525072, 1.5231362733855651, 1.5147028822693862, 1.5086957287278184, 1.5055316271109918, 1.5010574726739416, 1.4936429537949085, 1.4922556724993277, 1.4787593681228075, 1.4796278720787317, 1.4775608033372736, 1.4724823944780532, 1.4679310494405078, 1.4694015315577822, 1.4588131500457968, 1.446269520542078, 1.4543377666062465, 1.44314533698437, 1.4427437158777279, 1.4349389251817828, 1.4320494414570935, 1.4196993070966089, 1.4206027110601542, 1.4152699022226902, 1.4172847605588144, 1.41186521465617, 1.406581306195521, 1.4080352663695066, 1.4029391168031202, 1.4013277234106318, 1.399783232089341, 1.3894219396430127, 1.3950877012902196, 1.3884462284540795, 1.3862783764285207, 1.3768791197719246, 1.385245508924256, 1.3710860246772685, 1.3666948438357223, 1.3714108621313412, 1.3692707326431965, 1.3719133764120433, 1.3624249566077082, 1.3599007524620343, 1.3489311982469796, 1.3479722194044672, 1.3483282346037833, 1.3474231114248698, 1.3454173350118133, 1.341404034725062, 1.3358737239723655, 1.335392009184635], 'acc': [0.6127519964603002, 0.866776895529966, 0.869033391486709, 0.8691447322047329, 0.8691862596007517, 0.8692157556377718, 0.8692455998454361, 0.8692341749783492, 0.8691646121521319, 0.8691954546083208, 0.8693398620343654, 0.8697820017511085, 0.8707018315803575, 0.8724030634675247, 0.8745211127384885, 0.8770576813833686, 0.8800149193751414, 0.8825453779061727, 0.8858572801735581, 0.8892062567023613, 0.8934664244662825, 0.8972667930947579, 0.9004714777526172, 0.9033485170570853, 0.9058127463472954, 0.9076538647970115, 0.9099730430872932, 0.9117304865441066, 0.91330329794311, 0.914688370178172, 0.9159535553635993, 0.9173819571762203, 0.9184212620625589, 0.9194591281982615, 0.92064462203108, 0.9215665811262357, 0.9224432430355586, 0.9235333182895852, 0.9242005496211702, 0.9251223468058872, 0.9257701624788851, 0.9267294990526864, 0.9268568991833347, 0.9277461847290037, 0.9283717117811504, 0.9288178826196588, 0.9291665066246907, 0.9297743760852288, 0.9299824770553645, 0.9304079590148036, 0.9309156988658541, 0.9314477260525984, 0.931789461532631, 0.9319126693090538, 0.9326965742679328, 0.932761291593905, 0.9333848225282111, 0.9332528310128207, 0.9336628136272626, 0.9339790636573381, 0.9343006870340744, 0.9350106262944672, 0.9348865406738901, 0.9351453942348898, 0.9357658216023413, 0.9359428519822969, 0.93578417247859, 0.9362007572583342, 0.9365276878156344, 0.9365866588033968, 0.9366824056271622, 0.9368242650494448, 0.9373029476878775, 0.9372361876818861, 0.9374284213371122, 0.9380633055600721, 0.9378761007235601, 0.9381750653377038, 0.9383229236509015, 0.9386143113966046, 0.9388423663516805, 0.9385991794239792, 0.9390595132086074, 0.9391885772821461, 0.9392256980597777, 0.9393675593895928, 0.9393814575564132, 0.9395478587976098, 0.9397579098060362, 0.9398373893535916, 0.9399536902536334, 0.9400337897377573, 0.9400082799610154, 0.9401067374918717, 0.940499779114908, 0.9406512089379611, 0.9406257414371954, 0.9409732720469013, 0.9411386269260882, 0.941268875496866, 0.9413513705775024, 0.9417556933591837, 0.9416827730229762, 0.9417607229937702, 0.9417649624502588, 0.9419752932454387, 0.9420545877853248, 0.9419967280616415, 0.9421869463398435, 0.9422129657380218, 0.942377671205816, 0.9426353193901398, 0.9424503820696755, 0.942479792451592, 0.9426907012750815, 0.9428903496814689, 0.9428589509497356, 0.9431190538640791, 0.9431737829362448, 0.943057044671178, 0.9431668328416125, 0.9431519582135061, 0.9432151013885087, 0.943402330069177, 0.9435847388863771, 0.9437677257893424, 0.9436796980486087, 0.9436915108917168, 0.9439026554908186, 0.9440464142868692, 0.9440892564209298, 0.9440158503648975], 'mDice': [0.0191159016546634, 0.018664544763545184, 0.018236769444799523, 0.02005485556366151, 0.02409608299151445, 0.030535252357243987, 0.03892142877215385, 0.04808418224209279, 0.059170558495640596, 0.07342659418744252, 0.09037045423887312, 0.10869322732128296, 0.1285895014144746, 0.14868388728459309, 0.16980892141441722, 0.19035972502274345, 0.2137495333579135, 0.23639314738178566, 0.25878723936433023, 0.27908294269154704, 0.30012954609780634, 0.31895550338546735, 0.33501649713019815, 0.35097816106738533, 0.3649443219180095, 0.37748767502486696, 0.3926340792940742, 0.4031302483469759, 0.41370948546420455, 0.42341899813221257, 0.43112684964719983, 0.4414834668175756, 0.4487744766277271, 0.45570978419454844, 0.4631235281327613, 0.46873953057686807, 0.4753242563093607, 0.48122145207672423, 0.48734658491710303, 0.493070706595845, 0.4989983922418933, 0.504941787716509, 0.5072781783504386, 0.5136839172756004, 0.5171312970625129, 0.5216534942107001, 0.524932210330952, 0.5297596085524315, 0.5313997651562196, 0.5343111860906087, 0.5388866613796963, 0.5420521901755612, 0.5454333260129682, 0.5467310002113506, 0.5518834690425307, 0.5531450771448905, 0.5558531339145106, 0.5564760830399157, 0.5586880091552632, 0.5616122532125288, 0.5639177144941209, 0.5695968582401714, 0.5685474414781313, 0.5705900024949458, 0.5755592910844006, 0.577197226125288, 0.5766324620727677, 0.5789831575341645, 0.5818399682264694, 0.5830323492513126, 0.5840827861065715, 0.5857971964117785, 0.5879887669031758, 0.5887616102764872, 0.5910533203516987, 0.5946694539189453, 0.5947858122173293, 0.5964350880215066, 0.5985733903003654, 0.6003936627523871, 0.6026567760082897, 0.6024381549275172, 0.6047174910118246, 0.6059205222617047, 0.6069688385958292, 0.6076102548535445, 0.6097785511158417, 0.6102509671882878, 0.6128607680969025, 0.612833099315133, 0.6136441748565576, 0.6143257717923528, 0.6160390328322161, 0.6155117922075594, 0.6175895756969154, 0.6209236695767828, 0.6191221704275272, 0.6213783005172402, 0.6219478502010641, 0.6237829887777343, 0.6240003031757679, 0.627144189418247, 0.6277883398709831, 0.627980861885368, 0.6283065956154149, 0.6295866306654998, 0.630582484903422, 0.6296316905359048, 0.6317170533872832, 0.631724937931522, 0.6316124788823559, 0.6348140578704784, 0.6329506033299308, 0.6348970040940402, 0.6358526388137631, 0.6374544192766991, 0.6356154831475735, 0.6392709491959375, 0.6401878895386349, 0.6388909288148602, 0.6396573354610938, 0.6395553600266418, 0.6409382436784311, 0.6420074826907578, 0.6438455011014054, 0.6450526872443307, 0.6448441423362267, 0.6446767051669934, 0.645672967996077, 0.6463073246139356, 0.6479001176571005, 0.6480675715804445]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:03<00:06,  3.01s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.67s/it]predicting test subjects: 100%|██████████| 3/3 [00:06<00:00,  2.38s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:33,  1.60s/it]predicting train subjects:   1%|          | 2/285 [00:03<08:08,  1.73s/it]predicting train subjects:   1%|          | 3/285 [00:05<08:19,  1.77s/it]predicting train subjects:   1%|▏         | 4/285 [00:07<08:45,  1.87s/it]predicting train subjects:   2%|▏         | 5/285 [00:09<08:32,  1.83s/it]predicting train subjects:   2%|▏         | 6/285 [00:11<09:07,  1.96s/it]predicting train subjects:   2%|▏         | 7/285 [00:14<09:54,  2.14s/it]predicting train subjects:   3%|▎         | 8/285 [00:16<09:49,  2.13s/it]predicting train subjects:   3%|▎         | 9/285 [00:18<09:19,  2.03s/it]predicting train subjects:   4%|▎         | 10/285 [00:20<09:34,  2.09s/it]predicting train subjects:   4%|▍         | 11/285 [00:22<09:52,  2.16s/it]predicting train subjects:   4%|▍         | 12/285 [00:25<10:20,  2.27s/it]predicting train subjects:   5%|▍         | 13/285 [00:27<10:24,  2.30s/it]predicting train subjects:   5%|▍         | 14/285 [00:29<10:31,  2.33s/it]predicting train subjects:   5%|▌         | 15/285 [00:32<10:30,  2.33s/it]predicting train subjects:   6%|▌         | 16/285 [00:34<10:15,  2.29s/it]predicting train subjects:   6%|▌         | 17/285 [00:36<10:02,  2.25s/it]predicting train subjects:   6%|▋         | 18/285 [00:38<10:03,  2.26s/it]predicting train subjects:   7%|▋         | 19/285 [00:41<09:57,  2.25s/it]predicting train subjects:   7%|▋         | 20/285 [00:43<10:23,  2.35s/it]predicting train subjects:   7%|▋         | 21/285 [00:46<10:18,  2.34s/it]predicting train subjects:   8%|▊         | 22/285 [00:48<10:28,  2.39s/it]predicting train subjects:   8%|▊         | 23/285 [00:50<10:14,  2.35s/it]predicting train subjects:   8%|▊         | 24/285 [00:53<10:12,  2.35s/it]predicting train subjects:   9%|▉         | 25/285 [00:55<10:13,  2.36s/it]predicting train subjects:   9%|▉         | 26/285 [00:57<10:04,  2.33s/it]predicting train subjects:   9%|▉         | 27/285 [00:59<09:45,  2.27s/it]predicting train subjects:  10%|▉         | 28/285 [01:02<09:37,  2.25s/it]predicting train subjects:  10%|█         | 29/285 [01:04<09:35,  2.25s/it]predicting train subjects:  11%|█         | 30/285 [01:06<09:27,  2.23s/it]predicting train subjects:  11%|█         | 31/285 [01:08<09:18,  2.20s/it]predicting train subjects:  11%|█         | 32/285 [01:11<09:34,  2.27s/it]predicting train subjects:  12%|█▏        | 33/285 [01:13<09:41,  2.31s/it]predicting train subjects:  12%|█▏        | 34/285 [01:15<09:45,  2.33s/it]predicting train subjects:  12%|█▏        | 35/285 [01:18<09:48,  2.35s/it]predicting train subjects:  13%|█▎        | 36/285 [01:20<09:27,  2.28s/it]predicting train subjects:  13%|█▎        | 37/285 [01:22<09:21,  2.26s/it]predicting train subjects:  13%|█▎        | 38/285 [01:24<09:17,  2.26s/it]predicting train subjects:  14%|█▎        | 39/285 [01:27<09:25,  2.30s/it]predicting train subjects:  14%|█▍        | 40/285 [01:29<09:10,  2.25s/it]predicting train subjects:  14%|█▍        | 41/285 [01:31<09:00,  2.22s/it]predicting train subjects:  15%|█▍        | 42/285 [01:33<09:01,  2.23s/it]predicting train subjects:  15%|█▌        | 43/285 [01:36<09:22,  2.32s/it]predicting train subjects:  15%|█▌        | 44/285 [01:38<09:10,  2.28s/it]predicting train subjects:  16%|█▌        | 45/285 [01:40<09:11,  2.30s/it]predicting train subjects:  16%|█▌        | 46/285 [01:42<08:38,  2.17s/it]predicting train subjects:  16%|█▋        | 47/285 [01:44<08:13,  2.08s/it]predicting train subjects:  17%|█▋        | 48/285 [01:46<07:43,  1.95s/it]predicting train subjects:  17%|█▋        | 49/285 [01:48<07:44,  1.97s/it]predicting train subjects:  18%|█▊        | 50/285 [01:50<07:33,  1.93s/it]predicting train subjects:  18%|█▊        | 51/285 [01:51<07:30,  1.92s/it]predicting train subjects:  18%|█▊        | 52/285 [01:53<07:21,  1.89s/it]predicting train subjects:  19%|█▊        | 53/285 [01:55<07:29,  1.94s/it]predicting train subjects:  19%|█▉        | 54/285 [01:57<07:35,  1.97s/it]predicting train subjects:  19%|█▉        | 55/285 [01:59<07:34,  1.98s/it]predicting train subjects:  20%|█▉        | 56/285 [02:01<07:34,  1.98s/it]predicting train subjects:  20%|██        | 57/285 [02:03<07:32,  1.99s/it]predicting train subjects:  20%|██        | 58/285 [02:05<07:30,  1.99s/it]predicting train subjects:  21%|██        | 59/285 [02:07<07:22,  1.96s/it]predicting train subjects:  21%|██        | 60/285 [02:09<07:17,  1.94s/it]predicting train subjects:  21%|██▏       | 61/285 [02:11<07:15,  1.94s/it]predicting train subjects:  22%|██▏       | 62/285 [02:13<07:23,  1.99s/it]predicting train subjects:  22%|██▏       | 63/285 [02:15<07:15,  1.96s/it]predicting train subjects:  22%|██▏       | 64/285 [02:17<07:14,  1.97s/it]predicting train subjects:  23%|██▎       | 65/285 [02:19<07:40,  2.09s/it]predicting train subjects:  23%|██▎       | 66/285 [02:22<07:50,  2.15s/it]predicting train subjects:  24%|██▎       | 67/285 [02:24<07:42,  2.12s/it]predicting train subjects:  24%|██▍       | 68/285 [02:26<07:24,  2.05s/it]predicting train subjects:  24%|██▍       | 69/285 [02:28<07:28,  2.08s/it]predicting train subjects:  25%|██▍       | 70/285 [02:30<07:23,  2.06s/it]predicting train subjects:  25%|██▍       | 71/285 [02:32<07:12,  2.02s/it]predicting train subjects:  25%|██▌       | 72/285 [02:34<07:08,  2.01s/it]predicting train subjects:  26%|██▌       | 73/285 [02:36<06:56,  1.97s/it]predicting train subjects:  26%|██▌       | 74/285 [02:38<07:07,  2.02s/it]predicting train subjects:  26%|██▋       | 75/285 [02:40<07:10,  2.05s/it]predicting train subjects:  27%|██▋       | 76/285 [02:42<07:08,  2.05s/it]predicting train subjects:  27%|██▋       | 77/285 [02:44<07:08,  2.06s/it]predicting train subjects:  27%|██▋       | 78/285 [02:46<07:03,  2.05s/it]predicting train subjects:  28%|██▊       | 79/285 [02:48<06:54,  2.01s/it]predicting train subjects:  28%|██▊       | 80/285 [02:50<06:48,  1.99s/it]predicting train subjects:  28%|██▊       | 81/285 [02:52<06:48,  2.00s/it]predicting train subjects:  29%|██▉       | 82/285 [02:54<06:52,  2.03s/it]predicting train subjects:  29%|██▉       | 83/285 [02:56<06:45,  2.01s/it]predicting train subjects:  29%|██▉       | 84/285 [02:58<06:49,  2.04s/it]predicting train subjects:  30%|██▉       | 85/285 [03:00<07:05,  2.13s/it]predicting train subjects:  30%|███       | 86/285 [03:03<07:09,  2.16s/it]predicting train subjects:  31%|███       | 87/285 [03:05<07:05,  2.15s/it]predicting train subjects:  31%|███       | 88/285 [03:07<07:10,  2.19s/it]predicting train subjects:  31%|███       | 89/285 [03:09<07:03,  2.16s/it]predicting train subjects:  32%|███▏      | 90/285 [03:11<07:08,  2.20s/it]predicting train subjects:  32%|███▏      | 91/285 [03:14<07:11,  2.22s/it]predicting train subjects:  32%|███▏      | 92/285 [03:16<07:12,  2.24s/it]predicting train subjects:  33%|███▎      | 93/285 [03:18<07:16,  2.27s/it]predicting train subjects:  33%|███▎      | 94/285 [03:20<07:01,  2.21s/it]predicting train subjects:  33%|███▎      | 95/285 [03:23<07:02,  2.23s/it]predicting train subjects:  34%|███▎      | 96/285 [03:25<07:11,  2.28s/it]predicting train subjects:  34%|███▍      | 97/285 [03:27<07:10,  2.29s/it]predicting train subjects:  34%|███▍      | 98/285 [03:30<07:03,  2.26s/it]predicting train subjects:  35%|███▍      | 99/285 [03:32<07:07,  2.30s/it]predicting train subjects:  35%|███▌      | 100/285 [03:34<07:03,  2.29s/it]predicting train subjects:  35%|███▌      | 101/285 [03:37<07:04,  2.31s/it]predicting train subjects:  36%|███▌      | 102/285 [03:39<06:55,  2.27s/it]predicting train subjects:  36%|███▌      | 103/285 [03:41<06:43,  2.22s/it]predicting train subjects:  36%|███▋      | 104/285 [03:43<06:50,  2.27s/it]predicting train subjects:  37%|███▋      | 105/285 [03:46<06:52,  2.29s/it]predicting train subjects:  37%|███▋      | 106/285 [03:47<06:23,  2.14s/it]predicting train subjects:  38%|███▊      | 107/285 [03:49<06:02,  2.04s/it]predicting train subjects:  38%|███▊      | 108/285 [03:52<06:20,  2.15s/it]predicting train subjects:  38%|███▊      | 109/285 [03:54<06:14,  2.13s/it]predicting train subjects:  39%|███▊      | 110/285 [03:56<06:16,  2.15s/it]predicting train subjects:  39%|███▉      | 111/285 [03:58<06:23,  2.21s/it]predicting train subjects:  39%|███▉      | 112/285 [04:01<06:26,  2.23s/it]predicting train subjects:  40%|███▉      | 113/285 [04:03<06:26,  2.25s/it]predicting train subjects:  40%|████      | 114/285 [04:05<06:24,  2.25s/it]predicting train subjects:  40%|████      | 115/285 [04:07<05:59,  2.11s/it]predicting train subjects:  41%|████      | 116/285 [04:09<05:42,  2.03s/it]predicting train subjects:  41%|████      | 117/285 [04:11<05:29,  1.96s/it]predicting train subjects:  41%|████▏     | 118/285 [04:12<05:20,  1.92s/it]predicting train subjects:  42%|████▏     | 119/285 [04:14<05:22,  1.94s/it]predicting train subjects:  42%|████▏     | 120/285 [04:16<05:16,  1.92s/it]predicting train subjects:  42%|████▏     | 121/285 [04:18<05:01,  1.84s/it]predicting train subjects:  43%|████▎     | 122/285 [04:19<04:45,  1.75s/it]predicting train subjects:  43%|████▎     | 123/285 [04:21<04:31,  1.67s/it]predicting train subjects:  44%|████▎     | 124/285 [04:23<04:31,  1.69s/it]predicting train subjects:  44%|████▍     | 125/285 [04:24<04:29,  1.68s/it]predicting train subjects:  44%|████▍     | 126/285 [04:26<04:26,  1.68s/it]predicting train subjects:  45%|████▍     | 127/285 [04:28<04:22,  1.66s/it]predicting train subjects:  45%|████▍     | 128/285 [04:29<04:23,  1.68s/it]predicting train subjects:  45%|████▌     | 129/285 [04:31<04:21,  1.68s/it]predicting train subjects:  46%|████▌     | 130/285 [04:33<04:18,  1.67s/it]predicting train subjects:  46%|████▌     | 131/285 [04:34<04:16,  1.66s/it]predicting train subjects:  46%|████▋     | 132/285 [04:36<04:13,  1.65s/it]predicting train subjects:  47%|████▋     | 133/285 [04:38<04:10,  1.65s/it]predicting train subjects:  47%|████▋     | 134/285 [04:39<04:08,  1.64s/it]predicting train subjects:  47%|████▋     | 135/285 [04:41<04:07,  1.65s/it]predicting train subjects:  48%|████▊     | 136/285 [04:43<04:07,  1.66s/it]predicting train subjects:  48%|████▊     | 137/285 [04:44<04:05,  1.66s/it]predicting train subjects:  48%|████▊     | 138/285 [04:46<04:04,  1.67s/it]predicting train subjects:  49%|████▉     | 139/285 [04:47<04:00,  1.65s/it]predicting train subjects:  49%|████▉     | 140/285 [04:49<03:58,  1.65s/it]predicting train subjects:  49%|████▉     | 141/285 [04:51<03:58,  1.66s/it]predicting train subjects:  50%|████▉     | 142/285 [04:52<03:50,  1.61s/it]predicting train subjects:  50%|█████     | 143/285 [04:54<03:44,  1.58s/it]predicting train subjects:  51%|█████     | 144/285 [04:55<03:37,  1.54s/it]predicting train subjects:  51%|█████     | 145/285 [04:57<03:34,  1.53s/it]predicting train subjects:  51%|█████     | 146/285 [04:58<03:31,  1.52s/it]predicting train subjects:  52%|█████▏    | 147/285 [05:00<03:28,  1.51s/it]predicting train subjects:  52%|█████▏    | 148/285 [05:01<03:25,  1.50s/it]predicting train subjects:  52%|█████▏    | 149/285 [05:03<03:23,  1.50s/it]predicting train subjects:  53%|█████▎    | 150/285 [05:04<03:21,  1.49s/it]predicting train subjects:  53%|█████▎    | 151/285 [05:06<03:19,  1.49s/it]predicting train subjects:  53%|█████▎    | 152/285 [05:07<03:17,  1.49s/it]predicting train subjects:  54%|█████▎    | 153/285 [05:09<03:16,  1.49s/it]predicting train subjects:  54%|█████▍    | 154/285 [05:10<03:18,  1.51s/it]predicting train subjects:  54%|█████▍    | 155/285 [05:12<03:17,  1.52s/it]predicting train subjects:  55%|█████▍    | 156/285 [05:13<03:13,  1.50s/it]predicting train subjects:  55%|█████▌    | 157/285 [05:15<03:11,  1.50s/it]predicting train subjects:  55%|█████▌    | 158/285 [05:16<03:09,  1.49s/it]predicting train subjects:  56%|█████▌    | 159/285 [05:18<03:10,  1.51s/it]predicting train subjects:  56%|█████▌    | 160/285 [05:19<03:09,  1.51s/it]predicting train subjects:  56%|█████▋    | 161/285 [05:21<03:06,  1.50s/it]predicting train subjects:  57%|█████▋    | 162/285 [05:22<03:04,  1.50s/it]predicting train subjects:  57%|█████▋    | 163/285 [05:24<03:02,  1.50s/it]predicting train subjects:  58%|█████▊    | 164/285 [05:25<02:57,  1.47s/it]predicting train subjects:  58%|█████▊    | 165/285 [05:27<02:54,  1.45s/it]predicting train subjects:  58%|█████▊    | 166/285 [05:28<02:52,  1.45s/it]predicting train subjects:  59%|█████▊    | 167/285 [05:29<02:50,  1.44s/it]predicting train subjects:  59%|█████▉    | 168/285 [05:31<02:50,  1.46s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:32<02:48,  1.45s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:34<02:47,  1.45s/it]predicting train subjects:  60%|██████    | 171/285 [05:35<02:44,  1.45s/it]predicting train subjects:  60%|██████    | 172/285 [05:37<02:42,  1.44s/it]predicting train subjects:  61%|██████    | 173/285 [05:38<02:44,  1.47s/it]predicting train subjects:  61%|██████    | 174/285 [05:40<02:44,  1.48s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:41<02:43,  1.49s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:43<02:40,  1.48s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:44<02:38,  1.47s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:46<02:35,  1.45s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:47<02:32,  1.44s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:48<02:30,  1.43s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:50<02:28,  1.43s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:51<02:26,  1.42s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:53<02:26,  1.43s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:54<02:24,  1.43s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:55<02:21,  1.42s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:57<02:19,  1.41s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:58<02:19,  1.43s/it]predicting train subjects:  66%|██████▌   | 188/285 [06:00<02:17,  1.42s/it]predicting train subjects:  66%|██████▋   | 189/285 [06:01<02:16,  1.42s/it]predicting train subjects:  67%|██████▋   | 190/285 [06:03<02:14,  1.41s/it]predicting train subjects:  67%|██████▋   | 191/285 [06:04<02:11,  1.40s/it]predicting train subjects:  67%|██████▋   | 192/285 [06:05<02:09,  1.40s/it]predicting train subjects:  68%|██████▊   | 193/285 [06:07<02:12,  1.44s/it]predicting train subjects:  68%|██████▊   | 194/285 [06:08<02:09,  1.42s/it]predicting train subjects:  68%|██████▊   | 195/285 [06:10<02:06,  1.41s/it]predicting train subjects:  69%|██████▉   | 196/285 [06:11<02:12,  1.49s/it]predicting train subjects:  69%|██████▉   | 197/285 [06:13<02:16,  1.55s/it]predicting train subjects:  69%|██████▉   | 198/285 [06:15<02:18,  1.59s/it]predicting train subjects:  70%|██████▉   | 199/285 [06:16<02:19,  1.62s/it]predicting train subjects:  70%|███████   | 200/285 [06:18<02:20,  1.65s/it]predicting train subjects:  71%|███████   | 201/285 [06:20<02:20,  1.67s/it]predicting train subjects:  71%|███████   | 202/285 [06:21<02:18,  1.67s/it]predicting train subjects:  71%|███████   | 203/285 [06:23<02:15,  1.66s/it]predicting train subjects:  72%|███████▏  | 204/285 [06:25<02:14,  1.66s/it]predicting train subjects:  72%|███████▏  | 205/285 [06:26<02:12,  1.66s/it]predicting train subjects:  72%|███████▏  | 206/285 [06:28<02:11,  1.66s/it]predicting train subjects:  73%|███████▎  | 207/285 [06:30<02:10,  1.67s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:31<02:09,  1.68s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:33<02:07,  1.68s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:35<02:06,  1.69s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:36<02:03,  1.67s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:38<02:03,  1.69s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:40<02:01,  1.68s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:41<01:55,  1.63s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:43<01:50,  1.58s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:44<01:47,  1.56s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:46<01:44,  1.54s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:47<01:41,  1.52s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:49<01:38,  1.50s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:50<01:36,  1.49s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:52<01:34,  1.48s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:53<01:32,  1.47s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:55<01:31,  1.48s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:56<01:30,  1.48s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:58<01:28,  1.47s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:59<01:27,  1.49s/it]predicting train subjects:  80%|███████▉  | 227/285 [07:01<01:26,  1.50s/it]predicting train subjects:  80%|████████  | 228/285 [07:02<01:26,  1.52s/it]predicting train subjects:  80%|████████  | 229/285 [07:04<01:25,  1.52s/it]predicting train subjects:  81%|████████  | 230/285 [07:05<01:24,  1.53s/it]predicting train subjects:  81%|████████  | 231/285 [07:07<01:22,  1.53s/it]predicting train subjects:  81%|████████▏ | 232/285 [07:09<01:29,  1.68s/it]predicting train subjects:  82%|████████▏ | 233/285 [07:11<01:33,  1.80s/it]predicting train subjects:  82%|████████▏ | 234/285 [07:13<01:34,  1.84s/it]predicting train subjects:  82%|████████▏ | 235/285 [07:15<01:32,  1.86s/it]predicting train subjects:  83%|████████▎ | 236/285 [07:17<01:32,  1.89s/it]predicting train subjects:  83%|████████▎ | 237/285 [07:19<01:31,  1.90s/it]predicting train subjects:  84%|████████▎ | 238/285 [07:20<01:29,  1.90s/it]predicting train subjects:  84%|████████▍ | 239/285 [07:22<01:27,  1.91s/it]predicting train subjects:  84%|████████▍ | 240/285 [07:24<01:26,  1.92s/it]predicting train subjects:  85%|████████▍ | 241/285 [07:26<01:24,  1.92s/it]predicting train subjects:  85%|████████▍ | 242/285 [07:28<01:22,  1.93s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:30<01:20,  1.92s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:32<01:20,  1.95s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:34<01:18,  1.96s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:36<01:15,  1.93s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:38<01:12,  1.91s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:40<01:12,  1.96s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:42<01:09,  1.94s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:43<01:03,  1.81s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:45<00:58,  1.71s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:46<00:53,  1.63s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:48<00:50,  1.57s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:49<00:47,  1.52s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:51<00:44,  1.50s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:52<00:42,  1.48s/it]predicting train subjects:  90%|█████████ | 257/285 [07:53<00:41,  1.48s/it]predicting train subjects:  91%|█████████ | 258/285 [07:55<00:39,  1.47s/it]predicting train subjects:  91%|█████████ | 259/285 [07:56<00:37,  1.46s/it]predicting train subjects:  91%|█████████ | 260/285 [07:58<00:36,  1.46s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:59<00:35,  1.48s/it]predicting train subjects:  92%|█████████▏| 262/285 [08:01<00:33,  1.46s/it]predicting train subjects:  92%|█████████▏| 263/285 [08:02<00:31,  1.45s/it]predicting train subjects:  93%|█████████▎| 264/285 [08:04<00:30,  1.45s/it]predicting train subjects:  93%|█████████▎| 265/285 [08:05<00:29,  1.46s/it]predicting train subjects:  93%|█████████▎| 266/285 [08:07<00:27,  1.47s/it]predicting train subjects:  94%|█████████▎| 267/285 [08:08<00:26,  1.47s/it]predicting train subjects:  94%|█████████▍| 268/285 [08:10<00:27,  1.62s/it]predicting train subjects:  94%|█████████▍| 269/285 [08:12<00:27,  1.72s/it]predicting train subjects:  95%|█████████▍| 270/285 [08:14<00:26,  1.79s/it]predicting train subjects:  95%|█████████▌| 271/285 [08:16<00:25,  1.83s/it]predicting train subjects:  95%|█████████▌| 272/285 [08:18<00:24,  1.87s/it]predicting train subjects:  96%|█████████▌| 273/285 [08:20<00:22,  1.90s/it]predicting train subjects:  96%|█████████▌| 274/285 [08:22<00:20,  1.90s/it]predicting train subjects:  96%|█████████▋| 275/285 [08:24<00:19,  1.93s/it]predicting train subjects:  97%|█████████▋| 276/285 [08:26<00:17,  1.94s/it]predicting train subjects:  97%|█████████▋| 277/285 [08:28<00:15,  1.98s/it]predicting train subjects:  98%|█████████▊| 278/285 [08:30<00:13,  1.97s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:32<00:11,  1.95s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:33<00:09,  1.94s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:36<00:07,  1.97s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:37<00:05,  1.96s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:39<00:03,  1.96s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:41<00:01,  1.98s/it]predicting train subjects: 100%|██████████| 285/285 [08:43<00:00,  1.95s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<07:52,  1.66s/it]Loading train:   1%|          | 2/285 [00:03<08:05,  1.72s/it]Loading train:   1%|          | 3/285 [00:05<08:11,  1.74s/it]Loading train:   1%|▏         | 4/285 [00:07<08:16,  1.77s/it]Loading train:   2%|▏         | 5/285 [00:08<08:07,  1.74s/it]Loading train:   2%|▏         | 6/285 [00:10<08:37,  1.86s/it]Loading train:   2%|▏         | 7/285 [00:13<09:01,  1.95s/it]Loading train:   3%|▎         | 8/285 [00:15<08:58,  1.95s/it]Loading train:   3%|▎         | 9/285 [00:16<08:45,  1.90s/it]Loading train:   4%|▎         | 10/285 [00:18<08:25,  1.84s/it]Loading train:   4%|▍         | 11/285 [00:20<08:32,  1.87s/it]Loading train:   4%|▍         | 12/285 [00:22<08:09,  1.79s/it]Loading train:   5%|▍         | 13/285 [00:23<07:40,  1.69s/it]Loading train:   5%|▍         | 14/285 [00:25<07:35,  1.68s/it]Loading train:   5%|▌         | 15/285 [00:27<07:45,  1.73s/it]Loading train:   6%|▌         | 16/285 [00:29<08:07,  1.81s/it]Loading train:   6%|▌         | 17/285 [00:30<08:01,  1.80s/it]Loading train:   6%|▋         | 18/285 [00:32<07:45,  1.74s/it]Loading train:   7%|▋         | 19/285 [00:34<07:41,  1.74s/it]Loading train:   7%|▋         | 20/285 [00:35<07:22,  1.67s/it]Loading train:   7%|▋         | 21/285 [00:37<07:17,  1.66s/it]Loading train:   8%|▊         | 22/285 [00:39<07:23,  1.68s/it]Loading train:   8%|▊         | 23/285 [00:40<07:00,  1.61s/it]Loading train:   8%|▊         | 24/285 [00:42<07:10,  1.65s/it]Loading train:   9%|▉         | 25/285 [00:43<07:06,  1.64s/it]Loading train:   9%|▉         | 26/285 [00:45<07:09,  1.66s/it]Loading train:   9%|▉         | 27/285 [00:47<06:55,  1.61s/it]Loading train:  10%|▉         | 28/285 [00:48<06:48,  1.59s/it]Loading train:  10%|█         | 29/285 [00:50<06:36,  1.55s/it]Loading train:  11%|█         | 30/285 [00:51<06:23,  1.50s/it]Loading train:  11%|█         | 31/285 [00:52<06:17,  1.49s/it]Loading train:  11%|█         | 32/285 [00:54<06:14,  1.48s/it]Loading train:  12%|█▏        | 33/285 [00:55<06:20,  1.51s/it]Loading train:  12%|█▏        | 34/285 [00:57<06:09,  1.47s/it]Loading train:  12%|█▏        | 35/285 [00:58<06:09,  1.48s/it]Loading train:  13%|█▎        | 36/285 [01:00<05:54,  1.42s/it]Loading train:  13%|█▎        | 37/285 [01:01<05:51,  1.42s/it]Loading train:  13%|█▎        | 38/285 [01:02<05:47,  1.41s/it]Loading train:  14%|█▎        | 39/285 [01:04<05:53,  1.44s/it]Loading train:  14%|█▍        | 40/285 [01:05<05:41,  1.39s/it]Loading train:  14%|█▍        | 41/285 [01:06<05:35,  1.37s/it]Loading train:  15%|█▍        | 42/285 [01:08<05:28,  1.35s/it]Loading train:  15%|█▌        | 43/285 [01:09<05:27,  1.35s/it]Loading train:  15%|█▌        | 44/285 [01:11<05:37,  1.40s/it]Loading train:  16%|█▌        | 45/285 [01:12<05:40,  1.42s/it]Loading train:  16%|█▌        | 46/285 [01:13<05:35,  1.40s/it]Loading train:  16%|█▋        | 47/285 [01:15<05:48,  1.46s/it]Loading train:  17%|█▋        | 48/285 [01:16<05:40,  1.44s/it]Loading train:  17%|█▋        | 49/285 [01:18<05:45,  1.46s/it]Loading train:  18%|█▊        | 50/285 [01:19<05:33,  1.42s/it]Loading train:  18%|█▊        | 51/285 [01:21<05:34,  1.43s/it]Loading train:  18%|█▊        | 52/285 [01:22<05:33,  1.43s/it]Loading train:  19%|█▊        | 53/285 [01:24<05:27,  1.41s/it]Loading train:  19%|█▉        | 54/285 [01:25<05:30,  1.43s/it]Loading train:  19%|█▉        | 55/285 [01:26<05:23,  1.41s/it]Loading train:  20%|█▉        | 56/285 [01:28<05:14,  1.37s/it]Loading train:  20%|██        | 57/285 [01:29<05:15,  1.38s/it]Loading train:  20%|██        | 58/285 [01:30<05:12,  1.38s/it]Loading train:  21%|██        | 59/285 [01:32<05:19,  1.41s/it]Loading train:  21%|██        | 60/285 [01:33<05:03,  1.35s/it]Loading train:  21%|██▏       | 61/285 [01:34<04:58,  1.33s/it]Loading train:  22%|██▏       | 62/285 [01:36<05:29,  1.48s/it]Loading train:  22%|██▏       | 63/285 [01:38<05:31,  1.49s/it]Loading train:  22%|██▏       | 64/285 [01:40<05:45,  1.57s/it]Loading train:  23%|██▎       | 65/285 [01:42<06:16,  1.71s/it]Loading train:  23%|██▎       | 66/285 [01:44<06:30,  1.78s/it]Loading train:  24%|██▎       | 67/285 [01:45<06:12,  1.71s/it]Loading train:  24%|██▍       | 68/285 [01:47<06:31,  1.81s/it]Loading train:  24%|██▍       | 69/285 [01:49<06:04,  1.69s/it]Loading train:  25%|██▍       | 70/285 [01:50<05:44,  1.60s/it]Loading train:  25%|██▍       | 71/285 [01:51<05:32,  1.56s/it]Loading train:  25%|██▌       | 72/285 [01:53<05:23,  1.52s/it]Loading train:  26%|██▌       | 73/285 [01:54<05:16,  1.50s/it]Loading train:  26%|██▌       | 74/285 [01:56<05:12,  1.48s/it]Loading train:  26%|██▋       | 75/285 [01:57<05:15,  1.50s/it]Loading train:  27%|██▋       | 76/285 [01:59<05:27,  1.57s/it]Loading train:  27%|██▋       | 77/285 [02:01<05:29,  1.58s/it]Loading train:  27%|██▋       | 78/285 [02:02<05:28,  1.59s/it]Loading train:  28%|██▊       | 79/285 [02:04<05:47,  1.69s/it]Loading train:  28%|██▊       | 80/285 [02:06<05:44,  1.68s/it]Loading train:  28%|██▊       | 81/285 [02:07<05:29,  1.62s/it]Loading train:  29%|██▉       | 82/285 [02:09<05:13,  1.55s/it]Loading train:  29%|██▉       | 83/285 [02:10<05:11,  1.54s/it]Loading train:  29%|██▉       | 84/285 [02:11<04:53,  1.46s/it]Loading train:  30%|██▉       | 85/285 [02:13<05:04,  1.52s/it]Loading train:  30%|███       | 86/285 [02:15<05:03,  1.52s/it]Loading train:  31%|███       | 87/285 [02:16<05:01,  1.52s/it]Loading train:  31%|███       | 88/285 [02:18<04:55,  1.50s/it]Loading train:  31%|███       | 89/285 [02:19<04:56,  1.51s/it]Loading train:  32%|███▏      | 90/285 [02:21<04:57,  1.53s/it]Loading train:  32%|███▏      | 91/285 [02:22<04:58,  1.54s/it]Loading train:  32%|███▏      | 92/285 [02:24<04:50,  1.50s/it]Loading train:  33%|███▎      | 93/285 [02:25<04:49,  1.51s/it]Loading train:  33%|███▎      | 94/285 [02:27<04:47,  1.51s/it]Loading train:  33%|███▎      | 95/285 [02:28<04:39,  1.47s/it]Loading train:  34%|███▎      | 96/285 [02:30<04:39,  1.48s/it]Loading train:  34%|███▍      | 97/285 [02:31<04:40,  1.49s/it]Loading train:  34%|███▍      | 98/285 [02:33<04:47,  1.54s/it]Loading train:  35%|███▍      | 99/285 [02:34<04:38,  1.50s/it]Loading train:  35%|███▌      | 100/285 [02:36<04:48,  1.56s/it]Loading train:  35%|███▌      | 101/285 [02:37<04:35,  1.50s/it]Loading train:  36%|███▌      | 102/285 [02:39<04:42,  1.54s/it]Loading train:  36%|███▌      | 103/285 [02:40<04:44,  1.56s/it]Loading train:  36%|███▋      | 104/285 [02:42<04:34,  1.52s/it]Loading train:  37%|███▋      | 105/285 [02:43<04:37,  1.54s/it]Loading train:  37%|███▋      | 106/285 [02:45<04:24,  1.48s/it]Loading train:  38%|███▊      | 107/285 [02:46<04:19,  1.46s/it]Loading train:  38%|███▊      | 108/285 [02:48<04:18,  1.46s/it]Loading train:  38%|███▊      | 109/285 [02:49<04:25,  1.51s/it]Loading train:  39%|███▊      | 110/285 [02:51<04:17,  1.47s/it]Loading train:  39%|███▉      | 111/285 [02:52<04:12,  1.45s/it]Loading train:  39%|███▉      | 112/285 [02:54<04:17,  1.49s/it]Loading train:  40%|███▉      | 113/285 [02:55<04:06,  1.43s/it]Loading train:  40%|████      | 114/285 [02:56<04:10,  1.46s/it]Loading train:  40%|████      | 115/285 [02:58<04:16,  1.51s/it]Loading train:  41%|████      | 116/285 [03:00<04:10,  1.48s/it]Loading train:  41%|████      | 117/285 [03:01<04:03,  1.45s/it]Loading train:  41%|████▏     | 118/285 [03:02<04:00,  1.44s/it]Loading train:  42%|████▏     | 119/285 [03:04<03:57,  1.43s/it]Loading train:  42%|████▏     | 120/285 [03:05<04:00,  1.46s/it]Loading train:  42%|████▏     | 121/285 [03:07<04:07,  1.51s/it]Loading train:  43%|████▎     | 122/285 [03:09<04:12,  1.55s/it]Loading train:  43%|████▎     | 123/285 [03:10<04:16,  1.58s/it]Loading train:  44%|████▎     | 124/285 [03:12<04:04,  1.52s/it]Loading train:  44%|████▍     | 125/285 [03:13<03:50,  1.44s/it]Loading train:  44%|████▍     | 126/285 [03:14<03:42,  1.40s/it]Loading train:  45%|████▍     | 127/285 [03:15<03:37,  1.38s/it]Loading train:  45%|████▍     | 128/285 [03:17<03:47,  1.45s/it]Loading train:  45%|████▌     | 129/285 [03:18<03:38,  1.40s/it]Loading train:  46%|████▌     | 130/285 [03:20<03:43,  1.44s/it]Loading train:  46%|████▌     | 131/285 [03:21<03:35,  1.40s/it]Loading train:  46%|████▋     | 132/285 [03:22<03:26,  1.35s/it]Loading train:  47%|████▋     | 133/285 [03:24<03:26,  1.36s/it]Loading train:  47%|████▋     | 134/285 [03:25<03:25,  1.36s/it]Loading train:  47%|████▋     | 135/285 [03:27<03:25,  1.37s/it]Loading train:  48%|████▊     | 136/285 [03:28<03:18,  1.33s/it]Loading train:  48%|████▊     | 137/285 [03:29<03:19,  1.35s/it]Loading train:  48%|████▊     | 138/285 [03:31<03:19,  1.36s/it]Loading train:  49%|████▉     | 139/285 [03:32<03:32,  1.46s/it]Loading train:  49%|████▉     | 140/285 [03:33<03:17,  1.37s/it]Loading train:  49%|████▉     | 141/285 [03:35<03:15,  1.36s/it]Loading train:  50%|████▉     | 142/285 [03:36<03:15,  1.37s/it]Loading train:  50%|█████     | 143/285 [03:37<03:12,  1.35s/it]Loading train:  51%|█████     | 144/285 [03:39<03:25,  1.46s/it]Loading train:  51%|█████     | 145/285 [03:41<03:20,  1.43s/it]Loading train:  51%|█████     | 146/285 [03:42<03:14,  1.40s/it]Loading train:  52%|█████▏    | 147/285 [03:43<03:08,  1.37s/it]Loading train:  52%|█████▏    | 148/285 [03:45<03:08,  1.38s/it]Loading train:  52%|█████▏    | 149/285 [03:46<02:51,  1.26s/it]Loading train:  53%|█████▎    | 150/285 [03:47<02:47,  1.24s/it]Loading train:  53%|█████▎    | 151/285 [03:48<02:51,  1.28s/it]Loading train:  53%|█████▎    | 152/285 [03:49<02:53,  1.30s/it]Loading train:  54%|█████▎    | 153/285 [03:51<03:01,  1.38s/it]Loading train:  54%|█████▍    | 154/285 [03:52<03:03,  1.40s/it]Loading train:  54%|█████▍    | 155/285 [03:54<02:59,  1.38s/it]Loading train:  55%|█████▍    | 156/285 [03:55<02:56,  1.37s/it]Loading train:  55%|█████▌    | 157/285 [03:56<02:50,  1.33s/it]Loading train:  55%|█████▌    | 158/285 [03:58<02:47,  1.32s/it]Loading train:  56%|█████▌    | 159/285 [03:59<02:45,  1.31s/it]Loading train:  56%|█████▌    | 160/285 [04:00<02:41,  1.29s/it]Loading train:  56%|█████▋    | 161/285 [04:02<02:42,  1.31s/it]Loading train:  57%|█████▋    | 162/285 [04:03<02:36,  1.27s/it]Loading train:  57%|█████▋    | 163/285 [04:04<02:33,  1.26s/it]Loading train:  58%|█████▊    | 164/285 [04:05<02:27,  1.22s/it]Loading train:  58%|█████▊    | 165/285 [04:06<02:26,  1.22s/it]Loading train:  58%|█████▊    | 166/285 [04:07<02:21,  1.19s/it]Loading train:  59%|█████▊    | 167/285 [04:09<02:24,  1.22s/it]Loading train:  59%|█████▉    | 168/285 [04:10<02:24,  1.23s/it]Loading train:  59%|█████▉    | 169/285 [04:11<02:19,  1.20s/it]Loading train:  60%|█████▉    | 170/285 [04:12<02:17,  1.19s/it]Loading train:  60%|██████    | 171/285 [04:14<02:26,  1.29s/it]Loading train:  60%|██████    | 172/285 [04:15<02:29,  1.33s/it]Loading train:  61%|██████    | 173/285 [04:17<02:27,  1.32s/it]Loading train:  61%|██████    | 174/285 [04:18<02:24,  1.30s/it]Loading train:  61%|██████▏   | 175/285 [04:19<02:16,  1.24s/it]Loading train:  62%|██████▏   | 176/285 [04:20<02:18,  1.27s/it]Loading train:  62%|██████▏   | 177/285 [04:21<02:15,  1.25s/it]Loading train:  62%|██████▏   | 178/285 [04:23<02:09,  1.21s/it]Loading train:  63%|██████▎   | 179/285 [04:24<02:03,  1.16s/it]Loading train:  63%|██████▎   | 180/285 [04:25<02:05,  1.19s/it]Loading train:  64%|██████▎   | 181/285 [04:26<02:02,  1.18s/it]Loading train:  64%|██████▍   | 182/285 [04:27<02:00,  1.17s/it]Loading train:  64%|██████▍   | 183/285 [04:28<02:00,  1.18s/it]Loading train:  65%|██████▍   | 184/285 [04:29<01:54,  1.13s/it]Loading train:  65%|██████▍   | 185/285 [04:31<01:57,  1.18s/it]Loading train:  65%|██████▌   | 186/285 [04:32<01:59,  1.21s/it]Loading train:  66%|██████▌   | 187/285 [04:33<02:00,  1.23s/it]Loading train:  66%|██████▌   | 188/285 [04:34<01:54,  1.18s/it]Loading train:  66%|██████▋   | 189/285 [04:35<01:50,  1.15s/it]Loading train:  67%|██████▋   | 190/285 [04:37<01:51,  1.17s/it]Loading train:  67%|██████▋   | 191/285 [04:38<01:52,  1.20s/it]Loading train:  67%|██████▋   | 192/285 [04:39<01:56,  1.25s/it]Loading train:  68%|██████▊   | 193/285 [04:40<01:51,  1.22s/it]Loading train:  68%|██████▊   | 194/285 [04:41<01:45,  1.16s/it]Loading train:  68%|██████▊   | 195/285 [04:43<01:45,  1.17s/it]Loading train:  69%|██████▉   | 196/285 [04:44<01:47,  1.21s/it]Loading train:  69%|██████▉   | 197/285 [04:45<01:49,  1.24s/it]Loading train:  69%|██████▉   | 198/285 [04:47<01:50,  1.27s/it]Loading train:  70%|██████▉   | 199/285 [04:48<01:46,  1.24s/it]Loading train:  70%|███████   | 200/285 [04:49<01:46,  1.25s/it]Loading train:  71%|███████   | 201/285 [04:50<01:46,  1.27s/it]Loading train:  71%|███████   | 202/285 [04:52<01:47,  1.29s/it]Loading train:  71%|███████   | 203/285 [04:53<01:46,  1.30s/it]Loading train:  72%|███████▏  | 204/285 [04:54<01:45,  1.31s/it]Loading train:  72%|███████▏  | 205/285 [04:56<01:44,  1.31s/it]Loading train:  72%|███████▏  | 206/285 [04:57<01:42,  1.30s/it]Loading train:  73%|███████▎  | 207/285 [04:58<01:39,  1.28s/it]Loading train:  73%|███████▎  | 208/285 [04:59<01:36,  1.26s/it]Loading train:  73%|███████▎  | 209/285 [05:01<01:38,  1.29s/it]Loading train:  74%|███████▎  | 210/285 [05:02<01:37,  1.30s/it]Loading train:  74%|███████▍  | 211/285 [05:03<01:36,  1.30s/it]Loading train:  74%|███████▍  | 212/285 [05:05<01:33,  1.28s/it]Loading train:  75%|███████▍  | 213/285 [05:06<01:31,  1.27s/it]Loading train:  75%|███████▌  | 214/285 [05:07<01:30,  1.27s/it]Loading train:  75%|███████▌  | 215/285 [05:08<01:28,  1.26s/it]Loading train:  76%|███████▌  | 216/285 [05:10<01:29,  1.30s/it]Loading train:  76%|███████▌  | 217/285 [05:11<01:30,  1.33s/it]Loading train:  76%|███████▋  | 218/285 [05:12<01:29,  1.34s/it]Loading train:  77%|███████▋  | 219/285 [05:14<01:25,  1.29s/it]Loading train:  77%|███████▋  | 220/285 [05:15<01:24,  1.30s/it]Loading train:  78%|███████▊  | 221/285 [05:16<01:20,  1.26s/it]Loading train:  78%|███████▊  | 222/285 [05:17<01:17,  1.24s/it]Loading train:  78%|███████▊  | 223/285 [05:19<01:17,  1.25s/it]Loading train:  79%|███████▊  | 224/285 [05:20<01:19,  1.30s/it]Loading train:  79%|███████▉  | 225/285 [05:21<01:15,  1.25s/it]Loading train:  79%|███████▉  | 226/285 [05:22<01:11,  1.21s/it]Loading train:  80%|███████▉  | 227/285 [05:23<01:08,  1.18s/it]Loading train:  80%|████████  | 228/285 [05:25<01:07,  1.19s/it]Loading train:  80%|████████  | 229/285 [05:26<01:07,  1.20s/it]Loading train:  81%|████████  | 230/285 [05:27<01:05,  1.19s/it]Loading train:  81%|████████  | 231/285 [05:28<01:04,  1.20s/it]Loading train:  81%|████████▏ | 232/285 [05:30<01:08,  1.30s/it]Loading train:  82%|████████▏ | 233/285 [05:31<01:08,  1.32s/it]Loading train:  82%|████████▏ | 234/285 [05:32<01:07,  1.33s/it]Loading train:  82%|████████▏ | 235/285 [05:34<01:05,  1.31s/it]Loading train:  83%|████████▎ | 236/285 [05:35<01:05,  1.35s/it]Loading train:  83%|████████▎ | 237/285 [05:36<01:04,  1.34s/it]Loading train:  84%|████████▎ | 238/285 [05:38<01:03,  1.35s/it]Loading train:  84%|████████▍ | 239/285 [05:39<01:06,  1.45s/it]Loading train:  84%|████████▍ | 240/285 [05:41<01:06,  1.47s/it]Loading train:  85%|████████▍ | 241/285 [05:43<01:07,  1.54s/it]Loading train:  85%|████████▍ | 242/285 [05:44<01:06,  1.55s/it]Loading train:  85%|████████▌ | 243/285 [05:46<01:02,  1.49s/it]Loading train:  86%|████████▌ | 244/285 [05:47<00:59,  1.45s/it]Loading train:  86%|████████▌ | 245/285 [05:48<00:57,  1.44s/it]Loading train:  86%|████████▋ | 246/285 [05:50<00:53,  1.38s/it]Loading train:  87%|████████▋ | 247/285 [05:51<00:51,  1.34s/it]Loading train:  87%|████████▋ | 248/285 [05:52<00:49,  1.33s/it]Loading train:  87%|████████▋ | 249/285 [05:54<00:48,  1.35s/it]Loading train:  88%|████████▊ | 250/285 [05:55<00:48,  1.39s/it]Loading train:  88%|████████▊ | 251/285 [05:56<00:44,  1.30s/it]Loading train:  88%|████████▊ | 252/285 [05:58<00:43,  1.33s/it]Loading train:  89%|████████▉ | 253/285 [05:59<00:41,  1.29s/it]Loading train:  89%|████████▉ | 254/285 [06:00<00:38,  1.23s/it]Loading train:  89%|████████▉ | 255/285 [06:01<00:37,  1.26s/it]Loading train:  90%|████████▉ | 256/285 [06:02<00:35,  1.23s/it]Loading train:  90%|█████████ | 257/285 [06:04<00:34,  1.23s/it]Loading train:  91%|█████████ | 258/285 [06:05<00:32,  1.20s/it]Loading train:  91%|█████████ | 259/285 [06:06<00:31,  1.23s/it]Loading train:  91%|█████████ | 260/285 [06:07<00:30,  1.22s/it]Loading train:  92%|█████████▏| 261/285 [06:08<00:29,  1.21s/it]Loading train:  92%|█████████▏| 262/285 [06:10<00:28,  1.26s/it]Loading train:  92%|█████████▏| 263/285 [06:11<00:27,  1.26s/it]Loading train:  93%|█████████▎| 264/285 [06:12<00:26,  1.28s/it]Loading train:  93%|█████████▎| 265/285 [06:14<00:25,  1.27s/it]Loading train:  93%|█████████▎| 266/285 [06:15<00:23,  1.26s/it]Loading train:  94%|█████████▎| 267/285 [06:16<00:22,  1.25s/it]Loading train:  94%|█████████▍| 268/285 [06:18<00:23,  1.37s/it]Loading train:  94%|█████████▍| 269/285 [06:19<00:22,  1.42s/it]Loading train:  95%|█████████▍| 270/285 [06:21<00:20,  1.40s/it]Loading train:  95%|█████████▌| 271/285 [06:23<00:21,  1.55s/it]Loading train:  95%|█████████▌| 272/285 [06:24<00:19,  1.52s/it]Loading train:  96%|█████████▌| 273/285 [06:26<00:18,  1.56s/it]Loading train:  96%|█████████▌| 274/285 [06:27<00:17,  1.59s/it]Loading train:  96%|█████████▋| 275/285 [06:29<00:15,  1.55s/it]Loading train:  97%|█████████▋| 276/285 [06:30<00:13,  1.51s/it]Loading train:  97%|█████████▋| 277/285 [06:32<00:11,  1.49s/it]Loading train:  98%|█████████▊| 278/285 [06:33<00:10,  1.48s/it]Loading train:  98%|█████████▊| 279/285 [06:35<00:08,  1.50s/it]Loading train:  98%|█████████▊| 280/285 [06:36<00:07,  1.53s/it]Loading train:  99%|█████████▊| 281/285 [06:38<00:05,  1.48s/it]Loading train:  99%|█████████▉| 282/285 [06:39<00:04,  1.51s/it]Loading train:  99%|█████████▉| 283/285 [06:41<00:03,  1.54s/it]Loading train: 100%|█████████▉| 284/285 [06:42<00:01,  1.52s/it]Loading train: 100%|██████████| 285/285 [06:44<00:00,  1.52s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|▏         | 4/285 [00:00<00:08, 32.55it/s]concatenating: train:   4%|▍         | 11/285 [00:00<00:07, 38.02it/s]concatenating: train:   6%|▌         | 17/285 [00:00<00:06, 41.99it/s]concatenating: train:   9%|▉         | 25/285 [00:00<00:05, 47.58it/s]concatenating: train:  11%|█         | 32/285 [00:00<00:04, 52.07it/s]concatenating: train:  14%|█▍        | 41/285 [00:00<00:04, 59.23it/s]concatenating: train:  17%|█▋        | 49/285 [00:00<00:03, 62.30it/s]concatenating: train:  20%|█▉        | 56/285 [00:00<00:03, 63.95it/s]concatenating: train:  24%|██▎       | 67/285 [00:00<00:03, 72.50it/s]concatenating: train:  26%|██▋       | 75/285 [00:01<00:02, 72.08it/s]concatenating: train:  29%|██▉       | 83/285 [00:01<00:02, 72.60it/s]concatenating: train:  32%|███▏      | 91/285 [00:01<00:02, 73.90it/s]concatenating: train:  38%|███▊      | 109/285 [00:01<00:01, 89.67it/s]concatenating: train:  47%|████▋     | 134/285 [00:01<00:01, 110.71it/s]concatenating: train:  52%|█████▏    | 149/285 [00:01<00:01, 104.79it/s]concatenating: train:  57%|█████▋    | 163/285 [00:01<00:01, 93.30it/s] concatenating: train:  62%|██████▏   | 177/285 [00:01<00:01, 101.94it/s]concatenating: train:  66%|██████▋   | 189/285 [00:02<00:01, 91.27it/s] concatenating: train:  70%|███████   | 200/285 [00:02<00:00, 87.10it/s]concatenating: train:  74%|███████▎  | 210/285 [00:02<00:00, 79.40it/s]concatenating: train:  79%|███████▊  | 224/285 [00:02<00:00, 91.16it/s]concatenating: train:  87%|████████▋ | 248/285 [00:02<00:00, 111.97it/s]concatenating: train:  92%|█████████▏| 263/285 [00:02<00:00, 87.48it/s] concatenating: train:  98%|█████████▊| 278/285 [00:02<00:00, 99.78it/s]concatenating: train: 100%|██████████| 285/285 [00:03<00:00, 93.27it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.53s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.51s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 29.15it/s]2019-07-07 01:05:03.519696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-07 01:05:03.519810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-07 01:05:03.519825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-07 01:05:03.519834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-07 01:05:03.520253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:09,  3.94it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:07,  4.68it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.43it/s]loading the weights for Unet:  20%|██        | 8/40 [00:00<00:05,  5.67it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:06,  4.96it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.32it/s]loading the weights for Unet:  30%|███       | 12/40 [00:01<00:05,  4.74it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:04,  6.00it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.03it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.60it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:02<00:04,  4.91it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:02,  5.85it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.16it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:03<00:02,  5.22it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:03<00:02,  5.80it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  5.07it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  6.11it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:04<00:00,  6.46it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:05<00:00,  5.28it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  5.88it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:05<00:00,  4.92it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:05<00:00,  7.21it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 52, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 52, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 52, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 52, 40)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 52, 40)   14440       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 52, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 52, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 52, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 52, 40)   14440       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 52, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 52, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 52, 40)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 52, 20)   7220        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 52, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 52, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 52, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 52, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 52, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 26, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 26, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 26, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 26, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 26, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 26, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 26, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 13, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 13, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 13, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 13, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 13, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 13, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 13, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 13, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 26, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 26, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 26, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 26, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 26, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 26, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 26, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 26, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 52, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 52, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 52, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 52, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 52, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 52, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 52, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 52, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 52, 40)   7240        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 52, 40)   160         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 52, 40)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 52, 40)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 52, 13)   533         dropout_9[0][0]                  
==================================================================================================
Total params: 228,553
Trainable params: 84,753
Non-trainable params: 143,800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.49841486e-02 3.19966680e-02 7.50970181e-02 9.33357939e-03
 2.71292049e-02 7.07427267e-03 8.46489586e-02 1.12779077e-01
 8.61338510e-02 1.32649165e-02 2.94521391e-01 1.92807035e-01
 2.29878984e-04]
Train on 18361 samples, validate on 179 samples
Epoch 1/300
 - 27s - loss: 111.5349 - acc: 0.6469 - mDice: 0.0170 - val_loss: 20.8822 - val_acc: 0.9136 - val_mDice: 0.0083

Epoch 00001: val_mDice improved from -inf to 0.00832, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 17s - loss: 18.6201 - acc: 0.8854 - mDice: 0.0173 - val_loss: 7.5851 - val_acc: 0.9136 - val_mDice: 0.0117

Epoch 00002: val_mDice improved from 0.00832 to 0.01170, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 17s - loss: 9.5118 - acc: 0.8860 - mDice: 0.0208 - val_loss: 5.5384 - val_acc: 0.9136 - val_mDice: 0.0206

Epoch 00003: val_mDice improved from 0.01170 to 0.02058, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 17s - loss: 7.1860 - acc: 0.8861 - mDice: 0.0279 - val_loss: 4.7060 - val_acc: 0.9136 - val_mDice: 0.0250

Epoch 00004: val_mDice improved from 0.02058 to 0.02505, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 17s - loss: 6.0268 - acc: 0.8862 - mDice: 0.0382 - val_loss: 4.1894 - val_acc: 0.9136 - val_mDice: 0.0434

Epoch 00005: val_mDice improved from 0.02505 to 0.04344, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 17s - loss: 5.2682 - acc: 0.8861 - mDice: 0.0524 - val_loss: 3.7112 - val_acc: 0.9136 - val_mDice: 0.0770

Epoch 00006: val_mDice improved from 0.04344 to 0.07698, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 17s - loss: 4.6761 - acc: 0.8862 - mDice: 0.0756 - val_loss: 3.3845 - val_acc: 0.9136 - val_mDice: 0.1051

Epoch 00007: val_mDice improved from 0.07698 to 0.10513, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 18s - loss: 4.1680 - acc: 0.8879 - mDice: 0.1069 - val_loss: 2.9884 - val_acc: 0.9175 - val_mDice: 0.1543

Epoch 00008: val_mDice improved from 0.10513 to 0.15434, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 17s - loss: 3.7309 - acc: 0.8925 - mDice: 0.1429 - val_loss: 2.8020 - val_acc: 0.9240 - val_mDice: 0.1924

Epoch 00009: val_mDice improved from 0.15434 to 0.19243, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 18s - loss: 3.3531 - acc: 0.8986 - mDice: 0.1835 - val_loss: 2.4804 - val_acc: 0.9319 - val_mDice: 0.2478

Epoch 00010: val_mDice improved from 0.19243 to 0.24777, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 17s - loss: 3.0180 - acc: 0.9055 - mDice: 0.2301 - val_loss: 2.1500 - val_acc: 0.9371 - val_mDice: 0.3154

Epoch 00011: val_mDice improved from 0.24777 to 0.31542, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 17s - loss: 2.7522 - acc: 0.9115 - mDice: 0.2730 - val_loss: 2.1703 - val_acc: 0.9391 - val_mDice: 0.3476

Epoch 00012: val_mDice improved from 0.31542 to 0.34761, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 17s - loss: 2.5471 - acc: 0.9166 - mDice: 0.3092 - val_loss: 2.2320 - val_acc: 0.9437 - val_mDice: 0.3636

Epoch 00013: val_mDice improved from 0.34761 to 0.36358, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 17s - loss: 2.3861 - acc: 0.9205 - mDice: 0.3401 - val_loss: 2.0100 - val_acc: 0.9442 - val_mDice: 0.3904

Epoch 00014: val_mDice improved from 0.36358 to 0.39040, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 17s - loss: 2.2562 - acc: 0.9240 - mDice: 0.3663 - val_loss: 2.0183 - val_acc: 0.9472 - val_mDice: 0.4086

Epoch 00015: val_mDice improved from 0.39040 to 0.40857, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 17s - loss: 2.1419 - acc: 0.9273 - mDice: 0.3921 - val_loss: 2.2096 - val_acc: 0.9460 - val_mDice: 0.4114

Epoch 00016: val_mDice improved from 0.40857 to 0.41141, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 18s - loss: 2.0504 - acc: 0.9291 - mDice: 0.4123 - val_loss: 2.1395 - val_acc: 0.9474 - val_mDice: 0.4212

Epoch 00017: val_mDice improved from 0.41141 to 0.42120, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 17s - loss: 1.9705 - acc: 0.9311 - mDice: 0.4306 - val_loss: 2.1894 - val_acc: 0.9454 - val_mDice: 0.4238

Epoch 00018: val_mDice improved from 0.42120 to 0.42382, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 17s - loss: 1.9058 - acc: 0.9327 - mDice: 0.4454 - val_loss: 1.9109 - val_acc: 0.9495 - val_mDice: 0.4623

Epoch 00019: val_mDice improved from 0.42382 to 0.46227, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 18s - loss: 1.8442 - acc: 0.9345 - mDice: 0.4607 - val_loss: 1.8390 - val_acc: 0.9515 - val_mDice: 0.4737

Epoch 00020: val_mDice improved from 0.46227 to 0.47369, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 17s - loss: 1.7885 - acc: 0.9363 - mDice: 0.4746 - val_loss: 1.9877 - val_acc: 0.9505 - val_mDice: 0.4633

Epoch 00021: val_mDice did not improve from 0.47369
Epoch 22/300
 - 18s - loss: 1.7449 - acc: 0.9378 - mDice: 0.4857 - val_loss: 1.9399 - val_acc: 0.9503 - val_mDice: 0.4735

Epoch 00022: val_mDice did not improve from 0.47369
Epoch 23/300
 - 18s - loss: 1.7052 - acc: 0.9389 - mDice: 0.4956 - val_loss: 1.9053 - val_acc: 0.9508 - val_mDice: 0.4853

Epoch 00023: val_mDice improved from 0.47369 to 0.48530, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 18s - loss: 1.6625 - acc: 0.9401 - mDice: 0.5061 - val_loss: 1.8238 - val_acc: 0.9516 - val_mDice: 0.4998

Epoch 00024: val_mDice improved from 0.48530 to 0.49977, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 18s - loss: 1.6321 - acc: 0.9410 - mDice: 0.5142 - val_loss: 1.8661 - val_acc: 0.9522 - val_mDice: 0.4993

Epoch 00025: val_mDice did not improve from 0.49977
Epoch 26/300
 - 18s - loss: 1.5955 - acc: 0.9420 - mDice: 0.5237 - val_loss: 1.9621 - val_acc: 0.9522 - val_mDice: 0.4955

Epoch 00026: val_mDice did not improve from 0.49977
Epoch 27/300
 - 18s - loss: 1.5683 - acc: 0.9428 - mDice: 0.5308 - val_loss: 1.8418 - val_acc: 0.9515 - val_mDice: 0.5101

Epoch 00027: val_mDice improved from 0.49977 to 0.51015, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 18s - loss: 1.5454 - acc: 0.9434 - mDice: 0.5373 - val_loss: 1.9608 - val_acc: 0.9495 - val_mDice: 0.4972

Epoch 00028: val_mDice did not improve from 0.51015
Epoch 29/300
 - 18s - loss: 1.5171 - acc: 0.9442 - mDice: 0.5452 - val_loss: 1.9759 - val_acc: 0.9494 - val_mDice: 0.4960

Epoch 00029: val_mDice did not improve from 0.51015
Epoch 30/300
 - 18s - loss: 1.4958 - acc: 0.9447 - mDice: 0.5504 - val_loss: 1.9355 - val_acc: 0.9520 - val_mDice: 0.5094

Epoch 00030: val_mDice did not improve from 0.51015
Epoch 31/300
 - 18s - loss: 1.4738 - acc: 0.9453 - mDice: 0.5569 - val_loss: 1.9369 - val_acc: 0.9529 - val_mDice: 0.5085

Epoch 00031: val_mDice did not improve from 0.51015
Epoch 32/300
 - 18s - loss: 1.4549 - acc: 0.9457 - mDice: 0.5625 - val_loss: 2.0179 - val_acc: 0.9519 - val_mDice: 0.5017

Epoch 00032: val_mDice did not improve from 0.51015
Epoch 33/300
 - 18s - loss: 1.4450 - acc: 0.9460 - mDice: 0.5653 - val_loss: 2.1372 - val_acc: 0.9514 - val_mDice: 0.4919

Epoch 00033: val_mDice did not improve from 0.51015
Epoch 34/300
 - 18s - loss: 1.4152 - acc: 0.9467 - mDice: 0.5728 - val_loss: 2.1261 - val_acc: 0.9516 - val_mDice: 0.4934

Epoch 00034: val_mDice did not improve from 0.51015
Epoch 35/300
 - 18s - loss: 1.4030 - acc: 0.9469 - mDice: 0.5768 - val_loss: 2.0476 - val_acc: 0.9517 - val_mDice: 0.5078

Epoch 00035: val_mDice did not improve from 0.51015
Epoch 36/300
 - 18s - loss: 1.3862 - acc: 0.9474 - mDice: 0.5809 - val_loss: 2.1001 - val_acc: 0.9503 - val_mDice: 0.5073

Epoch 00036: val_mDice did not improve from 0.51015
Epoch 37/300
 - 18s - loss: 1.3733 - acc: 0.9476 - mDice: 0.5852 - val_loss: 2.1678 - val_acc: 0.9522 - val_mDice: 0.5050

Epoch 00037: val_mDice did not improve from 0.51015
Epoch 38/300
 - 17s - loss: 1.3600 - acc: 0.9479 - mDice: 0.5888 - val_loss: 2.1028 - val_acc: 0.9517 - val_mDice: 0.5158

Epoch 00038: val_mDice improved from 0.51015 to 0.51583, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 18s - loss: 1.3495 - acc: 0.9481 - mDice: 0.5916 - val_loss: 2.1218 - val_acc: 0.9519 - val_mDice: 0.5007

Epoch 00039: val_mDice did not improve from 0.51583
Epoch 40/300
 - 17s - loss: 1.3348 - acc: 0.9485 - mDice: 0.5953 - val_loss: 2.0978 - val_acc: 0.9520 - val_mDice: 0.5103

Epoch 00040: val_mDice did not improve from 0.51583
Epoch 41/300
 - 18s - loss: 1.3292 - acc: 0.9486 - mDice: 0.5977 - val_loss: 2.0847 - val_acc: 0.9519 - val_mDice: 0.5148

Epoch 00041: val_mDice did not improve from 0.51583
Epoch 42/300
 - 18s - loss: 1.3133 - acc: 0.9489 - mDice: 0.6018 - val_loss: 2.0465 - val_acc: 0.9504 - val_mDice: 0.5145

Epoch 00042: val_mDice did not improve from 0.51583
Epoch 43/300
 - 17s - loss: 1.3063 - acc: 0.9492 - mDice: 0.6044 - val_loss: 2.1930 - val_acc: 0.9506 - val_mDice: 0.5067

Epoch 00043: val_mDice did not improve from 0.51583
Epoch 44/300
 - 18s - loss: 1.2999 - acc: 0.9493 - mDice: 0.6061 - val_loss: 2.2082 - val_acc: 0.9511 - val_mDice: 0.5010

Epoch 00044: val_mDice did not improve from 0.51583
Epoch 45/300
 - 18s - loss: 1.2887 - acc: 0.9494 - mDice: 0.6089 - val_loss: 2.2125 - val_acc: 0.9521 - val_mDice: 0.5108

Epoch 00045: val_mDice did not improve from 0.51583
Epoch 46/300
 - 19s - loss: 1.2730 - acc: 0.9498 - mDice: 0.6133 - val_loss: 2.2372 - val_acc: 0.9497 - val_mDice: 0.5046

Epoch 00046: val_mDice did not improve from 0.51583
Epoch 47/300
 - 19s - loss: 1.2705 - acc: 0.9499 - mDice: 0.6138 - val_loss: 2.2097 - val_acc: 0.9497 - val_mDice: 0.5086

Epoch 00047: val_mDice did not improve from 0.51583
Epoch 48/300
 - 19s - loss: 1.2631 - acc: 0.9500 - mDice: 0.6163 - val_loss: 2.1442 - val_acc: 0.9510 - val_mDice: 0.5194

Epoch 00048: val_mDice improved from 0.51583 to 0.51941, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 19s - loss: 1.2554 - acc: 0.9502 - mDice: 0.6181 - val_loss: 2.1788 - val_acc: 0.9515 - val_mDice: 0.5208

Epoch 00049: val_mDice improved from 0.51941 to 0.52081, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 19s - loss: 1.2423 - acc: 0.9505 - mDice: 0.6221 - val_loss: 2.3966 - val_acc: 0.9495 - val_mDice: 0.4898

Epoch 00050: val_mDice did not improve from 0.52081
Epoch 51/300
 - 19s - loss: 1.2386 - acc: 0.9507 - mDice: 0.6229 - val_loss: 2.1209 - val_acc: 0.9512 - val_mDice: 0.5244

Epoch 00051: val_mDice improved from 0.52081 to 0.52442, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 19s - loss: 1.2257 - acc: 0.9510 - mDice: 0.6267 - val_loss: 2.1732 - val_acc: 0.9510 - val_mDice: 0.5181

Epoch 00052: val_mDice did not improve from 0.52442
Epoch 53/300
 - 19s - loss: 1.2215 - acc: 0.9510 - mDice: 0.6278 - val_loss: 2.1982 - val_acc: 0.9525 - val_mDice: 0.5231

Epoch 00053: val_mDice did not improve from 0.52442
Epoch 54/300
 - 19s - loss: 1.2097 - acc: 0.9515 - mDice: 0.6307 - val_loss: 2.1688 - val_acc: 0.9524 - val_mDice: 0.5264

Epoch 00054: val_mDice improved from 0.52442 to 0.52639, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 19s - loss: 1.2146 - acc: 0.9512 - mDice: 0.6297 - val_loss: 2.0814 - val_acc: 0.9511 - val_mDice: 0.5299

Epoch 00055: val_mDice improved from 0.52639 to 0.52985, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 19s - loss: 1.2045 - acc: 0.9515 - mDice: 0.6325 - val_loss: 2.2465 - val_acc: 0.9528 - val_mDice: 0.5242

Epoch 00056: val_mDice did not improve from 0.52985
Epoch 57/300
 - 19s - loss: 1.1928 - acc: 0.9518 - mDice: 0.6364 - val_loss: 2.2858 - val_acc: 0.9511 - val_mDice: 0.5067

Epoch 00057: val_mDice did not improve from 0.52985
Epoch 58/300
 - 19s - loss: 1.1923 - acc: 0.9518 - mDice: 0.6360 - val_loss: 2.1957 - val_acc: 0.9493 - val_mDice: 0.5273

Epoch 00058: val_mDice did not improve from 0.52985
Epoch 59/300
 - 19s - loss: 1.1876 - acc: 0.9520 - mDice: 0.6375 - val_loss: 2.2795 - val_acc: 0.9520 - val_mDice: 0.5218

Epoch 00059: val_mDice did not improve from 0.52985
Epoch 60/300
 - 18s - loss: 1.1834 - acc: 0.9521 - mDice: 0.6387 - val_loss: 2.2292 - val_acc: 0.9511 - val_mDice: 0.5256

Epoch 00060: val_mDice did not improve from 0.52985
Epoch 61/300
 - 17s - loss: 1.1739 - acc: 0.9524 - mDice: 0.6413 - val_loss: 2.1981 - val_acc: 0.9521 - val_mDice: 0.5297

Epoch 00061: val_mDice did not improve from 0.52985
Epoch 62/300
 - 18s - loss: 1.1704 - acc: 0.9525 - mDice: 0.6421 - val_loss: 2.3218 - val_acc: 0.9509 - val_mDice: 0.5204

Epoch 00062: val_mDice did not improve from 0.52985
Epoch 63/300
 - 17s - loss: 1.1637 - acc: 0.9526 - mDice: 0.6444 - val_loss: 2.2509 - val_acc: 0.9521 - val_mDice: 0.5251

Epoch 00063: val_mDice did not improve from 0.52985
Epoch 64/300
 - 17s - loss: 1.1620 - acc: 0.9526 - mDice: 0.6443 - val_loss: 2.2790 - val_acc: 0.9524 - val_mDice: 0.5284

Epoch 00064: val_mDice did not improve from 0.52985
Epoch 65/300
 - 17s - loss: 1.1526 - acc: 0.9529 - mDice: 0.6474 - val_loss: 2.2903 - val_acc: 0.9512 - val_mDice: 0.5208

Epoch 00065: val_mDice did not improve from 0.52985
Epoch 66/300
 - 17s - loss: 1.1477 - acc: 0.9530 - mDice: 0.6487 - val_loss: 2.2831 - val_acc: 0.9510 - val_mDice: 0.5271

Epoch 00066: val_mDice did not improve from 0.52985
Epoch 67/300
 - 17s - loss: 1.1509 - acc: 0.9529 - mDice: 0.6480 - val_loss: 2.2562 - val_acc: 0.9495 - val_mDice: 0.5136

Epoch 00067: val_mDice did not improve from 0.52985
Epoch 68/300
 - 17s - loss: 1.1435 - acc: 0.9531 - mDice: 0.6498 - val_loss: 2.3339 - val_acc: 0.9519 - val_mDice: 0.5224

Epoch 00068: val_mDice did not improve from 0.52985
Epoch 69/300
 - 17s - loss: 1.1445 - acc: 0.9531 - mDice: 0.6496 - val_loss: 2.3024 - val_acc: 0.9508 - val_mDice: 0.5146

Epoch 00069: val_mDice did not improve from 0.52985
Epoch 70/300
 - 17s - loss: 1.1352 - acc: 0.9534 - mDice: 0.6521 - val_loss: 2.3059 - val_acc: 0.9504 - val_mDice: 0.5190

Epoch 00070: val_mDice did not improve from 0.52985
Epoch 71/300
 - 17s - loss: 1.1275 - acc: 0.9534 - mDice: 0.6542 - val_loss: 2.3335 - val_acc: 0.9505 - val_mDice: 0.5130

Epoch 00071: val_mDice did not improve from 0.52985
Epoch 72/300
 - 17s - loss: 1.1266 - acc: 0.9536 - mDice: 0.6541 - val_loss: 2.2611 - val_acc: 0.9510 - val_mDice: 0.5235

Epoch 00072: val_mDice did not improve from 0.52985
Epoch 73/300
 - 17s - loss: 1.1246 - acc: 0.9536 - mDice: 0.6549 - val_loss: 2.1979 - val_acc: 0.9528 - val_mDice: 0.5361

Epoch 00073: val_mDice improved from 0.52985 to 0.53615, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM40_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 18s - loss: 1.1192 - acc: 0.9536 - mDice: 0.6569 - val_loss: 2.2768 - val_acc: 0.9503 - val_mDice: 0.5257

Epoch 00074: val_mDice did not improve from 0.53615
Epoch 75/300
 - 17s - loss: 1.1111 - acc: 0.9538 - mDice: 0.6589 - val_loss: 2.3809 - val_acc: 0.9515 - val_mDice: 0.5145

Epoch 00075: val_mDice did not improve from 0.53615
Epoch 76/300
 - 17s - loss: 1.1113 - acc: 0.9537 - mDice: 0.6590 - val_loss: 2.2967 - val_acc: 0.9515 - val_mDice: 0.5317

Epoch 00076: val_mDice did not improve from 0.53615
Epoch 77/300
 - 17s - loss: 1.1121 - acc: 0.9537 - mDice: 0.6589 - val_loss: 2.3080 - val_acc: 0.9517 - val_mDice: 0.5253

Epoch 00077: val_mDice did not improve from 0.53615
Epoch 78/300
 - 17s - loss: 1.1024 - acc: 0.9539 - mDice: 0.6620 - val_loss: 2.3711 - val_acc: 0.9509 - val_mDice: 0.5236

Epoch 00078: val_mDice did not improve from 0.53615
Epoch 79/300
 - 17s - loss: 1.1024 - acc: 0.9539 - mDice: 0.6614 - val_loss: 2.3196 - val_acc: 0.9491 - val_mDice: 0.5246

Epoch 00079: val_mDice did not improve from 0.53615
Epoch 80/300
 - 17s - loss: 1.0965 - acc: 0.9541 - mDice: 0.6634 - val_loss: 2.2456 - val_acc: 0.9515 - val_mDice: 0.5289

Epoch 00080: val_mDice did not improve from 0.53615
Epoch 81/300
 - 17s - loss: 1.0969 - acc: 0.9541 - mDice: 0.6629 - val_loss: 2.4095 - val_acc: 0.9513 - val_mDice: 0.5226

Epoch 00081: val_mDice did not improve from 0.53615
Epoch 82/300
 - 17s - loss: 1.0885 - acc: 0.9542 - mDice: 0.6659 - val_loss: 2.5566 - val_acc: 0.9500 - val_mDice: 0.5004

Epoch 00082: val_mDice did not improve from 0.53615
Epoch 83/300
 - 18s - loss: 1.0910 - acc: 0.9542 - mDice: 0.6652 - val_loss: 2.2973 - val_acc: 0.9522 - val_mDice: 0.5312

Epoch 00083: val_mDice did not improve from 0.53615
Epoch 84/300
 - 17s - loss: 1.0900 - acc: 0.9543 - mDice: 0.6658 - val_loss: 2.4627 - val_acc: 0.9510 - val_mDice: 0.5162

Epoch 00084: val_mDice did not improve from 0.53615
Epoch 85/300
 - 17s - loss: 1.0833 - acc: 0.9544 - mDice: 0.6676 - val_loss: 2.3035 - val_acc: 0.9508 - val_mDice: 0.5327

Epoch 00085: val_mDice did not improve from 0.53615
Epoch 86/300
 - 17s - loss: 1.0828 - acc: 0.9544 - mDice: 0.6675 - val_loss: 2.4462 - val_acc: 0.9510 - val_mDice: 0.5178

Epoch 00086: val_mDice did not improve from 0.53615
Epoch 87/300
 - 17s - loss: 1.0784 - acc: 0.9545 - mDice: 0.6684 - val_loss: 2.4084 - val_acc: 0.9503 - val_mDice: 0.5180

Epoch 00087: val_mDice did not improve from 0.53615
Epoch 88/300
 - 18s - loss: 1.0754 - acc: 0.9546 - mDice: 0.6695 - val_loss: 2.3183 - val_acc: 0.9514 - val_mDice: 0.5206

Epoch 00088: val_mDice did not improve from 0.53615
Epoch 89/300
 - 18s - loss: 1.0735 - acc: 0.9546 - mDice: 0.6703 - val_loss: 2.3423 - val_acc: 0.9513 - val_mDice: 0.5306

Epoch 00089: val_mDice did not improve from 0.53615
Epoch 90/300
 - 18s - loss: 1.0748 - acc: 0.9546 - mDice: 0.6698 - val_loss: 2.2576 - val_acc: 0.9519 - val_mDice: 0.5356

Epoch 00090: val_mDice did not improve from 0.53615
Epoch 91/300
 - 17s - loss: 1.0692 - acc: 0.9548 - mDice: 0.6717 - val_loss: 2.2416 - val_acc: 0.9505 - val_mDice: 0.5335

Epoch 00091: val_mDice did not improve from 0.53615
Epoch 92/300
 - 18s - loss: 1.0644 - acc: 0.9548 - mDice: 0.6724 - val_loss: 2.3676 - val_acc: 0.9510 - val_mDice: 0.5225

Epoch 00092: val_mDice did not improve from 0.53615
Epoch 93/300
 - 17s - loss: 1.0629 - acc: 0.9549 - mDice: 0.6731 - val_loss: 2.2872 - val_acc: 0.9515 - val_mDice: 0.5328

Epoch 00093: val_mDice did not improve from 0.53615
Epoch 94/300
 - 17s - loss: 1.0594 - acc: 0.9550 - mDice: 0.6745 - val_loss: 2.4224 - val_acc: 0.9515 - val_mDice: 0.5254

Epoch 00094: val_mDice did not improve from 0.53615
Epoch 95/300
 - 17s - loss: 1.0636 - acc: 0.9549 - mDice: 0.6731 - val_loss: 2.4078 - val_acc: 0.9509 - val_mDice: 0.5315

Epoch 00095: val_mDice did not improve from 0.53615
Epoch 96/300
 - 18s - loss: 1.0528 - acc: 0.9551 - mDice: 0.6757 - val_loss: 2.4641 - val_acc: 0.9511 - val_mDice: 0.5274

Epoch 00096: val_mDice did not improve from 0.53615
Epoch 97/300
 - 17s - loss: 1.0541 - acc: 0.9551 - mDice: 0.6757 - val_loss: 2.3207 - val_acc: 0.9512 - val_mDice: 0.5315

Epoch 00097: val_mDice did not improve from 0.53615
Epoch 98/300
 - 17s - loss: 1.0525 - acc: 0.9552 - mDice: 0.6764 - val_loss: 2.3750 - val_acc: 0.9510 - val_mDice: 0.5249

Epoch 00098: val_mDice did not improve from 0.53615
Epoch 99/300
 - 17s - loss: 1.0519 - acc: 0.9552 - mDice: 0.6765 - val_loss: 2.4609 - val_acc: 0.9509 - val_mDice: 0.5201

Epoch 00099: val_mDice did not improve from 0.53615
Epoch 100/300
 - 17s - loss: 1.0478 - acc: 0.9553 - mDice: 0.6778 - val_loss: 2.3261 - val_acc: 0.9514 - val_mDice: 0.5273

Epoch 00100: val_mDice did not improve from 0.53615
Epoch 101/300
 - 18s - loss: 1.0510 - acc: 0.9552 - mDice: 0.6766 - val_loss: 2.3388 - val_acc: 0.9519 - val_mDice: 0.5258

Epoch 00101: val_mDice did not improve from 0.53615
Epoch 102/300
 - 17s - loss: 1.0477 - acc: 0.9552 - mDice: 0.6778 - val_loss: 2.3931 - val_acc: 0.9525 - val_mDice: 0.5294

Epoch 00102: val_mDice did not improve from 0.53615
Epoch 103/300
 - 17s - loss: 1.0453 - acc: 0.9553 - mDice: 0.6781 - val_loss: 2.2414 - val_acc: 0.9499 - val_mDice: 0.5346

Epoch 00103: val_mDice did not improve from 0.53615
Restoring model weights from the end of the best epoch
Epoch 00103: early stopping
{'val_loss': [20.882156382725892, 7.5851208804039985, 5.538402392211573, 4.70600598617639, 4.189424999599351, 3.711175609567312, 3.3845481273182276, 2.988444654635211, 2.802035893807864, 2.480406157797275, 2.150045264366619, 2.1702943967041355, 2.232045495976283, 2.010033722030384, 2.018276839948899, 2.209557727728476, 2.139528159988659, 2.189422311729559, 1.9108662565327224, 1.838990208156948, 1.9876700060327626, 1.9398690828397953, 1.9052975663925682, 1.823841713660256, 1.8660962748127943, 1.9621000383153309, 1.8418031944242936, 1.960751144579669, 1.975921269901638, 1.9354516514186753, 1.936858658018059, 2.0178674319602923, 2.1371827218785633, 2.126093591391707, 2.0476332416747534, 2.1000827757339904, 2.1678381006144947, 2.102834038228296, 2.1218334776063204, 2.0977888480245066, 2.084739780958804, 2.0465080458358678, 2.193016623651515, 2.208203194527653, 2.212513177754493, 2.2371699916583867, 2.2096829987105044, 2.1442227310308533, 2.178763446861139, 2.396625400255512, 2.120938987039321, 2.1731840432023204, 2.1982228276449876, 2.168809982651439, 2.0814095022958083, 2.24650721443432, 2.2858177949596383, 2.1956781035694997, 2.279499219116552, 2.2292484890815265, 2.198098061471012, 2.3217652403442552, 2.2508885753887324, 2.2789967153325428, 2.290326366211449, 2.283106505537832, 2.256172934057992, 2.3338793083276164, 2.3023819936720353, 2.305925288013906, 2.333519365534436, 2.2611445634724707, 2.1978885727887714, 2.276770033649892, 2.380925654033043, 2.2966514672646974, 2.307977956100549, 2.371094579803211, 2.3196251751990293, 2.245615909885428, 2.4095475673675537, 2.556643139716633, 2.2973066122172265, 2.462740060337429, 2.3034712242680553, 2.4461579882232836, 2.4083539893507293, 2.3182659988296765, 2.3423267636219216, 2.257641969446363, 2.2416005014707254, 2.367594821493053, 2.2872368210520824, 2.4224218576314063, 2.4078296869160742, 2.464090524439039, 2.3207183310439468, 2.3750428327635014, 2.46092463472036, 2.3260876639595245, 2.338792987375952, 2.393068762464896, 2.2413671189846274], 'val_acc': [0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185566140287, 0.9136185722644102, 0.917467586821018, 0.9239839118952192, 0.9319092598041343, 0.9371281266212463, 0.9390929104895566, 0.9436609668438661, 0.9441815980985844, 0.9472062764221063, 0.9459893713450299, 0.9474129180668452, 0.945421219538044, 0.9495058146268962, 0.9514644249191497, 0.9505305460045458, 0.9502805524698182, 0.9507950094825063, 0.951635891831787, 0.952181322281587, 0.9521606804272316, 0.951505758575887, 0.9494706988334656, 0.9494458903147521, 0.9519623381465507, 0.9529065126813324, 0.9519189659443648, 0.9514272153044546, 0.9516400271953818, 0.9517309229467168, 0.9503218874585029, 0.9522144151799505, 0.9516648007504767, 0.9519086173792791, 0.9519933205743075, 0.9519024015138935, 0.9504003744551589, 0.9506070170988584, 0.9511317630719872, 0.9521090225134482, 0.9496566316934937, 0.949660778711628, 0.9509851086739055, 0.9514809493911999, 0.9495491898259637, 0.9511668888550231, 0.9509540962773328, 0.952516025338093, 0.9524334126344606, 0.9511462306843124, 0.9527618944977915, 0.9511173350850963, 0.9492785501080518, 0.951980925472089, 0.9511069805262475, 0.9520718079039504, 0.9508652620475385, 0.9520986965914678, 0.9524023972410064, 0.9511607219387033, 0.950997485128861, 0.9494954487464947, 0.9518528387533219, 0.9508177618074684, 0.9504417270921462, 0.950501626430277, 0.9509768279571107, 0.952803208508305, 0.9502970889293948, 0.9515140033301028, 0.9514520178294049, 0.9517143874861008, 0.9508590668273371, 0.9490739746466695, 0.9514871312919275, 0.9513259809776391, 0.9500223188426907, 0.952173077860358, 0.9509602954933764, 0.9507887892882917, 0.95099128458087, 0.9503053676482686, 0.9513528699981434, 0.9513404642403459, 0.9518714127593866, 0.9505429614189617, 0.9510243368548388, 0.9515367126997623, 0.9514995593598435, 0.9509003681843508, 0.9510656548611944, 0.951166884859181, 0.9510243565010625, 0.9508673418833556, 0.9514458239411508, 0.951879705130721, 0.9524829933763216, 0.9498838762331275], 'val_mDice': [0.008315346104888943, 0.01170219467463773, 0.020577186000580228, 0.02504626101704949, 0.04343733926142394, 0.07698036272432551, 0.10512846258765492, 0.15433722817698003, 0.19243171328272898, 0.2477742207949388, 0.3154212166477182, 0.3476117035197146, 0.3635769989237439, 0.39040191563148074, 0.408565379554333, 0.41140640331380196, 0.4211986301331547, 0.4238205202464951, 0.46226503762452964, 0.473690362116478, 0.4633038090593988, 0.4735168100402342, 0.48529877842471586, 0.4997678685454683, 0.49925671159888113, 0.4955203736294581, 0.5101482818246553, 0.4972180482728521, 0.4960141821280538, 0.5094442627283448, 0.5085000931883658, 0.5017232770027396, 0.4918500191672554, 0.49343006770704045, 0.507839782277965, 0.5073205299217608, 0.5050057862058032, 0.5158269959788083, 0.500691302994776, 0.5102924458141433, 0.5148081706222876, 0.5144697510996344, 0.5067408073214845, 0.5009783866019223, 0.5108160920982254, 0.5046039077156749, 0.5086240848349459, 0.5194066246794589, 0.5208147787515012, 0.48979132085539107, 0.5244237028020721, 0.5181088487529222, 0.5230536567432255, 0.5263855317451435, 0.5298542525182223, 0.524236014768398, 0.5067412956466888, 0.5272753261987058, 0.5218472129472808, 0.5255941482229606, 0.5297035002175656, 0.5204072408170007, 0.5250782190754427, 0.5283664484929772, 0.5208050688551791, 0.5271410117935202, 0.5135860731148852, 0.5224097582548024, 0.5146422406148644, 0.5189907860489531, 0.5129715477288103, 0.5234905214949027, 0.5361479913722204, 0.52567988950447, 0.514530928774253, 0.531701513675338, 0.5253217922908634, 0.523636260179168, 0.5245853236267687, 0.52886880742771, 0.5225667081065684, 0.5004152692230054, 0.5312009805074617, 0.5162480972998635, 0.5327303299690758, 0.5178312569357163, 0.5180136717897553, 0.5205962965941296, 0.5305988888500789, 0.535614330342362, 0.533537717005394, 0.5224610706947369, 0.5328182904413958, 0.5254188399741103, 0.5315495724451609, 0.5274070747737778, 0.5314636638377632, 0.5248714041110524, 0.520063319352752, 0.5272830228565791, 0.5258210917424889, 0.5294196212757899, 0.5346309873977853], 'loss': [111.5349094782914, 18.620060119563462, 9.511806252981659, 7.186043291204629, 6.0267723036853775, 5.268219617916656, 4.676142921868989, 4.16803885568403, 3.730861012098492, 3.3531495372906637, 3.0180348384861437, 2.752204481088468, 2.5471387966013794, 2.3860560685524423, 2.2561970833541065, 2.141851511950015, 2.050435483355024, 1.970473045591474, 1.9058325598685415, 1.844200637143043, 1.7884796272142995, 1.7448822835660254, 1.7052368955715804, 1.6625289956596603, 1.6321493877148383, 1.5955164895183114, 1.5682504940225137, 1.545438461012586, 1.5171460971990174, 1.4958052680673024, 1.473774892943246, 1.4549365484158099, 1.444977594346472, 1.4151679234666754, 1.4029696774076295, 1.386216807012224, 1.373255829650471, 1.3599566655976796, 1.3494901179059724, 1.3348096470175521, 1.3291869194092987, 1.3133365205414487, 1.306264227638122, 1.2998976873861965, 1.2887412957957898, 1.2730358657114427, 1.2705014156078527, 1.263085542281698, 1.2554478308395014, 1.2423405505221339, 1.2385851582029885, 1.2256888293485018, 1.2214848102161642, 1.2097107315432252, 1.214587312041788, 1.2044594326665163, 1.1928020175632637, 1.1923020249046496, 1.187594491148247, 1.1834373588844047, 1.1739470064721202, 1.170367076683782, 1.1636758936262424, 1.1619938784620547, 1.1526308895735093, 1.1477402108802846, 1.1508716252607027, 1.1434847554099123, 1.1444812510360411, 1.1352052237400465, 1.1275322224201638, 1.1265633756620588, 1.1246082905389199, 1.119223671845443, 1.1111102810573956, 1.111260050022617, 1.1121025902566213, 1.1024374456896735, 1.1023966010882504, 1.0964994990522452, 1.0968944031547783, 1.088471091544795, 1.0910043842612929, 1.0899515659693155, 1.083251157939762, 1.0828325707897772, 1.0784191068438986, 1.075378961062147, 1.0734654441001101, 1.074795435569715, 1.069154441307965, 1.0643925635904654, 1.0628597133397653, 1.0593846826685533, 1.0635960325489155, 1.0527529585147877, 1.054054370289255, 1.0525418884464224, 1.0518947135362677, 1.0477991467512717, 1.050998811154095, 1.0476827918233373, 1.045285538760063], 'acc': [0.6468906257195862, 0.8854347509818687, 0.8859526731597722, 0.8861289334533509, 0.8861519773374825, 0.8860704443415917, 0.8861636198051978, 0.8879391749851403, 0.8925497789481881, 0.898555060720062, 0.9055167687211276, 0.9115261591946168, 0.9166011521158042, 0.9205193438533085, 0.9240113364560082, 0.9272526278000279, 0.9291172678853281, 0.9310564970789662, 0.9326612300735421, 0.9344540853807073, 0.9362685936746836, 0.9377580991978675, 0.9389005958576243, 0.9400618878703421, 0.9410195461799036, 0.9420374909469709, 0.9428038049137649, 0.9433545583921881, 0.9441567236389913, 0.9447475406710771, 0.9452756386450917, 0.945739561605347, 0.9460048702383709, 0.9467177260130438, 0.9469400903906063, 0.9473760983054593, 0.9476065807213717, 0.9478933791872476, 0.9481329050278859, 0.948501257606597, 0.9486069216343074, 0.948901877197576, 0.9491517141288509, 0.9492977017164554, 0.949377724641445, 0.9497549801985362, 0.9498524463168287, 0.9499782705662797, 0.9501715318114337, 0.9505318056737849, 0.9506654453676229, 0.951028763442821, 0.9510415522343622, 0.9514656171424217, 0.9512081869312974, 0.9515416931465855, 0.9518387647997248, 0.9518285516910606, 0.9519598357013015, 0.9520500689853699, 0.9523550778210251, 0.9524773791441308, 0.9525807472065081, 0.9525710359539551, 0.9528865366841407, 0.9529793519163747, 0.9529332448912761, 0.9531466053803562, 0.9530837865548679, 0.9533735031389761, 0.9534435368728211, 0.953557782229382, 0.9535668035989935, 0.9536483986861669, 0.9537744649218407, 0.953736782041512, 0.9536938994001325, 0.9538664731556836, 0.9539179752727562, 0.9540644454726219, 0.9540741940799293, 0.9542135545438842, 0.9542349664891352, 0.9542901956478655, 0.9543913665222165, 0.9543862324265845, 0.9544867168492945, 0.9545655703240766, 0.9545852102476828, 0.9546214257032742, 0.9547822989544106, 0.9548221776885513, 0.9548977693501955, 0.9549540059186425, 0.9548935792517684, 0.9551401550873296, 0.9551053485166052, 0.9551520573502694, 0.9552123836653988, 0.9552598776046529, 0.9552064016749479, 0.9552102902149701, 0.9553088614782356], 'mDice': [0.016996716702994318, 0.017326828235578032, 0.020844985544438078, 0.02787997094222153, 0.038223047147654975, 0.05240621500094208, 0.07555032848060395, 0.10687685297433538, 0.14286098459903, 0.183492763786104, 0.23008381498774752, 0.2729915344752775, 0.30924426805842553, 0.34009563552808036, 0.3663483161184433, 0.39208390652227687, 0.4123120211151247, 0.43059324427897827, 0.4454045693025526, 0.46071712259866854, 0.47464844231109243, 0.4856677006324076, 0.49560221084374034, 0.5061219402644039, 0.5141564607815015, 0.5237099455985932, 0.530807437129182, 0.5373231567924082, 0.5451693772964504, 0.5503977968843277, 0.5568533015143383, 0.5625299450963043, 0.5652729399089597, 0.5727669111615361, 0.5768205430647768, 0.5809167371415072, 0.5852247925016604, 0.5887994187231788, 0.591644754532369, 0.5953490678102296, 0.5976884333755343, 0.601777513233264, 0.6043775031317437, 0.6060545829690156, 0.6088765321762939, 0.6133474755063828, 0.6138336380630087, 0.6162777258110919, 0.6181268905685667, 0.6221187363866303, 0.6228562034258727, 0.6267124321665758, 0.627820878599303, 0.6306860462869086, 0.629660125998953, 0.6325255810360679, 0.6363646989846228, 0.6359852452883501, 0.6375233403712237, 0.6386770173930816, 0.6412725492043404, 0.642138349192509, 0.6443963812967576, 0.6443065866186016, 0.6473942314200097, 0.6487006651381965, 0.6479952645440759, 0.649755813445565, 0.6495723721378672, 0.6520948846364462, 0.6542034524243211, 0.6541492102433248, 0.6549358995000649, 0.6569051673732924, 0.6589322467819225, 0.6589900519806188, 0.6589083133404255, 0.6619938020579962, 0.661447728630063, 0.6633636312387093, 0.6628528349338927, 0.6659069016415935, 0.6651693243300251, 0.6658303077675004, 0.6675863671287213, 0.6674557160944222, 0.6683953615855719, 0.669471206322753, 0.6703250521110116, 0.6697549225036034, 0.6717287077245925, 0.6724253332625936, 0.6731159579266287, 0.674515158683021, 0.6731049935287019, 0.6756641701489555, 0.6757403218722163, 0.6763614667155996, 0.6764988655858579, 0.677795261107306, 0.6766454698823777, 0.6778402794784505, 0.6781359477945873]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.63s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.33s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  2.11s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:24,  1.57s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:44,  1.64s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:34,  1.61s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:59,  1.70s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<07:43,  1.66s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<08:05,  1.74s/it]predicting train subjects:   2%|▏         | 7/285 [00:12<08:39,  1.87s/it]predicting train subjects:   3%|▎         | 8/285 [00:14<08:58,  1.95s/it]predicting train subjects:   3%|▎         | 9/285 [00:16<08:49,  1.92s/it]predicting train subjects:   4%|▎         | 10/285 [00:18<09:13,  2.01s/it]predicting train subjects:   4%|▍         | 11/285 [00:20<09:30,  2.08s/it]predicting train subjects:   4%|▍         | 12/285 [00:23<09:37,  2.12s/it]predicting train subjects:   5%|▍         | 13/285 [00:25<09:37,  2.12s/it]predicting train subjects:   5%|▍         | 14/285 [00:27<09:32,  2.11s/it]predicting train subjects:   5%|▌         | 15/285 [00:29<09:35,  2.13s/it]predicting train subjects:   6%|▌         | 16/285 [00:31<09:39,  2.15s/it]predicting train subjects:   6%|▌         | 17/285 [00:33<09:43,  2.18s/it]predicting train subjects:   6%|▋         | 18/285 [00:36<09:35,  2.15s/it]predicting train subjects:   7%|▋         | 19/285 [00:38<09:27,  2.13s/it]predicting train subjects:   7%|▋         | 20/285 [00:40<09:21,  2.12s/it]predicting train subjects:   7%|▋         | 21/285 [00:42<09:21,  2.13s/it]predicting train subjects:   8%|▊         | 22/285 [00:44<09:20,  2.13s/it]predicting train subjects:   8%|▊         | 23/285 [00:46<09:23,  2.15s/it]predicting train subjects:   8%|▊         | 24/285 [00:48<09:23,  2.16s/it]predicting train subjects:   9%|▉         | 25/285 [00:51<09:17,  2.15s/it]predicting train subjects:   9%|▉         | 26/285 [00:53<09:17,  2.15s/it]predicting train subjects:   9%|▉         | 27/285 [00:55<09:17,  2.16s/it]predicting train subjects:  10%|▉         | 28/285 [00:57<09:00,  2.10s/it]predicting train subjects:  10%|█         | 29/285 [00:59<08:42,  2.04s/it]predicting train subjects:  11%|█         | 30/285 [01:01<08:32,  2.01s/it]predicting train subjects:  11%|█         | 31/285 [01:03<08:24,  1.99s/it]predicting train subjects:  11%|█         | 32/285 [01:05<08:19,  1.98s/it]predicting train subjects:  12%|█▏        | 33/285 [01:07<08:14,  1.96s/it]predicting train subjects:  12%|█▏        | 34/285 [01:08<08:14,  1.97s/it]predicting train subjects:  12%|█▏        | 35/285 [01:10<08:12,  1.97s/it]predicting train subjects:  13%|█▎        | 36/285 [01:12<08:10,  1.97s/it]predicting train subjects:  13%|█▎        | 37/285 [01:14<08:07,  1.97s/it]predicting train subjects:  13%|█▎        | 38/285 [01:16<08:07,  1.97s/it]predicting train subjects:  14%|█▎        | 39/285 [01:18<08:06,  1.98s/it]predicting train subjects:  14%|█▍        | 40/285 [01:20<08:03,  1.97s/it]predicting train subjects:  14%|█▍        | 41/285 [01:22<07:59,  1.96s/it]predicting train subjects:  15%|█▍        | 42/285 [01:24<07:55,  1.96s/it]predicting train subjects:  15%|█▌        | 43/285 [01:26<07:49,  1.94s/it]predicting train subjects:  15%|█▌        | 44/285 [01:28<07:45,  1.93s/it]predicting train subjects:  16%|█▌        | 45/285 [01:30<07:42,  1.93s/it]predicting train subjects:  16%|█▌        | 46/285 [01:32<07:23,  1.86s/it]predicting train subjects:  16%|█▋        | 47/285 [01:33<07:12,  1.82s/it]predicting train subjects:  17%|█▋        | 48/285 [01:35<07:02,  1.78s/it]predicting train subjects:  17%|█▋        | 49/285 [01:37<06:57,  1.77s/it]predicting train subjects:  18%|█▊        | 50/285 [01:39<06:54,  1.76s/it]predicting train subjects:  18%|█▊        | 51/285 [01:40<06:52,  1.76s/it]predicting train subjects:  18%|█▊        | 52/285 [01:42<06:49,  1.76s/it]predicting train subjects:  19%|█▊        | 53/285 [01:44<06:46,  1.75s/it]predicting train subjects:  19%|█▉        | 54/285 [01:46<06:45,  1.76s/it]predicting train subjects:  19%|█▉        | 55/285 [01:47<06:44,  1.76s/it]predicting train subjects:  20%|█▉        | 56/285 [01:49<06:45,  1.77s/it]predicting train subjects:  20%|██        | 57/285 [01:51<06:39,  1.75s/it]predicting train subjects:  20%|██        | 58/285 [01:53<06:36,  1.75s/it]predicting train subjects:  21%|██        | 59/285 [01:54<06:34,  1.75s/it]predicting train subjects:  21%|██        | 60/285 [01:56<06:29,  1.73s/it]predicting train subjects:  21%|██▏       | 61/285 [01:58<06:24,  1.72s/it]predicting train subjects:  22%|██▏       | 62/285 [01:59<06:26,  1.73s/it]predicting train subjects:  22%|██▏       | 63/285 [02:01<06:20,  1.71s/it]predicting train subjects:  22%|██▏       | 64/285 [02:03<06:24,  1.74s/it]predicting train subjects:  23%|██▎       | 65/285 [02:05<06:49,  1.86s/it]predicting train subjects:  23%|██▎       | 66/285 [02:07<07:12,  1.97s/it]predicting train subjects:  24%|██▎       | 67/285 [02:09<07:04,  1.95s/it]predicting train subjects:  24%|██▍       | 68/285 [02:11<07:14,  2.00s/it]predicting train subjects:  24%|██▍       | 69/285 [02:14<07:26,  2.06s/it]predicting train subjects:  25%|██▍       | 70/285 [02:16<07:36,  2.12s/it]predicting train subjects:  25%|██▍       | 71/285 [02:18<07:37,  2.14s/it]predicting train subjects:  25%|██▌       | 72/285 [02:20<07:40,  2.16s/it]predicting train subjects:  26%|██▌       | 73/285 [02:22<07:30,  2.12s/it]predicting train subjects:  26%|██▌       | 74/285 [02:24<07:20,  2.09s/it]predicting train subjects:  26%|██▋       | 75/285 [02:26<07:29,  2.14s/it]predicting train subjects:  27%|██▋       | 76/285 [02:29<07:39,  2.20s/it]predicting train subjects:  27%|██▋       | 77/285 [02:31<07:27,  2.15s/it]predicting train subjects:  27%|██▋       | 78/285 [02:33<07:31,  2.18s/it]predicting train subjects:  28%|██▊       | 79/285 [02:35<07:14,  2.11s/it]predicting train subjects:  28%|██▊       | 80/285 [02:37<07:07,  2.08s/it]predicting train subjects:  28%|██▊       | 81/285 [02:39<06:55,  2.04s/it]predicting train subjects:  29%|██▉       | 82/285 [02:41<07:05,  2.10s/it]predicting train subjects:  29%|██▉       | 83/285 [02:43<07:09,  2.13s/it]predicting train subjects:  29%|██▉       | 84/285 [02:45<07:00,  2.09s/it]predicting train subjects:  30%|██▉       | 85/285 [02:48<07:09,  2.15s/it]predicting train subjects:  30%|███       | 86/285 [02:50<07:20,  2.21s/it]predicting train subjects:  31%|███       | 87/285 [02:52<07:20,  2.22s/it]predicting train subjects:  31%|███       | 88/285 [02:55<07:23,  2.25s/it]