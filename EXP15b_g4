*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
(0/456) train vimp2_845_05312013_VZ
(1/456) train vimp2_823_05202013_AJ
(2/456) train vimp2_915_07112013_LC
(3/456) train vimp2_901_07052013_AS
(4/456) train vimp2_ctrl_911_07082013_TTO
(5/456) train vimp2_ctrl_925_07152013_LS
(6/456) train vimp2_869_06142013_BL
(7/456) train vimp2_ANON724_03272013
(8/456) train vimp2_819_05172013_DS
(9/456) train vimp2_ctrl_918_07112013_TQ
(10/456) train vimp2_ctrl_902_07052013_SI
(11/456) train vimp2_ANON606_20130110
(12/456) train vimp2_943_07242013_PA
(13/456) train vimp2_824_05212013_JS
(14/456) train vimp2_ANON624_20130117
(15/456) train vimp2_ctrl_920_07122013_SW
(16/456) train vimp2_884_06272013_TS
(17/456) train vimp2_668_02282013_CD
(18/456) train vimp2_964_08092013_TG
(19/456) train vimp2_ctrl_921_07122013_MP
(20/456) train vimp2_972_08152013_DC
(21/456) train vimp2_988_08302013_CB
(22/456) train vimp2_ANON702_03152013
(23/456) train vimp2_ANON714_03222013
(24/456) train vimp2_972_08152013_DC_Aug0_Rot_-1_sd0
(25/456) train vimp2_972_08152013_DC_Aug0_Rot_1_sd2
(26/456) train vimp2_972_08152013_DC_Aug0_Rot_-7_sd1
(27/456) train vimp2_972_08152013_DC_Aug1_Rot_2_sd0
(28/456) train vimp2_972_08152013_DC_Aug1_Rot_5_sd1
(29/456) train vimp2_972_08152013_DC_Aug1_Rot_6_sd2
(30/456) train vimp2_972_08152013_DC_Aug2_Rot_-3_sd1
(31/456) train vimp2_972_08152013_DC_Aug2_Rot_-3_sd2
(32/456) train vimp2_972_08152013_DC_Aug2_Rot_-5_sd0
(33/456) train vimp2_972_08152013_DC_Aug3_Rot_2_sd1
(34/456) train vimp2_972_08152013_DC_Aug3_Rot_-3_sd0
(35/456) train vimp2_972_08152013_DC_Aug3_Rot_-6_sd2
(36/456) train vimp2_972_08152013_DC_Aug4_Rot_-2_sd2
(37/456) train vimp2_972_08152013_DC_Aug4_Rot_3_sd0
(38/456) train vimp2_972_08152013_DC_Aug4_Rot_6_sd1
(39/456) train vimp2_972_08152013_DC_Aug5_Rot_-1_sd2
(40/456) train vimp2_972_08152013_DC_Aug5_Rot_7_sd0
(41/456) train vimp2_972_08152013_DC_Aug5_Rot_7_sd1
(42/456) train vimp2_988_08302013_CB_Aug0_Rot_-3_sd2
(43/456) train vimp2_988_08302013_CB_Aug0_Rot_4_sd0
(44/456) train vimp2_988_08302013_CB_Aug0_Rot_-5_sd1
(45/456) train vimp2_988_08302013_CB_Aug1_Rot_-2_sd2
(46/456) train vimp2_988_08302013_CB_Aug1_Rot_-3_sd0
(47/456) train vimp2_988_08302013_CB_Aug1_Rot_-6_sd1
(48/456) train vimp2_988_08302013_CB_Aug2_Rot_-2_sd0
(49/456) train vimp2_988_08302013_CB_Aug2_Rot_4_sd1
(50/456) train vimp2_988_08302013_CB_Aug2_Rot_-5_sd2
(51/456) train vimp2_988_08302013_CB_Aug3_Rot_2_sd2
(52/456) train vimp2_988_08302013_CB_Aug3_Rot_-3_sd0
(53/456) train vimp2_988_08302013_CB_Aug3_Rot_3_sd1
(54/456) train vimp2_988_08302013_CB_Aug4_Rot_-2_sd0
(55/456) train vimp2_988_08302013_CB_Aug4_Rot_-6_sd1
(56/456) train vimp2_988_08302013_CB_Aug4_Rot_7_sd2
(57/456) train vimp2_988_08302013_CB_Aug5_Rot_-1_sd0
(58/456) train vimp2_988_08302013_CB_Aug5_Rot_-6_sd1
(59/456) train vimp2_988_08302013_CB_Aug5_Rot_7_sd2
(60/456) train vimp2_ANON702_03152013_Aug0_Rot_-2_sd1
(61/456) train vimp2_ANON702_03152013_Aug0_Rot_-3_sd2
(62/456) train vimp2_ANON702_03152013_Aug0_Rot_-4_sd0
(63/456) train vimp2_ANON702_03152013_Aug1_Rot_-4_sd1
(64/456) train vimp2_ANON702_03152013_Aug1_Rot_-5_sd2
(65/456) train vimp2_ANON702_03152013_Aug1_Rot_-7_sd0
(66/456) train vimp2_ANON702_03152013_Aug2_Rot_6_sd0
(67/456) train vimp2_ANON702_03152013_Aug2_Rot_6_sd2
(68/456) train vimp2_ANON702_03152013_Aug2_Rot_-7_sd1
(69/456) train vimp2_ANON702_03152013_Aug3_Rot_-1_sd2
(70/456) train vimp2_ANON702_03152013_Aug3_Rot_-3_sd0
(71/456) train vimp2_ANON702_03152013_Aug3_Rot_-6_sd1
(72/456) train vimp2_ANON702_03152013_Aug4_Rot_-2_sd0
(73/456) train vimp2_ANON702_03152013_Aug4_Rot_-3_sd2
(74/456) train vimp2_ANON702_03152013_Aug4_Rot_-7_sd1
(75/456) train vimp2_ANON702_03152013_Aug5_Rot_3_sd0
(76/456) train vimp2_ANON702_03152013_Aug5_Rot_3_sd2
(77/456) train vimp2_ANON702_03152013_Aug5_Rot_4_sd1
(78/456) train vimp2_ANON714_03222013_Aug0_Rot_-1_sd2
(79/456) train vimp2_ANON714_03222013_Aug0_Rot_-2_sd1
(80/456) train vimp2_ANON714_03222013_Aug0_Rot_4_sd0
(81/456) train vimp2_ANON714_03222013_Aug1_Rot_0_sd0
(82/456) train vimp2_ANON714_03222013_Aug1_Rot_1_sd1
(83/456) train vimp2_ANON714_03222013_Aug1_Rot_-6_sd2
(84/456) train vimp2_ANON714_03222013_Aug2_Rot_-2_sd0
(85/456) train vimp2_ANON714_03222013_Aug2_Rot_4_sd1
(86/456) train vimp2_ANON714_03222013_Aug2_Rot_6_sd2
(87/456) train vimp2_ANON714_03222013_Aug3_Rot_2_sd0
(88/456) train vimp2_ANON714_03222013_Aug3_Rot_-3_sd2
(89/456) train vimp2_ANON714_03222013_Aug3_Rot_-7_sd1
(90/456) train vimp2_ANON714_03222013_Aug4_Rot_1_sd0
(91/456) train vimp2_ANON714_03222013_Aug4_Rot_-2_sd1
(92/456) train vimp2_ANON714_03222013_Aug4_Rot_4_sd2
(93/456) train vimp2_ANON714_03222013_Aug5_Rot_1_sd0
(94/456) train vimp2_ANON714_03222013_Aug5_Rot_6_sd1
(95/456) train vimp2_ANON714_03222013_Aug5_Rot_-7_sd2
(96/456) train vimp2_668_02282013_CD_Aug0_Rot_7_sd0
(97/456) train vimp2_668_02282013_CD_Aug1_Rot_-1_sd0
(98/456) train vimp2_668_02282013_CD_Aug2_Rot_-4_sd0
(99/456) train vimp2_668_02282013_CD_Aug3_Rot_3_sd0
(100/456) train vimp2_668_02282013_CD_Aug4_Rot_-5_sd0
(101/456) train vimp2_668_02282013_CD_Aug5_Rot_-6_sd0
(102/456) train vimp2_819_05172013_DS_Aug0_Rot_1_sd0
(103/456) train vimp2_819_05172013_DS_Aug1_Rot_5_sd0
(104/456) train vimp2_819_05172013_DS_Aug2_Rot_-4_sd0
(105/456) train vimp2_819_05172013_DS_Aug3_Rot_2_sd0
(106/456) train vimp2_819_05172013_DS_Aug4_Rot_5_sd0
(107/456) train vimp2_819_05172013_DS_Aug5_Rot_4_sd0
(108/456) train vimp2_823_05202013_AJ_Aug0_Rot_-7_sd0
(109/456) train vimp2_823_05202013_AJ_Aug1_Rot_7_sd0
(110/456) train vimp2_823_05202013_AJ_Aug2_Rot_3_sd0
(111/456) train vimp2_823_05202013_AJ_Aug3_Rot_-5_sd0
(112/456) train vimp2_823_05202013_AJ_Aug4_Rot_-3_sd0
(113/456) train vimp2_823_05202013_AJ_Aug5_Rot_1_sd0
(114/456) train vimp2_824_05212013_JS_Aug0_Rot_-2_sd0
(115/456) train vimp2_824_05212013_JS_Aug1_Rot_6_sd0
(116/456) train vimp2_824_05212013_JS_Aug2_Rot_1_sd0
(117/456) train vimp2_824_05212013_JS_Aug3_Rot_-4_sd0
(118/456) train vimp2_824_05212013_JS_Aug4_Rot_7_sd0
(119/456) train vimp2_824_05212013_JS_Aug5_Rot_2_sd0
(120/456) train vimp2_845_05312013_VZ_Aug0_Rot_1_sd0
(121/456) train vimp2_845_05312013_VZ_Aug1_Rot_5_sd0
(122/456) train vimp2_845_05312013_VZ_Aug2_Rot_-6_sd0
(123/456) train vimp2_845_05312013_VZ_Aug3_Rot_3_sd0
(124/456) train vimp2_845_05312013_VZ_Aug4_Rot_-2_sd0
(125/456) train vimp2_845_05312013_VZ_Aug5_Rot_5_sd0
(126/456) train vimp2_869_06142013_BL_Aug0_Rot_-2_sd0
(127/456) train vimp2_869_06142013_BL_Aug1_Rot_-4_sd0
(128/456) train vimp2_869_06142013_BL_Aug2_Rot_-1_sd0
(129/456) train vimp2_869_06142013_BL_Aug3_Rot_3_sd0
(130/456) train vimp2_869_06142013_BL_Aug4_Rot_3_sd0
(131/456) train vimp2_869_06142013_BL_Aug5_Rot_1_sd0
(132/456) train vimp2_884_06272013_TS_Aug0_Rot_-7_sd0
(133/456) train vimp2_884_06272013_TS_Aug1_Rot_7_sd0
(134/456) train vimp2_884_06272013_TS_Aug2_Rot_-5_sd0
(135/456) train vimp2_884_06272013_TS_Aug3_Rot_-2_sd0
(136/456) train vimp2_884_06272013_TS_Aug4_Rot_6_sd0
(137/456) train vimp2_884_06272013_TS_Aug5_Rot_-1_sd0
(138/456) train vimp2_901_07052013_AS_Aug0_Rot_-4_sd0
(139/456) train vimp2_901_07052013_AS_Aug1_Rot_-2_sd0
(140/456) train vimp2_901_07052013_AS_Aug2_Rot_1_sd0
(141/456) train vimp2_901_07052013_AS_Aug3_Rot_2_sd0
(142/456) train vimp2_901_07052013_AS_Aug4_Rot_-7_sd0
(143/456) train vimp2_901_07052013_AS_Aug5_Rot_-5_sd0
(144/456) train vimp2_915_07112013_LC_Aug0_Rot_-2_sd0
(145/456) train vimp2_915_07112013_LC_Aug1_Rot_4_sd0
(146/456) train vimp2_915_07112013_LC_Aug2_Rot_-2_sd0
(147/456) train vimp2_915_07112013_LC_Aug3_Rot_-1_sd0
(148/456) train vimp2_915_07112013_LC_Aug4_Rot_1_sd0
(149/456) train vimp2_915_07112013_LC_Aug5_Rot_4_sd0
(150/456) train vimp2_943_07242013_PA_Aug0_Rot_-5_sd0
(151/456) train vimp2_943_07242013_PA_Aug1_Rot_-7_sd0
(152/456) train vimp2_943_07242013_PA_Aug2_Rot_-4_sd0
(153/456) train vimp2_943_07242013_PA_Aug3_Rot_6_sd0
(154/456) train vimp2_943_07242013_PA_Aug4_Rot_6_sd0
(155/456) train vimp2_943_07242013_PA_Aug5_Rot_5_sd0
(156/456) train vimp2_964_08092013_TG_Aug0_Rot_5_sd0
(157/456) train vimp2_964_08092013_TG_Aug1_Rot_-4_sd0
(158/456) train vimp2_964_08092013_TG_Aug2_Rot_7_sd0
(159/456) train vimp2_964_08092013_TG_Aug3_Rot_-1_sd0
(160/456) train vimp2_964_08092013_TG_Aug4_Rot_-1_sd0
(161/456) train vimp2_964_08092013_TG_Aug5_Rot_2_sd0
(162/456) train vimp2_ANON606_20130110_Aug0_Rot_1_sd0
(163/456) train vimp2_ANON606_20130110_Aug1_Rot_2_sd0
(164/456) train vimp2_ANON606_20130110_Aug2_Rot_-5_sd0
(165/456) train vimp2_ANON606_20130110_Aug3_Rot_-5_sd0
(166/456) train vimp2_ANON606_20130110_Aug4_Rot_3_sd0
(167/456) train vimp2_ANON606_20130110_Aug5_Rot_-1_sd0
(168/456) train vimp2_ANON624_20130117_Aug0_Rot_1_sd0
(169/456) train vimp2_ANON624_20130117_Aug1_Rot_-2_sd0
(170/456) train vimp2_ANON624_20130117_Aug2_Rot_5_sd0
(171/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd0
(172/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd0
(173/456) train vimp2_ANON624_20130117_Aug5_Rot_3_sd0
(174/456) train vimp2_ANON724_03272013_Aug0_Rot_-4_sd0
(175/456) train vimp2_ANON724_03272013_Aug1_Rot_-3_sd0
(176/456) train vimp2_ANON724_03272013_Aug2_Rot_-4_sd0
(177/456) train vimp2_ANON724_03272013_Aug3_Rot_5_sd0
(178/456) train vimp2_ANON724_03272013_Aug4_Rot_5_sd0
(179/456) train vimp2_ANON724_03272013_Aug5_Rot_-6_sd0
(180/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_5_sd0
(181/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_7_sd0
(182/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_4_sd0
(183/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_-4_sd0
(184/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_3_sd0
(185/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_-7_sd0
(186/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_6_sd0
(187/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_1_sd0
(188/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_4_sd0
(189/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_-2_sd0
(190/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_6_sd0
(191/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_-2_sd0
(192/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_5_sd0
(193/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_-3_sd0
(194/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_-4_sd0
(195/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_1_sd0
(196/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_3_sd0
(197/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_7_sd0
(198/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_7_sd0
(199/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_-6_sd0
(200/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_-4_sd0
(201/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_1_sd0
(202/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-2_sd0
(203/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_4_sd0
(204/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_-1_sd0
(205/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_-3_sd0
(206/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_6_sd0
(207/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_3_sd0
(208/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_1_sd0
(209/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_-5_sd0
(210/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_-5_sd0
(211/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_-2_sd0
(212/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_5_sd0
(213/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_3_sd0
(214/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_4_sd0
(215/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-7_sd0
(216/456) train vimp2_668_02282013_CD_Aug0_Rot_-5_sd1
(217/456) train vimp2_668_02282013_CD_Aug1_Rot_-6_sd1
(218/456) train vimp2_668_02282013_CD_Aug2_Rot_3_sd1
(219/456) train vimp2_668_02282013_CD_Aug3_Rot_4_sd1
(220/456) train vimp2_668_02282013_CD_Aug4_Rot_-6_sd1
(221/456) train vimp2_668_02282013_CD_Aug5_Rot_-2_sd1
(222/456) train vimp2_819_05172013_DS_Aug0_Rot_-1_sd1
(223/456) train vimp2_819_05172013_DS_Aug1_Rot_3_sd1
(224/456) train vimp2_819_05172013_DS_Aug2_Rot_6_sd1
(225/456) train vimp2_819_05172013_DS_Aug3_Rot_3_sd1
(226/456) train vimp2_819_05172013_DS_Aug4_Rot_-7_sd1
(227/456) train vimp2_819_05172013_DS_Aug5_Rot_-5_sd1
(228/456) train vimp2_823_05202013_AJ_Aug0_Rot_-7_sd1
(229/456) train vimp2_823_05202013_AJ_Aug1_Rot_3_sd1
(230/456) train vimp2_823_05202013_AJ_Aug2_Rot_6_sd1
(231/456) train vimp2_823_05202013_AJ_Aug3_Rot_-7_sd1
(232/456) train vimp2_823_05202013_AJ_Aug4_Rot_-4_sd1
(233/456) train vimp2_823_05202013_AJ_Aug5_Rot_-4_sd1
(234/456) train vimp2_824_05212013_JS_Aug0_Rot_7_sd1
(235/456) train vimp2_824_05212013_JS_Aug1_Rot_3_sd1
(236/456) train vimp2_824_05212013_JS_Aug2_Rot_3_sd1
(237/456) train vimp2_824_05212013_JS_Aug3_Rot_-4_sd1
(238/456) train vimp2_824_05212013_JS_Aug4_Rot_4_sd1
(239/456) train vimp2_824_05212013_JS_Aug5_Rot_6_sd1
(240/456) train vimp2_845_05312013_VZ_Aug0_Rot_5_sd1
(241/456) train vimp2_845_05312013_VZ_Aug1_Rot_6_sd1
(242/456) train vimp2_845_05312013_VZ_Aug2_Rot_1_sd1
(243/456) train vimp2_845_05312013_VZ_Aug3_Rot_-6_sd1
(244/456) train vimp2_845_05312013_VZ_Aug4_Rot_-2_sd1
(245/456) train vimp2_845_05312013_VZ_Aug5_Rot_-6_sd1
(246/456) train vimp2_869_06142013_BL_Aug0_Rot_-4_sd1
(247/456) train vimp2_869_06142013_BL_Aug1_Rot_-3_sd1
(248/456) train vimp2_869_06142013_BL_Aug2_Rot_-4_sd1
(249/456) train vimp2_869_06142013_BL_Aug3_Rot_1_sd1
(250/456) train vimp2_869_06142013_BL_Aug4_Rot_7_sd1
(251/456) train vimp2_869_06142013_BL_Aug5_Rot_5_sd1
(252/456) train vimp2_884_06272013_TS_Aug0_Rot_5_sd1
(253/456) train vimp2_884_06272013_TS_Aug1_Rot_-1_sd1
(254/456) train vimp2_884_06272013_TS_Aug2_Rot_-1_sd1
(255/456) train vimp2_884_06272013_TS_Aug3_Rot_3_sd1
(256/456) train vimp2_884_06272013_TS_Aug4_Rot_4_sd1
(257/456) train vimp2_884_06272013_TS_Aug5_Rot_3_sd1
(258/456) train vimp2_901_07052013_AS_Aug0_Rot_3_sd1
(259/456) train vimp2_901_07052013_AS_Aug1_Rot_5_sd1
(260/456) train vimp2_901_07052013_AS_Aug2_Rot_-5_sd1
(261/456) train vimp2_901_07052013_AS_Aug3_Rot_7_sd1
(262/456) train vimp2_901_07052013_AS_Aug4_Rot_7_sd1
(263/456) train vimp2_901_07052013_AS_Aug5_Rot_6_sd1
(264/456) train vimp2_915_07112013_LC_Aug0_Rot_-6_sd1
(265/456) train vimp2_915_07112013_LC_Aug1_Rot_-5_sd1
(266/456) train vimp2_915_07112013_LC_Aug2_Rot_7_sd1
(267/456) train vimp2_915_07112013_LC_Aug3_Rot_1_sd1
(268/456) train vimp2_915_07112013_LC_Aug4_Rot_6_sd1
(269/456) train vimp2_915_07112013_LC_Aug5_Rot_-7_sd1
(270/456) train vimp2_943_07242013_PA_Aug0_Rot_1_sd1
(271/456) train vimp2_943_07242013_PA_Aug1_Rot_6_sd1
(272/456) train vimp2_943_07242013_PA_Aug2_Rot_3_sd1
(273/456) train vimp2_943_07242013_PA_Aug3_Rot_6_sd1
(274/456) train vimp2_943_07242013_PA_Aug4_Rot_3_sd1
(275/456) train vimp2_943_07242013_PA_Aug5_Rot_3_sd1
(276/456) train vimp2_964_08092013_TG_Aug0_Rot_6_sd1
(277/456) train vimp2_964_08092013_TG_Aug1_Rot_-4_sd1
(278/456) train vimp2_964_08092013_TG_Aug2_Rot_5_sd1
(279/456) train vimp2_964_08092013_TG_Aug3_Rot_0_sd1
(280/456) train vimp2_964_08092013_TG_Aug4_Rot_3_sd1
(281/456) train vimp2_964_08092013_TG_Aug5_Rot_-1_sd1
(282/456) train vimp2_ANON606_20130110_Aug0_Rot_-6_sd1
(283/456) train vimp2_ANON606_20130110_Aug1_Rot_7_sd1
(284/456) train vimp2_ANON606_20130110_Aug2_Rot_1_sd1
(285/456) train vimp2_ANON606_20130110_Aug3_Rot_1_sd1
(286/456) train vimp2_ANON606_20130110_Aug4_Rot_4_sd1
(287/456) train vimp2_ANON606_20130110_Aug5_Rot_4_sd1
(288/456) train vimp2_ANON624_20130117_Aug0_Rot_7_sd1
(289/456) train vimp2_ANON624_20130117_Aug1_Rot_6_sd1
(290/456) train vimp2_ANON624_20130117_Aug2_Rot_-1_sd1
(291/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd1
(292/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd1
(293/456) train vimp2_ANON624_20130117_Aug5_Rot_4_sd1
(294/456) train vimp2_ANON724_03272013_Aug0_Rot_-2_sd1
(295/456) train vimp2_ANON724_03272013_Aug1_Rot_4_sd1
(296/456) train vimp2_ANON724_03272013_Aug2_Rot_-1_sd1
(297/456) train vimp2_ANON724_03272013_Aug3_Rot_-6_sd1
(298/456) train vimp2_ANON724_03272013_Aug4_Rot_-5_sd1
(299/456) train vimp2_ANON724_03272013_Aug5_Rot_-4_sd1
(300/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_-2_sd1
(301/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_7_sd1
(302/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_2_sd1
(303/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_-4_sd1
(304/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_-1_sd1
(305/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_-4_sd1
(306/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_7_sd1
(307/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_-1_sd1
(308/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_-5_sd1
(309/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_-3_sd1
(310/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_6_sd1
(311/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_6_sd1
(312/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_1_sd1
(313/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_-2_sd1
(314/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_3_sd1
(315/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_-1_sd1
(316/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_7_sd1
(317/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_6_sd1
(318/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_-3_sd1
(319/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_7_sd1
(320/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_-4_sd1
(321/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_-1_sd1
(322/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-5_sd1
(323/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_-5_sd1
(324/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_4_sd1
(325/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_3_sd1
(326/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_-1_sd1
(327/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_-2_sd1
(328/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_2_sd1
(329/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_4_sd1
(330/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_-2_sd1
(331/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_4_sd1
(332/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_-2_sd1
(333/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_2_sd1
(334/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_3_sd1
(335/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-4_sd1
(336/456) train vimp2_668_02282013_CD_Aug0_Rot_-3_sd2
(337/456) train vimp2_668_02282013_CD_Aug1_Rot_6_sd2
(338/456) train vimp2_668_02282013_CD_Aug2_Rot_-6_sd2
(339/456) train vimp2_668_02282013_CD_Aug3_Rot_7_sd2
(340/456) train vimp2_668_02282013_CD_Aug4_Rot_1_sd2
(341/456) train vimp2_668_02282013_CD_Aug5_Rot_7_sd2
(342/456) train vimp2_819_05172013_DS_Aug0_Rot_3_sd2
(343/456) train vimp2_819_05172013_DS_Aug1_Rot_-1_sd2
(344/456) train vimp2_819_05172013_DS_Aug2_Rot_-2_sd2
(345/456) train vimp2_819_05172013_DS_Aug3_Rot_-1_sd2
(346/456) train vimp2_819_05172013_DS_Aug4_Rot_7_sd2
(347/456) train vimp2_819_05172013_DS_Aug5_Rot_3_sd2
(348/456) train vimp2_823_05202013_AJ_Aug0_Rot_-6_sd2
(349/456) train vimp2_823_05202013_AJ_Aug1_Rot_-7_sd2
(350/456) train vimp2_823_05202013_AJ_Aug2_Rot_7_sd2
(351/456) train vimp2_823_05202013_AJ_Aug3_Rot_-2_sd2
(352/456) train vimp2_823_05202013_AJ_Aug4_Rot_1_sd2
(353/456) train vimp2_823_05202013_AJ_Aug5_Rot_1_sd2
(354/456) train vimp2_824_05212013_JS_Aug0_Rot_-5_sd2
(355/456) train vimp2_824_05212013_JS_Aug1_Rot_3_sd2
(356/456) train vimp2_824_05212013_JS_Aug2_Rot_5_sd2
(357/456) train vimp2_824_05212013_JS_Aug3_Rot_2_sd2
(358/456) train vimp2_824_05212013_JS_Aug4_Rot_5_sd2
(359/456) train vimp2_824_05212013_JS_Aug5_Rot_-7_sd2
(360/456) train vimp2_845_05312013_VZ_Aug0_Rot_-5_sd2
(361/456) train vimp2_845_05312013_VZ_Aug1_Rot_3_sd2
(362/456) train vimp2_845_05312013_VZ_Aug2_Rot_-3_sd2
(363/456) train vimp2_845_05312013_VZ_Aug3_Rot_-1_sd2
(364/456) train vimp2_845_05312013_VZ_Aug4_Rot_4_sd2
(365/456) train vimp2_845_05312013_VZ_Aug5_Rot_-2_sd2
(366/456) train vimp2_869_06142013_BL_Aug0_Rot_3_sd2
(367/456) train vimp2_869_06142013_BL_Aug1_Rot_7_sd2
(368/456) train vimp2_869_06142013_BL_Aug2_Rot_4_sd2
(369/456) train vimp2_869_06142013_BL_Aug3_Rot_1_sd2
(370/456) train vimp2_869_06142013_BL_Aug4_Rot_1_sd2
(371/456) train vimp2_869_06142013_BL_Aug5_Rot_1_sd2
(372/456) train vimp2_884_06272013_TS_Aug0_Rot_3_sd2
(373/456) train vimp2_884_06272013_TS_Aug1_Rot_-5_sd2
(374/456) train vimp2_884_06272013_TS_Aug2_Rot_-7_sd2
(375/456) train vimp2_884_06272013_TS_Aug3_Rot_-6_sd2
(376/456) train vimp2_884_06272013_TS_Aug4_Rot_-2_sd2
(377/456) train vimp2_884_06272013_TS_Aug5_Rot_-6_sd2
(378/456) train vimp2_901_07052013_AS_Aug0_Rot_2_sd2
(379/456) train vimp2_901_07052013_AS_Aug1_Rot_-3_sd2
(380/456) train vimp2_901_07052013_AS_Aug2_Rot_-6_sd2
(381/456) train vimp2_901_07052013_AS_Aug3_Rot_6_sd2
(382/456) train vimp2_901_07052013_AS_Aug4_Rot_-5_sd2
(383/456) train vimp2_901_07052013_AS_Aug5_Rot_1_sd2
(384/456) train vimp2_915_07112013_LC_Aug0_Rot_1_sd2
(385/456) train vimp2_915_07112013_LC_Aug1_Rot_7_sd2
(386/456) train vimp2_915_07112013_LC_Aug2_Rot_3_sd2
(387/456) train vimp2_915_07112013_LC_Aug3_Rot_6_sd2
(388/456) train vimp2_915_07112013_LC_Aug4_Rot_4_sd2
(389/456) train vimp2_915_07112013_LC_Aug5_Rot_4_sd2
(390/456) train vimp2_943_07242013_PA_Aug0_Rot_3_sd2
(391/456) train vimp2_943_07242013_PA_Aug1_Rot_-4_sd2
(392/456) train vimp2_943_07242013_PA_Aug2_Rot_3_sd2
(393/456) train vimp2_943_07242013_PA_Aug3_Rot_2_sd2
(394/456) train vimp2_943_07242013_PA_Aug4_Rot_-1_sd2
(395/456) train vimp2_943_07242013_PA_Aug5_Rot_-3_sd2
(396/456) train vimp2_964_08092013_TG_Aug0_Rot_2_sd2
(397/456) train vimp2_964_08092013_TG_Aug1_Rot_-6_sd2
(398/456) train vimp2_964_08092013_TG_Aug2_Rot_5_sd2
(399/456) train vimp2_964_08092013_TG_Aug3_Rot_-7_sd2
(400/456) train vimp2_964_08092013_TG_Aug4_Rot_-6_sd2
(401/456) train vimp2_964_08092013_TG_Aug5_Rot_5_sd2
(402/456) train vimp2_ANON606_20130110_Aug0_Rot_-6_sd2
(403/456) train vimp2_ANON606_20130110_Aug1_Rot_-5_sd2
(404/456) train vimp2_ANON606_20130110_Aug2_Rot_-3_sd2
(405/456) train vimp2_ANON606_20130110_Aug3_Rot_3_sd2
(406/456) train vimp2_ANON606_20130110_Aug4_Rot_-2_sd2
(407/456) train vimp2_ANON606_20130110_Aug5_Rot_-3_sd2
(408/456) train vimp2_ANON624_20130117_Aug0_Rot_2_sd2
(409/456) train vimp2_ANON624_20130117_Aug1_Rot_-2_sd2
(410/456) train vimp2_ANON624_20130117_Aug2_Rot_-6_sd2
(411/456) train vimp2_ANON624_20130117_Aug3_Rot_1_sd2
(412/456) train vimp2_ANON624_20130117_Aug4_Rot_-5_sd2
(413/456) train vimp2_ANON624_20130117_Aug5_Rot_-7_sd2
(414/456) train vimp2_ANON724_03272013_Aug0_Rot_-5_sd2
(415/456) train vimp2_ANON724_03272013_Aug1_Rot_6_sd2
(416/456) train vimp2_ANON724_03272013_Aug2_Rot_-3_sd2
(417/456) train vimp2_ANON724_03272013_Aug3_Rot_-6_sd2
(418/456) train vimp2_ANON724_03272013_Aug4_Rot_-1_sd2
(419/456) train vimp2_ANON724_03272013_Aug5_Rot_-7_sd2
(420/456) train vimp2_ctrl_902_07052013_SI_Aug0_Rot_-4_sd2
(421/456) train vimp2_ctrl_902_07052013_SI_Aug1_Rot_-3_sd2
(422/456) train vimp2_ctrl_902_07052013_SI_Aug2_Rot_-5_sd2
(423/456) train vimp2_ctrl_902_07052013_SI_Aug3_Rot_5_sd2
(424/456) train vimp2_ctrl_902_07052013_SI_Aug4_Rot_-2_sd2
(425/456) train vimp2_ctrl_902_07052013_SI_Aug5_Rot_5_sd2
(426/456) train vimp2_ctrl_911_07082013_TTO_Aug0_Rot_-3_sd2
(427/456) train vimp2_ctrl_911_07082013_TTO_Aug1_Rot_7_sd2
(428/456) train vimp2_ctrl_911_07082013_TTO_Aug2_Rot_3_sd2
(429/456) train vimp2_ctrl_911_07082013_TTO_Aug3_Rot_5_sd2
(430/456) train vimp2_ctrl_911_07082013_TTO_Aug4_Rot_7_sd2
(431/456) train vimp2_ctrl_911_07082013_TTO_Aug5_Rot_2_sd2
(432/456) train vimp2_ctrl_918_07112013_TQ_Aug0_Rot_-7_sd2
(433/456) train vimp2_ctrl_918_07112013_TQ_Aug1_Rot_5_sd2
(434/456) train vimp2_ctrl_918_07112013_TQ_Aug2_Rot_5_sd2
(435/456) train vimp2_ctrl_918_07112013_TQ_Aug3_Rot_4_sd2
(436/456) train vimp2_ctrl_918_07112013_TQ_Aug4_Rot_5_sd2
(437/456) train vimp2_ctrl_918_07112013_TQ_Aug5_Rot_-4_sd2
(438/456) train vimp2_ctrl_920_07122013_SW_Aug0_Rot_7_sd2
(439/456) train vimp2_ctrl_920_07122013_SW_Aug1_Rot_-2_sd2
(440/456) train vimp2_ctrl_920_07122013_SW_Aug2_Rot_1_sd2
(441/456) train vimp2_ctrl_920_07122013_SW_Aug3_Rot_-7_sd2
(442/456) train vimp2_ctrl_920_07122013_SW_Aug4_Rot_-3_sd2
(443/456) train vimp2_ctrl_920_07122013_SW_Aug5_Rot_7_sd2
(444/456) train vimp2_ctrl_921_07122013_MP_Aug0_Rot_1_sd2
(445/456) train vimp2_ctrl_921_07122013_MP_Aug1_Rot_-7_sd2
(446/456) train vimp2_ctrl_921_07122013_MP_Aug2_Rot_6_sd2
(447/456) train vimp2_ctrl_921_07122013_MP_Aug3_Rot_-2_sd2
(448/456) train vimp2_ctrl_921_07122013_MP_Aug4_Rot_-5_sd2
(449/456) train vimp2_ctrl_921_07122013_MP_Aug5_Rot_6_sd2
(450/456) train vimp2_ctrl_925_07152013_LS_Aug0_Rot_2_sd2
(451/456) train vimp2_ctrl_925_07152013_LS_Aug1_Rot_5_sd2
(452/456) train vimp2_ctrl_925_07152013_LS_Aug2_Rot_2_sd2
(453/456) train vimp2_ctrl_925_07152013_LS_Aug3_Rot_5_sd2
(454/456) train vimp2_ctrl_925_07152013_LS_Aug4_Rot_-4_sd2
(455/456) train vimp2_ctrl_925_07152013_LS_Aug5_Rot_-3_sd22019-07-06 16:59:47.519386: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-06 16:59:51.047893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:84:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-07-06 16:59:51.047978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 16:59:51.457781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 16:59:51.457852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 16:59:51.457866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 16:59:51.458368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a’: File exists
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<07:01,  1.48s/it]Loading train:   1%|          | 2/285 [00:03<07:27,  1.58s/it]Loading train:   1%|          | 3/285 [00:04<07:07,  1.52s/it]Loading train:   1%|▏         | 4/285 [00:07<08:17,  1.77s/it]Loading train:   2%|▏         | 5/285 [00:08<08:09,  1.75s/it]Loading train:   2%|▏         | 6/285 [00:10<08:20,  1.79s/it]Loading train:   2%|▏         | 7/285 [00:12<08:52,  1.91s/it]Loading train:   3%|▎         | 8/285 [00:15<09:25,  2.04s/it]Loading train:   3%|▎         | 9/285 [00:17<09:32,  2.07s/it]Loading train:   4%|▎         | 10/285 [00:19<09:30,  2.08s/it]Loading train:   4%|▍         | 11/285 [00:21<09:40,  2.12s/it]Loading train:   4%|▍         | 12/285 [00:23<08:59,  1.97s/it]Loading train:   5%|▍         | 13/285 [00:24<08:31,  1.88s/it]Loading train:   5%|▍         | 14/285 [00:26<08:13,  1.82s/it]Loading train:   5%|▌         | 15/285 [00:28<08:20,  1.85s/it]Loading train:   6%|▌         | 16/285 [00:30<08:12,  1.83s/it]Loading train:   6%|▌         | 17/285 [00:31<07:35,  1.70s/it]Loading train:   6%|▋         | 18/285 [00:33<07:31,  1.69s/it]Loading train:   7%|▋         | 19/285 [00:35<07:53,  1.78s/it]Loading train:   7%|▋         | 20/285 [00:37<08:30,  1.93s/it]Loading train:   7%|▋         | 21/285 [00:39<08:24,  1.91s/it]Loading train:   8%|▊         | 22/285 [00:41<08:12,  1.87s/it]Loading train:   8%|▊         | 23/285 [00:42<07:42,  1.76s/it]Loading train:   8%|▊         | 24/285 [00:44<07:01,  1.62s/it]Loading train:   9%|▉         | 25/285 [00:45<07:01,  1.62s/it]Loading train:   9%|▉         | 26/285 [00:47<06:41,  1.55s/it]Loading train:   9%|▉         | 27/285 [00:48<06:44,  1.57s/it]Loading train:  10%|▉         | 28/285 [00:50<06:46,  1.58s/it]Loading train:  10%|█         | 29/285 [00:51<06:21,  1.49s/it]Loading train:  11%|█         | 30/285 [00:52<06:09,  1.45s/it]Loading train:  11%|█         | 31/285 [00:54<05:41,  1.34s/it]Loading train:  11%|█         | 32/285 [00:55<05:53,  1.40s/it]Loading train:  12%|█▏        | 33/285 [00:57<06:10,  1.47s/it]Loading train:  12%|█▏        | 34/285 [00:58<06:05,  1.46s/it]Loading train:  12%|█▏        | 35/285 [01:00<06:30,  1.56s/it]Loading train:  13%|█▎        | 36/285 [01:01<06:18,  1.52s/it]Loading train:  13%|█▎        | 37/285 [01:03<06:20,  1.53s/it]Loading train:  13%|█▎        | 38/285 [01:05<06:26,  1.56s/it]Loading train:  14%|█▎        | 39/285 [01:06<06:15,  1.53s/it]Loading train:  14%|█▍        | 40/285 [01:08<06:30,  1.59s/it]Loading train:  14%|█▍        | 41/285 [01:09<06:13,  1.53s/it]Loading train:  15%|█▍        | 42/285 [01:10<05:52,  1.45s/it]Loading train:  15%|█▌        | 43/285 [01:12<05:38,  1.40s/it]Loading train:  15%|█▌        | 44/285 [01:14<06:11,  1.54s/it]Loading train:  16%|█▌        | 45/285 [01:16<06:46,  1.69s/it]Loading train:  16%|█▌        | 46/285 [01:17<06:54,  1.74s/it]Loading train:  16%|█▋        | 47/285 [01:19<06:31,  1.65s/it]Loading train:  17%|█▋        | 48/285 [01:20<05:50,  1.48s/it]Loading train:  17%|█▋        | 49/285 [01:21<05:29,  1.39s/it]Loading train:  18%|█▊        | 50/285 [01:22<05:07,  1.31s/it]Loading train:  18%|█▊        | 51/285 [01:23<04:56,  1.27s/it]Loading train:  18%|█▊        | 52/285 [01:25<04:43,  1.22s/it]Loading train:  19%|█▊        | 53/285 [01:26<05:02,  1.30s/it]Loading train:  19%|█▉        | 54/285 [01:27<05:05,  1.32s/it]Loading train:  19%|█▉        | 55/285 [01:29<05:13,  1.36s/it]Loading train:  20%|█▉        | 56/285 [01:30<05:03,  1.33s/it]Loading train:  20%|██        | 57/285 [01:32<05:23,  1.42s/it]Loading train:  20%|██        | 58/285 [01:33<05:36,  1.48s/it]Loading train:  21%|██        | 59/285 [01:35<05:47,  1.54s/it]Loading train:  21%|██        | 60/285 [01:36<05:36,  1.50s/it]Loading train:  21%|██▏       | 61/285 [01:38<05:15,  1.41s/it]Loading train:  22%|██▏       | 62/285 [01:39<05:10,  1.39s/it]Loading train:  22%|██▏       | 63/285 [01:40<05:03,  1.37s/it]Loading train:  22%|██▏       | 64/285 [01:42<05:39,  1.53s/it]Loading train:  23%|██▎       | 65/285 [01:44<06:10,  1.68s/it]Loading train:  23%|██▎       | 66/285 [01:46<06:20,  1.74s/it]Loading train:  24%|██▎       | 67/285 [01:48<06:01,  1.66s/it]Loading train:  24%|██▍       | 68/285 [01:49<05:29,  1.52s/it]Loading train:  24%|██▍       | 69/285 [01:50<04:52,  1.36s/it]Loading train:  25%|██▍       | 70/285 [01:51<04:38,  1.30s/it]Loading train:  25%|██▍       | 71/285 [01:52<04:40,  1.31s/it]Loading train:  25%|██▌       | 72/285 [01:54<04:45,  1.34s/it]Loading train:  26%|██▌       | 73/285 [01:55<04:39,  1.32s/it]Loading train:  26%|██▌       | 74/285 [01:57<05:09,  1.47s/it]Loading train:  26%|██▋       | 75/285 [01:58<05:03,  1.44s/it]Loading train:  27%|██▋       | 76/285 [01:59<04:55,  1.41s/it]Loading train:  27%|██▋       | 77/285 [02:01<05:26,  1.57s/it]Loading train:  27%|██▋       | 78/285 [02:03<05:31,  1.60s/it]Loading train:  28%|██▊       | 79/285 [02:04<05:01,  1.46s/it]Loading train:  28%|██▊       | 80/285 [02:05<04:40,  1.37s/it]Loading train:  28%|██▊       | 81/285 [02:07<04:54,  1.44s/it]Loading train:  29%|██▉       | 82/285 [02:09<04:59,  1.47s/it]Loading train:  29%|██▉       | 83/285 [02:10<04:58,  1.48s/it]Loading train:  29%|██▉       | 84/285 [02:11<04:44,  1.42s/it]Loading train:  30%|██▉       | 85/285 [02:13<04:38,  1.39s/it]Loading train:  30%|███       | 86/285 [02:14<04:57,  1.50s/it]Loading train:  31%|███       | 87/285 [02:16<04:57,  1.50s/it]Loading train:  31%|███       | 88/285 [02:18<05:23,  1.64s/it]Loading train:  31%|███       | 89/285 [02:20<05:28,  1.68s/it]Loading train:  32%|███▏      | 90/285 [02:22<05:53,  1.82s/it]Loading train:  32%|███▏      | 91/285 [02:24<06:06,  1.89s/it]Loading train:  32%|███▏      | 92/285 [02:26<06:23,  1.99s/it]Loading train:  33%|███▎      | 93/285 [02:28<06:18,  1.97s/it]Loading train:  33%|███▎      | 94/285 [02:30<06:36,  2.08s/it]Loading train:  33%|███▎      | 95/285 [02:32<05:59,  1.89s/it]Loading train:  34%|███▎      | 96/285 [02:33<05:33,  1.76s/it]Loading train:  34%|███▍      | 97/285 [02:35<05:05,  1.63s/it]Loading train:  34%|███▍      | 98/285 [02:36<05:14,  1.68s/it]Loading train:  35%|███▍      | 99/285 [02:38<05:01,  1.62s/it]Loading train:  35%|███▌      | 100/285 [02:40<05:07,  1.66s/it]Loading train:  35%|███▌      | 101/285 [02:42<05:27,  1.78s/it]Loading train:  36%|███▌      | 102/285 [02:43<05:27,  1.79s/it]Loading train:  36%|███▌      | 103/285 [02:45<05:35,  1.84s/it]Loading train:  36%|███▋      | 104/285 [02:47<05:16,  1.75s/it]Loading train:  37%|███▋      | 105/285 [02:48<04:41,  1.56s/it]Loading train:  37%|███▋      | 106/285 [02:50<05:01,  1.68s/it]Loading train:  38%|███▊      | 107/285 [02:52<05:27,  1.84s/it]Loading train:  38%|███▊      | 108/285 [02:55<05:54,  2.00s/it]Loading train:  38%|███▊      | 109/285 [02:57<06:11,  2.11s/it]Loading train:  39%|███▊      | 110/285 [02:59<06:20,  2.17s/it]Loading train:  39%|███▉      | 111/285 [03:01<06:05,  2.10s/it]Loading train:  39%|███▉      | 112/285 [03:03<05:35,  1.94s/it]Loading train:  40%|███▉      | 113/285 [03:05<05:23,  1.88s/it]Loading train:  40%|████      | 114/285 [03:07<05:31,  1.94s/it]Loading train:  40%|████      | 115/285 [03:09<05:42,  2.01s/it]Loading train:  41%|████      | 116/285 [03:11<05:40,  2.02s/it]Loading train:  41%|████      | 117/285 [03:13<05:53,  2.10s/it]Loading train:  41%|████▏     | 118/285 [03:15<05:19,  1.91s/it]Loading train:  42%|████▏     | 119/285 [03:17<05:32,  2.01s/it]Loading train:  42%|████▏     | 120/285 [03:18<05:09,  1.88s/it]Loading train:  42%|████▏     | 121/285 [03:20<05:16,  1.93s/it]Loading train:  43%|████▎     | 122/285 [03:22<05:17,  1.95s/it]Loading train:  43%|████▎     | 123/285 [03:24<05:13,  1.94s/it]Loading train:  44%|████▎     | 124/285 [03:26<05:12,  1.94s/it]Loading train:  44%|████▍     | 125/285 [03:28<04:55,  1.85s/it]Loading train:  44%|████▍     | 126/285 [03:30<04:56,  1.86s/it]Loading train:  45%|████▍     | 127/285 [03:32<05:05,  1.93s/it]Loading train:  45%|████▍     | 128/285 [03:34<04:50,  1.85s/it]Loading train:  45%|████▌     | 129/285 [03:35<04:13,  1.63s/it]Loading train:  46%|████▌     | 130/285 [03:36<04:05,  1.58s/it]Loading train:  46%|████▌     | 131/285 [03:38<04:03,  1.58s/it]Loading train:  46%|████▋     | 132/285 [03:40<04:21,  1.71s/it]Loading train:  47%|████▋     | 133/285 [03:42<04:32,  1.79s/it]Loading train:  47%|████▋     | 134/285 [03:43<04:24,  1.75s/it]Loading train:  47%|████▋     | 135/285 [03:45<04:01,  1.61s/it]Loading train:  48%|████▊     | 136/285 [03:46<03:58,  1.60s/it]Loading train:  48%|████▊     | 137/285 [03:48<03:55,  1.59s/it]Loading train:  48%|████▊     | 138/285 [03:50<04:19,  1.76s/it]Loading train:  49%|████▉     | 139/285 [03:52<04:10,  1.72s/it]Loading train:  49%|████▉     | 140/285 [03:53<04:13,  1.75s/it]Loading train:  49%|████▉     | 141/285 [03:55<04:09,  1.73s/it]Loading train:  50%|████▉     | 142/285 [03:57<04:13,  1.78s/it]Loading train:  50%|█████     | 143/285 [03:58<03:45,  1.59s/it]Loading train:  51%|█████     | 144/285 [04:00<03:52,  1.65s/it]Loading train:  51%|█████     | 145/285 [04:02<03:51,  1.65s/it]Loading train:  51%|█████     | 146/285 [04:04<04:00,  1.73s/it]Loading train:  52%|█████▏    | 147/285 [04:05<04:08,  1.80s/it]Loading train:  52%|█████▏    | 148/285 [04:07<03:55,  1.72s/it]Loading train:  52%|█████▏    | 149/285 [04:08<03:35,  1.59s/it]Loading train:  53%|█████▎    | 150/285 [04:10<03:25,  1.52s/it]Loading train:  53%|█████▎    | 151/285 [04:11<03:29,  1.57s/it]Loading train:  53%|█████▎    | 152/285 [04:13<03:26,  1.56s/it]Loading train:  54%|█████▎    | 153/285 [04:15<03:31,  1.60s/it]Loading train:  54%|█████▍    | 154/285 [04:16<03:40,  1.69s/it]Loading train:  54%|█████▍    | 155/285 [04:18<03:52,  1.79s/it]Loading train:  55%|█████▍    | 156/285 [04:20<03:27,  1.61s/it]Loading train:  55%|█████▌    | 157/285 [04:21<03:05,  1.45s/it]Loading train:  55%|█████▌    | 158/285 [04:22<02:57,  1.40s/it]Loading train:  56%|█████▌    | 159/285 [04:24<03:08,  1.50s/it]Loading train:  56%|█████▌    | 160/285 [04:26<03:19,  1.59s/it]Loading train:  56%|█████▋    | 161/285 [04:27<03:02,  1.47s/it]Loading train:  57%|█████▋    | 162/285 [04:28<03:02,  1.48s/it]Loading train:  57%|█████▋    | 163/285 [04:29<02:39,  1.31s/it]Loading train:  58%|█████▊    | 164/285 [04:30<02:32,  1.26s/it]Loading train:  58%|█████▊    | 165/285 [04:31<02:15,  1.13s/it]Loading train:  58%|█████▊    | 166/285 [04:32<02:03,  1.03s/it]Loading train:  59%|█████▊    | 167/285 [04:33<01:53,  1.04it/s]Loading train:  59%|█████▉    | 168/285 [04:34<01:49,  1.07it/s]Loading train:  59%|█████▉    | 169/285 [04:34<01:43,  1.12it/s]Loading train:  60%|█████▉    | 170/285 [04:35<01:42,  1.12it/s]Loading train:  60%|██████    | 171/285 [04:36<01:47,  1.06it/s]Loading train:  60%|██████    | 172/285 [04:37<01:50,  1.02it/s]Loading train:  61%|██████    | 173/285 [04:38<01:45,  1.06it/s]Loading train:  61%|██████    | 174/285 [04:39<01:41,  1.09it/s]Loading train:  61%|██████▏   | 175/285 [04:40<01:43,  1.06it/s]Loading train:  62%|██████▏   | 176/285 [04:41<01:41,  1.07it/s]Loading train:  62%|██████▏   | 177/285 [04:42<01:42,  1.05it/s]Loading train:  62%|██████▏   | 178/285 [04:43<01:46,  1.00it/s]Loading train:  63%|██████▎   | 179/285 [04:44<01:52,  1.06s/it]Loading train:  63%|██████▎   | 180/285 [04:46<01:54,  1.09s/it]Loading train:  64%|██████▎   | 181/285 [04:46<01:48,  1.05s/it]Loading train:  64%|██████▍   | 182/285 [04:47<01:47,  1.04s/it]Loading train:  64%|██████▍   | 183/285 [04:49<01:51,  1.10s/it]Loading train:  65%|██████▍   | 184/285 [04:50<01:49,  1.08s/it]Loading train:  65%|██████▍   | 185/285 [04:51<01:44,  1.05s/it]Loading train:  65%|██████▌   | 186/285 [04:52<01:45,  1.07s/it]Loading train:  66%|██████▌   | 187/285 [04:53<01:44,  1.06s/it]Loading train:  66%|██████▌   | 188/285 [04:54<01:42,  1.06s/it]Loading train:  66%|██████▋   | 189/285 [04:55<01:34,  1.02it/s]Loading train:  67%|██████▋   | 190/285 [04:56<01:32,  1.03it/s]Loading train:  67%|██████▋   | 191/285 [04:57<01:37,  1.04s/it]Loading train:  67%|██████▋   | 192/285 [04:58<01:32,  1.01it/s]Loading train:  68%|██████▊   | 193/285 [04:59<01:27,  1.05it/s]Loading train:  68%|██████▊   | 194/285 [04:59<01:24,  1.07it/s]Loading train:  68%|██████▊   | 195/285 [05:00<01:24,  1.07it/s]Loading train:  69%|██████▉   | 196/285 [05:02<01:31,  1.03s/it]Loading train:  69%|██████▉   | 197/285 [05:03<01:26,  1.02it/s]Loading train:  69%|██████▉   | 198/285 [05:04<01:26,  1.01it/s]Loading train:  70%|██████▉   | 199/285 [05:05<01:26,  1.00s/it]Loading train:  70%|███████   | 200/285 [05:05<01:18,  1.08it/s]Loading train:  71%|███████   | 201/285 [05:06<01:14,  1.12it/s]Loading train:  71%|███████   | 202/285 [05:07<01:19,  1.04it/s]Loading train:  71%|███████   | 203/285 [05:08<01:21,  1.01it/s]Loading train:  72%|███████▏  | 204/285 [05:09<01:21,  1.01s/it]Loading train:  72%|███████▏  | 205/285 [05:10<01:19,  1.00it/s]Loading train:  72%|███████▏  | 206/285 [05:11<01:20,  1.02s/it]Loading train:  73%|███████▎  | 207/285 [05:13<01:21,  1.04s/it]Loading train:  73%|███████▎  | 208/285 [05:14<01:19,  1.04s/it]Loading train:  73%|███████▎  | 209/285 [05:15<01:17,  1.02s/it]Loading train:  74%|███████▎  | 210/285 [05:15<01:14,  1.01it/s]Loading train:  74%|███████▍  | 211/285 [05:17<01:15,  1.03s/it]Loading train:  74%|███████▍  | 212/285 [05:18<01:21,  1.12s/it]Loading train:  75%|███████▍  | 213/285 [05:19<01:23,  1.16s/it]Loading train:  75%|███████▌  | 214/285 [05:20<01:24,  1.18s/it]Loading train:  75%|███████▌  | 215/285 [05:21<01:18,  1.13s/it]Loading train:  76%|███████▌  | 216/285 [05:22<01:13,  1.07s/it]Loading train:  76%|███████▌  | 217/285 [05:23<01:09,  1.02s/it]Loading train:  76%|███████▋  | 218/285 [05:24<01:08,  1.02s/it]Loading train:  77%|███████▋  | 219/285 [05:25<01:10,  1.06s/it]Loading train:  77%|███████▋  | 220/285 [05:27<01:09,  1.08s/it]Loading train:  78%|███████▊  | 221/285 [05:28<01:08,  1.07s/it]Loading train:  78%|███████▊  | 222/285 [05:28<01:04,  1.02s/it]Loading train:  78%|███████▊  | 223/285 [05:29<01:00,  1.02it/s]Loading train:  79%|███████▊  | 224/285 [05:30<01:00,  1.01it/s]Loading train:  79%|███████▉  | 225/285 [05:31<00:58,  1.02it/s]Loading train:  79%|███████▉  | 226/285 [05:32<01:00,  1.02s/it]Loading train:  80%|███████▉  | 227/285 [05:34<00:59,  1.03s/it]Loading train:  80%|████████  | 228/285 [05:35<00:58,  1.03s/it]Loading train:  80%|████████  | 229/285 [05:36<00:58,  1.04s/it]Loading train:  81%|████████  | 230/285 [05:37<00:57,  1.05s/it]Loading train:  81%|████████  | 231/285 [05:38<00:59,  1.11s/it]Loading train:  81%|████████▏ | 232/285 [05:39<01:03,  1.20s/it]Loading train:  82%|████████▏ | 233/285 [05:40<01:01,  1.19s/it]Loading train:  82%|████████▏ | 234/285 [05:42<01:06,  1.30s/it]Loading train:  82%|████████▏ | 235/285 [05:43<01:05,  1.30s/it]Loading train:  83%|████████▎ | 236/285 [05:45<01:04,  1.32s/it]Loading train:  83%|████████▎ | 237/285 [05:46<01:04,  1.35s/it]Loading train:  84%|████████▎ | 238/285 [05:47<01:01,  1.32s/it]Loading train:  84%|████████▍ | 239/285 [05:49<00:59,  1.29s/it]Loading train:  84%|████████▍ | 240/285 [05:50<00:55,  1.24s/it]Loading train:  85%|████████▍ | 241/285 [05:51<00:56,  1.29s/it]Loading train:  85%|████████▍ | 242/285 [05:52<00:52,  1.23s/it]Loading train:  85%|████████▌ | 243/285 [05:53<00:50,  1.21s/it]Loading train:  86%|████████▌ | 244/285 [05:55<00:51,  1.25s/it]Loading train:  86%|████████▌ | 245/285 [05:56<00:48,  1.22s/it]Loading train:  86%|████████▋ | 246/285 [05:57<00:48,  1.25s/it]Loading train:  87%|████████▋ | 247/285 [05:59<00:48,  1.27s/it]Loading train:  87%|████████▋ | 248/285 [06:00<00:47,  1.28s/it]Loading train:  87%|████████▋ | 249/285 [06:02<00:50,  1.40s/it]Loading train:  88%|████████▊ | 250/285 [06:02<00:44,  1.28s/it]Loading train:  88%|████████▊ | 251/285 [06:04<00:41,  1.22s/it]Loading train:  88%|████████▊ | 252/285 [06:05<00:40,  1.22s/it]Loading train:  89%|████████▉ | 253/285 [06:06<00:37,  1.16s/it]Loading train:  89%|████████▉ | 254/285 [06:07<00:34,  1.11s/it]Loading train:  89%|████████▉ | 255/285 [06:08<00:31,  1.05s/it]Loading train:  90%|████████▉ | 256/285 [06:09<00:29,  1.00s/it]Loading train:  90%|█████████ | 257/285 [06:10<00:27,  1.03it/s]Loading train:  91%|█████████ | 258/285 [06:11<00:26,  1.01it/s]Loading train:  91%|█████████ | 259/285 [06:11<00:24,  1.05it/s]Loading train:  91%|█████████ | 260/285 [06:13<00:25,  1.00s/it]Loading train:  92%|█████████▏| 261/285 [06:13<00:23,  1.04it/s]Loading train:  92%|█████████▏| 262/285 [06:14<00:22,  1.04it/s]Loading train:  92%|█████████▏| 263/285 [06:15<00:21,  1.01it/s]Loading train:  93%|█████████▎| 264/285 [06:16<00:21,  1.01s/it]Loading train:  93%|█████████▎| 265/285 [06:17<00:18,  1.06it/s]Loading train:  93%|█████████▎| 266/285 [06:18<00:17,  1.09it/s]Loading train:  94%|█████████▎| 267/285 [06:19<00:16,  1.08it/s]Loading train:  94%|█████████▍| 268/285 [06:20<00:16,  1.00it/s]Loading train:  94%|█████████▍| 269/285 [06:21<00:16,  1.01s/it]Loading train:  95%|█████████▍| 270/285 [06:22<00:15,  1.05s/it]Loading train:  95%|█████████▌| 271/285 [06:24<00:15,  1.11s/it]Loading train:  95%|█████████▌| 272/285 [06:25<00:14,  1.15s/it]Loading train:  96%|█████████▌| 273/285 [06:26<00:13,  1.16s/it]Loading train:  96%|█████████▌| 274/285 [06:27<00:12,  1.13s/it]Loading train:  96%|█████████▋| 275/285 [06:28<00:11,  1.11s/it]Loading train:  97%|█████████▋| 276/285 [06:29<00:10,  1.12s/it]Loading train:  97%|█████████▋| 277/285 [06:30<00:08,  1.07s/it]Loading train:  98%|█████████▊| 278/285 [06:31<00:07,  1.10s/it]Loading train:  98%|█████████▊| 279/285 [06:32<00:06,  1.07s/it]Loading train:  98%|█████████▊| 280/285 [06:34<00:05,  1.07s/it]Loading train:  99%|█████████▊| 281/285 [06:35<00:04,  1.13s/it]Loading train:  99%|█████████▉| 282/285 [06:36<00:03,  1.20s/it]Loading train:  99%|█████████▉| 283/285 [06:37<00:02,  1.18s/it]Loading train: 100%|█████████▉| 284/285 [06:39<00:01,  1.20s/it]Loading train: 100%|██████████| 285/285 [06:40<00:00,  1.18s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/285 [00:00<00:06, 45.07it/s]concatenating: train:   3%|▎         | 9/285 [00:00<00:07, 39.32it/s]concatenating: train:  10%|▉         | 28/285 [00:00<00:04, 51.56it/s]concatenating: train:  16%|█▋        | 47/285 [00:00<00:03, 65.74it/s]concatenating: train:  20%|██        | 58/285 [00:00<00:03, 66.62it/s]concatenating: train:  24%|██▍       | 69/285 [00:00<00:02, 75.54it/s]concatenating: train:  28%|██▊       | 81/285 [00:00<00:02, 83.30it/s]concatenating: train:  33%|███▎      | 93/285 [00:00<00:02, 90.39it/s]concatenating: train:  42%|████▏     | 121/285 [00:01<00:01, 113.26it/s]concatenating: train:  49%|████▉     | 141/285 [00:01<00:01, 129.83it/s]concatenating: train:  55%|█████▌    | 158/285 [00:01<00:01, 122.46it/s]concatenating: train:  61%|██████    | 174/285 [00:01<00:00, 125.54it/s]concatenating: train:  66%|██████▋   | 189/285 [00:01<00:00, 127.96it/s]concatenating: train:  72%|███████▏  | 204/285 [00:01<00:00, 128.34it/s]concatenating: train:  79%|███████▉  | 225/285 [00:01<00:00, 143.04it/s]concatenating: train:  85%|████████▍ | 241/285 [00:01<00:00, 137.09it/s]concatenating: train:  90%|████████▉ | 256/285 [00:01<00:00, 134.53it/s]concatenating: train:  95%|█████████▌| 271/285 [00:02<00:00, 100.84it/s]concatenating: train:  99%|█████████▉| 283/285 [00:02<00:00, 96.64it/s] concatenating: train: 100%|██████████| 285/285 [00:02<00:00, 120.00it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.64s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.54s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 67.35it/s]
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:10,  3.86it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:08,  4.62it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.35it/s]loading the weights for Unet:  20%|██        | 8/40 [00:02<00:08,  3.92it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:02<00:14,  2.11it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:03<00:12,  2.35it/s]loading the weights for Unet:  30%|███       | 12/40 [00:03<00:10,  2.70it/s]loading the weights for Unet:  40%|████      | 16/40 [00:04<00:06,  3.60it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:04<00:06,  3.59it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:04<00:04,  4.31it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:04<00:04,  4.16it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:05<00:03,  5.00it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:06<00:04,  3.08it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:07<00:07,  1.92it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:08<00:05,  2.17it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:08<00:04,  2.46it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:08<00:02,  3.23it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:08<00:01,  3.93it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:09<00:01,  3.90it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:09<00:00,  4.66it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:09<00:00,  4.36it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:09<00:00,  4.16it/s]
(0/4) test vimp2_ctrl_991_08302013_JF
(1/4) test vimp2_967_08132013_KW
(2/4) test vimp2_765_04162013_AW
(3/4) test vimp2_ANON695_03132013
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 80, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 80, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 80, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 80, 10)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 80, 10)   910         dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 80, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 80, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 80, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 80, 10)   910         dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 80, 10)   40          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 80, 10)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 80, 10)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 80, 20)   1820        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 80, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 80, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 80, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 80, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 80, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 40, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 40, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 40, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 40, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 40, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 40, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 40, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 20, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 20, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 20, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 20, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 20, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 20, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 20, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 20, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 40, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 40, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 40, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 40, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 40, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 40, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 10)   1810        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 10)   40          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 10)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 10)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 13)   143         dropout_9[0][0]                  
==================================================================================================
Total params: 189,493
Trainable params: 45,933
Non-trainable params: 143,560
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 24s - loss: 209.6170 - acc: 0.4094 - mDice: 0.0184 - val_loss: 152.6250 - val_acc: 0.8714 - val_mDice: 0.0168

Epoch 00001: val_mDice improved from -inf to 0.01682, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 10s - loss: 127.9470 - acc: 0.7341 - mDice: 0.0179 - val_loss: 96.5018 - val_acc: 0.9041 - val_mDice: 0.0172

Epoch 00002: val_mDice improved from 0.01682 to 0.01720, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 10s - loss: 76.1967 - acc: 0.8366 - mDice: 0.0177 - val_loss: 49.6091 - val_acc: 0.9047 - val_mDice: 0.0148

Epoch 00003: val_mDice did not improve from 0.01720
Epoch 4/300
 - 11s - loss: 47.4045 - acc: 0.8657 - mDice: 0.0176 - val_loss: 19.1519 - val_acc: 0.9047 - val_mDice: 0.0130

Epoch 00004: val_mDice did not improve from 0.01720
Epoch 5/300
 - 11s - loss: 31.7768 - acc: 0.8682 - mDice: 0.0179 - val_loss: 12.9675 - val_acc: 0.9047 - val_mDice: 0.0125

Epoch 00005: val_mDice did not improve from 0.01720
Epoch 6/300
 - 10s - loss: 23.0755 - acc: 0.8688 - mDice: 0.0182 - val_loss: 10.6634 - val_acc: 0.9047 - val_mDice: 0.0141

Epoch 00006: val_mDice did not improve from 0.01720
Epoch 7/300
 - 10s - loss: 18.1506 - acc: 0.8692 - mDice: 0.0188 - val_loss: 9.6083 - val_acc: 0.9047 - val_mDice: 0.0144

Epoch 00007: val_mDice did not improve from 0.01720
Epoch 8/300
 - 10s - loss: 14.8935 - acc: 0.8692 - mDice: 0.0199 - val_loss: 8.2508 - val_acc: 0.9047 - val_mDice: 0.0150

Epoch 00008: val_mDice did not improve from 0.01720
Epoch 9/300
 - 10s - loss: 12.7464 - acc: 0.8692 - mDice: 0.0213 - val_loss: 7.5634 - val_acc: 0.9047 - val_mDice: 0.0114

Epoch 00009: val_mDice did not improve from 0.01720
Epoch 10/300
 - 10s - loss: 11.2914 - acc: 0.8692 - mDice: 0.0231 - val_loss: 7.0876 - val_acc: 0.9047 - val_mDice: 0.0126

Epoch 00010: val_mDice did not improve from 0.01720
Epoch 11/300
 - 10s - loss: 10.2422 - acc: 0.8692 - mDice: 0.0251 - val_loss: 6.7001 - val_acc: 0.9047 - val_mDice: 0.0130

Epoch 00011: val_mDice did not improve from 0.01720
Epoch 12/300
 - 10s - loss: 9.4232 - acc: 0.8692 - mDice: 0.0276 - val_loss: 6.2017 - val_acc: 0.9047 - val_mDice: 0.0174

Epoch 00012: val_mDice improved from 0.01720 to 0.01744, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 10s - loss: 8.7908 - acc: 0.8692 - mDice: 0.0306 - val_loss: 5.8688 - val_acc: 0.9047 - val_mDice: 0.0229

Epoch 00013: val_mDice improved from 0.01744 to 0.02293, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 11s - loss: 8.2952 - acc: 0.8692 - mDice: 0.0337 - val_loss: 5.5530 - val_acc: 0.9047 - val_mDice: 0.0310

Epoch 00014: val_mDice improved from 0.02293 to 0.03102, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 10s - loss: 7.8807 - acc: 0.8692 - mDice: 0.0370 - val_loss: 5.7398 - val_acc: 0.9047 - val_mDice: 0.0293

Epoch 00015: val_mDice did not improve from 0.03102
Epoch 16/300
 - 11s - loss: 7.4086 - acc: 0.8692 - mDice: 0.0407 - val_loss: 5.5074 - val_acc: 0.9047 - val_mDice: 0.0354

Epoch 00016: val_mDice improved from 0.03102 to 0.03542, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 11s - loss: 7.0528 - acc: 0.8692 - mDice: 0.0444 - val_loss: 5.6463 - val_acc: 0.9047 - val_mDice: 0.0368

Epoch 00017: val_mDice improved from 0.03542 to 0.03681, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 11s - loss: 6.7777 - acc: 0.8692 - mDice: 0.0486 - val_loss: 5.7078 - val_acc: 0.9047 - val_mDice: 0.0369

Epoch 00018: val_mDice improved from 0.03681 to 0.03689, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 12s - loss: 6.5567 - acc: 0.8692 - mDice: 0.0522 - val_loss: 5.0485 - val_acc: 0.9047 - val_mDice: 0.0491

Epoch 00019: val_mDice improved from 0.03689 to 0.04908, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 11s - loss: 6.3581 - acc: 0.8692 - mDice: 0.0559 - val_loss: 5.0018 - val_acc: 0.9047 - val_mDice: 0.0566

Epoch 00020: val_mDice improved from 0.04908 to 0.05659, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 11s - loss: 6.1905 - acc: 0.8692 - mDice: 0.0593 - val_loss: 5.2094 - val_acc: 0.9047 - val_mDice: 0.0558

Epoch 00021: val_mDice did not improve from 0.05659
Epoch 22/300
 - 11s - loss: 6.0323 - acc: 0.8692 - mDice: 0.0629 - val_loss: 4.7761 - val_acc: 0.9047 - val_mDice: 0.0694

Epoch 00022: val_mDice improved from 0.05659 to 0.06941, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 11s - loss: 5.8912 - acc: 0.8692 - mDice: 0.0666 - val_loss: 4.6836 - val_acc: 0.9047 - val_mDice: 0.0774

Epoch 00023: val_mDice improved from 0.06941 to 0.07744, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 11s - loss: 5.7741 - acc: 0.8692 - mDice: 0.0699 - val_loss: 5.6061 - val_acc: 0.9047 - val_mDice: 0.0561

Epoch 00024: val_mDice did not improve from 0.07744
Epoch 25/300
 - 10s - loss: 5.6385 - acc: 0.8692 - mDice: 0.0741 - val_loss: 4.6815 - val_acc: 0.9047 - val_mDice: 0.0812

Epoch 00025: val_mDice improved from 0.07744 to 0.08118, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 10s - loss: 5.5210 - acc: 0.8692 - mDice: 0.0781 - val_loss: 4.7376 - val_acc: 0.9047 - val_mDice: 0.0856

Epoch 00026: val_mDice improved from 0.08118 to 0.08562, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 10s - loss: 5.4218 - acc: 0.8693 - mDice: 0.0823 - val_loss: 4.8504 - val_acc: 0.9047 - val_mDice: 0.0901

Epoch 00027: val_mDice improved from 0.08562 to 0.09010, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 10s - loss: 5.3022 - acc: 0.8694 - mDice: 0.0877 - val_loss: 3.9969 - val_acc: 0.9047 - val_mDice: 0.1141

Epoch 00028: val_mDice improved from 0.09010 to 0.11412, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 10s - loss: 5.2022 - acc: 0.8697 - mDice: 0.0937 - val_loss: 4.9837 - val_acc: 0.9048 - val_mDice: 0.0966

Epoch 00029: val_mDice did not improve from 0.11412
Epoch 30/300
 - 10s - loss: 5.1019 - acc: 0.8700 - mDice: 0.0997 - val_loss: 4.0405 - val_acc: 0.9049 - val_mDice: 0.1333

Epoch 00030: val_mDice improved from 0.11412 to 0.13332, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 10s - loss: 5.0065 - acc: 0.8703 - mDice: 0.1051 - val_loss: 4.1722 - val_acc: 0.9049 - val_mDice: 0.1322

Epoch 00031: val_mDice did not improve from 0.13332
Epoch 32/300
 - 10s - loss: 4.9112 - acc: 0.8705 - mDice: 0.1101 - val_loss: 4.1501 - val_acc: 0.9054 - val_mDice: 0.1368

Epoch 00032: val_mDice improved from 0.13332 to 0.13680, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 11s - loss: 4.8333 - acc: 0.8709 - mDice: 0.1150 - val_loss: 3.8301 - val_acc: 0.9052 - val_mDice: 0.1539

Epoch 00033: val_mDice improved from 0.13680 to 0.15386, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 10s - loss: 4.7534 - acc: 0.8714 - mDice: 0.1201 - val_loss: 3.8676 - val_acc: 0.9057 - val_mDice: 0.1575

Epoch 00034: val_mDice improved from 0.15386 to 0.15755, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 10s - loss: 4.6768 - acc: 0.8717 - mDice: 0.1253 - val_loss: 5.2277 - val_acc: 0.9053 - val_mDice: 0.1134

Epoch 00035: val_mDice did not improve from 0.15755
Epoch 36/300
 - 10s - loss: 4.6074 - acc: 0.8720 - mDice: 0.1310 - val_loss: 4.1002 - val_acc: 0.9058 - val_mDice: 0.1577

Epoch 00036: val_mDice improved from 0.15755 to 0.15771, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 10s - loss: 4.5326 - acc: 0.8726 - mDice: 0.1372 - val_loss: 3.8523 - val_acc: 0.9065 - val_mDice: 0.1721

Epoch 00037: val_mDice improved from 0.15771 to 0.17211, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 10s - loss: 4.4741 - acc: 0.8730 - mDice: 0.1417 - val_loss: 3.6693 - val_acc: 0.9063 - val_mDice: 0.1827

Epoch 00038: val_mDice improved from 0.17211 to 0.18266, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 10s - loss: 4.3548 - acc: 0.8735 - mDice: 0.1487 - val_loss: 4.0277 - val_acc: 0.9094 - val_mDice: 0.1747

Epoch 00039: val_mDice did not improve from 0.18266
Epoch 40/300
 - 10s - loss: 4.2810 - acc: 0.8746 - mDice: 0.1550 - val_loss: 3.9701 - val_acc: 0.9086 - val_mDice: 0.1831

Epoch 00040: val_mDice improved from 0.18266 to 0.18305, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 10s - loss: 4.2297 - acc: 0.8753 - mDice: 0.1602 - val_loss: 3.9745 - val_acc: 0.9100 - val_mDice: 0.1898

Epoch 00041: val_mDice improved from 0.18305 to 0.18978, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 10s - loss: 4.1668 - acc: 0.8764 - mDice: 0.1668 - val_loss: 3.6579 - val_acc: 0.9155 - val_mDice: 0.2063

Epoch 00042: val_mDice improved from 0.18978 to 0.20630, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 11s - loss: 4.1170 - acc: 0.8777 - mDice: 0.1728 - val_loss: 3.6808 - val_acc: 0.9157 - val_mDice: 0.2068

Epoch 00043: val_mDice improved from 0.20630 to 0.20685, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 44/300
 - 11s - loss: 4.0643 - acc: 0.8789 - mDice: 0.1789 - val_loss: 3.9878 - val_acc: 0.9175 - val_mDice: 0.2026

Epoch 00044: val_mDice did not improve from 0.20685
Epoch 45/300
 - 11s - loss: 4.0056 - acc: 0.8804 - mDice: 0.1865 - val_loss: 3.4738 - val_acc: 0.9177 - val_mDice: 0.2275

Epoch 00045: val_mDice improved from 0.20685 to 0.22753, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 11s - loss: 3.9592 - acc: 0.8813 - mDice: 0.1928 - val_loss: 3.6695 - val_acc: 0.9169 - val_mDice: 0.2238

Epoch 00046: val_mDice did not improve from 0.22753
Epoch 47/300
 - 11s - loss: 3.9277 - acc: 0.8822 - mDice: 0.1970 - val_loss: 3.4724 - val_acc: 0.9191 - val_mDice: 0.2382

Epoch 00047: val_mDice improved from 0.22753 to 0.23817, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 11s - loss: 3.8751 - acc: 0.8829 - mDice: 0.2026 - val_loss: 4.3223 - val_acc: 0.9206 - val_mDice: 0.2073

Epoch 00048: val_mDice did not improve from 0.23817
Epoch 49/300
 - 11s - loss: 3.8079 - acc: 0.8841 - mDice: 0.2085 - val_loss: 3.8944 - val_acc: 0.9209 - val_mDice: 0.2234

Epoch 00049: val_mDice did not improve from 0.23817
Epoch 50/300
 - 11s - loss: 3.7644 - acc: 0.8848 - mDice: 0.2136 - val_loss: 3.5101 - val_acc: 0.9215 - val_mDice: 0.2470

Epoch 00050: val_mDice improved from 0.23817 to 0.24697, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 11s - loss: 3.7228 - acc: 0.8856 - mDice: 0.2179 - val_loss: 3.4764 - val_acc: 0.9210 - val_mDice: 0.2572

Epoch 00051: val_mDice improved from 0.24697 to 0.25717, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 12s - loss: 3.6860 - acc: 0.8862 - mDice: 0.2216 - val_loss: 4.4339 - val_acc: 0.9190 - val_mDice: 0.2145

Epoch 00052: val_mDice did not improve from 0.25717
Epoch 53/300
 - 11s - loss: 3.6536 - acc: 0.8860 - mDice: 0.2247 - val_loss: 3.6390 - val_acc: 0.9191 - val_mDice: 0.2536

Epoch 00053: val_mDice did not improve from 0.25717
Epoch 54/300
 - 11s - loss: 3.6203 - acc: 0.8858 - mDice: 0.2281 - val_loss: 4.0893 - val_acc: 0.9189 - val_mDice: 0.2413

Epoch 00054: val_mDice did not improve from 0.25717
Epoch 55/300
 - 11s - loss: 3.5728 - acc: 0.8863 - mDice: 0.2330 - val_loss: 4.2331 - val_acc: 0.9188 - val_mDice: 0.2343

Epoch 00055: val_mDice did not improve from 0.25717
Epoch 56/300
 - 11s - loss: 3.5392 - acc: 0.8868 - mDice: 0.2370 - val_loss: 3.5291 - val_acc: 0.9181 - val_mDice: 0.2643

Epoch 00056: val_mDice improved from 0.25717 to 0.26426, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 57/300
 - 11s - loss: 3.5109 - acc: 0.8877 - mDice: 0.2398 - val_loss: 4.2599 - val_acc: 0.9175 - val_mDice: 0.2343

Epoch 00057: val_mDice did not improve from 0.26426
Epoch 58/300
 - 11s - loss: 3.4646 - acc: 0.8884 - mDice: 0.2445 - val_loss: 3.6335 - val_acc: 0.9191 - val_mDice: 0.2720

Epoch 00058: val_mDice improved from 0.26426 to 0.27199, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 11s - loss: 3.4211 - acc: 0.8887 - mDice: 0.2491 - val_loss: 4.0376 - val_acc: 0.9182 - val_mDice: 0.2589

Epoch 00059: val_mDice did not improve from 0.27199
Epoch 60/300
 - 11s - loss: 3.3825 - acc: 0.8890 - mDice: 0.2532 - val_loss: 3.4969 - val_acc: 0.9180 - val_mDice: 0.2810

Epoch 00060: val_mDice improved from 0.27199 to 0.28099, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 61/300
 - 11s - loss: 3.3510 - acc: 0.8894 - mDice: 0.2572 - val_loss: 3.8392 - val_acc: 0.9210 - val_mDice: 0.2797

Epoch 00061: val_mDice did not improve from 0.28099
Epoch 62/300
 - 11s - loss: 3.3234 - acc: 0.8900 - mDice: 0.2604 - val_loss: 3.8626 - val_acc: 0.9189 - val_mDice: 0.2709

Epoch 00062: val_mDice did not improve from 0.28099
Epoch 63/300
 - 11s - loss: 3.2954 - acc: 0.8905 - mDice: 0.2643 - val_loss: 3.2000 - val_acc: 0.9168 - val_mDice: 0.2988

Epoch 00063: val_mDice improved from 0.28099 to 0.29879, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 11s - loss: 3.2625 - acc: 0.8912 - mDice: 0.2682 - val_loss: 3.5602 - val_acc: 0.9212 - val_mDice: 0.2843

Epoch 00064: val_mDice did not improve from 0.29879
Epoch 65/300
 - 11s - loss: 3.2335 - acc: 0.8918 - mDice: 0.2725 - val_loss: 4.0287 - val_acc: 0.9189 - val_mDice: 0.2683

Epoch 00065: val_mDice did not improve from 0.29879
Epoch 66/300
 - 10s - loss: 3.2030 - acc: 0.8922 - mDice: 0.2755 - val_loss: 3.7747 - val_acc: 0.9208 - val_mDice: 0.2869

Epoch 00066: val_mDice did not improve from 0.29879
Epoch 67/300
 - 11s - loss: 3.1768 - acc: 0.8924 - mDice: 0.2784 - val_loss: 3.6752 - val_acc: 0.9228 - val_mDice: 0.2933

Epoch 00067: val_mDice did not improve from 0.29879
Epoch 68/300
 - 10s - loss: 3.1547 - acc: 0.8927 - mDice: 0.2810 - val_loss: 3.7444 - val_acc: 0.9212 - val_mDice: 0.2830

Epoch 00068: val_mDice did not improve from 0.29879
Epoch 69/300
 - 11s - loss: 3.1288 - acc: 0.8928 - mDice: 0.2843 - val_loss: 4.1980 - val_acc: 0.9234 - val_mDice: 0.2789

Epoch 00069: val_mDice did not improve from 0.29879
Epoch 70/300
 - 11s - loss: 3.1084 - acc: 0.8929 - mDice: 0.2876 - val_loss: 3.7511 - val_acc: 0.9223 - val_mDice: 0.2950

Epoch 00070: val_mDice did not improve from 0.29879
Epoch 71/300
 - 11s - loss: 3.0862 - acc: 0.8930 - mDice: 0.2906 - val_loss: 4.3597 - val_acc: 0.9206 - val_mDice: 0.2634

Epoch 00071: val_mDice did not improve from 0.29879
Epoch 72/300
 - 11s - loss: 3.0598 - acc: 0.8932 - mDice: 0.2934 - val_loss: 3.4253 - val_acc: 0.9225 - val_mDice: 0.3135

Epoch 00072: val_mDice improved from 0.29879 to 0.31347, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 73/300
 - 11s - loss: 3.0391 - acc: 0.8932 - mDice: 0.2960 - val_loss: 3.9564 - val_acc: 0.9206 - val_mDice: 0.2924

Epoch 00073: val_mDice did not improve from 0.31347
Epoch 74/300
 - 11s - loss: 3.0146 - acc: 0.8937 - mDice: 0.2995 - val_loss: 3.6802 - val_acc: 0.9236 - val_mDice: 0.3130

Epoch 00074: val_mDice did not improve from 0.31347
Epoch 75/300
 - 11s - loss: 2.9938 - acc: 0.8942 - mDice: 0.3023 - val_loss: 3.5555 - val_acc: 0.9227 - val_mDice: 0.3151

Epoch 00075: val_mDice improved from 0.31347 to 0.31514, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 76/300
 - 11s - loss: 2.9804 - acc: 0.8945 - mDice: 0.3038 - val_loss: 3.5155 - val_acc: 0.9257 - val_mDice: 0.3209

Epoch 00076: val_mDice improved from 0.31514 to 0.32094, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 77/300
 - 11s - loss: 2.9629 - acc: 0.8948 - mDice: 0.3062 - val_loss: 3.6573 - val_acc: 0.9242 - val_mDice: 0.3144

Epoch 00077: val_mDice did not improve from 0.32094
Epoch 78/300
 - 11s - loss: 2.9503 - acc: 0.8953 - mDice: 0.3088 - val_loss: 4.1007 - val_acc: 0.9236 - val_mDice: 0.2968

Epoch 00078: val_mDice did not improve from 0.32094
Epoch 79/300
 - 10s - loss: 2.9288 - acc: 0.8957 - mDice: 0.3111 - val_loss: 3.5043 - val_acc: 0.9223 - val_mDice: 0.3199

Epoch 00079: val_mDice did not improve from 0.32094
Epoch 80/300
 - 12s - loss: 2.9121 - acc: 0.8963 - mDice: 0.3133 - val_loss: 3.5871 - val_acc: 0.9300 - val_mDice: 0.3225

Epoch 00080: val_mDice improved from 0.32094 to 0.32250, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 81/300
 - 10s - loss: 2.8985 - acc: 0.8967 - mDice: 0.3155 - val_loss: 3.4983 - val_acc: 0.9247 - val_mDice: 0.3166

Epoch 00081: val_mDice did not improve from 0.32250
Epoch 82/300
 - 11s - loss: 2.8785 - acc: 0.8971 - mDice: 0.3185 - val_loss: 3.7761 - val_acc: 0.9279 - val_mDice: 0.3181

Epoch 00082: val_mDice did not improve from 0.32250
Epoch 83/300
 - 11s - loss: 2.8648 - acc: 0.8977 - mDice: 0.3209 - val_loss: 3.6637 - val_acc: 0.9255 - val_mDice: 0.3183

Epoch 00083: val_mDice did not improve from 0.32250
Epoch 84/300
 - 10s - loss: 2.8518 - acc: 0.8980 - mDice: 0.3226 - val_loss: 3.6478 - val_acc: 0.9273 - val_mDice: 0.3164

Epoch 00084: val_mDice did not improve from 0.32250
Epoch 85/300
 - 11s - loss: 2.8382 - acc: 0.8983 - mDice: 0.3240 - val_loss: 3.7782 - val_acc: 0.9255 - val_mDice: 0.3168

Epoch 00085: val_mDice did not improve from 0.32250
Epoch 86/300
 - 10s - loss: 2.8111 - acc: 0.8990 - mDice: 0.3286 - val_loss: 3.6510 - val_acc: 0.9289 - val_mDice: 0.3298

Epoch 00086: val_mDice improved from 0.32250 to 0.32980, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 87/300
 - 10s - loss: 2.7909 - acc: 0.8998 - mDice: 0.3327 - val_loss: 4.1706 - val_acc: 0.9268 - val_mDice: 0.3050

Epoch 00087: val_mDice did not improve from 0.32980
Epoch 88/300
 - 11s - loss: 2.7881 - acc: 0.8995 - mDice: 0.3325 - val_loss: 3.4186 - val_acc: 0.9300 - val_mDice: 0.3379

Epoch 00088: val_mDice improved from 0.32980 to 0.33795, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 89/300
 - 11s - loss: 2.7666 - acc: 0.9000 - mDice: 0.3365 - val_loss: 3.5764 - val_acc: 0.9276 - val_mDice: 0.3232

Epoch 00089: val_mDice did not improve from 0.33795
Epoch 90/300
 - 10s - loss: 2.7524 - acc: 0.9003 - mDice: 0.3391 - val_loss: 3.4327 - val_acc: 0.9307 - val_mDice: 0.3363

Epoch 00090: val_mDice did not improve from 0.33795
Epoch 91/300
 - 10s - loss: 2.7420 - acc: 0.9003 - mDice: 0.3406 - val_loss: 3.7404 - val_acc: 0.9293 - val_mDice: 0.3211

Epoch 00091: val_mDice did not improve from 0.33795
Epoch 92/300
 - 11s - loss: 2.7216 - acc: 0.9010 - mDice: 0.3448 - val_loss: 3.5767 - val_acc: 0.9294 - val_mDice: 0.3281

Epoch 00092: val_mDice did not improve from 0.33795
Epoch 93/300
 - 10s - loss: 2.7092 - acc: 0.9013 - mDice: 0.3471 - val_loss: 3.7937 - val_acc: 0.9279 - val_mDice: 0.3240

Epoch 00093: val_mDice did not improve from 0.33795
Epoch 94/300
 - 11s - loss: 2.6939 - acc: 0.9019 - mDice: 0.3498 - val_loss: 3.6205 - val_acc: 0.9315 - val_mDice: 0.3353

Epoch 00094: val_mDice did not improve from 0.33795
Epoch 95/300
 - 10s - loss: 2.6861 - acc: 0.9020 - mDice: 0.3501 - val_loss: 3.3264 - val_acc: 0.9321 - val_mDice: 0.3503

Epoch 00095: val_mDice improved from 0.33795 to 0.35028, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 96/300
 - 10s - loss: 2.6731 - acc: 0.9024 - mDice: 0.3538 - val_loss: 3.7875 - val_acc: 0.9314 - val_mDice: 0.3369

Epoch 00096: val_mDice did not improve from 0.35028
Epoch 97/300
 - 11s - loss: 2.6773 - acc: 0.9025 - mDice: 0.3528 - val_loss: 3.6035 - val_acc: 0.9321 - val_mDice: 0.3438

Epoch 00097: val_mDice did not improve from 0.35028
Epoch 98/300
 - 10s - loss: 2.6623 - acc: 0.9027 - mDice: 0.3560 - val_loss: 3.3412 - val_acc: 0.9334 - val_mDice: 0.3557

Epoch 00098: val_mDice improved from 0.35028 to 0.35571, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 99/300
 - 10s - loss: 2.6516 - acc: 0.9033 - mDice: 0.3579 - val_loss: 3.3777 - val_acc: 0.9321 - val_mDice: 0.3500

Epoch 00099: val_mDice did not improve from 0.35571
Epoch 100/300
 - 11s - loss: 2.6395 - acc: 0.9034 - mDice: 0.3595 - val_loss: 3.5033 - val_acc: 0.9328 - val_mDice: 0.3477

Epoch 00100: val_mDice did not improve from 0.35571
Epoch 101/300
 - 10s - loss: 2.6264 - acc: 0.9037 - mDice: 0.3622 - val_loss: 3.5502 - val_acc: 0.9332 - val_mDice: 0.3416

Epoch 00101: val_mDice did not improve from 0.35571
Epoch 102/300
 - 10s - loss: 2.6256 - acc: 0.9037 - mDice: 0.3615 - val_loss: 3.5787 - val_acc: 0.9334 - val_mDice: 0.3433

Epoch 00102: val_mDice did not improve from 0.35571
Epoch 103/300
 - 10s - loss: 2.6089 - acc: 0.9043 - mDice: 0.3649 - val_loss: 3.5983 - val_acc: 0.9323 - val_mDice: 0.3412

Epoch 00103: val_mDice did not improve from 0.35571
Epoch 104/300
 - 11s - loss: 2.6018 - acc: 0.9043 - mDice: 0.3660 - val_loss: 3.3636 - val_acc: 0.9335 - val_mDice: 0.3526

Epoch 00104: val_mDice did not improve from 0.35571
Epoch 105/300
 - 10s - loss: 2.5988 - acc: 0.9045 - mDice: 0.3665 - val_loss: 3.7441 - val_acc: 0.9342 - val_mDice: 0.3448

Epoch 00105: val_mDice did not improve from 0.35571
Epoch 106/300
 - 11s - loss: 2.5847 - acc: 0.9050 - mDice: 0.3700 - val_loss: 3.6960 - val_acc: 0.9325 - val_mDice: 0.3475

Epoch 00106: val_mDice did not improve from 0.35571
Epoch 107/300
 - 11s - loss: 2.5689 - acc: 0.9055 - mDice: 0.3726 - val_loss: 3.7525 - val_acc: 0.9368 - val_mDice: 0.3488

Epoch 00107: val_mDice did not improve from 0.35571
Epoch 108/300
 - 10s - loss: 2.5656 - acc: 0.9056 - mDice: 0.3729 - val_loss: 3.3828 - val_acc: 0.9338 - val_mDice: 0.3515

Epoch 00108: val_mDice did not improve from 0.35571
Epoch 109/300
 - 11s - loss: 2.5689 - acc: 0.9057 - mDice: 0.3724 - val_loss: 3.9849 - val_acc: 0.9326 - val_mDice: 0.3300

Epoch 00109: val_mDice did not improve from 0.35571
Epoch 110/300
 - 10s - loss: 2.5535 - acc: 0.9061 - mDice: 0.3749 - val_loss: 3.8487 - val_acc: 0.9359 - val_mDice: 0.3418

Epoch 00110: val_mDice did not improve from 0.35571
Epoch 111/300
 - 10s - loss: 2.5442 - acc: 0.9063 - mDice: 0.3768 - val_loss: 3.8568 - val_acc: 0.9326 - val_mDice: 0.3350

Epoch 00111: val_mDice did not improve from 0.35571
Epoch 112/300
 - 10s - loss: 2.5430 - acc: 0.9064 - mDice: 0.3781 - val_loss: 3.2322 - val_acc: 0.9339 - val_mDice: 0.3550

Epoch 00112: val_mDice did not improve from 0.35571
Epoch 113/300
 - 10s - loss: 2.5325 - acc: 0.9065 - mDice: 0.3789 - val_loss: 3.4171 - val_acc: 0.9357 - val_mDice: 0.3646

Epoch 00113: val_mDice improved from 0.35571 to 0.36460, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 114/300
 - 11s - loss: 2.5229 - acc: 0.9070 - mDice: 0.3811 - val_loss: 3.9575 - val_acc: 0.9363 - val_mDice: 0.3381

Epoch 00114: val_mDice did not improve from 0.36460
Epoch 115/300
 - 10s - loss: 2.5145 - acc: 0.9069 - mDice: 0.3827 - val_loss: 3.6384 - val_acc: 0.9364 - val_mDice: 0.3553

Epoch 00115: val_mDice did not improve from 0.36460
Epoch 116/300
 - 10s - loss: 2.5091 - acc: 0.9072 - mDice: 0.3842 - val_loss: 3.5010 - val_acc: 0.9357 - val_mDice: 0.3549

Epoch 00116: val_mDice did not improve from 0.36460
Epoch 117/300
 - 11s - loss: 2.4956 - acc: 0.9074 - mDice: 0.3863 - val_loss: 3.3579 - val_acc: 0.9369 - val_mDice: 0.3706

Epoch 00117: val_mDice improved from 0.36460 to 0.37059, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 118/300
 - 10s - loss: 2.4913 - acc: 0.9077 - mDice: 0.3876 - val_loss: 3.2408 - val_acc: 0.9342 - val_mDice: 0.3680

Epoch 00118: val_mDice did not improve from 0.37059
Epoch 119/300
 - 11s - loss: 2.4919 - acc: 0.9078 - mDice: 0.3878 - val_loss: 3.8062 - val_acc: 0.9375 - val_mDice: 0.3543

Epoch 00119: val_mDice did not improve from 0.37059
Epoch 120/300
 - 10s - loss: 2.4836 - acc: 0.9078 - mDice: 0.3888 - val_loss: 3.3017 - val_acc: 0.9376 - val_mDice: 0.3679

Epoch 00120: val_mDice did not improve from 0.37059
Epoch 121/300
 - 10s - loss: 2.4854 - acc: 0.9079 - mDice: 0.3886 - val_loss: 4.1253 - val_acc: 0.9333 - val_mDice: 0.3311

Epoch 00121: val_mDice did not improve from 0.37059
Epoch 122/300
 - 11s - loss: 2.4776 - acc: 0.9079 - mDice: 0.3904 - val_loss: 3.7877 - val_acc: 0.9370 - val_mDice: 0.3464

Epoch 00122: val_mDice did not improve from 0.37059
Epoch 123/300
 - 10s - loss: 2.4672 - acc: 0.9085 - mDice: 0.3929 - val_loss: 3.5372 - val_acc: 0.9358 - val_mDice: 0.3581

Epoch 00123: val_mDice did not improve from 0.37059
Epoch 124/300
 - 11s - loss: 2.4638 - acc: 0.9081 - mDice: 0.3924 - val_loss: 3.5363 - val_acc: 0.9383 - val_mDice: 0.3655

Epoch 00124: val_mDice did not improve from 0.37059
Epoch 125/300
 - 10s - loss: 2.4527 - acc: 0.9085 - mDice: 0.3953 - val_loss: 4.1361 - val_acc: 0.9361 - val_mDice: 0.3409

Epoch 00125: val_mDice did not improve from 0.37059
Epoch 126/300
 - 11s - loss: 2.4555 - acc: 0.9085 - mDice: 0.3945 - val_loss: 3.8084 - val_acc: 0.9361 - val_mDice: 0.3497

Epoch 00126: val_mDice did not improve from 0.37059
Epoch 127/300
 - 10s - loss: 2.4457 - acc: 0.9086 - mDice: 0.3964 - val_loss: 3.3218 - val_acc: 0.9373 - val_mDice: 0.3626

Epoch 00127: val_mDice did not improve from 0.37059
Epoch 128/300
 - 10s - loss: 2.4485 - acc: 0.9088 - mDice: 0.3960 - val_loss: 4.1371 - val_acc: 0.9365 - val_mDice: 0.3412

Epoch 00128: val_mDice did not improve from 0.37059
Epoch 129/300
 - 11s - loss: 2.4318 - acc: 0.9091 - mDice: 0.3989 - val_loss: 3.5705 - val_acc: 0.9392 - val_mDice: 0.3671

Epoch 00129: val_mDice did not improve from 0.37059
Epoch 130/300
 - 10s - loss: 2.4352 - acc: 0.9092 - mDice: 0.3986 - val_loss: 3.5596 - val_acc: 0.9373 - val_mDice: 0.3655

Epoch 00130: val_mDice did not improve from 0.37059
Epoch 131/300
 - 11s - loss: 2.4274 - acc: 0.9092 - mDice: 0.3996 - val_loss: 3.7301 - val_acc: 0.9352 - val_mDice: 0.3481

Epoch 00131: val_mDice did not improve from 0.37059
Epoch 132/300
 - 10s - loss: 2.4357 - acc: 0.9092 - mDice: 0.3992 - val_loss: 3.1356 - val_acc: 0.9375 - val_mDice: 0.3690

Epoch 00132: val_mDice did not improve from 0.37059
Epoch 133/300
 - 10s - loss: 2.4224 - acc: 0.9096 - mDice: 0.4021 - val_loss: 3.4044 - val_acc: 0.9369 - val_mDice: 0.3720

Epoch 00133: val_mDice improved from 0.37059 to 0.37198, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 134/300
 - 10s - loss: 2.4264 - acc: 0.9094 - mDice: 0.4012 - val_loss: 3.3750 - val_acc: 0.9387 - val_mDice: 0.3706

Epoch 00134: val_mDice did not improve from 0.37198
Epoch 135/300
 - 11s - loss: 2.4185 - acc: 0.9095 - mDice: 0.4018 - val_loss: 3.2183 - val_acc: 0.9388 - val_mDice: 0.3790

Epoch 00135: val_mDice improved from 0.37198 to 0.37902, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 136/300
 - 10s - loss: 2.4166 - acc: 0.9097 - mDice: 0.4033 - val_loss: 3.5315 - val_acc: 0.9375 - val_mDice: 0.3687

Epoch 00136: val_mDice did not improve from 0.37902
Epoch 137/300
 - 10s - loss: 2.4073 - acc: 0.9097 - mDice: 0.4048 - val_loss: 3.5347 - val_acc: 0.9390 - val_mDice: 0.3643

Epoch 00137: val_mDice did not improve from 0.37902
Epoch 138/300
 - 11s - loss: 2.4065 - acc: 0.9099 - mDice: 0.4047 - val_loss: 3.4292 - val_acc: 0.9382 - val_mDice: 0.3660

Epoch 00138: val_mDice did not improve from 0.37902
Epoch 139/300
 - 10s - loss: 2.4035 - acc: 0.9098 - mDice: 0.4055 - val_loss: 3.4468 - val_acc: 0.9368 - val_mDice: 0.3631

Epoch 00139: val_mDice did not improve from 0.37902
Epoch 140/300
 - 11s - loss: 2.4083 - acc: 0.9098 - mDice: 0.4045 - val_loss: 3.4075 - val_acc: 0.9378 - val_mDice: 0.3679

Epoch 00140: val_mDice did not improve from 0.37902
Epoch 141/300
 - 11s - loss: 2.3944 - acc: 0.9101 - mDice: 0.4074 - val_loss: 3.1670 - val_acc: 0.9380 - val_mDice: 0.3748

Epoch 00141: val_mDice did not improve from 0.37902
Epoch 142/300
 - 10s - loss: 2.3932 - acc: 0.9101 - mDice: 0.4076 - val_loss: 3.0965 - val_acc: 0.9386 - val_mDice: 0.3840

Epoch 00142: val_mDice improved from 0.37902 to 0.38397, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 143/300
 - 11s - loss: 2.3948 - acc: 0.9101 - mDice: 0.4075 - val_loss: 3.5978 - val_acc: 0.9366 - val_mDice: 0.3617

Epoch 00143: val_mDice did not improve from 0.38397
Epoch 144/300
 - 11s - loss: 2.3880 - acc: 0.9102 - mDice: 0.4083 - val_loss: 3.4208 - val_acc: 0.9381 - val_mDice: 0.3621

Epoch 00144: val_mDice did not improve from 0.38397
Epoch 145/300
 - 10s - loss: 2.3857 - acc: 0.9102 - mDice: 0.4086 - val_loss: 3.6858 - val_acc: 0.9365 - val_mDice: 0.3543

Epoch 00145: val_mDice did not improve from 0.38397
Epoch 146/300
 - 11s - loss: 2.3844 - acc: 0.9103 - mDice: 0.4089 - val_loss: 3.5229 - val_acc: 0.9400 - val_mDice: 0.3769

Epoch 00146: val_mDice did not improve from 0.38397
Epoch 147/300
 - 11s - loss: 2.3760 - acc: 0.9103 - mDice: 0.4111 - val_loss: 3.5728 - val_acc: 0.9371 - val_mDice: 0.3579

Epoch 00147: val_mDice did not improve from 0.38397
Epoch 148/300
 - 10s - loss: 2.3742 - acc: 0.9106 - mDice: 0.4114 - val_loss: 3.3225 - val_acc: 0.9400 - val_mDice: 0.3741

Epoch 00148: val_mDice did not improve from 0.38397
Epoch 149/300
 - 11s - loss: 2.3761 - acc: 0.9106 - mDice: 0.4118 - val_loss: 3.3822 - val_acc: 0.9375 - val_mDice: 0.3673

Epoch 00149: val_mDice did not improve from 0.38397
Epoch 150/300
 - 11s - loss: 2.3693 - acc: 0.9106 - mDice: 0.4122 - val_loss: 3.1628 - val_acc: 0.9378 - val_mDice: 0.3748

Epoch 00150: val_mDice did not improve from 0.38397
Epoch 151/300
 - 10s - loss: 2.3762 - acc: 0.9106 - mDice: 0.4111 - val_loss: 3.6826 - val_acc: 0.9368 - val_mDice: 0.3563

Epoch 00151: val_mDice did not improve from 0.38397
Epoch 152/300
 - 11s - loss: 2.3637 - acc: 0.9109 - mDice: 0.4139 - val_loss: 3.3062 - val_acc: 0.9393 - val_mDice: 0.3796

Epoch 00152: val_mDice did not improve from 0.38397
Epoch 153/300
 - 11s - loss: 2.3622 - acc: 0.9108 - mDice: 0.4138 - val_loss: 3.3593 - val_acc: 0.9376 - val_mDice: 0.3727

Epoch 00153: val_mDice did not improve from 0.38397
Epoch 154/300
 - 10s - loss: 2.3554 - acc: 0.9111 - mDice: 0.4143 - val_loss: 3.7162 - val_acc: 0.9382 - val_mDice: 0.3683

Epoch 00154: val_mDice did not improve from 0.38397
Epoch 155/300
 - 11s - loss: 2.3538 - acc: 0.9111 - mDice: 0.4165 - val_loss: 3.4395 - val_acc: 0.9390 - val_mDice: 0.3694

Epoch 00155: val_mDice did not improve from 0.38397
Epoch 156/300
 - 11s - loss: 2.3554 - acc: 0.9109 - mDice: 0.4149 - val_loss: 3.4756 - val_acc: 0.9373 - val_mDice: 0.3638

Epoch 00156: val_mDice did not improve from 0.38397
Epoch 157/300
 - 10s - loss: 2.3546 - acc: 0.9110 - mDice: 0.4157 - val_loss: 3.7483 - val_acc: 0.9358 - val_mDice: 0.3556

Epoch 00157: val_mDice did not improve from 0.38397
Epoch 158/300
 - 11s - loss: 2.3517 - acc: 0.9111 - mDice: 0.4162 - val_loss: 3.8919 - val_acc: 0.9380 - val_mDice: 0.3591

Epoch 00158: val_mDice did not improve from 0.38397
Epoch 159/300
 - 10s - loss: 2.3435 - acc: 0.9111 - mDice: 0.4168 - val_loss: 3.7355 - val_acc: 0.9370 - val_mDice: 0.3618

Epoch 00159: val_mDice did not improve from 0.38397
Epoch 160/300
 - 10s - loss: 2.3475 - acc: 0.9112 - mDice: 0.4175 - val_loss: 3.6046 - val_acc: 0.9374 - val_mDice: 0.3607

Epoch 00160: val_mDice did not improve from 0.38397
Epoch 161/300
 - 11s - loss: 2.3473 - acc: 0.9112 - mDice: 0.4167 - val_loss: 3.7159 - val_acc: 0.9380 - val_mDice: 0.3600

Epoch 00161: val_mDice did not improve from 0.38397
Epoch 162/300
 - 11s - loss: 2.3403 - acc: 0.9114 - mDice: 0.4185 - val_loss: 3.3900 - val_acc: 0.9382 - val_mDice: 0.3772

Epoch 00162: val_mDice did not improve from 0.38397
Epoch 163/300
 - 10s - loss: 2.3368 - acc: 0.9115 - mDice: 0.4182 - val_loss: 3.4512 - val_acc: 0.9378 - val_mDice: 0.3795

Epoch 00163: val_mDice did not improve from 0.38397
Epoch 164/300
 - 11s - loss: 2.3373 - acc: 0.9116 - mDice: 0.4190 - val_loss: 3.3895 - val_acc: 0.9387 - val_mDice: 0.3831

Epoch 00164: val_mDice did not improve from 0.38397
Epoch 165/300
 - 10s - loss: 2.3304 - acc: 0.9118 - mDice: 0.4200 - val_loss: 3.2867 - val_acc: 0.9402 - val_mDice: 0.3811

Epoch 00165: val_mDice did not improve from 0.38397
Epoch 166/300
 - 10s - loss: 2.3257 - acc: 0.9118 - mDice: 0.4215 - val_loss: 3.3279 - val_acc: 0.9375 - val_mDice: 0.3806

Epoch 00166: val_mDice did not improve from 0.38397
Epoch 167/300
 - 11s - loss: 2.3282 - acc: 0.9115 - mDice: 0.4203 - val_loss: 3.5861 - val_acc: 0.9338 - val_mDice: 0.3577

Epoch 00167: val_mDice did not improve from 0.38397
Epoch 168/300
 - 11s - loss: 2.3227 - acc: 0.9119 - mDice: 0.4217 - val_loss: 3.1905 - val_acc: 0.9385 - val_mDice: 0.3829

Epoch 00168: val_mDice did not improve from 0.38397
Epoch 169/300
 - 10s - loss: 2.3260 - acc: 0.9117 - mDice: 0.4210 - val_loss: 3.8426 - val_acc: 0.9351 - val_mDice: 0.3530

Epoch 00169: val_mDice did not improve from 0.38397
Epoch 170/300
 - 11s - loss: 2.3232 - acc: 0.9118 - mDice: 0.4214 - val_loss: 3.6797 - val_acc: 0.9396 - val_mDice: 0.3735

Epoch 00170: val_mDice did not improve from 0.38397
Epoch 171/300
 - 11s - loss: 2.3235 - acc: 0.9119 - mDice: 0.4221 - val_loss: 3.5857 - val_acc: 0.9378 - val_mDice: 0.3694

Epoch 00171: val_mDice did not improve from 0.38397
Epoch 172/300
 - 10s - loss: 2.3115 - acc: 0.9120 - mDice: 0.4235 - val_loss: 3.3380 - val_acc: 0.9380 - val_mDice: 0.3794

Epoch 00172: val_mDice did not improve from 0.38397
Restoring model weights from the end of the best epoch
Epoch 00172: early stopping
{'val_loss': [152.62503069923036, 96.50183827536446, 49.60914620899019, 19.151949371610367, 12.967497011025747, 10.66341443146978, 9.608314911524454, 8.250791170057797, 7.563417241686866, 7.087607196044354, 6.700092454396543, 6.201665974443867, 5.86880165107903, 5.553027892219169, 5.73984983740818, 5.507366503810599, 5.646331466113527, 5.707810302309337, 5.048500410591562, 5.001767720406254, 5.209445705016454, 4.77611022823978, 4.683622680959248, 5.606080678602059, 4.681531782040284, 4.737599419075108, 4.850397405525048, 3.9969350469875193, 4.983650907741061, 4.040515508635768, 4.172203906102195, 4.150072910601184, 3.830069118046335, 3.867603327369406, 5.227735779691665, 4.100213174885583, 3.852340537389474, 3.6692767515335056, 4.027712458966389, 3.970136784150132, 3.9745315301808573, 3.657909143849143, 3.6808149127200958, 3.987789243681445, 3.4737582332676364, 3.669512788986876, 3.4723750868191323, 4.322311639830115, 3.8943604048164118, 3.510147114373034, 3.4764244521613277, 4.433860238774547, 3.6390481620051323, 4.089334877828757, 4.233107886676278, 3.5290855598148134, 4.259936332192627, 3.6335101975128055, 4.037592392392634, 3.4969277138422643, 3.839158552299653, 3.862553222814486, 3.200015610039589, 3.5601643427452516, 4.028697793305453, 3.7746894180198156, 3.675166137738242, 3.744391948406008, 4.197987478847305, 3.7511083049300527, 4.359679934125216, 3.425279051464583, 3.9563565217180265, 3.68017202850786, 3.5555024347489788, 3.5155375747098807, 3.6573156978092376, 4.100691091535347, 3.5043352334316644, 3.587059732038705, 3.4983105621788475, 3.7760514718524756, 3.663717226568787, 3.647795798633957, 3.778155850135677, 3.6510219670211277, 4.170624916840877, 3.4185965106645155, 3.576421672788759, 3.4326823867547014, 3.740434591902331, 3.5766900709076297, 3.793727088054376, 3.620522398279891, 3.326436483656012, 3.7875465682840774, 3.6034505721403374, 3.3412255487360416, 3.377662977358947, 3.5033288153908435, 3.5501933889685287, 3.578736954967358, 3.5982726048234674, 3.3635950330423103, 3.7441010766086125, 3.6959627569387004, 3.752481623597088, 3.3827560816758444, 3.9849317012177337, 3.848682820131736, 3.8568168610023954, 3.2321732458658516, 3.4171302767381784, 3.957478822652428, 3.638404626997986, 3.5010358342191292, 3.357888494862155, 3.240848353088257, 3.8061782434538363, 3.3017030263186564, 4.12534405798873, 3.787748982442454, 3.5372222124909363, 3.536327517170104, 4.136092493399268, 3.8084189356526448, 3.3218152513727546, 4.137128016889273, 3.570535803967643, 3.5596129549445497, 3.7301282409489867, 3.135597028680855, 3.404416058123821, 3.3749937432418977, 3.21831470847662, 3.5315012730258917, 3.534731020663111, 3.429188179606128, 3.446814821412166, 3.407454803275565, 3.1669683185450377, 3.0964538188252066, 3.5978235422544893, 3.420827859968302, 3.6857946403324604, 3.5228597359022213, 3.572839362502453, 3.3225189851197814, 3.382202249973835, 3.162845029584354, 3.6826210358579243, 3.3061531916305067, 3.359324592032603, 3.716169038388346, 3.4394692211367546, 3.47564963193699, 3.7482654268276834, 3.891912712981658, 3.7354848266446163, 3.6046143012298715, 3.715913926162535, 3.3899505854628624, 3.451168602909006, 3.389547946264169, 3.2867061557869115, 3.32790437032513, 3.5861189157391586, 3.1904925857670605, 3.8426455771328794, 3.6796740169991695, 3.5857004653884186, 3.3380088792404248], 'val_acc': [0.8713759127117339, 0.9040888349215189, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047481758253915, 0.9047504464785258, 0.9048992877914792, 0.9049290333475385, 0.9054441310110546, 0.9051579520815894, 0.9056890777179173, 0.9052678715615046, 0.905817312853677, 0.9064926618621463, 0.9063347038768587, 0.9094391181355431, 0.9086332292783827, 0.9100182907921928, 0.9155242471467882, 0.9156639320509774, 0.9175114489737011, 0.9177243652797881, 0.9169322394189381, 0.9190521836280823, 0.9205792347590128, 0.9208997175807044, 0.9214903769038972, 0.9209706783294678, 0.919038440499987, 0.9190796670459566, 0.9188850607190814, 0.9188232421875, 0.9181455913044158, 0.9174542256764003, 0.9191414713859558, 0.9181753510520572, 0.9180402840886798, 0.9210210470926194, 0.9189400445847284, 0.9167948961257935, 0.9211973207337516, 0.9189262901033673, 0.9208127288591295, 0.9228090672265916, 0.9211538519178119, 0.9234203440802438, 0.9222527345021566, 0.9205952570551917, 0.9224839976855687, 0.9206044106256395, 0.9235943215233939, 0.9226625533331008, 0.9257211798713321, 0.9242170112473624, 0.9235828831082299, 0.9223351847557795, 0.9299977137928918, 0.9246588803472973, 0.9278685961450849, 0.9254555957657951, 0.9272916430518741, 0.925478492464338, 0.9289377303350539, 0.9267902970314026, 0.9299656635239011, 0.9276419281959534, 0.9306639461290269, 0.9293429737999326, 0.9293681383132935, 0.9279349786894662, 0.931469806603023, 0.9320902086439586, 0.9313804705937704, 0.9321451726413909, 0.9334043292772203, 0.932074194862729, 0.9327976249513172, 0.9331616305169605, 0.9334019961811247, 0.9322641974403745, 0.9335050497736249, 0.9342353571028936, 0.9325412114461263, 0.9368040192694891, 0.9338140856652033, 0.9326419347808474, 0.9358539410999843, 0.9326076110204061, 0.9339102676936558, 0.9357257116408575, 0.9362523130008152, 0.9364377487273443, 0.9357211334364754, 0.9369207961218697, 0.9342399324689593, 0.9375366398266384, 0.9375503943079994, 0.9333470832733881, 0.9369963464282808, 0.9357532206035796, 0.938253192674546, 0.9361378068015689, 0.9361057451793126, 0.9372504353523254, 0.936536158834185, 0.939237634340922, 0.9372527230353582, 0.9351922898065477, 0.9375045725277492, 0.9369207790919712, 0.9387133802686419, 0.9387637149719965, 0.9374748127801078, 0.9390201455070859, 0.9381822319257827, 0.9368383827663603, 0.9378044747170948, 0.9380494611603873, 0.938596628961109, 0.9366300503412882, 0.9380700616609483, 0.9364606028511411, 0.9400068776948112, 0.937065030847277, 0.9400411815870375, 0.9375480782418024, 0.9378205339113871, 0.9367788547561282, 0.939281131540026, 0.9375572176206679, 0.9381708048638844, 0.938969768228985, 0.9373328912825811, 0.935769206001645, 0.9380311171213785, 0.9370100583348956, 0.9373878013520014, 0.9379670137450808, 0.9382417656126476, 0.9378090727896917, 0.938736271290552, 0.9401831712041583, 0.93747250522886, 0.9337637083871024, 0.9384889943259103, 0.9350549550283522, 0.9396222801435561, 0.9378411031904674, 0.9380403161048889], 'val_mDice': [0.016823523467229234, 0.01719915330232609, 0.014782156706565902, 0.012978772067331843, 0.012524084448592649, 0.014080498921906664, 0.014442508674359747, 0.015040477161251363, 0.011376520330529837, 0.012605835617120777, 0.013037452341190406, 0.01743632066063583, 0.022934284050106293, 0.03102192991147084, 0.0293492367934613, 0.03542222235617893, 0.036805687860275306, 0.03688841635760452, 0.04907867494260981, 0.05659130235601749, 0.05577035376890784, 0.0694142336779762, 0.07743959683215335, 0.0560958706995561, 0.08118400942268116, 0.0856217494057048, 0.0900966568096053, 0.1141222764161371, 0.0965989852058036, 0.13331502276871884, 0.13221529516435804, 0.13680162546890123, 0.15386081163194917, 0.1575476936995983, 0.11338251252614316, 0.15771368704736233, 0.1721086340202462, 0.18266281059810094, 0.1746713425964117, 0.18305418454110622, 0.1897805534480583, 0.20629592643429837, 0.20684620857770955, 0.2025556182932286, 0.22752773140867552, 0.2238210491126492, 0.23816940893552133, 0.20731554388822543, 0.2234248085213559, 0.24696794147824958, 0.25716752412595917, 0.2144731410912105, 0.2536377140453884, 0.24130142276130973, 0.23430876886206015, 0.2642596931684585, 0.23433054451431548, 0.2719901595264673, 0.25886289464930695, 0.28098676956835245, 0.27971559204161167, 0.2708586861512491, 0.29879217133635566, 0.28425059219201404, 0.2683394297602631, 0.2869336378006708, 0.2932936112795557, 0.2829990504043443, 0.2789299988320896, 0.29500441528147175, 0.2633528173679397, 0.31346612193045165, 0.29240810471986023, 0.31299575684326036, 0.3151371052399987, 0.3209400651532979, 0.3144116613659121, 0.2967895833509309, 0.31990954945130007, 0.32250177212769077, 0.31655505964798586, 0.3180839034418265, 0.3183173680944102, 0.3164381930338485, 0.31677853546681856, 0.3297961153799579, 0.3050363778713204, 0.33794592285440084, 0.32321990627263275, 0.33633983933499884, 0.3211109580560809, 0.3280989278462671, 0.32403591124429587, 0.33531245181248304, 0.3502768755313896, 0.33691823296248913, 0.3438429825362705, 0.35571106691800414, 0.3500301382903542, 0.34770220589070094, 0.3416480358157839, 0.34326655896646635, 0.34121109261399224, 0.3526210981820311, 0.34483414409416063, 0.3474512048775241, 0.3487543304051672, 0.35152540586534, 0.3300490432551929, 0.3418279569596052, 0.33495214616968516, 0.35501317644403096, 0.3646019796530406, 0.3381133242732003, 0.3552925022585051, 0.35489058592135, 0.370588452156101, 0.36798090434500147, 0.354283331671641, 0.3679241312756425, 0.33113111227395986, 0.34636236461145536, 0.358085104397365, 0.3654704757389568, 0.3409173465555623, 0.34969927104456083, 0.36258668044493314, 0.34124974356520743, 0.36711333461460616, 0.3654561875654118, 0.34813900886192206, 0.36901365131849334, 0.3719823117412272, 0.37060111530479933, 0.37902271800807547, 0.36866617229368004, 0.3643135630658695, 0.3660255404455321, 0.363064139017037, 0.3679055754272711, 0.37477139312596547, 0.38397285838921863, 0.36167614676413085, 0.3620980623222533, 0.3543117064095679, 0.3768989463292417, 0.3578952419615927, 0.3740567674949056, 0.36732173143398195, 0.374798446893692, 0.35630709323145093, 0.3796309530735016, 0.3727317956231889, 0.36825067709599224, 0.36938383820511045, 0.3638070821762085, 0.3556315323249215, 0.35907689446494695, 0.3618342182820752, 0.36074996633189066, 0.35997817683077993, 0.3771731888077089, 0.3794571324473336, 0.3830755452315013, 0.3810573530693849, 0.3806448064389683, 0.35769328545956386, 0.38294310175946783, 0.35298959644777433, 0.37350858960832867, 0.36943724378943443, 0.3794166087394669], 'loss': [209.61704875298128, 127.94704782762852, 76.19671575187004, 47.404460665392286, 31.77679847517247, 23.075458521647146, 18.15062065797434, 14.89353727554342, 12.746427402161716, 11.291383360308766, 10.242177211735, 9.423233951611259, 8.790809082897592, 8.295227540764294, 7.880685027541511, 7.408608182032306, 7.0527871659387165, 6.777746084730635, 6.556731742081533, 6.358142206520388, 6.190546200320007, 6.032326008649789, 5.891221654743778, 5.774053087578313, 5.638507675430507, 5.52104342160759, 5.421767714902893, 5.302162401428428, 5.2022252195438075, 5.10192752068319, 5.00652160197755, 4.911177741876797, 4.83326555043028, 4.753448707464, 4.67680191722522, 4.607363196358829, 4.532574634550163, 4.474058273740878, 4.354809517021098, 4.280972302017447, 4.229656113487966, 4.166809419365731, 4.117041276235306, 4.064302448410967, 4.005596012928501, 3.959177084596074, 3.9277209133363677, 3.8751242355227538, 3.80791401881484, 3.7643768609500885, 3.722753908906756, 3.6860036172456816, 3.6536295373062417, 3.6202612583729628, 3.5727925743697257, 3.5392195689372525, 3.5108759045164226, 3.46460806126077, 3.421145770644921, 3.3825312261064777, 3.35100641635794, 3.3233709313723767, 3.295378906730974, 3.2624846992636085, 3.2334961394296577, 3.203007021137853, 3.1768074162047286, 3.1546856770516363, 3.128835508123353, 3.1084350344531217, 3.08623974211119, 3.0598423560046726, 3.0390571453954207, 3.0146482467007862, 2.993785468971704, 2.980402653217775, 2.9629313637807186, 2.9502590968012696, 2.928794447893533, 2.912138634250186, 2.898530782659504, 2.8784968684996075, 2.864793582943471, 2.8517564055040343, 2.838174560016259, 2.81110517643586, 2.7908915957205784, 2.788129605423262, 2.766577531083921, 2.752407358863438, 2.741995827641955, 2.7216159923471466, 2.709157561736459, 2.6939377521351626, 2.686091081287582, 2.6730860080796215, 2.6773204946646647, 2.662316417013815, 2.651562420968401, 2.6394808547722484, 2.626376550942688, 2.6256342159430828, 2.6088522470868436, 2.601827864061322, 2.59880675119496, 2.584729127601688, 2.5689373962079856, 2.5656117670610046, 2.568866256507394, 2.5534504987794997, 2.544168851038659, 2.5430436568244565, 2.5324827225561752, 2.5228871470222085, 2.514524092282637, 2.5090843714430355, 2.4956298184711994, 2.491283808182907, 2.491858436809883, 2.483634765860155, 2.485398683815488, 2.4775938428342283, 2.4672476546438484, 2.46381611724752, 2.452727060040367, 2.455490394021956, 2.445715939100018, 2.4484639223862503, 2.4317931176518943, 2.435202523856258, 2.4274156429828744, 2.4356538213377217, 2.4223869942920113, 2.426372982275746, 2.418472660881765, 2.4166090655745123, 2.4072976280136653, 2.4065332731714317, 2.4034922141111354, 2.4083397714117716, 2.3944156524738367, 2.3932042037542094, 2.3947895321975627, 2.3880409802318607, 2.385699572442744, 2.3843625005968785, 2.3760085471544694, 2.374201837798546, 2.3760752347891376, 2.369316410209394, 2.3762102255086712, 2.3637132009422017, 2.3622262248741035, 2.355431199487473, 2.3538211345764535, 2.3554496559674787, 2.3546480834196823, 2.351737952195589, 2.343474289551392, 2.3475432500964257, 2.34728677302214, 2.340270750849422, 2.3368291436120545, 2.3373288087861397, 2.330366124140635, 2.325685505074574, 2.3281749809365158, 2.322718191289341, 2.326040840181101, 2.3231656717063607, 2.3234655682190732, 2.3114975580756365], 'acc': [0.4093525706328407, 0.7340889662268627, 0.8365799847861717, 0.8657209548918939, 0.8681864589424015, 0.8688211163643447, 0.869226829070725, 0.8692395280086399, 0.8692396425640098, 0.869239643942949, 0.8692396453333793, 0.8692396459539019, 0.8692396445404893, 0.869239641713664, 0.8692396437131258, 0.8692396430236562, 0.869239643942949, 0.8692396429432181, 0.8692396431040943, 0.8692396471949472, 0.869239644528998, 0.8692391358613278, 0.8692348943019571, 0.8692245362166704, 0.8692139695208726, 0.8692425423006757, 0.8692508347925708, 0.8694241144465462, 0.8697105927896656, 0.8699908958985347, 0.8703011921864612, 0.8705289703309939, 0.8709126270435577, 0.8714239370714889, 0.8717018357931086, 0.8719734104814524, 0.8725669814691347, 0.8730273589431515, 0.8734986054338838, 0.8745528067585496, 0.8752973177356252, 0.8764065735391139, 0.8776997236564759, 0.8789106395999797, 0.8803561205622179, 0.8812522263064512, 0.8821775924270039, 0.8829446041172632, 0.884149190672611, 0.8848471463542465, 0.8856309838830011, 0.8861664883750745, 0.8859673917971527, 0.8858129739853282, 0.8863433092596996, 0.8868279701853984, 0.8876714712970859, 0.888353094076499, 0.8887392076909116, 0.8889500716069495, 0.8894166362315491, 0.8899519041781023, 0.8905118754173442, 0.8911753261574252, 0.8918405930293142, 0.8922013812771141, 0.8924192853899666, 0.8926691488192815, 0.8927696244850402, 0.8929436445603923, 0.892987878940366, 0.893168088694356, 0.89323340934068, 0.8937018040933199, 0.8941634550176422, 0.8944640890559595, 0.894828370489871, 0.8952739865926503, 0.8957124945628797, 0.896293202863623, 0.8967420392834384, 0.8970831054378985, 0.8977213704289557, 0.8979806200902265, 0.8983157106463519, 0.898983730811569, 0.8997659677137224, 0.8995376983565909, 0.9000003008408562, 0.9003118920790476, 0.900312772451257, 0.900985797947258, 0.901317250241382, 0.9018725860502671, 0.9019891172561293, 0.9023813677932662, 0.9024653673654626, 0.9027398397680282, 0.9033491389586421, 0.9034241049740617, 0.9037064063955829, 0.9036994265043737, 0.9043471957530869, 0.9043205750570008, 0.9045258999008744, 0.9050199929082556, 0.9055383221961089, 0.9056014205670803, 0.9056878278474714, 0.9061329588739312, 0.9063061452311819, 0.906410581761756, 0.9064743272343972, 0.9069979880755638, 0.9068884070102985, 0.9072228715740143, 0.9074233566057666, 0.9076886241873129, 0.9078422989692674, 0.9078432263288283, 0.9078648715711454, 0.9079017557917658, 0.9085005853409157, 0.9080888485549984, 0.9085150706025708, 0.9084629080119518, 0.9085541350638818, 0.9088210538331314, 0.9090999514123068, 0.9092149509040519, 0.9091786851261493, 0.9092455640311689, 0.909600947745301, 0.9094027583139904, 0.9094638189231589, 0.9096779701841666, 0.9097365968048584, 0.9099085782220971, 0.9097661890458666, 0.90978384440819, 0.9100746961832277, 0.9101138769350737, 0.9100696220433802, 0.91017764861905, 0.9102361800814494, 0.9103112117458636, 0.9103494905451026, 0.9105788494824949, 0.9105981703390155, 0.9106135593458984, 0.9106004873245053, 0.9109054303523179, 0.9107623655188858, 0.9110622335105405, 0.9111207636859299, 0.9109281614057954, 0.9110378104872342, 0.9111102652820935, 0.9110551212296818, 0.9111730629409097, 0.9111881507185352, 0.911441652363887, 0.9114731635021432, 0.9115530132488959, 0.9117711524593823, 0.9118126072197813, 0.9115142917642249, 0.9119013321981141, 0.9117236521999248, 0.9117631584963221, 0.9119372249499943, 0.9119876475607197], 'mDice': [0.018352299002908617, 0.017870582449130524, 0.017697330412720817, 0.017628916256820646, 0.017885652469682607, 0.01820846137427137, 0.018755810055181496, 0.0198679237348371, 0.021290391118716178, 0.02306698559845703, 0.02514814691381439, 0.027557806446731767, 0.03062213371547863, 0.03368878814370067, 0.037008670117926544, 0.040730663166039016, 0.0444332575186705, 0.04859388118072215, 0.052166845357174446, 0.055875766113851484, 0.05930921199787369, 0.06285150295275356, 0.0665669609683544, 0.069850758924658, 0.07409848955088046, 0.07813062301077593, 0.08232957153007063, 0.08767399992790208, 0.09370025391405848, 0.09965138560071579, 0.10514137183330595, 0.11007889302446361, 0.1149809963416566, 0.12013193089619109, 0.12534801678917692, 0.13104555962263606, 0.1371600004044116, 0.14172812988642255, 0.1486938120526157, 0.15500439349536793, 0.1601587880495451, 0.16679319173402127, 0.17282310639855855, 0.17885182134581595, 0.18650261501321816, 0.192769569380908, 0.19700097128194446, 0.20256332625640894, 0.20854971230213917, 0.2135840449916712, 0.2178987354982421, 0.22161737993339456, 0.2247419202600069, 0.22812675827611772, 0.2330025988252999, 0.2369886367955786, 0.23978395637391228, 0.24452074517030717, 0.2490619401068087, 0.25316878729802417, 0.25723831722427015, 0.26040926506713746, 0.26427141916712793, 0.26821850480683074, 0.27252636493366994, 0.2755465877522663, 0.2784494403264921, 0.28103212283391676, 0.28432456032949904, 0.2876446080479016, 0.29061254286136334, 0.29335328630912, 0.2960266027580182, 0.2994732556429032, 0.30232133701262687, 0.3038332112692261, 0.30622724859797706, 0.3087694445498904, 0.31114670417661355, 0.3132835413785897, 0.315489875217704, 0.3185156740985074, 0.32086209960013246, 0.32259799599509625, 0.32401843404916986, 0.3286296062750326, 0.33270322446283496, 0.3324508033427104, 0.3365152302690616, 0.33912003871699115, 0.3405766147327993, 0.34482387438522777, 0.3470754364632172, 0.3497555544950012, 0.35007743091281723, 0.3538287239415305, 0.35283964350087027, 0.35595698534281744, 0.35790101395812546, 0.3594978389044545, 0.362235522313869, 0.36145236502154293, 0.3648939169174861, 0.365987214987876, 0.36652686760470155, 0.3699754381687719, 0.3725611456952298, 0.37285796458238074, 0.3724323945949136, 0.3749276177488312, 0.37676644396501535, 0.3781244269691958, 0.37893877722106045, 0.38110220535701933, 0.38274410671527753, 0.38421760687645123, 0.3863036533311679, 0.38763618286528845, 0.387794857123281, 0.3887999527298556, 0.3886312858801438, 0.39040148379440964, 0.39288504724215745, 0.39243287581204217, 0.3952924137816899, 0.3944948796353084, 0.39637363618891525, 0.3959647935319779, 0.39887803160365154, 0.39862044793644325, 0.3996462288081175, 0.39921147859645806, 0.4020856629912739, 0.4012211922772339, 0.40175711226091004, 0.4032811687037231, 0.404814163866773, 0.40472524569906526, 0.4054757474313151, 0.4045472449883574, 0.40738951358947767, 0.40759029086440063, 0.4075000274611135, 0.40829704325277727, 0.4086201408820872, 0.4088503837424573, 0.4110798126462569, 0.4114342932529203, 0.41184633059011666, 0.41216876709279654, 0.4110838156319179, 0.41394019304430507, 0.41376510110806014, 0.414301694760967, 0.41645873275500905, 0.4149157709760061, 0.4156622587198096, 0.41618082618492747, 0.41684644504252577, 0.4174710232006784, 0.4166938775774369, 0.4184717501384205, 0.41820481162161316, 0.41902677687533174, 0.41995826417571985, 0.4214943106553218, 0.4202930106750361, 0.42171394838196524, 0.4210009567995441, 0.4213557999925115, 0.42212148405164795, 0.42350359799109466]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.34s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.08s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.87s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:32,  1.38s/it]predicting train subjects:   1%|          | 2/285 [00:03<06:51,  1.46s/it]predicting train subjects:   1%|          | 3/285 [00:04<06:42,  1.43s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:14,  1.55s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<06:55,  1.48s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:22,  1.59s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<07:45,  1.67s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<07:55,  1.72s/it]predicting train subjects:   3%|▎         | 9/285 [00:14<07:38,  1.66s/it]predicting train subjects:   4%|▎         | 10/285 [00:16<07:54,  1.72s/it]predicting train subjects:   4%|▍         | 11/285 [00:18<08:05,  1.77s/it]predicting train subjects:   4%|▍         | 12/285 [00:20<08:16,  1.82s/it]predicting train subjects:   5%|▍         | 13/285 [00:22<08:20,  1.84s/it]predicting train subjects:   5%|▍         | 14/285 [00:24<08:21,  1.85s/it]predicting train subjects:   5%|▌         | 15/285 [00:25<08:25,  1.87s/it]predicting train subjects:   6%|▌         | 16/285 [00:27<08:31,  1.90s/it]predicting train subjects:   6%|▌         | 17/285 [00:29<08:38,  1.94s/it]predicting train subjects:   6%|▋         | 18/285 [00:31<08:34,  1.93s/it]predicting train subjects:   7%|▋         | 19/285 [00:33<08:25,  1.90s/it]predicting train subjects:   7%|▋         | 20/285 [00:35<08:20,  1.89s/it]predicting train subjects:   7%|▋         | 21/285 [00:37<08:23,  1.91s/it]predicting train subjects:   8%|▊         | 22/285 [00:39<08:22,  1.91s/it]predicting train subjects:   8%|▊         | 23/285 [00:41<08:27,  1.94s/it]predicting train subjects:   8%|▊         | 24/285 [00:43<08:24,  1.93s/it]predicting train subjects:   9%|▉         | 25/285 [00:45<08:29,  1.96s/it]predicting train subjects:   9%|▉         | 26/285 [00:47<08:28,  1.96s/it]predicting train subjects:   9%|▉         | 27/285 [00:49<08:18,  1.93s/it]predicting train subjects:  10%|▉         | 28/285 [00:51<08:15,  1.93s/it]predicting train subjects:  10%|█         | 29/285 [00:52<08:04,  1.89s/it]predicting train subjects:  11%|█         | 30/285 [00:54<08:00,  1.88s/it]predicting train subjects:  11%|█         | 31/285 [00:56<07:54,  1.87s/it]predicting train subjects:  11%|█         | 32/285 [00:58<07:42,  1.83s/it]predicting train subjects:  12%|█▏        | 33/285 [01:00<07:38,  1.82s/it]predicting train subjects:  12%|█▏        | 34/285 [01:01<07:32,  1.80s/it]predicting train subjects:  12%|█▏        | 35/285 [01:03<07:30,  1.80s/it]predicting train subjects:  13%|█▎        | 36/285 [01:05<07:28,  1.80s/it]predicting train subjects:  13%|█▎        | 37/285 [01:07<07:20,  1.78s/it]predicting train subjects:  13%|█▎        | 38/285 [01:08<07:17,  1.77s/it]predicting train subjects:  14%|█▎        | 39/285 [01:10<07:20,  1.79s/it]predicting train subjects:  14%|█▍        | 40/285 [01:12<07:18,  1.79s/it]predicting train subjects:  14%|█▍        | 41/285 [01:14<07:14,  1.78s/it]predicting train subjects:  15%|█▍        | 42/285 [01:16<07:20,  1.81s/it]predicting train subjects:  15%|█▌        | 43/285 [01:17<07:12,  1.79s/it]predicting train subjects:  15%|█▌        | 44/285 [01:19<07:09,  1.78s/it]predicting train subjects:  16%|█▌        | 45/285 [01:21<07:09,  1.79s/it]predicting train subjects:  16%|█▌        | 46/285 [01:23<06:52,  1.72s/it]predicting train subjects:  16%|█▋        | 47/285 [01:24<06:33,  1.65s/it]predicting train subjects:  17%|█▋        | 48/285 [01:26<06:21,  1.61s/it]predicting train subjects:  17%|█▋        | 49/285 [01:27<06:16,  1.60s/it]predicting train subjects:  18%|█▊        | 50/285 [01:29<06:08,  1.57s/it]predicting train subjects:  18%|█▊        | 51/285 [01:30<06:03,  1.55s/it]predicting train subjects:  18%|█▊        | 52/285 [01:32<06:06,  1.57s/it]predicting train subjects:  19%|█▊        | 53/285 [01:33<06:09,  1.59s/it]predicting train subjects:  19%|█▉        | 54/285 [01:35<06:08,  1.59s/it]predicting train subjects:  19%|█▉        | 55/285 [01:37<06:02,  1.58s/it]predicting train subjects:  20%|█▉        | 56/285 [01:38<05:57,  1.56s/it]predicting train subjects:  20%|██        | 57/285 [01:40<05:51,  1.54s/it]predicting train subjects:  20%|██        | 58/285 [01:41<05:59,  1.58s/it]predicting train subjects:  21%|██        | 59/285 [01:43<05:50,  1.55s/it]predicting train subjects:  21%|██        | 60/285 [01:44<05:50,  1.56s/it]predicting train subjects:  21%|██▏       | 61/285 [01:46<05:46,  1.55s/it]predicting train subjects:  22%|██▏       | 62/285 [01:47<05:45,  1.55s/it]predicting train subjects:  22%|██▏       | 63/285 [01:49<05:40,  1.54s/it]predicting train subjects:  22%|██▏       | 64/285 [01:51<05:43,  1.55s/it]predicting train subjects:  23%|██▎       | 65/285 [01:52<06:02,  1.65s/it]predicting train subjects:  23%|██▎       | 66/285 [01:54<06:03,  1.66s/it]predicting train subjects:  24%|██▎       | 67/285 [01:56<06:03,  1.67s/it]predicting train subjects:  24%|██▍       | 68/285 [01:57<05:57,  1.65s/it]predicting train subjects:  24%|██▍       | 69/285 [01:59<05:53,  1.63s/it]predicting train subjects:  25%|██▍       | 70/285 [02:01<05:46,  1.61s/it]predicting train subjects:  25%|██▍       | 71/285 [02:02<05:45,  1.62s/it]predicting train subjects:  25%|██▌       | 72/285 [02:04<05:44,  1.62s/it]predicting train subjects:  26%|██▌       | 73/285 [02:05<05:43,  1.62s/it]predicting train subjects:  26%|██▌       | 74/285 [02:07<05:38,  1.61s/it]predicting train subjects:  26%|██▋       | 75/285 [02:09<05:38,  1.61s/it]predicting train subjects:  27%|██▋       | 76/285 [02:10<05:32,  1.59s/it]predicting train subjects:  27%|██▋       | 77/285 [02:12<05:30,  1.59s/it]predicting train subjects:  27%|██▋       | 78/285 [02:13<05:35,  1.62s/it]predicting train subjects:  28%|██▊       | 79/285 [02:15<05:33,  1.62s/it]predicting train subjects:  28%|██▊       | 80/285 [02:17<05:32,  1.62s/it]predicting train subjects:  28%|██▊       | 81/285 [02:18<05:31,  1.63s/it]predicting train subjects:  29%|██▉       | 82/285 [02:20<05:28,  1.62s/it]predicting train subjects:  29%|██▉       | 83/285 [02:22<05:26,  1.62s/it]predicting train subjects:  29%|██▉       | 84/285 [02:23<05:24,  1.61s/it]predicting train subjects:  30%|██▉       | 85/285 [02:25<05:32,  1.66s/it]predicting train subjects:  30%|███       | 86/285 [02:27<05:42,  1.72s/it]predicting train subjects:  31%|███       | 87/285 [02:29<05:47,  1.76s/it]predicting train subjects:  31%|███       | 88/285 [02:30<05:52,  1.79s/it]predicting train subjects:  31%|███       | 89/285 [02:32<05:51,  1.79s/it]predicting train subjects:  32%|███▏      | 90/285 [02:34<05:51,  1.81s/it]predicting train subjects:  32%|███▏      | 91/285 [02:36<05:47,  1.79s/it]predicting train subjects:  32%|███▏      | 92/285 [02:38<05:53,  1.83s/it]predicting train subjects:  33%|███▎      | 93/285 [02:40<05:52,  1.83s/it]predicting train subjects:  33%|███▎      | 94/285 [02:41<05:49,  1.83s/it]predicting train subjects:  33%|███▎      | 95/285 [02:43<05:50,  1.84s/it]predicting train subjects:  34%|███▎      | 96/285 [02:45<05:49,  1.85s/it]predicting train subjects:  34%|███▍      | 97/285 [02:47<05:42,  1.82s/it]predicting train subjects:  34%|███▍      | 98/285 [02:49<05:36,  1.80s/it]predicting train subjects:  35%|███▍      | 99/285 [02:50<05:32,  1.79s/it]predicting train subjects:  35%|███▌      | 100/285 [02:52<05:30,  1.78s/it]predicting train subjects:  35%|███▌      | 101/285 [02:54<05:42,  1.86s/it]predicting train subjects:  36%|███▌      | 102/285 [02:56<05:38,  1.85s/it]predicting train subjects:  36%|███▌      | 103/285 [02:58<05:29,  1.81s/it]predicting train subjects:  36%|███▋      | 104/285 [02:59<05:20,  1.77s/it]predicting train subjects:  37%|███▋      | 105/285 [03:01<05:18,  1.77s/it]predicting train subjects:  37%|███▋      | 106/285 [03:03<05:12,  1.75s/it]predicting train subjects:  38%|███▊      | 107/285 [03:05<05:10,  1.75s/it]predicting train subjects:  38%|███▊      | 108/285 [03:06<05:07,  1.73s/it]predicting train subjects:  38%|███▊      | 109/285 [03:08<05:07,  1.75s/it]predicting train subjects:  39%|███▊      | 110/285 [03:10<05:02,  1.73s/it]predicting train subjects:  39%|███▉      | 111/285 [03:12<04:59,  1.72s/it]predicting train subjects:  39%|███▉      | 112/285 [03:13<04:57,  1.72s/it]predicting train subjects:  40%|███▉      | 113/285 [03:15<05:01,  1.75s/it]predicting train subjects:  40%|████      | 114/285 [03:17<05:02,  1.77s/it]predicting train subjects:  40%|████      | 115/285 [03:19<05:05,  1.80s/it]predicting train subjects:  41%|████      | 116/285 [03:21<05:07,  1.82s/it]predicting train subjects:  41%|████      | 117/285 [03:23<05:15,  1.88s/it]predicting train subjects:  41%|████▏     | 118/285 [03:25<05:18,  1.91s/it]predicting train subjects:  42%|████▏     | 119/285 [03:26<05:06,  1.85s/it]predicting train subjects:  42%|████▏     | 120/285 [03:28<05:05,  1.85s/it]predicting train subjects:  42%|████▏     | 121/285 [03:30<04:50,  1.77s/it]predicting train subjects:  43%|████▎     | 122/285 [03:31<04:36,  1.70s/it]predicting train subjects:  43%|████▎     | 123/285 [03:33<04:22,  1.62s/it]predicting train subjects:  44%|████▎     | 124/285 [03:34<04:25,  1.65s/it]predicting train subjects:  44%|████▍     | 125/285 [03:36<04:24,  1.65s/it]predicting train subjects:  44%|████▍     | 126/285 [03:38<04:22,  1.65s/it]predicting train subjects:  45%|████▍     | 127/285 [03:39<04:14,  1.61s/it]predicting train subjects:  45%|████▍     | 128/285 [03:41<04:12,  1.61s/it]predicting train subjects:  45%|████▌     | 129/285 [03:43<04:10,  1.61s/it]predicting train subjects:  46%|████▌     | 130/285 [03:44<04:09,  1.61s/it]predicting train subjects:  46%|████▌     | 131/285 [03:46<04:06,  1.60s/it]predicting train subjects:  46%|████▋     | 132/285 [03:47<04:04,  1.60s/it]predicting train subjects:  47%|████▋     | 133/285 [03:49<04:01,  1.59s/it]predicting train subjects:  47%|████▋     | 134/285 [03:50<03:58,  1.58s/it]predicting train subjects:  47%|████▋     | 135/285 [03:52<03:59,  1.60s/it]predicting train subjects:  48%|████▊     | 136/285 [03:54<03:58,  1.60s/it]predicting train subjects:  48%|████▊     | 137/285 [03:55<03:55,  1.59s/it]predicting train subjects:  48%|████▊     | 138/285 [03:57<03:51,  1.58s/it]predicting train subjects:  49%|████▉     | 139/285 [03:58<03:52,  1.59s/it]predicting train subjects:  49%|████▉     | 140/285 [04:00<03:50,  1.59s/it]predicting train subjects:  49%|████▉     | 141/285 [04:02<03:52,  1.61s/it]predicting train subjects:  50%|████▉     | 142/285 [04:03<03:51,  1.62s/it]predicting train subjects:  50%|█████     | 143/285 [04:05<03:41,  1.56s/it]predicting train subjects:  51%|█████     | 144/285 [04:06<03:34,  1.52s/it]predicting train subjects:  51%|█████     | 145/285 [04:08<03:29,  1.49s/it]predicting train subjects:  51%|█████     | 146/285 [04:09<03:26,  1.48s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:10<03:23,  1.47s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:12<03:20,  1.47s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:13<03:18,  1.46s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:15<03:16,  1.45s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:16<03:13,  1.45s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:18<03:11,  1.44s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:19<03:08,  1.43s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:21<03:11,  1.46s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:22<03:10,  1.47s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:24<03:08,  1.46s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:25<03:04,  1.44s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:26<03:06,  1.47s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:28<03:04,  1.46s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:29<03:02,  1.46s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:31<02:59,  1.44s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:32<03:00,  1.47s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:34<03:00,  1.48s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:35<02:57,  1.47s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:37<02:56,  1.47s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:38<02:52,  1.45s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:40<02:50,  1.45s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:41<02:48,  1.44s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:42<02:45,  1.42s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:44<02:42,  1.41s/it]predicting train subjects:  60%|██████    | 171/285 [04:45<02:43,  1.43s/it]predicting train subjects:  60%|██████    | 172/285 [04:47<02:39,  1.41s/it]predicting train subjects:  61%|██████    | 173/285 [04:48<02:39,  1.42s/it]predicting train subjects:  61%|██████    | 174/285 [04:49<02:36,  1.41s/it]predicting train subjects:  61%|██████▏   | 175/285 [04:51<02:36,  1.42s/it]predicting train subjects:  62%|██████▏   | 176/285 [04:52<02:34,  1.42s/it]predicting train subjects:  62%|██████▏   | 177/285 [04:54<02:35,  1.44s/it]predicting train subjects:  62%|██████▏   | 178/285 [04:55<02:32,  1.42s/it]predicting train subjects:  63%|██████▎   | 179/285 [04:57<02:31,  1.43s/it]predicting train subjects:  63%|██████▎   | 180/285 [04:58<02:26,  1.40s/it]predicting train subjects:  64%|██████▎   | 181/285 [04:59<02:22,  1.37s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:01<02:20,  1.36s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:02<02:22,  1.40s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:03<02:20,  1.39s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:05<02:20,  1.40s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:07<02:25,  1.47s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:08<02:29,  1.53s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:10<02:26,  1.51s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:11<02:23,  1.49s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:13<02:20,  1.48s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:14<02:16,  1.46s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:15<02:17,  1.48s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:17<02:15,  1.47s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:18<02:10,  1.43s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:20<02:08,  1.43s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:21<02:14,  1.51s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:23<02:20,  1.59s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:25<02:20,  1.62s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:27<02:21,  1.64s/it]predicting train subjects:  70%|███████   | 200/285 [05:28<02:19,  1.64s/it]predicting train subjects:  71%|███████   | 201/285 [05:30<02:17,  1.64s/it]predicting train subjects:  71%|███████   | 202/285 [05:32<02:18,  1.67s/it]predicting train subjects:  71%|███████   | 203/285 [05:33<02:17,  1.68s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:35<02:16,  1.68s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:37<02:15,  1.69s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:38<02:11,  1.67s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:40<02:12,  1.69s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:42<02:11,  1.71s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:43<02:09,  1.70s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:45<02:07,  1.71s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:47<02:05,  1.70s/it]predicting train subjects:  74%|███████▍  | 212/285 [05:49<02:03,  1.69s/it]predicting train subjects:  75%|███████▍  | 213/285 [05:50<02:00,  1.67s/it]predicting train subjects:  75%|███████▌  | 214/285 [05:52<01:54,  1.61s/it]predicting train subjects:  75%|███████▌  | 215/285 [05:53<01:50,  1.58s/it]predicting train subjects:  76%|███████▌  | 216/285 [05:55<01:49,  1.59s/it]predicting train subjects:  76%|███████▌  | 217/285 [05:56<01:49,  1.61s/it]predicting train subjects:  76%|███████▋  | 218/285 [05:58<01:47,  1.61s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:00<01:44,  1.59s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:01<01:41,  1.56s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:03<01:44,  1.63s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:05<01:43,  1.64s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:06<01:40,  1.62s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:08<01:38,  1.61s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:09<01:35,  1.59s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:11<01:32,  1.56s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:12<01:30,  1.56s/it]predicting train subjects:  80%|████████  | 228/285 [06:14<01:28,  1.55s/it]predicting train subjects:  80%|████████  | 229/285 [06:15<01:26,  1.55s/it]predicting train subjects:  81%|████████  | 230/285 [06:17<01:25,  1.55s/it]predicting train subjects:  81%|████████  | 231/285 [06:18<01:23,  1.55s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:20<01:29,  1.68s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:22<01:32,  1.78s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:24<01:33,  1.84s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:26<01:33,  1.87s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:28<01:33,  1.92s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:30<01:33,  1.94s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:32<01:30,  1.93s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:34<01:29,  1.94s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:36<01:27,  1.94s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:38<01:23,  1.90s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:40<01:22,  1.91s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:42<01:19,  1.89s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:44<01:16,  1.88s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:46<01:15,  1.89s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:47<01:13,  1.88s/it]predicting train subjects:  87%|████████▋ | 247/285 [06:49<01:12,  1.90s/it]predicting train subjects:  87%|████████▋ | 248/285 [06:51<01:09,  1.89s/it]predicting train subjects:  87%|████████▋ | 249/285 [06:53<01:07,  1.87s/it]predicting train subjects:  88%|████████▊ | 250/285 [06:55<01:01,  1.75s/it]predicting train subjects:  88%|████████▊ | 251/285 [06:56<00:55,  1.64s/it]predicting train subjects:  88%|████████▊ | 252/285 [06:57<00:52,  1.59s/it]predicting train subjects:  89%|████████▉ | 253/285 [06:59<00:48,  1.52s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:00<00:45,  1.48s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:01<00:43,  1.45s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:03<00:41,  1.45s/it]predicting train subjects:  90%|█████████ | 257/285 [07:04<00:40,  1.44s/it]predicting train subjects:  91%|█████████ | 258/285 [07:06<00:38,  1.43s/it]predicting train subjects:  91%|█████████ | 259/285 [07:07<00:36,  1.42s/it]predicting train subjects:  91%|█████████ | 260/285 [07:09<00:35,  1.40s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:10<00:33,  1.41s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:11<00:32,  1.40s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:13<00:31,  1.41s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:14<00:29,  1.40s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:15<00:27,  1.39s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:17<00:26,  1.40s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:18<00:25,  1.41s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:20<00:26,  1.55s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:22<00:26,  1.63s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:24<00:25,  1.70s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:26<00:24,  1.75s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:28<00:23,  1.78s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:29<00:21,  1.81s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:31<00:19,  1.82s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:33<00:18,  1.84s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:35<00:16,  1.87s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:37<00:14,  1.87s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:39<00:13,  1.87s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:41<00:11,  1.88s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:43<00:09,  1.85s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:45<00:07,  1.89s/it]predicting train subjects:  99%|█████████▉| 282/285 [07:46<00:05,  1.87s/it]predicting train subjects:  99%|█████████▉| 283/285 [07:48<00:03,  1.86s/it]predicting train subjects: 100%|█████████▉| 284/285 [07:50<00:01,  1.87s/it]predicting train subjects: 100%|██████████| 285/285 [07:52<00:00,  1.86s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:28,  1.37s/it]Loading train:   1%|          | 2/285 [00:02<06:47,  1.44s/it]Loading train:   1%|          | 3/285 [00:04<06:40,  1.42s/it]Loading train:   1%|▏         | 4/285 [00:06<07:14,  1.55s/it]Loading train:   2%|▏         | 5/285 [00:07<06:55,  1.48s/it]Loading train:   2%|▏         | 6/285 [00:09<07:07,  1.53s/it]Loading train:   2%|▏         | 7/285 [00:11<07:33,  1.63s/it]Loading train:   3%|▎         | 8/285 [00:12<07:40,  1.66s/it]Loading train:   3%|▎         | 9/285 [00:14<07:28,  1.62s/it]Loading train:   4%|▎         | 10/285 [00:15<07:04,  1.54s/it]Loading train:   4%|▍         | 11/285 [00:16<06:34,  1.44s/it]Loading train:   4%|▍         | 12/285 [00:18<06:13,  1.37s/it]Loading train:   5%|▍         | 13/285 [00:19<06:21,  1.40s/it]Loading train:   5%|▍         | 14/285 [00:20<06:15,  1.39s/it]Loading train:   5%|▌         | 15/285 [00:22<06:08,  1.36s/it]Loading train:   6%|▌         | 16/285 [00:23<05:50,  1.30s/it]Loading train:   6%|▌         | 17/285 [00:24<05:43,  1.28s/it]Loading train:   6%|▋         | 18/285 [00:25<05:43,  1.29s/it]Loading train:   7%|▋         | 19/285 [00:27<05:45,  1.30s/it]Loading train:   7%|▋         | 20/285 [00:28<05:37,  1.27s/it]Loading train:   7%|▋         | 21/285 [00:29<05:27,  1.24s/it]Loading train:   8%|▊         | 22/285 [00:31<05:45,  1.32s/it]Loading train:   8%|▊         | 23/285 [00:32<05:51,  1.34s/it]Loading train:   8%|▊         | 24/285 [00:33<05:48,  1.34s/it]Loading train:   9%|▉         | 25/285 [00:35<05:39,  1.31s/it]Loading train:   9%|▉         | 26/285 [00:36<05:38,  1.31s/it]Loading train:   9%|▉         | 27/285 [00:37<05:37,  1.31s/it]Loading train:  10%|▉         | 28/285 [00:39<05:45,  1.34s/it]Loading train:  10%|█         | 29/285 [00:40<05:20,  1.25s/it]Loading train:  11%|█         | 30/285 [00:41<05:13,  1.23s/it]Loading train:  11%|█         | 31/285 [00:42<05:10,  1.22s/it]Loading train:  11%|█         | 32/285 [00:43<04:53,  1.16s/it]Loading train:  12%|█▏        | 33/285 [00:44<04:48,  1.14s/it]Loading train:  12%|█▏        | 34/285 [00:45<04:55,  1.18s/it]Loading train:  12%|█▏        | 35/285 [00:46<04:44,  1.14s/it]Loading train:  13%|█▎        | 36/285 [00:47<04:36,  1.11s/it]Loading train:  13%|█▎        | 37/285 [00:49<04:48,  1.16s/it]Loading train:  13%|█▎        | 38/285 [00:50<05:18,  1.29s/it]Loading train:  14%|█▎        | 39/285 [00:52<05:12,  1.27s/it]Loading train:  14%|█▍        | 40/285 [00:53<05:25,  1.33s/it]Loading train:  14%|█▍        | 41/285 [00:54<05:26,  1.34s/it]Loading train:  15%|█▍        | 42/285 [00:56<05:26,  1.34s/it]Loading train:  15%|█▌        | 43/285 [00:57<05:28,  1.36s/it]Loading train:  15%|█▌        | 44/285 [00:59<05:59,  1.49s/it]Loading train:  16%|█▌        | 45/285 [01:00<05:58,  1.49s/it]Loading train:  16%|█▌        | 46/285 [01:02<06:09,  1.54s/it]Loading train:  16%|█▋        | 47/285 [01:04<06:07,  1.55s/it]Loading train:  17%|█▋        | 48/285 [01:05<06:05,  1.54s/it]Loading train:  17%|█▋        | 49/285 [01:07<05:51,  1.49s/it]Loading train:  18%|█▊        | 50/285 [01:08<05:35,  1.43s/it]Loading train:  18%|█▊        | 51/285 [01:09<05:23,  1.38s/it]Loading train:  18%|█▊        | 52/285 [01:11<05:20,  1.38s/it]Loading train:  19%|█▊        | 53/285 [01:12<05:21,  1.38s/it]Loading train:  19%|█▉        | 54/285 [01:13<05:22,  1.39s/it]Loading train:  19%|█▉        | 55/285 [01:15<05:22,  1.40s/it]Loading train:  20%|█▉        | 56/285 [01:16<05:11,  1.36s/it]Loading train:  20%|██        | 57/285 [01:17<05:13,  1.37s/it]Loading train:  20%|██        | 58/285 [01:19<05:20,  1.41s/it]Loading train:  21%|██        | 59/285 [01:21<05:32,  1.47s/it]Loading train:  21%|██        | 60/285 [01:22<05:26,  1.45s/it]Loading train:  21%|██▏       | 61/285 [01:23<05:20,  1.43s/it]Loading train:  22%|██▏       | 62/285 [01:25<05:11,  1.40s/it]Loading train:  22%|██▏       | 63/285 [01:26<05:13,  1.41s/it]Loading train:  22%|██▏       | 64/285 [01:28<05:28,  1.49s/it]Loading train:  23%|██▎       | 65/285 [01:29<05:31,  1.51s/it]Loading train:  23%|██▎       | 66/285 [01:31<05:29,  1.51s/it]Loading train:  24%|██▎       | 67/285 [01:32<04:58,  1.37s/it]Loading train:  24%|██▍       | 68/285 [01:33<04:31,  1.25s/it]Loading train:  24%|██▍       | 69/285 [01:34<04:15,  1.18s/it]Loading train:  25%|██▍       | 70/285 [01:35<04:01,  1.12s/it]Loading train:  25%|██▍       | 71/285 [01:36<03:50,  1.08s/it]Loading train:  25%|██▌       | 72/285 [01:37<03:43,  1.05s/it]Loading train:  26%|██▌       | 73/285 [01:38<03:41,  1.04s/it]Loading train:  26%|██▌       | 74/285 [01:39<03:41,  1.05s/it]Loading train:  26%|██▋       | 75/285 [01:40<03:44,  1.07s/it]Loading train:  27%|██▋       | 76/285 [01:41<03:46,  1.08s/it]Loading train:  27%|██▋       | 77/285 [01:42<03:48,  1.10s/it]Loading train:  27%|██▋       | 78/285 [01:43<03:41,  1.07s/it]Loading train:  28%|██▊       | 79/285 [01:44<03:42,  1.08s/it]Loading train:  28%|██▊       | 80/285 [01:45<03:36,  1.06s/it]Loading train:  28%|██▊       | 81/285 [01:46<03:31,  1.04s/it]Loading train:  29%|██▉       | 82/285 [01:47<03:28,  1.03s/it]Loading train:  29%|██▉       | 83/285 [01:48<03:26,  1.02s/it]Loading train:  29%|██▉       | 84/285 [01:49<03:27,  1.03s/it]Loading train:  30%|██▉       | 85/285 [01:50<03:28,  1.04s/it]Loading train:  30%|███       | 86/285 [01:52<03:41,  1.11s/it]Loading train:  31%|███       | 87/285 [01:53<03:39,  1.11s/it]Loading train:  31%|███       | 88/285 [01:54<03:36,  1.10s/it]Loading train:  31%|███       | 89/285 [01:55<03:36,  1.10s/it]Loading train:  32%|███▏      | 90/285 [01:56<03:33,  1.10s/it]Loading train:  32%|███▏      | 91/285 [01:57<03:41,  1.14s/it]Loading train:  32%|███▏      | 92/285 [01:59<03:40,  1.14s/it]Loading train:  33%|███▎      | 93/285 [02:00<03:33,  1.11s/it]Loading train:  33%|███▎      | 94/285 [02:01<03:42,  1.17s/it]Loading train:  33%|███▎      | 95/285 [02:02<03:44,  1.18s/it]Loading train:  34%|███▎      | 96/285 [02:03<03:35,  1.14s/it]Loading train:  34%|███▍      | 97/285 [02:04<03:42,  1.19s/it]Loading train:  34%|███▍      | 98/285 [02:06<03:38,  1.17s/it]Loading train:  35%|███▍      | 99/285 [02:07<03:33,  1.15s/it]Loading train:  35%|███▌      | 100/285 [02:08<03:37,  1.17s/it]Loading train:  35%|███▌      | 101/285 [02:09<03:31,  1.15s/it]Loading train:  36%|███▌      | 102/285 [02:10<03:32,  1.16s/it]Loading train:  36%|███▌      | 103/285 [02:11<03:36,  1.19s/it]Loading train:  36%|███▋      | 104/285 [02:13<03:31,  1.17s/it]Loading train:  37%|███▋      | 105/285 [02:14<03:23,  1.13s/it]Loading train:  37%|███▋      | 106/285 [02:15<03:27,  1.16s/it]Loading train:  38%|███▊      | 107/285 [02:16<03:22,  1.14s/it]Loading train:  38%|███▊      | 108/285 [02:17<03:13,  1.09s/it]Loading train:  38%|███▊      | 109/285 [02:18<03:15,  1.11s/it]Loading train:  39%|███▊      | 110/285 [02:19<03:13,  1.11s/it]Loading train:  39%|███▉      | 111/285 [02:20<03:12,  1.10s/it]Loading train:  39%|███▉      | 112/285 [02:21<03:16,  1.13s/it]Loading train:  40%|███▉      | 113/285 [02:23<03:20,  1.17s/it]Loading train:  40%|████      | 114/285 [02:24<03:13,  1.13s/it]Loading train:  40%|████      | 115/285 [02:25<03:09,  1.12s/it]Loading train:  41%|████      | 116/285 [02:26<03:07,  1.11s/it]Loading train:  41%|████      | 117/285 [02:27<03:00,  1.08s/it]Loading train:  41%|████▏     | 118/285 [02:28<02:53,  1.04s/it]Loading train:  42%|████▏     | 119/285 [02:29<02:53,  1.05s/it]Loading train:  42%|████▏     | 120/285 [02:30<02:54,  1.06s/it]Loading train:  42%|████▏     | 121/285 [02:31<03:12,  1.17s/it]Loading train:  43%|████▎     | 122/285 [02:33<03:13,  1.19s/it]Loading train:  43%|████▎     | 123/285 [02:34<03:20,  1.24s/it]Loading train:  44%|████▎     | 124/285 [02:35<03:04,  1.15s/it]Loading train:  44%|████▍     | 125/285 [02:36<02:57,  1.11s/it]Loading train:  44%|████▍     | 126/285 [02:37<02:49,  1.07s/it]Loading train:  45%|████▍     | 127/285 [02:38<02:48,  1.07s/it]Loading train:  45%|████▍     | 128/285 [02:39<02:42,  1.03s/it]Loading train:  45%|████▌     | 129/285 [02:40<02:45,  1.06s/it]Loading train:  46%|████▌     | 130/285 [02:41<02:40,  1.04s/it]Loading train:  46%|████▌     | 131/285 [02:42<02:43,  1.06s/it]Loading train:  46%|████▋     | 132/285 [02:43<02:43,  1.07s/it]Loading train:  47%|████▋     | 133/285 [02:44<02:38,  1.04s/it]Loading train:  47%|████▋     | 134/285 [02:45<02:33,  1.02s/it]Loading train:  47%|████▋     | 135/285 [02:46<02:38,  1.06s/it]Loading train:  48%|████▊     | 136/285 [02:47<02:33,  1.03s/it]Loading train:  48%|████▊     | 137/285 [02:48<02:30,  1.02s/it]Loading train:  48%|████▊     | 138/285 [02:49<02:27,  1.01s/it]Loading train:  49%|████▉     | 139/285 [02:50<02:27,  1.01s/it]Loading train:  49%|████▉     | 140/285 [02:51<02:26,  1.01s/it]Loading train:  49%|████▉     | 141/285 [02:52<02:21,  1.01it/s]Loading train:  50%|████▉     | 142/285 [02:53<02:21,  1.01it/s]Loading train:  50%|█████     | 143/285 [02:54<02:19,  1.02it/s]Loading train:  51%|█████     | 144/285 [02:55<02:19,  1.01it/s]Loading train:  51%|█████     | 145/285 [02:56<02:15,  1.03it/s]Loading train:  51%|█████     | 146/285 [02:57<02:15,  1.03it/s]Loading train:  52%|█████▏    | 147/285 [02:58<02:14,  1.02it/s]Loading train:  52%|█████▏    | 148/285 [02:59<02:09,  1.06it/s]Loading train:  52%|█████▏    | 149/285 [03:00<02:15,  1.01it/s]Loading train:  53%|█████▎    | 150/285 [03:01<02:06,  1.07it/s]Loading train:  53%|█████▎    | 151/285 [03:02<02:09,  1.04it/s]Loading train:  53%|█████▎    | 152/285 [03:03<02:06,  1.05it/s]Loading train:  54%|█████▎    | 153/285 [03:04<02:00,  1.09it/s]Loading train:  54%|█████▍    | 154/285 [03:05<01:59,  1.10it/s]Loading train:  54%|█████▍    | 155/285 [03:06<02:03,  1.05it/s]Loading train:  55%|█████▍    | 156/285 [03:07<02:09,  1.00s/it]Loading train:  55%|█████▌    | 157/285 [03:08<02:02,  1.04it/s]Loading train:  55%|█████▌    | 158/285 [03:09<02:03,  1.03it/s]Loading train:  56%|█████▌    | 159/285 [03:10<02:01,  1.04it/s]Loading train:  56%|█████▌    | 160/285 [03:10<01:59,  1.05it/s]Loading train:  56%|█████▋    | 161/285 [03:11<01:59,  1.03it/s]Loading train:  57%|█████▋    | 162/285 [03:12<01:54,  1.08it/s]Loading train:  57%|█████▋    | 163/285 [03:13<01:54,  1.07it/s]Loading train:  58%|█████▊    | 164/285 [03:14<01:53,  1.06it/s]Loading train:  58%|█████▊    | 165/285 [03:15<01:49,  1.09it/s]Loading train:  58%|█████▊    | 166/285 [03:16<01:49,  1.08it/s]Loading train:  59%|█████▊    | 167/285 [03:17<01:47,  1.10it/s]Loading train:  59%|█████▉    | 168/285 [03:18<01:50,  1.06it/s]Loading train:  59%|█████▉    | 169/285 [03:19<01:48,  1.07it/s]Loading train:  60%|█████▉    | 170/285 [03:20<01:49,  1.05it/s]Loading train:  60%|██████    | 171/285 [03:21<01:48,  1.05it/s]Loading train:  60%|██████    | 172/285 [03:22<01:45,  1.07it/s]Loading train:  61%|██████    | 173/285 [03:23<01:46,  1.05it/s]Loading train:  61%|██████    | 174/285 [03:24<01:45,  1.05it/s]Loading train:  61%|██████▏   | 175/285 [03:25<01:46,  1.04it/s]Loading train:  62%|██████▏   | 176/285 [03:25<01:42,  1.07it/s]Loading train:  62%|██████▏   | 177/285 [03:26<01:36,  1.12it/s]Loading train:  62%|██████▏   | 178/285 [03:27<01:34,  1.13it/s]Loading train:  63%|██████▎   | 179/285 [03:28<01:33,  1.13it/s]Loading train:  63%|██████▎   | 180/285 [03:29<01:37,  1.08it/s]Loading train:  64%|██████▎   | 181/285 [03:30<01:35,  1.09it/s]Loading train:  64%|██████▍   | 182/285 [03:31<01:37,  1.06it/s]Loading train:  64%|██████▍   | 183/285 [03:32<01:35,  1.07it/s]Loading train:  65%|██████▍   | 184/285 [03:33<01:31,  1.10it/s]Loading train:  65%|██████▍   | 185/285 [03:34<01:29,  1.12it/s]Loading train:  65%|██████▌   | 186/285 [03:34<01:28,  1.11it/s]Loading train:  66%|██████▌   | 187/285 [03:35<01:29,  1.09it/s]Loading train:  66%|██████▌   | 188/285 [03:36<01:27,  1.11it/s]Loading train:  66%|██████▋   | 189/285 [03:37<01:25,  1.12it/s]Loading train:  67%|██████▋   | 190/285 [03:38<01:25,  1.11it/s]Loading train:  67%|██████▋   | 191/285 [03:39<01:26,  1.08it/s]Loading train:  67%|██████▋   | 192/285 [03:40<01:25,  1.09it/s]Loading train:  68%|██████▊   | 193/285 [03:41<01:24,  1.09it/s]Loading train:  68%|██████▊   | 194/285 [03:42<01:26,  1.05it/s]Loading train:  68%|██████▊   | 195/285 [03:43<01:23,  1.07it/s]Loading train:  69%|██████▉   | 196/285 [03:44<01:26,  1.03it/s]Loading train:  69%|██████▉   | 197/285 [03:45<01:24,  1.04it/s]Loading train:  69%|██████▉   | 198/285 [03:46<01:23,  1.04it/s]Loading train:  70%|██████▉   | 199/285 [03:47<01:24,  1.02it/s]Loading train:  70%|███████   | 200/285 [03:48<01:21,  1.04it/s]Loading train:  71%|███████   | 201/285 [03:49<01:22,  1.02it/s]Loading train:  71%|███████   | 202/285 [03:50<01:19,  1.04it/s]Loading train:  71%|███████   | 203/285 [03:51<01:20,  1.01it/s]Loading train:  72%|███████▏  | 204/285 [03:52<01:19,  1.01it/s]Loading train:  72%|███████▏  | 205/285 [03:53<01:25,  1.07s/it]Loading train:  72%|███████▏  | 206/285 [03:54<01:20,  1.01s/it]Loading train:  73%|███████▎  | 207/285 [03:55<01:16,  1.02it/s]Loading train:  73%|███████▎  | 208/285 [03:56<01:15,  1.01it/s]Loading train:  73%|███████▎  | 209/285 [03:57<01:18,  1.03s/it]Loading train:  74%|███████▎  | 210/285 [03:58<01:18,  1.04s/it]Loading train:  74%|███████▍  | 211/285 [03:59<01:16,  1.03s/it]Loading train:  74%|███████▍  | 212/285 [04:00<01:13,  1.01s/it]Loading train:  75%|███████▍  | 213/285 [04:01<01:10,  1.03it/s]Loading train:  75%|███████▌  | 214/285 [04:02<01:10,  1.01it/s]Loading train:  75%|███████▌  | 215/285 [04:03<01:06,  1.05it/s]Loading train:  76%|███████▌  | 216/285 [04:04<01:05,  1.05it/s]Loading train:  76%|███████▌  | 217/285 [04:05<01:03,  1.07it/s]Loading train:  76%|███████▋  | 218/285 [04:05<01:03,  1.06it/s]Loading train:  77%|███████▋  | 219/285 [04:06<01:01,  1.08it/s]Loading train:  77%|███████▋  | 220/285 [04:07<00:58,  1.11it/s]Loading train:  78%|███████▊  | 221/285 [04:08<00:58,  1.10it/s]Loading train:  78%|███████▊  | 222/285 [04:09<00:55,  1.13it/s]Loading train:  78%|███████▊  | 223/285 [04:10<00:53,  1.15it/s]Loading train:  79%|███████▊  | 224/285 [04:11<00:52,  1.17it/s]Loading train:  79%|███████▉  | 225/285 [04:11<00:49,  1.20it/s]Loading train:  79%|███████▉  | 226/285 [04:13<01:03,  1.08s/it]Loading train:  80%|███████▉  | 227/285 [04:14<01:00,  1.04s/it]Loading train:  80%|████████  | 228/285 [04:15<01:03,  1.12s/it]Loading train:  80%|████████  | 229/285 [04:17<01:08,  1.22s/it]Loading train:  81%|████████  | 230/285 [04:18<01:05,  1.18s/it]Loading train:  81%|████████  | 231/285 [04:19<01:06,  1.23s/it]Loading train:  81%|████████▏ | 232/285 [04:21<01:15,  1.42s/it]Loading train:  82%|████████▏ | 233/285 [04:23<01:17,  1.49s/it]Loading train:  82%|████████▏ | 234/285 [04:24<01:10,  1.38s/it]Loading train:  82%|████████▏ | 235/285 [04:25<01:11,  1.44s/it]Loading train:  83%|████████▎ | 236/285 [04:26<01:05,  1.33s/it]Loading train:  83%|████████▎ | 237/285 [04:28<01:13,  1.53s/it]Loading train:  84%|████████▎ | 238/285 [04:30<01:08,  1.46s/it]Loading train:  84%|████████▍ | 239/285 [04:32<01:13,  1.60s/it]Loading train:  84%|████████▍ | 240/285 [04:33<01:12,  1.61s/it]Loading train:  85%|████████▍ | 241/285 [04:35<01:12,  1.64s/it]Loading train:  85%|████████▍ | 242/285 [04:37<01:08,  1.59s/it]Loading train:  85%|████████▌ | 243/285 [04:38<01:00,  1.45s/it]Loading train:  86%|████████▌ | 244/285 [04:39<00:59,  1.46s/it]Loading train:  86%|████████▌ | 245/285 [04:41<00:58,  1.46s/it]Loading train:  86%|████████▋ | 246/285 [04:42<00:59,  1.53s/it]Loading train:  87%|████████▋ | 247/285 [04:44<00:59,  1.56s/it]Loading train:  87%|████████▋ | 248/285 [04:46<01:06,  1.79s/it]Loading train:  87%|████████▋ | 249/285 [04:48<01:08,  1.89s/it]Loading train:  88%|████████▊ | 250/285 [04:50<01:01,  1.76s/it]Loading train:  88%|████████▊ | 251/285 [04:51<00:53,  1.58s/it]Loading train:  88%|████████▊ | 252/285 [04:52<00:48,  1.46s/it]Loading train:  89%|████████▉ | 253/285 [04:53<00:44,  1.40s/it]Loading train:  89%|████████▉ | 254/285 [04:55<00:44,  1.44s/it]Loading train:  89%|████████▉ | 255/285 [04:56<00:40,  1.34s/it]Loading train:  90%|████████▉ | 256/285 [04:57<00:36,  1.27s/it]Loading train:  90%|█████████ | 257/285 [04:59<00:41,  1.49s/it]Loading train:  91%|█████████ | 258/285 [05:01<00:41,  1.52s/it]Loading train:  91%|█████████ | 259/285 [05:03<00:42,  1.65s/it]Loading train:  91%|█████████ | 260/285 [05:05<00:42,  1.72s/it]Loading train:  92%|█████████▏| 261/285 [05:06<00:40,  1.69s/it]Loading train:  92%|█████████▏| 262/285 [05:08<00:39,  1.71s/it]Loading train:  92%|█████████▏| 263/285 [05:10<00:37,  1.70s/it]Loading train:  93%|█████████▎| 264/285 [05:11<00:34,  1.66s/it]Loading train:  93%|█████████▎| 265/285 [05:13<00:32,  1.60s/it]Loading train:  93%|█████████▎| 266/285 [05:14<00:30,  1.60s/it]Loading train:  94%|█████████▎| 267/285 [05:16<00:30,  1.70s/it]Loading train:  94%|█████████▍| 268/285 [05:19<00:32,  1.91s/it]Loading train:  94%|█████████▍| 269/285 [05:21<00:32,  2.00s/it]Loading train:  95%|█████████▍| 270/285 [05:23<00:30,  2.04s/it]Loading train:  95%|█████████▌| 271/285 [05:26<00:31,  2.24s/it]Loading train:  95%|█████████▌| 272/285 [05:28<00:29,  2.29s/it]Loading train:  96%|█████████▌| 273/285 [05:31<00:28,  2.37s/it]Loading train:  96%|█████████▌| 274/285 [05:33<00:26,  2.42s/it]Loading train:  96%|█████████▋| 275/285 [05:35<00:23,  2.34s/it]Loading train:  97%|█████████▋| 276/285 [05:38<00:21,  2.36s/it]Loading train:  97%|█████████▋| 277/285 [05:40<00:19,  2.38s/it]Loading train:  98%|█████████▊| 278/285 [05:42<00:16,  2.37s/it]Loading train:  98%|█████████▊| 279/285 [05:44<00:13,  2.25s/it]Loading train:  98%|█████████▊| 280/285 [05:46<00:09,  1.97s/it]Loading train:  99%|█████████▊| 281/285 [05:47<00:07,  1.79s/it]Loading train:  99%|█████████▉| 282/285 [05:49<00:05,  1.70s/it]Loading train:  99%|█████████▉| 283/285 [05:50<00:03,  1.71s/it]Loading train: 100%|█████████▉| 284/285 [05:52<00:01,  1.74s/it]Loading train: 100%|██████████| 285/285 [05:54<00:00,  1.73s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   3%|▎         | 8/285 [00:00<00:03, 76.25it/s]concatenating: train:   9%|▉         | 27/285 [00:00<00:02, 91.87it/s]concatenating: train:  12%|█▏        | 34/285 [00:00<00:03, 77.18it/s]concatenating: train:  14%|█▍        | 41/285 [00:00<00:05, 43.28it/s]concatenating: train:  16%|█▋        | 47/285 [00:00<00:07, 31.08it/s]concatenating: train:  18%|█▊        | 52/285 [00:01<00:08, 26.40it/s]concatenating: train:  20%|██        | 58/285 [00:01<00:07, 31.11it/s]concatenating: train:  22%|██▏       | 63/285 [00:01<00:08, 27.45it/s]concatenating: train:  24%|██▍       | 68/285 [00:01<00:07, 30.62it/s]concatenating: train:  29%|██▉       | 84/285 [00:01<00:04, 40.34it/s]concatenating: train:  39%|███▊      | 110/285 [00:01<00:03, 53.96it/s]concatenating: train:  48%|████▊     | 138/285 [00:02<00:02, 71.11it/s]concatenating: train:  57%|█████▋    | 162/285 [00:02<00:01, 89.82it/s]concatenating: train:  64%|██████▎   | 181/285 [00:02<00:01, 82.44it/s]concatenating: train:  69%|██████▉   | 197/285 [00:02<00:01, 58.17it/s]concatenating: train:  73%|███████▎  | 209/285 [00:03<00:01, 42.49it/s]concatenating: train:  77%|███████▋  | 219/285 [00:03<00:02, 32.66it/s]concatenating: train:  79%|███████▉  | 226/285 [00:03<00:01, 34.51it/s]concatenating: train:  82%|████████▏ | 233/285 [00:04<00:01, 32.00it/s]concatenating: train:  84%|████████▍ | 239/285 [00:04<00:01, 34.16it/s]concatenating: train:  86%|████████▌ | 244/285 [00:04<00:01, 31.47it/s]concatenating: train:  93%|█████████▎| 265/285 [00:04<00:00, 42.20it/s]concatenating: train: 100%|██████████| 285/285 [00:04<00:00, 60.10it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:02<00:04,  2.22s/it]Loading test:  67%|██████▋   | 2/3 [00:04<00:02,  2.12s/it]Loading test: 100%|██████████| 3/3 [00:05<00:00,  1.99s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 48.98it/s]2019-07-06 17:52:27.533470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-06 17:52:27.533657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-06 17:52:27.533683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-06 17:52:27.533699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-06 17:52:27.534317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:09,  3.92it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:08,  4.36it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.36it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:06,  5.32it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:06,  5.13it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:05,  5.34it/s]loading the weights for Unet:  30%|███       | 12/40 [00:02<00:06,  4.45it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:04,  5.75it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  5.46it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:02<00:03,  5.70it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:03<00:04,  4.39it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:03,  5.12it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  5.66it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:04<00:03,  4.60it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:04<00:02,  5.37it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  5.12it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  5.99it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:05<00:01,  5.91it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:05<00:00,  5.11it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  5.16it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:06<00:00,  3.92it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:06<00:00,  6.44it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 4  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 52, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 52, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 52, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 52, 10)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 52, 10)   910         dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 52, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 52, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 52, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 52, 10)   910         dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 52, 10)   40          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 52, 10)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 52, 10)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 52, 20)   1820        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 52, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 52, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 52, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 52, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 52, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 26, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 26, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 26, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 26, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 26, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 26, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 26, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 13, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 13, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 13, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 13, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 13, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 13, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 13, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 13, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 26, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 26, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 26, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 26, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 26, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 26, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 26, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 26, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 52, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 52, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 52, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 52, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 52, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 52, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 52, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 52, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 52, 10)   1810        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 52, 10)   40          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 52, 10)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 52, 10)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 52, 13)   143         dropout_9[0][0]                  
==================================================================================================
Total params: 189,493
Trainable params: 45,933
Non-trainable params: 143,560
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.49841486e-02 3.19966680e-02 7.50970181e-02 9.33357939e-03
 2.71292049e-02 7.07427267e-03 8.46489586e-02 1.12779077e-01
 8.61338510e-02 1.32649165e-02 2.94521391e-01 1.92807035e-01
 2.29878984e-04]
Train on 18361 samples, validate on 179 samples
Epoch 1/300
 - 24s - loss: 208.2165 - acc: 0.2718 - mDice: 0.0150 - val_loss: 117.5181 - val_acc: 0.9136 - val_mDice: 0.0131

Epoch 00001: val_mDice improved from -inf to 0.01307, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 15s - loss: 106.7201 - acc: 0.8259 - mDice: 0.0151 - val_loss: 48.4659 - val_acc: 0.9136 - val_mDice: 0.0132

Epoch 00002: val_mDice improved from 0.01307 to 0.01324, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 16s - loss: 52.2694 - acc: 0.8860 - mDice: 0.0162 - val_loss: 22.2463 - val_acc: 0.9136 - val_mDice: 0.0161

Epoch 00003: val_mDice improved from 0.01324 to 0.01610, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 17s - loss: 28.0721 - acc: 0.8863 - mDice: 0.0176 - val_loss: 16.9125 - val_acc: 0.9136 - val_mDice: 0.0243

Epoch 00004: val_mDice improved from 0.01610 to 0.02428, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 14s - loss: 18.0419 - acc: 0.8863 - mDice: 0.0190 - val_loss: 9.2193 - val_acc: 0.9136 - val_mDice: 0.0245

Epoch 00005: val_mDice improved from 0.02428 to 0.02451, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 14s - loss: 13.3995 - acc: 0.8863 - mDice: 0.0211 - val_loss: 6.3767 - val_acc: 0.9136 - val_mDice: 0.0264

Epoch 00006: val_mDice improved from 0.02451 to 0.02639, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 14s - loss: 10.8915 - acc: 0.8863 - mDice: 0.0246 - val_loss: 5.4864 - val_acc: 0.9136 - val_mDice: 0.0326

Epoch 00007: val_mDice improved from 0.02639 to 0.03265, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 13s - loss: 9.2992 - acc: 0.8863 - mDice: 0.0297 - val_loss: 4.7017 - val_acc: 0.9136 - val_mDice: 0.0397

Epoch 00008: val_mDice improved from 0.03265 to 0.03972, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 14s - loss: 8.1876 - acc: 0.8863 - mDice: 0.0355 - val_loss: 4.2556 - val_acc: 0.9136 - val_mDice: 0.0509

Epoch 00009: val_mDice improved from 0.03972 to 0.05092, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 16s - loss: 7.3844 - acc: 0.8864 - mDice: 0.0416 - val_loss: 4.0959 - val_acc: 0.9136 - val_mDice: 0.0571

Epoch 00010: val_mDice improved from 0.05092 to 0.05706, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 18s - loss: 6.7897 - acc: 0.8872 - mDice: 0.0475 - val_loss: 3.9365 - val_acc: 0.9136 - val_mDice: 0.0682

Epoch 00011: val_mDice improved from 0.05706 to 0.06815, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 16s - loss: 6.3249 - acc: 0.8882 - mDice: 0.0530 - val_loss: 3.6691 - val_acc: 0.9144 - val_mDice: 0.0773

Epoch 00012: val_mDice improved from 0.06815 to 0.07734, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 14s - loss: 5.9593 - acc: 0.8893 - mDice: 0.0583 - val_loss: 3.5209 - val_acc: 0.9145 - val_mDice: 0.0828

Epoch 00013: val_mDice improved from 0.07734 to 0.08282, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 14s - loss: 5.6657 - acc: 0.8894 - mDice: 0.0635 - val_loss: 3.4876 - val_acc: 0.9174 - val_mDice: 0.0903

Epoch 00014: val_mDice improved from 0.08282 to 0.09026, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 14s - loss: 5.4059 - acc: 0.8905 - mDice: 0.0689 - val_loss: 3.3500 - val_acc: 0.9174 - val_mDice: 0.0995

Epoch 00015: val_mDice improved from 0.09026 to 0.09954, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 13s - loss: 5.1968 - acc: 0.8915 - mDice: 0.0746 - val_loss: 3.3027 - val_acc: 0.9151 - val_mDice: 0.1069

Epoch 00016: val_mDice improved from 0.09954 to 0.10689, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 14s - loss: 4.9722 - acc: 0.8922 - mDice: 0.0809 - val_loss: 3.2028 - val_acc: 0.9187 - val_mDice: 0.1148

Epoch 00017: val_mDice improved from 0.10689 to 0.11479, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 13s - loss: 4.7263 - acc: 0.8930 - mDice: 0.0910 - val_loss: 3.0415 - val_acc: 0.9194 - val_mDice: 0.1342

Epoch 00018: val_mDice improved from 0.11479 to 0.13422, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 14s - loss: 4.5285 - acc: 0.8948 - mDice: 0.1008 - val_loss: 3.0470 - val_acc: 0.9214 - val_mDice: 0.1444

Epoch 00019: val_mDice improved from 0.13422 to 0.14438, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 14s - loss: 4.3556 - acc: 0.8962 - mDice: 0.1091 - val_loss: 2.9908 - val_acc: 0.9205 - val_mDice: 0.1543

Epoch 00020: val_mDice improved from 0.14438 to 0.15427, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 13s - loss: 4.2007 - acc: 0.8972 - mDice: 0.1174 - val_loss: 2.8575 - val_acc: 0.9216 - val_mDice: 0.1635

Epoch 00021: val_mDice improved from 0.15427 to 0.16355, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 14s - loss: 4.0655 - acc: 0.8980 - mDice: 0.1256 - val_loss: 2.8499 - val_acc: 0.9216 - val_mDice: 0.1718

Epoch 00022: val_mDice improved from 0.16355 to 0.17176, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 14s - loss: 3.9402 - acc: 0.8995 - mDice: 0.1347 - val_loss: 2.8514 - val_acc: 0.9231 - val_mDice: 0.1805

Epoch 00023: val_mDice improved from 0.17176 to 0.18046, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 13s - loss: 3.8277 - acc: 0.9014 - mDice: 0.1450 - val_loss: 2.8334 - val_acc: 0.9276 - val_mDice: 0.1937

Epoch 00024: val_mDice improved from 0.18046 to 0.19370, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 14s - loss: 3.7233 - acc: 0.9033 - mDice: 0.1545 - val_loss: 2.7434 - val_acc: 0.9290 - val_mDice: 0.2021

Epoch 00025: val_mDice improved from 0.19370 to 0.20215, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 14s - loss: 3.6194 - acc: 0.9049 - mDice: 0.1645 - val_loss: 2.6978 - val_acc: 0.9306 - val_mDice: 0.2125

Epoch 00026: val_mDice improved from 0.20215 to 0.21248, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 13s - loss: 3.5248 - acc: 0.9061 - mDice: 0.1734 - val_loss: 2.6219 - val_acc: 0.9335 - val_mDice: 0.2238

Epoch 00027: val_mDice improved from 0.21248 to 0.22377, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 14s - loss: 3.4370 - acc: 0.9071 - mDice: 0.1833 - val_loss: 2.5304 - val_acc: 0.9343 - val_mDice: 0.2379

Epoch 00028: val_mDice improved from 0.22377 to 0.23792, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 14s - loss: 3.3449 - acc: 0.9080 - mDice: 0.1942 - val_loss: 2.4931 - val_acc: 0.9358 - val_mDice: 0.2458

Epoch 00029: val_mDice improved from 0.23792 to 0.24581, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 13s - loss: 3.2689 - acc: 0.9092 - mDice: 0.2029 - val_loss: 2.4920 - val_acc: 0.9377 - val_mDice: 0.2483

Epoch 00030: val_mDice improved from 0.24581 to 0.24829, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 14s - loss: 3.1927 - acc: 0.9104 - mDice: 0.2113 - val_loss: 2.5750 - val_acc: 0.9335 - val_mDice: 0.2584

Epoch 00031: val_mDice improved from 0.24829 to 0.25842, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 14s - loss: 3.1227 - acc: 0.9116 - mDice: 0.2190 - val_loss: 2.3688 - val_acc: 0.9386 - val_mDice: 0.2740

Epoch 00032: val_mDice improved from 0.25842 to 0.27397, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 14s - loss: 3.0542 - acc: 0.9128 - mDice: 0.2271 - val_loss: 2.4689 - val_acc: 0.9377 - val_mDice: 0.2777

Epoch 00033: val_mDice improved from 0.27397 to 0.27771, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 14s - loss: 2.9945 - acc: 0.9140 - mDice: 0.2347 - val_loss: 2.3530 - val_acc: 0.9401 - val_mDice: 0.2946

Epoch 00034: val_mDice improved from 0.27771 to 0.29457, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 13s - loss: 2.9330 - acc: 0.9151 - mDice: 0.2422 - val_loss: 2.3082 - val_acc: 0.9386 - val_mDice: 0.3057

Epoch 00035: val_mDice improved from 0.29457 to 0.30568, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 13s - loss: 2.8705 - acc: 0.9159 - mDice: 0.2495 - val_loss: 2.3764 - val_acc: 0.9426 - val_mDice: 0.3053

Epoch 00036: val_mDice did not improve from 0.30568
Epoch 37/300
 - 13s - loss: 2.8080 - acc: 0.9170 - mDice: 0.2594 - val_loss: 2.3513 - val_acc: 0.9390 - val_mDice: 0.3186

Epoch 00037: val_mDice improved from 0.30568 to 0.31858, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 13s - loss: 2.7455 - acc: 0.9178 - mDice: 0.2707 - val_loss: 2.2758 - val_acc: 0.9425 - val_mDice: 0.3309

Epoch 00038: val_mDice improved from 0.31858 to 0.33091, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 13s - loss: 2.6948 - acc: 0.9183 - mDice: 0.2813 - val_loss: 2.2388 - val_acc: 0.9410 - val_mDice: 0.3391

Epoch 00039: val_mDice improved from 0.33091 to 0.33914, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 40/300
 - 13s - loss: 2.6464 - acc: 0.9190 - mDice: 0.2898 - val_loss: 2.3151 - val_acc: 0.9420 - val_mDice: 0.3393

Epoch 00040: val_mDice improved from 0.33914 to 0.33928, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 13s - loss: 2.6058 - acc: 0.9197 - mDice: 0.2971 - val_loss: 2.1980 - val_acc: 0.9447 - val_mDice: 0.3532

Epoch 00041: val_mDice improved from 0.33928 to 0.35324, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 13s - loss: 2.5741 - acc: 0.9199 - mDice: 0.3027 - val_loss: 2.2231 - val_acc: 0.9424 - val_mDice: 0.3587

Epoch 00042: val_mDice improved from 0.35324 to 0.35868, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 13s - loss: 2.5322 - acc: 0.9204 - mDice: 0.3094 - val_loss: 2.5213 - val_acc: 0.9427 - val_mDice: 0.3494

Epoch 00043: val_mDice did not improve from 0.35868
Epoch 44/300
 - 13s - loss: 2.4972 - acc: 0.9203 - mDice: 0.3151 - val_loss: 2.5618 - val_acc: 0.9414 - val_mDice: 0.3354

Epoch 00044: val_mDice did not improve from 0.35868
Epoch 45/300
 - 14s - loss: 2.4665 - acc: 0.9203 - mDice: 0.3197 - val_loss: 2.2947 - val_acc: 0.9447 - val_mDice: 0.3761

Epoch 00045: val_mDice improved from 0.35868 to 0.37608, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 13s - loss: 2.4329 - acc: 0.9206 - mDice: 0.3257 - val_loss: 2.2389 - val_acc: 0.9443 - val_mDice: 0.3761

Epoch 00046: val_mDice improved from 0.37608 to 0.37614, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 14s - loss: 2.4084 - acc: 0.9209 - mDice: 0.3300 - val_loss: 2.2638 - val_acc: 0.9446 - val_mDice: 0.3730

Epoch 00047: val_mDice did not improve from 0.37614
Epoch 48/300
 - 13s - loss: 2.3734 - acc: 0.9214 - mDice: 0.3367 - val_loss: 2.3194 - val_acc: 0.9446 - val_mDice: 0.3743

Epoch 00048: val_mDice did not improve from 0.37614
Epoch 49/300
 - 14s - loss: 2.3374 - acc: 0.9221 - mDice: 0.3446 - val_loss: 2.2521 - val_acc: 0.9440 - val_mDice: 0.3847

Epoch 00049: val_mDice improved from 0.37614 to 0.38468, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 14s - loss: 2.3117 - acc: 0.9227 - mDice: 0.3505 - val_loss: 2.2054 - val_acc: 0.9459 - val_mDice: 0.3963

Epoch 00050: val_mDice improved from 0.38468 to 0.39626, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 14s - loss: 2.2874 - acc: 0.9231 - mDice: 0.3561 - val_loss: 2.1860 - val_acc: 0.9459 - val_mDice: 0.3971

Epoch 00051: val_mDice improved from 0.39626 to 0.39710, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 14s - loss: 2.2641 - acc: 0.9230 - mDice: 0.3601 - val_loss: 2.1988 - val_acc: 0.9463 - val_mDice: 0.4016

Epoch 00052: val_mDice improved from 0.39710 to 0.40163, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 14s - loss: 2.2385 - acc: 0.9230 - mDice: 0.3660 - val_loss: 2.2591 - val_acc: 0.9452 - val_mDice: 0.4014

Epoch 00053: val_mDice did not improve from 0.40163
Epoch 54/300
 - 14s - loss: 2.2092 - acc: 0.9228 - mDice: 0.3721 - val_loss: 2.5459 - val_acc: 0.9466 - val_mDice: 0.4007

Epoch 00054: val_mDice did not improve from 0.40163
Epoch 55/300
 - 14s - loss: 2.1838 - acc: 0.9228 - mDice: 0.3784 - val_loss: 2.1914 - val_acc: 0.9466 - val_mDice: 0.4190

Epoch 00055: val_mDice improved from 0.40163 to 0.41899, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 14s - loss: 2.1605 - acc: 0.9231 - mDice: 0.3836 - val_loss: 2.2817 - val_acc: 0.9457 - val_mDice: 0.4129

Epoch 00056: val_mDice did not improve from 0.41899
Epoch 57/300
 - 14s - loss: 2.1414 - acc: 0.9232 - mDice: 0.3874 - val_loss: 2.3392 - val_acc: 0.9464 - val_mDice: 0.4131

Epoch 00057: val_mDice did not improve from 0.41899
Epoch 58/300
 - 14s - loss: 2.1227 - acc: 0.9236 - mDice: 0.3913 - val_loss: 2.0913 - val_acc: 0.9454 - val_mDice: 0.4234

Epoch 00058: val_mDice improved from 0.41899 to 0.42342, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 59/300
 - 14s - loss: 2.1071 - acc: 0.9238 - mDice: 0.3954 - val_loss: 2.2915 - val_acc: 0.9444 - val_mDice: 0.4119

Epoch 00059: val_mDice did not improve from 0.42342
Epoch 60/300
 - 14s - loss: 2.0917 - acc: 0.9241 - mDice: 0.3979 - val_loss: 2.2414 - val_acc: 0.9460 - val_mDice: 0.4189

Epoch 00060: val_mDice did not improve from 0.42342
Epoch 61/300
 - 14s - loss: 2.0770 - acc: 0.9243 - mDice: 0.4017 - val_loss: 2.0641 - val_acc: 0.9455 - val_mDice: 0.4277

Epoch 00061: val_mDice improved from 0.42342 to 0.42769, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 62/300
 - 13s - loss: 2.0599 - acc: 0.9244 - mDice: 0.4054 - val_loss: 2.1634 - val_acc: 0.9466 - val_mDice: 0.4341

Epoch 00062: val_mDice improved from 0.42769 to 0.43408, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 15s - loss: 2.0411 - acc: 0.9247 - mDice: 0.4100 - val_loss: 2.1845 - val_acc: 0.9457 - val_mDice: 0.4301

Epoch 00063: val_mDice did not improve from 0.43408
Epoch 64/300
 - 13s - loss: 2.0289 - acc: 0.9247 - mDice: 0.4132 - val_loss: 2.1320 - val_acc: 0.9480 - val_mDice: 0.4454

Epoch 00064: val_mDice improved from 0.43408 to 0.44544, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB1_FM10_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 65/300
 - 14s - loss: 2.0120 - acc: 0.9252 - mDice: 0.4163 - val_loss: 2.2396 - val_acc: 0.9461 - val_mDice: 0.4351

Epoch 00065: val_mDice did not improve from 0.44544
Epoch 66/300
 - 14s - loss: 2.0080 - acc: 0.9251 - mDice: 0.4179 - val_loss: 2.2554 - val_acc: 0.9462 - val_mDice: 0.4317

Epoch 00066: val_mDice did not improve from 0.44544
Epoch 67/300
 - 13s - loss: 1.9942 - acc: 0.9252 - mDice: 0.4207 - val_loss: 2.2408 - val_acc: 0.9459 - val_mDice: 0.4335

Epoch 00067: val_mDice did not improve from 0.44544
Epoch 68/300
 - 14s - loss: 1.9809 - acc: 0.9255 - mDice: 0.4237 - val_loss: 2.2938 - val_acc: 0.9471 - val_mDice: 0.4325

Epoch 00068: val_mDice did not improve from 0.44544
Epoch 69/300
 - 13s - loss: 1.9720 - acc: 0.9255 - mDice: 0.4267 - val_loss: 2.2139 - val_acc: 0.9450 - val_mDice: 0.4282

Epoch 00069: val_mDice did not improve from 0.44544
Epoch 70/300
 - 14s - loss: 1.9677 - acc: 0.9254 - mDice: 0.4277 - val_loss: 2.3019 - val_acc: 0.9450 - val_mDice: 0.4300

Epoch 00070: val_mDice did not improve from 0.44544
Epoch 71/300
 - 13s - loss: 1.9525 - acc: 0.9257 - mDice: 0.4316 - val_loss: 2.2472 - val_acc: 0.9457 - val_mDice: 0.4295

Epoch 00071: val_mDice did not improve from 0.44544
Epoch 72/300
 - 14s - loss: 1.9422 - acc: 0.9259 - mDice: 0.4345 - val_loss: 2.0717 - val_acc: 0.9464 - val_mDice: 0.4545
