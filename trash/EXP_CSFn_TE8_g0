2020-01-20 22:18:27.306163: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:29.413087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:29.413131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:29.833830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:29.833865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:29.833876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:29.834321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['a']
TypeExperiment 8
CrossVal ['a']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:32.903172: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:39.131489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:39.131554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:39.549249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:39.549320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:39.549332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:39.549798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['b']
TypeExperiment 8
CrossVal ['b']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:42.645613: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:45.590577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:45.590646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:46.008132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:46.008207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:46.008219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:46.008672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['c']
TypeExperiment 8
CrossVal ['c']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:49.146003: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:50.986251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:50.986318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:51.407723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:51.407789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:51.407802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:51.408246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['d']
TypeExperiment 8
CrossVal ['d']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:21:28.359668: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:21:32.282503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:21:32.282567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:21:32.687984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:21:32.688056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:21:32.688068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:21:32.688521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:53,  2.33it/s]Loading train:   1%|          | 2/266 [00:00<01:40,  2.62it/s]Loading train:   1%|          | 3/266 [00:00<01:28,  2.96it/s]Loading train:   2%|▏         | 4/266 [00:01<01:20,  3.24it/s]Loading train:   2%|▏         | 5/266 [00:01<01:17,  3.38it/s]Loading train:   2%|▏         | 6/266 [00:01<01:14,  3.50it/s]Loading train:   3%|▎         | 7/266 [00:01<01:12,  3.55it/s]Loading train:   3%|▎         | 8/266 [00:02<01:10,  3.64it/s]Loading train:   3%|▎         | 9/266 [00:02<01:08,  3.72it/s]Loading train:   4%|▍         | 10/266 [00:02<01:07,  3.78it/s]Loading train:   4%|▍         | 11/266 [00:03<01:06,  3.82it/s]Loading train:   5%|▍         | 12/266 [00:03<01:05,  3.85it/s]Loading train:   5%|▍         | 13/266 [00:03<01:05,  3.87it/s]Loading train:   5%|▌         | 14/266 [00:03<01:04,  3.89it/s]Loading train:   6%|▌         | 15/266 [00:04<01:04,  3.90it/s]Loading train:   6%|▌         | 16/266 [00:04<01:03,  3.91it/s]Loading train:   6%|▋         | 17/266 [00:04<01:03,  3.92it/s]Loading train:   7%|▋         | 18/266 [00:04<01:03,  3.92it/s]Loading train:   7%|▋         | 19/266 [00:05<01:02,  3.93it/s]Loading train:   8%|▊         | 20/266 [00:05<01:02,  3.93it/s]Loading train:   8%|▊         | 21/266 [00:05<01:02,  3.92it/s]Loading train:   8%|▊         | 22/266 [00:05<01:02,  3.90it/s]Loading train:   9%|▊         | 23/266 [00:06<01:02,  3.91it/s]Loading train:   9%|▉         | 24/266 [00:06<01:00,  3.97it/s]Loading train:   9%|▉         | 25/266 [00:06<01:00,  4.00it/s]Loading train:  10%|▉         | 26/266 [00:06<00:59,  4.02it/s]Loading train:  10%|█         | 27/266 [00:07<01:02,  3.85it/s]Loading train:  11%|█         | 28/266 [00:07<01:01,  3.87it/s]Loading train:  11%|█         | 29/266 [00:07<01:00,  3.94it/s]Loading train:  11%|█▏        | 30/266 [00:07<01:01,  3.82it/s]Loading train:  12%|█▏        | 31/266 [00:08<01:01,  3.84it/s]Loading train:  12%|█▏        | 32/266 [00:08<01:00,  3.86it/s]Loading train:  12%|█▏        | 33/266 [00:08<01:00,  3.88it/s]Loading train:  13%|█▎        | 34/266 [00:08<00:59,  3.89it/s]Loading train:  13%|█▎        | 35/266 [00:09<00:59,  3.90it/s]Loading train:  14%|█▎        | 36/266 [00:09<00:58,  3.91it/s]Loading train:  14%|█▍        | 37/266 [00:09<00:58,  3.92it/s]Loading train:  14%|█▍        | 38/266 [00:09<00:57,  3.94it/s]Loading train:  15%|█▍        | 39/266 [00:10<01:01,  3.67it/s]Loading train:  15%|█▌        | 40/266 [00:10<00:59,  3.77it/s]Loading train:  15%|█▌        | 41/266 [00:10<00:58,  3.86it/s]Loading train:  16%|█▌        | 42/266 [00:10<00:54,  4.12it/s]Loading train:  16%|█▌        | 43/266 [00:11<00:51,  4.30it/s]Loading train:  17%|█▋        | 44/266 [00:11<00:49,  4.46it/s]Loading train:  17%|█▋        | 45/266 [00:11<00:48,  4.54it/s]Loading train:  17%|█▋        | 46/266 [00:11<00:47,  4.64it/s]Loading train:  18%|█▊        | 47/266 [00:11<00:46,  4.71it/s]Loading train:  18%|█▊        | 48/266 [00:12<00:46,  4.70it/s]Loading train:  18%|█▊        | 49/266 [00:12<00:46,  4.70it/s]Loading train:  19%|█▉        | 50/266 [00:12<00:45,  4.73it/s]Loading train:  19%|█▉        | 51/266 [00:12<00:45,  4.76it/s]Loading train:  20%|█▉        | 52/266 [00:12<00:45,  4.74it/s]Loading train:  20%|█▉        | 53/266 [00:13<00:44,  4.76it/s]Loading train:  20%|██        | 54/266 [00:13<00:44,  4.74it/s]Loading train:  21%|██        | 55/266 [00:13<00:44,  4.73it/s]Loading train:  21%|██        | 56/266 [00:13<00:44,  4.73it/s]Loading train:  21%|██▏       | 57/266 [00:14<00:43,  4.75it/s]Loading train:  22%|██▏       | 58/266 [00:14<00:43,  4.74it/s]Loading train:  22%|██▏       | 59/266 [00:14<00:43,  4.78it/s]Loading train:  23%|██▎       | 60/266 [00:14<00:43,  4.75it/s]Loading train:  23%|██▎       | 61/266 [00:14<00:43,  4.70it/s]Loading train:  23%|██▎       | 62/266 [00:15<00:44,  4.62it/s]Loading train:  24%|██▎       | 63/266 [00:15<00:43,  4.64it/s]Loading train:  24%|██▍       | 64/266 [00:15<00:43,  4.62it/s]Loading train:  24%|██▍       | 65/266 [00:15<00:43,  4.63it/s]Loading train:  25%|██▍       | 66/266 [00:15<00:43,  4.64it/s]Loading train:  25%|██▌       | 67/266 [00:16<00:42,  4.65it/s]Loading train:  26%|██▌       | 68/266 [00:16<00:42,  4.65it/s]Loading train:  26%|██▌       | 69/266 [00:16<00:42,  4.66it/s]Loading train:  26%|██▋       | 70/266 [00:16<00:42,  4.59it/s]Loading train:  27%|██▋       | 71/266 [00:17<00:42,  4.60it/s]Loading train:  27%|██▋       | 72/266 [00:17<00:41,  4.63it/s]Loading train:  27%|██▋       | 73/266 [00:17<00:41,  4.66it/s]Loading train:  28%|██▊       | 74/266 [00:17<00:41,  4.67it/s]Loading train:  28%|██▊       | 75/266 [00:17<00:40,  4.67it/s]Loading train:  29%|██▊       | 76/266 [00:18<00:40,  4.66it/s]Loading train:  29%|██▉       | 77/266 [00:18<00:40,  4.65it/s]Loading train:  29%|██▉       | 78/266 [00:18<00:42,  4.47it/s]Loading train:  30%|██▉       | 79/266 [00:18<00:43,  4.32it/s]Loading train:  30%|███       | 80/266 [00:19<00:43,  4.26it/s]Loading train:  30%|███       | 81/266 [00:19<00:44,  4.18it/s]Loading train:  31%|███       | 82/266 [00:19<00:45,  4.08it/s]Loading train:  31%|███       | 83/266 [00:19<00:46,  3.94it/s]Loading train:  32%|███▏      | 84/266 [00:20<00:48,  3.77it/s]Loading train:  32%|███▏      | 85/266 [00:20<00:47,  3.79it/s]Loading train:  32%|███▏      | 86/266 [00:20<00:49,  3.67it/s]Loading train:  33%|███▎      | 87/266 [00:20<00:47,  3.78it/s]Loading train:  33%|███▎      | 88/266 [00:21<00:45,  3.88it/s]Loading train:  33%|███▎      | 89/266 [00:21<00:44,  3.95it/s]Loading train:  34%|███▍      | 90/266 [00:21<00:44,  3.97it/s]Loading train:  34%|███▍      | 91/266 [00:21<00:43,  4.01it/s]Loading train:  35%|███▍      | 92/266 [00:22<00:42,  4.05it/s]Loading train:  35%|███▍      | 93/266 [00:22<00:42,  4.06it/s]Loading train:  35%|███▌      | 94/266 [00:22<00:42,  4.08it/s]Loading train:  36%|███▌      | 95/266 [00:22<00:41,  4.11it/s]Loading train:  36%|███▌      | 96/266 [00:23<00:40,  4.15it/s]Loading train:  36%|███▋      | 97/266 [00:23<00:42,  3.97it/s]Loading train:  37%|███▋      | 98/266 [00:23<00:42,  3.94it/s]Loading train:  37%|███▋      | 99/266 [00:23<00:40,  4.12it/s]Loading train:  38%|███▊      | 100/266 [00:24<00:40,  4.14it/s]Loading train:  38%|███▊      | 101/266 [00:24<00:38,  4.24it/s]Loading train:  38%|███▊      | 102/266 [00:24<00:37,  4.33it/s]Loading train:  39%|███▊      | 103/266 [00:24<00:37,  4.38it/s]Loading train:  39%|███▉      | 104/266 [00:25<00:36,  4.44it/s]Loading train:  39%|███▉      | 105/266 [00:25<00:36,  4.46it/s]Loading train:  40%|███▉      | 106/266 [00:25<00:35,  4.50it/s]Loading train:  40%|████      | 107/266 [00:25<00:35,  4.51it/s]Loading train:  41%|████      | 108/266 [00:25<00:35,  4.51it/s]Loading train:  41%|████      | 109/266 [00:26<00:34,  4.52it/s]Loading train:  41%|████▏     | 110/266 [00:26<00:34,  4.53it/s]Loading train:  42%|████▏     | 111/266 [00:26<00:35,  4.34it/s]Loading train:  42%|████▏     | 112/266 [00:26<00:34,  4.42it/s]Loading train:  42%|████▏     | 113/266 [00:27<00:34,  4.40it/s]Loading train:  43%|████▎     | 114/266 [00:27<00:34,  4.42it/s]Loading train:  43%|████▎     | 115/266 [00:27<00:34,  4.44it/s]Loading train:  44%|████▎     | 116/266 [00:27<00:33,  4.45it/s]Loading train:  44%|████▍     | 117/266 [00:27<00:33,  4.49it/s]Loading train:  44%|████▍     | 118/266 [00:28<00:32,  4.52it/s]Loading train:  45%|████▍     | 119/266 [00:28<00:34,  4.30it/s]Loading train:  45%|████▌     | 120/266 [00:28<00:35,  4.17it/s]Loading train:  45%|████▌     | 121/266 [00:28<00:35,  4.07it/s]Loading train:  46%|████▌     | 122/266 [00:29<00:35,  4.01it/s]Loading train:  46%|████▌     | 123/266 [00:29<00:36,  3.97it/s]Loading train:  47%|████▋     | 124/266 [00:29<00:36,  3.94it/s]Loading train:  47%|████▋     | 125/266 [00:29<00:36,  3.89it/s]Loading train:  47%|████▋     | 126/266 [00:30<00:36,  3.85it/s]Loading train:  48%|████▊     | 127/266 [00:30<00:36,  3.86it/s]Loading train:  48%|████▊     | 128/266 [00:30<00:35,  3.87it/s]Loading train:  48%|████▊     | 129/266 [00:30<00:35,  3.90it/s]Loading train:  49%|████▉     | 130/266 [00:31<00:34,  3.90it/s]Loading train:  49%|████▉     | 131/266 [00:31<00:34,  3.91it/s]Loading train:  50%|████▉     | 132/266 [00:31<00:34,  3.91it/s]Loading train:  50%|█████     | 133/266 [00:32<00:34,  3.89it/s]Loading train:  50%|█████     | 134/266 [00:32<00:34,  3.87it/s]Loading train:  51%|█████     | 135/266 [00:32<00:33,  3.87it/s]Loading train:  51%|█████     | 136/266 [00:32<00:33,  3.87it/s]Loading train:  52%|█████▏    | 137/266 [00:33<00:32,  4.00it/s]Loading train:  52%|█████▏    | 138/266 [00:33<00:31,  4.09it/s]Loading train:  52%|█████▏    | 139/266 [00:33<00:30,  4.15it/s]Loading train:  53%|█████▎    | 140/266 [00:33<00:29,  4.23it/s]Loading train:  53%|█████▎    | 141/266 [00:33<00:29,  4.24it/s]Loading train:  53%|█████▎    | 142/266 [00:34<00:29,  4.26it/s]Loading train:  54%|█████▍    | 143/266 [00:34<00:28,  4.29it/s]Loading train:  54%|█████▍    | 144/266 [00:34<00:28,  4.31it/s]Loading train:  55%|█████▍    | 145/266 [00:34<00:27,  4.33it/s]Loading train:  55%|█████▍    | 146/266 [00:35<00:27,  4.32it/s]Loading train:  55%|█████▌    | 147/266 [00:35<00:27,  4.29it/s]Loading train:  56%|█████▌    | 148/266 [00:35<00:27,  4.23it/s]Loading train:  56%|█████▌    | 149/266 [00:35<00:27,  4.22it/s]Loading train:  56%|█████▋    | 150/266 [00:36<00:27,  4.26it/s]Loading train:  57%|█████▋    | 151/266 [00:36<00:26,  4.30it/s]Loading train:  57%|█████▋    | 152/266 [00:36<00:26,  4.34it/s]Loading train:  58%|█████▊    | 153/266 [00:36<00:25,  4.35it/s]Loading train:  58%|█████▊    | 154/266 [00:36<00:25,  4.38it/s]Loading train:  58%|█████▊    | 155/266 [00:37<00:24,  4.61it/s]Loading train:  59%|█████▊    | 156/266 [00:37<00:22,  4.79it/s]Loading train:  59%|█████▉    | 157/266 [00:37<00:22,  4.89it/s]Loading train:  59%|█████▉    | 158/266 [00:37<00:21,  5.00it/s]Loading train:  60%|█████▉    | 159/266 [00:37<00:21,  5.07it/s]Loading train:  60%|██████    | 160/266 [00:38<00:20,  5.13it/s]Loading train:  61%|██████    | 161/266 [00:38<00:20,  5.17it/s]Loading train:  61%|██████    | 162/266 [00:38<00:20,  5.18it/s]Loading train:  61%|██████▏   | 163/266 [00:38<00:19,  5.18it/s]Loading train:  62%|██████▏   | 164/266 [00:38<00:19,  5.21it/s]Loading train:  62%|██████▏   | 165/266 [00:39<00:19,  5.23it/s]Loading train:  62%|██████▏   | 166/266 [00:39<00:19,  5.20it/s]Loading train:  63%|██████▎   | 167/266 [00:39<00:18,  5.23it/s]Loading train:  63%|██████▎   | 168/266 [00:39<00:18,  5.18it/s]Loading train:  64%|██████▎   | 169/266 [00:39<00:18,  5.18it/s]Loading train:  64%|██████▍   | 170/266 [00:40<00:18,  5.14it/s]Loading train:  64%|██████▍   | 171/266 [00:40<00:18,  5.16it/s]Loading train:  65%|██████▍   | 172/266 [00:40<00:18,  5.18it/s]Loading train:  65%|██████▌   | 173/266 [00:40<00:18,  5.03it/s]Loading train:  65%|██████▌   | 174/266 [00:40<00:18,  4.92it/s]Loading train:  66%|██████▌   | 175/266 [00:41<00:18,  4.83it/s]Loading train:  66%|██████▌   | 176/266 [00:41<00:18,  4.78it/s]Loading train:  67%|██████▋   | 177/266 [00:41<00:18,  4.76it/s]Loading train:  67%|██████▋   | 178/266 [00:41<00:25,  3.49it/s]Loading train:  67%|██████▋   | 179/266 [00:42<00:27,  3.22it/s]Loading train:  68%|██████▊   | 180/266 [00:42<00:29,  2.94it/s]Loading train:  68%|██████▊   | 181/266 [00:43<00:35,  2.40it/s]Loading train:  68%|██████▊   | 182/266 [00:43<00:38,  2.19it/s]Loading train:  69%|██████▉   | 183/266 [00:44<00:41,  2.02it/s]Loading train:  69%|██████▉   | 184/266 [00:44<00:41,  1.98it/s]Loading train:  70%|██████▉   | 185/266 [00:45<00:45,  1.79it/s]Loading train:  70%|██████▉   | 186/266 [00:46<00:45,  1.77it/s]Loading train:  70%|███████   | 187/266 [00:46<00:42,  1.84it/s]Loading train:  71%|███████   | 188/266 [00:47<00:36,  2.15it/s]Loading train:  71%|███████   | 189/266 [00:47<00:33,  2.27it/s]Loading train:  71%|███████▏  | 190/266 [00:47<00:32,  2.35it/s]Loading train:  72%|███████▏  | 191/266 [00:48<00:35,  2.13it/s]Loading train:  72%|███████▏  | 192/266 [00:48<00:34,  2.13it/s]Loading train:  73%|███████▎  | 193/266 [00:49<00:34,  2.13it/s]Loading train:  73%|███████▎  | 194/266 [00:49<00:31,  2.30it/s]Loading train:  73%|███████▎  | 195/266 [00:50<00:29,  2.42it/s]Loading train:  74%|███████▎  | 196/266 [00:50<00:26,  2.65it/s]Loading train:  74%|███████▍  | 197/266 [00:50<00:24,  2.77it/s]Loading train:  74%|███████▍  | 198/266 [00:50<00:23,  2.86it/s]Loading train:  75%|███████▍  | 199/266 [00:51<00:28,  2.32it/s]Loading train:  75%|███████▌  | 200/266 [00:52<00:30,  2.14it/s]Loading train:  76%|███████▌  | 201/266 [00:52<00:30,  2.16it/s]Loading train:  76%|███████▌  | 202/266 [00:52<00:26,  2.43it/s]Loading train:  76%|███████▋  | 203/266 [00:53<00:23,  2.65it/s]Loading train:  77%|███████▋  | 204/266 [00:53<00:22,  2.82it/s]Loading train:  77%|███████▋  | 205/266 [00:53<00:20,  2.97it/s]Loading train:  77%|███████▋  | 206/266 [00:54<00:19,  3.05it/s]Loading train:  78%|███████▊  | 207/266 [00:54<00:18,  3.11it/s]Loading train:  78%|███████▊  | 208/266 [00:54<00:18,  3.22it/s]Loading train:  79%|███████▊  | 209/266 [00:54<00:17,  3.19it/s]Loading train:  79%|███████▉  | 210/266 [00:55<00:17,  3.21it/s]Loading train:  79%|███████▉  | 211/266 [00:55<00:22,  2.43it/s]Loading train:  80%|███████▉  | 212/266 [00:56<00:26,  2.07it/s]Loading train:  80%|████████  | 213/266 [00:57<00:29,  1.78it/s]Loading train:  80%|████████  | 214/266 [00:58<00:32,  1.61it/s]Loading train:  81%|████████  | 215/266 [00:58<00:32,  1.55it/s]Loading train:  81%|████████  | 216/266 [00:59<00:29,  1.67it/s]Loading train:  82%|████████▏ | 217/266 [00:59<00:29,  1.64it/s]Loading train:  82%|████████▏ | 218/266 [01:00<00:29,  1.64it/s]Loading train:  82%|████████▏ | 219/266 [01:01<00:28,  1.63it/s]Loading train:  83%|████████▎ | 220/266 [01:01<00:27,  1.69it/s]Loading train:  83%|████████▎ | 221/266 [01:02<00:26,  1.72it/s]Loading train:  83%|████████▎ | 222/266 [01:02<00:24,  1.77it/s]Loading train:  84%|████████▍ | 223/266 [01:03<00:25,  1.70it/s]Loading train:  84%|████████▍ | 224/266 [01:04<00:25,  1.62it/s]Loading train:  85%|████████▍ | 225/266 [01:04<00:25,  1.59it/s]Loading train:  85%|████████▍ | 226/266 [01:05<00:24,  1.61it/s]Loading train:  85%|████████▌ | 227/266 [01:05<00:24,  1.59it/s]Loading train:  86%|████████▌ | 228/266 [01:06<00:24,  1.53it/s]Loading train:  86%|████████▌ | 229/266 [01:07<00:23,  1.56it/s]Loading train:  86%|████████▋ | 230/266 [01:07<00:21,  1.65it/s]Loading train:  87%|████████▋ | 231/266 [01:08<00:21,  1.60it/s]Loading train:  87%|████████▋ | 232/266 [01:09<00:21,  1.57it/s]Loading train:  88%|████████▊ | 233/266 [01:09<00:19,  1.68it/s]Loading train:  88%|████████▊ | 234/266 [01:10<00:19,  1.65it/s]Loading train:  88%|████████▊ | 235/266 [01:10<00:18,  1.64it/s]Loading train:  89%|████████▊ | 236/266 [01:11<00:18,  1.58it/s]Loading train:  89%|████████▉ | 237/266 [01:12<00:17,  1.62it/s]Loading train:  89%|████████▉ | 238/266 [01:12<00:16,  1.66it/s]Loading train:  90%|████████▉ | 239/266 [01:13<00:16,  1.66it/s]Loading train:  90%|█████████ | 240/266 [01:13<00:15,  1.67it/s]Loading train:  91%|█████████ | 241/266 [01:14<00:15,  1.65it/s]Loading train:  91%|█████████ | 242/266 [01:15<00:14,  1.63it/s]Loading train:  91%|█████████▏| 243/266 [01:15<00:13,  1.67it/s]Loading train:  92%|█████████▏| 244/266 [01:16<00:13,  1.67it/s]Loading train:  92%|█████████▏| 245/266 [01:16<00:12,  1.67it/s]Loading train:  92%|█████████▏| 246/266 [01:17<00:11,  1.78it/s]Loading train:  93%|█████████▎| 247/266 [01:18<00:11,  1.72it/s]Loading train:  93%|█████████▎| 248/266 [01:18<00:11,  1.63it/s]Loading train:  94%|█████████▎| 249/266 [01:19<00:10,  1.55it/s]Loading train:  94%|█████████▍| 250/266 [01:20<00:10,  1.55it/s]Loading train:  94%|█████████▍| 251/266 [01:20<00:09,  1.58it/s]Loading train:  95%|█████████▍| 252/266 [01:21<00:09,  1.51it/s]Loading train:  95%|█████████▌| 253/266 [01:22<00:08,  1.49it/s]Loading train:  95%|█████████▌| 254/266 [01:22<00:08,  1.50it/s]Loading train:  96%|█████████▌| 255/266 [01:23<00:07,  1.56it/s]Loading train:  96%|█████████▌| 256/266 [01:23<00:06,  1.60it/s]Loading train:  97%|█████████▋| 257/266 [01:24<00:05,  1.64it/s]Loading train:  97%|█████████▋| 258/266 [01:25<00:04,  1.68it/s]Loading train:  97%|█████████▋| 259/266 [01:25<00:04,  1.62it/s]Loading train:  98%|█████████▊| 260/266 [01:26<00:03,  1.64it/s]Loading train:  98%|█████████▊| 261/266 [01:26<00:03,  1.66it/s]Loading train:  98%|█████████▊| 262/266 [01:27<00:02,  1.59it/s]Loading train:  99%|█████████▉| 263/266 [01:28<00:01,  1.59it/s]Loading train:  99%|█████████▉| 264/266 [01:28<00:01,  1.66it/s]Loading train: 100%|█████████▉| 265/266 [01:29<00:00,  1.61it/s]Loading train: 100%|██████████| 266/266 [01:30<00:00,  1.63it/s]Loading train: 100%|██████████| 266/266 [01:30<00:00,  2.95it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:05, 47.99it/s]concatenating: train:   4%|▍         | 10/266 [00:00<00:05, 47.16it/s]concatenating: train:   6%|▌         | 15/266 [00:00<00:05, 45.67it/s]concatenating: train:   8%|▊         | 20/266 [00:00<00:05, 45.02it/s]concatenating: train:   9%|▉         | 25/266 [00:00<00:05, 44.91it/s]concatenating: train:  11%|█▏        | 30/266 [00:00<00:05, 45.10it/s]concatenating: train:  13%|█▎        | 35/266 [00:00<00:05, 44.93it/s]concatenating: train:  15%|█▌        | 40/266 [00:00<00:04, 45.81it/s]concatenating: train:  18%|█▊        | 47/266 [00:00<00:04, 49.66it/s]concatenating: train:  20%|█▉        | 53/266 [00:01<00:04, 52.22it/s]concatenating: train:  23%|██▎       | 60/266 [00:01<00:03, 54.62it/s]concatenating: train:  25%|██▌       | 67/266 [00:01<00:03, 56.74it/s]concatenating: train:  28%|██▊       | 74/266 [00:01<00:03, 58.31it/s]concatenating: train:  30%|███       | 80/266 [00:01<00:03, 58.30it/s]concatenating: train:  32%|███▏      | 86/266 [00:01<00:03, 57.42it/s]concatenating: train:  35%|███▍      | 92/266 [00:01<00:03, 57.48it/s]concatenating: train:  37%|███▋      | 98/266 [00:01<00:02, 57.46it/s]concatenating: train:  39%|███▉      | 105/266 [00:01<00:02, 59.49it/s]concatenating: train:  42%|████▏     | 112/266 [00:02<00:02, 60.65it/s]concatenating: train:  45%|████▍     | 119/266 [00:02<00:02, 60.89it/s]concatenating: train:  47%|████▋     | 126/266 [00:02<00:02, 57.32it/s]concatenating: train:  50%|████▉     | 132/266 [00:02<00:02, 56.69it/s]concatenating: train:  52%|█████▏    | 138/266 [00:02<00:02, 56.81it/s]concatenating: train:  55%|█████▍    | 145/266 [00:02<00:02, 58.02it/s]concatenating: train:  57%|█████▋    | 151/266 [00:02<00:02, 57.24it/s]concatenating: train:  59%|█████▉    | 158/266 [00:02<00:01, 59.77it/s]concatenating: train:  62%|██████▏   | 166/266 [00:02<00:01, 62.63it/s]concatenating: train:  65%|██████▌   | 173/266 [00:03<00:01, 64.66it/s]concatenating: train:  68%|██████▊   | 180/266 [00:03<00:01, 63.30it/s]concatenating: train:  70%|███████   | 187/266 [00:03<00:01, 59.55it/s]concatenating: train:  73%|███████▎  | 194/266 [00:03<00:01, 58.43it/s]concatenating: train:  75%|███████▌  | 200/266 [00:03<00:01, 55.97it/s]concatenating: train:  78%|███████▊  | 207/266 [00:03<00:01, 57.84it/s]concatenating: train:  80%|████████  | 213/266 [00:03<00:00, 57.82it/s]concatenating: train:  82%|████████▏ | 219/266 [00:03<00:00, 52.10it/s]concatenating: train:  85%|████████▍ | 225/266 [00:04<00:00, 49.29it/s]concatenating: train:  87%|████████▋ | 231/266 [00:04<00:00, 46.54it/s]concatenating: train:  89%|████████▊ | 236/266 [00:04<00:00, 42.45it/s]concatenating: train:  91%|█████████ | 241/266 [00:04<00:00, 44.01it/s]concatenating: train:  92%|█████████▏| 246/266 [00:04<00:00, 45.27it/s]concatenating: train:  94%|█████████▍| 251/266 [00:04<00:00, 45.70it/s]concatenating: train:  96%|█████████▌| 256/266 [00:04<00:00, 44.93it/s]concatenating: train:  98%|█████████▊| 261/266 [00:04<00:00, 44.93it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 44.98it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 53.09it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:01,  1.60it/s]Loading test:  50%|█████     | 2/4 [00:01<00:01,  1.67it/s]Loading test:  75%|███████▌  | 3/4 [00:01<00:00,  1.64it/s]Loading test: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s]Loading test: 100%|██████████| 4/4 [00:02<00:00,  1.75it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 61.47it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:00<03:25,  1.29it/s]Loading trainS:   1%|          | 2/266 [00:01<03:18,  1.33it/s]Loading trainS:   1%|          | 3/266 [00:02<03:02,  1.44it/s]Loading trainS:   2%|▏         | 4/266 [00:02<02:52,  1.52it/s]Loading trainS:   2%|▏         | 5/266 [00:03<02:48,  1.55it/s]Loading trainS:   2%|▏         | 6/266 [00:03<02:42,  1.60it/s]Loading trainS:   3%|▎         | 7/266 [00:04<02:40,  1.61it/s]Loading trainS:   3%|▎         | 8/266 [00:05<02:42,  1.59it/s]Loading trainS:   3%|▎         | 9/266 [00:05<02:41,  1.59it/s]Loading trainS:   4%|▍         | 10/266 [00:06<02:41,  1.58it/s]Loading trainS:   4%|▍         | 11/266 [00:07<02:47,  1.52it/s]Loading trainS:   5%|▍         | 12/266 [00:07<02:44,  1.54it/s]Loading trainS:   5%|▍         | 13/266 [00:08<02:44,  1.53it/s]Loading trainS:   5%|▌         | 14/266 [00:08<02:44,  1.53it/s]Loading trainS:   6%|▌         | 15/266 [00:09<02:45,  1.52it/s]Loading trainS:   6%|▌         | 16/266 [00:10<02:45,  1.51it/s]Loading trainS:   6%|▋         | 17/266 [00:10<02:42,  1.53it/s]Loading trainS:   7%|▋         | 18/266 [00:11<02:45,  1.50it/s]Loading trainS:   7%|▋         | 19/266 [00:12<02:33,  1.61it/s]Loading trainS:   8%|▊         | 20/266 [00:12<02:36,  1.57it/s]Loading trainS:   8%|▊         | 21/266 [00:13<02:42,  1.51it/s]Loading trainS:   8%|▊         | 22/266 [00:14<02:43,  1.49it/s]Loading trainS:   9%|▊         | 23/266 [00:14<02:37,  1.55it/s]Loading trainS:   9%|▉         | 24/266 [00:15<02:33,  1.58it/s]Loading trainS:   9%|▉         | 25/266 [00:16<02:35,  1.55it/s]Loading trainS:  10%|▉         | 26/266 [00:16<02:33,  1.57it/s]Loading trainS:  10%|█         | 27/266 [00:17<02:36,  1.53it/s]Loading trainS:  11%|█         | 28/266 [00:18<02:31,  1.57it/s]Loading trainS:  11%|█         | 29/266 [00:18<02:28,  1.59it/s]Loading trainS:  11%|█▏        | 30/266 [00:19<02:23,  1.65it/s]Loading trainS:  12%|█▏        | 31/266 [00:19<02:28,  1.58it/s]Loading trainS:  12%|█▏        | 32/266 [00:20<02:30,  1.55it/s]Loading trainS:  12%|█▏        | 33/266 [00:21<02:30,  1.55it/s]Loading trainS:  13%|█▎        | 34/266 [00:21<02:30,  1.54it/s]Loading trainS:  13%|█▎        | 35/266 [00:22<02:29,  1.55it/s]Loading trainS:  14%|█▎        | 36/266 [00:23<02:29,  1.54it/s]Loading trainS:  14%|█▍        | 37/266 [00:23<02:22,  1.60it/s]Loading trainS:  14%|█▍        | 38/266 [00:24<02:24,  1.58it/s]Loading trainS:  15%|█▍        | 39/266 [00:25<02:25,  1.56it/s]Loading trainS:  15%|█▌        | 40/266 [00:25<02:26,  1.55it/s]Loading trainS:  15%|█▌        | 41/266 [00:26<02:25,  1.55it/s]Loading trainS:  16%|█▌        | 42/266 [00:26<02:18,  1.62it/s]Loading trainS:  16%|█▌        | 43/266 [00:27<02:11,  1.70it/s]Loading trainS:  17%|█▋        | 44/266 [00:27<02:06,  1.75it/s]Loading trainS:  17%|█▋        | 45/266 [00:28<02:08,  1.73it/s]Loading trainS:  17%|█▋        | 46/266 [00:29<02:02,  1.79it/s]Loading trainS:  18%|█▊        | 47/266 [00:29<01:59,  1.83it/s]Loading trainS:  18%|█▊        | 48/266 [00:30<01:56,  1.88it/s]Loading trainS:  18%|█▊        | 49/266 [00:30<01:55,  1.87it/s]Loading trainS:  19%|█▉        | 50/266 [00:31<01:56,  1.86it/s]Loading trainS:  19%|█▉        | 51/266 [00:31<01:49,  1.97it/s]Loading trainS:  20%|█▉        | 52/266 [00:32<01:52,  1.91it/s]Loading trainS:  20%|█▉        | 53/266 [00:32<01:54,  1.87it/s]Loading trainS:  20%|██        | 54/266 [00:33<01:54,  1.86it/s]Loading trainS:  21%|██        | 55/266 [00:33<01:52,  1.87it/s]Loading trainS:  21%|██        | 56/266 [00:34<01:57,  1.79it/s]Loading trainS:  21%|██▏       | 57/266 [00:34<01:58,  1.76it/s]Loading trainS:  22%|██▏       | 58/266 [00:35<01:55,  1.80it/s]Loading trainS:  22%|██▏       | 59/266 [00:36<01:50,  1.87it/s]Loading trainS:  23%|██▎       | 60/266 [00:36<01:48,  1.90it/s]Loading trainS:  23%|██▎       | 61/266 [00:37<01:49,  1.87it/s]Loading trainS:  23%|██▎       | 62/266 [00:37<01:47,  1.90it/s]Loading trainS:  24%|██▎       | 63/266 [00:38<01:51,  1.82it/s]Loading trainS:  24%|██▍       | 64/266 [00:38<01:54,  1.76it/s]Loading trainS:  24%|██▍       | 65/266 [00:39<01:51,  1.81it/s]Loading trainS:  25%|██▍       | 66/266 [00:39<01:49,  1.82it/s]Loading trainS:  25%|██▌       | 67/266 [00:40<01:47,  1.84it/s]Loading trainS:  26%|██▌       | 68/266 [00:40<01:42,  1.93it/s]Loading trainS:  26%|██▌       | 69/266 [00:41<01:49,  1.80it/s]Loading trainS:  26%|██▋       | 70/266 [00:42<01:50,  1.78it/s]Loading trainS:  27%|██▋       | 71/266 [00:42<01:50,  1.77it/s]Loading trainS:  27%|██▋       | 72/266 [00:43<01:47,  1.80it/s]Loading trainS:  27%|██▋       | 73/266 [00:43<01:47,  1.80it/s]Loading trainS:  28%|██▊       | 74/266 [00:44<01:47,  1.79it/s]Loading trainS:  28%|██▊       | 75/266 [00:44<01:50,  1.73it/s]Loading trainS:  29%|██▊       | 76/266 [00:45<01:45,  1.80it/s]Loading trainS:  29%|██▉       | 77/266 [00:45<01:46,  1.77it/s]Loading trainS:  29%|██▉       | 78/266 [00:46<01:45,  1.79it/s]Loading trainS:  30%|██▉       | 79/266 [00:47<01:44,  1.80it/s]Loading trainS:  30%|███       | 80/266 [00:47<01:43,  1.80it/s]Loading trainS:  30%|███       | 81/266 [00:48<01:42,  1.81it/s]Loading trainS:  31%|███       | 82/266 [00:48<01:44,  1.76it/s]Loading trainS:  31%|███       | 83/266 [00:49<01:45,  1.74it/s]Loading trainS:  32%|███▏      | 84/266 [00:49<01:46,  1.71it/s]Loading trainS:  32%|███▏      | 85/266 [00:50<01:47,  1.68it/s]Loading trainS:  32%|███▏      | 86/266 [00:51<01:45,  1.70it/s]Loading trainS:  33%|███▎      | 87/266 [00:51<01:52,  1.60it/s]Loading trainS:  33%|███▎      | 88/266 [00:52<01:50,  1.62it/s]Loading trainS:  33%|███▎      | 89/266 [00:53<01:44,  1.69it/s]Loading trainS:  34%|███▍      | 90/266 [00:53<01:43,  1.70it/s]Loading trainS:  34%|███▍      | 91/266 [00:54<01:46,  1.64it/s]Loading trainS:  35%|███▍      | 92/266 [00:54<01:48,  1.60it/s]Loading trainS:  35%|███▍      | 93/266 [00:55<01:50,  1.57it/s]Loading trainS:  35%|███▌      | 94/266 [00:56<01:50,  1.56it/s]Loading trainS:  36%|███▌      | 95/266 [00:56<01:50,  1.55it/s]Loading trainS:  36%|███▌      | 96/266 [00:57<01:45,  1.61it/s]Loading trainS:  36%|███▋      | 97/266 [00:58<01:47,  1.57it/s]Loading trainS:  37%|███▋      | 98/266 [00:58<01:42,  1.64it/s]Loading trainS:  37%|███▋      | 99/266 [00:59<01:29,  1.86it/s]Loading trainS:  38%|███▊      | 100/266 [00:59<01:34,  1.76it/s]Loading trainS:  38%|███▊      | 101/266 [01:00<01:29,  1.84it/s]Loading trainS:  38%|███▊      | 102/266 [01:00<01:28,  1.84it/s]Loading trainS:  39%|███▊      | 103/266 [01:01<01:32,  1.76it/s]Loading trainS:  39%|███▉      | 104/266 [01:01<01:30,  1.79it/s]Loading trainS:  39%|███▉      | 105/266 [01:02<01:29,  1.79it/s]Loading trainS:  40%|███▉      | 106/266 [01:02<01:26,  1.84it/s]Loading trainS:  40%|████      | 107/266 [01:03<01:24,  1.88it/s]Loading trainS:  41%|████      | 108/266 [01:03<01:22,  1.92it/s]Loading trainS:  41%|████      | 109/266 [01:04<01:24,  1.87it/s]Loading trainS:  41%|████▏     | 110/266 [01:05<01:22,  1.89it/s]Loading trainS:  42%|████▏     | 111/266 [01:05<01:21,  1.91it/s]Loading trainS:  42%|████▏     | 112/266 [01:06<01:23,  1.84it/s]Loading trainS:  42%|████▏     | 113/266 [01:06<01:20,  1.90it/s]Loading trainS:  43%|████▎     | 114/266 [01:07<01:21,  1.86it/s]Loading trainS:  43%|████▎     | 115/266 [01:07<01:22,  1.84it/s]Loading trainS:  44%|████▎     | 116/266 [01:08<01:20,  1.86it/s]Loading trainS:  44%|████▍     | 117/266 [01:08<01:21,  1.83it/s]Loading trainS:  44%|████▍     | 118/266 [01:09<01:18,  1.88it/s]Loading trainS:  45%|████▍     | 119/266 [01:09<01:23,  1.75it/s]Loading trainS:  45%|████▌     | 120/266 [01:10<01:26,  1.68it/s]Loading trainS:  45%|████▌     | 121/266 [01:11<01:31,  1.58it/s]Loading trainS:  46%|████▌     | 122/266 [01:11<01:31,  1.58it/s]Loading trainS:  46%|████▌     | 123/266 [01:12<01:34,  1.52it/s]Loading trainS:  47%|████▋     | 124/266 [01:13<01:35,  1.49it/s]Loading trainS:  47%|████▋     | 125/266 [01:14<01:34,  1.50it/s]Loading trainS:  47%|████▋     | 126/266 [01:14<01:34,  1.49it/s]Loading trainS:  48%|████▊     | 127/266 [01:15<01:33,  1.49it/s]Loading trainS:  48%|████▊     | 128/266 [01:16<01:32,  1.49it/s]Loading trainS:  48%|████▊     | 129/266 [01:16<01:31,  1.50it/s]Loading trainS:  49%|████▉     | 130/266 [01:17<01:29,  1.52it/s]Loading trainS:  49%|████▉     | 131/266 [01:17<01:26,  1.56it/s]Loading trainS:  50%|████▉     | 132/266 [01:18<01:28,  1.52it/s]Loading trainS:  50%|█████     | 133/266 [01:19<01:25,  1.55it/s]Loading trainS:  50%|█████     | 134/266 [01:19<01:26,  1.52it/s]Loading trainS:  51%|█████     | 135/266 [01:20<01:26,  1.52it/s]Loading trainS:  51%|█████     | 136/266 [01:21<01:22,  1.57it/s]Loading trainS:  52%|█████▏    | 137/266 [01:21<01:17,  1.67it/s]Loading trainS:  52%|█████▏    | 138/266 [01:22<01:14,  1.73it/s]Loading trainS:  52%|█████▏    | 139/266 [01:22<01:15,  1.68it/s]Loading trainS:  53%|█████▎    | 140/266 [01:23<01:13,  1.72it/s]Loading trainS:  53%|█████▎    | 141/266 [01:23<01:09,  1.79it/s]Loading trainS:  53%|█████▎    | 142/266 [01:24<01:10,  1.75it/s]Loading trainS:  54%|█████▍    | 143/266 [01:25<01:08,  1.79it/s]Loading trainS:  54%|█████▍    | 144/266 [01:25<01:08,  1.77it/s]Loading trainS:  55%|█████▍    | 145/266 [01:26<01:09,  1.75it/s]Loading trainS:  55%|█████▍    | 146/266 [01:26<01:10,  1.70it/s]Loading trainS:  55%|█████▌    | 147/266 [01:27<01:07,  1.75it/s]Loading trainS:  56%|█████▌    | 148/266 [01:27<01:04,  1.83it/s]Loading trainS:  56%|█████▌    | 149/266 [01:28<01:02,  1.86it/s]Loading trainS:  56%|█████▋    | 150/266 [01:28<01:02,  1.87it/s]Loading trainS:  57%|█████▋    | 151/266 [01:29<01:02,  1.84it/s]Loading trainS:  57%|█████▋    | 152/266 [01:30<01:02,  1.83it/s]Loading trainS:  58%|█████▊    | 153/266 [01:30<01:00,  1.88it/s]Loading trainS:  58%|█████▊    | 154/266 [01:31<01:00,  1.85it/s]Loading trainS:  58%|█████▊    | 155/266 [01:31<00:56,  1.96it/s]Loading trainS:  59%|█████▊    | 156/266 [01:32<00:54,  2.02it/s]Loading trainS:  59%|█████▉    | 157/266 [01:32<00:52,  2.06it/s]Loading trainS:  59%|█████▉    | 158/266 [01:32<00:51,  2.09it/s]Loading trainS:  60%|█████▉    | 159/266 [01:33<00:49,  2.16it/s]Loading trainS:  60%|██████    | 160/266 [01:33<00:52,  2.01it/s]Loading trainS:  61%|██████    | 161/266 [01:34<00:50,  2.08it/s]Loading trainS:  61%|██████    | 162/266 [01:34<00:50,  2.05it/s]Loading trainS:  61%|██████▏   | 163/266 [01:35<00:49,  2.08it/s]Loading trainS:  62%|██████▏   | 164/266 [01:35<00:47,  2.13it/s]Loading trainS:  62%|██████▏   | 165/266 [01:36<00:47,  2.14it/s]Loading trainS:  62%|██████▏   | 166/266 [01:36<00:46,  2.14it/s]Loading trainS:  63%|██████▎   | 167/266 [01:37<00:46,  2.13it/s]Loading trainS:  63%|██████▎   | 168/266 [01:37<00:46,  2.10it/s]Loading trainS:  64%|██████▎   | 169/266 [01:38<00:43,  2.25it/s]Loading trainS:  64%|██████▍   | 170/266 [01:38<00:46,  2.07it/s]Loading trainS:  64%|██████▍   | 171/266 [01:39<00:44,  2.16it/s]Loading trainS:  65%|██████▍   | 172/266 [01:39<00:43,  2.16it/s]Loading trainS:  65%|██████▌   | 173/266 [01:40<00:44,  2.08it/s]Loading trainS:  65%|██████▌   | 174/266 [01:40<00:47,  1.96it/s]Loading trainS:  66%|██████▌   | 175/266 [01:41<00:43,  2.09it/s]Loading trainS:  66%|██████▌   | 176/266 [01:41<00:46,  1.92it/s]Loading trainS:  67%|██████▋   | 177/266 [01:42<00:48,  1.84it/s]Loading trainS:  67%|██████▋   | 178/266 [01:42<00:47,  1.85it/s]Loading trainS:  67%|██████▋   | 179/266 [01:43<00:48,  1.80it/s]Loading trainS:  68%|██████▊   | 180/266 [01:43<00:48,  1.78it/s]Loading trainS:  68%|██████▊   | 181/266 [01:44<00:45,  1.85it/s]Loading trainS:  68%|██████▊   | 182/266 [01:44<00:45,  1.84it/s]Loading trainS:  69%|██████▉   | 183/266 [01:45<00:44,  1.85it/s]Loading trainS:  69%|██████▉   | 184/266 [01:46<00:43,  1.90it/s]Loading trainS:  70%|██████▉   | 185/266 [01:46<00:43,  1.85it/s]Loading trainS:  70%|██████▉   | 186/266 [01:47<00:43,  1.83it/s]Loading trainS:  70%|███████   | 187/266 [01:47<00:45,  1.75it/s]Loading trainS:  71%|███████   | 188/266 [01:48<00:41,  1.88it/s]Loading trainS:  71%|███████   | 189/266 [01:48<00:39,  1.94it/s]Loading trainS:  71%|███████▏  | 190/266 [01:49<00:38,  1.98it/s]Loading trainS:  72%|███████▏  | 191/266 [01:49<00:38,  1.93it/s]Loading trainS:  72%|███████▏  | 192/266 [01:50<00:39,  1.85it/s]Loading trainS:  73%|███████▎  | 193/266 [01:50<00:38,  1.91it/s]Loading trainS:  73%|███████▎  | 194/266 [01:51<00:38,  1.87it/s]Loading trainS:  73%|███████▎  | 195/266 [01:51<00:38,  1.84it/s]Loading trainS:  74%|███████▎  | 196/266 [01:52<00:38,  1.84it/s]Loading trainS:  74%|███████▍  | 197/266 [01:53<00:38,  1.79it/s]Loading trainS:  74%|███████▍  | 198/266 [01:53<00:38,  1.78it/s]Loading trainS:  75%|███████▍  | 199/266 [01:54<00:36,  1.84it/s]Loading trainS:  75%|███████▌  | 200/266 [01:54<00:35,  1.84it/s]Loading trainS:  76%|███████▌  | 201/266 [01:55<00:33,  1.92it/s]Loading trainS:  76%|███████▌  | 202/266 [01:55<00:33,  1.94it/s]Loading trainS:  76%|███████▋  | 203/266 [01:56<00:30,  2.04it/s]Loading trainS:  77%|███████▋  | 204/266 [01:56<00:30,  2.07it/s]Loading trainS:  77%|███████▋  | 205/266 [01:57<00:30,  2.02it/s]Loading trainS:  77%|███████▋  | 206/266 [01:57<00:31,  1.93it/s]Loading trainS:  78%|███████▊  | 207/266 [01:58<00:31,  1.86it/s]Loading trainS:  78%|███████▊  | 208/266 [01:58<00:31,  1.82it/s]Loading trainS:  79%|███████▊  | 209/266 [01:59<00:32,  1.78it/s]Loading trainS:  79%|███████▉  | 210/266 [01:59<00:31,  1.81it/s]Loading trainS:  79%|███████▉  | 211/266 [02:00<00:31,  1.77it/s]Loading trainS:  80%|███████▉  | 212/266 [02:01<00:29,  1.82it/s]Loading trainS:  80%|████████  | 213/266 [02:01<00:27,  1.91it/s]Loading trainS:  80%|████████  | 214/266 [02:02<00:27,  1.90it/s]Loading trainS:  81%|████████  | 215/266 [02:02<00:26,  1.90it/s]Loading trainS:  81%|████████  | 216/266 [02:03<00:27,  1.83it/s]Loading trainS:  82%|████████▏ | 217/266 [02:03<00:27,  1.81it/s]Loading trainS:  82%|████████▏ | 218/266 [02:04<00:26,  1.81it/s]Loading trainS:  82%|████████▏ | 219/266 [02:04<00:26,  1.75it/s]Loading trainS:  83%|████████▎ | 220/266 [02:05<00:25,  1.82it/s]Loading trainS:  83%|████████▎ | 221/266 [02:06<00:26,  1.73it/s]Loading trainS:  83%|████████▎ | 222/266 [02:06<00:26,  1.65it/s]Loading trainS:  84%|████████▍ | 223/266 [02:07<00:25,  1.71it/s]Loading trainS:  84%|████████▍ | 224/266 [02:07<00:23,  1.77it/s]Loading trainS:  85%|████████▍ | 225/266 [02:08<00:22,  1.85it/s]Loading trainS:  85%|████████▍ | 226/266 [02:08<00:21,  1.88it/s]Loading trainS:  85%|████████▌ | 227/266 [02:09<00:19,  2.02it/s]Loading trainS:  86%|████████▌ | 228/266 [02:09<00:16,  2.30it/s]Loading trainS:  86%|████████▌ | 229/266 [02:09<00:14,  2.57it/s]Loading trainS:  86%|████████▋ | 230/266 [02:09<00:12,  2.87it/s]Loading trainS:  87%|████████▋ | 231/266 [02:10<00:12,  2.74it/s]Loading trainS:  87%|████████▋ | 232/266 [02:10<00:14,  2.40it/s]Loading trainS:  88%|████████▊ | 233/266 [02:11<00:14,  2.27it/s]Loading trainS:  88%|████████▊ | 234/266 [02:11<00:15,  2.12it/s]Loading trainS:  88%|████████▊ | 235/266 [02:12<00:15,  2.07it/s]Loading trainS:  89%|████████▊ | 236/266 [02:12<00:14,  2.06it/s]Loading trainS:  89%|████████▉ | 237/266 [02:13<00:14,  2.05it/s]Loading trainS:  89%|████████▉ | 238/266 [02:13<00:13,  2.09it/s]Loading trainS:  90%|████████▉ | 239/266 [02:14<00:12,  2.12it/s]Loading trainS:  90%|█████████ | 240/266 [02:14<00:12,  2.13it/s]Loading trainS:  91%|█████████ | 241/266 [02:15<00:11,  2.13it/s]Loading trainS:  91%|█████████ | 242/266 [02:15<00:11,  2.07it/s]Loading trainS:  91%|█████████▏| 243/266 [02:16<00:10,  2.15it/s]Loading trainS:  92%|█████████▏| 244/266 [02:16<00:09,  2.29it/s]Loading trainS:  92%|█████████▏| 245/266 [02:16<00:08,  2.47it/s]Loading trainS:  92%|█████████▏| 246/266 [02:17<00:07,  2.53it/s]Loading trainS:  93%|█████████▎| 247/266 [02:17<00:07,  2.41it/s]Loading trainS:  93%|█████████▎| 248/266 [02:18<00:07,  2.34it/s]Loading trainS:  94%|█████████▎| 249/266 [02:18<00:07,  2.27it/s]Loading trainS:  94%|█████████▍| 250/266 [02:19<00:07,  2.24it/s]Loading trainS:  94%|█████████▍| 251/266 [02:19<00:06,  2.18it/s]Loading trainS:  95%|█████████▍| 252/266 [02:20<00:06,  2.18it/s]Loading trainS:  95%|█████████▌| 253/266 [02:20<00:06,  2.05it/s]Loading trainS:  95%|█████████▌| 254/266 [02:21<00:06,  1.76it/s]Loading trainS:  96%|█████████▌| 255/266 [02:21<00:05,  2.02it/s]Loading trainS:  96%|█████████▌| 256/266 [02:22<00:05,  1.98it/s]Loading trainS:  97%|█████████▋| 257/266 [02:22<00:04,  1.89it/s]Loading trainS:  97%|█████████▋| 258/266 [02:23<00:04,  1.69it/s]Loading trainS:  97%|█████████▋| 259/266 [02:24<00:03,  1.88it/s]Loading trainS:  98%|█████████▊| 260/266 [02:24<00:03,  1.88it/s]Loading trainS:  98%|█████████▊| 261/266 [02:25<00:02,  1.95it/s]Loading trainS:  98%|█████████▊| 262/266 [02:25<00:01,  2.06it/s]Loading trainS:  99%|█████████▉| 263/266 [02:25<00:01,  2.18it/s]Loading trainS:  99%|█████████▉| 264/266 [02:26<00:00,  2.11it/s]Loading trainS: 100%|█████████▉| 265/266 [02:26<00:00,  2.16it/s]Loading trainS: 100%|██████████| 266/266 [02:27<00:00,  2.01it/s]Loading trainS: 100%|██████████| 266/266 [02:27<00:00,  1.81it/s]
Loading testS:   0%|          | 0/4 [00:00<?, ?it/s]Loading testS:  25%|██▌       | 1/4 [00:00<00:01,  1.83it/s]Loading testS:  50%|█████     | 2/4 [00:00<00:01,  1.95it/s]Loading testS:  75%|███████▌  | 3/4 [00:01<00:00,  2.04it/s]Loading testS: 100%|██████████| 4/4 [00:02<00:00,  1.88it/s]Loading testS: 100%|██████████| 4/4 [00:02<00:00,  1.96it/s]----------+++ 
CrossVal ['a']
TypeExperiment 8
CrossVal ['a']
(0/4) test vimp2_A_CSFn2
(1/4) test vimp2_ANON967_CSFn2
(2/4) test vimp2_B_CSFn2
(3/4) test vimp2_E_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 256 samples
Epoch 1/300
 - 83s - loss: 0.0976 - acc: 0.9895 - mDice: 0.8103 - val_loss: 0.1122 - val_acc: 0.9923 - val_mDice: 0.4377

Epoch 00001: val_mDice improved from -inf to 0.43775, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 77s - loss: 0.0635 - acc: 0.9932 - mDice: 0.8766 - val_loss: 0.1277 - val_acc: 0.9923 - val_mDice: 0.4716

Epoch 00002: val_mDice improved from 0.43775 to 0.47155, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 77s - loss: 0.0552 - acc: 0.9940 - mDice: 0.8927 - val_loss: 0.1812 - val_acc: 0.9916 - val_mDice: 0.4707

Epoch 00003: val_mDice did not improve from 0.47155
Epoch 4/300
 - 78s - loss: 0.0509 - acc: 0.9944 - mDice: 0.9010 - val_loss: 0.1294 - val_acc: 0.9932 - val_mDice: 0.5184

Epoch 00004: val_mDice improved from 0.47155 to 0.51840, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300
 - 79s - loss: 0.0462 - acc: 0.9948 - mDice: 0.9102 - val_loss: 0.1778 - val_acc: 0.9922 - val_mDice: 0.4926

Epoch 00005: val_mDice did not improve from 0.51840
Epoch 6/300
 - 80s - loss: 0.0452 - acc: 0.9949 - mDice: 0.9122 - val_loss: 0.0343 - val_acc: 0.9936 - val_mDice: 0.5038

Epoch 00006: val_mDice did not improve from 0.51840
Epoch 7/300
 - 80s - loss: 0.0437 - acc: 0.9950 - mDice: 0.9151 - val_loss: 0.1316 - val_acc: 0.9915 - val_mDice: 0.4887

Epoch 00007: val_mDice did not improve from 0.51840
Epoch 8/300
 - 79s - loss: 0.0421 - acc: 0.9952 - mDice: 0.9182 - val_loss: 2.5163e-04 - val_acc: 0.9941 - val_mDice: 0.5117

Epoch 00008: val_mDice did not improve from 0.51840
Epoch 9/300
 - 80s - loss: 0.0398 - acc: 0.9954 - mDice: 0.9228 - val_loss: 0.1134 - val_acc: 0.9937 - val_mDice: 0.4954

Epoch 00009: val_mDice did not improve from 0.51840
Epoch 10/300
 - 80s - loss: 0.0403 - acc: 0.9954 - mDice: 0.9217 - val_loss: -4.5842e-02 - val_acc: 0.9943 - val_mDice: 0.5011

Epoch 00010: val_mDice did not improve from 0.51840
Epoch 11/300
 - 80s - loss: 0.0385 - acc: 0.9955 - mDice: 0.9253 - val_loss: -1.0834e-02 - val_acc: 0.9941 - val_mDice: 0.5090

Epoch 00011: val_mDice did not improve from 0.51840
Epoch 12/300
 - 79s - loss: 0.0374 - acc: 0.9956 - mDice: 0.9274 - val_loss: -4.2839e-02 - val_acc: 0.9941 - val_mDice: 0.4956

Epoch 00012: val_mDice did not improve from 0.51840
Epoch 13/300
 - 79s - loss: 0.0377 - acc: 0.9956 - mDice: 0.9267 - val_loss: 0.0450 - val_acc: 0.9940 - val_mDice: 0.4988

Epoch 00013: val_mDice did not improve from 0.51840
Epoch 14/300
 - 79s - loss: 0.0360 - acc: 0.9957 - mDice: 0.9301 - val_loss: 0.0693 - val_acc: 0.9938 - val_mDice: 0.5057

Epoch 00014: val_mDice did not improve from 0.51840
Epoch 15/300
 - 80s - loss: 0.0366 - acc: 0.9957 - mDice: 0.9290 - val_loss: 0.0781 - val_acc: 0.9927 - val_mDice: 0.4901

Epoch 00015: val_mDice did not improve from 0.51840
Epoch 16/300
 - 80s - loss: 0.0352 - acc: 0.9958 - mDice: 0.9317 - val_loss: -1.6565e-02 - val_acc: 0.9921 - val_mDice: 0.4435

Epoch 00016: val_mDice did not improve from 0.51840
Epoch 17/300
 - 79s - loss: 0.0365 - acc: 0.9958 - mDice: 0.9290 - val_loss: -2.7723e-02 - val_acc: 0.9942 - val_mDice: 0.5120

Epoch 00017: val_mDice did not improve from 0.51840
Epoch 18/300
 - 80s - loss: 0.0344 - acc: 0.9958 - mDice: 0.9333 - val_loss: 0.0619 - val_acc: 0.9920 - val_mDice: 0.4219

Epoch 00018: val_mDice did not improve from 0.51840
Epoch 19/300
 - 80s - loss: 0.0349 - acc: 0.9959 - mDice: 0.9321 - val_loss: -5.1111e-03 - val_acc: 0.9942 - val_mDice: 0.4978

Epoch 00019: val_mDice did not improve from 0.51840

Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 20/300
 - 79s - loss: 0.0318 - acc: 0.9961 - mDice: 0.9384 - val_loss: -1.0087e-02 - val_acc: 0.9942 - val_mDice: 0.5077

Epoch 00020: val_mDice did not improve from 0.51840
Epoch 21/300
 - 79s - loss: 0.0312 - acc: 0.9961 - mDice: 0.9395 - val_loss: 0.0324 - val_acc: 0.9942 - val_mDice: 0.5011

Epoch 00021: val_mDice did not improve from 0.51840
Epoch 22/300
 - 80s - loss: 0.0313 - acc: 0.9961 - mDice: 0.9392 - val_loss: -8.4202e-03 - val_acc: 0.9942 - val_mDice: 0.5054

Epoch 00022: val_mDice did not improve from 0.51840
Epoch 23/300
 - 81s - loss: 0.0304 - acc: 0.9962 - mDice: 0.9410 - val_loss: 0.0708 - val_acc: 0.9937 - val_mDice: 0.5023

Epoch 00023: val_mDice did not improve from 0.51840
Epoch 24/300
 - 82s - loss: 0.0303 - acc: 0.9962 - mDice: 0.9411 - val_loss: -4.8407e-02 - val_acc: 0.9941 - val_mDice: 0.5064

Epoch 00024: val_mDice did not improve from 0.51840
Epoch 25/300
 - 81s - loss: 0.0312 - acc: 0.9962 - mDice: 0.9395 - val_loss: 0.0688 - val_acc: 0.9941 - val_mDice: 0.5063

Epoch 00025: val_mDice did not improve from 0.51840
Epoch 26/300
 - 81s - loss: 0.0301 - acc: 0.9962 - mDice: 0.9417 - val_loss: -4.4839e-02 - val_acc: 0.9944 - val_mDice: 0.4997

Epoch 00026: val_mDice did not improve from 0.51840
Epoch 27/300
 - 81s - loss: 0.0296 - acc: 0.9963 - mDice: 0.9425 - val_loss: 0.0630 - val_acc: 0.9938 - val_mDice: 0.5184

Epoch 00027: val_mDice improved from 0.51840 to 0.51844, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 28/300
 - 81s - loss: 0.0290 - acc: 0.9963 - mDice: 0.9438 - val_loss: 0.0299 - val_acc: 0.9939 - val_mDice: 0.5061

Epoch 00028: val_mDice did not improve from 0.51844
Epoch 29/300
 - 81s - loss: 0.0291 - acc: 0.9963 - mDice: 0.9437 - val_loss: 0.0589 - val_acc: 0.9938 - val_mDice: 0.4952

Epoch 00029: val_mDice did not improve from 0.51844
Epoch 30/300
 - 81s - loss: 0.0289 - acc: 0.9963 - mDice: 0.9440 - val_loss: 0.0945 - val_acc: 0.9937 - val_mDice: 0.4991

Epoch 00030: val_mDice did not improve from 0.51844
Epoch 31/300
 - 80s - loss: 0.0291 - acc: 0.9963 - mDice: 0.9436 - val_loss: -4.4243e-02 - val_acc: 0.9944 - val_mDice: 0.5055

Epoch 00031: val_mDice did not improve from 0.51844
Epoch 32/300
 - 79s - loss: 0.0292 - acc: 0.9963 - mDice: 0.9433 - val_loss: 0.0370 - val_acc: 0.9935 - val_mDice: 0.4921

Epoch 00032: val_mDice did not improve from 0.51844
Epoch 33/300
 - 80s - loss: 0.0291 - acc: 0.9963 - mDice: 0.9435 - val_loss: -2.0968e-03 - val_acc: 0.9942 - val_mDice: 0.4919

Epoch 00033: val_mDice did not improve from 0.51844
Epoch 34/300
 - 79s - loss: 0.0294 - acc: 0.9963 - mDice: 0.9430 - val_loss: -3.8210e-02 - val_acc: 0.9942 - val_mDice: 0.4858

Epoch 00034: val_mDice did not improve from 0.51844

Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 35/300
 - 80s - loss: 0.0276 - acc: 0.9964 - mDice: 0.9465 - val_loss: 0.0572 - val_acc: 0.9934 - val_mDice: 0.4988

Epoch 00035: val_mDice did not improve from 0.51844
Epoch 36/300
 - 80s - loss: 0.0278 - acc: 0.9964 - mDice: 0.9460 - val_loss: -5.5872e-03 - val_acc: 0.9942 - val_mDice: 0.4983

Epoch 00036: val_mDice did not improve from 0.51844
Epoch 37/300
 - 79s - loss: 0.0271 - acc: 0.9965 - mDice: 0.9474 - val_loss: -4.5703e-02 - val_acc: 0.9943 - val_mDice: 0.5010

Epoch 00037: val_mDice did not improve from 0.51844
Epoch 38/300
 - 79s - loss: 0.0269 - acc: 0.9964 - mDice: 0.9480 - val_loss: -3.3150e-03 - val_acc: 0.9943 - val_mDice: 0.4941

Epoch 00038: val_mDice did not improve from 0.51844
Epoch 39/300
 - 80s - loss: 0.0266 - acc: 0.9965 - mDice: 0.9485 - val_loss: -4.2153e-02 - val_acc: 0.9943 - val_mDice: 0.4939

Epoch 00039: val_mDice did not improve from 0.51844
Epoch 40/300
 - 79s - loss: 0.0269 - acc: 0.9965 - mDice: 0.9478 - val_loss: 0.0962 - val_acc: 0.9936 - val_mDice: 0.4988

Epoch 00040: val_mDice did not improve from 0.51844
Epoch 41/300
 - 80s - loss: 0.0268 - acc: 0.9965 - mDice: 0.9481 - val_loss: 0.0344 - val_acc: 0.9940 - val_mDice: 0.4971

Epoch 00041: val_mDice did not improve from 0.51844
Epoch 42/300
 - 80s - loss: 0.0265 - acc: 0.9965 - mDice: 0.9486 - val_loss: 0.0367 - val_acc: 0.9941 - val_mDice: 0.4924

Epoch 00042: val_mDice did not improve from 0.51844
Epoch 43/300
 - 80s - loss: 0.0265 - acc: 0.9965 - mDice: 0.9487 - val_loss: -3.9748e-02 - val_acc: 0.9943 - val_mDice: 0.4889

Epoch 00043: val_mDice did not improve from 0.51844
Epoch 44/300
 - 79s - loss: 0.0266 - acc: 0.9965 - mDice: 0.9485 - val_loss: 0.0858 - val_acc: 0.9941 - val_mDice: 0.5017

Epoch 00044: val_mDice did not improve from 0.51844
Epoch 45/300
 - 79s - loss: 0.0269 - acc: 0.9965 - mDice: 0.9479 - val_loss: 0.0720 - val_acc: 0.9941 - val_mDice: 0.5000

Epoch 00045: val_mDice did not improve from 0.51844
Epoch 46/300
 - 80s - loss: 0.0265 - acc: 0.9965 - mDice: 0.9486 - val_loss: -4.5352e-02 - val_acc: 0.9943 - val_mDice: 0.5002

Epoch 00046: val_mDice did not improve from 0.51844
Epoch 47/300
 - 79s - loss: 0.0261 - acc: 0.9965 - mDice: 0.9495 - val_loss: -5.1346e-03 - val_acc: 0.9943 - val_mDice: 0.4979

Epoch 00047: val_mDice did not improve from 0.51844
Epoch 48/300
 - 79s - loss: 0.0262 - acc: 0.9965 - mDice: 0.9492 - val_loss: 0.0332 - val_acc: 0.9940 - val_mDice: 0.4996

Epoch 00048: val_mDice did not improve from 0.51844
Epoch 49/300
 - 80s - loss: 0.0267 - acc: 0.9965 - mDice: 0.9482 - val_loss: 0.0331 - val_acc: 0.9941 - val_mDice: 0.4997

Epoch 00049: val_mDice did not improve from 0.51844

Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 50/300
 - 80s - loss: 0.0261 - acc: 0.9966 - mDice: 0.9494 - val_loss: 0.0741 - val_acc: 0.9940 - val_mDice: 0.4958

Epoch 00050: val_mDice did not improve from 0.51844
Epoch 51/300
 - 80s - loss: 0.0256 - acc: 0.9966 - mDice: 0.9504 - val_loss: 0.0386 - val_acc: 0.9941 - val_mDice: 0.4992

Epoch 00051: val_mDice did not improve from 0.51844
Epoch 52/300
 - 80s - loss: 0.0261 - acc: 0.9966 - mDice: 0.9495 - val_loss: -5.9928e-03 - val_acc: 0.9942 - val_mDice: 0.4997

Epoch 00052: val_mDice did not improve from 0.51844
Epoch 53/300
 - 80s - loss: 0.0259 - acc: 0.9966 - mDice: 0.9498 - val_loss: 7.7753e-04 - val_acc: 0.9941 - val_mDice: 0.4983

Epoch 00053: val_mDice did not improve from 0.51844
Epoch 54/300
 - 80s - loss: 0.0252 - acc: 0.9966 - mDice: 0.9512 - val_loss: 0.0323 - val_acc: 0.9942 - val_mDice: 0.5012

Epoch 00054: val_mDice did not improve from 0.51844
Epoch 55/300
 - 80s - loss: 0.0257 - acc: 0.9966 - mDice: 0.9503 - val_loss: 0.0323 - val_acc: 0.9941 - val_mDice: 0.5002

Epoch 00055: val_mDice did not improve from 0.51844
Epoch 56/300
 - 80s - loss: 0.0252 - acc: 0.9966 - mDice: 0.9511 - val_loss: 0.0718 - val_acc: 0.9941 - val_mDice: 0.5003

Epoch 00056: val_mDice did not improve from 0.51844
Epoch 57/300
 - 79s - loss: 0.0252 - acc: 0.9966 - mDice: 0.9511 - val_loss: 0.0737 - val_acc: 0.9942 - val_mDice: 0.4966

Epoch 00057: val_mDice did not improve from 0.51844
Epoch 58/300
 - 80s - loss: 0.0255 - acc: 0.9966 - mDice: 0.9506 - val_loss: 0.0736 - val_acc: 0.9940 - val_mDice: 0.4969

Epoch 00058: val_mDice did not improve from 0.51844
Epoch 59/300
 - 79s - loss: 0.0248 - acc: 0.9966 - mDice: 0.9519 - val_loss: -6.1988e-03 - val_acc: 0.9943 - val_mDice: 0.5000

Epoch 00059: val_mDice did not improve from 0.51844
Epoch 60/300
 - 81s - loss: 0.0256 - acc: 0.9966 - mDice: 0.9503 - val_loss: 0.0730 - val_acc: 0.9943 - val_mDice: 0.4980

Epoch 00060: val_mDice did not improve from 0.51844
Epoch 61/300
 - 80s - loss: 0.0249 - acc: 0.9966 - mDice: 0.9518 - val_loss: 0.0197 - val_acc: 0.9943 - val_mDice: 0.4908

Epoch 00061: val_mDice did not improve from 0.51844
Epoch 62/300
 - 80s - loss: 0.0252 - acc: 0.9966 - mDice: 0.9513 - val_loss: -4.3171e-02 - val_acc: 0.9942 - val_mDice: 0.4961

Epoch 00062: val_mDice did not improve from 0.51844
Epoch 63/300
 - 79s - loss: 0.0245 - acc: 0.9966 - mDice: 0.9525 - val_loss: -4.2046e-02 - val_acc: 0.9942 - val_mDice: 0.4937

Epoch 00063: val_mDice did not improve from 0.51844
Epoch 64/300
 - 79s - loss: 0.0259 - acc: 0.9966 - mDice: 0.9498 - val_loss: 0.0744 - val_acc: 0.9935 - val_mDice: 0.4956

Epoch 00064: val_mDice did not improve from 0.51844

Epoch 00064: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 65/300
 - 80s - loss: 0.0246 - acc: 0.9966 - mDice: 0.9524 - val_loss: 0.0339 - val_acc: 0.9940 - val_mDice: 0.4981

Epoch 00065: val_mDice did not improve from 0.51844
Epoch 66/300
 - 79s - loss: 0.0250 - acc: 0.9966 - mDice: 0.9517 - val_loss: -4.4750e-02 - val_acc: 0.9943 - val_mDice: 0.4992

Epoch 00066: val_mDice did not improve from 0.51844
Epoch 67/300
 - 79s - loss: 0.0244 - acc: 0.9966 - mDice: 0.9527 - val_loss: -4.9678e-03 - val_acc: 0.9942 - val_mDice: 0.4977

Epoch 00067: val_mDice did not improve from 0.51844
Restoring model weights from the end of the best epoch
Epoch 00067: early stopping
{'val_loss': [0.11223819735459983, 0.12773627607384697, 0.1811581124784425, 0.12936469539999962, 0.17781486734747887, 0.034325788263231516, 0.1316009961301461, 0.00025162583915516734, 0.11335271957796067, -0.045841642131563276, -0.01083428255515173, -0.04283937072614208, 0.04501349839847535, 0.0692688975832425, 0.0781476111151278, -0.016564670484513044, -0.027723421633709222, 0.06187920592492446, -0.005111087346449494, -0.010087389498949051, 0.03240095335058868, -0.008420244324952364, 0.07084591500461102, -0.04840721026994288, 0.06884938961593434, -0.04483915778109804, 0.06299458496505395, 0.02992914995411411, 0.05890900956001133, 0.09446199494414032, -0.04424279887462035, 0.03703039127867669, -0.002096772543154657, -0.03820978640578687, 0.05719877273077145, -0.0055872416705824435, -0.045703475596383214, -0.003314980771392584, -0.04215279238997027, 0.09620014065876603, 0.03443530813092366, 0.036711116845253855, -0.0397483283886686, 0.08583735121646896, 0.071999124775175, -0.04535244352882728, -0.005134641891345382, 0.03322799620218575, 0.03309773898217827, 0.07410285953665152, 0.03861164319096133, -0.005992824269924313, 0.0007775290869176388, 0.032334304647520185, 0.03234960342524573, 0.07184823701390997, 0.07367942354176193, 0.07362644647946581, -0.006198827235493809, 0.07296194898663089, 0.019655877200420946, -0.043170835357159376, -0.042045873298775405, 0.07443561154650524, 0.03387431753799319, -0.04474996792851016, -0.004967785789631307], 'val_acc': [0.9923215713351965, 0.9922638898715377, 0.9916184623725712, 0.9932301649823785, 0.9921619300730526, 0.9935684725642204, 0.9915364570915699, 0.9940551910549402, 0.9936563987284899, 0.9943093117326498, 0.9940645438618958, 0.9940910446457565, 0.994013097602874, 0.9937811237759888, 0.9927375209517777, 0.9920858512632549, 0.994167439173907, 0.9920315993949771, 0.9941870803013444, 0.9942369712516665, 0.994213585741818, 0.9941805331036448, 0.9937455705367029, 0.9941399972885847, 0.9941172450780869, 0.9943563900887966, 0.9938384862616658, 0.9939360888674855, 0.9938419158570468, 0.9936738600954413, 0.9943719804286957, 0.9934833473525941, 0.9942036112770438, 0.9942129594273865, 0.9933935515582561, 0.9942466430366039, 0.9942840565927327, 0.994318971876055, 0.9943008902482688, 0.9935977780260146, 0.9939557327888906, 0.9940863717347383, 0.9942837422713637, 0.9940545703284442, 0.9940938595682383, 0.9942862358875573, 0.9942974583245814, 0.9939685082063079, 0.9941169307567179, 0.9940349184907973, 0.9941468611359596, 0.9942163890227675, 0.9940682877786458, 0.9942201306112111, 0.994113186839968, 0.9940885580144823, 0.9941568332724273, 0.9939918983727694, 0.9943099287338555, 0.9942591111175716, 0.9943002685904503, 0.9942126497626305, 0.9941855273209512, 0.9934821063652635, 0.9940452142618597, 0.9942678399384022, 0.9942017369903624], 'val_mDice': [0.4377482255222276, 0.4715505929198116, 0.47073977533727884, 0.5184043850749731, 0.49259316991083324, 0.5037774884840474, 0.48868367972318083, 0.511660996126011, 0.4953963402658701, 0.501061863033101, 0.5090447241673248, 0.49555599805898964, 0.4987939272541553, 0.5057188315645406, 0.49012588104233146, 0.44346869573928416, 0.5120316793909296, 0.421885613724284, 0.49776930154108223, 0.5077433882979676, 0.5010514917375986, 0.5054188671056181, 0.5023375567431401, 0.5064426816534251, 0.5063154548407054, 0.49968079023528844, 0.5184404971078038, 0.5061377736274159, 0.49523822075570934, 0.4991472011897713, 0.505454174708575, 0.49208625219762325, 0.4918650328181684, 0.48577170819044113, 0.49876875011250377, 0.4983474675100297, 0.5009716528002173, 0.4940636263927445, 0.493926287163049, 0.49881998682394624, 0.4971376550383866, 0.49242643348407, 0.48889086057897657, 0.5016633949708194, 0.5000005220063031, 0.5001893459120765, 0.49788182077463716, 0.4995871498249471, 0.4996998026035726, 0.4958385950885713, 0.49921561498194933, 0.4997158225160092, 0.4982993705198169, 0.5011623789323494, 0.5002016673097387, 0.5003222252707928, 0.4965995729435235, 0.4968839680077508, 0.5000218376517296, 0.4979882901534438, 0.49083466932643205, 0.4961222887504846, 0.49366976832970977, 0.49560969695448875, 0.4981118772411719, 0.4991893924307078, 0.4976667370647192], 'loss': [0.09763487555849243, 0.06345077832411761, 0.05516939327610734, 0.05093242795546046, 0.04624120504065675, 0.04519203142936573, 0.04369095777563618, 0.042094691945546124, 0.039763999147858095, 0.04027399866890101, 0.038471225394186855, 0.03740276567065731, 0.03773498412235088, 0.03597762081627094, 0.03656364302418713, 0.03520535983935568, 0.03652651236716416, 0.03439539722363348, 0.034946009409376394, 0.03175029170797776, 0.031201182648087678, 0.03132079770732067, 0.030435578138643792, 0.030348153503753864, 0.031183208698482342, 0.030077009803640507, 0.029642469485012153, 0.029010986538559106, 0.029054971670867593, 0.028896197215271452, 0.029077135479185685, 0.029227044331642398, 0.029106469671907942, 0.02937435393128728, 0.02761410144292423, 0.02784674682880481, 0.02714626423722847, 0.0268571928422238, 0.026596072899785567, 0.026935255816101376, 0.02675892880628565, 0.02654839942883819, 0.026456079221469843, 0.02657567631257208, 0.026872074188006884, 0.026532430197007843, 0.02608264630990008, 0.026241027268886122, 0.026716645114947046, 0.026084397826527723, 0.025591266227783717, 0.026051910344151597, 0.025880322755387233, 0.02517951251831816, 0.02566049816966597, 0.02524491236699338, 0.02523866247771384, 0.025476782813718457, 0.024842705840334267, 0.025643423108761217, 0.024890092486002532, 0.025159321863005575, 0.024544201632008343, 0.02589929752736553, 0.024590355916847068, 0.024951921050745503, 0.02442846679567695], 'acc': [0.9894525977436973, 0.9931646455271922, 0.9939978301310437, 0.9943846247192596, 0.9947739795994728, 0.9949401132983171, 0.9950004539465369, 0.9952217410066976, 0.9953908419046749, 0.9953814580962123, 0.9955357108726116, 0.9955898114124407, 0.9955514824887636, 0.9957303077041362, 0.9957276460974006, 0.9957542347619934, 0.9957576607236799, 0.9958068048052358, 0.9958510548586185, 0.996090449355573, 0.9961337857480745, 0.9961399120695571, 0.9962009623433788, 0.9962036470246672, 0.996182285037151, 0.996223562863752, 0.9962592073554456, 0.9962697155220495, 0.9962858935813531, 0.9963004484996018, 0.9963132800770038, 0.9963163910904342, 0.9963424371013773, 0.996318167027688, 0.9963941999161522, 0.9964287038310363, 0.9964536736204578, 0.9964426054665889, 0.9964723154630647, 0.9964796972147357, 0.9964981104516811, 0.9964848625074076, 0.9965213098370324, 0.9965091419300303, 0.9965376030267206, 0.9965153321012659, 0.9965393657854408, 0.9965408083812776, 0.9965262796677432, 0.9965790953940321, 0.9965836678742946, 0.9965711335098756, 0.9965789980750972, 0.9965931786343488, 0.996590469472705, 0.996600888630537, 0.9966032772345647, 0.9965975503826219, 0.9966019974763737, 0.9966103347998484, 0.9966325871382383, 0.9966270983239922, 0.9966367746017197, 0.9966144669938201, 0.9966480335084572, 0.9966394365200864, 0.9966344938566838], 'mDice': [0.8102517742533196, 0.8766072615227993, 0.8927213978452994, 0.9009809400962041, 0.9101516414397793, 0.9121568564360629, 0.9151245899090745, 0.9182022254170892, 0.9227643111752927, 0.9217471369463516, 0.9252695761377753, 0.9273791119856217, 0.9267306861955557, 0.9301498017445998, 0.9289757602303732, 0.9316772968633064, 0.9290381198478344, 0.9332674299505815, 0.9321400606702869, 0.938399721586678, 0.9394726085374756, 0.9392259091509334, 0.9409654887071315, 0.9411367287518907, 0.9394888652762782, 0.941673513331784, 0.9425179040485993, 0.9437721150323352, 0.9436786618952387, 0.9439877067706969, 0.9436207224409436, 0.9433140191132147, 0.9435408604994072, 0.943031704498872, 0.9464897784300904, 0.9460082059539789, 0.9474009038596753, 0.947977728083032, 0.948486179747038, 0.9478062892314711, 0.9481485624533297, 0.9485724027929398, 0.9487436004655263, 0.9485115672814329, 0.9479069334920225, 0.9485820556429325, 0.9494712073808211, 0.9491580184122764, 0.9482107406171119, 0.9494492435574434, 0.9504338359879855, 0.9495077314115199, 0.9498499402936721, 0.9512481429275769, 0.950283609942663, 0.951111684661855, 0.9511200695125376, 0.9506424457212213, 0.9519054959068817, 0.950303265618242, 0.9517986433450888, 0.9512699109773624, 0.9524939673431967, 0.9497915272085568, 0.9523970676075086, 0.9516779698359643, 0.9527203114492298], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 9e-05, 9e-05, 9e-05]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:00<00:02,  1.36it/s]predicting test subjects:  50%|█████     | 2/4 [00:00<00:01,  1.79it/s]predicting test subjects:  75%|███████▌  | 3/4 [00:01<00:00,  2.28it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  2.78it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  3.29it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:06,  3.97it/s]predicting train subjects:   1%|          | 2/266 [00:00<01:00,  4.36it/s]predicting train subjects:   1%|          | 3/266 [00:00<01:07,  3.88it/s]predicting train subjects:   2%|▏         | 4/266 [00:01<01:14,  3.51it/s]predicting train subjects:   2%|▏         | 5/266 [00:01<01:11,  3.64it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<01:04,  4.01it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<00:59,  4.36it/s]predicting train subjects:   3%|▎         | 8/266 [00:01<00:55,  4.67it/s]predicting train subjects:   3%|▎         | 9/266 [00:02<00:52,  4.91it/s]predicting train subjects:   4%|▍         | 10/266 [00:02<00:50,  5.12it/s]predicting train subjects:   4%|▍         | 11/266 [00:02<00:48,  5.27it/s]predicting train subjects:   5%|▍         | 12/266 [00:02<00:47,  5.38it/s]predicting train subjects:   5%|▍         | 13/266 [00:02<00:46,  5.39it/s]predicting train subjects:   5%|▌         | 14/266 [00:02<00:46,  5.43it/s]predicting train subjects:   6%|▌         | 15/266 [00:03<00:45,  5.46it/s]predicting train subjects:   6%|▌         | 16/266 [00:03<00:45,  5.45it/s]predicting train subjects:   6%|▋         | 17/266 [00:03<00:45,  5.46it/s]predicting train subjects:   7%|▋         | 18/266 [00:03<00:45,  5.42it/s]predicting train subjects:   7%|▋         | 19/266 [00:03<00:45,  5.45it/s]predicting train subjects:   8%|▊         | 20/266 [00:04<00:44,  5.47it/s]predicting train subjects:   8%|▊         | 21/266 [00:04<00:44,  5.49it/s]predicting train subjects:   8%|▊         | 22/266 [00:04<00:44,  5.50it/s]predicting train subjects:   9%|▊         | 23/266 [00:04<00:44,  5.52it/s]predicting train subjects:   9%|▉         | 24/266 [00:04<00:43,  5.59it/s]predicting train subjects:   9%|▉         | 25/266 [00:04<00:42,  5.62it/s]predicting train subjects:  10%|▉         | 26/266 [00:05<00:42,  5.68it/s]predicting train subjects:  10%|█         | 27/266 [00:05<00:41,  5.72it/s]predicting train subjects:  11%|█         | 28/266 [00:05<00:41,  5.71it/s]predicting train subjects:  11%|█         | 29/266 [00:05<00:41,  5.71it/s]predicting train subjects:  11%|█▏        | 30/266 [00:05<00:41,  5.71it/s]predicting train subjects:  12%|█▏        | 31/266 [00:06<00:41,  5.71it/s]predicting train subjects:  12%|█▏        | 32/266 [00:06<00:40,  5.73it/s]predicting train subjects:  12%|█▏        | 33/266 [00:06<00:40,  5.74it/s]predicting train subjects:  13%|█▎        | 34/266 [00:06<00:40,  5.71it/s]predicting train subjects:  13%|█▎        | 35/266 [00:06<00:40,  5.71it/s]predicting train subjects:  14%|█▎        | 36/266 [00:06<00:40,  5.70it/s]predicting train subjects:  14%|█▍        | 37/266 [00:07<00:40,  5.62it/s]predicting train subjects:  14%|█▍        | 38/266 [00:07<00:40,  5.68it/s]predicting train subjects:  15%|█▍        | 39/266 [00:07<00:39,  5.69it/s]predicting train subjects:  15%|█▌        | 40/266 [00:07<00:39,  5.75it/s]predicting train subjects:  15%|█▌        | 41/266 [00:07<00:38,  5.77it/s]predicting train subjects:  16%|█▌        | 42/266 [00:07<00:37,  5.99it/s]predicting train subjects:  16%|█▌        | 43/266 [00:08<00:36,  6.18it/s]predicting train subjects:  17%|█▋        | 44/266 [00:08<00:35,  6.30it/s]predicting train subjects:  17%|█▋        | 45/266 [00:08<00:34,  6.41it/s]predicting train subjects:  17%|█▋        | 46/266 [00:08<00:33,  6.57it/s]predicting train subjects:  18%|█▊        | 47/266 [00:08<00:32,  6.69it/s]predicting train subjects:  18%|█▊        | 48/266 [00:08<00:32,  6.72it/s]predicting train subjects:  18%|█▊        | 49/266 [00:08<00:32,  6.75it/s]predicting train subjects:  19%|█▉        | 50/266 [00:09<00:31,  6.79it/s]predicting train subjects:  19%|█▉        | 51/266 [00:09<00:31,  6.80it/s]predicting train subjects:  20%|█▉        | 52/266 [00:09<00:31,  6.82it/s]predicting train subjects:  20%|█▉        | 53/266 [00:09<00:31,  6.83it/s]predicting train subjects:  20%|██        | 54/266 [00:09<00:30,  6.88it/s]predicting train subjects:  21%|██        | 55/266 [00:09<00:30,  6.90it/s]predicting train subjects:  21%|██        | 56/266 [00:09<00:30,  6.87it/s]predicting train subjects:  21%|██▏       | 57/266 [00:10<00:30,  6.91it/s]predicting train subjects:  22%|██▏       | 58/266 [00:10<00:29,  6.94it/s]predicting train subjects:  22%|██▏       | 59/266 [00:10<00:29,  6.96it/s]predicting train subjects:  23%|██▎       | 60/266 [00:10<00:29,  6.89it/s]predicting train subjects:  23%|██▎       | 61/266 [00:10<00:29,  6.85it/s]predicting train subjects:  23%|██▎       | 62/266 [00:10<00:30,  6.80it/s]predicting train subjects:  24%|██▎       | 63/266 [00:10<00:30,  6.76it/s]predicting train subjects:  24%|██▍       | 64/266 [00:11<00:29,  6.77it/s]predicting train subjects:  24%|██▍       | 65/266 [00:11<00:30,  6.62it/s]predicting train subjects:  25%|██▍       | 66/266 [00:11<00:29,  6.67it/s]predicting train subjects:  25%|██▌       | 67/266 [00:11<00:30,  6.61it/s]predicting train subjects:  26%|██▌       | 68/266 [00:11<00:29,  6.65it/s]predicting train subjects:  26%|██▌       | 69/266 [00:11<00:29,  6.64it/s]predicting train subjects:  26%|██▋       | 70/266 [00:12<00:29,  6.64it/s]predicting train subjects:  27%|██▋       | 71/266 [00:12<00:29,  6.67it/s]predicting train subjects:  27%|██▋       | 72/266 [00:12<00:28,  6.73it/s]predicting train subjects:  27%|██▋       | 73/266 [00:12<00:28,  6.68it/s]predicting train subjects:  28%|██▊       | 74/266 [00:12<00:28,  6.74it/s]predicting train subjects:  28%|██▊       | 75/266 [00:12<00:28,  6.73it/s]predicting train subjects:  29%|██▊       | 76/266 [00:12<00:28,  6.76it/s]predicting train subjects:  29%|██▉       | 77/266 [00:13<00:27,  6.80it/s]predicting train subjects:  29%|██▉       | 78/266 [00:13<00:29,  6.43it/s]predicting train subjects:  30%|██▉       | 79/266 [00:13<00:30,  6.23it/s]predicting train subjects:  30%|███       | 80/266 [00:13<00:30,  6.04it/s]predicting train subjects:  30%|███       | 81/266 [00:13<00:31,  5.94it/s]predicting train subjects:  31%|███       | 82/266 [00:13<00:31,  5.80it/s]predicting train subjects:  31%|███       | 83/266 [00:14<00:31,  5.79it/s]predicting train subjects:  32%|███▏      | 84/266 [00:14<00:31,  5.75it/s]predicting train subjects:  32%|███▏      | 85/266 [00:14<00:31,  5.77it/s]predicting train subjects:  32%|███▏      | 86/266 [00:14<00:31,  5.80it/s]predicting train subjects:  33%|███▎      | 87/266 [00:14<00:30,  5.79it/s]predicting train subjects:  33%|███▎      | 88/266 [00:14<00:30,  5.79it/s]predicting train subjects:  33%|███▎      | 89/266 [00:15<00:30,  5.79it/s]predicting train subjects:  34%|███▍      | 90/266 [00:15<00:30,  5.72it/s]predicting train subjects:  34%|███▍      | 91/266 [00:15<00:31,  5.54it/s]predicting train subjects:  35%|███▍      | 92/266 [00:15<00:32,  5.35it/s]predicting train subjects:  35%|███▍      | 93/266 [00:15<00:31,  5.47it/s]predicting train subjects:  35%|███▌      | 94/266 [00:16<00:30,  5.59it/s]predicting train subjects:  36%|███▌      | 95/266 [00:16<00:31,  5.43it/s]predicting train subjects:  36%|███▌      | 96/266 [00:16<00:40,  4.22it/s]predicting train subjects:  36%|███▋      | 97/266 [00:16<00:41,  4.09it/s]predicting train subjects:  37%|███▋      | 98/266 [00:17<00:39,  4.26it/s]predicting train subjects:  37%|███▋      | 99/266 [00:17<00:41,  4.00it/s]predicting train subjects:  38%|███▊      | 100/266 [00:17<00:36,  4.57it/s]predicting train subjects:  38%|███▊      | 101/266 [00:17<00:32,  5.03it/s]predicting train subjects:  38%|███▊      | 102/266 [00:17<00:30,  5.37it/s]predicting train subjects:  39%|███▊      | 103/266 [00:18<00:28,  5.65it/s]predicting train subjects:  39%|███▉      | 104/266 [00:18<00:27,  5.79it/s]predicting train subjects:  39%|███▉      | 105/266 [00:18<00:26,  5.97it/s]predicting train subjects:  40%|███▉      | 106/266 [00:18<00:26,  6.01it/s]predicting train subjects:  40%|████      | 107/266 [00:18<00:25,  6.12it/s]predicting train subjects:  41%|████      | 108/266 [00:18<00:25,  6.21it/s]predicting train subjects:  41%|████      | 109/266 [00:18<00:25,  6.28it/s]predicting train subjects:  41%|████▏     | 110/266 [00:19<00:24,  6.25it/s]predicting train subjects:  42%|████▏     | 111/266 [00:19<00:24,  6.28it/s]predicting train subjects:  42%|████▏     | 112/266 [00:19<00:24,  6.40it/s]predicting train subjects:  42%|████▏     | 113/266 [00:19<00:23,  6.48it/s]predicting train subjects:  43%|████▎     | 114/266 [00:19<00:23,  6.53it/s]predicting train subjects:  43%|████▎     | 115/266 [00:19<00:22,  6.58it/s]predicting train subjects:  44%|████▎     | 116/266 [00:20<00:23,  6.44it/s]predicting train subjects:  44%|████▍     | 117/266 [00:20<00:23,  6.47it/s]predicting train subjects:  44%|████▍     | 118/266 [00:20<00:22,  6.54it/s]predicting train subjects:  45%|████▍     | 119/266 [00:20<00:23,  6.23it/s]predicting train subjects:  45%|████▌     | 120/266 [00:20<00:24,  6.06it/s]predicting train subjects:  45%|████▌     | 121/266 [00:20<00:24,  5.94it/s]predicting train subjects:  46%|████▌     | 122/266 [00:21<00:24,  5.88it/s]predicting train subjects:  46%|████▌     | 123/266 [00:21<00:24,  5.82it/s]predicting train subjects:  47%|████▋     | 124/266 [00:21<00:24,  5.78it/s]predicting train subjects:  47%|████▋     | 125/266 [00:21<00:24,  5.76it/s]predicting train subjects:  47%|████▋     | 126/266 [00:21<00:24,  5.73it/s]predicting train subjects:  48%|████▊     | 127/266 [00:21<00:24,  5.64it/s]predicting train subjects:  48%|████▊     | 128/266 [00:22<00:24,  5.66it/s]predicting train subjects:  48%|████▊     | 129/266 [00:22<00:24,  5.65it/s]predicting train subjects:  49%|████▉     | 130/266 [00:22<00:24,  5.66it/s]predicting train subjects:  49%|████▉     | 131/266 [00:22<00:24,  5.58it/s]predicting train subjects:  50%|████▉     | 132/266 [00:22<00:24,  5.56it/s]predicting train subjects:  50%|█████     | 133/266 [00:23<00:23,  5.59it/s]predicting train subjects:  50%|█████     | 134/266 [00:23<00:23,  5.51it/s]predicting train subjects:  51%|█████     | 135/266 [00:23<00:23,  5.53it/s]predicting train subjects:  51%|█████     | 136/266 [00:23<00:24,  5.39it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:23<00:23,  5.48it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:23<00:22,  5.67it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:24<00:21,  5.81it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:24<00:21,  5.89it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:24<00:20,  5.98it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:24<00:20,  5.91it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:24<00:20,  5.98it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:24<00:20,  6.02it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:25<00:19,  6.08it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:25<00:19,  6.07it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:25<00:19,  6.09it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:25<00:19,  6.13it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:25<00:19,  6.12it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:25<00:19,  6.07it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:26<00:18,  6.05it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:26<00:19,  5.96it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:26<00:18,  5.95it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:26<00:18,  5.96it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:26<00:17,  6.30it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:26<00:16,  6.54it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:26<00:16,  6.81it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:27<00:15,  7.00it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:27<00:14,  7.16it/s]predicting train subjects:  60%|██████    | 160/266 [00:27<00:14,  7.28it/s]predicting train subjects:  61%|██████    | 161/266 [00:27<00:14,  7.37it/s]predicting train subjects:  61%|██████    | 162/266 [00:27<00:14,  7.37it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:27<00:13,  7.39it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:27<00:13,  7.42it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:28<00:13,  7.45it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:28<00:13,  7.45it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:28<00:13,  7.34it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:28<00:13,  7.28it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:28<00:13,  7.32it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:28<00:13,  7.36it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:28<00:12,  7.40it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:28<00:12,  7.45it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:29<00:13,  6.93it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:29<00:13,  6.80it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:29<00:14,  6.43it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:29<00:13,  6.51it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:29<00:13,  6.57it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:29<00:13,  6.60it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:30<00:13,  6.59it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:30<00:12,  6.65it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:30<00:12,  6.68it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:30<00:12,  6.68it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:30<00:12,  6.67it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:30<00:12,  6.67it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:30<00:12,  6.72it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:31<00:12,  6.28it/s]predicting train subjects:  70%|███████   | 187/266 [00:31<00:12,  6.39it/s]predicting train subjects:  71%|███████   | 188/266 [00:31<00:12,  6.47it/s]predicting train subjects:  71%|███████   | 189/266 [00:31<00:11,  6.51it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:31<00:11,  6.51it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:31<00:11,  6.54it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:32<00:15,  4.84it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:32<00:14,  5.10it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:32<00:13,  5.30it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:32<00:12,  5.56it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:32<00:12,  5.82it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:33<00:11,  6.01it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:33<00:11,  6.17it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:33<00:10,  6.32it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:33<00:10,  6.41it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:33<00:10,  6.47it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:33<00:09,  6.55it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:33<00:09,  6.60it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:34<00:09,  6.60it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:34<00:09,  6.60it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:34<00:09,  6.63it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:34<00:08,  6.64it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:34<00:08,  6.63it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:34<00:08,  6.56it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:35<00:08,  6.55it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:35<00:08,  6.60it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:35<00:08,  6.58it/s]predicting train subjects:  80%|████████  | 213/266 [00:35<00:07,  6.63it/s]predicting train subjects:  80%|████████  | 214/266 [00:35<00:07,  6.69it/s]predicting train subjects:  81%|████████  | 215/266 [00:35<00:07,  6.74it/s]predicting train subjects:  81%|████████  | 216/266 [00:35<00:07,  6.80it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:36<00:07,  6.87it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:36<00:06,  6.90it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:36<00:06,  6.89it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:36<00:06,  6.86it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:36<00:06,  6.88it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:36<00:06,  6.91it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:36<00:06,  6.93it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:37<00:06,  6.97it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:37<00:05,  6.97it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:37<00:05,  6.98it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:37<00:05,  6.83it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:37<00:05,  6.83it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:37<00:05,  6.82it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:37<00:05,  6.82it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:38<00:05,  6.69it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:38<00:05,  6.73it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:38<00:04,  6.68it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:38<00:04,  6.44it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:38<00:04,  6.42it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:38<00:04,  6.43it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:39<00:04,  6.51it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:39<00:04,  6.55it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:39<00:04,  6.50it/s]predicting train subjects:  90%|█████████ | 240/266 [00:39<00:03,  6.50it/s]predicting train subjects:  91%|█████████ | 241/266 [00:39<00:03,  6.43it/s]predicting train subjects:  91%|█████████ | 242/266 [00:39<00:03,  6.47it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:39<00:03,  6.49it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:40<00:03,  6.50it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:40<00:03,  6.55it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:40<00:03,  6.58it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:40<00:02,  6.63it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:40<00:02,  6.60it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:40<00:02,  6.28it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:41<00:02,  6.03it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:41<00:02,  5.94it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:41<00:02,  5.90it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:41<00:02,  5.89it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:41<00:02,  5.88it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:41<00:01,  5.82it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:42<00:01,  5.81it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:42<00:01,  5.81it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:42<00:01,  5.79it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:42<00:01,  5.77it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:42<00:01,  5.77it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:42<00:00,  5.77it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:43<00:00,  5.75it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:43<00:00,  5.70it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:43<00:00,  5.73it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:43<00:00,  5.71it/s]predicting train subjects: 100%|██████████| 266/266 [00:43<00:00,  5.68it/s]predicting train subjects: 100%|██████████| 266/266 [00:43<00:00,  6.07it/s]
predicting test subjects sagittal:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects sagittal:  25%|██▌       | 1/4 [00:00<00:00,  6.41it/s]predicting test subjects sagittal:  50%|█████     | 2/4 [00:00<00:00,  6.53it/s]predicting test subjects sagittal:  75%|███████▌  | 3/4 [00:00<00:00,  6.51it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  6.37it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  6.42it/s]
predicting train subjects sagittal:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/266 [00:00<00:51,  5.15it/s]predicting train subjects sagittal:   1%|          | 2/266 [00:00<00:49,  5.31it/s]predicting train subjects sagittal:   1%|          | 3/266 [00:00<00:46,  5.70it/s]predicting train subjects sagittal:   2%|▏         | 4/266 [00:00<00:43,  5.99it/s]predicting train subjects sagittal:   2%|▏         | 5/266 [00:00<00:44,  5.93it/s]predicting train subjects sagittal:   2%|▏         | 6/266 [00:01<00:44,  5.80it/s]predicting train subjects sagittal:   3%|▎         | 7/266 [00:01<00:45,  5.72it/s]predicting train subjects sagittal:   3%|▎         | 8/266 [00:01<00:45,  5.68it/s]predicting train subjects sagittal:   3%|▎         | 9/266 [00:01<00:45,  5.63it/s]predicting train subjects sagittal:   4%|▍         | 10/266 [00:01<00:45,  5.58it/s]predicting train subjects sagittal:   4%|▍         | 11/266 [00:01<00:46,  5.52it/s]predicting train subjects sagittal:   5%|▍         | 12/266 [00:02<00:46,  5.49it/s]predicting train subjects sagittal:   5%|▍         | 13/266 [00:02<00:46,  5.49it/s]predicting train subjects sagittal:   5%|▌         | 14/266 [00:02<00:46,  5.40it/s]predicting train subjects sagittal:   6%|▌         | 15/266 [00:02<00:46,  5.44it/s]predicting train subjects sagittal:   6%|▌         | 16/266 [00:02<00:46,  5.41it/s]predicting train subjects sagittal:   6%|▋         | 17/266 [00:03<00:46,  5.40it/s]predicting train subjects sagittal:   7%|▋         | 18/266 [00:03<00:45,  5.40it/s]predicting train subjects sagittal:   7%|▋         | 19/266 [00:03<00:45,  5.41it/s]predicting train subjects sagittal:   8%|▊         | 20/266 [00:03<00:45,  5.43it/s]predicting train subjects sagittal:   8%|▊         | 21/266 [00:03<00:44,  5.45it/s]predicting train subjects sagittal:   8%|▊         | 22/266 [00:03<00:45,  5.31it/s]predicting train subjects sagittal:   9%|▊         | 23/266 [00:04<00:45,  5.31it/s]predicting train subjects sagittal:   9%|▉         | 24/266 [00:04<00:45,  5.37it/s]predicting train subjects sagittal:   9%|▉         | 25/266 [00:04<00:44,  5.36it/s]predicting train subjects sagittal:  10%|▉         | 26/266 [00:04<00:44,  5.43it/s]predicting train subjects sagittal:  10%|█         | 27/266 [00:04<00:43,  5.52it/s]predicting train subjects sagittal:  11%|█         | 28/266 [00:05<00:43,  5.46it/s]predicting train subjects sagittal:  11%|█         | 29/266 [00:05<00:42,  5.51it/s]predicting train subjects sagittal:  11%|█▏        | 30/266 [00:05<00:42,  5.53it/s]predicting train subjects sagittal:  12%|█▏        | 31/266 [00:05<00:42,  5.56it/s]predicting train subjects sagittal:  12%|█▏        | 32/266 [00:05<00:43,  5.44it/s]predicting train subjects sagittal:  12%|█▏        | 33/266 [00:05<00:43,  5.30it/s]predicting train subjects sagittal:  13%|█▎        | 34/266 [00:06<00:44,  5.26it/s]predicting train subjects sagittal:  13%|█▎        | 35/266 [00:06<00:43,  5.31it/s]predicting train subjects sagittal:  14%|█▎        | 36/266 [00:06<00:42,  5.37it/s]predicting train subjects sagittal:  14%|█▍        | 37/266 [00:06<00:42,  5.40it/s]predicting train subjects sagittal:  14%|█▍        | 38/266 [00:06<00:42,  5.43it/s]predicting train subjects sagittal:  15%|█▍        | 39/266 [00:07<00:41,  5.48it/s]predicting train subjects sagittal:  15%|█▌        | 40/266 [00:07<00:40,  5.54it/s]predicting train subjects sagittal:  15%|█▌        | 41/266 [00:07<00:40,  5.58it/s]predicting train subjects sagittal:  16%|█▌        | 42/266 [00:07<00:38,  5.78it/s]predicting train subjects sagittal:  16%|█▌        | 43/266 [00:07<00:36,  6.04it/s]predicting train subjects sagittal:  17%|█▋        | 44/266 [00:07<00:35,  6.21it/s]predicting train subjects sagittal:  17%|█▋        | 45/266 [00:08<00:34,  6.34it/s]predicting train subjects sagittal:  17%|█▋        | 46/266 [00:08<00:34,  6.41it/s]predicting train subjects sagittal:  18%|█▊        | 47/266 [00:08<00:34,  6.35it/s]predicting train subjects sagittal:  18%|█▊        | 48/266 [00:08<00:34,  6.39it/s]predicting train subjects sagittal:  18%|█▊        | 49/266 [00:08<00:33,  6.50it/s]predicting train subjects sagittal:  19%|█▉        | 50/266 [00:08<00:33,  6.50it/s]predicting train subjects sagittal:  19%|█▉        | 51/266 [00:08<00:32,  6.57it/s]predicting train subjects sagittal:  20%|█▉        | 52/266 [00:09<00:32,  6.62it/s]predicting train subjects sagittal:  20%|█▉        | 53/266 [00:09<00:31,  6.68it/s]predicting train subjects sagittal:  20%|██        | 54/266 [00:09<00:31,  6.76it/s]predicting train subjects sagittal:  21%|██        | 55/266 [00:09<00:31,  6.81it/s]predicting train subjects sagittal:  21%|██        | 56/266 [00:09<00:30,  6.84it/s]predicting train subjects sagittal:  21%|██▏       | 57/266 [00:09<00:30,  6.83it/s]predicting train subjects sagittal:  22%|██▏       | 58/266 [00:09<00:30,  6.81it/s]predicting train subjects sagittal:  22%|██▏       | 59/266 [00:10<00:30,  6.83it/s]predicting train subjects sagittal:  23%|██▎       | 60/266 [00:10<00:30,  6.77it/s]predicting train subjects sagittal:  23%|██▎       | 61/266 [00:10<00:30,  6.76it/s]predicting train subjects sagittal:  23%|██▎       | 62/266 [00:10<00:30,  6.75it/s]predicting train subjects sagittal:  24%|██▎       | 63/266 [00:10<00:30,  6.76it/s]predicting train subjects sagittal:  24%|██▍       | 64/266 [00:10<00:29,  6.76it/s]predicting train subjects sagittal:  24%|██▍       | 65/266 [00:11<00:29,  6.76it/s]predicting train subjects sagittal:  25%|██▍       | 66/266 [00:11<00:29,  6.72it/s]predicting train subjects sagittal:  25%|██▌       | 67/266 [00:11<00:29,  6.70it/s]predicting train subjects sagittal:  26%|██▌       | 68/266 [00:11<00:29,  6.73it/s]predicting train subjects sagittal:  26%|██▌       | 69/266 [00:11<00:30,  6.56it/s]predicting train subjects sagittal:  26%|██▋       | 70/266 [00:11<00:30,  6.52it/s]predicting train subjects sagittal:  27%|██▋       | 71/266 [00:11<00:29,  6.52it/s]predicting train subjects sagittal:  27%|██▋       | 72/266 [00:12<00:29,  6.54it/s]predicting train subjects sagittal:  27%|██▋       | 73/266 [00:12<00:29,  6.52it/s]predicting train subjects sagittal:  28%|██▊       | 74/266 [00:12<00:29,  6.51it/s]predicting train subjects sagittal:  28%|██▊       | 75/266 [00:12<00:29,  6.49it/s]predicting train subjects sagittal:  29%|██▊       | 76/266 [00:12<00:29,  6.44it/s]predicting train subjects sagittal:  29%|██▉       | 77/266 [00:12<00:29,  6.45it/s]predicting train subjects sagittal:  29%|██▉       | 78/266 [00:13<00:30,  6.16it/s]predicting train subjects sagittal:  30%|██▉       | 79/266 [00:13<00:31,  5.93it/s]predicting train subjects sagittal:  30%|███       | 80/266 [00:13<00:31,  5.89it/s]predicting train subjects sagittal:  30%|███       | 81/266 [00:13<00:33,  5.57it/s]predicting train subjects sagittal:  31%|███       | 82/266 [00:13<00:32,  5.60it/s]predicting train subjects sagittal:  31%|███       | 83/266 [00:13<00:32,  5.63it/s]predicting train subjects sagittal:  32%|███▏      | 84/266 [00:14<00:32,  5.61it/s]predicting train subjects sagittal:  32%|███▏      | 85/266 [00:14<00:32,  5.60it/s]predicting train subjects sagittal:  32%|███▏      | 86/266 [00:14<00:32,  5.47it/s]predicting train subjects sagittal:  33%|███▎      | 87/266 [00:14<00:32,  5.44it/s]predicting train subjects sagittal:  33%|███▎      | 88/266 [00:14<00:32,  5.43it/s]predicting train subjects sagittal:  33%|███▎      | 89/266 [00:15<00:32,  5.40it/s]predicting train subjects sagittal:  34%|███▍      | 90/266 [00:15<00:32,  5.35it/s]predicting train subjects sagittal:  34%|███▍      | 91/266 [00:15<00:32,  5.46it/s]predicting train subjects sagittal:  35%|███▍      | 92/266 [00:15<00:31,  5.49it/s]predicting train subjects sagittal:  35%|███▍      | 93/266 [00:15<00:31,  5.56it/s]predicting train subjects sagittal:  35%|███▌      | 94/266 [00:15<00:30,  5.58it/s]predicting train subjects sagittal:  36%|███▌      | 95/266 [00:16<00:30,  5.52it/s]predicting train subjects sagittal:  36%|███▌      | 96/266 [00:16<00:29,  5.72it/s]predicting train subjects sagittal:  36%|███▋      | 97/266 [00:16<00:30,  5.47it/s]predicting train subjects sagittal:  37%|███▋      | 98/266 [00:16<00:30,  5.50it/s]predicting train subjects sagittal:  37%|███▋      | 99/266 [00:16<00:28,  5.93it/s]predicting train subjects sagittal:  38%|███▊      | 100/266 [00:17<00:27,  6.03it/s]predicting train subjects sagittal:  38%|███▊      | 101/266 [00:17<00:26,  6.18it/s]predicting train subjects sagittal:  38%|███▊      | 102/266 [00:17<00:25,  6.31it/s]predicting train subjects sagittal:  39%|███▊      | 103/266 [00:17<00:25,  6.38it/s]predicting train subjects sagittal:  39%|███▉      | 104/266 [00:17<00:25,  6.40it/s]predicting train subjects sagittal:  39%|███▉      | 105/266 [00:17<00:25,  6.26it/s]predicting train subjects sagittal:  40%|███▉      | 106/266 [00:17<00:26,  6.12it/s]predicting train subjects sagittal:  40%|████      | 107/266 [00:18<00:27,  5.86it/s]predicting train subjects sagittal:  41%|████      | 108/266 [00:18<00:26,  5.95it/s]predicting train subjects sagittal:  41%|████      | 109/266 [00:18<00:25,  6.07it/s]predicting train subjects sagittal:  41%|████▏     | 110/266 [00:18<00:25,  6.18it/s]predicting train subjects sagittal:  42%|████▏     | 111/266 [00:18<00:24,  6.28it/s]predicting train subjects sagittal:  42%|████▏     | 112/266 [00:18<00:24,  6.29it/s]predicting train subjects sagittal:  42%|████▏     | 113/266 [00:19<00:24,  6.32it/s]predicting train subjects sagittal:  43%|████▎     | 114/266 [00:19<00:23,  6.36it/s]predicting train subjects sagittal:  43%|████▎     | 115/266 [00:19<00:23,  6.38it/s]predicting train subjects sagittal:  44%|████▎     | 116/266 [00:19<00:23,  6.35it/s]predicting train subjects sagittal:  44%|████▍     | 117/266 [00:19<00:23,  6.37it/s]predicting train subjects sagittal:  44%|████▍     | 118/266 [00:19<00:23,  6.39it/s]predicting train subjects sagittal:  45%|████▍     | 119/266 [00:20<00:24,  6.04it/s]predicting train subjects sagittal:  45%|████▌     | 120/266 [00:20<00:24,  5.88it/s]predicting train subjects sagittal:  45%|████▌     | 121/266 [00:20<00:25,  5.77it/s]predicting train subjects sagittal:  46%|████▌     | 122/266 [00:20<00:25,  5.67it/s]predicting train subjects sagittal:  46%|████▌     | 123/266 [00:20<00:25,  5.64it/s]predicting train subjects sagittal:  47%|████▋     | 124/266 [00:20<00:25,  5.63it/s]predicting train subjects sagittal:  47%|████▋     | 125/266 [00:21<00:25,  5.60it/s]predicting train subjects sagittal:  47%|████▋     | 126/266 [00:21<00:25,  5.58it/s]predicting train subjects sagittal:  48%|████▊     | 127/266 [00:21<00:25,  5.50it/s]predicting train subjects sagittal:  48%|████▊     | 128/266 [00:21<00:25,  5.49it/s]predicting train subjects sagittal:  48%|████▊     | 129/266 [00:21<00:24,  5.50it/s]predicting train subjects sagittal:  49%|████▉     | 130/266 [00:22<00:24,  5.52it/s]predicting train subjects sagittal:  49%|████▉     | 131/266 [00:22<00:24,  5.52it/s]predicting train subjects sagittal:  50%|████▉     | 132/266 [00:22<00:24,  5.51it/s]predicting train subjects sagittal:  50%|█████     | 133/266 [00:22<00:24,  5.48it/s]predicting train subjects sagittal:  50%|█████     | 134/266 [00:22<00:24,  5.47it/s]predicting train subjects sagittal:  51%|█████     | 135/266 [00:22<00:23,  5.49it/s]predicting train subjects sagittal:  51%|█████     | 136/266 [00:23<00:23,  5.50it/s]predicting train subjects sagittal:  52%|█████▏    | 137/266 [00:23<00:22,  5.68it/s]predicting train subjects sagittal:  52%|█████▏    | 138/266 [00:23<00:22,  5.78it/s]predicting train subjects sagittal:  52%|█████▏    | 139/266 [00:23<00:21,  5.82it/s]predicting train subjects sagittal:  53%|█████▎    | 140/266 [00:23<00:21,  5.84it/s]predicting train subjects sagittal:  53%|█████▎    | 141/266 [00:23<00:21,  5.86it/s]predicting train subjects sagittal:  53%|█████▎    | 142/266 [00:24<00:21,  5.87it/s]predicting train subjects sagittal:  54%|█████▍    | 143/266 [00:24<00:21,  5.85it/s]predicting train subjects sagittal:  54%|█████▍    | 144/266 [00:24<00:20,  5.84it/s]predicting train subjects sagittal:  55%|█████▍    | 145/266 [00:24<00:20,  5.83it/s]predicting train subjects sagittal:  55%|█████▍    | 146/266 [00:24<00:20,  5.87it/s]predicting train subjects sagittal:  55%|█████▌    | 147/266 [00:24<00:20,  5.91it/s]predicting train subjects sagittal:  56%|█████▌    | 148/266 [00:25<00:19,  5.92it/s]predicting train subjects sagittal:  56%|█████▌    | 149/266 [00:25<00:19,  5.86it/s]predicting train subjects sagittal:  56%|█████▋    | 150/266 [00:25<00:19,  5.88it/s]predicting train subjects sagittal:  57%|█████▋    | 151/266 [00:25<00:19,  5.84it/s]predicting train subjects sagittal:  57%|█████▋    | 152/266 [00:25<00:19,  5.78it/s]predicting train subjects sagittal:  58%|█████▊    | 153/266 [00:26<00:19,  5.68it/s]predicting train subjects sagittal:  58%|█████▊    | 154/266 [00:26<00:19,  5.61it/s]predicting train subjects sagittal:  58%|█████▊    | 155/266 [00:26<00:18,  5.89it/s]predicting train subjects sagittal:  59%|█████▊    | 156/266 [00:26<00:17,  6.23it/s]predicting train subjects sagittal:  59%|█████▉    | 157/266 [00:26<00:16,  6.46it/s]predicting train subjects sagittal:  59%|█████▉    | 158/266 [00:26<00:16,  6.57it/s]predicting train subjects sagittal:  60%|█████▉    | 159/266 [00:26<00:15,  6.73it/s]predicting train subjects sagittal:  60%|██████    | 160/266 [00:27<00:15,  6.83it/s]predicting train subjects sagittal:  61%|██████    | 161/266 [00:27<00:15,  6.87it/s]predicting train subjects sagittal:  61%|██████    | 162/266 [00:27<00:14,  6.96it/s]predicting train subjects sagittal:  61%|██████▏   | 163/266 [00:27<00:14,  7.03it/s]predicting train subjects sagittal:  62%|██████▏   | 164/266 [00:27<00:14,  7.11it/s]predicting train subjects sagittal:  62%|██████▏   | 165/266 [00:27<00:14,  7.15it/s]predicting train subjects sagittal:  62%|██████▏   | 166/266 [00:27<00:14,  7.12it/s]predicting train subjects sagittal:  63%|██████▎   | 167/266 [00:28<00:13,  7.15it/s]predicting train subjects sagittal:  63%|██████▎   | 168/266 [00:28<00:13,  7.12it/s]predicting train subjects sagittal:  64%|██████▎   | 169/266 [00:28<00:13,  7.09it/s]predicting train subjects sagittal:  64%|██████▍   | 170/266 [00:28<00:13,  6.93it/s]predicting train subjects sagittal:  64%|██████▍   | 171/266 [00:28<00:13,  7.03it/s]predicting train subjects sagittal:  65%|██████▍   | 172/266 [00:28<00:13,  6.86it/s]predicting train subjects sagittal:  65%|██████▌   | 173/266 [00:28<00:13,  6.74it/s]predicting train subjects sagittal:  65%|██████▌   | 174/266 [00:29<00:13,  6.63it/s]predicting train subjects sagittal:  66%|██████▌   | 175/266 [00:29<00:13,  6.57it/s]predicting train subjects sagittal:  66%|██████▌   | 176/266 [00:29<00:13,  6.56it/s]predicting train subjects sagittal:  67%|██████▋   | 177/266 [00:29<00:14,  6.20it/s]predicting train subjects sagittal:  67%|██████▋   | 178/266 [00:29<00:14,  6.13it/s]predicting train subjects sagittal:  67%|██████▋   | 179/266 [00:29<00:14,  6.10it/s]predicting train subjects sagittal:  68%|██████▊   | 180/266 [00:30<00:14,  5.99it/s]predicting train subjects sagittal:  68%|██████▊   | 181/266 [00:30<00:13,  6.11it/s]predicting train subjects sagittal:  68%|██████▊   | 182/266 [00:30<00:13,  6.19it/s]predicting train subjects sagittal:  69%|██████▉   | 183/266 [00:30<00:13,  6.03it/s]predicting train subjects sagittal:  69%|██████▉   | 184/266 [00:30<00:13,  6.06it/s]predicting train subjects sagittal:  70%|██████▉   | 185/266 [00:30<00:13,  6.06it/s]predicting train subjects sagittal:  70%|██████▉   | 186/266 [00:31<00:12,  6.18it/s]predicting train subjects sagittal:  70%|███████   | 187/266 [00:31<00:12,  6.26it/s]predicting train subjects sagittal:  71%|███████   | 188/266 [00:31<00:12,  6.32it/s]predicting train subjects sagittal:  71%|███████   | 189/266 [00:31<00:12,  6.40it/s]predicting train subjects sagittal:  71%|███████▏  | 190/266 [00:31<00:12,  6.33it/s]predicting train subjects sagittal:  72%|███████▏  | 191/266 [00:31<00:11,  6.39it/s]predicting train subjects sagittal:  72%|███████▏  | 192/266 [00:31<00:11,  6.45it/s]predicting train subjects sagittal:  73%|███████▎  | 193/266 [00:32<00:11,  6.40it/s]predicting train subjects sagittal:  73%|███████▎  | 194/266 [00:32<00:11,  6.11it/s]predicting train subjects sagittal:  73%|███████▎  | 195/266 [00:32<00:11,  6.11it/s]predicting train subjects sagittal:  74%|███████▎  | 196/266 [00:32<00:11,  6.14it/s]predicting train subjects sagittal:  74%|███████▍  | 197/266 [00:32<00:11,  6.18it/s]predicting train subjects sagittal:  74%|███████▍  | 198/266 [00:32<00:10,  6.20it/s]predicting train subjects sagittal:  75%|███████▍  | 199/266 [00:33<00:10,  6.20it/s]predicting train subjects sagittal:  75%|███████▌  | 200/266 [00:33<00:10,  6.24it/s]predicting train subjects sagittal:  76%|███████▌  | 201/266 [00:33<00:10,  6.31it/s]predicting train subjects sagittal:  76%|███████▌  | 202/266 [00:33<00:10,  6.33it/s]predicting train subjects sagittal:  76%|███████▋  | 203/266 [00:33<00:09,  6.38it/s]predicting train subjects sagittal:  77%|███████▋  | 204/266 [00:33<00:09,  6.43it/s]predicting train subjects sagittal:  77%|███████▋  | 205/266 [00:34<00:09,  6.46it/s]predicting train subjects sagittal:  77%|███████▋  | 206/266 [00:34<00:09,  6.45it/s]predicting train subjects sagittal:  78%|███████▊  | 207/266 [00:34<00:09,  6.45it/s]predicting train subjects sagittal:  78%|███████▊  | 208/266 [00:34<00:08,  6.50it/s]predicting train subjects sagittal:  79%|███████▊  | 209/266 [00:34<00:08,  6.54it/s]predicting train subjects sagittal:  79%|███████▉  | 210/266 [00:34<00:08,  6.52it/s]predicting train subjects sagittal:  79%|███████▉  | 211/266 [00:34<00:08,  6.55it/s]predicting train subjects sagittal:  80%|███████▉  | 212/266 [00:35<00:08,  6.51it/s]predicting train subjects sagittal:  80%|████████  | 213/266 [00:35<00:08,  6.59it/s]predicting train subjects sagittal:  80%|████████  | 214/266 [00:35<00:07,  6.69it/s]predicting train subjects sagittal:  81%|████████  | 215/266 [00:35<00:07,  6.68it/s]predicting train subjects sagittal:  81%|████████  | 216/266 [00:35<00:07,  6.54it/s]predicting train subjects sagittal:  82%|████████▏ | 217/266 [00:35<00:07,  6.47it/s]predicting train subjects sagittal:  82%|████████▏ | 218/266 [00:36<00:07,  6.39it/s]predicting train subjects sagittal:  82%|████████▏ | 219/266 [00:36<00:07,  6.34it/s]predicting train subjects sagittal:  83%|████████▎ | 220/266 [00:36<00:07,  6.27it/s]predicting train subjects sagittal:  83%|████████▎ | 221/266 [00:36<00:07,  6.16it/s]predicting train subjects sagittal:  83%|████████▎ | 222/266 [00:36<00:07,  6.06it/s]predicting train subjects sagittal:  84%|████████▍ | 223/266 [00:36<00:06,  6.20it/s]predicting train subjects sagittal:  84%|████████▍ | 224/266 [00:37<00:06,  6.15it/s]predicting train subjects sagittal:  85%|████████▍ | 225/266 [00:37<00:06,  6.33it/s]predicting train subjects sagittal:  85%|████████▍ | 226/266 [00:37<00:06,  6.45it/s]predicting train subjects sagittal:  85%|████████▌ | 227/266 [00:37<00:05,  6.55it/s]predicting train subjects sagittal:  86%|████████▌ | 228/266 [00:37<00:05,  6.60it/s]predicting train subjects sagittal:  86%|████████▌ | 229/266 [00:37<00:05,  6.65it/s]predicting train subjects sagittal:  86%|████████▋ | 230/266 [00:37<00:05,  6.70it/s]predicting train subjects sagittal:  87%|████████▋ | 231/266 [00:38<00:05,  6.66it/s]predicting train subjects sagittal:  87%|████████▋ | 232/266 [00:38<00:05,  6.60it/s]predicting train subjects sagittal:  88%|████████▊ | 233/266 [00:38<00:05,  6.56it/s]predicting train subjects sagittal:  88%|████████▊ | 234/266 [00:38<00:04,  6.58it/s]predicting train subjects sagittal:  88%|████████▊ | 235/266 [00:38<00:04,  6.62it/s]predicting train subjects sagittal:  89%|████████▊ | 236/266 [00:38<00:04,  6.65it/s]predicting train subjects sagittal:  89%|████████▉ | 237/266 [00:38<00:04,  6.69it/s]predicting train subjects sagittal:  89%|████████▉ | 238/266 [00:39<00:04,  6.70it/s]predicting train subjects sagittal:  90%|████████▉ | 239/266 [00:39<00:04,  6.70it/s]predicting train subjects sagittal:  90%|█████████ | 240/266 [00:39<00:03,  6.71it/s]predicting train subjects sagittal:  91%|█████████ | 241/266 [00:39<00:03,  6.69it/s]predicting train subjects sagittal:  91%|█████████ | 242/266 [00:39<00:03,  6.69it/s]predicting train subjects sagittal:  91%|█████████▏| 243/266 [00:39<00:03,  6.69it/s]predicting train subjects sagittal:  92%|█████████▏| 244/266 [00:40<00:03,  6.66it/s]predicting train subjects sagittal:  92%|█████████▏| 245/266 [00:40<00:03,  6.67it/s]predicting train subjects sagittal:  92%|█████████▏| 246/266 [00:40<00:02,  6.69it/s]predicting train subjects sagittal:  93%|█████████▎| 247/266 [00:40<00:02,  6.68it/s]predicting train subjects sagittal:  93%|█████████▎| 248/266 [00:40<00:02,  6.65it/s]predicting train subjects sagittal:  94%|█████████▎| 249/266 [00:40<00:02,  6.35it/s]predicting train subjects sagittal:  94%|█████████▍| 250/266 [00:40<00:02,  6.19it/s]predicting train subjects sagittal:  94%|█████████▍| 251/266 [00:41<00:02,  5.87it/s]predicting train subjects sagittal:  95%|█████████▍| 252/266 [00:41<00:02,  5.72it/s]predicting train subjects sagittal:  95%|█████████▌| 253/266 [00:41<00:02,  5.60it/s]predicting train subjects sagittal:  95%|█████████▌| 254/266 [00:41<00:02,  5.46it/s]predicting train subjects sagittal:  96%|█████████▌| 255/266 [00:41<00:01,  5.51it/s]predicting train subjects sagittal:  96%|█████████▌| 256/266 [00:42<00:01,  5.50it/s]predicting train subjects sagittal:  97%|█████████▋| 257/266 [00:42<00:01,  5.52it/s]predicting train subjects sagittal:  97%|█████████▋| 258/266 [00:42<00:01,  5.29it/s]predicting train subjects sagittal:  97%|█████████▋| 259/266 [00:42<00:01,  5.25it/s]predicting train subjects sagittal:  98%|█████████▊| 260/266 [00:42<00:01,  5.26it/s]predicting train subjects sagittal:  98%|█████████▊| 261/266 [00:43<00:00,  5.30it/s]predicting train subjects sagittal:  98%|█████████▊| 262/266 [00:43<00:00,  5.43it/s]predicting train subjects sagittal:  99%|█████████▉| 263/266 [00:43<00:00,  5.51it/s]predicting train subjects sagittal:  99%|█████████▉| 264/266 [00:43<00:00,  5.59it/s]predicting train subjects sagittal: 100%|█████████▉| 265/266 [00:43<00:00,  5.66it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:43<00:00,  5.72it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:43<00:00,  6.06it/s]
saving BB  test1-THALAMUS:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 4/4 [00:00<00:00, 76.02it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 7/266 [00:00<00:03, 67.32it/s]saving BB  train1-THALAMUS:   5%|▌         | 14/266 [00:00<00:03, 66.47it/s]saving BB  train1-THALAMUS:   8%|▊         | 21/266 [00:00<00:03, 67.21it/s]saving BB  train1-THALAMUS:  11%|█         | 29/266 [00:00<00:03, 68.32it/s]saving BB  train1-THALAMUS:  14%|█▍        | 37/266 [00:00<00:03, 69.56it/s]saving BB  train1-THALAMUS:  17%|█▋        | 45/266 [00:00<00:03, 71.88it/s]saving BB  train1-THALAMUS:  20%|█▉        | 53/266 [00:00<00:02, 72.91it/s]saving BB  train1-THALAMUS:  23%|██▎       | 61/266 [00:00<00:02, 74.90it/s]saving BB  train1-THALAMUS:  26%|██▋       | 70/266 [00:00<00:02, 78.40it/s]saving BB  train1-THALAMUS:  30%|██▉       | 79/266 [00:01<00:02, 80.65it/s]saving BB  train1-THALAMUS:  33%|███▎      | 87/266 [00:01<00:02, 78.80it/s]saving BB  train1-THALAMUS:  36%|███▌      | 95/266 [00:01<00:02, 78.50it/s]saving BB  train1-THALAMUS:  39%|███▊      | 103/266 [00:01<00:02, 78.16it/s]saving BB  train1-THALAMUS:  42%|████▏     | 111/266 [00:01<00:02, 75.97it/s]saving BB  train1-THALAMUS:  45%|████▌     | 120/266 [00:01<00:01, 77.31it/s]saving BB  train1-THALAMUS:  48%|████▊     | 128/266 [00:01<00:01, 77.69it/s]saving BB  train1-THALAMUS:  51%|█████     | 136/266 [00:01<00:01, 76.49it/s]saving BB  train1-THALAMUS:  54%|█████▍    | 144/266 [00:01<00:01, 75.28it/s]saving BB  train1-THALAMUS:  57%|█████▋    | 152/266 [00:02<00:01, 75.27it/s]saving BB  train1-THALAMUS:  61%|██████    | 161/266 [00:02<00:01, 77.93it/s]saving BB  train1-THALAMUS:  64%|██████▍   | 170/266 [00:02<00:01, 80.22it/s]saving BB  train1-THALAMUS:  67%|██████▋   | 179/266 [00:02<00:01, 81.55it/s]saving BB  train1-THALAMUS:  71%|███████   | 188/266 [00:02<00:00, 82.17it/s]saving BB  train1-THALAMUS:  74%|███████▍  | 197/266 [00:02<00:00, 81.16it/s]saving BB  train1-THALAMUS:  77%|███████▋  | 206/266 [00:02<00:00, 79.93it/s]saving BB  train1-THALAMUS:  81%|████████  | 215/266 [00:02<00:00, 79.01it/s]saving BB  train1-THALAMUS:  84%|████████▍ | 223/266 [00:02<00:00, 78.97it/s]saving BB  train1-THALAMUS:  87%|████████▋ | 231/266 [00:02<00:00, 77.31it/s]saving BB  train1-THALAMUS:  90%|█████████ | 240/266 [00:03<00:00, 78.71it/s]saving BB  train1-THALAMUS:  93%|█████████▎| 248/266 [00:03<00:00, 78.06it/s]saving BB  train1-THALAMUS:  96%|█████████▌| 256/266 [00:03<00:00, 74.66it/s]saving BB  train1-THALAMUS:  99%|█████████▉| 264/266 [00:03<00:00, 73.68it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 76.71it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 4/4 [00:00<00:00, 73.96it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   3%|▎         | 8/266 [00:00<00:03, 68.16it/s]saving BB  train1-THALAMUS Sagittal:   6%|▌         | 15/266 [00:00<00:03, 66.82it/s]saving BB  train1-THALAMUS Sagittal:   8%|▊         | 22/266 [00:00<00:03, 65.73it/s]saving BB  train1-THALAMUS Sagittal:  11%|█▏        | 30/266 [00:00<00:03, 67.55it/s]saving BB  train1-THALAMUS Sagittal:  14%|█▍        | 38/266 [00:00<00:03, 69.32it/s]saving BB  train1-THALAMUS Sagittal:  17%|█▋        | 46/266 [00:00<00:03, 71.69it/s]saving BB  train1-THALAMUS Sagittal:  20%|██        | 54/266 [00:00<00:02, 72.70it/s]saving BB  train1-THALAMUS Sagittal:  24%|██▎       | 63/266 [00:00<00:02, 76.69it/s]saving BB  train1-THALAMUS Sagittal:  27%|██▋       | 71/266 [00:00<00:02, 76.48it/s]saving BB  train1-THALAMUS Sagittal:  30%|██▉       | 79/266 [00:01<00:02, 75.37it/s]saving BB  train1-THALAMUS Sagittal:  33%|███▎      | 87/266 [00:01<00:02, 71.40it/s]saving BB  train1-THALAMUS Sagittal:  36%|███▌      | 95/266 [00:01<00:02, 71.74it/s]saving BB  train1-THALAMUS Sagittal:  39%|███▊      | 103/266 [00:01<00:02, 73.66it/s]saving BB  train1-THALAMUS Sagittal:  42%|████▏     | 112/266 [00:01<00:02, 75.33it/s]saving BB  train1-THALAMUS Sagittal:  45%|████▌     | 120/266 [00:01<00:01, 76.36it/s]saving BB  train1-THALAMUS Sagittal:  48%|████▊     | 128/266 [00:01<00:01, 76.97it/s]saving BB  train1-THALAMUS Sagittal:  51%|█████     | 136/266 [00:01<00:01, 76.27it/s]saving BB  train1-THALAMUS Sagittal:  54%|█████▍    | 144/266 [00:01<00:01, 75.69it/s]saving BB  train1-THALAMUS Sagittal:  57%|█████▋    | 152/266 [00:02<00:01, 72.37it/s]saving BB  train1-THALAMUS Sagittal:  61%|██████    | 161/266 [00:02<00:01, 76.35it/s]saving BB  train1-THALAMUS Sagittal:  64%|██████▍   | 171/266 [00:02<00:01, 80.27it/s]saving BB  train1-THALAMUS Sagittal:  68%|██████▊   | 181/266 [00:02<00:01, 83.16it/s]saving BB  train1-THALAMUS Sagittal:  72%|███████▏  | 191/266 [00:02<00:00, 85.21it/s]saving BB  train1-THALAMUS Sagittal:  75%|███████▌  | 200/266 [00:02<00:00, 83.31it/s]saving BB  train1-THALAMUS Sagittal:  79%|███████▊  | 209/266 [00:02<00:00, 81.67it/s]saving BB  train1-THALAMUS Sagittal:  82%|████████▏ | 218/266 [00:02<00:00, 82.69it/s]saving BB  train1-THALAMUS Sagittal:  85%|████████▌ | 227/266 [00:02<00:00, 82.92it/s]saving BB  train1-THALAMUS Sagittal:  89%|████████▊ | 236/266 [00:03<00:00, 81.64it/s]saving BB  train1-THALAMUS Sagittal:  92%|█████████▏| 245/266 [00:03<00:00, 83.47it/s]saving BB  train1-THALAMUS Sagittal:  95%|█████████▌| 254/266 [00:03<00:00, 79.85it/s]saving BB  train1-THALAMUS Sagittal:  99%|█████████▉| 263/266 [00:03<00:00, 77.62it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 76.97it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<04:32,  1.03s/it]Loading train:   1%|          | 2/266 [00:01<04:22,  1.01it/s]Loading train:   1%|          | 3/266 [00:02<04:06,  1.07it/s]Loading train:   2%|▏         | 4/266 [00:03<03:52,  1.13it/s]Loading train:   2%|▏         | 5/266 [00:04<03:57,  1.10it/s]Loading train:   2%|▏         | 6/266 [00:05<03:34,  1.21it/s]Loading train:   3%|▎         | 7/266 [00:05<03:15,  1.32it/s]Loading train:   3%|▎         | 8/266 [00:06<03:03,  1.40it/s]Loading train:   3%|▎         | 9/266 [00:06<02:56,  1.45it/s]Loading train:   4%|▍         | 10/266 [00:07<02:50,  1.50it/s]Loading train:   4%|▍         | 11/266 [00:08<02:43,  1.56it/s]Loading train:   5%|▍         | 12/266 [00:08<02:40,  1.59it/s]Loading train:   5%|▍         | 13/266 [00:09<02:36,  1.62it/s]Loading train:   5%|▌         | 14/266 [00:09<02:39,  1.58it/s]Loading train:   6%|▌         | 15/266 [00:10<02:38,  1.58it/s]Loading train:   6%|▌         | 16/266 [00:11<02:39,  1.57it/s]Loading train:   6%|▋         | 17/266 [00:11<02:36,  1.59it/s]Loading train:   7%|▋         | 18/266 [00:12<02:36,  1.58it/s]Loading train:   7%|▋         | 19/266 [00:13<02:35,  1.59it/s]Loading train:   8%|▊         | 20/266 [00:13<02:33,  1.61it/s]Loading train:   8%|▊         | 21/266 [00:14<02:30,  1.63it/s]Loading train:   8%|▊         | 22/266 [00:14<02:30,  1.62it/s]Loading train:   9%|▊         | 23/266 [00:15<02:27,  1.65it/s]Loading train:   9%|▉         | 24/266 [00:16<02:23,  1.69it/s]Loading train:   9%|▉         | 25/266 [00:16<02:25,  1.66it/s]Loading train:  10%|▉         | 26/266 [00:17<02:28,  1.62it/s]Loading train:  10%|█         | 27/266 [00:17<02:23,  1.66it/s]Loading train:  11%|█         | 28/266 [00:18<02:22,  1.67it/s]Loading train:  11%|█         | 29/266 [00:19<02:21,  1.68it/s]Loading train:  11%|█▏        | 30/266 [00:19<02:20,  1.68it/s]Loading train:  12%|█▏        | 31/266 [00:20<02:18,  1.70it/s]Loading train:  12%|█▏        | 32/266 [00:20<02:13,  1.75it/s]Loading train:  12%|█▏        | 33/266 [00:21<02:12,  1.76it/s]Loading train:  13%|█▎        | 34/266 [00:21<02:12,  1.75it/s]Loading train:  13%|█▎        | 35/266 [00:22<02:14,  1.71it/s]Loading train:  14%|█▎        | 36/266 [00:23<02:15,  1.70it/s]Loading train:  14%|█▍        | 37/266 [00:23<02:12,  1.72it/s]Loading train:  14%|█▍        | 38/266 [00:24<02:11,  1.74it/s]Loading train:  15%|█▍        | 39/266 [00:24<02:09,  1.75it/s]Loading train:  15%|█▌        | 40/266 [00:25<02:08,  1.75it/s]Loading train:  15%|█▌        | 41/266 [00:26<02:10,  1.72it/s]Loading train:  16%|█▌        | 42/266 [00:26<02:09,  1.73it/s]Loading train:  16%|█▌        | 43/266 [00:27<02:07,  1.74it/s]Loading train:  17%|█▋        | 44/266 [00:27<02:03,  1.79it/s]Loading train:  17%|█▋        | 45/266 [00:28<02:00,  1.83it/s]Loading train:  17%|█▋        | 46/266 [00:28<01:58,  1.86it/s]Loading train:  18%|█▊        | 47/266 [00:29<01:57,  1.86it/s]Loading train:  18%|█▊        | 48/266 [00:29<01:59,  1.83it/s]Loading train:  18%|█▊        | 49/266 [00:30<02:02,  1.77it/s]Loading train:  19%|█▉        | 50/266 [00:31<02:02,  1.77it/s]Loading train:  19%|█▉        | 51/266 [00:31<01:59,  1.81it/s]Loading train:  20%|█▉        | 52/266 [00:32<01:56,  1.84it/s]Loading train:  20%|█▉        | 53/266 [00:32<01:55,  1.85it/s]Loading train:  20%|██        | 54/266 [00:33<01:54,  1.86it/s]Loading train:  21%|██        | 55/266 [00:33<01:52,  1.88it/s]Loading train:  21%|██        | 56/266 [00:34<01:52,  1.86it/s]Loading train:  21%|██▏       | 57/266 [00:34<01:52,  1.86it/s]Loading train:  22%|██▏       | 58/266 [00:35<01:54,  1.82it/s]Loading train:  22%|██▏       | 59/266 [00:35<01:54,  1.81it/s]Loading train:  23%|██▎       | 60/266 [00:36<01:50,  1.86it/s]Loading train:  23%|██▎       | 61/266 [00:36<01:47,  1.91it/s]Loading train:  23%|██▎       | 62/266 [00:37<01:44,  1.95it/s]Loading train:  24%|██▎       | 63/266 [00:37<01:42,  1.98it/s]Loading train:  24%|██▍       | 64/266 [00:38<01:42,  1.98it/s]Loading train:  24%|██▍       | 65/266 [00:38<01:42,  1.96it/s]Loading train:  25%|██▍       | 66/266 [00:39<01:41,  1.98it/s]Loading train:  25%|██▌       | 67/266 [00:39<01:41,  1.96it/s]Loading train:  26%|██▌       | 68/266 [00:40<01:38,  2.00it/s]Loading train:  26%|██▌       | 69/266 [00:40<01:37,  2.02it/s]Loading train:  26%|██▋       | 70/266 [00:41<01:35,  2.06it/s]Loading train:  27%|██▋       | 71/266 [00:41<01:34,  2.06it/s]Loading train:  27%|██▋       | 72/266 [00:42<01:34,  2.06it/s]Loading train:  27%|██▋       | 73/266 [00:42<01:34,  2.05it/s]Loading train:  28%|██▊       | 74/266 [00:43<01:34,  2.03it/s]Loading train:  28%|██▊       | 75/266 [00:43<01:36,  1.99it/s]Loading train:  29%|██▊       | 76/266 [00:44<01:35,  2.00it/s]Loading train:  29%|██▉       | 77/266 [00:44<01:34,  2.01it/s]Loading train:  29%|██▉       | 78/266 [00:45<01:40,  1.87it/s]Loading train:  30%|██▉       | 79/266 [00:46<01:43,  1.80it/s]Loading train:  30%|███       | 80/266 [00:46<01:44,  1.78it/s]Loading train:  30%|███       | 81/266 [00:47<01:44,  1.77it/s]Loading train:  31%|███       | 82/266 [00:47<01:44,  1.76it/s]Loading train:  31%|███       | 83/266 [00:48<01:43,  1.76it/s]Loading train:  32%|███▏      | 84/266 [00:48<01:45,  1.72it/s]Loading train:  32%|███▏      | 85/266 [00:49<01:44,  1.74it/s]Loading train:  32%|███▏      | 86/266 [00:50<01:47,  1.68it/s]Loading train:  33%|███▎      | 87/266 [00:50<01:47,  1.66it/s]Loading train:  33%|███▎      | 88/266 [00:51<01:46,  1.68it/s]Loading train:  33%|███▎      | 89/266 [00:51<01:44,  1.69it/s]Loading train:  34%|███▍      | 90/266 [00:52<01:40,  1.75it/s]Loading train:  34%|███▍      | 91/266 [00:52<01:39,  1.76it/s]Loading train:  35%|███▍      | 92/266 [00:53<01:39,  1.74it/s]Loading train:  35%|███▍      | 93/266 [00:54<01:39,  1.74it/s]Loading train:  35%|███▌      | 94/266 [00:54<01:38,  1.75it/s]Loading train:  36%|███▌      | 95/266 [00:55<01:36,  1.77it/s]Loading train:  36%|███▌      | 96/266 [00:56<01:48,  1.57it/s]Loading train:  36%|███▋      | 97/266 [00:56<02:00,  1.40it/s]Loading train:  37%|███▋      | 98/266 [00:57<02:03,  1.35it/s]Loading train:  37%|███▋      | 99/266 [00:58<01:59,  1.39it/s]Loading train:  38%|███▊      | 100/266 [00:59<02:02,  1.35it/s]Loading train:  38%|███▊      | 101/266 [00:59<01:51,  1.48it/s]Loading train:  38%|███▊      | 102/266 [01:00<01:44,  1.58it/s]Loading train:  39%|███▊      | 103/266 [01:00<01:38,  1.66it/s]Loading train:  39%|███▉      | 104/266 [01:01<01:32,  1.75it/s]Loading train:  39%|███▉      | 105/266 [01:01<01:28,  1.82it/s]Loading train:  40%|███▉      | 106/266 [01:02<01:25,  1.88it/s]Loading train:  40%|████      | 107/266 [01:02<01:23,  1.91it/s]Loading train:  41%|████      | 108/266 [01:03<01:23,  1.90it/s]Loading train:  41%|████      | 109/266 [01:03<01:22,  1.90it/s]Loading train:  41%|████▏     | 110/266 [01:04<01:22,  1.89it/s]Loading train:  42%|████▏     | 111/266 [01:04<01:21,  1.90it/s]Loading train:  42%|████▏     | 112/266 [01:05<01:19,  1.94it/s]Loading train:  42%|████▏     | 113/266 [01:05<01:17,  1.97it/s]Loading train:  43%|████▎     | 114/266 [01:06<01:16,  1.98it/s]Loading train:  43%|████▎     | 115/266 [01:06<01:15,  1.99it/s]Loading train:  44%|████▎     | 116/266 [01:07<01:14,  2.00it/s]Loading train:  44%|████▍     | 117/266 [01:07<01:13,  2.03it/s]Loading train:  44%|████▍     | 118/266 [01:08<01:12,  2.05it/s]Loading train:  45%|████▍     | 119/266 [01:08<01:15,  1.95it/s]Loading train:  45%|████▌     | 120/266 [01:09<01:15,  1.93it/s]Loading train:  45%|████▌     | 121/266 [01:09<01:16,  1.89it/s]Loading train:  46%|████▌     | 122/266 [01:10<01:16,  1.88it/s]Loading train:  46%|████▌     | 123/266 [01:11<01:18,  1.83it/s]Loading train:  47%|████▋     | 124/266 [01:11<01:17,  1.83it/s]Loading train:  47%|████▋     | 125/266 [01:12<01:16,  1.83it/s]Loading train:  47%|████▋     | 126/266 [01:12<01:16,  1.83it/s]Loading train:  48%|████▊     | 127/266 [01:13<01:15,  1.84it/s]Loading train:  48%|████▊     | 128/266 [01:13<01:15,  1.82it/s]Loading train:  48%|████▊     | 129/266 [01:14<01:15,  1.82it/s]Loading train:  49%|████▉     | 130/266 [01:14<01:14,  1.82it/s]Loading train:  49%|████▉     | 131/266 [01:15<01:14,  1.80it/s]Loading train:  50%|████▉     | 132/266 [01:16<01:15,  1.77it/s]Loading train:  50%|█████     | 133/266 [01:16<01:15,  1.75it/s]Loading train:  50%|█████     | 134/266 [01:17<01:15,  1.74it/s]Loading train:  51%|█████     | 135/266 [01:17<01:13,  1.78it/s]Loading train:  51%|█████     | 136/266 [01:18<01:11,  1.82it/s]Loading train:  52%|█████▏    | 137/266 [01:18<01:11,  1.81it/s]Loading train:  52%|█████▏    | 138/266 [01:19<01:11,  1.80it/s]Loading train:  52%|█████▏    | 139/266 [01:20<01:10,  1.79it/s]Loading train:  53%|█████▎    | 140/266 [01:20<01:10,  1.79it/s]Loading train:  53%|█████▎    | 141/266 [01:21<01:11,  1.75it/s]Loading train:  53%|█████▎    | 142/266 [01:21<01:10,  1.76it/s]Loading train:  54%|█████▍    | 143/266 [01:22<01:10,  1.74it/s]Loading train:  54%|█████▍    | 144/266 [01:22<01:11,  1.72it/s]Loading train:  55%|█████▍    | 145/266 [01:23<01:11,  1.69it/s]Loading train:  55%|█████▍    | 146/266 [01:24<01:09,  1.72it/s]Loading train:  55%|█████▌    | 147/266 [01:24<01:08,  1.74it/s]Loading train:  56%|█████▌    | 148/266 [01:25<01:07,  1.75it/s]Loading train:  56%|█████▌    | 149/266 [01:25<01:06,  1.76it/s]Loading train:  56%|█████▋    | 150/266 [01:26<01:05,  1.77it/s]Loading train:  57%|█████▋    | 151/266 [01:26<01:04,  1.79it/s]Loading train:  57%|█████▋    | 152/266 [01:27<01:03,  1.80it/s]Loading train:  58%|█████▊    | 153/266 [01:28<01:03,  1.77it/s]Loading train:  58%|█████▊    | 154/266 [01:28<01:04,  1.75it/s]Loading train:  58%|█████▊    | 155/266 [01:29<01:01,  1.81it/s]Loading train:  59%|█████▊    | 156/266 [01:29<00:59,  1.86it/s]Loading train:  59%|█████▉    | 157/266 [01:30<00:56,  1.94it/s]Loading train:  59%|█████▉    | 158/266 [01:30<00:54,  1.99it/s]Loading train:  60%|█████▉    | 159/266 [01:31<00:53,  2.00it/s]Loading train:  60%|██████    | 160/266 [01:31<00:52,  2.02it/s]Loading train:  61%|██████    | 161/266 [01:31<00:51,  2.06it/s]Loading train:  61%|██████    | 162/266 [01:32<00:50,  2.05it/s]Loading train:  61%|██████▏   | 163/266 [01:32<00:50,  2.04it/s]Loading train:  62%|██████▏   | 164/266 [01:33<00:49,  2.04it/s]Loading train:  62%|██████▏   | 165/266 [01:33<00:49,  2.02it/s]Loading train:  62%|██████▏   | 166/266 [01:34<00:49,  2.02it/s]Loading train:  63%|██████▎   | 167/266 [01:34<00:48,  2.04it/s]Loading train:  63%|██████▎   | 168/266 [01:35<00:47,  2.07it/s]Loading train:  64%|██████▎   | 169/266 [01:35<00:46,  2.11it/s]Loading train:  64%|██████▍   | 170/266 [01:36<00:44,  2.14it/s]Loading train:  64%|██████▍   | 171/266 [01:36<00:44,  2.14it/s]Loading train:  65%|██████▍   | 172/266 [01:37<00:43,  2.16it/s]Loading train:  65%|██████▌   | 173/266 [01:37<00:43,  2.13it/s]Loading train:  65%|██████▌   | 174/266 [01:38<00:43,  2.09it/s]Loading train:  66%|██████▌   | 175/266 [01:38<00:43,  2.11it/s]Loading train:  66%|██████▌   | 176/266 [01:39<00:43,  2.07it/s]Loading train:  67%|██████▋   | 177/266 [01:39<00:43,  2.04it/s]Loading train:  67%|██████▋   | 178/266 [01:40<00:43,  2.02it/s]Loading train:  67%|██████▋   | 179/266 [01:40<00:43,  2.02it/s]Loading train:  68%|██████▊   | 180/266 [01:41<00:42,  2.02it/s]Loading train:  68%|██████▊   | 181/266 [01:41<00:42,  2.02it/s]Loading train:  68%|██████▊   | 182/266 [01:42<00:41,  2.02it/s]Loading train:  69%|██████▉   | 183/266 [01:42<00:40,  2.03it/s]Loading train:  69%|██████▉   | 184/266 [01:43<00:40,  2.03it/s]Loading train:  70%|██████▉   | 185/266 [01:43<00:39,  2.03it/s]Loading train:  70%|██████▉   | 186/266 [01:44<00:39,  2.01it/s]Loading train:  70%|███████   | 187/266 [01:44<00:39,  1.98it/s]Loading train:  71%|███████   | 188/266 [01:45<00:40,  1.91it/s]Loading train:  71%|███████   | 189/266 [01:45<00:40,  1.90it/s]Loading train:  71%|███████▏  | 190/266 [01:46<00:38,  1.96it/s]Loading train:  72%|███████▏  | 191/266 [01:47<00:45,  1.63it/s]Loading train:  72%|███████▏  | 192/266 [01:47<00:48,  1.54it/s]Loading train:  73%|███████▎  | 193/266 [01:48<00:49,  1.48it/s]Loading train:  73%|███████▎  | 194/266 [01:49<00:53,  1.35it/s]Loading train:  73%|███████▎  | 195/266 [01:50<00:49,  1.44it/s]Loading train:  74%|███████▎  | 196/266 [01:50<00:45,  1.53it/s]Loading train:  74%|███████▍  | 197/266 [01:51<00:42,  1.61it/s]Loading train:  74%|███████▍  | 198/266 [01:51<00:40,  1.67it/s]Loading train:  75%|███████▍  | 199/266 [01:52<00:39,  1.71it/s]Loading train:  75%|███████▌  | 200/266 [01:52<00:37,  1.78it/s]Loading train:  76%|███████▌  | 201/266 [01:53<00:35,  1.81it/s]Loading train:  76%|███████▌  | 202/266 [01:53<00:35,  1.82it/s]Loading train:  76%|███████▋  | 203/266 [01:54<00:34,  1.83it/s]Loading train:  77%|███████▋  | 204/266 [01:54<00:33,  1.83it/s]Loading train:  77%|███████▋  | 205/266 [01:55<00:32,  1.88it/s]Loading train:  77%|███████▋  | 206/266 [01:55<00:32,  1.86it/s]Loading train:  78%|███████▊  | 207/266 [01:56<00:32,  1.83it/s]Loading train:  78%|███████▊  | 208/266 [01:57<00:32,  1.80it/s]Loading train:  79%|███████▊  | 209/266 [01:57<00:31,  1.79it/s]Loading train:  79%|███████▉  | 210/266 [01:58<00:31,  1.78it/s]Loading train:  79%|███████▉  | 211/266 [01:58<00:30,  1.78it/s]Loading train:  80%|███████▉  | 212/266 [01:59<00:29,  1.81it/s]Loading train:  80%|████████  | 213/266 [01:59<00:28,  1.83it/s]Loading train:  80%|████████  | 214/266 [02:00<00:27,  1.88it/s]Loading train:  81%|████████  | 215/266 [02:00<00:26,  1.92it/s]Loading train:  81%|████████  | 216/266 [02:01<00:26,  1.92it/s]Loading train:  82%|████████▏ | 217/266 [02:01<00:25,  1.93it/s]Loading train:  82%|████████▏ | 218/266 [02:02<00:24,  1.95it/s]Loading train:  82%|████████▏ | 219/266 [02:02<00:24,  1.95it/s]Loading train:  83%|████████▎ | 220/266 [02:03<00:23,  1.96it/s]Loading train:  83%|████████▎ | 221/266 [02:03<00:22,  1.97it/s]Loading train:  83%|████████▎ | 222/266 [02:04<00:22,  1.96it/s]Loading train:  84%|████████▍ | 223/266 [02:04<00:21,  1.96it/s]Loading train:  84%|████████▍ | 224/266 [02:08<00:56,  1.34s/it]Loading train:  85%|████████▍ | 225/266 [02:11<01:20,  1.96s/it]Loading train:  85%|████████▍ | 226/266 [02:15<01:39,  2.49s/it]Loading train:  85%|████████▌ | 227/266 [02:18<01:50,  2.82s/it]Loading train:  86%|████████▌ | 228/266 [02:22<01:56,  3.08s/it]Loading train:  86%|████████▌ | 229/266 [02:26<01:59,  3.24s/it]Loading train:  86%|████████▋ | 230/266 [02:29<02:01,  3.37s/it]Loading train:  87%|████████▋ | 231/266 [02:33<01:59,  3.41s/it]Loading train:  87%|████████▋ | 232/266 [02:37<01:57,  3.47s/it]Loading train:  88%|████████▊ | 233/266 [02:40<01:53,  3.45s/it]Loading train:  88%|████████▊ | 234/266 [02:43<01:50,  3.46s/it]Loading train:  88%|████████▊ | 235/266 [02:47<01:49,  3.52s/it]Loading train:  89%|████████▊ | 236/266 [02:51<01:45,  3.50s/it]Loading train:  89%|████████▉ | 237/266 [02:54<01:41,  3.49s/it]Loading train:  89%|████████▉ | 238/266 [02:57<01:36,  3.46s/it]Loading train:  90%|████████▉ | 239/266 [03:01<01:32,  3.43s/it]Loading train:  90%|█████████ | 240/266 [03:04<01:28,  3.42s/it]Loading train:  91%|█████████ | 241/266 [03:07<01:23,  3.35s/it]Loading train:  91%|█████████ | 242/266 [03:11<01:22,  3.45s/it]Loading train:  91%|█████████▏| 243/266 [03:15<01:20,  3.50s/it]Loading train:  92%|█████████▏| 244/266 [03:18<01:18,  3.58s/it]Loading train:  92%|█████████▏| 245/266 [03:22<01:15,  3.59s/it]Loading train:  92%|█████████▏| 246/266 [03:25<01:10,  3.55s/it]Loading train:  93%|█████████▎| 247/266 [03:29<01:06,  3.48s/it]Loading train:  93%|█████████▎| 248/266 [03:32<01:02,  3.47s/it]Loading train:  94%|█████████▎| 249/266 [03:41<01:24,  4.98s/it]Loading train:  94%|█████████▍| 250/266 [03:47<01:23,  5.21s/it]Loading train:  94%|█████████▍| 251/266 [03:52<01:18,  5.26s/it]Loading train:  95%|█████████▍| 252/266 [03:57<01:14,  5.33s/it]Loading train:  95%|█████████▌| 253/266 [04:02<01:07,  5.19s/it]Loading train:  95%|█████████▌| 254/266 [04:07<01:00,  5.00s/it]Loading train:  96%|█████████▌| 255/266 [04:18<01:15,  6.86s/it]Loading train:  96%|█████████▌| 256/266 [04:32<01:30,  9.02s/it]Loading train:  97%|█████████▋| 257/266 [04:46<01:34, 10.44s/it]Loading train:  97%|█████████▋| 258/266 [05:00<01:32, 11.51s/it]Loading train:  97%|█████████▋| 259/266 [05:13<01:23, 11.90s/it]Loading train:  98%|█████████▊| 260/266 [05:24<01:10, 11.72s/it]Loading train:  98%|█████████▊| 261/266 [05:35<00:57, 11.44s/it]Loading train:  98%|█████████▊| 262/266 [05:50<00:50, 12.56s/it]Loading train:  99%|█████████▉| 263/266 [06:05<00:40, 13.44s/it]Loading train:  99%|█████████▉| 264/266 [06:20<00:27, 13.65s/it]Loading train: 100%|█████████▉| 265/266 [06:29<00:12, 12.46s/it]Loading train: 100%|██████████| 266/266 [06:40<00:00, 11.82s/it]Loading train: 100%|██████████| 266/266 [06:40<00:00,  1.50s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   3%|▎         | 7/266 [00:00<00:04, 59.39it/s]concatenating: train:   5%|▍         | 13/266 [00:00<00:04, 59.19it/s]concatenating: train:   7%|▋         | 18/266 [00:00<00:04, 56.04it/s]concatenating: train:   9%|▉         | 24/266 [00:00<00:04, 56.40it/s]concatenating: train:  11%|█▏        | 30/266 [00:00<00:04, 56.94it/s]concatenating: train:  14%|█▎        | 36/266 [00:00<00:04, 56.42it/s]concatenating: train:  16%|█▌        | 42/266 [00:00<00:04, 54.79it/s]concatenating: train:  18%|█▊        | 48/266 [00:00<00:04, 52.56it/s]concatenating: train:  20%|█▉        | 53/266 [00:00<00:04, 51.70it/s]concatenating: train:  22%|██▏       | 58/266 [00:01<00:04, 50.18it/s]concatenating: train:  24%|██▍       | 64/266 [00:01<00:03, 51.95it/s]concatenating: train:  26%|██▋       | 70/266 [00:01<00:03, 53.64it/s]concatenating: train:  29%|██▊       | 76/266 [00:01<00:03, 54.82it/s]concatenating: train:  31%|███       | 82/266 [00:01<00:03, 54.73it/s]concatenating: train:  33%|███▎      | 88/266 [00:01<00:03, 55.88it/s]concatenating: train:  35%|███▌      | 94/266 [00:01<00:03, 55.49it/s]concatenating: train:  38%|███▊      | 100/266 [00:01<00:03, 55.17it/s]concatenating: train:  40%|███▉      | 106/266 [00:01<00:02, 54.33it/s]concatenating: train:  42%|████▏     | 113/266 [00:02<00:02, 56.64it/s]concatenating: train:  45%|████▍     | 119/266 [00:02<00:02, 56.58it/s]concatenating: train:  47%|████▋     | 126/266 [00:02<00:02, 58.02it/s]concatenating: train:  50%|█████     | 133/266 [00:02<00:02, 59.27it/s]concatenating: train:  53%|█████▎    | 140/266 [00:02<00:02, 58.62it/s]concatenating: train:  55%|█████▍    | 146/266 [00:02<00:02, 57.05it/s]concatenating: train:  57%|█████▋    | 152/266 [00:02<00:02, 56.58it/s]concatenating: train:  59%|█████▉    | 158/266 [00:02<00:01, 55.88it/s]concatenating: train:  62%|██████▏   | 164/266 [00:02<00:01, 54.71it/s]concatenating: train:  64%|██████▍   | 170/266 [00:03<00:01, 54.24it/s]concatenating: train:  66%|██████▌   | 176/266 [00:03<00:01, 55.33it/s]concatenating: train:  69%|██████▉   | 183/266 [00:03<00:01, 57.67it/s]concatenating: train:  71%|███████   | 189/266 [00:03<00:01, 56.00it/s]concatenating: train:  73%|███████▎  | 195/266 [00:03<00:01, 54.30it/s]concatenating: train:  76%|███████▌  | 201/266 [00:03<00:01, 51.06it/s]concatenating: train:  78%|███████▊  | 207/266 [00:03<00:01, 50.13it/s]concatenating: train:  80%|████████  | 213/266 [00:03<00:01, 49.62it/s]concatenating: train:  82%|████████▏ | 218/266 [00:04<00:00, 49.01it/s]concatenating: train:  84%|████████▍ | 223/266 [00:04<00:00, 48.82it/s]concatenating: train:  86%|████████▌ | 228/266 [00:04<00:00, 48.50it/s]concatenating: train:  88%|████████▊ | 233/266 [00:04<00:00, 48.74it/s]concatenating: train:  89%|████████▉ | 238/266 [00:04<00:00, 49.07it/s]concatenating: train:  92%|█████████▏| 244/266 [00:04<00:00, 49.28it/s]concatenating: train:  94%|█████████▍| 250/266 [00:04<00:00, 50.03it/s]concatenating: train:  96%|█████████▌| 256/266 [00:04<00:00, 50.66it/s]concatenating: train:  98%|█████████▊| 262/266 [00:04<00:00, 49.93it/s]concatenating: train: 100%|██████████| 266/266 [00:04<00:00, 53.35it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:33<01:40, 33.53s/it]Loading test:  50%|█████     | 2/4 [00:51<00:57, 28.99s/it]Loading test:  75%|███████▌  | 3/4 [01:03<00:23, 23.70s/it]Loading test: 100%|██████████| 4/4 [01:21<00:00, 22.19s/it]Loading test: 100%|██████████| 4/4 [01:21<00:00, 20.49s/it]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 57.85it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 0  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 0  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 0  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 84, 48, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 84, 48, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 84, 48, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 84, 48, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 84, 48, 40)   14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 84, 48, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 84, 48, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 42, 24, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 42, 24, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 42, 24, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 42, 24, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 42, 24, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 42, 24, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 42, 24, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 42, 24, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 42, 24, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 21, 12, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 21, 12, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 21, 12, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 21, 12, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 21, 12, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 21, 12, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 21, 12, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 21, 12, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 21, 12, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 21, 12, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 42, 24, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 42, 24, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 42, 24, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 42, 24, 80)   320         conv2d_7[0][0]                   2020-01-21 00:09:33.723094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 00:09:33.723196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 00:09:33.723209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 00:09:33.723217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 00:09:33.723554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
activation_7 (Activation)       (None, 42, 24, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 42, 24, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 42, 24, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 42, 24, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 42, 24, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 42, 24, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 84, 48, 40)   44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 84, 48, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 84, 48, 40)   28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 84, 48, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 84, 48, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 84, 48, 40)   14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 84, 48, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 84, 48, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 84, 48, 120)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 84, 48, 120)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 84, 48, 13)   1573        dropout_5[0][0]                  
==================================================================================================
Total params: 889,253
Trainable params: 887,653
Non-trainable params: 1,600
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0
------------------------------------------------------------------
class_weights [6.34327367e-02 3.28763067e-02 7.68767215e-02 9.55231921e-03
 2.76465870e-02 7.23295018e-03 8.42304097e-02 1.14264590e-01
 8.97202568e-02 1.36316292e-02 2.90890720e-01 1.89382812e-01
 2.61961451e-04]
Train on 10443 samples, validate on 152 samples
Epoch 1/300
 - 30s - loss: 0.5679 - acc: 0.9245 - mDice: 0.3874 - val_loss: 0.1896 - val_acc: 0.9445 - val_mDice: 0.2359

Epoch 00001: val_mDice improved from -inf to 0.23585, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/300
 - 25s - loss: 0.4607 - acc: 0.9377 - mDice: 0.5031 - val_loss: 0.5669 - val_acc: 0.9448 - val_mDice: 0.2758

Epoch 00002: val_mDice improved from 0.23585 to 0.27583, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 3/300
 - 26s - loss: 0.3955 - acc: 0.9417 - mDice: 0.5736 - val_loss: 0.3041 - val_acc: 0.9437 - val_mDice: 0.2992

Epoch 00003: val_mDice improved from 0.27583 to 0.29919, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/300
 - 25s - loss: 0.3417 - acc: 0.9443 - mDice: 0.6318 - val_loss: 0.1083 - val_acc: 0.9442 - val_mDice: 0.2980

Epoch 00004: val_mDice did not improve from 0.29919
Epoch 5/300
 - 25s - loss: 0.3244 - acc: 0.9458 - mDice: 0.6504 - val_loss: 0.0467 - val_acc: 0.9460 - val_mDice: 0.2997

Epoch 00005: val_mDice improved from 0.29919 to 0.29965, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 6/300
 - 25s - loss: 0.3129 - acc: 0.9472 - mDice: 0.6629 - val_loss: 0.1383 - val_acc: 0.9449 - val_mDice: 0.3028

Epoch 00006: val_mDice improved from 0.29965 to 0.30276, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 7/300
 - 25s - loss: 0.3025 - acc: 0.9482 - mDice: 0.6741 - val_loss: 0.1006 - val_acc: 0.9439 - val_mDice: 0.2997

Epoch 00007: val_mDice did not improve from 0.30276
Epoch 8/300
 - 25s - loss: 0.2998 - acc: 0.9487 - mDice: 0.6770 - val_loss: 0.0459 - val_acc: 0.9485 - val_mDice: 0.3109

Epoch 00008: val_mDice improved from 0.30276 to 0.31088, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 9/300
 - 25s - loss: 0.2951 - acc: 0.9494 - mDice: 0.6821 - val_loss: 0.0817 - val_acc: 0.9452 - val_mDice: 0.2978

Epoch 00009: val_mDice did not improve from 0.31088
Epoch 10/300
 - 25s - loss: 0.2901 - acc: 0.9498 - mDice: 0.6874 - val_loss: -6.8666e-03 - val_acc: 0.9487 - val_mDice: 0.3135

Epoch 00010: val_mDice improved from 0.31088 to 0.31352, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 11/300
 - 25s - loss: 0.2852 - acc: 0.9506 - mDice: 0.6927 - val_loss: 0.0587 - val_acc: 0.9451 - val_mDice: 0.3065

Epoch 00011: val_mDice did not improve from 0.31352
Epoch 12/300
 - 25s - loss: 0.2797 - acc: 0.9512 - mDice: 0.6987 - val_loss: 0.0440 - val_acc: 0.9467 - val_mDice: 0.3029

Epoch 00012: val_mDice did not improve from 0.31352
Epoch 13/300
 - 25s - loss: 0.2767 - acc: 0.9515 - mDice: 0.7019 - val_loss: -7.6032e-03 - val_acc: 0.9475 - val_mDice: 0.2991

Epoch 00013: val_mDice did not improve from 0.31352
Epoch 14/300
 - 25s - loss: 0.2751 - acc: 0.9519 - mDice: 0.7036 - val_loss: -4.9587e-03 - val_acc: 0.9476 - val_mDice: 0.3064

Epoch 00014: val_mDice did not improve from 0.31352
Epoch 15/300
 - 25s - loss: 0.2763 - acc: 0.9519 - mDice: 0.7023 - val_loss: 0.0226 - val_acc: 0.9464 - val_mDice: 0.3102

Epoch 00015: val_mDice did not improve from 0.31352
Epoch 16/300
 - 25s - loss: 0.2672 - acc: 0.9526 - mDice: 0.7121 - val_loss: 0.0123 - val_acc: 0.9495 - val_mDice: 0.3056

Epoch 00016: val_mDice did not improve from 0.31352
Epoch 17/300
 - 25s - loss: 0.2639 - acc: 0.9528 - mDice: 0.7157 - val_loss: -5.4116e-02 - val_acc: 0.9477 - val_mDice: 0.3078

Epoch 00017: val_mDice did not improve from 0.31352
Epoch 18/300
 - 25s - loss: 0.2681 - acc: 0.9530 - mDice: 0.7112 - val_loss: 0.0860 - val_acc: 0.9466 - val_mDice: 0.2891

Epoch 00018: val_mDice did not improve from 0.31352
Epoch 19/300
 - 25s - loss: 0.2621 - acc: 0.9533 - mDice: 0.7177 - val_loss: -3.4299e-02 - val_acc: 0.9512 - val_mDice: 0.3060

Epoch 00019: val_mDice did not improve from 0.31352
Epoch 20/300
 - 25s - loss: 0.2614 - acc: 0.9532 - mDice: 0.7184 - val_loss: 0.0615 - val_acc: 0.9460 - val_mDice: 0.3032

Epoch 00020: val_mDice did not improve from 0.31352
Epoch 21/300
 - 26s - loss: 0.2576 - acc: 0.9536 - mDice: 0.7224 - val_loss: 0.0134 - val_acc: 0.9480 - val_mDice: 0.3062

Epoch 00021: val_mDice did not improve from 0.31352
Epoch 22/300
 - 26s - loss: 0.2557 - acc: 0.9538 - mDice: 0.7245 - val_loss: -1.1539e-02 - val_acc: 0.9492 - val_mDice: 0.3064

Epoch 00022: val_mDice did not improve from 0.31352
Epoch 23/300
 - 27s - loss: 0.2587 - acc: 0.9537 - mDice: 0.7210 - val_loss: -4.6336e-02 - val_acc: 0.9487 - val_mDice: 0.3041

Epoch 00023: val_mDice did not improve from 0.31352
Epoch 24/300
 - 26s - loss: 0.2606 - acc: 0.9518 - mDice: 0.7110 - val_loss: -6.3854e-02 - val_acc: 0.9493 - val_mDice: 0.3029

Epoch 00024: val_mDice did not improve from 0.31352
Epoch 25/300
 - 26s - loss: 0.2902 - acc: 0.9482 - mDice: 0.6617 - val_loss: 0.0919 - val_acc: 0.9457 - val_mDice: 0.2783

Epoch 00025: val_mDice did not improve from 0.31352

Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 26/300
 - 27s - loss: 0.2693 - acc: 0.9500 - mDice: 0.6816 - val_loss: 0.0030 - val_acc: 0.9481 - val_mDice: 0.3032

Epoch 00026: val_mDice did not improve from 0.31352
Epoch 27/300
 - 27s - loss: 0.2538 - acc: 0.9504 - mDice: 0.6923 - val_loss: -2.8185e-02 - val_acc: 0.9489 - val_mDice: 0.3053

Epoch 00027: val_mDice did not improve from 0.31352
Epoch 28/300
 - 27s - loss: 0.2372 - acc: 0.9518 - mDice: 0.7041 - val_loss: -5.9924e-02 - val_acc: 0.9502 - val_mDice: 0.3081

Epoch 00028: val_mDice did not improve from 0.31352
Epoch 29/300
 - 27s - loss: 0.2338 - acc: 0.9525 - mDice: 0.7089 - val_loss: -2.9770e-02 - val_acc: 0.9510 - val_mDice: 0.3021

Epoch 00029: val_mDice did not improve from 0.31352
Epoch 30/300
 - 27s - loss: 0.2368 - acc: 0.9517 - mDice: 0.7035 - val_loss: -1.9607e-02 - val_acc: 0.9489 - val_mDice: 0.2994

Epoch 00030: val_mDice did not improve from 0.31352
Epoch 31/300
 - 27s - loss: 0.2245 - acc: 0.9526 - mDice: 0.7108 - val_loss: -4.8111e-02 - val_acc: 0.9478 - val_mDice: 0.2996

Epoch 00031: val_mDice did not improve from 0.31352
Epoch 32/300
 - 26s - loss: 0.2318 - acc: 0.9521 - mDice: 0.7080 - val_loss: -5.5113e-02 - val_acc: 0.9491 - val_mDice: 0.3050

Epoch 00032: val_mDice did not improve from 0.31352
Epoch 33/300
 - 26s - loss: 0.2225 - acc: 0.9530 - mDice: 0.7169 - val_loss: 0.0812 - val_acc: 0.9445 - val_mDice: 0.2977

Epoch 00033: val_mDice did not improve from 0.31352
Epoch 34/300
 - 25s - loss: 0.2220 - acc: 0.9529 - mDice: 0.7166 - val_loss: -3.4794e-02 - val_acc: 0.9475 - val_mDice: 0.2971

Epoch 00034: val_mDice did not improve from 0.31352
Epoch 35/300
 - 26s - loss: 0.2454 - acc: 0.9502 - mDice: 0.6952 - val_loss: -8.3456e-02 - val_acc: 0.9501 - val_mDice: 0.3037

Epoch 00035: val_mDice did not improve from 0.31352
Epoch 36/300
 - 26s - loss: 0.2250 - acc: 0.9526 - mDice: 0.7068 - val_loss: -6.2461e-02 - val_acc: 0.9505 - val_mDice: 0.3086

Epoch 00036: val_mDice did not improve from 0.31352
Epoch 37/300
 - 26s - loss: 0.2172 - acc: 0.9532 - mDice: 0.7203 - val_loss: -9.7603e-02 - val_acc: 0.9503 - val_mDice: 0.2981

Epoch 00037: val_mDice did not improve from 0.31352
Epoch 38/300
 - 26s - loss: 0.2173 - acc: 0.9531 - mDice: 0.7155 - val_loss: -7.9521e-02 - val_acc: 0.9511 - val_mDice: 0.3094

Epoch 00038: val_mDice did not improve from 0.31352
Epoch 39/300
 - 26s - loss: 0.2293 - acc: 0.9517 - mDice: 0.7006 - val_loss: -4.8634e-02 - val_acc: 0.9463 - val_mDice: 0.2978

Epoch 00039: val_mDice did not improve from 0.31352
Epoch 40/300
 - 26s - loss: 0.2303 - acc: 0.9520 - mDice: 0.6989 - val_loss: -4.6121e-02 - val_acc: 0.9491 - val_mDice: 0.3065

Epoch 00040: val_mDice did not improve from 0.31352

Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 41/300
 - 27s - loss: 0.2167 - acc: 0.9530 - mDice: 0.7121 - val_loss: -3.4644e-02 - val_acc: 0.9493 - val_mDice: 0.3072

Epoch 00041: val_mDice did not improve from 0.31352
Epoch 42/300
 - 26s - loss: 0.2161 - acc: 0.9530 - mDice: 0.7156 - val_loss: -4.0484e-02 - val_acc: 0.9491 - val_mDice: 0.3069

Epoch 00042: val_mDice did not improve from 0.31352
Epoch 43/300
 - 26s - loss: 0.2104 - acc: 0.9536 - mDice: 0.7200 - val_loss: -5.3226e-02 - val_acc: 0.9482 - val_mDice: 0.3038

Epoch 00043: val_mDice did not improve from 0.31352
Epoch 44/300
 - 26s - loss: 0.2100 - acc: 0.9534 - mDice: 0.7176 - val_loss: -2.2839e-02 - val_acc: 0.9487 - val_mDice: 0.2988

Epoch 00044: val_mDice did not improve from 0.31352
Epoch 45/300
 - 26s - loss: 0.2073 - acc: 0.9539 - mDice: 0.7262 - val_loss: -7.2010e-02 - val_acc: 0.9508 - val_mDice: 0.3048

Epoch 00045: val_mDice did not improve from 0.31352
Epoch 46/300
 - 26s - loss: 0.2063 - acc: 0.9539 - mDice: 0.7230 - val_loss: -7.9547e-02 - val_acc: 0.9503 - val_mDice: 0.3054

Epoch 00046: val_mDice did not improve from 0.31352
Epoch 47/300
 - 26s - loss: 0.2037 - acc: 0.9539 - mDice: 0.7219 - val_loss: -8.8477e-02 - val_acc: 0.9510 - val_mDice: 0.3040

Epoch 00047: val_mDice did not improve from 0.31352
Epoch 48/300
 - 26s - loss: 0.2021 - acc: 0.9541 - mDice: 0.7261 - val_loss: -8.0823e-02 - val_acc: 0.9504 - val_mDice: 0.3044

Epoch 00048: val_mDice did not improve from 0.31352
Epoch 49/300
 - 26s - loss: 0.2033 - acc: 0.9545 - mDice: 0.7289 - val_loss: -9.2371e-02 - val_acc: 0.9493 - val_mDice: 0.3005

Epoch 00049: val_mDice did not improve from 0.31352
Epoch 50/300
 - 27s - loss: 0.2000 - acc: 0.9544 - mDice: 0.7319 - val_loss: -8.0412e-02 - val_acc: 0.9504 - val_mDice: 0.3017

predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:01<00:04,  1.49s/it]predicting test subjects:  50%|█████     | 2/4 [00:02<00:02,  1.30s/it]predicting test subjects:  75%|███████▌  | 3/4 [00:03<00:01,  1.15s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.11s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:35,  2.79it/s]Loading train:   1%|          | 2/266 [00:00<01:31,  2.88it/s]Loading train:   1%|          | 3/266 [00:00<01:27,  3.02it/s]Loading train:   2%|▏         | 4/266 [00:01<01:22,  3.16it/s]Loading train:   2%|▏         | 5/266 [00:01<01:24,  3.08it/s]Loading train:   2%|▏         | 6/266 [00:01<01:29,  2.91it/s]Loading train:   3%|▎         | 7/266 [00:02<01:27,  2.96it/s]Loading train:   3%|▎         | 8/266 [00:02<01:25,  3.02it/s]Loading train:   3%|▎         | 9/266 [00:02<01:24,  3.03it/s]Loading train:   4%|▍         | 10/266 [00:03<01:23,  3.05it/s]Loading train:   4%|▍         | 11/266 [00:03<01:23,  3.07it/s]Loading train:   5%|▍         | 12/266 [00:03<01:23,  3.06it/s]Loading train:   5%|▍         | 13/266 [00:04<01:22,  3.05it/s]Loading train:   5%|▌         | 14/266 [00:04<01:22,  3.04it/s]Loading train:   6%|▌         | 15/266 [00:04<01:22,  3.04it/s]Loading train:   6%|▌         | 16/266 [00:05<01:21,  3.05it/s]Loading train:   6%|▋         | 17/266 [00:05<01:21,  3.05it/s]Loading train:   7%|▋         | 18/266 [00:05<01:21,  3.05it/s]Loading train:   7%|▋         | 19/266 [00:06<01:20,  3.06it/s]Loading train:   8%|▊         | 20/266 [00:06<01:21,  3.03it/s]Loading train:   8%|▊         | 21/266 [00:06<01:21,  3.02it/s]Loading train:   8%|▊         | 22/266 [00:07<01:21,  3.00it/s]Loading train:   9%|▊         | 23/266 [00:07<01:20,  3.02it/s]Loading train:   9%|▉         | 24/266 [00:07<01:18,  3.08it/s]Loading train:   9%|▉         | 25/266 [00:08<01:17,  3.13it/s]Loading train:  10%|▉         | 26/266 [00:08<01:15,  3.17it/s]Loading train:  10%|█         | 27/266 [00:08<01:15,  3.18it/s]Loading train:  11%|█         | 28/266 [00:09<01:15,  3.17it/s]Loading train:  11%|█         | 29/266 [00:09<01:14,  3.20it/s]Loading train:  11%|█▏        | 30/266 [00:09<01:14,  3.18it/s]Loading train:  12%|█▏        | 31/266 [00:10<01:13,  3.19it/s]Loading train:  12%|█▏        | 32/266 [00:10<01:12,  3.21it/s]Loading train:  12%|█▏        | 33/266 [00:10<01:12,  3.22it/s]Loading train:  13%|█▎        | 34/266 [00:10<01:11,  3.22it/s]Loading train:  13%|█▎        | 35/266 [00:11<01:11,  3.23it/s]Loading train:  14%|█▎        | 36/266 [00:11<01:10,  3.25it/s]Loading train:  14%|█▍        | 37/266 [00:11<01:11,  3.22it/s]Loading train:  14%|█▍        | 38/266 [00:12<01:10,  3.21it/s]Loading train:  15%|█▍        | 39/266 [00:12<01:10,  3.20it/s]Loading train:  15%|█▌        | 40/266 [00:12<01:10,  3.21it/s]Loading train:  15%|█▌        | 41/266 [00:13<01:10,  3.20it/s]Loading train:  16%|█▌        | 42/266 [00:13<01:09,  3.23it/s]Loading train:  16%|█▌        | 43/266 [00:13<01:07,  3.29it/s]Loading train:  17%|█▋        | 44/266 [00:14<01:05,  3.37it/s]Loading train:  17%|█▋        | 45/266 [00:14<01:04,  3.42it/s]Loading train:  17%|█▋        | 46/266 [00:14<01:03,  3.45it/s]Loading train:  18%|█▊        | 47/266 [00:14<01:02,  3.52it/s]Loading train:  18%|█▊        | 48/266 [00:15<01:01,  3.56it/s]Loading train:  18%|█▊        | 49/266 [00:15<01:00,  3.59it/s]Loading train:  19%|█▉        | 50/266 [00:15<00:59,  3.60it/s]Loading train:  19%|█▉        | 51/266 [00:15<00:59,  3.62it/s]Loading train:  20%|█▉        | 52/266 [00:16<00:59,  3.62it/s]Loading train:  20%|█▉        | 53/266 [00:16<00:58,  3.63it/s]Loading train:  20%|██        | 54/266 [00:16<00:58,  3.62it/s]Loading train:  21%|██        | 55/266 [00:17<00:59,  3.58it/s]Loading train:  21%|██        | 56/266 [00:17<00:58,  3.61it/s]Loading train:  21%|██▏       | 57/266 [00:17<00:57,  3.62it/s]Loading train:  22%|██▏       | 58/266 [00:17<00:57,  3.62it/s]Loading train:  22%|██▏       | 59/266 [00:18<00:56,  3.63it/s]Loading train:  23%|██▎       | 60/266 [00:18<00:56,  3.65it/s]Loading train:  23%|██▎       | 61/266 [00:18<00:55,  3.68it/s]Loading train:  23%|██▎       | 62/266 [00:18<00:54,  3.72it/s]Loading train:  24%|██▎       | 63/266 [00:19<00:53,  3.76it/s]Loading train:  24%|██▍       | 64/266 [00:19<00:53,  3.79it/s]Loading train:  24%|██▍       | 65/266 [00:19<00:53,  3.73it/s]Loading train:  25%|██▍       | 66/266 [00:20<00:53,  3.73it/s]Loading train:  25%|██▌       | 67/266 [00:20<00:53,  3.75it/s]Loading train:  26%|██▌       | 68/266 [00:20<00:52,  3.75it/s]Loading train:  26%|██▌       | 69/266 [00:20<00:52,  3.76it/s]Loading train:  26%|██▋       | 70/266 [00:21<00:51,  3.79it/s]Loading train:  27%|██▋       | 71/266 [00:21<00:51,  3.81it/s]Loading train:  27%|██▋       | 72/266 [00:21<00:50,  3.84it/s]Loading train:  27%|██▋       | 73/266 [00:21<00:50,  3.83it/s]Loading train:  28%|██▊       | 74/266 [00:22<00:50,  3.84it/s]Loading train:  28%|██▊       | 75/266 [00:22<00:49,  3.85it/s]Loading train:  29%|██▊       | 76/266 [00:22<00:49,  3.83it/s]Loading train:  29%|██▉       | 77/266 [00:22<00:49,  3.80it/s]Loading train:  29%|██▉       | 78/266 [00:23<00:52,  3.58it/s]Loading train:  30%|██▉       | 79/266 [00:23<00:53,  3.47it/s]Loading train:  30%|███       | 80/266 [00:23<00:55,  3.38it/s]Loading train:  30%|███       | 81/266 [00:24<00:55,  3.33it/s]Loading train:  31%|███       | 82/266 [00:24<00:56,  3.28it/s]Loading train:  31%|███       | 83/266 [00:24<00:56,  3.22it/s]Loading train:  32%|███▏      | 84/266 [00:25<00:56,  3.22it/s]Loading train:  32%|███▏      | 85/266 [00:25<00:56,  3.22it/s]Loading train:  32%|███▏      | 86/266 [00:25<00:56,  3.19it/s]Loading train:  33%|███▎      | 87/266 [00:26<00:55,  3.20it/s]Loading train:  33%|███▎      | 88/266 [00:26<00:55,  3.21it/s]Loading train:  33%|███▎      | 89/266 [00:26<00:55,  3.20it/s]Loading train:  34%|███▍      | 90/266 [00:27<00:55,  3.17it/s]Loading train:  34%|███▍      | 91/266 [00:27<00:55,  3.14it/s]Loading train:  35%|███▍      | 92/266 [00:27<00:55,  3.13it/s]Loading train:  35%|███▍      | 93/266 [00:27<00:54,  3.15it/s]Loading train:  35%|███▌      | 94/266 [00:28<00:54,  3.18it/s]Loading train:  36%|███▌      | 95/266 [00:28<00:53,  3.18it/s]Loading train:  36%|███▌      | 96/266 [00:28<00:54,  3.13it/s]Loading train:  36%|███▋      | 97/266 [00:29<00:54,  3.11it/s]Loading train:  37%|███▋      | 98/266 [00:29<00:53,  3.13it/s]Loading train:  37%|███▋      | 99/266 [00:29<00:51,  3.27it/s]Loading train:  38%|███▊      | 100/266 [00:30<00:49,  3.33it/s]Loading train:  38%|███▊      | 101/266 [00:30<00:49,  3.33it/s]Loading train:  38%|███▊      | 102/266 [00:30<00:48,  3.35it/s]Loading train:  39%|███▊      | 103/266 [00:31<00:48,  3.36it/s]Loading train:  39%|███▉      | 104/266 [00:31<00:48,  3.33it/s]Loading train:  39%|███▉      | 105/266 [00:31<00:47,  3.36it/s]Loading train:  40%|███▉      | 106/266 [00:31<00:47,  3.37it/s]Loading train:  40%|████      | 107/266 [00:32<00:47,  3.36it/s]Loading train:  41%|████      | 108/266 [00:32<00:46,  3.38it/s]Loading train:  41%|████      | 109/266 [00:32<00:46,  3.37it/s]Loading train:  41%|████▏     | 110/266 [00:33<00:46,  3.38it/s]Loading train:  42%|████▏     | 111/266 [00:33<00:45,  3.37it/s]Loading train:  42%|████▏     | 112/266 [00:33<00:46,  3.35it/s]Loading train:  42%|████▏     | 113/266 [00:33<00:45,  3.34it/s]Loading train:  43%|████▎     | 114/266 [00:34<00:45,  3.32it/s]Loading train:  43%|████▎     | 115/266 [00:34<00:45,  3.31it/s]Loading train:  44%|████▎     | 116/266 [00:34<00:45,  3.28it/s]Loading train:  44%|████▍     | 117/266 [00:35<00:45,  3.25it/s]Loading train:  44%|████▍     | 118/266 [00:35<00:46,  3.20it/s]Loading train:  45%|████▍     | 119/266 [00:35<00:48,  3.06it/s]Loading train:  45%|████▌     | 120/266 [00:36<00:49,  2.97it/s]Loading train:  45%|████▌     | 121/266 [00:36<00:49,  2.92it/s]Loading train:  46%|████▌     | 122/266 [00:36<00:48,  2.95it/s]Loading train:  46%|████▌     | 123/266 [00:37<00:48,  2.94it/s]Loading train:  47%|████▋     | 124/266 [00:37<00:47,  2.99it/s]Loading train:  47%|████▋     | 125/266 [00:37<00:46,  3.03it/s]Loading train:  47%|████▋     | 126/266 [00:38<00:45,  3.06it/s]Loading train:  48%|████▊     | 127/266 [00:38<00:45,  3.08it/s]Loading train:  48%|████▊     | 128/266 [00:38<00:44,  3.09it/s]Loading train:  48%|████▊     | 129/266 [00:39<00:44,  3.11it/s]Loading train:  49%|████▉     | 130/266 [00:39<00:43,  3.13it/s]Loading train:  49%|████▉     | 131/266 [00:39<00:43,  3.13it/s]Loading train:  50%|████▉     | 132/266 [00:40<00:43,  3.08it/s]Loading train:  50%|█████     | 133/266 [00:40<00:43,  3.08it/s]Loading train:  50%|█████     | 134/266 [00:40<00:43,  3.07it/s]Loading train:  51%|█████     | 135/266 [00:41<00:43,  3.02it/s]Loading train:  51%|█████     | 136/266 [00:41<00:42,  3.03it/s]Loading train:  52%|█████▏    | 137/266 [00:41<00:42,  3.04it/s]Loading train:  52%|█████▏    | 138/266 [00:42<00:40,  3.16it/s]Loading train:  52%|█████▏    | 139/266 [00:42<00:39,  3.23it/s]Loading train:  53%|█████▎    | 140/266 [00:42<00:38,  3.29it/s]Loading train:  53%|█████▎    | 141/266 [00:43<00:37,  3.31it/s]Loading train:  53%|█████▎    | 142/266 [00:43<00:37,  3.34it/s]Loading train:  54%|█████▍    | 143/266 [00:43<00:36,  3.35it/s]Loading train:  54%|█████▍    | 144/266 [00:43<00:36,  3.36it/s]Loading train:  55%|█████▍    | 145/266 [00:44<00:35,  3.40it/s]Loading train:  55%|█████▍    | 146/266 [00:44<00:35,  3.40it/s]Loading train:  55%|█████▌    | 147/266 [00:44<00:35,  3.39it/s]Loading train:  56%|█████▌    | 148/266 [00:45<00:34,  3.40it/s]Loading train:  56%|█████▌    | 149/266 [00:45<00:34,  3.40it/s]Loading train:  56%|█████▋    | 150/266 [00:45<00:34,  3.38it/s]Loading train:  57%|█████▋    | 151/266 [00:45<00:34,  3.36it/s]Loading train:  57%|█████▋    | 152/266 [00:46<00:34,  3.35it/s]Loading train:  58%|█████▊    | 153/266 [00:46<00:34,  3.31it/s]Loading train:  58%|█████▊    | 154/266 [00:46<00:33,  3.32it/s]Loading train:  58%|█████▊    | 155/266 [00:47<00:32,  3.45it/s]Loading train:  59%|█████▊    | 156/266 [00:47<00:31,  3.53it/s]Loading train:  59%|█████▉    | 157/266 [00:47<00:30,  3.62it/s]Loading train:  59%|█████▉    | 158/266 [00:47<00:29,  3.70it/s]Loading train:  60%|█████▉    | 159/266 [00:48<00:28,  3.72it/s]Loading train:  60%|██████    | 160/266 [00:48<00:28,  3.72it/s]Loading train:  61%|██████    | 161/266 [00:48<00:28,  3.75it/s]Loading train:  61%|██████    | 162/266 [00:48<00:27,  3.76it/s]Loading train:  61%|██████▏   | 163/266 [00:49<00:27,  3.77it/s]Loading train:  62%|██████▏   | 164/266 [00:49<00:27,  3.76it/s]Loading train:  62%|██████▏   | 165/266 [00:49<00:26,  3.75it/s]Loading train:  62%|██████▏   | 166/266 [00:50<00:26,  3.74it/s]Loading train:  63%|██████▎   | 167/266 [00:50<00:26,  3.77it/s]Loading train:  63%|██████▎   | 168/266 [00:50<00:26,  3.73it/s]Loading train:  64%|██████▎   | 169/266 [00:50<00:26,  3.73it/s]Loading train:  64%|██████▍   | 170/266 [00:51<00:25,  3.72it/s]Loading train:  64%|██████▍   | 171/266 [00:51<00:26,  3.63it/s]Loading train:  65%|██████▍   | 172/266 [00:51<00:26,  3.56it/s]Loading train:  65%|██████▌   | 173/266 [00:51<00:26,  3.51it/s]Loading train:  65%|██████▌   | 174/266 [00:52<00:26,  3.47it/s]Loading train:  66%|██████▌   | 175/266 [00:52<00:26,  3.47it/s]Loading train:  66%|██████▌   | 176/266 [00:52<00:25,  3.50it/s]Loading train:  67%|██████▋   | 177/266 [00:53<00:25,  3.53it/s]Loading train:  67%|██████▋   | 178/266 [00:53<00:24,  3.57it/s]Loading train:  67%|██████▋   | 179/266 [00:53<00:24,  3.59it/s]Loading train:  68%|██████▊   | 180/266 [00:53<00:23,  3.58it/s]Loading train:  68%|██████▊   | 181/266 [00:54<00:23,  3.63it/s]Loading train:  68%|██████▊   | 182/266 [00:54<00:23,  3.63it/s]Loading train:  69%|██████▉   | 183/266 [00:54<00:22,  3.65it/s]Loading train:  69%|██████▉   | 184/266 [00:55<00:22,  3.65it/s]Loading train:  70%|██████▉   | 185/266 [00:55<00:22,  3.67it/s]Loading train:  70%|██████▉   | 186/266 [00:55<00:21,  3.66it/s]Loading train:  70%|███████   | 187/266 [00:55<00:21,  3.65it/s]Loading train:  71%|███████   | 188/266 [00:56<00:21,  3.66it/s]Loading train:  71%|███████   | 189/266 [00:56<00:20,  3.67it/s]Loading train:  71%|███████▏  | 190/266 [00:56<00:20,  3.68it/s]Loading train:  72%|███████▏  | 191/266 [00:56<00:21,  3.55it/s]Loading train:  72%|███████▏  | 192/266 [00:57<00:20,  3.56it/s]Loading train:  73%|███████▎  | 193/266 [00:57<00:20,  3.49it/s]Loading train:  73%|███████▎  | 194/266 [00:57<00:21,  3.31it/s]Loading train:  73%|███████▎  | 195/266 [00:58<00:20,  3.38it/s]Loading train:  74%|███████▎  | 196/266 [00:58<00:20,  3.41it/s]Loading train:  74%|███████▍  | 197/266 [00:58<00:20,  3.42it/s]Loading train:  74%|███████▍  | 198/266 [00:59<00:19,  3.43it/s]Loading train:  75%|███████▍  | 199/266 [00:59<00:19,  3.42it/s]Loading train:  75%|███████▌  | 200/266 [00:59<00:18,  3.49it/s]Loading train:  76%|███████▌  | 201/266 [00:59<00:18,  3.55it/s]Loading train:  76%|███████▌  | 202/266 [01:00<00:17,  3.59it/s]Loading train:  76%|███████▋  | 203/266 [01:00<00:17,  3.59it/s]Loading train:  77%|███████▋  | 204/266 [01:00<00:17,  3.58it/s]Loading train:  77%|███████▋  | 205/266 [01:01<00:17,  3.56it/s]Loading train:  77%|███████▋  | 206/266 [01:01<00:17,  3.50it/s]Loading train:  78%|███████▊  | 207/266 [01:01<00:16,  3.54it/s]Loading train:  78%|███████▊  | 208/266 [01:01<00:16,  3.59it/s]Loading train:  79%|███████▊  | 209/266 [01:02<00:15,  3.60it/s]Loading train:  79%|███████▉  | 210/266 [01:02<00:15,  3.63it/s]Loading train:  79%|███████▉  | 211/266 [01:02<00:15,  3.61it/s]Loading train:  80%|███████▉  | 212/266 [01:02<00:14,  3.61it/s]Loading train:  80%|████████  | 213/266 [01:03<00:14,  3.69it/s]Loading train:  80%|████████  | 214/266 [01:03<00:13,  3.75it/s]Loading train:  81%|████████  | 215/266 [01:03<00:13,  3.77it/s]Loading train:  81%|████████  | 216/266 [01:03<00:13,  3.81it/s]Loading train:  82%|████████▏ | 217/266 [01:04<00:12,  3.84it/s]Loading train:  82%|████████▏ | 218/266 [01:04<00:12,  3.85it/s]Loading train:  82%|████████▏ | 219/266 [01:04<00:12,  3.87it/s]Loading train:  83%|████████▎ | 220/266 [01:05<00:11,  3.86it/s]Loading train:  83%|████████▎ | 221/266 [01:05<00:11,  3.85it/s]Loading train:  83%|████████▎ | 222/266 [01:05<00:11,  3.87it/s]Loading train:  84%|████████▍ | 223/266 [01:05<00:11,  3.90it/s]Loading train:  84%|████████▍ | 224/266 [01:06<00:10,  3.90it/s]Loading train:  85%|████████▍ | 225/266 [01:06<00:10,  3.90it/s]Loading train:  85%|████████▍ | 226/266 [01:06<00:10,  3.91it/s]Loading train:  85%|████████▌ | 227/266 [01:06<00:09,  3.90it/s]Loading train:  86%|████████▌ | 228/266 [01:07<00:09,  3.89it/s]Loading train:  86%|████████▌ | 229/266 [01:07<00:09,  3.84it/s]Loading train:  86%|████████▋ | 230/266 [01:07<00:09,  3.78it/s]Loading train:  87%|████████▋ | 231/266 [01:07<00:09,  3.74it/s]Loading train:  87%|████████▋ | 232/266 [01:08<00:09,  3.65it/s]Loading train:  88%|████████▊ | 233/266 [01:08<00:09,  3.66it/s]Loading train:  88%|████████▊ | 234/266 [01:08<00:08,  3.67it/s]Loading train:  88%|████████▊ | 235/266 [01:08<00:08,  3.69it/s]Loading train:  89%|████████▊ | 236/266 [01:09<00:08,  3.69it/s]Loading train:  89%|████████▉ | 237/266 [01:09<00:07,  3.69it/s]Loading train:  89%|████████▉ | 238/266 [01:09<00:07,  3.69it/s]Loading train:  90%|████████▉ | 239/266 [01:10<00:07,  3.71it/s]Loading train:  90%|█████████ | 240/266 [01:10<00:06,  3.73it/s]Loading train:  91%|█████████ | 241/266 [01:10<00:06,  3.73it/s]Loading train:  91%|█████████ | 242/266 [01:10<00:06,  3.73it/s]Loading train:  91%|█████████▏| 243/266 [01:11<00:06,  3.74it/s]Loading train:  92%|█████████▏| 244/266 [01:11<00:05,  3.71it/s]Loading train:  92%|█████████▏| 245/266 [01:11<00:05,  3.68it/s]Loading train:  92%|█████████▏| 246/266 [01:11<00:05,  3.69it/s]Loading train:  93%|█████████▎| 247/266 [01:12<00:05,  3.69it/s]Loading train:  93%|█████████▎| 248/266 [01:12<00:04,  3.72it/s]Loading train:  94%|█████████▎| 249/266 [01:12<00:04,  3.52it/s]Loading train:  94%|█████████▍| 250/266 [01:13<00:04,  3.38it/s]Loading train:  94%|█████████▍| 251/266 [01:13<00:04,  3.31it/s]Loading train:  95%|█████████▍| 252/266 [01:13<00:04,  3.24it/s]Loading train:  95%|█████████▌| 253/266 [01:14<00:04,  3.07it/s]Loading train:  95%|█████████▌| 254/266 [01:14<00:03,  3.09it/s]Loading train:  96%|█████████▌| 255/266 [01:14<00:03,  3.15it/s]Loading train:  96%|█████████▌| 256/266 [01:15<00:03,  3.18it/s]Loading train:  97%|█████████▋| 257/266 [01:15<00:02,  3.18it/s]Loading train:  97%|█████████▋| 258/266 [01:15<00:02,  3.18it/s]Loading train:  97%|█████████▋| 259/266 [01:16<00:02,  3.17it/s]Loading train:  98%|█████████▊| 260/266 [01:16<00:01,  3.18it/s]Loading train:  98%|█████████▊| 261/266 [01:16<00:01,  3.17it/s]Loading train:  98%|█████████▊| 262/266 [01:16<00:01,  3.19it/s]Loading train:  99%|█████████▉| 263/266 [01:17<00:00,  3.18it/s]Loading train:  99%|█████████▉| 264/266 [01:17<00:00,  3.13it/s]Loading train: 100%|█████████▉| 265/266 [01:17<00:00,  3.14it/s]Loading train: 100%|██████████| 266/266 [01:18<00:00,  3.10it/s]Loading train: 100%|██████████| 266/266 [01:18<00:00,  3.40it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:05, 48.38it/s]concatenating: train:   4%|▍         | 10/266 [00:00<00:05, 47.48it/s]concatenating: train:   6%|▌         | 15/266 [00:00<00:05, 45.10it/s]concatenating: train:   8%|▊         | 20/266 [00:00<00:05, 44.86it/s]concatenating: train:   9%|▉         | 25/266 [00:00<00:05, 44.35it/s]concatenating: train:  11%|█▏        | 30/266 [00:00<00:05, 44.85it/s]concatenating: train:  13%|█▎        | 35/266 [00:00<00:05, 44.99it/s]concatenating: train:  15%|█▌        | 40/266 [00:00<00:05, 44.98it/s]concatenating: train:  17%|█▋        | 45/266 [00:01<00:04, 45.22it/s]concatenating: train:  19%|█▉        | 51/266 [00:01<00:04, 47.41it/s]concatenating: train:  21%|██▏       | 57/266 [00:01<00:04, 48.58it/s]concatenating: train:  24%|██▎       | 63/266 [00:01<00:04, 50.12it/s]concatenating: train:  26%|██▌       | 69/266 [00:01<00:03, 50.90it/s]concatenating: train:  28%|██▊       | 75/266 [00:01<00:03, 52.22it/s]concatenating: train:  30%|███       | 81/266 [00:01<00:03, 51.12it/s]concatenating: train:  33%|███▎      | 87/266 [00:01<00:03, 49.12it/s]concatenating: train:  35%|███▍      | 93/266 [00:01<00:03, 48.93it/s]concatenating: train:  37%|███▋      | 98/266 [00:02<00:03, 48.77it/s]concatenating: train:  39%|███▉      | 104/266 [00:02<00:03, 49.58it/s]concatenating: train:  41%|████      | 109/266 [00:02<00:03, 49.63it/s]concatenating: train:  43%|████▎     | 114/266 [00:02<00:03, 48.87it/s]concatenating: train:  45%|████▌     | 120/266 [00:02<00:02, 48.90it/s]concatenating: train:  47%|████▋     | 126/266 [00:02<00:02, 49.18it/s]concatenating: train:  49%|████▉     | 131/266 [00:02<00:02, 48.84it/s]concatenating: train:  51%|█████     | 136/266 [00:02<00:02, 46.23it/s]concatenating: train:  53%|█████▎    | 141/266 [00:02<00:02, 46.66it/s]concatenating: train:  55%|█████▌    | 147/266 [00:03<00:02, 49.34it/s]concatenating: train:  57%|█████▋    | 152/266 [00:03<00:02, 47.75it/s]concatenating: train:  59%|█████▉    | 158/266 [00:03<00:02, 49.78it/s]concatenating: train:  62%|██████▏   | 164/266 [00:03<00:02, 49.39it/s]concatenating: train:  64%|██████▎   | 169/266 [00:03<00:01, 49.21it/s]concatenating: train:  65%|██████▌   | 174/266 [00:03<00:01, 49.13it/s]concatenating: train:  67%|██████▋   | 179/266 [00:03<00:01, 48.42it/s]concatenating: train:  70%|██████▉   | 185/266 [00:03<00:01, 48.88it/s]concatenating: train:  71%|███████▏  | 190/266 [00:03<00:01, 49.15it/s]concatenating: train:  74%|███████▎  | 196/266 [00:04<00:01, 51.11it/s]concatenating: train:  76%|███████▋  | 203/266 [00:04<00:01, 53.61it/s]concatenating: train:  79%|███████▊  | 209/266 [00:04<00:01, 53.45it/s]concatenating: train:  81%|████████  | 215/266 [00:04<00:01, 49.62it/s]concatenating: train:  83%|████████▎ | 221/266 [00:04<00:00, 48.90it/s]concatenating: train:  85%|████████▍ | 226/266 [00:04<00:00, 47.95it/s]concatenating: train:  87%|████████▋ | 231/266 [00:04<00:00, 47.40it/s]concatenating: train:  89%|████████▉ | 237/266 [00:04<00:00, 49.34it/s]concatenating: train:  91%|█████████▏| 243/266 [00:04<00:00, 50.87it/s]concatenating: train:  94%|█████████▎| 249/266 [00:05<00:00, 49.94it/s]concatenating: train:  96%|█████████▌| 255/266 [00:05<00:00, 47.32it/s]concatenating: train:  98%|█████████▊| 260/266 [00:05<00:00, 45.78it/s]concatenating: train: 100%|█████████▉| 265/266 [00:05<00:00, 44.06it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 48.41it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:00,  3.08it/s]Loading test:  50%|█████     | 2/4 [00:00<00:00,  3.19it/s]Loading test:  75%|███████▌  | 3/4 [00:00<00:00,  3.34it/s]Loading test: 100%|██████████| 4/4 [00:01<00:00,  3.30it/s]Loading test: 100%|██████████| 4/4 [00:01<00:00,  3.36it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 354.41it/s]
Epoch 00050: val_mDice did not improve from 0.31352
Restoring model weights from the end of the best epoch
Epoch 00050: early stopping
{'val_loss': [0.18957715678813034, 0.5669213600064579, 0.3041415340113944, 0.10830922148034006, 0.0466736520787603, 0.13826959192948907, 0.10057801841513107, 0.04590664929908503, 0.0817386440895988, -0.00686664612179524, 0.058683317301696854, 0.044022377531387304, -0.007603220666121496, -0.004958703917892356, 0.02260415192301336, 0.012332959422277972, -0.05411599424520606, 0.08595219883136451, -0.03429933649634844, 0.06145582186352266, 0.013415684059605394, -0.01153937888968932, -0.04633555609095646, -0.06385424762572113, 0.09187024456791971, 0.0030495914091405118, -0.02818456098535343, -0.05992378500339232, -0.029770486142584367, -0.01960748934039944, -0.04811084949362435, -0.055112952390979776, 0.08123210731795744, -0.034794411646496305, -0.08345644906359284, -0.06246147597649772, -0.09760311194756803, -0.0795209905818889, -0.048634475149140745, -0.04612147462505259, -0.034644186508988865, -0.04048362947804363, -0.053226445703522154, -0.022838985473897896, -0.07200996389024351, -0.07954726575285588, -0.08847660385684944, -0.08082326491498143, -0.0923706623970678, -0.08041161048765245], 'val_acc': [0.944516236060544, 0.9448148357240778, 0.9436922371387482, 0.9441768464289213, 0.9460108562519676, 0.9448849943123365, 0.9438782467653877, 0.9485008057795072, 0.9451901293114612, 0.9486672329275232, 0.9450595872966867, 0.9466521159598702, 0.9474826353160959, 0.9475723889313246, 0.9463975664816404, 0.9494863458369908, 0.9477420849235434, 0.9466243747033571, 0.9511539253749346, 0.9459961699812036, 0.947985191094248, 0.9492138632034001, 0.9486688775451559, 0.9493427558949119, 0.9456518888473511, 0.9480749392195752, 0.9488761001511624, 0.95023365710911, 0.9509940296411514, 0.9488532464755209, 0.9478106083054292, 0.9490735287729063, 0.9444933941489772, 0.947458164472329, 0.9501227038471323, 0.9505338817834854, 0.9503348179553684, 0.9511457665970451, 0.9462621306118212, 0.9490572159227572, 0.9492954423553065, 0.949057212001399, 0.9482038460279766, 0.9486574530601501, 0.9508112668991089, 0.9503494995205026, 0.9509630077763608, 0.9503690827833978, 0.949328075898321, 0.950444149343591], 'val_mDice': [0.23585067569480375, 0.27583460911716284, 0.2991904994766963, 0.29799369016760274, 0.29965493975109175, 0.30276156128629256, 0.2997463000448127, 0.3108793707858575, 0.29782599916583613, 0.31352133294077295, 0.3065306899186812, 0.30289620602209316, 0.29907212837746266, 0.30638419140718487, 0.310236381268815, 0.30558323928792225, 0.30782504399356087, 0.28912104764267016, 0.30598960473741355, 0.30321691871473666, 0.30617841764500264, 0.30637512179581744, 0.30405797260372264, 0.30293264965477745, 0.2783074911291662, 0.3031743847225842, 0.3053238340898564, 0.3081162680724734, 0.30211773259859337, 0.2994376450384918, 0.29964092628736244, 0.3050492556863709, 0.2976846361630841, 0.29713921003827926, 0.3037252360464711, 0.30858049071148824, 0.2981100167687002, 0.3094115696455303, 0.29778263776710157, 0.3065221487103324, 0.3072204769245888, 0.30694319474461834, 0.3037789900247988, 0.29882080460849564, 0.304828587135202, 0.3053689346109566, 0.30399278002349955, 0.3044210158680615, 0.3004540772618432, 0.30171000839848267], 'loss': [0.5679490093291976, 0.46070626619776966, 0.39550300314201997, 0.3416820919960038, 0.32441669457102373, 0.3128689061456127, 0.30246980592607026, 0.29978645237414736, 0.29508972866866456, 0.2900954257804955, 0.28519477111551483, 0.27966533447321057, 0.27670109962910217, 0.2750826789012508, 0.2762983818084107, 0.2672420901282131, 0.263860841560761, 0.2680637186016261, 0.2620735293996068, 0.26138530501366847, 0.2575724532061309, 0.25566028155130316, 0.2586806168030428, 0.26058519959170096, 0.2902278725935743, 0.26932577566874843, 0.2538197280894913, 0.23722972266891845, 0.23382894426547143, 0.23680172568954258, 0.22451036444799263, 0.2318339157529567, 0.2224932151473274, 0.22204912927499437, 0.24535895770910265, 0.22496105485164036, 0.21719576346309932, 0.21729609512039158, 0.22925236741098537, 0.2303212485081301, 0.21669935501951743, 0.21614669343183687, 0.21040129730762686, 0.2100095571945842, 0.20726080774764363, 0.2062507698500441, 0.20373083570582703, 0.20213787395755836, 0.2032648117070939, 0.20003945379981092], 'acc': [0.9245071073641417, 0.9377190185542948, 0.9416892186647675, 0.9443163873534498, 0.9457715898880634, 0.9471762761934546, 0.9481996643204211, 0.9486594547100318, 0.9494170179474826, 0.949832252829191, 0.9506255569191348, 0.9512357995917169, 0.9515038598466168, 0.9518915235476854, 0.9518725945610979, 0.9526196113037307, 0.9528232621390917, 0.9530185783981284, 0.9533124309406648, 0.9532167198876477, 0.953628394032511, 0.9538423288750897, 0.9537413940054811, 0.9518263308547368, 0.9481635185981132, 0.9500410100526306, 0.9504324255929782, 0.9517663158551265, 0.9525468185134492, 0.9517005054838616, 0.95259951788336, 0.9521025617860303, 0.9529932614408393, 0.9529121097312466, 0.9502332625256226, 0.9525571020811805, 0.9531915698781571, 0.9530847448390825, 0.9517147546068846, 0.9519696347354804, 0.9529982487451958, 0.9530089600132101, 0.9536224795885496, 0.9534366659810805, 0.9539091604303653, 0.9538705675843734, 0.95387277575867, 0.9541073496070251, 0.9544965326300894, 0.9543505921042378], 'mDice': [0.3873624613876545, 0.5030632233906213, 0.5735681486702805, 0.6317822190236511, 0.6504359969653972, 0.6628858064450512, 0.6741117394816358, 0.676986235209515, 0.6820553890518173, 0.687448399523306, 0.6927345242751897, 0.6987014661361052, 0.70189352169309, 0.7036236152958963, 0.7023004687153196, 0.7120890581298466, 0.715748776412839, 0.711179466884757, 0.7176669909387191, 0.7183823445687906, 0.7224128771408141, 0.7245307671867385, 0.7209892735123509, 0.7109773168730048, 0.6617069906297114, 0.6815707792236261, 0.6923015040021672, 0.7041472130884308, 0.7088576785397463, 0.7035118800309795, 0.7107556442882005, 0.7080117360700607, 0.7168572038785943, 0.716604301867375, 0.6952145542493899, 0.7067959717289648, 0.7203271334556902, 0.7154933748021044, 0.7006217893981779, 0.6989043608834367, 0.7120944784539853, 0.7156043393218311, 0.7199781508803265, 0.7176171950393055, 0.7262070467962246, 0.7229697494988331, 0.7218749433667406, 0.7261127107452526, 0.7288999258701365, 0.7319467975086925], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 76, 108, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 76, 108, 30)  300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 76, 108, 30)  120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 76, 108, 30)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 76, 108, 30)  8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 76, 108, 30)  120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 76, 108, 30)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 38, 54, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 38, 54, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 38, 54, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 38, 54, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 38, 54, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 38, 54, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 38, 54, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 38, 54, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 38, 54, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 19, 27, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 19, 27, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 19, 27, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 19, 27, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 19, 27, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 19, 27, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 19, 27, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 19, 27, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 19, 27, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 19, 27, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 38, 54, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 38, 54, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 38, 54, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 38, 54, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 38, 54, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 38, 54, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 38, 54, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 38, 54, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 38, 54, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 38, 54, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 76, 108, 30)  25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 76, 108, 60)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 76, 108, 30)  16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 76, 108, 30)  120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 76, 108, 30)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 76, 108, 30)  8130        activation_9[0][0]               
__________________________________________________________________________________________________2020-01-21 00:35:48.948791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 00:35:48.948878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 00:35:48.948892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 00:35:48.948899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 00:35:48.949212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

batch_normalization_10 (BatchNo (None, 76, 108, 30)  120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 76, 108, 30)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 76, 108, 90)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 76, 108, 90)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 76, 108, 2)   182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 499,142
Non-trainable params: 1,200
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1
------------------------------------------------------------------
class_weights [0.97453182 0.02546818]
Train on 27987 samples, validate on 396 samples
Epoch 1/300
 - 74s - loss: 0.0781 - acc: 0.9918 - mDice: 0.8481 - val_loss: -4.6525e-02 - val_acc: 0.9944 - val_mDice: 0.5635

Epoch 00001: val_mDice improved from -inf to 0.56349, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300
 - 71s - loss: 0.0504 - acc: 0.9944 - mDice: 0.9021 - val_loss: -4.7200e-02 - val_acc: 0.9949 - val_mDice: 0.5618

Epoch 00002: val_mDice did not improve from 0.56349
Epoch 3/300
 - 71s - loss: 0.0452 - acc: 0.9949 - mDice: 0.9121 - val_loss: -7.4612e-02 - val_acc: 0.9945 - val_mDice: 0.5664

Epoch 00003: val_mDice improved from 0.56349 to 0.56641, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 4/300
 - 71s - loss: 0.0421 - acc: 0.9952 - mDice: 0.9183 - val_loss: -7.5502e-02 - val_acc: 0.9949 - val_mDice: 0.5679

Epoch 00004: val_mDice improved from 0.56641 to 0.56793, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 5/300
 - 71s - loss: 0.0401 - acc: 0.9954 - mDice: 0.9221 - val_loss: -7.0718e-02 - val_acc: 0.9947 - val_mDice: 0.5585

Epoch 00005: val_mDice did not improve from 0.56793
Epoch 6/300
 - 70s - loss: 0.0383 - acc: 0.9955 - mDice: 0.9256 - val_loss: -7.3582e-02 - val_acc: 0.9948 - val_mDice: 0.5611

Epoch 00006: val_mDice did not improve from 0.56793
Epoch 7/300
 - 71s - loss: 0.0371 - acc: 0.9957 - mDice: 0.9280 - val_loss: -6.7569e-02 - val_acc: 0.9947 - val_mDice: 0.5586

Epoch 00007: val_mDice did not improve from 0.56793
Epoch 8/300
 - 70s - loss: 0.0359 - acc: 0.9958 - mDice: 0.9303 - val_loss: -8.4625e-02 - val_acc: 0.9949 - val_mDice: 0.5357

Epoch 00008: val_mDice did not improve from 0.56793
Epoch 9/300
 - 71s - loss: 0.0355 - acc: 0.9958 - mDice: 0.9312 - val_loss: -4.9119e-02 - val_acc: 0.9943 - val_mDice: 0.5547

Epoch 00009: val_mDice did not improve from 0.56793
Epoch 10/300
 - 71s - loss: 0.0343 - acc: 0.9959 - mDice: 0.9335 - val_loss: -6.2202e-02 - val_acc: 0.9947 - val_mDice: 0.5395

Epoch 00010: val_mDice did not improve from 0.56793
Epoch 11/300
 - 71s - loss: 0.0339 - acc: 0.9960 - mDice: 0.9342 - val_loss: -8.7745e-02 - val_acc: 0.9950 - val_mDice: 0.5418

Epoch 00011: val_mDice did not improve from 0.56793
Epoch 12/300
 - 71s - loss: 0.0333 - acc: 0.9960 - mDice: 0.9354 - val_loss: -8.8667e-02 - val_acc: 0.9948 - val_mDice: 0.5438

Epoch 00012: val_mDice did not improve from 0.56793
Epoch 13/300
 - 71s - loss: 0.0331 - acc: 0.9960 - mDice: 0.9357 - val_loss: -7.2108e-02 - val_acc: 0.9948 - val_mDice: 0.5612

Epoch 00013: val_mDice did not improve from 0.56793
Epoch 14/300
 - 71s - loss: 0.0324 - acc: 0.9961 - mDice: 0.9370 - val_loss: -5.9442e-02 - val_acc: 0.9948 - val_mDice: 0.5357

Epoch 00014: val_mDice did not improve from 0.56793
Epoch 15/300
 - 71s - loss: 0.0324 - acc: 0.9961 - mDice: 0.9370 - val_loss: -7.0657e-02 - val_acc: 0.9949 - val_mDice: 0.5582

Epoch 00015: val_mDice did not improve from 0.56793
Epoch 16/300
 - 71s - loss: 0.0317 - acc: 0.9962 - mDice: 0.9385 - val_loss: -6.4868e-02 - val_acc: 0.9947 - val_mDice: 0.5468

Epoch 00016: val_mDice did not improve from 0.56793
Epoch 17/300
 - 71s - loss: 0.0316 - acc: 0.9962 - mDice: 0.9386 - val_loss: -7.3703e-02 - val_acc: 0.9949 - val_mDice: 0.5469

Epoch 00017: val_mDice did not improve from 0.56793
Epoch 18/300
 - 71s - loss: 0.0311 - acc: 0.9962 - mDice: 0.9397 - val_loss: -7.4295e-02 - val_acc: 0.9945 - val_mDice: 0.5658

Epoch 00018: val_mDice did not improve from 0.56793
Epoch 19/300
 - 72s - loss: 0.0312 - acc: 0.9962 - mDice: 0.9394 - val_loss: -7.1144e-02 - val_acc: 0.9947 - val_mDice: 0.5565

Epoch 00019: val_mDice did not improve from 0.56793

Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 20/300
 - 71s - loss: 0.0298 - acc: 0.9964 - mDice: 0.9421 - val_loss: -4.3697e-02 - val_acc: 0.9948 - val_mDice: 0.5550

Epoch 00020: val_mDice did not improve from 0.56793
Epoch 21/300
 - 72s - loss: 0.0292 - acc: 0.9964 - mDice: 0.9434 - val_loss: -6.4205e-02 - val_acc: 0.9948 - val_mDice: 0.5427

Epoch 00021: val_mDice did not improve from 0.56793
Epoch 22/300
 - 71s - loss: 0.0289 - acc: 0.9964 - mDice: 0.9439 - val_loss: -9.2057e-02 - val_acc: 0.9948 - val_mDice: 0.5506

Epoch 00022: val_mDice did not improve from 0.56793
Epoch 23/300
 - 71s - loss: 0.0290 - acc: 0.9964 - mDice: 0.9436 - val_loss: -7.2200e-02 - val_acc: 0.9948 - val_mDice: 0.5614

Epoch 00023: val_mDice did not improve from 0.56793
Epoch 24/300
 - 71s - loss: 0.0285 - acc: 0.9964 - mDice: 0.9447 - val_loss: -6.7407e-02 - val_acc: 0.9947 - val_mDice: 0.5519

Epoch 00024: val_mDice did not improve from 0.56793
Epoch 25/300
 - 70s - loss: 0.0284 - acc: 0.9964 - mDice: 0.9448 - val_loss: -7.3682e-02 - val_acc: 0.9947 - val_mDice: 0.5630

Epoch 00025: val_mDice did not improve from 0.56793
Epoch 26/300
 - 70s - loss: 0.0282 - acc: 0.9965 - mDice: 0.9454 - val_loss: -6.9587e-02 - val_acc: 0.9948 - val_mDice: 0.5562

Epoch 00026: val_mDice did not improve from 0.56793
Epoch 27/300
 - 70s - loss: 0.0282 - acc: 0.9965 - mDice: 0.9453 - val_loss: -9.9028e-02 - val_acc: 0.9947 - val_mDice: 0.5647

Epoch 00027: val_mDice did not improve from 0.56793
Epoch 28/300
 - 70s - loss: 0.0281 - acc: 0.9965 - mDice: 0.9456 - val_loss: -4.3718e-02 - val_acc: 0.9948 - val_mDice: 0.5550

Epoch 00028: val_mDice did not improve from 0.56793
Epoch 29/300
 - 70s - loss: 0.0280 - acc: 0.9965 - mDice: 0.9456 - val_loss: -6.6892e-02 - val_acc: 0.9948 - val_mDice: 0.5508

Epoch 00029: val_mDice did not improve from 0.56793
Epoch 30/300
 - 71s - loss: 0.0278 - acc: 0.9965 - mDice: 0.9461 - val_loss: -9.7050e-02 - val_acc: 0.9947 - val_mDice: 0.5607

Epoch 00030: val_mDice did not improve from 0.56793
Epoch 31/300
 - 72s - loss: 0.0276 - acc: 0.9965 - mDice: 0.9464 - val_loss: -5.8842e-02 - val_acc: 0.9946 - val_mDice: 0.5437

Epoch 00031: val_mDice did not improve from 0.56793
Epoch 32/300
 - 72s - loss: 0.0273 - acc: 0.9965 - mDice: 0.9471 - val_loss: -4.0732e-02 - val_acc: 0.9946 - val_mDice: 0.5491

Epoch 00032: val_mDice did not improve from 0.56793
Epoch 33/300
 - 71s - loss: 0.0272 - acc: 0.9965 - mDice: 0.9472 - val_loss: -8.9386e-02 - val_acc: 0.9948 - val_mDice: 0.5452

Epoch 00033: val_mDice did not improve from 0.56793
Epoch 34/300
 - 71s - loss: 0.0274 - acc: 0.9966 - mDice: 0.9468 - val_loss: -9.1155e-02 - val_acc: 0.9947 - val_mDice: 0.5489

Epoch 00034: val_mDice did not improve from 0.56793

Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 35/300
 - 72s - loss: 0.0268 - acc: 0.9966 - mDice: 0.9480 - val_loss: -2.9040e-02 - val_acc: 0.9947 - val_mDice: 0.5561

Epoch 00035: val_mDice did not improve from 0.56793
Epoch 36/300
 - 72s - loss: 0.0263 - acc: 0.9966 - mDice: 0.9490 - val_loss: -3.8289e-02 - val_acc: 0.9947 - val_mDice: 0.5441

Epoch 00036: val_mDice did not improve from 0.56793
Epoch 37/300
 - 74s - loss: 0.0267 - acc: 0.9966 - mDice: 0.9482 - val_loss: -5.6256e-02 - val_acc: 0.9948 - val_mDice: 0.5500

Epoch 00037: val_mDice did not improve from 0.56793
Epoch 38/300
 - 74s - loss: 0.0263 - acc: 0.9966 - mDice: 0.9490 - val_loss: -9.1683e-02 - val_acc: 0.9948 - val_mDice: 0.5505

Epoch 00038: val_mDice did not improve from 0.56793
Epoch 39/300
 - 74s - loss: 0.0261 - acc: 0.9967 - mDice: 0.9494 - val_loss: -6.7066e-02 - val_acc: 0.9948 - val_mDice: 0.5512

Epoch 00039: val_mDice did not improve from 0.56793
Epoch 40/300
 - 73s - loss: 0.0260 - acc: 0.9966 - mDice: 0.9496 - val_loss: -6.4221e-02 - val_acc: 0.9948 - val_mDice: 0.5459

Epoch 00040: val_mDice did not improve from 0.56793
Epoch 41/300
 - 71s - loss: 0.0263 - acc: 0.9967 - mDice: 0.9490 - val_loss: -6.4374e-02 - val_acc: 0.9948 - val_mDice: 0.5458

Epoch 00041: val_mDice did not improve from 0.56793
Epoch 42/300
 - 71s - loss: 0.0261 - acc: 0.9967 - mDice: 0.9495 - val_loss: -9.1314e-02 - val_acc: 0.9947 - val_mDice: 0.5494

Epoch 00042: val_mDice did not improve from 0.56793
Epoch 43/300
 - 71s - loss: 0.0258 - acc: 0.9967 - mDice: 0.9500 - val_loss: -3.9746e-02 - val_acc: 0.9948 - val_mDice: 0.5470

Epoch 00043: val_mDice did not improve from 0.56793
Epoch 44/300
 - 71s - loss: 0.0259 - acc: 0.9967 - mDice: 0.9497 - val_loss: -3.8960e-02 - val_acc: 0.9948 - val_mDice: 0.5460

predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:00<00:02,  1.45it/s]predicting test subjects:  50%|█████     | 2/4 [00:00<00:01,  1.86it/s]predicting test subjects:  75%|███████▌  | 3/4 [00:01<00:00,  2.27it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  2.81it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  3.21it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:03,  4.15it/s]predicting train subjects:   1%|          | 2/266 [00:00<01:03,  4.17it/s]predicting train subjects:   1%|          | 3/266 [00:00<00:56,  4.68it/s]predicting train subjects:   2%|▏         | 4/266 [00:00<00:52,  4.97it/s]predicting train subjects:   2%|▏         | 5/266 [00:01<00:55,  4.67it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<00:53,  4.90it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<00:50,  5.09it/s]predicting train subjects:   3%|▎         | 8/266 [00:01<00:49,  5.17it/s]predicting train subjects:   3%|▎         | 9/266 [00:01<00:48,  5.33it/s]predicting train subjects:   4%|▍         | 10/266 [00:01<00:47,  5.42it/s]predicting train subjects:   4%|▍         | 11/266 [00:02<00:46,  5.51it/s]predicting train subjects:   5%|▍         | 12/266 [00:02<00:45,  5.58it/s]predicting train subjects:   5%|▍         | 13/266 [00:02<00:46,  5.44it/s]predicting train subjects:   5%|▌         | 14/266 [00:02<00:46,  5.43it/s]predicting train subjects:   6%|▌         | 15/266 [00:02<00:45,  5.51it/s]predicting train subjects:   6%|▌         | 16/266 [00:03<00:44,  5.57it/s]predicting train subjects:   6%|▋         | 17/266 [00:03<00:44,  5.55it/s]predicting train subjects:   7%|▋         | 18/266 [00:03<00:44,  5.56it/s]predicting train subjects:   7%|▋         | 19/266 [00:03<00:43,  5.63it/s]predicting train subjects:   8%|▊         | 20/266 [00:03<00:43,  5.66it/s]predicting train subjects:   8%|▊         | 21/266 [00:03<00:44,  5.53it/s]predicting train subjects:   8%|▊         | 22/266 [00:04<00:43,  5.58it/s]predicting train subjects:   9%|▊         | 23/266 [00:04<00:43,  5.61it/s]predicting train subjects:   9%|▉         | 24/266 [00:04<00:43,  5.61it/s]predicting train subjects:   9%|▉         | 25/266 [00:04<00:41,  5.76it/s]predicting train subjects:  10%|▉         | 26/266 [00:04<00:41,  5.83it/s]predicting train subjects:  10%|█         | 27/266 [00:04<00:40,  5.94it/s]predicting train subjects:  11%|█         | 28/266 [00:05<00:40,  5.91it/s]predicting train subjects:  11%|█         | 29/266 [00:05<00:40,  5.89it/s]predicting train subjects:  11%|█▏        | 30/266 [00:05<00:39,  5.92it/s]predicting train subjects:  12%|█▏        | 31/266 [00:05<00:39,  5.93it/s]predicting train subjects:  12%|█▏        | 32/266 [00:05<00:40,  5.81it/s]predicting train subjects:  12%|█▏        | 33/266 [00:05<00:40,  5.76it/s]predicting train subjects:  13%|█▎        | 34/266 [00:06<00:39,  5.80it/s]predicting train subjects:  13%|█▎        | 35/266 [00:06<00:40,  5.66it/s]predicting train subjects:  14%|█▎        | 36/266 [00:06<00:40,  5.69it/s]predicting train subjects:  14%|█▍        | 37/266 [00:06<00:39,  5.74it/s]predicting train subjects:  14%|█▍        | 38/266 [00:06<00:39,  5.77it/s]predicting train subjects:  15%|█▍        | 39/266 [00:07<00:39,  5.78it/s]predicting train subjects:  15%|█▌        | 40/266 [00:07<00:39,  5.76it/s]predicting train subjects:  15%|█▌        | 41/266 [00:07<00:39,  5.67it/s]predicting train subjects:  16%|█▌        | 42/266 [00:07<00:37,  5.90it/s]predicting train subjects:  16%|█▌        | 43/266 [00:07<00:36,  6.05it/s]predicting train subjects:  17%|█▋        | 44/266 [00:07<00:35,  6.24it/s]predicting train subjects:  17%|█▋        | 45/266 [00:07<00:34,  6.40it/s]predicting train subjects:  17%|█▋        | 46/266 [00:08<00:33,  6.51it/s]predicting train subjects:  18%|█▊        | 47/266 [00:08<00:33,  6.59it/s]predicting train subjects:  18%|█▊        | 48/266 [00:08<00:32,  6.62it/s]predicting train subjects:  18%|█▊        | 49/266 [00:08<00:32,  6.62it/s]predicting train subjects:  19%|█▉        | 50/266 [00:08<00:32,  6.65it/s]predicting train subjects:  19%|█▉        | 51/266 [00:08<00:32,  6.55it/s]predicting train subjects:  20%|█▉        | 52/266 [00:09<00:32,  6.55it/s]predicting train subjects:  20%|█▉        | 53/266 [00:09<00:33,  6.37it/s]predicting train subjects:  20%|██        | 54/266 [00:09<00:32,  6.45it/s]predicting train subjects:  21%|██        | 55/266 [00:09<00:32,  6.52it/s]predicting train subjects:  21%|██        | 56/266 [00:09<00:31,  6.57it/s]predicting train subjects:  21%|██▏       | 57/266 [00:09<00:31,  6.59it/s]predicting train subjects:  22%|██▏       | 58/266 [00:09<00:31,  6.59it/s]predicting train subjects:  22%|██▏       | 59/266 [00:10<00:31,  6.67it/s]predicting train subjects:  23%|██▎       | 60/266 [00:10<00:30,  6.80it/s]predicting train subjects:  23%|██▎       | 61/266 [00:10<00:29,  6.91it/s]predicting train subjects:  23%|██▎       | 62/266 [00:10<00:30,  6.75it/s]predicting train subjects:  24%|██▎       | 63/266 [00:10<00:29,  6.85it/s]predicting train subjects:  24%|██▍       | 64/266 [00:10<00:29,  6.93it/s]predicting train subjects:  24%|██▍       | 65/266 [00:10<00:28,  6.94it/s]predicting train subjects:  25%|██▍       | 66/266 [00:11<00:29,  6.80it/s]predicting train subjects:  25%|██▌       | 67/266 [00:11<00:28,  6.88it/s]predicting train subjects:  26%|██▌       | 68/266 [00:11<00:28,  6.95it/s]predicting train subjects:  26%|██▌       | 69/266 [00:11<00:28,  6.98it/s]predicting train subjects:  26%|██▋       | 70/266 [00:11<00:27,  7.04it/s]predicting train subjects:  27%|██▋       | 71/266 [00:11<00:27,  7.02it/s]predicting train subjects:  27%|██▋       | 72/266 [00:11<00:28,  6.90it/s]predicting train subjects:  27%|██▋       | 73/266 [00:12<00:28,  6.82it/s]predicting train subjects:  28%|██▊       | 74/266 [00:12<00:29,  6.61it/s]predicting train subjects:  28%|██▊       | 75/266 [00:12<00:28,  6.68it/s]predicting train subjects:  29%|██▊       | 76/266 [00:12<00:28,  6.78it/s]predicting train subjects:  29%|██▉       | 77/266 [00:12<00:28,  6.62it/s]predicting train subjects:  29%|██▉       | 78/266 [00:12<00:30,  6.16it/s]predicting train subjects:  30%|██▉       | 79/266 [00:13<00:31,  6.03it/s]predicting train subjects:  30%|███       | 80/266 [00:13<00:31,  5.93it/s]predicting train subjects:  30%|███       | 81/266 [00:13<00:31,  5.83it/s]predicting train subjects:  31%|███       | 82/266 [00:13<00:31,  5.78it/s]predicting train subjects:  31%|███       | 83/266 [00:13<00:31,  5.78it/s]predicting train subjects:  32%|███▏      | 84/266 [00:13<00:31,  5.69it/s]predicting train subjects:  32%|███▏      | 85/266 [00:14<00:31,  5.69it/s]predicting train subjects:  32%|███▏      | 86/266 [00:14<00:31,  5.66it/s]predicting train subjects:  33%|███▎      | 87/266 [00:14<00:31,  5.69it/s]predicting train subjects:  33%|███▎      | 88/266 [00:14<00:31,  5.66it/s]predicting train subjects:  33%|███▎      | 89/266 [00:14<00:31,  5.71it/s]predicting train subjects:  34%|███▍      | 90/266 [00:15<00:30,  5.75it/s]predicting train subjects:  34%|███▍      | 91/266 [00:15<00:30,  5.80it/s]predicting train subjects:  35%|███▍      | 92/266 [00:15<00:30,  5.67it/s]predicting train subjects:  35%|███▍      | 93/266 [00:15<00:30,  5.67it/s]predicting train subjects:  35%|███▌      | 94/266 [00:15<00:30,  5.63it/s]predicting train subjects:  36%|███▌      | 95/266 [00:15<00:29,  5.70it/s]predicting train subjects:  36%|███▌      | 96/266 [00:16<00:31,  5.37it/s]predicting train subjects:  36%|███▋      | 97/266 [00:16<00:33,  5.04it/s]predicting train subjects:  37%|███▋      | 98/266 [00:16<00:33,  5.02it/s]predicting train subjects:  37%|███▋      | 99/266 [00:16<00:30,  5.50it/s]predicting train subjects:  38%|███▊      | 100/266 [00:16<00:30,  5.46it/s]predicting train subjects:  38%|███▊      | 101/266 [00:17<00:28,  5.69it/s]predicting train subjects:  38%|███▊      | 102/266 [00:17<00:27,  5.90it/s]predicting train subjects:  39%|███▊      | 103/266 [00:17<00:27,  6.03it/s]predicting train subjects:  39%|███▉      | 104/266 [00:17<00:26,  6.13it/s]predicting train subjects:  39%|███▉      | 105/266 [00:17<00:25,  6.22it/s]predicting train subjects:  40%|███▉      | 106/266 [00:17<00:25,  6.25it/s]predicting train subjects:  40%|████      | 107/266 [00:17<00:25,  6.31it/s]predicting train subjects:  41%|████      | 108/266 [00:18<00:25,  6.27it/s]predicting train subjects:  41%|████      | 109/266 [00:18<00:24,  6.29it/s]predicting train subjects:  41%|████▏     | 110/266 [00:18<00:24,  6.35it/s]predicting train subjects:  42%|████▏     | 111/266 [00:18<00:24,  6.30it/s]predicting train subjects:  42%|████▏     | 112/266 [00:18<00:24,  6.31it/s]predicting train subjects:  42%|████▏     | 113/266 [00:18<00:24,  6.35it/s]predicting train subjects:  43%|████▎     | 114/266 [00:19<00:23,  6.37it/s]predicting train subjects:  43%|████▎     | 115/266 [00:19<00:23,  6.42it/s]predicting train subjects:  44%|████▎     | 116/266 [00:19<00:23,  6.41it/s]predicting train subjects:  44%|████▍     | 117/266 [00:19<00:23,  6.38it/s]predicting train subjects:  44%|████▍     | 118/266 [00:19<00:23,  6.37it/s]predicting train subjects:  45%|████▍     | 119/266 [00:19<00:23,  6.25it/s]predicting train subjects:  45%|████▌     | 120/266 [00:20<00:23,  6.10it/s]predicting train subjects:  45%|████▌     | 121/266 [00:20<00:24,  6.03it/s]predicting train subjects:  46%|████▌     | 122/266 [00:20<00:23,  6.03it/s]predicting train subjects:  46%|████▌     | 123/266 [00:20<00:23,  5.98it/s]predicting train subjects:  47%|████▋     | 124/266 [00:20<00:23,  5.97it/s]predicting train subjects:  47%|████▋     | 125/266 [00:20<00:23,  5.95it/s]predicting train subjects:  47%|████▋     | 126/266 [00:21<00:23,  5.91it/s]predicting train subjects:  48%|████▊     | 127/266 [00:21<00:23,  5.91it/s]predicting train subjects:  48%|████▊     | 128/266 [00:21<00:23,  5.92it/s]predicting train subjects:  48%|████▊     | 129/266 [00:21<00:23,  5.94it/s]predicting train subjects:  49%|████▉     | 130/266 [00:21<00:22,  5.94it/s]predicting train subjects:  49%|████▉     | 131/266 [00:21<00:22,  5.92it/s]predicting train subjects:  50%|████▉     | 132/266 [00:22<00:22,  5.93it/s]predicting train subjects:  50%|█████     | 133/266 [00:22<00:22,  5.90it/s]predicting train subjects:  50%|█████     | 134/266 [00:22<00:22,  5.91it/s]predicting train subjects:  51%|█████     | 135/266 [00:22<00:22,  5.90it/s]predicting train subjects:  51%|█████     | 136/266 [00:22<00:22,  5.79it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:22<00:21,  5.86it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:23<00:21,  5.94it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:23<00:21,  6.00it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:23<00:20,  6.05it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:23<00:21,  5.84it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:23<00:20,  5.93it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:23<00:20,  5.97it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:24<00:20,  6.08it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:24<00:19,  6.13it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:24<00:19,  6.10it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:24<00:19,  6.14it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:24<00:19,  6.20it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:24<00:18,  6.21it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:25<00:18,  6.21it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:25<00:18,  6.21it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:25<00:18,  6.19it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:25<00:18,  6.19it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:25<00:17,  6.23it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:25<00:17,  6.51it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:25<00:16,  6.71it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:26<00:15,  6.86it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:26<00:15,  7.05it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:26<00:14,  7.15it/s]predicting train subjects:  60%|██████    | 160/266 [00:26<00:14,  7.19it/s]predicting train subjects:  61%|██████    | 161/266 [00:26<00:14,  7.24it/s]predicting train subjects:  61%|██████    | 162/266 [00:26<00:14,  7.22it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:26<00:14,  7.24it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:27<00:14,  7.25it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:27<00:13,  7.31it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:27<00:13,  7.32it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:27<00:13,  7.14it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:27<00:13,  7.16it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:27<00:13,  7.20it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:27<00:13,  7.23it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:28<00:13,  7.13it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:28<00:13,  7.08it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:28<00:13,  6.85it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:28<00:13,  6.78it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:28<00:13,  6.77it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:28<00:13,  6.64it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:28<00:13,  6.70it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:29<00:13,  6.65it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:29<00:12,  6.70it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:29<00:12,  6.72it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:29<00:13,  6.50it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:29<00:12,  6.66it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:29<00:12,  6.63it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:29<00:12,  6.71it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:30<00:12,  6.58it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:30<00:12,  6.36it/s]predicting train subjects:  70%|███████   | 187/266 [00:30<00:12,  6.48it/s]predicting train subjects:  71%|███████   | 188/266 [00:30<00:12,  6.43it/s]predicting train subjects:  71%|███████   | 189/266 [00:30<00:11,  6.51it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:30<00:11,  6.53it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:31<00:12,  6.10it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:31<00:13,  5.46it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:31<00:12,  5.86it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:31<00:13,  5.32it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:31<00:12,  5.71it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:32<00:11,  6.04it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:32<00:10,  6.30it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:32<00:10,  6.51it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:32<00:10,  6.66it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:32<00:09,  6.71it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:32<00:09,  6.78it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:32<00:09,  6.82it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:33<00:09,  6.84it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:33<00:08,  6.89it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:33<00:08,  6.94it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:33<00:08,  6.98it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:33<00:08,  7.01it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:33<00:08,  6.98it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:33<00:08,  6.71it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:34<00:08,  6.70it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:34<00:08,  6.76it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:34<00:07,  6.78it/s]predicting train subjects:  80%|████████  | 213/266 [00:34<00:07,  6.97it/s]predicting train subjects:  80%|████████  | 214/266 [00:34<00:07,  6.90it/s]predicting train subjects:  81%|████████  | 215/266 [00:34<00:07,  7.09it/s]predicting train subjects:  81%|████████  | 216/266 [00:34<00:06,  7.23it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:35<00:06,  7.33it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:35<00:06,  7.40it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:35<00:06,  7.46it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:35<00:06,  7.48it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:35<00:05,  7.53it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:35<00:06,  7.29it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:35<00:05,  7.39it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:35<00:05,  7.27it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:36<00:05,  7.36it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:36<00:05,  7.42it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:36<00:05,  7.37it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:36<00:05,  7.38it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:36<00:04,  7.44it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:36<00:04,  7.45it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:36<00:04,  7.35it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:37<00:04,  7.31it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:37<00:04,  7.27it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:37<00:04,  7.14it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:37<00:04,  7.12it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:37<00:04,  7.08it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:37<00:04,  7.13it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:37<00:03,  7.18it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:38<00:03,  7.20it/s]predicting train subjects:  90%|█████████ | 240/266 [00:38<00:03,  6.93it/s]predicting train subjects:  91%|█████████ | 241/266 [00:38<00:03,  6.77it/s]predicting train subjects:  91%|█████████ | 242/266 [00:38<00:03,  6.77it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:38<00:03,  6.65it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:38<00:03,  6.78it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:38<00:03,  6.91it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:39<00:02,  6.98it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:39<00:02,  6.97it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:39<00:02,  6.98it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:39<00:02,  6.68it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:39<00:02,  6.51it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:39<00:02,  6.34it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:40<00:02,  6.27it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:40<00:02,  6.18it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:40<00:01,  6.15it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:40<00:01,  6.12it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:40<00:01,  6.12it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:40<00:01,  6.11it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:40<00:01,  6.10it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:41<00:01,  6.09it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:41<00:00,  6.09it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:41<00:00,  6.07it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:41<00:00,  6.02it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:41<00:00,  6.00it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:41<00:00,  5.91it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:42<00:00,  5.83it/s]predicting train subjects: 100%|██████████| 266/266 [00:42<00:00,  5.85it/s]predicting train subjects: 100%|██████████| 266/266 [00:42<00:00,  6.28it/s]
saving BB  test1-THALAMUS:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 4/4 [00:00<00:00, 75.64it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 8/266 [00:00<00:03, 73.73it/s]saving BB  train1-THALAMUS:   6%|▌         | 15/266 [00:00<00:03, 72.28it/s]saving BB  train1-THALAMUS:   8%|▊         | 22/266 [00:00<00:03, 70.65it/s]saving BB  train1-THALAMUS:  11%|█         | 29/266 [00:00<00:03, 69.48it/s]saving BB  train1-THALAMUS:  14%|█▍        | 37/266 [00:00<00:03, 71.82it/s]saving BB  train1-THALAMUS:  17%|█▋        | 46/266 [00:00<00:02, 73.86it/s]saving BB  train1-THALAMUS:  21%|██        | 55/266 [00:00<00:02, 76.54it/s]saving BB  train1-THALAMUS:  24%|██▍       | 64/266 [00:00<00:02, 79.55it/s]saving BB  train1-THALAMUS:  28%|██▊       | 74/266 [00:00<00:02, 82.89it/s]saving BB  train1-THALAMUS:  31%|███       | 83/266 [00:01<00:02, 82.52it/s]saving BB  train1-THALAMUS:  35%|███▍      | 92/266 [00:01<00:02, 81.07it/s]saving BB  train1-THALAMUS:  38%|███▊      | 101/266 [00:01<00:02, 79.64it/s]saving BB  train1-THALAMUS:  41%|████▏     | 110/266 [00:01<00:01, 80.24it/s]saving BB  train1-THALAMUS:  45%|████▍     | 119/266 [00:01<00:01, 80.47it/s]saving BB  train1-THALAMUS:  48%|████▊     | 128/266 [00:01<00:01, 79.43it/s]saving BB  train1-THALAMUS:  51%|█████     | 136/266 [00:01<00:01, 78.23it/s]saving BB  train1-THALAMUS:  54%|█████▍    | 144/266 [00:01<00:01, 78.11it/s]saving BB  train1-THALAMUS:  57%|█████▋    | 152/266 [00:01<00:01, 77.02it/s]saving BB  train1-THALAMUS:  61%|██████    | 161/266 [00:02<00:01, 79.92it/s]saving BB  train1-THALAMUS:  64%|██████▍   | 171/266 [00:02<00:01, 83.37it/s]saving BB  train1-THALAMUS:  68%|██████▊   | 181/266 [00:02<00:00, 85.46it/s]saving BB  train1-THALAMUS:  71%|███████▏  | 190/266 [00:02<00:00, 84.71it/s]saving BB  train1-THALAMUS:  75%|███████▍  | 199/266 [00:02<00:00, 84.24it/s]saving BB  train1-THALAMUS:  78%|███████▊  | 208/266 [00:02<00:00, 80.55it/s]saving BB  train1-THALAMUS:  82%|████████▏ | 217/266 [00:02<00:00, 79.35it/s]saving BB  train1-THALAMUS:  85%|████████▍ | 226/266 [00:02<00:00, 81.06it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 235/266 [00:02<00:00, 83.50it/s]saving BB  train1-THALAMUS:  92%|█████████▏| 244/266 [00:03<00:00, 82.06it/s]saving BB  train1-THALAMUS:  95%|█████████▌| 253/266 [00:03<00:00, 82.08it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 262/266 [00:03<00:00, 81.14it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 80.05it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<05:21,  1.21s/it]Loading train:   1%|          | 2/266 [00:02<05:02,  1.14s/it]Loading train:   1%|          | 3/266 [00:03<04:42,  1.08s/it]Loading train:   2%|▏         | 4/266 [00:03<04:25,  1.01s/it]Loading train:   2%|▏         | 5/266 [00:05<04:26,  1.02s/it]Loading train:   2%|▏         | 6/266 [00:05<04:07,  1.05it/s]Loading train:   3%|▎         | 7/266 [00:06<03:52,  1.12it/s]Loading train:   3%|▎         | 8/266 [00:07<03:39,  1.18it/s]Loading train:   3%|▎         | 9/266 [00:08<03:31,  1.22it/s]Loading train:   4%|▍         | 10/266 [00:08<03:26,  1.24it/s]Loading train:   4%|▍         | 11/266 [00:09<03:22,  1.26it/s]Loading train:   5%|▍         | 12/266 [00:10<03:21,  1.26it/s]Loading train:   5%|▍         | 13/266 [00:11<03:16,  1.28it/s]Loading train:   5%|▌         | 14/266 [00:11<03:12,  1.31it/s]Loading train:   6%|▌         | 15/266 [00:12<03:12,  1.31it/s]Loading train:   6%|▌         | 16/266 [00:13<03:11,  1.31it/s]Loading train:   6%|▋         | 17/266 [00:14<03:11,  1.30it/s]Loading train:   7%|▋         | 18/266 [00:15<03:15,  1.27it/s]Loading train:   7%|▋         | 19/266 [00:15<03:11,  1.29it/s]Loading train:   8%|▊         | 20/266 [00:16<03:08,  1.30it/s]Loading train:   8%|▊         | 21/266 [00:17<03:06,  1.31it/s]Loading train:   8%|▊         | 22/266 [00:18<03:05,  1.32it/s]Loading train:   9%|▊         | 23/266 [00:18<03:04,  1.32it/s]Loading train:   9%|▉         | 24/266 [00:19<02:57,  1.36it/s]Loading train:   9%|▉         | 25/266 [00:20<02:55,  1.37it/s]Loading train:  10%|▉         | 26/266 [00:20<02:49,  1.42it/s]Loading train:  10%|█         | 27/266 [00:21<02:44,  1.45it/s]Loading train:  11%|█         | 28/266 [00:22<02:41,  1.48it/s]Loading train:  11%|█         | 29/266 [00:22<02:39,  1.48it/s]Loading train:  11%|█▏        | 30/266 [00:23<02:37,  1.50it/s]Loading train:  12%|█▏        | 31/266 [00:24<02:35,  1.51it/s]Loading train:  12%|█▏        | 32/266 [00:24<02:35,  1.51it/s]Loading train:  12%|█▏        | 33/266 [00:25<02:35,  1.50it/s]Loading train:  13%|█▎        | 34/266 [00:26<02:34,  1.51it/s]Loading train:  13%|█▎        | 35/266 [00:26<02:31,  1.53it/s]Loading train:  14%|█▎        | 36/266 [00:27<02:28,  1.55it/s]Loading train:  14%|█▍        | 37/266 [00:27<02:25,  1.57it/s]Loading train:  14%|█▍        | 38/266 [00:28<02:24,  1.58it/s]Loading train:  15%|█▍        | 39/266 [00:29<02:26,  1.55it/s]Loading train:  15%|█▌        | 40/266 [00:29<02:26,  1.54it/s]Loading train:  15%|█▌        | 41/266 [00:30<02:27,  1.53it/s]Loading train:  16%|█▌        | 42/266 [00:31<02:30,  1.49it/s]Loading train:  16%|█▌        | 43/266 [00:31<02:26,  1.52it/s]Loading train:  17%|█▋        | 44/266 [00:32<02:24,  1.54it/s]Loading train:  17%|█▋        | 45/266 [00:33<02:22,  1.55it/s]Loading train:  17%|█▋        | 46/266 [00:33<02:21,  1.56it/s]Loading train:  18%|█▊        | 47/266 [00:34<02:20,  1.56it/s]Loading train:  18%|█▊        | 48/266 [00:35<02:20,  1.55it/s]Loading train:  18%|█▊        | 49/266 [00:35<02:19,  1.56it/s]Loading train:  19%|█▉        | 50/266 [00:36<02:19,  1.54it/s]Loading train:  19%|█▉        | 51/266 [00:37<02:18,  1.55it/s]Loading train:  20%|█▉        | 52/266 [00:37<02:16,  1.57it/s]Loading train:  20%|█▉        | 53/266 [00:38<02:15,  1.57it/s]Loading train:  20%|██        | 54/266 [00:38<02:14,  1.58it/s]Loading train:  21%|██        | 55/266 [00:39<02:13,  1.58it/s]Loading train:  21%|██        | 56/266 [00:40<02:12,  1.59it/s]Loading train:  21%|██▏       | 57/266 [00:40<02:12,  1.58it/s]Loading train:  22%|██▏       | 58/266 [00:41<02:11,  1.58it/s]Loading train:  22%|██▏       | 59/266 [00:42<02:12,  1.57it/s]Loading train:  23%|██▎       | 60/266 [00:42<02:10,  1.58it/s]Loading train:  23%|██▎       | 61/266 [00:43<02:06,  1.62it/s]Loading train:  23%|██▎       | 62/266 [00:43<02:04,  1.64it/s]Loading train:  24%|██▎       | 63/266 [00:44<02:02,  1.66it/s]Loading train:  24%|██▍       | 64/266 [00:45<02:00,  1.67it/s]Loading train:  24%|██▍       | 65/266 [00:45<01:59,  1.69it/s]Loading train:  25%|██▍       | 66/266 [00:46<01:57,  1.71it/s]Loading train:  25%|██▌       | 67/266 [00:46<01:57,  1.69it/s]Loading train:  26%|██▌       | 68/266 [00:47<01:56,  1.70it/s]Loading train:  26%|██▌       | 69/266 [00:47<01:54,  1.71it/s]Loading train:  26%|██▋       | 70/266 [00:48<01:54,  1.72it/s]Loading train:  27%|██▋       | 71/266 [00:49<01:54,  1.71it/s]Loading train:  27%|██▋       | 72/266 [00:49<01:53,  1.71it/s]Loading train:  27%|██▋       | 73/266 [00:50<01:52,  1.72it/s]Loading train:  28%|██▊       | 74/266 [00:50<01:52,  1.71it/s]Loading train:  28%|██▊       | 75/266 [00:51<01:51,  1.71it/s]Loading train:  29%|██▊       | 76/266 [00:52<01:49,  1.73it/s]Loading train:  29%|██▉       | 77/266 [00:52<01:48,  1.74it/s]Loading train:  29%|██▉       | 78/266 [00:53<01:54,  1.64it/s]Loading train:  30%|██▉       | 79/266 [00:53<01:57,  1.60it/s]Loading train:  30%|███       | 80/266 [00:54<01:58,  1.57it/s]Loading train:  30%|███       | 81/266 [00:55<02:01,  1.53it/s]Loading train:  31%|███       | 82/266 [00:55<02:00,  1.52it/s]Loading train:  31%|███       | 83/266 [00:56<01:59,  1.54it/s]Loading train:  32%|███▏      | 84/266 [00:57<01:57,  1.55it/s]Loading train:  32%|███▏      | 85/266 [00:57<01:56,  1.56it/s]Loading train:  32%|███▏      | 86/266 [00:58<01:55,  1.56it/s]Loading train:  33%|███▎      | 87/266 [00:59<01:55,  1.55it/s]Loading train:  33%|███▎      | 88/266 [00:59<01:56,  1.53it/s]Loading train:  33%|███▎      | 89/266 [01:00<01:59,  1.48it/s]Loading train:  34%|███▍      | 90/266 [01:01<01:57,  1.50it/s]Loading train:  34%|███▍      | 91/266 [01:01<01:54,  1.53it/s]Loading train:  35%|███▍      | 92/266 [01:02<01:51,  1.56it/s]Loading train:  35%|███▍      | 93/266 [01:03<01:49,  1.58it/s]Loading train:  35%|███▌      | 94/266 [01:03<01:49,  1.58it/s]Loading train:  36%|███▌      | 95/266 [01:04<01:48,  1.58it/s]Loading train:  36%|███▌      | 96/266 [01:05<02:02,  1.39it/s]Loading train:  36%|███▋      | 97/266 [01:06<02:17,  1.23it/s]Loading train:  37%|███▋      | 98/266 [01:07<02:20,  1.19it/s]Loading train:  37%|███▋      | 99/266 [01:07<02:17,  1.21it/s]Loading train:  38%|███▊      | 100/266 [01:08<02:17,  1.21it/s]Loading train:  38%|███▊      | 101/266 [01:09<02:08,  1.28it/s]Loading train:  38%|███▊      | 102/266 [01:10<02:02,  1.34it/s]Loading train:  39%|███▊      | 103/266 [01:10<01:58,  1.37it/s]Loading train:  39%|███▉      | 104/266 [01:11<01:53,  1.43it/s]Loading train:  39%|███▉      | 105/266 [01:12<01:49,  1.46it/s]Loading train:  40%|███▉      | 106/266 [01:12<01:47,  1.49it/s]Loading train:  40%|████      | 107/266 [01:13<01:44,  1.51it/s]Loading train:  41%|████      | 108/266 [01:14<01:44,  1.51it/s]Loading train:  41%|████      | 109/266 [01:14<01:43,  1.51it/s]Loading train:  41%|████▏     | 110/266 [01:15<01:43,  1.50it/s]Loading train:  42%|████▏     | 111/266 [01:16<01:42,  1.52it/s]Loading train:  42%|████▏     | 112/266 [01:16<01:41,  1.52it/s]Loading train:  42%|████▏     | 113/266 [01:17<01:40,  1.53it/s]Loading train:  43%|████▎     | 114/266 [01:17<01:38,  1.54it/s]Loading train:  43%|████▎     | 115/266 [01:18<01:37,  1.54it/s]Loading train:  44%|████▎     | 116/266 [01:19<01:37,  1.54it/s]Loading train:  44%|████▍     | 117/266 [01:19<01:36,  1.55it/s]Loading train:  44%|████▍     | 118/266 [01:20<01:36,  1.54it/s]Loading train:  45%|████▍     | 119/266 [01:21<01:38,  1.49it/s]Loading train:  45%|████▌     | 120/266 [01:21<01:38,  1.48it/s]Loading train:  45%|████▌     | 121/266 [01:22<01:38,  1.47it/s]Loading train:  46%|████▌     | 122/266 [01:23<01:37,  1.48it/s]Loading train:  46%|████▌     | 123/266 [01:24<01:35,  1.49it/s]Loading train:  47%|████▋     | 124/266 [01:24<01:34,  1.50it/s]Loading train:  47%|████▋     | 125/266 [01:25<01:33,  1.51it/s]Loading train:  47%|████▋     | 126/266 [01:25<01:32,  1.51it/s]Loading train:  48%|████▊     | 127/266 [01:26<01:30,  1.53it/s]Loading train:  48%|████▊     | 128/266 [01:27<01:30,  1.52it/s]Loading train:  48%|████▊     | 129/266 [01:27<01:30,  1.51it/s]Loading train:  49%|████▉     | 130/266 [01:28<01:31,  1.49it/s]Loading train:  49%|████▉     | 131/266 [01:29<01:32,  1.46it/s]Loading train:  50%|████▉     | 132/266 [01:30<01:31,  1.46it/s]Loading train:  50%|█████     | 133/266 [01:30<01:30,  1.46it/s]Loading train:  50%|█████     | 134/266 [01:31<01:29,  1.48it/s]Loading train:  51%|█████     | 135/266 [01:32<01:27,  1.50it/s]Loading train:  51%|█████     | 136/266 [01:32<01:25,  1.51it/s]Loading train:  52%|█████▏    | 137/266 [01:33<01:25,  1.51it/s]Loading train:  52%|█████▏    | 138/266 [01:34<01:25,  1.50it/s]Loading train:  52%|█████▏    | 139/266 [01:34<01:24,  1.50it/s]Loading train:  53%|█████▎    | 140/266 [01:35<01:23,  1.51it/s]Loading train:  53%|█████▎    | 141/266 [01:35<01:22,  1.52it/s]Loading train:  53%|█████▎    | 142/266 [01:36<01:22,  1.50it/s]Loading train:  54%|█████▍    | 143/266 [01:37<01:22,  1.49it/s]Loading train:  54%|█████▍    | 144/266 [01:38<01:22,  1.48it/s]Loading train:  55%|█████▍    | 145/266 [01:38<01:20,  1.50it/s]Loading train:  55%|█████▍    | 146/266 [01:39<01:19,  1.51it/s]Loading train:  55%|█████▌    | 147/266 [01:39<01:17,  1.53it/s]Loading train:  56%|█████▌    | 148/266 [01:40<01:17,  1.53it/s]Loading train:  56%|█████▌    | 149/266 [01:41<01:18,  1.49it/s]Loading train:  56%|█████▋    | 150/266 [01:42<01:19,  1.46it/s]Loading train:  57%|█████▋    | 151/266 [01:42<01:20,  1.43it/s]Loading train:  57%|█████▋    | 152/266 [01:43<01:20,  1.41it/s]Loading train:  58%|█████▊    | 153/266 [01:44<01:20,  1.40it/s]Loading train:  58%|█████▊    | 154/266 [01:44<01:18,  1.42it/s]Loading train:  58%|█████▊    | 155/266 [01:45<01:15,  1.47it/s]Loading train:  59%|█████▊    | 156/266 [01:46<01:12,  1.51it/s]Loading train:  59%|█████▉    | 157/266 [01:46<01:09,  1.57it/s]Loading train:  59%|█████▉    | 158/266 [01:47<01:06,  1.62it/s]Loading train:  60%|█████▉    | 159/266 [01:47<01:05,  1.64it/s]Loading train:  60%|██████    | 160/266 [01:48<01:04,  1.64it/s]Loading train:  61%|██████    | 161/266 [01:49<01:03,  1.66it/s]Loading train:  61%|██████    | 162/266 [01:49<01:01,  1.68it/s]Loading train:  61%|██████▏   | 163/266 [01:50<01:00,  1.71it/s]Loading train:  62%|██████▏   | 164/266 [01:50<00:59,  1.71it/s]Loading train:  62%|██████▏   | 165/266 [01:51<00:58,  1.73it/s]Loading train:  62%|██████▏   | 166/266 [01:51<00:57,  1.74it/s]Loading train:  63%|██████▎   | 167/266 [01:52<00:56,  1.75it/s]Loading train:  63%|██████▎   | 168/266 [01:53<00:56,  1.73it/s]Loading train:  64%|██████▎   | 169/266 [01:53<00:56,  1.72it/s]Loading train:  64%|██████▍   | 170/266 [01:54<00:55,  1.71it/s]Loading train:  64%|██████▍   | 171/266 [01:54<00:57,  1.67it/s]Loading train:  65%|██████▍   | 172/266 [01:55<00:57,  1.64it/s]Loading train:  65%|██████▌   | 173/266 [01:56<00:57,  1.62it/s]Loading train:  65%|██████▌   | 174/266 [01:56<00:55,  1.66it/s]Loading train:  66%|██████▌   | 175/266 [01:57<00:53,  1.71it/s]Loading train:  66%|██████▌   | 176/266 [01:57<00:51,  1.73it/s]Loading train:  67%|██████▋   | 177/266 [01:58<00:51,  1.74it/s]Loading train:  67%|██████▋   | 178/266 [01:59<00:50,  1.73it/s]Loading train:  67%|██████▋   | 179/266 [01:59<00:50,  1.73it/s]Loading train:  68%|██████▊   | 180/266 [02:00<00:49,  1.73it/s]Loading train:  68%|██████▊   | 181/266 [02:00<00:48,  1.75it/s]Loading train:  68%|██████▊   | 182/266 [02:01<00:49,  1.70it/s]Loading train:  69%|██████▉   | 183/266 [02:01<00:48,  1.71it/s]Loading train:  69%|██████▉   | 184/266 [02:02<00:47,  1.74it/s]Loading train:  70%|██████▉   | 185/266 [02:03<00:46,  1.76it/s]Loading train:  70%|██████▉   | 186/266 [02:03<00:45,  1.77it/s]Loading train:  70%|███████   | 187/266 [02:04<00:44,  1.78it/s]Loading train:  71%|███████   | 188/266 [02:04<00:43,  1.78it/s]Loading train:  71%|███████   | 189/266 [02:05<00:43,  1.77it/s]Loading train:  71%|███████▏  | 190/266 [02:05<00:42,  1.78it/s]Loading train:  72%|███████▏  | 191/266 [02:06<00:50,  1.50it/s]Loading train:  72%|███████▏  | 192/266 [02:07<00:53,  1.38it/s]Loading train:  73%|███████▎  | 193/266 [02:08<00:55,  1.32it/s]Loading train:  73%|███████▎  | 194/266 [02:09<01:00,  1.19it/s]Loading train:  73%|███████▎  | 195/266 [02:10<00:54,  1.31it/s]Loading train:  74%|███████▎  | 196/266 [02:10<00:49,  1.40it/s]Loading train:  74%|███████▍  | 197/266 [02:11<00:47,  1.47it/s]Loading train:  74%|███████▍  | 198/266 [02:11<00:44,  1.52it/s]Loading train:  75%|███████▍  | 199/266 [02:12<00:43,  1.56it/s]Loading train:  75%|███████▌  | 200/266 [02:13<00:42,  1.57it/s]Loading train:  76%|███████▌  | 201/266 [02:13<00:40,  1.60it/s]Loading train:  76%|███████▌  | 202/266 [02:14<00:39,  1.60it/s]Loading train:  76%|███████▋  | 203/266 [02:15<00:40,  1.57it/s]Loading train:  77%|███████▋  | 204/266 [02:15<00:39,  1.57it/s]Loading train:  77%|███████▋  | 205/266 [02:16<00:38,  1.59it/s]Loading train:  77%|███████▋  | 206/266 [02:16<00:37,  1.60it/s]Loading train:  78%|███████▊  | 207/266 [02:17<00:36,  1.61it/s]Loading train:  78%|███████▊  | 208/266 [02:18<00:35,  1.62it/s]Loading train:  79%|███████▊  | 209/266 [02:18<00:35,  1.63it/s]Loading train:  79%|███████▉  | 210/266 [02:19<00:34,  1.62it/s]Loading train:  79%|███████▉  | 211/266 [02:20<00:42,  1.30it/s]Loading train:  80%|███████▉  | 212/266 [02:24<01:33,  1.73s/it]Loading train:  80%|████████  | 213/266 [02:31<02:51,  3.23s/it]Loading train:  80%|████████  | 214/266 [02:36<03:25,  3.96s/it]Loading train:  81%|████████  | 215/266 [02:42<03:52,  4.56s/it]Loading train:  81%|████████  | 216/266 [02:48<04:02,  4.84s/it]Loading train:  82%|████████▏ | 217/266 [02:53<04:04,  4.99s/it]Loading train:  82%|████████▏ | 218/266 [02:58<03:59,  4.99s/it]Loading train:  82%|████████▏ | 219/266 [03:03<03:53,  4.97s/it]Loading train:  83%|████████▎ | 220/266 [03:08<03:42,  4.84s/it]Loading train:  83%|████████▎ | 221/266 [03:12<03:35,  4.79s/it]Loading train:  83%|████████▎ | 222/266 [03:16<03:20,  4.55s/it]Loading train:  84%|████████▍ | 223/266 [03:20<03:06,  4.33s/it]Loading train:  84%|████████▍ | 224/266 [03:24<02:55,  4.17s/it]Loading train:  85%|████████▍ | 225/266 [03:28<02:45,  4.05s/it]Loading train:  85%|████████▍ | 226/266 [03:31<02:34,  3.85s/it]Loading train:  85%|████████▌ | 227/266 [03:34<02:23,  3.69s/it]Loading train:  86%|████████▌ | 228/266 [03:37<02:13,  3.52s/it]Loading train:  86%|████████▌ | 229/266 [03:41<02:07,  3.46s/it]Loading train:  86%|████████▋ | 230/266 [03:44<02:03,  3.43s/it]Loading train:  87%|████████▋ | 231/266 [03:47<01:57,  3.35s/it]Loading train:  87%|████████▋ | 232/266 [03:50<01:51,  3.28s/it]Loading train:  88%|████████▊ | 233/266 [03:54<01:48,  3.28s/it]Loading train:  88%|████████▊ | 234/266 [03:57<01:44,  3.26s/it]Loading train:  88%|████████▊ | 235/266 [04:00<01:38,  3.19s/it]Loading train:  89%|████████▊ | 236/266 [04:03<01:35,  3.20s/it]Loading train:  89%|████████▉ | 237/266 [04:06<01:30,  3.11s/it]Loading train:  89%|████████▉ | 238/266 [04:09<01:27,  3.13s/it]Loading train:  90%|████████▉ | 239/266 [04:12<01:23,  3.11s/it]Loading train:  90%|█████████ | 240/266 [04:15<01:20,  3.10s/it]Loading train:  91%|█████████ | 241/266 [04:19<01:18,  3.14s/it]Loading train:  91%|█████████ | 242/266 [04:22<01:15,  3.15s/it]Loading train:  91%|█████████▏| 243/266 [04:25<01:12,  3.13s/it]Loading train:  92%|█████████▏| 244/266 [04:28<01:08,  3.13s/it]Loading train:  92%|█████████▏| 245/266 [04:31<01:07,  3.21s/it]Loading train:  92%|█████████▏| 246/266 [04:34<01:02,  3.12s/it]Loading train:  93%|█████████▎| 247/266 [04:37<00:59,  3.12s/it]Loading train:  93%|█████████▎| 248/266 [04:41<00:56,  3.13s/it]Loading train:  94%|█████████▎| 249/266 [04:46<01:03,  3.71s/it]Loading train:  94%|█████████▍| 250/266 [04:51<01:06,  4.16s/it]Loading train:  94%|█████████▍| 251/266 [04:56<01:07,  4.48s/it]Loading train:  95%|█████████▍| 252/266 [05:01<01:05,  4.70s/it]Loading train:  95%|█████████▌| 253/266 [05:07<01:03,  4.92s/it]Loading train:  95%|█████████▌| 254/266 [05:12<01:00,  5.03s/it]Loading train:  96%|█████████▌| 255/266 [05:17<00:55,  5.05s/it]Loading train:  96%|█████████▌| 256/266 [05:22<00:50,  5.06s/it]Loading train:  97%|█████████▋| 257/266 [05:27<00:44,  4.99s/it]Loading train:  97%|█████████▋| 258/266 [05:32<00:39,  4.96s/it]Loading train:  97%|█████████▋| 259/266 [05:37<00:35,  5.01s/it]Loading train:  98%|█████████▊| 260/266 [05:42<00:30,  5.03s/it]Loading train:  98%|█████████▊| 261/266 [05:47<00:24,  4.99s/it]Loading train:  98%|█████████▊| 262/266 [05:53<00:21,  5.36s/it]Loading train:  99%|█████████▉| 263/266 [06:02<00:18,  6.28s/it]Loading train:  99%|█████████▉| 264/266 [06:11<00:14,  7.11s/it]Loading train: 100%|█████████▉| 265/266 [06:22<00:08,  8.31s/it]Loading train: 100%|██████████| 266/266 [06:37<00:00, 10.35s/it]Loading train: 100%|██████████| 266/266 [06:37<00:00,  1.49s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/266 [00:00<00:05, 51.78it/s]concatenating: train:   5%|▍         | 12/266 [00:00<00:04, 51.01it/s]concatenating: train:   6%|▋         | 17/266 [00:00<00:05, 48.84it/s]concatenating: train:   8%|▊         | 22/266 [00:00<00:05, 47.42it/s]concatenating: train:  11%|█         | 29/266 [00:00<00:04, 51.41it/s]concatenating: train:  13%|█▎        | 35/266 [00:00<00:04, 53.68it/s]concatenating: train:  16%|█▌        | 42/266 [00:00<00:04, 55.66it/s]concatenating: train:  18%|█▊        | 48/266 [00:00<00:04, 53.69it/s]concatenating: train:  20%|██        | 54/266 [00:01<00:03, 54.58it/s]concatenating: train:  23%|██▎       | 60/266 [00:01<00:03, 54.84it/s]concatenating: train:  25%|██▌       | 67/266 [00:01<00:03, 57.91it/s]concatenating: train:  28%|██▊       | 75/266 [00:01<00:03, 61.52it/s]concatenating: train:  31%|███       | 82/266 [00:01<00:02, 62.31it/s]concatenating: train:  33%|███▎      | 89/266 [00:01<00:02, 62.04it/s]concatenating: train:  36%|███▌      | 96/266 [00:01<00:02, 60.68it/s]concatenating: train:  39%|███▊      | 103/266 [00:01<00:02, 60.12it/s]concatenating: train:  41%|████▏     | 110/266 [00:01<00:02, 58.32it/s]concatenating: train:  44%|████▎     | 116/266 [00:02<00:02, 58.64it/s]concatenating: train:  46%|████▌     | 122/266 [00:02<00:02, 57.00it/s]concatenating: train:  48%|████▊     | 129/266 [00:02<00:02, 58.84it/s]concatenating: train:  51%|█████     | 135/266 [00:02<00:02, 57.11it/s]concatenating: train:  53%|█████▎    | 141/266 [00:02<00:02, 56.38it/s]concatenating: train:  55%|█████▌    | 147/266 [00:02<00:02, 55.39it/s]concatenating: train:  58%|█████▊    | 153/266 [00:02<00:02, 55.55it/s]concatenating: train:  60%|█████▉    | 159/266 [00:02<00:01, 56.55it/s]concatenating: train:  62%|██████▏   | 165/266 [00:02<00:01, 56.37it/s]concatenating: train:  64%|██████▍   | 171/266 [00:03<00:01, 55.64it/s]concatenating: train:  67%|██████▋   | 178/266 [00:03<00:01, 58.27it/s]concatenating: train:  70%|██████▉   | 185/266 [00:03<00:01, 60.95it/s]concatenating: train:  72%|███████▏  | 192/266 [00:03<00:01, 59.43it/s]concatenating: train:  74%|███████▍  | 198/266 [00:03<00:01, 58.22it/s]concatenating: train:  77%|███████▋  | 204/266 [00:03<00:01, 56.71it/s]concatenating: train:  79%|███████▉  | 210/266 [00:03<00:00, 56.40it/s]concatenating: train:  81%|████████  | 216/266 [00:03<00:00, 54.75it/s]concatenating: train:  83%|████████▎ | 222/266 [00:03<00:00, 53.93it/s]concatenating: train:  86%|████████▌ | 228/266 [00:04<00:00, 52.70it/s]concatenating: train:  88%|████████▊ | 234/266 [00:04<00:00, 53.19it/s]concatenating: train:  90%|█████████ | 240/266 [00:04<00:00, 53.49it/s]concatenating: train:  92%|█████████▏| 246/266 [00:04<00:00, 54.44it/s]concatenating: train:  95%|█████████▍| 252/266 [00:04<00:00, 53.83it/s]concatenating: train:  97%|█████████▋| 258/266 [00:04<00:00, 52.89it/s]concatenating: train:  99%|█████████▉| 264/266 [00:04<00:00, 52.32it/s]concatenating: train: 100%|██████████| 266/266 [00:04<00:00, 56.19it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:24<01:14, 24.88s/it]Loading test:  50%|█████     | 2/4 [00:42<00:45, 22.76s/it]Loading test:  75%|███████▌  | 3/4 [01:01<00:21, 21.63s/it]Loading test: 100%|██████████| 4/4 [01:24<00:00, 21.88s/it]Loading test: 100%|██████████| 4/4 [01:24<00:00, 21.03s/it]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 59.26it/s]
Epoch 00044: val_mDice did not improve from 0.56793
Restoring model weights from the end of the best epoch
Epoch 00044: early stopping
{'val_loss': [-0.04652488020935444, -0.04719987492820229, -0.0746119993955198, -0.07550249768025948, -0.070718328985903, -0.07358153348769804, -0.06756860544585218, -0.08462493738742789, -0.04911873346627361, -0.0622022370948936, -0.08774486048655077, -0.08866694114274448, -0.07210784157117207, -0.059441866409597977, -0.07065665981534755, -0.06486798747621401, -0.0737025412646207, -0.07429497831999654, -0.07114405629008708, -0.04369687164823214, -0.06420537987441728, -0.0920571575712676, -0.07220002199814778, -0.06740718598317619, -0.0736822598058768, -0.06958656359200525, -0.09902793375982179, -0.043717771768569946, -0.06689247240622838, -0.09704971448941664, -0.058841941520722224, -0.040731562960027445, -0.08938559667781146, -0.09115463939279017, -0.029039512905809615, -0.03828894674326434, -0.05625616766587652, -0.09168332036245953, -0.06706592926021779, -0.06422146089901828, -0.06437446082932781, -0.09131446751681241, -0.039745972877500035, -0.038960371316984446], 'val_acc': [0.9944141687768878, 0.9948507426965116, 0.9944655461744829, 0.9948824234683105, 0.9946590686085248, 0.9948141249743375, 0.9946569086927356, 0.9949036568704278, 0.9943178756670519, 0.9946999881002638, 0.9949759545952382, 0.9947704359738514, 0.9948270483450457, 0.9948110514216952, 0.9948802710783602, 0.9946959843539228, 0.9949082656942233, 0.9945252366138227, 0.9947381350729201, 0.9947698248757256, 0.9947581327322758, 0.9947581297219402, 0.9947925875283251, 0.9947098318976585, 0.9946627607851317, 0.9948067376107881, 0.9946839866614101, 0.994759363959534, 0.9947895109653473, 0.9946845992647037, 0.9946362999352542, 0.9946009254816807, 0.9948116640249888, 0.9946575288218681, 0.9946550684745865, 0.9947430539612818, 0.9948338140742947, 0.9947630501154697, 0.9947753608828843, 0.9947599750576597, 0.994773514041997, 0.9947221351392341, 0.9947824366766997, 0.9948184327645735], 'val_mDice': [0.5634884242758607, 0.5618415257394916, 0.5664067128390977, 0.5679311580730207, 0.5584704326559798, 0.5611440084046788, 0.5585713983710968, 0.5356706893353751, 0.5547460273961828, 0.5394789380948679, 0.5418209236519731, 0.5437963862310756, 0.5612290285602964, 0.535727262187925, 0.5582477314153103, 0.5467693471923621, 0.5469099680582682, 0.5658171258189462, 0.5564665775557961, 0.5549627632805796, 0.5427208156418996, 0.5506456186148253, 0.561415000320083, 0.5518876539185794, 0.5629848974822748, 0.5561680136003879, 0.5646687557901999, 0.5550402083029651, 0.5508143996650522, 0.5607261896283939, 0.5436911954699647, 0.5490954797903095, 0.5452475647884186, 0.5489060000488253, 0.5560580321183108, 0.5441105058248015, 0.5500466306929035, 0.5504634208751448, 0.5511773388945695, 0.5458708905210399, 0.5457652044115644, 0.5494218688420575, 0.5470476792466761, 0.5460266624059942], 'loss': [0.07813144862645435, 0.050412731290483334, 0.04524822777782933, 0.042081015006489884, 0.040106246450278146, 0.038305475391021866, 0.037093031024968644, 0.03591136496077355, 0.03546226879712721, 0.034282177928415196, 0.03390348438290268, 0.033300964168342495, 0.03314721072546883, 0.03244276871404179, 0.03244014347520047, 0.031700638449004205, 0.03164508006927706, 0.031064616480359915, 0.03122367562790897, 0.029831964743159935, 0.02918060661782771, 0.02891762063370967, 0.029046299757884024, 0.02853586042102401, 0.028445463312244255, 0.028158303424085065, 0.028213277192396396, 0.028071199085383858, 0.02803853083794986, 0.027805473473770007, 0.02762169138464415, 0.027310199149872205, 0.027234566761915182, 0.02744812682054304, 0.026820010011353822, 0.02633813192241508, 0.02673642741597495, 0.02631778082510417, 0.026092834520702872, 0.026011991363274187, 0.026296954697707663, 0.02606320075868312, 0.025783301623307928, 0.025946884267595192], 'acc': [0.9918393081171692, 0.9943930170486376, 0.9948852264421539, 0.9951995736376097, 0.9953634399699344, 0.9955405973564344, 0.9956518252673595, 0.9957590133008174, 0.9958347462294173, 0.9959136908148145, 0.9959551108485314, 0.9959897276382932, 0.996040777151968, 0.9961047945125832, 0.996124201232897, 0.9961574508662767, 0.9961791604381413, 0.996206481156806, 0.9962140557236682, 0.9963534786100875, 0.9963777780311346, 0.9963994570542077, 0.9964275436351617, 0.9964426623052837, 0.9964458052273003, 0.996457171498905, 0.9964878130611654, 0.9964929891645528, 0.9965017963742956, 0.9965250810106651, 0.9965206187829526, 0.9965480568239695, 0.9965372216805012, 0.996559149016861, 0.9966096984793631, 0.9966101379270496, 0.9966294271290764, 0.9966387604919155, 0.9966579094812938, 0.996646117102607, 0.996656838407955, 0.996671835142738, 0.9966684185977696, 0.9966762755364984], 'mDice': [0.8480599894345076, 0.902050420841561, 0.9121042318349568, 0.9182649712306653, 0.9221253070723279, 0.925629540297679, 0.9279915587556287, 0.9302933981092592, 0.9311507307103487, 0.9334648021842546, 0.9341991089179797, 0.935382699497465, 0.9356628948308887, 0.9370367823913174, 0.937025714108861, 0.9384908173958043, 0.9385902373618744, 0.9397309133127094, 0.9394133143939188, 0.9421192341394847, 0.9434009038669808, 0.9439179234260718, 0.9436483978181592, 0.9446557450469985, 0.9448336959566705, 0.9454017698809525, 0.9452825675832925, 0.9455605528743704, 0.945618417858758, 0.9460741042341156, 0.9464422359460419, 0.9470536047445343, 0.9472070680782463, 0.9467701915014373, 0.9479953631560093, 0.9489550587256655, 0.9481536283682843, 0.9489849197731212, 0.9494242399572095, 0.9495872864110014, 0.9490171582061523, 0.949477633061966, 0.9500336427906172, 0.9497091148590255], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 1  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 48, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 48, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 48, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 48, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 48, 52, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 48, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 48, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 24, 26, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 24, 26, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 24, 26, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 24, 26, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 24, 26, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 24, 26, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 24, 26, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 24, 26, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 24, 26, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 12, 13, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 12, 13, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 12, 13, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 12, 13, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 12, 13, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 12, 13, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 12, 13, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 12, 13, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 12, 13, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 12, 13, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 24, 26, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 24, 26, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 24, 26, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 24, 26, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 24, 26, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 24, 26, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 24, 26, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 24, 26, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 24, 26, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24, 26, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 48, 52, 30)   25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 48, 52, 60)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 48, 52, 30)   16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 48, 52, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 48, 52, 30)   0           batch_normalization_9[0][0]      2020-01-21 01:41:49.585810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 01:41:49.585902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 01:41:49.585915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 01:41:49.585922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 01:41:49.586189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 48, 52, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 48, 52, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 48, 52, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 48, 52, 90)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 48, 52, 90)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 48, 52, 13)   1183        dropout_5[0][0]                  
==================================================================================================
Total params: 501,343
Trainable params: 500,143
Non-trainable params: 1,200
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1
------------------------------------------------------------------
class_weights [6.34757741e-02 3.28986124e-02 7.69288803e-02 9.55880020e-03
 2.76653444e-02 7.23785754e-03 8.42779903e-02 1.14342115e-01
 8.97811296e-02 1.36408778e-02 2.91088081e-01 1.88843905e-01
 2.60631767e-04]
Train on 16955 samples, validate on 244 samples
Epoch 1/300
 - 40s - loss: 0.4610 - acc: 0.9316 - mDice: 0.5030 - val_loss: 0.2253 - val_acc: 0.9374 - val_mDice: 0.1968

Epoch 00001: val_mDice improved from -inf to 0.19676, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300
 - 34s - loss: 0.3743 - acc: 0.9424 - mDice: 0.5966 - val_loss: -1.6671e-01 - val_acc: 0.9502 - val_mDice: 0.2346

Epoch 00002: val_mDice improved from 0.19676 to 0.23461, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300
 - 34s - loss: 0.3597 - acc: 0.9447 - mDice: 0.6124 - val_loss: 0.0037 - val_acc: 0.9496 - val_mDice: 0.2283

Epoch 00003: val_mDice did not improve from 0.23461
Epoch 4/300
 - 34s - loss: 0.3450 - acc: 0.9461 - mDice: 0.6283 - val_loss: -1.6704e-01 - val_acc: 0.9467 - val_mDice: 0.2292

Epoch 00004: val_mDice did not improve from 0.23461
Epoch 5/300
 - 34s - loss: 0.3360 - acc: 0.9474 - mDice: 0.6380 - val_loss: -6.8481e-02 - val_acc: 0.9437 - val_mDice: 0.2216

Epoch 00005: val_mDice did not improve from 0.23461
Epoch 6/300
 - 34s - loss: 0.3318 - acc: 0.9482 - mDice: 0.6424 - val_loss: -1.0004e-01 - val_acc: 0.9454 - val_mDice: 0.2375

Epoch 00006: val_mDice improved from 0.23461 to 0.23747, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 7/300
 - 35s - loss: 0.3244 - acc: 0.9488 - mDice: 0.6504 - val_loss: -1.4839e-01 - val_acc: 0.9460 - val_mDice: 0.2257

Epoch 00007: val_mDice did not improve from 0.23747
Epoch 8/300
 - 35s - loss: 0.3180 - acc: 0.9499 - mDice: 0.6574 - val_loss: -1.4540e-01 - val_acc: 0.9485 - val_mDice: 0.2254

Epoch 00008: val_mDice did not improve from 0.23747
Epoch 9/300
 - 34s - loss: 0.3145 - acc: 0.9500 - mDice: 0.6612 - val_loss: -2.3670e-01 - val_acc: 0.9497 - val_mDice: 0.2320

Epoch 00009: val_mDice did not improve from 0.23747
Epoch 10/300
 - 34s - loss: 0.3120 - acc: 0.9506 - mDice: 0.6639 - val_loss: -1.9721e-01 - val_acc: 0.9485 - val_mDice: 0.2368

Epoch 00010: val_mDice did not improve from 0.23747
Epoch 11/300
 - 34s - loss: 0.3061 - acc: 0.9512 - mDice: 0.6702 - val_loss: -1.8130e-01 - val_acc: 0.9489 - val_mDice: 0.2305

Epoch 00011: val_mDice did not improve from 0.23747
Epoch 12/300
 - 34s - loss: 0.3057 - acc: 0.9513 - mDice: 0.6706 - val_loss: -2.1085e-01 - val_acc: 0.9484 - val_mDice: 0.2373

Epoch 00012: val_mDice did not improve from 0.23747
Epoch 13/300
 - 34s - loss: 0.3041 - acc: 0.9515 - mDice: 0.6723 - val_loss: -1.5761e-01 - val_acc: 0.9470 - val_mDice: 0.2241

Epoch 00013: val_mDice did not improve from 0.23747
Epoch 14/300
 - 34s - loss: 0.3011 - acc: 0.9518 - mDice: 0.6756 - val_loss: -2.0610e-01 - val_acc: 0.9499 - val_mDice: 0.2303

Epoch 00014: val_mDice did not improve from 0.23747
Epoch 15/300
 - 34s - loss: 0.2970 - acc: 0.9523 - mDice: 0.6800 - val_loss: -1.6278e-01 - val_acc: 0.9475 - val_mDice: 0.2273

Epoch 00015: val_mDice did not improve from 0.23747
Epoch 16/300
 - 34s - loss: 0.2988 - acc: 0.9522 - mDice: 0.6780 - val_loss: 0.0068 - val_acc: 0.9375 - val_mDice: 0.2013

Epoch 00016: val_mDice did not improve from 0.23747
Epoch 17/300
 - 35s - loss: 0.2978 - acc: 0.9523 - mDice: 0.6791 - val_loss: -2.5291e-01 - val_acc: 0.9499 - val_mDice: 0.2292

Epoch 00017: val_mDice did not improve from 0.23747
Epoch 18/300
 - 34s - loss: 0.3014 - acc: 0.9501 - mDice: 0.6631 - val_loss: -2.4742e-01 - val_acc: 0.9514 - val_mDice: 0.2275

Epoch 00018: val_mDice did not improve from 0.23747
Epoch 19/300
 - 35s - loss: 0.2986 - acc: 0.9479 - mDice: 0.6370 - val_loss: -2.5595e-01 - val_acc: 0.9504 - val_mDice: 0.2245

Epoch 00019: val_mDice did not improve from 0.23747
Epoch 20/300
 - 36s - loss: 0.2767 - acc: 0.9491 - mDice: 0.6502 - val_loss: -2.7257e-01 - val_acc: 0.9509 - val_mDice: 0.2339

Epoch 00020: val_mDice did not improve from 0.23747
Epoch 21/300
 - 35s - loss: 0.3123 - acc: 0.9456 - mDice: 0.5803 - val_loss: -2.7259e-01 - val_acc: 0.9529 - val_mDice: 0.2174

Epoch 00021: val_mDice did not improve from 0.23747

Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 22/300
 - 35s - loss: 0.2842 - acc: 0.9481 - mDice: 0.6007 - val_loss: -2.5969e-01 - val_acc: 0.9504 - val_mDice: 0.2135

Epoch 00022: val_mDice did not improve from 0.23747
Epoch 23/300
 - 36s - loss: 0.2725 - acc: 0.9488 - mDice: 0.6134 - val_loss: -2.7192e-01 - val_acc: 0.9510 - val_mDice: 0.2116

Epoch 00023: val_mDice did not improve from 0.23747
Epoch 24/300
 - 37s - loss: 0.2636 - acc: 0.9493 - mDice: 0.6213 - val_loss: -2.7026e-01 - val_acc: 0.9500 - val_mDice: 0.2210

Epoch 00024: val_mDice did not improve from 0.23747
Epoch 25/300
 - 36s - loss: 0.2665 - acc: 0.9491 - mDice: 0.6083 - val_loss: -2.3997e-01 - val_acc: 0.9488 - val_mDice: 0.1905

Epoch 00025: val_mDice did not improve from 0.23747
Epoch 26/300
 - 36s - loss: 0.2599 - acc: 0.9495 - mDice: 0.6126 - val_loss: -2.6762e-01 - val_acc: 0.9506 - val_mDice: 0.2075

Epoch 00026: val_mDice did not improve from 0.23747
Epoch 27/300
 - 37s - loss: 0.2544 - acc: 0.9498 - mDice: 0.6190 - val_loss: -2.5999e-01 - val_acc: 0.9515 - val_mDice: 0.2112

Epoch 00027: val_mDice did not improve from 0.23747
Epoch 28/300
 - 37s - loss: 0.2611 - acc: 0.9492 - mDice: 0.6123 - val_loss: -2.6918e-01 - val_acc: 0.9494 - val_mDice: 0.2075

Epoch 00028: val_mDice did not improve from 0.23747
Epoch 29/300
 - 37s - loss: 0.2574 - acc: 0.9496 - mDice: 0.6146 - val_loss: -2.7375e-01 - val_acc: 0.9502 - val_mDice: 0.2125

Epoch 00029: val_mDice did not improve from 0.23747
Epoch 30/300
 - 36s - loss: 0.2568 - acc: 0.9501 - mDice: 0.6193 - val_loss: -2.7762e-01 - val_acc: 0.9512 - val_mDice: 0.2097

Epoch 00030: val_mDice did not improve from 0.23747
Epoch 31/300
 - 34s - loss: 0.2611 - acc: 0.9493 - mDice: 0.6133 - val_loss: -2.8018e-01 - val_acc: 0.9522 - val_mDice: 0.2126

Epoch 00031: val_mDice did not improve from 0.23747
Epoch 32/300
 - 35s - loss: 0.2551 - acc: 0.9501 - mDice: 0.6160 - val_loss: -2.5842e-01 - val_acc: 0.9500 - val_mDice: 0.2092

Epoch 00032: val_mDice did not improve from 0.23747
Epoch 33/300
 - 34s - loss: 0.2566 - acc: 0.9497 - mDice: 0.6186 - val_loss: -2.0465e-01 - val_acc: 0.9448 - val_mDice: 0.1881

Epoch 00033: val_mDice did not improve from 0.23747
Epoch 34/300
 - 34s - loss: 0.2521 - acc: 0.9497 - mDice: 0.6176 - val_loss: -2.6595e-01 - val_acc: 0.9487 - val_mDice: 0.2042

Epoch 00034: val_mDice did not improve from 0.23747
Epoch 35/300
 - 35s - loss: 0.2620 - acc: 0.9491 - mDice: 0.6113 - val_loss: -2.4310e-01 - val_acc: 0.9487 - val_mDice: 0.1993

Epoch 00035: val_mDice did not improve from 0.23747
Epoch 36/300
 - 35s - loss: 0.2550 - acc: 0.9496 - mDice: 0.6153 - val_loss: -2.5276e-01 - val_acc: 0.9502 - val_mDice: 0.2042

Epoch 00036: val_mDice did not improve from 0.23747

Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 37/300
 - 38s - loss: 0.2434 - acc: 0.9506 - mDice: 0.6272 - val_loss: -2.8473e-01 - val_acc: 0.9511 - val_mDice: 0.2107

Epoch 00037: val_mDice did not improve from 0.23747
Epoch 38/300
 - 38s - loss: 0.2436 - acc: 0.9510 - mDice: 0.6275 - val_loss: -2.8344e-01 - val_acc: 0.9512 - val_mDice: 0.2093

Epoch 00038: val_mDice did not improve from 0.23747
Epoch 39/300
 - 37s - loss: 0.2386 - acc: 0.9511 - mDice: 0.6283 - val_loss: -2.9324e-01 - val_acc: 0.9523 - val_mDice: 0.2133

Epoch 00039: val_mDice did not improve from 0.23747
Epoch 40/300
 - 36s - loss: 0.2357 - acc: 0.9512 - mDice: 0.6297 - val_loss: -2.8584e-01 - val_acc: 0.9516 - val_mDice: 0.2118

Epoch 00040: val_mDice did not improve from 0.23747
Epoch 41/300
 - 37s - loss: 0.2333 - acc: 0.9515 - mDice: 0.6333 - val_loss: -2.7417e-01 - val_acc: 0.9508 - val_mDice: 0.2060

Epoch 00041: val_mDice did not improve from 0.23747
Epoch 42/300
 - 37s - loss: 0.2335 - acc: 0.9513 - mDice: 0.6322 - val_loss: -2.7217e-01 - val_acc: 0.9516 - val_mDice: 0.2107

Epoch 00042: val_mDice did not improve from 0.23747
Epoch 43/300
 - 37s - loss: 0.2286 - acc: 0.9517 - mDice: 0.6325 - val_loss: -2.8517e-01 - val_acc: 0.9513 - val_mDice: 0.2111

Epoch 00043: val_mDice did not improve from 0.23747
Epoch 44/300
 - 37s - loss: 0.2368 - acc: 0.9515 - mDice: 0.6344 - val_loss: -2.8368e-01 - val_acc: 0.9516 - val_mDice: 0.2095

Epoch 00044: val_mDice did not improve from 0.23747
Epoch 45/300
 - 36s - loss: 0.2390 - acc: 0.9512 - mDice: 0.6296 - val_loss: -2.6982e-01 - val_acc: 0.9512 - val_mDice: 0.2094

Epoch 00045: val_mDice did not improve from 0.23747
Epoch 46/300
 - 35s - loss: 0.2342 - acc: 0.9515 - mDice: 0.6345 - val_loss: -2.7876e-01 - val_acc: 0.9507 - val_mDice: 0.2110

predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:01<00:04,  1.49s/it]predicting test subjects:  50%|█████     | 2/4 [00:02<00:02,  1.33s/it]predicting test subjects:  75%|███████▌  | 3/4 [00:03<00:01,  1.20s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:15,  3.50it/s]Loading train:   1%|          | 2/266 [00:00<01:13,  3.59it/s]Loading train:   1%|          | 3/266 [00:00<01:08,  3.85it/s]Loading train:   2%|▏         | 4/266 [00:00<01:06,  3.96it/s]Loading train:   2%|▏         | 5/266 [00:01<01:06,  3.91it/s]Loading train:   2%|▏         | 6/266 [00:01<01:06,  3.90it/s]Loading train:   3%|▎         | 7/266 [00:01<01:06,  3.91it/s]Loading train:   3%|▎         | 8/266 [00:02<01:06,  3.90it/s]Loading train:   3%|▎         | 9/266 [00:02<01:05,  3.90it/s]Loading train:   4%|▍         | 10/266 [00:02<01:05,  3.91it/s]Loading train:   4%|▍         | 11/266 [00:02<01:05,  3.92it/s]Loading train:   5%|▍         | 12/266 [00:03<01:05,  3.89it/s]Loading train:   5%|▍         | 13/266 [00:03<01:05,  3.88it/s]Loading train:   5%|▌         | 14/266 [00:03<01:04,  3.90it/s]Loading train:   6%|▌         | 15/266 [00:03<01:04,  3.91it/s]Loading train:   6%|▌         | 16/266 [00:04<01:04,  3.90it/s]Loading train:   6%|▋         | 17/266 [00:04<01:04,  3.89it/s]Loading train:   7%|▋         | 18/266 [00:04<01:03,  3.88it/s]Loading train:   7%|▋         | 19/266 [00:04<01:04,  3.83it/s]Loading train:   8%|▊         | 20/266 [00:05<01:03,  3.85it/s]Loading train:   8%|▊         | 21/266 [00:05<01:03,  3.86it/s]Loading train:   8%|▊         | 22/266 [00:05<01:02,  3.90it/s]Loading train:   9%|▊         | 23/266 [00:05<01:03,  3.84it/s]Loading train:   9%|▉         | 24/266 [00:06<01:01,  3.91it/s]Loading train:   9%|▉         | 25/266 [00:06<01:00,  3.99it/s]Loading train:  10%|▉         | 26/266 [00:06<01:00,  3.98it/s]Loading train:  10%|█         | 27/266 [00:06<00:59,  4.01it/s]Loading train:  11%|█         | 28/266 [00:07<00:58,  4.04it/s]Loading train:  11%|█         | 29/266 [00:07<00:58,  4.08it/s]Loading train:  11%|█▏        | 30/266 [00:07<00:57,  4.09it/s]Loading train:  12%|█▏        | 31/266 [00:07<00:57,  4.08it/s]Loading train:  12%|█▏        | 32/266 [00:08<00:57,  4.09it/s]Loading train:  12%|█▏        | 33/266 [00:08<00:56,  4.10it/s]Loading train:  13%|█▎        | 34/266 [00:08<00:56,  4.10it/s]Loading train:  13%|█▎        | 35/266 [00:08<00:56,  4.10it/s]Loading train:  14%|█▎        | 36/266 [00:09<00:55,  4.12it/s]Loading train:  14%|█▍        | 37/266 [00:09<00:55,  4.13it/s]Loading train:  14%|█▍        | 38/266 [00:09<00:54,  4.15it/s]Loading train:  15%|█▍        | 39/266 [00:09<00:55,  4.12it/s]Loading train:  15%|█▌        | 40/266 [00:10<00:54,  4.11it/s]Loading train:  15%|█▌        | 41/266 [00:10<00:54,  4.10it/s]Loading train:  16%|█▌        | 42/266 [00:10<00:52,  4.29it/s]Loading train:  16%|█▌        | 43/266 [00:10<00:50,  4.41it/s]Loading train:  17%|█▋        | 44/266 [00:10<00:49,  4.52it/s]Loading train:  17%|█▋        | 45/266 [00:11<00:47,  4.62it/s]Loading train:  17%|█▋        | 46/266 [00:11<00:46,  4.69it/s]Loading train:  18%|█▊        | 47/266 [00:11<00:46,  4.76it/s]Loading train:  18%|█▊        | 48/266 [00:11<00:45,  4.80it/s]Loading train:  18%|█▊        | 49/266 [00:11<00:45,  4.81it/s]Loading train:  19%|█▉        | 50/266 [00:12<00:44,  4.80it/s]Loading train:  19%|█▉        | 51/266 [00:12<00:44,  4.86it/s]Loading train:  20%|█▉        | 52/266 [00:12<00:44,  4.80it/s]Loading train:  20%|█▉        | 53/266 [00:12<00:43,  4.86it/s]Loading train:  20%|██        | 54/266 [00:12<00:43,  4.86it/s]Loading train:  21%|██        | 55/266 [00:13<00:43,  4.87it/s]Loading train:  21%|██        | 56/266 [00:13<00:43,  4.85it/s]Loading train:  21%|██▏       | 57/266 [00:13<00:42,  4.90it/s]Loading train:  22%|██▏       | 58/266 [00:13<00:42,  4.92it/s]Loading train:  22%|██▏       | 59/266 [00:13<00:42,  4.93it/s]Loading train:  23%|██▎       | 60/266 [00:14<00:42,  4.87it/s]Loading train:  23%|██▎       | 61/266 [00:14<00:43,  4.72it/s]Loading train:  23%|██▎       | 62/266 [00:14<00:44,  4.59it/s]Loading train:  24%|██▎       | 63/266 [00:14<00:43,  4.63it/s]Loading train:  24%|██▍       | 64/266 [00:15<00:43,  4.60it/s]Loading train:  24%|██▍       | 65/266 [00:15<00:43,  4.60it/s]Loading train:  25%|██▍       | 66/266 [00:15<00:43,  4.64it/s]Loading train:  25%|██▌       | 67/266 [00:15<00:42,  4.67it/s]Loading train:  26%|██▌       | 68/266 [00:15<00:42,  4.64it/s]Loading train:  26%|██▌       | 69/266 [00:16<00:42,  4.65it/s]Loading train:  26%|██▋       | 70/266 [00:16<00:41,  4.68it/s]Loading train:  27%|██▋       | 71/266 [00:16<00:41,  4.70it/s]Loading train:  27%|██▋       | 72/266 [00:16<00:41,  4.69it/s]Loading train:  27%|██▋       | 73/266 [00:17<00:41,  4.66it/s]Loading train:  28%|██▊       | 74/266 [00:17<00:41,  4.64it/s]Loading train:  28%|██▊       | 75/266 [00:17<00:40,  4.67it/s]Loading train:  29%|██▊       | 76/266 [00:17<00:40,  4.64it/s]Loading train:  29%|██▉       | 77/266 [00:17<00:41,  4.51it/s]Loading train:  29%|██▉       | 78/266 [00:18<00:43,  4.36it/s]Loading train:  30%|██▉       | 79/266 [00:18<00:43,  4.29it/s]Loading train:  30%|███       | 80/266 [00:18<00:44,  4.20it/s]Loading train:  30%|███       | 81/266 [00:18<00:44,  4.12it/s]Loading train:  31%|███       | 82/266 [00:19<00:45,  4.01it/s]Loading train:  31%|███       | 83/266 [00:19<00:46,  3.92it/s]Loading train:  32%|███▏      | 84/266 [00:19<00:47,  3.85it/s]Loading train:  32%|███▏      | 85/266 [00:19<00:47,  3.84it/s]Loading train:  32%|███▏      | 86/266 [00:20<00:46,  3.87it/s]Loading train:  33%|███▎      | 87/266 [00:20<00:45,  3.90it/s]Loading train:  33%|███▎      | 88/266 [00:20<00:44,  3.97it/s]Loading train:  33%|███▎      | 89/266 [00:20<00:44,  3.95it/s]Loading train:  34%|███▍      | 90/266 [00:21<00:43,  4.01it/s]Loading train:  34%|███▍      | 91/266 [00:21<00:43,  4.07it/s]Loading train:  35%|███▍      | 92/266 [00:21<00:42,  4.10it/s]Loading train:  35%|███▍      | 93/266 [00:21<00:43,  4.02it/s]Loading train:  35%|███▌      | 94/266 [00:22<00:43,  3.99it/s]Loading train:  36%|███▌      | 95/266 [00:22<00:42,  4.05it/s]Loading train:  36%|███▌      | 96/266 [00:22<00:41,  4.09it/s]Loading train:  36%|███▋      | 97/266 [00:22<00:43,  3.89it/s]Loading train:  37%|███▋      | 98/266 [00:23<00:42,  3.94it/s]Loading train:  37%|███▋      | 99/266 [00:23<00:40,  4.16it/s]Loading train:  38%|███▊      | 100/266 [00:23<00:39,  4.20it/s]Loading train:  38%|███▊      | 101/266 [00:23<00:38,  4.32it/s]Loading train:  38%|███▊      | 102/266 [00:24<00:37,  4.38it/s]Loading train:  39%|███▊      | 103/266 [00:24<00:37,  4.37it/s]Loading train:  39%|███▉      | 104/266 [00:24<00:37,  4.35it/s]Loading train:  39%|███▉      | 105/266 [00:24<00:36,  4.38it/s]Loading train:  40%|███▉      | 106/266 [00:24<00:36,  4.41it/s]Loading train:  40%|████      | 107/266 [00:25<00:36,  4.42it/s]Loading train:  41%|████      | 108/266 [00:25<00:36,  4.37it/s]Loading train:  41%|████      | 109/266 [00:25<00:35,  4.48it/s]Loading train:  41%|████▏     | 110/266 [00:25<00:34,  4.55it/s]Loading train:  42%|████▏     | 111/266 [00:26<00:34,  4.55it/s]Loading train:  42%|████▏     | 112/266 [00:26<00:33,  4.58it/s]Loading train:  42%|████▏     | 113/266 [00:26<00:33,  4.57it/s]Loading train:  43%|████▎     | 114/266 [00:26<00:33,  4.55it/s]Loading train:  43%|████▎     | 115/266 [00:26<00:34,  4.41it/s]Loading train:  44%|████▎     | 116/266 [00:27<00:34,  4.38it/s]Loading train:  44%|████▍     | 117/266 [00:27<00:34,  4.37it/s]Loading train:  44%|████▍     | 118/266 [00:27<00:33,  4.40it/s]Loading train:  45%|████▍     | 119/266 [00:27<00:34,  4.23it/s]Loading train:  45%|████▌     | 120/266 [00:28<00:35,  4.08it/s]Loading train:  45%|████▌     | 121/266 [00:28<00:36,  3.92it/s]Loading train:  46%|████▌     | 122/266 [00:28<00:36,  3.90it/s]Loading train:  46%|████▌     | 123/266 [00:28<00:36,  3.92it/s]Loading train:  47%|████▋     | 124/266 [00:29<00:36,  3.90it/s]Loading train:  47%|████▋     | 125/266 [00:29<00:36,  3.91it/s]Loading train:  47%|████▋     | 126/266 [00:29<00:35,  3.94it/s]Loading train:  48%|████▊     | 127/266 [00:30<00:35,  3.95it/s]Loading train:  48%|████▊     | 128/266 [00:30<00:35,  3.93it/s]Loading train:  48%|████▊     | 129/266 [00:30<00:35,  3.91it/s]Loading train:  49%|████▉     | 130/266 [00:30<00:34,  3.89it/s]Loading train:  49%|████▉     | 131/266 [00:31<00:34,  3.88it/s]Loading train:  50%|████▉     | 132/266 [00:31<00:34,  3.92it/s]Loading train:  50%|█████     | 133/266 [00:31<00:34,  3.87it/s]Loading train:  50%|█████     | 134/266 [00:31<00:34,  3.84it/s]Loading train:  51%|█████     | 135/266 [00:32<00:34,  3.82it/s]Loading train:  51%|█████     | 136/266 [00:32<00:34,  3.81it/s]Loading train:  52%|█████▏    | 137/266 [00:32<00:32,  3.92it/s]Loading train:  52%|█████▏    | 138/266 [00:32<00:31,  4.00it/s]Loading train:  52%|█████▏    | 139/266 [00:33<00:31,  4.03it/s]Loading train:  53%|█████▎    | 140/266 [00:33<00:30,  4.08it/s]Loading train:  53%|█████▎    | 141/266 [00:33<00:30,  4.16it/s]Loading train:  53%|█████▎    | 142/266 [00:33<00:29,  4.23it/s]Loading train:  54%|█████▍    | 143/266 [00:33<00:28,  4.26it/s]Loading train:  54%|█████▍    | 144/266 [00:34<00:28,  4.27it/s]Loading train:  55%|█████▍    | 145/266 [00:34<00:28,  4.30it/s]Loading train:  55%|█████▍    | 146/266 [00:34<00:27,  4.30it/s]Loading train:  55%|█████▌    | 147/266 [00:34<00:27,  4.31it/s]Loading train:  56%|█████▌    | 148/266 [00:35<00:27,  4.32it/s]Loading train:  56%|█████▌    | 149/266 [00:35<00:27,  4.33it/s]Loading train:  56%|█████▋    | 150/266 [00:35<00:26,  4.34it/s]Loading train:  57%|█████▋    | 151/266 [00:35<00:26,  4.28it/s]Loading train:  57%|█████▋    | 152/266 [00:36<00:26,  4.29it/s]Loading train:  58%|█████▊    | 153/266 [00:36<00:26,  4.29it/s]Loading train:  58%|█████▊    | 154/266 [00:36<00:25,  4.31it/s]Loading train:  58%|█████▊    | 155/266 [00:36<00:24,  4.50it/s]Loading train:  59%|█████▊    | 156/266 [00:36<00:23,  4.68it/s]Loading train:  59%|█████▉    | 157/266 [00:37<00:22,  4.76it/s]Loading train:  59%|█████▉    | 158/266 [00:37<00:22,  4.74it/s]Loading train:  60%|█████▉    | 159/266 [00:37<00:22,  4.82it/s]Loading train:  60%|██████    | 160/266 [00:37<00:21,  4.95it/s]Loading train:  61%|██████    | 161/266 [00:37<00:20,  5.03it/s]Loading train:  61%|██████    | 162/266 [00:38<00:20,  5.09it/s]Loading train:  61%|██████▏   | 163/266 [00:38<00:20,  5.14it/s]Loading train:  62%|██████▏   | 164/266 [00:38<00:19,  5.18it/s]Loading train:  62%|██████▏   | 165/266 [00:38<00:19,  5.18it/s]Loading train:  62%|██████▏   | 166/266 [00:38<00:19,  5.19it/s]Loading train:  63%|██████▎   | 167/266 [00:39<00:19,  5.16it/s]Loading train:  63%|██████▎   | 168/266 [00:39<00:18,  5.16it/s]Loading train:  64%|██████▎   | 169/266 [00:39<00:18,  5.20it/s]Loading train:  64%|██████▍   | 170/266 [00:39<00:18,  5.21it/s]Loading train:  64%|██████▍   | 171/266 [00:39<00:18,  5.20it/s]Loading train:  65%|██████▍   | 172/266 [00:40<00:18,  5.20it/s]Loading train:  65%|██████▌   | 173/266 [00:40<00:18,  4.97it/s]Loading train:  65%|██████▌   | 174/266 [00:40<00:18,  4.85it/s]Loading train:  66%|██████▌   | 175/266 [00:40<00:19,  4.78it/s]Loading train:  66%|██████▌   | 176/266 [00:40<00:19,  4.72it/s]Loading train:  67%|██████▋   | 177/266 [00:41<00:19,  4.64it/s]Loading train:  67%|██████▋   | 178/266 [00:41<00:19,  4.63it/s]Loading train:  67%|██████▋   | 179/266 [00:41<00:18,  4.60it/s]Loading train:  68%|██████▊   | 180/266 [00:41<00:18,  4.58it/s]Loading train:  68%|██████▊   | 181/266 [00:42<00:18,  4.54it/s]Loading train:  68%|██████▊   | 182/266 [00:42<00:18,  4.55it/s]Loading train:  69%|██████▉   | 183/266 [00:42<00:18,  4.51it/s]Loading train:  69%|██████▉   | 184/266 [00:42<00:18,  4.49it/s]Loading train:  70%|██████▉   | 185/266 [00:42<00:18,  4.36it/s]Loading train:  70%|██████▉   | 186/266 [00:43<00:18,  4.42it/s]Loading train:  70%|███████   | 187/266 [00:43<00:17,  4.48it/s]Loading train:  71%|███████   | 188/266 [00:43<00:17,  4.51it/s]Loading train:  71%|███████   | 189/266 [00:43<00:16,  4.55it/s]Loading train:  71%|███████▏  | 190/266 [00:44<00:16,  4.53it/s]Loading train:  72%|███████▏  | 191/266 [00:44<00:16,  4.43it/s]Loading train:  72%|███████▏  | 192/266 [00:44<00:16,  4.44it/s]Loading train:  73%|███████▎  | 193/266 [00:44<00:16,  4.43it/s]Loading train:  73%|███████▎  | 194/266 [00:44<00:16,  4.25it/s]Loading train:  73%|███████▎  | 195/266 [00:45<00:16,  4.35it/s]Loading train:  74%|███████▎  | 196/266 [00:45<00:15,  4.42it/s]Loading train:  74%|███████▍  | 197/266 [00:45<00:15,  4.46it/s]Loading train:  74%|███████▍  | 198/266 [00:45<00:15,  4.47it/s]Loading train:  75%|███████▍  | 199/266 [00:46<00:14,  4.50it/s]Loading train:  75%|███████▌  | 200/266 [00:46<00:14,  4.53it/s]Loading train:  76%|███████▌  | 201/266 [00:46<00:14,  4.53it/s]Loading train:  76%|███████▌  | 202/266 [00:46<00:14,  4.56it/s]Loading train:  76%|███████▋  | 203/266 [00:46<00:13,  4.59it/s]Loading train:  77%|███████▋  | 204/266 [00:47<00:13,  4.58it/s]Loading train:  77%|███████▋  | 205/266 [00:47<00:13,  4.59it/s]Loading train:  77%|███████▋  | 206/266 [00:47<00:13,  4.58it/s]Loading train:  78%|███████▊  | 207/266 [00:47<00:12,  4.59it/s]Loading train:  78%|███████▊  | 208/266 [00:48<00:12,  4.50it/s]Loading train:  79%|███████▊  | 209/266 [00:48<00:12,  4.39it/s]Loading train:  79%|███████▉  | 210/266 [00:48<00:12,  4.43it/s]Loading train:  79%|███████▉  | 211/266 [00:48<00:12,  4.41it/s]Loading train:  80%|███████▉  | 212/266 [00:48<00:12,  4.47it/s]Loading train:  80%|████████  | 213/266 [00:49<00:11,  4.54it/s]Loading train:  80%|████████  | 214/266 [00:49<00:11,  4.58it/s]Loading train:  81%|████████  | 215/266 [00:49<00:11,  4.58it/s]Loading train:  81%|████████  | 216/266 [00:49<00:10,  4.65it/s]Loading train:  82%|████████▏ | 217/266 [00:50<00:10,  4.70it/s]Loading train:  82%|████████▏ | 218/266 [00:50<00:10,  4.72it/s]Loading train:  82%|████████▏ | 219/266 [00:50<00:10,  4.68it/s]Loading train:  83%|████████▎ | 220/266 [00:50<00:09,  4.69it/s]Loading train:  83%|████████▎ | 221/266 [00:50<00:09,  4.68it/s]Loading train:  83%|████████▎ | 222/266 [00:51<00:09,  4.64it/s]Loading train:  84%|████████▍ | 223/266 [00:51<00:09,  4.66it/s]Loading train:  84%|████████▍ | 224/266 [00:51<00:08,  4.69it/s]Loading train:  85%|████████▍ | 225/266 [00:51<00:08,  4.66it/s]Loading train:  85%|████████▍ | 226/266 [00:51<00:08,  4.68it/s]Loading train:  85%|████████▌ | 227/266 [00:52<00:08,  4.67it/s]Loading train:  86%|████████▌ | 228/266 [00:52<00:08,  4.68it/s]Loading train:  86%|████████▌ | 229/266 [00:52<00:07,  4.70it/s]Loading train:  86%|████████▋ | 230/266 [00:52<00:07,  4.72it/s]Loading train:  87%|████████▋ | 231/266 [00:53<00:07,  4.60it/s]Loading train:  87%|████████▋ | 232/266 [00:53<00:07,  4.37it/s]Loading train:  88%|████████▊ | 233/266 [00:53<00:07,  4.46it/s]Loading train:  88%|████████▊ | 234/266 [00:53<00:07,  4.49it/s]Loading train:  88%|████████▊ | 235/266 [00:53<00:06,  4.53it/s]Loading train:  89%|████████▊ | 236/266 [00:54<00:06,  4.50it/s]Loading train:  89%|████████▉ | 237/266 [00:54<00:06,  4.49it/s]Loading train:  89%|████████▉ | 238/266 [00:54<00:06,  4.50it/s]Loading train:  90%|████████▉ | 239/266 [00:54<00:05,  4.52it/s]Loading train:  90%|█████████ | 240/266 [00:55<00:05,  4.53it/s]Loading train:  91%|█████████ | 241/266 [00:55<00:05,  4.60it/s]Loading train:  91%|█████████ | 242/266 [00:55<00:05,  4.62it/s]Loading train:  91%|█████████▏| 243/266 [00:55<00:04,  4.65it/s]Loading train:  92%|█████████▏| 244/266 [00:55<00:04,  4.65it/s]Loading train:  92%|█████████▏| 245/266 [00:56<00:04,  4.67it/s]Loading train:  92%|█████████▏| 246/266 [00:56<00:04,  4.68it/s]Loading train:  93%|█████████▎| 247/266 [00:56<00:04,  4.68it/s]Loading train:  93%|█████████▎| 248/266 [00:56<00:03,  4.67it/s]Loading train:  94%|█████████▎| 249/266 [00:56<00:03,  4.54it/s]Loading train:  94%|█████████▍| 250/266 [00:57<00:03,  4.46it/s]Loading train:  94%|█████████▍| 251/266 [00:57<00:03,  4.38it/s]Loading train:  95%|█████████▍| 252/266 [00:57<00:03,  4.31it/s]Loading train:  95%|█████████▌| 253/266 [00:57<00:03,  4.27it/s]Loading train:  95%|█████████▌| 254/266 [00:58<00:02,  4.23it/s]Loading train:  96%|█████████▌| 255/266 [00:58<00:02,  4.17it/s]Loading train:  96%|█████████▌| 256/266 [00:58<00:02,  4.12it/s]Loading train:  97%|█████████▋| 257/266 [00:58<00:02,  4.08it/s]Loading train:  97%|█████████▋| 258/266 [00:59<00:02,  3.94it/s]Loading train:  97%|█████████▋| 259/266 [00:59<00:01,  4.00it/s]Loading train:  98%|█████████▊| 260/266 [00:59<00:01,  4.06it/s]Loading train:  98%|█████████▊| 261/266 [00:59<00:01,  4.10it/s]Loading train:  98%|█████████▊| 262/266 [01:00<00:00,  4.09it/s]Loading train:  99%|█████████▉| 263/266 [01:00<00:00,  4.11it/s]Loading train:  99%|█████████▉| 264/266 [01:00<00:00,  4.11it/s]Loading train: 100%|█████████▉| 265/266 [01:00<00:00,  4.12it/s]Loading train: 100%|██████████| 266/266 [01:01<00:00,  4.12it/s]Loading train: 100%|██████████| 266/266 [01:01<00:00,  4.35it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/266 [00:00<00:04, 58.27it/s]concatenating: train:   5%|▍         | 12/266 [00:00<00:04, 58.29it/s]concatenating: train:   7%|▋         | 18/266 [00:00<00:04, 58.22it/s]concatenating: train:   9%|▉         | 24/266 [00:00<00:04, 57.60it/s]concatenating: train:  11%|█▏        | 30/266 [00:00<00:04, 57.35it/s]concatenating: train:  14%|█▎        | 36/266 [00:00<00:04, 56.37it/s]concatenating: train:  16%|█▌        | 42/266 [00:00<00:03, 57.30it/s]concatenating: train:  18%|█▊        | 49/266 [00:00<00:03, 58.81it/s]concatenating: train:  21%|██        | 56/266 [00:00<00:03, 60.18it/s]concatenating: train:  24%|██▍       | 64/266 [00:01<00:03, 63.65it/s]concatenating: train:  27%|██▋       | 71/266 [00:01<00:03, 64.90it/s]concatenating: train:  29%|██▉       | 78/266 [00:01<00:02, 63.43it/s]concatenating: train:  32%|███▏      | 85/266 [00:01<00:02, 62.40it/s]concatenating: train:  35%|███▍      | 92/266 [00:01<00:02, 63.39it/s]concatenating: train:  37%|███▋      | 99/266 [00:01<00:02, 62.61it/s]concatenating: train:  40%|███▉      | 106/266 [00:01<00:02, 64.36it/s]concatenating: train:  43%|████▎     | 114/266 [00:01<00:02, 67.70it/s]concatenating: train:  45%|████▌     | 121/266 [00:01<00:02, 66.95it/s]concatenating: train:  48%|████▊     | 128/266 [00:02<00:02, 63.88it/s]concatenating: train:  51%|█████     | 135/266 [00:02<00:02, 59.54it/s]concatenating: train:  53%|█████▎    | 142/266 [00:02<00:02, 60.11it/s]concatenating: train:  56%|█████▌    | 149/266 [00:02<00:01, 59.27it/s]concatenating: train:  58%|█████▊    | 155/266 [00:02<00:01, 56.62it/s]concatenating: train:  61%|██████▏   | 163/266 [00:02<00:01, 61.60it/s]concatenating: train:  64%|██████▍   | 170/266 [00:02<00:01, 63.40it/s]concatenating: train:  67%|██████▋   | 177/266 [00:02<00:01, 61.96it/s]concatenating: train:  69%|██████▉   | 184/266 [00:02<00:01, 59.13it/s]concatenating: train:  72%|███████▏  | 191/266 [00:03<00:01, 57.49it/s]concatenating: train:  74%|███████▍  | 197/266 [00:03<00:01, 56.53it/s]concatenating: train:  77%|███████▋  | 204/266 [00:03<00:01, 57.97it/s]concatenating: train:  79%|███████▉  | 211/266 [00:03<00:00, 58.84it/s]concatenating: train:  82%|████████▏ | 217/266 [00:03<00:00, 58.48it/s]concatenating: train:  85%|████████▍ | 225/266 [00:03<00:00, 61.22it/s]concatenating: train:  87%|████████▋ | 232/266 [00:03<00:00, 60.17it/s]concatenating: train:  90%|████████▉ | 239/266 [00:03<00:00, 59.42it/s]concatenating: train:  92%|█████████▏| 246/266 [00:04<00:00, 60.97it/s]concatenating: train:  95%|█████████▌| 253/266 [00:04<00:00, 62.65it/s]concatenating: train:  98%|█████████▊| 260/266 [00:04<00:00, 59.44it/s]concatenating: train: 100%|██████████| 266/266 [00:04<00:00, 60.53it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:00,  3.88it/s]Loading test:  50%|█████     | 2/4 [00:00<00:00,  3.91it/s]Loading test:  75%|███████▌  | 3/4 [00:00<00:00,  3.96it/s]Loading test: 100%|██████████| 4/4 [00:01<00:00,  3.96it/s]Loading test: 100%|██████████| 4/4 [00:01<00:00,  3.97it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 264.24it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:00<01:15,  3.49it/s]Loading trainS:   1%|          | 2/266 [00:00<01:14,  3.54it/s]Loading trainS:   1%|          | 3/266 [00:00<01:09,  3.78it/s]Loading trainS:   2%|▏         | 4/266 [00:01<01:06,  3.91it/s]Loading trainS:   2%|▏         | 5/266 [00:01<01:08,  3.79it/s]Loading trainS:   2%|▏         | 6/266 [00:01<01:11,  3.61it/s]Loading trainS:   3%|▎         | 7/266 [00:01<01:11,  3.63it/s]Loading trainS:   3%|▎         | 8/266 [00:02<01:10,  3.66it/s]Loading trainS:   3%|▎         | 9/266 [00:02<01:09,  3.68it/s]Loading trainS:   4%|▍         | 10/266 [00:02<01:09,  3.71it/s]Loading trainS:   4%|▍         | 11/266 [00:02<01:08,  3.74it/s]Loading trainS:   5%|▍         | 12/266 [00:03<01:07,  3.76it/s]Loading trainS:   5%|▍         | 13/266 [00:03<01:06,  3.80it/s]Loading trainS:   5%|▌         | 14/266 [00:03<01:05,  3.83it/s]Loading trainS:   6%|▌         | 15/266 [00:03<01:04,  3.87it/s]Loading trainS:   6%|▌         | 16/266 [00:04<01:04,  3.89it/s]Loading trainS:   6%|▋         | 17/266 [00:04<01:03,  3.92it/s]Loading trainS:   7%|▋         | 18/266 [00:04<01:03,  3.92it/s]Loading trainS:   7%|▋         | 19/266 [00:04<01:02,  3.94it/s]Loading trainS:   8%|▊         | 20/266 [00:05<01:03,  3.90it/s]Loading trainS:   8%|▊         | 21/266 [00:05<01:02,  3.92it/s]Loading trainS:   8%|▊         | 22/266 [00:05<01:02,  3.93it/s]Loading trainS:   9%|▊         | 23/266 [00:06<01:01,  3.92it/s]Loading trainS:   9%|▉         | 24/266 [00:06<01:00,  3.98it/s]Loading trainS:   9%|▉         | 25/266 [00:06<00:59,  4.03it/s]Loading trainS:  10%|▉         | 26/266 [00:06<00:59,  4.03it/s]Loading trainS:  10%|█         | 27/266 [00:06<00:58,  4.05it/s]Loading trainS:  11%|█         | 28/266 [00:07<00:58,  4.08it/s]Loading trainS:  11%|█         | 29/266 [00:07<00:58,  4.06it/s]Loading trainS:  11%|█▏        | 30/266 [00:07<00:57,  4.10it/s]Loading trainS:  12%|█▏        | 31/266 [00:07<00:57,  4.09it/s]Loading trainS:  12%|█▏        | 32/266 [00:08<00:57,  4.06it/s]Loading trainS:  12%|█▏        | 33/266 [00:08<00:58,  4.01it/s]Loading trainS:  13%|█▎        | 34/266 [00:08<00:59,  3.92it/s]Loading trainS:  13%|█▎        | 35/266 [00:08<00:58,  3.96it/s]Loading trainS:  14%|█▎        | 36/266 [00:09<00:57,  3.97it/s]Loading trainS:  14%|█▍        | 37/266 [00:09<00:57,  3.99it/s]Loading trainS:  14%|█▍        | 38/266 [00:09<00:57,  3.99it/s]Loading trainS:  15%|█▍        | 39/266 [00:09<00:58,  3.90it/s]Loading trainS:  15%|█▌        | 40/266 [00:10<00:57,  3.90it/s]Loading trainS:  15%|█▌        | 41/266 [00:10<00:56,  3.99it/s]Loading trainS:  16%|█▌        | 42/266 [00:10<00:52,  4.26it/s]Loading trainS:  16%|█▌        | 43/266 [00:10<00:49,  4.46it/s]Loading trainS:  17%|█▋        | 44/266 [00:11<00:48,  4.61it/s]Loading trainS:  17%|█▋        | 45/266 [00:11<00:46,  4.71it/s]Loading trainS:  17%|█▋        | 46/266 [00:11<00:45,  4.80it/s]Loading trainS:  18%|█▊        | 47/266 [00:11<00:44,  4.87it/s]Loading trainS:  18%|█▊        | 48/266 [00:11<00:44,  4.86it/s]Loading trainS:  18%|█▊        | 49/266 [00:12<00:44,  4.90it/s]Loading trainS:  19%|█▉        | 50/266 [00:12<00:44,  4.89it/s]Loading trainS:  19%|█▉        | 51/266 [00:12<00:43,  4.89it/s]Loading trainS:  20%|█▉        | 52/266 [00:12<00:43,  4.89it/s]Loading trainS:  20%|█▉        | 53/266 [00:12<00:43,  4.88it/s]Loading trainS:  20%|██        | 54/266 [00:13<00:43,  4.88it/s]Loading trainS:  21%|██        | 55/266 [00:13<00:43,  4.88it/s]Loading trainS:  21%|██        | 56/266 [00:13<00:43,  4.88it/s]Loading trainS:  21%|██▏       | 57/266 [00:13<00:42,  4.88it/s]Loading trainS:  22%|██▏       | 58/266 [00:13<00:42,  4.88it/s]Loading trainS:  22%|██▏       | 59/266 [00:14<00:42,  4.83it/s]Loading trainS:  23%|██▎       | 60/266 [00:14<00:43,  4.77it/s]Loading trainS:  23%|██▎       | 61/266 [00:14<00:43,  4.71it/s]Loading trainS:  23%|██▎       | 62/266 [00:14<00:43,  4.67it/s]Loading trainS:  24%|██▎       | 63/266 [00:15<00:43,  4.65it/s]Loading trainS:  24%|██▍       | 64/266 [00:15<00:44,  4.59it/s]Loading trainS:  24%|██▍       | 65/266 [00:15<00:43,  4.57it/s]Loading trainS:  25%|██▍       | 66/266 [00:15<00:44,  4.53it/s]Loading trainS:  25%|██▌       | 67/266 [00:15<00:45,  4.38it/s]Loading trainS:  26%|██▌       | 68/266 [00:16<00:44,  4.45it/s]Loading trainS:  26%|██▌       | 69/266 [00:16<00:44,  4.47it/s]Loading trainS:  26%|██▋       | 70/266 [00:16<00:43,  4.51it/s]Loading trainS:  27%|██▋       | 71/266 [00:16<00:42,  4.55it/s]Loading trainS:  27%|██▋       | 72/266 [00:17<00:42,  4.58it/s]Loading trainS:  27%|██▋       | 73/266 [00:17<00:41,  4.61it/s]Loading trainS:  28%|██▊       | 74/266 [00:17<00:41,  4.63it/s]Loading trainS:  28%|██▊       | 75/266 [00:17<00:41,  4.65it/s]Loading trainS:  29%|██▊       | 76/266 [00:17<00:40,  4.64it/s]Loading trainS:  29%|██▉       | 77/266 [00:18<00:40,  4.65it/s]Loading trainS:  29%|██▉       | 78/266 [00:18<00:42,  4.47it/s]Loading trainS:  30%|██▉       | 79/266 [00:18<00:42,  4.37it/s]Loading trainS:  30%|███       | 80/266 [00:18<00:43,  4.30it/s]Loading trainS:  30%|███       | 81/266 [00:19<00:43,  4.25it/s]Loading trainS:  31%|███       | 82/266 [00:19<00:43,  4.21it/s]Loading trainS:  31%|███       | 83/266 [00:19<00:43,  4.19it/s]Loading trainS:  32%|███▏      | 84/266 [00:19<00:43,  4.17it/s]Loading trainS:  32%|███▏      | 85/266 [00:20<00:43,  4.14it/s]Loading trainS:  32%|███▏      | 86/266 [00:20<00:43,  4.11it/s]Loading trainS:  33%|███▎      | 87/266 [00:20<00:43,  4.12it/s]Loading trainS:  33%|███▎      | 88/266 [00:20<00:43,  4.12it/s]Loading trainS:  33%|███▎      | 89/266 [00:21<00:42,  4.13it/s]Loading trainS:  34%|███▍      | 90/266 [00:21<00:42,  4.12it/s]Loading trainS:  34%|███▍      | 91/266 [00:21<00:42,  4.12it/s]Loading trainS:  35%|███▍      | 92/266 [00:21<00:42,  4.09it/s]Loading trainS:  35%|███▍      | 93/266 [00:21<00:42,  4.10it/s]Loading trainS:  35%|███▌      | 94/266 [00:22<00:42,  4.10it/s]Loading trainS:  36%|███▌      | 95/266 [00:22<00:41,  4.10it/s]Loading trainS:  36%|███▌      | 96/266 [00:22<00:40,  4.15it/s]Loading trainS:  36%|███▋      | 97/266 [00:23<00:44,  3.78it/s]Loading trainS:  37%|███▋      | 98/266 [00:23<00:43,  3.89it/s]Loading trainS:  37%|███▋      | 99/266 [00:23<00:39,  4.19it/s]Loading trainS:  38%|███▊      | 100/266 [00:23<00:39,  4.25it/s]Loading trainS:  38%|███▊      | 101/266 [00:23<00:37,  4.38it/s]Loading trainS:  38%|███▊      | 102/266 [00:24<00:37,  4.41it/s]Loading trainS:  39%|███▊      | 103/266 [00:24<00:36,  4.49it/s]Loading trainS:  39%|███▉      | 104/266 [00:24<00:35,  4.55it/s]Loading trainS:  39%|███▉      | 105/266 [00:24<00:35,  4.60it/s]Loading trainS:  40%|███▉      | 106/266 [00:24<00:34,  4.62it/s]Loading trainS:  40%|████      | 107/266 [00:25<00:34,  4.65it/s]Loading trainS:  41%|████      | 108/266 [00:25<00:33,  4.66it/s]Loading trainS:  41%|████      | 109/266 [00:25<00:33,  4.68it/s]Loading trainS:  41%|████▏     | 110/266 [00:25<00:33,  4.67it/s]Loading trainS:  42%|████▏     | 111/266 [00:26<00:33,  4.68it/s]Loading trainS:  42%|████▏     | 112/266 [00:26<00:32,  4.70it/s]Loading trainS:  42%|████▏     | 113/266 [00:26<00:32,  4.70it/s]Loading trainS:  43%|████▎     | 114/266 [00:26<00:32,  4.70it/s]Loading trainS:  43%|████▎     | 115/266 [00:26<00:32,  4.70it/s]Loading trainS:  44%|████▎     | 116/266 [00:27<00:31,  4.72it/s]Loading trainS:  44%|████▍     | 117/266 [00:27<00:31,  4.71it/s]Loading trainS:  44%|████▍     | 118/266 [00:27<00:31,  4.73it/s]Loading trainS:  45%|████▍     | 119/266 [00:27<00:32,  4.47it/s]Loading trainS:  45%|████▌     | 120/266 [00:28<00:33,  4.32it/s]Loading trainS:  45%|████▌     | 121/266 [00:28<00:34,  4.20it/s]Loading trainS:  46%|████▌     | 122/266 [00:28<00:34,  4.13it/s]Loading trainS:  46%|████▌     | 123/266 [00:28<00:35,  4.08it/s]Loading trainS:  47%|████▋     | 124/266 [00:29<00:35,  4.05it/s]Loading trainS:  47%|████▋     | 125/266 [00:29<00:34,  4.04it/s]Loading trainS:  47%|████▋     | 126/266 [00:29<00:34,  4.02it/s]Loading trainS:  48%|████▊     | 127/266 [00:29<00:34,  4.00it/s]Loading trainS:  48%|████▊     | 128/266 [00:30<00:34,  4.00it/s]Loading trainS:  48%|████▊     | 129/266 [00:30<00:34,  3.98it/s]Loading trainS:  49%|████▉     | 130/266 [00:30<00:34,  3.99it/s]Loading trainS:  49%|████▉     | 131/266 [00:30<00:33,  3.99it/s]Loading trainS:  50%|████▉     | 132/266 [00:31<00:33,  3.99it/s]Loading trainS:  50%|█████     | 133/266 [00:31<00:33,  3.98it/s]Loading trainS:  50%|█████     | 134/266 [00:31<00:33,  3.97it/s]Loading trainS:  51%|█████     | 135/266 [00:31<00:33,  3.89it/s]Loading trainS:  51%|█████     | 136/266 [00:32<00:33,  3.84it/s]Loading trainS:  52%|█████▏    | 137/266 [00:32<00:32,  3.97it/s]Loading trainS:  52%|█████▏    | 138/266 [00:32<00:31,  4.08it/s]Loading trainS:  52%|█████▏    | 139/266 [00:32<00:30,  4.17it/s]Loading trainS:  53%|█████▎    | 140/266 [00:32<00:29,  4.23it/s]Loading trainS:  53%|█████▎    | 141/266 [00:33<00:29,  4.26it/s]Loading trainS:  53%|█████▎    | 142/266 [00:33<00:29,  4.25it/s]Loading trainS:  54%|█████▍    | 143/266 [00:33<00:29,  4.22it/s]Loading trainS:  54%|█████▍    | 144/266 [00:33<00:29,  4.19it/s]Loading trainS:  55%|█████▍    | 145/266 [00:34<00:28,  4.18it/s]Loading trainS:  55%|█████▍    | 146/266 [00:34<00:28,  4.22it/s]Loading trainS:  55%|█████▌    | 147/266 [00:34<00:28,  4.24it/s]Loading trainS:  56%|█████▌    | 148/266 [00:34<00:27,  4.22it/s]Loading trainS:  56%|█████▌    | 149/266 [00:35<00:28,  4.16it/s]Loading trainS:  56%|█████▋    | 150/266 [00:35<00:27,  4.17it/s]Loading trainS:  57%|█████▋    | 151/266 [00:35<00:27,  4.15it/s]Loading trainS:  57%|█████▋    | 152/266 [00:35<00:27,  4.13it/s]Loading trainS:  58%|█████▊    | 153/266 [00:36<00:26,  4.20it/s]Loading trainS:  58%|█████▊    | 154/266 [00:36<00:26,  4.26it/s]Loading trainS:  58%|█████▊    | 155/266 [00:36<00:24,  4.52it/s]Loading trainS:  59%|█████▊    | 156/266 [00:36<00:23,  4.70it/s]Loading trainS:  59%|█████▉    | 157/266 [00:36<00:22,  4.86it/s]Loading trainS:  59%|█████▉    | 158/266 [00:37<00:21,  4.96it/s]Loading trainS:  60%|█████▉    | 159/266 [00:37<00:21,  5.05it/s]Loading trainS:  60%|██████    | 160/266 [00:37<00:20,  5.11it/s]Loading trainS:  61%|██████    | 161/266 [00:37<00:20,  5.14it/s]Loading trainS:  61%|██████    | 162/266 [00:37<00:20,  5.16it/s]Loading trainS:  61%|██████▏   | 163/266 [00:38<00:19,  5.17it/s]Loading trainS:  62%|██████▏   | 164/266 [00:38<00:19,  5.17it/s]Loading trainS:  62%|██████▏   | 165/266 [00:38<00:19,  5.17it/s]Loading trainS:  62%|██████▏   | 166/266 [00:38<00:19,  5.20it/s]Loading trainS:  63%|██████▎   | 167/266 [00:38<00:18,  5.22it/s]Loading trainS:  63%|██████▎   | 168/266 [00:38<00:18,  5.23it/s]Loading trainS:  64%|██████▎   | 169/266 [00:39<00:18,  5.25it/s]Loading trainS:  64%|██████▍   | 170/266 [00:39<00:18,  5.24it/s]Loading trainS:  64%|██████▍   | 171/266 [00:39<00:18,  5.24it/s]Loading trainS:  65%|██████▍   | 172/266 [00:39<00:17,  5.24it/s]Loading trainS:  65%|██████▌   | 173/266 [00:39<00:18,  5.08it/s]Loading trainS:  65%|██████▌   | 174/266 [00:40<00:18,  5.00it/s]Loading trainS:  66%|██████▌   | 175/266 [00:40<00:18,  4.93it/s]Loading trainS:  66%|██████▌   | 176/266 [00:40<00:18,  4.88it/s]Loading trainS:  67%|██████▋   | 177/266 [00:40<00:18,  4.84it/s]Loading trainS:  67%|██████▋   | 178/266 [00:41<00:18,  4.83it/s]Loading trainS:  67%|██████▋   | 179/266 [00:41<00:18,  4.79it/s]Loading trainS:  68%|██████▊   | 180/266 [00:41<00:17,  4.79it/s]Loading trainS:  68%|██████▊   | 181/266 [00:41<00:17,  4.78it/s]Loading trainS:  68%|██████▊   | 182/266 [00:41<00:17,  4.78it/s]Loading trainS:  69%|██████▉   | 183/266 [00:42<00:17,  4.76it/s]Loading trainS:  69%|██████▉   | 184/266 [00:42<00:17,  4.78it/s]Loading trainS:  70%|██████▉   | 185/266 [00:42<00:16,  4.79it/s]Loading trainS:  70%|██████▉   | 186/266 [00:42<00:16,  4.79it/s]Loading trainS:  70%|███████   | 187/266 [00:42<00:16,  4.77it/s]Loading trainS:  71%|███████   | 188/266 [00:43<00:16,  4.73it/s]Loading trainS:  71%|███████   | 189/266 [00:43<00:16,  4.71it/s]Loading trainS:  71%|███████▏  | 190/266 [00:43<00:16,  4.74it/s]Loading trainS:  72%|███████▏  | 191/266 [00:43<00:16,  4.60it/s]Loading trainS:  72%|███████▏  | 192/266 [00:43<00:16,  4.58it/s]Loading trainS:  73%|███████▎  | 193/266 [00:44<00:16,  4.50it/s]Loading trainS:  73%|███████▎  | 194/266 [00:44<00:16,  4.29it/s]Loading trainS:  73%|███████▎  | 195/266 [00:44<00:16,  4.40it/s]Loading trainS:  74%|███████▎  | 196/266 [00:44<00:15,  4.44it/s]Loading trainS:  74%|███████▍  | 197/266 [00:45<00:15,  4.46it/s]Loading trainS:  74%|███████▍  | 198/266 [00:45<00:15,  4.47it/s]Loading trainS:  75%|███████▍  | 199/266 [00:45<00:15,  4.43it/s]Loading trainS:  75%|███████▌  | 200/266 [00:45<00:15,  4.34it/s]Loading trainS:  76%|███████▌  | 201/266 [00:46<00:14,  4.37it/s]Loading trainS:  76%|███████▌  | 202/266 [00:46<00:14,  4.44it/s]Loading trainS:  76%|███████▋  | 203/266 [00:46<00:14,  4.49it/s]Loading trainS:  77%|███████▋  | 204/266 [00:46<00:13,  4.49it/s]Loading trainS:  77%|███████▋  | 205/266 [00:46<00:13,  4.48it/s]Loading trainS:  77%|███████▋  | 206/266 [00:47<00:13,  4.51it/s]Loading trainS:  78%|███████▊  | 207/266 [00:47<00:13,  4.54it/s]Loading trainS:  78%|███████▊  | 208/266 [00:47<00:12,  4.53it/s]Loading trainS:  79%|███████▊  | 209/266 [00:47<00:12,  4.48it/s]Loading trainS:  79%|███████▉  | 210/266 [00:48<00:12,  4.51it/s]Loading trainS:  79%|███████▉  | 211/266 [00:48<00:12,  4.53it/s]Loading trainS:  80%|███████▉  | 212/266 [00:48<00:11,  4.56it/s]Loading trainS:  80%|████████  | 213/266 [00:48<00:11,  4.63it/s]Loading trainS:  80%|████████  | 214/266 [00:48<00:11,  4.66it/s]Loading trainS:  81%|████████  | 215/266 [00:49<00:10,  4.70it/s]Loading trainS:  81%|████████  | 216/266 [00:49<00:10,  4.73it/s]Loading trainS:  82%|████████▏ | 217/266 [00:49<00:10,  4.74it/s]Loading trainS:  82%|████████▏ | 218/266 [00:49<00:10,  4.76it/s]Loading trainS:  82%|████████▏ | 219/266 [00:49<00:09,  4.76it/s]Loading trainS:  83%|████████▎ | 220/266 [00:50<00:09,  4.73it/s]Loading trainS:  83%|████████▎ | 221/266 [00:50<00:09,  4.69it/s]Loading trainS:  83%|████████▎ | 222/266 [00:50<00:09,  4.71it/s]Loading trainS:  84%|████████▍ | 223/266 [00:50<00:09,  4.72it/s]Loading trainS:  84%|████████▍ | 224/266 [00:51<00:08,  4.74it/s]Loading trainS:  85%|████████▍ | 225/266 [00:51<00:08,  4.76it/s]Loading trainS:  85%|████████▍ | 226/266 [00:51<00:08,  4.78it/s]Loading trainS:  85%|████████▌ | 227/266 [00:51<00:08,  4.79it/s]Loading trainS:  86%|████████▌ | 228/266 [00:51<00:07,  4.77it/s]Loading trainS:  86%|████████▌ | 229/266 [00:52<00:07,  4.75it/s]Loading trainS:  86%|████████▋ | 230/266 [00:52<00:07,  4.73it/s]Loading trainS:  87%|████████▋ | 231/266 [00:52<00:07,  4.62it/s]Loading trainS:  87%|████████▋ | 232/266 [00:52<00:07,  4.51it/s]Loading trainS:  88%|████████▊ | 233/266 [00:52<00:07,  4.43it/s]Loading trainS:  88%|████████▊ | 234/266 [00:53<00:07,  4.22it/s]Loading trainS:  88%|████████▊ | 235/266 [00:53<00:07,  4.24it/s]Loading trainS:  89%|████████▊ | 236/266 [00:53<00:07,  4.25it/s]Loading trainS:  89%|████████▉ | 237/266 [00:53<00:06,  4.27it/s]Loading trainS:  89%|████████▉ | 238/266 [00:54<00:06,  4.35it/s]Loading trainS:  90%|████████▉ | 239/266 [00:54<00:06,  4.39it/s]Loading trainS:  90%|█████████ | 240/266 [00:54<00:05,  4.45it/s]Loading trainS:  91%|█████████ | 241/266 [00:54<00:05,  4.50it/s]Loading trainS:  91%|█████████ | 242/266 [00:55<00:05,  4.53it/s]Loading trainS:  91%|█████████▏| 243/266 [00:55<00:05,  4.55it/s]Loading trainS:  92%|█████████▏| 244/266 [00:55<00:04,  4.55it/s]Loading trainS:  92%|█████████▏| 245/266 [00:55<00:04,  4.57it/s]Loading trainS:  92%|█████████▏| 246/266 [00:55<00:04,  4.56it/s]Loading trainS:  93%|█████████▎| 247/266 [00:56<00:04,  4.57it/s]Loading trainS:  93%|█████████▎| 248/266 [00:56<00:03,  4.56it/s]Loading trainS:  94%|█████████▎| 249/266 [00:56<00:03,  4.43it/s]Loading trainS:  94%|█████████▍| 250/266 [00:56<00:03,  4.36it/s]Loading trainS:  94%|█████████▍| 251/266 [00:57<00:03,  4.31it/s]Loading trainS:  95%|█████████▍| 252/266 [00:57<00:03,  4.28it/s]Loading trainS:  95%|█████████▌| 253/266 [00:57<00:03,  4.26it/s]Loading trainS:  95%|█████████▌| 254/266 [00:57<00:02,  4.23it/s]Loading trainS:  96%|█████████▌| 255/266 [00:58<00:02,  4.20it/s]Loading trainS:  96%|█████████▌| 256/266 [00:58<00:02,  4.20it/s]Loading trainS:  97%|█████████▋| 257/266 [00:58<00:02,  4.19it/s]Loading trainS:  97%|█████████▋| 258/266 [00:58<00:01,  4.18it/s]Loading trainS:  97%|█████████▋| 259/266 [00:58<00:01,  4.15it/s]Loading trainS:  98%|█████████▊| 260/266 [00:59<00:01,  4.17it/s]Loading trainS:  98%|█████████▊| 261/266 [00:59<00:01,  4.18it/s]Loading trainS:  98%|█████████▊| 262/266 [00:59<00:00,  4.19it/s]Loading trainS:  99%|█████████▉| 263/266 [00:59<00:00,  4.16it/s]Loading trainS:  99%|█████████▉| 264/266 [01:00<00:00,  4.18it/s]Loading trainS: 100%|█████████▉| 265/266 [01:00<00:00,  4.19it/s]Loading trainS: 100%|██████████| 266/266 [01:00<00:00,  4.19it/s]Loading trainS: 100%|██████████| 266/266 [01:00<00:00,  4.39it/s]
Loading testS:   0%|          | 0/4 [00:00<?, ?it/s]Loading testS:  25%|██▌       | 1/4 [00:00<00:00,  4.03it/s]Loading testS:  50%|█████     | 2/4 [00:00<00:00,  4.11it/s]Loading testS:  75%|███████▌  | 3/4 [00:00<00:00,  4.16it/s]Loading testS: 100%|██████████| 4/4 [00:00<00:00,  4.15it/s]Loading testS: 100%|██████████| 4/4 [00:00<00:00,  4.18it/s]
Epoch 00046: val_mDice did not improve from 0.23747
Restoring model weights from the end of the best epoch
Epoch 00046: early stopping
{'val_loss': [0.2252694945935099, -0.16671393771935444, 0.0037493650938888066, -0.16704389487668017, -0.06848104708812765, -0.10004285982519877, -0.14838875265272913, -0.14540146656342035, -0.23670278413251775, -0.1972137385460197, -0.1813029188387951, -0.21085034105468725, -0.15761431891349006, -0.20609560063047733, -0.16277709989579486, 0.006797989929614008, -0.2529149784050027, -0.24742176686031897, -0.2559454884451311, -0.2725745098451611, -0.2725945078294541, -0.25968926719038704, -0.27192322517455114, -0.2702620492591599, -0.23996732692371625, -0.26761811717840855, -0.25999279082065724, -0.26918189719769736, -0.27375486899419216, -0.27762333187656324, -0.2801805210269255, -0.25841735182978887, -0.20464783491753805, -0.2659510984505359, -0.2431038129105248, -0.2527566506054069, -0.2847339146976649, -0.28343717182879563, -0.2932403248230942, -0.2858442141670642, -0.27417365218809475, -0.27216882090328537, -0.28516517412375475, -0.28367519684777154, -0.2698228570816802, -0.27876185202497805], 'val_acc': [0.937424465769627, 0.9501990099422267, 0.9495816216117046, 0.9466556344853073, 0.9437378478831933, 0.9454175834773016, 0.9459758562142732, 0.948515983878589, 0.9497277565666886, 0.9485209124987243, 0.9489461843107567, 0.9483994030561603, 0.9470332904917295, 0.9499050916218367, 0.9474503446797855, 0.937486861572891, 0.9498673276823075, 0.9514255601851667, 0.9504075411890374, 0.9508984895026098, 0.9529410957312975, 0.9503517111793893, 0.9509674545194282, 0.9499822669341916, 0.9487901994439422, 0.9505947256674532, 0.9514698904068744, 0.9493583186728055, 0.9502449812459164, 0.9512121052038475, 0.9522350498887359, 0.9499740605471564, 0.9447985648131761, 0.9487097336620581, 0.9486883956877912, 0.9501957204498228, 0.9510708959376226, 0.9512367336476435, 0.952261328208642, 0.9516439447637464, 0.950816386547245, 0.9515782638651425, 0.9513188336716324, 0.9515864780691804, 0.951202250894953, 0.9507392146548287], 'val_mDice': [0.19675931613892317, 0.23460694011606154, 0.22832902401807856, 0.22916445026143653, 0.2216171612352377, 0.23746981593917627, 0.22571073115238402, 0.22539353163027373, 0.23196928233641093, 0.23679680871914643, 0.23050876810658175, 0.23729618034157596, 0.22413218412242952, 0.23026143835826976, 0.22730685862117125, 0.2013078384467813, 0.22921633179925505, 0.22747145290868204, 0.2245048901646352, 0.23389565785888766, 0.21740340462839994, 0.21347940374226843, 0.2116017719944481, 0.2210180047960555, 0.19053088055282344, 0.2075322518949626, 0.21117825662625617, 0.20750551237190357, 0.21246448895115344, 0.20974412557409436, 0.21255155622226293, 0.20924865222368083, 0.18809106729191835, 0.20417491371025803, 0.19925404636219876, 0.20415751574957958, 0.21065153416673668, 0.20926201206128128, 0.2133377100966993, 0.21184097923582695, 0.20601508879392852, 0.21067183626601932, 0.21110279561921222, 0.20946378285279038, 0.2093882039800042, 0.21102486651574], 'loss': [0.46097928345361594, 0.37431362292401665, 0.3596595957967533, 0.34496357503523695, 0.3359726806264893, 0.33184751488578174, 0.32443587757499825, 0.3179946064386562, 0.31446523107950825, 0.3119648620024855, 0.30606864736833184, 0.30574621730798535, 0.30414016963598195, 0.30112046725768954, 0.2970023390308871, 0.2988018349045956, 0.2977880608522235, 0.301414968455235, 0.29858311515524477, 0.2767494073315062, 0.31227958364073277, 0.28423711439977484, 0.2724585448207998, 0.2636448475739591, 0.2664785816929345, 0.2598782745470103, 0.2544067829591208, 0.2610609207805757, 0.2573979737438136, 0.2567619265024738, 0.2611429307485382, 0.25511757750778213, 0.25659013452989604, 0.252075842164297, 0.26203361990351026, 0.25495053024098646, 0.24341257972606664, 0.243624668547637, 0.23858493538171371, 0.23574601050909302, 0.23326982783789593, 0.23350411425027431, 0.22857887204115085, 0.2367951540262058, 0.2389687628534561, 0.2341865446526734], 'acc': [0.9316338642407785, 0.9423774942048213, 0.944653291172842, 0.9461422918188084, 0.9474350709157228, 0.9482225759547568, 0.9488174059826235, 0.9499252357513847, 0.9500271975906224, 0.9506056521783287, 0.9512380519752761, 0.9513268534745554, 0.9514529647042498, 0.9518091824862571, 0.9522724176746876, 0.9522428105307061, 0.952255995707398, 0.9500782382027725, 0.9479478343648919, 0.9490990007707624, 0.9455936810886638, 0.9480722911833873, 0.9488360262732701, 0.9493387001202683, 0.9491453617943033, 0.9495059974388329, 0.949793216912172, 0.9491895013803298, 0.9496274066512755, 0.9501452988068005, 0.9493120225331524, 0.9500961255724892, 0.9497157114158862, 0.9497418695936087, 0.9491037737300212, 0.9495773366228102, 0.9505993420660794, 0.9509517088009523, 0.9511449285328335, 0.9511742529106928, 0.9514735692618204, 0.9513354313932524, 0.9516744459318713, 0.951526004650782, 0.9511536477730711, 0.9514532474879492], 'mDice': [0.5030467167971269, 0.5966083100150729, 0.6124203931578701, 0.6282881513980595, 0.6379722318829284, 0.6424045158290329, 0.6504279005601321, 0.6573604242670188, 0.6611844607771972, 0.6638757622688437, 0.6702396564429505, 0.6705941607276447, 0.6723154860413275, 0.6755642372926967, 0.6799888543079329, 0.6780358578068287, 0.6791383771550441, 0.6631195522697582, 0.6370298920813932, 0.6501832729031364, 0.5803205825803696, 0.6007247389030822, 0.6133614849706767, 0.6213371921907446, 0.6082772574956281, 0.6126256063778258, 0.6189937353907556, 0.6123479195169839, 0.614631158189062, 0.6193246653353489, 0.6132684603302713, 0.6160365587242653, 0.6185826618950545, 0.6176203396671186, 0.6112964830367623, 0.6153253765979442, 0.6271939043167664, 0.6275300681239487, 0.6282877520240889, 0.6296505898333904, 0.6333354063633092, 0.6321735746809178, 0.6325105941249923, 0.6344399723075759, 0.6295642891453277, 0.6344508512889836], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 60) 0           concatenate_5[0][0]              2020-01-21 02:14:18.072598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 02:14:18.072675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 02:14:18.072687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 02:14:18.072695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 02:14:18.072973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 256 samples
Epoch 1/300
 - 48s - loss: 0.1431 - acc: 0.9858 - mDice: 0.7211 - val_loss: 0.2880 - val_acc: 0.9905 - val_mDice: 0.4280

Epoch 00001: val_mDice improved from -inf to 0.42805, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 45s - loss: 0.0823 - acc: 0.9914 - mDice: 0.8398 - val_loss: 0.1754 - val_acc: 0.9918 - val_mDice: 0.4429

Epoch 00002: val_mDice improved from 0.42805 to 0.44292, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 45s - loss: 0.0735 - acc: 0.9922 - mDice: 0.8570 - val_loss: 0.2564 - val_acc: 0.9924 - val_mDice: 0.4571

Epoch 00003: val_mDice improved from 0.44292 to 0.45711, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300
 - 45s - loss: 0.0682 - acc: 0.9928 - mDice: 0.8673 - val_loss: 0.1832 - val_acc: 0.9924 - val_mDice: 0.4550

Epoch 00004: val_mDice did not improve from 0.45711
Epoch 5/300
 - 45s - loss: 0.0642 - acc: 0.9931 - mDice: 0.8750 - val_loss: 0.0629 - val_acc: 0.9932 - val_mDice: 0.4539

Epoch 00005: val_mDice did not improve from 0.45711
Epoch 6/300
 - 44s - loss: 0.0628 - acc: 0.9934 - mDice: 0.8779 - val_loss: 0.0537 - val_acc: 0.9931 - val_mDice: 0.4677

Epoch 00006: val_mDice improved from 0.45711 to 0.46766, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/300
 - 44s - loss: 0.0601 - acc: 0.9936 - mDice: 0.8831 - val_loss: 0.0912 - val_acc: 0.9922 - val_mDice: 0.4764

Epoch 00007: val_mDice improved from 0.46766 to 0.47639, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/300
 - 44s - loss: 0.0598 - acc: 0.9937 - mDice: 0.8837 - val_loss: -2.4883e-02 - val_acc: 0.9920 - val_mDice: 0.4633

Epoch 00008: val_mDice did not improve from 0.47639
Epoch 9/300
 - 44s - loss: 0.0557 - acc: 0.9940 - mDice: 0.8916 - val_loss: -1.6493e-02 - val_acc: 0.9936 - val_mDice: 0.4832

Epoch 00009: val_mDice improved from 0.47639 to 0.48322, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 10/300
 - 44s - loss: 0.0545 - acc: 0.9940 - mDice: 0.8941 - val_loss: 0.1165 - val_acc: 0.9932 - val_mDice: 0.4662

Epoch 00010: val_mDice did not improve from 0.48322
Epoch 11/300
 - 43s - loss: 0.0540 - acc: 0.9941 - mDice: 0.8951 - val_loss: 0.1304 - val_acc: 0.9906 - val_mDice: 0.3918

Epoch 00011: val_mDice did not improve from 0.48322
Epoch 12/300
 - 45s - loss: 0.0555 - acc: 0.9941 - mDice: 0.8921 - val_loss: 0.1361 - val_acc: 0.9934 - val_mDice: 0.4904

Epoch 00012: val_mDice improved from 0.48322 to 0.49037, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 13/300
 - 44s - loss: 0.0519 - acc: 0.9943 - mDice: 0.8991 - val_loss: 0.0934 - val_acc: 0.9935 - val_mDice: 0.4880

Epoch 00013: val_mDice did not improve from 0.49037
Epoch 14/300
 - 44s - loss: 0.0513 - acc: 0.9944 - mDice: 0.9003 - val_loss: 0.0134 - val_acc: 0.9931 - val_mDice: 0.4731

Epoch 00014: val_mDice did not improve from 0.49037
Epoch 15/300
 - 44s - loss: 0.0504 - acc: 0.9945 - mDice: 0.9021 - val_loss: 0.1273 - val_acc: 0.9934 - val_mDice: 0.4883

Epoch 00015: val_mDice did not improve from 0.49037
Epoch 16/300
 - 44s - loss: 0.0510 - acc: 0.9945 - mDice: 0.9009 - val_loss: 0.3034 - val_acc: 0.9805 - val_mDice: 0.4043

Epoch 00016: val_mDice did not improve from 0.49037
Epoch 17/300
 - 45s - loss: 0.0488 - acc: 0.9946 - mDice: 0.9052 - val_loss: 0.2156 - val_acc: 0.9919 - val_mDice: 0.4757

Epoch 00017: val_mDice did not improve from 0.49037
Epoch 18/300
 - 45s - loss: 0.0492 - acc: 0.9945 - mDice: 0.9044 - val_loss: 0.2307 - val_acc: 0.9912 - val_mDice: 0.4178

Epoch 00018: val_mDice did not improve from 0.49037
Epoch 19/300
 - 44s - loss: 0.0486 - acc: 0.9946 - mDice: 0.9055 - val_loss: 0.2391 - val_acc: 0.9928 - val_mDice: 0.4704

Epoch 00019: val_mDice did not improve from 0.49037
Epoch 20/300
 - 44s - loss: 0.0483 - acc: 0.9947 - mDice: 0.9061 - val_loss: 0.1322 - val_acc: 0.9938 - val_mDice: 0.4922

Epoch 00020: val_mDice improved from 0.49037 to 0.49223, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 21/300
 - 45s - loss: 0.0469 - acc: 0.9947 - mDice: 0.9088 - val_loss: 0.1156 - val_acc: 0.9935 - val_mDice: 0.4848

Epoch 00021: val_mDice did not improve from 0.49223
Epoch 22/300
 - 44s - loss: 0.0458 - acc: 0.9948 - mDice: 0.9111 - val_loss: 0.1729 - val_acc: 0.9932 - val_mDice: 0.4838

Epoch 00022: val_mDice did not improve from 0.49223
Epoch 23/300
 - 45s - loss: 0.0464 - acc: 0.9948 - mDice: 0.9098 - val_loss: 0.1360 - val_acc: 0.9939 - val_mDice: 0.4940

Epoch 00023: val_mDice improved from 0.49223 to 0.49396, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 24/300
 - 44s - loss: 0.0455 - acc: 0.9949 - mDice: 0.9117 - val_loss: 0.1799 - val_acc: 0.9935 - val_mDice: 0.4915

Epoch 00024: val_mDice did not improve from 0.49396
Epoch 25/300
 - 44s - loss: 0.0454 - acc: 0.9949 - mDice: 0.9118 - val_loss: 0.1667 - val_acc: 0.9936 - val_mDice: 0.4920

Epoch 00025: val_mDice did not improve from 0.49396
Epoch 26/300
 - 44s - loss: 0.0451 - acc: 0.9950 - mDice: 0.9124 - val_loss: 0.1061 - val_acc: 0.9937 - val_mDice: 0.4815

Epoch 00026: val_mDice did not improve from 0.49396
Epoch 27/300
 - 45s - loss: 0.0445 - acc: 0.9950 - mDice: 0.9136 - val_loss: 0.0302 - val_acc: 0.9935 - val_mDice: 0.4774

Epoch 00027: val_mDice did not improve from 0.49396
Epoch 28/300
 - 45s - loss: 0.0426 - acc: 0.9951 - mDice: 0.9173 - val_loss: 0.0447 - val_acc: 0.9936 - val_mDice: 0.4653

Epoch 00028: val_mDice did not improve from 0.49396
Epoch 29/300
 - 44s - loss: 0.0446 - acc: 0.9950 - mDice: 0.9133 - val_loss: 0.0420 - val_acc: 0.9938 - val_mDice: 0.5005

Epoch 00029: val_mDice improved from 0.49396 to 0.50054, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 30/300
 - 45s - loss: 0.0440 - acc: 0.9951 - mDice: 0.9144 - val_loss: 0.1150 - val_acc: 0.9938 - val_mDice: 0.4775

Epoch 00030: val_mDice did not improve from 0.50054
Epoch 31/300
 - 44s - loss: 0.0437 - acc: 0.9950 - mDice: 0.9152 - val_loss: 0.1724 - val_acc: 0.9915 - val_mDice: 0.4762

Epoch 00031: val_mDice did not improve from 0.50054
Epoch 32/300
 - 45s - loss: 0.0443 - acc: 0.9951 - mDice: 0.9139 - val_loss: 0.0571 - val_acc: 0.9928 - val_mDice: 0.4572

Epoch 00032: val_mDice did not improve from 0.50054
Epoch 33/300
 - 45s - loss: 0.0428 - acc: 0.9952 - mDice: 0.9168 - val_loss: 0.1231 - val_acc: 0.9936 - val_mDice: 0.4747

Epoch 00033: val_mDice did not improve from 0.50054
Epoch 34/300
 - 46s - loss: 0.0434 - acc: 0.9951 - mDice: 0.9157 - val_loss: 0.1585 - val_acc: 0.9928 - val_mDice: 0.4814

Epoch 00034: val_mDice did not improve from 0.50054
Epoch 35/300
 - 46s - loss: 0.0427 - acc: 0.9952 - mDice: 0.9171 - val_loss: 0.1299 - val_acc: 0.9932 - val_mDice: 0.4745

Epoch 00035: val_mDice did not improve from 0.50054
Epoch 36/300
 - 45s - loss: 0.0433 - acc: 0.9951 - mDice: 0.9159 - val_loss: 0.1586 - val_acc: 0.9929 - val_mDice: 0.4766

Epoch 00036: val_mDice did not improve from 0.50054
Epoch 37/300
 - 45s - loss: 0.0426 - acc: 0.9952 - mDice: 0.9171 - val_loss: 0.1621 - val_acc: 0.9928 - val_mDice: 0.4897

Epoch 00037: val_mDice did not improve from 0.50054
Epoch 38/300
 - 46s - loss: 0.0435 - acc: 0.9951 - mDice: 0.9154 - val_loss: 0.1542 - val_acc: 0.9919 - val_mDice: 0.4287

Epoch 00038: val_mDice did not improve from 0.50054
Epoch 39/300
 - 47s - loss: 0.0411 - acc: 0.9953 - mDice: 0.9202 - val_loss: 0.1449 - val_acc: 0.9917 - val_mDice: 0.4826

Epoch 00039: val_mDice did not improve from 0.50054
Epoch 40/300
 - 47s - loss: 0.0418 - acc: 0.9953 - mDice: 0.9188 - val_loss: 0.1560 - val_acc: 0.9937 - val_mDice: 0.4842

Epoch 00040: val_mDice did not improve from 0.50054
Epoch 41/300
 - 47s - loss: 0.0416 - acc: 0.9953 - mDice: 0.9192 - val_loss: 0.2645 - val_acc: 0.9820 - val_mDice: 0.4090

Epoch 00041: val_mDice did not improve from 0.50054
Epoch 42/300
 - 47s - loss: 0.0401 - acc: 0.9953 - mDice: 0.9221 - val_loss: 0.0635 - val_acc: 0.9935 - val_mDice: 0.4907

Epoch 00042: val_mDice did not improve from 0.50054
Epoch 43/300
 - 47s - loss: 0.0423 - acc: 0.9952 - mDice: 0.9178 - val_loss: 0.1441 - val_acc: 0.9936 - val_mDice: 0.4914

Epoch 00043: val_mDice did not improve from 0.50054
Epoch 44/300
 - 46s - loss: 0.0404 - acc: 0.9953 - mDice: 0.9216 - val_loss: 0.2092 - val_acc: 0.9923 - val_mDice: 0.5074

Epoch 00044: val_mDice improved from 0.50054 to 0.50741, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 45/300
 - 45s - loss: 0.0407 - acc: 0.9953 - mDice: 0.9209 - val_loss: 0.1881 - val_acc: 0.9936 - val_mDice: 0.4963

Epoch 00045: val_mDice did not improve from 0.50741
Epoch 46/300
 - 44s - loss: 0.0398 - acc: 0.9954 - mDice: 0.9227 - val_loss: 0.2013 - val_acc: 0.9907 - val_mDice: 0.4674

Epoch 00046: val_mDice did not improve from 0.50741
Epoch 47/300
 - 44s - loss: 0.0411 - acc: 0.9953 - mDice: 0.9202 - val_loss: 0.0768 - val_acc: 0.9933 - val_mDice: 0.4894

Epoch 00047: val_mDice did not improve from 0.50741
Epoch 48/300
 - 44s - loss: 0.0400 - acc: 0.9954 - mDice: 0.9223 - val_loss: 0.0779 - val_acc: 0.9926 - val_mDice: 0.4890

Epoch 00048: val_mDice did not improve from 0.50741
Epoch 49/300
 - 45s - loss: 0.0414 - acc: 0.9953 - mDice: 0.9195 - val_loss: 0.0719 - val_acc: 0.9941 - val_mDice: 0.4962

Epoch 00049: val_mDice did not improve from 0.50741
Epoch 50/300
 - 45s - loss: 0.0400 - acc: 0.9954 - mDice: 0.9223 - val_loss: 0.0998 - val_acc: 0.9939 - val_mDice: 0.4951

Epoch 00050: val_mDice did not improve from 0.50741
Epoch 51/300
 - 44s - loss: 0.0395 - acc: 0.9954 - mDice: 0.9232 - val_loss: -1.6209e-02 - val_acc: 0.9935 - val_mDice: 0.4804

Epoch 00051: val_mDice did not improve from 0.50741
Epoch 52/300
 - 44s - loss: 0.0395 - acc: 0.9954 - mDice: 0.9233 - val_loss: 0.1403 - val_acc: 0.9928 - val_mDice: 0.4860

Epoch 00052: val_mDice did not improve from 0.50741
Epoch 53/300
 - 44s - loss: 0.0402 - acc: 0.9954 - mDice: 0.9219 - val_loss: 0.1527 - val_acc: 0.9930 - val_mDice: 0.4899

Epoch 00053: val_mDice did not improve from 0.50741
Epoch 54/300
 - 44s - loss: 0.0395 - acc: 0.9954 - mDice: 0.9233 - val_loss: 0.1347 - val_acc: 0.9940 - val_mDice: 0.4964

Epoch 00054: val_mDice did not improve from 0.50741
Epoch 55/300
 - 44s - loss: 0.0400 - acc: 0.9954 - mDice: 0.9224 - val_loss: 0.1173 - val_acc: 0.9937 - val_mDice: 0.4900

Epoch 00055: val_mDice did not improve from 0.50741
Epoch 56/300
 - 44s - loss: 0.0400 - acc: 0.9955 - mDice: 0.9224 - val_loss: 0.0935 - val_acc: 0.9935 - val_mDice: 0.4975

Epoch 00056: val_mDice did not improve from 0.50741
Epoch 57/300
 - 46s - loss: 0.0399 - acc: 0.9953 - mDice: 0.9226 - val_loss: 0.2598 - val_acc: 0.9931 - val_mDice: 0.4814

Epoch 00057: val_mDice did not improve from 0.50741
Epoch 58/300
 - 45s - loss: 0.0396 - acc: 0.9955 - mDice: 0.9231 - val_loss: 0.0666 - val_acc: 0.9935 - val_mDice: 0.4944

Epoch 00058: val_mDice did not improve from 0.50741
Epoch 59/300
 - 45s - loss: 0.0399 - acc: 0.9955 - mDice: 0.9225 - val_loss: 0.1512 - val_acc: 0.9918 - val_mDice: 0.4785

Epoch 00059: val_mDice did not improve from 0.50741

Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 60/300
 - 46s - loss: 0.0383 - acc: 0.9956 - mDice: 0.9257 - val_loss: 0.1631 - val_acc: 0.9939 - val_mDice: 0.4903

Epoch 00060: val_mDice did not improve from 0.50741
Epoch 61/300
 - 46s - loss: 0.0373 - acc: 0.9957 - mDice: 0.9276 - val_loss: 0.0968 - val_acc: 0.9936 - val_mDice: 0.4983

Epoch 00061: val_mDice did not improve from 0.50741
Epoch 62/300
 - 47s - loss: 0.0370 - acc: 0.9956 - mDice: 0.9282 - val_loss: 0.0855 - val_acc: 0.9927 - val_mDice: 0.4888

Epoch 00062: val_mDice did not improve from 0.50741
Epoch 63/300
 - 46s - loss: 0.0366 - acc: 0.9957 - mDice: 0.9290 - val_loss: 0.0082 - val_acc: 0.9939 - val_mDice: 0.4958

Epoch 00063: val_mDice did not improve from 0.50741
Epoch 64/300
 - 46s - loss: 0.0367 - acc: 0.9957 - mDice: 0.9288 - val_loss: 0.1212 - val_acc: 0.9932 - val_mDice: 0.4968

Epoch 00064: val_mDice did not improve from 0.50741
Epoch 65/300
 - 46s - loss: 0.0363 - acc: 0.9957 - mDice: 0.9295 - val_loss: 0.0334 - val_acc: 0.9939 - val_mDice: 0.4927

Epoch 00065: val_mDice did not improve from 0.50741
Epoch 66/300
 - 46s - loss: 0.0367 - acc: 0.9957 - mDice: 0.9287 - val_loss: 0.0940 - val_acc: 0.9939 - val_mDice: 0.4952

Epoch 00066: val_mDice did not improve from 0.50741
Epoch 67/300
 - 47s - loss: 0.0363 - acc: 0.9957 - mDice: 0.9295 - val_loss: 0.1602 - val_acc: 0.9938 - val_mDice: 0.4838

Epoch 00067: val_mDice did not improve from 0.50741
Epoch 68/300
 - 47s - loss: 0.0369 - acc: 0.9957 - mDice: 0.9285 - val_loss: 0.1975 - val_acc: 0.9924 - val_mDice: 0.4423

Epoch 00068: val_mDice did not improve from 0.50741
Epoch 69/300
 - 47s - loss: 0.0362 - acc: 0.9957 - mDice: 0.9297 - val_loss: 0.1837 - val_acc: 0.9931 - val_mDice: 0.4711

Epoch 00069: val_mDice did not improve from 0.50741
Epoch 70/300
 - 47s - loss: 0.0354 - acc: 0.9957 - mDice: 0.9312 - val_loss: 0.1825 - val_acc: 0.9937 - val_mDice: 0.4971

Epoch 00070: val_mDice did not improve from 0.50741
Epoch 71/300
 - 47s - loss: 0.0361 - acc: 0.9957 - mDice: 0.9298 - val_loss: 0.0945 - val_acc: 0.9939 - val_mDice: 0.4909

Epoch 00071: val_mDice did not improve from 0.50741
Epoch 72/300
 - 46s - loss: 0.0355 - acc: 0.9958 - mDice: 0.9311 - val_loss: 0.2024 - val_acc: 0.9938 - val_mDice: 0.4948

Epoch 00072: val_mDice did not improve from 0.50741
Epoch 73/300
 - 46s - loss: 0.0361 - acc: 0.9957 - mDice: 0.9299 - val_loss: 0.1265 - val_acc: 0.9938 - val_mDice: 0.4937

Epoch 00073: val_mDice did not improve from 0.50741
Epoch 74/300
 - 46s - loss: 0.0358 - acc: 0.9958 - mDice: 0.9306 - val_loss: 0.1109 - val_acc: 0.9933 - val_mDice: 0.4950

Epoch 00074: val_mDice did not improve from 0.50741

Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 75/300
 - 46s - loss: 0.0356 - acc: 0.9958 - mDice: 0.9309 - val_loss: 0.1677 - val_acc: 0.9927 - val_mDice: 0.4900

Epoch 00075: val_mDice did not improve from 0.50741
Epoch 76/300
 - 47s - loss: 0.0350 - acc: 0.9958 - mDice: 0.9321 - val_loss: 0.0915 - val_acc: 0.9939 - val_mDice: 0.4836

Epoch 00076: val_mDice did not improve from 0.50741
Epoch 77/300
 - 47s - loss: 0.0350 - acc: 0.9958 - mDice: 0.9321 - val_loss: 0.0895 - val_acc: 0.9937 - val_mDice: 0.4960

Epoch 00077: val_mDice did not improve from 0.50741
Epoch 78/300
 - 47s - loss: 0.0351 - acc: 0.9958 - mDice: 0.9318 - val_loss: 0.1780 - val_acc: 0.9935 - val_mDice: 0.4819

Epoch 00078: val_mDice did not improve from 0.50741
Epoch 79/300
 - 46s - loss: 0.0346 - acc: 0.9959 - mDice: 0.9329 - val_loss: 0.1774 - val_acc: 0.9938 - val_mDice: 0.4947

Epoch 00079: val_mDice did not improve from 0.50741
Epoch 80/300
 - 46s - loss: 0.0353 - acc: 0.9959 - mDice: 0.9314 - val_loss: 0.2025 - val_acc: 0.9935 - val_mDice: 0.5009

Epoch 00080: val_mDice did not improve from 0.50741
Epoch 81/300
 - 45s - loss: 0.0345 - acc: 0.9959 - mDice: 0.9331 - val_loss: 0.1963 - val_acc: 0.9939 - val_mDice: 0.4974

Epoch 00081: val_mDice did not improve from 0.50741
Epoch 82/300
 - 44s - loss: 0.0340 - acc: 0.9959 - mDice: 0.9341 - val_loss: 0.1824 - val_acc: 0.9941 - val_mDice: 0.4984

Epoch 00082: val_mDice did not improve from 0.50741
Epoch 83/300
 - 45s - loss: 0.0343 - acc: 0.9959 - mDice: 0.9334 - val_loss: 0.2188 - val_acc: 0.9937 - val_mDice: 0.4973

Epoch 00083: val_mDice did not improve from 0.50741
Epoch 84/300
 - 45s - loss: 0.0349 - acc: 0.9959 - mDice: 0.9323 - val_loss: 0.2181 - val_acc: 0.9940 - val_mDice: 0.4957

Epoch 00084: val_mDice did not improve from 0.50741
Restoring model weights from the end of the best epoch
Epoch 00084: early stopping
{'val_loss': [0.2879810924641788, 0.1754048097645864, 0.25642740773037076, 0.18318483186885715, 0.06293588574044406, 0.053704486694186926, 0.09122619335539639, -0.024882871424779296, -0.016492535592988133, 0.11651069758227095, 0.13041323784273118, 0.13614741509081796, 0.09344143635826185, 0.013385950587689877, 0.12729271879652515, 0.3033996086451225, 0.2155769478995353, 0.23066955717513338, 0.2391156405210495, 0.13224314141552895, 0.11556355119682848, 0.17294394713826478, 0.13597963005304337, 0.17987670976435766, 0.1667365279281512, 0.10611713281832635, 0.0301966198021546, 0.044650637020822614, 0.0420047000516206, 0.11504309892188758, 0.1724451424088329, 0.05705960839986801, 0.12310466670896858, 0.15854740905342624, 0.12993117776932195, 0.15856366709340364, 0.16206293168943375, 0.15416161180473864, 0.14493179228156805, 0.15603051939979196, 0.2644943753257394, 0.06354614457814023, 0.14414296171162277, 0.20915906794834882, 0.18811843619914725, 0.2012625111383386, 0.07679756497964263, 0.0779479174525477, 0.071862087410409, 0.09983288211515173, -0.016209283552598208, 0.14026810799259692, 0.15268416365142912, 0.13468395336531103, 0.11732722423039377, 0.0934716637711972, 0.25978594180196524, 0.06661736062960699, 0.15122313087340444, 0.1631020086351782, 0.09679242776473984, 0.0855410440126434, 0.008161804522387683, 0.12120969907846302, 0.03340796602424234, 0.09396120795281604, 0.16020938340807334, 0.19747693778481334, 0.18373248615534976, 0.18254396156407893, 0.09447268676012754, 0.20241888449527323, 0.1265455415705219, 0.11086898669600487, 0.16774273966439068, 0.09150643483735621, 0.08950633002677932, 0.17803146544611081, 0.17743814503774047, 0.20245450380025432, 0.19634434155886993, 0.1824038132908754, 0.2188430751557462, 0.21807055152021348], 'val_acc': [0.990467291790992, 0.991801492869854, 0.9923724005930126, 0.9924123054370284, 0.9932189448736608, 0.9931200942955911, 0.9921672325581312, 0.992030665744096, 0.9936102516949177, 0.9932254897430539, 0.9906409648247063, 0.9934041523374617, 0.9935182649642229, 0.993125710170716, 0.9934244155883789, 0.9805086650885642, 0.9918654100038111, 0.9911560607142746, 0.9927502986975014, 0.9937564902938902, 0.993535727262497, 0.9931593867950141, 0.9939158163033426, 0.9935419671237469, 0.9936311435885727, 0.9937465088441968, 0.9935163999907672, 0.9935809383168817, 0.9937870446592569, 0.9937645928002894, 0.9914803351275623, 0.9928192142397165, 0.99357626773417, 0.9928257544524968, 0.9932201858609915, 0.9928965349681675, 0.9928176496177912, 0.9918535631150007, 0.9916605586186051, 0.9936832184903324, 0.9820280717685819, 0.9934718105942011, 0.993563168682158, 0.9922526646405458, 0.9936448526568711, 0.9907011394388974, 0.9932604073546827, 0.9926252709701657, 0.9940586229786277, 0.9939466803334653, 0.9935195175930858, 0.9928117250092328, 0.9929716787301004, 0.9939831648953259, 0.99367292271927, 0.993458092212677, 0.9930886053480208, 0.9935064255259931, 0.9917506673373282, 0.9938827659934759, 0.9936326988972723, 0.9926598812453449, 0.9938606237992644, 0.993221745826304, 0.9939485569484532, 0.993905839510262, 0.9937686417251825, 0.9924263381399214, 0.9930864199995995, 0.9937047250568867, 0.993926421739161, 0.9938204023055732, 0.993780495133251, 0.9932622793130577, 0.9927004086785018, 0.9939320283010602, 0.9936888227239251, 0.9935419647954404, 0.9937935941852629, 0.9935357295908034, 0.9938687356188893, 0.9940545647405088, 0.9936610786244273, 0.9939837888814509], 'val_mDice': [0.42804626282304525, 0.4429215515847318, 0.457111275754869, 0.4549659229815006, 0.4539065662538633, 0.467660537105985, 0.47638590389396995, 0.46325258968863636, 0.4832244501449168, 0.46619946660939604, 0.3917960872068002, 0.4903671407373622, 0.48804919817484915, 0.4730866671889089, 0.4883252293802798, 0.40432971887639724, 0.4756984394043684, 0.4178033140487969, 0.47042011108715087, 0.4922347015235573, 0.4848428722470999, 0.4838335164822638, 0.4939637886127457, 0.4915007803356275, 0.4919951787451282, 0.4814690690545831, 0.47741045884921596, 0.46525949641363695, 0.5005360109498724, 0.4774750412980211, 0.4761956277070567, 0.45720065594650805, 0.4747076614876278, 0.48142251354875043, 0.47454631290747784, 0.47658892173785716, 0.48971770564094186, 0.4286802327260375, 0.4826195677742362, 0.4842138569802046, 0.4089999885763973, 0.49073185335146263, 0.4913784936070442, 0.5074094148585573, 0.49631522968411446, 0.4673900571651757, 0.4894254432292655, 0.4890359559794888, 0.49621641694102436, 0.49514881044160575, 0.48043000904726796, 0.4860139824450016, 0.48990749230142683, 0.4963616351597011, 0.4899892942921724, 0.49746589676942676, 0.4814063530648127, 0.49444752745330334, 0.47852398245595396, 0.49028450157493353, 0.4982920561451465, 0.4887670144671574, 0.4958194028586149, 0.4967823316110298, 0.492693496926222, 0.49516746948938817, 0.48376992548583075, 0.4423286426139282, 0.4711241618497297, 0.49709755985531956, 0.4908691401942633, 0.49475447623990476, 0.4936865164199953, 0.4950005887102076, 0.49003623833414167, 0.4836416622856632, 0.4959974344819784, 0.4818905924912542, 0.494712715735659, 0.5008855246705934, 0.4974094912176952, 0.4984084708848968, 0.4973254835931584, 0.49570894218049943], 'loss': [0.14312916032539727, 0.08233986422203138, 0.073494635086082, 0.06819168108188553, 0.06424810977933354, 0.06275244169106006, 0.06007197769701765, 0.059757097756577324, 0.0557264368424948, 0.05446890364339906, 0.05396845662539825, 0.05548324987882657, 0.05188165012449701, 0.05128809808299726, 0.05035693054486196, 0.05096695641042935, 0.04876398639256799, 0.04917632324955003, 0.04861915884386916, 0.04829355209556844, 0.04694418603515631, 0.04577531274379969, 0.04641022915532396, 0.04546320326457501, 0.0454073483220678, 0.045057267105557214, 0.0444588968462128, 0.042584461330210995, 0.04463601107431563, 0.04403643254063001, 0.04367883930717928, 0.04431072180107992, 0.042822391569503605, 0.04339098661403921, 0.0426619968303366, 0.04328124289023997, 0.042642077196277335, 0.0435240395982617, 0.041081520774104915, 0.04177285443618764, 0.04160065677670966, 0.04011608152695202, 0.04227099884390152, 0.04039382374326817, 0.040747718898742, 0.03983296353711447, 0.0410742444385839, 0.0400227151912403, 0.041443105325218804, 0.03997755180938299, 0.039544692841039605, 0.03953007456905074, 0.04022344484521881, 0.03950568276012873, 0.03996256405455844, 0.039965713479986474, 0.039850941211577284, 0.039574153858103985, 0.03989263510553765, 0.038260376431239224, 0.03728621932093075, 0.03699715712856606, 0.03655880268226452, 0.036665439638607286, 0.03634046788285304, 0.03669007056121892, 0.03631747491849041, 0.0368524539951277, 0.03623532466101317, 0.03544201120168085, 0.03614541446759585, 0.03549398685358287, 0.03609671448894642, 0.03575147528677877, 0.03560282467178757, 0.034971415214011155, 0.034965153133895484, 0.03510989263035687, 0.03459481582300054, 0.03531835385059055, 0.03446554425037792, 0.033977254304138504, 0.03432198169028647, 0.034851225978775255], 'acc': [0.9858422771319565, 0.9913826114807005, 0.9922411418534521, 0.9927883255948474, 0.9931427033656397, 0.9933910371881336, 0.9935874830586466, 0.9936877033139018, 0.9939783538511104, 0.9940366508847983, 0.9941244420723258, 0.994061143504655, 0.9943220260750752, 0.9944194343719691, 0.9944613989712234, 0.9945011009831483, 0.994582262571866, 0.9945360221928721, 0.994639399243631, 0.9946878096970639, 0.9947489350255087, 0.9948253619195804, 0.9948309440915837, 0.9948780741704886, 0.9948845856126294, 0.9949556888015453, 0.9949744686136349, 0.9950866187312704, 0.9949987477938778, 0.9950556387133447, 0.995029319602742, 0.9950544104158386, 0.9951853542858623, 0.9951419944725511, 0.9951767615079741, 0.9951067949000643, 0.9951849053293013, 0.995132312100704, 0.9952638492401958, 0.995266998099494, 0.9952716162995039, 0.9953324261395319, 0.9952490939362145, 0.9953145268515481, 0.9952907762807796, 0.9953575999850081, 0.9953383889931814, 0.995395256256384, 0.9953187140657246, 0.9954236448469015, 0.9954023380111676, 0.9953862848882408, 0.9953825196502959, 0.9954371118678494, 0.995428110119135, 0.9954519255784271, 0.99533884776266, 0.9954607966983029, 0.9955066472613965, 0.9955657549514939, 0.9956552626239834, 0.9956478754984068, 0.995671626810165, 0.9956970372459844, 0.9957201571238502, 0.9957436482374769, 0.995733553716166, 0.9956786146462552, 0.9957240760859621, 0.9957166750616377, 0.995711198851139, 0.995768952933396, 0.9957175420678782, 0.9957930318387084, 0.9958314686283024, 0.9958288990427728, 0.9958262422559319, 0.9958357811181875, 0.9958824201080044, 0.9958652399815346, 0.9958877667076595, 0.9958833561093741, 0.9958887303818722, 0.9958803516460847], 'mDice': [0.7211165484737506, 0.8397711392578046, 0.8570086084099711, 0.8673258832711724, 0.8750246623165383, 0.8778837809902486, 0.8831385385798662, 0.8837170628721503, 0.891623421300229, 0.8941062522399186, 0.89505954924722, 0.892064771638233, 0.89912766158283, 0.9002674454850188, 0.9021021121606686, 0.9008639046812605, 0.9052245963723649, 0.9044238253761365, 0.9054814552830059, 0.9061109307849118, 0.9087758281909728, 0.9110714882678017, 0.909800098052092, 0.9116654759696016, 0.9117762783657622, 0.912437513728611, 0.9136271922021565, 0.917316897891603, 0.9132559926955559, 0.9144214065216056, 0.9151510119202717, 0.9138717512379709, 0.9167893316613627, 0.9156675129363633, 0.9171070043835973, 0.915904023502019, 0.9171445886023967, 0.9154054151741163, 0.9202299550585981, 0.9188349047950354, 0.9191810697216709, 0.9221156781747725, 0.9178489202494348, 0.921577044337749, 0.9208786089769242, 0.9226684835891462, 0.9201976944878222, 0.9222699738593139, 0.9194719653498272, 0.9223490687983334, 0.9232197779180891, 0.9232544867754453, 0.9218755432145859, 0.9232820735321763, 0.9223674745479084, 0.9223531693050473, 0.9226477597165471, 0.9231328585566967, 0.9224711242066425, 0.9256988265043603, 0.9275977659948448, 0.9281772697437263, 0.9290378695098285, 0.9288171076242747, 0.9294511518980327, 0.928745684964989, 0.9294939569819883, 0.9284530142652209, 0.9296607681720725, 0.9312449973039792, 0.9298495806695637, 0.9311131807594913, 0.929938937633783, 0.9305881619231826, 0.9308738082048299, 0.932123874086141, 0.9321335854673823, 0.9318482790644803, 0.9328569902752673, 0.9314179735703378, 0.9331141026701206, 0.9340890458932914, 0.9333995160178965, 0.9323434824102554], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:00<00:01,  1.71it/s]predicting test subjects:  50%|█████     | 2/4 [00:00<00:00,  2.25it/s]predicting test subjects:  75%|███████▌  | 3/4 [00:00<00:00,  2.88it/s]predicting test subjects: 100%|██████████| 4/4 [00:00<00:00,  3.56it/s]predicting test subjects: 100%|██████████| 4/4 [00:00<00:00,  4.21it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<00:48,  5.47it/s]predicting train subjects:   1%|          | 2/266 [00:00<00:44,  5.89it/s]predicting train subjects:   1%|          | 3/266 [00:00<00:45,  5.76it/s]predicting train subjects:   2%|▏         | 4/266 [00:00<00:47,  5.57it/s]predicting train subjects:   2%|▏         | 5/266 [00:00<00:46,  5.60it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<00:43,  5.97it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<00:41,  6.25it/s]predicting train subjects:   3%|▎         | 8/266 [00:01<00:40,  6.41it/s]predicting train subjects:   3%|▎         | 9/266 [00:01<00:39,  6.55it/s]predicting train subjects:   4%|▍         | 10/266 [00:01<00:38,  6.69it/s]predicting train subjects:   4%|▍         | 11/266 [00:01<00:37,  6.72it/s]predicting train subjects:   5%|▍         | 12/266 [00:01<00:37,  6.79it/s]predicting train subjects:   5%|▍         | 13/266 [00:02<00:38,  6.63it/s]predicting train subjects:   5%|▌         | 14/266 [00:02<00:38,  6.57it/s]predicting train subjects:   6%|▌         | 15/266 [00:02<00:37,  6.69it/s]predicting train subjects:   6%|▌         | 16/266 [00:02<00:36,  6.82it/s]predicting train subjects:   6%|▋         | 17/266 [00:02<00:36,  6.87it/s]predicting train subjects:   7%|▋         | 18/266 [00:02<00:36,  6.88it/s]predicting train subjects:   7%|▋         | 19/266 [00:02<00:35,  6.88it/s]predicting train subjects:   8%|▊         | 20/266 [00:03<00:35,  6.89it/s]predicting train subjects:   8%|▊         | 21/266 [00:03<00:35,  6.83it/s]predicting train subjects:   8%|▊         | 22/266 [00:03<00:35,  6.82it/s]predicting train subjects:   9%|▊         | 23/266 [00:03<00:35,  6.82it/s]predicting train subjects:   9%|▉         | 24/266 [00:03<00:35,  6.86it/s]predicting train subjects:   9%|▉         | 25/266 [00:03<00:35,  6.87it/s]predicting train subjects:  10%|▉         | 26/266 [00:03<00:34,  6.90it/s]predicting train subjects:  10%|█         | 27/266 [00:04<00:34,  6.91it/s]predicting train subjects:  11%|█         | 28/266 [00:04<00:34,  6.92it/s]predicting train subjects:  11%|█         | 29/266 [00:04<00:34,  6.96it/s]predicting train subjects:  11%|█▏        | 30/266 [00:04<00:33,  7.02it/s]predicting train subjects:  12%|█▏        | 31/266 [00:04<00:33,  7.03it/s]predicting train subjects:  12%|█▏        | 32/266 [00:04<00:33,  7.02it/s]predicting train subjects:  12%|█▏        | 33/266 [00:04<00:33,  6.85it/s]predicting train subjects:  13%|█▎        | 34/266 [00:05<00:34,  6.82it/s]predicting train subjects:  13%|█▎        | 35/266 [00:05<00:34,  6.75it/s]predicting train subjects:  14%|█▎        | 36/266 [00:05<00:34,  6.76it/s]predicting train subjects:  14%|█▍        | 37/266 [00:05<00:33,  6.82it/s]predicting train subjects:  14%|█▍        | 38/266 [00:05<00:33,  6.81it/s]predicting train subjects:  15%|█▍        | 39/266 [00:05<00:33,  6.87it/s]predicting train subjects:  15%|█▌        | 40/266 [00:05<00:33,  6.82it/s]predicting train subjects:  15%|█▌        | 41/266 [00:06<00:32,  6.90it/s]predicting train subjects:  16%|█▌        | 42/266 [00:06<00:30,  7.31it/s]predicting train subjects:  16%|█▌        | 43/266 [00:06<00:29,  7.67it/s]predicting train subjects:  17%|█▋        | 44/266 [00:06<00:28,  7.92it/s]predicting train subjects:  17%|█▋        | 45/266 [00:06<00:27,  8.12it/s]predicting train subjects:  17%|█▋        | 46/266 [00:06<00:26,  8.30it/s]predicting train subjects:  18%|█▊        | 47/266 [00:06<00:26,  8.38it/s]predicting train subjects:  18%|█▊        | 48/266 [00:06<00:25,  8.43it/s]predicting train subjects:  18%|█▊        | 49/266 [00:07<00:25,  8.47it/s]predicting train subjects:  19%|█▉        | 50/266 [00:07<00:25,  8.48it/s]predicting train subjects:  19%|█▉        | 51/266 [00:07<00:25,  8.47it/s]predicting train subjects:  20%|█▉        | 52/266 [00:07<00:25,  8.53it/s]predicting train subjects:  20%|█▉        | 53/266 [00:07<00:24,  8.55it/s]predicting train subjects:  20%|██        | 54/266 [00:07<00:24,  8.58it/s]predicting train subjects:  21%|██        | 55/266 [00:07<00:24,  8.51it/s]predicting train subjects:  21%|██        | 56/266 [00:07<00:24,  8.52it/s]predicting train subjects:  21%|██▏       | 57/266 [00:07<00:24,  8.54it/s]predicting train subjects:  22%|██▏       | 58/266 [00:08<00:24,  8.56it/s]predicting train subjects:  22%|██▏       | 59/266 [00:08<00:24,  8.61it/s]predicting train subjects:  23%|██▎       | 60/266 [00:08<00:23,  8.59it/s]predicting train subjects:  23%|██▎       | 61/266 [00:08<00:23,  8.55it/s]predicting train subjects:  23%|██▎       | 62/266 [00:08<00:23,  8.55it/s]predicting train subjects:  24%|██▎       | 63/266 [00:08<00:23,  8.51it/s]predicting train subjects:  24%|██▍       | 64/266 [00:08<00:23,  8.54it/s]predicting train subjects:  24%|██▍       | 65/266 [00:08<00:23,  8.52it/s]predicting train subjects:  25%|██▍       | 66/266 [00:09<00:23,  8.54it/s]predicting train subjects:  25%|██▌       | 67/266 [00:09<00:23,  8.59it/s]predicting train subjects:  26%|██▌       | 68/266 [00:09<00:22,  8.62it/s]predicting train subjects:  26%|██▌       | 69/266 [00:09<00:22,  8.58it/s]predicting train subjects:  26%|██▋       | 70/266 [00:09<00:23,  8.44it/s]predicting train subjects:  27%|██▋       | 71/266 [00:09<00:22,  8.53it/s]predicting train subjects:  27%|██▋       | 72/266 [00:09<00:22,  8.59it/s]predicting train subjects:  27%|██▋       | 73/266 [00:09<00:22,  8.49it/s]predicting train subjects:  28%|██▊       | 74/266 [00:09<00:22,  8.47it/s]predicting train subjects:  28%|██▊       | 75/266 [00:10<00:22,  8.47it/s]predicting train subjects:  29%|██▊       | 76/266 [00:10<00:22,  8.53it/s]predicting train subjects:  29%|██▉       | 77/266 [00:10<00:22,  8.47it/s]predicting train subjects:  29%|██▉       | 78/266 [00:10<00:23,  7.94it/s]predicting train subjects:  30%|██▉       | 79/266 [00:10<00:24,  7.65it/s]predicting train subjects:  30%|███       | 80/266 [00:10<00:24,  7.46it/s]predicting train subjects:  30%|███       | 81/266 [00:10<00:25,  7.29it/s]predicting train subjects:  31%|███       | 82/266 [00:11<00:25,  7.26it/s]predicting train subjects:  31%|███       | 83/266 [00:11<00:25,  7.20it/s]predicting train subjects:  32%|███▏      | 84/266 [00:11<00:25,  7.12it/s]predicting train subjects:  32%|███▏      | 85/266 [00:11<00:25,  7.06it/s]predicting train subjects:  32%|███▏      | 86/266 [00:11<00:25,  7.14it/s]predicting train subjects:  33%|███▎      | 87/266 [00:11<00:25,  7.13it/s]predicting train subjects:  33%|███▎      | 88/266 [00:11<00:24,  7.16it/s]predicting train subjects:  33%|███▎      | 89/266 [00:12<00:24,  7.18it/s]predicting train subjects:  34%|███▍      | 90/266 [00:12<00:24,  7.21it/s]predicting train subjects:  34%|███▍      | 91/266 [00:12<00:24,  7.18it/s]predicting train subjects:  35%|███▍      | 92/266 [00:12<00:24,  7.24it/s]predicting train subjects:  35%|███▍      | 93/266 [00:12<00:23,  7.21it/s]predicting train subjects:  35%|███▌      | 94/266 [00:12<00:23,  7.20it/s]predicting train subjects:  36%|███▌      | 95/266 [00:12<00:23,  7.13it/s]predicting train subjects:  36%|███▌      | 96/266 [00:13<00:27,  6.27it/s]predicting train subjects:  36%|███▋      | 97/266 [00:13<00:27,  6.04it/s]predicting train subjects:  37%|███▋      | 98/266 [00:13<00:27,  6.03it/s]predicting train subjects:  37%|███▋      | 99/266 [00:13<00:27,  6.02it/s]predicting train subjects:  38%|███▊      | 100/266 [00:13<00:25,  6.56it/s]predicting train subjects:  38%|███▊      | 101/266 [00:13<00:23,  6.96it/s]predicting train subjects:  38%|███▊      | 102/266 [00:13<00:22,  7.34it/s]predicting train subjects:  39%|███▊      | 103/266 [00:14<00:21,  7.61it/s]predicting train subjects:  39%|███▉      | 104/266 [00:14<00:20,  7.79it/s]predicting train subjects:  39%|███▉      | 105/266 [00:14<00:20,  7.84it/s]predicting train subjects:  40%|███▉      | 106/266 [00:14<00:20,  7.95it/s]predicting train subjects:  40%|████      | 107/266 [00:14<00:19,  8.05it/s]predicting train subjects:  41%|████      | 108/266 [00:14<00:19,  8.15it/s]predicting train subjects:  41%|████      | 109/266 [00:14<00:19,  8.21it/s]predicting train subjects:  41%|████▏     | 110/266 [00:14<00:18,  8.23it/s]predicting train subjects:  42%|████▏     | 111/266 [00:15<00:18,  8.16it/s]predicting train subjects:  42%|████▏     | 112/266 [00:15<00:19,  8.02it/s]predicting train subjects:  42%|████▏     | 113/266 [00:15<00:19,  7.93it/s]predicting train subjects:  43%|████▎     | 114/266 [00:15<00:19,  7.94it/s]predicting train subjects:  43%|████▎     | 115/266 [00:15<00:19,  7.85it/s]predicting train subjects:  44%|████▎     | 116/266 [00:15<00:19,  7.82it/s]predicting train subjects:  44%|████▍     | 117/266 [00:15<00:19,  7.80it/s]predicting train subjects:  44%|████▍     | 118/266 [00:15<00:18,  7.88it/s]predicting train subjects:  45%|████▍     | 119/266 [00:16<00:19,  7.55it/s]predicting train subjects:  45%|████▌     | 120/266 [00:16<00:19,  7.36it/s]predicting train subjects:  45%|████▌     | 121/266 [00:16<00:19,  7.26it/s]predicting train subjects:  46%|████▌     | 122/266 [00:16<00:20,  7.16it/s]predicting train subjects:  46%|████▌     | 123/266 [00:16<00:20,  7.08it/s]predicting train subjects:  47%|████▋     | 124/266 [00:16<00:19,  7.13it/s]predicting train subjects:  47%|████▋     | 125/266 [00:16<00:19,  7.13it/s]predicting train subjects:  47%|████▋     | 126/266 [00:17<00:19,  7.05it/s]predicting train subjects:  48%|████▊     | 127/266 [00:17<00:19,  7.07it/s]predicting train subjects:  48%|████▊     | 128/266 [00:17<00:19,  7.04it/s]predicting train subjects:  48%|████▊     | 129/266 [00:17<00:19,  6.98it/s]predicting train subjects:  49%|████▉     | 130/266 [00:17<00:19,  6.93it/s]predicting train subjects:  49%|████▉     | 131/266 [00:17<00:19,  6.92it/s]predicting train subjects:  50%|████▉     | 132/266 [00:17<00:19,  6.93it/s]predicting train subjects:  50%|█████     | 133/266 [00:18<00:19,  6.97it/s]predicting train subjects:  50%|█████     | 134/266 [00:18<00:18,  6.98it/s]predicting train subjects:  51%|█████     | 135/266 [00:18<00:18,  6.96it/s]predicting train subjects:  51%|█████     | 136/266 [00:18<00:18,  6.96it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:18<00:18,  7.09it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:18<00:17,  7.18it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:18<00:17,  7.31it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:19<00:16,  7.42it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:19<00:16,  7.44it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:19<00:16,  7.49it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:19<00:16,  7.47it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:19<00:16,  7.51it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:19<00:16,  7.49it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:19<00:16,  7.48it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:19<00:15,  7.46it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:20<00:15,  7.42it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:20<00:15,  7.33it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:20<00:16,  7.25it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:20<00:15,  7.28it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:20<00:15,  7.18it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:20<00:15,  7.24it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:20<00:15,  7.30it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:21<00:14,  7.80it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:21<00:13,  8.16it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:21<00:12,  8.46it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:21<00:12,  8.77it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:21<00:11,  8.98it/s]predicting train subjects:  60%|██████    | 160/266 [00:21<00:11,  9.11it/s]predicting train subjects:  61%|██████    | 161/266 [00:21<00:11,  9.26it/s]predicting train subjects:  61%|██████    | 162/266 [00:21<00:11,  9.33it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:21<00:11,  9.34it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:22<00:10,  9.32it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:22<00:10,  9.26it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:22<00:10,  9.18it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:22<00:10,  9.14it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:22<00:10,  9.11it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:22<00:10,  9.15it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:22<00:10,  9.14it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:22<00:10,  9.25it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:22<00:10,  9.14it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:23<00:10,  8.86it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:23<00:10,  8.41it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:23<00:10,  8.37it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:23<00:10,  8.35it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:23<00:10,  8.31it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:23<00:10,  8.32it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:23<00:10,  8.36it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:23<00:10,  8.38it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:23<00:10,  8.40it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:24<00:09,  8.41it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:24<00:09,  8.43it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:24<00:09,  8.39it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:24<00:09,  8.29it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:24<00:09,  8.25it/s]predicting train subjects:  70%|███████   | 187/266 [00:24<00:09,  8.22it/s]predicting train subjects:  71%|███████   | 188/266 [00:24<00:09,  8.28it/s]predicting train subjects:  71%|███████   | 189/266 [00:24<00:09,  8.32it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:25<00:09,  8.29it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:25<00:09,  8.30it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:25<00:10,  7.00it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:25<00:09,  7.42it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:25<00:09,  7.40it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:25<00:09,  7.55it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:25<00:09,  7.77it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:25<00:08,  7.96it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:26<00:08,  8.01it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:26<00:08,  8.02it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:26<00:08,  8.05it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:26<00:07,  8.13it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:26<00:07,  8.20it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:26<00:07,  8.20it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:26<00:07,  8.23it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:26<00:07,  8.03it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:27<00:07,  8.16it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:27<00:07,  8.05it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:27<00:07,  8.00it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:27<00:07,  7.90it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:27<00:07,  7.99it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:27<00:06,  8.03it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:27<00:06,  8.13it/s]predicting train subjects:  80%|████████  | 213/266 [00:27<00:06,  8.41it/s]predicting train subjects:  80%|████████  | 214/266 [00:28<00:06,  8.59it/s]predicting train subjects:  81%|████████  | 215/266 [00:28<00:05,  8.70it/s]predicting train subjects:  81%|████████  | 216/266 [00:28<00:05,  8.75it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:28<00:05,  8.76it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:28<00:05,  8.77it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:28<00:05,  8.85it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:28<00:05,  8.93it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:28<00:05,  8.65it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:28<00:05,  8.78it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:29<00:04,  8.84it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:29<00:04,  8.73it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:29<00:04,  8.74it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:29<00:04,  8.77it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:29<00:04,  8.79it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:29<00:04,  8.76it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:29<00:04,  8.73it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:29<00:04,  8.75it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:30<00:04,  8.70it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:30<00:03,  8.70it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:30<00:03,  8.62it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:30<00:03,  8.66it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:30<00:03,  8.65it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:30<00:03,  8.65it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:30<00:03,  8.58it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:30<00:03,  8.30it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:30<00:03,  8.21it/s]predicting train subjects:  90%|█████████ | 240/266 [00:31<00:03,  7.98it/s]predicting train subjects:  91%|█████████ | 241/266 [00:31<00:03,  8.15it/s]predicting train subjects:  91%|█████████ | 242/266 [00:31<00:02,  8.34it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:31<00:02,  8.51it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:31<00:02,  8.54it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:31<00:02,  8.60it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:31<00:02,  8.50it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:31<00:02,  8.36it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:32<00:02,  8.30it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:32<00:02,  7.99it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:32<00:02,  7.79it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:32<00:01,  7.63it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:32<00:01,  7.59it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:32<00:01,  7.56it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:32<00:01,  7.36it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:32<00:01,  7.35it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:33<00:01,  7.30it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:33<00:01,  7.33it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:33<00:01,  7.24it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:33<00:00,  7.25it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:33<00:00,  7.16it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:33<00:00,  7.20it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:33<00:00,  7.15it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:34<00:00,  6.58it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:34<00:00,  6.82it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:34<00:00,  6.97it/s]predicting train subjects: 100%|██████████| 266/266 [00:34<00:00,  7.09it/s]predicting train subjects: 100%|██████████| 266/266 [00:34<00:00,  7.70it/s]
predicting test subjects sagittal:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects sagittal:  25%|██▌       | 1/4 [00:00<00:00,  7.45it/s]predicting test subjects sagittal:  50%|█████     | 2/4 [00:00<00:00,  7.58it/s]predicting test subjects sagittal:  75%|███████▌  | 3/4 [00:00<00:00,  7.90it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  7.91it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  7.98it/s]
predicting train subjects sagittal:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/266 [00:00<00:38,  6.93it/s]predicting train subjects sagittal:   1%|          | 2/266 [00:00<00:37,  7.02it/s]predicting train subjects sagittal:   1%|          | 3/266 [00:00<00:35,  7.46it/s]predicting train subjects sagittal:   2%|▏         | 4/266 [00:00<00:33,  7.77it/s]predicting train subjects sagittal:   2%|▏         | 5/266 [00:00<00:34,  7.56it/s]predicting train subjects sagittal:   2%|▏         | 6/266 [00:00<00:35,  7.33it/s]predicting train subjects sagittal:   3%|▎         | 7/266 [00:00<00:35,  7.25it/s]predicting train subjects sagittal:   3%|▎         | 8/266 [00:01<00:36,  7.13it/s]predicting train subjects sagittal:   3%|▎         | 9/266 [00:01<00:36,  7.04it/s]predicting train subjects sagittal:   4%|▍         | 10/266 [00:01<00:36,  7.02it/s]predicting train subjects sagittal:   4%|▍         | 11/266 [00:01<00:36,  7.00it/s]predicting train subjects sagittal:   5%|▍         | 12/266 [00:01<00:36,  6.92it/s]predicting train subjects sagittal:   5%|▍         | 13/266 [00:01<00:36,  6.89it/s]predicting train subjects sagittal:   5%|▌         | 14/266 [00:01<00:36,  6.94it/s]predicting train subjects sagittal:   6%|▌         | 15/266 [00:02<00:35,  7.03it/s]predicting train subjects sagittal:   6%|▌         | 16/266 [00:02<00:35,  7.06it/s]predicting train subjects sagittal:   6%|▋         | 17/266 [00:02<00:36,  6.91it/s]predicting train subjects sagittal:   7%|▋         | 18/266 [00:02<00:35,  6.96it/s]predicting train subjects sagittal:   7%|▋         | 19/266 [00:02<00:35,  6.99it/s]predicting train subjects sagittal:   8%|▊         | 20/266 [00:02<00:35,  6.97it/s]predicting train subjects sagittal:   8%|▊         | 21/266 [00:02<00:35,  6.98it/s]predicting train subjects sagittal:   8%|▊         | 22/266 [00:03<00:34,  6.98it/s]predicting train subjects sagittal:   9%|▊         | 23/266 [00:03<00:34,  6.96it/s]predicting train subjects sagittal:   9%|▉         | 24/266 [00:03<00:34,  7.06it/s]predicting train subjects sagittal:   9%|▉         | 25/266 [00:03<00:33,  7.15it/s]predicting train subjects sagittal:  10%|▉         | 26/266 [00:03<00:33,  7.23it/s]predicting train subjects sagittal:  10%|█         | 27/266 [00:03<00:33,  7.24it/s]predicting train subjects sagittal:  11%|█         | 28/266 [00:03<00:32,  7.23it/s]predicting train subjects sagittal:  11%|█         | 29/266 [00:04<00:32,  7.23it/s]predicting train subjects sagittal:  11%|█▏        | 30/266 [00:04<00:32,  7.27it/s]predicting train subjects sagittal:  12%|█▏        | 31/266 [00:04<00:32,  7.32it/s]predicting train subjects sagittal:  12%|█▏        | 32/266 [00:04<00:32,  7.27it/s]predicting train subjects sagittal:  12%|█▏        | 33/266 [00:04<00:32,  7.07it/s]predicting train subjects sagittal:  13%|█▎        | 34/266 [00:04<00:32,  7.09it/s]predicting train subjects sagittal:  13%|█▎        | 35/266 [00:04<00:32,  7.09it/s]predicting train subjects sagittal:  14%|█▎        | 36/266 [00:05<00:32,  7.15it/s]predicting train subjects sagittal:  14%|█▍        | 37/266 [00:05<00:31,  7.21it/s]predicting train subjects sagittal:  14%|█▍        | 38/266 [00:05<00:31,  7.24it/s]predicting train subjects sagittal:  15%|█▍        | 39/266 [00:05<00:31,  7.17it/s]predicting train subjects sagittal:  15%|█▌        | 40/266 [00:05<00:32,  7.01it/s]predicting train subjects sagittal:  15%|█▌        | 41/266 [00:05<00:31,  7.05it/s]predicting train subjects sagittal:  16%|█▌        | 42/266 [00:05<00:30,  7.32it/s]predicting train subjects sagittal:  16%|█▌        | 43/266 [00:05<00:29,  7.59it/s]predicting train subjects sagittal:  17%|█▋        | 44/266 [00:06<00:27,  7.93it/s]predicting train subjects sagittal:  17%|█▋        | 45/266 [00:06<00:26,  8.22it/s]predicting train subjects sagittal:  17%|█▋        | 46/266 [00:06<00:26,  8.39it/s]predicting train subjects sagittal:  18%|█▊        | 47/266 [00:06<00:25,  8.49it/s]predicting train subjects sagittal:  18%|█▊        | 48/266 [00:06<00:25,  8.58it/s]predicting train subjects sagittal:  18%|█▊        | 49/266 [00:06<00:25,  8.57it/s]predicting train subjects sagittal:  19%|█▉        | 50/266 [00:06<00:24,  8.64it/s]predicting train subjects sagittal:  19%|█▉        | 51/266 [00:06<00:24,  8.72it/s]predicting train subjects sagittal:  20%|█▉        | 52/266 [00:07<00:24,  8.72it/s]predicting train subjects sagittal:  20%|█▉        | 53/266 [00:07<00:24,  8.70it/s]predicting train subjects sagittal:  20%|██        | 54/266 [00:07<00:24,  8.70it/s]predicting train subjects sagittal:  21%|██        | 55/266 [00:07<00:24,  8.72it/s]predicting train subjects sagittal:  21%|██        | 56/266 [00:07<00:24,  8.74it/s]predicting train subjects sagittal:  21%|██▏       | 57/266 [00:07<00:24,  8.70it/s]predicting train subjects sagittal:  22%|██▏       | 58/266 [00:07<00:23,  8.70it/s]predicting train subjects sagittal:  22%|██▏       | 59/266 [00:07<00:24,  8.58it/s]predicting train subjects sagittal:  23%|██▎       | 60/266 [00:07<00:24,  8.35it/s]predicting train subjects sagittal:  23%|██▎       | 61/266 [00:08<00:24,  8.24it/s]predicting train subjects sagittal:  23%|██▎       | 62/266 [00:08<00:24,  8.18it/s]predicting train subjects sagittal:  24%|██▎       | 63/266 [00:08<00:24,  8.30it/s]predicting train subjects sagittal:  24%|██▍       | 64/266 [00:08<00:24,  8.25it/s]predicting train subjects sagittal:  24%|██▍       | 65/266 [00:08<00:24,  8.33it/s]predicting train subjects sagittal:  25%|██▍       | 66/266 [00:08<00:23,  8.43it/s]predicting train subjects sagittal:  25%|██▌       | 67/266 [00:08<00:23,  8.35it/s]predicting train subjects sagittal:  26%|██▌       | 68/266 [00:08<00:23,  8.29it/s]predicting train subjects sagittal:  26%|██▌       | 69/266 [00:09<00:24,  8.21it/s]predicting train subjects sagittal:  26%|██▋       | 70/266 [00:09<00:23,  8.25it/s]predicting train subjects sagittal:  27%|██▋       | 71/266 [00:09<00:23,  8.29it/s]predicting train subjects sagittal:  27%|██▋       | 72/266 [00:09<00:23,  8.32it/s]predicting train subjects sagittal:  27%|██▋       | 73/266 [00:09<00:23,  8.25it/s]predicting train subjects sagittal:  28%|██▊       | 74/266 [00:09<00:23,  8.21it/s]predicting train subjects sagittal:  28%|██▊       | 75/266 [00:09<00:23,  8.17it/s]predicting train subjects sagittal:  29%|██▊       | 76/266 [00:09<00:22,  8.28it/s]predicting train subjects sagittal:  29%|██▉       | 77/266 [00:10<00:22,  8.36it/s]predicting train subjects sagittal:  29%|██▉       | 78/266 [00:10<00:23,  7.92it/s]predicting train subjects sagittal:  30%|██▉       | 79/266 [00:10<00:24,  7.68it/s]predicting train subjects sagittal:  30%|███       | 80/266 [00:10<00:24,  7.53it/s]predicting train subjects sagittal:  30%|███       | 81/266 [00:10<00:24,  7.42it/s]predicting train subjects sagittal:  31%|███       | 82/266 [00:10<00:25,  7.35it/s]predicting train subjects sagittal:  31%|███       | 83/266 [00:10<00:25,  7.31it/s]predicting train subjects sagittal:  32%|███▏      | 84/266 [00:10<00:24,  7.28it/s]predicting train subjects sagittal:  32%|███▏      | 85/266 [00:11<00:24,  7.27it/s]predicting train subjects sagittal:  32%|███▏      | 86/266 [00:11<00:24,  7.27it/s]predicting train subjects sagittal:  33%|███▎      | 87/266 [00:11<00:24,  7.27it/s]predicting train subjects sagittal:  33%|███▎      | 88/266 [00:11<00:24,  7.25it/s]predicting train subjects sagittal:  33%|███▎      | 89/266 [00:11<00:24,  7.25it/s]predicting train subjects sagittal:  34%|███▍      | 90/266 [00:11<00:24,  7.21it/s]predicting train subjects sagittal:  34%|███▍      | 91/266 [00:11<00:24,  7.23it/s]predicting train subjects sagittal:  35%|███▍      | 92/266 [00:12<00:24,  7.23it/s]predicting train subjects sagittal:  35%|███▍      | 93/266 [00:12<00:24,  7.07it/s]predicting train subjects sagittal:  35%|███▌      | 94/266 [00:12<00:24,  7.02it/s]predicting train subjects sagittal:  36%|███▌      | 95/266 [00:12<00:24,  7.07it/s]predicting train subjects sagittal:  36%|███▌      | 96/266 [00:12<00:24,  6.99it/s]predicting train subjects sagittal:  36%|███▋      | 97/266 [00:12<00:24,  7.04it/s]predicting train subjects sagittal:  37%|███▋      | 98/266 [00:12<00:23,  7.27it/s]predicting train subjects sagittal:  37%|███▋      | 99/266 [00:13<00:21,  7.81it/s]predicting train subjects sagittal:  38%|███▊      | 100/266 [00:13<00:20,  8.07it/s]predicting train subjects sagittal:  38%|███▊      | 101/266 [00:13<00:20,  8.23it/s]predicting train subjects sagittal:  38%|███▊      | 102/266 [00:13<00:19,  8.26it/s]predicting train subjects sagittal:  39%|███▊      | 103/266 [00:13<00:20,  7.91it/s]predicting train subjects sagittal:  39%|███▉      | 104/266 [00:13<00:20,  8.05it/s]predicting train subjects sagittal:  39%|███▉      | 105/266 [00:13<00:19,  8.18it/s]predicting train subjects sagittal:  40%|███▉      | 106/266 [00:13<00:19,  8.22it/s]predicting train subjects sagittal:  40%|████      | 107/266 [00:14<00:19,  8.19it/s]predicting train subjects sagittal:  41%|████      | 108/266 [00:14<00:19,  8.08it/s]predicting train subjects sagittal:  41%|████      | 109/266 [00:14<00:19,  7.96it/s]predicting train subjects sagittal:  41%|████▏     | 110/266 [00:14<00:19,  8.05it/s]predicting train subjects sagittal:  42%|████▏     | 111/266 [00:14<00:19,  8.10it/s]predicting train subjects sagittal:  42%|████▏     | 112/266 [00:14<00:19,  8.00it/s]predicting train subjects sagittal:  42%|████▏     | 113/266 [00:14<00:18,  8.11it/s]predicting train subjects sagittal:  43%|████▎     | 114/266 [00:14<00:18,  8.13it/s]predicting train subjects sagittal:  43%|████▎     | 115/266 [00:15<00:18,  8.24it/s]predicting train subjects sagittal:  44%|████▎     | 116/266 [00:15<00:17,  8.34it/s]predicting train subjects sagittal:  44%|████▍     | 117/266 [00:15<00:17,  8.33it/s]predicting train subjects sagittal:  44%|████▍     | 118/266 [00:15<00:17,  8.32it/s]predicting train subjects sagittal:  45%|████▍     | 119/266 [00:15<00:19,  7.72it/s]predicting train subjects sagittal:  45%|████▌     | 120/266 [00:15<00:19,  7.45it/s]predicting train subjects sagittal:  45%|████▌     | 121/266 [00:15<00:20,  7.17it/s]predicting train subjects sagittal:  46%|████▌     | 122/266 [00:15<00:20,  7.07it/s]predicting train subjects sagittal:  46%|████▌     | 123/266 [00:16<00:20,  6.88it/s]predicting train subjects sagittal:  47%|████▋     | 124/266 [00:16<00:21,  6.75it/s]predicting train subjects sagittal:  47%|████▋     | 125/266 [00:16<00:21,  6.64it/s]predicting train subjects sagittal:  47%|████▋     | 126/266 [00:16<00:20,  6.72it/s]predicting train subjects sagittal:  48%|████▊     | 127/266 [00:16<00:20,  6.66it/s]predicting train subjects sagittal:  48%|████▊     | 128/266 [00:16<00:20,  6.74it/s]predicting train subjects sagittal:  48%|████▊     | 129/266 [00:17<00:20,  6.81it/s]predicting train subjects sagittal:  49%|████▉     | 130/266 [00:17<00:20,  6.77it/s]predicting train subjects sagittal:  49%|████▉     | 131/266 [00:17<00:19,  6.83it/s]predicting train subjects sagittal:  50%|████▉     | 132/266 [00:17<00:19,  6.77it/s]predicting train subjects sagittal:  50%|█████     | 133/266 [00:17<00:20,  6.62it/s]predicting train subjects sagittal:  50%|█████     | 134/266 [00:17<00:19,  6.68it/s]predicting train subjects sagittal:  51%|█████     | 135/266 [00:17<00:19,  6.74it/s]predicting train subjects sagittal:  51%|█████     | 136/266 [00:18<00:19,  6.54it/s]predicting train subjects sagittal:  52%|█████▏    | 137/266 [00:18<00:18,  6.80it/s]predicting train subjects sagittal:  52%|█████▏    | 138/266 [00:18<00:18,  7.00it/s]predicting train subjects sagittal:  52%|█████▏    | 139/266 [00:18<00:17,  7.13it/s]predicting train subjects sagittal:  53%|█████▎    | 140/266 [00:18<00:17,  7.11it/s]predicting train subjects sagittal:  53%|█████▎    | 141/266 [00:18<00:17,  7.24it/s]predicting train subjects sagittal:  53%|█████▎    | 142/266 [00:18<00:17,  7.29it/s]predicting train subjects sagittal:  54%|█████▍    | 143/266 [00:19<00:16,  7.27it/s]predicting train subjects sagittal:  54%|█████▍    | 144/266 [00:19<00:17,  7.14it/s]predicting train subjects sagittal:  55%|█████▍    | 145/266 [00:19<00:16,  7.25it/s]predicting train subjects sagittal:  55%|█████▍    | 146/266 [00:19<00:16,  7.34it/s]predicting train subjects sagittal:  55%|█████▌    | 147/266 [00:19<00:16,  7.43it/s]predicting train subjects sagittal:  56%|█████▌    | 148/266 [00:19<00:15,  7.47it/s]predicting train subjects sagittal:  56%|█████▌    | 149/266 [00:19<00:15,  7.52it/s]predicting train subjects sagittal:  56%|█████▋    | 150/266 [00:19<00:15,  7.51it/s]predicting train subjects sagittal:  57%|█████▋    | 151/266 [00:20<00:15,  7.41it/s]predicting train subjects sagittal:  57%|█████▋    | 152/266 [00:20<00:15,  7.34it/s]predicting train subjects sagittal:  58%|█████▊    | 153/266 [00:20<00:15,  7.37it/s]predicting train subjects sagittal:  58%|█████▊    | 154/266 [00:20<00:15,  7.03it/s]predicting train subjects sagittal:  58%|█████▊    | 155/266 [00:20<00:14,  7.40it/s]predicting train subjects sagittal:  59%|█████▊    | 156/266 [00:20<00:13,  7.89it/s]predicting train subjects sagittal:  59%|█████▉    | 157/266 [00:20<00:13,  8.28it/s]predicting train subjects sagittal:  59%|█████▉    | 158/266 [00:20<00:12,  8.58it/s]predicting train subjects sagittal:  60%|█████▉    | 159/266 [00:21<00:12,  8.77it/s]predicting train subjects sagittal:  60%|██████    | 160/266 [00:21<00:11,  8.95it/s]predicting train subjects sagittal:  61%|██████    | 161/266 [00:21<00:11,  9.09it/s]predicting train subjects sagittal:  61%|██████    | 162/266 [00:21<00:11,  9.18it/s]predicting train subjects sagittal:  61%|██████▏   | 163/266 [00:21<00:11,  9.19it/s]predicting train subjects sagittal:  62%|██████▏   | 164/266 [00:21<00:10,  9.28it/s]predicting train subjects sagittal:  62%|██████▏   | 165/266 [00:21<00:10,  9.31it/s]predicting train subjects sagittal:  62%|██████▏   | 166/266 [00:21<00:10,  9.33it/s]predicting train subjects sagittal:  63%|██████▎   | 167/266 [00:21<00:10,  9.34it/s]predicting train subjects sagittal:  63%|██████▎   | 168/266 [00:22<00:10,  9.34it/s]predicting train subjects sagittal:  64%|██████▎   | 169/266 [00:22<00:10,  9.39it/s]predicting train subjects sagittal:  64%|██████▍   | 170/266 [00:22<00:10,  9.42it/s]predicting train subjects sagittal:  64%|██████▍   | 171/266 [00:22<00:10,  9.40it/s]predicting train subjects sagittal:  65%|██████▍   | 172/266 [00:22<00:10,  9.33it/s]predicting train subjects sagittal:  65%|██████▌   | 173/266 [00:22<00:10,  8.99it/s]predicting train subjects sagittal:  65%|██████▌   | 174/266 [00:22<00:10,  8.86it/s]predicting train subjects sagittal:  66%|██████▌   | 175/266 [00:22<00:10,  8.81it/s]predicting train subjects sagittal:  66%|██████▌   | 176/266 [00:22<00:10,  8.73it/s]predicting train subjects sagittal:  67%|██████▋   | 177/266 [00:23<00:10,  8.65it/s]predicting train subjects sagittal:  67%|██████▋   | 178/266 [00:23<00:10,  8.62it/s]predicting train subjects sagittal:  67%|██████▋   | 179/266 [00:23<00:10,  8.52it/s]predicting train subjects sagittal:  68%|██████▊   | 180/266 [00:23<00:10,  8.55it/s]predicting train subjects sagittal:  68%|██████▊   | 181/266 [00:23<00:09,  8.53it/s]predicting train subjects sagittal:  68%|██████▊   | 182/266 [00:23<00:09,  8.49it/s]predicting train subjects sagittal:  69%|██████▉   | 183/266 [00:23<00:09,  8.54it/s]predicting train subjects sagittal:  69%|██████▉   | 184/266 [00:23<00:09,  8.52it/s]predicting train subjects sagittal:  70%|██████▉   | 185/266 [00:23<00:09,  8.51it/s]predicting train subjects sagittal:  70%|██████▉   | 186/266 [00:24<00:09,  8.55it/s]predicting train subjects sagittal:  70%|███████   | 187/266 [00:24<00:09,  8.50it/s]predicting train subjects sagittal:  71%|███████   | 188/266 [00:24<00:09,  8.55it/s]predicting train subjects sagittal:  71%|███████   | 189/266 [00:24<00:09,  8.51it/s]predicting train subjects sagittal:  71%|███████▏  | 190/266 [00:24<00:08,  8.49it/s]predicting train subjects sagittal:  72%|███████▏  | 191/266 [00:24<00:08,  8.50it/s]predicting train subjects sagittal:  72%|███████▏  | 192/266 [00:24<00:08,  8.51it/s]predicting train subjects sagittal:  73%|███████▎  | 193/266 [00:24<00:08,  8.48it/s]predicting train subjects sagittal:  73%|███████▎  | 194/266 [00:25<00:09,  7.99it/s]predicting train subjects sagittal:  73%|███████▎  | 195/266 [00:25<00:08,  8.05it/s]predicting train subjects sagittal:  74%|███████▎  | 196/266 [00:25<00:08,  8.02it/s]predicting train subjects sagittal:  74%|███████▍  | 197/266 [00:25<00:08,  7.99it/s]predicting train subjects sagittal:  74%|███████▍  | 198/266 [00:25<00:08,  8.01it/s]predicting train subjects sagittal:  75%|███████▍  | 199/266 [00:25<00:08,  8.04it/s]predicting train subjects sagittal:  75%|███████▌  | 200/266 [00:25<00:08,  8.10it/s]predicting train subjects sagittal:  76%|███████▌  | 201/266 [00:25<00:08,  8.10it/s]predicting train subjects sagittal:  76%|███████▌  | 202/266 [00:26<00:07,  8.16it/s]predicting train subjects sagittal:  76%|███████▋  | 203/266 [00:26<00:07,  8.22it/s]predicting train subjects sagittal:  77%|███████▋  | 204/266 [00:26<00:07,  8.21it/s]predicting train subjects sagittal:  77%|███████▋  | 205/266 [00:26<00:07,  8.20it/s]predicting train subjects sagittal:  77%|███████▋  | 206/266 [00:26<00:07,  8.20it/s]predicting train subjects sagittal:  78%|███████▊  | 207/266 [00:26<00:07,  8.24it/s]predicting train subjects sagittal:  78%|███████▊  | 208/266 [00:26<00:07,  8.19it/s]predicting train subjects sagittal:  79%|███████▊  | 209/266 [00:26<00:07,  8.03it/s]predicting train subjects sagittal:  79%|███████▉  | 210/266 [00:27<00:06,  8.07it/s]predicting train subjects sagittal:  79%|███████▉  | 211/266 [00:27<00:06,  8.06it/s]predicting train subjects sagittal:  80%|███████▉  | 212/266 [00:27<00:06,  8.11it/s]predicting train subjects sagittal:  80%|████████  | 213/266 [00:27<00:06,  8.30it/s]predicting train subjects sagittal:  80%|████████  | 214/266 [00:27<00:06,  8.44it/s]predicting train subjects sagittal:  81%|████████  | 215/266 [00:27<00:05,  8.55it/s]predicting train subjects sagittal:  81%|████████  | 216/266 [00:27<00:05,  8.52it/s]predicting train subjects sagittal:  82%|████████▏ | 217/266 [00:27<00:05,  8.60it/s]predicting train subjects sagittal:  82%|████████▏ | 218/266 [00:27<00:05,  8.59it/s]predicting train subjects sagittal:  82%|████████▏ | 219/266 [00:28<00:05,  8.31it/s]predicting train subjects sagittal:  83%|████████▎ | 220/266 [00:28<00:05,  8.42it/s]predicting train subjects sagittal:  83%|████████▎ | 221/266 [00:28<00:05,  8.51it/s]predicting train subjects sagittal:  83%|████████▎ | 222/266 [00:28<00:05,  8.52it/s]predicting train subjects sagittal:  84%|████████▍ | 223/266 [00:28<00:05,  8.46it/s]predicting train subjects sagittal:  84%|████████▍ | 224/266 [00:28<00:04,  8.49it/s]predicting train subjects sagittal:  85%|████████▍ | 225/266 [00:28<00:04,  8.48it/s]predicting train subjects sagittal:  85%|████████▍ | 226/266 [00:28<00:04,  8.47it/s]predicting train subjects sagittal:  85%|████████▌ | 227/266 [00:29<00:04,  8.57it/s]predicting train subjects sagittal:  86%|████████▌ | 228/266 [00:29<00:04,  8.63it/s]predicting train subjects sagittal:  86%|████████▌ | 229/266 [00:29<00:04,  8.70it/s]predicting train subjects sagittal:  86%|████████▋ | 230/266 [00:29<00:04,  8.76it/s]predicting train subjects sagittal:  87%|████████▋ | 231/266 [00:29<00:04,  8.62it/s]predicting train subjects sagittal:  87%|████████▋ | 232/266 [00:29<00:04,  8.39it/s]predicting train subjects sagittal:  88%|████████▊ | 233/266 [00:29<00:03,  8.36it/s]predicting train subjects sagittal:  88%|████████▊ | 234/266 [00:29<00:03,  8.35it/s]predicting train subjects sagittal:  88%|████████▊ | 235/266 [00:29<00:03,  8.42it/s]predicting train subjects sagittal:  89%|████████▊ | 236/266 [00:30<00:03,  8.46it/s]predicting train subjects sagittal:  89%|████████▉ | 237/266 [00:30<00:03,  8.48it/s]predicting train subjects sagittal:  89%|████████▉ | 238/266 [00:30<00:03,  8.51it/s]predicting train subjects sagittal:  90%|████████▉ | 239/266 [00:30<00:03,  8.53it/s]predicting train subjects sagittal:  90%|█████████ | 240/266 [00:30<00:03,  8.53it/s]predicting train subjects sagittal:  91%|█████████ | 241/266 [00:30<00:02,  8.53it/s]predicting train subjects sagittal:  91%|█████████ | 242/266 [00:30<00:02,  8.53it/s]predicting train subjects sagittal:  91%|█████████▏| 243/266 [00:30<00:02,  8.50it/s]predicting train subjects sagittal:  92%|█████████▏| 244/266 [00:31<00:02,  8.49it/s]predicting train subjects sagittal:  92%|█████████▏| 245/266 [00:31<00:02,  8.49it/s]predicting train subjects sagittal:  92%|█████████▏| 246/266 [00:31<00:02,  8.36it/s]predicting train subjects sagittal:  93%|█████████▎| 247/266 [00:31<00:02,  8.40it/s]predicting train subjects sagittal:  93%|█████████▎| 248/266 [00:31<00:02,  8.27it/s]predicting train subjects sagittal:  94%|█████████▎| 249/266 [00:31<00:02,  7.77it/s]predicting train subjects sagittal:  94%|█████████▍| 250/266 [00:31<00:02,  7.56it/s]predicting train subjects sagittal:  94%|█████████▍| 251/266 [00:31<00:02,  7.50it/s]predicting train subjects sagittal:  95%|█████████▍| 252/266 [00:32<00:01,  7.37it/s]predicting train subjects sagittal:  95%|█████████▌| 253/266 [00:32<00:01,  7.32it/s]predicting train subjects sagittal:  95%|█████████▌| 254/266 [00:32<00:01,  7.33it/s]predicting train subjects sagittal:  96%|█████████▌| 255/266 [00:32<00:01,  7.37it/s]predicting train subjects sagittal:  96%|█████████▌| 256/266 [00:32<00:01,  7.34it/s]predicting train subjects sagittal:  97%|█████████▋| 257/266 [00:32<00:01,  7.29it/s]predicting train subjects sagittal:  97%|█████████▋| 258/266 [00:32<00:01,  7.15it/s]predicting train subjects sagittal:  97%|█████████▋| 259/266 [00:33<00:00,  7.23it/s]predicting train subjects sagittal:  98%|█████████▊| 260/266 [00:33<00:00,  7.25it/s]predicting train subjects sagittal:  98%|█████████▊| 261/266 [00:33<00:00,  7.20it/s]predicting train subjects sagittal:  98%|█████████▊| 262/266 [00:33<00:00,  7.15it/s]predicting train subjects sagittal:  99%|█████████▉| 263/266 [00:33<00:00,  7.09it/s]predicting train subjects sagittal:  99%|█████████▉| 264/266 [00:33<00:00,  7.17it/s]predicting train subjects sagittal: 100%|█████████▉| 265/266 [00:33<00:00,  7.03it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:34<00:00,  6.87it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:34<00:00,  7.81it/s]
saving BB  test1-THALAMUS:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 4/4 [00:00<00:00, 71.59it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 7/266 [00:00<00:04, 61.31it/s]saving BB  train1-THALAMUS:   5%|▌         | 14/266 [00:00<00:04, 62.65it/s]saving BB  train1-THALAMUS:   8%|▊         | 21/266 [00:00<00:03, 64.37it/s]saving BB  train1-THALAMUS:  11%|█         | 28/266 [00:00<00:03, 65.91it/s]saving BB  train1-THALAMUS:  13%|█▎        | 34/266 [00:00<00:03, 63.74it/s]saving BB  train1-THALAMUS:  16%|█▌        | 42/266 [00:00<00:03, 66.71it/s]saving BB  train1-THALAMUS:  19%|█▉        | 51/266 [00:00<00:03, 70.38it/s]saving BB  train1-THALAMUS:  23%|██▎       | 60/266 [00:00<00:02, 74.07it/s]saving BB  train1-THALAMUS:  26%|██▌       | 69/266 [00:00<00:02, 77.69it/s]saving BB  train1-THALAMUS:  29%|██▉       | 78/266 [00:01<00:02, 79.50it/s]saving BB  train1-THALAMUS:  32%|███▏      | 86/266 [00:01<00:02, 78.16it/s]saving BB  train1-THALAMUS:  35%|███▌      | 94/266 [00:01<00:02, 76.78it/s]saving BB  train1-THALAMUS:  38%|███▊      | 102/266 [00:01<00:02, 77.51it/s]saving BB  train1-THALAMUS:  41%|████▏     | 110/266 [00:01<00:02, 77.80it/s]saving BB  train1-THALAMUS:  44%|████▍     | 118/266 [00:01<00:01, 77.83it/s]saving BB  train1-THALAMUS:  47%|████▋     | 126/266 [00:01<00:01, 76.34it/s]saving BB  train1-THALAMUS:  50%|█████     | 134/266 [00:01<00:01, 75.13it/s]saving BB  train1-THALAMUS:  53%|█████▎    | 142/266 [00:01<00:01, 74.94it/s]saving BB  train1-THALAMUS:  56%|█████▋    | 150/266 [00:02<00:01, 73.22it/s]saving BB  train1-THALAMUS:  60%|█████▉    | 159/266 [00:02<00:01, 75.78it/s]saving BB  train1-THALAMUS:  63%|██████▎   | 168/266 [00:02<00:01, 79.08it/s]saving BB  train1-THALAMUS:  67%|██████▋   | 178/266 [00:02<00:01, 82.05it/s]saving BB  train1-THALAMUS:  70%|███████   | 187/266 [00:02<00:00, 84.11it/s]saving BB  train1-THALAMUS:  74%|███████▎  | 196/266 [00:02<00:00, 83.63it/s]saving BB  train1-THALAMUS:  77%|███████▋  | 205/266 [00:02<00:00, 82.83it/s]saving BB  train1-THALAMUS:  80%|████████  | 214/266 [00:02<00:00, 82.45it/s]saving BB  train1-THALAMUS:  84%|████████▍ | 223/266 [00:02<00:00, 82.76it/s]saving BB  train1-THALAMUS:  87%|████████▋ | 232/266 [00:02<00:00, 82.84it/s]saving BB  train1-THALAMUS:  91%|█████████ | 241/266 [00:03<00:00, 83.22it/s]saving BB  train1-THALAMUS:  94%|█████████▍| 250/266 [00:03<00:00, 83.32it/s]saving BB  train1-THALAMUS:  97%|█████████▋| 259/266 [00:03<00:00, 80.38it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 77.81it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 4/4 [00:00<00:00, 83.25it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   3%|▎         | 7/266 [00:00<00:03, 68.37it/s]saving BB  train1-THALAMUS Sagittal:   5%|▌         | 14/266 [00:00<00:03, 68.62it/s]saving BB  train1-THALAMUS Sagittal:   8%|▊         | 22/266 [00:00<00:03, 68.98it/s]saving BB  train1-THALAMUS Sagittal:  11%|█         | 29/266 [00:00<00:03, 68.01it/s]saving BB  train1-THALAMUS Sagittal:  14%|█▍        | 37/266 [00:00<00:03, 69.01it/s]saving BB  train1-THALAMUS Sagittal:  17%|█▋        | 45/266 [00:00<00:03, 71.77it/s]saving BB  train1-THALAMUS Sagittal:  20%|██        | 54/266 [00:00<00:02, 74.64it/s]saving BB  train1-THALAMUS Sagittal:  24%|██▎       | 63/266 [00:00<00:02, 76.99it/s]saving BB  train1-THALAMUS Sagittal:  27%|██▋       | 71/266 [00:00<00:02, 76.22it/s]saving BB  train1-THALAMUS Sagittal:  30%|███       | 80/266 [00:01<00:02, 77.98it/s]saving BB  train1-THALAMUS Sagittal:  33%|███▎      | 88/266 [00:01<00:02, 76.39it/s]saving BB  train1-THALAMUS Sagittal:  36%|███▌      | 96/266 [00:01<00:02, 75.80it/s]saving BB  train1-THALAMUS Sagittal:  39%|███▉      | 105/266 [00:01<00:02, 77.41it/s]saving BB  train1-THALAMUS Sagittal:  42%|████▏     | 113/266 [00:01<00:01, 76.62it/s]saving BB  train1-THALAMUS Sagittal:  45%|████▌     | 121/266 [00:01<00:01, 75.78it/s]saving BB  train1-THALAMUS Sagittal:  48%|████▊     | 129/266 [00:01<00:01, 74.90it/s]saving BB  train1-THALAMUS Sagittal:  52%|█████▏    | 137/266 [00:01<00:01, 74.27it/s]saving BB  train1-THALAMUS Sagittal:  55%|█████▍    | 145/266 [00:01<00:01, 72.97it/s]saving BB  train1-THALAMUS Sagittal:  58%|█████▊    | 153/266 [00:02<00:01, 72.19it/s]saving BB  train1-THALAMUS Sagittal:  61%|██████    | 162/266 [00:02<00:01, 76.17it/s]saving BB  train1-THALAMUS Sagittal:  65%|██████▍   | 172/266 [00:02<00:01, 80.22it/s]saving BB  train1-THALAMUS Sagittal:  68%|██████▊   | 181/266 [00:02<00:01, 82.60it/s]saving BB  train1-THALAMUS Sagittal:  71%|███████▏  | 190/266 [00:02<00:00, 82.71it/s]saving BB  train1-THALAMUS Sagittal:  75%|███████▍  | 199/266 [00:02<00:00, 79.97it/s]saving BB  train1-THALAMUS Sagittal:  78%|███████▊  | 208/266 [00:02<00:00, 76.59it/s]saving BB  train1-THALAMUS Sagittal:  82%|████████▏ | 217/266 [00:02<00:00, 77.85it/s]saving BB  train1-THALAMUS Sagittal:  85%|████████▍ | 226/266 [00:02<00:00, 79.20it/s]saving BB  train1-THALAMUS Sagittal:  88%|████████▊ | 235/266 [00:03<00:00, 80.82it/s]saving BB  train1-THALAMUS Sagittal:  92%|█████████▏| 244/266 [00:03<00:00, 80.63it/s]saving BB  train1-THALAMUS Sagittal:  95%|█████████▌| 253/266 [00:03<00:00, 79.44it/s]saving BB  train1-THALAMUS Sagittal:  98%|█████████▊| 261/266 [00:03<00:00, 78.45it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 76.75it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<04:48,  1.09s/it]Loading train:   1%|          | 2/266 [00:02<04:35,  1.04s/it]Loading train:   1%|          | 3/266 [00:02<04:14,  1.03it/s]Loading train:   2%|▏         | 4/266 [00:03<03:53,  1.12it/s]Loading train:   2%|▏         | 5/266 [00:04<03:59,  1.09it/s]Loading train:   2%|▏         | 6/266 [00:05<03:35,  1.21it/s]Loading train:   3%|▎         | 7/266 [00:05<03:18,  1.31it/s]Loading train:   3%|▎         | 8/266 [00:06<03:07,  1.38it/s]Loading train:   3%|▎         | 9/266 [00:07<03:01,  1.42it/s]Loading train:   4%|▍         | 10/266 [00:07<02:53,  1.47it/s]Loading train:   4%|▍         | 11/266 [00:08<02:51,  1.48it/s]Loading train:   5%|▍         | 12/266 [00:08<02:46,  1.52it/s]Loading train:   5%|▍         | 13/266 [00:09<02:41,  1.57it/s]Loading train:   5%|▌         | 14/266 [00:10<02:38,  1.59it/s]Loading train:   6%|▌         | 15/266 [00:10<02:34,  1.62it/s]Loading train:   6%|▌         | 16/266 [00:11<02:32,  1.64it/s]Loading train:   6%|▋         | 17/266 [00:11<02:33,  1.62it/s]Loading train:   7%|▋         | 18/266 [00:12<02:34,  1.61it/s]Loading train:   7%|▋         | 19/266 [00:13<02:36,  1.58it/s]Loading train:   8%|▊         | 20/266 [00:13<02:34,  1.59it/s]Loading train:   8%|▊         | 21/266 [00:14<02:31,  1.62it/s]Loading train:   8%|▊         | 22/266 [00:15<02:30,  1.62it/s]Loading train:   9%|▊         | 23/266 [00:15<02:27,  1.64it/s]Loading train:   9%|▉         | 24/266 [00:16<02:27,  1.64it/s]Loading train:   9%|▉         | 25/266 [00:16<02:25,  1.66it/s]Loading train:  10%|▉         | 26/266 [00:17<02:25,  1.65it/s]Loading train:  10%|█         | 27/266 [00:18<02:26,  1.64it/s]Loading train:  11%|█         | 28/266 [00:18<02:29,  1.60it/s]Loading train:  11%|█         | 29/266 [00:19<02:26,  1.61it/s]Loading train:  11%|█▏        | 30/266 [00:19<02:24,  1.64it/s]Loading train:  12%|█▏        | 31/266 [00:20<02:24,  1.63it/s]Loading train:  12%|█▏        | 32/266 [00:21<02:21,  1.65it/s]Loading train:  12%|█▏        | 33/266 [00:21<02:19,  1.67it/s]Loading train:  13%|█▎        | 34/266 [00:22<02:19,  1.67it/s]Loading train:  13%|█▎        | 35/266 [00:22<02:19,  1.65it/s]Loading train:  14%|█▎        | 36/266 [00:23<02:21,  1.63it/s]Loading train:  14%|█▍        | 37/266 [00:24<02:19,  1.64it/s]Loading train:  14%|█▍        | 38/266 [00:24<02:19,  1.63it/s]Loading train:  15%|█▍        | 39/266 [00:25<02:17,  1.65it/s]Loading train:  15%|█▌        | 40/266 [00:25<02:15,  1.66it/s]Loading train:  15%|█▌        | 41/266 [00:26<02:12,  1.70it/s]Loading train:  16%|█▌        | 42/266 [00:27<02:06,  1.77it/s]Loading train:  16%|█▌        | 43/266 [00:27<02:02,  1.82it/s]Loading train:  17%|█▋        | 44/266 [00:28<01:57,  1.89it/s]Loading train:  17%|█▋        | 45/266 [00:28<01:53,  1.94it/s]Loading train:  17%|█▋        | 46/266 [00:29<01:51,  1.97it/s]Loading train:  18%|█▊        | 47/266 [00:29<01:52,  1.94it/s]Loading train:  18%|█▊        | 48/266 [00:30<01:51,  1.95it/s]Loading train:  18%|█▊        | 49/266 [00:30<01:50,  1.96it/s]Loading train:  19%|█▉        | 50/266 [00:31<01:49,  1.98it/s]Loading train:  19%|█▉        | 51/266 [00:31<01:48,  1.97it/s]Loading train:  20%|█▉        | 52/266 [00:32<01:49,  1.96it/s]Loading train:  20%|█▉        | 53/266 [00:32<01:50,  1.92it/s]Loading train:  20%|██        | 54/266 [00:33<01:49,  1.94it/s]Loading train:  21%|██        | 55/266 [00:33<01:48,  1.95it/s]Loading train:  21%|██        | 56/266 [00:34<01:46,  1.98it/s]Loading train:  21%|██▏       | 57/266 [00:34<01:44,  2.00it/s]Loading train:  22%|██▏       | 58/266 [00:35<01:44,  2.00it/s]Loading train:  22%|██▏       | 59/266 [00:35<01:43,  2.00it/s]Loading train:  23%|██▎       | 60/266 [00:36<01:43,  2.00it/s]Loading train:  23%|██▎       | 61/266 [00:36<01:41,  2.02it/s]Loading train:  23%|██▎       | 62/266 [00:37<01:41,  2.01it/s]Loading train:  24%|██▎       | 63/266 [00:37<01:40,  2.01it/s]Loading train:  24%|██▍       | 64/266 [00:38<01:40,  2.00it/s]Loading train:  24%|██▍       | 65/266 [00:38<01:38,  2.03it/s]Loading train:  25%|██▍       | 66/266 [00:39<01:36,  2.07it/s]Loading train:  25%|██▌       | 67/266 [00:39<01:34,  2.09it/s]Loading train:  26%|██▌       | 68/266 [00:39<01:33,  2.11it/s]Loading train:  26%|██▌       | 69/266 [00:40<01:32,  2.12it/s]Loading train:  26%|██▋       | 70/266 [00:40<01:32,  2.13it/s]Loading train:  27%|██▋       | 71/266 [00:41<01:30,  2.15it/s]Loading train:  27%|██▋       | 72/266 [00:41<01:31,  2.13it/s]Loading train:  27%|██▋       | 73/266 [00:42<01:31,  2.11it/s]Loading train:  28%|██▊       | 74/266 [00:42<01:30,  2.11it/s]Loading train:  28%|██▊       | 75/266 [00:43<01:30,  2.10it/s]Loading train:  29%|██▊       | 76/266 [00:43<01:30,  2.09it/s]Loading train:  29%|██▉       | 77/266 [00:44<01:30,  2.08it/s]Loading train:  29%|██▉       | 78/266 [00:44<01:37,  1.93it/s]Loading train:  30%|██▉       | 79/266 [00:45<01:41,  1.84it/s]Loading train:  30%|███       | 80/266 [00:46<01:44,  1.79it/s]Loading train:  30%|███       | 81/266 [00:46<01:43,  1.79it/s]Loading train:  31%|███       | 82/266 [00:47<01:46,  1.72it/s]Loading train:  31%|███       | 83/266 [00:47<01:46,  1.73it/s]Loading train:  32%|███▏      | 84/266 [00:48<01:45,  1.73it/s]Loading train:  32%|███▏      | 85/266 [00:49<01:45,  1.71it/s]Loading train:  32%|███▏      | 86/266 [00:49<01:49,  1.64it/s]Loading train:  33%|███▎      | 87/266 [00:50<01:50,  1.62it/s]Loading train:  33%|███▎      | 88/266 [00:50<01:47,  1.65it/s]Loading train:  33%|███▎      | 89/266 [00:51<01:45,  1.68it/s]Loading train:  34%|███▍      | 90/266 [00:52<01:43,  1.71it/s]Loading train:  34%|███▍      | 91/266 [00:52<01:41,  1.72it/s]Loading train:  35%|███▍      | 92/266 [00:53<01:39,  1.74it/s]Loading train:  35%|███▍      | 93/266 [00:53<01:39,  1.74it/s]Loading train:  35%|███▌      | 94/266 [00:54<01:38,  1.75it/s]Loading train:  36%|███▌      | 95/266 [00:54<01:38,  1.73it/s]Loading train:  36%|███▌      | 96/266 [00:55<01:51,  1.52it/s]Loading train:  36%|███▋      | 97/266 [00:56<02:08,  1.32it/s]Loading train:  37%|███▋      | 98/266 [00:57<02:08,  1.31it/s]Loading train:  37%|███▋      | 99/266 [00:58<02:02,  1.37it/s]Loading train:  38%|███▊      | 100/266 [00:58<02:02,  1.35it/s]Loading train:  38%|███▊      | 101/266 [00:59<01:53,  1.45it/s]Loading train:  38%|███▊      | 102/266 [01:00<01:47,  1.52it/s]Loading train:  39%|███▊      | 103/266 [01:00<01:40,  1.61it/s]Loading train:  39%|███▉      | 104/266 [01:01<01:35,  1.70it/s]Loading train:  39%|███▉      | 105/266 [01:01<01:33,  1.73it/s]Loading train:  40%|███▉      | 106/266 [01:02<01:30,  1.76it/s]Loading train:  40%|████      | 107/266 [01:02<01:28,  1.80it/s]Loading train:  41%|████      | 108/266 [01:03<01:26,  1.83it/s]Loading train:  41%|████      | 109/266 [01:03<01:24,  1.85it/s]Loading train:  41%|████▏     | 110/266 [01:04<01:23,  1.86it/s]Loading train:  42%|████▏     | 111/266 [01:04<01:24,  1.84it/s]Loading train:  42%|████▏     | 112/266 [01:05<01:23,  1.85it/s]Loading train:  42%|████▏     | 113/266 [01:05<01:23,  1.84it/s]Loading train:  43%|████▎     | 114/266 [01:06<01:23,  1.83it/s]Loading train:  43%|████▎     | 115/266 [01:07<01:22,  1.83it/s]Loading train:  44%|████▎     | 116/266 [01:07<01:21,  1.83it/s]Loading train:  44%|████▍     | 117/266 [01:08<01:20,  1.85it/s]Loading train:  44%|████▍     | 118/266 [01:08<01:19,  1.85it/s]Loading train:  45%|████▍     | 119/266 [01:09<01:20,  1.82it/s]Loading train:  45%|████▌     | 120/266 [01:09<01:21,  1.79it/s]Loading train:  45%|████▌     | 121/266 [01:10<01:22,  1.76it/s]Loading train:  46%|████▌     | 122/266 [01:10<01:21,  1.76it/s]Loading train:  46%|████▌     | 123/266 [01:11<01:22,  1.74it/s]Loading train:  47%|████▋     | 124/266 [01:12<01:23,  1.71it/s]Loading train:  47%|████▋     | 125/266 [01:12<01:22,  1.70it/s]Loading train:  47%|████▋     | 126/266 [01:13<01:21,  1.72it/s]Loading train:  48%|████▊     | 127/266 [01:13<01:21,  1.71it/s]Loading train:  48%|████▊     | 128/266 [01:14<01:20,  1.72it/s]Loading train:  48%|████▊     | 129/266 [01:15<01:19,  1.73it/s]Loading train:  49%|████▉     | 130/266 [01:15<01:19,  1.72it/s]Loading train:  49%|████▉     | 131/266 [01:16<01:19,  1.70it/s]Loading train:  50%|████▉     | 132/266 [01:16<01:19,  1.68it/s]Loading train:  50%|█████     | 133/266 [01:17<01:20,  1.65it/s]Loading train:  50%|█████     | 134/266 [01:18<01:20,  1.63it/s]Loading train:  51%|█████     | 135/266 [01:18<01:19,  1.65it/s]Loading train:  51%|█████     | 136/266 [01:19<01:18,  1.66it/s]Loading train:  52%|█████▏    | 137/266 [01:19<01:16,  1.69it/s]Loading train:  52%|█████▏    | 138/266 [01:20<01:15,  1.70it/s]Loading train:  52%|█████▏    | 139/266 [01:21<01:13,  1.72it/s]Loading train:  53%|█████▎    | 140/266 [01:21<01:12,  1.73it/s]Loading train:  53%|█████▎    | 141/266 [01:22<01:11,  1.74it/s]Loading train:  53%|█████▎    | 142/266 [01:22<01:11,  1.74it/s]Loading train:  54%|█████▍    | 143/266 [01:23<01:10,  1.75it/s]Loading train:  54%|█████▍    | 144/266 [01:23<01:09,  1.75it/s]Loading train:  55%|█████▍    | 145/266 [01:24<01:08,  1.77it/s]Loading train:  55%|█████▍    | 146/266 [01:24<01:07,  1.79it/s]Loading train:  55%|█████▌    | 147/266 [01:25<01:06,  1.80it/s]Loading train:  56%|█████▌    | 148/266 [01:26<01:07,  1.75it/s]Loading train:  56%|█████▌    | 149/266 [01:26<01:06,  1.75it/s]Loading train:  56%|█████▋    | 150/266 [01:27<01:05,  1.77it/s]Loading train:  57%|█████▋    | 151/266 [01:27<01:05,  1.75it/s]Loading train:  57%|█████▋    | 152/266 [01:28<01:05,  1.73it/s]Loading train:  58%|█████▊    | 153/266 [01:29<01:07,  1.68it/s]Loading train:  58%|█████▊    | 154/266 [01:29<01:07,  1.67it/s]Loading train:  58%|█████▊    | 155/266 [01:30<01:02,  1.76it/s]Loading train:  59%|█████▊    | 156/266 [01:30<00:58,  1.88it/s]Loading train:  59%|█████▉    | 157/266 [01:31<00:56,  1.93it/s]Loading train:  59%|█████▉    | 158/266 [01:31<00:54,  1.98it/s]Loading train:  60%|█████▉    | 159/266 [01:32<00:52,  2.02it/s]Loading train:  60%|██████    | 160/266 [01:32<00:51,  2.06it/s]Loading train:  61%|██████    | 161/266 [01:33<00:50,  2.08it/s]Loading train:  61%|██████    | 162/266 [01:33<00:50,  2.06it/s]Loading train:  61%|██████▏   | 163/266 [01:34<00:50,  2.03it/s]Loading train:  62%|██████▏   | 164/266 [01:34<00:50,  2.04it/s]Loading train:  62%|██████▏   | 165/266 [01:35<00:51,  1.98it/s]Loading train:  62%|██████▏   | 166/266 [01:35<00:49,  2.00it/s]Loading train:  63%|██████▎   | 167/266 [01:35<00:48,  2.05it/s]Loading train:  63%|██████▎   | 168/266 [01:36<00:47,  2.06it/s]Loading train:  64%|██████▎   | 169/266 [01:36<00:46,  2.07it/s]Loading train:  64%|██████▍   | 170/266 [01:37<00:46,  2.07it/s]Loading train:  64%|██████▍   | 171/266 [01:37<00:45,  2.08it/s]Loading train:  65%|██████▍   | 172/266 [01:38<00:44,  2.10it/s]Loading train:  65%|██████▌   | 173/266 [01:38<00:46,  2.01it/s]Loading train:  65%|██████▌   | 174/266 [01:39<00:45,  2.03it/s]Loading train:  66%|██████▌   | 175/266 [01:39<00:45,  2.01it/s]Loading train:  66%|██████▌   | 176/266 [01:40<00:45,  1.99it/s]Loading train:  67%|██████▋   | 177/266 [01:40<00:43,  2.03it/s]Loading train:  67%|██████▋   | 178/266 [01:41<00:43,  2.04it/s]Loading train:  67%|██████▋   | 179/266 [01:41<00:43,  2.01it/s]Loading train:  68%|██████▊   | 180/266 [01:42<00:44,  1.93it/s]Loading train:  68%|██████▊   | 181/266 [01:42<00:43,  1.97it/s]Loading train:  68%|██████▊   | 182/266 [01:43<00:42,  1.98it/s]Loading train:  69%|██████▉   | 183/266 [01:43<00:41,  1.98it/s]Loading train:  69%|██████▉   | 184/266 [01:44<00:40,  2.01it/s]Loading train:  70%|██████▉   | 185/266 [01:44<00:40,  2.01it/s]Loading train:  70%|██████▉   | 186/266 [01:45<00:40,  1.98it/s]Loading train:  70%|███████   | 187/266 [01:45<00:40,  1.96it/s]Loading train:  71%|███████   | 188/266 [01:46<00:39,  1.97it/s]Loading train:  71%|███████   | 189/266 [01:47<00:39,  1.93it/s]Loading train:  71%|███████▏  | 190/266 [01:47<00:39,  1.93it/s]Loading train:  72%|███████▏  | 191/266 [01:48<00:45,  1.65it/s]Loading train:  72%|███████▏  | 192/266 [01:49<00:47,  1.56it/s]Loading train:  73%|███████▎  | 193/266 [01:49<00:48,  1.52it/s]Loading train:  73%|███████▎  | 194/266 [01:50<00:52,  1.37it/s]Loading train:  73%|███████▎  | 195/266 [01:51<00:47,  1.49it/s]Loading train:  74%|███████▎  | 196/266 [01:51<00:43,  1.60it/s]Loading train:  74%|███████▍  | 197/266 [01:52<00:41,  1.67it/s]Loading train:  74%|███████▍  | 198/266 [01:52<00:39,  1.73it/s]Loading train:  75%|███████▍  | 199/266 [01:53<00:38,  1.74it/s]Loading train:  75%|███████▌  | 200/266 [01:53<00:37,  1.76it/s]Loading train:  76%|███████▌  | 201/266 [01:54<00:36,  1.77it/s]Loading train:  76%|███████▌  | 202/266 [01:55<00:36,  1.74it/s]Loading train:  76%|███████▋  | 203/266 [01:55<00:36,  1.74it/s]Loading train:  77%|███████▋  | 204/266 [01:56<00:34,  1.78it/s]Loading train:  77%|███████▋  | 205/266 [01:56<00:34,  1.77it/s]Loading train:  77%|███████▋  | 206/266 [01:57<00:33,  1.80it/s]Loading train:  78%|███████▊  | 207/266 [01:57<00:32,  1.83it/s]Loading train:  78%|███████▊  | 208/266 [01:58<00:31,  1.83it/s]Loading train:  79%|███████▊  | 209/266 [01:58<00:31,  1.82it/s]Loading train:  79%|███████▉  | 210/266 [01:59<00:30,  1.82it/s]Loading train:  79%|███████▉  | 211/266 [02:00<00:30,  1.79it/s]Loading train:  80%|███████▉  | 212/266 [02:00<00:30,  1.78it/s]Loading train:  80%|████████  | 213/266 [02:01<00:28,  1.85it/s]Loading train:  80%|████████  | 214/266 [02:01<00:27,  1.88it/s]Loading train:  81%|████████  | 215/266 [02:02<00:26,  1.93it/s]Loading train:  81%|████████  | 216/266 [02:02<00:25,  1.95it/s]Loading train:  82%|████████▏ | 217/266 [02:03<00:24,  1.98it/s]Loading train:  82%|████████▏ | 218/266 [02:03<00:24,  1.99it/s]Loading train:  82%|████████▏ | 219/266 [02:04<00:23,  2.00it/s]Loading train:  83%|████████▎ | 220/266 [02:04<00:22,  2.01it/s]Loading train:  83%|████████▎ | 221/266 [02:05<00:22,  1.99it/s]Loading train:  83%|████████▎ | 222/266 [02:05<00:21,  2.01it/s]Loading train:  84%|████████▍ | 223/266 [02:06<00:21,  2.01it/s]Loading train:  84%|████████▍ | 224/266 [02:06<00:20,  2.01it/s]Loading train:  85%|████████▍ | 225/266 [02:07<00:20,  2.03it/s]Loading train:  85%|████████▍ | 226/266 [02:07<00:19,  2.01it/s]Loading train:  85%|████████▌ | 227/266 [02:08<00:19,  1.95it/s]Loading train:  86%|████████▌ | 228/266 [02:08<00:19,  1.96it/s]Loading train:  86%|████████▌ | 229/266 [02:09<00:18,  1.97it/s]Loading train:  86%|████████▋ | 230/266 [02:09<00:18,  1.99it/s]Loading train:  87%|████████▋ | 231/266 [02:10<00:17,  2.00it/s]Loading train:  87%|████████▋ | 232/266 [02:10<00:16,  2.04it/s]Loading train:  88%|████████▊ | 233/266 [02:11<00:16,  2.06it/s]Loading train:  88%|████████▊ | 234/266 [02:11<00:15,  2.07it/s]Loading train:  88%|████████▊ | 235/266 [02:11<00:15,  2.06it/s]Loading train:  89%|████████▊ | 236/266 [02:12<00:14,  2.05it/s]Loading train:  89%|████████▉ | 237/266 [02:12<00:14,  2.06it/s]Loading train:  89%|████████▉ | 238/266 [02:13<00:13,  2.04it/s]Loading train:  90%|████████▉ | 239/266 [02:13<00:13,  2.04it/s]Loading train:  90%|█████████ | 240/266 [02:14<00:12,  2.01it/s]Loading train:  91%|█████████ | 241/266 [02:14<00:12,  2.04it/s]Loading train:  91%|█████████ | 242/266 [02:15<00:11,  2.06it/s]Loading train:  91%|█████████▏| 243/266 [02:15<00:11,  2.08it/s]Loading train:  92%|█████████▏| 244/266 [02:16<00:10,  2.10it/s]Loading train:  92%|█████████▏| 245/266 [02:16<00:09,  2.11it/s]Loading train:  92%|█████████▏| 246/266 [02:17<00:09,  2.12it/s]Loading train:  93%|█████████▎| 247/266 [02:17<00:08,  2.14it/s]Loading train:  93%|█████████▎| 248/266 [02:18<00:08,  2.14it/s]Loading train:  94%|█████████▎| 249/266 [02:18<00:08,  1.97it/s]Loading train:  94%|█████████▍| 250/266 [02:19<00:08,  1.88it/s]Loading train:  94%|█████████▍| 251/266 [02:19<00:08,  1.81it/s]Loading train:  95%|█████████▍| 252/266 [02:20<00:07,  1.80it/s]Loading train:  95%|█████████▌| 253/266 [02:21<00:07,  1.80it/s]Loading train:  95%|█████████▌| 254/266 [02:21<00:06,  1.78it/s]Loading train:  96%|█████████▌| 255/266 [02:22<00:06,  1.77it/s]Loading train:  96%|█████████▌| 256/266 [02:22<00:05,  1.76it/s]Loading train:  97%|█████████▋| 257/266 [02:23<00:05,  1.77it/s]Loading train:  97%|█████████▋| 258/266 [02:23<00:04,  1.78it/s]Loading train:  97%|█████████▋| 259/266 [02:24<00:03,  1.77it/s]Loading train:  98%|█████████▊| 260/266 [02:25<00:03,  1.76it/s]Loading train:  98%|█████████▊| 261/266 [02:25<00:02,  1.75it/s]Loading train:  98%|█████████▊| 262/266 [02:26<00:02,  1.71it/s]Loading train:  99%|█████████▉| 263/266 [02:26<00:01,  1.64it/s]Loading train:  99%|█████████▉| 264/266 [02:27<00:01,  1.62it/s]Loading train: 100%|█████████▉| 265/266 [02:28<00:00,  1.60it/s]Loading train: 100%|██████████| 266/266 [02:28<00:00,  1.60it/s]Loading train: 100%|██████████| 266/266 [02:28<00:00,  1.79it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:05, 49.07it/s]concatenating: train:   4%|▍         | 11/266 [00:00<00:05, 50.56it/s]concatenating: train:   6%|▋         | 17/266 [00:00<00:04, 52.48it/s]concatenating: train:   9%|▊         | 23/266 [00:00<00:04, 52.93it/s]concatenating: train:  11%|█         | 28/266 [00:00<00:04, 50.38it/s]concatenating: train:  12%|█▏        | 33/266 [00:00<00:04, 49.99it/s]concatenating: train:  15%|█▍        | 39/266 [00:00<00:04, 50.42it/s]concatenating: train:  17%|█▋        | 45/266 [00:00<00:04, 52.20it/s]concatenating: train:  19%|█▉        | 51/266 [00:00<00:04, 52.15it/s]concatenating: train:  21%|██▏       | 57/266 [00:01<00:03, 52.96it/s]concatenating: train:  24%|██▎       | 63/266 [00:01<00:03, 54.57it/s]concatenating: train:  26%|██▌       | 69/266 [00:01<00:03, 55.53it/s]concatenating: train:  28%|██▊       | 75/266 [00:01<00:03, 55.01it/s]concatenating: train:  30%|███       | 81/266 [00:01<00:03, 55.43it/s]concatenating: train:  33%|███▎      | 87/266 [00:01<00:03, 54.63it/s]concatenating: train:  35%|███▍      | 93/266 [00:01<00:03, 55.62it/s]concatenating: train:  37%|███▋      | 99/266 [00:01<00:03, 54.89it/s]concatenating: train:  39%|███▉      | 105/266 [00:01<00:02, 54.22it/s]concatenating: train:  42%|████▏     | 111/266 [00:02<00:02, 54.48it/s]concatenating: train:  44%|████▍     | 117/266 [00:02<00:02, 53.24it/s]concatenating: train:  46%|████▌     | 123/266 [00:02<00:02, 54.85it/s]concatenating: train:  48%|████▊     | 129/266 [00:02<00:02, 55.85it/s]concatenating: train:  51%|█████     | 135/266 [00:02<00:02, 54.23it/s]concatenating: train:  53%|█████▎    | 141/266 [00:02<00:02, 52.41it/s]concatenating: train:  56%|█████▌    | 148/266 [00:02<00:02, 54.84it/s]concatenating: train:  58%|█████▊    | 154/266 [00:02<00:02, 54.76it/s]concatenating: train:  60%|██████    | 160/266 [00:02<00:01, 54.32it/s]concatenating: train:  62%|██████▏   | 166/266 [00:03<00:01, 52.79it/s]concatenating: train:  65%|██████▍   | 172/266 [00:03<00:01, 52.05it/s]concatenating: train:  67%|██████▋   | 178/266 [00:03<00:01, 53.55it/s]concatenating: train:  70%|██████▉   | 185/266 [00:03<00:01, 55.65it/s]concatenating: train:  72%|███████▏  | 191/266 [00:03<00:01, 53.83it/s]concatenating: train:  74%|███████▍  | 197/266 [00:03<00:01, 51.66it/s]concatenating: train:  76%|███████▋  | 203/266 [00:03<00:01, 48.88it/s]concatenating: train:  78%|███████▊  | 208/266 [00:03<00:01, 46.72it/s]concatenating: train:  80%|████████  | 213/266 [00:04<00:01, 45.58it/s]concatenating: train:  82%|████████▏ | 218/266 [00:04<00:01, 46.26it/s]concatenating: train:  84%|████████▍ | 223/266 [00:04<00:00, 46.67it/s]concatenating: train:  86%|████████▌ | 228/266 [00:04<00:00, 46.55it/s]concatenating: train:  88%|████████▊ | 233/266 [00:04<00:00, 47.13it/s]concatenating: train:  90%|████████▉ | 239/266 [00:04<00:00, 48.33it/s]concatenating: train:  92%|█████████▏| 245/266 [00:04<00:00, 50.17it/s]concatenating: train:  94%|█████████▍| 251/266 [00:04<00:00, 50.44it/s]concatenating: train:  97%|█████████▋| 257/266 [00:04<00:00, 49.73it/s]concatenating: train:  98%|█████████▊| 262/266 [00:05<00:00, 49.62it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 52.22it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:10<00:32, 10.83s/it]Loading test:  50%|█████     | 2/4 [00:17<00:18,  9.47s/it]Loading test:  75%|███████▌  | 3/4 [00:22<00:08,  8.31s/it]Loading test: 100%|██████████| 4/4 [00:33<00:00,  8.94s/it]Loading test: 100%|██████████| 4/4 [00:33<00:00,  8.28s/it]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 49.63it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 0  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 84, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 84, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 84, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 84, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 84, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 84, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 84, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 42, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 42, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 42, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 42, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 42, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 42, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 42, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 42, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 42, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 21, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 21, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 21, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 21, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 21, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 21, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 21, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 21, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 21, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 21, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 42, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 42, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 42, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 42, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 42, 40)   0           batch_normalization_7[0][0]      2020-01-21 03:29:36.167082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 03:29:36.167181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 03:29:36.167193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 03:29:36.167201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 03:29:36.167487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 42, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 42, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 42, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 42, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 42, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 84, 20)   11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 84, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 84, 20)   7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 84, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 84, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 84, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 84, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 84, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 84, 60)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 84, 60)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 84, 13)   793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
 --- initialized from Model_7T /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2
------------------------------------------------------------------
class_weights [6.34091685e-02 3.28640916e-02 7.68481583e-02 9.54877008e-03
 2.76363150e-02 7.23026280e-03 8.42314784e-02 1.14222135e-01
 8.96869216e-02 1.36265644e-02 2.90782640e-01 1.89666219e-01
 2.47274784e-04]
Train on 10137 samples, validate on 147 samples
Epoch 1/300
 - 26s - loss: 0.6028 - acc: 0.9074 - mDice: 0.3507 - val_loss: 0.2951 - val_acc: 0.9337 - val_mDice: 0.2750

Epoch 00001: val_mDice improved from -inf to 0.27504, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/300
 - 22s - loss: 0.4931 - acc: 0.9261 - mDice: 0.4689 - val_loss: 0.2150 - val_acc: 0.9411 - val_mDice: 0.2928

Epoch 00002: val_mDice improved from 0.27504 to 0.29277, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 3/300
 - 21s - loss: 0.4603 - acc: 0.9313 - mDice: 0.5042 - val_loss: 0.2013 - val_acc: 0.9397 - val_mDice: 0.2966

Epoch 00003: val_mDice improved from 0.29277 to 0.29661, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/300
 - 22s - loss: 0.4433 - acc: 0.9343 - mDice: 0.5225 - val_loss: 0.1074 - val_acc: 0.9463 - val_mDice: 0.3052

Epoch 00004: val_mDice improved from 0.29661 to 0.30519, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/300
 - 22s - loss: 0.4321 - acc: 0.9357 - mDice: 0.5345 - val_loss: 0.0363 - val_acc: 0.9493 - val_mDice: 0.3052

Epoch 00005: val_mDice improved from 0.30519 to 0.30523, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 6/300
 - 22s - loss: 0.4229 - acc: 0.9369 - mDice: 0.5442 - val_loss: 0.0745 - val_acc: 0.9474 - val_mDice: 0.3058

Epoch 00006: val_mDice improved from 0.30523 to 0.30576, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 7/300
 - 22s - loss: 0.4137 - acc: 0.9386 - mDice: 0.5543 - val_loss: 0.0760 - val_acc: 0.9476 - val_mDice: 0.3047

Epoch 00007: val_mDice did not improve from 0.30576
Epoch 8/300
 - 22s - loss: 0.4307 - acc: 0.9369 - mDice: 0.5355 - val_loss: 0.0312 - val_acc: 0.9510 - val_mDice: 0.3116

Epoch 00008: val_mDice improved from 0.30576 to 0.31155, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 9/300
 - 22s - loss: 0.4138 - acc: 0.9384 - mDice: 0.5531 - val_loss: 0.1554 - val_acc: 0.9345 - val_mDice: 0.2017

Epoch 00009: val_mDice did not improve from 0.31155
Epoch 10/300
 - 22s - loss: 0.4234 - acc: 0.9359 - mDice: 0.5388 - val_loss: 0.0309 - val_acc: 0.9486 - val_mDice: 0.3110

Epoch 00010: val_mDice did not improve from 0.31155
Epoch 11/300
 - 21s - loss: 0.4281 - acc: 0.9344 - mDice: 0.5313 - val_loss: 0.2215 - val_acc: 0.9394 - val_mDice: 0.2959

Epoch 00011: val_mDice did not improve from 0.31155
Epoch 12/300
 - 21s - loss: 0.4343 - acc: 0.9323 - mDice: 0.5214 - val_loss: 0.0579 - val_acc: 0.9482 - val_mDice: 0.2975

Epoch 00012: val_mDice did not improve from 0.31155
Epoch 13/300
 - 21s - loss: 0.4164 - acc: 0.9359 - mDice: 0.5392 - val_loss: 0.0220 - val_acc: 0.9495 - val_mDice: 0.3087

Epoch 00013: val_mDice did not improve from 0.31155
Epoch 14/300
 - 21s - loss: 0.4162 - acc: 0.9366 - mDice: 0.5382 - val_loss: 0.0811 - val_acc: 0.9470 - val_mDice: 0.3134

Epoch 00014: val_mDice improved from 0.31155 to 0.31337, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 15/300
 - 21s - loss: 0.3939 - acc: 0.9399 - mDice: 0.5619 - val_loss: -7.5760e-03 - val_acc: 0.9519 - val_mDice: 0.3072

Epoch 00015: val_mDice did not improve from 0.31337
Epoch 16/300
 - 21s - loss: 0.3903 - acc: 0.9408 - mDice: 0.5673 - val_loss: 0.0184 - val_acc: 0.9482 - val_mDice: 0.2904

Epoch 00016: val_mDice did not improve from 0.31337
Epoch 17/300
 - 20s - loss: 0.4087 - acc: 0.9390 - mDice: 0.5429 - val_loss: 0.1131 - val_acc: 0.9444 - val_mDice: 0.2901

Epoch 00017: val_mDice did not improve from 0.31337
Epoch 18/300
 - 21s - loss: 0.4063 - acc: 0.9380 - mDice: 0.5487 - val_loss: 0.1185 - val_acc: 0.9471 - val_mDice: 0.3080

Epoch 00018: val_mDice did not improve from 0.31337
Epoch 19/300
 - 21s - loss: 0.3861 - acc: 0.9409 - mDice: 0.5714 - val_loss: -2.9245e-03 - val_acc: 0.9512 - val_mDice: 0.3022

Epoch 00019: val_mDice did not improve from 0.31337
Epoch 20/300
 - 20s - loss: 0.3848 - acc: 0.9415 - mDice: 0.5697 - val_loss: 0.1455 - val_acc: 0.9429 - val_mDice: 0.3009

Epoch 00020: val_mDice did not improve from 0.31337
Epoch 21/300
 - 21s - loss: 0.3849 - acc: 0.9415 - mDice: 0.5708 - val_loss: 0.1373 - val_acc: 0.9402 - val_mDice: 0.3040

Epoch 00021: val_mDice did not improve from 0.31337
Epoch 22/300
 - 20s - loss: 0.3812 - acc: 0.9421 - mDice: 0.5738 - val_loss: 0.0426 - val_acc: 0.9500 - val_mDice: 0.3113

Epoch 00022: val_mDice did not improve from 0.31337
Epoch 23/300
 - 20s - loss: 0.3809 - acc: 0.9424 - mDice: 0.5765 - val_loss: 0.1104 - val_acc: 0.9462 - val_mDice: 0.3044

Epoch 00023: val_mDice did not improve from 0.31337
Epoch 24/300
 - 21s - loss: 0.3762 - acc: 0.9426 - mDice: 0.5801 - val_loss: 0.2388 - val_acc: 0.9344 - val_mDice: 0.2156

Epoch 00024: val_mDice did not improve from 0.31337
Epoch 25/300
 - 21s - loss: 0.3763 - acc: 0.9421 - mDice: 0.5794 - val_loss: 0.0337 - val_acc: 0.9527 - val_mDice: 0.3201

Epoch 00025: val_mDice improved from 0.31337 to 0.32014, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 26/300
 - 20s - loss: 0.3695 - acc: 0.9433 - mDice: 0.5860 - val_loss: 0.0669 - val_acc: 0.9500 - val_mDice: 0.3139

Epoch 00026: val_mDice did not improve from 0.32014
Epoch 27/300
 - 21s - loss: 0.3717 - acc: 0.9431 - mDice: 0.5835 - val_loss: 0.0326 - val_acc: 0.9512 - val_mDice: 0.3205

Epoch 00027: val_mDice improved from 0.32014 to 0.32052, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 28/300
 - 22s - loss: 0.3664 - acc: 0.9441 - mDice: 0.5903 - val_loss: -4.2846e-03 - val_acc: 0.9525 - val_mDice: 0.3003

Epoch 00028: val_mDice did not improve from 0.32052
Epoch 29/300
 - 21s - loss: 0.3704 - acc: 0.9436 - mDice: 0.5846 - val_loss: -2.1472e-03 - val_acc: 0.9544 - val_mDice: 0.3206

Epoch 00029: val_mDice improved from 0.32052 to 0.32058, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 30/300
 - 21s - loss: 0.3790 - acc: 0.9426 - mDice: 0.5778 - val_loss: 0.0682 - val_acc: 0.9490 - val_mDice: 0.3126

Epoch 00030: val_mDice did not improve from 0.32058
Epoch 31/300
 - 21s - loss: 0.3775 - acc: 0.9428 - mDice: 0.5781 - val_loss: 0.0193 - val_acc: 0.9531 - val_mDice: 0.3200

Epoch 00031: val_mDice did not improve from 0.32058
Epoch 32/300
 - 22s - loss: 0.3661 - acc: 0.9435 - mDice: 0.5900 - val_loss: 0.0289 - val_acc: 0.9530 - val_mDice: 0.3214

Epoch 00032: val_mDice improved from 0.32058 to 0.32140, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 33/300
 - 21s - loss: 0.3703 - acc: 0.9433 - mDice: 0.5864 - val_loss: 0.0384 - val_acc: 0.9538 - val_mDice: 0.3170

Epoch 00033: val_mDice did not improve from 0.32140
Epoch 34/300
 - 21s - loss: 0.3633 - acc: 0.9443 - mDice: 0.5921 - val_loss: 0.0039 - val_acc: 0.9524 - val_mDice: 0.3199

Epoch 00034: val_mDice did not improve from 0.32140
Epoch 35/300
 - 22s - loss: 0.3588 - acc: 0.9452 - mDice: 0.5991 - val_loss: 0.0213 - val_acc: 0.9535 - val_mDice: 0.3195

Epoch 00035: val_mDice did not improve from 0.32140
Epoch 36/300
 - 21s - loss: 0.3527 - acc: 0.9457 - mDice: 0.6042 - val_loss: 0.0540 - val_acc: 0.9534 - val_mDice: 0.3243

Epoch 00036: val_mDice improved from 0.32140 to 0.32434, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 37/300
 - 21s - loss: 0.3574 - acc: 0.9455 - mDice: 0.5985 - val_loss: 0.1105 - val_acc: 0.9447 - val_mDice: 0.3021

Epoch 00037: val_mDice did not improve from 0.32434
Epoch 38/300
 - 22s - loss: 0.3575 - acc: 0.9451 - mDice: 0.5971 - val_loss: 0.0761 - val_acc: 0.9482 - val_mDice: 0.3144

Epoch 00038: val_mDice did not improve from 0.32434
Epoch 39/300
 - 22s - loss: 0.3495 - acc: 0.9465 - mDice: 0.6064 - val_loss: 0.0659 - val_acc: 0.9487 - val_mDice: 0.3150

Epoch 00039: val_mDice did not improve from 0.32434
Epoch 40/300
 - 22s - loss: 0.3614 - acc: 0.9445 - mDice: 0.5950 - val_loss: 0.1323 - val_acc: 0.9472 - val_mDice: 0.3060

Epoch 00040: val_mDice did not improve from 0.32434
Epoch 41/300
 - 22s - loss: 0.3547 - acc: 0.9455 - mDice: 0.6014 - val_loss: 0.0099 - val_acc: 0.9535 - val_mDice: 0.3188

Epoch 00041: val_mDice did not improve from 0.32434
Epoch 42/300
 - 23s - loss: 0.3483 - acc: 0.9466 - mDice: 0.6089 - val_loss: -6.7021e-03 - val_acc: 0.9534 - val_mDice: 0.3141

Epoch 00042: val_mDice did not improve from 0.32434
Epoch 43/300
 - 22s - loss: 0.3470 - acc: 0.9463 - mDice: 0.6089 - val_loss: 0.0245 - val_acc: 0.9514 - val_mDice: 0.3106

Epoch 00043: val_mDice did not improve from 0.32434
Epoch 44/300
 - 22s - loss: 0.3454 - acc: 0.9469 - mDice: 0.6097 - val_loss: 0.0061 - val_acc: 0.9535 - val_mDice: 0.3229

Epoch 00044: val_mDice did not improve from 0.32434
Epoch 45/300
 - 22s - loss: 0.3462 - acc: 0.9466 - mDice: 0.6108 - val_loss: 0.0092 - val_acc: 0.9545 - val_mDice: 0.3196

Epoch 00045: val_mDice did not improve from 0.32434
Epoch 46/300
 - 21s - loss: 0.3437 - acc: 0.9471 - mDice: 0.6131 - val_loss: 0.0674 - val_acc: 0.9491 - val_mDice: 0.3103

Epoch 00046: val_mDice did not improve from 0.32434
Epoch 47/300
 - 23s - loss: 0.3559 - acc: 0.9451 - mDice: 0.5998 - val_loss: 0.1444 - val_acc: 0.9383 - val_mDice: 0.3025

Epoch 00047: val_mDice did not improve from 0.32434
Epoch 48/300
 - 23s - loss: 0.3577 - acc: 0.9450 - mDice: 0.5969 - val_loss: 0.0046 - val_acc: 0.9544 - val_mDice: 0.3130

Epoch 00048: val_mDice did not improve from 0.32434
Epoch 49/300
 - 23s - loss: 0.3423 - acc: 0.9475 - mDice: 0.6160 - val_loss: -6.5798e-04 - val_acc: 0.9538 - val_mDice: 0.3177

Epoch 00049: val_mDice did not improve from 0.32434
Epoch 50/300
 - 23s - loss: 0.3473 - acc: 0.9467 - mDice: 0.6096 - val_loss: 0.1555 - val_acc: 0.9455 - val_mDice: 0.3088

Epoch 00050: val_mDice did not improve from 0.32434
Epoch 51/300
 - 23s - loss: 0.3497 - acc: 0.9461 - mDice: 0.6087 - val_loss: 0.0906 - val_acc: 0.9485 - val_mDice: 0.3113

Epoch 00051: val_mDice did not improve from 0.32434

Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 52/300
 - 23s - loss: 0.3373 - acc: 0.9480 - mDice: 0.6207 - val_loss: 0.0485 - val_acc: 0.9528 - val_mDice: 0.3225

Epoch 00052: val_mDice did not improve from 0.32434
Epoch 53/300
 - 24s - loss: 0.3388 - acc: 0.9479 - mDice: 0.6197 - val_loss: 0.1587 - val_acc: 0.9429 - val_mDice: 0.3019

Epoch 00053: val_mDice did not improve from 0.32434
Epoch 54/300
 - 23s - loss: 0.3360 - acc: 0.9481 - mDice: 0.6208 - val_loss: 0.0619 - val_acc: 0.9520 - val_mDice: 0.3193

Epoch 00054: val_mDice did not improve from 0.32434
Epoch 55/300
 - 23s - loss: 0.3315 - acc: 0.9486 - mDice: 0.6254 - val_loss: 0.0348 - val_acc: 0.9535 - val_mDice: 0.3237

Epoch 00055: val_mDice did not improve from 0.32434
Epoch 56/300
 - 23s - loss: 0.3310 - acc: 0.9488 - mDice: 0.6282 - val_loss: 0.0199 - val_acc: 0.9539 - val_mDice: 0.3228

Epoch 00056: val_mDice did not improve from 0.32434
Epoch 57/300
 - 23s - loss: 0.3256 - acc: 0.9492 - mDice: 0.6326 - val_loss: 0.0103 - val_acc: 0.9534 - val_mDice: 0.3083

Epoch 00057: val_mDice did not improve from 0.32434
Epoch 58/300
 - 23s - loss: 0.3264 - acc: 0.9490 - mDice: 0.6309 - val_loss: 0.0688 - val_acc: 0.9506 - val_mDice: 0.3152

Epoch 00058: val_mDice did not improve from 0.32434
Epoch 59/300
 - 23s - loss: 0.3285 - acc: 0.9492 - mDice: 0.6269 - val_loss: 0.0622 - val_acc: 0.9485 - val_mDice: 0.3132

Epoch 00059: val_mDice did not improve from 0.32434
Epoch 60/300
 - 23s - loss: 0.3279 - acc: 0.9492 - mDice: 0.6301 - val_loss: 0.0064 - val_acc: 0.9546 - val_mDice: 0.3150

Epoch 00060: val_mDice did not improve from 0.32434
Epoch 61/300
 - 24s - loss: 0.3229 - acc: 0.9495 - mDice: 0.6338 - val_loss: 0.0378 - val_acc: 0.9539 - val_mDice: 0.3178

Epoch 00061: val_mDice did not improve from 0.32434
Epoch 62/300
 - 23s - loss: 0.3218 - acc: 0.9496 - mDice: 0.6361 - val_loss: 0.0444 - val_acc: 0.9523 - val_mDice: 0.3156

Epoch 00062: val_mDice did not improve from 0.32434
Epoch 63/300
 - 24s - loss: 0.3235 - acc: 0.9496 - mDice: 0.6320 - val_loss: 0.0152 - val_acc: 0.9541 - val_mDice: 0.3221

Epoch 00063: val_mDice did not improve from 0.32434
Epoch 64/300
 - 23s - loss: 0.3267 - acc: 0.9494 - mDice: 0.6324 - val_loss: 0.0171 - val_acc: 0.9544 - val_mDice: 0.3224

Epoch 00064: val_mDice did not improve from 0.32434
Epoch 65/300
 - 23s - loss: 0.3219 - acc: 0.9498 - mDice: 0.6355 - val_loss: -5.9623e-03 - val_acc: 0.9538 - val_mDice: 0.3132

Epoch 00065: val_mDice did not improve from 0.32434
Epoch 66/300
 - 23s - loss: 0.3217 - acc: 0.9498 - mDice: 0.6364 - val_loss: 0.0286 - val_acc: 0.9541 - val_mDice: 0.3193

Epoch 00066: val_mDice did not improve from 0.32434

Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 67/300
 - 22s - loss: 0.3195 - acc: 0.9496 - mDice: 0.6383 - val_loss: 0.0624 - val_acc: 0.9498 - val_mDice: 0.3164

Epoch 00067: val_mDice did not improve from 0.32434
Epoch 68/300
 - 22s - loss: 0.3192 - acc: 0.9503 - mDice: 0.6405 - val_loss: 0.0194 - val_acc: 0.9532 - val_mDice: 0.3200

Epoch 00068: val_mDice did not improve from 0.32434
Epoch 69/300
 - 21s - loss: 0.3195 - acc: 0.9505 - mDice: 0.6396 - val_loss: 0.0073 - val_acc: 0.9540 - val_mDice: 0.3217

Epoch 00069: val_mDice did not improve from 0.32434
Epoch 70/300
 - 21s - loss: 0.3200 - acc: 0.9503 - mDice: 0.6372 - val_loss: -3.2414e-03 - val_acc: 0.9548 - val_mDice: 0.3217

Epoch 00070: val_mDice did not improve from 0.32434
Epoch 71/300
 - 21s - loss: 0.3163 - acc: 0.9506 - mDice: 0.6411 - val_loss: 0.0305 - val_acc: 0.9526 - val_mDice: 0.3158

Epoch 00071: val_mDice did not improve from 0.32434
Epoch 72/300
 - 21s - loss: 0.3158 - acc: 0.9507 - mDice: 0.6440 - val_loss: -3.2580e-03 - val_acc: 0.9543 - val_mDice: 0.3218

Epoch 00072: val_mDice did not improve from 0.32434
Epoch 73/300
 - 21s - loss: 0.3173 - acc: 0.9502 - mDice: 0.6400 - val_loss: 0.0302 - val_acc: 0.9525 - val_mDice: 0.3195

Epoch 00073: val_mDice did not improve from 0.32434
Epoch 74/300
 - 21s - loss: 0.3179 - acc: 0.9506 - mDice: 0.6422 - val_loss: 0.0248 - val_acc: 0.9546 - val_mDice: 0.3254

Epoch 00074: val_mDice improved from 0.32434 to 0.32538, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 75/300
 - 21s - loss: 0.3161 - acc: 0.9507 - mDice: 0.6442 - val_loss: -2.2806e-02 - val_acc: 0.9550 - val_mDice: 0.3202

Epoch 00075: val_mDice did not improve from 0.32538
Epoch 76/300
 - 20s - loss: 0.3164 - acc: 0.9506 - mDice: 0.6437 - val_loss: 0.0287 - val_acc: 0.9528 - val_mDice: 0.3212

Epoch 00076: val_mDice did not improve from 0.32538
Epoch 77/300
 - 21s - loss: 0.3160 - acc: 0.9509 - mDice: 0.6420 - val_loss: 0.0297 - val_acc: 0.9535 - val_mDice: 0.3175

Epoch 00077: val_mDice did not improve from 0.32538
Epoch 78/300
 - 20s - loss: 0.3132 - acc: 0.9511 - mDice: 0.6461 - val_loss: 0.0160 - val_acc: 0.9543 - val_mDice: 0.3223

Epoch 00078: val_mDice did not improve from 0.32538
Epoch 79/300
 - 20s - loss: 0.3147 - acc: 0.9506 - mDice: 0.6435 - val_loss: 0.0290 - val_acc: 0.9538 - val_mDice: 0.3209

Epoch 00079: val_mDice did not improve from 0.32538
Epoch 80/300
 - 20s - loss: 0.3163 - acc: 0.9508 - mDice: 0.6434 - val_loss: -2.4865e-03 - val_acc: 0.9549 - val_mDice: 0.3243

Epoch 00080: val_mDice did not improve from 0.32538
Epoch 81/300
 - 20s - loss: 0.3142 - acc: 0.9509 - mDice: 0.6459 - val_loss: 0.0106 - val_acc: 0.9490 - val_mDice: 0.2861

Epoch 00081: val_mDice did not improve from 0.32538
Epoch 82/300
 - 20s - loss: 0.3187 - acc: 0.9499 - mDice: 0.6394 - val_loss: 0.0080 - val_acc: 0.9538 - val_mDice: 0.3209

Epoch 00082: val_mDice did not improve from 0.32538
Epoch 83/300
 - 21s - loss: 0.3138 - acc: 0.9509 - mDice: 0.6459 - val_loss: 0.0173 - val_acc: 0.9530 - val_mDice: 0.3149

Epoch 00083: val_mDice did not improve from 0.32538
Epoch 84/300
 - 20s - loss: 0.3143 - acc: 0.9511 - mDice: 0.6454 - val_loss: 0.0067 - val_acc: 0.9558 - val_mDice: 0.3256

Epoch 00084: val_mDice improved from 0.32538 to 0.32564, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 85/300
 - 20s - loss: 0.3129 - acc: 0.9510 - mDice: 0.6452 - val_loss: 0.0298 - val_acc: 0.9535 - val_mDice: 0.3198

Epoch 00085: val_mDice did not improve from 0.32564
Epoch 86/300
 - 21s - loss: 0.3122 - acc: 0.9511 - mDice: 0.6474 - val_loss: -2.1754e-03 - val_acc: 0.9555 - val_mDice: 0.3202

Epoch 00086: val_mDice did not improve from 0.32564
Epoch 87/300
 - 20s - loss: 0.3112 - acc: 0.9511 - mDice: 0.6473 - val_loss: 0.0372 - val_acc: 0.9531 - val_mDice: 0.3203

Epoch 00087: val_mDice did not improve from 0.32564
Epoch 88/300
 - 20s - loss: 0.3127 - acc: 0.9511 - mDice: 0.6462 - val_loss: -9.2118e-03 - val_acc: 0.9542 - val_mDice: 0.3169

Epoch 00088: val_mDice did not improve from 0.32564
Epoch 89/300
 - 21s - loss: 0.3104 - acc: 0.9510 - mDice: 0.6484 - val_loss: 0.0232 - val_acc: 0.9528 - val_mDice: 0.3159

Epoch 00089: val_mDice did not improve from 0.32564

Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 90/300
 - 20s - loss: 0.3104 - acc: 0.9511 - mDice: 0.6490 - val_loss: 0.0154 - val_acc: 0.9531 - val_mDice: 0.3243

Epoch 00090: val_mDice did not improve from 0.32564
Epoch 91/300
 - 20s - loss: 0.3098 - acc: 0.9515 - mDice: 0.6505 - val_loss: 0.0163 - val_acc: 0.9540 - val_mDice: 0.3234

Epoch 00091: val_mDice did not improve from 0.32564
Epoch 92/300
 - 20s - loss: 0.3091 - acc: 0.9514 - mDice: 0.6509 - val_loss: 0.0346 - val_acc: 0.9543 - val_mDice: 0.3261

Epoch 00092: val_mDice improved from 0.32564 to 0.32608, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_Init_Main_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 93/300
 - 21s - loss: 0.3108 - acc: 0.9515 - mDice: 0.6507 - val_loss: 0.0164 - val_acc: 0.9539 - val_mDice: 0.3232

Epoch 00093: val_mDice did not improve from 0.32608
Epoch 94/300
 - 20s - loss: 0.3102 - acc: 0.9514 - mDice: 0.6495 - val_loss: 0.0388 - val_acc: 0.9537 - val_mDice: 0.3216

Epoch 00094: val_mDice did not improve from 0.32608
Epoch 95/300
 - 20s - loss: 0.3090 - acc: 0.9515 - mDice: 0.6518 - val_loss: 0.0291 - val_acc: 0.9526 - val_mDice: 0.3207

Epoch 00095: val_mDice did not improve from 0.32608
Epoch 96/300
 - 21s - loss: 0.3090 - acc: 0.9516 - mDice: 0.6496 - val_loss: 0.0157 - val_acc: 0.9546 - val_mDice: 0.3245

Epoch 00096: val_mDice did not improve from 0.32608
Epoch 97/300
 - 21s - loss: 0.3035 - acc: 0.9516 - mDice: 0.6539 - val_loss: -4.2423e-03 - val_acc: 0.9555 - val_mDice: 0.3228

Epoch 00097: val_mDice did not improve from 0.32608
Epoch 98/300
 - 21s - loss: 0.3086 - acc: 0.9516 - mDice: 0.6513 - val_loss: -1.3319e-03 - val_acc: 0.9548 - val_mDice: 0.3196

Epoch 00098: val_mDice did not improve from 0.32608
Epoch 99/300
 - 21s - loss: 0.3101 - acc: 0.9515 - mDice: 0.6515 - val_loss: 0.0161 - val_acc: 0.9547 - val_mDice: 0.3235

Epoch 00099: val_mDice did not improve from 0.32608
Epoch 100/300
 - 21s - loss: 0.3077 - acc: 0.9516 - mDice: 0.6519 - val_loss: 0.0171 - val_acc: 0.9545 - val_mDice: 0.3223

Epoch 00100: val_mDice did not improve from 0.32608
Epoch 101/300
 - 20s - loss: 0.3061 - acc: 0.9517 - mDice: 0.6527 - val_loss: 0.0313 - val_acc: 0.9529 - val_mDice: 0.3183

Epoch 00101: val_mDice did not improve from 0.32608
Epoch 102/300
 - 20s - loss: 0.3074 - acc: 0.9517 - mDice: 0.6512 - val_loss: 0.0065 - val_acc: 0.9551 - val_mDice: 0.3225

Epoch 00102: val_mDice did not improve from 0.32608
Epoch 103/300
 - 21s - loss: 0.3083 - acc: 0.9519 - mDice: 0.6520 - val_loss: 0.0181 - val_acc: 0.9540 - val_mDice: 0.3213

Epoch 00103: val_mDice did not improve from 0.32608
Epoch 104/300
 - 21s - loss: 0.3055 - acc: 0.9518 - mDice: 0.6532 - val_loss: 0.0197 - val_acc: 0.9529 - val_mDice: 0.3196

Epoch 00104: val_mDice did not improve from 0.32608

Epoch 00104: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 105/300
 - 20s - loss: 0.3069 - acc: 0.9518 - mDice: 0.6510 - val_loss: 0.0107 - val_acc: 0.9544 - val_mDice: 0.3207

Epoch 00105: val_mDice did not improve from 0.32608
Epoch 106/300
 - 21s - loss: 0.3038 - acc: 0.9520 - mDice: 0.6546 - val_loss: 0.0342 - val_acc: 0.9527 - val_mDice: 0.3187

Epoch 00106: val_mDice did not improve from 0.32608
Epoch 107/300
 - 20s - loss: 0.3054 - acc: 0.9518 - mDice: 0.6552 - val_loss: -2.2283e-03 - val_acc: 0.9550 - val_mDice: 0.3206

Epoch 00107: val_mDice did not improve from 0.32608
Epoch 108/300
 - 20s - loss: 0.3039 - acc: 0.9520 - mDice: 0.6570 - val_loss: 0.0071 - val_acc: 0.9547 - val_mDice: 0.3219

Epoch 00108: val_mDice did not improve from 0.32608
Epoch 109/300
 - 20s - loss: 0.3053 - acc: 0.9518 - mDice: 0.6547 - val_loss: -3.3330e-03 - val_acc: 0.9549 - val_mDice: 0.3218

Epoch 00109: val_mDice did not improve from 0.32608
Epoch 110/300
 - 21s - loss: 0.3063 - acc: 0.9519 - mDice: 0.6532 - val_loss: 0.0085 - val_acc: 0.9543 - val_mDice: 0.3215

Epoch 00110: val_mDice did not improve from 0.32608
Epoch 111/300
 - 21s - loss: 0.3042 - acc: 0.9520 - mDice: 0.6566 - val_loss: 0.0266 - val_acc: 0.9547 - val_mDice: 0.3234

Epoch 00111: val_mDice did not improve from 0.32608
Epoch 112/300
 - 20s - loss: 0.3058 - acc: 0.9519 - mDice: 0.6541 - val_loss: 0.0174 - val_acc: 0.9543 - val_mDice: 0.3220

Epoch 00112: val_mDice did not improve from 0.32608
Epoch 113/300
 - 20s - loss: 0.3061 - acc: 0.9521 - mDice: 0.6528 - val_loss: 0.0279 - val_acc: 0.9542 - val_mDice: 0.3220

Epoch 00113: val_mDice did not improve from 0.32608
Epoch 114/300
 - 21s - loss: 0.3030 - acc: 0.9520 - mDice: 0.6545 - val_loss: 0.0300 - val_acc: 0.9541 - val_mDice: 0.3198

Epoch 00114: val_mDice did not improve from 0.32608
Epoch 115/300
 - 20s - loss: 0.3014 - acc: 0.9521 - mDice: 0.6560 - val_loss: 0.0298 - val_acc: 0.9543 - val_mDice: 0.3197

Epoch 00115: val_mDice did not improve from 0.32608
Epoch 116/300
 - 20s - loss: 0.3046 - acc: 0.9520 - mDice: 0.6533 - val_loss: 0.0189 - val_acc: 0.9546 - val_mDice: 0.3188

Epoch 00116: val_mDice did not improve from 0.32608
Epoch 117/300
 - 20s - loss: 0.3054 - acc: 0.9521 - mDice: 0.6542 - val_loss: 0.0293 - val_acc: 0.9545 - val_mDice: 0.3205

Epoch 00117: val_mDice did not improve from 0.32608
Epoch 118/300
 - 20s - loss: 0.3031 - acc: 0.9520 - mDice: 0.6563 - val_loss: 0.0191 - val_acc: 0.9549 - val_mDice: 0.3202

Epoch 00118: val_mDice did not improve from 0.32608
Epoch 119/300
 - 19s - loss: 0.3051 - acc: 0.9520 - mDice: 0.6535 - val_loss: 0.0301 - val_acc: 0.9540 - val_mDice: 0.3197

Epoch 00119: val_mDice did not improve from 0.32608

Epoch 00119: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 120/300
 - 20s - loss: 0.3071 - acc: 0.9521 - mDice: 0.6552 - val_loss: 0.0176 - val_acc: 0.9545 - val_mDice: 0.3218

Epoch 00120: val_mDice did not improve from 0.32608
Epoch 121/300
 - 20s - loss: 0.3038 - acc: 0.9519 - mDice: 0.6550 - val_loss: 0.0126 - val_acc: 0.9540 - val_mDice: 0.3159

Epoch 00121: val_mDice did not improve from 0.32608
Epoch 122/300
 - 19s - loss: 0.3047 - acc: 0.9519 - mDice: 0.6542 - val_loss: 0.0187 - val_acc: 0.9544 - val_mDice: 0.3206

Epoch 00122: val_mDice did not improve from 0.32608
Epoch 123/300
 - 19s - loss: 0.3005 - acc: 0.9521 - mDice: 0.6572 - val_loss: 0.0075 - val_acc: 0.9544 - val_mDice: 0.3215

Epoch 00123: val_mDice did not improve from 0.32608
Epoch 124/300
 - 20s - loss: 0.3032 - acc: 0.9519 - mDice: 0.6547 - val_loss: 0.0075 - val_acc: 0.9544 - val_mDice: 0.3209

Epoch 00124: val_mDice did not improve from 0.32608
Epoch 125/300
 - 20s - loss: 0.3024 - acc: 0.9521 - mDice: 0.6572 - val_loss: 0.0076 - val_acc: 0.9547 - val_mDice: 0.3213

Epoch 00125: val_mDice did not improve from 0.32608
Epoch 126/300
 - 20s - loss: 0.3053 - acc: 0.9521 - mDice: 0.6557 - val_loss: -1.1746e-02 - val_acc: 0.9550 - val_mDice: 0.3204

Epoch 00126: val_mDice did not improve from 0.32608
Epoch 127/300
 - 20s - loss: 0.3021 - acc: 0.9522 - mDice: 0.6571 - val_loss: 0.0181 - val_acc: 0.9541 - val_mDice: 0.3213

Epoch 00127: val_mDice did not improve from 0.32608
Epoch 128/300
 - 20s - loss: 0.3028 - acc: 0.9521 - mDice: 0.6575 - val_loss: -1.0215e-03 - val_acc: 0.9546 - val_mDice: 0.3206

Epoch 00128: val_mDice did not improve from 0.32608
Epoch 129/300
 - 20s - loss: 0.3058 - acc: 0.9521 - mDice: 0.6522 - val_loss: 0.0082 - val_acc: 0.9544 - val_mDice: 0.3206

Epoch 00129: val_mDice did not improve from 0.32608
Epoch 130/300
 - 20s - loss: 0.3038 - acc: 0.9521 - mDice: 0.6547 - val_loss: 0.0084 - val_acc: 0.9546 - val_mDice: 0.3204

Epoch 00130: val_mDice did not improve from 0.32608
Epoch 131/300
 - 20s - loss: 0.3029 - acc: 0.9522 - mDice: 0.6558 - val_loss: 0.0094 - val_acc: 0.9540 - val_mDice: 0.3194

Epoch 00131: val_mDice did not improve from 0.32608
Epoch 132/300
 - 20s - loss: 0.3024 - acc: 0.9523 - mDice: 0.6558 - val_loss: 0.0091 - val_acc: 0.9545 - val_mDice: 0.3197

Epoch 00132: val_mDice did not improve from 0.32608
Restoring model weights from the end of the best epoch
Epoch 00132: early stopping
{'val_loss': [0.2950989819911061, 0.2150270727900218, 0.20132525090989814, 0.10744545289448329, 0.036290742824373605, 0.07453598947498669, 0.0760427340358293, 0.03121264790799342, 0.15539243192646374, 0.030880159020525256, 0.22147901471526849, 0.05792547870097923, 0.022017738902244437, 0.08106987428994766, -0.007575972474553958, 0.01842677965760231, 0.11305348271009874, 0.11846791001252172, -0.0029244660621597653, 0.14546413222631935, 0.1373373206932934, 0.04258980678052318, 0.1103548097398867, 0.23883186476178417, 0.03368274271044703, 0.06689316690360912, 0.032558471534629256, -0.004284559463968082, -0.0021472481288472, 0.06815910492516851, 0.01934424291371184, 0.028864787587401818, 0.03840881520185341, 0.003946989743818282, 0.021292908110839573, 0.05400667869213487, 0.11049390570292263, 0.07607267548640569, 0.0659147955649089, 0.1323435594308741, 0.009941732346200619, -0.00670207872175846, 0.024478533945116055, 0.0060995546277283, 0.009155917963405856, 0.06742413433230653, 0.14442447816230813, 0.004615756550005504, -0.0006579792722552812, 0.15545539023215268, 0.09063669885223617, 0.04849462659687412, 0.15870947033470992, 0.06186269386354707, 0.034757452415061646, 0.01986328679688123, 0.010296553246626238, 0.06884324114744356, 0.06215587835505503, 0.006431140461746527, 0.03780119246518125, 0.044362908216560776, 0.015218081814395327, 0.017081607817387094, -0.005962284345205138, 0.028551738531816574, 0.06242984290025672, 0.019368601969259533, 0.007270273936240851, -0.0032414112233740537, 0.030471211033208028, -0.0032580154029285017, 0.03024439721488628, 0.024773566179028175, -0.02280572175997037, 0.028734941466324996, 0.029699330655287723, 0.01600320211478642, 0.028965173061100805, -0.002486464624502221, 0.010576768290428888, 0.00799252315848863, 0.017331929864729343, 0.006660664466773571, 0.02977577531013359, -0.0021753893012092227, 0.037247853518343296, -0.00921176556421786, 0.023152287694669905, 0.015350471548482675, 0.01630781073959506, 0.03459226750597662, 0.01635717103878657, 0.03880471041818865, 0.02913738533753116, 0.015704149316970995, -0.004242347642069771, -0.0013319379111536506, 0.016053964846393688, 0.017114151243855354, 0.03131397737532246, 0.006516843007839456, 0.018130009144950075, 0.019746801657538837, 0.010667614689489612, 0.03419962730638835, -0.00222826285325751, 0.007087819126187538, -0.003332994067344536, 0.008453577174013164, 0.026648250389464046, 0.017429498334725697, 0.027914334215274474, 0.02997665553271365, 0.029838565939745935, 0.018927929935609402, 0.029322165641046706, 0.0191244283551667, 0.030081996896944078, 0.017630994231218382, 0.012616579959384438, 0.018738511696356496, 0.0074634005119200465, 0.007503898168096737, 0.007631161890062345, -0.011746208202473971, 0.018057002274154806, -0.0010214633986252507, 0.008234009932295805, 0.008379026385797125, 0.009382679272873872, 0.009096252016064262], 'val_acc': [0.9336578914908324, 0.9411038233309376, 0.9397348688573254, 0.9462510302764218, 0.9493393472262791, 0.9473739063658682, 0.9476075306230661, 0.9510244496014654, 0.9345424953772097, 0.9486042597666889, 0.9393719921306688, 0.9482351557738116, 0.9494592640675655, 0.9469736611762014, 0.9519261944861639, 0.9482289179652726, 0.9443805919212549, 0.9470562063106874, 0.9511661789044231, 0.9429150917092148, 0.9402472393042376, 0.9499950112939692, 0.9462463563802291, 0.9343774148396083, 0.95273448415354, 0.95000903541539, 0.9511552805803261, 0.952491529133855, 0.9544055620018317, 0.9489889298977495, 0.9531300647729108, 0.9529509670069428, 0.953781051700618, 0.9524058723936275, 0.9534851523483692, 0.9533761366694963, 0.9446780507256385, 0.9482055634868388, 0.9487257320053724, 0.9472103844694539, 0.9534602274700087, 0.9534026035646193, 0.9513530723091697, 0.9534758021231411, 0.9544834478371809, 0.949149351136214, 0.938278699813246, 0.9543572871052489, 0.9538402301924569, 0.9454988018185103, 0.9485497470615672, 0.9527547339192864, 0.9429493376186916, 0.9519775854486997, 0.9535131831558383, 0.9539477018271985, 0.9533699004828524, 0.9506397802813523, 0.9485263844736579, 0.9546158183999613, 0.953904085418805, 0.9523248830620124, 0.9540738398525991, 0.9544413884480795, 0.9537623617924801, 0.9540629476106086, 0.9497941120141218, 0.9531907986621467, 0.9539695045574993, 0.9547824648772778, 0.952588085414601, 0.9543168029817594, 0.9524650557511518, 0.9545690980898279, 0.9549833698337581, 0.9527640780624078, 0.9534508764338331, 0.9542981029367771, 0.9537545839945475, 0.9549210671664906, 0.9489889331415396, 0.9538199893471335, 0.9530023612132689, 0.9558041180883136, 0.9535116257310725, 0.9554708344595773, 0.9530755496349465, 0.9541875310495597, 0.952756282829103, 0.9531207165750516, 0.9540364677403249, 0.9543432747425676, 0.9539414717226612, 0.953714090950635, 0.9525507153296957, 0.9546204951344704, 0.9555128857392032, 0.9547886961982364, 0.954709259020228, 0.9545317215173423, 0.9529369461293123, 0.9551375512363148, 0.954008434500013, 0.9528777619608405, 0.9544055717332023, 0.9526550522466906, 0.9549786882335637, 0.9547388549564647, 0.95488058507037, 0.9542529501882541, 0.9546718873134276, 0.9542669645783042, 0.9541704070811369, 0.9541003213447778, 0.9543385992244798, 0.9546111392326095, 0.95452859693644, 0.9548914825835195, 0.9540193299857938, 0.9544927879255645, 0.9539554853017639, 0.9544398318342611, 0.9544491735445398, 0.9544258279865291, 0.9546516367367336, 0.9550051737804802, 0.9540738512058647, 0.9545690976843542, 0.9544040151193839, 0.9546017983332783, 0.953975724930666, 0.9544787723190931], 'val_mDice': [0.2750389638806687, 0.2927735920767395, 0.29661046465237934, 0.30518727308633375, 0.3052282297388226, 0.30575590149885945, 0.3047263552846552, 0.3115540194470866, 0.20166406395281253, 0.3110385886868652, 0.2959000411487761, 0.2975406097311552, 0.30871042431820006, 0.31336878529008555, 0.30724105978904126, 0.2903705178981736, 0.29014201107479276, 0.308005459961437, 0.30221414976582234, 0.3009402901947904, 0.30398701420243907, 0.31127136378061204, 0.30437069776512327, 0.2156465439803797, 0.32014103715314346, 0.3139253356018845, 0.3205246152926464, 0.300262486143988, 0.3205786505523993, 0.3125538623251883, 0.3199551428864602, 0.32139693910167333, 0.3169883091230782, 0.3198797334821857, 0.31949633288951146, 0.3243363078962378, 0.3020829611084088, 0.3144149608960768, 0.31500697308251646, 0.3060105222524429, 0.3187606299207324, 0.3141253878977023, 0.31059519779317235, 0.3229152513097744, 0.31958695729168096, 0.3103104909869278, 0.30247573473421085, 0.31297617045795023, 0.3177104400230103, 0.30882091495861, 0.31133646082107713, 0.32248295331690585, 0.3018724945228116, 0.31926597730845824, 0.3236852304465106, 0.3227962711635901, 0.30831912828951463, 0.315151741816884, 0.3132476601876369, 0.315002614442183, 0.31783766755644155, 0.3156116423355479, 0.32213756300154184, 0.32237170200769594, 0.31321122735536017, 0.31934802918409816, 0.3163655211730879, 0.31995310553279865, 0.3216881634426766, 0.3217089317080115, 0.315849585395281, 0.32176514740298395, 0.3195201459587837, 0.3253768466565074, 0.32022380626120533, 0.32115310557237287, 0.3174727690868637, 0.32232806882282505, 0.3208633065730536, 0.32428132707164403, 0.28613417134398506, 0.32091265906687494, 0.31489744762174127, 0.3256432852980231, 0.3198482371410545, 0.32021729425102674, 0.32032227673295405, 0.3168645583245219, 0.3158579086061238, 0.32434845264671613, 0.32342678744371245, 0.3260833546012437, 0.323235903974293, 0.3215623059037591, 0.3207346361105134, 0.3245237585233182, 0.3227671449687205, 0.3196261563674122, 0.3234899173788473, 0.3223450260401583, 0.31830018241794744, 0.3224670792720756, 0.32125590680813304, 0.3195695675858835, 0.3206923182825653, 0.3186640357281886, 0.320608215165787, 0.32187168877951955, 0.321815308715616, 0.3214771730356476, 0.3233675315063827, 0.3220354606505154, 0.32200439689921684, 0.31976461821064656, 0.3196791863056267, 0.3187737188168934, 0.3204570143502586, 0.3201547464241787, 0.3196631427727589, 0.32179966883189015, 0.31587781290821476, 0.32059854862033105, 0.3214651250920328, 0.3208714675943868, 0.32128161120982396, 0.3204389397485727, 0.32134444707510423, 0.32058788055465337, 0.32063790973351924, 0.32037908011147764, 0.31940870215089956, 0.3196954040908489], 'loss': [0.6027939492362343, 0.4930730437378659, 0.46034623131992386, 0.4433197134684397, 0.43207451973698063, 0.4229221387917066, 0.41367160138251735, 0.4306776281043812, 0.41375759191633954, 0.4233559968909698, 0.42806815667724873, 0.4342594993788309, 0.4163993911282414, 0.41621130673422246, 0.3939435194526961, 0.3903105936777642, 0.40870855925785693, 0.4063071337714755, 0.38606403587402366, 0.38484098432190356, 0.38489730859150884, 0.38122122930953567, 0.3808550069267914, 0.37620005744195284, 0.37625214558940345, 0.3694637644845988, 0.37168636561451407, 0.36641979471541397, 0.3704041369046946, 0.3789934296485434, 0.3774947967945124, 0.3660878778236016, 0.37032836232592725, 0.3632548801278273, 0.358801372757181, 0.3527279402378973, 0.35737719286637065, 0.357521163884547, 0.3495254052309192, 0.3613835385891887, 0.35474260068038543, 0.3483238261212514, 0.3470099683025244, 0.34537735685472415, 0.3462139917387816, 0.3436940733298896, 0.35586220182127837, 0.3577018766916162, 0.3422587360090507, 0.3473054411978814, 0.3497241731401236, 0.3372780376194708, 0.3387637536554185, 0.3360146361718202, 0.33153194637414773, 0.3309723094063091, 0.32556764722528975, 0.32636259740669255, 0.32848700513803974, 0.32794203163751473, 0.3228767923198591, 0.32176310385962237, 0.3235199835287039, 0.32674390139917986, 0.3219149625282545, 0.3217130389332454, 0.3195477060086513, 0.31915878598960745, 0.3195061972872844, 0.31997900536377377, 0.31633159402995653, 0.31583856969345625, 0.31733616905780637, 0.31787808183159855, 0.31614717127915326, 0.31641108872617174, 0.3159733167811354, 0.31323218586130397, 0.3146610905697351, 0.3163121698761029, 0.31417854624768504, 0.3186689805288316, 0.31384627952182437, 0.3142911387410662, 0.3129474759848616, 0.3121733623836548, 0.3112328811411299, 0.31266378458522065, 0.3104396294260373, 0.3103723959247424, 0.30984931193454573, 0.30906077721385405, 0.310800468229596, 0.3101699011229641, 0.30897123706526597, 0.30904347962636713, 0.3035217481386477, 0.3086479414734742, 0.31009694718686603, 0.3077253390225219, 0.30614523317518233, 0.30738742073765046, 0.3082819252447745, 0.30551258948435617, 0.30688827587965034, 0.3037811523301368, 0.30538746692101326, 0.30390065146101714, 0.3053430218873932, 0.3062727585077709, 0.30422947761944746, 0.3057526205640179, 0.3061478633411163, 0.30300295613856787, 0.3014430059999289, 0.3046418349336235, 0.30537328739525876, 0.30305382195568725, 0.3051067872375876, 0.3071494096260023, 0.303845374143146, 0.30465549904854206, 0.300477505554147, 0.30321892055814126, 0.30238860562742137, 0.3052987858257645, 0.3021340811950539, 0.3027944995743219, 0.30582231795768994, 0.30381778242426466, 0.30286958576853934, 0.3023866140373221], 'acc': [0.9073576392760064, 0.9261025275729683, 0.93130382092568, 0.9343283428964242, 0.9356921444374793, 0.9368552837520471, 0.9386228944018852, 0.936946095688357, 0.938387882589553, 0.9359128615013462, 0.9343914424713351, 0.9322984582505024, 0.9359444784461906, 0.9365681247138714, 0.9398504450698217, 0.9407844904542286, 0.939006422208566, 0.9380422282632879, 0.9408941363234651, 0.9414902281523622, 0.941463150456658, 0.9420628106146036, 0.9424408265527588, 0.9426097354087367, 0.9421019269726951, 0.9433485604818961, 0.94305688370329, 0.9440824849795835, 0.9436497237363181, 0.942572381953416, 0.9428009569815445, 0.9435267743067569, 0.9432794756763277, 0.9442633633443305, 0.9452486506836812, 0.9456791099422933, 0.9454981858377286, 0.9451258593942917, 0.9464630133432785, 0.9445344876923175, 0.9454870068710608, 0.946626817091125, 0.9462757200029129, 0.9469109508510736, 0.9466396893835919, 0.9470999823750694, 0.945146547921301, 0.944955528776145, 0.9474588482293126, 0.9467046424883221, 0.9461380006635025, 0.947981879875083, 0.9479391050195275, 0.9480672481615797, 0.9486417704707151, 0.9487627332551811, 0.9492391048656527, 0.9490280779223946, 0.94916762632532, 0.9492086620682194, 0.9494935415065582, 0.9495702822250505, 0.9495612723217968, 0.9494354321639825, 0.9497745352183219, 0.9497893060219174, 0.9496173718112736, 0.9502688860933233, 0.9505138109336417, 0.9502871781925696, 0.9506146749872022, 0.9506728745040732, 0.9502118360929216, 0.9505971712133312, 0.9507233504008598, 0.9505547347524598, 0.9508667829016251, 0.9510558374160679, 0.9506024570992868, 0.9508103901575312, 0.9508865218820146, 0.9498814497048584, 0.9509009095624529, 0.9511329627570945, 0.9510155018175379, 0.9511281970549914, 0.9511124793507355, 0.951100351413679, 0.95096251866257, 0.9510588184597343, 0.951468003064297, 0.9513820009764522, 0.9514932505675435, 0.9514372645897238, 0.9515119724176311, 0.9515853278016249, 0.951592871243636, 0.9516175780186253, 0.951494108893244, 0.9516246240790737, 0.9517438919207654, 0.9517355824793962, 0.9518811830381111, 0.9517717387909789, 0.9517922696246895, 0.9519514200472674, 0.9518381821000271, 0.9520348927802302, 0.9517883846977824, 0.9518935356406379, 0.9520149281230259, 0.9519405336649069, 0.9520931598690237, 0.9520120361541119, 0.9521356407173336, 0.9519883462036692, 0.9520552413543795, 0.9519890452720151, 0.9520446037039597, 0.9521117925914396, 0.9519495912483484, 0.9519445544177286, 0.952091422614419, 0.9519446457033269, 0.9520734441789566, 0.9520993247544424, 0.9521959188587275, 0.9520956892533051, 0.9520560312555706, 0.9521256137954962, 0.9521583379567393, 0.952268617668483], 'mDice': [0.35069683311170663, 0.4689128998381459, 0.5041746832779482, 0.5225028980586189, 0.5345485868204201, 0.5442277320604699, 0.5543017177653757, 0.5354642570342163, 0.5530871150555243, 0.5387646576715455, 0.5313353021522121, 0.5213963747365571, 0.5392337540991269, 0.5382102430748777, 0.561909855117668, 0.5673134764752892, 0.5429144000394394, 0.5487072070410257, 0.5714253021219113, 0.5696821780885432, 0.5707903153164636, 0.5738310919624585, 0.5764716143490259, 0.5800769829951153, 0.5794259912283025, 0.5859690915553941, 0.5835235291794112, 0.5902625185575219, 0.5846257681522988, 0.5778334695782266, 0.5780722615551147, 0.5900340449637159, 0.586422228773188, 0.5920691320008753, 0.5990696850555164, 0.6041665202216353, 0.5985388961095387, 0.5970896861222036, 0.6064109476322699, 0.5950252318671555, 0.6013745284590881, 0.6088539868449158, 0.6088627556248319, 0.609710438130134, 0.6107621722182802, 0.6130773439743209, 0.5998100347745651, 0.596889651655457, 0.6159758780486393, 0.6096263136091794, 0.6086507772827826, 0.6207212573520952, 0.6196942211808544, 0.6208274918946921, 0.6253848639926438, 0.6281898149174441, 0.632640060099481, 0.6309019400427371, 0.6268560697726153, 0.6301277030722047, 0.6338496560897302, 0.6361092912253513, 0.6320479329437667, 0.6323922513633646, 0.6354656490957853, 0.6364134216409728, 0.6383497414238956, 0.6404899493164126, 0.6396455651405849, 0.6372413744620227, 0.6410967168269854, 0.6439608842343213, 0.6400421822613203, 0.6421572419740256, 0.644208385233603, 0.6436700221113262, 0.6419674593444828, 0.6460883498171254, 0.64347582613327, 0.6434125992289141, 0.6458968925701862, 0.6393864003055204, 0.6458948946121448, 0.6453786909262715, 0.6451534007832039, 0.6473858593612589, 0.6472846713775924, 0.6461945992491237, 0.648441213733336, 0.6489580158088009, 0.6505427380349964, 0.6508620297860568, 0.6507053790071771, 0.6495397702334277, 0.651780413660505, 0.6495886731044279, 0.6538911582603485, 0.6512698484287938, 0.6514671252796232, 0.6518859822709873, 0.6526838736185088, 0.6512005386843394, 0.65196703038202, 0.6532135292471684, 0.6510492287421116, 0.6546256659745205, 0.6551960028404431, 0.6570184128846809, 0.6546579480841513, 0.6531930666615698, 0.656608001060642, 0.6540586274792318, 0.6528043677359511, 0.6545462228260038, 0.6560143017728877, 0.6532710296486542, 0.654232580498077, 0.6562550914432213, 0.6535486014411214, 0.6552380663562811, 0.6550415653654184, 0.654221373084406, 0.65721157772649, 0.6547294369907648, 0.6572454296602962, 0.6556997087488187, 0.6571085313797185, 0.6574532414454027, 0.652207169099309, 0.6547137883578084, 0.6558277126575488, 0.6558121331899702], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]predicting test subjects:  50%|█████     | 2/4 [00:02<00:02,  1.17s/it]predicting test subjects:  75%|███████▌  | 3/4 [00:02<00:01,  1.06s/it]predicting test subjects: 100%|██████████| 4/4 [00:03<00:00,  1.02s/it]predicting test subjects: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]cp: cannot stat '/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_CSFn2_Init_Main_wBiasCorrection_CV_a/sd0/vimp*': No such file or directory
cp: cannot stat '/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_CSFn2_Init_Main_wBiasCorrection_CV_a/sd1/vimp*': No such file or directory
cp: cannot stat '/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_CSFn2_Init_Main_wBiasCorrection_CV_a/sd2/vimp*': No such file or directory

  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.34it/s] 50%|█████     | 2/4 [00:00<00:00,  3.49it/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.64it/s]100%|██████████| 4/4 [00:01<00:00,  3.51it/s]100%|██████████| 4/4 [00:01<00:00,  3.59it/s]

CrossVal ['a']
2020-01-21 04:17:20.832358: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-21 04:17:26.152509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-21 04:17:26.152568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 04:17:26.579097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 04:17:26.579167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 04:17:26.579178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 04:17:26.579638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['b']
TypeExperiment 8
CrossVal ['b']
Traceback (most recent call last):
  File "main.py", line 1905, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1886, in Run_Csfn_with_Best_WMn_architecture
    predict_Thalamus_For_SD0(UserInfoB)
  File "main.py", line 1842, in predict_Thalamus_For_SD0
    Run(UserI, IV)
  File "main.py", line 230, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 99, in Loop_Over_Nuclei
    Flag_Thalmaus_NLayers = check_if_num_Layers_fit(UserI)
  File "main.py", line 49, in check_if_num_Layers_fit
    temp_params = preAnalysis(temp_params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 569, in preAnalysis
    params = find_correctNumLayers(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 501, in find_correctNumLayers
    MinInputSize = func_MinInputSize(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 495, in func_MinInputSize
    inputSizes = params.directories.Test.Input.inputSizes if params.WhichExperiment.TestOnly else np.concatenate((params.directories.Train.Input.inputSizes , params.directories.Test.Input.inputSizes),axis=0)
AttributeError: type object 'Input' has no attribute 'inputSizes'
2020-01-21 04:17:29.748620: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-21 04:17:35.002828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-21 04:17:35.002884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 04:17:35.420627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 04:17:35.420686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 04:17:35.420697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 04:17:35.421151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['c']
TypeExperiment 8
CrossVal ['c']
Traceback (most recent call last):
  File "main.py", line 1905, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1886, in Run_Csfn_with_Best_WMn_architecture
    predict_Thalamus_For_SD0(UserInfoB)
  File "main.py", line 1842, in predict_Thalamus_For_SD0
    Run(UserI, IV)
  File "main.py", line 230, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 99, in Loop_Over_Nuclei
    Flag_Thalmaus_NLayers = check_if_num_Layers_fit(UserI)
  File "main.py", line 49, in check_if_num_Layers_fit
    temp_params = preAnalysis(temp_params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 569, in preAnalysis
    params = find_correctNumLayers(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 501, in find_correctNumLayers
    MinInputSize = func_MinInputSize(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 495, in func_MinInputSize
    inputSizes = params.directories.Test.Input.inputSizes if params.WhichExperiment.TestOnly else np.concatenate((params.directories.Train.Input.inputSizes , params.directories.Test.Input.inputSizes),axis=0)
AttributeError: type object 'Input' has no attribute 'inputSizes'
2020-01-21 04:17:38.399674: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-21 04:17:43.660619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-21 04:17:43.660677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 04:17:44.072170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 04:17:44.072234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 04:17:44.072244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 04:17:44.072682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['d']
TypeExperiment 8
CrossVal ['d']
Traceback (most recent call last):
  File "main.py", line 1905, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1886, in Run_Csfn_with_Best_WMn_architecture
    predict_Thalamus_For_SD0(UserInfoB)
  File "main.py", line 1842, in predict_Thalamus_For_SD0
    Run(UserI, IV)
  File "main.py", line 230, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 99, in Loop_Over_Nuclei
    Flag_Thalmaus_NLayers = check_if_num_Layers_fit(UserI)
  File "main.py", line 49, in check_if_num_Layers_fit
    temp_params = preAnalysis(temp_params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 569, in preAnalysis
    params = find_correctNumLayers(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 501, in find_correctNumLayers
    MinInputSize = func_MinInputSize(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 495, in func_MinInputSize
    inputSizes = params.directories.Test.Input.inputSizes if params.WhichExperiment.TestOnly else np.concatenate((params.directories.Train.Input.inputSizes , params.directories.Test.Input.inputSizes),axis=0)
AttributeError: type object 'Input' has no attribute 'inputSizes'
