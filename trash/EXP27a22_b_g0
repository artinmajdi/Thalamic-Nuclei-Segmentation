2019-07-27 17:19:16.524799: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-27 17:19:19.230995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-07-27 17:19:19.231058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-27 17:19:19.616087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-27 17:19:19.616149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-27 17:19:19.616163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-27 17:19:19.616639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:21,  1.90it/s]Loading train:   5%|▍         | 2/41 [00:00<00:18,  2.07it/s]Loading train:   7%|▋         | 3/41 [00:01<00:16,  2.31it/s]Loading train:  10%|▉         | 4/41 [00:01<00:14,  2.54it/s]Loading train:  12%|█▏        | 5/41 [00:02<00:15,  2.28it/s]Loading train:  15%|█▍        | 6/41 [00:02<00:15,  2.27it/s]Loading train:  17%|█▋        | 7/41 [00:02<00:14,  2.36it/s]Loading train:  20%|█▉        | 8/41 [00:03<00:13,  2.53it/s]Loading train:  22%|██▏       | 9/41 [00:03<00:13,  2.43it/s]Loading train:  24%|██▍       | 10/41 [00:04<00:12,  2.45it/s]Loading train:  27%|██▋       | 11/41 [00:04<00:12,  2.41it/s]Loading train:  29%|██▉       | 12/41 [00:04<00:10,  2.64it/s]Loading train:  32%|███▏      | 13/41 [00:05<00:10,  2.72it/s]Loading train:  34%|███▍      | 14/41 [00:05<00:10,  2.54it/s]Loading train:  37%|███▋      | 15/41 [00:05<00:09,  2.70it/s]Loading train:  39%|███▉      | 16/41 [00:06<00:09,  2.61it/s]Loading train:  41%|████▏     | 17/41 [00:06<00:10,  2.35it/s]Loading train:  44%|████▍     | 18/41 [00:07<00:10,  2.27it/s]Loading train:  46%|████▋     | 19/41 [00:07<00:08,  2.47it/s]Loading train:  49%|████▉     | 20/41 [00:07<00:07,  2.72it/s]Loading train:  51%|█████     | 21/41 [00:08<00:06,  2.87it/s]Loading train:  54%|█████▎    | 22/41 [00:08<00:06,  2.92it/s]Loading train:  56%|█████▌    | 23/41 [00:08<00:06,  3.00it/s]Loading train:  59%|█████▊    | 24/41 [00:09<00:06,  2.77it/s]Loading train:  61%|██████    | 25/41 [00:09<00:06,  2.66it/s]Loading train:  63%|██████▎   | 26/41 [00:10<00:05,  2.54it/s]Loading train:  66%|██████▌   | 27/41 [00:10<00:05,  2.51it/s]Loading train:  68%|██████▊   | 28/41 [00:10<00:05,  2.47it/s]Loading train:  71%|███████   | 29/41 [00:11<00:04,  2.54it/s]Loading train:  73%|███████▎  | 30/41 [00:11<00:04,  2.46it/s]Loading train:  76%|███████▌  | 31/41 [00:12<00:04,  2.45it/s]Loading train:  78%|███████▊  | 32/41 [00:12<00:03,  2.59it/s]Loading train:  80%|████████  | 33/41 [00:12<00:03,  2.61it/s]Loading train:  83%|████████▎ | 34/41 [00:13<00:02,  2.46it/s]Loading train:  85%|████████▌ | 35/41 [00:13<00:02,  2.19it/s]Loading train:  88%|████████▊ | 36/41 [00:14<00:02,  2.17it/s]Loading train:  90%|█████████ | 37/41 [00:14<00:01,  2.20it/s]Loading train:  93%|█████████▎| 38/41 [00:15<00:01,  2.24it/s]Loading train:  95%|█████████▌| 39/41 [00:15<00:00,  2.40it/s]Loading train:  98%|█████████▊| 40/41 [00:15<00:00,  2.46it/s]Loading train: 100%|██████████| 41/41 [00:16<00:00,  2.43it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:  24%|██▍       | 10/41 [00:00<00:00, 97.15it/s]concatenating: train:  51%|█████     | 21/41 [00:00<00:00, 98.36it/s]concatenating: train:  83%|████████▎ | 34/41 [00:00<00:00, 104.73it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 116.67it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:05,  1.98it/s]Loading test:  18%|█▊        | 2/11 [00:00<00:04,  2.02it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:03,  2.20it/s]Loading test:  36%|███▋      | 4/11 [00:01<00:02,  2.40it/s]Loading test:  45%|████▌     | 5/11 [00:02<00:02,  2.49it/s]Loading test:  55%|█████▍    | 6/11 [00:02<00:01,  2.60it/s]Loading test:  64%|██████▎   | 7/11 [00:02<00:01,  2.68it/s]Loading test:  73%|███████▎  | 8/11 [00:03<00:01,  2.76it/s]Loading test:  82%|████████▏ | 9/11 [00:03<00:00,  2.82it/s]Loading test:  91%|█████████ | 10/11 [00:03<00:00,  2.66it/s]Loading test: 100%|██████████| 11/11 [00:04<00:00,  2.40it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  36%|███▋      | 4/11 [00:00<00:00, 32.75it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 80.48it/s]
Loading trainS:   0%|          | 0/41 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/41 [00:00<00:14,  2.70it/s]Loading trainS:   5%|▍         | 2/41 [00:00<00:16,  2.40it/s]Loading trainS:   7%|▋         | 3/41 [00:01<00:15,  2.46it/s]Loading trainS:  10%|▉         | 4/41 [00:01<00:14,  2.58it/s]Loading trainS:  12%|█▏        | 5/41 [00:02<00:15,  2.30it/s]Loading trainS:  15%|█▍        | 6/41 [00:02<00:15,  2.22it/s]Loading trainS:  17%|█▋        | 7/41 [00:03<00:14,  2.30it/s]Loading trainS:  20%|█▉        | 8/41 [00:03<00:13,  2.44it/s]Loading trainS:  22%|██▏       | 9/41 [00:03<00:13,  2.31it/s]Loading trainS:  24%|██▍       | 10/41 [00:04<00:12,  2.41it/s]Loading trainS:  27%|██▋       | 11/41 [00:04<00:11,  2.52it/s]Loading trainS:  29%|██▉       | 12/41 [00:04<00:10,  2.76it/s]Loading trainS:  32%|███▏      | 13/41 [00:05<00:10,  2.65it/s]Loading trainS:  34%|███▍      | 14/41 [00:05<00:10,  2.61it/s]Loading trainS:  37%|███▋      | 15/41 [00:06<00:10,  2.58it/s]Loading trainS:  39%|███▉      | 16/41 [00:06<00:10,  2.42it/s]Loading trainS:  41%|████▏     | 17/41 [00:07<00:10,  2.18it/s]Loading trainS:  44%|████▍     | 18/41 [00:07<00:09,  2.30it/s]Loading trainS:  46%|████▋     | 19/41 [00:07<00:09,  2.36it/s]Loading trainS:  49%|████▉     | 20/41 [00:08<00:08,  2.54it/s]Loading trainS:  51%|█████     | 21/41 [00:08<00:07,  2.75it/s]Loading trainS:  54%|█████▎    | 22/41 [00:08<00:06,  2.83it/s]Loading trainS:  56%|█████▌    | 23/41 [00:09<00:06,  2.93it/s]Loading trainS:  59%|█████▊    | 24/41 [00:09<00:05,  2.91it/s]Loading trainS:  61%|██████    | 25/41 [00:09<00:05,  2.91it/s]Loading trainS:  63%|██████▎   | 26/41 [00:10<00:05,  2.77it/s]Loading trainS:  66%|██████▌   | 27/41 [00:10<00:05,  2.58it/s]Loading trainS:  68%|██████▊   | 28/41 [00:11<00:04,  2.67it/s]Loading trainS:  71%|███████   | 29/41 [00:11<00:04,  2.75it/s]Loading trainS:  73%|███████▎  | 30/41 [00:11<00:04,  2.63it/s]Loading trainS:  76%|███████▌  | 31/41 [00:12<00:03,  2.55it/s]Loading trainS:  78%|███████▊  | 32/41 [00:12<00:03,  2.59it/s]Loading trainS:  80%|████████  | 33/41 [00:13<00:03,  2.50it/s]Loading trainS:  83%|████████▎ | 34/41 [00:13<00:02,  2.34it/s]Loading trainS:  85%|████████▌ | 35/41 [00:14<00:02,  2.19it/s]Loading trainS:  88%|████████▊ | 36/41 [00:14<00:02,  2.13it/s]Loading trainS:  90%|█████████ | 37/41 [00:14<00:01,  2.24it/s]Loading trainS:  93%|█████████▎| 38/41 [00:15<00:01,  2.34it/s]Loading trainS:  95%|█████████▌| 39/41 [00:15<00:00,  2.36it/s]Loading trainS:  98%|█████████▊| 40/41 [00:16<00:00,  2.33it/s]Loading trainS: 100%|██████████| 41/41 [00:16<00:00,  2.37it/s]
Loading testS:   0%|          | 0/11 [00:00<?, ?it/s]Loading testS:   9%|▉         | 1/11 [00:00<00:03,  3.09it/s]Loading testS:  18%|█▊        | 2/11 [00:00<00:03,  2.94it/s]Loading testS:  27%|██▋       | 3/11 [00:01<00:02,  3.03it/s]Loading testS:  36%|███▋      | 4/11 [00:01<00:02,  2.93it/s]Loading testS:  45%|████▌     | 5/11 [00:01<00:02,  2.89it/s]Loading testS:  55%|█████▍    | 6/11 [00:02<00:01,  2.68it/s]Loading testS:  64%|██████▎   | 7/11 [00:02<00:01,  2.62it/s]Loading testS:  73%|███████▎  | 8/11 [00:02<00:01,  2.60it/s]Loading testS:  82%|████████▏ | 9/11 [00:03<00:00,  2.70it/s]Loading testS:  91%|█████████ | 10/11 [00:03<00:00,  2.41it/s]Loading testS: 100%|██████████| 11/11 [00:04<00:00,  2.26it/s]---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 10 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 10 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 128, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 128, 10) 100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 128, 10) 40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 128, 10) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 128, 10) 910         activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 128, 10) 40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 128, 10) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 64, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 64, 10)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 64, 20)   1820        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 64, 20)   80          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 64, 20)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 64, 20)   3620        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 64, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 64, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 64, 30)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 32, 30)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 32, 30)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 32, 40)   10840       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 32, 40)   160         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 32, 40)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 32, 40)   14440       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 32, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 32, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 32, 70)   0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 32, 70)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 64, 20)   5620        dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 64, 50)   0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 64, 20)   9020        concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 64, 20)   80          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 64, 20)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 64, 20)   3620        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 64, 20)   80          conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 64, 20)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 64, 70)   0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 64, 70)   0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 128, 10) 2810        dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 128, 20) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 128, 10) 1810        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 128, 10) 40          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 128, 10) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 128, 10) 910         activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 128, 10) 40          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 128, 10) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 128, 30) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 128, 30) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 128, 2)  62          dropout_5[0][0]                  
==================================================================================================
Total params: 56,382
Trainable params: 55,982
Non-trainable params: 400
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97831385 0.02168615]
Train on 2690 samples, validate on 698 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 1.5248 - acc: 0.5516 - mDice: 0.0641 - val_loss: 1.3862 - val_acc: 0.6315 - val_mDice: 0.0794

Epoch 00001: val_mDice improved from -inf to 0.07939, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 1.0462 - acc: 0.9074 - mDice: 0.1538 - val_loss: 1.4320 - val_acc: 0.4404 - val_mDice: 0.0615

Epoch 00002: val_mDice did not improve from 0.07939
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.4747 - acc: 0.9862 - mDice: 0.4215 - val_loss: 0.1765 - val_acc: 0.9927 - val_mDice: 0.7314

Epoch 00003: val_mDice improved from 0.07939 to 0.73140, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.2659 - acc: 0.9892 - mDice: 0.6061 - val_loss: 0.1097 - val_acc: 0.9913 - val_mDice: 0.8161

Epoch 00004: val_mDice improved from 0.73140 to 0.81608, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.1840 - acc: 0.9904 - mDice: 0.7061 - val_loss: 0.2190 - val_acc: 0.9858 - val_mDice: 0.6558

Epoch 00005: val_mDice did not improve from 0.81608
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.1416 - acc: 0.9912 - mDice: 0.7661 - val_loss: 0.3403 - val_acc: 0.9826 - val_mDice: 0.5182

Epoch 00006: val_mDice did not improve from 0.81608
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.1226 - acc: 0.9916 - mDice: 0.7950 - val_loss: 0.2156 - val_acc: 0.9860 - val_mDice: 0.6609

Epoch 00007: val_mDice did not improve from 0.81608
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.1138 - acc: 0.9918 - mDice: 0.8086 - val_loss: 0.1395 - val_acc: 0.9891 - val_mDice: 0.7664

Epoch 00008: val_mDice did not improve from 0.81608
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.1069 - acc: 0.9922 - mDice: 0.8195 - val_loss: 0.2293 - val_acc: 0.9859 - val_mDice: 0.6445

Epoch 00009: val_mDice did not improve from 0.81608
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.1008 - acc: 0.9925 - mDice: 0.8291 - val_loss: 0.0940 - val_acc: 0.9920 - val_mDice: 0.8385

Epoch 00010: val_mDice improved from 0.81608 to 0.83848, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0959 - acc: 0.9929 - mDice: 0.8371 - val_loss: 0.1283 - val_acc: 0.9902 - val_mDice: 0.7836

Epoch 00011: val_mDice did not improve from 0.83848
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0937 - acc: 0.9932 - mDice: 0.8404 - val_loss: 0.1299 - val_acc: 0.9904 - val_mDice: 0.7814

Epoch 00012: val_mDice did not improve from 0.83848
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0896 - acc: 0.9935 - mDice: 0.8471 - val_loss: 0.1119 - val_acc: 0.9912 - val_mDice: 0.8090

Epoch 00013: val_mDice did not improve from 0.83848
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0878 - acc: 0.9937 - mDice: 0.8498 - val_loss: 0.0712 - val_acc: 0.9940 - val_mDice: 0.8773

Epoch 00014: val_mDice improved from 0.83848 to 0.87728, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0874 - acc: 0.9938 - mDice: 0.8504 - val_loss: 0.0894 - val_acc: 0.9926 - val_mDice: 0.8459

Epoch 00015: val_mDice did not improve from 0.87728
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0865 - acc: 0.9938 - mDice: 0.8517 - val_loss: 0.0741 - val_acc: 0.9939 - val_mDice: 0.8722

Epoch 00016: val_mDice did not improve from 0.87728
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0838 - acc: 0.9941 - mDice: 0.8560 - val_loss: 0.0715 - val_acc: 0.9941 - val_mDice: 0.8764

Epoch 00017: val_mDice did not improve from 0.87728
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0849 - acc: 0.9940 - mDice: 0.8538 - val_loss: 0.0641 - val_acc: 0.9947 - val_mDice: 0.8891

Epoch 00018: val_mDice improved from 0.87728 to 0.88908, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0804 - acc: 0.9943 - mDice: 0.8613 - val_loss: 0.0594 - val_acc: 0.9952 - val_mDice: 0.8972

Epoch 00019: val_mDice improved from 0.88908 to 0.89717, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0787 - acc: 0.9945 - mDice: 0.8639 - val_loss: 0.0593 - val_acc: 0.9951 - val_mDice: 0.8968

Epoch 00020: val_mDice did not improve from 0.89717
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0780 - acc: 0.9946 - mDice: 0.8649 - val_loss: 0.0609 - val_acc: 0.9950 - val_mDice: 0.8936

Epoch 00021: val_mDice did not improve from 0.89717
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0771 - acc: 0.9946 - mDice: 0.8662 - val_loss: 0.0657 - val_acc: 0.9946 - val_mDice: 0.8849

Epoch 00022: val_mDice did not improve from 0.89717
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0769 - acc: 0.9947 - mDice: 0.8663 - val_loss: 0.0630 - val_acc: 0.9948 - val_mDice: 0.8892

Epoch 00023: val_mDice did not improve from 0.89717
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0754 - acc: 0.9948 - mDice: 0.8686 - val_loss: 0.0577 - val_acc: 0.9952 - val_mDice: 0.8982

Epoch 00024: val_mDice improved from 0.89717 to 0.89822, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0763 - acc: 0.9947 - mDice: 0.8668 - val_loss: 0.0579 - val_acc: 0.9952 - val_mDice: 0.8975

Epoch 00025: val_mDice did not improve from 0.89822
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0757 - acc: 0.9948 - mDice: 0.8676 - val_loss: 0.0605 - val_acc: 0.9950 - val_mDice: 0.8926

Epoch 00026: val_mDice did not improve from 0.89822
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0731 - acc: 0.9949 - mDice: 0.8716 - val_loss: 0.0570 - val_acc: 0.9953 - val_mDice: 0.8985

Epoch 00027: val_mDice improved from 0.89822 to 0.89854, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0735 - acc: 0.9949 - mDice: 0.8705 - val_loss: 0.0545 - val_acc: 0.9955 - val_mDice: 0.9024

Epoch 00028: val_mDice improved from 0.89854 to 0.90236, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0734 - acc: 0.9949 - mDice: 0.8704 - val_loss: 0.0578 - val_acc: 0.9952 - val_mDice: 0.8959

Epoch 00029: val_mDice did not improve from 0.90236
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0728 - acc: 0.9950 - mDice: 0.8709 - val_loss: 0.0528 - val_acc: 0.9956 - val_mDice: 0.9038

Epoch 00030: val_mDice improved from 0.90236 to 0.90381, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0713 - acc: 0.9950 - mDice: 0.8732 - val_loss: 0.0550 - val_acc: 0.9954 - val_mDice: 0.8994

Epoch 00031: val_mDice did not improve from 0.90381
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0700 - acc: 0.9951 - mDice: 0.8751 - val_loss: 0.0534 - val_acc: 0.9955 - val_mDice: 0.9017

Epoch 00032: val_mDice did not improve from 0.90381
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0723 - acc: 0.9950 - mDice: 0.8710 - val_loss: 0.0547 - val_acc: 0.9954 - val_mDice: 0.8992

Epoch 00033: val_mDice did not improve from 0.90381
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0690 - acc: 0.9952 - mDice: 0.8763 - val_loss: 0.0523 - val_acc: 0.9956 - val_mDice: 0.9032

Epoch 00034: val_mDice did not improve from 0.90381
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0679 - acc: 0.9952 - mDice: 0.8780 - val_loss: 0.0526 - val_acc: 0.9955 - val_mDice: 0.9027

Epoch 00035: val_mDice did not improve from 0.90381
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0687 - acc: 0.9952 - mDice: 0.8764 - val_loss: 0.0535 - val_acc: 0.9955 - val_mDice: 0.9010

Epoch 00036: val_mDice did not improve from 0.90381
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0675 - acc: 0.9952 - mDice: 0.8783 - val_loss: 0.0536 - val_acc: 0.9955 - val_mDice: 0.9009

Epoch 00037: val_mDice did not improve from 0.90381
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0673 - acc: 0.9953 - mDice: 0.8787 - val_loss: 0.0518 - val_acc: 0.9956 - val_mDice: 0.9039

Epoch 00038: val_mDice improved from 0.90381 to 0.90391, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0677 - acc: 0.9952 - mDice: 0.8780 - val_loss: 0.0510 - val_acc: 0.9957 - val_mDice: 0.9053

Epoch 00039: val_mDice improved from 0.90391 to 0.90527, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0668 - acc: 0.9953 - mDice: 0.8794 - val_loss: 0.0511 - val_acc: 0.9956 - val_mDice: 0.9052

Epoch 00040: val_mDice did not improve from 0.90527
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0667 - acc: 0.9953 - mDice: 0.8795 - val_loss: 0.0517 - val_acc: 0.9956 - val_mDice: 0.9042

Epoch 00041: val_mDice did not improve from 0.90527
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0664 - acc: 0.9953 - mDice: 0.8800 - val_loss: 0.0508 - val_acc: 0.9957 - val_mDice: 0.9056

Epoch 00042: val_mDice improved from 0.90527 to 0.90556, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0671 - acc: 0.9953 - mDice: 0.8789 - val_loss: 0.0507 - val_acc: 0.9957 - val_mDice: 0.9058

Epoch 00043: val_mDice improved from 0.90556 to 0.90578, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0655 - acc: 0.9953 - mDice: 0.8815 - val_loss: 0.0511 - val_acc: 0.9956 - val_mDice: 0.9052

Epoch 00044: val_mDice did not improve from 0.90578
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0655 - acc: 0.9953 - mDice: 0.8815 - val_loss: 0.0509 - val_acc: 0.9957 - val_mDice: 0.9054

Epoch 00045: val_mDice did not improve from 0.90578
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0659 - acc: 0.9953 - mDice: 0.8808 - val_loss: 0.0507 - val_acc: 0.9957 - val_mDice: 0.9058

Epoch 00046: val_mDice improved from 0.90578 to 0.90584, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0650 - acc: 0.9954 - mDice: 0.8822 - val_loss: 0.0511 - val_acc: 0.9957 - val_mDice: 0.9051

Epoch 00047: val_mDice did not improve from 0.90584
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0647 - acc: 0.9954 - mDice: 0.8827 - val_loss: 0.0523 - val_acc: 0.9956 - val_mDice: 0.9030

Epoch 00048: val_mDice did not improve from 0.90584
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0646 - acc: 0.9954 - mDice: 0.8829 - val_loss: 0.0516 - val_acc: 0.9956 - val_mDice: 0.9041

Epoch 00049: val_mDice did not improve from 0.90584
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0649 - acc: 0.9954 - mDice: 0.8823 - val_loss: 0.0500 - val_acc: 0.9957 - val_mDice: 0.9070

Epoch 00050: val_mDice improved from 0.90584 to 0.90698, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0644 - acc: 0.9954 - mDice: 0.8832 - val_loss: 0.0504 - val_acc: 0.9957 - val_mDice: 0.9063

Epoch 00051: val_mDice did not improve from 0.90698
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0639 - acc: 0.9954 - mDice: 0.8840 - val_loss: 0.0505 - val_acc: 0.9957 - val_mDice: 0.9061

Epoch 00052: val_mDice did not improve from 0.90698
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0651 - acc: 0.9954 - mDice: 0.8819 - val_loss: 0.0502 - val_acc: 0.9957 - val_mDice: 0.9066

Epoch 00053: val_mDice did not improve from 0.90698
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0639 - acc: 0.9954 - mDice: 0.8839 - val_loss: 0.0509 - val_acc: 0.9957 - val_mDice: 0.9053

Epoch 00054: val_mDice did not improve from 0.90698
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0639 - acc: 0.9954 - mDice: 0.8839 - val_loss: 0.0502 - val_acc: 0.9957 - val_mDice: 0.9066

Epoch 00055: val_mDice did not improve from 0.90698
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0632 - acc: 0.9955 - mDice: 0.8851 - val_loss: 0.0501 - val_acc: 0.9957 - val_mDice: 0.9068

Epoch 00056: val_mDice did not improve from 0.90698
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0629 - acc: 0.9955 - mDice: 0.8856 - val_loss: 0.0502 - val_acc: 0.9957 - val_mDice: 0.9066

Epoch 00057: val_mDice did not improve from 0.90698
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0637 - acc: 0.9954 - mDice: 0.8842 - val_loss: 0.0502 - val_acc: 0.9957 - val_mDice: 0.9067

Epoch 00058: val_mDice did not improve from 0.90698
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0624 - acc: 0.9955 - mDice: 0.8864 - val_loss: 0.0504 - val_acc: 0.9957 - val_mDice: 0.9063

Epoch 00059: val_mDice did not improve from 0.90698
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0623 - acc: 0.9955 - mDice: 0.8866 - val_loss: 0.0506 - val_acc: 0.9957 - val_mDice: 0.9058

Epoch 00060: val_mDice did not improve from 0.90698
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0625 - acc: 0.9955 - mDice: 0.8862 - val_loss: 0.0504 - val_acc: 0.9957 - val_mDice: 0.9063

Epoch 00061: val_mDice did not improve from 0.90698
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0629 - acc: 0.9955 - mDice: 0.8856 - val_loss: 0.0501 - val_acc: 0.9957 - val_mDice: 0.9068

Epoch 00062: val_mDice did not improve from 0.90698
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0625 - acc: 0.9955 - mDice: 0.8862 - val_loss: 0.0499 - val_acc: 0.9957 - val_mDice: 0.9072

Epoch 00063: val_mDice improved from 0.90698 to 0.90719, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0631 - acc: 0.9955 - mDice: 0.8852 - val_loss: 0.0499 - val_acc: 0.9957 - val_mDice: 0.9072

Epoch 00064: val_mDice did not improve from 0.90719
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0626 - acc: 0.9955 - mDice: 0.8860 - val_loss: 0.0499 - val_acc: 0.9957 - val_mDice: 0.9072

Epoch 00065: val_mDice improved from 0.90719 to 0.90722, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0619 - acc: 0.9955 - mDice: 0.8873 - val_loss: 0.0500 - val_acc: 0.9957 - val_mDice: 0.9070

Epoch 00066: val_mDice did not improve from 0.90722
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0626 - acc: 0.9955 - mDice: 0.8860 - val_loss: 0.0498 - val_acc: 0.9957 - val_mDice: 0.9073

Epoch 00067: val_mDice improved from 0.90722 to 0.90729, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0622 - acc: 0.9955 - mDice: 0.8868 - val_loss: 0.0493 - val_acc: 0.9958 - val_mDice: 0.9082

Epoch 00068: val_mDice improved from 0.90729 to 0.90818, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0622 - acc: 0.9955 - mDice: 0.8867 - val_loss: 0.0499 - val_acc: 0.9957 - val_mDice: 0.9072

Epoch 00069: val_mDice did not improve from 0.90818
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0616 - acc: 0.9955 - mDice: 0.8878 - val_loss: 0.0493 - val_acc: 0.9958 - val_mDice: 0.9082

Epoch 00070: val_mDice improved from 0.90818 to 0.90825, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0618 - acc: 0.9955 - mDice: 0.8873 - val_loss: 0.0498 - val_acc: 0.9957 - val_mDice: 0.9073

Epoch 00071: val_mDice did not improve from 0.90825
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0614 - acc: 0.9955 - mDice: 0.8880 - val_loss: 0.0495 - val_acc: 0.9958 - val_mDice: 0.9078

Epoch 00072: val_mDice did not improve from 0.90825
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0616 - acc: 0.9956 - mDice: 0.8877 - val_loss: 0.0501 - val_acc: 0.9957 - val_mDice: 0.9068

Epoch 00073: val_mDice did not improve from 0.90825
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0611 - acc: 0.9956 - mDice: 0.8886 - val_loss: 0.0499 - val_acc: 0.9958 - val_mDice: 0.9072

Epoch 00074: val_mDice did not improve from 0.90825
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0606 - acc: 0.9956 - mDice: 0.8894 - val_loss: 0.0493 - val_acc: 0.9958 - val_mDice: 0.9081

Epoch 00075: val_mDice did not improve from 0.90825
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0619 - acc: 0.9955 - mDice: 0.8872 - val_loss: 0.0499 - val_acc: 0.9957 - val_mDice: 0.9072

Epoch 00076: val_mDice did not improve from 0.90825
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0609 - acc: 0.9956 - mDice: 0.8890 - val_loss: 0.0498 - val_acc: 0.9957 - val_mDice: 0.9073

Epoch 00077: val_mDice did not improve from 0.90825
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0618 - acc: 0.9955 - mDice: 0.8874 - val_loss: 0.0499 - val_acc: 0.9957 - val_mDice: 0.9072

Epoch 00078: val_mDice did not improve from 0.90825
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0611 - acc: 0.9956 - mDice: 0.8886 - val_loss: 0.0493 - val_acc: 0.9958 - val_mDice: 0.9082

Epoch 00079: val_mDice did not improve from 0.90825
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0608 - acc: 0.9956 - mDice: 0.8890 - val_loss: 0.0498 - val_acc: 0.9958 - val_mDice: 0.9074

Epoch 00080: val_mDice did not improve from 0.90825
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0608 - acc: 0.9956 - mDice: 0.8890 - val_loss: 0.0498 - val_acc: 0.9957 - val_mDice: 0.9073

Epoch 00081: val_mDice did not improve from 0.90825
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0611 - acc: 0.9956 - mDice: 0.8886 - val_loss: 0.0495 - val_acc: 0.9958 - val_mDice: 0.9079

Epoch 00082: val_mDice did not improve from 0.90825
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0601 - acc: 0.9956 - mDice: 0.8902 - val_loss: 0.0493 - val_acc: 0.9958 - val_mDice: 0.9081

Epoch 00083: val_mDice did not improve from 0.90825
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0608 - acc: 0.9956 - mDice: 0.8891 - val_loss: 0.0495 - val_acc: 0.9958 - val_mDice: 0.9079

Epoch 00084: val_mDice did not improve from 0.90825
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0612 - acc: 0.9956 - mDice: 0.8884 - val_loss: 0.0497 - val_acc: 0.9958 - val_mDice: 0.9075

Epoch 00085: val_mDice did not improve from 0.90825
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0605 - acc: 0.9956 - mDice: 0.8895 - val_loss: 0.0496 - val_acc: 0.9958 - val_mDice: 0.9077

Epoch 00086: val_mDice did not improve from 0.90825
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0606 - acc: 0.9956 - mDice: 0.8895 - val_loss: 0.0492 - val_acc: 0.9958 - val_mDice: 0.9083

Epoch 00087: val_mDice improved from 0.90825 to 0.90834, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 88/300

Epoch 00088: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0598 - acc: 0.9956 - mDice: 0.8907 - val_loss: 0.0494 - val_acc: 0.9958 - val_mDice: 0.9080

Epoch 00088: val_mDice did not improve from 0.90834
Epoch 89/300

Epoch 00089: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0599 - acc: 0.9956 - mDice: 0.8906 - val_loss: 0.0492 - val_acc: 0.9958 - val_mDice: 0.9084

Epoch 00089: val_mDice improved from 0.90834 to 0.90840, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 90/300

Epoch 00090: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0604 - acc: 0.9956 - mDice: 0.8897 - val_loss: 0.0494 - val_acc: 0.9958 - val_mDice: 0.9080

Epoch 00090: val_mDice did not improve from 0.90840
Epoch 91/300

Epoch 00091: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0598 - acc: 0.9956 - mDice: 0.8908 - val_loss: 0.0493 - val_acc: 0.9958 - val_mDice: 0.9083

Epoch 00091: val_mDice did not improve from 0.90840
Epoch 92/300

Epoch 00092: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0600 - acc: 0.9956 - mDice: 0.8905 - val_loss: 0.0496 - val_acc: 0.9958 - val_mDice: 0.9077

Epoch 00092: val_mDice did not improve from 0.90840
Epoch 93/300

Epoch 00093: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0602 - acc: 0.9956 - mDice: 0.8900 - val_loss: 0.0495 - val_acc: 0.9958 - val_mDice: 0.9078

Epoch 00093: val_mDice did not improve from 0.90840
Epoch 94/300

Epoch 00094: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0606 - acc: 0.9956 - mDice: 0.8894 - val_loss: 0.0497 - val_acc: 0.9958 - val_mDice: 0.9075

Epoch 00094: val_mDice did not improve from 0.90840
Epoch 95/300

Epoch 00095: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0598 - acc: 0.9956 - mDice: 0.8908 - val_loss: 0.0495 - val_acc: 0.9958 - val_mDice: 0.9078

Epoch 00095: val_mDice did not improve from 0.90840
Epoch 96/300

Epoch 00096: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0601 - acc: 0.9956 - mDice: 0.8903 - val_loss: 0.0494 - val_acc: 0.9958 - val_mDice: 0.9080

Epoch 00096: val_mDice did not improve from 0.90840
Epoch 97/300

Epoch 00097: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0603 - acc: 0.9956 - mDice: 0.8898 - val_loss: 0.0496 - val_acc: 0.9958 - val_mDice: 0.9077

Epoch 00097: val_mDice did not improve from 0.90840
Epoch 98/300

Epoch 00098: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0597 - acc: 0.9956 - mDice: 0.8909 - val_loss: 0.0494 - val_acc: 0.9958 - val_mDice: 0.9081

Epoch 00098: val_mDice did not improve from 0.90840
Epoch 99/300

Epoch 00099: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0604 - acc: 0.9956 - mDice: 0.8897 - val_loss: 0.0494 - val_acc: 0.9958 - val_mDice: 0.9080

Epoch 00099: val_mDice did not improve from 0.90840
Epoch 100/300

Epoch 00100: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0603 - acc: 0.9956 - mDice: 0.8899 - val_loss: 0.0496 - val_acc: 0.9958 - val_mDice: 0.9076

Epoch 00100: val_mDice did not improve from 0.90840
Epoch 101/300

Epoch 00101: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0595 - acc: 0.9957 - mDice: 0.8912 - val_loss: 0.0496 - val_acc: 0.9958 - val_mDice: 0.9077

Epoch 00101: val_mDice did not improve from 0.90840
Epoch 102/300

Epoch 00102: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0596 - acc: 0.9956 - mDice: 0.8911 - val_loss: 0.0494 - val_acc: 0.9958 - val_mDice: 0.9079

Epoch 00102: val_mDice did not improve from 0.90840
Epoch 103/300

Epoch 00103: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0598 - acc: 0.9956 - mDice: 0.8908 - val_loss: 0.0494 - val_acc: 0.9958 - val_mDice: 0.9080

Epoch 00103: val_mDice did not improve from 0.90840
Epoch 104/300

Epoch 00104: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0602 - acc: 0.9956 - mDice: 0.8900 - val_loss: 0.0495 - val_acc: 0.9958 - val_mDice: 0.9079

Epoch 00104: val_mDice did not improve from 0.90840
Epoch 105/300

Epoch 00105: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0596 - acc: 0.9956 - mDice: 0.8911 - val_loss: 0.0493 - val_acc: 0.9958 - val_mDice: 0.9082

Epoch 00105: val_mDice did not improve from 0.90840
Epoch 106/300

Epoch 00106: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0602 - acc: 0.9956 - mDice: 0.8901 - val_loss: 0.0493 - val_acc: 0.9958 - val_mDice: 0.9081

Epoch 00106: val_mDice did not improve from 0.90840
Epoch 107/300

Epoch 00107: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0599 - acc: 0.9957 - mDice: 0.8906 - val_loss: 0.0492 - val_acc: 0.9958 - val_mDice: 0.9084

Epoch 00107: val_mDice did not improve from 0.90840
Epoch 108/300

Epoch 00108: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0599 - acc: 0.9956 - mDice: 0.8905 - val_loss: 0.0493 - val_acc: 0.9958 - val_mDice: 0.9082

Epoch 00108: val_mDice did not improve from 0.90840
Epoch 109/300

Epoch 00109: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0596 - acc: 0.9956 - mDice: 0.8910 - val_loss: 0.0494 - val_acc: 0.9958 - val_mDice: 0.9081

Epoch 00109: val_mDice did not improve from 0.90840
Epoch 110/300

Epoch 00110: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0593 - acc: 0.9957 - mDice: 0.8916 - val_loss: 0.0493 - val_acc: 0.9958 - val_mDice: 0.9082

Epoch 00110: val_mDice did not improve from 0.90840
Epoch 111/300

Epoch 00111: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0593 - acc: 0.9957 - mDice: 0.8915 - val_loss: 0.0492 - val_acc: 0.9958 - val_mDice: 0.9083

Epoch 00111: val_mDice did not improve from 0.90840
Epoch 112/300

Epoch 00112: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0596 - acc: 0.9956 - mDice: 0.8910 - val_loss: 0.0492 - val_acc: 0.9958 - val_mDice: 0.9084

Epoch 00112: val_mDice did not improve from 0.90840
Epoch 113/300

Epoch 00113: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0589 - acc: 0.9957 - mDice: 0.8923 - val_loss: 0.0492 - val_acc: 0.9958 - val_mDice: 0.9084

Epoch 00113: val_mDice improved from 0.90840 to 0.90843, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 114/300

Epoch 00114: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0594 - acc: 0.9956 - mDice: 0.8913 - val_loss: 0.0492 - val_acc: 0.9958 - val_mDice: 0.9084

Epoch 00114: val_mDice did not improve from 0.90843
Epoch 115/300

Epoch 00115: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0596 - acc: 0.9957 - mDice: 0.8911 - val_loss: 0.0493 - val_acc: 0.9958 - val_mDice: 0.9083

Epoch 00115: val_mDice did not improve from 0.90843
Epoch 116/300

Epoch 00116: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0601 - acc: 0.9956 - mDice: 0.8902 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9085

Epoch 00116: val_mDice improved from 0.90843 to 0.90850, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 117/300

Epoch 00117: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0594 - acc: 0.9956 - mDice: 0.8913 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00117: val_mDice improved from 0.90850 to 0.90872, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 118/300

Epoch 00118: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0598 - acc: 0.9956 - mDice: 0.8906 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00118: val_mDice did not improve from 0.90872
Epoch 119/300

Epoch 00119: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0597 - acc: 0.9956 - mDice: 0.8909 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00119: val_mDice did not improve from 0.90872
Epoch 120/300

Epoch 00120: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0598 - acc: 0.9956 - mDice: 0.8907 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00120: val_mDice improved from 0.90872 to 0.90879, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 121/300

Epoch 00121: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0588 - acc: 0.9957 - mDice: 0.8924 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00121: val_mDice did not improve from 0.90879
Epoch 122/300

Epoch 00122: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0595 - acc: 0.9957 - mDice: 0.8912 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00122: val_mDice improved from 0.90879 to 0.90883, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 123/300

Epoch 00123: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0595 - acc: 0.9957 - mDice: 0.8912 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00123: val_mDice did not improve from 0.90883
Epoch 124/300

Epoch 00124: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0595 - acc: 0.9956 - mDice: 0.8912 - val_loss: 0.0492 - val_acc: 0.9958 - val_mDice: 0.9084

Epoch 00124: val_mDice did not improve from 0.90883
Epoch 125/300

Epoch 00125: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0593 - acc: 0.9957 - mDice: 0.8916 - val_loss: 0.0492 - val_acc: 0.9958 - val_mDice: 0.9084

Epoch 00125: val_mDice did not improve from 0.90883
Epoch 126/300

Epoch 00126: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8921 - val_loss: 0.0492 - val_acc: 0.9958 - val_mDice: 0.9084

Epoch 00126: val_mDice did not improve from 0.90883
Epoch 127/300

Epoch 00127: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0594 - acc: 0.9956 - mDice: 0.8914 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00127: val_mDice did not improve from 0.90883
Epoch 128/300

Epoch 00128: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0592 - acc: 0.9957 - mDice: 0.8917 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00128: val_mDice did not improve from 0.90883
Epoch 129/300

Epoch 00129: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0594 - acc: 0.9957 - mDice: 0.8914 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00129: val_mDice did not improve from 0.90883
Epoch 130/300

Epoch 00130: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0595 - acc: 0.9957 - mDice: 0.8912 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00130: val_mDice did not improve from 0.90883
Epoch 131/300

Epoch 00131: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0593 - acc: 0.9957 - mDice: 0.8916 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9085

Epoch 00131: val_mDice did not improve from 0.90883
Epoch 132/300

Epoch 00132: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8921 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00132: val_mDice did not improve from 0.90883
Epoch 133/300

Epoch 00133: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0592 - acc: 0.9957 - mDice: 0.8917 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00133: val_mDice did not improve from 0.90883
Epoch 134/300

Epoch 00134: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0589 - acc: 0.9957 - mDice: 0.8922 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00134: val_mDice did not improve from 0.90883
Epoch 135/300

Epoch 00135: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0596 - acc: 0.9957 - mDice: 0.8910 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00135: val_mDice did not improve from 0.90883
Epoch 136/300

Epoch 00136: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8920 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00136: val_mDice did not improve from 0.90883
Epoch 137/300

Epoch 00137: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0585 - acc: 0.9957 - mDice: 0.8930 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00137: val_mDice did not improve from 0.90883
Epoch 138/300

Epoch 00138: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0598 - acc: 0.9956 - mDice: 0.8907 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00138: val_mDice did not improve from 0.90883
Epoch 139/300

Epoch 00139: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0593 - acc: 0.9957 - mDice: 0.8915 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00139: val_mDice did not improve from 0.90883
Epoch 140/300

Epoch 00140: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0588 - acc: 0.9957 - mDice: 0.8923 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00140: val_mDice did not improve from 0.90883
Epoch 141/300

Epoch 00141: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0596 - acc: 0.9956 - mDice: 0.8911 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00141: val_mDice did not improve from 0.90883
Epoch 142/300

Epoch 00142: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0594 - acc: 0.9956 - mDice: 0.8913 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00142: val_mDice did not improve from 0.90883
Epoch 143/300

Epoch 00143: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0587 - acc: 0.9957 - mDice: 0.8926 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00143: val_mDice did not improve from 0.90883
Epoch 144/300

Epoch 00144: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 4s - loss: 0.0589 - acc: 0.9957 - mDice: 0.8922 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00144: val_mDice did not improve from 0.90883
Epoch 145/300

Epoch 00145: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0592 - acc: 0.9956 - mDice: 0.8918 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00145: val_mDice did not improve from 0.90883
Epoch 146/300

Epoch 00146: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8920 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00146: val_mDice did not improve from 0.90883
Epoch 147/300

Epoch 00147: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0593 - acc: 0.9957 - mDice: 0.8915 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00147: val_mDice did not improve from 0.90883
Epoch 148/300

Epoch 00148: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0586 - acc: 0.9957 - mDice: 0.8927 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00148: val_mDice did not improve from 0.90883
Epoch 149/300

Epoch 00149: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0591 - acc: 0.9957 - mDice: 0.8919 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00149: val_mDice did not improve from 0.90883
Epoch 150/300

Epoch 00150: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0584 - acc: 0.9957 - mDice: 0.8930 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00150: val_mDice did not improve from 0.90883
Epoch 151/300

Epoch 00151: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0592 - acc: 0.9957 - mDice: 0.8917 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00151: val_mDice did not improve from 0.90883
Epoch 152/300

Epoch 00152: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0593 - acc: 0.9957 - mDice: 0.8916 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00152: val_mDice did not improve from 0.90883
Epoch 153/300

Epoch 00153: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0591 - acc: 0.9957 - mDice: 0.8919 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00153: val_mDice did not improve from 0.90883
Epoch 154/300

Epoch 00154: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8920 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00154: val_mDice did not improve from 0.90883
Epoch 155/300

Epoch 00155: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0591 - acc: 0.9957 - mDice: 0.8919 - val_loss: 0.0489 - val_acc: 0.9958 - val_mDice: 0.9089

Epoch 00155: val_mDice improved from 0.90883 to 0.90893, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 156/300

Epoch 00156: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0586 - acc: 0.9957 - mDice: 0.8927 - val_loss: 0.0489 - val_acc: 0.9958 - val_mDice: 0.9089

Epoch 00156: val_mDice did not improve from 0.90893
Epoch 157/300

Epoch 00157: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0592 - acc: 0.9957 - mDice: 0.8917 - val_loss: 0.0489 - val_acc: 0.9958 - val_mDice: 0.9089

Epoch 00157: val_mDice improved from 0.90893 to 0.90893, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 158/300

Epoch 00158: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8920 - val_loss: 0.0489 - val_acc: 0.9958 - val_mDice: 0.9089

Epoch 00158: val_mDice did not improve from 0.90893
Epoch 159/300

Epoch 00159: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0587 - acc: 0.9957 - mDice: 0.8925 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00159: val_mDice did not improve from 0.90893
Epoch 160/300

Epoch 00160: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0588 - acc: 0.9957 - mDice: 0.8924 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00160: val_mDice did not improve from 0.90893
Epoch 161/300

Epoch 00161: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8920 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00161: val_mDice did not improve from 0.90893
Epoch 162/300

Epoch 00162: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 4s - loss: 0.0589 - acc: 0.9957 - mDice: 0.8922 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00162: val_mDice did not improve from 0.90893
Epoch 163/300

Epoch 00163: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0592 - acc: 0.9957 - mDice: 0.8917 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00163: val_mDice did not improve from 0.90893
Epoch 164/300

Epoch 00164: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0594 - acc: 0.9957 - mDice: 0.8914 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00164: val_mDice did not improve from 0.90893
Epoch 165/300

Epoch 00165: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8920 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00165: val_mDice did not improve from 0.90893
Epoch 166/300

Epoch 00166: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8921 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00166: val_mDice did not improve from 0.90893
Epoch 167/300

Epoch 00167: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0586 - acc: 0.9957 - mDice: 0.8928 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00167: val_mDice did not improve from 0.90893
Epoch 168/300

Epoch 00168: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0587 - acc: 0.9957 - mDice: 0.8926 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00168: val_mDice did not improve from 0.90893
Epoch 169/300

Epoch 00169: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8921 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00169: val_mDice did not improve from 0.90893
Epoch 170/300

Epoch 00170: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0595 - acc: 0.9956 - mDice: 0.8912 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00170: val_mDice did not improve from 0.90893
Epoch 171/300

Epoch 00171: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0586 - acc: 0.9957 - mDice: 0.8927 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00171: val_mDice did not improve from 0.90893
Epoch 172/300

Epoch 00172: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0589 - acc: 0.9957 - mDice: 0.8922 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00172: val_mDice did not improve from 0.90893
Epoch 173/300

Epoch 00173: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0602 - acc: 0.9956 - mDice: 0.8899 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00173: val_mDice did not improve from 0.90893
Epoch 174/300

Epoch 00174: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0592 - acc: 0.9957 - mDice: 0.8917 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00174: val_mDice did not improve from 0.90893
Epoch 175/300

Epoch 00175: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0589 - acc: 0.9957 - mDice: 0.8922 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00175: val_mDice did not improve from 0.90893
Epoch 176/300

Epoch 00176: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8920 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00176: val_mDice did not improve from 0.90893
Epoch 177/300

Epoch 00177: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0591 - acc: 0.9957 - mDice: 0.8919 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00177: val_mDice did not improve from 0.90893
Epoch 178/300

Epoch 00178: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0595 - acc: 0.9957 - mDice: 0.8913 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00178: val_mDice did not improve from 0.90893
Epoch 179/300

Epoch 00179: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0589 - acc: 0.9957 - mDice: 0.8922 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9088

Epoch 00179: val_mDice did not improve from 0.90893
Epoch 180/300

Epoch 00180: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 4s - loss: 0.0592 - acc: 0.9957 - mDice: 0.8918 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00180: val_mDice did not improve from 0.90893
Epoch 181/300

Epoch 00181: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8921 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00181: val_mDice did not improve from 0.90893
Epoch 182/300

Epoch 00182: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0587 - acc: 0.9957 - mDice: 0.8925 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00182: val_mDice did not improve from 0.90893
Epoch 183/300

Epoch 00183: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0592 - acc: 0.9957 - mDice: 0.8916 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00183: val_mDice did not improve from 0.90893
Epoch 184/300

Epoch 00184: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0589 - acc: 0.9957 - mDice: 0.8922 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00184: val_mDice did not improve from 0.90893
Epoch 185/300

Epoch 00185: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0589 - acc: 0.9957 - mDice: 0.8922 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00185: val_mDice did not improve from 0.90893
Epoch 186/300

Epoch 00186: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8921 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00186: val_mDice did not improve from 0.90893
Epoch 187/300

Epoch 00187: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0589 - acc: 0.9957 - mDice: 0.8922 - val_loss: 0.0491 - val_acc: 0.9958 - val_mDice: 0.9086

Epoch 00187: val_mDice did not improve from 0.90893
Epoch 188/300

Epoch 00188: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0590 - acc: 0.9957 - mDice: 0.8921 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00188: val_mDice did not improve from 0.90893
Epoch 189/300

Epoch 00189: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0587 - acc: 0.9957 - mDice: 0.8926 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00189: val_mDice did not improve from 0.90893
Epoch 190/300

Epoch 00190: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0594 - acc: 0.9956 - mDice: 0.8914 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00190: val_mDice did not improve from 0.90893
Epoch 191/300

Epoch 00191: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0592 - acc: 0.9957 - mDice: 0.8917 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00191: val_mDice did not improve from 0.90893
Epoch 192/300

Epoch 00192: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0588 - acc: 0.9957 - mDice: 0.8923 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00192: val_mDice did not improve from 0.90893
Epoch 193/300

Epoch 00193: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0590 - acc: 0.9956 - mDice: 0.8920 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00193: val_mDice did not improve from 0.90893
Epoch 194/300

Epoch 00194: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0588 - acc: 0.9957 - mDice: 0.8924 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00194: val_mDice did not improve from 0.90893
Epoch 195/300

Epoch 00195: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0589 - acc: 0.9957 - mDice: 0.8922 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00195: val_mDice did not improve from 0.90893
Epoch 196/300

Epoch 00196: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0592 - acc: 0.9957 - mDice: 0.8918 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00196: val_mDice did not improve from 0.90893
Epoch 197/300

Epoch 00197: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 4s - loss: 0.0584 - acc: 0.9957 - mDice: 0.8930 - val_loss: 0.0490 - val_acc: 0.9958 - val_mDice: 0.9087

Epoch 00197: val_mDice did not improve from 0.90893
Restoring model weights from the end of the best epoch
Epoch 00197: early stopping
{'val_loss': [1.386209471861066, 1.4319637248714197, 0.1765294598474202, 0.10974476998805317, 0.2190318511516112, 0.34030885025218427, 0.21558351059014613, 0.13954223645450733, 0.2293423701152419, 0.09402329118169823, 0.1282942911188377, 0.12993849694387277, 0.11191078728027536, 0.07124274221908056, 0.08944813221863826, 0.07409724200352556, 0.07152970024200428, 0.06413334852516822, 0.05940544061939149, 0.05934987984661728, 0.060930610759808206, 0.06569380659001604, 0.06302634583930232, 0.05769377218874273, 0.05793194541104543, 0.060540808572895545, 0.056993733685901306, 0.054519272433396396, 0.05779332403936837, 0.05278226174947867, 0.05501517053576117, 0.053390115093877456, 0.05472670627869986, 0.052286594134774114, 0.052589680433785674, 0.053514646190114555, 0.053560716801864715, 0.051834518885117205, 0.05102667172991785, 0.05105348167284512, 0.051655515548065946, 0.05083661390879434, 0.050728412040837514, 0.05107187616363979, 0.05091941601097755, 0.05067131151698722, 0.05108593901710046, 0.052274910866531056, 0.05162795101503247, 0.05001730699209566, 0.05038913472803753, 0.05050890122067962, 0.05020640067820221, 0.05092762339123682, 0.050222192365238526, 0.050093101833901636, 0.050241662675039815, 0.050165960132904926, 0.05036699735588877, 0.05063908723287049, 0.05037042333138706, 0.0501198534744346, 0.04989052224466657, 0.04989288661917164, 0.0498656830443523, 0.04998370658147301, 0.049817922495753855, 0.0493263650290604, 0.04986785494196722, 0.04928928955236615, 0.04983708982950978, 0.04952388585553128, 0.05010528690621983, 0.04985582890431997, 0.04934296253161308, 0.049898789677886364, 0.049843720675454097, 0.04989794995048668, 0.04929168298200072, 0.04977305341287807, 0.049787282580803324, 0.049502822936093566, 0.04933784644847954, 0.04949457489786654, 0.049691661199645534, 0.04958216707224832, 0.049231331646015446, 0.04944738715801676, 0.049195196661020076, 0.04940187593756569, 0.049260904857712694, 0.04958910314649087, 0.04953081935729543, 0.04972019883401073, 0.049516841384947816, 0.04943649930863463, 0.04955957214051809, 0.04936275993044192, 0.04940200943958793, 0.04962514474304836, 0.04959488919479799, 0.04944551169317226, 0.04943755676804436, 0.04945543598394339, 0.049313516214618026, 0.04933953741004952, 0.04921529598982423, 0.049296887906369644, 0.0493785532613026, 0.04930155646365488, 0.04922874689230263, 0.0492169228315695, 0.04917367234996875, 0.04918117138068792, 0.049252172521072675, 0.04913535193249284, 0.0490104439735071, 0.04908210243015713, 0.04905254726850542, 0.04897504879487278, 0.04910080503127978, 0.04895294095819181, 0.049091557398395755, 0.04917731233218679, 0.049188043516566894, 0.04916728180125996, 0.04907781091708167, 0.0490729504806606, 0.049058494102049696, 0.049084416347793314, 0.04912250215737389, 0.04909168472006533, 0.049104292596821114, 0.049062662787287145, 0.04910498217047115, 0.04900737074777527, 0.049089165228314934, 0.04907349979552977, 0.04898982833441144, 0.04899724846156074, 0.049020673609992495, 0.048989530695458194, 0.04909867520062493, 0.04896798732031382, 0.04903427612858379, 0.049051550342394494, 0.04896703153250894, 0.0490401084281312, 0.049066511076210566, 0.04906541183804373, 0.049008857693661936, 0.04897326908813507, 0.048985064510031893, 0.04896187465264326, 0.048894924151692484, 0.048922834314811536, 0.04889189911990248, 0.04893100465565151, 0.048950323285294806, 0.04900432815695219, 0.049024184573345675, 0.04897902396782421, 0.048983013422834155, 0.04897857062454552, 0.04897265590962161, 0.048994544103613556, 0.049009660755238764, 0.04900353382685806, 0.048987006421673265, 0.048982433665191547, 0.048999450225543154, 0.04901890229316359, 0.04904626545771145, 0.04902476830178482, 0.0490368941964256, 0.04902616131485021, 0.04900945228842063, 0.048989983376937475, 0.048975920117857806, 0.049025996740277654, 0.04904105767266115, 0.049063599116054166, 0.049069272530369225, 0.04906141653358082, 0.049053601771166126, 0.049047731599613044, 0.04905110562385666, 0.049036082648872305, 0.049029868875328654, 0.04902531796890207, 0.04901731704871429, 0.0490113636612209, 0.04901420179871912, 0.04903236684792363, 0.049026874745302006, 0.04902232763888501, 0.04901093799610193], 'val_acc': [0.6314812557061968, 0.4403638893519568, 0.9926537230909724, 0.9912627373520487, 0.9857924230460793, 0.9825959458392124, 0.9859685101959972, 0.9891036181873442, 0.9858894911741459, 0.9920370763215772, 0.9902220197598367, 0.9903785205502224, 0.9911588334763972, 0.9940495371476968, 0.9926191942398049, 0.9938750842580822, 0.9941034691040017, 0.9947167648626946, 0.9951900409422495, 0.9950992439401184, 0.994954899081528, 0.9945698163570852, 0.9947799623524903, 0.9952036342169972, 0.995158969160479, 0.9949795808696473, 0.9952829579227633, 0.9954904123158714, 0.9951845400654484, 0.9955697126238258, 0.9953650741659126, 0.9954950343230392, 0.9953729872375642, 0.9955670107743119, 0.995534033180991, 0.9954685978698185, 0.9954700505835962, 0.9955826538340069, 0.9956591631416605, 0.9956478766862165, 0.9955827496454845, 0.9956668900555389, 0.9956741221996297, 0.9956456485996957, 0.995661474059851, 0.9956797055665265, 0.9956521204002948, 0.9955718416539168, 0.9956064781008613, 0.9957213008984453, 0.9956984429441413, 0.9956872405158756, 0.9957270857939405, 0.9956635110357771, 0.9957101012027708, 0.9957194794556473, 0.9957054728764859, 0.9957173490592607, 0.9956927326825423, 0.9956796099258357, 0.9957056740635104, 0.9957233327507631, 0.9957314352251397, 0.995737232758869, 0.9957332711848284, 0.9957269901532498, 0.9957308647967341, 0.995782867202103, 0.9957362558575278, 0.9957830456744634, 0.9957433935564364, 0.9957614445413081, 0.9957286302203093, 0.9957533425792924, 0.9957850891402936, 0.9957429114248828, 0.9957413611917578, 0.9957440818278359, 0.9957818049072877, 0.9957521771291608, 0.9957494793785335, 0.9957769806883738, 0.9957818974738135, 0.9957714808362944, 0.9957600861019255, 0.9957654966324312, 0.995791262405308, 0.9957737961949455, 0.9957914596642328, 0.9957747539681486, 0.995791923350796, 0.9957624993214976, 0.9957761004524449, 0.9957622221342813, 0.9957707995671597, 0.995777941706529, 0.9957662580006443, 0.9957854819502735, 0.9957822810612982, 0.9957659820089367, 0.9957687835980281, 0.9957802591146234, 0.9957820031909341, 0.9957835349790688, 0.9957931897360137, 0.9957882671438179, 0.9957961738963523, 0.9957847759170314, 0.9957801633031457, 0.9957866243441672, 0.9957918345415832, 0.9957947386028432, 0.9957995575273617, 0.9957986871970759, 0.9957910601935619, 0.9958014814423285, 0.9958103644130565, 0.9958061111349089, 0.9958080488836185, 0.9958131639527045, 0.9958058300195929, 0.9958146221316646, 0.9958085316983198, 0.9957995561610662, 0.9957960873073698, 0.9957980098560409, 0.9958015871594492, 0.9958022566442845, 0.9958053478880393, 0.995806513167384, 0.9958052428540665, 0.9958068888986691, 0.9958070859868069, 0.9958111264644175, 0.9958073723965183, 0.9958092930665331, 0.9958037003063541, 0.9958044866094616, 0.9958114251707891, 0.9958072796592057, 0.9958059336872702, 0.995809685876513, 0.9958031373925742, 0.9958124867824565, 0.9958063949828162, 0.9958049654960632, 0.9958090092186258, 0.9958061203574041, 0.995807471452948, 0.9958069886382467, 0.9958100780033451, 0.9958140335998426, 0.9958128671249893, 0.9958135567626843, 0.9958168398001815, 0.995814905296424, 0.995817410570161, 0.9958158558965754, 0.9958143995962717, 0.9958105575730944, 0.9958088240855745, 0.9958124867824565, 0.9958123007954705, 0.9958121964446454, 0.9958131567796527, 0.9958115197867582, 0.9958093021182413, 0.9958102691139397, 0.9958100727089496, 0.9958105574023075, 0.9958097823711385, 0.9958092010123681, 0.9958076685410857, 0.9958089284363996, 0.9958086281929454, 0.9958079385552502, 0.9958084317879554, 0.9958108530345141, 0.9958119079854905, 0.9958094933996228, 0.995809489471523, 0.9958080423937145, 0.9958070899149066, 0.9958067922332567, 0.9958064986504934, 0.9958066050507619, 0.9958071753083806, 0.9958079563170927, 0.9958074622304528, 0.9958071831645802, 0.9958094933996228, 0.9958087248583578, 0.995809489471523, 0.9958078517954807, 0.9958081506726394, 0.9958083424663817, 0.9958087195639624], 'val_mDice': [0.07939222950306868, 0.061512235456004864, 0.7314001224444042, 0.8160777073193416, 0.655839221183755, 0.5182399019641658, 0.6609150681932882, 0.7664421002639399, 0.6444970512800026, 0.8384804694905322, 0.783612160935443, 0.7813653341337057, 0.8090355199865762, 0.8772764214471964, 0.8459114213727607, 0.8722090926074708, 0.8764347681684959, 0.8890754864004075, 0.8971738063845047, 0.8968471573895233, 0.8936100339138063, 0.884877275088455, 0.8891875559416064, 0.8982215692798866, 0.8975351592872751, 0.8926263531846098, 0.8985408964676297, 0.9023601545986951, 0.8959110746410993, 0.9038142215215033, 0.8994028530694693, 0.9017273696582433, 0.8991750912543354, 0.903246218119788, 0.9026692045110686, 0.9009566798934281, 0.9008947954136868, 0.903910762942623, 0.9052674223836992, 0.9052106770540035, 0.9041672864752718, 0.9055551889974272, 0.9057796667503423, 0.905156361510214, 0.9054231030551615, 0.9058429204290439, 0.9051394334495239, 0.9030032291111768, 0.9041453343749388, 0.9069807522617985, 0.906290407679484, 0.906098815294938, 0.9066051478713836, 0.9053442963214864, 0.9065896091625137, 0.9068150762842173, 0.9065616491189318, 0.9066863960044773, 0.9063198967786095, 0.9058482001367476, 0.9063231531733428, 0.9067693622542657, 0.9071946161182699, 0.9071681113502699, 0.9072248778575471, 0.9070292393594211, 0.9072930829914388, 0.9081841126553992, 0.9072244232226921, 0.9082452345714187, 0.9072669889660483, 0.9078312700662367, 0.9067955461135908, 0.9072319559518109, 0.9081311029486123, 0.9071536549182881, 0.9072508770961816, 0.9071577717376643, 0.9082281587800187, 0.9073746474219598, 0.9073383056673416, 0.9078509701698763, 0.908148909025001, 0.9078749578798397, 0.9075171846714949, 0.9077130651747258, 0.9083368434946995, 0.9079554839257183, 0.9083984016005836, 0.9080228381990362, 0.9082796978403982, 0.9076949233311978, 0.9077990480343728, 0.9074662831587914, 0.907821571587833, 0.9079648567134125, 0.9077479943504989, 0.9080976384417034, 0.9080235696795335, 0.907624622129096, 0.9076806361490812, 0.9079465951482341, 0.9079609065820973, 0.9079236615353122, 0.908180093526157, 0.9081331178930222, 0.9083605313027827, 0.9082143750778242, 0.9080692041227674, 0.9082046928241806, 0.9083335172481045, 0.9083539930660609, 0.908429385597863, 0.9084183399520153, 0.9082904285551142, 0.9084992772528641, 0.9087217294725785, 0.908590444179524, 0.9086443271199748, 0.908785212176577, 0.9085614518996296, 0.9088263357949462, 0.9085771935033935, 0.9084253707382945, 0.9084078444109264, 0.9084447422478465, 0.9086031802745809, 0.9086111442407427, 0.9086360612024892, 0.9085912919659328, 0.9085237059347268, 0.9085783249669225, 0.9085564838781056, 0.9086287925099916, 0.908553617731548, 0.9087264966486518, 0.9085808152114081, 0.9086093142585973, 0.9087594276171359, 0.9087456097575518, 0.90870142051347, 0.9087577161611322, 0.9085637305390869, 0.9087977643340911, 0.9086807537557061, 0.9086493771192338, 0.9088001191445272, 0.9086686444487476, 0.9086208343505859, 0.9086224730513499, 0.9087231582761836, 0.9087860837023716, 0.9087649723861826, 0.90880740320785, 0.9089262914179389, 0.9088761608375178, 0.908930807878773, 0.9088608345876109, 0.9088264408289191, 0.9087301272375877, 0.9086951292346747, 0.9087748474582902, 0.9087678593687478, 0.9087760306702644, 0.9087864001705859, 0.9087472494830374, 0.9087204531817177, 0.9087310368488716, 0.9087601824954451, 0.9087682714776528, 0.9087381743769932, 0.9087037345058255, 0.9086548926837124, 0.9086932010500343, 0.9086723337883935, 0.908691547149232, 0.9087213323929249, 0.9087559363903494, 0.908781389281538, 0.9086925153404389, 0.9086655251959334, 0.9086254937600953, 0.9086152689163186, 0.9086290577421215, 0.9086425914122246, 0.9086529604001852, 0.9086469808075694, 0.9086737851358758, 0.9086848099457159, 0.9086928726267336, 0.9087070619479292, 0.9087175235024154, 0.90871258297759, 0.9086802847747475, 0.9086900467858957, 0.9086979492687565, 0.9087182416615308], 'loss': [1.5248166056813803, 1.0461658097554316, 0.47468243544872807, 0.2658960361130619, 0.18400950264531885, 0.14160700862514042, 0.12260785408068767, 0.11384162074128049, 0.1068763916266452, 0.10084386793100258, 0.09586691886851335, 0.09372961507300019, 0.0896454746506028, 0.08783671567320381, 0.0873857393694633, 0.08652734731542133, 0.08378065557506448, 0.08487756334272903, 0.08036836873753807, 0.07870058233410009, 0.07797588621262724, 0.07711572089270588, 0.07693754521555174, 0.07538776625931042, 0.07631833812561177, 0.07565662774454705, 0.07311671265992976, 0.07352076055392014, 0.07338713727143617, 0.07283005639079779, 0.07128037717262609, 0.06999510465722988, 0.07225016086296521, 0.06896521707782072, 0.06789647837664559, 0.06874537177134624, 0.06751553398525847, 0.06729206246181935, 0.06765461724154569, 0.06679958010916373, 0.06674131845231393, 0.06644156217907442, 0.0670552436802467, 0.06545419974397992, 0.06547159133790593, 0.06588668644871411, 0.06500807641330261, 0.0647372716975478, 0.0645775920174822, 0.06492791579669294, 0.06442257676093552, 0.0638858089327369, 0.0650753495058384, 0.06391202328816666, 0.06391132620187497, 0.0632132253848487, 0.06292169355548448, 0.06374442642450776, 0.06243137832467884, 0.06234082788727541, 0.06251184649239243, 0.06289130625173062, 0.06251502892895702, 0.06311779106416666, 0.06261959159573657, 0.06187970653785649, 0.06261026395353243, 0.06216179031089336, 0.06217023759360207, 0.06158185992597647, 0.06182377946088748, 0.06140370702987267, 0.06159990845892066, 0.061074296144083086, 0.06059325359013887, 0.06192359835131018, 0.06086216842042469, 0.06175555169693156, 0.06109257133809164, 0.060828375910516125, 0.06084970008152568, 0.0610911560739948, 0.0601499948138198, 0.060792945699288496, 0.06116657111392145, 0.060513407573602455, 0.0605700384950106, 0.05983918622745457, 0.0598980627510849, 0.06044545056485332, 0.059764419526526474, 0.059953473536055325, 0.06020167244245129, 0.06058963154162173, 0.059787998121356434, 0.06007489591116799, 0.060313921407917616, 0.05972892707940814, 0.060383808754079844, 0.06026461755088271, 0.05953587616575695, 0.05958161528004147, 0.05975027764596904, 0.06019527228292922, 0.059565177858983714, 0.06015986476965996, 0.059854120237676625, 0.05990718814741723, 0.059612513460966736, 0.05928275983462103, 0.059313506857391625, 0.05964249486041335, 0.05888851593528981, 0.05944486087028865, 0.05960130130657476, 0.060099387922251536, 0.05944224704597076, 0.059845845052300777, 0.05966533343809688, 0.05977891481111041, 0.058810079241773894, 0.05953197354333108, 0.0595137679388532, 0.059479666492974446, 0.05927507744125717, 0.05896073226647306, 0.0594016490261794, 0.05919634269682005, 0.059415028025448104, 0.059486587223731895, 0.0592521726531167, 0.0589701528055074, 0.059193211361821256, 0.05894436011416318, 0.0596083404849453, 0.05903872764465091, 0.05848013791065234, 0.05976419357174391, 0.05933564204764189, 0.05883682426411423, 0.05956553930908331, 0.05942939655322124, 0.058676672677014395, 0.05891293673654914, 0.05915162573848958, 0.05901206359544208, 0.05932585565463318, 0.05863823821996668, 0.05908659366426858, 0.058448624904492534, 0.05920023105512321, 0.059274843938953374, 0.059085912365452506, 0.05902989320872441, 0.05906982820993462, 0.05863339163113704, 0.05918829730230636, 0.059038089359781554, 0.05870689105477918, 0.058788698346641424, 0.05904680111040413, 0.058872094994140825, 0.05922043223806473, 0.05939957438569973, 0.05904961704210721, 0.05896069866961706, 0.05857854303263377, 0.05867446121174607, 0.058951719709045385, 0.05946726127628057, 0.05858434653636691, 0.05889900920559483, 0.06021585044032136, 0.059187792047692055, 0.0589136651208188, 0.059031986835720814, 0.05912110255165614, 0.059478348100274026, 0.05889554482410388, 0.059162021708089624, 0.05895183089995916, 0.058739791821591475, 0.059231015344645456, 0.05891626663367545, 0.05889201552008164, 0.058966639621328684, 0.05890954155682631, 0.05895275389948742, 0.05868522900409415, 0.059384702916038964, 0.05917335006661131, 0.05882963122709976, 0.05898524787567805, 0.05880179963867461, 0.05889390591795116, 0.05916086731125431, 0.058419275558017], 'acc': [0.5516282718864072, 0.9073695241297045, 0.986203394414767, 0.9891650306691024, 0.9903526286210269, 0.991188851874114, 0.9915789520873456, 0.9918292732043781, 0.9921566050735104, 0.9924593827095174, 0.9929305250759904, 0.9931587062803786, 0.9934954731880954, 0.9936606968202556, 0.9937968657362417, 0.9938462427557623, 0.9940964309256316, 0.9940447158086698, 0.9943376387362144, 0.9944735663530995, 0.9945678788490012, 0.9946153013236461, 0.9946701980877987, 0.9947768327915092, 0.9947432676212495, 0.9947708009787208, 0.9949137619436895, 0.9949215025706806, 0.9949163708101861, 0.9949515189380007, 0.9950212738770978, 0.9951301066405711, 0.9949940059707953, 0.9951704680698069, 0.9952278731037693, 0.9951799556225206, 0.9952434729023051, 0.9952790491199848, 0.9952346629369657, 0.9952750697898156, 0.9952855604288746, 0.995289641242045, 0.9952648523571767, 0.9953302486235325, 0.9953458513025901, 0.9953374545813494, 0.9953673524041158, 0.9953675549269609, 0.9953923719523121, 0.9953837596350886, 0.9953973989061263, 0.9954094492369868, 0.9953684842276307, 0.9954310493841491, 0.9954239236377872, 0.9954589685099718, 0.9954850505275797, 0.995430977592681, 0.9954899562778969, 0.9955075604764946, 0.9954880340834976, 0.9954571145617829, 0.9954924474861542, 0.9954581090065626, 0.995474211788532, 0.9955205044338694, 0.9955023773097638, 0.9955188312938222, 0.9954969233739774, 0.9955315261968449, 0.9955240620999531, 0.9955106670085382, 0.995556610445994, 0.9955809951714867, 0.9955846241858812, 0.9955464439764342, 0.995581394456133, 0.9955413153180402, 0.9955630911770363, 0.9955615917546156, 0.9955911605331534, 0.9955657461318828, 0.9956029263570849, 0.9955850279020997, 0.9955626954376476, 0.9956158144766514, 0.9955986930978342, 0.9956433919282651, 0.9956235894483262, 0.9956217798158582, 0.9956216304718783, 0.9956341257325778, 0.9956026489406713, 0.9955886848353985, 0.9956289144253642, 0.995629570519614, 0.9955876064123275, 0.9956454386498406, 0.9956056065718923, 0.995626313757276, 0.995654049859171, 0.9956464898187432, 0.9956206522023368, 0.9956007502336042, 0.9956347988883802, 0.9956346510954506, 0.9956565829457846, 0.995641534656397, 0.9956337022958635, 0.9956546315030095, 0.9956759825958196, 0.9956425882626644, 0.9956663224333724, 0.9956483710211006, 0.9956583128099548, 0.9956299248238035, 0.9956181000599631, 0.9956318627502838, 0.9956383018245484, 0.9956425096022595, 0.9956821657467952, 0.9956542767556626, 0.9956630938115173, 0.9956491270473012, 0.9956500461553552, 0.9956711260359526, 0.995639082002817, 0.9956553509687402, 0.995660636283208, 0.9956566095352173, 0.9956588612170024, 0.995699848384219, 0.9956665672777311, 0.9956930844757194, 0.9956520465670022, 0.9956898757959387, 0.9957044793770659, 0.9956456677621182, 0.9956501520699285, 0.9956949612465047, 0.9956428927116677, 0.9956382798882665, 0.9956983954933053, 0.995682046315927, 0.9956487446026288, 0.9956613818952142, 0.9956602338964611, 0.9956947893015071, 0.9956832902582161, 0.9956994900916145, 0.9956669986912752, 0.9956579314731754, 0.9956671249910801, 0.9956688495373637, 0.995660811330306, 0.9956863026193526, 0.9956940102311315, 0.995668020611802, 0.995688303917314, 0.9956849461151321, 0.9956768609334102, 0.9956753340352423, 0.9956727787907682, 0.9956561852121885, 0.9956867278286958, 0.995678332880084, 0.995691708251003, 0.9956887566024043, 0.9956705011842862, 0.9956459230206713, 0.9957001690084606, 0.995681216504051, 0.9956366262471366, 0.9956628370019139, 0.9956699277388562, 0.9956888782490585, 0.9956744293297977, 0.9956818945345826, 0.9956620122863458, 0.9956856735576928, 0.9956676725118133, 0.9956844998558214, 0.9956549545646157, 0.9956943448148252, 0.9956910820698649, 0.9956824150227259, 0.9956795336145451, 0.9956712215363315, 0.995701120466991, 0.9956464902619003, 0.9956677757674434, 0.9956932593012388, 0.9956480435279222, 0.9956756081279768, 0.9956692288799357, 0.995665515222514, 0.9957041022502799], 'mDice': [0.06406833189903138, 0.15380904095101977, 0.421491828106593, 0.606111974742776, 0.706110834189064, 0.7661026778274309, 0.7950386008365447, 0.8086057227783486, 0.8194576710130203, 0.8291254440204805, 0.8370748660821454, 0.8403828607172771, 0.8470515874681863, 0.8498478558426896, 0.8504157595918082, 0.8516727344697292, 0.8560303806372291, 0.8538469723166143, 0.8612608891880645, 0.8638812412116608, 0.8649213028219996, 0.8661970819682436, 0.8662876785909376, 0.8686212573795957, 0.8667670147126492, 0.8675883430087433, 0.8716141176046492, 0.8705353665972287, 0.8703727349915912, 0.8709318841700218, 0.8732033480498871, 0.8750965406903547, 0.8710328092362358, 0.8763001264249525, 0.8779921669055981, 0.8763761134838969, 0.8783466326259769, 0.8786929105737395, 0.878036239554891, 0.8794282032211473, 0.8794963093938437, 0.8799526788931353, 0.8788836855427483, 0.8815415499378757, 0.8814844768286638, 0.8807773095967601, 0.8822192824462975, 0.8826742743914012, 0.8829359946197737, 0.8822952471258029, 0.8831521941826689, 0.8839977155387623, 0.8819358264646565, 0.8839405347423482, 0.8839227238109121, 0.8850789229665991, 0.8855701536494117, 0.8841750174206872, 0.8864276132175913, 0.8865985287609597, 0.8862407666156726, 0.8855958769312579, 0.886212641421747, 0.8851755939451735, 0.8860120873025802, 0.8872810801608855, 0.8860490410744479, 0.8868082309744172, 0.8867350067790999, 0.8877656634855448, 0.8873201832452228, 0.8880480386067501, 0.8877138965191894, 0.8886146505526007, 0.8894342119808977, 0.8871581751617801, 0.8889825745142969, 0.8874464245533854, 0.8885684842070682, 0.889015590169616, 0.8889700466815423, 0.8885530574614231, 0.8901737753343405, 0.8890745633153667, 0.8884015887643325, 0.8895376486406007, 0.8894598125081966, 0.8906832665315791, 0.8905525865607988, 0.8896603947678463, 0.8907852706856001, 0.8905135510579361, 0.8900193283992186, 0.8893604748311096, 0.8907634766571584, 0.8902542017206383, 0.889841593774278, 0.8908604381252841, 0.8897135676504511, 0.8899415607346035, 0.8912194586154697, 0.8911018734970944, 0.8907817699652179, 0.8900058331542742, 0.8911219990386395, 0.8900931836504032, 0.8906440296137643, 0.8905317382298438, 0.891040403160464, 0.8915896404631519, 0.8915319001807599, 0.8909915592590673, 0.8922644643092245, 0.891315980471643, 0.8910649897440659, 0.8901542573613305, 0.8912798383422057, 0.8906005762766728, 0.8909011217298117, 0.8906905686545106, 0.8924032890663715, 0.8911615962432663, 0.8911749700631351, 0.891232501618481, 0.8915634179647084, 0.892125598781614, 0.8913680323880845, 0.8917213394739371, 0.8913666537260034, 0.8912162071710182, 0.8916286429508025, 0.8921291174498632, 0.8917056849454859, 0.892176698796368, 0.8910151930546671, 0.8919876375162912, 0.8929804853790312, 0.8907105636862575, 0.8914728124788702, 0.892346387902157, 0.8910721414151245, 0.891294369467129, 0.8926227586863209, 0.8922099533134233, 0.8917677160975658, 0.8920392768533699, 0.8915112667810519, 0.8926685740513429, 0.8919137900203578, 0.8930268453842646, 0.8917167180089702, 0.8915766136353787, 0.8918916283930102, 0.8919783165906885, 0.8919289149759427, 0.892672992771886, 0.8917265064654297, 0.891993132665698, 0.8925285930970345, 0.8924123981628276, 0.891976819827211, 0.8922499090322331, 0.8916668180639415, 0.8913893854751019, 0.8919605342428923, 0.8921039323824489, 0.8927847754113293, 0.8925897217594558, 0.8921337909858023, 0.8912358860100955, 0.8927492645593381, 0.8922044905144929, 0.8899299158925904, 0.8917034046800606, 0.8921718708201412, 0.8919788036204626, 0.8918535317186973, 0.8912657415113484, 0.8921950880479635, 0.8918026167220786, 0.8921174420300026, 0.8924803492305003, 0.8916276928217438, 0.8922213606231718, 0.8922037355961853, 0.8920725679308952, 0.8922140141845193, 0.8920981604813643, 0.8925978969020915, 0.8913855594773274, 0.8917147687819811, 0.892327456890872, 0.892031418346561, 0.8923755126371703, 0.8922137527217652, 0.8917647055090582, 0.8930493194374454], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:00<00:08,  1.19it/s]predicting test subjects:  18%|█▊        | 2/11 [00:01<00:06,  1.48it/s]predicting test subjects:  27%|██▋       | 3/11 [00:01<00:04,  1.70it/s]predicting test subjects:  36%|███▋      | 4/11 [00:01<00:03,  2.00it/s]predicting test subjects:  45%|████▌     | 5/11 [00:02<00:02,  2.35it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:02<00:01,  2.58it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:02<00:01,  2.84it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:02<00:00,  3.10it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:03<00:00,  3.28it/s]predicting test subjects:  91%|█████████ | 10/11 [00:03<00:00,  3.47it/s]predicting test subjects: 100%|██████████| 11/11 [00:03<00:00,  3.19it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:13,  2.93it/s]predicting train subjects:   5%|▍         | 2/41 [00:00<00:12,  3.03it/s]predicting train subjects:   7%|▋         | 3/41 [00:00<00:11,  3.23it/s]predicting train subjects:  10%|▉         | 4/41 [00:01<00:10,  3.56it/s]predicting train subjects:  12%|█▏        | 5/41 [00:01<00:10,  3.30it/s]predicting train subjects:  15%|█▍        | 6/41 [00:01<00:10,  3.19it/s]predicting train subjects:  17%|█▋        | 7/41 [00:02<00:10,  3.20it/s]predicting train subjects:  20%|█▉        | 8/41 [00:02<00:10,  3.29it/s]predicting train subjects:  22%|██▏       | 9/41 [00:02<00:09,  3.32it/s]predicting train subjects:  24%|██▍       | 10/41 [00:02<00:09,  3.40it/s]predicting train subjects:  27%|██▋       | 11/41 [00:03<00:09,  3.27it/s]predicting train subjects:  29%|██▉       | 12/41 [00:03<00:07,  3.69it/s]predicting train subjects:  32%|███▏      | 13/41 [00:03<00:07,  3.87it/s]predicting train subjects:  34%|███▍      | 14/41 [00:04<00:07,  3.72it/s]predicting train subjects:  37%|███▋      | 15/41 [00:04<00:06,  3.87it/s]predicting train subjects:  39%|███▉      | 16/41 [00:04<00:06,  3.86it/s]predicting train subjects:  41%|████▏     | 17/41 [00:04<00:07,  3.38it/s]predicting train subjects:  44%|████▍     | 18/41 [00:05<00:06,  3.37it/s]predicting train subjects:  46%|████▋     | 19/41 [00:05<00:06,  3.45it/s]predicting train subjects:  49%|████▉     | 20/41 [00:05<00:05,  3.54it/s]predicting train subjects:  51%|█████     | 21/41 [00:05<00:05,  3.74it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:06<00:05,  3.80it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:06<00:04,  4.22it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:06<00:04,  4.10it/s]predicting train subjects:  61%|██████    | 25/41 [00:06<00:03,  4.10it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:07<00:03,  3.80it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:07<00:03,  3.56it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:07<00:03,  3.41it/s]predicting train subjects:  71%|███████   | 29/41 [00:08<00:03,  3.42it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:08<00:03,  3.23it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:08<00:03,  3.23it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:09<00:02,  3.35it/s]predicting train subjects:  80%|████████  | 33/41 [00:09<00:02,  3.29it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:09<00:02,  3.04it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:10<00:02,  2.71it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:10<00:01,  2.55it/s]predicting train subjects:  90%|█████████ | 37/41 [00:11<00:01,  2.64it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:11<00:01,  2.80it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:11<00:00,  2.97it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:12<00:00,  2.81it/s]predicting train subjects: 100%|██████████| 41/41 [00:12<00:00,  2.90it/s]
predicting test subjects sagittal:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects sagittal:   9%|▉         | 1/11 [00:00<00:02,  3.76it/s]predicting test subjects sagittal:  18%|█▊        | 2/11 [00:00<00:02,  3.61it/s]predicting test subjects sagittal:  27%|██▋       | 3/11 [00:00<00:02,  3.70it/s]predicting test subjects sagittal:  36%|███▋      | 4/11 [00:01<00:01,  3.66it/s]predicting test subjects sagittal:  45%|████▌     | 5/11 [00:01<00:01,  3.62it/s]predicting test subjects sagittal:  55%|█████▍    | 6/11 [00:01<00:01,  3.55it/s]predicting test subjects sagittal:  64%|██████▎   | 7/11 [00:01<00:01,  3.62it/s]predicting test subjects sagittal:  73%|███████▎  | 8/11 [00:02<00:00,  3.79it/s]predicting test subjects sagittal:  82%|████████▏ | 9/11 [00:02<00:00,  3.80it/s]predicting test subjects sagittal:  91%|█████████ | 10/11 [00:02<00:00,  3.73it/s]predicting test subjects sagittal: 100%|██████████| 11/11 [00:03<00:00,  3.54it/s]
predicting train subjects sagittal:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects sagittal:   2%|▏         | 1/41 [00:00<00:12,  3.10it/s]predicting train subjects sagittal:   5%|▍         | 2/41 [00:00<00:11,  3.33it/s]predicting train subjects sagittal:   7%|▋         | 3/41 [00:00<00:10,  3.62it/s]predicting train subjects sagittal:  10%|▉         | 4/41 [00:00<00:09,  3.93it/s]predicting train subjects sagittal:  12%|█▏        | 5/41 [00:01<00:10,  3.45it/s]predicting train subjects sagittal:  15%|█▍        | 6/41 [00:01<00:09,  3.61it/s]predicting train subjects sagittal:  17%|█▋        | 7/41 [00:01<00:08,  3.83it/s]predicting train subjects sagittal:  20%|█▉        | 8/41 [00:01<00:07,  4.33it/s]predicting train subjects sagittal:  22%|██▏       | 9/41 [00:02<00:07,  4.17it/s]predicting train subjects sagittal:  24%|██▍       | 10/41 [00:02<00:08,  3.83it/s]predicting train subjects sagittal:  27%|██▋       | 11/41 [00:02<00:08,  3.62it/s]predicting train subjects sagittal:  29%|██▉       | 12/41 [00:03<00:07,  4.04it/s]predicting train subjects sagittal:  32%|███▏      | 13/41 [00:03<00:06,  4.04it/s]predicting train subjects sagittal:  34%|███▍      | 14/41 [00:03<00:06,  4.07it/s]predicting train subjects sagittal:  37%|███▋      | 15/41 [00:03<00:06,  4.29it/s]predicting train subjects sagittal:  39%|███▉      | 16/41 [00:03<00:05,  4.31it/s]predicting train subjects sagittal:  41%|████▏     | 17/41 [00:04<00:06,  3.84it/s]predicting train subjects sagittal:  44%|████▍     | 18/41 [00:04<00:06,  3.78it/s]predicting train subjects sagittal:  46%|████▋     | 19/41 [00:04<00:05,  3.79it/s]predicting train subjects sagittal:  49%|████▉     | 20/41 [00:05<00:05,  4.11it/s]predicting train subjects sagittal:  51%|█████     | 21/41 [00:05<00:05,  3.98it/s]predicting train subjects sagittal:  54%|█████▎    | 22/41 [00:05<00:04,  3.82it/s]predicting train subjects sagittal:  56%|█████▌    | 23/41 [00:05<00:04,  4.18it/s]predicting train subjects sagittal:  59%|█████▊    | 24/41 [00:06<00:04,  3.93it/s]predicting train subjects sagittal:  61%|██████    | 25/41 [00:06<00:04,  3.83it/s]predicting train subjects sagittal:  63%|██████▎   | 26/41 [00:06<00:04,  3.67it/s]predicting train subjects sagittal:  66%|██████▌   | 27/41 [00:06<00:03,  3.56it/s]predicting train subjects sagittal:  68%|██████▊   | 28/41 [00:07<00:03,  3.58it/s]predicting train subjects sagittal:  71%|███████   | 29/41 [00:07<00:03,  3.77it/s]predicting train subjects sagittal:  73%|███████▎  | 30/41 [00:07<00:02,  3.71it/s]predicting train subjects sagittal:  76%|███████▌  | 31/41 [00:08<00:02,  3.71it/s]predicting train subjects sagittal:  78%|███████▊  | 32/41 [00:08<00:02,  3.76it/s]predicting train subjects sagittal:  80%|████████  | 33/41 [00:08<00:02,  3.79it/s]predicting train subjects sagittal:  83%|████████▎ | 34/41 [00:08<00:02,  3.45it/s]predicting train subjects sagittal:  85%|████████▌ | 35/41 [00:09<00:02,  2.94it/s]predicting train subjects sagittal:  88%|████████▊ | 36/41 [00:09<00:01,  2.79it/s]predicting train subjects sagittal:  90%|█████████ | 37/41 [00:10<00:01,  2.86it/s]predicting train subjects sagittal:  93%|█████████▎| 38/41 [00:10<00:00,  3.12it/s]predicting train subjects sagittal:  95%|█████████▌| 39/41 [00:10<00:00,  3.27it/s]predicting train subjects sagittal:  98%|█████████▊| 40/41 [00:10<00:00,  3.29it/s]predicting train subjects sagittal: 100%|██████████| 41/41 [00:11<00:00,  3.22it/s]
saving BB  test1-THALAMUS:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  82%|████████▏ | 9/11 [00:00<00:00, 85.16it/s]saving BB  test1-THALAMUS: 100%|██████████| 11/11 [00:00<00:00, 83.64it/s]
saving BB  train1-THALAMUS:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS:  22%|██▏       | 9/41 [00:00<00:00, 87.86it/s]saving BB  train1-THALAMUS:  44%|████▍     | 18/41 [00:00<00:00, 87.80it/s]saving BB  train1-THALAMUS:  66%|██████▌   | 27/41 [00:00<00:00, 87.48it/s]saving BB  train1-THALAMUS:  85%|████████▌ | 35/41 [00:00<00:00, 82.90it/s]saving BB  train1-THALAMUS: 100%|██████████| 41/41 [00:00<00:00, 82.16it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal:  82%|████████▏ | 9/11 [00:00<00:00, 83.75it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 11/11 [00:00<00:00, 82.45it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:  22%|██▏       | 9/41 [00:00<00:00, 82.32it/s]saving BB  train1-THALAMUS Sagittal:  44%|████▍     | 18/41 [00:00<00:00, 82.82it/s]saving BB  train1-THALAMUS Sagittal:  66%|██████▌   | 27/41 [00:00<00:00, 83.62it/s]saving BB  train1-THALAMUS Sagittal:  85%|████████▌ | 35/41 [00:00<00:00, 80.65it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 41/41 [00:00<00:00, 60.59it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:17,  2.24it/s]Loading train:   5%|▍         | 2/41 [00:01<00:19,  1.98it/s]Loading train:   7%|▋         | 3/41 [00:01<00:20,  1.86it/s]Loading train:  10%|▉         | 4/41 [00:02<00:19,  1.88it/s]Loading train:  12%|█▏        | 5/41 [00:02<00:20,  1.72it/s]Loading train:  15%|█▍        | 6/41 [00:03<00:18,  1.85it/s]Loading train:  17%|█▋        | 7/41 [00:03<00:17,  1.99it/s]Loading train:  20%|█▉        | 8/41 [00:04<00:14,  2.22it/s]Loading train:  22%|██▏       | 9/41 [00:04<00:15,  2.08it/s]Loading train:  24%|██▍       | 10/41 [00:05<00:14,  2.07it/s]Loading train:  27%|██▋       | 11/41 [00:05<00:14,  2.11it/s]Loading train:  29%|██▉       | 12/41 [00:05<00:11,  2.54it/s]Loading train:  32%|███▏      | 13/41 [00:06<00:10,  2.55it/s]Loading train:  34%|███▍      | 14/41 [00:06<00:10,  2.52it/s]Loading train:  37%|███▋      | 15/41 [00:06<00:08,  2.92it/s]Loading train:  39%|███▉      | 16/41 [00:07<00:09,  2.77it/s]Loading train:  41%|████▏     | 17/41 [00:07<00:09,  2.49it/s]Loading train:  44%|████▍     | 18/41 [00:08<00:09,  2.45it/s]Loading train:  46%|████▋     | 19/41 [00:08<00:09,  2.37it/s]Loading train:  49%|████▉     | 20/41 [00:08<00:08,  2.44it/s]Loading train:  51%|█████     | 21/41 [00:09<00:08,  2.33it/s]Loading train:  54%|█████▎    | 22/41 [00:10<00:09,  2.09it/s]Loading train:  56%|█████▌    | 23/41 [00:10<00:07,  2.46it/s]Loading train:  59%|█████▊    | 24/41 [00:10<00:07,  2.43it/s]Loading train:  61%|██████    | 25/41 [00:11<00:06,  2.45it/s]Loading train:  63%|██████▎   | 26/41 [00:11<00:06,  2.37it/s]Loading train:  66%|██████▌   | 27/41 [00:12<00:06,  2.14it/s]Loading train:  68%|██████▊   | 28/41 [00:12<00:06,  2.15it/s]Loading train:  71%|███████   | 29/41 [00:13<00:05,  2.20it/s]Loading train:  73%|███████▎  | 30/41 [00:13<00:05,  2.16it/s]Loading train:  76%|███████▌  | 31/41 [00:14<00:04,  2.05it/s]Loading train:  78%|███████▊  | 32/41 [00:14<00:04,  2.04it/s]Loading train:  80%|████████  | 33/41 [00:15<00:04,  1.96it/s]Loading train:  83%|████████▎ | 34/41 [00:15<00:03,  1.86it/s]Loading train:  85%|████████▌ | 35/41 [00:16<00:03,  1.68it/s]Loading train:  88%|████████▊ | 36/41 [00:17<00:02,  1.67it/s]Loading train:  90%|█████████ | 37/41 [00:17<00:02,  1.72it/s]Loading train:  93%|█████████▎| 38/41 [00:18<00:01,  1.74it/s]Loading train:  95%|█████████▌| 39/41 [00:18<00:01,  1.80it/s]Loading train:  98%|█████████▊| 40/41 [00:19<00:00,  1.74it/s]Loading train: 100%|██████████| 41/41 [00:19<00:00,  1.70it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:  15%|█▍        | 6/41 [00:00<00:00, 58.80it/s]concatenating: train:  29%|██▉       | 12/41 [00:00<00:00, 57.79it/s]concatenating: train:  71%|███████   | 29/41 [00:00<00:00, 71.92it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 119.89it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:06,  1.59it/s]Loading test:  18%|█▊        | 2/11 [00:01<00:05,  1.64it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:04,  1.70it/s]Loading test:  36%|███▋      | 4/11 [00:02<00:03,  1.85it/s]Loading test:  45%|████▌     | 5/11 [00:02<00:03,  1.86it/s]Loading test:  55%|█████▍    | 6/11 [00:03<00:02,  1.84it/s]Loading test:  64%|██████▎   | 7/11 [00:03<00:02,  1.92it/s]Loading test:  73%|███████▎  | 8/11 [00:04<00:01,  1.75it/s]Loading test:  82%|████████▏ | 9/11 [00:04<00:01,  1.92it/s]Loading test:  91%|█████████ | 10/11 [00:05<00:00,  1.84it/s]Loading test: 100%|██████████| 11/11 [00:06<00:00,  1.76it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  36%|███▋      | 4/11 [00:00<00:00, 36.29it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 57.15it/s]
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 10 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 92, 116, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 92, 116, 10)  100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 92, 116, 10)  40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 92, 116, 10)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 92, 116, 10)  910         activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 92, 116, 10)  40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 92, 116, 10)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 46, 58, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 46, 58, 10)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 46, 58, 20)   1820        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 46, 58, 20)   80          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 46, 58, 20)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 46, 58, 20)   3620        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 46, 58, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 46, 58, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 46, 58, 30)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 23, 29, 30)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 23, 29, 30)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 23, 29, 40)   10840       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 23, 29, 40)   160         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 23, 29, 40)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 23, 29, 40)   14440       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 23, 29, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 23, 29, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 23, 29, 70)   0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 23, 29, 70)   0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 46, 58, 20)   5620        dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 46, 58, 50)   0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 46, 58, 20)   9020        concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 46, 58, 20)   80          conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 58, 20)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 58, 20)   3620        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 46, 58, 20)   80          conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 46, 58, 20)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 46, 58, 70)   0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 46, 58, 70)   0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 92, 116, 10)  2810        dropout_4[0][0]                  2019-07-27 17:34:31.057852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-27 17:34:31.057941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-27 17:34:31.057958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-27 17:34:31.057968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-27 17:34:31.058416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 92, 116, 20)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 92, 116, 10)  1810        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 92, 116, 10)  40          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 92, 116, 10)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 92, 116, 10)  910         activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 92, 116, 10)  40          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 92, 116, 10)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 92, 116, 30)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 92, 116, 30)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 92, 116, 2)   62          dropout_5[0][0]                  
==================================================================================================
Total params: 56,382
Trainable params: 55,982
Non-trainable params: 400
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.98000917 0.01999083]
Train on 4060 samples, validate on 1096 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 1.3386 - acc: 0.9325 - mDice: 0.0830 - val_loss: 1.1289 - val_acc: 0.9756 - val_mDice: 0.1101

Epoch 00001: val_mDice improved from -inf to 0.11012, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.4772 - acc: 0.9830 - mDice: 0.4281 - val_loss: 0.2837 - val_acc: 0.9918 - val_mDice: 0.5776

Epoch 00002: val_mDice improved from 0.11012 to 0.57755, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.1556 - acc: 0.9872 - mDice: 0.7429 - val_loss: 0.0938 - val_acc: 0.9915 - val_mDice: 0.8379

Epoch 00003: val_mDice improved from 0.57755 to 0.83795, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.1216 - acc: 0.9900 - mDice: 0.7937 - val_loss: 0.0775 - val_acc: 0.9921 - val_mDice: 0.8655

Epoch 00004: val_mDice improved from 0.83795 to 0.86549, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.1043 - acc: 0.9913 - mDice: 0.8208 - val_loss: 0.0732 - val_acc: 0.9933 - val_mDice: 0.8729

Epoch 00005: val_mDice improved from 0.86549 to 0.87289, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0948 - acc: 0.9921 - mDice: 0.8364 - val_loss: 0.0684 - val_acc: 0.9942 - val_mDice: 0.8813

Epoch 00006: val_mDice improved from 0.87289 to 0.88127, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0876 - acc: 0.9928 - mDice: 0.8483 - val_loss: 0.0673 - val_acc: 0.9945 - val_mDice: 0.8832

Epoch 00007: val_mDice improved from 0.88127 to 0.88321, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0819 - acc: 0.9936 - mDice: 0.8580 - val_loss: 0.0645 - val_acc: 0.9952 - val_mDice: 0.8881

Epoch 00008: val_mDice improved from 0.88321 to 0.88813, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0801 - acc: 0.9939 - mDice: 0.8609 - val_loss: 0.0630 - val_acc: 0.9952 - val_mDice: 0.8908

Epoch 00009: val_mDice improved from 0.88813 to 0.89083, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0784 - acc: 0.9942 - mDice: 0.8637 - val_loss: 0.0618 - val_acc: 0.9954 - val_mDice: 0.8929

Epoch 00010: val_mDice improved from 0.89083 to 0.89290, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0760 - acc: 0.9946 - mDice: 0.8675 - val_loss: 0.0613 - val_acc: 0.9954 - val_mDice: 0.8936

Epoch 00011: val_mDice improved from 0.89290 to 0.89358, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0742 - acc: 0.9948 - mDice: 0.8700 - val_loss: 0.0601 - val_acc: 0.9955 - val_mDice: 0.8952

Epoch 00012: val_mDice improved from 0.89358 to 0.89520, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0733 - acc: 0.9950 - mDice: 0.8707 - val_loss: 0.0644 - val_acc: 0.9952 - val_mDice: 0.8857

Epoch 00013: val_mDice did not improve from 0.89520
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0699 - acc: 0.9952 - mDice: 0.8754 - val_loss: 0.0616 - val_acc: 0.9954 - val_mDice: 0.8882

Epoch 00014: val_mDice did not improve from 0.89520
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0680 - acc: 0.9953 - mDice: 0.8775 - val_loss: 0.0556 - val_acc: 0.9958 - val_mDice: 0.8975

Epoch 00015: val_mDice improved from 0.89520 to 0.89750, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0672 - acc: 0.9954 - mDice: 0.8781 - val_loss: 0.0590 - val_acc: 0.9956 - val_mDice: 0.8914

Epoch 00016: val_mDice did not improve from 0.89750
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0660 - acc: 0.9955 - mDice: 0.8800 - val_loss: 0.0561 - val_acc: 0.9958 - val_mDice: 0.8962

Epoch 00017: val_mDice did not improve from 0.89750
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 4s - loss: 0.0651 - acc: 0.9955 - mDice: 0.8811 - val_loss: 0.0542 - val_acc: 0.9960 - val_mDice: 0.8996

Epoch 00018: val_mDice improved from 0.89750 to 0.89962, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0627 - acc: 0.9957 - mDice: 0.8852 - val_loss: 0.0536 - val_acc: 0.9960 - val_mDice: 0.9005

Epoch 00019: val_mDice improved from 0.89962 to 0.90054, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0627 - acc: 0.9957 - mDice: 0.8852 - val_loss: 0.0543 - val_acc: 0.9960 - val_mDice: 0.8993

Epoch 00020: val_mDice did not improve from 0.90054
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0623 - acc: 0.9957 - mDice: 0.8859 - val_loss: 0.0529 - val_acc: 0.9961 - val_mDice: 0.9018

Epoch 00021: val_mDice improved from 0.90054 to 0.90178, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0614 - acc: 0.9958 - mDice: 0.8873 - val_loss: 0.0526 - val_acc: 0.9961 - val_mDice: 0.9023

Epoch 00022: val_mDice improved from 0.90178 to 0.90228, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0617 - acc: 0.9957 - mDice: 0.8868 - val_loss: 0.0541 - val_acc: 0.9960 - val_mDice: 0.8997

Epoch 00023: val_mDice did not improve from 0.90228
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0608 - acc: 0.9958 - mDice: 0.8883 - val_loss: 0.0527 - val_acc: 0.9961 - val_mDice: 0.9020

Epoch 00024: val_mDice did not improve from 0.90228
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0607 - acc: 0.9958 - mDice: 0.8884 - val_loss: 0.0510 - val_acc: 0.9962 - val_mDice: 0.9050

Epoch 00025: val_mDice improved from 0.90228 to 0.90503, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0605 - acc: 0.9958 - mDice: 0.8887 - val_loss: 0.0544 - val_acc: 0.9960 - val_mDice: 0.8991

Epoch 00026: val_mDice did not improve from 0.90503
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0599 - acc: 0.9958 - mDice: 0.8897 - val_loss: 0.0497 - val_acc: 0.9963 - val_mDice: 0.9073

Epoch 00027: val_mDice improved from 0.90503 to 0.90729, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0590 - acc: 0.9959 - mDice: 0.8913 - val_loss: 0.0501 - val_acc: 0.9963 - val_mDice: 0.9066

Epoch 00028: val_mDice did not improve from 0.90729
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0600 - acc: 0.9958 - mDice: 0.8896 - val_loss: 0.0509 - val_acc: 0.9962 - val_mDice: 0.9052

Epoch 00029: val_mDice did not improve from 0.90729
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0589 - acc: 0.9959 - mDice: 0.8915 - val_loss: 0.0510 - val_acc: 0.9962 - val_mDice: 0.9051

Epoch 00030: val_mDice did not improve from 0.90729
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0577 - acc: 0.9960 - mDice: 0.8936 - val_loss: 0.0505 - val_acc: 0.9962 - val_mDice: 0.9059

Epoch 00031: val_mDice did not improve from 0.90729
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0580 - acc: 0.9959 - mDice: 0.8931 - val_loss: 0.0522 - val_acc: 0.9961 - val_mDice: 0.9028

Epoch 00032: val_mDice did not improve from 0.90729
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0576 - acc: 0.9959 - mDice: 0.8936 - val_loss: 0.0500 - val_acc: 0.9963 - val_mDice: 0.9069

Epoch 00033: val_mDice did not improve from 0.90729
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0577 - acc: 0.9960 - mDice: 0.8935 - val_loss: 0.0537 - val_acc: 0.9961 - val_mDice: 0.9003

Epoch 00034: val_mDice did not improve from 0.90729
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 4s - loss: 0.0565 - acc: 0.9960 - mDice: 0.8956 - val_loss: 0.0491 - val_acc: 0.9963 - val_mDice: 0.9085

Epoch 00035: val_mDice improved from 0.90729 to 0.90850, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0564 - acc: 0.9960 - mDice: 0.8958 - val_loss: 0.0496 - val_acc: 0.9963 - val_mDice: 0.9076

Epoch 00036: val_mDice did not improve from 0.90850
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0556 - acc: 0.9961 - mDice: 0.8971 - val_loss: 0.0494 - val_acc: 0.9963 - val_mDice: 0.9079

Epoch 00037: val_mDice did not improve from 0.90850
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0556 - acc: 0.9961 - mDice: 0.8972 - val_loss: 0.0528 - val_acc: 0.9961 - val_mDice: 0.9018

Epoch 00038: val_mDice did not improve from 0.90850
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0553 - acc: 0.9961 - mDice: 0.8978 - val_loss: 0.0529 - val_acc: 0.9961 - val_mDice: 0.9016

Epoch 00039: val_mDice did not improve from 0.90850
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0555 - acc: 0.9961 - mDice: 0.8974 - val_loss: 0.0507 - val_acc: 0.9963 - val_mDice: 0.9056

Epoch 00040: val_mDice did not improve from 0.90850
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0550 - acc: 0.9961 - mDice: 0.8983 - val_loss: 0.0519 - val_acc: 0.9962 - val_mDice: 0.9035

Epoch 00041: val_mDice did not improve from 0.90850
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0552 - acc: 0.9961 - mDice: 0.8978 - val_loss: 0.0510 - val_acc: 0.9962 - val_mDice: 0.9051

Epoch 00042: val_mDice did not improve from 0.90850
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0553 - acc: 0.9961 - mDice: 0.8976 - val_loss: 0.0484 - val_acc: 0.9964 - val_mDice: 0.9096

Epoch 00043: val_mDice improved from 0.90850 to 0.90962, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0546 - acc: 0.9962 - mDice: 0.8989 - val_loss: 0.0490 - val_acc: 0.9964 - val_mDice: 0.9086

Epoch 00044: val_mDice did not improve from 0.90962
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0540 - acc: 0.9962 - mDice: 0.9000 - val_loss: 0.0503 - val_acc: 0.9963 - val_mDice: 0.9062

Epoch 00045: val_mDice did not improve from 0.90962
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0543 - acc: 0.9962 - mDice: 0.8995 - val_loss: 0.0491 - val_acc: 0.9964 - val_mDice: 0.9085

Epoch 00046: val_mDice did not improve from 0.90962
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0540 - acc: 0.9962 - mDice: 0.8999 - val_loss: 0.0494 - val_acc: 0.9963 - val_mDice: 0.9079

Epoch 00047: val_mDice did not improve from 0.90962
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0537 - acc: 0.9962 - mDice: 0.9005 - val_loss: 0.0491 - val_acc: 0.9964 - val_mDice: 0.9083

Epoch 00048: val_mDice did not improve from 0.90962
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0537 - acc: 0.9962 - mDice: 0.9004 - val_loss: 0.0498 - val_acc: 0.9963 - val_mDice: 0.9072

Epoch 00049: val_mDice did not improve from 0.90962
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0535 - acc: 0.9962 - mDice: 0.9009 - val_loss: 0.0500 - val_acc: 0.9963 - val_mDice: 0.9067

Epoch 00050: val_mDice did not improve from 0.90962
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0535 - acc: 0.9962 - mDice: 0.9008 - val_loss: 0.0501 - val_acc: 0.9963 - val_mDice: 0.9066

Epoch 00051: val_mDice did not improve from 0.90962
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0536 - acc: 0.9962 - mDice: 0.9006 - val_loss: 0.0487 - val_acc: 0.9964 - val_mDice: 0.9090

Epoch 00052: val_mDice did not improve from 0.90962
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 4s - loss: 0.0539 - acc: 0.9962 - mDice: 0.9000 - val_loss: 0.0478 - val_acc: 0.9964 - val_mDice: 0.9107

Epoch 00053: val_mDice improved from 0.90962 to 0.91068, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM10_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0531 - acc: 0.9962 - mDice: 0.9014 - val_loss: 0.0508 - val_acc: 0.9963 - val_mDice: 0.9054

Epoch 00054: val_mDice did not improve from 0.91068
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0528 - acc: 0.9963 - mDice: 0.9021 - val_loss: 0.0496 - val_acc: 0.9963 - val_mDice: 0.9075

Epoch 00055: val_mDice did not improve from 0.91068
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0525 - acc: 0.9963 - mDice: 0.9026 - val_loss: 0.0488 - val_acc: 0.9964 - val_mDice: 0.9088

Epoch 00056: val_mDice did not improve from 0.91068
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0521 - acc: 0.9963 - mDice: 0.9032 - val_loss: 0.0491 - val_acc: 0.9964 - val_mDice: 0.9083

Epoch 00057: val_mDice did not improve from 0.91068
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0529 - acc: 0.9963 - mDice: 0.9019 - val_loss: 0.0492 - val_acc: 0.9964 - val_mDice: 0.9081

Epoch 00058: val_mDice did not improve from 0.91068
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 5s - loss: 0.0527 - acc: 0.9963 - mDice: 0.9022 - val_loss: 0.0493 - val_acc: 0.9964 - val_mDice: 0.9081

Epoch 00059: val_mDice did not improve from 0.91068
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 5s - loss: 0.0525 - acc: 0.9963 - mDice: 0.9025 - val_loss: 0.0484 - val_acc: 0.9964 - val_mDice: 0.9096

Epoch 00060: val_mDice did not improve from 0.91068
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0521 - acc: 0.9963 - mDice: 0.9032 - val_loss: 0.0488 - val_acc: 0.9964 - val_mDice: 0.9089

Epoch 00061: val_mDice did not improve from 0.91068
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 5s - loss: 0.0520 - acc: 0.9963 - mDice: 0.9034 - val_loss: 0.0493 - val_acc: 0.9964 - val_mDice: 0.9080

Epoch 00062: val_mDice did not improve from 0.91068
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0520 - acc: 0.9963 - mDice: 0.9035 - val_loss: 0.0504 - val_acc: 0.9963 - val_mDice: 0.9061

Epoch 00063: val_mDice did not improve from 0.91068
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0519 - acc: 0.9963 - mDice: 0.9035 - val_loss: 0.0494 - val_acc: 0.9964 - val_mDice: 0.9078

Epoch 00064: val_mDice did not improve from 0.91068
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0518 - acc: 0.9963 - mDice: 0.9038 - val_loss: 0.0502 - val_acc: 0.9963 - val_mDice: 0.9065

Epoch 00065: val_mDice did not improve from 0.91068
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0521 - acc: 0.9963 - mDice: 0.9033 - val_loss: 0.0490 - val_acc: 0.9964 - val_mDice: 0.9085

Epoch 00066: val_mDice did not improve from 0.91068
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0515 - acc: 0.9963 - mDice: 0.9042 - val_loss: 0.0493 - val_acc: 0.9964 - val_mDice: 0.9080

Epoch 00067: val_mDice did not improve from 0.91068
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0520 - acc: 0.9963 - mDice: 0.9034 - val_loss: 0.0485 - val_acc: 0.9964 - val_mDice: 0.9095

Epoch 00068: val_mDice did not improve from 0.91068
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0521 - acc: 0.9963 - mDice: 0.9033 - val_loss: 0.0491 - val_acc: 0.9964 - val_mDice: 0.9084

Epoch 00069: val_mDice did not improve from 0.91068
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0519 - acc: 0.9963 - mDice: 0.9035 - val_loss: 0.0484 - val_acc: 0.9964 - val_mDice: 0.9096

Epoch 00070: val_mDice did not improve from 0.91068
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 5s - loss: 0.0517 - acc: 0.9963 - mDice: 0.9040 - val_loss: 0.0489 - val_acc: 0.9964 - val_mDice: 0.9087

Epoch 00071: val_mDice did not improve from 0.91068
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 4s - loss: 0.0517 - acc: 0.9963 - mDice: 0.9039 - val_loss: 0.0505 - val_acc: 0.9963 - val_mDice: 0.9060

Epoch 00072: val_mDice did not improve from 0.91068
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0515 - acc: 0.9963 - mDice: 0.9043 - val_loss: 0.0488 - val_acc: 0.9964 - val_mDice: 0.9090

Epoch 00073: val_mDice did not improve from 0.91068
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0513 - acc: 0.9963 - mDice: 0.9046 - val_loss: 0.0494 - val_acc: 0.9964 - val_mDice: 0.9078

Epoch 00074: val_mDice did not improve from 0.91068
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0511 - acc: 0.9964 - mDice: 0.9050 - val_loss: 0.0485 - val_acc: 0.9964 - val_mDice: 0.9094

Epoch 00075: val_mDice did not improve from 0.91068
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0508 - acc: 0.9964 - mDice: 0.9055 - val_loss: 0.0484 - val_acc: 0.9964 - val_mDice: 0.9097

Epoch 00076: val_mDice did not improve from 0.91068
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0515 - acc: 0.9963 - mDice: 0.9043 - val_loss: 0.0494 - val_acc: 0.9964 - val_mDice: 0.9077

Epoch 00077: val_mDice did not improve from 0.91068
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0515 - acc: 0.9963 - mDice: 0.9043 - val_loss: 0.0489 - val_acc: 0.9964 - val_mDice: 0.9087

Epoch 00078: val_mDice did not improve from 0.91068
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0510 - acc: 0.9964 - mDice: 0.9052 - val_loss: 0.0490 - val_acc: 0.9964 - val_mDice: 0.9085

Epoch 00079: val_mDice did not improve from 0.91068
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0510 - acc: 0.9964 - mDice: 0.9051 - val_loss: 0.0481 - val_acc: 0.9964 - val_mDice: 0.9101

Epoch 00080: val_mDice did not improve from 0.91068
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0508 - acc: 0.9964 - mDice: 0.9055 - val_loss: 0.0485 - val_acc: 0.9964 - val_mDice: 0.9095

Epoch 00081: val_mDice did not improve from 0.91068
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0510 - acc: 0.9963 - mDice: 0.9051 - val_loss: 0.0492 - val_acc: 0.9964 - val_mDice: 0.9081

Epoch 00082: val_mDice did not improve from 0.91068
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0508 - acc: 0.9964 - mDice: 0.9055 - val_loss: 0.0485 - val_acc: 0.9964 - val_mDice: 0.9094

Epoch 00083: val_mDice did not improve from 0.91068
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0514 - acc: 0.9963 - mDice: 0.9044 - val_loss: 0.0483 - val_acc: 0.9964 - val_mDice: 0.9098

Epoch 00084: val_mDice did not improve from 0.91068
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0512 - acc: 0.9964 - mDice: 0.9049 - val_loss: 0.0485 - val_acc: 0.9964 - val_mDice: 0.9095

Epoch 00085: val_mDice did not improve from 0.91068
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0507 - acc: 0.9964 - mDice: 0.9057 - val_loss: 0.0490 - val_acc: 0.9964 - val_mDice: 0.9086

Epoch 00086: val_mDice did not improve from 0.91068
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0506 - acc: 0.9964 - mDice: 0.9058 - val_loss: 0.0491 - val_acc: 0.9964 - val_mDice: 0.9084

Epoch 00087: val_mDice did not improve from 0.91068
Epoch 88/300

Epoch 00088: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0508 - acc: 0.9964 - mDice: 0.9055 - val_loss: 0.0488 - val_acc: 0.9964 - val_mDice: 0.9089

Epoch 00088: val_mDice did not improve from 0.91068
Epoch 89/300

Epoch 00089: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0503 - acc: 0.9964 - mDice: 0.9063 - val_loss: 0.0488 - val_acc: 0.9964 - val_mDice: 0.9089

Epoch 00089: val_mDice did not improve from 0.91068
Epoch 90/300

Epoch 00090: LearningRateScheduler setting learning rate to 6.25e-05.
 - 4s - loss: 0.0504 - acc: 0.9964 - mDice: 0.9062 - val_loss: 0.0489 - val_acc: 0.9964 - val_mDice: 0.9087

Epoch 00090: val_mDice did not improve from 0.91068
Epoch 91/300

Epoch 00091: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0505 - acc: 0.9964 - mDice: 0.9060 - val_loss: 0.0492 - val_acc: 0.9964 - val_mDice: 0.9081

Epoch 00091: val_mDice did not improve from 0.91068
Epoch 92/300

Epoch 00092: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0505 - acc: 0.9964 - mDice: 0.9060 - val_loss: 0.0488 - val_acc: 0.9964 - val_mDice: 0.9089

Epoch 00092: val_mDice did not improve from 0.91068
Epoch 93/300

Epoch 00093: LearningRateScheduler setting learning rate to 3.125e-05.
 - 4s - loss: 0.0502 - acc: 0.9964 - mDice: 0.9066 - val_loss: 0.0488 - val_acc: 0.9964 - val_mDice: 0.9089

Epoch 00093: val_mDice did not improve from 0.91068
Restoring model weights from the end of the best epoch
Epoch 00093: early stopping
{'val_loss': [1.1289272373610169, 0.2837424151545023, 0.09380152636635912, 0.07754110626495668, 0.07322114276407408, 0.06843059775113625, 0.06733488589253304, 0.06453102332179564, 0.06298583845206857, 0.0618143191792234, 0.06133300597595907, 0.06005053252090503, 0.06436320469055297, 0.061618029812935494, 0.055575740340091016, 0.058961256101292416, 0.056119906450927695, 0.054152562248989615, 0.05362302852101135, 0.05433906118528251, 0.0528956811847913, 0.05261093129261132, 0.05410086033172416, 0.05273451390982109, 0.051031498455979526, 0.054415873136289795, 0.0497495851132774, 0.05012663569382943, 0.05090834767333347, 0.05099436322594211, 0.0505065908995423, 0.05224994294019076, 0.049963375349549484, 0.05366826852797157, 0.049050133525781386, 0.049575291981879814, 0.049358072221605445, 0.05284544340178479, 0.052890798308118415, 0.05068039221104479, 0.05188539045026703, 0.05097641651756572, 0.04841309641725826, 0.049001214536328386, 0.0503124580956506, 0.04905151051000087, 0.04935736370945934, 0.04913448978786486, 0.049776425163676266, 0.05003136397767676, 0.05012383908849128, 0.04874894626601769, 0.04781358206413523, 0.05077740174792979, 0.049607543070821, 0.04884716588324004, 0.04913094724073027, 0.0492309107288827, 0.049252958628383, 0.04842201971115857, 0.04880473670298165, 0.04928900750122801, 0.05037570810013444, 0.04942111216866187, 0.05016602990455436, 0.04904656656032061, 0.04930806875120114, 0.04848737197581434, 0.04907890720578441, 0.048383906837144906, 0.04893490122835131, 0.050450681919490335, 0.04876339041294843, 0.0493925786067317, 0.04851379112279328, 0.04837065263494958, 0.04944477378506295, 0.048892395312551164, 0.04904047713825738, 0.048122243996519244, 0.04848544857036458, 0.04922538688474328, 0.04849196994935509, 0.04830432479290196, 0.0484627615932348, 0.048959987319625205, 0.04908052857720504, 0.048780631341964656, 0.0488045153745117, 0.04889032251480287, 0.04923881168891914, 0.0488030464136905, 0.048803004062306275], 'val_acc': [0.975634194421072, 0.9918472212161461, 0.9914678024114484, 0.992070274196402, 0.9933185203232034, 0.994188960233744, 0.9944630502349269, 0.9952179819998079, 0.9951838574705333, 0.9954160695528462, 0.9954415457527133, 0.9955433720219744, 0.9951708730119858, 0.9953868328219783, 0.9958261951912929, 0.9956137335648502, 0.9958378143989257, 0.9959622934786942, 0.9960041161436234, 0.9959587045829662, 0.9960504446151482, 0.9960818240677354, 0.9959897431578949, 0.9960710475914669, 0.9961867286776104, 0.9959948743781905, 0.9962649556407093, 0.9962515337188749, 0.9962157084994072, 0.9962082677078943, 0.9962364781076891, 0.9961338909002986, 0.996278369078671, 0.9960638639265603, 0.9963357511663089, 0.9963134285742349, 0.9963301933594864, 0.9961245712572641, 0.9961290953368166, 0.9962586223208991, 0.9961774018559143, 0.9962448599564768, 0.9963960952132288, 0.996356859476897, 0.9962834217687593, 0.9963657605822069, 0.9963475400513976, 0.9963601988597508, 0.996323943573193, 0.9963036832148141, 0.9963017201771701, 0.9963859328388298, 0.9964405652815408, 0.9962684509963015, 0.9963364311813439, 0.99638823457878, 0.9963766975994528, 0.9963622460835171, 0.9963615630229894, 0.9964175604990799, 0.9963879905042857, 0.9963681428101812, 0.9962994799996815, 0.9963653333430743, 0.9963089919438327, 0.9963790833079902, 0.9963710440771423, 0.9964133825180305, 0.996379175542915, 0.996425854681182, 0.9963866143766111, 0.9962994958797511, 0.9964025973403541, 0.9963669541978488, 0.9964253412981103, 0.9964254302700071, 0.9963681454206035, 0.9963953307945362, 0.9963894351555483, 0.9964432966535108, 0.9964268040047944, 0.9963767876590255, 0.9964273939602566, 0.9964370594842591, 0.9964260271865956, 0.9963984989771878, 0.9963872946091812, 0.9964048331671388, 0.9964053467677457, 0.9963954234645315, 0.9963750752219318, 0.9964018390126472, 0.9964049978412851], 'val_mDice': [0.11012010518325506, 0.5775516000542328, 0.8379467804066456, 0.8654855596323083, 0.8728897021199665, 0.8812650600924109, 0.883206325290847, 0.8881318636619262, 0.8908349512702357, 0.892896331574795, 0.8935771991301628, 0.8951950464805547, 0.8856781207732041, 0.8882153762953124, 0.8975047571380643, 0.8913635585429894, 0.8962289547398142, 0.8996156984872191, 0.9005361201554319, 0.899263940370866, 0.901782483297543, 0.9022802997679606, 0.8996523385935457, 0.9020408011265915, 0.9050340313110908, 0.8990660211900725, 0.9072891593414502, 0.9066062436921754, 0.9052043798631125, 0.9050791820470434, 0.9059260320054353, 0.9028451231709362, 0.90687016675072, 0.9002830190380124, 0.9085044119044812, 0.907565642229832, 0.9079396826942472, 0.9017517401354156, 0.901648180980752, 0.9055831223508738, 0.9034552615489403, 0.9050543906479857, 0.9096196733251976, 0.9085779198764885, 0.9062409888218789, 0.9084663571667497, 0.9079197281903594, 0.908329655654239, 0.9071839421334928, 0.9067255219403845, 0.9065566954821566, 0.908996189162679, 0.9106796892890094, 0.9053906833168364, 0.9074699319627163, 0.9088297027305965, 0.9083159030789006, 0.9081462399802939, 0.9081064029766696, 0.9095822059325058, 0.9089013864524174, 0.9080310661862366, 0.9061002267973266, 0.907782749538004, 0.9064653650687559, 0.908467824224138, 0.9079896152889642, 0.909457285908887, 0.9084015164497125, 0.9096388390464504, 0.908654817699516, 0.9059555289084024, 0.9089625610487304, 0.907836824873068, 0.9094074881859939, 0.9096580184724209, 0.9077432923508386, 0.90872896435487, 0.9084612566189174, 0.9101007289259974, 0.9094509171308393, 0.9081329548881002, 0.9094379613869381, 0.9097764133101832, 0.9094923423589582, 0.9086042383726496, 0.9083902698798771, 0.9089240523150367, 0.9088797534469271, 0.9087282397451192, 0.9081050614370917, 0.908883577280671, 0.9088772039343841], 'loss': [1.3386406860328073, 0.4772309206536251, 0.1556204794883141, 0.12158617700262023, 0.10430138459052946, 0.09475956148848745, 0.08762160618904188, 0.0818515425465377, 0.08007323617124792, 0.07836399972438812, 0.07598720234015892, 0.07422119624949441, 0.07330840422308503, 0.06987655416057614, 0.0680015247884055, 0.06724795431192285, 0.06595813466483737, 0.06513524440855815, 0.06270117989828435, 0.0627233082511155, 0.06227139115627176, 0.06140610647245581, 0.06165571141932986, 0.06081571861220698, 0.06072107012489159, 0.06051832107193952, 0.05993185564875603, 0.05899305951786159, 0.05998730162167784, 0.05887590334039604, 0.057689230650516565, 0.05797218335012497, 0.05763266681523746, 0.057691518290729944, 0.056503446671763075, 0.056383291158476484, 0.05563145852074247, 0.05558961268349234, 0.0552550773939182, 0.055453449671197995, 0.05496563030214145, 0.05519625579415284, 0.05534504557698231, 0.05457200350359156, 0.053957088442124754, 0.05425341568630317, 0.053990654996022805, 0.053685234006255717, 0.05374646441910067, 0.053467150575715335, 0.05347602302437933, 0.05363215155569204, 0.053913122052189164, 0.05313881227902591, 0.05277842182317391, 0.05245372950296684, 0.052147080140014, 0.05289260303519042, 0.05272008968618116, 0.0525302121535017, 0.05213345132144214, 0.0519959648584791, 0.05196497715972914, 0.0519341365824192, 0.051763414822775744, 0.05207011874379783, 0.05154625924643624, 0.052026960521642795, 0.05208003651258981, 0.051947912999561856, 0.05165167778656988, 0.051722525100549456, 0.05147826488088504, 0.05133979940062086, 0.05109398738650853, 0.05084869017815355, 0.05151904106433756, 0.05151832364316057, 0.05097963743535756, 0.05104536854941857, 0.0508000250593782, 0.05103726828186383, 0.05079654547307879, 0.05142058872530613, 0.05115442480800187, 0.05069592670340256, 0.050649348554646444, 0.050792910837627984, 0.050337376063827224, 0.050445927958476716, 0.05054110165667064, 0.050509656591368426, 0.05018231074504664], 'acc': [0.9324585381399821, 0.9830173198225463, 0.987225723677668, 0.9900011787273614, 0.9913269196237836, 0.9921106073656693, 0.992797876226491, 0.9935720389699702, 0.9939467504693957, 0.9942230655641978, 0.9945636432746361, 0.9948028910336236, 0.9949596497812881, 0.9952049164349223, 0.995336997685174, 0.9954030231889246, 0.995467742382012, 0.9955222154485768, 0.9956778883934021, 0.9956685839028194, 0.9957085797939395, 0.9957551280853196, 0.9957266969633807, 0.9957765053058493, 0.9957842160328269, 0.9958000928897576, 0.9958194370927482, 0.995882691420945, 0.9958171213201701, 0.9958862486144004, 0.9959702327333647, 0.9959461415929748, 0.9959495361215376, 0.995950411105978, 0.9960321340654871, 0.9960295055887382, 0.9960905260640412, 0.996088840104089, 0.9961115265714711, 0.9961018811892993, 0.9961363153504621, 0.9961112220886306, 0.9961074176680278, 0.9961606595903781, 0.9961904137592598, 0.9961664324323533, 0.9961977486539944, 0.9962073294399995, 0.9961987173028768, 0.9962151062312384, 0.9962113693429919, 0.996199647487678, 0.9961763611568019, 0.9962372894357578, 0.9962587297843595, 0.9962736823288678, 0.9962988372506767, 0.9962614715980191, 0.9962608517684373, 0.9962635081389855, 0.9963044289297658, 0.9963057387638562, 0.9963103591514926, 0.9963055341114552, 0.9963157376632314, 0.9962955886507269, 0.996331498247062, 0.9963059986166178, 0.9963030665378853, 0.9962980130623127, 0.9963254687821337, 0.9963165630260712, 0.9963276004556365, 0.9963374780903896, 0.9963536095149411, 0.9963742890968699, 0.996326398379697, 0.9963224494985758, 0.996367292157535, 0.9963587085014494, 0.9963725091788569, 0.9963467347210851, 0.9963674557032843, 0.996330737480389, 0.9963514417263087, 0.9963750463401155, 0.9963858497553858, 0.9963669947215489, 0.9964007162695447, 0.996388740433848, 0.9963797354345838, 0.9963858967344162, 0.9964106311939033], 'mDice': [0.08296601338486366, 0.428148036006049, 0.7429379996995034, 0.7936811781869146, 0.8208420784602611, 0.8363816967151435, 0.8482728439011598, 0.8579745683176764, 0.8608812660419295, 0.8636529953608959, 0.8674801561632767, 0.8700351509554632, 0.870662393534712, 0.8753610347292106, 0.8775136015685321, 0.8780715083841033, 0.8799500591649211, 0.8811362215450832, 0.885246036968795, 0.8851512001065786, 0.8858868406323964, 0.8873330513244779, 0.8868492950359589, 0.8882664148443438, 0.8883976443060513, 0.8887284677016911, 0.8897210852853183, 0.8913493590989137, 0.8895832162185255, 0.8915131978800731, 0.8935796618461609, 0.8930545657726344, 0.893633328928736, 0.8935146405191844, 0.8955951893858134, 0.8957848636974842, 0.8971024434554753, 0.8971674257311327, 0.8977578605337097, 0.8973984283766723, 0.8982501006478747, 0.8978431621208567, 0.8975783985823833, 0.8989372297460809, 0.900017797359692, 0.8994899233573763, 0.8999409613938167, 0.9004876798596876, 0.9003703934805733, 0.9008681219199608, 0.9008390739046294, 0.9005577940071745, 0.9000492595099463, 0.9014222002381762, 0.9020558619146863, 0.902628827858441, 0.9031755753925869, 0.9018556234871813, 0.9021564034992838, 0.9024889739276153, 0.9031910267956739, 0.9034321328689312, 0.9034820304715575, 0.9035340130622751, 0.9038393412317548, 0.9032880696169848, 0.9042224731351355, 0.9033621006411284, 0.9032706465627173, 0.903503252074049, 0.9040222805121849, 0.9038894161802208, 0.9043293289950328, 0.9045728876672942, 0.9050098616501381, 0.9054523650648558, 0.9042571548170644, 0.9042608873010269, 0.9052127702482815, 0.9050991405994434, 0.905522566123549, 0.9051069837485628, 0.9055345757254238, 0.904426278445521, 0.9048899881945455, 0.9057096789035891, 0.9057868792505687, 0.9055397915722702, 0.9063403947012765, 0.9061500776577466, 0.9059777248081903, 0.9060307717675646, 0.9066149941806135], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 3.125e-05, 3.125e-05, 3.125e-05]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s]predicting test subjects:  18%|█▊        | 2/11 [00:01<00:05,  1.51it/s]predicting test subjects:  27%|██▋       | 3/11 [00:01<00:04,  1.79it/s]predicting test subjects:  36%|███▋      | 4/11 [00:01<00:03,  2.10it/s]predicting test subjects:  45%|████▌     | 5/11 [00:02<00:02,  2.35it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:02<00:01,  2.54it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:02<00:01,  2.71it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:02<00:01,  2.89it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:03<00:00,  3.12it/s]predicting test subjects:  91%|█████████ | 10/11 [00:03<00:00,  3.19it/s]predicting test subjects: 100%|██████████| 11/11 [00:03<00:00,  3.01it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:13,  2.92it/s]predicting train subjects:   5%|▍         | 2/41 [00:00<00:12,  3.01it/s]predicting train subjects:   7%|▋         | 3/41 [00:00<00:11,  3.19it/s]predicting train subjects:  10%|▉         | 4/41 [00:01<00:11,  3.21it/s]predicting train subjects:  12%|█▏        | 5/41 [00:01<00:12,  2.98it/s]predicting train subjects:  15%|█▍        | 6/41 [00:01<00:11,  3.05it/s]predicting train subjects:  17%|█▋        | 7/41 [00:02<00:10,  3.34it/s]predicting train subjects:  20%|█▉        | 8/41 [00:02<00:08,  3.82it/s]predicting train subjects:  22%|██▏       | 9/41 [00:02<00:08,  3.71it/s]predicting train subjects:  24%|██▍       | 10/41 [00:02<00:08,  3.55it/s]predicting train subjects:  27%|██▋       | 11/41 [00:03<00:09,  3.27it/s]predicting train subjects:  29%|██▉       | 12/41 [00:03<00:08,  3.58it/s]predicting train subjects:  32%|███▏      | 13/41 [00:03<00:08,  3.43it/s]predicting train subjects:  34%|███▍      | 14/41 [00:04<00:08,  3.34it/s]predicting train subjects:  37%|███▋      | 15/41 [00:04<00:07,  3.59it/s]predicting train subjects:  39%|███▉      | 16/41 [00:04<00:07,  3.37it/s]predicting train subjects:  41%|████▏     | 17/41 [00:05<00:07,  3.10it/s]predicting train subjects:  44%|████▍     | 18/41 [00:05<00:07,  2.96it/s]predicting train subjects:  46%|████▋     | 19/41 [00:05<00:06,  3.19it/s]predicting train subjects:  49%|████▉     | 20/41 [00:05<00:06,  3.47it/s]predicting train subjects:  51%|█████     | 21/41 [00:06<00:05,  3.53it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:06<00:05,  3.47it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:06<00:04,  3.91it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:06<00:04,  3.78it/s]predicting train subjects:  61%|██████    | 25/41 [00:07<00:04,  3.47it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:07<00:04,  3.41it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:08<00:04,  3.07it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:08<00:04,  3.16it/s]predicting train subjects:  71%|███████   | 29/41 [00:08<00:03,  3.23it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:08<00:03,  3.09it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:09<00:03,  3.02it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:09<00:03,  2.99it/s]predicting train subjects:  80%|████████  | 33/41 [00:10<00:02,  3.02it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:10<00:02,  2.78it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:10<00:02,  2.50it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:11<00:02,  2.35it/s]predicting train subjects:  90%|█████████ | 37/41 [00:11<00:01,  2.43it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:12<00:01,  2.63it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:12<00:00,  2.81it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:12<00:00,  2.86it/s]predicting train subjects: 100%|██████████| 41/41 [00:13<00:00,  2.89it/s]
saving BB  test1-THALAMUS:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  55%|█████▍    | 6/11 [00:00<00:00, 53.32it/s]saving BB  test1-THALAMUS: 100%|██████████| 11/11 [00:00<00:00, 59.09it/s]
saving BB  train1-THALAMUS:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS:  17%|█▋        | 7/41 [00:00<00:00, 64.67it/s]saving BB  train1-THALAMUS:  34%|███▍      | 14/41 [00:00<00:00, 64.87it/s]saving BB  train1-THALAMUS:  54%|█████▎    | 22/41 [00:00<00:00, 67.24it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 30/41 [00:00<00:00, 70.26it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 36/41 [00:00<00:00, 64.21it/s]saving BB  train1-THALAMUS: 100%|██████████| 41/41 [00:00<00:00, 67.25it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:33,  1.21it/s]Loading train:   5%|▍         | 2/41 [00:01<00:27,  1.42it/s]Loading train:   7%|▋         | 3/41 [00:01<00:22,  1.67it/s]Loading train:  10%|▉         | 4/41 [00:01<00:18,  1.95it/s]Loading train:  12%|█▏        | 5/41 [00:02<00:17,  2.08it/s]Loading train:  15%|█▍        | 6/41 [00:02<00:16,  2.08it/s]Loading train:  17%|█▋        | 7/41 [00:03<00:15,  2.26it/s]Loading train:  20%|█▉        | 8/41 [00:03<00:13,  2.42it/s]Loading train:  22%|██▏       | 9/41 [00:03<00:13,  2.43it/s]Loading train:  24%|██▍       | 10/41 [00:04<00:12,  2.52it/s]Loading train:  27%|██▋       | 11/41 [00:04<00:11,  2.54it/s]Loading train:  29%|██▉       | 12/41 [00:04<00:10,  2.71it/s]Loading train:  32%|███▏      | 13/41 [00:05<00:10,  2.63it/s]Loading train:  34%|███▍      | 14/41 [00:05<00:10,  2.54it/s]Loading train:  37%|███▋      | 15/41 [00:06<00:10,  2.58it/s]Loading train:  39%|███▉      | 16/41 [00:06<00:09,  2.62it/s]Loading train:  41%|████▏     | 17/41 [00:06<00:09,  2.50it/s]Loading train:  44%|████▍     | 18/41 [00:07<00:09,  2.48it/s]Loading train:  46%|████▋     | 19/41 [00:07<00:08,  2.59it/s]Loading train:  49%|████▉     | 20/41 [00:08<00:07,  2.77it/s]Loading train:  51%|█████     | 21/41 [00:08<00:06,  2.89it/s]Loading train:  54%|█████▎    | 22/41 [00:08<00:06,  2.81it/s]Loading train:  56%|█████▌    | 23/41 [00:09<00:06,  2.92it/s]Loading train:  59%|█████▊    | 24/41 [00:09<00:05,  2.86it/s]Loading train:  61%|██████    | 25/41 [00:09<00:06,  2.65it/s]Loading train:  63%|██████▎   | 26/41 [00:10<00:05,  2.55it/s]Loading train:  66%|██████▌   | 27/41 [00:10<00:05,  2.53it/s]Loading train:  68%|██████▊   | 28/41 [00:11<00:04,  2.62it/s]Loading train:  71%|███████   | 29/41 [00:11<00:04,  2.73it/s]Loading train:  73%|███████▎  | 30/41 [00:11<00:04,  2.37it/s]Loading train:  76%|███████▌  | 31/41 [00:12<00:04,  2.28it/s]Loading train:  78%|███████▊  | 32/41 [00:12<00:03,  2.41it/s]Loading train:  80%|████████  | 33/41 [00:13<00:03,  2.41it/s]Loading train:  83%|████████▎ | 34/41 [00:13<00:03,  2.30it/s]Loading train:  85%|████████▌ | 35/41 [00:14<00:02,  2.13it/s]Loading train:  88%|████████▊ | 36/41 [00:14<00:02,  2.12it/s]Loading train:  90%|█████████ | 37/41 [00:15<00:01,  2.25it/s]Loading train:  93%|█████████▎| 38/41 [00:15<00:01,  2.33it/s]Loading train:  95%|█████████▌| 39/41 [00:15<00:00,  2.45it/s]Loading train:  98%|█████████▊| 40/41 [00:16<00:00,  2.46it/s]Loading train: 100%|██████████| 41/41 [00:16<00:00,  2.49it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:  32%|███▏      | 13/41 [00:00<00:00, 121.01it/s]concatenating: train:  61%|██████    | 25/41 [00:00<00:00, 120.50it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 144.21it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:04,  2.46it/s]Loading test:  18%|█▊        | 2/11 [00:00<00:03,  2.60it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:02,  2.72it/s]Loading test:  36%|███▋      | 4/11 [00:01<00:02,  2.82it/s]Loading test:  45%|████▌     | 5/11 [00:01<00:02,  2.79it/s]Loading test:  55%|█████▍    | 6/11 [00:02<00:01,  2.84it/s]Loading test:  64%|██████▎   | 7/11 [00:02<00:01,  2.87it/s]Loading test:  73%|███████▎  | 8/11 [00:02<00:01,  2.93it/s]Loading test:  82%|████████▏ | 9/11 [00:03<00:00,  2.99it/s]Loading test:  91%|█████████ | 10/11 [00:03<00:00,  2.97it/s]Loading test: 100%|██████████| 11/11 [00:03<00:00,  2.77it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 242.06it/s]
Loading trainS:   0%|          | 0/41 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/41 [00:00<00:18,  2.19it/s]Loading trainS:   5%|▍         | 2/41 [00:00<00:17,  2.28it/s]Loading trainS:   7%|▋         | 3/41 [00:01<00:15,  2.52it/s]Loading trainS:  10%|▉         | 4/41 [00:01<00:13,  2.65it/s]Loading trainS:  12%|█▏        | 5/41 [00:01<00:13,  2.62it/s]Loading trainS:  15%|█▍        | 6/41 [00:02<00:12,  2.72it/s]Loading trainS:  17%|█▋        | 7/41 [00:02<00:11,  2.84it/s]Loading trainS:  20%|█▉        | 8/41 [00:02<00:11,  2.96it/s]Loading trainS:  22%|██▏       | 9/41 [00:03<00:11,  2.88it/s]Loading trainS:  24%|██▍       | 10/41 [00:03<00:11,  2.64it/s]Loading trainS:  27%|██▋       | 11/41 [00:04<00:11,  2.60it/s]Loading trainS:  29%|██▉       | 12/41 [00:04<00:10,  2.74it/s]Loading trainS:  32%|███▏      | 13/41 [00:04<00:10,  2.64it/s]Loading trainS:  34%|███▍      | 14/41 [00:05<00:10,  2.62it/s]Loading trainS:  37%|███▋      | 15/41 [00:05<00:09,  2.70it/s]Loading trainS:  39%|███▉      | 16/41 [00:05<00:09,  2.74it/s]Loading trainS:  41%|████▏     | 17/41 [00:06<00:08,  2.67it/s]Loading trainS:  44%|████▍     | 18/41 [00:06<00:08,  2.74it/s]Loading trainS:  46%|████▋     | 19/41 [00:06<00:07,  2.82it/s]Loading trainS:  49%|████▉     | 20/41 [00:07<00:07,  2.94it/s]Loading trainS:  51%|█████     | 21/41 [00:07<00:06,  3.03it/s]Loading trainS:  54%|█████▎    | 22/41 [00:07<00:06,  2.96it/s]Loading trainS:  56%|█████▌    | 23/41 [00:08<00:06,  3.00it/s]Loading trainS:  59%|█████▊    | 24/41 [00:08<00:05,  2.89it/s]Loading trainS:  61%|██████    | 25/41 [00:09<00:05,  2.74it/s]Loading trainS:  63%|██████▎   | 26/41 [00:09<00:05,  2.68it/s]Loading trainS:  66%|██████▌   | 27/41 [00:09<00:05,  2.65it/s]Loading trainS:  68%|██████▊   | 28/41 [00:10<00:04,  2.68it/s]Loading trainS:  71%|███████   | 29/41 [00:10<00:04,  2.82it/s]Loading trainS:  73%|███████▎  | 30/41 [00:10<00:04,  2.72it/s]Loading trainS:  76%|███████▌  | 31/41 [00:11<00:04,  2.50it/s]Loading trainS:  78%|███████▊  | 32/41 [00:11<00:03,  2.55it/s]Loading trainS:  80%|████████  | 33/41 [00:12<00:03,  2.60it/s]Loading trainS:  83%|████████▎ | 34/41 [00:12<00:02,  2.52it/s]Loading trainS:  85%|████████▌ | 35/41 [00:13<00:02,  2.27it/s]Loading trainS:  88%|████████▊ | 36/41 [00:13<00:02,  2.23it/s]Loading trainS:  90%|█████████ | 37/41 [00:14<00:01,  2.07it/s]Loading trainS:  93%|█████████▎| 38/41 [00:14<00:01,  1.89it/s]Loading trainS:  95%|█████████▌| 39/41 [00:15<00:01,  1.97it/s]Loading trainS:  98%|█████████▊| 40/41 [00:15<00:00,  1.88it/s]Loading trainS: 100%|██████████| 41/41 [00:16<00:00,  1.88it/s]
Loading testS:   0%|          | 0/11 [00:00<?, ?it/s]Loading testS:   9%|▉         | 1/11 [00:00<00:05,  1.92it/s]Loading testS:  18%|█▊        | 2/11 [00:01<00:04,  1.95it/s]Loading testS:  27%|██▋       | 3/11 [00:01<00:03,  2.01it/s]Loading testS:  36%|███▋      | 4/11 [00:01<00:03,  2.00it/s]Loading testS:  45%|████▌     | 5/11 [00:02<00:02,  2.06it/s]Loading testS:  55%|█████▍    | 6/11 [00:02<00:02,  2.13it/s]Loading testS:  64%|██████▎   | 7/11 [00:03<00:01,  2.11it/s]Loading testS:  73%|███████▎  | 8/11 [00:03<00:01,  2.04it/s]Loading testS:  82%|████████▏ | 9/11 [00:04<00:00,  2.19it/s]Loading testS:  91%|█████████ | 10/11 [00:04<00:00,  2.11it/s]Loading testS: 100%|██████████| 11/11 [00:05<00:00,  2.06it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 15 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 15 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 128, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 128, 15) 150         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 128, 15) 60          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 128, 15) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 128, 15) 2040        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 128, 15) 60          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 128, 15) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 64, 15)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 64, 15)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 64, 30)   4080        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 64, 30)   120         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 64, 30)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 64, 30)   8130        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 64, 30)   120         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 64, 30)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 64, 45)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 32, 45)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 32, 45)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 32, 60)   24360       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 32, 60)   240         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 32, 60)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 32, 60)   32460       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 32, 60)   240         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 32, 60)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 32, 105)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 32, 105)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 64, 30)   12630       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 64, 75)   0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 64, 30)   20280       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 64, 30)   120         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 64, 30)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 64, 30)   8130        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 64, 30)   120         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 64, 30)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 64, 105)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 64, 105)  0           concatenate_4[0][0]              2019-07-27 17:42:43.915141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-27 17:42:43.915262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-27 17:42:43.915281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-27 17:42:43.915292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-27 17:42:43.915731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 128, 15) 6315        dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 128, 30) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 128, 15) 4065        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 128, 15) 60          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 128, 15) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 128, 15) 2040        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 128, 15) 60          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 128, 15) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 128, 45) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 128, 45) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 128, 2)  92          dropout_5[0][0]                  
==================================================================================================
Total params: 125,972
Trainable params: 125,372
Non-trainable params: 600
__________________________________________________________________________________________________
----- /array/ssd/msmajdi/experiments/keras/exp6/models/sE8_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1/1-THALAMUS/sd2/model_weights.h5
 --- initialized from Model_3T /array/ssd/msmajdi/experiments/keras/exp6/models/sE8_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97831385 0.02168615]
Train on 2690 samples, validate on 698 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0559 - acc: 0.9955 - mDice: 0.8965 - val_loss: 0.0512 - val_acc: 0.9959 - val_mDice: 0.9046

Epoch 00001: val_mDice improved from -inf to 0.90462, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0507 - acc: 0.9959 - mDice: 0.9055 - val_loss: 0.0485 - val_acc: 0.9960 - val_mDice: 0.9094

Epoch 00002: val_mDice improved from 0.90462 to 0.90942, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0491 - acc: 0.9960 - mDice: 0.9084 - val_loss: 0.0461 - val_acc: 0.9961 - val_mDice: 0.9138

Epoch 00003: val_mDice improved from 0.90942 to 0.91383, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0478 - acc: 0.9961 - mDice: 0.9107 - val_loss: 0.0457 - val_acc: 0.9962 - val_mDice: 0.9146

Epoch 00004: val_mDice improved from 0.91383 to 0.91463, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0475 - acc: 0.9961 - mDice: 0.9113 - val_loss: 0.0471 - val_acc: 0.9961 - val_mDice: 0.9119

Epoch 00005: val_mDice did not improve from 0.91463
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0474 - acc: 0.9962 - mDice: 0.9115 - val_loss: 0.0458 - val_acc: 0.9962 - val_mDice: 0.9143

Epoch 00006: val_mDice did not improve from 0.91463
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0460 - acc: 0.9962 - mDice: 0.9139 - val_loss: 0.0458 - val_acc: 0.9962 - val_mDice: 0.9143

Epoch 00007: val_mDice did not improve from 0.91463
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0459 - acc: 0.9963 - mDice: 0.9141 - val_loss: 0.0446 - val_acc: 0.9963 - val_mDice: 0.9164

Epoch 00008: val_mDice improved from 0.91463 to 0.91644, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0454 - acc: 0.9963 - mDice: 0.9150 - val_loss: 0.0440 - val_acc: 0.9963 - val_mDice: 0.9177

Epoch 00009: val_mDice improved from 0.91644 to 0.91766, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0451 - acc: 0.9964 - mDice: 0.9156 - val_loss: 0.0446 - val_acc: 0.9963 - val_mDice: 0.9164

Epoch 00010: val_mDice did not improve from 0.91766
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0445 - acc: 0.9964 - mDice: 0.9166 - val_loss: 0.0442 - val_acc: 0.9963 - val_mDice: 0.9172

Epoch 00011: val_mDice did not improve from 0.91766
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0436 - acc: 0.9964 - mDice: 0.9182 - val_loss: 0.0447 - val_acc: 0.9962 - val_mDice: 0.9162

Epoch 00012: val_mDice did not improve from 0.91766
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0439 - acc: 0.9964 - mDice: 0.9177 - val_loss: 0.0440 - val_acc: 0.9963 - val_mDice: 0.9176

Epoch 00013: val_mDice did not improve from 0.91766
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0436 - acc: 0.9965 - mDice: 0.9182 - val_loss: 0.0449 - val_acc: 0.9963 - val_mDice: 0.9159

Epoch 00014: val_mDice did not improve from 0.91766
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0436 - acc: 0.9965 - mDice: 0.9183 - val_loss: 0.0445 - val_acc: 0.9962 - val_mDice: 0.9166

Epoch 00015: val_mDice did not improve from 0.91766
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0428 - acc: 0.9965 - mDice: 0.9197 - val_loss: 0.0451 - val_acc: 0.9962 - val_mDice: 0.9157

Epoch 00016: val_mDice did not improve from 0.91766
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0432 - acc: 0.9965 - mDice: 0.9190 - val_loss: 0.0461 - val_acc: 0.9962 - val_mDice: 0.9138

Epoch 00017: val_mDice did not improve from 0.91766
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 5s - loss: 0.0428 - acc: 0.9965 - mDice: 0.9196 - val_loss: 0.0441 - val_acc: 0.9963 - val_mDice: 0.9174

Epoch 00018: val_mDice did not improve from 0.91766
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0419 - acc: 0.9966 - mDice: 0.9212 - val_loss: 0.0442 - val_acc: 0.9963 - val_mDice: 0.9173

Epoch 00019: val_mDice did not improve from 0.91766
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0414 - acc: 0.9966 - mDice: 0.9223 - val_loss: 0.0447 - val_acc: 0.9963 - val_mDice: 0.9163

Epoch 00020: val_mDice did not improve from 0.91766
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0413 - acc: 0.9966 - mDice: 0.9223 - val_loss: 0.0440 - val_acc: 0.9963 - val_mDice: 0.9175

Epoch 00021: val_mDice did not improve from 0.91766
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0415 - acc: 0.9966 - mDice: 0.9220 - val_loss: 0.0438 - val_acc: 0.9963 - val_mDice: 0.9179

Epoch 00022: val_mDice improved from 0.91766 to 0.91793, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0411 - acc: 0.9967 - mDice: 0.9226 - val_loss: 0.0445 - val_acc: 0.9963 - val_mDice: 0.9167

Epoch 00023: val_mDice did not improve from 0.91793
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0413 - acc: 0.9966 - mDice: 0.9224 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9157

Epoch 00024: val_mDice did not improve from 0.91793
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0410 - acc: 0.9967 - mDice: 0.9229 - val_loss: 0.0445 - val_acc: 0.9963 - val_mDice: 0.9166

Epoch 00025: val_mDice did not improve from 0.91793
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0414 - acc: 0.9966 - mDice: 0.9222 - val_loss: 0.0448 - val_acc: 0.9963 - val_mDice: 0.9162

Epoch 00026: val_mDice did not improve from 0.91793
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0415 - acc: 0.9966 - mDice: 0.9220 - val_loss: 0.0445 - val_acc: 0.9963 - val_mDice: 0.9167

Epoch 00027: val_mDice did not improve from 0.91793
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0411 - acc: 0.9966 - mDice: 0.9227 - val_loss: 0.0450 - val_acc: 0.9963 - val_mDice: 0.9158

Epoch 00028: val_mDice did not improve from 0.91793
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0410 - acc: 0.9967 - mDice: 0.9230 - val_loss: 0.0456 - val_acc: 0.9962 - val_mDice: 0.9147

Epoch 00029: val_mDice did not improve from 0.91793
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0406 - acc: 0.9967 - mDice: 0.9237 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9149

Epoch 00030: val_mDice did not improve from 0.91793
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0405 - acc: 0.9967 - mDice: 0.9237 - val_loss: 0.0448 - val_acc: 0.9963 - val_mDice: 0.9162

Epoch 00031: val_mDice did not improve from 0.91793
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0406 - acc: 0.9967 - mDice: 0.9236 - val_loss: 0.0442 - val_acc: 0.9963 - val_mDice: 0.9173

Epoch 00032: val_mDice did not improve from 0.91793
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0400 - acc: 0.9967 - mDice: 0.9248 - val_loss: 0.0448 - val_acc: 0.9963 - val_mDice: 0.9161

Epoch 00033: val_mDice did not improve from 0.91793
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0401 - acc: 0.9967 - mDice: 0.9244 - val_loss: 0.0442 - val_acc: 0.9963 - val_mDice: 0.9172

Epoch 00034: val_mDice did not improve from 0.91793
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0400 - acc: 0.9967 - mDice: 0.9248 - val_loss: 0.0448 - val_acc: 0.9963 - val_mDice: 0.9162

Epoch 00035: val_mDice did not improve from 0.91793
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 5s - loss: 0.0402 - acc: 0.9967 - mDice: 0.9244 - val_loss: 0.0446 - val_acc: 0.9963 - val_mDice: 0.9165

Epoch 00036: val_mDice did not improve from 0.91793
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0400 - acc: 0.9967 - mDice: 0.9246 - val_loss: 0.0451 - val_acc: 0.9962 - val_mDice: 0.9155

Epoch 00037: val_mDice did not improve from 0.91793
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0398 - acc: 0.9968 - mDice: 0.9252 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9152

Epoch 00038: val_mDice did not improve from 0.91793
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0402 - acc: 0.9967 - mDice: 0.9243 - val_loss: 0.0450 - val_acc: 0.9963 - val_mDice: 0.9158

Epoch 00039: val_mDice did not improve from 0.91793
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0396 - acc: 0.9968 - mDice: 0.9254 - val_loss: 0.0444 - val_acc: 0.9963 - val_mDice: 0.9169

Epoch 00040: val_mDice did not improve from 0.91793
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0393 - acc: 0.9968 - mDice: 0.9260 - val_loss: 0.0465 - val_acc: 0.9962 - val_mDice: 0.9131

Epoch 00041: val_mDice did not improve from 0.91793
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0395 - acc: 0.9968 - mDice: 0.9256 - val_loss: 0.0448 - val_acc: 0.9963 - val_mDice: 0.9161

Epoch 00042: val_mDice did not improve from 0.91793
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0394 - acc: 0.9968 - mDice: 0.9259 - val_loss: 0.0451 - val_acc: 0.9962 - val_mDice: 0.9155

Epoch 00043: val_mDice did not improve from 0.91793
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0391 - acc: 0.9968 - mDice: 0.9263 - val_loss: 0.0445 - val_acc: 0.9963 - val_mDice: 0.9166

Epoch 00044: val_mDice did not improve from 0.91793
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0396 - acc: 0.9968 - mDice: 0.9254 - val_loss: 0.0449 - val_acc: 0.9962 - val_mDice: 0.9159

Epoch 00045: val_mDice did not improve from 0.91793
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0394 - acc: 0.9968 - mDice: 0.9258 - val_loss: 0.0450 - val_acc: 0.9963 - val_mDice: 0.9157

Epoch 00046: val_mDice did not improve from 0.91793
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0393 - acc: 0.9968 - mDice: 0.9260 - val_loss: 0.0445 - val_acc: 0.9963 - val_mDice: 0.9167

Epoch 00047: val_mDice did not improve from 0.91793
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0392 - acc: 0.9968 - mDice: 0.9262 - val_loss: 0.0449 - val_acc: 0.9962 - val_mDice: 0.9159

Epoch 00048: val_mDice did not improve from 0.91793
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0389 - acc: 0.9968 - mDice: 0.9267 - val_loss: 0.0449 - val_acc: 0.9963 - val_mDice: 0.9159

Epoch 00049: val_mDice did not improve from 0.91793
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0390 - acc: 0.9968 - mDice: 0.9266 - val_loss: 0.0451 - val_acc: 0.9962 - val_mDice: 0.9155

Epoch 00050: val_mDice did not improve from 0.91793
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0388 - acc: 0.9968 - mDice: 0.9269 - val_loss: 0.0452 - val_acc: 0.9962 - val_mDice: 0.9153

Epoch 00051: val_mDice did not improve from 0.91793
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0392 - acc: 0.9968 - mDice: 0.9261 - val_loss: 0.0452 - val_acc: 0.9962 - val_mDice: 0.9155

Epoch 00052: val_mDice did not improve from 0.91793
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0390 - acc: 0.9968 - mDice: 0.9265 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9158

Epoch 00053: val_mDice did not improve from 0.91793
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 5s - loss: 0.0389 - acc: 0.9968 - mDice: 0.9268 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9148

Epoch 00054: val_mDice did not improve from 0.91793
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 5s - loss: 0.0387 - acc: 0.9968 - mDice: 0.9271 - val_loss: 0.0454 - val_acc: 0.9962 - val_mDice: 0.9150

Epoch 00055: val_mDice did not improve from 0.91793
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 5s - loss: 0.0390 - acc: 0.9968 - mDice: 0.9265 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9158

Epoch 00056: val_mDice did not improve from 0.91793
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 5s - loss: 0.0385 - acc: 0.9968 - mDice: 0.9274 - val_loss: 0.0454 - val_acc: 0.9962 - val_mDice: 0.9151

Epoch 00057: val_mDice did not improve from 0.91793
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 5s - loss: 0.0386 - acc: 0.9969 - mDice: 0.9273 - val_loss: 0.0452 - val_acc: 0.9962 - val_mDice: 0.9154

Epoch 00058: val_mDice did not improve from 0.91793
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0388 - acc: 0.9968 - mDice: 0.9269 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9152

Epoch 00059: val_mDice did not improve from 0.91793
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 5s - loss: 0.0388 - acc: 0.9968 - mDice: 0.9268 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9152

Epoch 00060: val_mDice did not improve from 0.91793
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 5s - loss: 0.0384 - acc: 0.9969 - mDice: 0.9277 - val_loss: 0.0451 - val_acc: 0.9962 - val_mDice: 0.9156

Epoch 00061: val_mDice did not improve from 0.91793
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 5s - loss: 0.0387 - acc: 0.9968 - mDice: 0.9271 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9153

Epoch 00062: val_mDice did not improve from 0.91793
Restoring model weights from the end of the best epoch
Epoch 00062: early stopping
{'val_loss': [0.051230835304048476, 0.048544010686498655, 0.04609677749539173, 0.045650395876954825, 0.04710946585395958, 0.04583688038681503, 0.04580784512198074, 0.04463069754676696, 0.04396097160299391, 0.04464247505738264, 0.04418484560270022, 0.04473445511137517, 0.04396920005409287, 0.04494196036697117, 0.044545449391220904, 0.04505812402355637, 0.046063848192080384, 0.044128246413260956, 0.04416659099708997, 0.044676561303500804, 0.04403203607331374, 0.04381038598652214, 0.04446787429829382, 0.045025245241159015, 0.044541675117090986, 0.044760396688066445, 0.04450299714646572, 0.04496483930672137, 0.045574532976550154, 0.04548994642829143, 0.04478022641899934, 0.0441510740635559, 0.04483721013525154, 0.04420516931353121, 0.04476605893718479, 0.04457957794980538, 0.045128883724567886, 0.04528858996180545, 0.04495631890600937, 0.04439404939853701, 0.04645130100171682, 0.04481489881659647, 0.045145158792037336, 0.04453252628445625, 0.04490762482528017, 0.04504497825330855, 0.04450567474423302, 0.04492487725308085, 0.0448940344112138, 0.04513606231627628, 0.04523881286766946, 0.04515430253308619, 0.04498826084727886, 0.04554827783502276, 0.0453909431880388, 0.044972980133441934, 0.04536607534888822, 0.045173252337641565, 0.04531466447478721, 0.0453180768407006, 0.045070413756250996, 0.045267359404304305], 'val_acc': [0.9958657501078608, 0.9960341267394474, 0.9961393784656907, 0.9961815568642494, 0.9961444038716292, 0.9962003651184476, 0.9961849275154507, 0.9962704159468839, 0.9963006138118427, 0.996287706929838, 0.9962838437290793, 0.996247747224518, 0.9962890474365912, 0.9962534298186316, 0.9962161950190634, 0.9962317332155725, 0.9962073134146311, 0.9962668529895793, 0.9962850872288461, 0.9962647294246706, 0.9962832512691574, 0.9962901930754369, 0.9962651241133069, 0.9962463942503178, 0.9962666593171805, 0.9962711860252315, 0.9962553622729458, 0.9962587488073332, 0.9962281511301297, 0.9962294221265952, 0.996251606497177, 0.9962721683917551, 0.9962556591006609, 0.996295788568207, 0.9962556509028875, 0.9962659751446677, 0.9962486197750343, 0.996235306420094, 0.996259995552052, 0.9962809536723489, 0.9961777872547379, 0.9962545047516809, 0.996239352021313, 0.9962648274563787, 0.9962483301203708, 0.9962528534126828, 0.9962655097502351, 0.9962363608587096, 0.9962502567679288, 0.9962316409906207, 0.9962296052102032, 0.9962474644013326, 0.9962466852712768, 0.9962177428611712, 0.9962259484908642, 0.9962332672239375, 0.9962206259156366, 0.9962225602486072, 0.9962164920175656, 0.9962159016070872, 0.9962326938921537, 0.9962324095318857], 'val_mDice': [0.9046202394204017, 0.9094172424095066, 0.9138346881442904, 0.9146318143623264, 0.9119484074477822, 0.9142503419031728, 0.9143318580009876, 0.9164376996649712, 0.9176628738214772, 0.9163941701708687, 0.9172328994062363, 0.9162483396366196, 0.9176302265301133, 0.9158732675891209, 0.9166349778885827, 0.9156591924691951, 0.9138291956360497, 0.9173506587146005, 0.9172721511996578, 0.9163418398545601, 0.9175213872532448, 0.9179298638614337, 0.9167261086084098, 0.9157132895764786, 0.9165920785983176, 0.9161878829357617, 0.9166701280626663, 0.9158148364214638, 0.914712516831122, 0.9148791025907741, 0.9161684781227549, 0.9173131540716548, 0.9160591439053117, 0.9171933058681324, 0.9161850294274382, 0.9165253272029253, 0.9155192716757001, 0.9152286778548385, 0.9158293739772457, 0.9168542927520664, 0.9131288480622037, 0.9160967136180845, 0.9154956052159853, 0.916610886340155, 0.9159312207241113, 0.9156753632605589, 0.9166580257579727, 0.915907601743168, 0.9159465061559376, 0.9155105039861618, 0.9153277771520751, 0.915472964331892, 0.9157772359670404, 0.9147623867906609, 0.9150493727372505, 0.9158094135942978, 0.9150986703897274, 0.9154470812283819, 0.9151904728146201, 0.915180880394911, 0.9156272937370235, 0.915266160295481], 'loss': [0.055900202447818556, 0.05073095932879856, 0.04910826160949845, 0.04781386375316457, 0.047458438805155594, 0.04736375130131342, 0.046038694707434415, 0.04591034031490411, 0.04538101421800688, 0.045066937513400185, 0.0445172132086355, 0.04363187165674667, 0.043915521145753256, 0.043597819345923605, 0.043583535516572264, 0.0427926710242454, 0.0431869933347055, 0.04283146385698956, 0.04192897566632267, 0.041355840321699484, 0.04133735977971864, 0.04152689151272011, 0.04114867954559929, 0.04130703140202508, 0.041007890757242545, 0.04141070371057464, 0.04150967477201086, 0.04110309862459016, 0.040974970422269684, 0.04055619166409216, 0.04054041751089149, 0.04061098903639166, 0.039953549195177934, 0.0401473345588131, 0.03995104502457225, 0.04017759916341438, 0.04003450165063032, 0.039755986401028794, 0.04020784879761114, 0.03962753275513206, 0.03927038866181799, 0.039507045552522276, 0.03935123956502592, 0.039132176680303415, 0.03960517015598963, 0.039417794438210564, 0.039271292979500105, 0.03918167282602158, 0.03888247365678996, 0.03895593589677243, 0.0388096037933596, 0.039224703183404576, 0.03900678007132945, 0.038850521994235346, 0.03868494576127113, 0.03900122682678212, 0.03850223569954195, 0.038585808321999794, 0.038765758248288394, 0.038832087817577625, 0.03837099024631277, 0.038654306217972674], 'acc': [0.9955259630228064, 0.9959202949442385, 0.9960323337285492, 0.9961185913989978, 0.996117058296629, 0.9961753718028724, 0.9962389659704329, 0.9962854323333967, 0.9963015789879299, 0.9963582869356007, 0.9963799500110867, 0.9964339258945565, 0.9964117425525055, 0.9964573077552824, 0.9964763389200969, 0.9965130868454405, 0.9965012490084624, 0.9965235557697962, 0.9965616149973249, 0.9966215041490293, 0.996627093026186, 0.9966163429185804, 0.9966573034963643, 0.9966300417942628, 0.9966722494164364, 0.9966444887193162, 0.9966307916162626, 0.9966468438783099, 0.9966678193953844, 0.9967115135884196, 0.9966945603877638, 0.9966726028343116, 0.9967316138700039, 0.9967170180441278, 0.9967440333508204, 0.996725005509685, 0.9967302192542633, 0.9967784061750958, 0.9967302892731025, 0.9967785304806933, 0.9967898518179429, 0.9967842884223257, 0.9968014674115802, 0.9968096773420568, 0.9967888744347158, 0.9967792834047935, 0.9967924265613343, 0.996799068158444, 0.9967947292061986, 0.9968213968560599, 0.9968196397377213, 0.9968163608175228, 0.9968144598946695, 0.9968148348056693, 0.9968471832878084, 0.9968410249536366, 0.9968340667207002, 0.9968630325838536, 0.9968360910628364, 0.9968369182157694, 0.9968598540387632, 0.9968404229245664], 'mDice': [0.8964599523845659, 0.9055263402293606, 0.9084042696261495, 0.9106934323186768, 0.9113474140379951, 0.9115171066890418, 0.9138516099036404, 0.9140985346638137, 0.915035430383505, 0.9155894617608932, 0.916604946980246, 0.9181678538871964, 0.9176625828760707, 0.9182282300686747, 0.9182783012939651, 0.9196791185765462, 0.9189757565140281, 0.9195945036898758, 0.921221555608799, 0.9222648449546785, 0.9222899097049103, 0.9219548952623814, 0.9226373680019024, 0.9223575266320466, 0.9228759516570648, 0.922151674346853, 0.9219634089771257, 0.9227370949925986, 0.922956316666089, 0.9237067041343916, 0.9237162231512672, 0.9236133867480054, 0.9247938906393086, 0.9244344456045158, 0.9247882474753937, 0.9243803436428197, 0.9246446217746096, 0.9251552463464134, 0.9243131036208908, 0.9253668758505782, 0.9260449511411022, 0.9255979025674133, 0.9258769369479892, 0.9262716021236433, 0.9254108707257807, 0.9257618742804545, 0.9260139946157604, 0.9261769218072572, 0.9267279618734764, 0.9266015949302446, 0.9268519182630631, 0.9261091329351234, 0.9264981245905937, 0.9267923572692729, 0.9270771347457148, 0.9265070229657963, 0.9274331198305888, 0.9272501563937248, 0.9269408514951685, 0.9268168835834942, 0.927664714896546, 0.9271415505267431], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:01<00:10,  1.03s/it]predicting test subjects:  18%|█▊        | 2/11 [00:01<00:07,  1.21it/s]predicting test subjects:  27%|██▋       | 3/11 [00:01<00:05,  1.44it/s]predicting test subjects:  36%|███▋      | 4/11 [00:02<00:04,  1.72it/s]predicting test subjects:  45%|████▌     | 5/11 [00:02<00:03,  1.91it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:02<00:02,  2.12it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:03<00:01,  2.33it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:03<00:01,  2.40it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:03<00:00,  2.43it/s]predicting test subjects:  91%|█████████ | 10/11 [00:04<00:00,  2.51it/s]predicting test subjects: 100%|██████████| 11/11 [00:04<00:00,  2.44it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:17,  2.32it/s]predicting train subjects:   5%|▍         | 2/41 [00:00<00:15,  2.47it/s]predicting train subjects:   7%|▋         | 3/41 [00:01<00:14,  2.60it/s]predicting train subjects:  10%|▉         | 4/41 [00:01<00:12,  2.97it/s]predicting train subjects:  12%|█▏        | 5/41 [00:01<00:13,  2.60it/s]predicting train subjects:  15%|█▍        | 6/41 [00:02<00:13,  2.59it/s]predicting train subjects:  17%|█▋        | 7/41 [00:02<00:13,  2.52it/s]predicting train subjects:  20%|█▉        | 8/41 [00:02<00:11,  2.83it/s]predicting train subjects:  22%|██▏       | 9/41 [00:03<00:10,  3.01it/s]predicting train subjects:  24%|██▍       | 10/41 [00:03<00:11,  2.80it/s]predicting train subjects:  27%|██▋       | 11/41 [00:03<00:10,  2.97it/s]predicting train subjects:  29%|██▉       | 12/41 [00:04<00:09,  3.08it/s]predicting train subjects:  32%|███▏      | 13/41 [00:04<00:09,  2.99it/s]predicting train subjects:  34%|███▍      | 14/41 [00:04<00:09,  2.81it/s]predicting train subjects:  37%|███▋      | 15/41 [00:05<00:08,  2.90it/s]predicting train subjects:  39%|███▉      | 16/41 [00:05<00:08,  3.05it/s]predicting train subjects:  41%|████▏     | 17/41 [00:06<00:08,  2.72it/s]predicting train subjects:  44%|████▍     | 18/41 [00:06<00:08,  2.87it/s]predicting train subjects:  46%|████▋     | 19/41 [00:06<00:07,  2.86it/s]predicting train subjects:  49%|████▉     | 20/41 [00:06<00:07,  2.91it/s]predicting train subjects:  51%|█████     | 21/41 [00:07<00:06,  2.94it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:07<00:06,  2.81it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:07<00:05,  3.18it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:08<00:05,  2.87it/s]predicting train subjects:  61%|██████    | 25/41 [00:08<00:05,  3.09it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:09<00:05,  2.72it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:09<00:05,  2.64it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:09<00:05,  2.57it/s]predicting train subjects:  71%|███████   | 29/41 [00:10<00:04,  2.48it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:10<00:04,  2.55it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:11<00:04,  2.35it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:11<00:03,  2.55it/s]predicting train subjects:  80%|████████  | 33/41 [00:11<00:03,  2.61it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:12<00:03,  2.24it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:12<00:02,  2.18it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:13<00:02,  2.07it/s]predicting train subjects:  90%|█████████ | 37/41 [00:13<00:01,  2.09it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:14<00:01,  2.13it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:14<00:00,  2.46it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:15<00:00,  2.41it/s]predicting train subjects: 100%|██████████| 41/41 [00:15<00:00,  2.51it/s]
predicting test subjects sagittal:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects sagittal:   9%|▉         | 1/11 [00:00<00:03,  2.88it/s]predicting test subjects sagittal:  18%|█▊        | 2/11 [00:00<00:02,  3.12it/s]predicting test subjects sagittal:  27%|██▋       | 3/11 [00:01<00:02,  2.77it/s]predicting test subjects sagittal:  36%|███▋      | 4/11 [00:01<00:02,  3.10it/s]predicting test subjects sagittal:  45%|████▌     | 5/11 [00:01<00:01,  3.18it/s]predicting test subjects sagittal:  55%|█████▍    | 6/11 [00:01<00:01,  3.04it/s]predicting test subjects sagittal:  64%|██████▎   | 7/11 [00:02<00:01,  3.00it/s]predicting test subjects sagittal:  73%|███████▎  | 8/11 [00:02<00:01,  2.88it/s]predicting test subjects sagittal:  82%|████████▏ | 9/11 [00:02<00:00,  3.27it/s]predicting test subjects sagittal:  91%|█████████ | 10/11 [00:03<00:00,  3.27it/s]predicting test subjects sagittal: 100%|██████████| 11/11 [00:03<00:00,  3.09it/s]
predicting train subjects sagittal:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects sagittal:   2%|▏         | 1/41 [00:00<00:16,  2.44it/s]predicting train subjects sagittal:   5%|▍         | 2/41 [00:00<00:15,  2.54it/s]predicting train subjects sagittal:   7%|▋         | 3/41 [00:01<00:14,  2.69it/s]predicting train subjects sagittal:  10%|▉         | 4/41 [00:01<00:13,  2.77it/s]predicting train subjects sagittal:  12%|█▏        | 5/41 [00:01<00:12,  3.00it/s]predicting train subjects sagittal:  15%|█▍        | 6/41 [00:02<00:12,  2.82it/s]predicting train subjects sagittal:  17%|█▋        | 7/41 [00:02<00:10,  3.13it/s]predicting train subjects sagittal:  20%|█▉        | 8/41 [00:02<00:09,  3.62it/s]predicting train subjects sagittal:  22%|██▏       | 9/41 [00:02<00:10,  3.15it/s]predicting train subjects sagittal:  24%|██▍       | 10/41 [00:03<00:09,  3.13it/s]predicting train subjects sagittal:  27%|██▋       | 11/41 [00:03<00:10,  2.98it/s]predicting train subjects sagittal:  29%|██▉       | 12/41 [00:03<00:08,  3.40it/s]predicting train subjects sagittal:  32%|███▏      | 13/41 [00:04<00:08,  3.32it/s]predicting train subjects sagittal:  34%|███▍      | 14/41 [00:04<00:08,  3.14it/s]predicting train subjects sagittal:  37%|███▋      | 15/41 [00:04<00:07,  3.45it/s]predicting train subjects sagittal:  39%|███▉      | 16/41 [00:05<00:08,  3.00it/s]predicting train subjects sagittal:  41%|████▏     | 17/41 [00:05<00:08,  2.91it/s]predicting train subjects sagittal:  44%|████▍     | 18/41 [00:05<00:08,  2.86it/s]predicting train subjects sagittal:  46%|████▋     | 19/41 [00:06<00:06,  3.15it/s]predicting train subjects sagittal:  49%|████▉     | 20/41 [00:06<00:05,  3.62it/s]predicting train subjects sagittal:  51%|█████     | 21/41 [00:06<00:05,  3.58it/s]predicting train subjects sagittal:  54%|█████▎    | 22/41 [00:06<00:05,  3.41it/s]predicting train subjects sagittal:  56%|█████▌    | 23/41 [00:07<00:04,  3.83it/s]predicting train subjects sagittal:  59%|█████▊    | 24/41 [00:07<00:05,  3.28it/s]predicting train subjects sagittal:  61%|██████    | 25/41 [00:07<00:04,  3.27it/s]predicting train subjects sagittal:  63%|██████▎   | 26/41 [00:08<00:04,  3.43it/s]predicting train subjects sagittal:  66%|██████▌   | 27/41 [00:08<00:04,  2.97it/s]predicting train subjects sagittal:  68%|██████▊   | 28/41 [00:08<00:04,  3.05it/s]predicting train subjects sagittal:  71%|███████   | 29/41 [00:09<00:03,  3.15it/s]predicting train subjects sagittal:  73%|███████▎  | 30/41 [00:09<00:03,  3.13it/s]predicting train subjects sagittal:  76%|███████▌  | 31/41 [00:09<00:03,  3.19it/s]predicting train subjects sagittal:  78%|███████▊  | 32/41 [00:10<00:02,  3.15it/s]predicting train subjects sagittal:  80%|████████  | 33/41 [00:10<00:02,  3.14it/s]predicting train subjects sagittal:  83%|████████▎ | 34/41 [00:10<00:02,  2.88it/s]predicting train subjects sagittal:  85%|████████▌ | 35/41 [00:11<00:02,  2.51it/s]predicting train subjects sagittal:  88%|████████▊ | 36/41 [00:11<00:02,  2.35it/s]predicting train subjects sagittal:  90%|█████████ | 37/41 [00:12<00:01,  2.42it/s]predicting train subjects sagittal:  93%|█████████▎| 38/41 [00:12<00:01,  2.46it/s]predicting train subjects sagittal:  95%|█████████▌| 39/41 [00:12<00:00,  2.75it/s]predicting train subjects sagittal:  98%|█████████▊| 40/41 [00:13<00:00,  2.75it/s]predicting train subjects sagittal: 100%|██████████| 41/41 [00:13<00:00,  2.78it/s]
saving BB  test1-THALAMUS:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  73%|███████▎  | 8/11 [00:00<00:00, 75.26it/s]saving BB  test1-THALAMUS: 100%|██████████| 11/11 [00:00<00:00, 72.64it/s]
saving BB  train1-THALAMUS:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS:  22%|██▏       | 9/41 [00:00<00:00, 85.33it/s]saving BB  train1-THALAMUS:  41%|████▏     | 17/41 [00:00<00:00, 82.94it/s]saving BB  train1-THALAMUS:  63%|██████▎   | 26/41 [00:00<00:00, 81.99it/s]saving BB  train1-THALAMUS:  83%|████████▎ | 34/41 [00:00<00:00, 78.52it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 40/41 [00:00<00:00, 63.61it/s]saving BB  train1-THALAMUS: 100%|██████████| 41/41 [00:00<00:00, 69.63it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal:  55%|█████▍    | 6/11 [00:00<00:00, 22.00it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 11/11 [00:00<00:00, 32.72it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:  22%|██▏       | 9/41 [00:00<00:00, 82.73it/s]saving BB  train1-THALAMUS Sagittal:  41%|████▏     | 17/41 [00:00<00:00, 78.28it/s]saving BB  train1-THALAMUS Sagittal:  61%|██████    | 25/41 [00:00<00:00, 76.96it/s]saving BB  train1-THALAMUS Sagittal:  80%|████████  | 33/41 [00:00<00:00, 76.78it/s]saving BB  train1-THALAMUS Sagittal:  95%|█████████▌| 39/41 [00:00<00:00, 70.76it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 41/41 [00:00<00:00, 72.31it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:29,  1.35it/s]Loading train:   5%|▍         | 2/41 [00:01<00:28,  1.35it/s]Loading train:   7%|▋         | 3/41 [00:02<00:27,  1.40it/s]Loading train:  10%|▉         | 4/41 [00:02<00:24,  1.51it/s]Loading train:  12%|█▏        | 5/41 [00:03<00:22,  1.58it/s]Loading train:  15%|█▍        | 6/41 [00:03<00:22,  1.56it/s]Loading train:  17%|█▋        | 7/41 [00:04<00:21,  1.56it/s]Loading train:  20%|█▉        | 8/41 [00:04<00:18,  1.79it/s]Loading train:  22%|██▏       | 9/41 [00:05<00:19,  1.61it/s]Loading train:  24%|██▍       | 10/41 [00:06<00:21,  1.47it/s]Loading train:  27%|██▋       | 11/41 [00:07<00:21,  1.42it/s]Loading train:  29%|██▉       | 12/41 [00:07<00:18,  1.60it/s]Loading train:  32%|███▏      | 13/41 [00:08<00:16,  1.71it/s]Loading train:  34%|███▍      | 14/41 [00:08<00:15,  1.79it/s]Loading train:  37%|███▋      | 15/41 [00:08<00:12,  2.07it/s]Loading train:  39%|███▉      | 16/41 [00:09<00:12,  1.95it/s]Loading train:  41%|████▏     | 17/41 [00:10<00:14,  1.67it/s]Loading train:  44%|████▍     | 18/41 [00:11<00:14,  1.56it/s]Loading train:  46%|████▋     | 19/41 [00:11<00:14,  1.52it/s]Loading train:  49%|████▉     | 20/41 [00:12<00:13,  1.54it/s]Loading train:  51%|█████     | 21/41 [00:13<00:13,  1.47it/s]Loading train:  54%|█████▎    | 22/41 [00:13<00:13,  1.41it/s]Loading train:  56%|█████▌    | 23/41 [00:14<00:10,  1.73it/s]Loading train:  59%|█████▊    | 24/41 [00:14<00:09,  1.76it/s]Loading train:  61%|██████    | 25/41 [00:15<00:08,  1.80it/s]Loading train:  63%|██████▎   | 26/41 [00:16<00:09,  1.66it/s]Loading train:  66%|██████▌   | 27/41 [00:16<00:08,  1.57it/s]Loading train:  68%|██████▊   | 28/41 [00:17<00:08,  1.62it/s]Loading train:  71%|███████   | 29/41 [00:17<00:06,  1.75it/s]Loading train:  73%|███████▎  | 30/41 [00:18<00:06,  1.58it/s]Loading train:  76%|███████▌  | 31/41 [00:19<00:06,  1.61it/s]Loading train:  78%|███████▊  | 32/41 [00:19<00:05,  1.79it/s]Loading train:  80%|████████  | 33/41 [00:20<00:05,  1.56it/s]Loading train:  83%|████████▎ | 34/41 [00:21<00:04,  1.47it/s]Loading train:  85%|████████▌ | 35/41 [00:21<00:04,  1.40it/s]Loading train:  88%|████████▊ | 36/41 [00:22<00:03,  1.35it/s]Loading train:  90%|█████████ | 37/41 [00:23<00:02,  1.36it/s]Loading train:  93%|█████████▎| 38/41 [00:24<00:02,  1.39it/s]Loading train:  95%|█████████▌| 39/41 [00:24<00:01,  1.47it/s]Loading train:  98%|█████████▊| 40/41 [00:25<00:00,  1.51it/s]Loading train: 100%|██████████| 41/41 [00:26<00:00,  1.45it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   5%|▍         | 2/41 [00:00<00:04,  9.50it/s]concatenating: train:  27%|██▋       | 11/41 [00:00<00:02, 12.95it/s]concatenating: train:  63%|██████▎   | 26/41 [00:00<00:00, 17.83it/s]concatenating: train:  93%|█████████▎| 38/41 [00:00<00:00, 23.94it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 76.64it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:05,  1.77it/s]Loading test:  18%|█▊        | 2/11 [00:01<00:05,  1.77it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:04,  1.61it/s]Loading test:  36%|███▋      | 4/11 [00:02<00:04,  1.54it/s]Loading test:  45%|████▌     | 5/11 [00:03<00:04,  1.43it/s]Loading test:  55%|█████▍    | 6/11 [00:04<00:03,  1.49it/s]Loading test:  64%|██████▎   | 7/11 [00:04<00:02,  1.49it/s]Loading test:  73%|███████▎  | 8/11 [00:05<00:01,  1.58it/s]Loading test:  82%|████████▏ | 9/11 [00:05<00:01,  1.55it/s]Loading test:  91%|█████████ | 10/11 [00:06<00:00,  1.49it/s]Loading test: 100%|██████████| 11/11 [00:07<00:00,  1.43it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:   9%|▉         | 1/11 [00:00<00:01,  6.48it/s]concatenating: validation:  18%|█▊        | 2/11 [00:00<00:01,  7.18it/s]concatenating: validation:  27%|██▋       | 3/11 [00:00<00:01,  6.65it/s]concatenating: validation:  64%|██████▎   | 7/11 [00:00<00:00,  8.87it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 18.09it/s]
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 15 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 92, 116, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 92, 116, 15)  150         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 92, 116, 15)  60          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 92, 116, 15)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 92, 116, 15)  2040        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 92, 116, 15)  60          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 92, 116, 15)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 46, 58, 15)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 46, 58, 15)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 46, 58, 30)   4080        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 46, 58, 30)   120         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 46, 58, 30)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 46, 58, 30)   8130        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 46, 58, 30)   120         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 46, 58, 30)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 46, 58, 45)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 23, 29, 45)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 23, 29, 45)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 23, 29, 60)   24360       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 23, 29, 60)   240         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 23, 29, 60)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 23, 29, 60)   32460       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 23, 29, 60)   240         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 23, 29, 60)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 23, 29, 105)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 23, 29, 105)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 46, 58, 30)   12630       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 46, 58, 75)   0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 46, 58, 30)   20280       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 46, 58, 30)   120         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 58, 30)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 58, 30)   8130        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 46, 58, 30)   120         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 46, 58, 30)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 46, 58, 105)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 46, 58, 105)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 92, 116, 15)  6315        dropout_4[0][0]                  2019-07-27 17:49:34.358882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-27 17:49:34.358985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-27 17:49:34.359006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-27 17:49:34.359020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-27 17:49:34.359485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 92, 116, 30)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 92, 116, 15)  4065        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 92, 116, 15)  60          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 92, 116, 15)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 92, 116, 15)  2040        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 92, 116, 15)  60          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 92, 116, 15)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 92, 116, 45)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 92, 116, 45)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 92, 116, 2)   92          dropout_5[0][0]                  
==================================================================================================
Total params: 125,972
Trainable params: 125,372
Non-trainable params: 600
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.98000917 0.01999083]
Train on 4060 samples, validate on 1096 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.4076 - acc: 0.7607 - mDice: 0.0781 - val_loss: 1.5534 - val_acc: 0.3792 - val_mDice: 0.0478

Epoch 00001: val_mDice improved from -inf to 0.04784, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.5057 - acc: 0.9864 - mDice: 0.4404 - val_loss: 1.7056 - val_acc: 0.9798 - val_mDice: 0.0337

Epoch 00002: val_mDice did not improve from 0.04784
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.1400 - acc: 0.9892 - mDice: 0.7793 - val_loss: 2.8216 - val_acc: 0.9798 - val_mDice: 0.0045

Epoch 00003: val_mDice did not improve from 0.04784
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.1011 - acc: 0.9903 - mDice: 0.8340 - val_loss: 0.6097 - val_acc: 0.9799 - val_mDice: 0.3063

Epoch 00004: val_mDice improved from 0.04784 to 0.30628, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0903 - acc: 0.9911 - mDice: 0.8505 - val_loss: 0.3105 - val_acc: 0.9818 - val_mDice: 0.5475

Epoch 00005: val_mDice improved from 0.30628 to 0.54750, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0818 - acc: 0.9919 - mDice: 0.8638 - val_loss: 0.1582 - val_acc: 0.9858 - val_mDice: 0.7375

Epoch 00006: val_mDice improved from 0.54750 to 0.73746, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0803 - acc: 0.9921 - mDice: 0.8653 - val_loss: 0.0928 - val_acc: 0.9899 - val_mDice: 0.8398

Epoch 00007: val_mDice improved from 0.73746 to 0.83977, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0755 - acc: 0.9926 - mDice: 0.8726 - val_loss: 0.1135 - val_acc: 0.9886 - val_mDice: 0.8057

Epoch 00008: val_mDice did not improve from 0.83977
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0727 - acc: 0.9929 - mDice: 0.8767 - val_loss: 0.1129 - val_acc: 0.9891 - val_mDice: 0.8067

Epoch 00009: val_mDice did not improve from 0.83977
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0697 - acc: 0.9934 - mDice: 0.8814 - val_loss: 0.0940 - val_acc: 0.9909 - val_mDice: 0.8375

Epoch 00010: val_mDice did not improve from 0.83977
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0676 - acc: 0.9939 - mDice: 0.8847 - val_loss: 0.0793 - val_acc: 0.9924 - val_mDice: 0.8624

Epoch 00011: val_mDice improved from 0.83977 to 0.86243, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0664 - acc: 0.9942 - mDice: 0.8863 - val_loss: 0.0791 - val_acc: 0.9928 - val_mDice: 0.8625

Epoch 00012: val_mDice improved from 0.86243 to 0.86253, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0657 - acc: 0.9944 - mDice: 0.8870 - val_loss: 0.0643 - val_acc: 0.9941 - val_mDice: 0.8874

Epoch 00013: val_mDice improved from 0.86253 to 0.88740, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0642 - acc: 0.9947 - mDice: 0.8890 - val_loss: 0.0578 - val_acc: 0.9947 - val_mDice: 0.8985

Epoch 00014: val_mDice improved from 0.88740 to 0.89852, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0633 - acc: 0.9949 - mDice: 0.8901 - val_loss: 0.0542 - val_acc: 0.9953 - val_mDice: 0.9050

Epoch 00015: val_mDice improved from 0.89852 to 0.90497, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0624 - acc: 0.9950 - mDice: 0.8914 - val_loss: 0.0518 - val_acc: 0.9958 - val_mDice: 0.9091

Epoch 00016: val_mDice improved from 0.90497 to 0.90913, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0605 - acc: 0.9953 - mDice: 0.8943 - val_loss: 0.0523 - val_acc: 0.9957 - val_mDice: 0.9082

Epoch 00017: val_mDice did not improve from 0.90913
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0587 - acc: 0.9956 - mDice: 0.8969 - val_loss: 0.0533 - val_acc: 0.9957 - val_mDice: 0.9052

Epoch 00018: val_mDice did not improve from 0.90913
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0585 - acc: 0.9956 - mDice: 0.8965 - val_loss: 0.0552 - val_acc: 0.9956 - val_mDice: 0.9014

Epoch 00019: val_mDice did not improve from 0.90913
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0567 - acc: 0.9958 - mDice: 0.8990 - val_loss: 0.0542 - val_acc: 0.9958 - val_mDice: 0.9024

Epoch 00020: val_mDice did not improve from 0.90913
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0553 - acc: 0.9960 - mDice: 0.9008 - val_loss: 0.0516 - val_acc: 0.9960 - val_mDice: 0.9062

Epoch 00021: val_mDice did not improve from 0.90913
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0547 - acc: 0.9960 - mDice: 0.9012 - val_loss: 0.0486 - val_acc: 0.9963 - val_mDice: 0.9101

Epoch 00022: val_mDice improved from 0.90913 to 0.91012, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0547 - acc: 0.9960 - mDice: 0.9007 - val_loss: 0.0489 - val_acc: 0.9963 - val_mDice: 0.9093

Epoch 00023: val_mDice did not improve from 0.91012
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0539 - acc: 0.9961 - mDice: 0.9017 - val_loss: 0.0483 - val_acc: 0.9963 - val_mDice: 0.9102

Epoch 00024: val_mDice improved from 0.91012 to 0.91019, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0536 - acc: 0.9961 - mDice: 0.9020 - val_loss: 0.0502 - val_acc: 0.9962 - val_mDice: 0.9068

Epoch 00025: val_mDice did not improve from 0.91019
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0525 - acc: 0.9962 - mDice: 0.9039 - val_loss: 0.0466 - val_acc: 0.9965 - val_mDice: 0.9130

Epoch 00026: val_mDice improved from 0.91019 to 0.91304, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0523 - acc: 0.9962 - mDice: 0.9041 - val_loss: 0.0473 - val_acc: 0.9964 - val_mDice: 0.9118

Epoch 00027: val_mDice did not improve from 0.91304
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0520 - acc: 0.9962 - mDice: 0.9045 - val_loss: 0.0475 - val_acc: 0.9964 - val_mDice: 0.9113

Epoch 00028: val_mDice did not improve from 0.91304
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0516 - acc: 0.9963 - mDice: 0.9051 - val_loss: 0.0455 - val_acc: 0.9966 - val_mDice: 0.9148

Epoch 00029: val_mDice improved from 0.91304 to 0.91482, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0514 - acc: 0.9963 - mDice: 0.9054 - val_loss: 0.0467 - val_acc: 0.9965 - val_mDice: 0.9127

Epoch 00030: val_mDice did not improve from 0.91482
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0513 - acc: 0.9963 - mDice: 0.9056 - val_loss: 0.0461 - val_acc: 0.9965 - val_mDice: 0.9139

Epoch 00031: val_mDice did not improve from 0.91482
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0505 - acc: 0.9963 - mDice: 0.9070 - val_loss: 0.0474 - val_acc: 0.9964 - val_mDice: 0.9115

Epoch 00032: val_mDice did not improve from 0.91482
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0498 - acc: 0.9964 - mDice: 0.9081 - val_loss: 0.0475 - val_acc: 0.9964 - val_mDice: 0.9114

Epoch 00033: val_mDice did not improve from 0.91482
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0505 - acc: 0.9963 - mDice: 0.9068 - val_loss: 0.0464 - val_acc: 0.9965 - val_mDice: 0.9132

Epoch 00034: val_mDice did not improve from 0.91482
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0503 - acc: 0.9964 - mDice: 0.9072 - val_loss: 0.0464 - val_acc: 0.9965 - val_mDice: 0.9133

Epoch 00035: val_mDice did not improve from 0.91482
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0501 - acc: 0.9964 - mDice: 0.9075 - val_loss: 0.0459 - val_acc: 0.9965 - val_mDice: 0.9141

Epoch 00036: val_mDice did not improve from 0.91482
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0489 - acc: 0.9965 - mDice: 0.9096 - val_loss: 0.0455 - val_acc: 0.9966 - val_mDice: 0.9148

Epoch 00037: val_mDice improved from 0.91482 to 0.91483, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0487 - acc: 0.9965 - mDice: 0.9099 - val_loss: 0.0458 - val_acc: 0.9965 - val_mDice: 0.9144

Epoch 00038: val_mDice did not improve from 0.91483
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0484 - acc: 0.9965 - mDice: 0.9104 - val_loss: 0.0457 - val_acc: 0.9966 - val_mDice: 0.9145

Epoch 00039: val_mDice did not improve from 0.91483
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0483 - acc: 0.9965 - mDice: 0.9105 - val_loss: 0.0472 - val_acc: 0.9965 - val_mDice: 0.9119

Epoch 00040: val_mDice did not improve from 0.91483
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0485 - acc: 0.9965 - mDice: 0.9102 - val_loss: 0.0477 - val_acc: 0.9964 - val_mDice: 0.9109

Epoch 00041: val_mDice did not improve from 0.91483
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0484 - acc: 0.9965 - mDice: 0.9104 - val_loss: 0.0453 - val_acc: 0.9966 - val_mDice: 0.9153

Epoch 00042: val_mDice improved from 0.91483 to 0.91526, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0475 - acc: 0.9965 - mDice: 0.9120 - val_loss: 0.0466 - val_acc: 0.9965 - val_mDice: 0.9128

Epoch 00043: val_mDice did not improve from 0.91526
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0474 - acc: 0.9966 - mDice: 0.9122 - val_loss: 0.0454 - val_acc: 0.9966 - val_mDice: 0.9151

Epoch 00044: val_mDice did not improve from 0.91526
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0472 - acc: 0.9966 - mDice: 0.9125 - val_loss: 0.0458 - val_acc: 0.9966 - val_mDice: 0.9143

Epoch 00045: val_mDice did not improve from 0.91526
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0479 - acc: 0.9965 - mDice: 0.9113 - val_loss: 0.0454 - val_acc: 0.9966 - val_mDice: 0.9151

Epoch 00046: val_mDice did not improve from 0.91526
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0477 - acc: 0.9965 - mDice: 0.9115 - val_loss: 0.0452 - val_acc: 0.9966 - val_mDice: 0.9154

Epoch 00047: val_mDice improved from 0.91526 to 0.91542, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0474 - acc: 0.9965 - mDice: 0.9120 - val_loss: 0.0450 - val_acc: 0.9966 - val_mDice: 0.9158

Epoch 00048: val_mDice improved from 0.91542 to 0.91575, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM15_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0478 - acc: 0.9965 - mDice: 0.9114 - val_loss: 0.0456 - val_acc: 0.9966 - val_mDice: 0.9146

Epoch 00049: val_mDice did not improve from 0.91575
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0473 - acc: 0.9966 - mDice: 0.9123 - val_loss: 0.0463 - val_acc: 0.9965 - val_mDice: 0.9135

Epoch 00050: val_mDice did not improve from 0.91575
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0472 - acc: 0.9966 - mDice: 0.9123 - val_loss: 0.0459 - val_acc: 0.9966 - val_mDice: 0.9141

Epoch 00051: val_mDice did not improve from 0.91575
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0469 - acc: 0.9966 - mDice: 0.9130 - val_loss: 0.0458 - val_acc: 0.9966 - val_mDice: 0.9143

Epoch 00052: val_mDice did not improve from 0.91575
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0466 - acc: 0.9966 - mDice: 0.9135 - val_loss: 0.0453 - val_acc: 0.9966 - val_mDice: 0.9152

Epoch 00053: val_mDice did not improve from 0.91575
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0464 - acc: 0.9966 - mDice: 0.9138 - val_loss: 0.0457 - val_acc: 0.9966 - val_mDice: 0.9145

Epoch 00054: val_mDice did not improve from 0.91575
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0461 - acc: 0.9966 - mDice: 0.9142 - val_loss: 0.0455 - val_acc: 0.9966 - val_mDice: 0.9148

Epoch 00055: val_mDice did not improve from 0.91575
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0456 - acc: 0.9967 - mDice: 0.9152 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9137

Epoch 00056: val_mDice did not improve from 0.91575
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0462 - acc: 0.9966 - mDice: 0.9142 - val_loss: 0.0464 - val_acc: 0.9965 - val_mDice: 0.9133

Epoch 00057: val_mDice did not improve from 0.91575
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0458 - acc: 0.9967 - mDice: 0.9148 - val_loss: 0.0453 - val_acc: 0.9966 - val_mDice: 0.9151

Epoch 00058: val_mDice did not improve from 0.91575
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0457 - acc: 0.9967 - mDice: 0.9151 - val_loss: 0.0452 - val_acc: 0.9966 - val_mDice: 0.9153

Epoch 00059: val_mDice did not improve from 0.91575
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0457 - acc: 0.9967 - mDice: 0.9150 - val_loss: 0.0461 - val_acc: 0.9965 - val_mDice: 0.9137

Epoch 00060: val_mDice did not improve from 0.91575
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0459 - acc: 0.9966 - mDice: 0.9146 - val_loss: 0.0455 - val_acc: 0.9966 - val_mDice: 0.9148

Epoch 00061: val_mDice did not improve from 0.91575
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0454 - acc: 0.9967 - mDice: 0.9155 - val_loss: 0.0458 - val_acc: 0.9966 - val_mDice: 0.9143

Epoch 00062: val_mDice did not improve from 0.91575
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0452 - acc: 0.9967 - mDice: 0.9159 - val_loss: 0.0458 - val_acc: 0.9966 - val_mDice: 0.9143

Epoch 00063: val_mDice did not improve from 0.91575
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0455 - acc: 0.9967 - mDice: 0.9153 - val_loss: 0.0458 - val_acc: 0.9966 - val_mDice: 0.9142

Epoch 00064: val_mDice did not improve from 0.91575
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0451 - acc: 0.9967 - mDice: 0.9160 - val_loss: 0.0460 - val_acc: 0.9966 - val_mDice: 0.9140

Epoch 00065: val_mDice did not improve from 0.91575
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0451 - acc: 0.9967 - mDice: 0.9160 - val_loss: 0.0454 - val_acc: 0.9966 - val_mDice: 0.9150

Epoch 00066: val_mDice did not improve from 0.91575
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0454 - acc: 0.9967 - mDice: 0.9156 - val_loss: 0.0462 - val_acc: 0.9966 - val_mDice: 0.9136

Epoch 00067: val_mDice did not improve from 0.91575
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0451 - acc: 0.9967 - mDice: 0.9160 - val_loss: 0.0458 - val_acc: 0.9966 - val_mDice: 0.9143

Epoch 00068: val_mDice did not improve from 0.91575
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0451 - acc: 0.9967 - mDice: 0.9160 - val_loss: 0.0456 - val_acc: 0.9966 - val_mDice: 0.9146

Epoch 00069: val_mDice did not improve from 0.91575
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0453 - acc: 0.9967 - mDice: 0.9156 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9138

Epoch 00070: val_mDice did not improve from 0.91575
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0450 - acc: 0.9967 - mDice: 0.9162 - val_loss: 0.0467 - val_acc: 0.9965 - val_mDice: 0.9127

Epoch 00071: val_mDice did not improve from 0.91575
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0450 - acc: 0.9967 - mDice: 0.9162 - val_loss: 0.0466 - val_acc: 0.9965 - val_mDice: 0.9129

Epoch 00072: val_mDice did not improve from 0.91575
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0445 - acc: 0.9967 - mDice: 0.9171 - val_loss: 0.0463 - val_acc: 0.9965 - val_mDice: 0.9133

Epoch 00073: val_mDice did not improve from 0.91575
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0446 - acc: 0.9967 - mDice: 0.9169 - val_loss: 0.0454 - val_acc: 0.9966 - val_mDice: 0.9150

Epoch 00074: val_mDice did not improve from 0.91575
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0447 - acc: 0.9967 - mDice: 0.9166 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9138

Epoch 00075: val_mDice did not improve from 0.91575
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0446 - acc: 0.9967 - mDice: 0.9168 - val_loss: 0.0457 - val_acc: 0.9966 - val_mDice: 0.9144

Epoch 00076: val_mDice did not improve from 0.91575
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0447 - acc: 0.9967 - mDice: 0.9168 - val_loss: 0.0456 - val_acc: 0.9966 - val_mDice: 0.9145

Epoch 00077: val_mDice did not improve from 0.91575
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0442 - acc: 0.9968 - mDice: 0.9175 - val_loss: 0.0459 - val_acc: 0.9966 - val_mDice: 0.9141

Epoch 00078: val_mDice did not improve from 0.91575
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0442 - acc: 0.9968 - mDice: 0.9175 - val_loss: 0.0458 - val_acc: 0.9966 - val_mDice: 0.9143

Epoch 00079: val_mDice did not improve from 0.91575
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0444 - acc: 0.9968 - mDice: 0.9172 - val_loss: 0.0459 - val_acc: 0.9966 - val_mDice: 0.9140

Epoch 00080: val_mDice did not improve from 0.91575
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0445 - acc: 0.9967 - mDice: 0.9171 - val_loss: 0.0455 - val_acc: 0.9966 - val_mDice: 0.9147

Epoch 00081: val_mDice did not improve from 0.91575
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0441 - acc: 0.9968 - mDice: 0.9177 - val_loss: 0.0452 - val_acc: 0.9966 - val_mDice: 0.9153

Epoch 00082: val_mDice did not improve from 0.91575
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0444 - acc: 0.9968 - mDice: 0.9173 - val_loss: 0.0454 - val_acc: 0.9966 - val_mDice: 0.9150

Epoch 00083: val_mDice did not improve from 0.91575
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0440 - acc: 0.9968 - mDice: 0.9179 - val_loss: 0.0457 - val_acc: 0.9966 - val_mDice: 0.9144

Epoch 00084: val_mDice did not improve from 0.91575
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0443 - acc: 0.9968 - mDice: 0.9173 - val_loss: 0.0460 - val_acc: 0.9966 - val_mDice: 0.9138

Epoch 00085: val_mDice did not improve from 0.91575
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0440 - acc: 0.9968 - mDice: 0.9179 - val_loss: 0.0457 - val_acc: 0.9966 - val_mDice: 0.9145

Epoch 00086: val_mDice did not improve from 0.91575
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0442 - acc: 0.9968 - mDice: 0.9175 - val_loss: 0.0452 - val_acc: 0.9966 - val_mDice: 0.9153

Epoch 00087: val_mDice did not improve from 0.91575
Epoch 88/300

Epoch 00088: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0442 - acc: 0.9968 - mDice: 0.9175 - val_loss: 0.0453 - val_acc: 0.9966 - val_mDice: 0.9152

Epoch 00088: val_mDice did not improve from 0.91575
Restoring model weights from the end of the best epoch
Epoch 00088: early stopping
{'val_loss': [1.5534394413885408, 1.70555580749999, 2.821619066008686, 0.6096814928481179, 0.31045725512026, 0.15817100043496946, 0.09276310914624346, 0.11345649842363205, 0.11292925903940723, 0.0939721109712646, 0.07934245869626094, 0.07911518173985672, 0.0643403654795711, 0.05781431500222126, 0.05416863565299198, 0.05183681636287348, 0.052286643480514955, 0.053333477205494896, 0.05519763504012223, 0.054215770418735314, 0.05160782464446813, 0.04864093381231719, 0.04890936851012011, 0.0483324501500295, 0.050232623520232465, 0.046603193560982276, 0.04733077439404752, 0.04754120772228624, 0.04554909784482778, 0.046733036406175064, 0.04610068805135079, 0.04741289720863757, 0.0474985553373168, 0.04641669523650712, 0.04636781710288385, 0.045903336854964276, 0.04550169360735556, 0.04578397117120071, 0.04567914316090789, 0.047175488223994734, 0.04769478962641128, 0.04525668827993591, 0.04662259389394826, 0.045354810658923904, 0.04580160526789888, 0.0453510562596965, 0.04517057131513627, 0.044975964828346766, 0.04563518659803119, 0.04625160898333484, 0.04591620732506696, 0.0458192348915295, 0.04527140736416744, 0.045654194837395294, 0.04553103305562569, 0.046098794992061425, 0.04635852284348794, 0.04534638223041148, 0.04523352850364507, 0.04614333208840694, 0.04548570470218241, 0.04577370375449205, 0.04577362223317588, 0.04584277266242208, 0.04596481578302209, 0.045408936883628806, 0.04617864692950771, 0.04578527729332882, 0.045604101728892676, 0.046084694877049345, 0.046691709748693626, 0.046577619062396734, 0.04634948141437813, 0.04538850147739379, 0.04606019749041021, 0.04574128071757129, 0.04564398026814426, 0.04590967874022296, 0.04578025560200649, 0.045938869176881156, 0.045530349260916675, 0.045203443698204344, 0.04538256427558669, 0.04571672923264712, 0.04604278587783775, 0.04567584192828975, 0.04522417092791004, 0.04528387021409334], 'val_acc': [0.37923776157145955, 0.9797695298699567, 0.9797695298699567, 0.979850142541593, 0.9817879865639401, 0.9858470635257498, 0.9899155480148149, 0.9885746284122885, 0.9891317180038368, 0.9909355881875449, 0.9923713996897648, 0.9927975872572321, 0.9941200309426245, 0.9947469758726385, 0.9952619225874434, 0.9958217479016659, 0.995718808722322, 0.995727093985481, 0.9956062111976373, 0.9957702832065359, 0.995997698855226, 0.996293861283003, 0.9962570865223878, 0.9963072712404014, 0.9961780050810236, 0.9964695453208728, 0.9964215764164055, 0.996414572435574, 0.9965659038428842, 0.9964822894030244, 0.9964904876521034, 0.9964188633173922, 0.996410044657923, 0.9964983593808473, 0.9965096431492019, 0.9965477729365774, 0.9965767629825286, 0.9965471775427351, 0.9965640180302362, 0.9964648409046396, 0.9964306454588897, 0.9966006977279691, 0.9964924591736202, 0.9965829087869964, 0.9965577828188013, 0.9965827406322869, 0.9965990731750962, 0.996616336768561, 0.9965730790239181, 0.996524784034186, 0.996556074079806, 0.996565558614522, 0.9966144627028138, 0.9965781245353448, 0.9965956546094296, 0.9965567584455448, 0.9965322143840094, 0.9965977916752335, 0.9966147985771625, 0.9965481906041612, 0.9965917193976632, 0.996575654640685, 0.9965785469887031, 0.9965694089875604, 0.9965647894100551, 0.9966016372624975, 0.9965541882671579, 0.9965846142629637, 0.996593175143221, 0.996562735877768, 0.9965193819825666, 0.9965287042360236, 0.9965444387745683, 0.9966100922031124, 0.996559741288206, 0.996585732176356, 0.9965838496267361, 0.9965765763373271, 0.9965869384090396, 0.9965798445861705, 0.9965987212031427, 0.996624279196245, 0.9966146373835794, 0.9965906175818756, 0.9965659878114714, 0.9965931755782914, 0.9966241177851266, 0.99662070465784], 'val_mDice': [0.04784197248790386, 0.033678687911779775, 0.004497770649163447, 0.30627839691447517, 0.5475046015786429, 0.7374643663855365, 0.839765691626681, 0.8057124438512064, 0.8066700877499406, 0.8374654696370564, 0.8624310763212886, 0.8625280596043942, 0.8874013782852758, 0.8985197578903532, 0.9049660933278773, 0.9091250430928529, 0.9081584221690241, 0.9051986284499621, 0.9013690441629313, 0.9024079670436191, 0.9061763612893377, 0.9101216360165255, 0.9093103400112068, 0.9101869017973433, 0.9067851063543862, 0.9130391639514561, 0.9117645144897656, 0.9113488119013988, 0.9148227757346021, 0.9127240037396006, 0.9138939915782344, 0.9115249597243149, 0.911368131420038, 0.9132371660131607, 0.9133283189178383, 0.9141337965091649, 0.9148313009390866, 0.9143568449211816, 0.9145315121995271, 0.9118783621892442, 0.9109473589563022, 0.9152623467201734, 0.9128471227892994, 0.9151088195125552, 0.9143014898265365, 0.9150984828924611, 0.9154212842457485, 0.9157527820037229, 0.9145766024606942, 0.9134712243167153, 0.9140829929035076, 0.9142551439521956, 0.9152240842363261, 0.9145260050784062, 0.9147627647340733, 0.9137449808364367, 0.9132879213260038, 0.9150837344409776, 0.9152871198462744, 0.9136598808486966, 0.9148339824954959, 0.9143202894360479, 0.9143184321205111, 0.9141934446609803, 0.9139724082320276, 0.914963802934563, 0.9135792686121307, 0.9142829925909529, 0.9146094948705965, 0.9137528101458167, 0.9126681217747014, 0.9128764493622049, 0.913277789624068, 0.9149929248068455, 0.9137935586219287, 0.9143605606399313, 0.9145348757287882, 0.914061473886462, 0.9142855538503967, 0.9140005324878832, 0.9147377135979868, 0.9153160581188481, 0.9149971556489485, 0.9144031105250338, 0.9138229767771533, 0.9144731836162344, 0.9152795504914583, 0.9151720465534795], 'loss': [1.407573269505806, 0.5057063060147422, 0.14004494345246865, 0.10113260608735343, 0.09029599047940352, 0.08179725827695114, 0.08028369294158344, 0.07550247435646104, 0.07273212178000088, 0.06972961175471104, 0.06764193831626418, 0.06644895032341844, 0.06574196434520148, 0.06420145809577016, 0.06332072162349235, 0.06241533079464447, 0.060540796335548014, 0.05866475588655824, 0.05851133164073446, 0.056704452142046, 0.055304570609859646, 0.054707275987992736, 0.05468116309769048, 0.05392123494535831, 0.053612902466886735, 0.05247631375425555, 0.05231522913710237, 0.05202535872021919, 0.0516326178984689, 0.05144996401564828, 0.05130013208744561, 0.050465060333753455, 0.049800355611323135, 0.05054369765182434, 0.05029978476501451, 0.050056983280945294, 0.04886527893505073, 0.04870621261778724, 0.048438650597198846, 0.048335806251012636, 0.04854377168299529, 0.048405853765351434, 0.04752015008641581, 0.047362755727151346, 0.04717874917857753, 0.04788115998647483, 0.04771960756094585, 0.047438954209606046, 0.04779697407789418, 0.04725434027355293, 0.04723445048986984, 0.04687676373226889, 0.046570036340082804, 0.046392311676969666, 0.046144927241678894, 0.04561607139597973, 0.04617214566234298, 0.04580713707605019, 0.04566951398852424, 0.04570318508911603, 0.045937921778468664, 0.0454150931221511, 0.045211992864244675, 0.045527846043186236, 0.04514898220306547, 0.04511347286439882, 0.04535889638395145, 0.045115382194959475, 0.04508468747285786, 0.045325524279196276, 0.04497235288540718, 0.04496306779202569, 0.044509254175747553, 0.04461390374622909, 0.04474708425073788, 0.044622217364645944, 0.0446560828042735, 0.04424801628504481, 0.04423671771813496, 0.044404506573242505, 0.04446121505255182, 0.04413754775606353, 0.0443546863208557, 0.043996583486865894, 0.04434862096101192, 0.04402744285578798, 0.044240014343132524, 0.044230461707843345], 'acc': [0.7606527100642914, 0.9863527581609529, 0.9891595385344745, 0.990335416324033, 0.9910605846367446, 0.9918962021179387, 0.9921437313991227, 0.9926308229051787, 0.9929320835714857, 0.9934486552999524, 0.9938612415872771, 0.9942273054216882, 0.9944119661899623, 0.9946688912772192, 0.9948541096278599, 0.9950245501372614, 0.995286080050351, 0.9955892125373991, 0.9956491756909, 0.9958151153155735, 0.9959665898031789, 0.9960346494989442, 0.9960451005714868, 0.9961105088882258, 0.996134004569406, 0.996206985612221, 0.9962185906071969, 0.9962352802013529, 0.9962589834711234, 0.9962839780769912, 0.9962956103785284, 0.9963467761213556, 0.996395610529801, 0.9963405505189755, 0.9963556395962908, 0.9963753088354477, 0.9964561071889154, 0.9964611803369569, 0.9964741069107808, 0.9964903308252983, 0.9964842843304714, 0.9964922690039197, 0.996546393838422, 0.996556618237143, 0.996568018286099, 0.9965222275315835, 0.9965293830251459, 0.9965468316242613, 0.99651673949998, 0.9965651355353482, 0.9965528508125268, 0.9965880786256837, 0.9966114339570107, 0.9966207076176047, 0.9966345329590032, 0.9966705406240641, 0.9966410850656444, 0.9966550704293651, 0.9966661568932933, 0.9966680216671798, 0.9966481037327809, 0.9966817277992888, 0.9966966862161758, 0.9966792810726636, 0.996706100813861, 0.9967041664522857, 0.9966905002523525, 0.9967022934570688, 0.9967146195801608, 0.9966898134776524, 0.9967133032864538, 0.9967133763975697, 0.9967423140121798, 0.9967354142607139, 0.9967357604374439, 0.9967386229284878, 0.996726481785328, 0.9967608281544277, 0.9967581195197082, 0.9967545464708301, 0.9967467458964568, 0.9967671938130421, 0.9967592094332126, 0.9967761453736592, 0.9967564400193727, 0.9967750205195008, 0.9967519849391994, 0.9967650480458302], 'mDice': [0.07809627997537552, 0.4404180576942237, 0.7793189451612276, 0.8340037147987065, 0.8504761284208063, 0.8638191149739797, 0.8653056603934377, 0.8725746734976181, 0.876698555030259, 0.881433143404317, 0.8846900803702218, 0.8863244975728942, 0.8870072969662145, 0.8890050012489845, 0.8901013479444194, 0.891365120269982, 0.8942799659198141, 0.896878212249925, 0.896460161714131, 0.8990474651599752, 0.9008108685756552, 0.9012352451315067, 0.900740111989928, 0.9017031615590815, 0.9020406260279011, 0.9038949039182053, 0.9040577097479345, 0.9044936381537338, 0.9050965737826718, 0.9053525466637071, 0.90555641363407, 0.9069962968379993, 0.9081290496393964, 0.9067514339104075, 0.907154390670983, 0.9075467234174607, 0.9096443999577039, 0.9099028207398401, 0.9103771691839097, 0.9105451844008685, 0.9101525351331738, 0.9103832673556699, 0.9119525519497876, 0.9122202173242429, 0.9125338643055244, 0.9112538712952525, 0.9115339088909732, 0.9120272876593867, 0.9113579583285477, 0.9123166318010227, 0.9123361612775643, 0.9129759927688561, 0.9134962834748142, 0.9138153574149597, 0.9142455443960106, 0.9151880926099317, 0.9141773370099185, 0.9148287224064907, 0.9150606164791314, 0.9149981922116773, 0.914564502063056, 0.9155053722447363, 0.9158609254019601, 0.9152779367756961, 0.9159600740583073, 0.9160170287921511, 0.9155577896851037, 0.9159985046668593, 0.9160382010666608, 0.9156031702539603, 0.9162339547584797, 0.91623714228569, 0.9170505110853411, 0.9168560170187738, 0.9166242812067417, 0.9168375345873715, 0.9167790941416923, 0.9174999007450536, 0.9175268458615383, 0.9172115361162008, 0.9171113087038688, 0.9176899776082909, 0.917292351205948, 0.9179357980272452, 0.9172999497704906, 0.9178731849628129, 0.9174835517488676, 0.9174889044221399], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:01<00:10,  1.08s/it]predicting test subjects:  18%|█▊        | 2/11 [00:01<00:08,  1.09it/s]predicting test subjects:  27%|██▋       | 3/11 [00:02<00:06,  1.23it/s]predicting test subjects:  36%|███▋      | 4/11 [00:02<00:04,  1.48it/s]predicting test subjects:  45%|████▌     | 5/11 [00:02<00:03,  1.68it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:03<00:02,  1.73it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:03<00:02,  1.98it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:04<00:01,  2.06it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:04<00:00,  2.13it/s]predicting test subjects:  91%|█████████ | 10/11 [00:05<00:00,  2.17it/s]predicting test subjects: 100%|██████████| 11/11 [00:05<00:00,  2.10it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:16,  2.48it/s]predicting train subjects:   5%|▍         | 2/41 [00:00<00:17,  2.21it/s]predicting train subjects:   7%|▋         | 3/41 [00:01<00:16,  2.25it/s]predicting train subjects:  10%|▉         | 4/41 [00:01<00:17,  2.12it/s]predicting train subjects:  12%|█▏        | 5/41 [00:02<00:18,  2.00it/s]predicting train subjects:  15%|█▍        | 6/41 [00:02<00:16,  2.08it/s]predicting train subjects:  17%|█▋        | 7/41 [00:03<00:15,  2.14it/s]predicting train subjects:  20%|█▉        | 8/41 [00:03<00:14,  2.31it/s]predicting train subjects:  22%|██▏       | 9/41 [00:04<00:12,  2.52it/s]predicting train subjects:  24%|██▍       | 10/41 [00:04<00:12,  2.41it/s]predicting train subjects:  27%|██▋       | 11/41 [00:04<00:13,  2.30it/s]predicting train subjects:  29%|██▉       | 12/41 [00:05<00:11,  2.63it/s]predicting train subjects:  32%|███▏      | 13/41 [00:05<00:10,  2.57it/s]predicting train subjects:  34%|███▍      | 14/41 [00:06<00:11,  2.40it/s]predicting train subjects:  37%|███▋      | 15/41 [00:06<00:09,  2.75it/s]predicting train subjects:  39%|███▉      | 16/41 [00:06<00:08,  2.82it/s]predicting train subjects:  41%|████▏     | 17/41 [00:07<00:09,  2.44it/s]predicting train subjects:  44%|████▍     | 18/41 [00:07<00:09,  2.44it/s]predicting train subjects:  46%|████▋     | 19/41 [00:08<00:09,  2.38it/s]predicting train subjects:  49%|████▉     | 20/41 [00:08<00:10,  2.04it/s]predicting train subjects:  51%|█████     | 21/41 [00:09<00:09,  2.14it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:09<00:09,  2.09it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:09<00:07,  2.34it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:10<00:07,  2.19it/s]predicting train subjects:  61%|██████    | 25/41 [00:10<00:06,  2.33it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:11<00:07,  2.13it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:12<00:07,  1.97it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:12<00:06,  2.07it/s]predicting train subjects:  71%|███████   | 29/41 [00:12<00:06,  2.00it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:13<00:05,  1.94it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:13<00:04,  2.10it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:14<00:04,  2.21it/s]predicting train subjects:  80%|████████  | 33/41 [00:14<00:03,  2.00it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:15<00:03,  1.95it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:16<00:03,  1.72it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:16<00:02,  1.69it/s]predicting train subjects:  90%|█████████ | 37/41 [00:17<00:02,  1.72it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:17<00:01,  1.83it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:18<00:01,  1.72it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:19<00:00,  1.71it/s]predicting train subjects: 100%|██████████| 41/41 [00:19<00:00,  1.71it/s]
saving BB  test1-THALAMUS:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  55%|█████▍    | 6/11 [00:00<00:00, 52.56it/s]saving BB  test1-THALAMUS: 100%|██████████| 11/11 [00:00<00:00, 58.45it/s]
saving BB  train1-THALAMUS:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS:  20%|█▉        | 8/41 [00:00<00:00, 75.40it/s]saving BB  train1-THALAMUS:  39%|███▉      | 16/41 [00:00<00:00, 73.98it/s]saving BB  train1-THALAMUS:  56%|█████▌    | 23/41 [00:00<00:00, 71.95it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 30/41 [00:00<00:00, 69.01it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 36/41 [00:00<00:00, 62.46it/s]saving BB  train1-THALAMUS: 100%|██████████| 41/41 [00:00<00:00, 65.50it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:36,  1.10it/s]Loading train:   5%|▍         | 2/41 [00:01<00:33,  1.18it/s]Loading train:   7%|▋         | 3/41 [00:02<00:29,  1.29it/s]Loading train:  10%|▉         | 4/41 [00:02<00:26,  1.38it/s]Loading train:  12%|█▏        | 5/41 [00:03<00:29,  1.21it/s]Loading train:  15%|█▍        | 6/41 [00:04<00:26,  1.34it/s]Loading train:  17%|█▋        | 7/41 [00:04<00:23,  1.46it/s]Loading train:  20%|█▉        | 8/41 [00:05<00:19,  1.66it/s]Loading train:  22%|██▏       | 9/41 [00:06<00:23,  1.38it/s]Loading train:  24%|██▍       | 10/41 [00:06<00:21,  1.47it/s]Loading train:  27%|██▋       | 11/41 [00:07<00:20,  1.47it/s]Loading train:  29%|██▉       | 12/41 [00:08<00:18,  1.56it/s]Loading train:  32%|███▏      | 13/41 [00:09<00:21,  1.33it/s]Loading train:  34%|███▍      | 14/41 [00:09<00:19,  1.38it/s]Loading train:  37%|███▋      | 15/41 [00:10<00:17,  1.49it/s]Loading train:  39%|███▉      | 16/41 [00:11<00:16,  1.54it/s]Loading train:  41%|████▏     | 17/41 [00:11<00:15,  1.54it/s]Loading train:  44%|████▍     | 18/41 [00:12<00:16,  1.36it/s]Loading train:  46%|████▋     | 19/41 [00:13<00:14,  1.56it/s]Loading train:  49%|████▉     | 20/41 [00:13<00:13,  1.55it/s]Loading train:  51%|█████     | 21/41 [00:14<00:12,  1.61it/s]Loading train:  54%|█████▎    | 22/41 [00:14<00:10,  1.85it/s]Loading train:  56%|█████▌    | 23/41 [00:15<00:09,  1.93it/s]Loading train:  59%|█████▊    | 24/41 [00:15<00:09,  1.76it/s]Loading train:  61%|██████    | 25/41 [00:16<00:09,  1.77it/s]Loading train:  63%|██████▎   | 26/41 [00:17<00:09,  1.63it/s]Loading train:  66%|██████▌   | 27/41 [00:17<00:08,  1.75it/s]Loading train:  68%|██████▊   | 28/41 [00:18<00:07,  1.64it/s]Loading train:  71%|███████   | 29/41 [00:18<00:07,  1.66it/s]Loading train:  73%|███████▎  | 30/41 [00:19<00:06,  1.60it/s]Loading train:  76%|███████▌  | 31/41 [00:20<00:06,  1.56it/s]Loading train:  78%|███████▊  | 32/41 [00:20<00:05,  1.53it/s]Loading train:  80%|████████  | 33/41 [00:21<00:05,  1.52it/s]Loading train:  83%|████████▎ | 34/41 [00:22<00:04,  1.43it/s]Loading train:  85%|████████▌ | 35/41 [00:23<00:04,  1.34it/s]Loading train:  88%|████████▊ | 36/41 [00:24<00:03,  1.28it/s]Loading train:  90%|█████████ | 37/41 [00:24<00:02,  1.34it/s]Loading train:  93%|█████████▎| 38/41 [00:25<00:02,  1.41it/s]Loading train:  95%|█████████▌| 39/41 [00:25<00:01,  1.51it/s]Loading train:  98%|█████████▊| 40/41 [00:26<00:00,  1.40it/s]Loading train: 100%|██████████| 41/41 [00:27<00:00,  1.40it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 3/41 [00:00<00:01, 27.19it/s]concatenating: train:  44%|████▍     | 18/41 [00:00<00:00, 36.00it/s]concatenating: train:  76%|███████▌  | 31/41 [00:00<00:00, 45.85it/s]concatenating: train:  95%|█████████▌| 39/41 [00:00<00:00, 31.20it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 48.90it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:06,  1.64it/s]Loading test:  18%|█▊        | 2/11 [00:01<00:05,  1.63it/s]Loading test:  27%|██▋       | 3/11 [00:02<00:05,  1.38it/s]Loading test:  36%|███▋      | 4/11 [00:02<00:04,  1.57it/s]Loading test:  45%|████▌     | 5/11 [00:03<00:03,  1.53it/s]Loading test:  55%|█████▍    | 6/11 [00:03<00:02,  1.70it/s]Loading test:  64%|██████▎   | 7/11 [00:04<00:02,  1.50it/s]Loading test:  73%|███████▎  | 8/11 [00:05<00:01,  1.51it/s]Loading test:  82%|████████▏ | 9/11 [00:05<00:01,  1.56it/s]Loading test:  91%|█████████ | 10/11 [00:06<00:00,  1.59it/s]Loading test: 100%|██████████| 11/11 [00:07<00:00,  1.48it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  18%|█▊        | 2/11 [00:00<00:00, 16.48it/s]concatenating: validation:  73%|███████▎  | 8/11 [00:00<00:00, 20.56it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 22.12it/s]
Loading trainS:   0%|          | 0/41 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/41 [00:00<00:33,  1.18it/s]Loading trainS:   5%|▍         | 2/41 [00:01<00:30,  1.29it/s]Loading trainS:   7%|▋         | 3/41 [00:02<00:27,  1.38it/s]Loading trainS:  10%|▉         | 4/41 [00:02<00:26,  1.41it/s]Loading trainS:  12%|█▏        | 5/41 [00:03<00:28,  1.27it/s]Loading trainS:  15%|█▍        | 6/41 [00:04<00:26,  1.30it/s]Loading trainS:  17%|█▋        | 7/41 [00:05<00:24,  1.37it/s]Loading trainS:  20%|█▉        | 8/41 [00:05<00:22,  1.46it/s]Loading trainS:  22%|██▏       | 9/41 [00:06<00:22,  1.40it/s]Loading trainS:  24%|██▍       | 10/41 [00:07<00:22,  1.38it/s]Loading trainS:  27%|██▋       | 11/41 [00:07<00:20,  1.46it/s]Loading trainS:  29%|██▉       | 12/41 [00:08<00:17,  1.67it/s]Loading trainS:  32%|███▏      | 13/41 [00:08<00:15,  1.84it/s]Loading trainS:  34%|███▍      | 14/41 [00:09<00:13,  1.94it/s]Loading trainS:  37%|███▋      | 15/41 [00:09<00:12,  2.10it/s]Loading trainS:  39%|███▉      | 16/41 [00:09<00:11,  2.21it/s]Loading trainS:  41%|████▏     | 17/41 [00:10<00:11,  2.14it/s]Loading trainS:  44%|████▍     | 18/41 [00:10<00:10,  2.13it/s]Loading trainS:  46%|████▋     | 19/41 [00:11<00:10,  2.13it/s]Loading trainS:  49%|████▉     | 20/41 [00:11<00:08,  2.34it/s]Loading trainS:  51%|█████     | 21/41 [00:11<00:07,  2.54it/s]Loading trainS:  54%|█████▎    | 22/41 [00:12<00:07,  2.57it/s]Loading trainS:  56%|█████▌    | 23/41 [00:12<00:06,  2.66it/s]Loading trainS:  59%|█████▊    | 24/41 [00:13<00:06,  2.62it/s]Loading trainS:  61%|██████    | 25/41 [00:13<00:06,  2.59it/s]Loading trainS:  63%|██████▎   | 26/41 [00:14<00:07,  2.13it/s]Loading trainS:  66%|██████▌   | 27/41 [00:14<00:07,  1.96it/s]Loading trainS:  68%|██████▊   | 28/41 [00:15<00:07,  1.74it/s]Loading trainS:  71%|███████   | 29/41 [00:15<00:06,  1.78it/s]Loading trainS:  73%|███████▎  | 30/41 [00:16<00:07,  1.46it/s]Loading trainS:  76%|███████▌  | 31/41 [00:17<00:06,  1.54it/s]Loading trainS:  78%|███████▊  | 32/41 [00:18<00:06,  1.46it/s]Loading trainS:  80%|████████  | 33/41 [00:18<00:05,  1.55it/s]Loading trainS:  83%|████████▎ | 34/41 [00:19<00:04,  1.53it/s]Loading trainS:  85%|████████▌ | 35/41 [00:20<00:04,  1.44it/s]Loading trainS:  88%|████████▊ | 36/41 [00:21<00:03,  1.25it/s]Loading trainS:  90%|█████████ | 37/41 [00:21<00:03,  1.31it/s]Loading trainS:  93%|█████████▎| 38/41 [00:22<00:02,  1.38it/s]Loading trainS:  95%|█████████▌| 39/41 [00:23<00:01,  1.42it/s]Loading trainS:  98%|█████████▊| 40/41 [00:23<00:00,  1.41it/s]Loading trainS: 100%|██████████| 41/41 [00:24<00:00,  1.48it/s]
Loading testS:   0%|          | 0/11 [00:00<?, ?it/s]Loading testS:   9%|▉         | 1/11 [00:00<00:06,  1.53it/s]Loading testS:  18%|█▊        | 2/11 [00:01<00:05,  1.53it/s]Loading testS:  27%|██▋       | 3/11 [00:01<00:04,  1.70it/s]Loading testS:  36%|███▋      | 4/11 [00:02<00:03,  1.79it/s]Loading testS:  45%|████▌     | 5/11 [00:02<00:03,  1.85it/s]Loading testS:  55%|█████▍    | 6/11 [00:03<00:02,  1.90it/s]Loading testS:  64%|██████▎   | 7/11 [00:03<00:02,  1.92it/s]Loading testS:  73%|███████▎  | 8/11 [00:04<00:01,  2.03it/s]Loading testS:  82%|████████▏ | 9/11 [00:04<00:00,  2.14it/s]Loading testS:  91%|█████████ | 10/11 [00:04<00:00,  2.29it/s]Loading testS: 100%|██████████| 11/11 [00:05<00:00,  2.26it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 128, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 128, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 128, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 128, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 128, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 128, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 128, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 64, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 64, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 64, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 64, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 64, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 64, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 64, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 64, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 64, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 32, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 32, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 32, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 32, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 32, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 32, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 32, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 32, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 32, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 32, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 64, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 64, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 64, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 64, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 64, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 64, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 64, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 64, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 64, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 64, 140)  0           concatenate_4[0][0]              2019-07-27 18:00:19.329057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-27 18:00:19.329174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-27 18:00:19.329191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-27 18:00:19.329201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-27 18:00:19.329561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 128, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 128, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 128, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 128, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 128, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 128, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 128, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 128, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 128, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 128, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 128, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
----- /array/ssd/msmajdi/experiments/keras/exp6/models/sE8_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1/1-THALAMUS/sd2/model_weights.h5
 --- initialized from Model_3T /array/ssd/msmajdi/experiments/keras/exp6/models/sE8_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97831385 0.02168615]
Train on 2690 samples, validate on 698 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.0561 - acc: 0.9955 - mDice: 0.8962 - val_loss: 0.0502 - val_acc: 0.9960 - val_mDice: 0.9065

Epoch 00001: val_mDice improved from -inf to 0.90655, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0505 - acc: 0.9959 - mDice: 0.9060 - val_loss: 0.0474 - val_acc: 0.9960 - val_mDice: 0.9116

Epoch 00002: val_mDice improved from 0.90655 to 0.91159, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0485 - acc: 0.9961 - mDice: 0.9095 - val_loss: 0.0473 - val_acc: 0.9961 - val_mDice: 0.9116

Epoch 00003: val_mDice did not improve from 0.91159
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0469 - acc: 0.9962 - mDice: 0.9124 - val_loss: 0.0474 - val_acc: 0.9961 - val_mDice: 0.9115

Epoch 00004: val_mDice did not improve from 0.91159
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0466 - acc: 0.9962 - mDice: 0.9129 - val_loss: 0.0461 - val_acc: 0.9962 - val_mDice: 0.9138

Epoch 00005: val_mDice improved from 0.91159 to 0.91381, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0453 - acc: 0.9963 - mDice: 0.9152 - val_loss: 0.0457 - val_acc: 0.9962 - val_mDice: 0.9146

Epoch 00006: val_mDice improved from 0.91381 to 0.91458, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0443 - acc: 0.9964 - mDice: 0.9171 - val_loss: 0.0457 - val_acc: 0.9962 - val_mDice: 0.9144

Epoch 00007: val_mDice did not improve from 0.91458
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0440 - acc: 0.9964 - mDice: 0.9175 - val_loss: 0.0463 - val_acc: 0.9961 - val_mDice: 0.9135

Epoch 00008: val_mDice did not improve from 0.91458
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0434 - acc: 0.9964 - mDice: 0.9186 - val_loss: 0.0460 - val_acc: 0.9962 - val_mDice: 0.9139

Epoch 00009: val_mDice did not improve from 0.91458
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0432 - acc: 0.9965 - mDice: 0.9190 - val_loss: 0.0458 - val_acc: 0.9962 - val_mDice: 0.9144

Epoch 00010: val_mDice did not improve from 0.91458
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0426 - acc: 0.9965 - mDice: 0.9200 - val_loss: 0.0458 - val_acc: 0.9962 - val_mDice: 0.9144

Epoch 00011: val_mDice did not improve from 0.91458
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0417 - acc: 0.9966 - mDice: 0.9216 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9152

Epoch 00012: val_mDice improved from 0.91458 to 0.91515, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0421 - acc: 0.9966 - mDice: 0.9209 - val_loss: 0.0464 - val_acc: 0.9962 - val_mDice: 0.9134

Epoch 00013: val_mDice did not improve from 0.91515
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0419 - acc: 0.9966 - mDice: 0.9213 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9152

Epoch 00014: val_mDice improved from 0.91515 to 0.91523, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0415 - acc: 0.9966 - mDice: 0.9220 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9153

Epoch 00015: val_mDice improved from 0.91523 to 0.91526, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0409 - acc: 0.9967 - mDice: 0.9232 - val_loss: 0.0454 - val_acc: 0.9962 - val_mDice: 0.9151

Epoch 00016: val_mDice did not improve from 0.91526
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0408 - acc: 0.9967 - mDice: 0.9233 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9153

Epoch 00017: val_mDice improved from 0.91526 to 0.91533, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0400 - acc: 0.9967 - mDice: 0.9246 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9157

Epoch 00018: val_mDice improved from 0.91533 to 0.91575, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0403 - acc: 0.9967 - mDice: 0.9242 - val_loss: 0.0448 - val_acc: 0.9962 - val_mDice: 0.9161

Epoch 00019: val_mDice improved from 0.91575 to 0.91614, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0397 - acc: 0.9968 - mDice: 0.9252 - val_loss: 0.0449 - val_acc: 0.9962 - val_mDice: 0.9160

Epoch 00020: val_mDice did not improve from 0.91614
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0396 - acc: 0.9968 - mDice: 0.9254 - val_loss: 0.0449 - val_acc: 0.9962 - val_mDice: 0.9159

Epoch 00021: val_mDice did not improve from 0.91614
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0396 - acc: 0.9968 - mDice: 0.9255 - val_loss: 0.0454 - val_acc: 0.9962 - val_mDice: 0.9151

Epoch 00022: val_mDice did not improve from 0.91614
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0397 - acc: 0.9968 - mDice: 0.9253 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9149

Epoch 00023: val_mDice did not improve from 0.91614
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0392 - acc: 0.9968 - mDice: 0.9263 - val_loss: 0.0454 - val_acc: 0.9962 - val_mDice: 0.9151

Epoch 00024: val_mDice did not improve from 0.91614
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0386 - acc: 0.9969 - mDice: 0.9272 - val_loss: 0.0456 - val_acc: 0.9962 - val_mDice: 0.9148

Epoch 00025: val_mDice did not improve from 0.91614
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0386 - acc: 0.9968 - mDice: 0.9271 - val_loss: 0.0448 - val_acc: 0.9962 - val_mDice: 0.9162

Epoch 00026: val_mDice improved from 0.91614 to 0.91619, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0386 - acc: 0.9969 - mDice: 0.9272 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9158

Epoch 00027: val_mDice did not improve from 0.91619
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0387 - acc: 0.9968 - mDice: 0.9271 - val_loss: 0.0452 - val_acc: 0.9962 - val_mDice: 0.9155

Epoch 00028: val_mDice did not improve from 0.91619
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0391 - acc: 0.9968 - mDice: 0.9263 - val_loss: 0.0457 - val_acc: 0.9962 - val_mDice: 0.9144

Epoch 00029: val_mDice did not improve from 0.91619
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0383 - acc: 0.9969 - mDice: 0.9277 - val_loss: 0.0454 - val_acc: 0.9962 - val_mDice: 0.9150

Epoch 00030: val_mDice did not improve from 0.91619
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0385 - acc: 0.9969 - mDice: 0.9275 - val_loss: 0.0460 - val_acc: 0.9962 - val_mDice: 0.9141

Epoch 00031: val_mDice did not improve from 0.91619
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0386 - acc: 0.9969 - mDice: 0.9273 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9152

Epoch 00032: val_mDice did not improve from 0.91619
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0380 - acc: 0.9969 - mDice: 0.9284 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9152

Epoch 00033: val_mDice did not improve from 0.91619
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0381 - acc: 0.9969 - mDice: 0.9281 - val_loss: 0.0447 - val_acc: 0.9962 - val_mDice: 0.9163

Epoch 00034: val_mDice improved from 0.91619 to 0.91631, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0377 - acc: 0.9969 - mDice: 0.9289 - val_loss: 0.0461 - val_acc: 0.9961 - val_mDice: 0.9137

Epoch 00035: val_mDice did not improve from 0.91631
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0374 - acc: 0.9969 - mDice: 0.9295 - val_loss: 0.0461 - val_acc: 0.9962 - val_mDice: 0.9138

Epoch 00036: val_mDice did not improve from 0.91631
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0374 - acc: 0.9969 - mDice: 0.9294 - val_loss: 0.0458 - val_acc: 0.9962 - val_mDice: 0.9144

Epoch 00037: val_mDice did not improve from 0.91631
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0372 - acc: 0.9970 - mDice: 0.9298 - val_loss: 0.0457 - val_acc: 0.9962 - val_mDice: 0.9144

Epoch 00038: val_mDice did not improve from 0.91631
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0373 - acc: 0.9970 - mDice: 0.9297 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9150

Epoch 00039: val_mDice did not improve from 0.91631
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0373 - acc: 0.9969 - mDice: 0.9295 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9153

Epoch 00040: val_mDice did not improve from 0.91631
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0371 - acc: 0.9970 - mDice: 0.9300 - val_loss: 0.0457 - val_acc: 0.9962 - val_mDice: 0.9145

Epoch 00041: val_mDice did not improve from 0.91631
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0371 - acc: 0.9970 - mDice: 0.9300 - val_loss: 0.0451 - val_acc: 0.9962 - val_mDice: 0.9155

Epoch 00042: val_mDice did not improve from 0.91631
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0368 - acc: 0.9970 - mDice: 0.9306 - val_loss: 0.0454 - val_acc: 0.9962 - val_mDice: 0.9151

Epoch 00043: val_mDice did not improve from 0.91631
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0368 - acc: 0.9970 - mDice: 0.9304 - val_loss: 0.0456 - val_acc: 0.9962 - val_mDice: 0.9147

Epoch 00044: val_mDice did not improve from 0.91631
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0367 - acc: 0.9970 - mDice: 0.9308 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9149

Epoch 00045: val_mDice did not improve from 0.91631
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0366 - acc: 0.9970 - mDice: 0.9308 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9149

Epoch 00046: val_mDice did not improve from 0.91631
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0369 - acc: 0.9970 - mDice: 0.9304 - val_loss: 0.0459 - val_acc: 0.9962 - val_mDice: 0.9141

Epoch 00047: val_mDice did not improve from 0.91631
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0366 - acc: 0.9970 - mDice: 0.9309 - val_loss: 0.0460 - val_acc: 0.9962 - val_mDice: 0.9140

Epoch 00048: val_mDice did not improve from 0.91631
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0368 - acc: 0.9970 - mDice: 0.9305 - val_loss: 0.0461 - val_acc: 0.9962 - val_mDice: 0.9137

Epoch 00049: val_mDice did not improve from 0.91631
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0365 - acc: 0.9970 - mDice: 0.9310 - val_loss: 0.0457 - val_acc: 0.9962 - val_mDice: 0.9144

Epoch 00050: val_mDice did not improve from 0.91631
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0368 - acc: 0.9970 - mDice: 0.9305 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9149

Epoch 00051: val_mDice did not improve from 0.91631
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0365 - acc: 0.9970 - mDice: 0.9310 - val_loss: 0.0454 - val_acc: 0.9962 - val_mDice: 0.9151

Epoch 00052: val_mDice did not improve from 0.91631
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0366 - acc: 0.9970 - mDice: 0.9309 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9148

Epoch 00053: val_mDice did not improve from 0.91631
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0363 - acc: 0.9970 - mDice: 0.9314 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9148

Epoch 00054: val_mDice did not improve from 0.91631
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0363 - acc: 0.9970 - mDice: 0.9315 - val_loss: 0.0456 - val_acc: 0.9962 - val_mDice: 0.9146

Epoch 00055: val_mDice did not improve from 0.91631
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0365 - acc: 0.9970 - mDice: 0.9310 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9152

Epoch 00056: val_mDice did not improve from 0.91631
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0362 - acc: 0.9970 - mDice: 0.9316 - val_loss: 0.0456 - val_acc: 0.9962 - val_mDice: 0.9148

Epoch 00057: val_mDice did not improve from 0.91631
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0364 - acc: 0.9970 - mDice: 0.9312 - val_loss: 0.0457 - val_acc: 0.9962 - val_mDice: 0.9145

Epoch 00058: val_mDice did not improve from 0.91631
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0360 - acc: 0.9971 - mDice: 0.9320 - val_loss: 0.0454 - val_acc: 0.9962 - val_mDice: 0.9150

Epoch 00059: val_mDice did not improve from 0.91631
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0363 - acc: 0.9970 - mDice: 0.9313 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9152

Epoch 00060: val_mDice did not improve from 0.91631
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0359 - acc: 0.9971 - mDice: 0.9322 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9149

Epoch 00061: val_mDice did not improve from 0.91631
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0360 - acc: 0.9971 - mDice: 0.9319 - val_loss: 0.0452 - val_acc: 0.9962 - val_mDice: 0.9155

Epoch 00062: val_mDice did not improve from 0.91631
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0362 - acc: 0.9970 - mDice: 0.9317 - val_loss: 0.0451 - val_acc: 0.9962 - val_mDice: 0.9155

Epoch 00063: val_mDice did not improve from 0.91631
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0362 - acc: 0.9970 - mDice: 0.9317 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9152

Epoch 00064: val_mDice did not improve from 0.91631
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0362 - acc: 0.9971 - mDice: 0.9315 - val_loss: 0.0456 - val_acc: 0.9962 - val_mDice: 0.9147

Epoch 00065: val_mDice did not improve from 0.91631
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0358 - acc: 0.9971 - mDice: 0.9322 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9149

Epoch 00066: val_mDice did not improve from 0.91631
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0361 - acc: 0.9971 - mDice: 0.9317 - val_loss: 0.0456 - val_acc: 0.9962 - val_mDice: 0.9147

Epoch 00067: val_mDice did not improve from 0.91631
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0360 - acc: 0.9971 - mDice: 0.9320 - val_loss: 0.0454 - val_acc: 0.9962 - val_mDice: 0.9151

Epoch 00068: val_mDice did not improve from 0.91631
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0363 - acc: 0.9970 - mDice: 0.9314 - val_loss: 0.0451 - val_acc: 0.9962 - val_mDice: 0.9156

Epoch 00069: val_mDice did not improve from 0.91631
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0358 - acc: 0.9971 - mDice: 0.9324 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9153

Epoch 00070: val_mDice did not improve from 0.91631
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0359 - acc: 0.9971 - mDice: 0.9322 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9152

Epoch 00071: val_mDice did not improve from 0.91631
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0358 - acc: 0.9971 - mDice: 0.9324 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9148

Epoch 00072: val_mDice did not improve from 0.91631
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 7s - loss: 0.0361 - acc: 0.9971 - mDice: 0.9318 - val_loss: 0.0457 - val_acc: 0.9962 - val_mDice: 0.9145

Epoch 00073: val_mDice did not improve from 0.91631
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 7s - loss: 0.0357 - acc: 0.9971 - mDice: 0.9326 - val_loss: 0.0456 - val_acc: 0.9962 - val_mDice: 0.9147

Epoch 00074: val_mDice did not improve from 0.91631
Restoring model weights from the end of the best epoch
Epoch 00074: early stopping
{'val_loss': [0.050150861488628524, 0.047356957703145665, 0.047346586132032485, 0.04741603745558542, 0.04610309771190058, 0.04566739040579017, 0.045737513146205756, 0.04628307365927109, 0.04602065981288352, 0.04578225187594365, 0.04575848069180732, 0.045340073772607355, 0.046354771450290023, 0.045323438792993824, 0.045289535384977445, 0.04540303333483658, 0.045251583120115846, 0.04501225088421458, 0.04480153462051662, 0.044893549781901104, 0.044942237319355365, 0.045377860053904075, 0.045516074173429975, 0.04539288810980013, 0.04555800847474347, 0.044781196328749974, 0.04496633436498806, 0.04517706913772149, 0.04574892269634244, 0.045414179008464756, 0.04595339613136726, 0.04533585103330093, 0.04533615088181031, 0.04471105738492272, 0.04614650241411518, 0.04611960486815789, 0.04579139638553376, 0.04574965557515792, 0.04546151920768798, 0.04526149836985293, 0.045688523027650946, 0.045144310567987, 0.04538304667204704, 0.04557784323321714, 0.04546319710405645, 0.04549843998343009, 0.045913157936580545, 0.0460123620009354, 0.04613283225321838, 0.045734831214719655, 0.04547412837602006, 0.0453851543537696, 0.04554287224666437, 0.04554112542708828, 0.045646406155090274, 0.04533904150427242, 0.04556867298816883, 0.04572841699067023, 0.04541268076929117, 0.04532658643913816, 0.04548023582401453, 0.04516330127601637, 0.04514443375874727, 0.04530585709863201, 0.045611959505815564, 0.0454942535790468, 0.045602084700562553, 0.04537793395128154, 0.04511055650248227, 0.045285185462765845, 0.045331318945461156, 0.04553450096430615, 0.04569504930928649, 0.04560367938099071], 'val_acc': [0.9959543282459663, 0.9960283213495184, 0.9960891244063063, 0.9960751125327496, 0.9961540504308006, 0.9961955369714338, 0.9962151393849392, 0.9961490414204092, 0.996184171100059, 0.9961712338179777, 0.9961725779112567, 0.9962358730911867, 0.9961542437616255, 0.9961688217939142, 0.9962114667824141, 0.9962001775943788, 0.9962078961396968, 0.9962392567221962, 0.9962306726286267, 0.9962410814099462, 0.996222753921006, 0.9961886002887628, 0.996164963033616, 0.996197390692965, 0.9962117463606477, 0.9962087709104435, 0.9962372322137172, 0.9962058702649224, 0.996179033145522, 0.9962003584577566, 0.9961725845719477, 0.9962162969788713, 0.9962031581681916, 0.9962318117775685, 0.9961470150332737, 0.9961575238957446, 0.9961598515510559, 0.9961806964396064, 0.9961901344679147, 0.996210799688596, 0.9961844479457014, 0.996213782482639, 0.9961942892019933, 0.9961862876627029, 0.9961995972603304, 0.9962044202837357, 0.9961724961043087, 0.9961706495558293, 0.9961610060708229, 0.9961888785007007, 0.996196216361912, 0.9962069241911771, 0.9961918778610775, 0.9961929353738582, 0.9961864724541803, 0.9962035543939104, 0.9961896673656124, 0.996182709163786, 0.9961963248116239, 0.9962006600675064, 0.996202977134027, 0.9962136832554225, 0.9962209227433519, 0.996210685432128, 0.9961908217145925, 0.9961883996140991, 0.9961833818935734, 0.9961939995473299, 0.9962115714748131, 0.99619719121381, 0.996200955699713, 0.9961950727725097, 0.9961844394063539, 0.996188004242315], 'val_mDice': [0.9065455188040747, 0.9115899835411662, 0.91157708178277, 0.9114519173231371, 0.9138095283235041, 0.9145758054659496, 0.9144418200312507, 0.9134746549122654, 0.9139348186531177, 0.9143793463365396, 0.9144194044151416, 0.9151545595644538, 0.9133511189061796, 0.9152283289371043, 0.9152596666341525, 0.9150564260674069, 0.9153349698443809, 0.9157463279699528, 0.9161436004420065, 0.9159698052529278, 0.9158886372189126, 0.9151112748080475, 0.914871711444035, 0.9150752319305879, 0.914764757313496, 0.9161883292020563, 0.9158376986454414, 0.9154660778605835, 0.9144373969908771, 0.9150334986028152, 0.9140655010682464, 0.9151745838217202, 0.9151778759109257, 0.9163101197655358, 0.9137322311756604, 0.9137702970245165, 0.9143692009086937, 0.9144296290881313, 0.9149569378882902, 0.9153071484798005, 0.9145366293311461, 0.9155220578939662, 0.915095849160137, 0.9147422797017247, 0.9149431412062877, 0.9148752709856006, 0.9141304925041417, 0.9139508012372648, 0.9137305294545127, 0.9144467197379956, 0.9149212719376244, 0.9150779342924626, 0.9147956810229829, 0.9148023644286104, 0.9146104663354278, 0.9151635880456613, 0.9147506497992486, 0.9144605445998446, 0.9150336577762506, 0.9151891768832603, 0.9149063186864115, 0.9154847844965478, 0.915514716104655, 0.9152223240680203, 0.9146708482316025, 0.9148886553879112, 0.914694244335579, 0.915100594817055, 0.9155811112384742, 0.9152700299861438, 0.9151813778972899, 0.9148086118151602, 0.9145218811950574, 0.9146881226482227], 'loss': [0.056062689210291686, 0.05049669666416583, 0.048501479525991535, 0.04685205686491218, 0.046608081620643574, 0.04528241453616149, 0.044257311406631895, 0.0440175779385416, 0.043394799015335876, 0.043180210356597123, 0.042616686136305995, 0.041745342965130466, 0.04210161694695958, 0.0419195537978167, 0.041487339745556115, 0.04085566241945033, 0.04078406355859622, 0.04004265556326586, 0.04026235165039846, 0.03971143514194896, 0.039604544902711994, 0.039574410115475994, 0.03966572281097834, 0.03915067126482836, 0.03862409828209966, 0.03864563052926808, 0.03860702376715756, 0.03868941289673951, 0.03910983532500976, 0.038338660556809165, 0.03846988464155162, 0.038581608451875166, 0.03796441202159265, 0.03812679507142992, 0.03767285015447876, 0.037353613479651486, 0.03741787699463199, 0.03717172199853291, 0.03726795598255214, 0.0373436780888795, 0.03709085744054344, 0.037082816879213075, 0.03677371838112746, 0.036838855317423334, 0.036655476129852704, 0.03664119983417394, 0.03686449653209364, 0.03659514020643713, 0.0368058317268205, 0.03653201995396703, 0.03681188812098538, 0.03653569522289538, 0.03658483707827263, 0.036328981315447054, 0.036263710684160316, 0.03650279188023181, 0.036180572272676514, 0.03639747615086545, 0.036005241706358014, 0.036348734798816944, 0.03586860375001085, 0.036016602387658726, 0.03615510311379309, 0.036155804160802336, 0.036235534921842436, 0.03584182374594823, 0.03614447351725128, 0.03597131523833399, 0.03630715997744227, 0.03575462014258573, 0.03586979118975565, 0.0357684706611261, 0.03608086123563986, 0.03567775214250203], 'acc': [0.9954933371242537, 0.9958710189645619, 0.9960574930928454, 0.9961964997660272, 0.9962193338844413, 0.9963114928578799, 0.9963856798122364, 0.9964161018899825, 0.9964289171101879, 0.99650047259703, 0.9965429113256887, 0.9965841187420388, 0.9965826953210796, 0.9965733606576033, 0.9966341186190183, 0.9966562988589688, 0.9966836434314685, 0.9967090086422888, 0.9967155141901349, 0.996782341189544, 0.9967987349042219, 0.9967636842709935, 0.9967813139511307, 0.9968045927778053, 0.9968536666777941, 0.9968449504402047, 0.9968612366892591, 0.9968313860627355, 0.9968365446342412, 0.9968575542744208, 0.9968851877854216, 0.996863753379056, 0.9968854330729374, 0.9968939268456073, 0.9969236161629064, 0.9969366910732369, 0.9969458329633266, 0.9969599780983198, 0.996959649718827, 0.9969424534464414, 0.9969628114239434, 0.9969749506078245, 0.9969725905741015, 0.9969999573044618, 0.9969950779219986, 0.9969969039512833, 0.9969952002333886, 0.9970182581461938, 0.9969939226111515, 0.9970272721854284, 0.9970007853437091, 0.9970012118825239, 0.9970255139591969, 0.997018385997049, 0.9970331810220909, 0.997023968005269, 0.9970299244813317, 0.9970469567855494, 0.9970561700239501, 0.997034791455393, 0.9970554177645857, 0.9970560661035842, 0.9970425759549477, 0.9970470795400967, 0.9970504306949204, 0.9970787269475292, 0.997051936986278, 0.9970519990282875, 0.9970395434301582, 0.9970724939413673, 0.9970647256170507, 0.9970699648431686, 0.9970514377696806, 0.9970751952061423], 'mDice': [0.8961706400803917, 0.9059776032281188, 0.9094848697070296, 0.9123982866900561, 0.9128520187392111, 0.9152036648257514, 0.9170537892327433, 0.9174668682552182, 0.9186079070027433, 0.9189566289625203, 0.9200003399724854, 0.9215606526371272, 0.9209076997072724, 0.9212547972742953, 0.9220164699625348, 0.923154300251858, 0.9232909460050023, 0.9246482040359185, 0.9242420770421791, 0.9252156517762677, 0.9254094430948279, 0.9254786050895776, 0.925320224026322, 0.9262545691546897, 0.9271873787433241, 0.9271411962225534, 0.9272313645338037, 0.9270815093721155, 0.9263117014253893, 0.9277076809822848, 0.9274715537475388, 0.9272761524388338, 0.9283959555360021, 0.9280943688849977, 0.9289213033413798, 0.929500545267722, 0.9293757116484376, 0.9298386500670565, 0.9296610789671264, 0.9295136689253456, 0.9299769215424265, 0.9299884801903622, 0.9305536405747707, 0.9304356863064394, 0.9307732130071931, 0.9307955000480311, 0.9303870573362897, 0.9308792678396941, 0.9304958810150402, 0.9309805531927201, 0.930493263506978, 0.93099512774705, 0.9309015163258549, 0.9313738159973826, 0.9314880905098188, 0.9310467830377883, 0.931646259743928, 0.931242017276225, 0.9319604646317577, 0.9313320435555894, 0.9322048263922057, 0.9319336064242962, 0.9316751832855679, 0.931683813772237, 0.931546578620003, 0.9322402836221745, 0.9316990147292835, 0.9320245267733323, 0.931409562387431, 0.9324166723786677, 0.9322021896068048, 0.9323869693234951, 0.9318131729129522, 0.932550536655582], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:00<00:09,  1.04it/s]predicting test subjects:  18%|█▊        | 2/11 [00:01<00:06,  1.31it/s]predicting test subjects:  27%|██▋       | 3/11 [00:01<00:05,  1.51it/s]predicting test subjects:  36%|███▋      | 4/11 [00:01<00:03,  1.87it/s]predicting test subjects:  45%|████▌     | 5/11 [00:02<00:02,  2.15it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:02<00:02,  2.43it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:02<00:01,  2.65it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:03<00:01,  2.81it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:03<00:00,  2.76it/s]predicting test subjects:  91%|█████████ | 10/11 [00:03<00:00,  2.86it/s]predicting test subjects: 100%|██████████| 11/11 [00:04<00:00,  2.73it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:15,  2.58it/s]predicting train subjects:   5%|▍         | 2/41 [00:00<00:14,  2.65it/s]predicting train subjects:   7%|▋         | 3/41 [00:01<00:13,  2.74it/s]predicting train subjects:  10%|▉         | 4/41 [00:01<00:12,  3.01it/s]predicting train subjects:  12%|█▏        | 5/41 [00:01<00:13,  2.76it/s]predicting train subjects:  15%|█▍        | 6/41 [00:02<00:13,  2.64it/s]predicting train subjects:  17%|█▋        | 7/41 [00:02<00:12,  2.83it/s]predicting train subjects:  20%|█▉        | 8/41 [00:02<00:11,  2.83it/s]predicting train subjects:  22%|██▏       | 9/41 [00:03<00:10,  3.08it/s]predicting train subjects:  24%|██▍       | 10/41 [00:03<00:09,  3.15it/s]predicting train subjects:  27%|██▋       | 11/41 [00:03<00:09,  3.04it/s]predicting train subjects:  29%|██▉       | 12/41 [00:03<00:08,  3.36it/s]predicting train subjects:  32%|███▏      | 13/41 [00:04<00:08,  3.15it/s]predicting train subjects:  34%|███▍      | 14/41 [00:04<00:09,  2.87it/s]predicting train subjects:  37%|███▋      | 15/41 [00:05<00:08,  3.10it/s]predicting train subjects:  39%|███▉      | 16/41 [00:05<00:07,  3.13it/s]predicting train subjects:  41%|████▏     | 17/41 [00:05<00:08,  2.93it/s]predicting train subjects:  44%|████▍     | 18/41 [00:06<00:07,  2.94it/s]predicting train subjects:  46%|████▋     | 19/41 [00:06<00:07,  3.04it/s]predicting train subjects:  49%|████▉     | 20/41 [00:06<00:07,  2.98it/s]predicting train subjects:  51%|█████     | 21/41 [00:06<00:06,  3.17it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:07<00:05,  3.27it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:07<00:04,  3.61it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:07<00:05,  3.30it/s]predicting train subjects:  61%|██████    | 25/41 [00:08<00:04,  3.33it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:08<00:05,  2.97it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:08<00:04,  2.82it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:09<00:04,  2.81it/s]predicting train subjects:  71%|███████   | 29/41 [00:09<00:03,  3.00it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:09<00:03,  2.96it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:10<00:03,  2.87it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:10<00:03,  2.88it/s]predicting train subjects:  80%|████████  | 33/41 [00:10<00:02,  2.89it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:11<00:02,  2.64it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:11<00:02,  2.43it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:12<00:02,  2.25it/s]predicting train subjects:  90%|█████████ | 37/41 [00:12<00:01,  2.36it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:13<00:01,  2.46it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:13<00:00,  2.70it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:13<00:00,  2.77it/s]predicting train subjects: 100%|██████████| 41/41 [00:14<00:00,  2.68it/s]
predicting test subjects sagittal:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects sagittal:   9%|▉         | 1/11 [00:00<00:02,  3.34it/s]predicting test subjects sagittal:  18%|█▊        | 2/11 [00:00<00:02,  3.31it/s]predicting test subjects sagittal:  27%|██▋       | 3/11 [00:00<00:02,  3.31it/s]predicting test subjects sagittal:  36%|███▋      | 4/11 [00:01<00:02,  3.25it/s]predicting test subjects sagittal:  45%|████▌     | 5/11 [00:01<00:01,  3.43it/s]predicting test subjects sagittal:  55%|█████▍    | 6/11 [00:01<00:01,  3.36it/s]predicting test subjects sagittal:  64%|██████▎   | 7/11 [00:02<00:01,  3.45it/s]predicting test subjects sagittal:  73%|███████▎  | 8/11 [00:02<00:00,  3.48it/s]predicting test subjects sagittal:  82%|████████▏ | 9/11 [00:02<00:00,  3.51it/s]predicting test subjects sagittal:  91%|█████████ | 10/11 [00:02<00:00,  3.50it/s]predicting test subjects sagittal: 100%|██████████| 11/11 [00:03<00:00,  3.25it/s]
predicting train subjects sagittal:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects sagittal:   2%|▏         | 1/41 [00:00<00:12,  3.20it/s]predicting train subjects sagittal:   5%|▍         | 2/41 [00:00<00:12,  3.19it/s]predicting train subjects sagittal:   7%|▋         | 3/41 [00:00<00:11,  3.36it/s]predicting train subjects sagittal:  10%|▉         | 4/41 [00:01<00:10,  3.40it/s]predicting train subjects sagittal:  12%|█▏        | 5/41 [00:01<00:11,  3.27it/s]predicting train subjects sagittal:  15%|█▍        | 6/41 [00:01<00:10,  3.32it/s]predicting train subjects sagittal:  17%|█▋        | 7/41 [00:02<00:10,  3.28it/s]predicting train subjects sagittal:  20%|█▉        | 8/41 [00:02<00:09,  3.52it/s]predicting train subjects sagittal:  22%|██▏       | 9/41 [00:02<00:09,  3.44it/s]predicting train subjects sagittal:  24%|██▍       | 10/41 [00:02<00:09,  3.39it/s]predicting train subjects sagittal:  27%|██▋       | 11/41 [00:03<00:09,  3.25it/s]predicting train subjects sagittal:  29%|██▉       | 12/41 [00:03<00:07,  3.63it/s]predicting train subjects sagittal:  32%|███▏      | 13/41 [00:03<00:08,  3.29it/s]predicting train subjects sagittal:  34%|███▍      | 14/41 [00:04<00:07,  3.50it/s]predicting train subjects sagittal:  37%|███▋      | 15/41 [00:04<00:06,  3.73it/s]predicting train subjects sagittal:  39%|███▉      | 16/41 [00:04<00:07,  3.39it/s]predicting train subjects sagittal:  41%|████▏     | 17/41 [00:05<00:07,  3.06it/s]predicting train subjects sagittal:  44%|████▍     | 18/41 [00:05<00:07,  3.12it/s]predicting train subjects sagittal:  46%|████▋     | 19/41 [00:05<00:07,  3.04it/s]predicting train subjects sagittal:  49%|████▉     | 20/41 [00:05<00:06,  3.37it/s]predicting train subjects sagittal:  51%|█████     | 21/41 [00:06<00:05,  3.69it/s]predicting train subjects sagittal:  54%|█████▎    | 22/41 [00:06<00:05,  3.51it/s]predicting train subjects sagittal:  56%|█████▌    | 23/41 [00:06<00:04,  3.85it/s]predicting train subjects sagittal:  59%|█████▊    | 24/41 [00:07<00:04,  3.44it/s]predicting train subjects sagittal:  61%|██████    | 25/41 [00:07<00:04,  3.31it/s]predicting train subjects sagittal:  63%|██████▎   | 26/41 [00:07<00:04,  3.26it/s]predicting train subjects sagittal:  66%|██████▌   | 27/41 [00:08<00:04,  3.19it/s]predicting train subjects sagittal:  68%|██████▊   | 28/41 [00:08<00:04,  3.15it/s]predicting train subjects sagittal:  71%|███████   | 29/41 [00:08<00:03,  3.24it/s]predicting train subjects sagittal:  73%|███████▎  | 30/41 [00:09<00:03,  3.02it/s]predicting train subjects sagittal:  76%|███████▌  | 31/41 [00:09<00:03,  2.70it/s]predicting train subjects sagittal:  78%|███████▊  | 32/41 [00:09<00:03,  2.75it/s]predicting train subjects sagittal:  80%|████████  | 33/41 [00:10<00:02,  2.80it/s]predicting train subjects sagittal:  83%|████████▎ | 34/41 [00:10<00:02,  2.67it/s]predicting train subjects sagittal:  85%|████████▌ | 35/41 [00:11<00:02,  2.52it/s]predicting train subjects sagittal:  88%|████████▊ | 36/41 [00:11<00:02,  2.41it/s]predicting train subjects sagittal:  90%|█████████ | 37/41 [00:11<00:01,  2.43it/s]predicting train subjects sagittal:  93%|█████████▎| 38/41 [00:12<00:01,  2.63it/s]predicting train subjects sagittal:  95%|█████████▌| 39/41 [00:12<00:00,  2.64it/s]predicting train subjects sagittal:  98%|█████████▊| 40/41 [00:12<00:00,  2.65it/s]predicting train subjects sagittal: 100%|██████████| 41/41 [00:13<00:00,  2.70it/s]
saving BB  test1-THALAMUS:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  73%|███████▎  | 8/11 [00:00<00:00, 78.02it/s]saving BB  test1-THALAMUS: 100%|██████████| 11/11 [00:00<00:00, 77.33it/s]
saving BB  train1-THALAMUS:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS:  20%|█▉        | 8/41 [00:00<00:00, 77.08it/s]saving BB  train1-THALAMUS:  37%|███▋      | 15/41 [00:00<00:00, 72.23it/s]saving BB  train1-THALAMUS:  54%|█████▎    | 22/41 [00:00<00:00, 69.45it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 30/41 [00:00<00:00, 71.75it/s]saving BB  train1-THALAMUS:  90%|█████████ | 37/41 [00:00<00:00, 69.77it/s]saving BB  train1-THALAMUS: 100%|██████████| 41/41 [00:00<00:00, 67.83it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal:  55%|█████▍    | 6/11 [00:00<00:00, 59.16it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 11/11 [00:00<00:00, 67.22it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:  22%|██▏       | 9/41 [00:00<00:00, 83.91it/s]saving BB  train1-THALAMUS Sagittal:  44%|████▍     | 18/41 [00:00<00:00, 84.17it/s]saving BB  train1-THALAMUS Sagittal:  66%|██████▌   | 27/41 [00:00<00:00, 85.26it/s]saving BB  train1-THALAMUS Sagittal:  85%|████████▌ | 35/41 [00:00<00:00, 79.17it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 41/41 [00:00<00:00, 67.06it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:23,  1.73it/s]Loading train:   5%|▍         | 2/41 [00:01<00:22,  1.74it/s]Loading train:   7%|▋         | 3/41 [00:01<00:20,  1.83it/s]Loading train:  10%|▉         | 4/41 [00:02<00:20,  1.84it/s]Loading train:  12%|█▏        | 5/41 [00:02<00:21,  1.70it/s]Loading train:  15%|█▍        | 6/41 [00:03<00:22,  1.57it/s]Loading train:  17%|█▋        | 7/41 [00:04<00:20,  1.65it/s]Loading train:  20%|█▉        | 8/41 [00:04<00:17,  1.88it/s]Loading train:  22%|██▏       | 9/41 [00:05<00:17,  1.84it/s]Loading train:  24%|██▍       | 10/41 [00:05<00:17,  1.77it/s]Loading train:  27%|██▋       | 11/41 [00:06<00:17,  1.74it/s]Loading train:  29%|██▉       | 12/41 [00:06<00:14,  2.00it/s]Loading train:  32%|███▏      | 13/41 [00:07<00:13,  2.06it/s]Loading train:  34%|███▍      | 14/41 [00:07<00:13,  1.95it/s]Loading train:  37%|███▋      | 15/41 [00:07<00:11,  2.19it/s]Loading train:  39%|███▉      | 16/41 [00:08<00:11,  2.16it/s]Loading train:  41%|████▏     | 17/41 [00:09<00:12,  1.93it/s]Loading train:  44%|████▍     | 18/41 [00:09<00:12,  1.83it/s]Loading train:  46%|████▋     | 19/41 [00:10<00:12,  1.81it/s]Loading train:  49%|████▉     | 20/41 [00:10<00:10,  1.93it/s]Loading train:  51%|█████     | 21/41 [00:11<00:10,  1.82it/s]Loading train:  54%|█████▎    | 22/41 [00:11<00:10,  1.78it/s]Loading train:  56%|█████▌    | 23/41 [00:12<00:08,  2.00it/s]Loading train:  59%|█████▊    | 24/41 [00:12<00:08,  2.03it/s]Loading train:  61%|██████    | 25/41 [00:13<00:08,  1.89it/s]Loading train:  63%|██████▎   | 26/41 [00:13<00:08,  1.87it/s]Loading train:  66%|██████▌   | 27/41 [00:14<00:07,  1.88it/s]Loading train:  68%|██████▊   | 28/41 [00:15<00:07,  1.76it/s]Loading train:  71%|███████   | 29/41 [00:15<00:06,  1.85it/s]Loading train:  73%|███████▎  | 30/41 [00:16<00:06,  1.71it/s]Loading train:  76%|███████▌  | 31/41 [00:16<00:05,  1.78it/s]Loading train:  78%|███████▊  | 32/41 [00:17<00:05,  1.80it/s]Loading train:  80%|████████  | 33/41 [00:17<00:04,  1.72it/s]Loading train:  83%|████████▎ | 34/41 [00:18<00:04,  1.64it/s]Loading train:  85%|████████▌ | 35/41 [00:19<00:03,  1.57it/s]Loading train:  88%|████████▊ | 36/41 [00:20<00:03,  1.44it/s]Loading train:  90%|█████████ | 37/41 [00:20<00:02,  1.48it/s]Loading train:  93%|█████████▎| 38/41 [00:21<00:02,  1.48it/s]Loading train:  95%|█████████▌| 39/41 [00:22<00:01,  1.53it/s]Loading train:  98%|█████████▊| 40/41 [00:22<00:00,  1.55it/s]Loading train: 100%|██████████| 41/41 [00:23<00:00,  1.53it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 3/41 [00:00<00:01, 20.42it/s]concatenating: train:  12%|█▏        | 5/41 [00:00<00:01, 18.70it/s]concatenating: train:  34%|███▍      | 14/41 [00:00<00:01, 24.52it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 92.25it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:06,  1.59it/s]Loading test:  18%|█▊        | 2/11 [00:01<00:05,  1.58it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:05,  1.53it/s]Loading test:  36%|███▋      | 4/11 [00:02<00:04,  1.57it/s]Loading test:  45%|████▌     | 5/11 [00:03<00:03,  1.58it/s]Loading test:  55%|█████▍    | 6/11 [00:03<00:03,  1.57it/s]Loading test:  64%|██████▎   | 7/11 [00:04<00:02,  1.50it/s]Loading test:  73%|███████▎  | 8/11 [00:05<00:01,  1.60it/s]Loading test:  82%|████████▏ | 9/11 [00:05<00:01,  1.68it/s]Loading test:  91%|█████████ | 10/11 [00:06<00:00,  1.80it/s]Loading test: 100%|██████████| 11/11 [00:06<00:00,  1.79it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  27%|██▋       | 3/11 [00:00<00:00, 26.86it/s]concatenating: validation:  82%|████████▏ | 9/11 [00:00<00:00, 31.98it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 42.98it/s]
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 92, 116, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 92, 116, 20)  200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 92, 116, 20)  80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 92, 116, 20)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 92, 116, 20)  3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 92, 116, 20)  80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 92, 116, 20)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 46, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 46, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 46, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 46, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 46, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 46, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 46, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 46, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 46, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 23, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 23, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 23, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 23, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 23, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 23, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 23, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 23, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 23, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 23, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 46, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 46, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 46, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 46, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 46, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 46, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 46, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 46, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 92, 116, 20)  11220       dropout_4[0][0]                  2019-07-27 18:09:49.384472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-27 18:09:49.384562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-27 18:09:49.384579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-27 18:09:49.384588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-27 18:09:49.384953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 92, 116, 40)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 92, 116, 20)  7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 92, 116, 20)  80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 92, 116, 20)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 92, 116, 20)  3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 92, 116, 20)  80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 92, 116, 20)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 92, 116, 60)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 92, 116, 60)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 92, 116, 2)   122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
----- /array/ssd/msmajdi/experiments/keras/exp6/models/sE8_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1/1-THALAMUS/sd1/model_weights.h5
 --- initialized from Model_3T /array/ssd/msmajdi/experiments/keras/exp6/models/sE8_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1/1-THALAMUS/sd1
------------------------------------------------------------------
class_weights [0.98000917 0.01999083]
Train on 4060 samples, validate on 1096 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.0497 - acc: 0.9962 - mDice: 0.9072 - val_loss: 0.0464 - val_acc: 0.9966 - val_mDice: 0.9132

Epoch 00001: val_mDice improved from -inf to 0.91316, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0464 - acc: 0.9965 - mDice: 0.9131 - val_loss: 0.0460 - val_acc: 0.9967 - val_mDice: 0.9137

Epoch 00002: val_mDice improved from 0.91316 to 0.91369, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0445 - acc: 0.9966 - mDice: 0.9164 - val_loss: 0.0447 - val_acc: 0.9967 - val_mDice: 0.9162

Epoch 00003: val_mDice improved from 0.91369 to 0.91616, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0439 - acc: 0.9967 - mDice: 0.9176 - val_loss: 0.0443 - val_acc: 0.9967 - val_mDice: 0.9169

Epoch 00004: val_mDice improved from 0.91616 to 0.91686, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0429 - acc: 0.9968 - mDice: 0.9192 - val_loss: 0.0446 - val_acc: 0.9967 - val_mDice: 0.9163

Epoch 00005: val_mDice did not improve from 0.91686
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0422 - acc: 0.9968 - mDice: 0.9205 - val_loss: 0.0442 - val_acc: 0.9967 - val_mDice: 0.9170

Epoch 00006: val_mDice improved from 0.91686 to 0.91700, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0416 - acc: 0.9969 - mDice: 0.9217 - val_loss: 0.0443 - val_acc: 0.9967 - val_mDice: 0.9169

Epoch 00007: val_mDice did not improve from 0.91700
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0413 - acc: 0.9969 - mDice: 0.9221 - val_loss: 0.0446 - val_acc: 0.9967 - val_mDice: 0.9164

Epoch 00008: val_mDice did not improve from 0.91700
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0407 - acc: 0.9969 - mDice: 0.9234 - val_loss: 0.0445 - val_acc: 0.9967 - val_mDice: 0.9164

Epoch 00009: val_mDice did not improve from 0.91700
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0402 - acc: 0.9970 - mDice: 0.9242 - val_loss: 0.0437 - val_acc: 0.9968 - val_mDice: 0.9179

Epoch 00010: val_mDice improved from 0.91700 to 0.91794, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0401 - acc: 0.9970 - mDice: 0.9244 - val_loss: 0.0440 - val_acc: 0.9968 - val_mDice: 0.9174

Epoch 00011: val_mDice did not improve from 0.91794
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0399 - acc: 0.9970 - mDice: 0.9248 - val_loss: 0.0455 - val_acc: 0.9967 - val_mDice: 0.9147

Epoch 00012: val_mDice did not improve from 0.91794
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0392 - acc: 0.9970 - mDice: 0.9259 - val_loss: 0.0440 - val_acc: 0.9968 - val_mDice: 0.9174

Epoch 00013: val_mDice did not improve from 0.91794
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0389 - acc: 0.9971 - mDice: 0.9265 - val_loss: 0.0452 - val_acc: 0.9967 - val_mDice: 0.9152

Epoch 00014: val_mDice did not improve from 0.91794
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0387 - acc: 0.9971 - mDice: 0.9269 - val_loss: 0.0454 - val_acc: 0.9967 - val_mDice: 0.9149

Epoch 00015: val_mDice did not improve from 0.91794
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0380 - acc: 0.9971 - mDice: 0.9282 - val_loss: 0.0472 - val_acc: 0.9966 - val_mDice: 0.9116

Epoch 00016: val_mDice did not improve from 0.91794
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0379 - acc: 0.9971 - mDice: 0.9284 - val_loss: 0.0460 - val_acc: 0.9967 - val_mDice: 0.9137

Epoch 00017: val_mDice did not improve from 0.91794
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0377 - acc: 0.9971 - mDice: 0.9287 - val_loss: 0.0458 - val_acc: 0.9967 - val_mDice: 0.9142

Epoch 00018: val_mDice did not improve from 0.91794
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0371 - acc: 0.9972 - mDice: 0.9298 - val_loss: 0.0458 - val_acc: 0.9967 - val_mDice: 0.9141

Epoch 00019: val_mDice did not improve from 0.91794
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0366 - acc: 0.9972 - mDice: 0.9308 - val_loss: 0.0451 - val_acc: 0.9967 - val_mDice: 0.9154

Epoch 00020: val_mDice did not improve from 0.91794
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0363 - acc: 0.9972 - mDice: 0.9313 - val_loss: 0.0452 - val_acc: 0.9967 - val_mDice: 0.9152

Epoch 00021: val_mDice did not improve from 0.91794
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0365 - acc: 0.9973 - mDice: 0.9310 - val_loss: 0.0458 - val_acc: 0.9967 - val_mDice: 0.9142

Epoch 00022: val_mDice did not improve from 0.91794
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0364 - acc: 0.9972 - mDice: 0.9310 - val_loss: 0.0449 - val_acc: 0.9967 - val_mDice: 0.9157

Epoch 00023: val_mDice did not improve from 0.91794
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0362 - acc: 0.9973 - mDice: 0.9315 - val_loss: 0.0454 - val_acc: 0.9967 - val_mDice: 0.9148

Epoch 00024: val_mDice did not improve from 0.91794
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0361 - acc: 0.9973 - mDice: 0.9317 - val_loss: 0.0452 - val_acc: 0.9967 - val_mDice: 0.9151

Epoch 00025: val_mDice did not improve from 0.91794
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0359 - acc: 0.9973 - mDice: 0.9321 - val_loss: 0.0456 - val_acc: 0.9967 - val_mDice: 0.9145

Epoch 00026: val_mDice did not improve from 0.91794
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0355 - acc: 0.9973 - mDice: 0.9328 - val_loss: 0.0447 - val_acc: 0.9967 - val_mDice: 0.9161

Epoch 00027: val_mDice did not improve from 0.91794
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0356 - acc: 0.9973 - mDice: 0.9326 - val_loss: 0.0462 - val_acc: 0.9967 - val_mDice: 0.9134

Epoch 00028: val_mDice did not improve from 0.91794
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0354 - acc: 0.9973 - mDice: 0.9330 - val_loss: 0.0470 - val_acc: 0.9966 - val_mDice: 0.9119

Epoch 00029: val_mDice did not improve from 0.91794
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0352 - acc: 0.9973 - mDice: 0.9333 - val_loss: 0.0455 - val_acc: 0.9967 - val_mDice: 0.9147

Epoch 00030: val_mDice did not improve from 0.91794
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0349 - acc: 0.9974 - mDice: 0.9338 - val_loss: 0.0448 - val_acc: 0.9967 - val_mDice: 0.9160

Epoch 00031: val_mDice did not improve from 0.91794
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0350 - acc: 0.9973 - mDice: 0.9337 - val_loss: 0.0464 - val_acc: 0.9966 - val_mDice: 0.9131

Epoch 00032: val_mDice did not improve from 0.91794
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0348 - acc: 0.9974 - mDice: 0.9341 - val_loss: 0.0451 - val_acc: 0.9967 - val_mDice: 0.9153

Epoch 00033: val_mDice did not improve from 0.91794
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0347 - acc: 0.9974 - mDice: 0.9343 - val_loss: 0.0469 - val_acc: 0.9966 - val_mDice: 0.9122

Epoch 00034: val_mDice did not improve from 0.91794
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0347 - acc: 0.9974 - mDice: 0.9343 - val_loss: 0.0469 - val_acc: 0.9966 - val_mDice: 0.9122

Epoch 00035: val_mDice did not improve from 0.91794
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0344 - acc: 0.9974 - mDice: 0.9347 - val_loss: 0.0459 - val_acc: 0.9967 - val_mDice: 0.9140

Epoch 00036: val_mDice did not improve from 0.91794
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0344 - acc: 0.9974 - mDice: 0.9348 - val_loss: 0.0462 - val_acc: 0.9966 - val_mDice: 0.9135

Epoch 00037: val_mDice did not improve from 0.91794
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0342 - acc: 0.9974 - mDice: 0.9351 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9135

Epoch 00038: val_mDice did not improve from 0.91794
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0341 - acc: 0.9974 - mDice: 0.9354 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9137

Epoch 00039: val_mDice did not improve from 0.91794
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0340 - acc: 0.9974 - mDice: 0.9354 - val_loss: 0.0468 - val_acc: 0.9966 - val_mDice: 0.9123

Epoch 00040: val_mDice did not improve from 0.91794
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0339 - acc: 0.9974 - mDice: 0.9356 - val_loss: 0.0475 - val_acc: 0.9966 - val_mDice: 0.9110

Epoch 00041: val_mDice did not improve from 0.91794
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0339 - acc: 0.9974 - mDice: 0.9358 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9136

Epoch 00042: val_mDice did not improve from 0.91794
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0339 - acc: 0.9974 - mDice: 0.9357 - val_loss: 0.0465 - val_acc: 0.9966 - val_mDice: 0.9129

Epoch 00043: val_mDice did not improve from 0.91794
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0337 - acc: 0.9974 - mDice: 0.9360 - val_loss: 0.0457 - val_acc: 0.9967 - val_mDice: 0.9143

Epoch 00044: val_mDice did not improve from 0.91794
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0339 - acc: 0.9974 - mDice: 0.9357 - val_loss: 0.0457 - val_acc: 0.9967 - val_mDice: 0.9144

Epoch 00045: val_mDice did not improve from 0.91794
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0338 - acc: 0.9974 - mDice: 0.9359 - val_loss: 0.0470 - val_acc: 0.9966 - val_mDice: 0.9119

Epoch 00046: val_mDice did not improve from 0.91794
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0336 - acc: 0.9975 - mDice: 0.9363 - val_loss: 0.0453 - val_acc: 0.9967 - val_mDice: 0.9150

Epoch 00047: val_mDice did not improve from 0.91794
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0335 - acc: 0.9975 - mDice: 0.9364 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9137

Epoch 00048: val_mDice did not improve from 0.91794
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0334 - acc: 0.9975 - mDice: 0.9366 - val_loss: 0.0459 - val_acc: 0.9966 - val_mDice: 0.9140

Epoch 00049: val_mDice did not improve from 0.91794
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0334 - acc: 0.9975 - mDice: 0.9367 - val_loss: 0.0467 - val_acc: 0.9966 - val_mDice: 0.9125

predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:01<00:10,  1.04s/it]predicting test subjects:  18%|█▊        | 2/11 [00:01<00:07,  1.19it/s]predicting test subjects:  27%|██▋       | 3/11 [00:01<00:05,  1.39it/s]predicting test subjects:  36%|███▋      | 4/11 [00:02<00:04,  1.59it/s]predicting test subjects:  45%|████▌     | 5/11 [00:02<00:03,  1.79it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:03<00:02,  1.98it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:03<00:02,  1.97it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:03<00:01,  2.11it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:04<00:00,  2.29it/s]predicting test subjects:  91%|█████████ | 10/11 [00:04<00:00,  2.40it/s]predicting test subjects: 100%|██████████| 11/11 [00:05<00:00,  2.40it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:20,  1.98it/s]predicting train subjects:   5%|▍         | 2/41 [00:00<00:18,  2.12it/s]predicting train subjects:   7%|▋         | 3/41 [00:01<00:17,  2.22it/s]predicting train subjects:  10%|▉         | 4/41 [00:01<00:16,  2.26it/s]predicting train subjects:  12%|█▏        | 5/41 [00:02<00:16,  2.14it/s]predicting train subjects:  15%|█▍        | 6/41 [00:02<00:16,  2.13it/s]predicting train subjects:  17%|█▋        | 7/41 [00:03<00:15,  2.18it/s]predicting train subjects:  20%|█▉        | 8/41 [00:03<00:13,  2.54it/s]predicting train subjects:  22%|██▏       | 9/41 [00:03<00:12,  2.57it/s]predicting train subjects:  24%|██▍       | 10/41 [00:04<00:12,  2.49it/s]predicting train subjects:  27%|██▋       | 11/41 [00:04<00:12,  2.45it/s]predicting train subjects:  29%|██▉       | 12/41 [00:04<00:10,  2.78it/s]predicting train subjects:  32%|███▏      | 13/41 [00:05<00:10,  2.74it/s]predicting train subjects:  34%|███▍      | 14/41 [00:05<00:09,  2.71it/s]predicting train subjects:  37%|███▋      | 15/41 [00:05<00:08,  2.89it/s]predicting train subjects:  39%|███▉      | 16/41 [00:06<00:08,  2.85it/s]predicting train subjects:  41%|████▏     | 17/41 [00:06<00:09,  2.58it/s]predicting train subjects:  44%|████▍     | 18/41 [00:07<00:09,  2.45it/s]predicting train subjects:  46%|████▋     | 19/41 [00:07<00:08,  2.48it/s]predicting train subjects:  49%|████▉     | 20/41 [00:07<00:07,  2.65it/s]predicting train subjects:  51%|█████     | 21/41 [00:08<00:07,  2.68it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:08<00:07,  2.66it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:08<00:06,  2.91it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:09<00:06,  2.82it/s]predicting train subjects:  61%|██████    | 25/41 [00:09<00:05,  2.69it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:10<00:05,  2.50it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:10<00:06,  2.30it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:11<00:05,  2.29it/s]predicting train subjects:  71%|███████   | 29/41 [00:11<00:04,  2.41it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:11<00:04,  2.40it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:12<00:04,  2.28it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:12<00:04,  2.24it/s]predicting train subjects:  80%|████████  | 33/41 [00:13<00:03,  2.30it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:13<00:03,  2.30it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:14<00:03,  1.94it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:15<00:02,  1.87it/s]predicting train subjects:  90%|█████████ | 37/41 [00:15<00:02,  1.88it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:15<00:01,  2.01it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:16<00:00,  2.20it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:16<00:00,  2.20it/s]predicting train subjects: 100%|██████████| 41/41 [00:17<00:00,  2.21it/s]
saving BB  test1-THALAMUS:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  73%|███████▎  | 8/11 [00:00<00:00, 73.33it/s]saving BB  test1-THALAMUS: 100%|██████████| 11/11 [00:00<00:00, 71.94it/s]
saving BB  train1-THALAMUS:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS:  20%|█▉        | 8/41 [00:00<00:00, 76.04it/s]saving BB  train1-THALAMUS:  37%|███▋      | 15/41 [00:00<00:00, 74.08it/s]saving BB  train1-THALAMUS:  54%|█████▎    | 22/41 [00:00<00:00, 72.59it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 30/41 [00:00<00:00, 72.79it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 36/41 [00:00<00:00, 67.28it/s]saving BB  train1-THALAMUS: 100%|██████████| 41/41 [00:00<00:00, 68.78it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:27,  1.46it/s]Loading train:   5%|▍         | 2/41 [00:01<00:27,  1.43it/s]Loading train:   7%|▋         | 3/41 [00:02<00:25,  1.50it/s]Loading train:  10%|▉         | 4/41 [00:02<00:22,  1.68it/s]Loading train:  12%|█▏        | 5/41 [00:03<00:21,  1.69it/s]Loading train:  15%|█▍        | 6/41 [00:03<00:19,  1.76it/s]Loading train:  17%|█▋        | 7/41 [00:03<00:18,  1.87it/s]Loading train:  20%|█▉        | 8/41 [00:04<00:16,  1.97it/s]Loading train:  22%|██▏       | 9/41 [00:05<00:17,  1.85it/s]Loading train:  24%|██▍       | 10/41 [00:05<00:17,  1.78it/s]Loading train:  27%|██▋       | 11/41 [00:06<00:16,  1.82it/s]Loading train:  29%|██▉       | 12/41 [00:06<00:14,  2.00it/s]Loading train:  32%|███▏      | 13/41 [00:07<00:14,  1.97it/s]Loading train:  34%|███▍      | 14/41 [00:07<00:13,  1.97it/s]Loading train:  37%|███▋      | 15/41 [00:08<00:12,  2.08it/s]Loading train:  39%|███▉      | 16/41 [00:08<00:12,  1.97it/s]Loading train:  41%|████▏     | 17/41 [00:09<00:12,  1.87it/s]Loading train:  44%|████▍     | 18/41 [00:09<00:12,  1.84it/s]Loading train:  46%|████▋     | 19/41 [00:10<00:11,  1.87it/s]Loading train:  49%|████▉     | 20/41 [00:10<00:10,  1.97it/s]Loading train:  51%|█████     | 21/41 [00:11<00:09,  2.00it/s]Loading train:  54%|█████▎    | 22/41 [00:11<00:09,  2.04it/s]Loading train:  56%|█████▌    | 23/41 [00:12<00:08,  2.21it/s]Loading train:  59%|█████▊    | 24/41 [00:12<00:08,  1.99it/s]Loading train:  61%|██████    | 25/41 [00:13<00:08,  1.92it/s]Loading train:  63%|██████▎   | 26/41 [00:13<00:07,  1.95it/s]Loading train:  66%|██████▌   | 27/41 [00:14<00:07,  1.81it/s]Loading train:  68%|██████▊   | 28/41 [00:14<00:07,  1.77it/s]Loading train:  71%|███████   | 29/41 [00:15<00:06,  1.88it/s]Loading train:  73%|███████▎  | 30/41 [00:15<00:05,  1.84it/s]Loading train:  76%|███████▌  | 31/41 [00:16<00:05,  1.83it/s]Loading train:  78%|███████▊  | 32/41 [00:17<00:04,  1.82it/s]Loading train:  80%|████████  | 33/41 [00:17<00:04,  1.75it/s]Loading train:  83%|████████▎ | 34/41 [00:18<00:04,  1.72it/s]Loading train:  85%|████████▌ | 35/41 [00:19<00:03,  1.55it/s]Loading train:  88%|████████▊ | 36/41 [00:19<00:03,  1.54it/s]Loading train:  90%|█████████ | 37/41 [00:20<00:02,  1.63it/s]Loading train:  93%|█████████▎| 38/41 [00:20<00:01,  1.62it/s]Loading train:  95%|█████████▌| 39/41 [00:21<00:01,  1.73it/s]Loading train:  98%|█████████▊| 40/41 [00:21<00:00,  1.75it/s]Loading train: 100%|██████████| 41/41 [00:22<00:00,  1.78it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 3/41 [00:00<00:01, 26.08it/s]concatenating: train:  17%|█▋        | 7/41 [00:00<00:01, 27.44it/s]concatenating: train:  24%|██▍       | 10/41 [00:00<00:01, 26.11it/s]concatenating: train:  32%|███▏      | 13/41 [00:00<00:01, 26.87it/s]concatenating: train:  39%|███▉      | 16/41 [00:00<00:01, 24.72it/s]concatenating: train:  46%|████▋     | 19/41 [00:00<00:00, 23.41it/s]concatenating: train:  54%|█████▎    | 22/41 [00:00<00:00, 22.57it/s]concatenating: train:  61%|██████    | 25/41 [00:01<00:00, 23.47it/s]concatenating: train:  73%|███████▎  | 30/41 [00:01<00:00, 27.52it/s]concatenating: train:  88%|████████▊ | 36/41 [00:01<00:00, 32.25it/s]concatenating: train:  98%|█████████▊| 40/41 [00:01<00:00, 30.76it/s]concatenating: train: 100%|██████████| 41/41 [00:01<00:00, 28.81it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:05,  1.70it/s]Loading test:  18%|█▊        | 2/11 [00:01<00:05,  1.70it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:04,  1.70it/s]Loading test:  36%|███▋      | 4/11 [00:02<00:03,  1.80it/s]Loading test:  45%|████▌     | 5/11 [00:02<00:03,  1.84it/s]Loading test:  55%|█████▍    | 6/11 [00:03<00:02,  1.82it/s]Loading test:  64%|██████▎   | 7/11 [00:03<00:02,  1.77it/s]Loading test:  73%|███████▎  | 8/11 [00:04<00:01,  1.77it/s]Loading test:  82%|████████▏ | 9/11 [00:04<00:01,  1.93it/s]Loading test:  91%|█████████ | 10/11 [00:05<00:00,  1.87it/s]Loading test: 100%|██████████| 11/11 [00:06<00:00,  1.86it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  18%|█▊        | 2/11 [00:00<00:00, 18.99it/s]concatenating: validation:  55%|█████▍    | 6/11 [00:00<00:00, 21.52it/s]concatenating: validation:  82%|████████▏ | 9/11 [00:00<00:00, 20.78it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 22.65it/s]
Loading trainS:   0%|          | 0/41 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/41 [00:00<00:20,  1.97it/s]Loading trainS:   5%|▍         | 2/41 [00:01<00:20,  1.92it/s]Loading trainS:   7%|▋         | 3/41 [00:01<00:20,  1.87it/s]Loading trainS:  10%|▉         | 4/41 [00:02<00:19,  1.91it/s]Loading trainS:  12%|█▏        | 5/41 [00:02<00:20,  1.78it/s]Loading trainS:  15%|█▍        | 6/41 [00:03<00:19,  1.81it/s]Loading trainS:  17%|█▋        | 7/41 [00:03<00:18,  1.89it/s]Loading trainS:  20%|█▉        | 8/41 [00:04<00:16,  1.97it/s]Loading trainS:  22%|██▏       | 9/41 [00:04<00:17,  1.85it/s]Loading trainS:  24%|██▍       | 10/41 [00:05<00:17,  1.77it/s]Loading trainS:  27%|██▋       | 11/41 [00:06<00:17,  1.72it/s]Loading trainS:  29%|██▉       | 12/41 [00:06<00:15,  1.88it/s]Loading trainS:  32%|███▏      | 13/41 [00:07<00:15,  1.85it/s]Loading trainS:  34%|███▍      | 14/41 [00:07<00:14,  1.87it/s]Loading trainS:  37%|███▋      | 15/41 [00:08<00:12,  2.01it/s]Loading trainS:  39%|███▉      | 16/41 [00:08<00:12,  1.97it/s]Loading trainS:  41%|████▏     | 17/41 [00:09<00:13,  1.81it/s]Loading trainS:  44%|████▍     | 18/41 [00:09<00:12,  1.85it/s]Loading trainS:  46%|████▋     | 19/41 [00:10<00:11,  1.84it/s]Loading trainS:  49%|████▉     | 20/41 [00:10<00:10,  2.05it/s]Loading trainS:  51%|█████     | 21/41 [00:11<00:09,  2.10it/s]Loading trainS:  54%|█████▎    | 22/41 [00:11<00:09,  2.08it/s]Loading trainS:  56%|█████▌    | 23/41 [00:11<00:08,  2.20it/s]Loading trainS:  59%|█████▊    | 24/41 [00:12<00:07,  2.13it/s]Loading trainS:  61%|██████    | 25/41 [00:12<00:07,  2.18it/s]Loading trainS:  63%|██████▎   | 26/41 [00:13<00:07,  2.08it/s]Loading trainS:  66%|██████▌   | 27/41 [00:14<00:07,  1.87it/s]Loading trainS:  68%|██████▊   | 28/41 [00:14<00:07,  1.84it/s]Loading trainS:  71%|███████   | 29/41 [00:15<00:06,  1.92it/s]Loading trainS:  73%|███████▎  | 30/41 [00:15<00:05,  1.91it/s]Loading trainS:  76%|███████▌  | 31/41 [00:16<00:05,  1.92it/s]Loading trainS:  78%|███████▊  | 32/41 [00:16<00:04,  1.88it/s]Loading trainS:  80%|████████  | 33/41 [00:17<00:04,  1.82it/s]Loading trainS:  83%|████████▎ | 34/41 [00:17<00:03,  1.82it/s]Loading trainS:  85%|████████▌ | 35/41 [00:18<00:03,  1.67it/s]Loading trainS:  88%|████████▊ | 36/41 [00:19<00:03,  1.63it/s]Loading trainS:  90%|█████████ | 37/41 [00:19<00:02,  1.76it/s]Loading trainS:  93%|█████████▎| 38/41 [00:20<00:01,  1.72it/s]Loading trainS:  95%|█████████▌| 39/41 [00:20<00:01,  1.83it/s]Loading trainS:  98%|█████████▊| 40/41 [00:21<00:00,  1.85it/s]Loading trainS: 100%|██████████| 41/41 [00:21<00:00,  1.89it/s]
Loading testS:   0%|          | 0/11 [00:00<?, ?it/s]Loading testS:   9%|▉         | 1/11 [00:00<00:04,  2.00it/s]Loading testS:  18%|█▊        | 2/11 [00:01<00:04,  1.99it/s]Loading testS:  27%|██▋       | 3/11 [00:01<00:04,  1.94it/s]Loading testS:  36%|███▋      | 4/11 [00:02<00:03,  1.99it/s]Loading testS:  45%|████▌     | 5/11 [00:02<00:02,  2.02it/s]Loading testS:  55%|█████▍    | 6/11 [00:02<00:02,  2.08it/s]Loading testS:  64%|██████▎   | 7/11 [00:03<00:02,  1.99it/s]Loading testS:  73%|███████▎  | 8/11 [00:04<00:01,  1.81it/s]Loading testS:  82%|████████▏ | 9/11 [00:04<00:01,  1.91it/s]Loading testS:  91%|█████████ | 10/11 [00:05<00:00,  1.79it/s]Loading testS: 100%|██████████| 11/11 [00:05<00:00,  1.80it/s]
Epoch 00050: val_mDice did not improve from 0.91794
Restoring model weights from the end of the best epoch
Epoch 00050: early stopping
{'val_loss': [0.04636828004498116, 0.04604346788467935, 0.04468195389847468, 0.04429244851000118, 0.0446205806449382, 0.044208986439952885, 0.04425492428623847, 0.04455750820356129, 0.04453602218388641, 0.04368331038603818, 0.04400473037721032, 0.04549318161813447, 0.043952246951143234, 0.045185491524256056, 0.04537958563407407, 0.04717328693092304, 0.046018099153998994, 0.04576912196013179, 0.04581904707707628, 0.04507322583592286, 0.045201384741133145, 0.045763737610439316, 0.04493177697116876, 0.04540307792651392, 0.045245152075577826, 0.04560582058327477, 0.044682741069989484, 0.046182580033901834, 0.047034336389960164, 0.04549349496399399, 0.04476864553009072, 0.04638577112588134, 0.045139813251847766, 0.04687621231007315, 0.04688836382634013, 0.04585752686499244, 0.04617469666702469, 0.04611302878245385, 0.04605424378311982, 0.04681043506321246, 0.04753275204749003, 0.04607684947006459, 0.046472301720267666, 0.04572367309218776, 0.045657166980043816, 0.047040977372522774, 0.04532189951380239, 0.04605818636389109, 0.04588435920648766, 0.04672523519962373], 'val_acc': [0.9966083079794027, 0.9966613887435328, 0.9967114105276818, 0.9967321790482876, 0.9966884198850089, 0.9967250088705634, 0.9967318477421782, 0.996745261397675, 0.9967470697677918, 0.9967651854466347, 0.9967532188352877, 0.9966519042088168, 0.9967729708139044, 0.9966996072852699, 0.9966973938646108, 0.9965961616839806, 0.9966501919892583, 0.9966619123507591, 0.9966692633002344, 0.996719446930572, 0.9967108164390508, 0.9966849199611775, 0.9967155184623969, 0.9966955994167467, 0.9966898712798626, 0.9966681512602924, 0.9967126926801501, 0.9966500247046896, 0.9966028243520834, 0.9966744845801027, 0.9967054333129939, 0.9966339492884866, 0.9966893481077069, 0.9965985541361092, 0.9966006168048748, 0.9966594276637056, 0.9966320723947817, 0.9966466859744413, 0.9966394316105947, 0.9966043714624252, 0.9965726554828839, 0.9966317334749403, 0.9966101774769108, 0.9966529361958051, 0.9966678904355878, 0.9965954775357768, 0.9966769599131424, 0.9966382299461504, 0.9966418151437801, 0.9965971753980122], 'val_mDice': [0.9131564149891374, 0.9136878465213915, 0.9161588682310424, 0.916856987415439, 0.9162912616764542, 0.9170000359524776, 0.916906350938073, 0.9163542196263362, 0.9164098814456132, 0.9179435113485712, 0.9173598735436906, 0.9147020293413287, 0.9174370160938179, 0.9152055500197585, 0.9148649566800054, 0.9116245005687658, 0.9137148332856867, 0.9141558556661119, 0.9140839779028928, 0.9154134131696102, 0.9151808021277407, 0.914169578656663, 0.9156769496681046, 0.9148204156952183, 0.9151107760241431, 0.9144743689655388, 0.9161353082987513, 0.9134074338596233, 0.9119001603039512, 0.9146606896045434, 0.9159766857641457, 0.9130589302873959, 0.9153085866548719, 0.9121870072218623, 0.9121586284498229, 0.9140071042262725, 0.9134505238846271, 0.9135497634428261, 0.9136603576858549, 0.9122929488220354, 0.9109895399017055, 0.9136268422551399, 0.9129174408251352, 0.9142594504965483, 0.9143736821456547, 0.9118739838582756, 0.9149808498629688, 0.9136540093561158, 0.9139732159402248, 0.9124606906497565], 'loss': [0.0497221720629725, 0.04639606783395918, 0.04452678414401162, 0.0438548898131683, 0.04294486175029736, 0.04224122699184958, 0.04160059193892432, 0.04134871023408885, 0.040665245867274664, 0.040189814941930066, 0.04008779702295223, 0.039876916927510296, 0.03923448657828012, 0.03892646289517727, 0.03869625929687998, 0.03800167280978757, 0.037868824883781634, 0.0377390387243238, 0.03713690696898939, 0.03658796863382673, 0.036318471646074005, 0.03645078564438914, 0.03644566166356866, 0.0361640132926955, 0.036057380564841146, 0.03585412063328503, 0.03547245711822228, 0.03559620056319707, 0.035383967545085354, 0.03519472723890995, 0.03490412361883177, 0.03499949088531175, 0.03475960099087854, 0.0346728454761464, 0.03466602194632216, 0.03441672529015929, 0.03435681216198529, 0.03421337933742941, 0.03406183900577681, 0.034044238018431684, 0.03394811947284074, 0.03386091200515554, 0.03388011208712467, 0.033748326501969635, 0.033881297968144486, 0.0337730764498558, 0.033554045942249554, 0.033512066113831375, 0.03341848052707799, 0.03336519625937116], 'acc': [0.996207765464125, 0.9964830649310145, 0.9966394525443392, 0.9966803903650181, 0.9967606425872577, 0.9968118324068379, 0.9968635736427871, 0.9968830303018317, 0.996936130699853, 0.9969603263685856, 0.9969693093464292, 0.9969900268052012, 0.99704145240079, 0.9970630530653328, 0.9970728608187783, 0.9971249241547044, 0.997133030092775, 0.9971429511831311, 0.9971926896442921, 0.9972392185568222, 0.9972474801129309, 0.9972506500230047, 0.9972447811676364, 0.9972543096894702, 0.9972721329463526, 0.9972858358486533, 0.9973156593116046, 0.997308734015291, 0.997325331119481, 0.9973360144446048, 0.9973576418284712, 0.9973440458034647, 0.9973635741055306, 0.9973793831364862, 0.9973706139132307, 0.9973937164386505, 0.9973977325585088, 0.9974045119262094, 0.9974149207176246, 0.9974189189267276, 0.9974254933484082, 0.9974370117257969, 0.9974360424896767, 0.9974413511201079, 0.9974293926079285, 0.9974444910810498, 0.9974511345031813, 0.9974530039749709, 0.9974621698774141, 0.9974760404361293], 'mDice': [0.9072041270768114, 0.9130766961374893, 0.9163996101600196, 0.9176093659964688, 0.9192375261795345, 0.9205057841216402, 0.9216620422936426, 0.9221177424116088, 0.9233522758695293, 0.9242199054492518, 0.9244011120843183, 0.9247835930345094, 0.9259498131099005, 0.9265093747618163, 0.9269274370423679, 0.9281856420592134, 0.9284312692769056, 0.9286688149268991, 0.9297607840575608, 0.9307632387565274, 0.9312621355056763, 0.931010043679787, 0.9310250869525477, 0.9315442965535695, 0.931730373739609, 0.9321039427677399, 0.932803783804325, 0.9325729752409047, 0.9329619281397664, 0.9333114315723551, 0.933840557272211, 0.93366781802013, 0.9341086912624942, 0.9342651458209371, 0.9342817302995128, 0.9347355010474256, 0.9348442871582332, 0.9351130958848399, 0.9353882027377048, 0.9354229311050453, 0.935598850543863, 0.9357552584168947, 0.9357171240698527, 0.9359621144280645, 0.9357213856551447, 0.9359169258859944, 0.9363216743093407, 0.936398680280582, 0.9365690620661956, 0.9366624828042656], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 128, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 128, 30) 300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 128, 30) 120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 128, 30) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 128, 30) 8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 128, 30) 120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 128, 30) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 64, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 64, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 64, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 64, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 64, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 64, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 64, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 64, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 64, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 32, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 32, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 32, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 32, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 32, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 32, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 32, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 32, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 32, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 32, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 64, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 64, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 64, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 64, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 64, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 64, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 64, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 64, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 64, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 64, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 128, 30) 25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 128, 60) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 128, 30) 16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 128, 30) 120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 128, 30) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 128, 30) 8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 128, 30) 120         conv2d_10[0][0]                  2019-07-27 18:17:31.118346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-27 18:17:31.118433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-27 18:17:31.118451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-27 18:17:31.118460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-27 18:17:31.118818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 128, 30) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 128, 90) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 128, 90) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 128, 2)  182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 499,142
Non-trainable params: 1,200
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97831385 0.02168615]
Train on 2690 samples, validate on 698 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 17s - loss: 0.7861 - acc: 0.8590 - mDice: 0.3375 - val_loss: 0.5788 - val_acc: 0.9388 - val_mDice: 0.3253

Epoch 00001: val_mDice improved from -inf to 0.32533, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.1301 - acc: 0.9893 - mDice: 0.7814 - val_loss: 0.3850 - val_acc: 0.9693 - val_mDice: 0.4733

Epoch 00002: val_mDice improved from 0.32533 to 0.47335, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0964 - acc: 0.9922 - mDice: 0.8341 - val_loss: 0.1150 - val_acc: 0.9924 - val_mDice: 0.8046

Epoch 00003: val_mDice improved from 0.47335 to 0.80463, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0861 - acc: 0.9931 - mDice: 0.8510 - val_loss: 0.0906 - val_acc: 0.9937 - val_mDice: 0.8442

Epoch 00004: val_mDice improved from 0.80463 to 0.84415, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0787 - acc: 0.9939 - mDice: 0.8632 - val_loss: 0.0750 - val_acc: 0.9943 - val_mDice: 0.8696

Epoch 00005: val_mDice improved from 0.84415 to 0.86957, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0746 - acc: 0.9942 - mDice: 0.8694 - val_loss: 0.0812 - val_acc: 0.9940 - val_mDice: 0.8579

Epoch 00006: val_mDice did not improve from 0.86957
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0703 - acc: 0.9947 - mDice: 0.8751 - val_loss: 0.0649 - val_acc: 0.9949 - val_mDice: 0.8817

Epoch 00007: val_mDice improved from 0.86957 to 0.88171, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0675 - acc: 0.9948 - mDice: 0.8781 - val_loss: 0.0584 - val_acc: 0.9952 - val_mDice: 0.8923

Epoch 00008: val_mDice improved from 0.88171 to 0.89233, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0637 - acc: 0.9951 - mDice: 0.8837 - val_loss: 0.0537 - val_acc: 0.9955 - val_mDice: 0.9004

Epoch 00009: val_mDice improved from 0.89233 to 0.90040, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0615 - acc: 0.9952 - mDice: 0.8873 - val_loss: 0.0548 - val_acc: 0.9955 - val_mDice: 0.8985

Epoch 00010: val_mDice did not improve from 0.90040
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0639 - acc: 0.9950 - mDice: 0.8830 - val_loss: 0.0528 - val_acc: 0.9956 - val_mDice: 0.9020

Epoch 00011: val_mDice improved from 0.90040 to 0.90198, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0597 - acc: 0.9953 - mDice: 0.8904 - val_loss: 0.0546 - val_acc: 0.9955 - val_mDice: 0.8989

Epoch 00012: val_mDice did not improve from 0.90198
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0584 - acc: 0.9954 - mDice: 0.8924 - val_loss: 0.0571 - val_acc: 0.9953 - val_mDice: 0.8944

Epoch 00013: val_mDice did not improve from 0.90198
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0568 - acc: 0.9955 - mDice: 0.8952 - val_loss: 0.0505 - val_acc: 0.9958 - val_mDice: 0.9061

Epoch 00014: val_mDice improved from 0.90198 to 0.90606, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0573 - acc: 0.9955 - mDice: 0.8943 - val_loss: 0.0619 - val_acc: 0.9950 - val_mDice: 0.8868

Epoch 00015: val_mDice did not improve from 0.90606
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0589 - acc: 0.9954 - mDice: 0.8916 - val_loss: 0.0517 - val_acc: 0.9957 - val_mDice: 0.9040

Epoch 00016: val_mDice did not improve from 0.90606
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0566 - acc: 0.9956 - mDice: 0.8955 - val_loss: 0.0496 - val_acc: 0.9958 - val_mDice: 0.9078

Epoch 00017: val_mDice improved from 0.90606 to 0.90776, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0542 - acc: 0.9957 - mDice: 0.8998 - val_loss: 0.0534 - val_acc: 0.9956 - val_mDice: 0.9011

Epoch 00018: val_mDice did not improve from 0.90776
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0545 - acc: 0.9957 - mDice: 0.8991 - val_loss: 0.0532 - val_acc: 0.9956 - val_mDice: 0.9016

Epoch 00019: val_mDice did not improve from 0.90776
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0509 - acc: 0.9960 - mDice: 0.9054 - val_loss: 0.0495 - val_acc: 0.9958 - val_mDice: 0.9079

Epoch 00020: val_mDice improved from 0.90776 to 0.90790, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0504 - acc: 0.9960 - mDice: 0.9063 - val_loss: 0.0589 - val_acc: 0.9951 - val_mDice: 0.8925

Epoch 00021: val_mDice did not improve from 0.90790
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0495 - acc: 0.9961 - mDice: 0.9080 - val_loss: 0.0667 - val_acc: 0.9944 - val_mDice: 0.8813

Epoch 00022: val_mDice did not improve from 0.90790
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0514 - acc: 0.9959 - mDice: 0.9046 - val_loss: 0.0487 - val_acc: 0.9959 - val_mDice: 0.9093

Epoch 00023: val_mDice improved from 0.90790 to 0.90926, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0489 - acc: 0.9961 - mDice: 0.9089 - val_loss: 0.0607 - val_acc: 0.9949 - val_mDice: 0.8894

Epoch 00024: val_mDice did not improve from 0.90926
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0499 - acc: 0.9961 - mDice: 0.9072 - val_loss: 0.0504 - val_acc: 0.9958 - val_mDice: 0.9062

Epoch 00025: val_mDice did not improve from 0.90926
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0488 - acc: 0.9961 - mDice: 0.9091 - val_loss: 0.0485 - val_acc: 0.9959 - val_mDice: 0.9097

Epoch 00026: val_mDice improved from 0.90926 to 0.90967, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0488 - acc: 0.9961 - mDice: 0.9092 - val_loss: 0.0478 - val_acc: 0.9959 - val_mDice: 0.9109

Epoch 00027: val_mDice improved from 0.90967 to 0.91088, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0473 - acc: 0.9962 - mDice: 0.9118 - val_loss: 0.0471 - val_acc: 0.9960 - val_mDice: 0.9121

Epoch 00028: val_mDice improved from 0.91088 to 0.91206, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0474 - acc: 0.9962 - mDice: 0.9116 - val_loss: 0.0483 - val_acc: 0.9958 - val_mDice: 0.9100

Epoch 00029: val_mDice did not improve from 0.91206
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0478 - acc: 0.9962 - mDice: 0.9109 - val_loss: 0.0466 - val_acc: 0.9960 - val_mDice: 0.9130

Epoch 00030: val_mDice improved from 0.91206 to 0.91299, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0486 - acc: 0.9961 - mDice: 0.9095 - val_loss: 0.0474 - val_acc: 0.9960 - val_mDice: 0.9116

Epoch 00031: val_mDice did not improve from 0.91299
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0465 - acc: 0.9963 - mDice: 0.9132 - val_loss: 0.0455 - val_acc: 0.9961 - val_mDice: 0.9150

Epoch 00032: val_mDice improved from 0.91299 to 0.91503, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0458 - acc: 0.9963 - mDice: 0.9145 - val_loss: 0.0499 - val_acc: 0.9958 - val_mDice: 0.9071

Epoch 00033: val_mDice did not improve from 0.91503
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0459 - acc: 0.9963 - mDice: 0.9143 - val_loss: 0.0465 - val_acc: 0.9961 - val_mDice: 0.9132

Epoch 00034: val_mDice did not improve from 0.91503
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0461 - acc: 0.9963 - mDice: 0.9139 - val_loss: 0.0449 - val_acc: 0.9962 - val_mDice: 0.9160

Epoch 00035: val_mDice improved from 0.91503 to 0.91599, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.0450 - acc: 0.9964 - mDice: 0.9159 - val_loss: 0.0455 - val_acc: 0.9961 - val_mDice: 0.9149

Epoch 00036: val_mDice did not improve from 0.91599
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.0449 - acc: 0.9964 - mDice: 0.9161 - val_loss: 0.0458 - val_acc: 0.9961 - val_mDice: 0.9144

Epoch 00037: val_mDice did not improve from 0.91599
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.0443 - acc: 0.9964 - mDice: 0.9170 - val_loss: 0.0454 - val_acc: 0.9961 - val_mDice: 0.9152

Epoch 00038: val_mDice did not improve from 0.91599
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.0437 - acc: 0.9965 - mDice: 0.9183 - val_loss: 0.0457 - val_acc: 0.9961 - val_mDice: 0.9146

Epoch 00039: val_mDice did not improve from 0.91599
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.0434 - acc: 0.9965 - mDice: 0.9187 - val_loss: 0.0455 - val_acc: 0.9961 - val_mDice: 0.9150

Epoch 00040: val_mDice did not improve from 0.91599
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.0435 - acc: 0.9965 - mDice: 0.9185 - val_loss: 0.0472 - val_acc: 0.9959 - val_mDice: 0.9119

Epoch 00041: val_mDice did not improve from 0.91599
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0433 - acc: 0.9965 - mDice: 0.9189 - val_loss: 0.0457 - val_acc: 0.9961 - val_mDice: 0.9146

Epoch 00042: val_mDice did not improve from 0.91599
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0430 - acc: 0.9966 - mDice: 0.9195 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9158

Epoch 00043: val_mDice did not improve from 0.91599
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0432 - acc: 0.9965 - mDice: 0.9190 - val_loss: 0.0452 - val_acc: 0.9962 - val_mDice: 0.9155

Epoch 00044: val_mDice did not improve from 0.91599
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0433 - acc: 0.9965 - mDice: 0.9190 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9159

Epoch 00045: val_mDice did not improve from 0.91599
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0425 - acc: 0.9966 - mDice: 0.9204 - val_loss: 0.0455 - val_acc: 0.9961 - val_mDice: 0.9150

Epoch 00046: val_mDice did not improve from 0.91599
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0427 - acc: 0.9966 - mDice: 0.9200 - val_loss: 0.0441 - val_acc: 0.9962 - val_mDice: 0.9175

Epoch 00047: val_mDice improved from 0.91599 to 0.91745, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0425 - acc: 0.9966 - mDice: 0.9203 - val_loss: 0.0461 - val_acc: 0.9961 - val_mDice: 0.9139

Epoch 00048: val_mDice did not improve from 0.91745
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0422 - acc: 0.9966 - mDice: 0.9208 - val_loss: 0.0449 - val_acc: 0.9962 - val_mDice: 0.9161

Epoch 00049: val_mDice did not improve from 0.91745
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0426 - acc: 0.9966 - mDice: 0.9201 - val_loss: 0.0460 - val_acc: 0.9961 - val_mDice: 0.9140

Epoch 00050: val_mDice did not improve from 0.91745
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0417 - acc: 0.9966 - mDice: 0.9218 - val_loss: 0.0469 - val_acc: 0.9960 - val_mDice: 0.9124

Epoch 00051: val_mDice did not improve from 0.91745
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0414 - acc: 0.9967 - mDice: 0.9222 - val_loss: 0.0449 - val_acc: 0.9962 - val_mDice: 0.9160

Epoch 00052: val_mDice did not improve from 0.91745
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0412 - acc: 0.9967 - mDice: 0.9227 - val_loss: 0.0451 - val_acc: 0.9961 - val_mDice: 0.9157

Epoch 00053: val_mDice did not improve from 0.91745
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.0424 - acc: 0.9966 - mDice: 0.9205 - val_loss: 0.0467 - val_acc: 0.9961 - val_mDice: 0.9127

Epoch 00054: val_mDice did not improve from 0.91745
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0407 - acc: 0.9967 - mDice: 0.9235 - val_loss: 0.0460 - val_acc: 0.9961 - val_mDice: 0.9141

Epoch 00055: val_mDice did not improve from 0.91745
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0405 - acc: 0.9967 - mDice: 0.9240 - val_loss: 0.0449 - val_acc: 0.9962 - val_mDice: 0.9160

Epoch 00056: val_mDice did not improve from 0.91745
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0404 - acc: 0.9968 - mDice: 0.9240 - val_loss: 0.0448 - val_acc: 0.9962 - val_mDice: 0.9162

Epoch 00057: val_mDice did not improve from 0.91745
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0405 - acc: 0.9967 - mDice: 0.9239 - val_loss: 0.0447 - val_acc: 0.9962 - val_mDice: 0.9163

Epoch 00058: val_mDice did not improve from 0.91745
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0399 - acc: 0.9968 - mDice: 0.9249 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9159

Epoch 00059: val_mDice did not improve from 0.91745
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0415 - acc: 0.9967 - mDice: 0.9222 - val_loss: 0.0449 - val_acc: 0.9962 - val_mDice: 0.9161

Epoch 00060: val_mDice did not improve from 0.91745
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0402 - acc: 0.9968 - mDice: 0.9244 - val_loss: 0.0454 - val_acc: 0.9961 - val_mDice: 0.9151

Epoch 00061: val_mDice did not improve from 0.91745
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0400 - acc: 0.9968 - mDice: 0.9248 - val_loss: 0.0449 - val_acc: 0.9962 - val_mDice: 0.9160

Epoch 00062: val_mDice did not improve from 0.91745
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0400 - acc: 0.9968 - mDice: 0.9248 - val_loss: 0.0444 - val_acc: 0.9962 - val_mDice: 0.9168

Epoch 00063: val_mDice did not improve from 0.91745
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0398 - acc: 0.9968 - mDice: 0.9253 - val_loss: 0.0446 - val_acc: 0.9962 - val_mDice: 0.9166

Epoch 00064: val_mDice did not improve from 0.91745
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0398 - acc: 0.9968 - mDice: 0.9253 - val_loss: 0.0452 - val_acc: 0.9961 - val_mDice: 0.9155

Epoch 00065: val_mDice did not improve from 0.91745
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0397 - acc: 0.9968 - mDice: 0.9253 - val_loss: 0.0445 - val_acc: 0.9962 - val_mDice: 0.9168

Epoch 00066: val_mDice did not improve from 0.91745
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0393 - acc: 0.9968 - mDice: 0.9260 - val_loss: 0.0448 - val_acc: 0.9962 - val_mDice: 0.9162

Epoch 00067: val_mDice did not improve from 0.91745
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0398 - acc: 0.9968 - mDice: 0.9251 - val_loss: 0.0458 - val_acc: 0.9961 - val_mDice: 0.9143

Epoch 00068: val_mDice did not improve from 0.91745
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0395 - acc: 0.9968 - mDice: 0.9258 - val_loss: 0.0449 - val_acc: 0.9962 - val_mDice: 0.9159

Epoch 00069: val_mDice did not improve from 0.91745
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0392 - acc: 0.9968 - mDice: 0.9262 - val_loss: 0.0451 - val_acc: 0.9962 - val_mDice: 0.9156

Epoch 00070: val_mDice did not improve from 0.91745
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0395 - acc: 0.9968 - mDice: 0.9257 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9159

Epoch 00071: val_mDice did not improve from 0.91745
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.0390 - acc: 0.9969 - mDice: 0.9266 - val_loss: 0.0443 - val_acc: 0.9962 - val_mDice: 0.9170

Epoch 00072: val_mDice did not improve from 0.91745
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0385 - acc: 0.9969 - mDice: 0.9275 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9154

Epoch 00073: val_mDice did not improve from 0.91745
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0386 - acc: 0.9969 - mDice: 0.9273 - val_loss: 0.0453 - val_acc: 0.9961 - val_mDice: 0.9152

Epoch 00074: val_mDice did not improve from 0.91745
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0383 - acc: 0.9969 - mDice: 0.9279 - val_loss: 0.0455 - val_acc: 0.9961 - val_mDice: 0.9150

Epoch 00075: val_mDice did not improve from 0.91745
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0383 - acc: 0.9969 - mDice: 0.9279 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9158

Epoch 00076: val_mDice did not improve from 0.91745
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0385 - acc: 0.9969 - mDice: 0.9276 - val_loss: 0.0451 - val_acc: 0.9962 - val_mDice: 0.9156

Epoch 00077: val_mDice did not improve from 0.91745
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0387 - acc: 0.9969 - mDice: 0.9272 - val_loss: 0.0454 - val_acc: 0.9961 - val_mDice: 0.9151

Epoch 00078: val_mDice did not improve from 0.91745
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0385 - acc: 0.9969 - mDice: 0.9276 - val_loss: 0.0454 - val_acc: 0.9961 - val_mDice: 0.9151

Epoch 00079: val_mDice did not improve from 0.91745
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0384 - acc: 0.9969 - mDice: 0.9277 - val_loss: 0.0448 - val_acc: 0.9962 - val_mDice: 0.9162

Epoch 00080: val_mDice did not improve from 0.91745
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0389 - acc: 0.9969 - mDice: 0.9268 - val_loss: 0.0458 - val_acc: 0.9961 - val_mDice: 0.9145

Epoch 00081: val_mDice did not improve from 0.91745
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0383 - acc: 0.9969 - mDice: 0.9278 - val_loss: 0.0456 - val_acc: 0.9961 - val_mDice: 0.9148

Epoch 00082: val_mDice did not improve from 0.91745
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0386 - acc: 0.9969 - mDice: 0.9274 - val_loss: 0.0451 - val_acc: 0.9962 - val_mDice: 0.9156

Epoch 00083: val_mDice did not improve from 0.91745
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0382 - acc: 0.9969 - mDice: 0.9281 - val_loss: 0.0451 - val_acc: 0.9962 - val_mDice: 0.9157

Epoch 00084: val_mDice did not improve from 0.91745
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0384 - acc: 0.9969 - mDice: 0.9277 - val_loss: 0.0455 - val_acc: 0.9961 - val_mDice: 0.9149

Epoch 00085: val_mDice did not improve from 0.91745
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0384 - acc: 0.9969 - mDice: 0.9277 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9159

Epoch 00086: val_mDice did not improve from 0.91745
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.0379 - acc: 0.9969 - mDice: 0.9287 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9159

Epoch 00087: val_mDice did not improve from 0.91745
Restoring model weights from the end of the best epoch
Epoch 00087: early stopping
{'val_loss': [0.5787958970725707, 0.38496039991392444, 0.11503476463791976, 0.09055097872941063, 0.0750435134102417, 0.08117838987392136, 0.06490759438378763, 0.058381709721163556, 0.053741141150366614, 0.05480929833428566, 0.05283287729011224, 0.05456658706794837, 0.0571253854130264, 0.050528508715096725, 0.061853246779359855, 0.05166541771125315, 0.04955800962140704, 0.05335721096029254, 0.05316381361388887, 0.049480474449928986, 0.058940789483508955, 0.06665250802023022, 0.04871047628685875, 0.0607401414382082, 0.05040570702884122, 0.04848509900976389, 0.04779933350304819, 0.04713654990993803, 0.04830918028779904, 0.046613461970429024, 0.04736912063720575, 0.04546278717268504, 0.0499316979957515, 0.04651080371870694, 0.044927747978863536, 0.04552736088505447, 0.04583148423443551, 0.04537477451016363, 0.045677740948760405, 0.04547809837199214, 0.04722889490361542, 0.0456817956443844, 0.045019267719335065, 0.04520675417217279, 0.0449638193884176, 0.04550023629919847, 0.04411109265761936, 0.04610950245727441, 0.04487787722943506, 0.04604961496796512, 0.04693629183835833, 0.044925988841278845, 0.04509964446539865, 0.04674447562684302, 0.04600281060211638, 0.04493634635449139, 0.04481985711393862, 0.04474227310806769, 0.04498137799451891, 0.04487176554378944, 0.04541243111079607, 0.04491790323211334, 0.04444688631162944, 0.044582954706640164, 0.04520831297666774, 0.044489456185894914, 0.044827925932612325, 0.04584065768173909, 0.044949495230926825, 0.045139788129609774, 0.04497249757492098, 0.04434576094748297, 0.045267798508221234, 0.04533476124178714, 0.04547316115616386, 0.04501257107771912, 0.045143015106291345, 0.045396647109172406, 0.04543035450714365, 0.04479387408938995, 0.04576759469457889, 0.045566623085805544, 0.04510637747396029, 0.045050057606574115, 0.04553608888797569, 0.04496352658486981, 0.044960022111420646], 'val_acc': [0.9388433286658673, 0.9692929433205066, 0.9923744071862076, 0.9936544109552159, 0.9943095688150401, 0.9940165327408252, 0.9949106925879645, 0.9951938933834306, 0.9955030450848249, 0.9954527780456325, 0.9955918015245039, 0.9954652263646823, 0.9953494276904786, 0.9957624936855284, 0.9949911692764151, 0.9956705529232079, 0.9958254039116781, 0.9955943336117916, 0.995590944174026, 0.9958440319856463, 0.9951056047292015, 0.9943719164689837, 0.9959143825451761, 0.9949035665025684, 0.9958087224673405, 0.9958696006703855, 0.9959264419823458, 0.9959745840903339, 0.9958436557420004, 0.9960247628326744, 0.9960074926857279, 0.9961237712712547, 0.9958204961779465, 0.9960640282890516, 0.9961533835077696, 0.9961172928099646, 0.9960774360891741, 0.9960992464363404, 0.996099251047588, 0.9961419054293701, 0.9959239155310273, 0.9961106319482143, 0.9961637919475151, 0.9961556025425821, 0.9961707638122974, 0.9961471040132736, 0.9962331661180643, 0.9960725235734081, 0.9961662176345344, 0.9961011910165278, 0.9960440622701344, 0.9961923587971225, 0.9961438194386939, 0.9960626770227208, 0.9960974231148859, 0.9961638056104709, 0.9961663069561081, 0.9961529872820507, 0.9961540529926048, 0.9961677540339167, 0.996140544940544, 0.996171511005194, 0.9961921634168542, 0.996188403542199, 0.9961499986812515, 0.9961897548085298, 0.9961870616691502, 0.9961051416602039, 0.9961581858659542, 0.9961596488269489, 0.9961654535337301, 0.9962063441987024, 0.9961527030925696, 0.9961429472297515, 0.9961381356490717, 0.9961522267677723, 0.9961598424993476, 0.9961457553087469, 0.9961407374174343, 0.996178656901876, 0.9961109221152384, 0.9961351579786372, 0.9961610033382318, 0.996161090781149, 0.9961345745704235, 0.9961595620671794, 0.996173930885457], 'val_mDice': [0.3253297815179415, 0.4733481108288369, 0.8046287209393301, 0.8441518563937321, 0.8695698959438029, 0.8579021652995003, 0.8817072433865173, 0.8923336648326207, 0.9003960505597571, 0.8984884536027226, 0.901978614849484, 0.89887331840302, 0.8944153164041077, 0.9060566153772239, 0.8867870053452543, 0.9040093162339875, 0.9077578847934319, 0.9010762999255882, 0.9015585993627423, 0.9079045545747424, 0.8924682077159171, 0.8812575545215333, 0.9092550851554105, 0.8894372660997604, 0.906222144819604, 0.9096704801720671, 0.9108811065255742, 0.9120623754566924, 0.9099798680718102, 0.9129920009214079, 0.9116498218224861, 0.915030274828389, 0.9071026842368708, 0.913168800423685, 0.9159929345876918, 0.9149086226023371, 0.9143674937223637, 0.9151889681816101, 0.914643236764181, 0.9150042940347447, 0.9118894558922951, 0.9146321149473545, 0.9158270965332971, 0.9154883725595337, 0.9159189353358097, 0.914963359135953, 0.9174540531327868, 0.9138713520030921, 0.9160787189929055, 0.9139698463729596, 0.9123908346225335, 0.9159813605270276, 0.9156765811422835, 0.9127222605284442, 0.9140577914038497, 0.9159732100615187, 0.9161755862755215, 0.9163184599753437, 0.9158890833144202, 0.9160761136380171, 0.9151059906598832, 0.9159927033421645, 0.916845107488441, 0.9165936703326709, 0.9154720116822972, 0.9167560651514114, 0.9161529711802573, 0.9143481956511992, 0.9159369456733879, 0.9155945280903048, 0.9158872367659407, 0.9170215688325614, 0.9153614307201353, 0.9152437265759553, 0.9149906519149299, 0.9158158029048012, 0.9155873572587284, 0.9151269137688558, 0.9150733858944693, 0.9162093218213166, 0.9144610048706688, 0.9148198820458442, 0.9156480095106414, 0.9157456675368257, 0.9148762038239093, 0.9159095873122229, 0.9159113973122954], 'loss': [0.7860706654623095, 0.13010539272350005, 0.0964288641750591, 0.0861282691525704, 0.07870691020360224, 0.07455475686540392, 0.07034339571586329, 0.06754842924030297, 0.06373804803508365, 0.061528822607932035, 0.06393178671487645, 0.059654070043652474, 0.05842079928484105, 0.056782879503132246, 0.05729950911825917, 0.058856787554725835, 0.056578739508155555, 0.05416810610868231, 0.054537707337327164, 0.05094585096858248, 0.050447978217805625, 0.0494637430323544, 0.05140221480377102, 0.048939783860094926, 0.04991445139216668, 0.048843334737098794, 0.04875329199167432, 0.04732961060832424, 0.04742425417035929, 0.04781070074627391, 0.048562518172104566, 0.046487030625121745, 0.04575663734379754, 0.045870980434812136, 0.04609036683814676, 0.04500093097801988, 0.044867638609333994, 0.04434840846460548, 0.04366671546019586, 0.04344184004241206, 0.04353804779706391, 0.04328655059619021, 0.04297128548243232, 0.0432323202019952, 0.04325501893588158, 0.04245028273656023, 0.042706146033058376, 0.04254133378927592, 0.042238449934025236, 0.04262429876010657, 0.04168788115165047, 0.04143907175465144, 0.04118596882902113, 0.04242080114920343, 0.04074332239902596, 0.04047156816355358, 0.040446946390499415, 0.04050557273803591, 0.039948241376633095, 0.041476178706578604, 0.04022767185666304, 0.04003405381257206, 0.040011619449658906, 0.039754163224568595, 0.03976222298014563, 0.039735936459333925, 0.03934913764268049, 0.039833068335255725, 0.0394761986254981, 0.039225916035445646, 0.03953064220600855, 0.03901875984325285, 0.03853122525775743, 0.03861184814273203, 0.03831521709723101, 0.038316204589982014, 0.03846853953478062, 0.03866564029599211, 0.038454679612998185, 0.038391995870380155, 0.03890276651787935, 0.03834349727154221, 0.03858817201176984, 0.03821368503027689, 0.03843289350599161, 0.03841936055113835, 0.03786306752092776], 'acc': [0.85897473309563, 0.9892570892674329, 0.9921574798658435, 0.9930799162520795, 0.9938614656490907, 0.9941837603717931, 0.9946539034187573, 0.9947985922536885, 0.9950628553181333, 0.9951988350059906, 0.9950174897576797, 0.9953110222479667, 0.995403580949209, 0.9955259674543785, 0.9954843496744518, 0.995422120874256, 0.9955679282379859, 0.995707333752657, 0.9957003750765634, 0.9959582763976768, 0.9960181914740779, 0.9960631488867409, 0.9959382716164713, 0.9960927752757162, 0.9960502067019948, 0.9960902694432708, 0.9961151323353934, 0.9962158788092518, 0.9962214674648299, 0.996215109931492, 0.9961377639309624, 0.9962597897061628, 0.9963286931186803, 0.9963387319589636, 0.9963175319827622, 0.9963934153429195, 0.9964188049273863, 0.9964479910396732, 0.9964927370663469, 0.9965211560735029, 0.9965238825982388, 0.9965208751118316, 0.9965530026801014, 0.9965400540695758, 0.9965275818530511, 0.9965933353041184, 0.9965764833648851, 0.9965827968040807, 0.9966083328520056, 0.9965929304800069, 0.9966478181594366, 0.9966523773608155, 0.996683994411979, 0.9966200984543584, 0.996725653627105, 0.9967424266843548, 0.9967528464182602, 0.9967341438545171, 0.9967767793449771, 0.9966628646761955, 0.9967710714801093, 0.9967755342947949, 0.9967687174290086, 0.9968042172020696, 0.9968026470960738, 0.9968006167713152, 0.9968132837554337, 0.99679513247926, 0.9968136850342875, 0.9968329603787248, 0.996818714647045, 0.9968559715384444, 0.996884667962014, 0.9968801065448492, 0.9968971844942597, 0.9969061994198086, 0.9968976110330745, 0.996886368134651, 0.9968876723463207, 0.9969060210490316, 0.9968743525916316, 0.9969051165651655, 0.9968864959855062, 0.9969145168159088, 0.9968997326482184, 0.996904219614972, 0.9969464542697354], 'mDice': [0.3374628249320399, 0.7813600144421744, 0.8340858992147623, 0.8509772466461012, 0.8632121422920085, 0.8694125179021331, 0.8751143445312756, 0.8780994829635195, 0.8836971962318988, 0.8872833362742427, 0.8829940094380574, 0.8903517960172603, 0.892427268746174, 0.8952363663003348, 0.8943046754620776, 0.891586881366361, 0.8955366846354035, 0.8997534294110692, 0.8990563369151828, 0.9053965173246249, 0.9062773069041369, 0.9080123201178796, 0.9045712732914212, 0.9089249306008719, 0.9071862369664982, 0.9090668454932458, 0.9092328665868057, 0.9117584784677923, 0.9115882752996395, 0.9108870465959314, 0.9095235021584096, 0.9132313409259328, 0.9145396885818708, 0.9143386613923821, 0.9139146889009441, 0.9158962271470563, 0.9161147036516977, 0.9170402767046677, 0.9182608843292889, 0.9186665504395297, 0.918495811272731, 0.9189449070111526, 0.9194987546998772, 0.9190450762727447, 0.9190059279863719, 0.9204374893004124, 0.9199646429944659, 0.9202715012220645, 0.9208175407023235, 0.9201109234285177, 0.9218146907352603, 0.9222485872892642, 0.9227047254604921, 0.9204937341487984, 0.9235050826710839, 0.9240091660208861, 0.9240309491919763, 0.9239260926565717, 0.924925875708073, 0.9221606453998381, 0.9244291738063429, 0.9247721096840047, 0.9248098779788247, 0.9252718609504983, 0.9252721685016022, 0.9253082556795453, 0.9260060532385532, 0.925125631257947, 0.9257818795048172, 0.9262393156835138, 0.9256834055411328, 0.9265970308541365, 0.9274967363775884, 0.927349790764564, 0.927890038401664, 0.9278715040160821, 0.9275949322601234, 0.9272370949996892, 0.9276176999049559, 0.9277397604679972, 0.9268015840239684, 0.9278322577919659, 0.9273768645680084, 0.9280620565201714, 0.9276647377191423, 0.9276849693968393, 0.9286944354333843], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:01<00:11,  1.12s/it]predicting test subjects:  18%|█▊        | 2/11 [00:01<00:07,  1.17it/s]predicting test subjects:  27%|██▋       | 3/11 [00:01<00:05,  1.36it/s]predicting test subjects:  36%|███▋      | 4/11 [00:02<00:04,  1.66it/s]predicting test subjects:  45%|████▌     | 5/11 [00:02<00:03,  1.90it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:02<00:02,  2.16it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:03<00:01,  2.45it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:03<00:01,  2.64it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:03<00:00,  2.51it/s]predicting test subjects:  91%|█████████ | 10/11 [00:04<00:00,  2.63it/s]predicting test subjects: 100%|██████████| 11/11 [00:04<00:00,  2.53it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:15,  2.65it/s]predicting train subjects:   5%|▍         | 2/41 [00:00<00:14,  2.78it/s]predicting train subjects:   7%|▋         | 3/41 [00:00<00:12,  3.06it/s]predicting train subjects:  10%|▉         | 4/41 [00:01<00:10,  3.39it/s]predicting train subjects:  12%|█▏        | 5/41 [00:01<00:11,  3.05it/s]predicting train subjects:  15%|█▍        | 6/41 [00:02<00:12,  2.74it/s]predicting train subjects:  17%|█▋        | 7/41 [00:02<00:11,  2.87it/s]predicting train subjects:  20%|█▉        | 8/41 [00:02<00:12,  2.62it/s]predicting train subjects:  22%|██▏       | 9/41 [00:03<00:11,  2.76it/s]predicting train subjects:  24%|██▍       | 10/41 [00:03<00:10,  2.82it/s]predicting train subjects:  27%|██▋       | 11/41 [00:03<00:10,  2.96it/s]predicting train subjects:  29%|██▉       | 12/41 [00:03<00:08,  3.44it/s]predicting train subjects:  32%|███▏      | 13/41 [00:04<00:07,  3.56it/s]predicting train subjects:  34%|███▍      | 14/41 [00:04<00:08,  3.06it/s]predicting train subjects:  37%|███▋      | 15/41 [00:04<00:07,  3.45it/s]predicting train subjects:  39%|███▉      | 16/41 [00:05<00:07,  3.52it/s]predicting train subjects:  41%|████▏     | 17/41 [00:05<00:07,  3.23it/s]predicting train subjects:  44%|████▍     | 18/41 [00:05<00:07,  3.01it/s]predicting train subjects:  46%|████▋     | 19/41 [00:06<00:07,  3.06it/s]predicting train subjects:  49%|████▉     | 20/41 [00:06<00:07,  2.97it/s]predicting train subjects:  51%|█████     | 21/41 [00:06<00:06,  3.21it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:07<00:05,  3.33it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:07<00:05,  3.60it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:07<00:04,  3.58it/s]predicting train subjects:  61%|██████    | 25/41 [00:07<00:04,  3.35it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:08<00:04,  3.33it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:08<00:04,  3.06it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:09<00:04,  2.76it/s]predicting train subjects:  71%|███████   | 29/41 [00:09<00:04,  2.99it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:09<00:03,  2.96it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:10<00:03,  2.85it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:10<00:03,  2.93it/s]predicting train subjects:  80%|████████  | 33/41 [00:10<00:02,  3.01it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:11<00:02,  2.57it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:11<00:02,  2.34it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:12<00:02,  2.13it/s]predicting train subjects:  90%|█████████ | 37/41 [00:12<00:01,  2.28it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:13<00:01,  2.40it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:13<00:00,  2.57it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:13<00:00,  2.61it/s]predicting train subjects: 100%|██████████| 41/41 [00:14<00:00,  2.73it/s]
predicting test subjects sagittal:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects sagittal:   9%|▉         | 1/11 [00:00<00:02,  3.55it/s]predicting test subjects sagittal:  18%|█▊        | 2/11 [00:00<00:02,  3.52it/s]predicting test subjects sagittal:  27%|██▋       | 3/11 [00:00<00:02,  3.61it/s]predicting test subjects sagittal:  36%|███▋      | 4/11 [00:01<00:01,  3.68it/s]predicting test subjects sagittal:  45%|████▌     | 5/11 [00:01<00:01,  3.75it/s]predicting test subjects sagittal:  55%|█████▍    | 6/11 [00:01<00:01,  3.51it/s]predicting test subjects sagittal:  64%|██████▎   | 7/11 [00:01<00:01,  3.50it/s]predicting test subjects sagittal:  73%|███████▎  | 8/11 [00:02<00:00,  3.47it/s]predicting test subjects sagittal:  82%|████████▏ | 9/11 [00:02<00:00,  3.61it/s]predicting test subjects sagittal:  91%|█████████ | 10/11 [00:02<00:00,  3.62it/s]predicting test subjects sagittal: 100%|██████████| 11/11 [00:03<00:00,  3.27it/s]
predicting train subjects sagittal:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects sagittal:   2%|▏         | 1/41 [00:00<00:13,  3.04it/s]predicting train subjects sagittal:   5%|▍         | 2/41 [00:00<00:12,  3.19it/s]predicting train subjects sagittal:   7%|▋         | 3/41 [00:00<00:11,  3.42it/s]predicting train subjects sagittal:  10%|▉         | 4/41 [00:01<00:10,  3.66it/s]predicting train subjects sagittal:  12%|█▏        | 5/41 [00:01<00:10,  3.45it/s]predicting train subjects sagittal:  15%|█▍        | 6/41 [00:01<00:11,  3.12it/s]predicting train subjects sagittal:  17%|█▋        | 7/41 [00:02<00:10,  3.35it/s]predicting train subjects sagittal:  20%|█▉        | 8/41 [00:02<00:08,  3.75it/s]predicting train subjects sagittal:  22%|██▏       | 9/41 [00:02<00:08,  3.60it/s]predicting train subjects sagittal:  24%|██▍       | 10/41 [00:02<00:08,  3.52it/s]predicting train subjects sagittal:  27%|██▋       | 11/41 [00:03<00:08,  3.45it/s]predicting train subjects sagittal:  29%|██▉       | 12/41 [00:03<00:07,  3.69it/s]predicting train subjects sagittal:  32%|███▏      | 13/41 [00:03<00:07,  3.73it/s]predicting train subjects sagittal:  34%|███▍      | 14/41 [00:03<00:07,  3.76it/s]predicting train subjects sagittal:  37%|███▋      | 15/41 [00:04<00:06,  3.94it/s]predicting train subjects sagittal:  39%|███▉      | 16/41 [00:04<00:06,  3.76it/s]predicting train subjects sagittal:  41%|████▏     | 17/41 [00:04<00:07,  3.19it/s]predicting train subjects sagittal:  44%|████▍     | 18/41 [00:05<00:07,  3.06it/s]predicting train subjects sagittal:  46%|████▋     | 19/41 [00:05<00:06,  3.20it/s]predicting train subjects sagittal:  49%|████▉     | 20/41 [00:05<00:05,  3.51it/s]predicting train subjects sagittal:  51%|█████     | 21/41 [00:05<00:05,  3.67it/s]predicting train subjects sagittal:  54%|█████▎    | 22/41 [00:06<00:05,  3.69it/s]predicting train subjects sagittal:  56%|█████▌    | 23/41 [00:06<00:04,  3.68it/s]predicting train subjects sagittal:  59%|█████▊    | 24/41 [00:06<00:04,  3.65it/s]predicting train subjects sagittal:  61%|██████    | 25/41 [00:07<00:04,  3.68it/s]predicting train subjects sagittal:  63%|██████▎   | 26/41 [00:07<00:04,  3.55it/s]predicting train subjects sagittal:  66%|██████▌   | 27/41 [00:07<00:04,  3.17it/s]predicting train subjects sagittal:  68%|██████▊   | 28/41 [00:08<00:04,  3.15it/s]predicting train subjects sagittal:  71%|███████   | 29/41 [00:08<00:03,  3.39it/s]predicting train subjects sagittal:  73%|███████▎  | 30/41 [00:08<00:03,  3.32it/s]predicting train subjects sagittal:  76%|███████▌  | 31/41 [00:08<00:02,  3.35it/s]predicting train subjects sagittal:  78%|███████▊  | 32/41 [00:09<00:02,  3.47it/s]predicting train subjects sagittal:  80%|████████  | 33/41 [00:09<00:02,  3.33it/s]predicting train subjects sagittal:  83%|████████▎ | 34/41 [00:09<00:02,  3.03it/s]predicting train subjects sagittal:  85%|████████▌ | 35/41 [00:10<00:02,  2.61it/s]predicting train subjects sagittal:  88%|████████▊ | 36/41 [00:10<00:02,  2.47it/s]predicting train subjects sagittal:  90%|█████████ | 37/41 [00:11<00:01,  2.53it/s]predicting train subjects sagittal:  93%|█████████▎| 38/41 [00:11<00:01,  2.73it/s]predicting train subjects sagittal:  95%|█████████▌| 39/41 [00:11<00:00,  2.90it/s]predicting train subjects sagittal:  98%|█████████▊| 40/41 [00:12<00:00,  2.91it/s]predicting train subjects sagittal: 100%|██████████| 41/41 [00:12<00:00,  2.89it/s]
saving BB  test1-THALAMUS:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  73%|███████▎  | 8/11 [00:00<00:00, 79.41it/s]saving BB  test1-THALAMUS: 100%|██████████| 11/11 [00:00<00:00, 76.82it/s]
saving BB  train1-THALAMUS:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS:  20%|█▉        | 8/41 [00:00<00:00, 79.20it/s]saving BB  train1-THALAMUS:  41%|████▏     | 17/41 [00:00<00:00, 78.37it/s]saving BB  train1-THALAMUS:  63%|██████▎   | 26/41 [00:00<00:00, 79.26it/s]saving BB  train1-THALAMUS:  83%|████████▎ | 34/41 [00:00<00:00, 77.18it/s]saving BB  train1-THALAMUS: 100%|██████████| 41/41 [00:00<00:00, 71.74it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal:  82%|████████▏ | 9/11 [00:00<00:00, 81.90it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 11/11 [00:00<00:00, 79.03it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:  22%|██▏       | 9/41 [00:00<00:00, 81.74it/s]saving BB  train1-THALAMUS Sagittal:  44%|████▍     | 18/41 [00:00<00:00, 81.34it/s]saving BB  train1-THALAMUS Sagittal:  66%|██████▌   | 27/41 [00:00<00:00, 81.79it/s]saving BB  train1-THALAMUS Sagittal:  85%|████████▌ | 35/41 [00:00<00:00, 77.08it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 41/41 [00:00<00:00, 74.89it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:24,  1.65it/s]Loading train:   5%|▍         | 2/41 [00:01<00:23,  1.67it/s]Loading train:   7%|▋         | 3/41 [00:01<00:22,  1.69it/s]Loading train:  10%|▉         | 4/41 [00:02<00:20,  1.77it/s]Loading train:  12%|█▏        | 5/41 [00:02<00:20,  1.78it/s]Loading train:  15%|█▍        | 6/41 [00:03<00:19,  1.78it/s]Loading train:  17%|█▋        | 7/41 [00:03<00:18,  1.88it/s]Loading train:  20%|█▉        | 8/41 [00:04<00:14,  2.22it/s]Loading train:  22%|██▏       | 9/41 [00:04<00:14,  2.15it/s]Loading train:  24%|██▍       | 10/41 [00:05<00:14,  2.08it/s]Loading train:  27%|██▋       | 11/41 [00:05<00:14,  2.13it/s]Loading train:  29%|██▉       | 12/41 [00:05<00:11,  2.48it/s]Loading train:  32%|███▏      | 13/41 [00:06<00:11,  2.33it/s]Loading train:  34%|███▍      | 14/41 [00:06<00:13,  2.00it/s]Loading train:  37%|███▋      | 15/41 [00:07<00:11,  2.23it/s]Loading train:  39%|███▉      | 16/41 [00:07<00:11,  2.16it/s]Loading train:  41%|████▏     | 17/41 [00:08<00:11,  2.06it/s]Loading train:  44%|████▍     | 18/41 [00:08<00:11,  2.05it/s]Loading train:  46%|████▋     | 19/41 [00:09<00:11,  1.93it/s]Loading train:  49%|████▉     | 20/41 [00:09<00:10,  1.97it/s]Loading train:  51%|█████     | 21/41 [00:10<00:10,  1.88it/s]Loading train:  54%|█████▎    | 22/41 [00:11<00:10,  1.89it/s]Loading train:  56%|█████▌    | 23/41 [00:11<00:08,  2.16it/s]Loading train:  59%|█████▊    | 24/41 [00:11<00:07,  2.18it/s]Loading train:  61%|██████    | 25/41 [00:12<00:07,  2.11it/s]Loading train:  63%|██████▎   | 26/41 [00:12<00:06,  2.16it/s]Loading train:  66%|██████▌   | 27/41 [00:13<00:06,  2.18it/s]Loading train:  68%|██████▊   | 28/41 [00:13<00:05,  2.24it/s]Loading train:  71%|███████   | 29/41 [00:13<00:05,  2.33it/s]Loading train:  73%|███████▎  | 30/41 [00:14<00:04,  2.21it/s]Loading train:  76%|███████▌  | 31/41 [00:14<00:04,  2.27it/s]Loading train:  78%|███████▊  | 32/41 [00:15<00:03,  2.31it/s]Loading train:  80%|████████  | 33/41 [00:15<00:03,  2.13it/s]Loading train:  83%|████████▎ | 34/41 [00:16<00:03,  1.91it/s]Loading train:  85%|████████▌ | 35/41 [00:17<00:03,  1.82it/s]Loading train:  88%|████████▊ | 36/41 [00:17<00:02,  1.88it/s]Loading train:  90%|█████████ | 37/41 [00:18<00:02,  1.92it/s]Loading train:  93%|█████████▎| 38/41 [00:18<00:01,  1.91it/s]Loading train:  95%|█████████▌| 39/41 [00:19<00:01,  1.91it/s]Loading train:  98%|█████████▊| 40/41 [00:19<00:00,  1.87it/s]Loading train: 100%|██████████| 41/41 [00:20<00:00,  1.91it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:  10%|▉         | 4/41 [00:00<00:01, 34.31it/s]concatenating: train:  24%|██▍       | 10/41 [00:00<00:00, 39.32it/s]concatenating: train:  98%|█████████▊| 40/41 [00:00<00:00, 53.15it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 126.28it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:06,  1.64it/s]Loading test:  18%|█▊        | 2/11 [00:01<00:05,  1.65it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:04,  1.72it/s]Loading test:  36%|███▋      | 4/11 [00:02<00:03,  1.90it/s]Loading test:  45%|████▌     | 5/11 [00:02<00:03,  1.96it/s]Loading test:  55%|█████▍    | 6/11 [00:03<00:02,  2.00it/s]Loading test:  64%|██████▎   | 7/11 [00:03<00:02,  1.84it/s]Loading test:  73%|███████▎  | 8/11 [00:04<00:01,  1.94it/s]Loading test:  82%|████████▏ | 9/11 [00:04<00:00,  2.03it/s]Loading test:  91%|█████████ | 10/11 [00:05<00:00,  2.10it/s]Loading test: 100%|██████████| 11/11 [00:05<00:00,  2.10it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 230.43it/s]
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 92, 116, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 92, 116, 30)  300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 92, 116, 30)  120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 92, 116, 30)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 92, 116, 30)  8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 92, 116, 30)  120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 92, 116, 30)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 46, 58, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 46, 58, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 46, 58, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 46, 58, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 46, 58, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 46, 58, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 46, 58, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 46, 58, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 46, 58, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 23, 29, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 23, 29, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 23, 29, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 23, 29, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 23, 29, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 23, 29, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 23, 29, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 23, 29, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 23, 29, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 23, 29, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 46, 58, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 46, 58, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 46, 58, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 46, 58, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 58, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 58, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 46, 58, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 46, 58, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 46, 58, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 46, 58, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 92, 116, 30)  25230       dropout_4[0][0]                  2019-07-27 18:32:50.017739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-27 18:32:50.017836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-27 18:32:50.017853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-27 18:32:50.017863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-27 18:32:50.018135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 92, 116, 60)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 92, 116, 30)  16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 92, 116, 30)  120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 92, 116, 30)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 92, 116, 30)  8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 92, 116, 30)  120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 92, 116, 30)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 92, 116, 90)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 92, 116, 90)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 92, 116, 2)   182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 499,142
Non-trainable params: 1,200
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.98000917 0.01999083]
Train on 4060 samples, validate on 1096 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 16s - loss: 0.6286 - acc: 0.9174 - mDice: 0.4401 - val_loss: 0.2552 - val_acc: 0.9867 - val_mDice: 0.6112

Epoch 00001: val_mDice improved from -inf to 0.61120, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.1021 - acc: 0.9875 - mDice: 0.8245 - val_loss: 0.1145 - val_acc: 0.9901 - val_mDice: 0.8052

Epoch 00002: val_mDice improved from 0.61120 to 0.80517, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0794 - acc: 0.9914 - mDice: 0.8624 - val_loss: 0.4595 - val_acc: 0.9826 - val_mDice: 0.4133

Epoch 00003: val_mDice did not improve from 0.80517
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0744 - acc: 0.9928 - mDice: 0.8709 - val_loss: 0.1923 - val_acc: 0.9872 - val_mDice: 0.6889

Epoch 00004: val_mDice did not improve from 0.80517
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0672 - acc: 0.9937 - mDice: 0.8834 - val_loss: 0.1878 - val_acc: 0.9876 - val_mDice: 0.6948

Epoch 00005: val_mDice did not improve from 0.80517
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0692 - acc: 0.9939 - mDice: 0.8799 - val_loss: 0.0798 - val_acc: 0.9928 - val_mDice: 0.8614

Epoch 00006: val_mDice improved from 0.80517 to 0.86143, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0643 - acc: 0.9944 - mDice: 0.8884 - val_loss: 0.0951 - val_acc: 0.9921 - val_mDice: 0.8357

Epoch 00007: val_mDice did not improve from 0.86143
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0617 - acc: 0.9950 - mDice: 0.8931 - val_loss: 0.0615 - val_acc: 0.9945 - val_mDice: 0.8935

Epoch 00008: val_mDice improved from 0.86143 to 0.89345, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0591 - acc: 0.9953 - mDice: 0.8977 - val_loss: 0.0711 - val_acc: 0.9939 - val_mDice: 0.8765

Epoch 00009: val_mDice did not improve from 0.89345
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0596 - acc: 0.9953 - mDice: 0.8967 - val_loss: 0.0601 - val_acc: 0.9949 - val_mDice: 0.8961

Epoch 00010: val_mDice improved from 0.89345 to 0.89608, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0574 - acc: 0.9956 - mDice: 0.9006 - val_loss: 0.0537 - val_acc: 0.9957 - val_mDice: 0.9075

Epoch 00011: val_mDice improved from 0.89608 to 0.90748, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0559 - acc: 0.9958 - mDice: 0.9033 - val_loss: 0.0570 - val_acc: 0.9955 - val_mDice: 0.9014

Epoch 00012: val_mDice did not improve from 0.90748
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0536 - acc: 0.9961 - mDice: 0.9071 - val_loss: 0.0625 - val_acc: 0.9953 - val_mDice: 0.8913

Epoch 00013: val_mDice did not improve from 0.90748
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.0536 - acc: 0.9962 - mDice: 0.9052 - val_loss: 0.0495 - val_acc: 0.9964 - val_mDice: 0.9106

Epoch 00014: val_mDice improved from 0.90748 to 0.91063, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.0497 - acc: 0.9964 - mDice: 0.9087 - val_loss: 0.0593 - val_acc: 0.9959 - val_mDice: 0.8905

Epoch 00015: val_mDice did not improve from 0.91063
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.0484 - acc: 0.9964 - mDice: 0.9100 - val_loss: 0.0507 - val_acc: 0.9964 - val_mDice: 0.9055

Epoch 00016: val_mDice did not improve from 0.91063
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.0464 - acc: 0.9966 - mDice: 0.9134 - val_loss: 0.0507 - val_acc: 0.9964 - val_mDice: 0.9054

Epoch 00017: val_mDice did not improve from 0.91063
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.0479 - acc: 0.9965 - mDice: 0.9107 - val_loss: 0.0454 - val_acc: 0.9966 - val_mDice: 0.9149

Epoch 00018: val_mDice improved from 0.91063 to 0.91493, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.0460 - acc: 0.9966 - mDice: 0.9141 - val_loss: 0.0468 - val_acc: 0.9966 - val_mDice: 0.9124

Epoch 00019: val_mDice did not improve from 0.91493
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.0447 - acc: 0.9967 - mDice: 0.9164 - val_loss: 0.0482 - val_acc: 0.9965 - val_mDice: 0.9098

Epoch 00020: val_mDice did not improve from 0.91493
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.0439 - acc: 0.9967 - mDice: 0.9177 - val_loss: 0.0456 - val_acc: 0.9967 - val_mDice: 0.9146

Epoch 00021: val_mDice did not improve from 0.91493
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.0431 - acc: 0.9968 - mDice: 0.9191 - val_loss: 0.0486 - val_acc: 0.9965 - val_mDice: 0.9091

Epoch 00022: val_mDice did not improve from 0.91493
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.0428 - acc: 0.9968 - mDice: 0.9198 - val_loss: 0.0474 - val_acc: 0.9966 - val_mDice: 0.9114

Epoch 00023: val_mDice did not improve from 0.91493
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.0426 - acc: 0.9968 - mDice: 0.9200 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9136

Epoch 00024: val_mDice did not improve from 0.91493
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.0423 - acc: 0.9969 - mDice: 0.9205 - val_loss: 0.0504 - val_acc: 0.9964 - val_mDice: 0.9058

Epoch 00025: val_mDice did not improve from 0.91493
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.0421 - acc: 0.9969 - mDice: 0.9209 - val_loss: 0.0475 - val_acc: 0.9966 - val_mDice: 0.9111

Epoch 00026: val_mDice did not improve from 0.91493
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.0424 - acc: 0.9968 - mDice: 0.9203 - val_loss: 0.0459 - val_acc: 0.9967 - val_mDice: 0.9140

Epoch 00027: val_mDice did not improve from 0.91493
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.0418 - acc: 0.9969 - mDice: 0.9214 - val_loss: 0.0459 - val_acc: 0.9966 - val_mDice: 0.9140

Epoch 00028: val_mDice did not improve from 0.91493
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.0412 - acc: 0.9969 - mDice: 0.9225 - val_loss: 0.0464 - val_acc: 0.9966 - val_mDice: 0.9131

Epoch 00029: val_mDice did not improve from 0.91493
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.0414 - acc: 0.9969 - mDice: 0.9221 - val_loss: 0.0457 - val_acc: 0.9966 - val_mDice: 0.9144

Epoch 00030: val_mDice did not improve from 0.91493
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.0417 - acc: 0.9969 - mDice: 0.9217 - val_loss: 0.0444 - val_acc: 0.9967 - val_mDice: 0.9166

Epoch 00031: val_mDice improved from 0.91493 to 0.91664, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.0404 - acc: 0.9970 - mDice: 0.9240 - val_loss: 0.0443 - val_acc: 0.9967 - val_mDice: 0.9168

Epoch 00032: val_mDice improved from 0.91664 to 0.91683, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.0404 - acc: 0.9970 - mDice: 0.9239 - val_loss: 0.0445 - val_acc: 0.9967 - val_mDice: 0.9165

Epoch 00033: val_mDice did not improve from 0.91683
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.0397 - acc: 0.9970 - mDice: 0.9251 - val_loss: 0.0464 - val_acc: 0.9966 - val_mDice: 0.9131

Epoch 00034: val_mDice did not improve from 0.91683
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.0397 - acc: 0.9970 - mDice: 0.9253 - val_loss: 0.0458 - val_acc: 0.9966 - val_mDice: 0.9141

Epoch 00035: val_mDice did not improve from 0.91683
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.0404 - acc: 0.9970 - mDice: 0.9239 - val_loss: 0.0449 - val_acc: 0.9966 - val_mDice: 0.9157

Epoch 00036: val_mDice did not improve from 0.91683
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0387 - acc: 0.9971 - mDice: 0.9271 - val_loss: 0.0450 - val_acc: 0.9967 - val_mDice: 0.9156

Epoch 00037: val_mDice did not improve from 0.91683
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0381 - acc: 0.9972 - mDice: 0.9281 - val_loss: 0.0455 - val_acc: 0.9967 - val_mDice: 0.9147

Epoch 00038: val_mDice did not improve from 0.91683
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.0382 - acc: 0.9971 - mDice: 0.9279 - val_loss: 0.0463 - val_acc: 0.9966 - val_mDice: 0.9132

Epoch 00039: val_mDice did not improve from 0.91683
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0378 - acc: 0.9972 - mDice: 0.9286 - val_loss: 0.0453 - val_acc: 0.9967 - val_mDice: 0.9150

Epoch 00040: val_mDice did not improve from 0.91683
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0376 - acc: 0.9972 - mDice: 0.9289 - val_loss: 0.0460 - val_acc: 0.9966 - val_mDice: 0.9138

Epoch 00041: val_mDice did not improve from 0.91683
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.0373 - acc: 0.9972 - mDice: 0.9296 - val_loss: 0.0454 - val_acc: 0.9967 - val_mDice: 0.9148

Epoch 00042: val_mDice did not improve from 0.91683
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0374 - acc: 0.9972 - mDice: 0.9294 - val_loss: 0.0456 - val_acc: 0.9967 - val_mDice: 0.9145

Epoch 00043: val_mDice did not improve from 0.91683
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0369 - acc: 0.9972 - mDice: 0.9303 - val_loss: 0.0451 - val_acc: 0.9967 - val_mDice: 0.9154

Epoch 00044: val_mDice did not improve from 0.91683
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0366 - acc: 0.9973 - mDice: 0.9309 - val_loss: 0.0457 - val_acc: 0.9966 - val_mDice: 0.9144

Epoch 00045: val_mDice did not improve from 0.91683
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0367 - acc: 0.9972 - mDice: 0.9306 - val_loss: 0.0452 - val_acc: 0.9966 - val_mDice: 0.9152

Epoch 00046: val_mDice did not improve from 0.91683
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0367 - acc: 0.9972 - mDice: 0.9306 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9136

Epoch 00047: val_mDice did not improve from 0.91683
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0365 - acc: 0.9973 - mDice: 0.9310 - val_loss: 0.0473 - val_acc: 0.9965 - val_mDice: 0.9114

Epoch 00048: val_mDice did not improve from 0.91683
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0367 - acc: 0.9972 - mDice: 0.9307 - val_loss: 0.0462 - val_acc: 0.9966 - val_mDice: 0.9135

Epoch 00049: val_mDice did not improve from 0.91683
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0362 - acc: 0.9973 - mDice: 0.9316 - val_loss: 0.0475 - val_acc: 0.9965 - val_mDice: 0.9110

Epoch 00050: val_mDice did not improve from 0.91683
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0365 - acc: 0.9973 - mDice: 0.9310 - val_loss: 0.0458 - val_acc: 0.9966 - val_mDice: 0.9142

Epoch 00051: val_mDice did not improve from 0.91683
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.0359 - acc: 0.9973 - mDice: 0.9322 - val_loss: 0.0455 - val_acc: 0.9967 - val_mDice: 0.9147

Epoch 00052: val_mDice did not improve from 0.91683
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0368 - acc: 0.9972 - mDice: 0.9305 - val_loss: 0.0450 - val_acc: 0.9967 - val_mDice: 0.9155

Epoch 00053: val_mDice did not improve from 0.91683
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.0355 - acc: 0.9973 - mDice: 0.9329 - val_loss: 0.0463 - val_acc: 0.9966 - val_mDice: 0.9133

Epoch 00054: val_mDice did not improve from 0.91683
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.0350 - acc: 0.9974 - mDice: 0.9337 - val_loss: 0.0467 - val_acc: 0.9966 - val_mDice: 0.9126

Epoch 00055: val_mDice did not improve from 0.91683
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.0352 - acc: 0.9974 - mDice: 0.9333 - val_loss: 0.0469 - val_acc: 0.9966 - val_mDice: 0.9122

Epoch 00056: val_mDice did not improve from 0.91683
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.0350 - acc: 0.9974 - mDice: 0.9338 - val_loss: 0.0467 - val_acc: 0.9966 - val_mDice: 0.9125

Epoch 00057: val_mDice did not improve from 0.91683
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.0351 - acc: 0.9974 - mDice: 0.9335 - val_loss: 0.0460 - val_acc: 0.9966 - val_mDice: 0.9137

Epoch 00058: val_mDice did not improve from 0.91683
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.0347 - acc: 0.9974 - mDice: 0.9342 - val_loss: 0.0456 - val_acc: 0.9966 - val_mDice: 0.9145

Epoch 00059: val_mDice did not improve from 0.91683
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.0343 - acc: 0.9974 - mDice: 0.9350 - val_loss: 0.0456 - val_acc: 0.9966 - val_mDice: 0.9145

Epoch 00060: val_mDice did not improve from 0.91683
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.0345 - acc: 0.9974 - mDice: 0.9346 - val_loss: 0.0463 - val_acc: 0.9966 - val_mDice: 0.9133

Epoch 00061: val_mDice did not improve from 0.91683
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.0342 - acc: 0.9974 - mDice: 0.9351 - val_loss: 0.0455 - val_acc: 0.9966 - val_mDice: 0.9146

Epoch 00062: val_mDice did not improve from 0.91683
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.0344 - acc: 0.9974 - mDice: 0.9349 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9135

Epoch 00063: val_mDice did not improve from 0.91683
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.0348 - acc: 0.9974 - mDice: 0.9340 - val_loss: 0.0470 - val_acc: 0.9966 - val_mDice: 0.9119

Epoch 00064: val_mDice did not improve from 0.91683
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.0345 - acc: 0.9974 - mDice: 0.9347 - val_loss: 0.0459 - val_acc: 0.9966 - val_mDice: 0.9140

Epoch 00065: val_mDice did not improve from 0.91683
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.0340 - acc: 0.9974 - mDice: 0.9356 - val_loss: 0.0459 - val_acc: 0.9966 - val_mDice: 0.9139

Epoch 00066: val_mDice did not improve from 0.91683
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.0341 - acc: 0.9974 - mDice: 0.9354 - val_loss: 0.0454 - val_acc: 0.9967 - val_mDice: 0.9149

Epoch 00067: val_mDice did not improve from 0.91683
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.0340 - acc: 0.9974 - mDice: 0.9356 - val_loss: 0.0469 - val_acc: 0.9966 - val_mDice: 0.9122

Epoch 00068: val_mDice did not improve from 0.91683
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.0340 - acc: 0.9974 - mDice: 0.9356 - val_loss: 0.0460 - val_acc: 0.9966 - val_mDice: 0.9138

Epoch 00069: val_mDice did not improve from 0.91683
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.0336 - acc: 0.9975 - mDice: 0.9364 - val_loss: 0.0475 - val_acc: 0.9965 - val_mDice: 0.9112

Epoch 00070: val_mDice did not improve from 0.91683
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.0339 - acc: 0.9975 - mDice: 0.9358 - val_loss: 0.0464 - val_acc: 0.9966 - val_mDice: 0.9131

Epoch 00071: val_mDice did not improve from 0.91683
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.0342 - acc: 0.9974 - mDice: 0.9351 - val_loss: 0.0457 - val_acc: 0.9966 - val_mDice: 0.9143

Epoch 00072: val_mDice did not improve from 0.91683
Restoring model weights from the end of the best epoch
Epoch 00072: early stopping
{'val_loss': [0.2551984763602271, 0.11447952826419015, 0.45946496507547196, 0.19228587903245523, 0.18778588688069017, 0.07978137683150542, 0.09506663919365319, 0.06151765090053099, 0.07107937587493092, 0.060057609362432554, 0.05372592859840306, 0.05704617286848761, 0.06252741691296118, 0.04947360274619865, 0.05928692277384936, 0.050656160054198145, 0.05073049811333635, 0.045405826823663537, 0.04684092667307297, 0.04820856744301145, 0.045591020943039524, 0.04864026736604036, 0.04735169206222478, 0.04609483923681461, 0.05044909441558114, 0.04752012438745829, 0.045910999586764915, 0.04590173455866149, 0.046410130608799685, 0.04567723570350748, 0.04442475411198, 0.04430511911963894, 0.04447448556821277, 0.04638705787378071, 0.04583366336232989, 0.044935549289858254, 0.04500037628858194, 0.045470960682978595, 0.046298540448837903, 0.0453282875301194, 0.04599507896732675, 0.0454399556356625, 0.04561956781540474, 0.04512196000203164, 0.045683278518654134, 0.04523446045163339, 0.04612106858433163, 0.04729975802130508, 0.046182315470310896, 0.04753374979987632, 0.04576242334433716, 0.04548653820189681, 0.04502193705878989, 0.0462912928013906, 0.04665298342541622, 0.046859075376478425, 0.04671820492422494, 0.04604078932617703, 0.0455817574376825, 0.04561739421728318, 0.04625454230954612, 0.04552176624645282, 0.04614524734988265, 0.04703496431891065, 0.04587629313723449, 0.0459380378933066, 0.04535353175588768, 0.04689531074496951, 0.045968186164641905, 0.04746446816971267, 0.046398404237889025, 0.04569898491358235], 'val_acc': [0.9867325375114915, 0.9900568549650429, 0.9825725002880514, 0.9872318380070428, 0.9876459771699279, 0.9927603219547411, 0.992058240149143, 0.9945283623507423, 0.9939350322650297, 0.9949139552394839, 0.995680514043265, 0.9954597586697905, 0.9952919228668631, 0.9964039706400711, 0.9958759539753851, 0.9964049060414307, 0.9964040513456303, 0.996611217947772, 0.9965651311578542, 0.9965289698465027, 0.9966534524068346, 0.9964968081373368, 0.9965634206785773, 0.9966000177129342, 0.9963876450583883, 0.9965603486464841, 0.9966581457287725, 0.9966178858367196, 0.9966085594500939, 0.9966212978763301, 0.9966596016918656, 0.9967306510810434, 0.9967409861783911, 0.9966218143048948, 0.9966429260960461, 0.9966151457633415, 0.9966894433881245, 0.9966693522721312, 0.9966335270526635, 0.9966805562050673, 0.9966313044955261, 0.9966636245703175, 0.9966652489056552, 0.9966643054554932, 0.9966389136592837, 0.9966373752503499, 0.9966283040325137, 0.9965465791034003, 0.9966084724360139, 0.9965457270180222, 0.9966382362546712, 0.9966598488118527, 0.9966714852047662, 0.9965876099402017, 0.9965901796835183, 0.9965785580829982, 0.9965895884228448, 0.9966115583903599, 0.9966330151923382, 0.9966337820039178, 0.9965953922619785, 0.9966371042014909, 0.9966079620984349, 0.9965625590216505, 0.996613604308915, 0.9966113845797351, 0.9966514806677825, 0.996570342213568, 0.996589842068888, 0.9965299045952567, 0.9965771815202532, 0.9965995012843696], 'val_mDice': [0.6112033927092587, 0.8051739909353047, 0.4132848053518003, 0.6889067999202839, 0.6947703457226718, 0.8614324390018073, 0.8356924661754692, 0.8934548368854244, 0.8765320312367738, 0.8960765334811523, 0.9074841448425377, 0.9014202741375805, 0.8913405494098245, 0.9106273079005471, 0.8904681760464271, 0.9055467913620663, 0.9053676770116291, 0.9149293181669973, 0.9123538445382222, 0.9098499943740177, 0.9145541317271491, 0.9090663601447196, 0.9113807619488152, 0.9136438082604512, 0.9058321124880854, 0.9110621859992507, 0.9139588246815396, 0.9140016769405699, 0.9130536206882366, 0.9143901536934567, 0.9166400315117662, 0.9168334424930767, 0.9165230088860449, 0.9130806912035838, 0.914085141498677, 0.9157451639210221, 0.9155861459074229, 0.9147326411121953, 0.9132417568760197, 0.9149690115973897, 0.9137985319116689, 0.9147826924811314, 0.9144505854070621, 0.9153611068307919, 0.9143520273866445, 0.9151720339364379, 0.9135536879953676, 0.9114474212601237, 0.9134651241946394, 0.9110200228917338, 0.9142208425668035, 0.9146997491373633, 0.9155422932040083, 0.9132734656768994, 0.912600041523467, 0.9122346150178979, 0.9124804356672468, 0.9137041799343415, 0.9145286255074243, 0.9144631711235882, 0.9133238572708882, 0.9146368999550812, 0.9135118272617786, 0.9119080161526255, 0.9140044176665536, 0.9138892854217195, 0.9149402436548776, 0.9121623367723757, 0.9138386682437284, 0.911155170985382, 0.9130737004923994, 0.9143395032325801], 'loss': [0.6286206695760412, 0.10214728139010557, 0.0793558872612239, 0.07436778834961318, 0.06721915664343998, 0.06919500176689308, 0.06434352642723493, 0.061670739552363976, 0.05906508349139115, 0.0596329151593112, 0.05744554638275372, 0.05588542603947259, 0.053597062789454246, 0.05362115668252184, 0.04967674611163844, 0.04840377584246579, 0.04636735100244067, 0.04788855309116429, 0.045958572182015246, 0.04467648491630413, 0.0439143690826564, 0.04314549320510456, 0.04275565339353284, 0.04259212601361016, 0.04231181907756575, 0.04211621127765754, 0.04242790128870551, 0.04183303250981669, 0.04121695212983145, 0.04144338444976384, 0.041661501719887035, 0.04038121075979594, 0.040408897429264234, 0.0397453797531539, 0.039655984916123266, 0.040439765111272556, 0.03867241472299463, 0.038109636402188854, 0.038190414404193754, 0.03784081749067518, 0.037634875001431686, 0.03728844632802926, 0.037381799992522585, 0.0369095578789711, 0.03657973425582125, 0.03672860115032478, 0.036746916200580265, 0.036492152332231916, 0.03666414940738913, 0.03616182546457047, 0.036525197602404746, 0.03585165001341862, 0.03676189607000116, 0.03547806594649266, 0.03502145972965386, 0.03523947597651059, 0.034980600323583105, 0.03512895015513368, 0.034731205222495086, 0.03431010493100277, 0.03451940006223218, 0.03424435886072701, 0.03437679312352476, 0.0348404109698211, 0.03447168019457991, 0.03397046792059403, 0.03408189954722456, 0.033983657237491, 0.033987221514503355, 0.03355617037621038, 0.033880581152556564, 0.03423018028656837], 'acc': [0.9174069039927327, 0.9875185821443943, 0.9913974956338629, 0.9927645425491145, 0.9937383931845867, 0.9938696910595072, 0.994380952689448, 0.9949503012478645, 0.9953289660326953, 0.9953305868092429, 0.9955785292122752, 0.995831315446957, 0.9961007689607555, 0.9961831848609624, 0.9963872312324975, 0.9964277435993326, 0.996573186566677, 0.9964556238921405, 0.9966023165016926, 0.9966831826811353, 0.9967393596184078, 0.9967935522788851, 0.9968234682318025, 0.9968291421241948, 0.9968520358865485, 0.9968640117222453, 0.9968475558487653, 0.996877748390724, 0.9969281524860213, 0.9969049838963401, 0.9969009155123104, 0.9969862902105735, 0.9969827098212218, 0.9970281329648248, 0.997035216521747, 0.9969940197291632, 0.9971071813494113, 0.9971525102413347, 0.9971457895974221, 0.9971703014937527, 0.997187634113387, 0.9972102982657296, 0.9972095372054377, 0.997238595497432, 0.9972580266116288, 0.9972491643111694, 0.997246650052188, 0.9972606600798997, 0.9972488871348902, 0.9972840171729402, 0.9972622996480595, 0.9973112720574064, 0.9972443836075919, 0.9973333862614749, 0.9973713967013241, 0.9973509455549305, 0.9973750437421752, 0.9973632003286202, 0.9973930396469943, 0.997419609518474, 0.9974096622960321, 0.9974314890471585, 0.9974185451498172, 0.9973845061997475, 0.9974144421187527, 0.9974434481465758, 0.9974405204721273, 0.9974429410666668, 0.997448600571731, 0.9974792435251433, 0.9974563756012564, 0.9974286817564753], 'mDice': [0.440111700683979, 0.8244694447869738, 0.8624217372222487, 0.8709486347113924, 0.8834117184131604, 0.8799278662709767, 0.8884249620249706, 0.8931252507153403, 0.8977443311602024, 0.8966972634122876, 0.9005698538178881, 0.903272641703413, 0.9070574876122874, 0.9052460164272139, 0.908749369271283, 0.9100032745323745, 0.9134489468165806, 0.910664614785481, 0.9140764533592562, 0.9163532430315252, 0.9177050787239827, 0.9190726479873281, 0.9197594109427165, 0.9200392391881332, 0.9205329650141335, 0.9208771818377114, 0.9203078447304336, 0.9213795738267194, 0.9224862153894209, 0.9220657979913534, 0.9216734150360371, 0.9239851735495581, 0.9239259303496976, 0.9251217853846808, 0.9252835462246035, 0.9238559139773176, 0.9270679075729671, 0.9280889592147226, 0.9279368149823156, 0.9285702957895589, 0.9289448651774176, 0.92957339175229, 0.9294066159008759, 0.9302640266606373, 0.9308659757299377, 0.9305925231261793, 0.9305542316930048, 0.9310187450770674, 0.9307042486561931, 0.9316206233842033, 0.930954956949638, 0.9321859226438213, 0.9305198192596436, 0.93286696942569, 0.9336996416152992, 0.9333024086623356, 0.9337776899337769, 0.9335035176699972, 0.9342338210843467, 0.9350058932609746, 0.9346195262641155, 0.9351221572589404, 0.9348783542957212, 0.9340327241150617, 0.9347018943044353, 0.935623975810159, 0.9354180452858873, 0.9355962250033035, 0.9355914064228829, 0.93638112274884, 0.9357883580799761, 0.9351452682993094], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:01<00:11,  1.11s/it]predicting test subjects:  18%|█▊        | 2/11 [00:01<00:08,  1.09it/s]predicting test subjects:  27%|██▋       | 3/11 [00:01<00:06,  1.31it/s]predicting test subjects:  36%|███▋      | 4/11 [00:02<00:04,  1.50it/s]predicting test subjects:  45%|████▌     | 5/11 [00:02<00:03,  1.61it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:03<00:02,  1.74it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:03<00:02,  1.90it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:04<00:01,  1.85it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:04<00:01,  1.98it/s]predicting test subjects:  91%|█████████ | 10/11 [00:05<00:00,  2.00it/s]predicting test subjects: 100%|██████████| 11/11 [00:05<00:00,  1.94it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:17,  2.33it/s]predicting train subjects:   5%|▍         | 2/41 [00:00<00:16,  2.35it/s]predicting train subjects:   7%|▋         | 3/41 [00:01<00:16,  2.30it/s]predicting train subjects:  10%|▉         | 4/41 [00:01<00:16,  2.18it/s]predicting train subjects:  12%|█▏        | 5/41 [00:02<00:17,  2.05it/s]predicting train subjects:  15%|█▍        | 6/41 [00:02<00:16,  2.07it/s]predicting train subjects:  17%|█▋        | 7/41 [00:03<00:15,  2.19it/s]predicting train subjects:  20%|█▉        | 8/41 [00:03<00:13,  2.50it/s]predicting train subjects:  22%|██▏       | 9/41 [00:03<00:13,  2.43it/s]predicting train subjects:  24%|██▍       | 10/41 [00:04<00:13,  2.28it/s]predicting train subjects:  27%|██▋       | 11/41 [00:05<00:14,  2.07it/s]predicting train subjects:  29%|██▉       | 12/41 [00:05<00:12,  2.32it/s]predicting train subjects:  32%|███▏      | 13/41 [00:05<00:13,  2.14it/s]predicting train subjects:  34%|███▍      | 14/41 [00:06<00:12,  2.13it/s]predicting train subjects:  37%|███▋      | 15/41 [00:06<00:10,  2.39it/s]predicting train subjects:  39%|███▉      | 16/41 [00:07<00:10,  2.48it/s]predicting train subjects:  41%|████▏     | 17/41 [00:07<00:10,  2.26it/s]predicting train subjects:  44%|████▍     | 18/41 [00:08<00:10,  2.23it/s]predicting train subjects:  46%|████▋     | 19/41 [00:08<00:09,  2.29it/s]predicting train subjects:  49%|████▉     | 20/41 [00:08<00:08,  2.45it/s]predicting train subjects:  51%|█████     | 21/41 [00:09<00:08,  2.47it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:09<00:08,  2.24it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:10<00:07,  2.50it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:10<00:06,  2.56it/s]predicting train subjects:  61%|██████    | 25/41 [00:10<00:06,  2.37it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:11<00:06,  2.33it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:11<00:06,  2.30it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:12<00:05,  2.34it/s]predicting train subjects:  71%|███████   | 29/41 [00:12<00:04,  2.42it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:13<00:04,  2.36it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:13<00:04,  2.26it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:14<00:04,  2.14it/s]predicting train subjects:  80%|████████  | 33/41 [00:14<00:03,  2.26it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:15<00:03,  2.02it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:15<00:03,  1.75it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:16<00:02,  1.67it/s]predicting train subjects:  90%|█████████ | 37/41 [00:16<00:02,  1.72it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:17<00:01,  1.73it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:18<00:01,  1.70it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:18<00:00,  1.71it/s]predicting train subjects: 100%|██████████| 41/41 [00:19<00:00,  1.74it/s]
saving BB  test1-THALAMUS:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  73%|███████▎  | 8/11 [00:00<00:00, 74.99it/s]saving BB  test1-THALAMUS: 100%|██████████| 11/11 [00:00<00:00, 72.65it/s]
saving BB  train1-THALAMUS:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS:  15%|█▍        | 6/41 [00:00<00:00, 56.19it/s]saving BB  train1-THALAMUS:  32%|███▏      | 13/41 [00:00<00:00, 57.72it/s]saving BB  train1-THALAMUS:  51%|█████     | 21/41 [00:00<00:00, 62.42it/s]saving BB  train1-THALAMUS:  68%|██████▊   | 28/41 [00:00<00:00, 62.99it/s]saving BB  train1-THALAMUS:  85%|████████▌ | 35/41 [00:00<00:00, 62.09it/s]saving BB  train1-THALAMUS: 100%|██████████| 41/41 [00:00<00:00, 63.60it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:26,  1.52it/s]Loading train:   5%|▍         | 2/41 [00:01<00:26,  1.47it/s]Loading train:   7%|▋         | 3/41 [00:01<00:23,  1.58it/s]Loading train:  10%|▉         | 4/41 [00:02<00:21,  1.71it/s]Loading train:  12%|█▏        | 5/41 [00:03<00:22,  1.59it/s]Loading train:  15%|█▍        | 6/41 [00:03<00:22,  1.56it/s]Loading train:  17%|█▋        | 7/41 [00:04<00:20,  1.66it/s]Loading train:  20%|█▉        | 8/41 [00:04<00:19,  1.72it/s]Loading train:  22%|██▏       | 9/41 [00:05<00:18,  1.74it/s]Loading train:  24%|██▍       | 10/41 [00:06<00:18,  1.65it/s]Loading train:  27%|██▋       | 11/41 [00:06<00:18,  1.66it/s]Loading train:  29%|██▉       | 12/41 [00:07<00:16,  1.76it/s]Loading train:  32%|███▏      | 13/41 [00:07<00:17,  1.64it/s]Loading train:  34%|███▍      | 14/41 [00:08<00:15,  1.78it/s]Loading train:  37%|███▋      | 15/41 [00:08<00:14,  1.82it/s]Loading train:  39%|███▉      | 16/41 [00:09<00:13,  1.80it/s]Loading train:  41%|████▏     | 17/41 [00:10<00:13,  1.72it/s]Loading train:  44%|████▍     | 18/41 [00:10<00:14,  1.64it/s]Loading train:  46%|████▋     | 19/41 [00:11<00:13,  1.57it/s]Loading train:  49%|████▉     | 20/41 [00:11<00:12,  1.65it/s]Loading train:  51%|█████     | 21/41 [00:12<00:11,  1.80it/s]Loading train:  54%|█████▎    | 22/41 [00:12<00:10,  1.83it/s]Loading train:  56%|█████▌    | 23/41 [00:13<00:09,  1.95it/s]Loading train:  59%|█████▊    | 24/41 [00:13<00:09,  1.82it/s]Loading train:  61%|██████    | 25/41 [00:14<00:08,  1.85it/s]Loading train:  63%|██████▎   | 26/41 [00:15<00:08,  1.84it/s]Loading train:  66%|██████▌   | 27/41 [00:15<00:07,  1.87it/s]Loading train:  68%|██████▊   | 28/41 [00:16<00:07,  1.76it/s]Loading train:  71%|███████   | 29/41 [00:16<00:07,  1.67it/s]Loading train:  73%|███████▎  | 30/41 [00:17<00:06,  1.74it/s]Loading train:  76%|███████▌  | 31/41 [00:17<00:05,  1.75it/s]Loading train:  78%|███████▊  | 32/41 [00:18<00:05,  1.75it/s]Loading train:  80%|████████  | 33/41 [00:19<00:04,  1.63it/s]Loading train:  83%|████████▎ | 34/41 [00:19<00:04,  1.67it/s]Loading train:  85%|████████▌ | 35/41 [00:20<00:04,  1.33it/s]Loading train:  88%|████████▊ | 36/41 [00:21<00:03,  1.36it/s]Loading train:  90%|█████████ | 37/41 [00:22<00:02,  1.40it/s]Loading train:  93%|█████████▎| 38/41 [00:22<00:02,  1.41it/s]Loading train:  95%|█████████▌| 39/41 [00:23<00:01,  1.53it/s]Loading train:  98%|█████████▊| 40/41 [00:24<00:00,  1.54it/s]Loading train: 100%|██████████| 41/41 [00:24<00:00,  1.72it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   5%|▍         | 2/41 [00:00<00:02, 14.09it/s]concatenating: train:  12%|█▏        | 5/41 [00:00<00:02, 16.01it/s]concatenating: train:  29%|██▉       | 12/41 [00:00<00:01, 20.60it/s]concatenating: train:  90%|█████████ | 37/41 [00:00<00:00, 27.78it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 62.30it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:04,  2.05it/s]Loading test:  18%|█▊        | 2/11 [00:00<00:04,  2.04it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:04,  1.98it/s]Loading test:  36%|███▋      | 4/11 [00:02<00:03,  1.99it/s]Loading test:  45%|████▌     | 5/11 [00:02<00:03,  1.91it/s]Loading test:  55%|█████▍    | 6/11 [00:03<00:02,  1.91it/s]Loading test:  64%|██████▎   | 7/11 [00:03<00:02,  1.84it/s]Loading test:  73%|███████▎  | 8/11 [00:04<00:01,  1.83it/s]Loading test:  82%|████████▏ | 9/11 [00:04<00:01,  1.73it/s]Loading test:  91%|█████████ | 10/11 [00:05<00:00,  1.82it/s]Loading test: 100%|██████████| 11/11 [00:06<00:00,  1.65it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  27%|██▋       | 3/11 [00:00<00:00, 22.95it/s]concatenating: validation:  64%|██████▎   | 7/11 [00:00<00:00, 24.04it/s]concatenating: validation:  82%|████████▏ | 9/11 [00:00<00:00, 22.55it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 24.62it/s]
Loading trainS:   0%|          | 0/41 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/41 [00:00<00:15,  2.53it/s]Loading trainS:   5%|▍         | 2/41 [00:00<00:17,  2.20it/s]Loading trainS:   7%|▋         | 3/41 [00:01<00:18,  2.06it/s]Loading trainS:  10%|▉         | 4/41 [00:02<00:18,  2.01it/s]Loading trainS:  12%|█▏        | 5/41 [00:02<00:18,  1.96it/s]Loading trainS:  15%|█▍        | 6/41 [00:03<00:18,  1.91it/s]Loading trainS:  17%|█▋        | 7/41 [00:03<00:16,  2.08it/s]Loading trainS:  20%|█▉        | 8/41 [00:04<00:15,  2.06it/s]Loading trainS:  22%|██▏       | 9/41 [00:04<00:16,  1.95it/s]Loading trainS:  24%|██▍       | 10/41 [00:05<00:16,  1.90it/s]Loading trainS:  27%|██▋       | 11/41 [00:05<00:17,  1.76it/s]Loading trainS:  29%|██▉       | 12/41 [00:06<00:15,  1.92it/s]Loading trainS:  32%|███▏      | 13/41 [00:06<00:14,  1.94it/s]Loading trainS:  34%|███▍      | 14/41 [00:07<00:14,  1.83it/s]Loading trainS:  37%|███▋      | 15/41 [00:07<00:14,  1.83it/s]Loading trainS:  39%|███▉      | 16/41 [00:08<00:13,  1.80it/s]Loading trainS:  41%|████▏     | 17/41 [00:09<00:13,  1.78it/s]Loading trainS:  44%|████▍     | 18/41 [00:09<00:13,  1.70it/s]Loading trainS:  46%|████▋     | 19/41 [00:10<00:12,  1.80it/s]Loading trainS:  49%|████▉     | 20/41 [00:10<00:11,  1.87it/s]Loading trainS:  51%|█████     | 21/41 [00:11<00:10,  1.83it/s]Loading trainS:  54%|█████▎    | 22/41 [00:11<00:09,  1.98it/s]Loading trainS:  56%|█████▌    | 23/41 [00:12<00:08,  2.02it/s]Loading trainS:  59%|█████▊    | 24/41 [00:12<00:08,  1.91it/s]Loading trainS:  61%|██████    | 25/41 [00:13<00:08,  1.96it/s]Loading trainS:  63%|██████▎   | 26/41 [00:13<00:07,  2.07it/s]Loading trainS:  66%|██████▌   | 27/41 [00:14<00:07,  1.86it/s]Loading trainS:  68%|██████▊   | 28/41 [00:14<00:07,  1.77it/s]Loading trainS:  71%|███████   | 29/41 [00:15<00:06,  1.85it/s]Loading trainS:  73%|███████▎  | 30/41 [00:16<00:06,  1.66it/s]Loading trainS:  76%|███████▌  | 31/41 [00:16<00:05,  1.74it/s]Loading trainS:  78%|███████▊  | 32/41 [00:17<00:04,  1.80it/s]Loading trainS:  80%|████████  | 33/41 [00:17<00:04,  1.62it/s]Loading trainS:  83%|████████▎ | 34/41 [00:18<00:04,  1.66it/s]Loading trainS:  85%|████████▌ | 35/41 [00:19<00:03,  1.63it/s]Loading trainS:  88%|████████▊ | 36/41 [00:19<00:03,  1.46it/s]Loading trainS:  90%|█████████ | 37/41 [00:20<00:02,  1.51it/s]Loading trainS:  93%|█████████▎| 38/41 [00:21<00:01,  1.66it/s]Loading trainS:  95%|█████████▌| 39/41 [00:21<00:01,  1.61it/s]Loading trainS:  98%|█████████▊| 40/41 [00:22<00:00,  1.74it/s]Loading trainS: 100%|██████████| 41/41 [00:22<00:00,  1.71it/s]
Loading testS:   0%|          | 0/11 [00:00<?, ?it/s]Loading testS:   9%|▉         | 1/11 [00:00<00:04,  2.14it/s]Loading testS:  18%|█▊        | 2/11 [00:00<00:04,  2.22it/s]Loading testS:  27%|██▋       | 3/11 [00:01<00:03,  2.01it/s]Loading testS:  36%|███▋      | 4/11 [00:02<00:03,  1.77it/s]Loading testS:  45%|████▌     | 5/11 [00:02<00:03,  1.76it/s]Loading testS:  55%|█████▍    | 6/11 [00:03<00:02,  1.82it/s]Loading testS:  64%|██████▎   | 7/11 [00:03<00:02,  1.68it/s]Loading testS:  73%|███████▎  | 8/11 [00:04<00:01,  1.64it/s]Loading testS:  82%|████████▏ | 9/11 [00:05<00:01,  1.57it/s]Loading testS:  91%|█████████ | 10/11 [00:06<00:00,  1.41it/s]Loading testS: 100%|██████████| 11/11 [00:07<00:00,  1.32it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 128, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 128, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 128, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 128, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 128, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 128, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 128, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 64, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 64, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 64, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 64, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 64, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 64, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 64, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 64, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 64, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 32, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 32, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 32, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 32, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 32, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 32, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 32, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 64, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 64, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 64, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 64, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 64, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 64, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 64, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 64, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 64, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 64, 280)  0           concatenate_4[0][0]              2019-07-27 18:48:03.741017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-27 18:48:03.741116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-27 18:48:03.741134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-27 18:48:03.741145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-27 18:48:03.741427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 128, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 128, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 128, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 128, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 128, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 128, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 128, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 128, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 128, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 128, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 128, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
----- /array/ssd/msmajdi/experiments/keras/exp6/models/sE8_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1/1-THALAMUS/sd2/model_weights.h5
 --- initialized from Model_3T /array/ssd/msmajdi/experiments/keras/exp6/models/sE8_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1/1-THALAMUS/sd2
------------------------------------------------------------------
class_weights [0.97831385 0.02168615]
Train on 2690 samples, validate on 698 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 22s - loss: 0.0526 - acc: 0.9957 - mDice: 0.9022 - val_loss: 0.0473 - val_acc: 0.9961 - val_mDice: 0.9117

Epoch 00001: val_mDice improved from -inf to 0.91165, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0467 - acc: 0.9962 - mDice: 0.9127 - val_loss: 0.0447 - val_acc: 0.9962 - val_mDice: 0.9164

Epoch 00002: val_mDice improved from 0.91165 to 0.91637, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0448 - acc: 0.9964 - mDice: 0.9161 - val_loss: 0.0441 - val_acc: 0.9962 - val_mDice: 0.9174

Epoch 00003: val_mDice improved from 0.91637 to 0.91736, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0435 - acc: 0.9965 - mDice: 0.9184 - val_loss: 0.0456 - val_acc: 0.9960 - val_mDice: 0.9147

Epoch 00004: val_mDice did not improve from 0.91736
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0422 - acc: 0.9966 - mDice: 0.9207 - val_loss: 0.0434 - val_acc: 0.9963 - val_mDice: 0.9187

Epoch 00005: val_mDice improved from 0.91736 to 0.91868, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0405 - acc: 0.9967 - mDice: 0.9239 - val_loss: 0.0444 - val_acc: 0.9962 - val_mDice: 0.9168

Epoch 00006: val_mDice did not improve from 0.91868
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0397 - acc: 0.9968 - mDice: 0.9253 - val_loss: 0.0444 - val_acc: 0.9962 - val_mDice: 0.9168

Epoch 00007: val_mDice did not improve from 0.91868
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0392 - acc: 0.9968 - mDice: 0.9261 - val_loss: 0.0435 - val_acc: 0.9963 - val_mDice: 0.9185

Epoch 00008: val_mDice did not improve from 0.91868
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0376 - acc: 0.9969 - mDice: 0.9291 - val_loss: 0.0458 - val_acc: 0.9962 - val_mDice: 0.9143

Epoch 00009: val_mDice did not improve from 0.91868
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0373 - acc: 0.9970 - mDice: 0.9297 - val_loss: 0.0445 - val_acc: 0.9962 - val_mDice: 0.9167

Epoch 00010: val_mDice did not improve from 0.91868
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0370 - acc: 0.9970 - mDice: 0.9302 - val_loss: 0.0437 - val_acc: 0.9963 - val_mDice: 0.9181

Epoch 00011: val_mDice did not improve from 0.91868
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0366 - acc: 0.9970 - mDice: 0.9309 - val_loss: 0.0447 - val_acc: 0.9963 - val_mDice: 0.9163

Epoch 00012: val_mDice did not improve from 0.91868
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0357 - acc: 0.9971 - mDice: 0.9325 - val_loss: 0.0445 - val_acc: 0.9962 - val_mDice: 0.9167

Epoch 00013: val_mDice did not improve from 0.91868
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0355 - acc: 0.9971 - mDice: 0.9329 - val_loss: 0.0443 - val_acc: 0.9963 - val_mDice: 0.9171

Epoch 00014: val_mDice did not improve from 0.91868
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0341 - acc: 0.9972 - mDice: 0.9355 - val_loss: 0.0444 - val_acc: 0.9963 - val_mDice: 0.9168

Epoch 00015: val_mDice did not improve from 0.91868
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0341 - acc: 0.9972 - mDice: 0.9354 - val_loss: 0.0444 - val_acc: 0.9962 - val_mDice: 0.9168

Epoch 00016: val_mDice did not improve from 0.91868
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0336 - acc: 0.9973 - mDice: 0.9364 - val_loss: 0.0453 - val_acc: 0.9962 - val_mDice: 0.9151

Epoch 00017: val_mDice did not improve from 0.91868
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.0330 - acc: 0.9973 - mDice: 0.9374 - val_loss: 0.0464 - val_acc: 0.9961 - val_mDice: 0.9132

Epoch 00018: val_mDice did not improve from 0.91868
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.0323 - acc: 0.9974 - mDice: 0.9388 - val_loss: 0.0454 - val_acc: 0.9962 - val_mDice: 0.9150

Epoch 00019: val_mDice did not improve from 0.91868
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.0316 - acc: 0.9974 - mDice: 0.9400 - val_loss: 0.0456 - val_acc: 0.9961 - val_mDice: 0.9147

Epoch 00020: val_mDice did not improve from 0.91868
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.0313 - acc: 0.9974 - mDice: 0.9406 - val_loss: 0.0458 - val_acc: 0.9961 - val_mDice: 0.9144

Epoch 00021: val_mDice did not improve from 0.91868
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.0314 - acc: 0.9974 - mDice: 0.9405 - val_loss: 0.0458 - val_acc: 0.9961 - val_mDice: 0.9143

Epoch 00022: val_mDice did not improve from 0.91868
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.0309 - acc: 0.9975 - mDice: 0.9413 - val_loss: 0.0460 - val_acc: 0.9961 - val_mDice: 0.9139

Epoch 00023: val_mDice did not improve from 0.91868
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.0309 - acc: 0.9975 - mDice: 0.9413 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9150

Epoch 00024: val_mDice did not improve from 0.91868
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.0304 - acc: 0.9975 - mDice: 0.9422 - val_loss: 0.0457 - val_acc: 0.9962 - val_mDice: 0.9145

Epoch 00025: val_mDice did not improve from 0.91868
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.0304 - acc: 0.9975 - mDice: 0.9423 - val_loss: 0.0455 - val_acc: 0.9962 - val_mDice: 0.9149

Epoch 00026: val_mDice did not improve from 0.91868
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.0301 - acc: 0.9975 - mDice: 0.9428 - val_loss: 0.0456 - val_acc: 0.9962 - val_mDice: 0.9147

Epoch 00027: val_mDice did not improve from 0.91868
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.0299 - acc: 0.9975 - mDice: 0.9432 - val_loss: 0.0456 - val_acc: 0.9961 - val_mDice: 0.9147

Epoch 00028: val_mDice did not improve from 0.91868
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.0298 - acc: 0.9975 - mDice: 0.9433 - val_loss: 0.0450 - val_acc: 0.9962 - val_mDice: 0.9158

Epoch 00029: val_mDice did not improve from 0.91868
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.0296 - acc: 0.9976 - mDice: 0.9436 - val_loss: 0.0449 - val_acc: 0.9962 - val_mDice: 0.9160

Epoch 00030: val_mDice did not improve from 0.91868
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0294 - acc: 0.9976 - mDice: 0.9441 - val_loss: 0.0460 - val_acc: 0.9961 - val_mDice: 0.9140

Epoch 00031: val_mDice did not improve from 0.91868
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0291 - acc: 0.9976 - mDice: 0.9447 - val_loss: 0.0462 - val_acc: 0.9961 - val_mDice: 0.9136

Epoch 00032: val_mDice did not improve from 0.91868
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0291 - acc: 0.9976 - mDice: 0.9447 - val_loss: 0.0456 - val_acc: 0.9961 - val_mDice: 0.9147

Epoch 00033: val_mDice did not improve from 0.91868
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0292 - acc: 0.9976 - mDice: 0.9445 - val_loss: 0.0465 - val_acc: 0.9961 - val_mDice: 0.9131

Epoch 00034: val_mDice did not improve from 0.91868
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0289 - acc: 0.9976 - mDice: 0.9451 - val_loss: 0.0455 - val_acc: 0.9961 - val_mDice: 0.9149

Epoch 00035: val_mDice did not improve from 0.91868
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0289 - acc: 0.9976 - mDice: 0.9450 - val_loss: 0.0465 - val_acc: 0.9961 - val_mDice: 0.9131

Epoch 00036: val_mDice did not improve from 0.91868
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0285 - acc: 0.9977 - mDice: 0.9457 - val_loss: 0.0459 - val_acc: 0.9961 - val_mDice: 0.9141

Epoch 00037: val_mDice did not improve from 0.91868
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0283 - acc: 0.9977 - mDice: 0.9461 - val_loss: 0.0458 - val_acc: 0.9961 - val_mDice: 0.9144

Epoch 00038: val_mDice did not improve from 0.91868
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0281 - acc: 0.9977 - mDice: 0.9465 - val_loss: 0.0459 - val_acc: 0.9961 - val_mDice: 0.9141

Epoch 00039: val_mDice did not improve from 0.91868
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0280 - acc: 0.9977 - mDice: 0.9467 - val_loss: 0.0457 - val_acc: 0.9961 - val_mDice: 0.9146

Epoch 00040: val_mDice did not improve from 0.91868
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0278 - acc: 0.9977 - mDice: 0.9471 - val_loss: 0.0457 - val_acc: 0.9961 - val_mDice: 0.9146

Epoch 00041: val_mDice did not improve from 0.91868
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0279 - acc: 0.9977 - mDice: 0.9469 - val_loss: 0.0457 - val_acc: 0.9961 - val_mDice: 0.9145

Epoch 00042: val_mDice did not improve from 0.91868
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0277 - acc: 0.9977 - mDice: 0.9473 - val_loss: 0.0456 - val_acc: 0.9961 - val_mDice: 0.9148

Epoch 00043: val_mDice did not improve from 0.91868
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0277 - acc: 0.9977 - mDice: 0.9472 - val_loss: 0.0459 - val_acc: 0.9961 - val_mDice: 0.9141

Epoch 00044: val_mDice did not improve from 0.91868
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0277 - acc: 0.9977 - mDice: 0.9473 - val_loss: 0.0459 - val_acc: 0.9961 - val_mDice: 0.9142

predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:01<00:13,  1.37s/it]predicting test subjects:  18%|█▊        | 2/11 [00:01<00:09,  1.06s/it]predicting test subjects:  27%|██▋       | 3/11 [00:02<00:07,  1.09it/s]predicting test subjects:  36%|███▋      | 4/11 [00:02<00:05,  1.36it/s]predicting test subjects:  45%|████▌     | 5/11 [00:02<00:03,  1.58it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:03<00:02,  1.73it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:03<00:02,  1.96it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:04<00:01,  2.20it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:04<00:00,  2.06it/s]predicting test subjects:  91%|█████████ | 10/11 [00:04<00:00,  2.32it/s]predicting test subjects: 100%|██████████| 11/11 [00:05<00:00,  2.30it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:19,  2.06it/s]predicting train subjects:   5%|▍         | 2/41 [00:00<00:17,  2.26it/s]predicting train subjects:   7%|▋         | 3/41 [00:01<00:14,  2.56it/s]predicting train subjects:  10%|▉         | 4/41 [00:01<00:12,  2.88it/s]predicting train subjects:  12%|█▏        | 5/41 [00:01<00:13,  2.59it/s]predicting train subjects:  15%|█▍        | 6/41 [00:02<00:15,  2.19it/s]predicting train subjects:  17%|█▋        | 7/41 [00:02<00:14,  2.36it/s]predicting train subjects:  20%|█▉        | 8/41 [00:03<00:15,  2.13it/s]predicting train subjects:  22%|██▏       | 9/41 [00:03<00:13,  2.38it/s]predicting train subjects:  24%|██▍       | 10/41 [00:04<00:12,  2.51it/s]predicting train subjects:  27%|██▋       | 11/41 [00:04<00:11,  2.61it/s]predicting train subjects:  29%|██▉       | 12/41 [00:04<00:09,  2.92it/s]predicting train subjects:  32%|███▏      | 13/41 [00:04<00:09,  3.07it/s]predicting train subjects:  34%|███▍      | 14/41 [00:05<00:11,  2.42it/s]predicting train subjects:  37%|███▋      | 15/41 [00:05<00:09,  2.72it/s]predicting train subjects:  39%|███▉      | 16/41 [00:06<00:08,  2.84it/s]predicting train subjects:  41%|████▏     | 17/41 [00:06<00:09,  2.65it/s]predicting train subjects:  44%|████▍     | 18/41 [00:06<00:08,  2.72it/s]predicting train subjects:  46%|████▋     | 19/41 [00:07<00:07,  2.82it/s]predicting train subjects:  49%|████▉     | 20/41 [00:07<00:08,  2.52it/s]predicting train subjects:  51%|█████     | 21/41 [00:07<00:07,  2.71it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:08<00:06,  2.80it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:08<00:05,  3.11it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:08<00:05,  3.16it/s]predicting train subjects:  61%|██████    | 25/41 [00:09<00:05,  3.15it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:09<00:04,  3.02it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:10<00:05,  2.61it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:10<00:05,  2.44it/s]predicting train subjects:  71%|███████   | 29/41 [00:10<00:04,  2.69it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:11<00:03,  2.75it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:11<00:04,  2.47it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:12<00:03,  2.59it/s]predicting train subjects:  80%|████████  | 33/41 [00:12<00:03,  2.63it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:12<00:03,  2.20it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:13<00:02,  2.11it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:14<00:02,  1.93it/s]predicting train subjects:  90%|█████████ | 37/41 [00:14<00:01,  2.12it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:14<00:01,  2.18it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:15<00:00,  2.34it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:15<00:00,  2.48it/s]predicting train subjects: 100%|██████████| 41/41 [00:15<00:00,  2.57it/s]
predicting test subjects sagittal:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects sagittal:   9%|▉         | 1/11 [00:00<00:02,  3.55it/s]predicting test subjects sagittal:  18%|█▊        | 2/11 [00:00<00:02,  3.39it/s]predicting test subjects sagittal:  27%|██▋       | 3/11 [00:00<00:02,  3.31it/s]predicting test subjects sagittal:  36%|███▋      | 4/11 [00:01<00:02,  3.38it/s]predicting test subjects sagittal:  45%|████▌     | 5/11 [00:01<00:01,  3.42it/s]predicting test subjects sagittal:  55%|█████▍    | 6/11 [00:01<00:01,  3.40it/s]predicting test subjects sagittal:  64%|██████▎   | 7/11 [00:02<00:01,  3.41it/s]predicting test subjects sagittal:  73%|███████▎  | 8/11 [00:02<00:00,  3.27it/s]predicting test subjects sagittal:  82%|████████▏ | 9/11 [00:02<00:00,  3.12it/s]predicting test subjects sagittal:  91%|█████████ | 10/11 [00:03<00:00,  2.99it/s]predicting test subjects sagittal: 100%|██████████| 11/11 [00:03<00:00,  2.85it/s]
predicting train subjects sagittal:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects sagittal:   2%|▏         | 1/41 [00:00<00:15,  2.52it/s]predicting train subjects sagittal:   5%|▍         | 2/41 [00:00<00:14,  2.63it/s]predicting train subjects sagittal:   7%|▋         | 3/41 [00:01<00:13,  2.77it/s]predicting train subjects sagittal:  10%|▉         | 4/41 [00:01<00:12,  2.97it/s]predicting train subjects sagittal:  12%|█▏        | 5/41 [00:01<00:12,  2.94it/s]predicting train subjects sagittal:  15%|█▍        | 6/41 [00:01<00:11,  3.00it/s]predicting train subjects sagittal:  17%|█▋        | 7/41 [00:02<00:10,  3.09it/s]predicting train subjects sagittal:  20%|█▉        | 8/41 [00:02<00:09,  3.30it/s]predicting train subjects sagittal:  22%|██▏       | 9/41 [00:02<00:09,  3.32it/s]predicting train subjects sagittal:  24%|██▍       | 10/41 [00:03<00:09,  3.19it/s]predicting train subjects sagittal:  27%|██▋       | 11/41 [00:03<00:09,  3.11it/s]predicting train subjects sagittal:  29%|██▉       | 12/41 [00:03<00:08,  3.34it/s]predicting train subjects sagittal:  32%|███▏      | 13/41 [00:04<00:08,  3.25it/s]predicting train subjects sagittal:  34%|███▍      | 14/41 [00:04<00:08,  3.17it/s]predicting train subjects sagittal:  37%|███▋      | 15/41 [00:04<00:08,  3.21it/s]predicting train subjects sagittal:  39%|███▉      | 16/41 [00:05<00:07,  3.27it/s]predicting train subjects sagittal:  41%|████▏     | 17/41 [00:05<00:07,  3.03it/s]predicting train subjects sagittal:  44%|████▍     | 18/41 [00:05<00:07,  3.04it/s]predicting train subjects sagittal:  46%|████▋     | 19/41 [00:06<00:06,  3.17it/s]predicting train subjects sagittal:  49%|████▉     | 20/41 [00:06<00:06,  3.42it/s]predicting train subjects sagittal:  51%|█████     | 21/41 [00:06<00:05,  3.53it/s]predicting train subjects sagittal:  54%|█████▎    | 22/41 [00:06<00:05,  3.31it/s]predicting train subjects sagittal:  56%|█████▌    | 23/41 [00:07<00:04,  3.68it/s]predicting train subjects sagittal:  59%|█████▊    | 24/41 [00:07<00:04,  3.57it/s]predicting train subjects sagittal:  61%|██████    | 25/41 [00:07<00:04,  3.51it/s]predicting train subjects sagittal:  63%|██████▎   | 26/41 [00:08<00:04,  3.29it/s]predicting train subjects sagittal:  66%|██████▌   | 27/41 [00:08<00:04,  3.20it/s]predicting train subjects sagittal:  68%|██████▊   | 28/41 [00:08<00:04,  3.17it/s]predicting train subjects sagittal:  71%|███████   | 29/41 [00:08<00:03,  3.32it/s]predicting train subjects sagittal:  73%|███████▎  | 30/41 [00:09<00:03,  3.16it/s]predicting train subjects sagittal:  76%|███████▌  | 31/41 [00:09<00:03,  2.80it/s]predicting train subjects sagittal:  78%|███████▊  | 32/41 [00:10<00:03,  2.83it/s]predicting train subjects sagittal:  80%|████████  | 33/41 [00:10<00:02,  2.77it/s]predicting train subjects sagittal:  83%|████████▎ | 34/41 [00:10<00:02,  2.65it/s]predicting train subjects sagittal:  85%|████████▌ | 35/41 [00:11<00:02,  2.38it/s]predicting train subjects sagittal:  88%|████████▊ | 36/41 [00:11<00:02,  2.30it/s]predicting train subjects sagittal:  90%|█████████ | 37/41 [00:12<00:01,  2.19it/s]predicting train subjects sagittal:  93%|█████████▎| 38/41 [00:12<00:01,  2.36it/s]predicting train subjects sagittal:  95%|█████████▌| 39/41 [00:13<00:00,  2.51it/s]predicting train subjects sagittal:  98%|█████████▊| 40/41 [00:13<00:00,  2.58it/s]predicting train subjects sagittal: 100%|██████████| 41/41 [00:13<00:00,  2.61it/s]
saving BB  test1-THALAMUS:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  73%|███████▎  | 8/11 [00:00<00:00, 78.11it/s]saving BB  test1-THALAMUS: 100%|██████████| 11/11 [00:00<00:00, 76.27it/s]
saving BB  train1-THALAMUS:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS:  22%|██▏       | 9/41 [00:00<00:00, 81.81it/s]saving BB  train1-THALAMUS:  41%|████▏     | 17/41 [00:00<00:00, 80.50it/s]saving BB  train1-THALAMUS:  61%|██████    | 25/41 [00:00<00:00, 79.39it/s]saving BB  train1-THALAMUS:  80%|████████  | 33/41 [00:00<00:00, 78.48it/s]saving BB  train1-THALAMUS:  95%|█████████▌| 39/41 [00:00<00:00, 71.73it/s]saving BB  train1-THALAMUS: 100%|██████████| 41/41 [00:00<00:00, 74.25it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal:  82%|████████▏ | 9/11 [00:00<00:00, 81.42it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 11/11 [00:00<00:00, 79.77it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:  22%|██▏       | 9/41 [00:00<00:00, 79.73it/s]saving BB  train1-THALAMUS Sagittal:  41%|████▏     | 17/41 [00:00<00:00, 79.58it/s]saving BB  train1-THALAMUS Sagittal:  63%|██████▎   | 26/41 [00:00<00:00, 80.38it/s]saving BB  train1-THALAMUS Sagittal:  83%|████████▎ | 34/41 [00:00<00:00, 77.70it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 41/41 [00:00<00:00, 73.01it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:25,  1.58it/s]Loading train:   5%|▍         | 2/41 [00:01<00:22,  1.70it/s]Loading train:   7%|▋         | 3/41 [00:01<00:22,  1.69it/s]Loading train:  10%|▉         | 4/41 [00:02<00:20,  1.79it/s]Loading train:  12%|█▏        | 5/41 [00:02<00:20,  1.73it/s]Loading train:  15%|█▍        | 6/41 [00:03<00:20,  1.71it/s]Loading train:  17%|█▋        | 7/41 [00:03<00:18,  1.88it/s]Loading train:  20%|█▉        | 8/41 [00:04<00:15,  2.19it/s]Loading train:  22%|██▏       | 9/41 [00:04<00:14,  2.23it/s]Loading train:  24%|██▍       | 10/41 [00:05<00:14,  2.16it/s]Loading train:  27%|██▋       | 11/41 [00:05<00:15,  1.95it/s]Loading train:  29%|██▉       | 12/41 [00:05<00:13,  2.22it/s]Loading train:  32%|███▏      | 13/41 [00:06<00:12,  2.26it/s]Loading train:  34%|███▍      | 14/41 [00:06<00:11,  2.30it/s]Loading train:  37%|███▋      | 15/41 [00:07<00:09,  2.66it/s]Loading train:  39%|███▉      | 16/41 [00:07<00:10,  2.47it/s]Loading train:  41%|████▏     | 17/41 [00:08<00:11,  2.11it/s]Loading train:  44%|████▍     | 18/41 [00:08<00:11,  1.93it/s]Loading train:  46%|████▋     | 19/41 [00:09<00:11,  1.92it/s]Loading train:  49%|████▉     | 20/41 [00:09<00:10,  1.94it/s]Loading train:  51%|█████     | 21/41 [00:10<00:10,  1.92it/s]Loading train:  54%|█████▎    | 22/41 [00:10<00:10,  1.83it/s]Loading train:  56%|█████▌    | 23/41 [00:11<00:08,  2.11it/s]Loading train:  59%|█████▊    | 24/41 [00:11<00:07,  2.14it/s]Loading train:  61%|██████    | 25/41 [00:12<00:07,  2.14it/s]Loading train:  63%|██████▎   | 26/41 [00:12<00:06,  2.16it/s]Loading train:  66%|██████▌   | 27/41 [00:13<00:07,  1.92it/s]Loading train:  68%|██████▊   | 28/41 [00:13<00:06,  1.95it/s]Loading train:  71%|███████   | 29/41 [00:14<00:05,  2.05it/s]Loading train:  73%|███████▎  | 30/41 [00:14<00:05,  2.04it/s]Loading train:  76%|███████▌  | 31/41 [00:15<00:04,  2.13it/s]Loading train:  78%|███████▊  | 32/41 [00:15<00:04,  2.22it/s]Loading train:  80%|████████  | 33/41 [00:15<00:03,  2.21it/s]Loading train:  83%|████████▎ | 34/41 [00:16<00:03,  2.12it/s]Loading train:  85%|████████▌ | 35/41 [00:17<00:03,  1.82it/s]Loading train:  88%|████████▊ | 36/41 [00:17<00:02,  1.74it/s]Loading train:  90%|█████████ | 37/41 [00:18<00:02,  1.72it/s]Loading train:  93%|█████████▎| 38/41 [00:19<00:01,  1.64it/s]Loading train:  95%|█████████▌| 39/41 [00:19<00:01,  1.65it/s]Loading train:  98%|█████████▊| 40/41 [00:20<00:00,  1.62it/s]Loading train: 100%|██████████| 41/41 [00:20<00:00,  1.62it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:  10%|▉         | 4/41 [00:00<00:01, 30.95it/s]concatenating: train:  22%|██▏       | 9/41 [00:00<00:00, 34.65it/s]concatenating: train:  34%|███▍      | 14/41 [00:00<00:00, 37.82it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 102.16it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:04,  2.19it/s]Loading test:  18%|█▊        | 2/11 [00:00<00:04,  2.12it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:03,  2.17it/s]Loading test:  36%|███▋      | 4/11 [00:01<00:03,  2.18it/s]Loading test:  45%|████▌     | 5/11 [00:02<00:02,  2.03it/s]Loading test:  55%|█████▍    | 6/11 [00:02<00:02,  2.10it/s]Loading test:  64%|██████▎   | 7/11 [00:03<00:01,  2.18it/s]Loading test:  73%|███████▎  | 8/11 [00:03<00:01,  2.21it/s]Loading test:  82%|████████▏ | 9/11 [00:04<00:00,  2.15it/s]Loading test:  91%|█████████ | 10/11 [00:04<00:00,  2.03it/s]Loading test: 100%|██████████| 11/11 [00:05<00:00,  2.04it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  36%|███▋      | 4/11 [00:00<00:00, 27.19it/s]concatenating: validation:  64%|██████▎   | 7/11 [00:00<00:00, 25.68it/s]concatenating: validation:  91%|█████████ | 10/11 [00:00<00:00, 25.72it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 25.74it/s]
Epoch 00045: val_mDice did not improve from 0.91868
Restoring model weights from the end of the best epoch
Epoch 00045: early stopping
{'val_loss': [0.04729252933046879, 0.044678906250990566, 0.044131517186120454, 0.0456347827583466, 0.04338645710900725, 0.04442319826871413, 0.04444452481915411, 0.04348686866910546, 0.04581967337766828, 0.044503403010972935, 0.043724124477295616, 0.044696893286824566, 0.04446638486532222, 0.04425941328691548, 0.04443212973226107, 0.04444237863564901, 0.04534801284657167, 0.046436591516849306, 0.045433367477105475, 0.04559499895290864, 0.04579970146660135, 0.04583438090916008, 0.046045845838792, 0.04545715710623217, 0.04571938488024012, 0.04549238563950219, 0.04560449656451329, 0.045619645633636026, 0.04498860716051222, 0.044888729587154605, 0.04598814545781018, 0.046236572557670684, 0.04558521928665973, 0.04649029736190949, 0.0455273984475088, 0.046505159957275694, 0.04592942570290798, 0.04575668597716657, 0.04594649218641926, 0.045675240286271006, 0.04568411893395435, 0.04571352714411167, 0.045560403089212484, 0.045924507134879555, 0.045877143025483645], 'val_acc': [0.9961210458531421, 0.9962497756610968, 0.9962497642183714, 0.9960313249795689, 0.9963127791369201, 0.996208569211058, 0.9962443694002648, 0.9963327589895801, 0.9961716078413934, 0.996213975642677, 0.9963052518729836, 0.9962619227119708, 0.9962474524462462, 0.996266271630809, 0.9962525719557929, 0.9962378164758655, 0.9961984282236728, 0.9961014692284658, 0.9961710264826231, 0.9961463342764999, 0.9961442202756603, 0.9961346635504233, 0.9961171848726136, 0.9961710544916825, 0.9961502712572201, 0.996171141251452, 0.9961632102471709, 0.9961474934075145, 0.9961958326036404, 0.9962157912787188, 0.9961283794446798, 0.996107355400621, 0.9961474023780713, 0.9960977993585318, 0.9961396809293752, 0.99611276183224, 0.996140351951293, 0.9961383252225837, 0.9961277285756218, 0.9961487604758801, 0.9961391021324092, 0.9961339838183711, 0.9961373575437377, 0.9961240590472618, 0.9961249210089872], 'val_mDice': [0.911654312836065, 0.9163672818153841, 0.917359687025021, 0.9147478297651668, 0.9186835189945035, 0.9168475096068615, 0.9167958446765014, 0.9184885581789863, 0.9143089537634207, 0.9167051902814718, 0.9180697588660998, 0.9163014026289341, 0.9167479286904321, 0.9171058749059552, 0.9168084940117888, 0.9167938654265636, 0.9151483323307639, 0.9132227287934639, 0.9150215792109426, 0.9147489543972179, 0.9143645266407199, 0.9142997896773767, 0.9139235263909173, 0.9149621076091996, 0.9144855180920708, 0.9149076255481359, 0.9147047402182418, 0.9146746007624191, 0.9158047652859401, 0.9159825160715164, 0.9140161250245605, 0.9135850331503204, 0.9147455661549609, 0.9131127112574428, 0.9148528888437332, 0.9130831184906399, 0.9141176382245171, 0.9144380886780157, 0.9140891326874239, 0.9145761253499028, 0.9145671252193287, 0.9145146070373775, 0.9147876273936734, 0.9141358089310392, 0.91421826025818], 'loss': [0.05259708331752444, 0.046714896897068696, 0.04480445450067963, 0.04353278502821922, 0.042226608565638056, 0.040456696638498164, 0.039677755526339696, 0.039235679730496, 0.037589102249495604, 0.03725011095570809, 0.03695001026622425, 0.03656822415311097, 0.03572327103535039, 0.03550517207295478, 0.03408112903177517, 0.0341103475637817, 0.03356879250904663, 0.03301217031689382, 0.03225084559015625, 0.03161487456231091, 0.031305972812177965, 0.031363116584191976, 0.030929361632765447, 0.030905291355010744, 0.030407694628856882, 0.030392719031100378, 0.03010332400193888, 0.02985564439962566, 0.029826682469492508, 0.02964752065675631, 0.029377206419092573, 0.02908288830108802, 0.02907664546237559, 0.029174058376136323, 0.02886138560049596, 0.02891068257474545, 0.028546858911288272, 0.028297492950379182, 0.02810836781411809, 0.027960646461321518, 0.0277591591292376, 0.027890848099742238, 0.027682210059081756, 0.02771427974686525, 0.02766754805737047], 'acc': [0.9957477665302036, 0.9961919680403954, 0.9963879802413146, 0.9964739857553107, 0.9965707515695281, 0.9966961825647319, 0.9967645362407301, 0.9968052167431573, 0.9969210166027112, 0.996967984619637, 0.9969924229671521, 0.9970450724810915, 0.9970858081565913, 0.9971073890264149, 0.9971994259100421, 0.997200476414209, 0.9972548094823901, 0.9973033030680121, 0.9973606369752424, 0.9973984737378514, 0.9974373536925334, 0.9974383565573001, 0.9974506723393295, 0.997464612514113, 0.9975060009601834, 0.997504424428408, 0.9975251344943136, 0.9975484913166571, 0.9975484399104207, 0.9975638394018974, 0.9975908757585575, 0.9976021490132498, 0.9976036327035896, 0.9976082599296002, 0.997626302853836, 0.9976330138050491, 0.9976599305092624, 0.9976697151988856, 0.9976900441495903, 0.9977010656909872, 0.9977152850548131, 0.997717189522924, 0.997724399912313, 0.9977177144426396, 0.9977346643196162], 'mDice': [0.9022457843376358, 0.9126711671680323, 0.9160602389658251, 0.918366959325443, 0.9206848889035363, 0.9238814022461278, 0.9252797998903409, 0.9261067733445575, 0.9290697107084622, 0.9297048445084725, 0.9302438577311632, 0.9309366321031932, 0.9324684639402482, 0.9328718349392973, 0.9354716200367669, 0.9354143721020355, 0.9364035971545819, 0.9374276264449478, 0.9388277364486212, 0.9399971680570269, 0.9405752196631024, 0.9404641349962652, 0.94126030126912, 0.9413042810769773, 0.9422165272847427, 0.942250144304396, 0.9427822752956121, 0.9432411107432, 0.9432879154566939, 0.9436180458636089, 0.9441235965069342, 0.9446684995105276, 0.9446816750175447, 0.9445011956984226, 0.9450768296603377, 0.9449790609813534, 0.945658555482843, 0.9461186705468756, 0.9464692336032825, 0.9467488713424002, 0.9471147627635517, 0.946867968956334, 0.9472580177633293, 0.9472003676634295, 0.9472894276384971], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 0  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 92, 116, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 92, 116, 40)  400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 92, 116, 40)  160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 92, 116, 40)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 92, 116, 40)  14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 92, 116, 40)  160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 92, 116, 40)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 46, 58, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 46, 58, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 46, 58, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 46, 58, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 46, 58, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 46, 58, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 46, 58, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 46, 58, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 46, 58, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 23, 29, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 23, 29, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 23, 29, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 23, 29, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 23, 29, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 23, 29, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 23, 29, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 23, 29, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 23, 29, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 23, 29, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 46, 58, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 46, 58, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 46, 58, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 46, 58, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 58, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 58, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 46, 58, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 46, 58, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 46, 58, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 46, 58, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 92, 116, 40)  44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 92, 116, 80)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 92, 116, 40)  28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 92, 116, 40)  160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 92, 116, 40)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 92, 116, 40)  14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 92, 116, 40)  160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 92, 116, 40)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 92, 116, 120) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 92, 116, 120) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 92, 116, 2)   242         dropout_5[0][0]                  2019-07-27 18:59:53.281799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-27 18:59:53.281900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-27 18:59:53.281918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-27 18:59:53.281928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-27 18:59:53.282180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)

==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
----- /array/ssd/msmajdi/experiments/keras/exp6/models/sE8_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1/1-THALAMUS/sd1/model_weights.h5
 --- initialized from Model_3T /array/ssd/msmajdi/experiments/keras/exp6/models/sE8_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1/1-THALAMUS/sd1
------------------------------------------------------------------
class_weights [0.98000917 0.01999083]
Train on 4060 samples, validate on 1096 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 21s - loss: 0.0491 - acc: 0.9963 - mDice: 0.9083 - val_loss: 0.0466 - val_acc: 0.9965 - val_mDice: 0.9129

Epoch 00001: val_mDice improved from -inf to 0.91290, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 15s - loss: 0.0441 - acc: 0.9966 - mDice: 0.9173 - val_loss: 0.0458 - val_acc: 0.9966 - val_mDice: 0.9141

Epoch 00002: val_mDice improved from 0.91290 to 0.91411, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 15s - loss: 0.0420 - acc: 0.9968 - mDice: 0.9210 - val_loss: 0.0449 - val_acc: 0.9967 - val_mDice: 0.9157

Epoch 00003: val_mDice improved from 0.91411 to 0.91574, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 15s - loss: 0.0405 - acc: 0.9969 - mDice: 0.9236 - val_loss: 0.0445 - val_acc: 0.9967 - val_mDice: 0.9164

Epoch 00004: val_mDice improved from 0.91574 to 0.91641, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 15s - loss: 0.0391 - acc: 0.9971 - mDice: 0.9262 - val_loss: 0.0439 - val_acc: 0.9967 - val_mDice: 0.9175

Epoch 00005: val_mDice improved from 0.91641 to 0.91749, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 14s - loss: 0.0382 - acc: 0.9971 - mDice: 0.9277 - val_loss: 0.0457 - val_acc: 0.9966 - val_mDice: 0.9143

Epoch 00006: val_mDice did not improve from 0.91749
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 14s - loss: 0.0370 - acc: 0.9972 - mDice: 0.9299 - val_loss: 0.0455 - val_acc: 0.9967 - val_mDice: 0.9146

Epoch 00007: val_mDice did not improve from 0.91749
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 15s - loss: 0.0366 - acc: 0.9972 - mDice: 0.9308 - val_loss: 0.0455 - val_acc: 0.9966 - val_mDice: 0.9146

Epoch 00008: val_mDice did not improve from 0.91749
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 15s - loss: 0.0357 - acc: 0.9973 - mDice: 0.9324 - val_loss: 0.0455 - val_acc: 0.9967 - val_mDice: 0.9146

Epoch 00009: val_mDice did not improve from 0.91749
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 16s - loss: 0.0353 - acc: 0.9973 - mDice: 0.9331 - val_loss: 0.0468 - val_acc: 0.9966 - val_mDice: 0.9123

Epoch 00010: val_mDice did not improve from 0.91749
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 16s - loss: 0.0348 - acc: 0.9974 - mDice: 0.9340 - val_loss: 0.0473 - val_acc: 0.9965 - val_mDice: 0.9115

Epoch 00011: val_mDice did not improve from 0.91749
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 16s - loss: 0.0338 - acc: 0.9974 - mDice: 0.9359 - val_loss: 0.0466 - val_acc: 0.9966 - val_mDice: 0.9128

Epoch 00012: val_mDice did not improve from 0.91749
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 16s - loss: 0.0333 - acc: 0.9975 - mDice: 0.9367 - val_loss: 0.0454 - val_acc: 0.9966 - val_mDice: 0.9149

Epoch 00013: val_mDice did not improve from 0.91749
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 16s - loss: 0.0329 - acc: 0.9975 - mDice: 0.9376 - val_loss: 0.0464 - val_acc: 0.9966 - val_mDice: 0.9130

Epoch 00014: val_mDice did not improve from 0.91749
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 16s - loss: 0.0326 - acc: 0.9975 - mDice: 0.9381 - val_loss: 0.0485 - val_acc: 0.9965 - val_mDice: 0.9093

Epoch 00015: val_mDice did not improve from 0.91749
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 16s - loss: 0.0321 - acc: 0.9976 - mDice: 0.9390 - val_loss: 0.0481 - val_acc: 0.9965 - val_mDice: 0.9099

Epoch 00016: val_mDice did not improve from 0.91749
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 16s - loss: 0.0316 - acc: 0.9976 - mDice: 0.9400 - val_loss: 0.0473 - val_acc: 0.9965 - val_mDice: 0.9114

Epoch 00017: val_mDice did not improve from 0.91749
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 16s - loss: 0.0312 - acc: 0.9976 - mDice: 0.9407 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9136

Epoch 00018: val_mDice did not improve from 0.91749
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 16s - loss: 0.0302 - acc: 0.9977 - mDice: 0.9426 - val_loss: 0.0468 - val_acc: 0.9965 - val_mDice: 0.9124

Epoch 00019: val_mDice did not improve from 0.91749
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 16s - loss: 0.0298 - acc: 0.9977 - mDice: 0.9432 - val_loss: 0.0473 - val_acc: 0.9965 - val_mDice: 0.9113

Epoch 00020: val_mDice did not improve from 0.91749
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 16s - loss: 0.0295 - acc: 0.9978 - mDice: 0.9437 - val_loss: 0.0468 - val_acc: 0.9966 - val_mDice: 0.9124

Epoch 00021: val_mDice did not improve from 0.91749
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 16s - loss: 0.0291 - acc: 0.9978 - mDice: 0.9446 - val_loss: 0.0481 - val_acc: 0.9965 - val_mDice: 0.9101

Epoch 00022: val_mDice did not improve from 0.91749
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 16s - loss: 0.0292 - acc: 0.9978 - mDice: 0.9443 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9136

Epoch 00023: val_mDice did not improve from 0.91749
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 16s - loss: 0.0288 - acc: 0.9978 - mDice: 0.9451 - val_loss: 0.0468 - val_acc: 0.9966 - val_mDice: 0.9124

Epoch 00024: val_mDice did not improve from 0.91749
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 16s - loss: 0.0287 - acc: 0.9978 - mDice: 0.9453 - val_loss: 0.0478 - val_acc: 0.9965 - val_mDice: 0.9105

Epoch 00025: val_mDice did not improve from 0.91749
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0282 - acc: 0.9979 - mDice: 0.9463 - val_loss: 0.0479 - val_acc: 0.9965 - val_mDice: 0.9103

Epoch 00026: val_mDice did not improve from 0.91749
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0281 - acc: 0.9979 - mDice: 0.9464 - val_loss: 0.0473 - val_acc: 0.9965 - val_mDice: 0.9115

Epoch 00027: val_mDice did not improve from 0.91749
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0278 - acc: 0.9979 - mDice: 0.9469 - val_loss: 0.0470 - val_acc: 0.9966 - val_mDice: 0.9119

Epoch 00028: val_mDice did not improve from 0.91749
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0277 - acc: 0.9979 - mDice: 0.9472 - val_loss: 0.0482 - val_acc: 0.9965 - val_mDice: 0.9098

Epoch 00029: val_mDice did not improve from 0.91749
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0276 - acc: 0.9979 - mDice: 0.9473 - val_loss: 0.0480 - val_acc: 0.9965 - val_mDice: 0.9102

Epoch 00030: val_mDice did not improve from 0.91749
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0274 - acc: 0.9979 - mDice: 0.9477 - val_loss: 0.0469 - val_acc: 0.9965 - val_mDice: 0.9122

Epoch 00031: val_mDice did not improve from 0.91749
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0270 - acc: 0.9979 - mDice: 0.9484 - val_loss: 0.0471 - val_acc: 0.9965 - val_mDice: 0.9118

Epoch 00032: val_mDice did not improve from 0.91749
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0269 - acc: 0.9980 - mDice: 0.9486 - val_loss: 0.0463 - val_acc: 0.9966 - val_mDice: 0.9132

Epoch 00033: val_mDice did not improve from 0.91749
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0269 - acc: 0.9980 - mDice: 0.9485 - val_loss: 0.0478 - val_acc: 0.9965 - val_mDice: 0.9105

Epoch 00034: val_mDice did not improve from 0.91749
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0269 - acc: 0.9980 - mDice: 0.9487 - val_loss: 0.0479 - val_acc: 0.9965 - val_mDice: 0.9104

Epoch 00035: val_mDice did not improve from 0.91749
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 15s - loss: 0.0267 - acc: 0.9980 - mDice: 0.9490 - val_loss: 0.0477 - val_acc: 0.9965 - val_mDice: 0.9108

Epoch 00036: val_mDice did not improve from 0.91749
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0263 - acc: 0.9980 - mDice: 0.9497 - val_loss: 0.0471 - val_acc: 0.9965 - val_mDice: 0.9117

Epoch 00037: val_mDice did not improve from 0.91749
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0261 - acc: 0.9980 - mDice: 0.9500 - val_loss: 0.0476 - val_acc: 0.9965 - val_mDice: 0.9108

Epoch 00038: val_mDice did not improve from 0.91749
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0258 - acc: 0.9980 - mDice: 0.9508 - val_loss: 0.0484 - val_acc: 0.9965 - val_mDice: 0.9094

Epoch 00039: val_mDice did not improve from 0.91749
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0257 - acc: 0.9980 - mDice: 0.9508 - val_loss: 0.0484 - val_acc: 0.9965 - val_mDice: 0.9095

Epoch 00040: val_mDice did not improve from 0.91749
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0257 - acc: 0.9980 - mDice: 0.9509 - val_loss: 0.0481 - val_acc: 0.9965 - val_mDice: 0.9101

Epoch 00041: val_mDice did not improve from 0.91749
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0255 - acc: 0.9981 - mDice: 0.9512 - val_loss: 0.0480 - val_acc: 0.9965 - val_mDice: 0.9101

Epoch 00042: val_mDice did not improve from 0.91749
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0255 - acc: 0.9981 - mDice: 0.9512 - val_loss: 0.0484 - val_acc: 0.9965 - val_mDice: 0.9095

Epoch 00043: val_mDice did not improve from 0.91749
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0252 - acc: 0.9981 - mDice: 0.9518 - val_loss: 0.0475 - val_acc: 0.9965 - val_mDice: 0.9110

Epoch 00044: val_mDice did not improve from 0.91749
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.0253 - acc: 0.9981 - mDice: 0.9516 - val_loss: 0.0477 - val_acc: 0.9965 - val_mDice: 0.9108

predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:01<00:16,  1.65s/it]predicting test subjects:  18%|█▊        | 2/11 [00:02<00:11,  1.31s/it]predicting test subjects:  27%|██▋       | 3/11 [00:02<00:08,  1.09s/it]predicting test subjects:  36%|███▋      | 4/11 [00:03<00:06,  1.02it/s]predicting test subjects:  45%|████▌     | 5/11 [00:04<00:05,  1.07it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:05<00:04,  1.13it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:06<00:02,  1.36it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:06<00:01,  1.41it/s]predicting test subjects:  91%|█████████ | 10/11 [00:07<00:00,  1.43it/s]predicting test subjects: 100%|██████████| 11/11 [00:08<00:00,  1.45it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:28,  1.40it/s]predicting train subjects:   5%|▍         | 2/41 [00:01<00:27,  1.40it/s]predicting train subjects:   7%|▋         | 3/41 [00:01<00:25,  1.51it/s]predicting train subjects:  10%|▉         | 4/41 [00:02<00:23,  1.56it/s]predicting train subjects:  12%|█▏        | 5/41 [00:03<00:23,  1.54it/s]predicting train subjects:  15%|█▍        | 6/41 [00:03<00:22,  1.52it/s]predicting train subjects:  17%|█▋        | 7/41 [00:04<00:21,  1.58it/s]predicting train subjects:  20%|█▉        | 8/41 [00:04<00:17,  1.85it/s]predicting train subjects:  22%|██▏       | 9/41 [00:05<00:18,  1.74it/s]predicting train subjects:  24%|██▍       | 10/41 [00:06<00:18,  1.69it/s]predicting train subjects:  27%|██▋       | 11/41 [00:06<00:18,  1.62it/s]predicting train subjects:  29%|██▉       | 12/41 [00:07<00:16,  1.79it/s]predicting train subjects:  32%|███▏      | 13/41 [00:07<00:16,  1.69it/s]predicting train subjects:  34%|███▍      | 14/41 [00:08<00:16,  1.63it/s]predicting train subjects:  37%|███▋      | 15/41 [00:08<00:14,  1.83it/s]predicting train subjects:  39%|███▉      | 16/41 [00:09<00:14,  1.72it/s]predicting train subjects:  41%|████▏     | 17/41 [00:10<00:15,  1.56it/s]predicting train subjects:  44%|████▍     | 18/41 [00:10<00:14,  1.63it/s]predicting train subjects:  46%|████▋     | 19/41 [00:11<00:13,  1.63it/s]predicting train subjects:  49%|████▉     | 20/41 [00:11<00:11,  1.81it/s]predicting train subjects:  51%|█████     | 21/41 [00:12<00:10,  1.92it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:12<00:10,  1.83it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:13<00:08,  2.13it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:13<00:08,  2.10it/s]predicting train subjects:  61%|██████    | 25/41 [00:14<00:08,  1.88it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:15<00:08,  1.81it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:15<00:07,  1.82it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:16<00:07,  1.75it/s]predicting train subjects:  71%|███████   | 29/41 [00:16<00:07,  1.70it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:17<00:07,  1.57it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:18<00:05,  1.68it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:18<00:05,  1.79it/s]predicting train subjects:  80%|████████  | 33/41 [00:19<00:04,  1.81it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:19<00:04,  1.59it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:20<00:04,  1.44it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:21<00:03,  1.35it/s]predicting train subjects:  90%|█████████ | 37/41 [00:22<00:03,  1.24it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:23<00:02,  1.33it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:23<00:01,  1.36it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:24<00:00,  1.34it/s]predicting train subjects: 100%|██████████| 41/41 [00:25<00:00,  1.42it/s]
saving BB  test1-THALAMUS:   0%|          | 0/11 [00:00<?, ?it/s]saving BB  test1-THALAMUS:  55%|█████▍    | 6/11 [00:00<00:00, 51.47it/s]saving BB  test1-THALAMUS:  91%|█████████ | 10/11 [00:00<00:00, 43.44it/s]saving BB  test1-THALAMUS: 100%|██████████| 11/11 [00:00<00:00, 39.72it/s]
saving BB  train1-THALAMUS:   0%|          | 0/41 [00:00<?, ?it/s]saving BB  train1-THALAMUS:  22%|██▏       | 9/41 [00:00<00:00, 78.40it/s]saving BB  train1-THALAMUS:  41%|████▏     | 17/41 [00:00<00:00, 76.49it/s]saving BB  train1-THALAMUS:  63%|██████▎   | 26/41 [00:00<00:00, 77.28it/s]saving BB  train1-THALAMUS:  78%|███████▊  | 32/41 [00:00<00:00, 70.95it/s]saving BB  train1-THALAMUS:  93%|█████████▎| 38/41 [00:00<00:00, 58.50it/s]saving BB  train1-THALAMUS: 100%|██████████| 41/41 [00:00<00:00, 61.96it/s]

Epoch 00045: val_mDice did not improve from 0.91749
Restoring model weights from the end of the best epoch
Epoch 00045: early stopping
{'val_loss': [0.046566067562594904, 0.04583629384555303, 0.04490095236930099, 0.04454688646279983, 0.043929394388938475, 0.04573249765229921, 0.04552353585451624, 0.04551828233865056, 0.04552371333604747, 0.04679737696899985, 0.04725179739677123, 0.046552866371008604, 0.0453685030449916, 0.04644271683790823, 0.04847544180161326, 0.04812178571783278, 0.04731080963881346, 0.04607338987182091, 0.046780988220533316, 0.04733842842444016, 0.046758677647279125, 0.04806227757711045, 0.04611650290117211, 0.046755415353461775, 0.04781612166523063, 0.04792620952710183, 0.04727566560363247, 0.047045054118128586, 0.04821461348039825, 0.04800090825960149, 0.04690546515214182, 0.04709291508434898, 0.046289018375703886, 0.047834005301995, 0.04790991596388121, 0.047688529806306765, 0.04713842857384334, 0.047627797295903634, 0.04844650797491526, 0.04839864449344412, 0.04806740531684273, 0.04804918757320321, 0.04840121521566906, 0.04752299835375191, 0.04767351813723136], 'val_acc': [0.9964838145423109, 0.9966273782027029, 0.9967054237414451, 0.9966899515503514, 0.9967416840313125, 0.9966392484459564, 0.9966517262650232, 0.9966330186729013, 0.9966613186971984, 0.9965924890372004, 0.9965219652130656, 0.9965758114835642, 0.9966304495821904, 0.9965884715971285, 0.9964815117146847, 0.9965008373243095, 0.9965263972752285, 0.9965993285614209, 0.9965487163867394, 0.9965432451589264, 0.9965623854285609, 0.9965150343240613, 0.9965952182338186, 0.9965517958150293, 0.9964945990673817, 0.9965054482004069, 0.9965311841373026, 0.9965524762651347, 0.9964821984733108, 0.9964975057727229, 0.9965353812614497, 0.9965438466437542, 0.9965790645049436, 0.9965063133378969, 0.9964794590525383, 0.9964873314338879, 0.9965325639630757, 0.9965181028755912, 0.9964691968294825, 0.9964666438363764, 0.9964836489980238, 0.9964835715554926, 0.9964768190453522, 0.9965267462016892, 0.9965150334539205], 'val_mDice': [0.9128978224131312, 0.9141080884167748, 0.9157399909339682, 0.9164117879241053, 0.9174946574834142, 0.9142512957545093, 0.9146294330593443, 0.9146449481048723, 0.914602091495138, 0.9123242279908953, 0.9115362287002758, 0.9127673417547323, 0.9149027647328203, 0.9129724628733893, 0.9093083441257477, 0.9099279951005086, 0.9114095977623097, 0.913638820830923, 0.9123644215347123, 0.9113452636847531, 0.9123916180029402, 0.9100606933127354, 0.9135641037982746, 0.9124138270416399, 0.9105144277976377, 0.9103107645998906, 0.9114788516159475, 0.9118801949233034, 0.9097853518315475, 0.9101682261828958, 0.9121505169972887, 0.9117991030216217, 0.9132407738344512, 0.9104695542015299, 0.9103590983108882, 0.910758920177056, 0.911728297489403, 0.9108488163808837, 0.9093791107191657, 0.9094669871086621, 0.9100571226899641, 0.9100890629482965, 0.90945107027562, 0.9110248173675398, 0.910759561035755], 'loss': [0.0490768366846545, 0.044056065931108786, 0.04197847582583357, 0.0405457957734909, 0.03907971025394102, 0.03824989881201331, 0.03704427793768826, 0.03656506441129839, 0.03567367480159393, 0.035281565847802045, 0.03483466438839001, 0.03379651948148981, 0.03333336926005744, 0.03285738123865257, 0.03256554642848193, 0.03211548264721051, 0.031572375097885505, 0.031167571676041693, 0.030153867793200638, 0.02981971698845255, 0.029539053443030185, 0.029088508264331396, 0.029200305740160894, 0.028805917862967906, 0.028698807417098526, 0.028169362113026564, 0.028083218812135054, 0.02781525249661777, 0.027662358205453517, 0.027628819682988626, 0.027394439612189538, 0.027017052537701987, 0.026895014211315238, 0.026939726934717793, 0.026853134277565725, 0.026685376105637384, 0.0262977722022921, 0.026134026507512103, 0.02575336213182346, 0.025723938632921634, 0.025665455098663057, 0.02548992716399907, 0.025522328050706186, 0.02517070224866491, 0.025286255771376817], 'acc': [0.9962897673616269, 0.9966451957308012, 0.9968297575494925, 0.9969416727573414, 0.9970536587273546, 0.9971093508997574, 0.9971990837839436, 0.9972396554618046, 0.9973122671319934, 0.9973327796447453, 0.9973690407029514, 0.9974392943194347, 0.9974758572179109, 0.9975131482913576, 0.9975298581452205, 0.9975654283180613, 0.9976021468345755, 0.9976359097828419, 0.9977075695404278, 0.9977367485098063, 0.9977513592818688, 0.9977930111838091, 0.9977811475105474, 0.9978066277621415, 0.9978204795292446, 0.997855792785513, 0.9978624837739127, 0.9978826788845908, 0.9978938463873464, 0.9978933886354193, 0.9979139240504485, 0.997941021555163, 0.9979505309917657, 0.997951362814222, 0.9979564162897946, 0.997967380021006, 0.997993411982588, 0.9980086963164982, 0.9980381462961582, 0.9980379492778496, 0.9980431539671761, 0.9980547789282399, 0.9980575553889345, 0.9980759705815997, 0.9980703882983165], 'mDice': [0.9083422049513004, 0.917282555490879, 0.9209900404432138, 0.9235723045071945, 0.9262256008650869, 0.9277352567376762, 0.9299297508935036, 0.9308015739389242, 0.9324317879864735, 0.9331473396916695, 0.9339671176055382, 0.9358730181097397, 0.9367241066664898, 0.9375996754087251, 0.9381357419666986, 0.9389658620204832, 0.9399689394852211, 0.9407131489861775, 0.9425863559022912, 0.9432018710474662, 0.9437263114699002, 0.9445593236702416, 0.9443494598266526, 0.9450831862506021, 0.9452794848991732, 0.9462647672944469, 0.9464230387668892, 0.9469192650517807, 0.94720420490932, 0.9472671870527596, 0.9477028374014229, 0.9484043579383437, 0.9486302513207121, 0.9485459630125261, 0.9487097988574963, 0.9490207648042388, 0.949742561197046, 0.9500469509603942, 0.9507560888534696, 0.9508121360111706, 0.9509224874045461, 0.9512490277807114, 0.9511860788162119, 0.9518466298216082, 0.9516300741087627], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
