2019-08-17 16:59:55.104694: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-17 16:59:55.726947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:08:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-08-17 16:59:55.727011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-17 16:59:56.536562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-17 16:59:56.536646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-17 16:59:56.536662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-17 16:59:56.537307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<08:18,  1.88s/it]Loading train:   1%|          | 2/266 [00:03<07:53,  1.79s/it]Loading train:   1%|          | 3/266 [00:04<07:02,  1.61s/it]Loading train:   2%|▏         | 4/266 [00:06<06:42,  1.54s/it]Loading train:   2%|▏         | 5/266 [00:06<05:53,  1.35s/it]Loading train:   2%|▏         | 6/266 [00:08<06:11,  1.43s/it]Loading train:   3%|▎         | 7/266 [00:09<06:05,  1.41s/it]Loading train:   3%|▎         | 8/266 [00:11<06:23,  1.49s/it]Loading train:   3%|▎         | 9/266 [00:13<06:21,  1.49s/it]Loading train:   4%|▍         | 10/266 [00:14<06:24,  1.50s/it]Loading train:   4%|▍         | 11/266 [00:16<06:47,  1.60s/it]Loading train:   5%|▍         | 12/266 [00:17<06:08,  1.45s/it]Loading train:   5%|▍         | 13/266 [00:19<06:15,  1.48s/it]Loading train:   5%|▌         | 14/266 [00:20<06:28,  1.54s/it]Loading train:   6%|▌         | 15/266 [00:22<06:17,  1.50s/it]Loading train:   6%|▌         | 16/266 [00:23<06:10,  1.48s/it]Loading train:   6%|▋         | 17/266 [00:25<06:05,  1.47s/it]Loading train:   7%|▋         | 18/266 [00:26<06:06,  1.48s/it]Loading train:   7%|▋         | 19/266 [00:27<05:51,  1.42s/it]Loading train:   8%|▊         | 20/266 [00:29<06:09,  1.50s/it]Loading train:   8%|▊         | 21/266 [00:31<06:21,  1.56s/it]Loading train:   8%|▊         | 22/266 [00:32<06:26,  1.58s/it]Loading train:   9%|▊         | 23/266 [00:34<06:09,  1.52s/it]Loading train:   9%|▉         | 24/266 [00:35<06:05,  1.51s/it]Loading train:   9%|▉         | 25/266 [00:38<07:12,  1.80s/it]Loading train:  10%|▉         | 26/266 [00:39<06:45,  1.69s/it]Loading train:  10%|█         | 27/266 [00:41<06:22,  1.60s/it]Loading train:  11%|█         | 28/266 [00:42<06:27,  1.63s/it]Loading train:  11%|█         | 29/266 [00:44<06:17,  1.59s/it]Loading train:  11%|█▏        | 30/266 [00:45<06:17,  1.60s/it]Loading train:  12%|█▏        | 31/266 [00:47<05:54,  1.51s/it]Loading train:  12%|█▏        | 32/266 [00:48<05:43,  1.47s/it]Loading train:  12%|█▏        | 33/266 [00:49<05:42,  1.47s/it]Loading train:  13%|█▎        | 34/266 [00:51<05:49,  1.51s/it]Loading train:  13%|█▎        | 35/266 [00:53<05:46,  1.50s/it]Loading train:  14%|█▎        | 36/266 [00:54<05:53,  1.53s/it]Loading train:  14%|█▍        | 37/266 [00:56<05:51,  1.53s/it]Loading train:  14%|█▍        | 38/266 [00:57<05:31,  1.45s/it]Loading train:  15%|█▍        | 39/266 [00:58<05:28,  1.45s/it]Loading train:  15%|█▌        | 40/266 [01:00<05:22,  1.43s/it]Loading train:  15%|█▌        | 41/266 [01:01<04:54,  1.31s/it]Loading train:  16%|█▌        | 42/266 [01:02<05:11,  1.39s/it]Loading train:  16%|█▌        | 43/266 [01:04<04:56,  1.33s/it]Loading train:  17%|█▋        | 44/266 [01:05<04:59,  1.35s/it]Loading train:  17%|█▋        | 45/266 [01:06<04:57,  1.35s/it]Loading train:  17%|█▋        | 46/266 [01:07<04:42,  1.28s/it]Loading train:  18%|█▊        | 47/266 [01:09<04:40,  1.28s/it]Loading train:  18%|█▊        | 48/266 [01:09<04:02,  1.11s/it]Loading train:  18%|█▊        | 49/266 [01:11<04:00,  1.11s/it]Loading train:  19%|█▉        | 50/266 [01:12<04:34,  1.27s/it]Loading train:  19%|█▉        | 51/266 [01:14<05:03,  1.41s/it]Loading train:  20%|█▉        | 52/266 [01:15<05:02,  1.41s/it]Loading train:  20%|█▉        | 53/266 [01:17<04:46,  1.35s/it]Loading train:  20%|██        | 54/266 [01:18<04:43,  1.34s/it]Loading train:  21%|██        | 55/266 [01:18<03:50,  1.09s/it]Loading train:  21%|██        | 56/266 [01:20<04:04,  1.17s/it]Loading train:  21%|██▏       | 57/266 [01:21<04:39,  1.34s/it]Loading train:  22%|██▏       | 58/266 [01:23<04:33,  1.31s/it]Loading train:  22%|██▏       | 59/266 [01:24<04:30,  1.31s/it]Loading train:  23%|██▎       | 60/266 [01:26<04:43,  1.38s/it]Loading train:  23%|██▎       | 61/266 [01:27<04:35,  1.34s/it]Loading train:  23%|██▎       | 62/266 [01:28<04:32,  1.34s/it]Loading train:  24%|██▎       | 63/266 [01:30<04:35,  1.36s/it]Loading train:  24%|██▍       | 64/266 [01:31<04:20,  1.29s/it]Loading train:  24%|██▍       | 65/266 [01:32<04:32,  1.36s/it]Loading train:  25%|██▍       | 66/266 [01:34<04:36,  1.38s/it]Loading train:  25%|██▌       | 67/266 [01:35<04:32,  1.37s/it]Loading train:  26%|██▌       | 68/266 [01:36<04:16,  1.29s/it]Loading train:  26%|██▌       | 69/266 [01:37<04:09,  1.27s/it]Loading train:  26%|██▋       | 70/266 [01:39<04:21,  1.33s/it]Loading train:  27%|██▋       | 71/266 [01:40<04:15,  1.31s/it]Loading train:  27%|██▋       | 72/266 [01:41<04:08,  1.28s/it]Loading train:  27%|██▋       | 73/266 [01:43<04:12,  1.31s/it]Loading train:  28%|██▊       | 74/266 [01:44<04:20,  1.36s/it]Loading train:  28%|██▊       | 75/266 [01:45<04:21,  1.37s/it]Loading train:  29%|██▊       | 76/266 [01:47<04:47,  1.51s/it]Loading train:  29%|██▉       | 77/266 [01:49<04:33,  1.45s/it]Loading train:  29%|██▉       | 78/266 [01:50<04:16,  1.36s/it]Loading train:  30%|██▉       | 79/266 [01:51<04:07,  1.32s/it]Loading train:  30%|███       | 80/266 [01:52<03:53,  1.25s/it]Loading train:  30%|███       | 81/266 [01:53<03:55,  1.27s/it]Loading train:  31%|███       | 82/266 [01:55<03:58,  1.30s/it]Loading train:  31%|███       | 83/266 [01:56<04:00,  1.32s/it]Loading train:  32%|███▏      | 84/266 [01:57<03:45,  1.24s/it]Loading train:  32%|███▏      | 85/266 [01:58<03:38,  1.21s/it]Loading train:  32%|███▏      | 86/266 [02:00<03:54,  1.30s/it]Loading train:  33%|███▎      | 87/266 [02:02<04:22,  1.47s/it]Loading train:  33%|███▎      | 88/266 [02:03<04:35,  1.55s/it]Loading train:  33%|███▎      | 89/266 [02:05<04:09,  1.41s/it]Loading train:  34%|███▍      | 90/266 [02:06<04:15,  1.45s/it]Loading train:  34%|███▍      | 91/266 [02:08<04:23,  1.51s/it]Loading train:  35%|███▍      | 92/266 [02:09<04:09,  1.43s/it]Loading train:  35%|███▍      | 93/266 [02:10<03:45,  1.30s/it]Loading train:  35%|███▌      | 94/266 [02:11<03:43,  1.30s/it]Loading train:  36%|███▌      | 95/266 [02:13<03:48,  1.34s/it]Loading train:  36%|███▌      | 96/266 [02:14<03:51,  1.36s/it]Loading train:  36%|███▋      | 97/266 [02:16<04:04,  1.45s/it]Loading train:  37%|███▋      | 98/266 [02:17<04:02,  1.44s/it]Loading train:  37%|███▋      | 99/266 [02:18<03:48,  1.37s/it]Loading train:  38%|███▊      | 100/266 [02:20<03:53,  1.41s/it]Loading train:  38%|███▊      | 101/266 [02:22<04:03,  1.48s/it]Loading train:  38%|███▊      | 102/266 [02:23<04:03,  1.48s/it]Loading train:  39%|███▊      | 103/266 [02:24<03:58,  1.47s/it]Loading train:  39%|███▉      | 104/266 [02:26<03:51,  1.43s/it]Loading train:  39%|███▉      | 105/266 [02:27<03:41,  1.38s/it]Loading train:  40%|███▉      | 106/266 [02:28<03:39,  1.37s/it]Loading train:  40%|████      | 107/266 [02:29<03:20,  1.26s/it]Loading train:  41%|████      | 108/266 [02:31<03:25,  1.30s/it]Loading train:  41%|████      | 109/266 [02:32<03:23,  1.30s/it]Loading train:  41%|████▏     | 110/266 [02:33<03:23,  1.30s/it]Loading train:  42%|████▏     | 111/266 [02:35<03:31,  1.36s/it]Loading train:  42%|████▏     | 112/266 [02:36<03:15,  1.27s/it]Loading train:  42%|████▏     | 113/266 [02:37<03:18,  1.30s/it]Loading train:  43%|████▎     | 114/266 [02:38<03:06,  1.22s/it]Loading train:  43%|████▎     | 115/266 [02:40<03:09,  1.26s/it]Loading train:  44%|████▎     | 116/266 [02:41<03:20,  1.34s/it]Loading train:  44%|████▍     | 117/266 [02:43<03:21,  1.35s/it]Loading train:  44%|████▍     | 118/266 [02:44<03:19,  1.35s/it]Loading train:  45%|████▍     | 119/266 [02:45<03:19,  1.36s/it]Loading train:  45%|████▌     | 120/266 [02:47<03:35,  1.48s/it]Loading train:  45%|████▌     | 121/266 [02:49<03:33,  1.48s/it]Loading train:  46%|████▌     | 122/266 [02:51<04:05,  1.71s/it]Loading train:  46%|████▌     | 123/266 [02:52<03:56,  1.66s/it]Loading train:  47%|████▋     | 124/266 [02:54<03:49,  1.62s/it]Loading train:  47%|████▋     | 125/266 [02:56<03:56,  1.68s/it]Loading train:  47%|████▋     | 126/266 [02:57<03:46,  1.62s/it]Loading train:  48%|████▊     | 127/266 [02:59<03:57,  1.71s/it]Loading train:  48%|████▊     | 128/266 [03:01<03:44,  1.63s/it]Loading train:  48%|████▊     | 129/266 [03:01<03:11,  1.40s/it]Loading train:  49%|████▉     | 130/266 [03:03<03:09,  1.39s/it]Loading train:  49%|████▉     | 131/266 [03:04<03:15,  1.45s/it]Loading train:  50%|████▉     | 132/266 [03:06<03:09,  1.42s/it]Loading train:  50%|█████     | 133/266 [03:08<03:33,  1.60s/it]Loading train:  50%|█████     | 134/266 [03:09<03:35,  1.64s/it]Loading train:  51%|█████     | 135/266 [03:11<03:19,  1.52s/it]Loading train:  51%|█████     | 136/266 [03:12<02:59,  1.38s/it]Loading train:  52%|█████▏    | 137/266 [03:13<02:54,  1.36s/it]Loading train:  52%|█████▏    | 138/266 [03:15<02:57,  1.39s/it]Loading train:  52%|█████▏    | 139/266 [03:16<03:02,  1.44s/it]Loading train:  53%|█████▎    | 140/266 [03:17<02:56,  1.40s/it]Loading train:  53%|█████▎    | 141/266 [03:19<03:04,  1.47s/it]Loading train:  53%|█████▎    | 142/266 [03:20<03:01,  1.46s/it]Loading train:  54%|█████▍    | 143/266 [03:22<03:03,  1.49s/it]Loading train:  54%|█████▍    | 144/266 [03:23<02:57,  1.46s/it]Loading train:  55%|█████▍    | 145/266 [03:25<03:00,  1.49s/it]Loading train:  55%|█████▍    | 146/266 [03:27<03:07,  1.56s/it]Loading train:  55%|█████▌    | 147/266 [03:28<03:07,  1.58s/it]Loading train:  56%|█████▌    | 148/266 [03:30<03:07,  1.59s/it]Loading train:  56%|█████▌    | 149/266 [03:31<02:48,  1.44s/it]Loading train:  56%|█████▋    | 150/266 [03:33<02:57,  1.53s/it]Loading train:  57%|█████▋    | 151/266 [03:34<02:54,  1.52s/it]Loading train:  57%|█████▋    | 152/266 [03:36<02:46,  1.46s/it]Loading train:  58%|█████▊    | 153/266 [03:36<02:20,  1.24s/it]Loading train:  58%|█████▊    | 154/266 [03:38<02:21,  1.26s/it]Loading train:  58%|█████▊    | 155/266 [03:39<02:29,  1.35s/it]Loading train:  59%|█████▊    | 156/266 [03:40<02:23,  1.31s/it]Loading train:  59%|█████▉    | 157/266 [03:42<02:25,  1.33s/it]Loading train:  59%|█████▉    | 158/266 [03:43<02:30,  1.39s/it]Loading train:  60%|█████▉    | 159/266 [03:45<02:29,  1.40s/it]Loading train:  60%|██████    | 160/266 [03:46<02:16,  1.29s/it]Loading train:  61%|██████    | 161/266 [03:48<02:33,  1.46s/it]Loading train:  61%|██████    | 162/266 [03:49<02:32,  1.46s/it]Loading train:  61%|██████▏   | 163/266 [03:50<02:18,  1.35s/it]Loading train:  62%|██████▏   | 164/266 [03:52<02:24,  1.42s/it]Loading train:  62%|██████▏   | 165/266 [03:53<02:22,  1.41s/it]Loading train:  62%|██████▏   | 166/266 [03:55<02:22,  1.42s/it]Loading train:  63%|██████▎   | 167/266 [03:56<02:17,  1.39s/it]Loading train:  63%|██████▎   | 168/266 [03:57<02:19,  1.42s/it]Loading train:  64%|██████▎   | 169/266 [03:59<02:21,  1.46s/it]Loading train:  64%|██████▍   | 170/266 [04:00<02:19,  1.45s/it]Loading train:  64%|██████▍   | 171/266 [04:02<02:16,  1.44s/it]Loading train:  65%|██████▍   | 172/266 [04:03<02:14,  1.43s/it]Loading train:  65%|██████▌   | 173/266 [04:05<02:10,  1.40s/it]Loading train:  65%|██████▌   | 174/266 [04:06<02:01,  1.32s/it]Loading train:  66%|██████▌   | 175/266 [04:07<01:50,  1.21s/it]Loading train:  66%|██████▌   | 176/266 [04:08<01:54,  1.28s/it]Loading train:  67%|██████▋   | 177/266 [04:09<01:50,  1.24s/it]Loading train:  67%|██████▋   | 178/266 [04:10<01:48,  1.23s/it]Loading train:  67%|██████▋   | 179/266 [04:12<01:49,  1.26s/it]Loading train:  68%|██████▊   | 180/266 [04:13<01:53,  1.32s/it]Loading train:  68%|██████▊   | 181/266 [04:15<01:52,  1.33s/it]Loading train:  68%|██████▊   | 182/266 [04:16<01:53,  1.35s/it]Loading train:  69%|██████▉   | 183/266 [04:17<01:55,  1.39s/it]Loading train:  69%|██████▉   | 184/266 [04:19<01:50,  1.35s/it]Loading train:  70%|██████▉   | 185/266 [04:20<01:55,  1.42s/it]Loading train:  70%|██████▉   | 186/266 [04:22<01:50,  1.38s/it]Loading train:  70%|███████   | 187/266 [04:23<01:46,  1.35s/it]Loading train:  71%|███████   | 188/266 [04:24<01:44,  1.34s/it]Loading train:  71%|███████   | 189/266 [04:25<01:41,  1.32s/it]Loading train:  71%|███████▏  | 190/266 [04:27<01:40,  1.32s/it]Loading train:  72%|███████▏  | 191/266 [04:28<01:32,  1.23s/it]Loading train:  72%|███████▏  | 192/266 [04:29<01:25,  1.16s/it]Loading train:  73%|███████▎  | 193/266 [04:30<01:30,  1.24s/it]Loading train:  73%|███████▎  | 194/266 [04:32<01:33,  1.29s/it]Loading train:  73%|███████▎  | 195/266 [04:33<01:37,  1.37s/it]Loading train:  74%|███████▎  | 196/266 [04:35<01:39,  1.42s/it]Loading train:  74%|███████▍  | 197/266 [04:36<01:36,  1.40s/it]Loading train:  74%|███████▍  | 198/266 [04:38<01:37,  1.43s/it]Loading train:  75%|███████▍  | 199/266 [04:39<01:34,  1.41s/it]Loading train:  75%|███████▌  | 200/266 [04:40<01:34,  1.44s/it]Loading train:  76%|███████▌  | 201/266 [04:41<01:26,  1.32s/it]Loading train:  76%|███████▌  | 202/266 [04:43<01:20,  1.25s/it]Loading train:  76%|███████▋  | 203/266 [04:44<01:26,  1.38s/it]Loading train:  77%|███████▋  | 204/266 [04:46<01:27,  1.40s/it]Loading train:  77%|███████▋  | 205/266 [04:47<01:22,  1.36s/it]Loading train:  77%|███████▋  | 206/266 [04:48<01:18,  1.31s/it]Loading train:  78%|███████▊  | 207/266 [04:49<01:17,  1.32s/it]Loading train:  78%|███████▊  | 208/266 [04:51<01:14,  1.29s/it]Loading train:  79%|███████▊  | 209/266 [04:52<01:05,  1.15s/it]Loading train:  79%|███████▉  | 210/266 [04:53<01:05,  1.17s/it]Loading train:  79%|███████▉  | 211/266 [04:54<01:08,  1.24s/it]Loading train:  80%|███████▉  | 212/266 [04:55<01:05,  1.21s/it]Loading train:  80%|████████  | 213/266 [04:56<00:59,  1.11s/it]Loading train:  80%|████████  | 214/266 [04:57<01:01,  1.18s/it]Loading train:  81%|████████  | 215/266 [04:58<00:53,  1.04s/it]Loading train:  81%|████████  | 216/266 [04:59<00:55,  1.10s/it]Loading train:  82%|████████▏ | 217/266 [05:01<00:54,  1.12s/it]Loading train:  82%|████████▏ | 218/266 [05:02<00:56,  1.17s/it]Loading train:  82%|████████▏ | 219/266 [05:03<00:57,  1.23s/it]Loading train:  83%|████████▎ | 220/266 [05:05<01:00,  1.32s/it]Loading train:  83%|████████▎ | 221/266 [05:06<00:59,  1.31s/it]Loading train:  83%|████████▎ | 222/266 [05:08<00:59,  1.36s/it]Loading train:  84%|████████▍ | 223/266 [05:09<01:04,  1.51s/it]Loading train:  84%|████████▍ | 224/266 [05:11<01:07,  1.60s/it]Loading train:  85%|████████▍ | 225/266 [05:13<01:08,  1.67s/it]Loading train:  85%|████████▍ | 226/266 [05:15<01:06,  1.66s/it]Loading train:  85%|████████▌ | 227/266 [05:16<01:03,  1.64s/it]Loading train:  86%|████████▌ | 228/266 [05:18<01:02,  1.64s/it]Loading train:  86%|████████▌ | 229/266 [05:19<00:58,  1.57s/it]Loading train:  86%|████████▋ | 230/266 [05:22<01:02,  1.75s/it]Loading train:  87%|████████▋ | 231/266 [05:23<01:03,  1.81s/it]Loading train:  87%|████████▋ | 232/266 [05:26<01:06,  1.96s/it]Loading train:  88%|████████▊ | 233/266 [05:27<00:58,  1.76s/it]Loading train:  88%|████████▊ | 234/266 [05:30<01:02,  1.97s/it]Loading train:  88%|████████▊ | 235/266 [05:32<01:01,  1.99s/it]Loading train:  89%|████████▊ | 236/266 [05:34<01:01,  2.04s/it]Loading train:  89%|████████▉ | 237/266 [05:35<00:55,  1.93s/it]Loading train:  89%|████████▉ | 238/266 [05:36<00:46,  1.67s/it]Loading train:  90%|████████▉ | 239/266 [05:38<00:43,  1.63s/it]Loading train:  90%|█████████ | 240/266 [05:40<00:43,  1.65s/it]Loading train:  91%|█████████ | 241/266 [05:42<00:44,  1.77s/it]Loading train:  91%|█████████ | 242/266 [05:44<00:46,  1.95s/it]Loading train:  91%|█████████▏| 243/266 [05:46<00:46,  2.03s/it]Loading train:  92%|█████████▏| 244/266 [05:48<00:39,  1.79s/it]Loading train:  92%|█████████▏| 245/266 [05:49<00:38,  1.83s/it]Loading train:  92%|█████████▏| 246/266 [05:51<00:35,  1.79s/it]Loading train:  93%|█████████▎| 247/266 [05:53<00:32,  1.69s/it]Loading train:  93%|█████████▎| 248/266 [05:54<00:28,  1.58s/it]Loading train:  94%|█████████▎| 249/266 [05:56<00:27,  1.60s/it]Loading train:  94%|█████████▍| 250/266 [05:57<00:22,  1.41s/it]Loading train:  94%|█████████▍| 251/266 [05:58<00:22,  1.48s/it]Loading train:  95%|█████████▍| 252/266 [06:00<00:21,  1.54s/it]Loading train:  95%|█████████▌| 253/266 [06:01<00:20,  1.55s/it]Loading train:  95%|█████████▌| 254/266 [06:04<00:20,  1.71s/it]Loading train:  96%|█████████▌| 255/266 [06:06<00:20,  1.83s/it]Loading train:  96%|█████████▌| 256/266 [06:07<00:16,  1.70s/it]Loading train:  97%|█████████▋| 257/266 [06:08<00:14,  1.60s/it]Loading train:  97%|█████████▋| 258/266 [06:10<00:12,  1.58s/it]Loading train:  97%|█████████▋| 259/266 [06:11<00:10,  1.51s/it]Loading train:  98%|█████████▊| 260/266 [06:13<00:08,  1.47s/it]Loading train:  98%|█████████▊| 261/266 [06:13<00:06,  1.26s/it]Loading train:  98%|█████████▊| 262/266 [06:14<00:04,  1.10s/it]Loading train:  99%|█████████▉| 263/266 [06:15<00:03,  1.08s/it]Loading train:  99%|█████████▉| 264/266 [06:17<00:02,  1.19s/it]Loading train: 100%|█████████▉| 265/266 [06:18<00:01,  1.35s/it]Loading train: 100%|██████████| 266/266 [06:20<00:00,  1.37s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   0%|          | 1/266 [00:00<00:36,  7.20it/s]concatenating: train:   1%|          | 2/266 [00:00<00:39,  6.72it/s]concatenating: train:   1%|          | 3/266 [00:00<00:46,  5.60it/s]concatenating: train:   2%|▏         | 4/266 [00:00<00:51,  5.11it/s]concatenating: train:   2%|▏         | 5/266 [00:01<00:52,  5.01it/s]concatenating: train:   2%|▏         | 6/266 [00:01<00:46,  5.62it/s]concatenating: train:   3%|▎         | 7/266 [00:01<00:44,  5.79it/s]concatenating: train:   3%|▎         | 8/266 [00:01<00:40,  6.33it/s]concatenating: train:   3%|▎         | 9/266 [00:01<00:39,  6.45it/s]concatenating: train:   4%|▍         | 10/266 [00:01<00:40,  6.29it/s]concatenating: train:   4%|▍         | 11/266 [00:01<00:38,  6.58it/s]concatenating: train:   5%|▍         | 13/266 [00:02<00:34,  7.27it/s]concatenating: train:   5%|▌         | 14/266 [00:02<00:39,  6.40it/s]concatenating: train:   6%|▌         | 16/266 [00:02<00:32,  7.69it/s]concatenating: train:   6%|▋         | 17/266 [00:02<00:31,  7.99it/s]concatenating: train:   7%|▋         | 18/266 [00:02<00:32,  7.70it/s]concatenating: train:   7%|▋         | 19/266 [00:02<00:31,  7.73it/s]concatenating: train:   8%|▊         | 20/266 [00:02<00:29,  8.22it/s]concatenating: train:   8%|▊         | 21/266 [00:03<00:35,  6.94it/s]concatenating: train:   8%|▊         | 22/266 [00:03<00:38,  6.31it/s]concatenating: train:   9%|▊         | 23/266 [00:03<00:38,  6.39it/s]concatenating: train:   9%|▉         | 24/266 [00:03<00:35,  6.81it/s]concatenating: train:  10%|▉         | 26/266 [00:03<00:28,  8.45it/s]concatenating: train:  11%|█         | 29/266 [00:03<00:22, 10.45it/s]concatenating: train:  14%|█▍        | 38/266 [00:03<00:16, 14.20it/s]concatenating: train:  16%|█▌        | 42/266 [00:04<00:15, 14.48it/s]concatenating: train:  17%|█▋        | 45/266 [00:04<00:19, 11.17it/s]concatenating: train:  18%|█▊        | 48/266 [00:04<00:22,  9.63it/s]concatenating: train:  19%|█▉        | 50/266 [00:05<00:23,  9.25it/s]concatenating: train:  20%|█▉        | 52/266 [00:05<00:21,  9.85it/s]concatenating: train:  20%|██        | 54/266 [00:05<00:18, 11.23it/s]concatenating: train:  21%|██        | 56/266 [00:05<00:17, 12.08it/s]concatenating: train:  22%|██▏       | 59/266 [00:05<00:14, 14.46it/s]concatenating: train:  23%|██▎       | 61/266 [00:05<00:14, 13.94it/s]concatenating: train:  24%|██▎       | 63/266 [00:06<00:21,  9.33it/s]concatenating: train:  24%|██▍       | 65/266 [00:06<00:29,  6.86it/s]concatenating: train:  25%|██▌       | 67/266 [00:07<00:30,  6.53it/s]concatenating: train:  26%|██▌       | 69/266 [00:07<00:24,  7.98it/s]concatenating: train:  27%|██▋       | 71/266 [00:07<00:22,  8.77it/s]concatenating: train:  28%|██▊       | 74/266 [00:07<00:18, 10.14it/s]concatenating: train:  29%|██▊       | 76/266 [00:08<00:24,  7.64it/s]concatenating: train:  29%|██▉       | 78/266 [00:08<00:26,  7.01it/s]concatenating: train:  30%|██▉       | 79/266 [00:08<00:26,  7.05it/s]concatenating: train:  30%|███       | 80/266 [00:08<00:25,  7.26it/s]concatenating: train:  31%|███       | 82/266 [00:08<00:20,  8.95it/s]concatenating: train:  48%|████▊     | 127/266 [00:08<00:10, 12.67it/s]concatenating: train:  65%|██████▌   | 173/266 [00:08<00:05, 17.89it/s]concatenating: train:  74%|███████▍  | 197/266 [00:09<00:03, 20.30it/s]concatenating: train:  81%|████████  | 215/266 [00:11<00:03, 14.56it/s]concatenating: train:  86%|████████▌ | 228/266 [00:13<00:03, 12.29it/s]concatenating: train:  89%|████████▉ | 237/266 [00:14<00:02, 10.79it/s]concatenating: train:  92%|█████████▏| 244/266 [00:15<00:02,  9.46it/s]concatenating: train:  94%|█████████▎| 249/266 [00:16<00:02,  8.36it/s]concatenating: train:  95%|█████████▌| 253/266 [00:16<00:01,  7.90it/s]concatenating: train:  96%|█████████▌| 256/266 [00:17<00:01,  7.49it/s]concatenating: train:  97%|█████████▋| 259/266 [00:17<00:00,  8.10it/s]concatenating: train:  99%|█████████▉| 263/266 [00:17<00:00, 10.45it/s]concatenating: train: 100%|██████████| 266/266 [00:17<00:00, 10.99it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:01<00:05,  1.25s/it]Loading test:  40%|████      | 2/5 [00:02<00:03,  1.18s/it]Loading test:  60%|██████    | 3/5 [00:03<00:02,  1.15s/it]Loading test:  80%|████████  | 4/5 [00:04<00:01,  1.22s/it]Loading test: 100%|██████████| 5/5 [00:05<00:00,  1.20s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation:  40%|████      | 2/5 [00:00<00:00, 13.55it/s]concatenating: validation:  80%|████████  | 4/5 [00:00<00:00, 12.52it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 11.69it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:01<07:13,  1.64s/it]Loading trainS:   1%|          | 2/266 [00:03<07:06,  1.62s/it]Loading trainS:   1%|          | 3/266 [00:04<06:42,  1.53s/it]Loading trainS:   2%|▏         | 4/266 [00:05<05:30,  1.26s/it]Loading trainS:   2%|▏         | 5/266 [00:06<05:19,  1.22s/it]Loading trainS:   2%|▏         | 6/266 [00:07<05:46,  1.33s/it]Loading trainS:   3%|▎         | 7/266 [00:09<06:00,  1.39s/it]Loading trainS:   3%|▎         | 8/266 [00:10<06:08,  1.43s/it]Loading trainS:   3%|▎         | 9/266 [00:12<06:11,  1.45s/it]Loading trainS:   4%|▍         | 10/266 [00:14<06:35,  1.55s/it]Loading trainS:   4%|▍         | 11/266 [00:15<06:36,  1.55s/it]Loading trainS:   5%|▍         | 12/266 [00:17<06:10,  1.46s/it]Loading trainS:   5%|▍         | 13/266 [00:18<06:19,  1.50s/it]Loading trainS:   5%|▌         | 14/266 [00:20<06:33,  1.56s/it]Loading trainS:   6%|▌         | 15/266 [00:21<06:35,  1.57s/it]Loading trainS:   6%|▌         | 16/266 [00:23<06:44,  1.62s/it]Loading trainS:   6%|▋         | 17/266 [00:24<06:11,  1.49s/it]Loading trainS:   7%|▋         | 18/266 [00:26<06:15,  1.51s/it]Loading trainS:   7%|▋         | 19/266 [00:28<06:26,  1.56s/it]Loading trainS:   8%|▊         | 20/266 [00:29<06:05,  1.49s/it]Loading trainS:   8%|▊         | 21/266 [00:30<06:05,  1.49s/it]Loading trainS:   8%|▊         | 22/266 [00:32<06:18,  1.55s/it]Loading trainS:   9%|▊         | 23/266 [00:34<06:44,  1.66s/it]Loading trainS:   9%|▉         | 24/266 [00:36<07:08,  1.77s/it]Loading trainS:   9%|▉         | 25/266 [00:37<06:27,  1.61s/it]Loading trainS:  10%|▉         | 26/266 [00:39<06:20,  1.58s/it]Loading trainS:  10%|█         | 27/266 [00:40<06:08,  1.54s/it]Loading trainS:  11%|█         | 28/266 [00:42<06:06,  1.54s/it]Loading trainS:  11%|█         | 29/266 [00:43<06:13,  1.58s/it]Loading trainS:  11%|█▏        | 30/266 [00:45<06:10,  1.57s/it]Loading trainS:  12%|█▏        | 31/266 [00:47<06:17,  1.61s/it]Loading trainS:  12%|█▏        | 32/266 [00:49<06:32,  1.68s/it]Loading trainS:  12%|█▏        | 33/266 [00:50<05:49,  1.50s/it]Loading trainS:  13%|█▎        | 34/266 [00:51<06:02,  1.56s/it]Loading trainS:  13%|█▎        | 35/266 [00:52<05:19,  1.38s/it]Loading trainS:  14%|█▎        | 36/266 [00:54<05:11,  1.35s/it]Loading trainS:  14%|█▍        | 37/266 [00:55<05:11,  1.36s/it]Loading trainS:  14%|█▍        | 38/266 [00:56<05:18,  1.40s/it]Loading trainS:  15%|█▍        | 39/266 [00:58<05:24,  1.43s/it]Loading trainS:  15%|█▌        | 40/266 [00:59<05:17,  1.41s/it]Loading trainS:  15%|█▌        | 41/266 [01:00<04:50,  1.29s/it]Loading trainS:  16%|█▌        | 42/266 [01:02<05:04,  1.36s/it]Loading trainS:  16%|█▌        | 43/266 [01:03<04:54,  1.32s/it]Loading trainS:  17%|█▋        | 44/266 [01:04<04:52,  1.32s/it]Loading trainS:  17%|█▋        | 45/266 [01:06<04:51,  1.32s/it]Loading trainS:  17%|█▋        | 46/266 [01:07<04:40,  1.28s/it]Loading trainS:  18%|█▊        | 47/266 [01:08<04:26,  1.22s/it]Loading trainS:  18%|█▊        | 48/266 [01:09<04:22,  1.20s/it]Loading trainS:  18%|█▊        | 49/266 [01:10<04:23,  1.22s/it]Loading trainS:  19%|█▉        | 50/266 [01:12<04:30,  1.25s/it]Loading trainS:  19%|█▉        | 51/266 [01:13<04:27,  1.25s/it]Loading trainS:  20%|█▉        | 52/266 [01:14<04:30,  1.26s/it]Loading trainS:  20%|█▉        | 53/266 [01:15<04:22,  1.23s/it]Loading trainS:  20%|██        | 54/266 [01:17<04:25,  1.25s/it]Loading trainS:  21%|██        | 55/266 [01:18<04:28,  1.27s/it]Loading trainS:  21%|██        | 56/266 [01:19<04:25,  1.27s/it]Loading trainS:  21%|██▏       | 57/266 [01:21<04:24,  1.27s/it]Loading trainS:  22%|██▏       | 58/266 [01:22<04:28,  1.29s/it]Loading trainS:  22%|██▏       | 59/266 [01:23<04:35,  1.33s/it]Loading trainS:  23%|██▎       | 60/266 [01:25<04:32,  1.32s/it]Loading trainS:  23%|██▎       | 61/266 [01:26<04:21,  1.27s/it]Loading trainS:  23%|██▎       | 62/266 [01:27<04:19,  1.27s/it]Loading trainS:  24%|██▎       | 63/266 [01:28<04:04,  1.21s/it]Loading trainS:  24%|██▍       | 64/266 [01:29<04:02,  1.20s/it]Loading trainS:  24%|██▍       | 65/266 [01:30<03:58,  1.19s/it]Loading trainS:  25%|██▍       | 66/266 [01:32<04:04,  1.22s/it]Loading trainS:  25%|██▌       | 67/266 [01:33<04:26,  1.34s/it]Loading trainS:  26%|██▌       | 68/266 [01:34<04:10,  1.26s/it]Loading trainS:  26%|██▌       | 69/266 [01:36<03:58,  1.21s/it]Loading trainS:  26%|██▋       | 70/266 [01:37<04:02,  1.24s/it]Loading trainS:  27%|██▋       | 71/266 [01:38<04:17,  1.32s/it]Loading trainS:  27%|██▋       | 72/266 [01:40<04:18,  1.33s/it]Loading trainS:  27%|██▋       | 73/266 [01:41<04:15,  1.33s/it]Loading trainS:  28%|██▊       | 74/266 [01:42<04:18,  1.35s/it]Loading trainS:  28%|██▊       | 75/266 [01:44<04:23,  1.38s/it]Loading trainS:  29%|██▊       | 76/266 [01:45<04:13,  1.33s/it]Loading trainS:  29%|██▉       | 77/266 [01:46<04:00,  1.27s/it]Loading trainS:  29%|██▉       | 78/266 [01:47<03:29,  1.11s/it]Loading trainS:  30%|██▉       | 79/266 [01:48<03:30,  1.13s/it]Loading trainS:  30%|███       | 80/266 [01:49<03:27,  1.12s/it]Loading trainS:  30%|███       | 81/266 [01:50<03:26,  1.12s/it]Loading trainS:  31%|███       | 82/266 [01:51<03:21,  1.10s/it]Loading trainS:  31%|███       | 83/266 [01:52<03:22,  1.11s/it]Loading trainS:  32%|███▏      | 84/266 [01:53<03:00,  1.01it/s]Loading trainS:  32%|███▏      | 85/266 [01:54<03:00,  1.00it/s]Loading trainS:  32%|███▏      | 86/266 [01:55<03:03,  1.02s/it]Loading trainS:  33%|███▎      | 87/266 [01:56<03:06,  1.04s/it]Loading trainS:  33%|███▎      | 88/266 [01:57<02:55,  1.01it/s]Loading trainS:  33%|███▎      | 89/266 [01:58<02:59,  1.01s/it]Loading trainS:  34%|███▍      | 90/266 [01:59<03:00,  1.03s/it]Loading trainS:  34%|███▍      | 91/266 [02:01<03:20,  1.15s/it]Loading trainS:  35%|███▍      | 92/266 [02:02<03:19,  1.15s/it]Loading trainS:  35%|███▍      | 93/266 [02:03<03:29,  1.21s/it]Loading trainS:  35%|███▌      | 94/266 [02:04<03:13,  1.12s/it]Loading trainS:  36%|███▌      | 95/266 [02:05<03:09,  1.11s/it]Loading trainS:  36%|███▌      | 96/266 [02:07<03:14,  1.15s/it]Loading trainS:  36%|███▋      | 97/266 [02:08<03:20,  1.18s/it]Loading trainS:  37%|███▋      | 98/266 [02:09<03:00,  1.08s/it]Loading trainS:  37%|███▋      | 99/266 [02:10<03:13,  1.16s/it]Loading trainS:  38%|███▊      | 100/266 [02:11<03:22,  1.22s/it]Loading trainS:  38%|███▊      | 101/266 [02:13<03:17,  1.20s/it]Loading trainS:  38%|███▊      | 102/266 [02:14<03:26,  1.26s/it]Loading trainS:  39%|███▊      | 103/266 [02:15<03:18,  1.22s/it]Loading trainS:  39%|███▉      | 104/266 [02:16<03:21,  1.25s/it]Loading trainS:  39%|███▉      | 105/266 [02:18<03:34,  1.33s/it]Loading trainS:  40%|███▉      | 106/266 [02:19<03:18,  1.24s/it]Loading trainS:  40%|████      | 107/266 [02:20<03:18,  1.25s/it]Loading trainS:  41%|████      | 108/266 [02:21<03:18,  1.26s/it]Loading trainS:  41%|████      | 109/266 [02:23<03:20,  1.28s/it]Loading trainS:  41%|████▏     | 110/266 [02:24<03:25,  1.32s/it]Loading trainS:  42%|████▏     | 111/266 [02:26<03:25,  1.33s/it]Loading trainS:  42%|████▏     | 112/266 [02:26<03:05,  1.20s/it]Loading trainS:  42%|████▏     | 113/266 [02:28<03:02,  1.19s/it]Loading trainS:  43%|████▎     | 114/266 [02:29<03:07,  1.23s/it]Loading trainS:  43%|████▎     | 115/266 [02:30<03:04,  1.22s/it]Loading trainS:  44%|████▎     | 116/266 [02:31<02:53,  1.16s/it]Loading trainS:  44%|████▍     | 117/266 [02:33<03:12,  1.29s/it]Loading trainS:  44%|████▍     | 118/266 [02:34<03:14,  1.31s/it]Loading trainS:  45%|████▍     | 119/266 [02:35<03:13,  1.31s/it]Loading trainS:  45%|████▌     | 120/266 [02:37<03:19,  1.37s/it]Loading trainS:  45%|████▌     | 121/266 [02:38<03:12,  1.33s/it]Loading trainS:  46%|████▌     | 122/266 [02:40<03:15,  1.36s/it]Loading trainS:  46%|████▌     | 123/266 [02:41<03:26,  1.44s/it]Loading trainS:  47%|████▋     | 124/266 [02:43<03:28,  1.47s/it]Loading trainS:  47%|████▋     | 125/266 [02:44<03:29,  1.49s/it]Loading trainS:  47%|████▋     | 126/266 [02:46<03:25,  1.47s/it]Loading trainS:  48%|████▊     | 127/266 [02:47<03:20,  1.44s/it]Loading trainS:  48%|████▊     | 128/266 [02:49<03:25,  1.49s/it]Loading trainS:  48%|████▊     | 129/266 [02:50<03:16,  1.43s/it]Loading trainS:  49%|████▉     | 130/266 [02:52<03:24,  1.50s/it]Loading trainS:  49%|████▉     | 131/266 [02:53<03:17,  1.46s/it]Loading trainS:  50%|████▉     | 132/266 [02:54<03:04,  1.38s/it]Loading trainS:  50%|█████     | 133/266 [02:55<02:55,  1.32s/it]Loading trainS:  50%|█████     | 134/266 [02:57<02:54,  1.32s/it]Loading trainS:  51%|█████     | 135/266 [02:58<02:58,  1.36s/it]Loading trainS:  51%|█████     | 136/266 [03:00<03:03,  1.41s/it]Loading trainS:  52%|█████▏    | 137/266 [03:01<03:06,  1.45s/it]Loading trainS:  52%|█████▏    | 138/266 [03:03<03:15,  1.53s/it]Loading trainS:  52%|█████▏    | 139/266 [03:04<03:14,  1.53s/it]Loading trainS:  53%|█████▎    | 140/266 [03:06<03:12,  1.53s/it]Loading trainS:  53%|█████▎    | 141/266 [03:07<02:57,  1.42s/it]Loading trainS:  53%|█████▎    | 142/266 [03:09<02:54,  1.40s/it]Loading trainS:  54%|█████▍    | 143/266 [03:10<02:48,  1.37s/it]Loading trainS:  54%|█████▍    | 144/266 [03:11<02:42,  1.33s/it]Loading trainS:  55%|█████▍    | 145/266 [03:12<02:41,  1.33s/it]Loading trainS:  55%|█████▍    | 146/266 [03:14<02:53,  1.44s/it]Loading trainS:  55%|█████▌    | 147/266 [03:16<02:52,  1.45s/it]Loading trainS:  56%|█████▌    | 148/266 [03:17<02:45,  1.40s/it]Loading trainS:  56%|█████▌    | 149/266 [03:19<02:56,  1.51s/it]Loading trainS:  56%|█████▋    | 150/266 [03:20<02:55,  1.51s/it]Loading trainS:  57%|█████▋    | 151/266 [03:21<02:46,  1.45s/it]Loading trainS:  57%|█████▋    | 152/266 [03:22<02:18,  1.22s/it]Loading trainS:  58%|█████▊    | 153/266 [03:23<02:17,  1.21s/it]Loading trainS:  58%|█████▊    | 154/266 [03:25<02:16,  1.22s/it]Loading trainS:  58%|█████▊    | 155/266 [03:26<02:10,  1.17s/it]Loading trainS:  59%|█████▊    | 156/266 [03:27<01:59,  1.09s/it]Loading trainS:  59%|█████▉    | 157/266 [03:28<01:57,  1.08s/it]Loading trainS:  59%|█████▉    | 158/266 [03:29<02:02,  1.13s/it]Loading trainS:  60%|█████▉    | 159/266 [03:30<02:07,  1.19s/it]Loading trainS:  60%|██████    | 160/266 [03:32<02:11,  1.24s/it]Loading trainS:  61%|██████    | 161/266 [03:33<02:11,  1.25s/it]Loading trainS:  61%|██████    | 162/266 [03:34<02:04,  1.19s/it]Loading trainS:  61%|██████▏   | 163/266 [03:35<02:04,  1.21s/it]Loading trainS:  62%|██████▏   | 164/266 [03:37<02:11,  1.29s/it]Loading trainS:  62%|██████▏   | 165/266 [03:38<02:11,  1.30s/it]Loading trainS:  62%|██████▏   | 166/266 [03:39<02:02,  1.22s/it]Loading trainS:  63%|██████▎   | 167/266 [03:40<01:57,  1.19s/it]Loading trainS:  63%|██████▎   | 168/266 [03:41<01:57,  1.20s/it]Loading trainS:  64%|██████▎   | 169/266 [03:43<01:57,  1.21s/it]Loading trainS:  64%|██████▍   | 170/266 [03:44<01:52,  1.18s/it]Loading trainS:  64%|██████▍   | 171/266 [03:45<01:59,  1.26s/it]Loading trainS:  65%|██████▍   | 172/266 [03:46<01:52,  1.20s/it]Loading trainS:  65%|██████▌   | 173/266 [03:48<01:57,  1.26s/it]Loading trainS:  65%|██████▌   | 174/266 [03:48<01:46,  1.16s/it]Loading trainS:  66%|██████▌   | 175/266 [03:50<01:45,  1.16s/it]Loading trainS:  66%|██████▌   | 176/266 [03:51<01:49,  1.21s/it]Loading trainS:  67%|██████▋   | 177/266 [03:52<01:49,  1.24s/it]Loading trainS:  67%|██████▋   | 178/266 [03:54<01:54,  1.30s/it]Loading trainS:  67%|██████▋   | 179/266 [03:55<01:54,  1.32s/it]Loading trainS:  68%|██████▊   | 180/266 [03:56<01:52,  1.31s/it]Loading trainS:  68%|██████▊   | 181/266 [03:58<01:51,  1.32s/it]Loading trainS:  68%|██████▊   | 182/266 [03:59<01:54,  1.36s/it]Loading trainS:  69%|██████▉   | 183/266 [04:01<01:54,  1.39s/it]Loading trainS:  69%|██████▉   | 184/266 [04:02<01:43,  1.26s/it]Loading trainS:  70%|██████▉   | 185/266 [04:03<01:51,  1.37s/it]Loading trainS:  70%|██████▉   | 186/266 [04:04<01:41,  1.27s/it]Loading trainS:  70%|███████   | 187/266 [04:05<01:39,  1.26s/it]Loading trainS:  71%|███████   | 188/266 [04:07<01:37,  1.25s/it]Loading trainS:  71%|███████   | 189/266 [04:08<01:41,  1.32s/it]Loading trainS:  71%|███████▏  | 190/266 [04:09<01:39,  1.30s/it]Loading trainS:  72%|███████▏  | 191/266 [04:11<01:38,  1.32s/it]Loading trainS:  72%|███████▏  | 192/266 [04:12<01:36,  1.30s/it]Loading trainS:  73%|███████▎  | 193/266 [04:13<01:34,  1.29s/it]Loading trainS:  73%|███████▎  | 194/266 [04:15<01:36,  1.34s/it]Loading trainS:  73%|███████▎  | 195/266 [04:16<01:34,  1.33s/it]Loading trainS:  74%|███████▎  | 196/266 [04:18<01:38,  1.41s/it]Loading trainS:  74%|███████▍  | 197/266 [04:19<01:42,  1.48s/it]Loading trainS:  74%|███████▍  | 198/266 [04:20<01:35,  1.40s/it]Loading trainS:  75%|███████▍  | 199/266 [04:22<01:27,  1.31s/it]Loading trainS:  75%|███████▌  | 200/266 [04:23<01:30,  1.37s/it]Loading trainS:  76%|███████▌  | 201/266 [04:25<01:29,  1.38s/it]Loading trainS:  76%|███████▌  | 202/266 [04:26<01:27,  1.37s/it]Loading trainS:  76%|███████▋  | 203/266 [04:27<01:18,  1.25s/it]Loading trainS:  77%|███████▋  | 204/266 [04:28<01:19,  1.28s/it]Loading trainS:  77%|███████▋  | 205/266 [04:30<01:22,  1.35s/it]Loading trainS:  77%|███████▋  | 206/266 [04:31<01:23,  1.39s/it]Loading trainS:  78%|███████▊  | 207/266 [04:32<01:17,  1.31s/it]Loading trainS:  78%|███████▊  | 208/266 [04:34<01:17,  1.34s/it]Loading trainS:  79%|███████▊  | 209/266 [04:35<01:17,  1.35s/it]Loading trainS:  79%|███████▉  | 210/266 [04:36<01:13,  1.32s/it]Loading trainS:  79%|███████▉  | 211/266 [04:38<01:13,  1.34s/it]Loading trainS:  80%|███████▉  | 212/266 [04:38<00:59,  1.11s/it]Loading trainS:  80%|████████  | 213/266 [04:40<01:03,  1.20s/it]Loading trainS:  80%|████████  | 214/266 [04:41<01:05,  1.25s/it]Loading trainS:  81%|████████  | 215/266 [04:43<01:09,  1.37s/it]Loading trainS:  81%|████████  | 216/266 [04:44<01:04,  1.30s/it]Loading trainS:  82%|████████▏ | 217/266 [04:45<01:02,  1.27s/it]Loading trainS:  82%|████████▏ | 218/266 [04:46<01:01,  1.27s/it]Loading trainS:  82%|████████▏ | 219/266 [04:48<01:01,  1.30s/it]Loading trainS:  83%|████████▎ | 220/266 [04:49<01:02,  1.36s/it]Loading trainS:  83%|████████▎ | 221/266 [04:50<00:58,  1.30s/it]Loading trainS:  83%|████████▎ | 222/266 [04:52<00:56,  1.29s/it]Loading trainS:  84%|████████▍ | 223/266 [04:53<00:54,  1.27s/it]Loading trainS:  84%|████████▍ | 224/266 [04:54<00:52,  1.24s/it]Loading trainS:  85%|████████▍ | 225/266 [04:55<00:47,  1.16s/it]Loading trainS:  85%|████████▍ | 226/266 [04:56<00:44,  1.11s/it]Loading trainS:  85%|████████▌ | 227/266 [04:57<00:45,  1.16s/it]Loading trainS:  86%|████████▌ | 228/266 [04:58<00:39,  1.03s/it]Loading trainS:  86%|████████▌ | 229/266 [04:59<00:38,  1.05s/it]Loading trainS:  86%|████████▋ | 230/266 [05:00<00:36,  1.01s/it]Loading trainS:  87%|████████▋ | 231/266 [05:01<00:32,  1.07it/s]Loading trainS:  87%|████████▋ | 232/266 [05:02<00:33,  1.03it/s]Loading trainS:  88%|████████▊ | 233/266 [05:03<00:32,  1.02it/s]Loading trainS:  88%|████████▊ | 234/266 [05:04<00:33,  1.04s/it]Loading trainS:  88%|████████▊ | 235/266 [05:05<00:28,  1.07it/s]Loading trainS:  89%|████████▊ | 236/266 [05:06<00:28,  1.05it/s]Loading trainS:  89%|████████▉ | 237/266 [05:07<00:27,  1.05it/s]Loading trainS:  89%|████████▉ | 238/266 [05:08<00:27,  1.03it/s]Loading trainS:  90%|████████▉ | 239/266 [05:08<00:25,  1.08it/s]Loading trainS:  90%|█████████ | 240/266 [05:09<00:24,  1.04it/s]Loading trainS:  91%|█████████ | 241/266 [05:10<00:22,  1.09it/s]Loading trainS:  91%|█████████ | 242/266 [05:11<00:19,  1.23it/s]Loading trainS:  91%|█████████▏| 243/266 [05:11<00:16,  1.37it/s]Loading trainS:  92%|█████████▏| 244/266 [05:12<00:13,  1.58it/s]Loading trainS:  92%|█████████▏| 245/266 [05:12<00:12,  1.62it/s]Loading trainS:  92%|█████████▏| 246/266 [05:13<00:11,  1.75it/s]Loading trainS:  93%|█████████▎| 247/266 [05:13<00:10,  1.85it/s]Loading trainS:  93%|█████████▎| 248/266 [05:14<00:09,  1.89it/s]Loading trainS:  94%|█████████▎| 249/266 [05:14<00:08,  1.90it/s]Loading trainS:  94%|█████████▍| 250/266 [05:15<00:08,  1.91it/s]Loading trainS:  94%|█████████▍| 251/266 [05:15<00:07,  1.97it/s]Loading trainS:  95%|█████████▍| 252/266 [05:16<00:07,  1.97it/s]Loading trainS:  95%|█████████▌| 253/266 [05:16<00:05,  2.20it/s]Loading trainS:  95%|█████████▌| 254/266 [05:17<00:05,  2.37it/s]Loading trainS:  96%|█████████▌| 255/266 [05:17<00:04,  2.47it/s]Loading trainS:  96%|█████████▌| 256/266 [05:17<00:03,  2.59it/s]Loading trainS:  97%|█████████▋| 257/266 [05:18<00:03,  2.63it/s]Loading trainS:  97%|█████████▋| 258/266 [05:18<00:02,  2.73it/s]Loading trainS:  97%|█████████▋| 259/266 [05:18<00:02,  2.83it/s]Loading trainS:  98%|█████████▊| 260/266 [05:19<00:02,  2.82it/s]Loading trainS:  98%|█████████▊| 261/266 [05:19<00:01,  2.89it/s]Loading trainS:  98%|█████████▊| 262/266 [05:19<00:01,  2.77it/s]Loading trainS:  99%|█████████▉| 263/266 [05:20<00:01,  2.73it/s]Loading trainS:  99%|█████████▉| 264/266 [05:20<00:00,  2.81it/s]Loading trainS: 100%|█████████▉| 265/266 [05:20<00:00,  2.89it/s]Loading trainS: 100%|██████████| 266/266 [05:21<00:00,  2.95it/s]
Loading testS:   0%|          | 0/5 [00:00<?, ?it/s]Loading testS:  20%|██        | 1/5 [00:00<00:01,  2.76it/s]Loading testS:  40%|████      | 2/5 [00:00<00:01,  2.79it/s]Loading testS:  60%|██████    | 3/5 [00:01<00:00,  2.89it/s]Loading testS:  80%|████████  | 4/5 [00:01<00:00,  2.81it/s]Loading testS: 100%|██████████| 5/5 [00:01<00:00,  2.82it/s]
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:07,  5.57it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:06,  6.68it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:06,  6.49it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:04,  8.34it/s]loading the weights for Res Unet:  23%|██▎       | 10/44 [00:00<00:04,  7.90it/s]loading the weights for Res Unet:  25%|██▌       | 11/44 [00:01<00:04,  7.37it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:04,  6.67it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:03,  8.68it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02,  9.16it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:02<00:03,  7.52it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:02,  9.33it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01,  9.43it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01,  9.70it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:03<00:01,  6.83it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:03<00:01,  8.61it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:03<00:00,  9.21it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00,  9.51it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:04<00:00,  7.59it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:04<00:00, 10.85it/s]---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 2  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 2  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 101,882
Non-trainable params: 121,280
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 320 samples
Epoch 1/300
 - 33s - loss: 0.6132 - acc: 0.9522 - mDice: 0.3964 - val_loss: 0.8975 - val_acc: 0.9803 - val_mDice: 0.6576

Epoch 00001: val_mDice improved from -inf to 0.65762, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 29s - loss: 0.1753 - acc: 0.9789 - mDice: 0.7191 - val_loss: 0.8688 - val_acc: 0.9882 - val_mDice: 0.6903

Epoch 00002: val_mDice improved from 0.65762 to 0.69034, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 28s - loss: 0.1358 - acc: 0.9823 - mDice: 0.7757 - val_loss: 0.8636 - val_acc: 0.9895 - val_mDice: 0.7041

Epoch 00003: val_mDice improved from 0.69034 to 0.70407, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 28s - loss: 0.1192 - acc: 0.9853 - mDice: 0.8008 - val_loss: 0.8470 - val_acc: 0.9894 - val_mDice: 0.7147

Epoch 00004: val_mDice improved from 0.70407 to 0.71475, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 28s - loss: 0.1063 - acc: 0.9887 - mDice: 0.8177 - val_loss: 0.8107 - val_acc: 0.9899 - val_mDice: 0.7400

Epoch 00005: val_mDice improved from 0.71475 to 0.74001, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 28s - loss: 0.0972 - acc: 0.9901 - mDice: 0.8291 - val_loss: 0.8140 - val_acc: 0.9897 - val_mDice: 0.7426

Epoch 00006: val_mDice improved from 0.74001 to 0.74265, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 28s - loss: 0.0925 - acc: 0.9905 - mDice: 0.8364 - val_loss: 0.7469 - val_acc: 0.9910 - val_mDice: 0.7585

Epoch 00007: val_mDice improved from 0.74265 to 0.75848, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 28s - loss: 0.0880 - acc: 0.9909 - mDice: 0.8435 - val_loss: 0.7350 - val_acc: 0.9909 - val_mDice: 0.7536

Epoch 00008: val_mDice did not improve from 0.75848
Epoch 9/300
 - 29s - loss: 0.0846 - acc: 0.9912 - mDice: 0.8489 - val_loss: 0.7637 - val_acc: 0.9913 - val_mDice: 0.7738

Epoch 00009: val_mDice improved from 0.75848 to 0.77381, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 29s - loss: 0.0813 - acc: 0.9914 - mDice: 0.8544 - val_loss: 0.7394 - val_acc: 0.9915 - val_mDice: 0.7723

Epoch 00010: val_mDice did not improve from 0.77381
Epoch 11/300
 - 29s - loss: 0.0788 - acc: 0.9917 - mDice: 0.8585 - val_loss: 0.6588 - val_acc: 0.9915 - val_mDice: 0.7688

Epoch 00011: val_mDice did not improve from 0.77381
Epoch 12/300
 - 29s - loss: 0.0758 - acc: 0.9919 - mDice: 0.8634 - val_loss: 0.7503 - val_acc: 0.9915 - val_mDice: 0.7725

Epoch 00012: val_mDice did not improve from 0.77381
Epoch 13/300
 - 29s - loss: 0.0745 - acc: 0.9920 - mDice: 0.8656 - val_loss: 0.7448 - val_acc: 0.9913 - val_mDice: 0.7712

Epoch 00013: val_mDice did not improve from 0.77381
Epoch 14/300
 - 29s - loss: 0.0727 - acc: 0.9921 - mDice: 0.8686 - val_loss: 0.7343 - val_acc: 0.9918 - val_mDice: 0.7788

Epoch 00014: val_mDice improved from 0.77381 to 0.77881, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 29s - loss: 0.0709 - acc: 0.9923 - mDice: 0.8717 - val_loss: 0.7314 - val_acc: 0.9917 - val_mDice: 0.7756

Epoch 00015: val_mDice did not improve from 0.77881
Epoch 16/300
 - 29s - loss: 0.0692 - acc: 0.9924 - mDice: 0.8745 - val_loss: 0.7334 - val_acc: 0.9919 - val_mDice: 0.7847

Epoch 00016: val_mDice improved from 0.77881 to 0.78472, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 29s - loss: 0.0683 - acc: 0.9925 - mDice: 0.8760 - val_loss: 0.7467 - val_acc: 0.9916 - val_mDice: 0.7799

Epoch 00017: val_mDice did not improve from 0.78472
Epoch 18/300
 - 29s - loss: 0.0664 - acc: 0.9927 - mDice: 0.8793 - val_loss: 0.7451 - val_acc: 0.9917 - val_mDice: 0.7802

Epoch 00018: val_mDice did not improve from 0.78472
Epoch 19/300
 - 29s - loss: 0.0662 - acc: 0.9927 - mDice: 0.8796 - val_loss: 0.7497 - val_acc: 0.9919 - val_mDice: 0.7835

Epoch 00019: val_mDice did not improve from 0.78472
Epoch 20/300
 - 29s - loss: 0.0649 - acc: 0.9929 - mDice: 0.8819 - val_loss: 0.7010 - val_acc: 0.9923 - val_mDice: 0.7870

Epoch 00020: val_mDice improved from 0.78472 to 0.78695, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 29s - loss: 0.0637 - acc: 0.9930 - mDice: 0.8839 - val_loss: 0.6721 - val_acc: 0.9920 - val_mDice: 0.7837

Epoch 00021: val_mDice did not improve from 0.78695
Epoch 22/300
 - 29s - loss: 0.0624 - acc: 0.9931 - mDice: 0.8862 - val_loss: 0.7123 - val_acc: 0.9922 - val_mDice: 0.7846

Epoch 00022: val_mDice did not improve from 0.78695
Epoch 23/300
 - 29s - loss: 0.0624 - acc: 0.9931 - mDice: 0.8867 - val_loss: 0.6437 - val_acc: 0.9923 - val_mDice: 0.7873

Epoch 00023: val_mDice improved from 0.78695 to 0.78732, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 28s - loss: 0.0635 - acc: 0.9930 - mDice: 0.8841 - val_loss: 0.7173 - val_acc: 0.9920 - val_mDice: 0.7835

Epoch 00024: val_mDice did not improve from 0.78732
Epoch 25/300
 - 29s - loss: 0.0600 - acc: 0.9933 - mDice: 0.8903 - val_loss: 0.6999 - val_acc: 0.9923 - val_mDice: 0.7911

Epoch 00025: val_mDice improved from 0.78732 to 0.79113, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 29s - loss: 0.0593 - acc: 0.9934 - mDice: 0.8914 - val_loss: 0.6765 - val_acc: 0.9923 - val_mDice: 0.7893

Epoch 00026: val_mDice did not improve from 0.79113
Epoch 27/300
 - 29s - loss: 0.0588 - acc: 0.9934 - mDice: 0.8923 - val_loss: 0.6851 - val_acc: 0.9921 - val_mDice: 0.7847

Epoch 00027: val_mDice did not improve from 0.79113
Epoch 28/300
 - 29s - loss: 0.0586 - acc: 0.9935 - mDice: 0.8927 - val_loss: 0.7167 - val_acc: 0.9921 - val_mDice: 0.7867

Epoch 00028: val_mDice did not improve from 0.79113
Epoch 29/300
 - 29s - loss: 0.0577 - acc: 0.9935 - mDice: 0.8941 - val_loss: 0.6861 - val_acc: 0.9921 - val_mDice: 0.7871

Epoch 00029: val_mDice did not improve from 0.79113
Epoch 30/300
 - 29s - loss: 0.0568 - acc: 0.9936 - mDice: 0.8957 - val_loss: 0.7249 - val_acc: 0.9924 - val_mDice: 0.7942

Epoch 00030: val_mDice improved from 0.79113 to 0.79418, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 29s - loss: 0.0564 - acc: 0.9936 - mDice: 0.8965 - val_loss: 0.7177 - val_acc: 0.9922 - val_mDice: 0.7918

Epoch 00031: val_mDice did not improve from 0.79418
Epoch 32/300
 - 29s - loss: 0.0557 - acc: 0.9937 - mDice: 0.8976 - val_loss: 0.6685 - val_acc: 0.9923 - val_mDice: 0.7876

Epoch 00032: val_mDice did not improve from 0.79418
Epoch 33/300
 - 29s - loss: 0.0552 - acc: 0.9937 - mDice: 0.8986 - val_loss: 0.6715 - val_acc: 0.9922 - val_mDice: 0.7865

Epoch 00033: val_mDice did not improve from 0.79418
Epoch 34/300
 - 29s - loss: 0.0548 - acc: 0.9938 - mDice: 0.8992 - val_loss: 0.6742 - val_acc: 0.9923 - val_mDice: 0.7886

Epoch 00034: val_mDice did not improve from 0.79418
Epoch 35/300
 - 29s - loss: 0.0550 - acc: 0.9938 - mDice: 0.8989 - val_loss: 0.5309 - val_acc: 0.9927 - val_mDice: 0.7959

Epoch 00035: val_mDice improved from 0.79418 to 0.79592, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 28s - loss: 0.0542 - acc: 0.9938 - mDice: 0.9002 - val_loss: 0.7452 - val_acc: 0.9921 - val_mDice: 0.7908

Epoch 00036: val_mDice did not improve from 0.79592
Epoch 37/300
 - 29s - loss: 0.0537 - acc: 0.9939 - mDice: 0.9011 - val_loss: 0.6522 - val_acc: 0.9924 - val_mDice: 0.7949

Epoch 00037: val_mDice did not improve from 0.79592
Epoch 38/300
 - 29s - loss: 0.0532 - acc: 0.9939 - mDice: 0.9020 - val_loss: 0.6589 - val_acc: 0.9924 - val_mDice: 0.7940

Epoch 00038: val_mDice did not improve from 0.79592
Epoch 39/300
 - 29s - loss: 0.0527 - acc: 0.9940 - mDice: 0.9029 - val_loss: 0.6281 - val_acc: 0.9926 - val_mDice: 0.7936

Epoch 00039: val_mDice did not improve from 0.79592
Epoch 40/300
 - 29s - loss: 0.0524 - acc: 0.9940 - mDice: 0.9034 - val_loss: 0.6964 - val_acc: 0.9921 - val_mDice: 0.7893

Epoch 00040: val_mDice did not improve from 0.79592
Epoch 41/300
 - 29s - loss: 0.0519 - acc: 0.9940 - mDice: 0.9043 - val_loss: 0.6500 - val_acc: 0.9923 - val_mDice: 0.7944

Epoch 00041: val_mDice did not improve from 0.79592
Epoch 42/300
 - 29s - loss: 0.0518 - acc: 0.9941 - mDice: 0.9044 - val_loss: 0.6774 - val_acc: 0.9921 - val_mDice: 0.7895

Epoch 00042: val_mDice did not improve from 0.79592
Epoch 43/300
 - 29s - loss: 0.0517 - acc: 0.9941 - mDice: 0.9047 - val_loss: 0.7085 - val_acc: 0.9922 - val_mDice: 0.7907

Epoch 00043: val_mDice did not improve from 0.79592
Epoch 44/300
 - 29s - loss: 0.0512 - acc: 0.9941 - mDice: 0.9055 - val_loss: 0.1041 - val_acc: 0.9925 - val_mDice: 0.7953

Epoch 00044: val_mDice did not improve from 0.79592
Epoch 45/300
 - 28s - loss: 0.0509 - acc: 0.9941 - mDice: 0.9059 - val_loss: 0.6704 - val_acc: 0.9925 - val_mDice: 0.7972

Epoch 00045: val_mDice improved from 0.79592 to 0.79723, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 29s - loss: 0.0498 - acc: 0.9942 - mDice: 0.9080 - val_loss: 0.6856 - val_acc: 0.9924 - val_mDice: 0.7950

Epoch 00046: val_mDice did not improve from 0.79723
Epoch 47/300
 - 29s - loss: 0.0497 - acc: 0.9943 - mDice: 0.9081 - val_loss: 0.1205 - val_acc: 0.9926 - val_mDice: 0.7953

Epoch 00047: val_mDice did not improve from 0.79723
Epoch 48/300
 - 29s - loss: 0.0505 - acc: 0.9942 - mDice: 0.9067 - val_loss: 0.6268 - val_acc: 0.9928 - val_mDice: 0.7990

Epoch 00048: val_mDice improved from 0.79723 to 0.79897, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 29s - loss: 0.0490 - acc: 0.9943 - mDice: 0.9094 - val_loss: 0.6374 - val_acc: 0.9926 - val_mDice: 0.7977

Epoch 00049: val_mDice did not improve from 0.79897
Epoch 50/300
 - 29s - loss: 0.0492 - acc: 0.9943 - mDice: 0.9090 - val_loss: 0.6650 - val_acc: 0.9925 - val_mDice: 0.7961

Epoch 00050: val_mDice did not improve from 0.79897
Epoch 51/300
 - 29s - loss: 0.0488 - acc: 0.9943 - mDice: 0.9097 - val_loss: 0.5838 - val_acc: 0.9926 - val_mDice: 0.7985

Epoch 00051: val_mDice did not improve from 0.79897
Epoch 52/300
 - 29s - loss: 0.0485 - acc: 0.9944 - mDice: 0.9103 - val_loss: 0.7224 - val_acc: 0.9922 - val_mDice: 0.7950

Epoch 00052: val_mDice did not improve from 0.79897
Epoch 53/300
 - 29s - loss: 0.0490 - acc: 0.9943 - mDice: 0.9094 - val_loss: 0.6869 - val_acc: 0.9927 - val_mDice: 0.7989

Epoch 00053: val_mDice did not improve from 0.79897
Epoch 54/300
 - 29s - loss: 0.0479 - acc: 0.9944 - mDice: 0.9113 - val_loss: 0.7264 - val_acc: 0.9924 - val_mDice: 0.7965

Epoch 00054: val_mDice did not improve from 0.79897
Epoch 55/300
 - 29s - loss: 0.0480 - acc: 0.9944 - mDice: 0.9111 - val_loss: 0.6625 - val_acc: 0.9926 - val_mDice: 0.7997

Epoch 00055: val_mDice improved from 0.79897 to 0.79974, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 29s - loss: 0.0476 - acc: 0.9944 - mDice: 0.9118 - val_loss: 0.6525 - val_acc: 0.9925 - val_mDice: 0.7975

Epoch 00056: val_mDice did not improve from 0.79974
Epoch 57/300
 - 29s - loss: 0.0473 - acc: 0.9945 - mDice: 0.9124 - val_loss: 0.6775 - val_acc: 0.9925 - val_mDice: 0.7961

Epoch 00057: val_mDice did not improve from 0.79974
Epoch 58/300
 - 29s - loss: 0.0478 - acc: 0.9944 - mDice: 0.9115 - val_loss: 0.6829 - val_acc: 0.9922 - val_mDice: 0.7914

Epoch 00058: val_mDice did not improve from 0.79974
Epoch 59/300
 - 29s - loss: 0.0473 - acc: 0.9945 - mDice: 0.9123 - val_loss: 0.6438 - val_acc: 0.9926 - val_mDice: 0.7961

Epoch 00059: val_mDice did not improve from 0.79974
Epoch 60/300
 - 28s - loss: 0.0469 - acc: 0.9945 - mDice: 0.9131 - val_loss: 0.6696 - val_acc: 0.9922 - val_mDice: 0.7933

Epoch 00060: val_mDice did not improve from 0.79974
Epoch 61/300
 - 29s - loss: 0.0468 - acc: 0.9945 - mDice: 0.9133 - val_loss: 0.0593 - val_acc: 0.9927 - val_mDice: 0.7954

Epoch 00061: val_mDice did not improve from 0.79974
Epoch 62/300
 - 29s - loss: 0.0467 - acc: 0.9945 - mDice: 0.9134 - val_loss: 0.6843 - val_acc: 0.9924 - val_mDice: 0.7941

Epoch 00062: val_mDice did not improve from 0.79974
Epoch 63/300
 - 29s - loss: 0.0465 - acc: 0.9946 - mDice: 0.9138 - val_loss: 0.6315 - val_acc: 0.9928 - val_mDice: 0.8028

Epoch 00063: val_mDice improved from 0.79974 to 0.80277, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 28s - loss: 0.0462 - acc: 0.9946 - mDice: 0.9144 - val_loss: 0.3346 - val_acc: 0.9926 - val_mDice: 0.7976

Epoch 00064: val_mDice did not improve from 0.80277
Epoch 65/300
 - 29s - loss: 0.0459 - acc: 0.9946 - mDice: 0.9148 - val_loss: 0.6414 - val_acc: 0.9926 - val_mDice: 0.8002

Epoch 00065: val_mDice did not improve from 0.80277
Epoch 66/300
 - 29s - loss: 0.0458 - acc: 0.9946 - mDice: 0.9151 - val_loss: 0.4848 - val_acc: 0.9926 - val_mDice: 0.7971

Epoch 00066: val_mDice did not improve from 0.80277
Epoch 67/300
 - 29s - loss: 0.0457 - acc: 0.9946 - mDice: 0.9152 - val_loss: 0.5069 - val_acc: 0.9925 - val_mDice: 0.7968

Epoch 00067: val_mDice did not improve from 0.80277
Epoch 68/300
 - 29s - loss: 0.0455 - acc: 0.9946 - mDice: 0.9155 - val_loss: 0.6196 - val_acc: 0.9927 - val_mDice: 0.7994

Epoch 00068: val_mDice did not improve from 0.80277
Epoch 69/300
 - 29s - loss: 0.0449 - acc: 0.9947 - mDice: 0.9167 - val_loss: 0.5979 - val_acc: 0.9927 - val_mDice: 0.7976

Epoch 00069: val_mDice did not improve from 0.80277
Epoch 70/300
 - 29s - loss: 0.0454 - acc: 0.9946 - mDice: 0.9158 - val_loss: 0.6282 - val_acc: 0.9927 - val_mDice: 0.8002

Epoch 00070: val_mDice did not improve from 0.80277
Epoch 71/300
 - 29s - loss: 0.0448 - acc: 0.9947 - mDice: 0.9168 - val_loss: 0.0791 - val_acc: 0.9926 - val_mDice: 0.7988

Epoch 00071: val_mDice did not improve from 0.80277
Epoch 72/300
 - 29s - loss: 0.0446 - acc: 0.9947 - mDice: 0.9172 - val_loss: 0.6321 - val_acc: 0.9925 - val_mDice: 0.7973

Epoch 00072: val_mDice did not improve from 0.80277
Epoch 73/300
 - 29s - loss: 0.0449 - acc: 0.9947 - mDice: 0.9166 - val_loss: 0.0584 - val_acc: 0.9926 - val_mDice: 0.7948

Epoch 00073: val_mDice did not improve from 0.80277
Epoch 74/300
 - 28s - loss: 0.0442 - acc: 0.9948 - mDice: 0.9179 - val_loss: 0.6091 - val_acc: 0.9927 - val_mDice: 0.7978

Epoch 00074: val_mDice did not improve from 0.80277
Epoch 75/300
 - 29s - loss: 0.0443 - acc: 0.9947 - mDice: 0.9177 - val_loss: 0.6053 - val_acc: 0.9924 - val_mDice: 0.7959

Epoch 00075: val_mDice did not improve from 0.80277
Epoch 76/300
 - 29s - loss: 0.0445 - acc: 0.9947 - mDice: 0.9174 - val_loss: 0.4018 - val_acc: 0.9925 - val_mDice: 0.7964

Epoch 00076: val_mDice did not improve from 0.80277
Epoch 77/300
 - 29s - loss: 0.0439 - acc: 0.9948 - mDice: 0.9185 - val_loss: 0.5885 - val_acc: 0.9926 - val_mDice: 0.7966

Epoch 00077: val_mDice did not improve from 0.80277
Epoch 78/300
 - 29s - loss: 0.0437 - acc: 0.9948 - mDice: 0.9189 - val_loss: 0.6925 - val_acc: 0.9927 - val_mDice: 0.7993

Epoch 00078: val_mDice did not improve from 0.80277
Epoch 79/300
 - 29s - loss: 0.0436 - acc: 0.9948 - mDice: 0.9189 - val_loss: 0.6284 - val_acc: 0.9928 - val_mDice: 0.8045

Epoch 00079: val_mDice improved from 0.80277 to 0.80447, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 80/300
 - 29s - loss: 0.0437 - acc: 0.9948 - mDice: 0.9188 - val_loss: 0.6318 - val_acc: 0.9926 - val_mDice: 0.7968

Epoch 00080: val_mDice did not improve from 0.80447
Epoch 81/300
 - 29s - loss: 0.0440 - acc: 0.9948 - mDice: 0.9183 - val_loss: 0.6439 - val_acc: 0.9926 - val_mDice: 0.8004

Epoch 00081: val_mDice did not improve from 0.80447
Epoch 82/300
 - 28s - loss: 0.0432 - acc: 0.9949 - mDice: 0.9198 - val_loss: 0.6858 - val_acc: 0.9926 - val_mDice: 0.7998

Epoch 00082: val_mDice did not improve from 0.80447
Epoch 83/300
 - 28s - loss: 0.0430 - acc: 0.9948 - mDice: 0.9201 - val_loss: 0.6183 - val_acc: 0.9928 - val_mDice: 0.8025

Epoch 00083: val_mDice did not improve from 0.80447
Epoch 84/300
 - 29s - loss: 0.0435 - acc: 0.9948 - mDice: 0.9191 - val_loss: 0.6804 - val_acc: 0.9924 - val_mDice: 0.7988

Epoch 00084: val_mDice did not improve from 0.80447
Epoch 85/300
 - 29s - loss: 0.0430 - acc: 0.9949 - mDice: 0.9200 - val_loss: 0.5871 - val_acc: 0.9928 - val_mDice: 0.8040

Epoch 00085: val_mDice did not improve from 0.80447
Epoch 86/300
 - 29s - loss: 0.0431 - acc: 0.9949 - mDice: 0.9199 - val_loss: 0.6476 - val_acc: 0.9925 - val_mDice: 0.7976

Epoch 00086: val_mDice did not improve from 0.80447
Epoch 87/300
 - 28s - loss: 0.0427 - acc: 0.9949 - mDice: 0.9206 - val_loss: 0.0557 - val_acc: 0.9928 - val_mDice: 0.8011

Epoch 00087: val_mDice did not improve from 0.80447
Epoch 88/300
 - 29s - loss: 0.0431 - acc: 0.9948 - mDice: 0.9199 - val_loss: 0.6770 - val_acc: 0.9923 - val_mDice: 0.7963

Epoch 00088: val_mDice did not improve from 0.80447
Epoch 89/300
 - 29s - loss: 0.0424 - acc: 0.9949 - mDice: 0.9210 - val_loss: 0.6067 - val_acc: 0.9927 - val_mDice: 0.7984

Epoch 00089: val_mDice did not improve from 0.80447
Epoch 90/300
 - 29s - loss: 0.0422 - acc: 0.9949 - mDice: 0.9214 - val_loss: 0.6621 - val_acc: 0.9926 - val_mDice: 0.8007

Epoch 00090: val_mDice did not improve from 0.80447
Epoch 91/300
 - 28s - loss: 0.0423 - acc: 0.9949 - mDice: 0.9214 - val_loss: 0.6515 - val_acc: 0.9925 - val_mDice: 0.7939

Epoch 00091: val_mDice did not improve from 0.80447
Epoch 92/300
 - 29s - loss: 0.0425 - acc: 0.9949 - mDice: 0.9210 - val_loss: 0.6374 - val_acc: 0.9926 - val_mDice: 0.7983

Epoch 00092: val_mDice did not improve from 0.80447
Epoch 93/300
 - 29s - loss: 0.0422 - acc: 0.9949 - mDice: 0.9216 - val_loss: 0.0561 - val_acc: 0.9928 - val_mDice: 0.7993

Epoch 00093: val_mDice did not improve from 0.80447
Epoch 94/300
 - 29s - loss: 0.0421 - acc: 0.9949 - mDice: 0.9217 - val_loss: 0.7185 - val_acc: 0.9921 - val_mDice: 0.7928

Epoch 00094: val_mDice did not improve from 0.80447
Epoch 95/300
 - 28s - loss: 0.0420 - acc: 0.9949 - mDice: 0.9219 - val_loss: 0.6189 - val_acc: 0.9925 - val_mDice: 0.7971

Epoch 00095: val_mDice did not improve from 0.80447
Epoch 96/300
 - 29s - loss: 0.0421 - acc: 0.9949 - mDice: 0.9216 - val_loss: 0.4077 - val_acc: 0.9926 - val_mDice: 0.7980

Epoch 00096: val_mDice did not improve from 0.80447
Epoch 97/300
 - 29s - loss: 0.0420 - acc: 0.9949 - mDice: 0.9218 - val_loss: 0.0656 - val_acc: 0.9929 - val_mDice: 0.8014

Epoch 00097: val_mDice did not improve from 0.80447
Epoch 98/300
 - 29s - loss: 0.0417 - acc: 0.9950 - mDice: 0.9225 - val_loss: 0.5806 - val_acc: 0.9924 - val_mDice: 0.7934

Epoch 00098: val_mDice did not improve from 0.80447
Epoch 99/300
 - 29s - loss: 0.0415 - acc: 0.9950 - mDice: 0.9227 - val_loss: 0.6410 - val_acc: 0.9928 - val_mDice: 0.8060

Epoch 00099: val_mDice improved from 0.80447 to 0.80602, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 100/300
 - 28s - loss: 0.0418 - acc: 0.9950 - mDice: 0.9222 - val_loss: 0.6197 - val_acc: 0.9925 - val_mDice: 0.7965

Epoch 00100: val_mDice did not improve from 0.80602
Epoch 101/300
 - 29s - loss: 0.0416 - acc: 0.9950 - mDice: 0.9225 - val_loss: 0.4537 - val_acc: 0.9927 - val_mDice: 0.8018

Epoch 00101: val_mDice did not improve from 0.80602
Epoch 102/300
 - 29s - loss: 0.0413 - acc: 0.9950 - mDice: 0.9232 - val_loss: 0.5616 - val_acc: 0.9926 - val_mDice: 0.7975

Epoch 00102: val_mDice did not improve from 0.80602
Epoch 103/300
 - 29s - loss: 0.0411 - acc: 0.9950 - mDice: 0.9235 - val_loss: 0.5160 - val_acc: 0.9927 - val_mDice: 0.7981

Epoch 00103: val_mDice did not improve from 0.80602
Epoch 104/300
 - 28s - loss: 0.0414 - acc: 0.9950 - mDice: 0.9230 - val_loss: 0.0751 - val_acc: 0.9929 - val_mDice: 0.8052

Epoch 00104: val_mDice did not improve from 0.80602
Epoch 105/300
 - 28s - loss: 0.0411 - acc: 0.9950 - mDice: 0.9234 - val_loss: 0.2050 - val_acc: 0.9925 - val_mDice: 0.7959

Epoch 00105: val_mDice did not improve from 0.80602
Epoch 106/300
 - 28s - loss: 0.0408 - acc: 0.9950 - mDice: 0.9240 - val_loss: 0.6685 - val_acc: 0.9923 - val_mDice: 0.7957

Epoch 00106: val_mDice did not improve from 0.80602
Epoch 107/300
 - 29s - loss: 0.0405 - acc: 0.9951 - mDice: 0.9245 - val_loss: 0.1776 - val_acc: 0.9931 - val_mDice: 0.8086

Epoch 00107: val_mDice improved from 0.80602 to 0.80856, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd2/best_model_weights_TF_CSFn2.h5
Epoch 108/300
 - 29s - loss: 0.0412 - acc: 0.9950 - mDice: 0.9233 - val_loss: 0.6494 - val_acc: 0.9925 - val_mDice: 0.8025

Epoch 00108: val_mDice did not improve from 0.80856
Epoch 109/300
 - 29s - loss: 0.0408 - acc: 0.9950 - mDice: 0.9240 - val_loss: 0.4377 - val_acc: 0.9926 - val_mDice: 0.7995

Epoch 00109: val_mDice did not improve from 0.80856
Epoch 110/300
 - 29s - loss: 0.0408 - acc: 0.9950 - mDice: 0.9240 - val_loss: 0.5447 - val_acc: 0.9929 - val_mDice: 0.8020

Epoch 00110: val_mDice did not improve from 0.80856
Epoch 111/300
 - 29s - loss: 0.0406 - acc: 0.9951 - mDice: 0.9244 - val_loss: 0.5165 - val_acc: 0.9929 - val_mDice: 0.8033

Epoch 00111: val_mDice did not improve from 0.80856
Epoch 112/300
 - 29s - loss: 0.0407 - acc: 0.9951 - mDice: 0.9241 - val_loss: 0.6218 - val_acc: 0.9926 - val_mDice: 0.7988

Epoch 00112: val_mDice did not improve from 0.80856
Epoch 113/300
 - 28s - loss: 0.0404 - acc: 0.9951 - mDice: 0.9248 - val_loss: 0.6089 - val_acc: 0.9927 - val_mDice: 0.8012

Epoch 00113: val_mDice did not improve from 0.80856
Epoch 114/300
 - 29s - loss: 0.0402 - acc: 0.9951 - mDice: 0.9251 - val_loss: 0.2350 - val_acc: 0.9929 - val_mDice: 0.8046

Epoch 00114: val_mDice did not improve from 0.80856
Epoch 115/300
 - 29s - loss: 0.0402 - acc: 0.9951 - mDice: 0.9251 - val_loss: 0.6064 - val_acc: 0.9927 - val_mDice: 0.8040

Epoch 00115: val_mDice did not improve from 0.80856
Epoch 116/300
 - 29s - loss: 0.0399 - acc: 0.9951 - mDice: 0.9256 - val_loss: 0.5798 - val_acc: 0.9927 - val_mDice: 0.8015

Epoch 00116: val_mDice did not improve from 0.80856
Epoch 117/300
 - 28s - loss: 0.0402 - acc: 0.9951 - mDice: 0.9250 - val_loss: 0.6218 - val_acc: 0.9928 - val_mDice: 0.8022

Epoch 00117: val_mDice did not improve from 0.80856
Epoch 118/300
 - 28s - loss: 0.0399 - acc: 0.9951 - mDice: 0.9256 - val_loss: 0.6501 - val_acc: 0.9928 - val_mDice: 0.8020

Epoch 00118: val_mDice did not improve from 0.80856
Epoch 119/300
 - 29s - loss: 0.0400 - acc: 0.9951 - mDice: 0.9255 - val_loss: 0.6268 - val_acc: 0.9927 - val_mDice: 0.8016

Epoch 00119: val_mDice did not improve from 0.80856
Epoch 120/300
 - 29s - loss: 0.0401 - acc: 0.9951 - mDice: 0.9252 - val_loss: 0.0813 - val_acc: 0.9930 - val_mDice: 0.8044

Epoch 00120: val_mDice did not improve from 0.80856
Epoch 121/300
 - 29s - loss: 0.0399 - acc: 0.9951 - mDice: 0.9257 - val_loss: 0.5913 - val_acc: 0.9928 - val_mDice: 0.8048

Epoch 00121: val_mDice did not improve from 0.80856
Epoch 122/300
 - 30s - loss: 0.0398 - acc: 0.9951 - mDice: 0.9258 - val_loss: 0.6273 - val_acc: 0.9925 - val_mDice: 0.7984

Epoch 00122: val_mDice did not improve from 0.80856
Epoch 123/300
 - 29s - loss: 0.0401 - acc: 0.9951 - mDice: 0.9253 - val_loss: 0.5345 - val_acc: 0.9929 - val_mDice: 0.8047

Epoch 00123: val_mDice did not improve from 0.80856
Epoch 124/300
 - 29s - loss: 0.0398 - acc: 0.9951 - mDice: 0.9257 - val_loss: 0.1681 - val_acc: 0.9928 - val_mDice: 0.8024

Epoch 00124: val_mDice did not improve from 0.80856
Epoch 125/300
 - 29s - loss: 0.0398 - acc: 0.9951 - mDice: 0.9258 - val_loss: 0.6686 - val_acc: 0.9926 - val_mDice: 0.7989

Epoch 00125: val_mDice did not improve from 0.80856
Epoch 126/300
 - 29s - loss: 0.0398 - acc: 0.9952 - mDice: 0.9258 - val_loss: 0.6019 - val_acc: 0.9924 - val_mDice: 0.7968

Epoch 00126: val_mDice did not improve from 0.80856
Epoch 127/300
 - 29s - loss: 0.0396 - acc: 0.9951 - mDice: 0.9262 - val_loss: 0.6022 - val_acc: 0.9927 - val_mDice: 0.8030

Epoch 00127: val_mDice did not improve from 0.80856
Epoch 128/300
 - 29s - loss: 0.0395 - acc: 0.9952 - mDice: 0.9263 - val_loss: 0.6172 - val_acc: 0.9929 - val_mDice: 0.8069

Epoch 00128: val_mDice did not improve from 0.80856
Epoch 129/300
 - 29s - loss: 0.0394 - acc: 0.9952 - mDice: 0.9265 - val_loss: 0.6105 - val_acc: 0.9926 - val_mDice: 0.8016

Epoch 00129: val_mDice did not improve from 0.80856
Epoch 130/300
 - 29s - loss: 0.0396 - acc: 0.9951 - mDice: 0.9262 - val_loss: 0.2943 - val_acc: 0.9926 - val_mDice: 0.7980

Epoch 00130: val_mDice did not improve from 0.80856
Epoch 131/300
 - 29s - loss: 0.0394 - acc: 0.9952 - mDice: 0.9265 - val_loss: 0.6374 - val_acc: 0.9925 - val_mDice: 0.7980

Epoch 00131: val_mDice did not improve from 0.80856
Epoch 132/300
 - 29s - loss: 0.0394 - acc: 0.9952 - mDice: 0.9265 - val_loss: 0.6075 - val_acc: 0.9927 - val_mDice: 0.8045

Epoch 00132: val_mDice did not improve from 0.80856
Epoch 133/300
 - 29s - loss: 0.0398 - acc: 0.9951 - mDice: 0.9258 - val_loss: 0.6124 - val_acc: 0.9926 - val_mDice: 0.8018

Epoch 00133: val_mDice did not improve from 0.80856
Epoch 134/300
 - 29s - loss: 0.0391 - acc: 0.9952 - mDice: 0.9271 - val_loss: 0.6039 - val_acc: 0.9928 - val_mDice: 0.8062

Epoch 00134: val_mDice did not improve from 0.80856
Epoch 135/300
 - 29s - loss: 0.0394 - acc: 0.9952 - mDice: 0.9264 - val_loss: 0.3354 - val_acc: 0.9924 - val_mDice: 0.7978

Epoch 00135: val_mDice did not improve from 0.80856
Epoch 136/300
 - 29s - loss: 0.0390 - acc: 0.9952 - mDice: 0.9272 - val_loss: 0.6087 - val_acc: 0.9925 - val_mDice: 0.7994

Epoch 00136: val_mDice did not improve from 0.80856
Epoch 137/300
 - 29s - loss: 0.0393 - acc: 0.9952 - mDice: 0.9267 - val_loss: 0.6074 - val_acc: 0.9929 - val_mDice: 0.8044

Epoch 00137: val_mDice did not improve from 0.80856
Epoch 138/300
 - 29s - loss: 0.0390 - acc: 0.9952 - mDice: 0.9271 - val_loss: 0.6291 - val_acc: 0.9928 - val_mDice: 0.8003

Epoch 00138: val_mDice did not improve from 0.80856
Epoch 139/300
 - 29s - loss: 0.0391 - acc: 0.9952 - mDice: 0.9271 - val_loss: 0.3815 - val_acc: 0.9929 - val_mDice: 0.8023

Epoch 00139: val_mDice did not improve from 0.80856
Epoch 140/300
 - 29s - loss: 0.0390 - acc: 0.9952 - mDice: 0.9273 - val_loss: 0.6084 - val_acc: 0.9926 - val_mDice: 0.8030

Epoch 00140: val_mDice did not improve from 0.80856
Epoch 141/300
 - 29s - loss: 0.0384 - acc: 0.9953 - mDice: 0.9284 - val_loss: 0.6222 - val_acc: 0.9927 - val_mDice: 0.8015

Epoch 00141: val_mDice did not improve from 0.80856
Epoch 142/300
 - 29s - loss: 0.0389 - acc: 0.9952 - mDice: 0.9275 - val_loss: 0.6285 - val_acc: 0.9927 - val_mDice: 0.8034

Epoch 00142: val_mDice did not improve from 0.80856
Epoch 143/300
 - 29s - loss: 0.0386 - acc: 0.9952 - mDice: 0.9279 - val_loss: 0.6282 - val_acc: 0.9925 - val_mDice: 0.7991

Epoch 00143: val_mDice did not improve from 0.80856
Epoch 144/300
 - 29s - loss: 0.0389 - acc: 0.9952 - mDice: 0.9274 - val_loss: 0.6040 - val_acc: 0.9928 - val_mDice: 0.8010

Epoch 00144: val_mDice did not improve from 0.80856
Epoch 145/300
 - 29s - loss: 0.0387 - acc: 0.9952 - mDice: 0.9279 - val_loss: 0.6071 - val_acc: 0.9927 - val_mDice: 0.8038

Epoch 00145: val_mDice did not improve from 0.80856
Epoch 146/300
 - 29s - loss: 0.0386 - acc: 0.9952 - mDice: 0.9279 - val_loss: 0.5950 - val_acc: 0.9928 - val_mDice: 0.8044

Epoch 00146: val_mDice did not improve from 0.80856
Epoch 147/300
 - 29s - loss: 0.0385 - acc: 0.9952 - mDice: 0.9282 - val_loss: 0.5566 - val_acc: 0.9925 - val_mDice: 0.7976

Epoch 00147: val_mDice did not improve from 0.80856
Restoring model weights from the end of the best epoch
Epoch 00147: early stopping
{'val_loss': [0.8975242825690657, 0.8687713146209717, 0.8635679399594665, 0.8469784075859934, 0.8106640328187495, 0.8140360631514341, 0.746874786214903, 0.7350192747544497, 0.7636865202803165, 0.7394257739651948, 0.6587962724734098, 0.7502596476115286, 0.7448212031740695, 0.7343055717647076, 0.7313817364629358, 0.7334005215670913, 0.7466777642257512, 0.7451030092779547, 0.7496794266626239, 0.7009534831158817, 0.6721480039414018, 0.7123231892473996, 0.643672380130738, 0.7172891108784825, 0.6998794670216739, 0.676459135254845, 0.6850528854411095, 0.7167372372932732, 0.6861228039488196, 0.7248576355632395, 0.7177409487776458, 0.6684723051730543, 0.671503814868629, 0.6742495847865939, 0.5308853649767116, 0.7451706661377102, 0.6521873651072383, 0.6589159485884011, 0.6280654945876449, 0.6964494895655662, 0.6499897061148658, 0.6774334153160453, 0.7084878706373274, 0.10407840693369508, 0.6703764533158392, 0.6856436135713011, 0.12045784667134285, 0.6268049610080197, 0.6373781191650778, 0.6649614810012281, 0.5837792928796262, 0.7223982838913798, 0.6868723542429507, 0.7264083763584495, 0.6625033569289371, 0.652494000736624, 0.6774767299648374, 0.6829051442909986, 0.6438036271138117, 0.6696064260322601, 0.05930077680386603, 0.6843315474689007, 0.6315153958275914, 0.33456952089909464, 0.6414487187284976, 0.4848363440250978, 0.5069229090586305, 0.6196124081034213, 0.5979116675443947, 0.6281507819658145, 0.07906763022765517, 0.6321195678319782, 0.05838452361058444, 0.6091020934982225, 0.6052505322732031, 0.40176961477845907, 0.5885142160113901, 0.6925163234118372, 0.6283577596768737, 0.6318239502143115, 0.6438899426721036, 0.6857761759310961, 0.6183019625023007, 0.6803578402614221, 0.5871380409225821, 0.6475887966807932, 0.05566428345628083, 0.6770009286701679, 0.6067337917629629, 0.6620913924416527, 0.651505395071581, 0.637383371940814, 0.05608412344008684, 0.7184859237167984, 0.6188967146445066, 0.407728681107983, 0.0656495849834755, 0.5806028707884252, 0.6410440967883915, 0.6197100395802408, 0.4536823359085247, 0.5616018783766776, 0.5159972074907273, 0.07509676879271865, 0.20495779532939196, 0.6685188832925633, 0.17764590308070183, 0.6493704405147582, 0.4376524402759969, 0.5446762325009331, 0.5164550206391141, 0.6218372349394485, 0.608881258754991, 0.2349886461161077, 0.6064360074233264, 0.5797536220634356, 0.6217652498744428, 0.6501394498627633, 0.6267879597144201, 0.08128795877564698, 0.5912726849783212, 0.6272512349532917, 0.5344563339604065, 0.16812589182518423, 0.6686426938977093, 0.6018592551117763, 0.6021962962113321, 0.6172260991297662, 0.6105173327960074, 0.2942509495187551, 0.6373656310606748, 0.6075319724623114, 0.6123919977108017, 0.6038851054618135, 0.33537462539970875, 0.608680694247596, 0.6073555392213166, 0.6291399841429666, 0.3814910809742287, 0.6084236967144534, 0.6221889565931633, 0.628470420720987, 0.6281533678993583, 0.6040372991701588, 0.6070755039108917, 0.5949805869022384, 0.5566473657963797], 'val_acc': [0.9803228303790092, 0.9881839714944363, 0.9895331896841526, 0.9894399084150791, 0.9899402819573879, 0.9896599091589451, 0.9910141304135323, 0.9908734373748302, 0.9912927597761154, 0.9914975538849831, 0.991536708548665, 0.9914521388709545, 0.991339648142457, 0.9918340500444174, 0.9916931092739105, 0.9918956477195024, 0.9916150346398354, 0.9917499870061874, 0.9918664656579494, 0.9923419114202261, 0.9920490588992834, 0.9921717792749405, 0.9922536071389914, 0.9920143857598305, 0.9923024941235781, 0.9922892674803734, 0.9920987002551556, 0.9921099226921797, 0.9920697640627623, 0.9923578687012196, 0.9922077003866434, 0.992333423346281, 0.992246612906456, 0.9922999888658524, 0.992749247699976, 0.9920710138976574, 0.9924282226711512, 0.9924409408122301, 0.992611289024353, 0.9920839853584766, 0.9923493899405003, 0.992108665406704, 0.9922101851552725, 0.9925234913825989, 0.992471119388938, 0.9923733416944742, 0.9926497135311365, 0.9927908945828676, 0.9926167968660593, 0.9925314839929342, 0.9926404934376478, 0.9922191686928272, 0.9926554393023252, 0.9923865664750338, 0.9926200341433287, 0.9925297293812037, 0.9925142675638199, 0.992226155474782, 0.9925778731703758, 0.9922368880361319, 0.9927068315446377, 0.9924070090055466, 0.9928130973130465, 0.9926035664975643, 0.9925698973238468, 0.992630260065198, 0.9924938213080168, 0.9926724154502153, 0.9926676712930202, 0.9926853869110346, 0.9926165398210287, 0.9925364591181278, 0.9926225282251835, 0.9926656801253557, 0.9924411829560995, 0.9925132747739553, 0.9926402419805527, 0.9926748964935541, 0.992787154391408, 0.9926038179546595, 0.9926107991486788, 0.9925536811351776, 0.9927929155528545, 0.9924339577555656, 0.9927946384996176, 0.9924778435379267, 0.9927529692649841, 0.992305738851428, 0.9926507100462914, 0.9926330000162125, 0.9924803376197815, 0.9926125463098288, 0.9928392935544252, 0.9921488463878632, 0.9924945645034313, 0.9926242716610432, 0.992856252938509, 0.9924097470939159, 0.99284503236413, 0.9925459306687117, 0.9927487429231405, 0.9925858583301306, 0.9926789030432701, 0.9928899295628071, 0.9925402086228132, 0.9923032559454441, 0.9930795002728701, 0.9925362225621939, 0.9926255103200674, 0.9928836915642023, 0.9928672332316637, 0.9925531838089228, 0.992667431011796, 0.9928500130772591, 0.992687376216054, 0.9927195515483618, 0.9927709549665451, 0.9927993658930063, 0.9927232917398214, 0.9929882120341063, 0.9928380455821753, 0.9924681186676025, 0.9928861893713474, 0.9927637074142694, 0.9925798773765564, 0.992445170879364, 0.9926651772111654, 0.9929430615156889, 0.992604061961174, 0.9925948306918144, 0.9924633838236332, 0.9926953595131636, 0.9926449842751026, 0.992805115878582, 0.9924426805227995, 0.9925469513982534, 0.9928984101861715, 0.9928016345947981, 0.9928752053529024, 0.992636488750577, 0.9926721602678299, 0.9926734045147896, 0.9925451930612326, 0.9927619639784098, 0.9926694221794605, 0.9928073529154062, 0.9925100393593311], 'val_mDice': [0.657615140080452, 0.6903411168605089, 0.704069035127759, 0.7147485390305519, 0.7400096673518419, 0.7426453474909067, 0.7584756053984165, 0.753560159355402, 0.7738077826797962, 0.7723447680473328, 0.7688095048069954, 0.7724574021995068, 0.7711957208812237, 0.778805548325181, 0.7756387908011675, 0.7847179472446442, 0.7799446024000645, 0.7802268676459789, 0.78345718793571, 0.7869506161659956, 0.7837202679365873, 0.7846342120319605, 0.7873162161558867, 0.7834979146718979, 0.791129944846034, 0.7893383596092463, 0.7846923172473907, 0.7866993825882673, 0.7870897930115461, 0.7941754534840584, 0.79181257635355, 0.7875518407672644, 0.7864612247794867, 0.7886466197669506, 0.7959236484020948, 0.7908248528838158, 0.7949229888617992, 0.7939606066793203, 0.7935744524002075, 0.7893260475248098, 0.794442854821682, 0.7895086240023375, 0.7907262537628412, 0.7953458931297064, 0.7972307689487934, 0.7949952688068151, 0.7952840253710747, 0.7989653293043375, 0.7977070473134518, 0.7960812840610743, 0.7984529435634613, 0.7950134016573429, 0.7988613564521074, 0.7965350802987814, 0.7997407019138336, 0.7975152507424355, 0.7961481995880604, 0.7913970295339823, 0.7960804365575314, 0.7933467160910368, 0.7954183500260115, 0.7941341493278742, 0.8027743920683861, 0.7976057287305593, 0.8002196066081524, 0.7971025165170431, 0.796811068430543, 0.7993524335324764, 0.7975828088819981, 0.8002095762640238, 0.7987673114985228, 0.7972650416195393, 0.7947924453765154, 0.7977768871933222, 0.7959451712667942, 0.796440877020359, 0.796603886410594, 0.7993386872112751, 0.8044662047177553, 0.7968406938016415, 0.8004121296107769, 0.7997519057244062, 0.8024532347917557, 0.7988386508077383, 0.8040369208902121, 0.7975685130804777, 0.8011444471776485, 0.7963313441723585, 0.7984446175396442, 0.8006511069834232, 0.7939118053764105, 0.7983381766825914, 0.7992742210626602, 0.7928420789539814, 0.797129301354289, 0.798016618937254, 0.8014335110783577, 0.7933654263615608, 0.8060167357325554, 0.7964956201612949, 0.8018092531710863, 0.7974973879754543, 0.7981169316917658, 0.8051764592528343, 0.79590929672122, 0.7957225572317839, 0.8085608389228582, 0.802519703283906, 0.7995436619967222, 0.8019794337451458, 0.8033058699220419, 0.7988060731440783, 0.8012018818408251, 0.8046470396220684, 0.8039609063416719, 0.8014881145209074, 0.8021984063088894, 0.802018865942955, 0.8016163669526577, 0.8044118992984295, 0.8048422448337078, 0.7983987685292959, 0.8046729676425457, 0.8024361822754145, 0.7988999132066965, 0.7968115340918303, 0.8029539417475462, 0.8068537525832653, 0.80156484618783, 0.797965731471777, 0.7980404887348413, 0.8044591546058655, 0.801792535930872, 0.8062081597745419, 0.797750623896718, 0.7993866223841906, 0.8044254966080189, 0.8003266900777817, 0.8022683206945658, 0.8029631432145834, 0.8014796860516071, 0.8033707179129124, 0.7990558631718159, 0.8009994868189096, 0.803750641644001, 0.8043870236724615, 0.797589598223567], 'loss': [0.6131597812082676, 0.1753049628456129, 0.1358073696345212, 0.11918039816683063, 0.10626117143950092, 0.09723594536167536, 0.09246124987364633, 0.08795903718887381, 0.08463012087933067, 0.08127411945951418, 0.0787811198419944, 0.07582086889523171, 0.07449705488880268, 0.07269751477479741, 0.07087204414293771, 0.06918976694584392, 0.0683120664802525, 0.06637465717982745, 0.06616152180275449, 0.06486765175808511, 0.06368896582614977, 0.062375642826703416, 0.06240645532169602, 0.06354458062829796, 0.05996074900593757, 0.05929669302917322, 0.0587902924372051, 0.05855746144080212, 0.057748202333189794, 0.05682779287509737, 0.05636045106332712, 0.05569413021956502, 0.05516953171433521, 0.05481729510640508, 0.05500025982393182, 0.0542407738140295, 0.0536970500129329, 0.05319274896895399, 0.05271660808815321, 0.052387793992587715, 0.05190674236720138, 0.051829772609561606, 0.05166052691218633, 0.05119508607273001, 0.050946544497890094, 0.049790998737065786, 0.04970313999613597, 0.05051100059419237, 0.04897040873159996, 0.04923300956154834, 0.048795629437288726, 0.04847651365653167, 0.0489792501893874, 0.04789977698983237, 0.048012499972888784, 0.04764700542078888, 0.047270772279657536, 0.04781998391617559, 0.047331980278643423, 0.0468866238518779, 0.04678377712110173, 0.04674668646279813, 0.046505943089169575, 0.04618359506398567, 0.04592863272646585, 0.04576823948356102, 0.04569951796314921, 0.04553412411639371, 0.04487138774440314, 0.04537832371278536, 0.04480932899398561, 0.04460727561196411, 0.044916859921749645, 0.044230758951007594, 0.044322910478295965, 0.044499139165032044, 0.043869687711093515, 0.043668762723052555, 0.043635904321740686, 0.04368188494429697, 0.04399437702624373, 0.04316851222352885, 0.0429837554981048, 0.04351307661708111, 0.04304332489934867, 0.0431142186641475, 0.04269564920605489, 0.04311677039927364, 0.04244573388616147, 0.04223473572224271, 0.042269474343400296, 0.04246091515914799, 0.042165968426898945, 0.04209294383824945, 0.04197779171791201, 0.04214788559273247, 0.04201353193229675, 0.04165007836404295, 0.041510378873507864, 0.041786536188503795, 0.041612078489929843, 0.0412702692229618, 0.04109573407821246, 0.04138823512383693, 0.04110815679667247, 0.04082865974117831, 0.04054981173149548, 0.04119459854529144, 0.04081663232635604, 0.04079503196289353, 0.04060291931655531, 0.04072617961086642, 0.04037685400199444, 0.04019486150602936, 0.040186801039749795, 0.03989192121902956, 0.04023980167733885, 0.03989978104096589, 0.039961412520885496, 0.04010988938205262, 0.039851663237067764, 0.039802291148009025, 0.0400767899842798, 0.03983286671916983, 0.03980988627415446, 0.03977836991072851, 0.03961017077971145, 0.039530427731072376, 0.03941142434637119, 0.03958182546609963, 0.03940370430678832, 0.0394236832288838, 0.039786203521241834, 0.03905275185429407, 0.03944988719200553, 0.03900156010656881, 0.039321338724328955, 0.03904674751345387, 0.0390952502066674, 0.038970256202417175, 0.038386376077757, 0.03886960011697978, 0.03864883298455346, 0.03889140471546848, 0.03865740155065165, 0.03862492008080837, 0.03848310194449051], 'acc': [0.9522241180652495, 0.9789094669596997, 0.9823160608207859, 0.9852866208261073, 0.9886585361619327, 0.9901124315767653, 0.9905401895073104, 0.9908593594231533, 0.9911587322729174, 0.9914141805160609, 0.9916501998679763, 0.991868921608491, 0.9919937413176746, 0.9921412926746797, 0.9922989423687455, 0.9924389883673287, 0.9925281952257645, 0.9926868179983775, 0.9927065483407719, 0.9928552783212832, 0.9929706351231016, 0.9930982900591031, 0.9931454204080883, 0.9929637983173004, 0.9933098478711716, 0.9933600244766414, 0.9934284850059936, 0.9934574291378387, 0.9935211612328677, 0.9935891992873787, 0.9936423155299404, 0.9937118838248081, 0.9937462410583886, 0.9937989908573838, 0.9937800215805007, 0.993843761584035, 0.9938781533047922, 0.9939289147864655, 0.9939841726965587, 0.9940068624680835, 0.9940374206482803, 0.9940608036674605, 0.9940753483503583, 0.9941162541517331, 0.9941472834697013, 0.9942316825179061, 0.9942602510996309, 0.9942001656703698, 0.9943149583575422, 0.9943021052775932, 0.9943266483055142, 0.9943542012675819, 0.9943482974853418, 0.9944045314933415, 0.9944150211421319, 0.9944416083870719, 0.9944724916262232, 0.9944364432675284, 0.9944736674935253, 0.9944989905272044, 0.9945340232108408, 0.9945214120674399, 0.9945849098435703, 0.9945624210741152, 0.9946000066398536, 0.9946305710318973, 0.9946083365188072, 0.9946287117090821, 0.9946651855481271, 0.9946381973516683, 0.9946963790472174, 0.9947202914307116, 0.9946938472106036, 0.994766917442824, 0.9947289806826682, 0.9947297042762692, 0.9947580817934942, 0.9947928100481562, 0.994798686552286, 0.9947925645243877, 0.994785695544405, 0.9948560379517402, 0.9948338880845863, 0.9948312441784981, 0.9948554893840241, 0.9948648605956646, 0.9948721532900879, 0.9948363792152511, 0.994908387547131, 0.9949024936401347, 0.9949158695747701, 0.9949111370684669, 0.9949229313020606, 0.9949116523539797, 0.9949419854603343, 0.9949365843598597, 0.9949425846507938, 0.9949598068751648, 0.9949806636878668, 0.9949578124775736, 0.9949872525599521, 0.9949968461377067, 0.9950256167533466, 0.9949817719312167, 0.9950091721386365, 0.9950283529230204, 0.9950759527269689, 0.995024015447245, 0.9950400729191571, 0.9950363956512136, 0.9950632566814017, 0.9950567015693539, 0.9950925404149868, 0.9950946182396903, 0.9950619449221577, 0.9951307940278109, 0.9950973021592138, 0.9951068070813783, 0.9951229178976123, 0.995123327858733, 0.9951191760666268, 0.9951455982786529, 0.9951062650094399, 0.9951284258737095, 0.9951333076236167, 0.9951534731969527, 0.9951472234903236, 0.9951560619720339, 0.9951941378027689, 0.9951458639337915, 0.9951546896178506, 0.995169478300987, 0.9951361316940407, 0.9951950184792276, 0.995155191177744, 0.9951969571433251, 0.9951744456871248, 0.9951896055229204, 0.9952095873654755, 0.9952129766585672, 0.9952513661979591, 0.9952053157546676, 0.995221341551008, 0.9952175546166123, 0.9952378356250388, 0.9952297493446659, 0.9952305429306154], 'mDice': [0.39637086996697196, 0.7190660538401381, 0.7756695116564858, 0.8007604178200065, 0.8177370039500107, 0.8290770877054431, 0.8363640685315274, 0.8435058292852845, 0.8489055381816232, 0.8543593780329879, 0.8585215333489518, 0.8633725984636411, 0.865576268738261, 0.8686334831700503, 0.8716851476663439, 0.8744849024284893, 0.8760074531490467, 0.8792855007814394, 0.8796452619936098, 0.8818835035300606, 0.883881081205995, 0.8861567560729213, 0.8867454598319645, 0.884144515945228, 0.890263328191308, 0.8914361628724873, 0.8922842650916555, 0.8926857175956665, 0.894080517100058, 0.8956903954741707, 0.896486533869569, 0.8976214224970602, 0.898564392693217, 0.8991706237236742, 0.8988523175869584, 0.9001846154129185, 0.9011291944427996, 0.9020246729294542, 0.902851848807556, 0.9034150533661189, 0.904256668734304, 0.9044070978038359, 0.9046927907124233, 0.9055076442183664, 0.9059400770934849, 0.9079926631668657, 0.9081480867482927, 0.9067437385895034, 0.9094495124499447, 0.908970853406211, 0.9097467396848499, 0.9103199315843951, 0.9094348799701738, 0.9113206036472842, 0.9111323113833153, 0.9117834950291184, 0.912444225736538, 0.911474280058472, 0.9123404601787294, 0.9131335795749781, 0.9133087366912871, 0.9133808700426456, 0.9138064095751534, 0.9143754646616612, 0.9148258954317738, 0.9151168791441964, 0.9152379225352174, 0.9155458923872026, 0.9167146054837185, 0.9158184144213541, 0.9168189473513376, 0.9171853425805544, 0.9166379921372876, 0.9178542730851748, 0.9176935669893004, 0.9173792291701401, 0.9184926098143997, 0.918856676826774, 0.9189143859510487, 0.9188324914890479, 0.9182840118261945, 0.9197549932687502, 0.9200879948970484, 0.9191380576817599, 0.9199722477807197, 0.9198580887088574, 0.9206110904257486, 0.9198526696792059, 0.9210494251302189, 0.921432630372017, 0.9213737624162746, 0.9210229410767292, 0.9215510943091898, 0.9216865031051902, 0.9218873725101426, 0.921587338302974, 0.9218244823523621, 0.9224810667800837, 0.9227340189480153, 0.9222414281513129, 0.9225415590610795, 0.923166798315384, 0.9234701854586809, 0.9229537789672319, 0.923447480548809, 0.9239544044724322, 0.9244508962882193, 0.9232995384063845, 0.9239831755243556, 0.9240095209712501, 0.9243608927482981, 0.9241364266421718, 0.9247588770680689, 0.9250923480890043, 0.925116968810025, 0.9256320482376797, 0.9250164477753421, 0.9256309248975335, 0.925505916353744, 0.9252402791858371, 0.9257133249221563, 0.9257910219138544, 0.925310184146187, 0.9257379270284118, 0.9257791053892858, 0.9258424428900707, 0.9261517887804114, 0.926297020687796, 0.9264983762165139, 0.9261926839309272, 0.926517692754337, 0.9264701646532458, 0.9258355505931554, 0.9271437377274293, 0.9264349085300991, 0.9272375713799685, 0.9266699049840394, 0.9271492372271601, 0.9270671806823533, 0.9272949152102492, 0.9283533415146734, 0.9274802573178446, 0.9278826914944964, 0.9274465090460903, 0.9278635790819794, 0.9279309186534321, 0.928179844914378]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:00<00:03,  1.10it/s]predicting test subjects:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]predicting test subjects:  60%|██████    | 3/5 [00:01<00:01,  1.64it/s]predicting test subjects:  80%|████████  | 4/5 [00:01<00:00,  2.00it/s]predicting test subjects: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:49,  2.41it/s]predicting train subjects:   1%|          | 2/266 [00:00<01:50,  2.38it/s]predicting train subjects:   1%|          | 3/266 [00:01<01:46,  2.46it/s]predicting train subjects:   2%|▏         | 4/266 [00:01<01:52,  2.34it/s]predicting train subjects:   2%|▏         | 5/266 [00:02<01:49,  2.39it/s]predicting train subjects:   2%|▏         | 6/266 [00:02<01:46,  2.44it/s]predicting train subjects:   3%|▎         | 7/266 [00:02<01:41,  2.56it/s]predicting train subjects:   3%|▎         | 8/266 [00:03<01:34,  2.73it/s]predicting train subjects:   3%|▎         | 9/266 [00:03<01:39,  2.59it/s]predicting train subjects:   4%|▍         | 10/266 [00:03<01:36,  2.66it/s]predicting train subjects:   4%|▍         | 11/266 [00:04<01:41,  2.51it/s]predicting train subjects:   5%|▍         | 12/266 [00:04<01:37,  2.62it/s]predicting train subjects:   5%|▍         | 13/266 [00:05<01:36,  2.61it/s]predicting train subjects:   5%|▌         | 14/266 [00:05<01:36,  2.60it/s]predicting train subjects:   6%|▌         | 15/266 [00:05<01:39,  2.52it/s]predicting train subjects:   6%|▌         | 16/266 [00:06<01:41,  2.46it/s]predicting train subjects:   6%|▋         | 17/266 [00:06<01:35,  2.60it/s]predicting train subjects:   7%|▋         | 18/266 [00:07<01:37,  2.56it/s]predicting train subjects:   7%|▋         | 19/266 [00:07<01:33,  2.64it/s]predicting train subjects:   8%|▊         | 20/266 [00:07<01:34,  2.59it/s]predicting train subjects:   8%|▊         | 21/266 [00:08<01:30,  2.72it/s]predicting train subjects:   8%|▊         | 22/266 [00:08<01:35,  2.56it/s]predicting train subjects:   9%|▊         | 23/266 [00:09<01:35,  2.54it/s]predicting train subjects:   9%|▉         | 24/266 [00:09<01:37,  2.49it/s]predicting train subjects:   9%|▉         | 25/266 [00:09<01:32,  2.61it/s]predicting train subjects:  10%|▉         | 26/266 [00:10<01:29,  2.69it/s]predicting train subjects:  10%|█         | 27/266 [00:10<01:30,  2.63it/s]predicting train subjects:  11%|█         | 28/266 [00:10<01:26,  2.76it/s]predicting train subjects:  11%|█         | 29/266 [00:11<01:26,  2.75it/s]predicting train subjects:  11%|█▏        | 30/266 [00:11<01:26,  2.72it/s]predicting train subjects:  12%|█▏        | 31/266 [00:11<01:22,  2.83it/s]predicting train subjects:  12%|█▏        | 32/266 [00:12<01:27,  2.68it/s]predicting train subjects:  12%|█▏        | 33/266 [00:12<01:26,  2.70it/s]predicting train subjects:  13%|█▎        | 34/266 [00:13<01:24,  2.73it/s]predicting train subjects:  13%|█▎        | 35/266 [00:13<01:26,  2.66it/s]predicting train subjects:  14%|█▎        | 36/266 [00:13<01:26,  2.64it/s]predicting train subjects:  14%|█▍        | 37/266 [00:14<01:27,  2.60it/s]predicting train subjects:  14%|█▍        | 38/266 [00:14<01:23,  2.72it/s]predicting train subjects:  15%|█▍        | 39/266 [00:14<01:24,  2.68it/s]predicting train subjects:  15%|█▌        | 40/266 [00:15<01:21,  2.77it/s]predicting train subjects:  15%|█▌        | 41/266 [00:15<01:23,  2.68it/s]predicting train subjects:  16%|█▌        | 42/266 [00:16<01:21,  2.75it/s]predicting train subjects:  16%|█▌        | 43/266 [00:16<01:17,  2.89it/s]predicting train subjects:  17%|█▋        | 44/266 [00:16<01:14,  2.99it/s]predicting train subjects:  17%|█▋        | 45/266 [00:16<01:11,  3.09it/s]predicting train subjects:  17%|█▋        | 46/266 [00:17<01:10,  3.13it/s]predicting train subjects:  18%|█▊        | 47/266 [00:17<01:10,  3.10it/s]predicting train subjects:  18%|█▊        | 48/266 [00:17<01:12,  3.00it/s]predicting train subjects:  18%|█▊        | 49/266 [00:18<01:10,  3.06it/s]predicting train subjects:  19%|█▉        | 50/266 [00:18<01:13,  2.95it/s]predicting train subjects:  19%|█▉        | 51/266 [00:18<01:11,  3.02it/s]predicting train subjects:  20%|█▉        | 52/266 [00:19<01:09,  3.09it/s]predicting train subjects:  20%|█▉        | 53/266 [00:19<01:11,  2.98it/s]predicting train subjects:  20%|██        | 54/266 [00:19<01:08,  3.11it/s]predicting train subjects:  21%|██        | 55/266 [00:20<01:06,  3.16it/s]predicting train subjects:  21%|██        | 56/266 [00:20<01:08,  3.08it/s]predicting train subjects:  21%|██▏       | 57/266 [00:20<01:11,  2.94it/s]predicting train subjects:  22%|██▏       | 58/266 [00:21<01:07,  3.08it/s]predicting train subjects:  22%|██▏       | 59/266 [00:21<01:03,  3.25it/s]predicting train subjects:  23%|██▎       | 60/266 [00:21<01:05,  3.13it/s]predicting train subjects:  23%|██▎       | 61/266 [00:22<01:06,  3.07it/s]predicting train subjects:  23%|██▎       | 62/266 [00:22<01:04,  3.18it/s]predicting train subjects:  24%|██▎       | 63/266 [00:22<01:02,  3.25it/s]predicting train subjects:  24%|██▍       | 64/266 [00:23<01:05,  3.08it/s]predicting train subjects:  24%|██▍       | 65/266 [00:23<01:05,  3.06it/s]predicting train subjects:  25%|██▍       | 66/266 [00:23<01:00,  3.31it/s]predicting train subjects:  25%|██▌       | 67/266 [00:23<01:01,  3.22it/s]predicting train subjects:  26%|██▌       | 68/266 [00:24<01:02,  3.18it/s]predicting train subjects:  26%|██▌       | 69/266 [00:24<00:58,  3.39it/s]predicting train subjects:  26%|██▋       | 70/266 [00:24<00:54,  3.61it/s]predicting train subjects:  27%|██▋       | 71/266 [00:25<00:54,  3.57it/s]predicting train subjects:  27%|██▋       | 72/266 [00:25<00:57,  3.39it/s]predicting train subjects:  27%|██▋       | 73/266 [00:25<00:54,  3.53it/s]predicting train subjects:  28%|██▊       | 74/266 [00:25<00:53,  3.59it/s]predicting train subjects:  28%|██▊       | 75/266 [00:26<00:53,  3.54it/s]predicting train subjects:  29%|██▊       | 76/266 [00:26<00:54,  3.50it/s]predicting train subjects:  29%|██▉       | 77/266 [00:26<00:50,  3.76it/s]predicting train subjects:  29%|██▉       | 78/266 [00:27<00:52,  3.56it/s]predicting train subjects:  30%|██▉       | 79/266 [00:27<00:56,  3.29it/s]predicting train subjects:  30%|███       | 80/266 [00:27<00:57,  3.21it/s]predicting train subjects:  30%|███       | 81/266 [00:28<00:58,  3.17it/s]predicting train subjects:  31%|███       | 82/266 [00:28<01:02,  2.96it/s]predicting train subjects:  31%|███       | 83/266 [00:28<01:03,  2.89it/s]predicting train subjects:  32%|███▏      | 84/266 [00:29<01:00,  3.01it/s]predicting train subjects:  32%|███▏      | 85/266 [00:29<01:03,  2.86it/s]predicting train subjects:  32%|███▏      | 86/266 [00:29<01:06,  2.71it/s]predicting train subjects:  33%|███▎      | 87/266 [00:30<01:03,  2.84it/s]predicting train subjects:  33%|███▎      | 88/266 [00:30<01:08,  2.62it/s]predicting train subjects:  33%|███▎      | 89/266 [00:31<01:05,  2.72it/s]predicting train subjects:  34%|███▍      | 90/266 [00:31<01:03,  2.77it/s]predicting train subjects:  34%|███▍      | 91/266 [00:31<01:03,  2.74it/s]predicting train subjects:  35%|███▍      | 92/266 [00:32<00:59,  2.91it/s]predicting train subjects:  35%|███▍      | 93/266 [00:32<01:01,  2.81it/s]predicting train subjects:  35%|███▌      | 94/266 [00:32<01:04,  2.66it/s]predicting train subjects:  36%|███▌      | 95/266 [00:33<01:01,  2.79it/s]predicting train subjects:  36%|███▌      | 96/266 [00:33<01:03,  2.69it/s]predicting train subjects:  36%|███▋      | 97/266 [00:33<01:02,  2.69it/s]predicting train subjects:  37%|███▋      | 98/266 [00:34<01:06,  2.55it/s]predicting train subjects:  37%|███▋      | 99/266 [00:34<01:02,  2.67it/s]predicting train subjects:  38%|███▊      | 100/266 [00:34<00:56,  2.96it/s]predicting train subjects:  38%|███▊      | 101/266 [00:35<00:56,  2.93it/s]predicting train subjects:  38%|███▊      | 102/266 [00:35<00:55,  2.98it/s]predicting train subjects:  39%|███▊      | 103/266 [00:35<00:52,  3.13it/s]predicting train subjects:  39%|███▉      | 104/266 [00:36<00:53,  3.01it/s]predicting train subjects:  39%|███▉      | 105/266 [00:36<00:52,  3.08it/s]predicting train subjects:  40%|███▉      | 106/266 [00:36<00:48,  3.31it/s]predicting train subjects:  40%|████      | 107/266 [00:37<00:47,  3.34it/s]predicting train subjects:  41%|████      | 108/266 [00:37<00:48,  3.24it/s]predicting train subjects:  41%|████      | 109/266 [00:37<00:49,  3.17it/s]predicting train subjects:  41%|████▏     | 110/266 [00:38<00:48,  3.21it/s]predicting train subjects:  42%|████▏     | 111/266 [00:38<00:47,  3.28it/s]predicting train subjects:  42%|████▏     | 112/266 [00:38<00:46,  3.30it/s]predicting train subjects:  42%|████▏     | 113/266 [00:38<00:44,  3.43it/s]predicting train subjects:  43%|████▎     | 114/266 [00:39<00:41,  3.71it/s]predicting train subjects:  43%|████▎     | 115/266 [00:39<00:39,  3.84it/s]predicting train subjects:  44%|████▎     | 116/266 [00:39<00:37,  4.04it/s]predicting train subjects:  44%|████▍     | 117/266 [00:39<00:36,  4.13it/s]predicting train subjects:  44%|████▍     | 118/266 [00:40<00:35,  4.12it/s]predicting train subjects:  45%|████▍     | 119/266 [00:40<00:36,  3.98it/s]predicting train subjects:  45%|████▌     | 120/266 [00:40<00:38,  3.76it/s]predicting train subjects:  45%|████▌     | 121/266 [00:40<00:40,  3.59it/s]predicting train subjects:  46%|████▌     | 122/266 [00:41<00:38,  3.73it/s]predicting train subjects:  46%|████▌     | 123/266 [00:41<00:37,  3.81it/s]predicting train subjects:  47%|████▋     | 124/266 [00:41<00:38,  3.65it/s]predicting train subjects:  47%|████▋     | 125/266 [00:42<00:37,  3.76it/s]predicting train subjects:  47%|████▋     | 126/266 [00:42<00:36,  3.84it/s]predicting train subjects:  48%|████▊     | 127/266 [00:42<00:37,  3.66it/s]predicting train subjects:  48%|████▊     | 128/266 [00:42<00:36,  3.74it/s]predicting train subjects:  48%|████▊     | 129/266 [00:43<00:36,  3.80it/s]predicting train subjects:  49%|████▉     | 130/266 [00:43<00:35,  3.87it/s]predicting train subjects:  49%|████▉     | 131/266 [00:43<00:34,  3.90it/s]predicting train subjects:  50%|████▉     | 132/266 [00:43<00:34,  3.91it/s]predicting train subjects:  50%|█████     | 133/266 [00:44<00:34,  3.82it/s]predicting train subjects:  50%|█████     | 134/266 [00:44<00:35,  3.72it/s]predicting train subjects:  51%|█████     | 135/266 [00:44<00:34,  3.79it/s]predicting train subjects:  51%|█████     | 136/266 [00:44<00:35,  3.62it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:45<00:35,  3.64it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:45<00:33,  3.81it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:45<00:32,  3.92it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:45<00:31,  3.99it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:46<00:30,  4.06it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:46<00:30,  4.12it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:46<00:29,  4.17it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:46<00:29,  4.12it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:47<00:29,  4.16it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:47<00:29,  4.09it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:47<00:28,  4.13it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:47<00:28,  4.16it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:48<00:27,  4.19it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:48<00:27,  4.21it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:48<00:27,  4.24it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:48<00:26,  4.23it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:49<00:26,  4.19it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:49<00:26,  4.22it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:49<00:25,  4.44it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:49<00:23,  4.75it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:49<00:21,  4.98it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:49<00:20,  5.17it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:50<00:20,  5.26it/s]predicting train subjects:  60%|██████    | 160/266 [00:50<00:19,  5.37it/s]predicting train subjects:  61%|██████    | 161/266 [00:50<00:19,  5.43it/s]predicting train subjects:  61%|██████    | 162/266 [00:50<00:20,  5.19it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:50<00:19,  5.32it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:51<00:19,  5.30it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:51<00:18,  5.41it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:51<00:18,  5.51it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:51<00:17,  5.58it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:51<00:17,  5.63it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:51<00:17,  5.60it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:52<00:17,  5.64it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:52<00:16,  5.67it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:52<00:17,  5.33it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:52<00:18,  5.12it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:52<00:19,  4.82it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:53<00:18,  4.83it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:53<00:18,  4.76it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:53<00:20,  4.45it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:53<00:20,  4.22it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:54<00:19,  4.40it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:54<00:20,  4.23it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:54<00:21,  3.89it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:54<00:20,  4.16it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:55<00:19,  4.29it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:55<00:18,  4.47it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:55<00:17,  4.64it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:55<00:18,  4.33it/s]predicting train subjects:  70%|███████   | 187/266 [00:56<00:21,  3.65it/s]predicting train subjects:  71%|███████   | 188/266 [00:56<00:21,  3.64it/s]predicting train subjects:  71%|███████   | 189/266 [00:56<00:22,  3.36it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:57<00:27,  2.81it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:57<00:26,  2.78it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:58<00:27,  2.71it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:58<00:29,  2.48it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:59<00:31,  2.29it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:59<00:30,  2.35it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:59<00:31,  2.22it/s]predicting train subjects:  74%|███████▍  | 197/266 [01:00<00:33,  2.07it/s]predicting train subjects:  74%|███████▍  | 198/266 [01:00<00:32,  2.10it/s]predicting train subjects:  75%|███████▍  | 199/266 [01:01<00:31,  2.16it/s]predicting train subjects:  75%|███████▌  | 200/266 [01:01<00:27,  2.41it/s]predicting train subjects:  76%|███████▌  | 201/266 [01:01<00:23,  2.76it/s]predicting train subjects:  76%|███████▌  | 202/266 [01:02<00:21,  2.96it/s]predicting train subjects:  76%|███████▋  | 203/266 [01:02<00:20,  3.04it/s]predicting train subjects:  77%|███████▋  | 204/266 [01:02<00:19,  3.12it/s]predicting train subjects:  77%|███████▋  | 205/266 [01:03<00:18,  3.36it/s]predicting train subjects:  77%|███████▋  | 206/266 [01:03<00:17,  3.46it/s]predicting train subjects:  78%|███████▊  | 207/266 [01:03<00:16,  3.48it/s]predicting train subjects:  78%|███████▊  | 208/266 [01:04<00:19,  3.01it/s]predicting train subjects:  79%|███████▊  | 209/266 [01:04<00:21,  2.69it/s]predicting train subjects:  79%|███████▉  | 210/266 [01:04<00:19,  2.87it/s]predicting train subjects:  79%|███████▉  | 211/266 [01:05<00:18,  2.99it/s]predicting train subjects:  80%|███████▉  | 212/266 [01:05<00:19,  2.81it/s]predicting train subjects:  80%|████████  | 213/266 [01:05<00:19,  2.78it/s]predicting train subjects:  80%|████████  | 214/266 [01:06<00:18,  2.82it/s]predicting train subjects:  81%|████████  | 215/266 [01:06<00:16,  3.08it/s]predicting train subjects:  81%|████████  | 216/266 [01:06<00:15,  3.15it/s]predicting train subjects:  82%|████████▏ | 217/266 [01:07<00:16,  2.99it/s]predicting train subjects:  82%|████████▏ | 218/266 [01:07<00:16,  2.86it/s]predicting train subjects:  82%|████████▏ | 219/266 [01:07<00:15,  2.99it/s]predicting train subjects:  83%|████████▎ | 220/266 [01:08<00:14,  3.23it/s]predicting train subjects:  83%|████████▎ | 221/266 [01:08<00:13,  3.34it/s]predicting train subjects:  83%|████████▎ | 222/266 [01:08<00:13,  3.24it/s]predicting train subjects:  84%|████████▍ | 223/266 [01:09<00:14,  2.98it/s]predicting train subjects:  84%|████████▍ | 224/266 [01:09<00:12,  3.31it/s]predicting train subjects:  85%|████████▍ | 225/266 [01:09<00:11,  3.52it/s]predicting train subjects:  85%|████████▍ | 226/266 [01:09<00:11,  3.35it/s]predicting train subjects:  85%|████████▌ | 227/266 [01:10<00:12,  3.06it/s]predicting train subjects:  86%|████████▌ | 228/266 [01:10<00:13,  2.87it/s]predicting train subjects:  86%|████████▌ | 229/266 [01:11<00:13,  2.77it/s]predicting train subjects:  86%|████████▋ | 230/266 [01:11<00:12,  2.93it/s]predicting train subjects:  87%|████████▋ | 231/266 [01:11<00:11,  2.97it/s]predicting train subjects:  87%|████████▋ | 232/266 [01:11<00:10,  3.39it/s]predicting train subjects:  88%|████████▊ | 233/266 [01:12<00:08,  3.77it/s]predicting train subjects:  88%|████████▊ | 234/266 [01:12<00:07,  4.03it/s]predicting train subjects:  88%|████████▊ | 235/266 [01:12<00:07,  4.07it/s]predicting train subjects:  89%|████████▊ | 236/266 [01:12<00:07,  3.99it/s]predicting train subjects:  89%|████████▉ | 237/266 [01:13<00:06,  4.15it/s]predicting train subjects:  89%|████████▉ | 238/266 [01:13<00:06,  4.04it/s]predicting train subjects:  90%|████████▉ | 239/266 [01:13<00:06,  4.32it/s]predicting train subjects:  90%|█████████ | 240/266 [01:13<00:05,  4.54it/s]predicting train subjects:  91%|█████████ | 241/266 [01:13<00:05,  4.66it/s]predicting train subjects:  91%|█████████ | 242/266 [01:14<00:04,  4.81it/s]predicting train subjects:  91%|█████████▏| 243/266 [01:14<00:04,  4.75it/s]predicting train subjects:  92%|█████████▏| 244/266 [01:14<00:04,  4.87it/s]predicting train subjects:  92%|█████████▏| 245/266 [01:14<00:04,  4.94it/s]predicting train subjects:  92%|█████████▏| 246/266 [01:14<00:04,  4.90it/s]predicting train subjects:  93%|█████████▎| 247/266 [01:15<00:04,  4.73it/s]predicting train subjects:  93%|█████████▎| 248/266 [01:15<00:03,  4.75it/s]predicting train subjects:  94%|█████████▎| 249/266 [01:15<00:03,  4.42it/s]predicting train subjects:  94%|█████████▍| 250/266 [01:15<00:03,  4.27it/s]predicting train subjects:  94%|█████████▍| 251/266 [01:16<00:03,  4.03it/s]predicting train subjects:  95%|█████████▍| 252/266 [01:16<00:03,  4.07it/s]predicting train subjects:  95%|█████████▌| 253/266 [01:16<00:03,  4.13it/s]predicting train subjects:  95%|█████████▌| 254/266 [01:16<00:03,  3.99it/s]predicting train subjects:  96%|█████████▌| 255/266 [01:17<00:02,  4.01it/s]predicting train subjects:  96%|█████████▌| 256/266 [01:17<00:02,  4.04it/s]predicting train subjects:  97%|█████████▋| 257/266 [01:17<00:02,  3.82it/s]predicting train subjects:  97%|█████████▋| 258/266 [01:17<00:02,  3.89it/s]predicting train subjects:  97%|█████████▋| 259/266 [01:18<00:01,  3.95it/s]predicting train subjects:  98%|█████████▊| 260/266 [01:18<00:01,  4.00it/s]predicting train subjects:  98%|█████████▊| 261/266 [01:18<00:01,  3.82it/s]predicting train subjects:  98%|█████████▊| 262/266 [01:18<00:01,  3.84it/s]predicting train subjects:  99%|█████████▉| 263/266 [01:19<00:00,  3.90it/s]predicting train subjects:  99%|█████████▉| 264/266 [01:19<00:00,  3.98it/s]predicting train subjects: 100%|█████████▉| 265/266 [01:19<00:00,  3.99it/s]predicting train subjects: 100%|██████████| 266/266 [01:19<00:00,  3.78it/s]
predicting test subjects sagittal:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects sagittal:  20%|██        | 1/5 [00:00<00:00,  4.17it/s]predicting test subjects sagittal:  40%|████      | 2/5 [00:00<00:00,  4.24it/s]predicting test subjects sagittal:  60%|██████    | 3/5 [00:00<00:00,  4.51it/s]predicting test subjects sagittal:  80%|████████  | 4/5 [00:00<00:00,  4.73it/s]predicting test subjects sagittal: 100%|██████████| 5/5 [00:01<00:00,  4.70it/s]
predicting train subjects sagittal:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/266 [00:00<01:22,  3.21it/s]predicting train subjects sagittal:   1%|          | 2/266 [00:00<01:22,  3.20it/s]predicting train subjects sagittal:   1%|          | 3/266 [00:00<01:15,  3.50it/s]predicting train subjects sagittal:   2%|▏         | 4/266 [00:01<01:07,  3.89it/s]predicting train subjects sagittal:   2%|▏         | 5/266 [00:01<01:06,  3.92it/s]predicting train subjects sagittal:   2%|▏         | 6/266 [00:01<01:05,  3.94it/s]predicting train subjects sagittal:   3%|▎         | 7/266 [00:01<01:05,  3.96it/s]predicting train subjects sagittal:   3%|▎         | 8/266 [00:02<01:06,  3.89it/s]predicting train subjects sagittal:   3%|▎         | 9/266 [00:02<01:10,  3.62it/s]predicting train subjects sagittal:   4%|▍         | 10/266 [00:02<01:10,  3.65it/s]predicting train subjects sagittal:   4%|▍         | 11/266 [00:02<01:08,  3.75it/s]predicting train subjects sagittal:   5%|▍         | 12/266 [00:03<01:06,  3.83it/s]predicting train subjects sagittal:   5%|▍         | 13/266 [00:03<01:07,  3.77it/s]predicting train subjects sagittal:   5%|▌         | 14/266 [00:03<01:06,  3.82it/s]predicting train subjects sagittal:   6%|▌         | 15/266 [00:03<01:06,  3.78it/s]predicting train subjects sagittal:   6%|▌         | 16/266 [00:04<01:07,  3.70it/s]predicting train subjects sagittal:   6%|▋         | 17/266 [00:04<01:06,  3.72it/s]predicting train subjects sagittal:   7%|▋         | 18/266 [00:04<01:05,  3.78it/s]predicting train subjects sagittal:   7%|▋         | 19/266 [00:05<01:09,  3.56it/s]predicting train subjects sagittal:   8%|▊         | 20/266 [00:05<01:13,  3.36it/s]predicting train subjects sagittal:   8%|▊         | 21/266 [00:05<01:09,  3.53it/s]predicting train subjects sagittal:   8%|▊         | 22/266 [00:05<01:07,  3.61it/s]predicting train subjects sagittal:   9%|▊         | 23/266 [00:06<01:11,  3.41it/s]predicting train subjects sagittal:   9%|▉         | 24/266 [00:06<01:12,  3.32it/s]predicting train subjects sagittal:   9%|▉         | 25/266 [00:06<01:09,  3.47it/s]predicting train subjects sagittal:  10%|▉         | 26/266 [00:07<01:05,  3.67it/s]predicting train subjects sagittal:  10%|█         | 27/266 [00:07<01:02,  3.83it/s]predicting train subjects sagittal:  11%|█         | 28/266 [00:07<01:01,  3.87it/s]predicting train subjects sagittal:  11%|█         | 29/266 [00:07<00:59,  3.97it/s]predicting train subjects sagittal:  11%|█▏        | 30/266 [00:08<00:59,  3.98it/s]predicting train subjects sagittal:  12%|█▏        | 31/266 [00:08<00:59,  3.94it/s]predicting train subjects sagittal:  12%|█▏        | 32/266 [00:08<00:59,  3.96it/s]predicting train subjects sagittal:  12%|█▏        | 33/266 [00:08<01:03,  3.64it/s]predicting train subjects sagittal:  13%|█▎        | 34/266 [00:09<01:04,  3.60it/s]predicting train subjects sagittal:  13%|█▎        | 35/266 [00:09<01:03,  3.66it/s]predicting train subjects sagittal:  14%|█▎        | 36/266 [00:09<01:05,  3.53it/s]predicting train subjects sagittal:  14%|█▍        | 37/266 [00:10<01:04,  3.53it/s]predicting train subjects sagittal:  14%|█▍        | 38/266 [00:10<01:02,  3.65it/s]predicting train subjects sagittal:  15%|█▍        | 39/266 [00:10<00:59,  3.81it/s]predicting train subjects sagittal:  15%|█▌        | 40/266 [00:10<00:59,  3.83it/s]predicting train subjects sagittal:  15%|█▌        | 41/266 [00:11<00:58,  3.83it/s]predicting train subjects sagittal:  16%|█▌        | 42/266 [00:11<00:58,  3.86it/s]predicting train subjects sagittal:  16%|█▌        | 43/266 [00:11<00:54,  4.09it/s]predicting train subjects sagittal:  17%|█▋        | 44/266 [00:11<00:52,  4.26it/s]predicting train subjects sagittal:  17%|█▋        | 45/266 [00:11<00:51,  4.30it/s]predicting train subjects sagittal:  17%|█▋        | 46/266 [00:12<00:52,  4.20it/s]predicting train subjects sagittal:  18%|█▊        | 47/266 [00:12<00:53,  4.10it/s]predicting train subjects sagittal:  18%|█▊        | 48/266 [00:12<00:50,  4.30it/s]predicting train subjects sagittal:  18%|█▊        | 49/266 [00:12<00:49,  4.34it/s]predicting train subjects sagittal:  19%|█▉        | 50/266 [00:13<00:48,  4.48it/s]predicting train subjects sagittal:  19%|█▉        | 51/266 [00:13<00:46,  4.62it/s]predicting train subjects sagittal:  20%|█▉        | 52/266 [00:13<00:44,  4.76it/s]predicting train subjects sagittal:  20%|█▉        | 53/266 [00:13<00:44,  4.83it/s]predicting train subjects sagittal:  20%|██        | 54/266 [00:13<00:43,  4.92it/s]predicting train subjects sagittal:  21%|██        | 55/266 [00:14<00:42,  4.98it/s]predicting train subjects sagittal:  21%|██        | 56/266 [00:14<00:42,  4.99it/s]predicting train subjects sagittal:  21%|██▏       | 57/266 [00:14<00:42,  4.97it/s]predicting train subjects sagittal:  22%|██▏       | 58/266 [00:14<00:42,  4.89it/s]predicting train subjects sagittal:  22%|██▏       | 59/266 [00:14<00:42,  4.84it/s]predicting train subjects sagittal:  23%|██▎       | 60/266 [00:15<00:44,  4.67it/s]predicting train subjects sagittal:  23%|██▎       | 61/266 [00:15<00:43,  4.70it/s]predicting train subjects sagittal:  23%|██▎       | 62/266 [00:15<00:42,  4.75it/s]predicting train subjects sagittal:  24%|██▎       | 63/266 [00:15<00:44,  4.55it/s]predicting train subjects sagittal:  24%|██▍       | 64/266 [00:15<00:42,  4.72it/s]predicting train subjects sagittal:  24%|██▍       | 65/266 [00:16<00:43,  4.59it/s]predicting train subjects sagittal:  25%|██▍       | 66/266 [00:16<00:42,  4.75it/s]predicting train subjects sagittal:  25%|██▌       | 67/266 [00:16<00:40,  4.89it/s]predicting train subjects sagittal:  26%|██▌       | 68/266 [00:16<00:39,  5.00it/s]predicting train subjects sagittal:  26%|██▌       | 69/266 [00:16<00:40,  4.91it/s]predicting train subjects sagittal:  26%|██▋       | 70/266 [00:17<00:39,  4.94it/s]predicting train subjects sagittal:  27%|██▋       | 71/266 [00:17<00:38,  5.03it/s]predicting train subjects sagittal:  27%|██▋       | 72/266 [00:17<00:37,  5.11it/s]predicting train subjects sagittal:  27%|██▋       | 73/266 [00:17<00:38,  5.01it/s]predicting train subjects sagittal:  28%|██▊       | 74/266 [00:17<00:38,  5.03it/s]predicting train subjects sagittal:  28%|██▊       | 75/266 [00:18<00:37,  5.08it/s]predicting train subjects sagittal:  29%|██▊       | 76/266 [00:18<00:38,  4.88it/s]predicting train subjects sagittal:  29%|██▉       | 77/266 [00:18<00:37,  4.99it/s]predicting train subjects sagittal:  29%|██▉       | 78/266 [00:18<00:41,  4.57it/s]predicting train subjects sagittal:  30%|██▉       | 79/266 [00:19<00:42,  4.43it/s]predicting train subjects sagittal:  30%|███       | 80/266 [00:19<00:43,  4.27it/s]predicting train subjects sagittal:  30%|███       | 81/266 [00:19<00:45,  4.08it/s]predicting train subjects sagittal:  31%|███       | 82/266 [00:19<00:48,  3.78it/s]predicting train subjects sagittal:  31%|███       | 83/266 [00:20<00:46,  3.89it/s]predicting train subjects sagittal:  32%|███▏      | 84/266 [00:20<00:45,  3.96it/s]predicting train subjects sagittal:  32%|███▏      | 85/266 [00:20<00:45,  3.99it/s]predicting train subjects sagittal:  32%|███▏      | 86/266 [00:20<00:46,  3.90it/s]predicting train subjects sagittal:  33%|███▎      | 87/266 [00:21<00:45,  3.89it/s]predicting train subjects sagittal:  33%|███▎      | 88/266 [00:21<00:45,  3.88it/s]predicting train subjects sagittal:  33%|███▎      | 89/266 [00:21<00:45,  3.93it/s]predicting train subjects sagittal:  34%|███▍      | 90/266 [00:21<00:45,  3.87it/s]predicting train subjects sagittal:  34%|███▍      | 91/266 [00:22<00:45,  3.88it/s]predicting train subjects sagittal:  35%|███▍      | 92/266 [00:22<00:44,  3.93it/s]predicting train subjects sagittal:  35%|███▍      | 93/266 [00:22<00:43,  3.99it/s]predicting train subjects sagittal:  35%|███▌      | 94/266 [00:22<00:44,  3.90it/s]predicting train subjects sagittal:  36%|███▌      | 95/266 [00:23<00:44,  3.88it/s]predicting train subjects sagittal:  36%|███▌      | 96/266 [00:23<00:41,  4.12it/s]predicting train subjects sagittal:  36%|███▋      | 97/266 [00:23<00:41,  4.05it/s]predicting train subjects sagittal:  37%|███▋      | 98/266 [00:23<00:42,  3.94it/s]predicting train subjects sagittal:  37%|███▋      | 99/266 [00:24<00:38,  4.29it/s]predicting train subjects sagittal:  38%|███▊      | 100/266 [00:24<00:36,  4.50it/s]predicting train subjects sagittal:  38%|███▊      | 101/266 [00:24<00:36,  4.53it/s]predicting train subjects sagittal:  38%|███▊      | 102/266 [00:24<00:35,  4.62it/s]predicting train subjects sagittal:  39%|███▊      | 103/266 [00:24<00:35,  4.63it/s]predicting train subjects sagittal:  39%|███▉      | 104/266 [00:25<00:34,  4.66it/s]predicting train subjects sagittal:  39%|███▉      | 105/266 [00:25<00:34,  4.67it/s]predicting train subjects sagittal:  40%|███▉      | 106/266 [00:25<00:33,  4.73it/s]predicting train subjects sagittal:  40%|████      | 107/266 [00:25<00:33,  4.72it/s]predicting train subjects sagittal:  41%|████      | 108/266 [00:26<00:34,  4.55it/s]predicting train subjects sagittal:  41%|████      | 109/266 [00:26<00:34,  4.59it/s]predicting train subjects sagittal:  41%|████▏     | 110/266 [00:26<00:34,  4.50it/s]predicting train subjects sagittal:  42%|████▏     | 111/266 [00:26<00:35,  4.38it/s]predicting train subjects sagittal:  42%|████▏     | 112/266 [00:26<00:35,  4.35it/s]predicting train subjects sagittal:  42%|████▏     | 113/266 [00:27<00:36,  4.17it/s]predicting train subjects sagittal:  43%|████▎     | 114/266 [00:27<00:34,  4.34it/s]predicting train subjects sagittal:  43%|████▎     | 115/266 [00:27<00:34,  4.40it/s]predicting train subjects sagittal:  44%|████▎     | 116/266 [00:27<00:33,  4.51it/s]predicting train subjects sagittal:  44%|████▍     | 117/266 [00:28<00:32,  4.57it/s]predicting train subjects sagittal:  44%|████▍     | 118/266 [00:28<00:32,  4.57it/s]predicting train subjects sagittal:  45%|████▍     | 119/266 [00:28<00:33,  4.34it/s]predicting train subjects sagittal:  45%|████▌     | 120/266 [00:28<00:34,  4.18it/s]predicting train subjects sagittal:  45%|████▌     | 121/266 [00:29<00:35,  4.07it/s]predicting train subjects sagittal:  46%|████▌     | 122/266 [00:29<00:36,  3.97it/s]predicting train subjects sagittal:  46%|████▌     | 123/266 [00:29<00:36,  3.93it/s]predicting train subjects sagittal:  47%|████▋     | 124/266 [00:29<00:36,  3.92it/s]predicting train subjects sagittal:  47%|████▋     | 125/266 [00:30<00:35,  3.98it/s]predicting train subjects sagittal:  47%|████▋     | 126/266 [00:30<00:35,  3.91it/s]predicting train subjects sagittal:  48%|████▊     | 127/266 [00:30<00:35,  3.91it/s]predicting train subjects sagittal:  48%|████▊     | 128/266 [00:30<00:35,  3.86it/s]predicting train subjects sagittal:  48%|████▊     | 129/266 [00:31<00:34,  3.93it/s]predicting train subjects sagittal:  49%|████▉     | 130/266 [00:31<00:34,  3.96it/s]predicting train subjects sagittal:  49%|████▉     | 131/266 [00:31<00:34,  3.95it/s]predicting train subjects sagittal:  50%|████▉     | 132/266 [00:31<00:34,  3.93it/s]predicting train subjects sagittal:  50%|█████     | 133/266 [00:32<00:34,  3.82it/s]predicting train subjects sagittal:  50%|█████     | 134/266 [00:32<00:34,  3.79it/s]predicting train subjects sagittal:  51%|█████     | 135/266 [00:32<00:34,  3.81it/s]predicting train subjects sagittal:  51%|█████     | 136/266 [00:32<00:33,  3.86it/s]predicting train subjects sagittal:  52%|█████▏    | 137/266 [00:33<00:32,  3.92it/s]predicting train subjects sagittal:  52%|█████▏    | 138/266 [00:33<00:32,  3.99it/s]predicting train subjects sagittal:  52%|█████▏    | 139/266 [00:33<00:32,  3.94it/s]predicting train subjects sagittal:  53%|█████▎    | 140/266 [00:33<00:31,  3.97it/s]predicting train subjects sagittal:  53%|█████▎    | 141/266 [00:34<00:31,  4.01it/s]predicting train subjects sagittal:  53%|█████▎    | 142/266 [00:34<00:30,  4.03it/s]predicting train subjects sagittal:  54%|█████▍    | 143/266 [00:34<00:30,  4.03it/s]predicting train subjects sagittal:  54%|█████▍    | 144/266 [00:34<00:30,  4.01it/s]predicting train subjects sagittal:  55%|█████▍    | 145/266 [00:35<00:29,  4.08it/s]predicting train subjects sagittal:  55%|█████▍    | 146/266 [00:35<00:29,  4.12it/s]predicting train subjects sagittal:  55%|█████▌    | 147/266 [00:35<00:28,  4.11it/s]predicting train subjects sagittal:  56%|█████▌    | 148/266 [00:35<00:28,  4.14it/s]predicting train subjects sagittal:  56%|█████▌    | 149/266 [00:36<00:28,  4.13it/s]predicting train subjects sagittal:  56%|█████▋    | 150/266 [00:36<00:28,  4.12it/s]predicting train subjects sagittal:  57%|█████▋    | 151/266 [00:36<00:27,  4.13it/s]predicting train subjects sagittal:  57%|█████▋    | 152/266 [00:36<00:27,  4.16it/s]predicting train subjects sagittal:  58%|█████▊    | 153/266 [00:37<00:27,  4.18it/s]predicting train subjects sagittal:  58%|█████▊    | 154/266 [00:37<00:26,  4.22it/s]predicting train subjects sagittal:  58%|█████▊    | 155/266 [00:37<00:24,  4.54it/s]predicting train subjects sagittal:  59%|█████▊    | 156/266 [00:37<00:22,  4.82it/s]predicting train subjects sagittal:  59%|█████▉    | 157/266 [00:37<00:21,  5.03it/s]predicting train subjects sagittal:  59%|█████▉    | 158/266 [00:38<00:21,  5.14it/s]predicting train subjects sagittal:  60%|█████▉    | 159/266 [00:38<00:20,  5.30it/s]predicting train subjects sagittal:  60%|██████    | 160/266 [00:38<00:20,  5.23it/s]predicting train subjects sagittal:  61%|██████    | 161/266 [00:38<00:19,  5.35it/s]predicting train subjects sagittal:  61%|██████    | 162/266 [00:38<00:18,  5.49it/s]predicting train subjects sagittal:  61%|██████▏   | 163/266 [00:38<00:18,  5.46it/s]predicting train subjects sagittal:  62%|██████▏   | 164/266 [00:39<00:18,  5.48it/s]predicting train subjects sagittal:  62%|██████▏   | 165/266 [00:39<00:18,  5.49it/s]predicting train subjects sagittal:  62%|██████▏   | 166/266 [00:39<00:18,  5.53it/s]predicting train subjects sagittal:  63%|██████▎   | 167/266 [00:39<00:18,  5.49it/s]predicting train subjects sagittal:  63%|██████▎   | 168/266 [00:39<00:17,  5.54it/s]predicting train subjects sagittal:  64%|██████▎   | 169/266 [00:40<00:17,  5.55it/s]predicting train subjects sagittal:  64%|██████▍   | 170/266 [00:40<00:18,  5.11it/s]predicting train subjects sagittal:  64%|██████▍   | 171/266 [00:40<00:17,  5.28it/s]predicting train subjects sagittal:  65%|██████▍   | 172/266 [00:40<00:18,  4.97it/s]predicting train subjects sagittal:  65%|██████▌   | 173/266 [00:40<00:20,  4.60it/s]predicting train subjects sagittal:  65%|██████▌   | 174/266 [00:41<00:20,  4.52it/s]predicting train subjects sagittal:  66%|██████▌   | 175/266 [00:41<00:20,  4.51it/s]predicting train subjects sagittal:  66%|██████▌   | 176/266 [00:41<00:20,  4.45it/s]predicting train subjects sagittal:  67%|██████▋   | 177/266 [00:41<00:21,  4.23it/s]predicting train subjects sagittal:  67%|██████▋   | 178/266 [00:42<00:21,  4.12it/s]predicting train subjects sagittal:  67%|██████▋   | 179/266 [00:42<00:21,  4.03it/s]predicting train subjects sagittal:  68%|██████▊   | 180/266 [00:42<00:20,  4.17it/s]predicting train subjects sagittal:  68%|██████▊   | 181/266 [00:42<00:19,  4.41it/s]predicting train subjects sagittal:  68%|██████▊   | 182/266 [00:43<00:18,  4.49it/s]predicting train subjects sagittal:  69%|██████▉   | 183/266 [00:43<00:17,  4.62it/s]predicting train subjects sagittal:  69%|██████▉   | 184/266 [00:43<00:17,  4.74it/s]predicting train subjects sagittal:  70%|██████▉   | 185/266 [00:43<00:16,  4.83it/s]predicting train subjects sagittal:  70%|██████▉   | 186/266 [00:43<00:16,  4.89it/s]predicting train subjects sagittal:  70%|███████   | 187/266 [00:44<00:17,  4.58it/s]predicting train subjects sagittal:  71%|███████   | 188/266 [00:44<00:16,  4.67it/s]predicting train subjects sagittal:  71%|███████   | 189/266 [00:44<00:16,  4.74it/s]predicting train subjects sagittal:  71%|███████▏  | 190/266 [00:44<00:15,  4.76it/s]predicting train subjects sagittal:  72%|███████▏  | 191/266 [00:44<00:15,  4.74it/s]predicting train subjects sagittal:  72%|███████▏  | 192/266 [00:45<00:15,  4.92it/s]predicting train subjects sagittal:  73%|███████▎  | 193/266 [00:45<00:16,  4.52it/s]predicting train subjects sagittal:  73%|███████▎  | 194/266 [00:45<00:17,  4.18it/s]predicting train subjects sagittal:  73%|███████▎  | 195/266 [00:45<00:16,  4.38it/s]predicting train subjects sagittal:  74%|███████▎  | 196/266 [00:46<00:15,  4.40it/s]predicting train subjects sagittal:  74%|███████▍  | 197/266 [00:46<00:15,  4.39it/s]predicting train subjects sagittal:  74%|███████▍  | 198/266 [00:46<00:15,  4.45it/s]predicting train subjects sagittal:  75%|███████▍  | 199/266 [00:46<00:14,  4.51it/s]predicting train subjects sagittal:  75%|███████▌  | 200/266 [00:46<00:15,  4.30it/s]predicting train subjects sagittal:  76%|███████▌  | 201/266 [00:47<00:15,  4.16it/s]predicting train subjects sagittal:  76%|███████▌  | 202/266 [00:47<00:15,  4.06it/s]predicting train subjects sagittal:  76%|███████▋  | 203/266 [00:47<00:14,  4.23it/s]predicting train subjects sagittal:  77%|███████▋  | 204/266 [00:47<00:14,  4.39it/s]predicting train subjects sagittal:  77%|███████▋  | 205/266 [00:48<00:14,  4.24it/s]predicting train subjects sagittal:  77%|███████▋  | 206/266 [00:48<00:14,  4.11it/s]predicting train subjects sagittal:  78%|███████▊  | 207/266 [00:48<00:14,  3.96it/s]predicting train subjects sagittal:  78%|███████▊  | 208/266 [00:48<00:15,  3.84it/s]predicting train subjects sagittal:  79%|███████▊  | 209/266 [00:49<00:14,  3.94it/s]predicting train subjects sagittal:  79%|███████▉  | 210/266 [00:49<00:14,  3.90it/s]predicting train subjects sagittal:  79%|███████▉  | 211/266 [00:49<00:14,  3.82it/s]predicting train subjects sagittal:  80%|███████▉  | 212/266 [00:50<00:14,  3.82it/s]predicting train subjects sagittal:  80%|████████  | 213/266 [00:50<00:13,  3.90it/s]predicting train subjects sagittal:  80%|████████  | 214/266 [00:50<00:13,  3.98it/s]predicting train subjects sagittal:  81%|████████  | 215/266 [00:50<00:11,  4.26it/s]predicting train subjects sagittal:  81%|████████  | 216/266 [00:50<00:10,  4.59it/s]predicting train subjects sagittal:  82%|████████▏ | 217/266 [00:51<00:10,  4.84it/s]predicting train subjects sagittal:  82%|████████▏ | 218/266 [00:51<00:09,  4.99it/s]predicting train subjects sagittal:  82%|████████▏ | 219/266 [00:51<00:09,  4.97it/s]predicting train subjects sagittal:  83%|████████▎ | 220/266 [00:51<00:09,  5.07it/s]predicting train subjects sagittal:  83%|████████▎ | 221/266 [00:51<00:08,  5.09it/s]predicting train subjects sagittal:  83%|████████▎ | 222/266 [00:52<00:08,  4.90it/s]predicting train subjects sagittal:  84%|████████▍ | 223/266 [00:52<00:08,  4.83it/s]predicting train subjects sagittal:  84%|████████▍ | 224/266 [00:52<00:08,  4.84it/s]predicting train subjects sagittal:  85%|████████▍ | 225/266 [00:52<00:08,  4.98it/s]predicting train subjects sagittal:  85%|████████▍ | 226/266 [00:52<00:07,  5.07it/s]predicting train subjects sagittal:  85%|████████▌ | 227/266 [00:53<00:07,  5.17it/s]predicting train subjects sagittal:  86%|████████▌ | 228/266 [00:53<00:07,  5.25it/s]predicting train subjects sagittal:  86%|████████▌ | 229/266 [00:53<00:07,  5.27it/s]predicting train subjects sagittal:  86%|████████▋ | 230/266 [00:53<00:07,  4.95it/s]predicting train subjects sagittal:  87%|████████▋ | 231/266 [00:53<00:07,  4.97it/s]predicting train subjects sagittal:  87%|████████▋ | 232/266 [00:54<00:06,  5.03it/s]predicting train subjects sagittal:  88%|████████▊ | 233/266 [00:54<00:06,  4.90it/s]predicting train subjects sagittal:  88%|████████▊ | 234/266 [00:54<00:06,  4.98it/s]predicting train subjects sagittal:  88%|████████▊ | 235/266 [00:54<00:06,  5.00it/s]predicting train subjects sagittal:  89%|████████▊ | 236/266 [00:54<00:06,  4.83it/s]predicting train subjects sagittal:  89%|████████▉ | 237/266 [00:55<00:05,  4.93it/s]predicting train subjects sagittal:  89%|████████▉ | 238/266 [00:55<00:05,  4.88it/s]predicting train subjects sagittal:  90%|████████▉ | 239/266 [00:55<00:05,  4.97it/s]predicting train subjects sagittal:  90%|█████████ | 240/266 [00:55<00:05,  5.04it/s]predicting train subjects sagittal:  91%|█████████ | 241/266 [00:55<00:05,  4.98it/s]predicting train subjects sagittal:  91%|█████████ | 242/266 [00:56<00:04,  4.91it/s]predicting train subjects sagittal:  91%|█████████▏| 243/266 [00:56<00:04,  4.81it/s]predicting train subjects sagittal:  92%|█████████▏| 244/266 [00:56<00:04,  4.79it/s]predicting train subjects sagittal:  92%|█████████▏| 245/266 [00:56<00:04,  4.85it/s]predicting train subjects sagittal:  92%|█████████▏| 246/266 [00:56<00:04,  4.77it/s]predicting train subjects sagittal:  93%|█████████▎| 247/266 [00:57<00:03,  4.88it/s]predicting train subjects sagittal:  93%|█████████▎| 248/266 [00:57<00:03,  4.99it/s]predicting train subjects sagittal:  94%|█████████▎| 249/266 [00:57<00:03,  4.62it/s]predicting train subjects sagittal:  94%|█████████▍| 250/266 [00:57<00:03,  4.36it/s]predicting train subjects sagittal:  94%|█████████▍| 251/266 [00:58<00:03,  3.97it/s]predicting train subjects sagittal:  95%|█████████▍| 252/266 [00:58<00:03,  4.00it/s]predicting train subjects sagittal:  95%|█████████▌| 253/266 [00:58<00:03,  4.06it/s]predicting train subjects sagittal:  95%|█████████▌| 254/266 [00:58<00:02,  4.09it/s]predicting train subjects sagittal:  96%|█████████▌| 255/266 [00:59<00:02,  4.02it/s]predicting train subjects sagittal:  96%|█████████▌| 256/266 [00:59<00:02,  4.05it/s]predicting train subjects sagittal:  97%|█████████▋| 257/266 [00:59<00:02,  4.12it/s]predicting train subjects sagittal:  97%|█████████▋| 258/266 [00:59<00:01,  4.05it/s]predicting train subjects sagittal:  97%|█████████▋| 259/266 [01:00<00:01,  4.00it/s]predicting train subjects sagittal:  98%|█████████▊| 260/266 [01:00<00:01,  3.92it/s]predicting train subjects sagittal:  98%|█████████▊| 261/266 [01:00<00:01,  3.99it/s]predicting train subjects sagittal:  98%|█████████▊| 262/266 [01:00<00:00,  4.03it/s]predicting train subjects sagittal:  99%|█████████▉| 263/266 [01:01<00:00,  3.99it/s]predicting train subjects sagittal:  99%|█████████▉| 264/266 [01:01<00:00,  4.00it/s]predicting train subjects sagittal: 100%|█████████▉| 265/266 [01:01<00:00,  3.91it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [01:01<00:00,  3.75it/s]
saving BB  test1-THALAMUS:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 5/5 [00:00<00:00, 76.08it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 7/266 [00:00<00:04, 59.42it/s]saving BB  train1-THALAMUS:   5%|▌         | 14/266 [00:00<00:04, 61.24it/s]saving BB  train1-THALAMUS:   8%|▊         | 21/266 [00:00<00:03, 62.17it/s]saving BB  train1-THALAMUS:  10%|█         | 27/266 [00:00<00:04, 58.04it/s]saving BB  train1-THALAMUS:  13%|█▎        | 35/266 [00:00<00:03, 61.42it/s]saving BB  train1-THALAMUS:  16%|█▌        | 43/266 [00:00<00:04, 52.07it/s]saving BB  train1-THALAMUS:  19%|█▉        | 51/266 [00:00<00:03, 56.94it/s]saving BB  train1-THALAMUS:  22%|██▏       | 59/266 [00:00<00:03, 61.45it/s]saving BB  train1-THALAMUS:  26%|██▌       | 68/266 [00:01<00:02, 66.66it/s]saving BB  train1-THALAMUS:  29%|██▉       | 77/266 [00:01<00:02, 68.46it/s]saving BB  train1-THALAMUS:  32%|███▏      | 84/266 [00:01<00:02, 68.89it/s]saving BB  train1-THALAMUS:  34%|███▍      | 91/266 [00:01<00:02, 66.94it/s]saving BB  train1-THALAMUS:  37%|███▋      | 98/266 [00:01<00:02, 67.49it/s]saving BB  train1-THALAMUS:  40%|███▉      | 106/266 [00:01<00:02, 70.15it/s]saving BB  train1-THALAMUS:  43%|████▎     | 114/266 [00:01<00:02, 71.65it/s]saving BB  train1-THALAMUS:  46%|████▌     | 122/266 [00:01<00:02, 70.68it/s]saving BB  train1-THALAMUS:  49%|████▉     | 130/266 [00:01<00:01, 70.69it/s]saving BB  train1-THALAMUS:  52%|█████▏    | 138/266 [00:02<00:01, 70.81it/s]saving BB  train1-THALAMUS:  55%|█████▍    | 146/266 [00:02<00:01, 70.55it/s]saving BB  train1-THALAMUS:  58%|█████▊    | 154/266 [00:02<00:01, 68.06it/s]saving BB  train1-THALAMUS:  61%|██████    | 161/266 [00:02<00:01, 68.29it/s]saving BB  train1-THALAMUS:  64%|██████▍   | 170/266 [00:02<00:01, 72.09it/s]saving BB  train1-THALAMUS:  67%|██████▋   | 179/266 [00:02<00:01, 74.91it/s]saving BB  train1-THALAMUS:  71%|███████   | 188/266 [00:02<00:01, 77.63it/s]saving BB  train1-THALAMUS:  74%|███████▎  | 196/266 [00:02<00:00, 75.77it/s]saving BB  train1-THALAMUS:  77%|███████▋  | 204/266 [00:02<00:00, 74.79it/s]saving BB  train1-THALAMUS:  80%|███████▉  | 212/266 [00:03<00:00, 74.62it/s]saving BB  train1-THALAMUS:  83%|████████▎ | 220/266 [00:03<00:00, 74.77it/s]saving BB  train1-THALAMUS:  86%|████████▌ | 228/266 [00:03<00:00, 75.44it/s]saving BB  train1-THALAMUS:  89%|████████▊ | 236/266 [00:03<00:00, 73.89it/s]saving BB  train1-THALAMUS:  92%|█████████▏| 244/266 [00:03<00:00, 74.98it/s]saving BB  train1-THALAMUS:  95%|█████████▍| 252/266 [00:03<00:00, 74.79it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 260/266 [00:03<00:00, 73.08it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 69.66it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 5/5 [00:00<00:00, 75.46it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   2%|▏         | 6/266 [00:00<00:04, 58.32it/s]saving BB  train1-THALAMUS Sagittal:   5%|▍         | 13/266 [00:00<00:04, 59.23it/s]saving BB  train1-THALAMUS Sagittal:   8%|▊         | 20/266 [00:00<00:04, 60.62it/s]saving BB  train1-THALAMUS Sagittal:  10%|█         | 27/266 [00:00<00:03, 62.07it/s]saving BB  train1-THALAMUS Sagittal:  13%|█▎        | 35/266 [00:00<00:03, 64.68it/s]saving BB  train1-THALAMUS Sagittal:  16%|█▌        | 43/266 [00:00<00:03, 66.53it/s]saving BB  train1-THALAMUS Sagittal:  19%|█▉        | 51/266 [00:00<00:03, 68.94it/s]saving BB  train1-THALAMUS Sagittal:  22%|██▏       | 59/266 [00:00<00:02, 71.31it/s]saving BB  train1-THALAMUS Sagittal:  25%|██▌       | 67/266 [00:00<00:02, 71.46it/s]saving BB  train1-THALAMUS Sagittal:  28%|██▊       | 75/266 [00:01<00:02, 71.41it/s]saving BB  train1-THALAMUS Sagittal:  31%|███       | 83/266 [00:01<00:02, 72.14it/s]saving BB  train1-THALAMUS Sagittal:  34%|███▍      | 91/266 [00:01<00:02, 71.97it/s]saving BB  train1-THALAMUS Sagittal:  37%|███▋      | 99/266 [00:01<00:02, 71.69it/s]saving BB  train1-THALAMUS Sagittal:  40%|████      | 107/266 [00:01<00:02, 72.73it/s]saving BB  train1-THALAMUS Sagittal:  43%|████▎     | 115/266 [00:01<00:02, 73.65it/s]saving BB  train1-THALAMUS Sagittal:  46%|████▌     | 123/266 [00:01<00:01, 72.99it/s]saving BB  train1-THALAMUS Sagittal:  49%|████▉     | 131/266 [00:01<00:01, 71.48it/s]saving BB  train1-THALAMUS Sagittal:  52%|█████▏    | 139/266 [00:01<00:01, 71.33it/s]saving BB  train1-THALAMUS Sagittal:  55%|█████▌    | 147/266 [00:02<00:01, 70.59it/s]saving BB  train1-THALAMUS Sagittal:  58%|█████▊    | 155/266 [00:02<00:01, 70.68it/s]saving BB  train1-THALAMUS Sagittal:  62%|██████▏   | 164/266 [00:02<00:01, 73.98it/s]saving BB  train1-THALAMUS Sagittal:  65%|██████▌   | 173/266 [00:02<00:01, 76.70it/s]saving BB  train1-THALAMUS Sagittal:  68%|██████▊   | 182/266 [00:02<00:01, 78.57it/s]saving BB  train1-THALAMUS Sagittal:  72%|███████▏  | 191/266 [00:02<00:00, 78.87it/s]saving BB  train1-THALAMUS Sagittal:  75%|███████▍  | 199/266 [00:02<00:00, 77.68it/s]saving BB  train1-THALAMUS Sagittal:  78%|███████▊  | 207/266 [00:02<00:00, 76.18it/s]saving BB  train1-THALAMUS Sagittal:  81%|████████  | 215/266 [00:02<00:00, 75.98it/s]saving BB  train1-THALAMUS Sagittal:  84%|████████▍ | 223/266 [00:03<00:00, 76.59it/s]saving BB  train1-THALAMUS Sagittal:  87%|████████▋ | 231/266 [00:03<00:00, 77.31it/s]saving BB  train1-THALAMUS Sagittal:  90%|█████████ | 240/266 [00:03<00:00, 78.49it/s]saving BB  train1-THALAMUS Sagittal:  94%|█████████▎| 249/266 [00:03<00:00, 78.61it/s]saving BB  train1-THALAMUS Sagittal:  97%|█████████▋| 257/266 [00:03<00:00, 75.32it/s]saving BB  train1-THALAMUS Sagittal: 100%|█████████▉| 265/266 [00:03<00:00, 73.93it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 73.16it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<07:00,  1.59s/it]Loading train:   1%|          | 2/266 [00:03<07:02,  1.60s/it]Loading train:   1%|          | 3/266 [00:04<06:33,  1.50s/it]Loading train:   2%|▏         | 4/266 [00:05<06:04,  1.39s/it]Loading train:   2%|▏         | 5/266 [00:07<06:21,  1.46s/it]Loading train:   2%|▏         | 6/266 [00:08<05:43,  1.32s/it]Loading train:   3%|▎         | 7/266 [00:09<05:18,  1.23s/it]Loading train:   3%|▎         | 8/266 [00:10<04:55,  1.15s/it]Loading train:   3%|▎         | 9/266 [00:11<04:43,  1.10s/it]Loading train:   4%|▍         | 10/266 [00:12<04:40,  1.09s/it]Loading train:   4%|▍         | 11/266 [00:13<04:26,  1.04s/it]Loading train:   5%|▍         | 12/266 [00:14<04:10,  1.01it/s]Loading train:   5%|▍         | 13/266 [00:14<04:03,  1.04it/s]Loading train:   5%|▌         | 14/266 [00:16<04:08,  1.01it/s]Loading train:   6%|▌         | 15/266 [00:16<04:05,  1.02it/s]Loading train:   6%|▌         | 16/266 [00:17<04:07,  1.01it/s]Loading train:   6%|▋         | 17/266 [00:18<04:02,  1.03it/s]Loading train:   7%|▋         | 18/266 [00:19<04:02,  1.02it/s]Loading train:   7%|▋         | 19/266 [00:20<04:03,  1.01it/s]Loading train:   8%|▊         | 20/266 [00:21<03:56,  1.04it/s]Loading train:   8%|▊         | 21/266 [00:22<03:53,  1.05it/s]Loading train:   8%|▊         | 22/266 [00:23<04:02,  1.01it/s]Loading train:   9%|▊         | 23/266 [00:24<03:55,  1.03it/s]Loading train:   9%|▉         | 24/266 [00:25<03:52,  1.04it/s]Loading train:   9%|▉         | 25/266 [00:26<03:50,  1.05it/s]Loading train:  10%|▉         | 26/266 [00:27<03:44,  1.07it/s]Loading train:  10%|█         | 27/266 [00:28<03:41,  1.08it/s]Loading train:  11%|█         | 28/266 [00:29<03:36,  1.10it/s]Loading train:  11%|█         | 29/266 [00:30<03:34,  1.10it/s]Loading train:  11%|█▏        | 30/266 [00:31<03:27,  1.14it/s]Loading train:  12%|█▏        | 31/266 [00:31<03:30,  1.12it/s]Loading train:  12%|█▏        | 32/266 [00:32<03:31,  1.11it/s]Loading train:  12%|█▏        | 33/266 [00:33<03:26,  1.13it/s]Loading train:  13%|█▎        | 34/266 [00:34<03:26,  1.12it/s]Loading train:  13%|█▎        | 35/266 [00:35<03:21,  1.14it/s]Loading train:  14%|█▎        | 36/266 [00:36<03:24,  1.12it/s]Loading train:  14%|█▍        | 37/266 [00:37<03:27,  1.10it/s]Loading train:  14%|█▍        | 38/266 [00:38<03:22,  1.13it/s]Loading train:  15%|█▍        | 39/266 [00:39<03:21,  1.13it/s]Loading train:  15%|█▌        | 40/266 [00:39<03:21,  1.12it/s]Loading train:  15%|█▌        | 41/266 [00:40<03:18,  1.14it/s]Loading train:  16%|█▌        | 42/266 [00:41<03:09,  1.18it/s]Loading train:  16%|█▌        | 43/266 [00:42<03:07,  1.19it/s]Loading train:  17%|█▋        | 44/266 [00:43<03:02,  1.22it/s]Loading train:  17%|█▋        | 45/266 [00:43<02:54,  1.27it/s]Loading train:  17%|█▋        | 46/266 [00:44<02:46,  1.32it/s]Loading train:  18%|█▊        | 47/266 [00:45<02:42,  1.35it/s]Loading train:  18%|█▊        | 48/266 [00:46<02:47,  1.30it/s]Loading train:  18%|█▊        | 49/266 [00:46<02:42,  1.33it/s]Loading train:  19%|█▉        | 50/266 [00:47<02:41,  1.33it/s]Loading train:  19%|█▉        | 51/266 [00:48<02:42,  1.32it/s]Loading train:  20%|█▉        | 52/266 [00:49<02:38,  1.35it/s]Loading train:  20%|█▉        | 53/266 [00:49<02:51,  1.24it/s]Loading train:  20%|██        | 54/266 [00:50<02:43,  1.30it/s]Loading train:  21%|██        | 55/266 [00:51<02:39,  1.32it/s]Loading train:  21%|██        | 56/266 [00:52<02:44,  1.28it/s]Loading train:  21%|██▏       | 57/266 [00:53<02:42,  1.29it/s]Loading train:  22%|██▏       | 58/266 [00:53<02:38,  1.31it/s]Loading train:  22%|██▏       | 59/266 [00:54<02:36,  1.32it/s]Loading train:  23%|██▎       | 60/266 [00:55<02:37,  1.31it/s]Loading train:  23%|██▎       | 61/266 [00:56<02:38,  1.29it/s]Loading train:  23%|██▎       | 62/266 [00:56<02:31,  1.35it/s]Loading train:  24%|██▎       | 63/266 [00:57<02:26,  1.39it/s]Loading train:  24%|██▍       | 64/266 [00:58<02:25,  1.39it/s]Loading train:  24%|██▍       | 65/266 [00:58<02:22,  1.41it/s]Loading train:  25%|██▍       | 66/266 [00:59<02:19,  1.43it/s]Loading train:  25%|██▌       | 67/266 [01:00<02:16,  1.46it/s]Loading train:  26%|██▌       | 68/266 [01:00<02:13,  1.48it/s]Loading train:  26%|██▌       | 69/266 [01:01<02:12,  1.48it/s]Loading train:  26%|██▋       | 70/266 [01:02<02:16,  1.43it/s]Loading train:  27%|██▋       | 71/266 [01:02<02:11,  1.48it/s]Loading train:  27%|██▋       | 72/266 [01:03<02:11,  1.48it/s]Loading train:  27%|██▋       | 73/266 [01:04<02:11,  1.47it/s]Loading train:  28%|██▊       | 74/266 [01:04<02:14,  1.43it/s]Loading train:  28%|██▊       | 75/266 [01:05<02:10,  1.47it/s]Loading train:  29%|██▊       | 76/266 [01:06<02:13,  1.42it/s]Loading train:  29%|██▉       | 77/266 [01:07<02:14,  1.40it/s]Loading train:  29%|██▉       | 78/266 [01:07<02:24,  1.30it/s]Loading train:  30%|██▉       | 79/266 [01:08<02:29,  1.25it/s]Loading train:  30%|███       | 80/266 [01:09<02:27,  1.26it/s]Loading train:  30%|███       | 81/266 [01:10<02:29,  1.23it/s]Loading train:  31%|███       | 82/266 [01:11<02:34,  1.19it/s]Loading train:  31%|███       | 83/266 [01:12<02:45,  1.10it/s]Loading train:  32%|███▏      | 84/266 [01:13<02:45,  1.10it/s]Loading train:  32%|███▏      | 85/266 [01:14<02:39,  1.14it/s]Loading train:  32%|███▏      | 86/266 [01:15<02:42,  1.11it/s]Loading train:  33%|███▎      | 87/266 [01:15<02:39,  1.12it/s]Loading train:  33%|███▎      | 88/266 [01:16<02:37,  1.13it/s]Loading train:  33%|███▎      | 89/266 [01:17<02:35,  1.14it/s]Loading train:  34%|███▍      | 90/266 [01:18<02:38,  1.11it/s]Loading train:  34%|███▍      | 91/266 [01:19<02:34,  1.13it/s]Loading train:  35%|███▍      | 92/266 [01:20<02:33,  1.13it/s]Loading train:  35%|███▍      | 93/266 [01:21<03:00,  1.04s/it]Loading train:  35%|███▌      | 94/266 [01:22<02:51,  1.00it/s]Loading train:  36%|███▌      | 95/266 [01:23<02:44,  1.04it/s]Loading train:  36%|███▌      | 96/266 [01:24<03:01,  1.07s/it]Loading train:  36%|███▋      | 97/266 [01:26<03:28,  1.23s/it]Loading train:  37%|███▋      | 98/266 [01:27<03:26,  1.23s/it]Loading train:  37%|███▋      | 99/266 [01:28<03:11,  1.15s/it]Loading train:  38%|███▊      | 100/266 [01:29<03:11,  1.15s/it]Loading train:  38%|███▊      | 101/266 [01:30<02:53,  1.05s/it]Loading train:  38%|███▊      | 102/266 [01:31<02:38,  1.03it/s]Loading train:  39%|███▊      | 103/266 [01:32<02:29,  1.09it/s]Loading train:  39%|███▉      | 104/266 [01:33<02:23,  1.13it/s]Loading train:  39%|███▉      | 105/266 [01:33<02:16,  1.18it/s]Loading train:  40%|███▉      | 106/266 [01:34<02:12,  1.21it/s]Loading train:  40%|████      | 107/266 [01:35<02:13,  1.19it/s]Loading train:  41%|████      | 108/266 [01:36<02:11,  1.20it/s]Loading train:  41%|████      | 109/266 [01:37<02:10,  1.21it/s]Loading train:  41%|████▏     | 110/266 [01:37<02:03,  1.26it/s]Loading train:  42%|████▏     | 111/266 [01:38<01:58,  1.31it/s]Loading train:  42%|████▏     | 112/266 [01:39<01:55,  1.34it/s]Loading train:  42%|████▏     | 113/266 [01:39<01:51,  1.37it/s]Loading train:  43%|████▎     | 114/266 [01:40<01:50,  1.38it/s]Loading train:  43%|████▎     | 115/266 [01:41<01:51,  1.36it/s]Loading train:  44%|████▎     | 116/266 [01:42<01:49,  1.37it/s]Loading train:  44%|████▍     | 117/266 [01:42<01:50,  1.35it/s]Loading train:  44%|████▍     | 118/266 [01:43<01:47,  1.38it/s]Loading train:  45%|████▍     | 119/266 [01:44<01:55,  1.27it/s]Loading train:  45%|████▌     | 120/266 [01:45<01:57,  1.24it/s]Loading train:  45%|████▌     | 121/266 [01:46<01:59,  1.21it/s]Loading train:  46%|████▌     | 122/266 [01:47<02:04,  1.16it/s]Loading train:  46%|████▌     | 123/266 [01:48<02:04,  1.15it/s]Loading train:  47%|████▋     | 124/266 [01:48<02:05,  1.14it/s]Loading train:  47%|████▋     | 125/266 [01:49<02:04,  1.13it/s]Loading train:  47%|████▋     | 126/266 [01:50<02:11,  1.06it/s]Loading train:  48%|████▊     | 127/266 [01:51<02:10,  1.06it/s]Loading train:  48%|████▊     | 128/266 [01:52<02:13,  1.03it/s]Loading train:  48%|████▊     | 129/266 [01:53<02:10,  1.05it/s]Loading train:  49%|████▉     | 130/266 [01:54<02:10,  1.04it/s]Loading train:  49%|████▉     | 131/266 [01:55<02:13,  1.01it/s]Loading train:  50%|████▉     | 132/266 [01:56<02:08,  1.04it/s]Loading train:  50%|█████     | 133/266 [01:57<02:09,  1.03it/s]Loading train:  50%|█████     | 134/266 [01:58<02:08,  1.02it/s]Loading train:  51%|█████     | 135/266 [01:59<02:04,  1.05it/s]Loading train:  51%|█████     | 136/266 [02:00<02:02,  1.06it/s]Loading train:  52%|█████▏    | 137/266 [02:01<02:01,  1.06it/s]Loading train:  52%|█████▏    | 138/266 [02:02<01:59,  1.07it/s]Loading train:  52%|█████▏    | 139/266 [02:03<01:57,  1.08it/s]Loading train:  53%|█████▎    | 140/266 [02:04<01:54,  1.10it/s]Loading train:  53%|█████▎    | 141/266 [02:04<01:50,  1.14it/s]Loading train:  53%|█████▎    | 142/266 [02:05<01:48,  1.14it/s]Loading train:  54%|█████▍    | 143/266 [02:06<01:46,  1.15it/s]Loading train:  54%|█████▍    | 144/266 [02:07<01:44,  1.17it/s]Loading train:  55%|█████▍    | 145/266 [02:08<01:41,  1.19it/s]Loading train:  55%|█████▍    | 146/266 [02:09<01:41,  1.18it/s]Loading train:  55%|█████▌    | 147/266 [02:09<01:38,  1.21it/s]Loading train:  56%|█████▌    | 148/266 [02:10<01:37,  1.21it/s]Loading train:  56%|█████▌    | 149/266 [02:11<01:35,  1.23it/s]Loading train:  56%|█████▋    | 150/266 [02:12<01:33,  1.24it/s]Loading train:  57%|█████▋    | 151/266 [02:13<01:31,  1.26it/s]Loading train:  57%|█████▋    | 152/266 [02:13<01:28,  1.28it/s]Loading train:  58%|█████▊    | 153/266 [02:14<01:27,  1.29it/s]Loading train:  58%|█████▊    | 154/266 [02:15<01:26,  1.29it/s]Loading train:  58%|█████▊    | 155/266 [02:16<01:26,  1.28it/s]Loading train:  59%|█████▊    | 156/266 [02:16<01:20,  1.37it/s]Loading train:  59%|█████▉    | 157/266 [02:17<01:18,  1.39it/s]Loading train:  59%|█████▉    | 158/266 [02:18<01:15,  1.43it/s]Loading train:  60%|█████▉    | 159/266 [02:18<01:12,  1.48it/s]Loading train:  60%|██████    | 160/266 [02:19<01:10,  1.51it/s]Loading train:  61%|██████    | 161/266 [02:20<01:11,  1.47it/s]Loading train:  61%|██████    | 162/266 [02:20<01:09,  1.49it/s]Loading train:  61%|██████▏   | 163/266 [02:21<01:09,  1.49it/s]Loading train:  62%|██████▏   | 164/266 [02:22<01:11,  1.44it/s]Loading train:  62%|██████▏   | 165/266 [02:22<01:09,  1.45it/s]Loading train:  62%|██████▏   | 166/266 [02:23<01:08,  1.45it/s]Loading train:  63%|██████▎   | 167/266 [02:24<01:06,  1.49it/s]Loading train:  63%|██████▎   | 168/266 [02:24<01:05,  1.50it/s]Loading train:  64%|██████▎   | 169/266 [02:25<01:02,  1.55it/s]Loading train:  64%|██████▍   | 170/266 [02:26<01:01,  1.57it/s]Loading train:  64%|██████▍   | 171/266 [02:26<00:59,  1.59it/s]Loading train:  65%|██████▍   | 172/266 [02:27<01:00,  1.56it/s]Loading train:  65%|██████▌   | 173/266 [02:28<01:02,  1.49it/s]Loading train:  65%|██████▌   | 174/266 [02:28<01:02,  1.47it/s]Loading train:  66%|██████▌   | 175/266 [02:29<01:01,  1.48it/s]Loading train:  66%|██████▌   | 176/266 [02:30<01:00,  1.48it/s]Loading train:  67%|██████▋   | 177/266 [02:30<00:59,  1.49it/s]Loading train:  67%|██████▋   | 178/266 [02:31<00:57,  1.54it/s]Loading train:  67%|██████▋   | 179/266 [02:32<00:55,  1.57it/s]Loading train:  68%|██████▊   | 180/266 [02:32<00:54,  1.58it/s]Loading train:  68%|██████▊   | 181/266 [02:33<00:53,  1.60it/s]Loading train:  68%|██████▊   | 182/266 [02:34<00:55,  1.51it/s]Loading train:  69%|██████▉   | 183/266 [02:34<00:56,  1.47it/s]Loading train:  69%|██████▉   | 184/266 [02:35<00:55,  1.47it/s]Loading train:  70%|██████▉   | 185/266 [02:36<00:55,  1.46it/s]Loading train:  70%|██████▉   | 186/266 [02:36<00:55,  1.43it/s]Loading train:  70%|███████   | 187/266 [02:37<00:56,  1.40it/s]Loading train:  71%|███████   | 188/266 [02:38<00:56,  1.38it/s]Loading train:  71%|███████   | 189/266 [02:39<00:54,  1.41it/s]Loading train:  71%|███████▏  | 190/266 [02:39<00:53,  1.42it/s]Loading train:  72%|███████▏  | 191/266 [02:40<01:03,  1.17it/s]Loading train:  72%|███████▏  | 192/266 [02:41<01:07,  1.10it/s]Loading train:  73%|███████▎  | 193/266 [02:43<01:10,  1.04it/s]Loading train:  73%|███████▎  | 194/266 [02:44<01:21,  1.13s/it]Loading train:  73%|███████▎  | 195/266 [02:45<01:14,  1.05s/it]Loading train:  74%|███████▎  | 196/266 [02:46<01:09,  1.01it/s]Loading train:  74%|███████▍  | 197/266 [02:47<01:05,  1.05it/s]Loading train:  74%|███████▍  | 198/266 [02:47<01:00,  1.12it/s]Loading train:  75%|███████▍  | 199/266 [02:48<00:59,  1.13it/s]Loading train:  75%|███████▌  | 200/266 [02:49<00:59,  1.12it/s]Loading train:  76%|███████▌  | 201/266 [02:50<00:56,  1.15it/s]Loading train:  76%|███████▌  | 202/266 [02:51<00:54,  1.17it/s]Loading train:  76%|███████▋  | 203/266 [02:52<00:54,  1.16it/s]Loading train:  77%|███████▋  | 204/266 [02:53<00:52,  1.17it/s]Loading train:  77%|███████▋  | 205/266 [02:53<00:51,  1.19it/s]Loading train:  77%|███████▋  | 206/266 [02:54<00:49,  1.22it/s]Loading train:  78%|███████▊  | 207/266 [02:55<00:48,  1.23it/s]Loading train:  78%|███████▊  | 208/266 [02:56<00:46,  1.24it/s]Loading train:  79%|███████▊  | 209/266 [02:57<00:46,  1.22it/s]Loading train:  79%|███████▉  | 210/266 [02:57<00:44,  1.27it/s]Loading train:  79%|███████▉  | 211/266 [02:58<00:41,  1.33it/s]Loading train:  80%|███████▉  | 212/266 [02:59<00:40,  1.34it/s]Loading train:  80%|████████  | 213/266 [02:59<00:38,  1.37it/s]Loading train:  80%|████████  | 214/266 [03:00<00:36,  1.44it/s]Loading train:  81%|████████  | 215/266 [03:01<00:34,  1.49it/s]Loading train:  81%|████████  | 216/266 [03:01<00:33,  1.49it/s]Loading train:  82%|████████▏ | 217/266 [03:02<00:31,  1.54it/s]Loading train:  82%|████████▏ | 218/266 [03:03<00:31,  1.51it/s]Loading train:  82%|████████▏ | 219/266 [03:03<00:32,  1.44it/s]Loading train:  83%|████████▎ | 220/266 [03:04<00:32,  1.44it/s]Loading train:  83%|████████▎ | 221/266 [03:05<00:29,  1.51it/s]Loading train:  83%|████████▎ | 222/266 [03:05<00:29,  1.48it/s]Loading train:  84%|████████▍ | 223/266 [03:06<00:29,  1.46it/s]Loading train:  84%|████████▍ | 224/266 [03:07<00:28,  1.45it/s]Loading train:  85%|████████▍ | 225/266 [03:07<00:29,  1.41it/s]Loading train:  85%|████████▍ | 226/266 [03:08<00:28,  1.41it/s]Loading train:  85%|████████▌ | 227/266 [03:09<00:26,  1.44it/s]Loading train:  86%|████████▌ | 228/266 [03:09<00:25,  1.47it/s]Loading train:  86%|████████▌ | 229/266 [03:10<00:26,  1.41it/s]Loading train:  86%|████████▋ | 230/266 [03:11<00:25,  1.41it/s]Loading train:  87%|████████▋ | 231/266 [03:12<00:24,  1.43it/s]Loading train:  87%|████████▋ | 232/266 [03:12<00:23,  1.44it/s]Loading train:  88%|████████▊ | 233/266 [03:13<00:22,  1.47it/s]Loading train:  88%|████████▊ | 234/266 [03:14<00:22,  1.44it/s]Loading train:  88%|████████▊ | 235/266 [03:14<00:21,  1.47it/s]Loading train:  89%|████████▊ | 236/266 [03:15<00:20,  1.44it/s]Loading train:  89%|████████▉ | 237/266 [03:16<00:20,  1.41it/s]Loading train:  89%|████████▉ | 238/266 [03:17<00:19,  1.41it/s]Loading train:  90%|████████▉ | 239/266 [03:17<00:19,  1.38it/s]Loading train:  90%|█████████ | 240/266 [03:18<00:18,  1.40it/s]Loading train:  91%|█████████ | 241/266 [03:19<00:17,  1.43it/s]Loading train:  91%|█████████ | 242/266 [03:19<00:16,  1.43it/s]Loading train:  91%|█████████▏| 243/266 [03:20<00:16,  1.43it/s]Loading train:  92%|█████████▏| 244/266 [03:21<00:15,  1.42it/s]Loading train:  92%|█████████▏| 245/266 [03:22<00:15,  1.39it/s]Loading train:  92%|█████████▏| 246/266 [03:22<00:14,  1.42it/s]Loading train:  93%|█████████▎| 247/266 [03:23<00:13,  1.41it/s]Loading train:  93%|█████████▎| 248/266 [03:24<00:12,  1.40it/s]Loading train:  94%|█████████▎| 249/266 [03:25<00:13,  1.28it/s]Loading train:  94%|█████████▍| 250/266 [03:26<00:13,  1.21it/s]Loading train:  94%|█████████▍| 251/266 [03:26<00:12,  1.16it/s]Loading train:  95%|█████████▍| 252/266 [03:27<00:12,  1.16it/s]Loading train:  95%|█████████▌| 253/266 [03:28<00:11,  1.16it/s]Loading train:  95%|█████████▌| 254/266 [03:29<00:10,  1.14it/s]Loading train:  96%|█████████▌| 255/266 [03:30<00:09,  1.15it/s]Loading train:  96%|█████████▌| 256/266 [03:31<00:08,  1.13it/s]Loading train:  97%|█████████▋| 257/266 [03:32<00:07,  1.14it/s]Loading train:  97%|█████████▋| 258/266 [03:33<00:07,  1.14it/s]Loading train:  97%|█████████▋| 259/266 [03:33<00:06,  1.17it/s]Loading train:  98%|█████████▊| 260/266 [03:34<00:05,  1.15it/s]Loading train:  98%|█████████▊| 261/266 [03:35<00:04,  1.17it/s]Loading train:  98%|█████████▊| 262/266 [03:36<00:03,  1.16it/s]Loading train:  99%|█████████▉| 263/266 [03:37<00:02,  1.14it/s]Loading train:  99%|█████████▉| 264/266 [03:38<00:01,  1.15it/s]Loading train: 100%|█████████▉| 265/266 [03:39<00:00,  1.14it/s]Loading train: 100%|██████████| 266/266 [03:39<00:00,  1.17it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   9%|▉         | 24/266 [00:00<00:01, 232.84it/s]concatenating: train:  17%|█▋        | 46/266 [00:00<00:00, 227.68it/s]concatenating: train:  26%|██▌       | 69/266 [00:00<00:00, 225.40it/s]concatenating: train:  35%|███▍      | 93/266 [00:00<00:00, 227.60it/s]concatenating: train:  43%|████▎     | 115/266 [00:00<00:00, 222.68it/s]concatenating: train:  51%|█████     | 135/266 [00:00<00:00, 212.36it/s]concatenating: train:  59%|█████▉    | 157/266 [00:00<00:00, 211.16it/s]concatenating: train:  68%|██████▊   | 180/266 [00:00<00:00, 214.26it/s]concatenating: train:  76%|███████▌  | 201/266 [00:00<00:00, 206.83it/s]concatenating: train:  83%|████████▎ | 222/266 [00:01<00:00, 205.87it/s]concatenating: train:  92%|█████████▏| 245/266 [00:01<00:00, 209.33it/s]concatenating: train: 100%|██████████| 266/266 [00:01<00:00, 217.13it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:01<00:04,  1.25s/it]Loading test:  40%|████      | 2/5 [00:02<00:03,  1.23s/it]Loading test:  60%|██████    | 3/5 [00:03<00:02,  1.19s/it]Loading test:  80%|████████  | 4/5 [00:04<00:01,  1.15s/it]Loading test: 100%|██████████| 5/5 [00:05<00:00,  1.18s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 735.97it/s]2019-08-17 18:29:32.462128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-17 18:29:32.462255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-17 18:29:32.462274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-17 18:29:32.462285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-17 18:29:32.462820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:07,  5.56it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:06,  6.61it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:06,  6.21it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:04,  7.99it/s]loading the weights for Res Unet:  23%|██▎       | 10/44 [00:00<00:03,  8.78it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:04,  7.76it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:02,  9.95it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02,  9.96it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:01<00:02,  7.70it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:02,  9.47it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01, 10.01it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01, 10.12it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:02<00:01,  7.84it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:03<00:00,  9.64it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:03<00:00, 10.17it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00, 10.38it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:03<00:00,  8.44it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:03<00:00, 11.55it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 2  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 2  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 84, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 84, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 84, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 84, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 84, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 84, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 84, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 42, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 42, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 42, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 42, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 42, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 42, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 42, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 42, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 42, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 21, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 21, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 21, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 21, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 21, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 21, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 21, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 21, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 21, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 21, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 42, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 42, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 42, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 42, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 42, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 42, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 42, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 42, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 42, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 42, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 84, 20)   11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 84, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 84, 20)   7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 84, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 84, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 84, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 84, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 84, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 84, 60)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 84, 60)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 84, 13)   793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 102,553
Non-trainable params: 121,280
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.33538622e-02 3.28354272e-02 7.67811304e-02 9.54044153e-03
 2.76122102e-02 7.22395648e-03 8.45747113e-02 1.14122510e-01
 8.96086956e-02 1.36146791e-02 2.90529016e-01 1.89945417e-01
 2.57943486e-04]
Train on 9760 samples, validate on 179 samples
Epoch 1/300
 - 13s - loss: 3.2570 - acc: 0.8034 - mDice: 0.0621 - val_loss: 1.9041 - val_acc: 0.9064 - val_mDice: 0.1604

Epoch 00001: val_mDice improved from -inf to 0.16038, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 8s - loss: 1.8418 - acc: 0.8760 - mDice: 0.1679 - val_loss: 1.4008 - val_acc: 0.9063 - val_mDice: 0.2586

Epoch 00002: val_mDice improved from 0.16038 to 0.25861, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 9s - loss: 1.4097 - acc: 0.8764 - mDice: 0.2432 - val_loss: 1.1538 - val_acc: 0.9065 - val_mDice: 0.3241

Epoch 00003: val_mDice improved from 0.25861 to 0.32407, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 9s - loss: 1.2029 - acc: 0.8767 - mDice: 0.2919 - val_loss: 0.9986 - val_acc: 0.9064 - val_mDice: 0.3809

Epoch 00004: val_mDice improved from 0.32407 to 0.38086, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 10s - loss: 1.0613 - acc: 0.8769 - mDice: 0.3336 - val_loss: 0.9306 - val_acc: 0.9064 - val_mDice: 0.4079

Epoch 00005: val_mDice improved from 0.38086 to 0.40794, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 11s - loss: 0.9637 - acc: 0.8772 - mDice: 0.3680 - val_loss: 0.8933 - val_acc: 0.9066 - val_mDice: 0.4277

Epoch 00006: val_mDice improved from 0.40794 to 0.42766, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 11s - loss: 0.8875 - acc: 0.8778 - mDice: 0.3973 - val_loss: 0.8452 - val_acc: 0.9069 - val_mDice: 0.4497

Epoch 00007: val_mDice improved from 0.42766 to 0.44971, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 11s - loss: 0.8357 - acc: 0.8814 - mDice: 0.4198 - val_loss: 0.7610 - val_acc: 0.9117 - val_mDice: 0.4700

Epoch 00008: val_mDice improved from 0.44971 to 0.47003, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 11s - loss: 0.7890 - acc: 0.8970 - mDice: 0.4395 - val_loss: 0.7493 - val_acc: 0.9268 - val_mDice: 0.4808

Epoch 00009: val_mDice improved from 0.47003 to 0.48077, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 11s - loss: 0.7462 - acc: 0.9026 - mDice: 0.4595 - val_loss: 0.6769 - val_acc: 0.9320 - val_mDice: 0.5042

Epoch 00010: val_mDice improved from 0.48077 to 0.50425, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 11s - loss: 0.7138 - acc: 0.9059 - mDice: 0.4735 - val_loss: 0.6641 - val_acc: 0.9323 - val_mDice: 0.5081

Epoch 00011: val_mDice improved from 0.50425 to 0.50812, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 11s - loss: 0.6780 - acc: 0.9090 - mDice: 0.4910 - val_loss: 0.6525 - val_acc: 0.9324 - val_mDice: 0.5144

Epoch 00012: val_mDice improved from 0.50812 to 0.51444, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 11s - loss: 0.6539 - acc: 0.9115 - mDice: 0.5037 - val_loss: 0.6269 - val_acc: 0.9342 - val_mDice: 0.5272

Epoch 00013: val_mDice improved from 0.51444 to 0.52717, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 11s - loss: 0.6370 - acc: 0.9134 - mDice: 0.5133 - val_loss: 0.6016 - val_acc: 0.9338 - val_mDice: 0.5376

Epoch 00014: val_mDice improved from 0.52717 to 0.53761, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 12s - loss: 0.6148 - acc: 0.9150 - mDice: 0.5256 - val_loss: 0.5879 - val_acc: 0.9363 - val_mDice: 0.5445

Epoch 00015: val_mDice improved from 0.53761 to 0.54455, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 11s - loss: 0.6180 - acc: 0.9148 - mDice: 0.5244 - val_loss: 0.5896 - val_acc: 0.9357 - val_mDice: 0.5429

Epoch 00016: val_mDice did not improve from 0.54455
Epoch 17/300
 - 12s - loss: 0.5973 - acc: 0.9165 - mDice: 0.5336 - val_loss: 0.5868 - val_acc: 0.9361 - val_mDice: 0.5450

Epoch 00017: val_mDice improved from 0.54455 to 0.54504, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 11s - loss: 0.5772 - acc: 0.9182 - mDice: 0.5448 - val_loss: 0.5884 - val_acc: 0.9363 - val_mDice: 0.5485

Epoch 00018: val_mDice improved from 0.54504 to 0.54848, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 11s - loss: 0.5712 - acc: 0.9193 - mDice: 0.5510 - val_loss: 0.5791 - val_acc: 0.9394 - val_mDice: 0.5527

Epoch 00019: val_mDice improved from 0.54848 to 0.55269, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 11s - loss: 0.5669 - acc: 0.9191 - mDice: 0.5512 - val_loss: 0.5579 - val_acc: 0.9363 - val_mDice: 0.5599

Epoch 00020: val_mDice improved from 0.55269 to 0.55988, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 12s - loss: 0.5533 - acc: 0.9206 - mDice: 0.5588 - val_loss: 0.5648 - val_acc: 0.9373 - val_mDice: 0.5575

Epoch 00021: val_mDice did not improve from 0.55988
Epoch 22/300
 - 11s - loss: 0.5445 - acc: 0.9214 - mDice: 0.5639 - val_loss: 0.5663 - val_acc: 0.9369 - val_mDice: 0.5567

Epoch 00022: val_mDice did not improve from 0.55988
Epoch 23/300
 - 13s - loss: 0.5355 - acc: 0.9221 - mDice: 0.5691 - val_loss: 0.5472 - val_acc: 0.9380 - val_mDice: 0.5667

Epoch 00023: val_mDice improved from 0.55988 to 0.56666, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 11s - loss: 0.5339 - acc: 0.9225 - mDice: 0.5710 - val_loss: 0.5522 - val_acc: 0.9389 - val_mDice: 0.5642

Epoch 00024: val_mDice did not improve from 0.56666
Epoch 25/300
 - 12s - loss: 0.5356 - acc: 0.9227 - mDice: 0.5703 - val_loss: 0.5518 - val_acc: 0.9391 - val_mDice: 0.5649

Epoch 00025: val_mDice did not improve from 0.56666
Epoch 26/300
 - 11s - loss: 0.5235 - acc: 0.9237 - mDice: 0.5773 - val_loss: 0.5375 - val_acc: 0.9386 - val_mDice: 0.5715

Epoch 00026: val_mDice improved from 0.56666 to 0.57148, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 13s - loss: 0.5208 - acc: 0.9234 - mDice: 0.5777 - val_loss: 0.5430 - val_acc: 0.9396 - val_mDice: 0.5692

Epoch 00027: val_mDice did not improve from 0.57148
Epoch 28/300
 - 12s - loss: 0.5081 - acc: 0.9248 - mDice: 0.5854 - val_loss: 0.5402 - val_acc: 0.9389 - val_mDice: 0.5700

Epoch 00028: val_mDice did not improve from 0.57148
Epoch 29/300
 - 13s - loss: 0.5019 - acc: 0.9253 - mDice: 0.5892 - val_loss: 0.5413 - val_acc: 0.9397 - val_mDice: 0.5716

Epoch 00029: val_mDice improved from 0.57148 to 0.57158, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 12s - loss: 0.4979 - acc: 0.9259 - mDice: 0.5917 - val_loss: 0.5356 - val_acc: 0.9393 - val_mDice: 0.5735

Epoch 00030: val_mDice improved from 0.57158 to 0.57349, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 12s - loss: 0.4951 - acc: 0.9261 - mDice: 0.5934 - val_loss: 0.5519 - val_acc: 0.9373 - val_mDice: 0.5666

Epoch 00031: val_mDice did not improve from 0.57349
Epoch 32/300
 - 12s - loss: 0.4897 - acc: 0.9268 - mDice: 0.5973 - val_loss: 0.5342 - val_acc: 0.9396 - val_mDice: 0.5731

Epoch 00032: val_mDice did not improve from 0.57349
Epoch 33/300
 - 12s - loss: 0.4897 - acc: 0.9267 - mDice: 0.5969 - val_loss: 0.5363 - val_acc: 0.9399 - val_mDice: 0.5734

Epoch 00033: val_mDice did not improve from 0.57349
Epoch 34/300
 - 13s - loss: 0.4794 - acc: 0.9274 - mDice: 0.6033 - val_loss: 0.5278 - val_acc: 0.9406 - val_mDice: 0.5785

Epoch 00034: val_mDice improved from 0.57349 to 0.57852, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 12s - loss: 0.4762 - acc: 0.9278 - mDice: 0.6051 - val_loss: 0.5308 - val_acc: 0.9397 - val_mDice: 0.5771

Epoch 00035: val_mDice did not improve from 0.57852
Epoch 36/300
 - 12s - loss: 0.4744 - acc: 0.9281 - mDice: 0.6065 - val_loss: 0.5250 - val_acc: 0.9396 - val_mDice: 0.5802

Epoch 00036: val_mDice improved from 0.57852 to 0.58018, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 12s - loss: 0.4681 - acc: 0.9285 - mDice: 0.6103 - val_loss: 0.5337 - val_acc: 0.9401 - val_mDice: 0.5769

Epoch 00037: val_mDice did not improve from 0.58018
Epoch 38/300
 - 13s - loss: 0.4648 - acc: 0.9288 - mDice: 0.6132 - val_loss: 0.5223 - val_acc: 0.9420 - val_mDice: 0.5819

Epoch 00038: val_mDice improved from 0.58018 to 0.58185, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 11s - loss: 0.4626 - acc: 0.9291 - mDice: 0.6137 - val_loss: 0.5368 - val_acc: 0.9404 - val_mDice: 0.5757

Epoch 00039: val_mDice did not improve from 0.58185
Epoch 40/300
 - 12s - loss: 0.4597 - acc: 0.9294 - mDice: 0.6159 - val_loss: 0.5284 - val_acc: 0.9410 - val_mDice: 0.5784

Epoch 00040: val_mDice did not improve from 0.58185
Epoch 41/300
 - 11s - loss: 0.4559 - acc: 0.9297 - mDice: 0.6181 - val_loss: 0.5226 - val_acc: 0.9400 - val_mDice: 0.5820

Epoch 00041: val_mDice improved from 0.58185 to 0.58196, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 11s - loss: 0.4530 - acc: 0.9298 - mDice: 0.6210 - val_loss: 0.5224 - val_acc: 0.9407 - val_mDice: 0.5817

Epoch 00042: val_mDice did not improve from 0.58196
Epoch 43/300
 - 12s - loss: 0.4623 - acc: 0.9295 - mDice: 0.6178 - val_loss: 0.5238 - val_acc: 0.9424 - val_mDice: 0.5793

Epoch 00043: val_mDice did not improve from 0.58196
Epoch 44/300
 - 11s - loss: 0.4598 - acc: 0.9294 - mDice: 0.6160 - val_loss: 0.5176 - val_acc: 0.9414 - val_mDice: 0.5843

Epoch 00044: val_mDice improved from 0.58196 to 0.58434, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 13s - loss: 0.4454 - acc: 0.9307 - mDice: 0.6258 - val_loss: 0.5164 - val_acc: 0.9413 - val_mDice: 0.5863

Epoch 00045: val_mDice improved from 0.58434 to 0.58632, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 11s - loss: 0.4451 - acc: 0.9307 - mDice: 0.6260 - val_loss: 0.5265 - val_acc: 0.9387 - val_mDice: 0.5770

Epoch 00046: val_mDice did not improve from 0.58632
Epoch 47/300
 - 13s - loss: 0.4552 - acc: 0.9291 - mDice: 0.6190 - val_loss: 0.5222 - val_acc: 0.9414 - val_mDice: 0.5820

Epoch 00047: val_mDice did not improve from 0.58632
Epoch 48/300
 - 11s - loss: 0.4377 - acc: 0.9312 - mDice: 0.6298 - val_loss: 0.5160 - val_acc: 0.9413 - val_mDice: 0.5864

Epoch 00048: val_mDice improved from 0.58632 to 0.58641, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 13s - loss: 0.4360 - acc: 0.9315 - mDice: 0.6315 - val_loss: 0.5107 - val_acc: 0.9409 - val_mDice: 0.5891

Epoch 00049: val_mDice improved from 0.58641 to 0.58914, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 11s - loss: 0.4392 - acc: 0.9312 - mDice: 0.6290 - val_loss: 0.5062 - val_acc: 0.9417 - val_mDice: 0.5901

Epoch 00050: val_mDice improved from 0.58914 to 0.59013, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 13s - loss: 0.4319 - acc: 0.9319 - mDice: 0.6339 - val_loss: 0.5212 - val_acc: 0.9399 - val_mDice: 0.5838

Epoch 00051: val_mDice did not improve from 0.59013
Epoch 52/300
 - 11s - loss: 0.4304 - acc: 0.9321 - mDice: 0.6357 - val_loss: 0.5052 - val_acc: 0.9419 - val_mDice: 0.5915

Epoch 00052: val_mDice improved from 0.59013 to 0.59148, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 11s - loss: 0.4287 - acc: 0.9322 - mDice: 0.6359 - val_loss: 0.5131 - val_acc: 0.9404 - val_mDice: 0.5882

Epoch 00053: val_mDice did not improve from 0.59148
Epoch 54/300
 - 13s - loss: 0.4271 - acc: 0.9324 - mDice: 0.6370 - val_loss: 0.5065 - val_acc: 0.9419 - val_mDice: 0.5911

Epoch 00054: val_mDice did not improve from 0.59148
Epoch 55/300
 - 11s - loss: 0.4258 - acc: 0.9325 - mDice: 0.6379 - val_loss: 0.5105 - val_acc: 0.9429 - val_mDice: 0.5902

Epoch 00055: val_mDice did not improve from 0.59148
Epoch 56/300
 - 13s - loss: 0.4251 - acc: 0.9328 - mDice: 0.6398 - val_loss: 0.5222 - val_acc: 0.9390 - val_mDice: 0.5822

Epoch 00056: val_mDice did not improve from 0.59148
Epoch 57/300
 - 12s - loss: 0.4309 - acc: 0.9316 - mDice: 0.6347 - val_loss: 0.5092 - val_acc: 0.9419 - val_mDice: 0.5911

Epoch 00057: val_mDice did not improve from 0.59148
Epoch 58/300
 - 12s - loss: 0.4248 - acc: 0.9328 - mDice: 0.6405 - val_loss: 0.5172 - val_acc: 0.9413 - val_mDice: 0.5877

Epoch 00058: val_mDice did not improve from 0.59148
Epoch 59/300
 - 12s - loss: 0.4210 - acc: 0.9329 - mDice: 0.6420 - val_loss: 0.5172 - val_acc: 0.9427 - val_mDice: 0.5884

Epoch 00059: val_mDice did not improve from 0.59148
Epoch 60/300
 - 11s - loss: 0.4194 - acc: 0.9333 - mDice: 0.6431 - val_loss: 0.5190 - val_acc: 0.9415 - val_mDice: 0.5862

Epoch 00060: val_mDice did not improve from 0.59148
Epoch 61/300
 - 13s - loss: 0.4159 - acc: 0.9333 - mDice: 0.6445 - val_loss: 0.5195 - val_acc: 0.9417 - val_mDice: 0.5873

Epoch 00061: val_mDice did not improve from 0.59148
Epoch 62/300
 - 11s - loss: 0.4142 - acc: 0.9336 - mDice: 0.6461 - val_loss: 0.5345 - val_acc: 0.9403 - val_mDice: 0.5843

Epoch 00062: val_mDice did not improve from 0.59148
Epoch 63/300
 - 12s - loss: 0.4766 - acc: 0.9268 - mDice: 0.6068 - val_loss: 0.5273 - val_acc: 0.9397 - val_mDice: 0.5823

Epoch 00063: val_mDice did not improve from 0.59148
Epoch 64/300
 - 10s - loss: 0.4337 - acc: 0.9314 - mDice: 0.6326 - val_loss: 0.5139 - val_acc: 0.9394 - val_mDice: 0.5882

Epoch 00064: val_mDice did not improve from 0.59148
Epoch 65/300
 - 11s - loss: 0.4251 - acc: 0.9323 - mDice: 0.6393 - val_loss: 0.5195 - val_acc: 0.9396 - val_mDice: 0.5843

Epoch 00065: val_mDice did not improve from 0.59148
Epoch 66/300
 - 8s - loss: 0.4183 - acc: 0.9329 - mDice: 0.6426 - val_loss: 0.5072 - val_acc: 0.9408 - val_mDice: 0.5920

Epoch 00066: val_mDice improved from 0.59148 to 0.59197, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 67/300
 - 9s - loss: 0.4175 - acc: 0.9332 - mDice: 0.6441 - val_loss: 0.5070 - val_acc: 0.9416 - val_mDice: 0.5919

Epoch 00067: val_mDice did not improve from 0.59197
Epoch 68/300
 - 8s - loss: 0.4117 - acc: 0.9335 - mDice: 0.6471 - val_loss: 0.5197 - val_acc: 0.9405 - val_mDice: 0.5859

Epoch 00068: val_mDice did not improve from 0.59197
Epoch 69/300
 - 8s - loss: 0.4122 - acc: 0.9336 - mDice: 0.6471 - val_loss: 0.5115 - val_acc: 0.9406 - val_mDice: 0.5890

Epoch 00069: val_mDice did not improve from 0.59197
Epoch 70/300
 - 9s - loss: 0.4100 - acc: 0.9337 - mDice: 0.6491 - val_loss: 0.5181 - val_acc: 0.9412 - val_mDice: 0.5890

Epoch 00070: val_mDice did not improve from 0.59197
Epoch 71/300
 - 8s - loss: 0.4073 - acc: 0.9341 - mDice: 0.6510 - val_loss: 0.5158 - val_acc: 0.9405 - val_mDice: 0.5882

Epoch 00071: val_mDice did not improve from 0.59197
Epoch 72/300
 - 9s - loss: 0.4047 - acc: 0.9341 - mDice: 0.6517 - val_loss: 0.5132 - val_acc: 0.9413 - val_mDice: 0.5894

Epoch 00072: val_mDice did not improve from 0.59197
Epoch 73/300
 - 8s - loss: 0.4057 - acc: 0.9342 - mDice: 0.6521 - val_loss: 0.5137 - val_acc: 0.9412 - val_mDice: 0.5886

Epoch 00073: val_mDice did not improve from 0.59197
Epoch 74/300
 - 8s - loss: 0.4073 - acc: 0.9343 - mDice: 0.6510 - val_loss: 0.4989 - val_acc: 0.9419 - val_mDice: 0.5944

Epoch 00074: val_mDice improved from 0.59197 to 0.59439, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 75/300
 - 9s - loss: 0.4703 - acc: 0.9282 - mDice: 0.6100 - val_loss: 0.5757 - val_acc: 0.9397 - val_mDice: 0.5746

Epoch 00075: val_mDice did not improve from 0.59439
Epoch 76/300
 - 8s - loss: 0.4237 - acc: 0.9325 - mDice: 0.6405 - val_loss: 0.5201 - val_acc: 0.9403 - val_mDice: 0.5866

Epoch 00076: val_mDice did not improve from 0.59439
Epoch 77/300
 - 9s - loss: 0.4161 - acc: 0.9332 - mDice: 0.6450 - val_loss: 0.5190 - val_acc: 0.9401 - val_mDice: 0.5849

Epoch 00077: val_mDice did not improve from 0.59439
Epoch 78/300
 - 8s - loss: 0.4093 - acc: 0.9340 - mDice: 0.6496 - val_loss: 0.5083 - val_acc: 0.9407 - val_mDice: 0.5911

Epoch 00078: val_mDice did not improve from 0.59439
Epoch 79/300
 - 8s - loss: 0.4078 - acc: 0.9342 - mDice: 0.6508 - val_loss: 0.5159 - val_acc: 0.9406 - val_mDice: 0.5873

Epoch 00079: val_mDice did not improve from 0.59439
Epoch 80/300
 - 8s - loss: 0.4067 - acc: 0.9345 - mDice: 0.6524 - val_loss: 0.5186 - val_acc: 0.9400 - val_mDice: 0.5853

Epoch 00080: val_mDice did not improve from 0.59439
Epoch 81/300
 - 9s - loss: 0.4193 - acc: 0.9329 - mDice: 0.6434 - val_loss: 0.5132 - val_acc: 0.9411 - val_mDice: 0.5882

Epoch 00081: val_mDice did not improve from 0.59439
Epoch 82/300
 - 8s - loss: 0.4049 - acc: 0.9343 - mDice: 0.6526 - val_loss: 0.5121 - val_acc: 0.9407 - val_mDice: 0.5893

Epoch 00082: val_mDice did not improve from 0.59439
Epoch 83/300
 - 8s - loss: 0.4045 - acc: 0.9345 - mDice: 0.6529 - val_loss: 0.5085 - val_acc: 0.9410 - val_mDice: 0.5904

Epoch 00083: val_mDice did not improve from 0.59439
Epoch 84/300
 - 8s - loss: 0.4012 - acc: 0.9348 - mDice: 0.6551 - val_loss: 0.5055 - val_acc: 0.9416 - val_mDice: 0.5919

Epoch 00084: val_mDice did not improve from 0.59439
Epoch 85/300
 - 9s - loss: 0.3990 - acc: 0.9348 - mDice: 0.6557 - val_loss: 0.5096 - val_acc: 0.9416 - val_mDice: 0.5909

Epoch 00085: val_mDice did not improve from 0.59439
Epoch 86/300
 - 8s - loss: 0.3999 - acc: 0.9347 - mDice: 0.6553 - val_loss: 0.5104 - val_acc: 0.9416 - val_mDice: 0.5893

Epoch 00086: val_mDice did not improve from 0.59439
Epoch 87/300
 - 8s - loss: 0.3964 - acc: 0.9352 - mDice: 0.6576 - val_loss: 0.5069 - val_acc: 0.9429 - val_mDice: 0.5921

Epoch 00087: val_mDice did not improve from 0.59439
Epoch 88/300
 - 8s - loss: 0.3962 - acc: 0.9352 - mDice: 0.6578 - val_loss: 0.5108 - val_acc: 0.9413 - val_mDice: 0.5892

Epoch 00088: val_mDice did not improve from 0.59439
Epoch 89/300
 - 8s - loss: 0.3944 - acc: 0.9353 - mDice: 0.6589 - val_loss: 0.5063 - val_acc: 0.9423 - val_mDice: 0.5922

Epoch 00089: val_mDice did not improve from 0.59439
Epoch 90/300
 - 8s - loss: 0.3947 - acc: 0.9352 - mDice: 0.6594 - val_loss: 0.5058 - val_acc: 0.9420 - val_mDice: 0.5915

Epoch 00090: val_mDice did not improve from 0.59439
Epoch 91/300
 - 8s - loss: 0.3950 - acc: 0.9356 - mDice: 0.6604 - val_loss: 0.5085 - val_acc: 0.9432 - val_mDice: 0.5936

Epoch 00091: val_mDice did not improve from 0.59439
Epoch 92/300
 - 8s - loss: 0.3921 - acc: 0.9354 - mDice: 0.6603 - val_loss: 0.5074 - val_acc: 0.9425 - val_mDice: 0.5919

Epoch 00092: val_mDice did not improve from 0.59439
Epoch 93/300
 - 8s - loss: 0.3919 - acc: 0.9356 - mDice: 0.6607 - val_loss: 0.5031 - val_acc: 0.9427 - val_mDice: 0.5956

Epoch 00093: val_mDice improved from 0.59439 to 0.59558, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 94/300
 - 8s - loss: 0.3930 - acc: 0.9356 - mDice: 0.6606 - val_loss: 0.5138 - val_acc: 0.9421 - val_mDice: 0.5895

Epoch 00094: val_mDice did not improve from 0.59558
Epoch 95/300
 - 8s - loss: 0.3925 - acc: 0.9356 - mDice: 0.6602 - val_loss: 0.5092 - val_acc: 0.9418 - val_mDice: 0.5908

Epoch 00095: val_mDice did not improve from 0.59558
Epoch 96/300
 - 8s - loss: 0.3907 - acc: 0.9357 - mDice: 0.6623 - val_loss: 0.5059 - val_acc: 0.9426 - val_mDice: 0.5928

Epoch 00096: val_mDice did not improve from 0.59558
Epoch 97/300
 - 8s - loss: 0.3904 - acc: 0.9358 - mDice: 0.6625 - val_loss: 0.5118 - val_acc: 0.9423 - val_mDice: 0.5899

Epoch 00097: val_mDice did not improve from 0.59558
Epoch 98/300
 - 8s - loss: 0.3927 - acc: 0.9360 - mDice: 0.6613 - val_loss: 0.5194 - val_acc: 0.9418 - val_mDice: 0.5860

Epoch 00098: val_mDice did not improve from 0.59558
Epoch 99/300
 - 8s - loss: 0.3883 - acc: 0.9359 - mDice: 0.6631 - val_loss: 0.5186 - val_acc: 0.9407 - val_mDice: 0.5864

Epoch 00099: val_mDice did not improve from 0.59558
Epoch 100/300
 - 8s - loss: 0.3889 - acc: 0.9361 - mDice: 0.6638 - val_loss: 0.5197 - val_acc: 0.9420 - val_mDice: 0.5870

Epoch 00100: val_mDice did not improve from 0.59558
Epoch 101/300
 - 8s - loss: 0.3877 - acc: 0.9360 - mDice: 0.6636 - val_loss: 0.5070 - val_acc: 0.9416 - val_mDice: 0.5914

Epoch 00101: val_mDice did not improve from 0.59558
Epoch 102/300
 - 8s - loss: 0.3874 - acc: 0.9361 - mDice: 0.6638 - val_loss: 0.5027 - val_acc: 0.9423 - val_mDice: 0.5941

Epoch 00102: val_mDice did not improve from 0.59558
Epoch 103/300
 - 8s - loss: 0.3867 - acc: 0.9363 - mDice: 0.6652 - val_loss: 0.5154 - val_acc: 0.9433 - val_mDice: 0.5898

Epoch 00103: val_mDice did not improve from 0.59558
Epoch 104/300
 - 8s - loss: 0.3846 - acc: 0.9364 - mDice: 0.6658 - val_loss: 0.5218 - val_acc: 0.9424 - val_mDice: 0.5878

Epoch 00104: val_mDice did not improve from 0.59558
Epoch 105/300
 - 8s - loss: 0.3907 - acc: 0.9364 - mDice: 0.6662 - val_loss: 0.5153 - val_acc: 0.9437 - val_mDice: 0.5872

Epoch 00105: val_mDice did not improve from 0.59558
Epoch 106/300
 - 8s - loss: 0.4020 - acc: 0.9345 - mDice: 0.6537 - val_loss: 0.5274 - val_acc: 0.9418 - val_mDice: 0.5833

Epoch 00106: val_mDice did not improve from 0.59558
Epoch 107/300
 - 9s - loss: 0.3846 - acc: 0.9362 - mDice: 0.6655 - val_loss: 0.5111 - val_acc: 0.9421 - val_mDice: 0.5907

Epoch 00107: val_mDice did not improve from 0.59558
Epoch 108/300
 - 9s - loss: 0.3844 - acc: 0.9363 - mDice: 0.6658 - val_loss: 0.5294 - val_acc: 0.9423 - val_mDice: 0.5822

Epoch 00108: val_mDice did not improve from 0.59558
Epoch 109/300
 - 9s - loss: 0.3812 - acc: 0.9365 - mDice: 0.6679 - val_loss: 0.5065 - val_acc: 0.9438 - val_mDice: 0.5951

Epoch 00109: val_mDice did not improve from 0.59558
Epoch 110/300
 - 9s - loss: 0.3831 - acc: 0.9367 - mDice: 0.6676 - val_loss: 0.5128 - val_acc: 0.9419 - val_mDice: 0.5903

Epoch 00110: val_mDice did not improve from 0.59558
Epoch 111/300
 - 9s - loss: 0.3892 - acc: 0.9355 - mDice: 0.6624 - val_loss: 0.5199 - val_acc: 0.9430 - val_mDice: 0.5869

Epoch 00111: val_mDice did not improve from 0.59558
Epoch 112/300
 - 9s - loss: 0.3818 - acc: 0.9366 - mDice: 0.6678 - val_loss: 0.5133 - val_acc: 0.9440 - val_mDice: 0.5918

Epoch 00112: val_mDice did not improve from 0.59558
Epoch 113/300
 - 8s - loss: 0.3811 - acc: 0.9366 - mDice: 0.6681 - val_loss: 0.5089 - val_acc: 0.9436 - val_mDice: 0.5949

Epoch 00113: val_mDice did not improve from 0.59558
Epoch 114/300
 - 9s - loss: 0.3775 - acc: 0.9370 - mDice: 0.6706 - val_loss: 0.5114 - val_acc: 0.9433 - val_mDice: 0.5916

Epoch 00114: val_mDice did not improve from 0.59558
Epoch 115/300
 - 8s - loss: 0.3784 - acc: 0.9369 - mDice: 0.6701 - val_loss: 0.5049 - val_acc: 0.9433 - val_mDice: 0.5950

Epoch 00115: val_mDice did not improve from 0.59558
Epoch 116/300
 - 8s - loss: 0.3787 - acc: 0.9372 - mDice: 0.6706 - val_loss: 0.5082 - val_acc: 0.9418 - val_mDice: 0.5937

Epoch 00116: val_mDice did not improve from 0.59558
Epoch 117/300
 - 9s - loss: 0.3801 - acc: 0.9370 - mDice: 0.6707 - val_loss: 0.5230 - val_acc: 0.9420 - val_mDice: 0.5883

Epoch 00117: val_mDice did not improve from 0.59558
Epoch 118/300
 - 8s - loss: 0.3836 - acc: 0.9363 - mDice: 0.6664 - val_loss: 0.5094 - val_acc: 0.9442 - val_mDice: 0.5930

Epoch 00118: val_mDice did not improve from 0.59558
Epoch 119/300
 - 8s - loss: 0.3778 - acc: 0.9372 - mDice: 0.6721 - val_loss: 0.5093 - val_acc: 0.9437 - val_mDice: 0.5934

Epoch 00119: val_mDice did not improve from 0.59558
Epoch 120/300
 - 8s - loss: 0.3768 - acc: 0.9373 - mDice: 0.6714 - val_loss: 0.4977 - val_acc: 0.9440 - val_mDice: 0.5998

Epoch 00120: val_mDice improved from 0.59558 to 0.59984, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 121/300
 - 9s - loss: 0.3745 - acc: 0.9373 - mDice: 0.6727 - val_loss: 0.5066 - val_acc: 0.9427 - val_mDice: 0.5943

Epoch 00121: val_mDice did not improve from 0.59984
Epoch 122/300
 - 8s - loss: 0.3747 - acc: 0.9374 - mDice: 0.6727 - val_loss: 0.5026 - val_acc: 0.9426 - val_mDice: 0.5956

Epoch 00122: val_mDice did not improve from 0.59984
Epoch 123/300
 - 8s - loss: 0.3753 - acc: 0.9376 - mDice: 0.6740 - val_loss: 0.5233 - val_acc: 0.9435 - val_mDice: 0.5925

Epoch 00123: val_mDice did not improve from 0.59984
Epoch 124/300
 - 8s - loss: 0.3760 - acc: 0.9373 - mDice: 0.6718 - val_loss: 0.5051 - val_acc: 0.9427 - val_mDice: 0.5939

Epoch 00124: val_mDice did not improve from 0.59984
Epoch 125/300
 - 8s - loss: 0.3731 - acc: 0.9375 - mDice: 0.6738 - val_loss: 0.5144 - val_acc: 0.9436 - val_mDice: 0.5905

Epoch 00125: val_mDice did not improve from 0.59984
Epoch 126/300
 - 8s - loss: 0.3722 - acc: 0.9376 - mDice: 0.6744 - val_loss: 0.5103 - val_acc: 0.9450 - val_mDice: 0.5958

Epoch 00126: val_mDice did not improve from 0.59984
Epoch 127/300
 - 9s - loss: 0.3722 - acc: 0.9376 - mDice: 0.6744 - val_loss: 0.5160 - val_acc: 0.9431 - val_mDice: 0.5938

Epoch 00127: val_mDice did not improve from 0.59984
Epoch 128/300
 - 8s - loss: 0.3726 - acc: 0.9376 - mDice: 0.6744 - val_loss: 0.5120 - val_acc: 0.9417 - val_mDice: 0.5902

Epoch 00128: val_mDice did not improve from 0.59984
Epoch 129/300
 - 8s - loss: 0.3766 - acc: 0.9379 - mDice: 0.6758 - val_loss: 0.5282 - val_acc: 0.9395 - val_mDice: 0.5842

Epoch 00129: val_mDice did not improve from 0.59984
Epoch 130/300
 - 8s - loss: 0.3743 - acc: 0.9376 - mDice: 0.6748 - val_loss: 0.5210 - val_acc: 0.9422 - val_mDice: 0.5888

Epoch 00130: val_mDice did not improve from 0.59984
Epoch 131/300
 - 8s - loss: 0.3711 - acc: 0.9377 - mDice: 0.6751 - val_loss: 0.5205 - val_acc: 0.9443 - val_mDice: 0.5909

Epoch 00131: val_mDice did not improve from 0.59984
Epoch 132/300
 - 8s - loss: 0.3707 - acc: 0.9378 - mDice: 0.6754 - val_loss: 0.5106 - val_acc: 0.9446 - val_mDice: 0.5972

Epoch 00132: val_mDice did not improve from 0.59984
Epoch 133/300
 - 8s - loss: 0.3693 - acc: 0.9378 - mDice: 0.6764 - val_loss: 0.4987 - val_acc: 0.9438 - val_mDice: 0.5999

Epoch 00133: val_mDice improved from 0.59984 to 0.59993, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 134/300
 - 9s - loss: 0.3687 - acc: 0.9381 - mDice: 0.6776 - val_loss: 0.5066 - val_acc: 0.9441 - val_mDice: 0.5981

Epoch 00134: val_mDice did not improve from 0.59993
Epoch 135/300
 - 8s - loss: 0.3677 - acc: 0.9380 - mDice: 0.6776 - val_loss: 0.5208 - val_acc: 0.9442 - val_mDice: 0.5909

Epoch 00135: val_mDice did not improve from 0.59993
Epoch 136/300
 - 8s - loss: 0.3678 - acc: 0.9381 - mDice: 0.6775 - val_loss: 0.5130 - val_acc: 0.9426 - val_mDice: 0.5919

Epoch 00136: val_mDice did not improve from 0.59993
Epoch 137/300
 - 8s - loss: 0.3718 - acc: 0.9377 - mDice: 0.6747 - val_loss: 0.5251 - val_acc: 0.9422 - val_mDice: 0.5888

Epoch 00137: val_mDice did not improve from 0.59993
Epoch 138/300
 - 8s - loss: 0.3704 - acc: 0.9382 - mDice: 0.6784 - val_loss: 0.4957 - val_acc: 0.9439 - val_mDice: 0.5991

Epoch 00138: val_mDice did not improve from 0.59993
Epoch 139/300
 - 8s - loss: 0.3721 - acc: 0.9374 - mDice: 0.6746 - val_loss: 0.5409 - val_acc: 0.9442 - val_mDice: 0.5857

Epoch 00139: val_mDice did not improve from 0.59993
Epoch 140/300
 - 8s - loss: 0.3692 - acc: 0.9380 - mDice: 0.6775 - val_loss: 0.4984 - val_acc: 0.9445 - val_mDice: 0.5996

Epoch 00140: val_mDice did not improve from 0.59993
Epoch 141/300
 - 8s - loss: 0.3984 - acc: 0.9348 - mDice: 0.6579 - val_loss: 0.5631 - val_acc: 0.9434 - val_mDice: 0.5849

Epoch 00141: val_mDice did not improve from 0.59993
Epoch 142/300
 - 9s - loss: 0.4267 - acc: 0.9307 - mDice: 0.6390 - val_loss: 0.5277 - val_acc: 0.9430 - val_mDice: 0.5829

Epoch 00142: val_mDice did not improve from 0.59993
Epoch 143/300
 - 8s - loss: 0.4057 - acc: 0.9344 - mDice: 0.6529 - val_loss: 0.5321 - val_acc: 0.9428 - val_mDice: 0.5831

Epoch 00143: val_mDice did not improve from 0.59993
Epoch 144/300
 - 8s - loss: 0.3857 - acc: 0.9364 - mDice: 0.6668 - val_loss: 0.5128 - val_acc: 0.9440 - val_mDice: 0.5929

Epoch 00144: val_mDice did not improve from 0.59993
Epoch 145/300
 - 8s - loss: 0.3905 - acc: 0.9355 - mDice: 0.6618 - val_loss: 0.5390 - val_acc: 0.9423 - val_mDice: 0.5834

Epoch 00145: val_mDice did not improve from 0.59993
Epoch 146/300
 - 8s - loss: 0.3767 - acc: 0.9369 - mDice: 0.6713 - val_loss: 0.5290 - val_acc: 0.9437 - val_mDice: 0.5917

Epoch 00146: val_mDice did not improve from 0.59993
Epoch 147/300
 - 8s - loss: 0.3761 - acc: 0.9372 - mDice: 0.6732 - val_loss: 0.5217 - val_acc: 0.9445 - val_mDice: 0.5955

Epoch 00147: val_mDice did not improve from 0.59993
Epoch 148/300
 - 8s - loss: 0.3713 - acc: 0.9375 - mDice: 0.6748 - val_loss: 0.5311 - val_acc: 0.9437 - val_mDice: 0.5927

Epoch 00148: val_mDice did not improve from 0.59993
Epoch 149/300
 - 8s - loss: 0.3715 - acc: 0.9377 - mDice: 0.6758 - val_loss: 0.5279 - val_acc: 0.9439 - val_mDice: 0.5903

Epoch 00149: val_mDice did not improve from 0.59993
Epoch 150/300
 - 9s - loss: 0.3662 - acc: 0.9380 - mDice: 0.6785 - val_loss: 0.5223 - val_acc: 0.9437 - val_mDice: 0.5900

Epoch 00150: val_mDice did not improve from 0.59993
Epoch 151/300
 - 8s - loss: 0.3682 - acc: 0.9380 - mDice: 0.6780 - val_loss: 0.5178 - val_acc: 0.9429 - val_mDice: 0.5912

Epoch 00151: val_mDice did not improve from 0.59993
Epoch 152/300
 - 8s - loss: 0.3677 - acc: 0.9380 - mDice: 0.6776 - val_loss: 0.5233 - val_acc: 0.9432 - val_mDice: 0.5923

Epoch 00152: val_mDice did not improve from 0.59993
Epoch 153/300
 - 8s - loss: 0.3635 - acc: 0.9383 - mDice: 0.6804 - val_loss: 0.5143 - val_acc: 0.9430 - val_mDice: 0.5925

Epoch 00153: val_mDice did not improve from 0.59993
Epoch 154/300
 - 8s - loss: 0.3654 - acc: 0.9383 - mDice: 0.6795 - val_loss: 0.5103 - val_acc: 0.9432 - val_mDice: 0.5954

Epoch 00154: val_mDice did not improve from 0.59993
Epoch 155/300
 - 8s - loss: 0.3616 - acc: 0.9385 - mDice: 0.6818 - val_loss: 0.5123 - val_acc: 0.9442 - val_mDice: 0.5953

Epoch 00155: val_mDice did not improve from 0.59993
Epoch 156/300
 - 8s - loss: 0.3628 - acc: 0.9384 - mDice: 0.6809 - val_loss: 0.5096 - val_acc: 0.9441 - val_mDice: 0.5949

Epoch 00156: val_mDice did not improve from 0.59993
Epoch 157/300
 - 8s - loss: 0.3647 - acc: 0.9386 - mDice: 0.6807 - val_loss: 0.5138 - val_acc: 0.9431 - val_mDice: 0.5933

Epoch 00157: val_mDice did not improve from 0.59993
Epoch 158/300
 - 8s - loss: 0.3657 - acc: 0.9381 - mDice: 0.6789 - val_loss: 0.5082 - val_acc: 0.9449 - val_mDice: 0.5975

Epoch 00158: val_mDice did not improve from 0.59993
Epoch 159/300
 - 9s - loss: 0.3638 - acc: 0.9385 - mDice: 0.6803 - val_loss: 0.5231 - val_acc: 0.9445 - val_mDice: 0.5921

Epoch 00159: val_mDice did not improve from 0.59993
Epoch 160/300
 - 8s - loss: 0.3641 - acc: 0.9384 - mDice: 0.6808 - val_loss: 0.5163 - val_acc: 0.9438 - val_mDice: 0.5936

Epoch 00160: val_mDice did not improve from 0.59993
Epoch 161/300
 - 8s - loss: 0.3641 - acc: 0.9387 - mDice: 0.6819 - val_loss: 0.5069 - val_acc: 0.9454 - val_mDice: 0.5998

Epoch 00161: val_mDice did not improve from 0.59993
Epoch 162/300
 - 8s - loss: 0.3623 - acc: 0.9387 - mDice: 0.6820 - val_loss: 0.5214 - val_acc: 0.9435 - val_mDice: 0.5941

Epoch 00162: val_mDice did not improve from 0.59993
Epoch 163/300
 - 8s - loss: 0.3623 - acc: 0.9388 - mDice: 0.6829 - val_loss: 0.5125 - val_acc: 0.9437 - val_mDice: 0.5939

Epoch 00163: val_mDice did not improve from 0.59993
Epoch 164/300
 - 8s - loss: 0.3639 - acc: 0.9387 - mDice: 0.6820 - val_loss: 0.5375 - val_acc: 0.9435 - val_mDice: 0.5873

Epoch 00164: val_mDice did not improve from 0.59993
Epoch 165/300
 - 8s - loss: 0.3673 - acc: 0.9384 - mDice: 0.6791 - val_loss: 0.5379 - val_acc: 0.9428 - val_mDice: 0.5878

Epoch 00165: val_mDice did not improve from 0.59993
Epoch 166/300
 - 8s - loss: 0.3621 - acc: 0.9387 - mDice: 0.6814 - val_loss: 0.5282 - val_acc: 0.9439 - val_mDice: 0.5926

Epoch 00166: val_mDice did not improve from 0.59993
Epoch 167/300
 - 8s - loss: 0.3606 - acc: 0.9390 - mDice: 0.6840 - val_loss: 0.5138 - val_acc: 0.9429 - val_mDice: 0.5954

Epoch 00167: val_mDice did not improve from 0.59993
Epoch 168/300
 - 8s - loss: 0.3627 - acc: 0.9386 - mDice: 0.6810 - val_loss: 0.5067 - val_acc: 0.9450 - val_mDice: 0.5977

Epoch 00168: val_mDice did not improve from 0.59993
Epoch 169/300
 - 9s - loss: 0.3611 - acc: 0.9390 - mDice: 0.6832 - val_loss: 0.5070 - val_acc: 0.9432 - val_mDice: 0.5981

Epoch 00169: val_mDice did not improve from 0.59993
Epoch 170/300
 - 8s - loss: 0.3583 - acc: 0.9390 - mDice: 0.6841 - val_loss: 0.5022 - val_acc: 0.9453 - val_mDice: 0.5998

Epoch 00170: val_mDice did not improve from 0.59993
Epoch 171/300
 - 8s - loss: 0.3605 - acc: 0.9390 - mDice: 0.6828 - val_loss: 0.5180 - val_acc: 0.9440 - val_mDice: 0.5915

Epoch 00171: val_mDice did not improve from 0.59993
Epoch 172/300
 - 8s - loss: 0.3575 - acc: 0.9394 - mDice: 0.6855 - val_loss: 0.5135 - val_acc: 0.9437 - val_mDice: 0.5961

Epoch 00172: val_mDice did not improve from 0.59993
Epoch 173/300
 - 8s - loss: 0.3616 - acc: 0.9390 - mDice: 0.6846 - val_loss: 0.5071 - val_acc: 0.9444 - val_mDice: 0.5976

Epoch 00173: val_mDice did not improve from 0.59993
Restoring model weights from the end of the best epoch
Epoch 00173: early stopping
{'val_loss': [1.904090252668498, 1.400806987085822, 1.1537939043684378, 0.9986140481586563, 0.9306063542152916, 0.8933106094765264, 0.8451538978342238, 0.7609543214297162, 0.7492942926603988, 0.6768800852685001, 0.6640798429537086, 0.6525466265625128, 0.6269411124996633, 0.6016464316645148, 0.5879165941443523, 0.5896487199394397, 0.5868328456772106, 0.588409717189533, 0.5791231919933297, 0.5578563636241678, 0.564838701120302, 0.5662706011500438, 0.5471750955674901, 0.5522432911662416, 0.5517680818142172, 0.537490960772477, 0.5429831555102791, 0.5401916264155724, 0.5412520261783174, 0.5355915651974066, 0.5519452784314501, 0.5341765990470375, 0.5363352923086901, 0.5277558491882666, 0.5307650895758048, 0.5250393506867925, 0.5337079177355634, 0.5223103268852447, 0.5367545385600468, 0.5283797216814989, 0.5226402892080765, 0.5223562634191034, 0.523836585063508, 0.5175824651505028, 0.5163930527990757, 0.5265375159306234, 0.5221899374903247, 0.5159764662801221, 0.5107462179727394, 0.5062074591327645, 0.5212061716524582, 0.5052020076932854, 0.5131211322446109, 0.5064812815389154, 0.510481417345601, 0.5222174911525662, 0.5091995390433839, 0.5171827497429022, 0.5171867592374706, 0.5190296609308467, 0.5195416052248225, 0.5345295702611934, 0.5272833247091517, 0.5138751990968289, 0.5194576209483865, 0.5071873808040299, 0.506962800658615, 0.5196673037619565, 0.5114989315664302, 0.5180603164201342, 0.5158485342337432, 0.5132141975717172, 0.5137238677320534, 0.4989282502142411, 0.5756766439816139, 0.5201330439671458, 0.5190124884664014, 0.5083398740717818, 0.5158583854829799, 0.518612568771373, 0.5132324379249658, 0.5120868801071657, 0.508474770204981, 0.505543082786006, 0.5096470716945286, 0.5103844452503673, 0.5068943190841035, 0.5108387030702729, 0.5063453007010774, 0.5057940644591881, 0.5085264808971789, 0.5073521088954457, 0.5031240326066256, 0.513751556253966, 0.5092241447731103, 0.5059309641742173, 0.5118152530832664, 0.5194284181688085, 0.5186441317616894, 0.5196951368001587, 0.5069830484230425, 0.5026604196878784, 0.5153695250023677, 0.5217748822113655, 0.5153492560932756, 0.5274044265294208, 0.5111272233491503, 0.5294411555348828, 0.5065394128834069, 0.5128151887621959, 0.5199257528315709, 0.5133243502851305, 0.508946862300681, 0.5114314631043866, 0.5049345231921979, 0.5081537016277207, 0.5230224358968895, 0.5093737752410953, 0.5092974511604735, 0.4977130387082446, 0.5066111487383284, 0.5025518894528543, 0.5233331968664457, 0.5050705031975687, 0.5144465012923299, 0.5102853839956848, 0.5160442077913764, 0.5119689330042407, 0.5281521344984044, 0.5210186231402711, 0.5204769108215523, 0.5105714458327054, 0.49865178222762807, 0.5065693798677882, 0.5208302599757744, 0.5130379994155309, 0.5251234180434456, 0.49569277440369464, 0.5409481277345946, 0.4984413367076959, 0.5631068508718267, 0.5276890706749602, 0.5321285176876537, 0.5128132536091619, 0.5389675048476491, 0.5290009124651968, 0.5216892929383496, 0.5311445773646818, 0.5279358174214815, 0.5223401370328232, 0.5177973640364641, 0.5232553037518229, 0.5143380349907796, 0.5102847176890134, 0.5122780238782894, 0.509551127529677, 0.5138088246630557, 0.5081903972772247, 0.5231277453166813, 0.5162513014657537, 0.5069098549182188, 0.5213891912106029, 0.5124845048568768, 0.537542360787951, 0.5379344381433625, 0.528244346879714, 0.513793607330855, 0.5066835475700527, 0.5070029673629632, 0.5021904647017325, 0.5180206268859309, 0.5135126182153904, 0.5070703256063621], 'val_acc': [0.9063542528525411, 0.906347850181537, 0.906466801406285, 0.9064117959757757, 0.9064412163622553, 0.9065818916485963, 0.9068990736034329, 0.9116747962029953, 0.9268460766563202, 0.9319863212841183, 0.9323380083345169, 0.9323521043335259, 0.9342270669990411, 0.9338011681700552, 0.9362747379521418, 0.9357298785747763, 0.9360867252562965, 0.9363450977389373, 0.9393596266235054, 0.9362696379256648, 0.9373068836148225, 0.9369065548454583, 0.9379770512687428, 0.9388697973176754, 0.9391076867806845, 0.9386216668443307, 0.9396371758183953, 0.9388685053287271, 0.9397471573765718, 0.9393353452229632, 0.937305587963019, 0.9395693963466409, 0.9399415744083554, 0.9406104886998011, 0.9397394870246589, 0.9395693893539173, 0.9400682113024109, 0.9420416674800425, 0.9403802532723496, 0.9410120861490345, 0.9400311075775317, 0.9407268579445738, 0.9424125745309798, 0.9414430923302081, 0.9413036923834731, 0.9386805159419609, 0.9413714858406749, 0.9413164784122445, 0.9409148792980769, 0.941669483424565, 0.9398968083232475, 0.9418523697879727, 0.9403840886147995, 0.9419009688846226, 0.9428563823913063, 0.9390245442949859, 0.9419419129467543, 0.941313935058743, 0.9426619670244568, 0.9415428551881673, 0.9416758814337534, 0.9402792333890606, 0.939725417997584, 0.9393686009518927, 0.9396128637830639, 0.9408445265040052, 0.9415556388860308, 0.9404992251422818, 0.9405823453178619, 0.9412026478591577, 0.9404838594644429, 0.9412538152833224, 0.9412051962074621, 0.941854930789777, 0.9397395050059484, 0.9403137431464381, 0.9401052830605533, 0.9406693244779576, 0.9406449934623761, 0.9400464572720022, 0.9411016033348425, 0.9407486063142062, 0.9409647697177013, 0.9416451853746809, 0.9415773949143607, 0.9416323936851331, 0.9429279852179842, 0.9413177527528901, 0.9423051459829235, 0.9419546969776047, 0.943185061049861, 0.9424611589762085, 0.9427182414678222, 0.9421235299643191, 0.9417986596762801, 0.9426044092498012, 0.9422782766086429, 0.9417948200050013, 0.9406501201278005, 0.9419841080404526, 0.9415658918838927, 0.9422629289120935, 0.9432528578369311, 0.9423984975122207, 0.9436902770117008, 0.9418459721117712, 0.9420621251926742, 0.9422667579277934, 0.9437772358595992, 0.9418792231788848, 0.9429829963092697, 0.9439895432754601, 0.943613538195967, 0.9432746135322742, 0.9433423886751996, 0.9417705029748672, 0.9420173687641847, 0.9441724516159995, 0.9437350444287561, 0.9439908236098689, 0.9426875520684865, 0.9426287159573432, 0.9434856242973712, 0.9427412745006923, 0.9435662097771075, 0.9449960995652822, 0.9431198382510819, 0.9416528557265937, 0.9395271869345084, 0.9421836377522133, 0.944273508460828, 0.9446251975091476, 0.9438322469508847, 0.9441251235301268, 0.9442018473614527, 0.942608259576659, 0.942175945090182, 0.9438731473917402, 0.9442274490548246, 0.9444538248317867, 0.9433564673588929, 0.9430021903368347, 0.942825667018997, 0.9440061726383657, 0.9422961719875229, 0.9437491044651862, 0.9444627668604505, 0.9436940997006507, 0.9439281691386047, 0.9437056203794213, 0.9429484555841158, 0.9432029923913199, 0.9430137000270395, 0.9432439088155438, 0.9442197723761617, 0.9440790707838602, 0.9430814678442545, 0.9448566959556921, 0.944461494517726, 0.9437951412280845, 0.9453849376246916, 0.9435137640164551, 0.9437248037514074, 0.9435124943376253, 0.9427821942547846, 0.9438769950547032, 0.9428960101564503, 0.9450140232480438, 0.9432336631433924, 0.9452736454303038, 0.9439537265447265, 0.9437119960784912, 0.9444269757697036], 'val_mDice': [0.16037888150641372, 0.2586067347220202, 0.32407404024507747, 0.3808566276920574, 0.4079358912380048, 0.4276634188337699, 0.44970907878609345, 0.47003023584461745, 0.4807652155114286, 0.5042494032636035, 0.5081246748316888, 0.5144360178675731, 0.5271727855645079, 0.5376138071108131, 0.5445478435335213, 0.5428877579433292, 0.5450363262405609, 0.5484839414751064, 0.5526910677302483, 0.5598801537598977, 0.5575175278679618, 0.556682373225356, 0.5666624940307446, 0.5641844782083394, 0.564888852268624, 0.5714837036319285, 0.5691548562582645, 0.5700168586310062, 0.5715801409502935, 0.5734930111709253, 0.5665855371086291, 0.5731380262188406, 0.5733915470831887, 0.5785244233781399, 0.5770933877822407, 0.5801798727925264, 0.5768767988215612, 0.5818518919651735, 0.5757294503670165, 0.5783952294781222, 0.5819624985396529, 0.5817470980090136, 0.5792677182725022, 0.5843443704051012, 0.5863172415248509, 0.5770153493188613, 0.5819504983598294, 0.5864146158681901, 0.5891435968809288, 0.5901291696718951, 0.5837570838422083, 0.5914754774317396, 0.5882035513829919, 0.5910933397335714, 0.5902355243373849, 0.5821932028125785, 0.5910800595523259, 0.5876707138295946, 0.5884019715825939, 0.5862157448044036, 0.5873192725234857, 0.5842943151569899, 0.5822651306344144, 0.5881678851623109, 0.5842870493840905, 0.5919741601917331, 0.5918904566232053, 0.5859244269365705, 0.5889771650623343, 0.5890306374214215, 0.5882143514782356, 0.5894283166144814, 0.5885891344960176, 0.5943932050433238, 0.5746268516812245, 0.5865885532102105, 0.5849090614798349, 0.591076327768784, 0.5873249988982131, 0.5853430568838919, 0.5881794691085815, 0.5893092082199438, 0.5904396978836486, 0.5919477982894003, 0.5908636970226991, 0.5893258582280335, 0.5920945046334293, 0.589246555080627, 0.5922498919444377, 0.5915461705383642, 0.5935855341357226, 0.5918908588713108, 0.5955826686747248, 0.5894880574508752, 0.590820088732842, 0.5928287046581673, 0.5899108741536486, 0.5859644429643727, 0.5863849863659736, 0.5869621744368996, 0.5914475897836952, 0.5941266667243489, 0.5898342831840728, 0.5877793514528754, 0.5872344174864572, 0.5833034971572834, 0.5906746973538531, 0.582164162364086, 0.5951378378788186, 0.5902766498773457, 0.5869186680410161, 0.5918301010931004, 0.5948799389034676, 0.5916187623359638, 0.5949721273097246, 0.5937381060429792, 0.5883057010906368, 0.5930461786978738, 0.5933519971437294, 0.5998421301388873, 0.5943193808614209, 0.595581521535053, 0.5925214317257844, 0.5939353741081067, 0.5905426973737152, 0.5958100111124902, 0.5938181337697546, 0.5902056297776419, 0.5841997578823367, 0.5887969332700335, 0.5909484301865434, 0.597185984670117, 0.5999275542504294, 0.5980973703235222, 0.5909030317594219, 0.5918869998867952, 0.5887669011867246, 0.5990795772834863, 0.585734321751408, 0.5995927982490156, 0.5848583222767494, 0.5828998941282987, 0.5830993702291777, 0.5928843450945849, 0.5834116246447217, 0.5916738450194204, 0.595452135834614, 0.5926993499255048, 0.5902584221775972, 0.5899668732168954, 0.5911998409132718, 0.5923445587717621, 0.5925129865134895, 0.595384395655307, 0.5953237760666362, 0.5949277165215775, 0.5933232147600398, 0.5975488381012858, 0.5920505919935983, 0.5935566145614539, 0.5997910902486833, 0.5940814151444249, 0.5939478917494833, 0.5872634179765286, 0.5877906613509748, 0.592605644764181, 0.5953969306119994, 0.5976559636313156, 0.5980609485556959, 0.5997870717634702, 0.5915068487881282, 0.5961260848871156, 0.597558382836134], 'loss': [3.2570343652709584, 1.8418338804459962, 1.4097454116725532, 1.2029017294283773, 1.0613133345471053, 0.9636756967081398, 0.8875411000774532, 0.8356936402374604, 0.7889541738712397, 0.7461936974012461, 0.7137732407475104, 0.6780180114580959, 0.6539011414422363, 0.6369705961742362, 0.6147513442718592, 0.6180477884338528, 0.5972978990586078, 0.5772189094883496, 0.5711781267443152, 0.5668937928486065, 0.5532858252036766, 0.5445007600928428, 0.5355245303057257, 0.533913162246835, 0.5356308962905505, 0.5234615985059836, 0.5207961416635357, 0.5081459107030122, 0.5018714182994893, 0.4979267716407776, 0.4951048688199676, 0.48973436681095694, 0.4896785880087829, 0.4793810579681494, 0.476232481998254, 0.47435246667534603, 0.46806411820723387, 0.46477727656115275, 0.4625597562885187, 0.4596941751412681, 0.45588841933573854, 0.4530067934242428, 0.4622987607402391, 0.45979842867274756, 0.44541826893071657, 0.4451028734262361, 0.45519239461568534, 0.4376691584093649, 0.4359658815760593, 0.4392317873044092, 0.43188137313747993, 0.43039765095979465, 0.42870958393714464, 0.42710884614679656, 0.4257978935283227, 0.4250887217702436, 0.43089471024567966, 0.42478807878176694, 0.42101346527333144, 0.4193551322048316, 0.4158769751303509, 0.41419484386922883, 0.4766124306155033, 0.4336694112261299, 0.4251026896851473, 0.4183005996537013, 0.41748342446250014, 0.41174783133214615, 0.41216641191209924, 0.4099597929381445, 0.407308736327486, 0.40474357361309843, 0.40567037203639256, 0.4072510120993266, 0.47028092014007883, 0.42366093061253673, 0.41611832120745884, 0.4092501780537308, 0.40776130190638243, 0.4066777533439339, 0.4193072746523091, 0.4048650146568896, 0.4045164003113254, 0.4012470695755032, 0.39904483007725144, 0.39990157682876115, 0.39635109709056676, 0.3962154079717202, 0.39437359366871294, 0.3947181958460905, 0.3950227048919826, 0.3921035896131738, 0.391884854307673, 0.39300960665721385, 0.3925484830788413, 0.39067992946652114, 0.3904497645734275, 0.39273476362472676, 0.3882674061555843, 0.38885085474027964, 0.3876792990343004, 0.38738289170089313, 0.38674631891924827, 0.3846137632661667, 0.39071681237489475, 0.40198638018403876, 0.3846430258794886, 0.38441213183715695, 0.38120628068925905, 0.3831223418905598, 0.3891506889132691, 0.3818474925260563, 0.38109170587458574, 0.3774998949565848, 0.3784088537890892, 0.37870292102948566, 0.3801156399985317, 0.3836281083096735, 0.3777635171398765, 0.37675793774303845, 0.37445000934674116, 0.3747357577390847, 0.37530472850213287, 0.3760322640482031, 0.3730838955609036, 0.3721714475360073, 0.37223556448445944, 0.3726183394672441, 0.37660321367324373, 0.3742966618327821, 0.37113535175191575, 0.3707235548339906, 0.36933705745051143, 0.3686829557428595, 0.36768052009407615, 0.3677568382537756, 0.3717729109415754, 0.370367938164072, 0.37209265517284634, 0.36924183011421413, 0.3984209157709704, 0.4266946008886959, 0.4057144863439388, 0.3857051850769852, 0.39049052834877224, 0.37665708723371144, 0.3760513636115633, 0.3713445166034288, 0.37148520264957774, 0.3662421813570574, 0.36818556582219286, 0.3676583572428246, 0.3635071703522909, 0.36541547088838017, 0.36162811627642055, 0.3628431645939585, 0.3646893871367955, 0.36572669840372, 0.36377014192279244, 0.36413086602678063, 0.3641179406740626, 0.3623444343321636, 0.36234922059735314, 0.36385061842252, 0.3672823673572208, 0.36205172663951507, 0.3605533010769086, 0.36274989196633706, 0.361058203961517, 0.3582755512878543, 0.3604587027039684, 0.3575257282154482, 0.36160486289223687], 'acc': [0.8033668840090271, 0.8760383367294171, 0.87637550157846, 0.8766948918460823, 0.8769275351259552, 0.8772006633340336, 0.8777958787366992, 0.8813985766323863, 0.8969685446898468, 0.9025703878676306, 0.9058685705187868, 0.9090210872351147, 0.9115235909941743, 0.9134302478222573, 0.9149866256191105, 0.9148290186631874, 0.9164694184895421, 0.9182259044686302, 0.9192887769981486, 0.919124462443297, 0.9206386665340329, 0.9214232719091119, 0.9220702779464057, 0.9225064075750405, 0.9226998560374877, 0.9236569610409072, 0.9233707891868763, 0.9247969110481075, 0.9253256250111783, 0.9258955555861114, 0.9261038491471869, 0.926779847714256, 0.926697845830292, 0.9274377877228572, 0.9277580898926884, 0.928069030896562, 0.9284719734406862, 0.9288071217473413, 0.9291134441240889, 0.9294091158348029, 0.9296796195453307, 0.9298374806759787, 0.9294863340185314, 0.9293826588231032, 0.9306997989044815, 0.9307273857784076, 0.929112787679082, 0.9311740946207867, 0.9315307035309369, 0.9312305558533942, 0.9318745819027306, 0.9320666241596957, 0.9321650026030228, 0.9323977601454884, 0.932483426432629, 0.9327651175563453, 0.9315573551737871, 0.9327828520756276, 0.932944467321771, 0.933320199002008, 0.9333415418008312, 0.9335871127540948, 0.9268098738594134, 0.9313959258257366, 0.9323059529310367, 0.932881159188806, 0.9331899185771825, 0.9335191587566353, 0.9335753360732657, 0.9337339259561945, 0.9341027137563854, 0.9340814153556941, 0.9342481708917462, 0.9342720965381528, 0.928155492258365, 0.9324988365539761, 0.9331833517453709, 0.9339858074290831, 0.9342072379515797, 0.9344704477879845, 0.9328529870168107, 0.9342688361762977, 0.9344505093625335, 0.9347963744133222, 0.9348182141414432, 0.9346923672395652, 0.9351550543405971, 0.9351763742380454, 0.9352516003685897, 0.9352093102624182, 0.9355778389411872, 0.9354498093856163, 0.9355606919795764, 0.935609270803264, 0.9356471055843791, 0.9356940895563266, 0.9358248592155879, 0.9359572495593399, 0.9358889910652012, 0.936131370360734, 0.9360256961501036, 0.936051009375541, 0.9362684254397134, 0.9364118877126545, 0.9364349464046173, 0.9345479466509624, 0.9362256174571202, 0.9363146364444592, 0.9364751054004568, 0.936694403285863, 0.935518890619278, 0.9366383162311843, 0.9366391126493938, 0.9369848197112318, 0.936933988980094, 0.9371895749182975, 0.9370270658467637, 0.9362815614240091, 0.9371850715308893, 0.9372997034768589, 0.9373225262541263, 0.9373751181925907, 0.9375583832991905, 0.9372558849634695, 0.93747558005032, 0.937567344026976, 0.9376337972820782, 0.9376194878924088, 0.9378512404004081, 0.9376416776756771, 0.9376739309337295, 0.9378278784942432, 0.9378323594077689, 0.9380779518577896, 0.9380253382393571, 0.9381065212922995, 0.9377107584207761, 0.9381633559699918, 0.9374084229596326, 0.9379684297887029, 0.9347809668569291, 0.9306945210841836, 0.9344089650472657, 0.9363809975444294, 0.9354575746005676, 0.9369167931622169, 0.9372339319498812, 0.9374862786565648, 0.9377177457828991, 0.9380223109218918, 0.9379603849937681, 0.9379655686436129, 0.9382649922102201, 0.9383483615810754, 0.9385228074598507, 0.9384499523239057, 0.9385900342317878, 0.9380600520821868, 0.9385433075491523, 0.9384450727432477, 0.9386941582941618, 0.9387123148705139, 0.9388424051223231, 0.9386560422841643, 0.9384084786670129, 0.9386682628852422, 0.9389608377193819, 0.9385652179356481, 0.9389873930543172, 0.9390491776046206, 0.9390027090052112, 0.9393616919634772, 0.9389745828191765], 'mDice': [0.06205351468553522, 0.16785476748358275, 0.2431876025025229, 0.291945930753575, 0.33362409109097035, 0.3680433839193133, 0.397290131748944, 0.4197953713172283, 0.4394914022051409, 0.4595142867355073, 0.47352469059043245, 0.4909693134612724, 0.5036529151753324, 0.513345812890129, 0.5256009685700057, 0.5244359101672642, 0.5336171140375195, 0.5447566997504136, 0.5509557391776413, 0.5512254970667304, 0.558824736044788, 0.5639340932496258, 0.5690881648390996, 0.5710310179923401, 0.5703490189902607, 0.577302210063475, 0.5777090017790677, 0.5854059984815903, 0.5892322497656111, 0.5916901089128901, 0.593391127884388, 0.5973013279745814, 0.5968946788887508, 0.6032853248055841, 0.6050647481909541, 0.6065446549996001, 0.610272610651665, 0.6131775636897713, 0.6137299587125661, 0.6159469904836083, 0.6181376273514795, 0.6209787316009646, 0.6178312538893985, 0.6159876437216508, 0.6257760111914307, 0.625953619597388, 0.61897603727755, 0.6298248455783383, 0.6314522150720729, 0.6290130486375973, 0.6339047647646217, 0.6356570387228591, 0.6359321909849761, 0.6370238642223546, 0.6378669976943829, 0.6398197866243417, 0.6346543047760353, 0.6405263605115356, 0.6420113018057385, 0.6431209026057212, 0.6444769284886415, 0.646090497919282, 0.6068278213382744, 0.6325804779886223, 0.6393328564333134, 0.6425937085000218, 0.6441128637458458, 0.6470583115444809, 0.6471434688103981, 0.6490766844544255, 0.6510423381553322, 0.6517264402059258, 0.6520807045038606, 0.6509780132623969, 0.6099694260686147, 0.6405223599467121, 0.6449765802406874, 0.6496470765378631, 0.6508158952852742, 0.6523752933154341, 0.6434408322159872, 0.6525964934806354, 0.6529219760025133, 0.6550991199910641, 0.6557252512114947, 0.6553386327062474, 0.6575721660476239, 0.6578354461759818, 0.65887768304006, 0.6593792609504012, 0.6604435257308308, 0.6603000670305041, 0.6607243041278886, 0.6606294513970125, 0.6602436829297269, 0.6623162346907326, 0.6625316608391825, 0.661322176395381, 0.6631448806309309, 0.6637857906886788, 0.663559940628341, 0.6637704115910609, 0.6652051797900044, 0.6658244657589764, 0.666205472083854, 0.6537221457625999, 0.665486541927838, 0.6658168534641383, 0.6679014068280087, 0.6676453405105677, 0.6623917433937065, 0.667849278596581, 0.6680690205243768, 0.6706105953112977, 0.6701413895018765, 0.6705847074750995, 0.6706740774336408, 0.666401486416332, 0.6721047884860977, 0.6714428197897848, 0.6727154758132872, 0.6726795452051475, 0.673986668469476, 0.6718161925673485, 0.6737737270896552, 0.6744205477784891, 0.674421215582578, 0.674361275539535, 0.6757966298915323, 0.674776204480011, 0.6750726276000992, 0.6754002505760701, 0.6763812239419241, 0.6776038018650696, 0.6775502778101162, 0.6774846190189729, 0.6747135519370681, 0.6783716057228749, 0.6746164857730513, 0.677460964402703, 0.6578947227509295, 0.6389932202511146, 0.6529363306452993, 0.6667783564475717, 0.661776677140447, 0.6713365251045735, 0.6731715654252005, 0.6747642008618253, 0.6757500277923756, 0.6784683550845404, 0.678028978040961, 0.6776171507405453, 0.6803728119760263, 0.6795369412322514, 0.6817617580050328, 0.6808996083306484, 0.6807274599788619, 0.6788674572207889, 0.6803145567413236, 0.6808271315864852, 0.6819211178016468, 0.6820010511112995, 0.6829292184871728, 0.682002031412281, 0.6790620665691915, 0.6813786420665804, 0.6840104449112885, 0.6809578980700891, 0.683245376851715, 0.6840684525668621, 0.6828052475437766, 0.6854973440898247, 0.6846411070618473]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:03<00:15,  3.89s/it]predicting test subjects:  40%|████      | 2/5 [00:07<00:11,  3.67s/it]predicting test subjects:  60%|██████    | 3/5 [00:09<00:06,  3.45s/it]predicting test subjects:  80%|████████  | 4/5 [00:12<00:03,  3.26s/it]predicting test subjects: 100%|██████████| 5/5 [00:15<00:00,  3.24s/it]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:03<16:55,  3.83s/it]predicting train subjects:   1%|          | 2/266 [00:07<16:31,  3.76s/it]predicting train subjects:   1%|          | 3/266 [00:10<15:38,  3.57s/it]predicting train subjects:   2%|▏         | 4/266 [00:13<14:45,  3.38s/it]predicting train subjects:   2%|▏         | 5/266 [00:16<14:48,  3.40s/it]predicting train subjects:   2%|▏         | 6/266 [00:20<15:01,  3.47s/it]predicting train subjects:   3%|▎         | 7/266 [00:24<15:09,  3.51s/it]predicting train subjects:   3%|▎         | 8/266 [00:28<15:34,  3.62s/it]predicting train subjects:   3%|▎         | 9/266 [00:31<15:38,  3.65s/it]predicting train subjects:   4%|▍         | 10/266 [00:35<15:26,  3.62s/it]predicting train subjects:   4%|▍         | 11/266 [00:38<15:23,  3.62s/it]predicting train subjects:   5%|▍         | 12/266 [00:42<15:19,  3.62s/it]predicting train subjects:   5%|▍         | 13/266 [00:46<15:11,  3.60s/it]predicting train subjects:   5%|▌         | 14/266 [00:49<15:15,  3.63s/it]predicting train subjects:   6%|▌         | 15/266 [00:53<15:06,  3.61s/it]predicting train subjects:   6%|▌         | 16/266 [00:57<15:03,  3.61s/it]predicting train subjects:   6%|▋         | 17/266 [01:00<14:59,  3.61s/it]predicting train subjects:   7%|▋         | 18/266 [01:04<14:59,  3.63s/it]predicting train subjects:   7%|▋         | 19/266 [01:07<14:31,  3.53s/it]predicting train subjects:   8%|▊         | 20/266 [01:11<14:30,  3.54s/it]predicting train subjects:   8%|▊         | 21/266 [01:14<14:23,  3.52s/it]predicting train subjects:   8%|▊         | 22/266 [01:18<14:35,  3.59s/it]predicting train subjects:   9%|▊         | 23/266 [01:21<14:34,  3.60s/it]predicting train subjects:   9%|▉         | 24/266 [01:25<14:17,  3.54s/it]predicting train subjects:   9%|▉         | 25/266 [01:28<14:08,  3.52s/it]predicting train subjects:  10%|▉         | 26/266 [01:32<13:52,  3.47s/it]predicting train subjects:  10%|█         | 27/266 [01:35<13:45,  3.45s/it]predicting train subjects:  11%|█         | 28/266 [01:39<13:38,  3.44s/it]predicting train subjects:  11%|█         | 29/266 [01:42<13:35,  3.44s/it]predicting train subjects:  11%|█▏        | 30/266 [01:45<13:33,  3.45s/it]predicting train subjects:  12%|█▏        | 31/266 [01:49<13:24,  3.42s/it]predicting train subjects:  12%|█▏        | 32/266 [01:52<13:13,  3.39s/it]predicting train subjects:  12%|█▏        | 33/266 [01:56<13:15,  3.41s/it]predicting train subjects:  13%|█▎        | 34/266 [01:59<13:06,  3.39s/it]predicting train subjects:  13%|█▎        | 35/266 [02:02<12:49,  3.33s/it]predicting train subjects:  14%|█▎        | 36/266 [02:05<12:44,  3.32s/it]predicting train subjects:  14%|█▍        | 37/266 [02:09<12:51,  3.37s/it]predicting train subjects:  14%|█▍        | 38/266 [02:12<12:52,  3.39s/it]predicting train subjects:  15%|█▍        | 39/266 [02:16<12:54,  3.41s/it]predicting train subjects:  15%|█▌        | 40/266 [02:19<12:47,  3.39s/it]predicting train subjects:  15%|█▌        | 41/266 [02:22<12:39,  3.38s/it]predicting train subjects:  16%|█▌        | 42/266 [02:25<12:05,  3.24s/it]predicting train subjects:  16%|█▌        | 43/266 [02:28<11:43,  3.15s/it]predicting train subjects:  17%|█▋        | 44/266 [02:31<11:29,  3.10s/it]predicting train subjects:  17%|█▋        | 45/266 [02:34<11:12,  3.04s/it]predicting train subjects:  17%|█▋        | 46/266 [02:37<11:01,  3.01s/it]predicting train subjects:  18%|█▊        | 47/266 [02:40<10:56,  3.00s/it]predicting train subjects:  18%|█▊        | 48/266 [02:43<10:49,  2.98s/it]predicting train subjects:  18%|█▊        | 49/266 [02:46<10:39,  2.95s/it]predicting train subjects:  19%|█▉        | 50/266 [02:49<10:24,  2.89s/it]predicting train subjects:  19%|█▉        | 51/266 [02:52<10:19,  2.88s/it]predicting train subjects:  20%|█▉        | 52/266 [02:54<10:16,  2.88s/it]predicting train subjects:  20%|█▉        | 53/266 [02:57<10:18,  2.91s/it]predicting train subjects:  20%|██        | 54/266 [03:00<10:10,  2.88s/it]predicting train subjects:  21%|██        | 55/266 [03:03<10:07,  2.88s/it]predicting train subjects:  21%|██        | 56/266 [03:06<10:08,  2.90s/it]predicting train subjects:  21%|██▏       | 57/266 [03:09<10:09,  2.92s/it]predicting train subjects:  22%|██▏       | 58/266 [03:12<10:09,  2.93s/it]predicting train subjects:  22%|██▏       | 59/266 [03:15<10:03,  2.92s/it]predicting train subjects:  23%|██▎       | 60/266 [03:18<09:58,  2.90s/it]predicting train subjects:  23%|██▎       | 61/266 [03:21<09:48,  2.87s/it]predicting train subjects:  23%|██▎       | 62/266 [03:23<09:34,  2.82s/it]predicting train subjects:  24%|██▎       | 63/266 [03:26<09:07,  2.70s/it]predicting train subjects:  24%|██▍       | 64/266 [03:28<08:57,  2.66s/it]predicting train subjects:  24%|██▍       | 65/266 [03:31<09:01,  2.70s/it]predicting train subjects:  25%|██▍       | 66/266 [03:34<08:50,  2.65s/it]predicting train subjects:  25%|██▌       | 67/266 [03:36<08:50,  2.67s/it]predicting train subjects:  26%|██▌       | 68/266 [03:39<08:47,  2.66s/it]predicting train subjects:  26%|██▌       | 69/266 [03:41<08:38,  2.63s/it]predicting train subjects:  26%|██▋       | 70/266 [03:44<08:34,  2.62s/it]predicting train subjects:  27%|██▋       | 71/266 [03:47<08:37,  2.65s/it]predicting train subjects:  27%|██▋       | 72/266 [03:49<08:35,  2.66s/it]predicting train subjects:  27%|██▋       | 73/266 [03:52<08:42,  2.71s/it]predicting train subjects:  28%|██▊       | 74/266 [03:55<08:38,  2.70s/it]predicting train subjects:  28%|██▊       | 75/266 [03:58<08:42,  2.74s/it]predicting train subjects:  29%|██▊       | 76/266 [04:00<08:15,  2.61s/it]predicting train subjects:  29%|██▉       | 77/266 [04:03<08:22,  2.66s/it]predicting train subjects:  29%|██▉       | 78/266 [04:05<08:07,  2.59s/it]predicting train subjects:  30%|██▉       | 79/266 [04:08<08:02,  2.58s/it]predicting train subjects:  30%|███       | 80/266 [04:11<08:34,  2.77s/it]predicting train subjects:  30%|███       | 81/266 [04:14<09:07,  2.96s/it]predicting train subjects:  31%|███       | 82/266 [04:18<09:40,  3.16s/it]predicting train subjects:  31%|███       | 83/266 [04:22<10:00,  3.28s/it]predicting train subjects:  32%|███▏      | 84/266 [04:25<10:08,  3.34s/it]predicting train subjects:  32%|███▏      | 85/266 [04:28<10:04,  3.34s/it]predicting train subjects:  32%|███▏      | 86/266 [04:32<10:11,  3.40s/it]predicting train subjects:  33%|███▎      | 87/266 [04:35<10:06,  3.39s/it]predicting train subjects:  33%|███▎      | 88/266 [04:39<10:05,  3.40s/it]predicting train subjects:  33%|███▎      | 89/266 [04:42<09:58,  3.38s/it]predicting train subjects:  34%|███▍      | 90/266 [04:46<09:55,  3.39s/it]predicting train subjects:  34%|███▍      | 91/266 [04:49<09:57,  3.41s/it]predicting train subjects:  35%|███▍      | 92/266 [04:52<09:55,  3.42s/it]predicting train subjects:  35%|███▍      | 93/266 [04:56<09:53,  3.43s/it]predicting train subjects:  35%|███▌      | 94/266 [04:59<09:45,  3.40s/it]predicting train subjects:  36%|███▌      | 95/266 [05:03<09:42,  3.41s/it]predicting train subjects:  36%|███▌      | 96/266 [05:06<09:24,  3.32s/it]predicting train subjects:  36%|███▋      | 97/266 [05:09<09:32,  3.39s/it]predicting train subjects:  37%|███▋      | 98/266 [05:13<09:21,  3.34s/it]predicting train subjects:  37%|███▋      | 99/266 [05:15<08:33,  3.08s/it]predicting train subjects:  38%|███▊      | 100/266 [05:18<08:08,  2.94s/it]predicting train subjects:  38%|███▊      | 101/266 [05:21<08:03,  2.93s/it]predicting train subjects:  38%|███▊      | 102/266 [05:24<08:06,  2.97s/it]predicting train subjects:  39%|███▊      | 103/266 [05:27<08:08,  3.00s/it]predicting train subjects:  39%|███▉      | 104/266 [05:30<08:08,  3.01s/it]predicting train subjects:  39%|███▉      | 105/266 [05:33<08:04,  3.01s/it]predicting train subjects:  40%|███▉      | 106/266 [05:36<08:02,  3.01s/it]predicting train subjects:  40%|████      | 107/266 [05:39<08:03,  3.04s/it]predicting train subjects:  41%|████      | 108/266 [05:42<08:00,  3.04s/it]predicting train subjects:  41%|████      | 109/266 [05:45<07:56,  3.04s/it]predicting train subjects:  41%|████▏     | 110/266 [05:48<07:51,  3.02s/it]predicting train subjects:  42%|████▏     | 111/266 [05:51<07:53,  3.05s/it]predicting train subjects:  42%|████▏     | 112/266 [05:54<07:48,  3.05s/it]predicting train subjects:  42%|████▏     | 113/266 [05:57<07:45,  3.04s/it]predicting train subjects:  43%|████▎     | 114/266 [06:00<07:45,  3.06s/it]predicting train subjects:  43%|████▎     | 115/266 [06:03<07:36,  3.02s/it]predicting train subjects:  44%|████▎     | 116/266 [06:06<07:31,  3.01s/it]predicting train subjects:  44%|████▍     | 117/266 [06:09<07:30,  3.02s/it]predicting train subjects:  44%|████▍     | 118/266 [06:12<07:26,  3.02s/it]predicting train subjects:  45%|████▍     | 119/266 [06:16<07:39,  3.13s/it]predicting train subjects:  45%|████▌     | 120/266 [06:19<07:45,  3.19s/it]predicting train subjects:  45%|████▌     | 121/266 [06:22<07:52,  3.26s/it]predicting train subjects:  46%|████▌     | 122/266 [06:26<07:53,  3.29s/it]predicting train subjects:  46%|████▌     | 123/266 [06:29<07:51,  3.30s/it]predicting train subjects:  47%|████▋     | 124/266 [06:32<07:51,  3.32s/it]predicting train subjects:  47%|████▋     | 125/266 [06:36<07:51,  3.35s/it]predicting train subjects:  47%|████▋     | 126/266 [06:39<07:49,  3.35s/it]predicting train subjects:  48%|████▊     | 127/266 [06:43<07:50,  3.39s/it]predicting train subjects:  48%|████▊     | 128/266 [06:46<07:48,  3.40s/it]predicting train subjects:  48%|████▊     | 129/266 [06:49<07:47,  3.41s/it]predicting train subjects:  49%|████▉     | 130/266 [06:53<07:44,  3.42s/it]predicting train subjects:  49%|████▉     | 131/266 [06:56<07:41,  3.42s/it]predicting train subjects:  50%|████▉     | 132/266 [07:00<07:36,  3.41s/it]predicting train subjects:  50%|█████     | 133/266 [07:03<07:31,  3.39s/it]predicting train subjects:  50%|█████     | 134/266 [07:07<07:30,  3.41s/it]predicting train subjects:  51%|█████     | 135/266 [07:10<07:29,  3.43s/it]predicting train subjects:  51%|█████     | 136/266 [07:13<07:27,  3.44s/it]predicting train subjects:  52%|█████▏    | 137/266 [07:17<07:20,  3.42s/it]predicting train subjects:  52%|█████▏    | 138/266 [07:20<07:12,  3.38s/it]predicting train subjects:  52%|█████▏    | 139/266 [07:23<07:03,  3.34s/it]predicting train subjects:  53%|█████▎    | 140/266 [07:27<06:59,  3.33s/it]predicting train subjects:  53%|█████▎    | 141/266 [07:30<06:54,  3.31s/it]predicting train subjects:  53%|█████▎    | 142/266 [07:33<06:51,  3.32s/it]predicting train subjects:  54%|█████▍    | 143/266 [07:37<06:46,  3.30s/it]predicting train subjects:  54%|█████▍    | 144/266 [07:40<06:43,  3.31s/it]predicting train subjects:  55%|█████▍    | 145/266 [07:43<06:39,  3.30s/it]predicting train subjects:  55%|█████▍    | 146/266 [07:46<06:33,  3.28s/it]predicting train subjects:  55%|█████▌    | 147/266 [07:50<06:35,  3.32s/it]predicting train subjects:  56%|█████▌    | 148/266 [07:53<06:30,  3.31s/it]predicting train subjects:  56%|█████▌    | 149/266 [07:56<06:26,  3.31s/it]predicting train subjects:  56%|█████▋    | 150/266 [08:00<06:24,  3.31s/it]predicting train subjects:  57%|█████▋    | 151/266 [08:03<06:18,  3.29s/it]predicting train subjects:  57%|█████▋    | 152/266 [08:06<06:17,  3.31s/it]predicting train subjects:  58%|█████▊    | 153/266 [08:10<06:14,  3.31s/it]predicting train subjects:  58%|█████▊    | 154/266 [08:13<06:16,  3.36s/it]predicting train subjects:  58%|█████▊    | 155/266 [08:16<05:51,  3.16s/it]predicting train subjects:  59%|█████▊    | 156/266 [08:18<05:21,  2.92s/it]predicting train subjects:  59%|█████▉    | 157/266 [08:20<05:00,  2.75s/it]predicting train subjects:  59%|█████▉    | 158/266 [08:23<04:45,  2.64s/it]predicting train subjects:  60%|█████▉    | 159/266 [08:25<04:34,  2.57s/it]predicting train subjects:  60%|██████    | 160/266 [08:28<04:25,  2.50s/it]predicting train subjects:  61%|██████    | 161/266 [08:30<04:16,  2.44s/it]predicting train subjects:  61%|██████    | 162/266 [08:32<04:13,  2.44s/it]predicting train subjects:  61%|██████▏   | 163/266 [08:35<04:08,  2.41s/it]predicting train subjects:  62%|██████▏   | 164/266 [08:37<04:02,  2.38s/it]predicting train subjects:  62%|██████▏   | 165/266 [08:39<04:02,  2.40s/it]predicting train subjects:  62%|██████▏   | 166/266 [08:42<04:00,  2.40s/it]predicting train subjects:  63%|██████▎   | 167/266 [08:44<03:59,  2.42s/it]predicting train subjects:  63%|██████▎   | 168/266 [08:47<03:58,  2.44s/it]predicting train subjects:  64%|██████▎   | 169/266 [08:49<03:55,  2.43s/it]predicting train subjects:  64%|██████▍   | 170/266 [08:52<03:51,  2.41s/it]predicting train subjects:  64%|██████▍   | 171/266 [08:54<03:49,  2.42s/it]predicting train subjects:  65%|██████▍   | 172/266 [08:56<03:45,  2.40s/it]predicting train subjects:  65%|██████▌   | 173/266 [08:59<03:55,  2.54s/it]predicting train subjects:  65%|██████▌   | 174/266 [09:02<04:12,  2.75s/it]predicting train subjects:  66%|██████▌   | 175/266 [09:06<04:26,  2.93s/it]predicting train subjects:  66%|██████▌   | 176/266 [09:09<04:42,  3.14s/it]predicting train subjects:  67%|██████▋   | 177/266 [09:13<04:52,  3.29s/it]predicting train subjects:  67%|██████▋   | 178/266 [09:17<04:59,  3.40s/it]predicting train subjects:  67%|██████▋   | 179/266 [09:20<04:53,  3.37s/it]predicting train subjects:  68%|██████▊   | 180/266 [09:23<04:51,  3.39s/it]predicting train subjects:  68%|██████▊   | 181/266 [09:27<04:41,  3.31s/it]predicting train subjects:  68%|██████▊   | 182/266 [09:30<04:35,  3.28s/it]predicting train subjects:  69%|██████▉   | 183/266 [09:33<04:42,  3.40s/it]predicting train subjects:  69%|██████▉   | 184/266 [09:37<04:42,  3.44s/it]predicting train subjects:  70%|██████▉   | 185/266 [09:40<04:37,  3.43s/it]predicting train subjects:  70%|██████▉   | 186/266 [09:44<04:27,  3.34s/it]predicting train subjects:  70%|███████   | 187/266 [09:47<04:21,  3.31s/it]predicting train subjects:  71%|███████   | 188/266 [09:50<04:20,  3.34s/it]predicting train subjects:  71%|███████   | 189/266 [09:54<04:17,  3.34s/it]predicting train subjects:  71%|███████▏  | 190/266 [09:57<04:11,  3.31s/it]predicting train subjects:  72%|███████▏  | 191/266 [10:01<04:18,  3.44s/it]predicting train subjects:  72%|███████▏  | 192/266 [10:04<04:05,  3.32s/it]predicting train subjects:  73%|███████▎  | 193/266 [10:07<04:00,  3.29s/it]predicting train subjects:  73%|███████▎  | 194/266 [10:11<04:09,  3.46s/it]predicting train subjects:  73%|███████▎  | 195/266 [10:14<04:04,  3.44s/it]predicting train subjects:  74%|███████▎  | 196/266 [10:18<04:04,  3.49s/it]predicting train subjects:  74%|███████▍  | 197/266 [10:21<04:02,  3.51s/it]predicting train subjects:  74%|███████▍  | 198/266 [10:24<03:52,  3.41s/it]predicting train subjects:  75%|███████▍  | 199/266 [10:28<03:49,  3.42s/it]predicting train subjects:  75%|███████▌  | 200/266 [10:32<03:51,  3.50s/it]predicting train subjects:  76%|███████▌  | 201/266 [10:35<03:46,  3.49s/it]predicting train subjects:  76%|███████▌  | 202/266 [10:39<03:45,  3.52s/it]predicting train subjects:  76%|███████▋  | 203/266 [10:42<03:40,  3.50s/it]predicting train subjects:  77%|███████▋  | 204/266 [10:45<03:32,  3.42s/it]predicting train subjects:  77%|███████▋  | 205/266 [10:49<03:27,  3.41s/it]predicting train subjects:  77%|███████▋  | 206/266 [10:52<03:23,  3.40s/it]predicting train subjects:  78%|███████▊  | 207/266 [10:56<03:23,  3.45s/it]predicting train subjects:  78%|███████▊  | 208/266 [10:59<03:19,  3.44s/it]predicting train subjects:  79%|███████▊  | 209/266 [11:02<03:12,  3.38s/it]predicting train subjects:  79%|███████▉  | 210/266 [11:06<03:08,  3.36s/it]predicting train subjects:  79%|███████▉  | 211/266 [11:09<03:02,  3.32s/it]predicting train subjects:  80%|███████▉  | 212/266 [11:12<02:54,  3.24s/it]predicting train subjects:  80%|████████  | 213/266 [11:15<02:43,  3.08s/it]predicting train subjects:  80%|████████  | 214/266 [11:17<02:36,  3.00s/it]predicting train subjects:  81%|████████  | 215/266 [11:20<02:30,  2.95s/it]predicting train subjects:  81%|████████  | 216/266 [11:23<02:22,  2.85s/it]predicting train subjects:  82%|████████▏ | 217/266 [11:26<02:23,  2.92s/it]predicting train subjects:  82%|████████▏ | 218/266 [11:29<02:21,  2.95s/it]predicting train subjects:  82%|████████▏ | 219/266 [11:32<02:19,  2.97s/it]predicting train subjects:  83%|████████▎ | 220/266 [11:35<02:13,  2.90s/it]predicting train subjects:  83%|████████▎ | 221/266 [11:38<02:13,  2.96s/it]predicting train subjects:  83%|████████▎ | 222/266 [11:41<02:07,  2.90s/it]predicting train subjects:  84%|████████▍ | 223/266 [11:43<02:03,  2.88s/it]predicting train subjects:  84%|████████▍ | 224/266 [11:46<02:00,  2.86s/it]predicting train subjects:  85%|████████▍ | 225/266 [11:49<01:57,  2.86s/it]predicting train subjects:  85%|████████▍ | 226/266 [11:52<01:58,  2.96s/it]predicting train subjects:  85%|████████▌ | 227/266 [11:55<01:54,  2.95s/it]predicting train subjects:  86%|████████▌ | 228/266 [11:58<01:52,  2.95s/it]predicting train subjects:  86%|████████▌ | 229/266 [12:01<01:47,  2.91s/it]predicting train subjects:  86%|████████▋ | 230/266 [12:04<01:44,  2.90s/it]predicting train subjects:  87%|████████▋ | 231/266 [12:07<01:43,  2.97s/it]predicting train subjects:  87%|████████▋ | 232/266 [12:10<01:40,  2.94s/it]predicting train subjects:  88%|████████▊ | 233/266 [12:13<01:38,  2.99s/it]predicting train subjects:  88%|████████▊ | 234/266 [12:16<01:35,  2.98s/it]predicting train subjects:  88%|████████▊ | 235/266 [12:19<01:34,  3.06s/it]predicting train subjects:  89%|████████▊ | 236/266 [12:22<01:30,  3.01s/it]predicting train subjects:  89%|████████▉ | 237/266 [12:25<01:26,  2.99s/it]predicting train subjects:  89%|████████▉ | 238/266 [12:28<01:27,  3.12s/it]predicting train subjects:  90%|████████▉ | 239/266 [12:32<01:24,  3.13s/it]predicting train subjects:  90%|█████████ | 240/266 [12:35<01:21,  3.15s/it]predicting train subjects:  91%|█████████ | 241/266 [12:38<01:19,  3.16s/it]predicting train subjects:  91%|█████████ | 242/266 [12:41<01:14,  3.11s/it]predicting train subjects:  91%|█████████▏| 243/266 [12:44<01:10,  3.06s/it]predicting train subjects:  92%|█████████▏| 244/266 [12:47<01:07,  3.06s/it]predicting train subjects:  92%|█████████▏| 245/266 [12:50<01:03,  3.05s/it]predicting train subjects:  92%|█████████▏| 246/266 [12:53<01:00,  3.03s/it]predicting train subjects:  93%|█████████▎| 247/266 [12:56<00:58,  3.10s/it]predicting train subjects:  93%|█████████▎| 248/266 [12:59<00:56,  3.14s/it]predicting train subjects:  94%|█████████▎| 249/266 [13:03<00:57,  3.38s/it]predicting train subjects:  94%|█████████▍| 250/266 [13:08<00:58,  3.63s/it]predicting train subjects:  94%|█████████▍| 251/266 [13:12<00:56,  3.77s/it]predicting train subjects:  95%|█████████▍| 252/266 [13:15<00:53,  3.79s/it]predicting train subjects:  95%|█████████▌| 253/266 [13:19<00:49,  3.84s/it]predicting train subjects:  95%|█████████▌| 254/266 [13:24<00:47,  3.94s/it]predicting train subjects:  96%|█████████▌| 255/266 [13:28<00:43,  3.93s/it]predicting train subjects:  96%|█████████▌| 256/266 [13:31<00:39,  3.92s/it]predicting train subjects:  97%|█████████▋| 257/266 [13:36<00:35,  3.97s/it]predicting train subjects:  97%|█████████▋| 258/266 [13:40<00:32,  4.01s/it]predicting train subjects:  97%|█████████▋| 259/266 [13:44<00:28,  4.07s/it]predicting train subjects:  98%|█████████▊| 260/266 [13:48<00:24,  4.07s/it]predicting train subjects:  98%|█████████▊| 261/266 [13:52<00:20,  4.05s/it]predicting train subjects:  98%|█████████▊| 262/266 [13:56<00:16,  4.08s/it]predicting train subjects:  99%|█████████▉| 263/266 [14:00<00:12,  4.06s/it]predicting train subjects:  99%|█████████▉| 264/266 [14:04<00:08,  4.04s/it]predicting train subjects: 100%|█████████▉| 265/266 [14:08<00:04,  4.01s/it]predicting train subjects: 100%|██████████| 266/266 [14:12<00:00,  3.97s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<04:33,  1.03s/it]Loading train:   1%|          | 2/266 [00:01<04:11,  1.05it/s]Loading train:   1%|          | 3/266 [00:02<03:51,  1.14it/s]Loading train:   2%|▏         | 4/266 [00:03<03:44,  1.17it/s]Loading train:   2%|▏         | 5/266 [00:04<03:56,  1.10it/s]Loading train:   2%|▏         | 6/266 [00:05<03:55,  1.10it/s]Loading train:   3%|▎         | 7/266 [00:06<03:51,  1.12it/s]Loading train:   3%|▎         | 8/266 [00:06<03:45,  1.14it/s]Loading train:   3%|▎         | 9/266 [00:07<03:35,  1.19it/s]Loading train:   4%|▍         | 10/266 [00:08<03:41,  1.16it/s]Loading train:   4%|▍         | 11/266 [00:09<03:43,  1.14it/s]Loading train:   5%|▍         | 12/266 [00:10<03:43,  1.14it/s]Loading train:   5%|▍         | 13/266 [00:11<03:33,  1.18it/s]Loading train:   5%|▌         | 14/266 [00:11<03:26,  1.22it/s]Loading train:   6%|▌         | 15/266 [00:12<03:30,  1.19it/s]Loading train:   6%|▌         | 16/266 [00:13<03:35,  1.16it/s]Loading train:   6%|▋         | 17/266 [00:14<03:37,  1.14it/s]Loading train:   7%|▋         | 18/266 [00:15<03:33,  1.16it/s]Loading train:   7%|▋         | 19/266 [00:16<03:22,  1.22it/s]Loading train:   8%|▊         | 20/266 [00:17<03:22,  1.22it/s]Loading train:   8%|▊         | 21/266 [00:17<03:26,  1.19it/s]Loading train:   8%|▊         | 22/266 [00:18<03:18,  1.23it/s]Loading train:   9%|▊         | 23/266 [00:19<03:19,  1.22it/s]Loading train:   9%|▉         | 24/266 [00:20<03:23,  1.19it/s]Loading train:   9%|▉         | 25/266 [00:21<03:19,  1.21it/s]Loading train:  10%|▉         | 26/266 [00:22<03:19,  1.20it/s]Loading train:  10%|█         | 27/266 [00:22<03:14,  1.23it/s]Loading train:  11%|█         | 28/266 [00:23<03:19,  1.19it/s]Loading train:  11%|█         | 29/266 [00:24<03:19,  1.19it/s]Loading train:  11%|█▏        | 30/266 [00:25<03:12,  1.23it/s]Loading train:  12%|█▏        | 31/266 [00:26<03:15,  1.20it/s]Loading train:  12%|█▏        | 32/266 [00:27<03:18,  1.18it/s]Loading train:  12%|█▏        | 33/266 [00:27<03:11,  1.22it/s]Loading train:  13%|█▎        | 34/266 [00:28<03:18,  1.17it/s]Loading train:  13%|█▎        | 35/266 [00:29<03:14,  1.19it/s]Loading train:  14%|█▎        | 36/266 [00:30<03:14,  1.18it/s]Loading train:  14%|█▍        | 37/266 [00:31<03:16,  1.16it/s]Loading train:  14%|█▍        | 38/266 [00:32<03:11,  1.19it/s]Loading train:  15%|█▍        | 39/266 [00:32<03:11,  1.18it/s]Loading train:  15%|█▌        | 40/266 [00:33<03:17,  1.15it/s]Loading train:  15%|█▌        | 41/266 [00:34<03:06,  1.20it/s]Loading train:  16%|█▌        | 42/266 [00:35<02:57,  1.26it/s]Loading train:  16%|█▌        | 43/266 [00:35<02:43,  1.37it/s]Loading train:  17%|█▋        | 44/266 [00:36<02:43,  1.36it/s]Loading train:  17%|█▋        | 45/266 [00:37<02:45,  1.34it/s]Loading train:  17%|█▋        | 46/266 [00:38<02:46,  1.32it/s]Loading train:  18%|█▊        | 47/266 [00:39<02:52,  1.27it/s]Loading train:  18%|█▊        | 48/266 [00:39<02:49,  1.29it/s]Loading train:  18%|█▊        | 49/266 [00:40<02:54,  1.24it/s]Loading train:  19%|█▉        | 50/266 [00:41<02:47,  1.29it/s]Loading train:  19%|█▉        | 51/266 [00:42<02:45,  1.30it/s]Loading train:  20%|█▉        | 52/266 [00:43<02:50,  1.26it/s]Loading train:  20%|█▉        | 53/266 [00:43<02:52,  1.24it/s]Loading train:  20%|██        | 54/266 [00:44<02:48,  1.26it/s]Loading train:  21%|██        | 55/266 [00:45<02:52,  1.23it/s]Loading train:  21%|██        | 56/266 [00:46<02:55,  1.20it/s]Loading train:  21%|██▏       | 57/266 [00:47<02:50,  1.23it/s]Loading train:  22%|██▏       | 58/266 [00:47<02:50,  1.22it/s]Loading train:  22%|██▏       | 59/266 [00:48<02:45,  1.25it/s]Loading train:  23%|██▎       | 60/266 [00:49<02:51,  1.20it/s]Loading train:  23%|██▎       | 61/266 [00:50<02:47,  1.23it/s]Loading train:  23%|██▎       | 62/266 [00:51<02:41,  1.27it/s]Loading train:  24%|██▎       | 63/266 [00:51<02:35,  1.30it/s]Loading train:  24%|██▍       | 64/266 [00:52<02:37,  1.28it/s]Loading train:  24%|██▍       | 65/266 [00:53<02:37,  1.27it/s]Loading train:  25%|██▍       | 66/266 [00:54<02:32,  1.31it/s]Loading train:  25%|██▌       | 67/266 [00:54<02:26,  1.36it/s]Loading train:  26%|██▌       | 68/266 [00:55<02:27,  1.34it/s]Loading train:  26%|██▌       | 69/266 [00:56<02:31,  1.30it/s]Loading train:  26%|██▋       | 70/266 [00:57<02:33,  1.27it/s]Loading train:  27%|██▋       | 71/266 [00:58<02:34,  1.26it/s]Loading train:  27%|██▋       | 72/266 [00:58<02:20,  1.38it/s]Loading train:  27%|██▋       | 73/266 [00:59<02:22,  1.35it/s]Loading train:  28%|██▊       | 74/266 [01:00<02:23,  1.34it/s]Loading train:  28%|██▊       | 75/266 [01:00<02:27,  1.30it/s]Loading train:  29%|██▊       | 76/266 [01:01<02:28,  1.28it/s]Loading train:  29%|██▉       | 77/266 [01:02<02:25,  1.30it/s]Loading train:  29%|██▉       | 78/266 [01:03<02:37,  1.19it/s]Loading train:  30%|██▉       | 79/266 [01:04<02:35,  1.20it/s]Loading train:  30%|███       | 80/266 [01:05<02:40,  1.16it/s]Loading train:  30%|███       | 81/266 [01:06<02:42,  1.14it/s]Loading train:  31%|███       | 82/266 [01:06<02:36,  1.17it/s]Loading train:  31%|███       | 83/266 [01:07<02:40,  1.14it/s]Loading train:  32%|███▏      | 84/266 [01:08<02:41,  1.13it/s]Loading train:  32%|███▏      | 85/266 [01:09<02:35,  1.17it/s]Loading train:  32%|███▏      | 86/266 [01:10<02:35,  1.16it/s]Loading train:  33%|███▎      | 87/266 [01:11<02:28,  1.20it/s]Loading train:  33%|███▎      | 88/266 [01:12<02:25,  1.22it/s]Loading train:  33%|███▎      | 89/266 [01:12<02:27,  1.20it/s]Loading train:  34%|███▍      | 90/266 [01:13<02:29,  1.18it/s]Loading train:  34%|███▍      | 91/266 [01:14<02:25,  1.20it/s]Loading train:  35%|███▍      | 92/266 [01:15<02:26,  1.19it/s]Loading train:  35%|███▍      | 93/266 [01:16<02:25,  1.19it/s]Loading train:  35%|███▌      | 94/266 [01:17<02:22,  1.21it/s]Loading train:  36%|███▌      | 95/266 [01:18<02:26,  1.16it/s]Loading train:  36%|███▌      | 96/266 [01:18<02:23,  1.19it/s]Loading train:  36%|███▋      | 97/266 [01:19<02:28,  1.14it/s]Loading train:  37%|███▋      | 98/266 [01:20<02:22,  1.18it/s]Loading train:  37%|███▋      | 99/266 [01:21<02:24,  1.16it/s]Loading train:  38%|███▊      | 100/266 [01:22<02:21,  1.18it/s]Loading train:  38%|███▊      | 101/266 [01:23<02:17,  1.20it/s]Loading train:  38%|███▊      | 102/266 [01:23<02:17,  1.20it/s]Loading train:  39%|███▊      | 103/266 [01:24<02:15,  1.21it/s]Loading train:  39%|███▉      | 104/266 [01:25<02:10,  1.25it/s]Loading train:  39%|███▉      | 105/266 [01:26<02:09,  1.24it/s]Loading train:  40%|███▉      | 106/266 [01:27<02:08,  1.24it/s]Loading train:  40%|████      | 107/266 [01:27<02:09,  1.22it/s]Loading train:  41%|████      | 108/266 [01:28<02:05,  1.26it/s]Loading train:  41%|████      | 109/266 [01:29<02:05,  1.25it/s]Loading train:  41%|████▏     | 110/266 [01:30<02:04,  1.25it/s]Loading train:  42%|████▏     | 111/266 [01:30<01:55,  1.34it/s]Loading train:  42%|████▏     | 112/266 [01:31<01:58,  1.30it/s]Loading train:  42%|████▏     | 113/266 [01:32<01:59,  1.28it/s]Loading train:  43%|████▎     | 114/266 [01:33<02:00,  1.26it/s]Loading train:  43%|████▎     | 115/266 [01:34<01:56,  1.29it/s]Loading train:  44%|████▎     | 116/266 [01:34<01:57,  1.27it/s]Loading train:  44%|████▍     | 117/266 [01:35<01:57,  1.26it/s]Loading train:  44%|████▍     | 118/266 [01:36<01:59,  1.24it/s]Loading train:  45%|████▍     | 119/266 [01:37<02:06,  1.17it/s]Loading train:  45%|████▌     | 120/266 [01:38<02:08,  1.14it/s]Loading train:  45%|████▌     | 121/266 [01:39<02:20,  1.03it/s]Loading train:  46%|████▌     | 122/266 [01:40<02:23,  1.00it/s]Loading train:  46%|████▌     | 123/266 [01:41<02:12,  1.08it/s]Loading train:  47%|████▋     | 124/266 [01:42<02:03,  1.15it/s]Loading train:  47%|████▋     | 125/266 [01:42<01:59,  1.18it/s]Loading train:  47%|████▋     | 126/266 [01:43<02:01,  1.15it/s]Loading train:  48%|████▊     | 127/266 [01:44<02:01,  1.14it/s]Loading train:  48%|████▊     | 128/266 [01:45<02:00,  1.15it/s]Loading train:  48%|████▊     | 129/266 [01:46<01:59,  1.15it/s]Loading train:  49%|████▉     | 130/266 [01:47<01:55,  1.17it/s]Loading train:  49%|████▉     | 131/266 [01:48<01:57,  1.15it/s]Loading train:  50%|████▉     | 132/266 [01:49<02:02,  1.09it/s]Loading train:  50%|█████     | 133/266 [01:50<02:02,  1.09it/s]Loading train:  50%|█████     | 134/266 [01:51<01:57,  1.12it/s]Loading train:  51%|█████     | 135/266 [01:51<01:51,  1.17it/s]Loading train:  51%|█████     | 136/266 [01:52<01:55,  1.13it/s]Loading train:  52%|█████▏    | 137/266 [01:53<01:50,  1.17it/s]Loading train:  52%|█████▏    | 138/266 [01:54<01:51,  1.14it/s]Loading train:  52%|█████▏    | 139/266 [01:55<01:47,  1.18it/s]Loading train:  53%|█████▎    | 140/266 [01:56<01:47,  1.18it/s]Loading train:  53%|█████▎    | 141/266 [01:57<01:48,  1.15it/s]Loading train:  53%|█████▎    | 142/266 [01:57<01:47,  1.16it/s]Loading train:  54%|█████▍    | 143/266 [01:58<01:39,  1.23it/s]Loading train:  54%|█████▍    | 144/266 [01:59<01:45,  1.15it/s]Loading train:  55%|█████▍    | 145/266 [02:00<01:44,  1.16it/s]Loading train:  55%|█████▍    | 146/266 [02:01<01:39,  1.20it/s]Loading train:  55%|█████▌    | 147/266 [02:01<01:34,  1.26it/s]Loading train:  56%|█████▌    | 148/266 [02:02<01:31,  1.29it/s]Loading train:  56%|█████▌    | 149/266 [02:03<01:34,  1.23it/s]Loading train:  56%|█████▋    | 150/266 [02:04<01:40,  1.16it/s]Loading train:  57%|█████▋    | 151/266 [02:05<01:40,  1.14it/s]Loading train:  57%|█████▋    | 152/266 [02:06<01:39,  1.14it/s]Loading train:  58%|█████▊    | 153/266 [02:07<01:36,  1.17it/s]Loading train:  58%|█████▊    | 154/266 [02:07<01:31,  1.23it/s]Loading train:  58%|█████▊    | 155/266 [02:08<01:29,  1.25it/s]Loading train:  59%|█████▊    | 156/266 [02:09<01:29,  1.23it/s]Loading train:  59%|█████▉    | 157/266 [02:10<01:22,  1.32it/s]Loading train:  59%|█████▉    | 158/266 [02:10<01:22,  1.31it/s]Loading train:  60%|█████▉    | 159/266 [02:11<01:22,  1.29it/s]Loading train:  60%|██████    | 160/266 [02:12<01:21,  1.30it/s]Loading train:  61%|██████    | 161/266 [02:13<01:17,  1.35it/s]Loading train:  61%|██████    | 162/266 [02:13<01:17,  1.34it/s]Loading train:  61%|██████▏   | 163/266 [02:14<01:14,  1.39it/s]Loading train:  62%|██████▏   | 164/266 [02:15<01:13,  1.40it/s]Loading train:  62%|██████▏   | 165/266 [02:16<01:16,  1.32it/s]Loading train:  62%|██████▏   | 166/266 [02:16<01:17,  1.29it/s]Loading train:  63%|██████▎   | 167/266 [02:17<01:14,  1.32it/s]Loading train:  63%|██████▎   | 168/266 [02:18<01:13,  1.33it/s]Loading train:  64%|██████▎   | 169/266 [02:19<01:13,  1.31it/s]Loading train:  64%|██████▍   | 170/266 [02:19<01:15,  1.27it/s]Loading train:  64%|██████▍   | 171/266 [02:20<01:16,  1.24it/s]Loading train:  65%|██████▍   | 172/266 [02:21<01:11,  1.31it/s]Loading train:  65%|██████▌   | 173/266 [02:22<01:14,  1.24it/s]Loading train:  65%|██████▌   | 174/266 [02:23<01:13,  1.26it/s]Loading train:  66%|██████▌   | 175/266 [02:23<01:10,  1.29it/s]Loading train:  66%|██████▌   | 176/266 [02:24<01:10,  1.27it/s]Loading train:  67%|██████▋   | 177/266 [02:25<01:11,  1.24it/s]Loading train:  67%|██████▋   | 178/266 [02:26<01:11,  1.24it/s]Loading train:  67%|██████▋   | 179/266 [02:27<01:10,  1.23it/s]Loading train:  68%|██████▊   | 180/266 [02:27<01:09,  1.24it/s]Loading train:  68%|██████▊   | 181/266 [02:28<01:04,  1.32it/s]Loading train:  68%|██████▊   | 182/266 [02:29<01:07,  1.24it/s]Loading train:  69%|██████▉   | 183/266 [02:30<01:08,  1.22it/s]Loading train:  69%|██████▉   | 184/266 [02:31<01:05,  1.25it/s]Loading train:  70%|██████▉   | 185/266 [02:31<01:06,  1.22it/s]Loading train:  70%|██████▉   | 186/266 [02:32<01:08,  1.17it/s]Loading train:  70%|███████   | 187/266 [02:33<01:06,  1.20it/s]Loading train:  71%|███████   | 188/266 [02:34<01:06,  1.17it/s]Loading train:  71%|███████   | 189/266 [02:35<01:04,  1.20it/s]Loading train:  71%|███████▏  | 190/266 [02:36<01:03,  1.20it/s]Loading train:  72%|███████▏  | 191/266 [02:37<01:02,  1.20it/s]Loading train:  72%|███████▏  | 192/266 [02:37<01:01,  1.20it/s]Loading train:  73%|███████▎  | 193/266 [02:38<00:58,  1.24it/s]Loading train:  73%|███████▎  | 194/266 [02:39<01:00,  1.20it/s]Loading train:  73%|███████▎  | 195/266 [02:40<00:58,  1.21it/s]Loading train:  74%|███████▎  | 196/266 [02:40<00:54,  1.28it/s]Loading train:  74%|███████▍  | 197/266 [02:41<00:51,  1.33it/s]Loading train:  74%|███████▍  | 198/266 [02:42<00:51,  1.32it/s]Loading train:  75%|███████▍  | 199/266 [02:43<00:51,  1.31it/s]Loading train:  75%|███████▌  | 200/266 [02:44<00:51,  1.27it/s]Loading train:  76%|███████▌  | 201/266 [02:44<00:51,  1.26it/s]Loading train:  76%|███████▌  | 202/266 [02:45<00:49,  1.29it/s]Loading train:  76%|███████▋  | 203/266 [02:46<00:47,  1.34it/s]Loading train:  77%|███████▋  | 204/266 [02:47<00:47,  1.32it/s]Loading train:  77%|███████▋  | 205/266 [02:47<00:48,  1.26it/s]Loading train:  77%|███████▋  | 206/266 [02:48<00:47,  1.25it/s]Loading train:  78%|███████▊  | 207/266 [02:49<00:47,  1.25it/s]Loading train:  78%|███████▊  | 208/266 [02:50<00:46,  1.26it/s]Loading train:  79%|███████▊  | 209/266 [02:51<00:43,  1.30it/s]Loading train:  79%|███████▉  | 210/266 [02:51<00:42,  1.33it/s]Loading train:  79%|███████▉  | 211/266 [02:52<00:41,  1.32it/s]Loading train:  80%|███████▉  | 212/266 [02:53<00:41,  1.30it/s]Loading train:  80%|████████  | 213/266 [02:53<00:37,  1.42it/s]Loading train:  80%|████████  | 214/266 [02:54<00:35,  1.45it/s]Loading train:  81%|████████  | 215/266 [02:55<00:36,  1.39it/s]Loading train:  81%|████████  | 216/266 [02:56<00:37,  1.34it/s]Loading train:  82%|████████▏ | 217/266 [02:56<00:36,  1.35it/s]Loading train:  82%|████████▏ | 218/266 [02:57<00:35,  1.34it/s]Loading train:  82%|████████▏ | 219/266 [02:58<00:34,  1.36it/s]Loading train:  83%|████████▎ | 220/266 [02:59<00:33,  1.36it/s]Loading train:  83%|████████▎ | 221/266 [02:59<00:33,  1.36it/s]Loading train:  83%|████████▎ | 222/266 [03:00<00:32,  1.37it/s]Loading train:  84%|████████▍ | 223/266 [03:01<00:32,  1.32it/s]Loading train:  84%|████████▍ | 224/266 [03:02<00:31,  1.35it/s]Loading train:  85%|████████▍ | 225/266 [03:02<00:30,  1.33it/s]Loading train:  85%|████████▍ | 226/266 [03:03<00:28,  1.39it/s]Loading train:  85%|████████▌ | 227/266 [03:04<00:28,  1.39it/s]Loading train:  86%|████████▌ | 228/266 [03:04<00:28,  1.35it/s]Loading train:  86%|████████▌ | 229/266 [03:05<00:27,  1.34it/s]Loading train:  86%|████████▋ | 230/266 [03:06<00:27,  1.29it/s]Loading train:  87%|████████▋ | 231/266 [03:07<00:27,  1.29it/s]Loading train:  87%|████████▋ | 232/266 [03:07<00:24,  1.40it/s]Loading train:  88%|████████▊ | 233/266 [03:08<00:25,  1.30it/s]Loading train:  88%|████████▊ | 234/266 [03:09<00:25,  1.25it/s]Loading train:  88%|████████▊ | 235/266 [03:10<00:24,  1.27it/s]Loading train:  89%|████████▊ | 236/266 [03:11<00:23,  1.26it/s]Loading train:  89%|████████▉ | 237/266 [03:12<00:23,  1.26it/s]Loading train:  89%|████████▉ | 238/266 [03:12<00:23,  1.21it/s]Loading train:  90%|████████▉ | 239/266 [03:13<00:22,  1.20it/s]Loading train:  90%|█████████ | 240/266 [03:14<00:20,  1.26it/s]Loading train:  91%|█████████ | 241/266 [03:15<00:19,  1.25it/s]Loading train:  91%|█████████ | 242/266 [03:16<00:20,  1.20it/s]Loading train:  91%|█████████▏| 243/266 [03:17<00:18,  1.22it/s]Loading train:  92%|█████████▏| 244/266 [03:17<00:17,  1.23it/s]Loading train:  92%|█████████▏| 245/266 [03:18<00:17,  1.20it/s]Loading train:  92%|█████████▏| 246/266 [03:19<00:15,  1.30it/s]Loading train:  93%|█████████▎| 247/266 [03:20<00:14,  1.33it/s]Loading train:  93%|█████████▎| 248/266 [03:20<00:14,  1.26it/s]Loading train:  94%|█████████▎| 249/266 [03:21<00:14,  1.21it/s]Loading train:  94%|█████████▍| 250/266 [03:22<00:13,  1.20it/s]Loading train:  94%|█████████▍| 251/266 [03:23<00:12,  1.17it/s]Loading train:  95%|█████████▍| 252/266 [03:24<00:10,  1.29it/s]Loading train:  95%|█████████▌| 253/266 [03:24<00:10,  1.29it/s]Loading train:  95%|█████████▌| 254/266 [03:25<00:09,  1.23it/s]Loading train:  96%|█████████▌| 255/266 [03:26<00:09,  1.22it/s]Loading train:  96%|█████████▌| 256/266 [03:27<00:07,  1.26it/s]Loading train:  97%|█████████▋| 257/266 [03:28<00:07,  1.21it/s]Loading train:  97%|█████████▋| 258/266 [03:29<00:06,  1.24it/s]Loading train:  97%|█████████▋| 259/266 [03:29<00:05,  1.27it/s]Loading train:  98%|█████████▊| 260/266 [03:30<00:04,  1.23it/s]Loading train:  98%|█████████▊| 261/266 [03:31<00:04,  1.21it/s]Loading train:  98%|█████████▊| 262/266 [03:32<00:03,  1.25it/s]Loading train:  99%|█████████▉| 263/266 [03:33<00:02,  1.23it/s]Loading train:  99%|█████████▉| 264/266 [03:34<00:01,  1.18it/s]Loading train: 100%|█████████▉| 265/266 [03:35<00:00,  1.13it/s]Loading train: 100%|██████████| 266/266 [03:35<00:00,  1.12it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   1%|          | 2/266 [00:00<00:18, 14.12it/s]concatenating: train:   2%|▏         | 4/266 [00:00<00:18, 14.45it/s]concatenating: train:   3%|▎         | 7/266 [00:00<00:16, 15.78it/s]concatenating: train:   3%|▎         | 9/266 [00:00<00:15, 16.35it/s]concatenating: train:   5%|▍         | 13/266 [00:00<00:13, 19.29it/s]concatenating: train:   6%|▋         | 17/266 [00:00<00:11, 21.81it/s]concatenating: train:   8%|▊         | 20/266 [00:00<00:11, 21.12it/s]concatenating: train:  10%|█         | 27/266 [00:01<00:09, 25.67it/s]concatenating: train:  12%|█▏        | 31/266 [00:01<00:08, 27.35it/s]concatenating: train:  17%|█▋        | 46/266 [00:01<00:06, 36.16it/s]concatenating: train:  24%|██▎       | 63/266 [00:01<00:04, 46.97it/s]concatenating: train:  27%|██▋       | 73/266 [00:01<00:04, 46.11it/s]concatenating: train:  31%|███       | 82/266 [00:01<00:03, 53.15it/s]concatenating: train:  34%|███▍      | 91/266 [00:01<00:02, 58.34it/s]concatenating: train:  37%|███▋      | 99/266 [00:02<00:03, 54.90it/s]concatenating: train:  41%|████      | 108/266 [00:02<00:02, 60.69it/s]concatenating: train:  44%|████▎     | 116/266 [00:02<00:02, 59.21it/s]concatenating: train:  46%|████▌     | 123/266 [00:02<00:02, 51.48it/s]concatenating: train:  50%|█████     | 133/266 [00:02<00:02, 57.66it/s]concatenating: train:  53%|█████▎    | 140/266 [00:02<00:02, 46.76it/s]concatenating: train:  55%|█████▍    | 146/266 [00:03<00:03, 33.21it/s]concatenating: train:  58%|█████▊    | 153/266 [00:03<00:02, 38.12it/s]concatenating: train:  59%|█████▉    | 158/266 [00:03<00:02, 40.55it/s]concatenating: train:  62%|██████▏   | 166/266 [00:03<00:02, 46.59it/s]concatenating: train:  71%|███████   | 189/266 [00:03<00:01, 59.72it/s]concatenating: train:  75%|███████▍  | 199/266 [00:03<00:01, 52.07it/s]concatenating: train:  78%|███████▊  | 207/266 [00:04<00:01, 48.32it/s]concatenating: train:  81%|████████  | 215/266 [00:04<00:00, 54.19it/s]concatenating: train:  85%|████████▍ | 225/266 [00:04<00:00, 61.73it/s]concatenating: train:  89%|████████▊ | 236/266 [00:04<00:00, 71.08it/s]concatenating: train:  92%|█████████▏| 245/266 [00:04<00:00, 71.34it/s]concatenating: train:  95%|█████████▌| 254/266 [00:04<00:00, 75.43it/s]concatenating: train:  99%|█████████▉| 263/266 [00:04<00:00, 68.12it/s]concatenating: train: 100%|██████████| 266/266 [00:04<00:00, 56.22it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:00<00:03,  1.01it/s]Loading test:  40%|████      | 2/5 [00:01<00:02,  1.06it/s]Loading test:  60%|██████    | 3/5 [00:02<00:01,  1.12it/s]Loading test:  80%|████████  | 4/5 [00:03<00:00,  1.18it/s]Loading test: 100%|██████████| 5/5 [00:04<00:00,  1.21it/s]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation:  40%|████      | 2/5 [00:00<00:00, 14.26it/s]concatenating: validation:  80%|████████  | 4/5 [00:00<00:00, 13.36it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 12.24it/s]2019-08-17 19:15:40.538386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-17 19:15:40.538489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-17 19:15:40.538506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-17 19:15:40.538520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-17 19:15:40.539085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:12,  3.51it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:09,  4.32it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:08,  4.55it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:06,  5.74it/s]loading the weights for Res Unet:  20%|██        | 9/44 [00:01<00:07,  4.75it/s]loading the weights for Res Unet:  25%|██▌       | 11/44 [00:01<00:06,  5.38it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:06,  4.79it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:02<00:04,  6.12it/s]loading the weights for Res Unet:  41%|████      | 18/44 [00:02<00:04,  5.42it/s]loading the weights for Res Unet:  45%|████▌     | 20/44 [00:02<00:03,  6.17it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:02<00:04,  5.13it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:03<00:02,  6.41it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:03<00:02,  6.62it/s]loading the weights for Res Unet:  64%|██████▎   | 28/44 [00:03<00:02,  5.49it/s]loading the weights for Res Unet:  68%|██████▊   | 30/44 [00:03<00:02,  6.35it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:04<00:02,  5.24it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:04<00:01,  6.50it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:04<00:01,  6.50it/s]loading the weights for Res Unet:  86%|████████▋ | 38/44 [00:04<00:01,  4.97it/s]loading the weights for Res Unet:  91%|█████████ | 40/44 [00:05<00:00,  5.56it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:05<00:00,  4.61it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:05<00:00,  7.98it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 2  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 2  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 76, 108, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 76, 108, 30)  300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 76, 108, 30)  120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 76, 108, 30)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 76, 108, 30)  8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 76, 108, 30)  120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 76, 108, 30)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 38, 54, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 38, 54, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 38, 54, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 38, 54, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 38, 54, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 38, 54, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 38, 54, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 38, 54, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 38, 54, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 19, 27, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 19, 27, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 19, 27, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 19, 27, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 19, 27, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 19, 27, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 19, 27, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 19, 27, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 19, 27, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 19, 27, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 38, 54, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 38, 54, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 38, 54, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 38, 54, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 38, 54, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 38, 54, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 38, 54, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 38, 54, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 38, 54, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 38, 54, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 76, 108, 30)  25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 76, 108, 60)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 76, 108, 30)  16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 76, 108, 30)  120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 76, 108, 30)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 76, 108, 30)  8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 76, 108, 30)  120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 76, 108, 30)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 76, 108, 90)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 76, 108, 90)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 76, 108, 2)   182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 228,422
Non-trainable params: 271,920
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97453182 0.02546818]
Train on 27987 samples, validate on 500 samples
Epoch 1/300
 - 47s - loss: 0.2155 - acc: 0.9742 - mDice: 0.7224 - val_loss: 0.0768 - val_acc: 0.9933 - val_mDice: 0.8616

Epoch 00001: val_mDice improved from -inf to 0.86156, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 43s - loss: 0.0703 - acc: 0.9929 - mDice: 0.8733 - val_loss: 0.0662 - val_acc: 0.9944 - val_mDice: 0.8791

Epoch 00002: val_mDice improved from 0.86156 to 0.87911, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 43s - loss: 0.0577 - acc: 0.9940 - mDice: 0.8943 - val_loss: 0.0612 - val_acc: 0.9947 - val_mDice: 0.8878

Epoch 00003: val_mDice improved from 0.87911 to 0.88780, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 42s - loss: 0.0517 - acc: 0.9944 - mDice: 0.9045 - val_loss: 0.0621 - val_acc: 0.9946 - val_mDice: 0.8862

Epoch 00004: val_mDice did not improve from 0.88780
Epoch 5/300
 - 43s - loss: 0.0479 - acc: 0.9948 - mDice: 0.9112 - val_loss: 0.0623 - val_acc: 0.9949 - val_mDice: 0.8859

Epoch 00005: val_mDice did not improve from 0.88780
Epoch 6/300
 - 41s - loss: 0.0454 - acc: 0.9950 - mDice: 0.9156 - val_loss: 0.0596 - val_acc: 0.9949 - val_mDice: 0.8907

Epoch 00006: val_mDice improved from 0.88780 to 0.89073, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 41s - loss: 0.0431 - acc: 0.9952 - mDice: 0.9198 - val_loss: 0.0587 - val_acc: 0.9949 - val_mDice: 0.8922

Epoch 00007: val_mDice improved from 0.89073 to 0.89219, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 41s - loss: 0.0417 - acc: 0.9953 - mDice: 0.9222 - val_loss: 0.0584 - val_acc: 0.9949 - val_mDice: 0.8928

Epoch 00008: val_mDice improved from 0.89219 to 0.89275, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/1-THALAMUS/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 41s - loss: 0.0408 - acc: 0.9954 - mDice: 0.9238 - val_loss: 0.0592 - val_acc: 0.9948 - val_mDice: 0.8917

Epoch 00009: val_mDice did not improve from 0.89275
Epoch 10/300
 - 41s - loss: 0.0396 - acc: 0.9955 - mDice: 0.9261 - val_loss: 0.0603 - val_acc: 0.9948 - val_mDice: 0.8895

Epoch 00010: val_mDice did not improve from 0.89275
Epoch 11/300
 - 41s - loss: 0.0385 - acc: 0.9956 - mDice: 0.9280 - val_loss: 0.0597 - val_acc: 0.9949 - val_mDice: 0.8904

Epoch 00011: val_mDice did not improve from 0.89275
Epoch 12/300
 - 42s - loss: 0.0377 - acc: 0.9957 - mDice: 0.9294 - val_loss: 0.0615 - val_acc: 0.9949 - val_mDice: 0.8873

Epoch 00012: val_mDice did not improve from 0.89275
Epoch 13/300
 - 42s - loss: 0.0370 - acc: 0.9957 - mDice: 0.9307 - val_loss: 0.0616 - val_acc: 0.9948 - val_mDice: 0.8872

Epoch 00013: val_mDice did not improve from 0.89275
Epoch 14/300
 - 43s - loss: 0.0366 - acc: 0.9958 - mDice: 0.9315 - val_loss: 0.0617 - val_acc: 0.9947 - val_mDice: 0.8869

Epoch 00014: val_mDice did not improve from 0.89275
Epoch 15/300
 - 43s - loss: 0.0357 - acc: 0.9959 - mDice: 0.9331 - val_loss: 0.0626 - val_acc: 0.9948 - val_mDice: 0.8855

Epoch 00015: val_mDice did not improve from 0.89275
Epoch 16/300
 - 43s - loss: 0.0353 - acc: 0.9959 - mDice: 0.9338 - val_loss: 0.0621 - val_acc: 0.9948 - val_mDice: 0.8865

Epoch 00016: val_mDice did not improve from 0.89275
Epoch 17/300
 - 42s - loss: 0.0349 - acc: 0.9960 - mDice: 0.9346 - val_loss: 0.0592 - val_acc: 0.9948 - val_mDice: 0.8913

Epoch 00017: val_mDice did not improve from 0.89275
Epoch 18/300
 - 43s - loss: 0.0343 - acc: 0.9960 - mDice: 0.9356 - val_loss: 0.0599 - val_acc: 0.9947 - val_mDice: 0.8904

Epoch 00018: val_mDice did not improve from 0.89275
Epoch 19/300
 - 43s - loss: 0.0342 - acc: 0.9960 - mDice: 0.9358 - val_loss: 0.0631 - val_acc: 0.9946 - val_mDice: 0.8851

Epoch 00019: val_mDice did not improve from 0.89275
Epoch 20/300
 - 43s - loss: 0.0336 - acc: 0.9961 - mDice: 0.9368 - val_loss: 0.0611 - val_acc: 0.9947 - val_mDice: 0.8880

Epoch 00020: val_mDice did not improve from 0.89275
Epoch 21/300
 - 43s - loss: 0.0333 - acc: 0.9961 - mDice: 0.9374 - val_loss: 0.0611 - val_acc: 0.9949 - val_mDice: 0.8883

Epoch 00021: val_mDice did not improve from 0.89275
Epoch 22/300
 - 43s - loss: 0.0329 - acc: 0.9961 - mDice: 0.9381 - val_loss: 0.0614 - val_acc: 0.9949 - val_mDice: 0.8876

Epoch 00022: val_mDice did not improve from 0.89275
Epoch 23/300
 - 43s - loss: 0.0327 - acc: 0.9962 - mDice: 0.9384 - val_loss: 0.0608 - val_acc: 0.9947 - val_mDice: 0.8886

Epoch 00023: val_mDice did not improve from 0.89275
Epoch 24/300
 - 43s - loss: 0.0325 - acc: 0.9962 - mDice: 0.9388 - val_loss: 0.0605 - val_acc: 0.9945 - val_mDice: 0.8895

Epoch 00024: val_mDice did not improve from 0.89275
Epoch 25/300
 - 43s - loss: 0.0322 - acc: 0.9962 - mDice: 0.9395 - val_loss: 0.0607 - val_acc: 0.9949 - val_mDice: 0.8889

Epoch 00025: val_mDice did not improve from 0.89275
Epoch 26/300
 - 43s - loss: 0.0319 - acc: 0.9963 - mDice: 0.9399 - val_loss: 0.0625 - val_acc: 0.9948 - val_mDice: 0.8861

Epoch 00026: val_mDice did not improve from 0.89275
Epoch 27/300
 - 43s - loss: 0.0318 - acc: 0.9963 - mDice: 0.9402 - val_loss: 0.0619 - val_acc: 0.9947 - val_mDice: 0.8867

Epoch 00027: val_mDice did not improve from 0.89275
Epoch 28/300
 - 43s - loss: 0.0315 - acc: 0.9963 - mDice: 0.9408 - val_loss: 0.0631 - val_acc: 0.9946 - val_mDice: 0.8850

Epoch 00028: val_mDice did not improve from 0.89275
Epoch 29/300
 - 43s - loss: 0.0314 - acc: 0.9963 - mDice: 0.9409 - val_loss: 0.0604 - val_acc: 0.9948 - val_mDice: 0.8893

Epoch 00029: val_mDice did not improve from 0.89275
Epoch 30/300
 - 43s - loss: 0.0312 - acc: 0.9963 - mDice: 0.9412 - val_loss: 0.0609 - val_acc: 0.9948 - val_mDice: 0.8885

Epoch 00030: val_mDice did not improve from 0.89275
Epoch 31/300
 - 42s - loss: 0.0311 - acc: 0.9963 - mDice: 0.9415 - val_loss: 0.0612 - val_acc: 0.9948 - val_mDice: 0.8880

Epoch 00031: val_mDice did not improve from 0.89275
Epoch 32/300
 - 43s - loss: 0.0309 - acc: 0.9964 - mDice: 0.9417 - val_loss: 0.0612 - val_acc: 0.9948 - val_mDice: 0.8881

Epoch 00032: val_mDice did not improve from 0.89275
Epoch 33/300
 - 42s - loss: 0.0307 - acc: 0.9964 - mDice: 0.9421 - val_loss: 0.0610 - val_acc: 0.9948 - val_mDice: 0.8883

Epoch 00033: val_mDice did not improve from 0.89275
Epoch 34/300
 - 43s - loss: 0.0306 - acc: 0.9964 - mDice: 0.9424 - val_loss: 0.0628 - val_acc: 0.9947 - val_mDice: 0.8856

Epoch 00034: val_mDice did not improve from 0.89275
Epoch 35/300
 - 42s - loss: 0.0304 - acc: 0.9964 - mDice: 0.9427 - val_loss: 0.0650 - val_acc: 0.9946 - val_mDice: 0.8819

Epoch 00035: val_mDice did not improve from 0.89275
Epoch 36/300
 - 42s - loss: 0.0303 - acc: 0.9964 - mDice: 0.9429 - val_loss: 0.0631 - val_acc: 0.9947 - val_mDice: 0.8849

Epoch 00036: val_mDice did not improve from 0.89275
Epoch 37/300
 - 43s - loss: 0.0303 - acc: 0.9964 - mDice: 0.9429 - val_loss: 0.0605 - val_acc: 0.9949 - val_mDice: 0.8893

Epoch 00037: val_mDice did not improve from 0.89275
Epoch 38/300
 - 42s - loss: 0.0302 - acc: 0.9964 - mDice: 0.9431 - val_loss: 0.0647 - val_acc: 0.9947 - val_mDice: 0.8821

Epoch 00038: val_mDice did not improve from 0.89275
Epoch 39/300
 - 42s - loss: 0.0299 - acc: 0.9964 - mDice: 0.9437 - val_loss: 0.0628 - val_acc: 0.9947 - val_mDice: 0.8855

Epoch 00039: val_mDice did not improve from 0.89275
Epoch 40/300
 - 43s - loss: 0.0299 - acc: 0.9964 - mDice: 0.9436 - val_loss: 0.0612 - val_acc: 0.9948 - val_mDice: 0.8881

Epoch 00040: val_mDice did not improve from 0.89275
Epoch 41/300
 - 43s - loss: 0.0298 - acc: 0.9964 - mDice: 0.9438 - val_loss: 0.0633 - val_acc: 0.9947 - val_mDice: 0.8848

Epoch 00041: val_mDice did not improve from 0.89275
Epoch 42/300
 - 44s - loss: 0.0296 - acc: 0.9965 - mDice: 0.9441 - val_loss: 0.0593 - val_acc: 0.9949 - val_mDice: 0.8914

Epoch 00042: val_mDice did not improve from 0.89275
Epoch 43/300
 - 44s - loss: 0.0295 - acc: 0.9965 - mDice: 0.9444 - val_loss: 0.0607 - val_acc: 0.9948 - val_mDice: 0.8891

Epoch 00043: val_mDice did not improve from 0.89275
Epoch 44/300
 - 44s - loss: 0.0295 - acc: 0.9965 - mDice: 0.9444 - val_loss: 0.0620 - val_acc: 0.9946 - val_mDice: 0.8871

Epoch 00044: val_mDice did not improve from 0.89275
Epoch 45/300
 - 43s - loss: 0.0294 - acc: 0.9965 - mDice: 0.9445 - val_loss: 0.0615 - val_acc: 0.9948 - val_mDice: 0.8875

Epoch 00045: val_mDice did not improve from 0.89275
Epoch 46/300
 - 44s - loss: 0.0293 - acc: 0.9965 - mDice: 0.9446 - val_loss: 0.0631 - val_acc: 0.9946 - val_mDice: 0.8848

Epoch 00046: val_mDice did not improve from 0.89275
Epoch 47/300
 - 44s - loss: 0.0293 - acc: 0.9965 - mDice: 0.9447 - val_loss: 0.0616 - val_acc: 0.9949 - val_mDice: 0.8873

Epoch 00047: val_mDice did not improve from 0.89275
Epoch 48/300
 - 42s - loss: 0.0292 - acc: 0.9965 - mDice: 0.9449 - val_loss: 0.0613 - val_acc: 0.9948 - val_mDice: 0.8880

predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:01<00:04,  1.04s/it]predicting test subjects:  40%|████      | 2/5 [00:01<00:02,  1.17it/s]predicting test subjects:  60%|██████    | 3/5 [00:02<00:01,  1.29it/s]predicting test subjects:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]predicting test subjects: 100%|██████████| 5/5 [00:03<00:00,  1.56it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<02:51,  1.54it/s]predicting train subjects:   1%|          | 2/266 [00:01<02:43,  1.62it/s]predicting train subjects:   1%|          | 3/266 [00:01<02:35,  1.69it/s]predicting train subjects:   2%|▏         | 4/266 [00:02<02:30,  1.74it/s]predicting train subjects:   2%|▏         | 5/266 [00:02<02:20,  1.86it/s]predicting train subjects:   2%|▏         | 6/266 [00:03<02:29,  1.74it/s]predicting train subjects:   3%|▎         | 7/266 [00:03<02:23,  1.81it/s]predicting train subjects:   3%|▎         | 8/266 [00:04<02:23,  1.80it/s]predicting train subjects:   3%|▎         | 9/266 [00:04<02:12,  1.93it/s]predicting train subjects:   4%|▍         | 10/266 [00:05<02:22,  1.80it/s]predicting train subjects:   4%|▍         | 11/266 [00:06<02:20,  1.82it/s]predicting train subjects:   5%|▍         | 12/266 [00:06<02:18,  1.84it/s]predicting train subjects:   5%|▍         | 13/266 [00:07<02:16,  1.86it/s]predicting train subjects:   5%|▌         | 14/266 [00:07<02:14,  1.87it/s]predicting train subjects:   6%|▌         | 15/266 [00:08<02:16,  1.84it/s]predicting train subjects:   6%|▌         | 16/266 [00:08<02:22,  1.75it/s]predicting train subjects:   6%|▋         | 17/266 [00:09<02:18,  1.79it/s]predicting train subjects:   7%|▋         | 18/266 [00:09<02:04,  1.98it/s]predicting train subjects:   7%|▋         | 19/266 [00:10<02:11,  1.88it/s]predicting train subjects:   8%|▊         | 20/266 [00:10<02:09,  1.90it/s]predicting train subjects:   8%|▊         | 21/266 [00:11<02:14,  1.82it/s]predicting train subjects:   8%|▊         | 22/266 [00:11<02:08,  1.90it/s]predicting train subjects:   9%|▊         | 23/266 [00:12<02:16,  1.78it/s]predicting train subjects:   9%|▉         | 24/266 [00:13<02:18,  1.75it/s]predicting train subjects:   9%|▉         | 25/266 [00:13<02:10,  1.84it/s]predicting train subjects:  10%|▉         | 26/266 [00:14<02:05,  1.92it/s]predicting train subjects:  10%|█         | 27/266 [00:14<02:04,  1.92it/s]predicting train subjects:  11%|█         | 28/266 [00:15<02:06,  1.88it/s]predicting train subjects:  11%|█         | 29/266 [00:15<02:06,  1.88it/s]predicting train subjects:  11%|█▏        | 30/266 [00:16<02:05,  1.89it/s]predicting train subjects:  12%|█▏        | 31/266 [00:16<01:58,  1.99it/s]predicting train subjects:  12%|█▏        | 32/266 [00:17<01:53,  2.07it/s]predicting train subjects:  12%|█▏        | 33/266 [00:17<01:52,  2.08it/s]predicting train subjects:  13%|█▎        | 34/266 [00:18<01:59,  1.93it/s]predicting train subjects:  13%|█▎        | 35/266 [00:18<01:56,  1.98it/s]predicting train subjects:  14%|█▎        | 36/266 [00:19<01:52,  2.05it/s]predicting train subjects:  14%|█▍        | 37/266 [00:19<01:51,  2.05it/s]predicting train subjects:  14%|█▍        | 38/266 [00:19<01:43,  2.20it/s]predicting train subjects:  15%|█▍        | 39/266 [00:20<01:44,  2.18it/s]predicting train subjects:  15%|█▌        | 40/266 [00:20<01:43,  2.18it/s]predicting train subjects:  15%|█▌        | 41/266 [00:21<01:38,  2.28it/s]predicting train subjects:  16%|█▌        | 42/266 [00:21<01:40,  2.22it/s]predicting train subjects:  16%|█▌        | 43/266 [00:22<01:34,  2.35it/s]predicting train subjects:  17%|█▋        | 44/266 [00:22<01:30,  2.45it/s]predicting train subjects:  17%|█▋        | 45/266 [00:23<01:39,  2.22it/s]predicting train subjects:  17%|█▋        | 46/266 [00:23<01:36,  2.28it/s]predicting train subjects:  18%|█▊        | 47/266 [00:23<01:40,  2.18it/s]predicting train subjects:  18%|█▊        | 48/266 [00:24<01:34,  2.30it/s]predicting train subjects:  18%|█▊        | 49/266 [00:24<01:28,  2.44it/s]predicting train subjects:  19%|█▉        | 50/266 [00:25<01:38,  2.18it/s]predicting train subjects:  19%|█▉        | 51/266 [00:25<01:36,  2.23it/s]predicting train subjects:  20%|█▉        | 52/266 [00:26<01:29,  2.38it/s]predicting train subjects:  20%|█▉        | 53/266 [00:26<01:30,  2.36it/s]predicting train subjects:  20%|██        | 54/266 [00:26<01:31,  2.32it/s]predicting train subjects:  21%|██        | 55/266 [00:27<01:27,  2.40it/s]predicting train subjects:  21%|██        | 56/266 [00:27<01:32,  2.27it/s]predicting train subjects:  21%|██▏       | 57/266 [00:28<01:34,  2.21it/s]predicting train subjects:  22%|██▏       | 58/266 [00:28<01:42,  2.03it/s]predicting train subjects:  22%|██▏       | 59/266 [00:29<01:40,  2.06it/s]predicting train subjects:  23%|██▎       | 60/266 [00:29<01:30,  2.27it/s]predicting train subjects:  23%|██▎       | 61/266 [00:30<01:28,  2.32it/s]predicting train subjects:  23%|██▎       | 62/266 [00:30<01:23,  2.44it/s]predicting train subjects:  24%|██▎       | 63/266 [00:30<01:22,  2.46it/s]predicting train subjects:  24%|██▍       | 64/266 [00:31<01:27,  2.32it/s]predicting train subjects:  24%|██▍       | 65/266 [00:31<01:25,  2.34it/s]predicting train subjects:  25%|██▍       | 66/266 [00:32<01:19,  2.52it/s]predicting train subjects:  25%|██▌       | 67/266 [00:32<01:24,  2.36it/s]predicting train subjects:  26%|██▌       | 68/266 [00:32<01:17,  2.54it/s]predicting train subjects:  26%|██▌       | 69/266 [00:33<01:17,  2.55it/s]predicting train subjects:  26%|██▋       | 70/266 [00:33<01:20,  2.44it/s]predicting train subjects:  27%|██▋       | 71/266 [00:34<01:20,  2.43it/s]predicting train subjects:  27%|██▋       | 72/266 [00:34<01:17,  2.51it/s]predicting train subjects:  27%|██▋       | 73/266 [00:34<01:16,  2.51it/s]predicting train subjects:  28%|██▊       | 74/266 [00:35<01:14,  2.59it/s]predicting train subjects:  28%|██▊       | 75/266 [00:35<01:09,  2.73it/s]predicting train subjects:  29%|██▊       | 76/266 [00:36<01:12,  2.63it/s]predicting train subjects:  29%|██▉       | 77/266 [00:36<01:14,  2.55it/s]predicting train subjects:  29%|██▉       | 78/266 [00:36<01:21,  2.32it/s]predicting train subjects:  30%|██▉       | 79/266 [00:37<01:24,  2.22it/s]predicting train subjects:  30%|███       | 80/266 [00:37<01:27,  2.11it/s]predicting train subjects:  30%|███       | 81/266 [00:38<01:36,  1.91it/s]predicting train subjects:  31%|███       | 82/266 [00:39<01:39,  1.85it/s]predicting train subjects:  31%|███       | 83/266 [00:39<01:35,  1.91it/s]predicting train subjects:  32%|███▏      | 84/266 [00:40<01:40,  1.81it/s]predicting train subjects:  32%|███▏      | 85/266 [00:40<01:35,  1.90it/s]predicting train subjects:  32%|███▏      | 86/266 [00:41<01:35,  1.88it/s]predicting train subjects:  33%|███▎      | 87/266 [00:41<01:35,  1.86it/s]predicting train subjects:  33%|███▎      | 88/266 [00:42<01:32,  1.92it/s]predicting train subjects:  33%|███▎      | 89/266 [00:42<01:26,  2.05it/s]predicting train subjects:  34%|███▍      | 90/266 [00:43<01:27,  2.01it/s]predicting train subjects:  34%|███▍      | 91/266 [00:43<01:22,  2.13it/s]predicting train subjects:  35%|███▍      | 92/266 [00:44<01:23,  2.09it/s]predicting train subjects:  35%|███▍      | 93/266 [00:44<01:23,  2.06it/s]predicting train subjects:  35%|███▌      | 94/266 [00:45<01:18,  2.18it/s]predicting train subjects:  36%|███▌      | 95/266 [00:45<01:17,  2.21it/s]predicting train subjects:  36%|███▌      | 96/266 [00:46<01:21,  2.09it/s]predicting train subjects:  36%|███▋      | 97/266 [00:46<01:28,  1.91it/s]predicting train subjects:  37%|███▋      | 98/266 [00:47<01:33,  1.79it/s]predicting train subjects:  37%|███▋      | 99/266 [00:47<01:27,  1.91it/s]predicting train subjects:  38%|███▊      | 100/266 [00:48<01:18,  2.12it/s]predicting train subjects:  38%|███▊      | 101/266 [00:48<01:24,  1.95it/s]predicting train subjects:  38%|███▊      | 102/266 [00:49<01:22,  1.99it/s]predicting train subjects:  39%|███▊      | 103/266 [00:49<01:25,  1.90it/s]predicting train subjects:  39%|███▉      | 104/266 [00:50<01:19,  2.04it/s]predicting train subjects:  39%|███▉      | 105/266 [00:50<01:17,  2.07it/s]predicting train subjects:  40%|███▉      | 106/266 [00:51<01:14,  2.16it/s]predicting train subjects:  40%|████      | 107/266 [00:51<01:13,  2.15it/s]predicting train subjects:  41%|████      | 108/266 [00:51<01:12,  2.19it/s]predicting train subjects:  41%|████      | 109/266 [00:52<01:16,  2.06it/s]predicting train subjects:  41%|████▏     | 110/266 [00:52<01:11,  2.19it/s]predicting train subjects:  42%|████▏     | 111/266 [00:53<01:12,  2.14it/s]predicting train subjects:  42%|████▏     | 112/266 [00:53<01:15,  2.05it/s]predicting train subjects:  42%|████▏     | 113/266 [00:54<01:18,  1.96it/s]predicting train subjects:  43%|████▎     | 114/266 [00:54<01:16,  1.99it/s]predicting train subjects:  43%|████▎     | 115/266 [00:55<01:19,  1.90it/s]predicting train subjects:  44%|████▎     | 116/266 [00:56<01:16,  1.97it/s]predicting train subjects:  44%|████▍     | 117/266 [00:56<01:10,  2.12it/s]predicting train subjects:  44%|████▍     | 118/266 [00:56<01:10,  2.10it/s]predicting train subjects:  45%|████▍     | 119/266 [00:57<01:09,  2.11it/s]predicting train subjects:  45%|████▌     | 120/266 [00:58<01:16,  1.91it/s]predicting train subjects:  45%|████▌     | 121/266 [00:58<01:13,  1.97it/s]predicting train subjects:  46%|████▌     | 122/266 [00:58<01:09,  2.08it/s]predicting train subjects:  46%|████▌     | 123/266 [00:59<01:12,  1.98it/s]predicting train subjects:  47%|████▋     | 124/266 [00:59<01:09,  2.05it/s]predicting train subjects:  47%|████▋     | 125/266 [01:00<01:11,  1.98it/s]predicting train subjects:  47%|████▋     | 126/266 [01:01<01:14,  1.87it/s]predicting train subjects:  48%|████▊     | 127/266 [01:01<01:14,  1.86it/s]predicting train subjects:  48%|████▊     | 128/266 [01:02<01:10,  1.95it/s]predicting train subjects:  48%|████▊     | 129/266 [01:02<01:12,  1.88it/s]predicting train subjects:  49%|████▉     | 130/266 [01:03<01:14,  1.82it/s]predicting train subjects:  49%|████▉     | 131/266 [01:03<01:16,  1.77it/s]predicting train subjects:  50%|████▉     | 132/266 [01:04<01:15,  1.76it/s]predicting train subjects:  50%|█████     | 133/266 [01:04<01:14,  1.79it/s]predicting train subjects:  50%|█████     | 134/266 [01:05<01:13,  1.81it/s]predicting train subjects:  51%|█████     | 135/266 [01:05<01:10,  1.86it/s]predicting train subjects:  51%|█████     | 136/266 [01:06<01:12,  1.79it/s]predicting train subjects:  52%|█████▏    | 137/266 [01:07<01:10,  1.84it/s]predicting train subjects:  52%|█████▏    | 138/266 [01:07<01:10,  1.82it/s]predicting train subjects:  52%|█████▏    | 139/266 [01:08<01:08,  1.85it/s]predicting train subjects:  53%|█████▎    | 140/266 [01:08<01:06,  1.90it/s]predicting train subjects:  53%|█████▎    | 141/266 [01:09<01:05,  1.90it/s]predicting train subjects:  53%|█████▎    | 142/266 [01:09<00:59,  2.07it/s]predicting train subjects:  54%|█████▍    | 143/266 [01:10<01:02,  1.96it/s]predicting train subjects:  54%|█████▍    | 144/266 [01:10<01:01,  2.00it/s]predicting train subjects:  55%|█████▍    | 145/266 [01:11<00:57,  2.11it/s]predicting train subjects:  55%|█████▍    | 146/266 [01:11<01:01,  1.95it/s]predicting train subjects:  55%|█████▌    | 147/266 [01:12<00:59,  2.01it/s]predicting train subjects:  56%|█████▌    | 148/266 [01:12<00:57,  2.04it/s]predicting train subjects:  56%|█████▌    | 149/266 [01:12<00:54,  2.15it/s]predicting train subjects:  56%|█████▋    | 150/266 [01:13<00:54,  2.11it/s]predicting train subjects:  57%|█████▋    | 151/266 [01:14<00:56,  2.02it/s]predicting train subjects:  57%|█████▋    | 152/266 [01:14<00:53,  2.12it/s]predicting train subjects:  58%|█████▊    | 153/266 [01:15<00:57,  1.96it/s]predicting train subjects:  58%|█████▊    | 154/266 [01:15<00:55,  2.01it/s]predicting train subjects:  58%|█████▊    | 155/266 [01:15<00:51,  2.14it/s]predicting train subjects:  59%|█████▊    | 156/266 [01:16<00:52,  2.11it/s]predicting train subjects:  59%|█████▉    | 157/266 [01:16<00:48,  2.27it/s]predicting train subjects:  59%|█████▉    | 158/266 [01:17<00:44,  2.40it/s]predicting train subjects:  60%|█████▉    | 159/266 [01:17<00:47,  2.25it/s]predicting train subjects:  60%|██████    | 160/266 [01:18<00:48,  2.18it/s]predicting train subjects:  61%|██████    | 161/266 [01:18<00:42,  2.45it/s]predicting train subjects:  61%|██████    | 162/266 [01:18<00:38,  2.73it/s]predicting train subjects:  61%|██████▏   | 163/266 [01:19<00:37,  2.73it/s]predicting train subjects:  62%|██████▏   | 164/266 [01:19<00:38,  2.64it/s]predicting train subjects:  62%|██████▏   | 165/266 [01:19<00:39,  2.58it/s]predicting train subjects:  62%|██████▏   | 166/266 [01:20<00:37,  2.67it/s]predicting train subjects:  63%|██████▎   | 167/266 [01:20<00:41,  2.41it/s]predicting train subjects:  63%|██████▎   | 168/266 [01:21<00:41,  2.35it/s]predicting train subjects:  64%|██████▎   | 169/266 [01:21<00:39,  2.46it/s]predicting train subjects:  64%|██████▍   | 170/266 [01:21<00:36,  2.66it/s]predicting train subjects:  64%|██████▍   | 171/266 [01:22<00:38,  2.48it/s]predicting train subjects:  65%|██████▍   | 172/266 [01:22<00:37,  2.51it/s]predicting train subjects:  65%|██████▌   | 173/266 [01:23<00:38,  2.42it/s]predicting train subjects:  65%|██████▌   | 174/266 [01:23<00:39,  2.34it/s]predicting train subjects:  66%|██████▌   | 175/266 [01:24<00:40,  2.25it/s]predicting train subjects:  66%|██████▌   | 176/266 [01:24<00:36,  2.44it/s]predicting train subjects:  67%|██████▋   | 177/266 [01:24<00:36,  2.42it/s]predicting train subjects:  67%|██████▋   | 178/266 [01:25<00:37,  2.32it/s]predicting train subjects:  67%|██████▋   | 179/266 [01:25<00:35,  2.48it/s]predicting train subjects:  68%|██████▊   | 180/266 [01:26<00:35,  2.41it/s]predicting train subjects:  68%|██████▊   | 181/266 [01:26<00:37,  2.26it/s]predicting train subjects:  68%|██████▊   | 182/266 [01:26<00:35,  2.35it/s]predicting train subjects:  69%|██████▉   | 183/266 [01:27<00:36,  2.25it/s]predicting train subjects:  69%|██████▉   | 184/266 [01:27<00:34,  2.36it/s]predicting train subjects:  70%|██████▉   | 185/266 [01:28<00:35,  2.30it/s]predicting train subjects:  70%|██████▉   | 186/266 [01:28<00:35,  2.28it/s]predicting train subjects:  70%|███████   | 187/266 [01:29<00:37,  2.11it/s]predicting train subjects:  71%|███████   | 188/266 [01:29<00:35,  2.20it/s]predicting train subjects:  71%|███████   | 189/266 [01:30<00:32,  2.34it/s]predicting train subjects:  71%|███████▏  | 190/266 [01:30<00:33,  2.27it/s]predicting train subjects:  72%|███████▏  | 191/266 [01:31<00:34,  2.17it/s]predicting train subjects:  72%|███████▏  | 192/266 [01:31<00:32,  2.29it/s]predicting train subjects:  73%|███████▎  | 193/266 [01:31<00:31,  2.29it/s]predicting train subjects:  73%|███████▎  | 194/266 [01:32<00:32,  2.21it/s]predicting train subjects:  73%|███████▎  | 195/266 [01:32<00:34,  2.04it/s]predicting train subjects:  74%|███████▎  | 196/266 [01:33<00:34,  2.03it/s]predicting train subjects:  74%|███████▍  | 197/266 [01:33<00:31,  2.19it/s]predicting train subjects:  74%|███████▍  | 198/266 [01:34<00:30,  2.24it/s]predicting train subjects:  75%|███████▍  | 199/266 [01:34<00:30,  2.21it/s]predicting train subjects:  75%|███████▌  | 200/266 [01:35<00:29,  2.21it/s]predicting train subjects:  76%|███████▌  | 201/266 [01:35<00:31,  2.08it/s]predicting train subjects:  76%|███████▌  | 202/266 [01:36<00:27,  2.29it/s]predicting train subjects:  76%|███████▋  | 203/266 [01:36<00:27,  2.30it/s]predicting train subjects:  77%|███████▋  | 204/266 [01:36<00:27,  2.27it/s]predicting train subjects:  77%|███████▋  | 205/266 [01:37<00:24,  2.44it/s]predicting train subjects:  77%|███████▋  | 206/266 [01:37<00:26,  2.27it/s]predicting train subjects:  78%|███████▊  | 207/266 [01:38<00:25,  2.35it/s]predicting train subjects:  78%|███████▊  | 208/266 [01:38<00:26,  2.15it/s]predicting train subjects:  79%|███████▊  | 209/266 [01:39<00:26,  2.17it/s]predicting train subjects:  79%|███████▉  | 210/266 [01:39<00:23,  2.39it/s]predicting train subjects:  79%|███████▉  | 211/266 [01:39<00:22,  2.41it/s]predicting train subjects:  80%|███████▉  | 212/266 [01:40<00:24,  2.22it/s]predicting train subjects:  80%|████████  | 213/266 [01:40<00:22,  2.36it/s]predicting train subjects:  80%|████████  | 214/266 [01:41<00:21,  2.39it/s]predicting train subjects:  81%|████████  | 215/266 [01:41<00:21,  2.40it/s]predicting train subjects:  81%|████████  | 216/266 [01:41<00:19,  2.58it/s]predicting train subjects:  82%|████████▏ | 217/266 [01:42<00:19,  2.53it/s]predicting train subjects:  82%|████████▏ | 218/266 [01:42<00:19,  2.50it/s]predicting train subjects:  82%|████████▏ | 219/266 [01:43<00:17,  2.70it/s]predicting train subjects:  83%|████████▎ | 220/266 [01:43<00:16,  2.75it/s]predicting train subjects:  83%|████████▎ | 221/266 [01:43<00:16,  2.78it/s]predicting train subjects:  83%|████████▎ | 222/266 [01:44<00:15,  2.86it/s]predicting train subjects:  84%|████████▍ | 223/266 [01:44<00:14,  2.99it/s]predicting train subjects:  84%|████████▍ | 224/266 [01:44<00:16,  2.58it/s]predicting train subjects:  85%|████████▍ | 225/266 [01:45<00:15,  2.64it/s]predicting train subjects:  85%|████████▍ | 226/266 [01:45<00:13,  2.91it/s]predicting train subjects:  85%|████████▌ | 227/266 [01:45<00:13,  2.87it/s]predicting train subjects:  86%|████████▌ | 228/266 [01:46<00:13,  2.78it/s]predicting train subjects:  86%|████████▌ | 229/266 [01:46<00:13,  2.80it/s]predicting train subjects:  86%|████████▋ | 230/266 [01:46<00:12,  2.88it/s]predicting train subjects:  87%|████████▋ | 231/266 [01:47<00:13,  2.60it/s]predicting train subjects:  87%|████████▋ | 232/266 [01:47<00:13,  2.46it/s]predicting train subjects:  88%|████████▊ | 233/266 [01:48<00:12,  2.62it/s]predicting train subjects:  88%|████████▊ | 234/266 [01:48<00:12,  2.65it/s]predicting train subjects:  88%|████████▊ | 235/266 [01:48<00:11,  2.67it/s]predicting train subjects:  89%|████████▊ | 236/266 [01:49<00:10,  2.82it/s]predicting train subjects:  89%|████████▉ | 237/266 [01:49<00:10,  2.79it/s]predicting train subjects:  89%|████████▉ | 238/266 [01:50<00:10,  2.58it/s]predicting train subjects:  90%|████████▉ | 239/266 [01:50<00:10,  2.50it/s]predicting train subjects:  90%|█████████ | 240/266 [01:50<00:10,  2.41it/s]predicting train subjects:  91%|█████████ | 241/266 [01:51<00:10,  2.29it/s]predicting train subjects:  91%|█████████ | 242/266 [01:51<00:10,  2.31it/s]predicting train subjects:  91%|█████████▏| 243/266 [01:52<00:10,  2.25it/s]predicting train subjects:  92%|█████████▏| 244/266 [01:52<00:09,  2.23it/s]predicting train subjects:  92%|█████████▏| 245/266 [01:53<00:09,  2.31it/s]predicting train subjects:  92%|█████████▏| 246/266 [01:53<00:08,  2.32it/s]predicting train subjects:  93%|█████████▎| 247/266 [01:54<00:08,  2.24it/s]predicting train subjects:  93%|█████████▎| 248/266 [01:54<00:08,  2.20it/s]predicting train subjects:  94%|█████████▎| 249/266 [01:54<00:07,  2.29it/s]predicting train subjects:  94%|█████████▍| 250/266 [01:55<00:07,  2.19it/s]predicting train subjects:  94%|█████████▍| 251/266 [01:55<00:06,  2.22it/s]predicting train subjects:  95%|█████████▍| 252/266 [01:56<00:06,  2.21it/s]predicting train subjects:  95%|█████████▌| 253/266 [01:56<00:06,  1.97it/s]predicting train subjects:  95%|█████████▌| 254/266 [01:57<00:05,  2.06it/s]predicting train subjects:  96%|█████████▌| 255/266 [01:57<00:05,  1.97it/s]predicting train subjects:  96%|█████████▌| 256/266 [01:58<00:04,  2.03it/s]predicting train subjects:  97%|█████████▋| 257/266 [01:58<00:04,  2.08it/s]predicting train subjects:  97%|█████████▋| 258/266 [01:59<00:03,  2.07it/s]predicting train subjects:  97%|█████████▋| 259/266 [01:59<00:03,  2.10it/s]predicting train subjects:  98%|█████████▊| 260/266 [02:00<00:02,  2.03it/s]predicting train subjects:  98%|█████████▊| 261/266 [02:00<00:02,  2.01it/s]predicting train subjects:  98%|█████████▊| 262/266 [02:01<00:02,  2.00it/s]predicting train subjects:  99%|█████████▉| 263/266 [02:01<00:01,  1.90it/s]predicting train subjects:  99%|█████████▉| 264/266 [02:02<00:01,  1.97it/s]predicting train subjects: 100%|█████████▉| 265/266 [02:03<00:00,  1.87it/s]predicting train subjects: 100%|██████████| 266/266 [02:03<00:00,  2.00it/s]
saving BB  test1-THALAMUS:   0%|          | 0/5 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 5/5 [00:00<00:00, 70.99it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 7/266 [00:00<00:04, 61.91it/s]saving BB  train1-THALAMUS:   5%|▌         | 14/266 [00:00<00:04, 62.50it/s]saving BB  train1-THALAMUS:   8%|▊         | 21/266 [00:00<00:03, 62.86it/s]saving BB  train1-THALAMUS:  11%|█         | 28/266 [00:00<00:03, 63.33it/s]saving BB  train1-THALAMUS:  14%|█▎        | 36/266 [00:00<00:03, 65.01it/s]saving BB  train1-THALAMUS:  16%|█▌        | 43/266 [00:00<00:03, 66.43it/s]saving BB  train1-THALAMUS:  19%|█▉        | 50/266 [00:00<00:03, 67.22it/s]saving BB  train1-THALAMUS:  22%|██▏       | 58/266 [00:00<00:02, 69.76it/s]saving BB  train1-THALAMUS:  25%|██▌       | 67/266 [00:00<00:02, 72.65it/s]saving BB  train1-THALAMUS:  29%|██▊       | 76/266 [00:01<00:02, 75.63it/s]saving BB  train1-THALAMUS:  32%|███▏      | 84/266 [00:01<00:02, 73.84it/s]saving BB  train1-THALAMUS:  35%|███▍      | 92/266 [00:01<00:02, 72.94it/s]saving BB  train1-THALAMUS:  38%|███▊      | 100/266 [00:01<00:02, 62.59it/s]saving BB  train1-THALAMUS:  40%|████      | 107/266 [00:01<00:02, 59.23it/s]saving BB  train1-THALAMUS:  43%|████▎     | 115/266 [00:01<00:02, 63.53it/s]saving BB  train1-THALAMUS:  46%|████▌     | 123/266 [00:01<00:02, 65.43it/s]saving BB  train1-THALAMUS:  49%|████▉     | 131/266 [00:01<00:02, 67.13it/s]saving BB  train1-THALAMUS:  52%|█████▏    | 138/266 [00:02<00:01, 67.16it/s]saving BB  train1-THALAMUS:  55%|█████▍    | 145/266 [00:02<00:01, 67.59it/s]saving BB  train1-THALAMUS:  57%|█████▋    | 152/266 [00:02<00:01, 67.83it/s]saving BB  train1-THALAMUS:  60%|██████    | 160/266 [00:02<00:01, 69.64it/s]saving BB  train1-THALAMUS:  64%|██████▎   | 169/266 [00:02<00:01, 72.63it/s]saving BB  train1-THALAMUS:  67%|██████▋   | 178/266 [00:02<00:01, 75.00it/s]saving BB  train1-THALAMUS:  70%|███████   | 187/266 [00:02<00:01, 77.28it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 195/266 [00:02<00:00, 76.24it/s]saving BB  train1-THALAMUS:  76%|███████▋  | 203/266 [00:02<00:00, 74.55it/s]saving BB  train1-THALAMUS:  79%|███████▉  | 211/266 [00:03<00:00, 73.53it/s]saving BB  train1-THALAMUS:  82%|████████▏ | 219/266 [00:03<00:00, 73.60it/s]saving BB  train1-THALAMUS:  85%|████████▌ | 227/266 [00:03<00:00, 74.73it/s]saving BB  train1-THALAMUS:  88%|████████▊ | 235/266 [00:03<00:00, 75.08it/s]saving BB  train1-THALAMUS:  92%|█████████▏| 244/266 [00:03<00:00, 76.77it/s]saving BB  train1-THALAMUS:  95%|█████████▍| 252/266 [00:03<00:00, 76.26it/s]saving BB  train1-THALAMUS:  98%|█████████▊| 260/266 [00:03<00:00, 74.72it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 69.78it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:02<09:58,  2.26s/it]Loading train:   1%|          | 2/266 [00:04<09:45,  2.22s/it]Loading train:   1%|          | 3/266 [00:05<08:47,  2.01s/it]Loading train:   2%|▏         | 4/266 [00:07<08:14,  1.89s/it]Loading train:   2%|▏         | 5/266 [00:09<08:15,  1.90s/it]Loading train:   2%|▏         | 6/266 [00:10<07:45,  1.79s/it]Loading train:   3%|▎         | 7/266 [00:12<07:06,  1.65s/it]Loading train:   3%|▎         | 8/266 [00:13<06:49,  1.59s/it]Loading train:   3%|▎         | 9/266 [00:14<06:18,  1.47s/it]Loading train:   4%|▍         | 10/266 [00:16<06:12,  1.45s/it]Loading train:   4%|▍         | 11/266 [00:17<05:59,  1.41s/it]Loading train:   5%|▍         | 12/266 [00:19<05:57,  1.41s/it]Loading train:   5%|▍         | 13/266 [00:20<05:42,  1.35s/it]Loading train:   5%|▌         | 14/266 [00:21<05:45,  1.37s/it]Loading train:   6%|▌         | 15/266 [00:22<05:39,  1.35s/it]Loading train:   6%|▌         | 16/266 [00:24<05:54,  1.42s/it]Loading train:   6%|▋         | 17/266 [00:25<05:48,  1.40s/it]Loading train:   7%|▋         | 18/266 [00:27<05:39,  1.37s/it]Loading train:   7%|▋         | 19/266 [00:28<05:34,  1.35s/it]Loading train:   8%|▊         | 20/266 [00:30<05:52,  1.43s/it]Loading train:   8%|▊         | 21/266 [00:31<05:52,  1.44s/it]Loading train:   8%|▊         | 22/266 [00:32<05:39,  1.39s/it]Loading train:   9%|▊         | 23/266 [00:34<05:34,  1.38s/it]Loading train:   9%|▉         | 24/266 [00:35<05:23,  1.34s/it]Loading train:   9%|▉         | 25/266 [00:36<05:03,  1.26s/it]Loading train:  10%|▉         | 26/266 [00:37<05:14,  1.31s/it]Loading train:  10%|█         | 27/266 [00:39<05:00,  1.26s/it]Loading train:  11%|█         | 28/266 [00:40<05:30,  1.39s/it]Loading train:  11%|█         | 29/266 [00:42<05:17,  1.34s/it]Loading train:  11%|█▏        | 30/266 [00:43<05:13,  1.33s/it]Loading train:  12%|█▏        | 31/266 [00:44<05:00,  1.28s/it]Loading train:  12%|█▏        | 32/266 [00:45<04:59,  1.28s/it]Loading train:  12%|█▏        | 33/266 [00:47<04:54,  1.26s/it]Loading train:  13%|█▎        | 34/266 [00:48<04:59,  1.29s/it]Loading train:  13%|█▎        | 35/266 [00:49<04:40,  1.21s/it]Loading train:  14%|█▎        | 36/266 [00:50<04:37,  1.21s/it]Loading train:  14%|█▍        | 37/266 [00:51<04:21,  1.14s/it]Loading train:  14%|█▍        | 38/266 [00:52<04:25,  1.17s/it]Loading train:  15%|█▍        | 39/266 [00:54<04:33,  1.20s/it]Loading train:  15%|█▌        | 40/266 [00:55<04:35,  1.22s/it]Loading train:  15%|█▌        | 41/266 [00:56<04:40,  1.25s/it]Loading train:  16%|█▌        | 42/266 [00:57<04:41,  1.26s/it]Loading train:  16%|█▌        | 43/266 [00:59<04:46,  1.28s/it]Loading train:  17%|█▋        | 44/266 [01:00<04:36,  1.24s/it]Loading train:  17%|█▋        | 45/266 [01:01<04:46,  1.30s/it]Loading train:  17%|█▋        | 46/266 [01:02<04:24,  1.20s/it]Loading train:  18%|█▊        | 47/266 [01:03<04:16,  1.17s/it]Loading train:  18%|█▊        | 48/266 [01:05<04:14,  1.17s/it]Loading train:  18%|█▊        | 49/266 [01:06<04:13,  1.17s/it]Loading train:  19%|█▉        | 50/266 [01:07<04:26,  1.24s/it]Loading train:  19%|█▉        | 51/266 [01:09<04:43,  1.32s/it]Loading train:  20%|█▉        | 52/266 [01:10<04:32,  1.28s/it]Loading train:  20%|█▉        | 53/266 [01:11<04:26,  1.25s/it]Loading train:  20%|██        | 54/266 [01:12<04:26,  1.26s/it]Loading train:  21%|██        | 55/266 [01:13<04:11,  1.19s/it]Loading train:  21%|██        | 56/266 [01:14<04:06,  1.18s/it]Loading train:  21%|██▏       | 57/266 [01:16<04:03,  1.17s/it]Loading train:  22%|██▏       | 58/266 [01:17<04:01,  1.16s/it]Loading train:  22%|██▏       | 59/266 [01:18<03:57,  1.15s/it]Loading train:  23%|██▎       | 60/266 [01:19<03:46,  1.10s/it]Loading train:  23%|██▎       | 61/266 [01:20<03:47,  1.11s/it]Loading train:  23%|██▎       | 62/266 [01:21<03:45,  1.11s/it]Loading train:  24%|██▎       | 63/266 [01:22<03:36,  1.07s/it]Loading train:  24%|██▍       | 64/266 [01:23<03:37,  1.07s/it]Loading train:  24%|██▍       | 65/266 [01:24<03:36,  1.08s/it]Loading train:  25%|██▍       | 66/266 [01:25<03:39,  1.10s/it]Loading train:  25%|██▌       | 67/266 [01:26<03:34,  1.08s/it]Loading train:  26%|██▌       | 68/266 [01:27<03:30,  1.06s/it]Loading train:  26%|██▌       | 69/266 [01:29<03:35,  1.09s/it]Loading train:  26%|██▋       | 70/266 [01:30<03:32,  1.08s/it]Loading train:  27%|██▋       | 71/266 [01:31<03:30,  1.08s/it]Loading train:  27%|██▋       | 72/266 [01:32<03:24,  1.05s/it]Loading train:  27%|██▋       | 73/266 [01:33<03:20,  1.04s/it]Loading train:  28%|██▊       | 74/266 [01:34<03:32,  1.11s/it]Loading train:  28%|██▊       | 75/266 [01:35<03:23,  1.06s/it]Loading train:  29%|██▊       | 76/266 [01:36<03:19,  1.05s/it]Loading train:  29%|██▉       | 77/266 [01:37<03:17,  1.04s/it]Loading train:  29%|██▉       | 78/266 [01:38<03:33,  1.13s/it]Loading train:  30%|██▉       | 79/266 [01:40<03:33,  1.14s/it]Loading train:  30%|███       | 80/266 [01:41<03:37,  1.17s/it]Loading train:  30%|███       | 81/266 [01:42<03:49,  1.24s/it]Loading train:  31%|███       | 82/266 [01:43<03:43,  1.21s/it]Loading train:  31%|███       | 83/266 [01:45<03:51,  1.27s/it]Loading train:  32%|███▏      | 84/266 [01:46<03:42,  1.22s/it]Loading train:  32%|███▏      | 85/266 [01:47<03:44,  1.24s/it]Loading train:  32%|███▏      | 86/266 [01:48<03:35,  1.20s/it]Loading train:  33%|███▎      | 87/266 [01:49<03:33,  1.19s/it]Loading train:  33%|███▎      | 88/266 [01:51<03:34,  1.21s/it]Loading train:  33%|███▎      | 89/266 [01:52<03:30,  1.19s/it]Loading train:  34%|███▍      | 90/266 [01:53<03:34,  1.22s/it]Loading train:  34%|███▍      | 91/266 [01:54<03:31,  1.21s/it]Loading train:  35%|███▍      | 92/266 [01:56<03:33,  1.23s/it]Loading train:  35%|███▍      | 93/266 [01:57<03:39,  1.27s/it]Loading train:  35%|███▌      | 94/266 [01:58<03:28,  1.21s/it]Loading train:  36%|███▌      | 95/266 [01:59<03:27,  1.21s/it]Loading train:  36%|███▌      | 96/266 [02:01<03:52,  1.37s/it]Loading train:  36%|███▋      | 97/266 [02:03<04:13,  1.50s/it]Loading train:  37%|███▋      | 98/266 [02:05<04:28,  1.60s/it]Loading train:  37%|███▋      | 99/266 [02:06<04:20,  1.56s/it]Loading train:  38%|███▊      | 100/266 [02:08<04:16,  1.54s/it]Loading train:  38%|███▊      | 101/266 [02:09<03:51,  1.40s/it]Loading train:  38%|███▊      | 102/266 [02:10<03:29,  1.28s/it]Loading train:  39%|███▊      | 103/266 [02:11<03:18,  1.22s/it]Loading train:  39%|███▉      | 104/266 [02:12<03:12,  1.19s/it]Loading train:  39%|███▉      | 105/266 [02:13<03:09,  1.18s/it]Loading train:  40%|███▉      | 106/266 [02:14<03:04,  1.15s/it]Loading train:  40%|████      | 107/266 [02:15<03:03,  1.16s/it]Loading train:  41%|████      | 108/266 [02:16<03:04,  1.16s/it]Loading train:  41%|████      | 109/266 [02:18<03:10,  1.21s/it]Loading train:  41%|████▏     | 110/266 [02:19<03:04,  1.19s/it]Loading train:  42%|████▏     | 111/266 [02:20<02:58,  1.15s/it]Loading train:  42%|████▏     | 112/266 [02:21<02:55,  1.14s/it]Loading train:  42%|████▏     | 113/266 [02:22<02:56,  1.15s/it]Loading train:  43%|████▎     | 114/266 [02:23<02:56,  1.16s/it]Loading train:  43%|████▎     | 115/266 [02:25<02:57,  1.18s/it]Loading train:  44%|████▎     | 116/266 [02:26<02:53,  1.16s/it]Loading train:  44%|████▍     | 117/266 [02:27<02:49,  1.14s/it]Loading train:  44%|████▍     | 118/266 [02:28<02:58,  1.21s/it]Loading train:  45%|████▍     | 119/266 [02:29<02:58,  1.21s/it]Loading train:  45%|████▌     | 120/266 [02:31<02:57,  1.22s/it]Loading train:  45%|████▌     | 121/266 [02:32<03:02,  1.26s/it]Loading train:  46%|████▌     | 122/266 [02:34<03:13,  1.34s/it]Loading train:  46%|████▌     | 123/266 [02:35<03:09,  1.33s/it]Loading train:  47%|████▋     | 124/266 [02:36<02:59,  1.26s/it]Loading train:  47%|████▋     | 125/266 [02:37<02:57,  1.26s/it]Loading train:  47%|████▋     | 126/266 [02:39<03:02,  1.31s/it]Loading train:  48%|████▊     | 127/266 [02:40<02:59,  1.29s/it]Loading train:  48%|████▊     | 128/266 [02:41<03:03,  1.33s/it]Loading train:  48%|████▊     | 129/266 [02:43<03:03,  1.34s/it]Loading train:  49%|████▉     | 130/266 [02:44<02:53,  1.28s/it]Loading train:  49%|████▉     | 131/266 [02:45<02:45,  1.22s/it]Loading train:  50%|████▉     | 132/266 [02:46<02:32,  1.14s/it]Loading train:  50%|█████     | 133/266 [02:47<02:31,  1.14s/it]Loading train:  50%|█████     | 134/266 [02:48<02:23,  1.09s/it]Loading train:  51%|█████     | 135/266 [02:49<02:21,  1.08s/it]Loading train:  51%|█████     | 136/266 [02:50<02:22,  1.09s/it]Loading train:  52%|█████▏    | 137/266 [02:51<02:20,  1.09s/it]Loading train:  52%|█████▏    | 138/266 [02:52<02:17,  1.07s/it]Loading train:  52%|█████▏    | 139/266 [02:53<02:14,  1.06s/it]Loading train:  53%|█████▎    | 140/266 [02:54<02:09,  1.03s/it]Loading train:  53%|█████▎    | 141/266 [02:55<02:08,  1.03s/it]Loading train:  53%|█████▎    | 142/266 [02:56<02:07,  1.03s/it]Loading train:  54%|█████▍    | 143/266 [02:57<02:05,  1.02s/it]Loading train:  54%|█████▍    | 144/266 [02:58<02:02,  1.01s/it]Loading train:  55%|█████▍    | 145/266 [02:59<02:00,  1.00it/s]Loading train:  55%|█████▍    | 146/266 [03:00<02:02,  1.02s/it]Loading train:  55%|█████▌    | 147/266 [03:01<02:04,  1.04s/it]Loading train:  56%|█████▌    | 148/266 [03:03<02:10,  1.10s/it]Loading train:  56%|█████▌    | 149/266 [03:04<02:27,  1.26s/it]Loading train:  56%|█████▋    | 150/266 [03:06<02:29,  1.29s/it]Loading train:  57%|█████▋    | 151/266 [03:07<02:40,  1.40s/it]Loading train:  57%|█████▋    | 152/266 [03:08<02:25,  1.28s/it]Loading train:  58%|█████▊    | 153/266 [03:10<02:31,  1.34s/it]Loading train:  58%|█████▊    | 154/266 [03:11<02:34,  1.38s/it]Loading train:  58%|█████▊    | 155/266 [03:12<02:28,  1.34s/it]Loading train:  59%|█████▊    | 156/266 [03:14<02:29,  1.36s/it]Loading train:  59%|█████▉    | 157/266 [03:15<02:22,  1.31s/it]Loading train:  59%|█████▉    | 158/266 [03:16<02:12,  1.23s/it]Loading train:  60%|█████▉    | 159/266 [03:18<02:21,  1.33s/it]Loading train:  60%|██████    | 160/266 [03:19<02:16,  1.29s/it]Loading train:  61%|██████    | 161/266 [03:20<02:11,  1.25s/it]Loading train:  61%|██████    | 162/266 [03:21<02:04,  1.20s/it]Loading train:  61%|██████▏   | 163/266 [03:22<02:01,  1.18s/it]Loading train:  62%|██████▏   | 164/266 [03:23<01:58,  1.16s/it]Loading train:  62%|██████▏   | 165/266 [03:24<01:56,  1.16s/it]Loading train:  62%|██████▏   | 166/266 [03:26<01:58,  1.19s/it]Loading train:  63%|██████▎   | 167/266 [03:27<01:56,  1.17s/it]Loading train:  63%|██████▎   | 168/266 [03:28<01:51,  1.14s/it]Loading train:  64%|██████▎   | 169/266 [03:29<01:53,  1.17s/it]Loading train:  64%|██████▍   | 170/266 [03:30<01:48,  1.13s/it]Loading train:  64%|██████▍   | 171/266 [03:31<01:49,  1.15s/it]Loading train:  65%|██████▍   | 172/266 [03:32<01:44,  1.11s/it]Loading train:  65%|██████▌   | 173/266 [03:34<01:43,  1.11s/it]Loading train:  65%|██████▌   | 174/266 [03:35<01:43,  1.13s/it]Loading train:  66%|██████▌   | 175/266 [03:36<01:44,  1.15s/it]Loading train:  66%|██████▌   | 176/266 [03:37<01:45,  1.17s/it]Loading train:  67%|██████▋   | 177/266 [03:38<01:45,  1.18s/it]Loading train:  67%|██████▋   | 178/266 [03:39<01:40,  1.14s/it]Loading train:  67%|██████▋   | 179/266 [03:40<01:36,  1.11s/it]Loading train:  68%|██████▊   | 180/266 [03:42<01:34,  1.10s/it]Loading train:  68%|██████▊   | 181/266 [03:43<01:34,  1.11s/it]Loading train:  68%|██████▊   | 182/266 [03:44<01:34,  1.12s/it]Loading train:  69%|██████▉   | 183/266 [03:45<01:34,  1.14s/it]Loading train:  69%|██████▉   | 184/266 [03:46<01:31,  1.12s/it]Loading train:  70%|██████▉   | 185/266 [03:47<01:30,  1.11s/it]Loading train:  70%|██████▉   | 186/266 [03:48<01:29,  1.12s/it]Loading train:  70%|███████   | 187/266 [03:49<01:28,  1.12s/it]Loading train:  71%|███████   | 188/266 [03:51<01:28,  1.13s/it]Loading train:  71%|███████   | 189/266 [03:51<01:22,  1.07s/it]Loading train:  71%|███████▏  | 190/266 [03:53<01:22,  1.09s/it]Loading train:  72%|███████▏  | 191/266 [03:54<01:39,  1.32s/it]Loading train:  72%|███████▏  | 192/266 [03:56<01:44,  1.41s/it]Loading train:  73%|███████▎  | 193/266 [03:58<01:47,  1.47s/it]Loading train:  73%|███████▎  | 194/266 [03:59<01:49,  1.52s/it]Loading train:  73%|███████▎  | 195/266 [04:01<01:46,  1.50s/it]Loading train:  74%|███████▎  | 196/266 [04:02<01:37,  1.39s/it]Loading train:  74%|███████▍  | 197/266 [04:03<01:32,  1.34s/it]Loading train:  74%|███████▍  | 198/266 [04:04<01:27,  1.29s/it]Loading train:  75%|███████▍  | 199/266 [04:06<01:24,  1.26s/it]Loading train:  75%|███████▌  | 200/266 [04:07<01:18,  1.20s/it]Loading train:  76%|███████▌  | 201/266 [04:08<01:16,  1.17s/it]Loading train:  76%|███████▌  | 202/266 [04:09<01:13,  1.16s/it]Loading train:  76%|███████▋  | 203/266 [04:10<01:12,  1.14s/it]Loading train:  77%|███████▋  | 204/266 [04:11<01:14,  1.20s/it]Loading train:  77%|███████▋  | 205/266 [04:13<01:18,  1.29s/it]Loading train:  77%|███████▋  | 206/266 [04:14<01:15,  1.27s/it]Loading train:  78%|███████▊  | 207/266 [04:15<01:13,  1.24s/it]Loading train:  78%|███████▊  | 208/266 [04:16<01:07,  1.16s/it]Loading train:  79%|███████▊  | 209/266 [04:17<01:06,  1.16s/it]Loading train:  79%|███████▉  | 210/266 [04:19<01:06,  1.18s/it]Loading train:  79%|███████▉  | 211/266 [04:20<01:06,  1.21s/it]Loading train:  80%|███████▉  | 212/266 [04:21<01:03,  1.18s/it]Loading train:  80%|████████  | 213/266 [04:22<01:08,  1.30s/it]Loading train:  80%|████████  | 214/266 [04:24<01:06,  1.27s/it]Loading train:  81%|████████  | 215/266 [04:25<01:02,  1.23s/it]Loading train:  81%|████████  | 216/266 [04:26<01:02,  1.25s/it]Loading train:  82%|████████▏ | 217/266 [04:27<00:58,  1.20s/it]Loading train:  82%|████████▏ | 218/266 [04:28<00:56,  1.17s/it]Loading train:  82%|████████▏ | 219/266 [04:29<00:53,  1.13s/it]Loading train:  83%|████████▎ | 220/266 [04:30<00:51,  1.12s/it]Loading train:  83%|████████▎ | 221/266 [04:32<00:52,  1.16s/it]Loading train:  83%|████████▎ | 222/266 [04:33<00:56,  1.28s/it]Loading train:  84%|████████▍ | 223/266 [04:34<00:53,  1.24s/it]Loading train:  84%|████████▍ | 224/266 [04:36<00:50,  1.21s/it]Loading train:  85%|████████▍ | 225/266 [04:37<00:47,  1.16s/it]Loading train:  85%|████████▍ | 226/266 [04:38<00:45,  1.14s/it]Loading train:  85%|████████▌ | 227/266 [04:39<00:43,  1.12s/it]Loading train:  86%|████████▌ | 228/266 [04:40<00:42,  1.13s/it]Loading train:  86%|████████▌ | 229/266 [04:41<00:40,  1.10s/it]Loading train:  86%|████████▋ | 230/266 [04:42<00:39,  1.10s/it]Loading train:  87%|████████▋ | 231/266 [04:43<00:38,  1.11s/it]Loading train:  87%|████████▋ | 232/266 [04:44<00:38,  1.12s/it]Loading train:  88%|████████▊ | 233/266 [04:46<00:37,  1.15s/it]Loading train:  88%|████████▊ | 234/266 [04:47<00:36,  1.13s/it]Loading train:  88%|████████▊ | 235/266 [04:48<00:33,  1.07s/it]Loading train:  89%|████████▊ | 236/266 [04:49<00:33,  1.13s/it]Loading train:  89%|████████▉ | 237/266 [04:50<00:31,  1.10s/it]Loading train:  89%|████████▉ | 238/266 [04:51<00:30,  1.09s/it]Loading train:  90%|████████▉ | 239/266 [04:52<00:29,  1.11s/it]Loading train:  90%|█████████ | 240/266 [04:53<00:30,  1.18s/it]Loading train:  91%|█████████ | 241/266 [04:55<00:30,  1.22s/it]Loading train:  91%|█████████ | 242/266 [04:56<00:30,  1.27s/it]Loading train:  91%|█████████▏| 243/266 [04:57<00:27,  1.19s/it]Loading train:  92%|█████████▏| 244/266 [04:58<00:26,  1.20s/it]Loading train:  92%|█████████▏| 245/266 [04:59<00:24,  1.16s/it]Loading train:  92%|█████████▏| 246/266 [05:01<00:23,  1.16s/it]Loading train:  93%|█████████▎| 247/266 [05:02<00:22,  1.19s/it]Loading train:  93%|█████████▎| 248/266 [05:03<00:22,  1.27s/it]Loading train:  94%|█████████▎| 249/266 [05:05<00:22,  1.34s/it]Loading train:  94%|█████████▍| 250/266 [05:06<00:21,  1.36s/it]Loading train:  94%|█████████▍| 251/266 [05:08<00:22,  1.48s/it]Loading train:  95%|█████████▍| 252/266 [05:09<00:20,  1.47s/it]Loading train:  95%|█████████▌| 253/266 [05:11<00:19,  1.47s/it]Loading train:  95%|█████████▌| 254/266 [05:12<00:17,  1.44s/it]Loading train:  96%|█████████▌| 255/266 [05:14<00:15,  1.44s/it]Loading train:  96%|█████████▌| 256/266 [05:15<00:13,  1.38s/it]Loading train:  97%|█████████▋| 257/266 [05:16<00:12,  1.41s/it]Loading train:  97%|█████████▋| 258/266 [05:18<00:11,  1.40s/it]Loading train:  97%|█████████▋| 259/266 [05:19<00:09,  1.36s/it]Loading train:  98%|█████████▊| 260/266 [05:20<00:07,  1.33s/it]Loading train:  98%|█████████▊| 261/266 [05:22<00:06,  1.33s/it]Loading train:  98%|█████████▊| 262/266 [05:23<00:05,  1.33s/it]Loading train:  99%|█████████▉| 263/266 [05:24<00:04,  1.36s/it]Loading train:  99%|█████████▉| 264/266 [05:26<00:02,  1.39s/it]Loading train: 100%|█████████▉| 265/266 [05:27<00:01,  1.39s/it]Loading train: 100%|██████████| 266/266 [05:28<00:00,  1.32s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   5%|▌         | 14/266 [00:00<00:01, 137.83it/s]concatenating: train:  10%|█         | 27/266 [00:00<00:01, 133.20it/s]concatenating: train:  14%|█▍        | 37/266 [00:00<00:01, 119.17it/s]concatenating: train:  17%|█▋        | 45/266 [00:00<00:02, 78.73it/s] concatenating: train:  20%|█▉        | 52/266 [00:00<00:03, 59.08it/s]concatenating: train:  22%|██▏       | 58/266 [00:00<00:03, 52.79it/s]concatenating: train:  24%|██▍       | 65/266 [00:00<00:03, 56.56it/s]concatenating: train:  27%|██▋       | 71/266 [00:01<00:04, 47.79it/s]concatenating: train:  35%|███▍      | 93/266 [00:01<00:02, 62.22it/s]concatenating: train:  42%|████▏     | 113/266 [00:01<00:01, 78.32it/s]concatenating: train:  49%|████▉     | 130/266 [00:01<00:01, 92.87it/s]concatenating: train:  55%|█████▍    | 146/266 [00:01<00:01, 100.07it/s]concatenating: train:  60%|██████    | 160/266 [00:02<00:01, 53.15it/s] concatenating: train:  64%|██████▍   | 171/266 [00:02<00:02, 39.39it/s]concatenating: train:  69%|██████▉   | 183/266 [00:02<00:01, 49.19it/s]concatenating: train:  77%|███████▋  | 206/266 [00:02<00:00, 64.34it/s]concatenating: train:  86%|████████▋ | 230/266 [00:02<00:00, 82.35it/s]concatenating: train:  94%|█████████▍| 251/266 [00:02<00:00, 99.50it/s]concatenating: train: 100%|██████████| 266/266 [00:03<00:00, 87.87it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:01<00:06,  1.64s/it]Loading test:  40%|████      | 2/5 [00:03<00:04,  1.60s/it]Loading test:  60%|██████    | 3/5 [00:04<00:03,  1.60s/it]Loading test:  80%|████████  | 4/5 [00:06<00:01,  1.54s/it]Loading test: 100%|██████████| 5/5 [00:07<00:00,  1.61s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation:  60%|██████    | 3/5 [00:00<00:00, 23.79it/s]concatenating: validation:  80%|████████  | 4/5 [00:00<00:00, 16.17it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 18.77it/s]2019-08-17 19:58:15.333835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-17 19:58:15.333947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-17 19:58:15.333963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-17 19:58:15.333973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-17 19:58:15.334370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:08,  4.87it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:07,  5.73it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:07,  5.34it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:05,  6.94it/s]loading the weights for Res Unet:  20%|██        | 9/44 [00:00<00:05,  6.37it/s]loading the weights for Res Unet:  25%|██▌       | 11/44 [00:01<00:04,  7.30it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:06,  5.20it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:03,  6.88it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:03,  7.65it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:02<00:03,  6.71it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:02,  8.42it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01,  9.04it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01,  8.38it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:03<00:01,  7.03it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:03<00:01,  8.88it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:03<00:00,  9.29it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00,  9.39it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:04<00:00,  7.47it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:04<00:00, 10.32it/s]
Epoch 00048: val_mDice did not improve from 0.89275
Restoring model weights from the end of the best epoch
Epoch 00048: early stopping
{'val_loss': [0.07677092142403126, 0.06624380685389042, 0.06120853461325169, 0.06213370561599731, 0.06230862326920032, 0.05956103429198265, 0.058673585578799245, 0.05838766694068909, 0.05915053188800812, 0.06027857176959515, 0.05972191654145718, 0.06148007959127426, 0.06162136159837246, 0.06173264645040035, 0.06264274157583713, 0.06205813698470593, 0.05924687460064888, 0.0598737794905901, 0.06311848796904088, 0.061149563640356064, 0.06108885183930397, 0.06142260767519474, 0.06083683557808399, 0.06054571159183979, 0.06070498749613762, 0.06245274804532528, 0.06185274496674538, 0.06311042681336403, 0.06039961650967598, 0.06089996509253979, 0.0612118311226368, 0.06116233356297016, 0.06102464236319065, 0.0627823792397976, 0.06498035229742527, 0.06310672126710415, 0.06048981212079525, 0.06474239453673362, 0.0627557635307312, 0.061243433877825736, 0.0633099190890789, 0.05927672050893307, 0.060651795193552974, 0.06200559549033642, 0.06153249219059944, 0.06311077363789082, 0.06163190901279449, 0.061271799728274345], 'val_acc': [0.9933291971683502, 0.9943737804889679, 0.9947054088115692, 0.9946011126041412, 0.9948615968227387, 0.9948835253715516, 0.9949108183383941, 0.9949205636978149, 0.9948204278945922, 0.9948289513587951, 0.9949127674102783, 0.9949388444423676, 0.9948396742343902, 0.9947139322757721, 0.9947729051113129, 0.9947504937648773, 0.994767302274704, 0.9947083353996277, 0.9946371793746949, 0.9946954131126404, 0.9948586761951447, 0.9948535561561584, 0.9947280704975128, 0.9945133924484253, 0.9948774337768554, 0.9947546362876892, 0.9947319686412811, 0.9945977091789245, 0.9948238253593444, 0.9947772979736328, 0.9948345482349396, 0.994797021150589, 0.9948399126529693, 0.9946649610996247, 0.9945640861988068, 0.9947054028511048, 0.9949020504951477, 0.9946581363677979, 0.9947300136089325, 0.994799941778183, 0.9946717858314514, 0.9948545277118683, 0.994791179895401, 0.9946359634399414, 0.9947714447975159, 0.9946483910083771, 0.994869875907898, 0.9948213994503021], 'val_mDice': [0.8615591764450073, 0.8791140377521515, 0.8878028869628907, 0.8862498044967652, 0.8858762800693512, 0.8907271981239319, 0.8921874582767486, 0.8927538812160491, 0.8917243659496308, 0.8894609391689301, 0.8903846442699432, 0.8873269915580749, 0.8872075617313385, 0.8868678331375122, 0.8854979455471039, 0.8865467131137847, 0.8913056910037994, 0.8904286861419678, 0.8851383626461029, 0.8879567444324493, 0.888251107931137, 0.8875747859477997, 0.888620114326477, 0.8895267486572266, 0.8888879835605621, 0.886053329706192, 0.8867419421672821, 0.8849988102912902, 0.8893449783325196, 0.8885178625583648, 0.88795086145401, 0.8881187558174133, 0.8882631480693817, 0.885645043849945, 0.8819324135780334, 0.8849285125732422, 0.8893485367298126, 0.8821470201015472, 0.8854704618453979, 0.8880668699741363, 0.8848228931427002, 0.8914344787597657, 0.8891452014446258, 0.8871222972869873, 0.8875146210193634, 0.8847828924655914, 0.8873115956783295, 0.8879631817340851], 'loss': [0.2154588589670574, 0.07025123863262787, 0.05765024780698407, 0.05172846801335411, 0.04790038595286009, 0.045438424650597393, 0.04309995265701331, 0.04170727133993172, 0.04083257496229713, 0.03955312188542102, 0.03849515303469962, 0.03771277735390544, 0.03699738238096412, 0.036564665100556774, 0.03566273569160388, 0.0352731459731915, 0.03485849506530888, 0.03426408468793135, 0.03416238789356704, 0.03364433357003916, 0.033276157292662215, 0.032913991887717785, 0.03273902878012969, 0.03251690378154841, 0.032157709801094184, 0.03192053551327723, 0.03176791766006796, 0.03145842782866343, 0.031390272539454936, 0.031226593706487022, 0.03105599083416088, 0.030929650741058235, 0.030715534179272936, 0.030561570233271773, 0.03039262460738843, 0.03031808274382499, 0.03030309156043935, 0.030162512137101586, 0.029866912412894527, 0.02993236174983653, 0.029796363462011446, 0.029639990001253474, 0.02950343489347419, 0.029496773115644463, 0.02942561625707025, 0.029347121690255155, 0.029297190370907944, 0.02921200492575205], 'acc': [0.974187511184163, 0.9929203890772773, 0.993955818682991, 0.9944436574738718, 0.9947807979906607, 0.9949788100790925, 0.9951772966662433, 0.9952940690954871, 0.9953834910836085, 0.9954831568051642, 0.9955786822262772, 0.995663374182146, 0.9957333947125068, 0.9957676317018214, 0.9958561453638844, 0.9958911008439902, 0.9959515267516204, 0.9960069872794838, 0.9960247742067849, 0.9960789631539851, 0.9961115250583374, 0.9961489348390774, 0.9961768713084159, 0.9961947480462672, 0.9962255958149994, 0.996250622456585, 0.996257607534426, 0.9962893388690206, 0.9963054323701582, 0.9963223410516527, 0.9963391210401872, 0.9963528027777103, 0.9963514803626145, 0.9963841378618531, 0.9963981334762814, 0.9963927482295302, 0.9963984593179877, 0.9964280219120147, 0.9964430631346465, 0.9964385048733241, 0.9964486307754017, 0.9964707095076017, 0.9964759627688364, 0.9964718990490854, 0.9964792642063678, 0.9964914303903228, 0.9964979741292807, 0.9964961764040401], 'mDice': [0.722382998615679, 0.8732525700264074, 0.8943177320103846, 0.9045408488376291, 0.9112415902866838, 0.9155909801579861, 0.9197516208906906, 0.9222363907454222, 0.9237965108493255, 0.9260952937094844, 0.9279988484567149, 0.9294006473344166, 0.9306932864249803, 0.9314773002548966, 0.9331081788648162, 0.9338135902839113, 0.9345553022265481, 0.9356364862632703, 0.9358179791268332, 0.9367592741120354, 0.9374323169803528, 0.9380914866572881, 0.9384099458555157, 0.9388146838280755, 0.9394770072907604, 0.9399093081169362, 0.9401900845631682, 0.9407570095278976, 0.9408809429759164, 0.9411790475915363, 0.9414926469166289, 0.9417249341058243, 0.9421269261713907, 0.9424003319273119, 0.9427130334280258, 0.9428555614146559, 0.9428809453028891, 0.943133907966915, 0.9436837488948909, 0.9435620621886587, 0.9438161976140561, 0.944098883010475, 0.944354346809942, 0.9443694153910355, 0.944501138491216, 0.9446403176198329, 0.9447337339028935, 0.9448974741426538]}
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 2  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 2  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 52, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 26, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 26, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 26, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 26, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 26, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 26, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 26, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 26, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 13, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 13, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 13, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 13, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 13, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 13, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 13, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 13, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 13, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 13, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 26, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 26, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 26, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 26, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 26, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 26, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 26, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 26, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 26, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 30)   25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 52, 60)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 52, 30)   16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 52, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 52, 30)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 52, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 52, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 52, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 52, 90)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 52, 90)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 52, 13)   1183        dropout_5[0][0]                  
==================================================================================================
Total params: 501,343
Trainable params: 229,423
Non-trainable params: 271,920
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.34723688e-02 3.28968475e-02 7.69247533e-02 9.55828741e-03
 2.76638603e-02 7.23751810e-03 8.42734692e-02 1.14335981e-01
 8.97763132e-02 1.36401461e-02 2.91072465e-01 1.88907693e-01
 2.40295974e-04]
Train on 16809 samples, validate on 306 samples
Epoch 1/300
 - 17s - loss: 1.7321 - acc: 0.7946 - mDice: 0.2473 - val_loss: 1.8824 - val_acc: 0.9160 - val_mDice: 0.3930

Epoch 00001: val_mDice improved from -inf to 0.39295, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 12s - loss: 0.7767 - acc: 0.8989 - mDice: 0.4560 - val_loss: 1.5057 - val_acc: 0.9288 - val_mDice: 0.4618

Epoch 00002: val_mDice improved from 0.39295 to 0.46181, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 12s - loss: 0.6462 - acc: 0.9138 - mDice: 0.5184 - val_loss: 1.4147 - val_acc: 0.9372 - val_mDice: 0.4945

Epoch 00003: val_mDice improved from 0.46181 to 0.49448, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 13s - loss: 0.5876 - acc: 0.9246 - mDice: 0.5495 - val_loss: 1.4099 - val_acc: 0.9345 - val_mDice: 0.5039

Epoch 00004: val_mDice improved from 0.49448 to 0.50394, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 12s - loss: 0.5516 - acc: 0.9290 - mDice: 0.5715 - val_loss: 1.3529 - val_acc: 0.9360 - val_mDice: 0.5160

Epoch 00005: val_mDice improved from 0.50394 to 0.51604, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 12s - loss: 0.5000 - acc: 0.9337 - mDice: 0.5994 - val_loss: 1.3073 - val_acc: 0.9389 - val_mDice: 0.5358

Epoch 00006: val_mDice improved from 0.51604 to 0.53576, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 12s - loss: 0.5186 - acc: 0.9329 - mDice: 0.5894 - val_loss: 1.3256 - val_acc: 0.9423 - val_mDice: 0.5286

Epoch 00007: val_mDice did not improve from 0.53576
Epoch 8/300
 - 13s - loss: 0.4893 - acc: 0.9364 - mDice: 0.6055 - val_loss: 1.2754 - val_acc: 0.9445 - val_mDice: 0.5424

Epoch 00008: val_mDice improved from 0.53576 to 0.54236, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 12s - loss: 0.4938 - acc: 0.9369 - mDice: 0.6067 - val_loss: 1.2697 - val_acc: 0.9423 - val_mDice: 0.5473

Epoch 00009: val_mDice improved from 0.54236 to 0.54728, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 13s - loss: 0.4569 - acc: 0.9391 - mDice: 0.6243 - val_loss: 1.2658 - val_acc: 0.9447 - val_mDice: 0.5455

Epoch 00010: val_mDice did not improve from 0.54728
Epoch 11/300
 - 13s - loss: 0.4517 - acc: 0.9390 - mDice: 0.6289 - val_loss: 1.2913 - val_acc: 0.9456 - val_mDice: 0.5323

Epoch 00011: val_mDice did not improve from 0.54728
Epoch 12/300
 - 13s - loss: 0.4629 - acc: 0.9396 - mDice: 0.6287 - val_loss: 1.2753 - val_acc: 0.9443 - val_mDice: 0.5446

Epoch 00012: val_mDice did not improve from 0.54728
Epoch 13/300
 - 13s - loss: 0.4319 - acc: 0.9414 - mDice: 0.6403 - val_loss: 1.2107 - val_acc: 0.9445 - val_mDice: 0.5570

Epoch 00013: val_mDice improved from 0.54728 to 0.55699, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 13s - loss: 0.4113 - acc: 0.9428 - mDice: 0.6541 - val_loss: 1.1942 - val_acc: 0.9479 - val_mDice: 0.5629

Epoch 00014: val_mDice improved from 0.55699 to 0.56287, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 13s - loss: 0.4140 - acc: 0.9434 - mDice: 0.6568 - val_loss: 1.1901 - val_acc: 0.9462 - val_mDice: 0.5630

Epoch 00015: val_mDice improved from 0.56287 to 0.56303, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 12s - loss: 0.3989 - acc: 0.9440 - mDice: 0.6630 - val_loss: 1.1945 - val_acc: 0.9457 - val_mDice: 0.5519

Epoch 00016: val_mDice did not improve from 0.56303
Epoch 17/300
 - 13s - loss: 0.3839 - acc: 0.9447 - mDice: 0.6700 - val_loss: 1.1367 - val_acc: 0.9479 - val_mDice: 0.5590

Epoch 00017: val_mDice did not improve from 0.56303
Epoch 18/300
 - 13s - loss: 0.3983 - acc: 0.9437 - mDice: 0.6630 - val_loss: 1.1887 - val_acc: 0.9509 - val_mDice: 0.5494

Epoch 00018: val_mDice did not improve from 0.56303
Epoch 19/300
 - 13s - loss: 0.3952 - acc: 0.9449 - mDice: 0.6704 - val_loss: 1.1569 - val_acc: 0.9426 - val_mDice: 0.5585

Epoch 00019: val_mDice did not improve from 0.56303
Epoch 20/300
 - 13s - loss: 0.3861 - acc: 0.9450 - mDice: 0.6711 - val_loss: 1.1509 - val_acc: 0.9458 - val_mDice: 0.5587

Epoch 00020: val_mDice did not improve from 0.56303
Epoch 21/300
 - 13s - loss: 0.3846 - acc: 0.9451 - mDice: 0.6711 - val_loss: 1.1329 - val_acc: 0.9468 - val_mDice: 0.5593

Epoch 00021: val_mDice did not improve from 0.56303
Epoch 22/300
 - 12s - loss: 0.3563 - acc: 0.9470 - mDice: 0.6880 - val_loss: 1.1025 - val_acc: 0.9479 - val_mDice: 0.5710

Epoch 00022: val_mDice improved from 0.56303 to 0.57096, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 13s - loss: 0.3729 - acc: 0.9466 - mDice: 0.6834 - val_loss: 1.1146 - val_acc: 0.9469 - val_mDice: 0.5670

Epoch 00023: val_mDice did not improve from 0.57096
Epoch 24/300
 - 13s - loss: 0.3773 - acc: 0.9461 - mDice: 0.6804 - val_loss: 1.1565 - val_acc: 0.9492 - val_mDice: 0.5575

Epoch 00024: val_mDice did not improve from 0.57096
Epoch 25/300
 - 13s - loss: 0.3616 - acc: 0.9470 - mDice: 0.6879 - val_loss: 1.1360 - val_acc: 0.9472 - val_mDice: 0.5655

Epoch 00025: val_mDice did not improve from 0.57096
Epoch 26/300
 - 13s - loss: 0.3543 - acc: 0.9475 - mDice: 0.6926 - val_loss: 1.1097 - val_acc: 0.9482 - val_mDice: 0.5582

Epoch 00026: val_mDice did not improve from 0.57096
Epoch 27/300
 - 13s - loss: 0.3472 - acc: 0.9481 - mDice: 0.6958 - val_loss: 1.0747 - val_acc: 0.9478 - val_mDice: 0.5712

Epoch 00027: val_mDice improved from 0.57096 to 0.57124, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 13s - loss: 0.3435 - acc: 0.9486 - mDice: 0.7018 - val_loss: 1.0982 - val_acc: 0.9465 - val_mDice: 0.5723

Epoch 00028: val_mDice improved from 0.57124 to 0.57227, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 13s - loss: 0.3615 - acc: 0.9466 - mDice: 0.6909 - val_loss: 1.0970 - val_acc: 0.9449 - val_mDice: 0.5666

Epoch 00029: val_mDice did not improve from 0.57227
Epoch 30/300
 - 13s - loss: 0.3356 - acc: 0.9486 - mDice: 0.7030 - val_loss: 1.0867 - val_acc: 0.9481 - val_mDice: 0.5684

Epoch 00030: val_mDice did not improve from 0.57227
Epoch 31/300
 - 13s - loss: 0.3509 - acc: 0.9481 - mDice: 0.6980 - val_loss: 1.1317 - val_acc: 0.9507 - val_mDice: 0.5568

Epoch 00031: val_mDice did not improve from 0.57227
Epoch 32/300
 - 13s - loss: 0.3535 - acc: 0.9476 - mDice: 0.6942 - val_loss: 1.0276 - val_acc: 0.9496 - val_mDice: 0.5687

Epoch 00032: val_mDice did not improve from 0.57227
Epoch 33/300
 - 13s - loss: 0.3403 - acc: 0.9485 - mDice: 0.7014 - val_loss: 1.0709 - val_acc: 0.9482 - val_mDice: 0.5747

Epoch 00033: val_mDice improved from 0.57227 to 0.57474, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 13s - loss: 0.3414 - acc: 0.9489 - mDice: 0.7042 - val_loss: 1.0939 - val_acc: 0.9474 - val_mDice: 0.5725

Epoch 00034: val_mDice did not improve from 0.57474
Epoch 35/300
 - 13s - loss: 0.3443 - acc: 0.9486 - mDice: 0.7023 - val_loss: 1.1038 - val_acc: 0.9490 - val_mDice: 0.5605

Epoch 00035: val_mDice did not improve from 0.57474
Epoch 36/300
 - 13s - loss: 0.3334 - acc: 0.9498 - mDice: 0.7116 - val_loss: 1.0378 - val_acc: 0.9494 - val_mDice: 0.5715

Epoch 00036: val_mDice did not improve from 0.57474
Epoch 37/300
 - 13s - loss: 0.3323 - acc: 0.9488 - mDice: 0.7078 - val_loss: 3.1559 - val_acc: 0.9419 - val_mDice: 0.4003

Epoch 00037: val_mDice did not improve from 0.57474
Epoch 38/300
 - 13s - loss: 0.3580 - acc: 0.9465 - mDice: 0.6896 - val_loss: 1.0986 - val_acc: 0.9488 - val_mDice: 0.5607

Epoch 00038: val_mDice did not improve from 0.57474
Epoch 39/300
 - 13s - loss: 0.3371 - acc: 0.9492 - mDice: 0.7066 - val_loss: 1.0732 - val_acc: 0.9462 - val_mDice: 0.5668

Epoch 00039: val_mDice did not improve from 0.57474
Epoch 40/300
 - 13s - loss: 0.3278 - acc: 0.9499 - mDice: 0.7138 - val_loss: 1.0758 - val_acc: 0.9477 - val_mDice: 0.5605

Epoch 00040: val_mDice did not improve from 0.57474
Epoch 41/300
 - 13s - loss: 0.3472 - acc: 0.9482 - mDice: 0.6994 - val_loss: 1.0849 - val_acc: 0.9447 - val_mDice: 0.5570

Epoch 00041: val_mDice did not improve from 0.57474
Epoch 42/300
 - 12s - loss: 0.3229 - acc: 0.9504 - mDice: 0.7172 - val_loss: 1.0232 - val_acc: 0.9478 - val_mDice: 0.5737

Epoch 00042: val_mDice did not improve from 0.57474
Epoch 43/300
 - 12s - loss: 0.3140 - acc: 0.9505 - mDice: 0.7186 - val_loss: 1.0296 - val_acc: 0.9460 - val_mDice: 0.5689

Epoch 00043: val_mDice did not improve from 0.57474
Epoch 44/300
 - 13s - loss: 0.3165 - acc: 0.9505 - mDice: 0.7191 - val_loss: 1.0140 - val_acc: 0.9488 - val_mDice: 0.5690

Epoch 00044: val_mDice did not improve from 0.57474
Epoch 45/300
 - 13s - loss: 0.3116 - acc: 0.9513 - mDice: 0.7256 - val_loss: 0.9967 - val_acc: 0.9492 - val_mDice: 0.5671

Epoch 00045: val_mDice did not improve from 0.57474
Epoch 46/300
 - 13s - loss: 0.3641 - acc: 0.9474 - mDice: 0.6924 - val_loss: 1.0335 - val_acc: 0.9467 - val_mDice: 0.5710

Epoch 00046: val_mDice did not improve from 0.57474
Epoch 47/300
 - 13s - loss: 0.3262 - acc: 0.9500 - mDice: 0.7147 - val_loss: 1.0380 - val_acc: 0.9492 - val_mDice: 0.5666

Epoch 00047: val_mDice did not improve from 0.57474
Epoch 48/300
 - 13s - loss: 0.3232 - acc: 0.9506 - mDice: 0.7191 - val_loss: 1.0155 - val_acc: 0.9479 - val_mDice: 0.5754

Epoch 00048: val_mDice improved from 0.57474 to 0.57536, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 13s - loss: 0.3369 - acc: 0.9486 - mDice: 0.7070 - val_loss: 1.0352 - val_acc: 0.9476 - val_mDice: 0.5628

Epoch 00049: val_mDice did not improve from 0.57536
Epoch 50/300
 - 13s - loss: 0.3331 - acc: 0.9501 - mDice: 0.7144 - val_loss: 1.0006 - val_acc: 0.9508 - val_mDice: 0.5763

Epoch 00050: val_mDice improved from 0.57536 to 0.57628, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 13s - loss: 0.3348 - acc: 0.9494 - mDice: 0.7081 - val_loss: 1.1405 - val_acc: 0.9470 - val_mDice: 0.5567

Epoch 00051: val_mDice did not improve from 0.57628
Epoch 52/300
 - 13s - loss: 0.3228 - acc: 0.9500 - mDice: 0.7127 - val_loss: 1.0223 - val_acc: 0.9470 - val_mDice: 0.5781

Epoch 00052: val_mDice improved from 0.57628 to 0.57806, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 53/300
 - 13s - loss: 0.3112 - acc: 0.9508 - mDice: 0.7226 - val_loss: 1.0131 - val_acc: 0.9478 - val_mDice: 0.5766

Epoch 00053: val_mDice did not improve from 0.57806
Epoch 54/300
 - 13s - loss: 0.3119 - acc: 0.9511 - mDice: 0.7240 - val_loss: 0.9846 - val_acc: 0.9485 - val_mDice: 0.5780

Epoch 00054: val_mDice did not improve from 0.57806
Epoch 55/300
 - 13s - loss: 0.3234 - acc: 0.9503 - mDice: 0.7176 - val_loss: 1.0125 - val_acc: 0.9467 - val_mDice: 0.5800

Epoch 00055: val_mDice improved from 0.57806 to 0.58001, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd1/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 13s - loss: 0.3040 - acc: 0.9513 - mDice: 0.7266 - val_loss: 1.0028 - val_acc: 0.9473 - val_mDice: 0.5758

Epoch 00056: val_mDice did not improve from 0.58001
Epoch 57/300
 - 13s - loss: 0.3017 - acc: 0.9519 - mDice: 0.7322 - val_loss: 0.9741 - val_acc: 0.9489 - val_mDice: 0.5800

Epoch 00057: val_mDice did not improve from 0.58001
Epoch 58/300
 - 13s - loss: 0.2999 - acc: 0.9516 - mDice: 0.7295 - val_loss: 0.9905 - val_acc: 0.9475 - val_mDice: 0.5776

Epoch 00058: val_mDice did not improve from 0.58001
Epoch 59/300
 - 13s - loss: 0.3024 - acc: 0.9516 - mDice: 0.7300 - val_loss: 0.9801 - val_acc: 0.9468 - val_mDice: 0.5724

Epoch 00059: val_mDice did not improve from 0.58001
Epoch 60/300
 - 12s - loss: 0.3186 - acc: 0.9509 - mDice: 0.7226 - val_loss: 0.9721 - val_acc: 0.9485 - val_mDice: 0.5727

Epoch 00060: val_mDice did not improve from 0.58001
Epoch 61/300
 - 12s - loss: 0.3130 - acc: 0.9515 - mDice: 0.7267 - val_loss: 0.9594 - val_acc: 0.9477 - val_mDice: 0.5731

Epoch 00061: val_mDice did not improve from 0.58001
Epoch 62/300
 - 13s - loss: 0.3035 - acc: 0.9517 - mDice: 0.7309 - val_loss: 2.6617 - val_acc: 0.9452 - val_mDice: 0.4421

Epoch 00062: val_mDice did not improve from 0.58001
Epoch 63/300
 - 12s - loss: 0.3089 - acc: 0.9510 - mDice: 0.7242 - val_loss: 0.9739 - val_acc: 0.9479 - val_mDice: 0.5754

Epoch 00063: val_mDice did not improve from 0.58001
Epoch 64/300
 - 12s - loss: 0.3025 - acc: 0.9521 - mDice: 0.7331 - val_loss: 0.9562 - val_acc: 0.9491 - val_mDice: 0.5782

Epoch 00064: val_mDice did not improve from 0.58001
Epoch 65/300
 - 13s - loss: 0.2971 - acc: 0.9523 - mDice: 0.7344 - val_loss: 0.9901 - val_acc: 0.9483 - val_mDice: 0.5707

Epoch 00065: val_mDice did not improve from 0.58001
Epoch 66/300
 - 13s - loss: 0.2919 - acc: 0.9528 - mDice: 0.7388 - val_loss: 0.9868 - val_acc: 0.9510 - val_mDice: 0.5726

Epoch 00066: val_mDice did not improve from 0.58001
Epoch 67/300
 - 13s - loss: 0.3152 - acc: 0.9506 - mDice: 0.7216 - val_loss: 0.9431 - val_acc: 0.9487 - val_mDice: 0.5696

Epoch 00067: val_mDice did not improve from 0.58001
Epoch 68/300
 - 12s - loss: 0.2879 - acc: 0.9528 - mDice: 0.7400 - val_loss: 0.9503 - val_acc: 0.9486 - val_mDice: 0.5726

Epoch 00068: val_mDice did not improve from 0.58001
Epoch 69/300
 - 13s - loss: 0.2921 - acc: 0.9527 - mDice: 0.7386 - val_loss: 0.9324 - val_acc: 0.9496 - val_mDice: 0.5723

Epoch 00069: val_mDice did not improve from 0.58001
Epoch 70/300
 - 13s - loss: 0.2942 - acc: 0.9515 - mDice: 0.7343 - val_loss: 0.9388 - val_acc: 0.9503 - val_mDice: 0.5705

Epoch 00070: val_mDice did not improve from 0.58001
Epoch 71/300
 - 13s - loss: 0.2963 - acc: 0.9519 - mDice: 0.7338 - val_loss: 0.9442 - val_acc: 0.9482 - val_mDice: 0.5670

Epoch 00071: val_mDice did not improve from 0.58001
Epoch 72/300
 - 13s - loss: 0.3071 - acc: 0.9513 - mDice: 0.7267 - val_loss: 0.9759 - val_acc: 0.9490 - val_mDice: 0.5690

Epoch 00072: val_mDice did not improve from 0.58001
Epoch 73/300
 - 13s - loss: 0.2939 - acc: 0.9526 - mDice: 0.7383 - val_loss: 0.9773 - val_acc: 0.9487 - val_mDice: 0.5619

Epoch 00073: val_mDice did not improve from 0.58001
Epoch 74/300
 - 13s - loss: 0.3159 - acc: 0.9517 - mDice: 0.7282 - val_loss: 1.0588 - val_acc: 0.9484 - val_mDice: 0.5529

Epoch 00074: val_mDice did not improve from 0.58001
Epoch 75/300
 - 12s - loss: 0.2878 - acc: 0.9530 - mDice: 0.7409 - val_loss: 0.9700 - val_acc: 0.9494 - val_mDice: 0.5723

Epoch 00075: val_mDice did not improve from 0.58001
Epoch 76/300
 - 13s - loss: 0.3039 - acc: 0.9515 - mDice: 0.7267 - val_loss: 0.9550 - val_acc: 0.9496 - val_mDice: 0.5703

Epoch 00076: val_mDice did not improve from 0.58001
Epoch 77/300
 - 12s - loss: 0.2997 - acc: 0.9524 - mDice: 0.7349 - val_loss: 0.9528 - val_acc: 0.9491 - val_mDice: 0.5701

Epoch 00077: val_mDice did not improve from 0.58001
Epoch 78/300
 - 13s - loss: 0.2882 - acc: 0.9534 - mDice: 0.7434 - val_loss: 0.9450 - val_acc: 0.9495 - val_mDice: 0.5705

Epoch 00078: val_mDice did not improve from 0.58001
Epoch 79/300
 - 12s - loss: 0.2877 - acc: 0.9529 - mDice: 0.7398 - val_loss: 0.9402 - val_acc: 0.9493 - val_mDice: 0.5745

Epoch 00079: val_mDice did not improve from 0.58001
Epoch 80/300
 - 13s - loss: 0.2987 - acc: 0.9524 - mDice: 0.7361 - val_loss: 0.9282 - val_acc: 0.9484 - val_mDice: 0.5752

Epoch 00080: val_mDice did not improve from 0.58001
Epoch 81/300
 - 13s - loss: 0.2784 - acc: 0.9537 - mDice: 0.7479 - val_loss: 0.9248 - val_acc: 0.9494 - val_mDice: 0.5742

Epoch 00081: val_mDice did not improve from 0.58001
Epoch 82/300
 - 13s - loss: 0.2786 - acc: 0.9534 - mDice: 0.7464 - val_loss: 0.9415 - val_acc: 0.9523 - val_mDice: 0.5720

Epoch 00082: val_mDice did not improve from 0.58001
Epoch 83/300
 - 12s - loss: 0.2862 - acc: 0.9534 - mDice: 0.7439 - val_loss: 1.1801 - val_acc: 0.9488 - val_mDice: 0.5587

Epoch 00083: val_mDice did not improve from 0.58001
Epoch 84/300
 - 13s - loss: 0.2910 - acc: 0.9531 - mDice: 0.7413 - val_loss: 0.9367 - val_acc: 0.9490 - val_mDice: 0.5753

Epoch 00084: val_mDice did not improve from 0.58001
Epoch 85/300
 - 13s - loss: 0.2836 - acc: 0.9527 - mDice: 0.7429 - val_loss: 0.9591 - val_acc: 0.9490 - val_mDice: 0.5709

Epoch 00085: val_mDice did not improve from 0.58001
Epoch 86/300
 - 13s - loss: 0.2883 - acc: 0.9537 - mDice: 0.7462 - val_loss: 0.9257 - val_acc: 0.9497 - val_mDice: 0.5773

Epoch 00086: val_mDice did not improve from 0.58001
Epoch 87/300
 - 14s - loss: 0.2760 - acc: 0.9538 - mDice: 0.7475 - val_loss: 0.9257 - val_acc: 0.9489 - val_mDice: 0.5730

Epoch 00087: val_mDice did not improve from 0.58001
Epoch 88/300
 - 13s - loss: 0.2878 - acc: 0.9535 - mDice: 0.7444 - val_loss: 0.9513 - val_acc: 0.9486 - val_mDice: 0.5599

Epoch 00088: val_mDice did not improve from 0.58001
Epoch 89/300
 - 13s - loss: 0.2924 - acc: 0.9526 - mDice: 0.7391 - val_loss: 0.9339 - val_acc: 0.9471 - val_mDice: 0.5735

Epoch 00089: val_mDice did not improve from 0.58001
Epoch 90/300
 - 13s - loss: 0.3083 - acc: 0.9523 - mDice: 0.7316 - val_loss: 0.9573 - val_acc: 0.9495 - val_mDice: 0.5664

Epoch 00090: val_mDice did not improve from 0.58001
Epoch 91/300
 - 13s - loss: 0.2935 - acc: 0.9532 - mDice: 0.7410 - val_loss: 0.9262 - val_acc: 0.9488 - val_mDice: 0.5676

Epoch 00091: val_mDice did not improve from 0.58001
Epoch 92/300
 - 13s - loss: 0.2880 - acc: 0.9534 - mDice: 0.7409 - val_loss: 0.9489 - val_acc: 0.9517 - val_mDice: 0.5716

Epoch 00092: val_mDice did not improve from 0.58001
Epoch 93/300
 - 13s - loss: 0.2871 - acc: 0.9533 - mDice: 0.7426 - val_loss: 1.1199 - val_acc: 0.9513 - val_mDice: 0.5284

Epoch 00093: val_mDice did not improve from 0.58001
Epoch 94/300
 - 13s - loss: 0.2899 - acc: 0.9532 - mDice: 0.7423 - val_loss: 1.0732 - val_acc: 0.9511 - val_mDice: 0.5505

Epoch 00094: val_mDice did not improve from 0.58001
Epoch 95/300
 - 14s - loss: 0.2896 - acc: 0.9528 - mDice: 0.7412 - val_loss: 1.1335 - val_acc: 0.9482 - val_mDice: 0.5588

Epoch 00095: val_mDice did not improve from 0.58001
Restoring model weights from the end of the best epoch
Epoch 00095: early stopping
{'val_loss': [1.8823983034277274, 1.5057124744832906, 1.4146624156852174, 1.409870181013556, 1.352907501209795, 1.3072509814321605, 1.3255536875693628, 1.2754368813209285, 1.2696863566738328, 1.2657659129769194, 1.2912704332981235, 1.2752883706996643, 1.2107360129262887, 1.1941721488058177, 1.1901403762935814, 1.1945392655001745, 1.1366889977377224, 1.188687908688402, 1.1569143874582901, 1.150913253523945, 1.1328788933411142, 1.1025393899359734, 1.1146432061990101, 1.1565467686045403, 1.1359705958101485, 1.1097252627992942, 1.0746818477032232, 1.0981926553779178, 1.0970395224935867, 1.0866980412427116, 1.1316734306952532, 1.0275796213570763, 1.0708681333298777, 1.0938527147754344, 1.1037822120329912, 1.0377920603050905, 3.1559498138677062, 1.0985752784348781, 1.073244575386733, 1.0757568492219338, 1.0849210111533893, 1.0231881225420758, 1.029556234872419, 1.0139893770607469, 0.9967397101759131, 1.0335312370770897, 1.0380236740205802, 1.015549007584067, 1.0351914469323127, 1.0006162717061884, 1.1404571352051753, 1.0222604639000363, 1.013066776437697, 0.9846211576383878, 1.0124567871779397, 1.0028446186212152, 0.9740810035880095, 0.9904862817595986, 0.9801231462971057, 0.9721265421583761, 0.9594251243701948, 2.6617414838348337, 0.9739152076197606, 0.9561512055350285, 0.9901139118702583, 0.9868256093240252, 0.9430531919781678, 0.9502591989885748, 0.9324047521633261, 0.9388450150731099, 0.9441649409047529, 0.9759345162732929, 0.9772767192787595, 1.0587763981102338, 0.9699696273195977, 0.9549563612813264, 0.9527710308046902, 0.9450375554608363, 0.9401565161016252, 0.9281961974365259, 0.9248357878790961, 0.9414782467620825, 1.1801241731721592, 0.9366771216486015, 0.9591271046524733, 0.9256578090728498, 0.9257034819110547, 0.9512607659779343, 0.933915821944966, 0.9572549080537036, 0.926170377949484, 0.9488890659964941, 1.1199031600765152, 1.0731680940568837, 1.133503616050957], 'val_acc': [0.9159814060903063, 0.9287946605994031, 0.9372256522864298, 0.934511188977684, 0.9359675306899875, 0.9389454605532628, 0.9422508820988773, 0.9444915854852963, 0.942322205484303, 0.9447006606588176, 0.9455684178015765, 0.9443054623853148, 0.9445350960968366, 0.9478888648008209, 0.9462162092620251, 0.9457364074544969, 0.9478997580366197, 0.9508812856050878, 0.9425941230424868, 0.9458294748480803, 0.9468011595065298, 0.9479033756100275, 0.9469316816018298, 0.9491892866059846, 0.9471733831112681, 0.948191011653227, 0.9477764738151451, 0.9465207796470791, 0.9448891967729806, 0.948065321819455, 0.9506613319995356, 0.949582074981889, 0.9482308887188731, 0.9473595163401436, 0.948968121039322, 0.9493717746017805, 0.9418798618846469, 0.9488327534370173, 0.9461557798915439, 0.9477305501894234, 0.9446583635666791, 0.9478091013197806, 0.9459950479806638, 0.9487542112668356, 0.9492037946881812, 0.9466766761798485, 0.9491856682534311, 0.9479154601595761, 0.9475879287408069, 0.9507519606671302, 0.9469872705297533, 0.946966738482706, 0.9477571323027019, 0.9484593104692846, 0.946739520901948, 0.9472761177549175, 0.9488895718568291, 0.9474562067611545, 0.9468313747761297, 0.9485294199457356, 0.9477184710938946, 0.9452058337872324, 0.9479396257525176, 0.9490551516121509, 0.9482719941076889, 0.9509501687062332, 0.9487493669285494, 0.9485982913596958, 0.9495699990029428, 0.9503338181115444, 0.9481644299295213, 0.9490466858047286, 0.9486623663528293, 0.9484000969556422, 0.9493983839851579, 0.9495929635428135, 0.9491155599456986, 0.9494612244219561, 0.9493149846207862, 0.9483723009333891, 0.949433431126713, 0.9522832308719361, 0.9487868368236068, 0.9490382172702964, 0.9489971270748213, 0.9496848045610914, 0.9489173620354896, 0.9486019307491826, 0.9470972625258701, 0.9494986900317124, 0.9488025444005829, 0.9516704838260327, 0.9513175912152708, 0.9511266265040129, 0.9482260603530734], 'val_mDice': [0.392952552369607, 0.4618114004142923, 0.4944832611804694, 0.5039363988474304, 0.5160411541271054, 0.5357619294154099, 0.5285897690291498, 0.542362904577863, 0.5472806169608839, 0.5455475829395593, 0.532259798906987, 0.5446108369266286, 0.5569886285689921, 0.5628683047941307, 0.5630296640146791, 0.5519088281233326, 0.5589889662425502, 0.5494358575129821, 0.5584532637027354, 0.5586962351709409, 0.5592775023938005, 0.5709593814669871, 0.5669920309111963, 0.5575272005562689, 0.5655099715477501, 0.5582408850976065, 0.5712427988745807, 0.5722737826553046, 0.5666445378774132, 0.5684453933651931, 0.5567502951115565, 0.5687392650573861, 0.574743765680229, 0.5724883066867691, 0.5604824113105636, 0.5714540372486987, 0.4002954026598946, 0.5606840510481323, 0.5667899244166668, 0.5605410771821838, 0.5570261304090226, 0.573730492075674, 0.5688606895068112, 0.5689738809672835, 0.5671389349340613, 0.5710465850588543, 0.5666286748608732, 0.5753592850725635, 0.5628181055577752, 0.5762772537719191, 0.5567339337826555, 0.578060604485811, 0.5766317338920107, 0.577992710704897, 0.5800113990610721, 0.5758089898458494, 0.5799765749499689, 0.5775558631988912, 0.572445375905707, 0.5726720288493274, 0.5730744837935454, 0.4420580416187352, 0.5753791994518704, 0.5782480188249762, 0.5706647264898992, 0.5725769101112497, 0.5695909010429009, 0.5726420913451638, 0.5722593813160666, 0.5704835984928935, 0.5670138800066281, 0.5690130275838515, 0.5619159279013771, 0.5528754405920802, 0.5722574720195696, 0.5703370546788172, 0.5700701251626015, 0.5705238444840207, 0.5744541444225248, 0.5752410766443395, 0.5741667235209271, 0.5720488202143339, 0.558711914260403, 0.5752851859806409, 0.5708650545357099, 0.5773316192276338, 0.5729544735422322, 0.5599316017690048, 0.573528622197949, 0.5663824031750361, 0.5675519108772278, 0.5716016495636865, 0.5284300340351715, 0.5505141602719531, 0.5587728196888967], 'loss': [1.7320920441753342, 0.7767096622148072, 0.6462347077631185, 0.5876449678873092, 0.5516447706795283, 0.5000253690686924, 0.5186200136091305, 0.489299859330559, 0.4938095666437559, 0.45686037046372185, 0.45168280541928735, 0.46288593714380044, 0.4319454689398634, 0.4113079664275849, 0.41403813970685394, 0.3989054456409037, 0.38393223011214966, 0.3983330755275182, 0.3952241784203335, 0.3861227565010566, 0.3845916978877023, 0.3563358517701313, 0.37291999074987087, 0.37725501676531703, 0.3615943652287536, 0.3543003809268374, 0.3472118591970078, 0.3434526631213559, 0.36153494096524275, 0.3355645119764469, 0.35094288367222004, 0.3534511372316699, 0.3403280816169531, 0.3413748843097284, 0.3442838578393255, 0.3334309298311751, 0.3322969269012099, 0.3579893202711892, 0.3370769283994062, 0.32776429042689076, 0.3471943006735922, 0.32286459961768116, 0.3139686984815591, 0.31652463382248847, 0.3116332128660833, 0.3641319771306307, 0.32622712032154827, 0.3231845131251748, 0.3369434454658227, 0.3331032328999541, 0.33476721246150754, 0.3228127166838938, 0.31115485241187735, 0.311890917593751, 0.32342854546964456, 0.3040225990211895, 0.30172736798357414, 0.2999196556614584, 0.3024101108037792, 0.31856863420959514, 0.3130300185966764, 0.3035308171161841, 0.3089493133331141, 0.30252513439447226, 0.2970604585136052, 0.2919170115008091, 0.3151620179707294, 0.28794532747993196, 0.29212066559372, 0.2941705953699693, 0.29634584847910156, 0.3071439870477992, 0.29390978618318914, 0.31589818251849966, 0.2877762824876722, 0.3039105607707423, 0.2997294887904785, 0.2881901897931058, 0.2877289544547418, 0.29867722281376224, 0.2783797868879357, 0.2786325174805183, 0.28615170686389213, 0.29100888007790254, 0.28362090855764177, 0.28832359837247457, 0.2759672302274689, 0.2878318804088669, 0.2923593958126697, 0.308314298404746, 0.29351241103216613, 0.28801093577232273, 0.2871271769347569, 0.2898566336427242, 0.28959393662092653], 'acc': [0.7945643466780901, 0.8989432402869552, 0.9138377623729954, 0.9245876956754795, 0.9290342347669718, 0.9337423053770163, 0.9329354022839825, 0.9363552408801629, 0.9368888007210865, 0.9390556139469118, 0.9389839981524082, 0.9396157260631577, 0.941355644571511, 0.942785805935763, 0.943370627873969, 0.9440226167328716, 0.944717181126773, 0.9436668341437401, 0.9448955247804874, 0.9450373493396049, 0.945139828496476, 0.947032043996136, 0.9465688682835066, 0.94605808360465, 0.9469589539148386, 0.9474569347685016, 0.9480517223593495, 0.9486270386917813, 0.9466482504295405, 0.9486035625501564, 0.9481039102523287, 0.9475625416001202, 0.9484875476532656, 0.9489463229906737, 0.9486300545541867, 0.9497508735601137, 0.9488261748703453, 0.9464789048857618, 0.9492385482701564, 0.9499432961224049, 0.9481928171773365, 0.9504202908043831, 0.9504811880745831, 0.950490101637339, 0.9513077827316653, 0.947388752689466, 0.950041974606424, 0.9505668406547786, 0.9485875449321591, 0.9501116977646319, 0.9493851650228562, 0.950046374609316, 0.950817567828835, 0.951092697050139, 0.9503466498065274, 0.951346067842137, 0.9519148917149672, 0.951649511354755, 0.9516078395416284, 0.9509211763180205, 0.9514896483323173, 0.9517260972454544, 0.9510325235744342, 0.9520792209334018, 0.9523037676014362, 0.952804873300142, 0.9506131316629864, 0.9528102192151652, 0.9526790466751985, 0.9514580549167717, 0.9518572926322726, 0.9513171340050608, 0.9525668593262625, 0.9516892000519547, 0.9529812373236435, 0.9515435288142347, 0.9524185284249694, 0.9533547774055654, 0.952861439682456, 0.9523794312885326, 0.9536674825842628, 0.953444587779404, 0.9533606742371469, 0.9530879430806618, 0.952718540973812, 0.9536884534088773, 0.9537671516457775, 0.953480054507527, 0.9526209830528594, 0.9522837465824565, 0.9532360569563688, 0.9533774612085945, 0.9533379030997852, 0.9532381712244994, 0.952750265722407], 'mDice': [0.24730187611189763, 0.45603872459671924, 0.518353401375268, 0.5494890177144947, 0.5715280150174314, 0.5993687037170887, 0.5893755441007286, 0.6054613825144975, 0.6066712107385487, 0.6243007379523123, 0.6289376157945239, 0.6287050489773961, 0.6403151555144743, 0.654133575010416, 0.6567722697304519, 0.6629738891611945, 0.6699563840078254, 0.6629812475317145, 0.6703504516400547, 0.67113741365504, 0.671080031842605, 0.6880189144311367, 0.6834327848359444, 0.6804011746912129, 0.6878911945351641, 0.6926270070205914, 0.695814189245518, 0.7017720195126425, 0.6909455928734974, 0.7030207456466706, 0.6979889598243957, 0.6941830175268606, 0.7013828848252099, 0.7042016104742237, 0.7023340904405594, 0.7115725095916443, 0.707832143821469, 0.689644382702644, 0.706558736885911, 0.7138039792510938, 0.6993963787064786, 0.7171623650507213, 0.7185958104610471, 0.7190778617650666, 0.7255642059067909, 0.6924297782934941, 0.7147428050714371, 0.7190961849168932, 0.7070084385659411, 0.7143806711072421, 0.7080535064369672, 0.7127310462848742, 0.7225850674800951, 0.723984066394957, 0.7176445500423314, 0.7266014671827796, 0.7321877307607882, 0.7294600091334278, 0.7299785723407645, 0.722647364986057, 0.7266827592305634, 0.7309354414930802, 0.7242088551729522, 0.7331038812955845, 0.7343912199773441, 0.7388480948738045, 0.7215603566927163, 0.739972434287707, 0.738577165405913, 0.7343480862245816, 0.7337698245986084, 0.7267424482133048, 0.7382915962621099, 0.7282014251060266, 0.740871502100815, 0.726696702292038, 0.7349025790224297, 0.7434498501445234, 0.7398407313908661, 0.7361049517139006, 0.747942637532703, 0.7463950097146002, 0.7439036812431375, 0.7413463366791199, 0.7428925933350813, 0.7461824614661688, 0.7474871382486942, 0.7444155731787112, 0.7390982150407399, 0.7316394188110616, 0.7409659924871103, 0.7409031016119274, 0.7426227109714056, 0.7423174672506475, 0.741243113426927]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:03<00:13,  3.28s/it]predicting test subjects:  40%|████      | 2/5 [00:05<00:09,  3.09s/it]predicting test subjects:  60%|██████    | 3/5 [00:08<00:05,  2.84s/it]predicting test subjects:  80%|████████  | 4/5 [00:10<00:02,  2.68s/it]predicting test subjects: 100%|██████████| 5/5 [00:13<00:00,  2.70s/it]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:03<16:01,  3.63s/it]predicting train subjects:   1%|          | 2/266 [00:06<14:50,  3.37s/it]predicting train subjects:   1%|          | 3/266 [00:08<13:41,  3.13s/it]predicting train subjects:   2%|▏         | 4/266 [00:11<12:35,  2.88s/it]predicting train subjects:   2%|▏         | 5/266 [00:14<12:40,  2.91s/it]predicting train subjects:   2%|▏         | 6/266 [00:17<12:46,  2.95s/it]predicting train subjects:   3%|▎         | 7/266 [00:20<13:26,  3.11s/it]predicting train subjects:   3%|▎         | 8/266 [00:24<13:35,  3.16s/it]predicting train subjects:   3%|▎         | 9/266 [00:26<13:14,  3.09s/it]predicting train subjects:   4%|▍         | 10/266 [00:29<12:59,  3.04s/it]predicting train subjects:   4%|▍         | 11/266 [00:33<13:07,  3.09s/it]predicting train subjects:   5%|▍         | 12/266 [00:36<13:01,  3.08s/it]predicting train subjects:   5%|▍         | 13/266 [00:38<12:40,  3.01s/it]predicting train subjects:   5%|▌         | 14/266 [00:41<12:26,  2.96s/it]predicting train subjects:   6%|▌         | 15/266 [00:44<12:16,  2.93s/it]predicting train subjects:   6%|▌         | 16/266 [00:47<12:08,  2.91s/it]predicting train subjects:   6%|▋         | 17/266 [00:50<12:07,  2.92s/it]predicting train subjects:   7%|▋         | 18/266 [00:53<12:00,  2.90s/it]predicting train subjects:   7%|▋         | 19/266 [00:56<11:50,  2.88s/it]predicting train subjects:   8%|▊         | 20/266 [00:59<11:54,  2.90s/it]predicting train subjects:   8%|▊         | 21/266 [01:01<11:45,  2.88s/it]predicting train subjects:   8%|▊         | 22/266 [01:04<11:34,  2.85s/it]predicting train subjects:   9%|▊         | 23/266 [01:07<11:33,  2.85s/it]predicting train subjects:   9%|▉         | 24/266 [01:10<11:18,  2.80s/it]predicting train subjects:   9%|▉         | 25/266 [01:13<11:09,  2.78s/it]predicting train subjects:  10%|▉         | 26/266 [01:15<11:04,  2.77s/it]predicting train subjects:  10%|█         | 27/266 [01:18<10:56,  2.75s/it]predicting train subjects:  11%|█         | 28/266 [01:21<10:46,  2.72s/it]predicting train subjects:  11%|█         | 29/266 [01:23<10:45,  2.72s/it]predicting train subjects:  11%|█▏        | 30/266 [01:26<10:35,  2.69s/it]predicting train subjects:  12%|█▏        | 31/266 [01:29<10:34,  2.70s/it]predicting train subjects:  12%|█▏        | 32/266 [01:31<10:32,  2.70s/it]predicting train subjects:  12%|█▏        | 33/266 [01:34<10:29,  2.70s/it]predicting train subjects:  13%|█▎        | 34/266 [01:37<10:16,  2.66s/it]predicting train subjects:  13%|█▎        | 35/266 [01:39<10:10,  2.64s/it]predicting train subjects:  14%|█▎        | 36/266 [01:42<10:10,  2.65s/it]predicting train subjects:  14%|█▍        | 37/266 [01:45<10:10,  2.67s/it]predicting train subjects:  14%|█▍        | 38/266 [01:47<10:05,  2.66s/it]predicting train subjects:  15%|█▍        | 39/266 [01:50<10:10,  2.69s/it]predicting train subjects:  15%|█▌        | 40/266 [01:53<10:15,  2.72s/it]predicting train subjects:  15%|█▌        | 41/266 [01:56<10:25,  2.78s/it]predicting train subjects:  16%|█▌        | 42/266 [01:58<09:57,  2.67s/it]predicting train subjects:  16%|█▌        | 43/266 [02:00<09:30,  2.56s/it]predicting train subjects:  17%|█▋        | 44/266 [02:03<09:14,  2.50s/it]predicting train subjects:  17%|█▋        | 45/266 [02:05<09:15,  2.51s/it]predicting train subjects:  17%|█▋        | 46/266 [02:08<08:51,  2.42s/it]predicting train subjects:  18%|█▊        | 47/266 [02:10<08:36,  2.36s/it]predicting train subjects:  18%|█▊        | 48/266 [02:12<08:27,  2.33s/it]predicting train subjects:  18%|█▊        | 49/266 [02:14<08:24,  2.32s/it]predicting train subjects:  19%|█▉        | 50/266 [02:17<08:17,  2.30s/it]predicting train subjects:  19%|█▉        | 51/266 [02:19<08:12,  2.29s/it]predicting train subjects:  20%|█▉        | 52/266 [02:21<08:14,  2.31s/it]predicting train subjects:  20%|█▉        | 53/266 [02:24<08:21,  2.35s/it]predicting train subjects:  20%|██        | 54/266 [02:26<08:31,  2.41s/it]predicting train subjects:  21%|██        | 55/266 [02:28<08:18,  2.36s/it]predicting train subjects:  21%|██        | 56/266 [02:31<08:10,  2.33s/it]predicting train subjects:  21%|██▏       | 57/266 [02:33<08:17,  2.38s/it]predicting train subjects:  22%|██▏       | 58/266 [02:35<08:07,  2.34s/it]predicting train subjects:  22%|██▏       | 59/266 [02:38<07:58,  2.31s/it]predicting train subjects:  23%|██▎       | 60/266 [02:40<07:46,  2.27s/it]predicting train subjects:  23%|██▎       | 61/266 [02:42<07:32,  2.21s/it]predicting train subjects:  23%|██▎       | 62/266 [02:44<07:31,  2.21s/it]predicting train subjects:  24%|██▎       | 63/266 [02:46<07:26,  2.20s/it]predicting train subjects:  24%|██▍       | 64/266 [02:48<07:18,  2.17s/it]predicting train subjects:  24%|██▍       | 65/266 [02:51<07:13,  2.16s/it]predicting train subjects:  25%|██▍       | 66/266 [02:53<07:11,  2.16s/it]predicting train subjects:  25%|██▌       | 67/266 [02:55<07:04,  2.13s/it]predicting train subjects:  26%|██▌       | 68/266 [02:57<07:07,  2.16s/it]predicting train subjects:  26%|██▌       | 69/266 [02:59<07:09,  2.18s/it]predicting train subjects:  26%|██▋       | 70/266 [03:01<07:07,  2.18s/it]predicting train subjects:  27%|██▋       | 71/266 [03:03<06:57,  2.14s/it]predicting train subjects:  27%|██▋       | 72/266 [03:06<06:53,  2.13s/it]predicting train subjects:  27%|██▋       | 73/266 [03:08<06:48,  2.12s/it]predicting train subjects:  28%|██▊       | 74/266 [03:10<06:44,  2.11s/it]predicting train subjects:  28%|██▊       | 75/266 [03:12<06:50,  2.15s/it]predicting train subjects:  29%|██▊       | 76/266 [03:14<06:46,  2.14s/it]predicting train subjects:  29%|██▉       | 77/266 [03:16<06:40,  2.12s/it]predicting train subjects:  29%|██▉       | 78/266 [03:19<07:10,  2.29s/it]predicting train subjects:  30%|██▉       | 79/266 [03:21<07:24,  2.37s/it]predicting train subjects:  30%|███       | 80/266 [03:24<07:36,  2.46s/it]predicting train subjects:  30%|███       | 81/266 [03:27<07:43,  2.50s/it]predicting train subjects:  31%|███       | 82/266 [03:29<07:46,  2.54s/it]predicting train subjects:  31%|███       | 83/266 [03:32<07:44,  2.54s/it]predicting train subjects:  32%|███▏      | 84/266 [03:34<07:45,  2.56s/it]predicting train subjects:  32%|███▏      | 85/266 [03:37<07:43,  2.56s/it]predicting train subjects:  32%|███▏      | 86/266 [03:40<07:39,  2.56s/it]predicting train subjects:  33%|███▎      | 87/266 [03:42<07:35,  2.54s/it]predicting train subjects:  33%|███▎      | 88/266 [03:45<07:32,  2.54s/it]predicting train subjects:  33%|███▎      | 89/266 [03:47<07:35,  2.57s/it]predicting train subjects:  34%|███▍      | 90/266 [03:50<07:37,  2.60s/it]predicting train subjects:  34%|███▍      | 91/266 [03:53<07:37,  2.61s/it]predicting train subjects:  35%|███▍      | 92/266 [03:55<07:35,  2.62s/it]predicting train subjects:  35%|███▍      | 93/266 [03:58<07:34,  2.63s/it]predicting train subjects:  35%|███▌      | 94/266 [04:00<07:29,  2.61s/it]predicting train subjects:  36%|███▌      | 95/266 [04:03<07:27,  2.61s/it]predicting train subjects:  36%|███▌      | 96/266 [04:05<07:07,  2.52s/it]predicting train subjects:  36%|███▋      | 97/266 [04:08<07:15,  2.58s/it]predicting train subjects:  37%|███▋      | 98/266 [04:11<07:11,  2.57s/it]predicting train subjects:  37%|███▋      | 99/266 [04:12<06:31,  2.34s/it]predicting train subjects:  38%|███▊      | 100/266 [04:15<06:17,  2.28s/it]predicting train subjects:  38%|███▊      | 101/266 [04:17<06:18,  2.29s/it]predicting train subjects:  38%|███▊      | 102/266 [04:19<06:16,  2.29s/it]predicting train subjects:  39%|███▊      | 103/266 [04:21<06:13,  2.29s/it]predicting train subjects:  39%|███▉      | 104/266 [04:24<06:10,  2.29s/it]predicting train subjects:  39%|███▉      | 105/266 [04:26<06:10,  2.30s/it]predicting train subjects:  40%|███▉      | 106/266 [04:28<06:10,  2.31s/it]predicting train subjects:  40%|████      | 107/266 [04:31<06:07,  2.31s/it]predicting train subjects:  41%|████      | 108/266 [04:33<06:01,  2.29s/it]predicting train subjects:  41%|████      | 109/266 [04:35<06:03,  2.31s/it]predicting train subjects:  41%|████▏     | 110/266 [04:38<06:00,  2.31s/it]predicting train subjects:  42%|████▏     | 111/266 [04:40<05:57,  2.31s/it]predicting train subjects:  42%|████▏     | 112/266 [04:42<05:56,  2.32s/it]predicting train subjects:  42%|████▏     | 113/266 [04:45<05:56,  2.33s/it]predicting train subjects:  43%|████▎     | 114/266 [04:47<05:56,  2.34s/it]predicting train subjects:  43%|████▎     | 115/266 [04:49<05:55,  2.36s/it]predicting train subjects:  44%|████▎     | 116/266 [04:52<05:49,  2.33s/it]predicting train subjects:  44%|████▍     | 117/266 [04:54<05:43,  2.31s/it]predicting train subjects:  44%|████▍     | 118/266 [04:56<05:42,  2.32s/it]predicting train subjects:  45%|████▍     | 119/266 [04:59<05:55,  2.42s/it]predicting train subjects:  45%|████▌     | 120/266 [05:02<06:02,  2.48s/it]predicting train subjects:  45%|████▌     | 121/266 [05:04<06:04,  2.51s/it]predicting train subjects:  46%|████▌     | 122/266 [05:07<06:05,  2.53s/it]predicting train subjects:  46%|████▌     | 123/266 [05:09<06:04,  2.55s/it]predicting train subjects:  47%|████▋     | 124/266 [05:12<06:02,  2.55s/it]predicting train subjects:  47%|████▋     | 125/266 [05:15<06:02,  2.57s/it]predicting train subjects:  47%|████▋     | 126/266 [05:17<06:02,  2.59s/it]predicting train subjects:  48%|████▊     | 127/266 [05:20<06:03,  2.61s/it]predicting train subjects:  48%|████▊     | 128/266 [05:22<05:59,  2.61s/it]predicting train subjects:  48%|████▊     | 129/266 [05:25<06:01,  2.64s/it]predicting train subjects:  49%|████▉     | 130/266 [05:28<05:58,  2.64s/it]predicting train subjects:  49%|████▉     | 131/266 [05:30<05:58,  2.65s/it]predicting train subjects:  50%|████▉     | 132/266 [05:33<05:55,  2.65s/it]predicting train subjects:  50%|█████     | 133/266 [05:36<05:52,  2.65s/it]predicting train subjects:  50%|█████     | 134/266 [05:38<05:49,  2.65s/it]predicting train subjects:  51%|█████     | 135/266 [05:41<05:48,  2.66s/it]predicting train subjects:  51%|█████     | 136/266 [05:44<05:48,  2.68s/it]predicting train subjects:  52%|█████▏    | 137/266 [05:46<05:40,  2.64s/it]predicting train subjects:  52%|█████▏    | 138/266 [05:49<05:33,  2.60s/it]predicting train subjects:  52%|█████▏    | 139/266 [05:51<05:27,  2.58s/it]predicting train subjects:  53%|█████▎    | 140/266 [05:54<05:27,  2.60s/it]predicting train subjects:  53%|█████▎    | 141/266 [05:57<05:21,  2.57s/it]predicting train subjects:  53%|█████▎    | 142/266 [05:59<05:26,  2.63s/it]predicting train subjects:  54%|█████▍    | 143/266 [06:02<05:18,  2.59s/it]predicting train subjects:  54%|█████▍    | 144/266 [06:04<05:13,  2.57s/it]predicting train subjects:  55%|█████▍    | 145/266 [06:07<05:08,  2.55s/it]predicting train subjects:  55%|█████▍    | 146/266 [06:09<05:03,  2.53s/it]predicting train subjects:  55%|█████▌    | 147/266 [06:12<04:59,  2.52s/it]predicting train subjects:  56%|█████▌    | 148/266 [06:14<04:54,  2.50s/it]predicting train subjects:  56%|█████▌    | 149/266 [06:17<04:51,  2.50s/it]predicting train subjects:  56%|█████▋    | 150/266 [06:19<04:49,  2.50s/it]predicting train subjects:  57%|█████▋    | 151/266 [06:22<04:44,  2.47s/it]predicting train subjects:  57%|█████▋    | 152/266 [06:24<04:42,  2.48s/it]predicting train subjects:  58%|█████▊    | 153/266 [06:27<04:39,  2.48s/it]predicting train subjects:  58%|█████▊    | 154/266 [06:29<04:38,  2.48s/it]predicting train subjects:  58%|█████▊    | 155/266 [06:31<04:14,  2.30s/it]predicting train subjects:  59%|█████▊    | 156/266 [06:33<03:59,  2.18s/it]predicting train subjects:  59%|█████▉    | 157/266 [06:35<03:47,  2.08s/it]predicting train subjects:  59%|█████▉    | 158/266 [06:37<03:37,  2.02s/it]predicting train subjects:  60%|█████▉    | 159/266 [06:38<03:32,  1.99s/it]predicting train subjects:  60%|██████    | 160/266 [06:40<03:26,  1.95s/it]predicting train subjects:  61%|██████    | 161/266 [06:42<03:23,  1.94s/it]predicting train subjects:  61%|██████    | 162/266 [06:44<03:21,  1.94s/it]predicting train subjects:  61%|██████▏   | 163/266 [06:46<03:21,  1.96s/it]predicting train subjects:  62%|██████▏   | 164/266 [06:48<03:15,  1.92s/it]predicting train subjects:  62%|██████▏   | 165/266 [06:50<03:13,  1.92s/it]predicting train subjects:  62%|██████▏   | 166/266 [06:52<03:10,  1.91s/it]predicting train subjects:  63%|██████▎   | 167/266 [06:54<03:09,  1.91s/it]predicting train subjects:  63%|██████▎   | 168/266 [06:56<03:07,  1.91s/it]predicting train subjects:  64%|██████▎   | 169/266 [06:58<03:03,  1.89s/it]predicting train subjects:  64%|██████▍   | 170/266 [06:59<03:01,  1.89s/it]predicting train subjects:  64%|██████▍   | 171/266 [07:01<03:01,  1.91s/it]predicting train subjects:  65%|██████▍   | 172/266 [07:03<02:59,  1.91s/it]predicting train subjects:  65%|██████▌   | 173/266 [07:05<03:01,  1.95s/it]predicting train subjects:  65%|██████▌   | 174/266 [07:07<03:00,  1.96s/it]predicting train subjects:  66%|██████▌   | 175/266 [07:09<03:01,  2.00s/it]predicting train subjects:  66%|██████▌   | 176/266 [07:11<03:00,  2.00s/it]predicting train subjects:  67%|██████▋   | 177/266 [07:14<03:01,  2.04s/it]predicting train subjects:  67%|██████▋   | 178/266 [07:16<03:00,  2.05s/it]predicting train subjects:  67%|██████▋   | 179/266 [07:18<02:58,  2.05s/it]predicting train subjects:  68%|██████▊   | 180/266 [07:20<02:55,  2.04s/it]predicting train subjects:  68%|██████▊   | 181/266 [07:22<02:54,  2.05s/it]predicting train subjects:  68%|██████▊   | 182/266 [07:24<02:54,  2.07s/it]predicting train subjects:  69%|██████▉   | 183/266 [07:26<02:51,  2.07s/it]predicting train subjects:  69%|██████▉   | 184/266 [07:28<02:48,  2.06s/it]predicting train subjects:  70%|██████▉   | 185/266 [07:30<02:47,  2.07s/it]predicting train subjects:  70%|██████▉   | 186/266 [07:32<02:46,  2.08s/it]predicting train subjects:  70%|███████   | 187/266 [07:34<02:45,  2.10s/it]predicting train subjects:  71%|███████   | 188/266 [07:36<02:43,  2.09s/it]predicting train subjects:  71%|███████   | 189/266 [07:38<02:40,  2.08s/it]predicting train subjects:  71%|███████▏  | 190/266 [07:40<02:36,  2.06s/it]predicting train subjects:  72%|███████▏  | 191/266 [07:43<02:39,  2.13s/it]predicting train subjects:  72%|███████▏  | 192/266 [07:45<02:34,  2.09s/it]predicting train subjects:  73%|███████▎  | 193/266 [07:47<02:29,  2.05s/it]predicting train subjects:  73%|███████▎  | 194/266 [07:49<02:39,  2.21s/it]predicting train subjects:  73%|███████▎  | 195/266 [07:51<02:36,  2.20s/it]predicting train subjects:  74%|███████▎  | 196/266 [07:54<02:34,  2.21s/it]predicting train subjects:  74%|███████▍  | 197/266 [07:56<02:31,  2.20s/it]predicting train subjects:  74%|███████▍  | 198/266 [07:58<02:29,  2.20s/it]predicting train subjects:  75%|███████▍  | 199/266 [08:00<02:27,  2.21s/it]predicting train subjects:  75%|███████▌  | 200/266 [08:03<02:25,  2.21s/it]predicting train subjects:  76%|███████▌  | 201/266 [08:05<02:22,  2.20s/it]predicting train subjects:  76%|███████▌  | 202/266 [08:07<02:21,  2.21s/it]predicting train subjects:  76%|███████▋  | 203/266 [08:09<02:19,  2.21s/it]predicting train subjects:  77%|███████▋  | 204/266 [08:11<02:16,  2.20s/it]predicting train subjects:  77%|███████▋  | 205/266 [08:14<02:14,  2.21s/it]predicting train subjects:  77%|███████▋  | 206/266 [08:16<02:13,  2.23s/it]predicting train subjects:  78%|███████▊  | 207/266 [08:18<02:11,  2.23s/it]predicting train subjects:  78%|███████▊  | 208/266 [08:20<02:09,  2.23s/it]predicting train subjects:  79%|███████▊  | 209/266 [08:23<02:07,  2.24s/it]predicting train subjects:  79%|███████▉  | 210/266 [08:25<02:05,  2.24s/it]predicting train subjects:  79%|███████▉  | 211/266 [08:27<02:03,  2.24s/it]predicting train subjects:  80%|███████▉  | 212/266 [08:29<02:01,  2.25s/it]predicting train subjects:  80%|████████  | 213/266 [08:31<01:55,  2.17s/it]predicting train subjects:  80%|████████  | 214/266 [08:33<01:49,  2.11s/it]predicting train subjects:  81%|████████  | 215/266 [08:35<01:47,  2.11s/it]predicting train subjects:  81%|████████  | 216/266 [08:37<01:43,  2.07s/it]predicting train subjects:  82%|████████▏ | 217/266 [08:39<01:40,  2.04s/it]predicting train subjects:  82%|████████▏ | 218/266 [08:41<01:38,  2.05s/it]predicting train subjects:  82%|████████▏ | 219/266 [08:43<01:36,  2.05s/it]predicting train subjects:  83%|████████▎ | 220/266 [08:45<01:34,  2.05s/it]predicting train subjects:  83%|████████▎ | 221/266 [08:47<01:31,  2.03s/it]predicting train subjects:  83%|████████▎ | 222/266 [08:49<01:29,  2.03s/it]predicting train subjects:  84%|████████▍ | 223/266 [08:51<01:26,  2.02s/it]predicting train subjects:  84%|████████▍ | 224/266 [08:54<01:24,  2.02s/it]predicting train subjects:  85%|████████▍ | 225/266 [08:56<01:22,  2.02s/it]predicting train subjects:  85%|████████▍ | 226/266 [08:58<01:20,  2.01s/it]predicting train subjects:  85%|████████▌ | 227/266 [08:59<01:17,  1.99s/it]predicting train subjects:  86%|████████▌ | 228/266 [09:01<01:15,  1.98s/it]predicting train subjects:  86%|████████▌ | 229/266 [09:03<01:13,  1.99s/it]predicting train subjects:  86%|████████▋ | 230/266 [09:06<01:12,  2.02s/it]predicting train subjects:  87%|████████▋ | 231/266 [09:08<01:11,  2.04s/it]predicting train subjects:  87%|████████▋ | 232/266 [09:10<01:08,  2.03s/it]predicting train subjects:  88%|████████▊ | 233/266 [09:12<01:06,  2.02s/it]predicting train subjects:  88%|████████▊ | 234/266 [09:14<01:04,  2.01s/it]predicting train subjects:  88%|████████▊ | 235/266 [09:16<01:02,  2.00s/it]predicting train subjects:  89%|████████▊ | 236/266 [09:18<01:00,  2.01s/it]predicting train subjects:  89%|████████▉ | 237/266 [09:20<00:58,  2.02s/it]predicting train subjects:  89%|████████▉ | 238/266 [09:22<00:56,  2.02s/it]predicting train subjects:  90%|████████▉ | 239/266 [09:24<00:54,  2.02s/it]predicting train subjects:  90%|█████████ | 240/266 [09:26<00:52,  2.02s/it]predicting train subjects:  91%|█████████ | 241/266 [09:28<00:50,  2.02s/it]predicting train subjects:  91%|█████████ | 242/266 [09:30<00:48,  2.03s/it]predicting train subjects:  91%|█████████▏| 243/266 [09:32<00:47,  2.06s/it]predicting train subjects:  92%|█████████▏| 244/266 [09:34<00:44,  2.03s/it]predicting train subjects:  92%|█████████▏| 245/266 [09:36<00:42,  2.03s/it]predicting train subjects:  92%|█████████▏| 246/266 [09:38<00:40,  2.02s/it]predicting train subjects:  93%|█████████▎| 247/266 [09:40<00:38,  2.02s/it]predicting train subjects:  93%|█████████▎| 248/266 [09:42<00:36,  2.02s/it]predicting train subjects:  94%|█████████▎| 249/266 [09:45<00:37,  2.20s/it]predicting train subjects:  94%|█████████▍| 250/266 [09:47<00:37,  2.31s/it]predicting train subjects:  94%|█████████▍| 251/266 [09:50<00:35,  2.39s/it]predicting train subjects:  95%|█████████▍| 252/266 [09:52<00:34,  2.45s/it]predicting train subjects:  95%|█████████▌| 253/266 [09:55<00:32,  2.48s/it]predicting train subjects:  95%|█████████▌| 254/266 [09:58<00:30,  2.53s/it]predicting train subjects:  96%|█████████▌| 255/266 [10:00<00:28,  2.56s/it]predicting train subjects:  96%|█████████▌| 256/266 [10:03<00:25,  2.56s/it]predicting train subjects:  97%|█████████▋| 257/266 [10:05<00:23,  2.59s/it]predicting train subjects:  97%|█████████▋| 258/266 [10:08<00:20,  2.59s/it]predicting train subjects:  97%|█████████▋| 259/266 [10:11<00:18,  2.60s/it]predicting train subjects:  98%|█████████▊| 260/266 [10:13<00:15,  2.59s/it]predicting train subjects:  98%|█████████▊| 261/266 [10:16<00:13,  2.61s/it]predicting train subjects:  98%|█████████▊| 262/266 [10:18<00:10,  2.62s/it]predicting train subjects:  99%|█████████▉| 263/266 [10:21<00:07,  2.62s/it]predicting train subjects:  99%|█████████▉| 264/266 [10:24<00:05,  2.62s/it]predicting train subjects: 100%|█████████▉| 265/266 [10:26<00:02,  2.63s/it]predicting train subjects: 100%|██████████| 266/266 [10:29<00:00,  2.64s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<07:35,  1.72s/it]Loading train:   1%|          | 2/266 [00:03<07:13,  1.64s/it]Loading train:   1%|          | 3/266 [00:04<06:49,  1.56s/it]Loading train:   2%|▏         | 4/266 [00:05<06:26,  1.47s/it]Loading train:   2%|▏         | 5/266 [00:07<06:23,  1.47s/it]Loading train:   2%|▏         | 6/266 [00:08<05:43,  1.32s/it]Loading train:   3%|▎         | 7/266 [00:09<05:25,  1.26s/it]Loading train:   3%|▎         | 8/266 [00:10<04:59,  1.16s/it]Loading train:   3%|▎         | 9/266 [00:11<04:41,  1.09s/it]Loading train:   4%|▍         | 10/266 [00:12<04:31,  1.06s/it]Loading train:   4%|▍         | 11/266 [00:13<04:20,  1.02s/it]Loading train:   5%|▍         | 12/266 [00:14<04:20,  1.02s/it]Loading train:   5%|▍         | 13/266 [00:15<04:20,  1.03s/it]Loading train:   5%|▌         | 14/266 [00:16<04:27,  1.06s/it]Loading train:   6%|▌         | 15/266 [00:17<04:13,  1.01s/it]Loading train:   6%|▌         | 16/266 [00:18<04:17,  1.03s/it]Loading train:   6%|▋         | 17/266 [00:19<04:08,  1.00it/s]Loading train:   7%|▋         | 18/266 [00:20<04:18,  1.04s/it]Loading train:   7%|▋         | 19/266 [00:21<04:05,  1.01it/s]Loading train:   8%|▊         | 20/266 [00:22<04:05,  1.00it/s]Loading train:   8%|▊         | 21/266 [00:23<03:59,  1.02it/s]Loading train:   8%|▊         | 22/266 [00:24<03:57,  1.03it/s]Loading train:   9%|▊         | 23/266 [00:25<03:55,  1.03it/s]Loading train:   9%|▉         | 24/266 [00:26<03:57,  1.02it/s]Loading train:   9%|▉         | 25/266 [00:27<03:49,  1.05it/s]Loading train:  10%|▉         | 26/266 [00:28<03:50,  1.04it/s]Loading train:  10%|█         | 27/266 [00:28<03:37,  1.10it/s]Loading train:  11%|█         | 28/266 [00:29<03:32,  1.12it/s]Loading train:  11%|█         | 29/266 [00:30<03:25,  1.15it/s]Loading train:  11%|█▏        | 30/266 [00:31<03:22,  1.17it/s]Loading train:  12%|█▏        | 31/266 [00:32<03:24,  1.15it/s]Loading train:  12%|█▏        | 32/266 [00:33<03:30,  1.11it/s]Loading train:  12%|█▏        | 33/266 [00:34<03:29,  1.11it/s]Loading train:  13%|█▎        | 34/266 [00:35<03:35,  1.08it/s]Loading train:  13%|█▎        | 35/266 [00:35<03:32,  1.09it/s]Loading train:  14%|█▎        | 36/266 [00:36<03:32,  1.08it/s]Loading train:  14%|█▍        | 37/266 [00:37<03:27,  1.10it/s]Loading train:  14%|█▍        | 38/266 [00:38<03:24,  1.11it/s]Loading train:  15%|█▍        | 39/266 [00:39<03:19,  1.14it/s]Loading train:  15%|█▌        | 40/266 [00:40<03:19,  1.13it/s]Loading train:  15%|█▌        | 41/266 [00:41<03:22,  1.11it/s]Loading train:  16%|█▌        | 42/266 [00:42<03:14,  1.15it/s]Loading train:  16%|█▌        | 43/266 [00:42<03:11,  1.16it/s]Loading train:  17%|█▋        | 44/266 [00:43<03:03,  1.21it/s]Loading train:  17%|█▋        | 45/266 [00:44<03:02,  1.21it/s]Loading train:  17%|█▋        | 46/266 [00:45<03:01,  1.21it/s]Loading train:  18%|█▊        | 47/266 [00:46<02:58,  1.23it/s]Loading train:  18%|█▊        | 48/266 [00:46<02:58,  1.22it/s]Loading train:  18%|█▊        | 49/266 [00:47<02:54,  1.24it/s]Loading train:  19%|█▉        | 50/266 [00:48<02:54,  1.24it/s]Loading train:  19%|█▉        | 51/266 [00:49<02:54,  1.23it/s]Loading train:  20%|█▉        | 52/266 [00:50<02:59,  1.19it/s]Loading train:  20%|█▉        | 53/266 [00:50<02:50,  1.25it/s]Loading train:  20%|██        | 54/266 [00:51<02:48,  1.26it/s]Loading train:  21%|██        | 55/266 [00:52<02:50,  1.23it/s]Loading train:  21%|██        | 56/266 [00:53<02:50,  1.23it/s]Loading train:  21%|██▏       | 57/266 [00:54<02:46,  1.26it/s]Loading train:  22%|██▏       | 58/266 [00:54<02:46,  1.25it/s]Loading train:  22%|██▏       | 59/266 [00:55<02:45,  1.25it/s]Loading train:  23%|██▎       | 60/266 [00:56<02:48,  1.22it/s]Loading train:  23%|██▎       | 61/266 [00:57<02:38,  1.29it/s]Loading train:  23%|██▎       | 62/266 [00:58<02:37,  1.29it/s]Loading train:  24%|██▎       | 63/266 [00:58<02:40,  1.27it/s]Loading train:  24%|██▍       | 64/266 [00:59<02:38,  1.28it/s]Loading train:  24%|██▍       | 65/266 [01:00<02:40,  1.25it/s]Loading train:  25%|██▍       | 66/266 [01:01<02:38,  1.26it/s]Loading train:  25%|██▌       | 67/266 [01:02<02:32,  1.30it/s]Loading train:  26%|██▌       | 68/266 [01:02<02:34,  1.28it/s]Loading train:  26%|██▌       | 69/266 [01:03<02:32,  1.29it/s]Loading train:  26%|██▋       | 70/266 [01:04<02:28,  1.32it/s]Loading train:  27%|██▋       | 71/266 [01:05<02:27,  1.32it/s]Loading train:  27%|██▋       | 72/266 [01:05<02:27,  1.32it/s]Loading train:  27%|██▋       | 73/266 [01:06<02:25,  1.32it/s]Loading train:  28%|██▊       | 74/266 [01:07<02:20,  1.37it/s]Loading train:  28%|██▊       | 75/266 [01:07<02:18,  1.37it/s]Loading train:  29%|██▊       | 76/266 [01:08<02:19,  1.37it/s]Loading train:  29%|██▉       | 77/266 [01:09<02:17,  1.38it/s]Loading train:  29%|██▉       | 78/266 [01:10<02:29,  1.25it/s]Loading train:  30%|██▉       | 79/266 [01:11<02:42,  1.15it/s]Loading train:  30%|███       | 80/266 [01:12<02:45,  1.13it/s]Loading train:  30%|███       | 81/266 [01:13<02:46,  1.11it/s]Loading train:  31%|███       | 82/266 [01:14<03:00,  1.02it/s]Loading train:  31%|███       | 83/266 [01:15<02:57,  1.03it/s]Loading train:  32%|███▏      | 84/266 [01:16<02:59,  1.02it/s]Loading train:  32%|███▏      | 85/266 [01:17<02:53,  1.04it/s]Loading train:  32%|███▏      | 86/266 [01:18<02:52,  1.04it/s]Loading train:  33%|███▎      | 87/266 [01:19<02:53,  1.03it/s]Loading train:  33%|███▎      | 88/266 [01:20<02:51,  1.04it/s]Loading train:  33%|███▎      | 89/266 [01:21<02:50,  1.04it/s]Loading train:  34%|███▍      | 90/266 [01:22<02:52,  1.02it/s]Loading train:  34%|███▍      | 91/266 [01:23<02:46,  1.05it/s]Loading train:  35%|███▍      | 92/266 [01:24<02:47,  1.04it/s]Loading train:  35%|███▍      | 93/266 [01:24<02:43,  1.06it/s]Loading train:  35%|███▌      | 94/266 [01:25<02:40,  1.07it/s]Loading train:  36%|███▌      | 95/266 [01:26<02:38,  1.08it/s]Loading train:  36%|███▌      | 96/266 [01:28<02:56,  1.04s/it]Loading train:  36%|███▋      | 97/266 [01:29<03:14,  1.15s/it]Loading train:  37%|███▋      | 98/266 [01:30<03:20,  1.19s/it]Loading train:  37%|███▋      | 99/266 [01:31<03:09,  1.13s/it]Loading train:  38%|███▊      | 100/266 [01:32<03:09,  1.14s/it]Loading train:  38%|███▊      | 101/266 [01:33<02:52,  1.05s/it]Loading train:  38%|███▊      | 102/266 [01:34<02:40,  1.02it/s]Loading train:  39%|███▊      | 103/266 [01:35<02:39,  1.02it/s]Loading train:  39%|███▉      | 104/266 [01:36<02:30,  1.07it/s]Loading train:  39%|███▉      | 105/266 [01:37<02:25,  1.11it/s]Loading train:  40%|███▉      | 106/266 [01:37<02:14,  1.19it/s]Loading train:  40%|████      | 107/266 [01:38<02:16,  1.17it/s]Loading train:  41%|████      | 108/266 [01:39<02:07,  1.24it/s]Loading train:  41%|████      | 109/266 [01:40<02:11,  1.20it/s]Loading train:  41%|████▏     | 110/266 [01:41<02:17,  1.13it/s]Loading train:  42%|████▏     | 111/266 [01:42<02:13,  1.16it/s]Loading train:  42%|████▏     | 112/266 [01:43<02:27,  1.05it/s]Loading train:  42%|████▏     | 113/266 [01:44<02:24,  1.06it/s]Loading train:  43%|████▎     | 114/266 [01:45<02:36,  1.03s/it]Loading train:  43%|████▎     | 115/266 [01:46<02:46,  1.10s/it]Loading train:  44%|████▎     | 116/266 [01:48<02:51,  1.15s/it]Loading train:  44%|████▍     | 117/266 [01:48<02:36,  1.05s/it]Loading train:  44%|████▍     | 118/266 [01:49<02:32,  1.03s/it]Loading train:  45%|████▍     | 119/266 [01:51<02:42,  1.10s/it]Loading train:  45%|████▌     | 120/266 [01:52<02:41,  1.11s/it]Loading train:  45%|████▌     | 121/266 [01:53<02:43,  1.13s/it]Loading train:  46%|████▌     | 122/266 [01:54<02:47,  1.16s/it]Loading train:  46%|████▌     | 123/266 [01:55<02:46,  1.16s/it]Loading train:  47%|████▋     | 124/266 [01:56<02:44,  1.16s/it]Loading train:  47%|████▋     | 125/266 [01:58<02:44,  1.17s/it]Loading train:  47%|████▋     | 126/266 [01:59<02:43,  1.17s/it]Loading train:  48%|████▊     | 127/266 [02:00<02:43,  1.18s/it]Loading train:  48%|████▊     | 128/266 [02:01<02:43,  1.18s/it]Loading train:  48%|████▊     | 129/266 [02:03<02:49,  1.24s/it]Loading train:  49%|████▉     | 130/266 [02:04<02:48,  1.24s/it]Loading train:  49%|████▉     | 131/266 [02:05<02:54,  1.29s/it]Loading train:  50%|████▉     | 132/266 [02:06<02:45,  1.23s/it]Loading train:  50%|█████     | 133/266 [02:07<02:35,  1.17s/it]Loading train:  50%|█████     | 134/266 [02:09<02:37,  1.19s/it]Loading train:  51%|█████     | 135/266 [02:10<02:34,  1.18s/it]Loading train:  51%|█████     | 136/266 [02:11<02:34,  1.19s/it]Loading train:  52%|█████▏    | 137/266 [02:12<02:34,  1.20s/it]Loading train:  52%|█████▏    | 138/266 [02:14<02:48,  1.32s/it]Loading train:  52%|█████▏    | 139/266 [02:16<03:03,  1.45s/it]Loading train:  53%|█████▎    | 140/266 [02:17<02:58,  1.42s/it]Loading train:  53%|█████▎    | 141/266 [02:18<02:46,  1.34s/it]Loading train:  53%|█████▎    | 142/266 [02:19<02:40,  1.29s/it]Loading train:  54%|█████▍    | 143/266 [02:20<02:34,  1.26s/it]Loading train:  54%|█████▍    | 144/266 [02:21<02:24,  1.19s/it]Loading train:  55%|█████▍    | 145/266 [02:23<02:27,  1.22s/it]Loading train:  55%|█████▍    | 146/266 [02:24<02:18,  1.15s/it]Loading train:  55%|█████▌    | 147/266 [02:25<02:12,  1.11s/it]Loading train:  56%|█████▌    | 148/266 [02:26<02:28,  1.26s/it]Loading train:  56%|█████▌    | 149/266 [02:27<02:16,  1.16s/it]Loading train:  56%|█████▋    | 150/266 [02:28<02:11,  1.14s/it]Loading train:  57%|█████▋    | 151/266 [02:30<02:15,  1.18s/it]Loading train:  57%|█████▋    | 152/266 [02:31<02:10,  1.14s/it]Loading train:  58%|█████▊    | 153/266 [02:32<02:06,  1.12s/it]Loading train:  58%|█████▊    | 154/266 [02:33<02:04,  1.11s/it]Loading train:  58%|█████▊    | 155/266 [02:34<02:00,  1.08s/it]Loading train:  59%|█████▊    | 156/266 [02:35<01:54,  1.04s/it]Loading train:  59%|█████▉    | 157/266 [02:36<01:58,  1.08s/it]Loading train:  59%|█████▉    | 158/266 [02:37<02:08,  1.19s/it]Loading train:  60%|█████▉    | 159/266 [02:38<01:55,  1.08s/it]Loading train:  60%|██████    | 160/266 [02:39<01:46,  1.01s/it]Loading train:  61%|██████    | 161/266 [02:40<01:47,  1.03s/it]Loading train:  61%|██████    | 162/266 [02:41<01:41,  1.02it/s]Loading train:  61%|██████▏   | 163/266 [02:42<01:38,  1.04it/s]Loading train:  62%|██████▏   | 164/266 [02:43<01:55,  1.13s/it]Loading train:  62%|██████▏   | 165/266 [02:44<01:44,  1.03s/it]Loading train:  62%|██████▏   | 166/266 [02:45<01:37,  1.03it/s]Loading train:  63%|██████▎   | 167/266 [02:46<01:37,  1.02it/s]Loading train:  63%|██████▎   | 168/266 [02:47<01:37,  1.00it/s]Loading train:  64%|██████▎   | 169/266 [02:48<01:32,  1.05it/s]Loading train:  64%|██████▍   | 170/266 [02:49<01:30,  1.06it/s]Loading train:  64%|██████▍   | 171/266 [02:50<01:33,  1.02it/s]Loading train:  65%|██████▍   | 172/266 [02:51<01:28,  1.06it/s]Loading train:  65%|██████▌   | 173/266 [02:52<01:28,  1.05it/s]Loading train:  65%|██████▌   | 174/266 [02:53<01:35,  1.03s/it]Loading train:  66%|██████▌   | 175/266 [02:54<01:27,  1.04it/s]Loading train:  66%|██████▌   | 176/266 [02:55<01:29,  1.01it/s]Loading train:  67%|██████▋   | 177/266 [02:56<01:26,  1.03it/s]Loading train:  67%|██████▋   | 178/266 [02:57<01:25,  1.03it/s]Loading train:  67%|██████▋   | 179/266 [02:58<01:21,  1.07it/s]Loading train:  68%|██████▊   | 180/266 [02:59<01:23,  1.03it/s]Loading train:  68%|██████▊   | 181/266 [03:00<01:20,  1.06it/s]Loading train:  68%|██████▊   | 182/266 [03:00<01:17,  1.09it/s]Loading train:  69%|██████▉   | 183/266 [03:01<01:17,  1.07it/s]Loading train:  69%|██████▉   | 184/266 [03:02<01:17,  1.05it/s]Loading train:  70%|██████▉   | 185/266 [03:03<01:15,  1.08it/s]Loading train:  70%|██████▉   | 186/266 [03:04<01:12,  1.11it/s]Loading train:  70%|███████   | 187/266 [03:05<01:12,  1.09it/s]Loading train:  71%|███████   | 188/266 [03:06<01:11,  1.08it/s]Loading train:  71%|███████   | 189/266 [03:07<01:12,  1.07it/s]Loading train:  71%|███████▏  | 190/266 [03:08<01:12,  1.04it/s]Loading train:  72%|███████▏  | 191/266 [03:09<01:24,  1.13s/it]Loading train:  72%|███████▏  | 192/266 [03:11<01:28,  1.20s/it]Loading train:  73%|███████▎  | 193/266 [03:12<01:33,  1.28s/it]Loading train:  73%|███████▎  | 194/266 [03:14<01:42,  1.42s/it]Loading train:  73%|███████▎  | 195/266 [03:15<01:30,  1.28s/it]Loading train:  74%|███████▎  | 196/266 [03:16<01:22,  1.18s/it]Loading train:  74%|███████▍  | 197/266 [03:17<01:18,  1.14s/it]Loading train:  74%|███████▍  | 198/266 [03:18<01:13,  1.08s/it]Loading train:  75%|███████▍  | 199/266 [03:19<01:15,  1.12s/it]Loading train:  75%|███████▌  | 200/266 [03:20<01:13,  1.11s/it]Loading train:  76%|███████▌  | 201/266 [03:21<01:11,  1.09s/it]Loading train:  76%|███████▌  | 202/266 [03:22<01:09,  1.08s/it]Loading train:  76%|███████▋  | 203/266 [03:24<01:13,  1.17s/it]Loading train:  77%|███████▋  | 204/266 [03:25<01:10,  1.14s/it]Loading train:  77%|███████▋  | 205/266 [03:26<01:09,  1.14s/it]Loading train:  77%|███████▋  | 206/266 [03:27<01:09,  1.17s/it]Loading train:  78%|███████▊  | 207/266 [03:29<01:11,  1.22s/it]Loading train:  78%|███████▊  | 208/266 [03:30<01:15,  1.31s/it]Loading train:  79%|███████▊  | 209/266 [03:31<01:07,  1.18s/it]Loading train:  79%|███████▉  | 210/266 [03:32<01:04,  1.15s/it]Loading train:  79%|███████▉  | 211/266 [03:33<01:03,  1.15s/it]Loading train:  80%|███████▉  | 212/266 [03:34<01:00,  1.11s/it]Loading train:  80%|████████  | 213/266 [03:35<01:00,  1.14s/it]Loading train:  80%|████████  | 214/266 [03:36<00:58,  1.13s/it]Loading train:  81%|████████  | 215/266 [03:37<00:54,  1.08s/it]Loading train:  81%|████████  | 216/266 [03:39<00:56,  1.13s/it]Loading train:  82%|████████▏ | 217/266 [03:40<00:54,  1.11s/it]Loading train:  82%|████████▏ | 218/266 [03:41<00:52,  1.09s/it]Loading train:  82%|████████▏ | 219/266 [03:42<00:51,  1.10s/it]Loading train:  83%|████████▎ | 220/266 [03:43<00:48,  1.06s/it]Loading train:  83%|████████▎ | 221/266 [03:44<00:46,  1.03s/it]Loading train:  83%|████████▎ | 222/266 [03:45<00:44,  1.02s/it]Loading train:  84%|████████▍ | 223/266 [03:46<00:43,  1.01s/it]Loading train:  84%|████████▍ | 224/266 [03:47<00:40,  1.03it/s]Loading train:  85%|████████▍ | 225/266 [03:48<00:38,  1.06it/s]Loading train:  85%|████████▍ | 226/266 [03:49<00:40,  1.00s/it]Loading train:  85%|████████▌ | 227/266 [03:50<00:39,  1.01s/it]Loading train:  86%|████████▌ | 228/266 [03:51<00:37,  1.01it/s]Loading train:  86%|████████▌ | 229/266 [03:52<00:39,  1.06s/it]Loading train:  86%|████████▋ | 230/266 [03:53<00:38,  1.06s/it]Loading train:  87%|████████▋ | 231/266 [03:54<00:35,  1.02s/it]Loading train:  87%|████████▋ | 232/266 [03:55<00:34,  1.02s/it]Loading train:  88%|████████▊ | 233/266 [03:56<00:34,  1.03s/it]Loading train:  88%|████████▊ | 234/266 [03:57<00:32,  1.03s/it]Loading train:  88%|████████▊ | 235/266 [03:58<00:31,  1.03s/it]Loading train:  89%|████████▊ | 236/266 [03:59<00:31,  1.04s/it]Loading train:  89%|████████▉ | 237/266 [04:00<00:31,  1.08s/it]Loading train:  89%|████████▉ | 238/266 [04:01<00:29,  1.05s/it]Loading train:  90%|████████▉ | 239/266 [04:03<00:30,  1.13s/it]Loading train:  90%|█████████ | 240/266 [04:04<00:28,  1.11s/it]Loading train:  91%|█████████ | 241/266 [04:05<00:27,  1.10s/it]Loading train:  91%|█████████ | 242/266 [04:06<00:26,  1.10s/it]Loading train:  91%|█████████▏| 243/266 [04:07<00:24,  1.07s/it]Loading train:  92%|█████████▏| 244/266 [04:08<00:23,  1.06s/it]Loading train:  92%|█████████▏| 245/266 [04:09<00:23,  1.10s/it]Loading train:  92%|█████████▏| 246/266 [04:10<00:21,  1.06s/it]Loading train:  93%|█████████▎| 247/266 [04:11<00:22,  1.18s/it]Loading train:  93%|█████████▎| 248/266 [04:12<00:19,  1.10s/it]Loading train:  94%|█████████▎| 249/266 [04:13<00:18,  1.11s/it]Loading train:  94%|█████████▍| 250/266 [04:15<00:19,  1.20s/it]Loading train:  94%|█████████▍| 251/266 [04:16<00:17,  1.16s/it]Loading train:  95%|█████████▍| 252/266 [04:17<00:16,  1.18s/it]Loading train:  95%|█████████▌| 253/266 [04:18<00:15,  1.16s/it]Loading train:  95%|█████████▌| 254/266 [04:20<00:14,  1.18s/it]Loading train:  96%|█████████▌| 255/266 [04:21<00:12,  1.16s/it]Loading train:  96%|█████████▌| 256/266 [04:22<00:11,  1.16s/it]Loading train:  97%|█████████▋| 257/266 [04:23<00:10,  1.17s/it]Loading train:  97%|█████████▋| 258/266 [04:24<00:09,  1.16s/it]Loading train:  97%|█████████▋| 259/266 [04:25<00:07,  1.14s/it]Loading train:  98%|█████████▊| 260/266 [04:26<00:07,  1.17s/it]Loading train:  98%|█████████▊| 261/266 [04:28<00:05,  1.16s/it]Loading train:  98%|█████████▊| 262/266 [04:29<00:04,  1.16s/it]Loading train:  99%|█████████▉| 263/266 [04:30<00:03,  1.12s/it]Loading train:  99%|█████████▉| 264/266 [04:31<00:02,  1.11s/it]Loading train: 100%|█████████▉| 265/266 [04:32<00:01,  1.11s/it]Loading train: 100%|██████████| 266/266 [04:33<00:00,  1.12s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 4/266 [00:00<00:06, 38.65it/s]concatenating: train:  14%|█▍        | 37/266 [00:00<00:04, 52.47it/s]concatenating: train:  26%|██▌       | 69/266 [00:00<00:02, 70.03it/s]concatenating: train:  32%|███▏      | 86/266 [00:00<00:02, 75.10it/s]concatenating: train:  38%|███▊      | 101/266 [00:00<00:02, 81.93it/s]concatenating: train:  47%|████▋     | 126/266 [00:00<00:01, 102.35it/s]concatenating: train:  54%|█████▍    | 143/266 [00:00<00:01, 112.43it/s]concatenating: train:  60%|██████    | 160/266 [00:01<00:00, 106.34it/s]concatenating: train:  66%|██████▌   | 176/266 [00:01<00:00, 117.59it/s]concatenating: train:  76%|███████▌  | 201/266 [00:01<00:00, 139.36it/s]concatenating: train:  88%|████████▊ | 233/266 [00:01<00:00, 167.11it/s]concatenating: train:  97%|█████████▋| 257/266 [00:01<00:00, 183.69it/s]concatenating: train: 100%|██████████| 266/266 [00:01<00:00, 163.07it/s]
Loading test:   0%|          | 0/5 [00:00<?, ?it/s]Loading test:  20%|██        | 1/5 [00:01<00:05,  1.40s/it]Loading test:  40%|████      | 2/5 [00:02<00:04,  1.44s/it]Loading test:  60%|██████    | 3/5 [00:04<00:02,  1.40s/it]Loading test:  80%|████████  | 4/5 [00:05<00:01,  1.39s/it]Loading test: 100%|██████████| 5/5 [00:07<00:00,  1.41s/it]
concatenating: validation:   0%|          | 0/5 [00:00<?, ?it/s]concatenating: validation:  60%|██████    | 3/5 [00:00<00:00, 25.76it/s]concatenating: validation: 100%|██████████| 5/5 [00:00<00:00, 34.47it/s]2019-08-17 20:34:35.587597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-08-17 20:34:35.587734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-17 20:34:35.587752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-08-17 20:34:35.587761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-08-17 20:34:35.588231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Res Unet:   0%|          | 0/44 [00:00<?, ?it/s]loading the weights for Res Unet:   2%|▏         | 1/44 [00:00<00:07,  5.48it/s]loading the weights for Res Unet:   7%|▋         | 3/44 [00:00<00:06,  6.49it/s]loading the weights for Res Unet:   9%|▉         | 4/44 [00:00<00:06,  6.15it/s]loading the weights for Res Unet:  18%|█▊        | 8/44 [00:00<00:04,  7.81it/s]loading the weights for Res Unet:  20%|██        | 9/44 [00:00<00:05,  6.62it/s]loading the weights for Res Unet:  25%|██▌       | 11/44 [00:01<00:04,  7.44it/s]loading the weights for Res Unet:  27%|██▋       | 12/44 [00:01<00:05,  6.39it/s]loading the weights for Res Unet:  39%|███▊      | 17/44 [00:01<00:03,  8.08it/s]loading the weights for Res Unet:  43%|████▎     | 19/44 [00:01<00:02,  8.40it/s]loading the weights for Res Unet:  48%|████▊     | 21/44 [00:02<00:03,  7.18it/s]loading the weights for Res Unet:  57%|█████▋    | 25/44 [00:02<00:02,  8.96it/s]loading the weights for Res Unet:  61%|██████▏   | 27/44 [00:02<00:01,  9.45it/s]loading the weights for Res Unet:  66%|██████▌   | 29/44 [00:02<00:01,  9.86it/s]loading the weights for Res Unet:  70%|███████   | 31/44 [00:03<00:01,  7.55it/s]loading the weights for Res Unet:  80%|███████▉  | 35/44 [00:03<00:00,  9.18it/s]loading the weights for Res Unet:  84%|████████▍ | 37/44 [00:03<00:00,  8.50it/s]loading the weights for Res Unet:  89%|████████▊ | 39/44 [00:03<00:00,  8.70it/s]loading the weights for Res Unet:  93%|█████████▎| 41/44 [00:04<00:00,  7.30it/s]loading the weights for Res Unet: 100%|██████████| 44/44 [00:04<00:00, 10.45it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 2  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 2  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 2  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
ResNet model address /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/model.h5
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 84, 48, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 84, 48, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 84, 48, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 84, 48, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 84, 48, 40)   14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 84, 48, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 84, 48, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 42, 24, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 42, 24, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 42, 24, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 42, 24, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 42, 24, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 42, 24, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 42, 24, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 42, 24, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 42, 24, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 21, 12, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 21, 12, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 21, 12, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 21, 12, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 21, 12, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 21, 12, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 21, 12, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 21, 12, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 21, 12, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 21, 12, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 42, 24, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 42, 24, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 42, 24, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 42, 24, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 42, 24, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 42, 24, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 42, 24, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 42, 24, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 42, 24, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 42, 24, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 84, 48, 40)   44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 84, 48, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 84, 48, 40)   28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 84, 48, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 84, 48, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 84, 48, 40)   14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 84, 48, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 84, 48, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 84, 48, 120)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 84, 48, 120)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 84, 48, 13)   1573        dropout_5[0][0]                  
==================================================================================================
Total params: 889,253
Trainable params: 406,693
Non-trainable params: 482,560
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.34131721e-02 3.28661667e-02 7.68530104e-02 9.54937299e-03
 2.76380599e-02 7.23076812e-03 8.45344037e-02 1.14229347e-01
 8.96925844e-02 1.36274248e-02 2.90801000e-01 1.89298384e-01
 2.66305445e-04]
Train on 10291 samples, validate on 187 samples
Epoch 1/300
 - 18s - loss: 2.6516 - acc: 0.6598 - mDice: 0.1117 - val_loss: 1.3351 - val_acc: 0.9020 - val_mDice: 0.2842

Epoch 00001: val_mDice improved from -inf to 0.28425, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 13s - loss: 1.2266 - acc: 0.8819 - mDice: 0.3016 - val_loss: 0.9234 - val_acc: 0.9117 - val_mDice: 0.4015

Epoch 00002: val_mDice improved from 0.28425 to 0.40154, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 13s - loss: 0.9324 - acc: 0.8900 - mDice: 0.3902 - val_loss: 0.7738 - val_acc: 0.9167 - val_mDice: 0.4613

Epoch 00003: val_mDice improved from 0.40154 to 0.46134, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 13s - loss: 0.7963 - acc: 0.8960 - mDice: 0.4439 - val_loss: 0.7029 - val_acc: 0.9190 - val_mDice: 0.4870

Epoch 00004: val_mDice improved from 0.46134 to 0.48701, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 13s - loss: 0.6997 - acc: 0.9016 - mDice: 0.4865 - val_loss: 0.6714 - val_acc: 0.9248 - val_mDice: 0.5045

Epoch 00005: val_mDice improved from 0.48701 to 0.50449, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 13s - loss: 0.6346 - acc: 0.9074 - mDice: 0.5199 - val_loss: 0.6047 - val_acc: 0.9291 - val_mDice: 0.5355

Epoch 00006: val_mDice improved from 0.50449 to 0.53551, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 13s - loss: 0.5704 - acc: 0.9130 - mDice: 0.5521 - val_loss: 0.6091 - val_acc: 0.9317 - val_mDice: 0.5393

Epoch 00007: val_mDice improved from 0.53551 to 0.53927, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 13s - loss: 0.5297 - acc: 0.9173 - mDice: 0.5759 - val_loss: 0.5799 - val_acc: 0.9328 - val_mDice: 0.5548

Epoch 00008: val_mDice improved from 0.53927 to 0.55477, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 13s - loss: 0.5065 - acc: 0.9201 - mDice: 0.5898 - val_loss: 0.5753 - val_acc: 0.9307 - val_mDice: 0.5593

Epoch 00009: val_mDice improved from 0.55477 to 0.55935, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 13s - loss: 0.4789 - acc: 0.9230 - mDice: 0.6052 - val_loss: 0.5424 - val_acc: 0.9370 - val_mDice: 0.5705

Epoch 00010: val_mDice improved from 0.55935 to 0.57050, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 13s - loss: 0.4521 - acc: 0.9267 - mDice: 0.6229 - val_loss: 0.5620 - val_acc: 0.9364 - val_mDice: 0.5631

Epoch 00011: val_mDice did not improve from 0.57050
Epoch 12/300
 - 13s - loss: 0.4314 - acc: 0.9295 - mDice: 0.6361 - val_loss: 0.5281 - val_acc: 0.9352 - val_mDice: 0.5785

Epoch 00012: val_mDice improved from 0.57050 to 0.57854, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 13s - loss: 0.4161 - acc: 0.9313 - mDice: 0.6460 - val_loss: 0.5216 - val_acc: 0.9380 - val_mDice: 0.5836

Epoch 00013: val_mDice improved from 0.57854 to 0.58356, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 13s - loss: 0.4025 - acc: 0.9330 - mDice: 0.6555 - val_loss: 0.5267 - val_acc: 0.9361 - val_mDice: 0.5806

Epoch 00014: val_mDice did not improve from 0.58356
Epoch 15/300
 - 13s - loss: 0.3932 - acc: 0.9342 - mDice: 0.6617 - val_loss: 0.5272 - val_acc: 0.9365 - val_mDice: 0.5817

Epoch 00015: val_mDice did not improve from 0.58356
Epoch 16/300
 - 13s - loss: 0.3785 - acc: 0.9358 - mDice: 0.6716 - val_loss: 0.5180 - val_acc: 0.9382 - val_mDice: 0.5872

Epoch 00016: val_mDice improved from 0.58356 to 0.58717, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 13s - loss: 0.3749 - acc: 0.9363 - mDice: 0.6743 - val_loss: 0.5265 - val_acc: 0.9373 - val_mDice: 0.5806

Epoch 00017: val_mDice did not improve from 0.58717
Epoch 18/300
 - 13s - loss: 0.3644 - acc: 0.9374 - mDice: 0.6813 - val_loss: 0.5291 - val_acc: 0.9379 - val_mDice: 0.5809

Epoch 00018: val_mDice did not improve from 0.58717
Epoch 19/300
 - 13s - loss: 0.3514 - acc: 0.9387 - mDice: 0.6902 - val_loss: 0.5206 - val_acc: 0.9371 - val_mDice: 0.5855

Epoch 00019: val_mDice did not improve from 0.58717
Epoch 20/300
 - 13s - loss: 0.3489 - acc: 0.9391 - mDice: 0.6921 - val_loss: 0.5080 - val_acc: 0.9385 - val_mDice: 0.5919

Epoch 00020: val_mDice improved from 0.58717 to 0.59187, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 13s - loss: 0.3428 - acc: 0.9398 - mDice: 0.6966 - val_loss: 0.5037 - val_acc: 0.9389 - val_mDice: 0.5930

Epoch 00021: val_mDice improved from 0.59187 to 0.59295, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 13s - loss: 0.3358 - acc: 0.9405 - mDice: 0.7015 - val_loss: 0.5050 - val_acc: 0.9393 - val_mDice: 0.5930

Epoch 00022: val_mDice improved from 0.59295 to 0.59297, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 13s - loss: 0.3298 - acc: 0.9410 - mDice: 0.7058 - val_loss: 0.5041 - val_acc: 0.9394 - val_mDice: 0.5938

Epoch 00023: val_mDice improved from 0.59297 to 0.59379, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 13s - loss: 0.3243 - acc: 0.9416 - mDice: 0.7099 - val_loss: 0.4989 - val_acc: 0.9413 - val_mDice: 0.5985

Epoch 00024: val_mDice improved from 0.59379 to 0.59852, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 13s - loss: 0.3199 - acc: 0.9422 - mDice: 0.7131 - val_loss: 0.5032 - val_acc: 0.9389 - val_mDice: 0.5951

Epoch 00025: val_mDice did not improve from 0.59852
Epoch 26/300
 - 13s - loss: 0.3156 - acc: 0.9426 - mDice: 0.7163 - val_loss: 0.4896 - val_acc: 0.9406 - val_mDice: 0.6028

Epoch 00026: val_mDice improved from 0.59852 to 0.60284, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 13s - loss: 0.3123 - acc: 0.9429 - mDice: 0.7188 - val_loss: 0.4933 - val_acc: 0.9401 - val_mDice: 0.6013

Epoch 00027: val_mDice did not improve from 0.60284
Epoch 28/300
 - 13s - loss: 0.3093 - acc: 0.9433 - mDice: 0.7212 - val_loss: 0.5138 - val_acc: 0.9401 - val_mDice: 0.5882

Epoch 00028: val_mDice did not improve from 0.60284
Epoch 29/300
 - 13s - loss: 0.3182 - acc: 0.9428 - mDice: 0.7188 - val_loss: 0.4873 - val_acc: 0.9394 - val_mDice: 0.6025

Epoch 00029: val_mDice did not improve from 0.60284
Epoch 30/300
 - 13s - loss: 0.3039 - acc: 0.9437 - mDice: 0.7251 - val_loss: 0.4865 - val_acc: 0.9409 - val_mDice: 0.6039

Epoch 00030: val_mDice improved from 0.60284 to 0.60393, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 13s - loss: 0.3051 - acc: 0.9438 - mDice: 0.7244 - val_loss: 0.4811 - val_acc: 0.9396 - val_mDice: 0.6066

Epoch 00031: val_mDice improved from 0.60393 to 0.60660, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 13s - loss: 0.2943 - acc: 0.9447 - mDice: 0.7322 - val_loss: 0.4836 - val_acc: 0.9424 - val_mDice: 0.6063

Epoch 00032: val_mDice did not improve from 0.60660
Epoch 33/300
 - 13s - loss: 0.2926 - acc: 0.9450 - mDice: 0.7335 - val_loss: 0.4792 - val_acc: 0.9397 - val_mDice: 0.6075

Epoch 00033: val_mDice improved from 0.60660 to 0.60753, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 13s - loss: 0.2910 - acc: 0.9451 - mDice: 0.7348 - val_loss: 0.4913 - val_acc: 0.9395 - val_mDice: 0.6013

Epoch 00034: val_mDice did not improve from 0.60753
Epoch 35/300
 - 13s - loss: 0.2879 - acc: 0.9455 - mDice: 0.7371 - val_loss: 0.4886 - val_acc: 0.9400 - val_mDice: 0.6034

Epoch 00035: val_mDice did not improve from 0.60753
Epoch 36/300
 - 13s - loss: 0.2836 - acc: 0.9460 - mDice: 0.7404 - val_loss: 0.4963 - val_acc: 0.9412 - val_mDice: 0.6012

Epoch 00036: val_mDice did not improve from 0.60753
Epoch 37/300
 - 13s - loss: 0.2816 - acc: 0.9461 - mDice: 0.7418 - val_loss: 0.4833 - val_acc: 0.9384 - val_mDice: 0.6065

Epoch 00037: val_mDice did not improve from 0.60753
Epoch 38/300
 - 14s - loss: 0.2873 - acc: 0.9464 - mDice: 0.7422 - val_loss: 0.4808 - val_acc: 0.9408 - val_mDice: 0.6079

Epoch 00038: val_mDice improved from 0.60753 to 0.60786, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 13s - loss: 0.2898 - acc: 0.9461 - mDice: 0.7404 - val_loss: 0.4982 - val_acc: 0.9400 - val_mDice: 0.5996

Epoch 00039: val_mDice did not improve from 0.60786
Epoch 40/300
 - 14s - loss: 0.2786 - acc: 0.9465 - mDice: 0.7441 - val_loss: 0.4816 - val_acc: 0.9411 - val_mDice: 0.6089

Epoch 00040: val_mDice improved from 0.60786 to 0.60892, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 13s - loss: 0.2748 - acc: 0.9468 - mDice: 0.7470 - val_loss: 0.4818 - val_acc: 0.9414 - val_mDice: 0.6086

Epoch 00041: val_mDice did not improve from 0.60892
Epoch 42/300
 - 13s - loss: 0.2716 - acc: 0.9472 - mDice: 0.7495 - val_loss: 0.4799 - val_acc: 0.9402 - val_mDice: 0.6079

Epoch 00042: val_mDice did not improve from 0.60892
Epoch 43/300
 - 14s - loss: 0.2815 - acc: 0.9463 - mDice: 0.7445 - val_loss: 0.4983 - val_acc: 0.9437 - val_mDice: 0.5995

Epoch 00043: val_mDice did not improve from 0.60892
Epoch 44/300
 - 13s - loss: 0.2753 - acc: 0.9467 - mDice: 0.7466 - val_loss: 0.4936 - val_acc: 0.9403 - val_mDice: 0.6018

Epoch 00044: val_mDice did not improve from 0.60892
Epoch 45/300
 - 13s - loss: 0.2688 - acc: 0.9474 - mDice: 0.7515 - val_loss: 0.4864 - val_acc: 0.9418 - val_mDice: 0.6043

Epoch 00045: val_mDice did not improve from 0.60892
Epoch 46/300
 - 14s - loss: 0.2674 - acc: 0.9477 - mDice: 0.7529 - val_loss: 0.4773 - val_acc: 0.9401 - val_mDice: 0.6087

Epoch 00046: val_mDice did not improve from 0.60892
Epoch 47/300
 - 13s - loss: 0.2653 - acc: 0.9480 - mDice: 0.7546 - val_loss: 0.4883 - val_acc: 0.9414 - val_mDice: 0.6050

Epoch 00047: val_mDice did not improve from 0.60892
Epoch 48/300
 - 13s - loss: 0.2717 - acc: 0.9480 - mDice: 0.7541 - val_loss: 0.4923 - val_acc: 0.9405 - val_mDice: 0.6020

Epoch 00048: val_mDice did not improve from 0.60892
Epoch 49/300
 - 14s - loss: 0.2604 - acc: 0.9483 - mDice: 0.7582 - val_loss: 0.4871 - val_acc: 0.9397 - val_mDice: 0.6044

Epoch 00049: val_mDice did not improve from 0.60892
Epoch 50/300
 - 13s - loss: 0.2599 - acc: 0.9485 - mDice: 0.7589 - val_loss: 0.4867 - val_acc: 0.9403 - val_mDice: 0.6049

Epoch 00050: val_mDice did not improve from 0.60892
Epoch 51/300
 - 13s - loss: 0.2598 - acc: 0.9485 - mDice: 0.7588 - val_loss: 0.4687 - val_acc: 0.9418 - val_mDice: 0.6140

Epoch 00051: val_mDice improved from 0.60892 to 0.61395, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 13s - loss: 0.2653 - acc: 0.9486 - mDice: 0.7595 - val_loss: 0.4819 - val_acc: 0.9407 - val_mDice: 0.6062

Epoch 00052: val_mDice did not improve from 0.61395
Epoch 53/300
 - 13s - loss: 0.2571 - acc: 0.9487 - mDice: 0.7608 - val_loss: 0.4863 - val_acc: 0.9406 - val_mDice: 0.6053

Epoch 00053: val_mDice did not improve from 0.61395
Epoch 54/300
 - 13s - loss: 0.2559 - acc: 0.9488 - mDice: 0.7618 - val_loss: 0.4800 - val_acc: 0.9411 - val_mDice: 0.6080

Epoch 00054: val_mDice did not improve from 0.61395
Epoch 55/300
 - 13s - loss: 0.2673 - acc: 0.9483 - mDice: 0.7572 - val_loss: 0.4815 - val_acc: 0.9378 - val_mDice: 0.6062

Epoch 00055: val_mDice did not improve from 0.61395
Epoch 56/300
 - 13s - loss: 0.2578 - acc: 0.9487 - mDice: 0.7604 - val_loss: 0.4692 - val_acc: 0.9416 - val_mDice: 0.6146

Epoch 00056: val_mDice improved from 0.61395 to 0.61457, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 57/300
 - 13s - loss: 0.2530 - acc: 0.9491 - mDice: 0.7641 - val_loss: 0.4823 - val_acc: 0.9401 - val_mDice: 0.6074

Epoch 00057: val_mDice did not improve from 0.61457
Epoch 58/300
 - 13s - loss: 0.2510 - acc: 0.9494 - mDice: 0.7657 - val_loss: 0.4760 - val_acc: 0.9415 - val_mDice: 0.6118

Epoch 00058: val_mDice did not improve from 0.61457
Epoch 59/300
 - 13s - loss: 0.2493 - acc: 0.9496 - mDice: 0.7671 - val_loss: 0.4759 - val_acc: 0.9424 - val_mDice: 0.6121

Epoch 00059: val_mDice did not improve from 0.61457
Epoch 60/300
 - 13s - loss: 0.2480 - acc: 0.9497 - mDice: 0.7681 - val_loss: 0.4678 - val_acc: 0.9412 - val_mDice: 0.6152

Epoch 00060: val_mDice improved from 0.61457 to 0.61518, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 61/300
 - 13s - loss: 0.2479 - acc: 0.9497 - mDice: 0.7682 - val_loss: 0.4752 - val_acc: 0.9410 - val_mDice: 0.6110

Epoch 00061: val_mDice did not improve from 0.61518
Epoch 62/300
 - 13s - loss: 0.2483 - acc: 0.9497 - mDice: 0.7680 - val_loss: 0.4858 - val_acc: 0.9408 - val_mDice: 0.6060

Epoch 00062: val_mDice did not improve from 0.61518
Epoch 63/300
 - 13s - loss: 0.2488 - acc: 0.9497 - mDice: 0.7677 - val_loss: 0.4785 - val_acc: 0.9414 - val_mDice: 0.6103

Epoch 00063: val_mDice did not improve from 0.61518
Epoch 64/300
 - 13s - loss: 0.2466 - acc: 0.9499 - mDice: 0.7692 - val_loss: 0.4776 - val_acc: 0.9419 - val_mDice: 0.6095

Epoch 00064: val_mDice did not improve from 0.61518
Epoch 65/300
 - 13s - loss: 0.2463 - acc: 0.9500 - mDice: 0.7695 - val_loss: 0.4741 - val_acc: 0.9418 - val_mDice: 0.6128

Epoch 00065: val_mDice did not improve from 0.61518
Epoch 66/300
 - 13s - loss: 0.2437 - acc: 0.9503 - mDice: 0.7716 - val_loss: 0.4826 - val_acc: 0.9429 - val_mDice: 0.6075

Epoch 00066: val_mDice did not improve from 0.61518
Epoch 67/300
 - 13s - loss: 0.2437 - acc: 0.9502 - mDice: 0.7716 - val_loss: 0.4871 - val_acc: 0.9405 - val_mDice: 0.6034

Epoch 00067: val_mDice did not improve from 0.61518
Epoch 68/300
 - 13s - loss: 0.2418 - acc: 0.9503 - mDice: 0.7730 - val_loss: 0.4739 - val_acc: 0.9421 - val_mDice: 0.6118

Epoch 00068: val_mDice did not improve from 0.61518
Epoch 69/300
 - 13s - loss: 0.2431 - acc: 0.9503 - mDice: 0.7723 - val_loss: 0.4690 - val_acc: 0.9405 - val_mDice: 0.6138

Epoch 00069: val_mDice did not improve from 0.61518
Epoch 70/300
 - 13s - loss: 0.2443 - acc: 0.9502 - mDice: 0.7713 - val_loss: 0.4739 - val_acc: 0.9417 - val_mDice: 0.6135

Epoch 00070: val_mDice did not improve from 0.61518
Epoch 71/300
 - 13s - loss: 0.2403 - acc: 0.9506 - mDice: 0.7743 - val_loss: 0.4710 - val_acc: 0.9410 - val_mDice: 0.6132

Epoch 00071: val_mDice did not improve from 0.61518
Epoch 72/300
 - 13s - loss: 0.2401 - acc: 0.9506 - mDice: 0.7745 - val_loss: 0.4854 - val_acc: 0.9412 - val_mDice: 0.6072

Epoch 00072: val_mDice did not improve from 0.61518
Epoch 73/300
 - 13s - loss: 0.2395 - acc: 0.9507 - mDice: 0.7750 - val_loss: 0.4692 - val_acc: 0.9434 - val_mDice: 0.6155

Epoch 00073: val_mDice improved from 0.61518 to 0.61555, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 13s - loss: 0.2380 - acc: 0.9509 - mDice: 0.7761 - val_loss: 0.4618 - val_acc: 0.9415 - val_mDice: 0.6184

Epoch 00074: val_mDice improved from 0.61555 to 0.61839, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 75/300
 - 13s - loss: 0.2402 - acc: 0.9506 - mDice: 0.7748 - val_loss: 0.4639 - val_acc: 0.9421 - val_mDice: 0.6165

Epoch 00075: val_mDice did not improve from 0.61839
Epoch 76/300
 - 13s - loss: 0.2379 - acc: 0.9508 - mDice: 0.7763 - val_loss: 0.4720 - val_acc: 0.9423 - val_mDice: 0.6122

Epoch 00076: val_mDice did not improve from 0.61839
Epoch 77/300
 - 13s - loss: 0.2348 - acc: 0.9512 - mDice: 0.7788 - val_loss: 0.4679 - val_acc: 0.9417 - val_mDice: 0.6148

Epoch 00077: val_mDice did not improve from 0.61839
Epoch 78/300
 - 13s - loss: 0.2376 - acc: 0.9510 - mDice: 0.7766 - val_loss: 0.4583 - val_acc: 0.9430 - val_mDice: 0.6208

Epoch 00078: val_mDice improved from 0.61839 to 0.62079, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_ResFCN_ResUnet2_TL_NL3_LS_MyLogDice_US1_FCNA0_FCNB0_FM0_permute8_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd0/best_model_weights_TF_CSFn2.h5
Epoch 79/300
 - 13s - loss: 0.2345 - acc: 0.9513 - mDice: 0.7790 - val_loss: 0.4718 - val_acc: 0.9423 - val_mDice: 0.6139

Epoch 00079: val_mDice did not improve from 0.62079
Epoch 80/300
 - 13s - loss: 0.2351 - acc: 0.9512 - mDice: 0.7786 - val_loss: 0.4711 - val_acc: 0.9413 - val_mDice: 0.6122

Epoch 00080: val_mDice did not improve from 0.62079
Epoch 81/300
 - 13s - loss: 0.2340 - acc: 0.9513 - mDice: 0.7796 - val_loss: 0.4665 - val_acc: 0.9412 - val_mDice: 0.6155

Epoch 00081: val_mDice did not improve from 0.62079
Epoch 82/300
 - 13s - loss: 0.2329 - acc: 0.9514 - mDice: 0.7804 - val_loss: 0.4856 - val_acc: 0.9413 - val_mDice: 0.6073

Epoch 00082: val_mDice did not improve from 0.62079
Epoch 83/300
 - 13s - loss: 0.2335 - acc: 0.9513 - mDice: 0.7799 - val_loss: 0.4770 - val_acc: 0.9419 - val_mDice: 0.6100

Epoch 00083: val_mDice did not improve from 0.62079
Epoch 84/300
 - 13s - loss: 0.2314 - acc: 0.9515 - mDice: 0.7816 - val_loss: 0.4814 - val_acc: 0.9410 - val_mDice: 0.6067

Epoch 00084: val_mDice did not improve from 0.62079
Epoch 85/300
 - 13s - loss: 0.2319 - acc: 0.9515 - mDice: 0.7812 - val_loss: 0.4782 - val_acc: 0.9402 - val_mDice: 0.6079

Epoch 00085: val_mDice did not improve from 0.62079
Epoch 86/300
 - 13s - loss: 0.2308 - acc: 0.9517 - mDice: 0.7820 - val_loss: 0.4849 - val_acc: 0.9403 - val_mDice: 0.6048

Epoch 00086: val_mDice did not improve from 0.62079
Epoch 87/300
 - 13s - loss: 0.2309 - acc: 0.9517 - mDice: 0.7820 - val_loss: 0.4774 - val_acc: 0.9414 - val_mDice: 0.6091

Epoch 00087: val_mDice did not improve from 0.62079
Epoch 88/300
 - 13s - loss: 0.2393 - acc: 0.9514 - mDice: 0.7800 - val_loss: 0.4756 - val_acc: 0.9421 - val_mDice: 0.6111

Epoch 00088: val_mDice did not improve from 0.62079
Epoch 89/300
 - 13s - loss: 0.2287 - acc: 0.9519 - mDice: 0.7837 - val_loss: 0.4697 - val_acc: 0.9420 - val_mDice: 0.6140

Epoch 00089: val_mDice did not improve from 0.62079
Epoch 90/300
 - 13s - loss: 0.2286 - acc: 0.9518 - mDice: 0.7838 - val_loss: 0.4631 - val_acc: 0.9420 - val_mDice: 0.6174

Epoch 00090: val_mDice did not improve from 0.62079
Epoch 91/300
 - 12s - loss: 0.2277 - acc: 0.9521 - mDice: 0.7846 - val_loss: 0.4667 - val_acc: 0.9414 - val_mDice: 0.6150

Epoch 00091: val_mDice did not improve from 0.62079
Epoch 92/300
 - 12s - loss: 0.2288 - acc: 0.9519 - mDice: 0.7837 - val_loss: 0.4601 - val_acc: 0.9439 - val_mDice: 0.6200

Epoch 00092: val_mDice did not improve from 0.62079
Epoch 93/300
 - 12s - loss: 0.2285 - acc: 0.9519 - mDice: 0.7840 - val_loss: 0.4597 - val_acc: 0.9426 - val_mDice: 0.6193

Epoch 00093: val_mDice did not improve from 0.62079
Epoch 94/300
 - 13s - loss: 0.2275 - acc: 0.9520 - mDice: 0.7848 - val_loss: 0.4739 - val_acc: 0.9417 - val_mDice: 0.6118

Epoch 00094: val_mDice did not improve from 0.62079
Epoch 95/300
 - 12s - loss: 0.2277 - acc: 0.9521 - mDice: 0.7847 - val_loss: 0.4698 - val_acc: 0.9425 - val_mDice: 0.6130

Epoch 00095: val_mDice did not improve from 0.62079
Epoch 96/300
 - 12s - loss: 0.2275 - acc: 0.9521 - mDice: 0.7849 - val_loss: 0.4690 - val_acc: 0.9429 - val_mDice: 0.6139

Epoch 00096: val_mDice did not improve from 0.62079
Epoch 97/300
 - 13s - loss: 0.2296 - acc: 0.9519 - mDice: 0.7835 - val_loss: 0.5345 - val_acc: 0.9458 - val_mDice: 0.5877

Epoch 00097: val_mDice did not improve from 0.62079
Epoch 98/300
 - 13s - loss: 0.2295 - acc: 0.9519 - mDice: 0.7831 - val_loss: 0.4725 - val_acc: 0.9402 - val_mDice: 0.6108

Epoch 00098: val_mDice did not improve from 0.62079
Epoch 99/300
 - 12s - loss: 0.2266 - acc: 0.9522 - mDice: 0.7855 - val_loss: 0.4639 - val_acc: 0.9418 - val_mDice: 0.6151

Epoch 00099: val_mDice did not improve from 0.62079
Epoch 100/300
 - 12s - loss: 0.2254 - acc: 0.9523 - mDice: 0.7865 - val_loss: 0.4661 - val_acc: 0.9408 - val_mDice: 0.6142

Epoch 00100: val_mDice did not improve from 0.62079
Epoch 101/300
 - 12s - loss: 0.2241 - acc: 0.9524 - mDice: 0.7876 - val_loss: 0.4769 - val_acc: 0.9411 - val_mDice: 0.6090

Epoch 00101: val_mDice did not improve from 0.62079
Epoch 102/300
 - 13s - loss: 0.2235 - acc: 0.9524 - mDice: 0.7881 - val_loss: 0.4825 - val_acc: 0.9413 - val_mDice: 0.6060

Epoch 00102: val_mDice did not improve from 0.62079
Epoch 103/300
 - 13s - loss: 0.2241 - acc: 0.9524 - mDice: 0.7876 - val_loss: 0.4725 - val_acc: 0.9414 - val_mDice: 0.6122

Epoch 00103: val_mDice did not improve from 0.62079
Epoch 104/300
 - 13s - loss: 0.2219 - acc: 0.9526 - mDice: 0.7894 - val_loss: 0.4732 - val_acc: 0.9410 - val_mDice: 0.6113

Epoch 00104: val_mDice did not improve from 0.62079
Epoch 105/300
 - 13s - loss: 0.2233 - acc: 0.9526 - mDice: 0.7882 - val_loss: 0.4670 - val_acc: 0.9433 - val_mDice: 0.6151

Epoch 00105: val_mDice did not improve from 0.62079
Epoch 106/300
 - 13s - loss: 0.2232 - acc: 0.9525 - mDice: 0.7884 - val_loss: 0.4790 - val_acc: 0.9410 - val_mDice: 0.6082

Epoch 00106: val_mDice did not improve from 0.62079
Epoch 107/300
 - 13s - loss: 0.2282 - acc: 0.9527 - mDice: 0.7889 - val_loss: 0.4807 - val_acc: 0.9414 - val_mDice: 0.6077

Epoch 00107: val_mDice did not improve from 0.62079
Epoch 108/300
 - 13s - loss: 0.2216 - acc: 0.9527 - mDice: 0.7896 - val_loss: 0.4693 - val_acc: 0.9430 - val_mDice: 0.6135

Epoch 00108: val_mDice did not improve from 0.62079
Epoch 109/300
 - 13s - loss: 0.2206 - acc: 0.9528 - mDice: 0.7905 - val_loss: 0.4712 - val_acc: 0.9420 - val_mDice: 0.6119

Epoch 00109: val_mDice did not improve from 0.62079
Epoch 110/300
 - 13s - loss: 0.2214 - acc: 0.9528 - mDice: 0.7898 - val_loss: 0.4754 - val_acc: 0.9433 - val_mDice: 0.6099

Epoch 00110: val_mDice did not improve from 0.62079
Epoch 111/300
 - 13s - loss: 0.2211 - acc: 0.9529 - mDice: 0.7902 - val_loss: 0.4753 - val_acc: 0.9426 - val_mDice: 0.6104

Epoch 00111: val_mDice did not improve from 0.62079
Epoch 112/300
 - 13s - loss: 0.2206 - acc: 0.9529 - mDice: 0.7906 - val_loss: 0.4670 - val_acc: 0.9420 - val_mDice: 0.6148

Epoch 00112: val_mDice did not improve from 0.62079
Epoch 113/300
 - 13s - loss: 0.2205 - acc: 0.9529 - mDice: 0.7906 - val_loss: 0.4671 - val_acc: 0.9428 - val_mDice: 0.6154

Epoch 00113: val_mDice did not improve from 0.62079
Epoch 114/300
 - 13s - loss: 0.2187 - acc: 0.9530 - mDice: 0.7920 - val_loss: 0.4839 - val_acc: 0.9419 - val_mDice: 0.6063

Epoch 00114: val_mDice did not improve from 0.62079
Epoch 115/300
 - 13s - loss: 0.2199 - acc: 0.9529 - mDice: 0.7911 - val_loss: 0.4680 - val_acc: 0.9422 - val_mDice: 0.6144

Epoch 00115: val_mDice did not improve from 0.62079
Epoch 116/300
 - 13s - loss: 0.2199 - acc: 0.9529 - mDice: 0.7911 - val_loss: 0.4657 - val_acc: 0.9424 - val_mDice: 0.6152

Epoch 00116: val_mDice did not improve from 0.62079
Epoch 117/300
 - 13s - loss: 0.2191 - acc: 0.9530 - mDice: 0.7917 - val_loss: 0.4620 - val_acc: 0.9441 - val_mDice: 0.6191

Epoch 00117: val_mDice did not improve from 0.62079
Epoch 118/300
 - 13s - loss: 0.2371 - acc: 0.9510 - mDice: 0.7794 - val_loss: 0.4718 - val_acc: 0.9426 - val_mDice: 0.6132

Epoch 00118: val_mDice did not improve from 0.62079
Restoring model weights from the end of the best epoch
Epoch 00118: early stopping
{'val_loss': [1.3351153154424167, 0.9233846718614752, 0.7737600902185083, 0.7028586446920181, 0.6714357411797671, 0.6046673416453887, 0.6091154976324602, 0.5798925075301512, 0.5753264605680252, 0.54236401051761, 0.5620068055104445, 0.5281253898526258, 0.5216375367845444, 0.526736810723728, 0.527206883392232, 0.5179529645863701, 0.5264616492279073, 0.5291314032626024, 0.520604034796118, 0.5079538337687121, 0.5037107601522762, 0.5049942813136361, 0.5041208724605846, 0.4988762065369815, 0.503208533487218, 0.4895826270554792, 0.4933104666477856, 0.5137819738630305, 0.4873249898938572, 0.4864961745585987, 0.48110775530019545, 0.4836314175536926, 0.4791513612882339, 0.49125107023167736, 0.4886069186231032, 0.4962934164758672, 0.48334121082555803, 0.4807531395379235, 0.4981653494312164, 0.4816369011121638, 0.48181853909543493, 0.47994871333958633, 0.4982950899371489, 0.49357048410145354, 0.4864308591832452, 0.47728699397913277, 0.4882601278032211, 0.49227962790326, 0.48711424667567493, 0.48673992345039857, 0.468705714227044, 0.4818577643703012, 0.4863018049275811, 0.4800313225085723, 0.48145895989183435, 0.46919326938409855, 0.482344207757297, 0.4760046617232542, 0.47588438114380455, 0.4677757835961918, 0.47516212377318723, 0.4858193628609499, 0.4784905564657507, 0.4775799457083411, 0.47407508358598394, 0.4825875741593978, 0.48709115210701437, 0.47388874719487156, 0.46900109715640226, 0.473926780536213, 0.4709749209052101, 0.4854377871528666, 0.4692055100106938, 0.46182363125729686, 0.4639421275911484, 0.47197016069595826, 0.4678970020722578, 0.45834679638638215, 0.4717743272449881, 0.4711381440812891, 0.46650519393344614, 0.4855568883890774, 0.4770284266395365, 0.48139881784903177, 0.4781800623246055, 0.4848732532345675, 0.4773840681116849, 0.47564947732629625, 0.4696942013215254, 0.46313606911802035, 0.4666772566058419, 0.46013785683535, 0.4597107540158665, 0.47393497697172315, 0.46979394491343573, 0.46903169521673477, 0.5345124455696759, 0.4724985679840659, 0.46388711712577124, 0.46607439865402994, 0.47691441697870346, 0.4825389871304048, 0.47248946616356385, 0.47322710981981003, 0.46704859673020677, 0.47897838143741384, 0.48068411449060083, 0.46931351219269046, 0.4712358641433206, 0.4753875971477937, 0.4752766011870481, 0.4669714732603593, 0.46710403765586606, 0.48391200243470506, 0.4680034632988792, 0.4656687402788968, 0.46202822643167835, 0.47181634836018405], 'val_acc': [0.9019674113727508, 0.911682492908947, 0.9166693238013568, 0.9190141934762026, 0.9248047668666126, 0.9290873417879808, 0.9316550410367588, 0.9328248277704984, 0.9307266569392566, 0.9370092701784429, 0.9364456072210628, 0.9352280535799934, 0.9379960365473905, 0.936135254760477, 0.936468137776788, 0.9381870139728893, 0.9373474870136077, 0.937919111812816, 0.9370570444168254, 0.9385252362266582, 0.9389138183491753, 0.9393382270068409, 0.9394018930547378, 0.9413303149575218, 0.9389363731292479, 0.9405677003018996, 0.9400875887131308, 0.9400769650617385, 0.9393952590896484, 0.9408714328220183, 0.9395796299618195, 0.942375433954963, 0.9396724551119269, 0.9394920672324889, 0.9400066841732372, 0.9412096139581446, 0.938360773305842, 0.9407573457070213, 0.939957612657292, 0.9411472866241944, 0.9413740765602193, 0.9402188824459831, 0.9437202753867695, 0.9403143716368446, 0.9418157488904535, 0.9401406540590174, 0.9414417230509182, 0.9404668957791864, 0.9397056239811494, 0.9403302803396542, 0.9417653501352524, 0.9407043240287087, 0.9406366756255614, 0.941086277923482, 0.9377891290634074, 0.9415730186962189, 0.9401353301211475, 0.9415279480225263, 0.9423661509299661, 0.9412122824612785, 0.9410279284824025, 0.9407520718115536, 0.9414457187933081, 0.9418674771161003, 0.9417573908433557, 0.9428608643817392, 0.9405358644092784, 0.9420969110121702, 0.9404721932615189, 0.9416592331493602, 0.9410438330415736, 0.9412308472363069, 0.9433502526206766, 0.9415491443266844, 0.94207037228314, 0.9422786047751891, 0.9416579027226902, 0.9429868462888952, 0.9423369854529273, 0.9413117348829055, 0.9411937106739391, 0.9412786058563599, 0.9419085686219567, 0.9409881288355048, 0.9401896981632009, 0.9402520420717045, 0.941378059871694, 0.9421141459980112, 0.941986838126565, 0.942005419476147, 0.9414098903457111, 0.9439298443615756, 0.9425929386348011, 0.9417109511752817, 0.9424510365501445, 0.9429245237360664, 0.9458397129640223, 0.9401565608493785, 0.9418223780744216, 0.9408143927706755, 0.9410570987405624, 0.9413117734506169, 0.9414443925102765, 0.9409562996364532, 0.9433489068944186, 0.9410305746736374, 0.9413634870141585, 0.9430239971946267, 0.9420239644892076, 0.9432613900638519, 0.942567756469237, 0.942016004559828, 0.9427825967895793, 0.9418780660246783, 0.9421990360805695, 0.9424311387985148, 0.944067789590295, 0.9425796818605718], 'val_mDice': [0.2842488037073676, 0.4015408333610086, 0.4613356429306581, 0.4870108802688313, 0.5044938676497516, 0.5355138510943734, 0.5392718780486979, 0.554766604607118, 0.5593485239355321, 0.5705042940410063, 0.5631267160655343, 0.5785353900914524, 0.5835642342898935, 0.5805784530818143, 0.5816763700010942, 0.5871691375492728, 0.5806007107948874, 0.5809285956270555, 0.5854980244356043, 0.5918732387496826, 0.5929500227943462, 0.5929717039041978, 0.5937920136247726, 0.5985214028128966, 0.5950830454494864, 0.6028372087580635, 0.6013154320538363, 0.5881597329588497, 0.602509890648133, 0.6039260001106058, 0.6065999008117513, 0.6062569732972007, 0.6075313419581735, 0.6013048657758988, 0.603394155515069, 0.60122420092955, 0.6064779411025226, 0.6078555332148139, 0.5995558060426763, 0.608920787744981, 0.6086372047822106, 0.6079186443339057, 0.5994575680258439, 0.6018116952263736, 0.6042738177559592, 0.6087078678416696, 0.6050399360172252, 0.6020120978355408, 0.6044247596659125, 0.6048703646277361, 0.6139500377012447, 0.6062319858826418, 0.6053348244192766, 0.608007174762175, 0.6061536242617643, 0.6145662429498479, 0.6073962516963163, 0.6117946859349541, 0.6121379290035064, 0.6151816232956667, 0.6110248878040415, 0.6060129254259528, 0.6103308510015355, 0.6095366554464249, 0.6127877904769571, 0.6074658291225128, 0.6033601072382799, 0.6118251021533089, 0.6137666638522224, 0.613538856493598, 0.6132213180077905, 0.6071569584907694, 0.6155460935862944, 0.618392566946101, 0.616483978727922, 0.6121551053409271, 0.6147658334058874, 0.6207854970891208, 0.613947216202231, 0.6122025662564976, 0.6155155948139129, 0.6072925373832172, 0.6099509912378648, 0.6066984110337528, 0.6079241129803785, 0.6048270910181464, 0.6091352050954645, 0.6111346508729905, 0.6140168328336216, 0.6174101166546664, 0.6149524985787703, 0.6199889753591568, 0.6193174445692868, 0.6117815091648203, 0.6130396816182264, 0.6139070943077618, 0.5877214641494547, 0.6107913099508234, 0.6151457209000613, 0.6142080384779741, 0.6090406941220085, 0.6059893621480401, 0.6121613494852648, 0.6113460494235238, 0.6150523519770984, 0.6082005121491172, 0.6077068674373117, 0.6134780632620827, 0.6118868159100334, 0.6098713116212324, 0.6104196772218388, 0.6148339761769708, 0.6153685533426663, 0.6062560785900463, 0.6144017204243869, 0.615225039063928, 0.6190664207234102, 0.6132195652487443], 'loss': [2.6516050496262165, 1.226626236959329, 0.9324020468890986, 0.7963344973489641, 0.6997408101364309, 0.6345872668781638, 0.5703663908929059, 0.5297053860061174, 0.5065384726808853, 0.47890481069291624, 0.45212993549658853, 0.4314307158478216, 0.41610947853100333, 0.402500160198473, 0.39322027636447765, 0.37850324782474015, 0.37494154062045004, 0.36440192042084246, 0.351431170849327, 0.3489199449167853, 0.3427577776726313, 0.33584218728172804, 0.3297733045646023, 0.32431726615082107, 0.3198801049012946, 0.3156495130209218, 0.3122558131352097, 0.3093269523882956, 0.3181771611625235, 0.3039311499616956, 0.30509613961858845, 0.29426208259517, 0.29257819451519845, 0.29097645427369595, 0.28788617942831557, 0.28358345628711434, 0.2815638679698632, 0.2872575946682895, 0.2898112654471673, 0.27862701420301644, 0.27475769321896015, 0.2715845175942484, 0.28145278463059226, 0.2752954705748912, 0.26884762514912924, 0.26743025011310145, 0.26527244665511274, 0.27168405817926783, 0.2604435761703971, 0.25989641739031444, 0.25979162930005867, 0.2653018371720157, 0.25712743210683653, 0.25593565741939245, 0.26733965691446554, 0.2578096420221525, 0.2530466009088142, 0.25098151972291305, 0.2492727572624172, 0.24801464129346834, 0.24789489235049514, 0.24826468639390692, 0.24875638283193313, 0.2466122636745465, 0.24630777604864193, 0.24366782304178625, 0.24367209859775182, 0.24179731791736017, 0.24309311533053013, 0.2442902913696274, 0.24031444547382785, 0.2400765865071037, 0.23947699761850563, 0.238030426171625, 0.24020249680439038, 0.2378657088805862, 0.23477785606250368, 0.23757923808636255, 0.23447657449143977, 0.23513443098478537, 0.2339729233590151, 0.2328523957278149, 0.23349481497871066, 0.23136037341818125, 0.23186608440048226, 0.23084906960915644, 0.23087657482581658, 0.23933408542209228, 0.22869640123379634, 0.2286126212367232, 0.22768722384221096, 0.22877316774183037, 0.22845687198894918, 0.22745703683773127, 0.22768190087115797, 0.22748315377180273, 0.22962407007583224, 0.22949483705706805, 0.22660257547627546, 0.22543594644549475, 0.2240678771755436, 0.22349320918527224, 0.22405744440938302, 0.2218557637141057, 0.22334415491584578, 0.22315018495848685, 0.2282213142416635, 0.22161944805069875, 0.22059421005117893, 0.22143312736573922, 0.2210512146528927, 0.22063344263663248, 0.22048308729934293, 0.21873890992625789, 0.2198793445473544, 0.21988183411567486, 0.2190991837624361, 0.23711188803602037], 'acc': [0.6597617487996431, 0.8819134251657375, 0.89002294816564, 0.8960064688271933, 0.9015534347839622, 0.9074376852388876, 0.9130223675043379, 0.9172949597678233, 0.9201050277713575, 0.922950471418266, 0.9267045244416717, 0.9294744398350411, 0.9313391750969283, 0.9329592699838234, 0.9341670772607907, 0.9357929046306234, 0.9363427757521827, 0.9374476748135565, 0.9386644046527398, 0.9391144051309485, 0.9398418470555429, 0.9404579721193552, 0.9409809956373597, 0.9416475614807763, 0.9421782005622081, 0.9426017851665501, 0.942890651012206, 0.9433246727831196, 0.9428338237492687, 0.9437057713287216, 0.9438063162383944, 0.9447237187682476, 0.9449767962267536, 0.9451483618980269, 0.9455237019086369, 0.9459824250757074, 0.9460771916072717, 0.9463823944889136, 0.946073912847912, 0.9464867977611341, 0.9468152588315706, 0.9472006491664408, 0.9463102889471042, 0.9466970738256282, 0.9474176238466081, 0.9477065848013759, 0.9479815436862159, 0.9479674698775716, 0.9482794246373795, 0.9485351512098715, 0.9484621522528774, 0.9486350955264398, 0.9487284841429514, 0.9487986406285148, 0.9482939798530065, 0.9486557986736066, 0.9490784685890432, 0.9494003047112323, 0.9495505460287922, 0.9496844209569175, 0.9497134833384065, 0.9496980382870011, 0.9496503932380361, 0.9499197848023183, 0.9499511884630578, 0.9502822042333988, 0.9502484178058886, 0.9503094867753653, 0.95034194868353, 0.9501955868574403, 0.9505592617855604, 0.950622258636025, 0.9506953080464309, 0.9508856993851676, 0.9505640798576046, 0.9508343628794027, 0.9511830723154777, 0.9509753298856765, 0.9512609153965705, 0.9511789742311691, 0.9513327358658883, 0.9513622585659808, 0.9513231911530429, 0.9515352513199123, 0.9515225223486676, 0.9517335948310688, 0.9517389674620406, 0.9514017351664558, 0.9519125630505497, 0.9518078487869973, 0.9520788050846993, 0.9518888000901685, 0.9519092382570129, 0.952036872339624, 0.9520608300880538, 0.9520714331236192, 0.9519173362699684, 0.9518545072867101, 0.9521738833315854, 0.9523066268255893, 0.9523622234388223, 0.9524052470572059, 0.9524031238089774, 0.9525853235431297, 0.952555607329211, 0.9525375793075413, 0.95267895133117, 0.9527249131034964, 0.9528117923611004, 0.9527976717074103, 0.9528589798889591, 0.9528790803521382, 0.9528582832079079, 0.9529505623995927, 0.9528919015886623, 0.9529377639253127, 0.9529812644387737, 0.9510220353462735], 'mDice': [0.1116564841366332, 0.3015913394406567, 0.3901585592766937, 0.44388623190579474, 0.48649075149204, 0.5199191904417825, 0.5520560923290837, 0.5758904722918228, 0.5897710075092065, 0.6052141968646303, 0.622871709494303, 0.6360533589605988, 0.6460427776067369, 0.6554741829723487, 0.6616534169474817, 0.6716374942737654, 0.6742989439946223, 0.6812543244475863, 0.6902013297980363, 0.6920802962408158, 0.6965853023628674, 0.7014922942101417, 0.7057902342538053, 0.709864272545151, 0.7131444479596475, 0.7162598128592629, 0.7187866838758419, 0.7212144657933275, 0.7188078655745751, 0.7250902281883865, 0.7244021651372075, 0.7321877527915751, 0.7335075586788069, 0.7347537149737544, 0.7370978583748354, 0.740382164540067, 0.7418337716107326, 0.7422153064905989, 0.740404408187262, 0.7440655689848349, 0.7470062830139789, 0.7495202620341518, 0.7444745993408004, 0.7466380372824468, 0.7515117477633935, 0.7528715789891485, 0.7546228355910083, 0.7540744292285442, 0.758205790756488, 0.7588670262209865, 0.7587597927214533, 0.759472199315036, 0.7608020569860918, 0.7618164665531503, 0.7571732268807446, 0.7604063346659102, 0.764096390186275, 0.7657480812746933, 0.7670681685797154, 0.7680699833279714, 0.7682319803773552, 0.767954300176459, 0.7676683577674782, 0.7692424262789755, 0.7695337131644069, 0.7715747999317872, 0.7716493218277092, 0.7730201560789984, 0.7722544252507576, 0.7713220061441284, 0.7742543534268305, 0.7745017189810892, 0.7749557449592699, 0.7761335604833266, 0.7748171201937053, 0.7763297590207641, 0.7788119495882259, 0.7766019726283765, 0.7789959586659392, 0.7785502928487877, 0.7796488740726597, 0.7803503107994479, 0.7798709213044435, 0.7815575102955384, 0.7812041775812596, 0.7820374153003483, 0.7820361757426716, 0.7799519042314959, 0.7837460987214131, 0.7838453255118115, 0.784614521355963, 0.7837252625280607, 0.7840167099017928, 0.784821376903865, 0.7846979454555035, 0.7848982138648092, 0.7834585065478405, 0.7830926662352768, 0.7855154190057916, 0.7865241693345281, 0.7875851248295497, 0.78807680468531, 0.7876412649507275, 0.7893653186553075, 0.7882377413152988, 0.7884094919978605, 0.7889097578531798, 0.789621332043659, 0.7904530267142749, 0.7898227267959886, 0.7901952850924047, 0.7906064256184442, 0.7905984844043366, 0.7919694090211558, 0.7910577454843578, 0.7911099767722005, 0.7917080816450185, 0.7794034464831209]}
predicting test subjects:   0%|          | 0/5 [00:00<?, ?it/s]predicting test subjects:  20%|██        | 1/5 [00:02<00:10,  2.68s/it]predicting test subjects:  40%|████      | 2/5 [00:04<00:07,  2.51s/it]predicting test subjects:  60%|██████    | 3/5 [00:06<00:04,  2.31s/it]predicting test subjects:  80%|████████  | 4/5 [00:08<00:02,  2.18s/it]predicting test subjects: 100%|██████████| 5/5 [00:10<00:00,  2.22s/it]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:02<11:34,  2.62s/it]predicting train subjects:   1%|          | 2/266 [00:05<11:20,  2.58s/it]predicting train subjects:   1%|          | 3/266 [00:07<10:39,  2.43s/it]predicting train subjects:   2%|▏         | 4/266 [00:09<09:53,  2.27s/it]predicting train subjects:   2%|▏         | 5/266 [00:11<10:07,  2.33s/it]predicting train subjects:   2%|▏         | 6/266 [00:14<10:41,  2.47s/it]predicting train subjects:   3%|▎         | 7/266 [00:16<10:48,  2.50s/it]predicting train subjects:   3%|▎         | 8/266 [00:19<11:03,  2.57s/it]predicting train subjects:   3%|▎         | 9/266 [00:22<11:12,  2.62s/it]predicting train subjects:   4%|▍         | 10/266 [00:25<11:10,  2.62s/it]predicting train subjects:   4%|▍         | 11/266 [00:27<11:12,  2.64s/it]predicting train subjects:   5%|▍         | 12/266 [00:30<11:07,  2.63s/it]predicting train subjects:   5%|▍         | 13/266 [00:32<11:08,  2.64s/it]predicting train subjects:   5%|▌         | 14/266 [00:35<11:02,  2.63s/it]predicting train subjects:   6%|▌         | 15/266 [00:38<10:59,  2.63s/it]predicting train subjects:   6%|▌         | 16/266 [00:40<10:56,  2.63s/it]predicting train subjects:   6%|▋         | 17/266 [00:43<10:51,  2.62s/it]predicting train subjects:   7%|▋         | 18/266 [00:46<10:51,  2.63s/it]predicting train subjects:   7%|▋         | 19/266 [00:48<10:53,  2.64s/it]predicting train subjects:   8%|▊         | 20/266 [00:51<10:50,  2.65s/it]predicting train subjects:   8%|▊         | 21/266 [00:53<10:44,  2.63s/it]predicting train subjects:   8%|▊         | 22/266 [00:56<10:42,  2.63s/it]predicting train subjects:   9%|▊         | 23/266 [00:59<10:49,  2.67s/it]predicting train subjects:   9%|▉         | 24/266 [01:01<10:29,  2.60s/it]predicting train subjects:   9%|▉         | 25/266 [01:04<10:20,  2.57s/it]predicting train subjects:  10%|▉         | 26/266 [01:06<10:09,  2.54s/it]predicting train subjects:  10%|█         | 27/266 [01:09<10:07,  2.54s/it]predicting train subjects:  11%|█         | 28/266 [01:11<09:58,  2.52s/it]predicting train subjects:  11%|█         | 29/266 [01:14<09:50,  2.49s/it]predicting train subjects:  11%|█▏        | 30/266 [01:16<09:43,  2.47s/it]predicting train subjects:  12%|█▏        | 31/266 [01:19<09:35,  2.45s/it]predicting train subjects:  12%|█▏        | 32/266 [01:21<09:33,  2.45s/it]predicting train subjects:  12%|█▏        | 33/266 [01:24<09:34,  2.46s/it]predicting train subjects:  13%|█▎        | 34/266 [01:26<09:29,  2.45s/it]predicting train subjects:  13%|█▎        | 35/266 [01:28<09:26,  2.45s/it]predicting train subjects:  14%|█▎        | 36/266 [01:31<09:21,  2.44s/it]predicting train subjects:  14%|█▍        | 37/266 [01:33<09:21,  2.45s/it]predicting train subjects:  14%|█▍        | 38/266 [01:36<09:22,  2.47s/it]predicting train subjects:  15%|█▍        | 39/266 [01:38<09:16,  2.45s/it]predicting train subjects:  15%|█▌        | 40/266 [01:41<09:14,  2.45s/it]predicting train subjects:  15%|█▌        | 41/266 [01:43<09:10,  2.45s/it]predicting train subjects:  16%|█▌        | 42/266 [01:45<08:42,  2.33s/it]predicting train subjects:  16%|█▌        | 43/266 [01:47<08:19,  2.24s/it]predicting train subjects:  17%|█▋        | 44/266 [01:49<08:00,  2.17s/it]predicting train subjects:  17%|█▋        | 45/266 [01:51<07:47,  2.11s/it]predicting train subjects:  17%|█▋        | 46/266 [01:53<07:39,  2.09s/it]predicting train subjects:  18%|█▊        | 47/266 [01:55<07:31,  2.06s/it]predicting train subjects:  18%|█▊        | 48/266 [01:57<07:28,  2.06s/it]predicting train subjects:  18%|█▊        | 49/266 [01:59<07:25,  2.05s/it]predicting train subjects:  19%|█▉        | 50/266 [02:01<07:23,  2.05s/it]predicting train subjects:  19%|█▉        | 51/266 [02:03<07:19,  2.04s/it]predicting train subjects:  20%|█▉        | 52/266 [02:05<07:13,  2.03s/it]predicting train subjects:  20%|█▉        | 53/266 [02:07<07:08,  2.01s/it]predicting train subjects:  20%|██        | 54/266 [02:09<07:08,  2.02s/it]predicting train subjects:  21%|██        | 55/266 [02:11<07:06,  2.02s/it]predicting train subjects:  21%|██        | 56/266 [02:13<07:04,  2.02s/it]predicting train subjects:  21%|██▏       | 57/266 [02:15<07:02,  2.02s/it]predicting train subjects:  22%|██▏       | 58/266 [02:17<07:00,  2.02s/it]predicting train subjects:  22%|██▏       | 59/266 [02:19<06:55,  2.01s/it]predicting train subjects:  23%|██▎       | 60/266 [02:21<06:51,  2.00s/it]predicting train subjects:  23%|██▎       | 61/266 [02:23<06:41,  1.96s/it]predicting train subjects:  23%|██▎       | 62/266 [02:25<06:34,  1.93s/it]predicting train subjects:  24%|██▎       | 63/266 [02:27<06:29,  1.92s/it]predicting train subjects:  24%|██▍       | 64/266 [02:29<06:26,  1.91s/it]predicting train subjects:  24%|██▍       | 65/266 [02:31<06:25,  1.92s/it]predicting train subjects:  25%|██▍       | 66/266 [02:33<06:20,  1.90s/it]predicting train subjects:  25%|██▌       | 67/266 [02:35<06:20,  1.91s/it]predicting train subjects:  26%|██▌       | 68/266 [02:37<06:18,  1.91s/it]predicting train subjects:  26%|██▌       | 69/266 [02:38<06:13,  1.89s/it]predicting train subjects:  26%|██▋       | 70/266 [02:40<06:10,  1.89s/it]predicting train subjects:  27%|██▋       | 71/266 [02:42<06:07,  1.89s/it]predicting train subjects:  27%|██▋       | 72/266 [02:44<06:05,  1.88s/it]predicting train subjects:  27%|██▋       | 73/266 [02:46<06:02,  1.88s/it]predicting train subjects:  28%|██▊       | 74/266 [02:48<06:01,  1.88s/it]predicting train subjects:  28%|██▊       | 75/266 [02:50<05:58,  1.88s/it]predicting train subjects:  29%|██▊       | 76/266 [02:52<05:56,  1.88s/it]predicting train subjects:  29%|██▉       | 77/266 [02:54<05:59,  1.90s/it]predicting train subjects:  29%|██▉       | 78/266 [02:56<06:32,  2.09s/it]predicting train subjects:  30%|██▉       | 79/266 [02:58<06:50,  2.19s/it]predicting train subjects:  30%|███       | 80/266 [03:01<07:06,  2.29s/it]predicting train subjects:  30%|███       | 81/266 [03:03<07:11,  2.34s/it]predicting train subjects:  31%|███       | 82/266 [03:06<07:23,  2.41s/it]predicting train subjects:  31%|███       | 83/266 [03:08<07:23,  2.42s/it]predicting train subjects:  32%|███▏      | 84/266 [03:11<07:24,  2.44s/it]predicting train subjects:  32%|███▏      | 85/266 [03:13<07:25,  2.46s/it]predicting train subjects:  32%|███▏      | 86/266 [03:16<07:22,  2.46s/it]predicting train subjects:  33%|███▎      | 87/266 [03:18<07:19,  2.46s/it]predicting train subjects:  33%|███▎      | 88/266 [03:21<07:19,  2.47s/it]predicting train subjects:  33%|███▎      | 89/266 [03:23<07:15,  2.46s/it]predicting train subjects:  34%|███▍      | 90/266 [03:26<07:16,  2.48s/it]predicting train subjects:  34%|███▍      | 91/266 [03:28<07:13,  2.48s/it]predicting train subjects:  35%|███▍      | 92/266 [03:31<07:08,  2.46s/it]predicting train subjects:  35%|███▍      | 93/266 [03:33<07:12,  2.50s/it]predicting train subjects:  35%|███▌      | 94/266 [03:36<07:08,  2.49s/it]predicting train subjects:  36%|███▌      | 95/266 [03:38<07:03,  2.48s/it]predicting train subjects:  36%|███▌      | 96/266 [03:40<06:41,  2.36s/it]predicting train subjects:  36%|███▋      | 97/266 [03:43<06:45,  2.40s/it]predicting train subjects:  37%|███▋      | 98/266 [03:45<06:43,  2.40s/it]predicting train subjects:  37%|███▋      | 99/266 [03:47<06:09,  2.21s/it]predicting train subjects:  38%|███▊      | 100/266 [03:49<05:56,  2.15s/it]predicting train subjects:  38%|███▊      | 101/266 [03:51<05:53,  2.14s/it]predicting train subjects:  38%|███▊      | 102/266 [03:53<05:50,  2.14s/it]predicting train subjects:  39%|███▊      | 103/266 [03:55<05:45,  2.12s/it]predicting train subjects:  39%|███▉      | 104/266 [03:57<05:44,  2.12s/it]predicting train subjects:  39%|███▉      | 105/266 [04:00<05:42,  2.13s/it]predicting train subjects:  40%|███▉      | 106/266 [04:02<05:34,  2.09s/it]predicting train subjects:  40%|████      | 107/266 [04:04<05:31,  2.08s/it]predicting train subjects:  41%|████      | 108/266 [04:06<05:28,  2.08s/it]predicting train subjects:  41%|████      | 109/266 [04:08<05:26,  2.08s/it]predicting train subjects:  41%|████▏     | 110/266 [04:10<05:25,  2.09s/it]predicting train subjects:  42%|████▏     | 111/266 [04:12<05:27,  2.11s/it]predicting train subjects:  42%|████▏     | 112/266 [04:14<05:25,  2.11s/it]predicting train subjects:  42%|████▏     | 113/266 [04:16<05:20,  2.10s/it]predicting train subjects:  43%|████▎     | 114/266 [04:18<05:15,  2.08s/it]predicting train subjects:  43%|████▎     | 115/266 [04:20<05:14,  2.08s/it]predicting train subjects:  44%|████▎     | 116/266 [04:22<05:11,  2.08s/it]predicting train subjects:  44%|████▍     | 117/266 [04:25<05:11,  2.09s/it]predicting train subjects:  44%|████▍     | 118/266 [04:27<05:08,  2.09s/it]predicting train subjects:  45%|████▍     | 119/266 [04:29<05:20,  2.18s/it]predicting train subjects:  45%|████▌     | 120/266 [04:31<05:29,  2.25s/it]predicting train subjects:  45%|████▌     | 121/266 [04:34<05:34,  2.31s/it]predicting train subjects:  46%|████▌     | 122/266 [04:36<05:39,  2.36s/it]predicting train subjects:  46%|████▌     | 123/266 [04:39<05:41,  2.39s/it]predicting train subjects:  47%|████▋     | 124/266 [04:41<05:40,  2.40s/it]predicting train subjects:  47%|████▋     | 125/266 [04:44<05:40,  2.42s/it]predicting train subjects:  47%|████▋     | 126/266 [04:46<05:39,  2.43s/it]predicting train subjects:  48%|████▊     | 127/266 [04:49<05:38,  2.44s/it]predicting train subjects:  48%|████▊     | 128/266 [04:51<05:35,  2.43s/it]predicting train subjects:  48%|████▊     | 129/266 [04:53<05:32,  2.42s/it]predicting train subjects:  49%|████▉     | 130/266 [04:56<05:30,  2.43s/it]predicting train subjects:  49%|████▉     | 131/266 [04:58<05:28,  2.43s/it]predicting train subjects:  50%|████▉     | 132/266 [05:01<05:26,  2.43s/it]predicting train subjects:  50%|█████     | 133/266 [05:03<05:24,  2.44s/it]predicting train subjects:  50%|█████     | 134/266 [05:06<05:23,  2.45s/it]predicting train subjects:  51%|█████     | 135/266 [05:08<05:19,  2.44s/it]predicting train subjects:  51%|█████     | 136/266 [05:11<05:17,  2.44s/it]predicting train subjects:  52%|█████▏    | 137/266 [05:13<05:13,  2.43s/it]predicting train subjects:  52%|█████▏    | 138/266 [05:15<05:07,  2.40s/it]predicting train subjects:  52%|█████▏    | 139/266 [05:18<05:02,  2.38s/it]predicting train subjects:  53%|█████▎    | 140/266 [05:20<04:55,  2.34s/it]predicting train subjects:  53%|█████▎    | 141/266 [05:22<04:54,  2.36s/it]predicting train subjects:  53%|█████▎    | 142/266 [05:25<04:55,  2.38s/it]predicting train subjects:  54%|█████▍    | 143/266 [05:27<04:51,  2.37s/it]predicting train subjects:  54%|█████▍    | 144/266 [05:29<04:49,  2.37s/it]predicting train subjects:  55%|█████▍    | 145/266 [05:32<04:51,  2.41s/it]predicting train subjects:  55%|█████▍    | 146/266 [05:34<04:49,  2.41s/it]predicting train subjects:  55%|█████▌    | 147/266 [05:37<04:48,  2.42s/it]predicting train subjects:  56%|█████▌    | 148/266 [05:39<04:46,  2.43s/it]predicting train subjects:  56%|█████▌    | 149/266 [05:42<04:42,  2.41s/it]predicting train subjects:  56%|█████▋    | 150/266 [05:44<04:39,  2.41s/it]predicting train subjects:  57%|█████▋    | 151/266 [05:46<04:37,  2.41s/it]predicting train subjects:  57%|█████▋    | 152/266 [05:49<04:35,  2.41s/it]predicting train subjects:  58%|█████▊    | 153/266 [05:51<04:30,  2.39s/it]predicting train subjects:  58%|█████▊    | 154/266 [05:54<04:29,  2.41s/it]predicting train subjects:  58%|█████▊    | 155/266 [05:55<04:04,  2.21s/it]predicting train subjects:  59%|█████▊    | 156/266 [05:57<03:46,  2.06s/it]predicting train subjects:  59%|█████▉    | 157/266 [05:59<03:32,  1.95s/it]predicting train subjects:  59%|█████▉    | 158/266 [06:01<03:23,  1.89s/it]predicting train subjects:  60%|█████▉    | 159/266 [06:02<03:18,  1.86s/it]predicting train subjects:  60%|██████    | 160/266 [06:04<03:11,  1.80s/it]predicting train subjects:  61%|██████    | 161/266 [06:06<03:08,  1.79s/it]predicting train subjects:  61%|██████    | 162/266 [06:07<03:05,  1.78s/it]predicting train subjects:  61%|██████▏   | 163/266 [06:09<03:02,  1.77s/it]predicting train subjects:  62%|██████▏   | 164/266 [06:11<02:59,  1.76s/it]predicting train subjects:  62%|██████▏   | 165/266 [06:13<02:56,  1.75s/it]predicting train subjects:  62%|██████▏   | 166/266 [06:14<02:55,  1.75s/it]predicting train subjects:  63%|██████▎   | 167/266 [06:16<02:52,  1.75s/it]predicting train subjects:  63%|██████▎   | 168/266 [06:18<02:50,  1.74s/it]predicting train subjects:  64%|██████▎   | 169/266 [06:20<02:47,  1.73s/it]predicting train subjects:  64%|██████▍   | 170/266 [06:21<02:47,  1.74s/it]predicting train subjects:  64%|██████▍   | 171/266 [06:23<02:47,  1.76s/it]predicting train subjects:  65%|██████▍   | 172/266 [06:25<02:46,  1.77s/it]predicting train subjects:  65%|██████▌   | 173/266 [06:27<02:49,  1.83s/it]predicting train subjects:  65%|██████▌   | 174/266 [06:29<02:51,  1.86s/it]predicting train subjects:  66%|██████▌   | 175/266 [06:31<02:51,  1.88s/it]predicting train subjects:  66%|██████▌   | 176/266 [06:33<02:51,  1.90s/it]predicting train subjects:  67%|██████▋   | 177/266 [06:35<02:50,  1.91s/it]predicting train subjects:  67%|██████▋   | 178/266 [06:37<02:48,  1.91s/it]predicting train subjects:  67%|██████▋   | 179/266 [06:39<02:48,  1.93s/it]predicting train subjects:  68%|██████▊   | 180/266 [06:41<02:46,  1.94s/it]predicting train subjects:  68%|██████▊   | 181/266 [06:43<02:45,  1.94s/it]predicting train subjects:  68%|██████▊   | 182/266 [06:44<02:43,  1.94s/it]predicting train subjects:  69%|██████▉   | 183/266 [06:46<02:41,  1.94s/it]predicting train subjects:  69%|██████▉   | 184/266 [06:48<02:40,  1.95s/it]predicting train subjects:  70%|██████▉   | 185/266 [06:50<02:38,  1.95s/it]predicting train subjects:  70%|██████▉   | 186/266 [06:52<02:36,  1.95s/it]predicting train subjects:  70%|███████   | 187/266 [06:54<02:34,  1.95s/it]predicting train subjects:  71%|███████   | 188/266 [06:56<02:31,  1.94s/it]predicting train subjects:  71%|███████   | 189/266 [06:58<02:29,  1.94s/it]predicting train subjects:  71%|███████▏  | 190/266 [07:00<02:27,  1.94s/it]predicting train subjects:  72%|███████▏  | 191/266 [07:02<02:28,  1.98s/it]predicting train subjects:  72%|███████▏  | 192/266 [07:04<02:24,  1.96s/it]predicting train subjects:  73%|███████▎  | 193/266 [07:06<02:20,  1.93s/it]predicting train subjects:  73%|███████▎  | 194/266 [07:08<02:31,  2.11s/it]predicting train subjects:  73%|███████▎  | 195/266 [07:10<02:28,  2.10s/it]predicting train subjects:  74%|███████▎  | 196/266 [07:13<02:27,  2.11s/it]predicting train subjects:  74%|███████▍  | 197/266 [07:15<02:25,  2.11s/it]predicting train subjects:  74%|███████▍  | 198/266 [07:17<02:22,  2.09s/it]predicting train subjects:  75%|███████▍  | 199/266 [07:19<02:20,  2.09s/it]predicting train subjects:  75%|███████▌  | 200/266 [07:21<02:18,  2.10s/it]predicting train subjects:  76%|███████▌  | 201/266 [07:23<02:17,  2.11s/it]predicting train subjects:  76%|███████▌  | 202/266 [07:25<02:13,  2.08s/it]predicting train subjects:  76%|███████▋  | 203/266 [07:27<02:09,  2.06s/it]predicting train subjects:  77%|███████▋  | 204/266 [07:29<02:08,  2.07s/it]predicting train subjects:  77%|███████▋  | 205/266 [07:31<02:06,  2.08s/it]predicting train subjects:  77%|███████▋  | 206/266 [07:33<02:05,  2.09s/it]predicting train subjects:  78%|███████▊  | 207/266 [07:35<02:02,  2.07s/it]predicting train subjects:  78%|███████▊  | 208/266 [07:38<02:00,  2.07s/it]predicting train subjects:  79%|███████▊  | 209/266 [07:40<01:58,  2.07s/it]predicting train subjects:  79%|███████▉  | 210/266 [07:42<01:56,  2.07s/it]predicting train subjects:  79%|███████▉  | 211/266 [07:44<01:54,  2.08s/it]predicting train subjects:  80%|███████▉  | 212/266 [07:46<01:52,  2.08s/it]predicting train subjects:  80%|████████  | 213/266 [07:48<01:46,  2.01s/it]predicting train subjects:  80%|████████  | 214/266 [07:50<01:41,  1.94s/it]predicting train subjects:  81%|████████  | 215/266 [07:51<01:36,  1.90s/it]predicting train subjects:  81%|████████  | 216/266 [07:53<01:33,  1.87s/it]predicting train subjects:  82%|████████▏ | 217/266 [07:55<01:31,  1.87s/it]predicting train subjects:  82%|████████▏ | 218/266 [07:57<01:29,  1.86s/it]predicting train subjects:  82%|████████▏ | 219/266 [07:59<01:26,  1.85s/it]predicting train subjects:  83%|████████▎ | 220/266 [08:00<01:24,  1.84s/it]predicting train subjects:  83%|████████▎ | 221/266 [08:02<01:23,  1.86s/it]predicting train subjects:  83%|████████▎ | 222/266 [08:04<01:22,  1.87s/it]predicting train subjects:  84%|████████▍ | 223/266 [08:06<01:20,  1.87s/it]predicting train subjects:  84%|████████▍ | 224/266 [08:08<01:18,  1.87s/it]predicting train subjects:  85%|████████▍ | 225/266 [08:10<01:15,  1.85s/it]predicting train subjects:  85%|████████▍ | 226/266 [08:12<01:13,  1.84s/it]predicting train subjects:  85%|████████▌ | 227/266 [08:13<01:11,  1.84s/it]predicting train subjects:  86%|████████▌ | 228/266 [08:15<01:10,  1.86s/it]predicting train subjects:  86%|████████▌ | 229/266 [08:17<01:09,  1.87s/it]predicting train subjects:  86%|████████▋ | 230/266 [08:19<01:07,  1.88s/it]predicting train subjects:  87%|████████▋ | 231/266 [08:21<01:05,  1.87s/it]predicting train subjects:  87%|████████▋ | 232/266 [08:23<01:03,  1.87s/it]predicting train subjects:  88%|████████▊ | 233/266 [08:25<01:01,  1.87s/it]predicting train subjects:  88%|████████▊ | 234/266 [08:27<01:00,  1.88s/it]predicting train subjects:  88%|████████▊ | 235/266 [08:29<00:58,  1.87s/it]predicting train subjects:  89%|████████▊ | 236/266 [08:30<00:55,  1.86s/it]predicting train subjects:  89%|████████▉ | 237/266 [08:32<00:53,  1.85s/it]predicting train subjects:  89%|████████▉ | 238/266 [08:34<00:52,  1.86s/it]predicting train subjects:  90%|████████▉ | 239/266 [08:36<00:50,  1.87s/it]predicting train subjects:  90%|█████████ | 240/266 [08:38<00:48,  1.88s/it]predicting train subjects:  91%|█████████ | 241/266 [08:40<00:46,  1.88s/it]predicting train subjects:  91%|█████████ | 242/266 [08:42<00:44,  1.87s/it]predicting train subjects:  91%|█████████▏| 243/266 [08:43<00:43,  1.88s/it]predicting train subjects:  92%|█████████▏| 244/266 [08:45<00:41,  1.87s/it]predicting train subjects:  92%|█████████▏| 245/266 [08:47<00:39,  1.88s/it]predicting train subjects:  92%|█████████▏| 246/266 [08:49<00:37,  1.88s/it]predicting train subjects:  93%|█████████▎| 247/266 [08:51<00:35,  1.89s/it]predicting train subjects:  93%|█████████▎| 248/266 [08:53<00:34,  1.90s/it]predicting train subjects:  94%|█████████▎| 249/266 [08:55<00:35,  2.09s/it]predicting train subjects:  94%|█████████▍| 250/266 [08:58<00:35,  2.20s/it]predicting train subjects:  94%|█████████▍| 251/266 [09:00<00:33,  2.25s/it]predicting train subjects:  95%|█████████▍| 252/266 [09:03<00:31,  2.28s/it]predicting train subjects:  95%|█████████▌| 253/266 [09:05<00:30,  2.35s/it]predicting train subjects:  95%|█████████▌| 254/266 [09:08<00:28,  2.39s/it]predicting train subjects:  96%|█████████▌| 255/266 [09:10<00:26,  2.42s/it]predicting train subjects:  96%|█████████▌| 256/266 [09:13<00:24,  2.43s/it]predicting train subjects:  97%|█████████▋| 257/266 [09:15<00:22,  2.44s/it]predicting train subjects:  97%|█████████▋| 258/266 [09:17<00:19,  2.45s/it]predicting train subjects:  97%|█████████▋| 259/266 [09:20<00:17,  2.46s/it]predicting train subjects:  98%|█████████▊| 260/266 [09:22<00:14,  2.46s/it]predicting train subjects:  98%|█████████▊| 261/266 [09:25<00:12,  2.48s/it]predicting train subjects:  98%|█████████▊| 262/266 [09:27<00:09,  2.46s/it]predicting train subjects:  99%|█████████▉| 263/266 [09:30<00:07,  2.46s/it]predicting train subjects:  99%|█████████▉| 264/266 [09:32<00:04,  2.48s/it]predicting train subjects: 100%|█████████▉| 265/266 [09:35<00:02,  2.50s/it]predicting train subjects: 100%|██████████| 266/266 [09:37<00:00,  2.51s/it]

