2019-07-28 20:34:02.810825: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-28 20:34:03.875297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:85:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-07-28 20:34:03.875348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-28 20:34:04.242815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 20:34:04.242864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-28 20:34:04.242877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-28 20:34:04.243331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:25,  1.55it/s]Loading train:   5%|▍         | 2/41 [00:01<00:24,  1.62it/s]Loading train:   7%|▋         | 3/41 [00:01<00:23,  1.65it/s]Loading train:  10%|▉         | 4/41 [00:02<00:21,  1.72it/s]Loading train:  12%|█▏        | 5/41 [00:02<00:22,  1.63it/s]Loading train:  15%|█▍        | 6/41 [00:03<00:20,  1.68it/s]Loading train:  17%|█▋        | 7/41 [00:04<00:19,  1.76it/s]Loading train:  20%|█▉        | 8/41 [00:04<00:17,  1.93it/s]Loading train:  22%|██▏       | 9/41 [00:05<00:17,  1.84it/s]Loading train:  24%|██▍       | 10/41 [00:05<00:17,  1.80it/s]Loading train:  27%|██▋       | 11/41 [00:06<00:17,  1.68it/s]Loading train:  29%|██▉       | 12/41 [00:06<00:15,  1.83it/s]Loading train:  32%|███▏      | 13/41 [00:07<00:15,  1.85it/s]Loading train:  34%|███▍      | 14/41 [00:07<00:14,  1.81it/s]Loading train:  37%|███▋      | 15/41 [00:08<00:13,  1.92it/s]Loading train:  39%|███▉      | 16/41 [00:08<00:13,  1.88it/s]Loading train:  41%|████▏     | 17/41 [00:09<00:13,  1.77it/s]Loading train:  44%|████▍     | 18/41 [00:10<00:13,  1.72it/s]Loading train:  46%|████▋     | 19/41 [00:10<00:12,  1.73it/s]Loading train:  49%|████▉     | 20/41 [00:11<00:11,  1.87it/s]Loading train:  51%|█████     | 21/41 [00:11<00:11,  1.81it/s]Loading train:  54%|█████▎    | 22/41 [00:12<00:10,  1.82it/s]Loading train:  56%|█████▌    | 23/41 [00:12<00:09,  1.97it/s]Loading train:  59%|█████▊    | 24/41 [00:13<00:08,  1.97it/s]Loading train:  61%|██████    | 25/41 [00:13<00:08,  1.95it/s]Loading train:  63%|██████▎   | 26/41 [00:14<00:07,  1.93it/s]Loading train:  66%|██████▌   | 27/41 [00:14<00:07,  1.82it/s]Loading train:  68%|██████▊   | 28/41 [00:15<00:07,  1.76it/s]Loading train:  71%|███████   | 29/41 [00:15<00:06,  1.85it/s]Loading train:  73%|███████▎  | 30/41 [00:16<00:06,  1.72it/s]Loading train:  76%|███████▌  | 31/41 [00:17<00:05,  1.79it/s]Loading train:  78%|███████▊  | 32/41 [00:17<00:04,  1.83it/s]Loading train:  80%|████████  | 33/41 [00:18<00:04,  1.78it/s]Loading train:  83%|████████▎ | 34/41 [00:18<00:03,  1.78it/s]Loading train:  85%|████████▌ | 35/41 [00:19<00:03,  1.62it/s]Loading train:  88%|████████▊ | 36/41 [00:20<00:03,  1.60it/s]Loading train:  90%|█████████ | 37/41 [00:20<00:02,  1.64it/s]Loading train:  93%|█████████▎| 38/41 [00:21<00:01,  1.68it/s]Loading train:  95%|█████████▌| 39/41 [00:21<00:01,  1.76it/s]Loading train:  98%|█████████▊| 40/41 [00:22<00:00,  1.78it/s]Loading train: 100%|██████████| 41/41 [00:22<00:00,  1.81it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 3/41 [00:00<00:01, 21.48it/s]concatenating: train:  15%|█▍        | 6/41 [00:00<00:01, 23.24it/s]concatenating: train:  27%|██▋       | 11/41 [00:00<00:01, 27.34it/s]concatenating: train:  37%|███▋      | 15/41 [00:00<00:00, 30.18it/s]concatenating: train:  54%|█████▎    | 22/41 [00:00<00:00, 35.72it/s]concatenating: train:  66%|██████▌   | 27/41 [00:00<00:00, 37.96it/s]concatenating: train:  78%|███████▊  | 32/41 [00:00<00:00, 40.60it/s]concatenating: train:  90%|█████████ | 37/41 [00:00<00:00, 37.59it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 43.08it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:05,  1.83it/s]Loading test:  18%|█▊        | 2/11 [00:01<00:05,  1.76it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:04,  1.76it/s]Loading test:  36%|███▋      | 4/11 [00:02<00:03,  1.91it/s]Loading test:  45%|████▌     | 5/11 [00:02<00:03,  1.84it/s]Loading test:  55%|█████▍    | 6/11 [00:03<00:02,  1.84it/s]Loading test:  64%|██████▎   | 7/11 [00:03<00:02,  1.84it/s]Loading test:  73%|███████▎  | 8/11 [00:04<00:01,  1.90it/s]Loading test:  82%|████████▏ | 9/11 [00:04<00:01,  1.95it/s]Loading test:  91%|█████████ | 10/11 [00:05<00:00,  1.76it/s]Loading test: 100%|██████████| 11/11 [00:06<00:00,  1.73it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  27%|██▋       | 3/11 [00:00<00:00, 27.86it/s]concatenating: validation:  73%|███████▎  | 8/11 [00:00<00:00, 30.92it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 35.23it/s]
Loading trainS:   0%|          | 0/41 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/41 [00:00<00:20,  1.95it/s]Loading trainS:   5%|▍         | 2/41 [00:01<00:20,  1.89it/s]Loading trainS:   7%|▋         | 3/41 [00:01<00:20,  1.85it/s]Loading trainS:  10%|▉         | 4/41 [00:02<00:19,  1.89it/s]Loading trainS:  12%|█▏        | 5/41 [00:02<00:20,  1.76it/s]Loading trainS:  15%|█▍        | 6/41 [00:03<00:20,  1.74it/s]Loading trainS:  17%|█▋        | 7/41 [00:03<00:18,  1.81it/s]Loading trainS:  20%|█▉        | 8/41 [00:04<00:17,  1.90it/s]Loading trainS:  22%|██▏       | 9/41 [00:04<00:16,  1.89it/s]Loading trainS:  24%|██▍       | 10/41 [00:05<00:17,  1.78it/s]Loading trainS:  27%|██▋       | 11/41 [00:06<00:17,  1.72it/s]Loading trainS:  29%|██▉       | 12/41 [00:06<00:15,  1.86it/s]Loading trainS:  32%|███▏      | 13/41 [00:07<00:15,  1.82it/s]Loading trainS:  34%|███▍      | 14/41 [00:07<00:14,  1.80it/s]Loading trainS:  37%|███▋      | 15/41 [00:08<00:14,  1.78it/s]Loading trainS:  39%|███▉      | 16/41 [00:08<00:14,  1.75it/s]Loading trainS:  41%|████▏     | 17/41 [00:09<00:13,  1.77it/s]Loading trainS:  44%|████▍     | 18/41 [00:10<00:13,  1.74it/s]Loading trainS:  46%|████▋     | 19/41 [00:10<00:12,  1.76it/s]Loading trainS:  49%|████▉     | 20/41 [00:11<00:11,  1.89it/s]Loading trainS:  51%|█████     | 21/41 [00:11<00:10,  1.91it/s]Loading trainS:  54%|█████▎    | 22/41 [00:12<00:10,  1.84it/s]Loading trainS:  56%|█████▌    | 23/41 [00:12<00:09,  1.83it/s]Loading trainS:  59%|█████▊    | 24/41 [00:13<00:09,  1.87it/s]Loading trainS:  61%|██████    | 25/41 [00:13<00:08,  1.99it/s]Loading trainS:  63%|██████▎   | 26/41 [00:14<00:07,  1.98it/s]Loading trainS:  66%|██████▌   | 27/41 [00:14<00:07,  1.89it/s]Loading trainS:  68%|██████▊   | 28/41 [00:15<00:08,  1.57it/s]Loading trainS:  71%|███████   | 29/41 [00:16<00:07,  1.69it/s]Loading trainS:  73%|███████▎  | 30/41 [00:16<00:06,  1.69it/s]Loading trainS:  76%|███████▌  | 31/41 [00:17<00:05,  1.72it/s]Loading trainS:  78%|███████▊  | 32/41 [00:17<00:05,  1.79it/s]Loading trainS:  80%|████████  | 33/41 [00:18<00:04,  1.71it/s]Loading trainS:  83%|████████▎ | 34/41 [00:19<00:04,  1.69it/s]Loading trainS:  85%|████████▌ | 35/41 [00:19<00:03,  1.64it/s]Loading trainS:  88%|████████▊ | 36/41 [00:20<00:02,  1.68it/s]Loading trainS:  90%|█████████ | 37/41 [00:20<00:02,  1.69it/s]Loading trainS:  93%|█████████▎| 38/41 [00:21<00:01,  1.57it/s]Loading trainS:  95%|█████████▌| 39/41 [00:21<00:01,  1.75it/s]Loading trainS:  98%|█████████▊| 40/41 [00:22<00:00,  1.76it/s]Loading trainS: 100%|██████████| 41/41 [00:23<00:00,  1.78it/s]
Loading testS:   0%|          | 0/11 [00:00<?, ?it/s]Loading testS:   9%|▉         | 1/11 [00:00<00:05,  1.85it/s]Loading testS:  18%|█▊        | 2/11 [00:01<00:04,  1.87it/s]Loading testS:  27%|██▋       | 3/11 [00:01<00:04,  1.86it/s]Loading testS:  36%|███▋      | 4/11 [00:02<00:03,  1.88it/s]Loading testS:  45%|████▌     | 5/11 [00:02<00:03,  1.90it/s]Loading testS:  55%|█████▍    | 6/11 [00:03<00:02,  1.93it/s]Loading testS:  64%|██████▎   | 7/11 [00:03<00:02,  1.88it/s]Loading testS:  73%|███████▎  | 8/11 [00:04<00:01,  1.83it/s]Loading testS:  82%|████████▏ | 9/11 [00:04<00:01,  1.85it/s]Loading testS:  91%|█████████ | 10/11 [00:05<00:00,  1.75it/s]Loading testS: 100%|██████████| 11/11 [00:06<00:00,  1.65it/s]---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 128, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 128, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 128, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 128, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 128, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 128, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 128, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 64, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 64, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 64, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 64, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 64, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 64, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 64, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 64, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 64, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 32, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 32, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 32, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 32, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 32, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 32, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 32, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 32, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 32, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 32, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 64, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 64, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 64, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 64, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 64, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 64, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 64, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 64, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 64, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 64, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 128, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 128, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 128, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 128, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 128, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 128, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 128, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 128, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 128, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 128, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 128, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97831385 0.02168615]
Train on 2690 samples, validate on 698 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 15s - loss: 1.1694 - acc: 0.8357 - mDice: 0.1320 - val_loss: 1.1282 - val_acc: 0.9367 - val_mDice: 0.1127

Epoch 00001: val_mDice improved from -inf to 0.11272, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.2286 - acc: 0.9806 - mDice: 0.6627 - val_loss: 0.9375 - val_acc: 0.8928 - val_mDice: 0.1594

Epoch 00002: val_mDice improved from 0.11272 to 0.15944, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.1151 - acc: 0.9838 - mDice: 0.8084 - val_loss: 0.3875 - val_acc: 0.9823 - val_mDice: 0.4846

Epoch 00003: val_mDice improved from 0.15944 to 0.48461, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0998 - acc: 0.9866 - mDice: 0.8309 - val_loss: 0.1698 - val_acc: 0.9922 - val_mDice: 0.7406


Epoch 00004: val_mDice improved from 0.48461 to 0.74060, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
Traceback (most recent call last):
  File "main.py", line 1570, in <module>
    EXP34_Resnet2_LogEDice_fineTune_ET_Ps_Main_NonCascade(UserInfoB)
  File "main.py", line 1559, in EXP34_Resnet2_LogEDice_fineTune_ET_Ps_Main_NonCascade
    Run(UserInfoB, IV)
  File "main.py", line 220, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 97, in Loop_Over_Nuclei
    if 1 in InitValues.Nuclei_Indexes and UserI['tempThalamus']: Run_Main(UserI)
  File "main.py", line 214, in Run_Main
    Loop_slicing_orientations(UserInfoB, InitValues)
  File "main.py", line 212, in Loop_slicing_orientations
    subRun(UserInfoB)
  File "main.py", line 206, in subRun
    else: normal_run(params)
  File "main.py", line 193, in normal_run
    choosingModel.check_Run(params, Data)              
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 53, in check_Run
    model      = trainingExperiment(Data, params) if not params.preprocess.TestOnly else loadModel(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 462, in trainingExperiment
    model, hist = modelTrain_Unet(Data, params, model)
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 434, in modelTrain_Unet
    model, hist = modelFit(model)
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 430, in modelFit
    else: hist = func_without_Generator()
  File "/array/ssd/msmajdi/code/thalamus/keras/modelFuncs/choosingModel.py", line 423, in func_without_Generator
    hist = model.fit(x=Data.Train.Image, y=Data.Train.Mask, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(Data.Validation.Image, Data.Validation.Mask), verbose=verbose, callbacks=callbacks) # , callbacks=[TQDMCallback()])        
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/training.py", line 1039, in fit
    validation_steps=validation_steps)
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/training_arrays.py", line 199, in fit_loop
    outs = f(ins_batch)
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2715, in __call__
    return self._call(inputs)
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2675, in _call
    fetched = self._callable_fn(*array_vals)
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1451, in __call__
    self._session._session, self._handle, args, status, None)
KeyboardInterrupt
2019-07-28 20:36:11.838601: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-28 20:36:15.477309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:85:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-07-28 20:36:15.477368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-28 20:36:15.859537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 20:36:15.859588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-28 20:36:15.859602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-28 20:36:15.861469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:24,  1.64it/s]Loading train:   5%|▍         | 2/41 [00:01<00:21,  1.82it/s]Loading train:   7%|▋         | 3/41 [00:01<00:18,  2.05it/s]Loading train:  10%|▉         | 4/41 [00:01<00:16,  2.19it/s]Loading train:  12%|█▏        | 5/41 [00:02<00:16,  2.19it/s]Loading train:  15%|█▍        | 6/41 [00:02<00:14,  2.36it/s]Loading train:  17%|█▋        | 7/41 [00:02<00:13,  2.58it/s]Loading train:  20%|█▉        | 8/41 [00:03<00:12,  2.57it/s]Loading train:  22%|██▏       | 9/41 [00:03<00:12,  2.61it/s]Loading train:  24%|██▍       | 10/41 [00:04<00:12,  2.54it/s]Loading train:  27%|██▋       | 11/41 [00:04<00:11,  2.53it/s]Loading train:  29%|██▉       | 12/41 [00:04<00:10,  2.71it/s]Loading train:  32%|███▏      | 13/41 [00:05<00:10,  2.66it/s]Loading train:  34%|███▍      | 14/41 [00:05<00:09,  2.72it/s]Loading train:  37%|███▋      | 15/41 [00:05<00:09,  2.85it/s]Loading train:  39%|███▉      | 16/41 [00:06<00:09,  2.66it/s]Loading train:  41%|████▏     | 17/41 [00:06<00:09,  2.54it/s]Loading train:  44%|████▍     | 18/41 [00:07<00:09,  2.51it/s]Loading train:  46%|████▋     | 19/41 [00:07<00:08,  2.58it/s]Loading train:  49%|████▉     | 20/41 [00:07<00:08,  2.55it/s]Loading train:  51%|█████     | 21/41 [00:08<00:08,  2.31it/s]Loading train:  54%|█████▎    | 22/41 [00:08<00:08,  2.17it/s]Loading train:  56%|█████▌    | 23/41 [00:09<00:07,  2.31it/s]Loading train:  59%|█████▊    | 24/41 [00:09<00:07,  2.22it/s]Loading train:  61%|██████    | 25/41 [00:10<00:06,  2.33it/s]Loading train:  63%|██████▎   | 26/41 [00:10<00:06,  2.32it/s]Loading train:  66%|██████▌   | 27/41 [00:11<00:06,  2.21it/s]Loading train:  68%|██████▊   | 28/41 [00:11<00:06,  2.12it/s]Loading train:  71%|███████   | 29/41 [00:11<00:05,  2.24it/s]Loading train:  73%|███████▎  | 30/41 [00:12<00:05,  2.10it/s]Loading train:  76%|███████▌  | 31/41 [00:13<00:04,  2.01it/s]Loading train:  78%|███████▊  | 32/41 [00:13<00:04,  2.08it/s]Loading train:  80%|████████  | 33/41 [00:13<00:03,  2.12it/s]Loading train:  83%|████████▎ | 34/41 [00:14<00:03,  2.07it/s]Loading train:  85%|████████▌ | 35/41 [00:15<00:03,  1.84it/s]Loading train:  88%|████████▊ | 36/41 [00:15<00:02,  1.82it/s]Loading train:  90%|█████████ | 37/41 [00:16<00:02,  1.93it/s]Loading train:  93%|█████████▎| 38/41 [00:16<00:01,  1.95it/s]Loading train:  95%|█████████▌| 39/41 [00:17<00:01,  1.98it/s]Loading train:  98%|█████████▊| 40/41 [00:17<00:00,  1.99it/s]Loading train: 100%|██████████| 41/41 [00:18<00:00,  2.10it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:  17%|█▋        | 7/41 [00:00<00:00, 64.00it/s]concatenating: train:  54%|█████▎    | 22/41 [00:00<00:00, 77.05it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 150.68it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:03,  2.66it/s]Loading test:  18%|█▊        | 2/11 [00:00<00:03,  2.58it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:03,  2.53it/s]Loading test:  36%|███▋      | 4/11 [00:01<00:02,  2.51it/s]Loading test:  45%|████▌     | 5/11 [00:02<00:02,  2.35it/s]Loading test:  55%|█████▍    | 6/11 [00:02<00:02,  2.33it/s]Loading test:  64%|██████▎   | 7/11 [00:03<00:01,  2.04it/s]Loading test:  73%|███████▎  | 8/11 [00:03<00:01,  2.06it/s]Loading test:  82%|████████▏ | 9/11 [00:04<00:00,  2.09it/s]Loading test:  91%|█████████ | 10/11 [00:04<00:00,  2.20it/s]Loading test: 100%|██████████| 11/11 [00:05<00:00,  2.11it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  27%|██▋       | 3/11 [00:00<00:00, 22.83it/s]concatenating: validation:  55%|█████▍    | 6/11 [00:00<00:00, 23.89it/s]concatenating: validation:  82%|████████▏ | 9/11 [00:00<00:00, 22.88it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 26.19it/s]
Loading trainS:   0%|          | 0/41 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/41 [00:00<00:14,  2.82it/s]Loading trainS:   5%|▍         | 2/41 [00:00<00:15,  2.49it/s]Loading trainS:   7%|▋         | 3/41 [00:01<00:15,  2.42it/s]Loading trainS:  10%|▉         | 4/41 [00:01<00:15,  2.40it/s]Loading trainS:  12%|█▏        | 5/41 [00:02<00:16,  2.18it/s]Loading trainS:  15%|█▍        | 6/41 [00:02<00:15,  2.21it/s]Loading trainS:  17%|█▋        | 7/41 [00:03<00:14,  2.34it/s]Loading trainS:  20%|█▉        | 8/41 [00:03<00:13,  2.48it/s]Loading trainS:  22%|██▏       | 9/41 [00:03<00:12,  2.54it/s]Loading trainS:  24%|██▍       | 10/41 [00:04<00:13,  2.33it/s]Loading trainS:  27%|██▋       | 11/41 [00:04<00:14,  2.08it/s]Loading trainS:  29%|██▉       | 12/41 [00:05<00:13,  2.17it/s]Loading trainS:  32%|███▏      | 13/41 [00:05<00:13,  2.15it/s]Loading trainS:  34%|███▍      | 14/41 [00:06<00:13,  2.06it/s]Loading trainS:  37%|███▋      | 15/41 [00:06<00:12,  2.14it/s]Loading trainS:  39%|███▉      | 16/41 [00:07<00:12,  2.00it/s]Loading trainS:  41%|████▏     | 17/41 [00:07<00:12,  1.97it/s]Loading trainS:  44%|████▍     | 18/41 [00:08<00:12,  1.89it/s]Loading trainS:  46%|████▋     | 19/41 [00:08<00:11,  1.91it/s]Loading trainS:  49%|████▉     | 20/41 [00:09<00:10,  2.08it/s]Loading trainS:  51%|█████     | 21/41 [00:09<00:09,  2.21it/s]Loading trainS:  54%|█████▎    | 22/41 [00:10<00:08,  2.20it/s]Loading trainS:  56%|█████▌    | 23/41 [00:10<00:07,  2.27it/s]Loading trainS:  59%|█████▊    | 24/41 [00:11<00:07,  2.19it/s]Loading trainS:  61%|██████    | 25/41 [00:11<00:07,  2.17it/s]Loading trainS:  63%|██████▎   | 26/41 [00:11<00:06,  2.22it/s]Loading trainS:  66%|██████▌   | 27/41 [00:12<00:06,  2.20it/s]Loading trainS:  68%|██████▊   | 28/41 [00:13<00:06,  2.08it/s]Loading trainS:  71%|███████   | 29/41 [00:13<00:05,  2.28it/s]Loading trainS:  73%|███████▎  | 30/41 [00:13<00:04,  2.31it/s]Loading trainS:  76%|███████▌  | 31/41 [00:14<00:04,  2.20it/s]Loading trainS:  78%|███████▊  | 32/41 [00:14<00:04,  2.22it/s]Loading trainS:  80%|████████  | 33/41 [00:15<00:03,  2.28it/s]Loading trainS:  83%|████████▎ | 34/41 [00:15<00:03,  2.24it/s]Loading trainS:  85%|████████▌ | 35/41 [00:16<00:03,  1.93it/s]Loading trainS:  88%|████████▊ | 36/41 [00:16<00:02,  1.86it/s]Loading trainS:  90%|█████████ | 37/41 [00:17<00:02,  1.90it/s]Loading trainS:  93%|█████████▎| 38/41 [00:17<00:01,  2.00it/s]Loading trainS:  95%|█████████▌| 39/41 [00:18<00:00,  2.08it/s]Loading trainS:  98%|█████████▊| 40/41 [00:18<00:00,  2.04it/s]Loading trainS: 100%|██████████| 41/41 [00:19<00:00,  2.12it/s]
Loading testS:   0%|          | 0/11 [00:00<?, ?it/s]Loading testS:   9%|▉         | 1/11 [00:00<00:04,  2.44it/s]Loading testS:  18%|█▊        | 2/11 [00:00<00:03,  2.50it/s]Loading testS:  27%|██▋       | 3/11 [00:01<00:03,  2.43it/s]Loading testS:  36%|███▋      | 4/11 [00:01<00:02,  2.51it/s]Loading testS:  45%|████▌     | 5/11 [00:02<00:02,  2.31it/s]Loading testS:  55%|█████▍    | 6/11 [00:02<00:02,  2.18it/s]Loading testS:  64%|██████▎   | 7/11 [00:03<00:01,  2.02it/s]Loading testS:  73%|███████▎  | 8/11 [00:03<00:01,  1.93it/s]Loading testS:  82%|████████▏ | 9/11 [00:04<00:01,  1.97it/s]Loading testS:  91%|█████████ | 10/11 [00:04<00:00,  1.81it/s]Loading testS: 100%|██████████| 11/11 [00:05<00:00,  1.74it/s]---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 128, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 128, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 128, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 128, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 128, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 128, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 128, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 64, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 64, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 64, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 64, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 64, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 64, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 64, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 64, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 64, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 32, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 32, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 32, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 32, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 32, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 32, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 32, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 32, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 32, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 32, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 64, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 64, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 64, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 64, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 64, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 64, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 64, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 64, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 64, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 64, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 128, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 128, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 128, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 128, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 128, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 128, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 128, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 128, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 128, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 128, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 128, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97831385 0.02168615]
Train on 2690 samples, validate on 698 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 15s - loss: 0.9348 - acc: 0.9659 - mDice: 0.2227 - val_loss: 0.8550 - val_acc: 0.9522 - val_mDice: 0.1915

Epoch 00001: val_mDice improved from -inf to 0.19151, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.1577 - acc: 0.9845 - mDice: 0.7418 - val_loss: 0.2880 - val_acc: 0.9847 - val_mDice: 0.5711

Epoch 00002: val_mDice improved from 0.19151 to 0.57109, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.1088 - acc: 0.9891 - mDice: 0.8144 - val_loss: 0.1991 - val_acc: 0.9885 - val_mDice: 0.6829

Epoch 00003: val_mDice improved from 0.57109 to 0.68288, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0956 - acc: 0.9910 - mDice: 0.8360 - val_loss: 0.1638 - val_acc: 0.9901 - val_mDice: 0.7317

Epoch 00004: val_mDice improved from 0.68288 to 0.73169, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0841 - acc: 0.9922 - mDice: 0.8551 - val_loss: 0.0735 - val_acc: 0.9945 - val_mDice: 0.8735

Epoch 00005: val_mDice improved from 0.73169 to 0.87354, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0795 - acc: 0.9928 - mDice: 0.8628 - val_loss: 0.0659 - val_acc: 0.9948 - val_mDice: 0.8869

Epoch 00006: val_mDice improved from 0.87354 to 0.88688, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0775 - acc: 0.9932 - mDice: 0.8662 - val_loss: 0.0625 - val_acc: 0.9952 - val_mDice: 0.8928

Epoch 00007: val_mDice improved from 0.88688 to 0.89283, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0763 - acc: 0.9935 - mDice: 0.8682 - val_loss: 0.1098 - val_acc: 0.9918 - val_mDice: 0.8250

Epoch 00008: val_mDice did not improve from 0.89283
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0737 - acc: 0.9938 - mDice: 0.8727 - val_loss: 0.0636 - val_acc: 0.9949 - val_mDice: 0.8909

Epoch 00009: val_mDice did not improve from 0.89283
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0704 - acc: 0.9941 - mDice: 0.8784 - val_loss: 0.0614 - val_acc: 0.9950 - val_mDice: 0.8949

Epoch 00010: val_mDice improved from 0.89283 to 0.89489, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0676 - acc: 0.9945 - mDice: 0.8834 - val_loss: 0.0579 - val_acc: 0.9953 - val_mDice: 0.9012

Epoch 00011: val_mDice improved from 0.89489 to 0.90116, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0691 - acc: 0.9945 - mDice: 0.8806 - val_loss: 0.0830 - val_acc: 0.9943 - val_mDice: 0.8578

Epoch 00012: val_mDice did not improve from 0.90116
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0683 - acc: 0.9946 - mDice: 0.8819 - val_loss: 0.0598 - val_acc: 0.9954 - val_mDice: 0.8977

Epoch 00013: val_mDice did not improve from 0.90116
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0666 - acc: 0.9948 - mDice: 0.8849 - val_loss: 0.0615 - val_acc: 0.9953 - val_mDice: 0.8946

Epoch 00014: val_mDice did not improve from 0.90116
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0668 - acc: 0.9949 - mDice: 0.8843 - val_loss: 0.0572 - val_acc: 0.9956 - val_mDice: 0.9022

Epoch 00015: val_mDice improved from 0.90116 to 0.90219, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0626 - acc: 0.9952 - mDice: 0.8911 - val_loss: 0.0551 - val_acc: 0.9956 - val_mDice: 0.9048

Epoch 00016: val_mDice improved from 0.90219 to 0.90476, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0622 - acc: 0.9953 - mDice: 0.8895 - val_loss: 0.0554 - val_acc: 0.9955 - val_mDice: 0.8991

Epoch 00017: val_mDice did not improve from 0.90476
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0588 - acc: 0.9954 - mDice: 0.8933 - val_loss: 0.0641 - val_acc: 0.9950 - val_mDice: 0.8823

Epoch 00018: val_mDice did not improve from 0.90476
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0572 - acc: 0.9956 - mDice: 0.8954 - val_loss: 0.0624 - val_acc: 0.9951 - val_mDice: 0.8853

Epoch 00019: val_mDice did not improve from 0.90476
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0552 - acc: 0.9957 - mDice: 0.8986 - val_loss: 0.0600 - val_acc: 0.9953 - val_mDice: 0.8892

Epoch 00020: val_mDice did not improve from 0.90476
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0550 - acc: 0.9957 - mDice: 0.8988 - val_loss: 0.0594 - val_acc: 0.9953 - val_mDice: 0.8903

Epoch 00021: val_mDice did not improve from 0.90476
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0549 - acc: 0.9956 - mDice: 0.8989 - val_loss: 0.0591 - val_acc: 0.9953 - val_mDice: 0.8907

Epoch 00022: val_mDice did not improve from 0.90476
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0543 - acc: 0.9957 - mDice: 0.8999 - val_loss: 0.0518 - val_acc: 0.9958 - val_mDice: 0.9038

Epoch 00023: val_mDice did not improve from 0.90476
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0544 - acc: 0.9957 - mDice: 0.8996 - val_loss: 0.0567 - val_acc: 0.9955 - val_mDice: 0.8950

Epoch 00024: val_mDice did not improve from 0.90476
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0534 - acc: 0.9958 - mDice: 0.9013 - val_loss: 0.0528 - val_acc: 0.9957 - val_mDice: 0.9019

Epoch 00025: val_mDice did not improve from 0.90476
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0529 - acc: 0.9958 - mDice: 0.9021 - val_loss: 0.0508 - val_acc: 0.9958 - val_mDice: 0.9055

Epoch 00026: val_mDice improved from 0.90476 to 0.90552, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0533 - acc: 0.9958 - mDice: 0.9014 - val_loss: 0.0566 - val_acc: 0.9955 - val_mDice: 0.8951

Epoch 00027: val_mDice did not improve from 0.90552
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0537 - acc: 0.9957 - mDice: 0.9007 - val_loss: 0.0515 - val_acc: 0.9958 - val_mDice: 0.9042

Epoch 00028: val_mDice did not improve from 0.90552
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0536 - acc: 0.9957 - mDice: 0.9009 - val_loss: 0.0489 - val_acc: 0.9959 - val_mDice: 0.9089

Epoch 00029: val_mDice improved from 0.90552 to 0.90893, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0538 - acc: 0.9957 - mDice: 0.9005 - val_loss: 0.0490 - val_acc: 0.9959 - val_mDice: 0.9086

Epoch 00030: val_mDice did not improve from 0.90893
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0515 - acc: 0.9959 - mDice: 0.9044 - val_loss: 0.0479 - val_acc: 0.9960 - val_mDice: 0.9107

Epoch 00031: val_mDice improved from 0.90893 to 0.91071, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0519 - acc: 0.9959 - mDice: 0.9037 - val_loss: 0.0490 - val_acc: 0.9959 - val_mDice: 0.9088

Epoch 00032: val_mDice did not improve from 0.91071
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0519 - acc: 0.9959 - mDice: 0.9037 - val_loss: 0.0458 - val_acc: 0.9961 - val_mDice: 0.9145

Epoch 00033: val_mDice improved from 0.91071 to 0.91449, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0523 - acc: 0.9958 - mDice: 0.9030 - val_loss: 0.0488 - val_acc: 0.9959 - val_mDice: 0.9090

Epoch 00034: val_mDice did not improve from 0.91449
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0508 - acc: 0.9960 - mDice: 0.9056 - val_loss: 0.0483 - val_acc: 0.9960 - val_mDice: 0.9100

Epoch 00035: val_mDice did not improve from 0.91449
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0505 - acc: 0.9960 - mDice: 0.9061 - val_loss: 0.0501 - val_acc: 0.9959 - val_mDice: 0.9068

Epoch 00036: val_mDice did not improve from 0.91449
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0495 - acc: 0.9960 - mDice: 0.9080 - val_loss: 0.0482 - val_acc: 0.9960 - val_mDice: 0.9101

Epoch 00037: val_mDice did not improve from 0.91449
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0493 - acc: 0.9961 - mDice: 0.9083 - val_loss: 0.0502 - val_acc: 0.9959 - val_mDice: 0.9065

Epoch 00038: val_mDice did not improve from 0.91449
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0488 - acc: 0.9961 - mDice: 0.9092 - val_loss: 0.0488 - val_acc: 0.9960 - val_mDice: 0.9090

Epoch 00039: val_mDice did not improve from 0.91449
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0481 - acc: 0.9962 - mDice: 0.9104 - val_loss: 0.0475 - val_acc: 0.9960 - val_mDice: 0.9114

Epoch 00040: val_mDice did not improve from 0.91449
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0478 - acc: 0.9962 - mDice: 0.9110 - val_loss: 0.0493 - val_acc: 0.9959 - val_mDice: 0.9081

Epoch 00041: val_mDice did not improve from 0.91449
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0497 - acc: 0.9961 - mDice: 0.9075 - val_loss: 0.0487 - val_acc: 0.9960 - val_mDice: 0.9092

Epoch 00042: val_mDice did not improve from 0.91449
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0478 - acc: 0.9962 - mDice: 0.9109 - val_loss: 0.0486 - val_acc: 0.9960 - val_mDice: 0.9094

Epoch 00043: val_mDice did not improve from 0.91449
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0479 - acc: 0.9962 - mDice: 0.9107 - val_loss: 0.0485 - val_acc: 0.9960 - val_mDice: 0.9095

Epoch 00044: val_mDice did not improve from 0.91449
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0481 - acc: 0.9962 - mDice: 0.9103 - val_loss: 0.0518 - val_acc: 0.9958 - val_mDice: 0.9037

Epoch 00045: val_mDice did not improve from 0.91449
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0485 - acc: 0.9961 - mDice: 0.9097 - val_loss: 0.0483 - val_acc: 0.9960 - val_mDice: 0.9100

Epoch 00046: val_mDice did not improve from 0.91449
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0480 - acc: 0.9962 - mDice: 0.9105 - val_loss: 0.0500 - val_acc: 0.9959 - val_mDice: 0.9068

Epoch 00047: val_mDice did not improve from 0.91449
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0470 - acc: 0.9962 - mDice: 0.9124 - val_loss: 0.0486 - val_acc: 0.9960 - val_mDice: 0.9093

Epoch 00048: val_mDice did not improve from 0.91449
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0473 - acc: 0.9962 - mDice: 0.9119 - val_loss: 0.0492 - val_acc: 0.9959 - val_mDice: 0.9082

Epoch 00049: val_mDice did not improve from 0.91449
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0473 - acc: 0.9962 - mDice: 0.9118 - val_loss: 0.0487 - val_acc: 0.9960 - val_mDice: 0.9092

Epoch 00050: val_mDice did not improve from 0.91449
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0479 - acc: 0.9962 - mDice: 0.9108 - val_loss: 0.0483 - val_acc: 0.9960 - val_mDice: 0.9100

Epoch 00051: val_mDice did not improve from 0.91449
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0472 - acc: 0.9962 - mDice: 0.9120 - val_loss: 0.0483 - val_acc: 0.9960 - val_mDice: 0.9099

Epoch 00052: val_mDice did not improve from 0.91449
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0470 - acc: 0.9962 - mDice: 0.9123 - val_loss: 0.0510 - val_acc: 0.9958 - val_mDice: 0.9051

Epoch 00053: val_mDice did not improve from 0.91449
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0471 - acc: 0.9963 - mDice: 0.9121 - val_loss: 0.0480 - val_acc: 0.9960 - val_mDice: 0.9105

Epoch 00054: val_mDice did not improve from 0.91449
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0467 - acc: 0.9963 - mDice: 0.9129 - val_loss: 0.0491 - val_acc: 0.9959 - val_mDice: 0.9085

Epoch 00055: val_mDice did not improve from 0.91449
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0465 - acc: 0.9963 - mDice: 0.9131 - val_loss: 0.0470 - val_acc: 0.9961 - val_mDice: 0.9123

Epoch 00056: val_mDice did not improve from 0.91449
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0463 - acc: 0.9963 - mDice: 0.9135 - val_loss: 0.0482 - val_acc: 0.9960 - val_mDice: 0.9100

Epoch 00057: val_mDice did not improve from 0.91449
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0458 - acc: 0.9963 - mDice: 0.9144 - val_loss: 0.0476 - val_acc: 0.9960 - val_mDice: 0.9112

Epoch 00058: val_mDice did not improve from 0.91449
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0458 - acc: 0.9963 - mDice: 0.9145 - val_loss: 0.0474 - val_acc: 0.9960 - val_mDice: 0.9115

Epoch 00059: val_mDice did not improve from 0.91449
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0457 - acc: 0.9964 - mDice: 0.9147 - val_loss: 0.0476 - val_acc: 0.9960 - val_mDice: 0.9112

Epoch 00060: val_mDice did not improve from 0.91449
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0457 - acc: 0.9963 - mDice: 0.9146 - val_loss: 0.0477 - val_acc: 0.9960 - val_mDice: 0.9110

Epoch 00061: val_mDice did not improve from 0.91449
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0456 - acc: 0.9964 - mDice: 0.9149 - val_loss: 0.0496 - val_acc: 0.9959 - val_mDice: 0.9075

Epoch 00062: val_mDice did not improve from 0.91449
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0457 - acc: 0.9964 - mDice: 0.9147 - val_loss: 0.0493 - val_acc: 0.9959 - val_mDice: 0.9082

Epoch 00063: val_mDice did not improve from 0.91449
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0452 - acc: 0.9964 - mDice: 0.9154 - val_loss: 0.0494 - val_acc: 0.9959 - val_mDice: 0.9079

Epoch 00064: val_mDice did not improve from 0.91449
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0455 - acc: 0.9964 - mDice: 0.9150 - val_loss: 0.0487 - val_acc: 0.9960 - val_mDice: 0.9092

Epoch 00065: val_mDice did not improve from 0.91449
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0455 - acc: 0.9964 - mDice: 0.9150 - val_loss: 0.0475 - val_acc: 0.9960 - val_mDice: 0.9113

Epoch 00066: val_mDice did not improve from 0.91449
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0462 - acc: 0.9964 - mDice: 0.9137 - val_loss: 0.0481 - val_acc: 0.9960 - val_mDice: 0.9102

Epoch 00067: val_mDice did not improve from 0.91449
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0448 - acc: 0.9964 - mDice: 0.9161 - val_loss: 0.0489 - val_acc: 0.9960 - val_mDice: 0.9089

Epoch 00068: val_mDice did not improve from 0.91449
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0447 - acc: 0.9964 - mDice: 0.9164 - val_loss: 0.0495 - val_acc: 0.9959 - val_mDice: 0.9077

Epoch 00069: val_mDice did not improve from 0.91449
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0446 - acc: 0.9964 - mDice: 0.9166 - val_loss: 0.0488 - val_acc: 0.9960 - val_mDice: 0.9091

Epoch 00070: val_mDice did not improve from 0.91449
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0443 - acc: 0.9964 - mDice: 0.9171 - val_loss: 0.0506 - val_acc: 0.9959 - val_mDice: 0.9058

Epoch 00071: val_mDice did not improve from 0.91449
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0449 - acc: 0.9964 - mDice: 0.9161 - val_loss: 0.0479 - val_acc: 0.9960 - val_mDice: 0.9106

Epoch 00072: val_mDice did not improve from 0.91449
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0445 - acc: 0.9964 - mDice: 0.9167 - val_loss: 0.0486 - val_acc: 0.9960 - val_mDice: 0.9093

Epoch 00073: val_mDice did not improve from 0.91449
Restoring model weights from the end of the best epoch
Epoch 00073: early stopping
{'val_loss': [0.855031322613145, 0.2879979464420275, 0.19909468779932804, 0.16375121145501179, 0.07350422420611012, 0.06591457794593876, 0.06254649523079908, 0.10975313476983319, 0.06362370040833437, 0.061392863043570584, 0.05786228565269692, 0.08298302619026862, 0.05983297729645896, 0.06149888713243697, 0.0571949386003195, 0.055138740272269206, 0.05541673199110864, 0.06414399146606724, 0.0623762904103301, 0.0600375767295545, 0.05939790482678181, 0.059137679668500975, 0.0517571972506948, 0.056724136476444996, 0.052821168371804464, 0.05077394693662922, 0.05663774056856475, 0.051509900397506345, 0.048857343711364576, 0.04903693405382954, 0.04786855022937999, 0.04895070399074978, 0.04576754738659435, 0.048836496186290566, 0.04826906397084794, 0.05007443033735185, 0.04819458536270013, 0.050185766231193925, 0.04881954995813889, 0.04746847329687936, 0.049293305884972, 0.04868908699356382, 0.04858027652631858, 0.04853030165022287, 0.051783511762034926, 0.048251267084931916, 0.05002667967304801, 0.04863804044004498, 0.04922176779042342, 0.048695511283368985, 0.048272379404494276, 0.04829467872536627, 0.051003992984493006, 0.047982520437172285, 0.04910646225285735, 0.04696290598472415, 0.048228637003113, 0.047595322815258384, 0.04740884474193128, 0.04758233369933842, 0.04770963851437186, 0.04964489241085285, 0.049268906503745, 0.049426733988489326, 0.048683946669357894, 0.04753525122829358, 0.04812616307320431, 0.04886555188151349, 0.04952947786851735, 0.04875295748000159, 0.0505840978171559, 0.0479193806626906, 0.048636487036687256], 'val_acc': [0.9522446446910628, 0.984742227802987, 0.9885209330856629, 0.9901378965992641, 0.9944872130295609, 0.9948069832386465, 0.9951971900838835, 0.9918443820196441, 0.9949477523309112, 0.9949604925248547, 0.9953244466836269, 0.9942861298435397, 0.9953904322703452, 0.9953021451531987, 0.9955956689949363, 0.9956311742350843, 0.9955245562132586, 0.9950173254682546, 0.9951441021566746, 0.9952711855784528, 0.9953177996556192, 0.99533081362104, 0.9957580721822372, 0.9954796993629981, 0.9957100204205445, 0.9958340018389902, 0.9954770880305664, 0.9957660776496275, 0.9959299070787293, 0.9959176561893911, 0.9960016073675073, 0.9959407136228501, 0.9960839188201379, 0.9959394629500316, 0.9959997753359185, 0.9958939251722101, 0.9959959051328948, 0.9958835881214088, 0.9959586544501405, 0.9960385704450416, 0.9959318470476691, 0.9959613607401151, 0.9959704404574069, 0.9959697660197501, 0.9957951129678327, 0.9959841344964538, 0.9958843640065125, 0.9959716007839301, 0.9959387806561751, 0.9959619439775418, 0.9959890480369414, 0.995996002993816, 0.995834773966781, 0.9959927317406182, 0.9959422635144012, 0.9960519615080432, 0.9959989032977632, 0.9960342134992167, 0.9960472348084765, 0.9960427943478342, 0.9960320885680125, 0.9959191073660865, 0.9959420758195456, 0.995933669173615, 0.9959750665634615, 0.9960441625220727, 0.9960116359770811, 0.9959623383246042, 0.9959179408912331, 0.9959670691230578, 0.9958578397688005, 0.9960110614497887, 0.9959662886267063], 'val_mDice': [0.19150582933665006, 0.5710879967342475, 0.6828795640721362, 0.7316942978383477, 0.8735409628354376, 0.8868761940466982, 0.8928296760706642, 0.8249543777167968, 0.8908688328669201, 0.894890648246153, 0.9011636256830057, 0.8577915334428279, 0.8976928719818421, 0.8946344251277454, 0.9021874328055832, 0.9047639974550394, 0.899062922144346, 0.882332604559923, 0.8852501243438283, 0.8892483972546706, 0.89031975539161, 0.890729020181563, 0.9038217989968024, 0.8949996217615624, 0.9018787977347059, 0.9055166432372479, 0.8951018965005192, 0.904175439161011, 0.9089286159990851, 0.9086206227455577, 0.9107104206836668, 0.9087593647675392, 0.9144862024353705, 0.9089616012778187, 0.9099655983099623, 0.9067601354894119, 0.910094053492505, 0.9065329290734321, 0.9089857355229835, 0.9114001680240249, 0.9081155895162107, 0.9091957189292142, 0.909403845027388, 0.9094982177958447, 0.9036615260351012, 0.9100019474084193, 0.9068285962913645, 0.9092970510950061, 0.9082461175399387, 0.9092030499589477, 0.9099667717870805, 0.9099152301648968, 0.9050857627972491, 0.9104924755315043, 0.9084828417417313, 0.9123218800755148, 0.9100346291987466, 0.9111732241075838, 0.9115061099003242, 0.9111989251522075, 0.9109625090500687, 0.9074831227518426, 0.9081578247868229, 0.9078728339733572, 0.90920380466647, 0.9112726984529578, 0.9102076746331245, 0.9088852940111243, 0.90768963098526, 0.9090737951519154, 0.9058166972887208, 0.910581504376365, 0.9092966096107461], 'loss': [0.9348285599268945, 0.15769254734746585, 0.10882268092667746, 0.09560966317206067, 0.08413740518921813, 0.07954260485322945, 0.07750879848535176, 0.07634775238076993, 0.07373293929605236, 0.07041821540177533, 0.06755065920738483, 0.0690948215337491, 0.06834397257260673, 0.0665564553389762, 0.0667662136949128, 0.06259756166917241, 0.06220363720309779, 0.05881771671007557, 0.05717696936378692, 0.05521385434004011, 0.05501469199763798, 0.05490699627862544, 0.05429354160470147, 0.05440979183661893, 0.05343548561616015, 0.05293042344286982, 0.05329560078542029, 0.053690995453237154, 0.053566032722869326, 0.05377398170946256, 0.051521906317609834, 0.051920958417277355, 0.05194276096264669, 0.05233319757374689, 0.050806606749287324, 0.05053018209382504, 0.0494693143909527, 0.0493129520611249, 0.04878951124431476, 0.04812047516989442, 0.04775930626740243, 0.049703453303934476, 0.04779249744067405, 0.04791061217346156, 0.04812211443611237, 0.0484849208598908, 0.04800505213245583, 0.04695223332780887, 0.04725785455129847, 0.047261294254472265, 0.04787303915863587, 0.0471704999698139, 0.04699551819591717, 0.047124574535619815, 0.04668710519279246, 0.046523015502992614, 0.0463381048194981, 0.04579055848008638, 0.045786282096099674, 0.0456659467856237, 0.04568880772889768, 0.045551556429122904, 0.045666526878411884, 0.0452464274702019, 0.04549150501263629, 0.04549854023028927, 0.046188644026181065, 0.04483507868581102, 0.04471796227542884, 0.04458570225531284, 0.04427525840118029, 0.0448812833644422, 0.044514573459620815], 'acc': [0.9659332553250196, 0.9844888654783313, 0.9890999058365378, 0.9909738178146816, 0.9921709735597376, 0.992762547664926, 0.993200368597605, 0.9934551924134719, 0.993798173493169, 0.994109405460854, 0.9944725869756649, 0.9944541633793855, 0.9946143713582404, 0.9948286190352033, 0.9948571304406375, 0.9952311041629891, 0.9952753181794319, 0.9954419696641235, 0.9955642947920194, 0.9956548989483858, 0.995650368330647, 0.9956499996238481, 0.995681273671331, 0.9957154998105698, 0.9957677449435549, 0.9957902566650986, 0.9957680201441825, 0.9957449075014618, 0.9957399552196375, 0.9957276904006873, 0.9959005443136931, 0.9959022416058083, 0.995873632041052, 0.9958436349510703, 0.9959555487650478, 0.9959881943840962, 0.9960445548965142, 0.9960661076258549, 0.9960937971962429, 0.9961605504099764, 0.9961775725215785, 0.9960501610568022, 0.9961682103823551, 0.9961639040021648, 0.9961647772434475, 0.9961333053262703, 0.9961701837614123, 0.9962270656482881, 0.9962087187182062, 0.9962173356885804, 0.9961791794096228, 0.9962490766021842, 0.9962495791424606, 0.9962538299064211, 0.9962515102000041, 0.9963092697597348, 0.9963059318996274, 0.9963209593606261, 0.9963318789758647, 0.9963741909615612, 0.9963279701076919, 0.9963627191724387, 0.9963616427435751, 0.9963932651126252, 0.9963688256572171, 0.9963640513030126, 0.996356382245911, 0.996402754881125, 0.9963996461332952, 0.996411289424258, 0.9964283181832182, 0.9963936446767757, 0.9964386623588193], 'mDice': [0.22267754032487763, 0.7417777863133795, 0.8144333779147124, 0.8359762075665272, 0.8550608042890697, 0.8628337615927799, 0.8662314359583376, 0.8682339187005196, 0.8726967406981939, 0.8783831682790167, 0.8833975576999905, 0.8805951981296326, 0.8819178739444917, 0.8848711061654924, 0.8843208527033214, 0.8910514209350245, 0.889501188102708, 0.8933215732911263, 0.8954348697095112, 0.8985895831345625, 0.8987992490091289, 0.8988780789215769, 0.8998709686626732, 0.8995940583346058, 0.901256742973753, 0.9020689393507947, 0.9014231790840405, 0.9007022963580589, 0.9009074699479851, 0.900470124301414, 0.9044248896017394, 0.9037192949131962, 0.9036534082933873, 0.9029542531222658, 0.9056070471341725, 0.9060910729670614, 0.9079860579568657, 0.9082546730466935, 0.9091598032575557, 0.9103753307494975, 0.9110070613237119, 0.9075455178115448, 0.9109121685134434, 0.9106933040246644, 0.9103433897504133, 0.9096878529924443, 0.9105092841010112, 0.9123867038900524, 0.9118618092129221, 0.9118409054873158, 0.9107560956345172, 0.91199232277817, 0.912308191278167, 0.9120919179295962, 0.9128683289187548, 0.913144834201132, 0.9134838007196617, 0.9144493195204043, 0.9144552788769889, 0.9146694150999133, 0.9146329425967759, 0.9148710152916749, 0.9146784865280067, 0.9154251716393964, 0.9149776218549026, 0.9149632163650484, 0.9137399522345305, 0.9161420819041454, 0.9163510174556293, 0.9165855517617831, 0.9171494032813714, 0.9160590856491855, 0.9167005226957754], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:01<00:10,  1.01s/it]predicting test subjects:  18%|█▊        | 2/11 [00:01<00:07,  1.22it/s]predicting test subjects:  27%|██▋       | 3/11 [00:01<00:05,  1.35it/s]predicting test subjects:  36%|███▋      | 4/11 [00:02<00:04,  1.57it/s]predicting test subjects:  45%|████▌     | 5/11 [00:02<00:03,  1.71it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:03<00:02,  1.91it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:03<00:01,  2.07it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:03<00:01,  2.25it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:04<00:00,  2.26it/s]predicting test subjects:  91%|█████████ | 10/11 [00:04<00:00,  2.35it/s]predicting test subjects: 100%|██████████| 11/11 [00:05<00:00,  2.05it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:20,  1.93it/s]predicting train subjects:   5%|▍         | 2/41 [00:00<00:18,  2.13it/s]predicting train subjects:   7%|▋         | 3/41 [00:01<00:17,  2.15it/s]predicting train subjects:  10%|▉         | 4/41 [00:01<00:16,  2.26it/s]predicting train subjects:  12%|█▏        | 5/41 [00:02<00:16,  2.14it/s]predicting train subjects:  15%|█▍        | 6/41 [00:02<00:16,  2.07it/s]predicting train subjects:  17%|█▋        | 7/41 [00:03<00:15,  2.20it/s]predicting train subjects:  20%|█▉        | 8/41 [00:03<00:14,  2.21it/s]predicting train subjects:  22%|██▏       | 9/41 [00:04<00:14,  2.16it/s]predicting train subjects:  24%|██▍       | 10/41 [00:04<00:14,  2.21it/s]predicting train subjects:  27%|██▋       | 11/41 [00:04<00:13,  2.25it/s]predicting train subjects:  29%|██▉       | 12/41 [00:05<00:12,  2.24it/s]predicting train subjects:  32%|███▏      | 13/41 [00:05<00:11,  2.39it/s]predicting train subjects:  34%|███▍      | 14/41 [00:06<00:11,  2.40it/s]predicting train subjects:  37%|███▋      | 15/41 [00:06<00:09,  2.73it/s]predicting train subjects:  39%|███▉      | 16/41 [00:06<00:09,  2.54it/s]predicting train subjects:  41%|████▏     | 17/41 [00:07<00:09,  2.59it/s]predicting train subjects:  44%|████▍     | 18/41 [00:07<00:08,  2.78it/s]predicting train subjects:  46%|████▋     | 19/41 [00:07<00:07,  3.05it/s]predicting train subjects:  49%|████▉     | 20/41 [00:08<00:07,  2.80it/s]predicting train subjects:  51%|█████     | 21/41 [00:08<00:06,  2.98it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:08<00:06,  3.11it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:08<00:04,  3.64it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:09<00:04,  3.68it/s]predicting train subjects:  61%|██████    | 25/41 [00:09<00:04,  3.52it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:09<00:04,  3.27it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:10<00:04,  2.96it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:10<00:04,  2.68it/s]predicting train subjects:  71%|███████   | 29/41 [00:11<00:04,  2.77it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:11<00:03,  2.76it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:11<00:03,  2.57it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:12<00:03,  2.61it/s]predicting train subjects:  80%|████████  | 33/41 [00:12<00:03,  2.58it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:13<00:03,  2.32it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:13<00:02,  2.13it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:14<00:02,  1.97it/s]predicting train subjects:  90%|█████████ | 37/41 [00:14<00:01,  2.08it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:15<00:01,  2.11it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:15<00:00,  2.13it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:16<00:00,  2.18it/s]predicting train subjects: 100%|██████████| 41/41 [00:16<00:00,  2.22it/s]
predicting test subjects sagittal:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects sagittal:   9%|▉         | 1/11 [00:00<00:03,  2.92it/s]predicting test subjects sagittal:  18%|█▊        | 2/11 [00:00<00:03,  2.84it/s]predicting test subjects sagittal:  27%|██▋       | 3/11 [00:01<00:02,  2.87it/s]predicting test subjects sagittal:  36%|███▋      | 4/11 [00:01<00:02,  2.96it/s]predicting test subjects sagittal:  45%|████▌     | 5/11 [00:01<00:02,  2.93it/s]predicting test subjects sagittal:  55%|█████▍    | 6/11 [00:02<00:01,  2.94it/s]predicting test subjects sagittal:  64%|██████▎   | 7/11 [00:02<00:01,  2.87it/s]predicting test subjects sagittal:  73%|███████▎  | 8/11 [00:02<00:01,  2.92it/s]predicting test subjects sagittal:  82%|████████▏ | 9/11 [00:03<00:00,  3.03it/s]predicting test subjects sagittal:  91%|█████████ | 10/11 [00:03<00:00,  2.96it/s]predicting test subjects sagittal: 100%|██████████| 11/11 [00:03<00:00,  2.78it/s]
predicting train subjects sagittal:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects sagittal:   2%|▏         | 1/41 [00:00<00:15,  2.56it/s]predicting train subjects sagittal:   5%|▍         | 2/41 [00:00<00:15,  2.59it/s]predicting train subjects sagittal:   7%|▋         | 3/41 [00:01<00:13,  2.74it/s]predicting train subjects sagittal:  10%|▉         | 4/41 [00:01<00:12,  2.98it/s]predicting train subjects sagittal:  12%|█▏        | 5/41 [00:01<00:12,  2.90it/s]predicting train subjects sagittal:  15%|█▍        | 6/41 [00:02<00:11,  2.99it/s]predicting train subjects sagittal:  17%|█▋        | 7/41 [00:02<00:10,  3.22it/s]predicting train subjects sagittal:  20%|█▉        | 8/41 [00:02<00:09,  3.53it/s]predicting train subjects sagittal:  22%|██▏       | 9/41 [00:02<00:09,  3.41it/s]predicting train subjects sagittal:  24%|██▍       | 10/41 [00:03<00:10,  2.92it/s]predicting train subjects sagittal:  27%|██▋       | 11/41 [00:03<00:11,  2.67it/s]predicting train subjects sagittal:  29%|██▉       | 12/41 [00:03<00:09,  2.99it/s]predicting train subjects sagittal:  32%|███▏      | 13/41 [00:04<00:09,  2.93it/s]predicting train subjects sagittal:  34%|███▍      | 14/41 [00:04<00:09,  2.93it/s]predicting train subjects sagittal:  37%|███▋      | 15/41 [00:04<00:08,  2.98it/s]predicting train subjects sagittal:  39%|███▉      | 16/41 [00:05<00:08,  2.98it/s]predicting train subjects sagittal:  41%|████▏     | 17/41 [00:05<00:08,  2.81it/s]predicting train subjects sagittal:  44%|████▍     | 18/41 [00:06<00:08,  2.79it/s]predicting train subjects sagittal:  46%|████▋     | 19/41 [00:06<00:07,  2.91it/s]predicting train subjects sagittal:  49%|████▉     | 20/41 [00:06<00:06,  3.03it/s]predicting train subjects sagittal:  51%|█████     | 21/41 [00:07<00:06,  3.07it/s]predicting train subjects sagittal:  54%|█████▎    | 22/41 [00:07<00:06,  3.11it/s]predicting train subjects sagittal:  56%|█████▌    | 23/41 [00:07<00:05,  3.39it/s]predicting train subjects sagittal:  59%|█████▊    | 24/41 [00:07<00:05,  3.28it/s]predicting train subjects sagittal:  61%|██████    | 25/41 [00:08<00:05,  2.97it/s]predicting train subjects sagittal:  63%|██████▎   | 26/41 [00:08<00:05,  2.89it/s]predicting train subjects sagittal:  66%|██████▌   | 27/41 [00:09<00:05,  2.76it/s]predicting train subjects sagittal:  68%|██████▊   | 28/41 [00:09<00:04,  2.66it/s]predicting train subjects sagittal:  71%|███████   | 29/41 [00:09<00:04,  2.55it/s]predicting train subjects sagittal:  73%|███████▎  | 30/41 [00:10<00:04,  2.55it/s]predicting train subjects sagittal:  76%|███████▌  | 31/41 [00:10<00:03,  2.58it/s]predicting train subjects sagittal:  78%|███████▊  | 32/41 [00:11<00:03,  2.68it/s]predicting train subjects sagittal:  80%|████████  | 33/41 [00:11<00:02,  2.72it/s]predicting train subjects sagittal:  83%|████████▎ | 34/41 [00:11<00:02,  2.53it/s]predicting train subjects sagittal:  85%|████████▌ | 35/41 [00:12<00:02,  2.28it/s]predicting train subjects sagittal:  88%|████████▊ | 36/41 [00:12<00:02,  2.21it/s]predicting train subjects sagittal:  90%|█████████ | 37/41 [00:13<00:01,  2.21it/s]predicting train subjects sagittal:  93%|█████████▎| 38/41 [00:13<00:01,  2.33it/s]predicting train subjects sagittal:  95%|█████████▌| 39/41 [00:14<00:00,  2.31it/s]predicting train subjects sagittal:  98%|█████████▊| 40/41 [00:14<00:00,  2.34it/s]predicting train subjects sagittal: 100%|██████████| 41/41 [00:14<00:00,  2.38it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:03<02:19,  3.48s/it]Loading train:   5%|▍         | 2/41 [00:06<02:14,  3.46s/it]Loading train:   7%|▋         | 3/41 [00:10<02:12,  3.48s/it]Loading train:  10%|▉         | 4/41 [00:13<02:01,  3.28s/it]Loading train:  12%|█▏        | 5/41 [00:17<02:06,  3.51s/it]Loading train:  15%|█▍        | 6/41 [00:20<01:59,  3.41s/it]Loading train:  17%|█▋        | 7/41 [00:23<01:51,  3.29s/it]Loading train:  20%|█▉        | 8/41 [00:26<01:42,  3.10s/it]Loading train:  22%|██▏       | 9/41 [00:28<01:36,  3.02s/it]Loading train:  24%|██▍       | 10/41 [00:31<01:31,  2.94s/it]Loading train:  27%|██▋       | 11/41 [00:34<01:30,  3.03s/it]Loading train:  29%|██▉       | 12/41 [00:37<01:20,  2.78s/it]Loading train:  32%|███▏      | 13/41 [00:40<01:19,  2.83s/it]Loading train:  34%|███▍      | 14/41 [00:42<01:14,  2.77s/it]Loading train:  37%|███▋      | 15/41 [00:45<01:08,  2.63s/it]Loading train:  39%|███▉      | 16/41 [00:48<01:09,  2.78s/it]Loading train:  41%|████▏     | 17/41 [00:52<01:16,  3.18s/it]Loading train:  44%|████▍     | 18/41 [00:55<01:13,  3.18s/it]Loading train:  46%|████▋     | 19/41 [00:58<01:08,  3.13s/it]Loading train:  49%|████▉     | 20/41 [01:00<01:01,  2.94s/it]Loading train:  51%|█████     | 21/41 [01:03<00:54,  2.73s/it]Loading train:  54%|█████▎    | 22/41 [01:05<00:51,  2.70s/it]Loading train:  56%|█████▌    | 23/41 [01:08<00:45,  2.54s/it]Loading train:  59%|█████▊    | 24/41 [01:10<00:42,  2.52s/it]Loading train:  61%|██████    | 25/41 [01:13<00:42,  2.65s/it]Loading train:  63%|██████▎   | 26/41 [01:15<00:38,  2.56s/it]Loading train:  66%|██████▌   | 27/41 [01:18<00:38,  2.75s/it]Loading train:  68%|██████▊   | 28/41 [01:22<00:36,  2.83s/it]Loading train:  71%|███████   | 29/41 [01:24<00:31,  2.61s/it]Loading train:  73%|███████▎  | 30/41 [01:27<00:29,  2.71s/it]Loading train:  76%|███████▌  | 31/41 [01:29<00:26,  2.66s/it]Loading train:  78%|███████▊  | 32/41 [01:32<00:24,  2.73s/it]Loading train:  80%|████████  | 33/41 [01:35<00:21,  2.71s/it]Loading train:  83%|████████▎ | 34/41 [01:38<00:19,  2.77s/it]Loading train:  85%|████████▌ | 35/41 [01:42<00:18,  3.13s/it]Loading train:  88%|████████▊ | 36/41 [01:46<00:17,  3.40s/it]Loading train:  90%|█████████ | 37/41 [01:49<00:13,  3.42s/it]Loading train:  93%|█████████▎| 38/41 [01:52<00:09,  3.21s/it]Loading train:  95%|█████████▌| 39/41 [01:54<00:06,  3.06s/it]Loading train:  98%|█████████▊| 40/41 [01:57<00:03,  3.01s/it]Loading train: 100%|██████████| 41/41 [02:00<00:00,  2.99s/it]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 3/41 [00:00<00:01, 28.29it/s]concatenating: train:  15%|█▍        | 6/41 [00:00<00:01, 27.28it/s]concatenating: train:  22%|██▏       | 9/41 [00:00<00:01, 27.13it/s]concatenating: train:  32%|███▏      | 13/41 [00:00<00:00, 28.25it/s]concatenating: train:  41%|████▏     | 17/41 [00:00<00:00, 29.08it/s]concatenating: train:  54%|█████▎    | 22/41 [00:00<00:00, 31.50it/s]concatenating: train:  66%|██████▌   | 27/41 [00:00<00:00, 35.34it/s]concatenating: train:  80%|████████  | 33/41 [00:00<00:00, 39.44it/s]concatenating: train:  93%|█████████▎| 38/41 [00:01<00:00, 41.97it/s]concatenating: train: 100%|██████████| 41/41 [00:01<00:00, 37.36it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:02<00:28,  2.81s/it]Loading test:  18%|█▊        | 2/11 [00:05<00:25,  2.86s/it]Loading test:  27%|██▋       | 3/11 [00:08<00:22,  2.78s/it]Loading test:  36%|███▋      | 4/11 [00:10<00:18,  2.71s/it]Loading test:  45%|████▌     | 5/11 [00:13<00:16,  2.73s/it]Loading test:  55%|█████▍    | 6/11 [00:15<00:12,  2.56s/it]Loading test:  64%|██████▎   | 7/11 [00:18<00:10,  2.63s/it]Loading test:  73%|███████▎  | 8/11 [00:21<00:07,  2.66s/it]Loading test:  82%|████████▏ | 9/11 [00:23<00:05,  2.55s/it]Loading test:  91%|█████████ | 10/11 [00:26<00:02,  2.65s/it]Loading test: 100%|██████████| 11/11 [00:29<00:00,  2.65s/it]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  27%|██▋       | 3/11 [00:00<00:00, 26.00it/s]concatenating: validation:  64%|██████▎   | 7/11 [00:00<00:00, 29.03it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 35.85it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 128, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 128, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 128, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 128, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 128, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 128, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 128, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 64, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 64, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 64, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 64, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 64, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 64, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 64, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 64, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 64, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 32, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 32, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 32, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 32, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 32, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 32, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 32, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 32, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 32, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 32, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 64, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 64, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 64, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 64, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 64, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 64, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 64, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 64, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 64, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-28 20:48:51.452913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-28 20:48:51.452992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 20:48:51.453007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-28 20:48:51.453017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-28 20:48:51.454946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 58, 64, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 128, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 128, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 128, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 128, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 128, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 128, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 128, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 128, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 128, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 128, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 128, 13) 793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.55070881e-02 3.08829500e-02 7.68487279e-02 1.00911714e-02
 2.67214904e-02 7.01275229e-03 8.03422392e-02 1.16956929e-01
 7.80699947e-02 1.36444062e-02 3.13207977e-01 1.80677386e-01
 3.68873537e-05]
Train on 2690 samples, validate on 698 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 14s - loss: 5.5952 - acc: 0.4003 - mDice: 0.0048 - val_loss: 5.1860 - val_acc: 0.7905 - val_mDice: 0.0078

Epoch 00001: val_mDice improved from -inf to 0.00779, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 4.1724 - acc: 0.7755 - mDice: 0.0340 - val_loss: 3.5935 - val_acc: 0.9480 - val_mDice: 0.0467

Epoch 00002: val_mDice improved from 0.00779 to 0.04673, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 2.8618 - acc: 0.8823 - mDice: 0.0945 - val_loss: 2.2990 - val_acc: 0.9774 - val_mDice: 0.1336

Epoch 00003: val_mDice improved from 0.04673 to 0.13365, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 2.0719 - acc: 0.9691 - mDice: 0.1557 - val_loss: 1.9911 - val_acc: 0.9796 - val_mDice: 0.1580

Epoch 00004: val_mDice improved from 0.13365 to 0.15804, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 1.5748 - acc: 0.9815 - mDice: 0.2161 - val_loss: 1.9870 - val_acc: 0.9833 - val_mDice: 0.1750

Epoch 00005: val_mDice improved from 0.15804 to 0.17495, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 1.2426 - acc: 0.9830 - mDice: 0.2800 - val_loss: 1.3998 - val_acc: 0.9830 - val_mDice: 0.2596

Epoch 00006: val_mDice improved from 0.17495 to 0.25964, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 1.0285 - acc: 0.9835 - mDice: 0.3406 - val_loss: 1.1804 - val_acc: 0.9834 - val_mDice: 0.3177

Epoch 00007: val_mDice improved from 0.25964 to 0.31768, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.9041 - acc: 0.9839 - mDice: 0.3862 - val_loss: 0.9118 - val_acc: 0.9831 - val_mDice: 0.4126

Epoch 00008: val_mDice improved from 0.31768 to 0.41255, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.7939 - acc: 0.9843 - mDice: 0.4317 - val_loss: 0.7068 - val_acc: 0.9832 - val_mDice: 0.4866

Epoch 00009: val_mDice improved from 0.41255 to 0.48662, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.7403 - acc: 0.9845 - mDice: 0.4567 - val_loss: 0.5993 - val_acc: 0.9836 - val_mDice: 0.5464

Epoch 00010: val_mDice improved from 0.48662 to 0.54641, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.7668 - acc: 0.9846 - mDice: 0.4489 - val_loss: 1.4050 - val_acc: 0.9831 - val_mDice: 0.3743

Epoch 00011: val_mDice did not improve from 0.54641
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.6698 - acc: 0.9849 - mDice: 0.4918 - val_loss: 0.5266 - val_acc: 0.9838 - val_mDice: 0.5793

Epoch 00012: val_mDice improved from 0.54641 to 0.57927, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.6099 - acc: 0.9851 - mDice: 0.5232 - val_loss: 0.5284 - val_acc: 0.9837 - val_mDice: 0.5796

Epoch 00013: val_mDice improved from 0.57927 to 0.57960, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.5745 - acc: 0.9853 - mDice: 0.5428 - val_loss: 0.4948 - val_acc: 0.9837 - val_mDice: 0.5969

Epoch 00014: val_mDice improved from 0.57960 to 0.59690, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.5513 - acc: 0.9855 - mDice: 0.5563 - val_loss: 0.5248 - val_acc: 0.9837 - val_mDice: 0.5850

Epoch 00015: val_mDice did not improve from 0.59690
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.5420 - acc: 0.9856 - mDice: 0.5624 - val_loss: 0.4677 - val_acc: 0.9837 - val_mDice: 0.6198

Epoch 00016: val_mDice improved from 0.59690 to 0.61978, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.5110 - acc: 0.9857 - mDice: 0.5807 - val_loss: 0.4532 - val_acc: 0.9839 - val_mDice: 0.6304

Epoch 00017: val_mDice improved from 0.61978 to 0.63042, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.4988 - acc: 0.9858 - mDice: 0.5884 - val_loss: 0.4281 - val_acc: 0.9842 - val_mDice: 0.6392

Epoch 00018: val_mDice improved from 0.63042 to 0.63923, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4707 - acc: 0.9860 - mDice: 0.6054 - val_loss: 0.4000 - val_acc: 0.9841 - val_mDice: 0.6575

Epoch 00019: val_mDice improved from 0.63923 to 0.65748, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4627 - acc: 0.9861 - mDice: 0.6107 - val_loss: 0.3997 - val_acc: 0.9841 - val_mDice: 0.6584

Epoch 00020: val_mDice improved from 0.65748 to 0.65839, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.4588 - acc: 0.9862 - mDice: 0.6141 - val_loss: 0.3906 - val_acc: 0.9843 - val_mDice: 0.6641

Epoch 00021: val_mDice improved from 0.65839 to 0.66406, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4449 - acc: 0.9863 - mDice: 0.6223 - val_loss: 0.3788 - val_acc: 0.9847 - val_mDice: 0.6714

Epoch 00022: val_mDice improved from 0.66406 to 0.67145, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4407 - acc: 0.9864 - mDice: 0.6251 - val_loss: 0.3744 - val_acc: 0.9846 - val_mDice: 0.6749

Epoch 00023: val_mDice improved from 0.67145 to 0.67495, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4402 - acc: 0.9864 - mDice: 0.6258 - val_loss: 0.3731 - val_acc: 0.9848 - val_mDice: 0.6764

Epoch 00024: val_mDice improved from 0.67495 to 0.67644, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4311 - acc: 0.9865 - mDice: 0.6315 - val_loss: 0.3716 - val_acc: 0.9849 - val_mDice: 0.6770

Epoch 00025: val_mDice improved from 0.67644 to 0.67703, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4235 - acc: 0.9866 - mDice: 0.6366 - val_loss: 0.3645 - val_acc: 0.9850 - val_mDice: 0.6816

Epoch 00026: val_mDice improved from 0.67703 to 0.68163, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4171 - acc: 0.9867 - mDice: 0.6409 - val_loss: 0.3618 - val_acc: 0.9851 - val_mDice: 0.6838

Epoch 00027: val_mDice improved from 0.68163 to 0.68379, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4195 - acc: 0.9866 - mDice: 0.6393 - val_loss: 0.3551 - val_acc: 0.9861 - val_mDice: 0.6879

Epoch 00028: val_mDice improved from 0.68379 to 0.68788, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.4064 - acc: 0.9868 - mDice: 0.6484 - val_loss: 0.3503 - val_acc: 0.9863 - val_mDice: 0.6920

Epoch 00029: val_mDice improved from 0.68788 to 0.69203, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4052 - acc: 0.9870 - mDice: 0.6492 - val_loss: 0.3607 - val_acc: 0.9862 - val_mDice: 0.6848

Epoch 00030: val_mDice did not improve from 0.69203
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3999 - acc: 0.9869 - mDice: 0.6526 - val_loss: 0.3476 - val_acc: 0.9865 - val_mDice: 0.6939

Epoch 00031: val_mDice improved from 0.69203 to 0.69386, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3983 - acc: 0.9870 - mDice: 0.6540 - val_loss: 0.3497 - val_acc: 0.9863 - val_mDice: 0.6929

Epoch 00032: val_mDice did not improve from 0.69386
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3955 - acc: 0.9871 - mDice: 0.6558 - val_loss: 0.3558 - val_acc: 0.9864 - val_mDice: 0.6882

Epoch 00033: val_mDice did not improve from 0.69386
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3903 - acc: 0.9870 - mDice: 0.6595 - val_loss: 0.3608 - val_acc: 0.9864 - val_mDice: 0.6852

Epoch 00034: val_mDice did not improve from 0.69386
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3880 - acc: 0.9871 - mDice: 0.6611 - val_loss: 0.3496 - val_acc: 0.9870 - val_mDice: 0.6926

Epoch 00035: val_mDice did not improve from 0.69386
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3839 - acc: 0.9872 - mDice: 0.6637 - val_loss: 0.3481 - val_acc: 0.9870 - val_mDice: 0.6937

Epoch 00036: val_mDice did not improve from 0.69386
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3706 - acc: 0.9874 - mDice: 0.6730 - val_loss: 0.3483 - val_acc: 0.9870 - val_mDice: 0.6937

Epoch 00037: val_mDice did not improve from 0.69386
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3743 - acc: 0.9874 - mDice: 0.6706 - val_loss: 0.3492 - val_acc: 0.9869 - val_mDice: 0.6932

Epoch 00038: val_mDice did not improve from 0.69386
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3707 - acc: 0.9873 - mDice: 0.6731 - val_loss: 0.3465 - val_acc: 0.9866 - val_mDice: 0.6951

Epoch 00039: val_mDice improved from 0.69386 to 0.69513, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3711 - acc: 0.9873 - mDice: 0.6730 - val_loss: 0.3424 - val_acc: 0.9870 - val_mDice: 0.6978

Epoch 00040: val_mDice improved from 0.69513 to 0.69784, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3694 - acc: 0.9874 - mDice: 0.6740 - val_loss: 0.3467 - val_acc: 0.9870 - val_mDice: 0.6947

Epoch 00041: val_mDice did not improve from 0.69784
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3647 - acc: 0.9875 - mDice: 0.6774 - val_loss: 0.3425 - val_acc: 0.9873 - val_mDice: 0.6976

Epoch 00042: val_mDice did not improve from 0.69784
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3646 - acc: 0.9875 - mDice: 0.6776 - val_loss: 0.3436 - val_acc: 0.9873 - val_mDice: 0.6968

Epoch 00043: val_mDice did not improve from 0.69784
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3585 - acc: 0.9875 - mDice: 0.6816 - val_loss: 0.3425 - val_acc: 0.9878 - val_mDice: 0.6972

Epoch 00044: val_mDice did not improve from 0.69784
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3621 - acc: 0.9875 - mDice: 0.6796 - val_loss: 0.3384 - val_acc: 0.9877 - val_mDice: 0.7005

Epoch 00045: val_mDice improved from 0.69784 to 0.70051, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3575 - acc: 0.9876 - mDice: 0.6824 - val_loss: 0.3431 - val_acc: 0.9876 - val_mDice: 0.6972

Epoch 00046: val_mDice did not improve from 0.70051
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3581 - acc: 0.9876 - mDice: 0.6824 - val_loss: 0.3405 - val_acc: 0.9876 - val_mDice: 0.6991

Epoch 00047: val_mDice did not improve from 0.70051
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3554 - acc: 0.9876 - mDice: 0.6842 - val_loss: 0.3366 - val_acc: 0.9881 - val_mDice: 0.7018

Epoch 00048: val_mDice improved from 0.70051 to 0.70179, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3547 - acc: 0.9877 - mDice: 0.6846 - val_loss: 0.3388 - val_acc: 0.9883 - val_mDice: 0.7001

Epoch 00049: val_mDice did not improve from 0.70179
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3552 - acc: 0.9877 - mDice: 0.6845 - val_loss: 0.3388 - val_acc: 0.9882 - val_mDice: 0.7004

Epoch 00050: val_mDice did not improve from 0.70179
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3496 - acc: 0.9877 - mDice: 0.6884 - val_loss: 0.3436 - val_acc: 0.9876 - val_mDice: 0.6974

Epoch 00051: val_mDice did not improve from 0.70179
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3548 - acc: 0.9877 - mDice: 0.6847 - val_loss: 0.3471 - val_acc: 0.9877 - val_mDice: 0.6953

Epoch 00052: val_mDice did not improve from 0.70179
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3476 - acc: 0.9878 - mDice: 0.6898 - val_loss: 0.3348 - val_acc: 0.9878 - val_mDice: 0.7036

Epoch 00053: val_mDice improved from 0.70179 to 0.70360, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3466 - acc: 0.9878 - mDice: 0.6905 - val_loss: 0.3378 - val_acc: 0.9880 - val_mDice: 0.7010

Epoch 00054: val_mDice did not improve from 0.70360
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3448 - acc: 0.9879 - mDice: 0.6921 - val_loss: 0.3448 - val_acc: 0.9880 - val_mDice: 0.6964

Epoch 00055: val_mDice did not improve from 0.70360
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3409 - acc: 0.9879 - mDice: 0.6946 - val_loss: 0.3381 - val_acc: 0.9880 - val_mDice: 0.7010

Epoch 00056: val_mDice did not improve from 0.70360
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3416 - acc: 0.9879 - mDice: 0.6942 - val_loss: 0.3404 - val_acc: 0.9883 - val_mDice: 0.6995

Epoch 00057: val_mDice did not improve from 0.70360
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3405 - acc: 0.9879 - mDice: 0.6949 - val_loss: 0.3383 - val_acc: 0.9880 - val_mDice: 0.7010

Epoch 00058: val_mDice did not improve from 0.70360
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3401 - acc: 0.9878 - mDice: 0.6953 - val_loss: 0.3383 - val_acc: 0.9881 - val_mDice: 0.7008

Epoch 00059: val_mDice did not improve from 0.70360
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3432 - acc: 0.9879 - mDice: 0.6931 - val_loss: 0.3362 - val_acc: 0.9882 - val_mDice: 0.7022

Epoch 00060: val_mDice did not improve from 0.70360
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3394 - acc: 0.9880 - mDice: 0.6959 - val_loss: 0.3369 - val_acc: 0.9879 - val_mDice: 0.7019

Epoch 00061: val_mDice did not improve from 0.70360
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3414 - acc: 0.9879 - mDice: 0.6948 - val_loss: 0.3382 - val_acc: 0.9877 - val_mDice: 0.7012

Epoch 00062: val_mDice did not improve from 0.70360
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3370 - acc: 0.9879 - mDice: 0.6974 - val_loss: 0.3376 - val_acc: 0.9879 - val_mDice: 0.7015

Epoch 00063: val_mDice did not improve from 0.70360
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3367 - acc: 0.9879 - mDice: 0.6980 - val_loss: 0.3336 - val_acc: 0.9881 - val_mDice: 0.7042

Epoch 00064: val_mDice improved from 0.70360 to 0.70424, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.3327 - acc: 0.9880 - mDice: 0.7005 - val_loss: 0.3335 - val_acc: 0.9881 - val_mDice: 0.7043

Epoch 00065: val_mDice improved from 0.70424 to 0.70430, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3378 - acc: 0.9880 - mDice: 0.6970 - val_loss: 0.3315 - val_acc: 0.9882 - val_mDice: 0.7056

Epoch 00066: val_mDice improved from 0.70430 to 0.70563, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3346 - acc: 0.9880 - mDice: 0.6994 - val_loss: 0.3388 - val_acc: 0.9881 - val_mDice: 0.7004

Epoch 00067: val_mDice did not improve from 0.70563
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3346 - acc: 0.9880 - mDice: 0.6993 - val_loss: 0.3376 - val_acc: 0.9882 - val_mDice: 0.7014

Epoch 00068: val_mDice did not improve from 0.70563
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3315 - acc: 0.9880 - mDice: 0.7016 - val_loss: 0.3380 - val_acc: 0.9882 - val_mDice: 0.7010

Epoch 00069: val_mDice did not improve from 0.70563
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3340 - acc: 0.9880 - mDice: 0.6999 - val_loss: 0.3332 - val_acc: 0.9882 - val_mDice: 0.7043

Epoch 00070: val_mDice did not improve from 0.70563
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3329 - acc: 0.9881 - mDice: 0.7006 - val_loss: 0.3351 - val_acc: 0.9884 - val_mDice: 0.7032

Epoch 00071: val_mDice did not improve from 0.70563
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3312 - acc: 0.9881 - mDice: 0.7020 - val_loss: 0.3335 - val_acc: 0.9882 - val_mDice: 0.7040

Epoch 00072: val_mDice did not improve from 0.70563
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3323 - acc: 0.9881 - mDice: 0.7010 - val_loss: 0.3353 - val_acc: 0.9882 - val_mDice: 0.7028

Epoch 00073: val_mDice did not improve from 0.70563
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3298 - acc: 0.9881 - mDice: 0.7029 - val_loss: 0.3360 - val_acc: 0.9882 - val_mDice: 0.7024

Epoch 00074: val_mDice did not improve from 0.70563
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3280 - acc: 0.9882 - mDice: 0.7042 - val_loss: 0.3353 - val_acc: 0.9882 - val_mDice: 0.7030

Epoch 00075: val_mDice did not improve from 0.70563
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.3306 - acc: 0.9882 - mDice: 0.7025 - val_loss: 0.3352 - val_acc: 0.9884 - val_mDice: 0.7031

Epoch 00076: val_mDice did not improve from 0.70563
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 14s - loss: 0.3324 - acc: 0.9882 - mDice: 0.7011 - val_loss: 0.3328 - val_acc: 0.9885 - val_mDice: 0.7046

Epoch 00077: val_mDice did not improve from 0.70563
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.3267 - acc: 0.9882 - mDice: 0.7051 - val_loss: 0.3318 - val_acc: 0.9883 - val_mDice: 0.7055

Epoch 00078: val_mDice did not improve from 0.70563
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3272 - acc: 0.9882 - mDice: 0.7049 - val_loss: 0.3311 - val_acc: 0.9884 - val_mDice: 0.7059

Epoch 00079: val_mDice improved from 0.70563 to 0.70590, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3260 - acc: 0.9882 - mDice: 0.7059 - val_loss: 0.3314 - val_acc: 0.9884 - val_mDice: 0.7058

Epoch 00080: val_mDice did not improve from 0.70590
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3266 - acc: 0.9882 - mDice: 0.7052 - val_loss: 0.3327 - val_acc: 0.9883 - val_mDice: 0.7049

Epoch 00081: val_mDice did not improve from 0.70590
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.3253 - acc: 0.9882 - mDice: 0.7063 - val_loss: 0.3314 - val_acc: 0.9881 - val_mDice: 0.7058

Epoch 00082: val_mDice did not improve from 0.70590
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.3247 - acc: 0.9882 - mDice: 0.7067 - val_loss: 0.3333 - val_acc: 0.9883 - val_mDice: 0.7045

Epoch 00083: val_mDice did not improve from 0.70590
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3239 - acc: 0.9882 - mDice: 0.7073 - val_loss: 0.3299 - val_acc: 0.9885 - val_mDice: 0.7068

Epoch 00084: val_mDice improved from 0.70590 to 0.70680, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3254 - acc: 0.9883 - mDice: 0.7063 - val_loss: 0.3313 - val_acc: 0.9884 - val_mDice: 0.7059

Epoch 00085: val_mDice did not improve from 0.70680
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3240 - acc: 0.9883 - mDice: 0.7072 - val_loss: 0.3311 - val_acc: 0.9885 - val_mDice: 0.7061

Epoch 00086: val_mDice did not improve from 0.70680
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3266 - acc: 0.9882 - mDice: 0.7055 - val_loss: 0.3298 - val_acc: 0.9885 - val_mDice: 0.7068

Epoch 00087: val_mDice improved from 0.70680 to 0.70683, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 88/300

Epoch 00088: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3239 - acc: 0.9882 - mDice: 0.7073 - val_loss: 0.3286 - val_acc: 0.9884 - val_mDice: 0.7079

Epoch 00088: val_mDice improved from 0.70683 to 0.70787, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 89/300

Epoch 00089: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3273 - acc: 0.9882 - mDice: 0.7049 - val_loss: 0.3324 - val_acc: 0.9883 - val_mDice: 0.7051

Epoch 00089: val_mDice did not improve from 0.70787
Epoch 90/300

Epoch 00090: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.3251 - acc: 0.9883 - mDice: 0.7064 - val_loss: 0.3313 - val_acc: 0.9885 - val_mDice: 0.7058

Epoch 00090: val_mDice did not improve from 0.70787
Epoch 91/300

Epoch 00091: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3214 - acc: 0.9883 - mDice: 0.7093 - val_loss: 0.3321 - val_acc: 0.9885 - val_mDice: 0.7053

Epoch 00091: val_mDice did not improve from 0.70787
Epoch 92/300

Epoch 00092: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3210 - acc: 0.9883 - mDice: 0.7095 - val_loss: 0.3321 - val_acc: 0.9885 - val_mDice: 0.7054

Epoch 00092: val_mDice did not improve from 0.70787
Epoch 93/300

Epoch 00093: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3211 - acc: 0.9883 - mDice: 0.7094 - val_loss: 0.3307 - val_acc: 0.9884 - val_mDice: 0.7062

Epoch 00093: val_mDice did not improve from 0.70787
Epoch 94/300

Epoch 00094: LearningRateScheduler setting learning rate to 3.125e-05.
 - 13s - loss: 0.3188 - acc: 0.9883 - mDice: 0.7111 - val_loss: 0.3332 - val_acc: 0.9884 - val_mDice: 0.7046

Epoch 00094: val_mDice did not improve from 0.70787
Epoch 95/300

Epoch 00095: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3236 - acc: 0.9883 - mDice: 0.7075 - val_loss: 0.3317 - val_acc: 0.9884 - val_mDice: 0.7056

Epoch 00095: val_mDice did not improve from 0.70787
Epoch 96/300

Epoch 00096: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3212 - acc: 0.9883 - mDice: 0.7092 - val_loss: 0.3309 - val_acc: 0.9884 - val_mDice: 0.7061

Epoch 00096: val_mDice did not improve from 0.70787
Epoch 97/300

Epoch 00097: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.3215 - acc: 0.9883 - mDice: 0.7093 - val_loss: 0.3339 - val_acc: 0.9885 - val_mDice: 0.7041

Epoch 00097: val_mDice did not improve from 0.70787
Epoch 98/300

Epoch 00098: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3230 - acc: 0.9883 - mDice: 0.7080 - val_loss: 0.3312 - val_acc: 0.9886 - val_mDice: 0.7059

Epoch 00098: val_mDice did not improve from 0.70787
Epoch 99/300

Epoch 00099: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.3222 - acc: 0.9884 - mDice: 0.7086 - val_loss: 0.3318 - val_acc: 0.9885 - val_mDice: 0.7055

Epoch 00099: val_mDice did not improve from 0.70787
Epoch 100/300

Epoch 00100: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3215 - acc: 0.9883 - mDice: 0.7090 - val_loss: 0.3333 - val_acc: 0.9885 - val_mDice: 0.7044

Epoch 00100: val_mDice did not improve from 0.70787
Epoch 101/300

Epoch 00101: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3745 - acc: 0.9884 - mDice: 0.7009 - val_loss: 0.3480 - val_acc: 0.9881 - val_mDice: 0.6951

Epoch 00101: val_mDice did not improve from 0.70787
Epoch 102/300

Epoch 00102: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3263 - acc: 0.9883 - mDice: 0.7055 - val_loss: 0.3397 - val_acc: 0.9884 - val_mDice: 0.7002

Epoch 00102: val_mDice did not improve from 0.70787
Epoch 103/300

Epoch 00103: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3243 - acc: 0.9883 - mDice: 0.7076 - val_loss: 0.3356 - val_acc: 0.9883 - val_mDice: 0.7029

Epoch 00103: val_mDice did not improve from 0.70787
Epoch 104/300

Epoch 00104: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3208 - acc: 0.9883 - mDice: 0.7096 - val_loss: 0.3333 - val_acc: 0.9885 - val_mDice: 0.7045

Epoch 00104: val_mDice did not improve from 0.70787
Epoch 105/300

Epoch 00105: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3201 - acc: 0.9883 - mDice: 0.7102 - val_loss: 0.3319 - val_acc: 0.9884 - val_mDice: 0.7055

Epoch 00105: val_mDice did not improve from 0.70787
Epoch 106/300

Epoch 00106: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3198 - acc: 0.9883 - mDice: 0.7104 - val_loss: 0.3316 - val_acc: 0.9885 - val_mDice: 0.7057

Epoch 00106: val_mDice did not improve from 0.70787
Epoch 107/300

Epoch 00107: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.3192 - acc: 0.9884 - mDice: 0.7110 - val_loss: 0.3317 - val_acc: 0.9885 - val_mDice: 0.7057

Epoch 00107: val_mDice did not improve from 0.70787
Epoch 108/300

Epoch 00108: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3196 - acc: 0.9884 - mDice: 0.7105 - val_loss: 0.3318 - val_acc: 0.9885 - val_mDice: 0.7056

Epoch 00108: val_mDice did not improve from 0.70787
Epoch 109/300

Epoch 00109: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.3152 - acc: 0.9884 - mDice: 0.7136 - val_loss: 0.3320 - val_acc: 0.9885 - val_mDice: 0.7056

Epoch 00109: val_mDice did not improve from 0.70787
Epoch 110/300

Epoch 00110: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.3198 - acc: 0.9884 - mDice: 0.7104 - val_loss: 0.3313 - val_acc: 0.9885 - val_mDice: 0.7060

Epoch 00110: val_mDice did not improve from 0.70787
Epoch 111/300

Epoch 00111: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3170 - acc: 0.9884 - mDice: 0.7125 - val_loss: 0.3317 - val_acc: 0.9885 - val_mDice: 0.7058

Epoch 00111: val_mDice did not improve from 0.70787
Epoch 112/300

Epoch 00112: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3198 - acc: 0.9884 - mDice: 0.7103 - val_loss: 0.3318 - val_acc: 0.9885 - val_mDice: 0.7056

Epoch 00112: val_mDice did not improve from 0.70787
Epoch 113/300

Epoch 00113: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3160 - acc: 0.9884 - mDice: 0.7131 - val_loss: 0.3302 - val_acc: 0.9886 - val_mDice: 0.7067

Epoch 00113: val_mDice did not improve from 0.70787
Epoch 114/300

Epoch 00114: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3186 - acc: 0.9884 - mDice: 0.7113 - val_loss: 0.3319 - val_acc: 0.9885 - val_mDice: 0.7055

Epoch 00114: val_mDice did not improve from 0.70787
Epoch 115/300

Epoch 00115: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3185 - acc: 0.9884 - mDice: 0.7115 - val_loss: 0.3317 - val_acc: 0.9886 - val_mDice: 0.7057

Epoch 00115: val_mDice did not improve from 0.70787
Epoch 116/300

Epoch 00116: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3178 - acc: 0.9884 - mDice: 0.7123 - val_loss: 0.3322 - val_acc: 0.9885 - val_mDice: 0.7053

Epoch 00116: val_mDice did not improve from 0.70787
Epoch 117/300

Epoch 00117: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3157 - acc: 0.9884 - mDice: 0.7135 - val_loss: 0.3321 - val_acc: 0.9886 - val_mDice: 0.7055

Epoch 00117: val_mDice did not improve from 0.70787
Epoch 118/300

Epoch 00118: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3173 - acc: 0.9884 - mDice: 0.7123 - val_loss: 0.3327 - val_acc: 0.9886 - val_mDice: 0.7050

Epoch 00118: val_mDice did not improve from 0.70787
Epoch 119/300

Epoch 00119: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3179 - acc: 0.9884 - mDice: 0.7118 - val_loss: 0.3314 - val_acc: 0.9886 - val_mDice: 0.7059

Epoch 00119: val_mDice did not improve from 0.70787
Epoch 120/300

Epoch 00120: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3169 - acc: 0.9884 - mDice: 0.7126 - val_loss: 0.3313 - val_acc: 0.9886 - val_mDice: 0.7060

Epoch 00120: val_mDice did not improve from 0.70787
Epoch 121/300

Epoch 00121: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3194 - acc: 0.9884 - mDice: 0.7109 - val_loss: 0.3318 - val_acc: 0.9886 - val_mDice: 0.7056

Epoch 00121: val_mDice did not improve from 0.70787
Epoch 122/300

Epoch 00122: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3154 - acc: 0.9884 - mDice: 0.7139 - val_loss: 0.3312 - val_acc: 0.9886 - val_mDice: 0.7059

Epoch 00122: val_mDice did not improve from 0.70787
Epoch 123/300

Epoch 00123: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3170 - acc: 0.9884 - mDice: 0.7123 - val_loss: 0.3316 - val_acc: 0.9886 - val_mDice: 0.7058

Epoch 00123: val_mDice did not improve from 0.70787
Epoch 124/300

Epoch 00124: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3180 - acc: 0.9884 - mDice: 0.7121 - val_loss: 0.3306 - val_acc: 0.9886 - val_mDice: 0.7066

Epoch 00124: val_mDice did not improve from 0.70787
Epoch 125/300

Epoch 00125: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3164 - acc: 0.9884 - mDice: 0.7128 - val_loss: 0.3310 - val_acc: 0.9886 - val_mDice: 0.7062

Epoch 00125: val_mDice did not improve from 0.70787
Epoch 126/300

Epoch 00126: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3177 - acc: 0.9884 - mDice: 0.7120 - val_loss: 0.3315 - val_acc: 0.9886 - val_mDice: 0.7058

Epoch 00126: val_mDice did not improve from 0.70787
Epoch 127/300

Epoch 00127: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3189 - acc: 0.9884 - mDice: 0.7111 - val_loss: 0.3307 - val_acc: 0.9886 - val_mDice: 0.7063

Epoch 00127: val_mDice did not improve from 0.70787
Epoch 128/300

Epoch 00128: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3167 - acc: 0.9884 - mDice: 0.7125 - val_loss: 0.3310 - val_acc: 0.9886 - val_mDice: 0.7062

Epoch 00128: val_mDice did not improve from 0.70787
Restoring model weights from the end of the best epoch
Epoch 00128: early stopping
{'val_loss': [5.185994559509365, 3.5934540234869052, 2.298950597686549, 1.991149847008779, 1.9869889550359339, 1.399752087442786, 1.180373830235107, 0.9117559611284972, 0.7068495950247975, 0.5993205829814375, 1.405011155202259, 0.5266188862159805, 0.5284293031794977, 0.49482985545366065, 0.5248292502666955, 0.46767608876897815, 0.45315236331398645, 0.42811165429801173, 0.39997479174403544, 0.3997333788427719, 0.39059990157712154, 0.3788422815779219, 0.3744014882426549, 0.3730897424384994, 0.37157894530064056, 0.36447138046808436, 0.3617832117572555, 0.3551127819242997, 0.3503392612011863, 0.3606871731130988, 0.34762523184533106, 0.34970360644225745, 0.35579481131709406, 0.360769739294462, 0.349592925740518, 0.34805856897018017, 0.34828923339488516, 0.34924718447264425, 0.3464652936062362, 0.3424157629211174, 0.34674820585715393, 0.3424716003473987, 0.34357751098949796, 0.34253740148421347, 0.3383991647415653, 0.343061684878986, 0.3404575059441236, 0.3366276165817392, 0.3388230332672425, 0.3388041360330445, 0.3436427214596538, 0.34708696467145467, 0.33476677545503764, 0.33783921069948586, 0.344809238924338, 0.33813450688277413, 0.3404262011235628, 0.33830095558590056, 0.3383436437834983, 0.3361832017030962, 0.33694791759665854, 0.33819457871524516, 0.33764265575859814, 0.3336163714826961, 0.3335449675946659, 0.33148162515252233, 0.33883772572678617, 0.3375991486375858, 0.33800966760148976, 0.33318890071871626, 0.33508299261246166, 0.3334875419351638, 0.3353402724751087, 0.33596850235687625, 0.33530537344323186, 0.3351776779210328, 0.3328317406355139, 0.3318288563657285, 0.33114161558000954, 0.3313693507523796, 0.3326671552179878, 0.3314235094283577, 0.33325713488638914, 0.3299122289634366, 0.33130433139281834, 0.3310977119770979, 0.32982225669147636, 0.3286378289703653, 0.3323725609690549, 0.331282234892121, 0.33206251400248027, 0.33211366520570135, 0.33074937022176376, 0.33318488816477165, 0.3316674452798073, 0.3309328623521635, 0.333884555921172, 0.3311796009882132, 0.33178140927522437, 0.33328553504793557, 0.3480459635271384, 0.33968035445513906, 0.33560763995421994, 0.3332842802762302, 0.331856700274186, 0.3315892033385684, 0.3316634574045766, 0.331832050081652, 0.3319544743501013, 0.3312992306698359, 0.33166610315057815, 0.331802654402987, 0.3301882972689959, 0.3318947564735795, 0.3316873226602986, 0.33223573159010156, 0.3320625734363381, 0.3326876604966926, 0.3314269115897509, 0.3312915827449208, 0.33181603060752407, 0.3312317624816239, 0.33155048995099984, 0.33057770261108704, 0.33096578529366105, 0.331454787862335, 0.3307123816969743, 0.330974804125406], 'val_acc': [0.7904873509120122, 0.94804949917561, 0.9774422905165008, 0.9795961998253634, 0.9832581389257764, 0.9829649195971667, 0.9833839670634884, 0.983077229264814, 0.9832163663170057, 0.9836417718398195, 0.9831222927672815, 0.9837628710577344, 0.983706432461397, 0.983729486649904, 0.9837053604317257, 0.9837105774606196, 0.9838716187927989, 0.9841731393917925, 0.9841145792799898, 0.9841287554505903, 0.9843247389383507, 0.9846537470134418, 0.9846398664750479, 0.9848076562826817, 0.9849337718548269, 0.9850056466878656, 0.9850910447730035, 0.9860616281927486, 0.9862862512853562, 0.9862257476180877, 0.9865162871286999, 0.9863424994542469, 0.9864005781176438, 0.9863752107907161, 0.9870108791272073, 0.9870467783728438, 0.9869821175804794, 0.9868544796134817, 0.98659935567987, 0.9869831870483464, 0.9869602214982653, 0.9873418420297027, 0.9872723544566201, 0.9878185825224933, 0.9876951807549484, 0.9875951086552573, 0.9875722508717403, 0.9880728292943414, 0.9882877106994475, 0.9882032886617164, 0.9875738868399131, 0.9876693198537416, 0.9878177135585031, 0.9880043196473217, 0.9879601290369444, 0.9880403181201749, 0.9882887735066236, 0.9880461054066874, 0.988107665049654, 0.9882274997610075, 0.9878771410972136, 0.9877278787700358, 0.9879062006671654, 0.9880943559643874, 0.9881364255716603, 0.9881811953205776, 0.988111043557055, 0.9881879461870494, 0.9881914126497285, 0.9881530255506237, 0.988420101667202, 0.9882441004231188, 0.9881952816572436, 0.9882471892758563, 0.9882152539952778, 0.9883708904329207, 0.9884894744714556, 0.9883120252956292, 0.9883649041796141, 0.9883798611881398, 0.9883164734416842, 0.9881404805661614, 0.9883232227710734, 0.9885194771269331, 0.9883799675884083, 0.9884786713430738, 0.9884598441315243, 0.9883896993702667, 0.9883438685220429, 0.9884916958972855, 0.9884654565322023, 0.9884801047579266, 0.9883728261321868, 0.9883801452068339, 0.9884430552621967, 0.9884451815596966, 0.9885293368282154, 0.9885533450326127, 0.9885297146089439, 0.9885034738775652, 0.9880940509388987, 0.9883510217625637, 0.9883113500040377, 0.9884783786142453, 0.988449235700263, 0.9884780817865301, 0.9885212179582918, 0.9884659302951955, 0.9885136947932421, 0.9885066452203985, 0.9885160082732367, 0.9885287346334375, 0.9885635738044892, 0.9885461089604222, 0.9885614685137839, 0.988534145847091, 0.988554711328195, 0.9885802767679821, 0.9885859503103874, 0.9885743859845793, 0.988581614883718, 0.9886103668663768, 0.9886187656561078, 0.988647719338152, 0.9885813245459067, 0.9885776599703682, 0.9886150064646002, 0.988626686242385], 'val_mDice': [0.007793614827630172, 0.0467330636643407, 0.13364939020151395, 0.15803795865067097, 0.1749516453988914, 0.2596405716372766, 0.31767776702399925, 0.4125540011420974, 0.48661702914019367, 0.5464128944115516, 0.3743463989485984, 0.5792663175943588, 0.5796003987249467, 0.5969033389856617, 0.585011192584106, 0.6197768005737261, 0.6304217189294219, 0.639228501087615, 0.6574826394247804, 0.6583921618994464, 0.6640584747224277, 0.6714466410584983, 0.6749472377293431, 0.6764402637167442, 0.6770260737072089, 0.681627853210471, 0.6837893611722142, 0.687878423228988, 0.6920345138683701, 0.6847757193283912, 0.693857137658193, 0.6929016849372995, 0.688214005576847, 0.6852157686364685, 0.6925759272793985, 0.6936917004407648, 0.6937319848120725, 0.6932060110534843, 0.6951269326373978, 0.6978416005656507, 0.6947127112686463, 0.6976468303483674, 0.6967892394024868, 0.6971911352480038, 0.7005119016313963, 0.6971729310672399, 0.6990754778542286, 0.7017861358757347, 0.7001381612097295, 0.7003763287320178, 0.6973720385898492, 0.6952991782764992, 0.7035994485319514, 0.7010401504429159, 0.6964422478033684, 0.7009857911776677, 0.6995203054395309, 0.7009673678772498, 0.7008246839217265, 0.7022009732730068, 0.7019134093833858, 0.7012070458393043, 0.7014770745206358, 0.704242049110653, 0.7043005760897879, 0.7056258961259465, 0.7003907206065334, 0.7013653281083422, 0.7010456699354942, 0.7043307184489886, 0.7031780450596851, 0.7040286918082688, 0.7027893213283025, 0.7023919289638115, 0.703048907241712, 0.7031174303809005, 0.7046276430004306, 0.7055270798226824, 0.7058999920984393, 0.7058471712820168, 0.7048800838710927, 0.7058049568474122, 0.7044983150965847, 0.7067969132289504, 0.7058712121067211, 0.7061430140689314, 0.7068328976972739, 0.7078691112960991, 0.705090049173907, 0.7058498309471546, 0.7053288790079789, 0.7053539650488037, 0.7062395710316633, 0.704588604178333, 0.7056119505177255, 0.7061335483731377, 0.7041082978248596, 0.7058701540815796, 0.7055133847589138, 0.7044429502377879, 0.6951215229608267, 0.7002064905057322, 0.7029340072484276, 0.7045167397632981, 0.705508638418848, 0.7056734543133601, 0.7057063220906736, 0.7056288896795672, 0.7055680107933744, 0.705993429808357, 0.7057574172757758, 0.7056404297536287, 0.7067260499670034, 0.7055001014624762, 0.7056855311707986, 0.7053399203841529, 0.7054962797629457, 0.7050025558744939, 0.7058904471575018, 0.7059656854686901, 0.7056051120375494, 0.7059322448719538, 0.7057749830549289, 0.7065535122822212, 0.7061778476040138, 0.7058343914655013, 0.706349574221581, 0.706168113601925], 'loss': [5.595211259494484, 4.172352534687652, 2.8618491422731194, 2.071927758840823, 1.574750630828971, 1.242634780788067, 1.028508257023907, 0.904137820780942, 0.7939442223332629, 0.7403182371841487, 0.7668391847255948, 0.6698139245625322, 0.6098599759619475, 0.5744826895596813, 0.5513321531306412, 0.5420021494525072, 0.5109943776325665, 0.49881496903621575, 0.4707171440567669, 0.46269706567423935, 0.45877709131701727, 0.44494616054690905, 0.44074088412589746, 0.44023121256367426, 0.43114014152700575, 0.42350773685040527, 0.4171488024709836, 0.41945648259832957, 0.40641081034029286, 0.40517596617950385, 0.3998861438058123, 0.3983477067105389, 0.3954768446741494, 0.39026337762304397, 0.387964814568984, 0.38385770023977, 0.3706108949884606, 0.37432238747639285, 0.37074292603478554, 0.37111684225748903, 0.36939132446250067, 0.3647026306634499, 0.3645603083988105, 0.3585267871285903, 0.36210735258559756, 0.3575058579444885, 0.3581495516583822, 0.3554069905032899, 0.3547148006556202, 0.35522953318397354, 0.3496480630676099, 0.3547818748259633, 0.3475802284855825, 0.34662888639478434, 0.3447782027233932, 0.3409209946052736, 0.3416400060999349, 0.3404887754013104, 0.3401331535945595, 0.3432302735330447, 0.339381401423628, 0.34137421713442606, 0.33695267722509165, 0.33666307199400153, 0.33272387801936126, 0.33780285162110313, 0.3346124978313659, 0.33455728653638334, 0.33146424497370386, 0.3340024372015744, 0.33286786566879667, 0.33118473863069897, 0.3322555554622168, 0.3298264080485447, 0.32801262696436345, 0.33061916577771694, 0.33242078973015, 0.326734806968377, 0.3271965546235719, 0.3260264793293184, 0.32664958828000773, 0.3252872691721721, 0.32474636499766524, 0.3238639973353276, 0.3254209176315251, 0.3240108895479082, 0.3265930061889847, 0.32385451609760413, 0.3272798451349195, 0.32513522902385894, 0.3213827080443003, 0.3210069984751563, 0.3210804323946233, 0.31881951299741806, 0.3235612661200385, 0.32124841490199574, 0.3214657849538725, 0.3229771107989173, 0.3221896288563328, 0.32149401096606345, 0.3745278825768751, 0.32625907083426264, 0.3243108890313641, 0.32075810144381894, 0.3201262708268644, 0.3197726045620929, 0.3191913325325707, 0.31962740365900516, 0.31516374243236384, 0.31982061865161343, 0.3170157830511327, 0.31979705232669875, 0.3160296848050724, 0.3185899438246475, 0.3184623856748347, 0.3178333316593808, 0.31571421913497955, 0.31730085179265105, 0.31793442867059246, 0.3169194591754431, 0.3194323569868577, 0.3154317612541653, 0.31701618045236096, 0.3180422665461288, 0.31643811311420456, 0.31771007490423975, 0.31890288704389974, 0.3167456181297515], 'acc': [0.40033497281898794, 0.7754906495707629, 0.882345888029687, 0.9691237813921223, 0.9814746078505392, 0.9829949191511785, 0.9834694895602514, 0.9839106660793262, 0.9842532975079845, 0.984467965076404, 0.9845611240340875, 0.9849086570030695, 0.9850765819000046, 0.9852748730368773, 0.9854601705827678, 0.9855566374874469, 0.985709766251447, 0.9858434834001676, 0.9860072767424317, 0.9860975607177139, 0.9862171114598951, 0.9862958782224407, 0.9863917439843642, 0.986434654010716, 0.9865090893546888, 0.9865822734442785, 0.9866709622751825, 0.9866142959842895, 0.9867652151664393, 0.9869728032984255, 0.9869229259544146, 0.9869608123506312, 0.9871086715322445, 0.9870307569610142, 0.98709151957558, 0.987171209656173, 0.987386728751172, 0.9873771443686078, 0.9873312255263772, 0.987345745129213, 0.9874087698397583, 0.9874656734413374, 0.9875014278082156, 0.9875127198970894, 0.987519876664456, 0.987589780282797, 0.9875626934062149, 0.9876014769742036, 0.9876690786124163, 0.9877197458841543, 0.9877346432785119, 0.9877265332799862, 0.9877562813156157, 0.9877630653877684, 0.9878549773010623, 0.9878853719030615, 0.9878890769189175, 0.9879313310725981, 0.9878391991317493, 0.9878781105505933, 0.9880070402719717, 0.9879082004820104, 0.9879486048974956, 0.9879472096170191, 0.98799662629911, 0.9879674581346902, 0.9879916946240961, 0.9880435223029892, 0.9880072498853322, 0.9879970362195295, 0.9880814574464989, 0.9881088168647653, 0.9881172093760126, 0.988094823289538, 0.9881536920717657, 0.9881617761455948, 0.9881720179518803, 0.9881994522637151, 0.988171288071955, 0.9882482505199192, 0.9881941452345441, 0.9882239375858946, 0.9882241181724577, 0.9881686125102983, 0.9882573893079084, 0.9882884138578819, 0.9882309715987139, 0.9882374328308389, 0.9881958520545392, 0.9882583396585457, 0.9882685293938591, 0.9882960957665425, 0.9882914120379876, 0.9882752653834544, 0.9882823510240888, 0.9882808764184717, 0.9882821261218047, 0.9883354292926292, 0.9883538821372844, 0.9883158701059986, 0.9884153234471176, 0.988331673313694, 0.988300404362519, 0.9883076386823973, 0.9882866308148466, 0.9883151493107962, 0.9883520804816462, 0.9884278388714701, 0.988374484959145, 0.9883511733388369, 0.9883739124000294, 0.9883682238125003, 0.9883762591390362, 0.9883752666884639, 0.9884000028376243, 0.9883769358400962, 0.98839496635593, 0.9884000054965675, 0.9883977522637322, 0.9884419633996531, 0.9884225816974853, 0.9883964706530801, 0.9884352054737757, 0.988449495964334, 0.9884499991693462, 0.988432371704995, 0.9883895600595439, 0.9884385823317177], 'mDice': [0.004830558644088339, 0.03395015008221328, 0.09451023857611263, 0.15574460075025665, 0.2160750218705174, 0.27995640781732295, 0.34056197777113506, 0.3862440678045209, 0.43174283544370234, 0.45674715055408976, 0.4489196024862807, 0.4917818106240056, 0.5231648945010727, 0.5427927917707365, 0.5563366481805823, 0.5624107499104893, 0.5806921237463402, 0.588367896009112, 0.6054451922058616, 0.6106691664036322, 0.6140534753693081, 0.6222504443838693, 0.6251022678325611, 0.6258000769136564, 0.631536887480867, 0.636625844971398, 0.6408949731450985, 0.639344119671109, 0.6484058049531675, 0.6492362543109624, 0.6525682153311804, 0.6539531715740502, 0.6558438608194372, 0.6594730083827193, 0.6610613124078091, 0.663749630123266, 0.6730315714077435, 0.6705622214366955, 0.6730990644724396, 0.6730365943731429, 0.67401685058849, 0.6773951188782333, 0.6775704276606053, 0.6815713145475848, 0.6795572334948968, 0.6824057117713872, 0.6823969942043262, 0.684242502907395, 0.6846469795836835, 0.6844586036462323, 0.6883853658424434, 0.6847162080520147, 0.6898406706334933, 0.6905027136040442, 0.6920743402938417, 0.6946053137123364, 0.694220972105473, 0.6949433569128185, 0.6953393531111537, 0.6931494246185047, 0.6958931959251489, 0.6947885931202913, 0.6974280124260147, 0.6979883175800281, 0.7004710748293143, 0.6969699717808834, 0.6993553731521266, 0.6993265974034164, 0.7016005482815455, 0.6998645348619794, 0.7006474334511172, 0.7019710939612973, 0.7010165317794205, 0.7028971179267288, 0.704187568013996, 0.7024825086824069, 0.701062595977216, 0.705109216023555, 0.704947488680205, 0.705900766370908, 0.7052000324522253, 0.706303465765205, 0.7067286002148483, 0.7073337954216287, 0.7062970366619776, 0.7072085290593285, 0.7055219214645017, 0.7072844458778551, 0.7049366506058931, 0.7063727525087095, 0.709251195524705, 0.7094864681307711, 0.7093716077645029, 0.7110819541831885, 0.7075233255620339, 0.7091670570320356, 0.7092887575298437, 0.7080349179891848, 0.7086339767980753, 0.7090294640303545, 0.7009047983304275, 0.7054588646250587, 0.7075570185831488, 0.709618179106801, 0.7102399612448029, 0.7103851510689604, 0.7110201262186894, 0.710452026594084, 0.7136489365623786, 0.7104441421625782, 0.7124940143641929, 0.7103334390540992, 0.7131235298170919, 0.711321384490201, 0.7115227427181258, 0.7122765719669016, 0.7134996300293167, 0.7122795100992054, 0.7118335395054303, 0.712616761377753, 0.7108723053701749, 0.7138610579710467, 0.7123364019571183, 0.7120703026707731, 0.712812893231119, 0.7120192152859997, 0.7111101234712566, 0.7125416670590086], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 7.8125e-06, 7.8125e-06]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:04<00:41,  4.13s/it]predicting test subjects:  18%|█▊        | 2/11 [00:07<00:34,  3.88s/it]predicting test subjects:  27%|██▋       | 3/11 [00:10<00:29,  3.75s/it]predicting test subjects:  36%|███▋      | 4/11 [00:13<00:24,  3.54s/it]predicting test subjects:  45%|████▌     | 5/11 [00:17<00:20,  3.43s/it]predicting test subjects:  55%|█████▍    | 6/11 [00:20<00:17,  3.46s/it]predicting test subjects:  64%|██████▎   | 7/11 [00:23<00:13,  3.42s/it]predicting test subjects:  73%|███████▎  | 8/11 [00:27<00:10,  3.36s/it]predicting test subjects:  82%|████████▏ | 9/11 [00:30<00:06,  3.25s/it]predicting test subjects:  91%|█████████ | 10/11 [00:33<00:03,  3.32s/it]predicting test subjects: 100%|██████████| 11/11 [00:38<00:00,  3.69s/it]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:03<02:32,  3.82s/it]predicting train subjects:   5%|▍         | 2/41 [00:07<02:31,  3.88s/it]predicting train subjects:   7%|▋         | 3/41 [00:11<02:23,  3.78s/it]predicting train subjects:  10%|▉         | 4/41 [00:14<02:13,  3.60s/it]predicting train subjects:  12%|█▏        | 5/41 [00:19<02:22,  3.96s/it]predicting train subjects:  15%|█▍        | 6/41 [00:23<02:16,  3.90s/it]predicting train subjects:  17%|█▋        | 7/41 [00:26<02:08,  3.78s/it]predicting train subjects:  20%|█▉        | 8/41 [00:29<01:52,  3.40s/it]predicting train subjects:  22%|██▏       | 9/41 [00:33<01:57,  3.66s/it]predicting train subjects:  24%|██▍       | 10/41 [00:38<02:05,  4.05s/it]predicting train subjects:  27%|██▋       | 11/41 [00:43<02:07,  4.25s/it]predicting train subjects:  29%|██▉       | 12/41 [00:45<01:48,  3.74s/it]predicting train subjects:  32%|███▏      | 13/41 [00:49<01:45,  3.75s/it]predicting train subjects:  34%|███▍      | 14/41 [00:53<01:45,  3.90s/it]predicting train subjects:  37%|███▋      | 15/41 [00:57<01:37,  3.75s/it]predicting train subjects:  39%|███▉      | 16/41 [01:01<01:39,  4.00s/it]predicting train subjects:  41%|████▏     | 17/41 [01:07<01:51,  4.63s/it]predicting train subjects:  44%|████▍     | 18/41 [01:11<01:43,  4.51s/it]predicting train subjects:  46%|████▋     | 19/41 [01:16<01:36,  4.37s/it]predicting train subjects:  49%|████▉     | 20/41 [01:19<01:25,  4.07s/it]predicting train subjects:  51%|█████     | 21/41 [01:23<01:19,  3.98s/it]predicting train subjects:  54%|█████▎    | 22/41 [01:27<01:16,  4.03s/it]predicting train subjects:  56%|█████▌    | 23/41 [01:29<01:03,  3.52s/it]predicting train subjects:  59%|█████▊    | 24/41 [01:33<01:03,  3.72s/it]predicting train subjects:  61%|██████    | 25/41 [01:38<01:02,  3.88s/it]predicting train subjects:  63%|██████▎   | 26/41 [01:42<01:02,  4.16s/it]predicting train subjects:  66%|██████▌   | 27/41 [01:47<01:00,  4.32s/it]predicting train subjects:  68%|██████▊   | 28/41 [01:51<00:56,  4.31s/it]predicting train subjects:  71%|███████   | 29/41 [01:55<00:49,  4.14s/it]predicting train subjects:  73%|███████▎  | 30/41 [02:00<00:48,  4.40s/it]predicting train subjects:  76%|███████▌  | 31/41 [02:05<00:44,  4.48s/it]predicting train subjects:  78%|███████▊  | 32/41 [02:09<00:38,  4.29s/it]predicting train subjects:  80%|████████  | 33/41 [02:13<00:35,  4.40s/it]predicting train subjects:  83%|████████▎ | 34/41 [02:19<00:33,  4.84s/it]predicting train subjects:  85%|████████▌ | 35/41 [02:27<00:33,  5.59s/it]predicting train subjects:  88%|████████▊ | 36/41 [02:33<00:29,  5.86s/it]predicting train subjects:  90%|█████████ | 37/41 [02:38<00:22,  5.67s/it]predicting train subjects:  93%|█████████▎| 38/41 [02:43<00:15,  5.27s/it]predicting train subjects:  95%|█████████▌| 39/41 [02:47<00:10,  5.02s/it]predicting train subjects:  98%|█████████▊| 40/41 [02:52<00:04,  4.96s/it]predicting train subjects: 100%|██████████| 41/41 [02:57<00:00,  4.99s/it]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:31,  1.28it/s]Loading train:   5%|▍         | 2/41 [00:01<00:31,  1.25it/s]Loading train:   7%|▋         | 3/41 [00:02<00:29,  1.27it/s]Loading train:  10%|▉         | 4/41 [00:03<00:28,  1.30it/s]Loading train:  12%|█▏        | 5/41 [00:03<00:28,  1.28it/s]Loading train:  15%|█▍        | 6/41 [00:04<00:24,  1.42it/s]Loading train:  17%|█▋        | 7/41 [00:05<00:23,  1.44it/s]Loading train:  20%|█▉        | 8/41 [00:05<00:21,  1.55it/s]Loading train:  22%|██▏       | 9/41 [00:06<00:22,  1.45it/s]Loading train:  24%|██▍       | 10/41 [00:07<00:23,  1.32it/s]Loading train:  27%|██▋       | 11/41 [00:08<00:24,  1.22it/s]Loading train:  29%|██▉       | 12/41 [00:09<00:24,  1.20it/s]Loading train:  32%|███▏      | 13/41 [00:10<00:23,  1.20it/s]Loading train:  34%|███▍      | 14/41 [00:10<00:23,  1.15it/s]Loading train:  37%|███▋      | 15/41 [00:11<00:21,  1.21it/s]Loading train:  39%|███▉      | 16/41 [00:12<00:21,  1.15it/s]Loading train:  41%|████▏     | 17/41 [00:14<00:24,  1.03s/it]Loading train:  44%|████▍     | 18/41 [00:15<00:23,  1.01s/it]Loading train:  46%|████▋     | 19/41 [00:16<00:23,  1.07s/it]Loading train:  49%|████▉     | 20/41 [00:17<00:20,  1.01it/s]Loading train:  51%|█████     | 21/41 [00:18<00:20,  1.00s/it]Loading train:  54%|█████▎    | 22/41 [00:19<00:19,  1.05s/it]Loading train:  56%|█████▌    | 23/41 [00:19<00:16,  1.11it/s]Loading train:  59%|█████▊    | 24/41 [00:20<00:16,  1.01it/s]Loading train:  61%|██████    | 25/41 [00:22<00:16,  1.00s/it]Loading train:  63%|██████▎   | 26/41 [00:22<00:14,  1.01it/s]Loading train:  66%|██████▌   | 27/41 [00:24<00:15,  1.08s/it]Loading train:  68%|██████▊   | 28/41 [00:25<00:13,  1.07s/it]Loading train:  71%|███████   | 29/41 [00:26<00:12,  1.07s/it]Loading train:  73%|███████▎  | 30/41 [00:28<00:15,  1.39s/it]Loading train:  76%|███████▌  | 31/41 [00:29<00:12,  1.27s/it]Loading train:  78%|███████▊  | 32/41 [00:30<00:11,  1.24s/it]Loading train:  80%|████████  | 33/41 [00:31<00:09,  1.18s/it]Loading train:  83%|████████▎ | 34/41 [00:32<00:07,  1.09s/it]Loading train:  85%|████████▌ | 35/41 [00:33<00:06,  1.15s/it]Loading train:  88%|████████▊ | 36/41 [00:34<00:05,  1.13s/it]Loading train:  90%|█████████ | 37/41 [00:35<00:04,  1.06s/it]Loading train:  93%|█████████▎| 38/41 [00:37<00:03,  1.09s/it]Loading train:  95%|█████████▌| 39/41 [00:37<00:02,  1.05s/it]Loading train:  98%|█████████▊| 40/41 [00:38<00:01,  1.00s/it]Loading train: 100%|██████████| 41/41 [00:39<00:00,  1.11it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   5%|▍         | 2/41 [00:00<00:05,  6.96it/s]concatenating: train:  10%|▉         | 4/41 [00:00<00:04,  8.65it/s]concatenating: train:  17%|█▋        | 7/41 [00:00<00:03, 10.67it/s]concatenating: train:  22%|██▏       | 9/41 [00:00<00:02, 11.96it/s]concatenating: train:  27%|██▋       | 11/41 [00:00<00:02, 11.70it/s]concatenating: train:  32%|███▏      | 13/41 [00:00<00:02, 13.35it/s]concatenating: train:  37%|███▋      | 15/41 [00:01<00:01, 14.46it/s]concatenating: train:  41%|████▏     | 17/41 [00:01<00:01, 13.91it/s]concatenating: train:  46%|████▋     | 19/41 [00:01<00:01, 15.05it/s]concatenating: train:  54%|█████▎    | 22/41 [00:01<00:01, 16.52it/s]concatenating: train:  61%|██████    | 25/41 [00:01<00:00, 16.90it/s]concatenating: train:  66%|██████▌   | 27/41 [00:01<00:00, 15.57it/s]concatenating: train:  73%|███████▎  | 30/41 [00:01<00:00, 17.81it/s]concatenating: train:  78%|███████▊  | 32/41 [00:01<00:00, 17.82it/s]concatenating: train:  83%|████████▎ | 34/41 [00:02<00:00, 16.93it/s]concatenating: train:  88%|████████▊ | 36/41 [00:02<00:00, 16.85it/s]concatenating: train:  95%|█████████▌| 39/41 [00:02<00:00, 18.68it/s]concatenating: train: 100%|██████████| 41/41 [00:02<00:00, 16.98it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:01<00:11,  1.13s/it]Loading test:  18%|█▊        | 2/11 [00:02<00:09,  1.06s/it]Loading test:  27%|██▋       | 3/11 [00:02<00:08,  1.00s/it]Loading test:  36%|███▋      | 4/11 [00:03<00:06,  1.04it/s]Loading test:  45%|████▌     | 5/11 [00:04<00:05,  1.10it/s]Loading test:  55%|█████▍    | 6/11 [00:05<00:04,  1.11it/s]Loading test:  64%|██████▎   | 7/11 [00:06<00:03,  1.08it/s]Loading test:  73%|███████▎  | 8/11 [00:07<00:02,  1.08it/s]Loading test:  82%|████████▏ | 9/11 [00:08<00:01,  1.08it/s]Loading test:  91%|█████████ | 10/11 [00:09<00:00,  1.15it/s]Loading test: 100%|██████████| 11/11 [00:09<00:00,  1.15it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  18%|█▊        | 2/11 [00:00<00:00, 11.52it/s]concatenating: validation:  36%|███▋      | 4/11 [00:00<00:00, 12.70it/s]concatenating: validation:  55%|█████▍    | 6/11 [00:00<00:00, 14.02it/s]concatenating: validation:  73%|███████▎  | 8/11 [00:00<00:00, 14.72it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 19.02it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 92, 116, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 92, 116, 20)  200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 92, 116, 20)  80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 92, 116, 20)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 92, 116, 20)  3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 92, 116, 20)  80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 92, 116, 20)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 46, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 46, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 46, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 46, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 46, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 46, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 46, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 46, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 46, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 23, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 23, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 23, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 23, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 23, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 23, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 23, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 23, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 23, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 23, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 46, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 46, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 46, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 46, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 46, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 46, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 46, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 46, 58, 140)  0           concatenate_4[0][0]              2019-07-28 21:15:04.684010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-28 21:15:04.684103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 21:15:04.684122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-28 21:15:04.684135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-28 21:15:04.684630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 92, 116, 20)  11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 92, 116, 40)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 92, 116, 20)  7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 92, 116, 20)  80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 92, 116, 20)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 92, 116, 20)  3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 92, 116, 20)  80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 92, 116, 20)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 92, 116, 60)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 92, 116, 60)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 92, 116, 2)   122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.98000917 0.01999083]
Train on 4060 samples, validate on 1096 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.9643 - acc: 0.8753 - mDice: 0.2305 - val_loss: 0.5253 - val_acc: 0.9880 - val_mDice: 0.3582

Epoch 00001: val_mDice improved from -inf to 0.35824, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.1302 - acc: 0.9863 - mDice: 0.7805 - val_loss: 0.3977 - val_acc: 0.9817 - val_mDice: 0.4633

Epoch 00002: val_mDice improved from 0.35824 to 0.46328, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0927 - acc: 0.9896 - mDice: 0.8397 - val_loss: 0.2874 - val_acc: 0.9833 - val_mDice: 0.5718

Epoch 00003: val_mDice improved from 0.46328 to 0.57177, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0818 - acc: 0.9911 - mDice: 0.8580 - val_loss: 0.2113 - val_acc: 0.9853 - val_mDice: 0.6634

Epoch 00004: val_mDice improved from 0.57177 to 0.66338, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0761 - acc: 0.9920 - mDice: 0.8680 - val_loss: 0.1684 - val_acc: 0.9867 - val_mDice: 0.7223

Epoch 00005: val_mDice improved from 0.66338 to 0.72226, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0728 - acc: 0.9924 - mDice: 0.8736 - val_loss: 0.1045 - val_acc: 0.9898 - val_mDice: 0.8201

Epoch 00006: val_mDice improved from 0.72226 to 0.82006, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0689 - acc: 0.9930 - mDice: 0.8804 - val_loss: 0.1120 - val_acc: 0.9897 - val_mDice: 0.8078

Epoch 00007: val_mDice did not improve from 0.82006
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0659 - acc: 0.9935 - mDice: 0.8856 - val_loss: 0.1083 - val_acc: 0.9903 - val_mDice: 0.8139

Epoch 00008: val_mDice did not improve from 0.82006
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0635 - acc: 0.9939 - mDice: 0.8899 - val_loss: 0.1067 - val_acc: 0.9906 - val_mDice: 0.8162

Epoch 00009: val_mDice did not improve from 0.82006
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0630 - acc: 0.9942 - mDice: 0.8908 - val_loss: 0.0785 - val_acc: 0.9926 - val_mDice: 0.8636

Epoch 00010: val_mDice improved from 0.82006 to 0.86362, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0608 - acc: 0.9945 - mDice: 0.8947 - val_loss: 0.0809 - val_acc: 0.9924 - val_mDice: 0.8595

Epoch 00011: val_mDice did not improve from 0.86362
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0609 - acc: 0.9946 - mDice: 0.8946 - val_loss: 0.0693 - val_acc: 0.9935 - val_mDice: 0.8797

Epoch 00012: val_mDice improved from 0.86362 to 0.87971, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0607 - acc: 0.9946 - mDice: 0.8949 - val_loss: 0.0520 - val_acc: 0.9957 - val_mDice: 0.9105

Epoch 00013: val_mDice improved from 0.87971 to 0.91054, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.0574 - acc: 0.9950 - mDice: 0.9007 - val_loss: 0.0665 - val_acc: 0.9939 - val_mDice: 0.8845

Epoch 00014: val_mDice did not improve from 0.91054
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0566 - acc: 0.9952 - mDice: 0.9023 - val_loss: 0.0576 - val_acc: 0.9949 - val_mDice: 0.9005

Epoch 00015: val_mDice did not improve from 0.91054
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0561 - acc: 0.9954 - mDice: 0.9031 - val_loss: 0.0605 - val_acc: 0.9947 - val_mDice: 0.8953

Epoch 00016: val_mDice did not improve from 0.91054
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0554 - acc: 0.9955 - mDice: 0.9043 - val_loss: 0.0514 - val_acc: 0.9958 - val_mDice: 0.9117

Epoch 00017: val_mDice improved from 0.91054 to 0.91168, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0538 - acc: 0.9957 - mDice: 0.9072 - val_loss: 0.0622 - val_acc: 0.9949 - val_mDice: 0.8923

Epoch 00018: val_mDice did not improve from 0.91168
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0528 - acc: 0.9959 - mDice: 0.9088 - val_loss: 0.0564 - val_acc: 0.9955 - val_mDice: 0.9026

Epoch 00019: val_mDice did not improve from 0.91168
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0528 - acc: 0.9959 - mDice: 0.9088 - val_loss: 0.0516 - val_acc: 0.9960 - val_mDice: 0.9114

Epoch 00020: val_mDice did not improve from 0.91168
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0511 - acc: 0.9961 - mDice: 0.9115 - val_loss: 0.0524 - val_acc: 0.9959 - val_mDice: 0.9096

Epoch 00021: val_mDice did not improve from 0.91168
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0512 - acc: 0.9962 - mDice: 0.9109 - val_loss: 0.0566 - val_acc: 0.9957 - val_mDice: 0.9008

Epoch 00022: val_mDice did not improve from 0.91168
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0500 - acc: 0.9963 - mDice: 0.9122 - val_loss: 0.0521 - val_acc: 0.9961 - val_mDice: 0.9085

Epoch 00023: val_mDice did not improve from 0.91168
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0486 - acc: 0.9964 - mDice: 0.9138 - val_loss: 0.0569 - val_acc: 0.9959 - val_mDice: 0.8983

Epoch 00024: val_mDice did not improve from 0.91168
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0481 - acc: 0.9965 - mDice: 0.9129 - val_loss: 0.0515 - val_acc: 0.9962 - val_mDice: 0.9053

Epoch 00025: val_mDice did not improve from 0.91168
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0470 - acc: 0.9965 - mDice: 0.9137 - val_loss: 0.0489 - val_acc: 0.9964 - val_mDice: 0.9089

Epoch 00026: val_mDice did not improve from 0.91168
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0461 - acc: 0.9966 - mDice: 0.9145 - val_loss: 0.0469 - val_acc: 0.9965 - val_mDice: 0.9124

Epoch 00027: val_mDice improved from 0.91168 to 0.91243, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0454 - acc: 0.9966 - mDice: 0.9155 - val_loss: 0.0479 - val_acc: 0.9965 - val_mDice: 0.9105

Epoch 00028: val_mDice did not improve from 0.91243
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0455 - acc: 0.9966 - mDice: 0.9152 - val_loss: 0.0477 - val_acc: 0.9965 - val_mDice: 0.9108

Epoch 00029: val_mDice did not improve from 0.91243
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0450 - acc: 0.9967 - mDice: 0.9161 - val_loss: 0.0462 - val_acc: 0.9966 - val_mDice: 0.9136

Epoch 00030: val_mDice improved from 0.91243 to 0.91361, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0448 - acc: 0.9967 - mDice: 0.9163 - val_loss: 0.0519 - val_acc: 0.9962 - val_mDice: 0.9034

Epoch 00031: val_mDice did not improve from 0.91361
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0453 - acc: 0.9966 - mDice: 0.9155 - val_loss: 0.0445 - val_acc: 0.9967 - val_mDice: 0.9165

Epoch 00032: val_mDice improved from 0.91361 to 0.91651, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0442 - acc: 0.9967 - mDice: 0.9174 - val_loss: 0.0474 - val_acc: 0.9965 - val_mDice: 0.9113

Epoch 00033: val_mDice did not improve from 0.91651
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0442 - acc: 0.9967 - mDice: 0.9174 - val_loss: 0.0455 - val_acc: 0.9966 - val_mDice: 0.9147

Epoch 00034: val_mDice did not improve from 0.91651
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0434 - acc: 0.9968 - mDice: 0.9188 - val_loss: 0.0462 - val_acc: 0.9966 - val_mDice: 0.9135

Epoch 00035: val_mDice did not improve from 0.91651
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0434 - acc: 0.9968 - mDice: 0.9187 - val_loss: 0.0485 - val_acc: 0.9965 - val_mDice: 0.9093

Epoch 00036: val_mDice did not improve from 0.91651
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0431 - acc: 0.9968 - mDice: 0.9193 - val_loss: 0.0455 - val_acc: 0.9966 - val_mDice: 0.9148

Epoch 00037: val_mDice did not improve from 0.91651
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0430 - acc: 0.9968 - mDice: 0.9193 - val_loss: 0.0454 - val_acc: 0.9966 - val_mDice: 0.9150

Epoch 00038: val_mDice did not improve from 0.91651
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0421 - acc: 0.9969 - mDice: 0.9210 - val_loss: 0.0451 - val_acc: 0.9966 - val_mDice: 0.9154

Epoch 00039: val_mDice did not improve from 0.91651
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0422 - acc: 0.9969 - mDice: 0.9209 - val_loss: 0.0455 - val_acc: 0.9966 - val_mDice: 0.9147

Epoch 00040: val_mDice did not improve from 0.91651
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0421 - acc: 0.9969 - mDice: 0.9210 - val_loss: 0.0457 - val_acc: 0.9966 - val_mDice: 0.9144

Epoch 00041: val_mDice did not improve from 0.91651
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0416 - acc: 0.9969 - mDice: 0.9220 - val_loss: 0.0454 - val_acc: 0.9966 - val_mDice: 0.9148

Epoch 00042: val_mDice did not improve from 0.91651
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0419 - acc: 0.9969 - mDice: 0.9214 - val_loss: 0.0467 - val_acc: 0.9966 - val_mDice: 0.9126

Epoch 00043: val_mDice did not improve from 0.91651
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0419 - acc: 0.9969 - mDice: 0.9214 - val_loss: 0.0464 - val_acc: 0.9966 - val_mDice: 0.9131

Epoch 00044: val_mDice did not improve from 0.91651
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0413 - acc: 0.9969 - mDice: 0.9225 - val_loss: 0.0453 - val_acc: 0.9966 - val_mDice: 0.9150

Epoch 00045: val_mDice did not improve from 0.91651
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0413 - acc: 0.9969 - mDice: 0.9225 - val_loss: 0.0447 - val_acc: 0.9967 - val_mDice: 0.9161

Epoch 00046: val_mDice did not improve from 0.91651
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0409 - acc: 0.9969 - mDice: 0.9232 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9137

Epoch 00047: val_mDice did not improve from 0.91651
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0415 - acc: 0.9969 - mDice: 0.9220 - val_loss: 0.0460 - val_acc: 0.9966 - val_mDice: 0.9138

Epoch 00048: val_mDice did not improve from 0.91651
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0412 - acc: 0.9969 - mDice: 0.9225 - val_loss: 0.0477 - val_acc: 0.9965 - val_mDice: 0.9107

Epoch 00049: val_mDice did not improve from 0.91651
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0407 - acc: 0.9970 - mDice: 0.9236 - val_loss: 0.0461 - val_acc: 0.9966 - val_mDice: 0.9136

Epoch 00050: val_mDice did not improve from 0.91651
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0403 - acc: 0.9970 - mDice: 0.9242 - val_loss: 0.0479 - val_acc: 0.9965 - val_mDice: 0.9105

Epoch 00051: val_mDice did not improve from 0.91651
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0406 - acc: 0.9970 - mDice: 0.9236 - val_loss: 0.0475 - val_acc: 0.9965 - val_mDice: 0.9112

Epoch 00052: val_mDice did not improve from 0.91651
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0406 - acc: 0.9970 - mDice: 0.9237 - val_loss: 0.0454 - val_acc: 0.9966 - val_mDice: 0.9150

Epoch 00053: val_mDice did not improve from 0.91651
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0405 - acc: 0.9970 - mDice: 0.9238 - val_loss: 0.0448 - val_acc: 0.9967 - val_mDice: 0.9161

Epoch 00054: val_mDice did not improve from 0.91651
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0401 - acc: 0.9970 - mDice: 0.9246 - val_loss: 0.0464 - val_acc: 0.9966 - val_mDice: 0.9130

Epoch 00055: val_mDice did not improve from 0.91651
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0398 - acc: 0.9970 - mDice: 0.9251 - val_loss: 0.0466 - val_acc: 0.9966 - val_mDice: 0.9127

Epoch 00056: val_mDice did not improve from 0.91651
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0398 - acc: 0.9970 - mDice: 0.9250 - val_loss: 0.0464 - val_acc: 0.9966 - val_mDice: 0.9131

Epoch 00057: val_mDice did not improve from 0.91651
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0397 - acc: 0.9970 - mDice: 0.9253 - val_loss: 0.0468 - val_acc: 0.9965 - val_mDice: 0.9124

Epoch 00058: val_mDice did not improve from 0.91651
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0397 - acc: 0.9970 - mDice: 0.9254 - val_loss: 0.0463 - val_acc: 0.9966 - val_mDice: 0.9133

Epoch 00059: val_mDice did not improve from 0.91651
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0396 - acc: 0.9970 - mDice: 0.9254 - val_loss: 0.0466 - val_acc: 0.9966 - val_mDice: 0.9127

Epoch 00060: val_mDice did not improve from 0.91651
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0394 - acc: 0.9971 - mDice: 0.9258 - val_loss: 0.0465 - val_acc: 0.9966 - val_mDice: 0.9129

Epoch 00061: val_mDice did not improve from 0.91651
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0394 - acc: 0.9971 - mDice: 0.9259 - val_loss: 0.0466 - val_acc: 0.9966 - val_mDice: 0.9127

Epoch 00062: val_mDice did not improve from 0.91651
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0393 - acc: 0.9971 - mDice: 0.9260 - val_loss: 0.0469 - val_acc: 0.9966 - val_mDice: 0.9122

Epoch 00063: val_mDice did not improve from 0.91651
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0392 - acc: 0.9971 - mDice: 0.9261 - val_loss: 0.0459 - val_acc: 0.9966 - val_mDice: 0.9140

Epoch 00064: val_mDice did not improve from 0.91651
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0390 - acc: 0.9971 - mDice: 0.9265 - val_loss: 0.0456 - val_acc: 0.9966 - val_mDice: 0.9144

Epoch 00065: val_mDice did not improve from 0.91651
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0392 - acc: 0.9971 - mDice: 0.9262 - val_loss: 0.0459 - val_acc: 0.9966 - val_mDice: 0.9140

Epoch 00066: val_mDice did not improve from 0.91651
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0390 - acc: 0.9971 - mDice: 0.9265 - val_loss: 0.0459 - val_acc: 0.9966 - val_mDice: 0.9139

Epoch 00067: val_mDice did not improve from 0.91651
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0389 - acc: 0.9971 - mDice: 0.9267 - val_loss: 0.0468 - val_acc: 0.9965 - val_mDice: 0.9123

Epoch 00068: val_mDice did not improve from 0.91651
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0389 - acc: 0.9971 - mDice: 0.9266 - val_loss: 0.0460 - val_acc: 0.9966 - val_mDice: 0.9137

Epoch 00069: val_mDice did not improve from 0.91651
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0391 - acc: 0.9971 - mDice: 0.9264 - val_loss: 0.0464 - val_acc: 0.9966 - val_mDice: 0.9131

Epoch 00070: val_mDice did not improve from 0.91651
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0386 - acc: 0.9971 - mDice: 0.9273 - val_loss: 0.0458 - val_acc: 0.9966 - val_mDice: 0.9141

Epoch 00071: val_mDice did not improve from 0.91651
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0388 - acc: 0.9971 - mDice: 0.9269 - val_loss: 0.0476 - val_acc: 0.9965 - val_mDice: 0.9110

Epoch 00072: val_mDice did not improve from 0.91651
Restoring model weights from the end of the best epoch
Epoch 00072: early stopping
{'val_loss': [0.5252748052351666, 0.39773478005489293, 0.2873968737077539, 0.21126196952196805, 0.16835859521244564, 0.10445509502922531, 0.11201393645066414, 0.10825825830663208, 0.10673650253536927, 0.07849652475140391, 0.08092240458966171, 0.06926819638614237, 0.05203289148418137, 0.06653706692703014, 0.057576363511981754, 0.060462090859774255, 0.05140482734915984, 0.06215440536284969, 0.05644308443922196, 0.05156122910769752, 0.05237505155323196, 0.05662214959951213, 0.052082640669532936, 0.05685461491998965, 0.05154025874161807, 0.04893234445557107, 0.04686149635291013, 0.047938406970487894, 0.04772556934804812, 0.04615291420125613, 0.051882892180859605, 0.04453372638536631, 0.047430423598219876, 0.04550612025833043, 0.04620210973233202, 0.04852781786046324, 0.045482124004812136, 0.04537452634995001, 0.045137176779608656, 0.04553405399413875, 0.045709054468430745, 0.04543370760324662, 0.046681351103160504, 0.04641338708355044, 0.045336752966807704, 0.04472665495517915, 0.0460918337840886, 0.046010671985627964, 0.04771811638815995, 0.04614147674428285, 0.04787907167507784, 0.04746036948024356, 0.04535488894440397, 0.04475143806070742, 0.04643878314888825, 0.04660635952749392, 0.04638679000637392, 0.04676368396158201, 0.046292920005473776, 0.046620422294431356, 0.04647679974998001, 0.046627295060749474, 0.046898072476695925, 0.04589267612101823, 0.045646892221522156, 0.04588894184379682, 0.04591704986608811, 0.04683868434742419, 0.04604010437581226, 0.04637224828131008, 0.04582149403536842, 0.04756116169593195], 'val_acc': [0.988005146710542, 0.9817168588620903, 0.9832745942756207, 0.9852823702088238, 0.9866808098163048, 0.9897674711516303, 0.9897454082965851, 0.9903042031465655, 0.9905907927203352, 0.992644813156476, 0.9924000399391146, 0.9934694993234899, 0.9957283021759813, 0.9938532055294427, 0.9948832610227766, 0.9946871334618895, 0.9957632644333109, 0.9948551293707242, 0.9954549809441949, 0.9959672593722378, 0.9959199663496365, 0.9956792294979095, 0.996116449145505, 0.9958548286970514, 0.9962085248345006, 0.9963910614487028, 0.9965278523681808, 0.9964744094079429, 0.9964662172498494, 0.996568894951883, 0.9962357176046301, 0.9966763575581739, 0.996496487272917, 0.9966212169532358, 0.9965808611281597, 0.9964516754568058, 0.9966280506040058, 0.9966354082970723, 0.9966491728368467, 0.9966126695601609, 0.9966165965055898, 0.9966378890684922, 0.9965597454213748, 0.9965718867134874, 0.9966349012225214, 0.9966798563943292, 0.9965871868342379, 0.996586498117795, 0.9965092152574636, 0.9965962519610885, 0.9964881028137068, 0.996499212118831, 0.996624891992903, 0.9966678062494654, 0.996564874248783, 0.9965695758370587, 0.9965868461741149, 0.9965488919376457, 0.9965712077861285, 0.9965776111522731, 0.9965727483704142, 0.9965651252844038, 0.9965587997958608, 0.9966041182514525, 0.9966274541224877, 0.9966154820727606, 0.99661326691182, 0.9965459726152629, 0.9966030851767881, 0.9965860802326759, 0.9966144635729546, 0.9965122949032887], 'val_mDice': [0.35824086544287465, 0.46327589561034294, 0.5717748069632662, 0.663381046622339, 0.7222623283410594, 0.8200563196718258, 0.8077789688632436, 0.8139054666470437, 0.8162216592009051, 0.8636219077301721, 0.8594613525554211, 0.8797083466592497, 0.9105416020772753, 0.8845263291014372, 0.9004992636015815, 0.8953310662377489, 0.9116756385260255, 0.8923321066111544, 0.9025507349602497, 0.9113642133935524, 0.9095697479091421, 0.9008495696704754, 0.9084885446694646, 0.8983323482701379, 0.9053248073497828, 0.9089323045128453, 0.9124302126630379, 0.9104630810065861, 0.9108193747318574, 0.9136126609179225, 0.9033963412699038, 0.9165050924694451, 0.9113050890664984, 0.9147326915803617, 0.9134708949684227, 0.9093146452503483, 0.9147698459399007, 0.9149610691697058, 0.9153801650896559, 0.9146799360313554, 0.9143545501423578, 0.9148393103676121, 0.9126027489665651, 0.9130839936054536, 0.9150215056690857, 0.9161104342363177, 0.9136574250938249, 0.913804635514308, 0.9107389595821827, 0.9135578259499404, 0.9104572147348501, 0.9112163279613439, 0.9149720631811741, 0.9160642397664759, 0.9130296378675169, 0.9127185351222101, 0.9131062557227421, 0.9124467670917511, 0.9132929659672897, 0.9126894006328862, 0.9129468441879662, 0.9126762571561076, 0.9121836613129525, 0.9139953766861101, 0.9144356916420651, 0.9139971389387646, 0.9139499427193273, 0.9122981252896525, 0.9137313694414431, 0.9131267756006144, 0.9141162055252242, 0.9109983006968115], 'loss': [0.964282279751571, 0.1301845981569713, 0.09269666932339739, 0.0818340425274055, 0.07605425018835538, 0.07282001587557675, 0.06891887743264584, 0.06594516283699445, 0.06352432389638107, 0.06301064565410755, 0.06078315350075661, 0.06086581301292762, 0.06065868874415388, 0.057417068648808106, 0.05655325015116795, 0.056073645414243194, 0.05540198629052181, 0.053755829395184966, 0.052799874852443564, 0.052772621105750796, 0.0511254668052267, 0.0511570525874058, 0.05002614519939634, 0.048563728209933626, 0.04810702435562176, 0.046955088456276016, 0.046144564071752756, 0.04544733035300166, 0.04554453431605705, 0.04497465337144917, 0.04481946834789708, 0.04528210268085226, 0.044170379877237265, 0.0441649344203801, 0.04336238423077931, 0.04339167209608214, 0.04306868559180809, 0.043045379056278706, 0.042124329400914054, 0.04219553988481977, 0.04209129343479138, 0.04157284116877124, 0.04188396636708617, 0.041880660474740816, 0.04127250081432864, 0.04125301807825201, 0.04086788580408825, 0.0415288173946841, 0.04124457987364877, 0.040664924538047445, 0.040325609150484866, 0.04064515539504624, 0.04056653663002212, 0.04051610783403143, 0.040077664251691604, 0.0397836033970558, 0.03983264511882378, 0.0396898478966922, 0.039651987642930646, 0.03962060921077658, 0.039423297356648984, 0.039379001599667694, 0.039287633918629494, 0.03924055694799705, 0.039040572464172474, 0.03920981054896204, 0.03903316781658844, 0.03892290408681766, 0.038931284576948055, 0.03906584811841913, 0.03858896455007234, 0.03877586939284954], 'acc': [0.8752702640488818, 0.9862667648075837, 0.9896427971388906, 0.9910606953310849, 0.991996135030474, 0.9924277660294707, 0.993016950015364, 0.9934987123376631, 0.9939453487325771, 0.9941548605270574, 0.9944555216234893, 0.9946005523498422, 0.9945716558418838, 0.9949764500697845, 0.9952207709768136, 0.995371270062301, 0.9954545186658211, 0.9957077717546172, 0.9958567496003776, 0.9959291078774213, 0.9961058914367789, 0.9961745988559253, 0.996312229504139, 0.9964496011804478, 0.9964786483149223, 0.9965490484472566, 0.9965936139299365, 0.9966328053051615, 0.9966218157354834, 0.9966639350787759, 0.9966712306285727, 0.9966346034275487, 0.9967163020166857, 0.9967122433220812, 0.9967712836312543, 0.9967682834329277, 0.9967896794450695, 0.9967904874843917, 0.9968510666504282, 0.9968539770013594, 0.9968620935097117, 0.996897782304604, 0.9968749722236483, 0.9968725759994808, 0.9969187320159574, 0.9969196572092366, 0.9969462928513588, 0.9969028557462646, 0.9969188764764758, 0.9969678750179084, 0.9969918284510156, 0.9969653352140793, 0.9969695366074887, 0.9969668573346632, 0.9970009908300316, 0.9970210558675193, 0.997023860809251, 0.9970319459003768, 0.9970378758284846, 0.9970382877758571, 0.9970522070753163, 0.9970562264249829, 0.9970643147459171, 0.997069907893101, 0.9970736436068718, 0.9970632174919392, 0.9970805495243354, 0.9970899881987736, 0.997087650698394, 0.997087904385158, 0.9971085205453957, 0.9970951538367812], 'mDice': [0.23050066700269436, 0.7804839754926747, 0.8396781221986405, 0.8580478441539069, 0.8680030879715981, 0.87361474371896, 0.8804232513376058, 0.8856302326535944, 0.8899041264515205, 0.8907988905319439, 0.8947412897213339, 0.894585059194142, 0.8949395230250993, 0.9007301345247353, 0.902256243921853, 0.9030932740037665, 0.9042950448731484, 0.9072061257409345, 0.9088460477114898, 0.9087699040990745, 0.9114694269419891, 0.9108864397838198, 0.9122035785261633, 0.9137723727766516, 0.9129421232369146, 0.9136748962801665, 0.914532823515643, 0.9154924553603374, 0.9151950443319499, 0.9161309502982153, 0.9163443683990704, 0.9154637872879141, 0.9174289477282557, 0.9174018963217148, 0.9188135565795335, 0.9187449736548174, 0.9193071015362669, 0.9193435528595459, 0.9209943795439057, 0.9208508997127928, 0.9210351177037056, 0.9219656707030799, 0.9213906021540975, 0.9213896317435015, 0.9224888001169477, 0.9225112035356718, 0.9232065651217117, 0.9219986041778414, 0.9225133563497384, 0.9235579709114112, 0.9241621553017001, 0.9235815972530196, 0.9237203025465528, 0.9238139957629988, 0.9245990109561112, 0.925131613985071, 0.9250351484185957, 0.9252929710989515, 0.9253601220440982, 0.925415868242386, 0.9257660660837671, 0.9258538188018235, 0.9260159281674277, 0.9260951894257456, 0.9264605526853664, 0.9261521623639638, 0.9264688377309902, 0.9266655879654908, 0.9266484884792948, 0.9263997421476055, 0.9272693601147882, 0.9269312420502085], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:01<00:14,  1.47s/it]predicting test subjects:  18%|█▊        | 2/11 [00:02<00:10,  1.22s/it]predicting test subjects:  27%|██▋       | 3/11 [00:02<00:07,  1.01it/s]predicting test subjects:  36%|███▋      | 4/11 [00:03<00:05,  1.19it/s]predicting test subjects:  45%|████▌     | 5/11 [00:03<00:04,  1.38it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:03<00:03,  1.56it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:04<00:02,  1.50it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:05<00:01,  1.54it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:05<00:01,  1.74it/s]predicting test subjects:  91%|█████████ | 10/11 [00:06<00:00,  1.74it/s]predicting test subjects: 100%|██████████| 11/11 [00:06<00:00,  1.67it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:33,  1.20it/s]predicting train subjects:   5%|▍         | 2/41 [00:01<00:27,  1.42it/s]predicting train subjects:   7%|▋         | 3/41 [00:01<00:26,  1.45it/s]predicting train subjects:  10%|▉         | 4/41 [00:02<00:24,  1.53it/s]predicting train subjects:  12%|█▏        | 5/41 [00:03<00:22,  1.58it/s]predicting train subjects:  15%|█▍        | 6/41 [00:03<00:20,  1.72it/s]predicting train subjects:  17%|█▋        | 7/41 [00:03<00:18,  1.85it/s]predicting train subjects:  20%|█▉        | 8/41 [00:04<00:16,  1.99it/s]predicting train subjects:  22%|██▏       | 9/41 [00:04<00:16,  1.97it/s]predicting train subjects:  24%|██▍       | 10/41 [00:05<00:16,  1.84it/s]predicting train subjects:  27%|██▋       | 11/41 [00:06<00:16,  1.84it/s]predicting train subjects:  29%|██▉       | 12/41 [00:06<00:14,  2.03it/s]predicting train subjects:  32%|███▏      | 13/41 [00:07<00:15,  1.83it/s]predicting train subjects:  34%|███▍      | 14/41 [00:07<00:15,  1.73it/s]predicting train subjects:  37%|███▋      | 15/41 [00:08<00:13,  1.92it/s]predicting train subjects:  39%|███▉      | 16/41 [00:08<00:13,  1.87it/s]predicting train subjects:  41%|████▏     | 17/41 [00:09<00:12,  1.86it/s]predicting train subjects:  44%|████▍     | 18/41 [00:09<00:11,  1.98it/s]predicting train subjects:  46%|████▋     | 19/41 [00:10<00:10,  2.01it/s]predicting train subjects:  49%|████▉     | 20/41 [00:10<00:09,  2.31it/s]predicting train subjects:  51%|█████     | 21/41 [00:10<00:08,  2.38it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:11<00:09,  2.02it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:11<00:08,  2.21it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:12<00:07,  2.14it/s]predicting train subjects:  61%|██████    | 25/41 [00:12<00:07,  2.09it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:13<00:07,  1.88it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:14<00:07,  1.84it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:14<00:06,  1.97it/s]predicting train subjects:  71%|███████   | 29/41 [00:15<00:06,  1.87it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:15<00:06,  1.74it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:16<00:05,  1.95it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:16<00:05,  1.64it/s]predicting train subjects:  80%|████████  | 33/41 [00:17<00:04,  1.61it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:18<00:04,  1.51it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:19<00:04,  1.44it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:19<00:03,  1.57it/s]predicting train subjects:  90%|█████████ | 37/41 [00:20<00:02,  1.64it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:20<00:01,  1.82it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:21<00:01,  1.97it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:21<00:00,  1.75it/s]predicting train subjects: 100%|██████████| 41/41 [00:22<00:00,  1.94it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:03<02:30,  3.75s/it]Loading train:   5%|▍         | 2/41 [00:07<02:29,  3.83s/it]Loading train:   7%|▋         | 3/41 [00:11<02:25,  3.82s/it]Loading train:  10%|▉         | 4/41 [00:15<02:18,  3.74s/it]Loading train:  12%|█▏        | 5/41 [00:20<02:29,  4.14s/it]Loading train:  15%|█▍        | 6/41 [00:23<02:19,  3.98s/it]Loading train:  17%|█▋        | 7/41 [00:27<02:12,  3.90s/it]Loading train:  20%|█▉        | 8/41 [00:29<01:53,  3.45s/it]Loading train:  22%|██▏       | 9/41 [00:33<01:55,  3.62s/it]Loading train:  24%|██▍       | 10/41 [00:38<01:59,  3.86s/it]Loading train:  27%|██▋       | 11/41 [00:42<02:01,  4.05s/it]Loading train:  29%|██▉       | 12/41 [00:44<01:34,  3.26s/it]Loading train:  32%|███▏      | 13/41 [00:47<01:34,  3.36s/it]Loading train:  34%|███▍      | 14/41 [00:51<01:35,  3.53s/it]Loading train:  37%|███▋      | 15/41 [00:54<01:22,  3.17s/it]Loading train:  39%|███▉      | 16/41 [00:57<01:23,  3.35s/it]Loading train:  41%|████▏     | 17/41 [01:02<01:32,  3.85s/it]Loading train:  44%|████▍     | 18/41 [01:06<01:29,  3.89s/it]Loading train:  46%|████▋     | 19/41 [01:10<01:21,  3.69s/it]Loading train:  49%|████▉     | 20/41 [01:13<01:15,  3.58s/it]Loading train:  51%|█████     | 21/41 [01:18<01:19,  3.95s/it]Loading train:  54%|█████▎    | 22/41 [01:22<01:15,  3.99s/it]Loading train:  56%|█████▌    | 23/41 [01:24<01:01,  3.43s/it]Loading train:  59%|█████▊    | 24/41 [01:28<01:02,  3.67s/it]Loading train:  61%|██████    | 25/41 [01:32<01:00,  3.77s/it]Loading train:  63%|██████▎   | 26/41 [01:35<00:52,  3.49s/it]Loading train:  66%|██████▌   | 27/41 [01:39<00:50,  3.62s/it]Loading train:  68%|██████▊   | 28/41 [01:44<00:51,  3.95s/it]Loading train:  71%|███████   | 29/41 [01:47<00:45,  3.79s/it]Loading train:  73%|███████▎  | 30/41 [01:52<00:44,  4.07s/it]Loading train:  76%|███████▌  | 31/41 [01:55<00:38,  3.87s/it]Loading train:  78%|███████▊  | 32/41 [02:01<00:39,  4.33s/it]Loading train:  80%|████████  | 33/41 [02:04<00:32,  4.08s/it]Loading train:  83%|████████▎ | 34/41 [02:08<00:29,  4.16s/it]Loading train:  85%|████████▌ | 35/41 [02:14<00:27,  4.54s/it]Loading train:  88%|████████▊ | 36/41 [02:19<00:23,  4.68s/it]Loading train:  90%|█████████ | 37/41 [02:23<00:18,  4.62s/it]Loading train:  93%|█████████▎| 38/41 [02:28<00:14,  4.72s/it]Loading train:  95%|█████████▌| 39/41 [02:32<00:08,  4.35s/it]Loading train:  98%|█████████▊| 40/41 [02:36<00:04,  4.32s/it]Loading train: 100%|██████████| 41/41 [02:39<00:00,  4.04s/it]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 3/41 [00:00<00:01, 22.53it/s]concatenating: train:  17%|█▋        | 7/41 [00:00<00:01, 25.10it/s]concatenating: train:  34%|███▍      | 14/41 [00:00<00:00, 30.96it/s]concatenating: train:  44%|████▍     | 18/41 [00:00<00:00, 32.63it/s]concatenating: train:  54%|█████▎    | 22/41 [00:00<00:00, 25.16it/s]concatenating: train:  61%|██████    | 25/41 [00:00<00:00, 19.00it/s]concatenating: train:  68%|██████▊   | 28/41 [00:01<00:00, 18.61it/s]concatenating: train:  76%|███████▌  | 31/41 [00:01<00:00, 15.24it/s]concatenating: train:  80%|████████  | 33/41 [00:01<00:00, 12.12it/s]concatenating: train:  85%|████████▌ | 35/41 [00:01<00:00, 11.11it/s]concatenating: train:  90%|█████████ | 37/41 [00:02<00:00, 10.29it/s]concatenating: train:  95%|█████████▌| 39/41 [00:02<00:00, 10.93it/s]concatenating: train: 100%|██████████| 41/41 [00:02<00:00,  8.15it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:04<00:43,  4.38s/it]Loading test:  18%|█▊        | 2/11 [00:08<00:38,  4.29s/it]Loading test:  27%|██▋       | 3/11 [00:12<00:32,  4.10s/it]Loading test:  36%|███▋      | 4/11 [00:15<00:27,  3.87s/it]Loading test:  45%|████▌     | 5/11 [00:19<00:22,  3.79s/it]Loading test:  55%|█████▍    | 6/11 [00:22<00:18,  3.64s/it]Loading test:  64%|██████▎   | 7/11 [00:25<00:14,  3.56s/it]Loading test:  73%|███████▎  | 8/11 [00:29<00:11,  3.72s/it]Loading test:  82%|████████▏ | 9/11 [00:34<00:08,  4.05s/it]Loading test:  91%|█████████ | 10/11 [00:38<00:03,  3.98s/it]Loading test: 100%|██████████| 11/11 [00:42<00:00,  3.96s/it]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:   9%|▉         | 1/11 [00:00<00:01,  7.72it/s]concatenating: validation:  18%|█▊        | 2/11 [00:00<00:01,  8.13it/s]concatenating: validation:  36%|███▋      | 4/11 [00:00<00:00,  8.39it/s]concatenating: validation:  45%|████▌     | 5/11 [00:00<00:00,  8.15it/s]concatenating: validation:  64%|██████▎   | 7/11 [00:00<00:00,  9.40it/s]concatenating: validation:  82%|████████▏ | 9/11 [00:00<00:00,  9.85it/s]concatenating: validation:  91%|█████████ | 10/11 [00:01<00:00,  9.88it/s]concatenating: validation: 100%|██████████| 11/11 [00:01<00:00, 10.60it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 92, 116, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 92, 116, 20)  200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 92, 116, 20)  80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 92, 116, 20)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 92, 116, 20)  3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 92, 116, 20)  80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 92, 116, 20)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 46, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 46, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 46, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 46, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 46, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 46, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 46, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 46, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 46, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 23, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 23, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 23, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 23, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 23, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 23, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 23, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 23, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 23, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 23, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 46, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 46, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 46, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 46, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 46, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 46, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 46, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-28 21:28:52.465416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-28 21:28:52.465525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 21:28:52.465540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-28 21:28:52.465549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-28 21:28:52.465945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 46, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 92, 116, 20)  11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 92, 116, 40)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 92, 116, 20)  7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 92, 116, 20)  80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 92, 116, 20)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 92, 116, 20)  3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 92, 116, 20)  80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 92, 116, 20)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 92, 116, 60)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 92, 116, 60)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 92, 116, 13)  793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.55072799e-02 3.08830404e-02 7.68489529e-02 1.00912010e-02
 2.67215687e-02 7.01277282e-03 8.03424745e-02 1.16957272e-01
 7.80702233e-02 1.36444461e-02 3.13208894e-01 1.80677915e-01
 3.39596122e-05]
Train on 4060 samples, validate on 1096 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 20s - loss: 5.3233 - acc: 0.3474 - mDice: 0.0063 - val_loss: 4.6138 - val_acc: 0.7990 - val_mDice: 0.0105

Epoch 00001: val_mDice improved from -inf to 0.01049, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 2.6943 - acc: 0.9391 - mDice: 0.0884 - val_loss: 1.6608 - val_acc: 0.9824 - val_mDice: 0.1956

Epoch 00002: val_mDice improved from 0.01049 to 0.19563, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 1.4058 - acc: 0.9839 - mDice: 0.2381 - val_loss: 1.1399 - val_acc: 0.9852 - val_mDice: 0.3436

Epoch 00003: val_mDice improved from 0.19563 to 0.34359, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.9972 - acc: 0.9853 - mDice: 0.3502 - val_loss: 0.8938 - val_acc: 0.9853 - val_mDice: 0.4299

Epoch 00004: val_mDice improved from 0.34359 to 0.42991, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.8059 - acc: 0.9858 - mDice: 0.4259 - val_loss: 0.7724 - val_acc: 0.9853 - val_mDice: 0.4653

Epoch 00005: val_mDice improved from 0.42991 to 0.46525, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.7221 - acc: 0.9862 - mDice: 0.4663 - val_loss: 0.5621 - val_acc: 0.9858 - val_mDice: 0.5598

Epoch 00006: val_mDice improved from 0.46525 to 0.55983, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.6564 - acc: 0.9866 - mDice: 0.4984 - val_loss: 0.5421 - val_acc: 0.9862 - val_mDice: 0.5727

Epoch 00007: val_mDice improved from 0.55983 to 0.57266, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.6064 - acc: 0.9869 - mDice: 0.5256 - val_loss: 0.5172 - val_acc: 0.9863 - val_mDice: 0.5865

Epoch 00008: val_mDice improved from 0.57266 to 0.58647, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.5760 - acc: 0.9871 - mDice: 0.5435 - val_loss: 0.4900 - val_acc: 0.9865 - val_mDice: 0.6020

Epoch 00009: val_mDice improved from 0.58647 to 0.60205, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.5655 - acc: 0.9874 - mDice: 0.5548 - val_loss: 0.7335 - val_acc: 0.9872 - val_mDice: 0.5239

Epoch 00010: val_mDice did not improve from 0.60205
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.6157 - acc: 0.9871 - mDice: 0.5220 - val_loss: 0.5318 - val_acc: 0.9879 - val_mDice: 0.5781

Epoch 00011: val_mDice did not improve from 0.60205
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.5211 - acc: 0.9878 - mDice: 0.5753 - val_loss: 0.5072 - val_acc: 0.9885 - val_mDice: 0.5919

Epoch 00012: val_mDice did not improve from 0.60205
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.4873 - acc: 0.9882 - mDice: 0.5958 - val_loss: 0.4129 - val_acc: 0.9884 - val_mDice: 0.6519

Epoch 00013: val_mDice improved from 0.60205 to 0.65192, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.4659 - acc: 0.9885 - mDice: 0.6101 - val_loss: 0.4125 - val_acc: 0.9880 - val_mDice: 0.6511

Epoch 00014: val_mDice did not improve from 0.65192
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.4995 - acc: 0.9884 - mDice: 0.5905 - val_loss: 0.4339 - val_acc: 0.9895 - val_mDice: 0.6407

Epoch 00015: val_mDice did not improve from 0.65192
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.4506 - acc: 0.9888 - mDice: 0.6198 - val_loss: 0.4108 - val_acc: 0.9885 - val_mDice: 0.6535

Epoch 00016: val_mDice improved from 0.65192 to 0.65345, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.4302 - acc: 0.9891 - mDice: 0.6329 - val_loss: 0.3948 - val_acc: 0.9895 - val_mDice: 0.6628

Epoch 00017: val_mDice improved from 0.65345 to 0.66281, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.4217 - acc: 0.9893 - mDice: 0.6399 - val_loss: 0.3805 - val_acc: 0.9900 - val_mDice: 0.6729

Epoch 00018: val_mDice improved from 0.66281 to 0.67292, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.4073 - acc: 0.9896 - mDice: 0.6482 - val_loss: 0.3706 - val_acc: 0.9899 - val_mDice: 0.6800

Epoch 00019: val_mDice improved from 0.67292 to 0.68002, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3977 - acc: 0.9897 - mDice: 0.6551 - val_loss: 0.3786 - val_acc: 0.9896 - val_mDice: 0.6750

Epoch 00020: val_mDice did not improve from 0.68002
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3939 - acc: 0.9898 - mDice: 0.6587 - val_loss: 0.3643 - val_acc: 0.9893 - val_mDice: 0.6842

Epoch 00021: val_mDice improved from 0.68002 to 0.68425, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3857 - acc: 0.9899 - mDice: 0.6637 - val_loss: 0.3817 - val_acc: 0.9891 - val_mDice: 0.6734

Epoch 00022: val_mDice did not improve from 0.68425
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3818 - acc: 0.9899 - mDice: 0.6658 - val_loss: 0.3644 - val_acc: 0.9896 - val_mDice: 0.6847

Epoch 00023: val_mDice improved from 0.68425 to 0.68468, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3714 - acc: 0.9901 - mDice: 0.6730 - val_loss: 0.3647 - val_acc: 0.9899 - val_mDice: 0.6854

Epoch 00024: val_mDice improved from 0.68468 to 0.68539, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3769 - acc: 0.9901 - mDice: 0.6695 - val_loss: 0.3644 - val_acc: 0.9900 - val_mDice: 0.6851

Epoch 00025: val_mDice did not improve from 0.68539
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3684 - acc: 0.9903 - mDice: 0.6753 - val_loss: 0.3653 - val_acc: 0.9910 - val_mDice: 0.6844

Epoch 00026: val_mDice did not improve from 0.68539
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3674 - acc: 0.9904 - mDice: 0.6763 - val_loss: 0.3560 - val_acc: 0.9905 - val_mDice: 0.6910

Epoch 00027: val_mDice improved from 0.68539 to 0.69099, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3655 - acc: 0.9904 - mDice: 0.6784 - val_loss: 0.3603 - val_acc: 0.9906 - val_mDice: 0.6887

Epoch 00028: val_mDice did not improve from 0.69099
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.4442 - acc: 0.9896 - mDice: 0.6368 - val_loss: 0.4632 - val_acc: 0.9884 - val_mDice: 0.6185

Epoch 00029: val_mDice did not improve from 0.69099
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3984 - acc: 0.9900 - mDice: 0.6548 - val_loss: 0.3953 - val_acc: 0.9900 - val_mDice: 0.6651

Epoch 00030: val_mDice did not improve from 0.69099
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3780 - acc: 0.9905 - mDice: 0.6688 - val_loss: 0.3721 - val_acc: 0.9897 - val_mDice: 0.6796

Epoch 00031: val_mDice did not improve from 0.69099
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3637 - acc: 0.9907 - mDice: 0.6787 - val_loss: 0.3682 - val_acc: 0.9905 - val_mDice: 0.6814

Epoch 00032: val_mDice did not improve from 0.69099
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3618 - acc: 0.9908 - mDice: 0.6802 - val_loss: 0.3503 - val_acc: 0.9904 - val_mDice: 0.6931

Epoch 00033: val_mDice improved from 0.69099 to 0.69314, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3586 - acc: 0.9909 - mDice: 0.6833 - val_loss: 0.3583 - val_acc: 0.9903 - val_mDice: 0.6886

Epoch 00034: val_mDice did not improve from 0.69314
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3552 - acc: 0.9910 - mDice: 0.6853 - val_loss: 0.3609 - val_acc: 0.9918 - val_mDice: 0.6873

Epoch 00035: val_mDice did not improve from 0.69314
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3496 - acc: 0.9911 - mDice: 0.6893 - val_loss: 0.3568 - val_acc: 0.9917 - val_mDice: 0.6909

Epoch 00036: val_mDice did not improve from 0.69314
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3668 - acc: 0.9911 - mDice: 0.6799 - val_loss: 0.3630 - val_acc: 0.9920 - val_mDice: 0.6865

Epoch 00037: val_mDice did not improve from 0.69314
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3408 - acc: 0.9913 - mDice: 0.6951 - val_loss: 0.3479 - val_acc: 0.9924 - val_mDice: 0.6960

Epoch 00038: val_mDice improved from 0.69314 to 0.69604, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3387 - acc: 0.9914 - mDice: 0.6973 - val_loss: 0.3491 - val_acc: 0.9917 - val_mDice: 0.6962

Epoch 00039: val_mDice improved from 0.69604 to 0.69615, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3366 - acc: 0.9914 - mDice: 0.6987 - val_loss: 0.3501 - val_acc: 0.9923 - val_mDice: 0.6951

Epoch 00040: val_mDice did not improve from 0.69615
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3326 - acc: 0.9915 - mDice: 0.7014 - val_loss: 0.3626 - val_acc: 0.9919 - val_mDice: 0.6875

Epoch 00041: val_mDice did not improve from 0.69615
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3328 - acc: 0.9916 - mDice: 0.7011 - val_loss: 0.3475 - val_acc: 0.9919 - val_mDice: 0.6971

Epoch 00042: val_mDice improved from 0.69615 to 0.69708, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3344 - acc: 0.9916 - mDice: 0.7005 - val_loss: 0.3466 - val_acc: 0.9922 - val_mDice: 0.6975

Epoch 00043: val_mDice improved from 0.69708 to 0.69751, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3305 - acc: 0.9917 - mDice: 0.7030 - val_loss: 0.3484 - val_acc: 0.9920 - val_mDice: 0.6960

Epoch 00044: val_mDice did not improve from 0.69751
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3313 - acc: 0.9916 - mDice: 0.7027 - val_loss: 0.3505 - val_acc: 0.9919 - val_mDice: 0.6956

Epoch 00045: val_mDice did not improve from 0.69751
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3262 - acc: 0.9918 - mDice: 0.7061 - val_loss: 0.3474 - val_acc: 0.9915 - val_mDice: 0.6969

Epoch 00046: val_mDice did not improve from 0.69751
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.4153 - acc: 0.9908 - mDice: 0.6582 - val_loss: 0.4219 - val_acc: 0.9913 - val_mDice: 0.6511

Epoch 00047: val_mDice did not improve from 0.69751
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3486 - acc: 0.9915 - mDice: 0.6897 - val_loss: 0.3659 - val_acc: 0.9920 - val_mDice: 0.6845

Epoch 00048: val_mDice did not improve from 0.69751
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3344 - acc: 0.9918 - mDice: 0.7002 - val_loss: 0.3646 - val_acc: 0.9924 - val_mDice: 0.6855

Epoch 00049: val_mDice did not improve from 0.69751
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3326 - acc: 0.9918 - mDice: 0.7022 - val_loss: 0.3498 - val_acc: 0.9925 - val_mDice: 0.6949

Epoch 00050: val_mDice did not improve from 0.69751
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3687 - acc: 0.9916 - mDice: 0.6972 - val_loss: 0.3548 - val_acc: 0.9922 - val_mDice: 0.6918

Epoch 00051: val_mDice did not improve from 0.69751
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3273 - acc: 0.9917 - mDice: 0.7055 - val_loss: 0.3480 - val_acc: 0.9921 - val_mDice: 0.6969

Epoch 00052: val_mDice did not improve from 0.69751
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3282 - acc: 0.9918 - mDice: 0.7053 - val_loss: 0.3504 - val_acc: 0.9923 - val_mDice: 0.6952

Epoch 00053: val_mDice did not improve from 0.69751
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.3269 - acc: 0.9920 - mDice: 0.7062 - val_loss: 0.3437 - val_acc: 0.9924 - val_mDice: 0.6994

Epoch 00054: val_mDice improved from 0.69751 to 0.69943, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.3196 - acc: 0.9920 - mDice: 0.7109 - val_loss: 0.3443 - val_acc: 0.9924 - val_mDice: 0.6989

Epoch 00055: val_mDice did not improve from 0.69943
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.3183 - acc: 0.9921 - mDice: 0.7121 - val_loss: 0.3446 - val_acc: 0.9927 - val_mDice: 0.6989

Epoch 00056: val_mDice did not improve from 0.69943
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.3175 - acc: 0.9921 - mDice: 0.7128 - val_loss: 0.3445 - val_acc: 0.9925 - val_mDice: 0.6993

Epoch 00057: val_mDice did not improve from 0.69943
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.3175 - acc: 0.9922 - mDice: 0.7127 - val_loss: 0.3419 - val_acc: 0.9927 - val_mDice: 0.7005

Epoch 00058: val_mDice improved from 0.69943 to 0.70045, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.3209 - acc: 0.9921 - mDice: 0.7110 - val_loss: 0.3448 - val_acc: 0.9926 - val_mDice: 0.6993

Epoch 00059: val_mDice did not improve from 0.70045
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3180 - acc: 0.9921 - mDice: 0.7124 - val_loss: 0.3490 - val_acc: 0.9924 - val_mDice: 0.6961

Epoch 00060: val_mDice did not improve from 0.70045
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3161 - acc: 0.9922 - mDice: 0.7138 - val_loss: 0.3426 - val_acc: 0.9928 - val_mDice: 0.7004

Epoch 00061: val_mDice did not improve from 0.70045
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3155 - acc: 0.9922 - mDice: 0.7144 - val_loss: 0.3430 - val_acc: 0.9928 - val_mDice: 0.6999

Epoch 00062: val_mDice did not improve from 0.70045
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3149 - acc: 0.9922 - mDice: 0.7151 - val_loss: 0.3442 - val_acc: 0.9927 - val_mDice: 0.6996

Epoch 00063: val_mDice did not improve from 0.70045
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3184 - acc: 0.9923 - mDice: 0.7128 - val_loss: 0.3435 - val_acc: 0.9928 - val_mDice: 0.7002

Epoch 00064: val_mDice did not improve from 0.70045
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3142 - acc: 0.9923 - mDice: 0.7153 - val_loss: 0.3416 - val_acc: 0.9930 - val_mDice: 0.7010

Epoch 00065: val_mDice improved from 0.70045 to 0.70104, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3128 - acc: 0.9923 - mDice: 0.7163 - val_loss: 0.3431 - val_acc: 0.9927 - val_mDice: 0.6998

Epoch 00066: val_mDice did not improve from 0.70104
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3118 - acc: 0.9923 - mDice: 0.7177 - val_loss: 0.3438 - val_acc: 0.9931 - val_mDice: 0.6989

Epoch 00067: val_mDice did not improve from 0.70104
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3417 - acc: 0.9923 - mDice: 0.7140 - val_loss: 0.3506 - val_acc: 0.9931 - val_mDice: 0.6954

Epoch 00068: val_mDice did not improve from 0.70104
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3162 - acc: 0.9923 - mDice: 0.7140 - val_loss: 0.3457 - val_acc: 0.9927 - val_mDice: 0.6985

Epoch 00069: val_mDice did not improve from 0.70104
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3236 - acc: 0.9923 - mDice: 0.7107 - val_loss: 0.3832 - val_acc: 0.9923 - val_mDice: 0.6723

Epoch 00070: val_mDice did not improve from 0.70104
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3519 - acc: 0.9919 - mDice: 0.7005 - val_loss: 0.3642 - val_acc: 0.9923 - val_mDice: 0.6866

Epoch 00071: val_mDice did not improve from 0.70104
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3136 - acc: 0.9924 - mDice: 0.7156 - val_loss: 0.3461 - val_acc: 0.9928 - val_mDice: 0.6979

Epoch 00072: val_mDice did not improve from 0.70104
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3091 - acc: 0.9924 - mDice: 0.7186 - val_loss: 0.3449 - val_acc: 0.9929 - val_mDice: 0.6989

Epoch 00073: val_mDice did not improve from 0.70104
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3448 - acc: 0.9924 - mDice: 0.7145 - val_loss: 0.3423 - val_acc: 0.9930 - val_mDice: 0.7003

Epoch 00074: val_mDice did not improve from 0.70104
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3400 - acc: 0.9924 - mDice: 0.7162 - val_loss: 0.3412 - val_acc: 0.9930 - val_mDice: 0.7008

Epoch 00075: val_mDice did not improve from 0.70104
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3100 - acc: 0.9925 - mDice: 0.7184 - val_loss: 0.3399 - val_acc: 0.9931 - val_mDice: 0.7015

Epoch 00076: val_mDice improved from 0.70104 to 0.70154, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3122 - acc: 0.9925 - mDice: 0.7173 - val_loss: 0.3423 - val_acc: 0.9931 - val_mDice: 0.7007

Epoch 00077: val_mDice did not improve from 0.70154
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3076 - acc: 0.9925 - mDice: 0.7202 - val_loss: 0.3441 - val_acc: 0.9931 - val_mDice: 0.6994

Epoch 00078: val_mDice did not improve from 0.70154
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3056 - acc: 0.9926 - mDice: 0.7214 - val_loss: 0.3449 - val_acc: 0.9932 - val_mDice: 0.6992

Epoch 00079: val_mDice did not improve from 0.70154
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3096 - acc: 0.9925 - mDice: 0.7188 - val_loss: 0.3435 - val_acc: 0.9931 - val_mDice: 0.7000

Epoch 00080: val_mDice did not improve from 0.70154
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3070 - acc: 0.9925 - mDice: 0.7208 - val_loss: 0.3397 - val_acc: 0.9932 - val_mDice: 0.7021

Epoch 00081: val_mDice improved from 0.70154 to 0.70207, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3119 - acc: 0.9925 - mDice: 0.7181 - val_loss: 0.3421 - val_acc: 0.9931 - val_mDice: 0.7008

Epoch 00082: val_mDice did not improve from 0.70207
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3048 - acc: 0.9926 - mDice: 0.7222 - val_loss: 0.3422 - val_acc: 0.9932 - val_mDice: 0.7008

Epoch 00083: val_mDice did not improve from 0.70207
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3260 - acc: 0.9926 - mDice: 0.7178 - val_loss: 0.3429 - val_acc: 0.9929 - val_mDice: 0.7003

Epoch 00084: val_mDice did not improve from 0.70207
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3229 - acc: 0.9923 - mDice: 0.7105 - val_loss: 0.3572 - val_acc: 0.9924 - val_mDice: 0.6926

Epoch 00085: val_mDice did not improve from 0.70207
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3465 - acc: 0.9924 - mDice: 0.7085 - val_loss: 0.3504 - val_acc: 0.9933 - val_mDice: 0.6954

Epoch 00086: val_mDice did not improve from 0.70207
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3103 - acc: 0.9927 - mDice: 0.7181 - val_loss: 0.3435 - val_acc: 0.9932 - val_mDice: 0.6998

Epoch 00087: val_mDice did not improve from 0.70207
Epoch 88/300

Epoch 00088: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3362 - acc: 0.9924 - mDice: 0.7036 - val_loss: 0.3520 - val_acc: 0.9932 - val_mDice: 0.6952

Epoch 00088: val_mDice did not improve from 0.70207
Epoch 89/300

Epoch 00089: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3318 - acc: 0.9925 - mDice: 0.7148 - val_loss: 0.3476 - val_acc: 0.9931 - val_mDice: 0.6973

Epoch 00089: val_mDice did not improve from 0.70207
Epoch 90/300

Epoch 00090: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3063 - acc: 0.9926 - mDice: 0.7213 - val_loss: 0.3424 - val_acc: 0.9931 - val_mDice: 0.7003

Epoch 00090: val_mDice did not improve from 0.70207
Epoch 91/300

Epoch 00091: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3295 - acc: 0.9926 - mDice: 0.7189 - val_loss: 0.3437 - val_acc: 0.9931 - val_mDice: 0.6993

Epoch 00091: val_mDice did not improve from 0.70207
Epoch 92/300

Epoch 00092: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3096 - acc: 0.9926 - mDice: 0.7191 - val_loss: 0.3413 - val_acc: 0.9932 - val_mDice: 0.7009

Epoch 00092: val_mDice did not improve from 0.70207
Epoch 93/300

Epoch 00093: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3076 - acc: 0.9927 - mDice: 0.7204 - val_loss: 0.3409 - val_acc: 0.9932 - val_mDice: 0.7011

Epoch 00093: val_mDice did not improve from 0.70207
Epoch 94/300

Epoch 00094: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3069 - acc: 0.9926 - mDice: 0.7217 - val_loss: 0.3403 - val_acc: 0.9931 - val_mDice: 0.7018

Epoch 00094: val_mDice did not improve from 0.70207
Epoch 95/300

Epoch 00095: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3035 - acc: 0.9926 - mDice: 0.7230 - val_loss: 0.3407 - val_acc: 0.9932 - val_mDice: 0.7015

Epoch 00095: val_mDice did not improve from 0.70207
Epoch 96/300

Epoch 00096: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3070 - acc: 0.9926 - mDice: 0.7212 - val_loss: 0.3388 - val_acc: 0.9932 - val_mDice: 0.7026

Epoch 00096: val_mDice improved from 0.70207 to 0.70257, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 97/300

Epoch 00097: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2996 - acc: 0.9927 - mDice: 0.7259 - val_loss: 0.3408 - val_acc: 0.9932 - val_mDice: 0.7015

Epoch 00097: val_mDice did not improve from 0.70257
Epoch 98/300

Epoch 00098: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3048 - acc: 0.9927 - mDice: 0.7226 - val_loss: 0.3397 - val_acc: 0.9933 - val_mDice: 0.7020

Epoch 00098: val_mDice did not improve from 0.70257
Epoch 99/300

Epoch 00099: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3081 - acc: 0.9927 - mDice: 0.7216 - val_loss: 0.3394 - val_acc: 0.9933 - val_mDice: 0.7023

Epoch 00099: val_mDice did not improve from 0.70257
Epoch 100/300

Epoch 00100: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3036 - acc: 0.9927 - mDice: 0.7232 - val_loss: 0.3403 - val_acc: 0.9933 - val_mDice: 0.7016

Epoch 00100: val_mDice did not improve from 0.70257
Epoch 101/300

Epoch 00101: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3029 - acc: 0.9927 - mDice: 0.7237 - val_loss: 0.3404 - val_acc: 0.9933 - val_mDice: 0.7017

Epoch 00101: val_mDice did not improve from 0.70257
Epoch 102/300

Epoch 00102: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3004 - acc: 0.9927 - mDice: 0.7257 - val_loss: 0.3399 - val_acc: 0.9933 - val_mDice: 0.7020

Epoch 00102: val_mDice did not improve from 0.70257
Epoch 103/300

Epoch 00103: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3027 - acc: 0.9927 - mDice: 0.7239 - val_loss: 0.3409 - val_acc: 0.9933 - val_mDice: 0.7013

Epoch 00103: val_mDice did not improve from 0.70257
Epoch 104/300

Epoch 00104: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3041 - acc: 0.9927 - mDice: 0.7227 - val_loss: 0.3390 - val_acc: 0.9933 - val_mDice: 0.7026

Epoch 00104: val_mDice improved from 0.70257 to 0.70263, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 105/300

Epoch 00105: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3028 - acc: 0.9927 - mDice: 0.7238 - val_loss: 0.3393 - val_acc: 0.9933 - val_mDice: 0.7024

Epoch 00105: val_mDice did not improve from 0.70263
Epoch 106/300

Epoch 00106: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3079 - acc: 0.9927 - mDice: 0.7223 - val_loss: 0.3386 - val_acc: 0.9933 - val_mDice: 0.7029

Epoch 00106: val_mDice improved from 0.70263 to 0.70287, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 107/300

Epoch 00107: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3017 - acc: 0.9927 - mDice: 0.7243 - val_loss: 0.3417 - val_acc: 0.9932 - val_mDice: 0.7010

Epoch 00107: val_mDice did not improve from 0.70287
Epoch 108/300

Epoch 00108: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3031 - acc: 0.9927 - mDice: 0.7236 - val_loss: 0.3409 - val_acc: 0.9933 - val_mDice: 0.7016

Epoch 00108: val_mDice did not improve from 0.70287
Epoch 109/300

Epoch 00109: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3025 - acc: 0.9927 - mDice: 0.7241 - val_loss: 0.3397 - val_acc: 0.9933 - val_mDice: 0.7023

Epoch 00109: val_mDice did not improve from 0.70287
Epoch 110/300

Epoch 00110: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3026 - acc: 0.9927 - mDice: 0.7239 - val_loss: 0.3406 - val_acc: 0.9933 - val_mDice: 0.7018

Epoch 00110: val_mDice did not improve from 0.70287
Epoch 111/300

Epoch 00111: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3055 - acc: 0.9927 - mDice: 0.7231 - val_loss: 0.3405 - val_acc: 0.9933 - val_mDice: 0.7019

Epoch 00111: val_mDice did not improve from 0.70287
Epoch 112/300

Epoch 00112: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3006 - acc: 0.9928 - mDice: 0.7253 - val_loss: 0.3392 - val_acc: 0.9933 - val_mDice: 0.7026

Epoch 00112: val_mDice did not improve from 0.70287
Epoch 113/300

Epoch 00113: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3389 - acc: 0.9928 - mDice: 0.7219 - val_loss: 0.3403 - val_acc: 0.9933 - val_mDice: 0.7019

Epoch 00113: val_mDice did not improve from 0.70287
Epoch 114/300

Epoch 00114: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.3001 - acc: 0.9928 - mDice: 0.7257 - val_loss: 0.3399 - val_acc: 0.9933 - val_mDice: 0.7021

Epoch 00114: val_mDice did not improve from 0.70287
Epoch 115/300

Epoch 00115: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.2990 - acc: 0.9928 - mDice: 0.7266 - val_loss: 0.3403 - val_acc: 0.9933 - val_mDice: 0.7018

Epoch 00115: val_mDice did not improve from 0.70287
Epoch 116/300

Epoch 00116: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.2987 - acc: 0.9928 - mDice: 0.7267 - val_loss: 0.3401 - val_acc: 0.9933 - val_mDice: 0.7021

Epoch 00116: val_mDice did not improve from 0.70287
Epoch 117/300

Epoch 00117: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3013 - acc: 0.9928 - mDice: 0.7250 - val_loss: 0.3411 - val_acc: 0.9933 - val_mDice: 0.7016

Epoch 00117: val_mDice did not improve from 0.70287
Epoch 118/300

Epoch 00118: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.2981 - acc: 0.9928 - mDice: 0.7273 - val_loss: 0.3412 - val_acc: 0.9933 - val_mDice: 0.7015

Epoch 00118: val_mDice did not improve from 0.70287
Epoch 119/300

Epoch 00119: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3205 - acc: 0.9928 - mDice: 0.7243 - val_loss: 0.3417 - val_acc: 0.9934 - val_mDice: 0.7012

Epoch 00119: val_mDice did not improve from 0.70287
Epoch 120/300

Epoch 00120: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3076 - acc: 0.9928 - mDice: 0.7227 - val_loss: 0.3406 - val_acc: 0.9934 - val_mDice: 0.7017

Epoch 00120: val_mDice did not improve from 0.70287
Epoch 121/300

Epoch 00121: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3027 - acc: 0.9928 - mDice: 0.7239 - val_loss: 0.3419 - val_acc: 0.9934 - val_mDice: 0.7007

Epoch 00121: val_mDice did not improve from 0.70287
Epoch 122/300

Epoch 00122: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3011 - acc: 0.9928 - mDice: 0.7251 - val_loss: 0.3410 - val_acc: 0.9934 - val_mDice: 0.7015

Epoch 00122: val_mDice did not improve from 0.70287
Epoch 123/300

Epoch 00123: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3024 - acc: 0.9928 - mDice: 0.7250 - val_loss: 0.3405 - val_acc: 0.9934 - val_mDice: 0.7018

Epoch 00123: val_mDice did not improve from 0.70287
Epoch 124/300

Epoch 00124: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.2986 - acc: 0.9928 - mDice: 0.7268 - val_loss: 0.3405 - val_acc: 0.9934 - val_mDice: 0.7018

Epoch 00124: val_mDice did not improve from 0.70287
Epoch 125/300

Epoch 00125: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3000 - acc: 0.9928 - mDice: 0.7260 - val_loss: 0.3404 - val_acc: 0.9934 - val_mDice: 0.7018

Epoch 00125: val_mDice did not improve from 0.70287
Epoch 126/300

Epoch 00126: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.2989 - acc: 0.9928 - mDice: 0.7268 - val_loss: 0.3410 - val_acc: 0.9934 - val_mDice: 0.7015

Epoch 00126: val_mDice did not improve from 0.70287
Epoch 127/300

Epoch 00127: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.3008 - acc: 0.9928 - mDice: 0.7256 - val_loss: 0.3413 - val_acc: 0.9933 - val_mDice: 0.7014

Epoch 00127: val_mDice did not improve from 0.70287
Epoch 128/300

Epoch 00128: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.2981 - acc: 0.9928 - mDice: 0.7278 - val_loss: 0.3410 - val_acc: 0.9933 - val_mDice: 0.7016

Epoch 00128: val_mDice did not improve from 0.70287
Epoch 129/300

Epoch 00129: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.2982 - acc: 0.9929 - mDice: 0.7274 - val_loss: 0.3410 - val_acc: 0.9933 - val_mDice: 0.7015

Epoch 00129: val_mDice did not improve from 0.70287
Epoch 130/300

Epoch 00130: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.3305 - acc: 0.9928 - mDice: 0.7247 - val_loss: 0.3412 - val_acc: 0.9934 - val_mDice: 0.7015

Epoch 00130: val_mDice did not improve from 0.70287
Epoch 131/300

Epoch 00131: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.2994 - acc: 0.9928 - mDice: 0.7267 - val_loss: 0.3404 - val_acc: 0.9934 - val_mDice: 0.7019

Epoch 00131: val_mDice did not improve from 0.70287
Epoch 132/300

Epoch 00132: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.2996 - acc: 0.9928 - mDice: 0.7261 - val_loss: 0.3400 - val_acc: 0.9934 - val_mDice: 0.7022

Epoch 00132: val_mDice did not improve from 0.70287
Epoch 133/300

Epoch 00133: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.3012 - acc: 0.9928 - mDice: 0.7251 - val_loss: 0.3397 - val_acc: 0.9934 - val_mDice: 0.7024

Epoch 00133: val_mDice did not improve from 0.70287
Epoch 134/300

Epoch 00134: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.3014 - acc: 0.9928 - mDice: 0.7252 - val_loss: 0.3398 - val_acc: 0.9934 - val_mDice: 0.7023

Epoch 00134: val_mDice did not improve from 0.70287
Epoch 135/300

Epoch 00135: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.2967 - acc: 0.9929 - mDice: 0.7282 - val_loss: 0.3397 - val_acc: 0.9934 - val_mDice: 0.7024

Epoch 00135: val_mDice did not improve from 0.70287
Epoch 136/300

Epoch 00136: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.2972 - acc: 0.9929 - mDice: 0.7279 - val_loss: 0.3396 - val_acc: 0.9934 - val_mDice: 0.7024

Epoch 00136: val_mDice did not improve from 0.70287
Epoch 137/300

Epoch 00137: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.2994 - acc: 0.9928 - mDice: 0.7263 - val_loss: 0.3407 - val_acc: 0.9933 - val_mDice: 0.7018

Epoch 00137: val_mDice did not improve from 0.70287
Epoch 138/300

Epoch 00138: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.3169 - acc: 0.9929 - mDice: 0.7250 - val_loss: 0.3400 - val_acc: 0.9934 - val_mDice: 0.7021

Epoch 00138: val_mDice did not improve from 0.70287
Epoch 139/300

Epoch 00139: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.2994 - acc: 0.9928 - mDice: 0.7271 - val_loss: 0.3403 - val_acc: 0.9934 - val_mDice: 0.7019

Epoch 00139: val_mDice did not improve from 0.70287
Epoch 140/300

Epoch 00140: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.2998 - acc: 0.9928 - mDice: 0.7263 - val_loss: 0.3405 - val_acc: 0.9934 - val_mDice: 0.7019

Epoch 00140: val_mDice did not improve from 0.70287
Epoch 141/300

Epoch 00141: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.2983 - acc: 0.9929 - mDice: 0.7272 - val_loss: 0.3409 - val_acc: 0.9934 - val_mDice: 0.7018

Epoch 00141: val_mDice did not improve from 0.70287
Epoch 142/300

Epoch 00142: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.3017 - acc: 0.9928 - mDice: 0.7252 - val_loss: 0.3398 - val_acc: 0.9934 - val_mDice: 0.7023

Epoch 00142: val_mDice did not improve from 0.70287
Epoch 143/300

Epoch 00143: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 12s - loss: 0.2991 - acc: 0.9929 - mDice: 0.7264 - val_loss: 0.3402 - val_acc: 0.9934 - val_mDice: 0.7020

Epoch 00143: val_mDice did not improve from 0.70287
Epoch 144/300

Epoch 00144: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 12s - loss: 0.3006 - acc: 0.9929 - mDice: 0.7259 - val_loss: 0.3396 - val_acc: 0.9934 - val_mDice: 0.7024

Epoch 00144: val_mDice did not improve from 0.70287
Epoch 145/300

Epoch 00145: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 12s - loss: 0.3001 - acc: 0.9929 - mDice: 0.7261 - val_loss: 0.3401 - val_acc: 0.9934 - val_mDice: 0.7022

Epoch 00145: val_mDice did not improve from 0.70287
Epoch 146/300

Epoch 00146: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 12s - loss: 0.3016 - acc: 0.9929 - mDice: 0.7249 - val_loss: 0.3402 - val_acc: 0.9934 - val_mDice: 0.7021

Epoch 00146: val_mDice did not improve from 0.70287
Restoring model weights from the end of the best epoch
Epoch 00146: early stopping
{'val_loss': [4.613824666851628, 1.660845888357093, 1.139907489292813, 0.8937741587631893, 0.7723569591550061, 0.5620846835366131, 0.5421351099753902, 0.5172326405988122, 0.4899979791719548, 0.7334900770309197, 0.5317599630921427, 0.5071908026716135, 0.4128518276406031, 0.4124918779099945, 0.4338728091577544, 0.4108450340963628, 0.3948284002551197, 0.38049561731571696, 0.37061180337502136, 0.37862036621918643, 0.3642821874279175, 0.38171297034425455, 0.36444903301061504, 0.36467043957571044, 0.36441467055221544, 0.3653293329760106, 0.35599301396495237, 0.36034245549762334, 0.4632178614609433, 0.39526284219574753, 0.37213693564608147, 0.36822134473898116, 0.3502892799077243, 0.3582939962401007, 0.36087988621562067, 0.35676621297632694, 0.3629849755959789, 0.34792184900410855, 0.3491389930357028, 0.350055993962897, 0.3626018750624065, 0.34751324442616344, 0.34660906545872233, 0.3484082893624793, 0.35048286535226514, 0.3473513333575569, 0.4218879935950258, 0.36593665508893286, 0.36459749447603296, 0.3498355403651286, 0.3548288363283568, 0.3479929739867684, 0.35038021962790594, 0.3437092617480424, 0.3442629691023026, 0.344558022644398, 0.3444686602719509, 0.34186700508542306, 0.3447558711262515, 0.3490298326759443, 0.3426324229714644, 0.3430129818781449, 0.34416496111963785, 0.3435045317576749, 0.3415904597644388, 0.34306911079987995, 0.3438163599828734, 0.350620803661155, 0.3456515268470249, 0.38321416547698695, 0.3641505054313771, 0.34614072566049814, 0.3449254444915883, 0.3423226393052261, 0.34124037482007574, 0.3398516293424759, 0.34232576283877786, 0.3441495826862154, 0.34488267027331093, 0.3435397062749758, 0.3397202102372246, 0.3421335861521916, 0.3422057317012418, 0.3429068466716439, 0.35718513911005356, 0.3504091222247068, 0.3435404083698335, 0.352038245838489, 0.3475634778176781, 0.3423643107288075, 0.3436592960879751, 0.3413473455792796, 0.3409357004574616, 0.3403054529080426, 0.34069473573761266, 0.3387792181685893, 0.34075863659381866, 0.3397499239140183, 0.3393668344747411, 0.34029753359347364, 0.3403657328908461, 0.33987053864411193, 0.34089139577028527, 0.33904681942106163, 0.33932211386026256, 0.3386487112971988, 0.34166158322435225, 0.3408778144495331, 0.3396841972304957, 0.3405690898534155, 0.34051826041545313, 0.33918135227078067, 0.3402797443261982, 0.33990136724318903, 0.3403362880331756, 0.3400622369164098, 0.3410852739628214, 0.34121887271639206, 0.3417452938147705, 0.34063029512219184, 0.3419099398990617, 0.34099096641705856, 0.34048699315664543, 0.3404674275513113, 0.34038837362814994, 0.34103725605855023, 0.34128836993753475, 0.3409906121609855, 0.34101905695495816, 0.34118027816506197, 0.34040016316584426, 0.34002115044498094, 0.3396921090401002, 0.3398258203681368, 0.33972886003499486, 0.3395592247265099, 0.3407421087587837, 0.3400334114902211, 0.34033192917160743, 0.3404943402992548, 0.34087835029311425, 0.3398287691665392, 0.3402303050469308, 0.3396481562161098, 0.34005992984684713, 0.3401769911394502], 'val_acc': [0.7990069078267926, 0.9823869727823856, 0.9851648144043275, 0.9853286229780991, 0.9853079332052356, 0.9857971254926529, 0.9862263994495364, 0.986274290258867, 0.9864595517189834, 0.9871532647279058, 0.9879391352190589, 0.9885440220798019, 0.9883523298441058, 0.9879970261215294, 0.9894612174834648, 0.9885111005201827, 0.9894965317127479, 0.9900248835991768, 0.989904939693256, 0.9895795492360192, 0.9893024629049928, 0.989094186220726, 0.9896341671038719, 0.9898949332915954, 0.9899726466540872, 0.9909723531590761, 0.9905033185534233, 0.9906280532370518, 0.9884023538036067, 0.9900497698435818, 0.9897371091111733, 0.9905441997260073, 0.990406289805461, 0.9903466029323801, 0.9918299641487373, 0.9917007721688625, 0.9919544338744922, 0.9923795220190591, 0.9916728134572941, 0.992322662015901, 0.9919235828149058, 0.9918555214892338, 0.9922076696462004, 0.9919862494851551, 0.9918966519571569, 0.9915034481643761, 0.991347926594045, 0.9920252287474862, 0.9924308166016628, 0.9924904923804485, 0.9921638162901801, 0.9921495444583197, 0.9922902731999864, 0.9924019301024667, 0.9923586593057118, 0.9926665349163278, 0.9924781024456024, 0.9927402238776214, 0.9926344776240579, 0.992431239055021, 0.9928033251832001, 0.9928308605712696, 0.9926673023805132, 0.9927792155394589, 0.9930473257155314, 0.9926649110160605, 0.9931230790858722, 0.9931235911637327, 0.9926838853063374, 0.992342172747981, 0.9922612002731239, 0.9927734800063781, 0.9928794174733824, 0.9930064575950595, 0.9929924381040308, 0.993099741039485, 0.9931406026339009, 0.9930630559033721, 0.9932070376664183, 0.9931012792308835, 0.9931500934771378, 0.993055535493976, 0.9931555623120635, 0.9928848856557024, 0.9923503631657927, 0.9933108282785346, 0.9932219936464824, 0.9932490228301417, 0.9930610199914361, 0.9931025502890566, 0.9931250456040793, 0.993161721821249, 0.9931506079478856, 0.9931217105719294, 0.9931562510285065, 0.993236195867079, 0.9932219951692289, 0.9932689246905111, 0.9932640584280891, 0.9932504657411227, 0.9932543078478235, 0.9933015003691624, 0.9932834573470763, 0.9932892827221947, 0.9932850853805124, 0.9932822735205183, 0.9932357527478768, 0.9933170645776457, 0.9932999632654399, 0.9933097943337295, 0.9933119401009414, 0.9933067194736787, 0.993308259622894, 0.9933037257542575, 0.9932975779919728, 0.9933265586839105, 0.9933107408293842, 0.9933150971892977, 0.9933811933019735, 0.9934224827881277, 0.99336461124629, 0.9933717864273238, 0.9933907694190088, 0.9933725528038331, 0.9933614371901881, 0.9933522961435527, 0.9933441703336953, 0.9933398070126555, 0.9933460513605689, 0.9933612790420978, 0.9933663067156381, 0.9933640067159695, 0.9933724729684148, 0.9933840941338643, 0.9933534910644057, 0.9933530583868931, 0.9933432399356452, 0.993393071158959, 0.9933944394553664, 0.9933845228957434, 0.993380510458981, 0.9933847800223496, 0.9933720407259725, 0.9933873421519342, 0.9933861568026299, 0.9933716889715543], 'val_mDice': [0.010490523396318193, 0.1956317176784042, 0.3435943586765415, 0.42991405116380565, 0.4652513152491437, 0.5598302376966406, 0.5726595277333781, 0.5864665323365343, 0.6020497172853373, 0.5238811497705697, 0.5780751811761926, 0.5919262929989474, 0.6519153590184928, 0.6511029657656259, 0.6407328469909891, 0.6534512188747852, 0.66280797123909, 0.6729209262089137, 0.6800225002922281, 0.6749793014822215, 0.6842480915306258, 0.6734084815439516, 0.6846819652693115, 0.6853930597757771, 0.6850842424552807, 0.6843997052986256, 0.690986652661414, 0.6887422445481711, 0.6185250519400965, 0.6650559351827107, 0.6795631029310018, 0.6814077594419465, 0.6931430895398133, 0.6886431838039064, 0.687338791624473, 0.690942255467394, 0.6864908331067022, 0.6960404874199498, 0.6961532363491337, 0.6950826011870029, 0.6875402851261362, 0.6970768416449972, 0.6975116072780024, 0.695981463376623, 0.6955973677826623, 0.6968676700209179, 0.6510772794267558, 0.6845330182653274, 0.6854629170720594, 0.6949372350299445, 0.6917833840324931, 0.69689637814125, 0.6952424791172473, 0.6994328679394548, 0.698947098350873, 0.6989347512704612, 0.6993338639718772, 0.7004533201238535, 0.6992644785094435, 0.6961216556764868, 0.7003796774975575, 0.6999491546710912, 0.6995525903945422, 0.7002027976686938, 0.7010355663560602, 0.6998363198155034, 0.69890970579029, 0.6953968964789036, 0.6984600096288389, 0.6722561860606618, 0.6866098183349971, 0.6978972454140656, 0.6988519177819691, 0.7002976991399361, 0.7007664187546194, 0.7015407920753869, 0.7007132759929573, 0.6993813573443977, 0.6991575238478445, 0.6999940883069142, 0.7020703053822482, 0.7008287817457296, 0.7008431081789254, 0.700323894293639, 0.6925700285138875, 0.6954068304413427, 0.6998096241133056, 0.6952483691003201, 0.6972573306003627, 0.7002505381612012, 0.699308955538882, 0.7008742779275797, 0.701147792765694, 0.7018432158188228, 0.7014668018278414, 0.7025673258913695, 0.7014758234476521, 0.7019917958409246, 0.7022587369828328, 0.7015663953158107, 0.7017072663254982, 0.7020489662668131, 0.7013041057290822, 0.7026302096617483, 0.7024243617579885, 0.7028689797777329, 0.7010268849613023, 0.7016294750854046, 0.7022854905493938, 0.7017640373567595, 0.7018658240346143, 0.7025848261631318, 0.7019118918989696, 0.7021441368290978, 0.7017979299935111, 0.7021086766336956, 0.7015921601848881, 0.701480553533039, 0.7012114587926517, 0.7016769321295466, 0.7007014496918142, 0.7015198862900699, 0.7018040183686862, 0.7018357008478068, 0.7018000215944582, 0.7015402486724575, 0.7014172436112035, 0.7015960819094721, 0.7015267314702055, 0.7015098522614388, 0.7019315146616776, 0.7021514064204084, 0.7024115704706986, 0.7023345284218335, 0.702371176141892, 0.7024386048752026, 0.701830134122041, 0.7021438938422795, 0.7019446146314161, 0.7019181603932902, 0.7017818428304073, 0.7023255459583588, 0.7020481024345342, 0.702429915214107, 0.7021838563637142, 0.7020943797852871], 'loss': [5.323315363212172, 2.6943443120994006, 1.40581124698, 0.997206357018701, 0.8059053376977667, 0.7220594715602292, 0.6563577678403244, 0.606435882340511, 0.57604611859533, 0.5655007485685677, 0.615657654448683, 0.5210909824359593, 0.4873433013267705, 0.46588309571660796, 0.49946701056851545, 0.4505918630825475, 0.4301757007984105, 0.421727307324339, 0.4073457631277921, 0.39771590297445286, 0.39390705136829995, 0.38565799373711274, 0.3817750042295221, 0.37135396670238136, 0.3769093803290663, 0.3683883577144792, 0.3673513606557705, 0.3654606218995719, 0.44416014328966, 0.3984442685037998, 0.3779756340193631, 0.36371048609611434, 0.3617672149477334, 0.35859189362361515, 0.35521853351827914, 0.3495562070696225, 0.3667912042786922, 0.34082922133906135, 0.33868041076683647, 0.3366309429330779, 0.3325702903305956, 0.3328196908452828, 0.33435755571708303, 0.33045091329537, 0.33134476965284115, 0.32618750609787817, 0.4153320602889131, 0.3485981738332457, 0.3343699250021592, 0.3325901072600792, 0.3687117209869065, 0.3272866292540076, 0.3282241423435399, 0.326924994661303, 0.3195713514764908, 0.3182514077924155, 0.31751716959065407, 0.3174856849785509, 0.32091611065888054, 0.31803018530014115, 0.3161305886477672, 0.3154993844149735, 0.31489779003735247, 0.318406872414603, 0.3142037068681764, 0.3128398454248024, 0.3117520666768398, 0.34165442033941523, 0.31619075733452595, 0.3236060375944147, 0.3519123361028474, 0.3136342276493317, 0.3091229225320769, 0.34483260283329215, 0.3399862805610807, 0.3099831988658811, 0.3121617656623201, 0.3075951371580509, 0.30564537321405455, 0.30958317506489497, 0.30696712823336936, 0.31187712045138694, 0.30476631011281696, 0.3260050577483154, 0.32292959094047546, 0.346522350616643, 0.31025225687496766, 0.3361984827541953, 0.3317955537088986, 0.3062757757203332, 0.3294668924338712, 0.3095585419039421, 0.307553802217756, 0.30687633435714423, 0.3035318697321004, 0.3070226412982189, 0.2996144500272027, 0.30478542162279776, 0.3081009276394774, 0.30360007065857575, 0.3029085292017519, 0.30042551156922515, 0.30273113873204577, 0.3041235957239649, 0.3028270553779132, 0.30789959694951624, 0.301672054482211, 0.3030827167292534, 0.3025391916629716, 0.30255464468096277, 0.30554205531557205, 0.30055963420515575, 0.3388612205759058, 0.30010485502299417, 0.29897860969815937, 0.2987408458892935, 0.3013133245148682, 0.29813779604258794, 0.3205057978630066, 0.3075559535637278, 0.3026948864824079, 0.30108589932249097, 0.30239127598372584, 0.2985650180595849, 0.29997441789199564, 0.29892081800352766, 0.3008405557406947, 0.2980711330921192, 0.2982158138246959, 0.33051683148139804, 0.29942741828599, 0.2996248670399483, 0.30123125802119965, 0.3014451103844666, 0.2967205910847105, 0.29722554607344376, 0.29943894635280366, 0.3168842532658225, 0.2993525263711149, 0.29979905867811496, 0.2982686333762014, 0.3016535890807072, 0.2990547657893796, 0.3005849247789148, 0.3000900779158024, 0.3016303649970463], 'acc': [0.3473931148813423, 0.9391402243980633, 0.9838501943156049, 0.985327037097198, 0.9858032100893593, 0.9862265146424618, 0.9866118478070339, 0.9869431803379153, 0.9871290208670893, 0.9874043582108221, 0.9871381556459249, 0.98780470000112, 0.9881973486815767, 0.9885207402882318, 0.9883881742731103, 0.9888484196122644, 0.9890893762334815, 0.9892787225727965, 0.9895687634721765, 0.9896803765461363, 0.9897797054845124, 0.9899263161743803, 0.989910013276368, 0.9901193474313895, 0.9900538745184837, 0.9902585871113933, 0.990394715311492, 0.9904351918568165, 0.989565074737436, 0.9899783542590775, 0.9904502885682243, 0.9906615559103453, 0.9908052295299586, 0.9909405620227306, 0.990998402017678, 0.9911165037765879, 0.9910822046801374, 0.991330262769032, 0.9914162156029875, 0.9914381413036967, 0.9915045896187205, 0.991562608721221, 0.9915878091539655, 0.991677984521894, 0.9916399265157765, 0.9917512841412587, 0.9908031260438741, 0.9915198868718641, 0.9918261076429208, 0.991819367913777, 0.9916302787846533, 0.9917346456367981, 0.9918391449110848, 0.9919681393454227, 0.9920441185312318, 0.9920906914866029, 0.9921030871973836, 0.9921604623935493, 0.9921303682139354, 0.9921466834439433, 0.9921713069154712, 0.9921944828456258, 0.9922288022017831, 0.9922516254955912, 0.9922528052564912, 0.9922858047955142, 0.9923196423229913, 0.9922922688164735, 0.9923238237503127, 0.9922515118650614, 0.991913207939693, 0.9923970050412446, 0.9923965889832069, 0.9924401825871961, 0.9923898477859685, 0.9924713916379243, 0.992498297996709, 0.9925056875045664, 0.992552263102508, 0.9925287374721959, 0.9925463120338365, 0.9924899680273873, 0.9925799314024413, 0.9925746723936109, 0.9922659364827161, 0.9924405807344784, 0.9926641298632316, 0.9923965405360818, 0.9924926188191757, 0.9925847121060188, 0.9926013018697354, 0.9926291645454068, 0.9926525175277823, 0.9926104284272406, 0.9926200679370335, 0.9926427778939308, 0.9926834916833587, 0.9926849718164341, 0.9926869264377162, 0.9926938103337594, 0.9926542680839012, 0.9927174871778254, 0.9927099182687956, 0.9926922873323187, 0.9927256342225474, 0.9926894210242285, 0.9926644070395108, 0.9927401519174059, 0.9927279705484512, 0.9927307393750534, 0.9927462688807783, 0.9927982006754194, 0.9927662569901039, 0.9927578738757542, 0.9928005537376028, 0.9927827413446211, 0.9927843685807853, 0.99280897355432, 0.9927781644125877, 0.9927622220786334, 0.9927537276827055, 0.9928176778877897, 0.9928158257395175, 0.9928430633004663, 0.9928171267650397, 0.9928303026213434, 0.9928375711581977, 0.9928410106104583, 0.9928573980707253, 0.9928408502945172, 0.9928253378186908, 0.9928299999002166, 0.9928211640254617, 0.9928263320124208, 0.9928677146657935, 0.9928534069084769, 0.9928194052480125, 0.99285878424574, 0.9928434981501161, 0.9928481006269971, 0.9928648524683684, 0.9928386044032468, 0.9928507047333741, 0.9928639293304218, 0.992867409889334, 0.9928535205390066], 'mDice': [0.006322979287065456, 0.08841524860394999, 0.2381243326247032, 0.35023638840966625, 0.4259322985933332, 0.4663252597078314, 0.49839452775241117, 0.5255892383934829, 0.5434924135067193, 0.5548104607119348, 0.521982719745542, 0.5752643208785597, 0.5958173154610131, 0.610052144879778, 0.5905476884888898, 0.6198396559419304, 0.6328528074208152, 0.6399433345630251, 0.6481937780756081, 0.6551089139994729, 0.6587156356849106, 0.6637425772074995, 0.6657775778488573, 0.6729837529764974, 0.6695247738819404, 0.6752733221195014, 0.6763069620860621, 0.6783860886625468, 0.6368057754239426, 0.6547627678058417, 0.6688194412903246, 0.6787492210054632, 0.6801662829709171, 0.6832859110949662, 0.6853321905206577, 0.6893063874667501, 0.6798895911630152, 0.6950955006289364, 0.6973324661771653, 0.6986610102536056, 0.7014360398494551, 0.7010554536222824, 0.7005215081675299, 0.7029915540089161, 0.7026571428834512, 0.7061091268003867, 0.6581859788283926, 0.6896711564416369, 0.7002249110508435, 0.70224335921809, 0.6971586387732933, 0.705528165612902, 0.7052724188184504, 0.7061759110154777, 0.7109325807082829, 0.7121249921803404, 0.7127724580576854, 0.7127380529647978, 0.7110053121749991, 0.712390710567606, 0.7138163861382771, 0.7144131114330198, 0.7150755051908821, 0.7128483541493346, 0.7152916665734916, 0.7162928868984354, 0.7176789970233522, 0.7140226032346341, 0.7140122731330947, 0.7107317700174641, 0.7005271814726843, 0.7156212435567321, 0.7185997167244333, 0.71451883656638, 0.7161743963880493, 0.7184104393855691, 0.7172762801494504, 0.7202412905951439, 0.7213779262134007, 0.7188377142539752, 0.7207766304462414, 0.7181407612532817, 0.7221966215542385, 0.7178010626379492, 0.7104660075873577, 0.7084949653724144, 0.7180708174047798, 0.7036152972376405, 0.71476291905483, 0.7212996212719697, 0.7188598704455521, 0.719073065102394, 0.7204093865573112, 0.721687391473742, 0.7230148597303869, 0.7211734297240309, 0.7259269336174274, 0.7225842722531023, 0.7215877926995602, 0.7232315560867046, 0.7236822696742166, 0.7256651039194004, 0.7239223005149165, 0.7227472234242068, 0.7238405608191278, 0.7222755316443044, 0.724286075589692, 0.723633470793663, 0.7240628018755043, 0.7238935511100468, 0.7231113802036041, 0.7252996094121135, 0.7219150635996475, 0.7257096782106484, 0.7265683174720539, 0.7266855994468839, 0.7250252827047714, 0.7273119999270133, 0.7242911836783874, 0.7227242562571182, 0.7238720543866087, 0.7250759898148147, 0.7249726244968734, 0.7267931426687194, 0.7259737755277474, 0.7267763059714745, 0.7256138724059307, 0.727828972445333, 0.7273766853539227, 0.724656099462744, 0.7266559386488252, 0.7261002480690115, 0.7251050492812847, 0.725241755323457, 0.7281698458300435, 0.7278940642408549, 0.7263013993578004, 0.7250075352015753, 0.7270806704835938, 0.7263397515700956, 0.7272366766859157, 0.7251581226663636, 0.726385552601274, 0.7258601711301381, 0.726085711288922, 0.7248529662639637], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 3.90625e-06, 3.90625e-06]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:05<00:51,  5.20s/it]predicting test subjects:  18%|█▊        | 2/11 [00:09<00:44,  4.89s/it]predicting test subjects:  27%|██▋       | 3/11 [00:13<00:38,  4.75s/it]predicting test subjects:  36%|███▋      | 4/11 [00:17<00:31,  4.49s/it]predicting test subjects:  45%|████▌     | 5/11 [00:21<00:25,  4.31s/it]predicting test subjects:  55%|█████▍    | 6/11 [00:26<00:22,  4.46s/it]predicting test subjects:  64%|██████▎   | 7/11 [00:30<00:17,  4.40s/it]predicting test subjects:  73%|███████▎  | 8/11 [00:34<00:13,  4.36s/it]predicting test subjects:  82%|████████▏ | 9/11 [00:38<00:08,  4.27s/it]predicting test subjects:  91%|█████████ | 10/11 [00:43<00:04,  4.37s/it]predicting test subjects: 100%|██████████| 11/11 [00:49<00:00,  4.73s/it]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:04<03:14,  4.86s/it]predicting train subjects:   5%|▍         | 2/41 [00:09<03:07,  4.80s/it]predicting train subjects:   7%|▋         | 3/41 [00:13<02:51,  4.52s/it]predicting train subjects:  10%|▉         | 4/41 [00:16<02:36,  4.22s/it]predicting train subjects:  12%|█▏        | 5/41 [00:22<02:44,  4.57s/it]predicting train subjects:  15%|█▍        | 6/41 [00:26<02:36,  4.47s/it]predicting train subjects:  17%|█▋        | 7/41 [00:30<02:23,  4.23s/it]predicting train subjects:  20%|█▉        | 8/41 [00:32<02:03,  3.74s/it]predicting train subjects:  22%|██▏       | 9/41 [00:37<02:09,  4.04s/it]predicting train subjects:  24%|██▍       | 10/41 [00:43<02:18,  4.48s/it]predicting train subjects:  27%|██▋       | 11/41 [00:48<02:22,  4.75s/it]predicting train subjects:  29%|██▉       | 12/41 [00:50<01:58,  4.07s/it]predicting train subjects:  32%|███▏      | 13/41 [00:55<01:56,  4.17s/it]predicting train subjects:  34%|███▍      | 14/41 [01:00<01:57,  4.34s/it]predicting train subjects:  37%|███▋      | 15/41 [01:02<01:41,  3.89s/it]predicting train subjects:  39%|███▉      | 16/41 [01:07<01:41,  4.05s/it]predicting train subjects:  41%|████▏     | 17/41 [01:13<01:52,  4.67s/it]predicting train subjects:  44%|████▍     | 18/41 [01:18<01:49,  4.75s/it]predicting train subjects:  46%|████▋     | 19/41 [01:22<01:42,  4.65s/it]predicting train subjects:  49%|████▉     | 20/41 [01:26<01:32,  4.38s/it]predicting train subjects:  51%|█████     | 21/41 [01:30<01:26,  4.35s/it]predicting train subjects:  54%|█████▎    | 22/41 [01:35<01:24,  4.45s/it]predicting train subjects:  56%|█████▌    | 23/41 [01:38<01:10,  3.89s/it]predicting train subjects:  59%|█████▊    | 24/41 [01:42<01:10,  4.15s/it]predicting train subjects:  61%|██████    | 25/41 [01:47<01:06,  4.16s/it]predicting train subjects:  63%|██████▎   | 26/41 [01:52<01:06,  4.45s/it]predicting train subjects:  66%|██████▌   | 27/41 [01:57<01:04,  4.60s/it]predicting train subjects:  68%|██████▊   | 28/41 [02:01<01:00,  4.62s/it]predicting train subjects:  71%|███████   | 29/41 [02:06<00:54,  4.51s/it]predicting train subjects:  73%|███████▎  | 30/41 [02:11<00:53,  4.84s/it]predicting train subjects:  76%|███████▌  | 31/41 [02:16<00:49,  4.92s/it]predicting train subjects:  78%|███████▊  | 32/41 [02:21<00:43,  4.80s/it]predicting train subjects:  80%|████████  | 33/41 [02:26<00:39,  4.92s/it]predicting train subjects:  83%|████████▎ | 34/41 [02:33<00:38,  5.43s/it]predicting train subjects:  85%|████████▌ | 35/41 [02:40<00:35,  5.98s/it]predicting train subjects:  88%|████████▊ | 36/41 [02:47<00:30,  6.20s/it]predicting train subjects:  90%|█████████ | 37/41 [02:52<00:24,  6.00s/it]predicting train subjects:  93%|█████████▎| 38/41 [02:57<00:16,  5.55s/it]predicting train subjects:  95%|█████████▌| 39/41 [03:01<00:10,  5.33s/it]predicting train subjects:  98%|█████████▊| 40/41 [03:07<00:05,  5.32s/it]predicting train subjects: 100%|██████████| 41/41 [03:12<00:00,  5.38s/it]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:04<03:02,  4.56s/it]Loading train:   5%|▍         | 2/41 [00:09<02:56,  4.53s/it]Loading train:   7%|▋         | 3/41 [00:13<02:52,  4.54s/it]Loading train:  10%|▉         | 4/41 [00:17<02:38,  4.28s/it]Loading train:  12%|█▏        | 5/41 [00:22<02:41,  4.48s/it]Loading train:  15%|█▍        | 6/41 [00:26<02:33,  4.37s/it]Loading train:  17%|█▋        | 7/41 [00:29<02:21,  4.15s/it]Loading train:  20%|█▉        | 8/41 [00:33<02:13,  4.03s/it]Loading train:  22%|██▏       | 9/41 [00:38<02:15,  4.24s/it]Loading train:  24%|██▍       | 10/41 [00:43<02:16,  4.40s/it]Loading train:  27%|██▋       | 11/41 [00:48<02:17,  4.59s/it]Loading train:  29%|██▉       | 12/41 [00:51<02:03,  4.27s/it]Loading train:  32%|███▏      | 13/41 [00:56<02:05,  4.49s/it]Loading train:  34%|███▍      | 14/41 [01:01<02:01,  4.49s/it]Loading train:  37%|███▋      | 15/41 [01:05<01:52,  4.32s/it]Loading train:  39%|███▉      | 16/41 [01:09<01:50,  4.40s/it]Loading train:  41%|████▏     | 17/41 [01:15<01:55,  4.83s/it]Loading train:  44%|████▍     | 18/41 [01:20<01:50,  4.79s/it]Loading train:  46%|████▋     | 19/41 [01:25<01:45,  4.81s/it]Loading train:  49%|████▉     | 20/41 [01:29<01:37,  4.62s/it]Loading train:  51%|█████     | 21/41 [01:33<01:31,  4.57s/it]Loading train:  54%|█████▎    | 22/41 [01:38<01:26,  4.55s/it]Loading train:  56%|█████▌    | 23/41 [01:42<01:18,  4.36s/it]Loading train:  59%|█████▊    | 24/41 [01:46<01:15,  4.42s/it]Loading train:  61%|██████    | 25/41 [01:51<01:13,  4.61s/it]Loading train:  63%|██████▎   | 26/41 [01:54<01:01,  4.12s/it]Loading train:  66%|██████▌   | 27/41 [01:59<01:01,  4.42s/it]Loading train:  68%|██████▊   | 28/41 [02:03<00:52,  4.06s/it]Loading train:  71%|███████   | 29/41 [02:06<00:45,  3.79s/it]Loading train:  73%|███████▎  | 30/41 [02:09<00:41,  3.73s/it]Loading train:  76%|███████▌  | 31/41 [02:12<00:34,  3.47s/it]Loading train:  78%|███████▊  | 32/41 [02:16<00:30,  3.44s/it]Loading train:  80%|████████  | 33/41 [02:19<00:26,  3.29s/it]Loading train:  83%|████████▎ | 34/41 [02:22<00:22,  3.22s/it]Loading train:  85%|████████▌ | 35/41 [02:26<00:20,  3.43s/it]Loading train:  88%|████████▊ | 36/41 [02:30<00:18,  3.67s/it]Loading train:  90%|█████████ | 37/41 [02:34<00:15,  3.79s/it]Loading train:  93%|█████████▎| 38/41 [02:37<00:11,  3.72s/it]Loading train:  95%|█████████▌| 39/41 [02:40<00:07,  3.51s/it]Loading train:  98%|█████████▊| 40/41 [02:44<00:03,  3.58s/it]Loading train: 100%|██████████| 41/41 [02:48<00:00,  3.58s/it]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   5%|▍         | 2/41 [00:00<00:02, 17.22it/s]concatenating: train:  10%|▉         | 4/41 [00:00<00:02, 16.40it/s]concatenating: train:  15%|█▍        | 6/41 [00:00<00:02, 16.72it/s]concatenating: train:  32%|███▏      | 13/41 [00:00<00:01, 21.65it/s]concatenating: train:  49%|████▉     | 20/41 [00:00<00:00, 24.72it/s]concatenating: train:  59%|█████▊    | 24/41 [00:00<00:00, 25.28it/s]concatenating: train:  71%|███████   | 29/41 [00:00<00:00, 28.96it/s]concatenating: train:  80%|████████  | 33/41 [00:01<00:00, 25.56it/s]concatenating: train:  93%|█████████▎| 38/41 [00:01<00:00, 29.06it/s]concatenating: train: 100%|██████████| 41/41 [00:01<00:00, 29.95it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:03<00:30,  3.08s/it]Loading test:  18%|█▊        | 2/11 [00:06<00:28,  3.17s/it]Loading test:  27%|██▋       | 3/11 [00:10<00:26,  3.29s/it]Loading test:  36%|███▋      | 4/11 [00:13<00:22,  3.26s/it]Loading test:  45%|████▌     | 5/11 [00:16<00:20,  3.39s/it]Loading test:  55%|█████▍    | 6/11 [00:19<00:16,  3.22s/it]Loading test:  64%|██████▎   | 7/11 [00:23<00:13,  3.35s/it]Loading test:  73%|███████▎  | 8/11 [00:26<00:10,  3.38s/it]Loading test:  82%|████████▏ | 9/11 [00:30<00:06,  3.35s/it]Loading test:  91%|█████████ | 10/11 [00:34<00:03,  3.51s/it]Loading test: 100%|██████████| 11/11 [00:37<00:00,  3.35s/it]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  18%|█▊        | 2/11 [00:00<00:00, 16.39it/s]concatenating: validation:  45%|████▌     | 5/11 [00:00<00:00, 18.62it/s]concatenating: validation:  64%|██████▎   | 7/11 [00:00<00:00, 17.45it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 26.19it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 128, 92, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 128, 92, 20)  200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 128, 92, 20)  80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 128, 92, 20)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 128, 92, 20)  3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 128, 92, 20)  80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 128, 92, 20)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 64, 46, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 64, 46, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 64, 46, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 64, 46, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 64, 46, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 64, 46, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 64, 46, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 64, 46, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 64, 46, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 32, 23, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 32, 23, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 23, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 23, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 23, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 23, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 23, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 23, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 32, 23, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 32, 23, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 64, 46, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 64, 46, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 64, 46, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 64, 46, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 64, 46, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 64, 46, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 64, 46, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 64, 46, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 64, 46, 140)  0           concatenate_3[0][0]              2019-07-28 22:03:40.425141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-28 22:03:40.425242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 22:03:40.425257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-28 22:03:40.425266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-28 22:03:40.425639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 64, 46, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 128, 92, 20)  11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 128, 92, 40)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 128, 92, 20)  7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 128, 92, 20)  80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 128, 92, 20)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 128, 92, 20)  3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 128, 92, 20)  80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 128, 92, 20)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 128, 92, 60)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 128, 92, 60)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 128, 92, 13)  793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.55074706e-02 3.08831303e-02 7.68491765e-02 1.00912303e-02
 2.67216464e-02 7.01279323e-03 8.03427082e-02 1.16957612e-01
 7.80704504e-02 1.36444858e-02 3.13209806e-01 1.80678441e-01
 3.10497652e-05]
Train on 4019 samples, validate on 1058 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 22s - loss: 5.5022 - acc: 0.5447 - mDice: 0.0072 - val_loss: 5.4345 - val_acc: 0.8997 - val_mDice: 0.0129

Epoch 00001: val_mDice improved from -inf to 0.01289, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 3.2490 - acc: 0.9683 - mDice: 0.0745 - val_loss: 2.8470 - val_acc: 0.9805 - val_mDice: 0.1454

Epoch 00002: val_mDice improved from 0.01289 to 0.14535, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 2.0206 - acc: 0.9829 - mDice: 0.1476 - val_loss: 2.3215 - val_acc: 0.9831 - val_mDice: 0.2282

Epoch 00003: val_mDice improved from 0.14535 to 0.22824, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 1.5903 - acc: 0.9844 - mDice: 0.1984 - val_loss: 2.0425 - val_acc: 0.9850 - val_mDice: 0.2860

Epoch 00004: val_mDice improved from 0.22824 to 0.28598, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 1.3551 - acc: 0.9850 - mDice: 0.2506 - val_loss: 1.9215 - val_acc: 0.9855 - val_mDice: 0.3230

Epoch 00005: val_mDice improved from 0.28598 to 0.32301, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 1.2347 - acc: 0.9852 - mDice: 0.2785 - val_loss: 1.8736 - val_acc: 0.9847 - val_mDice: 0.3361

Epoch 00006: val_mDice improved from 0.32301 to 0.33615, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 1.2218 - acc: 0.9853 - mDice: 0.2953 - val_loss: 1.7996 - val_acc: 0.9853 - val_mDice: 0.3630

Epoch 00007: val_mDice improved from 0.33615 to 0.36301, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 1.1159 - acc: 0.9854 - mDice: 0.3144 - val_loss: 1.7548 - val_acc: 0.9855 - val_mDice: 0.3853

Epoch 00008: val_mDice improved from 0.36301 to 0.38535, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 1.0732 - acc: 0.9855 - mDice: 0.3270 - val_loss: 1.7528 - val_acc: 0.9856 - val_mDice: 0.3773

Epoch 00009: val_mDice did not improve from 0.38535
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 1.1310 - acc: 0.9855 - mDice: 0.3166 - val_loss: 1.7275 - val_acc: 0.9847 - val_mDice: 0.3791

Epoch 00010: val_mDice did not improve from 0.38535
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 1.1167 - acc: 0.9854 - mDice: 0.3219 - val_loss: 1.7320 - val_acc: 0.9854 - val_mDice: 0.3917

Epoch 00011: val_mDice improved from 0.38535 to 0.39174, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 1.0911 - acc: 0.9853 - mDice: 0.3485 - val_loss: 1.9026 - val_acc: 0.9850 - val_mDice: 0.3340

Epoch 00012: val_mDice did not improve from 0.39174
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 1.0583 - acc: 0.9854 - mDice: 0.3329 - val_loss: 1.6978 - val_acc: 0.9845 - val_mDice: 0.4040

Epoch 00013: val_mDice improved from 0.39174 to 0.40400, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.9320 - acc: 0.9854 - mDice: 0.3801 - val_loss: 1.6909 - val_acc: 0.9853 - val_mDice: 0.4067

Epoch 00014: val_mDice improved from 0.40400 to 0.40672, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.9229 - acc: 0.9855 - mDice: 0.3881 - val_loss: 1.6624 - val_acc: 0.9855 - val_mDice: 0.4153

Epoch 00015: val_mDice improved from 0.40672 to 0.41532, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.9804 - acc: 0.9854 - mDice: 0.3725 - val_loss: 1.6601 - val_acc: 0.9852 - val_mDice: 0.3954

Epoch 00016: val_mDice did not improve from 0.41532
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 1.1467 - acc: 0.9853 - mDice: 0.3154 - val_loss: 1.7560 - val_acc: 0.9853 - val_mDice: 0.3640

Epoch 00017: val_mDice did not improve from 0.41532
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.9377 - acc: 0.9855 - mDice: 0.3776 - val_loss: 1.6389 - val_acc: 0.9853 - val_mDice: 0.4215

Epoch 00018: val_mDice improved from 0.41532 to 0.42148, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.9354 - acc: 0.9855 - mDice: 0.3931 - val_loss: 1.6625 - val_acc: 0.9851 - val_mDice: 0.4163

Epoch 00019: val_mDice did not improve from 0.42148
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.9134 - acc: 0.9856 - mDice: 0.3938 - val_loss: 1.6435 - val_acc: 0.9850 - val_mDice: 0.4219

Epoch 00020: val_mDice improved from 0.42148 to 0.42186, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.8946 - acc: 0.9855 - mDice: 0.3926 - val_loss: 1.6235 - val_acc: 0.9849 - val_mDice: 0.4282

Epoch 00021: val_mDice improved from 0.42186 to 0.42824, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.8672 - acc: 0.9856 - mDice: 0.4089 - val_loss: 1.6219 - val_acc: 0.9852 - val_mDice: 0.4330

Epoch 00022: val_mDice improved from 0.42824 to 0.43297, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.8942 - acc: 0.9856 - mDice: 0.4101 - val_loss: 1.6216 - val_acc: 0.9851 - val_mDice: 0.4345

Epoch 00023: val_mDice improved from 0.43297 to 0.43450, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.8996 - acc: 0.9857 - mDice: 0.4167 - val_loss: 1.6061 - val_acc: 0.9847 - val_mDice: 0.4323

Epoch 00024: val_mDice did not improve from 0.43450
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.9619 - acc: 0.9853 - mDice: 0.3817 - val_loss: 1.6049 - val_acc: 0.9853 - val_mDice: 0.4212

Epoch 00025: val_mDice did not improve from 0.43450
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.8509 - acc: 0.9857 - mDice: 0.4131 - val_loss: 1.5821 - val_acc: 0.9852 - val_mDice: 0.4357

Epoch 00026: val_mDice improved from 0.43450 to 0.43571, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.8693 - acc: 0.9858 - mDice: 0.4163 - val_loss: 1.6021 - val_acc: 0.9852 - val_mDice: 0.4267

Epoch 00027: val_mDice did not improve from 0.43571
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.8613 - acc: 0.9857 - mDice: 0.4141 - val_loss: 1.6229 - val_acc: 0.9849 - val_mDice: 0.4299

Epoch 00028: val_mDice did not improve from 0.43571
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.8767 - acc: 0.9858 - mDice: 0.4070 - val_loss: 1.5955 - val_acc: 0.9850 - val_mDice: 0.4327

Epoch 00029: val_mDice did not improve from 0.43571
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.8497 - acc: 0.9857 - mDice: 0.4105 - val_loss: 1.5885 - val_acc: 0.9851 - val_mDice: 0.4376

Epoch 00030: val_mDice improved from 0.43571 to 0.43758, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.8082 - acc: 0.9859 - mDice: 0.4285 - val_loss: 1.5786 - val_acc: 0.9852 - val_mDice: 0.4434

Epoch 00031: val_mDice improved from 0.43758 to 0.44340, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.8385 - acc: 0.9859 - mDice: 0.4293 - val_loss: 1.5873 - val_acc: 0.9851 - val_mDice: 0.4431

Epoch 00032: val_mDice did not improve from 0.44340
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.8507 - acc: 0.9859 - mDice: 0.4302 - val_loss: 1.5602 - val_acc: 0.9850 - val_mDice: 0.4426

Epoch 00033: val_mDice did not improve from 0.44340
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.8103 - acc: 0.9860 - mDice: 0.4369 - val_loss: 1.5888 - val_acc: 0.9849 - val_mDice: 0.4392

Epoch 00034: val_mDice did not improve from 0.44340
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.8838 - acc: 0.9856 - mDice: 0.4034 - val_loss: 1.6211 - val_acc: 0.9847 - val_mDice: 0.4298

Epoch 00035: val_mDice did not improve from 0.44340
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.8127 - acc: 0.9859 - mDice: 0.4307 - val_loss: 1.5737 - val_acc: 0.9847 - val_mDice: 0.4403

Epoch 00036: val_mDice did not improve from 0.44340
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.7976 - acc: 0.9860 - mDice: 0.4391 - val_loss: 1.5688 - val_acc: 0.9849 - val_mDice: 0.4451

Epoch 00037: val_mDice improved from 0.44340 to 0.44513, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.8418 - acc: 0.9858 - mDice: 0.4230 - val_loss: 1.5755 - val_acc: 0.9851 - val_mDice: 0.4434

Epoch 00038: val_mDice did not improve from 0.44513
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.8179 - acc: 0.9858 - mDice: 0.4294 - val_loss: 1.5582 - val_acc: 0.9851 - val_mDice: 0.4444

Epoch 00039: val_mDice did not improve from 0.44513
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.8127 - acc: 0.9860 - mDice: 0.4314 - val_loss: 1.5566 - val_acc: 0.9851 - val_mDice: 0.4462

Epoch 00040: val_mDice improved from 0.44513 to 0.44620, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.7696 - acc: 0.9861 - mDice: 0.4468 - val_loss: 1.5503 - val_acc: 0.9851 - val_mDice: 0.4468

Epoch 00041: val_mDice improved from 0.44620 to 0.44679, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.7908 - acc: 0.9861 - mDice: 0.4486 - val_loss: 1.5471 - val_acc: 0.9850 - val_mDice: 0.4476

Epoch 00042: val_mDice improved from 0.44679 to 0.44757, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.8469 - acc: 0.9858 - mDice: 0.4310 - val_loss: 1.5611 - val_acc: 0.9853 - val_mDice: 0.4463

Epoch 00043: val_mDice did not improve from 0.44757
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.7770 - acc: 0.9860 - mDice: 0.4479 - val_loss: 1.5485 - val_acc: 0.9853 - val_mDice: 0.4508

Epoch 00044: val_mDice improved from 0.44757 to 0.45081, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.7828 - acc: 0.9862 - mDice: 0.4512 - val_loss: 1.5617 - val_acc: 0.9852 - val_mDice: 0.4489

Epoch 00045: val_mDice did not improve from 0.45081
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.7671 - acc: 0.9862 - mDice: 0.4505 - val_loss: 1.5781 - val_acc: 0.9852 - val_mDice: 0.4509

Epoch 00046: val_mDice improved from 0.45081 to 0.45086, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.7873 - acc: 0.9861 - mDice: 0.4424 - val_loss: 1.5607 - val_acc: 0.9854 - val_mDice: 0.4555

Epoch 00047: val_mDice improved from 0.45086 to 0.45545, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.8094 - acc: 0.9863 - mDice: 0.4524 - val_loss: 1.5551 - val_acc: 0.9852 - val_mDice: 0.4538

Epoch 00048: val_mDice did not improve from 0.45545
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.7628 - acc: 0.9863 - mDice: 0.4555 - val_loss: 1.6016 - val_acc: 0.9853 - val_mDice: 0.4316

Epoch 00049: val_mDice did not improve from 0.45545
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.7815 - acc: 0.9861 - mDice: 0.4483 - val_loss: 1.5495 - val_acc: 0.9853 - val_mDice: 0.4537

Epoch 00050: val_mDice did not improve from 0.45545
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.7899 - acc: 0.9861 - mDice: 0.4462 - val_loss: 1.5488 - val_acc: 0.9854 - val_mDice: 0.4497

Epoch 00051: val_mDice did not improve from 0.45545
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.7752 - acc: 0.9862 - mDice: 0.4466 - val_loss: 1.5488 - val_acc: 0.9852 - val_mDice: 0.4521

Epoch 00052: val_mDice did not improve from 0.45545
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.7854 - acc: 0.9861 - mDice: 0.4440 - val_loss: 1.5455 - val_acc: 0.9852 - val_mDice: 0.4523

Epoch 00053: val_mDice did not improve from 0.45545
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.7554 - acc: 0.9861 - mDice: 0.4544 - val_loss: 1.5653 - val_acc: 0.9852 - val_mDice: 0.4547

Epoch 00054: val_mDice did not improve from 0.45545
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7457 - acc: 0.9863 - mDice: 0.4590 - val_loss: 1.5578 - val_acc: 0.9853 - val_mDice: 0.4537

Epoch 00055: val_mDice did not improve from 0.45545
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7508 - acc: 0.9863 - mDice: 0.4540 - val_loss: 1.5472 - val_acc: 0.9851 - val_mDice: 0.4537

Epoch 00056: val_mDice did not improve from 0.45545
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7357 - acc: 0.9864 - mDice: 0.4654 - val_loss: 1.5473 - val_acc: 0.9851 - val_mDice: 0.4542

Epoch 00057: val_mDice did not improve from 0.45545
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7629 - acc: 0.9863 - mDice: 0.4632 - val_loss: 1.5447 - val_acc: 0.9851 - val_mDice: 0.4543

Epoch 00058: val_mDice did not improve from 0.45545
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7431 - acc: 0.9862 - mDice: 0.4567 - val_loss: 1.5495 - val_acc: 0.9852 - val_mDice: 0.4526

Epoch 00059: val_mDice did not improve from 0.45545
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7313 - acc: 0.9864 - mDice: 0.4653 - val_loss: 1.5432 - val_acc: 0.9852 - val_mDice: 0.4559

Epoch 00060: val_mDice improved from 0.45545 to 0.45591, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7299 - acc: 0.9864 - mDice: 0.4675 - val_loss: 1.5419 - val_acc: 0.9852 - val_mDice: 0.4566

Epoch 00061: val_mDice improved from 0.45591 to 0.45656, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.8314 - acc: 0.9864 - mDice: 0.4664 - val_loss: 1.5624 - val_acc: 0.9848 - val_mDice: 0.4424

Epoch 00062: val_mDice did not improve from 0.45656
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.8087 - acc: 0.9863 - mDice: 0.4569 - val_loss: 1.5469 - val_acc: 0.9852 - val_mDice: 0.4519

Epoch 00063: val_mDice did not improve from 0.45656
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7294 - acc: 0.9864 - mDice: 0.4634 - val_loss: 1.5419 - val_acc: 0.9853 - val_mDice: 0.4555

Epoch 00064: val_mDice did not improve from 0.45656
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7244 - acc: 0.9864 - mDice: 0.4674 - val_loss: 1.5421 - val_acc: 0.9852 - val_mDice: 0.4562

Epoch 00065: val_mDice did not improve from 0.45656
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7143 - acc: 0.9864 - mDice: 0.4721 - val_loss: 1.5398 - val_acc: 0.9852 - val_mDice: 0.4571

Epoch 00066: val_mDice improved from 0.45656 to 0.45707, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7349 - acc: 0.9864 - mDice: 0.4672 - val_loss: 1.5409 - val_acc: 0.9852 - val_mDice: 0.4556

Epoch 00067: val_mDice did not improve from 0.45707
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.7152 - acc: 0.9864 - mDice: 0.4705 - val_loss: 1.5321 - val_acc: 0.9852 - val_mDice: 0.4591

Epoch 00068: val_mDice improved from 0.45707 to 0.45911, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.7751 - acc: 0.9861 - mDice: 0.4504 - val_loss: 1.5379 - val_acc: 0.9854 - val_mDice: 0.4554

Epoch 00069: val_mDice did not improve from 0.45911
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.7447 - acc: 0.9864 - mDice: 0.4663 - val_loss: 1.5447 - val_acc: 0.9855 - val_mDice: 0.4541

Epoch 00070: val_mDice did not improve from 0.45911
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.7511 - acc: 0.9864 - mDice: 0.4610 - val_loss: 1.5492 - val_acc: 0.9851 - val_mDice: 0.4554

Epoch 00071: val_mDice did not improve from 0.45911
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.7136 - acc: 0.9864 - mDice: 0.4707 - val_loss: 1.5404 - val_acc: 0.9852 - val_mDice: 0.4582

Epoch 00072: val_mDice did not improve from 0.45911
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.7103 - acc: 0.9864 - mDice: 0.4737 - val_loss: 1.5385 - val_acc: 0.9852 - val_mDice: 0.4583

Epoch 00073: val_mDice did not improve from 0.45911
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.7125 - acc: 0.9865 - mDice: 0.4755 - val_loss: 1.5382 - val_acc: 0.9852 - val_mDice: 0.4588

Epoch 00074: val_mDice did not improve from 0.45911
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.7057 - acc: 0.9865 - mDice: 0.4762 - val_loss: 1.5388 - val_acc: 0.9851 - val_mDice: 0.4587

Epoch 00075: val_mDice did not improve from 0.45911
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.7431 - acc: 0.9864 - mDice: 0.4744 - val_loss: 1.5363 - val_acc: 0.9852 - val_mDice: 0.4595

Epoch 00076: val_mDice improved from 0.45911 to 0.45953, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.7127 - acc: 0.9864 - mDice: 0.4738 - val_loss: 1.5320 - val_acc: 0.9853 - val_mDice: 0.4585

Epoch 00077: val_mDice did not improve from 0.45953
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.7409 - acc: 0.9865 - mDice: 0.4728 - val_loss: 1.5322 - val_acc: 0.9852 - val_mDice: 0.4592

Epoch 00078: val_mDice did not improve from 0.45953
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.7353 - acc: 0.9865 - mDice: 0.4756 - val_loss: 1.5412 - val_acc: 0.9852 - val_mDice: 0.4576

Epoch 00079: val_mDice did not improve from 0.45953
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.7077 - acc: 0.9865 - mDice: 0.4751 - val_loss: 1.5396 - val_acc: 0.9851 - val_mDice: 0.4576

Epoch 00080: val_mDice did not improve from 0.45953
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.7571 - acc: 0.9865 - mDice: 0.4750 - val_loss: 1.5355 - val_acc: 0.9852 - val_mDice: 0.4587

Epoch 00081: val_mDice did not improve from 0.45953
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.7089 - acc: 0.9865 - mDice: 0.4774 - val_loss: 1.5349 - val_acc: 0.9851 - val_mDice: 0.4579

Epoch 00082: val_mDice did not improve from 0.45953
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7672 - acc: 0.9865 - mDice: 0.4796 - val_loss: 1.5401 - val_acc: 0.9852 - val_mDice: 0.4576

Epoch 00083: val_mDice did not improve from 0.45953
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7091 - acc: 0.9864 - mDice: 0.4808 - val_loss: 1.5404 - val_acc: 0.9851 - val_mDice: 0.4584

Epoch 00084: val_mDice did not improve from 0.45953
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.6996 - acc: 0.9865 - mDice: 0.4800 - val_loss: 1.5452 - val_acc: 0.9851 - val_mDice: 0.4585

Epoch 00085: val_mDice did not improve from 0.45953
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7173 - acc: 0.9865 - mDice: 0.4808 - val_loss: 1.5472 - val_acc: 0.9852 - val_mDice: 0.4579

Epoch 00086: val_mDice did not improve from 0.45953
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.6981 - acc: 0.9866 - mDice: 0.4791 - val_loss: 1.5363 - val_acc: 0.9851 - val_mDice: 0.4598

Epoch 00087: val_mDice improved from 0.45953 to 0.45980, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 88/300

Epoch 00088: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7025 - acc: 0.9865 - mDice: 0.4814 - val_loss: 1.5356 - val_acc: 0.9852 - val_mDice: 0.4600

Epoch 00088: val_mDice improved from 0.45980 to 0.46004, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 89/300

Epoch 00089: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7158 - acc: 0.9866 - mDice: 0.4805 - val_loss: 1.5461 - val_acc: 0.9852 - val_mDice: 0.4586

Epoch 00089: val_mDice did not improve from 0.46004
Epoch 90/300

Epoch 00090: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7033 - acc: 0.9865 - mDice: 0.4783 - val_loss: 1.5435 - val_acc: 0.9852 - val_mDice: 0.4591

Epoch 00090: val_mDice did not improve from 0.46004
Epoch 91/300

Epoch 00091: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.7456 - acc: 0.9865 - mDice: 0.4808 - val_loss: 1.5439 - val_acc: 0.9852 - val_mDice: 0.4592

Epoch 00091: val_mDice did not improve from 0.46004
Epoch 92/300

Epoch 00092: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.7085 - acc: 0.9865 - mDice: 0.4794 - val_loss: 1.5406 - val_acc: 0.9852 - val_mDice: 0.4595

Epoch 00092: val_mDice did not improve from 0.46004
Epoch 93/300

Epoch 00093: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.6911 - acc: 0.9866 - mDice: 0.4830 - val_loss: 1.5424 - val_acc: 0.9852 - val_mDice: 0.4585

Epoch 00093: val_mDice did not improve from 0.46004
Epoch 94/300

Epoch 00094: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.6862 - acc: 0.9866 - mDice: 0.4851 - val_loss: 1.5420 - val_acc: 0.9852 - val_mDice: 0.4588

Epoch 00094: val_mDice did not improve from 0.46004
Epoch 95/300

Epoch 00095: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.7043 - acc: 0.9865 - mDice: 0.4790 - val_loss: 1.5431 - val_acc: 0.9852 - val_mDice: 0.4587

Epoch 00095: val_mDice did not improve from 0.46004
Epoch 96/300

Epoch 00096: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.7067 - acc: 0.9865 - mDice: 0.4794 - val_loss: 1.5395 - val_acc: 0.9852 - val_mDice: 0.4597

Epoch 00096: val_mDice did not improve from 0.46004
Epoch 97/300

Epoch 00097: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.7204 - acc: 0.9865 - mDice: 0.4777 - val_loss: 1.5321 - val_acc: 0.9852 - val_mDice: 0.4592

Epoch 00097: val_mDice did not improve from 0.46004
Epoch 98/300

Epoch 00098: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.6977 - acc: 0.9866 - mDice: 0.4834 - val_loss: 1.5353 - val_acc: 0.9852 - val_mDice: 0.4592

Epoch 00098: val_mDice did not improve from 0.46004
Epoch 99/300

Epoch 00099: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.7377 - acc: 0.9865 - mDice: 0.4805 - val_loss: 1.5393 - val_acc: 0.9852 - val_mDice: 0.4591

Epoch 00099: val_mDice did not improve from 0.46004
Epoch 100/300

Epoch 00100: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.7365 - acc: 0.9865 - mDice: 0.4788 - val_loss: 1.5404 - val_acc: 0.9852 - val_mDice: 0.4585

Epoch 00100: val_mDice did not improve from 0.46004
Epoch 101/300

Epoch 00101: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.6919 - acc: 0.9865 - mDice: 0.4833 - val_loss: 1.5409 - val_acc: 0.9852 - val_mDice: 0.4590

Epoch 00101: val_mDice did not improve from 0.46004
Epoch 102/300

Epoch 00102: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.6840 - acc: 0.9866 - mDice: 0.4858 - val_loss: 1.5397 - val_acc: 0.9852 - val_mDice: 0.4595

Epoch 00102: val_mDice did not improve from 0.46004
Epoch 103/300

Epoch 00103: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.6884 - acc: 0.9865 - mDice: 0.4838 - val_loss: 1.5396 - val_acc: 0.9852 - val_mDice: 0.4593

Epoch 00103: val_mDice did not improve from 0.46004
Epoch 104/300

Epoch 00104: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.6896 - acc: 0.9866 - mDice: 0.4837 - val_loss: 1.5419 - val_acc: 0.9852 - val_mDice: 0.4583

Epoch 00104: val_mDice did not improve from 0.46004
Epoch 105/300

Epoch 00105: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.7036 - acc: 0.9866 - mDice: 0.4824 - val_loss: 1.5413 - val_acc: 0.9853 - val_mDice: 0.4590

Epoch 00105: val_mDice did not improve from 0.46004
Epoch 106/300

Epoch 00106: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.7016 - acc: 0.9865 - mDice: 0.4802 - val_loss: 1.5382 - val_acc: 0.9852 - val_mDice: 0.4597

Epoch 00106: val_mDice did not improve from 0.46004
Epoch 107/300

Epoch 00107: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.6922 - acc: 0.9866 - mDice: 0.4852 - val_loss: 1.5403 - val_acc: 0.9852 - val_mDice: 0.4594

Epoch 00107: val_mDice did not improve from 0.46004
Epoch 108/300

Epoch 00108: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.7013 - acc: 0.9866 - mDice: 0.4850 - val_loss: 1.5389 - val_acc: 0.9852 - val_mDice: 0.4587

Epoch 00108: val_mDice did not improve from 0.46004
Epoch 109/300

Epoch 00109: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 12s - loss: 0.6890 - acc: 0.9866 - mDice: 0.4855 - val_loss: 1.5368 - val_acc: 0.9852 - val_mDice: 0.4592

Epoch 00109: val_mDice did not improve from 0.46004
Epoch 110/300

Epoch 00110: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.6891 - acc: 0.9866 - mDice: 0.4861 - val_loss: 1.5373 - val_acc: 0.9852 - val_mDice: 0.4592

Epoch 00110: val_mDice did not improve from 0.46004
Epoch 111/300

Epoch 00111: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 12s - loss: 0.6906 - acc: 0.9866 - mDice: 0.4859 - val_loss: 1.5398 - val_acc: 0.9852 - val_mDice: 0.4588

Epoch 00111: val_mDice did not improve from 0.46004
Epoch 112/300

Epoch 00112: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 12s - loss: 0.6846 - acc: 0.9866 - mDice: 0.4859 - val_loss: 1.5403 - val_acc: 0.9852 - val_mDice: 0.4591

Epoch 00112: val_mDice did not improve from 0.46004
Epoch 113/300

Epoch 00113: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 12s - loss: 0.7231 - acc: 0.9866 - mDice: 0.4842 - val_loss: 1.5371 - val_acc: 0.9852 - val_mDice: 0.4596

Epoch 00113: val_mDice did not improve from 0.46004
Epoch 114/300

Epoch 00114: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.6942 - acc: 0.9866 - mDice: 0.4836 - val_loss: 1.5368 - val_acc: 0.9852 - val_mDice: 0.4596

Epoch 00114: val_mDice did not improve from 0.46004
Epoch 115/300

Epoch 00115: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.6925 - acc: 0.9866 - mDice: 0.4864 - val_loss: 1.5375 - val_acc: 0.9852 - val_mDice: 0.4594

Epoch 00115: val_mDice did not improve from 0.46004
Epoch 116/300

Epoch 00116: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.6933 - acc: 0.9866 - mDice: 0.4871 - val_loss: 1.5369 - val_acc: 0.9852 - val_mDice: 0.4592

Epoch 00116: val_mDice did not improve from 0.46004
Epoch 117/300

Epoch 00117: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.7656 - acc: 0.9865 - mDice: 0.4843 - val_loss: 1.5388 - val_acc: 0.9852 - val_mDice: 0.4590

Epoch 00117: val_mDice did not improve from 0.46004
Epoch 118/300

Epoch 00118: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.7161 - acc: 0.9866 - mDice: 0.4866 - val_loss: 1.5419 - val_acc: 0.9852 - val_mDice: 0.4585

Epoch 00118: val_mDice did not improve from 0.46004
Epoch 119/300

Epoch 00119: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.7053 - acc: 0.9866 - mDice: 0.4843 - val_loss: 1.5411 - val_acc: 0.9853 - val_mDice: 0.4591

Epoch 00119: val_mDice did not improve from 0.46004
Epoch 120/300

Epoch 00120: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.6952 - acc: 0.9866 - mDice: 0.4889 - val_loss: 1.5397 - val_acc: 0.9853 - val_mDice: 0.4593

Epoch 00120: val_mDice did not improve from 0.46004
Epoch 121/300

Epoch 00121: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.6890 - acc: 0.9866 - mDice: 0.4878 - val_loss: 1.5458 - val_acc: 0.9852 - val_mDice: 0.4582

Epoch 00121: val_mDice did not improve from 0.46004
Epoch 122/300

Epoch 00122: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.6956 - acc: 0.9866 - mDice: 0.4864 - val_loss: 1.5418 - val_acc: 0.9852 - val_mDice: 0.4590

Epoch 00122: val_mDice did not improve from 0.46004
Epoch 123/300

Epoch 00123: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.6959 - acc: 0.9865 - mDice: 0.4848 - val_loss: 1.5404 - val_acc: 0.9851 - val_mDice: 0.4598

Epoch 00123: val_mDice did not improve from 0.46004
Epoch 124/300

Epoch 00124: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.6823 - acc: 0.9866 - mDice: 0.4876 - val_loss: 1.5414 - val_acc: 0.9852 - val_mDice: 0.4598

Epoch 00124: val_mDice did not improve from 0.46004
Epoch 125/300

Epoch 00125: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.6890 - acc: 0.9866 - mDice: 0.4856 - val_loss: 1.5383 - val_acc: 0.9852 - val_mDice: 0.4601

Epoch 00125: val_mDice improved from 0.46004 to 0.46008, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 126/300

Epoch 00126: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 13s - loss: 0.7067 - acc: 0.9866 - mDice: 0.4885 - val_loss: 1.5353 - val_acc: 0.9852 - val_mDice: 0.4608

Epoch 00126: val_mDice improved from 0.46008 to 0.46076, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 127/300

Epoch 00127: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 13s - loss: 0.7045 - acc: 0.9866 - mDice: 0.4834 - val_loss: 1.5364 - val_acc: 0.9852 - val_mDice: 0.4605

Epoch 00127: val_mDice did not improve from 0.46076
Epoch 128/300

Epoch 00128: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 12s - loss: 0.6803 - acc: 0.9867 - mDice: 0.4895 - val_loss: 1.5368 - val_acc: 0.9852 - val_mDice: 0.4602

Epoch 00128: val_mDice did not improve from 0.46076
Epoch 129/300

Epoch 00129: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 12s - loss: 0.6805 - acc: 0.9867 - mDice: 0.4896 - val_loss: 1.5376 - val_acc: 0.9852 - val_mDice: 0.4600

Epoch 00129: val_mDice did not improve from 0.46076
Epoch 130/300

Epoch 00130: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.6893 - acc: 0.9866 - mDice: 0.4870 - val_loss: 1.5385 - val_acc: 0.9852 - val_mDice: 0.4598

Epoch 00130: val_mDice did not improve from 0.46076
Epoch 131/300

Epoch 00131: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.6942 - acc: 0.9866 - mDice: 0.4860 - val_loss: 1.5385 - val_acc: 0.9852 - val_mDice: 0.4597

Epoch 00131: val_mDice did not improve from 0.46076
Epoch 132/300

Epoch 00132: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 12s - loss: 0.7390 - acc: 0.9866 - mDice: 0.4854 - val_loss: 1.5395 - val_acc: 0.9852 - val_mDice: 0.4594

Epoch 00132: val_mDice did not improve from 0.46076
Epoch 133/300

Epoch 00133: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7006 - acc: 0.9867 - mDice: 0.4852 - val_loss: 1.5343 - val_acc: 0.9852 - val_mDice: 0.4601

Epoch 00133: val_mDice did not improve from 0.46076
Epoch 134/300

Epoch 00134: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.6950 - acc: 0.9867 - mDice: 0.4879 - val_loss: 1.5360 - val_acc: 0.9852 - val_mDice: 0.4597

Epoch 00134: val_mDice did not improve from 0.46076
Epoch 135/300

Epoch 00135: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.6835 - acc: 0.9867 - mDice: 0.4872 - val_loss: 1.5355 - val_acc: 0.9852 - val_mDice: 0.4599

Epoch 00135: val_mDice did not improve from 0.46076
Epoch 136/300

Epoch 00136: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.6816 - acc: 0.9866 - mDice: 0.4896 - val_loss: 1.5354 - val_acc: 0.9852 - val_mDice: 0.4600

Epoch 00136: val_mDice did not improve from 0.46076
Epoch 137/300

Epoch 00137: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.6930 - acc: 0.9867 - mDice: 0.4881 - val_loss: 1.5361 - val_acc: 0.9852 - val_mDice: 0.4599

Epoch 00137: val_mDice did not improve from 0.46076
Epoch 138/300

Epoch 00138: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7133 - acc: 0.9867 - mDice: 0.4858 - val_loss: 1.5359 - val_acc: 0.9853 - val_mDice: 0.4595

Epoch 00138: val_mDice did not improve from 0.46076
Epoch 139/300

Epoch 00139: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.6895 - acc: 0.9866 - mDice: 0.4877 - val_loss: 1.5354 - val_acc: 0.9853 - val_mDice: 0.4597

Epoch 00139: val_mDice did not improve from 0.46076
Epoch 140/300

Epoch 00140: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.6949 - acc: 0.9867 - mDice: 0.4888 - val_loss: 1.5347 - val_acc: 0.9852 - val_mDice: 0.4598

Epoch 00140: val_mDice did not improve from 0.46076
Epoch 141/300

Epoch 00141: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7060 - acc: 0.9867 - mDice: 0.4881 - val_loss: 1.5351 - val_acc: 0.9853 - val_mDice: 0.4598

Epoch 00141: val_mDice did not improve from 0.46076
Epoch 142/300

Epoch 00142: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.6988 - acc: 0.9867 - mDice: 0.4868 - val_loss: 1.5349 - val_acc: 0.9852 - val_mDice: 0.4600

Epoch 00142: val_mDice did not improve from 0.46076
Epoch 143/300

Epoch 00143: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.6844 - acc: 0.9866 - mDice: 0.4867 - val_loss: 1.5341 - val_acc: 0.9852 - val_mDice: 0.4602

Epoch 00143: val_mDice did not improve from 0.46076
Epoch 144/300

Epoch 00144: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7158 - acc: 0.9866 - mDice: 0.4855 - val_loss: 1.5354 - val_acc: 0.9852 - val_mDice: 0.4598

Epoch 00144: val_mDice did not improve from 0.46076
Epoch 145/300

Epoch 00145: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6920 - acc: 0.9866 - mDice: 0.4865 - val_loss: 1.5356 - val_acc: 0.9852 - val_mDice: 0.4596

Epoch 00145: val_mDice did not improve from 0.46076
Epoch 146/300

Epoch 00146: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6931 - acc: 0.9866 - mDice: 0.4891 - val_loss: 1.5354 - val_acc: 0.9852 - val_mDice: 0.4596

Epoch 00146: val_mDice did not improve from 0.46076
Epoch 147/300

Epoch 00147: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6925 - acc: 0.9867 - mDice: 0.4861 - val_loss: 1.5359 - val_acc: 0.9852 - val_mDice: 0.4595

Epoch 00147: val_mDice did not improve from 0.46076
Epoch 148/300

Epoch 00148: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6909 - acc: 0.9866 - mDice: 0.4904 - val_loss: 1.5361 - val_acc: 0.9852 - val_mDice: 0.4594

Epoch 00148: val_mDice did not improve from 0.46076
Epoch 149/300

Epoch 00149: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.7192 - acc: 0.9866 - mDice: 0.4896 - val_loss: 1.5356 - val_acc: 0.9852 - val_mDice: 0.4597

Epoch 00149: val_mDice did not improve from 0.46076
Epoch 150/300

Epoch 00150: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6742 - acc: 0.9866 - mDice: 0.4897 - val_loss: 1.5359 - val_acc: 0.9852 - val_mDice: 0.4596

Epoch 00150: val_mDice did not improve from 0.46076
Epoch 151/300

Epoch 00151: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6924 - acc: 0.9866 - mDice: 0.4898 - val_loss: 1.5362 - val_acc: 0.9852 - val_mDice: 0.4595

Epoch 00151: val_mDice did not improve from 0.46076
Epoch 152/300

Epoch 00152: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6828 - acc: 0.9866 - mDice: 0.4869 - val_loss: 1.5366 - val_acc: 0.9852 - val_mDice: 0.4594

Epoch 00152: val_mDice did not improve from 0.46076
Epoch 153/300

Epoch 00153: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6958 - acc: 0.9866 - mDice: 0.4843 - val_loss: 1.5360 - val_acc: 0.9852 - val_mDice: 0.4596

Epoch 00153: val_mDice did not improve from 0.46076
Epoch 154/300

Epoch 00154: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6800 - acc: 0.9866 - mDice: 0.4888 - val_loss: 1.5358 - val_acc: 0.9852 - val_mDice: 0.4598

Epoch 00154: val_mDice did not improve from 0.46076
Epoch 155/300

Epoch 00155: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6992 - acc: 0.9866 - mDice: 0.4853 - val_loss: 1.5360 - val_acc: 0.9852 - val_mDice: 0.4598

Epoch 00155: val_mDice did not improve from 0.46076
Epoch 156/300

Epoch 00156: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.7354 - acc: 0.9866 - mDice: 0.4845 - val_loss: 1.5365 - val_acc: 0.9852 - val_mDice: 0.4597

Epoch 00156: val_mDice did not improve from 0.46076
Epoch 157/300

Epoch 00157: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6939 - acc: 0.9866 - mDice: 0.4877 - val_loss: 1.5368 - val_acc: 0.9852 - val_mDice: 0.4597

Epoch 00157: val_mDice did not improve from 0.46076
Epoch 158/300

Epoch 00158: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.7244 - acc: 0.9866 - mDice: 0.4869 - val_loss: 1.5368 - val_acc: 0.9852 - val_mDice: 0.4598

Epoch 00158: val_mDice did not improve from 0.46076
Epoch 159/300

Epoch 00159: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6956 - acc: 0.9866 - mDice: 0.4865 - val_loss: 1.5372 - val_acc: 0.9852 - val_mDice: 0.4598

Epoch 00159: val_mDice did not improve from 0.46076
Epoch 160/300

Epoch 00160: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6807 - acc: 0.9867 - mDice: 0.4894 - val_loss: 1.5374 - val_acc: 0.9852 - val_mDice: 0.4597

Epoch 00160: val_mDice did not improve from 0.46076
Epoch 161/300

Epoch 00161: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6926 - acc: 0.9867 - mDice: 0.4866 - val_loss: 1.5369 - val_acc: 0.9852 - val_mDice: 0.4597

Epoch 00161: val_mDice did not improve from 0.46076
Epoch 162/300

Epoch 00162: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 11s - loss: 0.6856 - acc: 0.9867 - mDice: 0.4895 - val_loss: 1.5371 - val_acc: 0.9852 - val_mDice: 0.4596

Epoch 00162: val_mDice did not improve from 0.46076
Epoch 163/300

Epoch 00163: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 11s - loss: 0.6936 - acc: 0.9867 - mDice: 0.4898 - val_loss: 1.5368 - val_acc: 0.9852 - val_mDice: 0.4596

Epoch 00163: val_mDice did not improve from 0.46076
Epoch 164/300

Epoch 00164: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 11s - loss: 0.7036 - acc: 0.9867 - mDice: 0.4905 - val_loss: 1.5367 - val_acc: 0.9852 - val_mDice: 0.4597

Epoch 00164: val_mDice did not improve from 0.46076
Epoch 165/300

Epoch 00165: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 11s - loss: 0.6876 - acc: 0.9867 - mDice: 0.4886 - val_loss: 1.5370 - val_acc: 0.9852 - val_mDice: 0.4596

Epoch 00165: val_mDice did not improve from 0.46076
Epoch 166/300

Epoch 00166: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 11s - loss: 0.6786 - acc: 0.9867 - mDice: 0.4895 - val_loss: 1.5367 - val_acc: 0.9852 - val_mDice: 0.4596

Epoch 00166: val_mDice did not improve from 0.46076
Restoring model weights from the end of the best epoch
Epoch 00166: early stopping
{'val_loss': [5.434491846196368, 2.847026742473667, 2.3214534581946964, 2.04252810266383, 1.921453604175373, 1.8736210247970033, 1.799605298357785, 1.7548046317127566, 1.7528485449590845, 1.7274794824181974, 1.731974221808248, 1.9025911052195472, 1.6977667628254016, 1.6909392452420269, 1.662356451914306, 1.6600850883638927, 1.756038152653238, 1.6388940164407395, 1.662504516387031, 1.6434787626753231, 1.6234873395786393, 1.6219404349931021, 1.6215997702232605, 1.606088427604935, 1.604863924655662, 1.5820968739251784, 1.6020823214590436, 1.622855005733007, 1.5954641004591223, 1.5884599266520971, 1.5785715244217495, 1.587274449759485, 1.5601679084431697, 1.5888365921766863, 1.621064079731759, 1.573661403755159, 1.568802411930322, 1.5754848486534363, 1.5581844794953028, 1.5565726594798732, 1.5502724334287734, 1.5471292175282152, 1.5610910807304887, 1.5484819358147837, 1.5616934842558574, 1.5780585627925419, 1.5607248670888083, 1.5550768289764347, 1.6016262370832475, 1.549539650545679, 1.548786916895039, 1.5487533726629101, 1.545536191792479, 1.5653343673915179, 1.557794760666182, 1.5472027491531661, 1.5472737586791214, 1.5447462420833133, 1.5495432887951683, 1.5432460461311845, 1.5418933231123002, 1.5623653376710889, 1.5468891009491197, 1.5419078954902172, 1.5421267666753613, 1.5397674108948736, 1.5408514073305184, 1.532119642583995, 1.5378966234582132, 1.5446705561279124, 1.5491981443250111, 1.5404427078585994, 1.538456635574312, 1.5381794289064317, 1.5387872912707987, 1.5363106152961745, 1.5320172983667124, 1.5321809371162227, 1.541214372124699, 1.5396208821713135, 1.535455124139335, 1.534900770295545, 1.5401449701511116, 1.5403987212081938, 1.5451934504373763, 1.5472208644582104, 1.5363301629154804, 1.5356111569305448, 1.5460936782040082, 1.543501296881692, 1.543879597083382, 1.5406455594785722, 1.5424239935180866, 1.5419661086087866, 1.5431397538103995, 1.5394860490283353, 1.5320969493267442, 1.5352699256348925, 1.5392551302684503, 1.5403621473925326, 1.5409424280173838, 1.5397019007706236, 1.5395504975363978, 1.5419469632363274, 1.5413150136096716, 1.5381543546182672, 1.540279285668876, 1.5389290166937786, 1.5367711487249076, 1.53733763442373, 1.5397738157463434, 1.540298191270666, 1.5370916644206345, 1.536786884801825, 1.5374855479130447, 1.5368729548102742, 1.538832868880721, 1.5419455088379252, 1.5411403041706193, 1.539719576871688, 1.5458170209354625, 1.5418414023963625, 1.5403820591748099, 1.5414240590115351, 1.5383209722479259, 1.5353052832453824, 1.5363930637309142, 1.5368497826877299, 1.5375589517654678, 1.5384852615781903, 1.5385426332459333, 1.5394958893157584, 1.5342597053271836, 1.536041663364562, 1.5354662124060503, 1.535399566976695, 1.5361447455977673, 1.5359144661512176, 1.5354232745269747, 1.5347280114720134, 1.5351191608126087, 1.5349199030484955, 1.5341058331761783, 1.5353989321703272, 1.5356307507462672, 1.535359091479296, 1.5358545365540877, 1.5361411114045487, 1.5356096223081217, 1.5358600165758332, 1.536201454065247, 1.5366411161783287, 1.5359568021247878, 1.5357801395913828, 1.5359653833908484, 1.5364793135673653, 1.5367922460649774, 1.5367611362713272, 1.5372202355830116, 1.537384419450237, 1.5369172853422977, 1.5370629744178181, 1.5367811368408635, 1.5367414734990024, 1.5369585119258253, 1.5367326434483375], 'val_acc': [0.8996506844891943, 0.9804831961828279, 0.9830931918607543, 0.9849793744222428, 0.9855050245169655, 0.9847057586806933, 0.9853060568888832, 0.9854723597068642, 0.9855619415443652, 0.9847336984987746, 0.9853920921936828, 0.9850300353280989, 0.9845053501751112, 0.9853453864898033, 0.9854826292829387, 0.9851523487914938, 0.9853425706492029, 0.9852674437831165, 0.9850845321139676, 0.9850175139584028, 0.9848806551588957, 0.9852185689884684, 0.9851381456468867, 0.9847402125315314, 0.9852588535031209, 0.9851931270444325, 0.9852455277524057, 0.98491219942421, 0.9849683840693959, 0.9851248965146185, 0.9851872709162519, 0.9851069983327321, 0.9849535434142404, 0.9849157333374023, 0.9847072910496327, 0.9847123451629524, 0.9849162965956617, 0.9851242522436189, 0.9850812410136477, 0.9850991337871732, 0.9850732008090317, 0.9850307549781294, 0.9852957847213024, 0.9852721783524425, 0.9852183334994181, 0.985222337263969, 0.9853791000712346, 0.9852219391860674, 0.9852505196695742, 0.9853120687327899, 0.9853802219681118, 0.9852417602656244, 0.9851965782547718, 0.985233653470564, 0.9852948174134094, 0.9850899030671002, 0.9851319633269355, 0.9850679904166151, 0.9851878099495612, 0.9852249053340327, 0.9851863031576622, 0.9848233372367398, 0.9852201771871354, 0.9852676112169579, 0.9851776301748135, 0.9852009147461962, 0.985245379473176, 0.9851522719476985, 0.9853892070584577, 0.9854632104502283, 0.9850782675418601, 0.9851930392712411, 0.9851782054892109, 0.9852147141791923, 0.985111486032915, 0.9851500140696707, 0.9852793191917001, 0.9852313238190793, 0.98518814470457, 0.9850907898129497, 0.9851728219165694, 0.9851446407503802, 0.9851810847653784, 0.9851359745279827, 0.9851464754241626, 0.9852374232108291, 0.9851182441179603, 0.9851647886977529, 0.9851980273574872, 0.9852298936456273, 0.9852085343380732, 0.9852174470915912, 0.985246339569921, 0.9852130214748851, 0.9852414399329115, 0.9851679172092214, 0.9851604519809802, 0.9851861446250815, 0.9851945621755488, 0.9852042759298603, 0.9851749070650688, 0.9851613379381104, 0.9851616615383747, 0.9852123818235271, 0.9852591826244202, 0.985199868904395, 0.9852174447254333, 0.9852176848341251, 0.9852139908109135, 0.9852359045881405, 0.9852421698362931, 0.9852253151300499, 0.985207167487514, 0.9852346233699633, 0.9852185585097689, 0.985189997856919, 0.9852106986965259, 0.9852497201335453, 0.9852796555241475, 0.9852632735987917, 0.985244651147233, 0.985205239519505, 0.9851497025255445, 0.9851842940642388, 0.985165993184809, 0.9852041167212344, 0.9852395209788375, 0.9852334854733515, 0.9852316562079302, 0.9852360622193277, 0.9852226589487721, 0.9852358239134228, 0.9852117442003044, 0.9852368590511762, 0.9852330082981708, 0.9852346969462072, 0.9852450598165083, 0.9852579768979481, 0.9852548532314697, 0.9852487513608879, 0.9852517917611423, 0.9852445676556607, 0.9852351080943146, 0.9852413590328455, 0.9852357413232439, 0.9852310821329489, 0.9852387994133457, 0.9852459328161071, 0.9852347810011505, 0.9852417598149276, 0.9852358977150151, 0.9852423282561996, 0.985226998144377, 0.9852221760272079, 0.9852245089462439, 0.9852277947508771, 0.9852342974035411, 0.9852347858461405, 0.985239283574326, 0.9852335830491973, 0.9852249904030438, 0.9852314035924036, 0.9852338089609417, 0.9852360662755985, 0.9852401578133408, 0.9852412056832773], 'val_mDice': [0.012892328985323156, 0.14535288917497574, 0.22823843785389913, 0.28597695891004205, 0.3230120212452342, 0.3361455367381609, 0.36300807107514604, 0.3853492497102074, 0.37729004387953274, 0.3791250009582658, 0.3917418248697918, 0.3340320318395057, 0.40400208768017576, 0.4067232352179706, 0.4153195861797421, 0.39540020295107187, 0.3639892617501107, 0.4214841664252919, 0.41633269057691885, 0.42185626010672805, 0.4282432171535486, 0.4329718317104141, 0.43449784666722346, 0.4322772660699368, 0.42119573949640776, 0.43571051972476316, 0.42672676077580857, 0.42994684740084105, 0.4327229701813543, 0.43758366032660234, 0.443399406078717, 0.443101224561692, 0.44257656582023785, 0.43915554999403783, 0.4298154335285912, 0.44033838094463873, 0.445131858222764, 0.4434023536530469, 0.4443502261572727, 0.44620098802481123, 0.4467921241455921, 0.4475700489204863, 0.44627407238253575, 0.45081455168263224, 0.4488531663584292, 0.45085949319747537, 0.4554548605375119, 0.4537724420526866, 0.4316261015931014, 0.4537243751953365, 0.44968490533896904, 0.45213962582226963, 0.4523429793529283, 0.4546574876698294, 0.45372075875182344, 0.4537263649686579, 0.4541633531683616, 0.4542620184748576, 0.45256962959913427, 0.45591201297657136, 0.45655898541381096, 0.44243590223879353, 0.4519110549389189, 0.45554301921200774, 0.45620426318708573, 0.45707126508954565, 0.4555650953844274, 0.4591141662266394, 0.45535422545384147, 0.4541283550184565, 0.4554239500556951, 0.45815638755228094, 0.4582708276040669, 0.458788919081253, 0.45868004337199636, 0.45953143471482233, 0.45852356261814925, 0.45924651340734646, 0.45758207358546654, 0.45759758678580054, 0.4586964573373134, 0.457862751093642, 0.4576354223375679, 0.45840205216255664, 0.4584530371158526, 0.4579434264838583, 0.45979524015084555, 0.4600411033881071, 0.45860457544114674, 0.45912796416318147, 0.45921259842256684, 0.4594810426953497, 0.4584624501321965, 0.45882583375393443, 0.4587124394347965, 0.4597219105049376, 0.45917184598081956, 0.45924669178818933, 0.4591034894460541, 0.4584764906104887, 0.45898403949676425, 0.4595179722797555, 0.4593430502984553, 0.4582901156537362, 0.4590150442375803, 0.45969789744916395, 0.45943563701215223, 0.4587048220576715, 0.4592213601803509, 0.4591692936464102, 0.4588267405029731, 0.45912656010549324, 0.45959856680159533, 0.45957592709275186, 0.4594348833626858, 0.4591834260527545, 0.4590359768268629, 0.4584642352751106, 0.45909360677553146, 0.459344483598616, 0.4582387170735007, 0.4590240890734137, 0.45979080854128124, 0.4598092156090899, 0.4600759677356549, 0.4607592102964118, 0.46048032654192306, 0.46024953262679746, 0.4599851856258168, 0.45983677597572087, 0.45966142477795113, 0.45939676469765844, 0.4600933196554788, 0.45965698324425575, 0.4599320244797574, 0.46001788493886675, 0.45985609417277723, 0.4595489924622168, 0.45974206691343755, 0.4598236780803798, 0.4598269306285226, 0.459962182331457, 0.4601616579687156, 0.4598170325603681, 0.45955035312977993, 0.4596007666524169, 0.4595291820303704, 0.45937685818113877, 0.45971427137465243, 0.4595646008792807, 0.45951611925416497, 0.45938813672286577, 0.45962938960352895, 0.4597635190081844, 0.45978877242365834, 0.459680855612223, 0.4597258822841038, 0.4598311623646429, 0.4597767367712548, 0.45969829691436653, 0.45972350268626483, 0.45963622247254465, 0.4596288848971149, 0.45966256619387064, 0.4595667475126754, 0.45960844214477475], 'loss': [5.502216633708657, 3.2489829748001346, 2.02059543195071, 1.590251291412179, 1.3551431446118152, 1.2346865353106147, 1.2218203003592205, 1.1158896997574699, 1.0732458341949227, 1.1310312912576261, 1.116713549966805, 1.0910528575819385, 1.0583205873410362, 0.9319619164297195, 0.9228645647067459, 0.9804434314774292, 1.1467094590456872, 0.9377348687435804, 0.9354461125100718, 0.9133589493157586, 0.8946061374595967, 0.8671582276680542, 0.8942342136119189, 0.8995985601576563, 0.9618780919348134, 0.8509358165127093, 0.8692720102474861, 0.8612877763483352, 0.876724871155871, 0.8497080892198386, 0.8081811922021872, 0.838535955175294, 0.8506556149293131, 0.810343006648481, 0.8838223763501712, 0.8126762782379476, 0.7976066948140134, 0.8418122427058357, 0.8179441640910358, 0.8127487816303991, 0.7696041119991472, 0.7908042265604194, 0.846885425055552, 0.7770250308688644, 0.7827626681203241, 0.7671029857214069, 0.7873355555101306, 0.8093849270757261, 0.762824347894801, 0.7814640597224561, 0.7899169036777911, 0.7752117673743866, 0.7853837979910652, 0.7553903231961302, 0.7456963088388436, 0.750810355222412, 0.735702279848553, 0.7629058706312519, 0.7430664378334912, 0.7313490463270125, 0.7299407373823672, 0.8313828892718503, 0.8086791506570559, 0.7293993677627866, 0.7244360933525701, 0.7142599836514643, 0.7348985831575567, 0.7151529819225606, 0.7750746031607167, 0.7447429182636823, 0.7510574578823631, 0.7136463772956931, 0.7103183391112597, 0.7125403513567873, 0.7056527846664539, 0.743087317878911, 0.712660131628877, 0.7409366624039125, 0.7353381973559894, 0.7076895407225606, 0.7571052242553362, 0.7089247467150644, 0.7672496310630584, 0.7091286033503182, 0.6995616999542158, 0.717330902481174, 0.6980876750867738, 0.7025411182684992, 0.715791522284947, 0.7033278022752113, 0.7455873966928915, 0.7085138282007054, 0.6910778625920864, 0.6861899826442996, 0.7043215063384114, 0.7066753952916685, 0.7204250089266194, 0.6976776388813293, 0.7376928567352447, 0.7364590892331403, 0.691885779941518, 0.6840112188557067, 0.6884254586432638, 0.689555693782067, 0.7036299090030687, 0.7015593465835664, 0.6922476612296559, 0.7012896808886238, 0.6889981103791737, 0.6890887854614789, 0.6905508084094296, 0.6845834471450335, 0.7230904312921597, 0.6942443313453885, 0.692501888372436, 0.6932860641344236, 0.7656114794300931, 0.7161286000767642, 0.7052606223501, 0.6952396654080146, 0.6890121795372395, 0.6955809350570181, 0.6959401364052406, 0.682344611766594, 0.688990416471031, 0.7067343391390931, 0.7045215036359466, 0.6802991689749398, 0.6805328575940119, 0.6893091119323449, 0.6942140763005856, 0.7390160089584392, 0.7005655196330595, 0.6950492635832761, 0.6834829262045077, 0.6815816624065512, 0.6929985895534154, 0.7132520917696453, 0.6895431931797739, 0.6948734434364624, 0.706014712196762, 0.6987687891233795, 0.6844426950550577, 0.7158011826569538, 0.6920271518960702, 0.6930849819700645, 0.692546718777279, 0.6909423272507083, 0.719214081408284, 0.6741747416557735, 0.6923911483111622, 0.682846290699194, 0.6957643751660281, 0.6799928625222017, 0.6991511368282876, 0.7354232967122333, 0.6938732190641131, 0.7244453186358821, 0.6955850802419434, 0.6806854585356664, 0.6925734546416492, 0.685551770737765, 0.6936437998223761, 0.7035636954652815, 0.6875751660990757, 0.678613040096637], 'acc': [0.5446631053722626, 0.9683372134384037, 0.9828996725211722, 0.984376881291066, 0.9850056060928407, 0.9851933516029933, 0.9852501768234151, 0.985374733890933, 0.9854532043771917, 0.9854522513109346, 0.9853701879059269, 0.9852815490808319, 0.9853656456137688, 0.9854476901689374, 0.9855023773786113, 0.9854002978915034, 0.9852590499513925, 0.9855204786818904, 0.9855336841139279, 0.9855539440129877, 0.9854552603740839, 0.9856329958484552, 0.9856406847289855, 0.9856918421542653, 0.9853455500093732, 0.9857431234605101, 0.9857743247642716, 0.985692621567677, 0.98583061788829, 0.9856995557724287, 0.9858841809594059, 0.9859318454694499, 0.9858926753850682, 0.9859647255063679, 0.9855600105022488, 0.9859029642976199, 0.9860271006325757, 0.9858418750389205, 0.9858221600944944, 0.986005293711942, 0.9860630190084752, 0.9861044112975753, 0.985761757720374, 0.9860059487104831, 0.9861626433893115, 0.9861583527892227, 0.9860666886761759, 0.9862579824201323, 0.9862961570520726, 0.9861201056795768, 0.9861042998892421, 0.9861960886989432, 0.986089768591257, 0.9861381919593413, 0.986265164431306, 0.986288443597047, 0.9863613625313103, 0.9863360643416502, 0.9862491017284901, 0.9863632228369142, 0.9863797061536466, 0.986375606617663, 0.9863129116670799, 0.986409328253182, 0.9863596066339421, 0.9864146491430624, 0.9864089448643608, 0.9864389044961457, 0.9860573688915458, 0.9863961439845751, 0.9864330784424409, 0.9864009592063868, 0.9864445675157972, 0.9864788846638582, 0.9864819490900678, 0.9864477998850268, 0.9864361825704753, 0.9864736019482391, 0.9864684217128625, 0.9864680381609036, 0.9865027399435657, 0.9864640102869363, 0.986519659846895, 0.9864137416367627, 0.9865491122088939, 0.9865265678457729, 0.98657072511411, 0.986532546611353, 0.9865582025326968, 0.9864924923048948, 0.986537329991619, 0.9865394601124262, 0.986585690343994, 0.9866079624893357, 0.9864923626251202, 0.986502294132264, 0.9864917509767599, 0.9865667995572476, 0.9864807224119442, 0.9864791791425412, 0.9865029016280239, 0.986562513065267, 0.9865072181670338, 0.986583958235093, 0.9865718549031706, 0.9865114620503693, 0.9866290286157403, 0.9866407936293855, 0.9865852041338237, 0.9865893614057819, 0.9866344995479673, 0.986598532304861, 0.9866328921653822, 0.9866275028169199, 0.9865881422616617, 0.9866473008430132, 0.9865094958237968, 0.9866169808100159, 0.9865576947586665, 0.9866062729445878, 0.9866066097946243, 0.9866229894298261, 0.9865405534475886, 0.9865916494143843, 0.9865735408885469, 0.9866117416225461, 0.986634799514015, 0.9866739238753014, 0.9866581415174968, 0.9866451666140977, 0.9865899417316715, 0.9865984541618222, 0.9866699763393189, 0.986684512012365, 0.9866680481090389, 0.9866452916370277, 0.9867028335517661, 0.9866696748160462, 0.9866207867571595, 0.9866551619674709, 0.9866870473231448, 0.9866587788229945, 0.9865987960839634, 0.9866300864016782, 0.9866467295490292, 0.9866493953404423, 0.9866594547476748, 0.9866491195039685, 0.986611154133421, 0.9866133532763773, 0.9866348608538534, 0.9866055946173315, 0.986608528295955, 0.986615543387428, 0.986636420555689, 0.9866132792117851, 0.986645340222451, 0.9866013920671995, 0.9866114683072938, 0.9866627103862966, 0.9866613944569285, 0.9866604418949156, 0.9866761043315564, 0.9866634052646311, 0.9866777698357149, 0.986659346542776], 'mDice': [0.007239021456115176, 0.07448239955595994, 0.14762879070520224, 0.19842617336450213, 0.2505898029094135, 0.2785174386303767, 0.2952836591580757, 0.3144170639600348, 0.3270264749252313, 0.3165891010478291, 0.3218859080637709, 0.348536082029224, 0.33291130331091157, 0.3801391340141482, 0.38808086745090586, 0.37253235816273356, 0.31541966364199203, 0.37764293165994717, 0.39305064957899427, 0.39377080975969386, 0.39264762726373476, 0.40887570418241337, 0.4101247335583691, 0.416701954101917, 0.3816513879051788, 0.413059823939859, 0.4162590871716827, 0.41409108933301436, 0.40695931155695253, 0.41052159035226604, 0.42846966997015146, 0.42930104403864777, 0.43024526178673683, 0.43685572462967004, 0.40338686708491606, 0.43072800685677787, 0.4391394239007077, 0.4229701120229348, 0.4293911983700468, 0.4314083166756005, 0.44676934899338205, 0.44863855631887156, 0.43100358150170853, 0.4478605242029415, 0.45117324893229577, 0.45048918442869695, 0.4423636196338291, 0.4524096701461956, 0.45554284731764555, 0.4483348897334562, 0.4461512322426139, 0.4465535568060878, 0.4440483120387097, 0.45442985136522585, 0.4589690794455943, 0.45403639400115087, 0.46536476038438995, 0.46324150613897264, 0.45669099450496986, 0.46527753302813113, 0.46751860714012133, 0.4664080834086186, 0.45686726369618125, 0.46342833619356216, 0.4673645175135115, 0.47206003425221565, 0.4671859228068188, 0.4704875111223958, 0.45040898442149724, 0.4662610303557729, 0.4609843176687852, 0.47074010160675867, 0.4737411885304248, 0.4755132784146994, 0.4761836200722654, 0.4744414539887674, 0.47378947483303835, 0.47279238890699854, 0.47557282937228545, 0.4751469011803888, 0.4750026253050434, 0.4774151770710856, 0.47962558163831887, 0.4808430128665254, 0.4800207315318233, 0.48079252907458037, 0.4791290605800844, 0.4814120099656645, 0.48045546769294967, 0.4782726673989131, 0.4807698632034089, 0.4794374902499318, 0.48300038670746065, 0.4850619346326072, 0.47895319078003595, 0.47944018360422924, 0.47770266639382486, 0.48337730998926476, 0.4804678363463688, 0.478842832461374, 0.4833084880030253, 0.4858012610805898, 0.48384991532891924, 0.48367684556050333, 0.4824071497385049, 0.48019448519642893, 0.4852370811312553, 0.48501444783160924, 0.4855448696915265, 0.48608419993944685, 0.485942082348437, 0.48585978538190944, 0.4842309029756657, 0.48364486558486586, 0.4863550055899054, 0.4871233453810526, 0.48425209683160575, 0.48658891405208576, 0.4842692938839271, 0.48887331678844204, 0.48777989863340876, 0.48641785373436214, 0.4847871308066887, 0.4875580411433106, 0.4855856381117571, 0.4885097132686597, 0.4833962660608199, 0.48947912939569493, 0.4895527265830608, 0.48702141064914134, 0.48604822011989396, 0.4854381768434728, 0.4851787058934551, 0.48794592889157895, 0.4871820720771439, 0.4896450721081527, 0.4880776798793349, 0.485781699737882, 0.48767513555061404, 0.48875310797171084, 0.48814504147762383, 0.4867653211244572, 0.48668089165679257, 0.4854758666667454, 0.4865141256171751, 0.4891312055384291, 0.4860678571289112, 0.4904423784552655, 0.4896301461500029, 0.4896981732766407, 0.48977396624780584, 0.48686833252922224, 0.4842665794403525, 0.4887901964725087, 0.485293324668777, 0.48448973266780004, 0.4877298611570099, 0.4869139796986334, 0.48647079754896083, 0.48937765882809564, 0.4865820051337036, 0.489526758742113, 0.4897861070124538, 0.4904657953279058, 0.4885763766813646, 0.48952296247942645], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:04<00:41,  4.19s/it]predicting test subjects:  18%|█▊        | 2/11 [00:07<00:36,  4.01s/it]predicting test subjects:  27%|██▋       | 3/11 [00:11<00:30,  3.87s/it]predicting test subjects:  36%|███▋      | 4/11 [00:14<00:26,  3.77s/it]predicting test subjects:  45%|████▌     | 5/11 [00:18<00:21,  3.62s/it]predicting test subjects:  55%|█████▍    | 6/11 [00:21<00:18,  3.64s/it]predicting test subjects:  64%|██████▎   | 7/11 [00:25<00:14,  3.64s/it]predicting test subjects:  73%|███████▎  | 8/11 [00:29<00:10,  3.64s/it]predicting test subjects:  82%|████████▏ | 9/11 [00:32<00:07,  3.53s/it]predicting test subjects:  91%|█████████ | 10/11 [00:35<00:03,  3.55s/it]predicting test subjects: 100%|██████████| 11/11 [00:40<00:00,  3.88s/it]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:03<02:30,  3.76s/it]predicting train subjects:   5%|▍         | 2/41 [00:07<02:27,  3.79s/it]predicting train subjects:   7%|▋         | 3/41 [00:10<02:17,  3.62s/it]predicting train subjects:  10%|▉         | 4/41 [00:13<02:07,  3.45s/it]predicting train subjects:  12%|█▏        | 5/41 [00:18<02:12,  3.69s/it]predicting train subjects:  15%|█▍        | 6/41 [00:21<02:06,  3.62s/it]predicting train subjects:  17%|█▋        | 7/41 [00:24<01:58,  3.49s/it]predicting train subjects:  20%|█▉        | 8/41 [00:27<01:43,  3.14s/it]predicting train subjects:  22%|██▏       | 9/41 [00:30<01:45,  3.31s/it]predicting train subjects:  24%|██▍       | 10/41 [00:35<01:54,  3.68s/it]predicting train subjects:  27%|██▋       | 11/41 [00:39<01:53,  3.78s/it]predicting train subjects:  29%|██▉       | 12/41 [00:41<01:36,  3.33s/it]predicting train subjects:  32%|███▏      | 13/41 [00:45<01:35,  3.43s/it]predicting train subjects:  34%|███▍      | 14/41 [00:49<01:34,  3.52s/it]predicting train subjects:  37%|███▋      | 15/41 [00:51<01:22,  3.16s/it]predicting train subjects:  39%|███▉      | 16/41 [00:54<01:22,  3.30s/it]predicting train subjects:  41%|████▏     | 17/41 [00:59<01:30,  3.77s/it]predicting train subjects:  44%|████▍     | 18/41 [01:03<01:28,  3.84s/it]predicting train subjects:  46%|████▋     | 19/41 [01:07<01:23,  3.79s/it]predicting train subjects:  49%|████▉     | 20/41 [01:10<01:13,  3.49s/it]predicting train subjects:  51%|█████     | 21/41 [01:13<01:09,  3.47s/it]predicting train subjects:  54%|█████▎    | 22/41 [01:17<01:07,  3.53s/it]predicting train subjects:  56%|█████▌    | 23/41 [01:19<00:56,  3.15s/it]predicting train subjects:  59%|█████▊    | 24/41 [01:23<00:56,  3.32s/it]predicting train subjects:  61%|██████    | 25/41 [01:27<00:54,  3.41s/it]predicting train subjects:  63%|██████▎   | 26/41 [01:31<00:54,  3.63s/it]predicting train subjects:  66%|██████▌   | 27/41 [01:35<00:52,  3.73s/it]predicting train subjects:  68%|██████▊   | 28/41 [01:38<00:48,  3.76s/it]predicting train subjects:  71%|███████   | 29/41 [01:42<00:43,  3.62s/it]predicting train subjects:  73%|███████▎  | 30/41 [01:46<00:41,  3.82s/it]predicting train subjects:  76%|███████▌  | 31/41 [01:50<00:38,  3.84s/it]predicting train subjects:  78%|███████▊  | 32/41 [01:53<00:33,  3.75s/it]predicting train subjects:  80%|████████  | 33/41 [01:58<00:30,  3.85s/it]predicting train subjects:  83%|████████▎ | 34/41 [02:03<00:29,  4.22s/it]predicting train subjects:  85%|████████▌ | 35/41 [02:09<00:28,  4.75s/it]predicting train subjects:  88%|████████▊ | 36/41 [02:14<00:25,  5.04s/it]predicting train subjects:  90%|█████████ | 37/41 [02:19<00:19,  4.95s/it]predicting train subjects:  93%|█████████▎| 38/41 [02:23<00:13,  4.55s/it]predicting train subjects:  95%|█████████▌| 39/41 [02:27<00:08,  4.36s/it]predicting train subjects:  98%|█████████▊| 40/41 [02:31<00:04,  4.35s/it]predicting train subjects: 100%|██████████| 41/41 [02:35<00:00,  4.37s/it]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:19,  2.10it/s]Loading train:   5%|▍         | 2/41 [00:01<00:20,  1.93it/s]Loading train:   7%|▋         | 3/41 [00:01<00:19,  1.94it/s]Loading train:  10%|▉         | 4/41 [00:02<00:18,  1.95it/s]Loading train:  12%|█▏        | 5/41 [00:02<00:18,  1.96it/s]Loading train:  15%|█▍        | 6/41 [00:03<00:18,  1.89it/s]Loading train:  17%|█▋        | 7/41 [00:03<00:16,  2.04it/s]Loading train:  20%|█▉        | 8/41 [00:03<00:15,  2.16it/s]Loading train:  22%|██▏       | 9/41 [00:04<00:14,  2.24it/s]Loading train:  24%|██▍       | 10/41 [00:04<00:13,  2.32it/s]Loading train:  27%|██▋       | 11/41 [00:05<00:13,  2.27it/s]Loading train:  29%|██▉       | 12/41 [00:05<00:12,  2.36it/s]Loading train:  32%|███▏      | 13/41 [00:06<00:11,  2.41it/s]Loading train:  34%|███▍      | 14/41 [00:06<00:11,  2.32it/s]Loading train:  37%|███▋      | 15/41 [00:06<00:11,  2.29it/s]Loading train:  39%|███▉      | 16/41 [00:07<00:10,  2.37it/s]Loading train:  41%|████▏     | 17/41 [00:07<00:10,  2.18it/s]Loading train:  44%|████▍     | 18/41 [00:08<00:10,  2.21it/s]Loading train:  46%|████▋     | 19/41 [00:08<00:10,  2.03it/s]Loading train:  49%|████▉     | 20/41 [00:09<00:09,  2.21it/s]Loading train:  51%|█████     | 21/41 [00:09<00:09,  2.11it/s]Loading train:  54%|█████▎    | 22/41 [00:10<00:09,  2.11it/s]Loading train:  56%|█████▌    | 23/41 [00:10<00:08,  2.08it/s]Loading train:  59%|█████▊    | 24/41 [00:11<00:08,  2.06it/s]Loading train:  61%|██████    | 25/41 [00:11<00:07,  2.09it/s]Loading train:  63%|██████▎   | 26/41 [00:12<00:07,  2.05it/s]Loading train:  66%|██████▌   | 27/41 [00:12<00:07,  1.97it/s]Loading train:  68%|██████▊   | 28/41 [00:13<00:07,  1.79it/s]Loading train:  71%|███████   | 29/41 [00:13<00:06,  1.86it/s]Loading train:  73%|███████▎  | 30/41 [00:14<00:05,  1.89it/s]Loading train:  76%|███████▌  | 31/41 [00:14<00:05,  1.92it/s]Loading train:  78%|███████▊  | 32/41 [00:15<00:04,  1.92it/s]Loading train:  80%|████████  | 33/41 [00:15<00:04,  1.97it/s]Loading train:  83%|████████▎ | 34/41 [00:16<00:03,  2.10it/s]Loading train:  85%|████████▌ | 35/41 [00:16<00:02,  2.03it/s]Loading train:  88%|████████▊ | 36/41 [00:17<00:02,  2.11it/s]Loading train:  90%|█████████ | 37/41 [00:17<00:01,  2.11it/s]Loading train:  93%|█████████▎| 38/41 [00:18<00:01,  2.15it/s]Loading train:  95%|█████████▌| 39/41 [00:18<00:00,  2.21it/s]Loading train:  98%|█████████▊| 40/41 [00:19<00:00,  2.11it/s]Loading train: 100%|██████████| 41/41 [00:19<00:00,  2.12it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:  10%|▉         | 4/41 [00:00<00:00, 38.32it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 205.98it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:05,  2.00it/s]Loading test:  18%|█▊        | 2/11 [00:00<00:04,  2.10it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:03,  2.33it/s]Loading test:  36%|███▋      | 4/11 [00:01<00:03,  2.18it/s]Loading test:  45%|████▌     | 5/11 [00:02<00:02,  2.08it/s]Loading test:  55%|█████▍    | 6/11 [00:02<00:02,  2.02it/s]Loading test:  64%|██████▎   | 7/11 [00:03<00:01,  2.12it/s]Loading test:  73%|███████▎  | 8/11 [00:03<00:01,  2.16it/s]Loading test:  82%|████████▏ | 9/11 [00:04<00:00,  2.04it/s]Loading test:  91%|█████████ | 10/11 [00:04<00:00,  2.00it/s]Loading test: 100%|██████████| 11/11 [00:05<00:00,  1.89it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 566.12it/s]
Loading trainS:   0%|          | 0/41 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/41 [00:00<00:14,  2.75it/s]Loading trainS:   5%|▍         | 2/41 [00:00<00:14,  2.70it/s]Loading trainS:   7%|▋         | 3/41 [00:01<00:15,  2.44it/s]Loading trainS:  10%|▉         | 4/41 [00:01<00:13,  2.68it/s]Loading trainS:  12%|█▏        | 5/41 [00:01<00:14,  2.52it/s]Loading trainS:  15%|█▍        | 6/41 [00:02<00:13,  2.58it/s]Loading trainS:  17%|█▋        | 7/41 [00:02<00:12,  2.75it/s]Loading trainS:  20%|█▉        | 8/41 [00:03<00:11,  2.80it/s]Loading trainS:  22%|██▏       | 9/41 [00:03<00:11,  2.73it/s]Loading trainS:  24%|██▍       | 10/41 [00:03<00:11,  2.67it/s]Loading trainS:  27%|██▋       | 11/41 [00:04<00:11,  2.69it/s]Loading trainS:  29%|██▉       | 12/41 [00:04<00:10,  2.78it/s]Loading trainS:  32%|███▏      | 13/41 [00:04<00:11,  2.49it/s]Loading trainS:  34%|███▍      | 14/41 [00:05<00:10,  2.48it/s]Loading trainS:  37%|███▋      | 15/41 [00:05<00:10,  2.49it/s]Loading trainS:  39%|███▉      | 16/41 [00:06<00:10,  2.48it/s]Loading trainS:  41%|████▏     | 17/41 [00:06<00:10,  2.26it/s]Loading trainS:  44%|████▍     | 18/41 [00:07<00:10,  2.16it/s]Loading trainS:  46%|████▋     | 19/41 [00:07<00:09,  2.21it/s]Loading trainS:  49%|████▉     | 20/41 [00:08<00:08,  2.39it/s]Loading trainS:  51%|█████     | 21/41 [00:08<00:08,  2.30it/s]Loading trainS:  54%|█████▎    | 22/41 [00:08<00:08,  2.31it/s]Loading trainS:  56%|█████▌    | 23/41 [00:09<00:07,  2.33it/s]Loading trainS:  59%|█████▊    | 24/41 [00:09<00:07,  2.29it/s]Loading trainS:  61%|██████    | 25/41 [00:10<00:07,  2.23it/s]Loading trainS:  63%|██████▎   | 26/41 [00:10<00:07,  2.09it/s]Loading trainS:  66%|██████▌   | 27/41 [00:11<00:07,  1.87it/s]Loading trainS:  68%|██████▊   | 28/41 [00:12<00:06,  1.87it/s]Loading trainS:  71%|███████   | 29/41 [00:12<00:06,  1.90it/s]Loading trainS:  73%|███████▎  | 30/41 [00:13<00:05,  1.86it/s]Loading trainS:  76%|███████▌  | 31/41 [00:13<00:04,  2.01it/s]Loading trainS:  78%|███████▊  | 32/41 [00:13<00:04,  2.03it/s]Loading trainS:  80%|████████  | 33/41 [00:14<00:03,  2.16it/s]Loading trainS:  83%|████████▎ | 34/41 [00:14<00:03,  2.25it/s]Loading trainS:  85%|████████▌ | 35/41 [00:15<00:02,  2.22it/s]Loading trainS:  88%|████████▊ | 36/41 [00:15<00:02,  2.33it/s]Loading trainS:  90%|█████████ | 37/41 [00:15<00:01,  2.40it/s]Loading trainS:  93%|█████████▎| 38/41 [00:16<00:01,  2.47it/s]Loading trainS:  95%|█████████▌| 39/41 [00:16<00:00,  2.53it/s]Loading trainS:  98%|█████████▊| 40/41 [00:17<00:00,  2.65it/s]Loading trainS: 100%|██████████| 41/41 [00:17<00:00,  2.67it/s]
Loading testS:   0%|          | 0/11 [00:00<?, ?it/s]Loading testS:   9%|▉         | 1/11 [00:00<00:02,  3.55it/s]Loading testS:  18%|█▊        | 2/11 [00:00<00:02,  3.28it/s]Loading testS:  27%|██▋       | 3/11 [00:00<00:02,  3.27it/s]Loading testS:  36%|███▋      | 4/11 [00:01<00:02,  2.98it/s]Loading testS:  45%|████▌     | 5/11 [00:01<00:02,  2.79it/s]Loading testS:  55%|█████▍    | 6/11 [00:02<00:01,  2.71it/s]Loading testS:  64%|██████▎   | 7/11 [00:02<00:01,  2.56it/s]Loading testS:  73%|███████▎  | 8/11 [00:03<00:01,  2.39it/s]Loading testS:  82%|████████▏ | 9/11 [00:03<00:00,  2.27it/s]Loading testS:  91%|█████████ | 10/11 [00:04<00:00,  2.22it/s]Loading testS: 100%|██████████| 11/11 [00:04<00:00,  2.23it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 128, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 128, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 128, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 128, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 128, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 128, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 128, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 64, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 64, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 64, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 64, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 64, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 64, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 64, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 64, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 64, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 32, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 32, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 32, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 32, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 32, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 32, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 32, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 32, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 32, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 32, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 64, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 64, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 64, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 64, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 64, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 64, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 64, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 64, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 64, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 64, 140)  0           concatenate_4[0][0]              2019-07-28 22:41:25.184738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-28 22:41:25.184843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 22:41:25.184858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-28 22:41:25.184867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-28 22:41:25.185247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 128, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 128, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 128, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 128, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 128, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 128, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 128, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 128, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 128, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 128, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 128, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97830779 0.02169221]
Train on 2677 samples, validate on 711 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.1707 - acc: 0.7916 - mDice: 0.1399 - val_loss: 0.8683 - val_acc: 0.9674 - val_mDice: 0.2825

Epoch 00001: val_mDice improved from -inf to 0.28247, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.1898 - acc: 0.9848 - mDice: 0.7096 - val_loss: 0.4605 - val_acc: 0.9854 - val_mDice: 0.5529

Epoch 00002: val_mDice improved from 0.28247 to 0.55293, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.1082 - acc: 0.9878 - mDice: 0.8182 - val_loss: 0.2830 - val_acc: 0.9923 - val_mDice: 0.7461

Epoch 00003: val_mDice improved from 0.55293 to 0.74611, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0933 - acc: 0.9900 - mDice: 0.8413 - val_loss: 0.1869 - val_acc: 0.9935 - val_mDice: 0.8428

Epoch 00004: val_mDice improved from 0.74611 to 0.84279, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0856 - acc: 0.9912 - mDice: 0.8537 - val_loss: 0.1781 - val_acc: 0.9934 - val_mDice: 0.8573

Epoch 00005: val_mDice improved from 0.84279 to 0.85731, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0824 - acc: 0.9918 - mDice: 0.8589 - val_loss: 0.1897 - val_acc: 0.9924 - val_mDice: 0.8613

Epoch 00006: val_mDice improved from 0.85731 to 0.86135, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0808 - acc: 0.9915 - mDice: 0.8613 - val_loss: 0.1857 - val_acc: 0.9928 - val_mDice: 0.8643

Epoch 00007: val_mDice improved from 0.86135 to 0.86427, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0767 - acc: 0.9923 - mDice: 0.8683 - val_loss: 0.1741 - val_acc: 0.9922 - val_mDice: 0.8514

Epoch 00008: val_mDice did not improve from 0.86427
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0725 - acc: 0.9930 - mDice: 0.8753 - val_loss: 0.1576 - val_acc: 0.9936 - val_mDice: 0.8687

Epoch 00009: val_mDice improved from 0.86427 to 0.86872, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0706 - acc: 0.9932 - mDice: 0.8786 - val_loss: 0.1580 - val_acc: 0.9933 - val_mDice: 0.8650

Epoch 00010: val_mDice did not improve from 0.86872
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0688 - acc: 0.9935 - mDice: 0.8818 - val_loss: 0.1749 - val_acc: 0.9921 - val_mDice: 0.8478

Epoch 00011: val_mDice did not improve from 0.86872
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0665 - acc: 0.9939 - mDice: 0.8856 - val_loss: 0.1758 - val_acc: 0.9910 - val_mDice: 0.8257

Epoch 00012: val_mDice did not improve from 0.86872
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0652 - acc: 0.9940 - mDice: 0.8880 - val_loss: 0.1550 - val_acc: 0.9927 - val_mDice: 0.8561

Epoch 00013: val_mDice did not improve from 0.86872
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0647 - acc: 0.9941 - mDice: 0.8888 - val_loss: 0.1552 - val_acc: 0.9926 - val_mDice: 0.8501

Epoch 00014: val_mDice did not improve from 0.86872
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0695 - acc: 0.9937 - mDice: 0.8802 - val_loss: 0.1651 - val_acc: 0.9927 - val_mDice: 0.8526

Epoch 00015: val_mDice did not improve from 0.86872
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0646 - acc: 0.9941 - mDice: 0.8889 - val_loss: 0.1542 - val_acc: 0.9929 - val_mDice: 0.8612

Epoch 00016: val_mDice did not improve from 0.86872
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0636 - acc: 0.9943 - mDice: 0.8907 - val_loss: 0.1528 - val_acc: 0.9939 - val_mDice: 0.8751

Epoch 00017: val_mDice improved from 0.86872 to 0.87510, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0626 - acc: 0.9945 - mDice: 0.8923 - val_loss: 0.1461 - val_acc: 0.9941 - val_mDice: 0.8768

Epoch 00018: val_mDice improved from 0.87510 to 0.87675, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0597 - acc: 0.9947 - mDice: 0.8976 - val_loss: 0.1435 - val_acc: 0.9937 - val_mDice: 0.8702

Epoch 00019: val_mDice did not improve from 0.87675
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0593 - acc: 0.9948 - mDice: 0.8984 - val_loss: 0.1440 - val_acc: 0.9944 - val_mDice: 0.8791

Epoch 00020: val_mDice improved from 0.87675 to 0.87907, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0579 - acc: 0.9949 - mDice: 0.9008 - val_loss: 0.1419 - val_acc: 0.9940 - val_mDice: 0.8730

Epoch 00021: val_mDice did not improve from 0.87907
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0589 - acc: 0.9949 - mDice: 0.8990 - val_loss: 0.1456 - val_acc: 0.9941 - val_mDice: 0.8754

Epoch 00022: val_mDice did not improve from 0.87907
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0578 - acc: 0.9949 - mDice: 0.9009 - val_loss: 0.1369 - val_acc: 0.9946 - val_mDice: 0.8823

Epoch 00023: val_mDice improved from 0.87907 to 0.88229, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0575 - acc: 0.9950 - mDice: 0.9014 - val_loss: 0.1392 - val_acc: 0.9944 - val_mDice: 0.8803

Epoch 00024: val_mDice did not improve from 0.88229
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0566 - acc: 0.9950 - mDice: 0.9032 - val_loss: 0.1409 - val_acc: 0.9946 - val_mDice: 0.8818

Epoch 00025: val_mDice did not improve from 0.88229
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0568 - acc: 0.9950 - mDice: 0.9028 - val_loss: 0.1368 - val_acc: 0.9947 - val_mDice: 0.8813

Epoch 00026: val_mDice did not improve from 0.88229
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0571 - acc: 0.9951 - mDice: 0.9023 - val_loss: 0.1353 - val_acc: 0.9952 - val_mDice: 0.8884

Epoch 00027: val_mDice improved from 0.88229 to 0.88843, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0564 - acc: 0.9951 - mDice: 0.9035 - val_loss: 0.1364 - val_acc: 0.9946 - val_mDice: 0.8819

Epoch 00028: val_mDice did not improve from 0.88843
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0551 - acc: 0.9952 - mDice: 0.9057 - val_loss: 0.1392 - val_acc: 0.9950 - val_mDice: 0.8863

Epoch 00029: val_mDice did not improve from 0.88843
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0551 - acc: 0.9953 - mDice: 0.9059 - val_loss: 0.1343 - val_acc: 0.9951 - val_mDice: 0.8865

Epoch 00030: val_mDice did not improve from 0.88843
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0548 - acc: 0.9953 - mDice: 0.9063 - val_loss: 0.1332 - val_acc: 0.9954 - val_mDice: 0.8903

Epoch 00031: val_mDice improved from 0.88843 to 0.89029, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0549 - acc: 0.9953 - mDice: 0.9062 - val_loss: 0.1542 - val_acc: 0.9947 - val_mDice: 0.8833

Epoch 00032: val_mDice did not improve from 0.89029
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0557 - acc: 0.9953 - mDice: 0.9048 - val_loss: 0.1290 - val_acc: 0.9953 - val_mDice: 0.8874

Epoch 00033: val_mDice did not improve from 0.89029
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0544 - acc: 0.9954 - mDice: 0.9071 - val_loss: 0.1322 - val_acc: 0.9948 - val_mDice: 0.8832

Epoch 00034: val_mDice did not improve from 0.89029
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0548 - acc: 0.9954 - mDice: 0.9063 - val_loss: 0.1369 - val_acc: 0.9954 - val_mDice: 0.8900

Epoch 00035: val_mDice did not improve from 0.89029
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0537 - acc: 0.9954 - mDice: 0.9083 - val_loss: 0.1335 - val_acc: 0.9955 - val_mDice: 0.8925

Epoch 00036: val_mDice improved from 0.89029 to 0.89246, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0528 - acc: 0.9956 - mDice: 0.9099 - val_loss: 0.1341 - val_acc: 0.9953 - val_mDice: 0.8900

Epoch 00037: val_mDice did not improve from 0.89246
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0525 - acc: 0.9956 - mDice: 0.9105 - val_loss: 0.1304 - val_acc: 0.9952 - val_mDice: 0.8894

Epoch 00038: val_mDice did not improve from 0.89246
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0522 - acc: 0.9956 - mDice: 0.9110 - val_loss: 0.1303 - val_acc: 0.9951 - val_mDice: 0.8879

Epoch 00039: val_mDice did not improve from 0.89246
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0525 - acc: 0.9956 - mDice: 0.9104 - val_loss: 0.1336 - val_acc: 0.9953 - val_mDice: 0.8903

Epoch 00040: val_mDice did not improve from 0.89246
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0524 - acc: 0.9956 - mDice: 0.9106 - val_loss: 0.1317 - val_acc: 0.9953 - val_mDice: 0.8894

Epoch 00041: val_mDice did not improve from 0.89246
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0512 - acc: 0.9957 - mDice: 0.9127 - val_loss: 0.1298 - val_acc: 0.9952 - val_mDice: 0.8888

Epoch 00042: val_mDice did not improve from 0.89246
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0521 - acc: 0.9957 - mDice: 0.9111 - val_loss: 0.1271 - val_acc: 0.9954 - val_mDice: 0.8905

Epoch 00043: val_mDice did not improve from 0.89246
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0520 - acc: 0.9956 - mDice: 0.9113 - val_loss: 0.1223 - val_acc: 0.9954 - val_mDice: 0.8899

Epoch 00044: val_mDice did not improve from 0.89246
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0525 - acc: 0.9956 - mDice: 0.9105 - val_loss: 0.1248 - val_acc: 0.9955 - val_mDice: 0.8922

Epoch 00045: val_mDice did not improve from 0.89246
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0518 - acc: 0.9957 - mDice: 0.9118 - val_loss: 0.1258 - val_acc: 0.9953 - val_mDice: 0.8904

Epoch 00046: val_mDice did not improve from 0.89246
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0510 - acc: 0.9957 - mDice: 0.9132 - val_loss: 0.1266 - val_acc: 0.9954 - val_mDice: 0.8910

Epoch 00047: val_mDice did not improve from 0.89246
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0516 - acc: 0.9957 - mDice: 0.9121 - val_loss: 0.1266 - val_acc: 0.9956 - val_mDice: 0.8930

Epoch 00048: val_mDice improved from 0.89246 to 0.89296, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd2/best_model_weights.h5
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0510 - acc: 0.9958 - mDice: 0.9132 - val_loss: 0.1323 - val_acc: 0.9954 - val_mDice: 0.8901

Epoch 00049: val_mDice did not improve from 0.89296
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0511 - acc: 0.9958 - mDice: 0.9129 - val_loss: 0.1261 - val_acc: 0.9954 - val_mDice: 0.8896

Epoch 00050: val_mDice did not improve from 0.89296
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0502 - acc: 0.9958 - mDice: 0.9145 - val_loss: 0.1277 - val_acc: 0.9954 - val_mDice: 0.8902

Epoch 00051: val_mDice did not improve from 0.89296
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0503 - acc: 0.9959 - mDice: 0.9144 - val_loss: 0.1246 - val_acc: 0.9953 - val_mDice: 0.8891

Epoch 00052: val_mDice did not improve from 0.89296
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0521 - acc: 0.9958 - mDice: 0.9111 - val_loss: 0.1223 - val_acc: 0.9955 - val_mDice: 0.8891

Epoch 00053: val_mDice did not improve from 0.89296
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0512 - acc: 0.9959 - mDice: 0.9127 - val_loss: 0.1252 - val_acc: 0.9955 - val_mDice: 0.8881

Epoch 00054: val_mDice did not improve from 0.89296
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0502 - acc: 0.9960 - mDice: 0.9144 - val_loss: 0.1363 - val_acc: 0.9955 - val_mDice: 0.8904

Epoch 00055: val_mDice did not improve from 0.89296
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0497 - acc: 0.9961 - mDice: 0.9148 - val_loss: 0.1339 - val_acc: 0.9956 - val_mDice: 0.8912

Epoch 00056: val_mDice did not improve from 0.89296
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0483 - acc: 0.9962 - mDice: 0.9157 - val_loss: 0.1288 - val_acc: 0.9957 - val_mDice: 0.8905

Epoch 00057: val_mDice did not improve from 0.89296
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0474 - acc: 0.9963 - mDice: 0.9157 - val_loss: 0.1243 - val_acc: 0.9957 - val_mDice: 0.8906

Epoch 00058: val_mDice did not improve from 0.89296
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0476 - acc: 0.9962 - mDice: 0.9145 - val_loss: 0.1216 - val_acc: 0.9958 - val_mDice: 0.8910

Epoch 00059: val_mDice did not improve from 0.89296
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0471 - acc: 0.9963 - mDice: 0.9149 - val_loss: 0.1177 - val_acc: 0.9958 - val_mDice: 0.8919

Epoch 00060: val_mDice did not improve from 0.89296
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0470 - acc: 0.9963 - mDice: 0.9146 - val_loss: 0.1195 - val_acc: 0.9958 - val_mDice: 0.8915

Epoch 00061: val_mDice did not improve from 0.89296
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0466 - acc: 0.9963 - mDice: 0.9151 - val_loss: 0.1220 - val_acc: 0.9957 - val_mDice: 0.8887

Epoch 00062: val_mDice did not improve from 0.89296
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0467 - acc: 0.9963 - mDice: 0.9146 - val_loss: 0.1196 - val_acc: 0.9958 - val_mDice: 0.8899

Epoch 00063: val_mDice did not improve from 0.89296
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0460 - acc: 0.9963 - mDice: 0.9157 - val_loss: 0.1169 - val_acc: 0.9958 - val_mDice: 0.8907

Epoch 00064: val_mDice did not improve from 0.89296
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0461 - acc: 0.9963 - mDice: 0.9155 - val_loss: 0.1199 - val_acc: 0.9958 - val_mDice: 0.8909

Epoch 00065: val_mDice did not improve from 0.89296
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0461 - acc: 0.9963 - mDice: 0.9152 - val_loss: 0.1240 - val_acc: 0.9958 - val_mDice: 0.8898

Epoch 00066: val_mDice did not improve from 0.89296
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0458 - acc: 0.9964 - mDice: 0.9156 - val_loss: 0.1181 - val_acc: 0.9958 - val_mDice: 0.8905

Epoch 00067: val_mDice did not improve from 0.89296
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0461 - acc: 0.9963 - mDice: 0.9150 - val_loss: 0.1188 - val_acc: 0.9958 - val_mDice: 0.8904

Epoch 00068: val_mDice did not improve from 0.89296
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0450 - acc: 0.9964 - mDice: 0.9170 - val_loss: 0.1204 - val_acc: 0.9958 - val_mDice: 0.8911

Epoch 00069: val_mDice did not improve from 0.89296
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0450 - acc: 0.9964 - mDice: 0.9169 - val_loss: 0.1188 - val_acc: 0.9958 - val_mDice: 0.8915

Epoch 00070: val_mDice did not improve from 0.89296
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0450 - acc: 0.9964 - mDice: 0.9168 - val_loss: 0.1176 - val_acc: 0.9958 - val_mDice: 0.8907

Epoch 00071: val_mDice did not improve from 0.89296
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0447 - acc: 0.9964 - mDice: 0.9172 - val_loss: 0.1233 - val_acc: 0.9958 - val_mDice: 0.8909

Epoch 00072: val_mDice did not improve from 0.89296
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0447 - acc: 0.9964 - mDice: 0.9172 - val_loss: 0.1207 - val_acc: 0.9958 - val_mDice: 0.8913

Epoch 00073: val_mDice did not improve from 0.89296
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0449 - acc: 0.9964 - mDice: 0.9169 - val_loss: 0.1234 - val_acc: 0.9958 - val_mDice: 0.8911

Epoch 00074: val_mDice did not improve from 0.89296
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0448 - acc: 0.9964 - mDice: 0.9170 - val_loss: 0.1222 - val_acc: 0.9958 - val_mDice: 0.8899

Epoch 00075: val_mDice did not improve from 0.89296
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0447 - acc: 0.9964 - mDice: 0.9171 - val_loss: 0.1181 - val_acc: 0.9958 - val_mDice: 0.8920

Epoch 00076: val_mDice did not improve from 0.89296
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0446 - acc: 0.9964 - mDice: 0.9173 - val_loss: 0.1189 - val_acc: 0.9958 - val_mDice: 0.8912

Epoch 00077: val_mDice did not improve from 0.89296
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0445 - acc: 0.9965 - mDice: 0.9175 - val_loss: 0.1173 - val_acc: 0.9958 - val_mDice: 0.8901

Epoch 00078: val_mDice did not improve from 0.89296
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0445 - acc: 0.9965 - mDice: 0.9174 - val_loss: 0.1225 - val_acc: 0.9958 - val_mDice: 0.8904

Epoch 00079: val_mDice did not improve from 0.89296
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0448 - acc: 0.9964 - mDice: 0.9169 - val_loss: 0.1232 - val_acc: 0.9958 - val_mDice: 0.8905

Epoch 00080: val_mDice did not improve from 0.89296
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0449 - acc: 0.9964 - mDice: 0.9166 - val_loss: 0.1206 - val_acc: 0.9957 - val_mDice: 0.8889

Epoch 00081: val_mDice did not improve from 0.89296
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0438 - acc: 0.9965 - mDice: 0.9186 - val_loss: 0.1226 - val_acc: 0.9958 - val_mDice: 0.8902

Epoch 00082: val_mDice did not improve from 0.89296
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0438 - acc: 0.9965 - mDice: 0.9185 - val_loss: 0.1179 - val_acc: 0.9958 - val_mDice: 0.8903

Epoch 00083: val_mDice did not improve from 0.89296
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0441 - acc: 0.9965 - mDice: 0.9180 - val_loss: 0.1268 - val_acc: 0.9958 - val_mDice: 0.8895

Epoch 00084: val_mDice did not improve from 0.89296
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0442 - acc: 0.9965 - mDice: 0.9178 - val_loss: 0.1149 - val_acc: 0.9958 - val_mDice: 0.8909

Epoch 00085: val_mDice did not improve from 0.89296
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0437 - acc: 0.9965 - mDice: 0.9186 - val_loss: 0.1174 - val_acc: 0.9958 - val_mDice: 0.8908

Epoch 00086: val_mDice did not improve from 0.89296
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0446 - acc: 0.9965 - mDice: 0.9171 - val_loss: 0.1132 - val_acc: 0.9958 - val_mDice: 0.8910

Epoch 00087: val_mDice did not improve from 0.89296
Epoch 88/300

Epoch 00088: LearningRateScheduler setting learning rate to 6.25e-05.
 - 6s - loss: 0.0438 - acc: 0.9965 - mDice: 0.9185 - val_loss: 0.1198 - val_acc: 0.9958 - val_mDice: 0.8916

Epoch 00088: val_mDice did not improve from 0.89296
Restoring model weights from the end of the best epoch
Epoch 00088: early stopping
{'val_loss': [0.868284368649146, 0.4604974601171523, 0.28301709840066325, 0.1868984959128872, 0.17808050317519492, 0.18972978637188295, 0.18568341993581394, 0.17411399663081484, 0.15761681647156528, 0.15802531587758983, 0.17487409358789147, 0.17576723959710863, 0.15495450664757004, 0.15517321375687246, 0.16506042246577107, 0.15421550275571907, 0.15279096218268748, 0.14611758073757805, 0.14351409992252892, 0.14404318874646843, 0.1419285022550159, 0.14559337058734625, 0.13694282462912102, 0.13923752335137288, 0.14092583386371574, 0.1367548257741244, 0.13526272528533695, 0.13639442155297463, 0.13921246701738837, 0.13427231145391316, 0.13319492729907298, 0.15418293560775187, 0.12895597096233932, 0.13219752067335883, 0.13689553664287266, 0.1335006157138009, 0.13410197359767811, 0.13036171324896914, 0.13034086371608256, 0.13363442627605673, 0.13172336890979155, 0.1298371391834589, 0.12706816137759158, 0.12225653719466112, 0.12478839462484154, 0.12581915394750157, 0.12659432008631957, 0.12664540918902842, 0.13230692202280342, 0.12608092941563845, 0.12769732229233458, 0.12456548737909556, 0.12234888459475902, 0.12516039561705583, 0.13626388457254016, 0.13392651403838907, 0.1287734681208593, 0.12433146447488191, 0.12158806487896942, 0.11774385199684298, 0.11954648901725452, 0.12196224538082814, 0.11960645321803758, 0.11693774084082469, 0.11986701630590334, 0.12396649974391766, 0.11812352587150622, 0.11884161381446527, 0.12042687227622031, 0.11881610143369092, 0.11762575264469984, 0.12326937664494066, 0.12071829283958413, 0.1233619955773092, 0.12216983404424456, 0.11809654035061891, 0.11893266722371307, 0.11729460812151515, 0.1224844900238866, 0.12321038957051252, 0.12062931176060195, 0.12263186948711861, 0.11789216117936012, 0.12682468336175096, 0.11489045854694566, 0.11739091166464086, 0.11319835766961303, 0.11977818325350556], 'val_acc': [0.9673590572071478, 0.9853902571479647, 0.9922900847074184, 0.9935302217969076, 0.993397230505105, 0.9924442016961035, 0.992762856007293, 0.9922036956969528, 0.9935739737858081, 0.9933037409131872, 0.9921115306526967, 0.9910260806103799, 0.9927464752089961, 0.9925659259495688, 0.9926998645444459, 0.9929257753696791, 0.9939325154246827, 0.9941300993730248, 0.9936756202440221, 0.9944298160059375, 0.994034723558171, 0.9940542312949351, 0.9945655653748331, 0.9944174256170517, 0.9945782575593887, 0.9947494176202015, 0.9952171589922469, 0.9946458933222646, 0.9949770470041095, 0.9950513138885069, 0.9953531095582557, 0.9947249889373779, 0.9952938905412806, 0.9947527373725688, 0.9954308806257074, 0.9955387641944295, 0.9953076306274672, 0.9952300607571287, 0.9951430010896192, 0.9953186294029869, 0.9952691768292133, 0.995204575789461, 0.9953552808104352, 0.9953708065210013, 0.9955170684390597, 0.9953225946627589, 0.9953965681346996, 0.9956166274772247, 0.995404054343952, 0.9953746041165122, 0.9954374027654591, 0.9953253443566388, 0.9955394180850473, 0.9954693260407481, 0.9955495701560492, 0.9956473184201955, 0.9956906093323784, 0.9957396762783517, 0.9957709372798099, 0.9957928090826537, 0.9957927084840971, 0.9957078536016193, 0.9957574738396539, 0.995770266622766, 0.9957726390720587, 0.9957674247135425, 0.9957895815456299, 0.995783042639452, 0.9957939408164151, 0.9958228545182197, 0.9958045623473477, 0.9957950809333898, 0.9958113778995562, 0.9958035060625036, 0.9957754809812822, 0.9958261575041608, 0.99580910604882, 0.9957844845520963, 0.995797545598026, 0.9957876869394809, 0.9957482942213657, 0.9957735779919202, 0.995778213908736, 0.9957662929797809, 0.9958031204347034, 0.9958028437886728, 0.9957995408027316, 0.995818771893465], 'val_mDice': [0.2824745768400855, 0.552926092040522, 0.746110077481062, 0.8427940591981139, 0.857311487197876, 0.8613498737372762, 0.8642727061163021, 0.8513782010970404, 0.8687167228022709, 0.8649718325517013, 0.8478277510899196, 0.8257385296157643, 0.8561148170680436, 0.8500527145155036, 0.8525878568238849, 0.8611798370270118, 0.8750959790708647, 0.8767545390900345, 0.8701974832558934, 0.8790702424136563, 0.8729999075459025, 0.8753787783798453, 0.8822912237647549, 0.8803088621415837, 0.8817527364577925, 0.8812539939638935, 0.8884259249422956, 0.8818693553345113, 0.8862869062168857, 0.8864649405123983, 0.890291667688748, 0.8833455134041702, 0.8874297980350113, 0.8831833485309585, 0.8899955977535113, 0.8924589043092794, 0.8900266324082172, 0.8894268889635089, 0.8878984112612138, 0.8902671467905809, 0.8894011105833844, 0.8888025743213551, 0.8905012815478146, 0.8898876052030196, 0.8921895935900939, 0.890352228019811, 0.890953480442868, 0.892957554587835, 0.8901007483947797, 0.8896362094101188, 0.8901933577493273, 0.8890518491613546, 0.8890880646417245, 0.8881446178452375, 0.8904348781172569, 0.8912355671787396, 0.8904522984339718, 0.8906493793895308, 0.8910238491331978, 0.891877268604756, 0.8914614025550552, 0.8886755434940301, 0.8899493224174833, 0.8906912367722823, 0.8909287331979486, 0.889842109505805, 0.890462173858943, 0.8904457343781548, 0.8911427314774396, 0.8915498119198656, 0.8907209552625396, 0.8909199308242476, 0.8913367860930882, 0.8910690849508079, 0.8898862387392926, 0.891958191760314, 0.8911693314124428, 0.890081475387981, 0.890446455334477, 0.8905424766567354, 0.88892921952088, 0.8901858380072227, 0.8902909467324258, 0.8895407497631347, 0.8909246421899809, 0.8908308508023934, 0.8909673295108242, 0.8916050405274296], 'loss': [1.1707259767261244, 0.189754555897344, 0.1081625713445288, 0.093315865575911, 0.08556534839758555, 0.08236914304797223, 0.08082562026790063, 0.07666216971026368, 0.07253997133634943, 0.07055938641582801, 0.06876027490004347, 0.06653673419165976, 0.06517857864504659, 0.06468456277055308, 0.06953680007915065, 0.06461183780224679, 0.0635669876389598, 0.06264658114459919, 0.059672665811148336, 0.059266861306895145, 0.05789564709121004, 0.05892243292662783, 0.057846030184903786, 0.05754140989356046, 0.056580513686520596, 0.05675440716501889, 0.05705927221085716, 0.05638971102629574, 0.055146239532333816, 0.055050986819317145, 0.05481217532019629, 0.05488774278936967, 0.055659222592268146, 0.05436760466719769, 0.05477332926905329, 0.05371649288055743, 0.0527744733651583, 0.05248203528937138, 0.05218987524520562, 0.05251514612459451, 0.05243659886431685, 0.05124540080524755, 0.05211268154835363, 0.05204342509667554, 0.0524653757674996, 0.05176704067446022, 0.051007786998786185, 0.0515666400257402, 0.05099302476573266, 0.051145121783608145, 0.0502492741437759, 0.05029539685441702, 0.0521427409505617, 0.051187162743627376, 0.050187959205282684, 0.04972094407901418, 0.04826936010392379, 0.04737543515780322, 0.04757316420700152, 0.047084642945050215, 0.04703517235066849, 0.04659883082497463, 0.04672940128834603, 0.04599640599834995, 0.04605370789883206, 0.046144325685536725, 0.04582144944835698, 0.046085460153942374, 0.04497873703930019, 0.04497228626705926, 0.044985275072674274, 0.04471451240018717, 0.04467045212130427, 0.044860655048755645, 0.04478644462198653, 0.04467334266964918, 0.04456116377105007, 0.044458227881275986, 0.04448339238144989, 0.04476900191105788, 0.04491840172691224, 0.04380948372883809, 0.04384568412389869, 0.044097271111581945, 0.04424792887739695, 0.04374340359994089, 0.044598475129224935, 0.04378246426788292], 'acc': [0.7916257170362618, 0.9847553191364903, 0.987781049312831, 0.9899991436996788, 0.9911708235072698, 0.9918055422144859, 0.9915350161250688, 0.9923213907796855, 0.9929693994301221, 0.9932366048929003, 0.993468185681481, 0.9938775690847873, 0.9940325491990356, 0.9941088255323667, 0.9937293584283537, 0.9940761410348399, 0.9943337993787481, 0.994461946804487, 0.9946964759634287, 0.9947764869942165, 0.9948742188621823, 0.994917896197365, 0.9948931663927856, 0.9949668255990506, 0.9950151608468527, 0.9950244364383998, 0.9951309383828381, 0.9950797202474019, 0.9952083242709929, 0.9952799235879373, 0.9952661140801348, 0.9953335869387989, 0.995259375237332, 0.995378850354247, 0.9953742217652425, 0.995423227559732, 0.9955536708664351, 0.9955720086795962, 0.995579288818073, 0.9956490763052213, 0.9955930232912648, 0.9956703829284386, 0.9956921691942625, 0.9956394353374243, 0.9956003148741895, 0.9956742043276086, 0.9957382597048692, 0.995715995444083, 0.9957769815049868, 0.9957536327380229, 0.9958256635151116, 0.9958545738830467, 0.9957816602135489, 0.9958934113077142, 0.9959986756041195, 0.9960745317436307, 0.9961730015113098, 0.9962542359624997, 0.9962253782969159, 0.9962685540580108, 0.9962809556760492, 0.9963032203376138, 0.9962934818693538, 0.9963378331133088, 0.9963413794894783, 0.9963377924120421, 0.9963517466230895, 0.996335218791691, 0.9964045869979032, 0.9964037488368095, 0.996394571012911, 0.9964217944157672, 0.9964235765434354, 0.9964285054044756, 0.9964259388868071, 0.9964327633403155, 0.9964401130138336, 0.9964500652750704, 0.9964612601495386, 0.996442992071808, 0.9964105723106376, 0.9964876074509519, 0.9964898443729698, 0.9964741699545072, 0.996478065475404, 0.9964998256015742, 0.9964645386499213, 0.9964872331462514], 'mDice': [0.13991577921071335, 0.7096381196122495, 0.8182190320008185, 0.8412581664170176, 0.8537292150271932, 0.8589310539628215, 0.861261489166891, 0.8682818628310445, 0.8753214066804323, 0.87863668170401, 0.8817833079123256, 0.8856387181561698, 0.8879787260398608, 0.8887759964324486, 0.8802167045834085, 0.8888914358005802, 0.8907480075549046, 0.8923292680378053, 0.8976383090108287, 0.8983626129202402, 0.900798943045427, 0.898955125749935, 0.9008820404479162, 0.9014226865714996, 0.9031616049023052, 0.9028140576330191, 0.9022797909208675, 0.9034828891894868, 0.9057217381719248, 0.9058952687790379, 0.9063132460944725, 0.9062210030194134, 0.9047778431141069, 0.9071138673724637, 0.9063490728945441, 0.9082558772035798, 0.9099452724686566, 0.9104886152292749, 0.9110213448229146, 0.9104191790243383, 0.9105780351326721, 0.9127323274119024, 0.9111483439964548, 0.9112683069496668, 0.9104842497289827, 0.9117781223759172, 0.9131580708986082, 0.9121345499357182, 0.913154196463105, 0.9128878865395191, 0.9145094902380928, 0.914405245432713, 0.9110569773122896, 0.9126855544203202, 0.9143911220709423, 0.9148462854484413, 0.9156870778300756, 0.9156691237399058, 0.9144556792894761, 0.9148632423130487, 0.9145663559859842, 0.9150810913448241, 0.914560779289029, 0.9157083309271196, 0.9154622532290935, 0.9151789909385236, 0.9155937861185, 0.9150359631475335, 0.9169631084266509, 0.9168618533385972, 0.916766144488345, 0.9172094771788109, 0.9172236285727996, 0.9168728411442583, 0.9169886985019289, 0.9171487741119433, 0.9173363112984729, 0.9175069886755489, 0.9174384811185481, 0.9169008885251082, 0.9166157662222801, 0.9185997419585298, 0.9185108901584242, 0.9180401661835101, 0.9177609522662414, 0.9186324393922908, 0.9170958229219196, 0.9185340520736216], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:00<00:07,  1.30it/s]predicting test subjects:  18%|█▊        | 2/11 [00:01<00:05,  1.59it/s]predicting test subjects:  27%|██▋       | 3/11 [00:01<00:04,  1.99it/s]predicting test subjects:  36%|███▋      | 4/11 [00:01<00:03,  2.23it/s]predicting test subjects:  45%|████▌     | 5/11 [00:01<00:02,  2.39it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:02<00:01,  2.59it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:02<00:01,  2.83it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:02<00:01,  2.94it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:03<00:00,  3.08it/s]predicting test subjects:  91%|█████████ | 10/11 [00:03<00:00,  3.18it/s]predicting test subjects: 100%|██████████| 11/11 [00:03<00:00,  3.25it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:11,  3.46it/s]predicting train subjects:   5%|▍         | 2/41 [00:00<00:11,  3.48it/s]predicting train subjects:   7%|▋         | 3/41 [00:00<00:11,  3.17it/s]predicting train subjects:  10%|▉         | 4/41 [00:01<00:11,  3.26it/s]predicting train subjects:  12%|█▏        | 5/41 [00:01<00:11,  3.19it/s]predicting train subjects:  15%|█▍        | 6/41 [00:01<00:10,  3.42it/s]predicting train subjects:  17%|█▋        | 7/41 [00:02<00:09,  3.61it/s]predicting train subjects:  20%|█▉        | 8/41 [00:02<00:08,  4.02it/s]predicting train subjects:  22%|██▏       | 9/41 [00:02<00:08,  3.90it/s]predicting train subjects:  24%|██▍       | 10/41 [00:02<00:08,  3.51it/s]predicting train subjects:  27%|██▋       | 11/41 [00:03<00:08,  3.36it/s]predicting train subjects:  29%|██▉       | 12/41 [00:03<00:07,  3.66it/s]predicting train subjects:  32%|███▏      | 13/41 [00:03<00:07,  3.76it/s]predicting train subjects:  34%|███▍      | 14/41 [00:03<00:07,  3.76it/s]predicting train subjects:  37%|███▋      | 15/41 [00:04<00:06,  4.04it/s]predicting train subjects:  39%|███▉      | 16/41 [00:04<00:06,  3.98it/s]predicting train subjects:  41%|████▏     | 17/41 [00:04<00:06,  3.55it/s]predicting train subjects:  44%|████▍     | 18/41 [00:05<00:06,  3.42it/s]predicting train subjects:  46%|████▋     | 19/41 [00:05<00:06,  3.58it/s]predicting train subjects:  49%|████▉     | 20/41 [00:05<00:05,  3.65it/s]predicting train subjects:  51%|█████     | 21/41 [00:05<00:05,  3.73it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:06<00:04,  3.83it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:06<00:04,  3.77it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:06<00:04,  3.78it/s]predicting train subjects:  61%|██████    | 25/41 [00:06<00:04,  3.75it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:07<00:04,  3.30it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:07<00:05,  2.68it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:08<00:05,  2.50it/s]predicting train subjects:  71%|███████   | 29/41 [00:08<00:04,  2.55it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:08<00:03,  2.76it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:09<00:03,  2.81it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:09<00:03,  2.71it/s]predicting train subjects:  80%|████████  | 33/41 [00:10<00:02,  2.75it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:10<00:02,  2.79it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:10<00:02,  2.74it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:11<00:01,  2.71it/s]predicting train subjects:  90%|█████████ | 37/41 [00:11<00:01,  2.80it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:11<00:01,  2.87it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:12<00:00,  3.14it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:12<00:00,  3.31it/s]predicting train subjects: 100%|██████████| 41/41 [00:12<00:00,  3.14it/s]
predicting test subjects sagittal:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects sagittal:   9%|▉         | 1/11 [00:00<00:02,  3.53it/s]predicting test subjects sagittal:  18%|█▊        | 2/11 [00:00<00:02,  3.52it/s]predicting test subjects sagittal:  27%|██▋       | 3/11 [00:00<00:02,  3.89it/s]predicting test subjects sagittal:  36%|███▋      | 4/11 [00:01<00:01,  3.72it/s]predicting test subjects sagittal:  45%|████▌     | 5/11 [00:01<00:01,  3.60it/s]predicting test subjects sagittal:  55%|█████▍    | 6/11 [00:01<00:01,  3.60it/s]predicting test subjects sagittal:  64%|██████▎   | 7/11 [00:01<00:01,  3.61it/s]predicting test subjects sagittal:  73%|███████▎  | 8/11 [00:02<00:00,  3.46it/s]predicting test subjects sagittal:  82%|████████▏ | 9/11 [00:02<00:00,  3.40it/s]predicting test subjects sagittal:  91%|█████████ | 10/11 [00:02<00:00,  3.46it/s]predicting test subjects sagittal: 100%|██████████| 11/11 [00:03<00:00,  3.44it/s]
predicting train subjects sagittal:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects sagittal:   2%|▏         | 1/41 [00:00<00:12,  3.28it/s]predicting train subjects sagittal:   5%|▍         | 2/41 [00:00<00:11,  3.34it/s]predicting train subjects sagittal:   7%|▋         | 3/41 [00:00<00:11,  3.20it/s]predicting train subjects sagittal:  10%|▉         | 4/41 [00:01<00:10,  3.50it/s]predicting train subjects sagittal:  12%|█▏        | 5/41 [00:01<00:11,  3.02it/s]predicting train subjects sagittal:  15%|█▍        | 6/41 [00:01<00:11,  3.15it/s]predicting train subjects sagittal:  17%|█▋        | 7/41 [00:02<00:09,  3.42it/s]predicting train subjects sagittal:  20%|█▉        | 8/41 [00:02<00:08,  3.74it/s]predicting train subjects sagittal:  22%|██▏       | 9/41 [00:02<00:08,  3.73it/s]predicting train subjects sagittal:  24%|██▍       | 10/41 [00:02<00:08,  3.59it/s]predicting train subjects sagittal:  27%|██▋       | 11/41 [00:03<00:08,  3.41it/s]predicting train subjects sagittal:  29%|██▉       | 12/41 [00:03<00:07,  3.80it/s]predicting train subjects sagittal:  32%|███▏      | 13/41 [00:03<00:07,  3.73it/s]predicting train subjects sagittal:  34%|███▍      | 14/41 [00:03<00:07,  3.72it/s]predicting train subjects sagittal:  37%|███▋      | 15/41 [00:04<00:06,  3.88it/s]predicting train subjects sagittal:  39%|███▉      | 16/41 [00:04<00:07,  3.53it/s]predicting train subjects sagittal:  41%|████▏     | 17/41 [00:04<00:07,  3.28it/s]predicting train subjects sagittal:  44%|████▍     | 18/41 [00:05<00:06,  3.37it/s]predicting train subjects sagittal:  46%|████▋     | 19/41 [00:05<00:06,  3.40it/s]predicting train subjects sagittal:  49%|████▉     | 20/41 [00:05<00:05,  3.56it/s]predicting train subjects sagittal:  51%|█████     | 21/41 [00:06<00:05,  3.50it/s]predicting train subjects sagittal:  54%|█████▎    | 22/41 [00:06<00:05,  3.59it/s]predicting train subjects sagittal:  56%|█████▌    | 23/41 [00:06<00:05,  3.45it/s]predicting train subjects sagittal:  59%|█████▊    | 24/41 [00:06<00:05,  3.12it/s]predicting train subjects sagittal:  61%|██████    | 25/41 [00:07<00:05,  3.11it/s]predicting train subjects sagittal:  63%|██████▎   | 26/41 [00:07<00:05,  2.93it/s]predicting train subjects sagittal:  66%|██████▌   | 27/41 [00:08<00:05,  2.58it/s]predicting train subjects sagittal:  68%|██████▊   | 28/41 [00:08<00:05,  2.50it/s]predicting train subjects sagittal:  71%|███████   | 29/41 [00:08<00:04,  2.57it/s]predicting train subjects sagittal:  73%|███████▎  | 30/41 [00:09<00:04,  2.55it/s]predicting train subjects sagittal:  76%|███████▌  | 31/41 [00:09<00:03,  2.58it/s]predicting train subjects sagittal:  78%|███████▊  | 32/41 [00:10<00:03,  2.71it/s]predicting train subjects sagittal:  80%|████████  | 33/41 [00:10<00:02,  2.76it/s]predicting train subjects sagittal:  83%|████████▎ | 34/41 [00:10<00:02,  3.06it/s]predicting train subjects sagittal:  85%|████████▌ | 35/41 [00:10<00:01,  3.27it/s]predicting train subjects sagittal:  88%|████████▊ | 36/41 [00:11<00:01,  3.13it/s]predicting train subjects sagittal:  90%|█████████ | 37/41 [00:11<00:01,  3.24it/s]predicting train subjects sagittal:  93%|█████████▎| 38/41 [00:11<00:00,  3.22it/s]predicting train subjects sagittal:  95%|█████████▌| 39/41 [00:12<00:00,  3.32it/s]predicting train subjects sagittal:  98%|█████████▊| 40/41 [00:12<00:00,  3.28it/s]predicting train subjects sagittal: 100%|██████████| 41/41 [00:12<00:00,  3.22it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:02<01:28,  2.21s/it]Loading train:   5%|▍         | 2/41 [00:04<01:24,  2.16s/it]Loading train:   7%|▋         | 3/41 [00:06<01:18,  2.08s/it]Loading train:  10%|▉         | 4/41 [00:08<01:17,  2.08s/it]Loading train:  12%|█▏        | 5/41 [00:10<01:18,  2.19s/it]Loading train:  15%|█▍        | 6/41 [00:12<01:14,  2.14s/it]Loading train:  17%|█▋        | 7/41 [00:14<01:09,  2.04s/it]Loading train:  20%|█▉        | 8/41 [00:16<01:05,  1.98s/it]Loading train:  22%|██▏       | 9/41 [00:18<01:03,  2.00s/it]Loading train:  24%|██▍       | 10/41 [00:20<01:03,  2.04s/it]Loading train:  27%|██▋       | 11/41 [00:22<01:04,  2.14s/it]Loading train:  29%|██▉       | 12/41 [00:24<00:58,  2.02s/it]Loading train:  32%|███▏      | 13/41 [00:26<00:56,  2.02s/it]Loading train:  34%|███▍      | 14/41 [00:28<00:56,  2.08s/it]Loading train:  37%|███▋      | 15/41 [00:30<00:52,  2.04s/it]Loading train:  39%|███▉      | 16/41 [00:32<00:49,  1.98s/it]Loading train:  41%|████▏     | 17/41 [00:35<00:52,  2.19s/it]Loading train:  44%|████▍     | 18/41 [00:37<00:50,  2.21s/it]Loading train:  46%|████▋     | 19/41 [00:39<00:49,  2.26s/it]Loading train:  49%|████▉     | 20/41 [00:41<00:45,  2.14s/it]Loading train:  51%|█████     | 21/41 [00:43<00:42,  2.11s/it]Loading train:  54%|█████▎    | 22/41 [00:46<00:41,  2.17s/it]Loading train:  56%|█████▌    | 23/41 [00:48<00:38,  2.11s/it]Loading train:  59%|█████▊    | 24/41 [00:50<00:36,  2.13s/it]Loading train:  61%|██████    | 25/41 [00:52<00:33,  2.11s/it]Loading train:  63%|██████▎   | 26/41 [00:54<00:33,  2.25s/it]Loading train:  66%|██████▌   | 27/41 [00:58<00:35,  2.51s/it]Loading train:  68%|██████▊   | 28/41 [01:01<00:35,  2.71s/it]Loading train:  71%|███████   | 29/41 [01:03<00:31,  2.59s/it]Loading train:  73%|███████▎  | 30/41 [01:05<00:27,  2.51s/it]Loading train:  76%|███████▌  | 31/41 [01:08<00:23,  2.38s/it]Loading train:  78%|███████▊  | 32/41 [01:10<00:21,  2.43s/it]Loading train:  80%|████████  | 33/41 [01:12<00:19,  2.38s/it]Loading train:  83%|████████▎ | 34/41 [01:14<00:15,  2.25s/it]Loading train:  85%|████████▌ | 35/41 [01:17<00:13,  2.27s/it]Loading train:  88%|████████▊ | 36/41 [01:18<00:10,  2.14s/it]Loading train:  90%|█████████ | 37/41 [01:21<00:08,  2.12s/it]Loading train:  93%|█████████▎| 38/41 [01:22<00:06,  2.06s/it]Loading train:  95%|█████████▌| 39/41 [01:24<00:04,  2.06s/it]Loading train:  98%|█████████▊| 40/41 [01:27<00:02,  2.10s/it]Loading train: 100%|██████████| 41/41 [01:29<00:00,  2.20s/it]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:  17%|█▋        | 7/41 [00:00<00:00, 67.28it/s]concatenating: train:  37%|███▋      | 15/41 [00:00<00:00, 69.14it/s]concatenating: train:  56%|█████▌    | 23/41 [00:00<00:00, 68.56it/s]concatenating: train:  73%|███████▎  | 30/41 [00:00<00:00, 68.59it/s]concatenating: train:  95%|█████████▌| 39/41 [00:00<00:00, 73.76it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 74.29it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:01<00:16,  1.62s/it]Loading test:  18%|█▊        | 2/11 [00:03<00:15,  1.75s/it]Loading test:  27%|██▋       | 3/11 [00:05<00:14,  1.84s/it]Loading test:  36%|███▋      | 4/11 [00:07<00:13,  1.90s/it]Loading test:  45%|████▌     | 5/11 [00:10<00:12,  2.08s/it]Loading test:  55%|█████▍    | 6/11 [00:12<00:10,  2.06s/it]Loading test:  64%|██████▎   | 7/11 [00:14<00:07,  1.97s/it]Loading test:  73%|███████▎  | 8/11 [00:16<00:06,  2.02s/it]Loading test:  82%|████████▏ | 9/11 [00:18<00:04,  2.02s/it]Loading test:  91%|█████████ | 10/11 [00:20<00:02,  2.08s/it]Loading test: 100%|██████████| 11/11 [00:22<00:00,  2.07s/it]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  27%|██▋       | 3/11 [00:00<00:00, 20.76it/s]concatenating: validation:  55%|█████▍    | 6/11 [00:00<00:00, 21.45it/s]concatenating: validation:  91%|█████████ | 10/11 [00:00<00:00, 24.85it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 26.54it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 128, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 128, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 128, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 128, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 128, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 128, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 128, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 64, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 64, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 64, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 64, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 64, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 64, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 64, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 64, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 64, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 32, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 32, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 32, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 32, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 32, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 32, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 32, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 32, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 32, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 32, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 64, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 64, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 64, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 64, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 64, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 64, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 64, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 64, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 64, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-28 22:53:31.328270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-28 22:53:31.328353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 22:53:31.328369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-28 22:53:31.328378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-28 22:53:31.328758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 58, 64, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 128, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 128, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 128, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 128, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 128, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 128, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 128, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 128, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 128, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 128, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 128, 13) 793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.54297164e-02 3.09682801e-02 7.58749556e-02 1.02386423e-02
 2.77640118e-02 7.04458794e-03 7.68871037e-02 1.11754580e-01
 7.79224397e-02 1.35776983e-02 3.25520904e-01 1.76979929e-01
 3.71512466e-05]
Train on 2677 samples, validate on 711 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 15s - loss: 5.6329 - acc: 0.3335 - mDice: 0.0046 - val_loss: 5.6935 - val_acc: 0.4966 - val_mDice: 0.0065

Epoch 00001: val_mDice improved from -inf to 0.00650, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 4.0776 - acc: 0.7837 - mDice: 0.0332 - val_loss: 3.4100 - val_acc: 0.9593 - val_mDice: 0.0636

Epoch 00002: val_mDice improved from 0.00650 to 0.06363, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 2.5402 - acc: 0.9601 - mDice: 0.1007 - val_loss: 2.8646 - val_acc: 0.9668 - val_mDice: 0.1004

Epoch 00003: val_mDice improved from 0.06363 to 0.10042, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 1.7987 - acc: 0.9813 - mDice: 0.1759 - val_loss: 1.9590 - val_acc: 0.9805 - val_mDice: 0.2046

Epoch 00004: val_mDice improved from 0.10042 to 0.20460, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 1.3301 - acc: 0.9836 - mDice: 0.2659 - val_loss: 1.4187 - val_acc: 0.9830 - val_mDice: 0.3519

Epoch 00005: val_mDice improved from 0.20460 to 0.35192, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 1.0741 - acc: 0.9843 - mDice: 0.3315 - val_loss: 1.4430 - val_acc: 0.9832 - val_mDice: 0.3947

Epoch 00006: val_mDice improved from 0.35192 to 0.39470, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.9126 - acc: 0.9848 - mDice: 0.3855 - val_loss: 1.5002 - val_acc: 0.9833 - val_mDice: 0.4201

Epoch 00007: val_mDice improved from 0.39470 to 0.42010, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.8003 - acc: 0.9852 - mDice: 0.4301 - val_loss: 1.2291 - val_acc: 0.9840 - val_mDice: 0.4830

Epoch 00008: val_mDice improved from 0.42010 to 0.48295, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.7175 - acc: 0.9854 - mDice: 0.4683 - val_loss: 1.3334 - val_acc: 0.9836 - val_mDice: 0.4838

Epoch 00009: val_mDice improved from 0.48295 to 0.48384, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.6570 - acc: 0.9857 - mDice: 0.4975 - val_loss: 1.3876 - val_acc: 0.9840 - val_mDice: 0.5053

Epoch 00010: val_mDice improved from 0.48384 to 0.50533, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.6180 - acc: 0.9858 - mDice: 0.5184 - val_loss: 1.2714 - val_acc: 0.9839 - val_mDice: 0.5255

Epoch 00011: val_mDice improved from 0.50533 to 0.52550, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.5862 - acc: 0.9861 - mDice: 0.5356 - val_loss: 1.2731 - val_acc: 0.9840 - val_mDice: 0.5331

Epoch 00012: val_mDice improved from 0.52550 to 0.53310, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.5586 - acc: 0.9862 - mDice: 0.5519 - val_loss: 1.0849 - val_acc: 0.9846 - val_mDice: 0.5522

Epoch 00013: val_mDice improved from 0.53310 to 0.55220, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.5404 - acc: 0.9864 - mDice: 0.5635 - val_loss: 0.9906 - val_acc: 0.9843 - val_mDice: 0.5457

Epoch 00014: val_mDice did not improve from 0.55220
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.5112 - acc: 0.9867 - mDice: 0.5801 - val_loss: 0.8114 - val_acc: 0.9849 - val_mDice: 0.5725

Epoch 00015: val_mDice improved from 0.55220 to 0.57251, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.4904 - acc: 0.9869 - mDice: 0.5928 - val_loss: 0.7900 - val_acc: 0.9857 - val_mDice: 0.5852

Epoch 00016: val_mDice improved from 0.57251 to 0.58522, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.4647 - acc: 0.9871 - mDice: 0.6084 - val_loss: 0.8651 - val_acc: 0.9857 - val_mDice: 0.5755

Epoch 00017: val_mDice did not improve from 0.58522
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.4613 - acc: 0.9873 - mDice: 0.6114 - val_loss: 0.8228 - val_acc: 0.9869 - val_mDice: 0.5812

Epoch 00018: val_mDice did not improve from 0.58522
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4361 - acc: 0.9875 - mDice: 0.6277 - val_loss: 0.7820 - val_acc: 0.9873 - val_mDice: 0.6010

Epoch 00019: val_mDice improved from 0.58522 to 0.60105, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4268 - acc: 0.9877 - mDice: 0.6337 - val_loss: 0.7369 - val_acc: 0.9875 - val_mDice: 0.6100

Epoch 00020: val_mDice improved from 0.60105 to 0.60999, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4233 - acc: 0.9878 - mDice: 0.6361 - val_loss: 0.7239 - val_acc: 0.9875 - val_mDice: 0.6175

Epoch 00021: val_mDice improved from 0.60999 to 0.61748, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4202 - acc: 0.9878 - mDice: 0.6383 - val_loss: 0.6859 - val_acc: 0.9876 - val_mDice: 0.6299

Epoch 00022: val_mDice improved from 0.61748 to 0.62987, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4098 - acc: 0.9880 - mDice: 0.6453 - val_loss: 0.6981 - val_acc: 0.9881 - val_mDice: 0.6305

Epoch 00023: val_mDice improved from 0.62987 to 0.63049, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4021 - acc: 0.9880 - mDice: 0.6509 - val_loss: 0.7053 - val_acc: 0.9871 - val_mDice: 0.6231

Epoch 00024: val_mDice did not improve from 0.63049
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4006 - acc: 0.9881 - mDice: 0.6519 - val_loss: 0.6799 - val_acc: 0.9876 - val_mDice: 0.6330

Epoch 00025: val_mDice improved from 0.63049 to 0.63297, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3942 - acc: 0.9882 - mDice: 0.6561 - val_loss: 0.6797 - val_acc: 0.9885 - val_mDice: 0.6360

Epoch 00026: val_mDice improved from 0.63297 to 0.63598, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3897 - acc: 0.9882 - mDice: 0.6593 - val_loss: 0.6721 - val_acc: 0.9882 - val_mDice: 0.6376

Epoch 00027: val_mDice improved from 0.63598 to 0.63759, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3912 - acc: 0.9883 - mDice: 0.6590 - val_loss: 0.7080 - val_acc: 0.9878 - val_mDice: 0.6254

Epoch 00028: val_mDice did not improve from 0.63759
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3872 - acc: 0.9883 - mDice: 0.6619 - val_loss: 0.6996 - val_acc: 0.9885 - val_mDice: 0.6267

Epoch 00029: val_mDice did not improve from 0.63759
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3784 - acc: 0.9885 - mDice: 0.6681 - val_loss: 0.6835 - val_acc: 0.9886 - val_mDice: 0.6343

Epoch 00030: val_mDice did not improve from 0.63759
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3835 - acc: 0.9884 - mDice: 0.6638 - val_loss: 0.6864 - val_acc: 0.9878 - val_mDice: 0.6380

Epoch 00031: val_mDice improved from 0.63759 to 0.63798, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3724 - acc: 0.9885 - mDice: 0.6717 - val_loss: 0.6672 - val_acc: 0.9886 - val_mDice: 0.6417

Epoch 00032: val_mDice improved from 0.63798 to 0.64172, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3755 - acc: 0.9885 - mDice: 0.6695 - val_loss: 0.6475 - val_acc: 0.9885 - val_mDice: 0.6383

Epoch 00033: val_mDice did not improve from 0.64172
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3669 - acc: 0.9887 - mDice: 0.6759 - val_loss: 0.6029 - val_acc: 0.9895 - val_mDice: 0.6433

Epoch 00034: val_mDice improved from 0.64172 to 0.64333, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3683 - acc: 0.9888 - mDice: 0.6750 - val_loss: 0.6684 - val_acc: 0.9888 - val_mDice: 0.6356

Epoch 00035: val_mDice did not improve from 0.64333
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3586 - acc: 0.9887 - mDice: 0.6816 - val_loss: 0.6601 - val_acc: 0.9896 - val_mDice: 0.6459

Epoch 00036: val_mDice improved from 0.64333 to 0.64588, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3558 - acc: 0.9889 - mDice: 0.6840 - val_loss: 0.6666 - val_acc: 0.9890 - val_mDice: 0.6418

Epoch 00037: val_mDice did not improve from 0.64588
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3508 - acc: 0.9889 - mDice: 0.6875 - val_loss: 0.6445 - val_acc: 0.9895 - val_mDice: 0.6499

Epoch 00038: val_mDice improved from 0.64588 to 0.64987, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3529 - acc: 0.9890 - mDice: 0.6858 - val_loss: 0.6468 - val_acc: 0.9894 - val_mDice: 0.6486

Epoch 00039: val_mDice did not improve from 0.64987
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3494 - acc: 0.9890 - mDice: 0.6890 - val_loss: 0.6350 - val_acc: 0.9896 - val_mDice: 0.6506

Epoch 00040: val_mDice improved from 0.64987 to 0.65058, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3495 - acc: 0.9891 - mDice: 0.6888 - val_loss: 0.6503 - val_acc: 0.9898 - val_mDice: 0.6480

Epoch 00041: val_mDice did not improve from 0.65058
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3448 - acc: 0.9891 - mDice: 0.6918 - val_loss: 0.6168 - val_acc: 0.9899 - val_mDice: 0.6479

Epoch 00042: val_mDice did not improve from 0.65058
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3456 - acc: 0.9892 - mDice: 0.6912 - val_loss: 0.6075 - val_acc: 0.9897 - val_mDice: 0.6524

Epoch 00043: val_mDice improved from 0.65058 to 0.65240, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3445 - acc: 0.9892 - mDice: 0.6919 - val_loss: 0.6043 - val_acc: 0.9903 - val_mDice: 0.6555

Epoch 00044: val_mDice improved from 0.65240 to 0.65547, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3415 - acc: 0.9892 - mDice: 0.6943 - val_loss: 0.5939 - val_acc: 0.9903 - val_mDice: 0.6557

Epoch 00045: val_mDice improved from 0.65547 to 0.65565, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3426 - acc: 0.9892 - mDice: 0.6935 - val_loss: 0.6403 - val_acc: 0.9900 - val_mDice: 0.6473

Epoch 00046: val_mDice did not improve from 0.65565
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3404 - acc: 0.9892 - mDice: 0.6949 - val_loss: 0.6346 - val_acc: 0.9905 - val_mDice: 0.6518

Epoch 00047: val_mDice did not improve from 0.65565
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3370 - acc: 0.9894 - mDice: 0.6978 - val_loss: 0.6113 - val_acc: 0.9907 - val_mDice: 0.6536

Epoch 00048: val_mDice did not improve from 0.65565
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3401 - acc: 0.9893 - mDice: 0.6956 - val_loss: 0.5716 - val_acc: 0.9902 - val_mDice: 0.6564

Epoch 00049: val_mDice improved from 0.65565 to 0.65635, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3370 - acc: 0.9893 - mDice: 0.6976 - val_loss: 0.5589 - val_acc: 0.9902 - val_mDice: 0.6558

Epoch 00050: val_mDice did not improve from 0.65635
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3351 - acc: 0.9893 - mDice: 0.6988 - val_loss: 0.5931 - val_acc: 0.9907 - val_mDice: 0.6563

Epoch 00051: val_mDice did not improve from 0.65635
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3312 - acc: 0.9893 - mDice: 0.7019 - val_loss: 0.5627 - val_acc: 0.9906 - val_mDice: 0.6568

Epoch 00052: val_mDice improved from 0.65635 to 0.65683, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3362 - acc: 0.9894 - mDice: 0.6983 - val_loss: 0.5430 - val_acc: 0.9908 - val_mDice: 0.6568

Epoch 00053: val_mDice did not improve from 0.65683
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3327 - acc: 0.9895 - mDice: 0.7008 - val_loss: 0.5421 - val_acc: 0.9906 - val_mDice: 0.6605

Epoch 00054: val_mDice improved from 0.65683 to 0.66054, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3266 - acc: 0.9895 - mDice: 0.7051 - val_loss: 0.5529 - val_acc: 0.9907 - val_mDice: 0.6586

Epoch 00055: val_mDice did not improve from 0.66054
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3277 - acc: 0.9895 - mDice: 0.7044 - val_loss: 0.5660 - val_acc: 0.9906 - val_mDice: 0.6562

Epoch 00056: val_mDice did not improve from 0.66054
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3274 - acc: 0.9896 - mDice: 0.7050 - val_loss: 0.5645 - val_acc: 0.9907 - val_mDice: 0.6575

Epoch 00057: val_mDice did not improve from 0.66054
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3228 - acc: 0.9896 - mDice: 0.7080 - val_loss: 0.5685 - val_acc: 0.9907 - val_mDice: 0.6562

Epoch 00058: val_mDice did not improve from 0.66054
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3261 - acc: 0.9896 - mDice: 0.7056 - val_loss: 0.5682 - val_acc: 0.9907 - val_mDice: 0.6608

Epoch 00059: val_mDice improved from 0.66054 to 0.66078, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3278 - acc: 0.9895 - mDice: 0.7047 - val_loss: 0.5408 - val_acc: 0.9909 - val_mDice: 0.6603

Epoch 00060: val_mDice did not improve from 0.66078
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3223 - acc: 0.9897 - mDice: 0.7085 - val_loss: 0.5583 - val_acc: 0.9910 - val_mDice: 0.6592

Epoch 00061: val_mDice did not improve from 0.66078
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3237 - acc: 0.9897 - mDice: 0.7075 - val_loss: 0.5420 - val_acc: 0.9909 - val_mDice: 0.6567

Epoch 00062: val_mDice did not improve from 0.66078
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3230 - acc: 0.9897 - mDice: 0.7082 - val_loss: 0.5318 - val_acc: 0.9908 - val_mDice: 0.6578

Epoch 00063: val_mDice did not improve from 0.66078
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3228 - acc: 0.9897 - mDice: 0.7084 - val_loss: 0.5426 - val_acc: 0.9909 - val_mDice: 0.6607

Epoch 00064: val_mDice did not improve from 0.66078
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3206 - acc: 0.9897 - mDice: 0.7098 - val_loss: 0.5758 - val_acc: 0.9907 - val_mDice: 0.6567

Epoch 00065: val_mDice did not improve from 0.66078
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3242 - acc: 0.9898 - mDice: 0.7079 - val_loss: 0.5594 - val_acc: 0.9908 - val_mDice: 0.6570

Epoch 00066: val_mDice did not improve from 0.66078
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3206 - acc: 0.9897 - mDice: 0.7099 - val_loss: 0.5509 - val_acc: 0.9911 - val_mDice: 0.6597

Epoch 00067: val_mDice did not improve from 0.66078
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3205 - acc: 0.9897 - mDice: 0.7100 - val_loss: 0.5369 - val_acc: 0.9912 - val_mDice: 0.6553

Epoch 00068: val_mDice did not improve from 0.66078
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3194 - acc: 0.9898 - mDice: 0.7107 - val_loss: 0.5191 - val_acc: 0.9912 - val_mDice: 0.6596

Epoch 00069: val_mDice did not improve from 0.66078
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3178 - acc: 0.9897 - mDice: 0.7120 - val_loss: 0.5343 - val_acc: 0.9911 - val_mDice: 0.6601

Epoch 00070: val_mDice did not improve from 0.66078
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3182 - acc: 0.9898 - mDice: 0.7116 - val_loss: 0.5401 - val_acc: 0.9911 - val_mDice: 0.6614

Epoch 00071: val_mDice improved from 0.66078 to 0.66136, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3150 - acc: 0.9898 - mDice: 0.7140 - val_loss: 0.5121 - val_acc: 0.9909 - val_mDice: 0.6608

Epoch 00072: val_mDice did not improve from 0.66136
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3153 - acc: 0.9898 - mDice: 0.7141 - val_loss: 0.5047 - val_acc: 0.9910 - val_mDice: 0.6616

Epoch 00073: val_mDice improved from 0.66136 to 0.66156, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3154 - acc: 0.9898 - mDice: 0.7138 - val_loss: 0.4711 - val_acc: 0.9910 - val_mDice: 0.6627

Epoch 00074: val_mDice improved from 0.66156 to 0.66274, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3159 - acc: 0.9898 - mDice: 0.7135 - val_loss: 0.5037 - val_acc: 0.9910 - val_mDice: 0.6587

Epoch 00075: val_mDice did not improve from 0.66274
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3145 - acc: 0.9899 - mDice: 0.7145 - val_loss: 0.4825 - val_acc: 0.9912 - val_mDice: 0.6616

Epoch 00076: val_mDice did not improve from 0.66274
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3132 - acc: 0.9900 - mDice: 0.7160 - val_loss: 0.4594 - val_acc: 0.9912 - val_mDice: 0.6609

Epoch 00077: val_mDice did not improve from 0.66274
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3142 - acc: 0.9900 - mDice: 0.7149 - val_loss: 0.4515 - val_acc: 0.9913 - val_mDice: 0.6618

Epoch 00078: val_mDice did not improve from 0.66274
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3115 - acc: 0.9899 - mDice: 0.7166 - val_loss: 0.4483 - val_acc: 0.9914 - val_mDice: 0.6633

Epoch 00079: val_mDice improved from 0.66274 to 0.66332, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3128 - acc: 0.9899 - mDice: 0.7158 - val_loss: 0.4549 - val_acc: 0.9914 - val_mDice: 0.6622

Epoch 00080: val_mDice did not improve from 0.66332
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3121 - acc: 0.9900 - mDice: 0.7163 - val_loss: 0.4518 - val_acc: 0.9913 - val_mDice: 0.6625

Epoch 00081: val_mDice did not improve from 0.66332
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3120 - acc: 0.9900 - mDice: 0.7165 - val_loss: 0.4489 - val_acc: 0.9913 - val_mDice: 0.6624

Epoch 00082: val_mDice did not improve from 0.66332
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3139 - acc: 0.9900 - mDice: 0.7152 - val_loss: 0.4507 - val_acc: 0.9914 - val_mDice: 0.6632

Epoch 00083: val_mDice did not improve from 0.66332
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3117 - acc: 0.9901 - mDice: 0.7167 - val_loss: 0.4438 - val_acc: 0.9914 - val_mDice: 0.6633

Epoch 00084: val_mDice did not improve from 0.66332
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3096 - acc: 0.9900 - mDice: 0.7182 - val_loss: 0.4456 - val_acc: 0.9913 - val_mDice: 0.6619

Epoch 00085: val_mDice did not improve from 0.66332
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3090 - acc: 0.9900 - mDice: 0.7188 - val_loss: 0.4445 - val_acc: 0.9914 - val_mDice: 0.6620

Epoch 00086: val_mDice did not improve from 0.66332
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3072 - acc: 0.9900 - mDice: 0.7199 - val_loss: 0.4394 - val_acc: 0.9914 - val_mDice: 0.6645

Epoch 00087: val_mDice improved from 0.66332 to 0.66450, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 88/300

Epoch 00088: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3130 - acc: 0.9901 - mDice: 0.7157 - val_loss: 0.4317 - val_acc: 0.9914 - val_mDice: 0.6665

Epoch 00088: val_mDice improved from 0.66450 to 0.66646, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 89/300

Epoch 00089: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3079 - acc: 0.9901 - mDice: 0.7195 - val_loss: 0.4334 - val_acc: 0.9914 - val_mDice: 0.6646

Epoch 00089: val_mDice did not improve from 0.66646
Epoch 90/300

Epoch 00090: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3080 - acc: 0.9901 - mDice: 0.7192 - val_loss: 0.4323 - val_acc: 0.9914 - val_mDice: 0.6656

Epoch 00090: val_mDice did not improve from 0.66646
Epoch 91/300

Epoch 00091: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3088 - acc: 0.9901 - mDice: 0.7189 - val_loss: 0.4281 - val_acc: 0.9914 - val_mDice: 0.6664

Epoch 00091: val_mDice did not improve from 0.66646
Epoch 92/300

Epoch 00092: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3090 - acc: 0.9901 - mDice: 0.7185 - val_loss: 0.4268 - val_acc: 0.9914 - val_mDice: 0.6664

Epoch 00092: val_mDice did not improve from 0.66646
Epoch 93/300

Epoch 00093: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3061 - acc: 0.9901 - mDice: 0.7209 - val_loss: 0.4272 - val_acc: 0.9915 - val_mDice: 0.6661

Epoch 00093: val_mDice did not improve from 0.66646
Epoch 94/300

Epoch 00094: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3095 - acc: 0.9901 - mDice: 0.7184 - val_loss: 0.4280 - val_acc: 0.9915 - val_mDice: 0.6659

Epoch 00094: val_mDice did not improve from 0.66646
Epoch 95/300

Epoch 00095: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3072 - acc: 0.9902 - mDice: 0.7199 - val_loss: 0.4254 - val_acc: 0.9915 - val_mDice: 0.6677

Epoch 00095: val_mDice improved from 0.66646 to 0.66771, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 96/300

Epoch 00096: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3095 - acc: 0.9901 - mDice: 0.7183 - val_loss: 0.4234 - val_acc: 0.9915 - val_mDice: 0.6681

Epoch 00096: val_mDice improved from 0.66771 to 0.66813, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 97/300

Epoch 00097: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3125 - acc: 0.9901 - mDice: 0.7164 - val_loss: 0.4223 - val_acc: 0.9915 - val_mDice: 0.6687

Epoch 00097: val_mDice improved from 0.66813 to 0.66873, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 98/300

Epoch 00098: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3055 - acc: 0.9902 - mDice: 0.7212 - val_loss: 0.4232 - val_acc: 0.9914 - val_mDice: 0.6678

Epoch 00098: val_mDice did not improve from 0.66873
Epoch 99/300

Epoch 00099: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3088 - acc: 0.9901 - mDice: 0.7189 - val_loss: 0.4199 - val_acc: 0.9915 - val_mDice: 0.6697

Epoch 00099: val_mDice improved from 0.66873 to 0.66966, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 100/300

Epoch 00100: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3067 - acc: 0.9901 - mDice: 0.7207 - val_loss: 0.4234 - val_acc: 0.9915 - val_mDice: 0.6694

Epoch 00100: val_mDice did not improve from 0.66966
Epoch 101/300

Epoch 00101: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3062 - acc: 0.9902 - mDice: 0.7209 - val_loss: 0.4240 - val_acc: 0.9915 - val_mDice: 0.6697

Epoch 00101: val_mDice improved from 0.66966 to 0.66974, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 102/300

Epoch 00102: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3076 - acc: 0.9901 - mDice: 0.7195 - val_loss: 0.4229 - val_acc: 0.9915 - val_mDice: 0.6699

Epoch 00102: val_mDice improved from 0.66974 to 0.66989, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 103/300

Epoch 00103: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3068 - acc: 0.9902 - mDice: 0.7204 - val_loss: 0.4260 - val_acc: 0.9915 - val_mDice: 0.6697

Epoch 00103: val_mDice did not improve from 0.66989
Epoch 104/300

Epoch 00104: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3078 - acc: 0.9901 - mDice: 0.7196 - val_loss: 0.4230 - val_acc: 0.9915 - val_mDice: 0.6697

Epoch 00104: val_mDice did not improve from 0.66989
Epoch 105/300

Epoch 00105: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3074 - acc: 0.9902 - mDice: 0.7198 - val_loss: 0.4184 - val_acc: 0.9915 - val_mDice: 0.6708

Epoch 00105: val_mDice improved from 0.66989 to 0.67077, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 106/300

Epoch 00106: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3061 - acc: 0.9902 - mDice: 0.7209 - val_loss: 0.4206 - val_acc: 0.9916 - val_mDice: 0.6701

Epoch 00106: val_mDice did not improve from 0.67077
Epoch 107/300

Epoch 00107: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3055 - acc: 0.9902 - mDice: 0.7213 - val_loss: 0.4203 - val_acc: 0.9916 - val_mDice: 0.6710

Epoch 00107: val_mDice improved from 0.67077 to 0.67105, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 108/300

Epoch 00108: LearningRateScheduler setting learning rate to 3.125e-05.
 - 9s - loss: 0.3041 - acc: 0.9903 - mDice: 0.7227 - val_loss: 0.4221 - val_acc: 0.9916 - val_mDice: 0.6723

Epoch 00108: val_mDice improved from 0.67105 to 0.67234, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 109/300

Epoch 00109: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3046 - acc: 0.9903 - mDice: 0.7220 - val_loss: 0.4247 - val_acc: 0.9916 - val_mDice: 0.6716

Epoch 00109: val_mDice did not improve from 0.67234
Epoch 110/300

Epoch 00110: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3024 - acc: 0.9902 - mDice: 0.7235 - val_loss: 0.4227 - val_acc: 0.9916 - val_mDice: 0.6718

Epoch 00110: val_mDice did not improve from 0.67234
Epoch 111/300

Epoch 00111: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3046 - acc: 0.9902 - mDice: 0.7220 - val_loss: 0.4215 - val_acc: 0.9917 - val_mDice: 0.6721

Epoch 00111: val_mDice did not improve from 0.67234
Epoch 112/300

Epoch 00112: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3066 - acc: 0.9902 - mDice: 0.7206 - val_loss: 0.4224 - val_acc: 0.9917 - val_mDice: 0.6720

Epoch 00112: val_mDice did not improve from 0.67234
Epoch 113/300

Epoch 00113: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3029 - acc: 0.9902 - mDice: 0.7233 - val_loss: 0.4224 - val_acc: 0.9916 - val_mDice: 0.6717

Epoch 00113: val_mDice did not improve from 0.67234
Epoch 114/300

Epoch 00114: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3031 - acc: 0.9902 - mDice: 0.7230 - val_loss: 0.4203 - val_acc: 0.9917 - val_mDice: 0.6716

Epoch 00114: val_mDice did not improve from 0.67234
Epoch 115/300

Epoch 00115: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3052 - acc: 0.9902 - mDice: 0.7218 - val_loss: 0.4199 - val_acc: 0.9917 - val_mDice: 0.6720

Epoch 00115: val_mDice did not improve from 0.67234
Epoch 116/300

Epoch 00116: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3062 - acc: 0.9902 - mDice: 0.7209 - val_loss: 0.4196 - val_acc: 0.9916 - val_mDice: 0.6716

Epoch 00116: val_mDice did not improve from 0.67234
Epoch 117/300

Epoch 00117: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3030 - acc: 0.9903 - mDice: 0.7231 - val_loss: 0.4221 - val_acc: 0.9916 - val_mDice: 0.6710

Epoch 00117: val_mDice did not improve from 0.67234
Epoch 118/300

Epoch 00118: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3030 - acc: 0.9902 - mDice: 0.7231 - val_loss: 0.4203 - val_acc: 0.9916 - val_mDice: 0.6718

Epoch 00118: val_mDice did not improve from 0.67234
Epoch 119/300

Epoch 00119: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3049 - acc: 0.9902 - mDice: 0.7218 - val_loss: 0.4212 - val_acc: 0.9916 - val_mDice: 0.6716

Epoch 00119: val_mDice did not improve from 0.67234
Epoch 120/300

Epoch 00120: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3027 - acc: 0.9903 - mDice: 0.7234 - val_loss: 0.4216 - val_acc: 0.9916 - val_mDice: 0.6717

Epoch 00120: val_mDice did not improve from 0.67234
Epoch 121/300

Epoch 00121: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3055 - acc: 0.9903 - mDice: 0.7213 - val_loss: 0.4207 - val_acc: 0.9916 - val_mDice: 0.6716

Epoch 00121: val_mDice did not improve from 0.67234
Epoch 122/300

Epoch 00122: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3042 - acc: 0.9903 - mDice: 0.7224 - val_loss: 0.4195 - val_acc: 0.9917 - val_mDice: 0.6723

Epoch 00122: val_mDice did not improve from 0.67234
Epoch 123/300

Epoch 00123: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3047 - acc: 0.9902 - mDice: 0.7221 - val_loss: 0.4189 - val_acc: 0.9917 - val_mDice: 0.6719

Epoch 00123: val_mDice did not improve from 0.67234
Epoch 124/300

Epoch 00124: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3008 - acc: 0.9903 - mDice: 0.7249 - val_loss: 0.4218 - val_acc: 0.9917 - val_mDice: 0.6713

Epoch 00124: val_mDice did not improve from 0.67234
Epoch 125/300

Epoch 00125: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3086 - acc: 0.9903 - mDice: 0.7209 - val_loss: 0.4208 - val_acc: 0.9916 - val_mDice: 0.6715

Epoch 00125: val_mDice did not improve from 0.67234
Epoch 126/300

Epoch 00126: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 9s - loss: 0.3046 - acc: 0.9903 - mDice: 0.7221 - val_loss: 0.4188 - val_acc: 0.9916 - val_mDice: 0.6720

Epoch 00126: val_mDice did not improve from 0.67234
Epoch 127/300

Epoch 00127: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3008 - acc: 0.9903 - mDice: 0.7249 - val_loss: 0.4217 - val_acc: 0.9917 - val_mDice: 0.6723

Epoch 00127: val_mDice did not improve from 0.67234
Epoch 128/300

Epoch 00128: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.2996 - acc: 0.9903 - mDice: 0.7257 - val_loss: 0.4216 - val_acc: 0.9917 - val_mDice: 0.6720

Epoch 00128: val_mDice did not improve from 0.67234
Epoch 129/300

Epoch 00129: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3038 - acc: 0.9903 - mDice: 0.7226 - val_loss: 0.4204 - val_acc: 0.9917 - val_mDice: 0.6723

Epoch 00129: val_mDice did not improve from 0.67234
Epoch 130/300

Epoch 00130: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3038 - acc: 0.9903 - mDice: 0.7228 - val_loss: 0.4207 - val_acc: 0.9917 - val_mDice: 0.6723

Epoch 00130: val_mDice did not improve from 0.67234
Epoch 131/300

Epoch 00131: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3034 - acc: 0.9903 - mDice: 0.7230 - val_loss: 0.4202 - val_acc: 0.9917 - val_mDice: 0.6720

Epoch 00131: val_mDice did not improve from 0.67234
Epoch 132/300

Epoch 00132: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3074 - acc: 0.9903 - mDice: 0.7213 - val_loss: 0.4199 - val_acc: 0.9917 - val_mDice: 0.6720

Epoch 00132: val_mDice did not improve from 0.67234
Epoch 133/300

Epoch 00133: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3015 - acc: 0.9903 - mDice: 0.7242 - val_loss: 0.4186 - val_acc: 0.9917 - val_mDice: 0.6722

Epoch 00133: val_mDice did not improve from 0.67234
Epoch 134/300

Epoch 00134: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3016 - acc: 0.9903 - mDice: 0.7244 - val_loss: 0.4195 - val_acc: 0.9917 - val_mDice: 0.6723

Epoch 00134: val_mDice did not improve from 0.67234
Epoch 135/300

Epoch 00135: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3007 - acc: 0.9903 - mDice: 0.7248 - val_loss: 0.4198 - val_acc: 0.9917 - val_mDice: 0.6725

Epoch 00135: val_mDice improved from 0.67234 to 0.67248, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 136/300

Epoch 00136: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3011 - acc: 0.9903 - mDice: 0.7244 - val_loss: 0.4206 - val_acc: 0.9917 - val_mDice: 0.6722

Epoch 00136: val_mDice did not improve from 0.67248
Epoch 137/300

Epoch 00137: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3083 - acc: 0.9903 - mDice: 0.7198 - val_loss: 0.4188 - val_acc: 0.9917 - val_mDice: 0.6725

Epoch 00137: val_mDice improved from 0.67248 to 0.67250, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 138/300

Epoch 00138: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3028 - acc: 0.9903 - mDice: 0.7234 - val_loss: 0.4197 - val_acc: 0.9917 - val_mDice: 0.6726

Epoch 00138: val_mDice improved from 0.67250 to 0.67256, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 139/300

Epoch 00139: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3034 - acc: 0.9903 - mDice: 0.7230 - val_loss: 0.4193 - val_acc: 0.9917 - val_mDice: 0.6724

Epoch 00139: val_mDice did not improve from 0.67256
Epoch 140/300

Epoch 00140: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3037 - acc: 0.9903 - mDice: 0.7229 - val_loss: 0.4205 - val_acc: 0.9917 - val_mDice: 0.6722

Epoch 00140: val_mDice did not improve from 0.67256
Epoch 141/300

Epoch 00141: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3028 - acc: 0.9903 - mDice: 0.7234 - val_loss: 0.4186 - val_acc: 0.9917 - val_mDice: 0.6726

Epoch 00141: val_mDice improved from 0.67256 to 0.67263, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 142/300

Epoch 00142: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 10s - loss: 0.3027 - acc: 0.9903 - mDice: 0.7234 - val_loss: 0.4187 - val_acc: 0.9917 - val_mDice: 0.6726

Epoch 00142: val_mDice did not improve from 0.67263
Epoch 143/300

Epoch 00143: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3039 - acc: 0.9903 - mDice: 0.7226 - val_loss: 0.4189 - val_acc: 0.9917 - val_mDice: 0.6725

Epoch 00143: val_mDice did not improve from 0.67263
Epoch 144/300

Epoch 00144: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 9s - loss: 0.3039 - acc: 0.9903 - mDice: 0.7224 - val_loss: 0.4195 - val_acc: 0.9917 - val_mDice: 0.6724

Epoch 00144: val_mDice did not improve from 0.67263
Epoch 145/300

Epoch 00145: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.3006 - acc: 0.9903 - mDice: 0.7251 - val_loss: 0.4191 - val_acc: 0.9917 - val_mDice: 0.6723

Epoch 00145: val_mDice did not improve from 0.67263
Epoch 146/300

Epoch 00146: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.3030 - acc: 0.9903 - mDice: 0.7235 - val_loss: 0.4186 - val_acc: 0.9917 - val_mDice: 0.6724

Epoch 00146: val_mDice did not improve from 0.67263
Epoch 147/300

Epoch 00147: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.3021 - acc: 0.9903 - mDice: 0.7242 - val_loss: 0.4185 - val_acc: 0.9917 - val_mDice: 0.6726

Epoch 00147: val_mDice did not improve from 0.67263
Epoch 148/300

Epoch 00148: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.3021 - acc: 0.9903 - mDice: 0.7241 - val_loss: 0.4183 - val_acc: 0.9917 - val_mDice: 0.6728

Epoch 00148: val_mDice improved from 0.67263 to 0.67278, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 149/300

Epoch 00149: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.2995 - acc: 0.9903 - mDice: 0.7256 - val_loss: 0.4186 - val_acc: 0.9917 - val_mDice: 0.6727

Epoch 00149: val_mDice did not improve from 0.67278
Epoch 150/300

Epoch 00150: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 10s - loss: 0.3045 - acc: 0.9903 - mDice: 0.7221 - val_loss: 0.4185 - val_acc: 0.9917 - val_mDice: 0.6727

Epoch 00150: val_mDice did not improve from 0.67278
Epoch 151/300

Epoch 00151: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.3026 - acc: 0.9903 - mDice: 0.7235 - val_loss: 0.4189 - val_acc: 0.9918 - val_mDice: 0.6727

Epoch 00151: val_mDice did not improve from 0.67278
Epoch 152/300

Epoch 00152: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.3003 - acc: 0.9903 - mDice: 0.7251 - val_loss: 0.4194 - val_acc: 0.9917 - val_mDice: 0.6725

Epoch 00152: val_mDice did not improve from 0.67278
Epoch 153/300

Epoch 00153: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.3025 - acc: 0.9904 - mDice: 0.7236 - val_loss: 0.4193 - val_acc: 0.9917 - val_mDice: 0.6725

Epoch 00153: val_mDice did not improve from 0.67278
Epoch 154/300

Epoch 00154: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 10s - loss: 0.3038 - acc: 0.9903 - mDice: 0.7227 - val_loss: 0.4188 - val_acc: 0.9917 - val_mDice: 0.6727

Epoch 00154: val_mDice did not improve from 0.67278
Epoch 155/300

Epoch 00155: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.2996 - acc: 0.9903 - mDice: 0.7257 - val_loss: 0.4192 - val_acc: 0.9917 - val_mDice: 0.6726

Epoch 00155: val_mDice did not improve from 0.67278
Epoch 156/300

Epoch 00156: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.3016 - acc: 0.9903 - mDice: 0.7242 - val_loss: 0.4185 - val_acc: 0.9917 - val_mDice: 0.6728

Epoch 00156: val_mDice did not improve from 0.67278
Epoch 157/300

Epoch 00157: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.3014 - acc: 0.9904 - mDice: 0.7246 - val_loss: 0.4173 - val_acc: 0.9918 - val_mDice: 0.6730

Epoch 00157: val_mDice improved from 0.67278 to 0.67296, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 158/300

Epoch 00158: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.3020 - acc: 0.9904 - mDice: 0.7239 - val_loss: 0.4191 - val_acc: 0.9918 - val_mDice: 0.6728

Epoch 00158: val_mDice did not improve from 0.67296
Epoch 159/300

Epoch 00159: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 10s - loss: 0.3003 - acc: 0.9903 - mDice: 0.7254 - val_loss: 0.4190 - val_acc: 0.9918 - val_mDice: 0.6728

Epoch 00159: val_mDice did not improve from 0.67296
Epoch 160/300

Epoch 00160: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.3019 - acc: 0.9903 - mDice: 0.7240 - val_loss: 0.4195 - val_acc: 0.9918 - val_mDice: 0.6727

Epoch 00160: val_mDice did not improve from 0.67296
Epoch 161/300

Epoch 00161: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 9s - loss: 0.3010 - acc: 0.9904 - mDice: 0.7247 - val_loss: 0.4191 - val_acc: 0.9918 - val_mDice: 0.6728

Epoch 00161: val_mDice did not improve from 0.67296
Epoch 162/300

Epoch 00162: LearningRateScheduler setting learning rate to 3.90625e-06.
 - 10s - loss: 0.3012 - acc: 0.9903 - mDice: 0.7246 - val_loss: 0.4193 - val_acc: 0.9918 - val_mDice: 0.6728

Epoch 00162: val_mDice did not improve from 0.67296
Epoch 163/300

Epoch 00163: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 9s - loss: 0.3010 - acc: 0.9904 - mDice: 0.7245 - val_loss: 0.4192 - val_acc: 0.9918 - val_mDice: 0.6728

Epoch 00163: val_mDice did not improve from 0.67296
Epoch 164/300

Epoch 00164: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 10s - loss: 0.3031 - acc: 0.9904 - mDice: 0.7231 - val_loss: 0.4191 - val_acc: 0.9918 - val_mDice: 0.6728

Epoch 00164: val_mDice did not improve from 0.67296
Epoch 165/300

Epoch 00165: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 10s - loss: 0.3002 - acc: 0.9904 - mDice: 0.7252 - val_loss: 0.4195 - val_acc: 0.9918 - val_mDice: 0.6727

Epoch 00165: val_mDice did not improve from 0.67296
Epoch 166/300

Epoch 00166: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 10s - loss: 0.2995 - acc: 0.9904 - mDice: 0.7258 - val_loss: 0.4194 - val_acc: 0.9918 - val_mDice: 0.6726

Epoch 00166: val_mDice did not improve from 0.67296
Epoch 167/300

Epoch 00167: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 10s - loss: 0.3006 - acc: 0.9904 - mDice: 0.7251 - val_loss: 0.4193 - val_acc: 0.9918 - val_mDice: 0.6726

Epoch 00167: val_mDice did not improve from 0.67296
Epoch 168/300

Epoch 00168: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 10s - loss: 0.3009 - acc: 0.9904 - mDice: 0.7248 - val_loss: 0.4194 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00168: val_mDice did not improve from 0.67296
Epoch 169/300

Epoch 00169: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 10s - loss: 0.2998 - acc: 0.9904 - mDice: 0.7256 - val_loss: 0.4192 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00169: val_mDice did not improve from 0.67296
Epoch 170/300

Epoch 00170: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 10s - loss: 0.3011 - acc: 0.9904 - mDice: 0.7246 - val_loss: 0.4193 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00170: val_mDice did not improve from 0.67296
Epoch 171/300

Epoch 00171: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 9s - loss: 0.3002 - acc: 0.9903 - mDice: 0.7252 - val_loss: 0.4193 - val_acc: 0.9918 - val_mDice: 0.6724

Epoch 00171: val_mDice did not improve from 0.67296
Epoch 172/300

Epoch 00172: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 10s - loss: 0.2999 - acc: 0.9904 - mDice: 0.7254 - val_loss: 0.4192 - val_acc: 0.9918 - val_mDice: 0.6724

Epoch 00172: val_mDice did not improve from 0.67296
Epoch 173/300

Epoch 00173: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 9s - loss: 0.3010 - acc: 0.9903 - mDice: 0.7247 - val_loss: 0.4191 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00173: val_mDice did not improve from 0.67296
Epoch 174/300

Epoch 00174: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 10s - loss: 0.3017 - acc: 0.9904 - mDice: 0.7243 - val_loss: 0.4194 - val_acc: 0.9918 - val_mDice: 0.6724

Epoch 00174: val_mDice did not improve from 0.67296
Epoch 175/300

Epoch 00175: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 9s - loss: 0.3013 - acc: 0.9904 - mDice: 0.7246 - val_loss: 0.4194 - val_acc: 0.9918 - val_mDice: 0.6724

Epoch 00175: val_mDice did not improve from 0.67296
Epoch 176/300

Epoch 00176: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 9s - loss: 0.3009 - acc: 0.9904 - mDice: 0.7248 - val_loss: 0.4194 - val_acc: 0.9918 - val_mDice: 0.6726

Epoch 00176: val_mDice did not improve from 0.67296
Epoch 177/300

Epoch 00177: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 10s - loss: 0.3001 - acc: 0.9904 - mDice: 0.7253 - val_loss: 0.4191 - val_acc: 0.9918 - val_mDice: 0.6727

Epoch 00177: val_mDice did not improve from 0.67296
Epoch 178/300

Epoch 00178: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 9s - loss: 0.3018 - acc: 0.9904 - mDice: 0.7241 - val_loss: 0.4194 - val_acc: 0.9918 - val_mDice: 0.6726

Epoch 00178: val_mDice did not improve from 0.67296
Epoch 179/300

Epoch 00179: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 10s - loss: 0.3024 - acc: 0.9904 - mDice: 0.7237 - val_loss: 0.4190 - val_acc: 0.9918 - val_mDice: 0.6727

Epoch 00179: val_mDice did not improve from 0.67296
Epoch 180/300

Epoch 00180: LearningRateScheduler setting learning rate to 1.953125e-06.
 - 9s - loss: 0.3019 - acc: 0.9904 - mDice: 0.7242 - val_loss: 0.4193 - val_acc: 0.9918 - val_mDice: 0.6727

Epoch 00180: val_mDice did not improve from 0.67296
Epoch 181/300

Epoch 00181: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 9s - loss: 0.3028 - acc: 0.9904 - mDice: 0.7235 - val_loss: 0.4193 - val_acc: 0.9918 - val_mDice: 0.6727

Epoch 00181: val_mDice did not improve from 0.67296
Epoch 182/300

Epoch 00182: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 10s - loss: 0.3002 - acc: 0.9904 - mDice: 0.7253 - val_loss: 0.4193 - val_acc: 0.9918 - val_mDice: 0.6726

Epoch 00182: val_mDice did not improve from 0.67296
Epoch 183/300

Epoch 00183: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 9s - loss: 0.3029 - acc: 0.9903 - mDice: 0.7232 - val_loss: 0.4190 - val_acc: 0.9918 - val_mDice: 0.6726

Epoch 00183: val_mDice did not improve from 0.67296
Epoch 184/300

Epoch 00184: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 10s - loss: 0.2998 - acc: 0.9904 - mDice: 0.7257 - val_loss: 0.4189 - val_acc: 0.9918 - val_mDice: 0.6726

Epoch 00184: val_mDice did not improve from 0.67296
Epoch 185/300

Epoch 00185: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 10s - loss: 0.2998 - acc: 0.9904 - mDice: 0.7256 - val_loss: 0.4191 - val_acc: 0.9918 - val_mDice: 0.6727

Epoch 00185: val_mDice did not improve from 0.67296
Epoch 186/300

Epoch 00186: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 9s - loss: 0.3003 - acc: 0.9904 - mDice: 0.7253 - val_loss: 0.4192 - val_acc: 0.9918 - val_mDice: 0.6727

Epoch 00186: val_mDice did not improve from 0.67296
Epoch 187/300

Epoch 00187: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 10s - loss: 0.3001 - acc: 0.9904 - mDice: 0.7252 - val_loss: 0.4195 - val_acc: 0.9918 - val_mDice: 0.6726

Epoch 00187: val_mDice did not improve from 0.67296
Epoch 188/300

Epoch 00188: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 10s - loss: 0.3019 - acc: 0.9904 - mDice: 0.7241 - val_loss: 0.4194 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00188: val_mDice did not improve from 0.67296
Epoch 189/300

Epoch 00189: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 10s - loss: 0.3021 - acc: 0.9904 - mDice: 0.7239 - val_loss: 0.4191 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00189: val_mDice did not improve from 0.67296
Epoch 190/300

Epoch 00190: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 9s - loss: 0.3012 - acc: 0.9904 - mDice: 0.7246 - val_loss: 0.4192 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00190: val_mDice did not improve from 0.67296
Epoch 191/300

Epoch 00191: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 10s - loss: 0.3003 - acc: 0.9904 - mDice: 0.7255 - val_loss: 0.4192 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00191: val_mDice did not improve from 0.67296
Epoch 192/300

Epoch 00192: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 9s - loss: 0.3022 - acc: 0.9904 - mDice: 0.7240 - val_loss: 0.4192 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00192: val_mDice did not improve from 0.67296
Epoch 193/300

Epoch 00193: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 9s - loss: 0.2991 - acc: 0.9904 - mDice: 0.7262 - val_loss: 0.4190 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00193: val_mDice did not improve from 0.67296
Epoch 194/300

Epoch 00194: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 9s - loss: 0.3023 - acc: 0.9904 - mDice: 0.7240 - val_loss: 0.4190 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00194: val_mDice did not improve from 0.67296
Epoch 195/300

Epoch 00195: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 10s - loss: 0.3010 - acc: 0.9904 - mDice: 0.7248 - val_loss: 0.4191 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00195: val_mDice did not improve from 0.67296
Epoch 196/300

Epoch 00196: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 9s - loss: 0.2999 - acc: 0.9904 - mDice: 0.7255 - val_loss: 0.4191 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00196: val_mDice did not improve from 0.67296
Epoch 197/300

Epoch 00197: LearningRateScheduler setting learning rate to 9.765625e-07.
 - 9s - loss: 0.3011 - acc: 0.9904 - mDice: 0.7247 - val_loss: 0.4190 - val_acc: 0.9918 - val_mDice: 0.6725

Epoch 00197: val_mDice did not improve from 0.67296
Restoring model weights from the end of the best epoch
Epoch 00197: early stopping
{'val_loss': [5.693518701652602, 3.409973768242301, 2.8646390676163085, 1.9590373535531818, 1.418704691818495, 1.4429534165668085, 1.5002412849673072, 1.2291382515983742, 1.3334052718641385, 1.3876308122264684, 1.2713592179884554, 1.2730768545099787, 1.0849294308703996, 0.990638183474373, 0.8113736465845766, 0.7900215171057464, 0.8651329781938706, 0.8227509699457976, 0.7820089200377968, 0.7369340500583461, 0.723908681742082, 0.6858761833689887, 0.6981185374548331, 0.7053301505398649, 0.6798973232717286, 0.6796787770991587, 0.672136363097887, 0.7079969963108604, 0.6995985891916245, 0.6835181874061939, 0.686381495786954, 0.6672380295819204, 0.6475249654633083, 0.6028640976099693, 0.6683678261506071, 0.660125958600963, 0.6665610261104278, 0.6444778915196029, 0.64680880534498, 0.6350380874719633, 0.650302984208162, 0.6167532262587514, 0.607453821412957, 0.6043173933163306, 0.5938510321363618, 0.6403314815794868, 0.6345703660519649, 0.6113135244608261, 0.5715651698253326, 0.5589066232474712, 0.5931209743274415, 0.5626956477614562, 0.5429614666645034, 0.5420965878437005, 0.5528768445201396, 0.5660339481552275, 0.5645296613878339, 0.5684905389190223, 0.5682362769055802, 0.5407902874188751, 0.5582728516703417, 0.5419819205789794, 0.5317670643413452, 0.5425814214135021, 0.5758498742778257, 0.559432386848997, 0.5508639234027782, 0.5368747898965613, 0.5190854827059975, 0.5343177657254805, 0.5401364325135737, 0.5120566940844142, 0.5046772027988139, 0.4710951941258126, 0.5037391980489095, 0.4824780976889673, 0.45941384013024394, 0.4515016075595689, 0.44831174570129223, 0.4549188788262433, 0.45182758511034915, 0.4489436479895762, 0.4506883188642027, 0.44377866307726727, 0.4455774591609563, 0.44453889743520575, 0.43943996620580617, 0.43167004387422286, 0.43337196706887204, 0.4322633122928367, 0.428106591671328, 0.42677818421070085, 0.4272113253798666, 0.4279756743864336, 0.425376101385189, 0.4233972704360254, 0.42230110698276097, 0.4232117293085562, 0.41993279866193084, 0.4233650482153591, 0.42403494473415754, 0.42293769112786833, 0.4259708658720035, 0.4229888867392989, 0.4183572570985883, 0.420561051234582, 0.42032915362158235, 0.4220893237158216, 0.42465174701143416, 0.42269664290249764, 0.4215247025805016, 0.42236781707125204, 0.42241346852856515, 0.4202924574477763, 0.41990475279034106, 0.4196128602101665, 0.4221479642910964, 0.42029291600953006, 0.42124985374311186, 0.42163058842955425, 0.4207013505085276, 0.4194655565940546, 0.41886601602310203, 0.4218261907539958, 0.42075539205312396, 0.41875255459974586, 0.42173114523773625, 0.4215620357108351, 0.42040397212139835, 0.4206908817197368, 0.4202035517967535, 0.41991798653045953, 0.4185811618209388, 0.41951745555706, 0.419768755781332, 0.42055045585927414, 0.41878929704888795, 0.41974456903803703, 0.41933311222307124, 0.4204699927744483, 0.41856168325775617, 0.4186561679370628, 0.41887534771287493, 0.41947502073859366, 0.41906292411680796, 0.4185544679939495, 0.41852535074773217, 0.418341681088744, 0.41857958510622867, 0.4185325445505134, 0.4188990527567481, 0.4194140850072504, 0.4193098898846054, 0.4188034541160916, 0.4191584008655468, 0.41850000225877293, 0.4173062882175258, 0.41911984697172916, 0.4189859011672888, 0.41952871790079793, 0.4191486880749087, 0.4193303900261301, 0.4192014697902481, 0.41912338502296415, 0.4195326484540679, 0.419421563000954, 0.41925311876584037, 0.4194265243541004, 0.41922591859278296, 0.41932307751705206, 0.41927262734092574, 0.4192004686669458, 0.4191269178766071, 0.4194172872269707, 0.4194188805404427, 0.4194302533749287, 0.4191470147855842, 0.41939534835842257, 0.41898818441919466, 0.4193060302533178, 0.4192835670986256, 0.4193247829979147, 0.4189848509183581, 0.41888634598540186, 0.41908097887508644, 0.4192185284551521, 0.4194522928420334, 0.4194156023688122, 0.4191359474353817, 0.4192191763098565, 0.41924650625505194, 0.4192488128122901, 0.419045368830363, 0.4189902099711315, 0.41906233561525197, 0.41912322322695234, 0.41902428001961795], 'val_acc': [0.4966308395738508, 0.9593360925357889, 0.9667817145460266, 0.9805414978461929, 0.9830209085542274, 0.9831971917641984, 0.9833127617333006, 0.9839912654646003, 0.9836218507984017, 0.9840464018568208, 0.9839300847757718, 0.9840225264660585, 0.9845577275535058, 0.9843221425004146, 0.9849021350951805, 0.9857252995508465, 0.9856694086694516, 0.9869049936742554, 0.9872813412576453, 0.9875404076904855, 0.9874634917107648, 0.9875926015749259, 0.9881097033054014, 0.9871378290334667, 0.987630585913249, 0.988523456785414, 0.9881963773451107, 0.987810565114189, 0.9885252591762194, 0.9885894829713846, 0.987761028708285, 0.9885602087914189, 0.9885288555746172, 0.9895085849171785, 0.9887536262828757, 0.9896492636153467, 0.9889941490484525, 0.9894996903281339, 0.9893848906086466, 0.9895806805493963, 0.9898227039101087, 0.9898855109422686, 0.9896780599521685, 0.9902774764515679, 0.9902981243053066, 0.9899809873556789, 0.9904583023570258, 0.990654377327019, 0.9902111652363537, 0.9901818156074707, 0.9906846994086157, 0.9906073894178817, 0.990846109792653, 0.9905915870612851, 0.990677682659294, 0.9905967930365883, 0.9907200346516154, 0.9906904670591931, 0.9907117017378452, 0.990873195954013, 0.991013673455068, 0.9908653325001734, 0.9908258391835015, 0.9908678893801532, 0.9906617713209278, 0.9908001614019337, 0.9910685164348318, 0.9911543270035971, 0.9912279316141636, 0.9910723978624733, 0.9910992073778027, 0.9909328676644927, 0.9910130111812372, 0.9910439871534517, 0.991023515347187, 0.9911844646545067, 0.99122783939882, 0.991330516992239, 0.9913648211000338, 0.9913718294661424, 0.9913213373739508, 0.9912720776140773, 0.9913680318706314, 0.9913506031707034, 0.9913414235524152, 0.9913871791292344, 0.9914127479290326, 0.9914084892568038, 0.9913881264323088, 0.9913848988952851, 0.9914208041967721, 0.9914246856244137, 0.9914671214488656, 0.9914594591921392, 0.991459257995026, 0.9914762172350233, 0.9914650340288165, 0.9914434388720034, 0.9914988519102544, 0.9914715813182075, 0.9914818926702572, 0.9914738364025175, 0.9914978040086234, 0.9915258626226969, 0.9915078470978556, 0.9915724481376079, 0.9916055115298715, 0.9916104408591441, 0.991599551065394, 0.9916211378389941, 0.9916572359543813, 0.9916520215958651, 0.9916484168142541, 0.9916510826760035, 0.9916510826760035, 0.9916327988883447, 0.9916111869651054, 0.9916435880835381, 0.9916331677497188, 0.9916438814959948, 0.9916196204774322, 0.9916593233744303, 0.9916552407496757, 0.9916687964051752, 0.991596792988301, 0.9916472934637056, 0.9916746646543092, 0.9916816646372048, 0.9916963520264659, 0.9917071412216595, 0.9916978777712407, 0.9916869795942777, 0.9916798790128255, 0.991707241820216, 0.9917324417586401, 0.9917147280294684, 0.991711039415727, 0.9917167148509609, 0.9917235387863824, 0.9917050621848233, 0.9917127412079759, 0.9917168154495175, 0.9917205962786024, 0.9917025975201871, 0.9917015580017691, 0.9917072502034291, 0.9917239998631001, 0.9917431471217031, 0.991737270489356, 0.9917390812633745, 0.9917502477031552, 0.9917354848649766, 0.9917290381741423, 0.9917421076032851, 0.9917478836370756, 0.9917473135785883, 0.991760575821631, 0.9917611458801183, 0.9917585051680081, 0.9917583123541079, 0.9917659746108344, 0.9917687243047143, 0.9917661758079476, 0.9917663602386346, 0.9917655051509037, 0.9917668213153522, 0.9917658823954908, 0.9917679530491138, 0.9917691099325145, 0.9917684392754706, 0.9917685398740272, 0.9917667291000087, 0.9917698560384759, 0.9917751626123356, 0.9917743159078177, 0.991780745832226, 0.991783679956793, 0.9917802847555083, 0.9917794129013512, 0.991779706313808, 0.9917822631937877, 0.9917781805690331, 0.9917830260661752, 0.9917829338508316, 0.9917836967232191, 0.9917853817490418, 0.9917862536031988, 0.9917823637923443, 0.9917823721755573, 0.9917840739678062, 0.9917846272598675, 0.9917858763586117, 0.9917865302492295, 0.9917874691690909, 0.9917898416183837, 0.9917896571876966, 0.991788517070722], 'val_mDice': [0.006500861683464839, 0.0636265556813963, 0.10041808277242797, 0.20459826653181418, 0.3519191101465882, 0.39469592178924173, 0.420099643547659, 0.4829523264942625, 0.48383532194145623, 0.5053280945736313, 0.5255007626470467, 0.5330953165448667, 0.5522026668621015, 0.5456540151990416, 0.5725141777100274, 0.5852234430621612, 0.5754590663225841, 0.581239413276168, 0.6010480319397359, 0.6099889130867315, 0.6174806762177565, 0.629870633368083, 0.6304926258602223, 0.6230875912597914, 0.6329718186047174, 0.6359773932965328, 0.6375905330674054, 0.6253582934957684, 0.6267442649594507, 0.6343021255337572, 0.6379798159485293, 0.6417240103924157, 0.6383457599645258, 0.643329160961253, 0.6356152887921125, 0.6458762996475069, 0.6418208448863398, 0.6498657282227035, 0.6486085061450213, 0.6505768510359752, 0.6480135159821114, 0.647856238522107, 0.6524000908922714, 0.6554681708205768, 0.6556530542011503, 0.6472806890302569, 0.651824340203308, 0.653618389711769, 0.656352281235106, 0.6558122095008775, 0.6562879316917452, 0.6568295459371747, 0.6567640981929044, 0.6605382793898656, 0.6586256241831934, 0.6561687810846857, 0.6575057107520338, 0.6562213354472872, 0.6607843853753327, 0.6602854333011074, 0.6592027497190966, 0.6567467533251069, 0.6578453063294186, 0.6606841975961891, 0.6566921115424562, 0.6570496471957651, 0.6596573629795079, 0.655295962858133, 0.659636840873965, 0.6601152205433691, 0.6613634055173849, 0.6607990727645938, 0.6615593547391825, 0.6627425446624327, 0.6586612360722237, 0.6615884696381001, 0.6608588450736302, 0.6618435959608076, 0.6633222773775941, 0.6622413039710451, 0.6625411044360884, 0.6623902988165575, 0.663225811745044, 0.6632632008752407, 0.6618708497864284, 0.6619798064064208, 0.6645008397672414, 0.6664577080395319, 0.6646437148672284, 0.6655673102152163, 0.6664472457896473, 0.6663794255960843, 0.6661220022730016, 0.6658896615233603, 0.6677078798350402, 0.668128566232244, 0.668725132439207, 0.6677736964406846, 0.6696643624795304, 0.6693618961527378, 0.6697385539150104, 0.6698915391699339, 0.6696909707977466, 0.6697334401550508, 0.6707674508021015, 0.6701063003218124, 0.6710489004137814, 0.6723422876725552, 0.6715616847895369, 0.6717860195707168, 0.6721436642057953, 0.6720475925842586, 0.6717289969555604, 0.671601907445744, 0.6720211603135164, 0.6716247684677275, 0.6710223172452044, 0.6718195440396981, 0.6715530919961621, 0.6716668773468705, 0.6716403696272109, 0.6723322194336839, 0.6718842037619418, 0.6713378362347138, 0.6714609940176104, 0.6719932861301299, 0.6723236517899482, 0.6720359902173993, 0.6722923069563596, 0.672261146553458, 0.6720329554942758, 0.6719521664701266, 0.6721925299546554, 0.6722629489442635, 0.6724767292602153, 0.6722264819675021, 0.6725042848815059, 0.672564694314734, 0.6724433976051342, 0.6722473729344193, 0.6726342749830372, 0.6725791805068819, 0.6724842909183851, 0.6723849498437594, 0.6723099033205485, 0.6724461724486532, 0.6726173408926791, 0.672783336894254, 0.6726547384060888, 0.6726799634941519, 0.6726608833012534, 0.6724760669863844, 0.6725416740117026, 0.6726854293490596, 0.6726253552443535, 0.6727586483318259, 0.6729646406428555, 0.6728257559522798, 0.6728124853260239, 0.6727349489885376, 0.6727907476545889, 0.6727628231719242, 0.672781618335579, 0.67275607468542, 0.6726711443540249, 0.6725546428422887, 0.6725628164750111, 0.6724969495700884, 0.6724992968697421, 0.6724899579704059, 0.672408347391378, 0.672421970112582, 0.6724575568519732, 0.6723820073359794, 0.672402135430509, 0.6725543745794712, 0.6727187861537799, 0.6726259336860538, 0.6726653096377431, 0.6726634485644463, 0.6726550318185455, 0.6725772104518155, 0.6726192354988281, 0.6726497084782597, 0.6726744221903268, 0.6726664162218654, 0.6725549697875977, 0.672474088548105, 0.6725270117720806, 0.6725192824496499, 0.6724969663365146, 0.6724846597797592, 0.6725162309601002, 0.6725273303341764, 0.6725287135643295, 0.6725027172206659, 0.6725290405096384], 'loss': [5.632938289713583, 4.07756003409034, 2.540195216070951, 1.798733688086594, 1.33005279852554, 1.0741322069502963, 0.9125740481571871, 0.8003005781946763, 0.7175426690668413, 0.6570348308564657, 0.6179709160421426, 0.5862386102635423, 0.5585563813834036, 0.54039119902635, 0.5112140601056255, 0.4903770137799512, 0.4646522860986598, 0.46134246088116304, 0.4361482869901896, 0.4267995407388065, 0.4233368816863791, 0.4202183776152058, 0.4097801299294652, 0.402076970531195, 0.4006040841730664, 0.39423003163763426, 0.38966912101309914, 0.3912273738136655, 0.3872047355137612, 0.37844457257181396, 0.38354827174732475, 0.3724468440919569, 0.3754940142839045, 0.3668631637635229, 0.3683358740686346, 0.35861662482208934, 0.3558270605611498, 0.35083342443485066, 0.3528950721927218, 0.3494481984741616, 0.34948133015124977, 0.34480310624154326, 0.34563690191291363, 0.3445452038570264, 0.34154475890837543, 0.34263108361778216, 0.3404448967973911, 0.33696521442908156, 0.3401493042384594, 0.337008022977026, 0.33511469999399923, 0.3311820775103471, 0.33619521305015115, 0.3327160874631452, 0.32663814634986454, 0.32770633884539657, 0.3273536248865259, 0.3228235623820486, 0.3260879912926806, 0.32782777930134305, 0.32227619307941585, 0.3236852883290657, 0.32295396301699164, 0.3227818122562919, 0.3205668672746716, 0.3241910419002771, 0.32058065079867865, 0.3204671605491888, 0.31936288669254514, 0.3177647270014538, 0.31817483691568915, 0.31502111244433223, 0.3152510747897968, 0.3154457362565613, 0.3159123461082617, 0.31449478450532187, 0.3132004567857193, 0.31420856385833074, 0.3114796965078583, 0.31279873106521505, 0.31214556513235203, 0.3119762135077467, 0.3139322705241251, 0.3116816650431772, 0.30957928808669705, 0.30900439824860265, 0.3072333979237022, 0.3130315215920707, 0.3078922103000836, 0.308034311636286, 0.30882282734896915, 0.30904235048751844, 0.30610092865427707, 0.3095314402744135, 0.30717210221700275, 0.30948338298865297, 0.31253099634124576, 0.3054659694021301, 0.3087828491866967, 0.3066622633006887, 0.30620792532617314, 0.30760825165715466, 0.3067570537117198, 0.30775474113762535, 0.30743348681975713, 0.30606659228204125, 0.3054891757480591, 0.3040927731928471, 0.30457000521790595, 0.30238391737863113, 0.30463646384019394, 0.3065708387485849, 0.3029248359312885, 0.3030625904590548, 0.305233032123607, 0.3061914115331491, 0.3029873776578351, 0.30301057199154674, 0.30491835402159895, 0.30272021494474616, 0.30552136084238096, 0.3041981424375127, 0.3047194020446219, 0.30075704110706303, 0.3085678570030962, 0.3045882425247666, 0.3008103732814262, 0.29964001829115633, 0.30381026620245, 0.3037739533027874, 0.3033748079045388, 0.3073653167292481, 0.3014922550934039, 0.3015917531135682, 0.3006786495102044, 0.3010526302482461, 0.308299447858249, 0.3027638978462946, 0.3033552149872215, 0.3036591113281642, 0.3028337225501645, 0.30272278669479846, 0.3039370299075849, 0.3039246346463363, 0.30062435629165657, 0.30302609156484606, 0.30210559966851847, 0.3021348224097684, 0.2995387637040755, 0.304479750059058, 0.3026474902422338, 0.3002763807573201, 0.302485578265883, 0.3038443920881302, 0.2996482972694414, 0.3016206710962916, 0.30144408200496964, 0.3019980280244355, 0.30034969076319423, 0.3019008902880876, 0.30095611077927814, 0.3012343694525153, 0.30095995809544723, 0.30306161572161033, 0.30020587175783936, 0.29950730851616575, 0.3006314313429525, 0.30086495094126536, 0.29979345858163875, 0.3011280491385925, 0.3002406523594626, 0.2999474087512266, 0.3010280339585163, 0.3017066086589912, 0.3012518221021901, 0.3009367536086844, 0.30009269889139356, 0.3018076475159998, 0.30240027205152303, 0.3018527136371792, 0.30276123162388224, 0.300188166818202, 0.3029321806952719, 0.2998427180576823, 0.29980072188742535, 0.3002920829519479, 0.3000659722755505, 0.3019393442346839, 0.3020591616051652, 0.3011766583295202, 0.3002998624157006, 0.3021710175028656, 0.2991308992729287, 0.3022847790013599, 0.3009754408082723, 0.29987325585284785, 0.3010964486240049], 'acc': [0.3334833932181192, 0.7837033740700204, 0.9600604752306178, 0.981286472779047, 0.983582812345068, 0.9843141643422996, 0.9848273195434872, 0.9851688527553261, 0.9854333631173862, 0.9857131987086507, 0.9858322756968196, 0.9860535962482776, 0.986209222528531, 0.9864405283831731, 0.986654084500066, 0.9868775603706195, 0.987060206057556, 0.9872785610274499, 0.9875228462443674, 0.9876672328743064, 0.9878119465393677, 0.9877966251843537, 0.9879738613656976, 0.9879832268874503, 0.9880720280558929, 0.9881559632854939, 0.9882235352032034, 0.9882561852560589, 0.9883224105585587, 0.9884878955628384, 0.9883927747434674, 0.988548586851276, 0.9885159523842448, 0.988674476781709, 0.9888043188763769, 0.9887452494042731, 0.9888671134049918, 0.9889365500107068, 0.988963321246538, 0.9890233479999632, 0.9890691382385343, 0.9891291687102918, 0.9891612678265278, 0.989155657686847, 0.9891650178871539, 0.9892154335530338, 0.9891777699635819, 0.9893701864927618, 0.9893393863659098, 0.9892992359246164, 0.9893332983871249, 0.9893446439987784, 0.9893782580571251, 0.9894611841714066, 0.9894658634366053, 0.989502010770284, 0.9896048835778263, 0.9895641288518638, 0.9896446070565988, 0.9895439251706277, 0.989664327131976, 0.9897033796632829, 0.9896578405346049, 0.9897342202019505, 0.9896566894992234, 0.9897542022750322, 0.9897482132884964, 0.9897475278462142, 0.989803658277186, 0.9897203353478208, 0.9898167177060668, 0.9898013485248381, 0.9898344087075243, 0.9898433363344138, 0.9898376006843174, 0.9899014795187208, 0.9899596604875187, 0.9899877571418386, 0.9899294277069326, 0.9899266707304925, 0.989976248302075, 0.9900223647742117, 0.9900170513663579, 0.9900562073652719, 0.989976931094767, 0.9900034924786083, 0.9900081749500336, 0.990078218022433, 0.9900783967473056, 0.9900879037883418, 0.9901131351447435, 0.9900858843776331, 0.9901400830124936, 0.9901248869897716, 0.9901618643576501, 0.9901358333594059, 0.9901012118333838, 0.9901649627304362, 0.9901336954519024, 0.9901207616892954, 0.9901681831179602, 0.990142119478547, 0.9902070558556524, 0.9901374895758802, 0.9902002555156404, 0.9902077599118617, 0.9902170901205899, 0.9902517132274598, 0.9902505822087296, 0.9902022488534918, 0.9901765937630614, 0.9902190053734621, 0.9902360575119862, 0.9901979179982601, 0.990242450149103, 0.9902399138234024, 0.9902642336094785, 0.9902181756955099, 0.9902323333460936, 0.9902677619728883, 0.9902794348200533, 0.9902554064443586, 0.9902403127537052, 0.9902920656387691, 0.9902710539216109, 0.9902562411765711, 0.990294629529113, 0.9902792030588478, 0.9902883683250251, 0.9903414725010459, 0.990304840025191, 0.9902746043055637, 0.9903432140387748, 0.9903271829498584, 0.9903030589217342, 0.9902995607280268, 0.9902893917125065, 0.9903214247893788, 0.9903073596072633, 0.9902872803231696, 0.9903187100282569, 0.9903128399392941, 0.9903215040989585, 0.9903192543965648, 0.9903179770269455, 0.9903187313585705, 0.9903316572837345, 0.9903234899778797, 0.9903404960936916, 0.9903147105944438, 0.9903219749021762, 0.9903277690659097, 0.9903557398758319, 0.9903484672185509, 0.9903303529283739, 0.9903384364715565, 0.990373274751862, 0.9903564980816476, 0.9903215468709131, 0.9903367979783908, 0.9903788309423256, 0.9903471381707917, 0.9903668600942023, 0.9903529477644911, 0.9903700797247005, 0.990375184082121, 0.9903586326938625, 0.9903683948303097, 0.9903743046185932, 0.9903533995752687, 0.9903460524400135, 0.9903752848110762, 0.990338911015372, 0.9903642475538215, 0.9903947375450377, 0.990379714614026, 0.9903868606253529, 0.9903586800970334, 0.9903811266896971, 0.9903807970940466, 0.9903757926640245, 0.9903728031916186, 0.9903498021443923, 0.9903918686846455, 0.9904043715660101, 0.99037777778592, 0.9903661334385478, 0.9903844895093887, 0.9903679367852024, 0.9903929476021919, 0.9903850439194204, 0.9903736286391328, 0.9903997162584498, 0.9903810993254429, 0.9903995109931453, 0.9903834903689966, 0.9903776752089314], 'mDice': [0.004627909527504994, 0.0332134352439613, 0.10066206921914819, 0.17593703170431876, 0.26591931160457283, 0.33152609268675826, 0.385522551876331, 0.4300758149314826, 0.46826971450046145, 0.4975472712053663, 0.5184308609545654, 0.5356017617000143, 0.5518783365206171, 0.5635146838228428, 0.5800979090440527, 0.5927934758156416, 0.6084431604213308, 0.6114327827183934, 0.6276573908164246, 0.633689134322221, 0.6360895568237048, 0.6382674611590003, 0.645341962720638, 0.6508647853151649, 0.6519182684442023, 0.6561399651277319, 0.6593108883000239, 0.6589779822426498, 0.6619270720676027, 0.6681012543092969, 0.6637699294321832, 0.6717421595712718, 0.6694662807635242, 0.6758557649348091, 0.6750041609786542, 0.6816388802783099, 0.6839526738967296, 0.6874500950593132, 0.6857634632721916, 0.6890403076165819, 0.6887801333198719, 0.6918012767885441, 0.6911905374507384, 0.6918934072003597, 0.6942882175805366, 0.6935446119860655, 0.694918906105335, 0.6977638401199643, 0.6955640163268444, 0.6976024052479572, 0.6988152639608487, 0.7018861214661268, 0.6983076800906396, 0.7007516651043127, 0.705112959985017, 0.7044363670888836, 0.7049611189893523, 0.7080301608574404, 0.7056395897101144, 0.7046843931485256, 0.7084536208205139, 0.7075489072332999, 0.7082287120783467, 0.7084104398270351, 0.7098319363496087, 0.7078865503792804, 0.7098614724661193, 0.7099944201030525, 0.710680691283899, 0.7120329270756507, 0.7116238093661158, 0.7140283189828065, 0.7141202235337644, 0.713805313113914, 0.7134505435963553, 0.7145300892515328, 0.7159808266060451, 0.7149474949234673, 0.7165838388262019, 0.7158312911322904, 0.7162910152113967, 0.7165466080773532, 0.7151597280933378, 0.7166760942808585, 0.7182285949326203, 0.7188209572477843, 0.7198827375348575, 0.7157496054046939, 0.7195275275173765, 0.719229331869753, 0.7189454214538172, 0.7185483234428317, 0.720916963287107, 0.7183500820681102, 0.7198725297551659, 0.7183360076903584, 0.7163716362490777, 0.7212405091441741, 0.718921351753423, 0.7206805745166854, 0.7208527366216386, 0.7195208401968554, 0.720406185814726, 0.7196367043009613, 0.7198291220324762, 0.7208525132099817, 0.7213411671838343, 0.7227156823546218, 0.7220397138934016, 0.723531141003255, 0.7219659004542736, 0.720638841045495, 0.7233217959979331, 0.7229718450383104, 0.721756193056274, 0.7208648147002324, 0.7231103399372564, 0.7230866750229816, 0.7218477479949299, 0.7234219263683828, 0.7213399615980682, 0.7223930686189026, 0.7221004192313464, 0.7248629346061296, 0.7209222133051872, 0.7220910565818385, 0.724947271393358, 0.72565817242912, 0.7226368661108505, 0.7228200737335453, 0.7229696107436171, 0.7213365120098668, 0.7242405059892472, 0.7243663558418465, 0.7248003958408896, 0.7244204451087521, 0.7198335289687954, 0.72344768684125, 0.7229684178936958, 0.7228528415261697, 0.723370794355758, 0.7234025490030139, 0.7225827299528795, 0.7223991274076653, 0.7250726078489088, 0.7234597166037889, 0.7241524712203108, 0.7240623183378676, 0.7256116149226899, 0.7221165962703735, 0.7234848378797409, 0.7250832283857557, 0.7235516990393367, 0.7226966840364971, 0.7256926251384007, 0.7242016863101536, 0.7245692706036844, 0.7238853296371462, 0.7253824957369155, 0.723982755488587, 0.7247195065489089, 0.7245960675561255, 0.7245298741822996, 0.7230989686980549, 0.7252159588898925, 0.7257603229895696, 0.7250662525064427, 0.7248149972644189, 0.7256060733606355, 0.7246239842609723, 0.72523765302123, 0.7254366042857122, 0.7246960597693765, 0.7243247406002653, 0.7245974490390502, 0.7248285094169614, 0.7253111160910303, 0.724140717950293, 0.7237333403312318, 0.7242002880613351, 0.7235084541439788, 0.7253182484758938, 0.7231748180688652, 0.7256884480929883, 0.7255903476202804, 0.7252972520105777, 0.7252295219858146, 0.7240901665541499, 0.7238858915061082, 0.7246229442634554, 0.7254768272901498, 0.7240067801459493, 0.7262200983015067, 0.7239774647469747, 0.7248020457785033, 0.7254519040209614, 0.7246804452672752], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 1.953125e-06, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07, 9.765625e-07]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:03<00:35,  3.58s/it]predicting test subjects:  18%|█▊        | 2/11 [00:06<00:31,  3.53s/it]predicting test subjects:  27%|██▋       | 3/11 [00:08<00:24,  3.06s/it]predicting test subjects:  36%|███▋      | 4/11 [00:12<00:22,  3.27s/it]predicting test subjects:  45%|████▌     | 5/11 [00:16<00:20,  3.46s/it]predicting test subjects:  55%|█████▍    | 6/11 [00:20<00:17,  3.45s/it]predicting test subjects:  64%|██████▎   | 7/11 [00:23<00:13,  3.30s/it]predicting test subjects:  73%|███████▎  | 8/11 [00:27<00:10,  3.56s/it]predicting test subjects:  82%|████████▏ | 9/11 [00:30<00:07,  3.62s/it]predicting test subjects:  91%|█████████ | 10/11 [00:34<00:03,  3.47s/it]predicting test subjects: 100%|██████████| 11/11 [00:37<00:00,  3.53s/it]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:03<02:29,  3.73s/it]predicting train subjects:   5%|▍         | 2/41 [00:07<02:22,  3.66s/it]predicting train subjects:   7%|▋         | 3/41 [00:10<02:10,  3.44s/it]predicting train subjects:  10%|▉         | 4/41 [00:13<02:02,  3.31s/it]predicting train subjects:  12%|█▏        | 5/41 [00:17<02:10,  3.61s/it]predicting train subjects:  15%|█▍        | 6/41 [00:20<02:03,  3.54s/it]predicting train subjects:  17%|█▋        | 7/41 [00:23<01:56,  3.41s/it]predicting train subjects:  20%|█▉        | 8/41 [00:26<01:40,  3.06s/it]predicting train subjects:  22%|██▏       | 9/41 [00:30<01:46,  3.32s/it]predicting train subjects:  24%|██▍       | 10/41 [00:34<01:53,  3.66s/it]predicting train subjects:  27%|██▋       | 11/41 [00:38<01:54,  3.80s/it]predicting train subjects:  29%|██▉       | 12/41 [00:40<01:35,  3.29s/it]predicting train subjects:  32%|███▏      | 13/41 [00:44<01:35,  3.40s/it]predicting train subjects:  34%|███▍      | 14/41 [00:48<01:35,  3.53s/it]predicting train subjects:  37%|███▋      | 15/41 [00:50<01:23,  3.22s/it]predicting train subjects:  39%|███▉      | 16/41 [00:54<01:23,  3.34s/it]predicting train subjects:  41%|████▏     | 17/41 [00:59<01:29,  3.75s/it]predicting train subjects:  44%|████▍     | 18/41 [01:02<01:25,  3.72s/it]predicting train subjects:  46%|████▋     | 19/41 [01:06<01:20,  3.64s/it]predicting train subjects:  49%|████▉     | 20/41 [01:08<01:10,  3.35s/it]predicting train subjects:  51%|█████     | 21/41 [01:12<01:07,  3.39s/it]predicting train subjects:  54%|█████▎    | 22/41 [01:15<01:04,  3.41s/it]predicting train subjects:  56%|█████▌    | 23/41 [01:19<01:02,  3.49s/it]predicting train subjects:  59%|█████▊    | 24/41 [01:23<01:01,  3.59s/it]predicting train subjects:  61%|██████    | 25/41 [01:27<00:57,  3.62s/it]predicting train subjects:  63%|██████▎   | 26/41 [01:32<01:02,  4.17s/it]predicting train subjects:  66%|██████▌   | 27/41 [01:39<01:08,  4.87s/it]predicting train subjects:  68%|██████▊   | 28/41 [01:44<01:06,  5.13s/it]predicting train subjects:  71%|███████   | 29/41 [01:49<00:58,  4.90s/it]predicting train subjects:  73%|███████▎  | 30/41 [01:52<00:50,  4.55s/it]predicting train subjects:  76%|███████▌  | 31/41 [01:56<00:44,  4.41s/it]predicting train subjects:  78%|███████▊  | 32/41 [02:01<00:40,  4.49s/it]predicting train subjects:  80%|████████  | 33/41 [02:06<00:36,  4.50s/it]predicting train subjects:  83%|████████▎ | 34/41 [02:09<00:28,  4.07s/it]predicting train subjects:  85%|████████▌ | 35/41 [02:12<00:23,  3.87s/it]predicting train subjects:  88%|████████▊ | 36/41 [02:16<00:19,  3.82s/it]predicting train subjects:  90%|█████████ | 37/41 [02:19<00:14,  3.74s/it]predicting train subjects:  93%|█████████▎| 38/41 [02:23<00:10,  3.60s/it]predicting train subjects:  95%|█████████▌| 39/41 [02:26<00:06,  3.49s/it]predicting train subjects:  98%|█████████▊| 40/41 [02:30<00:03,  3.56s/it]predicting train subjects: 100%|██████████| 41/41 [02:34<00:00,  3.89s/it]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:00<00:35,  1.14it/s]Loading train:   5%|▍         | 2/41 [00:01<00:31,  1.22it/s]Loading train:   7%|▋         | 3/41 [00:02<00:28,  1.32it/s]Loading train:  10%|▉         | 4/41 [00:02<00:24,  1.49it/s]Loading train:  12%|█▏        | 5/41 [00:03<00:23,  1.51it/s]Loading train:  15%|█▍        | 6/41 [00:03<00:22,  1.54it/s]Loading train:  17%|█▋        | 7/41 [00:04<00:20,  1.64it/s]Loading train:  20%|█▉        | 8/41 [00:04<00:17,  1.88it/s]Loading train:  22%|██▏       | 9/41 [00:05<00:17,  1.78it/s]Loading train:  24%|██▍       | 10/41 [00:05<00:17,  1.81it/s]Loading train:  27%|██▋       | 11/41 [00:06<00:17,  1.72it/s]Loading train:  29%|██▉       | 12/41 [00:06<00:14,  2.01it/s]Loading train:  32%|███▏      | 13/41 [00:07<00:15,  1.84it/s]Loading train:  34%|███▍      | 14/41 [00:08<00:15,  1.80it/s]Loading train:  37%|███▋      | 15/41 [00:08<00:13,  1.96it/s]Loading train:  39%|███▉      | 16/41 [00:09<00:13,  1.85it/s]Loading train:  41%|████▏     | 17/41 [00:09<00:14,  1.62it/s]Loading train:  44%|████▍     | 18/41 [00:10<00:14,  1.61it/s]Loading train:  46%|████▋     | 19/41 [00:11<00:13,  1.66it/s]Loading train:  49%|████▉     | 20/41 [00:11<00:12,  1.74it/s]Loading train:  51%|█████     | 21/41 [00:12<00:11,  1.70it/s]Loading train:  54%|█████▎    | 22/41 [00:12<00:11,  1.70it/s]Loading train:  56%|█████▌    | 23/41 [00:13<00:10,  1.68it/s]Loading train:  59%|█████▊    | 24/41 [00:14<00:10,  1.59it/s]Loading train:  61%|██████    | 25/41 [00:14<00:10,  1.47it/s]Loading train:  63%|██████▎   | 26/41 [00:15<00:10,  1.41it/s]Loading train:  66%|██████▌   | 27/41 [00:16<00:10,  1.34it/s]Loading train:  68%|██████▊   | 28/41 [00:17<00:09,  1.32it/s]Loading train:  71%|███████   | 29/41 [00:17<00:08,  1.42it/s]Loading train:  73%|███████▎  | 30/41 [00:18<00:07,  1.40it/s]Loading train:  76%|███████▌  | 31/41 [00:19<00:07,  1.40it/s]Loading train:  78%|███████▊  | 32/41 [00:19<00:06,  1.46it/s]Loading train:  80%|████████  | 33/41 [00:20<00:05,  1.46it/s]Loading train:  83%|████████▎ | 34/41 [00:21<00:04,  1.52it/s]Loading train:  85%|████████▌ | 35/41 [00:21<00:03,  1.59it/s]Loading train:  88%|████████▊ | 36/41 [00:22<00:02,  1.69it/s]Loading train:  90%|█████████ | 37/41 [00:22<00:02,  1.73it/s]Loading train:  93%|█████████▎| 38/41 [00:23<00:01,  1.53it/s]Loading train:  95%|█████████▌| 39/41 [00:24<00:01,  1.56it/s]Loading train:  98%|█████████▊| 40/41 [00:25<00:00,  1.40it/s]Loading train: 100%|██████████| 41/41 [00:25<00:00,  1.43it/s]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   5%|▍         | 2/41 [00:00<00:02, 15.18it/s]concatenating: train:  10%|▉         | 4/41 [00:00<00:02, 16.13it/s]concatenating: train:  15%|█▍        | 6/41 [00:00<00:02, 15.46it/s]concatenating: train:  20%|█▉        | 8/41 [00:00<00:02, 14.69it/s]concatenating: train:  24%|██▍       | 10/41 [00:00<00:01, 15.80it/s]concatenating: train:  32%|███▏      | 13/41 [00:00<00:01, 17.52it/s]concatenating: train:  39%|███▉      | 16/41 [00:00<00:01, 19.08it/s]concatenating: train:  46%|████▋     | 19/41 [00:01<00:01, 17.66it/s]concatenating: train:  51%|█████     | 21/41 [00:01<00:01, 15.00it/s]concatenating: train:  56%|█████▌    | 23/41 [00:01<00:01, 16.05it/s]concatenating: train:  61%|██████    | 25/41 [00:01<00:01, 15.21it/s]concatenating: train:  71%|███████   | 29/41 [00:01<00:00, 18.57it/s]concatenating: train: 100%|██████████| 41/41 [00:01<00:00, 23.93it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:00<00:06,  1.44it/s]Loading test:  18%|█▊        | 2/11 [00:01<00:06,  1.39it/s]Loading test:  27%|██▋       | 3/11 [00:01<00:04,  1.60it/s]Loading test:  36%|███▋      | 4/11 [00:02<00:04,  1.49it/s]Loading test:  45%|████▌     | 5/11 [00:03<00:04,  1.40it/s]Loading test:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s]Loading test:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s]Loading test:  73%|███████▎  | 8/11 [00:06<00:03,  1.00s/it]Loading test:  82%|████████▏ | 9/11 [00:07<00:01,  1.13it/s]Loading test:  91%|█████████ | 10/11 [00:08<00:00,  1.23it/s]Loading test: 100%|██████████| 11/11 [00:08<00:00,  1.25it/s]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  27%|██▋       | 3/11 [00:00<00:00, 26.82it/s]concatenating: validation:  45%|████▌     | 5/11 [00:00<00:00, 15.08it/s]concatenating: validation:  55%|█████▍    | 6/11 [00:00<00:00, 10.46it/s]concatenating: validation:  64%|██████▎   | 7/11 [00:00<00:00,  9.95it/s]concatenating: validation:  91%|█████████ | 10/11 [00:00<00:00, 11.85it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 13.22it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 92, 116, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 92, 116, 20)  200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 92, 116, 20)  80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 92, 116, 20)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 92, 116, 20)  3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 92, 116, 20)  80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 92, 116, 20)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 46, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 46, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 46, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 46, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 46, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 46, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 46, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 46, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 46, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 23, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 23, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 23, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 23, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 23, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 23, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 23, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 23, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 23, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 23, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 46, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 46, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 46, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 46, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 46, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 46, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 46, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 46, 58, 140)  0           concatenate_4[0][0]              2019-07-28 23:27:57.957538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-28 23:27:57.957657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 23:27:57.957673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-28 23:27:57.957682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-28 23:27:57.958060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 92, 116, 20)  11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 92, 116, 40)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 92, 116, 20)  7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 92, 116, 20)  80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 92, 116, 20)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 92, 116, 20)  3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 92, 116, 20)  80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 92, 116, 20)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 92, 116, 60)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 92, 116, 60)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 92, 116, 2)   122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.98017347 0.01982653]
Train on 4075 samples, validate on 1081 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 15s - loss: 1.1536 - acc: 0.7761 - mDice: 0.1522 - val_loss: 0.9690 - val_acc: 0.9358 - val_mDice: 0.1477

Epoch 00001: val_mDice improved from -inf to 0.14773, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.2049 - acc: 0.9874 - mDice: 0.6826 - val_loss: 0.1077 - val_acc: 0.9927 - val_mDice: 0.8152

Epoch 00002: val_mDice improved from 0.14773 to 0.81518, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.1034 - acc: 0.9919 - mDice: 0.8242 - val_loss: 0.0906 - val_acc: 0.9916 - val_mDice: 0.8437

Epoch 00003: val_mDice improved from 0.81518 to 0.84366, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0862 - acc: 0.9933 - mDice: 0.8519 - val_loss: 0.0914 - val_acc: 0.9923 - val_mDice: 0.8423

Epoch 00004: val_mDice did not improve from 0.84366
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0781 - acc: 0.9941 - mDice: 0.8653 - val_loss: 0.0780 - val_acc: 0.9935 - val_mDice: 0.8649

Epoch 00005: val_mDice improved from 0.84366 to 0.86493, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0737 - acc: 0.9946 - mDice: 0.8723 - val_loss: 0.0743 - val_acc: 0.9940 - val_mDice: 0.8709

Epoch 00006: val_mDice improved from 0.86493 to 0.87089, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0675 - acc: 0.9952 - mDice: 0.8820 - val_loss: 0.0692 - val_acc: 0.9946 - val_mDice: 0.8776

Epoch 00007: val_mDice improved from 0.87089 to 0.87763, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0650 - acc: 0.9954 - mDice: 0.8842 - val_loss: 0.0778 - val_acc: 0.9942 - val_mDice: 0.8607

Epoch 00008: val_mDice did not improve from 0.87763
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0603 - acc: 0.9957 - mDice: 0.8910 - val_loss: 0.0668 - val_acc: 0.9950 - val_mDice: 0.8781

Epoch 00009: val_mDice improved from 0.87763 to 0.87814, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0579 - acc: 0.9959 - mDice: 0.8944 - val_loss: 0.0722 - val_acc: 0.9948 - val_mDice: 0.8687

Epoch 00010: val_mDice did not improve from 0.87814
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0564 - acc: 0.9960 - mDice: 0.8967 - val_loss: 0.0774 - val_acc: 0.9946 - val_mDice: 0.8598

Epoch 00011: val_mDice did not improve from 0.87814
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0550 - acc: 0.9960 - mDice: 0.8989 - val_loss: 0.0701 - val_acc: 0.9949 - val_mDice: 0.8721

Epoch 00012: val_mDice did not improve from 0.87814
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0544 - acc: 0.9961 - mDice: 0.8997 - val_loss: 0.0626 - val_acc: 0.9953 - val_mDice: 0.8848

Epoch 00013: val_mDice improved from 0.87814 to 0.88481, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0520 - acc: 0.9962 - mDice: 0.9038 - val_loss: 0.0653 - val_acc: 0.9952 - val_mDice: 0.8800

Epoch 00014: val_mDice did not improve from 0.88481
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0524 - acc: 0.9962 - mDice: 0.9031 - val_loss: 0.0549 - val_acc: 0.9957 - val_mDice: 0.8982

Epoch 00015: val_mDice improved from 0.88481 to 0.89819, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0515 - acc: 0.9963 - mDice: 0.9046 - val_loss: 0.0560 - val_acc: 0.9957 - val_mDice: 0.8961

Epoch 00016: val_mDice did not improve from 0.89819
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0514 - acc: 0.9963 - mDice: 0.9047 - val_loss: 0.0562 - val_acc: 0.9957 - val_mDice: 0.8959

Epoch 00017: val_mDice did not improve from 0.89819
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0501 - acc: 0.9964 - mDice: 0.9070 - val_loss: 0.0569 - val_acc: 0.9957 - val_mDice: 0.8946

Epoch 00018: val_mDice did not improve from 0.89819
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0481 - acc: 0.9965 - mDice: 0.9106 - val_loss: 0.0524 - val_acc: 0.9959 - val_mDice: 0.9026

Epoch 00019: val_mDice improved from 0.89819 to 0.90257, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0483 - acc: 0.9965 - mDice: 0.9103 - val_loss: 0.0543 - val_acc: 0.9958 - val_mDice: 0.8992

Epoch 00020: val_mDice did not improve from 0.90257
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0472 - acc: 0.9966 - mDice: 0.9121 - val_loss: 0.0561 - val_acc: 0.9958 - val_mDice: 0.8961

Epoch 00021: val_mDice did not improve from 0.90257
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0469 - acc: 0.9966 - mDice: 0.9126 - val_loss: 0.0591 - val_acc: 0.9956 - val_mDice: 0.8907

Epoch 00022: val_mDice did not improve from 0.90257
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0472 - acc: 0.9966 - mDice: 0.9120 - val_loss: 0.0559 - val_acc: 0.9957 - val_mDice: 0.8964

Epoch 00023: val_mDice did not improve from 0.90257
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0464 - acc: 0.9966 - mDice: 0.9135 - val_loss: 0.0561 - val_acc: 0.9957 - val_mDice: 0.8960

Epoch 00024: val_mDice did not improve from 0.90257
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0457 - acc: 0.9967 - mDice: 0.9148 - val_loss: 0.0552 - val_acc: 0.9958 - val_mDice: 0.8976

Epoch 00025: val_mDice did not improve from 0.90257
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0460 - acc: 0.9966 - mDice: 0.9141 - val_loss: 0.0564 - val_acc: 0.9957 - val_mDice: 0.8956

Epoch 00026: val_mDice did not improve from 0.90257
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0459 - acc: 0.9967 - mDice: 0.9144 - val_loss: 0.0552 - val_acc: 0.9958 - val_mDice: 0.8975

Epoch 00027: val_mDice did not improve from 0.90257
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0466 - acc: 0.9966 - mDice: 0.9132 - val_loss: 0.0515 - val_acc: 0.9960 - val_mDice: 0.9042

Epoch 00028: val_mDice improved from 0.90257 to 0.90417, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/1-THALAMUS/sd1/best_model_weights.h5
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0452 - acc: 0.9967 - mDice: 0.9155 - val_loss: 0.0550 - val_acc: 0.9958 - val_mDice: 0.8979

Epoch 00029: val_mDice did not improve from 0.90417
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0451 - acc: 0.9967 - mDice: 0.9158 - val_loss: 0.0553 - val_acc: 0.9958 - val_mDice: 0.8973

Epoch 00030: val_mDice did not improve from 0.90417
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0445 - acc: 0.9967 - mDice: 0.9168 - val_loss: 0.0528 - val_acc: 0.9959 - val_mDice: 0.9018

Epoch 00031: val_mDice did not improve from 0.90417
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0446 - acc: 0.9967 - mDice: 0.9167 - val_loss: 0.0544 - val_acc: 0.9959 - val_mDice: 0.8991

Epoch 00032: val_mDice did not improve from 0.90417
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0441 - acc: 0.9968 - mDice: 0.9176 - val_loss: 0.0560 - val_acc: 0.9957 - val_mDice: 0.8962

Epoch 00033: val_mDice did not improve from 0.90417
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0436 - acc: 0.9968 - mDice: 0.9184 - val_loss: 0.0561 - val_acc: 0.9958 - val_mDice: 0.8960

Epoch 00034: val_mDice did not improve from 0.90417
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0441 - acc: 0.9968 - mDice: 0.9175 - val_loss: 0.0515 - val_acc: 0.9960 - val_mDice: 0.9041

Epoch 00035: val_mDice did not improve from 0.90417
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0431 - acc: 0.9968 - mDice: 0.9192 - val_loss: 0.0575 - val_acc: 0.9957 - val_mDice: 0.8936

Epoch 00036: val_mDice did not improve from 0.90417
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0430 - acc: 0.9968 - mDice: 0.9193 - val_loss: 0.0544 - val_acc: 0.9959 - val_mDice: 0.8990

Epoch 00037: val_mDice did not improve from 0.90417
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0428 - acc: 0.9969 - mDice: 0.9198 - val_loss: 0.0524 - val_acc: 0.9960 - val_mDice: 0.9026

Epoch 00038: val_mDice did not improve from 0.90417
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0423 - acc: 0.9969 - mDice: 0.9206 - val_loss: 0.0520 - val_acc: 0.9960 - val_mDice: 0.9031

Epoch 00039: val_mDice did not improve from 0.90417
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0418 - acc: 0.9969 - mDice: 0.9215 - val_loss: 0.0526 - val_acc: 0.9960 - val_mDice: 0.9022

Epoch 00040: val_mDice did not improve from 0.90417
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0418 - acc: 0.9969 - mDice: 0.9215 - val_loss: 0.0535 - val_acc: 0.9959 - val_mDice: 0.9006

Epoch 00041: val_mDice did not improve from 0.90417
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0415 - acc: 0.9969 - mDice: 0.9220 - val_loss: 0.0545 - val_acc: 0.9959 - val_mDice: 0.8988

Epoch 00042: val_mDice did not improve from 0.90417
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0424 - acc: 0.9969 - mDice: 0.9205 - val_loss: 0.0517 - val_acc: 0.9960 - val_mDice: 0.9037

Epoch 00043: val_mDice did not improve from 0.90417
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0421 - acc: 0.9969 - mDice: 0.9211 - val_loss: 0.0519 - val_acc: 0.9960 - val_mDice: 0.9033

Epoch 00044: val_mDice did not improve from 0.90417
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0413 - acc: 0.9969 - mDice: 0.9224 - val_loss: 0.0529 - val_acc: 0.9960 - val_mDice: 0.9016

Epoch 00045: val_mDice did not improve from 0.90417
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0413 - acc: 0.9969 - mDice: 0.9224 - val_loss: 0.0546 - val_acc: 0.9959 - val_mDice: 0.8986

Epoch 00046: val_mDice did not improve from 0.90417
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0410 - acc: 0.9970 - mDice: 0.9229 - val_loss: 0.0525 - val_acc: 0.9960 - val_mDice: 0.9024

Epoch 00047: val_mDice did not improve from 0.90417
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0411 - acc: 0.9970 - mDice: 0.9228 - val_loss: 0.0531 - val_acc: 0.9959 - val_mDice: 0.9013

Epoch 00048: val_mDice did not improve from 0.90417
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0408 - acc: 0.9970 - mDice: 0.9233 - val_loss: 0.0526 - val_acc: 0.9960 - val_mDice: 0.9021

Epoch 00049: val_mDice did not improve from 0.90417
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0413 - acc: 0.9970 - mDice: 0.9225 - val_loss: 0.0516 - val_acc: 0.9960 - val_mDice: 0.9039

Epoch 00050: val_mDice did not improve from 0.90417
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0412 - acc: 0.9969 - mDice: 0.9226 - val_loss: 0.0527 - val_acc: 0.9960 - val_mDice: 0.9020

Epoch 00051: val_mDice did not improve from 0.90417
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0411 - acc: 0.9970 - mDice: 0.9228 - val_loss: 0.0542 - val_acc: 0.9959 - val_mDice: 0.8994

Epoch 00052: val_mDice did not improve from 0.90417
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0407 - acc: 0.9970 - mDice: 0.9235 - val_loss: 0.0524 - val_acc: 0.9960 - val_mDice: 0.9025

Epoch 00053: val_mDice did not improve from 0.90417
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0405 - acc: 0.9970 - mDice: 0.9239 - val_loss: 0.0547 - val_acc: 0.9959 - val_mDice: 0.8984

Epoch 00054: val_mDice did not improve from 0.90417
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0400 - acc: 0.9970 - mDice: 0.9247 - val_loss: 0.0527 - val_acc: 0.9960 - val_mDice: 0.9020

Epoch 00055: val_mDice did not improve from 0.90417
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0400 - acc: 0.9970 - mDice: 0.9247 - val_loss: 0.0538 - val_acc: 0.9959 - val_mDice: 0.9001

Epoch 00056: val_mDice did not improve from 0.90417
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0399 - acc: 0.9970 - mDice: 0.9249 - val_loss: 0.0544 - val_acc: 0.9959 - val_mDice: 0.8989

Epoch 00057: val_mDice did not improve from 0.90417
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0400 - acc: 0.9970 - mDice: 0.9248 - val_loss: 0.0539 - val_acc: 0.9959 - val_mDice: 0.8998

Epoch 00058: val_mDice did not improve from 0.90417
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0397 - acc: 0.9971 - mDice: 0.9252 - val_loss: 0.0530 - val_acc: 0.9960 - val_mDice: 0.9014

Epoch 00059: val_mDice did not improve from 0.90417
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0396 - acc: 0.9971 - mDice: 0.9255 - val_loss: 0.0529 - val_acc: 0.9959 - val_mDice: 0.9016

Epoch 00060: val_mDice did not improve from 0.90417
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0396 - acc: 0.9971 - mDice: 0.9255 - val_loss: 0.0533 - val_acc: 0.9959 - val_mDice: 0.9010

Epoch 00061: val_mDice did not improve from 0.90417
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0396 - acc: 0.9971 - mDice: 0.9254 - val_loss: 0.0543 - val_acc: 0.9959 - val_mDice: 0.8991

Epoch 00062: val_mDice did not improve from 0.90417
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0399 - acc: 0.9970 - mDice: 0.9249 - val_loss: 0.0545 - val_acc: 0.9959 - val_mDice: 0.8987

Epoch 00063: val_mDice did not improve from 0.90417
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0393 - acc: 0.9971 - mDice: 0.9260 - val_loss: 0.0526 - val_acc: 0.9960 - val_mDice: 0.9022

Epoch 00064: val_mDice did not improve from 0.90417
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0393 - acc: 0.9971 - mDice: 0.9259 - val_loss: 0.0544 - val_acc: 0.9959 - val_mDice: 0.8989

Epoch 00065: val_mDice did not improve from 0.90417
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0391 - acc: 0.9971 - mDice: 0.9264 - val_loss: 0.0531 - val_acc: 0.9959 - val_mDice: 0.9012

Epoch 00066: val_mDice did not improve from 0.90417
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0391 - acc: 0.9971 - mDice: 0.9265 - val_loss: 0.0536 - val_acc: 0.9959 - val_mDice: 0.9004

Epoch 00067: val_mDice did not improve from 0.90417
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0394 - acc: 0.9971 - mDice: 0.9257 - val_loss: 0.0528 - val_acc: 0.9960 - val_mDice: 0.9017

Epoch 00068: val_mDice did not improve from 0.90417
Restoring model weights from the end of the best epoch
Epoch 00068: early stopping
{'val_loss': [0.9689504399440776, 0.10771650210312403, 0.09060114227187072, 0.09143936060968645, 0.07795444487229637, 0.07427543403785831, 0.06923510975770439, 0.0778277621679456, 0.06676049406789278, 0.0721576980260872, 0.07737397366340353, 0.07006937886392706, 0.06259042997461133, 0.06531329412671508, 0.05487702436448798, 0.05604293668482394, 0.05616665019496076, 0.05690738387150857, 0.052368990756454126, 0.05429040576149533, 0.05606393427152969, 0.059147637930184356, 0.05588317069050121, 0.05612671000333322, 0.055210538012053986, 0.05637292128666257, 0.05523919713210442, 0.05145728214541372, 0.05501092813571331, 0.055348485349836885, 0.052817330780980984, 0.05435288199856808, 0.05596382147137155, 0.05608736574098427, 0.05148614482966206, 0.05751697155651288, 0.054366989800435986, 0.05236588639382077, 0.05204044029488815, 0.05259145490625299, 0.053483229795613846, 0.054494753031957825, 0.05173093269714374, 0.05191741647693319, 0.052895285171921005, 0.05462446013443124, 0.05246412507658971, 0.053055499462371175, 0.05262925135011368, 0.0516050576295266, 0.05265893279527427, 0.05415151368871428, 0.0524094699193602, 0.054715183245622925, 0.052661010245969184, 0.05375277496371194, 0.054421835099101395, 0.053921883624257254, 0.05302913212988467, 0.05292549063550665, 0.053255992049866537, 0.05432785187643728, 0.05454516551775253, 0.05258665109099777, 0.05440574185879665, 0.053149170260342264, 0.05355296054102005, 0.05282604110932924], 'val_acc': [0.9357630653473981, 0.9926721742040685, 0.9916236697800396, 0.9922717803611897, 0.9934770180934231, 0.994009497317862, 0.9946150647391885, 0.9942109448065038, 0.9950057372354336, 0.9947798395465636, 0.9945594058557311, 0.9949271928701656, 0.9953365940721249, 0.9952053566093692, 0.9957468654688147, 0.9957020490065865, 0.9956658131319764, 0.9956831532314241, 0.9959376507837612, 0.9958470604355748, 0.9957578693353721, 0.9955902246277604, 0.9957448705052139, 0.9957238872993004, 0.9957733934505685, 0.9957184371410092, 0.9957811788317233, 0.9960088199426684, 0.995790899406419, 0.9957716446249271, 0.9959421571926537, 0.9958560036686589, 0.9957278063184789, 0.99575630301794, 0.9960109021904069, 0.9956851486361324, 0.99587540826127, 0.9959672101662184, 0.9960077773852, 0.9959547413266665, 0.9959223979496493, 0.9958714215872356, 0.9960221586461204, 0.9960005853487097, 0.9959675498740966, 0.9958558287144148, 0.9959760395384408, 0.9959477091916284, 0.9959643424163804, 0.9960245903612396, 0.9959686918461577, 0.9958794881743926, 0.9959856761373049, 0.9958575547678838, 0.9959776054699039, 0.9959073969878939, 0.9958812181426903, 0.9959108676213451, 0.9959527421725447, 0.9959471942538138, 0.9959379950681293, 0.9958825346830263, 0.9958793102978116, 0.9959722354828925, 0.9958615462389618, 0.995946238098268, 0.9959295013235412, 0.9959619139544287], 'val_mDice': [0.14772630559416638, 0.8151754858756264, 0.8436629735794914, 0.8423489320068641, 0.8649280881242108, 0.8708901633607581, 0.877627746855959, 0.8606806870525794, 0.8781360287450178, 0.8686999822523063, 0.859812147522502, 0.8721426327288095, 0.8848055802375271, 0.8800075308908256, 0.898186665624518, 0.8961225748062134, 0.8959203804675128, 0.8946142917865961, 0.9025703009138716, 0.8991961477532858, 0.8960832333145706, 0.8907043982941613, 0.8963984968924721, 0.895964261561384, 0.8975626967440702, 0.8955596672727707, 0.8975161231854357, 0.9041675871770543, 0.8979240717102707, 0.8973157699741113, 0.901771099340243, 0.8990687314389924, 0.8962195177082658, 0.8960212862789355, 0.9041210467014789, 0.8935689308597025, 0.8990424532343346, 0.9025551142458779, 0.9031324505144308, 0.9021540060294766, 0.9005874467050444, 0.8988278434852226, 0.9036662136372541, 0.9033397715142433, 0.9016137714743283, 0.8985716077489615, 0.9023906014563308, 0.9013321068306746, 0.9021011278322734, 0.9038916460128982, 0.9020465934971325, 0.8994084465955827, 0.9024754022691781, 0.8984491793123469, 0.9020276238916098, 0.9000990909630231, 0.8989206583934399, 0.8998084339579424, 0.9013764449118685, 0.9015553478947621, 0.9009796070015949, 0.8990850073115678, 0.8987144992706624, 0.9021642082935125, 0.8989464406279035, 0.9011565959089669, 0.900444142913289, 0.9017266886968728], 'loss': [1.1536128146151092, 0.20494972077981094, 0.10341582721544922, 0.08624801908168325, 0.07808704564542127, 0.07374735688504998, 0.06750475336437577, 0.0650325086127761, 0.060289745681856306, 0.05794957440140788, 0.056381149251768195, 0.0549536914410401, 0.05442497607297692, 0.05202835757125375, 0.052372547632163285, 0.051529913637901374, 0.05143277280809689, 0.050096599236587804, 0.04806603776988076, 0.048252026430485435, 0.04720517501600681, 0.04690015503416763, 0.047245608699833685, 0.04640942855389572, 0.04567817024932317, 0.04604798539375966, 0.0458705961658545, 0.04656077526006962, 0.04524051308906152, 0.045080880240245834, 0.0445386128374404, 0.04456818421468413, 0.04407114617846495, 0.04361516858902446, 0.044060260263140215, 0.04314315426477625, 0.043047671966209004, 0.042811788458582814, 0.04232875934224919, 0.04181212223380621, 0.04183441208549804, 0.0415273046155283, 0.0423797718677784, 0.04205683776480289, 0.04134179171791837, 0.04130541828055323, 0.04103883449559563, 0.04111346664147143, 0.04080620725736296, 0.04125506711975197, 0.041221949974634894, 0.041065462939212656, 0.040678757374272026, 0.0404910428300957, 0.040042362787240854, 0.04000705096619261, 0.0399209804650099, 0.03998924460422042, 0.039747843141753246, 0.03957703485627847, 0.03960213088001942, 0.039630178156440245, 0.0399209475316153, 0.0393228256857834, 0.03934638469588537, 0.039085786126873974, 0.03906030303861466, 0.03944451894405429], 'acc': [0.7761245556404254, 0.9873917417292215, 0.9918519646112173, 0.9933428680238549, 0.9941186641622906, 0.9945541788463944, 0.9951651227986155, 0.9954071216788029, 0.9957297857553681, 0.9958775189756616, 0.9959629496182401, 0.9960494944654359, 0.996081508010443, 0.9962411577716196, 0.9962144128384034, 0.9962703618535235, 0.9962790085494153, 0.9963623157308146, 0.9965086330665401, 0.9964967633317585, 0.9965685105031253, 0.9965953245484755, 0.99655682945544, 0.9966171438708628, 0.9966691326509955, 0.9966485116379392, 0.9966542139375136, 0.996599599993302, 0.9966876682328301, 0.9967156758337664, 0.9967420477808618, 0.9967496585260871, 0.9967728407105054, 0.9968032372509775, 0.9967718544913216, 0.99683568338675, 0.9968350376088195, 0.9968515034833568, 0.9968799765855988, 0.9969161244257827, 0.9969137252474124, 0.9969405469718886, 0.9968816718440846, 0.996898525697322, 0.9969444980650591, 0.9969471334679726, 0.9969614820977661, 0.9969558536640706, 0.9969776536058064, 0.996957766863466, 0.9969477836339752, 0.9969580407522939, 0.9969828052023437, 0.9970030364083367, 0.9970388200385439, 0.9970375061766502, 0.9970424281307525, 0.9970363385838233, 0.9970573311203097, 0.9970714486449774, 0.997062621306788, 0.9970623697239929, 0.9970440356277981, 0.9970886897455695, 0.9970821646824937, 0.9971061060033692, 0.9971058248010881, 0.9970788754568509], 'mDice': [0.1522470691635565, 0.6826017498970032, 0.8242276120770928, 0.8518838933640462, 0.8653287320780608, 0.87232980479492, 0.8819515580779935, 0.8842276331105846, 0.891028628393185, 0.8943553000140044, 0.8967010031448551, 0.8989491089721399, 0.8997124301875296, 0.9038208200156324, 0.9031229004538133, 0.9045524630078509, 0.9046753085464057, 0.9070024483042992, 0.9106057425218126, 0.9102543446183936, 0.9121087431176308, 0.9126399108237284, 0.9120056314702414, 0.9134886389129733, 0.9147919306725812, 0.9141091060784697, 0.9144150559887564, 0.9131676883785271, 0.9155256207003916, 0.9157962528474491, 0.9167574190654637, 0.9166896416365735, 0.9175850434537314, 0.9183882310346592, 0.9175438939428037, 0.9191738582827562, 0.9193292772111717, 0.9197516287762695, 0.920611748300447, 0.92154022447902, 0.9214924266733275, 0.9220424320068827, 0.9204901807878646, 0.9210724538089308, 0.9223687601235747, 0.9224264168300511, 0.9229062579892164, 0.9227712399389115, 0.9233242061240542, 0.9225018423027788, 0.9225634804532572, 0.9228369191380367, 0.9235363006591797, 0.9238729144166584, 0.924688111053654, 0.9247446502644592, 0.9248970604381679, 0.9247729566199648, 0.9252081966838954, 0.925520259544162, 0.9254745530204539, 0.9254213666623355, 0.9248908169430458, 0.9259749241401812, 0.9259313114581664, 0.9264003371168499, 0.92645374580395, 0.9257473133824354], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:00<00:08,  1.16it/s]predicting test subjects:  18%|█▊        | 2/11 [00:01<00:06,  1.41it/s]predicting test subjects:  27%|██▋       | 3/11 [00:01<00:04,  1.78it/s]predicting test subjects:  36%|███▋      | 4/11 [00:01<00:03,  2.04it/s]predicting test subjects:  45%|████▌     | 5/11 [00:02<00:02,  2.25it/s]predicting test subjects:  55%|█████▍    | 6/11 [00:02<00:02,  2.47it/s]predicting test subjects:  64%|██████▎   | 7/11 [00:02<00:01,  2.67it/s]predicting test subjects:  73%|███████▎  | 8/11 [00:03<00:01,  2.68it/s]predicting test subjects:  82%|████████▏ | 9/11 [00:03<00:00,  2.71it/s]predicting test subjects:  91%|█████████ | 10/11 [00:03<00:00,  2.92it/s]predicting test subjects: 100%|██████████| 11/11 [00:04<00:00,  2.95it/s]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:00<00:12,  3.24it/s]predicting train subjects:   5%|▍         | 2/41 [00:00<00:12,  3.22it/s]predicting train subjects:   7%|▋         | 3/41 [00:00<00:11,  3.37it/s]predicting train subjects:  10%|▉         | 4/41 [00:01<00:10,  3.51it/s]predicting train subjects:  12%|█▏        | 5/41 [00:01<00:11,  3.26it/s]predicting train subjects:  15%|█▍        | 6/41 [00:01<00:10,  3.26it/s]predicting train subjects:  17%|█▋        | 7/41 [00:02<00:10,  3.33it/s]predicting train subjects:  20%|█▉        | 8/41 [00:02<00:08,  3.71it/s]predicting train subjects:  22%|██▏       | 9/41 [00:02<00:09,  3.47it/s]predicting train subjects:  24%|██▍       | 10/41 [00:02<00:09,  3.28it/s]predicting train subjects:  27%|██▋       | 11/41 [00:03<00:09,  3.12it/s]predicting train subjects:  29%|██▉       | 12/41 [00:03<00:08,  3.59it/s]predicting train subjects:  32%|███▏      | 13/41 [00:03<00:07,  3.62it/s]predicting train subjects:  34%|███▍      | 14/41 [00:04<00:07,  3.56it/s]predicting train subjects:  37%|███▋      | 15/41 [00:04<00:06,  4.01it/s]predicting train subjects:  39%|███▉      | 16/41 [00:04<00:06,  3.82it/s]predicting train subjects:  41%|████▏     | 17/41 [00:04<00:07,  3.31it/s]predicting train subjects:  44%|████▍     | 18/41 [00:05<00:07,  3.08it/s]predicting train subjects:  46%|████▋     | 19/41 [00:05<00:07,  3.14it/s]predicting train subjects:  49%|████▉     | 20/41 [00:05<00:06,  3.33it/s]predicting train subjects:  51%|█████     | 21/41 [00:06<00:06,  3.28it/s]predicting train subjects:  54%|█████▎    | 22/41 [00:06<00:05,  3.26it/s]predicting train subjects:  56%|█████▌    | 23/41 [00:06<00:05,  3.22it/s]predicting train subjects:  59%|█████▊    | 24/41 [00:07<00:05,  3.03it/s]predicting train subjects:  61%|██████    | 25/41 [00:07<00:05,  3.03it/s]predicting train subjects:  63%|██████▎   | 26/41 [00:07<00:05,  2.82it/s]predicting train subjects:  66%|██████▌   | 27/41 [00:08<00:05,  2.43it/s]predicting train subjects:  68%|██████▊   | 28/41 [00:08<00:05,  2.29it/s]predicting train subjects:  71%|███████   | 29/41 [00:09<00:05,  2.36it/s]predicting train subjects:  73%|███████▎  | 30/41 [00:09<00:04,  2.54it/s]predicting train subjects:  76%|███████▌  | 31/41 [00:09<00:03,  2.72it/s]predicting train subjects:  78%|███████▊  | 32/41 [00:10<00:03,  2.75it/s]predicting train subjects:  80%|████████  | 33/41 [00:10<00:03,  2.66it/s]predicting train subjects:  83%|████████▎ | 34/41 [00:11<00:02,  2.78it/s]predicting train subjects:  85%|████████▌ | 35/41 [00:11<00:02,  2.87it/s]predicting train subjects:  88%|████████▊ | 36/41 [00:11<00:01,  2.83it/s]predicting train subjects:  90%|█████████ | 37/41 [00:12<00:01,  2.86it/s]predicting train subjects:  93%|█████████▎| 38/41 [00:12<00:01,  2.95it/s]predicting train subjects:  95%|█████████▌| 39/41 [00:12<00:00,  3.01it/s]predicting train subjects:  98%|█████████▊| 40/41 [00:13<00:00,  3.01it/s]predicting train subjects: 100%|██████████| 41/41 [00:13<00:00,  2.90it/s]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:02<01:36,  2.42s/it]Loading train:   5%|▍         | 2/41 [00:04<01:33,  2.39s/it]Loading train:   7%|▋         | 3/41 [00:07<01:29,  2.36s/it]Loading train:  10%|▉         | 4/41 [00:09<01:25,  2.31s/it]Loading train:  12%|█▏        | 5/41 [00:12<01:28,  2.47s/it]Loading train:  15%|█▍        | 6/41 [00:14<01:23,  2.38s/it]Loading train:  17%|█▋        | 7/41 [00:16<01:18,  2.31s/it]Loading train:  20%|█▉        | 8/41 [00:17<01:07,  2.03s/it]Loading train:  22%|██▏       | 9/41 [00:20<01:08,  2.13s/it]Loading train:  24%|██▍       | 10/41 [00:22<01:10,  2.28s/it]Loading train:  27%|██▋       | 11/41 [00:25<01:10,  2.34s/it]Loading train:  29%|██▉       | 12/41 [00:26<00:58,  2.03s/it]Loading train:  32%|███▏      | 13/41 [00:28<00:58,  2.08s/it]Loading train:  34%|███▍      | 14/41 [00:30<00:57,  2.12s/it]Loading train:  37%|███▋      | 15/41 [00:32<00:49,  1.89s/it]Loading train:  39%|███▉      | 16/41 [00:34<00:49,  2.00s/it]Loading train:  41%|████▏     | 17/41 [00:37<00:55,  2.32s/it]Loading train:  44%|████▍     | 18/41 [00:40<00:55,  2.42s/it]Loading train:  46%|████▋     | 19/41 [00:42<00:53,  2.41s/it]Loading train:  49%|████▉     | 20/41 [00:44<00:47,  2.28s/it]Loading train:  51%|█████     | 21/41 [00:47<00:46,  2.34s/it]Loading train:  54%|█████▎    | 22/41 [00:49<00:44,  2.36s/it]Loading train:  56%|█████▌    | 23/41 [00:52<00:43,  2.40s/it]Loading train:  59%|█████▊    | 24/41 [00:54<00:41,  2.43s/it]Loading train:  61%|██████    | 25/41 [00:56<00:38,  2.41s/it]Loading train:  63%|██████▎   | 26/41 [00:59<00:38,  2.53s/it]Loading train:  66%|██████▌   | 27/41 [01:03<00:39,  2.79s/it]Loading train:  68%|██████▊   | 28/41 [01:05<00:36,  2.79s/it]Loading train:  71%|███████   | 29/41 [01:08<00:32,  2.73s/it]Loading train:  73%|███████▎  | 30/41 [01:11<00:29,  2.67s/it]Loading train:  76%|███████▌  | 31/41 [01:13<00:25,  2.58s/it]Loading train:  78%|███████▊  | 32/41 [01:15<00:23,  2.56s/it]Loading train:  80%|████████  | 33/41 [01:18<00:20,  2.55s/it]Loading train:  83%|████████▎ | 34/41 [01:20<00:16,  2.35s/it]Loading train:  85%|████████▌ | 35/41 [01:22<00:13,  2.25s/it]Loading train:  88%|████████▊ | 36/41 [01:24<00:11,  2.26s/it]Loading train:  90%|█████████ | 37/41 [01:26<00:09,  2.30s/it]Loading train:  93%|█████████▎| 38/41 [01:29<00:06,  2.29s/it]Loading train:  95%|█████████▌| 39/41 [01:31<00:04,  2.28s/it]Loading train:  98%|█████████▊| 40/41 [01:33<00:02,  2.28s/it]Loading train: 100%|██████████| 41/41 [01:36<00:00,  2.33s/it]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:  10%|▉         | 4/41 [00:00<00:00, 39.35it/s]concatenating: train:  22%|██▏       | 9/41 [00:00<00:00, 41.16it/s]concatenating: train:  39%|███▉      | 16/41 [00:00<00:00, 46.78it/s]concatenating: train:  59%|█████▊    | 24/41 [00:00<00:00, 52.51it/s]concatenating: train:  78%|███████▊  | 32/41 [00:00<00:00, 57.61it/s]concatenating: train: 100%|██████████| 41/41 [00:00<00:00, 57.21it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:02<00:25,  2.52s/it]Loading test:  18%|█▊        | 2/11 [00:04<00:22,  2.45s/it]Loading test:  27%|██▋       | 3/11 [00:06<00:16,  2.12s/it]Loading test:  36%|███▋      | 4/11 [00:08<00:15,  2.22s/it]Loading test:  45%|████▌     | 5/11 [00:10<00:13,  2.27s/it]Loading test:  55%|█████▍    | 6/11 [00:13<00:11,  2.32s/it]Loading test:  64%|██████▎   | 7/11 [00:15<00:09,  2.28s/it]Loading test:  73%|███████▎  | 8/11 [00:18<00:07,  2.40s/it]Loading test:  82%|████████▏ | 9/11 [00:20<00:04,  2.39s/it]Loading test:  91%|█████████ | 10/11 [00:22<00:02,  2.35s/it]Loading test: 100%|██████████| 11/11 [00:25<00:00,  2.34s/it]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  27%|██▋       | 3/11 [00:00<00:00, 23.33it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 29.39it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 92, 116, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 92, 116, 20)  200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 92, 116, 20)  80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 92, 116, 20)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 92, 116, 20)  3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 92, 116, 20)  80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 92, 116, 20)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 46, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 46, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 46, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 46, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 46, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 46, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 46, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 46, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 46, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 23, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 23, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 23, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 23, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 23, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 23, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 23, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 23, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 23, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 23, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 46, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 46, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 46, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 46, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 46, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 46, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 46, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-28 23:39:23.688205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-28 23:39:23.688298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 23:39:23.688314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-28 23:39:23.688323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-28 23:39:23.688666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 46, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 92, 116, 20)  11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 92, 116, 40)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 92, 116, 20)  7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 92, 116, 20)  80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 92, 116, 20)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 92, 116, 20)  3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 92, 116, 20)  80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 92, 116, 20)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 92, 116, 60)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 92, 116, 60)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 92, 116, 13)  793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.54299287e-02 3.09683806e-02 7.58752017e-02 1.02386755e-02
 2.77641018e-02 7.04461079e-03 7.68873531e-02 1.11754942e-01
 7.79226924e-02 1.35777423e-02 3.25521960e-01 1.76980503e-01
 3.39074689e-05]
Train on 4075 samples, validate on 1081 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 16s - loss: 5.2343 - acc: 0.2705 - mDice: 0.0095 - val_loss: 4.7673 - val_acc: 0.4164 - val_mDice: 0.0122

Epoch 00001: val_mDice improved from -inf to 0.01223, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 2.7695 - acc: 0.7706 - mDice: 0.0994 - val_loss: 1.9087 - val_acc: 0.9844 - val_mDice: 0.2096

Epoch 00002: val_mDice improved from 0.01223 to 0.20958, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.6765 - acc: 0.9819 - mDice: 0.2031 - val_loss: 1.2925 - val_acc: 0.9845 - val_mDice: 0.2899

Epoch 00003: val_mDice improved from 0.20958 to 0.28989, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 1.1493 - acc: 0.9850 - mDice: 0.3038 - val_loss: 1.3241 - val_acc: 0.9845 - val_mDice: 0.3201

Epoch 00004: val_mDice improved from 0.28989 to 0.32012, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.9313 - acc: 0.9852 - mDice: 0.3767 - val_loss: 1.5695 - val_acc: 0.9845 - val_mDice: 0.2975

Epoch 00005: val_mDice did not improve from 0.32012
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.7942 - acc: 0.9854 - mDice: 0.4325 - val_loss: 1.5274 - val_acc: 0.9845 - val_mDice: 0.3135

Epoch 00006: val_mDice did not improve from 0.32012
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.7120 - acc: 0.9857 - mDice: 0.4716 - val_loss: 0.8989 - val_acc: 0.9846 - val_mDice: 0.4361

Epoch 00007: val_mDice improved from 0.32012 to 0.43614, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.6839 - acc: 0.9859 - mDice: 0.4871 - val_loss: 0.5933 - val_acc: 0.9849 - val_mDice: 0.5457

Epoch 00008: val_mDice improved from 0.43614 to 0.54573, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.6130 - acc: 0.9861 - mDice: 0.5236 - val_loss: 0.5541 - val_acc: 0.9852 - val_mDice: 0.5672

Epoch 00009: val_mDice improved from 0.54573 to 0.56717, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.5612 - acc: 0.9866 - mDice: 0.5527 - val_loss: 0.5005 - val_acc: 0.9850 - val_mDice: 0.5983

Epoch 00010: val_mDice improved from 0.56717 to 0.59833, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.5417 - acc: 0.9867 - mDice: 0.5637 - val_loss: 0.4572 - val_acc: 0.9862 - val_mDice: 0.6231

Epoch 00011: val_mDice improved from 0.59833 to 0.62311, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.5302 - acc: 0.9869 - mDice: 0.5717 - val_loss: 0.8003 - val_acc: 0.9854 - val_mDice: 0.5260

Epoch 00012: val_mDice did not improve from 0.62311
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.5869 - acc: 0.9867 - mDice: 0.5542 - val_loss: 3.8297 - val_acc: 0.9858 - val_mDice: 0.1915

Epoch 00013: val_mDice did not improve from 0.62311
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.5779 - acc: 0.9870 - mDice: 0.5432 - val_loss: 0.5616 - val_acc: 0.9868 - val_mDice: 0.5848

Epoch 00014: val_mDice did not improve from 0.62311
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.4842 - acc: 0.9875 - mDice: 0.5982 - val_loss: 0.4527 - val_acc: 0.9871 - val_mDice: 0.6292

Epoch 00015: val_mDice improved from 0.62311 to 0.62919, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.4591 - acc: 0.9876 - mDice: 0.6141 - val_loss: 0.4090 - val_acc: 0.9876 - val_mDice: 0.6531

Epoch 00016: val_mDice improved from 0.62919 to 0.65312, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.4434 - acc: 0.9878 - mDice: 0.6245 - val_loss: 0.4067 - val_acc: 0.9880 - val_mDice: 0.6551

Epoch 00017: val_mDice improved from 0.65312 to 0.65506, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.4313 - acc: 0.9879 - mDice: 0.6323 - val_loss: 0.4063 - val_acc: 0.9879 - val_mDice: 0.6574

Epoch 00018: val_mDice improved from 0.65506 to 0.65736, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.4504 - acc: 0.9878 - mDice: 0.6279 - val_loss: 0.7507 - val_acc: 0.9861 - val_mDice: 0.5429

Epoch 00019: val_mDice did not improve from 0.65736
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.4740 - acc: 0.9876 - mDice: 0.6055 - val_loss: 0.4269 - val_acc: 0.9869 - val_mDice: 0.6466

Epoch 00020: val_mDice did not improve from 0.65736
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.4242 - acc: 0.9881 - mDice: 0.6380 - val_loss: 0.3957 - val_acc: 0.9880 - val_mDice: 0.6649

Epoch 00021: val_mDice improved from 0.65736 to 0.66493, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.4050 - acc: 0.9882 - mDice: 0.6497 - val_loss: 0.3933 - val_acc: 0.9877 - val_mDice: 0.6655

Epoch 00022: val_mDice improved from 0.66493 to 0.66546, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.4045 - acc: 0.9883 - mDice: 0.6508 - val_loss: 0.3975 - val_acc: 0.9882 - val_mDice: 0.6632

Epoch 00023: val_mDice did not improve from 0.66546
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.4007 - acc: 0.9882 - mDice: 0.6538 - val_loss: 0.4173 - val_acc: 0.9886 - val_mDice: 0.6534

Epoch 00024: val_mDice did not improve from 0.66546
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.4043 - acc: 0.9883 - mDice: 0.6566 - val_loss: 0.4509 - val_acc: 0.9876 - val_mDice: 0.6335

Epoch 00025: val_mDice did not improve from 0.66546
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.4593 - acc: 0.9878 - mDice: 0.6213 - val_loss: 0.5507 - val_acc: 0.9879 - val_mDice: 0.6176

Epoch 00026: val_mDice did not improve from 0.66546
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.4132 - acc: 0.9882 - mDice: 0.6444 - val_loss: 0.4140 - val_acc: 0.9889 - val_mDice: 0.6645

Epoch 00027: val_mDice did not improve from 0.66546
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3882 - acc: 0.9885 - mDice: 0.6617 - val_loss: 0.3813 - val_acc: 0.9886 - val_mDice: 0.6746

Epoch 00028: val_mDice improved from 0.66546 to 0.67460, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3789 - acc: 0.9886 - mDice: 0.6686 - val_loss: 0.4178 - val_acc: 0.9875 - val_mDice: 0.6617

Epoch 00029: val_mDice did not improve from 0.67460
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3751 - acc: 0.9886 - mDice: 0.6711 - val_loss: 0.4019 - val_acc: 0.9884 - val_mDice: 0.6679

Epoch 00030: val_mDice did not improve from 0.67460
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3766 - acc: 0.9886 - mDice: 0.6724 - val_loss: 0.3883 - val_acc: 0.9885 - val_mDice: 0.6712

Epoch 00031: val_mDice did not improve from 0.67460
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3689 - acc: 0.9887 - mDice: 0.6755 - val_loss: 0.4061 - val_acc: 0.9887 - val_mDice: 0.6733

Epoch 00032: val_mDice did not improve from 0.67460
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3648 - acc: 0.9887 - mDice: 0.6782 - val_loss: 0.3765 - val_acc: 0.9887 - val_mDice: 0.6779

Epoch 00033: val_mDice improved from 0.67460 to 0.67790, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3721 - acc: 0.9887 - mDice: 0.6739 - val_loss: 0.4191 - val_acc: 0.9888 - val_mDice: 0.6690

Epoch 00034: val_mDice did not improve from 0.67790
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3582 - acc: 0.9888 - mDice: 0.6828 - val_loss: 0.3910 - val_acc: 0.9889 - val_mDice: 0.6678

Epoch 00035: val_mDice did not improve from 0.67790
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3584 - acc: 0.9889 - mDice: 0.6829 - val_loss: 0.3781 - val_acc: 0.9883 - val_mDice: 0.6765

Epoch 00036: val_mDice did not improve from 0.67790
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3512 - acc: 0.9889 - mDice: 0.6878 - val_loss: 0.3825 - val_acc: 0.9886 - val_mDice: 0.6755

Epoch 00037: val_mDice did not improve from 0.67790
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3472 - acc: 0.9888 - mDice: 0.6905 - val_loss: 0.3849 - val_acc: 0.9887 - val_mDice: 0.6746

Epoch 00038: val_mDice did not improve from 0.67790
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3781 - acc: 0.9886 - mDice: 0.6784 - val_loss: 0.4098 - val_acc: 0.9887 - val_mDice: 0.6649

Epoch 00039: val_mDice did not improve from 0.67790
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3461 - acc: 0.9888 - mDice: 0.6914 - val_loss: 0.3846 - val_acc: 0.9891 - val_mDice: 0.6755

Epoch 00040: val_mDice did not improve from 0.67790
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3480 - acc: 0.9889 - mDice: 0.6905 - val_loss: 0.4281 - val_acc: 0.9889 - val_mDice: 0.6688

Epoch 00041: val_mDice did not improve from 0.67790
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3447 - acc: 0.9888 - mDice: 0.6925 - val_loss: 0.3814 - val_acc: 0.9890 - val_mDice: 0.6771

Epoch 00042: val_mDice did not improve from 0.67790
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3435 - acc: 0.9889 - mDice: 0.6936 - val_loss: 0.3961 - val_acc: 0.9891 - val_mDice: 0.6755

Epoch 00043: val_mDice did not improve from 0.67790
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.4530 - acc: 0.9879 - mDice: 0.6366 - val_loss: 0.4256 - val_acc: 0.9874 - val_mDice: 0.6497

Epoch 00044: val_mDice did not improve from 0.67790
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3599 - acc: 0.9888 - mDice: 0.6819 - val_loss: 0.3876 - val_acc: 0.9888 - val_mDice: 0.6716

Epoch 00045: val_mDice did not improve from 0.67790
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3454 - acc: 0.9890 - mDice: 0.6919 - val_loss: 0.3820 - val_acc: 0.9889 - val_mDice: 0.6746

Epoch 00046: val_mDice did not improve from 0.67790
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3418 - acc: 0.9890 - mDice: 0.6951 - val_loss: 0.3823 - val_acc: 0.9895 - val_mDice: 0.6766

Epoch 00047: val_mDice did not improve from 0.67790
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3392 - acc: 0.9890 - mDice: 0.6965 - val_loss: 0.3782 - val_acc: 0.9891 - val_mDice: 0.6778

Epoch 00048: val_mDice did not improve from 0.67790
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3367 - acc: 0.9890 - mDice: 0.6984 - val_loss: 0.3755 - val_acc: 0.9895 - val_mDice: 0.6799

Epoch 00049: val_mDice improved from 0.67790 to 0.67993, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3403 - acc: 0.9891 - mDice: 0.6969 - val_loss: 0.3780 - val_acc: 0.9896 - val_mDice: 0.6784

Epoch 00050: val_mDice did not improve from 0.67993
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3364 - acc: 0.9891 - mDice: 0.6988 - val_loss: 0.3843 - val_acc: 0.9896 - val_mDice: 0.6769

Epoch 00051: val_mDice did not improve from 0.67993
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3338 - acc: 0.9892 - mDice: 0.7009 - val_loss: 0.3804 - val_acc: 0.9898 - val_mDice: 0.6783

Epoch 00052: val_mDice did not improve from 0.67993
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3657 - acc: 0.9891 - mDice: 0.6980 - val_loss: 0.3867 - val_acc: 0.9896 - val_mDice: 0.6733

Epoch 00053: val_mDice did not improve from 0.67993
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3366 - acc: 0.9891 - mDice: 0.6996 - val_loss: 0.3853 - val_acc: 0.9894 - val_mDice: 0.6762

Epoch 00054: val_mDice did not improve from 0.67993
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3271 - acc: 0.9891 - mDice: 0.7053 - val_loss: 0.3766 - val_acc: 0.9896 - val_mDice: 0.6796

Epoch 00055: val_mDice did not improve from 0.67993
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3260 - acc: 0.9891 - mDice: 0.7062 - val_loss: 0.3712 - val_acc: 0.9894 - val_mDice: 0.6826

Epoch 00056: val_mDice improved from 0.67993 to 0.68263, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3296 - acc: 0.9892 - mDice: 0.7041 - val_loss: 0.3777 - val_acc: 0.9897 - val_mDice: 0.6808

Epoch 00057: val_mDice did not improve from 0.68263
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3261 - acc: 0.9892 - mDice: 0.7064 - val_loss: 0.3798 - val_acc: 0.9897 - val_mDice: 0.6801

Epoch 00058: val_mDice did not improve from 0.68263
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3294 - acc: 0.9892 - mDice: 0.7041 - val_loss: 0.3793 - val_acc: 0.9895 - val_mDice: 0.6803

Epoch 00059: val_mDice did not improve from 0.68263
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3272 - acc: 0.9892 - mDice: 0.7066 - val_loss: 0.3793 - val_acc: 0.9896 - val_mDice: 0.6788

Epoch 00060: val_mDice did not improve from 0.68263
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3288 - acc: 0.9892 - mDice: 0.7050 - val_loss: 0.3759 - val_acc: 0.9898 - val_mDice: 0.6805

Epoch 00061: val_mDice did not improve from 0.68263
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3665 - acc: 0.9890 - mDice: 0.6915 - val_loss: 0.3804 - val_acc: 0.9896 - val_mDice: 0.6760

Epoch 00062: val_mDice did not improve from 0.68263
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3304 - acc: 0.9892 - mDice: 0.7029 - val_loss: 0.3789 - val_acc: 0.9896 - val_mDice: 0.6784

Epoch 00063: val_mDice did not improve from 0.68263
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3280 - acc: 0.9892 - mDice: 0.7053 - val_loss: 0.3755 - val_acc: 0.9897 - val_mDice: 0.6808

Epoch 00064: val_mDice did not improve from 0.68263
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3268 - acc: 0.9893 - mDice: 0.7060 - val_loss: 0.3758 - val_acc: 0.9897 - val_mDice: 0.6797

Epoch 00065: val_mDice did not improve from 0.68263
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3207 - acc: 0.9893 - mDice: 0.7101 - val_loss: 0.3818 - val_acc: 0.9896 - val_mDice: 0.6786

Epoch 00066: val_mDice did not improve from 0.68263
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3250 - acc: 0.9893 - mDice: 0.7078 - val_loss: 0.3956 - val_acc: 0.9896 - val_mDice: 0.6792

Epoch 00067: val_mDice did not improve from 0.68263
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3206 - acc: 0.9893 - mDice: 0.7103 - val_loss: 0.3893 - val_acc: 0.9897 - val_mDice: 0.6784

Epoch 00068: val_mDice did not improve from 0.68263
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3214 - acc: 0.9893 - mDice: 0.7100 - val_loss: 0.3802 - val_acc: 0.9898 - val_mDice: 0.6807

Epoch 00069: val_mDice did not improve from 0.68263
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3515 - acc: 0.9893 - mDice: 0.7085 - val_loss: 0.3804 - val_acc: 0.9897 - val_mDice: 0.6795

Epoch 00070: val_mDice did not improve from 0.68263
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3222 - acc: 0.9893 - mDice: 0.7094 - val_loss: 0.3886 - val_acc: 0.9895 - val_mDice: 0.6786

Epoch 00071: val_mDice did not improve from 0.68263
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3188 - acc: 0.9893 - mDice: 0.7117 - val_loss: 0.3801 - val_acc: 0.9897 - val_mDice: 0.6797

Epoch 00072: val_mDice did not improve from 0.68263
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3196 - acc: 0.9893 - mDice: 0.7116 - val_loss: 0.3738 - val_acc: 0.9897 - val_mDice: 0.6824

Epoch 00073: val_mDice did not improve from 0.68263
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3178 - acc: 0.9894 - mDice: 0.7123 - val_loss: 0.3748 - val_acc: 0.9897 - val_mDice: 0.6820

Epoch 00074: val_mDice did not improve from 0.68263
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3224 - acc: 0.9894 - mDice: 0.7102 - val_loss: 0.3746 - val_acc: 0.9897 - val_mDice: 0.6821

Epoch 00075: val_mDice did not improve from 0.68263
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3165 - acc: 0.9893 - mDice: 0.7134 - val_loss: 0.3778 - val_acc: 0.9898 - val_mDice: 0.6804

Epoch 00076: val_mDice did not improve from 0.68263
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3164 - acc: 0.9894 - mDice: 0.7136 - val_loss: 0.3743 - val_acc: 0.9898 - val_mDice: 0.6818

Epoch 00077: val_mDice did not improve from 0.68263
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3146 - acc: 0.9894 - mDice: 0.7149 - val_loss: 0.3821 - val_acc: 0.9899 - val_mDice: 0.6812

Epoch 00078: val_mDice did not improve from 0.68263
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3153 - acc: 0.9893 - mDice: 0.7145 - val_loss: 0.3923 - val_acc: 0.9897 - val_mDice: 0.6801

Epoch 00079: val_mDice did not improve from 0.68263
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3135 - acc: 0.9893 - mDice: 0.7158 - val_loss: 0.3758 - val_acc: 0.9898 - val_mDice: 0.6818

Epoch 00080: val_mDice did not improve from 0.68263
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3145 - acc: 0.9893 - mDice: 0.7152 - val_loss: 0.3794 - val_acc: 0.9897 - val_mDice: 0.6813

Epoch 00081: val_mDice did not improve from 0.68263
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3133 - acc: 0.9893 - mDice: 0.7159 - val_loss: 0.3909 - val_acc: 0.9898 - val_mDice: 0.6793

Epoch 00082: val_mDice did not improve from 0.68263
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3139 - acc: 0.9894 - mDice: 0.7153 - val_loss: 0.3844 - val_acc: 0.9897 - val_mDice: 0.6791

Epoch 00083: val_mDice did not improve from 0.68263
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3130 - acc: 0.9894 - mDice: 0.7162 - val_loss: 0.3866 - val_acc: 0.9897 - val_mDice: 0.6808

Epoch 00084: val_mDice did not improve from 0.68263
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3163 - acc: 0.9894 - mDice: 0.7141 - val_loss: 0.3741 - val_acc: 0.9899 - val_mDice: 0.6826

Epoch 00085: val_mDice improved from 0.68263 to 0.68263, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3107 - acc: 0.9894 - mDice: 0.7177 - val_loss: 0.3817 - val_acc: 0.9896 - val_mDice: 0.6808

Epoch 00086: val_mDice did not improve from 0.68263
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3149 - acc: 0.9894 - mDice: 0.7153 - val_loss: 0.3815 - val_acc: 0.9898 - val_mDice: 0.6795

Epoch 00087: val_mDice did not improve from 0.68263
Epoch 88/300

Epoch 00088: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3103 - acc: 0.9894 - mDice: 0.7179 - val_loss: 0.4048 - val_acc: 0.9896 - val_mDice: 0.6786

Epoch 00088: val_mDice did not improve from 0.68263
Epoch 89/300

Epoch 00089: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3117 - acc: 0.9894 - mDice: 0.7172 - val_loss: 0.3999 - val_acc: 0.9897 - val_mDice: 0.6785

Epoch 00089: val_mDice did not improve from 0.68263
Epoch 90/300

Epoch 00090: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3414 - acc: 0.9892 - mDice: 0.7016 - val_loss: 0.4000 - val_acc: 0.9894 - val_mDice: 0.6673

Epoch 00090: val_mDice did not improve from 0.68263
Epoch 91/300

Epoch 00091: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3156 - acc: 0.9894 - mDice: 0.7141 - val_loss: 0.4005 - val_acc: 0.9896 - val_mDice: 0.6768

Epoch 00091: val_mDice did not improve from 0.68263
Epoch 92/300

Epoch 00092: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3148 - acc: 0.9894 - mDice: 0.7151 - val_loss: 0.3859 - val_acc: 0.9897 - val_mDice: 0.6787

Epoch 00092: val_mDice did not improve from 0.68263
Epoch 93/300

Epoch 00093: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3130 - acc: 0.9894 - mDice: 0.7161 - val_loss: 0.3891 - val_acc: 0.9898 - val_mDice: 0.6790

Epoch 00093: val_mDice did not improve from 0.68263
Epoch 94/300

Epoch 00094: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3108 - acc: 0.9894 - mDice: 0.7175 - val_loss: 0.3882 - val_acc: 0.9898 - val_mDice: 0.6798

Epoch 00094: val_mDice did not improve from 0.68263
Epoch 95/300

Epoch 00095: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3117 - acc: 0.9894 - mDice: 0.7169 - val_loss: 0.3976 - val_acc: 0.9898 - val_mDice: 0.6792

Epoch 00095: val_mDice did not improve from 0.68263
Epoch 96/300

Epoch 00096: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3113 - acc: 0.9894 - mDice: 0.7174 - val_loss: 0.3868 - val_acc: 0.9899 - val_mDice: 0.6795

Epoch 00096: val_mDice did not improve from 0.68263
Epoch 97/300

Epoch 00097: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3136 - acc: 0.9894 - mDice: 0.7160 - val_loss: 0.3857 - val_acc: 0.9899 - val_mDice: 0.6801

Epoch 00097: val_mDice did not improve from 0.68263
Epoch 98/300

Epoch 00098: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3101 - acc: 0.9895 - mDice: 0.7182 - val_loss: 0.3968 - val_acc: 0.9899 - val_mDice: 0.6803

Epoch 00098: val_mDice did not improve from 0.68263
Epoch 99/300

Epoch 00099: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3143 - acc: 0.9895 - mDice: 0.7156 - val_loss: 0.3909 - val_acc: 0.9899 - val_mDice: 0.6796

Epoch 00099: val_mDice did not improve from 0.68263
Epoch 100/300

Epoch 00100: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3091 - acc: 0.9895 - mDice: 0.7190 - val_loss: 0.3852 - val_acc: 0.9899 - val_mDice: 0.6806

Epoch 00100: val_mDice did not improve from 0.68263
Epoch 101/300

Epoch 00101: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3126 - acc: 0.9894 - mDice: 0.7174 - val_loss: 0.4009 - val_acc: 0.9899 - val_mDice: 0.6795

Epoch 00101: val_mDice did not improve from 0.68263
Epoch 102/300

Epoch 00102: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3101 - acc: 0.9894 - mDice: 0.7182 - val_loss: 0.4007 - val_acc: 0.9899 - val_mDice: 0.6800

Epoch 00102: val_mDice did not improve from 0.68263
Epoch 103/300

Epoch 00103: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3131 - acc: 0.9895 - mDice: 0.7163 - val_loss: 0.3946 - val_acc: 0.9899 - val_mDice: 0.6804

Epoch 00103: val_mDice did not improve from 0.68263
Epoch 104/300

Epoch 00104: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3104 - acc: 0.9895 - mDice: 0.7182 - val_loss: 0.4011 - val_acc: 0.9898 - val_mDice: 0.6801

Epoch 00104: val_mDice did not improve from 0.68263
Epoch 105/300

Epoch 00105: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3099 - acc: 0.9894 - mDice: 0.7183 - val_loss: 0.3965 - val_acc: 0.9899 - val_mDice: 0.6800

Epoch 00105: val_mDice did not improve from 0.68263
Epoch 106/300

Epoch 00106: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3069 - acc: 0.9895 - mDice: 0.7207 - val_loss: 0.3823 - val_acc: 0.9900 - val_mDice: 0.6810

Epoch 00106: val_mDice did not improve from 0.68263
Epoch 107/300

Epoch 00107: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3103 - acc: 0.9895 - mDice: 0.7182 - val_loss: 0.3899 - val_acc: 0.9899 - val_mDice: 0.6803

Epoch 00107: val_mDice did not improve from 0.68263
Epoch 108/300

Epoch 00108: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3361 - acc: 0.9895 - mDice: 0.7115 - val_loss: 0.4407 - val_acc: 0.9894 - val_mDice: 0.6722

Epoch 00108: val_mDice did not improve from 0.68263
Epoch 109/300

Epoch 00109: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3137 - acc: 0.9893 - mDice: 0.7155 - val_loss: 0.4316 - val_acc: 0.9897 - val_mDice: 0.6746

Epoch 00109: val_mDice did not improve from 0.68263
Epoch 110/300

Epoch 00110: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3309 - acc: 0.9895 - mDice: 0.7184 - val_loss: 0.4218 - val_acc: 0.9899 - val_mDice: 0.6772

Epoch 00110: val_mDice did not improve from 0.68263
Epoch 111/300

Epoch 00111: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3102 - acc: 0.9895 - mDice: 0.7182 - val_loss: 0.4016 - val_acc: 0.9899 - val_mDice: 0.6779

Epoch 00111: val_mDice did not improve from 0.68263
Epoch 112/300

Epoch 00112: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3097 - acc: 0.9895 - mDice: 0.7187 - val_loss: 0.4053 - val_acc: 0.9899 - val_mDice: 0.6777

Epoch 00112: val_mDice did not improve from 0.68263
Epoch 113/300

Epoch 00113: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3084 - acc: 0.9895 - mDice: 0.7194 - val_loss: 0.4073 - val_acc: 0.9899 - val_mDice: 0.6786

Epoch 00113: val_mDice did not improve from 0.68263
Epoch 114/300

Epoch 00114: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3405 - acc: 0.9895 - mDice: 0.7170 - val_loss: 0.4035 - val_acc: 0.9899 - val_mDice: 0.6797

Epoch 00114: val_mDice did not improve from 0.68263
Epoch 115/300

Epoch 00115: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3073 - acc: 0.9895 - mDice: 0.7204 - val_loss: 0.4016 - val_acc: 0.9899 - val_mDice: 0.6796

Epoch 00115: val_mDice did not improve from 0.68263
Epoch 116/300

Epoch 00116: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3055 - acc: 0.9895 - mDice: 0.7218 - val_loss: 0.4064 - val_acc: 0.9899 - val_mDice: 0.6800

Epoch 00116: val_mDice did not improve from 0.68263
Epoch 117/300

Epoch 00117: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3061 - acc: 0.9895 - mDice: 0.7212 - val_loss: 0.4006 - val_acc: 0.9899 - val_mDice: 0.6795

Epoch 00117: val_mDice did not improve from 0.68263
Epoch 118/300

Epoch 00118: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3081 - acc: 0.9895 - mDice: 0.7198 - val_loss: 0.4034 - val_acc: 0.9899 - val_mDice: 0.6800

Epoch 00118: val_mDice did not improve from 0.68263
Epoch 119/300

Epoch 00119: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3075 - acc: 0.9895 - mDice: 0.7200 - val_loss: 0.3959 - val_acc: 0.9899 - val_mDice: 0.6797

Epoch 00119: val_mDice did not improve from 0.68263
Epoch 120/300

Epoch 00120: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3115 - acc: 0.9895 - mDice: 0.7182 - val_loss: 0.3931 - val_acc: 0.9899 - val_mDice: 0.6805

Epoch 00120: val_mDice did not improve from 0.68263
Epoch 121/300

Epoch 00121: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3085 - acc: 0.9895 - mDice: 0.7197 - val_loss: 0.3952 - val_acc: 0.9899 - val_mDice: 0.6805

Epoch 00121: val_mDice did not improve from 0.68263
Epoch 122/300

Epoch 00122: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3070 - acc: 0.9895 - mDice: 0.7207 - val_loss: 0.3874 - val_acc: 0.9900 - val_mDice: 0.6802

Epoch 00122: val_mDice did not improve from 0.68263
Epoch 123/300

Epoch 00123: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3056 - acc: 0.9895 - mDice: 0.7216 - val_loss: 0.3865 - val_acc: 0.9899 - val_mDice: 0.6804

Epoch 00123: val_mDice did not improve from 0.68263
Epoch 124/300

Epoch 00124: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 10s - loss: 0.3086 - acc: 0.9895 - mDice: 0.7200 - val_loss: 0.3891 - val_acc: 0.9899 - val_mDice: 0.6806

Epoch 00124: val_mDice did not improve from 0.68263
Epoch 125/300

Epoch 00125: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 12s - loss: 0.3088 - acc: 0.9895 - mDice: 0.7196 - val_loss: 0.3903 - val_acc: 0.9899 - val_mDice: 0.6801

Epoch 00125: val_mDice did not improve from 0.68263
Restoring model weights from the end of the best epoch
Epoch 00125: early stopping
{'val_loss': [4.767296862536067, 1.9087254251185444, 1.2925226871444604, 1.3240638669170128, 1.5695113824761433, 1.527362389436593, 0.8988870243911496, 0.5932562716794609, 0.5540917414299985, 0.5004620976077528, 0.4571782743401488, 0.8003040945276301, 3.8297006386943044, 0.5615664766851561, 0.45265041089079977, 0.40904086188626004, 0.4066686652304396, 0.4062545417526273, 0.7507420489349154, 0.4269486479633501, 0.39570638234145544, 0.3933025412433794, 0.3975047300415498, 0.4173382811806579, 0.45086985710701605, 0.5506892386241493, 0.413997710495498, 0.3812840832647188, 0.41781424723434624, 0.401870841758324, 0.3883099458098522, 0.40610973805768086, 0.37645400469221524, 0.41908991590459294, 0.39095514896950384, 0.3780522163161296, 0.3824823730858689, 0.38487030148065055, 0.40983045765583875, 0.38455325861312417, 0.42805538378194125, 0.38144261538155316, 0.3961425209685016, 0.4255622574899287, 0.3876225078800671, 0.3820372956643824, 0.38232525747534746, 0.37821654466762245, 0.37548064540427223, 0.3780487314845322, 0.38432700922376684, 0.3804345883284312, 0.38673712182221426, 0.3852643096038967, 0.37660292265152734, 0.3711697099718311, 0.3776593901678768, 0.3798258609511475, 0.3793241727087478, 0.37932924417628944, 0.3758792825918524, 0.38044537315756827, 0.3788603985342684, 0.37552396080540246, 0.3758074263253772, 0.3817914310260353, 0.39564548312900905, 0.3892883017149157, 0.3802225575691897, 0.38044210242850157, 0.3886201024662004, 0.3801240971472173, 0.3737687660134798, 0.3748202310680351, 0.3746313875632414, 0.3778044669359068, 0.37430878409514484, 0.38210008383679456, 0.39226787383637973, 0.3757790059099365, 0.3793697122012323, 0.3908529145975669, 0.3844226390867736, 0.38663272290842937, 0.3740641922447882, 0.3817325834043593, 0.38146910253329813, 0.4047773358283718, 0.39991025542573283, 0.399986838296428, 0.40049734069284304, 0.38589195539066906, 0.3891228566171944, 0.38822555434472245, 0.39762632122864666, 0.38675295176272256, 0.3857022508441244, 0.3967561481398195, 0.3908581504327737, 0.3851994651539474, 0.4009042088628146, 0.40071022218294877, 0.39462190874741104, 0.4010759681380534, 0.3965257884836329, 0.3822811802265051, 0.38986054084898697, 0.44068945559718675, 0.4315699064323574, 0.4218024722301331, 0.401559175907959, 0.40528245053833883, 0.4073235241766885, 0.4034546672415667, 0.40158528296292434, 0.40643019382872037, 0.4005796756323127, 0.40343311103706997, 0.3959405705897925, 0.3931112629081451, 0.3952080089664371, 0.3873997669491252, 0.38652929013024645, 0.38908581704480244, 0.39030613486244103], 'val_acc': [0.41636668560573287, 0.9843938707423144, 0.9844646875666425, 0.9844859326801071, 0.9844920829310668, 0.9844854125592799, 0.9845616064283387, 0.9848762541650954, 0.9851834570005137, 0.9849850507823545, 0.9861583627560534, 0.9853920154170125, 0.9858247386317469, 0.9868296332429891, 0.9870911476018801, 0.987595032586972, 0.988044041654338, 0.9879364771053374, 0.9860839933074259, 0.9868734920609339, 0.9880476866355106, 0.9877474306911147, 0.9882076158223606, 0.9885767182192242, 0.9875980778274218, 0.9878839439093019, 0.9889331552631649, 0.988597784077647, 0.9874843460448245, 0.9883939918271929, 0.9884595917287963, 0.9887169571963865, 0.9887449611236826, 0.9887954050142604, 0.9888930155875835, 0.988316930906294, 0.988632705838012, 0.9887357730208228, 0.9886616524670766, 0.9891048642245848, 0.9888781059350712, 0.9889547410769997, 0.989070454810528, 0.987368888594727, 0.9888007892269872, 0.9888599846894602, 0.9894994373789107, 0.9890910801128807, 0.9895250887147372, 0.9895794434640057, 0.9896134309587823, 0.9897600957687865, 0.9895663610439848, 0.9894380719434542, 0.9895719880863652, 0.9893506877848002, 0.9896669978349988, 0.9897085948744712, 0.9895379245556911, 0.9896261681570817, 0.9897760427406163, 0.9896213207721269, 0.989626344544925, 0.9897404175241384, 0.9897317504243207, 0.9895966032056429, 0.9896487963078311, 0.9897432783816723, 0.9897749129541487, 0.9896627469375991, 0.9895294299288881, 0.9896767871673189, 0.9897320138757478, 0.9897187540202533, 0.9897268062714181, 0.9898201662781722, 0.9897665024685925, 0.989850161695348, 0.9896870184341698, 0.9897854830466631, 0.9897446697999988, 0.9897533297318204, 0.9897257689521006, 0.9897091171456971, 0.9898991332243814, 0.9896250447115338, 0.9897776015040832, 0.989595397273001, 0.9897132868241867, 0.9894213446525376, 0.989648526350069, 0.9897093917350875, 0.9897673665980957, 0.9898425270626657, 0.9898324722939349, 0.9898683550068893, 0.9899025155812033, 0.9899282526572913, 0.9899250421118229, 0.9898684451030869, 0.9898726038090285, 0.9898726052977662, 0.9899194189842712, 0.9898381018726833, 0.9898740778798643, 0.9900238618158169, 0.989903722506337, 0.9893612663686331, 0.9897181355324638, 0.9898855997719884, 0.9899225378345274, 0.9899177692975301, 0.9899253084855869, 0.9899370047805709, 0.9898833502893862, 0.9899030299676336, 0.9899173394383158, 0.9899194081771385, 0.9898784141866946, 0.9899246177664518, 0.9898949688943198, 0.9899676093671412, 0.9899063244337952, 0.9898991374700407, 0.9898842189953793], 'val_mDice': [0.01223079592099794, 0.20957524813952874, 0.28988744394300603, 0.32012443813761554, 0.29752867278073475, 0.31346616556973067, 0.4361391439689297, 0.5457277401633884, 0.5671701674434898, 0.5983346565579177, 0.6231113782533333, 0.5260323590972047, 0.19153552518246023, 0.584783828170293, 0.6291898493960872, 0.6531231468414179, 0.6550637164345282, 0.6573636270473667, 0.5428614148820141, 0.6466428526124945, 0.6649346766264542, 0.6654592847846152, 0.6632160925402011, 0.653392271020697, 0.6335207485799763, 0.6176154273842133, 0.6645111636250908, 0.6746043038522612, 0.6617084914727083, 0.6679463415207629, 0.6712386289423643, 0.6732540576685148, 0.6778972833714587, 0.6689702853574232, 0.6677740122519854, 0.6765096218469074, 0.6755205551636208, 0.6746329616662201, 0.6648887010441127, 0.6755121484826164, 0.6687539364870337, 0.6771112863494774, 0.6754581019020434, 0.6497220630231111, 0.6715656602746574, 0.6745577852666433, 0.6765774947752233, 0.6778199404726196, 0.6799320462233043, 0.6783965913153268, 0.676933825953375, 0.6782528238093599, 0.6733000427346666, 0.6761825311856619, 0.6795991073594283, 0.6826261536155334, 0.6807942819970279, 0.680060760441144, 0.6803033405933856, 0.6788147489965017, 0.6804860779028266, 0.6760194234336337, 0.6784118306956614, 0.6808113858834336, 0.6797342197857555, 0.6785738301652102, 0.6791726160005328, 0.6784045715005612, 0.6807210583250132, 0.6794732527860771, 0.6785713039979573, 0.6797105553633189, 0.6824146354782923, 0.6819672551450632, 0.682062071082109, 0.6803674300879268, 0.6818318803700775, 0.6812218240738798, 0.6801052069796334, 0.681830277495865, 0.6812771748989186, 0.679297266019703, 0.6791141034275376, 0.6808175026102269, 0.6826316458995865, 0.6808101710735043, 0.6795000754276103, 0.6785815768943243, 0.6785187146709982, 0.667274578371939, 0.6768168316849066, 0.6787160627275126, 0.6790189418276629, 0.6797857569060206, 0.6792067248549978, 0.6795217872879, 0.6800643559631432, 0.6803417070721389, 0.679624107730929, 0.6805548703747696, 0.679539822462419, 0.6800046996536573, 0.6803768474012476, 0.6800736065929847, 0.6799629630036755, 0.6810277961780361, 0.6803093454994392, 0.672189358039436, 0.6746255151107287, 0.6771516347240233, 0.6778755589396065, 0.6777314751595948, 0.6786298781605807, 0.6796899450034813, 0.6796406503080991, 0.6800230510470825, 0.6794659640367774, 0.6800325644666899, 0.6796846596539627, 0.6805223538630765, 0.6804534001663589, 0.6801962875084785, 0.6804180569278212, 0.6805730262473156, 0.6801360524660123], 'loss': [5.2342592353469755, 2.769453180348215, 1.6765472150287746, 1.1492527043161216, 0.9313227852429349, 0.794230726598962, 0.7120333028717275, 0.6839023089847682, 0.6129623285831849, 0.5612247761773186, 0.5416635307797625, 0.5302243883624399, 0.5869490858967319, 0.5778672925533692, 0.48423269080238107, 0.45906250699897483, 0.4434298542013929, 0.4312712833559586, 0.45040623834528076, 0.47397912535930703, 0.42421457610247326, 0.40499551036606535, 0.4045355804493091, 0.40067059368443636, 0.4042622995522856, 0.4593333792101386, 0.4132132625287296, 0.38819483672182986, 0.3789302039365827, 0.37514182011042635, 0.3765878911398671, 0.36893037363795417, 0.36481947218713584, 0.37213399421217985, 0.35817751562668504, 0.3584143665670617, 0.3511738371263984, 0.34717086469468894, 0.37814547130666626, 0.3461198962173579, 0.3480414667743847, 0.34465823912181737, 0.3435390004716768, 0.45300580186346556, 0.3598549499467838, 0.3454278040151655, 0.34177118832348313, 0.33923365989345716, 0.3366960367542103, 0.34026087116610054, 0.33641509940287817, 0.333767663116104, 0.36570486481204356, 0.3365688951103234, 0.3271124999215998, 0.3259680527119549, 0.3295647812767263, 0.3260779283895083, 0.32941334232962205, 0.32723577293150263, 0.32877970838839293, 0.36654966539400485, 0.33039588394340563, 0.32795691526740606, 0.3268216586917456, 0.32070982895014477, 0.3249827734897473, 0.32064516211579913, 0.32137756570716575, 0.35147296614442136, 0.32219062261054854, 0.31875303021969237, 0.31963982647913364, 0.3178047336683683, 0.3223840789926564, 0.31654406489770104, 0.3164199839340397, 0.31457196057208475, 0.3153401995363411, 0.3135259083809297, 0.314479630966128, 0.31329653709212696, 0.3139044675358965, 0.31302383450642685, 0.3163054307545621, 0.3106592471248533, 0.3148656026717344, 0.31025849639272396, 0.31171682093041075, 0.34140588370568914, 0.31562090742807447, 0.3147588010214589, 0.31298955068266465, 0.3108496910955277, 0.3117487657289564, 0.3113417800950126, 0.31359437656548855, 0.31008246431321457, 0.3143128237109974, 0.30912739842947273, 0.3126223683357239, 0.3100659143339637, 0.31311954128230274, 0.31036013017402836, 0.30991031549459586, 0.30685578844298617, 0.31032970108868885, 0.33611458247424636, 0.3137272932778107, 0.3308917869088109, 0.3101708450200368, 0.30974154786829566, 0.30836577740914983, 0.34048301749434207, 0.30734853748163565, 0.30546398762544974, 0.3060582664115297, 0.30812702094850364, 0.3074801037647973, 0.3114694819494259, 0.30846161399882266, 0.3070037370444807, 0.30564234520028705, 0.3086213621259467, 0.30882369118965475], 'acc': [0.2704872326251188, 0.7706495527109486, 0.98193798621008, 0.9849959995849001, 0.9852200560043195, 0.9854267586959652, 0.9856624742227098, 0.9858521376650757, 0.9861474790456105, 0.9865656330541599, 0.9866937183163649, 0.9869258912063084, 0.986695946359927, 0.986971930126471, 0.9874526748627973, 0.9876453763136834, 0.9877713856521559, 0.9878612482474626, 0.9878460205405768, 0.9876450479396282, 0.9880694608015517, 0.9882275513344747, 0.9882589834599407, 0.9882461801628394, 0.9883430336882, 0.9878424109856775, 0.9881655098470442, 0.988535995863698, 0.9885898532311609, 0.9886397031918626, 0.9885848614335792, 0.9886797374011549, 0.9887272412060228, 0.9887010264981744, 0.9887756522447785, 0.9888674009797032, 0.9888601533474366, 0.9888140961436406, 0.9886203430181632, 0.9888366808189205, 0.9888811948840603, 0.9887787564400515, 0.9888551184004801, 0.9878835502577705, 0.9887639887493812, 0.9889849296376749, 0.9890000333815264, 0.9890336321175464, 0.9890315086563672, 0.9891143357095543, 0.9890836181084802, 0.9891622622320257, 0.9891251438965827, 0.9891097856445547, 0.9891247559178826, 0.9891395422578589, 0.9892200220581944, 0.9891865202254313, 0.9891941529110165, 0.9891794773698585, 0.9891621017017247, 0.9889714063310916, 0.9891906775579862, 0.9892243179806902, 0.989256931960217, 0.9892925459183067, 0.9892734373274025, 0.9893053060660333, 0.9893260038703497, 0.9892546563792083, 0.9892935533465052, 0.989303999517593, 0.9893136090296177, 0.9893553666541913, 0.9893598988012302, 0.9893383756737036, 0.9893847784381703, 0.9894063012000242, 0.9893460569937537, 0.9893291095283134, 0.989349575130486, 0.9893197965768218, 0.9893523820338805, 0.9893532033346913, 0.9893643146643609, 0.9893644283885604, 0.9893533843426616, 0.989379881715482, 0.9893784555920794, 0.9891890960237, 0.9893733690852767, 0.9894084882882476, 0.9894227860895403, 0.9893928726026617, 0.9893996832560907, 0.9894162774817344, 0.9894321019664133, 0.9894705692683261, 0.9894601556421058, 0.9894767791215627, 0.9894452530182213, 0.9894254094252557, 0.9894502663904904, 0.9894817694564538, 0.9894362848960548, 0.9894872220015964, 0.9895002348291362, 0.9894598777308786, 0.9893173129280652, 0.98947823193907, 0.9895297157252493, 0.9895019125353339, 0.989524746599373, 0.9895287240209755, 0.9895035661071356, 0.9894899071359927, 0.9895019812817953, 0.9895068129147488, 0.9894676314541168, 0.9894801637877716, 0.98953118682639, 0.9895178887741697, 0.9895370514846287, 0.9895037324881992, 0.989487294039112], 'mDice': [0.009530801640286402, 0.09935213561438344, 0.20310482492476153, 0.30382150722427603, 0.37666368740467937, 0.4324645191613882, 0.47159147335707774, 0.48711573510813566, 0.5236120648179318, 0.5527158577017989, 0.5636567082141806, 0.5716677392187294, 0.5541865960951963, 0.5431771168679548, 0.5981717614308457, 0.6141219402383442, 0.6245431219873253, 0.6323056103993048, 0.6279173343459521, 0.6054711502753883, 0.6379669228214427, 0.6497127505167861, 0.6507851082854476, 0.6537854576403378, 0.6566133850191268, 0.6212668543212985, 0.6443840228706781, 0.6617165811222755, 0.6685987674385492, 0.6710560643599809, 0.6723821327730191, 0.6755494479021412, 0.6782077229096114, 0.6739018903188179, 0.6828204707865335, 0.68294681105877, 0.6877650373552474, 0.6905054746229956, 0.6783952833684675, 0.6914234223541307, 0.6904782418824412, 0.6924630747250984, 0.6936472990761505, 0.6366406269600055, 0.6818679121374353, 0.6918828022991953, 0.695108184053854, 0.6965168202581581, 0.6983858844253914, 0.6969469558973254, 0.6988026006090129, 0.7009304699722243, 0.6979998591487393, 0.6995916926056329, 0.7052727886504191, 0.7061560172244815, 0.7040695317683776, 0.7063598142811126, 0.7040971747936646, 0.7065690821665196, 0.7049885198382512, 0.6914897195400636, 0.702920502680211, 0.7052516279045058, 0.7059890514502496, 0.7100635865714653, 0.7077591536235224, 0.7102644959110423, 0.709970477534218, 0.7084755963343052, 0.7094164750327362, 0.7117210041525905, 0.7115662448976668, 0.7122709710905157, 0.7101865421774929, 0.7133565117245072, 0.7136003020350918, 0.7148834039097184, 0.7144736749994243, 0.7158425300399218, 0.71518401061099, 0.7158710038735091, 0.7153053272721226, 0.7162087128206265, 0.7140733069437413, 0.7177209115467189, 0.7152663147522628, 0.7178926584910761, 0.7171786902872331, 0.7015794910536222, 0.714065467653099, 0.7150974653981215, 0.7161344055749156, 0.7174777066780745, 0.7168671097492147, 0.7173869938938164, 0.7160429830200101, 0.7181859703883071, 0.7156007480036262, 0.7190076591778387, 0.7174160078259334, 0.7182402822868955, 0.7163426718828868, 0.7182205659480183, 0.7182946336781321, 0.7207151958547486, 0.7182296726601255, 0.711486809824142, 0.7155327146038687, 0.7184064980664867, 0.7182164064214274, 0.7186886949773215, 0.7194351685559092, 0.7170009978709777, 0.7204316020743248, 0.7217899601152338, 0.7212348269538645, 0.7198049861229271, 0.7200465791064538, 0.71817215454359, 0.7196716925849213, 0.7206703497588269, 0.7216236605966018, 0.7199751263015841, 0.7195934218131691], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:04<00:49,  4.96s/it]predicting test subjects:  18%|█▊        | 2/11 [00:09<00:43,  4.78s/it]predicting test subjects:  27%|██▋       | 3/11 [00:11<00:32,  4.01s/it]predicting test subjects:  36%|███▋      | 4/11 [00:15<00:28,  4.13s/it]predicting test subjects:  45%|████▌     | 5/11 [00:20<00:25,  4.31s/it]predicting test subjects:  55%|█████▍    | 6/11 [00:24<00:21,  4.23s/it]predicting test subjects:  64%|██████▎   | 7/11 [00:28<00:16,  4.08s/it]predicting test subjects:  73%|███████▎  | 8/11 [00:33<00:12,  4.33s/it]predicting test subjects:  82%|████████▏ | 9/11 [00:38<00:08,  4.42s/it]predicting test subjects:  91%|█████████ | 10/11 [00:41<00:04,  4.24s/it]predicting test subjects: 100%|██████████| 11/11 [00:46<00:00,  4.49s/it]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:04<03:10,  4.76s/it]predicting train subjects:   5%|▍         | 2/41 [00:08<02:55,  4.51s/it]predicting train subjects:   7%|▋         | 3/41 [00:12<02:39,  4.18s/it]predicting train subjects:  10%|▉         | 4/41 [00:15<02:21,  3.84s/it]predicting train subjects:  12%|█▏        | 5/41 [00:19<02:28,  4.13s/it]predicting train subjects:  15%|█▍        | 6/41 [00:23<02:20,  4.01s/it]predicting train subjects:  17%|█▋        | 7/41 [00:26<02:08,  3.79s/it]predicting train subjects:  20%|█▉        | 8/41 [00:28<01:47,  3.25s/it]predicting train subjects:  22%|██▏       | 9/41 [00:33<01:51,  3.49s/it]predicting train subjects:  24%|██▍       | 10/41 [00:37<01:55,  3.73s/it]predicting train subjects:  27%|██▋       | 11/41 [00:41<01:57,  3.93s/it]predicting train subjects:  29%|██▉       | 12/41 [00:43<01:37,  3.38s/it]predicting train subjects:  32%|███▏      | 13/41 [00:47<01:36,  3.43s/it]predicting train subjects:  34%|███▍      | 14/41 [00:51<01:34,  3.51s/it]predicting train subjects:  37%|███▋      | 15/41 [00:53<01:19,  3.07s/it]predicting train subjects:  39%|███▉      | 16/41 [00:56<01:21,  3.28s/it]predicting train subjects:  41%|████▏     | 17/41 [01:01<01:30,  3.79s/it]predicting train subjects:  44%|████▍     | 18/41 [01:05<01:28,  3.86s/it]predicting train subjects:  46%|████▋     | 19/41 [01:09<01:24,  3.82s/it]predicting train subjects:  49%|████▉     | 20/41 [01:12<01:16,  3.63s/it]predicting train subjects:  51%|█████     | 21/41 [01:16<01:13,  3.69s/it]predicting train subjects:  54%|█████▎    | 22/41 [01:20<01:08,  3.62s/it]predicting train subjects:  56%|█████▌    | 23/41 [01:23<01:05,  3.64s/it]predicting train subjects:  59%|█████▊    | 24/41 [01:27<01:02,  3.69s/it]predicting train subjects:  61%|██████    | 25/41 [01:31<00:58,  3.66s/it]predicting train subjects:  63%|██████▎   | 26/41 [01:36<01:02,  4.15s/it]predicting train subjects:  66%|██████▌   | 27/41 [01:42<01:06,  4.76s/it]predicting train subjects:  68%|██████▊   | 28/41 [01:48<01:06,  5.08s/it]predicting train subjects:  71%|███████   | 29/41 [01:53<00:59,  4.97s/it]predicting train subjects:  73%|███████▎  | 30/41 [01:57<00:51,  4.67s/it]predicting train subjects:  76%|███████▌  | 31/41 [02:01<00:44,  4.50s/it]predicting train subjects:  78%|███████▊  | 32/41 [02:05<00:40,  4.51s/it]predicting train subjects:  80%|████████  | 33/41 [02:10<00:36,  4.55s/it]predicting train subjects:  83%|████████▎ | 34/41 [02:13<00:29,  4.16s/it]predicting train subjects:  85%|████████▌ | 35/41 [02:17<00:23,  3.97s/it]predicting train subjects:  88%|████████▊ | 36/41 [02:21<00:19,  3.99s/it]predicting train subjects:  90%|█████████ | 37/41 [02:24<00:15,  3.89s/it]predicting train subjects:  93%|█████████▎| 38/41 [02:28<00:11,  3.82s/it]predicting train subjects:  95%|█████████▌| 39/41 [02:32<00:07,  3.72s/it]predicting train subjects:  98%|█████████▊| 40/41 [02:35<00:03,  3.77s/it]predicting train subjects: 100%|██████████| 41/41 [02:40<00:00,  4.09s/it]
Loading train:   0%|          | 0/41 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/41 [00:03<02:15,  3.40s/it]Loading train:   5%|▍         | 2/41 [00:07<02:15,  3.48s/it]Loading train:   7%|▋         | 3/41 [00:10<02:16,  3.59s/it]Loading train:  10%|▉         | 4/41 [00:14<02:09,  3.50s/it]Loading train:  12%|█▏        | 5/41 [00:18<02:15,  3.75s/it]Loading train:  15%|█▍        | 6/41 [00:22<02:13,  3.83s/it]Loading train:  17%|█▋        | 7/41 [00:25<02:04,  3.66s/it]Loading train:  20%|█▉        | 8/41 [00:28<01:54,  3.47s/it]Loading train:  22%|██▏       | 9/41 [00:32<01:50,  3.46s/it]Loading train:  24%|██▍       | 10/41 [00:36<01:51,  3.61s/it]Loading train:  27%|██▋       | 11/41 [00:39<01:49,  3.65s/it]Loading train:  29%|██▉       | 12/41 [00:42<01:38,  3.38s/it]Loading train:  32%|███▏      | 13/41 [00:46<01:40,  3.58s/it]Loading train:  34%|███▍      | 14/41 [00:50<01:35,  3.55s/it]Loading train:  37%|███▋      | 15/41 [00:53<01:27,  3.36s/it]Loading train:  39%|███▉      | 16/41 [00:56<01:25,  3.42s/it]Loading train:  41%|████▏     | 17/41 [01:01<01:31,  3.79s/it]Loading train:  44%|████▍     | 18/41 [01:04<01:24,  3.68s/it]Loading train:  46%|████▋     | 19/41 [01:08<01:18,  3.57s/it]Loading train:  49%|████▉     | 20/41 [01:11<01:12,  3.46s/it]Loading train:  51%|█████     | 21/41 [01:14<01:08,  3.44s/it]Loading train:  54%|█████▎    | 22/41 [01:18<01:05,  3.43s/it]Loading train:  56%|█████▌    | 23/41 [01:21<01:02,  3.47s/it]Loading train:  59%|█████▊    | 24/41 [01:25<01:00,  3.55s/it]Loading train:  61%|██████    | 25/41 [01:29<00:59,  3.73s/it]Loading train:  63%|██████▎   | 26/41 [01:33<00:55,  3.67s/it]Loading train:  66%|██████▌   | 27/41 [01:38<00:59,  4.24s/it]Loading train:  68%|██████▊   | 28/41 [01:42<00:53,  4.12s/it]Loading train:  71%|███████   | 29/41 [01:46<00:47,  4.00s/it]Loading train:  73%|███████▎  | 30/41 [01:49<00:42,  3.88s/it]Loading train:  76%|███████▌  | 31/41 [01:52<00:35,  3.55s/it]Loading train:  78%|███████▊  | 32/41 [01:56<00:32,  3.59s/it]Loading train:  80%|████████  | 33/41 [02:00<00:29,  3.68s/it]Loading train:  83%|████████▎ | 34/41 [02:03<00:23,  3.41s/it]Loading train:  85%|████████▌ | 35/41 [02:06<00:21,  3.53s/it]Loading train:  88%|████████▊ | 36/41 [02:09<00:17,  3.42s/it]Loading train:  90%|█████████ | 37/41 [02:13<00:14,  3.55s/it]Loading train:  93%|█████████▎| 38/41 [02:17<00:10,  3.56s/it]Loading train:  95%|█████████▌| 39/41 [02:21<00:07,  3.65s/it]Loading train:  98%|█████████▊| 40/41 [02:24<00:03,  3.64s/it]Loading train: 100%|██████████| 41/41 [02:28<00:00,  3.53s/it]
concatenating: train:   0%|          | 0/41 [00:00<?, ?it/s]concatenating: train:   5%|▍         | 2/41 [00:00<00:03, 11.69it/s]concatenating: train:  10%|▉         | 4/41 [00:00<00:03, 12.09it/s]concatenating: train:  20%|█▉        | 8/41 [00:00<00:02, 14.99it/s]concatenating: train:  32%|███▏      | 13/41 [00:00<00:01, 18.81it/s]concatenating: train:  41%|████▏     | 17/41 [00:00<00:01, 20.31it/s]concatenating: train:  49%|████▉     | 20/41 [00:00<00:00, 21.79it/s]concatenating: train:  59%|█████▊    | 24/41 [00:00<00:00, 24.26it/s]concatenating: train:  71%|███████   | 29/41 [00:01<00:00, 28.09it/s]concatenating: train:  83%|████████▎ | 34/41 [00:01<00:00, 31.20it/s]concatenating: train:  98%|█████████▊| 40/41 [00:01<00:00, 36.34it/s]concatenating: train: 100%|██████████| 41/41 [00:01<00:00, 30.80it/s]
Loading test:   0%|          | 0/11 [00:00<?, ?it/s]Loading test:   9%|▉         | 1/11 [00:03<00:36,  3.70s/it]Loading test:  18%|█▊        | 2/11 [00:07<00:33,  3.70s/it]Loading test:  27%|██▋       | 3/11 [00:10<00:28,  3.52s/it]Loading test:  36%|███▋      | 4/11 [00:13<00:24,  3.43s/it]Loading test:  45%|████▌     | 5/11 [00:17<00:21,  3.60s/it]Loading test:  55%|█████▍    | 6/11 [00:21<00:18,  3.74s/it]Loading test:  64%|██████▎   | 7/11 [00:24<00:14,  3.51s/it]Loading test:  73%|███████▎  | 8/11 [00:28<00:10,  3.47s/it]Loading test:  82%|████████▏ | 9/11 [00:30<00:06,  3.23s/it]Loading test:  91%|█████████ | 10/11 [00:33<00:02,  2.96s/it]Loading test: 100%|██████████| 11/11 [00:35<00:00,  2.82s/it]
concatenating: validation:   0%|          | 0/11 [00:00<?, ?it/s]concatenating: validation:  27%|██▋       | 3/11 [00:00<00:00, 22.62it/s]concatenating: validation:  91%|█████████ | 10/11 [00:00<00:00, 28.13it/s]concatenating: validation: 100%|██████████| 11/11 [00:00<00:00, 44.36it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 128, 92, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 128, 92, 20)  200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 128, 92, 20)  80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 128, 92, 20)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 128, 92, 20)  3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 128, 92, 20)  80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 128, 92, 20)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 64, 46, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 64, 46, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 64, 46, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 64, 46, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 64, 46, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 64, 46, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 64, 46, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 64, 46, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 64, 46, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 32, 23, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 32, 23, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 23, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 23, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 23, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 23, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 23, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 23, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 32, 23, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 32, 23, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 64, 46, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 64, 46, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 64, 46, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 64, 46, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 64, 46, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 64, 46, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 64, 46, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 64, 46, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 64, 46, 140)  0           concatenate_3[0][0]              2019-07-29 00:07:53.680851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-29 00:07:53.680951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-29 00:07:53.680978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-29 00:07:53.680988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-29 00:07:53.681354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 64, 46, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 128, 92, 20)  11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 128, 92, 40)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 128, 92, 20)  7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 128, 92, 20)  80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 128, 92, 20)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 128, 92, 20)  3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 128, 92, 20)  80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 128, 92, 20)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 128, 92, 60)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 128, 92, 60)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 128, 92, 13)  793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.54301060e-02 3.09684645e-02 7.58754074e-02 1.02387033e-02
 2.77641771e-02 7.04462988e-03 7.68875615e-02 1.11755245e-01
 7.79229036e-02 1.35777791e-02 3.25522843e-01 1.76980983e-01
 3.11971894e-05]
Train on 4009 samples, validate on 1068 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 19s - loss: 5.7154 - acc: 0.3805 - mDice: 0.0040 - val_loss: 5.2915 - val_acc: 0.7065 - val_mDice: 0.0060

Epoch 00001: val_mDice improved from -inf to 0.00599, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 4.0047 - acc: 0.6868 - mDice: 0.0312 - val_loss: 2.9337 - val_acc: 0.9794 - val_mDice: 0.0909

Epoch 00002: val_mDice improved from 0.00599 to 0.09092, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 2.4618 - acc: 0.9680 - mDice: 0.1030 - val_loss: 3.0607 - val_acc: 0.9856 - val_mDice: 0.1311

Epoch 00003: val_mDice improved from 0.09092 to 0.13107, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 1.8391 - acc: 0.9836 - mDice: 0.1629 - val_loss: 2.5969 - val_acc: 0.9856 - val_mDice: 0.1764

Epoch 00004: val_mDice improved from 0.13107 to 0.17644, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.6203 - acc: 0.9846 - mDice: 0.1974 - val_loss: 1.7591 - val_acc: 0.9854 - val_mDice: 0.2426

Epoch 00005: val_mDice improved from 0.17644 to 0.24258, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.4842 - acc: 0.9850 - mDice: 0.2149 - val_loss: 1.6678 - val_acc: 0.9857 - val_mDice: 0.2390

Epoch 00006: val_mDice did not improve from 0.24258
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.3419 - acc: 0.9853 - mDice: 0.2542 - val_loss: 1.5938 - val_acc: 0.9856 - val_mDice: 0.2633

Epoch 00007: val_mDice improved from 0.24258 to 0.26327, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 1.2193 - acc: 0.9855 - mDice: 0.2834 - val_loss: 1.2679 - val_acc: 0.9857 - val_mDice: 0.3199

Epoch 00008: val_mDice improved from 0.26327 to 0.31992, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.1564 - acc: 0.9855 - mDice: 0.3008 - val_loss: 1.3056 - val_acc: 0.9857 - val_mDice: 0.3147

Epoch 00009: val_mDice did not improve from 0.31992
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.2146 - acc: 0.9854 - mDice: 0.2857 - val_loss: 1.2625 - val_acc: 0.9857 - val_mDice: 0.3329

Epoch 00010: val_mDice improved from 0.31992 to 0.33291, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.0876 - acc: 0.9856 - mDice: 0.3275 - val_loss: 1.2121 - val_acc: 0.9857 - val_mDice: 0.3545

Epoch 00011: val_mDice improved from 0.33291 to 0.35445, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.0345 - acc: 0.9857 - mDice: 0.3489 - val_loss: 1.2554 - val_acc: 0.9856 - val_mDice: 0.3354

Epoch 00012: val_mDice did not improve from 0.35445
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.2462 - acc: 0.9857 - mDice: 0.3010 - val_loss: 1.7201 - val_acc: 0.9847 - val_mDice: 0.2973

Epoch 00013: val_mDice did not improve from 0.35445
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 1.1670 - acc: 0.9856 - mDice: 0.3120 - val_loss: 1.2688 - val_acc: 0.9857 - val_mDice: 0.3461

Epoch 00014: val_mDice did not improve from 0.35445
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.0198 - acc: 0.9857 - mDice: 0.3494 - val_loss: 1.6161 - val_acc: 0.9858 - val_mDice: 0.3218

Epoch 00015: val_mDice did not improve from 0.35445
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.0939 - acc: 0.9856 - mDice: 0.3398 - val_loss: 1.3156 - val_acc: 0.9857 - val_mDice: 0.3408

Epoch 00016: val_mDice did not improve from 0.35445
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.0533 - acc: 0.9858 - mDice: 0.3408 - val_loss: 1.2504 - val_acc: 0.9858 - val_mDice: 0.3601

Epoch 00017: val_mDice improved from 0.35445 to 0.36010, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.1305 - acc: 0.9857 - mDice: 0.3238 - val_loss: 1.2897 - val_acc: 0.9853 - val_mDice: 0.3531

Epoch 00018: val_mDice did not improve from 0.36010
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.9898 - acc: 0.9858 - mDice: 0.3614 - val_loss: 1.1933 - val_acc: 0.9858 - val_mDice: 0.3755

Epoch 00019: val_mDice improved from 0.36010 to 0.37546, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.9357 - acc: 0.9858 - mDice: 0.3797 - val_loss: 1.0671 - val_acc: 0.9859 - val_mDice: 0.3946

Epoch 00020: val_mDice improved from 0.37546 to 0.39465, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.9113 - acc: 0.9859 - mDice: 0.3898 - val_loss: 1.0765 - val_acc: 0.9858 - val_mDice: 0.3972

Epoch 00021: val_mDice improved from 0.39465 to 0.39715, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.9745 - acc: 0.9859 - mDice: 0.3877 - val_loss: 1.0283 - val_acc: 0.9858 - val_mDice: 0.3985

Epoch 00022: val_mDice improved from 0.39715 to 0.39849, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.9198 - acc: 0.9858 - mDice: 0.3915 - val_loss: 1.0403 - val_acc: 0.9859 - val_mDice: 0.4020

Epoch 00023: val_mDice improved from 0.39849 to 0.40200, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.8939 - acc: 0.9859 - mDice: 0.3976 - val_loss: 0.9983 - val_acc: 0.9860 - val_mDice: 0.4044

Epoch 00024: val_mDice improved from 0.40200 to 0.40440, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 1.0329 - acc: 0.9858 - mDice: 0.3535 - val_loss: 1.0830 - val_acc: 0.9857 - val_mDice: 0.3743

Epoch 00025: val_mDice did not improve from 0.40440
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.9573 - acc: 0.9858 - mDice: 0.3729 - val_loss: 1.0504 - val_acc: 0.9858 - val_mDice: 0.3891

Epoch 00026: val_mDice did not improve from 0.40440
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.9265 - acc: 0.9858 - mDice: 0.3818 - val_loss: 1.0325 - val_acc: 0.9858 - val_mDice: 0.3925

Epoch 00027: val_mDice did not improve from 0.40440
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.9283 - acc: 0.9859 - mDice: 0.3867 - val_loss: 1.0665 - val_acc: 0.9859 - val_mDice: 0.3932

Epoch 00028: val_mDice did not improve from 0.40440
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.9237 - acc: 0.9860 - mDice: 0.3927 - val_loss: 0.9863 - val_acc: 0.9858 - val_mDice: 0.4062

Epoch 00029: val_mDice improved from 0.40440 to 0.40619, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.9587 - acc: 0.9858 - mDice: 0.3862 - val_loss: 0.9702 - val_acc: 0.9858 - val_mDice: 0.4073

Epoch 00030: val_mDice improved from 0.40619 to 0.40727, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.9020 - acc: 0.9860 - mDice: 0.3975 - val_loss: 0.9848 - val_acc: 0.9860 - val_mDice: 0.4098

Epoch 00031: val_mDice improved from 0.40727 to 0.40984, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.8808 - acc: 0.9859 - mDice: 0.4044 - val_loss: 0.9994 - val_acc: 0.9859 - val_mDice: 0.4090

Epoch 00032: val_mDice did not improve from 0.40984
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.9087 - acc: 0.9859 - mDice: 0.3984 - val_loss: 1.0319 - val_acc: 0.9859 - val_mDice: 0.4113

Epoch 00033: val_mDice improved from 0.40984 to 0.41126, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 14s - loss: 0.8909 - acc: 0.9860 - mDice: 0.4066 - val_loss: 0.9772 - val_acc: 0.9859 - val_mDice: 0.4149

Epoch 00034: val_mDice improved from 0.41126 to 0.41485, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 13s - loss: 0.9509 - acc: 0.9858 - mDice: 0.3759 - val_loss: 1.0619 - val_acc: 0.9859 - val_mDice: 0.4022

Epoch 00035: val_mDice did not improve from 0.41485
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.8650 - acc: 0.9859 - mDice: 0.4081 - val_loss: 1.0034 - val_acc: 0.9860 - val_mDice: 0.4141

Epoch 00036: val_mDice did not improve from 0.41485
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.8571 - acc: 0.9860 - mDice: 0.4176 - val_loss: 0.9949 - val_acc: 0.9859 - val_mDice: 0.4185

Epoch 00037: val_mDice improved from 0.41485 to 0.41850, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 14s - loss: 0.8820 - acc: 0.9859 - mDice: 0.4103 - val_loss: 1.0091 - val_acc: 0.9860 - val_mDice: 0.4130

Epoch 00038: val_mDice did not improve from 0.41850
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 14s - loss: 0.9772 - acc: 0.9855 - mDice: 0.3805 - val_loss: 1.2084 - val_acc: 0.9857 - val_mDice: 0.3850

Epoch 00039: val_mDice did not improve from 0.41850
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.8630 - acc: 0.9860 - mDice: 0.4079 - val_loss: 1.0562 - val_acc: 0.9858 - val_mDice: 0.4050

Epoch 00040: val_mDice did not improve from 0.41850
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 15s - loss: 0.8696 - acc: 0.9859 - mDice: 0.4062 - val_loss: 1.0247 - val_acc: 0.9859 - val_mDice: 0.4131

Epoch 00041: val_mDice did not improve from 0.41850
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.8750 - acc: 0.9857 - mDice: 0.4086 - val_loss: 1.0280 - val_acc: 0.9859 - val_mDice: 0.4033

Epoch 00042: val_mDice did not improve from 0.41850
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 14s - loss: 0.8706 - acc: 0.9860 - mDice: 0.4061 - val_loss: 1.0070 - val_acc: 0.9859 - val_mDice: 0.4120

Epoch 00043: val_mDice did not improve from 0.41850
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 14s - loss: 0.9070 - acc: 0.9859 - mDice: 0.4118 - val_loss: 1.0201 - val_acc: 0.9859 - val_mDice: 0.4097

Epoch 00044: val_mDice did not improve from 0.41850
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.8495 - acc: 0.9860 - mDice: 0.4199 - val_loss: 0.9765 - val_acc: 0.9859 - val_mDice: 0.4166

Epoch 00045: val_mDice did not improve from 0.41850
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.8377 - acc: 0.9861 - mDice: 0.4203 - val_loss: 1.0035 - val_acc: 0.9859 - val_mDice: 0.4129

Epoch 00046: val_mDice did not improve from 0.41850
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.8461 - acc: 0.9860 - mDice: 0.4192 - val_loss: 0.9928 - val_acc: 0.9859 - val_mDice: 0.4175

Epoch 00047: val_mDice did not improve from 0.41850
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 14s - loss: 0.8259 - acc: 0.9860 - mDice: 0.4284 - val_loss: 0.9790 - val_acc: 0.9859 - val_mDice: 0.4194

Epoch 00048: val_mDice improved from 0.41850 to 0.41942, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.8185 - acc: 0.9861 - mDice: 0.4318 - val_loss: 0.9558 - val_acc: 0.9859 - val_mDice: 0.4218

Epoch 00049: val_mDice improved from 0.41942 to 0.42183, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.8449 - acc: 0.9861 - mDice: 0.4309 - val_loss: 0.9561 - val_acc: 0.9860 - val_mDice: 0.4240

Epoch 00050: val_mDice improved from 0.42183 to 0.42398, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.8099 - acc: 0.9861 - mDice: 0.4341 - val_loss: 0.9633 - val_acc: 0.9860 - val_mDice: 0.4240

Epoch 00051: val_mDice improved from 0.42398 to 0.42401, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.7961 - acc: 0.9861 - mDice: 0.4358 - val_loss: 0.9598 - val_acc: 0.9860 - val_mDice: 0.4230

Epoch 00052: val_mDice did not improve from 0.42401
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.8225 - acc: 0.9860 - mDice: 0.4335 - val_loss: 0.9687 - val_acc: 0.9860 - val_mDice: 0.4214

Epoch 00053: val_mDice did not improve from 0.42401
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.8411 - acc: 0.9860 - mDice: 0.4280 - val_loss: 0.9614 - val_acc: 0.9860 - val_mDice: 0.4210

Epoch 00054: val_mDice did not improve from 0.42401
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.8091 - acc: 0.9861 - mDice: 0.4353 - val_loss: 0.9462 - val_acc: 0.9859 - val_mDice: 0.4254

Epoch 00055: val_mDice improved from 0.42401 to 0.42544, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.8368 - acc: 0.9861 - mDice: 0.4369 - val_loss: 0.9388 - val_acc: 0.9860 - val_mDice: 0.4270

Epoch 00056: val_mDice improved from 0.42544 to 0.42703, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.7912 - acc: 0.9862 - mDice: 0.4370 - val_loss: 0.9559 - val_acc: 0.9860 - val_mDice: 0.4245

Epoch 00057: val_mDice did not improve from 0.42703
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.7840 - acc: 0.9862 - mDice: 0.4442 - val_loss: 0.9680 - val_acc: 0.9860 - val_mDice: 0.4250

Epoch 00058: val_mDice did not improve from 0.42703
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.8301 - acc: 0.9861 - mDice: 0.4312 - val_loss: 0.9819 - val_acc: 0.9859 - val_mDice: 0.4203

Epoch 00059: val_mDice did not improve from 0.42703
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.8099 - acc: 0.9862 - mDice: 0.4381 - val_loss: 0.9698 - val_acc: 0.9860 - val_mDice: 0.4262

Epoch 00060: val_mDice did not improve from 0.42703
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.8107 - acc: 0.9860 - mDice: 0.4380 - val_loss: 0.9722 - val_acc: 0.9860 - val_mDice: 0.4253

Epoch 00061: val_mDice did not improve from 0.42703
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7781 - acc: 0.9862 - mDice: 0.4431 - val_loss: 0.9724 - val_acc: 0.9860 - val_mDice: 0.4261

Epoch 00062: val_mDice did not improve from 0.42703
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.8064 - acc: 0.9861 - mDice: 0.4392 - val_loss: 0.9920 - val_acc: 0.9859 - val_mDice: 0.4239

Epoch 00063: val_mDice did not improve from 0.42703
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.8074 - acc: 0.9860 - mDice: 0.4340 - val_loss: 0.9839 - val_acc: 0.9859 - val_mDice: 0.4256

Epoch 00064: val_mDice did not improve from 0.42703
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.8140 - acc: 0.9861 - mDice: 0.4411 - val_loss: 0.9793 - val_acc: 0.9860 - val_mDice: 0.4265

Epoch 00065: val_mDice did not improve from 0.42703
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.8715 - acc: 0.9862 - mDice: 0.4372 - val_loss: 0.9871 - val_acc: 0.9860 - val_mDice: 0.4221

Epoch 00066: val_mDice did not improve from 0.42703
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7955 - acc: 0.9863 - mDice: 0.4437 - val_loss: 0.9714 - val_acc: 0.9860 - val_mDice: 0.4241

Epoch 00067: val_mDice did not improve from 0.42703
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 14s - loss: 0.7888 - acc: 0.9862 - mDice: 0.4427 - val_loss: 0.9787 - val_acc: 0.9860 - val_mDice: 0.4236

Epoch 00068: val_mDice did not improve from 0.42703
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7680 - acc: 0.9862 - mDice: 0.4494 - val_loss: 0.9568 - val_acc: 0.9860 - val_mDice: 0.4279

Epoch 00069: val_mDice improved from 0.42703 to 0.42792, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.8195 - acc: 0.9862 - mDice: 0.4438 - val_loss: 0.9370 - val_acc: 0.9860 - val_mDice: 0.4300

Epoch 00070: val_mDice improved from 0.42792 to 0.43003, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7751 - acc: 0.9862 - mDice: 0.4467 - val_loss: 0.9499 - val_acc: 0.9860 - val_mDice: 0.4293

Epoch 00071: val_mDice did not improve from 0.43003
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.7946 - acc: 0.9862 - mDice: 0.4444 - val_loss: 0.9607 - val_acc: 0.9859 - val_mDice: 0.4281

Epoch 00072: val_mDice did not improve from 0.43003
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7624 - acc: 0.9862 - mDice: 0.4490 - val_loss: 0.9703 - val_acc: 0.9860 - val_mDice: 0.4274

Epoch 00073: val_mDice did not improve from 0.43003
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.8002 - acc: 0.9862 - mDice: 0.4477 - val_loss: 0.9606 - val_acc: 0.9859 - val_mDice: 0.4287

Epoch 00074: val_mDice did not improve from 0.43003
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7753 - acc: 0.9862 - mDice: 0.4489 - val_loss: 0.9632 - val_acc: 0.9860 - val_mDice: 0.4294

Epoch 00075: val_mDice did not improve from 0.43003
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7950 - acc: 0.9862 - mDice: 0.4493 - val_loss: 0.9811 - val_acc: 0.9861 - val_mDice: 0.4268

Epoch 00076: val_mDice did not improve from 0.43003
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7872 - acc: 0.9863 - mDice: 0.4474 - val_loss: 0.9765 - val_acc: 0.9861 - val_mDice: 0.4285

Epoch 00077: val_mDice did not improve from 0.43003
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.8100 - acc: 0.9862 - mDice: 0.4464 - val_loss: 0.9743 - val_acc: 0.9860 - val_mDice: 0.4271

Epoch 00078: val_mDice did not improve from 0.43003
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 13s - loss: 0.7737 - acc: 0.9862 - mDice: 0.4509 - val_loss: 0.9605 - val_acc: 0.9860 - val_mDice: 0.4275

Epoch 00079: val_mDice did not improve from 0.43003
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 13s - loss: 0.8084 - acc: 0.9862 - mDice: 0.4453 - val_loss: 0.9695 - val_acc: 0.9861 - val_mDice: 0.4278

Epoch 00080: val_mDice did not improve from 0.43003
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 13s - loss: 0.7889 - acc: 0.9863 - mDice: 0.4475 - val_loss: 0.9702 - val_acc: 0.9861 - val_mDice: 0.4284

Epoch 00081: val_mDice did not improve from 0.43003
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7577 - acc: 0.9863 - mDice: 0.4542 - val_loss: 0.9714 - val_acc: 0.9860 - val_mDice: 0.4272

Epoch 00082: val_mDice did not improve from 0.43003
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 13s - loss: 0.7453 - acc: 0.9863 - mDice: 0.4568 - val_loss: 0.9690 - val_acc: 0.9860 - val_mDice: 0.4284

Epoch 00083: val_mDice did not improve from 0.43003
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7506 - acc: 0.9863 - mDice: 0.4571 - val_loss: 0.9685 - val_acc: 0.9860 - val_mDice: 0.4283

Epoch 00084: val_mDice did not improve from 0.43003
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7707 - acc: 0.9862 - mDice: 0.4501 - val_loss: 0.9636 - val_acc: 0.9860 - val_mDice: 0.4302

Epoch 00085: val_mDice improved from 0.43003 to 0.43018, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 13s - loss: 0.7781 - acc: 0.9862 - mDice: 0.4474 - val_loss: 0.9760 - val_acc: 0.9861 - val_mDice: 0.4293

Epoch 00086: val_mDice did not improve from 0.43018
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.7582 - acc: 0.9863 - mDice: 0.4530 - val_loss: 0.9861 - val_acc: 0.9860 - val_mDice: 0.4267

Epoch 00087: val_mDice did not improve from 0.43018
Epoch 88/300

Epoch 00088: LearningRateScheduler setting learning rate to 6.25e-05.
 - 13s - loss: 0.8246 - acc: 0.9863 - mDice: 0.4472 - val_loss: 0.9741 - val_acc: 0.9860 - val_mDice: 0.4275

Epoch 00088: val_mDice did not improve from 0.43018
Epoch 89/300

Epoch 00089: LearningRateScheduler setting learning rate to 6.25e-05.
 - 13s - loss: 0.7605 - acc: 0.9862 - mDice: 0.4559 - val_loss: 0.9741 - val_acc: 0.9861 - val_mDice: 0.4270

Epoch 00089: val_mDice did not improve from 0.43018
Epoch 90/300

Epoch 00090: LearningRateScheduler setting learning rate to 6.25e-05.
 - 13s - loss: 0.7899 - acc: 0.9863 - mDice: 0.4540 - val_loss: 0.9708 - val_acc: 0.9861 - val_mDice: 0.4294

Epoch 00090: val_mDice did not improve from 0.43018
Epoch 91/300

Epoch 00091: LearningRateScheduler setting learning rate to 3.125e-05.
 - 13s - loss: 0.7595 - acc: 0.9863 - mDice: 0.4550 - val_loss: 0.9757 - val_acc: 0.9861 - val_mDice: 0.4289

Epoch 00091: val_mDice did not improve from 0.43018
Epoch 92/300

Epoch 00092: LearningRateScheduler setting learning rate to 3.125e-05.
 - 13s - loss: 0.7704 - acc: 0.9863 - mDice: 0.4558 - val_loss: 0.9715 - val_acc: 0.9861 - val_mDice: 0.4288

Epoch 00092: val_mDice did not improve from 0.43018
Epoch 93/300

Epoch 00093: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.7585 - acc: 0.9863 - mDice: 0.4568 - val_loss: 0.9808 - val_acc: 0.9861 - val_mDice: 0.4274

Epoch 00093: val_mDice did not improve from 0.43018
Epoch 94/300

Epoch 00094: LearningRateScheduler setting learning rate to 3.125e-05.
 - 13s - loss: 0.8074 - acc: 0.9862 - mDice: 0.4563 - val_loss: 0.9674 - val_acc: 0.9860 - val_mDice: 0.4288

Epoch 00094: val_mDice did not improve from 0.43018
Epoch 95/300

Epoch 00095: LearningRateScheduler setting learning rate to 3.125e-05.
 - 13s - loss: 0.7832 - acc: 0.9862 - mDice: 0.4501 - val_loss: 0.9611 - val_acc: 0.9860 - val_mDice: 0.4300

Epoch 00095: val_mDice did not improve from 0.43018
Epoch 96/300

Epoch 00096: LearningRateScheduler setting learning rate to 3.125e-05.
 - 13s - loss: 0.7951 - acc: 0.9863 - mDice: 0.4562 - val_loss: 0.9551 - val_acc: 0.9860 - val_mDice: 0.4313

Epoch 00096: val_mDice improved from 0.43018 to 0.43127, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 97/300

Epoch 00097: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.7838 - acc: 0.9863 - mDice: 0.4533 - val_loss: 0.9559 - val_acc: 0.9861 - val_mDice: 0.4312

Epoch 00097: val_mDice did not improve from 0.43127
Epoch 98/300

Epoch 00098: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.7823 - acc: 0.9863 - mDice: 0.4530 - val_loss: 0.9519 - val_acc: 0.9860 - val_mDice: 0.4335

Epoch 00098: val_mDice improved from 0.43127 to 0.43351, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_b/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 99/300

Epoch 00099: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.7905 - acc: 0.9863 - mDice: 0.4557 - val_loss: 0.9556 - val_acc: 0.9860 - val_mDice: 0.4333

Epoch 00099: val_mDice did not improve from 0.43351
Epoch 100/300

Epoch 00100: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.7719 - acc: 0.9863 - mDice: 0.4533 - val_loss: 0.9638 - val_acc: 0.9860 - val_mDice: 0.4311

Epoch 00100: val_mDice did not improve from 0.43351
Epoch 101/300

Epoch 00101: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.7622 - acc: 0.9863 - mDice: 0.4541 - val_loss: 0.9414 - val_acc: 0.9860 - val_mDice: 0.4314

Epoch 00101: val_mDice did not improve from 0.43351
Epoch 102/300

Epoch 00102: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.7557 - acc: 0.9863 - mDice: 0.4579 - val_loss: 0.9483 - val_acc: 0.9860 - val_mDice: 0.4317

Epoch 00102: val_mDice did not improve from 0.43351
Epoch 103/300

Epoch 00103: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.8721 - acc: 0.9863 - mDice: 0.4508 - val_loss: 0.9569 - val_acc: 0.9860 - val_mDice: 0.4310

Epoch 00103: val_mDice did not improve from 0.43351
Epoch 104/300

Epoch 00104: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.7549 - acc: 0.9863 - mDice: 0.4554 - val_loss: 0.9500 - val_acc: 0.9860 - val_mDice: 0.4314

Epoch 00104: val_mDice did not improve from 0.43351
Epoch 105/300

Epoch 00105: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.7674 - acc: 0.9862 - mDice: 0.4495 - val_loss: 0.9414 - val_acc: 0.9860 - val_mDice: 0.4297

Epoch 00105: val_mDice did not improve from 0.43351
Epoch 106/300

Epoch 00106: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.7590 - acc: 0.9863 - mDice: 0.4532 - val_loss: 0.9578 - val_acc: 0.9861 - val_mDice: 0.4287

Epoch 00106: val_mDice did not improve from 0.43351
Epoch 107/300

Epoch 00107: LearningRateScheduler setting learning rate to 3.125e-05.
 - 12s - loss: 0.7676 - acc: 0.9863 - mDice: 0.4570 - val_loss: 0.9620 - val_acc: 0.9861 - val_mDice: 0.4290

Epoch 00107: val_mDice did not improve from 0.43351
Epoch 108/300

Epoch 00108: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.7715 - acc: 0.9864 - mDice: 0.4536 - val_loss: 0.9648 - val_acc: 0.9861 - val_mDice: 0.4292

Epoch 00108: val_mDice did not improve from 0.43351
Epoch 109/300

Epoch 00109: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7427 - acc: 0.9863 - mDice: 0.4592 - val_loss: 0.9635 - val_acc: 0.9861 - val_mDice: 0.4294

Epoch 00109: val_mDice did not improve from 0.43351
Epoch 110/300

Epoch 00110: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7631 - acc: 0.9863 - mDice: 0.4564 - val_loss: 0.9599 - val_acc: 0.9860 - val_mDice: 0.4300

Epoch 00110: val_mDice did not improve from 0.43351
Epoch 111/300

Epoch 00111: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7683 - acc: 0.9863 - mDice: 0.4536 - val_loss: 0.9548 - val_acc: 0.9860 - val_mDice: 0.4305

Epoch 00111: val_mDice did not improve from 0.43351
Epoch 112/300

Epoch 00112: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.8230 - acc: 0.9863 - mDice: 0.4556 - val_loss: 0.9611 - val_acc: 0.9860 - val_mDice: 0.4298

Epoch 00112: val_mDice did not improve from 0.43351
Epoch 113/300

Epoch 00113: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.8724 - acc: 0.9863 - mDice: 0.4527 - val_loss: 0.9610 - val_acc: 0.9860 - val_mDice: 0.4297

Epoch 00113: val_mDice did not improve from 0.43351
Epoch 114/300

Epoch 00114: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7859 - acc: 0.9864 - mDice: 0.4551 - val_loss: 0.9641 - val_acc: 0.9861 - val_mDice: 0.4294

Epoch 00114: val_mDice did not improve from 0.43351
Epoch 115/300

Epoch 00115: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7563 - acc: 0.9864 - mDice: 0.4575 - val_loss: 0.9722 - val_acc: 0.9861 - val_mDice: 0.4283

Epoch 00115: val_mDice did not improve from 0.43351
Epoch 116/300

Epoch 00116: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7557 - acc: 0.9864 - mDice: 0.4587 - val_loss: 0.9722 - val_acc: 0.9861 - val_mDice: 0.4283

Epoch 00116: val_mDice did not improve from 0.43351
Epoch 117/300

Epoch 00117: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7487 - acc: 0.9864 - mDice: 0.4591 - val_loss: 0.9709 - val_acc: 0.9861 - val_mDice: 0.4288

Epoch 00117: val_mDice did not improve from 0.43351
Epoch 118/300

Epoch 00118: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7854 - acc: 0.9863 - mDice: 0.4603 - val_loss: 0.9668 - val_acc: 0.9860 - val_mDice: 0.4293

Epoch 00118: val_mDice did not improve from 0.43351
Epoch 119/300

Epoch 00119: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7485 - acc: 0.9863 - mDice: 0.4582 - val_loss: 0.9544 - val_acc: 0.9860 - val_mDice: 0.4306

Epoch 00119: val_mDice did not improve from 0.43351
Epoch 120/300

Epoch 00120: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7817 - acc: 0.9863 - mDice: 0.4552 - val_loss: 0.9643 - val_acc: 0.9860 - val_mDice: 0.4303

Epoch 00120: val_mDice did not improve from 0.43351
Epoch 121/300

Epoch 00121: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7467 - acc: 0.9863 - mDice: 0.4611 - val_loss: 0.9733 - val_acc: 0.9861 - val_mDice: 0.4290

Epoch 00121: val_mDice did not improve from 0.43351
Epoch 122/300

Epoch 00122: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7554 - acc: 0.9864 - mDice: 0.4567 - val_loss: 0.9722 - val_acc: 0.9861 - val_mDice: 0.4286

Epoch 00122: val_mDice did not improve from 0.43351
Epoch 123/300

Epoch 00123: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 12s - loss: 0.7631 - acc: 0.9863 - mDice: 0.4578 - val_loss: 0.9692 - val_acc: 0.9860 - val_mDice: 0.4280

Epoch 00123: val_mDice did not improve from 0.43351
Epoch 124/300

Epoch 00124: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7527 - acc: 0.9864 - mDice: 0.4582 - val_loss: 0.9697 - val_acc: 0.9861 - val_mDice: 0.4286

Epoch 00124: val_mDice did not improve from 0.43351
Epoch 125/300

Epoch 00125: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 12s - loss: 0.8325 - acc: 0.9863 - mDice: 0.4567 - val_loss: 0.9729 - val_acc: 0.9861 - val_mDice: 0.4283

Epoch 00125: val_mDice did not improve from 0.43351
Epoch 126/300

Epoch 00126: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.7900 - acc: 0.9863 - mDice: 0.4558 - val_loss: 0.9650 - val_acc: 0.9860 - val_mDice: 0.4294

Epoch 00126: val_mDice did not improve from 0.43351
Epoch 127/300

Epoch 00127: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7576 - acc: 0.9863 - mDice: 0.4559 - val_loss: 0.9675 - val_acc: 0.9860 - val_mDice: 0.4292

Epoch 00127: val_mDice did not improve from 0.43351
Epoch 128/300

Epoch 00128: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 12s - loss: 0.7520 - acc: 0.9863 - mDice: 0.4582 - val_loss: 0.9703 - val_acc: 0.9860 - val_mDice: 0.4291

Epoch 00128: val_mDice did not improve from 0.43351
Epoch 129/300

Epoch 00129: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7889 - acc: 0.9863 - mDice: 0.4560 - val_loss: 0.9685 - val_acc: 0.9860 - val_mDice: 0.4293

Epoch 00129: val_mDice did not improve from 0.43351
Epoch 130/300

Epoch 00130: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7661 - acc: 0.9864 - mDice: 0.4588 - val_loss: 0.9688 - val_acc: 0.9860 - val_mDice: 0.4297

Epoch 00130: val_mDice did not improve from 0.43351
Epoch 131/300

Epoch 00131: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7857 - acc: 0.9864 - mDice: 0.4590 - val_loss: 0.9578 - val_acc: 0.9860 - val_mDice: 0.4300

Epoch 00131: val_mDice did not improve from 0.43351
Epoch 132/300

Epoch 00132: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7445 - acc: 0.9863 - mDice: 0.4582 - val_loss: 0.9573 - val_acc: 0.9860 - val_mDice: 0.4301

Epoch 00132: val_mDice did not improve from 0.43351
Epoch 133/300

Epoch 00133: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7447 - acc: 0.9863 - mDice: 0.4582 - val_loss: 0.9571 - val_acc: 0.9860 - val_mDice: 0.4305

Epoch 00133: val_mDice did not improve from 0.43351
Epoch 134/300

Epoch 00134: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 12s - loss: 0.7952 - acc: 0.9864 - mDice: 0.4598 - val_loss: 0.9543 - val_acc: 0.9860 - val_mDice: 0.4306

Epoch 00134: val_mDice did not improve from 0.43351
Epoch 135/300

Epoch 00135: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7519 - acc: 0.9864 - mDice: 0.4582 - val_loss: 0.9587 - val_acc: 0.9861 - val_mDice: 0.4303

Epoch 00135: val_mDice did not improve from 0.43351
Epoch 136/300

Epoch 00136: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7831 - acc: 0.9863 - mDice: 0.4552 - val_loss: 0.9603 - val_acc: 0.9861 - val_mDice: 0.4300

Epoch 00136: val_mDice did not improve from 0.43351
Epoch 137/300

Epoch 00137: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 12s - loss: 0.7520 - acc: 0.9864 - mDice: 0.4613 - val_loss: 0.9651 - val_acc: 0.9861 - val_mDice: 0.4293

Epoch 00137: val_mDice did not improve from 0.43351
Epoch 138/300

Epoch 00138: LearningRateScheduler setting learning rate to 7.8125e-06.
 - 11s - loss: 0.7445 - acc: 0.9864 - mDice: 0.4601 - val_loss: 0.9671 - val_acc: 0.9861 - val_mDice: 0.4290

Epoch 00138: val_mDice did not improve from 0.43351
Restoring model weights from the end of the best epoch
Epoch 00138: early stopping
{'val_loss': [5.291526208656111, 2.933652131298508, 3.060705984129888, 2.5969158942333324, 1.7591208768694588, 1.6678489089458624, 1.593827190917083, 1.2679095370939162, 1.3056160548653049, 1.2624787328841534, 1.2121012518914898, 1.255423657903064, 1.7201303190059876, 1.2687697323520533, 1.6161119029316562, 1.315567936566885, 1.2503822211469158, 1.2896925025664912, 1.1933251567994163, 1.0670594584182853, 1.076490085669671, 1.028305647301763, 1.0402593025553986, 0.998278534144498, 1.08300340130981, 1.0504023702850056, 1.0325183645169833, 1.0664703953132202, 0.9862865068939295, 0.9701630908451723, 0.9848470571782258, 0.9993983422325792, 1.0319417398074147, 0.9771512369538068, 1.0618897178646332, 1.0034090950247947, 0.9949206153998215, 1.0090753690580303, 1.2084213740816723, 1.0561573340205217, 1.0247270730550817, 1.027969287575854, 1.0070422528388348, 1.0200548332728696, 0.9764874932471286, 1.003529662943065, 0.992771575513404, 0.9789876594079121, 0.9557848349492648, 0.9561097711213072, 0.9632568928632843, 0.9597805660315667, 0.9686869079254093, 0.961403854777304, 0.9462284639980016, 0.9387768636482039, 0.9559486548552353, 0.9679668626088774, 0.981856093201298, 0.9697570669070612, 0.9721831947230222, 0.9723782512579071, 0.9919645313020056, 0.9838525936398167, 0.9793386483906806, 0.9871151304423585, 0.9713528564806735, 0.9787252165851522, 0.9567699269409037, 0.9370129191027152, 0.9499045994397852, 0.9606526712799787, 0.9703245142872414, 0.9605794055631545, 0.9632179862104552, 0.9810981221413344, 0.9764994798081644, 0.9743388033984752, 0.9605448817492425, 0.9694749293255895, 0.970236553011762, 0.9713704577099518, 0.9690179027868121, 0.9684555742624548, 0.963598343986697, 0.9760214487711588, 0.986060040050678, 0.9741370240400793, 0.9740532987126697, 0.9708066580447365, 0.9756976078065593, 0.9715251971719864, 0.9807751520296161, 0.9674203261900484, 0.9611351898546969, 0.9550642415825348, 0.9558594619736689, 0.9519225111168422, 0.9556072760610544, 0.9637615321280805, 0.9414438263753827, 0.9483247352450082, 0.956919094149986, 0.9500273017401106, 0.9414042529095424, 0.9577874905161197, 0.9619697068960925, 0.9648211660903044, 0.9634553285573753, 0.9599113759030117, 0.9548255796736099, 0.9611290481206629, 0.9609527929445331, 0.9640831451737479, 0.9721921764955985, 0.97215669900737, 0.9708520299039977, 0.9668396354168095, 0.9544001029671801, 0.9643023719501852, 0.9732694032040428, 0.9722402098027061, 0.9691718397069067, 0.9696672494045357, 0.9729430961251705, 0.9650178507026215, 0.9675183735983202, 0.9703114008189141, 0.9684824923451028, 0.9687717090385237, 0.9578415155410767, 0.9573124109136031, 0.9570774518595206, 0.9542858618475525, 0.9586526204584244, 0.9603133650308244, 0.9650885866822375, 0.9670653017272663], 'val_acc': [0.706542822752106, 0.9793820622261991, 0.9855975745322553, 0.985616818572698, 0.9854198607166161, 0.9856599919804919, 0.9855982147799003, 0.9856667429320375, 0.9856737470358945, 0.9856883582104458, 0.9856600665421075, 0.9856335107753339, 0.9847405923439769, 0.9856603839870696, 0.985757069194808, 0.9856890861907702, 0.9857592964440249, 0.9853231746159242, 0.9857975528034825, 0.9858670493636685, 0.9858491440837303, 0.9858444556314847, 0.9858694931541043, 0.9859587139404669, 0.9856702426399631, 0.9858332454934995, 0.9857885636640399, 0.9859278684251764, 0.9857576234956805, 0.9858255346169632, 0.9859513932846012, 0.9858543930875228, 0.9858713348706564, 0.9858933559964213, 0.9859012226933397, 0.985961174250542, 0.9859407399031107, 0.9859551360991117, 0.9857223248213864, 0.9858473967970087, 0.9858626613902689, 0.9858521624897303, 0.9858893832463896, 0.9859323847606387, 0.9858803220009535, 0.9858772223361869, 0.9859032954616047, 0.9859245229749644, 0.9859467142083672, 0.985991388894199, 0.986029479610786, 0.9860280470901661, 0.9859811435924487, 0.9859553651416793, 0.9859419353445817, 0.9860058717066875, 0.9859848004601868, 0.985975249429767, 0.9859123636274302, 0.985973737213049, 0.9860202587499154, 0.9859879742400923, 0.9859174489974976, 0.9859025788664372, 0.985999421233988, 0.9859709547700061, 0.9859770096643141, 0.9860089637813497, 0.9859865406032805, 0.9859647525383739, 0.9859827975655316, 0.9859189585353552, 0.986010478007213, 0.985940264628621, 0.9860135805740785, 0.9860598516374938, 0.9860782147793288, 0.9860485640357942, 0.9860013310382428, 0.9860513442464536, 0.9860531672109826, 0.9860351951827717, 0.9860242370809063, 0.9860148581672697, 0.9860150032722101, 0.9860579451818144, 0.9860328197032772, 0.9860463434837284, 0.9860635011383657, 0.9860695522376214, 0.9860652649447266, 0.9860504615619388, 0.9860694707556164, 0.9860127809342374, 0.9860071477372102, 0.986038942685288, 0.9860509477751085, 0.9860167539075073, 0.9860331313440416, 0.9860265353199248, 0.9859972707787703, 0.9860313132907568, 0.9860463376795308, 0.9860157984472839, 0.9860020550002766, 0.9860519050212389, 0.9860661284307416, 0.9860619991906127, 0.9860640616899126, 0.9860198627250472, 0.9860193800836913, 0.9860320993130574, 0.9860495208354478, 0.9860590526673678, 0.9860703478591719, 0.9860678904511956, 0.9860534087548988, 0.9860367998201749, 0.9860183467132768, 0.9860330552197574, 0.986062239618337, 0.9860724105817101, 0.986049831136782, 0.9860515815488408, 0.9860521429933412, 0.9860184322135725, 0.9860254264949413, 0.9860339258494002, 0.9860293168700143, 0.9860391788714834, 0.9860336898864431, 0.9860374318080002, 0.9860415670755651, 0.9860374318080002, 0.9860598509677787, 0.9860571486673105, 0.9860616031657444, 0.9860609673828667], 'val_mDice': [0.005993747770653738, 0.09092260595787777, 0.13107254013614486, 0.17643769074245338, 0.24258461806658055, 0.23898071112034472, 0.26326992002765787, 0.31991579793812186, 0.31473481677444687, 0.33291491832626, 0.354450743519858, 0.33544357718153395, 0.29733630180470505, 0.346113839957598, 0.32183754059036124, 0.3407664455836185, 0.36010135978125457, 0.3530594398466389, 0.37546351401323685, 0.39464774683173676, 0.39715331591916886, 0.3984923916363091, 0.40200353310572523, 0.4043987098704563, 0.3743085481700826, 0.3890757673465357, 0.39253714089090014, 0.3931902719012807, 0.4061947227194068, 0.4072651267051697, 0.4098364296700624, 0.40900295091032535, 0.4112565823262104, 0.41485322258445656, 0.4022374638680662, 0.4140620002809089, 0.4184995337148731, 0.4129925206136168, 0.38503301132484324, 0.40497031216317797, 0.41312200020761525, 0.403343431958545, 0.41197507613607115, 0.409692949793312, 0.416629861468233, 0.412941934687368, 0.41745429918560645, 0.4194201175193215, 0.4218346935309721, 0.4239781753400738, 0.42401260662168155, 0.4230209423808123, 0.42136199320300244, 0.4210327404938387, 0.4254430683141344, 0.4270323887746432, 0.42445194062668734, 0.4250330075565795, 0.42032022623533616, 0.4261781832252102, 0.4253311316841997, 0.42612820194008644, 0.4239475731546066, 0.42564966861674847, 0.42651205440139056, 0.4220932126045227, 0.4240794386086839, 0.4235825602258189, 0.42791537689359, 0.43003240116080094, 0.42928724018822, 0.4280688193406952, 0.4274411169107487, 0.4286678879895014, 0.4293940173329486, 0.4267914198087842, 0.42852444773756165, 0.42712122380510253, 0.4275285429722361, 0.42777271607841894, 0.42839827419220283, 0.4272353853402513, 0.42837492122632287, 0.4282620593849639, 0.4301763784126396, 0.42925919207294333, 0.42672223500098183, 0.42753252967466576, 0.42699819803237915, 0.4294470503982087, 0.42892274506083145, 0.4288434962208351, 0.4274224326405186, 0.42883628071024177, 0.43004976352502344, 0.43127124731460315, 0.431227181288187, 0.4335095291950283, 0.433344310030955, 0.43110401521014813, 0.4314212429612763, 0.4317367178447238, 0.43096132682503835, 0.4313678264841158, 0.4297033568446556, 0.4287067460879851, 0.42904354213328844, 0.42917859922634083, 0.42940775185042107, 0.4300024343117346, 0.4304722080962935, 0.4297646876801266, 0.4296939070528366, 0.4294210136606452, 0.4283197205164906, 0.42830265829625647, 0.4288459143388584, 0.4292558032028684, 0.4306248594982347, 0.43025650022628154, 0.4289540790216753, 0.42863436528805937, 0.4279549900958601, 0.428564101569215, 0.4283246970578526, 0.4293986599097091, 0.42920066388358785, 0.42906553680530646, 0.4293057064661819, 0.4296537196814791, 0.42997577953874394, 0.43009422334392416, 0.43045820904135257, 0.4306279580468096, 0.43032191264048947, 0.42999062087205464, 0.42931957660096415, 0.4290232891670327], 'loss': [5.715409807684297, 4.00465852645128, 2.4617754410971844, 1.8391078990366312, 1.6203152980552287, 1.4841786670875121, 1.3418745454572685, 1.2193330680678747, 1.1563651537354018, 1.2145769428392097, 1.0875993362535232, 1.034501874140594, 1.2462466198418556, 1.1670443540679216, 1.0197814537956102, 1.0939124220474368, 1.0533141435664561, 1.1304979721011292, 0.9897557499700723, 0.9356581884062061, 0.9112582747910676, 0.9744522134453081, 0.9197757795404752, 0.893863702421029, 1.0328662723756306, 0.9573122488051824, 0.926488323382946, 0.9282998017582045, 0.9236821468169626, 0.9586771402194869, 0.9019548074656158, 0.8808319630483702, 0.9086555188122638, 0.8909303791120362, 0.9509483154470054, 0.8650481580171421, 0.8570960442244961, 0.8820156569372659, 0.9772093923468339, 0.8630239932011952, 0.8696298348276239, 0.8750473122015116, 0.8706123859812774, 0.9069988992809506, 0.8494854738778324, 0.8376850145777195, 0.8460681581176049, 0.8258534027591077, 0.8184671093323903, 0.8448608617409116, 0.8098829515142671, 0.7961003962940455, 0.8225431246601693, 0.8411190311163552, 0.8090738459220995, 0.8367802794659397, 0.7911776335467183, 0.7839622031307244, 0.8301484701965895, 0.8099094088400471, 0.8107309433610459, 0.7780983858228652, 0.8063972761102316, 0.8074207268441577, 0.8139875644637212, 0.8714795952932768, 0.7955428399061972, 0.7888492125470491, 0.7679613953250458, 0.819522873425787, 0.7751485869069146, 0.7946393303099223, 0.7624197180772487, 0.8002209646144571, 0.7753330763214751, 0.7949558883927712, 0.7872143073639807, 0.8100155613076868, 0.7737168970177019, 0.8084407729322282, 0.7888706139002871, 0.7577191772946219, 0.7452759496230204, 0.7506218380570026, 0.7707122543203649, 0.7781257953510584, 0.7582235030028909, 0.8245650154897833, 0.7604848017089532, 0.78987573542539, 0.759451130674081, 0.7704386269847007, 0.7584589720664401, 0.8073660621443245, 0.7831807010457473, 0.7951152184325342, 0.7838487541862901, 0.7823189066127816, 0.7904907006163584, 0.7718577605585529, 0.7621836058663501, 0.7557397789121358, 0.8720660376174101, 0.7548582708487698, 0.7674427358668235, 0.7589544639768193, 0.7675822412738719, 0.7714710575174173, 0.742684602469808, 0.7630946281753772, 0.7683396259725584, 0.8229799183802463, 0.8724225599232324, 0.7858668493493057, 0.7563116479675263, 0.7557279760922598, 0.7486851700644566, 0.7854047246216003, 0.7485234111763052, 0.781685050451479, 0.7467371323781288, 0.7553959375725449, 0.7631228347763096, 0.7527153772081512, 0.8324908821265815, 0.7899682909569915, 0.7575591208780754, 0.7519788071234407, 0.7889296068280497, 0.7661305662104244, 0.7857289963096834, 0.7444632823522562, 0.7447425021617363, 0.7952110499151034, 0.751887922662843, 0.7830649380316369, 0.7520434326529651, 0.7445411247397099], 'acc': [0.3805265033087489, 0.6867812346359475, 0.9679876613063865, 0.9835514846987544, 0.9845907570388615, 0.9849774159526373, 0.9852915857313398, 0.9854584376869668, 0.9855235519032135, 0.985449964564826, 0.9856107752499779, 0.9857482275305082, 0.9856946544178586, 0.9855533742970081, 0.9857280147762542, 0.9856409810968515, 0.9858217065130335, 0.9857185059992504, 0.9858095872785778, 0.985795485539221, 0.9858549401985496, 0.9858562131717812, 0.9858433747416454, 0.9858893626465943, 0.9857836803256439, 0.9858223013105937, 0.985822238687804, 0.9858823504106574, 0.9859791753981351, 0.9858222786670732, 0.98596869123996, 0.9859152883477091, 0.9858524137733881, 0.985957629738915, 0.9858211730893754, 0.9858819695296922, 0.9859502183942017, 0.9858647870077817, 0.9854559336971779, 0.9859506182314958, 0.9858621508291934, 0.9857135741274742, 0.9859623748425327, 0.9859197630077028, 0.985978497817172, 0.986066723544867, 0.9859757888611489, 0.9860414266883658, 0.9861034257479159, 0.9860659354670918, 0.9861297368646471, 0.9860812915058379, 0.9860074683804558, 0.9860301137892318, 0.986093259104531, 0.9861246898722013, 0.9861821600492234, 0.9861595824520676, 0.9860808504106517, 0.9861572066962674, 0.9860370819721475, 0.9861809507097856, 0.9860969875690854, 0.9860325075944304, 0.9861412398623064, 0.9861722058207949, 0.9862705082571039, 0.9861957354716869, 0.9862283575739995, 0.9861869291641909, 0.986150938098524, 0.9862449475879769, 0.9861920437898443, 0.9862106907216056, 0.9861676126205585, 0.9862170906755566, 0.9862978961081778, 0.9861838745339371, 0.986182453745944, 0.9861823316374513, 0.9863137208217099, 0.9863164935453362, 0.9862674781883052, 0.986252867244788, 0.9862137466304823, 0.9862463804039464, 0.9862947240676271, 0.9862513809496698, 0.9862347152293189, 0.9863298430232283, 0.9862898648840794, 0.9863164526442691, 0.9862888521106203, 0.9862459612534985, 0.9861954639724556, 0.986336256224308, 0.9863079603873871, 0.9862921609786955, 0.9862926060584276, 0.9862963063635414, 0.9862896366498809, 0.9863121128046592, 0.9863334007618819, 0.9863278769471455, 0.9862015867031075, 0.986293725136005, 0.9863491153948978, 0.9863804164120729, 0.9863419799205222, 0.986340245141386, 0.9862710974941411, 0.9863296693386535, 0.9863178908274816, 0.986372142190141, 0.9863628677579642, 0.9863531193432293, 0.986350981307226, 0.9863079370153488, 0.986267353136006, 0.9862860755659928, 0.9863308119369463, 0.9863921176706296, 0.9863234873899173, 0.9863518281570544, 0.9863391425223623, 0.9862746820541414, 0.986308854293514, 0.9863205213414727, 0.9862852899116541, 0.986355321191439, 0.9863600696551589, 0.9863163119511403, 0.9863426978821824, 0.9863736005785698, 0.9863720340572946, 0.9863307773546556, 0.9864125795928164, 0.9863844081837508], 'mDice': [0.004023857778545732, 0.031232397271569724, 0.1030100486256381, 0.16286532560781103, 0.19735403205938293, 0.2149473796664821, 0.2542027753505305, 0.2834030290283625, 0.3008318682075528, 0.28566912477466644, 0.32749140363332485, 0.3489019478250543, 0.30098370069219277, 0.3120405630492665, 0.3493736959523648, 0.33982336720004286, 0.3408060954764288, 0.3238145760182699, 0.3614340757723014, 0.3796549367173172, 0.38975025706381594, 0.38771376197372004, 0.3915022617639464, 0.3976069692067893, 0.353480036589058, 0.3728765259473869, 0.38177065400745425, 0.38665930492662676, 0.3927126254177385, 0.386241630139747, 0.3975025614647831, 0.4044001225106108, 0.3984095936951925, 0.4065580289642705, 0.3758513705930855, 0.408140576013635, 0.41763213977503877, 0.41029515818419315, 0.3804576047030611, 0.40790194209063846, 0.40617031463365894, 0.4085946300211735, 0.4060628684077307, 0.41175634092257546, 0.41992354842753565, 0.42028803963202077, 0.4192146734276684, 0.4284464768650943, 0.4317518472745847, 0.43088637455558204, 0.43412297630687696, 0.4357970534759021, 0.43350325731375206, 0.4279830102699821, 0.43533405098840167, 0.4369207011415196, 0.4370493160485924, 0.4441534609799374, 0.43122670980508143, 0.43808082671616255, 0.4379816293924579, 0.44308570424319205, 0.4392062256084353, 0.4339869451005431, 0.44107934254600195, 0.4371754850167293, 0.44365423469880655, 0.44271943913519485, 0.4493744732674056, 0.4437643914753197, 0.44672130553405404, 0.4444318903402427, 0.4489726608725491, 0.4476655743891653, 0.44886506086247435, 0.4493300523344092, 0.4473505992564099, 0.446408259062792, 0.45093452684389856, 0.44529650627035366, 0.44749770423336555, 0.4541795075489833, 0.4568274233459564, 0.45706176858169534, 0.45009896614867334, 0.44743194285548815, 0.4529881965537177, 0.4471588969290123, 0.4558880416628547, 0.4540092585746354, 0.45498837142194054, 0.4557726176064875, 0.45678722208710076, 0.4563202768393246, 0.45009872836940573, 0.4562406112433193, 0.4533458370673564, 0.4530444508179164, 0.45571990457221023, 0.45327592997754357, 0.4541383104435551, 0.4579321573680998, 0.45079660126472476, 0.45542668290464144, 0.44948319402767495, 0.45316818511881535, 0.45698834633434704, 0.4536213873107108, 0.45924853299525825, 0.45635778390579196, 0.4536141757657029, 0.45563707144192045, 0.4526510412711162, 0.45508294517067965, 0.45745710191152195, 0.45869888341852066, 0.4590634190551537, 0.4602826038153875, 0.4581863563177918, 0.455150743394934, 0.461070638251263, 0.4566572837236071, 0.45780498149181487, 0.45822379145428377, 0.4567473596781975, 0.4558429567482026, 0.4558700110645299, 0.4581927350852578, 0.45595153284566486, 0.45880245322389, 0.45895135234674334, 0.45823264213445686, 0.45820349672691696, 0.4598086755021191, 0.4581833464988124, 0.45517940462740447, 0.4612907228861761, 0.46006008043762575], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06]}
predicting test subjects:   0%|          | 0/11 [00:00<?, ?it/s]predicting test subjects:   9%|▉         | 1/11 [00:03<00:39,  3.92s/it]predicting test subjects:  18%|█▊        | 2/11 [00:07<00:34,  3.83s/it]predicting test subjects:  27%|██▋       | 3/11 [00:09<00:27,  3.38s/it]predicting test subjects:  36%|███▋      | 4/11 [00:13<00:24,  3.53s/it]predicting test subjects:  45%|████▌     | 5/11 [00:17<00:21,  3.65s/it]predicting test subjects:  55%|█████▍    | 6/11 [00:21<00:18,  3.66s/it]predicting test subjects:  64%|██████▎   | 7/11 [00:24<00:14,  3.63s/it]predicting test subjects:  73%|███████▎  | 8/11 [00:29<00:11,  3.80s/it]predicting test subjects:  82%|████████▏ | 9/11 [00:33<00:07,  3.85s/it]predicting test subjects:  91%|█████████ | 10/11 [00:36<00:03,  3.74s/it]predicting test subjects: 100%|██████████| 11/11 [00:40<00:00,  3.76s/it]
predicting train subjects:   0%|          | 0/41 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/41 [00:04<02:40,  4.00s/it]predicting train subjects:   5%|▍         | 2/41 [00:07<02:31,  3.89s/it]predicting train subjects:   7%|▋         | 3/41 [00:10<02:19,  3.67s/it]predicting train subjects:  10%|▉         | 4/41 [00:13<02:05,  3.38s/it]predicting train subjects:  12%|█▏        | 5/41 [00:17<02:10,  3.61s/it]predicting train subjects:  15%|█▍        | 6/41 [00:21<02:04,  3.55s/it]predicting train subjects:  17%|█▋        | 7/41 [00:24<01:55,  3.40s/it]predicting train subjects:  20%|█▉        | 8/41 [00:26<01:40,  3.04s/it]predicting train subjects:  22%|██▏       | 9/41 [00:29<01:42,  3.22s/it]predicting train subjects:  24%|██▍       | 10/41 [00:34<01:49,  3.52s/it]predicting train subjects:  27%|██▋       | 11/41 [00:38<01:51,  3.71s/it]predicting train subjects:  29%|██▉       | 12/41 [00:40<01:34,  3.27s/it]predicting train subjects:  32%|███▏      | 13/41 [00:44<01:33,  3.33s/it]predicting train subjects:  34%|███▍      | 14/41 [00:47<01:33,  3.45s/it]predicting train subjects:  37%|███▋      | 15/41 [00:50<01:22,  3.16s/it]predicting train subjects:  39%|███▉      | 16/41 [00:53<01:22,  3.29s/it]predicting train subjects:  41%|████▏     | 17/41 [00:58<01:30,  3.78s/it]predicting train subjects:  44%|████▍     | 18/41 [01:02<01:27,  3.82s/it]predicting train subjects:  46%|████▋     | 19/41 [01:05<01:20,  3.65s/it]predicting train subjects:  49%|████▉     | 20/41 [01:08<01:12,  3.43s/it]predicting train subjects:  51%|█████     | 21/41 [01:12<01:08,  3.42s/it]predicting train subjects:  54%|█████▎    | 22/41 [01:15<01:04,  3.37s/it]predicting train subjects:  56%|█████▌    | 23/41 [01:19<01:01,  3.43s/it]predicting train subjects:  59%|█████▊    | 24/41 [01:22<01:00,  3.54s/it]predicting train subjects:  61%|██████    | 25/41 [01:26<00:57,  3.59s/it]predicting train subjects:  63%|██████▎   | 26/41 [01:31<01:00,  4.03s/it]predicting train subjects:  66%|██████▌   | 27/41 [01:37<01:03,  4.54s/it]predicting train subjects:  68%|██████▊   | 28/41 [01:42<01:02,  4.81s/it]predicting train subjects:  71%|███████   | 29/41 [01:47<00:56,  4.71s/it]predicting train subjects:  73%|███████▎  | 30/41 [01:50<00:48,  4.40s/it]predicting train subjects:  76%|███████▌  | 31/41 [01:54<00:42,  4.22s/it]predicting train subjects:  78%|███████▊  | 32/41 [01:59<00:38,  4.27s/it]predicting train subjects:  80%|████████  | 33/41 [02:03<00:34,  4.27s/it]predicting train subjects:  83%|████████▎ | 34/41 [02:06<00:27,  3.95s/it]predicting train subjects:  85%|████████▌ | 35/41 [02:09<00:22,  3.75s/it]predicting train subjects:  88%|████████▊ | 36/41 [02:13<00:18,  3.70s/it]predicting train subjects:  90%|█████████ | 37/41 [02:17<00:14,  3.73s/it]predicting train subjects:  93%|█████████▎| 38/41 [02:20<00:10,  3.63s/it]predicting train subjects:  95%|█████████▌| 39/41 [02:23<00:06,  3.49s/it]predicting train subjects:  98%|█████████▊| 40/41 [02:27<00:03,  3.62s/it]predicting train subjects: 100%|██████████| 41/41 [02:32<00:00,  3.91s/it]
Loading train:   0%|          | 0/42 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/42 [00:00<00:35,  1.15it/s]Loading train:   5%|▍         | 2/42 [00:01<00:34,  1.16it/s]Loading train:   7%|▋         | 3/42 [00:02<00:31,  1.23it/s]Loading train:  10%|▉         | 4/42 [00:02<00:28,  1.35it/s]Loading train:  12%|█▏        | 5/42 [00:03<00:26,  1.40it/s]Loading train:  14%|█▍        | 6/42 [00:04<00:24,  1.44it/s]Loading train:  17%|█▋        | 7/42 [00:04<00:22,  1.55it/s]Loading train:  19%|█▉        | 8/42 [00:05<00:20,  1.64it/s]Loading train:  21%|██▏       | 9/42 [00:05<00:20,  1.61it/s]Loading train:  24%|██▍       | 10/42 [00:06<00:20,  1.57it/s]Loading train:  26%|██▌       | 11/42 [00:07<00:19,  1.57it/s]Loading train:  29%|██▊       | 12/42 [00:07<00:17,  1.72it/s]Loading train:  31%|███       | 13/42 [00:08<00:17,  1.69it/s]Loading train:  33%|███▎      | 14/42 [00:09<00:16,  1.65it/s]Loading train:  36%|███▌      | 15/42 [00:09<00:15,  1.71it/s]Loading train:  38%|███▊      | 16/42 [00:10<00:16,  1.62it/s]Loading train:  40%|████      | 17/42 [00:10<00:15,  1.56it/s]Loading train:  43%|████▎     | 18/42 [00:11<00:15,  1.56it/s]Loading train:  45%|████▌     | 19/42 [00:12<00:14,  1.64it/s]Loading train:  48%|████▊     | 20/42 [00:12<00:13,  1.62it/s]Loading train:  50%|█████     | 21/42 [00:13<00:13,  1.56it/s]Loading train:  52%|█████▏    | 22/42 [00:13<00:12,  1.63it/s]Loading train:  55%|█████▍    | 23/42 [00:14<00:10,  1.73it/s]Loading train:  57%|█████▋    | 24/42 [00:15<00:10,  1.68it/s]Loading train:  60%|█████▉    | 25/42 [00:15<00:09,  1.73it/s]Loading train:  62%|██████▏   | 26/42 [00:16<00:09,  1.71it/s]Loading train:  64%|██████▍   | 27/42 [00:16<00:09,  1.62it/s]Loading train:  67%|██████▋   | 28/42 [00:17<00:09,  1.49it/s]Loading train:  69%|██████▉   | 29/42 [00:18<00:09,  1.36it/s]Loading train:  71%|███████▏  | 30/42 [00:19<00:08,  1.44it/s]Loading train:  74%|███████▍  | 31/42 [00:19<00:07,  1.47it/s]Loading train:  76%|███████▌  | 32/42 [00:20<00:06,  1.44it/s]Loading train:  79%|███████▊  | 33/42 [00:21<00:06,  1.41it/s]Loading train:  81%|████████  | 34/42 [00:22<00:05,  1.38it/s]Loading train:  83%|████████▎ | 35/42 [00:22<00:04,  1.40it/s]Loading train:  86%|████████▌ | 36/42 [00:23<00:04,  1.45it/s]Loading train:  88%|████████▊ | 37/42 [00:23<00:03,  1.54it/s]Loading train:  90%|█████████ | 38/42 [00:24<00:02,  1.50it/s]Loading train:  93%|█████████▎| 39/42 [00:25<00:02,  1.47it/s]Loading train:  95%|█████████▌| 40/42 [00:25<00:01,  1.54it/s]Loading train:  98%|█████████▊| 41/42 [00:26<00:00,  1.54it/s]Loading train: 100%|██████████| 42/42 [00:27<00:00,  1.52it/s]
concatenating: train:   0%|          | 0/42 [00:00<?, ?it/s]concatenating: train:   5%|▍         | 2/42 [00:00<00:02, 14.79it/s]concatenating: train:  10%|▉         | 4/42 [00:00<00:02, 15.03it/s]concatenating: train:  17%|█▋        | 7/42 [00:00<00:02, 16.84it/s]concatenating: train:  26%|██▌       | 11/42 [00:00<00:01, 19.53it/s]concatenating: train:  33%|███▎      | 14/42 [00:00<00:01, 20.58it/s]concatenating: train:  38%|███▊      | 16/42 [00:00<00:01, 19.68it/s]concatenating: train:  45%|████▌     | 19/42 [00:00<00:01, 21.38it/s]concatenating: train:  52%|█████▏    | 22/42 [00:00<00:00, 22.15it/s]concatenating: train:  60%|█████▉    | 25/42 [00:01<00:00, 23.15it/s]concatenating: train:  69%|██████▉   | 29/42 [00:01<00:00, 26.06it/s]concatenating: train:  83%|████████▎ | 35/42 [00:01<00:00, 30.27it/s]concatenating: train:  93%|█████████▎| 39/42 [00:01<00:00, 30.56it/s]concatenating: train: 100%|██████████| 42/42 [00:01<00:00, 26.78it/s]
Loading test:   0%|          | 0/10 [00:00<?, ?it/s]Loading test:  10%|█         | 1/10 [00:00<00:08,  1.07it/s]Loading test:  20%|██        | 2/10 [00:01<00:06,  1.18it/s]Loading test:  30%|███       | 3/10 [00:02<00:05,  1.25it/s]Loading test:  40%|████      | 4/10 [00:03<00:04,  1.23it/s]Loading test:  50%|█████     | 5/10 [00:03<00:04,  1.22it/s]Loading test:  60%|██████    | 6/10 [00:04<00:03,  1.30it/s]Loading test:  70%|███████   | 7/10 [00:05<00:02,  1.35it/s]Loading test:  80%|████████  | 8/10 [00:05<00:01,  1.47it/s]Loading test:  90%|█████████ | 9/10 [00:06<00:00,  1.49it/s]Loading test: 100%|██████████| 10/10 [00:07<00:00,  1.53it/s]
concatenating: validation:   0%|          | 0/10 [00:00<?, ?it/s]concatenating: validation:  20%|██        | 2/10 [00:00<00:00, 13.20it/s]concatenating: validation:  50%|█████     | 5/10 [00:00<00:00, 15.19it/s]concatenating: validation:  80%|████████  | 8/10 [00:00<00:00, 16.43it/s]concatenating: validation: 100%|██████████| 10/10 [00:00<00:00, 15.87it/s]
Loading trainS:   0%|          | 0/42 [00:00<?, ?it/s]Loading trainS:   2%|▏         | 1/42 [00:00<00:21,  1.87it/s]Loading trainS:   5%|▍         | 2/42 [00:01<00:22,  1.81it/s]Loading trainS:   7%|▋         | 3/42 [00:01<00:22,  1.70it/s]Loading trainS:  10%|▉         | 4/42 [00:02<00:21,  1.77it/s]Loading trainS:  12%|█▏        | 5/42 [00:03<00:22,  1.62it/s]Loading trainS:  14%|█▍        | 6/42 [00:03<00:22,  1.57it/s]Loading trainS:  17%|█▋        | 7/42 [00:04<00:21,  1.66it/s]Loading trainS:  19%|█▉        | 8/42 [00:04<00:18,  1.81it/s]Loading trainS:  21%|██▏       | 9/42 [00:05<00:18,  1.74it/s]Loading trainS:  24%|██▍       | 10/42 [00:06<00:19,  1.63it/s]Loading trainS:  26%|██▌       | 11/42 [00:06<00:19,  1.63it/s]Loading trainS:  29%|██▊       | 12/42 [00:07<00:16,  1.77it/s]Loading trainS:  31%|███       | 13/42 [00:07<00:16,  1.79it/s]Loading trainS:  33%|███▎      | 14/42 [00:08<00:15,  1.78it/s]Loading trainS:  36%|███▌      | 15/42 [00:08<00:14,  1.87it/s]Loading trainS:  38%|███▊      | 16/42 [00:09<00:14,  1.82it/s]Loading trainS:  40%|████      | 17/42 [00:09<00:14,  1.68it/s]Loading trainS:  43%|████▎     | 18/42 [00:10<00:14,  1.65it/s]Loading trainS:  45%|████▌     | 19/42 [00:11<00:13,  1.72it/s]Loading trainS:  48%|████▊     | 20/42 [00:11<00:12,  1.72it/s]Loading trainS:  50%|█████     | 21/42 [00:12<00:11,  1.79it/s]Loading trainS:  52%|█████▏    | 22/42 [00:12<00:11,  1.79it/s]Loading trainS:  55%|█████▍    | 23/42 [00:13<00:10,  1.85it/s]Loading trainS:  57%|█████▋    | 24/42 [00:13<00:09,  1.81it/s]Loading trainS:  60%|█████▉    | 25/42 [00:14<00:09,  1.76it/s]Loading trainS:  62%|██████▏   | 26/42 [00:15<00:09,  1.75it/s]Loading trainS:  64%|██████▍   | 27/42 [00:15<00:08,  1.78it/s]Loading trainS:  67%|██████▋   | 28/42 [00:16<00:07,  1.75it/s]Loading trainS:  69%|██████▉   | 29/42 [00:16<00:07,  1.74it/s]Loading trainS:  71%|███████▏  | 30/42 [00:17<00:06,  1.77it/s]Loading trainS:  74%|███████▍  | 31/42 [00:17<00:06,  1.70it/s]Loading trainS:  76%|███████▌  | 32/42 [00:18<00:05,  1.71it/s]Loading trainS:  79%|███████▊  | 33/42 [00:19<00:05,  1.65it/s]Loading trainS:  81%|████████  | 34/42 [00:19<00:04,  1.62it/s]Loading trainS:  83%|████████▎ | 35/42 [00:20<00:04,  1.75it/s]Loading trainS:  86%|████████▌ | 36/42 [00:20<00:03,  1.74it/s]Loading trainS:  88%|████████▊ | 37/42 [00:21<00:02,  1.72it/s]Loading trainS:  90%|█████████ | 38/42 [00:22<00:02,  1.66it/s]Loading trainS:  93%|█████████▎| 39/42 [00:22<00:01,  1.63it/s]Loading trainS:  95%|█████████▌| 40/42 [00:23<00:01,  1.71it/s]Loading trainS:  98%|█████████▊| 41/42 [00:23<00:00,  1.74it/s]Loading trainS: 100%|██████████| 42/42 [00:24<00:00,  1.80it/s]
Loading testS:   0%|          | 0/10 [00:00<?, ?it/s]Loading testS:  10%|█         | 1/10 [00:00<00:04,  1.85it/s]Loading testS:  20%|██        | 2/10 [00:01<00:04,  1.80it/s]Loading testS:  30%|███       | 3/10 [00:01<00:04,  1.71it/s]Loading testS:  40%|████      | 4/10 [00:02<00:03,  1.51it/s]Loading testS:  50%|█████     | 5/10 [00:03<00:03,  1.49it/s]Loading testS:  60%|██████    | 6/10 [00:03<00:02,  1.56it/s]Loading testS:  70%|███████   | 7/10 [00:04<00:01,  1.53it/s]Loading testS:  80%|████████  | 8/10 [00:05<00:01,  1.59it/s]Loading testS:  90%|█████████ | 9/10 [00:05<00:00,  1.64it/s]Loading testS: 100%|██████████| 10/10 [00:06<00:00,  1.67it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 128, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 128, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 128, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 128, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 128, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 128, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 128, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 64, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 64, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 64, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 64, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 64, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 64, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 64, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 64, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 64, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 32, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 32, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 32, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 32, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 32, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 32, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 32, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 32, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 32, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 32, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 64, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 64, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 64, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 64, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 64, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 64, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 64, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 64, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 64, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 58, 64, 140)  0           concatenate_4[0][0]              2019-07-29 00:40:34.183407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-29 00:40:34.183495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-29 00:40:34.183510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-29 00:40:34.183520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-29 00:40:34.183905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 128, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 128, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 128, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 128, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 128, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 128, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 128, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 128, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 128, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 128, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 128, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97769541 0.02230459]
Train on 2671 samples, validate on 717 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.9615 - acc: 0.9176 - mDice: 0.2048 - val_loss: 1.1769 - val_acc: 0.9367 - val_mDice: 0.1809

Epoch 00001: val_mDice improved from -inf to 0.18090, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.1601 - acc: 0.9876 - mDice: 0.7413 - val_loss: 0.6932 - val_acc: 0.9765 - val_mDice: 0.4518

Epoch 00002: val_mDice improved from 0.18090 to 0.45179, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.1002 - acc: 0.9913 - mDice: 0.8288 - val_loss: 0.5236 - val_acc: 0.9878 - val_mDice: 0.6279

Epoch 00003: val_mDice improved from 0.45179 to 0.62794, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0830 - acc: 0.9931 - mDice: 0.8571 - val_loss: 0.3616 - val_acc: 0.9927 - val_mDice: 0.7721

Epoch 00004: val_mDice improved from 0.62794 to 0.77208, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0766 - acc: 0.9938 - mDice: 0.8677 - val_loss: 0.3831 - val_acc: 0.9926 - val_mDice: 0.7789

Epoch 00005: val_mDice improved from 0.77208 to 0.77886, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0741 - acc: 0.9941 - mDice: 0.8717 - val_loss: 0.3345 - val_acc: 0.9934 - val_mDice: 0.8095

Epoch 00006: val_mDice improved from 0.77886 to 0.80952, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0755 - acc: 0.9941 - mDice: 0.8688 - val_loss: 0.4810 - val_acc: 0.9864 - val_mDice: 0.6847

Epoch 00007: val_mDice did not improve from 0.80952
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0710 - acc: 0.9944 - mDice: 0.8754 - val_loss: 0.2950 - val_acc: 0.9926 - val_mDice: 0.7855

Epoch 00008: val_mDice did not improve from 0.80952
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0678 - acc: 0.9947 - mDice: 0.8792 - val_loss: 0.3134 - val_acc: 0.9937 - val_mDice: 0.8138

Epoch 00009: val_mDice improved from 0.80952 to 0.81382, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0626 - acc: 0.9951 - mDice: 0.8866 - val_loss: 0.3200 - val_acc: 0.9944 - val_mDice: 0.8308

Epoch 00010: val_mDice improved from 0.81382 to 0.83076, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0622 - acc: 0.9951 - mDice: 0.8866 - val_loss: 0.4458 - val_acc: 0.9885 - val_mDice: 0.7242

Epoch 00011: val_mDice did not improve from 0.83076
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0600 - acc: 0.9952 - mDice: 0.8902 - val_loss: 0.3484 - val_acc: 0.9942 - val_mDice: 0.8271

Epoch 00012: val_mDice did not improve from 0.83076
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0586 - acc: 0.9953 - mDice: 0.8925 - val_loss: 0.3701 - val_acc: 0.9936 - val_mDice: 0.8172

Epoch 00013: val_mDice did not improve from 0.83076
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0585 - acc: 0.9953 - mDice: 0.8924 - val_loss: 0.3484 - val_acc: 0.9942 - val_mDice: 0.8275

Epoch 00014: val_mDice did not improve from 0.83076
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0573 - acc: 0.9954 - mDice: 0.8944 - val_loss: 0.3102 - val_acc: 0.9945 - val_mDice: 0.8355

Epoch 00015: val_mDice improved from 0.83076 to 0.83546, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0560 - acc: 0.9955 - mDice: 0.8967 - val_loss: 0.3045 - val_acc: 0.9947 - val_mDice: 0.8407

Epoch 00016: val_mDice improved from 0.83546 to 0.84069, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0553 - acc: 0.9955 - mDice: 0.8978 - val_loss: 0.3291 - val_acc: 0.9947 - val_mDice: 0.8411

Epoch 00017: val_mDice improved from 0.84069 to 0.84113, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 6s - loss: 0.0546 - acc: 0.9956 - mDice: 0.8992 - val_loss: 0.3768 - val_acc: 0.9930 - val_mDice: 0.8195

Epoch 00018: val_mDice did not improve from 0.84113
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0531 - acc: 0.9957 - mDice: 0.9016 - val_loss: 0.3082 - val_acc: 0.9946 - val_mDice: 0.8383

Epoch 00019: val_mDice did not improve from 0.84113
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0522 - acc: 0.9958 - mDice: 0.9033 - val_loss: 0.3637 - val_acc: 0.9938 - val_mDice: 0.8248

Epoch 00020: val_mDice did not improve from 0.84113
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0528 - acc: 0.9957 - mDice: 0.9023 - val_loss: 0.3637 - val_acc: 0.9937 - val_mDice: 0.8238

Epoch 00021: val_mDice did not improve from 0.84113
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0515 - acc: 0.9958 - mDice: 0.9045 - val_loss: 0.3314 - val_acc: 0.9944 - val_mDice: 0.8305

Epoch 00022: val_mDice did not improve from 0.84113
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0515 - acc: 0.9958 - mDice: 0.9045 - val_loss: 0.3714 - val_acc: 0.9933 - val_mDice: 0.8192

Epoch 00023: val_mDice did not improve from 0.84113
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0513 - acc: 0.9958 - mDice: 0.9048 - val_loss: 0.3654 - val_acc: 0.9937 - val_mDice: 0.8272

Epoch 00024: val_mDice did not improve from 0.84113
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0518 - acc: 0.9958 - mDice: 0.9039 - val_loss: 0.3055 - val_acc: 0.9949 - val_mDice: 0.8448

Epoch 00025: val_mDice improved from 0.84113 to 0.84484, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0527 - acc: 0.9957 - mDice: 0.9022 - val_loss: 0.3143 - val_acc: 0.9950 - val_mDice: 0.8467

Epoch 00026: val_mDice improved from 0.84484 to 0.84670, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0507 - acc: 0.9959 - mDice: 0.9058 - val_loss: 0.3239 - val_acc: 0.9951 - val_mDice: 0.8523

Epoch 00027: val_mDice improved from 0.84670 to 0.85234, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd2/best_model_weights.h5
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0511 - acc: 0.9958 - mDice: 0.9051 - val_loss: 0.3229 - val_acc: 0.9947 - val_mDice: 0.8401

Epoch 00028: val_mDice did not improve from 0.85234
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0501 - acc: 0.9959 - mDice: 0.9070 - val_loss: 0.3404 - val_acc: 0.9945 - val_mDice: 0.8355

Epoch 00029: val_mDice did not improve from 0.85234
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0488 - acc: 0.9960 - mDice: 0.9092 - val_loss: 0.3559 - val_acc: 0.9941 - val_mDice: 0.8341

Epoch 00030: val_mDice did not improve from 0.85234
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0485 - acc: 0.9960 - mDice: 0.9098 - val_loss: 0.3099 - val_acc: 0.9949 - val_mDice: 0.8450

Epoch 00031: val_mDice did not improve from 0.85234
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0490 - acc: 0.9960 - mDice: 0.9087 - val_loss: 0.4333 - val_acc: 0.9884 - val_mDice: 0.7474

Epoch 00032: val_mDice did not improve from 0.85234
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0479 - acc: 0.9961 - mDice: 0.9107 - val_loss: 0.3010 - val_acc: 0.9949 - val_mDice: 0.8462

Epoch 00033: val_mDice did not improve from 0.85234
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0474 - acc: 0.9961 - mDice: 0.9117 - val_loss: 0.3015 - val_acc: 0.9950 - val_mDice: 0.8472

Epoch 00034: val_mDice did not improve from 0.85234
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0476 - acc: 0.9961 - mDice: 0.9113 - val_loss: 0.3504 - val_acc: 0.9943 - val_mDice: 0.8335

Epoch 00035: val_mDice did not improve from 0.85234
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 6s - loss: 0.0515 - acc: 0.9958 - mDice: 0.9045 - val_loss: 0.2492 - val_acc: 0.9942 - val_mDice: 0.8215

Epoch 00036: val_mDice did not improve from 0.85234
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0495 - acc: 0.9960 - mDice: 0.9079 - val_loss: 0.3592 - val_acc: 0.9940 - val_mDice: 0.8352

Epoch 00037: val_mDice did not improve from 0.85234
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0480 - acc: 0.9961 - mDice: 0.9105 - val_loss: 0.2371 - val_acc: 0.9945 - val_mDice: 0.8319

Epoch 00038: val_mDice did not improve from 0.85234
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0466 - acc: 0.9962 - mDice: 0.9130 - val_loss: 0.2748 - val_acc: 0.9948 - val_mDice: 0.8413

Epoch 00039: val_mDice did not improve from 0.85234
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0469 - acc: 0.9962 - mDice: 0.9125 - val_loss: 0.2881 - val_acc: 0.9948 - val_mDice: 0.8407

Epoch 00040: val_mDice did not improve from 0.85234
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0474 - acc: 0.9961 - mDice: 0.9117 - val_loss: 0.2233 - val_acc: 0.9949 - val_mDice: 0.8438

Epoch 00041: val_mDice did not improve from 0.85234
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0467 - acc: 0.9962 - mDice: 0.9129 - val_loss: 0.2943 - val_acc: 0.9949 - val_mDice: 0.8453

Epoch 00042: val_mDice did not improve from 0.85234
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0468 - acc: 0.9962 - mDice: 0.9126 - val_loss: 0.2706 - val_acc: 0.9946 - val_mDice: 0.8360

Epoch 00043: val_mDice did not improve from 0.85234
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0465 - acc: 0.9962 - mDice: 0.9132 - val_loss: 0.2992 - val_acc: 0.9947 - val_mDice: 0.8353

Epoch 00044: val_mDice did not improve from 0.85234
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0464 - acc: 0.9962 - mDice: 0.9135 - val_loss: 0.2913 - val_acc: 0.9949 - val_mDice: 0.8431

Epoch 00045: val_mDice did not improve from 0.85234
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0460 - acc: 0.9962 - mDice: 0.9141 - val_loss: 0.3020 - val_acc: 0.9949 - val_mDice: 0.8440

Epoch 00046: val_mDice did not improve from 0.85234
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0454 - acc: 0.9963 - mDice: 0.9152 - val_loss: 0.3030 - val_acc: 0.9949 - val_mDice: 0.8430

Epoch 00047: val_mDice did not improve from 0.85234
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0462 - acc: 0.9962 - mDice: 0.9137 - val_loss: 0.2894 - val_acc: 0.9949 - val_mDice: 0.8430

Epoch 00048: val_mDice did not improve from 0.85234
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0457 - acc: 0.9962 - mDice: 0.9147 - val_loss: 0.2956 - val_acc: 0.9950 - val_mDice: 0.8490

Epoch 00049: val_mDice did not improve from 0.85234
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0452 - acc: 0.9963 - mDice: 0.9155 - val_loss: 0.2622 - val_acc: 0.9947 - val_mDice: 0.8378

Epoch 00050: val_mDice did not improve from 0.85234
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0455 - acc: 0.9963 - mDice: 0.9151 - val_loss: 0.2941 - val_acc: 0.9949 - val_mDice: 0.8424

Epoch 00051: val_mDice did not improve from 0.85234
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0454 - acc: 0.9963 - mDice: 0.9152 - val_loss: 0.3020 - val_acc: 0.9947 - val_mDice: 0.8369

Epoch 00052: val_mDice did not improve from 0.85234
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0442 - acc: 0.9963 - mDice: 0.9173 - val_loss: 0.3106 - val_acc: 0.9947 - val_mDice: 0.8352

Epoch 00053: val_mDice did not improve from 0.85234
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 6s - loss: 0.0447 - acc: 0.9963 - mDice: 0.9165 - val_loss: 0.2937 - val_acc: 0.9947 - val_mDice: 0.8369

Epoch 00054: val_mDice did not improve from 0.85234
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0441 - acc: 0.9964 - mDice: 0.9174 - val_loss: 0.2917 - val_acc: 0.9947 - val_mDice: 0.8344

Epoch 00055: val_mDice did not improve from 0.85234
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0440 - acc: 0.9964 - mDice: 0.9177 - val_loss: 0.3072 - val_acc: 0.9947 - val_mDice: 0.8362

Epoch 00056: val_mDice did not improve from 0.85234
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0446 - acc: 0.9963 - mDice: 0.9166 - val_loss: 0.2990 - val_acc: 0.9948 - val_mDice: 0.8398

Epoch 00057: val_mDice did not improve from 0.85234
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0437 - acc: 0.9964 - mDice: 0.9181 - val_loss: 0.2754 - val_acc: 0.9947 - val_mDice: 0.8367

Epoch 00058: val_mDice did not improve from 0.85234
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0443 - acc: 0.9964 - mDice: 0.9172 - val_loss: 0.2957 - val_acc: 0.9948 - val_mDice: 0.8413

Epoch 00059: val_mDice did not improve from 0.85234
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0444 - acc: 0.9964 - mDice: 0.9169 - val_loss: 0.2543 - val_acc: 0.9947 - val_mDice: 0.8368

Epoch 00060: val_mDice did not improve from 0.85234
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0442 - acc: 0.9964 - mDice: 0.9174 - val_loss: 0.2497 - val_acc: 0.9947 - val_mDice: 0.8375

Epoch 00061: val_mDice did not improve from 0.85234
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0437 - acc: 0.9964 - mDice: 0.9181 - val_loss: 0.2698 - val_acc: 0.9947 - val_mDice: 0.8367

Epoch 00062: val_mDice did not improve from 0.85234
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0436 - acc: 0.9964 - mDice: 0.9184 - val_loss: 0.2906 - val_acc: 0.9950 - val_mDice: 0.8456

Epoch 00063: val_mDice did not improve from 0.85234
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0441 - acc: 0.9964 - mDice: 0.9176 - val_loss: 0.2209 - val_acc: 0.9949 - val_mDice: 0.8422

Epoch 00064: val_mDice did not improve from 0.85234
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0438 - acc: 0.9964 - mDice: 0.9181 - val_loss: 0.2294 - val_acc: 0.9948 - val_mDice: 0.8389

Epoch 00065: val_mDice did not improve from 0.85234
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0436 - acc: 0.9964 - mDice: 0.9184 - val_loss: 0.2863 - val_acc: 0.9948 - val_mDice: 0.8380

Epoch 00066: val_mDice did not improve from 0.85234
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 6s - loss: 0.0441 - acc: 0.9964 - mDice: 0.9175 - val_loss: 0.2969 - val_acc: 0.9949 - val_mDice: 0.8425

Epoch 00067: val_mDice did not improve from 0.85234
Restoring model weights from the end of the best epoch
Epoch 00067: early stopping
{'val_loss': [1.1768600870874637, 0.6932286229426226, 0.5236190636453935, 0.3616104559502675, 0.3831480539909134, 0.3345349448079677, 0.4809568176043416, 0.2950396188632215, 0.3134142211125319, 0.3199748683258413, 0.44578882943802467, 0.3484329451161282, 0.3700816153316152, 0.3483914703213853, 0.3101670517582275, 0.3045253492632005, 0.32914017429744186, 0.37681375674622825, 0.3082318744483187, 0.36372038715803473, 0.3636686753278996, 0.33138097729144234, 0.3714300252320544, 0.3653646736007023, 0.3055443125082027, 0.31429655910203314, 0.32389798658579133, 0.3229298849734302, 0.3403665096274313, 0.3558763001254413, 0.30993339016181487, 0.4333497270879386, 0.3009601579966073, 0.30150849901066995, 0.3504048640342271, 0.24921268555160347, 0.3592017037141938, 0.23714640418605326, 0.2748182727701993, 0.2880517039422045, 0.22329190926239248, 0.2943342101540692, 0.27060320487082257, 0.2992320752584452, 0.2913339904079544, 0.30203164534089955, 0.30295483387663774, 0.28942363095083995, 0.29556309243708306, 0.2621806055681476, 0.29414116962185466, 0.30199683331877947, 0.31060378881096007, 0.293702047489677, 0.2916767258025612, 0.30722887315261316, 0.29904676861593416, 0.2753865193663947, 0.2956979408174379, 0.2542897316327155, 0.2496849026224437, 0.2698430003107342, 0.29062025781040246, 0.2209066644082342, 0.229441527126557, 0.2862923679616162, 0.2968820111522116], 'val_acc': [0.9366711477854262, 0.9765211707046009, 0.9877510162244448, 0.9927131477093929, 0.9925518738342296, 0.9934376443280336, 0.9864348237983352, 0.992626057509099, 0.9937204678687092, 0.9943524501812508, 0.9884565191621395, 0.9941809191530885, 0.9935595581887323, 0.9942256248811964, 0.9945317456911797, 0.9947302287426287, 0.9946856184484569, 0.993014658179077, 0.9946480421674135, 0.9937510026548697, 0.993739720085841, 0.9944191426246568, 0.9932606807147443, 0.9937014011990908, 0.9948763780015283, 0.9949530354769826, 0.9950960098117632, 0.9947254509772882, 0.9945267881974682, 0.9940931842749421, 0.9948834280588637, 0.988377904376558, 0.9949270155140546, 0.9949782156545248, 0.9943347819346929, 0.9942082490881118, 0.9940065064024559, 0.9945149428342343, 0.9948190045822282, 0.994823230443473, 0.9949115257050369, 0.9949405504237325, 0.9946463556968018, 0.9946556452095425, 0.994886913143275, 0.9948905747139637, 0.9948761197147343, 0.9948767665539825, 0.9950465928227998, 0.9947452644922743, 0.9948770473691709, 0.9947061997577237, 0.9946610119552293, 0.9947259242398303, 0.9946500213408902, 0.9946910659139625, 0.9947925211661697, 0.9947130463113346, 0.9948469005535503, 0.9947116298489325, 0.9947418810765946, 0.9947075897014623, 0.9949757797614966, 0.994886254000697, 0.9947768208704567, 0.9947612219109528, 0.9948826896928178], 'val_mDice': [0.18089737124855382, 0.4517897610218788, 0.6279447462768235, 0.7720836286265481, 0.7788561627455836, 0.8095240908852848, 0.6847253903185473, 0.7854701584851892, 0.8138194742561883, 0.8307598125984479, 0.7241879048207813, 0.8271253840527608, 0.8171632033844207, 0.8275314271865696, 0.835460740319522, 0.8406855737481324, 0.8411314031901552, 0.8195160239976487, 0.8382534881017198, 0.8248103795523757, 0.823770016639635, 0.8305197646594613, 0.8191671747210302, 0.8271782468053587, 0.8448377646495442, 0.8467014697140065, 0.8523436021605296, 0.8400950471726422, 0.8354735424329047, 0.8341008233558517, 0.84496889486805, 0.7474417383866995, 0.8461859435715935, 0.847192173057354, 0.8335114439827295, 0.8215169028757008, 0.8351668902520855, 0.8319281633927732, 0.8412710958611184, 0.8407332990292559, 0.8437670341262924, 0.8453357502340106, 0.8359514270367483, 0.8353148128530803, 0.8431463311406858, 0.8439890676628762, 0.8429603264089076, 0.8430362329655944, 0.8489687156810255, 0.8377589441243575, 0.8423581781746453, 0.836886388653658, 0.8352455234926615, 0.8368599697469501, 0.834412346990205, 0.8361732527468161, 0.8397860507087229, 0.8366826687896601, 0.8413386311657426, 0.8367747525623487, 0.8375023615077616, 0.8367179577320689, 0.8455506095992638, 0.84222457897713, 0.8388981908933887, 0.83803398173557, 0.8425251293714409], 'loss': [0.9614538727115487, 0.1601483013100144, 0.10015506556209487, 0.08303710888305527, 0.07663862046812502, 0.07410326058784927, 0.0754701752328596, 0.07103722298169841, 0.06779886965574686, 0.06256399830877893, 0.06219787503484928, 0.05996961379231964, 0.058571338910192464, 0.05851067699985708, 0.057332423843725125, 0.055982721999471706, 0.05532064507127879, 0.05456133131865302, 0.05314712736299537, 0.05220334501591839, 0.052753250031982724, 0.05150256736621746, 0.0514539907146308, 0.05132696946621923, 0.05180474973514883, 0.0527480201414242, 0.05071042332894478, 0.05111786448935452, 0.05005155509607661, 0.04877970908820428, 0.04845996793932747, 0.04903624917529398, 0.047917230765344675, 0.04739791207442298, 0.04758561402140865, 0.05145316158718761, 0.049505930854191006, 0.04804511146793558, 0.0466233347837218, 0.04691621719764351, 0.04737322349331433, 0.04671700338954026, 0.04683967612234674, 0.04653578150901666, 0.04635708565565362, 0.04597631767838468, 0.0453943519437081, 0.04623579377540708, 0.04566013389164167, 0.0452224508315626, 0.045465575427218616, 0.04536616940989962, 0.04420433466809825, 0.04467828645716442, 0.04414851624464239, 0.04400964864839645, 0.044628596344062424, 0.04374574258984898, 0.044253616868188794, 0.04441775502068682, 0.0441726059745265, 0.04373808971524953, 0.04357610090601895, 0.04407260939060459, 0.0437824178859452, 0.04360473833808021, 0.04411832777659842], 'acc': [0.9176408029013204, 0.9875919255412325, 0.9913145093291228, 0.9930674512258677, 0.9937608414533566, 0.9940777838073924, 0.9940800861176905, 0.9944223314133176, 0.9946934132261787, 0.9950640198444436, 0.9950649063714834, 0.9952006342035249, 0.9953085787731001, 0.9952912539946636, 0.9953699066414899, 0.9954935091347875, 0.9955361731397843, 0.9955957262804338, 0.9957047151251707, 0.9957581434701543, 0.9956911189285659, 0.9957878939983149, 0.9958155284388509, 0.9958160082885987, 0.9957839281359936, 0.9957098522154145, 0.9958589223385392, 0.995828865039487, 0.9959254579256644, 0.9959906680849062, 0.9960299047670397, 0.9959700929676715, 0.9960654365440649, 0.9961068343687753, 0.9960984121945323, 0.9958090225171168, 0.9959775914149711, 0.9960563316723685, 0.9961606197391097, 0.9961647282419737, 0.9961046888024028, 0.996177107798656, 0.9961508895207415, 0.9961778637578272, 0.9961761519820536, 0.9962054210760672, 0.9962588786989596, 0.996201146010467, 0.99623260965333, 0.9962633691981954, 0.9962615353344354, 0.9962635752816452, 0.9963402760604578, 0.9963132899292217, 0.996354997224997, 0.9963740930268042, 0.9963196495283592, 0.9963865457782581, 0.9963723694238267, 0.9963502112688174, 0.9963583029284989, 0.9963999118130111, 0.9964055365411051, 0.996378228442136, 0.9963938598104382, 0.9963822598227041, 0.9963659152740042], 'mDice': [0.20484784942291864, 0.7412701479641519, 0.8287978121199799, 0.8570752105923549, 0.8677002175476434, 0.8717238830297193, 0.8687675511377306, 0.8753871244689758, 0.8791716244728633, 0.8865936507628858, 0.8865675510471119, 0.8901709464550553, 0.8924582548152191, 0.8924329512487945, 0.8944253768647757, 0.8967351570909544, 0.8978382648264858, 0.8991676356318469, 0.9016019708875501, 0.9032534260465964, 0.9022502650129175, 0.9044651613392575, 0.9045361498463516, 0.9047636095168036, 0.9038932893169993, 0.9022495790127734, 0.9058118111093765, 0.9050920559243288, 0.9069745677859093, 0.9092207485171839, 0.9097823611987481, 0.9087443661484456, 0.9107360970166148, 0.911653146276042, 0.9113339643182043, 0.9044533179565923, 0.9079325128833438, 0.9105142754189132, 0.9130151975427125, 0.9125153507154472, 0.9116764281939497, 0.9128612658808803, 0.9126310585501131, 0.9131830463646861, 0.9134771142145169, 0.9141448044187788, 0.9152048850095214, 0.9136988914044151, 0.9147124704641602, 0.9154975824434629, 0.915064581723215, 0.9152469626976193, 0.9173308228428825, 0.9164550336993441, 0.9174211533361497, 0.9176704871748236, 0.9165545513637947, 0.9181222300634809, 0.9172388280165503, 0.9169189072378295, 0.9173836231633368, 0.9181482668496035, 0.9184370223434174, 0.917561698566881, 0.9180557779585632, 0.918374479374873, 0.9174701830547252], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125]}
predicting test subjects:   0%|          | 0/10 [00:00<?, ?it/s]predicting test subjects:  10%|█         | 1/10 [00:00<00:07,  1.23it/s]predicting test subjects:  20%|██        | 2/10 [00:01<00:05,  1.52it/s]predicting test subjects:  30%|███       | 3/10 [00:01<00:04,  1.72it/s]predicting test subjects:  40%|████      | 4/10 [00:02<00:03,  1.71it/s]predicting test subjects:  50%|█████     | 5/10 [00:02<00:02,  1.80it/s]predicting test subjects:  60%|██████    | 6/10 [00:02<00:02,  1.98it/s]predicting test subjects:  70%|███████   | 7/10 [00:03<00:01,  2.25it/s]predicting test subjects:  80%|████████  | 8/10 [00:03<00:00,  2.46it/s]predicting test subjects:  90%|█████████ | 9/10 [00:03<00:00,  2.54it/s]predicting test subjects: 100%|██████████| 10/10 [00:04<00:00,  2.58it/s]
predicting train subjects:   0%|          | 0/42 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/42 [00:00<00:11,  3.57it/s]predicting train subjects:   5%|▍         | 2/42 [00:00<00:11,  3.56it/s]predicting train subjects:   7%|▋         | 3/42 [00:00<00:10,  3.67it/s]predicting train subjects:  10%|▉         | 4/42 [00:01<00:10,  3.66it/s]predicting train subjects:  12%|█▏        | 5/42 [00:01<00:11,  3.26it/s]predicting train subjects:  14%|█▍        | 6/42 [00:01<00:10,  3.32it/s]predicting train subjects:  17%|█▋        | 7/42 [00:02<00:10,  3.28it/s]predicting train subjects:  19%|█▉        | 8/42 [00:02<00:09,  3.72it/s]predicting train subjects:  21%|██▏       | 9/42 [00:02<00:09,  3.53it/s]predicting train subjects:  24%|██▍       | 10/42 [00:02<00:09,  3.49it/s]predicting train subjects:  26%|██▌       | 11/42 [00:03<00:09,  3.25it/s]predicting train subjects:  29%|██▊       | 12/42 [00:03<00:08,  3.57it/s]predicting train subjects:  31%|███       | 13/42 [00:03<00:08,  3.49it/s]predicting train subjects:  33%|███▎      | 14/42 [00:04<00:07,  3.55it/s]predicting train subjects:  36%|███▌      | 15/42 [00:04<00:06,  3.94it/s]predicting train subjects:  38%|███▊      | 16/42 [00:04<00:06,  3.75it/s]predicting train subjects:  40%|████      | 17/42 [00:04<00:07,  3.42it/s]predicting train subjects:  43%|████▎     | 18/42 [00:05<00:06,  3.44it/s]predicting train subjects:  45%|████▌     | 19/42 [00:05<00:06,  3.56it/s]predicting train subjects:  48%|████▊     | 20/42 [00:05<00:06,  3.63it/s]predicting train subjects:  50%|█████     | 21/42 [00:05<00:05,  3.74it/s]predicting train subjects:  52%|█████▏    | 22/42 [00:06<00:05,  3.78it/s]predicting train subjects:  55%|█████▍    | 23/42 [00:06<00:04,  4.15it/s]predicting train subjects:  57%|█████▋    | 24/42 [00:06<00:04,  4.09it/s]predicting train subjects:  60%|█████▉    | 25/42 [00:06<00:04,  4.08it/s]predicting train subjects:  62%|██████▏   | 26/42 [00:07<00:03,  4.04it/s]predicting train subjects:  64%|██████▍   | 27/42 [00:07<00:03,  3.80it/s]predicting train subjects:  67%|██████▋   | 28/42 [00:07<00:03,  3.62it/s]predicting train subjects:  69%|██████▉   | 29/42 [00:07<00:03,  3.61it/s]predicting train subjects:  71%|███████▏  | 30/42 [00:08<00:03,  3.59it/s]predicting train subjects:  74%|███████▍  | 31/42 [00:08<00:03,  3.37it/s]predicting train subjects:  76%|███████▌  | 32/42 [00:08<00:02,  3.39it/s]predicting train subjects:  79%|███████▊  | 33/42 [00:09<00:02,  3.56it/s]predicting train subjects:  81%|████████  | 34/42 [00:09<00:02,  3.49it/s]predicting train subjects:  83%|████████▎ | 35/42 [00:09<00:01,  3.54it/s]predicting train subjects:  86%|████████▌ | 36/42 [00:09<00:01,  3.67it/s]predicting train subjects:  88%|████████▊ | 37/42 [00:10<00:01,  3.50it/s]predicting train subjects:  90%|█████████ | 38/42 [00:10<00:01,  3.41it/s]predicting train subjects:  93%|█████████▎| 39/42 [00:10<00:00,  3.34it/s]predicting train subjects:  95%|█████████▌| 40/42 [00:11<00:00,  3.02it/s]predicting train subjects:  98%|█████████▊| 41/42 [00:11<00:00,  3.17it/s]predicting train subjects: 100%|██████████| 42/42 [00:11<00:00,  3.07it/s]
predicting test subjects sagittal:   0%|          | 0/10 [00:00<?, ?it/s]predicting test subjects sagittal:  10%|█         | 1/10 [00:00<00:02,  3.19it/s]predicting test subjects sagittal:  20%|██        | 2/10 [00:00<00:02,  3.14it/s]predicting test subjects sagittal:  30%|███       | 3/10 [00:01<00:02,  2.91it/s]predicting test subjects sagittal:  40%|████      | 4/10 [00:01<00:02,  2.51it/s]predicting test subjects sagittal:  50%|█████     | 5/10 [00:02<00:02,  2.30it/s]predicting test subjects sagittal:  60%|██████    | 6/10 [00:02<00:01,  2.38it/s]predicting test subjects sagittal:  70%|███████   | 7/10 [00:02<00:01,  2.55it/s]predicting test subjects sagittal:  80%|████████  | 8/10 [00:03<00:00,  2.78it/s]predicting test subjects sagittal:  90%|█████████ | 9/10 [00:03<00:00,  2.81it/s]predicting test subjects sagittal: 100%|██████████| 10/10 [00:03<00:00,  2.89it/s]
predicting train subjects sagittal:   0%|          | 0/42 [00:00<?, ?it/s]predicting train subjects sagittal:   2%|▏         | 1/42 [00:00<00:12,  3.20it/s]predicting train subjects sagittal:   5%|▍         | 2/42 [00:00<00:12,  3.15it/s]predicting train subjects sagittal:   7%|▋         | 3/42 [00:00<00:12,  3.12it/s]predicting train subjects sagittal:  10%|▉         | 4/42 [00:01<00:10,  3.46it/s]predicting train subjects sagittal:  12%|█▏        | 5/42 [00:01<00:11,  3.19it/s]predicting train subjects sagittal:  14%|█▍        | 6/42 [00:01<00:11,  3.14it/s]predicting train subjects sagittal:  17%|█▋        | 7/42 [00:02<00:10,  3.20it/s]predicting train subjects sagittal:  19%|█▉        | 8/42 [00:02<00:09,  3.48it/s]predicting train subjects sagittal:  21%|██▏       | 9/42 [00:02<00:09,  3.55it/s]predicting train subjects sagittal:  24%|██▍       | 10/42 [00:03<00:09,  3.42it/s]predicting train subjects sagittal:  26%|██▌       | 11/42 [00:03<00:09,  3.28it/s]predicting train subjects sagittal:  29%|██▊       | 12/42 [00:03<00:09,  3.33it/s]predicting train subjects sagittal:  31%|███       | 13/42 [00:03<00:08,  3.45it/s]predicting train subjects sagittal:  33%|███▎      | 14/42 [00:04<00:07,  3.52it/s]predicting train subjects sagittal:  36%|███▌      | 15/42 [00:04<00:07,  3.67it/s]predicting train subjects sagittal:  38%|███▊      | 16/42 [00:04<00:07,  3.68it/s]predicting train subjects sagittal:  40%|████      | 17/42 [00:05<00:07,  3.38it/s]predicting train subjects sagittal:  43%|████▎     | 18/42 [00:05<00:07,  3.27it/s]predicting train subjects sagittal:  45%|████▌     | 19/42 [00:05<00:06,  3.38it/s]predicting train subjects sagittal:  48%|████▊     | 20/42 [00:05<00:06,  3.60it/s]predicting train subjects sagittal:  50%|█████     | 21/42 [00:06<00:06,  3.32it/s]predicting train subjects sagittal:  52%|█████▏    | 22/42 [00:06<00:06,  3.18it/s]predicting train subjects sagittal:  55%|█████▍    | 23/42 [00:06<00:05,  3.60it/s]predicting train subjects sagittal:  57%|█████▋    | 24/42 [00:07<00:05,  3.54it/s]predicting train subjects sagittal:  60%|█████▉    | 25/42 [00:07<00:04,  3.54it/s]predicting train subjects sagittal:  62%|██████▏   | 26/42 [00:07<00:04,  3.63it/s]predicting train subjects sagittal:  64%|██████▍   | 27/42 [00:07<00:04,  3.54it/s]predicting train subjects sagittal:  67%|██████▋   | 28/42 [00:08<00:04,  3.42it/s]predicting train subjects sagittal:  69%|██████▉   | 29/42 [00:08<00:04,  3.14it/s]predicting train subjects sagittal:  71%|███████▏  | 30/42 [00:08<00:03,  3.36it/s]predicting train subjects sagittal:  74%|███████▍  | 31/42 [00:09<00:03,  3.30it/s]predicting train subjects sagittal:  76%|███████▌  | 32/42 [00:09<00:03,  3.20it/s]predicting train subjects sagittal:  79%|███████▊  | 33/42 [00:09<00:02,  3.17it/s]predicting train subjects sagittal:  81%|████████  | 34/42 [00:10<00:02,  3.09it/s]predicting train subjects sagittal:  83%|████████▎ | 35/42 [00:10<00:02,  3.30it/s]predicting train subjects sagittal:  86%|████████▌ | 36/42 [00:10<00:01,  3.35it/s]predicting train subjects sagittal:  88%|████████▊ | 37/42 [00:10<00:01,  3.38it/s]predicting train subjects sagittal:  90%|█████████ | 38/42 [00:11<00:01,  3.05it/s]predicting train subjects sagittal:  93%|█████████▎| 39/42 [00:11<00:00,  3.25it/s]predicting train subjects sagittal:  95%|█████████▌| 40/42 [00:11<00:00,  3.35it/s]predicting train subjects sagittal:  98%|█████████▊| 41/42 [00:12<00:00,  3.34it/s]predicting train subjects sagittal: 100%|██████████| 42/42 [00:12<00:00,  3.16it/s]
Loading train:   0%|          | 0/42 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/42 [00:02<01:38,  2.40s/it]Loading train:   5%|▍         | 2/42 [00:04<01:33,  2.33s/it]Loading train:   7%|▋         | 3/42 [00:06<01:24,  2.17s/it]Loading train:  10%|▉         | 4/42 [00:08<01:18,  2.06s/it]Loading train:  12%|█▏        | 5/42 [00:10<01:23,  2.25s/it]Loading train:  14%|█▍        | 6/42 [00:12<01:18,  2.18s/it]Loading train:  17%|█▋        | 7/42 [00:14<01:13,  2.10s/it]Loading train:  19%|█▉        | 8/42 [00:16<01:07,  1.99s/it]Loading train:  21%|██▏       | 9/42 [00:18<01:07,  2.04s/it]Loading train:  24%|██▍       | 10/42 [00:20<01:07,  2.11s/it]Loading train:  26%|██▌       | 11/42 [00:23<01:11,  2.29s/it]Loading train:  29%|██▊       | 12/42 [00:25<01:04,  2.14s/it]Loading train:  31%|███       | 13/42 [00:27<01:01,  2.12s/it]Loading train:  33%|███▎      | 14/42 [00:29<00:58,  2.08s/it]Loading train:  36%|███▌      | 15/42 [00:31<00:55,  2.07s/it]Loading train:  38%|███▊      | 16/42 [00:33<00:53,  2.05s/it]Loading train:  40%|████      | 17/42 [00:36<00:55,  2.21s/it]Loading train:  43%|████▎     | 18/42 [00:38<00:51,  2.17s/it]Loading train:  45%|████▌     | 19/42 [00:40<00:49,  2.14s/it]Loading train:  48%|████▊     | 20/42 [00:42<00:44,  2.03s/it]Loading train:  50%|█████     | 21/42 [00:43<00:40,  1.92s/it]Loading train:  52%|█████▏    | 22/42 [00:45<00:37,  1.89s/it]Loading train:  55%|█████▍    | 23/42 [00:47<00:34,  1.83s/it]Loading train:  57%|█████▋    | 24/42 [00:49<00:34,  1.92s/it]Loading train:  60%|█████▉    | 25/42 [00:51<00:33,  1.98s/it]Loading train:  62%|██████▏   | 26/42 [00:53<00:30,  1.93s/it]Loading train:  64%|██████▍   | 27/42 [00:55<00:30,  2.01s/it]Loading train:  67%|██████▋   | 28/42 [00:58<00:30,  2.18s/it]Loading train:  69%|██████▉   | 29/42 [01:00<00:28,  2.16s/it]Loading train:  71%|███████▏  | 30/42 [01:01<00:24,  2.01s/it]Loading train:  74%|███████▍  | 31/42 [01:04<00:22,  2.09s/it]Loading train:  76%|███████▌  | 32/42 [01:06<00:21,  2.13s/it]Loading train:  79%|███████▊  | 33/42 [01:08<00:19,  2.12s/it]Loading train:  81%|████████  | 34/42 [01:10<00:17,  2.17s/it]Loading train:  83%|████████▎ | 35/42 [01:12<00:14,  2.13s/it]Loading train:  86%|████████▌ | 36/42 [01:15<00:13,  2.19s/it]Loading train:  88%|████████▊ | 37/42 [01:17<00:10,  2.12s/it]Loading train:  90%|█████████ | 38/42 [01:19<00:08,  2.17s/it]Loading train:  93%|█████████▎| 39/42 [01:21<00:06,  2.23s/it]Loading train:  95%|█████████▌| 40/42 [01:23<00:04,  2.22s/it]Loading train:  98%|█████████▊| 41/42 [01:26<00:02,  2.27s/it]Loading train: 100%|██████████| 42/42 [01:28<00:00,  2.31s/it]
concatenating: train:   0%|          | 0/42 [00:00<?, ?it/s]concatenating: train:  12%|█▏        | 5/42 [00:00<00:00, 49.73it/s]concatenating: train:  26%|██▌       | 11/42 [00:00<00:00, 52.24it/s]concatenating: train:  45%|████▌     | 19/42 [00:00<00:00, 49.14it/s]concatenating: train:  55%|█████▍    | 23/42 [00:00<00:00, 39.32it/s]concatenating: train:  64%|██████▍   | 27/42 [00:00<00:00, 37.43it/s]concatenating: train:  74%|███████▍  | 31/42 [00:00<00:00, 36.06it/s]concatenating: train:  83%|████████▎ | 35/42 [00:00<00:00, 30.27it/s]concatenating: train: 100%|██████████| 42/42 [00:01<00:00, 40.85it/s]
Loading test:   0%|          | 0/10 [00:00<?, ?it/s]Loading test:  10%|█         | 1/10 [00:02<00:20,  2.30s/it]Loading test:  20%|██        | 2/10 [00:04<00:17,  2.23s/it]Loading test:  30%|███       | 3/10 [00:06<00:16,  2.32s/it]Loading test:  40%|████      | 4/10 [00:10<00:15,  2.61s/it]Loading test:  50%|█████     | 5/10 [00:12<00:13,  2.67s/it]Loading test:  60%|██████    | 6/10 [00:15<00:10,  2.55s/it]Loading test:  70%|███████   | 7/10 [00:17<00:07,  2.45s/it]Loading test:  80%|████████  | 8/10 [00:19<00:04,  2.37s/it]Loading test:  90%|█████████ | 9/10 [00:22<00:02,  2.40s/it]Loading test: 100%|██████████| 10/10 [00:24<00:00,  2.43s/it]
concatenating: validation:   0%|          | 0/10 [00:00<?, ?it/s]concatenating: validation:  30%|███       | 3/10 [00:00<00:00, 28.48it/s]concatenating: validation:  70%|███████   | 7/10 [00:00<00:00, 29.02it/s]concatenating: validation: 100%|██████████| 10/10 [00:00<00:00, 34.51it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 116, 128, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 116, 128, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 116, 128, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 116, 128, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 116, 128, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 116, 128, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 116, 128, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 58, 64, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 58, 64, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 58, 64, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 58, 64, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 58, 64, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 58, 64, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 58, 64, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 58, 64, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 58, 64, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 29, 32, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 29, 32, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 29, 32, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 29, 32, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 29, 32, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 29, 32, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 32, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 29, 32, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 32, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 29, 32, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 58, 64, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 64, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 58, 64, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 58, 64, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 58, 64, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 58, 64, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 58, 64, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 58, 64, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 58, 64, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-29 00:50:33.742912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-29 00:50:33.743016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-29 00:50:33.743032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-29 00:50:33.743042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-29 00:50:33.743412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 58, 64, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 116, 128, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 116, 128, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 116, 128, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 116, 128, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 116, 128, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 116, 128, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 116, 128, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 116, 128, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 116, 128, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 116, 128, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 116, 128, 13) 793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.55786429e-02 3.00675825e-02 7.53931796e-02 1.00198558e-02
 2.70599362e-02 6.80009919e-03 7.86788849e-02 1.15057294e-01
 7.52383999e-02 1.34502705e-02 3.22327393e-01 1.80290860e-01
 3.76011356e-05]
Train on 2671 samples, validate on 717 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 16s - loss: 5.6022 - acc: 0.1644 - mDice: 0.0047 - val_loss: 5.9063 - val_acc: 0.4525 - val_mDice: 0.0054

Epoch 00001: val_mDice improved from -inf to 0.00543, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 4.2408 - acc: 0.8588 - mDice: 0.0366 - val_loss: 4.3332 - val_acc: 0.9397 - val_mDice: 0.0414

Epoch 00002: val_mDice improved from 0.00543 to 0.04140, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 2.7264 - acc: 0.9741 - mDice: 0.1197 - val_loss: 2.7399 - val_acc: 0.9767 - val_mDice: 0.1162

Epoch 00003: val_mDice improved from 0.04140 to 0.11623, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 1.8899 - acc: 0.9815 - mDice: 0.1871 - val_loss: 2.2081 - val_acc: 0.9851 - val_mDice: 0.1946

Epoch 00004: val_mDice improved from 0.11623 to 0.19457, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 1.3489 - acc: 0.9837 - mDice: 0.2666 - val_loss: 2.5518 - val_acc: 0.9853 - val_mDice: 0.2040

Epoch 00005: val_mDice improved from 0.19457 to 0.20401, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 1.0579 - acc: 0.9849 - mDice: 0.3369 - val_loss: 2.5348 - val_acc: 0.9848 - val_mDice: 0.1954

Epoch 00006: val_mDice did not improve from 0.20401
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.8832 - acc: 0.9857 - mDice: 0.3970 - val_loss: 2.2203 - val_acc: 0.9853 - val_mDice: 0.2397

Epoch 00007: val_mDice improved from 0.20401 to 0.23968, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.7894 - acc: 0.9861 - mDice: 0.4361 - val_loss: 2.2070 - val_acc: 0.9860 - val_mDice: 0.2654

Epoch 00008: val_mDice improved from 0.23968 to 0.26537, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.7115 - acc: 0.9866 - mDice: 0.4724 - val_loss: 1.5844 - val_acc: 0.9867 - val_mDice: 0.3422

Epoch 00009: val_mDice improved from 0.26537 to 0.34222, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.6608 - acc: 0.9872 - mDice: 0.4976 - val_loss: 1.4599 - val_acc: 0.9868 - val_mDice: 0.3525

Epoch 00010: val_mDice improved from 0.34222 to 0.35247, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.6037 - acc: 0.9874 - mDice: 0.5274 - val_loss: 1.3304 - val_acc: 0.9872 - val_mDice: 0.3764

Epoch 00011: val_mDice improved from 0.35247 to 0.37638, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.5711 - acc: 0.9878 - mDice: 0.5461 - val_loss: 1.0861 - val_acc: 0.9882 - val_mDice: 0.4325

Epoch 00012: val_mDice improved from 0.37638 to 0.43251, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.5507 - acc: 0.9881 - mDice: 0.5582 - val_loss: 1.0669 - val_acc: 0.9881 - val_mDice: 0.4473

Epoch 00013: val_mDice improved from 0.43251 to 0.44729, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 9s - loss: 0.5272 - acc: 0.9883 - mDice: 0.5714 - val_loss: 1.1525 - val_acc: 0.9881 - val_mDice: 0.4508

Epoch 00014: val_mDice improved from 0.44729 to 0.45084, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.4998 - acc: 0.9887 - mDice: 0.5871 - val_loss: 1.0096 - val_acc: 0.9882 - val_mDice: 0.4674

Epoch 00015: val_mDice improved from 0.45084 to 0.46737, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.4855 - acc: 0.9889 - mDice: 0.5961 - val_loss: 0.9137 - val_acc: 0.9883 - val_mDice: 0.4660

Epoch 00016: val_mDice did not improve from 0.46737
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.4614 - acc: 0.9893 - mDice: 0.6118 - val_loss: 0.9666 - val_acc: 0.9881 - val_mDice: 0.4694

Epoch 00017: val_mDice improved from 0.46737 to 0.46941, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 10s - loss: 0.4491 - acc: 0.9894 - mDice: 0.6195 - val_loss: 1.2306 - val_acc: 0.9885 - val_mDice: 0.4355

Epoch 00018: val_mDice did not improve from 0.46941
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4305 - acc: 0.9898 - mDice: 0.6318 - val_loss: 0.8867 - val_acc: 0.9888 - val_mDice: 0.4977

Epoch 00019: val_mDice improved from 0.46941 to 0.49773, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4168 - acc: 0.9899 - mDice: 0.6409 - val_loss: 0.9579 - val_acc: 0.9888 - val_mDice: 0.4880

Epoch 00020: val_mDice did not improve from 0.49773
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4088 - acc: 0.9900 - mDice: 0.6463 - val_loss: 0.8990 - val_acc: 0.9894 - val_mDice: 0.5128

Epoch 00021: val_mDice improved from 0.49773 to 0.51278, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.4039 - acc: 0.9902 - mDice: 0.6496 - val_loss: 0.9183 - val_acc: 0.9895 - val_mDice: 0.5157

Epoch 00022: val_mDice improved from 0.51278 to 0.51568, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.4017 - acc: 0.9902 - mDice: 0.6514 - val_loss: 0.7738 - val_acc: 0.9898 - val_mDice: 0.5384

Epoch 00023: val_mDice improved from 0.51568 to 0.53842, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 9s - loss: 0.3951 - acc: 0.9903 - mDice: 0.6556 - val_loss: 0.7877 - val_acc: 0.9903 - val_mDice: 0.5394

Epoch 00024: val_mDice improved from 0.53842 to 0.53935, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3936 - acc: 0.9905 - mDice: 0.6571 - val_loss: 0.7468 - val_acc: 0.9902 - val_mDice: 0.5491

Epoch 00025: val_mDice improved from 0.53935 to 0.54907, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3908 - acc: 0.9905 - mDice: 0.6591 - val_loss: 0.8028 - val_acc: 0.9900 - val_mDice: 0.5315

Epoch 00026: val_mDice did not improve from 0.54907
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3883 - acc: 0.9906 - mDice: 0.6610 - val_loss: 0.7919 - val_acc: 0.9899 - val_mDice: 0.5275

Epoch 00027: val_mDice did not improve from 0.54907
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3797 - acc: 0.9907 - mDice: 0.6666 - val_loss: 0.8784 - val_acc: 0.9902 - val_mDice: 0.5210

Epoch 00028: val_mDice did not improve from 0.54907
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3761 - acc: 0.9909 - mDice: 0.6694 - val_loss: 0.8186 - val_acc: 0.9903 - val_mDice: 0.5305

Epoch 00029: val_mDice did not improve from 0.54907
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3764 - acc: 0.9909 - mDice: 0.6693 - val_loss: 0.8044 - val_acc: 0.9905 - val_mDice: 0.5427

Epoch 00030: val_mDice did not improve from 0.54907
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3686 - acc: 0.9910 - mDice: 0.6746 - val_loss: 0.8992 - val_acc: 0.9900 - val_mDice: 0.5245

Epoch 00031: val_mDice did not improve from 0.54907
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3702 - acc: 0.9911 - mDice: 0.6739 - val_loss: 0.7417 - val_acc: 0.9907 - val_mDice: 0.5569

Epoch 00032: val_mDice improved from 0.54907 to 0.55688, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3631 - acc: 0.9912 - mDice: 0.6786 - val_loss: 0.7443 - val_acc: 0.9911 - val_mDice: 0.5701

Epoch 00033: val_mDice improved from 0.55688 to 0.57006, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3641 - acc: 0.9913 - mDice: 0.6785 - val_loss: 0.8351 - val_acc: 0.9907 - val_mDice: 0.5570

Epoch 00034: val_mDice did not improve from 0.57006
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3556 - acc: 0.9914 - mDice: 0.6838 - val_loss: 0.8483 - val_acc: 0.9908 - val_mDice: 0.5470

Epoch 00035: val_mDice did not improve from 0.57006
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3526 - acc: 0.9915 - mDice: 0.6863 - val_loss: 0.8137 - val_acc: 0.9909 - val_mDice: 0.5447

Epoch 00036: val_mDice did not improve from 0.57006
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3498 - acc: 0.9915 - mDice: 0.6883 - val_loss: 0.8020 - val_acc: 0.9910 - val_mDice: 0.5632

Epoch 00037: val_mDice did not improve from 0.57006
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3445 - acc: 0.9916 - mDice: 0.6921 - val_loss: 0.8536 - val_acc: 0.9912 - val_mDice: 0.5642

Epoch 00038: val_mDice did not improve from 0.57006
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3410 - acc: 0.9917 - mDice: 0.6945 - val_loss: 0.7939 - val_acc: 0.9914 - val_mDice: 0.5800

Epoch 00039: val_mDice improved from 0.57006 to 0.57997, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3410 - acc: 0.9917 - mDice: 0.6945 - val_loss: 0.8033 - val_acc: 0.9914 - val_mDice: 0.5785

Epoch 00040: val_mDice did not improve from 0.57997
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3383 - acc: 0.9917 - mDice: 0.6967 - val_loss: 0.7905 - val_acc: 0.9913 - val_mDice: 0.5734

Epoch 00041: val_mDice did not improve from 0.57997
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3362 - acc: 0.9918 - mDice: 0.6980 - val_loss: 0.7869 - val_acc: 0.9914 - val_mDice: 0.5742

Epoch 00042: val_mDice did not improve from 0.57997
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3374 - acc: 0.9918 - mDice: 0.6973 - val_loss: 0.7818 - val_acc: 0.9913 - val_mDice: 0.5668

Epoch 00043: val_mDice did not improve from 0.57997
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3359 - acc: 0.9918 - mDice: 0.6985 - val_loss: 0.8456 - val_acc: 0.9912 - val_mDice: 0.5673

Epoch 00044: val_mDice did not improve from 0.57997
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3322 - acc: 0.9919 - mDice: 0.7013 - val_loss: 0.7514 - val_acc: 0.9914 - val_mDice: 0.5752

Epoch 00045: val_mDice did not improve from 0.57997
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3294 - acc: 0.9920 - mDice: 0.7031 - val_loss: 0.8033 - val_acc: 0.9915 - val_mDice: 0.5724

Epoch 00046: val_mDice did not improve from 0.57997
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3317 - acc: 0.9920 - mDice: 0.7015 - val_loss: 0.7508 - val_acc: 0.9917 - val_mDice: 0.5880

Epoch 00047: val_mDice improved from 0.57997 to 0.58798, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3287 - acc: 0.9920 - mDice: 0.7038 - val_loss: 0.7615 - val_acc: 0.9916 - val_mDice: 0.5822

Epoch 00048: val_mDice did not improve from 0.58798
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3288 - acc: 0.9920 - mDice: 0.7036 - val_loss: 0.7688 - val_acc: 0.9916 - val_mDice: 0.5773

Epoch 00049: val_mDice did not improve from 0.58798
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3291 - acc: 0.9921 - mDice: 0.7036 - val_loss: 0.7389 - val_acc: 0.9918 - val_mDice: 0.5904

Epoch 00050: val_mDice improved from 0.58798 to 0.59036, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3279 - acc: 0.9921 - mDice: 0.7043 - val_loss: 0.8026 - val_acc: 0.9918 - val_mDice: 0.5846

Epoch 00051: val_mDice did not improve from 0.59036
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3282 - acc: 0.9921 - mDice: 0.7041 - val_loss: 0.7262 - val_acc: 0.9918 - val_mDice: 0.5876

Epoch 00052: val_mDice did not improve from 0.59036
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3294 - acc: 0.9921 - mDice: 0.7032 - val_loss: 0.7632 - val_acc: 0.9918 - val_mDice: 0.5840

Epoch 00053: val_mDice did not improve from 0.59036
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 9s - loss: 0.3228 - acc: 0.9922 - mDice: 0.7083 - val_loss: 0.7371 - val_acc: 0.9919 - val_mDice: 0.5952

Epoch 00054: val_mDice improved from 0.59036 to 0.59521, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3208 - acc: 0.9922 - mDice: 0.7096 - val_loss: 0.7793 - val_acc: 0.9918 - val_mDice: 0.5861

Epoch 00055: val_mDice did not improve from 0.59521
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3203 - acc: 0.9923 - mDice: 0.7099 - val_loss: 0.8134 - val_acc: 0.9918 - val_mDice: 0.5800

Epoch 00056: val_mDice did not improve from 0.59521
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3159 - acc: 0.9923 - mDice: 0.7131 - val_loss: 0.7395 - val_acc: 0.9919 - val_mDice: 0.5854

Epoch 00057: val_mDice did not improve from 0.59521
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3156 - acc: 0.9923 - mDice: 0.7133 - val_loss: 0.7540 - val_acc: 0.9919 - val_mDice: 0.5877

Epoch 00058: val_mDice did not improve from 0.59521
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3183 - acc: 0.9923 - mDice: 0.7115 - val_loss: 0.7414 - val_acc: 0.9920 - val_mDice: 0.5874

Epoch 00059: val_mDice did not improve from 0.59521
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3166 - acc: 0.9923 - mDice: 0.7126 - val_loss: 0.7222 - val_acc: 0.9920 - val_mDice: 0.5888

Epoch 00060: val_mDice did not improve from 0.59521
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3177 - acc: 0.9923 - mDice: 0.7119 - val_loss: 0.7337 - val_acc: 0.9920 - val_mDice: 0.5902

Epoch 00061: val_mDice did not improve from 0.59521
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3149 - acc: 0.9923 - mDice: 0.7140 - val_loss: 0.7350 - val_acc: 0.9919 - val_mDice: 0.5878

Epoch 00062: val_mDice did not improve from 0.59521
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3137 - acc: 0.9924 - mDice: 0.7148 - val_loss: 0.7458 - val_acc: 0.9920 - val_mDice: 0.5873

Epoch 00063: val_mDice did not improve from 0.59521
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3126 - acc: 0.9924 - mDice: 0.7157 - val_loss: 0.7696 - val_acc: 0.9919 - val_mDice: 0.5803

Epoch 00064: val_mDice did not improve from 0.59521
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3125 - acc: 0.9924 - mDice: 0.7159 - val_loss: 0.7484 - val_acc: 0.9919 - val_mDice: 0.5839

Epoch 00065: val_mDice did not improve from 0.59521
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3145 - acc: 0.9924 - mDice: 0.7142 - val_loss: 0.7639 - val_acc: 0.9919 - val_mDice: 0.5839

Epoch 00066: val_mDice did not improve from 0.59521
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3133 - acc: 0.9924 - mDice: 0.7153 - val_loss: 0.7499 - val_acc: 0.9919 - val_mDice: 0.5825

Epoch 00067: val_mDice did not improve from 0.59521
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3090 - acc: 0.9924 - mDice: 0.7184 - val_loss: 0.7616 - val_acc: 0.9919 - val_mDice: 0.5855

Epoch 00068: val_mDice did not improve from 0.59521
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3104 - acc: 0.9924 - mDice: 0.7174 - val_loss: 0.7706 - val_acc: 0.9919 - val_mDice: 0.5817

Epoch 00069: val_mDice did not improve from 0.59521
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 9s - loss: 0.3115 - acc: 0.9924 - mDice: 0.7166 - val_loss: 0.7783 - val_acc: 0.9918 - val_mDice: 0.5801

Epoch 00070: val_mDice did not improve from 0.59521
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3112 - acc: 0.9924 - mDice: 0.7168 - val_loss: 0.7520 - val_acc: 0.9918 - val_mDice: 0.5776

Epoch 00071: val_mDice did not improve from 0.59521
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3078 - acc: 0.9925 - mDice: 0.7193 - val_loss: 0.7430 - val_acc: 0.9919 - val_mDice: 0.5820

Epoch 00072: val_mDice did not improve from 0.59521
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3075 - acc: 0.9925 - mDice: 0.7198 - val_loss: 0.7380 - val_acc: 0.9919 - val_mDice: 0.5826

Epoch 00073: val_mDice did not improve from 0.59521
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3058 - acc: 0.9925 - mDice: 0.7208 - val_loss: 0.7191 - val_acc: 0.9920 - val_mDice: 0.5881

Epoch 00074: val_mDice did not improve from 0.59521
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3061 - acc: 0.9926 - mDice: 0.7208 - val_loss: 0.7110 - val_acc: 0.9921 - val_mDice: 0.5906

Epoch 00075: val_mDice did not improve from 0.59521
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3063 - acc: 0.9925 - mDice: 0.7206 - val_loss: 0.7432 - val_acc: 0.9920 - val_mDice: 0.5839

Epoch 00076: val_mDice did not improve from 0.59521
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 9s - loss: 0.3060 - acc: 0.9925 - mDice: 0.7207 - val_loss: 0.7127 - val_acc: 0.9920 - val_mDice: 0.5884

Epoch 00077: val_mDice did not improve from 0.59521
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3073 - acc: 0.9925 - mDice: 0.7199 - val_loss: 0.7135 - val_acc: 0.9920 - val_mDice: 0.5911

Epoch 00078: val_mDice did not improve from 0.59521
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3064 - acc: 0.9925 - mDice: 0.7203 - val_loss: 0.7206 - val_acc: 0.9921 - val_mDice: 0.5899

Epoch 00079: val_mDice did not improve from 0.59521
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3037 - acc: 0.9926 - mDice: 0.7226 - val_loss: 0.7416 - val_acc: 0.9920 - val_mDice: 0.5848

Epoch 00080: val_mDice did not improve from 0.59521
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3055 - acc: 0.9926 - mDice: 0.7212 - val_loss: 0.7505 - val_acc: 0.9920 - val_mDice: 0.5851

Epoch 00081: val_mDice did not improve from 0.59521
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3048 - acc: 0.9926 - mDice: 0.7216 - val_loss: 0.7272 - val_acc: 0.9921 - val_mDice: 0.5889

Epoch 00082: val_mDice did not improve from 0.59521
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3054 - acc: 0.9926 - mDice: 0.7213 - val_loss: 0.7305 - val_acc: 0.9921 - val_mDice: 0.5853

Epoch 00083: val_mDice did not improve from 0.59521
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3010 - acc: 0.9926 - mDice: 0.7245 - val_loss: 0.7199 - val_acc: 0.9921 - val_mDice: 0.5871

Epoch 00084: val_mDice did not improve from 0.59521
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3024 - acc: 0.9926 - mDice: 0.7234 - val_loss: 0.7331 - val_acc: 0.9921 - val_mDice: 0.5844

Epoch 00085: val_mDice did not improve from 0.59521
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3017 - acc: 0.9926 - mDice: 0.7240 - val_loss: 0.7292 - val_acc: 0.9921 - val_mDice: 0.5885

Epoch 00086: val_mDice did not improve from 0.59521
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3006 - acc: 0.9926 - mDice: 0.7250 - val_loss: 0.7461 - val_acc: 0.9921 - val_mDice: 0.5838

Epoch 00087: val_mDice did not improve from 0.59521
Epoch 88/300

Epoch 00088: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.3031 - acc: 0.9926 - mDice: 0.7232 - val_loss: 0.7219 - val_acc: 0.9921 - val_mDice: 0.5891

Epoch 00088: val_mDice did not improve from 0.59521
Epoch 89/300

Epoch 00089: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3033 - acc: 0.9926 - mDice: 0.7228 - val_loss: 0.7234 - val_acc: 0.9921 - val_mDice: 0.5874

Epoch 00089: val_mDice did not improve from 0.59521
Epoch 90/300

Epoch 00090: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.3038 - acc: 0.9926 - mDice: 0.7225 - val_loss: 0.7255 - val_acc: 0.9921 - val_mDice: 0.5892

Epoch 00090: val_mDice did not improve from 0.59521
Epoch 91/300

Epoch 00091: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3031 - acc: 0.9926 - mDice: 0.7230 - val_loss: 0.7250 - val_acc: 0.9921 - val_mDice: 0.5879

Epoch 00091: val_mDice did not improve from 0.59521
Epoch 92/300

Epoch 00092: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3027 - acc: 0.9926 - mDice: 0.7235 - val_loss: 0.7171 - val_acc: 0.9921 - val_mDice: 0.5899

Epoch 00092: val_mDice did not improve from 0.59521
Epoch 93/300

Epoch 00093: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.3016 - acc: 0.9927 - mDice: 0.7243 - val_loss: 0.7091 - val_acc: 0.9921 - val_mDice: 0.5898

Epoch 00093: val_mDice did not improve from 0.59521
Epoch 94/300

Epoch 00094: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2994 - acc: 0.9927 - mDice: 0.7257 - val_loss: 0.7169 - val_acc: 0.9921 - val_mDice: 0.5888

Epoch 00094: val_mDice did not improve from 0.59521
Restoring model weights from the end of the best epoch
Epoch 00094: early stopping
{'val_loss': [5.906258895639928, 4.333210914537331, 2.7399086253912377, 2.2080671930246605, 2.5518260288105514, 2.5347826697005056, 2.2203230003266485, 2.2069693100502303, 1.584421047771991, 1.4599335093877306, 1.3303577268140252, 1.0861273410430015, 1.0668906433978007, 1.1525358339067615, 1.0095788870562736, 0.9137006527542901, 0.9666405858022564, 1.2306350447975276, 0.8866621852752388, 0.9578523033998667, 0.8990251160066995, 0.9183210598375009, 0.7737878975010317, 0.7876663322728049, 0.7467742267180354, 0.802822324521372, 0.7918520923106408, 0.8784040447059536, 0.8186383779411369, 0.804385861758574, 0.8992368533188995, 0.7416648794916384, 0.7443133043943588, 0.8351445864766546, 0.8482831758102779, 0.8137215873021295, 0.8019735764925118, 0.85359905703796, 0.7938980732004014, 0.8032542975876621, 0.7905124969562227, 0.78694483193061, 0.7818149261727326, 0.8455816630372776, 0.751443737886607, 0.8033350222280334, 0.7508156878180084, 0.7614878020359716, 0.7687728672679331, 0.7388696108569327, 0.8026072242436216, 0.7262256753993334, 0.763246351206153, 0.7370536485310213, 0.7793438246725659, 0.8134171202924295, 0.7394790200508788, 0.753972738025245, 0.7414438919043441, 0.7221714421129958, 0.7336980323911212, 0.7350007250717993, 0.7457917102377452, 0.7695549219389507, 0.7484314137422888, 0.7639458442166593, 0.7498661685355704, 0.7616447642591043, 0.7706176390042697, 0.7783133318567209, 0.7520380997757533, 0.7430229112194173, 0.7379937142009017, 0.7190642642841845, 0.7110361201660071, 0.7432043953088204, 0.712666578372652, 0.7135386678131721, 0.7205905638313027, 0.7416492663999316, 0.7505490924523964, 0.727220899224115, 0.7305027665453476, 0.7198772427094697, 0.7331412516877243, 0.7291631615477792, 0.7460953766997057, 0.7218837171086374, 0.7233986694942458, 0.7254818691203119, 0.7250195212942786, 0.7170854530906411, 0.7090862410836639, 0.7169289667070993], 'val_acc': [0.4525241151846935, 0.9396538628860165, 0.976667052449209, 0.9851056887681182, 0.985332178105204, 0.9847637830585283, 0.9853304739277732, 0.9860423795016408, 0.9867385068864171, 0.9868361105313693, 0.9871767049387742, 0.988173989571288, 0.9880607074939722, 0.9880893543009313, 0.9881512450373822, 0.9882797483287263, 0.9880927294035503, 0.9885390872237074, 0.9888041777590827, 0.9888176947956804, 0.9893620340062651, 0.9894577505864525, 0.9898259692943578, 0.9902603849041246, 0.9902096669211714, 0.9900441621969268, 0.989942252386564, 0.9901690908721966, 0.9902582318214193, 0.9905403272211136, 0.9899949322518278, 0.9907180355183749, 0.9910616309406701, 0.990701401084038, 0.9908037930519179, 0.9909063679071317, 0.9910166323434358, 0.9911650537280691, 0.9913906701937879, 0.9913815673923891, 0.991305303374095, 0.9913579749262982, 0.9913135665563693, 0.9912359724483729, 0.9913996815515196, 0.9914740003136245, 0.9917153035247676, 0.9916412923459063, 0.9915763839684437, 0.9918069882705455, 0.9918144035206347, 0.991809615197707, 0.9917937455649489, 0.9918885144562236, 0.9918176954926473, 0.9917706269433143, 0.9918738751564399, 0.9919423415238555, 0.9919562409612425, 0.9919686440477146, 0.9919619686600221, 0.9919402632586959, 0.991995603303364, 0.991858088654288, 0.9919212928543197, 0.9918597762055975, 0.991941111190881, 0.9919432725866469, 0.9918752717506271, 0.9918024825916796, 0.9917837782052438, 0.9918870014791874, 0.9919281511293462, 0.9920048058614904, 0.992070080013621, 0.9919579368256126, 0.9920166353467784, 0.992038423878711, 0.9920960084497513, 0.9920170011214465, 0.9919992527369841, 0.9920795767709634, 0.992053171248283, 0.9920828655008824, 0.9920696061691646, 0.9920740287174241, 0.992051217679033, 0.9921267168957485, 0.9920654745780274, 0.9921176972249561, 0.9921238156375859, 0.9921285042037857, 0.9921460761851014, 0.9921317994178921], 'val_mDice': [0.00542863518924477, 0.04140194037843449, 0.1162334590776196, 0.19457186241030194, 0.20401331711679987, 0.1954114477629442, 0.23968376458273108, 0.2653654191949544, 0.34221844485281566, 0.35246942748251986, 0.37637767557319735, 0.4325131221961443, 0.4472905414041282, 0.4508426311624599, 0.46737058974043927, 0.4659703003977798, 0.4694149812561365, 0.43545119890441786, 0.49772740989881914, 0.4880320285985992, 0.5127759782839021, 0.5156830223368135, 0.538420265688557, 0.5393548158089653, 0.5490712790309634, 0.531528759202199, 0.5275193451172471, 0.5209624185389222, 0.5305277503185193, 0.5427368182683756, 0.5245324665864973, 0.5568819065971853, 0.570064034920856, 0.5570371446915418, 0.5469739561799181, 0.5447309552542502, 0.5631885851111206, 0.5641955544592779, 0.5799703601347685, 0.5785287008458434, 0.5733813617685017, 0.574206665802534, 0.5667976588218614, 0.5672776840388193, 0.575165365364096, 0.5724265568758488, 0.587982097263948, 0.5822461685542448, 0.5773155800634514, 0.5903551934320724, 0.5846326820022392, 0.587572325722443, 0.5840346570460534, 0.595209704948436, 0.5861178601636049, 0.5800058153383901, 0.5853747265774502, 0.5876744142636096, 0.5874279403287497, 0.5887832934221296, 0.5902183147321353, 0.587820923644296, 0.5872567744102105, 0.5802730552987241, 0.5839261383524832, 0.5839213874383286, 0.5824708788963542, 0.5855064594928523, 0.5817318769013367, 0.5800900848340789, 0.5776037149016994, 0.5820106969551395, 0.5826214616767532, 0.5880545663700609, 0.5905685813855925, 0.5838817715478575, 0.5884266548076933, 0.59105290029837, 0.5899022646362645, 0.5848164422076451, 0.5851235557633298, 0.5889099221183, 0.5852600395928865, 0.5871133159227784, 0.5843807962649038, 0.588468768772886, 0.5838497662644008, 0.5890895590456293, 0.5874240664424922, 0.5892019807211691, 0.5878963688261173, 0.5898844580603776, 0.5898380129905925, 0.5888430102932237], 'loss': [5.602232343915785, 4.240846065937558, 2.726353401461151, 1.8898764301979867, 1.3489356618925499, 1.0579258013824897, 0.8832439504415969, 0.7893938943715811, 0.7115055955067774, 0.6607734340043249, 0.6037274389532968, 0.5711141531496894, 0.5507214683488441, 0.5272479908870695, 0.49983781000417254, 0.48545013591842406, 0.46136637710980466, 0.44914159675253235, 0.4305428610964689, 0.4167536322943774, 0.4088450775459794, 0.4039276671271251, 0.401710182684988, 0.3951486914022189, 0.39362720731758793, 0.39084514539280174, 0.3883389247039515, 0.379718024424514, 0.3760811524282275, 0.37643315685192597, 0.36862912863347913, 0.37024956720681373, 0.3630611278599275, 0.36411366127752176, 0.35557626669257825, 0.35255985009406937, 0.34982268009592826, 0.3444699126876994, 0.3409878942837924, 0.34095940111014605, 0.33828437940898526, 0.3362087709142313, 0.3374067259968465, 0.3358559639254834, 0.33216810022253795, 0.32935582923112583, 0.33172515609299186, 0.32865651812681945, 0.32881346503387715, 0.32907298594456585, 0.3278508061596804, 0.32823092415956734, 0.329440128073537, 0.3227620749427156, 0.32079645347523805, 0.3203324139988498, 0.31589244905815994, 0.315588315052738, 0.3183112958985203, 0.3166393605969185, 0.3176930503580107, 0.3149353260071882, 0.31372206834138266, 0.3125887456256234, 0.31246542628228285, 0.31450743319969504, 0.31331123743598177, 0.30897473213334337, 0.31040894914905215, 0.3114824990840585, 0.3111879152110881, 0.3078346709239264, 0.3074920880620464, 0.30575721742998835, 0.3060947263097995, 0.3062977625697974, 0.3060472701560777, 0.3073338907517462, 0.30641645880416735, 0.3037059172647627, 0.3055282258130512, 0.30478425872366066, 0.30539500870672515, 0.3009668060542492, 0.3024356237782472, 0.3017384842824775, 0.30057758237930865, 0.3030519220254163, 0.3032963326119252, 0.30381280132466515, 0.30309194207012946, 0.3027432318463035, 0.30155832319614906, 0.2993651056102223], 'acc': [0.16440252526807947, 0.8588345734334893, 0.9741431529135438, 0.9815182684959431, 0.9836865860350968, 0.9848698400817352, 0.9856918028390885, 0.9860925624451803, 0.9866328485803808, 0.9871502815672897, 0.987426486612482, 0.9877833505330216, 0.9880794538669665, 0.988320356203527, 0.9887108649041759, 0.9888702538833776, 0.9892748192382278, 0.9893725265817489, 0.9897525470295188, 0.9899241044609701, 0.9900002033466795, 0.9901520255932152, 0.990221182222627, 0.9903039682987075, 0.990514406056406, 0.9904914418439748, 0.9906044052094388, 0.9907368840192954, 0.9909058711761751, 0.9909144723651919, 0.9910448297540243, 0.9910661181186032, 0.9912070871649142, 0.9912616851534213, 0.9913841988075454, 0.9914541755084999, 0.9914968380183595, 0.9916396835509956, 0.9917325447981209, 0.9917235467283076, 0.9917262187837795, 0.9918106585484415, 0.9917941250322643, 0.9918456877362456, 0.9919345174912253, 0.9919678692914091, 0.991957595601862, 0.9919930706083083, 0.9920137154614154, 0.9920659704361905, 0.9920559311038053, 0.9920699998306809, 0.9921082489412851, 0.9921805890300041, 0.9922196240471526, 0.9922678353901616, 0.9922881860359918, 0.9923118378106506, 0.9922756243849997, 0.9922984214544921, 0.9923247949365639, 0.9923418853132076, 0.9923568178276496, 0.9924070678682017, 0.9924175162656349, 0.9923781763297794, 0.9924036169801924, 0.9924375524640485, 0.9924289300083802, 0.992420890299134, 0.9924191273762821, 0.9925104320517911, 0.9924967112901817, 0.992507452131969, 0.9925579833921891, 0.9925178949059016, 0.992524350126867, 0.9925283534568778, 0.9925147388053717, 0.9925678176230056, 0.9925849340864443, 0.9925617209671589, 0.9925795402119821, 0.9926163041828189, 0.9926349092002934, 0.992597746429529, 0.9926161829874517, 0.9926141372382172, 0.9925818642575569, 0.9926299706954984, 0.9926236917445027, 0.9926193579668768, 0.9926509986384359, 0.9926637573127932], 'mDice': [0.004672931462947526, 0.03661280531102867, 0.11967705073025467, 0.1871253660458476, 0.2665732516996069, 0.33685677025211924, 0.3970036125058139, 0.4360934242645215, 0.47241217368821087, 0.49755304734923783, 0.5274279975837556, 0.5461348772316736, 0.5581506019627633, 0.5714051656756942, 0.5870735042999408, 0.5960768466136822, 0.6118194238160354, 0.6195223425399583, 0.6317672832366547, 0.6409216025758441, 0.6463431127506086, 0.6495951596648896, 0.6514168222581612, 0.6556021200499255, 0.6571010986725876, 0.659109363659452, 0.661027040677051, 0.6665992744849979, 0.6694348238461565, 0.6693291769675751, 0.6746201843235298, 0.6739086338260655, 0.6786415888682058, 0.6784903760350898, 0.6838456912462026, 0.6862816434815241, 0.6883048793431392, 0.6921006756657208, 0.6945118074424255, 0.694513987374636, 0.696716512401021, 0.6980212150955415, 0.697266694625546, 0.6985312798906024, 0.7012927111102953, 0.7030894010310671, 0.7015183935090425, 0.7037844300403974, 0.7035728297354977, 0.7035916762064567, 0.7042899642353065, 0.7041238892484541, 0.7032251305814006, 0.708276103762195, 0.7095880704799068, 0.7098932604091706, 0.7131281846563096, 0.7133290448112016, 0.711471873604372, 0.7126281984465616, 0.7118947893570801, 0.7140387882234809, 0.7147952079862211, 0.7157152579634969, 0.7159259223125705, 0.7142320133603409, 0.7152710149280144, 0.718435743626295, 0.7173867245730904, 0.7165560412835379, 0.7167755881975046, 0.7193151836491667, 0.7197665178298236, 0.7207594342733402, 0.7207907696691447, 0.7206291010298563, 0.7207285049299942, 0.7199159530788181, 0.7202587666381213, 0.7225882608048794, 0.7211800400392253, 0.721597491869182, 0.7212598797356489, 0.7244874130960977, 0.7234328599379599, 0.7239530714921226, 0.7249955355518545, 0.7232356008542516, 0.7227588160481626, 0.7225472738750862, 0.7230129885968087, 0.7235063944403842, 0.7242910459620905, 0.7257452675217058], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05]}
predicting test subjects:   0%|          | 0/10 [00:00<?, ?it/s]predicting test subjects:  10%|█         | 1/10 [00:05<00:49,  5.52s/it]predicting test subjects:  20%|██        | 2/10 [00:10<00:43,  5.42s/it]predicting test subjects:  30%|███       | 3/10 [00:17<00:40,  5.72s/it]predicting test subjects:  40%|████      | 4/10 [00:24<00:37,  6.32s/it]predicting test subjects:  50%|█████     | 5/10 [00:31<00:32,  6.45s/it]predicting test subjects:  60%|██████    | 6/10 [00:37<00:24,  6.15s/it]predicting test subjects:  70%|███████   | 7/10 [00:41<00:17,  5.77s/it]predicting test subjects:  80%|████████  | 8/10 [00:46<00:10,  5.49s/it]predicting test subjects:  90%|█████████ | 9/10 [00:52<00:05,  5.45s/it]predicting test subjects: 100%|██████████| 10/10 [00:57<00:00,  5.45s/it]
predicting train subjects:   0%|          | 0/42 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/42 [00:04<03:15,  4.76s/it]predicting train subjects:   5%|▍         | 2/42 [00:09<03:09,  4.73s/it]predicting train subjects:   7%|▋         | 3/42 [00:13<02:55,  4.49s/it]predicting train subjects:  10%|▉         | 4/42 [00:16<02:36,  4.11s/it]predicting train subjects:  12%|█▏        | 5/42 [00:21<02:45,  4.48s/it]predicting train subjects:  14%|█▍        | 6/42 [00:26<02:41,  4.50s/it]predicting train subjects:  17%|█▋        | 7/42 [00:30<02:28,  4.25s/it]predicting train subjects:  19%|█▉        | 8/42 [00:32<02:07,  3.76s/it]predicting train subjects:  21%|██▏       | 9/42 [00:37<02:09,  3.93s/it]predicting train subjects:  24%|██▍       | 10/42 [00:43<02:25,  4.54s/it]predicting train subjects:  26%|██▌       | 11/42 [00:47<02:24,  4.65s/it]predicting train subjects:  29%|██▊       | 12/42 [00:50<02:04,  4.15s/it]predicting train subjects:  31%|███       | 13/42 [00:55<02:00,  4.15s/it]predicting train subjects:  33%|███▎      | 14/42 [00:59<01:55,  4.11s/it]predicting train subjects:  36%|███▌      | 15/42 [01:02<01:41,  3.77s/it]predicting train subjects:  38%|███▊      | 16/42 [01:06<01:42,  3.95s/it]predicting train subjects:  40%|████      | 17/42 [01:12<01:52,  4.51s/it]predicting train subjects:  43%|████▎     | 18/42 [01:16<01:48,  4.51s/it]predicting train subjects:  45%|████▌     | 19/42 [01:21<01:42,  4.44s/it]predicting train subjects:  48%|████▊     | 20/42 [01:24<01:31,  4.15s/it]predicting train subjects:  50%|█████     | 21/42 [01:28<01:26,  4.12s/it]predicting train subjects:  52%|█████▏    | 22/42 [01:33<01:25,  4.25s/it]predicting train subjects:  55%|█████▍    | 23/42 [01:35<01:10,  3.73s/it]predicting train subjects:  57%|█████▋    | 24/42 [01:39<01:09,  3.84s/it]predicting train subjects:  60%|█████▉    | 25/42 [01:43<01:04,  3.81s/it]predicting train subjects:  62%|██████▏   | 26/42 [01:47<01:03,  3.94s/it]predicting train subjects:  64%|██████▍   | 27/42 [01:53<01:05,  4.39s/it]predicting train subjects:  67%|██████▋   | 28/42 [01:57<01:02,  4.44s/it]predicting train subjects:  69%|██████▉   | 29/42 [02:02<00:59,  4.57s/it]predicting train subjects:  71%|███████▏  | 30/42 [02:06<00:52,  4.35s/it]predicting train subjects:  74%|███████▍  | 31/42 [02:11<00:51,  4.66s/it]predicting train subjects:  76%|███████▌  | 32/42 [02:16<00:46,  4.62s/it]predicting train subjects:  79%|███████▊  | 33/42 [02:20<00:40,  4.53s/it]predicting train subjects:  81%|████████  | 34/42 [02:25<00:37,  4.74s/it]predicting train subjects:  83%|████████▎ | 35/42 [02:29<00:31,  4.47s/it]predicting train subjects:  86%|████████▌ | 36/42 [02:33<00:26,  4.36s/it]predicting train subjects:  88%|████████▊ | 37/42 [02:38<00:22,  4.54s/it]predicting train subjects:  90%|█████████ | 38/42 [02:43<00:18,  4.52s/it]predicting train subjects:  93%|█████████▎| 39/42 [02:47<00:13,  4.47s/it]predicting train subjects:  95%|█████████▌| 40/42 [02:52<00:09,  4.51s/it]predicting train subjects:  98%|█████████▊| 41/42 [02:56<00:04,  4.38s/it]predicting train subjects: 100%|██████████| 42/42 [03:01<00:00,  4.65s/it]
Loading train:   0%|          | 0/42 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/42 [00:01<00:52,  1.28s/it]Loading train:   5%|▍         | 2/42 [00:02<00:47,  1.19s/it]Loading train:   7%|▋         | 3/42 [00:03<00:44,  1.14s/it]Loading train:  10%|▉         | 4/42 [00:03<00:37,  1.00it/s]Loading train:  12%|█▏        | 5/42 [00:04<00:34,  1.06it/s]Loading train:  14%|█▍        | 6/42 [00:05<00:32,  1.09it/s]Loading train:  17%|█▋        | 7/42 [00:06<00:31,  1.13it/s]Loading train:  19%|█▉        | 8/42 [00:07<00:29,  1.16it/s]Loading train:  21%|██▏       | 9/42 [00:08<00:28,  1.17it/s]Loading train:  24%|██▍       | 10/42 [00:09<00:28,  1.12it/s]Loading train:  26%|██▌       | 11/42 [00:10<00:30,  1.03it/s]Loading train:  29%|██▊       | 12/42 [00:10<00:24,  1.24it/s]Loading train:  31%|███       | 13/42 [00:11<00:24,  1.18it/s]Loading train:  33%|███▎      | 14/42 [00:12<00:24,  1.16it/s]Loading train:  36%|███▌      | 15/42 [00:13<00:20,  1.31it/s]Loading train:  38%|███▊      | 16/42 [00:13<00:20,  1.26it/s]Loading train:  40%|████      | 17/42 [00:15<00:22,  1.12it/s]Loading train:  43%|████▎     | 18/42 [00:16<00:22,  1.04it/s]Loading train:  45%|████▌     | 19/42 [00:17<00:22,  1.01it/s]Loading train:  48%|████▊     | 20/42 [00:18<00:20,  1.05it/s]Loading train:  50%|█████     | 21/42 [00:18<00:18,  1.12it/s]Loading train:  52%|█████▏    | 22/42 [00:19<00:18,  1.06it/s]Loading train:  55%|█████▍    | 23/42 [00:20<00:15,  1.25it/s]Loading train:  57%|█████▋    | 24/42 [00:21<00:14,  1.22it/s]Loading train:  60%|█████▉    | 25/42 [00:22<00:14,  1.18it/s]Loading train:  62%|██████▏   | 26/42 [00:23<00:14,  1.13it/s]Loading train:  64%|██████▍   | 27/42 [00:23<00:12,  1.18it/s]Loading train:  67%|██████▋   | 28/42 [00:24<00:13,  1.07it/s]Loading train:  69%|██████▉   | 29/42 [00:25<00:10,  1.18it/s]Loading train:  71%|███████▏  | 30/42 [00:26<00:10,  1.17it/s]Loading train:  74%|███████▍  | 31/42 [00:27<00:10,  1.06it/s]Loading train:  76%|███████▌  | 32/42 [00:28<00:09,  1.10it/s]Loading train:  79%|███████▊  | 33/42 [00:29<00:08,  1.02it/s]Loading train:  81%|████████  | 34/42 [00:30<00:07,  1.01it/s]Loading train:  83%|████████▎ | 35/42 [00:31<00:06,  1.11it/s]Loading train:  86%|████████▌ | 36/42 [00:32<00:05,  1.10it/s]Loading train:  88%|████████▊ | 37/42 [00:33<00:04,  1.15it/s]Loading train:  90%|█████████ | 38/42 [00:33<00:03,  1.15it/s]Loading train:  93%|█████████▎| 39/42 [00:34<00:02,  1.18it/s]Loading train:  95%|█████████▌| 40/42 [00:35<00:01,  1.20it/s]Loading train:  98%|█████████▊| 41/42 [00:36<00:00,  1.16it/s]Loading train: 100%|██████████| 42/42 [00:37<00:00,  1.10it/s]
concatenating: train:   0%|          | 0/42 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 1/42 [00:00<00:04,  8.29it/s]concatenating: train:   7%|▋         | 3/42 [00:00<00:04,  9.41it/s]concatenating: train:  12%|█▏        | 5/42 [00:00<00:03,  9.49it/s]concatenating: train:  17%|█▋        | 7/42 [00:00<00:03, 10.29it/s]concatenating: train:  21%|██▏       | 9/42 [00:00<00:02, 11.79it/s]concatenating: train:  29%|██▊       | 12/42 [00:00<00:02, 14.25it/s]concatenating: train:  33%|███▎      | 14/42 [00:01<00:02, 13.14it/s]concatenating: train:  40%|████      | 17/42 [00:01<00:01, 15.22it/s]concatenating: train:  48%|████▊     | 20/42 [00:01<00:01, 17.01it/s]concatenating: train:  55%|█████▍    | 23/42 [00:01<00:01, 18.13it/s]concatenating: train:  60%|█████▉    | 25/42 [00:01<00:00, 17.30it/s]concatenating: train:  67%|██████▋   | 28/42 [00:01<00:00, 18.50it/s]concatenating: train:  71%|███████▏  | 30/42 [00:01<00:00, 18.50it/s]concatenating: train:  76%|███████▌  | 32/42 [00:01<00:00, 18.72it/s]concatenating: train:  83%|████████▎ | 35/42 [00:02<00:00, 19.00it/s]concatenating: train:  88%|████████▊ | 37/42 [00:02<00:00, 17.12it/s]concatenating: train:  95%|█████████▌| 40/42 [00:02<00:00, 17.83it/s]concatenating: train: 100%|██████████| 42/42 [00:02<00:00, 17.15it/s]
Loading test:   0%|          | 0/10 [00:00<?, ?it/s]Loading test:  10%|█         | 1/10 [00:01<00:09,  1.05s/it]Loading test:  20%|██        | 2/10 [00:02<00:08,  1.10s/it]Loading test:  30%|███       | 3/10 [00:03<00:07,  1.03s/it]Loading test:  40%|████      | 4/10 [00:04<00:06,  1.06s/it]Loading test:  50%|█████     | 5/10 [00:05<00:05,  1.12s/it]Loading test:  60%|██████    | 6/10 [00:06<00:04,  1.08s/it]Loading test:  70%|███████   | 7/10 [00:07<00:03,  1.06s/it]Loading test:  80%|████████  | 8/10 [00:08<00:02,  1.02s/it]Loading test:  90%|█████████ | 9/10 [00:09<00:01,  1.03s/it]Loading test: 100%|██████████| 10/10 [00:10<00:00,  1.00s/it]
concatenating: validation:   0%|          | 0/10 [00:00<?, ?it/s]concatenating: validation:  20%|██        | 2/10 [00:00<00:00, 12.81it/s]concatenating: validation:  40%|████      | 4/10 [00:00<00:00, 13.22it/s]concatenating: validation:  60%|██████    | 6/10 [00:00<00:00, 13.86it/s]concatenating: validation:  80%|████████  | 8/10 [00:00<00:00, 14.34it/s]concatenating: validation: 100%|██████████| 10/10 [00:00<00:00, 14.33it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 92, 116, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 92, 116, 20)  200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 92, 116, 20)  80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 92, 116, 20)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 92, 116, 20)  3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 92, 116, 20)  80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 92, 116, 20)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 46, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 46, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 46, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 46, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 46, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 46, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 46, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 46, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 46, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 23, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 23, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 23, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 23, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 23, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 23, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 23, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 23, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 23, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 23, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 46, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 46, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 46, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 46, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 46, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 46, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 46, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 46, 58, 140)  0           concatenate_4[0][0]              2019-07-29 01:11:31.168995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-29 01:11:31.169084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-29 01:11:31.169103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-29 01:11:31.169116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-29 01:11:31.169607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 92, 116, 20)  11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 92, 116, 40)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 92, 116, 20)  7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 92, 116, 20)  80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 92, 116, 20)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 92, 116, 20)  3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 92, 116, 20)  80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 92, 116, 20)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 92, 116, 60)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 92, 116, 60)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 92, 116, 2)   122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97955913 0.02044087]
Train on 4055 samples, validate on 1101 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 15s - loss: 1.0406 - acc: 0.8119 - mDice: 0.2194 - val_loss: 0.8856 - val_acc: 0.9231 - val_mDice: 0.1871

Epoch 00001: val_mDice improved from -inf to 0.18705, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.1412 - acc: 0.9874 - mDice: 0.7855 - val_loss: 0.1532 - val_acc: 0.9912 - val_mDice: 0.7626

Epoch 00002: val_mDice improved from 0.18705 to 0.76260, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0934 - acc: 0.9902 - mDice: 0.8450 - val_loss: 0.1259 - val_acc: 0.9914 - val_mDice: 0.7999

Epoch 00003: val_mDice improved from 0.76260 to 0.79988, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0806 - acc: 0.9916 - mDice: 0.8644 - val_loss: 0.1225 - val_acc: 0.9916 - val_mDice: 0.8060

Epoch 00004: val_mDice improved from 0.79988 to 0.80600, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0733 - acc: 0.9923 - mDice: 0.8761 - val_loss: 0.1306 - val_acc: 0.9912 - val_mDice: 0.7922

Epoch 00005: val_mDice did not improve from 0.80600
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0685 - acc: 0.9930 - mDice: 0.8836 - val_loss: 0.1129 - val_acc: 0.9924 - val_mDice: 0.8212

Epoch 00006: val_mDice improved from 0.80600 to 0.82118, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0676 - acc: 0.9933 - mDice: 0.8844 - val_loss: 0.1020 - val_acc: 0.9931 - val_mDice: 0.8382

Epoch 00007: val_mDice improved from 0.82118 to 0.83824, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0637 - acc: 0.9938 - mDice: 0.8910 - val_loss: 0.0983 - val_acc: 0.9932 - val_mDice: 0.8422

Epoch 00008: val_mDice improved from 0.83824 to 0.84225, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0620 - acc: 0.9942 - mDice: 0.8937 - val_loss: 0.0982 - val_acc: 0.9933 - val_mDice: 0.8406

Epoch 00009: val_mDice did not improve from 0.84225
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0607 - acc: 0.9944 - mDice: 0.8958 - val_loss: 0.1235 - val_acc: 0.9926 - val_mDice: 0.8038

Epoch 00010: val_mDice did not improve from 0.84225
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0583 - acc: 0.9948 - mDice: 0.9000 - val_loss: 0.1102 - val_acc: 0.9929 - val_mDice: 0.8233

Epoch 00011: val_mDice did not improve from 0.84225
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 7s - loss: 0.0567 - acc: 0.9952 - mDice: 0.9027 - val_loss: 0.1004 - val_acc: 0.9933 - val_mDice: 0.8328

Epoch 00012: val_mDice did not improve from 0.84225
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0560 - acc: 0.9954 - mDice: 0.9037 - val_loss: 0.0950 - val_acc: 0.9937 - val_mDice: 0.8405

Epoch 00013: val_mDice did not improve from 0.84225
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0550 - acc: 0.9956 - mDice: 0.9049 - val_loss: 0.0926 - val_acc: 0.9941 - val_mDice: 0.8442

Epoch 00014: val_mDice improved from 0.84225 to 0.84418, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0524 - acc: 0.9960 - mDice: 0.9079 - val_loss: 0.0859 - val_acc: 0.9947 - val_mDice: 0.8526

Epoch 00015: val_mDice improved from 0.84418 to 0.85263, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0514 - acc: 0.9961 - mDice: 0.9067 - val_loss: 0.0932 - val_acc: 0.9946 - val_mDice: 0.8395

Epoch 00016: val_mDice did not improve from 0.85263
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0513 - acc: 0.9962 - mDice: 0.9055 - val_loss: 0.0747 - val_acc: 0.9954 - val_mDice: 0.8705

Epoch 00017: val_mDice improved from 0.85263 to 0.87050, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 8s - loss: 0.0487 - acc: 0.9963 - mDice: 0.9100 - val_loss: 0.0817 - val_acc: 0.9951 - val_mDice: 0.8576

Epoch 00018: val_mDice did not improve from 0.87050
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0474 - acc: 0.9964 - mDice: 0.9120 - val_loss: 0.0847 - val_acc: 0.9950 - val_mDice: 0.8523

Epoch 00019: val_mDice did not improve from 0.87050
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0462 - acc: 0.9965 - mDice: 0.9142 - val_loss: 0.0832 - val_acc: 0.9950 - val_mDice: 0.8549

Epoch 00020: val_mDice did not improve from 0.87050
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0460 - acc: 0.9965 - mDice: 0.9145 - val_loss: 0.0824 - val_acc: 0.9950 - val_mDice: 0.8556

Epoch 00021: val_mDice did not improve from 0.87050
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0462 - acc: 0.9965 - mDice: 0.9141 - val_loss: 0.0782 - val_acc: 0.9952 - val_mDice: 0.8627

Epoch 00022: val_mDice did not improve from 0.87050
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0455 - acc: 0.9966 - mDice: 0.9154 - val_loss: 0.0802 - val_acc: 0.9952 - val_mDice: 0.8590

Epoch 00023: val_mDice did not improve from 0.87050
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0460 - acc: 0.9965 - mDice: 0.9144 - val_loss: 0.0725 - val_acc: 0.9955 - val_mDice: 0.8727

Epoch 00024: val_mDice improved from 0.87050 to 0.87266, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/1-THALAMUS/sd1/best_model_weights.h5
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0450 - acc: 0.9966 - mDice: 0.9162 - val_loss: 0.0757 - val_acc: 0.9954 - val_mDice: 0.8676

Epoch 00025: val_mDice did not improve from 0.87266
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0447 - acc: 0.9966 - mDice: 0.9167 - val_loss: 0.0777 - val_acc: 0.9952 - val_mDice: 0.8630

Epoch 00026: val_mDice did not improve from 0.87266
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0442 - acc: 0.9966 - mDice: 0.9176 - val_loss: 0.0771 - val_acc: 0.9952 - val_mDice: 0.8641

Epoch 00027: val_mDice did not improve from 0.87266
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0438 - acc: 0.9967 - mDice: 0.9183 - val_loss: 0.0741 - val_acc: 0.9954 - val_mDice: 0.8690

Epoch 00028: val_mDice did not improve from 0.87266
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0436 - acc: 0.9967 - mDice: 0.9187 - val_loss: 0.0744 - val_acc: 0.9954 - val_mDice: 0.8686

Epoch 00029: val_mDice did not improve from 0.87266
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0440 - acc: 0.9967 - mDice: 0.9178 - val_loss: 0.0762 - val_acc: 0.9953 - val_mDice: 0.8655

Epoch 00030: val_mDice did not improve from 0.87266
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0436 - acc: 0.9967 - mDice: 0.9185 - val_loss: 0.0747 - val_acc: 0.9954 - val_mDice: 0.8677

Epoch 00031: val_mDice did not improve from 0.87266
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0430 - acc: 0.9967 - mDice: 0.9197 - val_loss: 0.0731 - val_acc: 0.9954 - val_mDice: 0.8703

Epoch 00032: val_mDice did not improve from 0.87266
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0432 - acc: 0.9967 - mDice: 0.9192 - val_loss: 0.0771 - val_acc: 0.9953 - val_mDice: 0.8642

Epoch 00033: val_mDice did not improve from 0.87266
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 7s - loss: 0.0429 - acc: 0.9967 - mDice: 0.9198 - val_loss: 0.0774 - val_acc: 0.9953 - val_mDice: 0.8633

Epoch 00034: val_mDice did not improve from 0.87266
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0425 - acc: 0.9968 - mDice: 0.9204 - val_loss: 0.0826 - val_acc: 0.9951 - val_mDice: 0.8546

Epoch 00035: val_mDice did not improve from 0.87266
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 8s - loss: 0.0418 - acc: 0.9968 - mDice: 0.9217 - val_loss: 0.0801 - val_acc: 0.9952 - val_mDice: 0.8587

Epoch 00036: val_mDice did not improve from 0.87266
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0416 - acc: 0.9968 - mDice: 0.9221 - val_loss: 0.0867 - val_acc: 0.9949 - val_mDice: 0.8481

Epoch 00037: val_mDice did not improve from 0.87266
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0412 - acc: 0.9969 - mDice: 0.9229 - val_loss: 0.0811 - val_acc: 0.9951 - val_mDice: 0.8574

Epoch 00038: val_mDice did not improve from 0.87266
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0411 - acc: 0.9969 - mDice: 0.9230 - val_loss: 0.0796 - val_acc: 0.9952 - val_mDice: 0.8599

Epoch 00039: val_mDice did not improve from 0.87266
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0407 - acc: 0.9969 - mDice: 0.9236 - val_loss: 0.0758 - val_acc: 0.9954 - val_mDice: 0.8659

Epoch 00040: val_mDice did not improve from 0.87266
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0405 - acc: 0.9969 - mDice: 0.9241 - val_loss: 0.0776 - val_acc: 0.9953 - val_mDice: 0.8628

Epoch 00041: val_mDice did not improve from 0.87266
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0406 - acc: 0.9969 - mDice: 0.9238 - val_loss: 0.0838 - val_acc: 0.9950 - val_mDice: 0.8526

Epoch 00042: val_mDice did not improve from 0.87266
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0404 - acc: 0.9969 - mDice: 0.9242 - val_loss: 0.0814 - val_acc: 0.9951 - val_mDice: 0.8569

Epoch 00043: val_mDice did not improve from 0.87266
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0403 - acc: 0.9969 - mDice: 0.9244 - val_loss: 0.0841 - val_acc: 0.9950 - val_mDice: 0.8517

Epoch 00044: val_mDice did not improve from 0.87266
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0411 - acc: 0.9969 - mDice: 0.9230 - val_loss: 0.0798 - val_acc: 0.9952 - val_mDice: 0.8592

Epoch 00045: val_mDice did not improve from 0.87266
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0405 - acc: 0.9969 - mDice: 0.9240 - val_loss: 0.0781 - val_acc: 0.9953 - val_mDice: 0.8622

Epoch 00046: val_mDice did not improve from 0.87266
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0400 - acc: 0.9969 - mDice: 0.9250 - val_loss: 0.0758 - val_acc: 0.9954 - val_mDice: 0.8660

Epoch 00047: val_mDice did not improve from 0.87266
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0399 - acc: 0.9970 - mDice: 0.9251 - val_loss: 0.0749 - val_acc: 0.9954 - val_mDice: 0.8674

Epoch 00048: val_mDice did not improve from 0.87266
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0398 - acc: 0.9970 - mDice: 0.9253 - val_loss: 0.0791 - val_acc: 0.9952 - val_mDice: 0.8600

Epoch 00049: val_mDice did not improve from 0.87266
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0397 - acc: 0.9970 - mDice: 0.9254 - val_loss: 0.0789 - val_acc: 0.9952 - val_mDice: 0.8606

Epoch 00050: val_mDice did not improve from 0.87266
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0395 - acc: 0.9970 - mDice: 0.9259 - val_loss: 0.0792 - val_acc: 0.9952 - val_mDice: 0.8600

Epoch 00051: val_mDice did not improve from 0.87266
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0397 - acc: 0.9970 - mDice: 0.9255 - val_loss: 0.0796 - val_acc: 0.9952 - val_mDice: 0.8591

Epoch 00052: val_mDice did not improve from 0.87266
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 8s - loss: 0.0394 - acc: 0.9970 - mDice: 0.9259 - val_loss: 0.0783 - val_acc: 0.9952 - val_mDice: 0.8614

Epoch 00053: val_mDice did not improve from 0.87266
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 7s - loss: 0.0392 - acc: 0.9970 - mDice: 0.9263 - val_loss: 0.0783 - val_acc: 0.9952 - val_mDice: 0.8616

Epoch 00054: val_mDice did not improve from 0.87266
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0390 - acc: 0.9970 - mDice: 0.9267 - val_loss: 0.0811 - val_acc: 0.9951 - val_mDice: 0.8567

Epoch 00055: val_mDice did not improve from 0.87266
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0387 - acc: 0.9970 - mDice: 0.9272 - val_loss: 0.0812 - val_acc: 0.9951 - val_mDice: 0.8565

Epoch 00056: val_mDice did not improve from 0.87266
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0390 - acc: 0.9970 - mDice: 0.9267 - val_loss: 0.0826 - val_acc: 0.9951 - val_mDice: 0.8542

Epoch 00057: val_mDice did not improve from 0.87266
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0388 - acc: 0.9970 - mDice: 0.9271 - val_loss: 0.0769 - val_acc: 0.9953 - val_mDice: 0.8635

Epoch 00058: val_mDice did not improve from 0.87266
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0386 - acc: 0.9970 - mDice: 0.9275 - val_loss: 0.0814 - val_acc: 0.9951 - val_mDice: 0.8562

Epoch 00059: val_mDice did not improve from 0.87266
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0384 - acc: 0.9971 - mDice: 0.9278 - val_loss: 0.0772 - val_acc: 0.9953 - val_mDice: 0.8631

Epoch 00060: val_mDice did not improve from 0.87266
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0383 - acc: 0.9971 - mDice: 0.9280 - val_loss: 0.0791 - val_acc: 0.9952 - val_mDice: 0.8602

Epoch 00061: val_mDice did not improve from 0.87266
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 8s - loss: 0.0386 - acc: 0.9970 - mDice: 0.9274 - val_loss: 0.0790 - val_acc: 0.9952 - val_mDice: 0.8603

Epoch 00062: val_mDice did not improve from 0.87266
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0383 - acc: 0.9971 - mDice: 0.9279 - val_loss: 0.0782 - val_acc: 0.9953 - val_mDice: 0.8619

Epoch 00063: val_mDice did not improve from 0.87266
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 7s - loss: 0.0382 - acc: 0.9971 - mDice: 0.9282 - val_loss: 0.0801 - val_acc: 0.9952 - val_mDice: 0.8583

Epoch 00064: val_mDice did not improve from 0.87266
Restoring model weights from the end of the best epoch
Epoch 00064: early stopping
{'val_loss': [0.8855693288330161, 0.15320679247649988, 0.12593717917759348, 0.12246018457315273, 0.13060976544582875, 0.11289751282017628, 0.10200196553316038, 0.09828583468479204, 0.09816349418893064, 0.12346662886840012, 0.11015580252124221, 0.10039777144261862, 0.09496276291741987, 0.09255011410847455, 0.08592831743348198, 0.09317579410706293, 0.07465472108228546, 0.08167908903038794, 0.08468116538347059, 0.08323897675803095, 0.08238696551236319, 0.07818654974030972, 0.0801613142580254, 0.07249391995214745, 0.07571858932330974, 0.07768775892192727, 0.07714527436649658, 0.07407281141081905, 0.07443872626048667, 0.07615586046085479, 0.07469071485745052, 0.07309160011395013, 0.07706092158628527, 0.07739043763612856, 0.0826317268160665, 0.08007571806320811, 0.08674859662255192, 0.0811389335088574, 0.079573138059539, 0.07584495069653635, 0.07757571822778406, 0.08375331506363161, 0.08135565345282993, 0.08412377405015042, 0.0798454500403218, 0.07806455848110902, 0.07576199142527948, 0.07485265174472906, 0.07910961899348111, 0.07893059594213259, 0.07916765720274749, 0.07960465132215692, 0.07829363975711133, 0.07833735834296242, 0.08109517958519352, 0.08124389004750646, 0.0826244183987731, 0.07693768723729955, 0.0813607298223459, 0.07722266528630235, 0.07907123139617012, 0.07895786185951043, 0.07816346928978053, 0.0801445282420497], 'val_acc': [0.9230863069751716, 0.9912031336766173, 0.9913524214405022, 0.9916368410004799, 0.9912463622565274, 0.9924468947582955, 0.9930800970417061, 0.9931902261780783, 0.9932899624515295, 0.9926408619148747, 0.9929013683187431, 0.9933202682463934, 0.993740451216806, 0.9940656348859906, 0.9946804505711139, 0.9946190052742746, 0.9953702565867938, 0.9951068376217183, 0.9949835193471189, 0.9950373096968455, 0.9950401139844016, 0.9952472685468294, 0.9951695713750023, 0.9954968880351082, 0.9953682048011108, 0.9952411402581931, 0.9952429213597491, 0.9954076867878816, 0.9953658173671103, 0.9952625080631821, 0.995379015925145, 0.9954437040178262, 0.9952730539167718, 0.995308026305986, 0.9950796067985376, 0.9951812270335129, 0.9948688737919502, 0.9950856538819357, 0.9951900784044673, 0.9953539126792028, 0.9952619017307375, 0.995005888683378, 0.9951300731471838, 0.9949920242423907, 0.9951887195701495, 0.9952596063293402, 0.9953524563985993, 0.9953658227807928, 0.9952223222976809, 0.995230069277397, 0.9952299068669207, 0.9951557773118881, 0.9952296524238413, 0.9952406313720343, 0.9951373978596617, 0.9951351836635025, 0.9950835263046971, 0.9953117834016694, 0.9951145846014343, 0.9952823275549644, 0.9952266694847612, 0.995235093174795, 0.9952755171423279, 0.9951831001676721], 'val_mDice': [0.18705324448313526, 0.7625979688576846, 0.7998758505735475, 0.8060028453397274, 0.7922304531967066, 0.8211791623623993, 0.8382412436655929, 0.8422481158340551, 0.8406320882861339, 0.8037637678955383, 0.8232743982614765, 0.8327615737481945, 0.8404761200921304, 0.8441770325781106, 0.852633794798405, 0.8395444144994751, 0.8704984816067008, 0.8576301852753766, 0.8523190920186194, 0.8549350159044812, 0.8555874458558986, 0.8627081058114144, 0.8589857713836199, 0.8726601650452852, 0.8675616779292745, 0.8630262300385658, 0.8641082411132868, 0.869036148506116, 0.8686019928210654, 0.8654997145230937, 0.867673768226284, 0.8702907531939237, 0.8642136942355011, 0.8632935630961184, 0.854565169354334, 0.8587098630529225, 0.8480883099402654, 0.8573773067502083, 0.8599033905743036, 0.8659494183044018, 0.8628322198973473, 0.852570871566232, 0.8569062080954986, 0.8517114278081327, 0.8592162805725292, 0.8622082821571426, 0.8659682525059615, 0.8673669261135479, 0.8600382833021755, 0.8605623223584528, 0.8599967116339439, 0.8591246702365719, 0.8614087407963585, 0.8615876142811927, 0.85673601815746, 0.8565075607542337, 0.8542458487033411, 0.8634534886920593, 0.856156293962567, 0.8631326955194153, 0.8602075908099598, 0.8603398903838945, 0.8618511848293793, 0.8583387442873782], 'loss': [1.0405853906630294, 0.14116315279987385, 0.09337236140607759, 0.08062380651510452, 0.07330898585948638, 0.0685383019148936, 0.0676458558910578, 0.06367607149220865, 0.0620228157283419, 0.06072222739090608, 0.05826979062523412, 0.05665433634665098, 0.056008431155569895, 0.055048342899829336, 0.052389576838210536, 0.05139506779124205, 0.05133045364157486, 0.048674545635534715, 0.04744215456151051, 0.04619640106870419, 0.046049565894462616, 0.046225090852207086, 0.04545451464583929, 0.046039893511227996, 0.04500159617104601, 0.04469216816614206, 0.044207458527880446, 0.04381843877705928, 0.04357746109807389, 0.04402357179547503, 0.0436429814579087, 0.042957770223608735, 0.04322428683464389, 0.042899243891900626, 0.04254717677004828, 0.041819091330474165, 0.04160257449095699, 0.04118267398984453, 0.04109339844136879, 0.040749974259429146, 0.04046938423200541, 0.040642333117057595, 0.0404309154604131, 0.040320640487272554, 0.041065138940379005, 0.04052035459093336, 0.03995439708324749, 0.039880916623240184, 0.03978605373187218, 0.03973111072061388, 0.039456691951221076, 0.039657793847244556, 0.03942177114886509, 0.039241557006073055, 0.03897891121511101, 0.0387212177405522, 0.03900880797429679, 0.03875570679049015, 0.038581875700655174, 0.03840688695504104, 0.03827972815782015, 0.038614888612350906, 0.03831873882364403, 0.03816455841046079], 'acc': [0.8118548681350878, 0.9874059512347682, 0.990203065584091, 0.9916050539945703, 0.9922713252971851, 0.9929823180162216, 0.993278958794515, 0.9937750633930014, 0.9942499083155621, 0.9944185721153984, 0.9948324143666081, 0.9951694919354525, 0.9953583090137759, 0.995597428002575, 0.9959834741311362, 0.9961467754973142, 0.9961603691045806, 0.9963185566715189, 0.9964195718453639, 0.9965061377802847, 0.9965127630827606, 0.9965018345601168, 0.9965502423511039, 0.9965184239071189, 0.9965862911814679, 0.9966115525265657, 0.9966433921371818, 0.996676413036892, 0.9966859860132126, 0.996656423579309, 0.9966771316734107, 0.9967268612029079, 0.9967134786004938, 0.9967372626911403, 0.9967625144818562, 0.9968198725239417, 0.9968302685000305, 0.9968673576556063, 0.9968696938490015, 0.9968912034087646, 0.9969176411922704, 0.9969055045547673, 0.9969108889635041, 0.9969245019735156, 0.9968726286611781, 0.9969042617501519, 0.9969450260355205, 0.996955811536415, 0.9969637390276653, 0.9969639520168893, 0.9969929436132147, 0.9969730993822382, 0.996980332122925, 0.9970033941962716, 0.9970197519597525, 0.9970441829555573, 0.997026781118607, 0.9970390686418508, 0.997049499013715, 0.9970605196259025, 0.9970752105906918, 0.997042006026361, 0.9970696218651702, 0.9970777345644414], 'mDice': [0.21936480101983438, 0.7854886980297827, 0.8450075987675922, 0.8644231000752102, 0.8760910777216628, 0.8836036462672101, 0.8844351160687965, 0.8910338861727685, 0.8936872474215327, 0.895801026488938, 0.9000299879419819, 0.9027480285352902, 0.9037054313390381, 0.9048686647826735, 0.9079335560487025, 0.9067060740161619, 0.9054960305681946, 0.9099730109903874, 0.9120466124114802, 0.9142457486374899, 0.9144658973272867, 0.9140955755066783, 0.9154445967015737, 0.9143792501066234, 0.9161953451307134, 0.9167249307091522, 0.917564276114345, 0.9182561688887647, 0.918665660074401, 0.9178394309248495, 0.9185056354202854, 0.9197237870843137, 0.9192235203765323, 0.9197983105291421, 0.9204185294899194, 0.9217233441025937, 0.9220946203324709, 0.9228531430152723, 0.923007059699951, 0.9236223490405759, 0.9241291692459774, 0.9237967054147168, 0.9241844559715355, 0.9243785098066459, 0.9230112539266687, 0.9239852178552441, 0.9250164846309751, 0.925138718102922, 0.9253102136745994, 0.9253944252627992, 0.925896296665789, 0.9255228270437803, 0.9259465402945344, 0.9262721962611272, 0.9267446662582981, 0.9272157951142138, 0.9266887136658082, 0.9271495138672806, 0.9274572956429751, 0.9277794957895902, 0.9280107303553533, 0.9273969600144797, 0.9279338315441635, 0.9282067979602132], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125]}
predicting test subjects:   0%|          | 0/10 [00:00<?, ?it/s]predicting test subjects:  10%|█         | 1/10 [00:01<00:11,  1.30s/it]predicting test subjects:  20%|██        | 2/10 [00:02<00:09,  1.18s/it]predicting test subjects:  30%|███       | 3/10 [00:02<00:07,  1.03s/it]predicting test subjects:  40%|████      | 4/10 [00:03<00:05,  1.08it/s]predicting test subjects:  50%|█████     | 5/10 [00:04<00:04,  1.20it/s]predicting test subjects:  60%|██████    | 6/10 [00:04<00:02,  1.36it/s]predicting test subjects:  70%|███████   | 7/10 [00:05<00:02,  1.50it/s]predicting test subjects:  80%|████████  | 8/10 [00:05<00:01,  1.55it/s]predicting test subjects:  90%|█████████ | 9/10 [00:06<00:00,  1.60it/s]predicting test subjects: 100%|██████████| 10/10 [00:07<00:00,  1.60it/s]
predicting train subjects:   0%|          | 0/42 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/42 [00:00<00:19,  2.13it/s]predicting train subjects:   5%|▍         | 2/42 [00:00<00:18,  2.13it/s]predicting train subjects:   7%|▋         | 3/42 [00:01<00:18,  2.15it/s]predicting train subjects:  10%|▉         | 4/42 [00:01<00:17,  2.18it/s]predicting train subjects:  12%|█▏        | 5/42 [00:02<00:17,  2.06it/s]predicting train subjects:  14%|█▍        | 6/42 [00:02<00:17,  2.05it/s]predicting train subjects:  17%|█▋        | 7/42 [00:03<00:17,  2.05it/s]predicting train subjects:  19%|█▉        | 8/42 [00:03<00:14,  2.37it/s]predicting train subjects:  21%|██▏       | 9/42 [00:04<00:15,  2.15it/s]predicting train subjects:  24%|██▍       | 10/42 [00:04<00:17,  1.86it/s]predicting train subjects:  26%|██▌       | 11/42 [00:05<00:18,  1.65it/s]predicting train subjects:  29%|██▊       | 12/42 [00:05<00:15,  1.93it/s]predicting train subjects:  31%|███       | 13/42 [00:06<00:15,  1.89it/s]predicting train subjects:  33%|███▎      | 14/42 [00:07<00:16,  1.73it/s]predicting train subjects:  36%|███▌      | 15/42 [00:07<00:13,  2.01it/s]predicting train subjects:  38%|███▊      | 16/42 [00:07<00:12,  2.08it/s]predicting train subjects:  40%|████      | 17/42 [00:08<00:13,  1.91it/s]predicting train subjects:  43%|████▎     | 18/42 [00:09<00:13,  1.80it/s]predicting train subjects:  45%|████▌     | 19/42 [00:09<00:12,  1.85it/s]predicting train subjects:  48%|████▊     | 20/42 [00:10<00:10,  2.05it/s]predicting train subjects:  50%|█████     | 21/42 [00:10<00:10,  1.91it/s]predicting train subjects:  52%|█████▏    | 22/42 [00:11<00:10,  1.94it/s]predicting train subjects:  55%|█████▍    | 23/42 [00:11<00:08,  2.27it/s]predicting train subjects:  57%|█████▋    | 24/42 [00:11<00:07,  2.32it/s]predicting train subjects:  60%|█████▉    | 25/42 [00:12<00:07,  2.22it/s]predicting train subjects:  62%|██████▏   | 26/42 [00:13<00:08,  1.97it/s]predicting train subjects:  64%|██████▍   | 27/42 [00:13<00:08,  1.83it/s]predicting train subjects:  67%|██████▋   | 28/42 [00:14<00:07,  1.82it/s]predicting train subjects:  69%|██████▉   | 29/42 [00:14<00:07,  1.76it/s]predicting train subjects:  71%|███████▏  | 30/42 [00:15<00:06,  1.91it/s]predicting train subjects:  74%|███████▍  | 31/42 [00:15<00:06,  1.71it/s]predicting train subjects:  76%|███████▌  | 32/42 [00:16<00:05,  1.73it/s]predicting train subjects:  79%|███████▊  | 33/42 [00:16<00:04,  1.88it/s]predicting train subjects:  81%|████████  | 34/42 [00:17<00:05,  1.58it/s]predicting train subjects:  83%|████████▎ | 35/42 [00:18<00:04,  1.72it/s]predicting train subjects:  86%|████████▌ | 36/42 [00:18<00:03,  1.82it/s]predicting train subjects:  88%|████████▊ | 37/42 [00:19<00:02,  1.80it/s]predicting train subjects:  90%|█████████ | 38/42 [00:19<00:02,  1.92it/s]predicting train subjects:  93%|█████████▎| 39/42 [00:20<00:01,  1.98it/s]predicting train subjects:  95%|█████████▌| 40/42 [00:20<00:01,  1.86it/s]predicting train subjects:  98%|█████████▊| 41/42 [00:21<00:00,  1.95it/s]predicting train subjects: 100%|██████████| 42/42 [00:21<00:00,  1.79it/s]
Loading train:   0%|          | 0/42 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/42 [00:04<03:15,  4.77s/it]Loading train:   5%|▍         | 2/42 [00:09<03:10,  4.76s/it]Loading train:   7%|▋         | 3/42 [00:13<02:59,  4.60s/it]Loading train:  10%|▉         | 4/42 [00:17<02:43,  4.30s/it]Loading train:  12%|█▏        | 5/42 [00:22<02:53,  4.68s/it]Loading train:  14%|█▍        | 6/42 [00:27<02:46,  4.63s/it]Loading train:  17%|█▋        | 7/42 [00:31<02:33,  4.39s/it]Loading train:  19%|█▉        | 8/42 [00:34<02:14,  3.96s/it]Loading train:  21%|██▏       | 9/42 [00:39<02:19,  4.22s/it]Loading train:  24%|██▍       | 10/42 [00:44<02:29,  4.67s/it]Loading train:  26%|██▌       | 11/42 [00:49<02:29,  4.83s/it]Loading train:  29%|██▊       | 12/42 [00:51<01:59,  3.99s/it]Loading train:  31%|███       | 13/42 [00:56<02:01,  4.17s/it]Loading train:  33%|███▎      | 14/42 [01:01<02:05,  4.48s/it]Loading train:  36%|███▌      | 15/42 [01:04<01:44,  3.87s/it]Loading train:  38%|███▊      | 16/42 [01:09<01:51,  4.29s/it]Loading train:  40%|████      | 17/42 [01:14<01:52,  4.48s/it]Loading train:  43%|████▎     | 18/42 [01:19<01:48,  4.54s/it]Loading train:  45%|████▌     | 19/42 [01:23<01:43,  4.52s/it]Loading train:  48%|████▊     | 20/42 [01:27<01:34,  4.30s/it]Loading train:  50%|█████     | 21/42 [01:31<01:31,  4.35s/it]Loading train:  52%|█████▏    | 22/42 [01:36<01:29,  4.49s/it]Loading train:  55%|█████▍    | 23/42 [01:39<01:14,  3.90s/it]Loading train:  57%|█████▋    | 24/42 [01:44<01:20,  4.47s/it]Loading train:  60%|█████▉    | 25/42 [01:50<01:21,  4.82s/it]Loading train:  62%|██████▏   | 26/42 [01:56<01:23,  5.24s/it]Loading train:  64%|██████▍   | 27/42 [02:01<01:15,  5.03s/it]Loading train:  67%|██████▋   | 28/42 [02:06<01:12,  5.20s/it]Loading train:  69%|██████▉   | 29/42 [02:13<01:13,  5.64s/it]Loading train:  71%|███████▏  | 30/42 [02:17<01:00,  5.05s/it]Loading train:  74%|███████▍  | 31/42 [02:23<00:58,  5.29s/it]Loading train:  76%|███████▌  | 32/42 [02:27<00:49,  4.91s/it]Loading train:  79%|███████▊  | 33/42 [02:32<00:45,  5.09s/it]Loading train:  81%|████████  | 34/42 [02:36<00:38,  4.84s/it]Loading train:  83%|████████▎ | 35/42 [02:41<00:32,  4.69s/it]Loading train:  86%|████████▌ | 36/42 [02:48<00:32,  5.34s/it]Loading train:  88%|████████▊ | 37/42 [02:54<00:27,  5.56s/it]Loading train:  90%|█████████ | 38/42 [03:02<00:25,  6.28s/it]Loading train:  93%|█████████▎| 39/42 [03:07<00:17,  5.99s/it]Loading train:  95%|█████████▌| 40/42 [03:11<00:10,  5.34s/it]Loading train:  98%|█████████▊| 41/42 [03:16<00:05,  5.27s/it]Loading train: 100%|██████████| 42/42 [03:20<00:00,  4.83s/it]
concatenating: train:   0%|          | 0/42 [00:00<?, ?it/s]concatenating: train:   7%|▋         | 3/42 [00:00<00:01, 25.40it/s]concatenating: train:  12%|█▏        | 5/42 [00:00<00:01, 20.59it/s]concatenating: train:  17%|█▋        | 7/42 [00:00<00:01, 18.18it/s]concatenating: train:  21%|██▏       | 9/42 [00:00<00:01, 16.85it/s]concatenating: train:  29%|██▊       | 12/42 [00:00<00:01, 18.92it/s]concatenating: train:  36%|███▌      | 15/42 [00:00<00:01, 20.41it/s]concatenating: train:  43%|████▎     | 18/42 [00:00<00:01, 20.37it/s]concatenating: train:  48%|████▊     | 20/42 [00:01<00:01, 18.67it/s]concatenating: train:  52%|█████▏    | 22/42 [00:01<00:01, 19.05it/s]concatenating: train:  57%|█████▋    | 24/42 [00:01<00:01, 17.85it/s]concatenating: train:  64%|██████▍   | 27/42 [00:01<00:00, 20.01it/s]concatenating: train:  74%|███████▍  | 31/42 [00:01<00:00, 22.28it/s]concatenating: train:  86%|████████▌ | 36/42 [00:01<00:00, 26.35it/s]concatenating: train:  95%|█████████▌| 40/42 [00:01<00:00, 25.78it/s]concatenating: train: 100%|██████████| 42/42 [00:01<00:00, 21.32it/s]
Loading test:   0%|          | 0/10 [00:00<?, ?it/s]Loading test:  10%|█         | 1/10 [00:05<00:45,  5.01s/it]Loading test:  20%|██        | 2/10 [00:09<00:39,  4.91s/it]Loading test:  30%|███       | 3/10 [00:14<00:33,  4.86s/it]Loading test:  40%|████      | 4/10 [00:19<00:29,  4.93s/it]Loading test:  50%|█████     | 5/10 [00:24<00:25,  5.03s/it]Loading test:  60%|██████    | 6/10 [00:29<00:19,  4.97s/it]Loading test:  70%|███████   | 7/10 [00:33<00:14,  4.76s/it]Loading test:  80%|████████  | 8/10 [00:37<00:09,  4.52s/it]Loading test:  90%|█████████ | 9/10 [00:42<00:04,  4.64s/it]Loading test: 100%|██████████| 10/10 [00:46<00:00,  4.33s/it]
concatenating: validation:   0%|          | 0/10 [00:00<?, ?it/s]concatenating: validation:  20%|██        | 2/10 [00:00<00:00,  9.44it/s]concatenating: validation:  30%|███       | 3/10 [00:00<00:00,  9.38it/s]concatenating: validation:  50%|█████     | 5/10 [00:00<00:00, 10.58it/s]concatenating: validation:  90%|█████████ | 9/10 [00:00<00:00, 13.56it/s]concatenating: validation: 100%|██████████| 10/10 [00:00<00:00, 17.53it/s]
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 92, 116, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 92, 116, 20)  200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 92, 116, 20)  80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 92, 116, 20)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 92, 116, 20)  3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 92, 116, 20)  80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 92, 116, 20)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 46, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 46, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 46, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 46, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 46, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 46, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 46, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 46, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 46, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 23, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 23, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 23, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 23, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 23, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 23, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 23, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 23, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 23, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 23, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 46, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 46, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 46, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 46, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 46, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 46, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 46, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 46, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 46, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2019-07-29 01:24:53.162356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-29 01:24:53.162444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-29 01:24:53.162463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-29 01:24:53.162477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-29 01:24:53.162967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 46, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 92, 116, 20)  11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 92, 116, 40)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 92, 116, 20)  7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 92, 116, 20)  80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 92, 116, 20)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 92, 116, 20)  3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 92, 116, 20)  80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 92, 116, 20)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 92, 116, 60)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 92, 116, 60)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 92, 116, 13)  793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.55788522e-02 3.00676784e-02 7.53934202e-02 1.00198878e-02
 2.70600226e-02 6.80012090e-03 7.86791360e-02 1.15057661e-01
 7.52386400e-02 1.34503134e-02 3.22328422e-01 1.80291436e-01
 3.44098048e-05]
Train on 4055 samples, validate on 1101 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 20s - loss: 5.2001 - acc: 0.2572 - mDice: 0.0101 - val_loss: 5.4296 - val_acc: 0.6488 - val_mDice: 0.0077

Epoch 00001: val_mDice improved from -inf to 0.00769, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 2.7541 - acc: 0.8853 - mDice: 0.0892 - val_loss: 3.2092 - val_acc: 0.9866 - val_mDice: 0.1352

Epoch 00002: val_mDice improved from 0.00769 to 0.13523, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 1.5165 - acc: 0.9788 - mDice: 0.2253 - val_loss: 7.2366 - val_acc: 0.9866 - val_mDice: 0.0579

Epoch 00003: val_mDice did not improve from 0.13523
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 1.0876 - acc: 0.9840 - mDice: 0.3305 - val_loss: 4.0859 - val_acc: 0.9866 - val_mDice: 0.1343

Epoch 00004: val_mDice did not improve from 0.13523
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.9000 - acc: 0.9850 - mDice: 0.3941 - val_loss: 2.0048 - val_acc: 0.9869 - val_mDice: 0.3023

Epoch 00005: val_mDice improved from 0.13523 to 0.30235, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.7780 - acc: 0.9854 - mDice: 0.4439 - val_loss: 1.6854 - val_acc: 0.9869 - val_mDice: 0.3433

Epoch 00006: val_mDice improved from 0.30235 to 0.34327, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.6882 - acc: 0.9859 - mDice: 0.4854 - val_loss: 1.9032 - val_acc: 0.9869 - val_mDice: 0.3147

Epoch 00007: val_mDice did not improve from 0.34327
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.6398 - acc: 0.9864 - mDice: 0.5098 - val_loss: 1.4866 - val_acc: 0.9869 - val_mDice: 0.3824

Epoch 00008: val_mDice improved from 0.34327 to 0.38238, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.5949 - acc: 0.9868 - mDice: 0.5327 - val_loss: 1.3819 - val_acc: 0.9872 - val_mDice: 0.4068

Epoch 00009: val_mDice improved from 0.38238 to 0.40677, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.5523 - acc: 0.9872 - mDice: 0.5568 - val_loss: 1.3823 - val_acc: 0.9873 - val_mDice: 0.4199

Epoch 00010: val_mDice improved from 0.40677 to 0.41993, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 14s - loss: 0.5643 - acc: 0.9873 - mDice: 0.5522 - val_loss: 1.3920 - val_acc: 0.9878 - val_mDice: 0.4349

Epoch 00011: val_mDice improved from 0.41993 to 0.43489, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.5086 - acc: 0.9877 - mDice: 0.5824 - val_loss: 1.1566 - val_acc: 0.9882 - val_mDice: 0.4799

Epoch 00012: val_mDice improved from 0.43489 to 0.47993, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.4779 - acc: 0.9880 - mDice: 0.6021 - val_loss: 1.2095 - val_acc: 0.9877 - val_mDice: 0.4689

Epoch 00013: val_mDice did not improve from 0.47993
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.4742 - acc: 0.9880 - mDice: 0.6059 - val_loss: 1.0895 - val_acc: 0.9883 - val_mDice: 0.5147

Epoch 00014: val_mDice improved from 0.47993 to 0.51472, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.4418 - acc: 0.9885 - mDice: 0.6252 - val_loss: 1.1258 - val_acc: 0.9886 - val_mDice: 0.5046

Epoch 00015: val_mDice did not improve from 0.51472
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 13s - loss: 0.4440 - acc: 0.9886 - mDice: 0.6249 - val_loss: 1.0278 - val_acc: 0.9886 - val_mDice: 0.5269

Epoch 00016: val_mDice improved from 0.51472 to 0.52686, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.4170 - acc: 0.9888 - mDice: 0.6420 - val_loss: 1.0991 - val_acc: 0.9888 - val_mDice: 0.5280

Epoch 00017: val_mDice improved from 0.52686 to 0.52799, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.4013 - acc: 0.9892 - mDice: 0.6527 - val_loss: 1.3017 - val_acc: 0.9884 - val_mDice: 0.4890

Epoch 00018: val_mDice did not improve from 0.52799
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3897 - acc: 0.9893 - mDice: 0.6604 - val_loss: 1.0783 - val_acc: 0.9898 - val_mDice: 0.5422

Epoch 00019: val_mDice improved from 0.52799 to 0.54218, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3818 - acc: 0.9896 - mDice: 0.6671 - val_loss: 1.1218 - val_acc: 0.9899 - val_mDice: 0.5405

Epoch 00020: val_mDice did not improve from 0.54218
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.4181 - acc: 0.9892 - mDice: 0.6423 - val_loss: 1.1429 - val_acc: 0.9903 - val_mDice: 0.5274

Epoch 00021: val_mDice did not improve from 0.54218
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3838 - acc: 0.9895 - mDice: 0.6644 - val_loss: 1.0671 - val_acc: 0.9900 - val_mDice: 0.5514

Epoch 00022: val_mDice improved from 0.54218 to 0.55142, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.4298 - acc: 0.9894 - mDice: 0.6567 - val_loss: 1.0427 - val_acc: 0.9890 - val_mDice: 0.5413

Epoch 00023: val_mDice did not improve from 0.55142
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3666 - acc: 0.9898 - mDice: 0.6766 - val_loss: 1.0329 - val_acc: 0.9903 - val_mDice: 0.5580

Epoch 00024: val_mDice improved from 0.55142 to 0.55805, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3588 - acc: 0.9899 - mDice: 0.6818 - val_loss: 0.9925 - val_acc: 0.9901 - val_mDice: 0.5554

Epoch 00025: val_mDice did not improve from 0.55805
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3565 - acc: 0.9901 - mDice: 0.6838 - val_loss: 1.0004 - val_acc: 0.9901 - val_mDice: 0.5522

Epoch 00026: val_mDice did not improve from 0.55805
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3527 - acc: 0.9902 - mDice: 0.6869 - val_loss: 1.1557 - val_acc: 0.9898 - val_mDice: 0.5341

Epoch 00027: val_mDice did not improve from 0.55805
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3502 - acc: 0.9902 - mDice: 0.6886 - val_loss: 0.9969 - val_acc: 0.9907 - val_mDice: 0.5638

Epoch 00028: val_mDice improved from 0.55805 to 0.56380, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3447 - acc: 0.9903 - mDice: 0.6922 - val_loss: 1.0246 - val_acc: 0.9905 - val_mDice: 0.5610

Epoch 00029: val_mDice did not improve from 0.56380
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3482 - acc: 0.9904 - mDice: 0.6900 - val_loss: 1.0569 - val_acc: 0.9907 - val_mDice: 0.5580

Epoch 00030: val_mDice did not improve from 0.56380
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.3452 - acc: 0.9904 - mDice: 0.6927 - val_loss: 0.9954 - val_acc: 0.9905 - val_mDice: 0.5621

Epoch 00031: val_mDice did not improve from 0.56380
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3432 - acc: 0.9905 - mDice: 0.6943 - val_loss: 0.9571 - val_acc: 0.9909 - val_mDice: 0.5771

Epoch 00032: val_mDice improved from 0.56380 to 0.57715, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3384 - acc: 0.9906 - mDice: 0.6975 - val_loss: 1.0008 - val_acc: 0.9914 - val_mDice: 0.5664

Epoch 00033: val_mDice did not improve from 0.57715
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3345 - acc: 0.9907 - mDice: 0.7009 - val_loss: 1.0790 - val_acc: 0.9911 - val_mDice: 0.5594

Epoch 00034: val_mDice did not improve from 0.57715
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.3740 - acc: 0.9904 - mDice: 0.6757 - val_loss: 1.1551 - val_acc: 0.9900 - val_mDice: 0.5067

Epoch 00035: val_mDice did not improve from 0.57715
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 10s - loss: 0.3491 - acc: 0.9906 - mDice: 0.6901 - val_loss: 0.9960 - val_acc: 0.9908 - val_mDice: 0.5713

Epoch 00036: val_mDice did not improve from 0.57715
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3315 - acc: 0.9909 - mDice: 0.7024 - val_loss: 1.0070 - val_acc: 0.9913 - val_mDice: 0.5762

Epoch 00037: val_mDice did not improve from 0.57715
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3263 - acc: 0.9910 - mDice: 0.7063 - val_loss: 1.0368 - val_acc: 0.9912 - val_mDice: 0.5722

Epoch 00038: val_mDice did not improve from 0.57715
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3199 - acc: 0.9911 - mDice: 0.7109 - val_loss: 0.9829 - val_acc: 0.9913 - val_mDice: 0.5745

Epoch 00039: val_mDice did not improve from 0.57715
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3206 - acc: 0.9910 - mDice: 0.7102 - val_loss: 1.0006 - val_acc: 0.9915 - val_mDice: 0.5779

Epoch 00040: val_mDice improved from 0.57715 to 0.57786, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3211 - acc: 0.9911 - mDice: 0.7100 - val_loss: 1.0312 - val_acc: 0.9919 - val_mDice: 0.5800

Epoch 00041: val_mDice improved from 0.57786 to 0.58001, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3211 - acc: 0.9911 - mDice: 0.7099 - val_loss: 1.0026 - val_acc: 0.9915 - val_mDice: 0.5812

Epoch 00042: val_mDice improved from 0.58001 to 0.58122, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3170 - acc: 0.9912 - mDice: 0.7132 - val_loss: 1.0077 - val_acc: 0.9913 - val_mDice: 0.5747

Epoch 00043: val_mDice did not improve from 0.58122
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3192 - acc: 0.9912 - mDice: 0.7119 - val_loss: 1.0457 - val_acc: 0.9916 - val_mDice: 0.5777

Epoch 00044: val_mDice did not improve from 0.58122
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3378 - acc: 0.9909 - mDice: 0.7011 - val_loss: 1.0031 - val_acc: 0.9913 - val_mDice: 0.5666

Epoch 00045: val_mDice did not improve from 0.58122
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3193 - acc: 0.9913 - mDice: 0.7115 - val_loss: 0.9708 - val_acc: 0.9917 - val_mDice: 0.5722

Epoch 00046: val_mDice did not improve from 0.58122
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3124 - acc: 0.9914 - mDice: 0.7164 - val_loss: 1.0089 - val_acc: 0.9916 - val_mDice: 0.5773

Epoch 00047: val_mDice did not improve from 0.58122
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3109 - acc: 0.9915 - mDice: 0.7178 - val_loss: 1.0757 - val_acc: 0.9915 - val_mDice: 0.5687

Epoch 00048: val_mDice did not improve from 0.58122
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3124 - acc: 0.9915 - mDice: 0.7169 - val_loss: 0.9826 - val_acc: 0.9918 - val_mDice: 0.5825

Epoch 00049: val_mDice improved from 0.58122 to 0.58252, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3121 - acc: 0.9915 - mDice: 0.7172 - val_loss: 1.0031 - val_acc: 0.9918 - val_mDice: 0.5733

Epoch 00050: val_mDice did not improve from 0.58252
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3062 - acc: 0.9915 - mDice: 0.7210 - val_loss: 1.0730 - val_acc: 0.9917 - val_mDice: 0.5671

Epoch 00051: val_mDice did not improve from 0.58252
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3399 - acc: 0.9916 - mDice: 0.7199 - val_loss: 1.0960 - val_acc: 0.9919 - val_mDice: 0.5648

Epoch 00052: val_mDice did not improve from 0.58252
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 10s - loss: 0.3094 - acc: 0.9916 - mDice: 0.7195 - val_loss: 1.0351 - val_acc: 0.9919 - val_mDice: 0.5674

Epoch 00053: val_mDice did not improve from 0.58252
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.3045 - acc: 0.9917 - mDice: 0.7224 - val_loss: 1.0392 - val_acc: 0.9917 - val_mDice: 0.5684

Epoch 00054: val_mDice did not improve from 0.58252
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3008 - acc: 0.9917 - mDice: 0.7253 - val_loss: 1.0214 - val_acc: 0.9918 - val_mDice: 0.5734

Epoch 00055: val_mDice did not improve from 0.58252
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3080 - acc: 0.9918 - mDice: 0.7217 - val_loss: 1.0780 - val_acc: 0.9916 - val_mDice: 0.5532

Epoch 00056: val_mDice did not improve from 0.58252
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3022 - acc: 0.9918 - mDice: 0.7243 - val_loss: 1.0369 - val_acc: 0.9917 - val_mDice: 0.5687

Epoch 00057: val_mDice did not improve from 0.58252
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3012 - acc: 0.9918 - mDice: 0.7254 - val_loss: 0.9964 - val_acc: 0.9921 - val_mDice: 0.5771

Epoch 00058: val_mDice did not improve from 0.58252
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.3002 - acc: 0.9918 - mDice: 0.7260 - val_loss: 1.0018 - val_acc: 0.9920 - val_mDice: 0.5758

Epoch 00059: val_mDice did not improve from 0.58252
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3022 - acc: 0.9919 - mDice: 0.7249 - val_loss: 1.0166 - val_acc: 0.9920 - val_mDice: 0.5744

Epoch 00060: val_mDice did not improve from 0.58252
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3049 - acc: 0.9919 - mDice: 0.7245 - val_loss: 1.0402 - val_acc: 0.9918 - val_mDice: 0.5637

Epoch 00061: val_mDice did not improve from 0.58252
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.3010 - acc: 0.9919 - mDice: 0.7254 - val_loss: 0.9754 - val_acc: 0.9922 - val_mDice: 0.5785

Epoch 00062: val_mDice did not improve from 0.58252
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.2942 - acc: 0.9919 - mDice: 0.7302 - val_loss: 0.9757 - val_acc: 0.9921 - val_mDice: 0.5767

Epoch 00063: val_mDice did not improve from 0.58252
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.2996 - acc: 0.9919 - mDice: 0.7267 - val_loss: 0.9700 - val_acc: 0.9922 - val_mDice: 0.5785

Epoch 00064: val_mDice did not improve from 0.58252
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.2983 - acc: 0.9920 - mDice: 0.7275 - val_loss: 0.9904 - val_acc: 0.9920 - val_mDice: 0.5762

Epoch 00065: val_mDice did not improve from 0.58252
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.2985 - acc: 0.9920 - mDice: 0.7272 - val_loss: 0.9717 - val_acc: 0.9921 - val_mDice: 0.5809

Epoch 00066: val_mDice did not improve from 0.58252
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.2911 - acc: 0.9920 - mDice: 0.7325 - val_loss: 0.9585 - val_acc: 0.9922 - val_mDice: 0.5826

Epoch 00067: val_mDice improved from 0.58252 to 0.58258, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.2979 - acc: 0.9920 - mDice: 0.7280 - val_loss: 1.0074 - val_acc: 0.9921 - val_mDice: 0.5813

Epoch 00068: val_mDice did not improve from 0.58258
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.2957 - acc: 0.9920 - mDice: 0.7296 - val_loss: 0.9779 - val_acc: 0.9923 - val_mDice: 0.5839

Epoch 00069: val_mDice improved from 0.58258 to 0.58393, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 11s - loss: 0.2943 - acc: 0.9920 - mDice: 0.7305 - val_loss: 1.0025 - val_acc: 0.9920 - val_mDice: 0.5711

Epoch 00070: val_mDice did not improve from 0.58393
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.2929 - acc: 0.9921 - mDice: 0.7313 - val_loss: 1.0096 - val_acc: 0.9921 - val_mDice: 0.5710

Epoch 00071: val_mDice did not improve from 0.58393
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 10s - loss: 0.2926 - acc: 0.9921 - mDice: 0.7318 - val_loss: 1.0267 - val_acc: 0.9919 - val_mDice: 0.5638

Epoch 00072: val_mDice did not improve from 0.58393
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.2886 - acc: 0.9922 - mDice: 0.7345 - val_loss: 0.9855 - val_acc: 0.9921 - val_mDice: 0.5756

Epoch 00073: val_mDice did not improve from 0.58393
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.2907 - acc: 0.9922 - mDice: 0.7333 - val_loss: 1.0277 - val_acc: 0.9922 - val_mDice: 0.5698

Epoch 00074: val_mDice did not improve from 0.58393
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.2916 - acc: 0.9922 - mDice: 0.7331 - val_loss: 1.0318 - val_acc: 0.9923 - val_mDice: 0.5758

Epoch 00075: val_mDice did not improve from 0.58393
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.2977 - acc: 0.9922 - mDice: 0.7280 - val_loss: 1.0051 - val_acc: 0.9923 - val_mDice: 0.5798

Epoch 00076: val_mDice did not improve from 0.58393
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.2897 - acc: 0.9922 - mDice: 0.7339 - val_loss: 1.0164 - val_acc: 0.9923 - val_mDice: 0.5745

Epoch 00077: val_mDice did not improve from 0.58393
Epoch 78/300

Epoch 00078: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.2894 - acc: 0.9922 - mDice: 0.7339 - val_loss: 0.9948 - val_acc: 0.9924 - val_mDice: 0.5769

Epoch 00078: val_mDice did not improve from 0.58393
Epoch 79/300

Epoch 00079: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.2864 - acc: 0.9923 - mDice: 0.7362 - val_loss: 1.0135 - val_acc: 0.9923 - val_mDice: 0.5754

Epoch 00079: val_mDice did not improve from 0.58393
Epoch 80/300

Epoch 00080: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.2889 - acc: 0.9923 - mDice: 0.7346 - val_loss: 1.0242 - val_acc: 0.9923 - val_mDice: 0.5734

Epoch 00080: val_mDice did not improve from 0.58393
Epoch 81/300

Epoch 00081: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.2888 - acc: 0.9923 - mDice: 0.7346 - val_loss: 1.0140 - val_acc: 0.9923 - val_mDice: 0.5764

Epoch 00081: val_mDice did not improve from 0.58393
Epoch 82/300

Epoch 00082: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.2920 - acc: 0.9923 - mDice: 0.7333 - val_loss: 1.0107 - val_acc: 0.9922 - val_mDice: 0.5710

Epoch 00082: val_mDice did not improve from 0.58393
Epoch 83/300

Epoch 00083: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.2901 - acc: 0.9923 - mDice: 0.7338 - val_loss: 1.0080 - val_acc: 0.9923 - val_mDice: 0.5757

Epoch 00083: val_mDice did not improve from 0.58393
Epoch 84/300

Epoch 00084: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.2888 - acc: 0.9923 - mDice: 0.7347 - val_loss: 1.0129 - val_acc: 0.9923 - val_mDice: 0.5749

Epoch 00084: val_mDice did not improve from 0.58393
Epoch 85/300

Epoch 00085: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.2964 - acc: 0.9922 - mDice: 0.7305 - val_loss: 1.0100 - val_acc: 0.9921 - val_mDice: 0.5646

Epoch 00085: val_mDice did not improve from 0.58393
Epoch 86/300

Epoch 00086: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.2887 - acc: 0.9924 - mDice: 0.7349 - val_loss: 1.0037 - val_acc: 0.9924 - val_mDice: 0.5763

Epoch 00086: val_mDice did not improve from 0.58393
Epoch 87/300

Epoch 00087: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.2882 - acc: 0.9924 - mDice: 0.7352 - val_loss: 0.9946 - val_acc: 0.9924 - val_mDice: 0.5766

Epoch 00087: val_mDice did not improve from 0.58393
Epoch 88/300

Epoch 00088: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.2858 - acc: 0.9924 - mDice: 0.7367 - val_loss: 1.0070 - val_acc: 0.9924 - val_mDice: 0.5748

Epoch 00088: val_mDice did not improve from 0.58393
Epoch 89/300

Epoch 00089: LearningRateScheduler setting learning rate to 6.25e-05.
 - 11s - loss: 0.2905 - acc: 0.9924 - mDice: 0.7342 - val_loss: 1.0110 - val_acc: 0.9924 - val_mDice: 0.5723

Epoch 00089: val_mDice did not improve from 0.58393
Epoch 90/300

Epoch 00090: LearningRateScheduler setting learning rate to 6.25e-05.
 - 10s - loss: 0.2929 - acc: 0.9925 - mDice: 0.7345 - val_loss: 1.0004 - val_acc: 0.9925 - val_mDice: 0.5787

Epoch 00090: val_mDice did not improve from 0.58393
Epoch 91/300

Epoch 00091: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2837 - acc: 0.9924 - mDice: 0.7385 - val_loss: 1.0163 - val_acc: 0.9924 - val_mDice: 0.5763

Epoch 00091: val_mDice did not improve from 0.58393
Epoch 92/300

Epoch 00092: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2867 - acc: 0.9924 - mDice: 0.7369 - val_loss: 1.0205 - val_acc: 0.9924 - val_mDice: 0.5747

Epoch 00092: val_mDice did not improve from 0.58393
Epoch 93/300

Epoch 00093: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2848 - acc: 0.9925 - mDice: 0.7375 - val_loss: 1.0540 - val_acc: 0.9924 - val_mDice: 0.5721

Epoch 00093: val_mDice did not improve from 0.58393
Epoch 94/300

Epoch 00094: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.2858 - acc: 0.9925 - mDice: 0.7369 - val_loss: 1.0405 - val_acc: 0.9925 - val_mDice: 0.5732

Epoch 00094: val_mDice did not improve from 0.58393
Epoch 95/300

Epoch 00095: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2871 - acc: 0.9925 - mDice: 0.7358 - val_loss: 1.0486 - val_acc: 0.9924 - val_mDice: 0.5726

Epoch 00095: val_mDice did not improve from 0.58393
Epoch 96/300

Epoch 00096: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.3411 - acc: 0.9924 - mDice: 0.7293 - val_loss: 1.0138 - val_acc: 0.9925 - val_mDice: 0.5708

Epoch 00096: val_mDice did not improve from 0.58393
Epoch 97/300

Epoch 00097: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2852 - acc: 0.9925 - mDice: 0.7372 - val_loss: 0.9965 - val_acc: 0.9925 - val_mDice: 0.5762

Epoch 00097: val_mDice did not improve from 0.58393
Epoch 98/300

Epoch 00098: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2856 - acc: 0.9925 - mDice: 0.7371 - val_loss: 1.0228 - val_acc: 0.9925 - val_mDice: 0.5737

Epoch 00098: val_mDice did not improve from 0.58393
Epoch 99/300

Epoch 00099: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.2821 - acc: 0.9925 - mDice: 0.7399 - val_loss: 1.0226 - val_acc: 0.9925 - val_mDice: 0.5749

Epoch 00099: val_mDice did not improve from 0.58393
Epoch 100/300

Epoch 00100: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2844 - acc: 0.9925 - mDice: 0.7381 - val_loss: 1.0239 - val_acc: 0.9924 - val_mDice: 0.5741

Epoch 00100: val_mDice did not improve from 0.58393
Epoch 101/300

Epoch 00101: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2925 - acc: 0.9925 - mDice: 0.7341 - val_loss: 1.0452 - val_acc: 0.9924 - val_mDice: 0.5704

Epoch 00101: val_mDice did not improve from 0.58393
Epoch 102/300

Epoch 00102: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2865 - acc: 0.9925 - mDice: 0.7362 - val_loss: 1.0065 - val_acc: 0.9924 - val_mDice: 0.5713

Epoch 00102: val_mDice did not improve from 0.58393
Epoch 103/300

Epoch 00103: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.2844 - acc: 0.9925 - mDice: 0.7380 - val_loss: 1.0199 - val_acc: 0.9924 - val_mDice: 0.5732

Epoch 00103: val_mDice did not improve from 0.58393
Epoch 104/300

Epoch 00104: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2845 - acc: 0.9926 - mDice: 0.7382 - val_loss: 1.0180 - val_acc: 0.9925 - val_mDice: 0.5736

Epoch 00104: val_mDice did not improve from 0.58393
Epoch 105/300

Epoch 00105: LearningRateScheduler setting learning rate to 3.125e-05.
 - 11s - loss: 0.2826 - acc: 0.9926 - mDice: 0.7396 - val_loss: 1.0380 - val_acc: 0.9924 - val_mDice: 0.5712

Epoch 00105: val_mDice did not improve from 0.58393
Epoch 106/300

Epoch 00106: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2850 - acc: 0.9926 - mDice: 0.7376 - val_loss: 1.0363 - val_acc: 0.9925 - val_mDice: 0.5719

Epoch 00106: val_mDice did not improve from 0.58393
Epoch 107/300

Epoch 00107: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2849 - acc: 0.9926 - mDice: 0.7378 - val_loss: 1.0267 - val_acc: 0.9925 - val_mDice: 0.5741

Epoch 00107: val_mDice did not improve from 0.58393
Epoch 108/300

Epoch 00108: LearningRateScheduler setting learning rate to 3.125e-05.
 - 10s - loss: 0.2822 - acc: 0.9926 - mDice: 0.7397 - val_loss: 1.0336 - val_acc: 0.9925 - val_mDice: 0.5745

Epoch 00108: val_mDice did not improve from 0.58393
Epoch 109/300

Epoch 00109: LearningRateScheduler setting learning rate to 1.5625e-05.
 - 11s - loss: 0.3127 - acc: 0.9926 - mDice: 0.7376 - val_loss: 1.0183 - val_acc: 0.9925 - val_mDice: 0.5767

Epoch 00109: val_mDice did not improve from 0.58393
Restoring model weights from the end of the best epoch
Epoch 00109: early stopping
{'val_loss': [5.42959326727622, 3.209204336386394, 7.236569130020072, 4.0858849409382305, 2.0047791935334307, 1.6854392276039782, 1.903213025438688, 1.4865770956042894, 1.381893715026918, 1.3823195242210478, 1.391995145557795, 1.1565576387902587, 1.2094758378712294, 1.0895339621943199, 1.1258157163614797, 1.0278367264285941, 1.0991305422501387, 1.301735487447664, 1.078337303407619, 1.1217562971496235, 1.142946537790463, 1.0670951301676916, 1.0426961606897949, 1.0328821201523686, 0.9924501872517433, 1.000392426803478, 1.1557042537008384, 0.9968536902729973, 1.0245714031708013, 1.0569419197122365, 0.9953637026744361, 0.9571294962114686, 1.000755229394724, 1.0789988294284416, 1.1550915570393354, 0.9960438080206446, 1.0070296167569417, 1.036838076744807, 0.9829304473818052, 1.000627590676636, 1.031222841178797, 1.0025587633022062, 1.007721394977171, 1.0457364798241806, 1.0030838728600693, 0.970792322674196, 1.0089447446350182, 1.0757081592007185, 0.9826220681080484, 1.0031142384219884, 1.0729571253900416, 1.0960143803467435, 1.0350790337797298, 1.0391530148231582, 1.021428784817376, 1.0779641197552798, 1.0369061309570187, 0.9963972233946382, 1.001788802519373, 1.0166492867101657, 1.0401984102394666, 0.9753547298377693, 0.9757430811127569, 0.969994425340526, 0.9904211910067635, 0.9716622759059382, 0.9584940841910408, 1.0074127888701159, 0.977943103601454, 1.0025033170152642, 1.0096492786823241, 1.0266513815801, 0.9855217382541902, 1.0276824109235967, 1.0317874945476855, 1.005102258828637, 1.0164375698209134, 0.9947983526728783, 1.013471682649867, 1.0241802457676055, 1.013971090425046, 1.0107428229580566, 1.0079944920907986, 1.0128722654268159, 1.0099755427059967, 1.0036878894395334, 0.994648520238393, 1.0070099206971213, 1.0110070133728941, 1.0004348043524927, 1.0163076750047633, 1.0205104362953803, 1.0539546983010328, 1.040458668913222, 1.0486318486912698, 1.0137936180012537, 0.9964917233377885, 1.0228477517222405, 1.0225696644925941, 1.023900917938468, 1.0452344941616492, 1.0065419638189372, 1.0198504333600036, 1.0180394442702942, 1.038021608334471, 1.0363171534793796, 1.0266650085769709, 1.0336141106651437, 1.0183468366297237], 'val_acc': [0.6488457891098701, 0.9866213882976397, 0.9865987374498886, 0.9866223194510367, 0.9869476871854278, 0.9869185832280879, 0.9869039446304972, 0.9869131316497692, 0.9871604557366506, 0.9872835141544879, 0.9878210603487048, 0.9882019941206958, 0.9877309279480811, 0.9883057419329097, 0.9886031046875167, 0.9886410654294935, 0.988807622786547, 0.9883786804777833, 0.9897992091217872, 0.9899137626443528, 0.9903147865922531, 0.989993511601863, 0.9890220100288495, 0.9903032175526632, 0.9900933399079173, 0.9901251669475752, 0.9898057542639792, 0.9907493483034943, 0.9904923283111822, 0.9907334374905066, 0.990521351063284, 0.9908964976086387, 0.9914466306147631, 0.9910931766953499, 0.9900169095378047, 0.9907842394874704, 0.9912764190219945, 0.9912066312403597, 0.9913302905769591, 0.991511053437, 0.9918613565931744, 0.9915435680143406, 0.9913239024315608, 0.9915929462127963, 0.9913160796602892, 0.9916770964942121, 0.9915525872094538, 0.9915407528994193, 0.991808757253607, 0.9917806764822673, 0.9916954759464385, 0.9918572638491735, 0.9918630510758097, 0.9917472361652121, 0.9918171105657676, 0.9915799912704759, 0.9917111431437119, 0.9921116636191358, 0.9920051115193658, 0.9920345457113406, 0.9918452671286628, 0.9921714902248955, 0.9920922609809092, 0.9921578748133051, 0.9920499476381696, 0.9921224585021224, 0.9922245984506217, 0.9921441673691115, 0.9922520079253263, 0.9920330190528641, 0.9921302921007591, 0.9919431465090025, 0.9921250191739642, 0.9922314034495756, 0.9922803431397454, 0.9922646867698371, 0.9922992368918132, 0.99236664265313, 0.9922856214802228, 0.9923083372921645, 0.9923205992831197, 0.9922216209252243, 0.9923096203349266, 0.9923105514883237, 0.9920884280936703, 0.9923662095585268, 0.9923789858493233, 0.9923910691887546, 0.9923983830738674, 0.9924744615546147, 0.9924215319804123, 0.9924308110322875, 0.9924159991968555, 0.9924518431889588, 0.9924430730232426, 0.9925289232009763, 0.9925343856066601, 0.9924633201959456, 0.9924667308159464, 0.9924408479997183, 0.9924017070749475, 0.9923671461256064, 0.9924228854010475, 0.9924547340954357, 0.9924070503796154, 0.9924627301045487, 0.9924717438859793, 0.9924937938149684, 0.9925231413880226], 'val_mDice': [0.007692927056475188, 0.135227312400707, 0.05793565161844257, 0.13433549888127505, 0.3023494924879637, 0.34326799180050743, 0.3147074612674661, 0.3823793578645947, 0.40676547222847725, 0.41993133432750374, 0.43488749988938763, 0.479928879820142, 0.4688695465619731, 0.5147180483624894, 0.504588071158753, 0.5268550373877752, 0.5279909849816512, 0.48901997134427827, 0.5421821904334018, 0.540479543964826, 0.5273872903538876, 0.5514200658391109, 0.5413152487249834, 0.5580456228715306, 0.5554062577402668, 0.5522167487755134, 0.5340809853481878, 0.5637981911121337, 0.560954673305411, 0.5580068554908552, 0.5621022494893849, 0.5771478396129002, 0.566412358258011, 0.5594333445993366, 0.5066502741203862, 0.5713012465339266, 0.5762193199724203, 0.5722079382064016, 0.5744603251891175, 0.5778595196128866, 0.580006291062912, 0.5812169258211224, 0.5746812169709928, 0.5776832609367197, 0.5665866707153043, 0.5721542128208655, 0.5772945422892349, 0.5686787910400792, 0.5825223649879893, 0.5732780743684691, 0.5670505637585521, 0.5647931934810573, 0.5674467830631973, 0.5684309120507375, 0.5733516815031365, 0.5531802705804615, 0.568701517679386, 0.5770889847631567, 0.5758296295689194, 0.5744496223387341, 0.563699529454667, 0.5785181211624006, 0.5766558928667254, 0.5785013982970318, 0.5762471056980615, 0.5809184857436899, 0.5825808652415263, 0.5812923681941712, 0.5839331977265624, 0.5711385897351438, 0.5710304405990241, 0.5638304674874426, 0.5756343688670339, 0.5698243858162432, 0.575781694116861, 0.5798051240333745, 0.5745044223402545, 0.5769144828269743, 0.5754453474342769, 0.5733858120647156, 0.5763590146367058, 0.5710416983518678, 0.575737537415216, 0.574856417272223, 0.5646312296336397, 0.5762575730532543, 0.5766161970394942, 0.5747971347415588, 0.5723222183379988, 0.5786944069069803, 0.5762558054859047, 0.5746657554936561, 0.5721300677967331, 0.5732105522129776, 0.5726274228117663, 0.5708490632860147, 0.576194235674367, 0.5736668498596206, 0.574926746422112, 0.5740937999332092, 0.5704445393923517, 0.5713496908721438, 0.5731662655829517, 0.5735676874365188, 0.5712410788661669, 0.5719273930566079, 0.5740969615238131, 0.57453262221261, 0.5766929820058131], 'loss': [5.200143010930862, 2.754053667059074, 1.516548971212601, 1.0875980568137915, 0.9000472516813642, 0.7779755978196234, 0.6881874877633355, 0.6398055249900912, 0.5949207835367958, 0.5522594422506198, 0.5643232121567544, 0.5085833025992872, 0.47792507698885756, 0.4742457917012945, 0.44184353142866517, 0.4439776124921886, 0.41696205830750427, 0.4012848892135479, 0.38970775125059043, 0.3818377042522266, 0.41811176601673317, 0.38377736848761795, 0.42975208493990025, 0.36662841563454096, 0.3588486677621355, 0.3565284082098748, 0.352664308836075, 0.3502417834413625, 0.34471849511054825, 0.3482296122134687, 0.34522507972693767, 0.34320404423738377, 0.33836856763695083, 0.33446453936149984, 0.3739860509826575, 0.3490551700206926, 0.3315288398768546, 0.32631369741718513, 0.3199274024010881, 0.3206070899669398, 0.3210647843327975, 0.32113353865802213, 0.3169960870916247, 0.31919089235301085, 0.33783506013168213, 0.3193239362039931, 0.3123937797825669, 0.3109470357436287, 0.3123917255904319, 0.3120510943426892, 0.3061716347822573, 0.3399381055490891, 0.30940881235531903, 0.3045217050136091, 0.30080701059830617, 0.30797242809606096, 0.3022242888863372, 0.30122254419561967, 0.30023490280170184, 0.30216552142591274, 0.3049039651080507, 0.30101947065640317, 0.29420312603363713, 0.2996006734788638, 0.29830623602749534, 0.29849838633572573, 0.29111117539805637, 0.297941479487425, 0.2957225881801728, 0.29426165173144875, 0.2928646152836768, 0.29258167232890897, 0.28859200283572645, 0.29073250223911207, 0.29157751093075396, 0.2976658302009032, 0.2896930432790011, 0.28938405202878537, 0.28644736520900094, 0.2889336228885133, 0.28878169553641475, 0.2920135663484674, 0.29010259006379124, 0.2888218033989908, 0.29643944317403764, 0.2886997761547051, 0.28820863619685616, 0.2858490098713359, 0.2904973319294126, 0.2928958993804734, 0.2836671331954796, 0.28667386965745767, 0.28480322308075856, 0.285770433678139, 0.2871471486535525, 0.34111203663005196, 0.2851984805890282, 0.2856395099445277, 0.2820812024626279, 0.2843902725473726, 0.29250357712535174, 0.2864719341921307, 0.28439013176722827, 0.2845451701363565, 0.2825729647782228, 0.2850404005088877, 0.28487535102152794, 0.28222072642622686, 0.31273506466028284], 'acc': [0.25718692832531087, 0.8852806768199812, 0.9787734959334246, 0.9840374665401426, 0.9849667343346611, 0.9853932174742589, 0.9858684799968094, 0.9863751697481487, 0.9867993888666833, 0.9872181317809183, 0.9873365111621476, 0.9877266667774074, 0.9879876999907958, 0.9880240914559981, 0.9885123615347208, 0.9886498165336401, 0.9888393651836457, 0.9891924808116495, 0.9893193129546546, 0.9895689141294666, 0.9891776189704123, 0.9894801736171972, 0.9894449587373934, 0.9897920351140155, 0.9899465858421843, 0.9900509373327365, 0.990156434246703, 0.9901988783394218, 0.9903314549443754, 0.9903543510654558, 0.9904404068698719, 0.9905224905502047, 0.9906412820927752, 0.9907303192318588, 0.9903935697457941, 0.9905753593556537, 0.9908922405043012, 0.9909940777959718, 0.99108447386216, 0.9910464887260362, 0.9911325621693113, 0.991116293188088, 0.9911580495122917, 0.9912449120740268, 0.9908589164366999, 0.9913037437692964, 0.9914044989169599, 0.9914708134572398, 0.9914604673844231, 0.9915096146110836, 0.991523918623401, 0.9915814148365496, 0.9915943536011711, 0.9917042770603288, 0.991674697869509, 0.991762299425946, 0.99176751891135, 0.9917742906163565, 0.9918055166886561, 0.991915441985195, 0.9918742602468272, 0.991897833185631, 0.991945433851823, 0.9919174054107007, 0.9920195822504093, 0.99197648757777, 0.9920040809270338, 0.9920250826346448, 0.9920325290995079, 0.9920441294421986, 0.9921019666732901, 0.9921170327578167, 0.9921847413559289, 0.992173097063081, 0.9922440814913127, 0.9921582845371543, 0.9922406896856946, 0.9922369387823027, 0.99226487623631, 0.9922736345178249, 0.9922828339178082, 0.9923227427185096, 0.9923141463470224, 0.9923442594073115, 0.9922167939557688, 0.9923700713052996, 0.9923995304048869, 0.9924399731485529, 0.9924181062549022, 0.9924656414368297, 0.992438444667915, 0.9924422118872515, 0.9924633308197802, 0.9924895118843906, 0.9924528427541477, 0.9923681928402986, 0.9925405593306451, 0.99251925651301, 0.9925433092287819, 0.9925014838925419, 0.9925260501195999, 0.9924507609278, 0.9925300659733548, 0.9925842297650442, 0.9925579593302142, 0.9925505664433857, 0.992581664413471, 0.992581738717165, 0.9925856453569838], 'mDice': [0.01009885055092238, 0.08921320521434638, 0.22527811752515128, 0.3304586171295434, 0.3941118681180933, 0.44385739102610533, 0.4854167043133817, 0.5098412377913695, 0.5326571610059162, 0.5568422061447444, 0.5522384071761671, 0.582433570327947, 0.6021281331593834, 0.6058966250660681, 0.6251609964817578, 0.6249097623895629, 0.6420051197529427, 0.652683595914288, 0.6603778331700147, 0.6670929303739573, 0.6422878464846958, 0.664395635210336, 0.6566878768136557, 0.6765713078761071, 0.6817704567779889, 0.6838454660740204, 0.686856325986794, 0.6886360493893982, 0.6922011506248784, 0.6899574550394654, 0.6926967257636102, 0.6942997467503154, 0.6974926551232061, 0.7008564361220664, 0.6757439585855063, 0.6901146455698919, 0.7024328544754282, 0.7063025164839372, 0.7109264786528601, 0.7101502296515958, 0.7099676487919141, 0.7099270880883778, 0.7131971817569286, 0.7118881457977907, 0.7011461916451978, 0.7114642119878023, 0.7164116113018313, 0.7177713793391217, 0.7168868994007275, 0.7171910649898167, 0.7209733095474807, 0.7199373613450442, 0.7195191137152741, 0.7224157110095465, 0.72525338135871, 0.7216936217577331, 0.7242963608037382, 0.7253684031831057, 0.7260083501053797, 0.7249355966595039, 0.7245337486855051, 0.7254202334272288, 0.7302292755146944, 0.7267070517734006, 0.7275262042126732, 0.7272067226552199, 0.7325255307845505, 0.7279810282216854, 0.7295819176992565, 0.7305070513861688, 0.7313040235480603, 0.7318243105814284, 0.7344961535327973, 0.7332845422842352, 0.7331162140636938, 0.7279932931083933, 0.7338804416680013, 0.7339470190272819, 0.7361744318878607, 0.7345658113718326, 0.7346372643322597, 0.7332912829742478, 0.7338266805567665, 0.7346980652591597, 0.7304830864237211, 0.7349010328182309, 0.7352380748153762, 0.7367232502903215, 0.7342046328008396, 0.7345092348047014, 0.7384691513275542, 0.7368678670335198, 0.7375036713227391, 0.7368955462987267, 0.7357993211170013, 0.7292837323448661, 0.7372234326990553, 0.7370785009552312, 0.7398573618781846, 0.7380825827212869, 0.734149154399679, 0.7362274450379735, 0.7379616410752848, 0.7381645052265444, 0.7395593171202005, 0.7375706027086213, 0.7378155662598945, 0.7396993787015264, 0.7375910371064258], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 1.5625e-05]}
predicting test subjects:   0%|          | 0/10 [00:00<?, ?it/s]predicting test subjects:  10%|█         | 1/10 [00:04<00:37,  4.15s/it]predicting test subjects:  20%|██        | 2/10 [00:07<00:31,  3.97s/it]predicting test subjects:  30%|███       | 3/10 [00:12<00:30,  4.35s/it]predicting test subjects:  40%|████      | 4/10 [00:18<00:28,  4.82s/it]predicting test subjects:  50%|█████     | 5/10 [00:24<00:25,  5.07s/it]predicting test subjects:  60%|██████    | 6/10 [00:29<00:19,  4.92s/it]predicting test subjects:  70%|███████   | 7/10 [00:32<00:13,  4.57s/it]predicting test subjects:  80%|████████  | 8/10 [00:36<00:08,  4.35s/it]predicting test subjects:  90%|█████████ | 9/10 [00:40<00:04,  4.34s/it]predicting test subjects: 100%|██████████| 10/10 [00:45<00:00,  4.38s/it]
predicting train subjects:   0%|          | 0/42 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/42 [00:03<02:35,  3.80s/it]predicting train subjects:   5%|▍         | 2/42 [00:07<02:27,  3.69s/it]predicting train subjects:   7%|▋         | 3/42 [00:10<02:18,  3.54s/it]predicting train subjects:  10%|▉         | 4/42 [00:13<02:08,  3.38s/it]predicting train subjects:  12%|█▏        | 5/42 [00:17<02:17,  3.72s/it]predicting train subjects:  14%|█▍        | 6/42 [00:21<02:09,  3.59s/it]predicting train subjects:  17%|█▋        | 7/42 [00:24<01:58,  3.40s/it]predicting train subjects:  19%|█▉        | 8/42 [00:26<01:40,  2.96s/it]predicting train subjects:  21%|██▏       | 9/42 [00:29<01:46,  3.22s/it]predicting train subjects:  24%|██▍       | 10/42 [00:34<01:52,  3.51s/it]predicting train subjects:  26%|██▌       | 11/42 [00:38<01:54,  3.70s/it]predicting train subjects:  29%|██▊       | 12/42 [00:40<01:33,  3.13s/it]predicting train subjects:  31%|███       | 13/42 [00:43<01:33,  3.21s/it]predicting train subjects:  33%|███▎      | 14/42 [00:47<01:32,  3.32s/it]predicting train subjects:  36%|███▌      | 15/42 [00:48<01:17,  2.88s/it]predicting train subjects:  38%|███▊      | 16/42 [00:52<01:20,  3.11s/it]predicting train subjects:  40%|████      | 17/42 [00:57<01:30,  3.61s/it]predicting train subjects:  43%|████▎     | 18/42 [01:01<01:27,  3.66s/it]predicting train subjects:  45%|████▌     | 19/42 [01:04<01:21,  3.56s/it]predicting train subjects:  48%|████▊     | 20/42 [01:07<01:14,  3.36s/it]predicting train subjects:  50%|█████     | 21/42 [01:10<01:11,  3.42s/it]predicting train subjects:  52%|█████▏    | 22/42 [01:14<01:10,  3.51s/it]predicting train subjects:  55%|█████▍    | 23/42 [01:16<00:57,  3.01s/it]predicting train subjects:  57%|█████▋    | 24/42 [01:20<00:57,  3.18s/it]predicting train subjects:  60%|█████▉    | 25/42 [01:23<00:54,  3.22s/it]predicting train subjects:  62%|██████▏   | 26/42 [01:26<00:53,  3.33s/it]predicting train subjects:  64%|██████▍   | 27/42 [01:30<00:52,  3.49s/it]predicting train subjects:  67%|██████▋   | 28/42 [01:34<00:51,  3.66s/it]predicting train subjects:  69%|██████▉   | 29/42 [01:38<00:47,  3.67s/it]predicting train subjects:  71%|███████▏  | 30/42 [01:41<00:42,  3.54s/it]predicting train subjects:  74%|███████▍  | 31/42 [01:46<00:41,  3.76s/it]predicting train subjects:  76%|███████▌  | 32/42 [01:50<00:38,  3.85s/it]predicting train subjects:  79%|███████▊  | 33/42 [01:53<00:33,  3.69s/it]predicting train subjects:  81%|████████  | 34/42 [01:57<00:30,  3.76s/it]predicting train subjects:  83%|████████▎ | 35/42 [02:00<00:24,  3.55s/it]predicting train subjects:  86%|████████▌ | 36/42 [02:03<00:20,  3.47s/it]predicting train subjects:  88%|████████▊ | 37/42 [02:07<00:17,  3.53s/it]predicting train subjects:  90%|█████████ | 38/42 [02:10<00:14,  3.53s/it]predicting train subjects:  93%|█████████▎| 39/42 [02:14<00:10,  3.49s/it]predicting train subjects:  95%|█████████▌| 40/42 [02:17<00:06,  3.44s/it]predicting train subjects:  98%|█████████▊| 41/42 [02:21<00:03,  3.48s/it]predicting train subjects: 100%|██████████| 42/42 [02:25<00:00,  3.82s/it]
Loading train:   0%|          | 0/42 [00:00<?, ?it/s]Loading train:   2%|▏         | 1/42 [00:02<01:50,  2.68s/it]Loading train:   5%|▍         | 2/42 [00:05<01:47,  2.69s/it]Loading train:   7%|▋         | 3/42 [00:07<01:43,  2.66s/it]Loading train:  10%|▉         | 4/42 [00:10<01:37,  2.56s/it]Loading train:  12%|█▏        | 5/42 [00:13<01:36,  2.61s/it]Loading train:  14%|█▍        | 6/42 [00:15<01:29,  2.50s/it]Loading train:  17%|█▋        | 7/42 [00:17<01:24,  2.41s/it]Loading train:  19%|█▉        | 8/42 [00:19<01:19,  2.34s/it]Loading train:  21%|██▏       | 9/42 [00:22<01:18,  2.38s/it]Loading train:  24%|██▍       | 10/42 [00:25<01:22,  2.57s/it]Loading train:  26%|██▌       | 11/42 [00:27<01:19,  2.56s/it]Loading train:  29%|██▊       | 12/42 [00:29<01:14,  2.48s/it]Loading train:  31%|███       | 13/42 [00:32<01:13,  2.52s/it]Loading train:  33%|███▎      | 14/42 [00:34<01:09,  2.47s/it]Loading train:  36%|███▌      | 15/42 [00:37<01:05,  2.44s/it]Loading train:  38%|███▊      | 16/42 [00:39<01:02,  2.42s/it]Loading train:  40%|████      | 17/42 [00:42<01:01,  2.48s/it]Loading train:  43%|████▎     | 18/42 [00:44<00:59,  2.46s/it]Loading train:  45%|████▌     | 19/42 [00:46<00:53,  2.34s/it]Loading train:  48%|████▊     | 20/42 [00:48<00:50,  2.30s/it]Loading train:  50%|█████     | 21/42 [00:51<00:47,  2.26s/it]Loading train:  52%|█████▏    | 22/42 [00:53<00:46,  2.31s/it]Loading train:  55%|█████▍    | 23/42 [00:55<00:42,  2.23s/it]Loading train:  57%|█████▋    | 24/42 [00:57<00:40,  2.26s/it]Loading train:  60%|█████▉    | 25/42 [01:00<00:38,  2.26s/it]Loading train:  62%|██████▏   | 26/42 [01:02<00:37,  2.33s/it]Loading train:  64%|██████▍   | 27/42 [01:05<00:35,  2.39s/it]Loading train:  67%|██████▋   | 28/42 [01:07<00:34,  2.43s/it]Loading train:  69%|██████▉   | 29/42 [01:10<00:32,  2.47s/it]Loading train:  71%|███████▏  | 30/42 [01:12<00:29,  2.43s/it]Loading train:  74%|███████▍  | 31/42 [01:15<00:27,  2.55s/it]Loading train:  76%|███████▌  | 32/42 [01:17<00:24,  2.48s/it]Loading train:  79%|███████▊  | 33/42 [01:20<00:21,  2.42s/it]Loading train:  81%|████████  | 34/42 [01:22<00:18,  2.32s/it]Loading train:  83%|████████▎ | 35/42 [01:24<00:15,  2.28s/it]Loading train:  86%|████████▌ | 36/42 [01:26<00:13,  2.30s/it]Loading train:  88%|████████▊ | 37/42 [01:28<00:11,  2.24s/it]Loading train:  90%|█████████ | 38/42 [01:31<00:08,  2.23s/it]Loading train:  93%|█████████▎| 39/42 [01:33<00:06,  2.22s/it]Loading train:  95%|█████████▌| 40/42 [01:35<00:04,  2.23s/it]Loading train:  98%|█████████▊| 41/42 [01:37<00:02,  2.30s/it]Loading train: 100%|██████████| 42/42 [01:40<00:00,  2.32s/it]
concatenating: train:   0%|          | 0/42 [00:00<?, ?it/s]concatenating: train:  10%|▉         | 4/42 [00:00<00:01, 36.92it/s]concatenating: train:  29%|██▊       | 12/42 [00:00<00:00, 43.41it/s]concatenating: train:  48%|████▊     | 20/42 [00:00<00:00, 49.51it/s]concatenating: train:  67%|██████▋   | 28/42 [00:00<00:00, 54.87it/s]concatenating: train:  86%|████████▌ | 36/42 [00:00<00:00, 59.85it/s]concatenating: train: 100%|██████████| 42/42 [00:00<00:00, 69.14it/s]
Loading test:   0%|          | 0/10 [00:00<?, ?it/s]Loading test:  10%|█         | 1/10 [00:02<00:21,  2.34s/it]Loading test:  20%|██        | 2/10 [00:04<00:19,  2.40s/it]Loading test:  30%|███       | 3/10 [00:07<00:16,  2.40s/it]Loading test:  40%|████      | 4/10 [00:10<00:15,  2.53s/it]Loading test:  50%|█████     | 5/10 [00:13<00:13,  2.73s/it]Loading test:  60%|██████    | 6/10 [00:16<00:11,  2.78s/it]Loading test:  70%|███████   | 7/10 [00:18<00:07,  2.62s/it]Loading test:  80%|████████  | 8/10 [00:20<00:05,  2.51s/it]Loading test:  90%|█████████ | 9/10 [00:23<00:02,  2.52s/it]Loading test: 100%|██████████| 10/10 [00:25<00:00,  2.50s/it]
concatenating: validation:   0%|          | 0/10 [00:00<?, ?it/s]concatenating: validation:  30%|███       | 3/10 [00:00<00:00, 23.28it/s]concatenating: validation:  60%|██████    | 6/10 [00:00<00:00, 24.20it/s]concatenating: validation: 100%|██████████| 10/10 [00:00<00:00, 30.18it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 0  | Dropout 0.3  | LR 0.001  | NL 3  |  normal |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c
---------------------------------------------------------------
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 128, 92, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 128, 92, 20)  200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 128, 92, 20)  80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 128, 92, 20)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 128, 92, 20)  3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 128, 92, 20)  80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 128, 92, 20)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 64, 46, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 64, 46, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 64, 46, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 64, 46, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 64, 46, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 64, 46, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 64, 46, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 64, 46, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 64, 46, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 32, 23, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 32, 23, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 23, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 23, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 23, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 23, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 23, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 23, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 32, 23, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 32, 23, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 64, 46, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 64, 46, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 64, 46, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 64, 46, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 64, 46, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 64, 46, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 64, 46, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 64, 46, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 64, 46, 140)  0           concatenate_3[0][0]              2019-07-29 01:50:46.265821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-29 01:50:46.265913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-29 01:50:46.265929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-29 01:50:46.265939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-29 01:50:46.266332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 64, 46, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 128, 92, 20)  11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 128, 92, 40)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 128, 92, 20)  7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 128, 92, 20)  80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 128, 92, 20)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 128, 92, 20)  3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 128, 92, 20)  80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 128, 92, 20)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 128, 92, 60)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 128, 92, 60)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 128, 92, 13)  793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.55790764e-02 3.00677812e-02 7.53936780e-02 1.00199220e-02
 2.70601151e-02 6.80014415e-03 7.86794050e-02 1.15058055e-01
 7.52388973e-02 1.34503594e-02 3.22329524e-01 1.80292052e-01
 3.09906439e-05]
Train on 4074 samples, validate on 1003 samples
Epoch 1/300

Epoch 00001: LearningRateScheduler setting learning rate to 0.001.
 - 18s - loss: 5.5524 - acc: 0.1411 - mDice: 0.0059 - val_loss: 5.1788 - val_acc: 0.2427 - val_mDice: 0.0092

Epoch 00001: val_mDice improved from -inf to 0.00920, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/300

Epoch 00002: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 3.3735 - acc: 0.5330 - mDice: 0.0738 - val_loss: 3.4545 - val_acc: 0.9678 - val_mDice: 0.1127

Epoch 00002: val_mDice improved from 0.00920 to 0.11272, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 3/300

Epoch 00003: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 2.3408 - acc: 0.9624 - mDice: 0.1435 - val_loss: 2.7881 - val_acc: 0.9818 - val_mDice: 0.1273

Epoch 00003: val_mDice improved from 0.11272 to 0.12729, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/300

Epoch 00004: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.6526 - acc: 0.9831 - mDice: 0.1922 - val_loss: 2.1372 - val_acc: 0.9826 - val_mDice: 0.1998

Epoch 00004: val_mDice improved from 0.12729 to 0.19980, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 5/300

Epoch 00005: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 1.3962 - acc: 0.9835 - mDice: 0.2452 - val_loss: 2.1813 - val_acc: 0.9844 - val_mDice: 0.2228

Epoch 00005: val_mDice improved from 0.19980 to 0.22280, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 6/300

Epoch 00006: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 1.2425 - acc: 0.9838 - mDice: 0.2744 - val_loss: 2.8641 - val_acc: 0.9858 - val_mDice: 0.2306

Epoch 00006: val_mDice improved from 0.22280 to 0.23063, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 7/300

Epoch 00007: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.1378 - acc: 0.9843 - mDice: 0.3051 - val_loss: 3.0330 - val_acc: 0.9863 - val_mDice: 0.2254

Epoch 00007: val_mDice did not improve from 0.23063
Epoch 8/300

Epoch 00008: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.1283 - acc: 0.9845 - mDice: 0.3120 - val_loss: 3.8346 - val_acc: 0.9860 - val_mDice: 0.1821

Epoch 00008: val_mDice did not improve from 0.23063
Epoch 9/300

Epoch 00009: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 1.0997 - acc: 0.9845 - mDice: 0.3185 - val_loss: 2.3396 - val_acc: 0.9857 - val_mDice: 0.2690

Epoch 00009: val_mDice improved from 0.23063 to 0.26904, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 10/300

Epoch 00010: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.9950 - acc: 0.9849 - mDice: 0.3535 - val_loss: 2.3650 - val_acc: 0.9859 - val_mDice: 0.2750

Epoch 00010: val_mDice improved from 0.26904 to 0.27498, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 11/300

Epoch 00011: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.9749 - acc: 0.9850 - mDice: 0.3626 - val_loss: 2.2096 - val_acc: 0.9862 - val_mDice: 0.2973

Epoch 00011: val_mDice improved from 0.27498 to 0.29727, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 12/300

Epoch 00012: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.9645 - acc: 0.9851 - mDice: 0.3787 - val_loss: 2.1378 - val_acc: 0.9861 - val_mDice: 0.2963

Epoch 00012: val_mDice did not improve from 0.29727
Epoch 13/300

Epoch 00013: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.9077 - acc: 0.9853 - mDice: 0.3860 - val_loss: 2.1235 - val_acc: 0.9863 - val_mDice: 0.3266

Epoch 00013: val_mDice improved from 0.29727 to 0.32655, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 14/300

Epoch 00014: LearningRateScheduler setting learning rate to 0.001.
 - 12s - loss: 0.9283 - acc: 0.9852 - mDice: 0.3822 - val_loss: 3.0033 - val_acc: 0.9865 - val_mDice: 0.2594

Epoch 00014: val_mDice did not improve from 0.32655
Epoch 15/300

Epoch 00015: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.8822 - acc: 0.9854 - mDice: 0.3992 - val_loss: 2.1979 - val_acc: 0.9860 - val_mDice: 0.3244

Epoch 00015: val_mDice did not improve from 0.32655
Epoch 16/300

Epoch 00016: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.8624 - acc: 0.9854 - mDice: 0.4060 - val_loss: 2.0901 - val_acc: 0.9857 - val_mDice: 0.3110

Epoch 00016: val_mDice did not improve from 0.32655
Epoch 17/300

Epoch 00017: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.9308 - acc: 0.9854 - mDice: 0.3940 - val_loss: 1.8289 - val_acc: 0.9861 - val_mDice: 0.3237

Epoch 00017: val_mDice did not improve from 0.32655
Epoch 18/300

Epoch 00018: LearningRateScheduler setting learning rate to 0.001.
 - 11s - loss: 0.8483 - acc: 0.9856 - mDice: 0.4120 - val_loss: 1.7857 - val_acc: 0.9847 - val_mDice: 0.3363

Epoch 00018: val_mDice improved from 0.32655 to 0.33625, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 19/300

Epoch 00019: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.9011 - acc: 0.9857 - mDice: 0.4313 - val_loss: 1.9561 - val_acc: 0.9857 - val_mDice: 0.3416

Epoch 00019: val_mDice improved from 0.33625 to 0.34159, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 20/300

Epoch 00020: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.7886 - acc: 0.9858 - mDice: 0.4360 - val_loss: 2.1029 - val_acc: 0.9863 - val_mDice: 0.3255

Epoch 00020: val_mDice did not improve from 0.34159
Epoch 21/300

Epoch 00021: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.8046 - acc: 0.9857 - mDice: 0.4319 - val_loss: 1.8579 - val_acc: 0.9860 - val_mDice: 0.3476

Epoch 00021: val_mDice improved from 0.34159 to 0.34761, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 22/300

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.7787 - acc: 0.9858 - mDice: 0.4411 - val_loss: 2.1006 - val_acc: 0.9861 - val_mDice: 0.3333

Epoch 00022: val_mDice did not improve from 0.34761
Epoch 23/300

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.7796 - acc: 0.9858 - mDice: 0.4411 - val_loss: 1.8424 - val_acc: 0.9861 - val_mDice: 0.3500

Epoch 00023: val_mDice improved from 0.34761 to 0.35004, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 24/300

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.7609 - acc: 0.9858 - mDice: 0.4481 - val_loss: 1.8672 - val_acc: 0.9855 - val_mDice: 0.3530

Epoch 00024: val_mDice improved from 0.35004 to 0.35300, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 25/300

Epoch 00025: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.8135 - acc: 0.9859 - mDice: 0.4422 - val_loss: 1.9821 - val_acc: 0.9841 - val_mDice: 0.3334

Epoch 00025: val_mDice did not improve from 0.35300
Epoch 26/300

Epoch 00026: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.8125 - acc: 0.9858 - mDice: 0.4389 - val_loss: 3.5230 - val_acc: 0.9860 - val_mDice: 0.2550

Epoch 00026: val_mDice did not improve from 0.35300
Epoch 27/300

Epoch 00027: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.8145 - acc: 0.9858 - mDice: 0.4334 - val_loss: 2.2678 - val_acc: 0.9863 - val_mDice: 0.3352

Epoch 00027: val_mDice did not improve from 0.35300
Epoch 28/300

Epoch 00028: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.8020 - acc: 0.9859 - mDice: 0.4313 - val_loss: 2.3562 - val_acc: 0.9861 - val_mDice: 0.3257

Epoch 00028: val_mDice did not improve from 0.35300
Epoch 29/300

Epoch 00029: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.7594 - acc: 0.9860 - mDice: 0.4493 - val_loss: 2.0372 - val_acc: 0.9861 - val_mDice: 0.3407

Epoch 00029: val_mDice did not improve from 0.35300
Epoch 30/300

Epoch 00030: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.7470 - acc: 0.9861 - mDice: 0.4556 - val_loss: 1.7688 - val_acc: 0.9860 - val_mDice: 0.3574

Epoch 00030: val_mDice improved from 0.35300 to 0.35738, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 31/300

Epoch 00031: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.7973 - acc: 0.9861 - mDice: 0.4523 - val_loss: 2.7023 - val_acc: 0.9859 - val_mDice: 0.2811

Epoch 00031: val_mDice did not improve from 0.35738
Epoch 32/300

Epoch 00032: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.7604 - acc: 0.9860 - mDice: 0.4499 - val_loss: 1.7685 - val_acc: 0.9860 - val_mDice: 0.3553

Epoch 00032: val_mDice did not improve from 0.35738
Epoch 33/300

Epoch 00033: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.7506 - acc: 0.9861 - mDice: 0.4554 - val_loss: 1.9174 - val_acc: 0.9858 - val_mDice: 0.3501

Epoch 00033: val_mDice did not improve from 0.35738
Epoch 34/300

Epoch 00034: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.7788 - acc: 0.9860 - mDice: 0.4469 - val_loss: 1.7595 - val_acc: 0.9839 - val_mDice: 0.3519

Epoch 00034: val_mDice did not improve from 0.35738
Epoch 35/300

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005.
 - 11s - loss: 0.7425 - acc: 0.9862 - mDice: 0.4616 - val_loss: 1.7078 - val_acc: 0.9854 - val_mDice: 0.3598

Epoch 00035: val_mDice improved from 0.35738 to 0.35978, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 36/300

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005.
 - 12s - loss: 0.7713 - acc: 0.9862 - mDice: 0.4605 - val_loss: 1.6505 - val_acc: 0.9855 - val_mDice: 0.3646

Epoch 00036: val_mDice improved from 0.35978 to 0.36460, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 37/300

Epoch 00037: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.7083 - acc: 0.9864 - mDice: 0.4760 - val_loss: 1.6547 - val_acc: 0.9857 - val_mDice: 0.3674

Epoch 00037: val_mDice improved from 0.36460 to 0.36736, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_normal_FM20_Res_Unet2_NL3_LS_MyLogDice_US1_wLRScheduler_Main_Ps_ET_Init_3T_CV_c/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 38/300

Epoch 00038: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.7184 - acc: 0.9863 - mDice: 0.4727 - val_loss: 1.7302 - val_acc: 0.9859 - val_mDice: 0.3596

Epoch 00038: val_mDice did not improve from 0.36736
Epoch 39/300

Epoch 00039: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.7460 - acc: 0.9864 - mDice: 0.4740 - val_loss: 1.8833 - val_acc: 0.9860 - val_mDice: 0.3533

Epoch 00039: val_mDice did not improve from 0.36736
Epoch 40/300

Epoch 00040: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.7814 - acc: 0.9864 - mDice: 0.4728 - val_loss: 1.9902 - val_acc: 0.9860 - val_mDice: 0.3526

Epoch 00040: val_mDice did not improve from 0.36736
Epoch 41/300

Epoch 00041: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.7369 - acc: 0.9864 - mDice: 0.4734 - val_loss: 2.0715 - val_acc: 0.9855 - val_mDice: 0.3478

Epoch 00041: val_mDice did not improve from 0.36736
Epoch 42/300

Epoch 00042: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.7108 - acc: 0.9865 - mDice: 0.4747 - val_loss: 1.8131 - val_acc: 0.9857 - val_mDice: 0.3599

Epoch 00042: val_mDice did not improve from 0.36736
Epoch 43/300

Epoch 00043: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.7294 - acc: 0.9865 - mDice: 0.4817 - val_loss: 1.6828 - val_acc: 0.9861 - val_mDice: 0.3616

Epoch 00043: val_mDice did not improve from 0.36736
Epoch 44/300

Epoch 00044: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.7600 - acc: 0.9864 - mDice: 0.4608 - val_loss: 2.9718 - val_acc: 0.9867 - val_mDice: 0.2771

Epoch 00044: val_mDice did not improve from 0.36736
Epoch 45/300

Epoch 00045: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.6968 - acc: 0.9865 - mDice: 0.4797 - val_loss: 1.7804 - val_acc: 0.9858 - val_mDice: 0.3599

Epoch 00045: val_mDice did not improve from 0.36736
Epoch 46/300

Epoch 00046: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.6965 - acc: 0.9865 - mDice: 0.4798 - val_loss: 1.6792 - val_acc: 0.9856 - val_mDice: 0.3655

Epoch 00046: val_mDice did not improve from 0.36736
Epoch 47/300

Epoch 00047: LearningRateScheduler setting learning rate to 0.00025.
 - 11s - loss: 0.6800 - acc: 0.9866 - mDice: 0.4887 - val_loss: 1.9077 - val_acc: 0.9858 - val_mDice: 0.3599

Epoch 00047: val_mDice did not improve from 0.36736
Epoch 48/300

Epoch 00048: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.7097 - acc: 0.9864 - mDice: 0.4757 - val_loss: 2.5421 - val_acc: 0.9861 - val_mDice: 0.3249

Epoch 00048: val_mDice did not improve from 0.36736
Epoch 49/300

Epoch 00049: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.7114 - acc: 0.9865 - mDice: 0.4843 - val_loss: 1.9795 - val_acc: 0.9852 - val_mDice: 0.3449

Epoch 00049: val_mDice did not improve from 0.36736
Epoch 50/300

Epoch 00050: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.6722 - acc: 0.9866 - mDice: 0.4914 - val_loss: 1.7647 - val_acc: 0.9859 - val_mDice: 0.3598

Epoch 00050: val_mDice did not improve from 0.36736
Epoch 51/300

Epoch 00051: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.6724 - acc: 0.9867 - mDice: 0.4917 - val_loss: 2.0395 - val_acc: 0.9859 - val_mDice: 0.3453

Epoch 00051: val_mDice did not improve from 0.36736
Epoch 52/300

Epoch 00052: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.6652 - acc: 0.9867 - mDice: 0.4951 - val_loss: 1.9532 - val_acc: 0.9858 - val_mDice: 0.3516

Epoch 00052: val_mDice did not improve from 0.36736
Epoch 53/300

Epoch 00053: LearningRateScheduler setting learning rate to 0.00025.
 - 12s - loss: 0.6731 - acc: 0.9867 - mDice: 0.4921 - val_loss: 1.9532 - val_acc: 0.9855 - val_mDice: 0.3559

Epoch 00053: val_mDice did not improve from 0.36736
Epoch 54/300

Epoch 00054: LearningRateScheduler setting learning rate to 0.00025.
 - 13s - loss: 0.6658 - acc: 0.9867 - mDice: 0.4964 - val_loss: 1.7371 - val_acc: 0.9853 - val_mDice: 0.3619

Epoch 00054: val_mDice did not improve from 0.36736
Epoch 55/300

Epoch 00055: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.6623 - acc: 0.9869 - mDice: 0.4996 - val_loss: 1.8000 - val_acc: 0.9858 - val_mDice: 0.3584

Epoch 00055: val_mDice did not improve from 0.36736
Epoch 56/300

Epoch 00056: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.6627 - acc: 0.9869 - mDice: 0.4997 - val_loss: 1.8117 - val_acc: 0.9859 - val_mDice: 0.3579

Epoch 00056: val_mDice did not improve from 0.36736
Epoch 57/300

Epoch 00057: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.6514 - acc: 0.9868 - mDice: 0.5027 - val_loss: 1.9161 - val_acc: 0.9860 - val_mDice: 0.3583

Epoch 00057: val_mDice did not improve from 0.36736
Epoch 58/300

Epoch 00058: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.6584 - acc: 0.9869 - mDice: 0.5039 - val_loss: 1.8171 - val_acc: 0.9861 - val_mDice: 0.3600

Epoch 00058: val_mDice did not improve from 0.36736
Epoch 59/300

Epoch 00059: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.6453 - acc: 0.9868 - mDice: 0.5053 - val_loss: 1.9343 - val_acc: 0.9860 - val_mDice: 0.3559

Epoch 00059: val_mDice did not improve from 0.36736
Epoch 60/300

Epoch 00060: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.6638 - acc: 0.9869 - mDice: 0.5001 - val_loss: 1.8524 - val_acc: 0.9859 - val_mDice: 0.3574

Epoch 00060: val_mDice did not improve from 0.36736
Epoch 61/300

Epoch 00061: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.6615 - acc: 0.9868 - mDice: 0.5007 - val_loss: 1.9728 - val_acc: 0.9860 - val_mDice: 0.3556

Epoch 00061: val_mDice did not improve from 0.36736
Epoch 62/300

Epoch 00062: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.6773 - acc: 0.9867 - mDice: 0.4931 - val_loss: 2.0685 - val_acc: 0.9863 - val_mDice: 0.3411

Epoch 00062: val_mDice did not improve from 0.36736
Epoch 63/300

Epoch 00063: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.6431 - acc: 0.9869 - mDice: 0.5059 - val_loss: 1.9030 - val_acc: 0.9860 - val_mDice: 0.3524

Epoch 00063: val_mDice did not improve from 0.36736
Epoch 64/300

Epoch 00064: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.6641 - acc: 0.9869 - mDice: 0.5010 - val_loss: 1.9326 - val_acc: 0.9857 - val_mDice: 0.3546

Epoch 00064: val_mDice did not improve from 0.36736
Epoch 65/300

Epoch 00065: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.6700 - acc: 0.9868 - mDice: 0.4962 - val_loss: 1.9672 - val_acc: 0.9860 - val_mDice: 0.3500

Epoch 00065: val_mDice did not improve from 0.36736
Epoch 66/300

Epoch 00066: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.6798 - acc: 0.9869 - mDice: 0.5060 - val_loss: 1.7782 - val_acc: 0.9858 - val_mDice: 0.3644

Epoch 00066: val_mDice did not improve from 0.36736
Epoch 67/300

Epoch 00067: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.6483 - acc: 0.9870 - mDice: 0.5049 - val_loss: 1.7974 - val_acc: 0.9859 - val_mDice: 0.3597

Epoch 00067: val_mDice did not improve from 0.36736
Epoch 68/300

Epoch 00068: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.6921 - acc: 0.9871 - mDice: 0.5074 - val_loss: 1.9927 - val_acc: 0.9859 - val_mDice: 0.3479

Epoch 00068: val_mDice did not improve from 0.36736
Epoch 69/300

Epoch 00069: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.6649 - acc: 0.9868 - mDice: 0.5021 - val_loss: 2.1128 - val_acc: 0.9860 - val_mDice: 0.3459

Epoch 00069: val_mDice did not improve from 0.36736
Epoch 70/300

Epoch 00070: LearningRateScheduler setting learning rate to 0.000125.
 - 13s - loss: 0.6597 - acc: 0.9869 - mDice: 0.5012 - val_loss: 1.9040 - val_acc: 0.9855 - val_mDice: 0.3611

Epoch 00070: val_mDice did not improve from 0.36736
Epoch 71/300

Epoch 00071: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.6429 - acc: 0.9869 - mDice: 0.5071 - val_loss: 1.8802 - val_acc: 0.9857 - val_mDice: 0.3556

Epoch 00071: val_mDice did not improve from 0.36736
Epoch 72/300

Epoch 00072: LearningRateScheduler setting learning rate to 0.000125.
 - 12s - loss: 0.6313 - acc: 0.9871 - mDice: 0.5131 - val_loss: 1.8475 - val_acc: 0.9858 - val_mDice: 0.3600

Epoch 00072: val_mDice did not improve from 0.36736
Epoch 73/300

Epoch 00073: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.6708 - acc: 0.9871 - mDice: 0.5108 - val_loss: 1.8368 - val_acc: 0.9859 - val_mDice: 0.3609

Epoch 00073: val_mDice did not improve from 0.36736
Epoch 74/300

Epoch 00074: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.6358 - acc: 0.9871 - mDice: 0.5137 - val_loss: 1.8765 - val_acc: 0.9860 - val_mDice: 0.3577

Epoch 00074: val_mDice did not improve from 0.36736
Epoch 75/300

Epoch 00075: LearningRateScheduler setting learning rate to 6.25e-05.
 - 13s - loss: 0.6276 - acc: 0.9871 - mDice: 0.5165 - val_loss: 1.8955 - val_acc: 0.9860 - val_mDice: 0.3543

Epoch 00075: val_mDice did not improve from 0.36736
Epoch 76/300

Epoch 00076: LearningRateScheduler setting learning rate to 6.25e-05.
 - 13s - loss: 0.6401 - acc: 0.9871 - mDice: 0.5110 - val_loss: 1.9252 - val_acc: 0.9860 - val_mDice: 0.3541

Epoch 00076: val_mDice did not improve from 0.36736
Epoch 77/300

Epoch 00077: LearningRateScheduler setting learning rate to 6.25e-05.
 - 12s - loss: 0.6345 - acc: 0.9871 - mDice: 0.5135 - val_loss: 1.9030 - val_acc: 0.9860 - val_mDice: 0.3555

Epoch 00077: val_mDice did not improve from 0.36736
Restoring model weights from the end of the best epoch
Epoch 00077: early stopping
{'val_loss': [5.178805953127556, 3.454495202746253, 2.7880521253241617, 2.1372142052008645, 2.181327621577863, 2.864131791285004, 3.0330007891593165, 3.834567961165104, 2.3395579626648164, 2.36503928442182, 2.2096093288090746, 2.1377722301844466, 2.1235141326280558, 3.00332734829645, 2.1978958789277767, 2.090138565626363, 1.8289265827547874, 1.7857153862566202, 1.956063384192059, 2.102929227730094, 1.8579444431236471, 2.1005716656640185, 1.8423610674895174, 1.8671825806854492, 1.9821376562831647, 3.5229568979438257, 2.2678263967082364, 2.356209920149144, 2.037155595400992, 1.768834662223504, 2.7022885223209916, 1.7684760257705734, 1.9174265298149284, 1.759515619705824, 1.707820904456963, 1.6505362181696792, 1.6547490461754537, 1.7302178254987997, 1.8833039798147062, 1.9902336659246047, 2.071528057039438, 1.8130661250349294, 1.6827621103402743, 2.971796038619067, 1.7803949634669904, 1.679161588786725, 1.907700090798162, 2.5421139944348474, 1.9794552378497594, 1.7647026657226197, 2.039532145379429, 1.9532318448022024, 1.9531927831389255, 1.7371114476965526, 1.7999732919847027, 1.8117452868198232, 1.9161227267618075, 1.8170787284999403, 1.934254601850348, 1.8523943702340244, 1.9728207160325968, 2.068465943113044, 1.9029856320036016, 1.9326308831379397, 1.9671880809046098, 1.7781962554452426, 1.7974123191738414, 1.9927107085021638, 2.1128042060380445, 1.9040334310274893, 1.8802149371872632, 1.8474830045538433, 1.836754526003289, 1.8765064457715568, 1.8955024106433598, 1.9251830663424307, 1.9030476144161206], 'val_acc': [0.24267880563602848, 0.9678200504715635, 0.9817735968772341, 0.9825570071682498, 0.9844304467008216, 0.9857922807885547, 0.9863188086097999, 0.9860107958138524, 0.9856720595868492, 0.9859396836099216, 0.9862021546539732, 0.9861222202494994, 0.9863254227642999, 0.9864971174200178, 0.985996664224094, 0.9857199810081322, 0.9860919128030033, 0.9847243219643742, 0.9856721487263977, 0.9862893450176965, 0.9860452631059459, 0.986074055180117, 0.9860746375585007, 0.9854769034019615, 0.9840718145265893, 0.9859854801820734, 0.9863368504544198, 0.9861368807339121, 0.9861120642836049, 0.9859542516269094, 0.9859113220203434, 0.9859666955078825, 0.9857568134695797, 0.9838743763692596, 0.9853687949575194, 0.9854768320903227, 0.9856861971192441, 0.9859229279895604, 0.9859985183267033, 0.9860027554265761, 0.9855094215092607, 0.9857026285093542, 0.9860876757031305, 0.9866715219180107, 0.9858316490918784, 0.9856364988496272, 0.9858338478674085, 0.9861251856251466, 0.9852486526740275, 0.9859222505289916, 0.9858522224996763, 0.9858088769085505, 0.985513569469585, 0.9853282424055805, 0.9857554526058057, 0.9859268204165123, 0.9859603547146646, 0.9861010466354081, 0.9860074917079207, 0.9858586643177158, 0.9860069033869004, 0.9862806271698515, 0.9860037775600657, 0.9857322525526448, 0.9860312028278263, 0.9858269128105338, 0.9858529950424301, 0.9859069125840101, 0.9860301925796098, 0.9855041682185347, 0.9857108234051812, 0.985829028389152, 0.9859400962666048, 0.9859644075928039, 0.9859808270976408, 0.9859550122843901, 0.9859988748848973], 'val_mDice': [0.009198585462014554, 0.11271831977848755, 0.127292424718945, 0.1997995016685987, 0.22279679968373725, 0.23062672727887082, 0.22544826576829788, 0.1821103189159129, 0.26904101467738717, 0.2749829333004424, 0.2972663479559086, 0.2963104126519957, 0.32655336519894546, 0.2593542990513361, 0.32441065324505686, 0.3109979044909729, 0.32370891254896655, 0.3362545712014376, 0.3415865160651126, 0.32549430045149735, 0.3476132624824406, 0.33327077746629, 0.350035168416241, 0.3529984440843938, 0.33342171003549903, 0.254977717327097, 0.33524965315849214, 0.32571411783531207, 0.3406644981737032, 0.3573759216610955, 0.2810818277826813, 0.35532361132316553, 0.35011316254987557, 0.3519041439590758, 0.35978053600814264, 0.36459505736293013, 0.36735747191509005, 0.3596308863056027, 0.35333041833381235, 0.3525942259450496, 0.34783212087327914, 0.3598905460962864, 0.3616446809302774, 0.27710335962437205, 0.35988926991508347, 0.36548549534078845, 0.3598813387237541, 0.32488871235790423, 0.344932667264791, 0.3598190546927162, 0.345303224453541, 0.3515973222903454, 0.355930244577965, 0.3619443212941304, 0.3583604070624469, 0.35793838255545674, 0.3583197044589346, 0.36004210858497165, 0.3559348085228491, 0.35738577083837714, 0.35560627431717373, 0.3411154588638013, 0.35237910621544183, 0.35464336440071154, 0.349992370290509, 0.36439568487739754, 0.35968664606451867, 0.3478639927188516, 0.3459018982894875, 0.361119386027841, 0.3555730482932934, 0.36001107985096226, 0.36086358449394895, 0.35773608852122146, 0.35434083108709435, 0.3541328934051936, 0.3555160086390266], 'loss': [5.552410000027192, 3.3735167726208664, 2.340777833437182, 1.6526285676787633, 1.3961999284676845, 1.2425016336249088, 1.13781347202213, 1.1282739736428726, 1.0996930622914052, 0.9950176539465558, 0.9748509642418893, 0.9644549775193823, 0.907711572224566, 0.9282771710033914, 0.8822088774127239, 0.8624097899473, 0.9307779200122704, 0.8483297387115148, 0.9010968896238957, 0.7886181335620103, 0.8045921009079638, 0.7787471745139428, 0.7796012910195408, 0.7609254481105402, 0.8135398175708207, 0.8124970974264393, 0.8145288374708397, 0.8020344199242402, 0.7594105084561577, 0.7469713273853663, 0.7972831186794099, 0.7604332269262203, 0.7505568762473985, 0.7787840605425846, 0.7425265497709527, 0.771339705134351, 0.7082613545653278, 0.7184060986684594, 0.7459806080360768, 0.7813680894420495, 0.7369469376130027, 0.7107503796887854, 0.7294338220875815, 0.760015416683727, 0.6968103334139419, 0.6965484997192668, 0.6799646069386688, 0.7097357566823664, 0.7114104737706435, 0.6721747094471782, 0.6724212253626269, 0.6652466426249164, 0.6730527539257917, 0.6657760369174903, 0.662278618599541, 0.6626831596542587, 0.6514037648544162, 0.6583864876143652, 0.6452809398702154, 0.6637567385599551, 0.6614725506065754, 0.6773405002798587, 0.6430607702606849, 0.6640579734997943, 0.6699731114686677, 0.6797818990390443, 0.6483071044957924, 0.6921412784443454, 0.6648971339505738, 0.6597463537152046, 0.6429358305858524, 0.6313122929634858, 0.6707583698034871, 0.6357746226681057, 0.6276061138687733, 0.6401495034342369, 0.6345473819640436], 'acc': [0.1410820053101215, 0.5330418135146037, 0.9624249326521471, 0.9831021835713861, 0.9834807956692747, 0.9837923051389807, 0.9843338749604186, 0.9844608509663454, 0.984540942666808, 0.9848915333541867, 0.9849584202410605, 0.9851439567717138, 0.9852638737384045, 0.9852334187495457, 0.9854455318413473, 0.9854046731645942, 0.9854234960012478, 0.9855919562839794, 0.9857387829189741, 0.9857780753132404, 0.985680146871622, 0.9858388192640781, 0.9858398079052824, 0.985845168197512, 0.9858908619316524, 0.985837332366668, 0.9857957517626711, 0.9859134733062429, 0.9860075450025473, 0.986054027408259, 0.9860944859701801, 0.9860191763939667, 0.9860668598171771, 0.9859708170125341, 0.9861721268868294, 0.9862063127987281, 0.9864061830613797, 0.986329250816744, 0.9863500345789105, 0.9863992890541496, 0.9863672070955024, 0.9864842077944872, 0.9865397176850235, 0.9864289284513227, 0.9864796853908208, 0.9865281467468175, 0.9866327627300924, 0.9863936377028126, 0.9864870414876681, 0.9866372413593119, 0.9866891415258453, 0.9866990266796649, 0.9866943971222385, 0.9867079852841096, 0.9868663356885877, 0.9868969800838607, 0.9868399726156517, 0.9869254510310215, 0.9868461592676128, 0.9869005643507986, 0.986777063175692, 0.9867172415313852, 0.9868716084319233, 0.986867275727281, 0.9868214784970047, 0.9869446904271856, 0.9869820231834228, 0.9870829512573189, 0.9868478952053428, 0.9869114872457704, 0.9869456904216555, 0.9871081119667506, 0.9870723151786104, 0.9870687153155627, 0.9871134455144318, 0.9871381460362109, 0.9870865522030234], 'mDice': [0.0058927583653584055, 0.07383758080030517, 0.14351904279587605, 0.1922076250928023, 0.24515768967543272, 0.2744327504727322, 0.30512775978915646, 0.3119904879383673, 0.3184662065672184, 0.35345833525098186, 0.3626147116816506, 0.37868018457862634, 0.3859569130572374, 0.3822188881693603, 0.399211634241253, 0.4060482455049008, 0.3939742419025625, 0.4119897614112951, 0.43133758933859007, 0.43603451101582136, 0.43194797728713613, 0.44114556023462415, 0.44112308281686413, 0.4480775699758272, 0.44221571740357385, 0.43890603234151093, 0.43343759615981936, 0.43133354593739537, 0.4493308629425472, 0.45564784433657946, 0.4523489778979362, 0.4498775011777995, 0.4554441687401803, 0.44689867808997896, 0.461623713995934, 0.4605337313038494, 0.4759891113640341, 0.47272207136240785, 0.474035718772244, 0.47279332622805237, 0.4733504388164994, 0.4747146303652547, 0.48174711302559226, 0.4608275227296218, 0.4796689991337678, 0.47981594474396405, 0.48873430730433925, 0.4756593547384705, 0.48431972232471215, 0.4914486056516495, 0.4916979242436255, 0.4951127427550116, 0.4920814650926042, 0.4964079099598034, 0.49961059717904954, 0.4996671502532828, 0.5027011320119286, 0.5039103250908512, 0.5053464012457333, 0.5000579346028039, 0.5006860508658925, 0.493114797373759, 0.5058706620480421, 0.5009822552995349, 0.4961815077590193, 0.5060385670741516, 0.5049105608761749, 0.5074381624066368, 0.5021244573809671, 0.501231773597795, 0.5071087468764802, 0.5131435923911101, 0.5108019041704909, 0.5136982403260326, 0.5165381207294071, 0.5110449432507413, 0.513465587523385], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05]}
predicting test subjects:   0%|          | 0/10 [00:00<?, ?it/s]predicting test subjects:  10%|█         | 1/10 [00:04<00:42,  4.74s/it]predicting test subjects:  20%|██        | 2/10 [00:08<00:36,  4.54s/it]predicting test subjects:  30%|███       | 3/10 [00:14<00:33,  4.82s/it]predicting test subjects:  40%|████      | 4/10 [00:20<00:31,  5.29s/it]predicting test subjects:  50%|█████     | 5/10 [00:26<00:27,  5.50s/it]predicting test subjects:  60%|██████    | 6/10 [00:32<00:22,  5.50s/it]predicting test subjects:  70%|███████   | 7/10 [00:36<00:15,  5.03s/it]predicting test subjects:  80%|████████  | 8/10 [00:40<00:09,  4.76s/it]predicting test subjects:  90%|█████████ | 9/10 [00:44<00:04,  4.66s/it]predicting test subjects: 100%|██████████| 10/10 [00:49<00:00,  4.83s/it]
predicting train subjects:   0%|          | 0/42 [00:00<?, ?it/s]predicting train subjects:   2%|▏         | 1/42 [00:04<02:47,  4.08s/it]predicting train subjects:   5%|▍         | 2/42 [00:08<02:45,  4.14s/it]predicting train subjects:   7%|▋         | 3/42 [00:11<02:31,  3.90s/it]predicting train subjects:  10%|▉         | 4/42 [00:14<02:21,  3.71s/it]predicting train subjects:  12%|█▏        | 5/42 [00:19<02:30,  4.06s/it]predicting train subjects:  14%|█▍        | 6/42 [00:24<02:29,  4.15s/it]predicting train subjects:  17%|█▋        | 7/42 [00:28<02:22,  4.08s/it]predicting train subjects:  19%|█▉        | 8/42 [00:30<02:03,  3.64s/it]predicting train subjects:  21%|██▏       | 9/42 [00:35<02:06,  3.83s/it]predicting train subjects:  24%|██▍       | 10/42 [00:40<02:15,  4.23s/it]predicting train subjects:  26%|██▌       | 11/42 [00:44<02:14,  4.35s/it]predicting train subjects:  29%|██▊       | 12/42 [00:47<01:56,  3.88s/it]predicting train subjects:  31%|███       | 13/42 [00:51<01:55,  3.98s/it]predicting train subjects:  33%|███▎      | 14/42 [00:55<01:50,  3.93s/it]predicting train subjects:  36%|███▌      | 15/42 [00:58<01:37,  3.62s/it]predicting train subjects:  38%|███▊      | 16/42 [01:02<01:39,  3.84s/it]predicting train subjects:  40%|████      | 17/42 [01:08<01:48,  4.35s/it]predicting train subjects:  43%|████▎     | 18/42 [01:12<01:45,  4.39s/it]predicting train subjects:  45%|████▌     | 19/42 [01:16<01:37,  4.25s/it]predicting train subjects:  48%|████▊     | 20/42 [01:20<01:27,  3.98s/it]predicting train subjects:  50%|█████     | 21/42 [01:23<01:21,  3.88s/it]predicting train subjects:  52%|█████▏    | 22/42 [01:27<01:18,  3.94s/it]predicting train subjects:  55%|█████▍    | 23/42 [01:30<01:06,  3.52s/it]predicting train subjects:  57%|█████▋    | 24/42 [01:34<01:06,  3.71s/it]predicting train subjects:  60%|█████▉    | 25/42 [01:38<01:02,  3.67s/it]predicting train subjects:  62%|██████▏   | 26/42 [01:42<00:59,  3.75s/it]predicting train subjects:  64%|██████▍   | 27/42 [01:46<00:59,  3.94s/it]predicting train subjects:  67%|██████▋   | 28/42 [01:50<00:56,  4.04s/it]predicting train subjects:  69%|██████▉   | 29/42 [01:54<00:53,  4.11s/it]predicting train subjects:  71%|███████▏  | 30/42 [01:58<00:47,  3.94s/it]predicting train subjects:  74%|███████▍  | 31/42 [02:03<00:46,  4.24s/it]predicting train subjects:  76%|███████▌  | 32/42 [02:08<00:43,  4.34s/it]predicting train subjects:  79%|███████▊  | 33/42 [02:11<00:37,  4.19s/it]predicting train subjects:  81%|████████  | 34/42 [02:16<00:34,  4.33s/it]predicting train subjects:  83%|████████▎ | 35/42 [02:20<00:28,  4.13s/it]predicting train subjects:  86%|████████▌ | 36/42 [02:23<00:24,  4.02s/it]predicting train subjects:  88%|████████▊ | 37/42 [02:28<00:20,  4.13s/it]predicting train subjects:  90%|█████████ | 38/42 [02:32<00:16,  4.07s/it]predicting train subjects:  93%|█████████▎| 39/42 [02:36<00:12,  4.13s/it]predicting train subjects:  95%|█████████▌| 40/42 [02:40<00:07,  3.99s/it]predicting train subjects:  98%|█████████▊| 41/42 [02:44<00:03,  3.95s/it]predicting train subjects: 100%|██████████| 42/42 [02:48<00:00,  4.23s/it]

