*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
(0/285) train vimp2_ANON967_CSFn2
(1/285) train vimp2_E_CSFn2
(2/285) train vimp2_G_CSFn2
(3/285) train vimp2_J_CSFn2
(4/285) train vimp2_K_CSFn2
(5/285) train vimp2_L_CSFn2
(6/285) train vimp2_ANON911_CSFn2
(7/285) train vimp2_D_CSFn2
(8/285) train vimp2_F_CSFn2
(9/285) train vimp2_ANON911_CSFn_Aug0_Rot_-4_sd0
(10/285) train vimp2_ANON911_CSFn_Aug0_Rot_-4_sd1
(11/285) train vimp2_ANON911_CSFn_Aug0_Rot_7_sd2
(12/285) train vimp2_ANON911_CSFn_Aug1_Rot_1_sd2
(13/285) train vimp2_ANON911_CSFn_Aug1_Rot_3_sd1
(14/285) train vimp2_ANON911_CSFn_Aug1_Rot_4_sd0
(15/285) train vimp2_ANON911_CSFn_Aug2_Rot_1_sd1
(16/285) train vimp2_ANON911_CSFn_Aug2_Rot_2_sd2
(17/285) train vimp2_ANON911_CSFn_Aug2_Rot_5_sd0
(18/285) train vimp2_ANON911_CSFn_Aug3_Rot_-4_sd0
(19/285) train vimp2_ANON911_CSFn_Aug3_Rot_6_sd1
(20/285) train vimp2_ANON911_CSFn_Aug3_Rot_7_sd2
(21/285) train vimp2_ANON911_CSFn_Aug4_Rot_-2_sd1
(22/285) train vimp2_ANON911_CSFn_Aug4_Rot_7_sd0
(23/285) train vimp2_ANON911_CSFn_Aug4_Rot_7_sd2
(24/285) train vimp2_ANON911_CSFn_Aug5_Rot_1_sd0
(25/285) train vimp2_ANON911_CSFn_Aug5_Rot_-6_sd1
(26/285) train vimp2_ANON911_CSFn_Aug5_Rot_7_sd2
(27/285) train vimp2_D_CSFn_Aug0_Rot_4_sd0
(28/285) train vimp2_D_CSFn_Aug0_Rot_-5_sd2
(29/285) train vimp2_D_CSFn_Aug0_Rot_7_sd1
(30/285) train vimp2_D_CSFn_Aug1_Rot_-2_sd1
(31/285) train vimp2_D_CSFn_Aug1_Rot_-4_sd0
(32/285) train vimp2_D_CSFn_Aug1_Rot_5_sd2
(33/285) train vimp2_D_CSFn_Aug2_Rot_1_sd1
(34/285) train vimp2_D_CSFn_Aug2_Rot_-1_sd2
(35/285) train vimp2_D_CSFn_Aug2_Rot_-5_sd0
(36/285) train vimp2_D_CSFn_Aug3_Rot_-3_sd0
(37/285) train vimp2_D_CSFn_Aug3_Rot_-3_sd1
(38/285) train vimp2_D_CSFn_Aug3_Rot_3_sd2
(39/285) train vimp2_D_CSFn_Aug4_Rot_-4_sd0
(40/285) train vimp2_D_CSFn_Aug4_Rot_-5_sd2
(41/285) train vimp2_D_CSFn_Aug4_Rot_6_sd1
(42/285) train vimp2_D_CSFn_Aug5_Rot_1_sd2
(43/285) train vimp2_D_CSFn_Aug5_Rot_2_sd0
(44/285) train vimp2_D_CSFn_Aug5_Rot_7_sd1
(45/285) train vimp2_F_CSFn_Aug0_Rot_-1_sd2
(46/285) train vimp2_F_CSFn_Aug0_Rot_5_sd0
(47/285) train vimp2_F_CSFn_Aug0_Rot_7_sd1
(48/285) train vimp2_F_CSFn_Aug1_Rot_-5_sd2
(49/285) train vimp2_F_CSFn_Aug1_Rot_-7_sd0
(50/285) train vimp2_F_CSFn_Aug1_Rot_-7_sd1
(51/285) train vimp2_F_CSFn_Aug2_Rot_-3_sd1
(52/285) train vimp2_F_CSFn_Aug2_Rot_4_sd0
(53/285) train vimp2_F_CSFn_Aug2_Rot_-7_sd2
(54/285) train vimp2_F_CSFn_Aug3_Rot_2_sd0
(55/285) train vimp2_F_CSFn_Aug3_Rot_-3_sd2
(56/285) train vimp2_F_CSFn_Aug3_Rot_-7_sd1
(57/285) train vimp2_F_CSFn_Aug4_Rot_-1_sd0
(58/285) train vimp2_F_CSFn_Aug4_Rot_-2_sd1
(59/285) train vimp2_F_CSFn_Aug4_Rot_-2_sd2
(60/285) train vimp2_F_CSFn_Aug5_Rot_-1_sd1
(61/285) train vimp2_F_CSFn_Aug5_Rot_4_sd0
(62/285) train vimp2_F_CSFn_Aug5_Rot_-5_sd2
(63/285) train vimp2_ANON972_CSFn2
(64/285) train vimp2_H_CSFn2
(65/285) train vimp2_I_CSFn2
(66/285) train vimp2_ANON972_CSFn_Aug0_Rot_-1_sd1
(67/285) train vimp2_ANON972_CSFn_Aug0_Rot_3_sd0
(68/285) train vimp2_ANON972_CSFn_Aug0_Rot_5_sd2
(69/285) train vimp2_ANON972_CSFn_Aug1_Rot_3_sd1
(70/285) train vimp2_ANON972_CSFn_Aug1_Rot_3_sd2
(71/285) train vimp2_ANON972_CSFn_Aug1_Rot_-5_sd0
(72/285) train vimp2_ANON972_CSFn_Aug2_Rot_1_sd1
(73/285) train vimp2_ANON972_CSFn_Aug2_Rot_-5_sd2
(74/285) train vimp2_ANON972_CSFn_Aug2_Rot_7_sd0
(75/285) train vimp2_ANON972_CSFn_Aug3_Rot_-3_sd0
(76/285) train vimp2_ANON972_CSFn_Aug3_Rot_-6_sd1
(77/285) train vimp2_ANON972_CSFn_Aug3_Rot_7_sd2
(78/285) train vimp2_ANON972_CSFn_Aug4_Rot_-4_sd0
(79/285) train vimp2_ANON972_CSFn_Aug4_Rot_4_sd1
(80/285) train vimp2_ANON972_CSFn_Aug4_Rot_4_sd2
(81/285) train vimp2_ANON972_CSFn_Aug5_Rot_-1_sd0
(82/285) train vimp2_ANON972_CSFn_Aug5_Rot_2_sd2
(83/285) train vimp2_ANON972_CSFn_Aug5_Rot_-4_sd1
(84/285) train vimp2_H_CSFn_Aug0_Rot_-3_sd2
(85/285) train vimp2_H_CSFn_Aug0_Rot_-4_sd0
(86/285) train vimp2_H_CSFn_Aug0_Rot_-4_sd1
(87/285) train vimp2_H_CSFn_Aug1_Rot_-1_sd2
(88/285) train vimp2_H_CSFn_Aug1_Rot_-2_sd0
(89/285) train vimp2_H_CSFn_Aug1_Rot_-4_sd1
(90/285) train vimp2_H_CSFn_Aug2_Rot_-1_sd1
(91/285) train vimp2_H_CSFn_Aug2_Rot_2_sd2
(92/285) train vimp2_H_CSFn_Aug2_Rot_6_sd0
(93/285) train vimp2_H_CSFn_Aug3_Rot_-1_sd0
(94/285) train vimp2_H_CSFn_Aug3_Rot_1_sd1
(95/285) train vimp2_H_CSFn_Aug3_Rot_-1_sd2
(96/285) train vimp2_H_CSFn_Aug4_Rot_-3_sd0
(97/285) train vimp2_H_CSFn_Aug4_Rot_4_sd1
(98/285) train vimp2_H_CSFn_Aug4_Rot_5_sd2
(99/285) train vimp2_H_CSFn_Aug5_Rot_-1_sd2
(100/285) train vimp2_H_CSFn_Aug5_Rot_-2_sd1
(101/285) train vimp2_H_CSFn_Aug5_Rot_-3_sd0
(102/285) train vimp2_I_CSFn_Aug0_Rot_1_sd1
(103/285) train vimp2_I_CSFn_Aug0_Rot_6_sd2
(104/285) train vimp2_I_CSFn_Aug0_Rot_-7_sd0
(105/285) train vimp2_I_CSFn_Aug1_Rot_2_sd0
(106/285) train vimp2_I_CSFn_Aug1_Rot_-5_sd2
(107/285) train vimp2_I_CSFn_Aug1_Rot_-6_sd1
(108/285) train vimp2_I_CSFn_Aug2_Rot_1_sd1
(109/285) train vimp2_I_CSFn_Aug2_Rot_-6_sd0
(110/285) train vimp2_I_CSFn_Aug2_Rot_-7_sd2
(111/285) train vimp2_I_CSFn_Aug3_Rot_-1_sd0
(112/285) train vimp2_I_CSFn_Aug3_Rot_-1_sd2
(113/285) train vimp2_I_CSFn_Aug3_Rot_-5_sd1
(114/285) train vimp2_I_CSFn_Aug4_Rot_2_sd1
(115/285) train vimp2_I_CSFn_Aug4_Rot_-6_sd0
(116/285) train vimp2_I_CSFn_Aug4_Rot_6_sd2
(117/285) train vimp2_I_CSFn_Aug5_Rot_-2_sd1
(118/285) train vimp2_I_CSFn_Aug5_Rot_3_sd2
(119/285) train vimp2_I_CSFn_Aug5_Rot_5_sd0
(120/285) train vimp2_ANON988_CSFn2
(121/285) train vimp2_M_CSFn2
(122/285) train vimp2_N_CSFn2
(123/285) train vimp2_ANON988_CSFn_Aug0_Rot_2_sd0
(124/285) train vimp2_ANON988_CSFn_Aug0_Rot_-2_sd2
(125/285) train vimp2_ANON988_CSFn_Aug0_Rot_4_sd1
(126/285) train vimp2_ANON988_CSFn_Aug1_Rot_-2_sd0
(127/285) train vimp2_ANON988_CSFn_Aug1_Rot_3_sd1
(128/285) train vimp2_ANON988_CSFn_Aug1_Rot_5_sd2
(129/285) train vimp2_ANON988_CSFn_Aug2_Rot_-6_sd2
(130/285) train vimp2_ANON988_CSFn_Aug2_Rot_-7_sd0
(131/285) train vimp2_ANON988_CSFn_Aug2_Rot_-7_sd1
(132/285) train vimp2_ANON988_CSFn_Aug3_Rot_-5_sd0
(133/285) train vimp2_ANON988_CSFn_Aug3_Rot_-7_sd1
(134/285) train vimp2_ANON988_CSFn_Aug3_Rot_-7_sd2
(135/285) train vimp2_ANON988_CSFn_Aug4_Rot_-1_sd0
(136/285) train vimp2_ANON988_CSFn_Aug4_Rot_2_sd1
(137/285) train vimp2_ANON988_CSFn_Aug4_Rot_-4_sd2
(138/285) train vimp2_ANON988_CSFn_Aug5_Rot_2_sd0
(139/285) train vimp2_ANON988_CSFn_Aug5_Rot_4_sd2
(140/285) train vimp2_ANON988_CSFn_Aug5_Rot_-5_sd1
(141/285) train vimp2_M_CSFn_Aug0_Rot_-1_sd1
(142/285) train vimp2_M_CSFn_Aug0_Rot_1_sd2
(143/285) train vimp2_M_CSFn_Aug0_Rot_-2_sd0
(144/285) train vimp2_M_CSFn_Aug1_Rot_-3_sd2
(145/285) train vimp2_M_CSFn_Aug1_Rot_-5_sd0
(146/285) train vimp2_M_CSFn_Aug1_Rot_-5_sd1
(147/285) train vimp2_M_CSFn_Aug2_Rot_-2_sd1
(148/285) train vimp2_M_CSFn_Aug2_Rot_4_sd2
(149/285) train vimp2_M_CSFn_Aug2_Rot_7_sd0
(150/285) train vimp2_M_CSFn_Aug3_Rot_2_sd1
(151/285) train vimp2_M_CSFn_Aug3_Rot_-3_sd0
(152/285) train vimp2_M_CSFn_Aug3_Rot_4_sd2
(153/285) train vimp2_M_CSFn_Aug4_Rot_4_sd0
(154/285) train vimp2_M_CSFn_Aug4_Rot_-5_sd1
(155/285) train vimp2_M_CSFn_Aug4_Rot_7_sd2
(156/285) train vimp2_M_CSFn_Aug5_Rot_-3_sd2
(157/285) train vimp2_M_CSFn_Aug5_Rot_4_sd1
(158/285) train vimp2_M_CSFn_Aug5_Rot_7_sd0
(159/285) train vimp2_N_CSFn_Aug0_Rot_2_sd0
(160/285) train vimp2_N_CSFn_Aug0_Rot_3_sd1
(161/285) train vimp2_N_CSFn_Aug0_Rot_6_sd2
(162/285) train vimp2_N_CSFn_Aug1_Rot_2_sd2
(163/285) train vimp2_N_CSFn_Aug1_Rot_3_sd1
(164/285) train vimp2_N_CSFn_Aug1_Rot_-4_sd0
(165/285) train vimp2_N_CSFn_Aug2_Rot_-1_sd1
(166/285) train vimp2_N_CSFn_Aug2_Rot_2_sd2
(167/285) train vimp2_N_CSFn_Aug2_Rot_5_sd0
(168/285) train vimp2_N_CSFn_Aug3_Rot_4_sd0
(169/285) train vimp2_N_CSFn_Aug3_Rot_6_sd1
(170/285) train vimp2_N_CSFn_Aug3_Rot_-6_sd2
(171/285) train vimp2_N_CSFn_Aug4_Rot_-1_sd0
(172/285) train vimp2_N_CSFn_Aug4_Rot_-1_sd2
(173/285) train vimp2_N_CSFn_Aug4_Rot_-2_sd1
(174/285) train vimp2_N_CSFn_Aug5_Rot_-1_sd0
(175/285) train vimp2_N_CSFn_Aug5_Rot_5_sd1
(176/285) train vimp2_N_CSFn_Aug5_Rot_-5_sd2
(177/285) train vimp2_ANON967_CSFn_Aug0_Rot_-3_sd0
(178/285) train vimp2_ANON967_CSFn_Aug0_Rot_-5_sd2
(179/285) train vimp2_ANON967_CSFn_Aug0_Rot_7_sd1
(180/285) train vimp2_ANON967_CSFn_Aug1_Rot_1_sd2
(181/285) train 2019-07-05 17:39:36.429498: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-05 17:39:38.905229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:85:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-07-05 17:39:38.905297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 17:39:39.285047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 17:39:39.285125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 17:39:39.285140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 17:39:39.285599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a’: File exists
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:14,  1.32s/it]Loading train:   1%|          | 2/285 [00:02<06:30,  1.38s/it]Loading train:   1%|          | 3/285 [00:04<06:19,  1.34s/it]Loading train:   1%|▏         | 4/285 [00:05<06:24,  1.37s/it]Loading train:   2%|▏         | 5/285 [00:06<05:56,  1.27s/it]Loading train:   2%|▏         | 6/285 [00:08<06:17,  1.35s/it]Loading train:   2%|▏         | 7/285 [00:09<06:33,  1.41s/it]Loading train:   3%|▎         | 8/285 [00:11<06:40,  1.44s/it]Loading train:   3%|▎         | 9/285 [00:12<06:19,  1.38s/it]Loading train:   4%|▎         | 10/285 [00:13<05:55,  1.29s/it]Loading train:   4%|▍         | 11/285 [00:14<05:55,  1.30s/it]Loading train:   4%|▍         | 12/285 [00:15<05:22,  1.18s/it]Loading train:   5%|▍         | 13/285 [00:16<05:06,  1.13s/it]Loading train:   5%|▍         | 14/285 [00:17<04:52,  1.08s/it]Loading train:   5%|▌         | 15/285 [00:18<04:55,  1.09s/it]Loading train:   6%|▌         | 16/285 [00:19<04:45,  1.06s/it]Loading train:   6%|▌         | 17/285 [00:20<04:31,  1.01s/it]Loading train:   6%|▋         | 18/285 [00:21<04:28,  1.01s/it]Loading train:   7%|▋         | 19/285 [00:22<04:40,  1.05s/it]Loading train:   7%|▋         | 20/285 [00:24<04:57,  1.12s/it]Loading train:   7%|▋         | 21/285 [00:25<04:47,  1.09s/it]Loading train:   8%|▊         | 22/285 [00:26<04:32,  1.04s/it]Loading train:   8%|▊         | 23/285 [00:26<04:23,  1.01s/it]Loading train:   8%|▊         | 24/285 [00:28<04:26,  1.02s/it]Loading train:   9%|▉         | 25/285 [00:29<04:21,  1.01s/it]Loading train:   9%|▉         | 26/285 [00:30<04:27,  1.03s/it]Loading train:   9%|▉         | 27/285 [00:31<04:34,  1.06s/it]Loading train:  10%|▉         | 28/285 [00:32<04:33,  1.06s/it]Loading train:  10%|█         | 29/285 [00:33<04:29,  1.05s/it]Loading train:  11%|█         | 30/285 [00:34<04:21,  1.03s/it]Loading train:  11%|█         | 31/285 [00:35<04:15,  1.01s/it]Loading train:  11%|█         | 32/285 [00:36<04:16,  1.01s/it]Loading train:  12%|█▏        | 33/285 [00:37<04:08,  1.02it/s]Loading train:  12%|█▏        | 34/285 [00:38<04:07,  1.01it/s]Loading train:  12%|█▏        | 35/285 [00:39<04:24,  1.06s/it]Loading train:  13%|█▎        | 36/285 [00:41<05:18,  1.28s/it]Loading train:  13%|█▎        | 37/285 [00:42<05:50,  1.41s/it]Loading train:  13%|█▎        | 38/285 [00:44<06:02,  1.47s/it]Loading train:  14%|█▎        | 39/285 [00:46<06:44,  1.64s/it]Loading train:  14%|█▍        | 40/285 [00:48<07:25,  1.82s/it]Loading train:  14%|█▍        | 41/285 [00:50<06:43,  1.65s/it]Loading train:  15%|█▍        | 42/285 [00:51<06:47,  1.68s/it]Loading train:  15%|█▌        | 43/285 [00:53<07:17,  1.81s/it]Loading train:  15%|█▌        | 44/285 [00:55<07:30,  1.87s/it]Loading train:  16%|█▌        | 45/285 [00:57<07:17,  1.82s/it]Loading train:  16%|█▌        | 46/285 [00:58<06:31,  1.64s/it]Loading train:  16%|█▋        | 47/285 [01:00<06:56,  1.75s/it]Loading train:  17%|█▋        | 48/285 [01:02<07:08,  1.81s/it]Loading train:  17%|█▋        | 49/285 [01:04<07:24,  1.88s/it]Loading train:  18%|█▊        | 50/285 [01:07<07:42,  1.97s/it]Loading train:  18%|█▊        | 51/285 [01:08<07:07,  1.83s/it]Loading train:  18%|█▊        | 52/285 [01:10<07:00,  1.80s/it]Loading train:  19%|█▊        | 53/285 [01:11<05:58,  1.55s/it]Loading train:  19%|█▉        | 54/285 [01:12<05:05,  1.32s/it]Loading train:  19%|█▉        | 55/285 [01:12<04:30,  1.17s/it]vimp2_ANON967_CSFn_Aug1_Rot_3_sd0
(182/285) train vimp2_ANON967_CSFn_Aug1_Rot_-6_sd1
(183/285) train vimp2_ANON967_CSFn_Aug2_Rot_4_sd2
(184/285) train vimp2_ANON967_CSFn_Aug2_Rot_-5_sd0
(185/285) train vimp2_ANON967_CSFn_Aug2_Rot_-7_sd1
(186/285) train vimp2_ANON967_CSFn_Aug3_Rot_4_sd2
(187/285) train vimp2_ANON967_CSFn_Aug3_Rot_5_sd1
(188/285) train vimp2_ANON967_CSFn_Aug3_Rot_-7_sd0
(189/285) train vimp2_ANON967_CSFn_Aug4_Rot_-4_sd0
(190/285) train vimp2_ANON967_CSFn_Aug4_Rot_6_sd1
(191/285) train vimp2_ANON967_CSFn_Aug4_Rot_6_sd2
(192/285) train vimp2_ANON967_CSFn_Aug5_Rot_-5_sd1
(193/285) train vimp2_ANON967_CSFn_Aug5_Rot_-7_sd0
(194/285) train vimp2_ANON967_CSFn_Aug5_Rot_-7_sd2
(195/285) train vimp2_E_CSFn_Aug0_Rot_-3_sd0
(196/285) train vimp2_E_CSFn_Aug0_Rot_-3_sd2
(197/285) train vimp2_E_CSFn_Aug0_Rot_6_sd1
(198/285) train vimp2_E_CSFn_Aug1_Rot_0_sd1
(199/285) train vimp2_E_CSFn_Aug1_Rot_1_sd0
(200/285) train vimp2_E_CSFn_Aug1_Rot_4_sd2
(201/285) train vimp2_E_CSFn_Aug2_Rot_2_sd1
(202/285) train vimp2_E_CSFn_Aug2_Rot_-3_sd0
(203/285) train vimp2_E_CSFn_Aug2_Rot_-7_sd2
(204/285) train vimp2_E_CSFn_Aug3_Rot_-1_sd2
(205/285) train vimp2_E_CSFn_Aug3_Rot_5_sd1
(206/285) train vimp2_E_CSFn_Aug3_Rot_7_sd0
(207/285) train vimp2_E_CSFn_Aug4_Rot_-5_sd1
(208/285) train vimp2_E_CSFn_Aug4_Rot_-5_sd2
(209/285) train vimp2_E_CSFn_Aug4_Rot_7_sd0
(210/285) train vimp2_E_CSFn_Aug5_Rot_-1_sd2
(211/285) train vimp2_E_CSFn_Aug5_Rot_6_sd0
(212/285) train vimp2_E_CSFn_Aug5_Rot_-6_sd1
(213/285) train vimp2_G_CSFn_Aug0_Rot_-6_sd1
(214/285) train vimp2_G_CSFn_Aug0_Rot_-7_sd0
(215/285) train vimp2_G_CSFn_Aug0_Rot_7_sd2
(216/285) train vimp2_G_CSFn_Aug1_Rot_2_sd0
(217/285) train vimp2_G_CSFn_Aug1_Rot_-2_sd2
(218/285) train vimp2_G_CSFn_Aug1_Rot_-5_sd1
(219/285) train vimp2_G_CSFn_Aug2_Rot_1_sd0
(220/285) train vimp2_G_CSFn_Aug2_Rot_-1_sd2
(221/285) train vimp2_G_CSFn_Aug2_Rot_4_sd1
(222/285) train vimp2_G_CSFn_Aug3_Rot_-5_sd2
(223/285) train vimp2_G_CSFn_Aug3_Rot_-6_sd0
(224/285) train vimp2_G_CSFn_Aug3_Rot_7_sd1
(225/285) train vimp2_G_CSFn_Aug4_Rot_-2_sd0
(226/285) train vimp2_G_CSFn_Aug4_Rot_2_sd1
(227/285) train vimp2_G_CSFn_Aug4_Rot_-6_sd2
(228/285) train vimp2_G_CSFn_Aug5_Rot_-1_sd2
(229/285) train vimp2_G_CSFn_Aug5_Rot_4_sd0
(230/285) train vimp2_G_CSFn_Aug5_Rot_4_sd1
(231/285) train vimp2_J_CSFn_Aug0_Rot_1_sd1
(232/285) train vimp2_J_CSFn_Aug0_Rot_-3_sd2
(233/285) train vimp2_J_CSFn_Aug0_Rot_-4_sd0
(234/285) train vimp2_J_CSFn_Aug1_Rot_-2_sd2
(235/285) train vimp2_J_CSFn_Aug1_Rot_4_sd0
(236/285) train vimp2_J_CSFn_Aug1_Rot_-6_sd1
(237/285) train vimp2_J_CSFn_Aug2_Rot_4_sd1
(238/285) train vimp2_J_CSFn_Aug2_Rot_-4_sd2
(239/285) train vimp2_J_CSFn_Aug2_Rot_7_sd0
(240/285) train vimp2_J_CSFn_Aug3_Rot_-1_sd1
(241/285) train vimp2_J_CSFn_Aug3_Rot_-2_sd0
(242/285) train vimp2_J_CSFn_Aug3_Rot_-3_sd2
(243/285) train vimp2_J_CSFn_Aug4_Rot_4_sd2
(244/285) train vimp2_J_CSFn_Aug4_Rot_5_sd0
(245/285) train vimp2_J_CSFn_Aug4_Rot_7_sd1
(246/285) train vimp2_J_CSFn_Aug5_Rot_4_sd2
(247/285) train vimp2_J_CSFn_Aug5_Rot_-6_sd1
(248/285) train vimp2_J_CSFn_Aug5_Rot_-7_sd0
(249/285) train vimp2_K_CSFn_Aug0_Rot_-2_sd2
(250/285) train vimp2_K_CSFn_Aug0_Rot_-6_sd0
(251/285) train vimp2_K_CSFn_Aug0_Rot_-6_sd1
(252/285) train vimp2_K_CSFn_Aug1_Rot_1_sd1
(253/285) train vimp2_K_CSFn_Aug1_Rot_1_sd2
(254/285) train vimp2_K_CSFn_Aug1_Rot_-5_sd0
(255/285) train vimp2_K_CSFn_Aug2_Rot_0_sd0
(256/285) train vimp2_K_CSFn_Aug2_Rot_-2_sd1
(257/285) train vimp2_K_CSFn_Aug2_Rot_-5_sd2
(258/285) train vimp2_K_CSFn_Aug3_Rot_5_sd2
(259/285) train vimp2_K_CSFn_Aug3_Rot_6_sd0
(260/285) train vimp2_K_CSFn_Aug3_Rot_-6_sd1
(261/285) train vimp2_K_CSFn_Aug4_Rot_-1_sd0
(262/285) train vimp2_K_CSFn_Aug4_Rot_2_sd2
(263/285) train vimp2_K_CSFn_Aug4_Rot_-6_sd1
(264/285) train vimp2_K_CSFn_Aug5_Rot_-2_sd1
(265/285) train vimp2_K_CSFn_Aug5_Rot_-2_sd2
(266/285) train vimp2_K_CSFn_Aug5_Rot_-3_sd0
(267/285) train vimp2_L_CSFn_Aug0_Rot_4_sd0
(268/285) train vimp2_L_CSFn_Aug0_Rot_5_sd2
(269/285) train vimp2_L_CSFn_Aug0_Rot_-7_sd1
(270/285) train vimp2_L_CSFn_Aug1_Rot_3_sd0
(271/285) train vimp2_L_CSFn_Aug1_Rot_5_sd1
(272/285) train vimp2_L_CSFn_Aug1_Rot_-5_sd2
(273/285) train vimp2_L_CSFn_Aug2_Rot_-4_sd1
(274/285) train vimp2_L_CSFn_Aug2_Rot_5_sd0
(275/285) train vimp2_L_CSFn_Aug2_Rot_-7_sd2
(276/285) train vimp2_L_CSFn_Aug3_Rot_5_sd2
(277/285) train vimp2_L_CSFn_Aug3_Rot_-7_sd0
(278/285) train vimp2_L_CSFn_Aug3_Rot_7_sd1
(279/285) train vimp2_L_CSFn_Aug4_Rot_-1_sd0
(280/285) train vimp2_L_CSFn_Aug4_Rot_3_sd2
(281/285) train vimp2_L_CSFn_Aug4_Rot_-7_sd1
(282/285) train vimp2_L_CSFn_Aug5_Rot_-1_sd0
(283/285) train vimp2_L_CSFn_Aug5_Rot_-5_sd1
(284/285) train vimp2_L_CSFn_Aug5_Rot_6_sd2
(0/3) test vimp2_A_CSFn2
(1/3) test vimp2_ANON765_CSFn2
(2/3) test vimp2_B_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Traceback (most recent call last):
  File "main.py", line 584, in <module>
    print(e)
  File "main.py", line 570, in EXP15a_TL_CSFn2_a
    Run(UserInfoB, IV)
  File "main.py", line 219, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 101, in Loop_Over_Nuclei
    if not check_if_num_Layers_fit(UserInfoB): Run_Main(UserInfoB)
  File "main.py", line 213, in Run_Main
    Loop_slicing_orientations(UserInfoB, InitValues)
  File "main.py", line 211, in Loop_slicing_orientations
    subRun(UserInfoB)
  File "main.py", line 205, in subRun
    else: normal_run(params)
  File "main.py", line 191, in normal_run
    Data, params = datasets.loadDataset(params)                             
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 384, in loadDataset
    Data = main_ReadingDataset(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 364, in main_ReadingDataset
    DataAll.Train_ForTest = readingAllSubjects(params.directories.Train.Input.Subjects, 'train')
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 328, in readingAllSubjects
    im, imF = readingImage(params, subject, mode)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 148, in readingImage
    imF, im = readingWithTranpose(subject2.address + '/' + subject2.ImageProcessed + '.nii.gz' , params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 106, in readingWithTranpose
    Image = ImageF.get_data()
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/nibabel/dataobj_images.py", line 203, in get_data
    data = np.asanyarray(self._dataobj)
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/numpy/core/numeric.py", line 553, in asanyarray
    return array(a, dtype, copy=False, order=order, subok=True)
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/nibabel/arrayproxy.py", line 355, in __array__
    raw_data = self.get_unscaled()
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/nibabel/arrayproxy.py", line 350, in get_unscaled
    mmap=self._mmap)
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/nibabel/volumeutils.py", line 524, in array_from_file
    n_read = infile.readinto(data_bytes)
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/gzip.py", line 276, in read
    return self._buffer.read(size)
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/_compression.py", line 68, in readinto
    data = self.read(len(byte_view))
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/gzip.py", line 471, in read
    uncompress = self._decompressor.decompress(buf, size)
KeyboardInterrupt
Exception ignored in: <bound method tqdm.__del__ of Loading train:  19%|█▉        | 55/285 [01:12<04:30,  1.17s/it]>
Traceback (most recent call last):
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/tqdm/_tqdm.py", line 931, in __del__
    self.close()
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/tqdm/_tqdm.py", line 1133, in close
    self._decr_instances(self)
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/tqdm/_tqdm.py", line 496, in _decr_instances
    cls.monitor.exit()
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/tqdm/_monitor.py", line 52, in exit
    self.join()
  File "/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/threading.py", line 1053, in join
    raise RuntimeError("cannot join current thread")
RuntimeError: cannot join current thread
*** DATASET ALREADY EXIST; PLEASE REMOVE 'train' & 'test' SUBFOLDERS ***
(0/285) train vimp2_ANON967_CSFn2
(1/285) train vimp2_E_CSFn2
(2/285) train vimp2_G_CSFn2
(3/285) train vimp2_J_CSFn2
(4/285) train vimp2_K_CSFn2
(5/285) train vimp2_L_CSFn2
(6/285) train vimp2_ANON911_CSFn2
(7/285) train vimp2_D_CSFn2
(8/285) train vimp2_F_CSFn2
(9/285) train vimp2_ANON911_CSFn_Aug0_Rot_-4_sd0
(10/285) train vimp2_ANON911_CSFn_Aug0_Rot_-4_sd1
(11/285) train vimp2_ANON911_CSFn_Aug0_Rot_7_sd2
(12/285) train vimp2_ANON911_CSFn_Aug1_Rot_1_sd2
(13/285) train vimp2_ANON911_CSFn_Aug1_Rot_3_sd1
(14/285) train vimp2_ANON911_CSFn_Aug1_Rot_4_sd0
(15/285) train vimp2_ANON911_CSFn_Aug2_Rot_1_sd1
(16/285) train vimp2_ANON911_CSFn_Aug2_Rot_2_sd2
(17/285) train vimp2_ANON911_CSFn_Aug2_Rot_5_sd0
(18/285) train vimp2_ANON911_CSFn_Aug3_Rot_-4_sd0
(19/285) train vimp2_ANON911_CSFn_Aug3_Rot_6_sd1
(20/285) train vimp2_ANON911_CSFn_Aug3_Rot_7_sd2
(21/285) train vimp2_ANON911_CSFn_Aug4_Rot_-2_sd1
(22/285) train vimp2_ANON911_CSFn_Aug4_Rot_7_sd0
(23/285) train vimp2_ANON911_CSFn_Aug4_Rot_7_sd2
(24/285) train vimp2_ANON911_CSFn_Aug5_Rot_1_sd0
(25/285) train vimp2_ANON911_CSFn_Aug5_Rot_-6_sd1
(26/285) train vimp2_ANON911_CSFn_Aug5_Rot_7_sd2
(27/285) train vimp2_D_CSFn_Aug0_Rot_4_sd0
(28/285) train vimp2_D_CSFn_Aug0_Rot_-5_sd2
(29/285) train vimp2_D_CSFn_Aug0_Rot_7_sd1
(30/285) train vimp2_D_CSFn_Aug1_Rot_-2_sd1
(31/285) train vimp2_D_CSFn_Aug1_Rot_-4_sd0
(32/285) train vimp2_D_CSFn_Aug1_Rot_5_sd2
(33/285) train vimp2_D_CSFn_Aug2_Rot_1_sd1
(34/285) train vimp2_D_CSFn_Aug2_Rot_-1_sd2
(35/285) train vimp2_D_CSFn_Aug2_Rot_-5_sd0
(36/285) train vimp2_D_CSFn_Aug3_Rot_-3_sd0
(37/285) train vimp2_D_CSFn_Aug3_Rot_-3_sd1
(38/285) train vimp2_D_CSFn_Aug3_Rot_3_sd2
(39/285) train vimp2_D_CSFn_Aug4_Rot_-4_sd0
(40/285) train vimp2_D_CSFn_Aug4_Rot_-5_sd2
(41/285) train vimp2_D_CSFn_Aug4_Rot_6_sd1
(42/285) train vimp2_D_CSFn_Aug5_Rot_1_sd2
(43/285) train vimp2_D_CSFn_Aug5_Rot_2_sd0
(44/285) train vimp2_D_CSFn_Aug5_Rot_7_sd1
(45/285) train vimp2_F_CSFn_Aug0_Rot_-1_sd2
(46/285) train vimp2_F_CSFn_Aug0_Rot_5_sd0
(47/285) train vimp2_F_CSFn_Aug0_Rot_7_sd1
(48/285) train vimp2_F_CSFn_Aug1_Rot_-5_sd2
(49/285) train vimp2_F_CSFn_Aug1_Rot_-7_sd0
(50/285) train vimp2_F_CSFn_Aug1_Rot_-7_sd1
(51/285) train vimp2_F_CSFn_Aug2_Rot_-3_sd1
(52/285) train vimp2_F_CSFn_Aug2_Rot_4_sd0
(53/285) train vimp2_F_CSFn_Aug2_Rot_-7_sd2
(54/285) train vimp2_F_CSFn_Aug3_Rot_2_sd0
(55/285) train vimp2_F_CSFn_Aug3_Rot_-3_sd2
(56/285) train vimp2_F_CSFn_Aug3_Rot_-7_sd1
(57/285) train vimp2_F_CSFn_Aug4_Rot_-1_sd0
(58/285) train vimp2_F_CSFn_Aug4_Rot_-2_sd1
(59/285) train vimp2_F_CSFn_Aug4_Rot_-2_sd2
(60/285) train vimp2_F_CSFn_Aug5_Rot_-1_sd1
(61/285) train vimp2_F_CSFn_Aug5_Rot_4_sd0
(62/285) train vimp2_F_CSFn_Aug5_Rot_-5_sd2
(63/285) train vimp2_ANON972_CSFn2
(64/285) train vimp2_H_CSFn2
(65/285) train vimp2_I_CSFn2
(66/285) train vimp2_ANON972_CSFn_Aug0_Rot_-1_sd1
(67/285) train vimp2_ANON972_CSFn_Aug0_Rot_3_sd0
(68/285) train vimp2_ANON972_CSFn_Aug0_Rot_5_sd2
(69/285) train vimp2_ANON972_CSFn_Aug1_Rot_3_sd1
(70/285) train vimp2_ANON972_CSFn_Aug1_Rot_3_sd2
(71/285) train vimp2_ANON972_CSFn_Aug1_Rot_-5_sd0
(72/285) train vimp2_ANON972_CSFn_Aug2_Rot_1_sd1
(73/285) train vimp2_ANON972_CSFn_Aug2_Rot_-5_sd2
(74/285) train vimp2_ANON972_CSFn_Aug2_Rot_7_sd0
(75/285) train vimp2_ANON972_CSFn_Aug3_Rot_-3_sd0
(76/285) train vimp2_ANON972_CSFn_Aug3_Rot_-6_sd1
(77/285) train vimp2_ANON972_CSFn_Aug3_Rot_7_sd2
(78/285) train vimp2_ANON972_CSFn_Aug4_Rot_-4_sd0
(79/285) train vimp2_ANON972_CSFn_Aug4_Rot_4_sd1
(80/285) train vimp2_ANON972_CSFn_Aug4_Rot_4_sd2
(81/285) train vimp2_ANON972_CSFn_Aug5_Rot_-1_sd0
(82/285) train vimp2_ANON972_CSFn_Aug5_Rot_2_sd2
(83/285) train vimp2_ANON972_CSFn_Aug5_Rot_-4_sd1
(84/285) train vimp2_H_CSFn_Aug0_Rot_-3_sd2
(85/285) train vimp2_H_CSFn_Aug0_Rot_-4_sd0
(86/285) train vimp2_H_CSFn_Aug0_Rot_-4_sd1
(87/285) train vimp2_H_CSFn_Aug1_Rot_-1_sd2
(88/285) train vimp2_H_CSFn_Aug1_Rot_-2_sd0
(89/285) train vimp2_H_CSFn_Aug1_Rot_-4_sd1
(90/285) train vimp2_H_CSFn_Aug2_Rot_-1_sd1
(91/285) train vimp2_H_CSFn_Aug2_Rot_2_sd2
(92/285) train vimp2_H_CSFn_Aug2_Rot_6_sd0
(93/285) train vimp2_H_CSFn_Aug3_Rot_-1_sd0
(94/285) train vimp2_H_CSFn_Aug3_Rot_1_sd1
(95/285) train vimp2_H_CSFn_Aug3_Rot_-1_sd2
(96/285) train vimp2_H_CSFn_Aug4_Rot_-3_sd0
(97/285) train vimp2_H_CSFn_Aug4_Rot_4_sd1
(98/285) train vimp2_H_CSFn_Aug4_Rot_5_sd2
(99/285) train vimp2_H_CSFn_Aug5_Rot_-1_sd2
(100/285) train vimp2_H_CSFn_Aug5_Rot_-2_sd1
(101/285) train vimp2_H_CSFn_Aug5_Rot_-3_sd0
(102/285) train vimp2_I_CSFn_Aug0_Rot_1_sd1
(103/285) train vimp2_I_CSFn_Aug0_Rot_6_sd2
(104/285) train vimp2_I_CSFn_Aug0_Rot_-7_sd0
(105/285) train vimp2_I_CSFn_Aug1_Rot_2_sd0
(106/285) train vimp2_I_CSFn_Aug1_Rot_-5_sd2
(107/285) train vimp2_I_CSFn_Aug1_Rot_-6_sd1
(108/285) train vimp2_I_CSFn_Aug2_Rot_1_sd1
(109/285) train vimp2_I_CSFn_Aug2_Rot_-6_sd0
(110/285) train vimp2_I_CSFn_Aug2_Rot_-7_sd2
(111/285) train vimp2_I_CSFn_Aug3_Rot_-1_sd0
(112/285) train vimp2_I_CSFn_Aug3_Rot_-1_sd2
(113/285) train vimp2_I_CSFn_Aug3_Rot_-5_sd1
(114/285) train vimp2_I_CSFn_Aug4_Rot_2_sd1
(115/285) train vimp2_I_CSFn_Aug4_Rot_-6_sd0
(116/285) train vimp2_I_CSFn_Aug4_Rot_6_sd2
(117/285) train vimp2_I_CSFn_Aug5_Rot_-2_sd1
(118/285) train vimp2_I_CSFn_Aug5_Rot_3_sd2
(119/285) train vimp2_I_CSFn_Aug5_Rot_5_sd0
(120/285) train vimp2_ANON988_CSFn2
(121/285) train vimp2_M_CSFn2
(122/285) train vimp2_N_CSFn2
(123/285) train vimp2_ANON988_CSFn_Aug0_Rot_2_sd0
(124/285) train vimp2_ANON988_CSFn_Aug0_Rot_-2_sd2
(125/285) train vimp2_ANON988_CSFn_Aug0_Rot_4_sd1
(126/285) train vimp2_ANON988_CSFn_Aug1_Rot_-2_sd0
(127/285) train vimp2_ANON988_CSFn_Aug1_Rot_3_sd1
(128/285) train vimp2_ANON988_CSFn_Aug1_Rot_5_sd2
(129/285) train vimp2_ANON988_CSFn_Aug2_Rot_-6_sd2
(130/285) train vimp2_ANON988_CSFn_Aug2_Rot_-7_sd0
(131/285) train vimp2_ANON988_CSFn_Aug2_Rot_-7_sd1
(132/285) train vimp2_ANON988_CSFn_Aug3_Rot_-5_sd0
(133/285) train vimp2_ANON988_CSFn_Aug3_Rot_-7_sd1
(134/285) train vimp2_ANON988_CSFn_Aug3_Rot_-7_sd2
(135/285) train vimp2_ANON988_CSFn_Aug4_Rot_-1_sd0
(136/285) train vimp2_ANON988_CSFn_Aug4_Rot_2_sd1
(137/285) train vimp2_ANON988_CSFn_Aug4_Rot_-4_sd2
(138/285) train vimp2_ANON988_CSFn_Aug5_Rot_2_sd0
(139/285) train vimp2_ANON988_CSFn_Aug5_Rot_4_sd2
(140/285) train vimp2_ANON988_CSFn_Aug5_Rot_-5_sd1
(141/285) train vimp2_M_CSFn_Aug0_Rot_-1_sd1
(142/285) train vimp2_M_CSFn_Aug0_Rot_1_sd2
(143/285) train vimp2_M_CSFn_Aug0_Rot_-2_sd0
(144/285) train vimp2_M_CSFn_Aug1_Rot_-3_sd2
(145/285) train vimp2_M_CSFn_Aug1_Rot_-5_sd0
(146/285) train vimp2_M_CSFn_Aug1_Rot_-5_sd1
(147/285) train vimp2_M_CSFn_Aug2_Rot_-2_sd1
(148/285) train vimp2_M_CSFn_Aug2_Rot_4_sd2
(149/285) train vimp2_M_CSFn_Aug2_Rot_7_sd0
(150/285) train vimp2_M_CSFn_Aug3_Rot_2_sd1
(151/285) train vimp2_M_CSFn_Aug3_Rot_-3_sd0
(152/285) train vimp2_M_CSFn_Aug3_Rot_4_sd2
(153/285) train vimp2_M_CSFn_Aug4_Rot_4_sd0
(154/285) train vimp2_M_CSFn_Aug4_Rot_-5_sd1
(155/285) train vimp2_M_CSFn_Aug4_Rot_7_sd2
(156/285) train vimp2_M_CSFn_Aug5_Rot_-3_sd2
(157/285) train vimp2_M_CSFn_Aug5_Rot_4_sd1
(158/285) train vimp2_M_CSFn_Aug5_Rot_7_sd0
(159/285) train vimp2_N_CSFn_Aug0_Rot_2_sd0
(160/285) train vimp2_N_CSFn_Aug0_Rot_3_sd1
(161/285) train vimp2_N_CSFn_Aug0_Rot_6_sd2
(162/285) train vimp2_N_CSFn_Aug1_Rot_2_sd2
(163/285) train vimp2_N_CSFn_Aug1_Rot_3_sd1
(164/285) train vimp2_N_CSFn_Aug1_Rot_-4_sd0
(165/285) train vimp2_N_CSFn_Aug2_Rot_-1_sd1
(166/285) train vimp2_N_CSFn_Aug2_Rot_2_sd2
(167/285) train vimp2_N_CSFn_Aug2_Rot_5_sd0
(168/285) train vimp2_N_CSFn_Aug3_Rot_4_sd0
(169/285) train vimp2_N_CSFn_Aug3_Rot_6_sd1
(170/285) train vimp2_N_CSFn_Aug3_Rot_-6_sd2
(171/285) train vimp2_N_CSFn_Aug4_Rot_-1_sd0
(172/285) train vimp2_N_CSFn_Aug4_Rot_-1_sd2
(173/285) train vimp2_N_CSFn_Aug4_Rot_-2_sd1
(174/285) train vimp2_N_CSFn_Aug5_Rot_-1_sd0
(175/285) train vimp2_N_CSFn_Aug5_Rot_5_sd1
(176/285) train vimp2_N_CSFn_Aug5_Rot_-5_sd2
(177/285) train vimp2_ANON967_CSFn_Aug0_Rot_-3_sd0
(178/285) train vimp2_ANON967_CSFn_Aug0_Rot_-5_sd2
(179/285) train vimp2_ANON967_CSFn_Aug0_Rot_7_sd1
(180/285) train vimp2_ANON967_CSFn_Aug1_Rot_1_sd2
(181/285) train 2019-07-05 17:41:33.242379: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-05 17:41:35.707860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:85:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2019-07-05 17:41:35.707934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 17:41:36.082487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 17:41:36.082567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 17:41:36.082578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 17:41:36.083027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a’: File exists
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Using TensorFlow backend.
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:35,  1.39s/it]Loading train:   1%|          | 2/285 [00:02<06:30,  1.38s/it]Loading train:   1%|          | 3/285 [00:03<06:15,  1.33s/it]Loading train:   1%|▏         | 4/285 [00:05<06:51,  1.47s/it]Loading train:   2%|▏         | 5/285 [00:06<06:14,  1.34s/it]Loading train:   2%|▏         | 6/285 [00:08<06:39,  1.43s/it]Loading train:   2%|▏         | 7/285 [00:10<06:56,  1.50s/it]Loading train:   3%|▎         | 8/285 [00:11<06:52,  1.49s/it]Loading train:   3%|▎         | 9/285 [00:12<06:35,  1.43s/it]Loading train:   4%|▎         | 10/285 [00:13<06:05,  1.33s/it]Loading train:   4%|▍         | 11/285 [00:14<05:39,  1.24s/it]Loading train:   4%|▍         | 12/285 [00:15<05:11,  1.14s/it]Loading train:   5%|▍         | 13/285 [00:16<05:05,  1.12s/it]Loading train:   5%|▍         | 14/285 [00:18<05:13,  1.16s/it]Loading train:   5%|▌         | 15/285 [00:19<05:05,  1.13s/it]Loading train:   6%|▌         | 16/285 [00:20<04:52,  1.09s/it]Loading train:   6%|▌         | 17/285 [00:21<04:43,  1.06s/it]Loading train:   6%|▋         | 18/285 [00:22<04:35,  1.03s/it]Loading train:   7%|▋         | 19/285 [00:23<04:25,  1.00it/s]Loading train:   7%|▋         | 20/285 [00:24<04:24,  1.00it/s]Loading train:   7%|▋         | 21/285 [00:25<04:23,  1.00it/s]Loading train:   8%|▊         | 22/285 [00:26<04:25,  1.01s/it]Loading train:   8%|▊         | 23/285 [00:27<04:16,  1.02it/s]Loading train:   8%|▊         | 24/285 [00:27<04:10,  1.04it/s]Loading train:   9%|▉         | 25/285 [00:28<04:10,  1.04it/s]Loading train:   9%|▉         | 26/285 [00:29<04:04,  1.06it/s]Loading train:   9%|▉         | 27/285 [00:31<05:22,  1.25s/it]Loading train:  10%|▉         | 28/285 [00:33<05:51,  1.37s/it]Loading train:  10%|█         | 29/285 [00:35<06:38,  1.56s/it]Loading train:  11%|█         | 30/285 [00:36<06:28,  1.52s/it]Loading train:  11%|█         | 31/285 [00:38<06:52,  1.62s/it]Loading train:  11%|█         | 32/285 [00:40<07:18,  1.73s/it]Loading train:  12%|█▏        | 33/285 [00:42<07:00,  1.67s/it]Loading train:  12%|█▏        | 34/285 [00:43<06:25,  1.54s/it]Loading train:  12%|█▏        | 35/285 [00:45<06:27,  1.55s/it]Loading train:  13%|█▎        | 36/285 [00:46<06:27,  1.56s/it]Loading train:  13%|█▎        | 37/285 [00:48<06:20,  1.53s/it]Loading train:  13%|█▎        | 38/285 [00:49<06:26,  1.56s/it]Loading train:  14%|█▎        | 39/285 [00:50<05:55,  1.45s/it]Loading train:  14%|█▍        | 40/285 [00:52<06:25,  1.57s/it]Loading train:  14%|█▍        | 41/285 [00:54<06:28,  1.59s/it]Loading train:  15%|█▍        | 42/285 [00:56<06:40,  1.65s/it]Loading train:  15%|█▌        | 43/285 [00:57<06:47,  1.68s/it]Loading train:  15%|█▌        | 44/285 [00:59<06:46,  1.69s/it]Loading train:  16%|█▌        | 45/285 [01:01<06:50,  1.71s/it]Loading train:  16%|█▌        | 46/285 [01:03<06:51,  1.72s/it]Loading train:  16%|█▋        | 47/285 [01:04<06:42,  1.69s/it]Loading train:  17%|█▋        | 48/285 [01:06<06:20,  1.61s/it]Loading train:  17%|█▋        | 49/285 [01:08<06:53,  1.75s/it]Loading train:  18%|█▊        | 50/285 [01:10<06:52,  1.76s/it]Loading train:  18%|█▊        | 51/285 [01:11<06:31,  1.67s/it]Loading train:  18%|█▊        | 52/285 [01:13<06:14,  1.61s/it]Loading train:  19%|█▊        | 53/285 [01:14<06:01,  1.56s/it]Loading train:  19%|█▉        | 54/285 [01:15<05:44,  1.49s/it]Loading train:  19%|█▉        | 55/285 [01:17<05:41,  1.49s/it]Loading train:  20%|█▉        | 56/285 [01:18<05:46,  1.51s/it]Loading train:  20%|██        | 57/285 [01:20<06:10,  1.63s/it]Loading train:  20%|██        | 58/285 [01:22<06:29,  1.71s/it]Loading train:  21%|██        | 59/285 [01:24<06:23,  1.70s/it]Loading train:  21%|██        | 60/285 [01:26<06:32,  1.74s/it]Loading train:  21%|██▏       | 61/285 [01:27<06:30,  1.74s/it]Loading train:  22%|██▏       | 62/285 [01:29<06:32,  1.76s/it]Loading train:  22%|██▏       | 63/285 [01:30<05:56,  1.61s/it]Loading train:  22%|██▏       | 64/285 [01:32<05:58,  1.62s/it]Loading train:  23%|██▎       | 65/285 [01:34<06:17,  1.72s/it]Loading train:  23%|██▎       | 66/285 [01:36<06:38,  1.82s/it]Loading train:  24%|██▎       | 67/285 [01:37<06:03,  1.67s/it]Loading train:  24%|██▍       | 68/285 [01:39<05:57,  1.65s/it]Loading train:  24%|██▍       | 69/285 [01:41<05:55,  1.64s/it]Loading train:  25%|██▍       | 70/285 [01:42<05:55,  1.65s/it]Loading train:  25%|██▍       | 71/285 [01:44<05:55,  1.66s/it]Loading train:  25%|██▌       | 72/285 [01:46<05:47,  1.63s/it]Loading train:  26%|██▌       | 73/285 [01:47<05:47,  1.64s/it]Loading train:  26%|██▌       | 74/285 [01:49<06:06,  1.74s/it]Loading train:  26%|██▋       | 75/285 [01:51<06:10,  1.76s/it]Loading train:  27%|██▋       | 76/285 [01:53<06:14,  1.79s/it]Loading train:  27%|██▋       | 77/285 [01:54<05:49,  1.68s/it]Loading train:  27%|██▋       | 78/285 [01:56<06:18,  1.83s/it]Loading train:  28%|██▊       | 79/285 [01:59<06:58,  2.03s/it]Loading train:  28%|██▊       | 80/285 [02:01<06:59,  2.04s/it]Loading train:  28%|██▊       | 81/285 [02:03<06:37,  1.95s/it]Loading train:  29%|██▉       | 82/285 [02:04<06:21,  1.88s/it]Loading train:  29%|██▉       | 83/285 [02:06<06:13,  1.85s/it]Loading train:  29%|██▉       | 84/285 [02:08<05:48,  1.73s/it]Loading train:  30%|██▉       | 85/285 [02:09<05:44,  1.72s/it]Loading train:  30%|███       | 86/285 [02:11<05:35,  1.69s/it]Loading train:  31%|███       | 87/285 [02:13<05:26,  1.65s/it]Loading train:  31%|███       | 88/285 [02:15<05:56,  1.81s/it]Loading train:  31%|███       | 89/285 [02:16<05:48,  1.78s/it]Loading train:  32%|███▏      | 90/285 [02:18<05:43,  1.76s/it]Loading train:  32%|███▏      | 91/285 [02:20<05:45,  1.78s/it]Loading train:  32%|███▏      | 92/285 [02:22<05:41,  1.77s/it]Loading train:  33%|███▎      | 93/285 [02:24<05:48,  1.82s/it]Loading train:  33%|███▎      | 94/285 [02:25<05:38,  1.77s/it]Loading train:  33%|███▎      | 95/285 [02:27<05:16,  1.67s/it]Loading train:  34%|███▎      | 96/285 [02:29<05:51,  1.86s/it]Loading train:  34%|███▍      | 97/285 [02:32<06:34,  2.10s/it]Loading train:  34%|███▍      | 98/285 [02:34<06:58,  2.24s/it]Loading train:  35%|███▍      | 99/285 [02:38<08:14,  2.66s/it]Loading train:  35%|███▌      | 100/285 [02:41<08:32,  2.77s/it]Loading train:  35%|███▌      | 101/285 [02:44<08:21,  2.73s/it]Loading train:  36%|███▌      | 102/285 [02:47<08:53,  2.92s/it]Loading train:  36%|███▌      | 103/285 [02:50<08:49,  2.91s/it]Loading train:  36%|███▋      | 104/285 [02:52<08:07,  2.69s/it]Loading train:  37%|███▋      | 105/285 [02:54<07:09,  2.39s/it]Loading train:  37%|███▋      | 106/285 [02:55<06:25,  2.15s/it]Loading train:  38%|███▊      | 107/285 [02:57<05:34,  1.88s/it]Loading train:  38%|███▊      | 108/285 [02:58<05:13,  1.77s/it]Loading train:  38%|███▊      | 109/285 [03:00<05:11,  1.77s/it]Loading train:  39%|███▊      | 110/285 [03:02<05:21,  1.84s/it]Loading train:  39%|███▉      | 111/285 [03:03<05:08,  1.78s/it]Loading train:  39%|███▉      | 112/285 [03:05<04:58,  1.73s/it]Loading train:  40%|███▉      | 113/285 [03:07<04:53,  1.71s/it]Loading train:  40%|████      | 114/285 [03:08<04:48,  1.69s/it]Loading train:  40%|████      | 115/285 [03:10<04:38,  1.64s/it]Loading train:  41%|████      | 116/285 [03:12<04:44,  1.68s/it]Loading train:  41%|████      | 117/285 [03:14<05:00,  1.79s/it]Loading train:  41%|████▏     | 118/285 [03:16<05:19,  1.91s/it]Loading train:  42%|████▏     | 119/285 [03:18<05:16,  1.91s/it]Loading train:  42%|████▏     | 120/285 [03:20<05:04,  1.85s/it]Loading train:  42%|████▏     | 121/285 [03:22<05:28,  2.00s/it]Loading train:  43%|████▎     | 122/285 [03:24<05:22,  1.98s/it]Loading train:  43%|████▎     | 123/285 [03:26<05:22,  1.99s/it]Loading train:  44%|████▎     | 124/285 [03:28<05:08,  1.92s/it]Loading train:  44%|████▍     | 125/285 [03:29<04:39,  1.75s/it]Loading train:  44%|████▍     | 126/285 [03:31<04:32,  1.71s/it]Loading train:  45%|████▍     | 127/285 [03:33<04:40,  1.78s/it]Loading train:  45%|████▍     | 128/285 [03:34<04:23,  1.68s/it]Loading train:  45%|████▌     | 129/285 [03:36<04:21,  1.67s/it]Loading train:  46%|████▌     | 130/285 [03:38<04:35,  1.78s/it]Loading train:  46%|████▌     | 131/285 [03:40<04:43,  1.84s/it]Loading train:  46%|████▋     | 132/285 [03:43<05:34,  2.19s/it]Loading train:  47%|████▋     | 133/285 [03:44<05:00,  1.98s/it]Loading train:  47%|████▋     | 134/285 [03:45<04:30,  1.79s/it]Loading train:  47%|████▋     | 135/285 [03:47<04:05,  1.63s/it]Loading train:  48%|████▊     | 136/285 [03:48<04:08,  1.67s/it]Loading train:  48%|████▊     | 137/285 [03:50<03:52,  1.57s/it]Loading train:  48%|████▊     | 138/285 [03:52<04:00,  1.63s/it]Loading train:  49%|████▉     | 139/285 [03:53<04:00,  1.65s/it]Loading train:  49%|████▉     | 140/285 [03:55<04:04,  1.69s/it]Loading train:  49%|████▉     | 141/285 [03:57<04:10,  1.74s/it]Loading train:  50%|████▉     | 142/285 [03:58<04:01,  1.69s/it]Loading train:  50%|█████     | 143/285 [04:00<03:56,  1.67s/it]Loading train:  51%|█████     | 144/285 [04:02<03:50,  1.64s/it]Loading train:  51%|█████     | 145/285 [04:03<03:42,  1.59s/it]Loading train:  51%|█████     | 146/285 [04:05<03:39,  1.58s/it]Loading train:  52%|█████▏    | 147/285 [04:07<03:48,  1.66s/it]Loading train:  52%|█████▏    | 148/285 [04:08<03:22,  1.48s/it]Loading train:  52%|█████▏    | 149/285 [04:09<03:25,  1.51s/it]Loading train:  53%|█████▎    | 150/285 [04:11<03:28,  1.54s/it]Loading train:  53%|█████▎    | 151/285 [04:12<03:30,  1.57s/it]Loading train:  53%|█████▎    | 152/285 [04:14<03:33,  1.60s/it]Loading train:  54%|█████▎    | 153/285 [04:15<03:20,  1.52s/it]Loading train:  54%|█████▍    | 154/285 [04:17<03:16,  1.50s/it]Loading train:  54%|█████▍    | 155/285 [04:19<03:23,  1.57s/it]Loading train:  55%|█████▍    | 156/285 [04:20<03:21,  1.56s/it]Loading train:  55%|█████▌    | 157/285 [04:22<03:15,  1.53s/it]Loading train:  55%|█████▌    | 158/285 [04:23<03:11,  1.51s/it]Loading train:  56%|█████▌    | 159/285 [04:25<03:12,  1.53s/it]Loading train:  56%|█████▌    | 160/285 [04:26<03:16,  1.58s/it]Loading train:  56%|█████▋    | 161/285 [04:28<03:32,  1.71s/it]Loading train:  57%|█████▋    | 162/285 [04:30<03:19,  1.62s/it]Loading train:  57%|█████▋    | 163/285 [04:31<03:09,  1.55s/it]Loading train:  58%|█████▊    | 164/285 [04:32<02:47,  1.38s/it]Loading train:  58%|█████▊    | 165/285 [04:34<02:53,  1.44s/it]Loading train:  58%|█████▊    | 166/285 [04:35<02:47,  1.41s/it]Loading train:  59%|█████▊    | 167/285 [04:36<02:41,  1.37s/it]Loading train:  59%|█████▉    | 168/285 [04:38<02:47,  1.43s/it]Loading train:  59%|█████▉    | 169/285 [04:39<02:46,  1.44s/it]Loading train:  60%|█████▉    | 170/285 [04:41<02:41,  1.40s/it]Loading train:  60%|██████    | 171/285 [04:42<02:43,  1.43s/it]Loading train:  60%|██████    | 172/285 [04:44<02:43,  1.45s/it]Loading train:  61%|██████    | 173/285 [04:46<03:01,  1.62s/it]Loading train:  61%|██████    | 174/285 [04:47<02:51,  1.55s/it]Loading train:  61%|██████▏   | 175/285 [04:49<02:53,  1.58s/it]Loading train:  62%|██████▏   | 176/285 [04:50<02:56,  1.61s/it]Loading train:  62%|██████▏   | 177/285 [04:52<02:53,  1.60s/it]Loading train:  62%|██████▏   | 178/285 [04:54<02:56,  1.65s/it]Loading train:  63%|██████▎   | 179/285 [04:56<02:58,  1.69s/it]Loading train:  63%|██████▎   | 180/285 [04:57<02:56,  1.68s/it]Loading train:  64%|██████▎   | 181/285 [04:59<02:47,  1.61s/it]Loading train:  64%|██████▍   | 182/285 [05:00<02:38,  1.54s/it]Loading train:  64%|██████▍   | 183/285 [05:02<02:39,  1.56s/it]Loading train:  65%|██████▍   | 184/285 [05:03<02:36,  1.55s/it]Loading train:  65%|██████▍   | 185/285 [05:05<02:36,  1.56s/it]Loading train:  65%|██████▌   | 186/285 [05:07<02:50,  1.72s/it]Loading train:  66%|██████▌   | 187/285 [05:09<02:55,  1.79s/it]Loading train:  66%|██████▌   | 188/285 [05:11<02:59,  1.85s/it]Loading train:  66%|██████▋   | 189/285 [05:13<03:02,  1.90s/it]Loading train:  67%|██████▋   | 190/285 [05:14<02:48,  1.77s/it]Loading train:  67%|██████▋   | 191/285 [05:16<02:44,  1.75s/it]Loading train:  67%|██████▋   | 192/285 [05:18<02:48,  1.81s/it]Loading train:  68%|██████▊   | 193/285 [05:20<02:40,  1.75s/it]Loading train:  68%|██████▊   | 194/285 [05:21<02:37,  1.73s/it]Loading train:  68%|██████▊   | 195/285 [05:23<02:31,  1.69s/it]Loading train:  69%|██████▉   | 196/285 [05:25<02:40,  1.80s/it]Loading train:  69%|██████▉   | 197/285 [05:27<02:38,  1.80s/it]Loading train:  69%|██████▉   | 198/285 [05:28<02:36,  1.80s/it]Loading train:  70%|██████▉   | 199/285 [05:30<02:30,  1.76s/it]Loading train:  70%|███████   | 200/285 [05:33<03:04,  2.17s/it]Loading train:  71%|███████   | 201/285 [05:36<03:11,  2.28s/it]Loading train:  71%|███████   | 202/285 [05:38<03:07,  2.26s/it]Loading train:  71%|███████   | 203/285 [05:40<03:08,  2.30s/it]Loading train:  72%|███████▏  | 204/285 [05:42<02:57,  2.19s/it]Loading train:  72%|███████▏  | 205/285 [05:44<02:37,  1.97s/it]Loading train:  72%|███████▏  | 206/285 [05:45<02:26,  1.86s/it]Loading train:  73%|███████▎  | 207/285 [05:47<02:17,  1.76s/it]Loading train:  73%|███████▎  | 208/285 [05:48<02:08,  1.66s/it]Loading train:  73%|███████▎  | 209/285 [05:50<02:07,  1.68s/it]Loading train:  74%|███████▎  | 210/285 [05:52<02:03,  1.64s/it]Loading train:  74%|███████▍  | 211/285 [05:53<02:00,  1.62s/it]Loading train:  74%|███████▍  | 212/285 [05:55<01:54,  1.56s/it]Loading train:  75%|███████▍  | 213/285 [05:56<01:54,  1.60s/it]Loading train:  75%|███████▌  | 214/285 [05:58<02:04,  1.76s/it]Loading train:  75%|███████▌  | 215/285 [06:00<02:02,  1.75s/it]Loading train:  76%|███████▌  | 216/285 [06:02<01:59,  1.73s/it]Loading train:  76%|███████▌  | 217/285 [06:04<01:58,  1.74s/it]Loading train:  76%|███████▋  | 218/285 [06:05<01:58,  1.77s/it]Loading train:  77%|███████▋  | 219/285 [06:07<01:59,  1.81s/it]Loading train:  77%|███████▋  | 220/285 [06:09<02:03,  1.90s/it]Loading train:  78%|███████▊  | 221/285 [06:11<02:03,  1.92s/it]Loading train:  78%|███████▊  | 222/285 [06:13<01:51,  1.77s/it]Loading train:  78%|███████▊  | 223/285 [06:15<01:50,  1.78s/it]Loading train:  79%|███████▊  | 224/285 [06:16<01:43,  1.70s/it]Loading train:  79%|███████▉  | 225/285 [06:18<01:40,  1.68s/it]Loading train:  79%|███████▉  | 226/285 [06:19<01:36,  1.63s/it]Loading train:  80%|███████▉  | 227/285 [06:21<01:37,  1.68s/it]Loading train:  80%|████████  | 228/285 [06:23<01:38,  1.73s/it]Loading train:  80%|████████  | 229/285 [06:24<01:29,  1.60s/it]Loading train:  81%|████████  | 230/285 [06:25<01:21,  1.49s/it]Loading train:  81%|████████  | 231/285 [06:27<01:21,  1.52s/it]Loading train:  81%|████████▏ | 232/285 [06:29<01:19,  1.50s/it]Loading train:  82%|████████▏ | 233/285 [06:31<01:26,  1.66s/it]Loading train:  82%|████████▏ | 234/285 [06:32<01:27,  1.72s/it]Loading train:  82%|████████▏ | 235/285 [06:35<01:31,  1.83s/it]Loading train:  83%|████████▎ | 236/285 [06:37<01:33,  1.90s/it]Loading train:  83%|████████▎ | 237/285 [06:38<01:26,  1.81s/it]Loading train:  84%|████████▎ | 238/285 [06:40<01:25,  1.81s/it]Loading train:  84%|████████▍ | 239/285 [06:42<01:27,  1.90s/it]Loading train:  84%|████████▍ | 240/285 [06:44<01:25,  1.90s/it]Loading train:  85%|████████▍ | 241/285 [06:46<01:21,  1.85s/it]Loading train:  85%|████████▍ | 242/285 [06:47<01:17,  1.80s/it]Loading train:  85%|████████▌ | 243/285 [06:50<01:19,  1.90s/it]Loading train:  86%|████████▌ | 244/285 [06:51<01:18,  1.91s/it]Loading train:  86%|████████▌ | 245/285 [06:53<01:13,  1.85s/it]Loading train:  86%|████████▋ | 246/285 [06:55<01:09,  1.79s/it]Loading train:  87%|████████▋ | 247/285 [06:57<01:10,  1.85s/it]Loading train:  87%|████████▋ | 248/285 [06:59<01:09,  1.87s/it]Loading train:  87%|████████▋ | 249/285 [07:02<01:22,  2.28s/it]Loading train:  88%|████████▊ | 250/285 [07:05<01:25,  2.44s/it]Loading train:  88%|████████▊ | 251/285 [07:07<01:21,  2.40s/it]Loading train:  88%|████████▊ | 252/285 [07:10<01:19,  2.41s/it]Loading train:  89%|████████▉ | 253/285 [07:12<01:17,  2.43s/it]Loading train:  89%|████████▉ | 254/285 [07:13<01:03,  2.03s/it]Loading train:  89%|████████▉ | 255/285 [07:15<00:56,  1.89s/it]Loading train:  90%|████████▉ | 256/285 [07:16<00:50,  1.75s/it]Loading train:  90%|█████████ | 257/285 [07:18<00:48,  1.75s/it]Loading train:  91%|█████████ | 258/285 [07:20<00:47,  1.75s/it]Loading train:  91%|█████████ | 259/285 [07:21<00:43,  1.67s/it]Loading train:  91%|█████████ | 260/285 [07:23<00:40,  1.61s/it]Loading train:  92%|█████████▏| 261/285 [07:24<00:35,  1.50s/it]Loading train:  92%|█████████▏| 262/285 [07:25<00:34,  1.50s/it]Loading train:  92%|█████████▏| 263/285 [07:27<00:35,  1.60s/it]Loading train:  93%|█████████▎| 264/285 [07:28<00:32,  1.53s/it]Loading train:  93%|█████████▎| 265/285 [07:30<00:30,  1.55s/it]Loading train:  93%|█████████▎| 266/285 [07:32<00:30,  1.59s/it]Loading train:  94%|█████████▎| 267/285 [07:33<00:27,  1.52s/it]Loading train:  94%|█████████▍| 268/285 [07:35<00:26,  1.57s/it]Loading train:  94%|█████████▍| 269/285 [07:37<00:25,  1.62s/it]Loading train:  95%|█████████▍| 270/285 [07:39<00:26,  1.77s/it]Loading train:  95%|█████████▌| 271/285 [07:41<00:25,  1.80s/it]Loading train:  95%|█████████▌| 272/285 [07:42<00:23,  1.81s/it]Loading train:  96%|█████████▌| 273/285 [07:44<00:22,  1.84s/it]Loading train:  96%|█████████▌| 274/285 [07:46<00:20,  1.84s/it]Loading train:  96%|█████████▋| 275/285 [07:48<00:19,  1.93s/it]Loading train:  97%|█████████▋| 276/285 [07:50<00:17,  1.91s/it]Loading train:  97%|█████████▋| 277/285 [07:52<00:14,  1.86s/it]Loading train:  98%|█████████▊| 278/285 [07:54<00:12,  1.82s/it]Loading train:  98%|█████████▊| 279/285 [07:56<00:11,  1.90s/it]Loading train:  98%|█████████▊| 280/285 [07:57<00:08,  1.78s/it]Loading train:  99%|█████████▊| 281/285 [07:59<00:07,  1.84s/it]Loading train:  99%|█████████▉| 282/285 [08:01<00:05,  1.82s/it]Loading train:  99%|█████████▉| 283/285 [08:03<00:03,  1.80s/it]Loading train: 100%|█████████▉| 284/285 [08:05<00:01,  1.81s/it]Loading train: 100%|██████████| 285/285 [08:06<00:00,  1.83s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 2/285 [00:00<00:14, 18.92it/s]concatenating: train:   2%|▏         | 5/285 [00:00<00:14, 19.92it/s]concatenating: train:   3%|▎         | 8/285 [00:00<00:12, 21.56it/s]concatenating: train:   4%|▍         | 11/285 [00:00<00:12, 21.92it/s]concatenating: train:   5%|▍         | 14/285 [00:00<00:11, 23.40it/s]concatenating: train:   6%|▋         | 18/285 [00:00<00:10, 25.67it/s]concatenating: train:   7%|▋         | 21/285 [00:00<00:10, 25.76it/s]concatenating: train:   9%|▉         | 25/285 [00:00<00:09, 27.65it/s]concatenating: train:  10%|█         | 29/285 [00:01<00:09, 28.39it/s]concatenating: train:  12%|█▏        | 33/285 [00:01<00:08, 28.69it/s]concatenating: train:  13%|█▎        | 36/285 [00:01<00:09, 26.32it/s]concatenating: train:  14%|█▎        | 39/285 [00:01<00:09, 26.46it/s]concatenating: train:  15%|█▌        | 43/285 [00:01<00:08, 28.94it/s]concatenating: train:  16%|█▋        | 47/285 [00:01<00:07, 30.64it/s]concatenating: train:  18%|█▊        | 52/285 [00:01<00:06, 33.87it/s]concatenating: train:  20%|██        | 57/285 [00:01<00:06, 35.85it/s]concatenating: train:  22%|██▏       | 62/285 [00:02<00:05, 37.78it/s]concatenating: train:  23%|██▎       | 66/285 [00:02<00:06, 36.30it/s]concatenating: train:  25%|██▍       | 70/285 [00:02<00:06, 31.10it/s]concatenating: train:  26%|██▌       | 74/285 [00:02<00:07, 28.98it/s]concatenating: train:  27%|██▋       | 78/285 [00:02<00:08, 24.96it/s]concatenating: train:  29%|██▉       | 82/285 [00:02<00:07, 26.55it/s]concatenating: train:  30%|██▉       | 85/285 [00:02<00:07, 27.23it/s]concatenating: train:  32%|███▏      | 90/285 [00:03<00:06, 31.07it/s]concatenating: train:  33%|███▎      | 94/285 [00:03<00:05, 32.65it/s]concatenating: train:  34%|███▍      | 98/285 [00:03<00:05, 34.20it/s]concatenating: train:  36%|███▋      | 104/285 [00:03<00:04, 38.93it/s]concatenating: train:  39%|███▊      | 110/285 [00:03<00:04, 42.08it/s]concatenating: train:  40%|████      | 115/285 [00:03<00:03, 42.76it/s]concatenating: train:  42%|████▏     | 120/285 [00:03<00:03, 43.30it/s]concatenating: train:  44%|████▍     | 125/285 [00:03<00:03, 43.26it/s]concatenating: train:  46%|████▌     | 131/285 [00:03<00:03, 45.95it/s]concatenating: train:  48%|████▊     | 136/285 [00:04<00:03, 46.90it/s]concatenating: train:  49%|████▉     | 141/285 [00:04<00:03, 43.04it/s]concatenating: train:  51%|█████     | 146/285 [00:04<00:03, 44.39it/s]concatenating: train:  53%|█████▎    | 151/285 [00:04<00:03, 39.46it/s]concatenating: train:  55%|█████▍    | 156/285 [00:04<00:03, 40.84it/s]concatenating: train:  56%|█████▋    | 161/285 [00:04<00:03, 36.13it/s]concatenating: train:  58%|█████▊    | 165/285 [00:04<00:03, 35.61it/s]concatenating: train:  60%|█████▉    | 170/285 [00:04<00:03, 37.21it/s]concatenating: train:  61%|██████    | 174/285 [00:05<00:03, 29.87it/s]concatenating: train:  62%|██████▏   | 178/285 [00:05<00:04, 26.02it/s]concatenating: train:  64%|██████▎   | 181/285 [00:05<00:03, 26.24it/s]concatenating: train:  65%|██████▍   | 184/285 [00:05<00:04, 23.43it/s]concatenating: train:  66%|██████▌   | 187/285 [00:05<00:04, 22.36it/s]concatenating: train:  67%|██████▋   | 191/285 [00:05<00:03, 24.12it/s]concatenating: train:  68%|██████▊   | 194/285 [00:06<00:03, 25.36it/s]concatenating: train:  69%|██████▉   | 197/285 [00:06<00:03, 23.59it/s]concatenating: train:  71%|███████   | 201/285 [00:06<00:03, 25.64it/s]concatenating: train:  72%|███████▏  | 204/285 [00:06<00:03, 22.09it/s]concatenating: train:  73%|███████▎  | 207/285 [00:06<00:03, 23.63it/s]concatenating: train:  79%|███████▊  | 224/285 [00:06<00:01, 31.76it/s]concatenating: train:  85%|████████▌ | 243/285 [00:06<00:01, 40.81it/s]concatenating: train:  88%|████████▊ | 252/285 [00:07<00:01, 30.90it/s]concatenating: train:  91%|█████████ | 259/285 [00:07<00:00, 31.17it/s]concatenating: train:  93%|█████████▎| 265/285 [00:07<00:00, 31.41it/s]concatenating: train:  95%|█████████▌| 271/285 [00:07<00:00, 36.62it/s]concatenating: train:  97%|█████████▋| 277/285 [00:07<00:00, 39.19it/s]concatenating: train:  99%|█████████▉| 283/285 [00:08<00:00, 39.94it/s]concatenating: train: 100%|██████████| 285/285 [00:08<00:00, 35.14it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:02<00:04,  2.15s/it]Loading test:  67%|██████▋   | 2/3 [00:04<00:02,  2.06s/it]Loading test: 100%|██████████| 3/3 [00:05<00:00,  1.99s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation:  67%|██████▋   | 2/3 [00:00<00:00, 15.77it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 15.60it/s]
/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:10,  3.73it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:08,  4.15it/s]loading the weights for Unet:  10%|█         | 4/40 [00:00<00:08,  4.12it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:06,  5.33it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:07,  4.04it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:06,  4.60it/s]loading the weights for Unet:  30%|███       | 12/40 [00:02<00:06,  4.37it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:04,  5.35it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:04,  4.79it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:03<00:04,  4.89it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:03<00:04,  4.61it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:02,  5.68it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:03<00:02,  6.34it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:04<00:02,  5.27it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:04<00:02,  5.88it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:04<00:02,  4.98it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:04<00:01,  5.94it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:05<00:01,  5.72it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:05<00:00,  5.01it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:05<00:00,  5.57it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:06<00:00,  4.77it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:06<00:00,  6.65it/s]vimp2_ANON967_CSFn_Aug1_Rot_3_sd0
(182/285) train vimp2_ANON967_CSFn_Aug1_Rot_-6_sd1
(183/285) train vimp2_ANON967_CSFn_Aug2_Rot_4_sd2
(184/285) train vimp2_ANON967_CSFn_Aug2_Rot_-5_sd0
(185/285) train vimp2_ANON967_CSFn_Aug2_Rot_-7_sd1
(186/285) train vimp2_ANON967_CSFn_Aug3_Rot_4_sd2
(187/285) train vimp2_ANON967_CSFn_Aug3_Rot_5_sd1
(188/285) train vimp2_ANON967_CSFn_Aug3_Rot_-7_sd0
(189/285) train vimp2_ANON967_CSFn_Aug4_Rot_-4_sd0
(190/285) train vimp2_ANON967_CSFn_Aug4_Rot_6_sd1
(191/285) train vimp2_ANON967_CSFn_Aug4_Rot_6_sd2
(192/285) train vimp2_ANON967_CSFn_Aug5_Rot_-5_sd1
(193/285) train vimp2_ANON967_CSFn_Aug5_Rot_-7_sd0
(194/285) train vimp2_ANON967_CSFn_Aug5_Rot_-7_sd2
(195/285) train vimp2_E_CSFn_Aug0_Rot_-3_sd0
(196/285) train vimp2_E_CSFn_Aug0_Rot_-3_sd2
(197/285) train vimp2_E_CSFn_Aug0_Rot_6_sd1
(198/285) train vimp2_E_CSFn_Aug1_Rot_0_sd1
(199/285) train vimp2_E_CSFn_Aug1_Rot_1_sd0
(200/285) train vimp2_E_CSFn_Aug1_Rot_4_sd2
(201/285) train vimp2_E_CSFn_Aug2_Rot_2_sd1
(202/285) train vimp2_E_CSFn_Aug2_Rot_-3_sd0
(203/285) train vimp2_E_CSFn_Aug2_Rot_-7_sd2
(204/285) train vimp2_E_CSFn_Aug3_Rot_-1_sd2
(205/285) train vimp2_E_CSFn_Aug3_Rot_5_sd1
(206/285) train vimp2_E_CSFn_Aug3_Rot_7_sd0
(207/285) train vimp2_E_CSFn_Aug4_Rot_-5_sd1
(208/285) train vimp2_E_CSFn_Aug4_Rot_-5_sd2
(209/285) train vimp2_E_CSFn_Aug4_Rot_7_sd0
(210/285) train vimp2_E_CSFn_Aug5_Rot_-1_sd2
(211/285) train vimp2_E_CSFn_Aug5_Rot_6_sd0
(212/285) train vimp2_E_CSFn_Aug5_Rot_-6_sd1
(213/285) train vimp2_G_CSFn_Aug0_Rot_-6_sd1
(214/285) train vimp2_G_CSFn_Aug0_Rot_-7_sd0
(215/285) train vimp2_G_CSFn_Aug0_Rot_7_sd2
(216/285) train vimp2_G_CSFn_Aug1_Rot_2_sd0
(217/285) train vimp2_G_CSFn_Aug1_Rot_-2_sd2
(218/285) train vimp2_G_CSFn_Aug1_Rot_-5_sd1
(219/285) train vimp2_G_CSFn_Aug2_Rot_1_sd0
(220/285) train vimp2_G_CSFn_Aug2_Rot_-1_sd2
(221/285) train vimp2_G_CSFn_Aug2_Rot_4_sd1
(222/285) train vimp2_G_CSFn_Aug3_Rot_-5_sd2
(223/285) train vimp2_G_CSFn_Aug3_Rot_-6_sd0
(224/285) train vimp2_G_CSFn_Aug3_Rot_7_sd1
(225/285) train vimp2_G_CSFn_Aug4_Rot_-2_sd0
(226/285) train vimp2_G_CSFn_Aug4_Rot_2_sd1
(227/285) train vimp2_G_CSFn_Aug4_Rot_-6_sd2
(228/285) train vimp2_G_CSFn_Aug5_Rot_-1_sd2
(229/285) train vimp2_G_CSFn_Aug5_Rot_4_sd0
(230/285) train vimp2_G_CSFn_Aug5_Rot_4_sd1
(231/285) train vimp2_J_CSFn_Aug0_Rot_1_sd1
(232/285) train vimp2_J_CSFn_Aug0_Rot_-3_sd2
(233/285) train vimp2_J_CSFn_Aug0_Rot_-4_sd0
(234/285) train vimp2_J_CSFn_Aug1_Rot_-2_sd2
(235/285) train vimp2_J_CSFn_Aug1_Rot_4_sd0
(236/285) train vimp2_J_CSFn_Aug1_Rot_-6_sd1
(237/285) train vimp2_J_CSFn_Aug2_Rot_4_sd1
(238/285) train vimp2_J_CSFn_Aug2_Rot_-4_sd2
(239/285) train vimp2_J_CSFn_Aug2_Rot_7_sd0
(240/285) train vimp2_J_CSFn_Aug3_Rot_-1_sd1
(241/285) train vimp2_J_CSFn_Aug3_Rot_-2_sd0
(242/285) train vimp2_J_CSFn_Aug3_Rot_-3_sd2
(243/285) train vimp2_J_CSFn_Aug4_Rot_4_sd2
(244/285) train vimp2_J_CSFn_Aug4_Rot_5_sd0
(245/285) train vimp2_J_CSFn_Aug4_Rot_7_sd1
(246/285) train vimp2_J_CSFn_Aug5_Rot_4_sd2
(247/285) train vimp2_J_CSFn_Aug5_Rot_-6_sd1
(248/285) train vimp2_J_CSFn_Aug5_Rot_-7_sd0
(249/285) train vimp2_K_CSFn_Aug0_Rot_-2_sd2
(250/285) train vimp2_K_CSFn_Aug0_Rot_-6_sd0
(251/285) train vimp2_K_CSFn_Aug0_Rot_-6_sd1
(252/285) train vimp2_K_CSFn_Aug1_Rot_1_sd1
(253/285) train vimp2_K_CSFn_Aug1_Rot_1_sd2
(254/285) train vimp2_K_CSFn_Aug1_Rot_-5_sd0
(255/285) train vimp2_K_CSFn_Aug2_Rot_0_sd0
(256/285) train vimp2_K_CSFn_Aug2_Rot_-2_sd1
(257/285) train vimp2_K_CSFn_Aug2_Rot_-5_sd2
(258/285) train vimp2_K_CSFn_Aug3_Rot_5_sd2
(259/285) train vimp2_K_CSFn_Aug3_Rot_6_sd0
(260/285) train vimp2_K_CSFn_Aug3_Rot_-6_sd1
(261/285) train vimp2_K_CSFn_Aug4_Rot_-1_sd0
(262/285) train vimp2_K_CSFn_Aug4_Rot_2_sd2
(263/285) train vimp2_K_CSFn_Aug4_Rot_-6_sd1
(264/285) train vimp2_K_CSFn_Aug5_Rot_-2_sd1
(265/285) train vimp2_K_CSFn_Aug5_Rot_-2_sd2
(266/285) train vimp2_K_CSFn_Aug5_Rot_-3_sd0
(267/285) train vimp2_L_CSFn_Aug0_Rot_4_sd0
(268/285) train vimp2_L_CSFn_Aug0_Rot_5_sd2
(269/285) train vimp2_L_CSFn_Aug0_Rot_-7_sd1
(270/285) train vimp2_L_CSFn_Aug1_Rot_3_sd0
(271/285) train vimp2_L_CSFn_Aug1_Rot_5_sd1
(272/285) train vimp2_L_CSFn_Aug1_Rot_-5_sd2
(273/285) train vimp2_L_CSFn_Aug2_Rot_-4_sd1
(274/285) train vimp2_L_CSFn_Aug2_Rot_5_sd0
(275/285) train vimp2_L_CSFn_Aug2_Rot_-7_sd2
(276/285) train vimp2_L_CSFn_Aug3_Rot_5_sd2
(277/285) train vimp2_L_CSFn_Aug3_Rot_-7_sd0
(278/285) train vimp2_L_CSFn_Aug3_Rot_7_sd1
(279/285) train vimp2_L_CSFn_Aug4_Rot_-1_sd0
(280/285) train vimp2_L_CSFn_Aug4_Rot_3_sd2
(281/285) train vimp2_L_CSFn_Aug4_Rot_-7_sd1
(282/285) train vimp2_L_CSFn_Aug5_Rot_-1_sd0
(283/285) train vimp2_L_CSFn_Aug5_Rot_-5_sd1
(284/285) train vimp2_L_CSFn_Aug5_Rot_6_sd2
(0/3) test vimp2_A_CSFn2
(1/3) test vimp2_ANON765_CSFn2
(2/3) test vimp2_B_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 80, 10)   100         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 80, 10)   40          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 80, 10)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 52, 80, 10)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 80, 10)   910         dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 80, 10)   40          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 80, 10)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 52, 80, 10)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 52, 80, 10)   910         dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 52, 80, 10)   40          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 52, 80, 10)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 52, 80, 10)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 52, 80, 20)   1820        dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 52, 80, 20)   80          conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 52, 80, 20)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 52, 80, 20)   3620        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 52, 80, 20)   80          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 80, 20)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_5[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 40, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 26, 40, 40)   7240        dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 26, 40, 40)   160         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 26, 40, 40)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 40, 40)   14440       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 40, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 40, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_7[0][0]               
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 13, 20, 40)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 13, 20, 80)   28880       dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 13, 20, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 13, 20, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 13, 20, 80)   57680       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 13, 20, 80)   320         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 13, 20, 80)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 13, 20, 80)   0           activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_1[0][0]         
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 40, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 26, 40, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 40, 40)   14440       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 40, 40)   160         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 40, 40)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 26, 40, 40)   0           activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_7[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 13)   273         dropout_8[0][0]                  
==================================================================================================
Total params: 187,773
Trainable params: 44,233
Non-trainable params: 143,540
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 20s - loss: 106.9210 - acc: 0.5830 - mDice: 0.0158 - val_loss: 26.5716 - val_acc: 0.9047 - val_mDice: 0.0148

Epoch 00001: val_mDice improved from -inf to 0.01477, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 10s - loss: 24.7403 - acc: 0.8380 - mDice: 0.0155 - val_loss: 11.0767 - val_acc: 0.9047 - val_mDice: 0.0142

Epoch 00002: val_mDice did not improve from 0.01477
Epoch 3/300
 - 10s - loss: 13.9725 - acc: 0.8665 - mDice: 0.0181 - val_loss: 7.7179 - val_acc: 0.9047 - val_mDice: 0.0217

Epoch 00003: val_mDice improved from 0.01477 to 0.02171, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 10s - loss: 10.3981 - acc: 0.8685 - mDice: 0.0265 - val_loss: 6.5991 - val_acc: 0.9047 - val_mDice: 0.0359

Epoch 00004: val_mDice improved from 0.02171 to 0.03587, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 9s - loss: 8.7459 - acc: 0.8686 - mDice: 0.0348 - val_loss: 5.8003 - val_acc: 0.9047 - val_mDice: 0.0377

Epoch 00005: val_mDice improved from 0.03587 to 0.03767, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 10s - loss: 7.8350 - acc: 0.8681 - mDice: 0.0415 - val_loss: 5.5510 - val_acc: 0.9047 - val_mDice: 0.0402

Epoch 00006: val_mDice improved from 0.03767 to 0.04018, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 9s - loss: 7.2125 - acc: 0.8676 - mDice: 0.0475 - val_loss: 5.4476 - val_acc: 0.9047 - val_mDice: 0.0379

Epoch 00007: val_mDice did not improve from 0.04018
Epoch 8/300
 - 10s - loss: 6.7195 - acc: 0.8676 - mDice: 0.0539 - val_loss: 5.3204 - val_acc: 0.9047 - val_mDice: 0.0372

Epoch 00008: val_mDice did not improve from 0.04018
Epoch 9/300
 - 9s - loss: 6.3076 - acc: 0.8681 - mDice: 0.0610 - val_loss: 5.1903 - val_acc: 0.9047 - val_mDice: 0.0388

Epoch 00009: val_mDice did not improve from 0.04018
Epoch 10/300
 - 9s - loss: 5.9219 - acc: 0.8685 - mDice: 0.0719 - val_loss: 5.0082 - val_acc: 0.9047 - val_mDice: 0.0454

Epoch 00010: val_mDice improved from 0.04018 to 0.04536, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 10s - loss: 5.5827 - acc: 0.8692 - mDice: 0.0856 - val_loss: 4.7072 - val_acc: 0.9047 - val_mDice: 0.0650

Epoch 00011: val_mDice improved from 0.04536 to 0.06495, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 9s - loss: 5.2980 - acc: 0.8700 - mDice: 0.0990 - val_loss: 4.6446 - val_acc: 0.9048 - val_mDice: 0.0787

Epoch 00012: val_mDice improved from 0.06495 to 0.07870, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 9s - loss: 5.0607 - acc: 0.8706 - mDice: 0.1118 - val_loss: 4.5564 - val_acc: 0.9049 - val_mDice: 0.0907

Epoch 00013: val_mDice improved from 0.07870 to 0.09073, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 10s - loss: 4.8467 - acc: 0.8710 - mDice: 0.1230 - val_loss: 4.2896 - val_acc: 0.9055 - val_mDice: 0.1117

Epoch 00014: val_mDice improved from 0.09073 to 0.11166, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 9s - loss: 4.6646 - acc: 0.8716 - mDice: 0.1342 - val_loss: 4.1125 - val_acc: 0.9062 - val_mDice: 0.1274

Epoch 00015: val_mDice improved from 0.11166 to 0.12743, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 9s - loss: 4.4716 - acc: 0.8720 - mDice: 0.1466 - val_loss: 4.1111 - val_acc: 0.9063 - val_mDice: 0.1353

Epoch 00016: val_mDice improved from 0.12743 to 0.13535, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 9s - loss: 4.2902 - acc: 0.8727 - mDice: 0.1601 - val_loss: 3.8865 - val_acc: 0.9067 - val_mDice: 0.1571

Epoch 00017: val_mDice improved from 0.13535 to 0.15713, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 10s - loss: 4.1490 - acc: 0.8739 - mDice: 0.1725 - val_loss: 3.8111 - val_acc: 0.9089 - val_mDice: 0.1732

Epoch 00018: val_mDice improved from 0.15713 to 0.17319, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 9s - loss: 3.9939 - acc: 0.8760 - mDice: 0.1869 - val_loss: 3.6516 - val_acc: 0.9128 - val_mDice: 0.1904

Epoch 00019: val_mDice improved from 0.17319 to 0.19036, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 9s - loss: 3.8552 - acc: 0.8788 - mDice: 0.2015 - val_loss: 3.6157 - val_acc: 0.9155 - val_mDice: 0.2017

Epoch 00020: val_mDice improved from 0.19036 to 0.20167, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 9s - loss: 3.7205 - acc: 0.8818 - mDice: 0.2156 - val_loss: 3.5281 - val_acc: 0.9192 - val_mDice: 0.2200

Epoch 00021: val_mDice improved from 0.20167 to 0.21996, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 10s - loss: 3.6161 - acc: 0.8844 - mDice: 0.2283 - val_loss: 3.4886 - val_acc: 0.9219 - val_mDice: 0.2330

Epoch 00022: val_mDice improved from 0.21996 to 0.23296, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 10s - loss: 3.5102 - acc: 0.8867 - mDice: 0.2409 - val_loss: 3.4805 - val_acc: 0.9232 - val_mDice: 0.2384

Epoch 00023: val_mDice improved from 0.23296 to 0.23836, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 9s - loss: 3.4153 - acc: 0.8885 - mDice: 0.2521 - val_loss: 3.4902 - val_acc: 0.9228 - val_mDice: 0.2452

Epoch 00024: val_mDice improved from 0.23836 to 0.24518, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 9s - loss: 3.3311 - acc: 0.8902 - mDice: 0.2622 - val_loss: 3.4255 - val_acc: 0.9241 - val_mDice: 0.2664

Epoch 00025: val_mDice improved from 0.24518 to 0.26643, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 9s - loss: 3.2427 - acc: 0.8916 - mDice: 0.2742 - val_loss: 3.2722 - val_acc: 0.9245 - val_mDice: 0.2795

Epoch 00026: val_mDice improved from 0.26643 to 0.27953, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 10s - loss: 3.1646 - acc: 0.8931 - mDice: 0.2844 - val_loss: 3.3375 - val_acc: 0.9259 - val_mDice: 0.2876

Epoch 00027: val_mDice improved from 0.27953 to 0.28764, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 10s - loss: 3.1050 - acc: 0.8941 - mDice: 0.2931 - val_loss: 3.2828 - val_acc: 0.9254 - val_mDice: 0.2833

Epoch 00028: val_mDice did not improve from 0.28764
Epoch 29/300
 - 9s - loss: 3.0339 - acc: 0.8957 - mDice: 0.3028 - val_loss: 3.3011 - val_acc: 0.9246 - val_mDice: 0.2926

Epoch 00029: val_mDice improved from 0.28764 to 0.29265, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 9s - loss: 2.9671 - acc: 0.8970 - mDice: 0.3120 - val_loss: 3.2081 - val_acc: 0.9249 - val_mDice: 0.3132

Epoch 00030: val_mDice improved from 0.29265 to 0.31324, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 9s - loss: 2.8970 - acc: 0.8987 - mDice: 0.3227 - val_loss: 3.3591 - val_acc: 0.9257 - val_mDice: 0.3050

Epoch 00031: val_mDice did not improve from 0.31324
Epoch 32/300
 - 9s - loss: 2.8535 - acc: 0.8997 - mDice: 0.3290 - val_loss: 3.2639 - val_acc: 0.9270 - val_mDice: 0.3176

Epoch 00032: val_mDice improved from 0.31324 to 0.31760, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 10s - loss: 2.7977 - acc: 0.9010 - mDice: 0.3379 - val_loss: 3.2458 - val_acc: 0.9276 - val_mDice: 0.3298

Epoch 00033: val_mDice improved from 0.31760 to 0.32984, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 10s - loss: 2.7471 - acc: 0.9023 - mDice: 0.3462 - val_loss: 3.2162 - val_acc: 0.9286 - val_mDice: 0.3325

Epoch 00034: val_mDice improved from 0.32984 to 0.33252, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 9s - loss: 2.7152 - acc: 0.9029 - mDice: 0.3525 - val_loss: 3.1572 - val_acc: 0.9273 - val_mDice: 0.3372

Epoch 00035: val_mDice improved from 0.33252 to 0.33716, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 9s - loss: 2.6736 - acc: 0.9036 - mDice: 0.3588 - val_loss: 3.1789 - val_acc: 0.9308 - val_mDice: 0.3498

Epoch 00036: val_mDice improved from 0.33716 to 0.34978, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 9s - loss: 2.6409 - acc: 0.9043 - mDice: 0.3648 - val_loss: 3.3190 - val_acc: 0.9267 - val_mDice: 0.3467

Epoch 00037: val_mDice did not improve from 0.34978
Epoch 38/300
 - 9s - loss: 2.6060 - acc: 0.9053 - mDice: 0.3708 - val_loss: 3.2438 - val_acc: 0.9277 - val_mDice: 0.3518

Epoch 00038: val_mDice improved from 0.34978 to 0.35179, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 10s - loss: 2.5752 - acc: 0.9062 - mDice: 0.3761 - val_loss: 3.4511 - val_acc: 0.9257 - val_mDice: 0.3410

Epoch 00039: val_mDice did not improve from 0.35179
Epoch 40/300
 - 9s - loss: 2.5507 - acc: 0.9070 - mDice: 0.3809 - val_loss: 3.1132 - val_acc: 0.9267 - val_mDice: 0.3543

Epoch 00040: val_mDice improved from 0.35179 to 0.35432, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 9s - loss: 2.5135 - acc: 0.9075 - mDice: 0.3865 - val_loss: 3.3133 - val_acc: 0.9295 - val_mDice: 0.3550

Epoch 00041: val_mDice improved from 0.35432 to 0.35497, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 9s - loss: 2.4845 - acc: 0.9084 - mDice: 0.3927 - val_loss: 3.1996 - val_acc: 0.9310 - val_mDice: 0.3701

Epoch 00042: val_mDice improved from 0.35497 to 0.37008, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 9s - loss: 2.4720 - acc: 0.9088 - mDice: 0.3955 - val_loss: 3.2117 - val_acc: 0.9304 - val_mDice: 0.3639

Epoch 00043: val_mDice did not improve from 0.37008
Epoch 44/300
 - 9s - loss: 2.4397 - acc: 0.9099 - mDice: 0.4007 - val_loss: 3.0834 - val_acc: 0.9311 - val_mDice: 0.3710

Epoch 00044: val_mDice improved from 0.37008 to 0.37099, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 10s - loss: 2.4192 - acc: 0.9105 - mDice: 0.4043 - val_loss: 3.3718 - val_acc: 0.9307 - val_mDice: 0.3587

Epoch 00045: val_mDice did not improve from 0.37099
Epoch 46/300
 - 9s - loss: 2.4126 - acc: 0.9107 - mDice: 0.4065 - val_loss: 3.2345 - val_acc: 0.9291 - val_mDice: 0.3610

Epoch 00046: val_mDice did not improve from 0.37099
Epoch 47/300
 - 9s - loss: 2.3774 - acc: 0.9118 - mDice: 0.4136 - val_loss: 3.1176 - val_acc: 0.9309 - val_mDice: 0.3734

Epoch 00047: val_mDice improved from 0.37099 to 0.37340, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 9s - loss: 2.3647 - acc: 0.9118 - mDice: 0.4144 - val_loss: 3.2871 - val_acc: 0.9309 - val_mDice: 0.3663

Epoch 00048: val_mDice did not improve from 0.37340
Epoch 49/300
 - 9s - loss: 2.3378 - acc: 0.9129 - mDice: 0.4208 - val_loss: 3.1182 - val_acc: 0.9302 - val_mDice: 0.3734

Epoch 00049: val_mDice improved from 0.37340 to 0.37344, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 9s - loss: 2.3185 - acc: 0.9135 - mDice: 0.4246 - val_loss: 3.0271 - val_acc: 0.9314 - val_mDice: 0.3748

Epoch 00050: val_mDice improved from 0.37344 to 0.37484, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 9s - loss: 2.3062 - acc: 0.9140 - mDice: 0.4264 - val_loss: 3.2090 - val_acc: 0.9332 - val_mDice: 0.3789

Epoch 00051: val_mDice improved from 0.37484 to 0.37890, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 10s - loss: 2.2952 - acc: 0.9139 - mDice: 0.4289 - val_loss: 3.1584 - val_acc: 0.9313 - val_mDice: 0.3772

Epoch 00052: val_mDice did not improve from 0.37890
Epoch 53/300
 - 10s - loss: 2.2795 - acc: 0.9146 - mDice: 0.4311 - val_loss: 3.1848 - val_acc: 0.9336 - val_mDice: 0.3832

Epoch 00053: val_mDice improved from 0.37890 to 0.38316, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 10s - loss: 2.2622 - acc: 0.9150 - mDice: 0.4351 - val_loss: 3.2836 - val_acc: 0.9317 - val_mDice: 0.3688

Epoch 00054: val_mDice did not improve from 0.38316
Epoch 55/300
 - 10s - loss: 2.2576 - acc: 0.9152 - mDice: 0.4363 - val_loss: 3.2003 - val_acc: 0.9322 - val_mDice: 0.3818

Epoch 00055: val_mDice did not improve from 0.38316
Epoch 56/300
 - 9s - loss: 2.2382 - acc: 0.9157 - mDice: 0.4398 - val_loss: 3.0781 - val_acc: 0.9330 - val_mDice: 0.3794

Epoch 00056: val_mDice did not improve from 0.38316
Epoch 57/300
 - 10s - loss: 2.2245 - acc: 0.9161 - mDice: 0.4423 - val_loss: 3.0821 - val_acc: 0.9336 - val_mDice: 0.3876

Epoch 00057: val_mDice improved from 0.38316 to 0.38765, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 58/300
 - 9s - loss: 2.2096 - acc: 0.9164 - mDice: 0.4452 - val_loss: 3.3472 - val_acc: 0.9315 - val_mDice: 0.3782

Epoch 00058: val_mDice did not improve from 0.38765
Epoch 59/300
 - 9s - loss: 2.1920 - acc: 0.9168 - mDice: 0.4494 - val_loss: 3.2928 - val_acc: 0.9315 - val_mDice: 0.3821

Epoch 00059: val_mDice did not improve from 0.38765
Epoch 60/300
 - 10s - loss: 2.1824 - acc: 0.9170 - mDice: 0.4509 - val_loss: 3.2129 - val_acc: 0.9339 - val_mDice: 0.3877

Epoch 00060: val_mDice improved from 0.38765 to 0.38774, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 61/300
 - 9s - loss: 2.1741 - acc: 0.9174 - mDice: 0.4540 - val_loss: 3.2212 - val_acc: 0.9335 - val_mDice: 0.3894

Epoch 00061: val_mDice improved from 0.38774 to 0.38936, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 62/300
 - 9s - loss: 2.1681 - acc: 0.9173 - mDice: 0.4545 - val_loss: 3.2830 - val_acc: 0.9343 - val_mDice: 0.3841

Epoch 00062: val_mDice did not improve from 0.38936
Epoch 63/300
 - 10s - loss: 2.1497 - acc: 0.9177 - mDice: 0.4577 - val_loss: 3.2487 - val_acc: 0.9328 - val_mDice: 0.3898

Epoch 00063: val_mDice improved from 0.38936 to 0.38984, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 64/300
 - 10s - loss: 2.1422 - acc: 0.9181 - mDice: 0.4598 - val_loss: 3.1046 - val_acc: 0.9310 - val_mDice: 0.3868

Epoch 00064: val_mDice did not improve from 0.38984
Epoch 65/300
 - 10s - loss: 2.1296 - acc: 0.9184 - mDice: 0.4616 - val_loss: 3.1651 - val_acc: 0.9347 - val_mDice: 0.3958

Epoch 00065: val_mDice improved from 0.38984 to 0.39581, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 66/300
 - 10s - loss: 2.1187 - acc: 0.9182 - mDice: 0.4639 - val_loss: 3.2515 - val_acc: 0.9328 - val_mDice: 0.3873

Epoch 00066: val_mDice did not improve from 0.39581
Epoch 67/300
 - 10s - loss: 2.1103 - acc: 0.9187 - mDice: 0.4663 - val_loss: 3.3759 - val_acc: 0.9337 - val_mDice: 0.3867

Epoch 00067: val_mDice did not improve from 0.39581
Epoch 68/300
 - 10s - loss: 2.1035 - acc: 0.9190 - mDice: 0.4678 - val_loss: 3.2515 - val_acc: 0.9327 - val_mDice: 0.3907

Epoch 00068: val_mDice did not improve from 0.39581
Epoch 69/300
 - 10s - loss: 2.0929 - acc: 0.9193 - mDice: 0.4701 - val_loss: 3.1453 - val_acc: 0.9318 - val_mDice: 0.3980

Epoch 00069: val_mDice improved from 0.39581 to 0.39796, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 70/300
 - 10s - loss: 2.0862 - acc: 0.9194 - mDice: 0.4718 - val_loss: 3.3265 - val_acc: 0.9328 - val_mDice: 0.3899

Epoch 00070: val_mDice did not improve from 0.39796
Epoch 71/300
 - 9s - loss: 2.0723 - acc: 0.9197 - mDice: 0.4744 - val_loss: 3.0673 - val_acc: 0.9331 - val_mDice: 0.3969

Epoch 00071: val_mDice did not improve from 0.39796
Epoch 72/300
 - 10s - loss: 2.0679 - acc: 0.9200 - mDice: 0.4753 - val_loss: 3.2563 - val_acc: 0.9328 - val_mDice: 0.3940

Epoch 00072: val_mDice did not improve from 0.39796
Epoch 73/300
 - 9s - loss: 2.0494 - acc: 0.9205 - mDice: 0.4788 - val_loss: 3.3303 - val_acc: 0.9332 - val_mDice: 0.3894

Epoch 00073: val_mDice did not improve from 0.39796
Epoch 74/300
 - 9s - loss: 2.0507 - acc: 0.9202 - mDice: 0.4784 - val_loss: 3.1460 - val_acc: 0.9365 - val_mDice: 0.4013

Epoch 00074: val_mDice improved from 0.39796 to 0.40126, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 75/300
 - 10s - loss: 2.0401 - acc: 0.9208 - mDice: 0.4813 - val_loss: 3.4203 - val_acc: 0.9350 - val_mDice: 0.3897

Epoch 00075: val_mDice did not improve from 0.40126
Epoch 76/300
 - 9s - loss: 2.0240 - acc: 0.9210 - mDice: 0.4841 - val_loss: 3.3643 - val_acc: 0.9352 - val_mDice: 0.3922

Epoch 00076: val_mDice did not improve from 0.40126
Epoch 77/300
 - 9s - loss: 2.0214 - acc: 0.9211 - mDice: 0.4847 - val_loss: 3.2785 - val_acc: 0.9365 - val_mDice: 0.4007

Epoch 00077: val_mDice did not improve from 0.40126
Epoch 78/300
 - 10s - loss: 2.0143 - acc: 0.9211 - mDice: 0.4862 - val_loss: 3.2551 - val_acc: 0.9347 - val_mDice: 0.4004

Epoch 00078: val_mDice did not improve from 0.40126
Epoch 79/300
 - 9s - loss: 2.0067 - acc: 0.9214 - mDice: 0.4882 - val_loss: 3.2355 - val_acc: 0.9358 - val_mDice: 0.4007

Epoch 00079: val_mDice did not improve from 0.40126
Epoch 80/300
 - 9s - loss: 1.9911 - acc: 0.9217 - mDice: 0.4912 - val_loss: 3.4992 - val_acc: 0.9330 - val_mDice: 0.3883

Epoch 00080: val_mDice did not improve from 0.40126
Epoch 81/300
 - 9s - loss: 1.9943 - acc: 0.9218 - mDice: 0.4907 - val_loss: 3.6292 - val_acc: 0.9311 - val_mDice: 0.3957

Epoch 00081: val_mDice did not improve from 0.40126
Epoch 82/300
 - 10s - loss: 1.9895 - acc: 0.9215 - mDice: 0.4926 - val_loss: 3.2745 - val_acc: 0.9366 - val_mDice: 0.4106

Epoch 00082: val_mDice improved from 0.40126 to 0.41055, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 83/300
 - 9s - loss: 1.9797 - acc: 0.9218 - mDice: 0.4939 - val_loss: 3.5923 - val_acc: 0.9359 - val_mDice: 0.3903

Epoch 00083: val_mDice did not improve from 0.41055
Epoch 84/300
 - 9s - loss: 1.9672 - acc: 0.9220 - mDice: 0.4964 - val_loss: 3.2710 - val_acc: 0.9374 - val_mDice: 0.4086

Epoch 00084: val_mDice did not improve from 0.41055
Epoch 85/300
 - 9s - loss: 1.9570 - acc: 0.9222 - mDice: 0.4984 - val_loss: 3.3969 - val_acc: 0.9357 - val_mDice: 0.4034

Epoch 00085: val_mDice did not improve from 0.41055
Epoch 86/300
 - 10s - loss: 1.9624 - acc: 0.9221 - mDice: 0.4989 - val_loss: 3.3786 - val_acc: 0.9346 - val_mDice: 0.4076

Epoch 00086: val_mDice did not improve from 0.41055
Epoch 87/300
 - 10s - loss: 1.9548 - acc: 0.9222 - mDice: 0.4990 - val_loss: 3.3118 - val_acc: 0.9353 - val_mDice: 0.4110

Epoch 00087: val_mDice improved from 0.41055 to 0.41098, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 88/300
 - 10s - loss: 1.9479 - acc: 0.9223 - mDice: 0.5010 - val_loss: 3.2595 - val_acc: 0.9367 - val_mDice: 0.4094

Epoch 00088: val_mDice did not improve from 0.41098
Epoch 89/300
 - 10s - loss: 1.9349 - acc: 0.9225 - mDice: 0.5031 - val_loss: 3.3042 - val_acc: 0.9346 - val_mDice: 0.4010

Epoch 00089: val_mDice did not improve from 0.41098
Epoch 90/300
 - 10s - loss: 1.9346 - acc: 0.9226 - mDice: 0.5035 - val_loss: 3.2304 - val_acc: 0.9362 - val_mDice: 0.4162

Epoch 00090: val_mDice improved from 0.41098 to 0.41622, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 91/300
 - 10s - loss: 1.9261 - acc: 0.9228 - mDice: 0.5058 - val_loss: 3.1222 - val_acc: 0.9370 - val_mDice: 0.4134

Epoch 00091: val_mDice did not improve from 0.41622
Epoch 92/300
 - 10s - loss: 1.9195 - acc: 0.9231 - mDice: 0.5069 - val_loss: 3.4956 - val_acc: 0.9334 - val_mDice: 0.4014

Epoch 00092: val_mDice did not improve from 0.41622
Epoch 93/300
 - 9s - loss: 1.9289 - acc: 0.9228 - mDice: 0.5051 - val_loss: 3.2057 - val_acc: 0.9374 - val_mDice: 0.4181

Epoch 00093: val_mDice improved from 0.41622 to 0.41814, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 94/300
 - 10s - loss: 1.9186 - acc: 0.9230 - mDice: 0.5067 - val_loss: 3.1823 - val_acc: 0.9326 - val_mDice: 0.4173

Epoch 00094: val_mDice did not improve from 0.41814
Epoch 95/300
 - 9s - loss: 1.8996 - acc: 0.9236 - mDice: 0.5114 - val_loss: 3.5384 - val_acc: 0.9314 - val_mDice: 0.3969

Epoch 00095: val_mDice did not improve from 0.41814
Epoch 96/300
 - 9s - loss: 1.9006 - acc: 0.9233 - mDice: 0.5111 - val_loss: 3.3129 - val_acc: 0.9343 - val_mDice: 0.4122

Epoch 00096: val_mDice did not improve from 0.41814
Epoch 97/300
 - 10s - loss: 1.9018 - acc: 0.9234 - mDice: 0.5104 - val_loss: 3.5595 - val_acc: 0.9342 - val_mDice: 0.3979

Epoch 00097: val_mDice did not improve from 0.41814
Epoch 98/300
 - 10s - loss: 1.8936 - acc: 0.9236 - mDice: 0.5133 - val_loss: 3.2565 - val_acc: 0.9358 - val_mDice: 0.4098

Epoch 00098: val_mDice did not improve from 0.41814
Epoch 99/300
 - 9s - loss: 1.8897 - acc: 0.9236 - mDice: 0.5125 - val_loss: 3.1306 - val_acc: 0.9366 - val_mDice: 0.4170

Epoch 00099: val_mDice did not improve from 0.41814
Epoch 100/300
 - 9s - loss: 1.8959 - acc: 0.9234 - mDice: 0.5119 - val_loss: 3.3231 - val_acc: 0.9367 - val_mDice: 0.4138

Epoch 00100: val_mDice did not improve from 0.41814
Epoch 101/300
 - 9s - loss: 1.8885 - acc: 0.9237 - mDice: 0.5149 - val_loss: 3.1986 - val_acc: 0.9377 - val_mDice: 0.4227

Epoch 00101: val_mDice improved from 0.41814 to 0.42267, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 102/300
 - 10s - loss: 1.8766 - acc: 0.9239 - mDice: 0.5167 - val_loss: 3.5359 - val_acc: 0.9367 - val_mDice: 0.3976

Epoch 00102: val_mDice did not improve from 0.42267
Epoch 103/300
 - 9s - loss: 1.8849 - acc: 0.9235 - mDice: 0.5149 - val_loss: 3.2491 - val_acc: 0.9379 - val_mDice: 0.4190

Epoch 00103: val_mDice did not improve from 0.42267
Epoch 104/300
 - 9s - loss: 1.8679 - acc: 0.9238 - mDice: 0.5184 - val_loss: 3.2988 - val_acc: 0.9356 - val_mDice: 0.4090

Epoch 00104: val_mDice did not improve from 0.42267
Epoch 105/300
 - 10s - loss: 1.8585 - acc: 0.9242 - mDice: 0.5209 - val_loss: 3.1252 - val_acc: 0.9361 - val_mDice: 0.4211

Epoch 00105: val_mDice did not improve from 0.42267
Epoch 106/300
 - 9s - loss: 1.8587 - acc: 0.9242 - mDice: 0.5201 - val_loss: 3.0556 - val_acc: 0.9369 - val_mDice: 0.4202

Epoch 00106: val_mDice did not improve from 0.42267
Epoch 107/300
 - 9s - loss: 1.8476 - acc: 0.9244 - mDice: 0.5234 - val_loss: 3.6378 - val_acc: 0.9308 - val_mDice: 0.4037

Epoch 00107: val_mDice did not improve from 0.42267
Epoch 108/300
 - 9s - loss: 1.8448 - acc: 0.9246 - mDice: 0.5243 - val_loss: 3.2159 - val_acc: 0.9373 - val_mDice: 0.4190

Epoch 00108: val_mDice did not improve from 0.42267
Epoch 109/300
 - 9s - loss: 1.8482 - acc: 0.9244 - mDice: 0.5235 - val_loss: 3.3780 - val_acc: 0.9327 - val_mDice: 0.4109

Epoch 00109: val_mDice did not improve from 0.42267
Epoch 110/300
 - 10s - loss: 1.8378 - acc: 0.9245 - mDice: 0.5252 - val_loss: 3.4025 - val_acc: 0.9373 - val_mDice: 0.4208

Epoch 00110: val_mDice did not improve from 0.42267
Epoch 111/300
 - 9s - loss: 1.8400 - acc: 0.9245 - mDice: 0.5248 - val_loss: 3.4058 - val_acc: 0.9325 - val_mDice: 0.4044

Epoch 00111: val_mDice did not improve from 0.42267
Epoch 112/300
 - 10s - loss: 1.8331 - acc: 0.9245 - mDice: 0.5267 - val_loss: 3.4117 - val_acc: 0.9321 - val_mDice: 0.4107

Epoch 00112: val_mDice did not improve from 0.42267
Epoch 113/300
 - 9s - loss: 1.8257 - acc: 0.9249 - mDice: 0.5280 - val_loss: 3.3090 - val_acc: 0.9371 - val_mDice: 0.4193

Epoch 00113: val_mDice did not improve from 0.42267
Epoch 114/300
 - 10s - loss: 1.8269 - acc: 0.9250 - mDice: 0.5286 - val_loss: 3.4745 - val_acc: 0.9342 - val_mDice: 0.4058

Epoch 00114: val_mDice did not improve from 0.42267
Epoch 115/300
 - 9s - loss: 1.8108 - acc: 0.9251 - mDice: 0.5317 - val_loss: 3.3305 - val_acc: 0.9361 - val_mDice: 0.4158

Epoch 00115: val_mDice did not improve from 0.42267
Epoch 116/300
 - 9s - loss: 1.8142 - acc: 0.9251 - mDice: 0.5311 - val_loss: 3.2095 - val_acc: 0.9335 - val_mDice: 0.4169

Epoch 00116: val_mDice did not improve from 0.42267
Epoch 117/300
 - 10s - loss: 1.8182 - acc: 0.9251 - mDice: 0.5300 - val_loss: 3.0775 - val_acc: 0.9366 - val_mDice: 0.4252

Epoch 00117: val_mDice improved from 0.42267 to 0.42523, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 118/300
 - 9s - loss: 1.8092 - acc: 0.9255 - mDice: 0.5317 - val_loss: 3.0999 - val_acc: 0.9365 - val_mDice: 0.4265

Epoch 00118: val_mDice improved from 0.42523 to 0.42649, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 119/300
 - 9s - loss: 1.8038 - acc: 0.9255 - mDice: 0.5335 - val_loss: 3.2786 - val_acc: 0.9382 - val_mDice: 0.4249

Epoch 00119: val_mDice did not improve from 0.42649
Epoch 120/300
 - 9s - loss: 1.8079 - acc: 0.9254 - mDice: 0.5333 - val_loss: 3.2438 - val_acc: 0.9384 - val_mDice: 0.4223

Epoch 00120: val_mDice did not improve from 0.42649
Epoch 121/300
 - 10s - loss: 1.7925 - acc: 0.9256 - mDice: 0.5361 - val_loss: 3.1399 - val_acc: 0.9368 - val_mDice: 0.4230

Epoch 00121: val_mDice did not improve from 0.42649
Epoch 122/300
 - 10s - loss: 1.7931 - acc: 0.9255 - mDice: 0.5362 - val_loss: 3.4993 - val_acc: 0.9321 - val_mDice: 0.4109

Epoch 00122: val_mDice did not improve from 0.42649
Epoch 123/300
 - 9s - loss: 1.7850 - acc: 0.9259 - mDice: 0.5379 - val_loss: 3.3972 - val_acc: 0.9340 - val_mDice: 0.4119

Epoch 00123: val_mDice did not improve from 0.42649
Epoch 124/300
 - 9s - loss: 1.7805 - acc: 0.9260 - mDice: 0.5386 - val_loss: 3.3217 - val_acc: 0.9335 - val_mDice: 0.4148

Epoch 00124: val_mDice did not improve from 0.42649
Epoch 125/300
 - 9s - loss: 1.7802 - acc: 0.9259 - mDice: 0.5390 - val_loss: 3.5987 - val_acc: 0.9342 - val_mDice: 0.4100

Epoch 00125: val_mDice did not improve from 0.42649
Epoch 126/300
 - 9s - loss: 1.7749 - acc: 0.9262 - mDice: 0.5398 - val_loss: 3.2432 - val_acc: 0.9386 - val_mDice: 0.4334

Epoch 00126: val_mDice improved from 0.42649 to 0.43342, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 127/300
 - 10s - loss: 1.7734 - acc: 0.9264 - mDice: 0.5407 - val_loss: 3.3390 - val_acc: 0.9353 - val_mDice: 0.4194

Epoch 00127: val_mDice did not improve from 0.43342
Epoch 128/300
 - 10s - loss: 1.7657 - acc: 0.9264 - mDice: 0.5431 - val_loss: 3.5393 - val_acc: 0.9339 - val_mDice: 0.4118

Epoch 00128: val_mDice did not improve from 0.43342
Epoch 129/300
 - 9s - loss: 1.7673 - acc: 0.9265 - mDice: 0.5417 - val_loss: 3.3519 - val_acc: 0.9359 - val_mDice: 0.4068

Epoch 00129: val_mDice did not improve from 0.43342
Epoch 130/300
 - 9s - loss: 1.7624 - acc: 0.9266 - mDice: 0.5434 - val_loss: 3.3378 - val_acc: 0.9365 - val_mDice: 0.4240

Epoch 00130: val_mDice did not improve from 0.43342
Epoch 131/300
 - 9s - loss: 1.7575 - acc: 0.9267 - mDice: 0.5451 - val_loss: 3.2381 - val_acc: 0.9378 - val_mDice: 0.4213

Epoch 00131: val_mDice did not improve from 0.43342
Epoch 132/300
 - 9s - loss: 1.7585 - acc: 0.9267 - mDice: 0.5431 - val_loss: 3.2489 - val_acc: 0.9338 - val_mDice: 0.4200

Epoch 00132: val_mDice did not improve from 0.43342
Epoch 133/300
 - 10s - loss: 1.7629 - acc: 0.9266 - mDice: 0.5435 - val_loss: 3.3738 - val_acc: 0.9342 - val_mDice: 0.4191

Epoch 00133: val_mDice did not improve from 0.43342
Epoch 134/300
 - 9s - loss: 1.7528 - acc: 0.9270 - mDice: 0.5454 - val_loss: 3.3073 - val_acc: 0.9388 - val_mDice: 0.4281

Epoch 00134: val_mDice did not improve from 0.43342
Epoch 135/300
 - 10s - loss: 1.7483 - acc: 0.9270 - mDice: 0.5461 - val_loss: 3.3430 - val_acc: 0.9374 - val_mDice: 0.4188

Epoch 00135: val_mDice did not improve from 0.43342
Epoch 136/300
 - 10s - loss: 1.7439 - acc: 0.9272 - mDice: 0.5478 - val_loss: 3.4762 - val_acc: 0.9361 - val_mDice: 0.4160

Epoch 00136: val_mDice did not improve from 0.43342
Epoch 137/300
 - 10s - loss: 1.7375 - acc: 0.9273 - mDice: 0.5486 - val_loss: 3.1244 - val_acc: 0.9384 - val_mDice: 0.4348

Epoch 00137: val_mDice improved from 0.43342 to 0.43481, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 138/300
 - 9s - loss: 1.7371 - acc: 0.9273 - mDice: 0.5495 - val_loss: 3.0934 - val_acc: 0.9369 - val_mDice: 0.4364

Epoch 00138: val_mDice improved from 0.43481 to 0.43644, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 139/300
 - 10s - loss: 1.7433 - acc: 0.9272 - mDice: 0.5482 - val_loss: 3.1292 - val_acc: 0.9365 - val_mDice: 0.4383

Epoch 00139: val_mDice improved from 0.43644 to 0.43826, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 140/300
 - 9s - loss: 1.7384 - acc: 0.9275 - mDice: 0.5493 - val_loss: 3.3698 - val_acc: 0.9349 - val_mDice: 0.4139

Epoch 00140: val_mDice did not improve from 0.43826
Epoch 141/300
 - 9s - loss: 1.7290 - acc: 0.9276 - mDice: 0.5511 - val_loss: 3.2458 - val_acc: 0.9365 - val_mDice: 0.4191

Epoch 00141: val_mDice did not improve from 0.43826
Epoch 142/300
 - 9s - loss: 1.7330 - acc: 0.9275 - mDice: 0.5504 - val_loss: 3.3275 - val_acc: 0.9361 - val_mDice: 0.4240

Epoch 00142: val_mDice did not improve from 0.43826
Epoch 143/300
 - 10s - loss: 1.7227 - acc: 0.9279 - mDice: 0.5526 - val_loss: 3.3895 - val_acc: 0.9373 - val_mDice: 0.4171

Epoch 00143: val_mDice did not improve from 0.43826
Epoch 144/300
 - 9s - loss: 1.7319 - acc: 0.9277 - mDice: 0.5512 - val_loss: 3.2114 - val_acc: 0.9388 - val_mDice: 0.4268

Epoch 00144: val_mDice did not improve from 0.43826
Epoch 145/300
 - 9s - loss: 1.7159 - acc: 0.9281 - mDice: 0.5543 - val_loss: 3.3451 - val_acc: 0.9372 - val_mDice: 0.4233

Epoch 00145: val_mDice did not improve from 0.43826
Epoch 146/300
 - 9s - loss: 1.7197 - acc: 0.9280 - mDice: 0.5529 - val_loss: 3.3779 - val_acc: 0.9357 - val_mDice: 0.4233

Epoch 00146: val_mDice did not improve from 0.43826
Epoch 147/300
 - 10s - loss: 1.7131 - acc: 0.9281 - mDice: 0.5549 - val_loss: 3.4586 - val_acc: 0.9329 - val_mDice: 0.4078

Epoch 00147: val_mDice did not improve from 0.43826
Epoch 148/300
 - 9s - loss: 1.7137 - acc: 0.9283 - mDice: 0.5543 - val_loss: 3.5032 - val_acc: 0.9332 - val_mDice: 0.4130

Epoch 00148: val_mDice did not improve from 0.43826
Epoch 149/300
 - 9s - loss: 1.7195 - acc: 0.9285 - mDice: 0.5542 - val_loss: 3.4771 - val_acc: 0.9311 - val_mDice: 0.4070

Epoch 00149: val_mDice did not improve from 0.43826
Epoch 150/300
 - 9s - loss: 1.7076 - acc: 0.9286 - mDice: 0.5559 - val_loss: 3.2775 - val_acc: 0.9357 - val_mDice: 0.4245

Epoch 00150: val_mDice did not improve from 0.43826
Epoch 151/300
 - 9s - loss: 1.7157 - acc: 0.9283 - mDice: 0.5548 - val_loss: 3.3944 - val_acc: 0.9352 - val_mDice: 0.4166

Epoch 00151: val_mDice did not improve from 0.43826
Epoch 152/300
 - 9s - loss: 1.7048 - acc: 0.9286 - mDice: 0.5568 - val_loss: 3.2961 - val_acc: 0.9371 - val_mDice: 0.4257

Epoch 00152: val_mDice did not improve from 0.43826
Epoch 153/300
 - 10s - loss: 1.7010 - acc: 0.9286 - mDice: 0.5575 - val_loss: 3.2285 - val_acc: 0.9391 - val_mDice: 0.4241

Epoch 00153: val_mDice did not improve from 0.43826
Epoch 154/300
 - 9s - loss: 1.7000 - acc: 0.9286 - mDice: 0.5578 - val_loss: 3.2742 - val_acc: 0.9375 - val_mDice: 0.4338

Epoch 00154: val_mDice did not improve from 0.43826
Epoch 155/300
 - 9s - loss: 1.7020 - acc: 0.9288 - mDice: 0.5578 - val_loss: 3.1683 - val_acc: 0.9378 - val_mDice: 0.4299

Epoch 00155: val_mDice did not improve from 0.43826
Epoch 156/300
 - 10s - loss: 1.6975 - acc: 0.9290 - mDice: 0.5591 - val_loss: 3.3371 - val_acc: 0.9389 - val_mDice: 0.4204

Epoch 00156: val_mDice did not improve from 0.43826
Epoch 157/300
 - 10s - loss: 1.7085 - acc: 0.9286 - mDice: 0.5567 - val_loss: 3.5067 - val_acc: 0.9366 - val_mDice: 0.4168

Epoch 00157: val_mDice did not improve from 0.43826
Epoch 158/300
 - 10s - loss: 1.6951 - acc: 0.9290 - mDice: 0.5593 - val_loss: 3.3268 - val_acc: 0.9383 - val_mDice: 0.4257

Epoch 00158: val_mDice did not improve from 0.43826
Epoch 159/300
 - 10s - loss: 1.6899 - acc: 0.9291 - mDice: 0.5607 - val_loss: 3.4111 - val_acc: 0.9375 - val_mDice: 0.4231

Epoch 00159: val_mDice did not improve from 0.43826
Epoch 160/300
 - 10s - loss: 1.6816 - acc: 0.9293 - mDice: 0.5624 - val_loss: 3.4266 - val_acc: 0.9388 - val_mDice: 0.4224

Epoch 00160: val_mDice did not improve from 0.43826
Epoch 161/300
 - 10s - loss: 1.6924 - acc: 0.9290 - mDice: 0.5603 - val_loss: 3.3777 - val_acc: 0.9355 - val_mDice: 0.4215

Epoch 00161: val_mDice did not improve from 0.43826
Epoch 162/300
 - 9s - loss: 1.6884 - acc: 0.9293 - mDice: 0.5612 - val_loss: 3.4820 - val_acc: 0.9374 - val_mDice: 0.4209

Epoch 00162: val_mDice did not improve from 0.43826
Epoch 163/300
 - 10s - loss: 1.6886 - acc: 0.9294 - mDice: 0.5611 - val_loss: 3.3217 - val_acc: 0.9371 - val_mDice: 0.4208

Epoch 00163: val_mDice did not improve from 0.43826
Epoch 164/300
 - 10s - loss: 1.6810 - acc: 0.9295 - mDice: 0.5629 - val_loss: 3.1859 - val_acc: 0.9376 - val_mDice: 0.4347

Epoch 00164: val_mDice did not improve from 0.43826
Epoch 165/300
 - 9s - loss: 1.6799 - acc: 0.9295 - mDice: 0.5627 - val_loss: 3.3897 - val_acc: 0.9358 - val_mDice: 0.4148

Epoch 00165: val_mDice did not improve from 0.43826
Epoch 166/300
 - 10s - loss: 1.6837 - acc: 0.9295 - mDice: 0.5617 - val_loss: 3.2964 - val_acc: 0.9392 - val_mDice: 0.4264

Epoch 00166: val_mDice did not improve from 0.43826
Epoch 167/300
 - 10s - loss: 1.6784 - acc: 0.9297 - mDice: 0.5635 - val_loss: 3.1250 - val_acc: 0.9372 - val_mDice: 0.4371

Epoch 00167: val_mDice did not improve from 0.43826
Epoch 168/300
 - 9s - loss: 1.6818 - acc: 0.9296 - mDice: 0.5627 - val_loss: 3.2544 - val_acc: 0.9385 - val_mDice: 0.4213

Epoch 00168: val_mDice did not improve from 0.43826
Epoch 169/300
 - 10s - loss: 1.6816 - acc: 0.9295 - mDice: 0.5630 - val_loss: 3.2716 - val_acc: 0.9377 - val_mDice: 0.4233

Epoch 00169: val_mDice did not improve from 0.43826
Restoring model weights from the end of the best epoch
Epoch 00169: early stopping
{'val_loss': [26.571553292728606, 11.076651070799146, 7.717934798626673, 6.599065300964174, 5.800258736525263, 5.551003713692937, 5.447581870037885, 5.320408963553962, 5.19025670098407, 5.008208991693599, 4.707159731537104, 4.644628436171582, 4.5564383406724245, 4.289622170806286, 4.112520448907855, 4.111127232822279, 3.8864954861679246, 3.811072594619223, 3.6515670999263725, 3.615654467693752, 3.528058656180898, 3.4885670552030206, 3.4805136733644066, 3.490205989574038, 3.4254641212256893, 3.272169031779326, 3.3375044741357365, 3.2827540820553187, 3.3010764351292026, 3.208118936046958, 3.3591039770593247, 3.2639350991784815, 3.245836892653079, 3.2162178672761437, 3.157226750316719, 3.1789368176832795, 3.3190422333954346, 3.2438147606860315, 3.451059673025849, 3.1131988413898006, 3.313316185914335, 3.199595339684969, 3.211748557010045, 3.0834450713757957, 3.3718223830773715, 3.234519262886828, 3.117608823370011, 3.2871108639187048, 3.118179283370929, 3.0270840614324523, 3.2090473169726983, 3.158351065546629, 3.184798931348182, 3.283612592944077, 3.2002603515893933, 3.0781202366619946, 3.0820629001994218, 3.3472043372069797, 3.292753413607854, 3.212900904450743, 3.2211917123668607, 3.283005913143002, 3.2486517854212296, 3.104643097191694, 3.165057081307861, 3.2514708968810737, 3.3758675100592277, 3.2515271163456854, 3.1452501326178512, 3.3265137384157804, 3.06730527559384, 3.256274727527939, 3.3302588586784188, 3.145983357448131, 3.420328036238927, 3.364320708616149, 3.2784802352876534, 3.2550981476060334, 3.235455860915993, 3.4991683885011646, 3.62916966949013, 3.2745103136680664, 3.592327628306867, 3.2709727389959706, 3.396859205128359, 3.3786211291860258, 3.3117938599150096, 3.259501985784265, 3.3041784612107135, 3.23044890027848, 3.1222420973437175, 3.4955576264875985, 3.205670980448347, 3.182312625083363, 3.538395818189851, 3.3128871565817724, 3.5595087577322766, 3.2564571278968026, 3.130566877734271, 3.3231200156733394, 3.1986010922562507, 3.535858570664589, 3.2490907935203897, 3.298817189144237, 3.125173428029354, 3.0555667088677487, 3.6377812701144387, 3.21594866358542, 3.3779574663910483, 3.4024980878900912, 3.4058449663487926, 3.411733307764821, 3.3089551405227255, 3.4744578804155544, 3.330499240418985, 3.2094668435110223, 3.077520574420868, 3.0998788015872596, 3.2786157718371776, 3.243789972138724, 3.139930664739084, 3.4992752450385263, 3.3972301741707183, 3.321651559367421, 3.5987384157432687, 3.243197698279151, 3.338962296755718, 3.539276192186489, 3.35187875572592, 3.337763470531042, 3.2381135459900614, 3.248871780510637, 3.3738494741597345, 3.3073090318856493, 3.3430409174456837, 3.4762462879575433, 3.1243791348034784, 3.093417916868237, 3.1292462688205496, 3.36982628986949, 3.2457999114523686, 3.3274998061270233, 3.3895043786836876, 3.2114038378488097, 3.3451387969599593, 3.377901423478588, 3.458579628528761, 3.5032269336017117, 3.477125306281128, 3.277457574160681, 3.3943508796926056, 3.2961119327339388, 3.228470884307864, 3.2742247458414306, 3.1682584721462, 3.337102394617562, 3.5067491073028316, 3.3267531720699655, 3.4111097190706503, 3.426597604439372, 3.377662183716893, 3.4820194595182934, 3.321689693212864, 3.1858823501194515, 3.3897172311276553, 3.2964054946122423, 3.124954726879618, 3.2544488945443715, 3.2716158240412674], 'val_acc': [0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9048099943569728, 0.9049221646218073, 0.9054555978093829, 0.9061950360025678, 0.9063346811703273, 0.9066689780780247, 0.9088736318406605, 0.9127816160519918, 0.9154945242972601, 0.9192330780483428, 0.9218818573724656, 0.9232463496071952, 0.9228044663156781, 0.9241117210615248, 0.9244711654526847, 0.9258722521009899, 0.9254052298409599, 0.9246451287042528, 0.9248901321774438, 0.9256914059321085, 0.9269688356490362, 0.9276007413864136, 0.9286492835907709, 0.9273076908929008, 0.9307921188218253, 0.9267239201636541, 0.9276648107029143, 0.925746310324896, 0.9267124391737438, 0.9294620127904982, 0.9310119237218585, 0.9303754369417826, 0.931110333828699, 0.9306868343126207, 0.9291162944975353, 0.9309295132046654, 0.9309340459959847, 0.9302404068765187, 0.9313644880340213, 0.933216603029342, 0.9313255349795023, 0.9336103513127282, 0.9316758456684294, 0.9321565855117071, 0.9330333953812009, 0.9335759878158569, 0.9315292977151417, 0.9315041190101987, 0.9338942368825277, 0.9335187645185561, 0.9342696950549171, 0.9327815827869234, 0.9309660991032919, 0.9346749300048465, 0.9327838903381711, 0.9337454381443205, 0.9327357922281537, 0.9317948562758309, 0.9327587229864938, 0.9331249793370565, 0.932841127827054, 0.9331616532234919, 0.9365384578704834, 0.9349542402085804, 0.935171712012518, 0.9365178460166568, 0.9346817817006793, 0.9358104637690953, 0.9330356972558158, 0.9311034821328663, 0.936597968850817, 0.9359409326598758, 0.9374496454284305, 0.935716552393777, 0.9346199489775158, 0.9352976424353463, 0.9366803935595921, 0.93456274554843, 0.9362499912579855, 0.9369849023364839, 0.933429499467214, 0.9373671894981748, 0.9326213626634507, 0.9314354203996205, 0.9342582452864874, 0.9341781423205421, 0.9358424657867068, 0.9366323437009539, 0.9367055751028515, 0.9377014835675558, 0.936705558072953, 0.9379418378784543, 0.9356227375212169, 0.9361240750267392, 0.9368612823032197, 0.9308241747674488, 0.9372618851207551, 0.9326877565610976, 0.9372504552205404, 0.9325366190501622, 0.9320672807239351, 0.937092505750202, 0.934205603031885, 0.9361309238842556, 0.9334523564293271, 0.9365545028731936, 0.9364789582434154, 0.9381867931002662, 0.9384432264736721, 0.9368269415128798, 0.9321383039156595, 0.9339720635187059, 0.9334638544491359, 0.9341941192036584, 0.9385576673916408, 0.9352541111764454, 0.9338965188889277, 0.9358791056133452, 0.936522438412621, 0.9378319411050706, 0.9338026699565706, 0.9341643509410676, 0.9387774836449396, 0.9374473690986633, 0.9360553877694267, 0.9383882795061383, 0.9369322345370338, 0.9365315976597014, 0.9349427648953029, 0.9365224355743045, 0.936080569312686, 0.9373443382126945, 0.9387911841982887, 0.9371978072893052, 0.9357051565533593, 0.9328502813975016, 0.9332188481376285, 0.9311080489839826, 0.935709692182995, 0.9351762817019508, 0.9370604498045785, 0.9391300167356219, 0.9375434915224711, 0.9378319751648676, 0.9389285927727109, 0.9366392039117359, 0.938301268078032, 0.9374908293996539, 0.9388301429294404, 0.9355173877307347, 0.9373878382501148, 0.9370902152288527, 0.9375823963256109, 0.935755485580081, 0.939184966541472, 0.937227535815466, 0.9385485365277245, 0.9376625305130368], 'val_mDice': [0.01477157722582065, 0.014165325433991495, 0.02170890053024604, 0.03586767784630259, 0.037672260643116066, 0.04018151533922979, 0.037947318028836025, 0.0372340769640037, 0.038764268647701966, 0.04535846775841145, 0.06495394251708474, 0.0787029755523517, 0.09072731519561439, 0.11166077621635936, 0.12743306683287733, 0.13534928601057755, 0.1571290964881579, 0.17319408759829544, 0.19036236813380605, 0.20167405770293304, 0.21996196643227622, 0.23296296729573182, 0.23835989948184716, 0.2451823397229115, 0.2664250128325962, 0.27952565031037446, 0.2876437020798524, 0.283263105544306, 0.29264831117221285, 0.31324167478652226, 0.3050113408161061, 0.3175991780701138, 0.3298407101205417, 0.3325160669961146, 0.337161477122988, 0.34977814980915617, 0.34669616215285803, 0.351786836449589, 0.34103651327036677, 0.35432257946758045, 0.3549699229853494, 0.3700830551485221, 0.36389710878332454, 0.3709916729657423, 0.3587031823893388, 0.3610170737263702, 0.37339846399568377, 0.366344941513879, 0.37343652581884745, 0.3748381565369311, 0.37889821508101057, 0.3771761428742182, 0.3831603562548047, 0.3687774958532481, 0.38181229893650326, 0.3793673689166705, 0.38764560506457374, 0.3781967578189714, 0.3821088479210933, 0.387738770849648, 0.3893643943149419, 0.3841332945795286, 0.3898382014816716, 0.3868249376260099, 0.39580746385313215, 0.38730683443801744, 0.3867122210739624, 0.3907095784587519, 0.3979638296933401, 0.38991874279010863, 0.3968529424497059, 0.39399995583863484, 0.3894077963417485, 0.40126295520790983, 0.38969319810469943, 0.3921549205801317, 0.4006517290004662, 0.40043074407038237, 0.4007489315810658, 0.3882789709383533, 0.395660996259678, 0.4105502475230467, 0.39033830343257814, 0.40860699330057415, 0.4033533748061884, 0.4075794844400315, 0.41098259868366377, 0.40938075481071357, 0.4010458281707196, 0.4162167053492296, 0.41338751802132245, 0.4014003736277421, 0.4181400255433151, 0.4173496610351971, 0.3968664091967401, 0.4122117200777644, 0.39786082860969363, 0.40980683267116547, 0.416958681174687, 0.41384405004126684, 0.4226698933967522, 0.39760174983668894, 0.41895420388096855, 0.40897235753280775, 0.42113749789340155, 0.42020559523786816, 0.403714857285931, 0.41899303453309195, 0.41087929123923894, 0.42076361197091283, 0.40441022103741053, 0.4106842226215771, 0.4192600143807275, 0.4058173713939531, 0.4158075320578757, 0.4168779328465462, 0.42523208331494106, 0.42648746853783015, 0.42492731367903097, 0.42227277620917275, 0.42303934338546934, 0.41094245211709113, 0.4119034526603563, 0.4147669080467451, 0.41001585090444204, 0.4334178717718238, 0.4194027717624392, 0.41179684088343665, 0.4067967495038396, 0.4239969877969651, 0.4212875481517542, 0.4200099845017706, 0.41911133467441514, 0.42809104919433594, 0.41880823157372926, 0.4160352395403953, 0.4348123570283254, 0.4364429229781741, 0.43826010078191757, 0.4138511479610488, 0.4190803416782901, 0.4240111564951284, 0.41708535328507423, 0.42678246221372057, 0.42326426825353075, 0.42331332429533913, 0.4078340455889702, 0.41295806460437323, 0.40695888212039355, 0.42452958935783025, 0.4165719334213507, 0.42566585824603126, 0.4240877355138461, 0.4338271092800867, 0.4299335554242134, 0.4203681204290617, 0.41684347249212717, 0.4257032072969845, 0.42308261564799715, 0.4224454764099348, 0.42154501218880924, 0.4208956076985314, 0.42079769926411764, 0.4347051680088043, 0.4147749122764383, 0.4264427075783412, 0.43712881491297767, 0.4212539994290897, 0.4232595775808607], 'loss': [106.92097729921387, 24.740318705402682, 13.97246314225823, 10.39807441309511, 8.745894638541026, 7.835044226619764, 7.212503772170782, 6.719515535535899, 6.307643126908216, 5.921943834160297, 5.58266224694753, 5.2979984568428025, 5.06068827210995, 4.846689228012265, 4.664582643845372, 4.471571535417885, 4.29022996026196, 4.149030610548409, 3.993862713649596, 3.8552087413706206, 3.720465928620632, 3.616078203115616, 3.51024427937018, 3.4152660068345018, 3.331100922226561, 3.242660544891681, 3.1645613214012194, 3.1050342834131417, 3.033855325619058, 2.9671292106425438, 2.8969740866695455, 2.853495293875202, 2.7976988608009306, 2.747142738061349, 2.7152015304482693, 2.673645888354844, 2.640943754286993, 2.6059558585771643, 2.5751795425381943, 2.5506888262357283, 2.513539882140144, 2.484497213538313, 2.471994887226598, 2.4397109754746973, 2.4191768754762744, 2.4126385666856605, 2.3774439489678287, 2.3647317073238088, 2.337787298285619, 2.3185491375732865, 2.306239557192692, 2.2951827424096996, 2.2795488334975307, 2.2622075846642455, 2.2575642306703076, 2.238195203829299, 2.224533777579835, 2.2095568930227127, 2.191982905943315, 2.1823735482802573, 2.1741498144738864, 2.168116794125248, 2.149681107189376, 2.142234257542147, 2.1295945697697642, 2.118672317706576, 2.1102625495187346, 2.1035022081978547, 2.092947154783297, 2.0862250800350624, 2.0723226098212657, 2.0679258877127435, 2.0494412730281146, 2.050695087814552, 2.0401333595990723, 2.023988557707627, 2.0213571389746017, 2.01432497454949, 2.0066822056184734, 1.991111818043878, 1.9943243087958298, 1.9894517342065694, 1.9797309036541701, 1.9672443724239357, 1.9569752703564278, 1.9623618402163003, 1.9548134870190874, 1.9478638652939775, 1.934863240658628, 1.9345614532296405, 1.92612923798084, 1.9194998357345172, 1.9288660207189114, 1.9186459617254383, 1.8995815683978587, 1.9006166869053749, 1.9018197129497598, 1.8935558669985846, 1.8896811209089015, 1.8959036282958013, 1.8885350034304476, 1.8766054800992207, 1.8848750154384188, 1.8678548486563242, 1.8585259138974743, 1.8587383708214746, 1.8475524162267107, 1.8448270619294767, 1.8481872088651288, 1.8377980506096085, 1.8400061980409845, 1.8330824751373154, 1.8256903710520656, 1.8269205570037088, 1.8108195276050778, 1.814192453284748, 1.818239428276774, 1.8092228806912842, 1.8038037629262638, 1.8078857288669479, 1.7924688618974187, 1.7931462618940157, 1.7849533838046316, 1.7805410045315264, 1.7802496254731401, 1.7748809436485904, 1.7733751766158135, 1.7656843082188227, 1.7672633644885034, 1.7624004567453162, 1.7574750700276414, 1.7585415343960655, 1.7629109484303174, 1.7528389953845311, 1.748327239230347, 1.7438821845002226, 1.7375236313214806, 1.7371309136342332, 1.7432668921297998, 1.7383740763915212, 1.7290431274162543, 1.7330411918732802, 1.7227426216692876, 1.7318786956084034, 1.7158831696255672, 1.7197331899343622, 1.7130981152839506, 1.713728763084088, 1.7194749171135857, 1.70762620108744, 1.7156647967308594, 1.7048008569247097, 1.7009942531815516, 1.7000189795989578, 1.7020180503228552, 1.6975499091636521, 1.7084664995325125, 1.6951012043725877, 1.689908495706838, 1.6816230042720315, 1.6924099027018844, 1.6883809308176536, 1.6885882475621121, 1.680969038610861, 1.6799054583855992, 1.6836609596091014, 1.6783673097017713, 1.6817986840615273, 1.6815926916042643], 'acc': [0.5829790822913106, 0.837953959514301, 0.8664997593846789, 0.8684808368946698, 0.8685514639286676, 0.8680790351355078, 0.867587304101506, 0.8675854768347644, 0.8681284130044772, 0.8685252123056452, 0.8692157978907655, 0.8699554682168653, 0.8705835397679704, 0.871031847605936, 0.8715947829668108, 0.8720385492112288, 0.8727381280536295, 0.873851559606618, 0.8759931670532352, 0.8787542306000221, 0.8817645526125364, 0.8843626458336444, 0.8867191332991192, 0.8885245641716096, 0.8901978041800732, 0.8916465763942755, 0.8930852006160618, 0.8941191513344849, 0.8956894360113539, 0.8969701889601428, 0.8986949173269277, 0.8996750874879711, 0.9010009089025524, 0.902312802043475, 0.9028771077596041, 0.9036472930553356, 0.904253378007737, 0.9052649654242628, 0.9061951998090477, 0.9069618846332725, 0.9075235220676722, 0.9084484059732039, 0.9088347023085452, 0.9099408109936844, 0.9104969346019972, 0.9106646764823093, 0.9118346919874615, 0.9117796097459971, 0.9129217746332704, 0.9134820918368539, 0.9140372424886846, 0.9139473338864409, 0.9146006198914864, 0.9150170376931906, 0.9151978058256073, 0.9156939772734964, 0.9160561786696289, 0.9163562107320601, 0.9168427720312579, 0.9170174197537466, 0.9173758173347416, 0.9173324602243641, 0.9176593481954269, 0.9181148360661282, 0.918372324040705, 0.9181712610036256, 0.9186508942712174, 0.9189868833900579, 0.919273404491782, 0.9193826859595345, 0.9197395348424785, 0.9199778048293954, 0.920486843813493, 0.9201916374060743, 0.9207751264640172, 0.9209743088234452, 0.9211407055026515, 0.9211466628874982, 0.9214292665609901, 0.9216745852826532, 0.9217538562771532, 0.9214955098325172, 0.9218441604970392, 0.9219500317218701, 0.9221790392251288, 0.9221417292110811, 0.9222103901766204, 0.9223152922715253, 0.9225373694909108, 0.922560258065204, 0.9227974259694786, 0.9230583386428558, 0.9227971438040549, 0.9230177904774923, 0.9235689538463315, 0.9233208979649283, 0.9233573016942018, 0.9236076983707774, 0.9236276702129798, 0.9233894642433128, 0.9236524184935581, 0.9239470725439177, 0.9235189938007441, 0.9238485437793632, 0.9241678087219972, 0.9241764958777684, 0.9244116663082454, 0.924563093580261, 0.9243598276878888, 0.9245357757040318, 0.9244800462000214, 0.924520251986285, 0.9248615689467943, 0.9249580115257993, 0.9251135421415182, 0.9251392405916092, 0.925113778100989, 0.9254777575357355, 0.9255340643211301, 0.9254025836222568, 0.9256387533929183, 0.9255481304197567, 0.9258835641593631, 0.9259674706072023, 0.9258966799850566, 0.9262120040919662, 0.9263870188420923, 0.926423399761414, 0.9264822775328897, 0.9265729496537409, 0.9267201589692275, 0.926664825841276, 0.9266209173276058, 0.9269680074696737, 0.9269558182451245, 0.9271797309933956, 0.9273033041197463, 0.9273290926030727, 0.9272254246357252, 0.9275179181412417, 0.9276187171337575, 0.9275146990077952, 0.9278826250123545, 0.9277467660552761, 0.9280668145456088, 0.9279576301436813, 0.9280679492746357, 0.9283248825824304, 0.9284735057816838, 0.9285877212545925, 0.9282809707478151, 0.9286097094505675, 0.9285782414616154, 0.9285890850483945, 0.9287504103801052, 0.9290200117720513, 0.9285893615142031, 0.9290211471101096, 0.9290528706394686, 0.9293189269167531, 0.9289519129885123, 0.9293235166122947, 0.9293503254991012, 0.9295052526221009, 0.929482176121384, 0.9294724871548288, 0.9297114342353421, 0.9295578788350445, 0.9294964929686039], 'mDice': [0.015842468754112066, 0.01554073532747009, 0.018052535431062024, 0.0264760532937276, 0.03480141261006817, 0.04152948982440508, 0.04752626032906965, 0.053878242271683544, 0.06100143358493463, 0.07193096174408022, 0.08564336290100026, 0.0989897296447411, 0.1117994550099165, 0.12298102317769702, 0.13420971977542953, 0.1465781175443288, 0.16005996126176675, 0.17250019522480406, 0.1868997805778613, 0.20150151435685842, 0.21557690277307828, 0.22834512795617362, 0.2409463598562937, 0.2520827172704347, 0.26215032159604157, 0.274170981836797, 0.2844124066822787, 0.29308578091617354, 0.3028041740155114, 0.3120426554451886, 0.3226514220272205, 0.32904517958567325, 0.33791271384704175, 0.3461829005970945, 0.35251703491233, 0.35882715160662393, 0.364808265876053, 0.3708127313710969, 0.3761201105596749, 0.3809138293932324, 0.3864681459309762, 0.3927056481205293, 0.3954555655412874, 0.4006801556594022, 0.4043098062489427, 0.4065445036138439, 0.413583133758229, 0.4143693784953129, 0.4208427405504448, 0.42456293245122134, 0.42644399591831655, 0.42894284711859465, 0.4310552027095874, 0.435074152946702, 0.4362766562639828, 0.43977221786826476, 0.4423402168742949, 0.44520382402442843, 0.4493638713143339, 0.4509041829216749, 0.45396190944930503, 0.45447846681184106, 0.4576704702626448, 0.4598201718120325, 0.46160117570825776, 0.4638644090697603, 0.4663101444877318, 0.467759910129917, 0.47011321001666945, 0.4718086719398028, 0.47440632943912164, 0.4753439328057793, 0.47883569597853204, 0.47840552547659654, 0.4812691894849877, 0.48409153548927003, 0.4846845330910713, 0.48616292591795474, 0.48821740164126143, 0.49124100329744025, 0.49068810469455654, 0.49261319582141017, 0.4939407613641935, 0.4963582827563642, 0.4984191933509263, 0.49891385880191363, 0.4989874956234126, 0.5010439783179601, 0.5030738613567348, 0.5034567249724464, 0.5058260819850824, 0.5069351739706, 0.5051486222919114, 0.5067435178093445, 0.5113811735153566, 0.5111073924094606, 0.5104012675935279, 0.5132788625300356, 0.5125356430214035, 0.5119064023252486, 0.5149350759494459, 0.5167321140352003, 0.5148833755217744, 0.5184287918034559, 0.5208808323287633, 0.5201140493125798, 0.523389136145426, 0.5243348667611041, 0.5235042482045705, 0.5251577575817626, 0.524835405412703, 0.5266811246928944, 0.5280228254191559, 0.5286008548892852, 0.5316785098455167, 0.5310786044595327, 0.5300411190029649, 0.5317354558266632, 0.5335101244512587, 0.5333227198294461, 0.5360592063713883, 0.5361763297167039, 0.5378776213004453, 0.5386428063444855, 0.5389702339978583, 0.5397928625098171, 0.5406946487938804, 0.543061491265011, 0.541707045325935, 0.543359939822669, 0.5450889801436682, 0.5430563804568733, 0.5435261726839041, 0.5454373617886532, 0.5461489320467818, 0.5477900284205831, 0.5485543678416805, 0.5494555970113448, 0.5481751171269811, 0.549265207227586, 0.5510913983737202, 0.5503917287971235, 0.5525754501255624, 0.5512272215588189, 0.5542919171300035, 0.5529372119618584, 0.5548707085904161, 0.554323335109429, 0.5541893484299643, 0.5559474555472085, 0.5547573950011675, 0.5568065572295181, 0.5574861084056631, 0.5578295406013732, 0.5578299835166284, 0.5590653972334316, 0.5567219529350483, 0.5592897475190951, 0.5607419818702957, 0.5623725311292351, 0.56025524459548, 0.5611886558262443, 0.561114808509684, 0.5628680133833369, 0.5627170670278412, 0.5617374138540676, 0.5634520343096012, 0.562661313047661, 0.5629909306977189]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.48s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.22s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  2.00s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<07:13,  1.53s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:33,  1.60s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:26,  1.58s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<08:00,  1.71s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<07:34,  1.62s/it]predicting train subjects:   2%|▏         | 6/285 [00:10<08:01,  1.72s/it]predicting train subjects:   2%|▏         | 7/285 [00:12<08:20,  1.80s/it]predicting train subjects:   3%|▎         | 8/285 [00:14<08:32,  1.85s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<08:10,  1.78s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:26,  1.84s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:39,  1.90s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:44,  1.92s/it]predicting train subjects:   5%|▍         | 13/285 [00:23<08:52,  1.96s/it]predicting train subjects:   5%|▍         | 14/285 [00:25<08:55,  1.98s/it]predicting train subjects:   5%|▌         | 15/285 [00:27<08:57,  1.99s/it]predicting train subjects:   6%|▌         | 16/285 [00:29<09:01,  2.01s/it]predicting train subjects:   6%|▌         | 17/285 [00:31<08:58,  2.01s/it]predicting train subjects:   6%|▋         | 18/285 [00:34<09:03,  2.04s/it]predicting train subjects:   7%|▋         | 19/285 [00:35<08:54,  2.01s/it]predicting train subjects:   7%|▋         | 20/285 [00:38<08:56,  2.02s/it]predicting train subjects:   7%|▋         | 21/285 [00:40<08:55,  2.03s/it]predicting train subjects:   8%|▊         | 22/285 [00:42<08:50,  2.02s/it]predicting train subjects:   8%|▊         | 23/285 [00:44<08:53,  2.04s/it]predicting train subjects:   8%|▊         | 24/285 [00:46<08:47,  2.02s/it]predicting train subjects:   9%|▉         | 25/285 [00:48<08:42,  2.01s/it]predicting train subjects:   9%|▉         | 26/285 [00:50<08:41,  2.01s/it]predicting train subjects:   9%|▉         | 27/285 [00:52<08:35,  2.00s/it]predicting train subjects:  10%|▉         | 28/285 [00:54<08:28,  1.98s/it]predicting train subjects:  10%|█         | 29/285 [00:56<08:25,  1.98s/it]predicting train subjects:  11%|█         | 30/285 [00:57<08:23,  1.97s/it]predicting train subjects:  11%|█         | 31/285 [00:59<08:13,  1.94s/it]predicting train subjects:  11%|█         | 32/285 [01:01<08:13,  1.95s/it]predicting train subjects:  12%|█▏        | 33/285 [01:03<08:11,  1.95s/it]predicting train subjects:  12%|█▏        | 34/285 [01:05<08:10,  1.95s/it]predicting train subjects:  12%|█▏        | 35/285 [01:07<08:07,  1.95s/it]predicting train subjects:  13%|█▎        | 36/285 [01:09<08:04,  1.95s/it]predicting train subjects:  13%|█▎        | 37/285 [01:11<08:00,  1.94s/it]predicting train subjects:  13%|█▎        | 38/285 [01:13<07:56,  1.93s/it]predicting train subjects:  14%|█▎        | 39/285 [01:15<07:56,  1.94s/it]predicting train subjects:  14%|█▍        | 40/285 [01:17<07:51,  1.92s/it]predicting train subjects:  14%|█▍        | 41/285 [01:19<07:47,  1.92s/it]predicting train subjects:  15%|█▍        | 42/285 [01:21<07:45,  1.92s/it]predicting train subjects:  15%|█▌        | 43/285 [01:22<07:40,  1.90s/it]predicting train subjects:  15%|█▌        | 44/285 [01:24<07:40,  1.91s/it]predicting train subjects:  16%|█▌        | 45/285 [01:26<07:42,  1.93s/it]predicting train subjects:  16%|█▌        | 46/285 [01:28<07:21,  1.85s/it]predicting train subjects:  16%|█▋        | 47/285 [01:30<07:01,  1.77s/it]predicting train subjects:  17%|█▋        | 48/285 [01:31<06:50,  1.73s/it]predicting train subjects:  17%|█▋        | 49/285 [01:33<06:42,  1.71s/it]predicting train subjects:  18%|█▊        | 50/285 [01:34<06:30,  1.66s/it]predicting train subjects:  18%|█▊        | 51/285 [01:36<06:25,  1.65s/it]predicting train subjects:  18%|█▊        | 52/285 [01:38<06:21,  1.64s/it]predicting train subjects:  19%|█▊        | 53/285 [01:39<06:19,  1.64s/it]predicting train subjects:  19%|█▉        | 54/285 [01:41<06:13,  1.62s/it]predicting train subjects:  19%|█▉        | 55/285 [01:43<06:20,  1.65s/it]predicting train subjects:  20%|█▉        | 56/285 [01:44<06:16,  1.65s/it]predicting train subjects:  20%|██        | 57/285 [01:46<06:14,  1.64s/it]predicting train subjects:  20%|██        | 58/285 [01:48<06:13,  1.65s/it]predicting train subjects:  21%|██        | 59/285 [01:49<06:14,  1.66s/it]predicting train subjects:  21%|██        | 60/285 [01:51<06:17,  1.68s/it]predicting train subjects:  21%|██▏       | 61/285 [01:53<06:20,  1.70s/it]predicting train subjects:  22%|██▏       | 62/285 [01:54<06:17,  1.69s/it]predicting train subjects:  22%|██▏       | 63/285 [01:56<06:22,  1.72s/it]predicting train subjects:  22%|██▏       | 64/285 [01:58<06:21,  1.73s/it]predicting train subjects:  23%|██▎       | 65/285 [02:00<06:35,  1.80s/it]predicting train subjects:  23%|██▎       | 66/285 [02:02<06:36,  1.81s/it]predicting train subjects:  24%|██▎       | 67/285 [02:03<06:30,  1.79s/it]predicting train subjects:  24%|██▍       | 68/285 [02:05<06:21,  1.76s/it]predicting train subjects:  24%|██▍       | 69/285 [02:07<06:18,  1.75s/it]predicting train subjects:  25%|██▍       | 70/285 [02:09<06:13,  1.74s/it]predicting train subjects:  25%|██▍       | 71/285 [02:10<06:14,  1.75s/it]predicting train subjects:  25%|██▌       | 72/285 [02:12<06:13,  1.75s/it]predicting train subjects:  26%|██▌       | 73/285 [02:14<06:14,  1.76s/it]predicting train subjects:  26%|██▌       | 74/285 [02:16<06:09,  1.75s/it]predicting train subjects:  26%|██▋       | 75/285 [02:17<06:05,  1.74s/it]predicting train subjects:  27%|██▋       | 76/285 [02:19<05:59,  1.72s/it]predicting train subjects:  27%|██▋       | 77/285 [02:21<05:57,  1.72s/it]predicting train subjects:  27%|██▋       | 78/285 [02:23<06:06,  1.77s/it]predicting train subjects:  28%|██▊       | 79/285 [02:24<06:01,  1.75s/it]predicting train subjects:  28%|██▊       | 80/285 [02:26<05:59,  1.75s/it]predicting train subjects:  28%|██▊       | 81/285 [02:28<06:00,  1.77s/it]predicting train subjects:  29%|██▉       | 82/285 [02:30<05:53,  1.74s/it]predicting train subjects:  29%|██▉       | 83/285 [02:31<05:52,  1.75s/it]predicting train subjects:  29%|██▉       | 84/285 [02:33<05:49,  1.74s/it]predicting train subjects:  30%|██▉       | 85/285 [02:35<05:59,  1.80s/it]predicting train subjects:  30%|███       | 86/285 [02:37<06:08,  1.85s/it]predicting train subjects:  31%|███       | 87/285 [02:39<06:06,  1.85s/it]predicting train subjects:  31%|███       | 88/285 [02:41<06:10,  1.88s/it]predicting train subjects:  31%|███       | 89/285 [02:43<06:13,  1.90s/it]predicting train subjects:  32%|███▏      | 90/285 [02:45<06:15,  1.93s/it]predicting train subjects:  32%|███▏      | 91/285 [02:47<06:17,  1.94s/it]predicting train subjects:  32%|███▏      | 92/285 [02:49<06:14,  1.94s/it]predicting train subjects:  33%|███▎      | 93/285 [02:51<06:18,  1.97s/it]predicting train subjects:  33%|███▎      | 94/285 [02:53<06:11,  1.94s/it]predicting train subjects:  33%|███▎      | 95/285 [02:55<06:10,  1.95s/it]predicting train subjects:  34%|███▎      | 96/285 [02:56<06:06,  1.94s/it]predicting train subjects:  34%|███▍      | 97/285 [02:58<05:58,  1.91s/it]predicting train subjects:  34%|███▍      | 98/285 [03:00<05:55,  1.90s/it]predicting train subjects:  35%|███▍      | 99/285 [03:02<05:53,  1.90s/it]predicting train subjects:  35%|███▌      | 100/285 [03:04<05:58,  1.94s/it]predicting train subjects:  35%|███▌      | 101/285 [03:06<05:55,  1.93s/it]predicting train subjects:  36%|███▌      | 102/285 [03:08<05:51,  1.92s/it]predicting train subjects:  36%|███▌      | 103/285 [03:10<05:45,  1.90s/it]predicting train subjects:  36%|███▋      | 104/285 [03:12<05:42,  1.89s/it]predicting train subjects:  37%|███▋      | 105/285 [03:13<05:36,  1.87s/it]predicting train subjects:  37%|███▋      | 106/285 [03:15<05:33,  1.87s/it]predicting train subjects:  38%|███▊      | 107/285 [03:17<05:34,  1.88s/it]predicting train subjects:  38%|███▊      | 108/285 [03:19<05:31,  1.87s/it]predicting train subjects:  38%|███▊      | 109/285 [03:21<05:28,  1.87s/it]predicting train subjects:  39%|███▊      | 110/285 [03:23<05:25,  1.86s/it]predicting train subjects:  39%|███▉      | 111/285 [03:25<05:25,  1.87s/it]predicting train subjects:  39%|███▉      | 112/285 [03:27<05:24,  1.87s/it]predicting train subjects:  40%|███▉      | 113/285 [03:28<05:22,  1.87s/it]predicting train subjects:  40%|████      | 114/285 [03:30<05:19,  1.87s/it]predicting train subjects:  40%|████      | 115/285 [03:32<05:15,  1.86s/it]predicting train subjects:  41%|████      | 116/285 [03:34<05:11,  1.84s/it]predicting train subjects:  41%|████      | 117/285 [03:36<05:08,  1.84s/it]predicting train subjects:  41%|████▏     | 118/285 [03:38<05:09,  1.86s/it]predicting train subjects:  42%|████▏     | 119/285 [03:39<05:07,  1.85s/it]predicting train subjects:  42%|████▏     | 120/285 [03:41<05:06,  1.86s/it]predicting train subjects:  42%|████▏     | 121/285 [03:43<04:54,  1.80s/it]predicting train subjects:  43%|████▎     | 122/285 [03:45<04:39,  1.71s/it]predicting train subjects:  43%|████▎     | 123/285 [03:46<04:24,  1.63s/it]predicting train subjects:  44%|████▎     | 124/285 [03:48<04:31,  1.69s/it]predicting train subjects:  44%|████▍     | 125/285 [03:49<04:29,  1.68s/it]predicting train subjects:  44%|████▍     | 126/285 [03:51<04:27,  1.68s/it]predicting train subjects:  45%|████▍     | 127/285 [03:53<04:26,  1.69s/it]predicting train subjects:  45%|████▍     | 128/285 [03:55<04:23,  1.68s/it]predicting train subjects:  45%|████▌     | 129/285 [03:56<04:24,  1.70s/it]predicting train subjects:  46%|████▌     | 130/285 [03:58<04:22,  1.69s/it]predicting train subjects:  46%|████▌     | 131/285 [04:00<04:19,  1.69s/it]predicting train subjects:  46%|████▋     | 132/285 [04:01<04:18,  1.69s/it]predicting train subjects:  47%|████▋     | 133/285 [04:03<04:16,  1.69s/it]predicting train subjects:  47%|████▋     | 134/285 [04:05<04:14,  1.68s/it]predicting train subjects:  47%|████▋     | 135/285 [04:06<04:13,  1.69s/it]predicting train subjects:  48%|████▊     | 136/285 [04:08<04:10,  1.68s/it]predicting train subjects:  48%|████▊     | 137/285 [04:10<04:08,  1.68s/it]predicting train subjects:  48%|████▊     | 138/285 [04:11<04:06,  1.68s/it]predicting train subjects:  49%|████▉     | 139/285 [04:13<04:07,  1.69s/it]predicting train subjects:  49%|████▉     | 140/285 [04:15<04:04,  1.68s/it]predicting train subjects:  49%|████▉     | 141/285 [04:16<04:01,  1.68s/it]predicting train subjects:  50%|████▉     | 142/285 [04:18<03:52,  1.62s/it]predicting train subjects:  50%|█████     | 143/285 [04:19<03:45,  1.59s/it]predicting train subjects:  51%|█████     | 144/285 [04:21<03:39,  1.56s/it]predicting train subjects:  51%|█████     | 145/285 [04:22<03:34,  1.53s/it]predicting train subjects:  51%|█████     | 146/285 [04:24<03:28,  1.50s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:25<03:26,  1.49s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:27<03:23,  1.49s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:28<03:22,  1.49s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:30<03:19,  1.48s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:31<03:17,  1.47s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:33<03:16,  1.48s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:34<03:15,  1.48s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:36<03:15,  1.49s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:37<03:17,  1.52s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:39<03:14,  1.51s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:40<03:13,  1.51s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:42<03:09,  1.50s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:43<03:13,  1.54s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:45<03:09,  1.52s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:46<03:09,  1.53s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:48<03:04,  1.50s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:49<03:00,  1.48s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:51<02:57,  1.47s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:52<02:58,  1.48s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:54<02:57,  1.49s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:55<02:55,  1.49s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:57<02:57,  1.52s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:58<02:56,  1.52s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:00<02:51,  1.49s/it]predicting train subjects:  60%|██████    | 171/285 [05:01<02:49,  1.49s/it]predicting train subjects:  60%|██████    | 172/285 [05:03<02:46,  1.47s/it]predicting train subjects:  61%|██████    | 173/285 [05:04<02:43,  1.46s/it]predicting train subjects:  61%|██████    | 174/285 [05:06<02:41,  1.45s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:07<02:38,  1.44s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:08<02:39,  1.46s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:10<02:37,  1.46s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:11<02:36,  1.46s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:13<02:35,  1.47s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:14<02:31,  1.44s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:16<02:30,  1.44s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:17<02:27,  1.43s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:18<02:25,  1.43s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:20<02:23,  1.42s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:21<02:20,  1.41s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:23<02:20,  1.42s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:24<02:19,  1.42s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:26<02:17,  1.42s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:27<02:16,  1.42s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:28<02:13,  1.41s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:30<02:10,  1.39s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:31<02:11,  1.42s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:33<02:12,  1.44s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:34<02:10,  1.43s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:36<02:09,  1.44s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:37<02:15,  1.53s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:39<02:19,  1.59s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:41<02:22,  1.63s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:42<02:21,  1.64s/it]predicting train subjects:  70%|███████   | 200/285 [05:44<02:22,  1.67s/it]predicting train subjects:  71%|███████   | 201/285 [05:46<02:20,  1.67s/it]predicting train subjects:  71%|███████   | 202/285 [05:47<02:18,  1.67s/it]predicting train subjects:  71%|███████   | 203/285 [05:49<02:17,  1.67s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:51<02:16,  1.69s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:53<02:16,  1.71s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:54<02:16,  1.72s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:56<02:15,  1.74s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:58<02:13,  1.73s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:00<02:10,  1.72s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:01<02:07,  1.70s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:03<02:05,  1.70s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:05<02:04,  1.70s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:06<02:02,  1.70s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:08<01:57,  1.65s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:09<01:52,  1.61s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:11<01:48,  1.58s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:12<01:46,  1.56s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:14<01:43,  1.55s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:16<01:43,  1.56s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:17<01:41,  1.57s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:19<01:39,  1.55s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:20<01:36,  1.53s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:22<01:35,  1.54s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:23<01:33,  1.54s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:25<01:31,  1.53s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:26<01:29,  1.51s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:28<01:27,  1.51s/it]predicting train subjects:  80%|████████  | 228/285 [06:29<01:25,  1.49s/it]predicting train subjects:  80%|████████  | 229/285 [06:31<01:24,  1.51s/it]predicting train subjects:  81%|████████  | 230/285 [06:32<01:23,  1.52s/it]predicting train subjects:  81%|████████  | 231/285 [06:34<01:21,  1.50s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:36<01:26,  1.63s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:38<01:29,  1.72s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:40<01:31,  1.79s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:41<01:31,  1.83s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:43<01:30,  1.85s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:45<01:30,  1.88s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:47<01:28,  1.89s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:49<01:26,  1.89s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:51<01:24,  1.88s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:53<01:22,  1.88s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:55<01:20,  1.87s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:57<01:18,  1.87s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:58<01:17,  1.88s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:00<01:15,  1.88s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:02<01:13,  1.87s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:04<01:10,  1.86s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:06<01:09,  1.87s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:08<01:07,  1.87s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:09<01:00,  1.73s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:11<00:54,  1.61s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:12<00:51,  1.55s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:13<00:48,  1.52s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:15<00:46,  1.51s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:16<00:44,  1.49s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:18<00:43,  1.49s/it]predicting train subjects:  90%|█████████ | 257/285 [07:19<00:41,  1.47s/it]predicting train subjects:  91%|█████████ | 258/285 [07:21<00:39,  1.47s/it]predicting train subjects:  91%|█████████ | 259/285 [07:22<00:37,  1.45s/it]predicting train subjects:  91%|█████████ | 260/285 [07:24<00:36,  1.46s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:25<00:34,  1.45s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:27<00:33,  1.48s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:28<00:32,  1.47s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:29<00:30,  1.46s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:31<00:29,  1.46s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:32<00:27,  1.45s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:34<00:26,  1.45s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:36<00:27,  1.61s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:38<00:27,  1.71s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:40<00:27,  1.81s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:42<00:25,  1.83s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:43<00:23,  1.84s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:45<00:22,  1.88s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:47<00:20,  1.90s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:49<00:19,  1.91s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:51<00:16,  1.89s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:53<00:14,  1.87s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:55<00:13,  1.87s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:57<00:11,  1.88s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:59<00:09,  1.88s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:00<00:07,  1.87s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:02<00:05,  1.87s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:04<00:03,  1.90s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:06<00:01,  1.90s/it]predicting train subjects: 100%|██████████| 285/285 [08:08<00:00,  1.88s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:53,  1.45s/it]Loading train:   1%|          | 2/285 [00:03<07:04,  1.50s/it]Loading train:   1%|          | 3/285 [00:04<07:02,  1.50s/it]Loading train:   1%|▏         | 4/285 [00:06<07:26,  1.59s/it]Loading train:   2%|▏         | 5/285 [00:07<07:07,  1.53s/it]Loading train:   2%|▏         | 6/285 [00:09<07:20,  1.58s/it]Loading train:   2%|▏         | 7/285 [00:11<07:56,  1.72s/it]Loading train:   3%|▎         | 8/285 [00:13<08:10,  1.77s/it]Loading train:   3%|▎         | 9/285 [00:14<07:49,  1.70s/it]Loading train:   4%|▎         | 10/285 [00:16<07:50,  1.71s/it]Loading train:   4%|▍         | 11/285 [00:17<07:16,  1.59s/it]Loading train:   4%|▍         | 12/285 [00:19<06:58,  1.53s/it]Loading train:   5%|▍         | 13/285 [00:20<06:44,  1.49s/it]Loading train:   5%|▍         | 14/285 [00:22<06:30,  1.44s/it]Loading train:   5%|▌         | 15/285 [00:23<06:25,  1.43s/it]Loading train:   6%|▌         | 16/285 [00:24<06:29,  1.45s/it]Loading train:   6%|▌         | 17/285 [00:26<06:26,  1.44s/it]Loading train:   6%|▋         | 18/285 [00:27<06:16,  1.41s/it]Loading train:   7%|▋         | 19/285 [00:29<06:16,  1.42s/it]Loading train:   7%|▋         | 20/285 [00:30<06:10,  1.40s/it]Loading train:   7%|▋         | 21/285 [00:31<06:13,  1.41s/it]Loading train:   8%|▊         | 22/285 [00:33<06:03,  1.38s/it]Loading train:   8%|▊         | 23/285 [00:34<05:55,  1.36s/it]Loading train:   8%|▊         | 24/285 [00:36<06:01,  1.39s/it]Loading train:   9%|▉         | 25/285 [00:37<06:01,  1.39s/it]Loading train:   9%|▉         | 26/285 [00:38<05:58,  1.38s/it]Loading train:   9%|▉         | 27/285 [00:40<05:52,  1.36s/it]Loading train:  10%|▉         | 28/285 [00:41<05:42,  1.33s/it]Loading train:  10%|█         | 29/285 [00:42<05:32,  1.30s/it]Loading train:  11%|█         | 30/285 [00:43<05:30,  1.30s/it]Loading train:  11%|█         | 31/285 [00:45<05:19,  1.26s/it]Loading train:  11%|█         | 32/285 [00:46<05:07,  1.22s/it]Loading train:  12%|█▏        | 33/285 [00:47<05:02,  1.20s/it]Loading train:  12%|█▏        | 34/285 [00:48<05:01,  1.20s/it]Loading train:  12%|█▏        | 35/285 [00:49<05:06,  1.23s/it]Loading train:  13%|█▎        | 36/285 [00:51<05:11,  1.25s/it]Loading train:  13%|█▎        | 37/285 [00:52<05:42,  1.38s/it]Loading train:  13%|█▎        | 38/285 [00:54<05:54,  1.43s/it]Loading train:  14%|█▎        | 39/285 [00:56<06:28,  1.58s/it]Loading train:  14%|█▍        | 40/285 [00:57<06:26,  1.58s/it]Loading train:  14%|█▍        | 41/285 [00:59<06:13,  1.53s/it]Loading train:  15%|█▍        | 42/285 [01:00<06:09,  1.52s/it]Loading train:  15%|█▌        | 43/285 [01:02<06:01,  1.49s/it]Loading train:  15%|█▌        | 44/285 [01:03<05:43,  1.43s/it]Loading train:  16%|█▌        | 45/285 [01:04<05:40,  1.42s/it]Loading train:  16%|█▌        | 46/285 [01:06<05:20,  1.34s/it]Loading train:  16%|█▋        | 47/285 [01:07<05:03,  1.27s/it]Loading train:  17%|█▋        | 48/285 [01:08<04:34,  1.16s/it]Loading train:  17%|█▋        | 49/285 [01:08<04:15,  1.08s/it]Loading train:  18%|█▊        | 50/285 [01:09<04:04,  1.04s/it]Loading train:  18%|█▊        | 51/285 [01:10<03:55,  1.01s/it]Loading train:  18%|█▊        | 52/285 [01:11<03:59,  1.03s/it]Loading train:  19%|█▊        | 53/285 [01:12<03:54,  1.01s/it]Loading train:  19%|█▉        | 54/285 [01:13<03:51,  1.00s/it]Loading train:  19%|█▉        | 55/285 [01:14<03:47,  1.01it/s]Loading train:  20%|█▉        | 56/285 [01:15<03:46,  1.01it/s]Loading train:  20%|██        | 57/285 [01:16<03:43,  1.02it/s]Loading train:  20%|██        | 58/285 [01:17<03:41,  1.03it/s]Loading train:  21%|██        | 59/285 [01:18<03:44,  1.01it/s]Loading train:  21%|██        | 60/285 [01:19<03:42,  1.01it/s]Loading train:  21%|██▏       | 61/285 [01:20<03:39,  1.02it/s]Loading train:  22%|██▏       | 62/285 [01:21<03:34,  1.04it/s]Loading train:  22%|██▏       | 63/285 [01:22<03:35,  1.03it/s]Loading train:  22%|██▏       | 64/285 [01:24<04:11,  1.14s/it]Loading train:  23%|██▎       | 65/285 [01:25<04:44,  1.29s/it]Loading train:  23%|██▎       | 66/285 [01:27<04:47,  1.31s/it]Loading train:  24%|██▎       | 67/285 [01:28<04:25,  1.22s/it]Loading train:  24%|██▍       | 68/285 [01:29<04:15,  1.18s/it]Loading train:  24%|██▍       | 69/285 [01:30<04:06,  1.14s/it]Loading train:  25%|██▍       | 70/285 [01:31<03:55,  1.09s/it]Loading train:  25%|██▍       | 71/285 [01:32<03:46,  1.06s/it]Loading train:  25%|██▌       | 72/285 [01:33<03:39,  1.03s/it]Loading train:  26%|██▌       | 73/285 [01:34<03:34,  1.01s/it]Loading train:  26%|██▌       | 74/285 [01:35<03:41,  1.05s/it]Loading train:  26%|██▋       | 75/285 [01:36<03:36,  1.03s/it]Loading train:  27%|██▋       | 76/285 [01:37<03:29,  1.00s/it]Loading train:  27%|██▋       | 77/285 [01:38<03:27,  1.00it/s]Loading train:  27%|██▋       | 78/285 [01:39<03:30,  1.02s/it]Loading train:  28%|██▊       | 79/285 [01:40<03:29,  1.02s/it]Loading train:  28%|██▊       | 80/285 [01:41<03:27,  1.01s/it]Loading train:  28%|██▊       | 81/285 [01:42<03:30,  1.03s/it]Loading train:  29%|██▉       | 82/285 [01:43<03:27,  1.02s/it]Loading train:  29%|██▉       | 83/285 [01:44<03:32,  1.05s/it]Loading train:  29%|██▉       | 84/285 [01:45<03:33,  1.06s/it]Loading train:  30%|██▉       | 85/285 [01:46<03:44,  1.12s/it]Loading train:  30%|███       | 86/285 [01:47<03:44,  1.13s/it]Loading train:  31%|███       | 87/285 [01:49<03:38,  1.11s/it]Loading train:  31%|███       | 88/285 [01:50<03:34,  1.09s/it]Loading train:  31%|███       | 89/285 [01:51<03:33,  1.09s/it]Loading train:  32%|███▏      | 90/285 [01:52<03:33,  1.10s/it]Loading train:  32%|███▏      | 91/285 [01:53<03:31,  1.09s/it]Loading train:  32%|███▏      | 92/285 [01:54<03:23,  1.06s/it]Loading train:  33%|███▎      | 93/285 [01:55<03:22,  1.05s/it]Loading train:  33%|███▎      | 94/285 [01:56<03:22,  1.06s/it]Loading train:  33%|███▎      | 95/285 [01:57<03:18,  1.05s/it]Loading train:  34%|███▎      | 96/285 [01:58<03:17,  1.05s/it]Loading train:  34%|███▍      | 97/285 [01:59<03:23,  1.08s/it]Loading train:  34%|███▍      | 98/285 [02:00<03:25,  1.10s/it]Loading train:  35%|███▍      | 99/285 [02:01<03:23,  1.10s/it]Loading train:  35%|███▌      | 100/285 [02:03<03:26,  1.11s/it]Loading train:  35%|███▌      | 101/285 [02:04<03:29,  1.14s/it]Loading train:  36%|███▌      | 102/285 [02:05<03:31,  1.16s/it]Loading train:  36%|███▌      | 103/285 [02:06<03:28,  1.15s/it]Loading train:  36%|███▋      | 104/285 [02:07<03:19,  1.10s/it]Loading train:  37%|███▋      | 105/285 [02:08<03:20,  1.12s/it]Loading train:  37%|███▋      | 106/285 [02:09<03:21,  1.13s/it]Loading train:  38%|███▊      | 107/285 [02:10<03:17,  1.11s/it]Loading train:  38%|███▊      | 108/285 [02:12<03:17,  1.12s/it]Loading train:  38%|███▊      | 109/285 [02:13<03:16,  1.11s/it]Loading train:  39%|███▊      | 110/285 [02:14<03:09,  1.08s/it]Loading train:  39%|███▉      | 111/285 [02:15<03:05,  1.06s/it]Loading train:  39%|███▉      | 112/285 [02:16<03:05,  1.07s/it]Loading train:  40%|███▉      | 113/285 [02:17<03:00,  1.05s/it]Loading train:  40%|████      | 114/285 [02:18<02:57,  1.04s/it]Loading train:  40%|████      | 115/285 [02:19<03:00,  1.06s/it]Loading train:  41%|████      | 116/285 [02:20<02:57,  1.05s/it]Loading train:  41%|████      | 117/285 [02:21<03:06,  1.11s/it]Loading train:  41%|████▏     | 118/285 [02:22<03:03,  1.10s/it]Loading train:  42%|████▏     | 119/285 [02:23<03:00,  1.09s/it]Loading train:  42%|████▏     | 120/285 [02:24<02:58,  1.08s/it]Loading train:  42%|████▏     | 121/285 [02:26<03:14,  1.18s/it]Loading train:  43%|████▎     | 122/285 [02:27<03:19,  1.22s/it]Loading train:  43%|████▎     | 123/285 [02:28<03:21,  1.24s/it]Loading train:  44%|████▎     | 124/285 [02:29<03:05,  1.15s/it]Loading train:  44%|████▍     | 125/285 [02:30<02:54,  1.09s/it]Loading train:  44%|████▍     | 126/285 [02:31<02:44,  1.03s/it]Loading train:  45%|████▍     | 127/285 [02:32<02:35,  1.01it/s]Loading train:  45%|████▍     | 128/285 [02:33<02:31,  1.04it/s]Loading train:  45%|████▌     | 129/285 [02:34<02:25,  1.07it/s]Loading train:  46%|████▌     | 130/285 [02:35<02:24,  1.08it/s]Loading train:  46%|████▌     | 131/285 [02:36<02:24,  1.07it/s]Loading train:  46%|████▋     | 132/285 [02:37<02:20,  1.09it/s]Loading train:  47%|████▋     | 133/285 [02:38<02:23,  1.06it/s]Loading train:  47%|████▋     | 134/285 [02:39<02:26,  1.03it/s]Loading train:  47%|████▋     | 135/285 [02:40<02:28,  1.01it/s]Loading train:  48%|████▊     | 136/285 [02:41<02:31,  1.02s/it]Loading train:  48%|████▊     | 137/285 [02:42<02:29,  1.01s/it]Loading train:  48%|████▊     | 138/285 [02:43<02:23,  1.03it/s]Loading train:  49%|████▉     | 139/285 [02:44<02:20,  1.04it/s]Loading train:  49%|████▉     | 140/285 [02:45<02:18,  1.05it/s]Loading train:  49%|████▉     | 141/285 [02:46<02:20,  1.03it/s]Loading train:  50%|████▉     | 142/285 [02:47<02:20,  1.02it/s]Loading train:  50%|█████     | 143/285 [02:47<02:16,  1.04it/s]Loading train:  51%|█████     | 144/285 [02:48<02:11,  1.07it/s]Loading train:  51%|█████     | 145/285 [02:49<02:12,  1.06it/s]Loading train:  51%|█████     | 146/285 [02:50<02:11,  1.06it/s]Loading train:  52%|█████▏    | 147/285 [02:51<02:08,  1.08it/s]Loading train:  52%|█████▏    | 148/285 [02:52<02:05,  1.09it/s]Loading train:  52%|█████▏    | 149/285 [02:53<02:05,  1.09it/s]Loading train:  53%|█████▎    | 150/285 [02:54<02:06,  1.06it/s]Loading train:  53%|█████▎    | 151/285 [02:55<02:05,  1.07it/s]Loading train:  53%|█████▎    | 152/285 [02:56<02:04,  1.07it/s]Loading train:  54%|█████▎    | 153/285 [02:57<02:06,  1.04it/s]Loading train:  54%|█████▍    | 154/285 [02:58<02:05,  1.05it/s]Loading train:  54%|█████▍    | 155/285 [02:59<02:01,  1.07it/s]Loading train:  55%|█████▍    | 156/285 [03:00<01:59,  1.08it/s]Loading train:  55%|█████▌    | 157/285 [03:01<02:02,  1.04it/s]Loading train:  55%|█████▌    | 158/285 [03:02<02:04,  1.02it/s]Loading train:  56%|█████▌    | 159/285 [03:03<02:02,  1.03it/s]Loading train:  56%|█████▌    | 160/285 [03:04<02:02,  1.02it/s]Loading train:  56%|█████▋    | 161/285 [03:04<01:59,  1.04it/s]Loading train:  57%|█████▋    | 162/285 [03:05<01:56,  1.06it/s]Loading train:  57%|█████▋    | 163/285 [03:06<01:54,  1.06it/s]Loading train:  58%|█████▊    | 164/285 [03:07<01:53,  1.07it/s]Loading train:  58%|█████▊    | 165/285 [03:08<01:52,  1.06it/s]Loading train:  58%|█████▊    | 166/285 [03:09<01:50,  1.08it/s]Loading train:  59%|█████▊    | 167/285 [03:10<01:49,  1.07it/s]Loading train:  59%|█████▉    | 168/285 [03:11<01:48,  1.08it/s]Loading train:  59%|█████▉    | 169/285 [03:12<01:47,  1.08it/s]Loading train:  60%|█████▉    | 170/285 [03:13<01:45,  1.09it/s]Loading train:  60%|██████    | 171/285 [03:14<01:49,  1.04it/s]Loading train:  60%|██████    | 172/285 [03:15<01:44,  1.08it/s]Loading train:  61%|██████    | 173/285 [03:16<01:42,  1.09it/s]Loading train:  61%|██████    | 174/285 [03:17<01:41,  1.09it/s]Loading train:  61%|██████▏   | 175/285 [03:17<01:39,  1.10it/s]Loading train:  62%|██████▏   | 176/285 [03:18<01:40,  1.09it/s]Loading train:  62%|██████▏   | 177/285 [03:19<01:37,  1.11it/s]Loading train:  62%|██████▏   | 178/285 [03:20<01:35,  1.12it/s]Loading train:  63%|██████▎   | 179/285 [03:21<01:34,  1.12it/s]Loading train:  63%|██████▎   | 180/285 [03:22<01:36,  1.09it/s]Loading train:  64%|██████▎   | 181/285 [03:23<01:37,  1.07it/s]Loading train:  64%|██████▍   | 182/285 [03:24<01:37,  1.06it/s]Loading train:  64%|██████▍   | 183/285 [03:25<01:35,  1.07it/s]Loading train:  65%|██████▍   | 184/285 [03:26<01:32,  1.09it/s]Loading train:  65%|██████▍   | 185/285 [03:27<01:31,  1.09it/s]Loading train:  65%|██████▌   | 186/285 [03:28<01:31,  1.08it/s]Loading train:  66%|██████▌   | 187/285 [03:29<01:32,  1.06it/s]Loading train:  66%|██████▌   | 188/285 [03:29<01:30,  1.08it/s]Loading train:  66%|██████▋   | 189/285 [03:30<01:26,  1.11it/s]Loading train:  67%|██████▋   | 190/285 [03:31<01:23,  1.14it/s]Loading train:  67%|██████▋   | 191/285 [03:32<01:23,  1.13it/s]Loading train:  67%|██████▋   | 192/285 [03:33<01:19,  1.17it/s]Loading train:  68%|██████▊   | 193/285 [03:34<01:21,  1.13it/s]Loading train:  68%|██████▊   | 194/285 [03:35<01:24,  1.07it/s]Loading train:  68%|██████▊   | 195/285 [03:36<01:25,  1.05it/s]Loading train:  69%|██████▉   | 196/285 [03:37<01:29,  1.01s/it]Loading train:  69%|██████▉   | 197/285 [03:38<01:28,  1.01s/it]Loading train:  69%|██████▉   | 198/285 [03:39<01:29,  1.02s/it]Loading train:  70%|██████▉   | 199/285 [03:40<01:26,  1.00s/it]Loading train:  70%|███████   | 200/285 [03:41<01:25,  1.01s/it]Loading train:  71%|███████   | 201/285 [03:42<01:24,  1.00s/it]Loading train:  71%|███████   | 202/285 [03:43<01:23,  1.01s/it]Loading train:  71%|███████   | 203/285 [03:44<01:26,  1.06s/it]Loading train:  72%|███████▏  | 204/285 [03:45<01:23,  1.03s/it]Loading train:  72%|███████▏  | 205/285 [03:46<01:21,  1.02s/it]Loading train:  72%|███████▏  | 206/285 [03:47<01:20,  1.02s/it]Loading train:  73%|███████▎  | 207/285 [03:48<01:17,  1.01it/s]Loading train:  73%|███████▎  | 208/285 [03:49<01:15,  1.02it/s]Loading train:  73%|███████▎  | 209/285 [03:50<01:16,  1.00s/it]Loading train:  74%|███████▎  | 210/285 [03:51<01:12,  1.03it/s]Loading train:  74%|███████▍  | 211/285 [03:52<01:10,  1.05it/s]Loading train:  74%|███████▍  | 212/285 [03:53<01:07,  1.08it/s]Loading train:  75%|███████▍  | 213/285 [03:54<01:07,  1.07it/s]Loading train:  75%|███████▌  | 214/285 [03:55<01:06,  1.07it/s]Loading train:  75%|███████▌  | 215/285 [03:56<01:05,  1.08it/s]Loading train:  76%|███████▌  | 216/285 [03:56<01:02,  1.10it/s]Loading train:  76%|███████▌  | 217/285 [03:57<00:59,  1.13it/s]Loading train:  76%|███████▋  | 218/285 [03:58<00:58,  1.15it/s]Loading train:  77%|███████▋  | 219/285 [03:59<00:58,  1.13it/s]Loading train:  77%|███████▋  | 220/285 [04:00<01:01,  1.06it/s]Loading train:  78%|███████▊  | 221/285 [04:01<01:01,  1.05it/s]Loading train:  78%|███████▊  | 222/285 [04:02<00:59,  1.06it/s]Loading train:  78%|███████▊  | 223/285 [04:03<00:57,  1.08it/s]Loading train:  79%|███████▊  | 224/285 [04:04<00:58,  1.04it/s]Loading train:  79%|███████▉  | 225/285 [04:05<00:56,  1.06it/s]Loading train:  79%|███████▉  | 226/285 [04:06<00:56,  1.04it/s]Loading train:  80%|███████▉  | 227/285 [04:07<00:54,  1.07it/s]Loading train:  80%|████████  | 228/285 [04:08<00:54,  1.04it/s]Loading train:  80%|████████  | 229/285 [04:09<00:55,  1.00it/s]Loading train:  81%|████████  | 230/285 [04:10<00:55,  1.01s/it]Loading train:  81%|████████  | 231/285 [04:11<00:52,  1.03it/s]Loading train:  81%|████████▏ | 232/285 [04:12<00:57,  1.08s/it]Loading train:  82%|████████▏ | 233/285 [04:13<00:58,  1.12s/it]Loading train:  82%|████████▏ | 234/285 [04:14<00:56,  1.11s/it]Loading train:  82%|████████▏ | 235/285 [04:15<00:55,  1.10s/it]Loading train:  83%|████████▎ | 236/285 [04:17<00:59,  1.22s/it]Loading train:  83%|████████▎ | 237/285 [04:18<00:57,  1.20s/it]Loading train:  84%|████████▎ | 238/285 [04:19<00:57,  1.21s/it]Loading train:  84%|████████▍ | 239/285 [04:20<00:55,  1.20s/it]Loading train:  84%|████████▍ | 240/285 [04:22<00:53,  1.20s/it]Loading train:  85%|████████▍ | 241/285 [04:23<00:51,  1.17s/it]Loading train:  85%|████████▍ | 242/285 [04:24<00:49,  1.16s/it]Loading train:  85%|████████▌ | 243/285 [04:25<00:48,  1.16s/it]Loading train:  86%|████████▌ | 244/285 [04:26<00:47,  1.16s/it]Loading train:  86%|████████▌ | 245/285 [04:27<00:46,  1.17s/it]Loading train:  86%|████████▋ | 246/285 [04:29<00:45,  1.17s/it]Loading train:  87%|████████▋ | 247/285 [04:30<00:44,  1.17s/it]Loading train:  87%|████████▋ | 248/285 [04:31<00:43,  1.17s/it]Loading train:  87%|████████▋ | 249/285 [04:32<00:41,  1.16s/it]Loading train:  88%|████████▊ | 250/285 [04:33<00:40,  1.17s/it]Loading train:  88%|████████▊ | 251/285 [04:34<00:36,  1.09s/it]Loading train:  88%|████████▊ | 252/285 [04:35<00:34,  1.05s/it]Loading train:  89%|████████▉ | 253/285 [04:36<00:32,  1.03s/it]Loading train:  89%|████████▉ | 254/285 [04:37<00:33,  1.09s/it]Loading train:  89%|████████▉ | 255/285 [04:38<00:32,  1.08s/it]Loading train:  90%|████████▉ | 256/285 [04:39<00:30,  1.05s/it]Loading train:  90%|█████████ | 257/285 [04:40<00:29,  1.04s/it]Loading train:  91%|█████████ | 258/285 [04:41<00:27,  1.03s/it]Loading train:  91%|█████████ | 259/285 [04:42<00:26,  1.01s/it]Loading train:  91%|█████████ | 260/285 [04:43<00:25,  1.04s/it]Loading train:  92%|█████████▏| 261/285 [04:44<00:23,  1.01it/s]Loading train:  92%|█████████▏| 262/285 [04:45<00:22,  1.01it/s]Loading train:  92%|█████████▏| 263/285 [04:46<00:22,  1.02s/it]Loading train:  93%|█████████▎| 264/285 [04:48<00:21,  1.05s/it]Loading train:  93%|█████████▎| 265/285 [04:49<00:21,  1.08s/it]Loading train:  93%|█████████▎| 266/285 [04:50<00:20,  1.10s/it]Loading train:  94%|█████████▎| 267/285 [04:51<00:19,  1.09s/it]Loading train:  94%|█████████▍| 268/285 [04:52<00:19,  1.14s/it]Loading train:  94%|█████████▍| 269/285 [04:53<00:18,  1.17s/it]Loading train:  95%|█████████▍| 270/285 [04:54<00:17,  1.15s/it]Loading train:  95%|█████████▌| 271/285 [04:55<00:15,  1.10s/it]Loading train:  95%|█████████▌| 272/285 [04:56<00:13,  1.08s/it]Loading train:  96%|█████████▌| 273/285 [04:58<00:13,  1.12s/it]Loading train:  96%|█████████▌| 274/285 [04:59<00:12,  1.13s/it]Loading train:  96%|█████████▋| 275/285 [05:00<00:11,  1.15s/it]Loading train:  97%|█████████▋| 276/285 [05:01<00:10,  1.15s/it]Loading train:  97%|█████████▋| 277/285 [05:02<00:09,  1.13s/it]Loading train:  98%|█████████▊| 278/285 [05:04<00:08,  1.21s/it]Loading train:  98%|█████████▊| 279/285 [05:05<00:07,  1.28s/it]Loading train:  98%|█████████▊| 280/285 [05:06<00:06,  1.29s/it]Loading train:  99%|█████████▊| 281/285 [05:08<00:05,  1.33s/it]Loading train:  99%|█████████▉| 282/285 [05:10<00:04,  1.43s/it]Loading train:  99%|█████████▉| 283/285 [05:11<00:02,  1.43s/it]Loading train: 100%|█████████▉| 284/285 [05:12<00:01,  1.36s/it]Loading train: 100%|██████████| 285/285 [05:13<00:00,  1.32s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/285 [00:00<00:04, 58.16it/s]concatenating: train:   6%|▌         | 17/285 [00:00<00:04, 63.01it/s]concatenating: train:  15%|█▌        | 44/285 [00:00<00:02, 81.67it/s]concatenating: train:  24%|██▍       | 69/285 [00:00<00:02, 102.23it/s]concatenating: train:  33%|███▎      | 95/285 [00:00<00:01, 124.69it/s]concatenating: train:  42%|████▏     | 121/285 [00:00<00:01, 147.07it/s]concatenating: train:  52%|█████▏    | 149/285 [00:00<00:00, 171.18it/s]concatenating: train:  62%|██████▏   | 176/285 [00:00<00:00, 191.95it/s]concatenating: train:  71%|███████   | 201/285 [00:00<00:00, 204.70it/s]concatenating: train:  79%|███████▉  | 225/285 [00:01<00:00, 205.95it/s]concatenating: train:  87%|████████▋ | 248/285 [00:01<00:00, 177.69it/s]concatenating: train:  94%|█████████▍| 269/285 [00:01<00:00, 181.81it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 200.54it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.43s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.40s/it]Loading test: 100%|██████████| 3/3 [00:04<00:00,  1.39s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 32.86it/s]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<05:36,  1.19s/it]Loading train:   1%|          | 2/285 [00:02<05:55,  1.26s/it]Loading train:   1%|          | 3/285 [00:03<05:59,  1.28s/it]Loading train:   1%|▏         | 4/285 [00:05<06:25,  1.37s/it]Loading train:   2%|▏         | 5/285 [00:06<05:58,  1.28s/it]Loading train:   2%|▏         | 6/285 [00:08<06:13,  1.34s/it]Loading train:   2%|▏         | 7/285 [00:09<06:38,  1.43s/it]Loading train:   3%|▎         | 8/285 [00:11<06:51,  1.49s/it]Loading train:   3%|▎         | 9/285 [00:12<06:33,  1.42s/it]Loading train:   4%|▎         | 10/285 [00:13<06:03,  1.32s/it]Loading train:   4%|▍         | 11/285 [00:14<05:34,  1.22s/it]Loading train:   4%|▍         | 12/285 [00:15<05:25,  1.19s/it]Loading train:   5%|▍         | 13/285 [00:16<05:05,  1.12s/it]Loading train:   5%|▍         | 14/285 [00:17<04:56,  1.09s/it]Loading train:   5%|▌         | 15/285 [00:18<04:50,  1.08s/it]Loading train:   6%|▌         | 16/285 [00:19<04:35,  1.02s/it]Loading train:   6%|▌         | 17/285 [00:20<04:32,  1.02s/it]Loading train:   6%|▋         | 18/285 [00:21<04:24,  1.01it/s]Loading train:   7%|▋         | 19/285 [00:22<04:25,  1.00it/s]Loading train:   7%|▋         | 20/285 [00:23<04:24,  1.00it/s]Loading train:   7%|▋         | 21/285 [00:24<04:23,  1.00it/s]Loading train:   8%|▊         | 22/285 [00:25<04:22,  1.00it/s]Loading train:   8%|▊         | 23/285 [00:26<04:24,  1.01s/it]Loading train:   8%|▊         | 24/285 [00:27<04:27,  1.03s/it]Loading train:   9%|▉         | 25/285 [00:28<04:21,  1.01s/it]Loading train:   9%|▉         | 26/285 [00:29<04:22,  1.01s/it]Loading train:   9%|▉         | 27/285 [00:30<04:09,  1.03it/s]Loading train:  10%|▉         | 28/285 [00:31<04:15,  1.01it/s]Loading train:  10%|█         | 29/285 [00:32<04:14,  1.01it/s]Loading train:  11%|█         | 30/285 [00:33<04:08,  1.02it/s]Loading train:  11%|█         | 31/285 [00:34<04:08,  1.02it/s]Loading train:  11%|█         | 32/285 [00:35<04:12,  1.00it/s]Loading train:  12%|█▏        | 33/285 [00:36<04:12,  1.00s/it]Loading train:  12%|█▏        | 34/285 [00:37<04:08,  1.01it/s]Loading train:  12%|█▏        | 35/285 [00:38<04:07,  1.01it/s]Loading train:  13%|█▎        | 36/285 [00:39<04:06,  1.01it/s]Loading train:  13%|█▎        | 37/285 [00:40<03:55,  1.05it/s]Loading train:  13%|█▎        | 38/285 [00:41<03:54,  1.05it/s]Loading train:  14%|█▎        | 39/285 [00:42<03:52,  1.06it/s]Loading train:  14%|█▍        | 40/285 [00:43<03:55,  1.04it/s]Loading train:  14%|█▍        | 41/285 [00:44<03:52,  1.05it/s]Loading train:  15%|█▍        | 42/285 [00:45<03:48,  1.06it/s]Loading train:  15%|█▌        | 43/285 [00:46<03:49,  1.05it/s]Loading train:  15%|█▌        | 44/285 [00:47<03:45,  1.07it/s]Loading train:  16%|█▌        | 45/285 [00:47<03:41,  1.08it/s]Loading train:  16%|█▌        | 46/285 [00:48<03:39,  1.09it/s]Loading train:  16%|█▋        | 47/285 [00:49<03:36,  1.10it/s]Loading train:  17%|█▋        | 48/285 [00:50<03:25,  1.15it/s]Loading train:  17%|█▋        | 49/285 [00:51<03:21,  1.17it/s]Loading train:  18%|█▊        | 50/285 [00:52<03:16,  1.19it/s]Loading train:  18%|█▊        | 51/285 [00:52<03:09,  1.23it/s]Loading train:  18%|█▊        | 52/285 [00:53<03:05,  1.26it/s]Loading train:  19%|█▊        | 53/285 [00:54<03:02,  1.27it/s]Loading train:  19%|█▉        | 54/285 [00:55<03:03,  1.26it/s]Loading train:  19%|█▉        | 55/285 [00:55<03:01,  1.26it/s]Loading train:  20%|█▉        | 56/285 [00:56<02:59,  1.27it/s]Loading train:  20%|██        | 57/285 [00:57<03:01,  1.25it/s]Loading train:  20%|██        | 58/285 [00:58<03:00,  1.26it/s]Loading train:  21%|██        | 59/285 [00:59<02:56,  1.28it/s]Loading train:  21%|██        | 60/285 [00:59<02:54,  1.29it/s]Loading train:  21%|██▏       | 61/285 [01:00<02:56,  1.27it/s]Loading train:  22%|██▏       | 62/285 [01:01<02:57,  1.26it/s]Loading train:  22%|██▏       | 63/285 [01:02<02:57,  1.25it/s]Loading train:  22%|██▏       | 64/285 [01:03<03:32,  1.04it/s]Loading train:  23%|██▎       | 65/285 [01:05<04:18,  1.17s/it]Loading train:  23%|██▎       | 66/285 [01:06<04:19,  1.19s/it]Loading train:  24%|██▎       | 67/285 [01:07<03:54,  1.08s/it]Loading train:  24%|██▍       | 68/285 [01:08<03:33,  1.01it/s]Loading train:  24%|██▍       | 69/285 [01:08<03:22,  1.07it/s]Loading train:  25%|██▍       | 70/285 [01:09<03:13,  1.11it/s]Loading train:  25%|██▍       | 71/285 [01:10<03:07,  1.14it/s]Loading train:  25%|██▌       | 72/285 [01:11<02:57,  1.20it/s]Loading train:  26%|██▌       | 73/285 [01:12<02:59,  1.18it/s]Loading train:  26%|██▌       | 74/285 [01:13<02:55,  1.20it/s]Loading train:  26%|██▋       | 75/285 [01:13<02:49,  1.24it/s]Loading train:  27%|██▋       | 76/285 [01:14<02:42,  1.29it/s]Loading train:  27%|██▋       | 77/285 [01:15<02:42,  1.28it/s]Loading train:  27%|██▋       | 78/285 [01:16<02:44,  1.25it/s]Loading train:  28%|██▊       | 79/285 [01:16<02:49,  1.22it/s]Loading train:  28%|██▊       | 80/285 [01:17<02:47,  1.22it/s]Loading train:  28%|██▊       | 81/285 [01:18<02:48,  1.21it/s]Loading train:  29%|██▉       | 82/285 [01:19<02:55,  1.15it/s]Loading train:  29%|██▉       | 83/285 [01:20<02:52,  1.17it/s]Loading train:  29%|██▉       | 84/285 [01:21<02:52,  1.17it/s]Loading train:  30%|██▉       | 85/285 [01:22<03:03,  1.09it/s]Loading train:  30%|███       | 86/285 [01:23<03:08,  1.05it/s]Loading train:  31%|███       | 87/285 [01:24<03:04,  1.08it/s]Loading train:  31%|███       | 88/285 [01:25<03:03,  1.07it/s]Loading train:  31%|███       | 89/285 [01:26<03:06,  1.05it/s]Loading train:  32%|███▏      | 90/285 [01:27<03:11,  1.02it/s]Loading train:  32%|███▏      | 91/285 [01:28<03:14,  1.00s/it]Loading train:  32%|███▏      | 92/285 [01:29<03:12,  1.00it/s]Loading train:  33%|███▎      | 93/285 [01:30<03:11,  1.00it/s]Loading train:  33%|███▎      | 94/285 [01:31<03:10,  1.00it/s]Loading train:  33%|███▎      | 95/285 [01:32<03:08,  1.01it/s]Loading train:  34%|███▎      | 96/285 [01:33<03:05,  1.02it/s]Loading train:  34%|███▍      | 97/285 [01:34<03:04,  1.02it/s]Loading train:  34%|███▍      | 98/285 [01:35<03:03,  1.02it/s]Loading train:  35%|███▍      | 99/285 [01:36<03:04,  1.01it/s]Loading train:  35%|███▌      | 100/285 [01:37<03:04,  1.00it/s]Loading train:  35%|███▌      | 101/285 [01:38<03:01,  1.01it/s]Loading train:  36%|███▌      | 102/285 [01:39<02:57,  1.03it/s]Loading train:  36%|███▌      | 103/285 [01:40<02:54,  1.04it/s]Loading train:  36%|███▋      | 104/285 [01:40<02:52,  1.05it/s]Loading train:  37%|███▋      | 105/285 [01:41<02:44,  1.10it/s]Loading train:  37%|███▋      | 106/285 [01:42<02:39,  1.13it/s]Loading train:  38%|███▊      | 107/285 [01:43<02:37,  1.13it/s]Loading train:  38%|███▊      | 108/285 [01:44<02:33,  1.15it/s]Loading train:  38%|███▊      | 109/285 [01:45<02:35,  1.13it/s]Loading train:  39%|███▊      | 110/285 [01:46<02:35,  1.13it/s]Loading train:  39%|███▉      | 111/285 [01:46<02:31,  1.15it/s]Loading train:  39%|███▉      | 112/285 [01:47<02:31,  1.14it/s]Loading train:  40%|███▉      | 113/285 [01:48<02:29,  1.15it/s]Loading train:  40%|████      | 114/285 [01:49<02:30,  1.13it/s]Loading train:  40%|████      | 115/285 [01:50<02:30,  1.13it/s]Loading train:  41%|████      | 116/285 [01:51<02:33,  1.10it/s]Loading train:  41%|████      | 117/285 [01:52<02:31,  1.11it/s]Loading train:  41%|████▏     | 118/285 [01:53<02:31,  1.10it/s]Loading train:  42%|████▏     | 119/285 [01:54<02:34,  1.07it/s]Loading train:  42%|████▏     | 120/285 [01:55<02:37,  1.05it/s]Loading train:  42%|████▏     | 121/285 [01:56<02:54,  1.06s/it]Loading train:  43%|████▎     | 122/285 [01:57<02:55,  1.08s/it]Loading train:  43%|████▎     | 123/285 [01:58<03:01,  1.12s/it]Loading train:  44%|████▎     | 124/285 [01:59<02:50,  1.06s/it]Loading train:  44%|████▍     | 125/285 [02:00<02:36,  1.02it/s]Loading train:  44%|████▍     | 126/285 [02:01<02:27,  1.08it/s]Loading train:  45%|████▍     | 127/285 [02:02<02:19,  1.13it/s]Loading train:  45%|████▍     | 128/285 [02:03<02:18,  1.14it/s]Loading train:  45%|████▌     | 129/285 [02:03<02:15,  1.15it/s]Loading train:  46%|████▌     | 130/285 [02:04<02:10,  1.19it/s]Loading train:  46%|████▌     | 131/285 [02:05<02:06,  1.21it/s]Loading train:  46%|████▋     | 132/285 [02:06<02:08,  1.19it/s]Loading train:  47%|████▋     | 133/285 [02:07<02:06,  1.20it/s]Loading train:  47%|████▋     | 134/285 [02:07<02:02,  1.23it/s]Loading train:  47%|████▋     | 135/285 [02:08<02:01,  1.23it/s]Loading train:  48%|████▊     | 136/285 [02:09<02:00,  1.24it/s]Loading train:  48%|████▊     | 137/285 [02:10<01:58,  1.25it/s]Loading train:  48%|████▊     | 138/285 [02:11<01:58,  1.24it/s]Loading train:  49%|████▉     | 139/285 [02:11<01:57,  1.24it/s]Loading train:  49%|████▉     | 140/285 [02:12<01:56,  1.24it/s]Loading train:  49%|████▉     | 141/285 [02:13<01:59,  1.21it/s]Loading train:  50%|████▉     | 142/285 [02:14<01:52,  1.27it/s]Loading train:  50%|█████     | 143/285 [02:15<01:52,  1.27it/s]Loading train:  51%|█████     | 144/285 [02:15<01:50,  1.28it/s]Loading train:  51%|█████     | 145/285 [02:16<01:47,  1.30it/s]Loading train:  51%|█████     | 146/285 [02:17<01:45,  1.32it/s]Loading train:  52%|█████▏    | 147/285 [02:18<01:43,  1.33it/s]Loading train:  52%|█████▏    | 148/285 [02:18<01:43,  1.33it/s]Loading train:  52%|█████▏    | 149/285 [02:19<01:44,  1.31it/s]Loading train:  53%|█████▎    | 150/285 [02:20<01:46,  1.27it/s]Loading train:  53%|█████▎    | 151/285 [02:21<01:43,  1.29it/s]Loading train:  53%|█████▎    | 152/285 [02:22<01:43,  1.29it/s]Loading train:  54%|█████▎    | 153/285 [02:22<01:44,  1.27it/s]Loading train:  54%|█████▍    | 154/285 [02:23<01:42,  1.27it/s]Loading train:  54%|█████▍    | 155/285 [02:24<01:42,  1.27it/s]Loading train:  55%|█████▍    | 156/285 [02:25<01:40,  1.28it/s]Loading train:  55%|█████▌    | 157/285 [02:25<01:39,  1.28it/s]Loading train:  55%|█████▌    | 158/285 [02:26<01:33,  1.35it/s]Loading train:  56%|█████▌    | 159/285 [02:27<01:35,  1.32it/s]Loading train:  56%|█████▌    | 160/285 [02:28<01:38,  1.27it/s]Loading train:  56%|█████▋    | 161/285 [02:29<01:37,  1.27it/s]Loading train:  57%|█████▋    | 162/285 [02:29<01:36,  1.28it/s]Loading train:  57%|█████▋    | 163/285 [02:30<01:35,  1.28it/s]Loading train:  58%|█████▊    | 164/285 [02:31<01:34,  1.28it/s]Loading train:  58%|█████▊    | 165/285 [02:32<01:30,  1.32it/s]Loading train:  58%|█████▊    | 166/285 [02:32<01:29,  1.33it/s]Loading train:  59%|█████▊    | 167/285 [02:33<01:28,  1.34it/s]Loading train:  59%|█████▉    | 168/285 [02:34<01:28,  1.32it/s]Loading train:  59%|█████▉    | 169/285 [02:35<01:26,  1.35it/s]Loading train:  60%|█████▉    | 170/285 [02:35<01:27,  1.32it/s]Loading train:  60%|██████    | 171/285 [02:36<01:27,  1.30it/s]Loading train:  60%|██████    | 172/285 [02:37<01:26,  1.31it/s]Loading train:  61%|██████    | 173/285 [02:38<01:26,  1.30it/s]Loading train:  61%|██████    | 174/285 [02:38<01:23,  1.32it/s]Loading train:  61%|██████▏   | 175/285 [02:39<01:23,  1.32it/s]Loading train:  62%|██████▏   | 176/285 [02:40<01:20,  1.35it/s]Loading train:  62%|██████▏   | 177/285 [02:41<01:18,  1.38it/s]Loading train:  62%|██████▏   | 178/285 [02:41<01:18,  1.36it/s]Loading train:  63%|██████▎   | 179/285 [02:42<01:16,  1.38it/s]Loading train:  63%|██████▎   | 180/285 [02:43<01:14,  1.41it/s]Loading train:  64%|██████▎   | 181/285 [02:43<01:14,  1.39it/s]Loading train:  64%|██████▍   | 182/285 [02:44<01:14,  1.38it/s]Loading train:  64%|██████▍   | 183/285 [02:45<01:13,  1.39it/s]Loading train:  65%|██████▍   | 184/285 [02:46<01:11,  1.41it/s]Loading train:  65%|██████▍   | 185/285 [02:46<01:11,  1.40it/s]Loading train:  65%|██████▌   | 186/285 [02:47<01:12,  1.36it/s]Loading train:  66%|██████▌   | 187/285 [02:48<01:09,  1.40it/s]Loading train:  66%|██████▌   | 188/285 [02:48<01:09,  1.39it/s]Loading train:  66%|██████▋   | 189/285 [02:49<01:09,  1.39it/s]Loading train:  67%|██████▋   | 190/285 [02:50<01:08,  1.38it/s]Loading train:  67%|██████▋   | 191/285 [02:51<01:09,  1.35it/s]Loading train:  67%|██████▋   | 192/285 [02:51<01:08,  1.36it/s]Loading train:  68%|██████▊   | 193/285 [02:52<01:09,  1.33it/s]Loading train:  68%|██████▊   | 194/285 [02:53<01:08,  1.32it/s]Loading train:  68%|██████▊   | 195/285 [02:54<01:06,  1.35it/s]Loading train:  69%|██████▉   | 196/285 [02:54<01:08,  1.30it/s]Loading train:  69%|██████▉   | 197/285 [02:55<01:11,  1.23it/s]Loading train:  69%|██████▉   | 198/285 [02:56<01:11,  1.22it/s]Loading train:  70%|██████▉   | 199/285 [02:57<01:09,  1.24it/s]Loading train:  70%|███████   | 200/285 [02:58<01:09,  1.22it/s]Loading train:  71%|███████   | 201/285 [02:59<01:08,  1.22it/s]Loading train:  71%|███████   | 202/285 [03:00<01:08,  1.21it/s]Loading train:  71%|███████   | 203/285 [03:00<01:08,  1.20it/s]Loading train:  72%|███████▏  | 204/285 [03:01<01:07,  1.21it/s]Loading train:  72%|███████▏  | 205/285 [03:02<01:06,  1.21it/s]Loading train:  72%|███████▏  | 206/285 [03:03<01:04,  1.23it/s]Loading train:  73%|███████▎  | 207/285 [03:04<01:04,  1.21it/s]Loading train:  73%|███████▎  | 208/285 [03:04<01:03,  1.21it/s]Loading train:  73%|███████▎  | 209/285 [03:05<01:01,  1.24it/s]Loading train:  74%|███████▎  | 210/285 [03:06<00:59,  1.26it/s]Loading train:  74%|███████▍  | 211/285 [03:07<00:59,  1.24it/s]Loading train:  74%|███████▍  | 212/285 [03:08<00:58,  1.24it/s]Loading train:  75%|███████▍  | 213/285 [03:08<00:58,  1.23it/s]Loading train:  75%|███████▌  | 214/285 [03:09<00:56,  1.26it/s]Loading train:  75%|███████▌  | 215/285 [03:10<00:53,  1.30it/s]Loading train:  76%|███████▌  | 216/285 [03:11<00:52,  1.32it/s]Loading train:  76%|███████▌  | 217/285 [03:11<00:51,  1.31it/s]Loading train:  76%|███████▋  | 218/285 [03:12<00:52,  1.28it/s]Loading train:  77%|███████▋  | 219/285 [03:13<00:50,  1.30it/s]Loading train:  77%|███████▋  | 220/285 [03:14<00:49,  1.30it/s]Loading train:  78%|███████▊  | 221/285 [03:15<00:48,  1.32it/s]Loading train:  78%|███████▊  | 222/285 [03:15<00:46,  1.35it/s]Loading train:  78%|███████▊  | 223/285 [03:16<00:45,  1.37it/s]Loading train:  79%|███████▊  | 224/285 [03:17<00:44,  1.39it/s]Loading train:  79%|███████▉  | 225/285 [03:17<00:42,  1.41it/s]Loading train:  79%|███████▉  | 226/285 [03:18<00:43,  1.35it/s]Loading train:  80%|███████▉  | 227/285 [03:19<00:41,  1.41it/s]Loading train:  80%|████████  | 228/285 [03:19<00:40,  1.42it/s]Loading train:  80%|████████  | 229/285 [03:20<00:38,  1.46it/s]Loading train:  81%|████████  | 230/285 [03:21<00:38,  1.42it/s]Loading train:  81%|████████  | 231/285 [03:22<00:38,  1.40it/s]Loading train:  81%|████████▏ | 232/285 [03:23<00:41,  1.27it/s]Loading train:  82%|████████▏ | 233/285 [03:24<00:45,  1.15it/s]Loading train:  82%|████████▏ | 234/285 [03:25<00:45,  1.13it/s]Loading train:  82%|████████▏ | 235/285 [03:25<00:44,  1.13it/s]Loading train:  83%|████████▎ | 236/285 [03:26<00:45,  1.08it/s]Loading train:  83%|████████▎ | 237/285 [03:27<00:44,  1.07it/s]Loading train:  84%|████████▎ | 238/285 [03:28<00:44,  1.05it/s]Loading train:  84%|████████▍ | 239/285 [03:29<00:44,  1.04it/s]Loading train:  84%|████████▍ | 240/285 [03:30<00:42,  1.05it/s]Loading train:  85%|████████▍ | 241/285 [03:31<00:41,  1.06it/s]Loading train:  85%|████████▍ | 242/285 [03:32<00:40,  1.06it/s]Loading train:  85%|████████▌ | 243/285 [03:33<00:40,  1.05it/s]Loading train:  86%|████████▌ | 244/285 [03:34<00:39,  1.03it/s]Loading train:  86%|████████▌ | 245/285 [03:35<00:38,  1.03it/s]Loading train:  86%|████████▋ | 246/285 [03:36<00:36,  1.07it/s]Loading train:  87%|████████▋ | 247/285 [03:37<00:35,  1.08it/s]Loading train:  87%|████████▋ | 248/285 [03:38<00:37,  1.01s/it]Loading train:  87%|████████▋ | 249/285 [03:39<00:35,  1.01it/s]Loading train:  88%|████████▊ | 250/285 [03:40<00:32,  1.07it/s]Loading train:  88%|████████▊ | 251/285 [03:41<00:29,  1.15it/s]Loading train:  88%|████████▊ | 252/285 [03:41<00:26,  1.22it/s]Loading train:  89%|████████▉ | 253/285 [03:42<00:25,  1.28it/s]Loading train:  89%|████████▉ | 254/285 [03:43<00:23,  1.30it/s]Loading train:  89%|████████▉ | 255/285 [03:43<00:22,  1.34it/s]Loading train:  90%|████████▉ | 256/285 [03:44<00:21,  1.35it/s]Loading train:  90%|█████████ | 257/285 [03:45<00:20,  1.38it/s]Loading train:  91%|█████████ | 258/285 [03:46<00:20,  1.35it/s]Loading train:  91%|█████████ | 259/285 [03:46<00:18,  1.38it/s]Loading train:  91%|█████████ | 260/285 [03:47<00:17,  1.39it/s]Loading train:  92%|█████████▏| 261/285 [03:48<00:16,  1.43it/s]Loading train:  92%|█████████▏| 262/285 [03:48<00:16,  1.41it/s]Loading train:  92%|█████████▏| 263/285 [03:49<00:15,  1.39it/s]Loading train:  93%|█████████▎| 264/285 [03:50<00:15,  1.37it/s]Loading train:  93%|█████████▎| 265/285 [03:51<00:14,  1.36it/s]Loading train:  93%|█████████▎| 266/285 [03:51<00:13,  1.40it/s]Loading train:  94%|█████████▎| 267/285 [03:52<00:13,  1.35it/s]Loading train:  94%|█████████▍| 268/285 [03:53<00:14,  1.20it/s]Loading train:  94%|█████████▍| 269/285 [03:54<00:13,  1.20it/s]Loading train:  95%|█████████▍| 270/285 [03:55<00:13,  1.15it/s]Loading train:  95%|█████████▌| 271/285 [03:56<00:12,  1.13it/s]Loading train:  95%|█████████▌| 272/285 [03:57<00:11,  1.12it/s]Loading train:  96%|█████████▌| 273/285 [03:58<00:10,  1.13it/s]Loading train:  96%|█████████▌| 274/285 [03:59<00:10,  1.10it/s]Loading train:  96%|█████████▋| 275/285 [04:00<00:09,  1.06it/s]Loading train:  97%|█████████▋| 276/285 [04:01<00:08,  1.06it/s]Loading train:  97%|█████████▋| 277/285 [04:01<00:07,  1.07it/s]Loading train:  98%|█████████▊| 278/285 [04:02<00:06,  1.10it/s]Loading train:  98%|█████████▊| 279/285 [04:03<00:05,  1.08it/s]Loading train:  98%|█████████▊| 280/285 [04:04<00:04,  1.08it/s]Loading train:  99%|█████████▊| 281/285 [04:05<00:03,  1.09it/s]Loading train:  99%|█████████▉| 282/285 [04:06<00:02,  1.07it/s]Loading train:  99%|█████████▉| 283/285 [04:07<00:01,  1.07it/s]Loading train: 100%|█████████▉| 284/285 [04:08<00:00,  1.09it/s]Loading train: 100%|██████████| 285/285 [04:09<00:00,  1.09it/s]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 3/285 [00:00<00:10, 26.52it/s]concatenating: train:   8%|▊         | 23/285 [00:00<00:07, 35.82it/s]concatenating: train:  19%|█▉        | 55/285 [00:00<00:04, 48.81it/s]concatenating: train:  31%|███       | 87/285 [00:00<00:03, 65.38it/s]concatenating: train:  41%|████      | 117/285 [00:00<00:01, 85.42it/s]concatenating: train:  52%|█████▏    | 149/285 [00:00<00:01, 109.34it/s]concatenating: train:  64%|██████▍   | 183/285 [00:00<00:00, 136.92it/s]concatenating: train:  76%|███████▌  | 217/285 [00:00<00:00, 166.68it/s]concatenating: train:  87%|████████▋ | 248/285 [00:00<00:00, 192.61it/s]concatenating: train:  98%|█████████▊| 280/285 [00:01<00:00, 218.65it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 275.49it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.16s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.16s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.14s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 58.20it/s]2019-07-05 18:36:07.556756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 18:36:07.556862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 18:36:07.556877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 18:36:07.556885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 18:36:07.557347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:13,  2.81it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:11,  3.29it/s]loading the weights for Unet:  10%|█         | 4/40 [00:01<00:11,  3.04it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:08,  3.88it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:08,  3.47it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:02<00:07,  3.95it/s]loading the weights for Unet:  30%|███       | 12/40 [00:02<00:07,  3.53it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:05,  4.43it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:03<00:06,  3.74it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:03<00:05,  4.11it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:04<00:05,  3.52it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:04<00:04,  4.24it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:04<00:03,  4.53it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:05<00:03,  3.78it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:05<00:02,  4.12it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:05<00:03,  3.57it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:06<00:01,  4.33it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:06<00:01,  4.58it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:06<00:01,  3.78it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:07<00:00,  4.20it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:07<00:00,  3.63it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:07<00:00,  5.20it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12, 12, 80), (None, 13, 13, 80)]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 20)   200         input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 20)   80          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 20)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 20)   0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 20)   3620        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 20)   80          conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 20)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 20)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 20)   3620        dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 20)   80          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 20)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 52, 80, 20)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 20)   3620        dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 52, 80, 20)   80          conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 52, 80, 20)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 52, 80, 20)   3620        activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 52, 80, 20)   80          conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 52, 80, 20)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_16[0][0]              
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 26, 40, 20)   0           max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 26, 40, 40)   7240        dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 26, 40, 40)   160         conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 26, 40, 40)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 26, 40, 40)   14440       activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 26, 40, 40)   160         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 26, 40, 40)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_18[0][0]              
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 13, 20, 40)   0           max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 13, 20, 80)   28880       dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 13, 20, 80)   320         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 13, 20, 80)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 13, 20, 80)   57680       activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 13, 20, 80)   320         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 13, 20, 80)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 13, 20, 80)   0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_13[0][0]                 
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 26, 40, 40)   160         conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 26, 40, 40)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 26, 40, 40)   14440       activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 26, 40, 40)   160         conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 26, 40, 40)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 26, 40, 40)   0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_14[0][0]                 
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_3[0][0]         
                                                                 activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 52, 80, 20)   80          conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 52, 80, 20)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 52, 80, 20)   3620        activation_23[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 52, 80, 20)   80          conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 52, 80, 20)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 52, 80, 20)   0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 52, 80, 13)   273         dropout_15[0][0]                 
==================================================================================================
Total params: 195,213
Trainable params: 51,613
Non-trainable params: 143,600
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 18s - loss: 194.3664 - acc: 0.2378 - mDice: 0.0157 - val_loss: 47.6683 - val_acc: 0.9047 - val_mDice: 0.0126

Epoch 00001: val_mDice improved from -inf to 0.01263, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 11s - loss: 42.8014 - acc: 0.7703 - mDice: 0.0137 - val_loss: 14.9336 - val_acc: 0.9047 - val_mDice: 0.0110

Epoch 00002: val_mDice did not improve from 0.01263
Epoch 3/300
 - 10s - loss: 20.9868 - acc: 0.8525 - mDice: 0.0149 - val_loss: 9.0751 - val_acc: 0.9047 - val_mDice: 0.0131

Epoch 00003: val_mDice improved from 0.01263 to 0.01315, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 10s - loss: 14.4623 - acc: 0.8635 - mDice: 0.0172 - val_loss: 7.4553 - val_acc: 0.9047 - val_mDice: 0.0171

Epoch 00004: val_mDice improved from 0.01315 to 0.01714, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 11s - loss: 11.4892 - acc: 0.8665 - mDice: 0.0215 - val_loss: 6.6640 - val_acc: 0.9047 - val_mDice: 0.0207

Epoch 00005: val_mDice improved from 0.01714 to 0.02068, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 10s - loss: 9.7885 - acc: 0.8677 - mDice: 0.0276 - val_loss: 6.0231 - val_acc: 0.9047 - val_mDice: 0.0286

Epoch 00006: val_mDice improved from 0.02068 to 0.02855, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 10s - loss: 8.5985 - acc: 0.8680 - mDice: 0.0371 - val_loss: 5.5870 - val_acc: 0.9047 - val_mDice: 0.0389

Epoch 00007: val_mDice improved from 0.02855 to 0.03893, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 11s - loss: 7.6957 - acc: 0.8682 - mDice: 0.0485 - val_loss: 5.1475 - val_acc: 0.9047 - val_mDice: 0.0487

Epoch 00008: val_mDice improved from 0.03893 to 0.04869, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 10s - loss: 7.0496 - acc: 0.8685 - mDice: 0.0576 - val_loss: 4.8466 - val_acc: 0.9047 - val_mDice: 0.0611

Epoch 00009: val_mDice improved from 0.04869 to 0.06113, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 10s - loss: 6.4827 - acc: 0.8691 - mDice: 0.0697 - val_loss: 4.5796 - val_acc: 0.9047 - val_mDice: 0.0704

Epoch 00010: val_mDice improved from 0.06113 to 0.07039, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 10s - loss: 6.0274 - acc: 0.8699 - mDice: 0.0811 - val_loss: 4.2567 - val_acc: 0.9047 - val_mDice: 0.0967

Epoch 00011: val_mDice improved from 0.07039 to 0.09666, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 10s - loss: 5.6358 - acc: 0.8712 - mDice: 0.0962 - val_loss: 4.0458 - val_acc: 0.9048 - val_mDice: 0.1141

Epoch 00012: val_mDice improved from 0.09666 to 0.11411, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 11s - loss: 5.3238 - acc: 0.8728 - mDice: 0.1100 - val_loss: 3.9407 - val_acc: 0.9041 - val_mDice: 0.1221

Epoch 00013: val_mDice improved from 0.11411 to 0.12215, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 10s - loss: 5.0541 - acc: 0.8743 - mDice: 0.1230 - val_loss: 3.7772 - val_acc: 0.9040 - val_mDice: 0.1433

Epoch 00014: val_mDice improved from 0.12215 to 0.14330, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 10s - loss: 4.8142 - acc: 0.8757 - mDice: 0.1369 - val_loss: 3.6964 - val_acc: 0.9039 - val_mDice: 0.1569

Epoch 00015: val_mDice improved from 0.14330 to 0.15686, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 11s - loss: 4.6013 - acc: 0.8766 - mDice: 0.1499 - val_loss: 3.5837 - val_acc: 0.9065 - val_mDice: 0.1690

Epoch 00016: val_mDice improved from 0.15686 to 0.16895, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 10s - loss: 4.3955 - acc: 0.8774 - mDice: 0.1632 - val_loss: 3.4203 - val_acc: 0.9064 - val_mDice: 0.1950

Epoch 00017: val_mDice improved from 0.16895 to 0.19502, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 10s - loss: 4.2131 - acc: 0.8782 - mDice: 0.1775 - val_loss: 3.4382 - val_acc: 0.9075 - val_mDice: 0.1964

Epoch 00018: val_mDice improved from 0.19502 to 0.19639, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 10s - loss: 4.0488 - acc: 0.8789 - mDice: 0.1908 - val_loss: 3.3222 - val_acc: 0.9085 - val_mDice: 0.2298

Epoch 00019: val_mDice improved from 0.19639 to 0.22976, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 11s - loss: 3.9125 - acc: 0.8795 - mDice: 0.2020 - val_loss: 3.4835 - val_acc: 0.9075 - val_mDice: 0.2303

Epoch 00020: val_mDice improved from 0.22976 to 0.23031, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 10s - loss: 3.7792 - acc: 0.8806 - mDice: 0.2144 - val_loss: 3.2933 - val_acc: 0.9099 - val_mDice: 0.2421

Epoch 00021: val_mDice improved from 0.23031 to 0.24211, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 10s - loss: 3.6620 - acc: 0.8820 - mDice: 0.2256 - val_loss: 3.0867 - val_acc: 0.9120 - val_mDice: 0.2686

Epoch 00022: val_mDice improved from 0.24211 to 0.26864, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 10s - loss: 3.5599 - acc: 0.8836 - mDice: 0.2366 - val_loss: 3.2201 - val_acc: 0.9104 - val_mDice: 0.2658

Epoch 00023: val_mDice did not improve from 0.26864
Epoch 24/300
 - 10s - loss: 3.4575 - acc: 0.8854 - mDice: 0.2471 - val_loss: 3.0936 - val_acc: 0.9147 - val_mDice: 0.2762

Epoch 00024: val_mDice improved from 0.26864 to 0.27619, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 10s - loss: 3.3704 - acc: 0.8874 - mDice: 0.2568 - val_loss: 3.2072 - val_acc: 0.9185 - val_mDice: 0.2738

Epoch 00025: val_mDice did not improve from 0.27619
Epoch 26/300
 - 10s - loss: 3.2822 - acc: 0.8893 - mDice: 0.2679 - val_loss: 3.2227 - val_acc: 0.9157 - val_mDice: 0.2779

Epoch 00026: val_mDice improved from 0.27619 to 0.27794, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 10s - loss: 3.2078 - acc: 0.8908 - mDice: 0.2771 - val_loss: 3.0064 - val_acc: 0.9217 - val_mDice: 0.2966

Epoch 00027: val_mDice improved from 0.27794 to 0.29657, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 10s - loss: 3.1415 - acc: 0.8924 - mDice: 0.2850 - val_loss: 3.1314 - val_acc: 0.9222 - val_mDice: 0.2940

Epoch 00028: val_mDice did not improve from 0.29657
Epoch 29/300
 - 10s - loss: 3.0817 - acc: 0.8938 - mDice: 0.2936 - val_loss: 2.9595 - val_acc: 0.9251 - val_mDice: 0.3154

Epoch 00029: val_mDice improved from 0.29657 to 0.31540, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 11s - loss: 3.0225 - acc: 0.8955 - mDice: 0.3020 - val_loss: 2.9825 - val_acc: 0.9240 - val_mDice: 0.3196

Epoch 00030: val_mDice improved from 0.31540 to 0.31964, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 10s - loss: 2.9688 - acc: 0.8965 - mDice: 0.3092 - val_loss: 3.0084 - val_acc: 0.9260 - val_mDice: 0.3228

Epoch 00031: val_mDice improved from 0.31964 to 0.32277, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 10s - loss: 2.9083 - acc: 0.8980 - mDice: 0.3181 - val_loss: 3.0547 - val_acc: 0.9266 - val_mDice: 0.3253

Epoch 00032: val_mDice improved from 0.32277 to 0.32528, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 10s - loss: 2.8593 - acc: 0.8994 - mDice: 0.3261 - val_loss: 3.0114 - val_acc: 0.9261 - val_mDice: 0.3339

Epoch 00033: val_mDice improved from 0.32528 to 0.33394, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 10s - loss: 2.8126 - acc: 0.9006 - mDice: 0.3326 - val_loss: 3.0371 - val_acc: 0.9281 - val_mDice: 0.3360

Epoch 00034: val_mDice improved from 0.33394 to 0.33604, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 11s - loss: 2.7625 - acc: 0.9018 - mDice: 0.3416 - val_loss: 3.0121 - val_acc: 0.9298 - val_mDice: 0.3404

Epoch 00035: val_mDice improved from 0.33604 to 0.34036, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 36/300
 - 10s - loss: 2.7403 - acc: 0.9022 - mDice: 0.3459 - val_loss: 2.9268 - val_acc: 0.9299 - val_mDice: 0.3496

Epoch 00036: val_mDice improved from 0.34036 to 0.34959, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 10s - loss: 2.6845 - acc: 0.9037 - mDice: 0.3544 - val_loss: 2.9566 - val_acc: 0.9307 - val_mDice: 0.3576

Epoch 00037: val_mDice improved from 0.34959 to 0.35760, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 10s - loss: 2.6612 - acc: 0.9043 - mDice: 0.3587 - val_loss: 3.2038 - val_acc: 0.9297 - val_mDice: 0.3427

Epoch 00038: val_mDice did not improve from 0.35760
Epoch 39/300
 - 10s - loss: 2.6259 - acc: 0.9051 - mDice: 0.3645 - val_loss: 3.0560 - val_acc: 0.9277 - val_mDice: 0.3413

Epoch 00039: val_mDice did not improve from 0.35760
Epoch 40/300
 - 10s - loss: 2.5931 - acc: 0.9061 - mDice: 0.3712 - val_loss: 2.9653 - val_acc: 0.9329 - val_mDice: 0.3686

Epoch 00040: val_mDice improved from 0.35760 to 0.36861, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 10s - loss: 2.5748 - acc: 0.9067 - mDice: 0.3742 - val_loss: 3.0147 - val_acc: 0.9336 - val_mDice: 0.3671

Epoch 00041: val_mDice did not improve from 0.36861
Epoch 42/300
 - 10s - loss: 2.5338 - acc: 0.9076 - mDice: 0.3819 - val_loss: 2.8689 - val_acc: 0.9334 - val_mDice: 0.3781

Epoch 00042: val_mDice improved from 0.36861 to 0.37814, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 10s - loss: 2.5113 - acc: 0.9078 - mDice: 0.3857 - val_loss: 2.9568 - val_acc: 0.9323 - val_mDice: 0.3710

Epoch 00043: val_mDice did not improve from 0.37814
Epoch 44/300
 - 10s - loss: 2.4858 - acc: 0.9085 - mDice: 0.3904 - val_loss: 2.9214 - val_acc: 0.9319 - val_mDice: 0.3706

Epoch 00044: val_mDice did not improve from 0.37814
Epoch 45/300
 - 10s - loss: 2.4650 - acc: 0.9090 - mDice: 0.3948 - val_loss: 3.0267 - val_acc: 0.9306 - val_mDice: 0.3590

Epoch 00045: val_mDice did not improve from 0.37814
Epoch 46/300
 - 10s - loss: 2.4500 - acc: 0.9094 - mDice: 0.3974 - val_loss: 2.9507 - val_acc: 0.9340 - val_mDice: 0.3778

Epoch 00046: val_mDice did not improve from 0.37814
Epoch 47/300
 - 11s - loss: 2.4202 - acc: 0.9099 - mDice: 0.4028 - val_loss: 2.9064 - val_acc: 0.9317 - val_mDice: 0.3716

Epoch 00047: val_mDice did not improve from 0.37814
Epoch 48/300
 - 10s - loss: 2.3902 - acc: 0.9106 - mDice: 0.4089 - val_loss: 2.9771 - val_acc: 0.9339 - val_mDice: 0.3842

Epoch 00048: val_mDice improved from 0.37814 to 0.38422, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 10s - loss: 2.3691 - acc: 0.9112 - mDice: 0.4127 - val_loss: 2.8519 - val_acc: 0.9347 - val_mDice: 0.3909

Epoch 00049: val_mDice improved from 0.38422 to 0.39090, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 10s - loss: 2.3558 - acc: 0.9114 - mDice: 0.4153 - val_loss: 2.8694 - val_acc: 0.9349 - val_mDice: 0.3931

Epoch 00050: val_mDice improved from 0.39090 to 0.39311, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 10s - loss: 2.3398 - acc: 0.9118 - mDice: 0.4183 - val_loss: 2.7920 - val_acc: 0.9344 - val_mDice: 0.3977

Epoch 00051: val_mDice improved from 0.39311 to 0.39770, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 10s - loss: 2.3237 - acc: 0.9118 - mDice: 0.4206 - val_loss: 2.9494 - val_acc: 0.9316 - val_mDice: 0.3780

Epoch 00052: val_mDice did not improve from 0.39770
Epoch 53/300
 - 10s - loss: 2.3036 - acc: 0.9124 - mDice: 0.4255 - val_loss: 2.9759 - val_acc: 0.9336 - val_mDice: 0.3851

Epoch 00053: val_mDice did not improve from 0.39770
Epoch 54/300
 - 11s - loss: 2.2846 - acc: 0.9128 - mDice: 0.4293 - val_loss: 2.8990 - val_acc: 0.9324 - val_mDice: 0.3952

Epoch 00054: val_mDice did not improve from 0.39770
Epoch 55/300
 - 10s - loss: 2.2684 - acc: 0.9132 - mDice: 0.4315 - val_loss: 2.9516 - val_acc: 0.9345 - val_mDice: 0.3945

Epoch 00055: val_mDice did not improve from 0.39770
Epoch 56/300
 - 10s - loss: 2.2534 - acc: 0.9137 - mDice: 0.4356 - val_loss: 2.9340 - val_acc: 0.9344 - val_mDice: 0.3985

Epoch 00056: val_mDice improved from 0.39770 to 0.39848, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 57/300
 - 10s - loss: 2.2292 - acc: 0.9142 - mDice: 0.4392 - val_loss: 3.0639 - val_acc: 0.9361 - val_mDice: 0.4015

Epoch 00057: val_mDice improved from 0.39848 to 0.40151, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 58/300
 - 10s - loss: 2.2236 - acc: 0.9146 - mDice: 0.4414 - val_loss: 3.1328 - val_acc: 0.9353 - val_mDice: 0.3977

Epoch 00058: val_mDice did not improve from 0.40151
Epoch 59/300
 - 10s - loss: 2.2090 - acc: 0.9152 - mDice: 0.4448 - val_loss: 2.8306 - val_acc: 0.9350 - val_mDice: 0.4080

Epoch 00059: val_mDice improved from 0.40151 to 0.40797, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 60/300
 - 10s - loss: 2.1940 - acc: 0.9153 - mDice: 0.4473 - val_loss: 2.9710 - val_acc: 0.9336 - val_mDice: 0.4025

Epoch 00060: val_mDice did not improve from 0.40797
Epoch 61/300
 - 11s - loss: 2.1867 - acc: 0.9155 - mDice: 0.4487 - val_loss: 3.0652 - val_acc: 0.9328 - val_mDice: 0.3987

Epoch 00061: val_mDice did not improve from 0.40797
Epoch 62/300
 - 10s - loss: 2.1593 - acc: 0.9164 - mDice: 0.4547 - val_loss: 2.9463 - val_acc: 0.9349 - val_mDice: 0.4021

Epoch 00062: val_mDice did not improve from 0.40797
Epoch 63/300
 - 10s - loss: 2.1583 - acc: 0.9167 - mDice: 0.4555 - val_loss: 3.0310 - val_acc: 0.9341 - val_mDice: 0.4026

Epoch 00063: val_mDice did not improve from 0.40797
Epoch 64/300
 - 10s - loss: 2.1412 - acc: 0.9169 - mDice: 0.4584 - val_loss: 3.0243 - val_acc: 0.9345 - val_mDice: 0.4081

Epoch 00064: val_mDice improved from 0.40797 to 0.40815, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 65/300
 - 10s - loss: 2.1344 - acc: 0.9171 - mDice: 0.4603 - val_loss: 3.0905 - val_acc: 0.9357 - val_mDice: 0.4075

Epoch 00065: val_mDice did not improve from 0.40815
Epoch 66/300
 - 10s - loss: 2.1152 - acc: 0.9178 - mDice: 0.4642 - val_loss: 2.9299 - val_acc: 0.9353 - val_mDice: 0.4110

Epoch 00066: val_mDice improved from 0.40815 to 0.41102, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 67/300
 - 10s - loss: 2.1113 - acc: 0.9178 - mDice: 0.4647 - val_loss: 3.0081 - val_acc: 0.9355 - val_mDice: 0.4103

Epoch 00067: val_mDice did not improve from 0.41102
Epoch 68/300
 - 10s - loss: 2.1011 - acc: 0.9181 - mDice: 0.4672 - val_loss: 2.9853 - val_acc: 0.9355 - val_mDice: 0.4142

Epoch 00068: val_mDice improved from 0.41102 to 0.41421, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 69/300
 - 10s - loss: 2.0868 - acc: 0.9184 - mDice: 0.4699 - val_loss: 2.9245 - val_acc: 0.9365 - val_mDice: 0.4182

Epoch 00069: val_mDice improved from 0.41421 to 0.41820, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 70/300
 - 10s - loss: 2.0739 - acc: 0.9185 - mDice: 0.4731 - val_loss: 3.1358 - val_acc: 0.9367 - val_mDice: 0.4138

Epoch 00070: val_mDice did not improve from 0.41820
Epoch 71/300
 - 10s - loss: 2.0686 - acc: 0.9184 - mDice: 0.4744 - val_loss: 3.0423 - val_acc: 0.9373 - val_mDice: 0.4176

Epoch 00071: val_mDice did not improve from 0.41820
Epoch 72/300
 - 10s - loss: 2.0488 - acc: 0.9188 - mDice: 0.4781 - val_loss: 2.9801 - val_acc: 0.9372 - val_mDice: 0.4205

Epoch 00072: val_mDice improved from 0.41820 to 0.42049, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 73/300
 - 10s - loss: 2.0446 - acc: 0.9189 - mDice: 0.4789 - val_loss: 3.3099 - val_acc: 0.9360 - val_mDice: 0.3985

Epoch 00073: val_mDice did not improve from 0.42049
Epoch 74/300
 - 10s - loss: 2.0378 - acc: 0.9191 - mDice: 0.4807 - val_loss: 3.1496 - val_acc: 0.9354 - val_mDice: 0.4076

Epoch 00074: val_mDice did not improve from 0.42049
Epoch 75/300
 - 10s - loss: 2.0296 - acc: 0.9193 - mDice: 0.4825 - val_loss: 3.1520 - val_acc: 0.9330 - val_mDice: 0.4020

Epoch 00075: val_mDice did not improve from 0.42049
Epoch 76/300
 - 10s - loss: 2.0201 - acc: 0.9198 - mDice: 0.4851 - val_loss: 3.1356 - val_acc: 0.9364 - val_mDice: 0.4135

Epoch 00076: val_mDice did not improve from 0.42049
Epoch 77/300
 - 11s - loss: 2.0105 - acc: 0.9198 - mDice: 0.4867 - val_loss: 3.2556 - val_acc: 0.9361 - val_mDice: 0.4072

Epoch 00077: val_mDice did not improve from 0.42049
Epoch 78/300
 - 10s - loss: 2.0007 - acc: 0.9200 - mDice: 0.4880 - val_loss: 3.0590 - val_acc: 0.9372 - val_mDice: 0.4193

Epoch 00078: val_mDice did not improve from 0.42049
Epoch 79/300
 - 10s - loss: 1.9905 - acc: 0.9203 - mDice: 0.4906 - val_loss: 3.0583 - val_acc: 0.9372 - val_mDice: 0.4198

Epoch 00079: val_mDice did not improve from 0.42049
Epoch 80/300
 - 10s - loss: 1.9808 - acc: 0.9206 - mDice: 0.4933 - val_loss: 3.1763 - val_acc: 0.9355 - val_mDice: 0.4144

Epoch 00080: val_mDice did not improve from 0.42049
Epoch 81/300
 - 11s - loss: 1.9728 - acc: 0.9206 - mDice: 0.4941 - val_loss: 3.2293 - val_acc: 0.9353 - val_mDice: 0.4105

Epoch 00081: val_mDice did not improve from 0.42049
Epoch 82/300
 - 11s - loss: 1.9744 - acc: 0.9205 - mDice: 0.4947 - val_loss: 3.0565 - val_acc: 0.9374 - val_mDice: 0.4184

Epoch 00082: val_mDice did not improve from 0.42049
Epoch 83/300
 - 11s - loss: 1.9675 - acc: 0.9208 - mDice: 0.4965 - val_loss: 3.0930 - val_acc: 0.9356 - val_mDice: 0.4126

Epoch 00083: val_mDice did not improve from 0.42049
Epoch 84/300
 - 11s - loss: 1.9542 - acc: 0.9209 - mDice: 0.4990 - val_loss: 3.1936 - val_acc: 0.9358 - val_mDice: 0.4071

Epoch 00084: val_mDice did not improve from 0.42049
Epoch 85/300
 - 12s - loss: 1.9612 - acc: 0.9208 - mDice: 0.4975 - val_loss: 2.9469 - val_acc: 0.9398 - val_mDice: 0.4339

Epoch 00085: val_mDice improved from 0.42049 to 0.43395, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 86/300
 - 11s - loss: 1.9412 - acc: 0.9214 - mDice: 0.5020 - val_loss: 3.0971 - val_acc: 0.9397 - val_mDice: 0.4250

Epoch 00086: val_mDice did not improve from 0.43395
Epoch 87/300
 - 11s - loss: 1.9316 - acc: 0.9216 - mDice: 0.5035 - val_loss: 3.2837 - val_acc: 0.9369 - val_mDice: 0.4164

Epoch 00087: val_mDice did not improve from 0.43395
Epoch 88/300
 - 10s - loss: 1.9201 - acc: 0.9218 - mDice: 0.5061 - val_loss: 3.1475 - val_acc: 0.9381 - val_mDice: 0.4242

Epoch 00088: val_mDice did not improve from 0.43395
Epoch 89/300
 - 10s - loss: 1.9167 - acc: 0.9220 - mDice: 0.5068 - val_loss: 3.1240 - val_acc: 0.9378 - val_mDice: 0.4229

Epoch 00089: val_mDice did not improve from 0.43395
Epoch 90/300
 - 11s - loss: 1.9099 - acc: 0.9222 - mDice: 0.5080 - val_loss: 3.0530 - val_acc: 0.9376 - val_mDice: 0.4316

Epoch 00090: val_mDice did not improve from 0.43395
Epoch 91/300
 - 11s - loss: 1.9120 - acc: 0.9219 - mDice: 0.5088 - val_loss: 3.4177 - val_acc: 0.9375 - val_mDice: 0.4102

Epoch 00091: val_mDice did not improve from 0.43395
Epoch 92/300
 - 10s - loss: 1.9111 - acc: 0.9220 - mDice: 0.5088 - val_loss: 3.1893 - val_acc: 0.9385 - val_mDice: 0.4244

Epoch 00092: val_mDice did not improve from 0.43395
Epoch 93/300
 - 10s - loss: 1.8904 - acc: 0.9227 - mDice: 0.5134 - val_loss: 3.2218 - val_acc: 0.9379 - val_mDice: 0.4215

Epoch 00093: val_mDice did not improve from 0.43395
Epoch 94/300
 - 10s - loss: 1.8921 - acc: 0.9226 - mDice: 0.5128 - val_loss: 3.1141 - val_acc: 0.9361 - val_mDice: 0.4272

Epoch 00094: val_mDice did not improve from 0.43395
Epoch 95/300
 - 11s - loss: 1.8829 - acc: 0.9229 - mDice: 0.5150 - val_loss: 3.1528 - val_acc: 0.9398 - val_mDice: 0.4271

Epoch 00095: val_mDice did not improve from 0.43395
Epoch 96/300
 - 10s - loss: 1.8838 - acc: 0.9228 - mDice: 0.5146 - val_loss: 3.1980 - val_acc: 0.9376 - val_mDice: 0.4183

Epoch 00096: val_mDice did not improve from 0.43395
Epoch 97/300
 - 10s - loss: 1.8724 - acc: 0.9228 - mDice: 0.5169 - val_loss: 3.0243 - val_acc: 0.9396 - val_mDice: 0.4352

Epoch 00097: val_mDice improved from 0.43395 to 0.43516, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 98/300
 - 10s - loss: 1.8645 - acc: 0.9232 - mDice: 0.5188 - val_loss: 3.1647 - val_acc: 0.9385 - val_mDice: 0.4303

Epoch 00098: val_mDice did not improve from 0.43516
Epoch 99/300
 - 10s - loss: 1.8589 - acc: 0.9234 - mDice: 0.5213 - val_loss: 3.1280 - val_acc: 0.9403 - val_mDice: 0.4339

Epoch 00099: val_mDice did not improve from 0.43516
Epoch 100/300
 - 11s - loss: 1.8519 - acc: 0.9235 - mDice: 0.5215 - val_loss: 3.3218 - val_acc: 0.9392 - val_mDice: 0.4215

Epoch 00100: val_mDice did not improve from 0.43516
Epoch 101/300
 - 10s - loss: 1.8413 - acc: 0.9237 - mDice: 0.5246 - val_loss: 3.1857 - val_acc: 0.9404 - val_mDice: 0.4289

Epoch 00101: val_mDice did not improve from 0.43516
Epoch 102/300
 - 10s - loss: 1.8472 - acc: 0.9237 - mDice: 0.5232 - val_loss: 3.3157 - val_acc: 0.9399 - val_mDice: 0.4260

Epoch 00102: val_mDice did not improve from 0.43516
Epoch 103/300
 - 10s - loss: 1.8357 - acc: 0.9237 - mDice: 0.5255 - val_loss: 3.4400 - val_acc: 0.9377 - val_mDice: 0.4212

Epoch 00103: val_mDice did not improve from 0.43516
Epoch 104/300
 - 10s - loss: 1.8328 - acc: 0.9239 - mDice: 0.5264 - val_loss: 3.1457 - val_acc: 0.9395 - val_mDice: 0.4330

Epoch 00104: val_mDice did not improve from 0.43516
Epoch 105/300
 - 10s - loss: 1.8209 - acc: 0.9243 - mDice: 0.5288 - val_loss: 3.2265 - val_acc: 0.9360 - val_mDice: 0.4258

Epoch 00105: val_mDice did not improve from 0.43516
Epoch 106/300
 - 10s - loss: 1.8262 - acc: 0.9244 - mDice: 0.5279 - val_loss: 3.0011 - val_acc: 0.9385 - val_mDice: 0.4320

Epoch 00106: val_mDice did not improve from 0.43516
Epoch 107/300
 - 10s - loss: 1.8162 - acc: 0.9245 - mDice: 0.5298 - val_loss: 3.1559 - val_acc: 0.9404 - val_mDice: 0.4348

Epoch 00107: val_mDice did not improve from 0.43516
Epoch 108/300
 - 10s - loss: 1.8127 - acc: 0.9246 - mDice: 0.5312 - val_loss: 3.2242 - val_acc: 0.9376 - val_mDice: 0.4140

Epoch 00108: val_mDice did not improve from 0.43516
Epoch 109/300
 - 10s - loss: 1.8112 - acc: 0.9245 - mDice: 0.5319 - val_loss: 3.1541 - val_acc: 0.9411 - val_mDice: 0.4355

Epoch 00109: val_mDice improved from 0.43516 to 0.43550, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 110/300
 - 10s - loss: 1.8053 - acc: 0.9248 - mDice: 0.5331 - val_loss: 3.3926 - val_acc: 0.9410 - val_mDice: 0.4283

Epoch 00110: val_mDice did not improve from 0.43550
Epoch 111/300
 - 10s - loss: 1.8013 - acc: 0.9249 - mDice: 0.5344 - val_loss: 3.3692 - val_acc: 0.9410 - val_mDice: 0.4301

Epoch 00111: val_mDice did not improve from 0.43550
Epoch 112/300
 - 10s - loss: 1.7952 - acc: 0.9251 - mDice: 0.5352 - val_loss: 3.1671 - val_acc: 0.9368 - val_mDice: 0.4269

Epoch 00112: val_mDice did not improve from 0.43550
Epoch 113/300
 - 11s - loss: 1.7883 - acc: 0.9252 - mDice: 0.5370 - val_loss: 3.0535 - val_acc: 0.9375 - val_mDice: 0.4290

Epoch 00113: val_mDice did not improve from 0.43550
Epoch 114/300
 - 10s - loss: 1.7772 - acc: 0.9255 - mDice: 0.5390 - val_loss: 3.1844 - val_acc: 0.9392 - val_mDice: 0.4299

Epoch 00114: val_mDice did not improve from 0.43550
Epoch 115/300
 - 10s - loss: 1.7859 - acc: 0.9252 - mDice: 0.5375 - val_loss: 3.2022 - val_acc: 0.9401 - val_mDice: 0.4304

Epoch 00115: val_mDice did not improve from 0.43550
Epoch 116/300
 - 11s - loss: 1.7723 - acc: 0.9256 - mDice: 0.5400 - val_loss: 3.3255 - val_acc: 0.9403 - val_mDice: 0.4360

Epoch 00116: val_mDice improved from 0.43550 to 0.43596, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 117/300
 - 10s - loss: 1.7716 - acc: 0.9257 - mDice: 0.5405 - val_loss: 3.2491 - val_acc: 0.9395 - val_mDice: 0.4340

Epoch 00117: val_mDice did not improve from 0.43596
Epoch 118/300
 - 10s - loss: 1.7702 - acc: 0.9258 - mDice: 0.5409 - val_loss: 3.2363 - val_acc: 0.9390 - val_mDice: 0.4313

Epoch 00118: val_mDice did not improve from 0.43596
Epoch 119/300
 - 10s - loss: 1.7616 - acc: 0.9259 - mDice: 0.5425 - val_loss: 3.3035 - val_acc: 0.9411 - val_mDice: 0.4375

Epoch 00119: val_mDice improved from 0.43596 to 0.43750, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 120/300
 - 11s - loss: 1.7667 - acc: 0.9260 - mDice: 0.5421 - val_loss: 3.3238 - val_acc: 0.9417 - val_mDice: 0.4374

Epoch 00120: val_mDice did not improve from 0.43750
Epoch 121/300
 - 10s - loss: 1.7540 - acc: 0.9263 - mDice: 0.5445 - val_loss: 3.4288 - val_acc: 0.9403 - val_mDice: 0.4293

Epoch 00121: val_mDice did not improve from 0.43750
Epoch 122/300
 - 10s - loss: 1.7521 - acc: 0.9262 - mDice: 0.5450 - val_loss: 3.3464 - val_acc: 0.9411 - val_mDice: 0.4320

Epoch 00122: val_mDice did not improve from 0.43750
Epoch 123/300
 - 10s - loss: 1.7463 - acc: 0.9265 - mDice: 0.5468 - val_loss: 3.3390 - val_acc: 0.9367 - val_mDice: 0.4210

Epoch 00123: val_mDice did not improve from 0.43750
Epoch 124/300
 - 10s - loss: 1.7442 - acc: 0.9265 - mDice: 0.5475 - val_loss: 3.2868 - val_acc: 0.9414 - val_mDice: 0.4438

Epoch 00124: val_mDice improved from 0.43750 to 0.44377, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 125/300
 - 10s - loss: 1.7352 - acc: 0.9269 - mDice: 0.5491 - val_loss: 3.3499 - val_acc: 0.9408 - val_mDice: 0.4363

Epoch 00125: val_mDice did not improve from 0.44377
Epoch 126/300
 - 10s - loss: 1.7283 - acc: 0.9270 - mDice: 0.5505 - val_loss: 3.0971 - val_acc: 0.9402 - val_mDice: 0.4424

Epoch 00126: val_mDice did not improve from 0.44377
Epoch 127/300
 - 10s - loss: 1.7305 - acc: 0.9271 - mDice: 0.5505 - val_loss: 3.1210 - val_acc: 0.9383 - val_mDice: 0.4405

Epoch 00127: val_mDice did not improve from 0.44377
Epoch 128/300
 - 10s - loss: 1.7289 - acc: 0.9273 - mDice: 0.5509 - val_loss: 3.2973 - val_acc: 0.9415 - val_mDice: 0.4368

Epoch 00128: val_mDice did not improve from 0.44377
Epoch 129/300
 - 11s - loss: 1.7242 - acc: 0.9272 - mDice: 0.5520 - val_loss: 3.1783 - val_acc: 0.9411 - val_mDice: 0.4430

Epoch 00129: val_mDice did not improve from 0.44377
Epoch 130/300
 - 10s - loss: 1.7261 - acc: 0.9272 - mDice: 0.5516 - val_loss: 3.1758 - val_acc: 0.9407 - val_mDice: 0.4392

Epoch 00130: val_mDice did not improve from 0.44377
Epoch 131/300
 - 10s - loss: 1.7201 - acc: 0.9271 - mDice: 0.5529 - val_loss: 3.3088 - val_acc: 0.9426 - val_mDice: 0.4413

Epoch 00131: val_mDice did not improve from 0.44377
Epoch 132/300
 - 11s - loss: 1.7098 - acc: 0.9277 - mDice: 0.5549 - val_loss: 3.3327 - val_acc: 0.9410 - val_mDice: 0.4362

Epoch 00132: val_mDice did not improve from 0.44377
Epoch 133/300
 - 10s - loss: 1.7113 - acc: 0.9279 - mDice: 0.5552 - val_loss: 3.2084 - val_acc: 0.9398 - val_mDice: 0.4374

Epoch 00133: val_mDice did not improve from 0.44377
Epoch 134/300
 - 10s - loss: 1.7060 - acc: 0.9280 - mDice: 0.5564 - val_loss: 3.2268 - val_acc: 0.9396 - val_mDice: 0.4380

Epoch 00134: val_mDice did not improve from 0.44377
Epoch 135/300
 - 10s - loss: 1.6993 - acc: 0.9279 - mDice: 0.5576 - val_loss: 3.3596 - val_acc: 0.9391 - val_mDice: 0.4279

Epoch 00135: val_mDice did not improve from 0.44377
Epoch 136/300
 - 11s - loss: 1.7012 - acc: 0.9280 - mDice: 0.5574 - val_loss: 3.2018 - val_acc: 0.9402 - val_mDice: 0.4330

Epoch 00136: val_mDice did not improve from 0.44377
Epoch 137/300
 - 10s - loss: 1.6990 - acc: 0.9280 - mDice: 0.5576 - val_loss: 3.3840 - val_acc: 0.9410 - val_mDice: 0.4402

Epoch 00137: val_mDice did not improve from 0.44377
Epoch 138/300
 - 10s - loss: 1.6997 - acc: 0.9281 - mDice: 0.5578 - val_loss: 3.0584 - val_acc: 0.9388 - val_mDice: 0.4459

Epoch 00138: val_mDice improved from 0.44377 to 0.44588, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 139/300
 - 10s - loss: 1.6910 - acc: 0.9283 - mDice: 0.5597 - val_loss: 3.2820 - val_acc: 0.9417 - val_mDice: 0.4410

Epoch 00139: val_mDice did not improve from 0.44588
Epoch 140/300
 - 10s - loss: 1.6933 - acc: 0.9284 - mDice: 0.5598 - val_loss: 3.3007 - val_acc: 0.9395 - val_mDice: 0.4398

Epoch 00140: val_mDice did not improve from 0.44588
Epoch 141/300
 - 10s - loss: 1.6921 - acc: 0.9283 - mDice: 0.5602 - val_loss: 3.3142 - val_acc: 0.9414 - val_mDice: 0.4361

Epoch 00141: val_mDice did not improve from 0.44588
Epoch 142/300
 - 11s - loss: 1.6819 - acc: 0.9286 - mDice: 0.5620 - val_loss: 3.2121 - val_acc: 0.9401 - val_mDice: 0.4386

Epoch 00142: val_mDice did not improve from 0.44588
Epoch 143/300
 - 10s - loss: 1.6865 - acc: 0.9284 - mDice: 0.5610 - val_loss: 3.2013 - val_acc: 0.9401 - val_mDice: 0.4376

Epoch 00143: val_mDice did not improve from 0.44588
Epoch 144/300
 - 10s - loss: 1.6809 - acc: 0.9286 - mDice: 0.5624 - val_loss: 3.1970 - val_acc: 0.9401 - val_mDice: 0.4410

Epoch 00144: val_mDice did not improve from 0.44588
Epoch 145/300
 - 10s - loss: 1.6823 - acc: 0.9284 - mDice: 0.5617 - val_loss: 3.2349 - val_acc: 0.9411 - val_mDice: 0.4401

Epoch 00145: val_mDice did not improve from 0.44588
Epoch 146/300
 - 10s - loss: 1.6745 - acc: 0.9289 - mDice: 0.5641 - val_loss: 3.1427 - val_acc: 0.9413 - val_mDice: 0.4459

Epoch 00146: val_mDice improved from 0.44588 to 0.44594, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 147/300
 - 10s - loss: 1.6693 - acc: 0.9289 - mDice: 0.5653 - val_loss: 3.2847 - val_acc: 0.9365 - val_mDice: 0.4389

Epoch 00147: val_mDice did not improve from 0.44594
Epoch 148/300
 - 10s - loss: 1.6718 - acc: 0.9290 - mDice: 0.5647 - val_loss: 3.0787 - val_acc: 0.9398 - val_mDice: 0.4467

Epoch 00148: val_mDice improved from 0.44594 to 0.44671, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 149/300
 - 11s - loss: 1.6694 - acc: 0.9289 - mDice: 0.5651 - val_loss: 3.2612 - val_acc: 0.9399 - val_mDice: 0.4283

Epoch 00149: val_mDice did not improve from 0.44671
Epoch 150/300
 - 10s - loss: 1.6627 - acc: 0.9292 - mDice: 0.5662 - val_loss: 3.3781 - val_acc: 0.9419 - val_mDice: 0.4435

Epoch 00150: val_mDice did not improve from 0.44671
Epoch 151/300
 - 10s - loss: 1.6640 - acc: 0.9292 - mDice: 0.5671 - val_loss: 3.3636 - val_acc: 0.9409 - val_mDice: 0.4389

Epoch 00151: val_mDice did not improve from 0.44671
Epoch 152/300
 - 10s - loss: 1.6685 - acc: 0.9290 - mDice: 0.5654 - val_loss: 3.2951 - val_acc: 0.9423 - val_mDice: 0.4454

Epoch 00152: val_mDice did not improve from 0.44671
Epoch 153/300
 - 10s - loss: 1.6613 - acc: 0.9293 - mDice: 0.5674 - val_loss: 3.2553 - val_acc: 0.9410 - val_mDice: 0.4399

Epoch 00153: val_mDice did not improve from 0.44671
Epoch 154/300
 - 10s - loss: 1.6578 - acc: 0.9295 - mDice: 0.5683 - val_loss: 3.1800 - val_acc: 0.9413 - val_mDice: 0.4489

Epoch 00154: val_mDice improved from 0.44671 to 0.44889, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 155/300
 - 10s - loss: 1.6557 - acc: 0.9295 - mDice: 0.5684 - val_loss: 3.1387 - val_acc: 0.9429 - val_mDice: 0.4510

Epoch 00155: val_mDice improved from 0.44889 to 0.45096, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 156/300
 - 10s - loss: 1.6490 - acc: 0.9297 - mDice: 0.5701 - val_loss: 3.3570 - val_acc: 0.9411 - val_mDice: 0.4380

Epoch 00156: val_mDice did not improve from 0.45096
Epoch 157/300
 - 10s - loss: 1.6515 - acc: 0.9295 - mDice: 0.5704 - val_loss: 3.1591 - val_acc: 0.9392 - val_mDice: 0.4416

Epoch 00157: val_mDice did not improve from 0.45096
Epoch 158/300
 - 10s - loss: 1.6538 - acc: 0.9295 - mDice: 0.5694 - val_loss: 3.3582 - val_acc: 0.9419 - val_mDice: 0.4525

Epoch 00158: val_mDice improved from 0.45096 to 0.45248, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 159/300
 - 10s - loss: 1.6601 - acc: 0.9295 - mDice: 0.5682 - val_loss: 3.2880 - val_acc: 0.9410 - val_mDice: 0.4415

Epoch 00159: val_mDice did not improve from 0.45248
Epoch 160/300
 - 10s - loss: 1.6438 - acc: 0.9299 - mDice: 0.5719 - val_loss: 3.1608 - val_acc: 0.9410 - val_mDice: 0.4484

Epoch 00160: val_mDice did not improve from 0.45248
Epoch 161/300
 - 10s - loss: 1.6360 - acc: 0.9301 - mDice: 0.5729 - val_loss: 3.1329 - val_acc: 0.9410 - val_mDice: 0.4465

Epoch 00161: val_mDice did not improve from 0.45248
Epoch 162/300
 - 10s - loss: 1.6345 - acc: 0.9302 - mDice: 0.5738 - val_loss: 3.2264 - val_acc: 0.9418 - val_mDice: 0.4427

Epoch 00162: val_mDice did not improve from 0.45248
Epoch 163/300
 - 10s - loss: 1.6418 - acc: 0.9300 - mDice: 0.5730 - val_loss: 3.5120 - val_acc: 0.9387 - val_mDice: 0.4298

Epoch 00163: val_mDice did not improve from 0.45248
Epoch 164/300
 - 10s - loss: 1.6374 - acc: 0.9302 - mDice: 0.5726 - val_loss: 3.2047 - val_acc: 0.9403 - val_mDice: 0.4445

Epoch 00164: val_mDice did not improve from 0.45248
Epoch 165/300
 - 11s - loss: 1.6353 - acc: 0.9301 - mDice: 0.5737 - val_loss: 3.3748 - val_acc: 0.9363 - val_mDice: 0.4340

Epoch 00165: val_mDice did not improve from 0.45248
Epoch 166/300
 - 10s - loss: 1.6377 - acc: 0.9301 - mDice: 0.5732 - val_loss: 3.2274 - val_acc: 0.9405 - val_mDice: 0.4447

Epoch 00166: val_mDice did not improve from 0.45248
Epoch 167/300
 - 10s - loss: 1.6342 - acc: 0.9303 - mDice: 0.5740 - val_loss: 3.2156 - val_acc: 0.9417 - val_mDice: 0.4520

Epoch 00167: val_mDice did not improve from 0.45248
Epoch 168/300
 - 10s - loss: 1.6280 - acc: 0.9304 - mDice: 0.5755 - val_loss: 3.2197 - val_acc: 0.9404 - val_mDice: 0.4408

Epoch 00168: val_mDice did not improve from 0.45248
Epoch 169/300
 - 10s - loss: 1.6225 - acc: 0.9306 - mDice: 0.5763 - val_loss: 3.2420 - val_acc: 0.9394 - val_mDice: 0.4485

Epoch 00169: val_mDice did not improve from 0.45248
Epoch 170/300
 - 11s - loss: 1.6270 - acc: 0.9306 - mDice: 0.5757 - val_loss: 3.1119 - val_acc: 0.9417 - val_mDice: 0.4504

Epoch 00170: val_mDice did not improve from 0.45248
Epoch 171/300
 - 10s - loss: 1.6207 - acc: 0.9307 - mDice: 0.5773 - val_loss: 3.2109 - val_acc: 0.9402 - val_mDice: 0.4438

Epoch 00171: val_mDice did not improve from 0.45248
Epoch 172/300
 - 10s - loss: 1.6162 - acc: 0.9307 - mDice: 0.5781 - val_loss: 3.1604 - val_acc: 0.9398 - val_mDice: 0.4420

Epoch 00172: val_mDice did not improve from 0.45248
Epoch 173/300
 - 11s - loss: 1.6223 - acc: 0.9307 - mDice: 0.5771 - val_loss: 3.1914 - val_acc: 0.9401 - val_mDice: 0.4412

Epoch 00173: val_mDice did not improve from 0.45248
Epoch 174/300
 - 10s - loss: 1.6282 - acc: 0.9305 - mDice: 0.5760 - val_loss: 3.0954 - val_acc: 0.9419 - val_mDice: 0.4541

Epoch 00174: val_mDice improved from 0.45248 to 0.45405, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 175/300
 - 11s - loss: 1.6158 - acc: 0.9309 - mDice: 0.5783 - val_loss: 3.4703 - val_acc: 0.9395 - val_mDice: 0.4379

Epoch 00175: val_mDice did not improve from 0.45405
Epoch 176/300
 - 10s - loss: 1.6068 - acc: 0.9309 - mDice: 0.5801 - val_loss: 3.3141 - val_acc: 0.9410 - val_mDice: 0.4457

Epoch 00176: val_mDice did not improve from 0.45405
Epoch 177/300
 - 10s - loss: 1.6174 - acc: 0.9309 - mDice: 0.5787 - val_loss: 3.2151 - val_acc: 0.9377 - val_mDice: 0.4345

Epoch 00177: val_mDice did not improve from 0.45405
Epoch 178/300
 - 10s - loss: 1.6160 - acc: 0.9309 - mDice: 0.5787 - val_loss: 3.2635 - val_acc: 0.9414 - val_mDice: 0.4432

Epoch 00178: val_mDice did not improve from 0.45405
Epoch 179/300
 - 10s - loss: 1.6085 - acc: 0.9311 - mDice: 0.5800 - val_loss: 3.4052 - val_acc: 0.9415 - val_mDice: 0.4380

Epoch 00179: val_mDice did not improve from 0.45405
Epoch 180/300
 - 10s - loss: 1.6140 - acc: 0.9311 - mDice: 0.5792 - val_loss: 3.2734 - val_acc: 0.9400 - val_mDice: 0.4369

Epoch 00180: val_mDice did not improve from 0.45405
Epoch 181/300
 - 10s - loss: 1.6102 - acc: 0.9313 - mDice: 0.5802 - val_loss: 3.1574 - val_acc: 0.9411 - val_mDice: 0.4517

Epoch 00181: val_mDice did not improve from 0.45405
Epoch 182/300
 - 10s - loss: 1.6053 - acc: 0.9313 - mDice: 0.5816 - val_loss: 3.2423 - val_acc: 0.9415 - val_mDice: 0.4463

Epoch 00182: val_mDice did not improve from 0.45405
Epoch 183/300
 - 10s - loss: 1.6025 - acc: 0.9314 - mDice: 0.5817 - val_loss: 3.4516 - val_acc: 0.9420 - val_mDice: 0.4341

Epoch 00183: val_mDice did not improve from 0.45405
Epoch 184/300
 - 10s - loss: 1.6094 - acc: 0.9310 - mDice: 0.5801 - val_loss: 3.4204 - val_acc: 0.9402 - val_mDice: 0.4380

Epoch 00184: val_mDice did not improve from 0.45405
Epoch 185/300
 - 10s - loss: 1.5964 - acc: 0.9314 - mDice: 0.5833 - val_loss: 3.1796 - val_acc: 0.9406 - val_mDice: 0.4429

Epoch 00185: val_mDice did not improve from 0.45405
Epoch 186/300
 - 11s - loss: 1.5946 - acc: 0.9316 - mDice: 0.5834 - val_loss: 3.1916 - val_acc: 0.9413 - val_mDice: 0.4501

Epoch 00186: val_mDice did not improve from 0.45405
Epoch 187/300
 - 10s - loss: 1.5949 - acc: 0.9315 - mDice: 0.5839 - val_loss: 3.3605 - val_acc: 0.9388 - val_mDice: 0.4433

Epoch 00187: val_mDice did not improve from 0.45405
Epoch 188/300
 - 10s - loss: 1.5995 - acc: 0.9315 - mDice: 0.5833 - val_loss: 3.4030 - val_acc: 0.9402 - val_mDice: 0.4491

Epoch 00188: val_mDice did not improve from 0.45405
Epoch 189/300
 - 10s - loss: 1.5974 - acc: 0.9316 - mDice: 0.5831 - val_loss: 3.4660 - val_acc: 0.9405 - val_mDice: 0.4331

Epoch 00189: val_mDice did not improve from 0.45405
Epoch 190/300
 - 10s - loss: 1.5989 - acc: 0.9317 - mDice: 0.5832 - val_loss: 3.3671 - val_acc: 0.9420 - val_mDice: 0.4459

Epoch 00190: val_mDice did not improve from 0.45405
Epoch 191/300
 - 10s - loss: 1.5906 - acc: 0.9318 - mDice: 0.5847 - val_loss: 3.3986 - val_acc: 0.9406 - val_mDice: 0.4403

Epoch 00191: val_mDice did not improve from 0.45405
Epoch 192/300
 - 11s - loss: 1.5941 - acc: 0.9318 - mDice: 0.5843 - val_loss: 3.3090 - val_acc: 0.9426 - val_mDice: 0.4511

Epoch 00192: val_mDice did not improve from 0.45405
Epoch 193/300
 - 10s - loss: 1.5910 - acc: 0.9319 - mDice: 0.5850 - val_loss: 3.3521 - val_acc: 0.9387 - val_mDice: 0.4298

Epoch 00193: val_mDice did not improve from 0.45405
Epoch 194/300
 - 10s - loss: 1.5841 - acc: 0.9321 - mDice: 0.5865 - val_loss: 3.3175 - val_acc: 0.9423 - val_mDice: 0.4506

Epoch 00194: val_mDice did not improve from 0.45405
Epoch 195/300
 - 10s - loss: 1.5879 - acc: 0.9319 - mDice: 0.5853 - val_loss: 3.5245 - val_acc: 0.9413 - val_mDice: 0.4405

Epoch 00195: val_mDice did not improve from 0.45405
Epoch 196/300
 - 11s - loss: 1.5866 - acc: 0.9320 - mDice: 0.5857 - val_loss: 3.2780 - val_acc: 0.9412 - val_mDice: 0.4433

Epoch 00196: val_mDice did not improve from 0.45405
Epoch 197/300
 - 10s - loss: 1.5837 - acc: 0.9321 - mDice: 0.5867 - val_loss: 3.2339 - val_acc: 0.9415 - val_mDice: 0.4507

Epoch 00197: val_mDice did not improve from 0.45405
Epoch 198/300
 - 11s - loss: 1.5810 - acc: 0.9320 - mDice: 0.5874 - val_loss: 3.3146 - val_acc: 0.9423 - val_mDice: 0.4486

Epoch 00198: val_mDice did not improve from 0.45405
Epoch 199/300
 - 10s - loss: 1.5823 - acc: 0.9322 - mDice: 0.5871 - val_loss: 3.3099 - val_acc: 0.9400 - val_mDice: 0.4386

Epoch 00199: val_mDice did not improve from 0.45405
Epoch 200/300
 - 10s - loss: 1.5790 - acc: 0.9323 - mDice: 0.5883 - val_loss: 3.2159 - val_acc: 0.9406 - val_mDice: 0.4424

Epoch 00200: val_mDice did not improve from 0.45405
Epoch 201/300
 - 11s - loss: 1.5745 - acc: 0.9324 - mDice: 0.5895 - val_loss: 3.3226 - val_acc: 0.9412 - val_mDice: 0.4392

Epoch 00201: val_mDice did not improve from 0.45405
Epoch 202/300
 - 10s - loss: 1.5727 - acc: 0.9325 - mDice: 0.5892 - val_loss: 3.1671 - val_acc: 0.9411 - val_mDice: 0.4464

Epoch 00202: val_mDice did not improve from 0.45405
Epoch 203/300
 - 10s - loss: 1.5799 - acc: 0.9322 - mDice: 0.5876 - val_loss: 3.2790 - val_acc: 0.9425 - val_mDice: 0.4505

Epoch 00203: val_mDice did not improve from 0.45405
Epoch 204/300
 - 10s - loss: 1.5782 - acc: 0.9324 - mDice: 0.5886 - val_loss: 3.2618 - val_acc: 0.9409 - val_mDice: 0.4376

Epoch 00204: val_mDice did not improve from 0.45405
Restoring model weights from the end of the best epoch
Epoch 00204: early stopping
{'val_loss': [47.66826601255508, 14.933570035866328, 9.075082769706135, 7.4553170800209045, 6.6640430724336985, 6.023106962797188, 5.58695573998349, 5.147491151023479, 4.846631839516617, 4.579597770812965, 4.256700362477984, 4.045806843077853, 3.9406709773022506, 3.777151437564975, 3.696411893658695, 3.583660949997249, 3.4202533664980104, 3.438179080241493, 3.3221882092988206, 3.4835409613414887, 3.293258367461108, 3.0866958626679013, 3.2200951936344304, 3.093627712822386, 3.207243690001113, 3.222687124008579, 3.006423153471024, 3.131360569880122, 2.9595090614720467, 2.9824991818251356, 3.008382480813279, 3.054749353345306, 3.0113769596265185, 3.0371337980475452, 3.0121024285132685, 2.9267529152067646, 2.9566313946797025, 3.203772850866829, 3.0559653859097686, 2.965307147419524, 3.014742903344865, 2.8689400010431805, 2.9567541234372627, 2.921360821086204, 3.0266865892779258, 2.950727199603404, 2.9064067080173466, 2.9771108435732976, 2.8519171168805943, 2.8693915679163875, 2.7920103824830482, 2.949385293072001, 2.975894753045092, 2.899047023328465, 2.951556975376748, 2.9339637674524317, 3.063890226384891, 3.1327603501711216, 2.8305615712445054, 2.970981786759304, 3.0652068051553907, 2.9462704146280885, 3.0310030163132717, 3.0242766793373796, 3.090526901207687, 2.9298666198294434, 3.0081036874713996, 2.9852840288630906, 2.9244670263003734, 3.135775336286142, 3.0423137582173303, 2.9800697430923937, 3.309893015195571, 3.149628540212732, 3.151969850573334, 3.1355941889674535, 3.25561747970503, 3.0589563096429977, 3.058293738136334, 3.176259583261396, 3.2293037945582044, 3.0565421499666714, 3.093040774310274, 3.193614181796355, 2.94689348883306, 3.0971110862502385, 3.2837470185366415, 3.147509122112145, 3.1240480753726194, 3.0529790874127123, 3.4177308729849756, 3.1892744816085767, 3.2217765947848203, 3.1141159394312474, 3.152809520318572, 3.1979827077482783, 3.024307369986283, 3.1646712554308274, 3.1279975980564596, 3.321778541829969, 3.1856801066148495, 3.315708316019958, 3.439962776821284, 3.145692421079037, 3.2264930023368272, 3.0010533703358044, 3.1559385911650244, 3.224183909240223, 3.1540781855804934, 3.3925583930020884, 3.3691781129954115, 3.167121297702016, 3.0534864085770788, 3.1843855171464384, 3.2021752715864706, 3.3254729337502447, 3.2491398175202666, 3.2362547034309026, 3.303519008991619, 3.32380256569013, 3.4287992240639316, 3.3463715899824384, 3.339000392971294, 3.2867985295043107, 3.349851098155514, 3.097126751184641, 3.121023704364364, 3.2972537888036597, 3.178298536045033, 3.175832807662941, 3.308815767190286, 3.33267591434664, 3.2084364164398895, 3.2267833136554276, 3.359568770043552, 3.2017687898173572, 3.384044190580469, 3.058443529863975, 3.2819866763782644, 3.3006694669879617, 3.314217069053224, 3.2120653561405126, 3.2012530398510752, 3.196991227966334, 3.234852295657176, 3.142678337437766, 3.2847091790421734, 3.0786779708495096, 3.2611957364715636, 3.3780572259842994, 3.363590845439051, 3.2951434583935355, 3.2552630742124857, 3.179959364656714, 3.1386574593017853, 3.3570093281700144, 3.159138944066529, 3.3581727540031787, 3.2880112986922976, 3.1607681663618203, 3.1328892314673533, 3.226350601163826, 3.512047349297929, 3.2047211498110775, 3.3748088936498832, 3.2274458420108116, 3.2156221231977855, 3.2196603338234127, 3.242022392384353, 3.111866802021506, 3.210886903218038, 3.1604025763060366, 3.191436119754577, 3.0953852410117784, 3.4703025117161728, 3.3141167182031843, 3.2150760887722885, 3.263507450642508, 3.4052282134957967, 3.2734187007438216, 3.1574096751088896, 3.2423484934316504, 3.4515963283677897, 3.420441310143187, 3.179644962506635, 3.1915996758976863, 3.3604596327502456, 3.40295919516523, 3.465968777536459, 3.367098213616936, 3.3985825770845017, 3.308984445031023, 3.3521029624112306, 3.3175454337948134, 3.5245032183870317, 3.278045290127574, 3.2339055444601748, 3.314555227689977, 3.3098663291228667, 3.215908754195663, 3.322620738120306, 3.167146417001883, 3.278955428195851, 3.2617878987276483], 'val_acc': [0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047367090270633, 0.9048145356632414, 0.9041277198564439, 0.9040361926669166, 0.9039263100851149, 0.9064606456529527, 0.9063507063048226, 0.9075160452297756, 0.9085072988555545, 0.9075205865360442, 0.9098534555662245, 0.9119642944563002, 0.9104006631033761, 0.914722964877174, 0.9184730052947998, 0.915675353436243, 0.9217147401400975, 0.9221909528686887, 0.9250755366824922, 0.9240430309658959, 0.9259798271315438, 0.926611704485757, 0.9260531124614534, 0.928115847564879, 0.9297939766021002, 0.92989924124309, 0.9307440746398199, 0.9297138367380414, 0.9277472751481193, 0.932880026953561, 0.9336126588639759, 0.9333607951800028, 0.932268758614858, 0.9318612359818959, 0.9306158253124782, 0.9340384460630871, 0.9317216333888826, 0.933885049252283, 0.9346726423218137, 0.9348923876172021, 0.9344253568422227, 0.9316345992542449, 0.9335531336920602, 0.9323970051038832, 0.9345101004555112, 0.9343727316175189, 0.9361332598186675, 0.9352907311348688, 0.9350274659338451, 0.9335920498484657, 0.9328090917496454, 0.9349152814774286, 0.9341277338209606, 0.9345238123621259, 0.9356639101391747, 0.935270124957675, 0.9354945250919887, 0.9354807507424128, 0.9365040915352958, 0.9366552262079149, 0.9373031372115725, 0.9372046618234544, 0.9360027341615587, 0.9353640334946769, 0.9329624772071838, 0.9364354411760966, 0.9361217703138079, 0.9372275585219974, 0.937170338063013, 0.9354876336597261, 0.9353411027363369, 0.9374130112784249, 0.9356272759891692, 0.9358378960972741, 0.9397985622996375, 0.9397115366799491, 0.9369391288076129, 0.93809065364656, 0.9377816177549816, 0.9375503545715695, 0.9374588131904602, 0.9385439498083932, 0.9378822985149565, 0.9361355020886376, 0.9397733580498469, 0.9375915669259571, 0.9396130669684637, 0.9384958885964894, 0.9403067912374224, 0.9391506371043977, 0.9404052439190093, 0.9398878188360305, 0.9377243632362002, 0.9394871649287996, 0.9360324910708836, 0.9385164749054682, 0.9404166511126927, 0.9376419356891087, 0.9410783024061293, 0.941030238355909, 0.9410073217891511, 0.9368383487065634, 0.9374633630116781, 0.939221631912958, 0.9400526313554673, 0.9402884755815778, 0.9394596815109253, 0.9390132767813546, 0.941110372543335, 0.9416620646204267, 0.9403067515009925, 0.9411034669194903, 0.93673532917386, 0.941389662878854, 0.9408424922398159, 0.9401739722206479, 0.9382806874456859, 0.9415224620274135, 0.941055417060852, 0.9407165816852024, 0.9425503810246786, 0.9409752516519456, 0.939755053747268, 0.939629146030971, 0.9390590786933899, 0.9402128968920026, 0.9410050539743333, 0.9388232373055958, 0.9417285010928199, 0.9395444052559989, 0.9413530088606334, 0.9401464944794065, 0.9400938749313354, 0.9401419531731379, 0.9411057886623201, 0.9412683305286226, 0.9365293270065671, 0.9397802210989452, 0.9399496146610805, 0.9419299506005787, 0.9409272159848895, 0.9422733755338759, 0.9409890032949901, 0.9412866263162523, 0.942916677111671, 0.9410622715950012, 0.9392032878739494, 0.9419208027067638, 0.941000467255002, 0.9410279279663449, 0.9409569870857966, 0.9418017296564012, 0.9386767205737886, 0.9402884528750465, 0.9363393102373395, 0.9404578606287638, 0.9417284585180736, 0.9404327131453014, 0.939425357750484, 0.9417490646952674, 0.9402357907522292, 0.9398351538748968, 0.9401213469959441, 0.9419230563299996, 0.9394848658925011, 0.9410371042433239, 0.9377083068802243, 0.9414354335694086, 0.9414881127221244, 0.9399679672150385, 0.9411492574782598, 0.9415247042973837, 0.9419620037078857, 0.9402128997303191, 0.9406067019417172, 0.9413347102346874, 0.9387843154725575, 0.9402152271497817, 0.9404716264633906, 0.9420238023712522, 0.940581480662028, 0.9426373640696207, 0.9386950390679496, 0.9422573503993806, 0.941268299307142, 0.9411836181368146, 0.9415361938022432, 0.9422664585567656, 0.9399541900271461, 0.9405586208615985, 0.9411836152984983, 0.9411355427333287, 0.9425297549792698, 0.9409180311929612], 'val_mDice': [0.012629814573474937, 0.011049970175095257, 0.013148576813927363, 0.017137439633231787, 0.020679434162697623, 0.028552204370498657, 0.03892807431873821, 0.04868856950529984, 0.061125281045124644, 0.07039369460904882, 0.09666346705385617, 0.11410740221894923, 0.12214605510234833, 0.14330276004260495, 0.15686234042403244, 0.16895018442578258, 0.19501993450380506, 0.19639495680374758, 0.229758343127157, 0.23030728882267362, 0.24211486899072215, 0.26864215696141835, 0.2657850761676118, 0.2761878674583776, 0.2737924624234438, 0.2779437099539098, 0.2965690092671485, 0.29402872192717733, 0.3154026176780462, 0.3196364139162359, 0.32277329363638446, 0.32528257334516164, 0.33394303403439973, 0.33604416091527256, 0.34036481185328393, 0.3495898362958715, 0.3575958331071195, 0.34269682725980166, 0.34133389805044445, 0.36860951435353073, 0.36711601230005425, 0.37814000710135415, 0.37103842864079134, 0.3705873542598316, 0.3590294195427781, 0.3777956063193934, 0.3716448291781403, 0.38422400628527004, 0.39089915759506677, 0.39311156838777517, 0.3976959861992371, 0.37800805341629756, 0.3850517631286666, 0.3952030787865321, 0.3944696704191821, 0.398475739484032, 0.40151444849159035, 0.3977313903825624, 0.40796752149860066, 0.4024803184327625, 0.3987134217861153, 0.40211688975493115, 0.40261885204485487, 0.4081473393099649, 0.4074596008729367, 0.41102094912812825, 0.4103353502494948, 0.41420862992249785, 0.41820210307126954, 0.41382570937275887, 0.41755452148971106, 0.4204881608131386, 0.3985184918911684, 0.4075562207116967, 0.4020209154557614, 0.41345198984657017, 0.4072470831729117, 0.4193062074482441, 0.4198348398009936, 0.4144457912161237, 0.4105494852576937, 0.4184367890868868, 0.41256962219874066, 0.40709047640363377, 0.4339493756138143, 0.42503070103980245, 0.4164002562562625, 0.4241868088997546, 0.4228882349672772, 0.43155867606401443, 0.4102383316272781, 0.4243972808832214, 0.4214863307064488, 0.4271782434412411, 0.42710855851570767, 0.4182703101209232, 0.43515742676598684, 0.43027406292302267, 0.4339314491621086, 0.4215398099096048, 0.42888441859256654, 0.4260131429348673, 0.4211692698299885, 0.43298627010413576, 0.4258254656479472, 0.43200101100263144, 0.4347518650548799, 0.41399144132932025, 0.4355024386729513, 0.4283386213438852, 0.4301214760967663, 0.4268824500697, 0.4289927558884734, 0.42991206653061365, 0.43036423942872454, 0.4359596346815427, 0.43403349887757076, 0.43129842958989595, 0.43749794293017613, 0.4373714161061105, 0.42925390370544936, 0.4319669668163572, 0.42103272800644237, 0.4437675069840181, 0.43627069074483144, 0.44239944503420875, 0.4404878843398321, 0.436784589219661, 0.443032951404651, 0.4392380792470205, 0.4413406905673799, 0.43616400144639467, 0.4373732166630881, 0.4379567972251347, 0.42786931211040136, 0.43300520061027437, 0.44024895042890594, 0.44587823659891174, 0.44095053399602574, 0.4398059555817218, 0.43609687756924403, 0.43859719218952314, 0.43763592342535657, 0.4409553366047995, 0.44010629948405994, 0.4459414835132304, 0.4388820532531965, 0.446714967134453, 0.4282815717160702, 0.44347796651224297, 0.4388786792045548, 0.4454286116219702, 0.43991980453332263, 0.44888592032449587, 0.4509609831230981, 0.43800356416475206, 0.4415552791740213, 0.4524827734345481, 0.4415417498066312, 0.44839087554386686, 0.4465058380294414, 0.4426526107958385, 0.4297785246301265, 0.4444644193918932, 0.4339508978383882, 0.4447463316222032, 0.452017499932221, 0.440786932905515, 0.44847550456012997, 0.450358277098054, 0.44378997279064997, 0.44200076570823077, 0.44117556583313716, 0.4540546735127767, 0.43793321063830737, 0.44573557784869555, 0.4344835982081436, 0.44324108585715294, 0.4380085961449714, 0.4368719806273778, 0.45172372460365295, 0.4462514768044154, 0.43410038611009005, 0.4380224050865287, 0.4428669430670284, 0.4501473580797513, 0.443267109138625, 0.4490698850935414, 0.43313698781033355, 0.4459205833928926, 0.4403319713615236, 0.45107170123429524, 0.4297683409282139, 0.4506436816993214, 0.4405020769862902, 0.443338772193307, 0.45065070866119294, 0.4486451078028906, 0.4385607127277624, 0.4424413944638911, 0.4392352384470758, 0.44638846371145474, 0.4505023221884455, 0.43762569892264547], 'loss': [194.3663762197206, 42.80138322606996, 20.9867906666041, 14.462275996152114, 11.489172891543646, 9.788486729013867, 8.598476188822199, 7.695708220970753, 7.0495891826608315, 6.482652164548951, 6.0273770822687185, 5.635833418481907, 5.323760867325676, 5.054095611375537, 4.814228795593774, 4.601338816305013, 4.39554816402864, 4.213064087823059, 4.0488214544278245, 3.91247576005155, 3.7791532516387565, 3.6619995678231874, 3.55988508335354, 3.4575180763313025, 3.370420548275576, 3.282238596823718, 3.207806751956502, 3.141521310999326, 3.0817193802275913, 3.0225275261957845, 2.9687799186588872, 2.908271728831494, 2.859293988979642, 2.8126276991880763, 2.762479059678271, 2.7402789216982, 2.6844780333404805, 2.6612005612972913, 2.625906500831054, 2.593081476004891, 2.574811603766957, 2.5337533583548204, 2.5112661691168863, 2.4858215715881946, 2.4650023968113577, 2.4500474418223144, 2.420243955518047, 2.39024952824506, 2.3690682200308086, 2.355776448435514, 2.339779793423266, 2.3236993881969847, 2.3036415480409076, 2.2846236266863698, 2.268379167207799, 2.253449668216871, 2.229168668978242, 2.2236487591728578, 2.2089558705121584, 2.1939541500658573, 2.186736786643595, 2.1593292009768423, 2.1583173087228347, 2.141218686990938, 2.1344430324358816, 2.115162501933604, 2.1113491310695474, 2.1010873906525522, 2.086770533963805, 2.0738701628930816, 2.0686002688668057, 2.048763105215216, 2.044620055188925, 2.037826582970407, 2.029561283318597, 2.020064954470871, 2.010498969775991, 2.000652439129566, 1.9904615494794462, 1.9808358956743854, 1.9728441985328693, 1.9744340182040176, 1.967536411695405, 1.9541677217022035, 1.9612145111099797, 1.9411780715149585, 1.9316125866165734, 1.9201085374652707, 1.916687592665814, 1.9099453214232072, 1.9119654050424009, 1.9111355534862962, 1.8903681950139843, 1.8921094452780935, 1.8828725342532677, 1.8838074283607207, 1.8723647270631028, 1.864500394556043, 1.8589272520917677, 1.8519122242490886, 1.84129112888812, 1.8471647317592914, 1.8357488393829533, 1.8328220035980634, 1.8209094009073985, 1.8262455397909814, 1.8162351171014763, 1.8127420705845299, 1.8112439801519675, 1.8052751256340795, 1.8012874131260612, 1.7951909458336168, 1.7882769631355835, 1.7771790916804338, 1.7859211208182812, 1.7722849530109166, 1.7716359046007917, 1.7701786271588569, 1.7615884573031695, 1.7667394916699162, 1.7539862885924693, 1.7520672337315275, 1.746306533525738, 1.744159606166352, 1.735224124161246, 1.7283031280178176, 1.730490197008323, 1.728859679464984, 1.7241957576973075, 1.72608087093632, 1.720126707613158, 1.709846437586418, 1.7112673904616778, 1.706010878304422, 1.6993318475554484, 1.7012077587887757, 1.6990270737349609, 1.699740622115774, 1.6909860348182078, 1.6932894804814613, 1.6921481295130214, 1.6818510253741512, 1.6865286346067463, 1.6809341106337574, 1.6823485140306815, 1.674497616803552, 1.6693314870060674, 1.671799073105307, 1.669398219663099, 1.662657121392098, 1.6640290131751847, 1.6685441429269827, 1.6613032294992816, 1.6577721337764646, 1.655747624980661, 1.6490162871167544, 1.651493128333819, 1.6537559305470138, 1.6601437921581044, 1.6438363132987934, 1.6359803194206346, 1.6345183793257858, 1.6417617189279752, 1.6374158315491258, 1.6352868132768672, 1.637710888582961, 1.6342296665611584, 1.628042508164467, 1.6224789522000767, 1.6270476490035093, 1.6207233368603509, 1.6161827217849987, 1.6222823676470963, 1.628201842216115, 1.6157981637633327, 1.6067550516965097, 1.6173511342504063, 1.6159504156984161, 1.608453180510942, 1.6139726969279695, 1.6101600662324846, 1.6053409314417577, 1.6024715178316675, 1.6093502075703865, 1.5964437899664001, 1.5946004850362936, 1.5948536558051962, 1.5994582165636628, 1.5974360317399743, 1.5988918629045636, 1.5905518829420349, 1.5940925106276935, 1.590956914234143, 1.5840761084443507, 1.5878770946881802, 1.586631684995511, 1.5837084891273494, 1.5809919111572848, 1.5822774312710841, 1.5790318777640706, 1.5744513644633413, 1.5726689261141737, 1.5798634741148647, 1.57818033142726], 'acc': [0.23775843600343022, 0.7703225800635751, 0.8524953287668672, 0.8635111282343853, 0.8665040929767468, 0.8676766351810512, 0.8680329480128917, 0.8681750600108619, 0.8685143926204779, 0.8690756332421546, 0.8698858610462632, 0.8712333732196447, 0.8727799066349057, 0.8742934678339512, 0.8756608401899373, 0.8766027496273356, 0.8774149206324949, 0.8781655967201092, 0.878862116121708, 0.8794688974498714, 0.8806199808099584, 0.882048568951732, 0.8836109080203401, 0.8853879092813848, 0.8873546868936161, 0.8892773490938488, 0.8908285870587915, 0.8923801980957213, 0.8937628337338133, 0.8955241050337789, 0.8964677535777426, 0.8980098193105944, 0.899350446153519, 0.9005727865619192, 0.9017690079897521, 0.9022253533997286, 0.9036629345115035, 0.9043412200283494, 0.9051392365333913, 0.9061201432203083, 0.9067213118179744, 0.9076388937925497, 0.9077867533582838, 0.9084873816653072, 0.9090415359646548, 0.9093619067705226, 0.909907417982696, 0.910599397640824, 0.9111973925410886, 0.9113686381449055, 0.9117594281326766, 0.911803616398627, 0.9124033024183744, 0.9127897386356013, 0.9131777066014188, 0.9137151536165645, 0.9141842451931218, 0.9145839390024781, 0.9151850363204813, 0.9153034649351233, 0.9154961409540289, 0.9164411354690061, 0.9166721352405669, 0.9169476015479987, 0.9170801669318988, 0.9178127465460465, 0.9177806527830168, 0.9180569761011305, 0.9183842093700477, 0.9185337821757469, 0.9184339587252242, 0.9188452597318966, 0.9189163477222699, 0.919090789603617, 0.9192674263768833, 0.919799012050294, 0.9197534317337802, 0.9199555602426678, 0.9203019306359549, 0.920579487219789, 0.920561063524062, 0.9205362963405261, 0.9207983912703663, 0.9208643603628257, 0.920835625890916, 0.9214249116872946, 0.921591307516155, 0.9218075459164784, 0.9219599273454392, 0.9222210232913207, 0.9219253058260706, 0.922014031930537, 0.9226751472533082, 0.9225677659522138, 0.922868655094274, 0.9228442984898702, 0.9228242813150432, 0.9232434135239915, 0.9234452153966678, 0.9234960772799508, 0.9237483277448644, 0.9236967676012691, 0.9236997596808935, 0.9239210955481183, 0.9242684418970802, 0.924354939589915, 0.9245034719630429, 0.9246320539479636, 0.9245452953828238, 0.9248104296554277, 0.9249211237199371, 0.9251017705631642, 0.9252125557410779, 0.9254939983822786, 0.9252286371018171, 0.9255649741614786, 0.9256662588853103, 0.9257555659483319, 0.9259198303739299, 0.9259699286581672, 0.9262722242836873, 0.9262074628316623, 0.9264630706686723, 0.9265082530118561, 0.9268628997749415, 0.9270172018478067, 0.927113062135282, 0.9272772313106582, 0.9272135107738332, 0.9272338107231152, 0.9271119042515685, 0.9277000308082769, 0.9278541878167434, 0.9280169713766055, 0.9279484066035629, 0.9279803152555028, 0.9279998510771458, 0.9281006203420318, 0.9283417512484775, 0.9283962955772704, 0.9283359572679201, 0.928600417997696, 0.9283645274966857, 0.9285533070587252, 0.928412588225961, 0.928851068674863, 0.9288984526208032, 0.9290014284296442, 0.9289024647591728, 0.9291649734511963, 0.9292034867964937, 0.9290085910417819, 0.9293054859831725, 0.9294665560157905, 0.9294624569927633, 0.9296780471193186, 0.9295442052399558, 0.9295217757834527, 0.9295142446153537, 0.9299463300385962, 0.9301054736012918, 0.9301870858850841, 0.9300392673312986, 0.9301650967698164, 0.930127832938739, 0.9300980096220028, 0.9303069530894859, 0.9304070339419834, 0.9305725946852943, 0.9306432465746702, 0.9306592105991789, 0.9307317853272709, 0.9306850481267744, 0.9304909407230662, 0.9309081943917187, 0.9308773964522821, 0.9309272602364257, 0.930901541435491, 0.9311492010097824, 0.9310890947529087, 0.9312988909448713, 0.9313494997238732, 0.9313607144521246, 0.9310432156777368, 0.9313916510094193, 0.931590069068841, 0.9315468985174395, 0.9315287779010466, 0.9316486950000633, 0.9317104442284612, 0.9317819360756382, 0.9317781788456617, 0.9319199225749495, 0.9320576544416556, 0.9319350286924657, 0.9320219938456725, 0.9320789720135569, 0.9320470422868304, 0.9321565930039432, 0.9323136811888882, 0.9324139010959078, 0.9324798945840991, 0.932231652224158, 0.9324000640965299], 'mDice': [0.015677885884422316, 0.013747632286793434, 0.014864377662358496, 0.017210838048271254, 0.02149937625637582, 0.027623495325110013, 0.03712545111215204, 0.04846054860032522, 0.05763736751892214, 0.06973096903636894, 0.08112467144417905, 0.09623125422309997, 0.10998534503983054, 0.12298014504339094, 0.1369185205171948, 0.1499343064304995, 0.16323182330969008, 0.17750770919971529, 0.1908079192763421, 0.2020446661438169, 0.21437571222597818, 0.22556056867094887, 0.23663836006015534, 0.2470914737576576, 0.25684153106898866, 0.26792202263938364, 0.2770877440460206, 0.28502844066341326, 0.29360601062349023, 0.3020175238703174, 0.30917277186965353, 0.31805214265166437, 0.3261388946595347, 0.3325603495208151, 0.3416067294953116, 0.3459380519040775, 0.35435752741560117, 0.3586838052455276, 0.3645094982720947, 0.3712485551730686, 0.37417377135945307, 0.38192035591347223, 0.38572376507404615, 0.39038064667938166, 0.394781995324103, 0.3974419778853178, 0.40282938130241563, 0.40886873622172276, 0.4127024881913755, 0.4152997245282044, 0.41832272765584505, 0.4205645835362856, 0.4254885925616008, 0.42934275850156656, 0.43149510320151774, 0.4355536628164766, 0.4392156435824185, 0.4413885233695047, 0.44484002765534536, 0.4472863537706298, 0.4486629701209983, 0.45465006860713175, 0.4554661991958423, 0.45841592119933783, 0.4602584008343628, 0.4642200480197835, 0.4647255395048037, 0.46719776453345824, 0.4698897683484857, 0.4731243799905775, 0.4744121880229783, 0.478067144064952, 0.47894403755561404, 0.48072150659878315, 0.48254230185834707, 0.4851243378271688, 0.4866658906406214, 0.48804646590230827, 0.4905560754766716, 0.493339965585709, 0.4940977481051817, 0.49471327794041736, 0.4964940609995377, 0.49903099772349613, 0.4974923266745358, 0.502005269595463, 0.5034518208098315, 0.5061435307844172, 0.5067514074211845, 0.5080431195405813, 0.5088249160440437, 0.508844450957885, 0.5133759567612096, 0.5128428937038843, 0.5149544396565005, 0.5146446804308997, 0.51687452589498, 0.5188204578887251, 0.5213300529325171, 0.5214694772516806, 0.5245717517760119, 0.5232484645413277, 0.5255012618615169, 0.5263800564381298, 0.5287682519658627, 0.5278824098036196, 0.5298090865783478, 0.5312005794068994, 0.53186949068856, 0.5330715469250404, 0.5344473108956642, 0.5352389647571343, 0.5369949828964772, 0.5389970637296283, 0.5375149736106568, 0.5400433615726246, 0.5404776268688406, 0.5408840234830748, 0.5425433688295216, 0.5420762876658075, 0.5445210951151865, 0.5450238614889475, 0.5467661387754448, 0.547502299425619, 0.5491210053324217, 0.550502393065056, 0.5504597371567277, 0.5508918278659036, 0.5520168813823482, 0.5516465683077166, 0.5528724196560699, 0.5549441397316497, 0.5552000899477595, 0.556422501520911, 0.5576387597389802, 0.5574334703338429, 0.5576176435151771, 0.557772529614737, 0.5597361197631575, 0.5597542935031812, 0.5601558609805816, 0.5620024717219514, 0.5609569141164735, 0.5624371650983355, 0.5617273587906593, 0.5641187192940036, 0.5652808293524978, 0.5647347012833867, 0.5651202698491179, 0.566204115193957, 0.5671049804385973, 0.5654382969525501, 0.5673557365356623, 0.5683045034949114, 0.5683796598039336, 0.5700745034452254, 0.5703574776396669, 0.5693687863815996, 0.5681928100672351, 0.5719079948790804, 0.5729043754121116, 0.5738055042639527, 0.5729979613577904, 0.5725548584865241, 0.5736947360240082, 0.5731901936570597, 0.5739813382073544, 0.5755451567053036, 0.5763283316078318, 0.5757148728242885, 0.5772599580087068, 0.5780688981364545, 0.5770914310753541, 0.5759624291045463, 0.578343657978886, 0.5800581375757853, 0.578670297982216, 0.5786951886497529, 0.5800452712231664, 0.579239030897399, 0.5801516527014475, 0.5816346168035438, 0.5816717437289458, 0.580117368746383, 0.5832964670619042, 0.5834138759740267, 0.5838697141182544, 0.5833386858809952, 0.5831352587446716, 0.5832126013157523, 0.5847247505155813, 0.5843114923805728, 0.5849598453687017, 0.5865047370534837, 0.5852770840521835, 0.5857456784292479, 0.5867327327026851, 0.587444556731582, 0.58708484445851, 0.5882851758604453, 0.5895354332366798, 0.5891742196091343, 0.5875810393138913, 0.5885719368286346]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:05,  2.51s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:04<00:02,  2.24s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  2.00s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:46,  1.43s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:09,  1.52s/it]predicting train subjects:   1%|          | 3/285 [00:04<07:11,  1.53s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:41,  1.64s/it]predicting train subjects:   2%|▏         | 5/285 [00:08<07:21,  1.58s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:50,  1.69s/it]predicting train subjects:   2%|▏         | 7/285 [00:12<08:21,  1.81s/it]predicting train subjects:   3%|▎         | 8/285 [00:14<08:32,  1.85s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<08:12,  1.78s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:28,  1.85s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:42,  1.91s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:50,  1.94s/it]predicting train subjects:   5%|▍         | 13/285 [00:23<08:55,  1.97s/it]predicting train subjects:   5%|▍         | 14/285 [00:25<08:57,  1.98s/it]predicting train subjects:   5%|▌         | 15/285 [00:27<08:58,  2.00s/it]predicting train subjects:   6%|▌         | 16/285 [00:29<09:00,  2.01s/it]predicting train subjects:   6%|▌         | 17/285 [00:31<08:58,  2.01s/it]predicting train subjects:   6%|▋         | 18/285 [00:33<09:04,  2.04s/it]predicting train subjects:   7%|▋         | 19/285 [00:35<09:00,  2.03s/it]predicting train subjects:   7%|▋         | 20/285 [00:37<08:57,  2.03s/it]predicting train subjects:   7%|▋         | 21/285 [00:40<08:57,  2.04s/it]predicting train subjects:   8%|▊         | 22/285 [00:42<08:55,  2.04s/it]predicting train subjects:   8%|▊         | 23/285 [00:44<08:59,  2.06s/it]predicting train subjects:   8%|▊         | 24/285 [00:46<08:53,  2.04s/it]predicting train subjects:   9%|▉         | 25/285 [00:48<08:45,  2.02s/it]predicting train subjects:   9%|▉         | 26/285 [00:50<08:46,  2.03s/it]predicting train subjects:   9%|▉         | 27/285 [00:52<08:42,  2.03s/it]predicting train subjects:  10%|▉         | 28/285 [00:54<08:34,  2.00s/it]predicting train subjects:  10%|█         | 29/285 [00:56<08:30,  2.00s/it]predicting train subjects:  11%|█         | 30/285 [00:58<08:22,  1.97s/it]predicting train subjects:  11%|█         | 31/285 [01:00<08:17,  1.96s/it]predicting train subjects:  11%|█         | 32/285 [01:02<08:18,  1.97s/it]predicting train subjects:  12%|█▏        | 33/285 [01:04<08:19,  1.98s/it]predicting train subjects:  12%|█▏        | 34/285 [01:05<08:12,  1.96s/it]predicting train subjects:  12%|█▏        | 35/285 [01:07<08:04,  1.94s/it]predicting train subjects:  13%|█▎        | 36/285 [01:09<07:58,  1.92s/it]predicting train subjects:  13%|█▎        | 37/285 [01:11<07:53,  1.91s/it]predicting train subjects:  13%|█▎        | 38/285 [01:13<07:53,  1.92s/it]predicting train subjects:  14%|█▎        | 39/285 [01:15<07:52,  1.92s/it]predicting train subjects:  14%|█▍        | 40/285 [01:17<07:53,  1.93s/it]predicting train subjects:  14%|█▍        | 41/285 [01:19<07:54,  1.94s/it]predicting train subjects:  15%|█▍        | 42/285 [01:21<07:52,  1.94s/it]predicting train subjects:  15%|█▌        | 43/285 [01:23<07:50,  1.94s/it]predicting train subjects:  15%|█▌        | 44/285 [01:25<07:47,  1.94s/it]predicting train subjects:  16%|█▌        | 45/285 [01:27<07:48,  1.95s/it]predicting train subjects:  16%|█▌        | 46/285 [01:28<07:32,  1.89s/it]predicting train subjects:  16%|█▋        | 47/285 [01:30<07:07,  1.80s/it]predicting train subjects:  17%|█▋        | 48/285 [01:32<06:55,  1.75s/it]predicting train subjects:  17%|█▋        | 49/285 [01:33<06:47,  1.73s/it]predicting train subjects:  18%|█▊        | 50/285 [01:35<06:39,  1.70s/it]predicting train subjects:  18%|█▊        | 51/285 [01:37<06:33,  1.68s/it]predicting train subjects:  18%|█▊        | 52/285 [01:38<06:33,  1.69s/it]predicting train subjects:  19%|█▊        | 53/285 [01:40<06:34,  1.70s/it]predicting train subjects:  19%|█▉        | 54/285 [01:42<06:30,  1.69s/it]predicting train subjects:  19%|█▉        | 55/285 [01:43<06:31,  1.70s/it]predicting train subjects:  20%|█▉        | 56/285 [01:45<06:28,  1.70s/it]predicting train subjects:  20%|██        | 57/285 [01:47<06:29,  1.71s/it]predicting train subjects:  20%|██        | 58/285 [01:49<06:30,  1.72s/it]predicting train subjects:  21%|██        | 59/285 [01:50<06:23,  1.70s/it]predicting train subjects:  21%|██        | 60/285 [01:52<06:20,  1.69s/it]predicting train subjects:  21%|██▏       | 61/285 [01:54<06:24,  1.72s/it]predicting train subjects:  22%|██▏       | 62/285 [01:55<06:19,  1.70s/it]predicting train subjects:  22%|██▏       | 63/285 [01:57<06:24,  1.73s/it]predicting train subjects:  22%|██▏       | 64/285 [01:59<06:25,  1.74s/it]predicting train subjects:  23%|██▎       | 65/285 [02:01<06:35,  1.80s/it]predicting train subjects:  23%|██▎       | 66/285 [02:03<06:38,  1.82s/it]predicting train subjects:  24%|██▎       | 67/285 [02:05<06:40,  1.83s/it]predicting train subjects:  24%|██▍       | 68/285 [02:06<06:31,  1.80s/it]predicting train subjects:  24%|██▍       | 69/285 [02:08<06:27,  1.79s/it]predicting train subjects:  25%|██▍       | 70/285 [02:10<06:18,  1.76s/it]predicting train subjects:  25%|██▍       | 71/285 [02:12<06:15,  1.75s/it]predicting train subjects:  25%|██▌       | 72/285 [02:13<06:11,  1.75s/it]predicting train subjects:  26%|██▌       | 73/285 [02:15<06:09,  1.74s/it]predicting train subjects:  26%|██▌       | 74/285 [02:17<06:03,  1.72s/it]predicting train subjects:  26%|██▋       | 75/285 [02:18<06:00,  1.72s/it]predicting train subjects:  27%|██▋       | 76/285 [02:20<05:57,  1.71s/it]predicting train subjects:  27%|██▋       | 77/285 [02:22<05:57,  1.72s/it]predicting train subjects:  27%|██▋       | 78/285 [02:24<05:57,  1.73s/it]predicting train subjects:  28%|██▊       | 79/285 [02:25<05:52,  1.71s/it]predicting train subjects:  28%|██▊       | 80/285 [02:27<05:50,  1.71s/it]predicting train subjects:  28%|██▊       | 81/285 [02:29<05:48,  1.71s/it]predicting train subjects:  29%|██▉       | 82/285 [02:30<05:46,  1.71s/it]predicting train subjects:  29%|██▉       | 83/285 [02:32<05:42,  1.70s/it]predicting train subjects:  29%|██▉       | 84/285 [02:34<05:43,  1.71s/it]predicting train subjects:  30%|██▉       | 85/285 [02:36<05:54,  1.77s/it]predicting train subjects:  30%|███       | 86/285 [02:38<06:01,  1.82s/it]predicting train subjects:  31%|███       | 87/285 [02:40<06:07,  1.86s/it]predicting train subjects:  31%|███       | 88/285 [02:42<06:15,  1.90s/it]predicting train subjects:  31%|███       | 89/285 [02:44<06:16,  1.92s/it]predicting train subjects:  32%|███▏      | 90/285 [02:46<06:22,  1.96s/it]predicting train subjects:  32%|███▏      | 91/285 [02:48<06:21,  1.97s/it]predicting train subjects:  32%|███▏      | 92/285 [02:50<06:23,  1.99s/it]predicting train subjects:  33%|███▎      | 93/285 [02:52<06:27,  2.02s/it]predicting train subjects:  33%|███▎      | 94/285 [02:54<06:23,  2.01s/it]predicting train subjects:  33%|███▎      | 95/285 [02:56<06:17,  1.98s/it]predicting train subjects:  34%|███▎      | 96/285 [02:58<06:14,  1.98s/it]predicting train subjects:  34%|███▍      | 97/285 [02:59<06:10,  1.97s/it]predicting train subjects:  34%|███▍      | 98/285 [03:01<06:08,  1.97s/it]predicting train subjects:  35%|███▍      | 99/285 [03:03<06:04,  1.96s/it]predicting train subjects:  35%|███▌      | 100/285 [03:05<06:04,  1.97s/it]predicting train subjects:  35%|███▌      | 101/285 [03:07<06:01,  1.96s/it]predicting train subjects:  36%|███▌      | 102/285 [03:09<05:59,  1.96s/it]predicting train subjects:  36%|███▌      | 103/285 [03:11<05:53,  1.94s/it]predicting train subjects:  36%|███▋      | 104/285 [03:13<05:47,  1.92s/it]predicting train subjects:  37%|███▋      | 105/285 [03:15<05:41,  1.90s/it]predicting train subjects:  37%|███▋      | 106/285 [03:17<05:39,  1.89s/it]predicting train subjects:  38%|███▊      | 107/285 [03:19<05:35,  1.88s/it]predicting train subjects:  38%|███▊      | 108/285 [03:20<05:31,  1.87s/it]predicting train subjects:  38%|███▊      | 109/285 [03:22<05:27,  1.86s/it]predicting train subjects:  39%|███▊      | 110/285 [03:24<05:24,  1.85s/it]predicting train subjects:  39%|███▉      | 111/285 [03:26<05:19,  1.83s/it]predicting train subjects:  39%|███▉      | 112/285 [03:28<05:17,  1.84s/it]predicting train subjects:  40%|███▉      | 113/285 [03:30<05:16,  1.84s/it]predicting train subjects:  40%|████      | 114/285 [03:32<05:29,  1.93s/it]predicting train subjects:  40%|████      | 115/285 [03:34<05:27,  1.93s/it]predicting train subjects:  41%|████      | 116/285 [03:36<05:24,  1.92s/it]predicting train subjects:  41%|████      | 117/285 [03:37<05:19,  1.90s/it]predicting train subjects:  41%|████▏     | 118/285 [03:39<05:15,  1.89s/it]predicting train subjects:  42%|████▏     | 119/285 [03:41<05:13,  1.89s/it]predicting train subjects:  42%|████▏     | 120/285 [03:43<05:12,  1.89s/it]predicting train subjects:  42%|████▏     | 121/285 [03:45<05:01,  1.84s/it]predicting train subjects:  43%|████▎     | 122/285 [03:46<04:44,  1.75s/it]predicting train subjects:  43%|████▎     | 123/285 [03:48<04:31,  1.68s/it]predicting train subjects:  44%|████▎     | 124/285 [03:50<04:30,  1.68s/it]predicting train subjects:  44%|████▍     | 125/285 [03:51<04:25,  1.66s/it]predicting train subjects:  44%|████▍     | 126/285 [03:53<04:25,  1.67s/it]predicting train subjects:  45%|████▍     | 127/285 [03:54<04:20,  1.65s/it]predicting train subjects:  45%|████▍     | 128/285 [03:56<04:22,  1.67s/it]predicting train subjects:  45%|████▌     | 129/285 [03:58<04:21,  1.68s/it]predicting train subjects:  46%|████▌     | 130/285 [04:00<04:20,  1.68s/it]predicting train subjects:  46%|████▌     | 131/285 [04:01<04:17,  1.67s/it]predicting train subjects:  46%|████▋     | 132/285 [04:03<04:17,  1.68s/it]predicting train subjects:  47%|████▋     | 133/285 [04:05<04:16,  1.69s/it]predicting train subjects:  47%|████▋     | 134/285 [04:06<04:13,  1.68s/it]predicting train subjects:  47%|████▋     | 135/285 [04:08<04:13,  1.69s/it]predicting train subjects:  48%|████▊     | 136/285 [04:10<04:14,  1.70s/it]predicting train subjects:  48%|████▊     | 137/285 [04:11<04:14,  1.72s/it]predicting train subjects:  48%|████▊     | 138/285 [04:13<04:12,  1.71s/it]predicting train subjects:  49%|████▉     | 139/285 [04:15<04:11,  1.73s/it]predicting train subjects:  49%|████▉     | 140/285 [04:17<04:07,  1.71s/it]predicting train subjects:  49%|████▉     | 141/285 [04:18<04:07,  1.72s/it]predicting train subjects:  50%|████▉     | 142/285 [04:20<03:58,  1.67s/it]predicting train subjects:  50%|█████     | 143/285 [04:21<03:52,  1.64s/it]predicting train subjects:  51%|█████     | 144/285 [04:23<03:47,  1.61s/it]predicting train subjects:  51%|█████     | 145/285 [04:25<03:41,  1.58s/it]predicting train subjects:  51%|█████     | 146/285 [04:26<03:35,  1.55s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:28<03:33,  1.55s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:29<03:31,  1.55s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:31<03:27,  1.53s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:32<03:23,  1.51s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:34<03:21,  1.50s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:35<03:17,  1.49s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:37<03:19,  1.51s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:38<03:19,  1.53s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:40<03:20,  1.54s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:41<03:18,  1.54s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:43<03:18,  1.55s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:44<03:15,  1.54s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:46<03:14,  1.55s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:47<03:11,  1.53s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:49<03:07,  1.51s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:50<03:05,  1.51s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:52<03:03,  1.50s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:53<02:59,  1.48s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:55<02:58,  1.48s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:56<02:56,  1.48s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:58<02:53,  1.47s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:59<02:51,  1.47s/it]predicting train subjects:  59%|█████▉    | 169/285 [05:01<02:50,  1.47s/it]predicting train subjects:  60%|█████▉    | 170/285 [05:02<02:48,  1.47s/it]predicting train subjects:  60%|██████    | 171/285 [05:04<02:47,  1.47s/it]predicting train subjects:  60%|██████    | 172/285 [05:05<02:46,  1.47s/it]predicting train subjects:  61%|██████    | 173/285 [05:06<02:44,  1.47s/it]predicting train subjects:  61%|██████    | 174/285 [05:08<02:43,  1.47s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:09<02:41,  1.47s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:11<02:45,  1.52s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:13<02:42,  1.51s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:14<02:38,  1.49s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:15<02:33,  1.45s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:17<02:32,  1.45s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:18<02:29,  1.44s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:20<02:27,  1.43s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:21<02:27,  1.45s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:23<02:31,  1.50s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:24<02:26,  1.47s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:26<02:25,  1.47s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:27<02:23,  1.46s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:28<02:20,  1.44s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:30<02:18,  1.44s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:31<02:16,  1.44s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:33<02:13,  1.42s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:34<02:12,  1.42s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:36<02:11,  1.43s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:37<02:08,  1.41s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:38<02:05,  1.39s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:40<02:13,  1.50s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:42<02:16,  1.55s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:43<02:21,  1.63s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:45<02:22,  1.66s/it]predicting train subjects:  70%|███████   | 200/285 [05:47<02:26,  1.72s/it]predicting train subjects:  71%|███████   | 201/285 [05:49<02:24,  1.72s/it]predicting train subjects:  71%|███████   | 202/285 [05:50<02:22,  1.71s/it]predicting train subjects:  71%|███████   | 203/285 [05:52<02:21,  1.73s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:54<02:21,  1.74s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:56<02:18,  1.73s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:58<02:18,  1.76s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:59<02:16,  1.75s/it]predicting train subjects:  73%|███████▎  | 208/285 [06:01<02:12,  1.72s/it]predicting train subjects:  73%|███████▎  | 209/285 [06:03<02:09,  1.70s/it]predicting train subjects:  74%|███████▎  | 210/285 [06:04<02:08,  1.71s/it]predicting train subjects:  74%|███████▍  | 211/285 [06:06<02:08,  1.74s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:08<02:07,  1.75s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:10<02:06,  1.75s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:11<01:59,  1.68s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:13<01:55,  1.64s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:14<01:51,  1.62s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:16<01:49,  1.60s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:17<01:45,  1.58s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:19<01:43,  1.57s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:20<01:41,  1.56s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:22<01:37,  1.53s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:23<01:34,  1.51s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:25<01:33,  1.51s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:26<01:30,  1.49s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:28<01:30,  1.50s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:29<01:27,  1.49s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:31<01:26,  1.49s/it]predicting train subjects:  80%|████████  | 228/285 [06:32<01:25,  1.50s/it]predicting train subjects:  80%|████████  | 229/285 [06:34<01:24,  1.50s/it]predicting train subjects:  81%|████████  | 230/285 [06:35<01:22,  1.51s/it]predicting train subjects:  81%|████████  | 231/285 [06:37<01:21,  1.50s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:39<01:26,  1.64s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:41<01:29,  1.72s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:43<01:30,  1.78s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:45<01:31,  1.83s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:47<01:31,  1.87s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:48<01:30,  1.88s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:50<01:28,  1.88s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:52<01:27,  1.90s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:54<01:25,  1.91s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:56<01:23,  1.90s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:58<01:21,  1.90s/it]predicting train subjects:  85%|████████▌ | 243/285 [07:00<01:20,  1.91s/it]predicting train subjects:  86%|████████▌ | 244/285 [07:02<01:17,  1.89s/it]predicting train subjects:  86%|████████▌ | 245/285 [07:04<01:15,  1.89s/it]predicting train subjects:  86%|████████▋ | 246/285 [07:06<01:14,  1.91s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:08<01:12,  1.92s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:09<01:11,  1.92s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:11<01:09,  1.93s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:13<01:02,  1.77s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:14<00:56,  1.66s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:16<00:52,  1.60s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:17<00:49,  1.55s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:18<00:46,  1.49s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:20<00:43,  1.46s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:21<00:41,  1.44s/it]predicting train subjects:  90%|█████████ | 257/285 [07:23<00:39,  1.43s/it]predicting train subjects:  91%|█████████ | 258/285 [07:24<00:38,  1.44s/it]predicting train subjects:  91%|█████████ | 259/285 [07:25<00:37,  1.43s/it]predicting train subjects:  91%|█████████ | 260/285 [07:27<00:35,  1.43s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:28<00:33,  1.41s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:30<00:32,  1.42s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:31<00:31,  1.44s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:33<00:30,  1.43s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:34<00:28,  1.45s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:36<00:27,  1.46s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:37<00:25,  1.44s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:39<00:26,  1.58s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:41<00:26,  1.68s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:43<00:26,  1.79s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:45<00:25,  1.84s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:47<00:24,  1.88s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:49<00:22,  1.90s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:51<00:21,  1.92s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:53<00:19,  1.94s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:55<00:17,  1.96s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:57<00:15,  1.96s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:59<00:13,  1.95s/it]predicting train subjects:  98%|█████████▊| 279/285 [08:01<00:11,  1.96s/it]predicting train subjects:  98%|█████████▊| 280/285 [08:02<00:09,  1.93s/it]predicting train subjects:  99%|█████████▊| 281/285 [08:04<00:07,  1.92s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:06<00:05,  1.93s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:08<00:03,  1.95s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:10<00:01,  1.94s/it]predicting train subjects: 100%|██████████| 285/285 [08:12<00:00,  1.93s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:34,  1.39s/it]Loading train:   1%|          | 2/285 [00:02<06:43,  1.43s/it]Loading train:   1%|          | 3/285 [00:04<06:41,  1.42s/it]Loading train:   1%|▏         | 4/285 [00:06<07:04,  1.51s/it]Loading train:   2%|▏         | 5/285 [00:07<06:37,  1.42s/it]Loading train:   2%|▏         | 6/285 [00:08<07:03,  1.52s/it]Loading train:   2%|▏         | 7/285 [00:10<07:34,  1.64s/it]Loading train:   3%|▎         | 8/285 [00:12<07:38,  1.65s/it]Loading train:   3%|▎         | 9/285 [00:13<07:15,  1.58s/it]Loading train:   4%|▎         | 10/285 [00:15<06:44,  1.47s/it]Loading train:   4%|▍         | 11/285 [00:16<06:21,  1.39s/it]Loading train:   4%|▍         | 12/285 [00:17<06:02,  1.33s/it]Loading train:   5%|▍         | 13/285 [00:18<05:44,  1.27s/it]Loading train:   5%|▍         | 14/285 [00:19<05:39,  1.25s/it]Loading train:   5%|▌         | 15/285 [00:21<05:37,  1.25s/it]Loading train:   6%|▌         | 16/285 [00:22<05:36,  1.25s/it]Loading train:   6%|▌         | 17/285 [00:23<05:28,  1.23s/it]Loading train:   6%|▋         | 18/285 [00:24<05:28,  1.23s/it]Loading train:   7%|▋         | 19/285 [00:26<05:33,  1.26s/it]Loading train:   7%|▋         | 20/285 [00:27<05:50,  1.32s/it]Loading train:   7%|▋         | 21/285 [00:28<05:43,  1.30s/it]Loading train:   8%|▊         | 22/285 [00:30<05:42,  1.30s/it]Loading train:   8%|▊         | 23/285 [00:31<05:36,  1.28s/it]Loading train:   8%|▊         | 24/285 [00:32<05:27,  1.25s/it]Loading train:   9%|▉         | 25/285 [00:33<05:25,  1.25s/it]Loading train:   9%|▉         | 26/285 [00:35<05:23,  1.25s/it]Loading train:   9%|▉         | 27/285 [00:36<05:20,  1.24s/it]Loading train:  10%|▉         | 28/285 [00:37<05:14,  1.23s/it]Loading train:  10%|█         | 29/285 [00:38<04:59,  1.17s/it]Loading train:  11%|█         | 30/285 [00:39<04:48,  1.13s/it]Loading train:  11%|█         | 31/285 [00:40<04:47,  1.13s/it]Loading train:  11%|█         | 32/285 [00:41<04:44,  1.12s/it]Loading train:  12%|█▏        | 33/285 [00:42<04:35,  1.09s/it]Loading train:  12%|█▏        | 34/285 [00:43<04:36,  1.10s/it]Loading train:  12%|█▏        | 35/285 [00:45<04:36,  1.11s/it]Loading train:  13%|█▎        | 36/285 [00:46<04:35,  1.11s/it]Loading train:  13%|█▎        | 37/285 [00:47<04:28,  1.08s/it]Loading train:  13%|█▎        | 38/285 [00:48<04:26,  1.08s/it]Loading train:  14%|█▎        | 39/285 [00:49<04:22,  1.07s/it]Loading train:  14%|█▍        | 40/285 [00:50<04:25,  1.09s/it]Loading train:  14%|█▍        | 41/285 [00:51<04:22,  1.07s/it]Loading train:  15%|█▍        | 42/285 [00:52<04:31,  1.12s/it]Loading train:  15%|█▌        | 43/285 [00:53<04:29,  1.11s/it]Loading train:  15%|█▌        | 44/285 [00:54<04:17,  1.07s/it]Loading train:  16%|█▌        | 45/285 [00:55<04:14,  1.06s/it]Loading train:  16%|█▌        | 46/285 [00:56<04:14,  1.06s/it]Loading train:  16%|█▋        | 47/285 [00:57<04:05,  1.03s/it]Loading train:  17%|█▋        | 48/285 [00:58<04:00,  1.01s/it]Loading train:  17%|█▋        | 49/285 [00:59<03:51,  1.02it/s]Loading train:  18%|█▊        | 50/285 [01:00<03:51,  1.02it/s]Loading train:  18%|█▊        | 51/285 [01:01<03:54,  1.00s/it]Loading train:  18%|█▊        | 52/285 [01:02<04:00,  1.03s/it]Loading train:  19%|█▊        | 53/285 [01:03<03:51,  1.00it/s]Loading train:  19%|█▉        | 54/285 [01:04<03:48,  1.01it/s]Loading train:  19%|█▉        | 55/285 [01:05<03:48,  1.01it/s]Loading train:  20%|█▉        | 56/285 [01:06<03:51,  1.01s/it]Loading train:  20%|██        | 57/285 [01:07<03:52,  1.02s/it]Loading train:  20%|██        | 58/285 [01:08<03:49,  1.01s/it]Loading train:  21%|██        | 59/285 [01:09<03:47,  1.01s/it]Loading train:  21%|██        | 60/285 [01:10<03:47,  1.01s/it]Loading train:  21%|██▏       | 61/285 [01:11<03:41,  1.01it/s]Loading train:  22%|██▏       | 62/285 [01:12<03:36,  1.03it/s]Loading train:  22%|██▏       | 63/285 [01:13<03:34,  1.04it/s]Loading train:  22%|██▏       | 64/285 [01:15<04:11,  1.14s/it]Loading train:  23%|██▎       | 65/285 [01:16<04:47,  1.31s/it]Loading train:  23%|██▎       | 66/285 [01:18<04:57,  1.36s/it]Loading train:  24%|██▎       | 67/285 [01:19<04:29,  1.24s/it]Loading train:  24%|██▍       | 68/285 [01:20<04:07,  1.14s/it]Loading train:  24%|██▍       | 69/285 [01:21<03:54,  1.08s/it]Loading train:  25%|██▍       | 70/285 [01:22<03:55,  1.10s/it]Loading train:  25%|██▍       | 71/285 [01:23<03:50,  1.08s/it]Loading train:  25%|██▌       | 72/285 [01:24<03:49,  1.08s/it]Loading train:  26%|██▌       | 73/285 [01:25<03:39,  1.04s/it]Loading train:  26%|██▌       | 74/285 [01:26<03:32,  1.01s/it]Loading train:  26%|██▋       | 75/285 [01:27<03:30,  1.00s/it]Loading train:  27%|██▋       | 76/285 [01:28<03:32,  1.02s/it]Loading train:  27%|██▋       | 77/285 [01:29<03:27,  1.00it/s]Loading train:  27%|██▋       | 78/285 [01:30<03:30,  1.02s/it]Loading train:  28%|██▊       | 79/285 [01:31<03:27,  1.01s/it]Loading train:  28%|██▊       | 80/285 [01:32<03:24,  1.00it/s]Loading train:  28%|██▊       | 81/285 [01:33<03:23,  1.00it/s]Loading train:  29%|██▉       | 82/285 [01:34<03:30,  1.03s/it]Loading train:  29%|██▉       | 83/285 [01:35<03:29,  1.04s/it]Loading train:  29%|██▉       | 84/285 [01:36<03:23,  1.01s/it]Loading train:  30%|██▉       | 85/285 [01:37<03:33,  1.07s/it]Loading train:  30%|███       | 86/285 [01:38<03:41,  1.11s/it]Loading train:  31%|███       | 87/285 [01:40<03:38,  1.10s/it]Loading train:  31%|███       | 88/285 [01:41<03:42,  1.13s/it]Loading train:  31%|███       | 89/285 [01:42<03:41,  1.13s/it]Loading train:  32%|███▏      | 90/285 [01:43<03:37,  1.11s/it]Loading train:  32%|███▏      | 91/285 [01:44<03:38,  1.13s/it]Loading train:  32%|███▏      | 92/285 [01:45<03:35,  1.12s/it]Loading train:  33%|███▎      | 93/285 [01:46<03:34,  1.12s/it]Loading train:  33%|███▎      | 94/285 [01:47<03:34,  1.13s/it]Loading train:  33%|███▎      | 95/285 [01:48<03:30,  1.11s/it]Loading train:  34%|███▎      | 96/285 [01:50<03:31,  1.12s/it]Loading train:  34%|███▍      | 97/285 [01:51<03:24,  1.09s/it]Loading train:  34%|███▍      | 98/285 [01:52<03:24,  1.09s/it]Loading train:  35%|███▍      | 99/285 [01:53<03:26,  1.11s/it]Loading train:  35%|███▌      | 100/285 [01:54<03:27,  1.12s/it]Loading train:  35%|███▌      | 101/285 [01:55<03:22,  1.10s/it]Loading train:  36%|███▌      | 102/285 [01:56<03:19,  1.09s/it]Loading train:  36%|███▌      | 103/285 [01:57<03:25,  1.13s/it]Loading train:  36%|███▋      | 104/285 [01:58<03:23,  1.12s/it]Loading train:  37%|███▋      | 105/285 [02:00<03:18,  1.10s/it]Loading train:  37%|███▋      | 106/285 [02:01<03:10,  1.06s/it]Loading train:  38%|███▊      | 107/285 [02:02<03:07,  1.05s/it]Loading train:  38%|███▊      | 108/285 [02:03<03:05,  1.05s/it]Loading train:  38%|███▊      | 109/285 [02:04<03:11,  1.09s/it]Loading train:  39%|███▊      | 110/285 [02:05<03:13,  1.10s/it]Loading train:  39%|███▉      | 111/285 [02:06<03:11,  1.10s/it]Loading train:  39%|███▉      | 112/285 [02:07<03:10,  1.10s/it]Loading train:  40%|███▉      | 113/285 [02:08<03:08,  1.09s/it]Loading train:  40%|████      | 114/285 [02:09<03:06,  1.09s/it]Loading train:  40%|████      | 115/285 [02:10<03:03,  1.08s/it]Loading train:  41%|████      | 116/285 [02:11<03:04,  1.09s/it]Loading train:  41%|████      | 117/285 [02:12<03:02,  1.08s/it]Loading train:  41%|████▏     | 118/285 [02:14<02:58,  1.07s/it]Loading train:  42%|████▏     | 119/285 [02:15<02:55,  1.06s/it]Loading train:  42%|████▏     | 120/285 [02:16<02:51,  1.04s/it]Loading train:  42%|████▏     | 121/285 [02:17<03:09,  1.15s/it]Loading train:  43%|████▎     | 122/285 [02:18<03:16,  1.21s/it]Loading train:  43%|████▎     | 123/285 [02:20<03:20,  1.24s/it]Loading train:  44%|████▎     | 124/285 [02:21<03:03,  1.14s/it]Loading train:  44%|████▍     | 125/285 [02:21<02:53,  1.08s/it]Loading train:  44%|████▍     | 126/285 [02:22<02:46,  1.05s/it]Loading train:  45%|████▍     | 127/285 [02:23<02:40,  1.01s/it]Loading train:  45%|████▍     | 128/285 [02:24<02:35,  1.01it/s]Loading train:  45%|████▌     | 129/285 [02:25<02:34,  1.01it/s]Loading train:  46%|████▌     | 130/285 [02:26<02:30,  1.03it/s]Loading train:  46%|████▌     | 131/285 [02:27<02:28,  1.04it/s]Loading train:  46%|████▋     | 132/285 [02:28<02:27,  1.03it/s]Loading train:  47%|████▋     | 133/285 [02:29<02:26,  1.04it/s]Loading train:  47%|████▋     | 134/285 [02:30<02:23,  1.05it/s]Loading train:  47%|████▋     | 135/285 [02:31<02:23,  1.05it/s]Loading train:  48%|████▊     | 136/285 [02:32<02:18,  1.07it/s]Loading train:  48%|████▊     | 137/285 [02:33<02:18,  1.07it/s]Loading train:  48%|████▊     | 138/285 [02:34<02:17,  1.07it/s]Loading train:  49%|████▉     | 139/285 [02:35<02:17,  1.06it/s]Loading train:  49%|████▉     | 140/285 [02:36<02:20,  1.04it/s]Loading train:  49%|████▉     | 141/285 [02:37<02:18,  1.04it/s]Loading train:  50%|████▉     | 142/285 [02:38<02:21,  1.01it/s]Loading train:  50%|█████     | 143/285 [02:39<02:21,  1.00it/s]Loading train:  51%|█████     | 144/285 [02:40<02:15,  1.04it/s]Loading train:  51%|█████     | 145/285 [02:40<02:09,  1.08it/s]Loading train:  51%|█████     | 146/285 [02:41<02:06,  1.10it/s]Loading train:  52%|█████▏    | 147/285 [02:42<02:05,  1.10it/s]Loading train:  52%|█████▏    | 148/285 [02:43<02:00,  1.14it/s]Loading train:  52%|█████▏    | 149/285 [02:44<02:02,  1.11it/s]Loading train:  53%|█████▎    | 150/285 [02:45<01:58,  1.14it/s]Loading train:  53%|█████▎    | 151/285 [02:46<01:59,  1.12it/s]Loading train:  53%|█████▎    | 152/285 [02:47<02:00,  1.10it/s]Loading train:  54%|█████▎    | 153/285 [02:48<01:59,  1.11it/s]Loading train:  54%|█████▍    | 154/285 [02:49<02:01,  1.08it/s]Loading train:  54%|█████▍    | 155/285 [02:50<02:00,  1.08it/s]Loading train:  55%|█████▍    | 156/285 [02:51<02:02,  1.05it/s]Loading train:  55%|█████▌    | 157/285 [02:51<02:02,  1.05it/s]Loading train:  55%|█████▌    | 158/285 [02:52<01:59,  1.06it/s]Loading train:  56%|█████▌    | 159/285 [02:53<02:00,  1.05it/s]Loading train:  56%|█████▌    | 160/285 [02:54<01:56,  1.07it/s]Loading train:  56%|█████▋    | 161/285 [02:55<01:53,  1.09it/s]Loading train:  57%|█████▋    | 162/285 [02:56<01:49,  1.12it/s]Loading train:  57%|█████▋    | 163/285 [02:57<01:48,  1.12it/s]Loading train:  58%|█████▊    | 164/285 [02:58<01:49,  1.11it/s]Loading train:  58%|█████▊    | 165/285 [02:59<01:48,  1.11it/s]Loading train:  58%|█████▊    | 166/285 [03:00<01:48,  1.09it/s]Loading train:  59%|█████▊    | 167/285 [03:01<01:48,  1.09it/s]Loading train:  59%|█████▉    | 168/285 [03:01<01:44,  1.12it/s]Loading train:  59%|█████▉    | 169/285 [03:02<01:43,  1.12it/s]Loading train:  60%|█████▉    | 170/285 [03:03<01:43,  1.11it/s]Loading train:  60%|██████    | 171/285 [03:04<01:43,  1.11it/s]Loading train:  60%|██████    | 172/285 [03:05<01:40,  1.13it/s]Loading train:  61%|██████    | 173/285 [03:06<01:39,  1.13it/s]Loading train:  61%|██████    | 174/285 [03:07<01:39,  1.12it/s]Loading train:  61%|██████▏   | 175/285 [03:08<01:37,  1.12it/s]Loading train:  62%|██████▏   | 176/285 [03:08<01:33,  1.16it/s]Loading train:  62%|██████▏   | 177/285 [03:09<01:31,  1.19it/s]Loading train:  62%|██████▏   | 178/285 [03:10<01:32,  1.15it/s]Loading train:  63%|██████▎   | 179/285 [03:11<01:31,  1.16it/s]Loading train:  63%|██████▎   | 180/285 [03:12<01:32,  1.14it/s]Loading train:  64%|██████▎   | 181/285 [03:13<01:38,  1.06it/s]Loading train:  64%|██████▍   | 182/285 [03:14<01:35,  1.08it/s]Loading train:  64%|██████▍   | 183/285 [03:15<01:32,  1.11it/s]Loading train:  65%|██████▍   | 184/285 [03:16<01:28,  1.14it/s]Loading train:  65%|██████▍   | 185/285 [03:16<01:27,  1.14it/s]Loading train:  65%|██████▌   | 186/285 [03:17<01:26,  1.14it/s]Loading train:  66%|██████▌   | 187/285 [03:18<01:28,  1.11it/s]Loading train:  66%|██████▌   | 188/285 [03:19<01:27,  1.11it/s]Loading train:  66%|██████▋   | 189/285 [03:20<01:24,  1.14it/s]Loading train:  67%|██████▋   | 190/285 [03:21<01:23,  1.14it/s]Loading train:  67%|██████▋   | 191/285 [03:22<01:21,  1.15it/s]Loading train:  67%|██████▋   | 192/285 [03:23<01:21,  1.14it/s]Loading train:  68%|██████▊   | 193/285 [03:24<01:20,  1.14it/s]Loading train:  68%|██████▊   | 194/285 [03:24<01:19,  1.14it/s]Loading train:  68%|██████▊   | 195/285 [03:25<01:16,  1.18it/s]Loading train:  69%|██████▉   | 196/285 [03:26<01:19,  1.12it/s]Loading train:  69%|██████▉   | 197/285 [03:27<01:21,  1.08it/s]Loading train:  69%|██████▉   | 198/285 [03:28<01:22,  1.05it/s]Loading train:  70%|██████▉   | 199/285 [03:29<01:21,  1.06it/s]Loading train:  70%|███████   | 200/285 [03:30<01:21,  1.04it/s]Loading train:  71%|███████   | 201/285 [03:31<01:22,  1.02it/s]Loading train:  71%|███████   | 202/285 [03:32<01:22,  1.01it/s]Loading train:  71%|███████   | 203/285 [03:33<01:20,  1.01it/s]Loading train:  72%|███████▏  | 204/285 [03:34<01:20,  1.01it/s]Loading train:  72%|███████▏  | 205/285 [03:35<01:19,  1.00it/s]Loading train:  72%|███████▏  | 206/285 [03:36<01:16,  1.04it/s]Loading train:  73%|███████▎  | 207/285 [03:37<01:13,  1.06it/s]Loading train:  73%|███████▎  | 208/285 [03:38<01:12,  1.06it/s]Loading train:  73%|███████▎  | 209/285 [03:39<01:11,  1.07it/s]Loading train:  74%|███████▎  | 210/285 [03:40<01:10,  1.07it/s]Loading train:  74%|███████▍  | 211/285 [03:41<01:11,  1.04it/s]Loading train:  74%|███████▍  | 212/285 [03:42<01:10,  1.04it/s]Loading train:  75%|███████▍  | 213/285 [03:43<01:12,  1.01s/it]Loading train:  75%|███████▌  | 214/285 [03:44<01:10,  1.01it/s]Loading train:  75%|███████▌  | 215/285 [03:45<01:08,  1.02it/s]Loading train:  76%|███████▌  | 216/285 [03:46<01:07,  1.02it/s]Loading train:  76%|███████▌  | 217/285 [03:47<01:04,  1.06it/s]Loading train:  76%|███████▋  | 218/285 [03:47<01:01,  1.09it/s]Loading train:  77%|███████▋  | 219/285 [03:48<00:59,  1.10it/s]Loading train:  77%|███████▋  | 220/285 [03:49<00:57,  1.14it/s]Loading train:  78%|███████▊  | 221/285 [03:50<00:55,  1.16it/s]Loading train:  78%|███████▊  | 222/285 [03:51<00:55,  1.14it/s]Loading train:  78%|███████▊  | 223/285 [03:52<00:53,  1.15it/s]Loading train:  79%|███████▊  | 224/285 [03:53<00:56,  1.08it/s]Loading train:  79%|███████▉  | 225/285 [03:54<00:54,  1.09it/s]Loading train:  79%|███████▉  | 226/285 [03:55<00:53,  1.11it/s]Loading train:  80%|███████▉  | 227/285 [03:55<00:52,  1.10it/s]Loading train:  80%|████████  | 228/285 [03:56<00:51,  1.10it/s]Loading train:  80%|████████  | 229/285 [03:57<00:50,  1.11it/s]Loading train:  81%|████████  | 230/285 [03:58<00:49,  1.10it/s]Loading train:  81%|████████  | 231/285 [03:59<00:48,  1.12it/s]Loading train:  81%|████████▏ | 232/285 [04:00<00:50,  1.05it/s]Loading train:  82%|████████▏ | 233/285 [04:01<00:51,  1.00it/s]Loading train:  82%|████████▏ | 234/285 [04:02<00:51,  1.01s/it]Loading train:  82%|████████▏ | 235/285 [04:03<00:51,  1.03s/it]Loading train:  83%|████████▎ | 236/285 [04:04<00:51,  1.05s/it]Loading train:  83%|████████▎ | 237/285 [04:05<00:50,  1.05s/it]Loading train:  84%|████████▎ | 238/285 [04:07<00:49,  1.05s/it]Loading train:  84%|████████▍ | 239/285 [04:08<00:48,  1.06s/it]Loading train:  84%|████████▍ | 240/285 [04:09<00:48,  1.08s/it]Loading train:  85%|████████▍ | 241/285 [04:10<00:47,  1.08s/it]Loading train:  85%|████████▍ | 242/285 [04:11<00:46,  1.09s/it]Loading train:  85%|████████▌ | 243/285 [04:12<00:46,  1.12s/it]Loading train:  86%|████████▌ | 244/285 [04:13<00:46,  1.13s/it]Loading train:  86%|████████▌ | 245/285 [04:14<00:44,  1.12s/it]Loading train:  86%|████████▋ | 246/285 [04:15<00:43,  1.10s/it]Loading train:  87%|████████▋ | 247/285 [04:16<00:40,  1.07s/it]Loading train:  87%|████████▋ | 248/285 [04:18<00:39,  1.07s/it]Loading train:  87%|████████▋ | 249/285 [04:19<00:38,  1.07s/it]Loading train:  88%|████████▊ | 250/285 [04:19<00:35,  1.03s/it]Loading train:  88%|████████▊ | 251/285 [04:20<00:33,  1.00it/s]Loading train:  88%|████████▊ | 252/285 [04:21<00:32,  1.01it/s]Loading train:  89%|████████▉ | 253/285 [04:22<00:32,  1.00s/it]Loading train:  89%|████████▉ | 254/285 [04:24<00:31,  1.03s/it]Loading train:  89%|████████▉ | 255/285 [04:24<00:30,  1.01s/it]Loading train:  90%|████████▉ | 256/285 [04:25<00:29,  1.00s/it]Loading train:  90%|█████████ | 257/285 [04:26<00:27,  1.01it/s]Loading train:  91%|█████████ | 258/285 [04:27<00:26,  1.03it/s]Loading train:  91%|█████████ | 259/285 [04:28<00:24,  1.06it/s]Loading train:  91%|█████████ | 260/285 [04:29<00:23,  1.06it/s]Loading train:  92%|█████████▏| 261/285 [04:30<00:22,  1.07it/s]Loading train:  92%|█████████▏| 262/285 [04:31<00:21,  1.06it/s]Loading train:  92%|█████████▏| 263/285 [04:32<00:20,  1.08it/s]Loading train:  93%|█████████▎| 264/285 [04:33<00:19,  1.07it/s]Loading train:  93%|█████████▎| 265/285 [04:34<00:18,  1.06it/s]Loading train:  93%|█████████▎| 266/285 [04:35<00:18,  1.01it/s]Loading train:  94%|█████████▎| 267/285 [04:36<00:17,  1.00it/s]Loading train:  94%|█████████▍| 268/285 [04:37<00:18,  1.08s/it]Loading train:  94%|█████████▍| 269/285 [04:38<00:17,  1.09s/it]Loading train:  95%|█████████▍| 270/285 [04:39<00:16,  1.07s/it]Loading train:  95%|█████████▌| 271/285 [04:40<00:14,  1.06s/it]Loading train:  95%|█████████▌| 272/285 [04:41<00:13,  1.07s/it]Loading train:  96%|█████████▌| 273/285 [04:43<00:12,  1.08s/it]Loading train:  96%|█████████▌| 274/285 [04:44<00:11,  1.06s/it]Loading train:  96%|█████████▋| 275/285 [04:45<00:10,  1.06s/it]Loading train:  97%|█████████▋| 276/285 [04:46<00:09,  1.07s/it]Loading train:  97%|█████████▋| 277/285 [04:47<00:08,  1.12s/it]Loading train:  98%|█████████▊| 278/285 [04:48<00:07,  1.11s/it]Loading train:  98%|█████████▊| 279/285 [04:49<00:06,  1.09s/it]Loading train:  98%|█████████▊| 280/285 [04:50<00:05,  1.11s/it]Loading train:  99%|█████████▊| 281/285 [04:51<00:04,  1.10s/it]Loading train:  99%|█████████▉| 282/285 [04:52<00:03,  1.07s/it]Loading train:  99%|█████████▉| 283/285 [04:53<00:02,  1.09s/it]Loading train: 100%|█████████▉| 284/285 [04:55<00:01,  1.08s/it]Loading train: 100%|██████████| 285/285 [04:56<00:00,  1.08s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|▏         | 4/285 [00:00<00:07, 38.23it/s]concatenating: train:   6%|▌         | 17/285 [00:00<00:05, 48.46it/s]concatenating: train:  15%|█▌        | 44/285 [00:00<00:03, 64.26it/s]concatenating: train:  20%|█▉        | 56/285 [00:00<00:03, 64.67it/s]concatenating: train:  29%|██▉       | 84/285 [00:00<00:02, 83.82it/s]concatenating: train:  40%|███▉      | 113/285 [00:00<00:01, 106.50it/s]concatenating: train:  51%|█████     | 144/285 [00:00<00:01, 132.42it/s]concatenating: train:  61%|██████▏   | 175/285 [00:00<00:00, 159.24it/s]concatenating: train:  72%|███████▏  | 206/285 [00:00<00:00, 186.15it/s]concatenating: train:  84%|████████▎ | 238/285 [00:01<00:00, 211.66it/s]concatenating: train:  94%|█████████▍| 269/285 [00:01<00:00, 233.60it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 228.86it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.46s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.41s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.35s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 70.75it/s]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<05:47,  1.23s/it]Loading train:   1%|          | 2/285 [00:02<06:02,  1.28s/it]Loading train:   1%|          | 3/285 [00:03<05:57,  1.27s/it]Loading train:   1%|▏         | 4/285 [00:05<06:20,  1.35s/it]Loading train:   2%|▏         | 5/285 [00:06<05:57,  1.28s/it]Loading train:   2%|▏         | 6/285 [00:08<06:20,  1.36s/it]Loading train:   2%|▏         | 7/285 [00:09<06:39,  1.44s/it]Loading train:   3%|▎         | 8/285 [00:11<06:53,  1.49s/it]Loading train:   3%|▎         | 9/285 [00:12<06:29,  1.41s/it]Loading train:   4%|▎         | 10/285 [00:13<06:03,  1.32s/it]Loading train:   4%|▍         | 11/285 [00:14<05:32,  1.21s/it]Loading train:   4%|▍         | 12/285 [00:15<05:26,  1.20s/it]Loading train:   5%|▍         | 13/285 [00:16<05:15,  1.16s/it]Loading train:   5%|▍         | 14/285 [00:18<05:20,  1.18s/it]Loading train:   5%|▌         | 15/285 [00:19<05:14,  1.16s/it]Loading train:   6%|▌         | 16/285 [00:20<05:10,  1.16s/it]Loading train:   6%|▌         | 17/285 [00:21<05:00,  1.12s/it]Loading train:   6%|▋         | 18/285 [00:22<04:58,  1.12s/it]Loading train:   7%|▋         | 19/285 [00:23<04:58,  1.12s/it]Loading train:   7%|▋         | 20/285 [00:24<04:47,  1.09s/it]Loading train:   7%|▋         | 21/285 [00:25<04:51,  1.11s/it]Loading train:   8%|▊         | 22/285 [00:26<04:59,  1.14s/it]Loading train:   8%|▊         | 23/285 [00:28<04:56,  1.13s/it]Loading train:   8%|▊         | 24/285 [00:29<04:47,  1.10s/it]Loading train:   9%|▉         | 25/285 [00:30<04:46,  1.10s/it]Loading train:   9%|▉         | 26/285 [00:31<04:39,  1.08s/it]Loading train:   9%|▉         | 27/285 [00:32<04:34,  1.06s/it]Loading train:  10%|▉         | 28/285 [00:33<04:51,  1.13s/it]Loading train:  10%|█         | 29/285 [00:34<04:37,  1.09s/it]Loading train:  11%|█         | 30/285 [00:35<04:29,  1.06s/it]Loading train:  11%|█         | 31/285 [00:36<04:28,  1.06s/it]Loading train:  11%|█         | 32/285 [00:37<04:22,  1.04s/it]Loading train:  12%|█▏        | 33/285 [00:38<04:17,  1.02s/it]Loading train:  12%|█▏        | 34/285 [00:39<04:15,  1.02s/it]Loading train:  12%|█▏        | 35/285 [00:40<04:19,  1.04s/it]Loading train:  13%|█▎        | 36/285 [00:41<04:14,  1.02s/it]Loading train:  13%|█▎        | 37/285 [00:42<04:10,  1.01s/it]Loading train:  13%|█▎        | 38/285 [00:43<04:03,  1.01it/s]Loading train:  14%|█▎        | 39/285 [00:44<04:04,  1.01it/s]Loading train:  14%|█▍        | 40/285 [00:45<04:06,  1.00s/it]Loading train:  14%|█▍        | 41/285 [00:46<04:06,  1.01s/it]Loading train:  15%|█▍        | 42/285 [00:47<03:59,  1.02it/s]Loading train:  15%|█▌        | 43/285 [00:48<04:02,  1.00s/it]Loading train:  15%|█▌        | 44/285 [00:49<04:07,  1.03s/it]Loading train:  16%|█▌        | 45/285 [00:50<04:08,  1.03s/it]Loading train:  16%|█▌        | 46/285 [00:51<04:06,  1.03s/it]Loading train:  16%|█▋        | 47/285 [00:52<03:53,  1.02it/s]Loading train:  17%|█▋        | 48/285 [00:53<03:49,  1.03it/s]Loading train:  17%|█▋        | 49/285 [00:54<03:42,  1.06it/s]Loading train:  18%|█▊        | 50/285 [00:55<03:35,  1.09it/s]Loading train:  18%|█▊        | 51/285 [00:56<03:27,  1.13it/s]Loading train:  18%|█▊        | 52/285 [00:56<03:23,  1.14it/s]Loading train:  19%|█▊        | 53/285 [00:57<03:27,  1.12it/s]Loading train:  19%|█▉        | 54/285 [00:58<03:22,  1.14it/s]Loading train:  19%|█▉        | 55/285 [00:59<03:22,  1.14it/s]Loading train:  20%|█▉        | 56/285 [01:00<03:27,  1.10it/s]Loading train:  20%|██        | 57/285 [01:01<03:27,  1.10it/s]Loading train:  20%|██        | 58/285 [01:02<03:29,  1.08it/s]Loading train:  21%|██        | 59/285 [01:03<03:23,  1.11it/s]Loading train:  21%|██        | 60/285 [01:04<03:13,  1.16it/s]Loading train:  21%|██▏       | 61/285 [01:04<03:02,  1.23it/s]Loading train:  22%|██▏       | 62/285 [01:05<03:17,  1.13it/s]Loading train:  22%|██▏       | 63/285 [01:06<03:15,  1.13it/s]Loading train:  22%|██▏       | 64/285 [01:08<03:52,  1.05s/it]Loading train:  23%|██▎       | 65/285 [01:09<04:32,  1.24s/it]Loading train:  23%|██▎       | 66/285 [01:11<04:43,  1.30s/it]Loading train:  24%|██▎       | 67/285 [01:13<05:11,  1.43s/it]Loading train:  24%|██▍       | 68/285 [01:14<05:04,  1.40s/it]Loading train:  24%|██▍       | 69/285 [01:15<04:38,  1.29s/it]Loading train:  25%|██▍       | 70/285 [01:16<04:47,  1.34s/it]Loading train:  25%|██▍       | 71/285 [01:18<04:36,  1.29s/it]Loading train:  25%|██▌       | 72/285 [01:19<04:38,  1.31s/it]Loading train:  26%|██▌       | 73/285 [01:20<04:39,  1.32s/it]Loading train:  26%|██▌       | 74/285 [01:21<04:11,  1.19s/it]Loading train:  26%|██▋       | 75/285 [01:22<04:02,  1.16s/it]Loading train:  27%|██▋       | 76/285 [01:23<03:43,  1.07s/it]Loading train:  27%|██▋       | 77/285 [01:24<03:26,  1.01it/s]Loading train:  27%|██▋       | 78/285 [01:25<03:14,  1.07it/s]Loading train:  28%|██▊       | 79/285 [01:26<03:06,  1.10it/s]Loading train:  28%|██▊       | 80/285 [01:26<03:04,  1.11it/s]Loading train:  28%|██▊       | 81/285 [01:27<02:59,  1.14it/s]Loading train:  29%|██▉       | 82/285 [01:28<02:53,  1.17it/s]Loading train:  29%|██▉       | 83/285 [01:29<02:50,  1.18it/s]Loading train:  29%|██▉       | 84/285 [01:30<02:49,  1.18it/s]Loading train:  30%|██▉       | 85/285 [01:31<03:00,  1.11it/s]Loading train:  30%|███       | 86/285 [01:32<03:02,  1.09it/s]Loading train:  31%|███       | 87/285 [01:33<03:06,  1.06it/s]Loading train:  31%|███       | 88/285 [01:34<03:04,  1.07it/s]Loading train:  31%|███       | 89/285 [01:35<03:09,  1.03it/s]Loading train:  32%|███▏      | 90/285 [01:36<03:05,  1.05it/s]Loading train:  32%|███▏      | 91/285 [01:36<03:01,  1.07it/s]Loading train:  32%|███▏      | 92/285 [01:37<02:55,  1.10it/s]Loading train:  33%|███▎      | 93/285 [01:38<02:55,  1.10it/s]Loading train:  33%|███▎      | 94/285 [01:39<02:54,  1.10it/s]Loading train:  33%|███▎      | 95/285 [01:40<02:58,  1.06it/s]Loading train:  34%|███▎      | 96/285 [01:41<03:03,  1.03it/s]Loading train:  34%|███▍      | 97/285 [01:42<03:04,  1.02it/s]Loading train:  34%|███▍      | 98/285 [01:43<03:05,  1.01it/s]Loading train:  35%|███▍      | 99/285 [01:44<02:59,  1.04it/s]Loading train:  35%|███▌      | 100/285 [01:45<02:59,  1.03it/s]Loading train:  35%|███▌      | 101/285 [01:46<02:57,  1.03it/s]Loading train:  36%|███▌      | 102/285 [01:47<02:58,  1.03it/s]Loading train:  36%|███▌      | 103/285 [01:48<03:04,  1.01s/it]Loading train:  36%|███▋      | 104/285 [01:49<02:59,  1.01it/s]Loading train:  37%|███▋      | 105/285 [01:50<02:54,  1.03it/s]Loading train:  37%|███▋      | 106/285 [01:51<02:51,  1.04it/s]Loading train:  38%|███▊      | 107/285 [01:52<02:47,  1.06it/s]Loading train:  38%|███▊      | 108/285 [01:53<02:43,  1.09it/s]Loading train:  38%|███▊      | 109/285 [01:54<02:37,  1.12it/s]Loading train:  39%|███▊      | 110/285 [01:54<02:36,  1.12it/s]Loading train:  39%|███▉      | 111/285 [01:55<02:31,  1.14it/s]Loading train:  39%|███▉      | 112/285 [01:56<02:30,  1.15it/s]Loading train:  40%|███▉      | 113/285 [01:57<02:29,  1.15it/s]Loading train:  40%|████      | 114/285 [01:58<02:31,  1.13it/s]Loading train:  40%|████      | 115/285 [01:59<02:31,  1.12it/s]Loading train:  41%|████      | 116/285 [02:00<02:36,  1.08it/s]Loading train:  41%|████      | 117/285 [02:01<02:38,  1.06it/s]Loading train:  41%|████▏     | 118/285 [02:02<02:38,  1.06it/s]Loading train:  42%|████▏     | 119/285 [02:03<02:43,  1.02it/s]Loading train:  42%|████▏     | 120/285 [02:04<02:39,  1.03it/s]Loading train:  42%|████▏     | 121/285 [02:05<02:55,  1.07s/it]Loading train:  43%|████▎     | 122/285 [02:06<02:58,  1.09s/it]Loading train:  43%|████▎     | 123/285 [02:07<03:01,  1.12s/it]Loading train:  44%|████▎     | 124/285 [02:08<02:47,  1.04s/it]Loading train:  44%|████▍     | 125/285 [02:09<02:34,  1.04it/s]Loading train:  44%|████▍     | 126/285 [02:10<02:24,  1.10it/s]Loading train:  45%|████▍     | 127/285 [02:11<02:18,  1.14it/s]Loading train:  45%|████▍     | 128/285 [02:11<02:13,  1.17it/s]Loading train:  45%|████▌     | 129/285 [02:12<02:10,  1.19it/s]Loading train:  46%|████▌     | 130/285 [02:13<02:09,  1.20it/s]Loading train:  46%|████▌     | 131/285 [02:14<02:11,  1.17it/s]Loading train:  46%|████▋     | 132/285 [02:15<02:11,  1.16it/s]Loading train:  47%|████▋     | 133/285 [02:16<02:08,  1.18it/s]Loading train:  47%|████▋     | 134/285 [02:16<02:06,  1.19it/s]Loading train:  47%|████▋     | 135/285 [02:17<02:01,  1.23it/s]Loading train:  48%|████▊     | 136/285 [02:18<02:03,  1.21it/s]Loading train:  48%|████▊     | 137/285 [02:19<01:59,  1.24it/s]Loading train:  48%|████▊     | 138/285 [02:20<02:01,  1.21it/s]Loading train:  49%|████▉     | 139/285 [02:21<01:59,  1.22it/s]Loading train:  49%|████▉     | 140/285 [02:21<01:57,  1.24it/s]Loading train:  49%|████▉     | 141/285 [02:22<01:59,  1.20it/s]Loading train:  50%|████▉     | 142/285 [02:23<01:55,  1.23it/s]Loading train:  50%|█████     | 143/285 [02:24<01:52,  1.27it/s]Loading train:  51%|█████     | 144/285 [02:24<01:49,  1.29it/s]Loading train:  51%|█████     | 145/285 [02:25<01:43,  1.35it/s]Loading train:  51%|█████     | 146/285 [02:26<01:43,  1.34it/s]Loading train:  52%|█████▏    | 147/285 [02:27<01:41,  1.35it/s]Loading train:  52%|█████▏    | 148/285 [02:27<01:41,  1.35it/s]Loading train:  52%|█████▏    | 149/285 [02:28<01:41,  1.35it/s]Loading train:  53%|█████▎    | 150/285 [02:29<01:41,  1.34it/s]Loading train:  53%|█████▎    | 151/285 [02:30<01:42,  1.30it/s]Loading train:  53%|█████▎    | 152/285 [02:30<01:43,  1.29it/s]Loading train:  54%|█████▎    | 153/285 [02:31<01:41,  1.31it/s]Loading train:  54%|█████▍    | 154/285 [02:32<01:39,  1.31it/s]Loading train:  54%|█████▍    | 155/285 [02:33<01:42,  1.27it/s]Loading train:  55%|█████▍    | 156/285 [02:34<01:39,  1.30it/s]Loading train:  55%|█████▌    | 157/285 [02:34<01:37,  1.31it/s]Loading train:  55%|█████▌    | 158/285 [02:35<01:34,  1.34it/s]Loading train:  56%|█████▌    | 159/285 [02:36<01:34,  1.33it/s]Loading train:  56%|█████▌    | 160/285 [02:37<01:37,  1.28it/s]Loading train:  56%|█████▋    | 161/285 [02:37<01:34,  1.31it/s]Loading train:  57%|█████▋    | 162/285 [02:38<01:32,  1.33it/s]Loading train:  57%|█████▋    | 163/285 [02:39<01:29,  1.36it/s]Loading train:  58%|█████▊    | 164/285 [02:39<01:28,  1.36it/s]Loading train:  58%|█████▊    | 165/285 [02:40<01:26,  1.39it/s]Loading train:  58%|█████▊    | 166/285 [02:41<01:27,  1.36it/s]Loading train:  59%|█████▊    | 167/285 [02:42<01:28,  1.34it/s]Loading train:  59%|█████▉    | 168/285 [02:42<01:26,  1.35it/s]Loading train:  59%|█████▉    | 169/285 [02:43<01:27,  1.33it/s]Loading train:  60%|█████▉    | 170/285 [02:44<01:25,  1.35it/s]Loading train:  60%|██████    | 171/285 [02:45<01:25,  1.34it/s]Loading train:  60%|██████    | 172/285 [02:45<01:23,  1.35it/s]Loading train:  61%|██████    | 173/285 [02:46<01:22,  1.35it/s]Loading train:  61%|██████    | 174/285 [02:47<01:22,  1.35it/s]Loading train:  61%|██████▏   | 175/285 [02:48<01:21,  1.35it/s]Loading train:  62%|██████▏   | 176/285 [02:48<01:18,  1.39it/s]Loading train:  62%|██████▏   | 177/285 [02:49<01:22,  1.30it/s]Loading train:  62%|██████▏   | 178/285 [02:50<01:22,  1.29it/s]Loading train:  63%|██████▎   | 179/285 [02:51<01:19,  1.33it/s]Loading train:  63%|██████▎   | 180/285 [02:51<01:18,  1.35it/s]Loading train:  64%|██████▎   | 181/285 [02:52<01:15,  1.37it/s]Loading train:  64%|██████▍   | 182/285 [02:53<01:14,  1.39it/s]Loading train:  64%|██████▍   | 183/285 [02:54<01:13,  1.38it/s]Loading train:  65%|██████▍   | 184/285 [02:54<01:11,  1.42it/s]Loading train:  65%|██████▍   | 185/285 [02:55<01:10,  1.43it/s]Loading train:  65%|██████▌   | 186/285 [02:56<01:11,  1.38it/s]Loading train:  66%|██████▌   | 187/285 [02:56<01:10,  1.38it/s]Loading train:  66%|██████▌   | 188/285 [02:57<01:09,  1.39it/s]Loading train:  66%|██████▋   | 189/285 [02:58<01:08,  1.39it/s]Loading train:  67%|██████▋   | 190/285 [02:58<01:07,  1.40it/s]Loading train:  67%|██████▋   | 191/285 [02:59<01:09,  1.36it/s]Loading train:  67%|██████▋   | 192/285 [03:00<01:09,  1.34it/s]Loading train:  68%|██████▊   | 193/285 [03:01<01:07,  1.36it/s]Loading train:  68%|██████▊   | 194/285 [03:01<01:03,  1.44it/s]Loading train:  68%|██████▊   | 195/285 [03:02<01:01,  1.47it/s]Loading train:  69%|██████▉   | 196/285 [03:03<01:03,  1.41it/s]Loading train:  69%|██████▉   | 197/285 [03:04<01:07,  1.30it/s]Loading train:  69%|██████▉   | 198/285 [03:04<01:07,  1.30it/s]Loading train:  70%|██████▉   | 199/285 [03:05<01:05,  1.32it/s]Loading train:  70%|███████   | 200/285 [03:06<01:06,  1.28it/s]Loading train:  71%|███████   | 201/285 [03:07<01:08,  1.22it/s]Loading train:  71%|███████   | 202/285 [03:08<01:11,  1.16it/s]Loading train:  71%|███████   | 203/285 [03:09<01:08,  1.20it/s]Loading train:  72%|███████▏  | 204/285 [03:09<01:06,  1.23it/s]Loading train:  72%|███████▏  | 205/285 [03:10<01:07,  1.18it/s]Loading train:  72%|███████▏  | 206/285 [03:11<01:04,  1.22it/s]Loading train:  73%|███████▎  | 207/285 [03:12<01:02,  1.24it/s]Loading train:  73%|███████▎  | 208/285 [03:13<01:00,  1.26it/s]Loading train:  73%|███████▎  | 209/285 [03:13<00:59,  1.27it/s]Loading train:  74%|███████▎  | 210/285 [03:14<00:59,  1.26it/s]Loading train:  74%|███████▍  | 211/285 [03:15<00:57,  1.28it/s]Loading train:  74%|███████▍  | 212/285 [03:16<00:57,  1.27it/s]Loading train:  75%|███████▍  | 213/285 [03:17<00:57,  1.26it/s]Loading train:  75%|███████▌  | 214/285 [03:17<00:57,  1.23it/s]Loading train:  75%|███████▌  | 215/285 [03:18<00:55,  1.27it/s]Loading train:  76%|███████▌  | 216/285 [03:19<00:55,  1.25it/s]Loading train:  76%|███████▌  | 217/285 [03:20<00:55,  1.23it/s]Loading train:  76%|███████▋  | 218/285 [03:21<00:53,  1.26it/s]Loading train:  77%|███████▋  | 219/285 [03:21<00:51,  1.29it/s]Loading train:  77%|███████▋  | 220/285 [03:22<00:50,  1.28it/s]Loading train:  78%|███████▊  | 221/285 [03:23<00:49,  1.29it/s]Loading train:  78%|███████▊  | 222/285 [03:24<00:49,  1.27it/s]Loading train:  78%|███████▊  | 223/285 [03:25<00:49,  1.26it/s]Loading train:  79%|███████▊  | 224/285 [03:25<00:46,  1.31it/s]Loading train:  79%|███████▉  | 225/285 [03:26<00:45,  1.32it/s]Loading train:  79%|███████▉  | 226/285 [03:27<00:45,  1.30it/s]Loading train:  80%|███████▉  | 227/285 [03:28<00:44,  1.29it/s]Loading train:  80%|████████  | 228/285 [03:28<00:44,  1.28it/s]Loading train:  80%|████████  | 229/285 [03:29<00:44,  1.25it/s]Loading train:  81%|████████  | 230/285 [03:30<00:43,  1.26it/s]Loading train:  81%|████████  | 231/285 [03:31<00:42,  1.28it/s]Loading train:  81%|████████▏ | 232/285 [03:32<00:45,  1.17it/s]Loading train:  82%|████████▏ | 233/285 [03:33<00:47,  1.10it/s]Loading train:  82%|████████▏ | 234/285 [03:34<00:47,  1.08it/s]Loading train:  82%|████████▏ | 235/285 [03:35<00:45,  1.09it/s]Loading train:  83%|████████▎ | 236/285 [03:36<00:47,  1.03it/s]Loading train:  83%|████████▎ | 237/285 [03:37<00:47,  1.02it/s]Loading train:  84%|████████▎ | 238/285 [03:38<00:46,  1.00it/s]Loading train:  84%|████████▍ | 239/285 [03:39<00:45,  1.02it/s]Loading train:  84%|████████▍ | 240/285 [03:40<00:44,  1.01it/s]Loading train:  85%|████████▍ | 241/285 [03:41<00:42,  1.02it/s]Loading train:  85%|████████▍ | 242/285 [03:42<00:44,  1.03s/it]Loading train:  85%|████████▌ | 243/285 [03:43<00:45,  1.08s/it]Loading train:  86%|████████▌ | 244/285 [03:44<00:42,  1.04s/it]Loading train:  86%|████████▌ | 245/285 [03:45<00:43,  1.09s/it]Loading train:  86%|████████▋ | 246/285 [03:46<00:42,  1.10s/it]Loading train:  87%|████████▋ | 247/285 [03:47<00:39,  1.04s/it]Loading train:  87%|████████▋ | 248/285 [03:48<00:39,  1.06s/it]Loading train:  87%|████████▋ | 249/285 [03:49<00:37,  1.06s/it]Loading train:  88%|████████▊ | 250/285 [03:50<00:33,  1.04it/s]Loading train:  88%|████████▊ | 251/285 [03:51<00:32,  1.05it/s]Loading train:  88%|████████▊ | 252/285 [03:52<00:28,  1.16it/s]Loading train:  89%|████████▉ | 253/285 [03:53<00:27,  1.18it/s]Loading train:  89%|████████▉ | 254/285 [03:53<00:25,  1.21it/s]Loading train:  89%|████████▉ | 255/285 [03:54<00:23,  1.26it/s]Loading train:  90%|████████▉ | 256/285 [03:55<00:23,  1.25it/s]Loading train:  90%|█████████ | 257/285 [03:56<00:21,  1.28it/s]Loading train:  91%|█████████ | 258/285 [03:56<00:21,  1.23it/s]Loading train:  91%|█████████ | 259/285 [03:57<00:20,  1.28it/s]Loading train:  91%|█████████ | 260/285 [03:58<00:19,  1.29it/s]Loading train:  92%|█████████▏| 261/285 [03:59<00:18,  1.28it/s]Loading train:  92%|█████████▏| 262/285 [03:59<00:17,  1.29it/s]Loading train:  92%|█████████▏| 263/285 [04:00<00:17,  1.28it/s]Loading train:  93%|█████████▎| 264/285 [04:01<00:16,  1.30it/s]Loading train:  93%|█████████▎| 265/285 [04:02<00:15,  1.27it/s]Loading train:  93%|█████████▎| 266/285 [04:02<00:14,  1.34it/s]Loading train:  94%|█████████▎| 267/285 [04:03<00:13,  1.37it/s]Loading train:  94%|█████████▍| 268/285 [04:04<00:14,  1.19it/s]Loading train:  94%|█████████▍| 269/285 [04:05<00:13,  1.17it/s]Loading train:  95%|█████████▍| 270/285 [04:06<00:13,  1.12it/s]Loading train:  95%|█████████▌| 271/285 [04:07<00:12,  1.15it/s]Loading train:  95%|█████████▌| 272/285 [04:08<00:12,  1.04it/s]Loading train:  96%|█████████▌| 273/285 [04:09<00:11,  1.06it/s]Loading train:  96%|█████████▌| 274/285 [04:10<00:10,  1.04it/s]Loading train:  96%|█████████▋| 275/285 [04:11<00:09,  1.02it/s]Loading train:  97%|█████████▋| 276/285 [04:12<00:08,  1.03it/s]Loading train:  97%|█████████▋| 277/285 [04:13<00:07,  1.03it/s]Loading train:  98%|█████████▊| 278/285 [04:14<00:06,  1.02it/s]Loading train:  98%|█████████▊| 279/285 [04:15<00:06,  1.01s/it]Loading train:  98%|█████████▊| 280/285 [04:16<00:04,  1.01it/s]Loading train:  99%|█████████▊| 281/285 [04:17<00:03,  1.02it/s]Loading train:  99%|█████████▉| 282/285 [04:18<00:02,  1.03it/s]Loading train:  99%|█████████▉| 283/285 [04:19<00:01,  1.02it/s]Loading train: 100%|█████████▉| 284/285 [04:20<00:00,  1.03it/s]Loading train: 100%|██████████| 285/285 [04:21<00:00,  1.01it/s]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|▏         | 4/285 [00:00<00:09, 30.43it/s]concatenating: train:  10%|▉         | 28/285 [00:00<00:06, 41.18it/s]concatenating: train:  21%|██        | 59/285 [00:00<00:04, 55.65it/s]concatenating: train:  32%|███▏      | 90/285 [00:00<00:02, 73.66it/s]concatenating: train:  42%|████▏     | 121/285 [00:00<00:01, 95.42it/s]concatenating: train:  54%|█████▎    | 153/285 [00:00<00:01, 120.82it/s]concatenating: train:  64%|██████▍   | 183/285 [00:00<00:00, 146.72it/s]concatenating: train:  75%|███████▌  | 214/285 [00:00<00:00, 167.78it/s]concatenating: train:  85%|████████▍ | 241/285 [00:01<00:00, 135.05it/s]concatenating: train:  96%|█████████▌| 274/285 [00:01<00:00, 163.60it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 220.55it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.37s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.34s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.31s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 28.54it/s]2019-07-05 19:30:00.231382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 19:30:00.231503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 19:30:00.231520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 19:30:00.231530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 19:30:00.231999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:14,  2.73it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:11,  3.24it/s]loading the weights for Unet:  10%|█         | 4/40 [00:01<00:11,  3.08it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:08,  3.92it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:09,  3.38it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:02<00:07,  3.80it/s]loading the weights for Unet:  30%|███       | 12/40 [00:02<00:08,  3.33it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:05,  4.18it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:03<00:07,  3.03it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:03<00:05,  3.51it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:04<00:06,  3.29it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:04<00:04,  4.03it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:05<00:03,  4.16it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:05<00:04,  3.38it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:05<00:03,  3.78it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:06<00:03,  3.32it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:06<00:01,  4.07it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:06<00:01,  4.34it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:07<00:01,  3.66it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:07<00:00,  4.06it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:08<00:00,  3.28it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:08<00:00,  4.91it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12, 12, 80), (None, 13, 13, 80)]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 30)   300         input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 30)   120         conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 30)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 30)   0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 30)   8130        dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 30)   120         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 30)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 30)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 30)   8130        dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 30)   120         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 30)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 52, 80, 30)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 20)   5420        dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 52, 80, 20)   80          conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 52, 80, 20)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 52, 80, 20)   3620        activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 52, 80, 20)   80          conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 52, 80, 20)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_16[0][0]              
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 26, 40, 20)   0           max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 26, 40, 40)   7240        dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 26, 40, 40)   160         conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 26, 40, 40)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 26, 40, 40)   14440       activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 26, 40, 40)   160         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 26, 40, 40)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_18[0][0]              
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 13, 20, 40)   0           max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 13, 20, 80)   28880       dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 13, 20, 80)   320         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 13, 20, 80)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 13, 20, 80)   57680       activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 13, 20, 80)   320         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 13, 20, 80)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 13, 20, 80)   0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_13[0][0]                 
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 26, 40, 40)   160         conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 26, 40, 40)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 26, 40, 40)   14440       activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 26, 40, 40)   160         conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 26, 40, 40)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 26, 40, 40)   0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_14[0][0]                 
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_3[0][0]         
                                                                 activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 52, 80, 20)   80          conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 52, 80, 20)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 52, 80, 20)   3620        activation_23[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 52, 80, 20)   80          conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 52, 80, 20)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 52, 80, 20)   0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 52, 80, 13)   273         dropout_15[0][0]                 
==================================================================================================
Total params: 206,253
Trainable params: 62,593
Non-trainable params: 143,660
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 20s - loss: 196.6551 - acc: 0.4137 - mDice: 0.0154 - val_loss: 46.8477 - val_acc: 0.9047 - val_mDice: 0.0119

Epoch 00001: val_mDice improved from -inf to 0.01189, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 11s - loss: 44.4858 - acc: 0.8226 - mDice: 0.0133 - val_loss: 16.5815 - val_acc: 0.9047 - val_mDice: 0.0101

Epoch 00002: val_mDice did not improve from 0.01189
Epoch 3/300
 - 11s - loss: 21.8230 - acc: 0.8568 - mDice: 0.0124 - val_loss: 9.6254 - val_acc: 0.9047 - val_mDice: 0.0093

Epoch 00003: val_mDice did not improve from 0.01189
Epoch 4/300
 - 11s - loss: 15.0320 - acc: 0.8637 - mDice: 0.0147 - val_loss: 7.7530 - val_acc: 0.9047 - val_mDice: 0.0096

Epoch 00004: val_mDice did not improve from 0.01189
Epoch 5/300
 - 11s - loss: 11.8530 - acc: 0.8666 - mDice: 0.0205 - val_loss: 6.8733 - val_acc: 0.9047 - val_mDice: 0.0178

Epoch 00005: val_mDice improved from 0.01189 to 0.01779, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 11s - loss: 10.0501 - acc: 0.8676 - mDice: 0.0252 - val_loss: 6.2490 - val_acc: 0.9047 - val_mDice: 0.0229

Epoch 00006: val_mDice improved from 0.01779 to 0.02290, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 11s - loss: 8.9960 - acc: 0.8680 - mDice: 0.0294 - val_loss: 5.9603 - val_acc: 0.9047 - val_mDice: 0.0243

Epoch 00007: val_mDice improved from 0.02290 to 0.02432, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 11s - loss: 8.2204 - acc: 0.8682 - mDice: 0.0345 - val_loss: 5.6420 - val_acc: 0.9047 - val_mDice: 0.0269

Epoch 00008: val_mDice improved from 0.02432 to 0.02692, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 9/300
 - 11s - loss: 7.5362 - acc: 0.8682 - mDice: 0.0423 - val_loss: 5.4768 - val_acc: 0.9047 - val_mDice: 0.0309

Epoch 00009: val_mDice improved from 0.02692 to 0.03094, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 11s - loss: 6.9419 - acc: 0.8680 - mDice: 0.0531 - val_loss: 5.1386 - val_acc: 0.9047 - val_mDice: 0.0461

Epoch 00010: val_mDice improved from 0.03094 to 0.04610, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 11s - loss: 6.3996 - acc: 0.8681 - mDice: 0.0668 - val_loss: 4.8239 - val_acc: 0.9047 - val_mDice: 0.0647

Epoch 00011: val_mDice improved from 0.04610 to 0.06468, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 11s - loss: 5.9528 - acc: 0.8701 - mDice: 0.0821 - val_loss: 4.5358 - val_acc: 0.9053 - val_mDice: 0.0862

Epoch 00012: val_mDice improved from 0.06468 to 0.08623, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 13/300
 - 11s - loss: 5.6292 - acc: 0.8731 - mDice: 0.0949 - val_loss: 4.4002 - val_acc: 0.9094 - val_mDice: 0.1015

Epoch 00013: val_mDice improved from 0.08623 to 0.10154, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 11s - loss: 5.3067 - acc: 0.8753 - mDice: 0.1085 - val_loss: 4.2361 - val_acc: 0.9098 - val_mDice: 0.1153

Epoch 00014: val_mDice improved from 0.10154 to 0.11532, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 11s - loss: 5.0469 - acc: 0.8771 - mDice: 0.1214 - val_loss: 4.1400 - val_acc: 0.9076 - val_mDice: 0.1295

Epoch 00015: val_mDice improved from 0.11532 to 0.12945, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 11s - loss: 4.8141 - acc: 0.8782 - mDice: 0.1354 - val_loss: 3.8391 - val_acc: 0.9099 - val_mDice: 0.1529

Epoch 00016: val_mDice improved from 0.12945 to 0.15293, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 11s - loss: 4.5540 - acc: 0.8790 - mDice: 0.1526 - val_loss: 3.6648 - val_acc: 0.9093 - val_mDice: 0.1705

Epoch 00017: val_mDice improved from 0.15293 to 0.17053, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 11s - loss: 4.3300 - acc: 0.8796 - mDice: 0.1708 - val_loss: 3.7298 - val_acc: 0.9098 - val_mDice: 0.1852

Epoch 00018: val_mDice improved from 0.17053 to 0.18520, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 19/300
 - 11s - loss: 4.1415 - acc: 0.8805 - mDice: 0.1872 - val_loss: 3.8128 - val_acc: 0.9101 - val_mDice: 0.1969

Epoch 00019: val_mDice improved from 0.18520 to 0.19692, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 11s - loss: 3.9558 - acc: 0.8820 - mDice: 0.2053 - val_loss: 3.4210 - val_acc: 0.9088 - val_mDice: 0.2297

Epoch 00020: val_mDice improved from 0.19692 to 0.22974, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 11s - loss: 3.7908 - acc: 0.8837 - mDice: 0.2214 - val_loss: 3.3582 - val_acc: 0.9114 - val_mDice: 0.2472

Epoch 00021: val_mDice improved from 0.22974 to 0.24716, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 11s - loss: 3.6496 - acc: 0.8863 - mDice: 0.2372 - val_loss: 3.3190 - val_acc: 0.9141 - val_mDice: 0.2704

Epoch 00022: val_mDice improved from 0.24716 to 0.27035, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 11s - loss: 3.5157 - acc: 0.8887 - mDice: 0.2527 - val_loss: 3.1415 - val_acc: 0.9178 - val_mDice: 0.2915

Epoch 00023: val_mDice improved from 0.27035 to 0.29153, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 11s - loss: 3.3989 - acc: 0.8904 - mDice: 0.2670 - val_loss: 3.1382 - val_acc: 0.9167 - val_mDice: 0.2995

Epoch 00024: val_mDice improved from 0.29153 to 0.29947, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 11s - loss: 3.2984 - acc: 0.8923 - mDice: 0.2790 - val_loss: 3.0630 - val_acc: 0.9217 - val_mDice: 0.3107

Epoch 00025: val_mDice improved from 0.29947 to 0.31074, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 11s - loss: 3.2190 - acc: 0.8934 - mDice: 0.2879 - val_loss: 2.9663 - val_acc: 0.9227 - val_mDice: 0.3252

Epoch 00026: val_mDice improved from 0.31074 to 0.32524, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 11s - loss: 3.1308 - acc: 0.8950 - mDice: 0.2990 - val_loss: 2.9906 - val_acc: 0.9216 - val_mDice: 0.3283

Epoch 00027: val_mDice improved from 0.32524 to 0.32826, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 28/300
 - 11s - loss: 3.0565 - acc: 0.8959 - mDice: 0.3082 - val_loss: 2.9625 - val_acc: 0.9253 - val_mDice: 0.3373

Epoch 00028: val_mDice improved from 0.32826 to 0.33731, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 11s - loss: 2.9839 - acc: 0.8969 - mDice: 0.3173 - val_loss: 2.9330 - val_acc: 0.9240 - val_mDice: 0.3331

Epoch 00029: val_mDice did not improve from 0.33731
Epoch 30/300
 - 11s - loss: 2.9345 - acc: 0.8980 - mDice: 0.3244 - val_loss: 3.0780 - val_acc: 0.9271 - val_mDice: 0.3340

Epoch 00030: val_mDice did not improve from 0.33731
Epoch 31/300
 - 11s - loss: 2.8792 - acc: 0.8992 - mDice: 0.3319 - val_loss: 3.0828 - val_acc: 0.9277 - val_mDice: 0.3343

Epoch 00031: val_mDice did not improve from 0.33731
Epoch 32/300
 - 11s - loss: 2.8322 - acc: 0.8999 - mDice: 0.3368 - val_loss: 2.8520 - val_acc: 0.9264 - val_mDice: 0.3512

Epoch 00032: val_mDice improved from 0.33731 to 0.35118, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 11s - loss: 2.7752 - acc: 0.9013 - mDice: 0.3462 - val_loss: 2.9439 - val_acc: 0.9295 - val_mDice: 0.3543

Epoch 00033: val_mDice improved from 0.35118 to 0.35435, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 11s - loss: 2.7380 - acc: 0.9022 - mDice: 0.3514 - val_loss: 2.9166 - val_acc: 0.9305 - val_mDice: 0.3567

Epoch 00034: val_mDice improved from 0.35435 to 0.35674, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 11s - loss: 2.7063 - acc: 0.9025 - mDice: 0.3554 - val_loss: 2.9822 - val_acc: 0.9260 - val_mDice: 0.3497

Epoch 00035: val_mDice did not improve from 0.35674
Epoch 36/300
 - 11s - loss: 2.6677 - acc: 0.9034 - mDice: 0.3617 - val_loss: 2.8809 - val_acc: 0.9300 - val_mDice: 0.3665

Epoch 00036: val_mDice improved from 0.35674 to 0.36652, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 11s - loss: 2.6197 - acc: 0.9046 - mDice: 0.3699 - val_loss: 3.0783 - val_acc: 0.9282 - val_mDice: 0.3525

Epoch 00037: val_mDice did not improve from 0.36652
Epoch 38/300
 - 11s - loss: 2.5942 - acc: 0.9053 - mDice: 0.3737 - val_loss: 2.9102 - val_acc: 0.9312 - val_mDice: 0.3678

Epoch 00038: val_mDice improved from 0.36652 to 0.36784, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 39/300
 - 11s - loss: 2.5565 - acc: 0.9064 - mDice: 0.3807 - val_loss: 3.0762 - val_acc: 0.9303 - val_mDice: 0.3664

Epoch 00039: val_mDice did not improve from 0.36784
Epoch 40/300
 - 11s - loss: 2.5353 - acc: 0.9067 - mDice: 0.3838 - val_loss: 3.0033 - val_acc: 0.9303 - val_mDice: 0.3735

Epoch 00040: val_mDice improved from 0.36784 to 0.37352, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 41/300
 - 11s - loss: 2.5024 - acc: 0.9073 - mDice: 0.3893 - val_loss: 3.0176 - val_acc: 0.9293 - val_mDice: 0.3714

Epoch 00041: val_mDice did not improve from 0.37352
Epoch 42/300
 - 11s - loss: 2.4816 - acc: 0.9079 - mDice: 0.3933 - val_loss: 3.0146 - val_acc: 0.9302 - val_mDice: 0.3713

Epoch 00042: val_mDice did not improve from 0.37352
Epoch 43/300
 - 11s - loss: 2.4571 - acc: 0.9085 - mDice: 0.3976 - val_loss: 3.0304 - val_acc: 0.9303 - val_mDice: 0.3755

Epoch 00043: val_mDice improved from 0.37352 to 0.37553, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 44/300
 - 11s - loss: 2.4143 - acc: 0.9096 - mDice: 0.4057 - val_loss: 2.9791 - val_acc: 0.9330 - val_mDice: 0.3903

Epoch 00044: val_mDice improved from 0.37553 to 0.39029, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 45/300
 - 11s - loss: 2.4039 - acc: 0.9099 - mDice: 0.4077 - val_loss: 2.9380 - val_acc: 0.9329 - val_mDice: 0.3916

Epoch 00045: val_mDice improved from 0.39029 to 0.39155, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 46/300
 - 11s - loss: 2.3741 - acc: 0.9107 - mDice: 0.4136 - val_loss: 2.9768 - val_acc: 0.9323 - val_mDice: 0.3867

Epoch 00046: val_mDice did not improve from 0.39155
Epoch 47/300
 - 11s - loss: 2.3517 - acc: 0.9113 - mDice: 0.4176 - val_loss: 2.8945 - val_acc: 0.9318 - val_mDice: 0.3931

Epoch 00047: val_mDice improved from 0.39155 to 0.39306, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 11s - loss: 2.3310 - acc: 0.9118 - mDice: 0.4210 - val_loss: 3.0356 - val_acc: 0.9312 - val_mDice: 0.3915

Epoch 00048: val_mDice did not improve from 0.39306
Epoch 49/300
 - 11s - loss: 2.3124 - acc: 0.9122 - mDice: 0.4251 - val_loss: 2.8639 - val_acc: 0.9340 - val_mDice: 0.3971

Epoch 00049: val_mDice improved from 0.39306 to 0.39706, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 50/300
 - 11s - loss: 2.2984 - acc: 0.9125 - mDice: 0.4276 - val_loss: 2.9106 - val_acc: 0.9338 - val_mDice: 0.3999

Epoch 00050: val_mDice improved from 0.39706 to 0.39989, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 11s - loss: 2.2818 - acc: 0.9128 - mDice: 0.4300 - val_loss: 2.8877 - val_acc: 0.9340 - val_mDice: 0.4021

Epoch 00051: val_mDice improved from 0.39989 to 0.40212, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 11s - loss: 2.2528 - acc: 0.9137 - mDice: 0.4360 - val_loss: 3.1412 - val_acc: 0.9329 - val_mDice: 0.3943

Epoch 00052: val_mDice did not improve from 0.40212
Epoch 53/300
 - 11s - loss: 2.2511 - acc: 0.9139 - mDice: 0.4372 - val_loss: 2.9563 - val_acc: 0.9352 - val_mDice: 0.4063

Epoch 00053: val_mDice improved from 0.40212 to 0.40625, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 11s - loss: 2.2295 - acc: 0.9146 - mDice: 0.4413 - val_loss: 3.1749 - val_acc: 0.9345 - val_mDice: 0.3996

Epoch 00054: val_mDice did not improve from 0.40625
Epoch 55/300
 - 11s - loss: 2.2222 - acc: 0.9148 - mDice: 0.4428 - val_loss: 3.5768 - val_acc: 0.9318 - val_mDice: 0.3761

Epoch 00055: val_mDice did not improve from 0.40625
Epoch 56/300
 - 11s - loss: 2.2067 - acc: 0.9153 - mDice: 0.4458 - val_loss: 3.3298 - val_acc: 0.9270 - val_mDice: 0.3719

Epoch 00056: val_mDice did not improve from 0.40625
Epoch 57/300
 - 11s - loss: 2.1864 - acc: 0.9156 - mDice: 0.4501 - val_loss: 2.9907 - val_acc: 0.9356 - val_mDice: 0.4082

Epoch 00057: val_mDice improved from 0.40625 to 0.40817, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 58/300
 - 11s - loss: 2.1731 - acc: 0.9158 - mDice: 0.4523 - val_loss: 2.9811 - val_acc: 0.9339 - val_mDice: 0.4045

Epoch 00058: val_mDice did not improve from 0.40817
Epoch 59/300
 - 11s - loss: 2.1748 - acc: 0.9155 - mDice: 0.4520 - val_loss: 3.1145 - val_acc: 0.9350 - val_mDice: 0.4068

Epoch 00059: val_mDice did not improve from 0.40817
Epoch 60/300
 - 11s - loss: 2.1472 - acc: 0.9166 - mDice: 0.4576 - val_loss: 2.9438 - val_acc: 0.9368 - val_mDice: 0.4157

Epoch 00060: val_mDice improved from 0.40817 to 0.41569, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 61/300
 - 11s - loss: 2.1348 - acc: 0.9169 - mDice: 0.4597 - val_loss: 2.9859 - val_acc: 0.9361 - val_mDice: 0.4140

Epoch 00061: val_mDice did not improve from 0.41569
Epoch 62/300
 - 11s - loss: 2.1160 - acc: 0.9172 - mDice: 0.4643 - val_loss: 2.9221 - val_acc: 0.9366 - val_mDice: 0.4175

Epoch 00062: val_mDice improved from 0.41569 to 0.41748, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 63/300
 - 11s - loss: 2.1025 - acc: 0.9175 - mDice: 0.4670 - val_loss: 3.0338 - val_acc: 0.9329 - val_mDice: 0.4108

Epoch 00063: val_mDice did not improve from 0.41748
Epoch 64/300
 - 11s - loss: 2.1023 - acc: 0.9173 - mDice: 0.4677 - val_loss: 2.9336 - val_acc: 0.9365 - val_mDice: 0.4174

Epoch 00064: val_mDice did not improve from 0.41748
Epoch 65/300
 - 11s - loss: 2.0771 - acc: 0.9180 - mDice: 0.4719 - val_loss: 3.0932 - val_acc: 0.9338 - val_mDice: 0.4109

Epoch 00065: val_mDice did not improve from 0.41748
Epoch 66/300
 - 11s - loss: 2.0672 - acc: 0.9183 - mDice: 0.4744 - val_loss: 3.2464 - val_acc: 0.9352 - val_mDice: 0.4131

Epoch 00066: val_mDice did not improve from 0.41748
Epoch 67/300
 - 11s - loss: 2.0654 - acc: 0.9182 - mDice: 0.4751 - val_loss: 3.2079 - val_acc: 0.9347 - val_mDice: 0.4093

Epoch 00067: val_mDice did not improve from 0.41748
Epoch 68/300
 - 11s - loss: 2.0534 - acc: 0.9185 - mDice: 0.4777 - val_loss: 3.3720 - val_acc: 0.9328 - val_mDice: 0.3972

Epoch 00068: val_mDice did not improve from 0.41748
Epoch 69/300
 - 11s - loss: 2.0453 - acc: 0.9186 - mDice: 0.4798 - val_loss: 3.1067 - val_acc: 0.9346 - val_mDice: 0.4140

Epoch 00069: val_mDice did not improve from 0.41748
Epoch 70/300
 - 11s - loss: 2.0333 - acc: 0.9188 - mDice: 0.4817 - val_loss: 3.2273 - val_acc: 0.9338 - val_mDice: 0.4086

Epoch 00070: val_mDice did not improve from 0.41748
Epoch 71/300
 - 11s - loss: 2.0161 - acc: 0.9192 - mDice: 0.4854 - val_loss: 3.2540 - val_acc: 0.9360 - val_mDice: 0.4134

Epoch 00071: val_mDice did not improve from 0.41748
Epoch 72/300
 - 11s - loss: 2.0067 - acc: 0.9195 - mDice: 0.4876 - val_loss: 3.2627 - val_acc: 0.9367 - val_mDice: 0.4131

Epoch 00072: val_mDice did not improve from 0.41748
Epoch 73/300
 - 11s - loss: 2.0039 - acc: 0.9196 - mDice: 0.4880 - val_loss: 3.1304 - val_acc: 0.9333 - val_mDice: 0.4208

Epoch 00073: val_mDice improved from 0.41748 to 0.42081, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 74/300
 - 11s - loss: 1.9936 - acc: 0.9200 - mDice: 0.4906 - val_loss: 3.0049 - val_acc: 0.9372 - val_mDice: 0.4290

Epoch 00074: val_mDice improved from 0.42081 to 0.42895, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 75/300
 - 11s - loss: 1.9854 - acc: 0.9200 - mDice: 0.4920 - val_loss: 3.1222 - val_acc: 0.9366 - val_mDice: 0.4245

Epoch 00075: val_mDice did not improve from 0.42895
Epoch 76/300
 - 11s - loss: 1.9727 - acc: 0.9202 - mDice: 0.4953 - val_loss: 3.1171 - val_acc: 0.9359 - val_mDice: 0.4234

Epoch 00076: val_mDice did not improve from 0.42895
Epoch 77/300
 - 11s - loss: 1.9655 - acc: 0.9205 - mDice: 0.4966 - val_loss: 3.4125 - val_acc: 0.9332 - val_mDice: 0.4156

Epoch 00077: val_mDice did not improve from 0.42895
Epoch 78/300
 - 11s - loss: 1.9586 - acc: 0.9209 - mDice: 0.4989 - val_loss: 3.1986 - val_acc: 0.9347 - val_mDice: 0.4071

Epoch 00078: val_mDice did not improve from 0.42895
Epoch 79/300
 - 12s - loss: 1.9528 - acc: 0.9209 - mDice: 0.4999 - val_loss: 3.2246 - val_acc: 0.9345 - val_mDice: 0.4172

Epoch 00079: val_mDice did not improve from 0.42895
Epoch 80/300
 - 11s - loss: 1.9397 - acc: 0.9211 - mDice: 0.5016 - val_loss: 2.9918 - val_acc: 0.9375 - val_mDice: 0.4284

Epoch 00080: val_mDice did not improve from 0.42895
Epoch 81/300
 - 12s - loss: 1.9315 - acc: 0.9213 - mDice: 0.5039 - val_loss: 3.1607 - val_acc: 0.9373 - val_mDice: 0.4314

Epoch 00081: val_mDice improved from 0.42895 to 0.43143, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 82/300
 - 12s - loss: 1.9332 - acc: 0.9214 - mDice: 0.5044 - val_loss: 3.3214 - val_acc: 0.9375 - val_mDice: 0.4231

Epoch 00082: val_mDice did not improve from 0.43143
Epoch 83/300
 - 11s - loss: 1.9138 - acc: 0.9216 - mDice: 0.5072 - val_loss: 3.2175 - val_acc: 0.9356 - val_mDice: 0.4236

Epoch 00083: val_mDice did not improve from 0.43143
Epoch 84/300
 - 11s - loss: 1.9126 - acc: 0.9218 - mDice: 0.5082 - val_loss: 3.4268 - val_acc: 0.9327 - val_mDice: 0.4026

Epoch 00084: val_mDice did not improve from 0.43143
Epoch 85/300
 - 11s - loss: 1.9079 - acc: 0.9215 - mDice: 0.5088 - val_loss: 3.3418 - val_acc: 0.9360 - val_mDice: 0.4183

Epoch 00085: val_mDice did not improve from 0.43143
Epoch 86/300
 - 11s - loss: 1.8992 - acc: 0.9218 - mDice: 0.5112 - val_loss: 3.1897 - val_acc: 0.9334 - val_mDice: 0.4110

Epoch 00086: val_mDice did not improve from 0.43143
Epoch 87/300
 - 11s - loss: 1.8836 - acc: 0.9224 - mDice: 0.5142 - val_loss: 3.0777 - val_acc: 0.9386 - val_mDice: 0.4403

Epoch 00087: val_mDice improved from 0.43143 to 0.44032, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 88/300
 - 11s - loss: 1.8832 - acc: 0.9224 - mDice: 0.5145 - val_loss: 3.0305 - val_acc: 0.9377 - val_mDice: 0.4411

Epoch 00088: val_mDice improved from 0.44032 to 0.44107, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 89/300
 - 11s - loss: 1.8703 - acc: 0.9228 - mDice: 0.5170 - val_loss: 3.0447 - val_acc: 0.9375 - val_mDice: 0.4356

Epoch 00089: val_mDice did not improve from 0.44107
Epoch 90/300
 - 11s - loss: 1.8700 - acc: 0.9227 - mDice: 0.5175 - val_loss: 3.2230 - val_acc: 0.9367 - val_mDice: 0.4221

Epoch 00090: val_mDice did not improve from 0.44107
Epoch 91/300
 - 11s - loss: 1.8612 - acc: 0.9231 - mDice: 0.5193 - val_loss: 3.1262 - val_acc: 0.9383 - val_mDice: 0.4454

Epoch 00091: val_mDice improved from 0.44107 to 0.44541, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 92/300
 - 11s - loss: 1.8507 - acc: 0.9233 - mDice: 0.5215 - val_loss: 3.2143 - val_acc: 0.9374 - val_mDice: 0.4359

Epoch 00092: val_mDice did not improve from 0.44541
Epoch 93/300
 - 11s - loss: 1.8521 - acc: 0.9231 - mDice: 0.5212 - val_loss: 3.3120 - val_acc: 0.9377 - val_mDice: 0.4310

Epoch 00093: val_mDice did not improve from 0.44541
Epoch 94/300
 - 11s - loss: 1.8402 - acc: 0.9235 - mDice: 0.5251 - val_loss: 3.5217 - val_acc: 0.9377 - val_mDice: 0.4234

Epoch 00094: val_mDice did not improve from 0.44541
Epoch 95/300
 - 11s - loss: 1.8354 - acc: 0.9236 - mDice: 0.5253 - val_loss: 3.0686 - val_acc: 0.9391 - val_mDice: 0.4456

Epoch 00095: val_mDice improved from 0.44541 to 0.44563, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 96/300
 - 11s - loss: 1.8386 - acc: 0.9234 - mDice: 0.5248 - val_loss: 3.3902 - val_acc: 0.9384 - val_mDice: 0.4328

Epoch 00096: val_mDice did not improve from 0.44563
Epoch 97/300
 - 11s - loss: 1.8320 - acc: 0.9236 - mDice: 0.5254 - val_loss: 3.0576 - val_acc: 0.9364 - val_mDice: 0.4430

Epoch 00097: val_mDice did not improve from 0.44563
Epoch 98/300
 - 11s - loss: 1.8200 - acc: 0.9241 - mDice: 0.5287 - val_loss: 2.9895 - val_acc: 0.9383 - val_mDice: 0.4438

Epoch 00098: val_mDice did not improve from 0.44563
Epoch 99/300
 - 11s - loss: 1.8111 - acc: 0.9242 - mDice: 0.5313 - val_loss: 3.3221 - val_acc: 0.9388 - val_mDice: 0.4377

Epoch 00099: val_mDice did not improve from 0.44563
Epoch 100/300
 - 11s - loss: 1.8133 - acc: 0.9243 - mDice: 0.5307 - val_loss: 3.2327 - val_acc: 0.9382 - val_mDice: 0.4440

Epoch 00100: val_mDice did not improve from 0.44563
Epoch 101/300
 - 11s - loss: 1.8034 - acc: 0.9246 - mDice: 0.5328 - val_loss: 3.0750 - val_acc: 0.9383 - val_mDice: 0.4475

Epoch 00101: val_mDice improved from 0.44563 to 0.44748, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 102/300
 - 11s - loss: 1.8003 - acc: 0.9245 - mDice: 0.5339 - val_loss: 3.2580 - val_acc: 0.9377 - val_mDice: 0.4388

Epoch 00102: val_mDice did not improve from 0.44748
Epoch 103/300
 - 11s - loss: 1.7927 - acc: 0.9246 - mDice: 0.5353 - val_loss: 3.0856 - val_acc: 0.9395 - val_mDice: 0.4487

Epoch 00103: val_mDice improved from 0.44748 to 0.44872, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 104/300
 - 11s - loss: 1.7842 - acc: 0.9250 - mDice: 0.5375 - val_loss: 3.2161 - val_acc: 0.9359 - val_mDice: 0.4346

Epoch 00104: val_mDice did not improve from 0.44872
Epoch 105/300
 - 11s - loss: 1.7808 - acc: 0.9252 - mDice: 0.5373 - val_loss: 3.6922 - val_acc: 0.9338 - val_mDice: 0.4143

Epoch 00105: val_mDice did not improve from 0.44872
Epoch 106/300
 - 11s - loss: 1.7804 - acc: 0.9251 - mDice: 0.5382 - val_loss: 3.2031 - val_acc: 0.9359 - val_mDice: 0.4368

Epoch 00106: val_mDice did not improve from 0.44872
Epoch 107/300
 - 11s - loss: 1.7701 - acc: 0.9253 - mDice: 0.5405 - val_loss: 3.0033 - val_acc: 0.9410 - val_mDice: 0.4547

Epoch 00107: val_mDice improved from 0.44872 to 0.45469, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 108/300
 - 11s - loss: 1.7674 - acc: 0.9255 - mDice: 0.5411 - val_loss: 3.4134 - val_acc: 0.9396 - val_mDice: 0.4456

Epoch 00108: val_mDice did not improve from 0.45469
Epoch 109/300
 - 11s - loss: 1.7625 - acc: 0.9256 - mDice: 0.5425 - val_loss: 3.1886 - val_acc: 0.9387 - val_mDice: 0.4368

Epoch 00109: val_mDice did not improve from 0.45469
Epoch 110/300
 - 11s - loss: 1.7580 - acc: 0.9258 - mDice: 0.5433 - val_loss: 3.2567 - val_acc: 0.9390 - val_mDice: 0.4457

Epoch 00110: val_mDice did not improve from 0.45469
Epoch 111/300
 - 11s - loss: 1.7554 - acc: 0.9259 - mDice: 0.5440 - val_loss: 3.5986 - val_acc: 0.9401 - val_mDice: 0.4345

Epoch 00111: val_mDice did not improve from 0.45469
Epoch 112/300
 - 11s - loss: 1.7540 - acc: 0.9258 - mDice: 0.5432 - val_loss: 3.1065 - val_acc: 0.9371 - val_mDice: 0.4444

Epoch 00112: val_mDice did not improve from 0.45469
Epoch 113/300
 - 11s - loss: 1.7435 - acc: 0.9261 - mDice: 0.5465 - val_loss: 3.2170 - val_acc: 0.9397 - val_mDice: 0.4462

Epoch 00113: val_mDice did not improve from 0.45469
Epoch 114/300
 - 11s - loss: 1.7472 - acc: 0.9261 - mDice: 0.5457 - val_loss: 3.5569 - val_acc: 0.9363 - val_mDice: 0.4375

Epoch 00114: val_mDice did not improve from 0.45469
Epoch 115/300
 - 11s - loss: 1.7388 - acc: 0.9263 - mDice: 0.5479 - val_loss: 3.3441 - val_acc: 0.9375 - val_mDice: 0.4438

Epoch 00115: val_mDice did not improve from 0.45469
Epoch 116/300
 - 11s - loss: 1.7328 - acc: 0.9264 - mDice: 0.5484 - val_loss: 3.3051 - val_acc: 0.9386 - val_mDice: 0.4433

Epoch 00116: val_mDice did not improve from 0.45469
Epoch 117/300
 - 11s - loss: 1.7248 - acc: 0.9267 - mDice: 0.5515 - val_loss: 3.2247 - val_acc: 0.9382 - val_mDice: 0.4458

Epoch 00117: val_mDice did not improve from 0.45469
Epoch 118/300
 - 11s - loss: 1.7281 - acc: 0.9265 - mDice: 0.5496 - val_loss: 3.2836 - val_acc: 0.9394 - val_mDice: 0.4479

Epoch 00118: val_mDice did not improve from 0.45469
Epoch 119/300
 - 11s - loss: 1.7126 - acc: 0.9269 - mDice: 0.5536 - val_loss: 3.5354 - val_acc: 0.9400 - val_mDice: 0.4414

Epoch 00119: val_mDice did not improve from 0.45469
Epoch 120/300
 - 11s - loss: 1.7163 - acc: 0.9266 - mDice: 0.5531 - val_loss: 3.0622 - val_acc: 0.9370 - val_mDice: 0.4434

Epoch 00120: val_mDice did not improve from 0.45469
Epoch 121/300
 - 11s - loss: 1.7080 - acc: 0.9269 - mDice: 0.5548 - val_loss: 3.1695 - val_acc: 0.9399 - val_mDice: 0.4562

Epoch 00121: val_mDice improved from 0.45469 to 0.45621, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 122/300
 - 11s - loss: 1.7048 - acc: 0.9272 - mDice: 0.5563 - val_loss: 3.4041 - val_acc: 0.9383 - val_mDice: 0.4422

Epoch 00122: val_mDice did not improve from 0.45621
Epoch 123/300
 - 11s - loss: 1.6975 - acc: 0.9273 - mDice: 0.5582 - val_loss: 3.5692 - val_acc: 0.9370 - val_mDice: 0.4370

Epoch 00123: val_mDice did not improve from 0.45621
Epoch 124/300
 - 11s - loss: 1.6935 - acc: 0.9274 - mDice: 0.5587 - val_loss: 3.2634 - val_acc: 0.9389 - val_mDice: 0.4510

Epoch 00124: val_mDice did not improve from 0.45621
Epoch 125/300
 - 11s - loss: 1.6864 - acc: 0.9278 - mDice: 0.5606 - val_loss: 3.1017 - val_acc: 0.9397 - val_mDice: 0.4568

Epoch 00125: val_mDice improved from 0.45621 to 0.45679, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 126/300
 - 11s - loss: 1.6909 - acc: 0.9276 - mDice: 0.5598 - val_loss: 3.2016 - val_acc: 0.9411 - val_mDice: 0.4533

Epoch 00126: val_mDice did not improve from 0.45679
Epoch 127/300
 - 11s - loss: 1.6806 - acc: 0.9279 - mDice: 0.5613 - val_loss: 3.1351 - val_acc: 0.9387 - val_mDice: 0.4562

Epoch 00127: val_mDice did not improve from 0.45679
Epoch 128/300
 - 11s - loss: 1.6840 - acc: 0.9279 - mDice: 0.5617 - val_loss: 3.1007 - val_acc: 0.9393 - val_mDice: 0.4519

Epoch 00128: val_mDice did not improve from 0.45679
Epoch 129/300
 - 11s - loss: 1.6752 - acc: 0.9282 - mDice: 0.5632 - val_loss: 3.3128 - val_acc: 0.9392 - val_mDice: 0.4517

Epoch 00129: val_mDice did not improve from 0.45679
Epoch 130/300
 - 11s - loss: 1.6685 - acc: 0.9284 - mDice: 0.5648 - val_loss: 3.3402 - val_acc: 0.9376 - val_mDice: 0.4448

Epoch 00130: val_mDice did not improve from 0.45679
Epoch 131/300
 - 11s - loss: 1.6668 - acc: 0.9285 - mDice: 0.5657 - val_loss: 3.1793 - val_acc: 0.9400 - val_mDice: 0.4481

Epoch 00131: val_mDice did not improve from 0.45679
Epoch 132/300
 - 11s - loss: 1.6635 - acc: 0.9283 - mDice: 0.5657 - val_loss: 3.1025 - val_acc: 0.9382 - val_mDice: 0.4552

Epoch 00132: val_mDice did not improve from 0.45679
Epoch 133/300
 - 11s - loss: 1.6685 - acc: 0.9284 - mDice: 0.5656 - val_loss: 3.3805 - val_acc: 0.9399 - val_mDice: 0.4527

Epoch 00133: val_mDice did not improve from 0.45679
Epoch 134/300
 - 11s - loss: 1.6628 - acc: 0.9286 - mDice: 0.5659 - val_loss: 3.2989 - val_acc: 0.9399 - val_mDice: 0.4460

Epoch 00134: val_mDice did not improve from 0.45679
Epoch 135/300
 - 11s - loss: 1.6513 - acc: 0.9288 - mDice: 0.5687 - val_loss: 3.3955 - val_acc: 0.9405 - val_mDice: 0.4487

Epoch 00135: val_mDice did not improve from 0.45679
Epoch 136/300
 - 11s - loss: 1.6529 - acc: 0.9289 - mDice: 0.5684 - val_loss: 3.0935 - val_acc: 0.9403 - val_mDice: 0.4580

Epoch 00136: val_mDice improved from 0.45679 to 0.45802, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 137/300
 - 11s - loss: 1.6572 - acc: 0.9287 - mDice: 0.5679 - val_loss: 3.4325 - val_acc: 0.9386 - val_mDice: 0.4455

Epoch 00137: val_mDice did not improve from 0.45802
Epoch 138/300
 - 11s - loss: 1.6463 - acc: 0.9290 - mDice: 0.5703 - val_loss: 3.3977 - val_acc: 0.9416 - val_mDice: 0.4608

Epoch 00138: val_mDice improved from 0.45802 to 0.46081, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 139/300
 - 11s - loss: 1.6469 - acc: 0.9291 - mDice: 0.5703 - val_loss: 3.3132 - val_acc: 0.9369 - val_mDice: 0.4421

Epoch 00139: val_mDice did not improve from 0.46081
Epoch 140/300
 - 11s - loss: 1.6486 - acc: 0.9289 - mDice: 0.5695 - val_loss: 3.3912 - val_acc: 0.9398 - val_mDice: 0.4434

Epoch 00140: val_mDice did not improve from 0.46081
Epoch 141/300
 - 11s - loss: 1.6328 - acc: 0.9295 - mDice: 0.5731 - val_loss: 3.3443 - val_acc: 0.9401 - val_mDice: 0.4553

Epoch 00141: val_mDice did not improve from 0.46081
Epoch 142/300
 - 11s - loss: 1.6395 - acc: 0.9295 - mDice: 0.5721 - val_loss: 3.4598 - val_acc: 0.9408 - val_mDice: 0.4487

Epoch 00142: val_mDice did not improve from 0.46081
Epoch 143/300
 - 11s - loss: 1.6297 - acc: 0.9297 - mDice: 0.5743 - val_loss: 3.2402 - val_acc: 0.9385 - val_mDice: 0.4614

Epoch 00143: val_mDice improved from 0.46081 to 0.46138, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 144/300
 - 11s - loss: 1.6306 - acc: 0.9297 - mDice: 0.5741 - val_loss: 3.1538 - val_acc: 0.9414 - val_mDice: 0.4675

Epoch 00144: val_mDice improved from 0.46138 to 0.46747, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 145/300
 - 11s - loss: 1.6307 - acc: 0.9298 - mDice: 0.5740 - val_loss: 3.4067 - val_acc: 0.9424 - val_mDice: 0.4565

Epoch 00145: val_mDice did not improve from 0.46747
Epoch 146/300
 - 11s - loss: 1.6212 - acc: 0.9301 - mDice: 0.5762 - val_loss: 3.2976 - val_acc: 0.9405 - val_mDice: 0.4571

Epoch 00146: val_mDice did not improve from 0.46747
Epoch 147/300
 - 11s - loss: 1.6194 - acc: 0.9302 - mDice: 0.5772 - val_loss: 3.4073 - val_acc: 0.9414 - val_mDice: 0.4527

Epoch 00147: val_mDice did not improve from 0.46747
Epoch 148/300
 - 11s - loss: 1.6197 - acc: 0.9301 - mDice: 0.5768 - val_loss: 3.3973 - val_acc: 0.9418 - val_mDice: 0.4576

Epoch 00148: val_mDice did not improve from 0.46747
Epoch 149/300
 - 11s - loss: 1.6193 - acc: 0.9301 - mDice: 0.5771 - val_loss: 3.1906 - val_acc: 0.9425 - val_mDice: 0.4567

Epoch 00149: val_mDice did not improve from 0.46747
Epoch 150/300
 - 11s - loss: 1.6141 - acc: 0.9302 - mDice: 0.5778 - val_loss: 3.1179 - val_acc: 0.9428 - val_mDice: 0.4659

Epoch 00150: val_mDice did not improve from 0.46747
Epoch 151/300
 - 11s - loss: 1.6129 - acc: 0.9304 - mDice: 0.5794 - val_loss: 3.2521 - val_acc: 0.9404 - val_mDice: 0.4577

Epoch 00151: val_mDice did not improve from 0.46747
Epoch 152/300
 - 11s - loss: 1.6034 - acc: 0.9306 - mDice: 0.5808 - val_loss: 3.3884 - val_acc: 0.9399 - val_mDice: 0.4529

Epoch 00152: val_mDice did not improve from 0.46747
Epoch 153/300
 - 11s - loss: 1.6098 - acc: 0.9305 - mDice: 0.5797 - val_loss: 3.4474 - val_acc: 0.9404 - val_mDice: 0.4521

Epoch 00153: val_mDice did not improve from 0.46747
Epoch 154/300
 - 11s - loss: 1.6110 - acc: 0.9305 - mDice: 0.5785 - val_loss: 3.3343 - val_acc: 0.9405 - val_mDice: 0.4568

Epoch 00154: val_mDice did not improve from 0.46747
Epoch 155/300
 - 11s - loss: 1.6082 - acc: 0.9307 - mDice: 0.5799 - val_loss: 3.2946 - val_acc: 0.9362 - val_mDice: 0.4551

Epoch 00155: val_mDice did not improve from 0.46747
Epoch 156/300
 - 11s - loss: 1.6004 - acc: 0.9308 - mDice: 0.5817 - val_loss: 3.4965 - val_acc: 0.9412 - val_mDice: 0.4552

Epoch 00156: val_mDice did not improve from 0.46747
Epoch 157/300
 - 11s - loss: 1.6034 - acc: 0.9309 - mDice: 0.5818 - val_loss: 3.2438 - val_acc: 0.9435 - val_mDice: 0.4674

Epoch 00157: val_mDice did not improve from 0.46747
Epoch 158/300
 - 11s - loss: 1.5942 - acc: 0.9311 - mDice: 0.5828 - val_loss: 3.2319 - val_acc: 0.9416 - val_mDice: 0.4592

Epoch 00158: val_mDice did not improve from 0.46747
Epoch 159/300
 - 11s - loss: 1.5919 - acc: 0.9313 - mDice: 0.5836 - val_loss: 3.2735 - val_acc: 0.9404 - val_mDice: 0.4517

Epoch 00159: val_mDice did not improve from 0.46747
Epoch 160/300
 - 11s - loss: 1.5902 - acc: 0.9314 - mDice: 0.5850 - val_loss: 3.3576 - val_acc: 0.9410 - val_mDice: 0.4494

Epoch 00160: val_mDice did not improve from 0.46747
Epoch 161/300
 - 11s - loss: 1.5855 - acc: 0.9314 - mDice: 0.5852 - val_loss: 3.5273 - val_acc: 0.9415 - val_mDice: 0.4519

Epoch 00161: val_mDice did not improve from 0.46747
Epoch 162/300
 - 11s - loss: 1.5911 - acc: 0.9313 - mDice: 0.5842 - val_loss: 3.2564 - val_acc: 0.9432 - val_mDice: 0.4694

Epoch 00162: val_mDice improved from 0.46747 to 0.46944, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 163/300
 - 11s - loss: 1.5878 - acc: 0.9312 - mDice: 0.5844 - val_loss: 3.4399 - val_acc: 0.9407 - val_mDice: 0.4512

Epoch 00163: val_mDice did not improve from 0.46944
Epoch 164/300
 - 11s - loss: 1.5913 - acc: 0.9314 - mDice: 0.5847 - val_loss: 3.2518 - val_acc: 0.9398 - val_mDice: 0.4599

Epoch 00164: val_mDice did not improve from 0.46944
Epoch 165/300
 - 11s - loss: 1.5757 - acc: 0.9318 - mDice: 0.5877 - val_loss: 3.4544 - val_acc: 0.9393 - val_mDice: 0.4540

Epoch 00165: val_mDice did not improve from 0.46944
Epoch 166/300
 - 11s - loss: 1.5752 - acc: 0.9319 - mDice: 0.5883 - val_loss: 3.4573 - val_acc: 0.9419 - val_mDice: 0.4567

Epoch 00166: val_mDice did not improve from 0.46944
Epoch 167/300
 - 11s - loss: 1.5759 - acc: 0.9319 - mDice: 0.5883 - val_loss: 3.4803 - val_acc: 0.9379 - val_mDice: 0.4502

Epoch 00167: val_mDice did not improve from 0.46944
Epoch 168/300
 - 11s - loss: 1.5793 - acc: 0.9319 - mDice: 0.5875 - val_loss: 3.1652 - val_acc: 0.9409 - val_mDice: 0.4679

Epoch 00168: val_mDice did not improve from 0.46944
Epoch 169/300
 - 11s - loss: 1.5646 - acc: 0.9321 - mDice: 0.5903 - val_loss: 3.4664 - val_acc: 0.9388 - val_mDice: 0.4507

Epoch 00169: val_mDice did not improve from 0.46944
Epoch 170/300
 - 11s - loss: 1.5728 - acc: 0.9320 - mDice: 0.5887 - val_loss: 3.2550 - val_acc: 0.9419 - val_mDice: 0.4723

Epoch 00170: val_mDice improved from 0.46944 to 0.47231, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 171/300
 - 11s - loss: 1.5678 - acc: 0.9324 - mDice: 0.5908 - val_loss: 3.2737 - val_acc: 0.9418 - val_mDice: 0.4621

Epoch 00171: val_mDice did not improve from 0.47231
Epoch 172/300
 - 11s - loss: 1.5650 - acc: 0.9324 - mDice: 0.5909 - val_loss: 3.3566 - val_acc: 0.9379 - val_mDice: 0.4535

Epoch 00172: val_mDice did not improve from 0.47231
Epoch 173/300
 - 11s - loss: 1.5689 - acc: 0.9324 - mDice: 0.5904 - val_loss: 3.2299 - val_acc: 0.9427 - val_mDice: 0.4661

Epoch 00173: val_mDice did not improve from 0.47231
Epoch 174/300
 - 11s - loss: 1.5636 - acc: 0.9326 - mDice: 0.5913 - val_loss: 3.1984 - val_acc: 0.9410 - val_mDice: 0.4697

Epoch 00174: val_mDice did not improve from 0.47231
Epoch 175/300
 - 11s - loss: 1.5602 - acc: 0.9327 - mDice: 0.5923 - val_loss: 3.1405 - val_acc: 0.9433 - val_mDice: 0.4712

Epoch 00175: val_mDice did not improve from 0.47231
Epoch 176/300
 - 11s - loss: 1.5585 - acc: 0.9327 - mDice: 0.5928 - val_loss: 3.4404 - val_acc: 0.9429 - val_mDice: 0.4632

Epoch 00176: val_mDice did not improve from 0.47231
Epoch 177/300
 - 11s - loss: 1.5609 - acc: 0.9327 - mDice: 0.5921 - val_loss: 3.2404 - val_acc: 0.9406 - val_mDice: 0.4619

Epoch 00177: val_mDice did not improve from 0.47231
Epoch 178/300
 - 11s - loss: 1.5523 - acc: 0.9329 - mDice: 0.5938 - val_loss: 3.5338 - val_acc: 0.9396 - val_mDice: 0.4559

Epoch 00178: val_mDice did not improve from 0.47231
Epoch 179/300
 - 11s - loss: 1.5582 - acc: 0.9325 - mDice: 0.5927 - val_loss: 3.3694 - val_acc: 0.9429 - val_mDice: 0.4618

Epoch 00179: val_mDice did not improve from 0.47231
Epoch 180/300
 - 11s - loss: 1.5567 - acc: 0.9329 - mDice: 0.5935 - val_loss: 3.2869 - val_acc: 0.9427 - val_mDice: 0.4639

Epoch 00180: val_mDice did not improve from 0.47231
Epoch 181/300
 - 11s - loss: 1.5567 - acc: 0.9328 - mDice: 0.5929 - val_loss: 3.2578 - val_acc: 0.9427 - val_mDice: 0.4730

Epoch 00181: val_mDice improved from 0.47231 to 0.47301, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 182/300
 - 11s - loss: 1.5505 - acc: 0.9329 - mDice: 0.5947 - val_loss: 3.4220 - val_acc: 0.9432 - val_mDice: 0.4609

Epoch 00182: val_mDice did not improve from 0.47301
Epoch 183/300
 - 11s - loss: 1.5458 - acc: 0.9330 - mDice: 0.5955 - val_loss: 3.1621 - val_acc: 0.9435 - val_mDice: 0.4700

Epoch 00183: val_mDice did not improve from 0.47301
Epoch 184/300
 - 11s - loss: 1.5428 - acc: 0.9332 - mDice: 0.5965 - val_loss: 3.6705 - val_acc: 0.9408 - val_mDice: 0.4491

Epoch 00184: val_mDice did not improve from 0.47301
Epoch 185/300
 - 11s - loss: 1.5358 - acc: 0.9335 - mDice: 0.5986 - val_loss: 3.3608 - val_acc: 0.9423 - val_mDice: 0.4672

Epoch 00185: val_mDice did not improve from 0.47301
Epoch 186/300
 - 11s - loss: 1.5546 - acc: 0.9327 - mDice: 0.5940 - val_loss: 3.3820 - val_acc: 0.9407 - val_mDice: 0.4508

Epoch 00186: val_mDice did not improve from 0.47301
Epoch 187/300
 - 11s - loss: 1.5414 - acc: 0.9333 - mDice: 0.5967 - val_loss: 3.2510 - val_acc: 0.9429 - val_mDice: 0.4668

Epoch 00187: val_mDice did not improve from 0.47301
Epoch 188/300
 - 11s - loss: 1.5429 - acc: 0.9332 - mDice: 0.5964 - val_loss: 3.5469 - val_acc: 0.9411 - val_mDice: 0.4551

Epoch 00188: val_mDice did not improve from 0.47301
Epoch 189/300
 - 11s - loss: 1.5319 - acc: 0.9336 - mDice: 0.5990 - val_loss: 3.4001 - val_acc: 0.9388 - val_mDice: 0.4614

Epoch 00189: val_mDice did not improve from 0.47301
Epoch 190/300
 - 11s - loss: 1.5327 - acc: 0.9337 - mDice: 0.5994 - val_loss: 3.4211 - val_acc: 0.9431 - val_mDice: 0.4618

Epoch 00190: val_mDice did not improve from 0.47301
Epoch 191/300
 - 11s - loss: 1.5284 - acc: 0.9337 - mDice: 0.6001 - val_loss: 3.4268 - val_acc: 0.9359 - val_mDice: 0.4605

Epoch 00191: val_mDice did not improve from 0.47301
Epoch 192/300
 - 11s - loss: 1.5277 - acc: 0.9337 - mDice: 0.6008 - val_loss: 3.5961 - val_acc: 0.9420 - val_mDice: 0.4559

Epoch 00192: val_mDice did not improve from 0.47301
Epoch 193/300
 - 11s - loss: 1.5304 - acc: 0.9335 - mDice: 0.5993 - val_loss: 3.4851 - val_acc: 0.9425 - val_mDice: 0.4698

Epoch 00193: val_mDice did not improve from 0.47301
Epoch 194/300
 - 11s - loss: 1.5300 - acc: 0.9337 - mDice: 0.5996 - val_loss: 3.4292 - val_acc: 0.9416 - val_mDice: 0.4572

Epoch 00194: val_mDice did not improve from 0.47301
Epoch 195/300
 - 11s - loss: 1.5243 - acc: 0.9339 - mDice: 0.6017 - val_loss: 3.6861 - val_acc: 0.9415 - val_mDice: 0.4503

Epoch 00195: val_mDice did not improve from 0.47301
Epoch 196/300
 - 11s - loss: 1.5251 - acc: 0.9337 - mDice: 0.6012 - val_loss: 3.3335 - val_acc: 0.9434 - val_mDice: 0.4641

Epoch 00196: val_mDice did not improve from 0.47301
Epoch 197/300
 - 11s - loss: 1.5208 - acc: 0.9341 - mDice: 0.6025 - val_loss: 3.2819 - val_acc: 0.9409 - val_mDice: 0.4701

Epoch 00197: val_mDice did not improve from 0.47301
Epoch 198/300
 - 11s - loss: 1.5266 - acc: 0.9341 - mDice: 0.6012 - val_loss: 3.2419 - val_acc: 0.9408 - val_mDice: 0.4668

Epoch 00198: val_mDice did not improve from 0.47301
Epoch 199/300
 - 11s - loss: 1.5260 - acc: 0.9339 - mDice: 0.6009 - val_loss: 3.3732 - val_acc: 0.9382 - val_mDice: 0.4580

Epoch 00199: val_mDice did not improve from 0.47301
Epoch 200/300
 - 11s - loss: 1.5233 - acc: 0.9339 - mDice: 0.6019 - val_loss: 3.5011 - val_acc: 0.9391 - val_mDice: 0.4597

Epoch 00200: val_mDice did not improve from 0.47301
Epoch 201/300
 - 11s - loss: 1.5181 - acc: 0.9341 - mDice: 0.6029 - val_loss: 3.6338 - val_acc: 0.9420 - val_mDice: 0.4546

Epoch 00201: val_mDice did not improve from 0.47301
Epoch 202/300
 - 11s - loss: 1.5170 - acc: 0.9341 - mDice: 0.6033 - val_loss: 3.4530 - val_acc: 0.9405 - val_mDice: 0.4538

Epoch 00202: val_mDice did not improve from 0.47301
Epoch 203/300
 - 11s - loss: 1.5095 - acc: 0.9345 - mDice: 0.6051 - val_loss: 3.2684 - val_acc: 0.9442 - val_mDice: 0.4745

Epoch 00203: val_mDice improved from 0.47301 to 0.47451, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 204/300
 - 11s - loss: 1.5067 - acc: 0.9345 - mDice: 0.6058 - val_loss: 3.2876 - val_acc: 0.9425 - val_mDice: 0.4649

Epoch 00204: val_mDice did not improve from 0.47451
Epoch 205/300
 - 11s - loss: 1.5187 - acc: 0.9341 - mDice: 0.6029 - val_loss: 3.5812 - val_acc: 0.9403 - val_mDice: 0.4510

Epoch 00205: val_mDice did not improve from 0.47451
Epoch 206/300
 - 11s - loss: 1.5139 - acc: 0.9343 - mDice: 0.6038 - val_loss: 3.2754 - val_acc: 0.9416 - val_mDice: 0.4718

Epoch 00206: val_mDice did not improve from 0.47451
Epoch 207/300
 - 11s - loss: 1.5160 - acc: 0.9343 - mDice: 0.6039 - val_loss: 3.3756 - val_acc: 0.9407 - val_mDice: 0.4681

Epoch 00207: val_mDice did not improve from 0.47451
Epoch 208/300
 - 11s - loss: 1.5083 - acc: 0.9347 - mDice: 0.6055 - val_loss: 3.4543 - val_acc: 0.9384 - val_mDice: 0.4492

Epoch 00208: val_mDice did not improve from 0.47451
Epoch 209/300
 - 11s - loss: 1.5103 - acc: 0.9346 - mDice: 0.6056 - val_loss: 3.3678 - val_acc: 0.9392 - val_mDice: 0.4654

Epoch 00209: val_mDice did not improve from 0.47451
Epoch 210/300
 - 11s - loss: 1.5125 - acc: 0.9344 - mDice: 0.6046 - val_loss: 3.2969 - val_acc: 0.9399 - val_mDice: 0.4666

Epoch 00210: val_mDice did not improve from 0.47451
Epoch 211/300
 - 11s - loss: 1.5062 - acc: 0.9347 - mDice: 0.6059 - val_loss: 3.5082 - val_acc: 0.9436 - val_mDice: 0.4665

Epoch 00211: val_mDice did not improve from 0.47451
Epoch 212/300
 - 11s - loss: 1.5046 - acc: 0.9346 - mDice: 0.6069 - val_loss: 3.5124 - val_acc: 0.9423 - val_mDice: 0.4595

Epoch 00212: val_mDice did not improve from 0.47451
Epoch 213/300
 - 11s - loss: 1.4993 - acc: 0.9348 - mDice: 0.6076 - val_loss: 3.4421 - val_acc: 0.9439 - val_mDice: 0.4685

Epoch 00213: val_mDice did not improve from 0.47451
Epoch 214/300
 - 11s - loss: 1.4977 - acc: 0.9351 - mDice: 0.6081 - val_loss: 3.6792 - val_acc: 0.9424 - val_mDice: 0.4579

Epoch 00214: val_mDice did not improve from 0.47451
Epoch 215/300
 - 11s - loss: 1.4951 - acc: 0.9349 - mDice: 0.6088 - val_loss: 3.5558 - val_acc: 0.9407 - val_mDice: 0.4621

Epoch 00215: val_mDice did not improve from 0.47451
Epoch 216/300
 - 11s - loss: 1.4950 - acc: 0.9351 - mDice: 0.6095 - val_loss: 3.4110 - val_acc: 0.9419 - val_mDice: 0.4656

Epoch 00216: val_mDice did not improve from 0.47451
Epoch 217/300
 - 11s - loss: 1.4966 - acc: 0.9351 - mDice: 0.6088 - val_loss: 3.4817 - val_acc: 0.9400 - val_mDice: 0.4561

Epoch 00217: val_mDice did not improve from 0.47451
Epoch 218/300
 - 11s - loss: 1.4938 - acc: 0.9351 - mDice: 0.6096 - val_loss: 3.7208 - val_acc: 0.9422 - val_mDice: 0.4581

Epoch 00218: val_mDice did not improve from 0.47451
Epoch 219/300
 - 11s - loss: 1.4920 - acc: 0.9351 - mDice: 0.6098 - val_loss: 3.6574 - val_acc: 0.9429 - val_mDice: 0.4629

Epoch 00219: val_mDice did not improve from 0.47451
Epoch 220/300
 - 11s - loss: 1.4967 - acc: 0.9350 - mDice: 0.6090 - val_loss: 3.4216 - val_acc: 0.9396 - val_mDice: 0.4588

Epoch 00220: val_mDice did not improve from 0.47451
Epoch 221/300
 - 11s - loss: 1.4953 - acc: 0.9351 - mDice: 0.6092 - val_loss: 3.3341 - val_acc: 0.9400 - val_mDice: 0.4691

Epoch 00221: val_mDice did not improve from 0.47451
Epoch 222/300
 - 11s - loss: 1.4940 - acc: 0.9352 - mDice: 0.6093 - val_loss: 3.4116 - val_acc: 0.9422 - val_mDice: 0.4652

Epoch 00222: val_mDice did not improve from 0.47451
Epoch 223/300
 - 11s - loss: 1.4882 - acc: 0.9353 - mDice: 0.6102 - val_loss: 3.4880 - val_acc: 0.9420 - val_mDice: 0.4620

Epoch 00223: val_mDice did not improve from 0.47451
Epoch 224/300
 - 11s - loss: 1.4825 - acc: 0.9356 - mDice: 0.6122 - val_loss: 3.3304 - val_acc: 0.9436 - val_mDice: 0.4723

Epoch 00224: val_mDice did not improve from 0.47451
Epoch 225/300
 - 11s - loss: 1.4838 - acc: 0.9355 - mDice: 0.6113 - val_loss: 3.6341 - val_acc: 0.9426 - val_mDice: 0.4547

Epoch 00225: val_mDice did not improve from 0.47451
Epoch 226/300
 - 11s - loss: 1.4854 - acc: 0.9354 - mDice: 0.6115 - val_loss: 3.4406 - val_acc: 0.9424 - val_mDice: 0.4660

Epoch 00226: val_mDice did not improve from 0.47451
Epoch 227/300
 - 11s - loss: 1.4843 - acc: 0.9355 - mDice: 0.6115 - val_loss: 3.5395 - val_acc: 0.9400 - val_mDice: 0.4645

Epoch 00227: val_mDice did not improve from 0.47451
Epoch 228/300
 - 11s - loss: 1.4948 - acc: 0.9351 - mDice: 0.6098 - val_loss: 3.6584 - val_acc: 0.9367 - val_mDice: 0.4491

Epoch 00228: val_mDice did not improve from 0.47451
Epoch 229/300
 - 11s - loss: 1.4768 - acc: 0.9357 - mDice: 0.6135 - val_loss: 3.4704 - val_acc: 0.9386 - val_mDice: 0.4576

Epoch 00229: val_mDice did not improve from 0.47451
Epoch 230/300
 - 11s - loss: 1.4828 - acc: 0.9355 - mDice: 0.6129 - val_loss: 3.5708 - val_acc: 0.9422 - val_mDice: 0.4583

Epoch 00230: val_mDice did not improve from 0.47451
Epoch 231/300
 - 11s - loss: 1.4843 - acc: 0.9354 - mDice: 0.6114 - val_loss: 3.4530 - val_acc: 0.9395 - val_mDice: 0.4609

Epoch 00231: val_mDice did not improve from 0.47451
Epoch 232/300
 - 11s - loss: 1.4818 - acc: 0.9356 - mDice: 0.6124 - val_loss: 3.5627 - val_acc: 0.9405 - val_mDice: 0.4600

Epoch 00232: val_mDice did not improve from 0.47451
Epoch 233/300
 - 11s - loss: 1.4797 - acc: 0.9357 - mDice: 0.6129 - val_loss: 3.5946 - val_acc: 0.9372 - val_mDice: 0.4450

Epoch 00233: val_mDice did not improve from 0.47451
Restoring model weights from the end of the best epoch
Epoch 00233: early stopping
{'val_loss': [46.847668556939986, 16.581490022795542, 9.625416149695715, 7.752952997883161, 6.873272869203772, 6.248983607050919, 5.960290760688839, 5.6420093196488565, 5.476830140997966, 5.138599966076159, 4.823912515881515, 4.53576531385382, 4.400209916844254, 4.236101938766383, 4.140038284428773, 3.8390553288516545, 3.6648013571365956, 3.729766809603288, 3.8128139880441485, 3.421043593392131, 3.3581684938232814, 3.319016867805095, 3.1414799262281683, 3.1382292933052494, 3.0629993676695797, 2.96625823953322, 2.9906362103564397, 2.9625291858489313, 2.9330169209500863, 3.078024790888386, 3.0828255504990616, 2.8519961160740683, 2.943892303987273, 2.9165852644170323, 2.9821928777687607, 2.8808617659267926, 3.0783091078379323, 2.910244960958759, 3.0762226265367296, 3.0032846968000135, 3.0175548341967877, 3.014627978338727, 3.0304264788781956, 2.9791255034062836, 2.9379881684269225, 2.976769691488395, 2.8944533790594766, 3.035622156380365, 2.863871061442686, 2.9106187919110416, 2.8877380135513486, 3.1412002590174475, 2.9562608912986303, 3.1749293852287033, 3.576802388454477, 3.3297586927101728, 2.990690026897937, 2.981078349009511, 3.1144875583371947, 2.943817812550281, 2.9858528481175504, 2.922138314428074, 3.0338301943792474, 2.933569169780683, 3.0931875413017615, 3.2464358619458618, 3.207855970443537, 3.3719519652160153, 3.106704121255981, 3.227296388575009, 3.2539657409790728, 3.26265813159712, 3.130386899691075, 3.004853706030796, 3.122213930822909, 3.117084753078719, 3.412546448343034, 3.198647237116737, 3.224578774822432, 2.9918338854664137, 3.160683277772651, 3.321387823316313, 3.217521247941823, 3.426790696568787, 3.3417902123404755, 3.1897118805597224, 3.077743938436643, 3.0305269720210206, 3.044697209837891, 3.2230167398672727, 3.1261755879968405, 3.214270158005612, 3.312005516962104, 3.521650711806225, 3.0686412422163856, 3.3901742138412025, 3.0575652912347797, 2.989479910316212, 3.3220788944911743, 3.232702659775636, 3.075009254539119, 3.257999752160339, 3.0856394104748253, 3.216100162205597, 3.6921978701305176, 3.203054820763923, 3.0032956367136823, 3.413361645968897, 3.1885715703746036, 3.2566699171438813, 3.5986235452638495, 3.1064854177779386, 3.2170325438270257, 3.5568551983285164, 3.3440761316478964, 3.3050875958897885, 3.2247108849696815, 3.283559732893038, 3.535409784986682, 3.0622126260728, 3.1694525052527234, 3.4040764696186496, 3.5691992448243712, 3.2633506186040386, 3.1017270087752316, 3.201642598158547, 3.1350503045888174, 3.100658663526355, 3.3128172941949394, 3.3402414867831838, 3.17934050901039, 3.1025228303901495, 3.380455976418619, 3.2988949561425085, 3.3954863240720616, 3.093538558310164, 3.432477990564491, 3.3977448010728475, 3.3131948302810392, 3.391174681462525, 3.3443028219487694, 3.4598139816939475, 3.2402357451085533, 3.153836908861108, 3.40671521057153, 3.297639022248664, 3.407320144187127, 3.397319060816829, 3.190636859164529, 3.1178840198110613, 3.252062695832657, 3.3883651980598057, 3.4473622076080312, 3.334272120746651, 3.2946161881444, 3.496491402941978, 3.2437684439743557, 3.2319171775487208, 3.2735286887646433, 3.3575576832739724, 3.5273225034941875, 3.256402097265458, 3.4399416935186657, 3.251842224127835, 3.454414421160306, 3.457308427785479, 3.480314082599112, 3.16522996176389, 3.466392291798478, 3.254969957972034, 3.273658951345299, 3.3565900457490767, 3.229881400065053, 3.198396860533172, 3.140513669344641, 3.4403568744393334, 3.240431240338477, 3.5338150786147233, 3.3693857547339228, 3.286858141488795, 3.257811569675271, 3.421997524043989, 3.162067923827895, 3.6705255667191174, 3.3607692426026223, 3.382030223851048, 3.25104552678143, 3.54690773742983, 3.4000540616522943, 3.4210820169010687, 3.426839276216924, 3.5961455379257954, 3.485110383015126, 3.4291960779533146, 3.6860575482382307, 3.3335004896369007, 3.281891804077618, 3.241916702727654, 3.3732437190289297, 3.5010701336764862, 3.6337508816227673, 3.4530255637309026, 3.2683684752943614, 3.2875500155745874, 3.581187756288619, 3.2753987266859483, 3.375572941798185, 3.454266707745514, 3.3677794120629274, 3.2969280011464086, 3.508245714319249, 3.5123563619064435, 3.4420817226748026, 3.6791946639173796, 3.5558061399940577, 3.4110425892507745, 3.481670110641668, 3.720805931148962, 3.657391658740207, 3.4215858741441654, 3.3340916311856184, 3.4116279585286975, 3.4879621869352246, 3.3303615511838522, 3.634063929474602, 3.4406178291842697, 3.539538947754495, 3.6583789166595255, 3.4704029755888595, 3.5707647493552592, 3.4530158209658803, 3.562684773134866, 3.5945525501544275], 'val_acc': [0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9052792815935045, 0.9093933275767735, 0.9097710450490316, 0.9076007377533686, 0.9099129835764567, 0.9093200479234967, 0.9097847966920762, 0.9100709983280727, 0.9088003493490673, 0.9114400120008559, 0.9140956912721906, 0.9178480023429507, 0.9166551941917056, 0.9216758126304263, 0.9227266737392971, 0.9215728243192037, 0.9253456848008292, 0.9240064195224217, 0.927099366982778, 0.927728948139009, 0.9264010928926014, 0.9295009289469037, 0.9304555995123727, 0.9260073048727853, 0.929961085319519, 0.9282142690249852, 0.9312362727664766, 0.9302701666241601, 0.9303205382256281, 0.9293429652849833, 0.9302220486459278, 0.9302655515216646, 0.9330311445962816, 0.9328640302022299, 0.9322641775721595, 0.9318406468346006, 0.9312477367264884, 0.9339789123762221, 0.933793516386123, 0.9340361782482692, 0.9328937729199728, 0.9351717063358852, 0.9345261028834752, 0.9318337894621349, 0.9270169678188506, 0.9355792318071637, 0.9339445744241986, 0.9349931194668725, 0.9368292320342291, 0.9360920474642799, 0.936611703463963, 0.9329074905032203, 0.9365018464270092, 0.9337820410728455, 0.9351648262568882, 0.9346680414109003, 0.9328456975164867, 0.9345925052960714, 0.9337912088348752, 0.9359867090270633, 0.9367422376360212, 0.9332806922140575, 0.9371542902219863, 0.9366392067500523, 0.9358631088620141, 0.9332234320186433, 0.934700071811676, 0.9344917592548189, 0.9375182957876296, 0.9372916846048265, 0.9375457848821368, 0.9355540076891581, 0.9326900385674977, 0.9359523937815711, 0.933411161104838, 0.9385874299775987, 0.9377449438685462, 0.9375045725277492, 0.9366551949864342, 0.9382943908373514, 0.9373557539213271, 0.9377312177703494, 0.937747251419794, 0.9391300621486846, 0.9383584885370164, 0.9363919269470942, 0.9382555229323251, 0.9388324107442584, 0.9382120001883734, 0.9383241704532078, 0.9377449608984447, 0.9394757350285848, 0.9358859998839242, 0.9337751609938485, 0.9358860027222407, 0.9409638387816293, 0.9396451512972513, 0.938653860773359, 0.9389675146057492, 0.9400732630775088, 0.9371268351872762, 0.9396955087071374, 0.9363187097367787, 0.9374679241861615, 0.9386446901730129, 0.9381982797668094, 0.9393909970919291, 0.939981684798286, 0.9369574245952424, 0.9398535064288548, 0.9383287656874884, 0.9370100838797433, 0.938894217922574, 0.939686329591842, 0.9411423859142122, 0.9387271120434716, 0.9392559641883487, 0.939159787836529, 0.9375687042872111, 0.9399587880997431, 0.9381776508830842, 0.9398694776353382, 0.9399405036653791, 0.9404624587013608, 0.9403479808852786, 0.9386309527215504, 0.9416414953413463, 0.9369253743262518, 0.939755014010838, 0.9400778668267387, 0.9408310424713862, 0.9384775559107462, 0.9414193857283819, 0.9423832581156776, 0.9404807743572053, 0.9414125510624477, 0.9417513552166167, 0.9424908303079151, 0.9427632888158163, 0.9404487382797968, 0.9399381648926508, 0.9404166653042748, 0.9404899506341844, 0.9362339973449707, 0.9412110731715247, 0.9434821208318075, 0.9416483413605463, 0.9403662766729083, 0.9409546823728652, 0.9415247213272822, 0.9432371741249448, 0.9407074423063368, 0.9398420197623116, 0.9393132101921808, 0.9419367909431458, 0.9378708686147418, 0.94093405348914, 0.9388438434827895, 0.9419047406741551, 0.9417536826360793, 0.9379418435550871, 0.9427358309427897, 0.9410210365340823, 0.9432600509552729, 0.9428937804131281, 0.9406066820735023, 0.9396154085795084, 0.9429166515668234, 0.9427037778354826, 0.9426694143386114, 0.9432096992220197, 0.9434912772405715, 0.9408264614286876, 0.9423053945813861, 0.9407188920747667, 0.942893752029964, 0.9410622857865834, 0.9388072263626825, 0.9431112891151792, 0.9358997344970703, 0.942030668258667, 0.9425434725625175, 0.9415865398588634, 0.9414995397840228, 0.9433768561908177, 0.9408951685542152, 0.9407737993058705, 0.9382188660757882, 0.9391346204848516, 0.9420215033349537, 0.940547145548321, 0.9441620707511902, 0.9425068554424104, 0.9403068025906881, 0.9415980094955081, 0.9407074394680205, 0.9384203240984962, 0.9392216035297939, 0.9398603751545861, 0.9436446797280085, 0.9422664869399298, 0.94390336672465, 0.9424496378217425, 0.9406547659919375, 0.9418978918166387, 0.940020603793008, 0.942193227154868, 0.9429463999611991, 0.9395512824966794, 0.9399747933660235, 0.942152011962164, 0.9420467217763265, 0.9436401242301578, 0.9425755455380395, 0.9423832410857791, 0.9400457910129002, 0.9367353717486063, 0.9385760171072823, 0.9422024005935306, 0.939542123249599, 0.9405242573647272, 0.9372023627871559], 'val_mDice': [0.011889373672948707, 0.010110584605995211, 0.009262420813597384, 0.009575179329008929, 0.017787283508195764, 0.02290433563203329, 0.02431627440576752, 0.026919298938342502, 0.030941518112307505, 0.04609699720250709, 0.06468169759249404, 0.08622867480984756, 0.10154088852660996, 0.11531784740232286, 0.12945010832377843, 0.1529347953342256, 0.1705325762963011, 0.18519623879165875, 0.19692402749898888, 0.22974020375737123, 0.2471583237250646, 0.27035182820899145, 0.29153158231860116, 0.29947488187324434, 0.31074238204885096, 0.32523632182606627, 0.32826102312122074, 0.33731402669634136, 0.33310379540281637, 0.3339940850578603, 0.33425486575634705, 0.35117564474542934, 0.3543475887605122, 0.35673565914233524, 0.34965748162496657, 0.36651928580942605, 0.35254133741060895, 0.3678423936168353, 0.3664376863411495, 0.3735221755646524, 0.3713685020449616, 0.37126240222936585, 0.3755297242175965, 0.3902879972897825, 0.3915516668487163, 0.38672884624628795, 0.393060255973112, 0.39152307418130694, 0.39705718344166163, 0.39988720168670017, 0.40212206542491913, 0.3943278271527517, 0.4062501011150224, 0.3996498661027068, 0.3760890467535882, 0.37191441300369443, 0.40817094452324365, 0.4044602981635502, 0.40678822266913595, 0.4156898975017525, 0.4139713699973765, 0.4174829838531358, 0.41082063175383066, 0.417365221395379, 0.41090962042411167, 0.4131166266188735, 0.40927628250349135, 0.3972149184417157, 0.4140018897042388, 0.40858847754342215, 0.41340367105745135, 0.41305133806807653, 0.4208071150240444, 0.42895300313830376, 0.4244674557731265, 0.42337787630302565, 0.41556894069626216, 0.40707893953436897, 0.41715100230205626, 0.42839843017004786, 0.431433908109154, 0.4231275554214205, 0.4235514123879728, 0.4026480675453231, 0.41827909488763126, 0.4110043848909083, 0.44032042154244017, 0.4410669326427437, 0.43561756504433496, 0.4221134866986956, 0.44540666682379587, 0.43591781457265216, 0.43095053430824054, 0.42337404156015035, 0.44563435905036475, 0.4327902055922009, 0.443045702008974, 0.44379300127426785, 0.4377019625334513, 0.4439865379106431, 0.44747902772256304, 0.4388321430555412, 0.4487208564366613, 0.4346240083021777, 0.4142562197077842, 0.4368153696968442, 0.45468727800817715, 0.44561744162014555, 0.43679006823471617, 0.44568616134070216, 0.4344868301635697, 0.44439898058772087, 0.4462140372821263, 0.43751064634748865, 0.44379603401536033, 0.44329318900903064, 0.4458079373552686, 0.44794961792372523, 0.44138686678239275, 0.4434289187192917, 0.45621325962600257, 0.4422077408858708, 0.43695710670380367, 0.45097586494826136, 0.4567929257949193, 0.45333104935430346, 0.45620601517813547, 0.4519293247943833, 0.45173452226888566, 0.4447632715815589, 0.4480797726483572, 0.45524530573969796, 0.45271762850738706, 0.4459813613267172, 0.44869453903465045, 0.4580245307158856, 0.44549142888614107, 0.4608059686919053, 0.44209053331897374, 0.443396846275954, 0.4552604006159873, 0.4487499463416281, 0.4613827168941498, 0.4674742113621462, 0.456479119935206, 0.45705249320183483, 0.4527448674752599, 0.457649718437876, 0.45671657969554263, 0.46594416509781567, 0.4577378272300675, 0.4529435796042283, 0.45212551383745103, 0.45682666823267937, 0.45505000225135256, 0.4552234622339408, 0.46739835451756206, 0.4592413671669506, 0.4517074681463696, 0.44944996049716357, 0.45191352991830736, 0.4694354574949968, 0.4512212187761352, 0.45993022194930483, 0.45404324325777234, 0.45671691266553743, 0.4501533882603759, 0.4679012138928686, 0.4507197844130652, 0.47231421655132655, 0.4620852569739024, 0.4534665621107533, 0.46613950353293193, 0.46966569622357685, 0.47117736616304945, 0.4632355859946637, 0.4619424802561601, 0.45589381775685717, 0.4618210114893459, 0.4639209530183247, 0.4730146846600941, 0.46085642899076146, 0.46998024731874466, 0.44910287608702976, 0.46723945616256624, 0.4508204630443028, 0.46680399119144395, 0.45514089046489625, 0.4613852323520751, 0.46179610845588503, 0.46054171291845186, 0.45586002618074417, 0.4698033134142558, 0.4571787862195855, 0.45028544678574517, 0.4641128015660104, 0.4700536546962602, 0.4668216197973206, 0.45803876646927427, 0.45971960326035816, 0.45463495878946214, 0.4538137173014028, 0.474513869200434, 0.46490965748117086, 0.4510101324745587, 0.4718375819779578, 0.46805152226062047, 0.449174763191314, 0.465440676680633, 0.46659262176780475, 0.4664899045158, 0.45949676632881165, 0.46853249324929147, 0.4579190340425287, 0.46209650788278805, 0.4656085144905817, 0.4561139288402739, 0.4581487679055759, 0.4629182984076795, 0.4588237450945945, 0.46908647584773244, 0.46518921781153905, 0.4619842214243753, 0.47232619602055775, 0.4546750198517527, 0.4660325763481004, 0.46449966285200345, 0.4491259225067638, 0.4576264978164718, 0.45834426439943765, 0.46092276736384347, 0.45997825389107067, 0.44495616835497676], 'loss': [196.65510183348692, 44.48583956722673, 21.823017172761016, 15.03200011862848, 11.85296044819429, 10.050054486831627, 8.99595528695081, 8.220421682416255, 7.536187656311394, 6.9419349572413545, 6.399639849720696, 5.95284522933401, 5.629231022988668, 5.306694321763844, 5.046892153405491, 4.814107894207991, 4.5540461186754095, 4.330016757795202, 4.141457968739983, 3.9557791803759694, 3.790761799671665, 3.649568113399099, 3.515698805833474, 3.3989169002843314, 3.298392121539494, 3.2189502158973036, 3.1308273662830426, 3.0564562379911684, 2.9839465845658504, 2.934543949297818, 2.8791718375873585, 2.832180002049626, 2.7752155809933816, 2.7380453193029135, 2.7063342400178163, 2.6676774618137036, 2.6196779309841443, 2.594190689141704, 2.5565217406114953, 2.535259309269133, 2.502354485050479, 2.4816256597457143, 2.4571479265460487, 2.414331261514583, 2.4038988198989477, 2.374113131700188, 2.3516708786418175, 2.33099214468063, 2.3124010254474006, 2.29835253619541, 2.2818016855845866, 2.2528426985026373, 2.2511264899573575, 2.2295015520366466, 2.222174468862314, 2.206677560220685, 2.1863697078477586, 2.1730567777641756, 2.174766909554443, 2.147213905400478, 2.134817598503588, 2.115960285018721, 2.102452973582185, 2.102307095655431, 2.077136265627102, 2.067192052103362, 2.065442900954585, 2.053449941688083, 2.0452640062125864, 2.0333071691994036, 2.016099401193247, 2.006659761729074, 2.0039295027980875, 1.9935763485748368, 1.9853664829203772, 1.972663356179053, 1.965492397851284, 1.9585808989444127, 1.9527731303514164, 1.939693610103277, 1.9315215732082824, 1.933222119624798, 1.9138218782438532, 1.9125815743032117, 1.9079350278812819, 1.8991897911700941, 1.8835514174827794, 1.8832123835054189, 1.8702893692196416, 1.8700141710239453, 1.8612424146929112, 1.8507354467480954, 1.852077907717984, 1.840169245460853, 1.835432591386813, 1.8386051979785667, 1.8320172683843785, 1.8199759439073455, 1.811051151921162, 1.8132974749151993, 1.8034425869552484, 1.8002600185623374, 1.7926996650322569, 1.78420518764715, 1.7808465144067502, 1.7803519169489543, 1.7700551497538655, 1.7673675100858992, 1.7624754126370241, 1.7580480174749877, 1.7553766020143655, 1.7540149157369485, 1.7434627847540969, 1.7471864395847618, 1.73879498690338, 1.7327734840980311, 1.724812296973457, 1.7281099570516862, 1.7126355535473372, 1.7163008900995955, 1.7079842936356036, 1.704762266439258, 1.6974752211998212, 1.6935448041420946, 1.6864151960396458, 1.690887541975018, 1.6805824509516418, 1.6840437859130177, 1.6751757536133016, 1.668538999001323, 1.666760675268686, 1.6635116073054248, 1.6685386918656095, 1.6628166888981444, 1.6512614674988293, 1.6529099608929327, 1.6572059977829101, 1.6462524335738755, 1.646879991745935, 1.648568534956793, 1.6328140145118513, 1.6395249784808639, 1.6297211651630155, 1.6306456739281514, 1.6307326229362789, 1.62123842969758, 1.6193813289179377, 1.6197059312399031, 1.61932800660502, 1.6140745716194254, 1.612895766342447, 1.6033994398848823, 1.6098329227554884, 1.6109635783960898, 1.6082327607189595, 1.6003582630861648, 1.6033565312514813, 1.5942136743566493, 1.591871558137544, 1.590217008442692, 1.5855305881566433, 1.5910580952042948, 1.5878471572527382, 1.5912726617213364, 1.575688662385582, 1.5751531005526223, 1.5758701947030935, 1.5792856836033988, 1.5645961303896634, 1.5728265355496087, 1.5678294708394627, 1.5650122810104161, 1.5689490732715519, 1.563645198254634, 1.560226550859731, 1.5585045332345104, 1.56087586627844, 1.5523441467345829, 1.5581635738673962, 1.5567400337345734, 1.5566887334750432, 1.5505356618795916, 1.5457655761383025, 1.5428084680425975, 1.5358042357306685, 1.5545784794390258, 1.5414453937709964, 1.542859023491128, 1.531949093122576, 1.5327317863846412, 1.5283941697449221, 1.5277063110004805, 1.5304147300174156, 1.5300007764833934, 1.5243416049472625, 1.5251400566110267, 1.5207596254505034, 1.5265542805160381, 1.5260240880423068, 1.5233288244496013, 1.518128561380628, 1.5170194679587155, 1.509516127495171, 1.506660713426911, 1.5186502282413095, 1.5138714655943635, 1.516009899332088, 1.5082906670530845, 1.510264227781908, 1.5124768325766318, 1.5061932611447069, 1.5046360202439608, 1.4993382812994211, 1.4977290290891723, 1.4950583443284058, 1.4949510742042804, 1.4965505179858791, 1.4938153153397065, 1.4920443309302593, 1.4967139883263851, 1.4953036789538339, 1.4940146959790932, 1.4882335350971725, 1.4824524419211642, 1.4837950873471464, 1.4853982612855499, 1.484329614803376, 1.4948018416747988, 1.476796777050275, 1.4828201447559133, 1.4843003822241718, 1.481809186273534, 1.479698794734301], 'acc': [0.4137364034768081, 0.82260534967649, 0.8567979911951654, 0.8636739539897853, 0.8665854466405015, 0.8675927992314761, 0.8679764299970016, 0.8682055551237698, 0.8682270579906196, 0.867998331170425, 0.8681451267122372, 0.8700957279203484, 0.873071057432898, 0.87532632784598, 0.8770710300084642, 0.8782096475977554, 0.8789885924895283, 0.8795913599441569, 0.8804866726145203, 0.88204215071393, 0.8836619046029958, 0.8862528203883704, 0.8886831989402323, 0.8904304009700111, 0.8923174217989708, 0.8934196623414611, 0.8950316591921951, 0.8959183540444441, 0.8969260449696305, 0.8979793449277383, 0.899197605514839, 0.8998964223095004, 0.9012519736406728, 0.9021616523404192, 0.9025410014079535, 0.9034291791483826, 0.904570089488308, 0.9053198113270479, 0.9064163486921238, 0.9066757829703644, 0.9072683096069901, 0.9079057902000027, 0.908536273745505, 0.9095749727489633, 0.9098919894921887, 0.9107306687294552, 0.911254745780422, 0.9118195361132242, 0.9121698917325852, 0.9125461586757687, 0.9127509034114328, 0.9137288015059293, 0.913877265253852, 0.9146452954745324, 0.9147795303285708, 0.9152592338416212, 0.9156297274078964, 0.9158108150175318, 0.9155256868856638, 0.9166311654760494, 0.9168683292320153, 0.9171959122918304, 0.9174873892434052, 0.9173411773146061, 0.9179912384636075, 0.9182779644067425, 0.9181600694645341, 0.9185107736682212, 0.9186211624815672, 0.9187568138393566, 0.9192340827319165, 0.9195173610858612, 0.9196268677573592, 0.9200004890449065, 0.9199701127850621, 0.9201707795839768, 0.9204865685772385, 0.9209493107365486, 0.9208739804513703, 0.9210986950067925, 0.9212572853620833, 0.9213994236403356, 0.9215593774561848, 0.9217915572204501, 0.9214961400421824, 0.9218356113041299, 0.9223782922207149, 0.9223800778894589, 0.9227673943180189, 0.9227476729141487, 0.9230992833336631, 0.9233302609617595, 0.9230614227896508, 0.9235170719157944, 0.9236113555012628, 0.9234228573002659, 0.9236025052284813, 0.9240628616277115, 0.9241817566455387, 0.924291244965655, 0.924618844009673, 0.9244868361538537, 0.9245587864638248, 0.9249502963727441, 0.9252068326609751, 0.9250896740722729, 0.9253499440106211, 0.925471800093433, 0.9256229047281609, 0.9257658499386215, 0.9258532555479753, 0.9258087900118352, 0.9260684078613504, 0.9260867860865588, 0.9263076584923352, 0.9264293316243953, 0.9267298218623415, 0.9265138391862836, 0.9268894765288332, 0.9266312022601706, 0.9268984924811537, 0.9271696705406333, 0.9273360684723759, 0.9273808826847713, 0.9278280305476324, 0.9275541596498152, 0.9279058848775051, 0.9278921675930277, 0.9282277178874658, 0.9283924479808149, 0.9285271916978732, 0.928312136013659, 0.9284352024832925, 0.9286476195789335, 0.9288187192794972, 0.9289254960651593, 0.9286706965278058, 0.9289654645015216, 0.9290977073975191, 0.9288851980278665, 0.9295100949048215, 0.9295257378203666, 0.9297457073082785, 0.9296954009995793, 0.9297859595762826, 0.9301050361673198, 0.9301736926053424, 0.9300551667410168, 0.9300883971865705, 0.9301970270136452, 0.9303685668600945, 0.9305856170304047, 0.9304816200022112, 0.9304955924363961, 0.9306742703958308, 0.9307986801705288, 0.9308648589306859, 0.9310502816607136, 0.9312663122319247, 0.9313586542595782, 0.9313672940258072, 0.9312780851777266, 0.9312269189625731, 0.9313568249703924, 0.9317565104026935, 0.931856822974424, 0.9318590477548492, 0.9318706096310319, 0.9321363242301772, 0.9320308451526583, 0.9323773314227988, 0.9324212220677158, 0.9323746014909982, 0.9325757525503601, 0.932671220805435, 0.9326619524153207, 0.9326566715035476, 0.9328939959059245, 0.9325469495911394, 0.932862757223062, 0.9328077744146016, 0.9329318588515892, 0.9329872400470614, 0.9332124230166219, 0.9334640943730569, 0.9327252116510352, 0.9332557540535582, 0.9332213192425506, 0.9335591668840868, 0.9336775535215227, 0.9336822564394598, 0.9337248427803079, 0.9335304564861565, 0.9337167843173688, 0.9339006768845859, 0.9337466730877502, 0.9341175368762048, 0.9340538173735843, 0.9338805602882463, 0.9339379344874548, 0.9341374629376273, 0.9341445579702555, 0.9345278663398077, 0.9345325921481346, 0.9341459941009072, 0.93425140117482, 0.9343403333387601, 0.93469931128077, 0.9345928181084177, 0.9344150645553158, 0.9346718320686601, 0.9346433285001111, 0.9348305133305144, 0.9350557393344675, 0.9349198408248177, 0.9351306113522017, 0.9350763426158891, 0.9351259729915992, 0.9351063735778791, 0.9350231136341695, 0.9350675180373405, 0.935151302540994, 0.935312186508756, 0.9355877650618346, 0.9355273108181752, 0.93540929758597, 0.9355281235649104, 0.9351156644447015, 0.9357264760442844, 0.9355224481386465, 0.9354149510871015, 0.9355536547496826, 0.935724036677945], 'mDice': [0.015415516471716855, 0.013297584053277625, 0.012405723384889433, 0.014742332784103846, 0.020481752519355197, 0.025247402676993973, 0.029366721451736404, 0.03449753023352218, 0.04232917270559049, 0.05310820643356454, 0.0668148155275144, 0.08205777218524767, 0.09490010508032509, 0.1084878035729989, 0.12135681150585603, 0.13539026891631337, 0.15264710724778963, 0.17082450936036875, 0.18720568888318867, 0.2052655838284852, 0.22138169930979584, 0.23720768002420808, 0.2527399565044202, 0.26702759858613667, 0.27900661407740607, 0.28791776048394785, 0.2989725248418793, 0.30823659409050863, 0.31725417759254393, 0.32438648329641584, 0.3318714961590555, 0.33679532512939114, 0.34618185497051907, 0.3514301959825247, 0.35535233395279364, 0.36167942465259456, 0.3698825226977815, 0.3737411616060898, 0.38072023702904234, 0.38381496463089476, 0.38926008050097466, 0.39334182143900837, 0.39761743705029523, 0.40570895048334166, 0.40774456574918216, 0.4136238317166033, 0.4176042834549987, 0.4209705992702898, 0.4250969624896258, 0.42757127254322264, 0.43003694581254637, 0.4359883811861328, 0.43715444713951607, 0.4412533030795849, 0.4427656172292642, 0.4458300705323312, 0.45007650405656907, 0.45228525583101004, 0.45196668076014185, 0.4575952101712147, 0.45974599263652893, 0.4642761650459142, 0.46702225568645234, 0.4677053066216523, 0.4719426977524115, 0.47439110115908784, 0.4751314845576046, 0.47770834676397084, 0.4798112612642119, 0.48166993857349344, 0.4854403965236917, 0.48762496002198, 0.48801942973185625, 0.49058125587656065, 0.49203764244336623, 0.49527096667913934, 0.496578787100299, 0.4988841939367769, 0.49988199289258284, 0.5015987286407732, 0.5038731532196147, 0.5044002207644257, 0.5072348274223051, 0.5081830589854467, 0.5088319074666314, 0.5111978967823642, 0.5141644235495683, 0.5145298928246462, 0.5169649304441801, 0.517524240528064, 0.5193101572972032, 0.521525448702056, 0.5211961333844067, 0.5250884492272744, 0.5253449033307321, 0.5248127577046059, 0.5253601506171439, 0.5287262333227716, 0.53134779996074, 0.5306579106824348, 0.5327866562579854, 0.5339196375254287, 0.5352527044034082, 0.5375024000068195, 0.5372971148074466, 0.5381614895990319, 0.54050482414535, 0.5410523730159059, 0.542478135975657, 0.5432555199887497, 0.543986052958428, 0.5432152891977228, 0.5464714206388881, 0.5456557943684438, 0.5478918019208877, 0.5483733238754692, 0.5514667596986167, 0.5495759778705801, 0.5535954754316258, 0.5531137782828253, 0.5547953552424805, 0.5563132360258704, 0.5582255283853602, 0.5587117371587641, 0.5606285709853988, 0.5597922861035995, 0.5612784725681582, 0.5616865346306249, 0.5632257273207746, 0.5648196999774358, 0.5656599822429557, 0.5656550508462925, 0.5655871590530112, 0.56591964943551, 0.5686564816085502, 0.5684170494746996, 0.5678887233146703, 0.5702722429631647, 0.5702978335917054, 0.5695185429883964, 0.5731045981006906, 0.5721273393053665, 0.5742872981223524, 0.5740563764906766, 0.574042956218201, 0.576237825637657, 0.5771671327203368, 0.5767528595137463, 0.5771486166931425, 0.5778401393478537, 0.5793834332213688, 0.5807888884867963, 0.5796637510619045, 0.5785441268885414, 0.5798818455373066, 0.5816688766961201, 0.5818118624818653, 0.5828327356631388, 0.5836380539612432, 0.5850012340319508, 0.5852247860957047, 0.5842451099855767, 0.5844491083589344, 0.5847046369299255, 0.5876716632867839, 0.5883032544191941, 0.5882811518976724, 0.5875393258415254, 0.5903481297210758, 0.5886992262764338, 0.5908345516210166, 0.5909293027472952, 0.5904306748572034, 0.5913118650992757, 0.5922697925696327, 0.5927834809918107, 0.5920644574411714, 0.5937592339970736, 0.5926979418685263, 0.5935061183857371, 0.5928601559035369, 0.5947131172917081, 0.5955260492069817, 0.5965134981787685, 0.5986395214526483, 0.5940269627896536, 0.596680085978941, 0.5964065561961962, 0.5989763576997735, 0.5993874089575835, 0.6001167057772604, 0.6007528438121339, 0.5993067983122076, 0.5995800849420155, 0.6017145143323765, 0.6011516149158489, 0.6025155274790331, 0.6012175941894756, 0.600864242583496, 0.6018825814010138, 0.6028596811127245, 0.6032586898880932, 0.6050689591822744, 0.6057994932206306, 0.6029274166951823, 0.603829681045958, 0.6039285757005249, 0.6055099863479287, 0.6055615498239603, 0.6046108183705419, 0.6058961743561087, 0.60685330423657, 0.6075893512966317, 0.6080982936860417, 0.6088126164675908, 0.6094707041915635, 0.6087958446375455, 0.6095888954907777, 0.6097867254441428, 0.6090151042705652, 0.609181403286406, 0.6092812508109081, 0.6102102759809698, 0.6122416529394378, 0.6112854224260726, 0.611472423800113, 0.6115075152159795, 0.6098208522116354, 0.6135205961684858, 0.612884849465052, 0.6113600753073934, 0.6123721592913984, 0.6129245932722266]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.38s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.10s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.89s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:45,  1.43s/it]predicting train subjects:   1%|          | 2/285 [00:03<07:01,  1.49s/it]predicting train subjects:   1%|          | 3/285 [00:04<06:52,  1.46s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:21,  1.57s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<07:03,  1.51s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:28,  1.61s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<07:50,  1.69s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<08:01,  1.74s/it]predicting train subjects:   3%|▎         | 9/285 [00:14<07:45,  1.69s/it]predicting train subjects:   4%|▎         | 10/285 [00:16<08:02,  1.75s/it]predicting train subjects:   4%|▍         | 11/285 [00:18<08:16,  1.81s/it]predicting train subjects:   4%|▍         | 12/285 [00:20<08:33,  1.88s/it]predicting train subjects:   5%|▍         | 13/285 [00:22<08:40,  1.91s/it]predicting train subjects:   5%|▍         | 14/285 [00:24<08:41,  1.92s/it]predicting train subjects:   5%|▌         | 15/285 [00:26<08:38,  1.92s/it]predicting train subjects:   6%|▌         | 16/285 [00:28<08:35,  1.92s/it]predicting train subjects:   6%|▌         | 17/285 [00:30<08:31,  1.91s/it]predicting train subjects:   6%|▋         | 18/285 [00:32<08:31,  1.92s/it]predicting train subjects:   7%|▋         | 19/285 [00:34<08:27,  1.91s/it]predicting train subjects:   7%|▋         | 20/285 [00:36<08:30,  1.93s/it]predicting train subjects:   7%|▋         | 21/285 [00:38<08:30,  1.93s/it]predicting train subjects:   8%|▊         | 22/285 [00:39<08:24,  1.92s/it]predicting train subjects:   8%|▊         | 23/285 [00:41<08:20,  1.91s/it]predicting train subjects:   8%|▊         | 24/285 [00:43<08:16,  1.90s/it]predicting train subjects:   9%|▉         | 25/285 [00:45<08:19,  1.92s/it]predicting train subjects:   9%|▉         | 26/285 [00:47<08:16,  1.92s/it]predicting train subjects:   9%|▉         | 27/285 [00:49<08:09,  1.90s/it]predicting train subjects:  10%|▉         | 28/285 [00:51<07:57,  1.86s/it]predicting train subjects:  10%|█         | 29/285 [00:53<07:56,  1.86s/it]predicting train subjects:  11%|█         | 30/285 [00:54<07:49,  1.84s/it]predicting train subjects:  11%|█         | 31/285 [00:56<07:41,  1.82s/it]predicting train subjects:  11%|█         | 32/285 [00:58<07:38,  1.81s/it]predicting train subjects:  12%|█▏        | 33/285 [01:00<07:42,  1.83s/it]predicting train subjects:  12%|█▏        | 34/285 [01:02<07:37,  1.82s/it]predicting train subjects:  12%|█▏        | 35/285 [01:03<07:32,  1.81s/it]predicting train subjects:  13%|█▎        | 36/285 [01:05<07:34,  1.82s/it]predicting train subjects:  13%|█▎        | 37/285 [01:07<07:33,  1.83s/it]predicting train subjects:  13%|█▎        | 38/285 [01:09<07:33,  1.84s/it]predicting train subjects:  14%|█▎        | 39/285 [01:11<07:28,  1.83s/it]predicting train subjects:  14%|█▍        | 40/285 [01:13<07:27,  1.83s/it]predicting train subjects:  14%|█▍        | 41/285 [01:15<07:31,  1.85s/it]predicting train subjects:  15%|█▍        | 42/285 [01:16<07:30,  1.85s/it]predicting train subjects:  15%|█▌        | 43/285 [01:18<07:36,  1.88s/it]predicting train subjects:  15%|█▌        | 44/285 [01:20<07:27,  1.86s/it]predicting train subjects:  16%|█▌        | 45/285 [01:22<07:19,  1.83s/it]predicting train subjects:  16%|█▌        | 46/285 [01:23<07:01,  1.77s/it]predicting train subjects:  16%|█▋        | 47/285 [01:25<06:43,  1.69s/it]predicting train subjects:  17%|█▋        | 48/285 [01:27<06:28,  1.64s/it]predicting train subjects:  17%|█▋        | 49/285 [01:28<06:21,  1.62s/it]predicting train subjects:  18%|█▊        | 50/285 [01:30<06:09,  1.57s/it]predicting train subjects:  18%|█▊        | 51/285 [01:31<06:08,  1.57s/it]predicting train subjects:  18%|█▊        | 52/285 [01:33<06:04,  1.56s/it]predicting train subjects:  19%|█▊        | 53/285 [01:34<06:01,  1.56s/it]predicting train subjects:  19%|█▉        | 54/285 [01:36<05:56,  1.55s/it]predicting train subjects:  19%|█▉        | 55/285 [01:37<05:56,  1.55s/it]predicting train subjects:  20%|█▉        | 56/285 [01:39<05:54,  1.55s/it]predicting train subjects:  20%|██        | 57/285 [01:40<05:57,  1.57s/it]predicting train subjects:  20%|██        | 58/285 [01:42<05:55,  1.57s/it]predicting train subjects:  21%|██        | 59/285 [01:44<05:49,  1.55s/it]predicting train subjects:  21%|██        | 60/285 [01:45<05:42,  1.52s/it]predicting train subjects:  21%|██▏       | 61/285 [01:47<05:42,  1.53s/it]predicting train subjects:  22%|██▏       | 62/285 [01:48<05:47,  1.56s/it]predicting train subjects:  22%|██▏       | 63/285 [01:50<05:48,  1.57s/it]predicting train subjects:  22%|██▏       | 64/285 [01:51<05:48,  1.58s/it]predicting train subjects:  23%|██▎       | 65/285 [01:53<06:03,  1.65s/it]predicting train subjects:  23%|██▎       | 66/285 [01:55<06:08,  1.68s/it]predicting train subjects:  24%|██▎       | 67/285 [01:57<05:59,  1.65s/it]predicting train subjects:  24%|██▍       | 68/285 [01:58<05:56,  1.64s/it]predicting train subjects:  24%|██▍       | 69/285 [02:00<05:50,  1.62s/it]predicting train subjects:  25%|██▍       | 70/285 [02:01<05:48,  1.62s/it]predicting train subjects:  25%|██▍       | 71/285 [02:03<05:45,  1.62s/it]predicting train subjects:  25%|██▌       | 72/285 [02:05<05:47,  1.63s/it]predicting train subjects:  26%|██▌       | 73/285 [02:06<05:43,  1.62s/it]predicting train subjects:  26%|██▌       | 74/285 [02:08<05:38,  1.60s/it]predicting train subjects:  26%|██▋       | 75/285 [02:09<05:33,  1.59s/it]predicting train subjects:  27%|██▋       | 76/285 [02:11<05:30,  1.58s/it]predicting train subjects:  27%|██▋       | 77/285 [02:12<05:24,  1.56s/it]predicting train subjects:  27%|██▋       | 78/285 [02:14<05:27,  1.58s/it]predicting train subjects:  28%|██▊       | 79/285 [02:16<05:25,  1.58s/it]predicting train subjects:  28%|██▊       | 80/285 [02:17<05:27,  1.60s/it]predicting train subjects:  28%|██▊       | 81/285 [02:19<05:30,  1.62s/it]predicting train subjects:  29%|██▉       | 82/285 [02:20<05:26,  1.61s/it]predicting train subjects:  29%|██▉       | 83/285 [02:22<05:23,  1.60s/it]predicting train subjects:  29%|██▉       | 84/285 [02:24<05:20,  1.60s/it]predicting train subjects:  30%|██▉       | 85/285 [02:25<05:27,  1.64s/it]predicting train subjects:  30%|███       | 86/285 [02:27<05:45,  1.74s/it]predicting train subjects:  31%|███       | 87/285 [02:29<05:47,  1.76s/it]predicting train subjects:  31%|███       | 88/285 [02:31<05:53,  1.79s/it]predicting train subjects:  31%|███       | 89/285 [02:33<05:52,  1.80s/it]predicting train subjects:  32%|███▏      | 90/285 [02:35<05:50,  1.80s/it]predicting train subjects:  32%|███▏      | 91/285 [02:36<05:48,  1.80s/it]predicting train subjects:  32%|███▏      | 92/285 [02:38<05:52,  1.82s/it]predicting train subjects:  33%|███▎      | 93/285 [02:40<05:58,  1.87s/it]predicting train subjects:  33%|███▎      | 94/285 [02:42<05:51,  1.84s/it]predicting train subjects:  33%|███▎      | 95/285 [02:44<05:52,  1.86s/it]predicting train subjects:  34%|███▎      | 96/285 [02:46<05:52,  1.87s/it]predicting train subjects:  34%|███▍      | 97/285 [02:48<05:51,  1.87s/it]predicting train subjects:  34%|███▍      | 98/285 [02:50<05:45,  1.85s/it]predicting train subjects:  35%|███▍      | 99/285 [02:51<05:43,  1.85s/it]predicting train subjects:  35%|███▌      | 100/285 [02:53<05:40,  1.84s/it]predicting train subjects:  35%|███▌      | 101/285 [02:55<05:38,  1.84s/it]predicting train subjects:  36%|███▌      | 102/285 [02:57<05:37,  1.84s/it]predicting train subjects:  36%|███▌      | 103/285 [02:59<05:37,  1.85s/it]predicting train subjects:  36%|███▋      | 104/285 [03:01<05:32,  1.84s/it]predicting train subjects:  37%|███▋      | 105/285 [03:02<05:29,  1.83s/it]predicting train subjects:  37%|███▋      | 106/285 [03:04<05:24,  1.81s/it]predicting train subjects:  38%|███▊      | 107/285 [03:06<05:22,  1.81s/it]predicting train subjects:  38%|███▊      | 108/285 [03:08<05:15,  1.78s/it]predicting train subjects:  38%|███▊      | 109/285 [03:09<05:07,  1.75s/it]predicting train subjects:  39%|███▊      | 110/285 [03:11<05:02,  1.73s/it]predicting train subjects:  39%|███▉      | 111/285 [03:13<04:57,  1.71s/it]predicting train subjects:  39%|███▉      | 112/285 [03:14<04:57,  1.72s/it]predicting train subjects:  40%|███▉      | 113/285 [03:16<04:56,  1.73s/it]predicting train subjects:  40%|████      | 114/285 [03:18<04:55,  1.73s/it]predicting train subjects:  40%|████      | 115/285 [03:20<04:52,  1.72s/it]predicting train subjects:  41%|████      | 116/285 [03:21<04:51,  1.72s/it]predicting train subjects:  41%|████      | 117/285 [03:23<04:51,  1.73s/it]predicting train subjects:  41%|████▏     | 118/285 [03:25<04:51,  1.74s/it]predicting train subjects:  42%|████▏     | 119/285 [03:27<04:53,  1.77s/it]predicting train subjects:  42%|████▏     | 120/285 [03:28<04:52,  1.78s/it]predicting train subjects:  42%|████▏     | 121/285 [03:30<04:40,  1.71s/it]predicting train subjects:  43%|████▎     | 122/285 [03:31<04:25,  1.63s/it]predicting train subjects:  43%|████▎     | 123/285 [03:33<04:15,  1.57s/it]predicting train subjects:  44%|████▎     | 124/285 [03:35<04:15,  1.59s/it]predicting train subjects:  44%|████▍     | 125/285 [03:36<04:15,  1.59s/it]predicting train subjects:  44%|████▍     | 126/285 [03:38<04:17,  1.62s/it]predicting train subjects:  45%|████▍     | 127/285 [03:39<04:14,  1.61s/it]predicting train subjects:  45%|████▍     | 128/285 [03:41<04:13,  1.62s/it]predicting train subjects:  45%|████▌     | 129/285 [03:43<04:12,  1.62s/it]predicting train subjects:  46%|████▌     | 130/285 [03:44<04:08,  1.60s/it]predicting train subjects:  46%|████▌     | 131/285 [03:46<04:02,  1.57s/it]predicting train subjects:  46%|████▋     | 132/285 [03:47<03:58,  1.56s/it]predicting train subjects:  47%|████▋     | 133/285 [03:49<03:57,  1.56s/it]predicting train subjects:  47%|████▋     | 134/285 [03:50<03:58,  1.58s/it]predicting train subjects:  47%|████▋     | 135/285 [03:52<03:55,  1.57s/it]predicting train subjects:  48%|████▊     | 136/285 [03:54<03:51,  1.55s/it]predicting train subjects:  48%|████▊     | 137/285 [03:55<03:51,  1.57s/it]predicting train subjects:  48%|████▊     | 138/285 [03:57<03:54,  1.59s/it]predicting train subjects:  49%|████▉     | 139/285 [03:58<03:52,  1.59s/it]predicting train subjects:  49%|████▉     | 140/285 [04:00<03:48,  1.58s/it]predicting train subjects:  49%|████▉     | 141/285 [04:01<03:45,  1.57s/it]predicting train subjects:  50%|████▉     | 142/285 [04:03<03:37,  1.52s/it]predicting train subjects:  50%|█████     | 143/285 [04:04<03:34,  1.51s/it]predicting train subjects:  51%|█████     | 144/285 [04:06<03:29,  1.49s/it]predicting train subjects:  51%|█████     | 145/285 [04:07<03:23,  1.45s/it]predicting train subjects:  51%|█████     | 146/285 [04:08<03:17,  1.42s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:10<03:14,  1.41s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:11<03:12,  1.40s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:13<03:13,  1.42s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:14<03:11,  1.42s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:15<03:06,  1.39s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:17<03:05,  1.40s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:18<03:03,  1.39s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:20<03:05,  1.42s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:21<03:04,  1.42s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:23<03:00,  1.40s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:24<03:02,  1.42s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:25<03:01,  1.43s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:27<03:01,  1.44s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:28<03:01,  1.45s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:30<03:00,  1.46s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:31<02:58,  1.45s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:33<02:57,  1.46s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:34<02:55,  1.45s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:36<02:55,  1.46s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:37<02:51,  1.45s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:39<02:50,  1.45s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:40<02:49,  1.45s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:41<02:47,  1.44s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:43<02:44,  1.43s/it]predicting train subjects:  60%|██████    | 171/285 [04:44<02:41,  1.42s/it]predicting train subjects:  60%|██████    | 172/285 [04:46<02:41,  1.43s/it]predicting train subjects:  61%|██████    | 173/285 [04:47<02:40,  1.43s/it]predicting train subjects:  61%|██████    | 174/285 [04:49<02:39,  1.43s/it]predicting train subjects:  61%|██████▏   | 175/285 [04:50<02:37,  1.44s/it]predicting train subjects:  62%|██████▏   | 176/285 [04:51<02:37,  1.44s/it]predicting train subjects:  62%|██████▏   | 177/285 [04:53<02:36,  1.45s/it]predicting train subjects:  62%|██████▏   | 178/285 [04:54<02:33,  1.43s/it]predicting train subjects:  63%|██████▎   | 179/285 [04:56<02:31,  1.43s/it]predicting train subjects:  63%|██████▎   | 180/285 [04:57<02:29,  1.42s/it]predicting train subjects:  64%|██████▎   | 181/285 [04:59<02:27,  1.41s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:00<02:24,  1.40s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:01<02:22,  1.40s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:03<02:20,  1.39s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:04<02:18,  1.39s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:05<02:17,  1.39s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:07<02:16,  1.39s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:08<02:16,  1.40s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:10<02:12,  1.38s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:11<02:08,  1.35s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:12<02:05,  1.33s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:14<02:04,  1.34s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:15<02:01,  1.33s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:16<02:00,  1.32s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:17<01:57,  1.31s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:19<02:03,  1.39s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:21<02:08,  1.46s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:22<02:13,  1.53s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:24<02:15,  1.58s/it]predicting train subjects:  70%|███████   | 200/285 [05:26<02:15,  1.59s/it]predicting train subjects:  71%|███████   | 201/285 [05:27<02:14,  1.60s/it]predicting train subjects:  71%|███████   | 202/285 [05:29<02:15,  1.64s/it]predicting train subjects:  71%|███████   | 203/285 [05:31<02:15,  1.65s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:32<02:13,  1.65s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:34<02:11,  1.65s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:36<02:11,  1.67s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:37<02:11,  1.68s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:39<02:09,  1.68s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:41<02:07,  1.68s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:42<02:06,  1.69s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:44<02:03,  1.67s/it]predicting train subjects:  74%|███████▍  | 212/285 [05:46<02:02,  1.68s/it]predicting train subjects:  75%|███████▍  | 213/285 [05:47<01:59,  1.66s/it]predicting train subjects:  75%|███████▌  | 214/285 [05:49<01:53,  1.59s/it]predicting train subjects:  75%|███████▌  | 215/285 [05:50<01:48,  1.55s/it]predicting train subjects:  76%|███████▌  | 216/285 [05:52<01:44,  1.52s/it]predicting train subjects:  76%|███████▌  | 217/285 [05:53<01:42,  1.51s/it]predicting train subjects:  76%|███████▋  | 218/285 [05:55<01:39,  1.49s/it]predicting train subjects:  77%|███████▋  | 219/285 [05:56<01:37,  1.48s/it]predicting train subjects:  77%|███████▋  | 220/285 [05:58<01:35,  1.47s/it]predicting train subjects:  78%|███████▊  | 221/285 [05:59<01:33,  1.46s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:00<01:30,  1.44s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:02<01:28,  1.43s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:03<01:25,  1.40s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:05<01:24,  1.41s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:06<01:23,  1.42s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:07<01:22,  1.42s/it]predicting train subjects:  80%|████████  | 228/285 [06:09<01:23,  1.46s/it]predicting train subjects:  80%|████████  | 229/285 [06:10<01:21,  1.45s/it]predicting train subjects:  81%|████████  | 230/285 [06:12<01:19,  1.44s/it]predicting train subjects:  81%|████████  | 231/285 [06:13<01:17,  1.44s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:15<01:21,  1.54s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:17<01:24,  1.62s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:19<01:25,  1.68s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:21<01:27,  1.74s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:22<01:26,  1.77s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:24<01:25,  1.79s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:26<01:23,  1.78s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:28<01:21,  1.78s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:30<01:20,  1.78s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:31<01:18,  1.78s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:33<01:16,  1.77s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:35<01:13,  1.76s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:36<01:11,  1.73s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:38<01:10,  1.76s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:40<01:08,  1.76s/it]predicting train subjects:  87%|████████▋ | 247/285 [06:42<01:07,  1.77s/it]predicting train subjects:  87%|████████▋ | 248/285 [06:44<01:05,  1.77s/it]predicting train subjects:  87%|████████▋ | 249/285 [06:45<01:04,  1.78s/it]predicting train subjects:  88%|████████▊ | 250/285 [06:47<00:57,  1.65s/it]predicting train subjects:  88%|████████▊ | 251/285 [06:48<00:52,  1.55s/it]predicting train subjects:  88%|████████▊ | 252/285 [06:49<00:49,  1.50s/it]predicting train subjects:  89%|████████▉ | 253/285 [06:51<00:46,  1.46s/it]predicting train subjects:  89%|████████▉ | 254/285 [06:52<00:43,  1.41s/it]predicting train subjects:  89%|████████▉ | 255/285 [06:53<00:41,  1.39s/it]predicting train subjects:  90%|████████▉ | 256/285 [06:55<00:39,  1.38s/it]predicting train subjects:  90%|█████████ | 257/285 [06:56<00:37,  1.35s/it]predicting train subjects:  91%|█████████ | 258/285 [06:57<00:35,  1.33s/it]predicting train subjects:  91%|█████████ | 259/285 [06:59<00:34,  1.33s/it]predicting train subjects:  91%|█████████ | 260/285 [07:00<00:34,  1.37s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:01<00:32,  1.35s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:03<00:30,  1.34s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:04<00:29,  1.35s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:05<00:28,  1.34s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:07<00:27,  1.35s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:08<00:25,  1.34s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:09<00:24,  1.34s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:11<00:25,  1.48s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:13<00:25,  1.59s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:15<00:24,  1.66s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:17<00:23,  1.70s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:19<00:22,  1.74s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:20<00:21,  1.77s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:22<00:19,  1.79s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:24<00:18,  1.81s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:26<00:16,  1.83s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:28<00:14,  1.81s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:30<00:12,  1.81s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:31<00:10,  1.82s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:33<00:09,  1.84s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:35<00:07,  1.84s/it]predicting train subjects:  99%|█████████▉| 282/285 [07:37<00:05,  1.86s/it]predicting train subjects:  99%|█████████▉| 283/285 [07:39<00:03,  1.87s/it]predicting train subjects: 100%|█████████▉| 284/285 [07:41<00:01,  1.85s/it]predicting train subjects: 100%|██████████| 285/285 [07:43<00:00,  1.83s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:43,  1.42s/it]Loading train:   1%|          | 2/285 [00:02<06:46,  1.44s/it]Loading train:   1%|          | 3/285 [00:04<06:34,  1.40s/it]Loading train:   1%|▏         | 4/285 [00:05<06:47,  1.45s/it]Loading train:   2%|▏         | 5/285 [00:07<06:30,  1.39s/it]Loading train:   2%|▏         | 6/285 [00:08<06:43,  1.45s/it]Loading train:   2%|▏         | 7/285 [00:10<07:12,  1.56s/it]Loading train:   3%|▎         | 8/285 [00:12<07:18,  1.58s/it]Loading train:   3%|▎         | 9/285 [00:13<06:56,  1.51s/it]Loading train:   4%|▎         | 10/285 [00:14<06:26,  1.40s/it]Loading train:   4%|▍         | 11/285 [00:15<06:08,  1.34s/it]Loading train:   4%|▍         | 12/285 [00:16<05:45,  1.27s/it]Loading train:   5%|▍         | 13/285 [00:17<05:33,  1.23s/it]Loading train:   5%|▍         | 14/285 [00:19<05:40,  1.26s/it]Loading train:   5%|▌         | 15/285 [00:20<05:34,  1.24s/it]Loading train:   6%|▌         | 16/285 [00:21<05:26,  1.21s/it]Loading train:   6%|▌         | 17/285 [00:22<05:20,  1.20s/it]Loading train:   6%|▋         | 18/285 [00:24<05:26,  1.22s/it]Loading train:   7%|▋         | 19/285 [00:25<05:33,  1.25s/it]Loading train:   7%|▋         | 20/285 [00:26<05:35,  1.27s/it]Loading train:   7%|▋         | 21/285 [00:27<05:31,  1.25s/it]Loading train:   8%|▊         | 22/285 [00:29<05:33,  1.27s/it]Loading train:   8%|▊         | 23/285 [00:30<05:21,  1.23s/it]Loading train:   8%|▊         | 24/285 [00:31<05:32,  1.27s/it]Loading train:   9%|▉         | 25/285 [00:32<05:27,  1.26s/it]Loading train:   9%|▉         | 26/285 [00:34<05:25,  1.26s/it]Loading train:   9%|▉         | 27/285 [00:35<05:21,  1.25s/it]Loading train:  10%|▉         | 28/285 [00:36<05:12,  1.21s/it]Loading train:  10%|█         | 29/285 [00:37<05:08,  1.21s/it]Loading train:  11%|█         | 30/285 [00:38<04:58,  1.17s/it]Loading train:  11%|█         | 31/285 [00:39<04:52,  1.15s/it]Loading train:  11%|█         | 32/285 [00:41<04:49,  1.15s/it]Loading train:  12%|█▏        | 33/285 [00:42<04:43,  1.13s/it]Loading train:  12%|█▏        | 34/285 [00:43<04:30,  1.08s/it]Loading train:  12%|█▏        | 35/285 [00:44<04:29,  1.08s/it]Loading train:  13%|█▎        | 36/285 [00:45<04:37,  1.11s/it]Loading train:  13%|█▎        | 37/285 [00:46<04:35,  1.11s/it]Loading train:  13%|█▎        | 38/285 [00:47<04:39,  1.13s/it]Loading train:  14%|█▎        | 39/285 [00:48<04:33,  1.11s/it]Loading train:  14%|█▍        | 40/285 [00:49<04:32,  1.11s/it]Loading train:  14%|█▍        | 41/285 [00:51<04:31,  1.11s/it]Loading train:  15%|█▍        | 42/285 [00:52<04:44,  1.17s/it]Loading train:  15%|█▌        | 43/285 [00:53<04:35,  1.14s/it]Loading train:  15%|█▌        | 44/285 [00:54<04:28,  1.12s/it]Loading train:  16%|█▌        | 45/285 [00:55<04:26,  1.11s/it]Loading train:  16%|█▌        | 46/285 [00:56<04:21,  1.09s/it]Loading train:  16%|█▋        | 47/285 [00:57<04:13,  1.06s/it]Loading train:  17%|█▋        | 48/285 [00:58<04:03,  1.03s/it]Loading train:  17%|█▋        | 49/285 [00:59<04:01,  1.02s/it]Loading train:  18%|█▊        | 50/285 [01:00<04:05,  1.04s/it]Loading train:  18%|█▊        | 51/285 [01:01<03:56,  1.01s/it]Loading train:  18%|█▊        | 52/285 [01:02<04:03,  1.04s/it]Loading train:  19%|█▊        | 53/285 [01:03<04:02,  1.04s/it]Loading train:  19%|█▉        | 54/285 [01:04<03:58,  1.03s/it]Loading train:  19%|█▉        | 55/285 [01:05<03:57,  1.03s/it]Loading train:  20%|█▉        | 56/285 [01:07<04:10,  1.09s/it]Loading train:  20%|██        | 57/285 [01:07<03:59,  1.05s/it]Loading train:  20%|██        | 58/285 [01:09<03:59,  1.05s/it]Loading train:  21%|██        | 59/285 [01:10<03:54,  1.04s/it]Loading train:  21%|██        | 60/285 [01:10<03:43,  1.01it/s]Loading train:  21%|██▏       | 61/285 [01:11<03:43,  1.00it/s]Loading train:  22%|██▏       | 62/285 [01:12<03:47,  1.02s/it]Loading train:  22%|██▏       | 63/285 [01:13<03:40,  1.01it/s]Loading train:  22%|██▏       | 64/285 [01:15<04:21,  1.18s/it]Loading train:  23%|██▎       | 65/285 [01:17<04:50,  1.32s/it]Loading train:  23%|██▎       | 66/285 [01:18<05:01,  1.37s/it]Loading train:  24%|██▎       | 67/285 [01:19<04:42,  1.29s/it]Loading train:  24%|██▍       | 68/285 [01:20<04:25,  1.22s/it]Loading train:  24%|██▍       | 69/285 [01:21<04:16,  1.19s/it]Loading train:  25%|██▍       | 70/285 [01:23<04:10,  1.17s/it]Loading train:  25%|██▍       | 71/285 [01:24<04:03,  1.14s/it]Loading train:  25%|██▌       | 72/285 [01:25<04:00,  1.13s/it]Loading train:  26%|██▌       | 73/285 [01:26<03:53,  1.10s/it]Loading train:  26%|██▌       | 74/285 [01:27<03:49,  1.09s/it]Loading train:  26%|██▋       | 75/285 [01:28<03:47,  1.09s/it]Loading train:  27%|██▋       | 76/285 [01:29<03:38,  1.04s/it]Loading train:  27%|██▋       | 77/285 [01:30<03:35,  1.04s/it]Loading train:  27%|██▋       | 78/285 [01:31<03:34,  1.04s/it]Loading train:  28%|██▊       | 79/285 [01:32<03:32,  1.03s/it]Loading train:  28%|██▊       | 80/285 [01:33<03:31,  1.03s/it]Loading train:  28%|██▊       | 81/285 [01:34<03:29,  1.03s/it]Loading train:  29%|██▉       | 82/285 [01:35<03:27,  1.02s/it]Loading train:  29%|██▉       | 83/285 [01:36<03:25,  1.02s/it]Loading train:  29%|██▉       | 84/285 [01:37<03:31,  1.05s/it]Loading train:  30%|██▉       | 85/285 [01:38<03:33,  1.07s/it]Loading train:  30%|███       | 86/285 [01:40<03:46,  1.14s/it]Loading train:  31%|███       | 87/285 [01:41<03:41,  1.12s/it]Loading train:  31%|███       | 88/285 [01:42<03:35,  1.09s/it]Loading train:  31%|███       | 89/285 [01:43<03:30,  1.07s/it]Loading train:  32%|███▏      | 90/285 [01:44<03:29,  1.08s/it]Loading train:  32%|███▏      | 91/285 [01:45<03:31,  1.09s/it]Loading train:  32%|███▏      | 92/285 [01:46<03:29,  1.08s/it]Loading train:  33%|███▎      | 93/285 [01:47<03:35,  1.12s/it]Loading train:  33%|███▎      | 94/285 [01:48<03:31,  1.11s/it]Loading train:  33%|███▎      | 95/285 [01:49<03:34,  1.13s/it]Loading train:  34%|███▎      | 96/285 [01:50<03:29,  1.11s/it]Loading train:  34%|███▍      | 97/285 [01:52<03:34,  1.14s/it]Loading train:  34%|███▍      | 98/285 [01:53<03:33,  1.14s/it]Loading train:  35%|███▍      | 99/285 [01:54<03:26,  1.11s/it]Loading train:  35%|███▌      | 100/285 [01:55<03:31,  1.14s/it]Loading train:  35%|███▌      | 101/285 [01:56<03:32,  1.15s/it]Loading train:  36%|███▌      | 102/285 [01:57<03:32,  1.16s/it]Loading train:  36%|███▌      | 103/285 [01:59<03:27,  1.14s/it]Loading train:  36%|███▋      | 104/285 [02:00<03:22,  1.12s/it]Loading train:  37%|███▋      | 105/285 [02:01<03:13,  1.08s/it]Loading train:  37%|███▋      | 106/285 [02:02<03:17,  1.10s/it]Loading train:  38%|███▊      | 107/285 [02:03<03:14,  1.09s/it]Loading train:  38%|███▊      | 108/285 [02:04<03:13,  1.10s/it]Loading train:  38%|███▊      | 109/285 [02:05<03:10,  1.08s/it]Loading train:  39%|███▊      | 110/285 [02:06<03:09,  1.08s/it]Loading train:  39%|███▉      | 111/285 [02:07<03:08,  1.08s/it]Loading train:  39%|███▉      | 112/285 [02:08<03:05,  1.07s/it]Loading train:  40%|███▉      | 113/285 [02:09<03:00,  1.05s/it]Loading train:  40%|████      | 114/285 [02:10<02:59,  1.05s/it]Loading train:  40%|████      | 115/285 [02:11<03:01,  1.07s/it]Loading train:  41%|████      | 116/285 [02:12<03:01,  1.07s/it]Loading train:  41%|████      | 117/285 [02:14<03:00,  1.08s/it]Loading train:  41%|████▏     | 118/285 [02:15<03:03,  1.10s/it]Loading train:  42%|████▏     | 119/285 [02:16<03:01,  1.09s/it]Loading train:  42%|████▏     | 120/285 [02:17<03:02,  1.11s/it]Loading train:  42%|████▏     | 121/285 [02:18<03:18,  1.21s/it]Loading train:  43%|████▎     | 122/285 [02:20<03:16,  1.21s/it]Loading train:  43%|████▎     | 123/285 [02:21<03:25,  1.27s/it]Loading train:  44%|████▎     | 124/285 [02:22<03:09,  1.18s/it]Loading train:  44%|████▍     | 125/285 [02:23<02:55,  1.10s/it]Loading train:  44%|████▍     | 126/285 [02:24<02:49,  1.06s/it]Loading train:  45%|████▍     | 127/285 [02:25<02:43,  1.03s/it]Loading train:  45%|████▍     | 128/285 [02:26<02:39,  1.02s/it]Loading train:  45%|████▌     | 129/285 [02:27<02:35,  1.00it/s]Loading train:  46%|████▌     | 130/285 [02:28<02:34,  1.00it/s]Loading train:  46%|████▌     | 131/285 [02:29<02:28,  1.04it/s]Loading train:  46%|████▋     | 132/285 [02:30<02:31,  1.01it/s]Loading train:  47%|████▋     | 133/285 [02:31<02:27,  1.03it/s]Loading train:  47%|████▋     | 134/285 [02:32<02:25,  1.04it/s]Loading train:  47%|████▋     | 135/285 [02:33<02:27,  1.02it/s]Loading train:  48%|████▊     | 136/285 [02:33<02:22,  1.04it/s]Loading train:  48%|████▊     | 137/285 [02:35<02:28,  1.00s/it]Loading train:  48%|████▊     | 138/285 [02:36<02:32,  1.04s/it]Loading train:  49%|████▉     | 139/285 [02:37<02:27,  1.01s/it]Loading train:  49%|████▉     | 140/285 [02:38<02:21,  1.03it/s]Loading train:  49%|████▉     | 141/285 [02:39<02:22,  1.01it/s]Loading train:  50%|████▉     | 142/285 [02:40<02:23,  1.00s/it]Loading train:  50%|█████     | 143/285 [02:41<02:18,  1.02it/s]Loading train:  51%|█████     | 144/285 [02:41<02:17,  1.03it/s]Loading train:  51%|█████     | 145/285 [02:43<02:18,  1.01it/s]Loading train:  51%|█████     | 146/285 [02:43<02:13,  1.04it/s]Loading train:  52%|█████▏    | 147/285 [02:44<02:11,  1.05it/s]Loading train:  52%|█████▏    | 148/285 [02:45<02:11,  1.04it/s]Loading train:  52%|█████▏    | 149/285 [02:46<02:12,  1.03it/s]Loading train:  53%|█████▎    | 150/285 [02:47<02:08,  1.05it/s]Loading train:  53%|█████▎    | 151/285 [02:48<02:04,  1.08it/s]Loading train:  53%|█████▎    | 152/285 [02:49<02:05,  1.06it/s]Loading train:  54%|█████▎    | 153/285 [02:50<02:06,  1.04it/s]Loading train:  54%|█████▍    | 154/285 [02:51<02:07,  1.03it/s]Loading train:  54%|█████▍    | 155/285 [02:52<02:01,  1.07it/s]Loading train:  55%|█████▍    | 156/285 [02:53<01:57,  1.09it/s]Loading train:  55%|█████▌    | 157/285 [02:54<01:55,  1.11it/s]Loading train:  55%|█████▌    | 158/285 [02:55<01:54,  1.11it/s]Loading train:  56%|█████▌    | 159/285 [02:56<01:55,  1.09it/s]Loading train:  56%|█████▌    | 160/285 [02:56<01:55,  1.08it/s]Loading train:  56%|█████▋    | 161/285 [02:57<01:56,  1.07it/s]Loading train:  57%|█████▋    | 162/285 [02:58<01:52,  1.10it/s]Loading train:  57%|█████▋    | 163/285 [02:59<01:50,  1.11it/s]Loading train:  58%|█████▊    | 164/285 [03:00<01:51,  1.09it/s]Loading train:  58%|█████▊    | 165/285 [03:01<01:49,  1.10it/s]Loading train:  58%|█████▊    | 166/285 [03:02<01:49,  1.09it/s]Loading train:  59%|█████▊    | 167/285 [03:03<01:51,  1.06it/s]Loading train:  59%|█████▉    | 168/285 [03:04<01:50,  1.06it/s]Loading train:  59%|█████▉    | 169/285 [03:05<01:49,  1.06it/s]Loading train:  60%|█████▉    | 170/285 [03:06<01:44,  1.10it/s]Loading train:  60%|██████    | 171/285 [03:07<01:42,  1.11it/s]Loading train:  60%|██████    | 172/285 [03:07<01:40,  1.12it/s]Loading train:  61%|██████    | 173/285 [03:08<01:40,  1.12it/s]Loading train:  61%|██████    | 174/285 [03:09<01:38,  1.12it/s]Loading train:  61%|██████▏   | 175/285 [03:10<01:35,  1.15it/s]Loading train:  62%|██████▏   | 176/285 [03:11<01:34,  1.15it/s]Loading train:  62%|██████▏   | 177/285 [03:12<01:37,  1.11it/s]Loading train:  62%|██████▏   | 178/285 [03:13<01:36,  1.11it/s]Loading train:  63%|██████▎   | 179/285 [03:14<01:34,  1.13it/s]Loading train:  63%|██████▎   | 180/285 [03:15<01:36,  1.09it/s]Loading train:  64%|██████▎   | 181/285 [03:16<01:37,  1.07it/s]Loading train:  64%|██████▍   | 182/285 [03:16<01:33,  1.10it/s]Loading train:  64%|██████▍   | 183/285 [03:17<01:31,  1.11it/s]Loading train:  65%|██████▍   | 184/285 [03:18<01:30,  1.12it/s]Loading train:  65%|██████▍   | 185/285 [03:19<01:29,  1.12it/s]Loading train:  65%|██████▌   | 186/285 [03:20<01:27,  1.13it/s]Loading train:  66%|██████▌   | 187/285 [03:21<01:25,  1.15it/s]Loading train:  66%|██████▌   | 188/285 [03:22<01:23,  1.16it/s]Loading train:  66%|██████▋   | 189/285 [03:22<01:23,  1.15it/s]Loading train:  67%|██████▋   | 190/285 [03:23<01:22,  1.15it/s]Loading train:  67%|██████▋   | 191/285 [03:24<01:22,  1.15it/s]Loading train:  67%|██████▋   | 192/285 [03:25<01:20,  1.16it/s]Loading train:  68%|██████▊   | 193/285 [03:26<01:20,  1.14it/s]Loading train:  68%|██████▊   | 194/285 [03:27<01:20,  1.13it/s]Loading train:  68%|██████▊   | 195/285 [03:28<01:20,  1.12it/s]Loading train:  69%|██████▉   | 196/285 [03:29<01:23,  1.07it/s]Loading train:  69%|██████▉   | 197/285 [03:30<01:23,  1.05it/s]Loading train:  69%|██████▉   | 198/285 [03:31<01:24,  1.03it/s]Loading train:  70%|██████▉   | 199/285 [03:32<01:22,  1.05it/s]Loading train:  70%|███████   | 200/285 [03:33<01:19,  1.07it/s]Loading train:  71%|███████   | 201/285 [03:34<01:18,  1.07it/s]Loading train:  71%|███████   | 202/285 [03:35<01:18,  1.05it/s]Loading train:  71%|███████   | 203/285 [03:36<01:17,  1.06it/s]Loading train:  72%|███████▏  | 204/285 [03:36<01:14,  1.09it/s]Loading train:  72%|███████▏  | 205/285 [03:37<01:13,  1.09it/s]Loading train:  72%|███████▏  | 206/285 [03:38<01:12,  1.08it/s]Loading train:  73%|███████▎  | 207/285 [03:39<01:13,  1.06it/s]Loading train:  73%|███████▎  | 208/285 [03:40<01:16,  1.01it/s]Loading train:  73%|███████▎  | 209/285 [03:41<01:13,  1.03it/s]Loading train:  74%|███████▎  | 210/285 [03:42<01:11,  1.05it/s]Loading train:  74%|███████▍  | 211/285 [03:43<01:12,  1.02it/s]Loading train:  74%|███████▍  | 212/285 [03:44<01:12,  1.01it/s]Loading train:  75%|███████▍  | 213/285 [03:45<01:11,  1.01it/s]Loading train:  75%|███████▌  | 214/285 [03:46<01:09,  1.03it/s]Loading train:  75%|███████▌  | 215/285 [03:47<01:06,  1.05it/s]Loading train:  76%|███████▌  | 216/285 [03:48<01:05,  1.05it/s]Loading train:  76%|███████▌  | 217/285 [03:49<01:04,  1.06it/s]Loading train:  76%|███████▋  | 218/285 [03:50<01:02,  1.08it/s]Loading train:  77%|███████▋  | 219/285 [03:51<01:03,  1.04it/s]Loading train:  77%|███████▋  | 220/285 [03:52<01:01,  1.06it/s]Loading train:  78%|███████▊  | 221/285 [03:53<00:58,  1.10it/s]Loading train:  78%|███████▊  | 222/285 [03:53<00:56,  1.12it/s]Loading train:  78%|███████▊  | 223/285 [03:54<00:55,  1.13it/s]Loading train:  79%|███████▊  | 224/285 [03:55<00:55,  1.10it/s]Loading train:  79%|███████▉  | 225/285 [03:56<00:56,  1.06it/s]Loading train:  79%|███████▉  | 226/285 [03:57<00:55,  1.07it/s]Loading train:  80%|███████▉  | 227/285 [03:58<00:53,  1.08it/s]Loading train:  80%|████████  | 228/285 [03:59<00:52,  1.08it/s]Loading train:  80%|████████  | 229/285 [04:00<00:50,  1.10it/s]Loading train:  81%|████████  | 230/285 [04:01<00:48,  1.13it/s]Loading train:  81%|████████  | 231/285 [04:02<00:46,  1.15it/s]Loading train:  81%|████████▏ | 232/285 [04:03<00:49,  1.08it/s]Loading train:  82%|████████▏ | 233/285 [04:04<00:50,  1.03it/s]Loading train:  82%|████████▏ | 234/285 [04:05<00:50,  1.01it/s]Loading train:  82%|████████▏ | 235/285 [04:06<00:50,  1.02s/it]Loading train:  83%|████████▎ | 236/285 [04:07<00:50,  1.04s/it]Loading train:  83%|████████▎ | 237/285 [04:08<00:51,  1.06s/it]Loading train:  84%|████████▎ | 238/285 [04:09<00:49,  1.06s/it]Loading train:  84%|████████▍ | 239/285 [04:10<00:50,  1.09s/it]Loading train:  84%|████████▍ | 240/285 [04:11<00:47,  1.06s/it]Loading train:  85%|████████▍ | 241/285 [04:12<00:47,  1.09s/it]Loading train:  85%|████████▍ | 242/285 [04:13<00:45,  1.07s/it]Loading train:  85%|████████▌ | 243/285 [04:14<00:43,  1.05s/it]Loading train:  86%|████████▌ | 244/285 [04:15<00:42,  1.03s/it]Loading train:  86%|████████▌ | 245/285 [04:16<00:41,  1.04s/it]Loading train:  86%|████████▋ | 246/285 [04:17<00:40,  1.04s/it]Loading train:  87%|████████▋ | 247/285 [04:19<00:39,  1.04s/it]Loading train:  87%|████████▋ | 248/285 [04:20<00:39,  1.06s/it]Loading train:  87%|████████▋ | 249/285 [04:21<00:37,  1.05s/it]Loading train:  88%|████████▊ | 250/285 [04:22<00:36,  1.03s/it]Loading train:  88%|████████▊ | 251/285 [04:23<00:35,  1.04s/it]Loading train:  88%|████████▊ | 252/285 [04:24<00:33,  1.01s/it]Loading train:  89%|████████▉ | 253/285 [04:25<00:32,  1.02s/it]Loading train:  89%|████████▉ | 254/285 [04:26<00:30,  1.01it/s]Loading train:  89%|████████▉ | 255/285 [04:27<00:29,  1.03it/s]Loading train:  90%|████████▉ | 256/285 [04:28<00:28,  1.03it/s]Loading train:  90%|█████████ | 257/285 [04:28<00:26,  1.05it/s]Loading train:  91%|█████████ | 258/285 [04:29<00:25,  1.05it/s]Loading train:  91%|█████████ | 259/285 [04:30<00:23,  1.09it/s]Loading train:  91%|█████████ | 260/285 [04:31<00:22,  1.09it/s]Loading train:  92%|█████████▏| 261/285 [04:32<00:21,  1.11it/s]Loading train:  92%|█████████▏| 262/285 [04:33<00:20,  1.10it/s]Loading train:  92%|█████████▏| 263/285 [04:34<00:20,  1.10it/s]Loading train:  93%|█████████▎| 264/285 [04:35<00:19,  1.10it/s]Loading train:  93%|█████████▎| 265/285 [04:36<00:18,  1.08it/s]Loading train:  93%|█████████▎| 266/285 [04:37<00:18,  1.05it/s]Loading train:  94%|█████████▎| 267/285 [04:38<00:16,  1.07it/s]Loading train:  94%|█████████▍| 268/285 [04:39<00:17,  1.02s/it]Loading train:  94%|█████████▍| 269/285 [04:40<00:17,  1.07s/it]Loading train:  95%|█████████▍| 270/285 [04:41<00:16,  1.11s/it]Loading train:  95%|█████████▌| 271/285 [04:42<00:15,  1.12s/it]Loading train:  95%|█████████▌| 272/285 [04:44<00:14,  1.13s/it]Loading train:  96%|█████████▌| 273/285 [04:45<00:13,  1.11s/it]Loading train:  96%|█████████▌| 274/285 [04:46<00:12,  1.15s/it]Loading train:  96%|█████████▋| 275/285 [04:47<00:11,  1.16s/it]Loading train:  97%|█████████▋| 276/285 [04:48<00:10,  1.19s/it]Loading train:  97%|█████████▋| 277/285 [04:49<00:09,  1.16s/it]Loading train:  98%|█████████▊| 278/285 [04:50<00:07,  1.14s/it]Loading train:  98%|█████████▊| 279/285 [04:52<00:07,  1.17s/it]Loading train:  98%|█████████▊| 280/285 [04:53<00:05,  1.14s/it]Loading train:  99%|█████████▊| 281/285 [04:54<00:04,  1.09s/it]Loading train:  99%|█████████▉| 282/285 [04:55<00:03,  1.06s/it]Loading train:  99%|█████████▉| 283/285 [04:56<00:02,  1.05s/it]Loading train: 100%|█████████▉| 284/285 [04:57<00:01,  1.04s/it]Loading train: 100%|██████████| 285/285 [04:58<00:00,  1.04s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/285 [00:00<00:04, 59.74it/s]concatenating: train:   9%|▉         | 25/285 [00:00<00:03, 74.98it/s]concatenating: train:  20%|██        | 57/285 [00:00<00:02, 97.21it/s]concatenating: train:  28%|██▊       | 80/285 [00:00<00:01, 117.10it/s]concatenating: train:  35%|███▍      | 99/285 [00:00<00:01, 130.31it/s]concatenating: train:  43%|████▎     | 123/285 [00:00<00:01, 150.58it/s]concatenating: train:  52%|█████▏    | 149/285 [00:00<00:00, 171.68it/s]concatenating: train:  60%|█████▉    | 170/285 [00:00<00:00, 168.17it/s]concatenating: train:  70%|██████▉   | 199/285 [00:00<00:00, 191.54it/s]concatenating: train:  80%|████████  | 228/285 [00:01<00:00, 212.78it/s]concatenating: train:  89%|████████▉ | 253/285 [00:01<00:00, 213.49it/s]concatenating: train:  97%|█████████▋| 277/285 [00:01<00:00, 155.86it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 198.01it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.47s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.42s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.35s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 210.15it/s]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:10,  1.30s/it]Loading train:   1%|          | 2/285 [00:02<06:13,  1.32s/it]Loading train:   1%|          | 3/285 [00:03<05:59,  1.28s/it]Loading train:   1%|▏         | 4/285 [00:05<06:23,  1.36s/it]Loading train:   2%|▏         | 5/285 [00:06<05:53,  1.26s/it]Loading train:   2%|▏         | 6/285 [00:08<06:21,  1.37s/it]Loading train:   2%|▏         | 7/285 [00:09<06:38,  1.44s/it]Loading train:   3%|▎         | 8/285 [00:11<06:49,  1.48s/it]Loading train:   3%|▎         | 9/285 [00:12<06:28,  1.41s/it]Loading train:   4%|▎         | 10/285 [00:13<05:54,  1.29s/it]Loading train:   4%|▍         | 11/285 [00:14<05:31,  1.21s/it]Loading train:   4%|▍         | 12/285 [00:15<05:09,  1.13s/it]Loading train:   5%|▍         | 13/285 [00:16<04:51,  1.07s/it]Loading train:   5%|▍         | 14/285 [00:17<04:38,  1.03s/it]Loading train:   5%|▌         | 15/285 [00:18<04:27,  1.01it/s]Loading train:   6%|▌         | 16/285 [00:19<04:16,  1.05it/s]Loading train:   6%|▌         | 17/285 [00:19<04:12,  1.06it/s]Loading train:   6%|▋         | 18/285 [00:20<04:11,  1.06it/s]Loading train:   7%|▋         | 19/285 [00:21<04:16,  1.04it/s]Loading train:   7%|▋         | 20/285 [00:23<04:22,  1.01it/s]Loading train:   7%|▋         | 21/285 [00:23<04:20,  1.01it/s]Loading train:   8%|▊         | 22/285 [00:24<04:15,  1.03it/s]Loading train:   8%|▊         | 23/285 [00:25<04:10,  1.05it/s]Loading train:   8%|▊         | 24/285 [00:26<04:16,  1.02it/s]Loading train:   9%|▉         | 25/285 [00:27<04:12,  1.03it/s]Loading train:   9%|▉         | 26/285 [00:28<04:19,  1.00s/it]Loading train:   9%|▉         | 27/285 [00:29<04:15,  1.01it/s]Loading train:  10%|▉         | 28/285 [00:30<04:19,  1.01s/it]Loading train:  10%|█         | 29/285 [00:31<04:03,  1.05it/s]Loading train:  11%|█         | 30/285 [00:32<04:04,  1.04it/s]Loading train:  11%|█         | 31/285 [00:33<04:03,  1.04it/s]Loading train:  11%|█         | 32/285 [00:34<03:58,  1.06it/s]Loading train:  12%|█▏        | 33/285 [00:35<03:51,  1.09it/s]Loading train:  12%|█▏        | 34/285 [00:36<03:43,  1.12it/s]Loading train:  12%|█▏        | 35/285 [00:37<03:39,  1.14it/s]Loading train:  13%|█▎        | 36/285 [00:37<03:39,  1.13it/s]Loading train:  13%|█▎        | 37/285 [00:38<03:38,  1.14it/s]Loading train:  13%|█▎        | 38/285 [00:39<03:48,  1.08it/s]Loading train:  14%|█▎        | 39/285 [00:40<03:49,  1.07it/s]Loading train:  14%|█▍        | 40/285 [00:41<03:43,  1.10it/s]Loading train:  14%|█▍        | 41/285 [00:42<03:40,  1.11it/s]Loading train:  15%|█▍        | 42/285 [00:43<03:41,  1.10it/s]Loading train:  15%|█▌        | 43/285 [00:44<03:47,  1.06it/s]Loading train:  15%|█▌        | 44/285 [00:45<04:08,  1.03s/it]Loading train:  16%|█▌        | 45/285 [00:46<04:06,  1.03s/it]Loading train:  16%|█▌        | 46/285 [00:47<03:56,  1.01it/s]Loading train:  16%|█▋        | 47/285 [00:48<03:43,  1.06it/s]Loading train:  17%|█▋        | 48/285 [00:49<03:34,  1.11it/s]Loading train:  17%|█▋        | 49/285 [00:50<03:27,  1.14it/s]Loading train:  18%|█▊        | 50/285 [00:50<03:22,  1.16it/s]Loading train:  18%|█▊        | 51/285 [00:51<03:23,  1.15it/s]Loading train:  18%|█▊        | 52/285 [00:52<03:18,  1.17it/s]Loading train:  19%|█▊        | 53/285 [00:53<03:11,  1.21it/s]Loading train:  19%|█▉        | 54/285 [00:54<03:02,  1.27it/s]Loading train:  19%|█▉        | 55/285 [00:54<02:56,  1.30it/s]Loading train:  20%|█▉        | 56/285 [00:55<02:51,  1.34it/s]Loading train:  20%|██        | 57/285 [00:56<02:48,  1.35it/s]Loading train:  20%|██        | 58/285 [00:57<02:46,  1.36it/s]Loading train:  21%|██        | 59/285 [00:57<02:49,  1.33it/s]Loading train:  21%|██        | 60/285 [00:58<02:50,  1.32it/s]Loading train:  21%|██▏       | 61/285 [00:59<02:53,  1.29it/s]Loading train:  22%|██▏       | 62/285 [01:00<02:46,  1.34it/s]Loading train:  22%|██▏       | 63/285 [01:00<02:40,  1.39it/s]Loading train:  22%|██▏       | 64/285 [01:01<03:13,  1.14it/s]Loading train:  23%|██▎       | 65/285 [01:03<03:55,  1.07s/it]Loading train:  23%|██▎       | 66/285 [01:04<04:10,  1.15s/it]Loading train:  24%|██▎       | 67/285 [01:05<03:51,  1.06s/it]Loading train:  24%|██▍       | 68/285 [01:06<03:34,  1.01it/s]Loading train:  24%|██▍       | 69/285 [01:07<03:23,  1.06it/s]Loading train:  25%|██▍       | 70/285 [01:08<03:28,  1.03it/s]Loading train:  25%|██▍       | 71/285 [01:09<03:34,  1.00s/it]Loading train:  25%|██▌       | 72/285 [01:10<03:24,  1.04it/s]Loading train:  26%|██▌       | 73/285 [01:11<03:11,  1.11it/s]Loading train:  26%|██▌       | 74/285 [01:11<03:04,  1.14it/s]Loading train:  26%|██▋       | 75/285 [01:12<03:00,  1.16it/s]Loading train:  27%|██▋       | 76/285 [01:13<02:58,  1.17it/s]Loading train:  27%|██▋       | 77/285 [01:14<02:59,  1.16it/s]Loading train:  27%|██▋       | 78/285 [01:15<03:05,  1.12it/s]Loading train:  28%|██▊       | 79/285 [01:16<03:00,  1.14it/s]Loading train:  28%|██▊       | 80/285 [01:17<02:58,  1.15it/s]Loading train:  28%|██▊       | 81/285 [01:17<02:55,  1.16it/s]Loading train:  29%|██▉       | 82/285 [01:18<02:52,  1.17it/s]Loading train:  29%|██▉       | 83/285 [01:19<02:49,  1.19it/s]Loading train:  29%|██▉       | 84/285 [01:20<02:56,  1.14it/s]Loading train:  30%|██▉       | 85/285 [01:21<03:07,  1.07it/s]Loading train:  30%|███       | 86/285 [01:22<03:09,  1.05it/s]Loading train:  31%|███       | 87/285 [01:23<03:10,  1.04it/s]Loading train:  31%|███       | 88/285 [01:24<03:03,  1.08it/s]Loading train:  31%|███       | 89/285 [01:25<03:03,  1.07it/s]Loading train:  32%|███▏      | 90/285 [01:26<03:02,  1.07it/s]Loading train:  32%|███▏      | 91/285 [01:27<03:04,  1.05it/s]Loading train:  32%|███▏      | 92/285 [01:28<03:00,  1.07it/s]Loading train:  33%|███▎      | 93/285 [01:29<03:16,  1.02s/it]Loading train:  33%|███▎      | 94/285 [01:30<03:16,  1.03s/it]Loading train:  33%|███▎      | 95/285 [01:31<03:09,  1.00it/s]Loading train:  34%|███▎      | 96/285 [01:32<03:01,  1.04it/s]Loading train:  34%|███▍      | 97/285 [01:33<03:02,  1.03it/s]Loading train:  34%|███▍      | 98/285 [01:34<02:57,  1.05it/s]Loading train:  35%|███▍      | 99/285 [01:35<02:54,  1.07it/s]Loading train:  35%|███▌      | 100/285 [01:35<02:52,  1.08it/s]Loading train:  35%|███▌      | 101/285 [01:36<02:50,  1.08it/s]Loading train:  36%|███▌      | 102/285 [01:37<02:47,  1.10it/s]Loading train:  36%|███▌      | 103/285 [01:38<02:53,  1.05it/s]Loading train:  36%|███▋      | 104/285 [01:39<02:46,  1.09it/s]Loading train:  37%|███▋      | 105/285 [01:40<02:48,  1.07it/s]Loading train:  37%|███▋      | 106/285 [01:41<02:44,  1.09it/s]Loading train:  38%|███▊      | 107/285 [01:42<02:44,  1.08it/s]Loading train:  38%|███▊      | 108/285 [01:43<02:46,  1.07it/s]Loading train:  38%|███▊      | 109/285 [01:44<02:45,  1.06it/s]Loading train:  39%|███▊      | 110/285 [01:45<02:45,  1.06it/s]Loading train:  39%|███▉      | 111/285 [01:46<02:42,  1.07it/s]Loading train:  39%|███▉      | 112/285 [01:47<02:43,  1.06it/s]Loading train:  40%|███▉      | 113/285 [01:48<02:46,  1.03it/s]Loading train:  40%|████      | 114/285 [01:49<02:45,  1.04it/s]Loading train:  40%|████      | 115/285 [01:50<02:47,  1.02it/s]Loading train:  41%|████      | 116/285 [01:51<02:40,  1.05it/s]Loading train:  41%|████      | 117/285 [01:51<02:33,  1.09it/s]Loading train:  41%|████▏     | 118/285 [01:52<02:34,  1.08it/s]Loading train:  42%|████▏     | 119/285 [01:53<02:40,  1.04it/s]Loading train:  42%|████▏     | 120/285 [01:55<02:44,  1.00it/s]Loading train:  42%|████▏     | 121/285 [01:56<02:57,  1.08s/it]Loading train:  43%|████▎     | 122/285 [01:57<02:59,  1.10s/it]Loading train:  43%|████▎     | 123/285 [01:58<03:06,  1.15s/it]Loading train:  44%|████▎     | 124/285 [01:59<02:59,  1.12s/it]Loading train:  44%|████▍     | 125/285 [02:00<02:46,  1.04s/it]Loading train:  44%|████▍     | 126/285 [02:01<02:37,  1.01it/s]Loading train:  45%|████▍     | 127/285 [02:02<02:34,  1.02it/s]Loading train:  45%|████▍     | 128/285 [02:03<02:25,  1.08it/s]Loading train:  45%|████▌     | 129/285 [02:04<02:23,  1.09it/s]Loading train:  46%|████▌     | 130/285 [02:04<02:17,  1.13it/s]Loading train:  46%|████▌     | 131/285 [02:05<02:15,  1.14it/s]Loading train:  46%|████▋     | 132/285 [02:06<02:15,  1.13it/s]Loading train:  47%|████▋     | 133/285 [02:07<02:11,  1.15it/s]Loading train:  47%|████▋     | 134/285 [02:08<02:11,  1.15it/s]Loading train:  47%|████▋     | 135/285 [02:09<02:12,  1.13it/s]Loading train:  48%|████▊     | 136/285 [02:10<02:20,  1.06it/s]Loading train:  48%|████▊     | 137/285 [02:11<02:10,  1.14it/s]Loading train:  48%|████▊     | 138/285 [02:11<01:59,  1.23it/s]Loading train:  49%|████▉     | 139/285 [02:12<02:05,  1.17it/s]Loading train:  49%|████▉     | 140/285 [02:13<02:03,  1.17it/s]Loading train:  49%|████▉     | 141/285 [02:14<02:07,  1.13it/s]Loading train:  50%|████▉     | 142/285 [02:15<02:08,  1.12it/s]Loading train:  50%|█████     | 143/285 [02:16<02:13,  1.07it/s]Loading train:  51%|█████     | 144/285 [02:17<02:10,  1.08it/s]Loading train:  51%|█████     | 145/285 [02:18<02:04,  1.12it/s]Loading train:  51%|█████     | 146/285 [02:19<02:04,  1.11it/s]Loading train:  52%|█████▏    | 147/285 [02:20<02:05,  1.10it/s]Loading train:  52%|█████▏    | 148/285 [02:21<02:14,  1.02it/s]Loading train:  52%|█████▏    | 149/285 [02:22<02:08,  1.06it/s]Loading train:  53%|█████▎    | 150/285 [02:23<02:07,  1.06it/s]Loading train:  53%|█████▎    | 151/285 [02:23<01:57,  1.15it/s]Loading train:  53%|█████▎    | 152/285 [02:24<01:58,  1.12it/s]Loading train:  54%|█████▎    | 153/285 [02:25<01:58,  1.11it/s]Loading train:  54%|█████▍    | 154/285 [02:26<01:50,  1.18it/s]Loading train:  54%|█████▍    | 155/285 [02:27<01:51,  1.17it/s]Loading train:  55%|█████▍    | 156/285 [02:28<01:53,  1.14it/s]Loading train:  55%|█████▌    | 157/285 [02:28<01:49,  1.17it/s]Loading train:  55%|█████▌    | 158/285 [02:29<01:47,  1.18it/s]Loading train:  56%|█████▌    | 159/285 [02:30<01:49,  1.15it/s]Loading train:  56%|█████▌    | 160/285 [02:31<01:54,  1.10it/s]Loading train:  56%|█████▋    | 161/285 [02:32<01:50,  1.12it/s]Loading train:  57%|█████▋    | 162/285 [02:33<01:50,  1.11it/s]Loading train:  57%|█████▋    | 163/285 [02:34<01:48,  1.13it/s]Loading train:  58%|█████▊    | 164/285 [02:35<01:43,  1.17it/s]Loading train:  58%|█████▊    | 165/285 [02:35<01:36,  1.24it/s]Loading train:  58%|█████▊    | 166/285 [02:36<01:34,  1.26it/s]Loading train:  59%|█████▊    | 167/285 [02:37<01:36,  1.22it/s]Loading train:  59%|█████▉    | 168/285 [02:38<01:36,  1.21it/s]Loading train:  59%|█████▉    | 169/285 [02:39<01:34,  1.23it/s]Loading train:  60%|█████▉    | 170/285 [02:39<01:34,  1.22it/s]Loading train:  60%|██████    | 171/285 [02:40<01:30,  1.25it/s]Loading train:  60%|██████    | 172/285 [02:41<01:37,  1.15it/s]Loading train:  61%|██████    | 173/285 [02:42<01:35,  1.17it/s]Loading train:  61%|██████    | 174/285 [02:43<01:31,  1.21it/s]Loading train:  61%|██████▏   | 175/285 [02:43<01:28,  1.24it/s]Loading train:  62%|██████▏   | 176/285 [02:44<01:30,  1.20it/s]Loading train:  62%|██████▏   | 177/285 [02:45<01:30,  1.19it/s]Loading train:  62%|██████▏   | 178/285 [02:46<01:29,  1.20it/s]Loading train:  63%|██████▎   | 179/285 [02:47<01:27,  1.21it/s]Loading train:  63%|██████▎   | 180/285 [02:48<01:26,  1.22it/s]Loading train:  64%|██████▎   | 181/285 [02:48<01:24,  1.24it/s]Loading train:  64%|██████▍   | 182/285 [02:49<01:21,  1.26it/s]Loading train:  64%|██████▍   | 183/285 [02:50<01:20,  1.27it/s]Loading train:  65%|██████▍   | 184/285 [02:51<01:20,  1.26it/s]Loading train:  65%|██████▍   | 185/285 [02:52<01:17,  1.29it/s]Loading train:  65%|██████▌   | 186/285 [02:52<01:14,  1.33it/s]Loading train:  66%|██████▌   | 187/285 [02:53<01:18,  1.25it/s]Loading train:  66%|██████▌   | 188/285 [02:54<01:21,  1.20it/s]Loading train:  66%|██████▋   | 189/285 [02:55<01:15,  1.26it/s]Loading train:  67%|██████▋   | 190/285 [02:56<01:19,  1.20it/s]Loading train:  67%|██████▋   | 191/285 [02:57<01:19,  1.18it/s]Loading train:  67%|██████▋   | 192/285 [02:57<01:18,  1.19it/s]Loading train:  68%|██████▊   | 193/285 [02:58<01:20,  1.14it/s]Loading train:  68%|██████▊   | 194/285 [02:59<01:17,  1.18it/s]Loading train:  68%|██████▊   | 195/285 [03:00<01:16,  1.18it/s]Loading train:  69%|██████▉   | 196/285 [03:01<01:20,  1.11it/s]Loading train:  69%|██████▉   | 197/285 [03:02<01:21,  1.08it/s]Loading train:  69%|██████▉   | 198/285 [03:03<01:18,  1.11it/s]Loading train:  70%|██████▉   | 199/285 [03:04<01:20,  1.07it/s]Loading train:  70%|███████   | 200/285 [03:05<01:17,  1.10it/s]Loading train:  71%|███████   | 201/285 [03:05<01:13,  1.14it/s]Loading train:  71%|███████   | 202/285 [03:06<01:12,  1.14it/s]Loading train:  71%|███████   | 203/285 [03:07<01:11,  1.15it/s]Loading train:  72%|███████▏  | 204/285 [03:08<01:12,  1.12it/s]Loading train:  72%|███████▏  | 205/285 [03:09<01:11,  1.12it/s]Loading train:  72%|███████▏  | 206/285 [03:10<01:15,  1.04it/s]Loading train:  73%|███████▎  | 207/285 [03:11<01:13,  1.07it/s]Loading train:  73%|███████▎  | 208/285 [03:12<01:16,  1.01it/s]Loading train:  73%|███████▎  | 209/285 [03:13<01:20,  1.06s/it]Loading train:  74%|███████▎  | 210/285 [03:15<01:23,  1.11s/it]Loading train:  74%|███████▍  | 211/285 [03:16<01:23,  1.13s/it]Loading train:  74%|███████▍  | 212/285 [03:17<01:26,  1.19s/it]Loading train:  75%|███████▍  | 213/285 [03:19<01:30,  1.25s/it]Loading train:  75%|███████▌  | 214/285 [03:20<01:23,  1.18s/it]Loading train:  75%|███████▌  | 215/285 [03:21<01:20,  1.15s/it]Loading train:  76%|███████▌  | 216/285 [03:22<01:18,  1.14s/it]Loading train:  76%|███████▌  | 217/285 [03:23<01:13,  1.08s/it]Loading train:  76%|███████▋  | 218/285 [03:24<01:15,  1.13s/it]Loading train:  77%|███████▋  | 219/285 [03:25<01:08,  1.04s/it]Loading train:  77%|███████▋  | 220/285 [03:26<01:01,  1.05it/s]Loading train:  78%|███████▊  | 221/285 [03:26<00:58,  1.09it/s]Loading train:  78%|███████▊  | 222/285 [03:27<00:58,  1.08it/s]Loading train:  78%|███████▊  | 223/285 [03:28<00:55,  1.12it/s]Loading train:  79%|███████▊  | 224/285 [03:29<00:50,  1.22it/s]Loading train:  79%|███████▉  | 225/285 [03:29<00:46,  1.29it/s]Loading train:  79%|███████▉  | 226/285 [03:30<00:47,  1.26it/s]Loading train:  80%|███████▉  | 227/285 [03:31<00:46,  1.25it/s]Loading train:  80%|████████  | 228/285 [03:32<00:45,  1.26it/s]Loading train:  80%|████████  | 229/285 [03:33<00:44,  1.26it/s]Loading train:  81%|████████  | 230/285 [03:34<00:45,  1.21it/s]Loading train:  81%|████████  | 231/285 [03:34<00:42,  1.28it/s]Loading train:  81%|████████▏ | 232/285 [03:35<00:46,  1.13it/s]Loading train:  82%|████████▏ | 233/285 [03:36<00:48,  1.07it/s]Loading train:  82%|████████▏ | 234/285 [03:38<00:51,  1.02s/it]Loading train:  82%|████████▏ | 235/285 [03:39<00:54,  1.08s/it]Loading train:  83%|████████▎ | 236/285 [03:40<00:53,  1.09s/it]Loading train:  83%|████████▎ | 237/285 [03:41<00:50,  1.05s/it]Loading train:  84%|████████▎ | 238/285 [03:42<00:48,  1.03s/it]Loading train:  84%|████████▍ | 239/285 [03:43<00:46,  1.02s/it]Loading train:  84%|████████▍ | 240/285 [03:44<00:45,  1.02s/it]Loading train:  85%|████████▍ | 241/285 [03:45<00:43,  1.00it/s]Loading train:  85%|████████▍ | 242/285 [03:46<00:41,  1.05it/s]Loading train:  85%|████████▌ | 243/285 [03:47<00:41,  1.01it/s]Loading train:  86%|████████▌ | 244/285 [03:48<00:39,  1.03it/s]Loading train:  86%|████████▌ | 245/285 [03:49<00:37,  1.06it/s]Loading train:  86%|████████▋ | 246/285 [03:50<00:37,  1.04it/s]Loading train:  87%|████████▋ | 247/285 [03:51<00:36,  1.05it/s]Loading train:  87%|████████▋ | 248/285 [03:52<00:36,  1.02it/s]Loading train:  87%|████████▋ | 249/285 [03:53<00:34,  1.03it/s]Loading train:  88%|████████▊ | 250/285 [03:53<00:33,  1.06it/s]Loading train:  88%|████████▊ | 251/285 [03:54<00:30,  1.12it/s]Loading train:  88%|████████▊ | 252/285 [03:55<00:28,  1.16it/s]Loading train:  89%|████████▉ | 253/285 [03:56<00:25,  1.23it/s]Loading train:  89%|████████▉ | 254/285 [03:56<00:24,  1.27it/s]Loading train:  89%|████████▉ | 255/285 [03:57<00:23,  1.30it/s]Loading train:  90%|████████▉ | 256/285 [03:58<00:22,  1.29it/s]Loading train:  90%|█████████ | 257/285 [03:59<00:21,  1.33it/s]Loading train:  91%|█████████ | 258/285 [03:59<00:19,  1.38it/s]Loading train:  91%|█████████ | 259/285 [04:00<00:18,  1.39it/s]Loading train:  91%|█████████ | 260/285 [04:01<00:18,  1.37it/s]Loading train:  92%|█████████▏| 261/285 [04:02<00:17,  1.34it/s]Loading train:  92%|█████████▏| 262/285 [04:02<00:17,  1.34it/s]Loading train:  92%|█████████▏| 263/285 [04:03<00:16,  1.36it/s]Loading train:  93%|█████████▎| 264/285 [04:04<00:15,  1.38it/s]Loading train:  93%|█████████▎| 265/285 [04:04<00:14,  1.37it/s]Loading train:  93%|█████████▎| 266/285 [04:05<00:14,  1.33it/s]Loading train:  94%|█████████▎| 267/285 [04:06<00:13,  1.38it/s]Loading train:  94%|█████████▍| 268/285 [04:07<00:13,  1.23it/s]Loading train:  94%|█████████▍| 269/285 [04:08<00:13,  1.17it/s]Loading train:  95%|█████████▍| 270/285 [04:09<00:13,  1.14it/s]Loading train:  95%|█████████▌| 271/285 [04:10<00:12,  1.13it/s]Loading train:  95%|█████████▌| 272/285 [04:11<00:11,  1.12it/s]Loading train:  96%|█████████▌| 273/285 [04:12<00:10,  1.11it/s]Loading train:  96%|█████████▌| 274/285 [04:12<00:10,  1.10it/s]Loading train:  96%|█████████▋| 275/285 [04:13<00:09,  1.08it/s]Loading train:  97%|█████████▋| 276/285 [04:14<00:08,  1.09it/s]Loading train:  97%|█████████▋| 277/285 [04:15<00:07,  1.04it/s]Loading train:  98%|█████████▊| 278/285 [04:16<00:06,  1.05it/s]Loading train:  98%|█████████▊| 279/285 [04:17<00:05,  1.07it/s]Loading train:  98%|█████████▊| 280/285 [04:18<00:04,  1.04it/s]Loading train:  99%|█████████▊| 281/285 [04:19<00:03,  1.04it/s]Loading train:  99%|█████████▉| 282/285 [04:20<00:02,  1.03it/s]Loading train:  99%|█████████▉| 283/285 [04:21<00:01,  1.02it/s]Loading train: 100%|█████████▉| 284/285 [04:22<00:01,  1.00s/it]Loading train: 100%|██████████| 285/285 [04:23<00:00,  1.00it/s]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 3/285 [00:00<00:10, 28.13it/s]concatenating: train:   4%|▎         | 10/285 [00:00<00:08, 34.06it/s]concatenating: train:   6%|▌         | 17/285 [00:00<00:06, 39.45it/s]concatenating: train:  15%|█▌        | 44/285 [00:00<00:04, 52.95it/s]concatenating: train:  27%|██▋       | 78/285 [00:00<00:02, 70.85it/s]concatenating: train:  37%|███▋      | 106/285 [00:00<00:01, 91.26it/s]concatenating: train:  48%|████▊     | 138/285 [00:00<00:01, 115.93it/s]concatenating: train:  60%|██████    | 172/285 [00:00<00:00, 144.02it/s]concatenating: train:  73%|███████▎  | 207/285 [00:00<00:00, 174.76it/s]concatenating: train:  84%|████████▍ | 240/285 [00:01<00:00, 202.90it/s]concatenating: train:  97%|█████████▋| 277/285 [00:01<00:00, 234.59it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 247.15it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.35s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.33s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 62.48it/s]2019-07-05 20:30:38.868257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 20:30:38.868368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 20:30:38.868385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 20:30:38.868395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 20:30:38.868795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:13,  2.92it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:10,  3.49it/s]loading the weights for Unet:  10%|█         | 4/40 [00:01<00:10,  3.28it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:07,  4.24it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:07,  3.96it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:01<00:06,  4.57it/s]loading the weights for Unet:  30%|███       | 12/40 [00:02<00:06,  4.19it/s]loading the weights for Unet:  40%|████      | 16/40 [00:02<00:04,  5.21it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:02<00:05,  3.87it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:03<00:04,  4.27it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:03<00:05,  3.68it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:03<00:03,  4.51it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:04<00:03,  4.83it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:04<00:03,  3.97it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:04<00:02,  4.54it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:05<00:02,  3.99it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:05<00:01,  4.85it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:05<00:01,  5.26it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:06<00:01,  4.41it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:06<00:00,  4.89it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:06<00:00,  3.95it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:06<00:00,  5.84it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12, 12, 80), (None, 13, 13, 80)]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 40)   400         input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 40)   160         conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 40)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 40)   0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 40)   14440       dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 40)   160         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 40)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 40)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 40)   14440       dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 40)   160         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 40)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 52, 80, 40)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 20)   7220        dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 52, 80, 20)   80          conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 52, 80, 20)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 52, 80, 20)   3620        activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 52, 80, 20)   80          conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 52, 80, 20)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_16[0][0]              
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 26, 40, 20)   0           max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 26, 40, 40)   7240        dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 26, 40, 40)   160         conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 26, 40, 40)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 26, 40, 40)   14440       activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 26, 40, 40)   160         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 26, 40, 40)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_18[0][0]              
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 13, 20, 40)   0           max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 13, 20, 80)   28880       dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 13, 20, 80)   320         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 13, 20, 80)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 13, 20, 80)   57680       activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 13, 20, 80)   320         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 13, 20, 80)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 13, 20, 80)   0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_13[0][0]                 
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 26, 40, 40)   160         conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 26, 40, 40)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 26, 40, 40)   14440       activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 26, 40, 40)   160         conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 26, 40, 40)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 26, 40, 40)   0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_14[0][0]                 
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_3[0][0]         
                                                                 activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 52, 80, 20)   80          conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 52, 80, 20)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 52, 80, 20)   3620        activation_23[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 52, 80, 20)   80          conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 52, 80, 20)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 52, 80, 20)   0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 52, 80, 13)   273         dropout_15[0][0]                 
==================================================================================================
Total params: 220,893
Trainable params: 77,173
Non-trainable params: 143,720
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 20s - loss: 172.4512 - acc: 0.7062 - mDice: 0.0151 - val_loss: 42.0949 - val_acc: 0.9001 - val_mDice: 0.0126

Epoch 00001: val_mDice improved from -inf to 0.01259, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 12s - loss: 39.8473 - acc: 0.8517 - mDice: 0.0139 - val_loss: 14.7424 - val_acc: 0.9046 - val_mDice: 0.0110

Epoch 00002: val_mDice did not improve from 0.01259
Epoch 3/300
 - 12s - loss: 19.6725 - acc: 0.8659 - mDice: 0.0174 - val_loss: 9.1624 - val_acc: 0.9047 - val_mDice: 0.0144

Epoch 00003: val_mDice improved from 0.01259 to 0.01437, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 12s - loss: 13.5550 - acc: 0.8675 - mDice: 0.0263 - val_loss: 7.2898 - val_acc: 0.9047 - val_mDice: 0.0156

Epoch 00004: val_mDice improved from 0.01437 to 0.01555, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 5/300
 - 12s - loss: 10.7027 - acc: 0.8677 - mDice: 0.0353 - val_loss: 6.1758 - val_acc: 0.9047 - val_mDice: 0.0336

Epoch 00005: val_mDice improved from 0.01555 to 0.03358, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 6/300
 - 12s - loss: 8.8915 - acc: 0.8679 - mDice: 0.0478 - val_loss: 5.4845 - val_acc: 0.9047 - val_mDice: 0.0489

Epoch 00006: val_mDice improved from 0.03358 to 0.04893, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 7/300
 - 12s - loss: 7.6519 - acc: 0.8684 - mDice: 0.0665 - val_loss: 5.1229 - val_acc: 0.9047 - val_mDice: 0.0662

Epoch 00007: val_mDice improved from 0.04893 to 0.06623, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 8/300
 - 12s - loss: 6.7676 - acc: 0.8697 - mDice: 0.0861 - val_loss: 5.2818 - val_acc: 0.9047 - val_mDice: 0.0647

Epoch 00008: val_mDice did not improve from 0.06623
Epoch 9/300
 - 12s - loss: 6.1109 - acc: 0.8713 - mDice: 0.1044 - val_loss: 5.4202 - val_acc: 0.9049 - val_mDice: 0.0720

Epoch 00009: val_mDice improved from 0.06623 to 0.07204, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 12s - loss: 5.6119 - acc: 0.8733 - mDice: 0.1226 - val_loss: 4.4481 - val_acc: 0.9057 - val_mDice: 0.1194

Epoch 00010: val_mDice improved from 0.07204 to 0.11945, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 12s - loss: 5.1895 - acc: 0.8752 - mDice: 0.1424 - val_loss: 4.7522 - val_acc: 0.9051 - val_mDice: 0.1129

Epoch 00011: val_mDice did not improve from 0.11945
Epoch 12/300
 - 12s - loss: 4.8546 - acc: 0.8766 - mDice: 0.1599 - val_loss: 5.7129 - val_acc: 0.9050 - val_mDice: 0.0947

Epoch 00012: val_mDice did not improve from 0.11945
Epoch 13/300
 - 12s - loss: 4.5748 - acc: 0.8778 - mDice: 0.1764 - val_loss: 5.6901 - val_acc: 0.9050 - val_mDice: 0.0960

Epoch 00013: val_mDice did not improve from 0.11945
Epoch 14/300
 - 12s - loss: 4.3393 - acc: 0.8790 - mDice: 0.1915 - val_loss: 3.9426 - val_acc: 0.9092 - val_mDice: 0.1912

Epoch 00014: val_mDice improved from 0.11945 to 0.19117, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 15/300
 - 12s - loss: 4.1240 - acc: 0.8803 - mDice: 0.2073 - val_loss: 3.6191 - val_acc: 0.9110 - val_mDice: 0.2204

Epoch 00015: val_mDice improved from 0.19117 to 0.22043, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 16/300
 - 12s - loss: 3.9424 - acc: 0.8817 - mDice: 0.2219 - val_loss: 3.4398 - val_acc: 0.9141 - val_mDice: 0.2545

Epoch 00016: val_mDice improved from 0.22043 to 0.25449, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 13s - loss: 3.7820 - acc: 0.8837 - mDice: 0.2369 - val_loss: 3.3057 - val_acc: 0.9148 - val_mDice: 0.2683

Epoch 00017: val_mDice improved from 0.25449 to 0.26831, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 12s - loss: 3.6254 - acc: 0.8854 - mDice: 0.2512 - val_loss: 3.6128 - val_acc: 0.9139 - val_mDice: 0.2581

Epoch 00018: val_mDice did not improve from 0.26831
Epoch 19/300
 - 12s - loss: 3.4873 - acc: 0.8873 - mDice: 0.2647 - val_loss: 3.8779 - val_acc: 0.9138 - val_mDice: 0.2552

Epoch 00019: val_mDice did not improve from 0.26831
Epoch 20/300
 - 12s - loss: 3.3799 - acc: 0.8893 - mDice: 0.2759 - val_loss: 3.3053 - val_acc: 0.9207 - val_mDice: 0.2956

Epoch 00020: val_mDice improved from 0.26831 to 0.29564, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 21/300
 - 12s - loss: 3.2643 - acc: 0.8912 - mDice: 0.2882 - val_loss: 3.3090 - val_acc: 0.9203 - val_mDice: 0.3002

Epoch 00021: val_mDice improved from 0.29564 to 0.30024, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 12s - loss: 3.1844 - acc: 0.8928 - mDice: 0.2966 - val_loss: 3.3876 - val_acc: 0.9196 - val_mDice: 0.3073

Epoch 00022: val_mDice improved from 0.30024 to 0.30734, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 12s - loss: 3.1052 - acc: 0.8944 - mDice: 0.3061 - val_loss: 3.0722 - val_acc: 0.9229 - val_mDice: 0.3224

Epoch 00023: val_mDice improved from 0.30734 to 0.32240, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 12s - loss: 3.0210 - acc: 0.8961 - mDice: 0.3153 - val_loss: 3.0641 - val_acc: 0.9257 - val_mDice: 0.3319

Epoch 00024: val_mDice improved from 0.32240 to 0.33190, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 13s - loss: 2.9605 - acc: 0.8974 - mDice: 0.3229 - val_loss: 3.1475 - val_acc: 0.9235 - val_mDice: 0.3211

Epoch 00025: val_mDice did not improve from 0.33190
Epoch 26/300
 - 12s - loss: 2.8966 - acc: 0.8986 - mDice: 0.3315 - val_loss: 3.4382 - val_acc: 0.9227 - val_mDice: 0.3143

Epoch 00026: val_mDice did not improve from 0.33190
Epoch 27/300
 - 12s - loss: 2.8446 - acc: 0.8992 - mDice: 0.3381 - val_loss: 3.4089 - val_acc: 0.9228 - val_mDice: 0.3233

Epoch 00027: val_mDice did not improve from 0.33190
Epoch 28/300
 - 12s - loss: 2.7826 - acc: 0.9001 - mDice: 0.3485 - val_loss: 3.4356 - val_acc: 0.9239 - val_mDice: 0.3375

Epoch 00028: val_mDice improved from 0.33190 to 0.33750, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 29/300
 - 12s - loss: 2.7326 - acc: 0.9008 - mDice: 0.3563 - val_loss: 3.1193 - val_acc: 0.9267 - val_mDice: 0.3534

Epoch 00029: val_mDice improved from 0.33750 to 0.35335, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 12s - loss: 2.6699 - acc: 0.9020 - mDice: 0.3647 - val_loss: 3.4301 - val_acc: 0.9240 - val_mDice: 0.3413

Epoch 00030: val_mDice did not improve from 0.35335
Epoch 31/300
 - 12s - loss: 2.6398 - acc: 0.9025 - mDice: 0.3695 - val_loss: 3.1057 - val_acc: 0.9268 - val_mDice: 0.3602

Epoch 00031: val_mDice improved from 0.35335 to 0.36019, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 12s - loss: 2.5930 - acc: 0.9036 - mDice: 0.3765 - val_loss: 2.9994 - val_acc: 0.9270 - val_mDice: 0.3692

Epoch 00032: val_mDice improved from 0.36019 to 0.36921, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 33/300
 - 12s - loss: 2.5493 - acc: 0.9045 - mDice: 0.3840 - val_loss: 3.4632 - val_acc: 0.9274 - val_mDice: 0.3418

Epoch 00033: val_mDice did not improve from 0.36921
Epoch 34/300
 - 13s - loss: 2.5195 - acc: 0.9054 - mDice: 0.3896 - val_loss: 2.9093 - val_acc: 0.9289 - val_mDice: 0.3831

Epoch 00034: val_mDice improved from 0.36921 to 0.38306, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 12s - loss: 2.4808 - acc: 0.9061 - mDice: 0.3957 - val_loss: 2.9995 - val_acc: 0.9282 - val_mDice: 0.3734

Epoch 00035: val_mDice did not improve from 0.38306
Epoch 36/300
 - 12s - loss: 2.4415 - acc: 0.9071 - mDice: 0.4026 - val_loss: 3.2762 - val_acc: 0.9252 - val_mDice: 0.3631

Epoch 00036: val_mDice did not improve from 0.38306
Epoch 37/300
 - 12s - loss: 2.4192 - acc: 0.9075 - mDice: 0.4060 - val_loss: 2.9786 - val_acc: 0.9308 - val_mDice: 0.3900

Epoch 00037: val_mDice improved from 0.38306 to 0.39004, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 38/300
 - 12s - loss: 2.3862 - acc: 0.9086 - mDice: 0.4129 - val_loss: 3.0190 - val_acc: 0.9303 - val_mDice: 0.3833

Epoch 00038: val_mDice did not improve from 0.39004
Epoch 39/300
 - 12s - loss: 2.3515 - acc: 0.9096 - mDice: 0.4188 - val_loss: 3.0288 - val_acc: 0.9288 - val_mDice: 0.3883

Epoch 00039: val_mDice did not improve from 0.39004
Epoch 40/300
 - 12s - loss: 2.3300 - acc: 0.9100 - mDice: 0.4224 - val_loss: 3.1310 - val_acc: 0.9302 - val_mDice: 0.3813

Epoch 00040: val_mDice did not improve from 0.39004
Epoch 41/300
 - 12s - loss: 2.3006 - acc: 0.9104 - mDice: 0.4280 - val_loss: 2.9319 - val_acc: 0.9306 - val_mDice: 0.4035

Epoch 00041: val_mDice improved from 0.39004 to 0.40349, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 42/300
 - 12s - loss: 2.2836 - acc: 0.9111 - mDice: 0.4321 - val_loss: 3.2095 - val_acc: 0.9298 - val_mDice: 0.3858

Epoch 00042: val_mDice did not improve from 0.40349
Epoch 43/300
 - 12s - loss: 2.2619 - acc: 0.9116 - mDice: 0.4359 - val_loss: 3.2040 - val_acc: 0.9276 - val_mDice: 0.3881

Epoch 00043: val_mDice did not improve from 0.40349
Epoch 44/300
 - 12s - loss: 2.2329 - acc: 0.9124 - mDice: 0.4416 - val_loss: 3.1436 - val_acc: 0.9316 - val_mDice: 0.4017

Epoch 00044: val_mDice did not improve from 0.40349
Epoch 45/300
 - 12s - loss: 2.2125 - acc: 0.9130 - mDice: 0.4450 - val_loss: 3.1633 - val_acc: 0.9302 - val_mDice: 0.3987

Epoch 00045: val_mDice did not improve from 0.40349
Epoch 46/300
 - 12s - loss: 2.2013 - acc: 0.9133 - mDice: 0.4477 - val_loss: 3.2245 - val_acc: 0.9303 - val_mDice: 0.4002

Epoch 00046: val_mDice did not improve from 0.40349
Epoch 47/300
 - 12s - loss: 2.1752 - acc: 0.9139 - mDice: 0.4524 - val_loss: 2.9908 - val_acc: 0.9315 - val_mDice: 0.4127

Epoch 00047: val_mDice improved from 0.40349 to 0.41273, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 12s - loss: 2.1548 - acc: 0.9146 - mDice: 0.4572 - val_loss: 3.1990 - val_acc: 0.9322 - val_mDice: 0.4045

Epoch 00048: val_mDice did not improve from 0.41273
Epoch 49/300
 - 12s - loss: 2.1391 - acc: 0.9148 - mDice: 0.4609 - val_loss: 3.2862 - val_acc: 0.9304 - val_mDice: 0.3991

Epoch 00049: val_mDice did not improve from 0.41273
Epoch 50/300
 - 13s - loss: 2.1216 - acc: 0.9153 - mDice: 0.4644 - val_loss: 3.1048 - val_acc: 0.9319 - val_mDice: 0.4096

Epoch 00050: val_mDice did not improve from 0.41273
Epoch 51/300
 - 12s - loss: 2.1069 - acc: 0.9158 - mDice: 0.4669 - val_loss: 3.1109 - val_acc: 0.9327 - val_mDice: 0.4185

Epoch 00051: val_mDice improved from 0.41273 to 0.41854, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 52/300
 - 12s - loss: 2.0954 - acc: 0.9160 - mDice: 0.4699 - val_loss: 3.1849 - val_acc: 0.9324 - val_mDice: 0.4152

Epoch 00052: val_mDice did not improve from 0.41854
Epoch 53/300
 - 12s - loss: 2.0769 - acc: 0.9166 - mDice: 0.4746 - val_loss: 3.2681 - val_acc: 0.9318 - val_mDice: 0.4062

Epoch 00053: val_mDice did not improve from 0.41854
Epoch 54/300
 - 12s - loss: 2.0640 - acc: 0.9169 - mDice: 0.4769 - val_loss: 3.1487 - val_acc: 0.9310 - val_mDice: 0.4073

Epoch 00054: val_mDice did not improve from 0.41854
Epoch 55/300
 - 12s - loss: 2.0529 - acc: 0.9170 - mDice: 0.4791 - val_loss: 3.2824 - val_acc: 0.9321 - val_mDice: 0.4090

Epoch 00055: val_mDice did not improve from 0.41854
Epoch 56/300
 - 12s - loss: 2.0342 - acc: 0.9176 - mDice: 0.4823 - val_loss: 3.3751 - val_acc: 0.9335 - val_mDice: 0.4141

Epoch 00056: val_mDice did not improve from 0.41854
Epoch 57/300
 - 12s - loss: 2.0167 - acc: 0.9182 - mDice: 0.4866 - val_loss: 3.2302 - val_acc: 0.9331 - val_mDice: 0.4201

Epoch 00057: val_mDice improved from 0.41854 to 0.42014, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 58/300
 - 12s - loss: 1.9985 - acc: 0.9185 - mDice: 0.4894 - val_loss: 3.2815 - val_acc: 0.9342 - val_mDice: 0.4174

Epoch 00058: val_mDice did not improve from 0.42014
Epoch 59/300
 - 13s - loss: 1.9882 - acc: 0.9188 - mDice: 0.4931 - val_loss: 3.4233 - val_acc: 0.9308 - val_mDice: 0.4137

Epoch 00059: val_mDice did not improve from 0.42014
Epoch 60/300
 - 12s - loss: 1.9769 - acc: 0.9192 - mDice: 0.4950 - val_loss: 3.3066 - val_acc: 0.9346 - val_mDice: 0.4153

Epoch 00060: val_mDice did not improve from 0.42014
Epoch 61/300
 - 12s - loss: 1.9643 - acc: 0.9194 - mDice: 0.4983 - val_loss: 3.1143 - val_acc: 0.9344 - val_mDice: 0.4304

Epoch 00061: val_mDice improved from 0.42014 to 0.43040, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 62/300
 - 12s - loss: 1.9538 - acc: 0.9197 - mDice: 0.5005 - val_loss: 3.2433 - val_acc: 0.9339 - val_mDice: 0.4193

Epoch 00062: val_mDice did not improve from 0.43040
Epoch 63/300
 - 12s - loss: 1.9378 - acc: 0.9200 - mDice: 0.5046 - val_loss: 3.2389 - val_acc: 0.9358 - val_mDice: 0.4296

Epoch 00063: val_mDice did not improve from 0.43040
Epoch 64/300
 - 12s - loss: 1.9252 - acc: 0.9205 - mDice: 0.5070 - val_loss: 3.2716 - val_acc: 0.9355 - val_mDice: 0.4304

Epoch 00064: val_mDice did not improve from 0.43040
Epoch 65/300
 - 12s - loss: 1.9207 - acc: 0.9206 - mDice: 0.5090 - val_loss: 3.2130 - val_acc: 0.9359 - val_mDice: 0.4315

Epoch 00065: val_mDice improved from 0.43040 to 0.43146, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 66/300
 - 12s - loss: 1.9089 - acc: 0.9207 - mDice: 0.5110 - val_loss: 3.3232 - val_acc: 0.9333 - val_mDice: 0.4297

Epoch 00066: val_mDice did not improve from 0.43146
Epoch 67/300
 - 13s - loss: 1.8969 - acc: 0.9210 - mDice: 0.5140 - val_loss: 3.2812 - val_acc: 0.9355 - val_mDice: 0.4305

Epoch 00067: val_mDice did not improve from 0.43146
Epoch 68/300
 - 13s - loss: 1.8784 - acc: 0.9214 - mDice: 0.5184 - val_loss: 3.2825 - val_acc: 0.9346 - val_mDice: 0.4221

Epoch 00068: val_mDice did not improve from 0.43146
Epoch 69/300
 - 13s - loss: 1.8841 - acc: 0.9214 - mDice: 0.5168 - val_loss: 3.3472 - val_acc: 0.9321 - val_mDice: 0.4262

Epoch 00069: val_mDice did not improve from 0.43146
Epoch 70/300
 - 13s - loss: 1.8638 - acc: 0.9221 - mDice: 0.5211 - val_loss: 3.4157 - val_acc: 0.9362 - val_mDice: 0.4345

Epoch 00070: val_mDice improved from 0.43146 to 0.43452, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 71/300
 - 13s - loss: 1.8572 - acc: 0.9220 - mDice: 0.5236 - val_loss: 3.3038 - val_acc: 0.9365 - val_mDice: 0.4384

Epoch 00071: val_mDice improved from 0.43452 to 0.43843, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 72/300
 - 13s - loss: 1.8513 - acc: 0.9221 - mDice: 0.5248 - val_loss: 3.3237 - val_acc: 0.9367 - val_mDice: 0.4427

Epoch 00072: val_mDice improved from 0.43843 to 0.44273, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 73/300
 - 12s - loss: 1.8320 - acc: 0.9228 - mDice: 0.5290 - val_loss: 3.4421 - val_acc: 0.9355 - val_mDice: 0.4328

Epoch 00073: val_mDice did not improve from 0.44273
Epoch 74/300
 - 12s - loss: 1.8298 - acc: 0.9229 - mDice: 0.5296 - val_loss: 3.5776 - val_acc: 0.9348 - val_mDice: 0.4219

Epoch 00074: val_mDice did not improve from 0.44273
Epoch 75/300
 - 13s - loss: 1.8238 - acc: 0.9230 - mDice: 0.5316 - val_loss: 3.1674 - val_acc: 0.9381 - val_mDice: 0.4506

Epoch 00075: val_mDice improved from 0.44273 to 0.45059, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 76/300
 - 13s - loss: 1.8049 - acc: 0.9235 - mDice: 0.5357 - val_loss: 3.1856 - val_acc: 0.9389 - val_mDice: 0.4470

Epoch 00076: val_mDice did not improve from 0.45059
Epoch 77/300
 - 13s - loss: 1.8003 - acc: 0.9238 - mDice: 0.5369 - val_loss: 3.1978 - val_acc: 0.9342 - val_mDice: 0.4444

Epoch 00077: val_mDice did not improve from 0.45059
Epoch 78/300
 - 12s - loss: 1.8020 - acc: 0.9236 - mDice: 0.5362 - val_loss: 3.3508 - val_acc: 0.9349 - val_mDice: 0.4369

Epoch 00078: val_mDice did not improve from 0.45059
Epoch 79/300
 - 13s - loss: 1.7880 - acc: 0.9242 - mDice: 0.5402 - val_loss: 3.1451 - val_acc: 0.9363 - val_mDice: 0.4493

Epoch 00079: val_mDice did not improve from 0.45059
Epoch 80/300
 - 13s - loss: 1.7765 - acc: 0.9243 - mDice: 0.5421 - val_loss: 3.2382 - val_acc: 0.9362 - val_mDice: 0.4400

Epoch 00080: val_mDice did not improve from 0.45059
Epoch 81/300
 - 13s - loss: 1.7711 - acc: 0.9246 - mDice: 0.5439 - val_loss: 3.3084 - val_acc: 0.9349 - val_mDice: 0.4411

Epoch 00081: val_mDice did not improve from 0.45059
Epoch 82/300
 - 13s - loss: 1.7757 - acc: 0.9244 - mDice: 0.5433 - val_loss: 3.3098 - val_acc: 0.9377 - val_mDice: 0.4478

Epoch 00082: val_mDice did not improve from 0.45059
Epoch 83/300
 - 13s - loss: 1.7695 - acc: 0.9248 - mDice: 0.5443 - val_loss: 3.5682 - val_acc: 0.9369 - val_mDice: 0.4270

Epoch 00083: val_mDice did not improve from 0.45059
Epoch 84/300
 - 13s - loss: 1.7522 - acc: 0.9252 - mDice: 0.5473 - val_loss: 3.4136 - val_acc: 0.9377 - val_mDice: 0.4390

Epoch 00084: val_mDice did not improve from 0.45059
Epoch 85/300
 - 13s - loss: 1.7546 - acc: 0.9251 - mDice: 0.5472 - val_loss: 3.4859 - val_acc: 0.9369 - val_mDice: 0.4356

Epoch 00085: val_mDice did not improve from 0.45059
Epoch 86/300
 - 13s - loss: 1.7388 - acc: 0.9257 - mDice: 0.5513 - val_loss: 3.3890 - val_acc: 0.9373 - val_mDice: 0.4387

Epoch 00086: val_mDice did not improve from 0.45059
Epoch 87/300
 - 13s - loss: 1.7330 - acc: 0.9260 - mDice: 0.5527 - val_loss: 3.3919 - val_acc: 0.9381 - val_mDice: 0.4431

Epoch 00087: val_mDice did not improve from 0.45059
Epoch 88/300
 - 13s - loss: 1.7304 - acc: 0.9260 - mDice: 0.5532 - val_loss: 3.5617 - val_acc: 0.9371 - val_mDice: 0.4316

Epoch 00088: val_mDice did not improve from 0.45059
Epoch 89/300
 - 13s - loss: 1.7287 - acc: 0.9260 - mDice: 0.5530 - val_loss: 3.5452 - val_acc: 0.9386 - val_mDice: 0.4452

Epoch 00089: val_mDice did not improve from 0.45059
Epoch 90/300
 - 13s - loss: 1.7105 - acc: 0.9265 - mDice: 0.5575 - val_loss: 3.3055 - val_acc: 0.9362 - val_mDice: 0.4378

Epoch 00090: val_mDice did not improve from 0.45059
Epoch 91/300
 - 13s - loss: 1.7107 - acc: 0.9267 - mDice: 0.5574 - val_loss: 3.5549 - val_acc: 0.9375 - val_mDice: 0.4404

Epoch 00091: val_mDice did not improve from 0.45059
Epoch 92/300
 - 13s - loss: 1.7092 - acc: 0.9267 - mDice: 0.5583 - val_loss: 3.6142 - val_acc: 0.9365 - val_mDice: 0.4361

Epoch 00092: val_mDice did not improve from 0.45059
Epoch 93/300
 - 13s - loss: 1.7036 - acc: 0.9267 - mDice: 0.5590 - val_loss: 3.2833 - val_acc: 0.9396 - val_mDice: 0.4534

Epoch 00093: val_mDice improved from 0.45059 to 0.45340, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 94/300
 - 13s - loss: 1.6970 - acc: 0.9270 - mDice: 0.5606 - val_loss: 3.6089 - val_acc: 0.9379 - val_mDice: 0.4403

Epoch 00094: val_mDice did not improve from 0.45340
Epoch 95/300
 - 13s - loss: 1.6852 - acc: 0.9276 - mDice: 0.5634 - val_loss: 3.5929 - val_acc: 0.9378 - val_mDice: 0.4368

Epoch 00095: val_mDice did not improve from 0.45340
Epoch 96/300
 - 13s - loss: 1.6859 - acc: 0.9277 - mDice: 0.5632 - val_loss: 3.5135 - val_acc: 0.9369 - val_mDice: 0.4395

Epoch 00096: val_mDice did not improve from 0.45340
Epoch 97/300
 - 13s - loss: 1.6747 - acc: 0.9279 - mDice: 0.5654 - val_loss: 3.3319 - val_acc: 0.9370 - val_mDice: 0.4516

Epoch 00097: val_mDice did not improve from 0.45340
Epoch 98/300
 - 13s - loss: 1.6718 - acc: 0.9281 - mDice: 0.5668 - val_loss: 3.4658 - val_acc: 0.9395 - val_mDice: 0.4479

Epoch 00098: val_mDice did not improve from 0.45340
Epoch 99/300
 - 13s - loss: 1.6701 - acc: 0.9281 - mDice: 0.5666 - val_loss: 3.5333 - val_acc: 0.9382 - val_mDice: 0.4436

Epoch 00099: val_mDice did not improve from 0.45340
Epoch 100/300
 - 13s - loss: 1.6720 - acc: 0.9283 - mDice: 0.5663 - val_loss: 3.6529 - val_acc: 0.9380 - val_mDice: 0.4383

Epoch 00100: val_mDice did not improve from 0.45340
Epoch 101/300
 - 13s - loss: 1.6605 - acc: 0.9286 - mDice: 0.5689 - val_loss: 3.4088 - val_acc: 0.9390 - val_mDice: 0.4526

Epoch 00101: val_mDice did not improve from 0.45340
Epoch 102/300
 - 13s - loss: 1.6568 - acc: 0.9287 - mDice: 0.5700 - val_loss: 3.6939 - val_acc: 0.9407 - val_mDice: 0.4450

Epoch 00102: val_mDice did not improve from 0.45340
Epoch 103/300
 - 13s - loss: 1.6571 - acc: 0.9290 - mDice: 0.5704 - val_loss: 3.3332 - val_acc: 0.9401 - val_mDice: 0.4488

Epoch 00103: val_mDice did not improve from 0.45340
Epoch 104/300
 - 13s - loss: 1.6439 - acc: 0.9291 - mDice: 0.5723 - val_loss: 3.4525 - val_acc: 0.9389 - val_mDice: 0.4321

Epoch 00104: val_mDice did not improve from 0.45340
Epoch 105/300
 - 12s - loss: 1.6375 - acc: 0.9294 - mDice: 0.5743 - val_loss: 3.4332 - val_acc: 0.9415 - val_mDice: 0.4532

Epoch 00105: val_mDice did not improve from 0.45340
Epoch 106/300
 - 13s - loss: 1.6330 - acc: 0.9296 - mDice: 0.5754 - val_loss: 3.5380 - val_acc: 0.9408 - val_mDice: 0.4435

Epoch 00106: val_mDice did not improve from 0.45340
Epoch 107/300
 - 12s - loss: 1.6335 - acc: 0.9296 - mDice: 0.5751 - val_loss: 3.6091 - val_acc: 0.9378 - val_mDice: 0.4402

Epoch 00107: val_mDice did not improve from 0.45340
Epoch 108/300
 - 12s - loss: 1.6331 - acc: 0.9296 - mDice: 0.5746 - val_loss: 3.6916 - val_acc: 0.9387 - val_mDice: 0.4480

Epoch 00108: val_mDice did not improve from 0.45340
Epoch 109/300
 - 13s - loss: 1.6243 - acc: 0.9299 - mDice: 0.5774 - val_loss: 3.5570 - val_acc: 0.9390 - val_mDice: 0.4419

Epoch 00109: val_mDice did not improve from 0.45340
Epoch 110/300
 - 12s - loss: 1.6220 - acc: 0.9301 - mDice: 0.5781 - val_loss: 3.4122 - val_acc: 0.9405 - val_mDice: 0.4500

Epoch 00110: val_mDice did not improve from 0.45340
Epoch 111/300
 - 12s - loss: 1.6109 - acc: 0.9302 - mDice: 0.5801 - val_loss: 3.4896 - val_acc: 0.9391 - val_mDice: 0.4536

Epoch 00111: val_mDice improved from 0.45340 to 0.45361, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 112/300
 - 12s - loss: 1.6147 - acc: 0.9303 - mDice: 0.5787 - val_loss: 3.3786 - val_acc: 0.9425 - val_mDice: 0.4607

Epoch 00112: val_mDice improved from 0.45361 to 0.46070, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 113/300
 - 12s - loss: 1.6138 - acc: 0.9305 - mDice: 0.5801 - val_loss: 3.3269 - val_acc: 0.9405 - val_mDice: 0.4555

Epoch 00113: val_mDice did not improve from 0.46070
Epoch 114/300
 - 13s - loss: 1.6092 - acc: 0.9305 - mDice: 0.5814 - val_loss: 3.4313 - val_acc: 0.9418 - val_mDice: 0.4550

Epoch 00114: val_mDice did not improve from 0.46070
Epoch 115/300
 - 12s - loss: 1.6014 - acc: 0.9308 - mDice: 0.5830 - val_loss: 3.3725 - val_acc: 0.9415 - val_mDice: 0.4569

Epoch 00115: val_mDice did not improve from 0.46070
Epoch 116/300
 - 12s - loss: 1.5965 - acc: 0.9311 - mDice: 0.5838 - val_loss: 3.3195 - val_acc: 0.9408 - val_mDice: 0.4593

Epoch 00116: val_mDice did not improve from 0.46070
Epoch 117/300
 - 12s - loss: 1.5946 - acc: 0.9310 - mDice: 0.5838 - val_loss: 3.2762 - val_acc: 0.9412 - val_mDice: 0.4578

Epoch 00117: val_mDice did not improve from 0.46070
Epoch 118/300
 - 12s - loss: 1.5908 - acc: 0.9311 - mDice: 0.5847 - val_loss: 3.2513 - val_acc: 0.9408 - val_mDice: 0.4621

Epoch 00118: val_mDice improved from 0.46070 to 0.46205, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 119/300
 - 12s - loss: 1.5794 - acc: 0.9315 - mDice: 0.5884 - val_loss: 3.4225 - val_acc: 0.9398 - val_mDice: 0.4564

Epoch 00119: val_mDice did not improve from 0.46205
Epoch 120/300
 - 12s - loss: 1.5878 - acc: 0.9315 - mDice: 0.5858 - val_loss: 3.6672 - val_acc: 0.9412 - val_mDice: 0.4490

Epoch 00120: val_mDice did not improve from 0.46205
Epoch 121/300
 - 12s - loss: 1.5783 - acc: 0.9314 - mDice: 0.5881 - val_loss: 3.8037 - val_acc: 0.9400 - val_mDice: 0.4423

Epoch 00121: val_mDice did not improve from 0.46205
Epoch 122/300
 - 12s - loss: 1.5784 - acc: 0.9312 - mDice: 0.5880 - val_loss: 3.5132 - val_acc: 0.9421 - val_mDice: 0.4526

Epoch 00122: val_mDice did not improve from 0.46205
Epoch 123/300
 - 12s - loss: 1.5686 - acc: 0.9316 - mDice: 0.5905 - val_loss: 3.4848 - val_acc: 0.9413 - val_mDice: 0.4555

Epoch 00123: val_mDice did not improve from 0.46205
Epoch 124/300
 - 12s - loss: 1.5733 - acc: 0.9316 - mDice: 0.5898 - val_loss: 3.4560 - val_acc: 0.9399 - val_mDice: 0.4511

Epoch 00124: val_mDice did not improve from 0.46205
Epoch 125/300
 - 12s - loss: 1.5721 - acc: 0.9314 - mDice: 0.5902 - val_loss: 3.7083 - val_acc: 0.9400 - val_mDice: 0.4389

Epoch 00125: val_mDice did not improve from 0.46205
Epoch 126/300
 - 12s - loss: 1.5623 - acc: 0.9317 - mDice: 0.5920 - val_loss: 3.5454 - val_acc: 0.9391 - val_mDice: 0.4450

Epoch 00126: val_mDice did not improve from 0.46205
Epoch 127/300
 - 13s - loss: 1.5661 - acc: 0.9316 - mDice: 0.5915 - val_loss: 3.5247 - val_acc: 0.9399 - val_mDice: 0.4520

Epoch 00127: val_mDice did not improve from 0.46205
Epoch 128/300
 - 12s - loss: 1.5488 - acc: 0.9321 - mDice: 0.5950 - val_loss: 3.2975 - val_acc: 0.9424 - val_mDice: 0.4666

Epoch 00128: val_mDice improved from 0.46205 to 0.46658, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 129/300
 - 12s - loss: 1.5509 - acc: 0.9318 - mDice: 0.5951 - val_loss: 3.8913 - val_acc: 0.9400 - val_mDice: 0.4336

Epoch 00129: val_mDice did not improve from 0.46658
Epoch 130/300
 - 12s - loss: 1.5480 - acc: 0.9320 - mDice: 0.5958 - val_loss: 3.7891 - val_acc: 0.9391 - val_mDice: 0.4443

Epoch 00130: val_mDice did not improve from 0.46658
Epoch 131/300
 - 12s - loss: 1.5527 - acc: 0.9318 - mDice: 0.5952 - val_loss: 3.4926 - val_acc: 0.9413 - val_mDice: 0.4551

Epoch 00131: val_mDice did not improve from 0.46658
Epoch 132/300
 - 12s - loss: 1.5439 - acc: 0.9320 - mDice: 0.5968 - val_loss: 3.5381 - val_acc: 0.9392 - val_mDice: 0.4460

Epoch 00132: val_mDice did not improve from 0.46658
Epoch 133/300
 - 12s - loss: 1.5454 - acc: 0.9319 - mDice: 0.5967 - val_loss: 3.5464 - val_acc: 0.9405 - val_mDice: 0.4475

Epoch 00133: val_mDice did not improve from 0.46658
Epoch 134/300
 - 12s - loss: 1.5360 - acc: 0.9322 - mDice: 0.5988 - val_loss: 3.5444 - val_acc: 0.9368 - val_mDice: 0.4409

Epoch 00134: val_mDice did not improve from 0.46658
Epoch 135/300
 - 13s - loss: 1.5323 - acc: 0.9323 - mDice: 0.6002 - val_loss: 4.1982 - val_acc: 0.9392 - val_mDice: 0.4379

Epoch 00135: val_mDice did not improve from 0.46658
Epoch 136/300
 - 12s - loss: 1.5357 - acc: 0.9321 - mDice: 0.5987 - val_loss: 3.4475 - val_acc: 0.9423 - val_mDice: 0.4659

Epoch 00136: val_mDice did not improve from 0.46658
Epoch 137/300
 - 12s - loss: 1.5317 - acc: 0.9321 - mDice: 0.6005 - val_loss: 3.4511 - val_acc: 0.9401 - val_mDice: 0.4627

Epoch 00137: val_mDice did not improve from 0.46658
Epoch 138/300
 - 12s - loss: 1.5412 - acc: 0.9321 - mDice: 0.5979 - val_loss: 3.6083 - val_acc: 0.9416 - val_mDice: 0.4599

Epoch 00138: val_mDice did not improve from 0.46658
Epoch 139/300
 - 12s - loss: 1.5292 - acc: 0.9323 - mDice: 0.6008 - val_loss: 3.6942 - val_acc: 0.9408 - val_mDice: 0.4534

Epoch 00139: val_mDice did not improve from 0.46658
Epoch 140/300
 - 12s - loss: 1.5256 - acc: 0.9324 - mDice: 0.6014 - val_loss: 3.4100 - val_acc: 0.9412 - val_mDice: 0.4607

Epoch 00140: val_mDice did not improve from 0.46658
Epoch 141/300
 - 12s - loss: 1.5215 - acc: 0.9325 - mDice: 0.6027 - val_loss: 3.5463 - val_acc: 0.9401 - val_mDice: 0.4541

Epoch 00141: val_mDice did not improve from 0.46658
Epoch 142/300
 - 12s - loss: 1.5148 - acc: 0.9326 - mDice: 0.6042 - val_loss: 3.4638 - val_acc: 0.9410 - val_mDice: 0.4650

Epoch 00142: val_mDice did not improve from 0.46658
Epoch 143/300
 - 12s - loss: 1.5191 - acc: 0.9326 - mDice: 0.6030 - val_loss: 3.7926 - val_acc: 0.9402 - val_mDice: 0.4496

Epoch 00143: val_mDice did not improve from 0.46658
Epoch 144/300
 - 12s - loss: 1.5184 - acc: 0.9327 - mDice: 0.6033 - val_loss: 3.5689 - val_acc: 0.9407 - val_mDice: 0.4543

Epoch 00144: val_mDice did not improve from 0.46658
Epoch 145/300
 - 12s - loss: 1.5105 - acc: 0.9329 - mDice: 0.6055 - val_loss: 3.5912 - val_acc: 0.9411 - val_mDice: 0.4567

Epoch 00145: val_mDice did not improve from 0.46658
Epoch 146/300
 - 12s - loss: 1.5136 - acc: 0.9328 - mDice: 0.6049 - val_loss: 3.5785 - val_acc: 0.9392 - val_mDice: 0.4502

Epoch 00146: val_mDice did not improve from 0.46658
Epoch 147/300
 - 12s - loss: 1.5128 - acc: 0.9328 - mDice: 0.6047 - val_loss: 3.4020 - val_acc: 0.9396 - val_mDice: 0.4645

Epoch 00147: val_mDice did not improve from 0.46658
Epoch 148/300
 - 12s - loss: 1.5057 - acc: 0.9329 - mDice: 0.6069 - val_loss: 3.7703 - val_acc: 0.9383 - val_mDice: 0.4437

Epoch 00148: val_mDice did not improve from 0.46658
Epoch 149/300
 - 12s - loss: 1.5052 - acc: 0.9331 - mDice: 0.6065 - val_loss: 3.5060 - val_acc: 0.9393 - val_mDice: 0.4533

Epoch 00149: val_mDice did not improve from 0.46658
Epoch 150/300
 - 12s - loss: 1.5045 - acc: 0.9331 - mDice: 0.6072 - val_loss: 3.4150 - val_acc: 0.9410 - val_mDice: 0.4626

Epoch 00150: val_mDice did not improve from 0.46658
Epoch 151/300
 - 12s - loss: 1.5018 - acc: 0.9332 - mDice: 0.6073 - val_loss: 3.4543 - val_acc: 0.9410 - val_mDice: 0.4626

Epoch 00151: val_mDice did not improve from 0.46658
Epoch 152/300
 - 13s - loss: 1.4924 - acc: 0.9332 - mDice: 0.6099 - val_loss: 3.6517 - val_acc: 0.9389 - val_mDice: 0.4419

Epoch 00152: val_mDice did not improve from 0.46658
Epoch 153/300
 - 12s - loss: 1.4958 - acc: 0.9332 - mDice: 0.6090 - val_loss: 3.6013 - val_acc: 0.9402 - val_mDice: 0.4519

Epoch 00153: val_mDice did not improve from 0.46658
Epoch 154/300
 - 12s - loss: 1.4860 - acc: 0.9337 - mDice: 0.6112 - val_loss: 3.5793 - val_acc: 0.9402 - val_mDice: 0.4612

Epoch 00154: val_mDice did not improve from 0.46658
Epoch 155/300
 - 12s - loss: 1.4939 - acc: 0.9334 - mDice: 0.6097 - val_loss: 3.4026 - val_acc: 0.9424 - val_mDice: 0.4653

Epoch 00155: val_mDice did not improve from 0.46658
Epoch 156/300
 - 12s - loss: 1.4955 - acc: 0.9335 - mDice: 0.6091 - val_loss: 3.7751 - val_acc: 0.9389 - val_mDice: 0.4431

Epoch 00156: val_mDice did not improve from 0.46658
Epoch 157/300
 - 12s - loss: 1.4833 - acc: 0.9336 - mDice: 0.6113 - val_loss: 3.5892 - val_acc: 0.9410 - val_mDice: 0.4561

Epoch 00157: val_mDice did not improve from 0.46658
Epoch 158/300
 - 12s - loss: 1.4840 - acc: 0.9338 - mDice: 0.6122 - val_loss: 3.2981 - val_acc: 0.9415 - val_mDice: 0.4726

Epoch 00158: val_mDice improved from 0.46658 to 0.47258, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 159/300
 - 12s - loss: 1.4860 - acc: 0.9336 - mDice: 0.6110 - val_loss: 3.4330 - val_acc: 0.9403 - val_mDice: 0.4541

Epoch 00159: val_mDice did not improve from 0.47258
Epoch 160/300
 - 12s - loss: 1.4864 - acc: 0.9338 - mDice: 0.6116 - val_loss: 3.5043 - val_acc: 0.9405 - val_mDice: 0.4599

Epoch 00160: val_mDice did not improve from 0.47258
Epoch 161/300
 - 12s - loss: 1.4743 - acc: 0.9343 - mDice: 0.6147 - val_loss: 3.6649 - val_acc: 0.9389 - val_mDice: 0.4456

Epoch 00161: val_mDice did not improve from 0.47258
Epoch 162/300
 - 12s - loss: 1.4741 - acc: 0.9342 - mDice: 0.6142 - val_loss: 3.6319 - val_acc: 0.9410 - val_mDice: 0.4541

Epoch 00162: val_mDice did not improve from 0.47258
Epoch 163/300
 - 12s - loss: 1.4743 - acc: 0.9340 - mDice: 0.6144 - val_loss: 3.7993 - val_acc: 0.9402 - val_mDice: 0.4494

Epoch 00163: val_mDice did not improve from 0.47258
Epoch 164/300
 - 12s - loss: 1.4731 - acc: 0.9342 - mDice: 0.6151 - val_loss: 3.9616 - val_acc: 0.9370 - val_mDice: 0.4336

Epoch 00164: val_mDice did not improve from 0.47258
Epoch 165/300
 - 12s - loss: 1.4740 - acc: 0.9341 - mDice: 0.6142 - val_loss: 3.7220 - val_acc: 0.9383 - val_mDice: 0.4446

Epoch 00165: val_mDice did not improve from 0.47258
Epoch 166/300
 - 12s - loss: 1.4745 - acc: 0.9342 - mDice: 0.6142 - val_loss: 3.6466 - val_acc: 0.9373 - val_mDice: 0.4364

Epoch 00166: val_mDice did not improve from 0.47258
Epoch 167/300
 - 12s - loss: 1.4642 - acc: 0.9345 - mDice: 0.6164 - val_loss: 3.6838 - val_acc: 0.9387 - val_mDice: 0.4425

Epoch 00167: val_mDice did not improve from 0.47258
Epoch 168/300
 - 12s - loss: 1.4647 - acc: 0.9347 - mDice: 0.6164 - val_loss: 3.4694 - val_acc: 0.9414 - val_mDice: 0.4619

Epoch 00168: val_mDice did not improve from 0.47258
Epoch 169/300
 - 12s - loss: 1.4677 - acc: 0.9345 - mDice: 0.6165 - val_loss: 3.7630 - val_acc: 0.9411 - val_mDice: 0.4513

Epoch 00169: val_mDice did not improve from 0.47258
Epoch 170/300
 - 12s - loss: 1.4626 - acc: 0.9345 - mDice: 0.6170 - val_loss: 3.3757 - val_acc: 0.9406 - val_mDice: 0.4696

Epoch 00170: val_mDice did not improve from 0.47258
Epoch 171/300
 - 12s - loss: 1.4591 - acc: 0.9349 - mDice: 0.6182 - val_loss: 3.6753 - val_acc: 0.9382 - val_mDice: 0.4440

Epoch 00171: val_mDice did not improve from 0.47258
Epoch 172/300
 - 12s - loss: 1.4580 - acc: 0.9347 - mDice: 0.6181 - val_loss: 3.5138 - val_acc: 0.9416 - val_mDice: 0.4573

Epoch 00172: val_mDice did not improve from 0.47258
Epoch 173/300
 - 12s - loss: 1.4587 - acc: 0.9350 - mDice: 0.6183 - val_loss: 3.4131 - val_acc: 0.9417 - val_mDice: 0.4704

Epoch 00173: val_mDice did not improve from 0.47258
Epoch 174/300
 - 12s - loss: 1.4595 - acc: 0.9348 - mDice: 0.6178 - val_loss: 3.9118 - val_acc: 0.9395 - val_mDice: 0.4364

Epoch 00174: val_mDice did not improve from 0.47258
Epoch 175/300
 - 12s - loss: 1.4558 - acc: 0.9350 - mDice: 0.6190 - val_loss: 3.6116 - val_acc: 0.9404 - val_mDice: 0.4569

Epoch 00175: val_mDice did not improve from 0.47258
Epoch 176/300
 - 12s - loss: 1.4492 - acc: 0.9351 - mDice: 0.6210 - val_loss: 3.5340 - val_acc: 0.9406 - val_mDice: 0.4604

Epoch 00176: val_mDice did not improve from 0.47258
Epoch 177/300
 - 12s - loss: 1.4522 - acc: 0.9353 - mDice: 0.6201 - val_loss: 3.6779 - val_acc: 0.9415 - val_mDice: 0.4465

Epoch 00177: val_mDice did not improve from 0.47258
Epoch 178/300
 - 12s - loss: 1.4538 - acc: 0.9351 - mDice: 0.6191 - val_loss: 3.4990 - val_acc: 0.9426 - val_mDice: 0.4626

Epoch 00178: val_mDice did not improve from 0.47258
Epoch 179/300
 - 12s - loss: 1.4504 - acc: 0.9351 - mDice: 0.6201 - val_loss: 3.5596 - val_acc: 0.9426 - val_mDice: 0.4634

Epoch 00179: val_mDice did not improve from 0.47258
Epoch 180/300
 - 12s - loss: 1.4459 - acc: 0.9353 - mDice: 0.6215 - val_loss: 3.5054 - val_acc: 0.9416 - val_mDice: 0.4670

Epoch 00180: val_mDice did not improve from 0.47258
Epoch 181/300
 - 12s - loss: 1.4409 - acc: 0.9356 - mDice: 0.6225 - val_loss: 3.5619 - val_acc: 0.9439 - val_mDice: 0.4635

Epoch 00181: val_mDice did not improve from 0.47258
Epoch 182/300
 - 12s - loss: 1.4431 - acc: 0.9355 - mDice: 0.6219 - val_loss: 3.4728 - val_acc: 0.9407 - val_mDice: 0.4583

Epoch 00182: val_mDice did not improve from 0.47258
Epoch 183/300
 - 12s - loss: 1.4394 - acc: 0.9355 - mDice: 0.6227 - val_loss: 3.6818 - val_acc: 0.9409 - val_mDice: 0.4531

Epoch 00183: val_mDice did not improve from 0.47258
Epoch 184/300
 - 12s - loss: 1.4386 - acc: 0.9357 - mDice: 0.6226 - val_loss: 3.5109 - val_acc: 0.9395 - val_mDice: 0.4623

Epoch 00184: val_mDice did not improve from 0.47258
Epoch 185/300
 - 12s - loss: 1.4445 - acc: 0.9356 - mDice: 0.6221 - val_loss: 3.5915 - val_acc: 0.9411 - val_mDice: 0.4592

Epoch 00185: val_mDice did not improve from 0.47258
Epoch 186/300
 - 12s - loss: 1.4459 - acc: 0.9356 - mDice: 0.6214 - val_loss: 3.5632 - val_acc: 0.9410 - val_mDice: 0.4490

Epoch 00186: val_mDice did not improve from 0.47258
Epoch 187/300
 - 13s - loss: 1.4398 - acc: 0.9357 - mDice: 0.6226 - val_loss: 3.8887 - val_acc: 0.9377 - val_mDice: 0.4419

Epoch 00187: val_mDice did not improve from 0.47258
Epoch 188/300
 - 12s - loss: 1.4335 - acc: 0.9359 - mDice: 0.6244 - val_loss: 3.6877 - val_acc: 0.9403 - val_mDice: 0.4510

Epoch 00188: val_mDice did not improve from 0.47258
Restoring model weights from the end of the best epoch
Epoch 00188: early stopping
{'val_loss': [42.094895442326866, 14.742419577780224, 9.16241267323494, 7.28983727310385, 6.175813314105783, 5.484463160414071, 5.122858164211114, 5.281844509144624, 5.420248893311336, 4.448060304281258, 4.752169969713404, 5.712877723610117, 5.690078776329756, 3.942552698598731, 3.619123395443672, 3.439818156528331, 3.3056901142089847, 3.6127968026059016, 3.8779124497835125, 3.3052575926163366, 3.3089998773343505, 3.3876209225328195, 3.0721596743290625, 3.0640950939573703, 3.147523997041086, 3.438243574861969, 3.4089487634288766, 3.435550983862153, 3.11932519712441, 3.4300716043937776, 3.1057425711215254, 2.999360984923052, 3.463240175574486, 2.909287827288998, 2.9994600936548697, 3.2761947910434435, 2.9786004689965573, 3.0189801072923554, 3.028812124305183, 3.1310258423687802, 2.9319139050231096, 3.209502322909733, 3.2039537710536803, 3.1435967553628696, 3.1632611049738313, 3.224454567235495, 2.9908395654388835, 3.1990249067589285, 3.2861977737130865, 3.104793102147856, 3.110856784041971, 3.184864049156507, 3.268118174502715, 3.148670268910272, 3.282390594305027, 3.375145284525518, 3.230203870528688, 3.281488720216744, 3.4233298179516125, 3.3066363193183426, 3.1143270543599058, 3.243315024367933, 3.2389209903776646, 3.2716222140893696, 3.2130017537579296, 3.323194365127988, 3.2812114055268466, 3.282473140462701, 3.3472147758251856, 3.4156801946062063, 3.3038261699978086, 3.323700819630176, 3.442142000754497, 3.577627169943991, 3.1673711463809013, 3.1855681983842734, 3.197822824275742, 3.350822812145842, 3.1450871994186724, 3.238235574747835, 3.3083758368822083, 3.309768236707896, 3.56824922630386, 3.4135645511781885, 3.4859417576919354, 3.3890293140540875, 3.3918706101498435, 3.561658330372579, 3.545181132853031, 3.3054905025998043, 3.5548814771297788, 3.6141533760265228, 3.283305974810251, 3.608864624496727, 3.592879532348542, 3.513530190657115, 3.3319276002058316, 3.4657996039660204, 3.533304717303032, 3.6528562618463876, 3.4087880330071565, 3.6938623826995136, 3.3331849707423578, 3.4525412031271983, 3.4331664783170535, 3.5379539310399974, 3.609074424530956, 3.6916145316458175, 3.557012710764649, 3.412233979529923, 3.4895838202154708, 3.378636894326302, 3.326872471254319, 3.4312650267522606, 3.3725021147124825, 3.319505628686221, 3.2762273965580833, 3.2513398387291956, 3.4225004788355102, 3.6671500180049668, 3.803705616521516, 3.513189680497384, 3.4848424962145232, 3.4559801349061585, 3.7082583566772795, 3.54540983534285, 3.524693835149741, 3.297460272974734, 3.8913390935797776, 3.789128559508494, 3.492637630906843, 3.538107878373315, 3.546426709186995, 3.544391694500865, 4.198249917965205, 3.447545112665033, 3.4510941985063255, 3.608295399274322, 3.6942223777018843, 3.4099721928526248, 3.546271678038119, 3.463827540876255, 3.7926373288390183, 3.568935791296618, 3.5911531418207145, 3.578542147408284, 3.4020407359514917, 3.7703149990134297, 3.506026118860713, 3.415011417231567, 3.454331922012248, 3.6517491599190093, 3.6012581276618656, 3.579255569881449, 3.4026135509124114, 3.7750842724039795, 3.5892211108113683, 3.2980982607585334, 3.4330341380000826, 3.504346774491881, 3.664877842313477, 3.631941691750572, 3.799277746539918, 3.961607300454662, 3.7219873136796413, 3.646608213428408, 3.683791643585123, 3.469442257968088, 3.7629677390103184, 3.3757499571712244, 3.6752668918066083, 3.5138047661720995, 3.4131177156010555, 3.911841230733054, 3.6115714915628945, 3.533998830953524, 3.6778656836554764, 3.498963374334077, 3.5595992339908014, 3.505397759488828, 3.561902525258206, 3.4727917817820395, 3.681839694180304, 3.5108679308378625, 3.5915402405052665, 3.563248829250889, 3.8886519747431434, 3.6876561625949327], 'val_acc': [0.9001305103302002, 0.9045650107519967, 0.9047321677207947, 0.9047413070996603, 0.9047229744139171, 0.9047413070996603, 0.904734438373929, 0.904739036446526, 0.9048992877914792, 0.9057211506934393, 0.905096153418223, 0.9049977206048512, 0.9049953931853885, 0.9091575003805614, 0.9110370902788072, 0.9140704926990327, 0.914755037852696, 0.9138736469405038, 0.9138140990620568, 0.9206845107532683, 0.9203388407116845, 0.9196428486279079, 0.9228709056263878, 0.925698249112992, 0.9234958943866548, 0.922676268078032, 0.9227632738295055, 0.9238507321902684, 0.9266689476512727, 0.9240316039039975, 0.9268452212924049, 0.9269757469495138, 0.9273740564073835, 0.9289033668381828, 0.9282417723110744, 0.9252472321192423, 0.9307715410277957, 0.9302976216588702, 0.9288118368103391, 0.9301694404511225, 0.9305563086555118, 0.9298443254970369, 0.9276373840513683, 0.931627767426627, 0.9302472500574022, 0.9303227890105474, 0.931499549320766, 0.9321543290501549, 0.9304143502598717, 0.9319459426970709, 0.93266023624511, 0.9324198819342113, 0.9317696832475209, 0.9309890156700498, 0.9321176835468837, 0.9334935829752967, 0.9330517507734752, 0.9341689348220825, 0.9308310576847622, 0.9346359854652768, 0.9344139382952735, 0.9338576254390535, 0.9357944386346, 0.9355013767878214, 0.9358699775877453, 0.9333035718827021, 0.9354716056869143, 0.9345512986183167, 0.9320535943621681, 0.93617901631764, 0.9364583549045381, 0.9366597873823983, 0.9355425664356777, 0.9347962197803316, 0.9380517488434201, 0.9389354331152779, 0.934219343321664, 0.9349130250158764, 0.9362706258183434, 0.9362385812259856, 0.9348992506663004, 0.9376968627884275, 0.936911614168258, 0.9377174718039376, 0.9369436899820963, 0.937273329212552, 0.9381043910980225, 0.9371474214962551, 0.9385531260853722, 0.9361584385236105, 0.9374816616376241, 0.9364537511553083, 0.9396085228238787, 0.9378662875720433, 0.9377632964225042, 0.9368566643624079, 0.9370421171188354, 0.9395146313167754, 0.938189080783299, 0.9379944801330566, 0.9390498939014617, 0.940712000642504, 0.9401305147579738, 0.9389148212614513, 0.9414514615422204, 0.9408402017184666, 0.9378136651856559, 0.9387202433177403, 0.9389789643741789, 0.9405288355691093, 0.9390521957760766, 0.9424977160635448, 0.9405105199132647, 0.9417696935789925, 0.9415064283779689, 0.9407829954510644, 0.9412362632297334, 0.9408287491117205, 0.939787106854575, 0.9411790342557997, 0.9399862459727696, 0.942094794341496, 0.9412706182116554, 0.9398694776353382, 0.9399999805859157, 0.9391140256609235, 0.9398603240648905, 0.9423832155409313, 0.9399656511488415, 0.9391117010797773, 0.9413461429732186, 0.9392376258259728, 0.9404739198230562, 0.9368040362993876, 0.9391941428184509, 0.9423236932073321, 0.9401350929623559, 0.9415545066197714, 0.9407829415230524, 0.9411881963411967, 0.9400549502599806, 0.9409866957437425, 0.9401762797718957, 0.9407303219749814, 0.9410691204525176, 0.9391643773941767, 0.9396245593116397, 0.9382738272349039, 0.939345257622855, 0.9410485483351207, 0.9410210564022973, 0.9389354160853794, 0.9401716760226658, 0.9402312295777457, 0.9424427520661127, 0.938882759639195, 0.941018765880948, 0.9414789364451454, 0.9402838973771959, 0.9405311629885719, 0.9389308634258452, 0.9410393862497239, 0.9402449670292082, 0.9370306843803042, 0.9382829552605039, 0.9373420391763959, 0.9387408665248326, 0.9414468804995219, 0.9410943445705232, 0.9405654895873297, 0.9381776650746664, 0.9415613327707563, 0.9417033138729277, 0.9395009165718442, 0.9404120814232599, 0.9405860702196757, 0.9415498943555922, 0.942635056518373, 0.9426282275290716, 0.9416483640670776, 0.9438644448916117, 0.9406845427694774, 0.940867699327923, 0.9395306507746378, 0.9410508218265715, 0.9409684027944293, 0.9377129219827198, 0.9403433970042637], 'val_mDice': [0.012593252639774056, 0.011015436412500483, 0.014368276128412358, 0.015553898277825542, 0.03358143452732336, 0.048929174253273575, 0.06623122277891352, 0.06471731028674792, 0.0720368314248931, 0.11944867944666407, 0.1128540278060384, 0.09474277643624061, 0.09597767761650695, 0.19116539820762618, 0.22042529524437018, 0.25448823986308916, 0.2683139851405507, 0.25806972160491914, 0.2551685547722237, 0.29564222254391226, 0.30024266584465903, 0.30733840877101537, 0.3224033374960224, 0.3318974394794731, 0.32113176584243774, 0.31425105316919233, 0.3233370719743626, 0.3375020034034692, 0.3533522913321143, 0.3413076059271892, 0.3601942812757833, 0.36920583106222604, 0.34177074545905706, 0.3830570209594, 0.3734067248269206, 0.3631397838748637, 0.39003655686974525, 0.38333061443907873, 0.388268666607993, 0.38127318521340686, 0.40348876924032256, 0.385776361007066, 0.38806468957946416, 0.4016841337794349, 0.3986802596066679, 0.40022029674478937, 0.4127297082117626, 0.40453060947003816, 0.39913704210803624, 0.4095715321600437, 0.41854339252625195, 0.4152169523849374, 0.4062073715031147, 0.40732240978451, 0.40899316256954554, 0.41409762966490926, 0.420139651568163, 0.41736638599208425, 0.4136728018167473, 0.41530484670684453, 0.4304040384789308, 0.41931591573215665, 0.4295729858179887, 0.4303691121084349, 0.43146338242860066, 0.4297031694579692, 0.4304828202085836, 0.42207524677117664, 0.42619566512959345, 0.4345169702456111, 0.4384259205488932, 0.4427319287898995, 0.43275316272463116, 0.4218684873055844, 0.45059275644875707, 0.4469983061509473, 0.444402126151891, 0.43690806964323636, 0.4492665915971711, 0.4399623987930162, 0.44111390518290655, 0.44784884349930854, 0.4269657191776094, 0.4389669829536052, 0.4356156408431984, 0.43872901016757604, 0.44310581045491354, 0.43157517111727167, 0.44519075538430897, 0.43783186801842283, 0.4404322022483462, 0.4360761818076883, 0.4533994879041399, 0.4403091374607313, 0.43678279788721175, 0.439532886658396, 0.451628032539572, 0.44787225127220154, 0.4435661094529288, 0.4383046382239887, 0.45262941913235755, 0.4450403017302354, 0.4488426752033688, 0.43211922652664636, 0.45324755637418657, 0.44354971533729914, 0.44017258357434047, 0.44795340867269606, 0.44190457090735435, 0.4500026289551031, 0.45360920099275454, 0.4606994346138977, 0.45551258644887377, 0.45500073333581287, 0.45693629181810785, 0.45926517869035405, 0.45781284836786135, 0.4620544832377207, 0.4564362047683625, 0.4490318926317351, 0.44230550598530544, 0.4526384597023328, 0.45547472933928174, 0.45105090435771716, 0.4389307071410474, 0.4449702840121019, 0.4519619515963963, 0.466584471364816, 0.4335853576305367, 0.4442514640589555, 0.45505467660370325, 0.44603251115906806, 0.4474760072217101, 0.44090851999464487, 0.4378849784178393, 0.4658794039416881, 0.46266739744515645, 0.4599116611338797, 0.45339414406390416, 0.4607184021955445, 0.45409653300330755, 0.46501181842315764, 0.44962037585320924, 0.4543215829346861, 0.45670780539512634, 0.4501954700265612, 0.4645356067589351, 0.44368255847976323, 0.45327975636436824, 0.4626220216353734, 0.4625770690895262, 0.44192297756671906, 0.45193962291592643, 0.4612232010279383, 0.46534709898488863, 0.4431473556019011, 0.4561443910712287, 0.47258452059967176, 0.454141969482104, 0.4599259818593661, 0.44563856401613783, 0.45406401583126615, 0.449424755360399, 0.4336048348673752, 0.44456383266619276, 0.4363818819678965, 0.4424913388987382, 0.46191690276776043, 0.4513281299954369, 0.4696247269000326, 0.44398938296806245, 0.4573172988990943, 0.4703721253290063, 0.43635548154513043, 0.4569072155725388, 0.4603671669250443, 0.4464959292894318, 0.46264742421252386, 0.46344408854132607, 0.46696134301878156, 0.4634513051382133, 0.458286457828113, 0.45314652792045046, 0.4623458406754902, 0.45923052515302387, 0.44902302750519346, 0.4418909375866254, 0.450961136924369], 'loss': [172.4512431128351, 39.84732677105973, 19.672451491619732, 13.554994026406183, 10.702661812868516, 8.89152442685023, 7.651912620170098, 6.767632710398197, 6.110911716632538, 5.611881688814621, 5.189527351180092, 4.854588137441502, 4.574841315148308, 4.33934123910919, 4.12402023082297, 3.9423848323241026, 3.7819618785774316, 3.6254288317818437, 3.4872728709152305, 3.37985218368745, 3.2643184858593886, 3.184389370724671, 3.105204118890801, 3.0210491160747055, 2.960528136035117, 2.896624239832766, 2.844619455998496, 2.7825897909759765, 2.7326252244675757, 2.6698826456980047, 2.6397551930614904, 2.592973897975144, 2.549318001820491, 2.519475469521206, 2.4807619915914967, 2.441484482366041, 2.419181665566148, 2.386204634118177, 2.351529914633672, 2.3299856782993738, 2.3006091778140916, 2.2835778467729186, 2.2618664468624057, 2.2328776798888117, 2.212514123660143, 2.2013241082274755, 2.1751903123839043, 2.154845227206031, 2.139099155797052, 2.121560860810906, 2.1068749522712387, 2.0953674225855914, 2.0768701731871384, 2.0640321647773847, 2.052895643128718, 2.0341660079409993, 2.016690557325233, 1.998517354674162, 1.9881991480778978, 1.9768904172916828, 1.964296659799079, 1.9538285868324616, 1.9378155206746854, 1.9252155476138613, 1.920680120682151, 1.9089398999330922, 1.8968552523135496, 1.8783927612597022, 1.884120658226778, 1.8637663125761998, 1.85721788770412, 1.8513466770625515, 1.8320343334688165, 1.8297959034075644, 1.8237590099635876, 1.8048628104406446, 1.8002704178319402, 1.8020351439375442, 1.787989427019733, 1.7765046293068558, 1.7710972960247615, 1.7756798107637384, 1.769518873998143, 1.7522376530290409, 1.7546333339785665, 1.738799556454919, 1.7329613176273753, 1.730353243667679, 1.7286761084893592, 1.7104595596215295, 1.7106578795464484, 1.709154967446306, 1.7036104854968372, 1.6969662260544973, 1.6851992511050604, 1.6858808006283312, 1.6747192139567635, 1.671823474651636, 1.6700759828952505, 1.6719569712906002, 1.660466121712562, 1.656753297207878, 1.6570753669012528, 1.6439281211828758, 1.6374775742620362, 1.6330047886978805, 1.6334722745802353, 1.633129744394588, 1.6243140665993472, 1.6219980780825993, 1.6109046819744621, 1.6147348492826274, 1.6138468138487188, 1.6091796458514411, 1.6014335351526243, 1.5964503701491142, 1.594568578567674, 1.590842311566612, 1.5794262861686106, 1.5877803801249188, 1.578310695162439, 1.578383711001674, 1.5685584157653596, 1.5733160732659803, 1.5720529268765602, 1.5623416850763132, 1.566089100867125, 1.548802245529672, 1.5509095036595089, 1.548039640837008, 1.5527198803270854, 1.5439419593796326, 1.5453803387592817, 1.5360216577089383, 1.5322732018532725, 1.5357022558261555, 1.5317348350282576, 1.5411869034915158, 1.52919171064236, 1.5256213934315361, 1.5214502614933056, 1.5148214229343253, 1.5191265564330723, 1.518403305560609, 1.5105036416677973, 1.5135570719381368, 1.512812987214135, 1.5056652953359508, 1.5052161597552869, 1.504471052904131, 1.5017709547277818, 1.4924367630621913, 1.495751580016241, 1.4859836205412065, 1.4938810176419137, 1.4955271455578338, 1.4832682114059303, 1.484029566779724, 1.4860084893410666, 1.486395975011425, 1.4742542962842258, 1.4740570728227722, 1.47428975384291, 1.4730978672826638, 1.474011359978715, 1.474487774813821, 1.4642455644958714, 1.464665919754899, 1.4676718769538648, 1.4626145726664117, 1.4591190080686827, 1.4579979432716244, 1.4587468930768075, 1.459519471119244, 1.4557835612656502, 1.4491584895317646, 1.4522380503657606, 1.453763045303068, 1.450414413600982, 1.44586008696375, 1.4409126436087536, 1.4430884473328878, 1.43942773762709, 1.4386385074810002, 1.444491144386771, 1.4458911717914766, 1.4397709664292755, 1.43345369248347], 'acc': [0.706177404282156, 0.851701370996847, 0.8658524548844241, 0.8674963793475112, 0.8677343080010238, 0.8678840214125517, 0.8684239735694342, 0.869730957503123, 0.8712653947147717, 0.8732565041219933, 0.8752467326973591, 0.8765777951953543, 0.8777700088514582, 0.8789591869683723, 0.8802600781858277, 0.8816899614502337, 0.8837022724605742, 0.885372592151705, 0.8873175693679826, 0.8893190569010732, 0.891244799444527, 0.8928326515533663, 0.8943875503259655, 0.8960991816206293, 0.8973927726308022, 0.8985535462099806, 0.8992169547315413, 0.9000876570382237, 0.9008349947601195, 0.9020059856463886, 0.9024800357847101, 0.9035880209072629, 0.9044893584270709, 0.9054231812481892, 0.9061361123928565, 0.907115794145144, 0.9075209304319771, 0.9085989533441476, 0.9095869745526995, 0.9100148462941243, 0.9103686074909962, 0.9111256315249893, 0.9115796621327964, 0.9124156977368707, 0.9129518298875718, 0.9133451948488566, 0.9139221748773455, 0.9145916746212885, 0.914830833116156, 0.9152921572893553, 0.9157590035392941, 0.9160350480359116, 0.9165601433759851, 0.916909738682772, 0.9169950596349006, 0.9175960416850819, 0.9181796248326395, 0.9184959234013731, 0.9188279702021296, 0.9191749270697286, 0.9194491647653596, 0.9197339223418044, 0.9200463722913693, 0.9205379861500168, 0.920602865788618, 0.9207291549079929, 0.9209707167329231, 0.9214041722586143, 0.9214057226228833, 0.9220654267163457, 0.9220126399604116, 0.9221388612129979, 0.9228414958305204, 0.9229377538844848, 0.9229928794131473, 0.9235009007976077, 0.9237824091337677, 0.92359858751297, 0.9241727657439461, 0.924298936125169, 0.9245951872283792, 0.9243802710813158, 0.9248256323101757, 0.9251968675388729, 0.9250627009078489, 0.925742820643588, 0.9259577831689715, 0.9260345314862252, 0.9260338569321751, 0.9264586210044014, 0.9267077413463906, 0.9267281566669147, 0.9267367049405312, 0.9270422027155639, 0.9275614369207433, 0.9276862434047436, 0.9279348963553078, 0.9281138302228665, 0.928088409893311, 0.9283384360374457, 0.9285920032743362, 0.9286934957583332, 0.9289807816426925, 0.929137329605243, 0.9293574651519021, 0.9295757901859302, 0.929560587944864, 0.9296454428730154, 0.9298597618704796, 0.9301437532853501, 0.9301667413386118, 0.9302566713603769, 0.930468989953798, 0.9305122760143357, 0.9307977568213825, 0.9310536430432246, 0.9310478040173216, 0.9310636718263511, 0.931542408059552, 0.9314594769928297, 0.9313935530721188, 0.9312378142595705, 0.9315677084558751, 0.931575981562184, 0.9313876648295792, 0.931714501963752, 0.9315532253890316, 0.9320679653212586, 0.9318390749358983, 0.9319877423876073, 0.9318370358755944, 0.932046396793923, 0.9318683383114102, 0.9322158943970171, 0.9322628845753917, 0.932092873386411, 0.9320813141761773, 0.9321000129587738, 0.9322902040718745, 0.9323569886114916, 0.9325105023218622, 0.932624854033959, 0.9325589746759649, 0.9326760881273921, 0.9329161940913497, 0.932811967848996, 0.9327668971539187, 0.932900647046549, 0.9330670456217965, 0.9330683385151439, 0.9331693448656346, 0.9332231032795499, 0.9332340659721982, 0.9337416910495467, 0.9333592634658857, 0.93348962893623, 0.9335749909117982, 0.9338355625620324, 0.9336242110301287, 0.9337588833526125, 0.934257052768419, 0.9341509049513769, 0.9340200761230046, 0.9342429219056169, 0.9341393184726215, 0.9342446809033794, 0.9345337040672096, 0.9346521355431503, 0.9345333099204358, 0.9345108334880724, 0.9349306149360461, 0.9347301773512917, 0.9349583980088889, 0.9347806213930298, 0.9350144273402352, 0.9351080631920201, 0.9353396193195636, 0.9350683246018318, 0.9351274092601447, 0.9352780520306954, 0.9355756702486527, 0.9354851889614887, 0.9354887072442928, 0.9357172750842946, 0.9356047273808323, 0.9355500213024771, 0.9357084001514846, 0.9358955131391581], 'mDice': [0.015139592995323817, 0.013886494356266262, 0.017384425998926277, 0.026260343915509056, 0.03533482864349707, 0.04783935662045147, 0.06650488327808732, 0.08608421832840177, 0.10439002920385268, 0.12264310568140173, 0.14236724466022233, 0.15987476990623742, 0.17637971022902643, 0.19145860884526067, 0.20727679066766497, 0.22194655524275542, 0.23688670984232887, 0.25117669696520123, 0.2646862104928675, 0.2758714442299537, 0.2881674903974796, 0.29663016766464867, 0.3061195215738553, 0.31531093420264394, 0.3228794041570726, 0.33154346416261765, 0.3380745286734228, 0.34850431190696274, 0.35625154434979617, 0.36465105143773757, 0.3694935799828609, 0.37652407369104357, 0.3840075285765209, 0.3895817672112371, 0.3957364377613263, 0.40263548240579805, 0.4060016744210813, 0.412887970319207, 0.41881592234757126, 0.42235655855967036, 0.42799827354202985, 0.4320811598671409, 0.43593412356177164, 0.4415862769238954, 0.4449912410768811, 0.4477018537389444, 0.4524391466352736, 0.4572052714370546, 0.46089031710913353, 0.46439364497340296, 0.46691584912296985, 0.46989178408748133, 0.47458223983320813, 0.476924608984227, 0.47905103528754706, 0.4822535553212016, 0.486566175408094, 0.4894361252359096, 0.49314446859629096, 0.49496684917623146, 0.4982683089557631, 0.500469991058565, 0.504599125198668, 0.5069968138606143, 0.5089833522224463, 0.5110059764393498, 0.5140377607376837, 0.5183997388770407, 0.5167912979311674, 0.5211068438683858, 0.5236046335153413, 0.5247748277999542, 0.5289698647744904, 0.5296237897109912, 0.5315598780556821, 0.5357401031553343, 0.5369189409982591, 0.5362411970482733, 0.540224940486788, 0.5421062378104721, 0.5439309347841076, 0.5432848784650983, 0.5443434679763313, 0.5473300499196271, 0.547183613913216, 0.5512667390131321, 0.5527189339151818, 0.5532107669676984, 0.5530090871656104, 0.5574871072976926, 0.5574141461908507, 0.5582953543476409, 0.559046617564563, 0.5605983014324623, 0.5633576238157664, 0.5632415263437227, 0.5654321296242176, 0.5667951538403277, 0.5665818404802035, 0.5662631460276785, 0.5688983267020554, 0.5700248912140563, 0.5704019912663099, 0.572331248794605, 0.574253855354643, 0.5754466432814472, 0.5751321898826818, 0.5745756025716062, 0.5774102531601888, 0.5781376928131269, 0.5801141278142067, 0.5787009788841923, 0.5801494423309365, 0.581380099512052, 0.5830360692958968, 0.5837900075743325, 0.5837647783887255, 0.58469101202288, 0.5883645023687373, 0.5858321013031426, 0.5880586340957371, 0.5879793400384797, 0.5905083044199946, 0.5897585839946674, 0.5901925115312298, 0.5920008517469955, 0.5914662332577997, 0.5950344705802295, 0.59510071755191, 0.595806185268864, 0.5952349676087525, 0.5967789760715175, 0.5966696707711, 0.5987687038736397, 0.6001813417573137, 0.5987210411176944, 0.6004898330253835, 0.5979459585195703, 0.6007911984760085, 0.6013982275765549, 0.6026694183522436, 0.6042068948836737, 0.6029822285611528, 0.6033320847057713, 0.6055054220018852, 0.604925074167327, 0.6046975355330335, 0.6068914972887762, 0.6065166136398926, 0.6072259621857355, 0.6073058384770669, 0.6099458501498456, 0.6090123505520642, 0.6111676324096423, 0.609704098134733, 0.6091435283324795, 0.6113340793281983, 0.6122245356506618, 0.6110319152189997, 0.6115807635788286, 0.6146795200157055, 0.6141528022847746, 0.6144498608487866, 0.6151205844816638, 0.6142391239330446, 0.6142407190438891, 0.6163982504568326, 0.6163874732889558, 0.6165393209190803, 0.6170121572691971, 0.6181587693271596, 0.6181370270649822, 0.6182760440914852, 0.6178470028143112, 0.6189520871545025, 0.6210445938644461, 0.6201100271975853, 0.6190562319590082, 0.6200988316136057, 0.621523458833108, 0.6224848476475734, 0.6219356518755167, 0.6226918076260002, 0.6226472462076247, 0.6221497922773785, 0.6213559194040454, 0.6226022098251499, 0.6244170021776384]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.34s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.07s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.88s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:23,  1.35s/it]predicting train subjects:   1%|          | 2/285 [00:03<06:52,  1.46s/it]predicting train subjects:   1%|          | 3/285 [00:04<06:49,  1.45s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:26,  1.59s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<07:08,  1.53s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:31,  1.62s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<07:59,  1.73s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<08:08,  1.76s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<07:59,  1.74s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:14,  1.80s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:35,  1.88s/it]predicting train subjects:   4%|▍         | 12/285 [00:21<08:45,  1.93s/it]predicting train subjects:   5%|▍         | 13/285 [00:23<08:53,  1.96s/it]predicting train subjects:   5%|▍         | 14/285 [00:25<08:46,  1.94s/it]predicting train subjects:   5%|▌         | 15/285 [00:27<08:50,  1.96s/it]predicting train subjects:   6%|▌         | 16/285 [00:29<09:07,  2.04s/it]predicting train subjects:   6%|▌         | 17/285 [00:31<08:58,  2.01s/it]predicting train subjects:   6%|▋         | 18/285 [00:33<08:43,  1.96s/it]predicting train subjects:   7%|▋         | 19/285 [00:35<08:37,  1.94s/it]predicting train subjects:   7%|▋         | 20/285 [00:37<08:38,  1.96s/it]predicting train subjects:   7%|▋         | 21/285 [00:38<08:35,  1.95s/it]predicting train subjects:   8%|▊         | 22/285 [00:40<08:34,  1.95s/it]predicting train subjects:   8%|▊         | 23/285 [00:42<08:30,  1.95s/it]predicting train subjects:   8%|▊         | 24/285 [00:44<08:30,  1.96s/it]predicting train subjects:   9%|▉         | 25/285 [00:46<08:29,  1.96s/it]predicting train subjects:   9%|▉         | 26/285 [00:48<08:25,  1.95s/it]predicting train subjects:   9%|▉         | 27/285 [00:50<08:20,  1.94s/it]predicting train subjects:  10%|▉         | 28/285 [00:52<08:10,  1.91s/it]predicting train subjects:  10%|█         | 29/285 [00:54<08:06,  1.90s/it]predicting train subjects:  11%|█         | 30/285 [00:56<07:54,  1.86s/it]predicting train subjects:  11%|█         | 31/285 [00:57<07:50,  1.85s/it]predicting train subjects:  11%|█         | 32/285 [00:59<07:45,  1.84s/it]predicting train subjects:  12%|█▏        | 33/285 [01:01<07:41,  1.83s/it]predicting train subjects:  12%|█▏        | 34/285 [01:03<07:34,  1.81s/it]predicting train subjects:  12%|█▏        | 35/285 [01:05<07:33,  1.82s/it]predicting train subjects:  13%|█▎        | 36/285 [01:07<07:34,  1.83s/it]predicting train subjects:  13%|█▎        | 37/285 [01:08<07:27,  1.80s/it]predicting train subjects:  13%|█▎        | 38/285 [01:10<07:23,  1.79s/it]predicting train subjects:  14%|█▎        | 39/285 [01:12<07:26,  1.82s/it]predicting train subjects:  14%|█▍        | 40/285 [01:14<07:33,  1.85s/it]predicting train subjects:  14%|█▍        | 41/285 [01:16<07:31,  1.85s/it]predicting train subjects:  15%|█▍        | 42/285 [01:18<07:31,  1.86s/it]predicting train subjects:  15%|█▌        | 43/285 [01:19<07:28,  1.85s/it]predicting train subjects:  15%|█▌        | 44/285 [01:21<07:29,  1.87s/it]predicting train subjects:  16%|█▌        | 45/285 [01:23<07:28,  1.87s/it]predicting train subjects:  16%|█▌        | 46/285 [01:25<07:08,  1.79s/it]predicting train subjects:  16%|█▋        | 47/285 [01:26<06:46,  1.71s/it]predicting train subjects:  17%|█▋        | 48/285 [01:28<06:31,  1.65s/it]predicting train subjects:  17%|█▋        | 49/285 [01:29<06:22,  1.62s/it]predicting train subjects:  18%|█▊        | 50/285 [01:31<06:15,  1.60s/it]predicting train subjects:  18%|█▊        | 51/285 [01:33<06:18,  1.62s/it]predicting train subjects:  18%|█▊        | 52/285 [01:34<06:12,  1.60s/it]predicting train subjects:  19%|█▊        | 53/285 [01:36<06:08,  1.59s/it]predicting train subjects:  19%|█▉        | 54/285 [01:37<06:02,  1.57s/it]predicting train subjects:  19%|█▉        | 55/285 [01:39<06:00,  1.57s/it]predicting train subjects:  20%|█▉        | 56/285 [01:40<05:57,  1.56s/it]predicting train subjects:  20%|██        | 57/285 [01:42<05:55,  1.56s/it]predicting train subjects:  20%|██        | 58/285 [01:43<05:53,  1.56s/it]predicting train subjects:  21%|██        | 59/285 [01:45<05:53,  1.56s/it]predicting train subjects:  21%|██        | 60/285 [01:47<05:53,  1.57s/it]predicting train subjects:  21%|██▏       | 61/285 [01:48<05:51,  1.57s/it]predicting train subjects:  22%|██▏       | 62/285 [01:50<05:51,  1.58s/it]predicting train subjects:  22%|██▏       | 63/285 [01:51<05:46,  1.56s/it]predicting train subjects:  22%|██▏       | 64/285 [01:53<05:54,  1.60s/it]predicting train subjects:  23%|██▎       | 65/285 [01:55<06:13,  1.70s/it]predicting train subjects:  23%|██▎       | 66/285 [01:57<06:16,  1.72s/it]predicting train subjects:  24%|██▎       | 67/285 [01:58<06:15,  1.72s/it]predicting train subjects:  24%|██▍       | 68/285 [02:00<06:02,  1.67s/it]predicting train subjects:  24%|██▍       | 69/285 [02:02<05:56,  1.65s/it]predicting train subjects:  25%|██▍       | 70/285 [02:03<05:55,  1.65s/it]predicting train subjects:  25%|██▍       | 71/285 [02:05<05:50,  1.64s/it]predicting train subjects:  25%|██▌       | 72/285 [02:06<05:40,  1.60s/it]predicting train subjects:  26%|██▌       | 73/285 [02:08<05:37,  1.59s/it]predicting train subjects:  26%|██▌       | 74/285 [02:10<05:41,  1.62s/it]predicting train subjects:  26%|██▋       | 75/285 [02:11<05:39,  1.62s/it]predicting train subjects:  27%|██▋       | 76/285 [02:13<05:42,  1.64s/it]predicting train subjects:  27%|██▋       | 77/285 [02:15<05:39,  1.63s/it]predicting train subjects:  27%|██▋       | 78/285 [02:16<05:34,  1.62s/it]predicting train subjects:  28%|██▊       | 79/285 [02:18<05:28,  1.59s/it]predicting train subjects:  28%|██▊       | 80/285 [02:19<05:22,  1.58s/it]predicting train subjects:  28%|██▊       | 81/285 [02:21<05:26,  1.60s/it]predicting train subjects:  29%|██▉       | 82/285 [02:22<05:28,  1.62s/it]predicting train subjects:  29%|██▉       | 83/285 [02:24<05:31,  1.64s/it]predicting train subjects:  29%|██▉       | 84/285 [02:26<05:37,  1.68s/it]predicting train subjects:  30%|██▉       | 85/285 [02:28<05:40,  1.70s/it]predicting train subjects:  30%|███       | 86/285 [02:30<05:44,  1.73s/it]predicting train subjects:  31%|███       | 87/285 [02:31<05:50,  1.77s/it]predicting train subjects:  31%|███       | 88/285 [02:33<05:57,  1.81s/it]predicting train subjects:  31%|███       | 89/285 [02:35<05:59,  1.83s/it]predicting train subjects:  32%|███▏      | 90/285 [02:37<05:57,  1.83s/it]predicting train subjects:  32%|███▏      | 91/285 [02:39<05:54,  1.83s/it]predicting train subjects:  32%|███▏      | 92/285 [02:41<05:54,  1.83s/it]predicting train subjects:  33%|███▎      | 93/285 [02:43<05:54,  1.84s/it]predicting train subjects:  33%|███▎      | 94/285 [02:44<05:57,  1.87s/it]predicting train subjects:  33%|███▎      | 95/285 [02:46<05:50,  1.84s/it]predicting train subjects:  34%|███▎      | 96/285 [02:48<05:44,  1.82s/it]predicting train subjects:  34%|███▍      | 97/285 [02:50<05:38,  1.80s/it]predicting train subjects:  34%|███▍      | 98/285 [02:52<05:39,  1.81s/it]predicting train subjects:  35%|███▍      | 99/285 [02:53<05:39,  1.83s/it]predicting train subjects:  35%|███▌      | 100/285 [02:55<05:40,  1.84s/it]predicting train subjects:  35%|███▌      | 101/285 [02:57<05:41,  1.86s/it]predicting train subjects:  36%|███▌      | 102/285 [02:59<05:40,  1.86s/it]predicting train subjects:  36%|███▌      | 103/285 [03:01<05:34,  1.84s/it]predicting train subjects:  36%|███▋      | 104/285 [03:03<05:32,  1.84s/it]predicting train subjects:  37%|███▋      | 105/285 [03:05<05:30,  1.84s/it]predicting train subjects:  37%|███▋      | 106/285 [03:06<05:22,  1.80s/it]predicting train subjects:  38%|███▊      | 107/285 [03:08<05:22,  1.81s/it]predicting train subjects:  38%|███▊      | 108/285 [03:10<05:25,  1.84s/it]predicting train subjects:  38%|███▊      | 109/285 [03:12<05:21,  1.83s/it]predicting train subjects:  39%|███▊      | 110/285 [03:14<05:15,  1.80s/it]predicting train subjects:  39%|███▉      | 111/285 [03:15<05:13,  1.80s/it]predicting train subjects:  39%|███▉      | 112/285 [03:17<05:13,  1.81s/it]predicting train subjects:  40%|███▉      | 113/285 [03:19<05:10,  1.81s/it]predicting train subjects:  40%|████      | 114/285 [03:21<05:06,  1.79s/it]predicting train subjects:  40%|████      | 115/285 [03:23<05:02,  1.78s/it]predicting train subjects:  41%|████      | 116/285 [03:24<04:59,  1.77s/it]predicting train subjects:  41%|████      | 117/285 [03:26<05:00,  1.79s/it]predicting train subjects:  41%|████▏     | 118/285 [03:28<04:56,  1.77s/it]predicting train subjects:  42%|████▏     | 119/285 [03:30<04:55,  1.78s/it]predicting train subjects:  42%|████▏     | 120/285 [03:31<04:55,  1.79s/it]predicting train subjects:  42%|████▏     | 121/285 [03:33<04:44,  1.74s/it]predicting train subjects:  43%|████▎     | 122/285 [03:34<04:28,  1.65s/it]predicting train subjects:  43%|████▎     | 123/285 [03:36<04:16,  1.58s/it]predicting train subjects:  44%|████▎     | 124/285 [03:37<04:13,  1.57s/it]predicting train subjects:  44%|████▍     | 125/285 [03:39<04:11,  1.57s/it]predicting train subjects:  44%|████▍     | 126/285 [03:41<04:12,  1.59s/it]predicting train subjects:  45%|████▍     | 127/285 [03:42<04:14,  1.61s/it]predicting train subjects:  45%|████▍     | 128/285 [03:44<04:10,  1.60s/it]predicting train subjects:  45%|████▌     | 129/285 [03:46<04:12,  1.62s/it]predicting train subjects:  46%|████▌     | 130/285 [03:47<04:08,  1.61s/it]predicting train subjects:  46%|████▌     | 131/285 [03:49<04:04,  1.59s/it]predicting train subjects:  46%|████▋     | 132/285 [03:50<04:02,  1.59s/it]predicting train subjects:  47%|████▋     | 133/285 [03:52<04:00,  1.58s/it]predicting train subjects:  47%|████▋     | 134/285 [03:54<04:03,  1.61s/it]predicting train subjects:  47%|████▋     | 135/285 [03:55<04:03,  1.63s/it]predicting train subjects:  48%|████▊     | 136/285 [03:57<04:01,  1.62s/it]predicting train subjects:  48%|████▊     | 137/285 [03:59<04:05,  1.66s/it]predicting train subjects:  48%|████▊     | 138/285 [04:00<04:03,  1.65s/it]predicting train subjects:  49%|████▉     | 139/285 [04:02<04:00,  1.65s/it]predicting train subjects:  49%|████▉     | 140/285 [04:03<03:55,  1.62s/it]predicting train subjects:  49%|████▉     | 141/285 [04:05<03:50,  1.60s/it]predicting train subjects:  50%|████▉     | 142/285 [04:06<03:40,  1.54s/it]predicting train subjects:  50%|█████     | 143/285 [04:08<03:35,  1.51s/it]predicting train subjects:  51%|█████     | 144/285 [04:09<03:28,  1.48s/it]predicting train subjects:  51%|█████     | 145/285 [04:11<03:26,  1.47s/it]predicting train subjects:  51%|█████     | 146/285 [04:12<03:24,  1.47s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:14<03:21,  1.46s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:15<03:17,  1.44s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:16<03:14,  1.43s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:18<03:12,  1.42s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:19<03:11,  1.43s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:21<03:11,  1.44s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:22<03:08,  1.43s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:24<03:07,  1.43s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:25<03:06,  1.43s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:26<03:03,  1.43s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:28<03:03,  1.43s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:29<03:02,  1.43s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:31<03:02,  1.45s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:32<02:59,  1.44s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:34<02:58,  1.44s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:35<02:57,  1.44s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:36<02:54,  1.43s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:38<02:54,  1.44s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:39<02:50,  1.42s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:41<02:50,  1.43s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:42<02:51,  1.45s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:44<02:50,  1.45s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:45<02:47,  1.44s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:46<02:44,  1.43s/it]predicting train subjects:  60%|██████    | 171/285 [04:48<02:42,  1.42s/it]predicting train subjects:  60%|██████    | 172/285 [04:49<02:39,  1.41s/it]predicting train subjects:  61%|██████    | 173/285 [04:51<02:39,  1.42s/it]predicting train subjects:  61%|██████    | 174/285 [04:52<02:35,  1.40s/it]predicting train subjects:  61%|██████▏   | 175/285 [04:54<02:36,  1.43s/it]predicting train subjects:  62%|██████▏   | 176/285 [04:55<02:37,  1.45s/it]predicting train subjects:  62%|██████▏   | 177/285 [04:57<02:38,  1.47s/it]predicting train subjects:  62%|██████▏   | 178/285 [04:58<02:34,  1.45s/it]predicting train subjects:  63%|██████▎   | 179/285 [04:59<02:32,  1.44s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:01<02:29,  1.42s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:02<02:26,  1.41s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:04<02:25,  1.41s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:05<02:22,  1.40s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:06<02:20,  1.39s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:08<02:19,  1.39s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:09<02:17,  1.39s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:11<02:17,  1.40s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:12<02:14,  1.39s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:13<02:11,  1.37s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:15<02:09,  1.37s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:16<02:07,  1.36s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:17<02:08,  1.38s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:19<02:06,  1.37s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:20<02:06,  1.39s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:22<02:04,  1.39s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:23<02:09,  1.46s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:25<02:14,  1.53s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:26<02:15,  1.55s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:28<02:15,  1.58s/it]predicting train subjects:  70%|███████   | 200/285 [05:30<02:15,  1.59s/it]predicting train subjects:  71%|███████   | 201/285 [05:31<02:14,  1.60s/it]predicting train subjects:  71%|███████   | 202/285 [05:33<02:13,  1.61s/it]predicting train subjects:  71%|███████   | 203/285 [05:35<02:13,  1.63s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:36<02:11,  1.63s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:38<02:09,  1.62s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:39<02:07,  1.62s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:41<02:08,  1.65s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:43<02:05,  1.63s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:44<02:03,  1.62s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:46<02:01,  1.63s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:48<02:01,  1.64s/it]predicting train subjects:  74%|███████▍  | 212/285 [05:49<01:58,  1.63s/it]predicting train subjects:  75%|███████▍  | 213/285 [05:51<01:56,  1.61s/it]predicting train subjects:  75%|███████▌  | 214/285 [05:52<01:52,  1.58s/it]predicting train subjects:  75%|███████▌  | 215/285 [05:54<01:46,  1.52s/it]predicting train subjects:  76%|███████▌  | 216/285 [05:55<01:43,  1.50s/it]predicting train subjects:  76%|███████▌  | 217/285 [05:57<01:41,  1.49s/it]predicting train subjects:  76%|███████▋  | 218/285 [05:58<01:37,  1.45s/it]predicting train subjects:  77%|███████▋  | 219/285 [05:59<01:35,  1.45s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:01<01:33,  1.44s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:02<01:31,  1.42s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:04<01:28,  1.41s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:05<01:27,  1.42s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:06<01:25,  1.41s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:08<01:23,  1.39s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:09<01:22,  1.40s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:11<01:21,  1.41s/it]predicting train subjects:  80%|████████  | 228/285 [06:12<01:20,  1.41s/it]predicting train subjects:  80%|████████  | 229/285 [06:14<01:20,  1.44s/it]predicting train subjects:  81%|████████  | 230/285 [06:15<01:19,  1.45s/it]predicting train subjects:  81%|████████  | 231/285 [06:17<01:18,  1.45s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:18<01:23,  1.58s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:20<01:27,  1.68s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:22<01:27,  1.72s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:24<01:30,  1.81s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:26<01:28,  1.80s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:28<01:27,  1.82s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:30<01:25,  1.82s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:31<01:24,  1.83s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:33<01:21,  1.81s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:35<01:19,  1.81s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:37<01:16,  1.79s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:39<01:16,  1.82s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:40<01:13,  1.80s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:42<01:11,  1.78s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:44<01:08,  1.76s/it]predicting train subjects:  87%|████████▋ | 247/285 [06:46<01:06,  1.75s/it]predicting train subjects:  87%|████████▋ | 248/285 [06:47<01:05,  1.77s/it]predicting train subjects:  87%|████████▋ | 249/285 [06:49<01:03,  1.76s/it]predicting train subjects:  88%|████████▊ | 250/285 [06:51<00:57,  1.65s/it]predicting train subjects:  88%|████████▊ | 251/285 [06:52<00:52,  1.55s/it]predicting train subjects:  88%|████████▊ | 252/285 [06:53<00:48,  1.48s/it]predicting train subjects:  89%|████████▉ | 253/285 [06:54<00:45,  1.43s/it]predicting train subjects:  89%|████████▉ | 254/285 [06:56<00:42,  1.39s/it]predicting train subjects:  89%|████████▉ | 255/285 [06:57<00:41,  1.38s/it]predicting train subjects:  90%|████████▉ | 256/285 [06:59<00:40,  1.39s/it]predicting train subjects:  90%|█████████ | 257/285 [07:00<00:38,  1.37s/it]predicting train subjects:  91%|█████████ | 258/285 [07:01<00:36,  1.36s/it]predicting train subjects:  91%|█████████ | 259/285 [07:03<00:35,  1.38s/it]predicting train subjects:  91%|█████████ | 260/285 [07:04<00:34,  1.38s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:05<00:32,  1.35s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:07<00:30,  1.34s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:08<00:29,  1.32s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:09<00:27,  1.33s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:11<00:26,  1.34s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:12<00:25,  1.34s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:13<00:24,  1.36s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:15<00:25,  1.49s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:17<00:25,  1.59s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:19<00:25,  1.67s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:21<00:24,  1.73s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:22<00:22,  1.74s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:24<00:21,  1.76s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:26<00:19,  1.76s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:28<00:17,  1.76s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:30<00:15,  1.76s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:31<00:14,  1.76s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:33<00:12,  1.76s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:35<00:10,  1.81s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:37<00:09,  1.84s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:39<00:07,  1.84s/it]predicting train subjects:  99%|█████████▉| 282/285 [07:41<00:05,  1.83s/it]predicting train subjects:  99%|█████████▉| 283/285 [07:42<00:03,  1.84s/it]predicting train subjects: 100%|█████████▉| 284/285 [07:44<00:01,  1.83s/it]predicting train subjects: 100%|██████████| 285/285 [07:46<00:00,  1.82s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<06:36,  1.40s/it]Loading train:   1%|          | 2/285 [00:02<06:46,  1.44s/it]Loading train:   1%|          | 3/285 [00:04<06:40,  1.42s/it]Loading train:   1%|▏         | 4/285 [00:06<07:08,  1.52s/it]Loading train:   2%|▏         | 5/285 [00:07<06:39,  1.43s/it]Loading train:   2%|▏         | 6/285 [00:08<06:55,  1.49s/it]Loading train:   2%|▏         | 7/285 [00:10<07:19,  1.58s/it]Loading train:   3%|▎         | 8/285 [00:12<07:17,  1.58s/it]Loading train:   3%|▎         | 9/285 [00:13<06:56,  1.51s/it]Loading train:   4%|▎         | 10/285 [00:14<06:30,  1.42s/it]Loading train:   4%|▍         | 11/285 [00:15<06:05,  1.33s/it]Loading train:   4%|▍         | 12/285 [00:17<05:43,  1.26s/it]Loading train:   5%|▍         | 13/285 [00:18<05:38,  1.25s/it]Loading train:   5%|▍         | 14/285 [00:19<05:35,  1.24s/it]Loading train:   5%|▌         | 15/285 [00:20<05:37,  1.25s/it]Loading train:   6%|▌         | 16/285 [00:21<05:33,  1.24s/it]Loading train:   6%|▌         | 17/285 [00:23<05:34,  1.25s/it]Loading train:   6%|▋         | 18/285 [00:24<05:30,  1.24s/it]Loading train:   7%|▋         | 19/285 [00:25<05:30,  1.24s/it]Loading train:   7%|▋         | 20/285 [00:26<05:31,  1.25s/it]Loading train:   7%|▋         | 21/285 [00:28<05:24,  1.23s/it]Loading train:   8%|▊         | 22/285 [00:29<05:25,  1.24s/it]Loading train:   8%|▊         | 23/285 [00:30<05:24,  1.24s/it]Loading train:   8%|▊         | 24/285 [00:31<05:22,  1.24s/it]Loading train:   9%|▉         | 25/285 [00:33<05:17,  1.22s/it]Loading train:   9%|▉         | 26/285 [00:34<05:13,  1.21s/it]Loading train:   9%|▉         | 27/285 [00:35<05:06,  1.19s/it]Loading train:  10%|▉         | 28/285 [00:36<04:58,  1.16s/it]Loading train:  10%|█         | 29/285 [00:37<04:54,  1.15s/it]Loading train:  11%|█         | 30/285 [00:38<04:49,  1.13s/it]Loading train:  11%|█         | 31/285 [00:39<04:42,  1.11s/it]Loading train:  11%|█         | 32/285 [00:40<04:39,  1.10s/it]Loading train:  12%|█▏        | 33/285 [00:41<04:37,  1.10s/it]Loading train:  12%|█▏        | 34/285 [00:43<04:32,  1.08s/it]Loading train:  12%|█▏        | 35/285 [00:44<04:30,  1.08s/it]Loading train:  13%|█▎        | 36/285 [00:45<04:29,  1.08s/it]Loading train:  13%|█▎        | 37/285 [00:46<04:26,  1.08s/it]Loading train:  13%|█▎        | 38/285 [00:47<04:38,  1.13s/it]Loading train:  14%|█▎        | 39/285 [00:48<04:44,  1.16s/it]Loading train:  14%|█▍        | 40/285 [00:49<04:35,  1.12s/it]Loading train:  14%|█▍        | 41/285 [00:50<04:30,  1.11s/it]Loading train:  15%|█▍        | 42/285 [00:51<04:25,  1.09s/it]Loading train:  15%|█▌        | 43/285 [00:53<04:28,  1.11s/it]Loading train:  15%|█▌        | 44/285 [00:54<04:28,  1.11s/it]Loading train:  16%|█▌        | 45/285 [00:55<04:21,  1.09s/it]Loading train:  16%|█▌        | 46/285 [00:56<04:22,  1.10s/it]Loading train:  16%|█▋        | 47/285 [00:57<04:10,  1.05s/it]Loading train:  17%|█▋        | 48/285 [00:58<04:02,  1.02s/it]Loading train:  17%|█▋        | 49/285 [00:59<04:00,  1.02s/it]Loading train:  18%|█▊        | 50/285 [01:00<03:57,  1.01s/it]Loading train:  18%|█▊        | 51/285 [01:01<03:51,  1.01it/s]Loading train:  18%|█▊        | 52/285 [01:02<03:48,  1.02it/s]Loading train:  19%|█▊        | 53/285 [01:03<03:47,  1.02it/s]Loading train:  19%|█▉        | 54/285 [01:04<03:44,  1.03it/s]Loading train:  19%|█▉        | 55/285 [01:05<03:47,  1.01it/s]Loading train:  20%|█▉        | 56/285 [01:06<03:45,  1.02it/s]Loading train:  20%|██        | 57/285 [01:07<03:56,  1.04s/it]Loading train:  20%|██        | 58/285 [01:08<03:53,  1.03s/it]Loading train:  21%|██        | 59/285 [01:09<03:49,  1.02s/it]Loading train:  21%|██        | 60/285 [01:10<03:47,  1.01s/it]Loading train:  21%|██▏       | 61/285 [01:11<03:44,  1.00s/it]Loading train:  22%|██▏       | 62/285 [01:12<03:40,  1.01it/s]Loading train:  22%|██▏       | 63/285 [01:13<03:40,  1.01it/s]Loading train:  22%|██▏       | 64/285 [01:14<04:16,  1.16s/it]Loading train:  23%|██▎       | 65/285 [01:16<04:46,  1.30s/it]Loading train:  23%|██▎       | 66/285 [01:17<04:59,  1.37s/it]Loading train:  24%|██▎       | 67/285 [01:18<04:42,  1.30s/it]Loading train:  24%|██▍       | 68/285 [01:19<04:20,  1.20s/it]Loading train:  24%|██▍       | 69/285 [01:21<04:15,  1.18s/it]Loading train:  25%|██▍       | 70/285 [01:22<04:08,  1.15s/it]Loading train:  25%|██▍       | 71/285 [01:23<04:02,  1.14s/it]Loading train:  25%|██▌       | 72/285 [01:24<03:57,  1.11s/it]Loading train:  26%|██▌       | 73/285 [01:25<03:53,  1.10s/it]Loading train:  26%|██▌       | 74/285 [01:26<03:47,  1.08s/it]Loading train:  26%|██▋       | 75/285 [01:27<03:47,  1.08s/it]Loading train:  27%|██▋       | 76/285 [01:28<03:44,  1.07s/it]Loading train:  27%|██▋       | 77/285 [01:29<03:29,  1.01s/it]Loading train:  27%|██▋       | 78/285 [01:30<03:19,  1.04it/s]Loading train:  28%|██▊       | 79/285 [01:31<03:16,  1.05it/s]Loading train:  28%|██▊       | 80/285 [01:32<03:12,  1.06it/s]Loading train:  28%|██▊       | 81/285 [01:33<03:09,  1.08it/s]Loading train:  29%|██▉       | 82/285 [01:33<03:10,  1.06it/s]Loading train:  29%|██▉       | 83/285 [01:34<03:08,  1.07it/s]Loading train:  29%|██▉       | 84/285 [01:35<03:08,  1.07it/s]Loading train:  30%|██▉       | 85/285 [01:36<03:13,  1.04it/s]Loading train:  30%|███       | 86/285 [01:37<03:16,  1.01it/s]Loading train:  31%|███       | 87/285 [01:38<03:19,  1.01s/it]Loading train:  31%|███       | 88/285 [01:40<03:23,  1.03s/it]Loading train:  31%|███       | 89/285 [01:41<03:19,  1.02s/it]Loading train:  32%|███▏      | 90/285 [01:42<03:19,  1.02s/it]Loading train:  32%|███▏      | 91/285 [01:43<03:18,  1.02s/it]Loading train:  32%|███▏      | 92/285 [01:44<03:20,  1.04s/it]Loading train:  33%|███▎      | 93/285 [01:45<03:15,  1.02s/it]Loading train:  33%|███▎      | 94/285 [01:46<03:20,  1.05s/it]Loading train:  33%|███▎      | 95/285 [01:47<03:15,  1.03s/it]Loading train:  34%|███▎      | 96/285 [01:48<03:13,  1.02s/it]Loading train:  34%|███▍      | 97/285 [01:49<03:15,  1.04s/it]Loading train:  34%|███▍      | 98/285 [01:50<03:17,  1.06s/it]Loading train:  35%|███▍      | 99/285 [01:51<03:18,  1.07s/it]Loading train:  35%|███▌      | 100/285 [01:52<03:13,  1.04s/it]Loading train:  35%|███▌      | 101/285 [01:53<03:16,  1.07s/it]Loading train:  36%|███▌      | 102/285 [01:54<03:25,  1.12s/it]Loading train:  36%|███▌      | 103/285 [01:56<03:32,  1.17s/it]Loading train:  36%|███▋      | 104/285 [01:57<03:35,  1.19s/it]Loading train:  37%|███▋      | 105/285 [01:58<03:30,  1.17s/it]Loading train:  37%|███▋      | 106/285 [01:59<03:27,  1.16s/it]Loading train:  38%|███▊      | 107/285 [02:00<03:24,  1.15s/it]Loading train:  38%|███▊      | 108/285 [02:01<03:20,  1.13s/it]Loading train:  38%|███▊      | 109/285 [02:02<03:16,  1.12s/it]Loading train:  39%|███▊      | 110/285 [02:04<03:14,  1.11s/it]Loading train:  39%|███▉      | 111/285 [02:05<03:14,  1.12s/it]Loading train:  39%|███▉      | 112/285 [02:06<03:12,  1.11s/it]Loading train:  40%|███▉      | 113/285 [02:07<03:09,  1.10s/it]Loading train:  40%|████      | 114/285 [02:08<03:03,  1.08s/it]Loading train:  40%|████      | 115/285 [02:09<03:04,  1.09s/it]Loading train:  41%|████      | 116/285 [02:10<03:05,  1.10s/it]Loading train:  41%|████      | 117/285 [02:11<03:03,  1.09s/it]Loading train:  41%|████▏     | 118/285 [02:12<03:01,  1.08s/it]Loading train:  42%|████▏     | 119/285 [02:13<03:02,  1.10s/it]Loading train:  42%|████▏     | 120/285 [02:14<02:59,  1.09s/it]Loading train:  42%|████▏     | 121/285 [02:16<03:14,  1.18s/it]Loading train:  43%|████▎     | 122/285 [02:17<03:22,  1.24s/it]Loading train:  43%|████▎     | 123/285 [02:19<03:23,  1.25s/it]Loading train:  44%|████▎     | 124/285 [02:19<03:07,  1.17s/it]Loading train:  44%|████▍     | 125/285 [02:20<02:52,  1.08s/it]Loading train:  44%|████▍     | 126/285 [02:21<02:46,  1.05s/it]Loading train:  45%|████▍     | 127/285 [02:22<02:41,  1.02s/it]Loading train:  45%|████▍     | 128/285 [02:23<02:36,  1.00it/s]Loading train:  45%|████▌     | 129/285 [02:24<02:34,  1.01it/s]Loading train:  46%|████▌     | 130/285 [02:25<02:30,  1.03it/s]Loading train:  46%|████▌     | 131/285 [02:26<02:29,  1.03it/s]Loading train:  46%|████▋     | 132/285 [02:27<02:26,  1.04it/s]Loading train:  47%|████▋     | 133/285 [02:28<02:25,  1.04it/s]Loading train:  47%|████▋     | 134/285 [02:29<02:25,  1.04it/s]Loading train:  47%|████▋     | 135/285 [02:30<02:22,  1.05it/s]Loading train:  48%|████▊     | 136/285 [02:31<02:20,  1.06it/s]Loading train:  48%|████▊     | 137/285 [02:32<02:16,  1.08it/s]Loading train:  48%|████▊     | 138/285 [02:33<02:16,  1.07it/s]Loading train:  49%|████▉     | 139/285 [02:34<02:15,  1.08it/s]Loading train:  49%|████▉     | 140/285 [02:34<02:11,  1.10it/s]Loading train:  49%|████▉     | 141/285 [02:35<02:09,  1.11it/s]Loading train:  50%|████▉     | 142/285 [02:36<02:10,  1.09it/s]Loading train:  50%|█████     | 143/285 [02:37<02:09,  1.10it/s]Loading train:  51%|█████     | 144/285 [02:38<02:07,  1.11it/s]Loading train:  51%|█████     | 145/285 [02:39<02:05,  1.12it/s]Loading train:  51%|█████     | 146/285 [02:40<02:04,  1.12it/s]Loading train:  52%|█████▏    | 147/285 [02:41<02:00,  1.14it/s]Loading train:  52%|█████▏    | 148/285 [02:41<01:57,  1.17it/s]Loading train:  52%|█████▏    | 149/285 [02:42<01:55,  1.18it/s]Loading train:  53%|█████▎    | 150/285 [02:43<01:57,  1.15it/s]Loading train:  53%|█████▎    | 151/285 [02:44<01:59,  1.12it/s]Loading train:  53%|█████▎    | 152/285 [02:45<01:58,  1.12it/s]Loading train:  54%|█████▎    | 153/285 [02:46<01:57,  1.12it/s]Loading train:  54%|█████▍    | 154/285 [02:47<01:59,  1.10it/s]Loading train:  54%|█████▍    | 155/285 [02:48<02:00,  1.08it/s]Loading train:  55%|█████▍    | 156/285 [02:49<01:58,  1.09it/s]Loading train:  55%|█████▌    | 157/285 [02:50<01:51,  1.14it/s]Loading train:  55%|█████▌    | 158/285 [02:50<01:49,  1.16it/s]Loading train:  56%|█████▌    | 159/285 [02:51<01:47,  1.17it/s]Loading train:  56%|█████▌    | 160/285 [02:52<01:50,  1.13it/s]Loading train:  56%|█████▋    | 161/285 [02:53<01:50,  1.12it/s]Loading train:  57%|█████▋    | 162/285 [02:54<01:49,  1.12it/s]Loading train:  57%|█████▋    | 163/285 [02:55<01:48,  1.12it/s]Loading train:  58%|█████▊    | 164/285 [02:56<01:45,  1.14it/s]Loading train:  58%|█████▊    | 165/285 [02:57<01:43,  1.16it/s]Loading train:  58%|█████▊    | 166/285 [02:57<01:38,  1.20it/s]Loading train:  59%|█████▊    | 167/285 [02:58<01:40,  1.18it/s]Loading train:  59%|█████▉    | 168/285 [02:59<01:42,  1.14it/s]Loading train:  59%|█████▉    | 169/285 [03:00<01:41,  1.14it/s]Loading train:  60%|█████▉    | 170/285 [03:01<01:42,  1.12it/s]Loading train:  60%|██████    | 171/285 [03:02<01:39,  1.14it/s]Loading train:  60%|██████    | 172/285 [03:03<01:44,  1.08it/s]Loading train:  61%|██████    | 173/285 [03:04<01:41,  1.10it/s]Loading train:  61%|██████    | 174/285 [03:05<01:41,  1.09it/s]Loading train:  61%|██████▏   | 175/285 [03:06<01:42,  1.08it/s]Loading train:  62%|██████▏   | 176/285 [03:06<01:40,  1.09it/s]Loading train:  62%|██████▏   | 177/285 [03:07<01:40,  1.07it/s]Loading train:  62%|██████▏   | 178/285 [03:08<01:40,  1.06it/s]Loading train:  63%|██████▎   | 179/285 [03:09<01:34,  1.13it/s]Loading train:  63%|██████▎   | 180/285 [03:10<01:32,  1.14it/s]Loading train:  64%|██████▎   | 181/285 [03:11<01:33,  1.11it/s]Loading train:  64%|██████▍   | 182/285 [03:12<01:33,  1.10it/s]Loading train:  64%|██████▍   | 183/285 [03:13<01:33,  1.10it/s]Loading train:  65%|██████▍   | 184/285 [03:14<01:34,  1.07it/s]Loading train:  65%|██████▍   | 185/285 [03:15<01:31,  1.09it/s]Loading train:  65%|██████▌   | 186/285 [03:16<01:29,  1.10it/s]Loading train:  66%|██████▌   | 187/285 [03:16<01:29,  1.10it/s]Loading train:  66%|██████▌   | 188/285 [03:17<01:31,  1.06it/s]Loading train:  66%|██████▋   | 189/285 [03:18<01:28,  1.09it/s]Loading train:  67%|██████▋   | 190/285 [03:19<01:24,  1.12it/s]Loading train:  67%|██████▋   | 191/285 [03:20<01:22,  1.14it/s]Loading train:  67%|██████▋   | 192/285 [03:21<01:23,  1.12it/s]Loading train:  68%|██████▊   | 193/285 [03:22<01:20,  1.14it/s]Loading train:  68%|██████▊   | 194/285 [03:23<01:18,  1.15it/s]Loading train:  68%|██████▊   | 195/285 [03:23<01:15,  1.19it/s]Loading train:  69%|██████▉   | 196/285 [03:24<01:19,  1.12it/s]Loading train:  69%|██████▉   | 197/285 [03:25<01:21,  1.09it/s]Loading train:  69%|██████▉   | 198/285 [03:26<01:18,  1.11it/s]Loading train:  70%|██████▉   | 199/285 [03:27<01:16,  1.12it/s]Loading train:  70%|███████   | 200/285 [03:28<01:15,  1.13it/s]Loading train:  71%|███████   | 201/285 [03:29<01:15,  1.12it/s]Loading train:  71%|███████   | 202/285 [03:30<01:13,  1.13it/s]Loading train:  71%|███████   | 203/285 [03:31<01:14,  1.10it/s]Loading train:  72%|███████▏  | 204/285 [03:32<01:12,  1.11it/s]Loading train:  72%|███████▏  | 205/285 [03:33<01:12,  1.10it/s]Loading train:  72%|███████▏  | 206/285 [03:33<01:12,  1.09it/s]Loading train:  73%|███████▎  | 207/285 [03:34<01:12,  1.08it/s]Loading train:  73%|███████▎  | 208/285 [03:35<01:13,  1.05it/s]Loading train:  73%|███████▎  | 209/285 [03:36<01:11,  1.06it/s]Loading train:  74%|███████▎  | 210/285 [03:37<01:11,  1.05it/s]Loading train:  74%|███████▍  | 211/285 [03:38<01:09,  1.06it/s]Loading train:  74%|███████▍  | 212/285 [03:39<01:10,  1.04it/s]Loading train:  75%|███████▍  | 213/285 [03:40<01:10,  1.02it/s]Loading train:  75%|███████▌  | 214/285 [03:41<01:07,  1.06it/s]Loading train:  75%|███████▌  | 215/285 [03:42<01:03,  1.10it/s]Loading train:  76%|███████▌  | 216/285 [03:43<01:02,  1.10it/s]Loading train:  76%|███████▌  | 217/285 [03:44<01:00,  1.12it/s]Loading train:  76%|███████▋  | 218/285 [03:45<01:00,  1.10it/s]Loading train:  77%|███████▋  | 219/285 [03:46<00:59,  1.12it/s]Loading train:  77%|███████▋  | 220/285 [03:46<00:57,  1.14it/s]Loading train:  78%|███████▊  | 221/285 [03:47<00:55,  1.15it/s]Loading train:  78%|███████▊  | 222/285 [03:48<00:55,  1.14it/s]Loading train:  78%|███████▊  | 223/285 [03:49<00:53,  1.17it/s]Loading train:  79%|███████▊  | 224/285 [03:50<00:52,  1.16it/s]Loading train:  79%|███████▉  | 225/285 [03:51<00:51,  1.17it/s]Loading train:  79%|███████▉  | 226/285 [03:51<00:49,  1.18it/s]Loading train:  80%|███████▉  | 227/285 [03:52<00:50,  1.15it/s]Loading train:  80%|████████  | 228/285 [03:53<00:48,  1.17it/s]Loading train:  80%|████████  | 229/285 [03:54<00:47,  1.17it/s]Loading train:  81%|████████  | 230/285 [03:55<00:45,  1.20it/s]Loading train:  81%|████████  | 231/285 [03:56<00:44,  1.22it/s]Loading train:  81%|████████▏ | 232/285 [03:57<00:45,  1.16it/s]Loading train:  82%|████████▏ | 233/285 [03:58<00:48,  1.06it/s]Loading train:  82%|████████▏ | 234/285 [03:59<00:49,  1.04it/s]Loading train:  82%|████████▏ | 235/285 [04:00<00:48,  1.03it/s]Loading train:  83%|████████▎ | 236/285 [04:01<00:48,  1.01it/s]Loading train:  83%|████████▎ | 237/285 [04:02<00:47,  1.00it/s]Loading train:  84%|████████▎ | 238/285 [04:03<00:47,  1.00s/it]Loading train:  84%|████████▍ | 239/285 [04:04<00:47,  1.02s/it]Loading train:  84%|████████▍ | 240/285 [04:05<00:45,  1.01s/it]Loading train:  85%|████████▍ | 241/285 [04:06<00:44,  1.02s/it]Loading train:  85%|████████▍ | 242/285 [04:07<00:44,  1.02s/it]Loading train:  85%|████████▌ | 243/285 [04:08<00:42,  1.02s/it]Loading train:  86%|████████▌ | 244/285 [04:09<00:41,  1.01s/it]Loading train:  86%|████████▌ | 245/285 [04:10<00:40,  1.01s/it]Loading train:  86%|████████▋ | 246/285 [04:11<00:40,  1.03s/it]Loading train:  87%|████████▋ | 247/285 [04:12<00:39,  1.03s/it]Loading train:  87%|████████▋ | 248/285 [04:13<00:36,  1.00it/s]Loading train:  87%|████████▋ | 249/285 [04:14<00:36,  1.01s/it]Loading train:  88%|████████▊ | 250/285 [04:15<00:34,  1.03it/s]Loading train:  88%|████████▊ | 251/285 [04:16<00:32,  1.06it/s]Loading train:  88%|████████▊ | 252/285 [04:17<00:30,  1.07it/s]Loading train:  89%|████████▉ | 253/285 [04:18<00:29,  1.07it/s]Loading train:  89%|████████▉ | 254/285 [04:18<00:28,  1.10it/s]Loading train:  89%|████████▉ | 255/285 [04:19<00:26,  1.13it/s]Loading train:  90%|████████▉ | 256/285 [04:20<00:25,  1.14it/s]Loading train:  90%|█████████ | 257/285 [04:21<00:25,  1.12it/s]Loading train:  91%|█████████ | 258/285 [04:22<00:23,  1.13it/s]Loading train:  91%|█████████ | 259/285 [04:23<00:22,  1.14it/s]Loading train:  91%|█████████ | 260/285 [04:24<00:21,  1.14it/s]Loading train:  92%|█████████▏| 261/285 [04:25<00:21,  1.14it/s]Loading train:  92%|█████████▏| 262/285 [04:25<00:20,  1.14it/s]Loading train:  92%|█████████▏| 263/285 [04:26<00:18,  1.16it/s]Loading train:  93%|█████████▎| 264/285 [04:27<00:17,  1.17it/s]Loading train:  93%|█████████▎| 265/285 [04:28<00:16,  1.19it/s]Loading train:  93%|█████████▎| 266/285 [04:29<00:15,  1.19it/s]Loading train:  94%|█████████▎| 267/285 [04:30<00:15,  1.16it/s]Loading train:  94%|█████████▍| 268/285 [04:31<00:16,  1.05it/s]Loading train:  94%|█████████▍| 269/285 [04:32<00:16,  1.02s/it]Loading train:  95%|█████████▍| 270/285 [04:33<00:15,  1.03s/it]Loading train:  95%|█████████▌| 271/285 [04:34<00:14,  1.06s/it]Loading train:  95%|█████████▌| 272/285 [04:35<00:14,  1.08s/it]Loading train:  96%|█████████▌| 273/285 [04:36<00:13,  1.09s/it]Loading train:  96%|█████████▌| 274/285 [04:38<00:11,  1.09s/it]Loading train:  96%|█████████▋| 275/285 [04:39<00:10,  1.09s/it]Loading train:  97%|█████████▋| 276/285 [04:40<00:09,  1.09s/it]Loading train:  97%|█████████▋| 277/285 [04:41<00:08,  1.09s/it]Loading train:  98%|█████████▊| 278/285 [04:42<00:07,  1.08s/it]Loading train:  98%|█████████▊| 279/285 [04:43<00:06,  1.10s/it]Loading train:  98%|█████████▊| 280/285 [04:44<00:05,  1.09s/it]Loading train:  99%|█████████▊| 281/285 [04:45<00:04,  1.07s/it]Loading train:  99%|█████████▉| 282/285 [04:46<00:03,  1.06s/it]Loading train:  99%|█████████▉| 283/285 [04:47<00:02,  1.11s/it]Loading train: 100%|█████████▉| 284/285 [04:48<00:01,  1.10s/it]Loading train: 100%|██████████| 285/285 [04:50<00:00,  1.10s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|▏         | 4/285 [00:00<00:09, 30.57it/s]concatenating: train:   4%|▎         | 10/285 [00:00<00:07, 35.65it/s]concatenating: train:  12%|█▏        | 34/285 [00:00<00:05, 47.87it/s]concatenating: train:  21%|██▏       | 61/285 [00:00<00:03, 63.50it/s]concatenating: train:  29%|██▉       | 84/285 [00:00<00:02, 80.84it/s]concatenating: train:  40%|███▉      | 113/285 [00:00<00:01, 102.96it/s]concatenating: train:  50%|█████     | 143/285 [00:00<00:01, 127.89it/s]concatenating: train:  61%|██████    | 173/285 [00:00<00:00, 153.74it/s]concatenating: train:  71%|███████   | 203/285 [00:00<00:00, 179.50it/s]concatenating: train:  82%|████████▏ | 234/285 [00:01<00:00, 204.51it/s]concatenating: train:  92%|█████████▏| 263/285 [00:01<00:00, 223.26it/s]concatenating: train: 100%|██████████| 285/285 [00:01<00:00, 231.97it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.32s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.30s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.28s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 74.49it/s]mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a’: File exists
mkdir: cannot create directory ‘/array/ssd/msmajdi/experiments/keras/exp6/results/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a’: File exists

Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<05:54,  1.25s/it]Loading train:   1%|          | 2/285 [00:02<06:03,  1.28s/it]Loading train:   1%|          | 3/285 [00:03<06:01,  1.28s/it]Loading train:   1%|▏         | 4/285 [00:05<06:30,  1.39s/it]Loading train:   2%|▏         | 5/285 [00:06<06:04,  1.30s/it]Loading train:   2%|▏         | 6/285 [00:08<06:24,  1.38s/it]Loading train:   2%|▏         | 7/285 [00:09<06:47,  1.47s/it]Loading train:   3%|▎         | 8/285 [00:11<06:52,  1.49s/it]Loading train:   3%|▎         | 9/285 [00:12<06:32,  1.42s/it]Loading train:   4%|▎         | 10/285 [00:13<05:54,  1.29s/it]Loading train:   4%|▍         | 11/285 [00:14<05:29,  1.20s/it]Loading train:   4%|▍         | 12/285 [00:15<05:05,  1.12s/it]Loading train:   5%|▍         | 13/285 [00:16<04:57,  1.09s/it]Loading train:   5%|▍         | 14/285 [00:17<04:50,  1.07s/it]Loading train:   5%|▌         | 15/285 [00:18<04:43,  1.05s/it]Loading train:   6%|▌         | 16/285 [00:19<04:30,  1.00s/it]Loading train:   6%|▌         | 17/285 [00:20<04:25,  1.01it/s]Loading train:   6%|▋         | 18/285 [00:21<04:23,  1.01it/s]Loading train:   7%|▋         | 19/285 [00:22<04:19,  1.03it/s]Loading train:   7%|▋         | 20/285 [00:23<04:16,  1.03it/s]Loading train:   7%|▋         | 21/285 [00:24<04:17,  1.02it/s]Loading train:   8%|▊         | 22/285 [00:25<04:18,  1.02it/s]Loading train:   8%|▊         | 23/285 [00:26<04:13,  1.03it/s]Loading train:   8%|▊         | 24/285 [00:27<04:13,  1.03it/s]Loading train:   9%|▉         | 25/285 [00:28<04:12,  1.03it/s]Loading train:   9%|▉         | 26/285 [00:29<04:19,  1.00s/it]Loading train:   9%|▉         | 27/285 [00:30<04:18,  1.00s/it]Loading train:  10%|▉         | 28/285 [00:31<04:23,  1.02s/it]Loading train:  10%|█         | 29/285 [00:32<04:16,  1.00s/it]Loading train:  11%|█         | 30/285 [00:33<04:11,  1.02it/s]Loading train:  11%|█         | 31/285 [00:34<04:10,  1.02it/s]Loading train:  11%|█         | 32/285 [00:35<04:13,  1.00s/it]Loading train:  12%|█▏        | 33/285 [00:36<04:11,  1.00it/s]Loading train:  12%|█▏        | 34/285 [00:37<04:09,  1.01it/s]Loading train:  12%|█▏        | 35/285 [00:38<04:08,  1.01it/s]Loading train:  13%|█▎        | 36/285 [00:39<04:03,  1.02it/s]Loading train:  13%|█▎        | 37/285 [00:40<03:56,  1.05it/s]Loading train:  13%|█▎        | 38/285 [00:41<03:57,  1.04it/s]Loading train:  14%|█▎        | 39/285 [00:42<03:59,  1.03it/s]Loading train:  14%|█▍        | 40/285 [00:43<04:04,  1.00it/s]Loading train:  14%|█▍        | 41/285 [00:44<04:07,  1.01s/it]Loading train:  15%|█▍        | 42/285 [00:45<04:06,  1.02s/it]Loading train:  15%|█▌        | 43/285 [00:46<04:04,  1.01s/it]Loading train:  15%|█▌        | 44/285 [00:47<04:09,  1.03s/it]Loading train:  16%|█▌        | 45/285 [00:48<04:00,  1.00s/it]Loading train:  16%|█▌        | 46/285 [00:49<03:48,  1.05it/s]Loading train:  16%|█▋        | 47/285 [00:49<03:36,  1.10it/s]Loading train:  17%|█▋        | 48/285 [00:50<03:24,  1.16it/s]Loading train:  17%|█▋        | 49/285 [00:51<03:16,  1.20it/s]Loading train:  18%|█▊        | 50/285 [00:52<03:18,  1.18it/s]Loading train:  18%|█▊        | 51/285 [00:53<03:15,  1.19it/s]Loading train:  18%|█▊        | 52/285 [00:53<03:08,  1.23it/s]Loading train:  19%|█▊        | 53/285 [00:54<03:08,  1.23it/s]Loading train:  19%|█▉        | 54/285 [00:55<03:06,  1.24it/s]Loading train:  19%|█▉        | 55/285 [00:56<03:01,  1.26it/s]Loading train:  20%|█▉        | 56/285 [00:57<03:03,  1.25it/s]Loading train:  20%|██        | 57/285 [00:57<02:57,  1.28it/s]Loading train:  20%|██        | 58/285 [00:58<02:56,  1.29it/s]Loading train:  21%|██        | 59/285 [00:59<02:51,  1.32it/s]Loading train:  21%|██        | 60/285 [00:59<02:47,  1.34it/s]Loading train:  21%|██▏       | 61/285 [01:00<02:49,  1.33it/s]Loading train:  22%|██▏       | 62/285 [01:01<02:52,  1.30it/s]Loading train:  22%|██▏       | 63/285 [01:02<02:52,  1.29it/s]Loading train:  22%|██▏       | 64/285 [01:03<03:32,  1.04it/s]Loading train:  23%|██▎       | 65/285 [01:05<04:08,  1.13s/it]Loading train:  23%|██▎       | 66/285 [01:06<04:17,  1.17s/it]Loading train:  24%|██▎       | 67/285 [01:07<03:54,  1.07s/it]Loading train:  24%|██▍       | 68/285 [01:08<03:38,  1.01s/it]Loading train:  24%|██▍       | 69/285 [01:08<03:20,  1.07it/s]Loading train:  25%|██▍       | 70/285 [01:09<03:15,  1.10it/s]Loading train:  25%|██▍       | 71/285 [01:10<03:11,  1.12it/s]Loading train:  25%|██▌       | 72/285 [01:11<03:06,  1.14it/s]Loading train:  26%|██▌       | 73/285 [01:12<03:00,  1.17it/s]Loading train:  26%|██▌       | 74/285 [01:13<02:59,  1.17it/s]Loading train:  26%|██▋       | 75/285 [01:13<02:55,  1.20it/s]Loading train:  27%|██▋       | 76/285 [01:14<02:57,  1.18it/s]Loading train:  27%|██▋       | 77/285 [01:15<03:00,  1.15it/s]Loading train:  27%|██▋       | 78/285 [01:16<02:59,  1.16it/s]Loading train:  28%|██▊       | 79/285 [01:17<02:54,  1.18it/s]Loading train:  28%|██▊       | 80/285 [01:18<02:51,  1.20it/s]Loading train:  28%|██▊       | 81/285 [01:19<02:53,  1.18it/s]Loading train:  29%|██▉       | 82/285 [01:19<02:51,  1.18it/s]Loading train:  29%|██▉       | 83/285 [01:20<02:48,  1.20it/s]Loading train:  29%|██▉       | 84/285 [01:21<02:49,  1.18it/s]Loading train:  30%|██▉       | 85/285 [01:22<03:00,  1.11it/s]Loading train:  30%|███       | 86/285 [01:23<03:02,  1.09it/s]Loading train:  31%|███       | 87/285 [01:24<03:04,  1.07it/s]Loading train:  31%|███       | 88/285 [01:25<03:05,  1.06it/s]Loading train:  31%|███       | 89/285 [01:26<03:04,  1.06it/s]Loading train:  32%|███▏      | 90/285 [01:27<03:06,  1.04it/s]Loading train:  32%|███▏      | 91/285 [01:28<03:06,  1.04it/s]Loading train:  32%|███▏      | 92/285 [01:29<03:06,  1.03it/s]Loading train:  33%|███▎      | 93/285 [01:30<03:11,  1.00it/s]Loading train:  33%|███▎      | 94/285 [01:31<03:07,  1.02it/s]Loading train:  33%|███▎      | 95/285 [01:32<03:08,  1.01it/s]Loading train:  34%|███▎      | 96/285 [01:33<03:06,  1.01it/s]Loading train:  34%|███▍      | 97/285 [01:34<03:02,  1.03it/s]Loading train:  34%|███▍      | 98/285 [01:35<03:04,  1.02it/s]Loading train:  35%|███▍      | 99/285 [01:36<03:07,  1.01s/it]Loading train:  35%|███▌      | 100/285 [01:37<03:04,  1.01it/s]Loading train:  35%|███▌      | 101/285 [01:38<03:03,  1.00it/s]Loading train:  36%|███▌      | 102/285 [01:39<03:02,  1.00it/s]Loading train:  36%|███▌      | 103/285 [01:40<02:56,  1.03it/s]Loading train:  36%|███▋      | 104/285 [01:41<02:54,  1.04it/s]Loading train:  37%|███▋      | 105/285 [01:42<02:48,  1.07it/s]Loading train:  37%|███▋      | 106/285 [01:43<02:44,  1.09it/s]Loading train:  38%|███▊      | 107/285 [01:43<02:45,  1.08it/s]Loading train:  38%|███▊      | 108/285 [01:44<02:37,  1.12it/s]Loading train:  38%|███▊      | 109/285 [01:45<02:33,  1.15it/s]Loading train:  39%|███▊      | 110/285 [01:46<02:33,  1.14it/s]Loading train:  39%|███▉      | 111/285 [01:47<02:30,  1.16it/s]Loading train:  39%|███▉      | 112/285 [01:48<02:32,  1.13it/s]Loading train:  40%|███▉      | 113/285 [01:49<02:36,  1.10it/s]Loading train:  40%|████      | 114/285 [01:50<02:35,  1.10it/s]Loading train:  40%|████      | 115/285 [01:51<02:32,  1.12it/s]Loading train:  41%|████      | 116/285 [01:51<02:31,  1.12it/s]Loading train:  41%|████      | 117/285 [01:52<02:30,  1.12it/s]Loading train:  41%|████▏     | 118/285 [01:53<02:29,  1.12it/s]Loading train:  42%|████▏     | 119/285 [01:54<02:29,  1.11it/s]Loading train:  42%|████▏     | 120/285 [01:55<02:30,  1.10it/s]Loading train:  42%|████▏     | 121/285 [01:56<02:45,  1.01s/it]Loading train:  43%|████▎     | 122/285 [01:57<02:52,  1.06s/it]Loading train:  43%|████▎     | 123/285 [01:59<02:55,  1.08s/it]Loading train:  44%|████▎     | 124/285 [01:59<02:42,  1.01s/it]Loading train:  44%|████▍     | 125/285 [02:00<02:33,  1.04it/s]Loading train:  44%|████▍     | 126/285 [02:01<02:30,  1.06it/s]Loading train:  45%|████▍     | 127/285 [02:02<02:23,  1.10it/s]Loading train:  45%|████▍     | 128/285 [02:03<02:15,  1.16it/s]Loading train:  45%|████▌     | 129/285 [02:04<02:10,  1.20it/s]Loading train:  46%|████▌     | 130/285 [02:04<02:07,  1.21it/s]Loading train:  46%|████▌     | 131/285 [02:05<02:04,  1.24it/s]Loading train:  46%|████▋     | 132/285 [02:06<02:01,  1.26it/s]Loading train:  47%|████▋     | 133/285 [02:07<02:01,  1.25it/s]Loading train:  47%|████▋     | 134/285 [02:07<02:01,  1.24it/s]Loading train:  47%|████▋     | 135/285 [02:08<02:06,  1.19it/s]Loading train:  48%|████▊     | 136/285 [02:09<02:01,  1.23it/s]Loading train:  48%|████▊     | 137/285 [02:10<01:55,  1.28it/s]Loading train:  48%|████▊     | 138/285 [02:11<01:55,  1.28it/s]Loading train:  49%|████▉     | 139/285 [02:11<01:56,  1.25it/s]Loading train:  49%|████▉     | 140/285 [02:12<01:58,  1.23it/s]Loading train:  49%|████▉     | 141/285 [02:13<01:54,  1.26it/s]Loading train:  50%|████▉     | 142/285 [02:14<01:55,  1.23it/s]Loading train:  50%|█████     | 143/285 [02:15<01:55,  1.23it/s]Loading train:  51%|█████     | 144/285 [02:15<01:48,  1.30it/s]Loading train:  51%|█████     | 145/285 [02:16<01:46,  1.32it/s]Loading train:  51%|█████     | 146/285 [02:17<01:45,  1.32it/s]Loading train:  52%|█████▏    | 147/285 [02:18<01:46,  1.30it/s]Loading train:  52%|█████▏    | 148/285 [02:19<01:47,  1.28it/s]Loading train:  52%|█████▏    | 149/285 [02:19<01:46,  1.28it/s]Loading train:  53%|█████▎    | 150/285 [02:20<01:44,  1.29it/s]Loading train:  53%|█████▎    | 151/285 [02:21<01:41,  1.32it/s]Loading train:  53%|█████▎    | 152/285 [02:21<01:36,  1.38it/s]Loading train:  54%|█████▎    | 153/285 [02:22<01:33,  1.41it/s]Loading train:  54%|█████▍    | 154/285 [02:23<01:33,  1.40it/s]Loading train:  54%|█████▍    | 155/285 [02:23<01:30,  1.44it/s]Loading train:  55%|█████▍    | 156/285 [02:24<01:29,  1.45it/s]Loading train:  55%|█████▌    | 157/285 [02:25<01:28,  1.44it/s]Loading train:  55%|█████▌    | 158/285 [02:26<01:31,  1.39it/s]Loading train:  56%|█████▌    | 159/285 [02:26<01:28,  1.42it/s]Loading train:  56%|█████▌    | 160/285 [02:27<01:32,  1.35it/s]Loading train:  56%|█████▋    | 161/285 [02:28<01:33,  1.33it/s]Loading train:  57%|█████▋    | 162/285 [02:29<01:31,  1.34it/s]Loading train:  57%|█████▋    | 163/285 [02:29<01:33,  1.30it/s]Loading train:  58%|█████▊    | 164/285 [02:30<01:32,  1.31it/s]Loading train:  58%|█████▊    | 165/285 [02:31<01:29,  1.34it/s]Loading train:  58%|█████▊    | 166/285 [02:32<01:28,  1.35it/s]Loading train:  59%|█████▊    | 167/285 [02:32<01:29,  1.32it/s]Loading train:  59%|█████▉    | 168/285 [02:33<01:28,  1.32it/s]Loading train:  59%|█████▉    | 169/285 [02:34<01:25,  1.36it/s]Loading train:  60%|█████▉    | 170/285 [02:35<01:24,  1.36it/s]Loading train:  60%|██████    | 171/285 [02:35<01:25,  1.34it/s]Loading train:  60%|██████    | 172/285 [02:36<01:19,  1.42it/s]Loading train:  61%|██████    | 173/285 [02:37<01:17,  1.44it/s]Loading train:  61%|██████    | 174/285 [02:37<01:14,  1.48it/s]Loading train:  61%|██████▏   | 175/285 [02:38<01:15,  1.45it/s]Loading train:  62%|██████▏   | 176/285 [02:39<01:16,  1.43it/s]Loading train:  62%|██████▏   | 177/285 [02:39<01:16,  1.42it/s]Loading train:  62%|██████▏   | 178/285 [02:40<01:18,  1.37it/s]Loading train:  63%|██████▎   | 179/285 [02:41<01:16,  1.38it/s]Loading train:  63%|██████▎   | 180/285 [02:42<01:14,  1.41it/s]Loading train:  64%|██████▎   | 181/285 [02:42<01:17,  1.34it/s]Loading train:  64%|██████▍   | 182/285 [02:43<01:18,  1.32it/s]Loading train:  64%|██████▍   | 183/285 [02:44<01:16,  1.33it/s]Loading train:  65%|██████▍   | 184/285 [02:45<01:13,  1.38it/s]Loading train:  65%|██████▍   | 185/285 [02:45<01:10,  1.42it/s]Loading train:  65%|██████▌   | 186/285 [02:46<01:09,  1.43it/s]Loading train:  66%|██████▌   | 187/285 [02:47<01:08,  1.43it/s]Loading train:  66%|██████▌   | 188/285 [02:47<01:04,  1.49it/s]Loading train:  66%|██████▋   | 189/285 [02:48<01:05,  1.46it/s]Loading train:  67%|██████▋   | 190/285 [02:49<01:03,  1.49it/s]Loading train:  67%|██████▋   | 191/285 [02:49<01:01,  1.52it/s]Loading train:  67%|██████▋   | 192/285 [02:50<00:59,  1.56it/s]Loading train:  68%|██████▊   | 193/285 [02:51<01:01,  1.49it/s]Loading train:  68%|██████▊   | 194/285 [02:51<01:03,  1.43it/s]Loading train:  68%|██████▊   | 195/285 [02:52<01:04,  1.40it/s]Loading train:  69%|██████▉   | 196/285 [02:53<01:06,  1.33it/s]Loading train:  69%|██████▉   | 197/285 [02:54<01:08,  1.29it/s]Loading train:  69%|██████▉   | 198/285 [02:55<01:07,  1.28it/s]Loading train:  70%|██████▉   | 199/285 [02:55<01:05,  1.32it/s]Loading train:  70%|███████   | 200/285 [02:56<01:04,  1.31it/s]Loading train:  71%|███████   | 201/285 [02:57<01:04,  1.29it/s]Loading train:  71%|███████   | 202/285 [02:58<01:05,  1.26it/s]Loading train:  71%|███████   | 203/285 [02:59<01:08,  1.20it/s]Loading train:  72%|███████▏  | 204/285 [02:59<01:07,  1.21it/s]Loading train:  72%|███████▏  | 205/285 [03:00<01:04,  1.25it/s]Loading train:  72%|███████▏  | 206/285 [03:01<01:03,  1.24it/s]Loading train:  73%|███████▎  | 207/285 [03:02<01:03,  1.23it/s]Loading train:  73%|███████▎  | 208/285 [03:03<01:03,  1.20it/s]Loading train:  73%|███████▎  | 209/285 [03:04<01:04,  1.19it/s]Loading train:  74%|███████▎  | 210/285 [03:04<01:04,  1.17it/s]Loading train:  74%|███████▍  | 211/285 [03:05<01:03,  1.17it/s]Loading train:  74%|███████▍  | 212/285 [03:06<01:04,  1.13it/s]Loading train:  75%|███████▍  | 213/285 [03:07<01:05,  1.10it/s]Loading train:  75%|███████▌  | 214/285 [03:08<01:02,  1.13it/s]Loading train:  75%|███████▌  | 215/285 [03:09<00:59,  1.17it/s]Loading train:  76%|███████▌  | 216/285 [03:10<00:57,  1.21it/s]Loading train:  76%|███████▌  | 217/285 [03:10<00:54,  1.25it/s]Loading train:  76%|███████▋  | 218/285 [03:11<00:51,  1.29it/s]Loading train:  77%|███████▋  | 219/285 [03:12<00:50,  1.29it/s]Loading train:  77%|███████▋  | 220/285 [03:13<00:51,  1.27it/s]Loading train:  78%|███████▊  | 221/285 [03:14<00:51,  1.25it/s]Loading train:  78%|███████▊  | 222/285 [03:14<00:49,  1.28it/s]Loading train:  78%|███████▊  | 223/285 [03:15<00:50,  1.23it/s]Loading train:  79%|███████▊  | 224/285 [03:16<00:47,  1.27it/s]Loading train:  79%|███████▉  | 225/285 [03:17<00:46,  1.30it/s]Loading train:  79%|███████▉  | 226/285 [03:17<00:46,  1.26it/s]Loading train:  80%|███████▉  | 227/285 [03:18<00:44,  1.30it/s]Loading train:  80%|████████  | 228/285 [03:19<00:42,  1.35it/s]Loading train:  80%|████████  | 229/285 [03:20<00:41,  1.34it/s]Loading train:  81%|████████  | 230/285 [03:20<00:40,  1.35it/s]Loading train:  81%|████████  | 231/285 [03:21<00:38,  1.41it/s]Loading train:  81%|████████▏ | 232/285 [03:22<00:41,  1.28it/s]Loading train:  82%|████████▏ | 233/285 [03:23<00:43,  1.19it/s]Loading train:  82%|████████▏ | 234/285 [03:24<00:46,  1.11it/s]Loading train:  82%|████████▏ | 235/285 [03:25<00:46,  1.08it/s]Loading train:  83%|████████▎ | 236/285 [03:26<00:45,  1.07it/s]Loading train:  83%|████████▎ | 237/285 [03:27<00:44,  1.07it/s]Loading train:  84%|████████▎ | 238/285 [03:28<00:43,  1.07it/s]Loading train:  84%|████████▍ | 239/285 [03:29<00:42,  1.07it/s]Loading train:  84%|████████▍ | 240/285 [03:30<00:42,  1.07it/s]Loading train:  85%|████████▍ | 241/285 [03:30<00:40,  1.09it/s]Loading train:  85%|████████▍ | 242/285 [03:32<00:40,  1.05it/s]Loading train:  85%|████████▌ | 243/285 [03:33<00:41,  1.02it/s]Loading train:  86%|████████▌ | 244/285 [03:34<00:40,  1.02it/s]Loading train:  86%|████████▌ | 245/285 [03:35<00:38,  1.03it/s]Loading train:  86%|████████▋ | 246/285 [03:35<00:38,  1.02it/s]Loading train:  87%|████████▋ | 247/285 [03:36<00:36,  1.03it/s]Loading train:  87%|████████▋ | 248/285 [03:37<00:35,  1.05it/s]Loading train:  87%|████████▋ | 249/285 [03:38<00:33,  1.07it/s]Loading train:  88%|████████▊ | 250/285 [03:39<00:29,  1.18it/s]Loading train:  88%|████████▊ | 251/285 [03:39<00:26,  1.29it/s]Loading train:  88%|████████▊ | 252/285 [03:40<00:24,  1.35it/s]Loading train:  89%|████████▉ | 253/285 [03:41<00:22,  1.39it/s]Loading train:  89%|████████▉ | 254/285 [03:41<00:21,  1.42it/s]Loading train:  89%|████████▉ | 255/285 [03:42<00:20,  1.44it/s]Loading train:  90%|████████▉ | 256/285 [03:43<00:20,  1.44it/s]Loading train:  90%|█████████ | 257/285 [03:44<00:19,  1.43it/s]Loading train:  91%|█████████ | 258/285 [03:44<00:19,  1.40it/s]Loading train:  91%|█████████ | 259/285 [03:45<00:19,  1.36it/s]Loading train:  91%|█████████ | 260/285 [03:46<00:18,  1.33it/s]Loading train:  92%|█████████▏| 261/285 [03:47<00:17,  1.35it/s]Loading train:  92%|█████████▏| 262/285 [03:47<00:16,  1.41it/s]Loading train:  92%|█████████▏| 263/285 [03:48<00:16,  1.37it/s]Loading train:  93%|█████████▎| 264/285 [03:49<00:15,  1.35it/s]Loading train:  93%|█████████▎| 265/285 [03:49<00:14,  1.39it/s]Loading train:  93%|█████████▎| 266/285 [03:50<00:14,  1.35it/s]Loading train:  94%|█████████▎| 267/285 [03:51<00:13,  1.31it/s]Loading train:  94%|█████████▍| 268/285 [03:52<00:14,  1.15it/s]Loading train:  94%|█████████▍| 269/285 [03:53<00:14,  1.13it/s]Loading train:  95%|█████████▍| 270/285 [03:54<00:13,  1.10it/s]Loading train:  95%|█████████▌| 271/285 [03:55<00:12,  1.08it/s]Loading train:  95%|█████████▌| 272/285 [03:56<00:12,  1.07it/s]Loading train:  96%|█████████▌| 273/285 [03:57<00:11,  1.06it/s]Loading train:  96%|█████████▌| 274/285 [03:58<00:10,  1.03it/s]Loading train:  96%|█████████▋| 275/285 [03:59<00:09,  1.05it/s]Loading train:  97%|█████████▋| 276/285 [04:00<00:08,  1.01it/s]Loading train:  97%|█████████▋| 277/285 [04:01<00:07,  1.02it/s]Loading train:  98%|█████████▊| 278/285 [04:02<00:06,  1.04it/s]Loading train:  98%|█████████▊| 279/285 [04:03<00:05,  1.08it/s]Loading train:  98%|█████████▊| 280/285 [04:04<00:04,  1.07it/s]Loading train:  99%|█████████▊| 281/285 [04:05<00:03,  1.05it/s]Loading train:  99%|█████████▉| 282/285 [04:06<00:02,  1.03it/s]Loading train:  99%|█████████▉| 283/285 [04:07<00:02,  1.01s/it]Loading train: 100%|█████████▉| 284/285 [04:08<00:01,  1.02s/it]Loading train: 100%|██████████| 285/285 [04:09<00:00,  1.01it/s]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:  10%|▉         | 28/285 [00:00<00:00, 276.84it/s]concatenating: train:  21%|██        | 59/285 [00:00<00:00, 282.34it/s]concatenating: train:  32%|███▏      | 91/285 [00:00<00:00, 291.20it/s]concatenating: train:  44%|████▎     | 124/285 [00:00<00:00, 299.60it/s]concatenating: train:  55%|█████▌    | 158/285 [00:00<00:00, 308.35it/s]concatenating: train:  68%|██████▊   | 195/285 [00:00<00:00, 323.56it/s]concatenating: train:  81%|████████  | 231/285 [00:00<00:00, 332.83it/s]concatenating: train:  93%|█████████▎| 266/285 [00:00<00:00, 336.52it/s]concatenating: train: 100%|██████████| 285/285 [00:00<00:00, 326.50it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:02,  1.23s/it]Loading test:  67%|██████▋   | 2/3 [00:02<00:01,  1.26s/it]Loading test: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 65.52it/s]2019-07-05 21:27:19.608331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-07-05 21:27:19.608449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-05 21:27:19.608468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-07-05 21:27:19.608479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-07-05 21:27:19.608940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15123 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)

/array/ssd/msmajdi/anaconda3/envs/keras-gpu/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.
  warnings.warn('No training configuration found in save file: '
loading the weights for Unet:   0%|          | 0/40 [00:00<?, ?it/s]loading the weights for Unet:   2%|▎         | 1/40 [00:00<00:13,  2.79it/s]loading the weights for Unet:   8%|▊         | 3/40 [00:00<00:11,  3.27it/s]loading the weights for Unet:  10%|█         | 4/40 [00:01<00:11,  3.06it/s]loading the weights for Unet:  20%|██        | 8/40 [00:01<00:08,  3.78it/s]loading the weights for Unet:  22%|██▎       | 9/40 [00:01<00:09,  3.35it/s]loading the weights for Unet:  28%|██▊       | 11/40 [00:02<00:07,  3.81it/s]loading the weights for Unet:  30%|███       | 12/40 [00:02<00:08,  3.34it/s]loading the weights for Unet:  40%|████      | 16/40 [00:03<00:05,  4.20it/s]loading the weights for Unet:  42%|████▎     | 17/40 [00:03<00:06,  3.56it/s]loading the weights for Unet:  48%|████▊     | 19/40 [00:03<00:05,  3.77it/s]loading the weights for Unet:  50%|█████     | 20/40 [00:04<00:06,  3.24it/s]loading the weights for Unet:  57%|█████▊    | 23/40 [00:04<00:04,  3.92it/s]loading the weights for Unet:  62%|██████▎   | 25/40 [00:05<00:03,  4.21it/s]loading the weights for Unet:  65%|██████▌   | 26/40 [00:05<00:03,  3.52it/s]loading the weights for Unet:  70%|███████   | 28/40 [00:05<00:03,  3.70it/s]loading the weights for Unet:  72%|███████▎  | 29/40 [00:06<00:03,  3.17it/s]loading the weights for Unet:  80%|████████  | 32/40 [00:06<00:02,  3.90it/s]loading the weights for Unet:  85%|████████▌ | 34/40 [00:07<00:01,  4.25it/s]loading the weights for Unet:  88%|████████▊ | 35/40 [00:07<00:01,  3.58it/s]loading the weights for Unet:  92%|█████████▎| 37/40 [00:07<00:00,  3.93it/s]loading the weights for Unet:  95%|█████████▌| 38/40 [00:08<00:00,  3.40it/s]loading the weights for Unet: 100%|██████████| 40/40 [00:08<00:00,  4.83it/s]
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12, 12, 80), (None, 13, 13, 80)]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 2  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 52, 80, 1)    0                                            
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 52, 80, 50)   500         input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 52, 80, 50)   200         conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 52, 80, 50)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 52, 80, 50)   0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 52, 80, 50)   22550       dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 52, 80, 50)   200         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 52, 80, 50)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 52, 80, 50)   0           activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 52, 80, 50)   22550       dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 52, 80, 50)   200         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 52, 80, 50)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 52, 80, 50)   0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 52, 80, 20)   9020        dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 52, 80, 20)   80          conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 52, 80, 20)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 52, 80, 20)   3620        activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 52, 80, 20)   80          conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 52, 80, 20)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 26, 40, 20)   0           activation_16[0][0]              
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 26, 40, 20)   0           max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 26, 40, 40)   7240        dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 26, 40, 40)   160         conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 26, 40, 40)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 26, 40, 40)   14440       activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 26, 40, 40)   160         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 26, 40, 40)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 13, 20, 40)   0           activation_18[0][0]              
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 13, 20, 40)   0           max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 13, 20, 80)   28880       dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 13, 20, 80)   320         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 13, 20, 80)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 13, 20, 80)   57680       activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 13, 20, 80)   320         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 13, 20, 80)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 13, 20, 80)   0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 26, 40, 40)   12840       dropout_13[0][0]                 
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 26, 40, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 26, 40, 40)   28840       concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 26, 40, 40)   160         conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 26, 40, 40)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 26, 40, 40)   14440       activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 26, 40, 40)   160         conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 26, 40, 40)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 26, 40, 40)   0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 52, 80, 20)   3220        dropout_14[0][0]                 
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 52, 80, 40)   0           conv2d_transpose_3[0][0]         
                                                                 activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 52, 80, 20)   7220        concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 52, 80, 20)   80          conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 52, 80, 20)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 52, 80, 20)   3620        activation_23[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 52, 80, 20)   80          conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 52, 80, 20)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 52, 80, 20)   0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 52, 80, 13)   273         dropout_15[0][0]                 
==================================================================================================
Total params: 239,133
Trainable params: 95,353
Non-trainable params: 143,780
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.47467835e-02 3.18797950e-02 7.48227142e-02 9.29948699e-03
 2.70301111e-02 7.04843275e-03 8.49024940e-02 1.12367134e-01
 8.58192333e-02 1.32164642e-02 2.93445604e-01 1.95153089e-01
 2.68657757e-04]
Train on 10374 samples, validate on 105 samples
Epoch 1/300
 - 22s - loss: 300.9453 - acc: 0.3097 - mDice: 0.0154 - val_loss: 84.4524 - val_acc: 0.8985 - val_mDice: 0.0131

Epoch 00001: val_mDice improved from -inf to 0.01308, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 2/300
 - 14s - loss: 68.7246 - acc: 0.8059 - mDice: 0.0147 - val_loss: 23.3349 - val_acc: 0.9047 - val_mDice: 0.0132

Epoch 00002: val_mDice improved from 0.01308 to 0.01323, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 3/300
 - 13s - loss: 28.5389 - acc: 0.8612 - mDice: 0.0173 - val_loss: 12.2072 - val_acc: 0.9047 - val_mDice: 0.0158

Epoch 00003: val_mDice improved from 0.01323 to 0.01576, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 4/300
 - 13s - loss: 17.7102 - acc: 0.8675 - mDice: 0.0227 - val_loss: 8.4274 - val_acc: 0.9047 - val_mDice: 0.0132

Epoch 00004: val_mDice did not improve from 0.01576
Epoch 5/300
 - 13s - loss: 13.2880 - acc: 0.8686 - mDice: 0.0270 - val_loss: 7.3369 - val_acc: 0.9047 - val_mDice: 0.0098

Epoch 00005: val_mDice did not improve from 0.01576
Epoch 6/300
 - 13s - loss: 10.8882 - acc: 0.8688 - mDice: 0.0314 - val_loss: 7.0817 - val_acc: 0.9047 - val_mDice: 0.0078

Epoch 00006: val_mDice did not improve from 0.01576
Epoch 7/300
 - 13s - loss: 9.3251 - acc: 0.8688 - mDice: 0.0367 - val_loss: 7.0072 - val_acc: 0.9047 - val_mDice: 0.0064

Epoch 00007: val_mDice did not improve from 0.01576
Epoch 8/300
 - 13s - loss: 8.2624 - acc: 0.8688 - mDice: 0.0432 - val_loss: 6.8848 - val_acc: 0.9047 - val_mDice: 0.0068

Epoch 00008: val_mDice did not improve from 0.01576
Epoch 9/300
 - 13s - loss: 7.4878 - acc: 0.8690 - mDice: 0.0507 - val_loss: 6.7633 - val_acc: 0.9047 - val_mDice: 0.0169

Epoch 00009: val_mDice improved from 0.01576 to 0.01690, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 10/300
 - 13s - loss: 6.9041 - acc: 0.8690 - mDice: 0.0584 - val_loss: 6.1962 - val_acc: 0.9047 - val_mDice: 0.0292

Epoch 00010: val_mDice improved from 0.01690 to 0.02924, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 11/300
 - 13s - loss: 6.3715 - acc: 0.8695 - mDice: 0.0699 - val_loss: 5.9486 - val_acc: 0.9047 - val_mDice: 0.0390

Epoch 00011: val_mDice improved from 0.02924 to 0.03905, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 12/300
 - 13s - loss: 5.9313 - acc: 0.8706 - mDice: 0.0823 - val_loss: 6.2247 - val_acc: 0.9048 - val_mDice: 0.0384

Epoch 00012: val_mDice did not improve from 0.03905
Epoch 13/300
 - 13s - loss: 5.5519 - acc: 0.8719 - mDice: 0.0964 - val_loss: 4.4744 - val_acc: 0.9060 - val_mDice: 0.1047

Epoch 00013: val_mDice improved from 0.03905 to 0.10469, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 14/300
 - 13s - loss: 5.2414 - acc: 0.8729 - mDice: 0.1103 - val_loss: 5.5497 - val_acc: 0.9051 - val_mDice: 0.0674

Epoch 00014: val_mDice did not improve from 0.10469
Epoch 15/300
 - 13s - loss: 4.9464 - acc: 0.8738 - mDice: 0.1249 - val_loss: 5.5863 - val_acc: 0.9049 - val_mDice: 0.0751

Epoch 00015: val_mDice did not improve from 0.10469
Epoch 16/300
 - 13s - loss: 4.6978 - acc: 0.8750 - mDice: 0.1397 - val_loss: 4.1131 - val_acc: 0.9065 - val_mDice: 0.1474

Epoch 00016: val_mDice improved from 0.10469 to 0.14736, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 17/300
 - 13s - loss: 4.5003 - acc: 0.8761 - mDice: 0.1528 - val_loss: 3.9600 - val_acc: 0.9077 - val_mDice: 0.1619

Epoch 00017: val_mDice improved from 0.14736 to 0.16189, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 18/300
 - 13s - loss: 4.3212 - acc: 0.8772 - mDice: 0.1652 - val_loss: 4.5793 - val_acc: 0.9075 - val_mDice: 0.1479

Epoch 00018: val_mDice did not improve from 0.16189
Epoch 19/300
 - 13s - loss: 4.1691 - acc: 0.8782 - mDice: 0.1766 - val_loss: 4.5208 - val_acc: 0.9074 - val_mDice: 0.1621

Epoch 00019: val_mDice improved from 0.16189 to 0.16207, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 20/300
 - 14s - loss: 4.0116 - acc: 0.8791 - mDice: 0.1902 - val_loss: 5.4183 - val_acc: 0.9061 - val_mDice: 0.1209

Epoch 00020: val_mDice did not improve from 0.16207
Epoch 21/300
 - 13s - loss: 3.8745 - acc: 0.8802 - mDice: 0.2020 - val_loss: 4.0791 - val_acc: 0.9104 - val_mDice: 0.1873

Epoch 00021: val_mDice improved from 0.16207 to 0.18726, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 22/300
 - 13s - loss: 3.7617 - acc: 0.8808 - mDice: 0.2133 - val_loss: 4.3237 - val_acc: 0.9088 - val_mDice: 0.1928

Epoch 00022: val_mDice improved from 0.18726 to 0.19276, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 23/300
 - 13s - loss: 3.6398 - acc: 0.8820 - mDice: 0.2254 - val_loss: 3.5600 - val_acc: 0.9110 - val_mDice: 0.2299

Epoch 00023: val_mDice improved from 0.19276 to 0.22985, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 24/300
 - 14s - loss: 3.5611 - acc: 0.8831 - mDice: 0.2337 - val_loss: 3.7518 - val_acc: 0.9119 - val_mDice: 0.2335

Epoch 00024: val_mDice improved from 0.22985 to 0.23348, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 25/300
 - 13s - loss: 3.4610 - acc: 0.8845 - mDice: 0.2456 - val_loss: 3.7734 - val_acc: 0.9135 - val_mDice: 0.2430

Epoch 00025: val_mDice improved from 0.23348 to 0.24297, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 26/300
 - 13s - loss: 3.3830 - acc: 0.8856 - mDice: 0.2538 - val_loss: 3.4587 - val_acc: 0.9133 - val_mDice: 0.2642

Epoch 00026: val_mDice improved from 0.24297 to 0.26419, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 27/300
 - 13s - loss: 3.3010 - acc: 0.8874 - mDice: 0.2644 - val_loss: 3.6917 - val_acc: 0.9123 - val_mDice: 0.2550

Epoch 00027: val_mDice did not improve from 0.26419
Epoch 28/300
 - 14s - loss: 3.2226 - acc: 0.8885 - mDice: 0.2742 - val_loss: 3.9250 - val_acc: 0.9124 - val_mDice: 0.2455

Epoch 00028: val_mDice did not improve from 0.26419
Epoch 29/300
 - 14s - loss: 3.1527 - acc: 0.8901 - mDice: 0.2832 - val_loss: 3.5841 - val_acc: 0.9156 - val_mDice: 0.2697

Epoch 00029: val_mDice improved from 0.26419 to 0.26970, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 30/300
 - 13s - loss: 3.0882 - acc: 0.8913 - mDice: 0.2910 - val_loss: 3.5857 - val_acc: 0.9170 - val_mDice: 0.2838

Epoch 00030: val_mDice improved from 0.26970 to 0.28377, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 31/300
 - 14s - loss: 3.0322 - acc: 0.8926 - mDice: 0.2982 - val_loss: 3.6593 - val_acc: 0.9202 - val_mDice: 0.2839

Epoch 00031: val_mDice improved from 0.28377 to 0.28388, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 32/300
 - 13s - loss: 2.9711 - acc: 0.8939 - mDice: 0.3075 - val_loss: 3.6856 - val_acc: 0.9201 - val_mDice: 0.2833

Epoch 00032: val_mDice did not improve from 0.28388
Epoch 33/300
 - 13s - loss: 2.9185 - acc: 0.8950 - mDice: 0.3149 - val_loss: 3.2191 - val_acc: 0.9226 - val_mDice: 0.3154

Epoch 00033: val_mDice improved from 0.28388 to 0.31541, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 34/300
 - 14s - loss: 2.8693 - acc: 0.8963 - mDice: 0.3221 - val_loss: 3.2102 - val_acc: 0.9262 - val_mDice: 0.3226

Epoch 00034: val_mDice improved from 0.31541 to 0.32256, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 35/300
 - 13s - loss: 2.8287 - acc: 0.8972 - mDice: 0.3280 - val_loss: 3.2481 - val_acc: 0.9213 - val_mDice: 0.3129

Epoch 00035: val_mDice did not improve from 0.32256
Epoch 36/300
 - 13s - loss: 2.7855 - acc: 0.8980 - mDice: 0.3345 - val_loss: 2.9894 - val_acc: 0.9281 - val_mDice: 0.3423

Epoch 00036: val_mDice improved from 0.32256 to 0.34232, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 37/300
 - 14s - loss: 2.7414 - acc: 0.8990 - mDice: 0.3412 - val_loss: 3.2712 - val_acc: 0.9278 - val_mDice: 0.3286

Epoch 00037: val_mDice did not improve from 0.34232
Epoch 38/300
 - 13s - loss: 2.7081 - acc: 0.8999 - mDice: 0.3466 - val_loss: 3.0632 - val_acc: 0.9293 - val_mDice: 0.3421

Epoch 00038: val_mDice did not improve from 0.34232
Epoch 39/300
 - 13s - loss: 2.6749 - acc: 0.9010 - mDice: 0.3523 - val_loss: 3.7135 - val_acc: 0.9264 - val_mDice: 0.3108

Epoch 00039: val_mDice did not improve from 0.34232
Epoch 40/300
 - 14s - loss: 2.6396 - acc: 0.9018 - mDice: 0.3582 - val_loss: 3.3376 - val_acc: 0.9263 - val_mDice: 0.3300

Epoch 00040: val_mDice did not improve from 0.34232
Epoch 41/300
 - 13s - loss: 2.6047 - acc: 0.9026 - mDice: 0.3638 - val_loss: 3.3536 - val_acc: 0.9298 - val_mDice: 0.3348

Epoch 00041: val_mDice did not improve from 0.34232
Epoch 42/300
 - 14s - loss: 2.5705 - acc: 0.9032 - mDice: 0.3696 - val_loss: 3.3355 - val_acc: 0.9317 - val_mDice: 0.3446

Epoch 00042: val_mDice improved from 0.34232 to 0.34464, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 43/300
 - 13s - loss: 2.5441 - acc: 0.9041 - mDice: 0.3746 - val_loss: 3.3404 - val_acc: 0.9309 - val_mDice: 0.3420

Epoch 00043: val_mDice did not improve from 0.34464
Epoch 44/300
 - 13s - loss: 2.5017 - acc: 0.9050 - mDice: 0.3821 - val_loss: 3.7675 - val_acc: 0.9263 - val_mDice: 0.3240

Epoch 00044: val_mDice did not improve from 0.34464
Epoch 45/300
 - 14s - loss: 2.4857 - acc: 0.9052 - mDice: 0.3849 - val_loss: 3.4693 - val_acc: 0.9311 - val_mDice: 0.3439

Epoch 00045: val_mDice did not improve from 0.34464
Epoch 46/300
 - 14s - loss: 2.4553 - acc: 0.9062 - mDice: 0.3913 - val_loss: 3.1394 - val_acc: 0.9315 - val_mDice: 0.3596

Epoch 00046: val_mDice improved from 0.34464 to 0.35955, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 47/300
 - 13s - loss: 2.4195 - acc: 0.9071 - mDice: 0.3978 - val_loss: 3.1893 - val_acc: 0.9329 - val_mDice: 0.3626

Epoch 00047: val_mDice improved from 0.35955 to 0.36257, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 48/300
 - 13s - loss: 2.4113 - acc: 0.9074 - mDice: 0.4006 - val_loss: 2.9983 - val_acc: 0.9340 - val_mDice: 0.3774

Epoch 00048: val_mDice improved from 0.36257 to 0.37740, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 49/300
 - 14s - loss: 2.3644 - acc: 0.9086 - mDice: 0.4091 - val_loss: 3.2999 - val_acc: 0.9329 - val_mDice: 0.3638

Epoch 00049: val_mDice did not improve from 0.37740
Epoch 50/300
 - 13s - loss: 2.3484 - acc: 0.9093 - mDice: 0.4130 - val_loss: 3.1169 - val_acc: 0.9340 - val_mDice: 0.3816

Epoch 00050: val_mDice improved from 0.37740 to 0.38163, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 51/300
 - 13s - loss: 2.3273 - acc: 0.9098 - mDice: 0.4176 - val_loss: 3.1061 - val_acc: 0.9311 - val_mDice: 0.3684

Epoch 00051: val_mDice did not improve from 0.38163
Epoch 52/300
 - 13s - loss: 2.3077 - acc: 0.9103 - mDice: 0.4213 - val_loss: 3.6683 - val_acc: 0.9284 - val_mDice: 0.3430

Epoch 00052: val_mDice did not improve from 0.38163
Epoch 53/300
 - 14s - loss: 2.2766 - acc: 0.9111 - mDice: 0.4269 - val_loss: 3.0084 - val_acc: 0.9334 - val_mDice: 0.3829

Epoch 00053: val_mDice improved from 0.38163 to 0.38287, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 54/300
 - 13s - loss: 2.2540 - acc: 0.9119 - mDice: 0.4320 - val_loss: 3.0529 - val_acc: 0.9351 - val_mDice: 0.3925

Epoch 00054: val_mDice improved from 0.38287 to 0.39249, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 55/300
 - 13s - loss: 2.2278 - acc: 0.9126 - mDice: 0.4368 - val_loss: 2.9302 - val_acc: 0.9374 - val_mDice: 0.4028

Epoch 00055: val_mDice improved from 0.39249 to 0.40285, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 56/300
 - 13s - loss: 2.2101 - acc: 0.9134 - mDice: 0.4410 - val_loss: 2.9536 - val_acc: 0.9348 - val_mDice: 0.3991

Epoch 00056: val_mDice did not improve from 0.40285
Epoch 57/300
 - 13s - loss: 2.1942 - acc: 0.9138 - mDice: 0.4449 - val_loss: 2.9680 - val_acc: 0.9364 - val_mDice: 0.4042

Epoch 00057: val_mDice improved from 0.40285 to 0.40419, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 58/300
 - 14s - loss: 2.1720 - acc: 0.9147 - mDice: 0.4489 - val_loss: 3.2426 - val_acc: 0.9355 - val_mDice: 0.3916

Epoch 00058: val_mDice did not improve from 0.40419
Epoch 59/300
 - 14s - loss: 2.1590 - acc: 0.9149 - mDice: 0.4522 - val_loss: 3.0105 - val_acc: 0.9366 - val_mDice: 0.3977

Epoch 00059: val_mDice did not improve from 0.40419
Epoch 60/300
 - 13s - loss: 2.1347 - acc: 0.9157 - mDice: 0.4565 - val_loss: 3.1445 - val_acc: 0.9374 - val_mDice: 0.3976

Epoch 00060: val_mDice did not improve from 0.40419
Epoch 61/300
 - 13s - loss: 2.1296 - acc: 0.9160 - mDice: 0.4588 - val_loss: 3.1045 - val_acc: 0.9373 - val_mDice: 0.4031

Epoch 00061: val_mDice did not improve from 0.40419
Epoch 62/300
 - 13s - loss: 2.1079 - acc: 0.9168 - mDice: 0.4631 - val_loss: 3.1767 - val_acc: 0.9379 - val_mDice: 0.4024

Epoch 00062: val_mDice did not improve from 0.40419
Epoch 63/300
 - 14s - loss: 2.0796 - acc: 0.9171 - mDice: 0.4684 - val_loss: 3.5045 - val_acc: 0.9368 - val_mDice: 0.3882

Epoch 00063: val_mDice did not improve from 0.40419
Epoch 64/300
 - 14s - loss: 2.0716 - acc: 0.9176 - mDice: 0.4706 - val_loss: 3.3681 - val_acc: 0.9376 - val_mDice: 0.3956

Epoch 00064: val_mDice did not improve from 0.40419
Epoch 65/300
 - 13s - loss: 2.0588 - acc: 0.9181 - mDice: 0.4734 - val_loss: 3.5005 - val_acc: 0.9362 - val_mDice: 0.3843

Epoch 00065: val_mDice did not improve from 0.40419
Epoch 66/300
 - 14s - loss: 2.0400 - acc: 0.9185 - mDice: 0.4775 - val_loss: 3.1253 - val_acc: 0.9379 - val_mDice: 0.3976

Epoch 00066: val_mDice did not improve from 0.40419
Epoch 67/300
 - 13s - loss: 2.0272 - acc: 0.9187 - mDice: 0.4804 - val_loss: 3.1433 - val_acc: 0.9396 - val_mDice: 0.4138

Epoch 00067: val_mDice improved from 0.40419 to 0.41381, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 68/300
 - 13s - loss: 2.0255 - acc: 0.9190 - mDice: 0.4817 - val_loss: 3.1368 - val_acc: 0.9393 - val_mDice: 0.4143

Epoch 00068: val_mDice improved from 0.41381 to 0.41429, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 69/300
 - 14s - loss: 2.0030 - acc: 0.9194 - mDice: 0.4848 - val_loss: 3.1265 - val_acc: 0.9403 - val_mDice: 0.4213

Epoch 00069: val_mDice improved from 0.41429 to 0.42132, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 70/300
 - 13s - loss: 1.9808 - acc: 0.9203 - mDice: 0.4909 - val_loss: 3.1533 - val_acc: 0.9392 - val_mDice: 0.4179

Epoch 00070: val_mDice did not improve from 0.42132
Epoch 71/300
 - 13s - loss: 1.9661 - acc: 0.9206 - mDice: 0.4939 - val_loss: 3.1267 - val_acc: 0.9374 - val_mDice: 0.4151

Epoch 00071: val_mDice did not improve from 0.42132
Epoch 72/300
 - 13s - loss: 1.9562 - acc: 0.9209 - mDice: 0.4965 - val_loss: 3.1552 - val_acc: 0.9385 - val_mDice: 0.4131

Epoch 00072: val_mDice did not improve from 0.42132
Epoch 73/300
 - 13s - loss: 1.9510 - acc: 0.9211 - mDice: 0.4971 - val_loss: 3.4965 - val_acc: 0.9373 - val_mDice: 0.4000

Epoch 00073: val_mDice did not improve from 0.42132
Epoch 74/300
 - 13s - loss: 1.9370 - acc: 0.9214 - mDice: 0.5003 - val_loss: 3.3084 - val_acc: 0.9399 - val_mDice: 0.4143

Epoch 00074: val_mDice did not improve from 0.42132
Epoch 75/300
 - 13s - loss: 1.9233 - acc: 0.9219 - mDice: 0.5035 - val_loss: 3.2394 - val_acc: 0.9404 - val_mDice: 0.4185

Epoch 00075: val_mDice did not improve from 0.42132
Epoch 76/300
 - 13s - loss: 1.9111 - acc: 0.9223 - mDice: 0.5060 - val_loss: 3.1755 - val_acc: 0.9399 - val_mDice: 0.4196

Epoch 00076: val_mDice did not improve from 0.42132
Epoch 77/300
 - 13s - loss: 1.9025 - acc: 0.9225 - mDice: 0.5085 - val_loss: 3.0990 - val_acc: 0.9407 - val_mDice: 0.4262

Epoch 00077: val_mDice improved from 0.42132 to 0.42624, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 78/300
 - 13s - loss: 1.8972 - acc: 0.9226 - mDice: 0.5094 - val_loss: 3.3394 - val_acc: 0.9390 - val_mDice: 0.4140

Epoch 00078: val_mDice did not improve from 0.42624
Epoch 79/300
 - 13s - loss: 1.8807 - acc: 0.9233 - mDice: 0.5136 - val_loss: 3.1369 - val_acc: 0.9376 - val_mDice: 0.4194

Epoch 00079: val_mDice did not improve from 0.42624
Epoch 80/300
 - 13s - loss: 1.8738 - acc: 0.9234 - mDice: 0.5152 - val_loss: 3.0798 - val_acc: 0.9411 - val_mDice: 0.4295

Epoch 00080: val_mDice improved from 0.42624 to 0.42952, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 81/300
 - 13s - loss: 1.8552 - acc: 0.9238 - mDice: 0.5185 - val_loss: 3.0361 - val_acc: 0.9428 - val_mDice: 0.4424

Epoch 00081: val_mDice improved from 0.42952 to 0.44243, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 82/300
 - 13s - loss: 1.8476 - acc: 0.9240 - mDice: 0.5208 - val_loss: 3.2146 - val_acc: 0.9409 - val_mDice: 0.4258

Epoch 00082: val_mDice did not improve from 0.44243
Epoch 83/300
 - 13s - loss: 1.8412 - acc: 0.9243 - mDice: 0.5223 - val_loss: 3.2856 - val_acc: 0.9400 - val_mDice: 0.4219

Epoch 00083: val_mDice did not improve from 0.44243
Epoch 84/300
 - 13s - loss: 1.8378 - acc: 0.9244 - mDice: 0.5233 - val_loss: 3.2613 - val_acc: 0.9390 - val_mDice: 0.4172

Epoch 00084: val_mDice did not improve from 0.44243
Epoch 85/300
 - 13s - loss: 1.8305 - acc: 0.9247 - mDice: 0.5250 - val_loss: 3.2706 - val_acc: 0.9408 - val_mDice: 0.4249

Epoch 00085: val_mDice did not improve from 0.44243
Epoch 86/300
 - 13s - loss: 1.8155 - acc: 0.9250 - mDice: 0.5279 - val_loss: 3.3583 - val_acc: 0.9407 - val_mDice: 0.4172

Epoch 00086: val_mDice did not improve from 0.44243
Epoch 87/300
 - 13s - loss: 1.8044 - acc: 0.9255 - mDice: 0.5306 - val_loss: 3.5242 - val_acc: 0.9402 - val_mDice: 0.4193

Epoch 00087: val_mDice did not improve from 0.44243
Epoch 88/300
 - 13s - loss: 1.7990 - acc: 0.9255 - mDice: 0.5316 - val_loss: 3.5135 - val_acc: 0.9404 - val_mDice: 0.4180

Epoch 00088: val_mDice did not improve from 0.44243
Epoch 89/300
 - 13s - loss: 1.7949 - acc: 0.9258 - mDice: 0.5329 - val_loss: 3.2339 - val_acc: 0.9379 - val_mDice: 0.4235

Epoch 00089: val_mDice did not improve from 0.44243
Epoch 90/300
 - 13s - loss: 1.7880 - acc: 0.9258 - mDice: 0.5347 - val_loss: 3.2092 - val_acc: 0.9400 - val_mDice: 0.4286

Epoch 00090: val_mDice did not improve from 0.44243
Epoch 91/300
 - 13s - loss: 1.7730 - acc: 0.9263 - mDice: 0.5390 - val_loss: 3.1141 - val_acc: 0.9415 - val_mDice: 0.4356

Epoch 00091: val_mDice did not improve from 0.44243
Epoch 92/300
 - 13s - loss: 1.7725 - acc: 0.9266 - mDice: 0.5387 - val_loss: 3.3252 - val_acc: 0.9415 - val_mDice: 0.4284

Epoch 00092: val_mDice did not improve from 0.44243
Epoch 93/300
 - 13s - loss: 1.7630 - acc: 0.9267 - mDice: 0.5407 - val_loss: 3.2438 - val_acc: 0.9425 - val_mDice: 0.4334

Epoch 00093: val_mDice did not improve from 0.44243
Epoch 94/300
 - 13s - loss: 1.7486 - acc: 0.9270 - mDice: 0.5441 - val_loss: 3.5915 - val_acc: 0.9399 - val_mDice: 0.4134

Epoch 00094: val_mDice did not improve from 0.44243
Epoch 95/300
 - 13s - loss: 1.7424 - acc: 0.9272 - mDice: 0.5453 - val_loss: 3.3436 - val_acc: 0.9428 - val_mDice: 0.4341

Epoch 00095: val_mDice did not improve from 0.44243
Epoch 96/300
 - 13s - loss: 1.7393 - acc: 0.9272 - mDice: 0.5458 - val_loss: 3.4044 - val_acc: 0.9424 - val_mDice: 0.4355

Epoch 00096: val_mDice did not improve from 0.44243
Epoch 97/300
 - 13s - loss: 1.7396 - acc: 0.9271 - mDice: 0.5459 - val_loss: 3.4704 - val_acc: 0.9417 - val_mDice: 0.4317

Epoch 00097: val_mDice did not improve from 0.44243
Epoch 98/300
 - 13s - loss: 1.7134 - acc: 0.9278 - mDice: 0.5521 - val_loss: 3.6095 - val_acc: 0.9394 - val_mDice: 0.4162

Epoch 00098: val_mDice did not improve from 0.44243
Epoch 99/300
 - 13s - loss: 1.7138 - acc: 0.9279 - mDice: 0.5529 - val_loss: 3.2509 - val_acc: 0.9422 - val_mDice: 0.4451

Epoch 00099: val_mDice improved from 0.44243 to 0.44507, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 100/300
 - 13s - loss: 1.7109 - acc: 0.9278 - mDice: 0.5527 - val_loss: 3.3667 - val_acc: 0.9423 - val_mDice: 0.4436

Epoch 00100: val_mDice did not improve from 0.44507
Epoch 101/300
 - 13s - loss: 1.6959 - acc: 0.9280 - mDice: 0.5566 - val_loss: 3.3732 - val_acc: 0.9427 - val_mDice: 0.4381

Epoch 00101: val_mDice did not improve from 0.44507
Epoch 102/300
 - 13s - loss: 1.6969 - acc: 0.9281 - mDice: 0.5561 - val_loss: 3.5643 - val_acc: 0.9422 - val_mDice: 0.4345

Epoch 00102: val_mDice did not improve from 0.44507
Epoch 103/300
 - 13s - loss: 1.6857 - acc: 0.9284 - mDice: 0.5598 - val_loss: 3.5656 - val_acc: 0.9438 - val_mDice: 0.4345

Epoch 00103: val_mDice did not improve from 0.44507
Epoch 104/300
 - 13s - loss: 1.6760 - acc: 0.9288 - mDice: 0.5605 - val_loss: 3.3284 - val_acc: 0.9427 - val_mDice: 0.4508

Epoch 00104: val_mDice improved from 0.44507 to 0.45084, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 105/300
 - 13s - loss: 1.6751 - acc: 0.9288 - mDice: 0.5620 - val_loss: 3.2608 - val_acc: 0.9408 - val_mDice: 0.4397

Epoch 00105: val_mDice did not improve from 0.45084
Epoch 106/300
 - 13s - loss: 1.6653 - acc: 0.9290 - mDice: 0.5634 - val_loss: 3.5609 - val_acc: 0.9416 - val_mDice: 0.4392

Epoch 00106: val_mDice did not improve from 0.45084
Epoch 107/300
 - 13s - loss: 1.6667 - acc: 0.9288 - mDice: 0.5635 - val_loss: 3.3597 - val_acc: 0.9430 - val_mDice: 0.4494

Epoch 00107: val_mDice did not improve from 0.45084
Epoch 108/300
 - 13s - loss: 1.6587 - acc: 0.9292 - mDice: 0.5652 - val_loss: 3.4407 - val_acc: 0.9427 - val_mDice: 0.4441

Epoch 00108: val_mDice did not improve from 0.45084
Epoch 109/300
 - 13s - loss: 1.6489 - acc: 0.9293 - mDice: 0.5674 - val_loss: 3.1883 - val_acc: 0.9439 - val_mDice: 0.4572

Epoch 00109: val_mDice improved from 0.45084 to 0.45721, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 110/300
 - 13s - loss: 1.6512 - acc: 0.9292 - mDice: 0.5672 - val_loss: 3.3749 - val_acc: 0.9423 - val_mDice: 0.4421

Epoch 00110: val_mDice did not improve from 0.45721
Epoch 111/300
 - 13s - loss: 1.6467 - acc: 0.9294 - mDice: 0.5688 - val_loss: 3.3599 - val_acc: 0.9428 - val_mDice: 0.4484

Epoch 00111: val_mDice did not improve from 0.45721
Epoch 112/300
 - 13s - loss: 1.6355 - acc: 0.9297 - mDice: 0.5706 - val_loss: 3.3363 - val_acc: 0.9431 - val_mDice: 0.4503

Epoch 00112: val_mDice did not improve from 0.45721
Epoch 113/300
 - 13s - loss: 1.6334 - acc: 0.9296 - mDice: 0.5712 - val_loss: 3.4341 - val_acc: 0.9425 - val_mDice: 0.4453

Epoch 00113: val_mDice did not improve from 0.45721
Epoch 114/300
 - 13s - loss: 1.6272 - acc: 0.9301 - mDice: 0.5733 - val_loss: 3.4801 - val_acc: 0.9419 - val_mDice: 0.4399

Epoch 00114: val_mDice did not improve from 0.45721
Epoch 115/300
 - 13s - loss: 1.6220 - acc: 0.9300 - mDice: 0.5739 - val_loss: 3.5127 - val_acc: 0.9420 - val_mDice: 0.4426

Epoch 00115: val_mDice did not improve from 0.45721
Epoch 116/300
 - 13s - loss: 1.6178 - acc: 0.9302 - mDice: 0.5758 - val_loss: 3.4395 - val_acc: 0.9431 - val_mDice: 0.4500

Epoch 00116: val_mDice did not improve from 0.45721
Epoch 117/300
 - 13s - loss: 1.6114 - acc: 0.9306 - mDice: 0.5772 - val_loss: 3.6363 - val_acc: 0.9428 - val_mDice: 0.4445

Epoch 00117: val_mDice did not improve from 0.45721
Epoch 118/300
 - 13s - loss: 1.6064 - acc: 0.9304 - mDice: 0.5786 - val_loss: 3.6751 - val_acc: 0.9435 - val_mDice: 0.4462

Epoch 00118: val_mDice did not improve from 0.45721
Epoch 119/300
 - 13s - loss: 1.6038 - acc: 0.9307 - mDice: 0.5788 - val_loss: 3.4373 - val_acc: 0.9437 - val_mDice: 0.4478

Epoch 00119: val_mDice did not improve from 0.45721
Epoch 120/300
 - 13s - loss: 1.5898 - acc: 0.9310 - mDice: 0.5818 - val_loss: 3.6201 - val_acc: 0.9412 - val_mDice: 0.4442

Epoch 00120: val_mDice did not improve from 0.45721
Epoch 121/300
 - 13s - loss: 1.5868 - acc: 0.9312 - mDice: 0.5826 - val_loss: 3.3955 - val_acc: 0.9446 - val_mDice: 0.4551

Epoch 00121: val_mDice did not improve from 0.45721
Epoch 122/300
 - 13s - loss: 1.5887 - acc: 0.9310 - mDice: 0.5828 - val_loss: 3.3546 - val_acc: 0.9414 - val_mDice: 0.4513

Epoch 00122: val_mDice did not improve from 0.45721
Epoch 123/300
 - 13s - loss: 1.5873 - acc: 0.9313 - mDice: 0.5832 - val_loss: 3.2988 - val_acc: 0.9445 - val_mDice: 0.4624

Epoch 00123: val_mDice improved from 0.45721 to 0.46240, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 124/300
 - 13s - loss: 1.5778 - acc: 0.9314 - mDice: 0.5850 - val_loss: 3.5115 - val_acc: 0.9428 - val_mDice: 0.4506

Epoch 00124: val_mDice did not improve from 0.46240
Epoch 125/300
 - 13s - loss: 1.5757 - acc: 0.9313 - mDice: 0.5855 - val_loss: 3.9964 - val_acc: 0.9429 - val_mDice: 0.4392

Epoch 00125: val_mDice did not improve from 0.46240
Epoch 126/300
 - 13s - loss: 1.5669 - acc: 0.9317 - mDice: 0.5884 - val_loss: 3.5969 - val_acc: 0.9425 - val_mDice: 0.4480

Epoch 00126: val_mDice did not improve from 0.46240
Epoch 127/300
 - 13s - loss: 1.5670 - acc: 0.9315 - mDice: 0.5879 - val_loss: 3.4291 - val_acc: 0.9453 - val_mDice: 0.4619

Epoch 00127: val_mDice did not improve from 0.46240
Epoch 128/300
 - 13s - loss: 1.5561 - acc: 0.9318 - mDice: 0.5911 - val_loss: 3.8737 - val_acc: 0.9424 - val_mDice: 0.4371

Epoch 00128: val_mDice did not improve from 0.46240
Epoch 129/300
 - 13s - loss: 1.5581 - acc: 0.9318 - mDice: 0.5901 - val_loss: 3.5548 - val_acc: 0.9431 - val_mDice: 0.4499

Epoch 00129: val_mDice did not improve from 0.46240
Epoch 130/300
 - 13s - loss: 1.5469 - acc: 0.9321 - mDice: 0.5926 - val_loss: 3.7875 - val_acc: 0.9422 - val_mDice: 0.4443

Epoch 00130: val_mDice did not improve from 0.46240
Epoch 131/300
 - 13s - loss: 1.5468 - acc: 0.9323 - mDice: 0.5931 - val_loss: 3.4534 - val_acc: 0.9425 - val_mDice: 0.4509

Epoch 00131: val_mDice did not improve from 0.46240
Epoch 132/300
 - 13s - loss: 1.5449 - acc: 0.9322 - mDice: 0.5934 - val_loss: 3.6062 - val_acc: 0.9431 - val_mDice: 0.4508

Epoch 00132: val_mDice did not improve from 0.46240
Epoch 133/300
 - 13s - loss: 1.5414 - acc: 0.9323 - mDice: 0.5945 - val_loss: 3.4888 - val_acc: 0.9421 - val_mDice: 0.4479

Epoch 00133: val_mDice did not improve from 0.46240
Epoch 134/300
 - 13s - loss: 1.5328 - acc: 0.9326 - mDice: 0.5962 - val_loss: 3.4546 - val_acc: 0.9439 - val_mDice: 0.4601

Epoch 00134: val_mDice did not improve from 0.46240
Epoch 135/300
 - 13s - loss: 1.5377 - acc: 0.9323 - mDice: 0.5951 - val_loss: 3.5761 - val_acc: 0.9428 - val_mDice: 0.4533

Epoch 00135: val_mDice did not improve from 0.46240
Epoch 136/300
 - 13s - loss: 1.5381 - acc: 0.9322 - mDice: 0.5949 - val_loss: 3.6259 - val_acc: 0.9451 - val_mDice: 0.4571

Epoch 00136: val_mDice did not improve from 0.46240
Epoch 137/300
 - 13s - loss: 1.5259 - acc: 0.9328 - mDice: 0.5981 - val_loss: 3.4905 - val_acc: 0.9435 - val_mDice: 0.4557

Epoch 00137: val_mDice did not improve from 0.46240
Epoch 138/300
 - 13s - loss: 1.5192 - acc: 0.9329 - mDice: 0.5991 - val_loss: 3.6153 - val_acc: 0.9441 - val_mDice: 0.4542

Epoch 00138: val_mDice did not improve from 0.46240
Epoch 139/300
 - 13s - loss: 1.5167 - acc: 0.9330 - mDice: 0.5995 - val_loss: 3.5806 - val_acc: 0.9407 - val_mDice: 0.4458

Epoch 00139: val_mDice did not improve from 0.46240
Epoch 140/300
 - 13s - loss: 1.5228 - acc: 0.9329 - mDice: 0.5990 - val_loss: 3.8533 - val_acc: 0.9428 - val_mDice: 0.4436

Epoch 00140: val_mDice did not improve from 0.46240
Epoch 141/300
 - 13s - loss: 1.5180 - acc: 0.9329 - mDice: 0.5995 - val_loss: 3.5353 - val_acc: 0.9421 - val_mDice: 0.4535

Epoch 00141: val_mDice did not improve from 0.46240
Epoch 142/300
 - 13s - loss: 1.5154 - acc: 0.9329 - mDice: 0.6007 - val_loss: 3.5736 - val_acc: 0.9430 - val_mDice: 0.4517

Epoch 00142: val_mDice did not improve from 0.46240
Epoch 143/300
 - 13s - loss: 1.5088 - acc: 0.9332 - mDice: 0.6028 - val_loss: 3.5420 - val_acc: 0.9436 - val_mDice: 0.4548

Epoch 00143: val_mDice did not improve from 0.46240
Epoch 144/300
 - 13s - loss: 1.5045 - acc: 0.9333 - mDice: 0.6026 - val_loss: 3.3959 - val_acc: 0.9451 - val_mDice: 0.4659

Epoch 00144: val_mDice improved from 0.46240 to 0.46594, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 145/300
 - 13s - loss: 1.5081 - acc: 0.9333 - mDice: 0.6024 - val_loss: 3.5470 - val_acc: 0.9440 - val_mDice: 0.4623

Epoch 00145: val_mDice did not improve from 0.46594
Epoch 146/300
 - 13s - loss: 1.4994 - acc: 0.9335 - mDice: 0.6040 - val_loss: 3.5059 - val_acc: 0.9407 - val_mDice: 0.4439

Epoch 00146: val_mDice did not improve from 0.46594
Epoch 147/300
 - 13s - loss: 1.4946 - acc: 0.9338 - mDice: 0.6059 - val_loss: 3.6163 - val_acc: 0.9439 - val_mDice: 0.4592

Epoch 00147: val_mDice did not improve from 0.46594
Epoch 148/300
 - 13s - loss: 1.4975 - acc: 0.9338 - mDice: 0.6053 - val_loss: 3.7479 - val_acc: 0.9438 - val_mDice: 0.4515

Epoch 00148: val_mDice did not improve from 0.46594
Epoch 149/300
 - 13s - loss: 1.4941 - acc: 0.9338 - mDice: 0.6062 - val_loss: 3.4076 - val_acc: 0.9433 - val_mDice: 0.4611

Epoch 00149: val_mDice did not improve from 0.46594
Epoch 150/300
 - 13s - loss: 1.4810 - acc: 0.9342 - mDice: 0.6088 - val_loss: 3.8426 - val_acc: 0.9421 - val_mDice: 0.4464

Epoch 00150: val_mDice did not improve from 0.46594
Epoch 151/300
 - 13s - loss: 1.4877 - acc: 0.9340 - mDice: 0.6074 - val_loss: 3.7782 - val_acc: 0.9420 - val_mDice: 0.4523

Epoch 00151: val_mDice did not improve from 0.46594
Epoch 152/300
 - 13s - loss: 1.4912 - acc: 0.9338 - mDice: 0.6065 - val_loss: 3.5040 - val_acc: 0.9453 - val_mDice: 0.4590

Epoch 00152: val_mDice did not improve from 0.46594
Epoch 153/300
 - 13s - loss: 1.4862 - acc: 0.9337 - mDice: 0.6073 - val_loss: 3.7123 - val_acc: 0.9418 - val_mDice: 0.4533

Epoch 00153: val_mDice did not improve from 0.46594
Epoch 154/300
 - 13s - loss: 1.4742 - acc: 0.9341 - mDice: 0.6112 - val_loss: 3.6318 - val_acc: 0.9405 - val_mDice: 0.4473

Epoch 00154: val_mDice did not improve from 0.46594
Epoch 155/300
 - 13s - loss: 1.4730 - acc: 0.9340 - mDice: 0.6112 - val_loss: 3.7005 - val_acc: 0.9424 - val_mDice: 0.4490

Epoch 00155: val_mDice did not improve from 0.46594
Epoch 156/300
 - 13s - loss: 1.4755 - acc: 0.9340 - mDice: 0.6108 - val_loss: 3.6494 - val_acc: 0.9431 - val_mDice: 0.4570

Epoch 00156: val_mDice did not improve from 0.46594
Epoch 157/300
 - 13s - loss: 1.4655 - acc: 0.9343 - mDice: 0.6130 - val_loss: 3.6820 - val_acc: 0.9438 - val_mDice: 0.4643

Epoch 00157: val_mDice did not improve from 0.46594
Epoch 158/300
 - 13s - loss: 1.4656 - acc: 0.9343 - mDice: 0.6131 - val_loss: 3.6779 - val_acc: 0.9430 - val_mDice: 0.4630

Epoch 00158: val_mDice did not improve from 0.46594
Epoch 159/300
 - 13s - loss: 1.4605 - acc: 0.9343 - mDice: 0.6141 - val_loss: 3.6160 - val_acc: 0.9444 - val_mDice: 0.4655

Epoch 00159: val_mDice did not improve from 0.46594
Epoch 160/300
 - 13s - loss: 1.4623 - acc: 0.9343 - mDice: 0.6147 - val_loss: 3.3068 - val_acc: 0.9429 - val_mDice: 0.4685

Epoch 00160: val_mDice improved from 0.46594 to 0.46849, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 161/300
 - 13s - loss: 1.4614 - acc: 0.9344 - mDice: 0.6138 - val_loss: 3.5752 - val_acc: 0.9418 - val_mDice: 0.4568

Epoch 00161: val_mDice did not improve from 0.46849
Epoch 162/300
 - 13s - loss: 1.4571 - acc: 0.9347 - mDice: 0.6148 - val_loss: 3.5396 - val_acc: 0.9444 - val_mDice: 0.4703

Epoch 00162: val_mDice improved from 0.46849 to 0.47034, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 163/300
 - 13s - loss: 1.4584 - acc: 0.9345 - mDice: 0.6148 - val_loss: 3.6681 - val_acc: 0.9423 - val_mDice: 0.4626

Epoch 00163: val_mDice did not improve from 0.47034
Epoch 164/300
 - 13s - loss: 1.4475 - acc: 0.9351 - mDice: 0.6183 - val_loss: 3.8089 - val_acc: 0.9417 - val_mDice: 0.4453

Epoch 00164: val_mDice did not improve from 0.47034
Epoch 165/300
 - 13s - loss: 1.4503 - acc: 0.9350 - mDice: 0.6173 - val_loss: 3.5030 - val_acc: 0.9414 - val_mDice: 0.4541

Epoch 00165: val_mDice did not improve from 0.47034
Epoch 166/300
 - 13s - loss: 1.4464 - acc: 0.9350 - mDice: 0.6180 - val_loss: 3.7061 - val_acc: 0.9430 - val_mDice: 0.4558

Epoch 00166: val_mDice did not improve from 0.47034
Epoch 167/300
 - 13s - loss: 1.4523 - acc: 0.9349 - mDice: 0.6170 - val_loss: 3.4119 - val_acc: 0.9431 - val_mDice: 0.4681

Epoch 00167: val_mDice did not improve from 0.47034
Epoch 168/300
 - 13s - loss: 1.4465 - acc: 0.9350 - mDice: 0.6176 - val_loss: 3.5073 - val_acc: 0.9421 - val_mDice: 0.4635

Epoch 00168: val_mDice did not improve from 0.47034
Epoch 169/300
 - 13s - loss: 1.4406 - acc: 0.9352 - mDice: 0.6191 - val_loss: 3.5235 - val_acc: 0.9426 - val_mDice: 0.4618

Epoch 00169: val_mDice did not improve from 0.47034
Epoch 170/300
 - 13s - loss: 1.4459 - acc: 0.9353 - mDice: 0.6185 - val_loss: 3.9707 - val_acc: 0.9420 - val_mDice: 0.4453

Epoch 00170: val_mDice did not improve from 0.47034
Epoch 171/300
 - 13s - loss: 1.4310 - acc: 0.9355 - mDice: 0.6214 - val_loss: 3.8504 - val_acc: 0.9433 - val_mDice: 0.4575

Epoch 00171: val_mDice did not improve from 0.47034
Epoch 172/300
 - 13s - loss: 1.4335 - acc: 0.9356 - mDice: 0.6215 - val_loss: 3.4603 - val_acc: 0.9444 - val_mDice: 0.4688

Epoch 00172: val_mDice did not improve from 0.47034
Epoch 173/300
 - 13s - loss: 1.4262 - acc: 0.9357 - mDice: 0.6227 - val_loss: 3.7041 - val_acc: 0.9435 - val_mDice: 0.4660

Epoch 00173: val_mDice did not improve from 0.47034
Epoch 174/300
 - 13s - loss: 1.4306 - acc: 0.9356 - mDice: 0.6226 - val_loss: 3.5893 - val_acc: 0.9421 - val_mDice: 0.4585

Epoch 00174: val_mDice did not improve from 0.47034
Epoch 175/300
 - 13s - loss: 1.4297 - acc: 0.9356 - mDice: 0.6225 - val_loss: 3.6316 - val_acc: 0.9420 - val_mDice: 0.4554

Epoch 00175: val_mDice did not improve from 0.47034
Epoch 176/300
 - 13s - loss: 1.4206 - acc: 0.9359 - mDice: 0.6248 - val_loss: 3.8535 - val_acc: 0.9442 - val_mDice: 0.4585

Epoch 00176: val_mDice did not improve from 0.47034
Epoch 177/300
 - 13s - loss: 1.4202 - acc: 0.9360 - mDice: 0.6253 - val_loss: 3.7461 - val_acc: 0.9433 - val_mDice: 0.4580

Epoch 00177: val_mDice did not improve from 0.47034
Epoch 178/300
 - 13s - loss: 1.4146 - acc: 0.9360 - mDice: 0.6263 - val_loss: 3.8367 - val_acc: 0.9423 - val_mDice: 0.4522

Epoch 00178: val_mDice did not improve from 0.47034
Epoch 179/300
 - 13s - loss: 1.4180 - acc: 0.9361 - mDice: 0.6256 - val_loss: 3.7435 - val_acc: 0.9443 - val_mDice: 0.4644

Epoch 00179: val_mDice did not improve from 0.47034
Epoch 180/300
 - 13s - loss: 1.4220 - acc: 0.9361 - mDice: 0.6242 - val_loss: 3.4385 - val_acc: 0.9427 - val_mDice: 0.4639

Epoch 00180: val_mDice did not improve from 0.47034
Epoch 181/300
 - 13s - loss: 1.4156 - acc: 0.9361 - mDice: 0.6264 - val_loss: 3.6855 - val_acc: 0.9428 - val_mDice: 0.4540

Epoch 00181: val_mDice did not improve from 0.47034
Epoch 182/300
 - 13s - loss: 1.4137 - acc: 0.9362 - mDice: 0.6262 - val_loss: 3.7751 - val_acc: 0.9436 - val_mDice: 0.4543

Epoch 00182: val_mDice did not improve from 0.47034
Epoch 183/300
 - 13s - loss: 1.4084 - acc: 0.9364 - mDice: 0.6280 - val_loss: 3.5008 - val_acc: 0.9436 - val_mDice: 0.4717

Epoch 00183: val_mDice improved from 0.47034 to 0.47170, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 184/300
 - 13s - loss: 1.4022 - acc: 0.9367 - mDice: 0.6301 - val_loss: 3.3588 - val_acc: 0.9441 - val_mDice: 0.4758

Epoch 00184: val_mDice improved from 0.47170 to 0.47584, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 185/300
 - 13s - loss: 1.4082 - acc: 0.9366 - mDice: 0.6281 - val_loss: 3.6156 - val_acc: 0.9421 - val_mDice: 0.4613

Epoch 00185: val_mDice did not improve from 0.47584
Epoch 186/300
 - 13s - loss: 1.4054 - acc: 0.9367 - mDice: 0.6295 - val_loss: 3.7034 - val_acc: 0.9427 - val_mDice: 0.4640

Epoch 00186: val_mDice did not improve from 0.47584
Epoch 187/300
 - 13s - loss: 1.4031 - acc: 0.9368 - mDice: 0.6294 - val_loss: 3.4345 - val_acc: 0.9434 - val_mDice: 0.4674

Epoch 00187: val_mDice did not improve from 0.47584
Epoch 188/300
 - 13s - loss: 1.3994 - acc: 0.9369 - mDice: 0.6298 - val_loss: 3.6137 - val_acc: 0.9435 - val_mDice: 0.4693

Epoch 00188: val_mDice did not improve from 0.47584
Epoch 189/300
 - 13s - loss: 1.3927 - acc: 0.9373 - mDice: 0.6323 - val_loss: 3.5085 - val_acc: 0.9437 - val_mDice: 0.4705

Epoch 00189: val_mDice did not improve from 0.47584
Epoch 190/300
 - 13s - loss: 1.3995 - acc: 0.9371 - mDice: 0.6313 - val_loss: 3.7976 - val_acc: 0.9445 - val_mDice: 0.4642

Epoch 00190: val_mDice did not improve from 0.47584
Epoch 191/300
 - 13s - loss: 1.4005 - acc: 0.9371 - mDice: 0.6304 - val_loss: 3.7207 - val_acc: 0.9435 - val_mDice: 0.4650

Epoch 00191: val_mDice did not improve from 0.47584
Epoch 192/300
 - 13s - loss: 1.3967 - acc: 0.9372 - mDice: 0.6314 - val_loss: 3.7779 - val_acc: 0.9395 - val_mDice: 0.4501

Epoch 00192: val_mDice did not improve from 0.47584
Epoch 193/300
 - 13s - loss: 1.3919 - acc: 0.9374 - mDice: 0.6323 - val_loss: 3.6722 - val_acc: 0.9438 - val_mDice: 0.4703

Epoch 00193: val_mDice did not improve from 0.47584
Epoch 194/300
 - 13s - loss: 1.3900 - acc: 0.9374 - mDice: 0.6333 - val_loss: 3.6628 - val_acc: 0.9448 - val_mDice: 0.4676

Epoch 00194: val_mDice did not improve from 0.47584
Epoch 195/300
 - 13s - loss: 1.3841 - acc: 0.9376 - mDice: 0.6345 - val_loss: 3.6030 - val_acc: 0.9430 - val_mDice: 0.4504

Epoch 00195: val_mDice did not improve from 0.47584
Epoch 196/300
 - 13s - loss: 1.3834 - acc: 0.9377 - mDice: 0.6346 - val_loss: 3.4869 - val_acc: 0.9443 - val_mDice: 0.4707

Epoch 00196: val_mDice did not improve from 0.47584
Epoch 197/300
 - 13s - loss: 1.3816 - acc: 0.9377 - mDice: 0.6354 - val_loss: 3.5440 - val_acc: 0.9440 - val_mDice: 0.4688

Epoch 00197: val_mDice did not improve from 0.47584
Epoch 198/300
 - 13s - loss: 1.3765 - acc: 0.9379 - mDice: 0.6362 - val_loss: 3.6109 - val_acc: 0.9426 - val_mDice: 0.4538

Epoch 00198: val_mDice did not improve from 0.47584
Epoch 199/300
 - 13s - loss: 1.3783 - acc: 0.9378 - mDice: 0.6363 - val_loss: 4.1735 - val_acc: 0.9434 - val_mDice: 0.4475

Epoch 00199: val_mDice did not improve from 0.47584
Epoch 200/300
 - 13s - loss: 1.3811 - acc: 0.9378 - mDice: 0.6359 - val_loss: 3.5459 - val_acc: 0.9434 - val_mDice: 0.4621

Epoch 00200: val_mDice did not improve from 0.47584
Epoch 201/300
 - 13s - loss: 1.3756 - acc: 0.9381 - mDice: 0.6366 - val_loss: 3.6993 - val_acc: 0.9427 - val_mDice: 0.4608

Epoch 00201: val_mDice did not improve from 0.47584
Epoch 202/300
 - 13s - loss: 1.3828 - acc: 0.9378 - mDice: 0.6352 - val_loss: 4.0984 - val_acc: 0.9415 - val_mDice: 0.4391

Epoch 00202: val_mDice did not improve from 0.47584
Epoch 203/300
 - 13s - loss: 1.3791 - acc: 0.9380 - mDice: 0.6363 - val_loss: 3.6967 - val_acc: 0.9446 - val_mDice: 0.4685

Epoch 00203: val_mDice did not improve from 0.47584
Epoch 204/300
 - 13s - loss: 1.3762 - acc: 0.9381 - mDice: 0.6373 - val_loss: 3.5386 - val_acc: 0.9424 - val_mDice: 0.4587

Epoch 00204: val_mDice did not improve from 0.47584
Epoch 205/300
 - 13s - loss: 1.3716 - acc: 0.9382 - mDice: 0.6376 - val_loss: 3.5955 - val_acc: 0.9438 - val_mDice: 0.4688

Epoch 00205: val_mDice did not improve from 0.47584
Epoch 206/300
 - 13s - loss: 1.3715 - acc: 0.9382 - mDice: 0.6381 - val_loss: 3.3950 - val_acc: 0.9440 - val_mDice: 0.4778

Epoch 00206: val_mDice improved from 0.47584 to 0.47782, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a/MultiClass_24567891011121314/sd2/best_model_weights_TF_CSFn2.h5
Epoch 207/300
 - 13s - loss: 1.3707 - acc: 0.9383 - mDice: 0.6382 - val_loss: 3.6585 - val_acc: 0.9431 - val_mDice: 0.4613

Epoch 00207: val_mDice did not improve from 0.47782
Epoch 208/300
 - 13s - loss: 1.3724 - acc: 0.9383 - mDice: 0.6382 - val_loss: 3.4381 - val_acc: 0.9429 - val_mDice: 0.4732

Epoch 00208: val_mDice did not improve from 0.47782
Epoch 209/300
 - 13s - loss: 1.3690 - acc: 0.9384 - mDice: 0.6386 - val_loss: 3.7070 - val_acc: 0.9414 - val_mDice: 0.4556

Epoch 00209: val_mDice did not improve from 0.47782
Epoch 210/300
 - 13s - loss: 1.3625 - acc: 0.9385 - mDice: 0.6408 - val_loss: 3.7491 - val_acc: 0.9434 - val_mDice: 0.4550

Epoch 00210: val_mDice did not improve from 0.47782
Epoch 211/300
 - 13s - loss: 1.3628 - acc: 0.9387 - mDice: 0.6407 - val_loss: 3.7060 - val_acc: 0.9436 - val_mDice: 0.4622

Epoch 00211: val_mDice did not improve from 0.47782
Epoch 212/300
 - 13s - loss: 1.3702 - acc: 0.9383 - mDice: 0.6388 - val_loss: 3.7093 - val_acc: 0.9432 - val_mDice: 0.4614

Epoch 00212: val_mDice did not improve from 0.47782
Epoch 213/300
 - 13s - loss: 1.3701 - acc: 0.9384 - mDice: 0.6384 - val_loss: 3.7108 - val_acc: 0.9437 - val_mDice: 0.4630

Epoch 00213: val_mDice did not improve from 0.47782
Epoch 214/300
 - 13s - loss: 1.3584 - acc: 0.9388 - mDice: 0.6420 - val_loss: 3.6439 - val_acc: 0.9445 - val_mDice: 0.4751

Epoch 00214: val_mDice did not improve from 0.47782
Epoch 215/300
 - 13s - loss: 1.3495 - acc: 0.9392 - mDice: 0.6438 - val_loss: 3.7396 - val_acc: 0.9435 - val_mDice: 0.4666

Epoch 00215: val_mDice did not improve from 0.47782
Epoch 216/300
 - 13s - loss: 1.3532 - acc: 0.9391 - mDice: 0.6430 - val_loss: 3.5676 - val_acc: 0.9425 - val_mDice: 0.4672

Epoch 00216: val_mDice did not improve from 0.47782
Epoch 217/300
 - 13s - loss: 1.3517 - acc: 0.9391 - mDice: 0.6434 - val_loss: 3.6902 - val_acc: 0.9436 - val_mDice: 0.4686

Epoch 00217: val_mDice did not improve from 0.47782
Epoch 218/300
 - 13s - loss: 1.3524 - acc: 0.9390 - mDice: 0.6432 - val_loss: 3.7196 - val_acc: 0.9428 - val_mDice: 0.4680

Epoch 00218: val_mDice did not improve from 0.47782
Epoch 219/300
 - 13s - loss: 1.3534 - acc: 0.9392 - mDice: 0.6427 - val_loss: 3.6826 - val_acc: 0.9436 - val_mDice: 0.4675

Epoch 00219: val_mDice did not improve from 0.47782
Epoch 220/300
 - 13s - loss: 1.3455 - acc: 0.9397 - mDice: 0.6456 - val_loss: 3.7712 - val_acc: 0.9443 - val_mDice: 0.4696

Epoch 00220: val_mDice did not improve from 0.47782
Epoch 221/300
 - 13s - loss: 1.3455 - acc: 0.9394 - mDice: 0.6447 - val_loss: 3.5404 - val_acc: 0.9431 - val_mDice: 0.4702

Epoch 00221: val_mDice did not improve from 0.47782
Epoch 222/300
 - 13s - loss: 1.3536 - acc: 0.9393 - mDice: 0.6436 - val_loss: 3.5246 - val_acc: 0.9424 - val_mDice: 0.4683

Epoch 00222: val_mDice did not improve from 0.47782
Epoch 223/300
 - 13s - loss: 1.3458 - acc: 0.9395 - mDice: 0.6452 - val_loss: 3.7412 - val_acc: 0.9427 - val_mDice: 0.4557

Epoch 00223: val_mDice did not improve from 0.47782
Epoch 224/300
 - 13s - loss: 1.3488 - acc: 0.9394 - mDice: 0.6449 - val_loss: 3.5659 - val_acc: 0.9409 - val_mDice: 0.4627

Epoch 00224: val_mDice did not improve from 0.47782
Epoch 225/300
 - 13s - loss: 1.3432 - acc: 0.9397 - mDice: 0.6455 - val_loss: 3.5460 - val_acc: 0.9419 - val_mDice: 0.4591

Epoch 00225: val_mDice did not improve from 0.47782
Epoch 226/300
 - 13s - loss: 1.3424 - acc: 0.9397 - mDice: 0.6459 - val_loss: 3.6087 - val_acc: 0.9439 - val_mDice: 0.4644

Epoch 00226: val_mDice did not improve from 0.47782
Epoch 227/300
 - 13s - loss: 1.3349 - acc: 0.9399 - mDice: 0.6480 - val_loss: 3.6471 - val_acc: 0.9430 - val_mDice: 0.4697

Epoch 00227: val_mDice did not improve from 0.47782
Epoch 228/300
 - 13s - loss: 1.3359 - acc: 0.9398 - mDice: 0.6475 - val_loss: 3.6913 - val_acc: 0.9429 - val_mDice: 0.4638

Epoch 00228: val_mDice did not improve from 0.47782
Epoch 229/300
 - 13s - loss: 1.3348 - acc: 0.9399 - mDice: 0.6477 - val_loss: 3.6079 - val_acc: 0.9435 - val_mDice: 0.4748

Epoch 00229: val_mDice did not improve from 0.47782
Epoch 230/300
 - 13s - loss: 1.3418 - acc: 0.9399 - mDice: 0.6466 - val_loss: 3.8829 - val_acc: 0.9416 - val_mDice: 0.4603

Epoch 00230: val_mDice did not improve from 0.47782
Epoch 231/300
 - 13s - loss: 1.3338 - acc: 0.9399 - mDice: 0.6480 - val_loss: 3.7061 - val_acc: 0.9404 - val_mDice: 0.4487

Epoch 00231: val_mDice did not improve from 0.47782
Epoch 232/300
 - 13s - loss: 1.3337 - acc: 0.9402 - mDice: 0.6484 - val_loss: 3.8007 - val_acc: 0.9431 - val_mDice: 0.4686

Epoch 00232: val_mDice did not improve from 0.47782
Epoch 233/300
 - 13s - loss: 1.3352 - acc: 0.9401 - mDice: 0.6480 - val_loss: 3.7493 - val_acc: 0.9414 - val_mDice: 0.4598

Epoch 00233: val_mDice did not improve from 0.47782
Epoch 234/300
 - 13s - loss: 1.3362 - acc: 0.9402 - mDice: 0.6479 - val_loss: 3.5829 - val_acc: 0.9421 - val_mDice: 0.4594

Epoch 00234: val_mDice did not improve from 0.47782
Epoch 235/300
 - 13s - loss: 1.3368 - acc: 0.9401 - mDice: 0.6481 - val_loss: 3.8037 - val_acc: 0.9418 - val_mDice: 0.4553

Epoch 00235: val_mDice did not improve from 0.47782
Epoch 236/300
 - 13s - loss: 1.3279 - acc: 0.9404 - mDice: 0.6487 - val_loss: 3.7134 - val_acc: 0.9438 - val_mDice: 0.4739

Epoch 00236: val_mDice did not improve from 0.47782
Restoring model weights from the end of the best epoch
Epoch 00236: early stopping
{'val_loss': [84.45242018926712, 23.33486631370726, 12.207176727907997, 8.427361363456363, 7.336938176126707, 7.081693622682776, 7.00717787905818, 6.884845073734011, 6.7633225208237056, 6.196206122902887, 5.948614030544247, 6.224726767057464, 4.474350342881822, 5.54973683958607, 5.586278232790175, 4.113134861923754, 3.95999195612967, 4.579331128813681, 4.520752729227145, 5.41829276217946, 4.079095344857445, 4.323740815832501, 3.56000028612713, 3.751838488149501, 3.7734268541846956, 3.4587336848268198, 3.6917054282412645, 3.9250466937554025, 3.584079276171646, 3.585680040176071, 3.659280631802089, 3.68561066656063, 3.21914421678299, 3.210248047508122, 3.2481226156953547, 2.9893794667330527, 3.271160239486822, 3.063151658778744, 3.7135380741785324, 3.3375747971946286, 3.353616574646107, 3.3355428416370634, 3.340446992777288, 3.767531300007942, 3.4693289674552426, 3.1393779642613873, 3.189305180683732, 2.9982890920003964, 3.2999111661643146, 3.116869584657252, 3.1061188148645065, 3.6682661426297964, 3.0083668149032055, 3.052872170605475, 2.9302437558237995, 2.9536020745124136, 2.9679596424768016, 3.242581628351694, 3.010501225079809, 3.144518284127116, 3.1045148271978613, 3.176698875569162, 3.5045281561268937, 3.368105578058887, 3.500523331198132, 3.125349944612632, 3.1433438771803464, 3.136773162831863, 3.1265020891580555, 3.1533267156087925, 3.126714888960123, 3.1552437048494104, 3.4964901162311435, 3.3083898676115844, 3.239437336412569, 3.1754508865997195, 3.0989627748772146, 3.339367899467193, 3.136912971823698, 3.0797946352866434, 3.036053558722848, 3.2145754386715235, 3.285585770250431, 3.2612534192435088, 3.27064662431145, 3.3583319333958483, 3.5242388009404144, 3.513479287159585, 3.2339104942179153, 3.209218488207885, 3.1140881872159385, 3.325180187533122, 3.2437533893666806, 3.5915428258567337, 3.3435719157790853, 3.40440814857859, 3.470358838149834, 3.6095230106175658, 3.2509142009186602, 3.3667373625960733, 3.3732262582385113, 3.564286506779137, 3.5656354919829893, 3.3284315571543717, 3.260800698461632, 3.560891206841916, 3.3597133676964965, 3.440709633774878, 3.1883181355716217, 3.37487226915324, 3.3599131826222655, 3.3362961050256024, 3.434086413189237, 3.4800823674138104, 3.5126700012368106, 3.439505536242255, 3.636287383491262, 3.675086181256033, 3.437311014737047, 3.6200594188246344, 3.395474606370997, 3.3546100807420554, 3.2987626055920765, 3.511514911694186, 3.996386410335877, 3.596944692234198, 3.429103705100715, 3.873718306643977, 3.554826009042916, 3.78750382133183, 3.453399378717655, 3.606225324617255, 3.488779924588189, 3.4545838053205182, 3.5761351382271167, 3.6258976036638377, 3.4904719521956786, 3.6153228608004393, 3.5805770793841, 3.85330153604792, 3.5352890134478607, 3.5736469957711443, 3.541991252895622, 3.395854063598173, 3.547020441586418, 3.505853977675239, 3.616333870394599, 3.7478944361653355, 3.4075984710029195, 3.8425699550924555, 3.778160907904662, 3.504023222607516, 3.7122926005561436, 3.6318109767245397, 3.70049263097878, 3.6493755899635807, 3.682034179523942, 3.677900445292748, 3.6159578472198475, 3.306837811289976, 3.5751741784403013, 3.539638931685615, 3.668143683179681, 3.808920665488889, 3.503044360051198, 3.7061263533929982, 3.411861675037515, 3.507277464050622, 3.523513915017247, 3.9707044743090156, 3.8503918536007404, 3.4603023425720276, 3.7040838855096982, 3.5893142233336612, 3.6315915456396484, 3.8535162845981263, 3.7461006796830114, 3.8367062532564713, 3.7434694008192135, 3.4385227679408024, 3.6855400681185224, 3.775149243528999, 3.5007513645770296, 3.3587885085227236, 3.615598755888641, 3.703418152606381, 3.4344657506971132, 3.6136724160806764, 3.5084833017594756, 3.797592466963189, 3.7207222634128163, 3.777906339659932, 3.672222451839064, 3.662782866419071, 3.6030102597904348, 3.486870839593134, 3.5440094615554525, 3.610860025111054, 4.173500659964269, 3.545875072612294, 3.699252635373601, 4.098421537051244, 3.696695668290236, 3.5385642647743225, 3.595455755302239, 3.3950339024443004, 3.658506683960912, 3.43807132892488, 3.706983281166426, 3.749085840325625, 3.7059740804667984, 3.709313090447159, 3.7107633719930337, 3.643914273518714, 3.7395995674832236, 3.5676250538921783, 3.6902330525308136, 3.7195527854685984, 3.6825626365441297, 3.7712465677676454, 3.5404213483090556, 3.5246248741501143, 3.741245934473617, 3.5659321820629493, 3.5460190171198476, 3.6087321109981056, 3.647144877396169, 3.6912599434810027, 3.6079304057971706, 3.882897051184305, 3.7060524593329145, 3.80068685850572, 3.7492907021549486, 3.5829173280813156, 3.8037141422475025, 3.7134121889691976], 'val_acc': [0.8984546718143281, 0.9046588823908851, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047413070996603, 0.9047459051722572, 0.9047756507283166, 0.9060210472061521, 0.9051190302485511, 0.9048718128885541, 0.9065384722891308, 0.9077060421307882, 0.907474832875388, 0.907376400062016, 0.9060897344634646, 0.9103686099960691, 0.9088255308923268, 0.9110095926693508, 0.9119047636077517, 0.9135073111170814, 0.913301272051675, 0.9122710767246428, 0.9123741047722953, 0.9155700689270383, 0.917021510146913, 0.9201579434531075, 0.9200572229567028, 0.9225709637006124, 0.9261607158751715, 0.9212660363742283, 0.9281226907457624, 0.9277930288087755, 0.9292697140148708, 0.9263736435345241, 0.9262797889255342, 0.929761929171426, 0.9316529291016715, 0.930913473878588, 0.9262797718956357, 0.9310599849337623, 0.9314903730437869, 0.932930398555029, 0.9339674824760074, 0.9329281137103126, 0.9340018289429801, 0.9310691129593622, 0.9283974369366964, 0.9333722335951669, 0.9350961333229428, 0.9374015444800967, 0.9347710524286542, 0.9363713491530645, 0.9355334043502808, 0.93656135173071, 0.9373992397671654, 0.9373168320882888, 0.9379166818800426, 0.9368062927609399, 0.9375595450401306, 0.9362064997355143, 0.9378846060662043, 0.9395696236973717, 0.9393040509451003, 0.9402541291146052, 0.9391964560463315, 0.9374381786301023, 0.9384958885964894, 0.9373214131309873, 0.939887835865929, 0.9403708605539232, 0.9399084448814392, 0.9406799333436149, 0.9389537289029076, 0.937628223782494, 0.9411057773090544, 0.9428090509914216, 0.9408791320664542, 0.939990846883683, 0.9389789359910148, 0.9407600873992557, 0.9406799163137164, 0.9401877323786417, 0.9404166539510092, 0.9379487378256661, 0.9400480701809838, 0.9415247525487628, 0.9414766345705304, 0.9424725004604885, 0.9399153136071705, 0.942763260432652, 0.9424404672213963, 0.9417147664796739, 0.93938186055138, 0.942154313836779, 0.9423306101844424, 0.9427403637341091, 0.9421794953800383, 0.9438346851439703, 0.9427151850291661, 0.9407554950032916, 0.9415659109751383, 0.9430174203146071, 0.9427220764614287, 0.9439331576937721, 0.9423443277676901, 0.9428159083638873, 0.9431135370617821, 0.9425228805769057, 0.941920797030131, 0.9419574084736052, 0.943120402949197, 0.9428434173266093, 0.9435279199055263, 0.9436584086645217, 0.9412156825973874, 0.9446176517577398, 0.94137590272086, 0.9445100625356039, 0.9427953220549083, 0.9429189534414382, 0.942538925579616, 0.9453136438415164, 0.9424381823766799, 0.9430631824902126, 0.9422229954174587, 0.9424679336093721, 0.9430929677827018, 0.9421245569274539, 0.9439079789888292, 0.9427541097005209, 0.9451373503321693, 0.9435004535175505, 0.9440636663209825, 0.9407028612636384, 0.9428365486008781, 0.9420787436621529, 0.9429693278812227, 0.9436240735508147, 0.9450824175562177, 0.9440338696752276, 0.9407394783837455, 0.9439377103533063, 0.9437935125260126, 0.9433333391234988, 0.9421016715821766, 0.9419711317334857, 0.9452884395917257, 0.9417948580923534, 0.9405196791603452, 0.9423534529549735, 0.9431158275831313, 0.9438415794145494, 0.9429601714724586, 0.9444276349885123, 0.9429350012824649, 0.9417857357433864, 0.9444093590690976, 0.9423122576304844, 0.9417399139631362, 0.9414056738217672, 0.9429647468385243, 0.943136473496755, 0.942133693468003, 0.9425549194926307, 0.9419551548503694, 0.943250888869876, 0.9444276548567272, 0.9435370819909232, 0.9421451375597999, 0.9419573971203395, 0.9442032717523121, 0.9433218808401198, 0.9422573276928493, 0.9443452471778506, 0.9427449703216553, 0.9427792940820966, 0.9436286432402474, 0.9436172218549819, 0.9441300545419965, 0.9421199560165405, 0.9426831290835426, 0.9433631017094567, 0.9435462242081052, 0.9436698442413693, 0.9445375289235797, 0.9434523554075331, 0.9395192464192709, 0.9438026774497259, 0.9448260210809254, 0.9429578979810079, 0.9442650931222099, 0.9440178388640994, 0.9426121740114122, 0.9433882747377668, 0.94336998462677, 0.9427266716957092, 0.9415269891421, 0.9446428758757455, 0.9423694837661016, 0.9438187082608541, 0.9439651880945478, 0.9430746067137945, 0.9429097828410921, 0.9413530088606334, 0.9434249344326201, 0.9436446910812741, 0.943163928531465, 0.9436767271586827, 0.9445489843686422, 0.9434798445020404, 0.9425343161537534, 0.9435782858303615, 0.9428250874791827, 0.9435828952562242, 0.9443360709008717, 0.9431272915431431, 0.9424015709332058, 0.9426945958818708, 0.9408630955786932, 0.9418979060082209, 0.943946863923754, 0.9429532715252468, 0.9429280984969366, 0.9435347772779918, 0.941595699105944, 0.9403754359199887, 0.9430517355600992, 0.9413827884764898, 0.9421268275805882, 0.9417788244429088, 0.9437866352853321], 'val_mDice': [0.013081565660069742, 0.013230677572123352, 0.01575503257724146, 0.013163591879198239, 0.00983450359698119, 0.007815678736993245, 0.006447370920795947, 0.006751551840265858, 0.016902544390177354, 0.02924464845342473, 0.03904889418100495, 0.03843593441637322, 0.10468730844912075, 0.06742996245711333, 0.0751271338057926, 0.14735944267539752, 0.16189496693689198, 0.1479383914487525, 0.16206506688502573, 0.12092571262092817, 0.18725913195382982, 0.19275624500144095, 0.22985205338114784, 0.2334781980940274, 0.24296663222568377, 0.26419311788465294, 0.25504352595834506, 0.2455357877271516, 0.26970398452665123, 0.2837749971520333, 0.28388232453947976, 0.2833210474678448, 0.3154060984296458, 0.3225613675479378, 0.31288445980421137, 0.342320042884066, 0.3285644206085375, 0.34210025439304964, 0.31076866831807864, 0.32999201970441, 0.33484463429167155, 0.3446426684302943, 0.34202853661207927, 0.32398271933197975, 0.34391725799512296, 0.3595544633766015, 0.3625739850103855, 0.3773963383975483, 0.36375464498996735, 0.3816291667698395, 0.36835493919040474, 0.34296301947463126, 0.38286547345064936, 0.3924861524608873, 0.4028455202600786, 0.39905087428078767, 0.4041850445348592, 0.3916001282632351, 0.3976677367020221, 0.3976477400532791, 0.4030766247638634, 0.4023500338551544, 0.38821216495264144, 0.39561300689265844, 0.3842857435700439, 0.39760622133811313, 0.41380597225257326, 0.4142894514259838, 0.421315804478668, 0.4179034025541374, 0.41507931763217565, 0.4130889656288283, 0.4000230471470526, 0.4143212217660177, 0.4184598968852134, 0.41964007665713626, 0.4262446482621488, 0.4140065936815171, 0.41939939310153324, 0.4295183608219737, 0.44243423871341203, 0.42584441495793207, 0.42193657035628956, 0.41723922906177385, 0.42486480126778287, 0.41717529154959176, 0.4193012277994837, 0.418021376643862, 0.42349913503442493, 0.42864773387000676, 0.43559828578006654, 0.42838428879068013, 0.4334291223259199, 0.41339127879057613, 0.4341257237607524, 0.4354680228446211, 0.4317155180587655, 0.41624458524442853, 0.4450721155319895, 0.4436288293273676, 0.4381388933176086, 0.43448372841590926, 0.4345434939577466, 0.4508388058415481, 0.4397282896652108, 0.43917850866204217, 0.4494150642837797, 0.444079985221227, 0.4572107967521463, 0.44207689112850596, 0.4483577481337956, 0.450279342631499, 0.4452676315392767, 0.4399021065660885, 0.44264631487783934, 0.4500174591583865, 0.4445144459605217, 0.4461657227504821, 0.44777428004003705, 0.44420045738418895, 0.4550640175030345, 0.4512705543921107, 0.46239618868345306, 0.45057932748681023, 0.43917473778128624, 0.4480175175482318, 0.4619321862146968, 0.4370982822562967, 0.44987073239116443, 0.4443191018487726, 0.4508884283048766, 0.4508268512075856, 0.44790486210868474, 0.46006810416777927, 0.45330874763783957, 0.4570745020395234, 0.45572077358762425, 0.4541872688347385, 0.44579942985659554, 0.4435929365101315, 0.4535048540149416, 0.45170511802037555, 0.45477787050462903, 0.46593822680768515, 0.4623345422248046, 0.44387905512537273, 0.4592259231777418, 0.45154474604697453, 0.46106333569401786, 0.44641837584120886, 0.452255719296989, 0.4590383636809531, 0.45331602987079395, 0.44727720586316927, 0.4490031000404131, 0.4569534659385681, 0.46428782031649635, 0.4630026285137449, 0.46551124858004705, 0.46849281287619043, 0.45676581455128534, 0.47034005395003725, 0.46258989508662907, 0.4452820849560556, 0.4541385308617637, 0.45579118991181966, 0.4680981245778856, 0.46348652208135244, 0.461766893487601, 0.44534560363917125, 0.4575257652572223, 0.4688120981057485, 0.4659759233750048, 0.45848991260642097, 0.4553555516260011, 0.4584526272401923, 0.45803328177758623, 0.45222792668001993, 0.4643563755920955, 0.46392324478143737, 0.45401361691100256, 0.4543275265466599, 0.47169910335824605, 0.47583628942569095, 0.4612581832777886, 0.46397892474418595, 0.46737027505324, 0.46925710922195796, 0.4704642090059462, 0.4642290783425172, 0.4650059260782741, 0.4501086853089787, 0.4702723891962142, 0.46760575171737445, 0.45037104402269634, 0.47066574366319747, 0.46881536341139246, 0.45383525941343533, 0.44754460808776675, 0.46208040522677557, 0.4608287942551431, 0.4390832026089941, 0.4685239023750737, 0.4586956085903304, 0.46877736615992727, 0.4778207941424279, 0.4612967801235971, 0.4732382318803242, 0.45563135260627385, 0.45502318548304693, 0.4621512744398344, 0.46135308256461505, 0.4630126610753082, 0.4751336510692324, 0.4665903860614413, 0.46721918260057765, 0.46862170561438515, 0.4680088733633359, 0.46748609592517215, 0.4695786287387212, 0.4701790398075467, 0.46829424301783246, 0.45568462851501645, 0.4626661553269341, 0.45914546630921815, 0.46439903956793605, 0.46966190742594854, 0.46384393281879877, 0.4748043166030021, 0.4602825159118289, 0.44869463252169745, 0.46856032666705905, 0.459849992678279, 0.4594465948286511, 0.45528727024793625, 0.4739301822015217], 'loss': [300.9453013669969, 68.72457201494196, 28.53885853541065, 17.710167946235416, 13.287988252163577, 10.888219238040028, 9.325057312590506, 8.262401333245005, 7.487820093586239, 6.9040555147759735, 6.371488638183435, 5.93129106098364, 5.55185869596403, 5.241405723916328, 4.946429055022532, 4.697793589177011, 4.500337094085764, 4.321191315249209, 4.169128253599204, 4.011637833391009, 3.874545172870975, 3.7617394972426688, 3.639847057918742, 3.561143982311607, 3.4610365918543566, 3.3830146271529307, 3.3009643000813607, 3.2225724167508476, 3.1527343374560832, 3.0882238112181764, 3.032206797705511, 2.971147711621283, 2.9184679806243943, 2.869267808510799, 2.8287107784531584, 2.785486586847263, 2.7413986961529484, 2.7081299213768317, 2.674859578609007, 2.6396257471252276, 2.6046914270394383, 2.5705399536226947, 2.5441402745472113, 2.501705130286039, 2.48569640990096, 2.4552566666168265, 2.419512356042356, 2.4112567793594426, 2.3644312026989307, 2.348383965578662, 2.32729146495269, 2.307724177136411, 2.2765927993656008, 2.253952972066639, 2.227839831950142, 2.210070128704694, 2.1942060989567786, 2.1720120304922896, 2.158959965290351, 2.1346614064107587, 2.1296449875542027, 2.107911371049427, 2.079613925083676, 2.071602372781192, 2.058813375178297, 2.0399570502319246, 2.0272149840416143, 2.025468621199729, 2.0029780604785548, 1.9807777749519944, 1.9661469441607482, 1.9561714817844698, 1.9509769557228847, 1.9369857569570046, 1.9232557741047125, 1.9111046635716182, 1.9025491179816867, 1.8971688693167732, 1.8807229986994258, 1.873804559247076, 1.8551879974833705, 1.8476257385121895, 1.8412276897812845, 1.8378344059266267, 1.8305330340609929, 1.8155007399826903, 1.8043706707810079, 1.7990357884327433, 1.7948847437309396, 1.788023129924313, 1.773000634794454, 1.7725117407438955, 1.7629522027274376, 1.7485750672995113, 1.7424319679851543, 1.7393348202531638, 1.7395550862015017, 1.7133512171518603, 1.7137643339226236, 1.7108698070497441, 1.695926286683782, 1.6969263575682412, 1.6857245963548726, 1.67597746600851, 1.6751423720245258, 1.6652755991489137, 1.6666589536624583, 1.6587176511162205, 1.6488923302776026, 1.6512000477794326, 1.6466505468431778, 1.6354615725325508, 1.6333704333970007, 1.6271575062358128, 1.622015199818454, 1.617830951916636, 1.6113691032794668, 1.6064182998634429, 1.6038325433214438, 1.5898347093301952, 1.5868355545070054, 1.5887160485526697, 1.5872617524461248, 1.577761724088517, 1.5757064217428998, 1.566909807436678, 1.5670087897113367, 1.55605734704615, 1.5581111780636936, 1.5469031123405297, 1.5468364846897145, 1.5448906605290107, 1.5413936803583146, 1.532848947535872, 1.5377077825592964, 1.538086785294772, 1.5259218616983852, 1.5192150174939427, 1.5166693754639267, 1.522835565849791, 1.5180219091246623, 1.5153777782963447, 1.5088307117712574, 1.5044825253561096, 1.5081375246680446, 1.4994158483503042, 1.4945916621837172, 1.4974706898954577, 1.494083222681924, 1.4809993349887605, 1.4876766345485777, 1.4912347276017275, 1.4862347141910845, 1.474150152203663, 1.4729817057244232, 1.4755034398683262, 1.4654550400455035, 1.4655914859595225, 1.460459521493862, 1.462314958204211, 1.4613667786960223, 1.457110266660665, 1.4583992307439393, 1.447498430538343, 1.4503374524559616, 1.4463778735586093, 1.4522683039689674, 1.4464515502199584, 1.4405768125623044, 1.445890632201096, 1.430950202899607, 1.4334873671887443, 1.4261617315328028, 1.4306361075883567, 1.4296980937000228, 1.4205615259168325, 1.4202218513073248, 1.4145823426161148, 1.4179889826685426, 1.4220013944588348, 1.4156272802275685, 1.4137082083597103, 1.4084036210058097, 1.4022132916925776, 1.408183075408612, 1.4053548910908784, 1.4031329566236863, 1.3994430152809594, 1.3927396008084454, 1.3995129124654473, 1.4004508223243133, 1.3967334827017135, 1.391865801668728, 1.3900335732052407, 1.3841426074217573, 1.383352599002411, 1.3816013544516363, 1.3764930543629263, 1.3782842595130924, 1.381054414865344, 1.3756280757109047, 1.3827685236655058, 1.3790752558274284, 1.3762329487136868, 1.3716180981504036, 1.3715355916820282, 1.3707431069637555, 1.3723928861151962, 1.3690471963338224, 1.3624558424889892, 1.3628090168438323, 1.3701560607599068, 1.3700589976123194, 1.3584447713378125, 1.3494830573250935, 1.3531660112728843, 1.3516863539473087, 1.352423325471159, 1.3533892074898992, 1.3455307711312967, 1.3454681372329957, 1.3535724651475667, 1.3457826319654622, 1.3488427236035585, 1.3432221514930012, 1.3423567575045074, 1.3348860922451766, 1.3358778886673697, 1.3348169683697644, 1.341787508486874, 1.333760990914017, 1.333651710859677, 1.335212522006251, 1.3361846177661167, 1.3367606890507233, 1.3278823032047287], 'acc': [0.3097388428480226, 0.8059195219861397, 0.8611568216760884, 0.8675449944677807, 0.8685659679783685, 0.8687615152818288, 0.8687739818914791, 0.8688470438217551, 0.8689746934829339, 0.8690257184372625, 0.8695100118526954, 0.8706492124919328, 0.8719434265412874, 0.8728679363176454, 0.8738218285409108, 0.8749595430629525, 0.8761398214836077, 0.8771751408106288, 0.8781660619167228, 0.8791371909368239, 0.8802088673275194, 0.8807521742824784, 0.8819699482419529, 0.8830826129898621, 0.8844688690097111, 0.8856494759511276, 0.8874407984030507, 0.88852375484924, 0.8900850268717513, 0.8913133435794469, 0.8925579496033968, 0.8938829132373884, 0.8950148809156092, 0.8963110016700917, 0.8972108551181303, 0.8980336152268485, 0.8989611835821714, 0.8998540629190541, 0.9010074873615924, 0.9018202847383696, 0.9026398056385856, 0.903211454538934, 0.9040930281950371, 0.9049949220478454, 0.9051767955838497, 0.9061501284474832, 0.907051004034167, 0.9073733472745957, 0.908573114299719, 0.9093341460364941, 0.9098407326462596, 0.9103470833769924, 0.9110896697457135, 0.9118553355683797, 0.912565298638455, 0.9133716118986124, 0.9138070764696986, 0.9146619274801479, 0.9149068595127998, 0.9156711794678729, 0.9159803626646259, 0.9167747617985946, 0.9171449591112845, 0.917575813026954, 0.9180631885437555, 0.9184908231306288, 0.918715336935493, 0.9189543534570837, 0.919389382754536, 0.9202848813403703, 0.9206208704936842, 0.9209007644023605, 0.9211070864605541, 0.9214465976083166, 0.9219115169746512, 0.9222964733603466, 0.9224855311693243, 0.9226008815666097, 0.923286095517252, 0.9234308041610629, 0.9238166153281554, 0.9240138988653911, 0.9242519675271921, 0.9244000839548164, 0.9247415398204628, 0.9249852179262894, 0.9255356612016131, 0.9255480138993984, 0.9257655523175883, 0.9257950042745201, 0.9262601549234237, 0.926577931668962, 0.9266705491626931, 0.9270246152545024, 0.927191009440127, 0.9272328621164002, 0.927120126980634, 0.9277778653044082, 0.9278857516879311, 0.9278266603762367, 0.9279849256121494, 0.9281318381339662, 0.9284210677709519, 0.9288186943206985, 0.9288027538300847, 0.9289504039234708, 0.9288311849238902, 0.9292048987727278, 0.9293130186910871, 0.9292164834014065, 0.9294113608162274, 0.9296734586533311, 0.929582481166405, 0.9300632994828115, 0.9300351681588862, 0.9302374144061949, 0.9305673127967727, 0.9304399801996693, 0.9306668346733032, 0.9309756710094043, 0.9311879224944533, 0.9310481276198667, 0.9313388887411462, 0.9313813111031471, 0.9312539165112378, 0.9316762692555541, 0.9315318139343564, 0.9318137036046472, 0.9317874466382357, 0.9320989920267002, 0.9323041342185922, 0.9321648023343164, 0.9322509781367888, 0.9326156987972932, 0.9323303849682864, 0.9322033598975958, 0.9328247346077347, 0.932908176410536, 0.9329675640332715, 0.9329471494251992, 0.932949305051826, 0.9329424226860883, 0.9332299895982464, 0.9333491137497959, 0.9333337501613579, 0.9334939388909826, 0.9338076374929306, 0.9337739675564873, 0.933808005761613, 0.9342068432727393, 0.9339622170198439, 0.9337690085350766, 0.9336725912366227, 0.9341354030553422, 0.9340067574898356, 0.9339746837558971, 0.9342585847583239, 0.9342727667912597, 0.9343031441082907, 0.9343330354986564, 0.9344136713556364, 0.9346723199947873, 0.9345463078583232, 0.9351182601023024, 0.9349881739935388, 0.9349926420896297, 0.9349469303372326, 0.9350335924340876, 0.9351624042192176, 0.9352573154518226, 0.9355065743657052, 0.9356084127680792, 0.9357369222093093, 0.9356357074206749, 0.9356469506355111, 0.9358515750242608, 0.936007640587932, 0.9360331039518801, 0.9361092728044293, 0.9360902244607584, 0.9361411335727073, 0.9362150970074352, 0.9364390983870201, 0.9366870824486758, 0.9365636245326912, 0.9367473562696386, 0.9368411782667175, 0.9369213540751234, 0.9372647340320222, 0.9371217417767541, 0.9370847828521052, 0.937156429197923, 0.9374027260011254, 0.9373698215712604, 0.9375949604722239, 0.9376715427766951, 0.9377441157466395, 0.9378661575265902, 0.9378390951677855, 0.9378385859715219, 0.9380942177446173, 0.9378376365374435, 0.9380138384622486, 0.9381117370401202, 0.9382118399901176, 0.9381726532398862, 0.9382979046386033, 0.9383234124848303, 0.9384084966386332, 0.9385368015257315, 0.9387008833338214, 0.9383413264442092, 0.9384262934916048, 0.9388045064281907, 0.9392167076178316, 0.9391304378061825, 0.9390912565372342, 0.9390216944579424, 0.9391898065719252, 0.9396538649516366, 0.9394079450469773, 0.9393429740076927, 0.9395380330747645, 0.9393889690056826, 0.9396615373231046, 0.9396753040195683, 0.939905909471988, 0.9398446389423162, 0.9399329522153833, 0.9398648000439169, 0.9398854732030799, 0.940206076888788, 0.9400738789117334, 0.9401900192113287, 0.9401068548281022, 0.9403723315142795], 'mDice': [0.015400196236551256, 0.014670600532172548, 0.017341701355508717, 0.022687854227350125, 0.02698345091039502, 0.03143913440258374, 0.036654910108235066, 0.04321043451604113, 0.05072027146356607, 0.05837380767072559, 0.06990514096380407, 0.08229041976323816, 0.09642598852558312, 0.1102882796384614, 0.12493271459233722, 0.13970158621596995, 0.1527544181655316, 0.1651662992637052, 0.17663271950691772, 0.19021672148890226, 0.2020202839629646, 0.21330568464431593, 0.22536039212224845, 0.23366060304301586, 0.24564158551629073, 0.2538379033923678, 0.2643751722207666, 0.27419944992639067, 0.28316824136728125, 0.2909719589957617, 0.298188290402727, 0.30748071538613714, 0.3148638326778838, 0.32211991760205366, 0.32799919040671477, 0.3345450718542595, 0.3412307331459541, 0.34660427442101677, 0.3522739637695527, 0.35816384094952386, 0.3637505573460609, 0.3695501786379633, 0.37464705663470144, 0.3820963004937585, 0.38485553977874193, 0.39128723758388995, 0.3977804712840581, 0.4006384478636691, 0.409123994348273, 0.4130359972249526, 0.41763582601583094, 0.42125076148030566, 0.4269445382700689, 0.43203199036736833, 0.43677658796976915, 0.4409780492609306, 0.444924320447461, 0.44893175809972197, 0.45215139843283625, 0.4565314091318302, 0.4587771071791534, 0.4631187009609157, 0.46840696689800787, 0.4705736271351869, 0.47341611556219004, 0.47747773082586803, 0.4803928241050379, 0.4816990466976938, 0.4847857249490967, 0.4909210186519902, 0.4938716286520795, 0.49652875685521763, 0.4970854876621486, 0.5003047736825937, 0.5035420343989417, 0.5059554769235791, 0.5084895735327531, 0.5093873950794802, 0.5136330289856326, 0.5152486066540611, 0.5185178684917102, 0.5208447246692165, 0.5222649696545724, 0.52328661375476, 0.5249516232891167, 0.5279039674028625, 0.5306399522407772, 0.5315573396930029, 0.532868028640839, 0.5346816590647259, 0.5390378986798616, 0.538670786548171, 0.5406509027422244, 0.5440906647521682, 0.5452949630287954, 0.5457543416797299, 0.5459080239079558, 0.5521265174557943, 0.5528624157742864, 0.5527423723184789, 0.55658552139647, 0.5561274389138634, 0.5598269266408373, 0.5604756488858884, 0.5620142162156422, 0.5633904485429485, 0.5634676592299904, 0.5651714658148835, 0.567412655960739, 0.5672180764772126, 0.5687949489439432, 0.570570230070327, 0.5712436980219368, 0.573338122790274, 0.5738789035746742, 0.5758331914340598, 0.5772197376746352, 0.5785810309429033, 0.5787646085948919, 0.5818456070993547, 0.5826416534680678, 0.582776450825859, 0.5831734013736696, 0.5850165352348834, 0.5854608159270249, 0.5883902140681353, 0.5879305861624583, 0.5911002461680884, 0.5900576813661779, 0.5926298647573143, 0.5931292024218602, 0.593411714648704, 0.5944980897723261, 0.5962035435115255, 0.5950747686175223, 0.5948689780484419, 0.5980700661801915, 0.5991381971574551, 0.5995289287241709, 0.5989531509122891, 0.5995491778135805, 0.6007421619174612, 0.6028132596778024, 0.6026472358522512, 0.6023958227596279, 0.6040000172738788, 0.6058500598730415, 0.6053195913931481, 0.6061702235047932, 0.6088205555373818, 0.6073825472187669, 0.606474005810576, 0.6073117753501938, 0.6112009159098799, 0.6112003459023997, 0.6108070927960624, 0.6129823188021483, 0.6131306527229502, 0.6140592185339073, 0.614692545543867, 0.6137884232794271, 0.6148234503288041, 0.6148344800723686, 0.6182811586112674, 0.6172663451206346, 0.6179934494385536, 0.6170071480658924, 0.6176132371091466, 0.6191130009648058, 0.6184816624770303, 0.6214068211271787, 0.621523637704498, 0.6227372258252346, 0.6225949846896128, 0.6225080320732612, 0.6247619551134082, 0.6253124914556334, 0.6263363426283695, 0.6256449250488215, 0.6242271473992875, 0.626387432163015, 0.626182061205118, 0.6280289665913108, 0.6301163269739425, 0.6280841268692399, 0.6295002317161994, 0.6294017888480629, 0.629792167325918, 0.632292194519977, 0.631273164141493, 0.6304464088553566, 0.6314112738030527, 0.6323329077213744, 0.6333268223779611, 0.6344561797703349, 0.6346329348795626, 0.6354143264690345, 0.6362268345328869, 0.6363116800842521, 0.6358893966086457, 0.6365732479031109, 0.6351904969052679, 0.636270816279717, 0.6372586343705321, 0.6375739835051872, 0.6381368827953122, 0.6381665750668645, 0.6381971909403686, 0.6386219138857347, 0.640775185996957, 0.640686860129579, 0.638791297657402, 0.6383553096869788, 0.6420154793887141, 0.6438151749889814, 0.6430296530515811, 0.6434407592371338, 0.6432378203257076, 0.6427029953863388, 0.6455636687515006, 0.6446770046495945, 0.6436225945535413, 0.6451865321343773, 0.644906559522749, 0.645513682919751, 0.6458749694357218, 0.6479942178206796, 0.6475099840558561, 0.647738562272375, 0.6466364421802195, 0.6480040563331856, 0.6483947144484092, 0.6479710206053311, 0.6478724918063031, 0.6480948851704712, 0.6486651289088431]}
predicting test subjects:   0%|          | 0/3 [00:00<?, ?it/s]predicting test subjects:  33%|███▎      | 1/3 [00:02<00:04,  2.44s/it]predicting test subjects:  67%|██████▋   | 2/3 [00:03<00:02,  2.17s/it]predicting test subjects: 100%|██████████| 3/3 [00:05<00:00,  1.91s/it]
predicting train subjects:   0%|          | 0/285 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/285 [00:01<06:12,  1.31s/it]predicting train subjects:   1%|          | 2/285 [00:02<06:36,  1.40s/it]predicting train subjects:   1%|          | 3/285 [00:04<06:40,  1.42s/it]predicting train subjects:   1%|▏         | 4/285 [00:06<07:19,  1.56s/it]predicting train subjects:   2%|▏         | 5/285 [00:07<07:05,  1.52s/it]predicting train subjects:   2%|▏         | 6/285 [00:09<07:39,  1.65s/it]predicting train subjects:   2%|▏         | 7/285 [00:11<08:10,  1.77s/it]predicting train subjects:   3%|▎         | 8/285 [00:13<08:22,  1.81s/it]predicting train subjects:   3%|▎         | 9/285 [00:15<08:02,  1.75s/it]predicting train subjects:   4%|▎         | 10/285 [00:17<08:24,  1.83s/it]predicting train subjects:   4%|▍         | 11/285 [00:19<08:23,  1.84s/it]predicting train subjects:   4%|▍         | 12/285 [00:20<08:23,  1.84s/it]predicting train subjects:   5%|▍         | 13/285 [00:22<08:29,  1.87s/it]predicting train subjects:   5%|▍         | 14/285 [00:24<08:31,  1.89s/it]predicting train subjects:   5%|▌         | 15/285 [00:26<08:33,  1.90s/it]predicting train subjects:   6%|▌         | 16/285 [00:28<08:35,  1.92s/it]predicting train subjects:   6%|▌         | 17/285 [00:30<08:30,  1.91s/it]predicting train subjects:   6%|▋         | 18/285 [00:32<08:34,  1.93s/it]predicting train subjects:   7%|▋         | 19/285 [00:34<08:58,  2.03s/it]predicting train subjects:   7%|▋         | 20/285 [00:36<08:59,  2.03s/it]predicting train subjects:   7%|▋         | 21/285 [00:38<08:50,  2.01s/it]predicting train subjects:   8%|▊         | 22/285 [00:40<08:41,  1.98s/it]predicting train subjects:   8%|▊         | 23/285 [00:42<08:39,  1.98s/it]predicting train subjects:   8%|▊         | 24/285 [00:44<08:31,  1.96s/it]predicting train subjects:   9%|▉         | 25/285 [00:46<08:26,  1.95s/it]predicting train subjects:   9%|▉         | 26/285 [00:48<08:28,  1.96s/it]predicting train subjects:   9%|▉         | 27/285 [00:50<08:25,  1.96s/it]predicting train subjects:  10%|▉         | 28/285 [00:52<08:25,  1.97s/it]predicting train subjects:  10%|█         | 29/285 [00:54<08:11,  1.92s/it]predicting train subjects:  11%|█         | 30/285 [00:56<08:13,  1.94s/it]predicting train subjects:  11%|█         | 31/285 [00:58<08:07,  1.92s/it]predicting train subjects:  11%|█         | 32/285 [00:59<07:56,  1.88s/it]predicting train subjects:  12%|█▏        | 33/285 [01:01<07:46,  1.85s/it]predicting train subjects:  12%|█▏        | 34/285 [01:03<07:37,  1.82s/it]predicting train subjects:  12%|█▏        | 35/285 [01:05<07:45,  1.86s/it]predicting train subjects:  13%|█▎        | 36/285 [01:07<07:36,  1.83s/it]predicting train subjects:  13%|█▎        | 37/285 [01:09<07:42,  1.87s/it]predicting train subjects:  13%|█▎        | 38/285 [01:10<07:37,  1.85s/it]predicting train subjects:  14%|█▎        | 39/285 [01:12<07:37,  1.86s/it]predicting train subjects:  14%|█▍        | 40/285 [01:14<07:31,  1.84s/it]predicting train subjects:  14%|█▍        | 41/285 [01:16<07:28,  1.84s/it]predicting train subjects:  15%|█▍        | 42/285 [01:18<07:26,  1.84s/it]predicting train subjects:  15%|█▌        | 43/285 [01:20<07:23,  1.83s/it]predicting train subjects:  15%|█▌        | 44/285 [01:21<07:21,  1.83s/it]predicting train subjects:  16%|█▌        | 45/285 [01:23<07:17,  1.82s/it]predicting train subjects:  16%|█▌        | 46/285 [01:25<06:59,  1.76s/it]predicting train subjects:  16%|█▋        | 47/285 [01:26<06:43,  1.70s/it]predicting train subjects:  17%|█▋        | 48/285 [01:28<06:30,  1.65s/it]predicting train subjects:  17%|█▋        | 49/285 [01:30<06:23,  1.63s/it]predicting train subjects:  18%|█▊        | 50/285 [01:31<06:19,  1.62s/it]predicting train subjects:  18%|█▊        | 51/285 [01:33<06:16,  1.61s/it]predicting train subjects:  18%|█▊        | 52/285 [01:34<06:13,  1.60s/it]predicting train subjects:  19%|█▊        | 53/285 [01:36<06:09,  1.59s/it]predicting train subjects:  19%|█▉        | 54/285 [01:37<06:00,  1.56s/it]predicting train subjects:  19%|█▉        | 55/285 [01:39<05:56,  1.55s/it]predicting train subjects:  20%|█▉        | 56/285 [01:40<05:51,  1.53s/it]predicting train subjects:  20%|██        | 57/285 [01:42<05:52,  1.55s/it]predicting train subjects:  20%|██        | 58/285 [01:43<05:47,  1.53s/it]predicting train subjects:  21%|██        | 59/285 [01:45<05:50,  1.55s/it]predicting train subjects:  21%|██        | 60/285 [01:47<05:50,  1.56s/it]predicting train subjects:  21%|██▏       | 61/285 [01:48<05:52,  1.57s/it]predicting train subjects:  22%|██▏       | 62/285 [01:50<05:51,  1.58s/it]predicting train subjects:  22%|██▏       | 63/285 [01:51<05:52,  1.59s/it]predicting train subjects:  22%|██▏       | 64/285 [01:53<05:59,  1.63s/it]predicting train subjects:  23%|██▎       | 65/285 [01:55<06:10,  1.68s/it]predicting train subjects:  23%|██▎       | 66/285 [01:57<06:16,  1.72s/it]predicting train subjects:  24%|██▎       | 67/285 [01:58<06:10,  1.70s/it]predicting train subjects:  24%|██▍       | 68/285 [02:00<06:00,  1.66s/it]predicting train subjects:  24%|██▍       | 69/285 [02:02<05:57,  1.66s/it]predicting train subjects:  25%|██▍       | 70/285 [02:03<06:03,  1.69s/it]predicting train subjects:  25%|██▍       | 71/285 [02:05<05:59,  1.68s/it]predicting train subjects:  25%|██▌       | 72/285 [02:07<05:55,  1.67s/it]predicting train subjects:  26%|██▌       | 73/285 [02:08<05:52,  1.66s/it]predicting train subjects:  26%|██▌       | 74/285 [02:10<05:45,  1.64s/it]predicting train subjects:  26%|██▋       | 75/285 [02:12<05:41,  1.62s/it]predicting train subjects:  27%|██▋       | 76/285 [02:13<05:37,  1.62s/it]predicting train subjects:  27%|██▋       | 77/285 [02:15<05:35,  1.61s/it]predicting train subjects:  27%|██▋       | 78/285 [02:16<05:34,  1.62s/it]predicting train subjects:  28%|██▊       | 79/285 [02:18<05:32,  1.62s/it]predicting train subjects:  28%|██▊       | 80/285 [02:20<05:27,  1.60s/it]predicting train subjects:  28%|██▊       | 81/285 [02:21<05:28,  1.61s/it]predicting train subjects:  29%|██▉       | 82/285 [02:23<05:30,  1.63s/it]predicting train subjects:  29%|██▉       | 83/285 [02:25<05:33,  1.65s/it]predicting train subjects:  29%|██▉       | 84/285 [02:26<05:31,  1.65s/it]predicting train subjects:  30%|██▉       | 85/285 [02:28<05:43,  1.72s/it]predicting train subjects:  30%|███       | 86/285 [02:30<05:49,  1.76s/it]predicting train subjects:  31%|███       | 87/285 [02:32<05:55,  1.79s/it]predicting train subjects:  31%|███       | 88/285 [02:34<06:00,  1.83s/it]predicting train subjects:  31%|███       | 89/285 [02:36<06:01,  1.85s/it]predicting train subjects:  32%|███▏      | 90/285 [02:37<05:58,  1.84s/it]predicting train subjects:  32%|███▏      | 91/285 [02:39<05:56,  1.84s/it]predicting train subjects:  32%|███▏      | 92/285 [02:41<05:46,  1.80s/it]predicting train subjects:  33%|███▎      | 93/285 [02:43<05:46,  1.81s/it]predicting train subjects:  33%|███▎      | 94/285 [02:45<05:45,  1.81s/it]predicting train subjects:  33%|███▎      | 95/285 [02:46<05:45,  1.82s/it]predicting train subjects:  34%|███▎      | 96/285 [02:48<05:47,  1.84s/it]predicting train subjects:  34%|███▍      | 97/285 [02:50<05:45,  1.84s/it]predicting train subjects:  34%|███▍      | 98/285 [02:52<05:43,  1.84s/it]predicting train subjects:  35%|███▍      | 99/285 [02:54<05:46,  1.86s/it]predicting train subjects:  35%|███▌      | 100/285 [02:56<05:46,  1.87s/it]predicting train subjects:  35%|███▌      | 101/285 [02:58<05:47,  1.89s/it]predicting train subjects:  36%|███▌      | 102/285 [03:00<05:45,  1.89s/it]predicting train subjects:  36%|███▌      | 103/285 [03:02<05:45,  1.90s/it]predicting train subjects:  36%|███▋      | 104/285 [03:03<05:37,  1.86s/it]predicting train subjects:  37%|███▋      | 105/285 [03:05<05:35,  1.87s/it]predicting train subjects:  37%|███▋      | 106/285 [03:07<05:30,  1.85s/it]predicting train subjects:  38%|███▊      | 107/285 [03:09<05:25,  1.83s/it]predicting train subjects:  38%|███▊      | 108/285 [03:11<05:25,  1.84s/it]predicting train subjects:  38%|███▊      | 109/285 [03:12<05:19,  1.81s/it]predicting train subjects:  39%|███▊      | 110/285 [03:14<05:21,  1.84s/it]predicting train subjects:  39%|███▉      | 111/285 [03:16<05:26,  1.88s/it]predicting train subjects:  39%|███▉      | 112/285 [03:18<05:21,  1.86s/it]predicting train subjects:  40%|███▉      | 113/285 [03:20<05:17,  1.84s/it]predicting train subjects:  40%|████      | 114/285 [03:22<05:17,  1.86s/it]predicting train subjects:  40%|████      | 115/285 [03:24<05:22,  1.90s/it]predicting train subjects:  41%|████      | 116/285 [03:26<05:15,  1.87s/it]predicting train subjects:  41%|████      | 117/285 [03:27<05:10,  1.85s/it]predicting train subjects:  41%|████▏     | 118/285 [03:29<05:06,  1.83s/it]predicting train subjects:  42%|████▏     | 119/285 [03:31<05:16,  1.91s/it]predicting train subjects:  42%|████▏     | 120/285 [03:33<05:10,  1.88s/it]predicting train subjects:  42%|████▏     | 121/285 [03:35<05:01,  1.84s/it]predicting train subjects:  43%|████▎     | 122/285 [03:36<04:51,  1.79s/it]predicting train subjects:  43%|████▎     | 123/285 [03:38<04:44,  1.75s/it]predicting train subjects:  44%|████▎     | 124/285 [03:40<04:37,  1.72s/it]predicting train subjects:  44%|████▍     | 125/285 [03:41<04:32,  1.70s/it]predicting train subjects:  44%|████▍     | 126/285 [03:43<04:31,  1.70s/it]predicting train subjects:  45%|████▍     | 127/285 [03:45<04:28,  1.70s/it]predicting train subjects:  45%|████▍     | 128/285 [03:47<04:25,  1.69s/it]predicting train subjects:  45%|████▌     | 129/285 [03:48<04:22,  1.68s/it]predicting train subjects:  46%|████▌     | 130/285 [03:50<04:23,  1.70s/it]predicting train subjects:  46%|████▌     | 131/285 [03:52<04:24,  1.72s/it]predicting train subjects:  46%|████▋     | 132/285 [03:53<04:23,  1.72s/it]predicting train subjects:  47%|████▋     | 133/285 [03:55<04:19,  1.71s/it]predicting train subjects:  47%|████▋     | 134/285 [03:57<04:25,  1.76s/it]predicting train subjects:  47%|████▋     | 135/285 [03:59<04:21,  1.75s/it]predicting train subjects:  48%|████▊     | 136/285 [04:00<04:17,  1.73s/it]predicting train subjects:  48%|████▊     | 137/285 [04:02<04:11,  1.70s/it]predicting train subjects:  48%|████▊     | 138/285 [04:04<04:08,  1.69s/it]predicting train subjects:  49%|████▉     | 139/285 [04:05<04:12,  1.73s/it]predicting train subjects:  49%|████▉     | 140/285 [04:07<04:14,  1.75s/it]predicting train subjects:  49%|████▉     | 141/285 [04:09<04:08,  1.73s/it]predicting train subjects:  50%|████▉     | 142/285 [04:11<04:04,  1.71s/it]predicting train subjects:  50%|█████     | 143/285 [04:12<03:50,  1.63s/it]predicting train subjects:  51%|█████     | 144/285 [04:14<03:42,  1.58s/it]predicting train subjects:  51%|█████     | 145/285 [04:15<03:40,  1.58s/it]predicting train subjects:  51%|█████     | 146/285 [04:17<03:32,  1.53s/it]predicting train subjects:  52%|█████▏    | 147/285 [04:18<03:32,  1.54s/it]predicting train subjects:  52%|█████▏    | 148/285 [04:20<03:34,  1.56s/it]predicting train subjects:  52%|█████▏    | 149/285 [04:21<03:32,  1.57s/it]predicting train subjects:  53%|█████▎    | 150/285 [04:23<03:29,  1.55s/it]predicting train subjects:  53%|█████▎    | 151/285 [04:24<03:29,  1.57s/it]predicting train subjects:  53%|█████▎    | 152/285 [04:26<03:29,  1.57s/it]predicting train subjects:  54%|█████▎    | 153/285 [04:28<03:26,  1.57s/it]predicting train subjects:  54%|█████▍    | 154/285 [04:29<03:21,  1.54s/it]predicting train subjects:  54%|█████▍    | 155/285 [04:30<03:16,  1.51s/it]predicting train subjects:  55%|█████▍    | 156/285 [04:32<03:14,  1.51s/it]predicting train subjects:  55%|█████▌    | 157/285 [04:34<03:15,  1.53s/it]predicting train subjects:  55%|█████▌    | 158/285 [04:35<03:10,  1.50s/it]predicting train subjects:  56%|█████▌    | 159/285 [04:37<03:12,  1.53s/it]predicting train subjects:  56%|█████▌    | 160/285 [04:38<03:13,  1.55s/it]predicting train subjects:  56%|█████▋    | 161/285 [04:40<03:07,  1.51s/it]predicting train subjects:  57%|█████▋    | 162/285 [04:41<03:05,  1.51s/it]predicting train subjects:  57%|█████▋    | 163/285 [04:43<03:05,  1.52s/it]predicting train subjects:  58%|█████▊    | 164/285 [04:44<03:06,  1.54s/it]predicting train subjects:  58%|█████▊    | 165/285 [04:46<03:00,  1.51s/it]predicting train subjects:  58%|█████▊    | 166/285 [04:47<03:00,  1.51s/it]predicting train subjects:  59%|█████▊    | 167/285 [04:49<02:59,  1.52s/it]predicting train subjects:  59%|█████▉    | 168/285 [04:50<02:57,  1.51s/it]predicting train subjects:  59%|█████▉    | 169/285 [04:52<02:56,  1.52s/it]predicting train subjects:  60%|█████▉    | 170/285 [04:53<02:52,  1.50s/it]predicting train subjects:  60%|██████    | 171/285 [04:55<02:48,  1.48s/it]predicting train subjects:  60%|██████    | 172/285 [04:56<02:45,  1.46s/it]predicting train subjects:  61%|██████    | 173/285 [04:58<02:45,  1.47s/it]predicting train subjects:  61%|██████    | 174/285 [04:59<02:48,  1.52s/it]predicting train subjects:  61%|██████▏   | 175/285 [05:01<02:48,  1.53s/it]predicting train subjects:  62%|██████▏   | 176/285 [05:02<02:47,  1.54s/it]predicting train subjects:  62%|██████▏   | 177/285 [05:04<02:44,  1.52s/it]predicting train subjects:  62%|██████▏   | 178/285 [05:05<02:36,  1.47s/it]predicting train subjects:  63%|██████▎   | 179/285 [05:07<02:39,  1.51s/it]predicting train subjects:  63%|██████▎   | 180/285 [05:08<02:38,  1.51s/it]predicting train subjects:  64%|██████▎   | 181/285 [05:10<02:38,  1.52s/it]predicting train subjects:  64%|██████▍   | 182/285 [05:11<02:38,  1.54s/it]predicting train subjects:  64%|██████▍   | 183/285 [05:13<02:35,  1.53s/it]predicting train subjects:  65%|██████▍   | 184/285 [05:14<02:33,  1.52s/it]predicting train subjects:  65%|██████▍   | 185/285 [05:16<02:33,  1.53s/it]predicting train subjects:  65%|██████▌   | 186/285 [05:17<02:32,  1.54s/it]predicting train subjects:  66%|██████▌   | 187/285 [05:19<02:32,  1.55s/it]predicting train subjects:  66%|██████▌   | 188/285 [05:21<02:28,  1.53s/it]predicting train subjects:  66%|██████▋   | 189/285 [05:22<02:24,  1.50s/it]predicting train subjects:  67%|██████▋   | 190/285 [05:23<02:22,  1.50s/it]predicting train subjects:  67%|██████▋   | 191/285 [05:25<02:20,  1.49s/it]predicting train subjects:  67%|██████▋   | 192/285 [05:26<02:18,  1.48s/it]predicting train subjects:  68%|██████▊   | 193/285 [05:28<02:20,  1.53s/it]predicting train subjects:  68%|██████▊   | 194/285 [05:30<02:17,  1.51s/it]predicting train subjects:  68%|██████▊   | 195/285 [05:31<02:16,  1.52s/it]predicting train subjects:  69%|██████▉   | 196/285 [05:33<02:19,  1.56s/it]predicting train subjects:  69%|██████▉   | 197/285 [05:34<02:22,  1.62s/it]predicting train subjects:  69%|██████▉   | 198/285 [05:36<02:25,  1.67s/it]predicting train subjects:  70%|██████▉   | 199/285 [05:38<02:25,  1.69s/it]predicting train subjects:  70%|███████   | 200/285 [05:40<02:22,  1.68s/it]predicting train subjects:  71%|███████   | 201/285 [05:41<02:23,  1.71s/it]predicting train subjects:  71%|███████   | 202/285 [05:43<02:25,  1.76s/it]predicting train subjects:  71%|███████   | 203/285 [05:45<02:25,  1.77s/it]predicting train subjects:  72%|███████▏  | 204/285 [05:47<02:24,  1.79s/it]predicting train subjects:  72%|███████▏  | 205/285 [05:49<02:21,  1.77s/it]predicting train subjects:  72%|███████▏  | 206/285 [05:50<02:17,  1.74s/it]predicting train subjects:  73%|███████▎  | 207/285 [05:52<02:20,  1.80s/it]predicting train subjects:  73%|███████▎  | 208/285 [05:54<02:18,  1.80s/it]predicting train subjects:  73%|███████▎  | 209/285 [05:56<02:18,  1.82s/it]predicting train subjects:  74%|███████▎  | 210/285 [05:58<02:14,  1.79s/it]predicting train subjects:  74%|███████▍  | 211/285 [05:59<02:12,  1.79s/it]predicting train subjects:  74%|███████▍  | 212/285 [06:01<02:08,  1.76s/it]predicting train subjects:  75%|███████▍  | 213/285 [06:03<02:03,  1.72s/it]predicting train subjects:  75%|███████▌  | 214/285 [06:04<01:57,  1.65s/it]predicting train subjects:  75%|███████▌  | 215/285 [06:06<01:51,  1.59s/it]predicting train subjects:  76%|███████▌  | 216/285 [06:07<01:48,  1.58s/it]predicting train subjects:  76%|███████▌  | 217/285 [06:09<01:47,  1.58s/it]predicting train subjects:  76%|███████▋  | 218/285 [06:10<01:44,  1.55s/it]predicting train subjects:  77%|███████▋  | 219/285 [06:12<01:43,  1.57s/it]predicting train subjects:  77%|███████▋  | 220/285 [06:13<01:41,  1.56s/it]predicting train subjects:  78%|███████▊  | 221/285 [06:15<01:40,  1.56s/it]predicting train subjects:  78%|███████▊  | 222/285 [06:17<01:36,  1.54s/it]predicting train subjects:  78%|███████▊  | 223/285 [06:18<01:34,  1.52s/it]predicting train subjects:  79%|███████▊  | 224/285 [06:19<01:31,  1.51s/it]predicting train subjects:  79%|███████▉  | 225/285 [06:21<01:30,  1.50s/it]predicting train subjects:  79%|███████▉  | 226/285 [06:22<01:29,  1.51s/it]predicting train subjects:  80%|███████▉  | 227/285 [06:24<01:28,  1.52s/it]predicting train subjects:  80%|████████  | 228/285 [06:26<01:26,  1.53s/it]predicting train subjects:  80%|████████  | 229/285 [06:27<01:26,  1.54s/it]predicting train subjects:  81%|████████  | 230/285 [06:29<01:23,  1.53s/it]predicting train subjects:  81%|████████  | 231/285 [06:30<01:21,  1.50s/it]predicting train subjects:  81%|████████▏ | 232/285 [06:32<01:26,  1.64s/it]predicting train subjects:  82%|████████▏ | 233/285 [06:34<01:29,  1.72s/it]predicting train subjects:  82%|████████▏ | 234/285 [06:36<01:30,  1.77s/it]predicting train subjects:  82%|████████▏ | 235/285 [06:38<01:29,  1.79s/it]predicting train subjects:  83%|████████▎ | 236/285 [06:40<01:29,  1.83s/it]predicting train subjects:  83%|████████▎ | 237/285 [06:42<01:29,  1.86s/it]predicting train subjects:  84%|████████▎ | 238/285 [06:43<01:28,  1.87s/it]predicting train subjects:  84%|████████▍ | 239/285 [06:45<01:26,  1.88s/it]predicting train subjects:  84%|████████▍ | 240/285 [06:47<01:25,  1.89s/it]predicting train subjects:  85%|████████▍ | 241/285 [06:49<01:23,  1.90s/it]predicting train subjects:  85%|████████▍ | 242/285 [06:51<01:21,  1.89s/it]predicting train subjects:  85%|████████▌ | 243/285 [06:53<01:18,  1.88s/it]predicting train subjects:  86%|████████▌ | 244/285 [06:55<01:17,  1.88s/it]predicting train subjects:  86%|████████▌ | 245/285 [06:57<01:16,  1.90s/it]predicting train subjects:  86%|████████▋ | 246/285 [06:59<01:12,  1.87s/it]predicting train subjects:  87%|████████▋ | 247/285 [07:01<01:13,  1.93s/it]predicting train subjects:  87%|████████▋ | 248/285 [07:02<01:11,  1.93s/it]predicting train subjects:  87%|████████▋ | 249/285 [07:04<01:09,  1.94s/it]predicting train subjects:  88%|████████▊ | 250/285 [07:06<01:03,  1.82s/it]predicting train subjects:  88%|████████▊ | 251/285 [07:07<00:57,  1.70s/it]predicting train subjects:  88%|████████▊ | 252/285 [07:09<00:53,  1.62s/it]predicting train subjects:  89%|████████▉ | 253/285 [07:10<00:50,  1.58s/it]predicting train subjects:  89%|████████▉ | 254/285 [07:12<00:47,  1.54s/it]predicting train subjects:  89%|████████▉ | 255/285 [07:13<00:45,  1.51s/it]predicting train subjects:  90%|████████▉ | 256/285 [07:15<00:43,  1.49s/it]predicting train subjects:  90%|█████████ | 257/285 [07:16<00:41,  1.48s/it]predicting train subjects:  91%|█████████ | 258/285 [07:18<00:40,  1.49s/it]predicting train subjects:  91%|█████████ | 259/285 [07:19<00:38,  1.49s/it]predicting train subjects:  91%|█████████ | 260/285 [07:21<00:37,  1.50s/it]predicting train subjects:  92%|█████████▏| 261/285 [07:22<00:35,  1.48s/it]predicting train subjects:  92%|█████████▏| 262/285 [07:24<00:33,  1.47s/it]predicting train subjects:  92%|█████████▏| 263/285 [07:25<00:32,  1.48s/it]predicting train subjects:  93%|█████████▎| 264/285 [07:27<00:31,  1.48s/it]predicting train subjects:  93%|█████████▎| 265/285 [07:28<00:29,  1.47s/it]predicting train subjects:  93%|█████████▎| 266/285 [07:29<00:27,  1.47s/it]predicting train subjects:  94%|█████████▎| 267/285 [07:31<00:26,  1.46s/it]predicting train subjects:  94%|█████████▍| 268/285 [07:33<00:27,  1.63s/it]predicting train subjects:  94%|█████████▍| 269/285 [07:35<00:27,  1.71s/it]predicting train subjects:  95%|█████████▍| 270/285 [07:37<00:26,  1.77s/it]predicting train subjects:  95%|█████████▌| 271/285 [07:39<00:25,  1.83s/it]predicting train subjects:  95%|█████████▌| 272/285 [07:41<00:24,  1.89s/it]predicting train subjects:  96%|█████████▌| 273/285 [07:43<00:22,  1.89s/it]predicting train subjects:  96%|█████████▌| 274/285 [07:45<00:21,  1.92s/it]predicting train subjects:  96%|█████████▋| 275/285 [07:47<00:19,  1.93s/it]predicting train subjects:  97%|█████████▋| 276/285 [07:49<00:17,  1.95s/it]predicting train subjects:  97%|█████████▋| 277/285 [07:50<00:15,  1.94s/it]predicting train subjects:  98%|█████████▊| 278/285 [07:52<00:13,  1.94s/it]predicting train subjects:  98%|█████████▊| 279/285 [07:54<00:11,  1.94s/it]predicting train subjects:  98%|█████████▊| 280/285 [07:56<00:09,  1.96s/it]predicting train subjects:  99%|█████████▊| 281/285 [07:58<00:07,  1.95s/it]predicting train subjects:  99%|█████████▉| 282/285 [08:00<00:05,  1.95s/it]predicting train subjects:  99%|█████████▉| 283/285 [08:02<00:03,  1.95s/it]predicting train subjects: 100%|█████████▉| 284/285 [08:04<00:01,  1.97s/it]predicting train subjects: 100%|██████████| 285/285 [08:06<00:00,  1.95s/it]
Loading train:   0%|          | 0/285 [00:00<?, ?it/s]Loading train:   0%|          | 1/285 [00:01<07:03,  1.49s/it]Loading train:   1%|          | 2/285 [00:02<07:03,  1.50s/it]Loading train:   1%|          | 3/285 [00:04<06:53,  1.47s/it]Loading train:   1%|▏         | 4/285 [00:06<07:09,  1.53s/it]Loading train:   2%|▏         | 5/285 [00:07<06:47,  1.46s/it]Loading train:   2%|▏         | 6/285 [00:08<06:58,  1.50s/it]Loading train:   2%|▏         | 7/285 [00:10<07:22,  1.59s/it]Loading train:   3%|▎         | 8/285 [00:12<07:28,  1.62s/it]Loading train:   3%|▎         | 9/285 [00:13<07:11,  1.56s/it]Loading train:   4%|▎         | 10/285 [00:15<06:42,  1.46s/it]Loading train:   4%|▍         | 11/285 [00:16<06:25,  1.41s/it]Loading train:   4%|▍         | 12/285 [00:17<06:08,  1.35s/it]Loading train:   5%|▍         | 13/285 [00:18<06:01,  1.33s/it]Loading train:   5%|▍         | 14/285 [00:20<05:58,  1.32s/it]Loading train:   5%|▌         | 15/285 [00:21<05:53,  1.31s/it]Loading train:   6%|▌         | 16/285 [00:22<05:42,  1.27s/it]Loading train:   6%|▌         | 17/285 [00:23<05:41,  1.28s/it]Loading train:   6%|▋         | 18/285 [00:25<05:39,  1.27s/it]Loading train:   7%|▋         | 19/285 [00:26<05:53,  1.33s/it]Loading train:   7%|▋         | 20/285 [00:27<05:52,  1.33s/it]Loading train:   7%|▋         | 21/285 [00:29<05:42,  1.30s/it]Loading train:   8%|▊         | 22/285 [00:30<05:42,  1.30s/it]Loading train:   8%|▊         | 23/285 [00:31<05:30,  1.26s/it]Loading train:   8%|▊         | 24/285 [00:32<05:26,  1.25s/it]Loading train:   9%|▉         | 25/285 [00:34<05:21,  1.24s/it]Loading train:   9%|▉         | 26/285 [00:35<05:28,  1.27s/it]Loading train:   9%|▉         | 27/285 [00:36<05:39,  1.32s/it]Loading train:  10%|▉         | 28/285 [00:38<05:30,  1.29s/it]Loading train:  10%|█         | 29/285 [00:39<05:12,  1.22s/it]Loading train:  11%|█         | 30/285 [00:40<05:03,  1.19s/it]Loading train:  11%|█         | 31/285 [00:41<04:57,  1.17s/it]Loading train:  11%|█         | 32/285 [00:42<04:51,  1.15s/it]Loading train:  12%|█▏        | 33/285 [00:43<04:46,  1.14s/it]Loading train:  12%|█▏        | 34/285 [00:44<04:52,  1.17s/it]Loading train:  12%|█▏        | 35/285 [00:45<04:35,  1.10s/it]Loading train:  13%|█▎        | 36/285 [00:46<04:31,  1.09s/it]Loading train:  13%|█▎        | 37/285 [00:47<04:28,  1.08s/it]Loading train:  13%|█▎        | 38/285 [00:48<04:20,  1.05s/it]Loading train:  14%|█▎        | 39/285 [00:50<04:20,  1.06s/it]Loading train:  14%|█▍        | 40/285 [00:51<04:17,  1.05s/it]Loading train:  14%|█▍        | 41/285 [00:52<04:22,  1.08s/it]Loading train:  15%|█▍        | 42/285 [00:53<04:27,  1.10s/it]Loading train:  15%|█▌        | 43/285 [00:54<04:24,  1.09s/it]Loading train:  15%|█▌        | 44/285 [00:55<04:25,  1.10s/it]Loading train:  16%|█▌        | 45/285 [00:56<04:21,  1.09s/it]Loading train:  16%|█▌        | 46/285 [00:57<04:14,  1.07s/it]Loading train:  16%|█▋        | 47/285 [00:58<04:10,  1.05s/it]Loading train:  17%|█▋        | 48/285 [00:59<04:05,  1.03s/it]Loading train:  17%|█▋        | 49/285 [01:00<04:10,  1.06s/it]Loading train:  18%|█▊        | 50/285 [01:01<04:05,  1.04s/it]Loading train:  18%|█▊        | 51/285 [01:02<04:03,  1.04s/it]Loading train:  18%|█▊        | 52/285 [01:03<04:02,  1.04s/it]Loading train:  19%|█▊        | 53/285 [01:04<03:56,  1.02s/it]Loading train:  19%|█▉        | 54/285 [01:05<03:56,  1.02s/it]Loading train:  19%|█▉        | 55/285 [01:06<03:50,  1.00s/it]Loading train:  20%|█▉        | 56/285 [01:07<04:00,  1.05s/it]Loading train:  20%|██        | 57/285 [01:08<03:54,  1.03s/it]Loading train:  20%|██        | 58/285 [01:09<03:49,  1.01s/it]Loading train:  21%|██        | 59/285 [01:10<03:46,  1.00s/it]Loading train:  21%|██        | 60/285 [01:11<03:45,  1.00s/it]Loading train:  21%|██▏       | 61/285 [01:12<03:47,  1.01s/it]Loading train:  22%|██▏       | 62/285 [01:13<03:48,  1.03s/it]Loading train:  22%|██▏       | 63/285 [01:14<03:46,  1.02s/it]Loading train:  22%|██▏       | 64/285 [01:16<04:26,  1.21s/it]Loading train:  23%|██▎       | 65/285 [01:18<04:56,  1.35s/it]Loading train:  23%|██▎       | 66/285 [01:19<05:05,  1.39s/it]Loading train:  24%|██▎       | 67/285 [01:20<04:44,  1.30s/it]Loading train:  24%|██▍       | 68/285 [01:21<04:23,  1.22s/it]Loading train:  24%|██▍       | 69/285 [01:22<04:11,  1.17s/it]Loading train:  25%|██▍       | 70/285 [01:23<03:57,  1.11s/it]Loading train:  25%|██▍       | 71/285 [01:24<03:51,  1.08s/it]Loading train:  25%|██▌       | 72/285 [01:25<03:45,  1.06s/it]Loading train:  26%|██▌       | 73/285 [01:27<03:48,  1.08s/it]Loading train:  26%|██▌       | 74/285 [01:28<03:42,  1.06s/it]Loading train:  26%|██▋       | 75/285 [01:29<03:37,  1.04s/it]Loading train:  27%|██▋       | 76/285 [01:30<03:35,  1.03s/it]Loading train:  27%|██▋       | 77/285 [01:31<03:33,  1.03s/it]Loading train:  27%|██▋       | 78/285 [01:32<03:36,  1.05s/it]Loading train:  28%|██▊       | 79/285 [01:33<03:35,  1.04s/it]Loading train:  28%|██▊       | 80/285 [01:34<03:30,  1.03s/it]Loading train:  28%|██▊       | 81/285 [01:35<03:36,  1.06s/it]Loading train:  29%|██▉       | 82/285 [01:36<03:34,  1.06s/it]Loading train:  29%|██▉       | 83/285 [01:37<03:40,  1.09s/it]Loading train:  29%|██▉       | 84/285 [01:38<03:37,  1.08s/it]Loading train:  30%|██▉       | 85/285 [01:39<03:44,  1.12s/it]Loading train:  30%|███       | 86/285 [01:41<03:45,  1.14s/it]Loading train:  31%|███       | 87/285 [01:42<03:45,  1.14s/it]Loading train:  31%|███       | 88/285 [01:43<03:44,  1.14s/it]Loading train:  31%|███       | 89/285 [01:44<03:42,  1.13s/it]Loading train:  32%|███▏      | 90/285 [01:45<03:36,  1.11s/it]Loading train:  32%|███▏      | 91/285 [01:46<03:35,  1.11s/it]Loading train:  32%|███▏      | 92/285 [01:47<03:32,  1.10s/it]Loading train:  33%|███▎      | 93/285 [01:48<03:30,  1.10s/it]Loading train:  33%|███▎      | 94/285 [01:49<03:23,  1.07s/it]Loading train:  33%|███▎      | 95/285 [01:50<03:28,  1.10s/it]Loading train:  34%|███▎      | 96/285 [01:52<03:33,  1.13s/it]Loading train:  34%|███▍      | 97/285 [01:53<03:34,  1.14s/it]Loading train:  34%|███▍      | 98/285 [01:54<03:40,  1.18s/it]Loading train:  35%|███▍      | 99/285 [01:55<03:33,  1.15s/it]Loading train:  35%|███▌      | 100/285 [01:56<03:34,  1.16s/it]Loading train:  35%|███▌      | 101/285 [01:58<03:35,  1.17s/it]Loading train:  36%|███▌      | 102/285 [01:59<03:32,  1.16s/it]Loading train:  36%|███▌      | 103/285 [02:00<03:38,  1.20s/it]Loading train:  36%|███▋      | 104/285 [02:01<03:31,  1.17s/it]Loading train:  37%|███▋      | 105/285 [02:02<03:26,  1.15s/it]Loading train:  37%|███▋      | 106/285 [02:03<03:23,  1.13s/it]Loading train:  38%|███▊      | 107/285 [02:04<03:18,  1.11s/it]Loading train:  38%|███▊      | 108/285 [02:05<03:13,  1.09s/it]Loading train:  38%|███▊      | 109/285 [02:06<03:13,  1.10s/it]Loading train:  39%|███▊      | 110/285 [02:08<03:15,  1.12s/it]Loading train:  39%|███▉      | 111/285 [02:09<03:16,  1.13s/it]Loading train:  39%|███▉      | 112/285 [02:10<03:13,  1.12s/it]Loading train:  40%|███▉      | 113/285 [02:11<03:08,  1.09s/it]Loading train:  40%|████      | 114/285 [02:12<03:06,  1.09s/it]Loading train:  40%|████      | 115/285 [02:13<03:07,  1.10s/it]Loading train:  41%|████      | 116/285 [02:14<03:04,  1.09s/it]Loading train:  41%|████      | 117/285 [02:15<03:09,  1.13s/it]Loading train:  41%|████▏     | 118/285 [02:17<03:11,  1.15s/it]Loading train:  42%|████▏     | 119/285 [02:18<03:05,  1.12s/it]Loading train:  42%|████▏     | 120/285 [02:19<03:11,  1.16s/it]Loading train:  42%|████▏     | 121/285 [02:20<03:21,  1.23s/it]Loading train:  43%|████▎     | 122/285 [02:22<03:23,  1.25s/it]Loading train:  43%|████▎     | 123/285 [02:23<03:23,  1.25s/it]Loading train:  44%|████▎     | 124/285 [02:24<03:06,  1.16s/it]Loading train:  44%|████▍     | 125/285 [02:25<02:52,  1.08s/it]Loading train:  44%|████▍     | 126/285 [02:26<02:46,  1.05s/it]Loading train:  45%|████▍     | 127/285 [02:26<02:33,  1.03it/s]Loading train:  45%|████▍     | 128/285 [02:27<02:29,  1.05it/s]Loading train:  45%|████▌     | 129/285 [02:28<02:32,  1.02it/s]Loading train:  46%|████▌     | 130/285 [02:29<02:35,  1.00s/it]Loading train:  46%|████▌     | 131/285 [02:30<02:30,  1.02it/s]Loading train:  46%|████▋     | 132/285 [02:31<02:26,  1.04it/s]Loading train:  47%|████▋     | 133/285 [02:32<02:30,  1.01it/s]Loading train:  47%|████▋     | 134/285 [02:33<02:27,  1.02it/s]Loading train:  47%|████▋     | 135/285 [02:34<02:23,  1.05it/s]Loading train:  48%|████▊     | 136/285 [02:35<02:27,  1.01it/s]Loading train:  48%|████▊     | 137/285 [02:36<02:26,  1.01it/s]Loading train:  48%|████▊     | 138/285 [02:37<02:25,  1.01it/s]Loading train:  49%|████▉     | 139/285 [02:38<02:24,  1.01it/s]Loading train:  49%|████▉     | 140/285 [02:39<02:23,  1.01it/s]Loading train:  49%|████▉     | 141/285 [02:40<02:21,  1.02it/s]Loading train:  50%|████▉     | 142/285 [02:41<02:25,  1.02s/it]Loading train:  50%|█████     | 143/285 [02:42<02:21,  1.00it/s]Loading train:  51%|█████     | 144/285 [02:43<02:18,  1.02it/s]Loading train:  51%|█████     | 145/285 [02:44<02:16,  1.03it/s]Loading train:  51%|█████     | 146/285 [02:45<02:15,  1.03it/s]Loading train:  52%|█████▏    | 147/285 [02:46<02:14,  1.02it/s]Loading train:  52%|█████▏    | 148/285 [02:47<02:15,  1.01it/s]Loading train:  52%|█████▏    | 149/285 [02:48<02:15,  1.00it/s]Loading train:  53%|█████▎    | 150/285 [02:49<02:11,  1.03it/s]Loading train:  53%|█████▎    | 151/285 [02:50<02:08,  1.04it/s]Loading train:  53%|█████▎    | 152/285 [02:51<02:10,  1.02it/s]Loading train:  54%|█████▎    | 153/285 [02:52<02:10,  1.01it/s]Loading train:  54%|█████▍    | 154/285 [02:53<02:06,  1.03it/s]Loading train:  54%|█████▍    | 155/285 [02:54<02:03,  1.05it/s]Loading train:  55%|█████▍    | 156/285 [02:55<02:05,  1.03it/s]Loading train:  55%|█████▌    | 157/285 [02:56<02:00,  1.06it/s]Loading train:  55%|█████▌    | 158/285 [02:57<02:00,  1.05it/s]Loading train:  56%|█████▌    | 159/285 [02:58<02:01,  1.04it/s]Loading train:  56%|█████▌    | 160/285 [02:59<02:00,  1.04it/s]Loading train:  56%|█████▋    | 161/285 [02:59<01:52,  1.10it/s]Loading train:  57%|█████▋    | 162/285 [03:00<01:50,  1.11it/s]Loading train:  57%|█████▋    | 163/285 [03:01<01:52,  1.08it/s]Loading train:  58%|█████▊    | 164/285 [03:02<01:53,  1.07it/s]Loading train:  58%|█████▊    | 165/285 [03:03<01:51,  1.08it/s]Loading train:  58%|█████▊    | 166/285 [03:04<01:47,  1.11it/s]Loading train:  59%|█████▊    | 167/285 [03:05<01:45,  1.12it/s]Loading train:  59%|█████▉    | 168/285 [03:06<01:48,  1.08it/s]Loading train:  59%|█████▉    | 169/285 [03:07<01:56,  1.00s/it]Loading train:  60%|█████▉    | 170/285 [03:08<01:49,  1.05it/s]Loading train:  60%|██████    | 171/285 [03:09<01:47,  1.06it/s]Loading train:  60%|██████    | 172/285 [03:10<01:45,  1.07it/s]Loading train:  61%|██████    | 173/285 [03:11<01:46,  1.05it/s]Loading train:  61%|██████    | 174/285 [03:12<01:43,  1.07it/s]Loading train:  61%|██████▏   | 175/285 [03:13<01:44,  1.05it/s]Loading train:  62%|██████▏   | 176/285 [03:14<01:42,  1.06it/s]Loading train:  62%|██████▏   | 177/285 [03:14<01:40,  1.07it/s]Loading train:  62%|██████▏   | 178/285 [03:15<01:40,  1.06it/s]Loading train:  63%|██████▎   | 179/285 [03:16<01:39,  1.07it/s]Loading train:  63%|██████▎   | 180/285 [03:17<01:36,  1.09it/s]Loading train:  64%|██████▎   | 181/285 [03:18<01:36,  1.08it/s]Loading train:  64%|██████▍   | 182/285 [03:19<01:31,  1.12it/s]Loading train:  64%|██████▍   | 183/285 [03:20<01:31,  1.12it/s]Loading train:  65%|██████▍   | 184/285 [03:21<01:45,  1.05s/it]Loading train:  65%|██████▍   | 185/285 [03:23<02:00,  1.21s/it]Loading train:  65%|██████▌   | 186/285 [03:24<01:58,  1.20s/it]Loading train:  66%|██████▌   | 187/285 [03:25<01:53,  1.16s/it]Loading train:  66%|██████▌   | 188/285 [03:27<02:06,  1.30s/it]Loading train:  66%|██████▋   | 189/285 [03:28<02:10,  1.36s/it]Loading train:  67%|██████▋   | 190/285 [03:30<02:15,  1.43s/it]Loading train:  67%|██████▋   | 191/285 [03:31<02:12,  1.41s/it]Loading train:  67%|██████▋   | 192/285 [03:33<02:21,  1.52s/it]Loading train:  68%|██████▊   | 193/285 [03:34<02:09,  1.41s/it]Loading train:  68%|██████▊   | 194/285 [03:36<02:13,  1.46s/it]Loading train:  68%|██████▊   | 195/285 [03:38<02:23,  1.60s/it]Loading train:  69%|██████▉   | 196/285 [03:39<02:26,  1.65s/it]Loading train:  69%|██████▉   | 197/285 [03:41<02:20,  1.59s/it]Loading train:  69%|██████▉   | 198/285 [03:43<02:25,  1.68s/it]Loading train:  70%|██████▉   | 199/285 [03:44<02:22,  1.66s/it]Loading train:  70%|███████   | 200/285 [03:46<02:25,  1.71s/it]Loading train:  71%|███████   | 201/285 [03:48<02:15,  1.61s/it]Loading train:  71%|███████   | 202/285 [03:49<02:04,  1.49s/it]Loading train:  71%|███████   | 203/285 [03:50<02:07,  1.55s/it]Loading train:  72%|███████▏  | 204/285 [03:52<02:03,  1.52s/it]Loading train:  72%|███████▏  | 205/285 [03:54<02:08,  1.60s/it]Loading train:  72%|███████▏  | 206/285 [03:55<02:01,  1.53s/it]Loading train:  73%|███████▎  | 207/285 [03:56<01:53,  1.45s/it]Loading train:  73%|███████▎  | 208/285 [03:58<01:57,  1.53s/it]Loading train:  73%|███████▎  | 209/285 [04:00<01:59,  1.57s/it]Loading train:  74%|███████▎  | 210/285 [04:01<01:58,  1.58s/it]Loading train:  74%|███████▍  | 211/285 [04:03<01:55,  1.56s/it]Loading train:  74%|███████▍  | 212/285 [04:04<01:52,  1.55s/it]Loading train:  75%|███████▍  | 213/285 [04:06<02:00,  1.68s/it]Loading train:  75%|███████▌  | 214/285 [04:08<01:51,  1.57s/it]Loading train:  75%|███████▌  | 215/285 [04:09<01:50,  1.59s/it]Loading train:  76%|███████▌  | 216/285 [04:11<01:45,  1.53s/it]Loading train:  76%|███████▌  | 217/285 [04:12<01:44,  1.54s/it]Loading train:  76%|███████▋  | 218/285 [04:14<01:48,  1.61s/it]Loading train:  77%|███████▋  | 219/285 [04:16<01:49,  1.65s/it]Loading train:  77%|███████▋  | 220/285 [04:17<01:43,  1.59s/it]Loading train:  78%|███████▊  | 221/285 [04:19<01:41,  1.59s/it]Loading train:  78%|███████▊  | 222/285 [04:20<01:37,  1.55s/it]Loading train:  78%|███████▊  | 223/285 [04:22<01:34,  1.53s/it]Loading train:  79%|███████▊  | 224/285 [04:23<01:31,  1.50s/it]Loading train:  79%|███████▉  | 225/285 [04:25<01:30,  1.51s/it]Loading train:  79%|███████▉  | 226/285 [04:26<01:29,  1.52s/it]Loading train:  80%|███████▉  | 227/285 [04:28<01:34,  1.63s/it]Loading train:  80%|████████  | 228/285 [04:30<01:29,  1.56s/it]Loading train:  80%|████████  | 229/285 [04:31<01:22,  1.48s/it]Loading train:  81%|████████  | 230/285 [04:32<01:23,  1.52s/it]Loading train:  81%|████████  | 231/285 [04:34<01:22,  1.53s/it]Loading train:  81%|████████▏ | 232/285 [04:36<01:20,  1.52s/it]Loading train:  82%|████████▏ | 233/285 [04:37<01:19,  1.53s/it]Loading train:  82%|████████▏ | 234/285 [04:39<01:19,  1.55s/it]Loading train:  82%|████████▏ | 235/285 [04:40<01:19,  1.59s/it]Loading train:  83%|████████▎ | 236/285 [04:42<01:16,  1.57s/it]Loading train:  83%|████████▎ | 237/285 [04:44<01:18,  1.63s/it]Loading train:  84%|████████▎ | 238/285 [04:45<01:16,  1.62s/it]Loading train:  84%|████████▍ | 239/285 [04:47<01:15,  1.64s/it]Loading train:  84%|████████▍ | 240/285 [04:49<01:13,  1.63s/it]Loading train:  85%|████████▍ | 241/285 [04:50<01:10,  1.60s/it]Loading train:  85%|████████▍ | 242/285 [04:51<01:05,  1.53s/it]Loading train:  85%|████████▌ | 243/285 [04:53<01:06,  1.59s/it]Loading train:  86%|████████▌ | 244/285 [04:55<01:07,  1.64s/it]Loading train:  86%|████████▌ | 245/285 [04:57<01:05,  1.65s/it]Loading train:  86%|████████▋ | 246/285 [04:58<01:02,  1.60s/it]Loading train:  87%|████████▋ | 247/285 [05:00<01:01,  1.62s/it]Loading train:  87%|████████▋ | 248/285 [05:01<00:56,  1.52s/it]Loading train:  87%|████████▋ | 249/285 [05:03<01:00,  1.68s/it]Loading train:  88%|████████▊ | 250/285 [05:05<01:03,  1.82s/it]Loading train:  88%|████████▊ | 251/285 [05:07<01:00,  1.77s/it]Loading train:  88%|████████▊ | 252/285 [05:09<00:57,  1.73s/it]Loading train:  89%|████████▉ | 253/285 [05:10<00:52,  1.63s/it]Loading train:  89%|████████▉ | 254/285 [05:12<00:50,  1.62s/it]Loading train:  89%|████████▉ | 255/285 [05:13<00:48,  1.63s/it]Loading train:  90%|████████▉ | 256/285 [05:15<00:46,  1.62s/it]Loading train:  90%|█████████ | 257/285 [05:16<00:42,  1.51s/it]Loading train:  91%|█████████ | 258/285 [05:18<00:40,  1.51s/it]Loading train:  91%|█████████ | 259/285 [05:19<00:36,  1.42s/it]Loading train:  91%|█████████ | 260/285 [05:20<00:35,  1.40s/it]Loading train:  92%|█████████▏| 261/285 [05:21<00:33,  1.38s/it]Loading train:  92%|█████████▏| 262/285 [05:23<00:32,  1.40s/it]Loading train:  92%|█████████▏| 263/285 [05:24<00:30,  1.39s/it]Loading train:  93%|█████████▎| 264/285 [05:26<00:29,  1.39s/it]Loading train:  93%|█████████▎| 265/285 [05:28<00:30,  1.53s/it]Loading train:  93%|█████████▎| 266/285 [05:29<00:29,  1.56s/it]Loading train:  94%|█████████▎| 267/285 [05:31<00:27,  1.54s/it]Loading train:  94%|█████████▍| 268/285 [05:32<00:27,  1.62s/it]Loading train:  94%|█████████▍| 269/285 [05:34<00:26,  1.68s/it]Loading train:  95%|█████████▍| 270/285 [05:36<00:25,  1.71s/it]Loading train:  95%|█████████▌| 271/285 [05:38<00:24,  1.77s/it]Loading train:  95%|█████████▌| 272/285 [05:40<00:22,  1.72s/it]Loading train:  96%|█████████▌| 273/285 [05:41<00:21,  1.76s/it]Loading train:  96%|█████████▌| 274/285 [05:43<00:19,  1.77s/it]Loading train:  96%|█████████▋| 275/285 [05:45<00:18,  1.82s/it]Loading train:  97%|█████████▋| 276/285 [05:47<00:16,  1.87s/it]Loading train:  97%|█████████▋| 277/285 [05:49<00:15,  1.98s/it]Loading train:  98%|█████████▊| 278/285 [05:51<00:13,  1.89s/it]Loading train:  98%|█████████▊| 279/285 [05:53<00:11,  1.88s/it]Loading train:  98%|█████████▊| 280/285 [05:54<00:08,  1.75s/it]Loading train:  99%|█████████▊| 281/285 [05:56<00:06,  1.72s/it]Loading train:  99%|█████████▉| 282/285 [05:57<00:04,  1.66s/it]Loading train:  99%|█████████▉| 283/285 [05:59<00:03,  1.72s/it]Loading train: 100%|█████████▉| 284/285 [06:01<00:01,  1.69s/it]Loading train: 100%|██████████| 285/285 [06:02<00:00,  1.61s/it]
concatenating: train:   0%|          | 0/285 [00:00<?, ?it/s]concatenating: train:   1%|          | 2/285 [00:00<00:15, 18.01it/s]concatenating: train:   2%|▏         | 5/285 [00:00<00:15, 18.49it/s]concatenating: train:   3%|▎         | 8/285 [00:00<00:14, 19.59it/s]concatenating: train:   4%|▍         | 11/285 [00:00<00:12, 21.49it/s]concatenating: train:   5%|▍         | 14/285 [00:00<00:11, 23.06it/s]concatenating: train:   6%|▌         | 17/285 [00:00<00:12, 21.57it/s]concatenating: train:   7%|▋         | 21/285 [00:00<00:11, 23.78it/s]concatenating: train:   8%|▊         | 24/285 [00:01<00:10, 24.13it/s]concatenating: train:  11%|█         | 31/285 [00:01<00:08, 29.87it/s]concatenating: train:  12%|█▏        | 35/285 [00:01<00:07, 31.44it/s]concatenating: train:  18%|█▊        | 51/285 [00:01<00:05, 41.37it/s]concatenating: train:  21%|██        | 59/285 [00:01<00:04, 46.44it/s]concatenating: train:  24%|██▎       | 67/285 [00:01<00:04, 48.88it/s]concatenating: train:  26%|██▌       | 74/285 [00:01<00:04, 50.93it/s]concatenating: train:  28%|██▊       | 81/285 [00:01<00:04, 50.20it/s]concatenating: train:  31%|███       | 87/285 [00:02<00:04, 43.27it/s]concatenating: train:  33%|███▎      | 93/285 [00:02<00:04, 40.32it/s]concatenating: train:  35%|███▌      | 100/285 [00:02<00:04, 44.78it/s]concatenating: train:  37%|███▋      | 106/285 [00:02<00:03, 47.09it/s]concatenating: train:  40%|███▉      | 113/285 [00:02<00:03, 50.85it/s]concatenating: train:  42%|████▏     | 119/285 [00:02<00:03, 51.64it/s]concatenating: train:  44%|████▍     | 125/285 [00:02<00:03, 48.63it/s]concatenating: train:  50%|█████     | 143/285 [00:02<00:02, 62.13it/s]concatenating: train:  60%|█████▉    | 170/285 [00:03<00:01, 80.67it/s]concatenating: train:  71%|███████   | 201/285 [00:03<00:00, 103.43it/s]concatenating: train:  78%|███████▊  | 222/285 [00:03<00:00, 117.90it/s]concatenating: train:  85%|████████▍ | 241/285 [00:03<00:00, 76.61it/s] concatenating: train:  90%|████████▉ | 256/285 [00:04<00:00, 61.54it/s]concatenating: train:  94%|█████████▍| 268/285 [00:04<00:00, 57.94it/s]concatenating: train:  98%|█████████▊| 278/285 [00:04<00:00, 61.42it/s]concatenating: train: 100%|██████████| 285/285 [00:04<00:00, 62.83it/s]
Loading test:   0%|          | 0/3 [00:00<?, ?it/s]Loading test:  33%|███▎      | 1/3 [00:01<00:03,  1.99s/it]Loading test:  67%|██████▋   | 2/3 [00:03<00:01,  1.89s/it]Loading test: 100%|██████████| 3/3 [00:05<00:00,  1.89s/it]
concatenating: validation:   0%|          | 0/3 [00:00<?, ?it/s]concatenating: validation:  67%|██████▋   | 2/3 [00:00<00:00, 19.03it/s]concatenating: validation: 100%|██████████| 3/3 [00:00<00:00, 23.19it/s]

---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 5  | SD 1  | Dropout 0.3  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE13_Cascade_FM20_FCN_Unet_TL_NL3_LS_MyBCE_US1_FCNA3_FCNB0_CSFn2_TL_Main_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 12, 12, 80), (None, 13, 13, 80)]
