2020-01-20 22:18:12.690087: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:16.877829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:08:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:16.877898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:17.300573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:17.300643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:17.300656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:17.301131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['a']
TypeExperiment 10
CrossVal ['a']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:20.340218: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:23.240279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:08:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:23.240342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:23.671314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:23.671375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:23.671387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:23.671851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['b']
TypeExperiment 10
CrossVal ['b']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:26.743745: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:29.412382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:08:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:29.412427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:29.840631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:29.840672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:29.840685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:29.841138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['c']
TypeExperiment 10
CrossVal ['c']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:18:34.544912: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:18:39.103071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:08:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:18:39.103133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:18:39.517108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:18:39.517179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:18:39.517191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:18:39.517695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['d']
TypeExperiment 10
CrossVal ['d']
Traceback (most recent call last):
  File "main.py", line 1902, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1877, in Run_Csfn_with_Best_WMn_architecture
    applyPreprocess.main(paramFunc.Run(UserInfoB, terminal=True), 'experiment')
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 178, in Run
    UserInfoB = temp_Experiments_preSet_V2(UserInfoB)
  File "/array/ssd/msmajdi/code/thalamus/keras/Parameters/paramFunc.py", line 159, in temp_Experiments_preSet_V2
    a,b,c,d,e = TypeExperimentFuncs().main(TypeExperiment=UserInfoB['TypeExperiment'], perm_Index=UserInfoB['permutation_Index'])
ValueError: too many values to unpack (expected 5)
2020-01-20 22:21:52.505796: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-20 22:21:54.971090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:08:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-20 22:21:54.971157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-20 22:21:55.394348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-20 22:21:55.394416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-20 22:21:55.394430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-20 22:21:55.394916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:53,  2.33it/s]Loading train:   1%|          | 2/266 [00:00<01:40,  2.62it/s]Loading train:   1%|          | 3/266 [00:00<01:27,  3.01it/s]Loading train:   2%|▏         | 4/266 [00:01<01:18,  3.33it/s]Loading train:   2%|▏         | 5/266 [00:01<01:14,  3.49it/s]Loading train:   2%|▏         | 6/266 [00:01<01:12,  3.60it/s]Loading train:   3%|▎         | 7/266 [00:01<01:10,  3.69it/s]Loading train:   3%|▎         | 8/266 [00:02<01:08,  3.77it/s]Loading train:   3%|▎         | 9/266 [00:02<01:07,  3.82it/s]Loading train:   4%|▍         | 10/266 [00:02<01:06,  3.84it/s]Loading train:   4%|▍         | 11/266 [00:02<01:05,  3.88it/s]Loading train:   5%|▍         | 12/266 [00:03<01:05,  3.89it/s]Loading train:   5%|▍         | 13/266 [00:03<01:05,  3.88it/s]Loading train:   5%|▌         | 14/266 [00:03<01:04,  3.91it/s]Loading train:   6%|▌         | 15/266 [00:03<01:04,  3.88it/s]Loading train:   6%|▌         | 16/266 [00:04<01:03,  3.91it/s]Loading train:   6%|▋         | 17/266 [00:04<01:03,  3.92it/s]Loading train:   7%|▋         | 18/266 [00:04<01:03,  3.93it/s]Loading train:   7%|▋         | 19/266 [00:04<01:02,  3.92it/s]Loading train:   8%|▊         | 20/266 [00:05<01:02,  3.93it/s]Loading train:   8%|▊         | 21/266 [00:05<01:02,  3.94it/s]Loading train:   8%|▊         | 22/266 [00:05<01:01,  3.95it/s]Loading train:   9%|▊         | 23/266 [00:05<01:01,  3.95it/s]Loading train:   9%|▉         | 24/266 [00:06<01:00,  4.00it/s]Loading train:   9%|▉         | 25/266 [00:06<00:59,  4.02it/s]Loading train:  10%|▉         | 26/266 [00:06<00:59,  4.06it/s]Loading train:  10%|█         | 27/266 [00:06<00:59,  4.05it/s]Loading train:  11%|█         | 28/266 [00:07<00:58,  4.06it/s]Loading train:  11%|█         | 29/266 [00:07<00:58,  4.04it/s]Loading train:  11%|█▏        | 30/266 [00:07<00:57,  4.07it/s]Loading train:  12%|█▏        | 31/266 [00:07<00:57,  4.08it/s]Loading train:  12%|█▏        | 32/266 [00:08<00:57,  4.10it/s]Loading train:  12%|█▏        | 33/266 [00:08<00:56,  4.10it/s]Loading train:  13%|█▎        | 34/266 [00:08<00:57,  4.06it/s]Loading train:  13%|█▎        | 35/266 [00:08<00:56,  4.07it/s]Loading train:  14%|█▎        | 36/266 [00:09<00:56,  4.10it/s]Loading train:  14%|█▍        | 37/266 [00:09<00:55,  4.10it/s]Loading train:  14%|█▍        | 38/266 [00:09<00:55,  4.11it/s]Loading train:  15%|█▍        | 39/266 [00:09<00:55,  4.07it/s]Loading train:  15%|█▌        | 40/266 [00:10<00:55,  4.10it/s]Loading train:  15%|█▌        | 41/266 [00:10<00:54,  4.13it/s]Loading train:  16%|█▌        | 42/266 [00:10<00:51,  4.35it/s]Loading train:  16%|█▌        | 43/266 [00:10<00:49,  4.52it/s]Loading train:  17%|█▋        | 44/266 [00:10<00:47,  4.65it/s]Loading train:  17%|█▋        | 45/266 [00:11<00:46,  4.74it/s]Loading train:  17%|█▋        | 46/266 [00:11<00:45,  4.79it/s]Loading train:  18%|█▊        | 47/266 [00:11<00:45,  4.84it/s]Loading train:  18%|█▊        | 48/266 [00:11<00:44,  4.87it/s]Loading train:  18%|█▊        | 49/266 [00:11<00:44,  4.91it/s]Loading train:  19%|█▉        | 50/266 [00:12<00:44,  4.90it/s]Loading train:  19%|█▉        | 51/266 [00:12<00:43,  4.90it/s]Loading train:  20%|█▉        | 52/266 [00:12<00:43,  4.91it/s]Loading train:  20%|█▉        | 53/266 [00:12<00:43,  4.92it/s]Loading train:  20%|██        | 54/266 [00:12<00:43,  4.90it/s]Loading train:  21%|██        | 55/266 [00:13<00:43,  4.91it/s]Loading train:  21%|██        | 56/266 [00:13<00:42,  4.89it/s]Loading train:  21%|██▏       | 57/266 [00:13<00:42,  4.91it/s]Loading train:  22%|██▏       | 58/266 [00:13<00:42,  4.92it/s]Loading train:  22%|██▏       | 59/266 [00:14<00:42,  4.87it/s]Loading train:  23%|██▎       | 60/266 [00:14<00:42,  4.83it/s]Loading train:  23%|██▎       | 61/266 [00:14<00:42,  4.80it/s]Loading train:  23%|██▎       | 62/266 [00:14<00:42,  4.78it/s]Loading train:  24%|██▎       | 63/266 [00:14<00:42,  4.77it/s]Loading train:  24%|██▍       | 64/266 [00:15<00:42,  4.75it/s]Loading train:  24%|██▍       | 65/266 [00:15<00:42,  4.73it/s]Loading train:  25%|██▍       | 66/266 [00:15<00:42,  4.73it/s]Loading train:  25%|██▌       | 67/266 [00:15<00:42,  4.68it/s]Loading train:  26%|██▌       | 68/266 [00:15<00:42,  4.70it/s]Loading train:  26%|██▌       | 69/266 [00:16<00:41,  4.71it/s]Loading train:  26%|██▋       | 70/266 [00:16<00:41,  4.72it/s]Loading train:  27%|██▋       | 71/266 [00:16<00:41,  4.73it/s]Loading train:  27%|██▋       | 72/266 [00:16<00:41,  4.72it/s]Loading train:  27%|██▋       | 73/266 [00:16<00:41,  4.69it/s]Loading train:  28%|██▊       | 74/266 [00:17<00:40,  4.69it/s]Loading train:  28%|██▊       | 75/266 [00:17<00:40,  4.69it/s]Loading train:  29%|██▊       | 76/266 [00:17<00:40,  4.70it/s]Loading train:  29%|██▉       | 77/266 [00:17<00:40,  4.71it/s]Loading train:  29%|██▉       | 78/266 [00:18<00:41,  4.54it/s]Loading train:  30%|██▉       | 79/266 [00:18<00:42,  4.43it/s]Loading train:  30%|███       | 80/266 [00:18<00:42,  4.33it/s]Loading train:  30%|███       | 81/266 [00:18<00:43,  4.26it/s]Loading train:  31%|███       | 82/266 [00:19<00:57,  3.19it/s]Loading train:  31%|███       | 83/266 [00:19<01:05,  2.79it/s]Loading train:  32%|███▏      | 84/266 [00:20<01:19,  2.29it/s]Loading train:  32%|███▏      | 85/266 [00:20<01:27,  2.08it/s]Loading train:  32%|███▏      | 86/266 [00:21<01:32,  1.95it/s]Loading train:  33%|███▎      | 87/266 [00:22<01:31,  1.95it/s]Loading train:  33%|███▎      | 88/266 [00:22<01:33,  1.91it/s]Loading train:  33%|███▎      | 89/266 [00:23<01:42,  1.72it/s]Loading train:  34%|███▍      | 90/266 [00:23<01:45,  1.68it/s]Loading train:  34%|███▍      | 91/266 [00:24<01:38,  1.77it/s]Loading train:  35%|███▍      | 92/266 [00:25<01:37,  1.78it/s]Loading train:  35%|███▍      | 93/266 [00:25<01:31,  1.88it/s]Loading train:  35%|███▌      | 94/266 [00:25<01:28,  1.95it/s]Loading train:  36%|███▌      | 95/266 [00:26<01:26,  1.98it/s]Loading train:  36%|███▌      | 96/266 [00:26<01:22,  2.06it/s]Loading train:  36%|███▋      | 97/266 [00:27<01:23,  2.02it/s]Loading train:  37%|███▋      | 98/266 [00:27<01:21,  2.07it/s]Loading train:  37%|███▋      | 99/266 [00:28<01:14,  2.25it/s]Loading train:  38%|███▊      | 100/266 [00:28<01:11,  2.32it/s]Loading train:  38%|███▊      | 101/266 [00:29<01:16,  2.17it/s]Loading train:  38%|███▊      | 102/266 [00:29<01:18,  2.08it/s]Loading train:  39%|███▊      | 103/266 [00:30<01:15,  2.16it/s]Loading train:  39%|███▉      | 104/266 [00:30<01:12,  2.24it/s]Loading train:  39%|███▉      | 105/266 [00:30<01:09,  2.32it/s]Loading train:  40%|███▉      | 106/266 [00:31<01:04,  2.47it/s]Loading train:  40%|████      | 107/266 [00:31<01:04,  2.45it/s]Loading train:  41%|████      | 108/266 [00:32<01:04,  2.45it/s]Loading train:  41%|████      | 109/266 [00:32<01:05,  2.41it/s]Loading train:  41%|████▏     | 110/266 [00:33<01:12,  2.14it/s]Loading train:  42%|████▏     | 111/266 [00:33<01:13,  2.12it/s]Loading train:  42%|████▏     | 112/266 [00:34<01:18,  1.96it/s]Loading train:  42%|████▏     | 113/266 [00:34<01:18,  1.95it/s]Loading train:  43%|████▎     | 114/266 [00:35<01:24,  1.80it/s]Loading train:  43%|████▎     | 115/266 [00:36<01:30,  1.68it/s]Loading train:  44%|████▎     | 116/266 [00:36<01:30,  1.66it/s]Loading train:  44%|████▍     | 117/266 [00:37<01:31,  1.63it/s]Loading train:  44%|████▍     | 118/266 [00:37<01:31,  1.62it/s]Loading train:  45%|████▍     | 119/266 [00:38<01:42,  1.43it/s]Loading train:  45%|████▌     | 120/266 [00:39<01:43,  1.41it/s]Loading train:  45%|████▌     | 121/266 [00:40<01:48,  1.33it/s]Loading train:  46%|████▌     | 122/266 [00:41<01:47,  1.34it/s]Loading train:  46%|████▌     | 123/266 [00:41<01:47,  1.34it/s]Loading train:  47%|████▋     | 124/266 [00:42<01:46,  1.34it/s]Loading train:  47%|████▋     | 125/266 [00:43<01:43,  1.36it/s]Loading train:  47%|████▋     | 126/266 [00:44<01:42,  1.37it/s]Loading train:  48%|████▊     | 127/266 [00:44<01:40,  1.39it/s]Loading train:  48%|████▊     | 128/266 [00:45<01:38,  1.40it/s]Loading train:  48%|████▊     | 129/266 [00:46<01:36,  1.42it/s]Loading train:  49%|████▉     | 130/266 [00:46<01:35,  1.43it/s]Loading train:  49%|████▉     | 131/266 [00:47<01:37,  1.38it/s]Loading train:  50%|████▉     | 132/266 [00:48<01:35,  1.41it/s]Loading train:  50%|█████     | 133/266 [00:48<01:30,  1.47it/s]Loading train:  50%|█████     | 134/266 [00:49<01:29,  1.48it/s]Loading train:  51%|█████     | 135/266 [00:50<01:31,  1.42it/s]Loading train:  51%|█████     | 136/266 [00:51<01:32,  1.40it/s]Loading train:  52%|█████▏    | 137/266 [00:51<01:28,  1.46it/s]Loading train:  52%|█████▏    | 138/266 [00:52<01:26,  1.47it/s]Loading train:  52%|█████▏    | 139/266 [00:52<01:24,  1.51it/s]Loading train:  53%|█████▎    | 140/266 [00:53<01:22,  1.53it/s]Loading train:  53%|█████▎    | 141/266 [00:54<01:21,  1.53it/s]Loading train:  53%|█████▎    | 142/266 [00:54<01:23,  1.48it/s]Loading train:  54%|█████▍    | 143/266 [00:55<01:20,  1.52it/s]Loading train:  54%|█████▍    | 144/266 [00:56<01:19,  1.54it/s]Loading train:  55%|█████▍    | 145/266 [00:56<01:16,  1.58it/s]Loading train:  55%|█████▍    | 146/266 [00:57<01:16,  1.57it/s]Loading train:  55%|█████▌    | 147/266 [00:58<01:13,  1.61it/s]Loading train:  56%|█████▌    | 148/266 [00:58<01:16,  1.54it/s]Loading train:  56%|█████▌    | 149/266 [00:59<01:15,  1.54it/s]Loading train:  56%|█████▋    | 150/266 [00:59<01:14,  1.56it/s]Loading train:  57%|█████▋    | 151/266 [01:00<01:13,  1.58it/s]Loading train:  57%|█████▋    | 152/266 [01:01<01:11,  1.59it/s]Loading train:  58%|█████▊    | 153/266 [01:01<01:09,  1.62it/s]Loading train:  58%|█████▊    | 154/266 [01:02<01:07,  1.65it/s]Loading train:  58%|█████▊    | 155/266 [01:02<01:01,  1.82it/s]Loading train:  59%|█████▊    | 156/266 [01:03<01:00,  1.82it/s]Loading train:  59%|█████▉    | 157/266 [01:03<00:57,  1.89it/s]Loading train:  59%|█████▉    | 158/266 [01:04<00:56,  1.90it/s]Loading train:  60%|█████▉    | 159/266 [01:04<00:55,  1.94it/s]Loading train:  60%|██████    | 160/266 [01:05<00:55,  1.92it/s]Loading train:  61%|██████    | 161/266 [01:05<00:52,  1.99it/s]Loading train:  61%|██████    | 162/266 [01:06<00:52,  1.96it/s]Loading train:  61%|██████▏   | 163/266 [01:06<00:51,  2.00it/s]Loading train:  62%|██████▏   | 164/266 [01:07<00:49,  2.06it/s]Loading train:  62%|██████▏   | 165/266 [01:07<00:49,  2.06it/s]Loading train:  62%|██████▏   | 166/266 [01:08<00:50,  2.00it/s]Loading train:  63%|██████▎   | 167/266 [01:08<00:48,  2.05it/s]Loading train:  63%|██████▎   | 168/266 [01:09<00:48,  2.04it/s]Loading train:  64%|██████▎   | 169/266 [01:09<00:49,  1.96it/s]Loading train:  64%|██████▍   | 170/266 [01:10<00:49,  1.93it/s]Loading train:  64%|██████▍   | 171/266 [01:10<00:48,  1.97it/s]Loading train:  65%|██████▍   | 172/266 [01:11<00:49,  1.91it/s]Loading train:  65%|██████▌   | 173/266 [01:11<00:49,  1.89it/s]Loading train:  65%|██████▌   | 174/266 [01:12<00:48,  1.88it/s]Loading train:  66%|██████▌   | 175/266 [01:13<00:47,  1.90it/s]Loading train:  66%|██████▌   | 176/266 [01:13<00:51,  1.75it/s]Loading train:  67%|██████▋   | 177/266 [01:14<00:52,  1.70it/s]Loading train:  67%|██████▋   | 178/266 [01:14<00:52,  1.67it/s]Loading train:  67%|██████▋   | 179/266 [01:15<00:51,  1.69it/s]Loading train:  68%|██████▊   | 180/266 [01:16<00:52,  1.65it/s]Loading train:  68%|██████▊   | 181/266 [01:16<00:50,  1.68it/s]Loading train:  68%|██████▊   | 182/266 [01:17<00:50,  1.66it/s]Loading train:  69%|██████▉   | 183/266 [01:17<00:49,  1.66it/s]Loading train:  69%|██████▉   | 184/266 [01:18<00:50,  1.64it/s]Loading train:  70%|██████▉   | 185/266 [01:19<00:50,  1.60it/s]Loading train:  70%|██████▉   | 186/266 [01:19<00:50,  1.60it/s]Loading train:  70%|███████   | 187/266 [01:20<00:50,  1.57it/s]Loading train:  71%|███████   | 188/266 [01:21<00:50,  1.55it/s]Loading train:  71%|███████   | 189/266 [01:21<00:49,  1.55it/s]Loading train:  71%|███████▏  | 190/266 [01:22<00:48,  1.57it/s]Loading train:  72%|███████▏  | 191/266 [01:23<00:46,  1.60it/s]Loading train:  72%|███████▏  | 192/266 [01:23<00:45,  1.63it/s]Loading train:  73%|███████▎  | 193/266 [01:24<00:44,  1.63it/s]Loading train:  73%|███████▎  | 194/266 [01:24<00:46,  1.53it/s]Loading train:  73%|███████▎  | 195/266 [01:25<00:43,  1.62it/s]Loading train:  74%|███████▎  | 196/266 [01:26<00:43,  1.61it/s]Loading train:  74%|███████▍  | 197/266 [01:26<00:42,  1.64it/s]Loading train:  74%|███████▍  | 198/266 [01:27<00:41,  1.64it/s]Loading train:  75%|███████▍  | 199/266 [01:27<00:41,  1.60it/s]Loading train:  75%|███████▌  | 200/266 [01:28<00:41,  1.60it/s]Loading train:  76%|███████▌  | 201/266 [01:29<00:39,  1.64it/s]Loading train:  76%|███████▌  | 202/266 [01:29<00:38,  1.67it/s]Loading train:  76%|███████▋  | 203/266 [01:30<00:36,  1.74it/s]Loading train:  77%|███████▋  | 204/266 [01:30<00:36,  1.69it/s]Loading train:  77%|███████▋  | 205/266 [01:31<00:36,  1.68it/s]Loading train:  77%|███████▋  | 206/266 [01:31<00:32,  1.83it/s]Loading train:  78%|███████▊  | 207/266 [01:32<00:33,  1.79it/s]Loading train:  78%|███████▊  | 208/266 [01:33<00:34,  1.70it/s]Loading train:  79%|███████▊  | 209/266 [01:33<00:33,  1.70it/s]Loading train:  79%|███████▉  | 210/266 [01:34<00:33,  1.66it/s]Loading train:  79%|███████▉  | 211/266 [01:34<00:32,  1.70it/s]Loading train:  80%|███████▉  | 212/266 [01:35<00:34,  1.57it/s]Loading train:  80%|████████  | 213/266 [01:36<00:34,  1.52it/s]Loading train:  80%|████████  | 214/266 [01:36<00:32,  1.62it/s]Loading train:  81%|████████  | 215/266 [01:37<00:31,  1.63it/s]Loading train:  81%|████████  | 216/266 [01:38<00:29,  1.69it/s]Loading train:  82%|████████▏ | 217/266 [01:38<00:29,  1.68it/s]Loading train:  82%|████████▏ | 218/266 [01:39<00:28,  1.68it/s]Loading train:  82%|████████▏ | 219/266 [01:39<00:28,  1.65it/s]Loading train:  83%|████████▎ | 220/266 [01:40<00:26,  1.71it/s]Loading train:  83%|████████▎ | 221/266 [01:41<00:26,  1.72it/s]Loading train:  83%|████████▎ | 222/266 [01:41<00:24,  1.76it/s]Loading train:  84%|████████▍ | 223/266 [01:42<00:26,  1.65it/s]Loading train:  84%|████████▍ | 224/266 [01:42<00:24,  1.72it/s]Loading train:  85%|████████▍ | 225/266 [01:43<00:23,  1.76it/s]Loading train:  85%|████████▍ | 226/266 [01:43<00:22,  1.77it/s]Loading train:  85%|████████▌ | 227/266 [01:44<00:21,  1.79it/s]Loading train:  86%|████████▌ | 228/266 [01:45<00:21,  1.77it/s]Loading train:  86%|████████▌ | 229/266 [01:45<00:20,  1.79it/s]Loading train:  86%|████████▋ | 230/266 [01:46<00:20,  1.78it/s]Loading train:  87%|████████▋ | 231/266 [01:46<00:21,  1.66it/s]Loading train:  87%|████████▋ | 232/266 [01:47<00:20,  1.65it/s]Loading train:  88%|████████▊ | 233/266 [01:47<00:19,  1.73it/s]Loading train:  88%|████████▊ | 234/266 [01:48<00:18,  1.73it/s]Loading train:  88%|████████▊ | 235/266 [01:49<00:17,  1.75it/s]Loading train:  89%|████████▊ | 236/266 [01:49<00:17,  1.75it/s]Loading train:  89%|████████▉ | 237/266 [01:50<00:16,  1.77it/s]Loading train:  89%|████████▉ | 238/266 [01:50<00:15,  1.79it/s]Loading train:  90%|████████▉ | 239/266 [01:51<00:15,  1.72it/s]Loading train:  90%|█████████ | 240/266 [01:51<00:14,  1.79it/s]Loading train:  91%|█████████ | 241/266 [01:52<00:13,  1.81it/s]Loading train:  91%|█████████ | 242/266 [01:52<00:13,  1.83it/s]Loading train:  91%|█████████▏| 243/266 [01:53<00:12,  1.86it/s]Loading train:  92%|█████████▏| 244/266 [01:53<00:11,  1.89it/s]Loading train:  92%|█████████▏| 245/266 [01:54<00:11,  1.88it/s]Loading train:  92%|█████████▏| 246/266 [01:55<00:11,  1.76it/s]Loading train:  93%|█████████▎| 247/266 [01:55<00:10,  1.81it/s]Loading train:  93%|█████████▎| 248/266 [01:56<00:10,  1.80it/s]Loading train:  94%|█████████▎| 249/266 [01:56<00:09,  1.77it/s]Loading train:  94%|█████████▍| 250/266 [01:57<00:08,  1.82it/s]Loading train:  94%|█████████▍| 251/266 [01:57<00:08,  1.75it/s]Loading train:  95%|█████████▍| 252/266 [01:58<00:08,  1.69it/s]Loading train:  95%|█████████▌| 253/266 [01:59<00:07,  1.69it/s]Loading train:  95%|█████████▌| 254/266 [01:59<00:07,  1.67it/s]Loading train:  96%|█████████▌| 255/266 [02:00<00:06,  1.67it/s]Loading train:  96%|█████████▌| 256/266 [02:00<00:05,  1.71it/s]Loading train:  97%|█████████▋| 257/266 [02:01<00:05,  1.62it/s]Loading train:  97%|█████████▋| 258/266 [02:02<00:04,  1.62it/s]Loading train:  97%|█████████▋| 259/266 [02:02<00:04,  1.60it/s]Loading train:  98%|█████████▊| 260/266 [02:03<00:03,  1.62it/s]Loading train:  98%|█████████▊| 261/266 [02:04<00:02,  1.71it/s]Loading train:  98%|█████████▊| 262/266 [02:04<00:02,  1.78it/s]Loading train:  99%|█████████▉| 263/266 [02:05<00:01,  1.69it/s]Loading train:  99%|█████████▉| 264/266 [02:05<00:01,  1.68it/s]Loading train: 100%|█████████▉| 265/266 [02:06<00:00,  1.66it/s]Loading train: 100%|██████████| 266/266 [02:06<00:00,  1.72it/s]Loading train: 100%|██████████| 266/266 [02:06<00:00,  2.09it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:05, 47.84it/s]concatenating: train:   4%|▍         | 11/266 [00:00<00:05, 49.12it/s]concatenating: train:   6%|▋         | 17/266 [00:00<00:04, 50.44it/s]concatenating: train:   9%|▊         | 23/266 [00:00<00:04, 51.71it/s]concatenating: train:  11%|█         | 29/266 [00:00<00:04, 52.99it/s]concatenating: train:  13%|█▎        | 35/266 [00:00<00:04, 53.86it/s]concatenating: train:  15%|█▌        | 41/266 [00:00<00:04, 54.65it/s]concatenating: train:  18%|█▊        | 48/266 [00:00<00:03, 57.61it/s]concatenating: train:  21%|██        | 55/266 [00:00<00:03, 59.92it/s]concatenating: train:  23%|██▎       | 62/266 [00:01<00:03, 60.80it/s]concatenating: train:  26%|██▌       | 69/266 [00:01<00:03, 60.72it/s]concatenating: train:  29%|██▊       | 76/266 [00:01<00:03, 59.18it/s]concatenating: train:  31%|███       | 82/266 [00:01<00:03, 58.31it/s]concatenating: train:  33%|███▎      | 88/266 [00:01<00:03, 53.51it/s]concatenating: train:  35%|███▌      | 94/266 [00:01<00:03, 50.13it/s]concatenating: train:  38%|███▊      | 100/266 [00:01<00:03, 49.89it/s]concatenating: train:  40%|███▉      | 106/266 [00:01<00:03, 50.46it/s]concatenating: train:  42%|████▏     | 112/266 [00:02<00:03, 51.24it/s]concatenating: train:  44%|████▍     | 118/266 [00:02<00:02, 50.74it/s]concatenating: train:  47%|████▋     | 124/266 [00:02<00:03, 46.55it/s]concatenating: train:  48%|████▊     | 129/266 [00:02<00:03, 43.95it/s]concatenating: train:  50%|█████     | 134/266 [00:02<00:03, 42.35it/s]concatenating: train:  52%|█████▏    | 139/266 [00:02<00:02, 43.11it/s]concatenating: train:  54%|█████▍    | 144/266 [00:02<00:02, 43.29it/s]concatenating: train:  56%|█████▌    | 149/266 [00:02<00:02, 43.58it/s]concatenating: train:  58%|█████▊    | 154/266 [00:03<00:02, 44.60it/s]concatenating: train:  60%|██████    | 160/266 [00:03<00:02, 47.44it/s]concatenating: train:  62%|██████▏   | 166/266 [00:03<00:02, 49.42it/s]concatenating: train:  65%|██████▍   | 172/266 [00:03<00:01, 50.90it/s]concatenating: train:  67%|██████▋   | 178/266 [00:03<00:01, 50.70it/s]concatenating: train:  69%|██████▉   | 184/266 [00:03<00:01, 50.63it/s]concatenating: train:  71%|███████▏  | 190/266 [00:03<00:01, 50.44it/s]concatenating: train:  74%|███████▎  | 196/266 [00:03<00:01, 50.24it/s]concatenating: train:  76%|███████▌  | 202/266 [00:03<00:01, 50.17it/s]concatenating: train:  78%|███████▊  | 208/266 [00:04<00:01, 49.17it/s]concatenating: train:  80%|████████  | 213/266 [00:04<00:01, 49.13it/s]concatenating: train:  82%|████████▏ | 219/266 [00:04<00:00, 49.62it/s]concatenating: train:  84%|████████▍ | 224/266 [00:04<00:00, 49.73it/s]concatenating: train:  86%|████████▋ | 230/266 [00:04<00:00, 49.90it/s]concatenating: train:  88%|████████▊ | 235/266 [00:04<00:00, 49.59it/s]concatenating: train:  90%|█████████ | 240/266 [00:04<00:00, 49.54it/s]concatenating: train:  92%|█████████▏| 245/266 [00:04<00:00, 48.97it/s]concatenating: train:  94%|█████████▍| 250/266 [00:04<00:00, 48.58it/s]concatenating: train:  96%|█████████▌| 255/266 [00:05<00:00, 47.46it/s]concatenating: train:  98%|█████████▊| 260/266 [00:05<00:00, 46.66it/s]concatenating: train: 100%|█████████▉| 265/266 [00:05<00:00, 45.35it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 50.23it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:01,  1.67it/s]Loading test:  50%|█████     | 2/4 [00:01<00:01,  1.71it/s]Loading test:  75%|███████▌  | 3/4 [00:01<00:00,  1.69it/s]Loading test: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]Loading test: 100%|██████████| 4/4 [00:02<00:00,  1.64it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 58.29it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:00<03:14,  1.36it/s]Loading trainS:   1%|          | 2/266 [00:01<03:03,  1.44it/s]Loading trainS:   1%|          | 3/266 [00:01<02:48,  1.56it/s]Loading trainS:   2%|▏         | 4/266 [00:02<02:35,  1.69it/s]Loading trainS:   2%|▏         | 5/266 [00:02<02:35,  1.68it/s]Loading trainS:   2%|▏         | 6/266 [00:03<02:37,  1.65it/s]Loading trainS:   3%|▎         | 7/266 [00:04<02:33,  1.68it/s]Loading trainS:   3%|▎         | 8/266 [00:04<02:31,  1.70it/s]Loading trainS:   3%|▎         | 9/266 [00:05<02:40,  1.60it/s]Loading trainS:   4%|▍         | 10/266 [00:06<02:43,  1.56it/s]Loading trainS:   4%|▍         | 11/266 [00:06<02:43,  1.56it/s]Loading trainS:   5%|▍         | 12/266 [00:07<02:45,  1.54it/s]Loading trainS:   5%|▍         | 13/266 [00:08<02:44,  1.54it/s]Loading trainS:   5%|▌         | 14/266 [00:08<02:44,  1.54it/s]Loading trainS:   6%|▌         | 15/266 [00:09<02:45,  1.51it/s]Loading trainS:   6%|▌         | 16/266 [00:09<02:40,  1.56it/s]Loading trainS:   6%|▋         | 17/266 [00:10<02:37,  1.59it/s]Loading trainS:   7%|▋         | 18/266 [00:11<02:34,  1.61it/s]Loading trainS:   7%|▋         | 19/266 [00:11<02:30,  1.64it/s]Loading trainS:   8%|▊         | 20/266 [00:12<02:35,  1.58it/s]Loading trainS:   8%|▊         | 21/266 [00:13<02:32,  1.60it/s]Loading trainS:   8%|▊         | 22/266 [00:13<02:35,  1.57it/s]Loading trainS:   9%|▊         | 23/266 [00:14<02:30,  1.61it/s]Loading trainS:   9%|▉         | 24/266 [00:14<02:28,  1.63it/s]Loading trainS:   9%|▉         | 25/266 [00:15<02:34,  1.56it/s]Loading trainS:  10%|▉         | 26/266 [00:16<02:37,  1.52it/s]Loading trainS:  10%|█         | 27/266 [00:16<02:34,  1.55it/s]Loading trainS:  11%|█         | 28/266 [00:17<02:26,  1.63it/s]Loading trainS:  11%|█         | 29/266 [00:18<02:24,  1.64it/s]Loading trainS:  11%|█▏        | 30/266 [00:18<02:21,  1.67it/s]Loading trainS:  12%|█▏        | 31/266 [00:19<02:15,  1.73it/s]Loading trainS:  12%|█▏        | 32/266 [00:19<02:16,  1.72it/s]Loading trainS:  12%|█▏        | 33/266 [00:20<02:18,  1.68it/s]Loading trainS:  13%|█▎        | 34/266 [00:20<02:11,  1.76it/s]Loading trainS:  13%|█▎        | 35/266 [00:21<02:12,  1.74it/s]Loading trainS:  14%|█▎        | 36/266 [00:22<02:16,  1.68it/s]Loading trainS:  14%|█▍        | 37/266 [00:22<02:16,  1.67it/s]Loading trainS:  14%|█▍        | 38/266 [00:23<02:14,  1.69it/s]Loading trainS:  15%|█▍        | 39/266 [00:23<02:19,  1.63it/s]Loading trainS:  15%|█▌        | 40/266 [00:24<02:16,  1.66it/s]Loading trainS:  15%|█▌        | 41/266 [00:25<02:20,  1.60it/s]Loading trainS:  16%|█▌        | 42/266 [00:25<02:15,  1.65it/s]Loading trainS:  16%|█▌        | 43/266 [00:26<02:07,  1.75it/s]Loading trainS:  17%|█▋        | 44/266 [00:26<02:03,  1.79it/s]Loading trainS:  17%|█▋        | 45/266 [00:27<02:01,  1.82it/s]Loading trainS:  17%|█▋        | 46/266 [00:27<01:56,  1.89it/s]Loading trainS:  18%|█▊        | 47/266 [00:28<01:55,  1.90it/s]Loading trainS:  18%|█▊        | 48/266 [00:28<01:50,  1.97it/s]Loading trainS:  18%|█▊        | 49/266 [00:29<01:46,  2.05it/s]Loading trainS:  19%|█▉        | 50/266 [00:29<01:50,  1.96it/s]Loading trainS:  19%|█▉        | 51/266 [00:30<01:48,  1.98it/s]Loading trainS:  20%|█▉        | 52/266 [00:30<01:49,  1.96it/s]Loading trainS:  20%|█▉        | 53/266 [00:31<01:49,  1.94it/s]Loading trainS:  20%|██        | 54/266 [00:31<01:52,  1.89it/s]Loading trainS:  21%|██        | 55/266 [00:32<01:47,  1.96it/s]Loading trainS:  21%|██        | 56/266 [00:32<01:46,  1.97it/s]Loading trainS:  21%|██▏       | 57/266 [00:33<01:42,  2.03it/s]Loading trainS:  22%|██▏       | 58/266 [00:33<01:46,  1.95it/s]Loading trainS:  22%|██▏       | 59/266 [00:34<01:45,  1.96it/s]Loading trainS:  23%|██▎       | 60/266 [00:34<01:43,  1.98it/s]Loading trainS:  23%|██▎       | 61/266 [00:35<01:40,  2.03it/s]Loading trainS:  23%|██▎       | 62/266 [00:35<01:42,  1.99it/s]Loading trainS:  24%|██▎       | 63/266 [00:36<01:42,  1.98it/s]Loading trainS:  24%|██▍       | 64/266 [00:36<01:46,  1.89it/s]Loading trainS:  24%|██▍       | 65/266 [00:37<01:50,  1.82it/s]Loading trainS:  25%|██▍       | 66/266 [00:38<01:46,  1.87it/s]Loading trainS:  25%|██▌       | 67/266 [00:38<01:46,  1.87it/s]Loading trainS:  26%|██▌       | 68/266 [00:39<01:45,  1.87it/s]Loading trainS:  26%|██▌       | 69/266 [00:39<01:41,  1.93it/s]Loading trainS:  26%|██▋       | 70/266 [00:40<01:42,  1.92it/s]Loading trainS:  27%|██▋       | 71/266 [00:40<01:44,  1.87it/s]Loading trainS:  27%|██▋       | 72/266 [00:41<01:44,  1.86it/s]Loading trainS:  27%|██▋       | 73/266 [00:41<01:42,  1.88it/s]Loading trainS:  28%|██▊       | 74/266 [00:42<01:42,  1.88it/s]Loading trainS:  28%|██▊       | 75/266 [00:42<01:40,  1.90it/s]Loading trainS:  29%|██▊       | 76/266 [00:43<01:36,  1.96it/s]Loading trainS:  29%|██▉       | 77/266 [00:43<01:40,  1.88it/s]Loading trainS:  29%|██▉       | 78/266 [00:44<01:40,  1.86it/s]Loading trainS:  30%|██▉       | 79/266 [00:45<01:44,  1.79it/s]Loading trainS:  30%|███       | 80/266 [00:45<01:47,  1.73it/s]Loading trainS:  30%|███       | 81/266 [00:46<01:48,  1.70it/s]Loading trainS:  31%|███       | 82/266 [00:46<01:49,  1.67it/s]Loading trainS:  31%|███       | 83/266 [00:47<01:50,  1.65it/s]Loading trainS:  32%|███▏      | 84/266 [00:48<01:46,  1.70it/s]Loading trainS:  32%|███▏      | 85/266 [00:48<01:49,  1.65it/s]Loading trainS:  32%|███▏      | 86/266 [00:49<01:47,  1.67it/s]Loading trainS:  33%|███▎      | 87/266 [00:49<01:49,  1.63it/s]Loading trainS:  33%|███▎      | 88/266 [00:50<01:49,  1.63it/s]Loading trainS:  33%|███▎      | 89/266 [00:51<01:47,  1.64it/s]Loading trainS:  34%|███▍      | 90/266 [00:51<01:47,  1.64it/s]Loading trainS:  34%|███▍      | 91/266 [00:52<01:46,  1.65it/s]Loading trainS:  35%|███▍      | 92/266 [00:52<01:46,  1.64it/s]Loading trainS:  35%|███▍      | 93/266 [00:53<01:47,  1.60it/s]Loading trainS:  35%|███▌      | 94/266 [00:54<01:46,  1.61it/s]Loading trainS:  36%|███▌      | 95/266 [00:54<01:41,  1.68it/s]Loading trainS:  36%|███▌      | 96/266 [00:55<01:38,  1.72it/s]Loading trainS:  36%|███▋      | 97/266 [00:56<01:44,  1.62it/s]Loading trainS:  37%|███▋      | 98/266 [00:56<01:39,  1.69it/s]Loading trainS:  37%|███▋      | 99/266 [00:56<01:27,  1.91it/s]Loading trainS:  38%|███▊      | 100/266 [00:57<01:27,  1.89it/s]Loading trainS:  38%|███▊      | 101/266 [00:57<01:24,  1.95it/s]Loading trainS:  38%|███▊      | 102/266 [00:58<01:25,  1.92it/s]Loading trainS:  39%|███▊      | 103/266 [00:59<01:25,  1.91it/s]Loading trainS:  39%|███▉      | 104/266 [00:59<01:22,  1.95it/s]Loading trainS:  39%|███▉      | 105/266 [01:00<01:24,  1.91it/s]Loading trainS:  40%|███▉      | 106/266 [01:00<01:21,  1.97it/s]Loading trainS:  40%|████      | 107/266 [01:01<01:20,  1.97it/s]Loading trainS:  41%|████      | 108/266 [01:01<01:20,  1.95it/s]Loading trainS:  41%|████      | 109/266 [01:02<01:24,  1.85it/s]Loading trainS:  41%|████▏     | 110/266 [01:02<01:24,  1.85it/s]Loading trainS:  42%|████▏     | 111/266 [01:03<01:23,  1.87it/s]Loading trainS:  42%|████▏     | 112/266 [01:03<01:21,  1.89it/s]Loading trainS:  42%|████▏     | 113/266 [01:04<01:19,  1.92it/s]Loading trainS:  43%|████▎     | 114/266 [01:04<01:22,  1.85it/s]Loading trainS:  43%|████▎     | 115/266 [01:05<01:18,  1.94it/s]Loading trainS:  44%|████▎     | 116/266 [01:05<01:18,  1.91it/s]Loading trainS:  44%|████▍     | 117/266 [01:06<01:18,  1.89it/s]Loading trainS:  44%|████▍     | 118/266 [01:06<01:16,  1.94it/s]Loading trainS:  45%|████▍     | 119/266 [01:07<01:23,  1.76it/s]Loading trainS:  45%|████▌     | 120/266 [01:08<01:28,  1.65it/s]Loading trainS:  45%|████▌     | 121/266 [01:08<01:33,  1.55it/s]Loading trainS:  46%|████▌     | 122/266 [01:09<01:34,  1.52it/s]Loading trainS:  46%|████▌     | 123/266 [01:10<01:35,  1.50it/s]Loading trainS:  47%|████▋     | 124/266 [01:11<01:36,  1.47it/s]Loading trainS:  47%|████▋     | 125/266 [01:11<01:37,  1.45it/s]Loading trainS:  47%|████▋     | 126/266 [01:12<01:40,  1.39it/s]Loading trainS:  48%|████▊     | 127/266 [01:13<01:33,  1.49it/s]Loading trainS:  48%|████▊     | 128/266 [01:13<01:22,  1.67it/s]Loading trainS:  48%|████▊     | 129/266 [01:13<01:14,  1.84it/s]Loading trainS:  49%|████▉     | 130/266 [01:14<01:03,  2.14it/s]Loading trainS:  49%|████▉     | 131/266 [01:14<01:01,  2.21it/s]Loading trainS:  50%|████▉     | 132/266 [01:15<01:00,  2.21it/s]Loading trainS:  50%|█████     | 133/266 [01:15<01:04,  2.07it/s]Loading trainS:  50%|█████     | 134/266 [01:16<01:02,  2.10it/s]Loading trainS:  51%|█████     | 135/266 [01:16<01:03,  2.05it/s]Loading trainS:  51%|█████     | 136/266 [01:17<01:04,  2.02it/s]Loading trainS:  52%|█████▏    | 137/266 [01:17<01:01,  2.09it/s]Loading trainS:  52%|█████▏    | 138/266 [01:18<01:03,  2.02it/s]Loading trainS:  52%|█████▏    | 139/266 [01:18<01:02,  2.05it/s]Loading trainS:  53%|█████▎    | 140/266 [01:19<01:01,  2.06it/s]Loading trainS:  53%|█████▎    | 141/266 [01:19<01:01,  2.02it/s]Loading trainS:  53%|█████▎    | 142/266 [01:20<01:01,  2.02it/s]Loading trainS:  54%|█████▍    | 143/266 [01:20<00:58,  2.11it/s]Loading trainS:  54%|█████▍    | 144/266 [01:20<00:49,  2.47it/s]Loading trainS:  55%|█████▍    | 145/266 [01:21<00:46,  2.62it/s]Loading trainS:  55%|█████▍    | 146/266 [01:21<00:43,  2.74it/s]Loading trainS:  55%|█████▌    | 147/266 [01:21<00:47,  2.52it/s]Loading trainS:  56%|█████▌    | 148/266 [01:22<00:49,  2.38it/s]Loading trainS:  56%|█████▌    | 149/266 [01:22<00:50,  2.34it/s]Loading trainS:  56%|█████▋    | 150/266 [01:23<00:50,  2.28it/s]Loading trainS:  57%|█████▋    | 151/266 [01:23<00:52,  2.20it/s]Loading trainS:  57%|█████▋    | 152/266 [01:24<00:52,  2.19it/s]Loading trainS:  58%|█████▊    | 153/266 [01:24<00:52,  2.14it/s]Loading trainS:  58%|█████▊    | 154/266 [01:25<00:58,  1.93it/s]Loading trainS:  58%|█████▊    | 155/266 [01:25<00:53,  2.07it/s]Loading trainS:  59%|█████▊    | 156/266 [01:26<00:53,  2.06it/s]Loading trainS:  59%|█████▉    | 157/266 [01:26<00:48,  2.24it/s]Loading trainS:  59%|█████▉    | 158/266 [01:26<00:43,  2.48it/s]Loading trainS:  60%|█████▉    | 159/266 [01:27<00:45,  2.33it/s]Loading trainS:  60%|██████    | 160/266 [01:27<00:45,  2.32it/s]Loading trainS:  61%|██████    | 161/266 [01:28<00:43,  2.42it/s]Loading trainS:  61%|██████    | 162/266 [01:28<00:44,  2.34it/s]Loading trainS:  61%|██████▏   | 163/266 [01:29<00:41,  2.47it/s]Loading trainS:  62%|██████▏   | 164/266 [01:29<00:40,  2.54it/s]Loading trainS:  62%|██████▏   | 165/266 [01:29<00:38,  2.64it/s]Loading trainS:  62%|██████▏   | 166/266 [01:30<00:37,  2.66it/s]Loading trainS:  63%|██████▎   | 167/266 [01:30<00:38,  2.59it/s]Loading trainS:  63%|██████▎   | 168/266 [01:30<00:38,  2.52it/s]Loading trainS:  64%|██████▎   | 169/266 [01:31<00:39,  2.44it/s]Loading trainS:  64%|██████▍   | 170/266 [01:31<00:39,  2.44it/s]Loading trainS:  64%|██████▍   | 171/266 [01:32<00:38,  2.48it/s]Loading trainS:  65%|██████▍   | 172/266 [01:32<00:36,  2.57it/s]Loading trainS:  65%|██████▌   | 173/266 [01:33<00:41,  2.22it/s]Loading trainS:  65%|██████▌   | 174/266 [01:33<00:43,  2.09it/s]Loading trainS:  66%|██████▌   | 175/266 [01:34<00:43,  2.07it/s]Loading trainS:  66%|██████▌   | 176/266 [01:34<00:43,  2.07it/s]Loading trainS:  67%|██████▋   | 177/266 [01:35<00:42,  2.11it/s]Loading trainS:  67%|██████▋   | 178/266 [01:35<00:42,  2.09it/s]Loading trainS:  67%|██████▋   | 179/266 [01:36<00:40,  2.15it/s]Loading trainS:  68%|██████▊   | 180/266 [01:36<00:39,  2.20it/s]Loading trainS:  68%|██████▊   | 181/266 [01:36<00:38,  2.21it/s]Loading trainS:  68%|██████▊   | 182/266 [01:37<00:41,  2.04it/s]Loading trainS:  69%|██████▉   | 183/266 [01:37<00:37,  2.19it/s]Loading trainS:  69%|██████▉   | 184/266 [01:38<00:36,  2.24it/s]Loading trainS:  70%|██████▉   | 185/266 [01:38<00:34,  2.33it/s]Loading trainS:  70%|██████▉   | 186/266 [01:39<00:35,  2.23it/s]Loading trainS:  70%|███████   | 187/266 [01:39<00:36,  2.16it/s]Loading trainS:  71%|███████   | 188/266 [01:40<00:35,  2.19it/s]Loading trainS:  71%|███████   | 189/266 [01:40<00:33,  2.27it/s]Loading trainS:  71%|███████▏  | 190/266 [01:40<00:33,  2.29it/s]Loading trainS:  72%|███████▏  | 191/266 [01:41<00:32,  2.28it/s]Loading trainS:  72%|███████▏  | 192/266 [01:41<00:33,  2.23it/s]Loading trainS:  73%|███████▎  | 193/266 [01:42<00:32,  2.25it/s]Loading trainS:  73%|███████▎  | 194/266 [01:42<00:33,  2.13it/s]Loading trainS:  73%|███████▎  | 195/266 [01:43<00:35,  1.99it/s]Loading trainS:  74%|███████▎  | 196/266 [01:43<00:36,  1.94it/s]Loading trainS:  74%|███████▍  | 197/266 [01:44<00:35,  1.96it/s]Loading trainS:  74%|███████▍  | 198/266 [01:44<00:34,  1.99it/s]Loading trainS:  75%|███████▍  | 199/266 [01:45<00:29,  2.29it/s]Loading trainS:  75%|███████▌  | 200/266 [01:45<00:25,  2.57it/s]Loading trainS:  76%|███████▌  | 201/266 [01:45<00:23,  2.74it/s]Loading trainS:  76%|███████▌  | 202/266 [01:46<00:21,  2.97it/s]Loading trainS:  76%|███████▋  | 203/266 [01:46<00:23,  2.69it/s]Loading trainS:  77%|███████▋  | 204/266 [01:46<00:25,  2.45it/s]Loading trainS:  77%|███████▋  | 205/266 [01:47<00:24,  2.46it/s]Loading trainS:  77%|███████▋  | 206/266 [01:47<00:24,  2.43it/s]Loading trainS:  78%|███████▊  | 207/266 [01:48<00:23,  2.49it/s]Loading trainS:  78%|███████▊  | 208/266 [01:48<00:24,  2.38it/s]Loading trainS:  79%|███████▊  | 209/266 [01:49<00:25,  2.24it/s]Loading trainS:  79%|███████▉  | 210/266 [01:49<00:29,  1.93it/s]Loading trainS:  79%|███████▉  | 211/266 [01:50<00:29,  1.87it/s]Loading trainS:  80%|███████▉  | 212/266 [01:51<00:30,  1.75it/s]Loading trainS:  80%|████████  | 213/266 [01:51<00:30,  1.74it/s]Loading trainS:  80%|████████  | 214/266 [01:52<00:28,  1.81it/s]Loading trainS:  81%|████████  | 215/266 [01:52<00:29,  1.74it/s]Loading trainS:  81%|████████  | 216/266 [01:53<00:27,  1.80it/s]Loading trainS:  82%|████████▏ | 217/266 [01:53<00:27,  1.78it/s]Loading trainS:  82%|████████▏ | 218/266 [01:54<00:28,  1.71it/s]Loading trainS:  82%|████████▏ | 219/266 [01:55<00:28,  1.68it/s]Loading trainS:  83%|████████▎ | 220/266 [01:55<00:26,  1.73it/s]Loading trainS:  83%|████████▎ | 221/266 [01:56<00:25,  1.77it/s]Loading trainS:  83%|████████▎ | 222/266 [01:56<00:23,  1.85it/s]Loading trainS:  84%|████████▍ | 223/266 [01:57<00:22,  1.92it/s]Loading trainS:  84%|████████▍ | 224/266 [01:57<00:22,  1.90it/s]Loading trainS:  85%|████████▍ | 225/266 [01:58<00:21,  1.87it/s]Loading trainS:  85%|████████▍ | 226/266 [01:58<00:22,  1.79it/s]Loading trainS:  85%|████████▌ | 227/266 [01:59<00:21,  1.84it/s]Loading trainS:  86%|████████▌ | 228/266 [01:59<00:20,  1.88it/s]Loading trainS:  86%|████████▌ | 229/266 [02:00<00:19,  1.91it/s]Loading trainS:  86%|████████▋ | 230/266 [02:01<00:20,  1.79it/s]Loading trainS:  87%|████████▋ | 231/266 [02:01<00:20,  1.75it/s]Loading trainS:  87%|████████▋ | 232/266 [02:02<00:20,  1.68it/s]Loading trainS:  88%|████████▊ | 233/266 [02:02<00:20,  1.62it/s]Loading trainS:  88%|████████▊ | 234/266 [02:03<00:18,  1.74it/s]Loading trainS:  88%|████████▊ | 235/266 [02:03<00:17,  1.77it/s]Loading trainS:  89%|████████▊ | 236/266 [02:04<00:16,  1.80it/s]Loading trainS:  89%|████████▉ | 237/266 [02:05<00:17,  1.68it/s]Loading trainS:  89%|████████▉ | 238/266 [02:05<00:16,  1.72it/s]Loading trainS:  90%|████████▉ | 239/266 [02:06<00:16,  1.60it/s]Loading trainS:  90%|█████████ | 240/266 [02:07<00:16,  1.56it/s]Loading trainS:  91%|█████████ | 241/266 [02:07<00:15,  1.58it/s]Loading trainS:  91%|█████████ | 242/266 [02:08<00:14,  1.68it/s]Loading trainS:  91%|█████████▏| 243/266 [02:08<00:14,  1.62it/s]Loading trainS:  92%|█████████▏| 244/266 [02:09<00:12,  1.69it/s]Loading trainS:  92%|█████████▏| 245/266 [02:10<00:12,  1.74it/s]Loading trainS:  92%|█████████▏| 246/266 [02:10<00:11,  1.68it/s]Loading trainS:  93%|█████████▎| 247/266 [02:11<00:11,  1.66it/s]Loading trainS:  93%|█████████▎| 248/266 [02:11<00:10,  1.66it/s]Loading trainS:  94%|█████████▎| 249/266 [02:12<00:10,  1.64it/s]Loading trainS:  94%|█████████▍| 250/266 [02:13<00:09,  1.72it/s]Loading trainS:  94%|█████████▍| 251/266 [02:13<00:08,  1.70it/s]Loading trainS:  95%|█████████▍| 252/266 [02:14<00:08,  1.66it/s]Loading trainS:  95%|█████████▌| 253/266 [02:14<00:08,  1.58it/s]Loading trainS:  95%|█████████▌| 254/266 [02:15<00:07,  1.56it/s]Loading trainS:  96%|█████████▌| 255/266 [02:16<00:07,  1.54it/s]Loading trainS:  96%|█████████▌| 256/266 [02:16<00:06,  1.51it/s]Loading trainS:  97%|█████████▋| 257/266 [02:17<00:05,  1.51it/s]Loading trainS:  97%|█████████▋| 258/266 [02:18<00:05,  1.55it/s]Loading trainS:  97%|█████████▋| 259/266 [02:18<00:04,  1.59it/s]Loading trainS:  98%|█████████▊| 260/266 [02:19<00:03,  1.61it/s]Loading trainS:  98%|█████████▊| 261/266 [02:20<00:03,  1.61it/s]Loading trainS:  98%|█████████▊| 262/266 [02:20<00:02,  1.60it/s]Loading trainS:  99%|█████████▉| 263/266 [02:21<00:01,  1.59it/s]Loading trainS:  99%|█████████▉| 264/266 [02:21<00:01,  1.57it/s]Loading trainS: 100%|█████████▉| 265/266 [02:22<00:00,  1.58it/s]Loading trainS: 100%|██████████| 266/266 [02:23<00:00,  1.58it/s]Loading trainS: 100%|██████████| 266/266 [02:23<00:00,  1.86it/s]
Loading testS:   0%|          | 0/4 [00:00<?, ?it/s]Loading testS:  25%|██▌       | 1/4 [00:00<00:01,  1.65it/s]Loading testS:  50%|█████     | 2/4 [00:01<00:01,  1.55it/s]Loading testS:  75%|███████▌  | 3/4 [00:01<00:00,  1.88it/s]Loading testS: 100%|██████████| 4/4 [00:02<00:00,  1.99it/s]Loading testS: 100%|██████████| 4/4 [00:02<00:00,  1.96it/s]----------+++ 
CrossVal ['a']
TypeExperiment 10
CrossVal ['a']
(0/4) test vimp2_A_CSFn2
(1/4) test vimp2_ANON967_CSFn2
(2/4) test vimp2_B_CSFn2
(3/4) test vimp2_E_CSFn2
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 2  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 2  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 40) 400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 40) 160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 40) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 40) 14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 40) 160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 40) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 80)   320         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 54, 58, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 40) 44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 80) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 40) 28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 40) 160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 40) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 40) 14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 40) 160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 40) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 120 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 120 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  242         dropout_5[0][0]                  
==================================================================================================
Total params: 887,922
Trainable params: 886,322
Non-trainable params: 1,600
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 256 samples
Epoch 1/300
 - 85s - loss: 0.1175 - acc: 0.9856 - mDice: 0.7748 - val_loss: 0.2882 - val_acc: 0.9929 - val_mDice: 0.4274

Epoch 00001: val_mDice improved from -inf to 0.42738, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 78s - loss: 0.0757 - acc: 0.9921 - mDice: 0.8529 - val_loss: 0.2835 - val_acc: 0.9920 - val_mDice: 0.4372

Epoch 00002: val_mDice improved from 0.42738 to 0.43720, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 83s - loss: 0.0687 - acc: 0.9928 - mDice: 0.8664 - val_loss: 0.2780 - val_acc: 0.9924 - val_mDice: 0.4465

Epoch 00003: val_mDice improved from 0.43720 to 0.44649, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300
 - 85s - loss: 0.0611 - acc: 0.9935 - mDice: 0.8811 - val_loss: 0.4212 - val_acc: 0.9804 - val_mDice: 0.1630

Epoch 00004: val_mDice did not improve from 0.44649
Epoch 5/300
 - 85s - loss: 0.0581 - acc: 0.9938 - mDice: 0.8870 - val_loss: 0.2822 - val_acc: 0.9918 - val_mDice: 0.4280

Epoch 00005: val_mDice did not improve from 0.44649
Epoch 6/300
 - 85s - loss: 0.0532 - acc: 0.9942 - mDice: 0.8966 - val_loss: 0.2403 - val_acc: 0.9943 - val_mDice: 0.5034

Epoch 00006: val_mDice improved from 0.44649 to 0.50343, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 7/300
 - 84s - loss: 0.0522 - acc: 0.9943 - mDice: 0.8986 - val_loss: 0.2392 - val_acc: 0.9921 - val_mDice: 0.4868

Epoch 00007: val_mDice did not improve from 0.50343
Epoch 8/300
 - 84s - loss: 0.0494 - acc: 0.9946 - mDice: 0.9040 - val_loss: 0.2502 - val_acc: 0.9936 - val_mDice: 0.4826

Epoch 00008: val_mDice did not improve from 0.50343
Epoch 9/300
 - 85s - loss: 0.0450 - acc: 0.9949 - mDice: 0.9126 - val_loss: 0.2585 - val_acc: 0.9938 - val_mDice: 0.4826

Epoch 00009: val_mDice did not improve from 0.50343
Epoch 10/300
 - 84s - loss: 0.0442 - acc: 0.9950 - mDice: 0.9142 - val_loss: 0.2647 - val_acc: 0.9913 - val_mDice: 0.4563

Epoch 00010: val_mDice did not improve from 0.50343
Epoch 11/300
 - 85s - loss: 0.0432 - acc: 0.9951 - mDice: 0.9161 - val_loss: 0.2503 - val_acc: 0.9893 - val_mDice: 0.4717

Epoch 00011: val_mDice did not improve from 0.50343
Epoch 12/300
 - 85s - loss: 0.0415 - acc: 0.9952 - mDice: 0.9194 - val_loss: 0.2655 - val_acc: 0.9902 - val_mDice: 0.4298

Epoch 00012: val_mDice did not improve from 0.50343
Epoch 13/300
 - 86s - loss: 0.0410 - acc: 0.9953 - mDice: 0.9204 - val_loss: 0.1973 - val_acc: 0.9938 - val_mDice: 0.4959

Epoch 00013: val_mDice did not improve from 0.50343
Epoch 14/300
 - 86s - loss: 0.0395 - acc: 0.9954 - mDice: 0.9232 - val_loss: 0.2140 - val_acc: 0.9937 - val_mDice: 0.5073

Epoch 00014: val_mDice improved from 0.50343 to 0.50731, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 15/300
 - 85s - loss: 0.0385 - acc: 0.9955 - mDice: 0.9253 - val_loss: 0.2383 - val_acc: 0.9926 - val_mDice: 0.4951

Epoch 00015: val_mDice did not improve from 0.50731
Epoch 16/300
 - 86s - loss: 0.0385 - acc: 0.9956 - mDice: 0.9253 - val_loss: 0.1580 - val_acc: 0.9921 - val_mDice: 0.4894

Epoch 00016: val_mDice did not improve from 0.50731
Epoch 17/300
 - 86s - loss: 0.0363 - acc: 0.9957 - mDice: 0.9295 - val_loss: -1.4568e-02 - val_acc: 0.9940 - val_mDice: 0.4997

Epoch 00017: val_mDice did not improve from 0.50731
Epoch 18/300
 - 85s - loss: 0.0362 - acc: 0.9957 - mDice: 0.9298 - val_loss: 0.1310 - val_acc: 0.9922 - val_mDice: 0.4885

Epoch 00018: val_mDice did not improve from 0.50731
Epoch 19/300
 - 85s - loss: 0.0359 - acc: 0.9957 - mDice: 0.9303 - val_loss: 0.1072 - val_acc: 0.9936 - val_mDice: 0.4848

Epoch 00019: val_mDice did not improve from 0.50731
Epoch 20/300
 - 85s - loss: 0.0353 - acc: 0.9958 - mDice: 0.9315 - val_loss: 0.1458 - val_acc: 0.9936 - val_mDice: 0.4943

Epoch 00020: val_mDice did not improve from 0.50731
Epoch 21/300
 - 85s - loss: 0.0359 - acc: 0.9959 - mDice: 0.9303 - val_loss: 0.0340 - val_acc: 0.9940 - val_mDice: 0.4971

Epoch 00021: val_mDice did not improve from 0.50731
Epoch 22/300
 - 85s - loss: 0.0345 - acc: 0.9959 - mDice: 0.9330 - val_loss: 0.0600 - val_acc: 0.9890 - val_mDice: 0.3756

Epoch 00022: val_mDice did not improve from 0.50731
Epoch 23/300
 - 85s - loss: 0.0349 - acc: 0.9958 - mDice: 0.9324 - val_loss: 0.0150 - val_acc: 0.9940 - val_mDice: 0.5053

Epoch 00023: val_mDice did not improve from 0.50731
Epoch 24/300
 - 84s - loss: 0.0341 - acc: 0.9959 - mDice: 0.9338 - val_loss: 0.0946 - val_acc: 0.9927 - val_mDice: 0.4965

Epoch 00024: val_mDice did not improve from 0.50731
Epoch 25/300
 - 84s - loss: 0.0329 - acc: 0.9960 - mDice: 0.9362 - val_loss: 0.0719 - val_acc: 0.9937 - val_mDice: 0.4794

Epoch 00025: val_mDice did not improve from 0.50731
Epoch 26/300
 - 84s - loss: 0.0326 - acc: 0.9960 - mDice: 0.9367 - val_loss: 0.0309 - val_acc: 0.9941 - val_mDice: 0.5038

Epoch 00026: val_mDice did not improve from 0.50731
Epoch 27/300
 - 84s - loss: 0.0323 - acc: 0.9960 - mDice: 0.9373 - val_loss: -1.6809e-02 - val_acc: 0.9940 - val_mDice: 0.5005

Epoch 00027: val_mDice did not improve from 0.50731
Epoch 28/300
 - 84s - loss: 0.0341 - acc: 0.9959 - mDice: 0.9339 - val_loss: 0.0709 - val_acc: 0.9940 - val_mDice: 0.5025

Epoch 00028: val_mDice did not improve from 0.50731
Epoch 29/300
 - 84s - loss: 0.0326 - acc: 0.9961 - mDice: 0.9368 - val_loss: 0.1979 - val_acc: 0.9886 - val_mDice: 0.4740

Epoch 00029: val_mDice did not improve from 0.50731

Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 30/300
 - 84s - loss: 0.0301 - acc: 0.9963 - mDice: 0.9415 - val_loss: 0.0724 - val_acc: 0.9937 - val_mDice: 0.5026

Epoch 00030: val_mDice did not improve from 0.50731
Epoch 31/300
 - 84s - loss: 0.0305 - acc: 0.9963 - mDice: 0.9408 - val_loss: 0.0129 - val_acc: 0.9933 - val_mDice: 0.4699

Epoch 00031: val_mDice did not improve from 0.50731
Epoch 32/300
 - 84s - loss: 0.0290 - acc: 0.9963 - mDice: 0.9437 - val_loss: 0.0013 - val_acc: 0.9938 - val_mDice: 0.4870

Epoch 00032: val_mDice did not improve from 0.50731
Epoch 33/300
 - 84s - loss: 0.0290 - acc: 0.9963 - mDice: 0.9437 - val_loss: -3.5130e-02 - val_acc: 0.9937 - val_mDice: 0.4826

Epoch 00033: val_mDice did not improve from 0.50731
Epoch 34/300
 - 84s - loss: 0.0289 - acc: 0.9964 - mDice: 0.9439 - val_loss: -3.7471e-02 - val_acc: 0.9939 - val_mDice: 0.4854

Epoch 00034: val_mDice did not improve from 0.50731
Epoch 35/300
 - 84s - loss: 0.0291 - acc: 0.9963 - mDice: 0.9435 - val_loss: -4.5642e-02 - val_acc: 0.9940 - val_mDice: 0.5020

Epoch 00035: val_mDice did not improve from 0.50731
Epoch 36/300
 - 84s - loss: 0.0284 - acc: 0.9964 - mDice: 0.9450 - val_loss: 0.1343 - val_acc: 0.9936 - val_mDice: 0.5022

Epoch 00036: val_mDice did not improve from 0.50731
Epoch 37/300
 - 84s - loss: 0.0292 - acc: 0.9964 - mDice: 0.9434 - val_loss: -6.2654e-03 - val_acc: 0.9940 - val_mDice: 0.5073

Epoch 00037: val_mDice improved from 0.50731 to 0.50735, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 38/300
 - 84s - loss: 0.0283 - acc: 0.9964 - mDice: 0.9451 - val_loss: -1.9056e-02 - val_acc: 0.9931 - val_mDice: 0.4674

Epoch 00038: val_mDice did not improve from 0.50735
Epoch 39/300
 - 84s - loss: 0.0281 - acc: 0.9964 - mDice: 0.9455 - val_loss: 0.0374 - val_acc: 0.9940 - val_mDice: 0.4972

Epoch 00039: val_mDice did not improve from 0.50735
Epoch 40/300
 - 84s - loss: 0.0281 - acc: 0.9964 - mDice: 0.9455 - val_loss: 0.0700 - val_acc: 0.9940 - val_mDice: 0.4892

Epoch 00040: val_mDice did not improve from 0.50735
Epoch 41/300
 - 84s - loss: 0.0291 - acc: 0.9964 - mDice: 0.9436 - val_loss: 0.1096 - val_acc: 0.9935 - val_mDice: 0.5076

Epoch 00041: val_mDice improved from 0.50735 to 0.50763, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 42/300
 - 84s - loss: 0.0281 - acc: 0.9964 - mDice: 0.9455 - val_loss: 0.0047 - val_acc: 0.9937 - val_mDice: 0.4810

Epoch 00042: val_mDice did not improve from 0.50763
Epoch 43/300
 - 84s - loss: 0.0276 - acc: 0.9965 - mDice: 0.9466 - val_loss: 0.0564 - val_acc: 0.9940 - val_mDice: 0.5047

Epoch 00043: val_mDice did not improve from 0.50763
Epoch 44/300
 - 84s - loss: 0.0278 - acc: 0.9965 - mDice: 0.9461 - val_loss: 0.0887 - val_acc: 0.9938 - val_mDice: 0.4886

Epoch 00044: val_mDice did not improve from 0.50763

Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 45/300
 - 84s - loss: 0.0267 - acc: 0.9965 - mDice: 0.9483 - val_loss: 0.0542 - val_acc: 0.9939 - val_mDice: 0.4857

Epoch 00045: val_mDice did not improve from 0.50763
Epoch 46/300
 - 85s - loss: 0.0261 - acc: 0.9966 - mDice: 0.9495 - val_loss: 0.0471 - val_acc: 0.9938 - val_mDice: 0.4873

Epoch 00046: val_mDice did not improve from 0.50763
Epoch 47/300
 - 85s - loss: 0.0259 - acc: 0.9966 - mDice: 0.9498 - val_loss: 0.0521 - val_acc: 0.9940 - val_mDice: 0.4868

Epoch 00047: val_mDice did not improve from 0.50763
Epoch 48/300
 - 85s - loss: 0.0260 - acc: 0.9966 - mDice: 0.9496 - val_loss: 0.0955 - val_acc: 0.9938 - val_mDice: 0.4895

Epoch 00048: val_mDice did not improve from 0.50763
Epoch 49/300
 - 85s - loss: 0.0258 - acc: 0.9966 - mDice: 0.9500 - val_loss: 0.0463 - val_acc: 0.9937 - val_mDice: 0.4967

Epoch 00049: val_mDice did not improve from 0.50763
Epoch 50/300
 - 85s - loss: 0.0262 - acc: 0.9966 - mDice: 0.9493 - val_loss: 0.0915 - val_acc: 0.9937 - val_mDice: 0.4812

Epoch 00050: val_mDice did not improve from 0.50763
Epoch 51/300
 - 85s - loss: 0.0257 - acc: 0.9966 - mDice: 0.9502 - val_loss: 0.0321 - val_acc: 0.9940 - val_mDice: 0.4915

Epoch 00051: val_mDice did not improve from 0.50763
Epoch 52/300
 - 85s - loss: 0.0261 - acc: 0.9966 - mDice: 0.9494 - val_loss: 0.0126 - val_acc: 0.9939 - val_mDice: 0.4775

Epoch 00052: val_mDice did not improve from 0.50763
Epoch 53/300
 - 84s - loss: 0.0257 - acc: 0.9966 - mDice: 0.9502 - val_loss: 0.0653 - val_acc: 0.9940 - val_mDice: 0.4861

Epoch 00053: val_mDice did not improve from 0.50763
Epoch 54/300
 - 84s - loss: 0.0258 - acc: 0.9966 - mDice: 0.9500 - val_loss: 0.0022 - val_acc: 0.9941 - val_mDice: 0.4823

Epoch 00054: val_mDice did not improve from 0.50763
Epoch 55/300
 - 85s - loss: 0.0253 - acc: 0.9966 - mDice: 0.9509 - val_loss: 0.0290 - val_acc: 0.9940 - val_mDice: 0.4811

Epoch 00055: val_mDice did not improve from 0.50763
Epoch 56/300
 - 85s - loss: 0.0259 - acc: 0.9966 - mDice: 0.9498 - val_loss: -1.4325e-02 - val_acc: 0.9940 - val_mDice: 0.4805

Epoch 00056: val_mDice did not improve from 0.50763
Epoch 57/300
 - 85s - loss: 0.0255 - acc: 0.9966 - mDice: 0.9507 - val_loss: 0.0961 - val_acc: 0.9937 - val_mDice: 0.4891

Epoch 00057: val_mDice did not improve from 0.50763
Epoch 58/300
 - 84s - loss: 0.0255 - acc: 0.9967 - mDice: 0.9505 - val_loss: 0.0991 - val_acc: 0.9938 - val_mDice: 0.4837

Epoch 00058: val_mDice did not improve from 0.50763
Epoch 59/300
 - 84s - loss: 0.0247 - acc: 0.9966 - mDice: 0.9521 - val_loss: 0.1239 - val_acc: 0.9938 - val_mDice: 0.4863

Epoch 00059: val_mDice did not improve from 0.50763

Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 60/300
 - 84s - loss: 0.0248 - acc: 0.9967 - mDice: 0.9519 - val_loss: -6.4382e-03 - val_acc: 0.9939 - val_mDice: 0.4862

Epoch 00060: val_mDice did not improve from 0.50763
Epoch 61/300
 - 84s - loss: 0.0252 - acc: 0.9967 - mDice: 0.9512 - val_loss: 0.0476 - val_acc: 0.9939 - val_mDice: 0.4858

Epoch 00061: val_mDice did not improve from 0.50763
Epoch 62/300
 - 84s - loss: 0.0247 - acc: 0.9967 - mDice: 0.9521 - val_loss: 0.0199 - val_acc: 0.9940 - val_mDice: 0.4847

Epoch 00062: val_mDice did not improve from 0.50763
Epoch 63/300
 - 84s - loss: 0.0245 - acc: 0.9967 - mDice: 0.9526 - val_loss: 0.0326 - val_acc: 0.9940 - val_mDice: 0.4879

Epoch 00063: val_mDice did not improve from 0.50763
Epoch 64/300
 - 85s - loss: 0.0245 - acc: 0.9967 - mDice: 0.9525 - val_loss: 0.0591 - val_acc: 0.9939 - val_mDice: 0.4861

Epoch 00064: val_mDice did not improve from 0.50763
Epoch 65/300
 - 85s - loss: 0.0246 - acc: 0.9967 - mDice: 0.9524 - val_loss: 0.1047 - val_acc: 0.9939 - val_mDice: 0.4901

Epoch 00065: val_mDice did not improve from 0.50763
Epoch 66/300
 - 84s - loss: 0.0241 - acc: 0.9967 - mDice: 0.9533 - val_loss: 0.0453 - val_acc: 0.9940 - val_mDice: 0.4854

Epoch 00066: val_mDice did not improve from 0.50763
Epoch 67/300
 - 85s - loss: 0.0241 - acc: 0.9967 - mDice: 0.9534 - val_loss: 0.1013 - val_acc: 0.9937 - val_mDice: 0.4873

Epoch 00067: val_mDice did not improve from 0.50763
Epoch 68/300
 - 86s - loss: 0.0248 - acc: 0.9967 - mDice: 0.9521 - val_loss: 0.0230 - val_acc: 0.9940 - val_mDice: 0.4986

Epoch 00068: val_mDice did not improve from 0.50763
Epoch 69/300
 - 86s - loss: 0.0241 - acc: 0.9967 - mDice: 0.9533 - val_loss: 0.0686 - val_acc: 0.9940 - val_mDice: 0.4946

Epoch 00069: val_mDice did not improve from 0.50763
Epoch 70/300
 - 86s - loss: 0.0242 - acc: 0.9967 - mDice: 0.9531 - val_loss: 0.0229 - val_acc: 0.9938 - val_mDice: 0.4828

Epoch 00070: val_mDice did not improve from 0.50763
Epoch 71/300
 - 86s - loss: 0.0248 - acc: 0.9967 - mDice: 0.9519 - val_loss: 0.0458 - val_acc: 0.9939 - val_mDice: 0.4866

Epoch 00071: val_mDice did not improve from 0.50763
Epoch 72/300
 - 86s - loss: 0.0243 - acc: 0.9967 - mDice: 0.9529 - val_loss: 0.0129 - val_acc: 0.9940 - val_mDice: 0.4850

Epoch 00072: val_mDice did not improve from 0.50763
Epoch 73/300
 - 84s - loss: 0.0251 - acc: 0.9967 - mDice: 0.9514 - val_loss: 0.0545 - val_acc: 0.9939 - val_mDice: 0.4869

Epoch 00073: val_mDice did not improve from 0.50763
Epoch 74/300
 - 84s - loss: 0.0245 - acc: 0.9967 - mDice: 0.9525 - val_loss: 0.0306 - val_acc: 0.9939 - val_mDice: 0.4862

Epoch 00074: val_mDice did not improve from 0.50763

Epoch 00074: ReduceLROnPlateau reducing learning rate to 9e-05.
Epoch 75/300
 - 84s - loss: 0.0237 - acc: 0.9967 - mDice: 0.9541 - val_loss: 0.0349 - val_acc: 0.9940 - val_mDice: 0.4878

Epoch 00075: val_mDice did not improve from 0.50763
Epoch 76/300
 - 84s - loss: 0.0242 - acc: 0.9968 - mDice: 0.9532 - val_loss: 0.0106 - val_acc: 0.9939 - val_mDice: 0.4885

Epoch 00076: val_mDice did not improve from 0.50763
Epoch 77/300
 - 84s - loss: 0.0236 - acc: 0.9968 - mDice: 0.9542 - val_loss: 0.0558 - val_acc: 0.9939 - val_mDice: 0.4851

Epoch 00077: val_mDice did not improve from 0.50763
Epoch 78/300
 - 84s - loss: 0.0239 - acc: 0.9968 - mDice: 0.9537 - val_loss: 0.0174 - val_acc: 0.9940 - val_mDice: 0.4876

Epoch 00078: val_mDice did not improve from 0.50763
Epoch 79/300
 - 85s - loss: 0.0244 - acc: 0.9968 - mDice: 0.9527 - val_loss: 0.0612 - val_acc: 0.9938 - val_mDice: 0.4838

Epoch 00079: val_mDice did not improve from 0.50763
Epoch 80/300
 - 86s - loss: 0.0240 - acc: 0.9968 - mDice: 0.9536 - val_loss: 0.0207 - val_acc: 0.9938 - val_mDice: 0.4872

Epoch 00080: val_mDice did not improve from 0.50763
Epoch 81/300
 - 86s - loss: 0.0235 - acc: 0.9968 - mDice: 0.9544 - val_loss: 0.0369 - val_acc: 0.9938 - val_mDice: 0.4884

Epoch 00081: val_mDice did not improve from 0.50763
Restoring model weights from the end of the best epoch
Epoch 00081: early stopping
{'val_loss': [0.2881768165389076, 0.283486605854705, 0.2779859863803722, 0.421199708012864, 0.28216585173504427, 0.2402898503933102, 0.23921059619169682, 0.2502466789446771, 0.25849319592816755, 0.2646507389144972, 0.25034231186145917, 0.2655334284645505, 0.19726212043315172, 0.21402459841920063, 0.2383066323818639, 0.15795495989732444, -0.014568092243280262, 0.13099334842991084, 0.10722372122108936, 0.14582173968665302, 0.033992014767136425, 0.06003716302802786, 0.015046636632177979, 0.09460524073801935, 0.07185398798901588, 0.03092037804890424, -0.016808793181553483, 0.07086377375526354, 0.19793354370631278, 0.07235370686976239, 0.012909472978208214, 0.0012870787177234888, -0.035130036470945925, -0.03747079469030723, -0.045641654171049595, 0.13425649306736887, -0.006265391130000353, -0.019056268967688084, 0.037400093220639974, 0.07002830208512023, 0.10956549673574045, 0.004691336071118712, 0.05640387116000056, 0.08865701826289296, 0.05422972020460293, 0.04707159899407998, 0.05210700206225738, 0.09547248535091057, 0.046348462579771876, 0.09152218932285905, 0.03211799677228555, 0.012558531074319035, 0.065281183749903, 0.0021751796011812985, 0.028974346292670816, -0.014324997668154538, 0.09609157301019877, 0.09913584624882787, 0.12388217239640653, -0.006438247510232031, 0.047598973906133324, 0.01985581446206197, 0.03261926188133657, 0.05912978993728757, 0.10471139516448602, 0.045316783885937184, 0.10132237756624818, 0.022996319748926908, 0.06861174252117053, 0.022910403495188802, 0.04584175307536498, 0.012939015170559287, 0.05446402309462428, 0.030568344751372933, 0.03492816677317023, 0.010554255626630038, 0.05580590438330546, 0.017401672201231122, 0.06119626807048917, 0.020650948805268854, 0.036866547306999564], 'val_acc': [0.9928853125311434, 0.992014451418072, 0.9923767615109682, 0.9803627375513315, 0.9918498168699443, 0.9942616024054587, 0.9920615283772349, 0.9936180491931736, 0.9937745742499828, 0.9912932524457574, 0.989321424625814, 0.9901738809421659, 0.9938310100696981, 0.9937468157149851, 0.992645850405097, 0.992103623226285, 0.9940140289254487, 0.9922236669808626, 0.9936336348764598, 0.9935927893966436, 0.994033055845648, 0.9889547377824783, 0.9940093606710434, 0.99265645025298, 0.9936819677241147, 0.9941175477579236, 0.9939706944860518, 0.9939641449600458, 0.9885768345557153, 0.9936797842383385, 0.9932616553269327, 0.9937910935841501, 0.9937327904626727, 0.9938603141345084, 0.9939541635103524, 0.9936068202368915, 0.9940062337554991, 0.9930970254354179, 0.9939781753346324, 0.9939956353046, 0.9934820970520377, 0.9937427649274468, 0.9939822293817997, 0.9937655264511704, 0.9939148779958487, 0.9938210262916982, 0.994031498208642, 0.9938469054177403, 0.9936651303432882, 0.9937415216118097, 0.99399688327685, 0.99389773234725, 0.9939953233115375, 0.9940542574040592, 0.9939625849947333, 0.99402339104563, 0.9937402736395597, 0.9938422394916415, 0.9937820550985634, 0.9938827659934759, 0.993919872213155, 0.993988461792469, 0.994013721588999, 0.9939273539930582, 0.9939223611727357, 0.9939903384074569, 0.9937153281643987, 0.994013721588999, 0.9939853465184569, 0.9937761295586824, 0.9939167546108365, 0.9940286902710795, 0.9938621874898672, 0.9938715440221131, 0.9939625882543623, 0.9938512779772282, 0.9938606237992644, 0.9939847248606384, 0.9938228959217668, 0.9938288214616477, 0.9938306934200227], 'val_mDice': [0.4273821946045122, 0.43720256898180043, 0.44648591754723443, 0.16296835447692015, 0.42800980314497206, 0.5034343886654831, 0.48682372434996957, 0.48264858458423987, 0.4825820040423423, 0.45626067265402526, 0.47172124963253736, 0.42978463461622596, 0.49594908370636426, 0.5073136661667377, 0.49514427490066737, 0.489417786013221, 0.49973615212365985, 0.4884807369671762, 0.48477595730219036, 0.4942951409611851, 0.49707291182130575, 0.3755840449593961, 0.5053186288569123, 0.4965324723161757, 0.4794222686905414, 0.5037968861870468, 0.500471328268759, 0.5024693285774706, 0.473957258509472, 0.5026267352513969, 0.4699068231275305, 0.487030299024526, 0.4825723568774265, 0.4853944748174399, 0.501969998003915, 0.5021682981168851, 0.5073488695779815, 0.4673960676882416, 0.497227133018896, 0.48922674322966486, 0.5076340777304722, 0.4810431796187764, 0.5046810512430966, 0.4886174900457263, 0.4857040283968672, 0.48726748034823686, 0.48676111390705046, 0.4895053949439898, 0.49665852333419025, 0.4812389181461185, 0.49152598454384133, 0.4774925945093855, 0.4860965046424771, 0.482341650640592, 0.4810967664058552, 0.4805260442663055, 0.48908724915236235, 0.4836729751076998, 0.48630786498337436, 0.4862082772888244, 0.4858009511881445, 0.4846642760094381, 0.4878565180115402, 0.4860858665779233, 0.4900824569631368, 0.48542698728851974, 0.48727023880931325, 0.4985937039600685, 0.49464268726296723, 0.4828040988650173, 0.48664355535449205, 0.485003228703333, 0.4868972836993635, 0.4862122843042612, 0.4878428082764393, 0.488514507887885, 0.4851024190429598, 0.4876164975576103, 0.48377091996371746, 0.4871814558282566, 0.488366211082561], 'loss': [0.11745659512804958, 0.07565455859936224, 0.0686550056733874, 0.06114474654048802, 0.05807166393191415, 0.05318667852422897, 0.052163521338834624, 0.0493759362236318, 0.04500016160729151, 0.044164673116331224, 0.043182063528791156, 0.041518462536828615, 0.04095608984290397, 0.0395417416458957, 0.03848554456263064, 0.038470924340350376, 0.036304920222737916, 0.036178135244816154, 0.03591680043272348, 0.03528810281921544, 0.03587083032706812, 0.03452728086662885, 0.034851988526265684, 0.034104532850346835, 0.03286137571055202, 0.03260133861352775, 0.032317796092083524, 0.03405463326589508, 0.03255569414334443, 0.03014704675015581, 0.030488241943032453, 0.029046345268847622, 0.02902541802751349, 0.028916975561855313, 0.029131681261971753, 0.028352649501769734, 0.02919345174559985, 0.028348156947097643, 0.028114007159407938, 0.02813038870430829, 0.029068069056626934, 0.02808133722220728, 0.0275581860638903, 0.027815284477759977, 0.02670635455602329, 0.026089822101104103, 0.02594396421077485, 0.025991177896845557, 0.025818792434606844, 0.02616851925036211, 0.025696458283085102, 0.026126666839175125, 0.025728121364388134, 0.025803292900119575, 0.02534600321917192, 0.025894803875925042, 0.025467698141695302, 0.0255271023202333, 0.02474881095920579, 0.02483883692877653, 0.02518728491084645, 0.0247401249681226, 0.02447271784281667, 0.024522644591114032, 0.024570214792703438, 0.024142608578035858, 0.02410112861364273, 0.024751437823400134, 0.02410277553994273, 0.024204674277610864, 0.024805792878421187, 0.024317323140937635, 0.02505306011918282, 0.024514807084129706, 0.023694570826725134, 0.024184963277986797, 0.02364188304924724, 0.02389974601456855, 0.02441180399330676, 0.02399094090033619, 0.023544287559004683], 'acc': [0.9855862035386527, 0.9920971120804102, 0.9927746697115929, 0.9934738253250733, 0.9937911584663436, 0.994178700470628, 0.9943239539428859, 0.9945612987242635, 0.9948735777836194, 0.9950077907194503, 0.9951204970293764, 0.9952241474498754, 0.9952987106859968, 0.9954385620674855, 0.9955325170276306, 0.9955548120733336, 0.9956770105493239, 0.9957009140894196, 0.9957261808286689, 0.995799862910497, 0.9958558725025425, 0.995880620542187, 0.9958184152457223, 0.9959189781151514, 0.9960178577679714, 0.9960349578744958, 0.9960364553797352, 0.9959433721764739, 0.9960944648056035, 0.996261025002793, 0.996283000716629, 0.9963426642873858, 0.9963279932692337, 0.9963522904756054, 0.9963481967299882, 0.9963809610438067, 0.9963554213641759, 0.9963859707633004, 0.9964363919784206, 0.9964293484573125, 0.9963972094694163, 0.9964481141767505, 0.996467598780697, 0.9964688226114904, 0.996520994258589, 0.9965805418817953, 0.9965645632871812, 0.9966031473882664, 0.9965992472971498, 0.9965883509074269, 0.9966134461872362, 0.9966248574400386, 0.9966210318980101, 0.9966097096955304, 0.9966247100385207, 0.9966446521931219, 0.9966288960407984, 0.9966640824278264, 0.9966410455061637, 0.9966915237982541, 0.9967090936298222, 0.9966954936808904, 0.9966978991337738, 0.9966958315097955, 0.9967026204559825, 0.9967119260173344, 0.9967028478843708, 0.9967189332230298, 0.996739758048529, 0.9967142730082205, 0.9967282531955897, 0.9967167212307433, 0.9967400038700784, 0.9967233426578925, 0.996740361089364, 0.9967569845249336, 0.9967531218711018, 0.9967510768438419, 0.9967573693339606, 0.9967525369117965, 0.9967605181240075], 'mDice': [0.7747896980567378, 0.8528584220837307, 0.8664473235528801, 0.8810752089311209, 0.887041240446859, 0.8966065700638459, 0.898567120743169, 0.9040106676034081, 0.9125936566654559, 0.9141916297683461, 0.916091811661107, 0.9193614307988793, 0.9204455342382258, 0.9232001670888247, 0.925253755347987, 0.9252757863313101, 0.9295396271123971, 0.9297757945332403, 0.9302817409263195, 0.9315095116061118, 0.930310539439303, 0.9329749332136901, 0.9323651167910676, 0.9338012470266358, 0.9362374594350912, 0.9367449205553572, 0.9373076970020182, 0.9338895851925606, 0.9368066300744169, 0.9415233552324325, 0.9408301874273286, 0.9436842102725788, 0.9437283528824779, 0.9439299086307252, 0.9435020754932042, 0.9450481581025885, 0.9433737467414787, 0.9450527900109048, 0.9454916142220695, 0.9454641357606139, 0.9436046139671368, 0.945549772192103, 0.9465854004020265, 0.9460697337990233, 0.9482564214728249, 0.9494583087404029, 0.9497567684209706, 0.9496481842635691, 0.9499875449671679, 0.9492932657089136, 0.9502308675050875, 0.9493565764215415, 0.9501532343146665, 0.9500102591359464, 0.9509185848646606, 0.9498110048250411, 0.9506675971500326, 0.950537118185158, 0.9520948494479287, 0.9518944947398524, 0.9511878089067121, 0.952090619110931, 0.9526179656481596, 0.9525157956416646, 0.9524202220279525, 0.9532705462462286, 0.9533557013184459, 0.9520544690843159, 0.9533424636510741, 0.9531395572381691, 0.951939698945386, 0.9529178065609347, 0.9514368352912729, 0.9525154816408536, 0.954149886133004, 0.9531645883981451, 0.9542484854632476, 0.9537344952486694, 0.9527073206837285, 0.9535521607691505, 0.9544378657699448], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05, 9e-05]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:00<00:02,  1.30it/s]predicting test subjects:  50%|█████     | 2/4 [00:00<00:01,  1.71it/s]predicting test subjects:  75%|███████▌  | 3/4 [00:01<00:00,  2.18it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  2.70it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  3.20it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:06,  4.00it/s]predicting train subjects:   1%|          | 2/266 [00:00<01:00,  4.38it/s]predicting train subjects:   1%|          | 3/266 [00:00<01:06,  3.96it/s]predicting train subjects:   2%|▏         | 4/266 [00:01<01:12,  3.62it/s]predicting train subjects:   2%|▏         | 5/266 [00:01<01:10,  3.72it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<01:03,  4.09it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<00:58,  4.40it/s]predicting train subjects:   3%|▎         | 8/266 [00:01<00:55,  4.64it/s]predicting train subjects:   3%|▎         | 9/266 [00:02<00:53,  4.78it/s]predicting train subjects:   4%|▍         | 10/266 [00:02<00:51,  4.98it/s]predicting train subjects:   4%|▍         | 11/266 [00:02<00:49,  5.10it/s]predicting train subjects:   5%|▍         | 12/266 [00:02<00:48,  5.22it/s]predicting train subjects:   5%|▍         | 13/266 [00:02<00:48,  5.26it/s]predicting train subjects:   5%|▌         | 14/266 [00:02<00:47,  5.30it/s]predicting train subjects:   6%|▌         | 15/266 [00:03<00:47,  5.33it/s]predicting train subjects:   6%|▌         | 16/266 [00:03<00:46,  5.35it/s]predicting train subjects:   6%|▋         | 17/266 [00:03<00:45,  5.42it/s]predicting train subjects:   7%|▋         | 18/266 [00:03<00:45,  5.43it/s]predicting train subjects:   7%|▋         | 19/266 [00:03<00:45,  5.44it/s]predicting train subjects:   8%|▊         | 20/266 [00:04<00:45,  5.45it/s]predicting train subjects:   8%|▊         | 21/266 [00:04<00:44,  5.47it/s]predicting train subjects:   8%|▊         | 22/266 [00:04<00:44,  5.47it/s]predicting train subjects:   9%|▊         | 23/266 [00:04<00:44,  5.48it/s]predicting train subjects:   9%|▉         | 24/266 [00:04<00:43,  5.54it/s]predicting train subjects:   9%|▉         | 25/266 [00:04<00:43,  5.57it/s]predicting train subjects:  10%|▉         | 26/266 [00:05<00:42,  5.62it/s]predicting train subjects:  10%|█         | 27/266 [00:05<00:42,  5.63it/s]predicting train subjects:  11%|█         | 28/266 [00:05<00:42,  5.66it/s]predicting train subjects:  11%|█         | 29/266 [00:05<00:41,  5.65it/s]predicting train subjects:  11%|█▏        | 30/266 [00:05<00:41,  5.64it/s]predicting train subjects:  12%|█▏        | 31/266 [00:06<00:41,  5.62it/s]predicting train subjects:  12%|█▏        | 32/266 [00:06<00:41,  5.63it/s]predicting train subjects:  12%|█▏        | 33/266 [00:06<00:41,  5.62it/s]predicting train subjects:  13%|█▎        | 34/266 [00:06<00:41,  5.61it/s]predicting train subjects:  13%|█▎        | 35/266 [00:06<00:41,  5.63it/s]predicting train subjects:  14%|█▎        | 36/266 [00:06<00:40,  5.69it/s]predicting train subjects:  14%|█▍        | 37/266 [00:07<00:39,  5.73it/s]predicting train subjects:  14%|█▍        | 38/266 [00:07<00:39,  5.72it/s]predicting train subjects:  15%|█▍        | 39/266 [00:07<00:39,  5.76it/s]predicting train subjects:  15%|█▌        | 40/266 [00:07<00:39,  5.77it/s]predicting train subjects:  15%|█▌        | 41/266 [00:07<00:38,  5.79it/s]predicting train subjects:  16%|█▌        | 42/266 [00:07<00:39,  5.74it/s]predicting train subjects:  16%|█▌        | 43/266 [00:08<00:38,  5.77it/s]predicting train subjects:  17%|█▋        | 44/266 [00:08<00:38,  5.74it/s]predicting train subjects:  17%|█▋        | 45/266 [00:08<00:39,  5.60it/s]predicting train subjects:  17%|█▋        | 46/266 [00:08<00:36,  5.96it/s]predicting train subjects:  18%|█▊        | 47/266 [00:08<00:35,  6.13it/s]predicting train subjects:  18%|█▊        | 48/266 [00:08<00:36,  5.93it/s]predicting train subjects:  18%|█▊        | 49/266 [00:09<00:37,  5.76it/s]predicting train subjects:  19%|█▉        | 50/266 [00:09<00:36,  5.98it/s]predicting train subjects:  19%|█▉        | 51/266 [00:09<00:34,  6.25it/s]predicting train subjects:  20%|█▉        | 52/266 [00:09<00:33,  6.46it/s]predicting train subjects:  20%|█▉        | 53/266 [00:09<00:32,  6.60it/s]predicting train subjects:  20%|██        | 54/266 [00:09<00:31,  6.64it/s]predicting train subjects:  21%|██        | 55/266 [00:10<00:33,  6.38it/s]predicting train subjects:  21%|██        | 56/266 [00:10<00:32,  6.44it/s]predicting train subjects:  21%|██▏       | 57/266 [00:10<00:31,  6.61it/s]predicting train subjects:  22%|██▏       | 58/266 [00:10<00:30,  6.74it/s]predicting train subjects:  22%|██▏       | 59/266 [00:10<00:30,  6.86it/s]predicting train subjects:  23%|██▎       | 60/266 [00:10<00:30,  6.80it/s]predicting train subjects:  23%|██▎       | 61/266 [00:10<00:30,  6.72it/s]predicting train subjects:  23%|██▎       | 62/266 [00:11<00:30,  6.70it/s]predicting train subjects:  24%|██▎       | 63/266 [00:11<00:31,  6.45it/s]predicting train subjects:  24%|██▍       | 64/266 [00:11<00:31,  6.50it/s]predicting train subjects:  24%|██▍       | 65/266 [00:11<00:30,  6.55it/s]predicting train subjects:  25%|██▍       | 66/266 [00:11<00:30,  6.60it/s]predicting train subjects:  25%|██▌       | 67/266 [00:11<00:29,  6.64it/s]predicting train subjects:  26%|██▌       | 68/266 [00:12<00:29,  6.63it/s]predicting train subjects:  26%|██▌       | 69/266 [00:12<00:29,  6.64it/s]predicting train subjects:  26%|██▋       | 70/266 [00:12<00:29,  6.63it/s]predicting train subjects:  27%|██▋       | 71/266 [00:12<00:29,  6.64it/s]predicting train subjects:  27%|██▋       | 72/266 [00:12<00:29,  6.64it/s]predicting train subjects:  27%|██▋       | 73/266 [00:12<00:28,  6.68it/s]predicting train subjects:  28%|██▊       | 74/266 [00:12<00:28,  6.75it/s]predicting train subjects:  28%|██▊       | 75/266 [00:13<00:28,  6.82it/s]predicting train subjects:  29%|██▊       | 76/266 [00:13<00:27,  6.82it/s]predicting train subjects:  29%|██▉       | 77/266 [00:13<00:27,  6.77it/s]predicting train subjects:  29%|██▉       | 78/266 [00:13<00:29,  6.35it/s]predicting train subjects:  30%|██▉       | 79/266 [00:13<00:30,  6.05it/s]predicting train subjects:  30%|███       | 80/266 [00:13<00:31,  5.96it/s]predicting train subjects:  30%|███       | 81/266 [00:14<00:31,  5.87it/s]predicting train subjects:  31%|███       | 82/266 [00:14<00:32,  5.70it/s]predicting train subjects:  31%|███       | 83/266 [00:14<00:32,  5.58it/s]predicting train subjects:  32%|███▏      | 84/266 [00:14<00:32,  5.61it/s]predicting train subjects:  32%|███▏      | 85/266 [00:14<00:32,  5.63it/s]predicting train subjects:  32%|███▏      | 86/266 [00:14<00:31,  5.65it/s]predicting train subjects:  33%|███▎      | 87/266 [00:15<00:32,  5.57it/s]predicting train subjects:  33%|███▎      | 88/266 [00:15<00:31,  5.56it/s]predicting train subjects:  33%|███▎      | 89/266 [00:15<00:32,  5.52it/s]predicting train subjects:  34%|███▍      | 90/266 [00:15<00:32,  5.43it/s]predicting train subjects:  34%|███▍      | 91/266 [00:15<00:31,  5.50it/s]predicting train subjects:  35%|███▍      | 92/266 [00:16<00:33,  5.27it/s]predicting train subjects:  35%|███▍      | 93/266 [00:16<00:32,  5.32it/s]predicting train subjects:  35%|███▌      | 94/266 [00:16<00:31,  5.39it/s]predicting train subjects:  36%|███▌      | 95/266 [00:16<00:31,  5.41it/s]predicting train subjects:  36%|███▌      | 96/266 [00:16<00:38,  4.41it/s]predicting train subjects:  36%|███▋      | 97/266 [00:17<00:40,  4.19it/s]predicting train subjects:  37%|███▋      | 98/266 [00:17<00:38,  4.36it/s]predicting train subjects:  37%|███▋      | 99/266 [00:17<00:40,  4.13it/s]predicting train subjects:  38%|███▊      | 100/266 [00:17<00:35,  4.68it/s]predicting train subjects:  38%|███▊      | 101/266 [00:18<00:32,  5.09it/s]predicting train subjects:  38%|███▊      | 102/266 [00:18<00:30,  5.38it/s]predicting train subjects:  39%|███▊      | 103/266 [00:18<00:29,  5.56it/s]predicting train subjects:  39%|███▉      | 104/266 [00:18<00:28,  5.65it/s]predicting train subjects:  39%|███▉      | 105/266 [00:18<00:27,  5.77it/s]predicting train subjects:  40%|███▉      | 106/266 [00:18<00:27,  5.90it/s]predicting train subjects:  40%|████      | 107/266 [00:18<00:26,  6.06it/s]predicting train subjects:  41%|████      | 108/266 [00:19<00:25,  6.11it/s]predicting train subjects:  41%|████      | 109/266 [00:19<00:25,  6.22it/s]predicting train subjects:  41%|████▏     | 110/266 [00:19<00:25,  6.18it/s]predicting train subjects:  42%|████▏     | 111/266 [00:19<00:25,  6.14it/s]predicting train subjects:  42%|████▏     | 112/266 [00:19<00:25,  6.10it/s]predicting train subjects:  42%|████▏     | 113/266 [00:19<00:25,  6.12it/s]predicting train subjects:  43%|████▎     | 114/266 [00:20<00:25,  6.06it/s]predicting train subjects:  43%|████▎     | 115/266 [00:20<00:24,  6.05it/s]predicting train subjects:  44%|████▎     | 116/266 [00:20<00:25,  5.86it/s]predicting train subjects:  44%|████▍     | 117/266 [00:20<00:25,  5.85it/s]predicting train subjects:  44%|████▍     | 118/266 [00:20<00:25,  5.83it/s]predicting train subjects:  45%|████▍     | 119/266 [00:21<00:25,  5.66it/s]predicting train subjects:  45%|████▌     | 120/266 [00:21<00:26,  5.58it/s]predicting train subjects:  45%|████▌     | 121/266 [00:21<00:26,  5.48it/s]predicting train subjects:  46%|████▌     | 122/266 [00:21<00:26,  5.45it/s]predicting train subjects:  46%|████▌     | 123/266 [00:21<00:26,  5.43it/s]predicting train subjects:  47%|████▋     | 124/266 [00:21<00:26,  5.45it/s]predicting train subjects:  47%|████▋     | 125/266 [00:22<00:26,  5.40it/s]predicting train subjects:  47%|████▋     | 126/266 [00:22<00:26,  5.30it/s]predicting train subjects:  48%|████▊     | 127/266 [00:22<00:26,  5.29it/s]predicting train subjects:  48%|████▊     | 128/266 [00:22<00:26,  5.31it/s]predicting train subjects:  48%|████▊     | 129/266 [00:22<00:25,  5.31it/s]predicting train subjects:  49%|████▉     | 130/266 [00:23<00:25,  5.30it/s]predicting train subjects:  49%|████▉     | 131/266 [00:23<00:25,  5.34it/s]predicting train subjects:  50%|████▉     | 132/266 [00:23<00:25,  5.31it/s]predicting train subjects:  50%|█████     | 133/266 [00:23<00:24,  5.32it/s]predicting train subjects:  50%|█████     | 134/266 [00:23<00:24,  5.33it/s]predicting train subjects:  51%|█████     | 135/266 [00:24<00:24,  5.27it/s]predicting train subjects:  51%|█████     | 136/266 [00:24<00:24,  5.25it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:24<00:23,  5.39it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:24<00:23,  5.52it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:24<00:23,  5.46it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:24<00:22,  5.55it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:25<00:22,  5.66it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:25<00:21,  5.71it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:25<00:21,  5.77it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:25<00:21,  5.81it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:25<00:20,  5.81it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:25<00:20,  5.75it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:26<00:20,  5.78it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:26<00:20,  5.81it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:26<00:20,  5.79it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:26<00:20,  5.80it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:26<00:19,  5.81it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:26<00:19,  5.74it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:27<00:19,  5.69it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:27<00:19,  5.76it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:27<00:18,  6.03it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:27<00:17,  6.34it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:27<00:16,  6.54it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:27<00:16,  6.66it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:28<00:15,  6.85it/s]predicting train subjects:  60%|██████    | 160/266 [00:28<00:15,  6.96it/s]predicting train subjects:  61%|██████    | 161/266 [00:28<00:14,  7.01it/s]predicting train subjects:  61%|██████    | 162/266 [00:28<00:14,  7.08it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:28<00:14,  7.12it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:28<00:14,  7.18it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:28<00:14,  7.20it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:29<00:13,  7.24it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:29<00:13,  7.25it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:29<00:13,  7.24it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:29<00:13,  7.23it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:29<00:13,  7.23it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:29<00:13,  7.15it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:29<00:13,  7.15it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:30<00:13,  6.87it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:30<00:13,  6.77it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:30<00:13,  6.59it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:30<00:13,  6.57it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:30<00:13,  6.55it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:30<00:13,  6.37it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:30<00:13,  6.41it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:31<00:13,  6.47it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:31<00:13,  6.50it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:31<00:12,  6.49it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:31<00:12,  6.50it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:31<00:12,  6.50it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:31<00:12,  6.38it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:32<00:12,  6.34it/s]predicting train subjects:  70%|███████   | 187/266 [00:32<00:12,  6.24it/s]predicting train subjects:  71%|███████   | 188/266 [00:32<00:12,  6.35it/s]predicting train subjects:  71%|███████   | 189/266 [00:32<00:11,  6.43it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:32<00:11,  6.54it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:32<00:11,  6.51it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:33<00:15,  4.93it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:33<00:13,  5.24it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:33<00:13,  5.24it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:33<00:12,  5.55it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:33<00:12,  5.74it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:33<00:11,  5.90it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:34<00:11,  5.96it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:34<00:10,  6.11it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:34<00:10,  6.22it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:34<00:10,  6.32it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:34<00:10,  6.35it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:34<00:09,  6.35it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:35<00:09,  6.38it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:35<00:09,  6.39it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:35<00:09,  6.39it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:35<00:09,  6.42it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:35<00:08,  6.50it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:35<00:08,  6.57it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:35<00:08,  6.64it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:36<00:08,  6.65it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:36<00:08,  6.69it/s]predicting train subjects:  80%|████████  | 213/266 [00:36<00:07,  6.64it/s]predicting train subjects:  80%|████████  | 214/266 [00:36<00:07,  6.66it/s]predicting train subjects:  81%|████████  | 215/266 [00:36<00:07,  6.69it/s]predicting train subjects:  81%|████████  | 216/266 [00:36<00:07,  6.70it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:37<00:07,  6.73it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:37<00:07,  6.73it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:37<00:06,  6.78it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:37<00:06,  6.85it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:37<00:06,  6.87it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:37<00:06,  6.94it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:37<00:06,  6.84it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:38<00:06,  6.89it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:38<00:05,  6.96it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:38<00:05,  7.02it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:38<00:05,  6.89it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:38<00:05,  6.77it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:38<00:05,  6.69it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:38<00:05,  6.79it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:39<00:05,  6.62it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:39<00:05,  6.45it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:39<00:05,  6.55it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:39<00:04,  6.57it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:39<00:04,  6.56it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:39<00:04,  6.64it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:39<00:04,  6.70it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:40<00:04,  6.63it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:40<00:04,  6.59it/s]predicting train subjects:  90%|█████████ | 240/266 [00:40<00:03,  6.60it/s]predicting train subjects:  91%|█████████ | 241/266 [00:40<00:03,  6.66it/s]predicting train subjects:  91%|█████████ | 242/266 [00:40<00:03,  6.72it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:40<00:03,  6.64it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:41<00:03,  6.54it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:41<00:03,  6.57it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:41<00:03,  6.59it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:41<00:02,  6.59it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:41<00:02,  6.59it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:41<00:02,  6.13it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:42<00:02,  5.98it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:42<00:02,  5.89it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:42<00:02,  5.85it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:42<00:02,  5.73it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:42<00:02,  5.70it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:42<00:01,  5.72it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:43<00:01,  5.73it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:43<00:01,  5.71it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:43<00:01,  5.76it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:43<00:01,  5.68it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:43<00:01,  5.62it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:43<00:00,  5.57it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:44<00:00,  5.55it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:44<00:00,  5.57it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:44<00:00,  5.64it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:44<00:00,  5.67it/s]predicting train subjects: 100%|██████████| 266/266 [00:44<00:00,  5.67it/s]predicting train subjects: 100%|██████████| 266/266 [00:44<00:00,  5.93it/s]
predicting test subjects sagittal:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects sagittal:  25%|██▌       | 1/4 [00:00<00:00,  5.91it/s]predicting test subjects sagittal:  50%|█████     | 2/4 [00:00<00:00,  6.11it/s]predicting test subjects sagittal:  75%|███████▌  | 3/4 [00:00<00:00,  6.27it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  6.28it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  6.36it/s]
predicting train subjects sagittal:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/266 [00:00<00:49,  5.36it/s]predicting train subjects sagittal:   1%|          | 2/266 [00:00<00:48,  5.42it/s]predicting train subjects sagittal:   1%|          | 3/266 [00:00<00:45,  5.75it/s]predicting train subjects sagittal:   2%|▏         | 4/266 [00:00<00:43,  5.96it/s]predicting train subjects sagittal:   2%|▏         | 5/266 [00:00<00:44,  5.83it/s]predicting train subjects sagittal:   2%|▏         | 6/266 [00:01<00:45,  5.73it/s]predicting train subjects sagittal:   3%|▎         | 7/266 [00:01<00:46,  5.57it/s]predicting train subjects sagittal:   3%|▎         | 8/266 [00:01<00:46,  5.53it/s]predicting train subjects sagittal:   3%|▎         | 9/266 [00:01<00:46,  5.50it/s]predicting train subjects sagittal:   4%|▍         | 10/266 [00:01<00:48,  5.32it/s]predicting train subjects sagittal:   4%|▍         | 11/266 [00:01<00:47,  5.37it/s]predicting train subjects sagittal:   5%|▍         | 12/266 [00:02<00:48,  5.22it/s]predicting train subjects sagittal:   5%|▍         | 13/266 [00:02<00:48,  5.19it/s]predicting train subjects sagittal:   5%|▌         | 14/266 [00:02<00:47,  5.28it/s]predicting train subjects sagittal:   6%|▌         | 15/266 [00:02<00:48,  5.20it/s]predicting train subjects sagittal:   6%|▌         | 16/266 [00:02<00:47,  5.24it/s]predicting train subjects sagittal:   6%|▋         | 17/266 [00:03<00:46,  5.30it/s]predicting train subjects sagittal:   7%|▋         | 18/266 [00:03<00:46,  5.36it/s]predicting train subjects sagittal:   7%|▋         | 19/266 [00:03<00:45,  5.39it/s]predicting train subjects sagittal:   8%|▊         | 20/266 [00:03<00:45,  5.46it/s]predicting train subjects sagittal:   8%|▊         | 21/266 [00:03<00:44,  5.51it/s]predicting train subjects sagittal:   8%|▊         | 22/266 [00:04<00:44,  5.50it/s]predicting train subjects sagittal:   9%|▊         | 23/266 [00:04<00:44,  5.41it/s]predicting train subjects sagittal:   9%|▉         | 24/266 [00:04<00:44,  5.48it/s]predicting train subjects sagittal:   9%|▉         | 25/266 [00:04<00:43,  5.54it/s]predicting train subjects sagittal:  10%|▉         | 26/266 [00:04<00:43,  5.46it/s]predicting train subjects sagittal:  10%|█         | 27/266 [00:04<00:43,  5.51it/s]predicting train subjects sagittal:  11%|█         | 28/266 [00:05<00:43,  5.53it/s]predicting train subjects sagittal:  11%|█         | 29/266 [00:05<00:42,  5.62it/s]predicting train subjects sagittal:  11%|█▏        | 30/266 [00:05<00:41,  5.65it/s]predicting train subjects sagittal:  12%|█▏        | 31/266 [00:05<00:41,  5.70it/s]predicting train subjects sagittal:  12%|█▏        | 32/266 [00:05<00:40,  5.73it/s]predicting train subjects sagittal:  12%|█▏        | 33/266 [00:05<00:40,  5.75it/s]predicting train subjects sagittal:  13%|█▎        | 34/266 [00:06<00:40,  5.72it/s]predicting train subjects sagittal:  13%|█▎        | 35/266 [00:06<00:40,  5.69it/s]predicting train subjects sagittal:  14%|█▎        | 36/266 [00:06<00:41,  5.57it/s]predicting train subjects sagittal:  14%|█▍        | 37/266 [00:06<00:40,  5.59it/s]predicting train subjects sagittal:  14%|█▍        | 38/266 [00:06<00:40,  5.60it/s]predicting train subjects sagittal:  15%|█▍        | 39/266 [00:07<00:40,  5.61it/s]predicting train subjects sagittal:  15%|█▌        | 40/266 [00:07<00:41,  5.43it/s]predicting train subjects sagittal:  15%|█▌        | 41/266 [00:07<00:41,  5.41it/s]predicting train subjects sagittal:  16%|█▌        | 42/266 [00:07<00:39,  5.69it/s]predicting train subjects sagittal:  16%|█▌        | 43/266 [00:07<00:37,  5.87it/s]predicting train subjects sagittal:  17%|█▋        | 44/266 [00:07<00:36,  6.07it/s]predicting train subjects sagittal:  17%|█▋        | 45/266 [00:08<00:35,  6.22it/s]predicting train subjects sagittal:  17%|█▋        | 46/266 [00:08<00:34,  6.30it/s]predicting train subjects sagittal:  18%|█▊        | 47/266 [00:08<00:34,  6.34it/s]predicting train subjects sagittal:  18%|█▊        | 48/266 [00:08<00:33,  6.45it/s]predicting train subjects sagittal:  18%|█▊        | 49/266 [00:08<00:33,  6.42it/s]predicting train subjects sagittal:  19%|█▉        | 50/266 [00:08<00:33,  6.50it/s]predicting train subjects sagittal:  19%|█▉        | 51/266 [00:08<00:33,  6.45it/s]predicting train subjects sagittal:  20%|█▉        | 52/266 [00:09<00:32,  6.53it/s]predicting train subjects sagittal:  20%|█▉        | 53/266 [00:09<00:32,  6.58it/s]predicting train subjects sagittal:  20%|██        | 54/266 [00:09<00:33,  6.40it/s]predicting train subjects sagittal:  21%|██        | 55/266 [00:09<00:32,  6.48it/s]predicting train subjects sagittal:  21%|██        | 56/266 [00:09<00:32,  6.56it/s]predicting train subjects sagittal:  21%|██▏       | 57/266 [00:09<00:31,  6.62it/s]predicting train subjects sagittal:  22%|██▏       | 58/266 [00:10<00:31,  6.63it/s]predicting train subjects sagittal:  22%|██▏       | 59/266 [00:10<00:31,  6.63it/s]predicting train subjects sagittal:  23%|██▎       | 60/266 [00:10<00:31,  6.63it/s]predicting train subjects sagittal:  23%|██▎       | 61/266 [00:10<00:30,  6.67it/s]predicting train subjects sagittal:  23%|██▎       | 62/266 [00:10<00:30,  6.68it/s]predicting train subjects sagittal:  24%|██▎       | 63/266 [00:10<00:30,  6.55it/s]predicting train subjects sagittal:  24%|██▍       | 64/266 [00:10<00:30,  6.55it/s]predicting train subjects sagittal:  24%|██▍       | 65/266 [00:11<00:30,  6.53it/s]predicting train subjects sagittal:  25%|██▍       | 66/266 [00:11<00:30,  6.62it/s]predicting train subjects sagittal:  25%|██▌       | 67/266 [00:11<00:29,  6.68it/s]predicting train subjects sagittal:  26%|██▌       | 68/266 [00:11<00:29,  6.73it/s]predicting train subjects sagittal:  26%|██▌       | 69/266 [00:11<00:29,  6.75it/s]predicting train subjects sagittal:  26%|██▋       | 70/266 [00:11<00:29,  6.75it/s]predicting train subjects sagittal:  27%|██▋       | 71/266 [00:11<00:28,  6.80it/s]predicting train subjects sagittal:  27%|██▋       | 72/266 [00:12<00:28,  6.84it/s]predicting train subjects sagittal:  27%|██▋       | 73/266 [00:12<00:28,  6.86it/s]predicting train subjects sagittal:  28%|██▊       | 74/266 [00:12<00:27,  6.87it/s]predicting train subjects sagittal:  28%|██▊       | 75/266 [00:12<00:27,  6.89it/s]predicting train subjects sagittal:  29%|██▊       | 76/266 [00:12<00:27,  6.88it/s]predicting train subjects sagittal:  29%|██▉       | 77/266 [00:12<00:27,  6.79it/s]predicting train subjects sagittal:  29%|██▉       | 78/266 [00:13<00:29,  6.32it/s]predicting train subjects sagittal:  30%|██▉       | 79/266 [00:13<00:30,  6.05it/s]predicting train subjects sagittal:  30%|███       | 80/266 [00:13<00:31,  5.92it/s]predicting train subjects sagittal:  30%|███       | 81/266 [00:13<00:31,  5.79it/s]predicting train subjects sagittal:  31%|███       | 82/266 [00:13<00:32,  5.71it/s]predicting train subjects sagittal:  31%|███       | 83/266 [00:13<00:31,  5.74it/s]predicting train subjects sagittal:  32%|███▏      | 84/266 [00:14<00:32,  5.68it/s]predicting train subjects sagittal:  32%|███▏      | 85/266 [00:14<00:31,  5.67it/s]predicting train subjects sagittal:  32%|███▏      | 86/266 [00:14<00:31,  5.69it/s]predicting train subjects sagittal:  33%|███▎      | 87/266 [00:14<00:31,  5.71it/s]predicting train subjects sagittal:  33%|███▎      | 88/266 [00:14<00:31,  5.71it/s]predicting train subjects sagittal:  33%|███▎      | 89/266 [00:14<00:30,  5.75it/s]predicting train subjects sagittal:  34%|███▍      | 90/266 [00:15<00:30,  5.75it/s]predicting train subjects sagittal:  34%|███▍      | 91/266 [00:15<00:30,  5.72it/s]predicting train subjects sagittal:  35%|███▍      | 92/266 [00:15<00:30,  5.70it/s]predicting train subjects sagittal:  35%|███▍      | 93/266 [00:15<00:30,  5.67it/s]predicting train subjects sagittal:  35%|███▌      | 94/266 [00:15<00:30,  5.65it/s]predicting train subjects sagittal:  36%|███▌      | 95/266 [00:16<00:30,  5.65it/s]predicting train subjects sagittal:  36%|███▌      | 96/266 [00:16<00:28,  5.87it/s]predicting train subjects sagittal:  36%|███▋      | 97/266 [00:16<00:29,  5.71it/s]predicting train subjects sagittal:  37%|███▋      | 98/266 [00:16<00:28,  5.80it/s]predicting train subjects sagittal:  37%|███▋      | 99/266 [00:16<00:27,  6.08it/s]predicting train subjects sagittal:  38%|███▊      | 100/266 [00:16<00:26,  6.15it/s]predicting train subjects sagittal:  38%|███▊      | 101/266 [00:17<00:26,  6.12it/s]predicting train subjects sagittal:  38%|███▊      | 102/266 [00:17<00:27,  5.99it/s]predicting train subjects sagittal:  39%|███▊      | 103/266 [00:17<00:27,  5.96it/s]predicting train subjects sagittal:  39%|███▉      | 104/266 [00:17<00:26,  6.06it/s]predicting train subjects sagittal:  39%|███▉      | 105/266 [00:17<00:26,  6.15it/s]predicting train subjects sagittal:  40%|███▉      | 106/266 [00:17<00:25,  6.19it/s]predicting train subjects sagittal:  40%|████      | 107/266 [00:18<00:25,  6.23it/s]predicting train subjects sagittal:  41%|████      | 108/266 [00:18<00:25,  6.26it/s]predicting train subjects sagittal:  41%|████      | 109/266 [00:18<00:24,  6.29it/s]predicting train subjects sagittal:  41%|████▏     | 110/266 [00:18<00:25,  6.16it/s]predicting train subjects sagittal:  42%|████▏     | 111/266 [00:18<00:25,  6.10it/s]predicting train subjects sagittal:  42%|████▏     | 112/266 [00:18<00:24,  6.21it/s]predicting train subjects sagittal:  42%|████▏     | 113/266 [00:18<00:25,  6.07it/s]predicting train subjects sagittal:  43%|████▎     | 114/266 [00:19<00:25,  6.06it/s]predicting train subjects sagittal:  43%|████▎     | 115/266 [00:19<00:25,  6.03it/s]predicting train subjects sagittal:  44%|████▎     | 116/266 [00:19<00:25,  5.77it/s]predicting train subjects sagittal:  44%|████▍     | 117/266 [00:19<00:24,  5.99it/s]predicting train subjects sagittal:  44%|████▍     | 118/266 [00:19<00:24,  6.03it/s]predicting train subjects sagittal:  45%|████▍     | 119/266 [00:20<00:25,  5.70it/s]predicting train subjects sagittal:  45%|████▌     | 120/266 [00:20<00:25,  5.63it/s]predicting train subjects sagittal:  45%|████▌     | 121/266 [00:20<00:26,  5.57it/s]predicting train subjects sagittal:  46%|████▌     | 122/266 [00:20<00:25,  5.54it/s]predicting train subjects sagittal:  46%|████▌     | 123/266 [00:20<00:26,  5.47it/s]predicting train subjects sagittal:  47%|████▋     | 124/266 [00:20<00:26,  5.46it/s]predicting train subjects sagittal:  47%|████▋     | 125/266 [00:21<00:25,  5.44it/s]predicting train subjects sagittal:  47%|████▋     | 126/266 [00:21<00:25,  5.43it/s]predicting train subjects sagittal:  48%|████▊     | 127/266 [00:21<00:25,  5.43it/s]predicting train subjects sagittal:  48%|████▊     | 128/266 [00:21<00:25,  5.43it/s]predicting train subjects sagittal:  48%|████▊     | 129/266 [00:21<00:25,  5.34it/s]predicting train subjects sagittal:  49%|████▉     | 130/266 [00:22<00:25,  5.41it/s]predicting train subjects sagittal:  49%|████▉     | 131/266 [00:22<00:24,  5.41it/s]predicting train subjects sagittal:  50%|████▉     | 132/266 [00:22<00:24,  5.45it/s]predicting train subjects sagittal:  50%|█████     | 133/266 [00:22<00:24,  5.49it/s]predicting train subjects sagittal:  50%|█████     | 134/266 [00:22<00:23,  5.50it/s]predicting train subjects sagittal:  51%|█████     | 135/266 [00:22<00:23,  5.56it/s]predicting train subjects sagittal:  51%|█████     | 136/266 [00:23<00:23,  5.57it/s]predicting train subjects sagittal:  52%|█████▏    | 137/266 [00:23<00:22,  5.68it/s]predicting train subjects sagittal:  52%|█████▏    | 138/266 [00:23<00:22,  5.67it/s]predicting train subjects sagittal:  52%|█████▏    | 139/266 [00:23<00:22,  5.62it/s]predicting train subjects sagittal:  53%|█████▎    | 140/266 [00:23<00:22,  5.71it/s]predicting train subjects sagittal:  53%|█████▎    | 141/266 [00:24<00:22,  5.68it/s]predicting train subjects sagittal:  53%|█████▎    | 142/266 [00:24<00:21,  5.71it/s]predicting train subjects sagittal:  54%|█████▍    | 143/266 [00:24<00:21,  5.67it/s]predicting train subjects sagittal:  54%|█████▍    | 144/266 [00:24<00:21,  5.72it/s]predicting train subjects sagittal:  55%|█████▍    | 145/266 [00:24<00:20,  5.79it/s]predicting train subjects sagittal:  55%|█████▍    | 146/266 [00:24<00:20,  5.84it/s]predicting train subjects sagittal:  55%|█████▌    | 147/266 [00:25<00:20,  5.79it/s]predicting train subjects sagittal:  56%|█████▌    | 148/266 [00:25<00:20,  5.85it/s]predicting train subjects sagittal:  56%|█████▌    | 149/266 [00:25<00:20,  5.85it/s]predicting train subjects sagittal:  56%|█████▋    | 150/266 [00:25<00:20,  5.77it/s]predicting train subjects sagittal:  57%|█████▋    | 151/266 [00:25<00:19,  5.87it/s]predicting train subjects sagittal:  57%|█████▋    | 152/266 [00:25<00:19,  5.97it/s]predicting train subjects sagittal:  58%|█████▊    | 153/266 [00:26<00:18,  6.02it/s]predicting train subjects sagittal:  58%|█████▊    | 154/266 [00:26<00:18,  5.99it/s]predicting train subjects sagittal:  58%|█████▊    | 155/266 [00:26<00:17,  6.39it/s]predicting train subjects sagittal:  59%|█████▊    | 156/266 [00:26<00:16,  6.70it/s]predicting train subjects sagittal:  59%|█████▉    | 157/266 [00:26<00:15,  6.94it/s]predicting train subjects sagittal:  59%|█████▉    | 158/266 [00:26<00:15,  7.15it/s]predicting train subjects sagittal:  60%|█████▉    | 159/266 [00:26<00:14,  7.29it/s]predicting train subjects sagittal:  60%|██████    | 160/266 [00:27<00:14,  7.26it/s]predicting train subjects sagittal:  61%|██████    | 161/266 [00:27<00:14,  7.24it/s]predicting train subjects sagittal:  61%|██████    | 162/266 [00:27<00:14,  7.21it/s]predicting train subjects sagittal:  61%|██████▏   | 163/266 [00:27<00:14,  7.10it/s]predicting train subjects sagittal:  62%|██████▏   | 164/266 [00:27<00:14,  7.25it/s]predicting train subjects sagittal:  62%|██████▏   | 165/266 [00:27<00:13,  7.36it/s]predicting train subjects sagittal:  62%|██████▏   | 166/266 [00:27<00:13,  7.31it/s]predicting train subjects sagittal:  63%|██████▎   | 167/266 [00:27<00:13,  7.33it/s]predicting train subjects sagittal:  63%|██████▎   | 168/266 [00:28<00:13,  7.33it/s]predicting train subjects sagittal:  64%|██████▎   | 169/266 [00:28<00:13,  7.29it/s]predicting train subjects sagittal:  64%|██████▍   | 170/266 [00:28<00:13,  7.34it/s]predicting train subjects sagittal:  64%|██████▍   | 171/266 [00:28<00:12,  7.35it/s]predicting train subjects sagittal:  65%|██████▍   | 172/266 [00:28<00:12,  7.35it/s]predicting train subjects sagittal:  65%|██████▌   | 173/266 [00:28<00:13,  7.12it/s]predicting train subjects sagittal:  65%|██████▌   | 174/266 [00:28<00:13,  6.81it/s]predicting train subjects sagittal:  66%|██████▌   | 175/266 [00:29<00:13,  6.73it/s]predicting train subjects sagittal:  66%|██████▌   | 176/266 [00:29<00:13,  6.60it/s]predicting train subjects sagittal:  67%|██████▋   | 177/266 [00:29<00:13,  6.61it/s]predicting train subjects sagittal:  67%|██████▋   | 178/266 [00:29<00:13,  6.61it/s]predicting train subjects sagittal:  67%|██████▋   | 179/266 [00:29<00:13,  6.47it/s]predicting train subjects sagittal:  68%|██████▊   | 180/266 [00:29<00:13,  6.34it/s]predicting train subjects sagittal:  68%|██████▊   | 181/266 [00:30<00:13,  6.31it/s]predicting train subjects sagittal:  68%|██████▊   | 182/266 [00:30<00:13,  6.45it/s]predicting train subjects sagittal:  69%|██████▉   | 183/266 [00:30<00:12,  6.41it/s]predicting train subjects sagittal:  69%|██████▉   | 184/266 [00:30<00:12,  6.44it/s]predicting train subjects sagittal:  70%|██████▉   | 185/266 [00:30<00:12,  6.52it/s]predicting train subjects sagittal:  70%|██████▉   | 186/266 [00:30<00:12,  6.55it/s]predicting train subjects sagittal:  70%|███████   | 187/266 [00:30<00:12,  6.46it/s]predicting train subjects sagittal:  71%|███████   | 188/266 [00:31<00:12,  6.41it/s]predicting train subjects sagittal:  71%|███████   | 189/266 [00:31<00:11,  6.46it/s]predicting train subjects sagittal:  71%|███████▏  | 190/266 [00:31<00:11,  6.46it/s]predicting train subjects sagittal:  72%|███████▏  | 191/266 [00:31<00:11,  6.40it/s]predicting train subjects sagittal:  72%|███████▏  | 192/266 [00:31<00:11,  6.43it/s]predicting train subjects sagittal:  73%|███████▎  | 193/266 [00:31<00:11,  6.40it/s]predicting train subjects sagittal:  73%|███████▎  | 194/266 [00:32<00:11,  6.14it/s]predicting train subjects sagittal:  73%|███████▎  | 195/266 [00:32<00:11,  6.22it/s]predicting train subjects sagittal:  74%|███████▎  | 196/266 [00:32<00:11,  6.23it/s]predicting train subjects sagittal:  74%|███████▍  | 197/266 [00:32<00:10,  6.32it/s]predicting train subjects sagittal:  74%|███████▍  | 198/266 [00:32<00:10,  6.40it/s]predicting train subjects sagittal:  75%|███████▍  | 199/266 [00:32<00:10,  6.49it/s]predicting train subjects sagittal:  75%|███████▌  | 200/266 [00:33<00:10,  6.55it/s]predicting train subjects sagittal:  76%|███████▌  | 201/266 [00:33<00:10,  6.48it/s]predicting train subjects sagittal:  76%|███████▌  | 202/266 [00:33<00:10,  6.35it/s]predicting train subjects sagittal:  76%|███████▋  | 203/266 [00:33<00:10,  6.12it/s]predicting train subjects sagittal:  77%|███████▋  | 204/266 [00:33<00:10,  6.07it/s]predicting train subjects sagittal:  77%|███████▋  | 205/266 [00:33<00:09,  6.17it/s]predicting train subjects sagittal:  77%|███████▋  | 206/266 [00:34<00:09,  6.27it/s]predicting train subjects sagittal:  78%|███████▊  | 207/266 [00:34<00:09,  6.34it/s]predicting train subjects sagittal:  78%|███████▊  | 208/266 [00:34<00:09,  6.40it/s]predicting train subjects sagittal:  79%|███████▊  | 209/266 [00:34<00:08,  6.42it/s]predicting train subjects sagittal:  79%|███████▉  | 210/266 [00:34<00:09,  6.18it/s]predicting train subjects sagittal:  79%|███████▉  | 211/266 [00:34<00:09,  6.09it/s]predicting train subjects sagittal:  80%|███████▉  | 212/266 [00:34<00:08,  6.19it/s]predicting train subjects sagittal:  80%|████████  | 213/266 [00:35<00:08,  6.17it/s]predicting train subjects sagittal:  80%|████████  | 214/266 [00:35<00:08,  6.24it/s]predicting train subjects sagittal:  81%|████████  | 215/266 [00:35<00:08,  6.35it/s]predicting train subjects sagittal:  81%|████████  | 216/266 [00:35<00:07,  6.36it/s]predicting train subjects sagittal:  82%|████████▏ | 217/266 [00:35<00:07,  6.51it/s]predicting train subjects sagittal:  82%|████████▏ | 218/266 [00:35<00:07,  6.59it/s]predicting train subjects sagittal:  82%|████████▏ | 219/266 [00:36<00:07,  6.66it/s]predicting train subjects sagittal:  83%|████████▎ | 220/266 [00:36<00:06,  6.65it/s]predicting train subjects sagittal:  83%|████████▎ | 221/266 [00:36<00:06,  6.49it/s]predicting train subjects sagittal:  83%|████████▎ | 222/266 [00:36<00:06,  6.57it/s]predicting train subjects sagittal:  84%|████████▍ | 223/266 [00:36<00:06,  6.64it/s]predicting train subjects sagittal:  84%|████████▍ | 224/266 [00:36<00:06,  6.65it/s]predicting train subjects sagittal:  85%|████████▍ | 225/266 [00:36<00:06,  6.47it/s]predicting train subjects sagittal:  85%|████████▍ | 226/266 [00:37<00:06,  6.57it/s]predicting train subjects sagittal:  85%|████████▌ | 227/266 [00:37<00:05,  6.64it/s]predicting train subjects sagittal:  86%|████████▌ | 228/266 [00:37<00:05,  6.69it/s]predicting train subjects sagittal:  86%|████████▌ | 229/266 [00:37<00:05,  6.73it/s]predicting train subjects sagittal:  86%|████████▋ | 230/266 [00:37<00:05,  6.75it/s]predicting train subjects sagittal:  87%|████████▋ | 231/266 [00:37<00:05,  6.72it/s]predicting train subjects sagittal:  87%|████████▋ | 232/266 [00:37<00:05,  6.73it/s]predicting train subjects sagittal:  88%|████████▊ | 233/266 [00:38<00:04,  6.77it/s]predicting train subjects sagittal:  88%|████████▊ | 234/266 [00:38<00:04,  6.82it/s]predicting train subjects sagittal:  88%|████████▊ | 235/266 [00:38<00:04,  6.77it/s]predicting train subjects sagittal:  89%|████████▊ | 236/266 [00:38<00:04,  6.67it/s]predicting train subjects sagittal:  89%|████████▉ | 237/266 [00:38<00:04,  6.67it/s]predicting train subjects sagittal:  89%|████████▉ | 238/266 [00:38<00:04,  6.71it/s]predicting train subjects sagittal:  90%|████████▉ | 239/266 [00:39<00:04,  6.73it/s]predicting train subjects sagittal:  90%|█████████ | 240/266 [00:39<00:03,  6.74it/s]predicting train subjects sagittal:  91%|█████████ | 241/266 [00:39<00:03,  6.75it/s]predicting train subjects sagittal:  91%|█████████ | 242/266 [00:39<00:03,  6.75it/s]predicting train subjects sagittal:  91%|█████████▏| 243/266 [00:39<00:03,  6.66it/s]predicting train subjects sagittal:  92%|█████████▏| 244/266 [00:39<00:03,  6.60it/s]predicting train subjects sagittal:  92%|█████████▏| 245/266 [00:39<00:03,  6.58it/s]predicting train subjects sagittal:  92%|█████████▏| 246/266 [00:40<00:03,  6.59it/s]predicting train subjects sagittal:  93%|█████████▎| 247/266 [00:40<00:02,  6.47it/s]predicting train subjects sagittal:  93%|█████████▎| 248/266 [00:40<00:02,  6.57it/s]predicting train subjects sagittal:  94%|█████████▎| 249/266 [00:40<00:02,  6.33it/s]predicting train subjects sagittal:  94%|█████████▍| 250/266 [00:40<00:02,  5.98it/s]predicting train subjects sagittal:  94%|█████████▍| 251/266 [00:40<00:02,  5.83it/s]predicting train subjects sagittal:  95%|█████████▍| 252/266 [00:41<00:02,  5.82it/s]predicting train subjects sagittal:  95%|█████████▌| 253/266 [00:41<00:02,  5.76it/s]predicting train subjects sagittal:  95%|█████████▌| 254/266 [00:41<00:02,  5.75it/s]predicting train subjects sagittal:  96%|█████████▌| 255/266 [00:41<00:01,  5.79it/s]predicting train subjects sagittal:  96%|█████████▌| 256/266 [00:41<00:01,  5.77it/s]predicting train subjects sagittal:  97%|█████████▋| 257/266 [00:41<00:01,  5.56it/s]predicting train subjects sagittal:  97%|█████████▋| 258/266 [00:42<00:01,  5.52it/s]predicting train subjects sagittal:  97%|█████████▋| 259/266 [00:42<00:01,  5.51it/s]predicting train subjects sagittal:  98%|█████████▊| 260/266 [00:42<00:01,  5.64it/s]predicting train subjects sagittal:  98%|█████████▊| 261/266 [00:42<00:00,  5.65it/s]predicting train subjects sagittal:  98%|█████████▊| 262/266 [00:42<00:00,  5.74it/s]predicting train subjects sagittal:  99%|█████████▉| 263/266 [00:43<00:00,  5.77it/s]predicting train subjects sagittal:  99%|█████████▉| 264/266 [00:43<00:00,  5.78it/s]predicting train subjects sagittal: 100%|█████████▉| 265/266 [00:43<00:00,  5.82it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:43<00:00,  5.84it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:43<00:00,  6.11it/s]
saving BB  test1-THALAMUS:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 4/4 [00:00<00:00, 73.85it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 8/266 [00:00<00:03, 74.32it/s]saving BB  train1-THALAMUS:   6%|▌         | 15/266 [00:00<00:03, 72.13it/s]saving BB  train1-THALAMUS:   8%|▊         | 22/266 [00:00<00:03, 71.44it/s]saving BB  train1-THALAMUS:  11%|█▏        | 30/266 [00:00<00:03, 73.02it/s]saving BB  train1-THALAMUS:  14%|█▍        | 38/266 [00:00<00:03, 74.56it/s]saving BB  train1-THALAMUS:  18%|█▊        | 47/266 [00:00<00:02, 76.14it/s]saving BB  train1-THALAMUS:  21%|██        | 56/266 [00:00<00:02, 77.49it/s]saving BB  train1-THALAMUS:  24%|██▍       | 65/266 [00:00<00:02, 79.11it/s]saving BB  train1-THALAMUS:  28%|██▊       | 74/266 [00:00<00:02, 80.24it/s]saving BB  train1-THALAMUS:  31%|███       | 83/266 [00:01<00:02, 80.46it/s]saving BB  train1-THALAMUS:  34%|███▍      | 91/266 [00:01<00:02, 79.39it/s]saving BB  train1-THALAMUS:  37%|███▋      | 99/266 [00:01<00:02, 79.11it/s]saving BB  train1-THALAMUS:  41%|████      | 108/266 [00:01<00:01, 80.07it/s]saving BB  train1-THALAMUS:  44%|████▎     | 116/266 [00:01<00:01, 78.66it/s]saving BB  train1-THALAMUS:  47%|████▋     | 124/266 [00:01<00:01, 77.87it/s]saving BB  train1-THALAMUS:  50%|████▉     | 132/266 [00:01<00:01, 77.70it/s]saving BB  train1-THALAMUS:  53%|█████▎    | 140/266 [00:01<00:01, 77.11it/s]saving BB  train1-THALAMUS:  56%|█████▌    | 148/266 [00:01<00:01, 75.05it/s]saving BB  train1-THALAMUS:  59%|█████▊    | 156/266 [00:02<00:01, 75.54it/s]saving BB  train1-THALAMUS:  62%|██████▏   | 165/266 [00:02<00:01, 77.64it/s]saving BB  train1-THALAMUS:  65%|██████▌   | 174/266 [00:02<00:01, 80.43it/s]saving BB  train1-THALAMUS:  69%|██████▉   | 183/266 [00:02<00:01, 81.28it/s]saving BB  train1-THALAMUS:  72%|███████▏  | 192/266 [00:02<00:00, 81.22it/s]saving BB  train1-THALAMUS:  76%|███████▌  | 201/266 [00:02<00:00, 81.05it/s]saving BB  train1-THALAMUS:  79%|███████▉  | 210/266 [00:02<00:00, 81.93it/s]saving BB  train1-THALAMUS:  82%|████████▏ | 219/266 [00:02<00:00, 82.27it/s]saving BB  train1-THALAMUS:  86%|████████▌ | 228/266 [00:02<00:00, 82.80it/s]saving BB  train1-THALAMUS:  89%|████████▉ | 237/266 [00:02<00:00, 83.08it/s]saving BB  train1-THALAMUS:  92%|█████████▏| 246/266 [00:03<00:00, 83.81it/s]saving BB  train1-THALAMUS:  96%|█████████▌| 255/266 [00:03<00:00, 81.08it/s]saving BB  train1-THALAMUS:  99%|█████████▉| 264/266 [00:03<00:00, 78.58it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 79.01it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 4/4 [00:00<00:00, 77.91it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   3%|▎         | 8/266 [00:00<00:03, 72.48it/s]saving BB  train1-THALAMUS Sagittal:   5%|▌         | 14/266 [00:00<00:03, 66.64it/s]saving BB  train1-THALAMUS Sagittal:   8%|▊         | 21/266 [00:00<00:03, 67.37it/s]saving BB  train1-THALAMUS Sagittal:  11%|█         | 29/266 [00:00<00:03, 70.27it/s]saving BB  train1-THALAMUS Sagittal:  14%|█▎        | 36/266 [00:00<00:03, 67.58it/s]saving BB  train1-THALAMUS Sagittal:  17%|█▋        | 44/266 [00:00<00:03, 70.02it/s]saving BB  train1-THALAMUS Sagittal:  20%|█▉        | 53/266 [00:00<00:02, 74.15it/s]saving BB  train1-THALAMUS Sagittal:  23%|██▎       | 62/266 [00:00<00:02, 76.90it/s]saving BB  train1-THALAMUS Sagittal:  27%|██▋       | 72/266 [00:00<00:02, 80.53it/s]saving BB  train1-THALAMUS Sagittal:  30%|███       | 81/266 [00:01<00:02, 81.64it/s]saving BB  train1-THALAMUS Sagittal:  34%|███▍      | 90/266 [00:01<00:02, 80.45it/s]saving BB  train1-THALAMUS Sagittal:  37%|███▋      | 98/266 [00:01<00:02, 78.86it/s]saving BB  train1-THALAMUS Sagittal:  40%|████      | 107/266 [00:01<00:01, 79.65it/s]saving BB  train1-THALAMUS Sagittal:  43%|████▎     | 115/266 [00:01<00:01, 78.23it/s]saving BB  train1-THALAMUS Sagittal:  46%|████▌     | 123/266 [00:01<00:01, 75.77it/s]saving BB  train1-THALAMUS Sagittal:  49%|████▉     | 131/266 [00:01<00:01, 75.48it/s]saving BB  train1-THALAMUS Sagittal:  52%|█████▏    | 139/266 [00:01<00:01, 76.06it/s]saving BB  train1-THALAMUS Sagittal:  55%|█████▌    | 147/266 [00:01<00:01, 76.00it/s]saving BB  train1-THALAMUS Sagittal:  58%|█████▊    | 155/266 [00:02<00:01, 76.40it/s]saving BB  train1-THALAMUS Sagittal:  62%|██████▏   | 164/266 [00:02<00:01, 79.19it/s]saving BB  train1-THALAMUS Sagittal:  65%|██████▌   | 173/266 [00:02<00:01, 81.78it/s]saving BB  train1-THALAMUS Sagittal:  69%|██████▉   | 183/266 [00:02<00:00, 84.19it/s]saving BB  train1-THALAMUS Sagittal:  72%|███████▏  | 192/266 [00:02<00:00, 84.38it/s]saving BB  train1-THALAMUS Sagittal:  76%|███████▌  | 201/266 [00:02<00:00, 81.40it/s]saving BB  train1-THALAMUS Sagittal:  79%|███████▉  | 210/266 [00:02<00:00, 79.53it/s]saving BB  train1-THALAMUS Sagittal:  82%|████████▏ | 219/266 [00:02<00:00, 80.76it/s]saving BB  train1-THALAMUS Sagittal:  86%|████████▌ | 228/266 [00:02<00:00, 81.30it/s]saving BB  train1-THALAMUS Sagittal:  89%|████████▉ | 237/266 [00:03<00:00, 81.29it/s]saving BB  train1-THALAMUS Sagittal:  92%|█████████▏| 246/266 [00:03<00:00, 80.45it/s]saving BB  train1-THALAMUS Sagittal:  96%|█████████▌| 255/266 [00:03<00:00, 78.11it/s]saving BB  train1-THALAMUS Sagittal:  99%|█████████▉| 263/266 [00:03<00:00, 77.66it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 77.63it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<05:16,  1.19s/it]Loading train:   1%|          | 2/266 [00:02<04:55,  1.12s/it]Loading train:   1%|          | 3/266 [00:02<04:30,  1.03s/it]Loading train:   2%|▏         | 4/266 [00:03<04:10,  1.05it/s]Loading train:   2%|▏         | 5/266 [00:04<04:09,  1.05it/s]Loading train:   2%|▏         | 6/266 [00:05<03:46,  1.15it/s]Loading train:   3%|▎         | 7/266 [00:06<03:27,  1.25it/s]Loading train:   3%|▎         | 8/266 [00:06<03:14,  1.33it/s]Loading train:   3%|▎         | 9/266 [00:07<03:03,  1.40it/s]Loading train:   4%|▍         | 10/266 [00:07<02:54,  1.47it/s]Loading train:   4%|▍         | 11/266 [00:08<02:49,  1.50it/s]Loading train:   5%|▍         | 12/266 [00:09<02:46,  1.52it/s]Loading train:   5%|▍         | 13/266 [00:09<02:43,  1.54it/s]Loading train:   5%|▌         | 14/266 [00:10<02:41,  1.56it/s]Loading train:   6%|▌         | 15/266 [00:11<02:40,  1.56it/s]Loading train:   6%|▌         | 16/266 [00:11<02:39,  1.57it/s]Loading train:   6%|▋         | 17/266 [00:12<02:38,  1.57it/s]Loading train:   7%|▋         | 18/266 [00:12<02:41,  1.54it/s]Loading train:   7%|▋         | 19/266 [00:13<02:39,  1.55it/s]Loading train:   8%|▊         | 20/266 [00:14<02:35,  1.59it/s]Loading train:   8%|▊         | 21/266 [00:14<02:32,  1.60it/s]Loading train:   8%|▊         | 22/266 [00:15<02:30,  1.62it/s]Loading train:   9%|▊         | 23/266 [00:16<02:32,  1.59it/s]Loading train:   9%|▉         | 24/266 [00:16<02:31,  1.60it/s]Loading train:   9%|▉         | 25/266 [00:17<02:33,  1.57it/s]Loading train:  10%|▉         | 26/266 [00:17<02:31,  1.58it/s]Loading train:  10%|█         | 27/266 [00:18<02:29,  1.60it/s]Loading train:  11%|█         | 28/266 [00:19<02:26,  1.63it/s]Loading train:  11%|█         | 29/266 [00:19<02:21,  1.68it/s]Loading train:  11%|█▏        | 30/266 [00:20<02:20,  1.69it/s]Loading train:  12%|█▏        | 31/266 [00:20<02:20,  1.68it/s]Loading train:  12%|█▏        | 32/266 [00:21<02:22,  1.64it/s]Loading train:  12%|█▏        | 33/266 [00:22<02:24,  1.61it/s]Loading train:  13%|█▎        | 34/266 [00:22<02:20,  1.65it/s]Loading train:  13%|█▎        | 35/266 [00:23<02:17,  1.68it/s]Loading train:  14%|█▎        | 36/266 [00:23<02:15,  1.70it/s]Loading train:  14%|█▍        | 37/266 [00:24<02:17,  1.66it/s]Loading train:  14%|█▍        | 38/266 [00:25<02:18,  1.65it/s]Loading train:  15%|█▍        | 39/266 [00:25<02:23,  1.59it/s]Loading train:  15%|█▌        | 40/266 [00:26<02:20,  1.61it/s]Loading train:  15%|█▌        | 41/266 [00:27<02:17,  1.64it/s]Loading train:  16%|█▌        | 42/266 [00:27<02:13,  1.68it/s]Loading train:  16%|█▌        | 43/266 [00:28<02:08,  1.74it/s]Loading train:  17%|█▋        | 44/266 [00:28<02:05,  1.77it/s]Loading train:  17%|█▋        | 45/266 [00:29<02:07,  1.73it/s]Loading train:  17%|█▋        | 46/266 [00:29<02:08,  1.71it/s]Loading train:  18%|█▊        | 47/266 [00:30<02:08,  1.71it/s]Loading train:  18%|█▊        | 48/266 [00:31<02:05,  1.73it/s]Loading train:  18%|█▊        | 49/266 [00:31<02:02,  1.77it/s]Loading train:  19%|█▉        | 50/266 [00:32<02:02,  1.76it/s]Loading train:  19%|█▉        | 51/266 [00:32<02:03,  1.74it/s]Loading train:  20%|█▉        | 52/266 [00:33<02:03,  1.73it/s]Loading train:  20%|█▉        | 53/266 [00:33<02:03,  1.72it/s]Loading train:  20%|██        | 54/266 [00:34<02:02,  1.73it/s]Loading train:  21%|██        | 55/266 [00:35<02:00,  1.75it/s]Loading train:  21%|██        | 56/266 [00:35<02:02,  1.72it/s]Loading train:  21%|██▏       | 57/266 [00:36<02:03,  1.69it/s]Loading train:  22%|██▏       | 58/266 [00:36<02:06,  1.65it/s]Loading train:  22%|██▏       | 59/266 [00:37<02:04,  1.67it/s]Loading train:  23%|██▎       | 60/266 [00:38<01:59,  1.73it/s]Loading train:  23%|██▎       | 61/266 [00:38<01:56,  1.76it/s]Loading train:  23%|██▎       | 62/266 [00:39<01:54,  1.78it/s]Loading train:  24%|██▎       | 63/266 [00:39<01:55,  1.76it/s]Loading train:  24%|██▍       | 64/266 [00:40<01:52,  1.79it/s]Loading train:  24%|██▍       | 65/266 [00:40<01:48,  1.85it/s]Loading train:  25%|██▍       | 66/266 [00:41<01:45,  1.89it/s]Loading train:  25%|██▌       | 67/266 [00:41<01:44,  1.90it/s]Loading train:  26%|██▌       | 68/266 [00:42<01:48,  1.82it/s]Loading train:  26%|██▌       | 69/266 [00:42<01:48,  1.82it/s]Loading train:  26%|██▋       | 70/266 [00:43<01:48,  1.81it/s]Loading train:  27%|██▋       | 71/266 [00:43<01:46,  1.83it/s]Loading train:  27%|██▋       | 72/266 [00:44<01:43,  1.88it/s]Loading train:  27%|██▋       | 73/266 [00:44<01:41,  1.91it/s]Loading train:  28%|██▊       | 74/266 [00:45<01:42,  1.87it/s]Loading train:  28%|██▊       | 75/266 [00:46<01:43,  1.84it/s]Loading train:  29%|██▊       | 76/266 [00:46<01:43,  1.83it/s]Loading train:  29%|██▉       | 77/266 [00:47<01:43,  1.83it/s]Loading train:  29%|██▉       | 78/266 [00:47<01:50,  1.70it/s]Loading train:  30%|██▉       | 79/266 [00:48<01:50,  1.70it/s]Loading train:  30%|███       | 80/266 [00:49<01:48,  1.72it/s]Loading train:  30%|███       | 81/266 [00:49<01:46,  1.74it/s]Loading train:  31%|███       | 82/266 [00:50<01:46,  1.73it/s]Loading train:  31%|███       | 83/266 [00:50<01:46,  1.71it/s]Loading train:  32%|███▏      | 84/266 [00:51<01:50,  1.65it/s]Loading train:  32%|███▏      | 85/266 [00:52<01:52,  1.61it/s]Loading train:  32%|███▏      | 86/266 [00:52<01:50,  1.63it/s]Loading train:  33%|███▎      | 87/266 [00:53<01:48,  1.65it/s]Loading train:  33%|███▎      | 88/266 [00:53<01:48,  1.63it/s]Loading train:  33%|███▎      | 89/266 [00:54<01:48,  1.63it/s]Loading train:  34%|███▍      | 90/266 [00:55<01:49,  1.61it/s]Loading train:  34%|███▍      | 91/266 [00:55<01:47,  1.63it/s]Loading train:  35%|███▍      | 92/266 [00:56<01:45,  1.66it/s]Loading train:  35%|███▍      | 93/266 [00:56<01:44,  1.66it/s]Loading train:  35%|███▌      | 94/266 [00:57<01:45,  1.63it/s]Loading train:  36%|███▌      | 95/266 [00:58<01:45,  1.62it/s]Loading train:  36%|███▌      | 96/266 [00:59<01:57,  1.45it/s]Loading train:  36%|███▋      | 97/266 [01:00<02:11,  1.29it/s]Loading train:  37%|███▋      | 98/266 [01:00<02:11,  1.27it/s]Loading train:  37%|███▋      | 99/266 [01:01<02:06,  1.32it/s]Loading train:  38%|███▊      | 100/266 [01:02<02:09,  1.28it/s]Loading train:  38%|███▊      | 101/266 [01:02<01:58,  1.39it/s]Loading train:  38%|███▊      | 102/266 [01:03<01:48,  1.52it/s]Loading train:  39%|███▊      | 103/266 [01:04<01:41,  1.61it/s]Loading train:  39%|███▉      | 104/266 [01:04<01:35,  1.70it/s]Loading train:  39%|███▉      | 105/266 [01:05<01:31,  1.76it/s]Loading train:  40%|███▉      | 106/266 [01:05<01:29,  1.79it/s]Loading train:  40%|████      | 107/266 [01:06<01:28,  1.79it/s]Loading train:  41%|████      | 108/266 [01:06<01:28,  1.79it/s]Loading train:  41%|████      | 109/266 [01:07<01:28,  1.78it/s]Loading train:  41%|████▏     | 110/266 [01:07<01:24,  1.84it/s]Loading train:  42%|████▏     | 111/266 [01:08<01:22,  1.88it/s]Loading train:  42%|████▏     | 112/266 [01:08<01:21,  1.90it/s]Loading train:  42%|████▏     | 113/266 [01:09<01:19,  1.93it/s]Loading train:  43%|████▎     | 114/266 [01:09<01:17,  1.96it/s]Loading train:  43%|████▎     | 115/266 [01:10<01:16,  1.98it/s]Loading train:  44%|████▎     | 116/266 [01:10<01:15,  1.99it/s]Loading train:  44%|████▍     | 117/266 [01:11<01:14,  2.01it/s]Loading train:  44%|████▍     | 118/266 [01:11<01:13,  2.01it/s]Loading train:  45%|████▍     | 119/266 [01:12<01:16,  1.93it/s]Loading train:  45%|████▌     | 120/266 [01:12<01:17,  1.89it/s]Loading train:  45%|████▌     | 121/266 [01:13<01:19,  1.83it/s]Loading train:  46%|████▌     | 122/266 [01:14<01:23,  1.73it/s]Loading train:  46%|████▌     | 123/266 [01:14<01:21,  1.74it/s]Loading train:  47%|████▋     | 124/266 [01:15<01:20,  1.77it/s]Loading train:  47%|████▋     | 125/266 [01:15<01:18,  1.79it/s]Loading train:  47%|████▋     | 126/266 [01:16<01:17,  1.80it/s]Loading train:  48%|████▊     | 127/266 [01:16<01:18,  1.78it/s]Loading train:  48%|████▊     | 128/266 [01:17<01:17,  1.79it/s]Loading train:  48%|████▊     | 129/266 [01:17<01:16,  1.80it/s]Loading train:  49%|████▉     | 130/266 [01:18<01:15,  1.80it/s]Loading train:  49%|████▉     | 131/266 [01:19<01:17,  1.73it/s]Loading train:  50%|████▉     | 132/266 [01:19<01:17,  1.73it/s]Loading train:  50%|█████     | 133/266 [01:20<01:16,  1.74it/s]Loading train:  50%|█████     | 134/266 [01:20<01:15,  1.75it/s]Loading train:  51%|█████     | 135/266 [01:21<01:14,  1.76it/s]Loading train:  51%|█████     | 136/266 [01:22<01:15,  1.71it/s]Loading train:  52%|█████▏    | 137/266 [01:22<01:15,  1.70it/s]Loading train:  52%|█████▏    | 138/266 [01:23<01:15,  1.69it/s]Loading train:  52%|█████▏    | 139/266 [01:23<01:15,  1.67it/s]Loading train:  53%|█████▎    | 140/266 [01:24<01:14,  1.69it/s]Loading train:  53%|█████▎    | 141/266 [01:25<01:14,  1.67it/s]Loading train:  53%|█████▎    | 142/266 [01:25<01:14,  1.66it/s]Loading train:  54%|█████▍    | 143/266 [01:26<01:14,  1.65it/s]Loading train:  54%|█████▍    | 144/266 [01:26<01:14,  1.65it/s]Loading train:  55%|█████▍    | 145/266 [01:27<01:15,  1.61it/s]Loading train:  55%|█████▍    | 146/266 [01:28<01:12,  1.65it/s]Loading train:  55%|█████▌    | 147/266 [01:28<01:11,  1.67it/s]Loading train:  56%|█████▌    | 148/266 [01:29<01:09,  1.70it/s]Loading train:  56%|█████▌    | 149/266 [01:29<01:09,  1.69it/s]Loading train:  56%|█████▋    | 150/266 [01:30<01:08,  1.70it/s]Loading train:  57%|█████▋    | 151/266 [01:31<01:07,  1.71it/s]Loading train:  57%|█████▋    | 152/266 [01:31<01:06,  1.71it/s]Loading train:  58%|█████▊    | 153/266 [01:32<01:05,  1.71it/s]Loading train:  58%|█████▊    | 154/266 [01:32<01:06,  1.70it/s]Loading train:  58%|█████▊    | 155/266 [01:33<01:02,  1.78it/s]Loading train:  59%|█████▊    | 156/266 [01:33<00:59,  1.86it/s]Loading train:  59%|█████▉    | 157/266 [01:34<00:56,  1.93it/s]Loading train:  59%|█████▉    | 158/266 [01:34<00:54,  1.97it/s]Loading train:  60%|█████▉    | 159/266 [01:35<00:52,  2.02it/s]Loading train:  60%|██████    | 160/266 [01:35<00:51,  2.05it/s]Loading train:  61%|██████    | 161/266 [01:36<00:50,  2.07it/s]Loading train:  61%|██████    | 162/266 [01:36<00:50,  2.05it/s]Loading train:  61%|██████▏   | 163/266 [01:37<00:51,  1.99it/s]Loading train:  62%|██████▏   | 164/266 [01:37<00:50,  2.00it/s]Loading train:  62%|██████▏   | 165/266 [01:38<00:50,  2.01it/s]Loading train:  62%|██████▏   | 166/266 [01:38<00:50,  1.97it/s]Loading train:  63%|██████▎   | 167/266 [01:39<00:49,  1.99it/s]Loading train:  63%|██████▎   | 168/266 [01:39<00:48,  2.01it/s]Loading train:  64%|██████▎   | 169/266 [01:40<00:46,  2.07it/s]Loading train:  64%|██████▍   | 170/266 [01:40<00:45,  2.09it/s]Loading train:  64%|██████▍   | 171/266 [01:41<00:45,  2.10it/s]Loading train:  65%|██████▍   | 172/266 [01:41<00:45,  2.06it/s]Loading train:  65%|██████▌   | 173/266 [01:42<00:46,  2.02it/s]Loading train:  65%|██████▌   | 174/266 [01:42<00:45,  2.02it/s]Loading train:  66%|██████▌   | 175/266 [01:43<00:45,  2.01it/s]Loading train:  66%|██████▌   | 176/266 [01:43<00:44,  2.02it/s]Loading train:  67%|██████▋   | 177/266 [01:44<00:44,  2.00it/s]Loading train:  67%|██████▋   | 178/266 [01:44<00:43,  2.01it/s]Loading train:  67%|██████▋   | 179/266 [01:45<00:43,  2.00it/s]Loading train:  68%|██████▊   | 180/266 [01:45<00:43,  1.98it/s]Loading train:  68%|██████▊   | 181/266 [01:46<00:43,  1.98it/s]Loading train:  68%|██████▊   | 182/266 [01:46<00:42,  1.98it/s]Loading train:  69%|██████▉   | 183/266 [01:47<00:41,  2.00it/s]Loading train:  69%|██████▉   | 184/266 [01:47<00:41,  1.97it/s]Loading train:  70%|██████▉   | 185/266 [01:48<00:41,  1.96it/s]Loading train:  70%|██████▉   | 186/266 [01:48<00:40,  1.97it/s]Loading train:  70%|███████   | 187/266 [01:49<00:39,  1.99it/s]Loading train:  71%|███████   | 188/266 [01:49<00:39,  1.98it/s]Loading train:  71%|███████   | 189/266 [01:50<00:39,  1.95it/s]Loading train:  71%|███████▏  | 190/266 [01:50<00:38,  1.98it/s]Loading train:  72%|███████▏  | 191/266 [01:51<00:45,  1.63it/s]Loading train:  72%|███████▏  | 192/266 [01:52<00:48,  1.51it/s]Loading train:  73%|███████▎  | 193/266 [01:53<00:50,  1.46it/s]Loading train:  73%|███████▎  | 194/266 [01:53<00:54,  1.33it/s]Loading train:  73%|███████▎  | 195/266 [01:54<00:47,  1.48it/s]Loading train:  74%|███████▎  | 196/266 [01:54<00:44,  1.58it/s]Loading train:  74%|███████▍  | 197/266 [01:55<00:42,  1.64it/s]Loading train:  74%|███████▍  | 198/266 [01:56<00:40,  1.70it/s]Loading train:  75%|███████▍  | 199/266 [01:56<00:38,  1.72it/s]Loading train:  75%|███████▌  | 200/266 [01:57<00:37,  1.78it/s]Loading train:  76%|███████▌  | 201/266 [01:57<00:36,  1.79it/s]Loading train:  76%|███████▌  | 202/266 [01:58<00:35,  1.81it/s]Loading train:  76%|███████▋  | 203/266 [01:58<00:34,  1.84it/s]Loading train:  77%|███████▋  | 204/266 [01:59<00:34,  1.82it/s]Loading train:  77%|███████▋  | 205/266 [01:59<00:33,  1.85it/s]Loading train:  77%|███████▋  | 206/266 [02:00<00:32,  1.85it/s]Loading train:  78%|███████▊  | 207/266 [02:00<00:31,  1.87it/s]Loading train:  78%|███████▊  | 208/266 [02:01<00:30,  1.89it/s]Loading train:  79%|███████▊  | 209/266 [02:01<00:30,  1.85it/s]Loading train:  79%|███████▉  | 210/266 [02:02<00:30,  1.84it/s]Loading train:  79%|███████▉  | 211/266 [02:03<00:29,  1.84it/s]Loading train:  80%|███████▉  | 212/266 [02:03<00:29,  1.85it/s]Loading train:  80%|████████  | 213/266 [02:04<00:29,  1.80it/s]Loading train:  80%|████████  | 214/266 [02:04<00:28,  1.82it/s]Loading train:  81%|████████  | 215/266 [02:05<00:27,  1.83it/s]Loading train:  81%|████████  | 216/266 [02:05<00:27,  1.85it/s]Loading train:  82%|████████▏ | 217/266 [02:06<00:26,  1.85it/s]Loading train:  82%|████████▏ | 218/266 [02:06<00:25,  1.86it/s]Loading train:  82%|████████▏ | 219/266 [02:07<00:25,  1.85it/s]Loading train:  83%|████████▎ | 220/266 [02:08<00:25,  1.82it/s]Loading train:  83%|████████▎ | 221/266 [02:08<00:24,  1.81it/s]Loading train:  83%|████████▎ | 222/266 [02:11<00:54,  1.23s/it]Loading train:  84%|████████▍ | 223/266 [02:17<01:57,  2.73s/it]Loading train:  84%|████████▍ | 224/266 [02:26<03:10,  4.54s/it]Loading train:  85%|████████▍ | 225/266 [02:35<03:58,  5.82s/it]Loading train:  85%|████████▍ | 226/266 [02:43<04:27,  6.68s/it]Loading train:  85%|████████▌ | 227/266 [02:50<04:20,  6.68s/it]Loading train:  86%|████████▌ | 228/266 [02:56<04:08,  6.55s/it]Loading train:  86%|████████▌ | 229/266 [03:00<03:36,  5.84s/it]Loading train:  86%|████████▋ | 230/266 [03:04<03:02,  5.06s/it]Loading train:  87%|████████▋ | 231/266 [03:08<02:48,  4.81s/it]Loading train:  87%|████████▋ | 232/266 [03:15<03:02,  5.38s/it]Loading train:  88%|████████▊ | 233/266 [03:23<03:27,  6.27s/it]Loading train:  88%|████████▊ | 234/266 [03:32<03:43,  6.98s/it]Loading train:  88%|████████▊ | 235/266 [03:41<03:55,  7.61s/it]Loading train:  89%|████████▊ | 236/266 [03:51<04:15,  8.52s/it]Loading train:  89%|████████▉ | 237/266 [03:58<03:50,  7.94s/it]Loading train:  89%|████████▉ | 238/266 [04:07<03:48,  8.18s/it]Loading train:  90%|████████▉ | 239/266 [04:13<03:26,  7.64s/it]Loading train:  90%|█████████ | 240/266 [04:21<03:20,  7.71s/it]Loading train:  91%|█████████ | 241/266 [04:29<03:14,  7.78s/it]Loading train:  91%|█████████ | 242/266 [04:40<03:27,  8.65s/it]Loading train:  91%|█████████▏| 243/266 [04:48<03:20,  8.72s/it]Loading train:  92%|█████████▏| 244/266 [04:56<03:03,  8.32s/it]Loading train:  92%|█████████▏| 245/266 [05:02<02:40,  7.67s/it]Loading train:  92%|█████████▏| 246/266 [05:08<02:23,  7.17s/it]Loading train:  93%|█████████▎| 247/266 [05:15<02:12,  6.99s/it]Loading train:  93%|█████████▎| 248/266 [05:21<02:02,  6.82s/it]Loading train:  94%|█████████▎| 249/266 [05:39<02:51, 10.10s/it]Loading train:  94%|█████████▍| 250/266 [05:48<02:38,  9.92s/it]Loading train:  94%|█████████▍| 251/266 [05:57<02:22,  9.52s/it]Loading train:  95%|█████████▍| 252/266 [06:06<02:10,  9.35s/it]Loading train:  95%|█████████▌| 253/266 [06:15<01:59,  9.23s/it]Loading train:  95%|█████████▌| 254/266 [06:23<01:46,  8.91s/it]Loading train:  96%|█████████▌| 255/266 [06:32<01:39,  9.05s/it]Loading train:  96%|█████████▌| 256/266 [06:45<01:40, 10.02s/it]Loading train:  97%|█████████▋| 257/266 [06:59<01:42, 11.40s/it]Loading train:  97%|█████████▋| 258/266 [07:10<01:28, 11.10s/it]Loading train:  97%|█████████▋| 259/266 [07:18<01:12, 10.39s/it]Loading train:  98%|█████████▊| 260/266 [07:26<00:58,  9.71s/it]Loading train:  98%|█████████▊| 261/266 [07:33<00:44,  8.83s/it]Loading train:  98%|█████████▊| 262/266 [07:40<00:32,  8.11s/it]Loading train:  99%|█████████▉| 263/266 [07:45<00:21,  7.23s/it]Loading train:  99%|█████████▉| 264/266 [07:51<00:13,  6.80s/it]Loading train: 100%|█████████▉| 265/266 [07:58<00:06,  6.85s/it]Loading train: 100%|██████████| 266/266 [08:04<00:00,  6.85s/it]Loading train: 100%|██████████| 266/266 [08:04<00:00,  1.82s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:05, 44.67it/s]concatenating: train:   4%|▍         | 10/266 [00:00<00:05, 45.37it/s]concatenating: train:   6%|▌         | 15/266 [00:00<00:05, 45.54it/s]concatenating: train:   8%|▊         | 20/266 [00:00<00:05, 45.61it/s]concatenating: train:   9%|▉         | 25/266 [00:00<00:05, 46.03it/s]concatenating: train:  11%|█▏        | 30/266 [00:00<00:05, 46.50it/s]concatenating: train:  13%|█▎        | 35/266 [00:00<00:04, 47.01it/s]concatenating: train:  15%|█▌        | 40/266 [00:00<00:04, 46.40it/s]concatenating: train:  17%|█▋        | 45/266 [00:00<00:05, 43.81it/s]concatenating: train:  19%|█▉        | 50/266 [00:01<00:05, 43.18it/s]concatenating: train:  21%|██        | 55/266 [00:01<00:05, 41.74it/s]concatenating: train:  23%|██▎       | 60/266 [00:01<00:04, 42.14it/s]concatenating: train:  25%|██▍       | 66/266 [00:01<00:04, 44.26it/s]concatenating: train:  27%|██▋       | 72/266 [00:01<00:04, 46.34it/s]concatenating: train:  29%|██▉       | 78/266 [00:01<00:03, 48.06it/s]concatenating: train:  31%|███       | 83/266 [00:01<00:03, 48.12it/s]concatenating: train:  33%|███▎      | 88/266 [00:01<00:03, 47.26it/s]concatenating: train:  35%|███▍      | 93/266 [00:02<00:03, 47.35it/s]concatenating: train:  37%|███▋      | 98/266 [00:02<00:03, 48.06it/s]concatenating: train:  39%|███▉      | 105/266 [00:02<00:03, 51.28it/s]concatenating: train:  42%|████▏     | 111/266 [00:02<00:02, 52.05it/s]concatenating: train:  44%|████▍     | 117/266 [00:02<00:02, 53.91it/s]concatenating: train:  47%|████▋     | 124/266 [00:02<00:02, 55.46it/s]concatenating: train:  49%|████▉     | 130/266 [00:02<00:02, 55.30it/s]concatenating: train:  51%|█████     | 136/266 [00:02<00:02, 53.16it/s]concatenating: train:  53%|█████▎    | 142/266 [00:02<00:02, 45.29it/s]concatenating: train:  55%|█████▌    | 147/266 [00:03<00:02, 43.68it/s]concatenating: train:  57%|█████▋    | 152/266 [00:03<00:02, 41.67it/s]concatenating: train:  59%|█████▉    | 157/266 [00:03<00:02, 42.71it/s]concatenating: train:  61%|██████    | 162/266 [00:03<00:02, 44.02it/s]concatenating: train:  63%|██████▎   | 167/266 [00:03<00:02, 45.04it/s]concatenating: train:  65%|██████▍   | 172/266 [00:03<00:02, 45.83it/s]concatenating: train:  67%|██████▋   | 177/266 [00:03<00:01, 46.97it/s]concatenating: train:  68%|██████▊   | 182/266 [00:03<00:01, 47.78it/s]concatenating: train:  70%|███████   | 187/266 [00:03<00:01, 48.31it/s]concatenating: train:  72%|███████▏  | 192/266 [00:04<00:01, 47.91it/s]concatenating: train:  74%|███████▍  | 197/266 [00:04<00:01, 46.48it/s]concatenating: train:  76%|███████▌  | 202/266 [00:04<00:01, 43.91it/s]concatenating: train:  78%|███████▊  | 207/266 [00:04<00:01, 44.13it/s]concatenating: train:  80%|███████▉  | 212/266 [00:04<00:01, 44.36it/s]concatenating: train:  82%|████████▏ | 217/266 [00:04<00:01, 42.91it/s]concatenating: train:  83%|████████▎ | 222/266 [00:04<00:01, 41.56it/s]concatenating: train:  85%|████████▌ | 227/266 [00:04<00:00, 40.99it/s]concatenating: train:  87%|████████▋ | 232/266 [00:05<00:00, 41.25it/s]concatenating: train:  89%|████████▉ | 237/266 [00:05<00:00, 42.63it/s]concatenating: train:  91%|█████████ | 242/266 [00:05<00:00, 43.28it/s]concatenating: train:  93%|█████████▎| 247/266 [00:05<00:00, 44.07it/s]concatenating: train:  95%|█████████▍| 252/266 [00:05<00:00, 43.21it/s]concatenating: train:  97%|█████████▋| 257/266 [00:05<00:00, 41.99it/s]concatenating: train:  98%|█████████▊| 262/266 [00:05<00:00, 42.37it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 45.63it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:13<00:41, 13.78s/it]Loading test:  50%|█████     | 2/4 [00:19<00:22, 11.37s/it]Loading test:  75%|███████▌  | 3/4 [00:27<00:10, 10.37s/it]Loading test: 100%|██████████| 4/4 [00:39<00:00, 10.70s/it]Loading test: 100%|██████████| 4/4 [00:39<00:00,  9.76s/it]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 59.86it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 2  | SD 0  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 2  | SD 0  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 2  | SD 0  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 40 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 84, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 84, 52, 40)   400         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 84, 52, 40)   160         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 84, 52, 40)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 84, 52, 40)   14440       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 84, 52, 40)   160         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 84, 52, 40)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 42, 26, 40)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 42, 26, 40)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 42, 26, 80)   28880       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 42, 26, 80)   320         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 42, 26, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 42, 26, 80)   57680       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 42, 26, 80)   320         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 42, 26, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 42, 26, 120)  0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 21, 13, 120)  0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 21, 13, 120)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 21, 13, 160)  172960      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 21, 13, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 21, 13, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 21, 13, 160)  230560      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 21, 13, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 21, 13, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 21, 13, 280)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 21, 13, 280)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 42, 26, 80)   89680       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 42, 26, 200)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 42, 26, 80)   144080      concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 42, 26, 80)   320         conv2d_7[0][0]                   2020-01-21 00:37:07.424403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 00:37:07.424504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 00:37:07.424516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 00:37:07.424524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 00:37:07.424888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
activation_7 (Activation)       (None, 42, 26, 80)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 42, 26, 80)   57680       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 42, 26, 80)   320         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 42, 26, 80)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 42, 26, 280)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 42, 26, 280)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 84, 52, 40)   44840       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 84, 52, 80)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 84, 52, 40)   28840       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 84, 52, 40)   160         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 84, 52, 40)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 84, 52, 40)   14440       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 84, 52, 40)   160         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 84, 52, 40)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 84, 52, 120)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 84, 52, 120)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 84, 52, 13)   1573        dropout_5[0][0]                  
==================================================================================================
Total params: 889,253
Trainable params: 887,653
Non-trainable params: 1,600
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.34323683e-02 3.28761158e-02 7.68762751e-02 9.55226375e-03
 2.76464264e-02 7.23290818e-03 8.42210951e-02 1.14263927e-01
 8.97197359e-02 1.36315500e-02 2.90889031e-01 1.89418903e-01
 2.39401553e-04]
Train on 10434 samples, validate on 152 samples
Epoch 1/300
 - 31s - loss: 0.5586 - acc: 0.9070 - mDice: 0.3979 - val_loss: 0.6695 - val_acc: 0.9409 - val_mDice: 0.2771

Epoch 00001: val_mDice improved from -inf to 0.27706, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 2/300
 - 26s - loss: 0.3876 - acc: 0.9415 - mDice: 0.5822 - val_loss: 0.6747 - val_acc: 0.9463 - val_mDice: 0.2711

Epoch 00002: val_mDice did not improve from 0.27706
Epoch 3/300
 - 26s - loss: 0.3550 - acc: 0.9459 - mDice: 0.6174 - val_loss: 0.6651 - val_acc: 0.9471 - val_mDice: 0.2789

Epoch 00003: val_mDice improved from 0.27706 to 0.27887, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 4/300
 - 26s - loss: 0.3347 - acc: 0.9485 - mDice: 0.6393 - val_loss: 0.6499 - val_acc: 0.9509 - val_mDice: 0.2942

Epoch 00004: val_mDice improved from 0.27887 to 0.29417, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 5/300
 - 27s - loss: 0.3213 - acc: 0.9497 - mDice: 0.6537 - val_loss: 0.6609 - val_acc: 0.9455 - val_mDice: 0.2845

Epoch 00005: val_mDice did not improve from 0.29417
Epoch 6/300
 - 27s - loss: 0.3126 - acc: 0.9510 - mDice: 0.6631 - val_loss: 0.6272 - val_acc: 0.9529 - val_mDice: 0.3074

Epoch 00006: val_mDice improved from 0.29417 to 0.30739, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd0/best_model_weights.h5
Epoch 7/300
 - 26s - loss: 0.3019 - acc: 0.9521 - mDice: 0.6747 - val_loss: 0.6410 - val_acc: 0.9518 - val_mDice: 0.3028

Epoch 00007: val_mDice did not improve from 0.30739
Epoch 8/300
 - 26s - loss: 0.2916 - acc: 0.9533 - mDice: 0.6858 - val_loss: 0.5948 - val_acc: 0.9506 - val_mDice: 0.2995

Epoch 00008: val_mDice did not improve from 0.30739
Epoch 9/300
 - 27s - loss: 0.2882 - acc: 0.9536 - mDice: 0.6895 - val_loss: 0.6321 - val_acc: 0.9506 - val_mDice: 0.2896

Epoch 00009: val_mDice did not improve from 0.30739
Epoch 10/300
 - 27s - loss: 0.2814 - acc: 0.9543 - mDice: 0.6969 - val_loss: 0.4436 - val_acc: 0.9533 - val_mDice: 0.3005

Epoch 00010: val_mDice did not improve from 0.30739
Epoch 11/300
 - 27s - loss: 0.2834 - acc: 0.9542 - mDice: 0.6946 - val_loss: 0.4855 - val_acc: 0.9549 - val_mDice: 0.2950

Epoch 00011: val_mDice did not improve from 0.30739
Epoch 12/300
 - 27s - loss: 0.2755 - acc: 0.9553 - mDice: 0.7031 - val_loss: 0.3698 - val_acc: 0.9513 - val_mDice: 0.2980

Epoch 00012: val_mDice did not improve from 0.30739
Epoch 13/300
 - 26s - loss: 0.2723 - acc: 0.9555 - mDice: 0.7066 - val_loss: 0.2990 - val_acc: 0.9525 - val_mDice: 0.2849

Epoch 00013: val_mDice did not improve from 0.30739
Epoch 14/300
 - 26s - loss: 0.2684 - acc: 0.9559 - mDice: 0.7108 - val_loss: 0.1702 - val_acc: 0.9531 - val_mDice: 0.3011

Epoch 00014: val_mDice did not improve from 0.30739
Epoch 15/300
 - 26s - loss: 0.2634 - acc: 0.9564 - mDice: 0.7162 - val_loss: 0.2985 - val_acc: 0.9556 - val_mDice: 0.2957

Epoch 00015: val_mDice did not improve from 0.30739
Epoch 16/300
 - 26s - loss: 0.2660 - acc: 0.9563 - mDice: 0.7134 - val_loss: 0.2560 - val_acc: 0.9519 - val_mDice: 0.2923

Epoch 00016: val_mDice did not improve from 0.30739
Epoch 17/300
 - 26s - loss: 0.2668 - acc: 0.9563 - mDice: 0.7125 - val_loss: 0.3546 - val_acc: 0.9542 - val_mDice: 0.3046

Epoch 00017: val_mDice did not improve from 0.30739
Epoch 18/300
 - 26s - loss: 0.2581 - acc: 0.9568 - mDice: 0.7219 - val_loss: 0.2126 - val_acc: 0.9483 - val_mDice: 0.2906

Epoch 00018: val_mDice did not improve from 0.30739
Epoch 19/300
 - 26s - loss: 0.2545 - acc: 0.9572 - mDice: 0.7258 - val_loss: 0.1741 - val_acc: 0.9537 - val_mDice: 0.2999

Epoch 00019: val_mDice did not improve from 0.30739
Epoch 20/300
 - 26s - loss: 0.2550 - acc: 0.9573 - mDice: 0.7253 - val_loss: 0.0836 - val_acc: 0.9545 - val_mDice: 0.2995

Epoch 00020: val_mDice did not improve from 0.30739
Epoch 21/300
 - 26s - loss: 0.2518 - acc: 0.9578 - mDice: 0.7287 - val_loss: 0.2035 - val_acc: 0.9506 - val_mDice: 0.2876

Epoch 00021: val_mDice did not improve from 0.30739

Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 22/300
 - 26s - loss: 0.2408 - acc: 0.9589 - mDice: 0.7406 - val_loss: 0.0915 - val_acc: 0.9517 - val_mDice: 0.2942

Epoch 00022: val_mDice did not improve from 0.30739
Epoch 23/300
 - 26s - loss: 0.2365 - acc: 0.9591 - mDice: 0.7453 - val_loss: 0.1168 - val_acc: 0.9526 - val_mDice: 0.2964

Epoch 00023: val_mDice did not improve from 0.30739
Epoch 24/300
 - 26s - loss: 0.2389 - acc: 0.9593 - mDice: 0.7427 - val_loss: 0.0547 - val_acc: 0.9519 - val_mDice: 0.2944

Epoch 00024: val_mDice did not improve from 0.30739
Epoch 25/300
 - 26s - loss: 0.2360 - acc: 0.9595 - mDice: 0.7458 - val_loss: 0.0961 - val_acc: 0.9524 - val_mDice: 0.2988

Epoch 00025: val_mDice did not improve from 0.30739
Epoch 26/300
 - 26s - loss: 0.2319 - acc: 0.9598 - mDice: 0.7502 - val_loss: 0.0643 - val_acc: 0.9546 - val_mDice: 0.3032

Epoch 00026: val_mDice did not improve from 0.30739
Epoch 27/300
 - 26s - loss: 0.2337 - acc: 0.9598 - mDice: 0.7483 - val_loss: 0.0649 - val_acc: 0.9537 - val_mDice: 0.3023

Epoch 00027: val_mDice did not improve from 0.30739
Epoch 28/300
 - 26s - loss: 0.2332 - acc: 0.9596 - mDice: 0.7488 - val_loss: 0.1389 - val_acc: 0.9555 - val_mDice: 0.3055

Epoch 00028: val_mDice did not improve from 0.30739
Epoch 29/300
 - 26s - loss: 0.2317 - acc: 0.9600 - mDice: 0.7504 - val_loss: 0.0677 - val_acc: 0.9555 - val_mDice: 0.2980

Epoch 00029: val_mDice did not improve from 0.30739
Epoch 30/300
 - 26s - loss: 0.2284 - acc: 0.9601 - mDice: 0.7540 - val_loss: 0.2330 - val_acc: 0.9510 - val_mDice: 0.2780

Epoch 00030: val_mDice did not improve from 0.30739
Epoch 31/300
 - 26s - loss: 0.2309 - acc: 0.9601 - mDice: 0.7513 - val_loss: 0.0789 - val_acc: 0.9506 - val_mDice: 0.2939

Epoch 00031: val_mDice did not improve from 0.30739
Epoch 32/300
 - 27s - loss: 0.2290 - acc: 0.9603 - mDice: 0.7533 - val_loss: 0.0832 - val_acc: 0.9546 - val_mDice: 0.3020

Epoch 00032: val_mDice did not improve from 0.30739
Epoch 33/300
 - 26s - loss: 0.2293 - acc: 0.9603 - mDice: 0.7530 - val_loss: 0.0564 - val_acc: 0.9548 - val_mDice: 0.3017

Epoch 00033: val_mDice did not improve from 0.30739
Epoch 34/300
 - 26s - loss: 0.2311 - acc: 0.9603 - mDice: 0.7511 - val_loss: 0.0191 - val_acc: 0.9549 - val_mDice: 0.3004

Epoch 00034: val_mDice did not improve from 0.30739
Epoch 35/300
 - 26s - loss: 0.2249 - acc: 0.9606 - mDice: 0.7577 - val_loss: 0.0386 - val_acc: 0.9524 - val_mDice: 0.2977

Epoch 00035: val_mDice did not improve from 0.30739
Epoch 36/300
 - 26s - loss: 0.2258 - acc: 0.9606 - mDice: 0.7568 - val_loss: 0.0633 - val_acc: 0.9519 - val_mDice: 0.2903

Epoch 00036: val_mDice did not improve from 0.30739

Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 37/300
 - 26s - loss: 0.2221 - acc: 0.9611 - mDice: 0.7607 - val_loss: 0.0285 - val_acc: 0.9541 - val_mDice: 0.3055

Epoch 00037: val_mDice did not improve from 0.30739
Epoch 38/300
 - 26s - loss: 0.2161 - acc: 0.9613 - mDice: 0.7673 - val_loss: 0.0213 - val_acc: 0.9536 - val_mDice: 0.3026

Epoch 00038: val_mDice did not improve from 0.30739
Epoch 39/300
 - 26s - loss: 0.2180 - acc: 0.9614 - mDice: 0.7652 - val_loss: 0.0281 - val_acc: 0.9530 - val_mDice: 0.3026

Epoch 00039: val_mDice did not improve from 0.30739
Epoch 40/300
 - 26s - loss: 0.2154 - acc: 0.9615 - mDice: 0.7680 - val_loss: 0.0206 - val_acc: 0.9540 - val_mDice: 0.2999

Epoch 00040: val_mDice did not improve from 0.30739
Epoch 41/300
 - 26s - loss: 0.2167 - acc: 0.9615 - mDice: 0.7666 - val_loss: -6.5946e-03 - val_acc: 0.9542 - val_mDice: 0.3042

Epoch 00041: val_mDice did not improve from 0.30739
Epoch 42/300
 - 26s - loss: 0.2154 - acc: 0.9616 - mDice: 0.7680 - val_loss: -1.8830e-02 - val_acc: 0.9549 - val_mDice: 0.3067

Epoch 00042: val_mDice did not improve from 0.30739
Epoch 43/300
 - 25s - loss: 0.2137 - acc: 0.9617 - mDice: 0.7699 - val_loss: -2.2573e-03 - val_acc: 0.9528 - val_mDice: 0.3004

Epoch 00043: val_mDice did not improve from 0.30739
Epoch 44/300
 - 26s - loss: 0.2132 - acc: 0.9618 - mDice: 0.7704 - val_loss: 0.0081 - val_acc: 0.9537 - val_mDice: 0.3025

Epoch 00044: val_mDice did not improve from 0.30739
Epoch 45/300
 - 26s - loss: 0.2143 - acc: 0.9618 - mDice: 0.7692 - val_loss: 0.0045 - val_acc: 0.9552 - val_mDice: 0.3031

Epoch 00045: val_mDice did not improve from 0.30739
Epoch 46/300
 - 26s - loss: 0.2151 - acc: 0.9618 - mDice: 0.7683 - val_loss: -1.3206e-02 - val_acc: 0.9523 - val_mDice: 0.2993

predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]predicting test subjects:  50%|█████     | 2/4 [00:02<00:02,  1.27s/it]predicting test subjects:  75%|███████▌  | 3/4 [00:03<00:01,  1.13s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:36,  2.74it/s]Loading train:   1%|          | 2/266 [00:00<01:32,  2.84it/s]Loading train:   1%|          | 3/266 [00:00<01:28,  2.97it/s]Loading train:   2%|▏         | 4/266 [00:01<01:23,  3.13it/s]Loading train:   2%|▏         | 5/266 [00:01<01:24,  3.08it/s]Loading train:   2%|▏         | 6/266 [00:01<01:24,  3.08it/s]Loading train:   3%|▎         | 7/266 [00:02<01:24,  3.08it/s]Loading train:   3%|▎         | 8/266 [00:02<01:23,  3.10it/s]Loading train:   3%|▎         | 9/266 [00:02<01:22,  3.10it/s]Loading train:   4%|▍         | 10/266 [00:03<01:22,  3.11it/s]Loading train:   4%|▍         | 11/266 [00:03<01:21,  3.14it/s]Loading train:   5%|▍         | 12/266 [00:03<01:20,  3.14it/s]Loading train:   5%|▍         | 13/266 [00:04<01:20,  3.16it/s]Loading train:   5%|▌         | 14/266 [00:04<01:19,  3.16it/s]Loading train:   6%|▌         | 15/266 [00:04<01:19,  3.16it/s]Loading train:   6%|▌         | 16/266 [00:05<01:19,  3.16it/s]Loading train:   6%|▋         | 17/266 [00:05<01:18,  3.15it/s]Loading train:   7%|▋         | 18/266 [00:05<01:18,  3.16it/s]Loading train:   7%|▋         | 19/266 [00:06<01:18,  3.16it/s]Loading train:   8%|▊         | 20/266 [00:06<01:17,  3.16it/s]Loading train:   8%|▊         | 21/266 [00:06<01:17,  3.17it/s]Loading train:   8%|▊         | 22/266 [00:07<01:17,  3.16it/s]Loading train:   9%|▊         | 23/266 [00:07<01:17,  3.15it/s]Loading train:   9%|▉         | 24/266 [00:07<01:15,  3.19it/s]Loading train:   9%|▉         | 25/266 [00:07<01:14,  3.24it/s]Loading train:  10%|▉         | 26/266 [00:08<01:13,  3.27it/s]Loading train:  10%|█         | 27/266 [00:08<01:12,  3.29it/s]Loading train:  11%|█         | 28/266 [00:08<01:12,  3.29it/s]Loading train:  11%|█         | 29/266 [00:09<01:11,  3.29it/s]Loading train:  11%|█▏        | 30/266 [00:09<01:11,  3.29it/s]Loading train:  12%|█▏        | 31/266 [00:09<01:11,  3.27it/s]Loading train:  12%|█▏        | 32/266 [00:10<01:11,  3.27it/s]Loading train:  12%|█▏        | 33/266 [00:10<01:10,  3.31it/s]Loading train:  13%|█▎        | 34/266 [00:10<01:09,  3.32it/s]Loading train:  13%|█▎        | 35/266 [00:10<01:09,  3.34it/s]Loading train:  14%|█▎        | 36/266 [00:11<01:08,  3.35it/s]Loading train:  14%|█▍        | 37/266 [00:11<01:08,  3.32it/s]Loading train:  14%|█▍        | 38/266 [00:11<01:08,  3.35it/s]Loading train:  15%|█▍        | 39/266 [00:12<01:08,  3.29it/s]Loading train:  15%|█▌        | 40/266 [00:12<01:08,  3.32it/s]Loading train:  15%|█▌        | 41/266 [00:12<01:07,  3.35it/s]Loading train:  16%|█▌        | 42/266 [00:13<01:04,  3.45it/s]Loading train:  16%|█▌        | 43/266 [00:13<01:03,  3.53it/s]Loading train:  17%|█▋        | 44/266 [00:13<01:02,  3.57it/s]Loading train:  17%|█▋        | 45/266 [00:13<01:01,  3.60it/s]Loading train:  17%|█▋        | 46/266 [00:14<01:00,  3.62it/s]Loading train:  18%|█▊        | 47/266 [00:14<01:00,  3.64it/s]Loading train:  18%|█▊        | 48/266 [00:14<00:59,  3.65it/s]Loading train:  18%|█▊        | 49/266 [00:14<00:59,  3.67it/s]Loading train:  19%|█▉        | 50/266 [00:15<00:58,  3.67it/s]Loading train:  19%|█▉        | 51/266 [00:15<00:58,  3.66it/s]Loading train:  20%|█▉        | 52/266 [00:15<00:58,  3.64it/s]Loading train:  20%|█▉        | 53/266 [00:16<00:58,  3.61it/s]Loading train:  20%|██        | 54/266 [00:16<00:58,  3.63it/s]Loading train:  21%|██        | 55/266 [00:16<00:58,  3.63it/s]Loading train:  21%|██        | 56/266 [00:16<00:58,  3.59it/s]Loading train:  21%|██▏       | 57/266 [00:17<01:01,  3.42it/s]Loading train:  22%|██▏       | 58/266 [00:17<01:00,  3.45it/s]Loading train:  22%|██▏       | 59/266 [00:17<00:59,  3.48it/s]Loading train:  23%|██▎       | 60/266 [00:17<00:57,  3.58it/s]Loading train:  23%|██▎       | 61/266 [00:18<00:55,  3.68it/s]Loading train:  23%|██▎       | 62/266 [00:18<00:54,  3.72it/s]Loading train:  24%|██▎       | 63/266 [00:18<00:54,  3.74it/s]Loading train:  24%|██▍       | 64/266 [00:19<00:53,  3.75it/s]Loading train:  24%|██▍       | 65/266 [00:19<00:54,  3.71it/s]Loading train:  25%|██▍       | 66/266 [00:19<00:53,  3.75it/s]Loading train:  25%|██▌       | 67/266 [00:19<00:53,  3.74it/s]Loading train:  26%|██▌       | 68/266 [00:20<00:52,  3.77it/s]Loading train:  26%|██▌       | 69/266 [00:20<00:51,  3.80it/s]Loading train:  26%|██▋       | 70/266 [00:20<00:51,  3.80it/s]Loading train:  27%|██▋       | 71/266 [00:20<00:51,  3.82it/s]Loading train:  27%|██▋       | 72/266 [00:21<00:50,  3.86it/s]Loading train:  27%|██▋       | 73/266 [00:21<00:49,  3.86it/s]Loading train:  28%|██▊       | 74/266 [00:21<00:49,  3.86it/s]Loading train:  28%|██▊       | 75/266 [00:21<00:49,  3.86it/s]Loading train:  29%|██▊       | 76/266 [00:22<00:49,  3.86it/s]Loading train:  29%|██▉       | 77/266 [00:22<00:49,  3.85it/s]Loading train:  29%|██▉       | 78/266 [00:22<00:52,  3.56it/s]Loading train:  30%|██▉       | 79/266 [00:23<00:53,  3.47it/s]Loading train:  30%|███       | 80/266 [00:23<00:54,  3.40it/s]Loading train:  30%|███       | 81/266 [00:23<00:55,  3.33it/s]Loading train:  31%|███       | 82/266 [00:24<00:55,  3.31it/s]Loading train:  31%|███       | 83/266 [00:24<00:56,  3.24it/s]Loading train:  32%|███▏      | 84/266 [00:24<00:57,  3.14it/s]Loading train:  32%|███▏      | 85/266 [00:24<00:57,  3.14it/s]Loading train:  32%|███▏      | 86/266 [00:25<00:57,  3.16it/s]Loading train:  33%|███▎      | 87/266 [00:25<00:56,  3.18it/s]Loading train:  33%|███▎      | 88/266 [00:25<00:55,  3.21it/s]Loading train:  33%|███▎      | 89/266 [00:26<00:54,  3.22it/s]Loading train:  34%|███▍      | 90/266 [00:26<00:54,  3.23it/s]Loading train:  34%|███▍      | 91/266 [00:26<00:54,  3.22it/s]Loading train:  35%|███▍      | 92/266 [00:27<00:53,  3.24it/s]Loading train:  35%|███▍      | 93/266 [00:27<00:53,  3.25it/s]Loading train:  35%|███▌      | 94/266 [00:27<00:53,  3.23it/s]Loading train:  36%|███▌      | 95/266 [00:28<00:52,  3.25it/s]Loading train:  36%|███▌      | 96/266 [00:28<00:52,  3.26it/s]Loading train:  36%|███▋      | 97/266 [00:28<00:52,  3.20it/s]Loading train:  37%|███▋      | 98/266 [00:29<00:52,  3.22it/s]Loading train:  37%|███▋      | 99/266 [00:29<00:49,  3.37it/s]Loading train:  38%|███▊      | 100/266 [00:29<00:48,  3.40it/s]Loading train:  38%|███▊      | 101/266 [00:29<00:48,  3.41it/s]Loading train:  38%|███▊      | 102/266 [00:30<00:48,  3.37it/s]Loading train:  39%|███▊      | 103/266 [00:30<00:48,  3.35it/s]Loading train:  39%|███▉      | 104/266 [00:30<00:47,  3.38it/s]Loading train:  39%|███▉      | 105/266 [00:31<00:47,  3.41it/s]Loading train:  40%|███▉      | 106/266 [00:31<00:46,  3.41it/s]Loading train:  40%|████      | 107/266 [00:31<00:46,  3.43it/s]Loading train:  41%|████      | 108/266 [00:31<00:45,  3.46it/s]Loading train:  41%|████      | 109/266 [00:32<00:45,  3.46it/s]Loading train:  41%|████▏     | 110/266 [00:32<00:44,  3.47it/s]Loading train:  42%|████▏     | 111/266 [00:32<00:44,  3.48it/s]Loading train:  42%|████▏     | 112/266 [00:33<00:44,  3.47it/s]Loading train:  42%|████▏     | 113/266 [00:33<00:44,  3.44it/s]Loading train:  43%|████▎     | 114/266 [00:33<00:44,  3.44it/s]Loading train:  43%|████▎     | 115/266 [00:33<00:44,  3.42it/s]Loading train:  44%|████▎     | 116/266 [00:34<00:44,  3.36it/s]Loading train:  44%|████▍     | 117/266 [00:34<00:44,  3.33it/s]Loading train:  44%|████▍     | 118/266 [00:34<00:44,  3.34it/s]Loading train:  45%|████▍     | 119/266 [00:35<00:44,  3.30it/s]Loading train:  45%|████▌     | 120/266 [00:35<00:45,  3.24it/s]Loading train:  45%|████▌     | 121/266 [00:35<00:45,  3.20it/s]Loading train:  46%|████▌     | 122/266 [00:36<00:46,  3.13it/s]Loading train:  46%|████▌     | 123/266 [00:36<00:45,  3.14it/s]Loading train:  47%|████▋     | 124/266 [00:36<00:46,  3.08it/s]Loading train:  47%|████▋     | 125/266 [00:37<00:46,  3.06it/s]Loading train:  47%|████▋     | 126/266 [00:37<00:45,  3.10it/s]Loading train:  48%|████▊     | 127/266 [00:37<00:45,  3.08it/s]Loading train:  48%|████▊     | 128/266 [00:38<00:44,  3.08it/s]Loading train:  48%|████▊     | 129/266 [00:38<00:44,  3.06it/s]Loading train:  49%|████▉     | 130/266 [00:38<00:44,  3.08it/s]Loading train:  49%|████▉     | 131/266 [00:39<00:44,  3.06it/s]Loading train:  50%|████▉     | 132/266 [00:39<00:43,  3.08it/s]Loading train:  50%|█████     | 133/266 [00:39<00:43,  3.09it/s]Loading train:  50%|█████     | 134/266 [00:40<00:42,  3.10it/s]Loading train:  51%|█████     | 135/266 [00:40<00:42,  3.07it/s]Loading train:  51%|█████     | 136/266 [00:40<00:41,  3.10it/s]Loading train:  52%|█████▏    | 137/266 [00:40<00:40,  3.21it/s]Loading train:  52%|█████▏    | 138/266 [00:41<00:38,  3.30it/s]Loading train:  52%|█████▏    | 139/266 [00:41<00:37,  3.39it/s]Loading train:  53%|█████▎    | 140/266 [00:41<00:36,  3.43it/s]Loading train:  53%|█████▎    | 141/266 [00:42<00:36,  3.47it/s]Loading train:  53%|█████▎    | 142/266 [00:42<00:35,  3.50it/s]Loading train:  54%|█████▍    | 143/266 [00:42<00:34,  3.52it/s]Loading train:  54%|█████▍    | 144/266 [00:42<00:34,  3.52it/s]Loading train:  55%|█████▍    | 145/266 [00:43<00:34,  3.51it/s]Loading train:  55%|█████▍    | 146/266 [00:43<00:34,  3.48it/s]Loading train:  55%|█████▌    | 147/266 [00:43<00:34,  3.47it/s]Loading train:  56%|█████▌    | 148/266 [00:44<00:33,  3.48it/s]Loading train:  56%|█████▌    | 149/266 [00:44<00:33,  3.48it/s]Loading train:  56%|█████▋    | 150/266 [00:44<00:33,  3.46it/s]Loading train:  57%|█████▋    | 151/266 [00:44<00:33,  3.45it/s]Loading train:  57%|█████▋    | 152/266 [00:45<00:33,  3.44it/s]Loading train:  58%|█████▊    | 153/266 [00:45<00:32,  3.47it/s]Loading train:  58%|█████▊    | 154/266 [00:45<00:32,  3.48it/s]Loading train:  58%|█████▊    | 155/266 [00:46<00:31,  3.57it/s]Loading train:  59%|█████▊    | 156/266 [00:46<00:29,  3.68it/s]Loading train:  59%|█████▉    | 157/266 [00:46<00:29,  3.72it/s]Loading train:  59%|█████▉    | 158/266 [00:46<00:28,  3.75it/s]Loading train:  60%|█████▉    | 159/266 [00:47<00:28,  3.79it/s]Loading train:  60%|██████    | 160/266 [00:47<00:27,  3.81it/s]Loading train:  61%|██████    | 161/266 [00:47<00:27,  3.84it/s]Loading train:  61%|██████    | 162/266 [00:47<00:26,  3.87it/s]Loading train:  61%|██████▏   | 163/266 [00:48<00:26,  3.89it/s]Loading train:  62%|██████▏   | 164/266 [00:48<00:26,  3.89it/s]Loading train:  62%|██████▏   | 165/266 [00:48<00:25,  3.93it/s]Loading train:  62%|██████▏   | 166/266 [00:48<00:25,  3.94it/s]Loading train:  63%|██████▎   | 167/266 [00:49<00:25,  3.93it/s]Loading train:  63%|██████▎   | 168/266 [00:49<00:24,  3.93it/s]Loading train:  64%|██████▎   | 169/266 [00:49<00:24,  3.95it/s]Loading train:  64%|██████▍   | 170/266 [00:49<00:24,  3.87it/s]Loading train:  64%|██████▍   | 171/266 [00:50<00:24,  3.81it/s]Loading train:  65%|██████▍   | 172/266 [00:50<00:24,  3.79it/s]Loading train:  65%|██████▌   | 173/266 [00:50<00:24,  3.76it/s]Loading train:  65%|██████▌   | 174/266 [00:51<00:24,  3.74it/s]Loading train:  66%|██████▌   | 175/266 [00:51<00:24,  3.72it/s]Loading train:  66%|██████▌   | 176/266 [00:51<00:24,  3.71it/s]Loading train:  67%|██████▋   | 177/266 [00:51<00:23,  3.71it/s]Loading train:  67%|██████▋   | 178/266 [00:52<00:23,  3.70it/s]Loading train:  67%|██████▋   | 179/266 [00:52<00:23,  3.64it/s]Loading train:  68%|██████▊   | 180/266 [00:52<00:24,  3.58it/s]Loading train:  68%|██████▊   | 181/266 [00:52<00:23,  3.59it/s]Loading train:  68%|██████▊   | 182/266 [00:53<00:23,  3.59it/s]Loading train:  69%|██████▉   | 183/266 [00:53<00:22,  3.64it/s]Loading train:  69%|██████▉   | 184/266 [00:53<00:22,  3.67it/s]Loading train:  70%|██████▉   | 185/266 [00:54<00:21,  3.68it/s]Loading train:  70%|██████▉   | 186/266 [00:54<00:21,  3.71it/s]Loading train:  70%|███████   | 187/266 [00:54<00:21,  3.69it/s]Loading train:  71%|███████   | 188/266 [00:54<00:21,  3.71it/s]Loading train:  71%|███████   | 189/266 [00:55<00:20,  3.71it/s]Loading train:  71%|███████▏  | 190/266 [00:55<00:20,  3.68it/s]Loading train:  72%|███████▏  | 191/266 [00:55<00:21,  3.53it/s]Loading train:  72%|███████▏  | 192/266 [00:55<00:20,  3.62it/s]Loading train:  73%|███████▎  | 193/266 [00:56<00:20,  3.62it/s]Loading train:  73%|███████▎  | 194/266 [00:56<00:20,  3.46it/s]Loading train:  73%|███████▎  | 195/266 [00:56<00:19,  3.57it/s]Loading train:  74%|███████▎  | 196/266 [00:57<00:19,  3.63it/s]Loading train:  74%|███████▍  | 197/266 [00:57<00:19,  3.61it/s]Loading train:  74%|███████▍  | 198/266 [00:57<00:18,  3.66it/s]Loading train:  75%|███████▍  | 199/266 [00:57<00:18,  3.67it/s]Loading train:  75%|███████▌  | 200/266 [00:58<00:18,  3.60it/s]Loading train:  76%|███████▌  | 201/266 [00:58<00:18,  3.54it/s]Loading train:  76%|███████▌  | 202/266 [00:58<00:18,  3.54it/s]Loading train:  76%|███████▋  | 203/266 [00:59<00:17,  3.58it/s]Loading train:  77%|███████▋  | 204/266 [00:59<00:17,  3.62it/s]Loading train:  77%|███████▋  | 205/266 [00:59<00:16,  3.65it/s]Loading train:  77%|███████▋  | 206/266 [00:59<00:16,  3.67it/s]Loading train:  78%|███████▊  | 207/266 [01:00<00:16,  3.68it/s]Loading train:  78%|███████▊  | 208/266 [01:00<00:15,  3.70it/s]Loading train:  79%|███████▊  | 209/266 [01:00<00:15,  3.69it/s]Loading train:  79%|███████▉  | 210/266 [01:00<00:15,  3.71it/s]Loading train:  79%|███████▉  | 211/266 [01:01<00:14,  3.73it/s]Loading train:  80%|███████▉  | 212/266 [01:01<00:14,  3.73it/s]Loading train:  80%|████████  | 213/266 [01:01<00:13,  3.82it/s]Loading train:  80%|████████  | 214/266 [01:01<00:13,  3.82it/s]Loading train:  81%|████████  | 215/266 [01:02<00:13,  3.80it/s]Loading train:  81%|████████  | 216/266 [01:02<00:13,  3.78it/s]Loading train:  82%|████████▏ | 217/266 [01:02<00:13,  3.75it/s]Loading train:  82%|████████▏ | 218/266 [01:03<00:12,  3.78it/s]Loading train:  82%|████████▏ | 219/266 [01:03<00:12,  3.75it/s]Loading train:  83%|████████▎ | 220/266 [01:03<00:12,  3.81it/s]Loading train:  83%|████████▎ | 221/266 [01:03<00:11,  3.86it/s]Loading train:  83%|████████▎ | 222/266 [01:04<00:11,  3.91it/s]Loading train:  84%|████████▍ | 223/266 [01:04<00:10,  3.95it/s]Loading train:  84%|████████▍ | 224/266 [01:04<00:10,  3.98it/s]Loading train:  85%|████████▍ | 225/266 [01:04<00:10,  3.98it/s]Loading train:  85%|████████▍ | 226/266 [01:05<00:09,  4.02it/s]Loading train:  85%|████████▌ | 227/266 [01:05<00:09,  4.03it/s]Loading train:  86%|████████▌ | 228/266 [01:05<00:09,  4.02it/s]Loading train:  86%|████████▌ | 229/266 [01:05<00:09,  3.92it/s]Loading train:  86%|████████▋ | 230/266 [01:06<00:09,  3.90it/s]Loading train:  87%|████████▋ | 231/266 [01:06<00:09,  3.77it/s]Loading train:  87%|████████▋ | 232/266 [01:06<00:09,  3.74it/s]Loading train:  88%|████████▊ | 233/266 [01:06<00:08,  3.69it/s]Loading train:  88%|████████▊ | 234/266 [01:07<00:08,  3.65it/s]Loading train:  88%|████████▊ | 235/266 [01:07<00:08,  3.62it/s]Loading train:  89%|████████▊ | 236/266 [01:07<00:08,  3.65it/s]Loading train:  89%|████████▉ | 237/266 [01:07<00:07,  3.66it/s]Loading train:  89%|████████▉ | 238/266 [01:08<00:07,  3.67it/s]Loading train:  90%|████████▉ | 239/266 [01:08<00:07,  3.67it/s]Loading train:  90%|█████████ | 240/266 [01:08<00:07,  3.67it/s]Loading train:  91%|█████████ | 241/266 [01:09<00:06,  3.67it/s]Loading train:  91%|█████████ | 242/266 [01:09<00:06,  3.69it/s]Loading train:  91%|█████████▏| 243/266 [01:09<00:06,  3.74it/s]Loading train:  92%|█████████▏| 244/266 [01:09<00:05,  3.78it/s]Loading train:  92%|█████████▏| 245/266 [01:10<00:05,  3.79it/s]Loading train:  92%|█████████▏| 246/266 [01:10<00:05,  3.81it/s]Loading train:  93%|█████████▎| 247/266 [01:10<00:04,  3.83it/s]Loading train:  93%|█████████▎| 248/266 [01:10<00:04,  3.83it/s]Loading train:  94%|█████████▎| 249/266 [01:11<00:04,  3.67it/s]Loading train:  94%|█████████▍| 250/266 [01:11<00:04,  3.56it/s]Loading train:  94%|█████████▍| 251/266 [01:11<00:04,  3.49it/s]Loading train:  95%|█████████▍| 252/266 [01:12<00:04,  3.43it/s]Loading train:  95%|█████████▌| 253/266 [01:12<00:03,  3.38it/s]Loading train:  95%|█████████▌| 254/266 [01:12<00:03,  3.37it/s]Loading train:  96%|█████████▌| 255/266 [01:13<00:03,  3.35it/s]Loading train:  96%|█████████▌| 256/266 [01:13<00:02,  3.35it/s]Loading train:  97%|█████████▋| 257/266 [01:13<00:02,  3.35it/s]Loading train:  97%|█████████▋| 258/266 [01:13<00:02,  3.32it/s]Loading train:  97%|█████████▋| 259/266 [01:14<00:02,  3.32it/s]Loading train:  98%|█████████▊| 260/266 [01:14<00:01,  3.22it/s]Loading train:  98%|█████████▊| 261/266 [01:14<00:01,  3.26it/s]Loading train:  98%|█████████▊| 262/266 [01:15<00:01,  3.29it/s]Loading train:  99%|█████████▉| 263/266 [01:15<00:00,  3.28it/s]Loading train:  99%|█████████▉| 264/266 [01:15<00:00,  3.28it/s]Loading train: 100%|█████████▉| 265/266 [01:16<00:00,  3.29it/s]Loading train: 100%|██████████| 266/266 [01:16<00:00,  3.32it/s]Loading train: 100%|██████████| 266/266 [01:16<00:00,  3.48it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:06, 42.13it/s]concatenating: train:   4%|▍         | 10/266 [00:00<00:06, 41.75it/s]concatenating: train:   5%|▌         | 14/266 [00:00<00:06, 40.95it/s]concatenating: train:   7%|▋         | 18/266 [00:00<00:06, 40.34it/s]concatenating: train:   9%|▊         | 23/266 [00:00<00:06, 40.48it/s]concatenating: train:  11%|█         | 28/266 [00:00<00:05, 40.88it/s]concatenating: train:  12%|█▏        | 33/266 [00:00<00:05, 41.79it/s]concatenating: train:  14%|█▍        | 38/266 [00:00<00:05, 41.63it/s]concatenating: train:  16%|█▌        | 43/266 [00:01<00:05, 42.75it/s]concatenating: train:  18%|█▊        | 48/266 [00:01<00:05, 43.22it/s]concatenating: train:  20%|█▉        | 53/266 [00:01<00:04, 44.20it/s]concatenating: train:  22%|██▏       | 58/266 [00:01<00:04, 43.81it/s]concatenating: train:  24%|██▎       | 63/266 [00:01<00:04, 45.17it/s]concatenating: train:  26%|██▌       | 68/266 [00:01<00:04, 46.35it/s]concatenating: train:  27%|██▋       | 73/266 [00:01<00:04, 47.36it/s]concatenating: train:  29%|██▉       | 78/266 [00:01<00:04, 46.24it/s]concatenating: train:  31%|███       | 83/266 [00:01<00:03, 46.64it/s]concatenating: train:  33%|███▎      | 88/266 [00:02<00:04, 44.26it/s]concatenating: train:  35%|███▍      | 93/266 [00:02<00:04, 42.93it/s]concatenating: train:  37%|███▋      | 98/266 [00:02<00:03, 43.40it/s]concatenating: train:  39%|███▉      | 104/266 [00:02<00:03, 45.43it/s]concatenating: train:  41%|████      | 109/266 [00:02<00:03, 46.53it/s]concatenating: train:  43%|████▎     | 114/266 [00:02<00:03, 46.49it/s]concatenating: train:  45%|████▍     | 119/266 [00:02<00:03, 46.23it/s]concatenating: train:  47%|████▋     | 124/266 [00:02<00:03, 44.98it/s]concatenating: train:  48%|████▊     | 129/266 [00:02<00:03, 42.96it/s]concatenating: train:  50%|█████     | 134/266 [00:03<00:03, 42.39it/s]concatenating: train:  52%|█████▏    | 139/266 [00:03<00:02, 44.02it/s]concatenating: train:  55%|█████▍    | 145/266 [00:03<00:02, 46.22it/s]concatenating: train:  56%|█████▋    | 150/266 [00:03<00:02, 45.09it/s]concatenating: train:  58%|█████▊    | 155/266 [00:03<00:02, 44.47it/s]concatenating: train:  60%|██████    | 160/266 [00:03<00:02, 45.36it/s]concatenating: train:  62%|██████▏   | 165/266 [00:03<00:02, 44.72it/s]concatenating: train:  64%|██████▍   | 170/266 [00:03<00:02, 45.15it/s]concatenating: train:  66%|██████▌   | 175/266 [00:03<00:01, 45.73it/s]concatenating: train:  68%|██████▊   | 180/266 [00:04<00:01, 45.71it/s]concatenating: train:  70%|██████▉   | 185/266 [00:04<00:01, 45.59it/s]concatenating: train:  71%|███████▏  | 190/266 [00:04<00:01, 45.46it/s]concatenating: train:  73%|███████▎  | 195/266 [00:04<00:01, 45.41it/s]concatenating: train:  75%|███████▌  | 200/266 [00:04<00:01, 45.85it/s]concatenating: train:  77%|███████▋  | 205/266 [00:04<00:01, 45.87it/s]concatenating: train:  79%|███████▉  | 210/266 [00:04<00:01, 44.93it/s]concatenating: train:  81%|████████  | 215/266 [00:04<00:01, 45.68it/s]concatenating: train:  83%|████████▎ | 220/266 [00:04<00:00, 46.58it/s]concatenating: train:  85%|████████▍ | 225/266 [00:05<00:00, 47.31it/s]concatenating: train:  86%|████████▋ | 230/266 [00:05<00:00, 48.07it/s]concatenating: train:  88%|████████▊ | 235/266 [00:05<00:00, 48.01it/s]concatenating: train:  90%|█████████ | 240/266 [00:05<00:00, 47.23it/s]concatenating: train:  92%|█████████▏| 246/266 [00:05<00:00, 49.23it/s]concatenating: train:  95%|█████████▍| 252/266 [00:05<00:00, 50.99it/s]concatenating: train:  97%|█████████▋| 258/266 [00:05<00:00, 51.63it/s]concatenating: train:  99%|█████████▉| 264/266 [00:05<00:00, 52.10it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 45.69it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:00,  3.43it/s]Loading test:  50%|█████     | 2/4 [00:00<00:00,  3.53it/s]Loading test:  75%|███████▌  | 3/4 [00:00<00:00,  3.66it/s]Loading test: 100%|██████████| 4/4 [00:01<00:00,  3.60it/s]Loading test: 100%|██████████| 4/4 [00:01<00:00,  3.65it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 368.13it/s]
Epoch 00046: val_mDice did not improve from 0.30739
Restoring model weights from the end of the best epoch
Epoch 00046: early stopping
{'val_loss': [0.6694621224152414, 0.6747075287919295, 0.6651148843137842, 0.6499445199182159, 0.6608658009453824, 0.6271879378902284, 0.6409602114244511, 0.5948137695852079, 0.6320700394479852, 0.44362938732497004, 0.4855177763261293, 0.36981341497678505, 0.29896077876420396, 0.17017278702635513, 0.2985198517685364, 0.2560159283150968, 0.35459677732892725, 0.21255123470664808, 0.17405991947376415, 0.08363299769055295, 0.20345078112165393, 0.09150057347295315, 0.11683270342549995, 0.05466711035880603, 0.09612406352779974, 0.06427797341817304, 0.06494621775652233, 0.1388616063938427, 0.06767190745296447, 0.23299111798405647, 0.07894799446589068, 0.08317383626892574, 0.056448740296457946, 0.01906512618849152, 0.03864339964562341, 0.06326661712342971, 0.0285347701344443, 0.021272862477130013, 0.02807101014217264, 0.020551769553046478, -0.006594573980883548, -0.01883037820899565, -0.002257333602756262, 0.0081377708794255, 0.0045409779995679855, -0.013205543117548683], 'val_acc': [0.9409099668264389, 0.946255361563281, 0.947064176986092, 0.9508823077929648, 0.945476656681613, 0.9529276952931756, 0.9518281888020667, 0.9506051736442667, 0.9505840908539923, 0.9532726038443414, 0.9549263802013899, 0.9512934951405776, 0.9524924041409242, 0.9531490959619221, 0.9556192238079874, 0.95193061310994, 0.9541717822614469, 0.9483248371826974, 0.9537289801396822, 0.9545302493007559, 0.9505840932068071, 0.9516805888790834, 0.9525978408361736, 0.9518778982915377, 0.952433672390486, 0.9545784576943046, 0.9537018702218407, 0.9554520304265776, 0.9554776368956817, 0.9510118467243094, 0.950621748441144, 0.9546130966199072, 0.9548209560544867, 0.9548962633860739, 0.9524140812848744, 0.9518522839797171, 0.954144677833507, 0.9535919122005764, 0.9530451760480279, 0.9540211597555562, 0.9541732966899872, 0.9548540797672773, 0.9528373197505349, 0.9536913249053454, 0.9552170673483297, 0.9522996191915712], 'val_mDice': [0.2770554766941227, 0.27112301929216637, 0.2788654410917508, 0.2941735030004853, 0.28448897364892456, 0.3073921348703535, 0.30278684179249565, 0.2994639377452825, 0.2895707406691815, 0.30049154632969904, 0.29499355850643233, 0.29795659902064425, 0.28486772685458783, 0.3010863614709754, 0.29567133439214605, 0.2922620315496859, 0.30462815622357947, 0.29062469882008274, 0.2998665132020649, 0.29952004159751694, 0.2876054225979667, 0.29423235327397523, 0.29641822911798954, 0.29436887094849035, 0.2988235461280534, 0.30317239071193497, 0.3022782617297612, 0.3054936377233581, 0.29802953922434855, 0.27795706189384584, 0.29386924726790503, 0.3020135484831898, 0.30174385805271176, 0.30038970228480666, 0.2976812015434629, 0.29027606066512435, 0.3055262957748614, 0.3025839678747089, 0.3025629013580711, 0.29988147416397143, 0.30422835218671124, 0.3067067971355037, 0.30044494784976306, 0.3024692220711394, 0.3031286969780922, 0.29930378163331434], 'loss': [0.5586029345821965, 0.38758754567131587, 0.35500006130629624, 0.3347088561213197, 0.3213456143232879, 0.3126115352372363, 0.3018538403441354, 0.2916277911651859, 0.2882115019648519, 0.28136004075673615, 0.28344690443250625, 0.27553995413206217, 0.27234227220137414, 0.2684059401073286, 0.2633981244852425, 0.26597573590879586, 0.26680655089664534, 0.25813864541391957, 0.2545012240639952, 0.2549650598148151, 0.2518438530255031, 0.24083927222337442, 0.2364579781715683, 0.23885711633955722, 0.23598503088480396, 0.23193422889878565, 0.23371622473349946, 0.2332263887728314, 0.23171204867800785, 0.22838756905642646, 0.2308840790493223, 0.22901846183392882, 0.2292784422076203, 0.23107432323110408, 0.22494392036157754, 0.22579695850764972, 0.22212987019683475, 0.21606467258365808, 0.21798437257676329, 0.21542973047201264, 0.2167135905278056, 0.2154342907386103, 0.21368552443052083, 0.2131819230444888, 0.21427636147002413, 0.21511009055032157], 'acc': [0.9070475949147652, 0.9414740026122838, 0.9459159738125873, 0.9485234596707799, 0.9497302862243769, 0.9509506067462824, 0.9520892400907927, 0.9533374041964411, 0.9536254532793735, 0.9543170935010371, 0.9542253999753956, 0.9552946529307081, 0.9555117859793779, 0.9558912429723654, 0.9563831712108749, 0.9562891299682992, 0.9563452124778443, 0.9568125450430598, 0.9571948538389566, 0.957340479788251, 0.9578422158299585, 0.9588639228901965, 0.9591486795505665, 0.9593470749759985, 0.9595071605071268, 0.9597821523226013, 0.9597514347370177, 0.9596046241439803, 0.9600278320087654, 0.9601119552989423, 0.9600726162219655, 0.9602508895443344, 0.9602720624006227, 0.9603096271283283, 0.9605815480192754, 0.9605803417821303, 0.9611443037590991, 0.9612812408547258, 0.9614119458660121, 0.9614777269778362, 0.9615370131370686, 0.9616295403165563, 0.9617250741202187, 0.9617927423468581, 0.9617944743776312, 0.9618032736661266], 'mDice': [0.39793182394949467, 0.582236987896205, 0.6173914205814907, 0.6392764648658598, 0.6537078555137096, 0.6631293790548817, 0.67474452169526, 0.6857729400680375, 0.689460454674683, 0.6968626374882575, 0.6945963273485753, 0.7031254380604351, 0.7065755537535602, 0.710826545830931, 0.7162385214179418, 0.7134459479252269, 0.7125444908879693, 0.7219293517743762, 0.7258481813763575, 0.7253387919760125, 0.7286941571975545, 0.7405744366189266, 0.7453048060012087, 0.7426921946348439, 0.7458091625450717, 0.7501856842750741, 0.7482531994255608, 0.748784881208691, 0.7504108552962901, 0.7540090004271741, 0.7513130811614873, 0.753311776871514, 0.7530268821790344, 0.7510794390943376, 0.7577099212632226, 0.756804592043407, 0.7607424323451869, 0.7673159170726581, 0.7652282289545129, 0.7679874434116039, 0.7665859586163821, 0.7679748549266224, 0.769858557898719, 0.7703851828498358, 0.7691928007684038, 0.7682649166345916], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 2  | SD 1  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 2  | SD 1  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 76, 108, 1)   0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 76, 108, 30)  300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 76, 108, 30)  120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 76, 108, 30)  0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 76, 108, 30)  8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 76, 108, 30)  120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 76, 108, 30)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 38, 54, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 38, 54, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 38, 54, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 38, 54, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 38, 54, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 38, 54, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 38, 54, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 38, 54, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 38, 54, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 19, 27, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 19, 27, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 19, 27, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 19, 27, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 19, 27, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 19, 27, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 19, 27, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 19, 27, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 19, 27, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 19, 27, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 38, 54, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 38, 54, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 38, 54, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 38, 54, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 38, 54, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 38, 54, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 38, 54, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 38, 54, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 38, 54, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 38, 54, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 76, 108, 30)  25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 76, 108, 60)  0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 76, 108, 30)  16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 76, 108, 30)  120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 76, 108, 30)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 76, 108, 30)  8130        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 76, 108, 30)  120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 76, 108, 30)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 76, 108, 90)  0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              2020-01-21 01:02:33.147026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 01:02:33.147120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 01:02:33.147133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 01:02:33.147140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 01:02:33.147421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)

__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 76, 108, 90)  0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 76, 108, 2)   182         dropout_5[0][0]                  
==================================================================================================
Total params: 500,342
Trainable params: 499,142
Non-trainable params: 1,200
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97453182 0.02546818]
Train on 27987 samples, validate on 396 samples
Epoch 1/300
 - 75s - loss: 0.0773 - acc: 0.9911 - mDice: 0.8511 - val_loss: 0.2235 - val_acc: 0.9936 - val_mDice: 0.5565

Epoch 00001: val_mDice improved from -inf to 0.55650, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 2/300
 - 70s - loss: 0.0520 - acc: 0.9942 - mDice: 0.8989 - val_loss: 0.2200 - val_acc: 0.9947 - val_mDice: 0.5619

Epoch 00002: val_mDice improved from 0.55650 to 0.56193, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd1/best_model_weights.h5
Epoch 3/300
 - 70s - loss: 0.0455 - acc: 0.9948 - mDice: 0.9117 - val_loss: 0.2129 - val_acc: 0.9944 - val_mDice: 0.5587

Epoch 00003: val_mDice did not improve from 0.56193
Epoch 4/300
 - 71s - loss: 0.0424 - acc: 0.9951 - mDice: 0.9176 - val_loss: 0.2205 - val_acc: 0.9948 - val_mDice: 0.5603

Epoch 00004: val_mDice did not improve from 0.56193
Epoch 5/300
 - 71s - loss: 0.0406 - acc: 0.9953 - mDice: 0.9211 - val_loss: 0.2339 - val_acc: 0.9943 - val_mDice: 0.5173

Epoch 00005: val_mDice did not improve from 0.56193
Epoch 6/300
 - 72s - loss: 0.0382 - acc: 0.9956 - mDice: 0.9258 - val_loss: 0.2177 - val_acc: 0.9944 - val_mDice: 0.5439

Epoch 00006: val_mDice did not improve from 0.56193
Epoch 7/300
 - 72s - loss: 0.0371 - acc: 0.9957 - mDice: 0.9281 - val_loss: 0.2119 - val_acc: 0.9946 - val_mDice: 0.5414

Epoch 00007: val_mDice did not improve from 0.56193
Epoch 8/300
 - 72s - loss: 0.0355 - acc: 0.9958 - mDice: 0.9310 - val_loss: 0.1864 - val_acc: 0.9948 - val_mDice: 0.5607

Epoch 00008: val_mDice did not improve from 0.56193
Epoch 9/300
 - 72s - loss: 0.0349 - acc: 0.9958 - mDice: 0.9323 - val_loss: 0.1134 - val_acc: 0.9946 - val_mDice: 0.5478

Epoch 00009: val_mDice did not improve from 0.56193
Epoch 10/300
 - 73s - loss: 0.0342 - acc: 0.9959 - mDice: 0.9335 - val_loss: 0.1585 - val_acc: 0.9945 - val_mDice: 0.5428

Epoch 00010: val_mDice did not improve from 0.56193
Epoch 11/300
 - 73s - loss: 0.0339 - acc: 0.9959 - mDice: 0.9342 - val_loss: 0.0826 - val_acc: 0.9946 - val_mDice: 0.5483

Epoch 00011: val_mDice did not improve from 0.56193
Epoch 12/300
 - 73s - loss: 0.0331 - acc: 0.9960 - mDice: 0.9357 - val_loss: 0.1062 - val_acc: 0.9949 - val_mDice: 0.5369

Epoch 00012: val_mDice did not improve from 0.56193
Epoch 13/300
 - 72s - loss: 0.0325 - acc: 0.9961 - mDice: 0.9369 - val_loss: -2.3661e-02 - val_acc: 0.9946 - val_mDice: 0.5497

Epoch 00013: val_mDice did not improve from 0.56193
Epoch 14/300
 - 72s - loss: 0.0320 - acc: 0.9961 - mDice: 0.9380 - val_loss: -1.6295e-02 - val_acc: 0.9947 - val_mDice: 0.5406

Epoch 00014: val_mDice did not improve from 0.56193
Epoch 15/300
 - 72s - loss: 0.0316 - acc: 0.9962 - mDice: 0.9386 - val_loss: 0.0611 - val_acc: 0.9948 - val_mDice: 0.5472

Epoch 00015: val_mDice did not improve from 0.56193
Epoch 16/300
 - 74s - loss: 0.0317 - acc: 0.9962 - mDice: 0.9385 - val_loss: -1.1405e-03 - val_acc: 0.9948 - val_mDice: 0.5536

Epoch 00016: val_mDice did not improve from 0.56193
Epoch 17/300
 - 74s - loss: 0.0322 - acc: 0.9962 - mDice: 0.9375 - val_loss: 0.0276 - val_acc: 0.9947 - val_mDice: 0.5444

Epoch 00017: val_mDice did not improve from 0.56193

Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 18/300
 - 74s - loss: 0.0291 - acc: 0.9964 - mDice: 0.9435 - val_loss: 0.0232 - val_acc: 0.9948 - val_mDice: 0.5457

Epoch 00018: val_mDice did not improve from 0.56193
Epoch 19/300
 - 74s - loss: 0.0295 - acc: 0.9963 - mDice: 0.9428 - val_loss: -2.4490e-02 - val_acc: 0.9948 - val_mDice: 0.5455

Epoch 00019: val_mDice did not improve from 0.56193
Epoch 20/300
 - 72s - loss: 0.0284 - acc: 0.9965 - mDice: 0.9448 - val_loss: -9.1914e-03 - val_acc: 0.9949 - val_mDice: 0.5449

Epoch 00020: val_mDice did not improve from 0.56193
Epoch 21/300
 - 72s - loss: 0.0284 - acc: 0.9965 - mDice: 0.9450 - val_loss: 0.0339 - val_acc: 0.9949 - val_mDice: 0.5416

Epoch 00021: val_mDice did not improve from 0.56193
Epoch 22/300
 - 72s - loss: 0.0281 - acc: 0.9965 - mDice: 0.9455 - val_loss: 0.0152 - val_acc: 0.9948 - val_mDice: 0.5487

Epoch 00022: val_mDice did not improve from 0.56193
Epoch 23/300
 - 71s - loss: 0.0280 - acc: 0.9965 - mDice: 0.9457 - val_loss: 0.1034 - val_acc: 0.9949 - val_mDice: 0.5450

Epoch 00023: val_mDice did not improve from 0.56193
Epoch 24/300
 - 73s - loss: 0.0277 - acc: 0.9965 - mDice: 0.9463 - val_loss: 0.0166 - val_acc: 0.9950 - val_mDice: 0.5471

Epoch 00024: val_mDice did not improve from 0.56193
Epoch 25/300
 - 73s - loss: 0.0276 - acc: 0.9965 - mDice: 0.9465 - val_loss: -4.7217e-02 - val_acc: 0.9946 - val_mDice: 0.5422

Epoch 00025: val_mDice did not improve from 0.56193
Epoch 26/300
 - 73s - loss: 0.0275 - acc: 0.9965 - mDice: 0.9467 - val_loss: -8.2234e-03 - val_acc: 0.9949 - val_mDice: 0.5503

Epoch 00026: val_mDice did not improve from 0.56193
Epoch 27/300
 - 72s - loss: 0.0278 - acc: 0.9965 - mDice: 0.9460 - val_loss: 0.0232 - val_acc: 0.9949 - val_mDice: 0.5498

Epoch 00027: val_mDice did not improve from 0.56193
Epoch 28/300
 - 73s - loss: 0.0276 - acc: 0.9965 - mDice: 0.9466 - val_loss: -6.0786e-02 - val_acc: 0.9947 - val_mDice: 0.5473

Epoch 00028: val_mDice did not improve from 0.56193
Epoch 29/300
 - 74s - loss: 0.0270 - acc: 0.9965 - mDice: 0.9476 - val_loss: 0.0423 - val_acc: 0.9947 - val_mDice: 0.5512

Epoch 00029: val_mDice did not improve from 0.56193
Epoch 30/300
 - 74s - loss: 0.0270 - acc: 0.9966 - mDice: 0.9476 - val_loss: -2.2510e-02 - val_acc: 0.9947 - val_mDice: 0.5395

Epoch 00030: val_mDice did not improve from 0.56193
Epoch 31/300
 - 75s - loss: 0.0267 - acc: 0.9966 - mDice: 0.9482 - val_loss: 0.0198 - val_acc: 0.9948 - val_mDice: 0.5369

Epoch 00031: val_mDice did not improve from 0.56193
Epoch 32/300
 - 75s - loss: 0.0266 - acc: 0.9966 - mDice: 0.9484 - val_loss: 0.0693 - val_acc: 0.9949 - val_mDice: 0.5450

Epoch 00032: val_mDice did not improve from 0.56193

Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 33/300
 - 74s - loss: 0.0260 - acc: 0.9966 - mDice: 0.9497 - val_loss: -2.4317e-02 - val_acc: 0.9949 - val_mDice: 0.5422

Epoch 00033: val_mDice did not improve from 0.56193
Epoch 34/300
 - 74s - loss: 0.0257 - acc: 0.9967 - mDice: 0.9501 - val_loss: 0.0059 - val_acc: 0.9948 - val_mDice: 0.5475

Epoch 00034: val_mDice did not improve from 0.56193
Epoch 35/300
 - 72s - loss: 0.0255 - acc: 0.9967 - mDice: 0.9505 - val_loss: -2.0963e-03 - val_acc: 0.9949 - val_mDice: 0.5390

Epoch 00035: val_mDice did not improve from 0.56193
Epoch 36/300
 - 72s - loss: 0.0254 - acc: 0.9967 - mDice: 0.9507 - val_loss: -2.3855e-02 - val_acc: 0.9949 - val_mDice: 0.5476

Epoch 00036: val_mDice did not improve from 0.56193
Epoch 37/300
 - 72s - loss: 0.0255 - acc: 0.9967 - mDice: 0.9505 - val_loss: -1.7092e-02 - val_acc: 0.9949 - val_mDice: 0.5468

Epoch 00037: val_mDice did not improve from 0.56193
Epoch 38/300
 - 73s - loss: 0.0252 - acc: 0.9967 - mDice: 0.9512 - val_loss: 0.0048 - val_acc: 0.9949 - val_mDice: 0.5478

Epoch 00038: val_mDice did not improve from 0.56193
Epoch 39/300
 - 72s - loss: 0.0254 - acc: 0.9967 - mDice: 0.9509 - val_loss: 0.0387 - val_acc: 0.9948 - val_mDice: 0.5462

Epoch 00039: val_mDice did not improve from 0.56193
Epoch 40/300
 - 72s - loss: 0.0251 - acc: 0.9967 - mDice: 0.9513 - val_loss: 0.0036 - val_acc: 0.9948 - val_mDice: 0.5456

Epoch 00040: val_mDice did not improve from 0.56193
Epoch 41/300
 - 71s - loss: 0.0254 - acc: 0.9967 - mDice: 0.9507 - val_loss: -2.0539e-02 - val_acc: 0.9950 - val_mDice: 0.5491

Epoch 00041: val_mDice did not improve from 0.56193
Epoch 42/300
 - 72s - loss: 0.0250 - acc: 0.9967 - mDice: 0.9515 - val_loss: 0.0183 - val_acc: 0.9949 - val_mDice: 0.5436

predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:00<00:02,  1.45it/s]predicting test subjects:  50%|█████     | 2/4 [00:00<00:01,  1.87it/s]predicting test subjects:  75%|███████▌  | 3/4 [00:01<00:00,  2.28it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  2.81it/s]predicting test subjects: 100%|██████████| 4/4 [00:01<00:00,  3.22it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<01:07,  3.92it/s]predicting train subjects:   1%|          | 2/266 [00:00<01:04,  4.07it/s]predicting train subjects:   1%|          | 3/266 [00:00<00:57,  4.61it/s]predicting train subjects:   2%|▏         | 4/266 [00:00<00:53,  4.88it/s]predicting train subjects:   2%|▏         | 5/266 [00:01<00:56,  4.66it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<00:53,  4.88it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<00:50,  5.10it/s]predicting train subjects:   3%|▎         | 8/266 [00:01<00:49,  5.22it/s]predicting train subjects:   3%|▎         | 9/266 [00:01<00:48,  5.33it/s]predicting train subjects:   4%|▍         | 10/266 [00:01<00:46,  5.46it/s]predicting train subjects:   4%|▍         | 11/266 [00:02<00:46,  5.52it/s]predicting train subjects:   5%|▍         | 12/266 [00:02<00:45,  5.54it/s]predicting train subjects:   5%|▍         | 13/266 [00:02<00:45,  5.58it/s]predicting train subjects:   5%|▌         | 14/266 [00:02<00:45,  5.60it/s]predicting train subjects:   6%|▌         | 15/266 [00:02<00:44,  5.61it/s]predicting train subjects:   6%|▌         | 16/266 [00:02<00:44,  5.63it/s]predicting train subjects:   6%|▋         | 17/266 [00:03<00:44,  5.63it/s]predicting train subjects:   7%|▋         | 18/266 [00:03<00:44,  5.59it/s]predicting train subjects:   7%|▋         | 19/266 [00:03<00:43,  5.62it/s]predicting train subjects:   8%|▊         | 20/266 [00:03<00:43,  5.64it/s]predicting train subjects:   8%|▊         | 21/266 [00:03<00:43,  5.67it/s]predicting train subjects:   8%|▊         | 22/266 [00:04<00:42,  5.69it/s]predicting train subjects:   9%|▊         | 23/266 [00:04<00:42,  5.67it/s]predicting train subjects:   9%|▉         | 24/266 [00:04<00:42,  5.72it/s]predicting train subjects:   9%|▉         | 25/266 [00:04<00:42,  5.73it/s]predicting train subjects:  10%|▉         | 26/266 [00:04<00:42,  5.69it/s]predicting train subjects:  10%|█         | 27/266 [00:04<00:42,  5.66it/s]predicting train subjects:  11%|█         | 28/266 [00:05<00:42,  5.64it/s]predicting train subjects:  11%|█         | 29/266 [00:05<00:41,  5.68it/s]predicting train subjects:  11%|█▏        | 30/266 [00:05<00:41,  5.72it/s]predicting train subjects:  12%|█▏        | 31/266 [00:05<00:40,  5.76it/s]predicting train subjects:  12%|█▏        | 32/266 [00:05<00:40,  5.75it/s]predicting train subjects:  12%|█▏        | 33/266 [00:05<00:40,  5.69it/s]predicting train subjects:  13%|█▎        | 34/266 [00:06<00:40,  5.75it/s]predicting train subjects:  13%|█▎        | 35/266 [00:06<00:39,  5.80it/s]predicting train subjects:  14%|█▎        | 36/266 [00:06<00:39,  5.82it/s]predicting train subjects:  14%|█▍        | 37/266 [00:06<00:39,  5.82it/s]predicting train subjects:  14%|█▍        | 38/266 [00:06<00:39,  5.84it/s]predicting train subjects:  15%|█▍        | 39/266 [00:07<00:38,  5.84it/s]predicting train subjects:  15%|█▌        | 40/266 [00:07<00:38,  5.81it/s]predicting train subjects:  15%|█▌        | 41/266 [00:07<00:38,  5.84it/s]predicting train subjects:  16%|█▌        | 42/266 [00:07<00:37,  6.01it/s]predicting train subjects:  16%|█▌        | 43/266 [00:07<00:36,  6.11it/s]predicting train subjects:  17%|█▋        | 44/266 [00:07<00:35,  6.25it/s]predicting train subjects:  17%|█▋        | 45/266 [00:07<00:34,  6.36it/s]predicting train subjects:  17%|█▋        | 46/266 [00:08<00:34,  6.45it/s]predicting train subjects:  18%|█▊        | 47/266 [00:08<00:33,  6.52it/s]predicting train subjects:  18%|█▊        | 48/266 [00:08<00:33,  6.55it/s]predicting train subjects:  18%|█▊        | 49/266 [00:08<00:33,  6.51it/s]predicting train subjects:  19%|█▉        | 50/266 [00:08<00:33,  6.41it/s]predicting train subjects:  19%|█▉        | 51/266 [00:08<00:33,  6.49it/s]predicting train subjects:  20%|█▉        | 52/266 [00:09<00:32,  6.57it/s]predicting train subjects:  20%|█▉        | 53/266 [00:09<00:32,  6.63it/s]predicting train subjects:  20%|██        | 54/266 [00:09<00:31,  6.64it/s]predicting train subjects:  21%|██        | 55/266 [00:09<00:32,  6.51it/s]predicting train subjects:  21%|██        | 56/266 [00:09<00:32,  6.52it/s]predicting train subjects:  21%|██▏       | 57/266 [00:09<00:31,  6.55it/s]predicting train subjects:  22%|██▏       | 58/266 [00:09<00:31,  6.59it/s]predicting train subjects:  22%|██▏       | 59/266 [00:10<00:32,  6.45it/s]predicting train subjects:  23%|██▎       | 60/266 [00:10<00:31,  6.58it/s]predicting train subjects:  23%|██▎       | 61/266 [00:10<00:30,  6.62it/s]predicting train subjects:  23%|██▎       | 62/266 [00:10<00:30,  6.67it/s]predicting train subjects:  24%|██▎       | 63/266 [00:10<00:29,  6.78it/s]predicting train subjects:  24%|██▍       | 64/266 [00:10<00:29,  6.86it/s]predicting train subjects:  24%|██▍       | 65/266 [00:10<00:28,  6.95it/s]predicting train subjects:  25%|██▍       | 66/266 [00:11<00:28,  6.98it/s]predicting train subjects:  25%|██▌       | 67/266 [00:11<00:28,  6.96it/s]predicting train subjects:  26%|██▌       | 68/266 [00:11<00:28,  6.91it/s]predicting train subjects:  26%|██▌       | 69/266 [00:11<00:28,  6.92it/s]predicting train subjects:  26%|██▋       | 70/266 [00:11<00:28,  6.96it/s]predicting train subjects:  27%|██▋       | 71/266 [00:11<00:27,  7.05it/s]predicting train subjects:  27%|██▋       | 72/266 [00:11<00:28,  6.77it/s]predicting train subjects:  27%|██▋       | 73/266 [00:12<00:28,  6.86it/s]predicting train subjects:  28%|██▊       | 74/266 [00:12<00:27,  6.95it/s]predicting train subjects:  28%|██▊       | 75/266 [00:12<00:27,  6.98it/s]predicting train subjects:  29%|██▊       | 76/266 [00:12<00:27,  6.96it/s]predicting train subjects:  29%|██▉       | 77/266 [00:12<00:27,  6.96it/s]predicting train subjects:  29%|██▉       | 78/266 [00:12<00:28,  6.52it/s]predicting train subjects:  30%|██▉       | 79/266 [00:13<00:30,  6.20it/s]predicting train subjects:  30%|███       | 80/266 [00:13<00:30,  6.10it/s]predicting train subjects:  30%|███       | 81/266 [00:13<00:33,  5.60it/s]predicting train subjects:  31%|███       | 82/266 [00:13<00:32,  5.72it/s]predicting train subjects:  31%|███       | 83/266 [00:13<00:31,  5.81it/s]predicting train subjects:  32%|███▏      | 84/266 [00:13<00:31,  5.77it/s]predicting train subjects:  32%|███▏      | 85/266 [00:14<00:31,  5.76it/s]predicting train subjects:  32%|███▏      | 86/266 [00:14<00:31,  5.68it/s]predicting train subjects:  33%|███▎      | 87/266 [00:14<00:33,  5.42it/s]predicting train subjects:  33%|███▎      | 88/266 [00:14<00:32,  5.47it/s]predicting train subjects:  33%|███▎      | 89/266 [00:14<00:32,  5.41it/s]predicting train subjects:  34%|███▍      | 90/266 [00:15<00:32,  5.49it/s]predicting train subjects:  34%|███▍      | 91/266 [00:15<00:31,  5.56it/s]predicting train subjects:  35%|███▍      | 92/266 [00:15<00:30,  5.63it/s]predicting train subjects:  35%|███▍      | 93/266 [00:15<00:30,  5.64it/s]predicting train subjects:  35%|███▌      | 94/266 [00:15<00:30,  5.65it/s]predicting train subjects:  36%|███▌      | 95/266 [00:15<00:30,  5.65it/s]predicting train subjects:  36%|███▌      | 96/266 [00:16<00:31,  5.35it/s]predicting train subjects:  36%|███▋      | 97/266 [00:16<00:33,  5.01it/s]predicting train subjects:  37%|███▋      | 98/266 [00:16<00:34,  4.90it/s]predicting train subjects:  37%|███▋      | 99/266 [00:16<00:31,  5.35it/s]predicting train subjects:  38%|███▊      | 100/266 [00:16<00:30,  5.37it/s]predicting train subjects:  38%|███▊      | 101/266 [00:17<00:29,  5.64it/s]predicting train subjects:  38%|███▊      | 102/266 [00:17<00:28,  5.78it/s]predicting train subjects:  39%|███▊      | 103/266 [00:17<00:27,  5.97it/s]predicting train subjects:  39%|███▉      | 104/266 [00:17<00:26,  6.10it/s]predicting train subjects:  39%|███▉      | 105/266 [00:17<00:25,  6.21it/s]predicting train subjects:  40%|███▉      | 106/266 [00:17<00:25,  6.27it/s]predicting train subjects:  40%|████      | 107/266 [00:18<00:25,  6.25it/s]predicting train subjects:  41%|████      | 108/266 [00:18<00:25,  6.29it/s]predicting train subjects:  41%|████      | 109/266 [00:18<00:24,  6.32it/s]predicting train subjects:  41%|████▏     | 110/266 [00:18<00:24,  6.29it/s]predicting train subjects:  42%|████▏     | 111/266 [00:18<00:24,  6.25it/s]predicting train subjects:  42%|████▏     | 112/266 [00:18<00:24,  6.24it/s]predicting train subjects:  42%|████▏     | 113/266 [00:18<00:24,  6.26it/s]predicting train subjects:  43%|████▎     | 114/266 [00:19<00:24,  6.25it/s]predicting train subjects:  43%|████▎     | 115/266 [00:19<00:24,  6.16it/s]predicting train subjects:  44%|████▎     | 116/266 [00:19<00:24,  6.22it/s]predicting train subjects:  44%|████▍     | 117/266 [00:19<00:23,  6.24it/s]predicting train subjects:  44%|████▍     | 118/266 [00:19<00:23,  6.27it/s]predicting train subjects:  45%|████▍     | 119/266 [00:19<00:24,  6.01it/s]predicting train subjects:  45%|████▌     | 120/266 [00:20<00:24,  5.95it/s]predicting train subjects:  45%|████▌     | 121/266 [00:20<00:24,  5.90it/s]predicting train subjects:  46%|████▌     | 122/266 [00:20<00:24,  5.83it/s]predicting train subjects:  46%|████▌     | 123/266 [00:20<00:25,  5.70it/s]predicting train subjects:  47%|████▋     | 124/266 [00:20<00:24,  5.76it/s]predicting train subjects:  47%|████▋     | 125/266 [00:20<00:24,  5.84it/s]predicting train subjects:  47%|████▋     | 126/266 [00:21<00:23,  5.85it/s]predicting train subjects:  48%|████▊     | 127/266 [00:21<00:24,  5.79it/s]predicting train subjects:  48%|████▊     | 128/266 [00:21<00:23,  5.82it/s]predicting train subjects:  48%|████▊     | 129/266 [00:21<00:23,  5.80it/s]predicting train subjects:  49%|████▉     | 130/266 [00:21<00:23,  5.85it/s]predicting train subjects:  49%|████▉     | 131/266 [00:22<00:23,  5.84it/s]predicting train subjects:  50%|████▉     | 132/266 [00:22<00:23,  5.71it/s]predicting train subjects:  50%|█████     | 133/266 [00:22<00:23,  5.68it/s]predicting train subjects:  50%|█████     | 134/266 [00:22<00:22,  5.75it/s]predicting train subjects:  51%|█████     | 135/266 [00:22<00:23,  5.61it/s]predicting train subjects:  51%|█████     | 136/266 [00:22<00:23,  5.64it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:23<00:22,  5.69it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:23<00:21,  5.86it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:23<00:21,  5.97it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:23<00:20,  6.02it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:23<00:20,  6.09it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:23<00:20,  6.01it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:24<00:20,  6.05it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:24<00:20,  6.03it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:24<00:20,  6.02it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:24<00:19,  6.04it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:24<00:19,  6.13it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:24<00:19,  6.11it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:25<00:19,  6.06it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:25<00:19,  6.04it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:25<00:18,  6.09it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:25<00:18,  6.08it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:25<00:18,  6.12it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:25<00:18,  6.12it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:26<00:17,  6.32it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:26<00:16,  6.59it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:26<00:16,  6.80it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:26<00:15,  6.93it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:26<00:15,  7.08it/s]predicting train subjects:  60%|██████    | 160/266 [00:26<00:15,  7.06it/s]predicting train subjects:  61%|██████    | 161/266 [00:26<00:14,  7.08it/s]predicting train subjects:  61%|██████    | 162/266 [00:26<00:14,  7.13it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:27<00:14,  7.04it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:27<00:14,  7.02it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:27<00:14,  6.94it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:27<00:14,  7.02it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:27<00:14,  7.05it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:27<00:14,  6.65it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:28<00:14,  6.86it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:28<00:13,  6.98it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:28<00:13,  7.10it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:28<00:13,  7.10it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:28<00:13,  6.97it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:28<00:13,  6.86it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:28<00:13,  6.77it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:29<00:13,  6.75it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:29<00:13,  6.67it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:29<00:13,  6.71it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:29<00:12,  6.76it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:29<00:12,  6.73it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:29<00:12,  6.75it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:29<00:12,  6.80it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:30<00:12,  6.81it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:30<00:12,  6.77it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:30<00:11,  6.79it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:30<00:11,  6.78it/s]predicting train subjects:  70%|███████   | 187/266 [00:30<00:11,  6.76it/s]predicting train subjects:  71%|███████   | 188/266 [00:30<00:15,  5.19it/s]predicting train subjects:  71%|███████   | 189/266 [00:31<00:13,  5.59it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:31<00:12,  5.92it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:31<00:12,  5.80it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:31<00:13,  5.37it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:31<00:12,  5.77it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:32<00:13,  5.34it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:32<00:12,  5.68it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:32<00:11,  5.99it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:32<00:11,  6.17it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:32<00:10,  6.35it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:32<00:10,  6.46it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:32<00:10,  6.59it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:33<00:09,  6.64it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:33<00:09,  6.68it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:33<00:09,  6.73it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:33<00:09,  6.73it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:33<00:09,  6.76it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:33<00:08,  6.74it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:33<00:08,  6.72it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:34<00:08,  6.69it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:34<00:08,  6.65it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:34<00:08,  6.59it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:34<00:08,  6.65it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:34<00:08,  6.68it/s]predicting train subjects:  80%|████████  | 213/266 [00:34<00:07,  6.88it/s]predicting train subjects:  80%|████████  | 214/266 [00:34<00:07,  7.01it/s]predicting train subjects:  81%|████████  | 215/266 [00:35<00:07,  7.17it/s]predicting train subjects:  81%|████████  | 216/266 [00:35<00:06,  7.20it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:35<00:06,  7.31it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:35<00:06,  7.38it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:35<00:06,  7.42it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:35<00:06,  7.43it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:35<00:06,  7.44it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:36<00:05,  7.44it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:36<00:05,  7.47it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:36<00:05,  7.47it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:36<00:05,  7.44it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:36<00:05,  7.29it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:36<00:05,  7.36it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:36<00:05,  7.38it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:36<00:04,  7.41it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:37<00:04,  7.40it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:37<00:04,  7.30it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:37<00:04,  7.20it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:37<00:04,  7.16it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:37<00:04,  7.10it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:37<00:04,  6.96it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:37<00:04,  6.99it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:38<00:04,  6.80it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:38<00:04,  6.78it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:38<00:03,  6.90it/s]predicting train subjects:  90%|█████████ | 240/266 [00:38<00:03,  6.96it/s]predicting train subjects:  91%|█████████ | 241/266 [00:38<00:03,  6.36it/s]predicting train subjects:  91%|█████████ | 242/266 [00:38<00:03,  6.53it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:39<00:03,  6.68it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:39<00:03,  6.57it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:39<00:03,  6.70it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:39<00:02,  6.89it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:39<00:02,  6.91it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:39<00:02,  6.89it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:39<00:02,  6.54it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:40<00:02,  6.34it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:40<00:02,  6.31it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:40<00:02,  6.16it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:40<00:02,  5.90it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:40<00:02,  5.82it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:40<00:01,  5.95it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:41<00:01,  6.04it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:41<00:01,  6.07it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:41<00:01,  6.09it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:41<00:01,  6.09it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:41<00:00,  6.10it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:41<00:00,  6.11it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:42<00:00,  6.12it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:42<00:00,  6.07it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:42<00:00,  6.08it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:42<00:00,  6.08it/s]predicting train subjects: 100%|██████████| 266/266 [00:42<00:00,  6.10it/s]predicting train subjects: 100%|██████████| 266/266 [00:42<00:00,  6.22it/s]
saving BB  test1-THALAMUS:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 4/4 [00:00<00:00, 75.61it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 8/266 [00:00<00:03, 75.85it/s]saving BB  train1-THALAMUS:   6%|▌         | 15/266 [00:00<00:03, 73.11it/s]saving BB  train1-THALAMUS:   8%|▊         | 22/266 [00:00<00:03, 71.76it/s]saving BB  train1-THALAMUS:  11%|█▏        | 30/266 [00:00<00:03, 72.60it/s]saving BB  train1-THALAMUS:  14%|█▍        | 38/266 [00:00<00:03, 74.61it/s]saving BB  train1-THALAMUS:  18%|█▊        | 47/266 [00:00<00:02, 76.85it/s]saving BB  train1-THALAMUS:  21%|██        | 56/266 [00:00<00:02, 78.65it/s]saving BB  train1-THALAMUS:  24%|██▍       | 65/266 [00:00<00:02, 80.44it/s]saving BB  train1-THALAMUS:  28%|██▊       | 75/266 [00:00<00:02, 83.24it/s]saving BB  train1-THALAMUS:  32%|███▏      | 84/266 [00:01<00:02, 81.60it/s]saving BB  train1-THALAMUS:  35%|███▍      | 92/266 [00:01<00:02, 79.95it/s]saving BB  train1-THALAMUS:  38%|███▊      | 100/266 [00:01<00:02, 79.88it/s]saving BB  train1-THALAMUS:  41%|████      | 109/266 [00:01<00:02, 78.09it/s]saving BB  train1-THALAMUS:  44%|████▍     | 118/266 [00:01<00:01, 79.86it/s]saving BB  train1-THALAMUS:  47%|████▋     | 126/266 [00:01<00:01, 78.58it/s]saving BB  train1-THALAMUS:  50%|█████     | 134/266 [00:01<00:01, 77.82it/s]saving BB  train1-THALAMUS:  53%|█████▎    | 142/266 [00:01<00:01, 77.67it/s]saving BB  train1-THALAMUS:  56%|█████▋    | 150/266 [00:01<00:01, 76.51it/s]saving BB  train1-THALAMUS:  59%|█████▉    | 158/266 [00:02<00:01, 76.82it/s]saving BB  train1-THALAMUS:  63%|██████▎   | 168/266 [00:02<00:01, 81.38it/s]saving BB  train1-THALAMUS:  67%|██████▋   | 178/266 [00:02<00:01, 84.40it/s]saving BB  train1-THALAMUS:  70%|███████   | 187/266 [00:02<00:00, 85.87it/s]saving BB  train1-THALAMUS:  74%|███████▎  | 196/266 [00:02<00:00, 84.29it/s]saving BB  train1-THALAMUS:  77%|███████▋  | 205/266 [00:02<00:00, 82.57it/s]saving BB  train1-THALAMUS:  80%|████████  | 214/266 [00:02<00:00, 83.08it/s]saving BB  train1-THALAMUS:  84%|████████▍ | 223/266 [00:02<00:00, 84.54it/s]saving BB  train1-THALAMUS:  87%|████████▋ | 232/266 [00:02<00:00, 85.88it/s]saving BB  train1-THALAMUS:  91%|█████████ | 241/266 [00:02<00:00, 85.99it/s]saving BB  train1-THALAMUS:  94%|█████████▍| 250/266 [00:03<00:00, 86.04it/s]saving BB  train1-THALAMUS:  97%|█████████▋| 259/266 [00:03<00:00, 83.92it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 80.84it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<05:27,  1.24s/it]Loading train:   1%|          | 2/266 [00:02<05:09,  1.17s/it]Loading train:   1%|          | 3/266 [00:03<04:48,  1.10s/it]Loading train:   2%|▏         | 4/266 [00:04<04:31,  1.04s/it]Loading train:   2%|▏         | 5/266 [00:05<04:30,  1.04s/it]Loading train:   2%|▏         | 6/266 [00:05<04:06,  1.06it/s]Loading train:   3%|▎         | 7/266 [00:06<03:48,  1.13it/s]Loading train:   3%|▎         | 8/266 [00:07<03:36,  1.19it/s]Loading train:   3%|▎         | 9/266 [00:08<03:30,  1.22it/s]Loading train:   4%|▍         | 10/266 [00:08<03:25,  1.24it/s]Loading train:   4%|▍         | 11/266 [00:09<03:24,  1.25it/s]Loading train:   5%|▍         | 12/266 [00:10<03:21,  1.26it/s]Loading train:   5%|▍         | 13/266 [00:11<03:16,  1.29it/s]Loading train:   5%|▌         | 14/266 [00:11<03:12,  1.31it/s]Loading train:   6%|▌         | 15/266 [00:12<03:09,  1.32it/s]Loading train:   6%|▌         | 16/266 [00:13<03:11,  1.30it/s]Loading train:   6%|▋         | 17/266 [00:14<03:09,  1.31it/s]Loading train:   7%|▋         | 18/266 [00:14<03:10,  1.30it/s]Loading train:   7%|▋         | 19/266 [00:15<03:04,  1.34it/s]Loading train:   8%|▊         | 20/266 [00:16<03:01,  1.35it/s]Loading train:   8%|▊         | 21/266 [00:17<03:01,  1.35it/s]Loading train:   8%|▊         | 22/266 [00:17<03:01,  1.34it/s]Loading train:   9%|▊         | 23/266 [00:18<03:04,  1.32it/s]Loading train:   9%|▉         | 24/266 [00:19<02:56,  1.37it/s]Loading train:   9%|▉         | 25/266 [00:19<02:50,  1.41it/s]Loading train:  10%|▉         | 26/266 [00:20<02:48,  1.43it/s]Loading train:  10%|█         | 27/266 [00:21<02:41,  1.48it/s]Loading train:  11%|█         | 28/266 [00:21<02:38,  1.50it/s]Loading train:  11%|█         | 29/266 [00:22<02:37,  1.51it/s]Loading train:  11%|█▏        | 30/266 [00:23<02:34,  1.53it/s]Loading train:  12%|█▏        | 31/266 [00:23<02:33,  1.53it/s]Loading train:  12%|█▏        | 32/266 [00:24<02:34,  1.51it/s]Loading train:  12%|█▏        | 33/266 [00:25<02:37,  1.48it/s]Loading train:  13%|█▎        | 34/266 [00:25<02:38,  1.47it/s]Loading train:  13%|█▎        | 35/266 [00:26<02:37,  1.47it/s]Loading train:  14%|█▎        | 36/266 [00:27<02:33,  1.50it/s]Loading train:  14%|█▍        | 37/266 [00:27<02:28,  1.54it/s]Loading train:  14%|█▍        | 38/266 [00:28<02:25,  1.56it/s]Loading train:  15%|█▍        | 39/266 [00:29<02:25,  1.56it/s]Loading train:  15%|█▌        | 40/266 [00:29<02:26,  1.54it/s]Loading train:  15%|█▌        | 41/266 [00:30<02:26,  1.54it/s]Loading train:  16%|█▌        | 42/266 [00:31<02:27,  1.52it/s]Loading train:  16%|█▌        | 43/266 [00:31<02:26,  1.52it/s]Loading train:  17%|█▋        | 44/266 [00:32<02:25,  1.53it/s]Loading train:  17%|█▋        | 45/266 [00:33<02:25,  1.52it/s]Loading train:  17%|█▋        | 46/266 [00:33<02:22,  1.54it/s]Loading train:  18%|█▊        | 47/266 [00:34<02:20,  1.56it/s]Loading train:  18%|█▊        | 48/266 [00:34<02:18,  1.58it/s]Loading train:  18%|█▊        | 49/266 [00:35<02:17,  1.58it/s]Loading train:  19%|█▉        | 50/266 [00:36<02:16,  1.58it/s]Loading train:  19%|█▉        | 51/266 [00:36<02:16,  1.58it/s]Loading train:  20%|█▉        | 52/266 [00:37<02:15,  1.59it/s]Loading train:  20%|█▉        | 53/266 [00:38<02:15,  1.57it/s]Loading train:  20%|██        | 54/266 [00:38<02:13,  1.58it/s]Loading train:  21%|██        | 55/266 [00:39<02:13,  1.58it/s]Loading train:  21%|██        | 56/266 [00:40<02:11,  1.59it/s]Loading train:  21%|██▏       | 57/266 [00:40<02:13,  1.57it/s]Loading train:  22%|██▏       | 58/266 [00:41<02:13,  1.56it/s]Loading train:  22%|██▏       | 59/266 [00:41<02:11,  1.57it/s]Loading train:  23%|██▎       | 60/266 [00:42<02:09,  1.59it/s]Loading train:  23%|██▎       | 61/266 [00:43<02:04,  1.65it/s]Loading train:  23%|██▎       | 62/266 [00:43<02:00,  1.70it/s]Loading train:  24%|██▎       | 63/266 [00:44<01:57,  1.73it/s]Loading train:  24%|██▍       | 64/266 [00:44<01:55,  1.75it/s]Loading train:  24%|██▍       | 65/266 [00:45<01:54,  1.75it/s]Loading train:  25%|██▍       | 66/266 [00:45<01:54,  1.75it/s]Loading train:  25%|██▌       | 67/266 [00:46<01:53,  1.76it/s]Loading train:  26%|██▌       | 68/266 [00:47<01:52,  1.76it/s]Loading train:  26%|██▌       | 69/266 [00:47<01:53,  1.74it/s]Loading train:  26%|██▋       | 70/266 [00:48<01:52,  1.74it/s]Loading train:  27%|██▋       | 71/266 [00:48<01:51,  1.75it/s]Loading train:  27%|██▋       | 72/266 [00:49<01:49,  1.77it/s]Loading train:  27%|██▋       | 73/266 [00:49<01:49,  1.76it/s]Loading train:  28%|██▊       | 74/266 [00:50<01:47,  1.78it/s]Loading train:  28%|██▊       | 75/266 [00:51<01:46,  1.79it/s]Loading train:  29%|██▊       | 76/266 [00:51<01:46,  1.79it/s]Loading train:  29%|██▉       | 77/266 [00:52<01:47,  1.76it/s]Loading train:  29%|██▉       | 78/266 [00:52<01:55,  1.63it/s]Loading train:  30%|██▉       | 79/266 [00:53<01:57,  1.60it/s]Loading train:  30%|███       | 80/266 [00:54<02:00,  1.55it/s]Loading train:  30%|███       | 81/266 [00:54<02:01,  1.53it/s]Loading train:  31%|███       | 82/266 [00:55<01:59,  1.54it/s]Loading train:  31%|███       | 83/266 [00:56<01:58,  1.54it/s]Loading train:  32%|███▏      | 84/266 [00:56<01:58,  1.54it/s]Loading train:  32%|███▏      | 85/266 [00:57<01:58,  1.53it/s]Loading train:  32%|███▏      | 86/266 [00:58<01:58,  1.52it/s]Loading train:  33%|███▎      | 87/266 [00:58<01:59,  1.50it/s]Loading train:  33%|███▎      | 88/266 [00:59<02:01,  1.46it/s]Loading train:  33%|███▎      | 89/266 [01:00<02:01,  1.46it/s]Loading train:  34%|███▍      | 90/266 [01:00<02:00,  1.46it/s]Loading train:  34%|███▍      | 91/266 [01:01<01:57,  1.49it/s]Loading train:  35%|███▍      | 92/266 [01:02<01:56,  1.50it/s]Loading train:  35%|███▍      | 93/266 [01:02<01:55,  1.50it/s]Loading train:  35%|███▌      | 94/266 [01:03<01:54,  1.51it/s]Loading train:  36%|███▌      | 95/266 [01:04<01:52,  1.53it/s]Loading train:  36%|███▌      | 96/266 [01:05<02:06,  1.35it/s]Loading train:  36%|███▋      | 97/266 [01:06<02:21,  1.19it/s]Loading train:  37%|███▋      | 98/266 [01:07<02:25,  1.16it/s]Loading train:  37%|███▋      | 99/266 [01:07<02:20,  1.19it/s]Loading train:  38%|███▊      | 100/266 [01:08<02:20,  1.18it/s]Loading train:  38%|███▊      | 101/266 [01:09<02:10,  1.26it/s]Loading train:  38%|███▊      | 102/266 [01:10<02:03,  1.33it/s]Loading train:  39%|███▊      | 103/266 [01:10<01:57,  1.39it/s]Loading train:  39%|███▉      | 104/266 [01:11<01:52,  1.44it/s]Loading train:  39%|███▉      | 105/266 [01:12<01:48,  1.48it/s]Loading train:  40%|███▉      | 106/266 [01:12<01:49,  1.47it/s]Loading train:  40%|████      | 107/266 [01:13<01:46,  1.49it/s]Loading train:  41%|████      | 108/266 [01:14<01:47,  1.47it/s]Loading train:  41%|████      | 109/266 [01:14<01:44,  1.51it/s]Loading train:  41%|████▏     | 110/266 [01:15<01:43,  1.51it/s]Loading train:  42%|████▏     | 111/266 [01:16<01:45,  1.47it/s]Loading train:  42%|████▏     | 112/266 [01:16<01:43,  1.49it/s]Loading train:  42%|████▏     | 113/266 [01:17<01:41,  1.51it/s]Loading train:  43%|████▎     | 114/266 [01:17<01:39,  1.53it/s]Loading train:  43%|████▎     | 115/266 [01:18<01:37,  1.56it/s]Loading train:  44%|████▎     | 116/266 [01:19<01:35,  1.57it/s]Loading train:  44%|████▍     | 117/266 [01:19<01:36,  1.55it/s]Loading train:  44%|████▍     | 118/266 [01:20<01:35,  1.54it/s]Loading train:  45%|████▍     | 119/266 [01:21<01:38,  1.50it/s]Loading train:  45%|████▌     | 120/266 [01:21<01:39,  1.47it/s]Loading train:  45%|████▌     | 121/266 [01:22<01:39,  1.46it/s]Loading train:  46%|████▌     | 122/266 [01:23<01:39,  1.45it/s]Loading train:  46%|████▌     | 123/266 [01:24<01:37,  1.46it/s]Loading train:  47%|████▋     | 124/266 [01:24<01:36,  1.48it/s]Loading train:  47%|████▋     | 125/266 [01:25<01:35,  1.48it/s]Loading train:  47%|████▋     | 126/266 [01:26<01:33,  1.50it/s]Loading train:  48%|████▊     | 127/266 [01:26<01:31,  1.52it/s]Loading train:  48%|████▊     | 128/266 [01:27<01:32,  1.50it/s]Loading train:  48%|████▊     | 129/266 [01:27<01:30,  1.52it/s]Loading train:  49%|████▉     | 130/266 [01:28<01:29,  1.53it/s]Loading train:  49%|████▉     | 131/266 [01:29<01:29,  1.51it/s]Loading train:  50%|████▉     | 132/266 [01:29<01:27,  1.52it/s]Loading train:  50%|█████     | 133/266 [01:30<01:26,  1.53it/s]Loading train:  50%|█████     | 134/266 [01:31<01:27,  1.51it/s]Loading train:  51%|█████     | 135/266 [01:31<01:28,  1.48it/s]Loading train:  51%|█████     | 136/266 [01:32<01:27,  1.49it/s]Loading train:  52%|█████▏    | 137/266 [01:33<01:26,  1.48it/s]Loading train:  52%|█████▏    | 138/266 [01:34<01:26,  1.48it/s]Loading train:  52%|█████▏    | 139/266 [01:34<01:25,  1.49it/s]Loading train:  53%|█████▎    | 140/266 [01:35<01:24,  1.49it/s]Loading train:  53%|█████▎    | 141/266 [01:36<01:25,  1.46it/s]Loading train:  53%|█████▎    | 142/266 [01:36<01:24,  1.47it/s]Loading train:  54%|█████▍    | 143/266 [01:37<01:24,  1.46it/s]Loading train:  54%|█████▍    | 144/266 [01:38<01:24,  1.45it/s]Loading train:  55%|█████▍    | 145/266 [01:38<01:22,  1.46it/s]Loading train:  55%|█████▍    | 146/266 [01:39<01:21,  1.48it/s]Loading train:  55%|█████▌    | 147/266 [01:40<01:19,  1.49it/s]Loading train:  56%|█████▌    | 148/266 [01:40<01:18,  1.50it/s]Loading train:  56%|█████▌    | 149/266 [01:41<01:18,  1.50it/s]Loading train:  56%|█████▋    | 150/266 [01:42<01:17,  1.50it/s]Loading train:  57%|█████▋    | 151/266 [01:42<01:16,  1.50it/s]Loading train:  57%|█████▋    | 152/266 [01:43<01:15,  1.50it/s]Loading train:  58%|█████▊    | 153/266 [01:44<01:16,  1.47it/s]Loading train:  58%|█████▊    | 154/266 [01:44<01:15,  1.49it/s]Loading train:  58%|█████▊    | 155/266 [01:45<01:12,  1.54it/s]Loading train:  59%|█████▊    | 156/266 [01:45<01:08,  1.60it/s]Loading train:  59%|█████▉    | 157/266 [01:46<01:06,  1.65it/s]Loading train:  59%|█████▉    | 158/266 [01:47<01:03,  1.69it/s]Loading train:  60%|█████▉    | 159/266 [01:47<01:02,  1.71it/s]Loading train:  60%|██████    | 160/266 [01:48<01:01,  1.71it/s]Loading train:  61%|██████    | 161/266 [01:48<01:01,  1.72it/s]Loading train:  61%|██████    | 162/266 [01:49<01:01,  1.70it/s]Loading train:  61%|██████▏   | 163/266 [01:50<01:00,  1.70it/s]Loading train:  62%|██████▏   | 164/266 [01:50<01:00,  1.68it/s]Loading train:  62%|██████▏   | 165/266 [01:51<00:58,  1.72it/s]Loading train:  62%|██████▏   | 166/266 [01:51<00:57,  1.73it/s]Loading train:  63%|██████▎   | 167/266 [01:52<00:57,  1.71it/s]Loading train:  63%|██████▎   | 168/266 [01:52<00:57,  1.70it/s]Loading train:  64%|██████▎   | 169/266 [01:53<00:57,  1.69it/s]Loading train:  64%|██████▍   | 170/266 [01:54<00:56,  1.69it/s]Loading train:  64%|██████▍   | 171/266 [01:54<00:56,  1.69it/s]Loading train:  65%|██████▍   | 172/266 [01:55<00:56,  1.66it/s]Loading train:  65%|██████▌   | 173/266 [01:55<00:55,  1.66it/s]Loading train:  65%|██████▌   | 174/266 [01:56<00:54,  1.70it/s]Loading train:  66%|██████▌   | 175/266 [01:57<00:52,  1.72it/s]Loading train:  66%|██████▌   | 176/266 [01:57<00:52,  1.72it/s]Loading train:  67%|██████▋   | 177/266 [01:58<00:51,  1.73it/s]Loading train:  67%|██████▋   | 178/266 [01:58<00:50,  1.74it/s]Loading train:  67%|██████▋   | 179/266 [01:59<00:49,  1.76it/s]Loading train:  68%|██████▊   | 180/266 [01:59<00:49,  1.74it/s]Loading train:  68%|██████▊   | 181/266 [02:00<00:48,  1.75it/s]Loading train:  68%|██████▊   | 182/266 [02:01<00:47,  1.77it/s]Loading train:  69%|██████▉   | 183/266 [02:01<00:46,  1.77it/s]Loading train:  69%|██████▉   | 184/266 [02:02<00:46,  1.76it/s]Loading train:  70%|██████▉   | 185/266 [02:02<00:47,  1.72it/s]Loading train:  70%|██████▉   | 186/266 [02:03<00:46,  1.73it/s]Loading train:  70%|███████   | 187/266 [02:03<00:44,  1.76it/s]Loading train:  71%|███████   | 188/266 [02:04<00:44,  1.75it/s]Loading train:  71%|███████   | 189/266 [02:05<00:43,  1.76it/s]Loading train:  71%|███████▏  | 190/266 [02:05<00:43,  1.76it/s]Loading train:  72%|███████▏  | 191/266 [02:06<00:50,  1.49it/s]Loading train:  72%|███████▏  | 192/266 [02:07<00:52,  1.40it/s]Loading train:  73%|███████▎  | 193/266 [02:08<00:54,  1.34it/s]Loading train:  73%|███████▎  | 194/266 [02:09<00:59,  1.21it/s]Loading train:  73%|███████▎  | 195/266 [02:09<00:53,  1.32it/s]Loading train:  74%|███████▎  | 196/266 [02:10<00:49,  1.41it/s]Loading train:  74%|███████▍  | 197/266 [02:10<00:46,  1.48it/s]Loading train:  74%|███████▍  | 198/266 [02:11<00:44,  1.54it/s]Loading train:  75%|███████▍  | 199/266 [02:12<00:42,  1.58it/s]Loading train:  75%|███████▌  | 200/266 [02:12<00:41,  1.58it/s]Loading train:  76%|███████▌  | 201/266 [02:13<00:40,  1.60it/s]Loading train:  76%|███████▌  | 202/266 [02:14<00:40,  1.60it/s]Loading train:  76%|███████▋  | 203/266 [02:14<00:39,  1.61it/s]Loading train:  77%|███████▋  | 204/266 [02:15<00:38,  1.63it/s]Loading train:  77%|███████▋  | 205/266 [02:15<00:37,  1.64it/s]Loading train:  77%|███████▋  | 206/266 [02:16<00:36,  1.64it/s]Loading train:  78%|███████▊  | 207/266 [02:17<00:35,  1.64it/s]Loading train:  78%|███████▊  | 208/266 [02:17<00:35,  1.65it/s]Loading train:  79%|███████▊  | 209/266 [02:18<00:34,  1.66it/s]Loading train:  79%|███████▉  | 210/266 [02:18<00:34,  1.65it/s]Loading train:  79%|███████▉  | 211/266 [02:19<00:33,  1.66it/s]Loading train:  80%|███████▉  | 212/266 [02:20<00:41,  1.30it/s]Loading train:  80%|████████  | 213/266 [02:25<01:44,  1.98s/it]Loading train:  80%|████████  | 214/266 [02:33<03:21,  3.88s/it]Loading train:  81%|████████  | 215/266 [02:42<04:32,  5.33s/it]Loading train:  81%|████████  | 216/266 [02:51<05:16,  6.33s/it]Loading train:  82%|████████▏ | 217/266 [02:57<05:08,  6.29s/it]Loading train:  82%|████████▏ | 218/266 [03:03<05:03,  6.33s/it]Loading train:  82%|████████▏ | 219/266 [03:10<05:08,  6.57s/it]Loading train:  83%|████████▎ | 220/266 [03:19<05:29,  7.15s/it]Loading train:  83%|████████▎ | 221/266 [03:28<05:47,  7.73s/it]Loading train:  83%|████████▎ | 222/266 [03:37<05:52,  8.01s/it]Loading train:  84%|████████▍ | 223/266 [03:46<06:06,  8.53s/it]Loading train:  84%|████████▍ | 224/266 [03:55<05:54,  8.45s/it]Loading train:  85%|████████▍ | 225/266 [04:03<05:45,  8.43s/it]Loading train:  85%|████████▍ | 226/266 [04:09<05:07,  7.68s/it]Loading train:  85%|████████▌ | 227/266 [04:15<04:40,  7.19s/it]Loading train:  86%|████████▌ | 228/266 [04:21<04:17,  6.76s/it]Loading train:  86%|████████▌ | 229/266 [04:28<04:15,  6.91s/it]Loading train:  86%|████████▋ | 230/266 [04:35<04:05,  6.82s/it]Loading train:  87%|████████▋ | 231/266 [04:42<04:08,  7.10s/it]Loading train:  87%|████████▋ | 232/266 [04:49<04:01,  7.11s/it]Loading train:  88%|████████▊ | 233/266 [04:57<03:56,  7.17s/it]Loading train:  88%|████████▊ | 234/266 [05:04<03:46,  7.07s/it]Loading train:  88%|████████▊ | 235/266 [05:12<03:53,  7.53s/it]Loading train:  89%|████████▊ | 236/266 [05:20<03:45,  7.52s/it]Loading train:  89%|████████▉ | 237/266 [05:27<03:35,  7.44s/it]Loading train:  89%|████████▉ | 238/266 [05:35<03:31,  7.57s/it]Loading train:  90%|████████▉ | 239/266 [05:44<03:36,  8.00s/it]Loading train:  90%|█████████ | 240/266 [05:52<03:32,  8.19s/it]Loading train:  91%|█████████ | 241/266 [06:01<03:30,  8.42s/it]Loading train:  91%|█████████ | 242/266 [06:10<03:25,  8.55s/it]Loading train:  91%|█████████▏| 243/266 [06:21<03:32,  9.23s/it]Loading train:  92%|█████████▏| 244/266 [06:28<03:04,  8.41s/it]Loading train:  92%|█████████▏| 245/266 [06:36<02:59,  8.53s/it]Loading train:  92%|█████████▏| 246/266 [06:41<02:27,  7.38s/it]Loading train:  93%|█████████▎| 247/266 [06:45<02:01,  6.41s/it]Loading train:  93%|█████████▎| 248/266 [06:48<01:33,  5.19s/it]Loading train:  94%|█████████▎| 249/266 [06:50<01:16,  4.49s/it]Loading train:  94%|█████████▍| 250/266 [06:55<01:13,  4.62s/it]Loading train:  94%|█████████▍| 251/266 [07:01<01:12,  4.82s/it]Loading train:  95%|█████████▍| 252/266 [07:07<01:14,  5.34s/it]Loading train:  95%|█████████▌| 253/266 [07:13<01:09,  5.37s/it]Loading train:  95%|█████████▌| 254/266 [07:18<01:03,  5.27s/it]Loading train:  96%|█████████▌| 255/266 [07:23<00:58,  5.28s/it]Loading train:  96%|█████████▌| 256/266 [07:28<00:52,  5.28s/it]Loading train:  97%|█████████▋| 257/266 [07:33<00:47,  5.22s/it]Loading train:  97%|█████████▋| 258/266 [07:38<00:41,  5.18s/it]Loading train:  97%|█████████▋| 259/266 [07:44<00:37,  5.29s/it]Loading train:  98%|█████████▊| 260/266 [07:50<00:32,  5.36s/it]Loading train:  98%|█████████▊| 261/266 [07:55<00:26,  5.39s/it]Loading train:  98%|█████████▊| 262/266 [08:01<00:21,  5.47s/it]Loading train:  99%|█████████▉| 263/266 [08:06<00:16,  5.57s/it]Loading train:  99%|█████████▉| 264/266 [08:12<00:11,  5.62s/it]Loading train: 100%|█████████▉| 265/266 [08:18<00:05,  5.68s/it]Loading train: 100%|██████████| 266/266 [08:24<00:00,  5.68s/it]Loading train: 100%|██████████| 266/266 [08:24<00:00,  1.90s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:05, 49.37it/s]concatenating: train:   4%|▍         | 10/266 [00:00<00:05, 48.06it/s]concatenating: train:   6%|▌         | 15/266 [00:00<00:05, 46.36it/s]concatenating: train:   8%|▊         | 20/266 [00:00<00:05, 45.13it/s]concatenating: train:   9%|▉         | 25/266 [00:00<00:05, 45.54it/s]concatenating: train:  12%|█▏        | 31/266 [00:00<00:04, 48.18it/s]concatenating: train:  14%|█▍        | 37/266 [00:00<00:04, 50.31it/s]concatenating: train:  16%|█▌        | 42/266 [00:00<00:04, 49.75it/s]concatenating: train:  18%|█▊        | 47/266 [00:00<00:04, 48.88it/s]concatenating: train:  20%|█▉        | 52/266 [00:01<00:04, 47.32it/s]concatenating: train:  21%|██▏       | 57/266 [00:01<00:04, 45.94it/s]concatenating: train:  23%|██▎       | 62/266 [00:01<00:04, 44.69it/s]concatenating: train:  26%|██▌       | 68/266 [00:01<00:04, 46.88it/s]concatenating: train:  28%|██▊       | 74/266 [00:01<00:03, 49.07it/s]concatenating: train:  30%|███       | 80/266 [00:01<00:03, 50.74it/s]concatenating: train:  32%|███▏      | 86/266 [00:01<00:03, 51.82it/s]concatenating: train:  35%|███▍      | 92/266 [00:01<00:03, 52.84it/s]concatenating: train:  37%|███▋      | 98/266 [00:01<00:03, 52.24it/s]concatenating: train:  39%|███▉      | 104/266 [00:02<00:03, 49.34it/s]concatenating: train:  41%|████▏     | 110/266 [00:02<00:03, 50.55it/s]concatenating: train:  44%|████▎     | 116/266 [00:02<00:03, 49.92it/s]concatenating: train:  46%|████▌     | 122/266 [00:02<00:02, 49.95it/s]concatenating: train:  48%|████▊     | 128/266 [00:02<00:02, 51.62it/s]concatenating: train:  51%|█████     | 135/266 [00:02<00:02, 54.15it/s]concatenating: train:  53%|█████▎    | 141/266 [00:02<00:02, 53.07it/s]concatenating: train:  55%|█████▌    | 147/266 [00:02<00:02, 53.52it/s]concatenating: train:  58%|█████▊    | 153/266 [00:03<00:02, 52.00it/s]concatenating: train:  60%|██████    | 160/266 [00:03<00:01, 54.51it/s]concatenating: train:  62%|██████▏   | 166/266 [00:03<00:01, 55.43it/s]concatenating: train:  65%|██████▍   | 172/266 [00:03<00:01, 55.75it/s]concatenating: train:  67%|██████▋   | 179/266 [00:03<00:01, 57.22it/s]concatenating: train:  70%|███████   | 187/266 [00:03<00:01, 62.25it/s]concatenating: train:  73%|███████▎  | 194/266 [00:03<00:01, 62.12it/s]concatenating: train:  76%|███████▌  | 201/266 [00:03<00:01, 59.61it/s]concatenating: train:  78%|███████▊  | 208/266 [00:03<00:01, 56.85it/s]concatenating: train:  80%|████████  | 214/266 [00:04<00:00, 54.12it/s]concatenating: train:  83%|████████▎ | 220/266 [00:04<00:00, 53.66it/s]concatenating: train:  85%|████████▍ | 226/266 [00:04<00:00, 53.07it/s]concatenating: train:  87%|████████▋ | 232/266 [00:04<00:00, 53.24it/s]concatenating: train:  89%|████████▉ | 238/266 [00:04<00:00, 53.48it/s]concatenating: train:  92%|█████████▏| 244/266 [00:04<00:00, 54.21it/s]concatenating: train:  94%|█████████▍| 250/266 [00:04<00:00, 49.37it/s]concatenating: train:  96%|█████████▌| 256/266 [00:04<00:00, 48.32it/s]concatenating: train:  98%|█████████▊| 261/266 [00:05<00:00, 45.73it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 43.80it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 51.26it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:10<00:31, 10.34s/it]Loading test:  50%|█████     | 2/4 [00:17<00:18,  9.37s/it]Loading test:  75%|███████▌  | 3/4 [00:24<00:08,  8.69s/it]Loading test: 100%|██████████| 4/4 [00:35<00:00,  9.46s/it]Loading test: 100%|██████████| 4/4 [00:35<00:00,  8.95s/it]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 42.67it/s]
Epoch 00042: val_mDice did not improve from 0.56193
Restoring model weights from the end of the best epoch
Epoch 00042: early stopping
{'val_loss': [0.2234980600951898, 0.21997213931848306, 0.21293591884802085, 0.22050967887796538, 0.23388115188690148, 0.21766311667784297, 0.2118903226924665, 0.18643730280525755, 0.11342925518149077, 0.1584872610942282, 0.0825711562100685, 0.10621711175249081, -0.023660901896279268, -0.01629526683627957, 0.061084175087285766, -0.0011405092899245445, 0.02762051679269232, 0.023177429017695515, -0.024490320095510193, -0.009191376843837777, 0.033852938600260805, 0.015180372830593225, 0.10338583994995464, 0.016615919467776713, -0.047216929570593015, -0.008223379651705423, 0.023233961916030055, -0.06078577478124638, 0.04233059663363178, -0.022509809094245988, 0.01984584436874197, 0.06926491103991113, -0.02431692536732163, 0.005878714599994698, -0.0020963314431484297, -0.023854563031533754, -0.01709207734375289, 0.004834171309314593, 0.03868179264092686, 0.003592288140395675, -0.020538997544784738, 0.018336435008530664], 'val_acc': [0.9935656521055434, 0.9946559875300436, 0.994437246009557, 0.994796896823729, 0.994289879546021, 0.9944473998715179, 0.9945815359101151, 0.9948110514216952, 0.9946402961557562, 0.9945033936187474, 0.9946326077586473, 0.9948531976251891, 0.9946236881342801, 0.9946630648290268, 0.9948030484445167, 0.9948205866596915, 0.9947485899684405, 0.994757825678045, 0.99481782016128, 0.9949184210613521, 0.9949408766597209, 0.9948298193589605, 0.994913496152319, 0.9949565695391761, 0.9946255319648318, 0.9948504296216097, 0.9948624288192903, 0.9947082860903307, 0.9946562990997777, 0.9946590655981892, 0.9947781288864637, 0.9948984203916608, 0.9949322640895844, 0.9947824366766997, 0.9948904234351534, 0.9948762733526905, 0.9948664295552957, 0.9948799625189617, 0.994844278000822, 0.994805512404201, 0.9949808734836001, 0.9949458015687538], 'val_mDice': [0.5564982188169403, 0.5619290282930991, 0.5587061276339521, 0.5603141232271387, 0.5173482256708175, 0.5439104683843797, 0.5413856124035036, 0.5607110585528191, 0.5477704852819443, 0.542774045030878, 0.5482857403436777, 0.5369374870952933, 0.5497160531354673, 0.540570005061025, 0.5471728945320303, 0.5535555450302182, 0.5444108785101862, 0.5456802602845592, 0.5454655953052671, 0.5448699757607296, 0.5415928017611455, 0.5486970018863028, 0.5450390956618569, 0.5471096360924268, 0.5421941140392122, 0.5503409935838797, 0.5498439438558463, 0.5472636282105338, 0.5511975947842754, 0.5394602920671906, 0.5368553339080377, 0.5449902815650208, 0.542200015801372, 0.5475149860586783, 0.5389836705242744, 0.547589309739344, 0.5467625963266449, 0.5478276688643177, 0.546218314857194, 0.5455590001862458, 0.549136180091988, 0.5435725128409838], 'loss': [0.07733793499720043, 0.05204645742594666, 0.04547377703378288, 0.04244759802045293, 0.04062192252382791, 0.03822398489961356, 0.037056380948947296, 0.035542296888139355, 0.03488022734737475, 0.03424556630726101, 0.03391042692265923, 0.03314299826298376, 0.032520363339530345, 0.03197828991935666, 0.03164185330526756, 0.031693477368999676, 0.03218512046780963, 0.029138942286422765, 0.02950991832584278, 0.028445924028973064, 0.02835012132657125, 0.02812108161829631, 0.027985040327507392, 0.027679277861553206, 0.027601398722758377, 0.02746597806024846, 0.027810318609200118, 0.027551053289615417, 0.027031510385766896, 0.027009462073509745, 0.02673247009947032, 0.02664331534246022, 0.025954179223780592, 0.025733721130815235, 0.025537053489495937, 0.02542074478255696, 0.025521232945770723, 0.025198786569083795, 0.025351656039317338, 0.025121183341631454, 0.025434816167315993, 0.025033915170021848], 'acc': [0.991082400355082, 0.9941897368462611, 0.9948235110641612, 0.9951302399141048, 0.9953129217888494, 0.9955525416442256, 0.9956579105721225, 0.995768347055526, 0.9958299786245367, 0.9958967259319775, 0.9959420469935379, 0.9960138398819713, 0.9960734652915764, 0.9961064836154027, 0.9961514000843741, 0.9961826080047003, 0.9961602589271968, 0.9963668782888138, 0.9963382995025979, 0.9964520219744996, 0.9964555609787197, 0.9964689209508322, 0.9964794555345728, 0.9964875309406065, 0.9965092792266323, 0.9965255082592804, 0.996537961947742, 0.9965379224966955, 0.9965460716082906, 0.9965757344804717, 0.9965850116142818, 0.9965823249477501, 0.9966498034753553, 0.9966565081939209, 0.9966845122778041, 0.9966851162766492, 0.9966889740029012, 0.9967180480697034, 0.9967135344389214, 0.9967257062155372, 0.9967154458936662, 0.9967273386676421], 'mDice': [0.8510709421255083, 0.8989456388343973, 0.9117067224424735, 0.9175736317042689, 0.9211206723031334, 0.9257863676786108, 0.9280557852936834, 0.9310210761939759, 0.9323089807457865, 0.9335431953878202, 0.9341837529708119, 0.9356799071917099, 0.9368944961286014, 0.9379523166311206, 0.9386065685021113, 0.9384823966695892, 0.9375131742993952, 0.9434830991321469, 0.9427578932721055, 0.9448214781950085, 0.945010473148724, 0.9454593637703733, 0.9457276475158208, 0.946330331461595, 0.9464761664145254, 0.9467423361959609, 0.9460461767820375, 0.9465629088089355, 0.9475959251985616, 0.947626944797856, 0.9481750503632316, 0.9483548603725914, 0.9496937637080008, 0.9501247380513038, 0.9505048189621695, 0.9507356403354204, 0.950536066848936, 0.9511694087303051, 0.9508585820743439, 0.9513157825571856, 0.9506965496193606, 0.9514847498540305], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 2  | SD 1  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 2  | SD 1  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 30 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label values min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 48, 52, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 48, 52, 30)   300         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 48, 52, 30)   120         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 48, 52, 30)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 48, 52, 30)   8130        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 48, 52, 30)   120         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 48, 52, 30)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 24, 26, 30)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 24, 26, 30)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 24, 26, 60)   16260       dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 24, 26, 60)   240         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 24, 26, 60)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 24, 26, 60)   32460       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 24, 26, 60)   240         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 24, 26, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 24, 26, 90)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 12, 13, 90)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 12, 13, 90)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 12, 13, 120)  97320       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 12, 13, 120)  480         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 12, 13, 120)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 12, 13, 120)  129720      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 12, 13, 120)  480         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 12, 13, 120)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 12, 13, 210)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 12, 13, 210)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 24, 26, 60)   50460       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 24, 26, 150)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 24, 26, 60)   81060       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 24, 26, 60)   240         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 24, 26, 60)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 24, 26, 60)   32460       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 24, 26, 60)   240         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 24, 26, 60)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 24, 26, 210)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24, 26, 210)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 48, 52, 30)   25230       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 48, 52, 60)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 48, 52, 30)   16230       concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 48, 52, 30)   120         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 48, 52, 30)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 48, 52, 30)   8130        activation_9[0][0]               
__________________________________________________________________________________________________2020-01-21 02:05:58.444315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 02:05:58.444427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 02:05:58.444441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 02:05:58.444448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 02:05:58.444762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)

batch_normalization_10 (BatchNo (None, 48, 52, 30)   120         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 48, 52, 30)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 48, 52, 90)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 48, 52, 90)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 48, 52, 13)   1183        dropout_5[0][0]                  
==================================================================================================
Total params: 501,343
Trainable params: 500,143
Non-trainable params: 1,200
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.34747611e-02 3.28980874e-02 7.69276526e-02 9.55864766e-03
 2.76649029e-02 7.23777460e-03 8.42766454e-02 1.14340291e-01
 8.97796968e-02 1.36406602e-02 2.91083436e-01 1.88855671e-01
 2.61773532e-04]
Train on 16890 samples, validate on 242 samples
Epoch 1/300
 - 42s - loss: 0.5362 - acc: 0.9158 - mDice: 0.4221 - val_loss: 0.7261 - val_acc: 0.9462 - val_mDice: 0.2151

Epoch 00001: val_mDice improved from -inf to 0.21506, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 2/300
 - 37s - loss: 0.3966 - acc: 0.9385 - mDice: 0.5726 - val_loss: 0.7037 - val_acc: 0.9478 - val_mDice: 0.2335

Epoch 00002: val_mDice improved from 0.21506 to 0.23354, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 3/300
 - 36s - loss: 0.3702 - acc: 0.9424 - mDice: 0.6012 - val_loss: 0.6942 - val_acc: 0.9508 - val_mDice: 0.2353

Epoch 00003: val_mDice improved from 0.23354 to 0.23530, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 4/300
 - 37s - loss: 0.3536 - acc: 0.9446 - mDice: 0.6190 - val_loss: 0.6473 - val_acc: 0.9452 - val_mDice: 0.2222

Epoch 00004: val_mDice did not improve from 0.23530
Epoch 5/300
 - 37s - loss: 0.3432 - acc: 0.9463 - mDice: 0.6302 - val_loss: 0.5679 - val_acc: 0.9468 - val_mDice: 0.2278

Epoch 00005: val_mDice did not improve from 0.23530
Epoch 6/300
 - 36s - loss: 0.3356 - acc: 0.9476 - mDice: 0.6384 - val_loss: 0.5610 - val_acc: 0.9484 - val_mDice: 0.2271

Epoch 00006: val_mDice did not improve from 0.23530
Epoch 7/300
 - 34s - loss: 0.3260 - acc: 0.9487 - mDice: 0.6487 - val_loss: 0.4275 - val_acc: 0.9494 - val_mDice: 0.2344

Epoch 00007: val_mDice did not improve from 0.23530
Epoch 8/300
 - 33s - loss: 0.3274 - acc: 0.9487 - mDice: 0.6472 - val_loss: 0.3462 - val_acc: 0.9474 - val_mDice: 0.2364

Epoch 00008: val_mDice improved from 0.23530 to 0.23636, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 9/300
 - 34s - loss: 0.3160 - acc: 0.9499 - mDice: 0.6596 - val_loss: 0.1907 - val_acc: 0.9493 - val_mDice: 0.2401

Epoch 00009: val_mDice improved from 0.23636 to 0.24011, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 10/300
 - 35s - loss: 0.3134 - acc: 0.9503 - mDice: 0.6623 - val_loss: 0.0080 - val_acc: 0.9486 - val_mDice: 0.2342

Epoch 00010: val_mDice did not improve from 0.24011
Epoch 11/300
 - 37s - loss: 0.3087 - acc: 0.9510 - mDice: 0.6674 - val_loss: 0.2124 - val_acc: 0.9471 - val_mDice: 0.2353

Epoch 00011: val_mDice did not improve from 0.24011
Epoch 12/300
 - 36s - loss: 0.3082 - acc: 0.9510 - mDice: 0.6680 - val_loss: -3.1536e-02 - val_acc: 0.9436 - val_mDice: 0.2235

Epoch 00012: val_mDice did not improve from 0.24011
Epoch 13/300
 - 33s - loss: 0.3051 - acc: 0.9512 - mDice: 0.6712 - val_loss: 0.1187 - val_acc: 0.9461 - val_mDice: 0.2255

Epoch 00013: val_mDice did not improve from 0.24011
Epoch 14/300
 - 34s - loss: 0.3034 - acc: 0.9515 - mDice: 0.6731 - val_loss: -9.4724e-02 - val_acc: 0.9503 - val_mDice: 0.2364

Epoch 00014: val_mDice did not improve from 0.24011
Epoch 15/300
 - 36s - loss: 0.3005 - acc: 0.9519 - mDice: 0.6762 - val_loss: -9.1422e-02 - val_acc: 0.9515 - val_mDice: 0.2318

Epoch 00015: val_mDice did not improve from 0.24011
Epoch 16/300
 - 36s - loss: 0.2969 - acc: 0.9522 - mDice: 0.6801 - val_loss: -1.1650e-01 - val_acc: 0.9480 - val_mDice: 0.2220

Epoch 00016: val_mDice did not improve from 0.24011
Epoch 17/300
 - 36s - loss: 0.2985 - acc: 0.9524 - mDice: 0.6783 - val_loss: -9.7264e-02 - val_acc: 0.9489 - val_mDice: 0.2293

Epoch 00017: val_mDice did not improve from 0.24011
Epoch 18/300
 - 36s - loss: 0.2950 - acc: 0.9525 - mDice: 0.6822 - val_loss: -7.4195e-02 - val_acc: 0.9495 - val_mDice: 0.2292

Epoch 00018: val_mDice did not improve from 0.24011
Epoch 19/300
 - 36s - loss: 0.2949 - acc: 0.9529 - mDice: 0.6823 - val_loss: -1.1974e-02 - val_acc: 0.9453 - val_mDice: 0.2249

Epoch 00019: val_mDice did not improve from 0.24011
Epoch 20/300
 - 35s - loss: 0.2901 - acc: 0.9529 - mDice: 0.6875 - val_loss: -1.8370e-01 - val_acc: 0.9512 - val_mDice: 0.2330

Epoch 00020: val_mDice did not improve from 0.24011
Epoch 21/300
 - 35s - loss: 0.2939 - acc: 0.9528 - mDice: 0.6831 - val_loss: -1.1540e-01 - val_acc: 0.9481 - val_mDice: 0.2327

Epoch 00021: val_mDice did not improve from 0.24011
Epoch 22/300
 - 35s - loss: 0.2984 - acc: 0.9493 - mDice: 0.6534 - val_loss: -1.7322e-01 - val_acc: 0.9469 - val_mDice: 0.2132

Epoch 00022: val_mDice did not improve from 0.24011
Epoch 23/300
 - 36s - loss: 0.2788 - acc: 0.9490 - mDice: 0.6412 - val_loss: -1.8293e-01 - val_acc: 0.9466 - val_mDice: 0.2261

Epoch 00023: val_mDice did not improve from 0.24011
Epoch 24/300
 - 34s - loss: 0.2919 - acc: 0.9477 - mDice: 0.6285 - val_loss: -2.1160e-01 - val_acc: 0.9487 - val_mDice: 0.2317

Epoch 00024: val_mDice did not improve from 0.24011

Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 25/300
 - 33s - loss: 0.2547 - acc: 0.9507 - mDice: 0.6634 - val_loss: -1.8393e-01 - val_acc: 0.9472 - val_mDice: 0.2297

Epoch 00025: val_mDice did not improve from 0.24011
Epoch 26/300
 - 33s - loss: 0.2493 - acc: 0.9516 - mDice: 0.6679 - val_loss: -2.0077e-01 - val_acc: 0.9508 - val_mDice: 0.2425

Epoch 00026: val_mDice improved from 0.24011 to 0.24248, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd1/best_model_weights.h5
Epoch 27/300
 - 33s - loss: 0.2406 - acc: 0.9517 - mDice: 0.6706 - val_loss: -2.1379e-01 - val_acc: 0.9490 - val_mDice: 0.2293

Epoch 00027: val_mDice did not improve from 0.24248
Epoch 28/300
 - 34s - loss: 0.2402 - acc: 0.9512 - mDice: 0.6624 - val_loss: -2.0436e-01 - val_acc: 0.9493 - val_mDice: 0.2295

Epoch 00028: val_mDice did not improve from 0.24248
Epoch 29/300
 - 34s - loss: 0.2377 - acc: 0.9510 - mDice: 0.6598 - val_loss: -2.4951e-01 - val_acc: 0.9511 - val_mDice: 0.2265

Epoch 00029: val_mDice did not improve from 0.24248
Epoch 30/300
 - 33s - loss: 0.2401 - acc: 0.9508 - mDice: 0.6556 - val_loss: -1.6069e-01 - val_acc: 0.9457 - val_mDice: 0.2150

Epoch 00030: val_mDice did not improve from 0.24248
Epoch 31/300
 - 33s - loss: 0.2309 - acc: 0.9511 - mDice: 0.6620 - val_loss: -2.1906e-01 - val_acc: 0.9494 - val_mDice: 0.2270

Epoch 00031: val_mDice did not improve from 0.24248
Epoch 32/300
 - 33s - loss: 0.2360 - acc: 0.9508 - mDice: 0.6521 - val_loss: -1.2004e-01 - val_acc: 0.9432 - val_mDice: 0.1937

Epoch 00032: val_mDice did not improve from 0.24248
Epoch 33/300
 - 32s - loss: 0.2361 - acc: 0.9503 - mDice: 0.6515 - val_loss: -2.3347e-01 - val_acc: 0.9492 - val_mDice: 0.2281

Epoch 00033: val_mDice did not improve from 0.24248
Epoch 34/300
 - 33s - loss: 0.2228 - acc: 0.9513 - mDice: 0.6654 - val_loss: -1.6767e-01 - val_acc: 0.9446 - val_mDice: 0.2071

Epoch 00034: val_mDice did not improve from 0.24248
Epoch 35/300
 - 32s - loss: 0.2309 - acc: 0.9509 - mDice: 0.6530 - val_loss: -2.4628e-01 - val_acc: 0.9485 - val_mDice: 0.2263

Epoch 00035: val_mDice did not improve from 0.24248
Epoch 36/300
 - 33s - loss: 0.2310 - acc: 0.9509 - mDice: 0.6581 - val_loss: -2.4457e-01 - val_acc: 0.9516 - val_mDice: 0.2278

Epoch 00036: val_mDice did not improve from 0.24248
Epoch 37/300
 - 32s - loss: 0.2269 - acc: 0.9514 - mDice: 0.6651 - val_loss: -2.4786e-01 - val_acc: 0.9519 - val_mDice: 0.2346

Epoch 00037: val_mDice did not improve from 0.24248
Epoch 38/300
 - 33s - loss: 0.2229 - acc: 0.9510 - mDice: 0.6607 - val_loss: -2.5828e-01 - val_acc: 0.9507 - val_mDice: 0.2206

Epoch 00038: val_mDice did not improve from 0.24248
Epoch 39/300
 - 32s - loss: 0.2216 - acc: 0.9508 - mDice: 0.6578 - val_loss: -2.3080e-01 - val_acc: 0.9495 - val_mDice: 0.2372

Epoch 00039: val_mDice did not improve from 0.24248
Epoch 40/300
 - 33s - loss: 0.2283 - acc: 0.9504 - mDice: 0.6508 - val_loss: -2.2841e-01 - val_acc: 0.9503 - val_mDice: 0.2275

Epoch 00040: val_mDice did not improve from 0.24248
Epoch 41/300
 - 32s - loss: 0.2241 - acc: 0.9508 - mDice: 0.6568 - val_loss: -2.3049e-01 - val_acc: 0.9508 - val_mDice: 0.2409

Epoch 00041: val_mDice did not improve from 0.24248

Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 42/300
 - 33s - loss: 0.2149 - acc: 0.9513 - mDice: 0.6611 - val_loss: -2.3796e-01 - val_acc: 0.9485 - val_mDice: 0.2177

Epoch 00042: val_mDice did not improve from 0.24248
Epoch 43/300
 - 32s - loss: 0.2089 - acc: 0.9517 - mDice: 0.6648 - val_loss: -2.4361e-01 - val_acc: 0.9504 - val_mDice: 0.2309

Epoch 00043: val_mDice did not improve from 0.24248
Epoch 44/300
 - 33s - loss: 0.2046 - acc: 0.9521 - mDice: 0.6732 - val_loss: -2.6027e-01 - val_acc: 0.9514 - val_mDice: 0.2411

Epoch 00044: val_mDice did not improve from 0.24248
Epoch 45/300
 - 32s - loss: 0.2037 - acc: 0.9526 - mDice: 0.6781 - val_loss: -2.6530e-01 - val_acc: 0.9513 - val_mDice: 0.2396

Epoch 00045: val_mDice did not improve from 0.24248
Epoch 46/300
 - 33s - loss: 0.2000 - acc: 0.9523 - mDice: 0.6734 - val_loss: -2.5770e-01 - val_acc: 0.9512 - val_mDice: 0.2388

Epoch 00046: val_mDice did not improve from 0.24248
Epoch 47/300
 - 33s - loss: 0.2014 - acc: 0.9525 - mDice: 0.6737 - val_loss: -2.4553e-01 - val_acc: 0.9505 - val_mDice: 0.2320

Epoch 00047: val_mDice did not improve from 0.24248
Epoch 48/300
 - 34s - loss: 0.1994 - acc: 0.9526 - mDice: 0.6752 - val_loss: -2.4280e-01 - val_acc: 0.9499 - val_mDice: 0.2367

Epoch 00048: val_mDice did not improve from 0.24248
Epoch 49/300
 - 33s - loss: 0.1974 - acc: 0.9526 - mDice: 0.6761 - val_loss: -2.4517e-01 - val_acc: 0.9505 - val_mDice: 0.2370

Epoch 00049: val_mDice did not improve from 0.24248
Epoch 50/300
 - 32s - loss: 0.1981 - acc: 0.9531 - mDice: 0.6814 - val_loss: -2.4095e-01 - val_acc: 0.9507 - val_mDice: 0.2357

Epoch 00050: val_mDice did not improve from 0.24248
Epoch 51/300
 - 33s - loss: 0.1987 - acc: 0.9530 - mDice: 0.6804 - val_loss: -2.6284e-01 - val_acc: 0.9500 - val_mDice: 0.2265

Epoch 00051: val_mDice did not improve from 0.24248
Epoch 52/300
 - 34s - loss: 0.2037 - acc: 0.9529 - mDice: 0.6800 - val_loss: -2.4171e-01 - val_acc: 0.9501 - val_mDice: 0.2280

Epoch 00052: val_mDice did not improve from 0.24248
Epoch 53/300
 - 33s - loss: 0.1939 - acc: 0.9532 - mDice: 0.6842 - val_loss: -2.6069e-01 - val_acc: 0.9527 - val_mDice: 0.2353

Epoch 00053: val_mDice did not improve from 0.24248
Epoch 54/300
 - 33s - loss: 0.2000 - acc: 0.9528 - mDice: 0.6757 - val_loss: -2.4906e-01 - val_acc: 0.9512 - val_mDice: 0.2345

Epoch 00054: val_mDice did not improve from 0.24248
Epoch 55/300
 - 33s - loss: 0.2003 - acc: 0.9526 - mDice: 0.6776 - val_loss: -2.6386e-01 - val_acc: 0.9505 - val_mDice: 0.2244

Epoch 00055: val_mDice did not improve from 0.24248
Epoch 56/300
 - 32s - loss: 0.1944 - acc: 0.9531 - mDice: 0.6817 - val_loss: -2.4725e-01 - val_acc: 0.9508 - val_mDice: 0.2329

Epoch 00056: val_mDice did not improve from 0.24248

Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 57/300
 - 33s - loss: 0.1922 - acc: 0.9532 - mDice: 0.6867 - val_loss: -2.5553e-01 - val_acc: 0.9513 - val_mDice: 0.2355

Epoch 00057: val_mDice did not improve from 0.24248
Epoch 58/300
 - 32s - loss: 0.1914 - acc: 0.9533 - mDice: 0.6878 - val_loss: -2.7556e-01 - val_acc: 0.9519 - val_mDice: 0.2371

Epoch 00058: val_mDice did not improve from 0.24248
Epoch 59/300
 - 33s - loss: 0.1869 - acc: 0.9536 - mDice: 0.6904 - val_loss: -2.3402e-01 - val_acc: 0.9507 - val_mDice: 0.2347

Epoch 00059: val_mDice did not improve from 0.24248
Epoch 60/300
 - 33s - loss: 0.1874 - acc: 0.9537 - mDice: 0.6890 - val_loss: -2.4587e-01 - val_acc: 0.9513 - val_mDice: 0.2316

Epoch 00060: val_mDice did not improve from 0.24248
Epoch 61/300
 - 33s - loss: 0.1897 - acc: 0.9537 - mDice: 0.6939 - val_loss: -2.6692e-01 - val_acc: 0.9517 - val_mDice: 0.2348

Epoch 00061: val_mDice did not improve from 0.24248
Epoch 62/300
 - 33s - loss: 0.1877 - acc: 0.9538 - mDice: 0.6880 - val_loss: -2.6903e-01 - val_acc: 0.9513 - val_mDice: 0.2368

Epoch 00062: val_mDice did not improve from 0.24248
Epoch 63/300
 - 33s - loss: 0.1839 - acc: 0.9539 - mDice: 0.6926 - val_loss: -2.7534e-01 - val_acc: 0.9519 - val_mDice: 0.2370

Epoch 00063: val_mDice did not improve from 0.24248
Epoch 64/300
 - 33s - loss: 0.1862 - acc: 0.9539 - mDice: 0.6929 - val_loss: -2.6289e-01 - val_acc: 0.9513 - val_mDice: 0.2373

Epoch 00064: val_mDice did not improve from 0.24248
Epoch 65/300
 - 33s - loss: 0.1842 - acc: 0.9537 - mDice: 0.6897 - val_loss: -2.5962e-01 - val_acc: 0.9511 - val_mDice: 0.2336

Epoch 00065: val_mDice did not improve from 0.24248
Epoch 66/300
 - 33s - loss: 0.1818 - acc: 0.9539 - mDice: 0.6921 - val_loss: -2.7103e-01 - val_acc: 0.9512 - val_mDice: 0.2391

Epoch 00066: val_mDice did not improve from 0.24248
Restoring model weights from the end of the best epoch
Epoch 00066: early stopping
{'val_loss': [0.726142137503821, 0.7036989188391315, 0.6941957966355253, 0.6473035088255386, 0.5679003182213661, 0.5609677055281056, 0.42747657025648544, 0.3462356637070371, 0.19071173123738244, 0.007997285944124884, 0.21240441565794393, -0.031535912212754084, 0.11870825964926689, -0.09472357220024116, -0.09142155988788432, -0.11650117375110546, -0.09726406960201657, -0.07419496563778928, -0.011973994045156585, -0.18370447412502666, -0.11539770899848505, -0.17322005176135485, -0.18293369836982123, -0.21160143427743214, -0.18392986836573802, -0.2007665818914086, -0.2137920841139795, -0.2043591027786909, -0.24951197836578878, -0.1606876117077248, -0.21905764145006942, -0.12004102267755949, -0.2334716295561761, -0.16766672795407536, -0.2462762869617417, -0.24456964076057938, -0.2478633446018558, -0.25827622184268206, -0.23080442832351838, -0.2284138237611075, -0.23048963219173685, -0.23796349168131667, -0.24361155390708653, -0.2602723146796473, -0.26530336296524515, -0.25769807943258405, -0.24552904512379164, -0.24280267311952825, -0.24516518604325915, -0.2409536976882926, -0.26284480311961705, -0.24171253683984525, -0.26068638885501494, -0.24905942540523435, -0.26385532182541266, -0.2472549728310305, -0.255533462559635, -0.2755584395990884, -0.23401713112661662, -0.2458737751320851, -0.2669214227560076, -0.269027677780279, -0.2753367728961647, -0.26289372595627447, -0.25962123053132996, -0.2710269566871657], 'val_acc': [0.9462097932484524, 0.9478223057817822, 0.9508221459782813, 0.9452032308933164, 0.9467577865301084, 0.9483752496971571, 0.9493851346417892, 0.94740675796162, 0.9492526921358976, 0.9485540508238737, 0.9470971729144577, 0.9435659026311449, 0.9461071698133611, 0.9503122331682315, 0.9514677997463006, 0.947967991356022, 0.9488934381934236, 0.9495159190548353, 0.945342301337187, 0.9511532547060123, 0.948078910181345, 0.9469249997257201, 0.94664686031578, 0.9487179443855916, 0.9471816051104838, 0.9507857206439184, 0.9490407798901077, 0.9493106386878274, 0.9511168293716493, 0.9457379707620164, 0.9494397689488309, 0.943241415437588, 0.9491550114529193, 0.9445608873997838, 0.9484563711260962, 0.951631692815418, 0.9519181079115749, 0.9507095695527132, 0.9494993686676025, 0.9503172010429635, 0.9508370397504696, 0.9484894837229705, 0.9504446786297255, 0.9514148279655078, 0.95125258510763, 0.9512443069584113, 0.9504976602625256, 0.9498685504779343, 0.9504893747243014, 0.9507244633249015, 0.9500192049121069, 0.9501450273616254, 0.9526796557686545, 0.9512261004487345, 0.9505059310227386, 0.9508436627624449, 0.9512658286685786, 0.9519065219508714, 0.9507161851756829, 0.9513154975638902, 0.9516532243775927, 0.9512691389430653, 0.9518651238157729, 0.9513303987250841, 0.951068818076583, 0.9511698085414476], 'val_mDice': [0.21505998555293754, 0.2335395269896373, 0.23530357817480388, 0.22221525315164534, 0.2278408574417603, 0.22714797037938408, 0.234359685978121, 0.23636257322120272, 0.24010902886425167, 0.23423386445222807, 0.23526828520554155, 0.2235324283646158, 0.22551916847544268, 0.23643287098851085, 0.23184902381059552, 0.22196442997160037, 0.22934118393531516, 0.2291738445108587, 0.22494434540675692, 0.23296964501053835, 0.23270649881648622, 0.21318676581313786, 0.22611181936727082, 0.23165340173589297, 0.2296608316996866, 0.24248383323515743, 0.22932513932551235, 0.22950743194207673, 0.22645690563050183, 0.2149993302839354, 0.22702164301448616, 0.19372393230884527, 0.22814440887329007, 0.20705822639721483, 0.22632202764680562, 0.22780302357821425, 0.2346063288282757, 0.22056109956965958, 0.23720255675764124, 0.22750528575467668, 0.24086399050044618, 0.21773429387364507, 0.23094100138742077, 0.24113566329902855, 0.2395912875567586, 0.2388439299769638, 0.2319814997517373, 0.23671855580461912, 0.23698351138140544, 0.23569302186985647, 0.22646979636643544, 0.22801172696361857, 0.23529074285641188, 0.2344565512227618, 0.2244432144298041, 0.23291274535754494, 0.2354835343262381, 0.23712344498427446, 0.2346921344249209, 0.2315608510547433, 0.23475358832227297, 0.23680411540895455, 0.23696671093790983, 0.2372554106222204, 0.23362054897487655, 0.23905781982851423], 'loss': [0.5361657262271928, 0.3966139527627743, 0.37015293331242227, 0.3536136224027778, 0.343180509600645, 0.3355670326944637, 0.3260175370390137, 0.3274029937720002, 0.3159747145333327, 0.3133909321170082, 0.30869732926836235, 0.30816299531359587, 0.30514232687268333, 0.30341942640999636, 0.300529390061798, 0.2968904497162557, 0.2985100790972961, 0.2949743255875815, 0.2948536000409163, 0.29006808801223855, 0.2938624141818324, 0.29843732573272846, 0.2787985685438606, 0.2918767652691046, 0.2547202342630421, 0.24925963114078617, 0.24060539162126102, 0.24017661756627998, 0.23773031496958968, 0.24006605249839377, 0.23094275589277466, 0.23604684919688895, 0.23610215251794572, 0.22278713334499692, 0.2308605146304031, 0.23101078228812164, 0.22686903985217766, 0.22285090340439245, 0.22164770930559805, 0.22833062948494778, 0.22407782812584345, 0.21489857839086288, 0.2088591190914825, 0.20463485694782618, 0.20365882320468431, 0.20000522394222497, 0.20137225285264926, 0.19940028594096607, 0.19744146211287023, 0.19814871126345301, 0.19873924160718337, 0.20373663079464183, 0.19385472091303826, 0.19997288079242642, 0.20027345644785427, 0.19437587103117387, 0.19222179356728922, 0.1913748564798576, 0.18691867245867122, 0.18744764218576318, 0.1896539861769203, 0.18773342213101846, 0.18387106428819533, 0.1861704651211644, 0.18415383863256896, 0.1817621886776331], 'acc': [0.9157923965446337, 0.9385424023764313, 0.9424057537243167, 0.9446232978446446, 0.9462954343741571, 0.9475695629879312, 0.9487424756804467, 0.9486797108249314, 0.9498944199600469, 0.9503271787247056, 0.9509530222719417, 0.9509535206359403, 0.9511518718011991, 0.9515283192806233, 0.9519246895684512, 0.9521610656738846, 0.9523777300045291, 0.9524760044418615, 0.9529040187262021, 0.9528923250995069, 0.9527989128343165, 0.9492805304084041, 0.9489928706642295, 0.9477001210159631, 0.9506993073906116, 0.9515938817431335, 0.9517371789317924, 0.9511979137249004, 0.9509606605242384, 0.9508418677261131, 0.9511165045788473, 0.9508134268528615, 0.9502556143673727, 0.9512681035758336, 0.9508633584648706, 0.9508575703915652, 0.9514066320047215, 0.9510204609221011, 0.9508249789670489, 0.9503768970092561, 0.9507724861903893, 0.9512945759783418, 0.9517084525985221, 0.9520643090905347, 0.9526495202325942, 0.9523390657171968, 0.9524999621565206, 0.952564292577966, 0.9526248037144586, 0.953069802146469, 0.9529699143606788, 0.9528939609276323, 0.9532095640633077, 0.9528399970208208, 0.9526428795199623, 0.9530635154621356, 0.9532284217871739, 0.9533032135119306, 0.9536010728847917, 0.9536539700131503, 0.9537020741375754, 0.9538279837444028, 0.9539073762842859, 0.9539016362405374, 0.9537310862484735, 0.9538509690105174], 'mDice': [0.4221030897474663, 0.572609825352016, 0.6011630259299857, 0.6190022325431052, 0.6302410394323867, 0.6384359691501581, 0.6487269770859119, 0.6472236278972519, 0.6595565724154067, 0.6623341508734643, 0.6673786038474404, 0.6679740915060608, 0.6712309832118403, 0.6730672842726473, 0.676177776295663, 0.6801249525157427, 0.6783459799539556, 0.6821804775014441, 0.6822850211016732, 0.687489289708784, 0.6830797373648576, 0.6534077076604198, 0.6411579100601486, 0.6285024242968584, 0.6633936137825586, 0.6678703453782185, 0.6706432776206904, 0.6623553722091086, 0.6598493476426411, 0.6556412491168829, 0.6620173977986816, 0.6520747766119266, 0.6514532382563065, 0.6653946147930558, 0.6530255826003739, 0.6581042298485782, 0.6651405196277399, 0.6607318497078174, 0.6578426355790499, 0.6508189217800074, 0.6568426539588779, 0.6610806507073329, 0.6648402277748566, 0.6732375772218863, 0.6780710879552286, 0.6734045287290926, 0.673744905182001, 0.6752498122698601, 0.6761049333763518, 0.6813805330216073, 0.6804345627025855, 0.6799927940553572, 0.6841739202442079, 0.675733958990211, 0.6775659187931221, 0.6816626962752932, 0.6867356593789541, 0.6877502614115031, 0.6903566721454042, 0.6890088054221929, 0.6939433220577635, 0.6879961986277779, 0.6926023133447566, 0.6928593339370933, 0.6896582818052485, 0.6920812277486156], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125]}
predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]predicting test subjects:  50%|█████     | 2/4 [00:02<00:02,  1.30s/it]predicting test subjects:  75%|███████▌  | 3/4 [00:03<00:01,  1.17s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]predicting test subjects: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:00<01:21,  3.27it/s]Loading train:   1%|          | 2/266 [00:00<01:17,  3.39it/s]Loading train:   1%|          | 3/266 [00:00<01:11,  3.65it/s]Loading train:   2%|▏         | 4/266 [00:01<01:08,  3.82it/s]Loading train:   2%|▏         | 5/266 [00:01<01:09,  3.78it/s]Loading train:   2%|▏         | 6/266 [00:01<01:09,  3.77it/s]Loading train:   3%|▎         | 7/266 [00:01<01:08,  3.77it/s]Loading train:   3%|▎         | 8/266 [00:02<01:07,  3.80it/s]Loading train:   3%|▎         | 9/266 [00:02<01:07,  3.80it/s]Loading train:   4%|▍         | 10/266 [00:02<01:06,  3.82it/s]Loading train:   4%|▍         | 11/266 [00:02<01:06,  3.83it/s]Loading train:   5%|▍         | 12/266 [00:03<01:06,  3.83it/s]Loading train:   5%|▍         | 13/266 [00:03<01:07,  3.76it/s]Loading train:   5%|▌         | 14/266 [00:03<01:06,  3.77it/s]Loading train:   6%|▌         | 15/266 [00:03<01:07,  3.75it/s]Loading train:   6%|▌         | 16/266 [00:04<01:06,  3.78it/s]Loading train:   6%|▋         | 17/266 [00:04<01:05,  3.81it/s]Loading train:   7%|▋         | 18/266 [00:04<01:05,  3.81it/s]Loading train:   7%|▋         | 19/266 [00:04<01:04,  3.83it/s]Loading train:   8%|▊         | 20/266 [00:05<01:04,  3.83it/s]Loading train:   8%|▊         | 21/266 [00:05<01:03,  3.83it/s]Loading train:   8%|▊         | 22/266 [00:05<01:04,  3.81it/s]Loading train:   9%|▊         | 23/266 [00:06<01:03,  3.81it/s]Loading train:   9%|▉         | 24/266 [00:06<01:02,  3.87it/s]Loading train:   9%|▉         | 25/266 [00:06<01:02,  3.88it/s]Loading train:  10%|▉         | 26/266 [00:06<01:01,  3.91it/s]Loading train:  10%|█         | 27/266 [00:07<01:01,  3.91it/s]Loading train:  11%|█         | 28/266 [00:07<01:01,  3.90it/s]Loading train:  11%|█         | 29/266 [00:07<01:01,  3.88it/s]Loading train:  11%|█▏        | 30/266 [00:07<01:02,  3.79it/s]Loading train:  12%|█▏        | 31/266 [00:08<01:01,  3.85it/s]Loading train:  12%|█▏        | 32/266 [00:08<01:00,  3.89it/s]Loading train:  12%|█▏        | 33/266 [00:08<00:58,  3.96it/s]Loading train:  13%|█▎        | 34/266 [00:08<00:58,  3.98it/s]Loading train:  13%|█▎        | 35/266 [00:09<00:57,  4.04it/s]Loading train:  14%|█▎        | 36/266 [00:09<00:56,  4.04it/s]Loading train:  14%|█▍        | 37/266 [00:09<00:57,  4.01it/s]Loading train:  14%|█▍        | 38/266 [00:09<00:57,  3.95it/s]Loading train:  15%|█▍        | 39/266 [00:10<00:57,  3.98it/s]Loading train:  15%|█▌        | 40/266 [00:10<00:56,  4.00it/s]Loading train:  15%|█▌        | 41/266 [00:10<00:56,  4.00it/s]Loading train:  16%|█▌        | 42/266 [00:10<00:53,  4.15it/s]Loading train:  16%|█▌        | 43/266 [00:11<00:51,  4.32it/s]Loading train:  17%|█▋        | 44/266 [00:11<00:49,  4.46it/s]Loading train:  17%|█▋        | 45/266 [00:11<00:48,  4.54it/s]Loading train:  17%|█▋        | 46/266 [00:11<00:47,  4.60it/s]Loading train:  18%|█▊        | 47/266 [00:11<00:46,  4.68it/s]Loading train:  18%|█▊        | 48/266 [00:12<00:46,  4.71it/s]Loading train:  18%|█▊        | 49/266 [00:12<00:45,  4.72it/s]Loading train:  19%|█▉        | 50/266 [00:12<00:46,  4.69it/s]Loading train:  19%|█▉        | 51/266 [00:12<00:47,  4.55it/s]Loading train:  20%|█▉        | 52/266 [00:12<00:46,  4.63it/s]Loading train:  20%|█▉        | 53/266 [00:13<00:45,  4.71it/s]Loading train:  20%|██        | 54/266 [00:13<00:44,  4.74it/s]Loading train:  21%|██        | 55/266 [00:13<00:43,  4.82it/s]Loading train:  21%|██        | 56/266 [00:13<00:43,  4.82it/s]Loading train:  21%|██▏       | 57/266 [00:13<00:42,  4.87it/s]Loading train:  22%|██▏       | 58/266 [00:14<00:42,  4.85it/s]Loading train:  22%|██▏       | 59/266 [00:14<00:42,  4.85it/s]Loading train:  23%|██▎       | 60/266 [00:14<00:44,  4.68it/s]Loading train:  23%|██▎       | 61/266 [00:14<00:45,  4.52it/s]Loading train:  23%|██▎       | 62/266 [00:15<00:45,  4.48it/s]Loading train:  24%|██▎       | 63/266 [00:15<00:45,  4.49it/s]Loading train:  24%|██▍       | 64/266 [00:15<00:44,  4.52it/s]Loading train:  24%|██▍       | 65/266 [00:15<00:44,  4.52it/s]Loading train:  25%|██▍       | 66/266 [00:15<00:44,  4.50it/s]Loading train:  25%|██▌       | 67/266 [00:16<00:44,  4.48it/s]Loading train:  26%|██▌       | 68/266 [00:16<00:44,  4.49it/s]Loading train:  26%|██▌       | 69/266 [00:16<00:44,  4.46it/s]Loading train:  26%|██▋       | 70/266 [00:16<00:44,  4.36it/s]Loading train:  27%|██▋       | 71/266 [00:17<00:44,  4.41it/s]Loading train:  27%|██▋       | 72/266 [00:17<00:43,  4.46it/s]Loading train:  27%|██▋       | 73/266 [00:17<00:42,  4.49it/s]Loading train:  28%|██▊       | 74/266 [00:17<00:42,  4.55it/s]Loading train:  28%|██▊       | 75/266 [00:17<00:41,  4.58it/s]Loading train:  29%|██▊       | 76/266 [00:18<00:41,  4.59it/s]Loading train:  29%|██▉       | 77/266 [00:18<00:41,  4.57it/s]Loading train:  29%|██▉       | 78/266 [00:18<00:43,  4.35it/s]Loading train:  30%|██▉       | 79/266 [00:18<00:43,  4.30it/s]Loading train:  30%|███       | 80/266 [00:19<00:43,  4.24it/s]Loading train:  30%|███       | 81/266 [00:19<00:44,  4.20it/s]Loading train:  31%|███       | 82/266 [00:19<00:44,  4.17it/s]Loading train:  31%|███       | 83/266 [00:19<00:43,  4.17it/s]Loading train:  32%|███▏      | 84/266 [00:20<00:44,  4.13it/s]Loading train:  32%|███▏      | 85/266 [00:20<00:43,  4.12it/s]Loading train:  32%|███▏      | 86/266 [00:20<00:43,  4.12it/s]Loading train:  33%|███▎      | 87/266 [00:20<00:43,  4.11it/s]Loading train:  33%|███▎      | 88/266 [00:21<00:43,  4.11it/s]Loading train:  33%|███▎      | 89/266 [00:21<00:42,  4.12it/s]Loading train:  34%|███▍      | 90/266 [00:21<00:42,  4.13it/s]Loading train:  34%|███▍      | 91/266 [00:21<00:42,  4.14it/s]Loading train:  35%|███▍      | 92/266 [00:22<00:41,  4.15it/s]Loading train:  35%|███▍      | 93/266 [00:22<00:41,  4.15it/s]Loading train:  35%|███▌      | 94/266 [00:22<00:41,  4.14it/s]Loading train:  36%|███▌      | 95/266 [00:22<00:41,  4.16it/s]Loading train:  36%|███▌      | 96/266 [00:22<00:40,  4.19it/s]Loading train:  36%|███▋      | 97/266 [00:23<00:42,  3.94it/s]Loading train:  37%|███▋      | 98/266 [00:23<00:42,  3.95it/s]Loading train:  37%|███▋      | 99/266 [00:23<00:40,  4.16it/s]Loading train:  38%|███▊      | 100/266 [00:23<00:39,  4.19it/s]Loading train:  38%|███▊      | 101/266 [00:24<00:38,  4.28it/s]Loading train:  38%|███▊      | 102/266 [00:24<00:37,  4.35it/s]Loading train:  39%|███▊      | 103/266 [00:24<00:37,  4.38it/s]Loading train:  39%|███▉      | 104/266 [00:24<00:37,  4.37it/s]Loading train:  39%|███▉      | 105/266 [00:25<00:37,  4.35it/s]Loading train:  40%|███▉      | 106/266 [00:25<00:36,  4.38it/s]Loading train:  40%|████      | 107/266 [00:25<00:35,  4.43it/s]Loading train:  41%|████      | 108/266 [00:25<00:35,  4.46it/s]Loading train:  41%|████      | 109/266 [00:25<00:34,  4.50it/s]Loading train:  41%|████▏     | 110/266 [00:26<00:34,  4.51it/s]Loading train:  42%|████▏     | 111/266 [00:26<00:34,  4.49it/s]Loading train:  42%|████▏     | 112/266 [00:26<00:33,  4.55it/s]Loading train:  42%|████▏     | 113/266 [00:26<00:33,  4.59it/s]Loading train:  43%|████▎     | 114/266 [00:27<00:33,  4.59it/s]Loading train:  43%|████▎     | 115/266 [00:27<00:32,  4.58it/s]Loading train:  44%|████▎     | 116/266 [00:27<00:32,  4.59it/s]Loading train:  44%|████▍     | 117/266 [00:27<00:33,  4.50it/s]Loading train:  44%|████▍     | 118/266 [00:27<00:32,  4.51it/s]Loading train:  45%|████▍     | 119/266 [00:28<00:34,  4.29it/s]Loading train:  45%|████▌     | 120/266 [00:28<00:34,  4.19it/s]Loading train:  45%|████▌     | 121/266 [00:28<00:35,  4.10it/s]Loading train:  46%|████▌     | 122/266 [00:28<00:36,  3.99it/s]Loading train:  46%|████▌     | 123/266 [00:29<00:36,  3.94it/s]Loading train:  47%|████▋     | 124/266 [00:29<00:36,  3.87it/s]Loading train:  47%|████▋     | 125/266 [00:29<00:36,  3.83it/s]Loading train:  47%|████▋     | 126/266 [00:30<00:37,  3.70it/s]Loading train:  48%|████▊     | 127/266 [00:30<00:36,  3.80it/s]Loading train:  48%|████▊     | 128/266 [00:30<00:36,  3.80it/s]Loading train:  48%|████▊     | 129/266 [00:30<00:35,  3.84it/s]Loading train:  49%|████▉     | 130/266 [00:31<00:35,  3.79it/s]Loading train:  49%|████▉     | 131/266 [00:31<00:35,  3.80it/s]Loading train:  50%|████▉     | 132/266 [00:31<00:35,  3.79it/s]Loading train:  50%|█████     | 133/266 [00:31<00:34,  3.86it/s]Loading train:  50%|█████     | 134/266 [00:32<00:34,  3.86it/s]Loading train:  51%|█████     | 135/266 [00:32<00:33,  3.90it/s]Loading train:  51%|█████     | 136/266 [00:32<00:33,  3.87it/s]Loading train:  52%|█████▏    | 137/266 [00:32<00:32,  4.03it/s]Loading train:  52%|█████▏    | 138/266 [00:33<00:31,  4.11it/s]Loading train:  52%|█████▏    | 139/266 [00:33<00:30,  4.18it/s]Loading train:  53%|█████▎    | 140/266 [00:33<00:29,  4.21it/s]Loading train:  53%|█████▎    | 141/266 [00:33<00:29,  4.23it/s]Loading train:  53%|█████▎    | 142/266 [00:34<00:29,  4.25it/s]Loading train:  54%|█████▍    | 143/266 [00:34<00:28,  4.27it/s]Loading train:  54%|█████▍    | 144/266 [00:34<00:28,  4.31it/s]Loading train:  55%|█████▍    | 145/266 [00:34<00:27,  4.34it/s]Loading train:  55%|█████▍    | 146/266 [00:34<00:27,  4.33it/s]Loading train:  55%|█████▌    | 147/266 [00:35<00:27,  4.29it/s]Loading train:  56%|█████▌    | 148/266 [00:35<00:27,  4.28it/s]Loading train:  56%|█████▌    | 149/266 [00:35<00:27,  4.21it/s]Loading train:  56%|█████▋    | 150/266 [00:35<00:28,  4.13it/s]Loading train:  57%|█████▋    | 151/266 [00:36<00:27,  4.18it/s]Loading train:  57%|█████▋    | 152/266 [00:36<00:27,  4.19it/s]Loading train:  58%|█████▊    | 153/266 [00:36<00:27,  4.18it/s]Loading train:  58%|█████▊    | 154/266 [00:36<00:26,  4.15it/s]Loading train:  58%|█████▊    | 155/266 [00:37<00:25,  4.39it/s]Loading train:  59%|█████▊    | 156/266 [00:37<00:23,  4.62it/s]Loading train:  59%|█████▉    | 157/266 [00:37<00:22,  4.76it/s]Loading train:  59%|█████▉    | 158/266 [00:37<00:22,  4.86it/s]Loading train:  60%|█████▉    | 159/266 [00:37<00:21,  4.96it/s]Loading train:  60%|██████    | 160/266 [00:38<00:21,  5.02it/s]Loading train:  61%|██████    | 161/266 [00:38<00:20,  5.07it/s]Loading train:  61%|██████    | 162/266 [00:38<00:20,  5.11it/s]Loading train:  61%|██████▏   | 163/266 [00:38<00:20,  5.12it/s]Loading train:  62%|██████▏   | 164/266 [00:38<00:19,  5.16it/s]Loading train:  62%|██████▏   | 165/266 [00:39<00:19,  5.15it/s]Loading train:  62%|██████▏   | 166/266 [00:39<00:19,  5.15it/s]Loading train:  63%|██████▎   | 167/266 [00:39<00:19,  5.16it/s]Loading train:  63%|██████▎   | 168/266 [00:39<00:19,  5.12it/s]Loading train:  64%|██████▎   | 169/266 [00:39<00:18,  5.13it/s]Loading train:  64%|██████▍   | 170/266 [00:39<00:18,  5.14it/s]Loading train:  64%|██████▍   | 171/266 [00:40<00:18,  5.12it/s]Loading train:  65%|██████▍   | 172/266 [00:40<00:18,  5.11it/s]Loading train:  65%|██████▌   | 173/266 [00:40<00:18,  4.93it/s]Loading train:  65%|██████▌   | 174/266 [00:40<00:19,  4.75it/s]Loading train:  66%|██████▌   | 175/266 [00:41<00:19,  4.70it/s]Loading train:  66%|██████▌   | 176/266 [00:41<00:19,  4.61it/s]Loading train:  67%|██████▋   | 177/266 [00:41<00:19,  4.61it/s]Loading train:  67%|██████▋   | 178/266 [00:41<00:19,  4.62it/s]Loading train:  67%|██████▋   | 179/266 [00:41<00:18,  4.61it/s]Loading train:  68%|██████▊   | 180/266 [00:42<00:18,  4.64it/s]Loading train:  68%|██████▊   | 181/266 [00:42<00:18,  4.66it/s]Loading train:  68%|██████▊   | 182/266 [00:42<00:18,  4.65it/s]Loading train:  69%|██████▉   | 183/266 [00:42<00:17,  4.65it/s]Loading train:  69%|██████▉   | 184/266 [00:43<00:17,  4.64it/s]Loading train:  70%|██████▉   | 185/266 [00:43<00:17,  4.61it/s]Loading train:  70%|██████▉   | 186/266 [00:43<00:17,  4.63it/s]Loading train:  70%|███████   | 187/266 [00:43<00:17,  4.64it/s]Loading train:  71%|███████   | 188/266 [00:43<00:16,  4.63it/s]Loading train:  71%|███████   | 189/266 [00:44<00:16,  4.56it/s]Loading train:  71%|███████▏  | 190/266 [00:44<00:16,  4.59it/s]Loading train:  72%|███████▏  | 191/266 [00:44<00:16,  4.47it/s]Loading train:  72%|███████▏  | 192/266 [00:44<00:16,  4.51it/s]Loading train:  73%|███████▎  | 193/266 [00:44<00:16,  4.47it/s]Loading train:  73%|███████▎  | 194/266 [00:45<00:16,  4.30it/s]Loading train:  73%|███████▎  | 195/266 [00:45<00:16,  4.41it/s]Loading train:  74%|███████▎  | 196/266 [00:45<00:15,  4.41it/s]Loading train:  74%|███████▍  | 197/266 [00:45<00:15,  4.40it/s]Loading train:  74%|███████▍  | 198/266 [00:46<00:15,  4.44it/s]Loading train:  75%|███████▍  | 199/266 [00:46<00:14,  4.48it/s]Loading train:  75%|███████▌  | 200/266 [00:46<00:14,  4.46it/s]Loading train:  76%|███████▌  | 201/266 [00:46<00:14,  4.43it/s]Loading train:  76%|███████▌  | 202/266 [00:47<00:14,  4.38it/s]Loading train:  76%|███████▋  | 203/266 [00:47<00:14,  4.42it/s]Loading train:  77%|███████▋  | 204/266 [00:47<00:13,  4.48it/s]Loading train:  77%|███████▋  | 205/266 [00:47<00:13,  4.49it/s]Loading train:  77%|███████▋  | 206/266 [00:47<00:13,  4.52it/s]Loading train:  78%|███████▊  | 207/266 [00:48<00:13,  4.54it/s]Loading train:  78%|███████▊  | 208/266 [00:48<00:12,  4.56it/s]Loading train:  79%|███████▊  | 209/266 [00:48<00:12,  4.57it/s]Loading train:  79%|███████▉  | 210/266 [00:48<00:12,  4.61it/s]Loading train:  79%|███████▉  | 211/266 [00:49<00:11,  4.63it/s]Loading train:  80%|███████▉  | 212/266 [00:49<00:11,  4.63it/s]Loading train:  80%|████████  | 213/266 [00:49<00:11,  4.65it/s]Loading train:  80%|████████  | 214/266 [00:49<00:11,  4.68it/s]Loading train:  81%|████████  | 215/266 [00:49<00:11,  4.62it/s]Loading train:  81%|████████  | 216/266 [00:50<00:10,  4.64it/s]Loading train:  82%|████████▏ | 217/266 [00:50<00:10,  4.68it/s]Loading train:  82%|████████▏ | 218/266 [00:50<00:10,  4.70it/s]Loading train:  82%|████████▏ | 219/266 [00:50<00:09,  4.73it/s]Loading train:  83%|████████▎ | 220/266 [00:50<00:09,  4.70it/s]Loading train:  83%|████████▎ | 221/266 [00:51<00:09,  4.70it/s]Loading train:  83%|████████▎ | 222/266 [00:51<00:09,  4.70it/s]Loading train:  84%|████████▍ | 223/266 [00:51<00:09,  4.68it/s]Loading train:  84%|████████▍ | 224/266 [00:51<00:09,  4.56it/s]Loading train:  85%|████████▍ | 225/266 [00:52<00:09,  4.51it/s]Loading train:  85%|████████▍ | 226/266 [00:52<00:08,  4.49it/s]Loading train:  85%|████████▌ | 227/266 [00:52<00:08,  4.55it/s]Loading train:  86%|████████▌ | 228/266 [00:52<00:08,  4.61it/s]Loading train:  86%|████████▌ | 229/266 [00:52<00:07,  4.66it/s]Loading train:  86%|████████▋ | 230/266 [00:53<00:07,  4.70it/s]Loading train:  87%|████████▋ | 231/266 [00:53<00:07,  4.61it/s]Loading train:  87%|████████▋ | 232/266 [00:53<00:07,  4.62it/s]Loading train:  88%|████████▊ | 233/266 [00:53<00:07,  4.54it/s]Loading train:  88%|████████▊ | 234/266 [00:53<00:07,  4.52it/s]Loading train:  88%|████████▊ | 235/266 [00:54<00:06,  4.51it/s]Loading train:  89%|████████▊ | 236/266 [00:54<00:06,  4.50it/s]Loading train:  89%|████████▉ | 237/266 [00:54<00:06,  4.49it/s]Loading train:  89%|████████▉ | 238/266 [00:54<00:06,  4.48it/s]Loading train:  90%|████████▉ | 239/266 [00:55<00:05,  4.52it/s]Loading train:  90%|█████████ | 240/266 [00:55<00:05,  4.48it/s]Loading train:  91%|█████████ | 241/266 [00:55<00:05,  4.52it/s]Loading train:  91%|█████████ | 242/266 [00:55<00:05,  4.54it/s]Loading train:  91%|█████████▏| 243/266 [00:55<00:05,  4.55it/s]Loading train:  92%|█████████▏| 244/266 [00:56<00:04,  4.55it/s]Loading train:  92%|█████████▏| 245/266 [00:56<00:04,  4.53it/s]Loading train:  92%|█████████▏| 246/266 [00:56<00:04,  4.55it/s]Loading train:  93%|█████████▎| 247/266 [00:56<00:04,  4.54it/s]Loading train:  93%|█████████▎| 248/266 [00:57<00:03,  4.51it/s]Loading train:  94%|█████████▎| 249/266 [00:57<00:03,  4.35it/s]Loading train:  94%|█████████▍| 250/266 [00:57<00:03,  4.25it/s]Loading train:  94%|█████████▍| 251/266 [00:57<00:03,  4.14it/s]Loading train:  95%|█████████▍| 252/266 [00:58<00:03,  4.03it/s]Loading train:  95%|█████████▌| 253/266 [00:58<00:03,  4.01it/s]Loading train:  95%|█████████▌| 254/266 [00:58<00:02,  4.06it/s]Loading train:  96%|█████████▌| 255/266 [00:58<00:02,  4.10it/s]Loading train:  96%|█████████▌| 256/266 [00:59<00:02,  4.16it/s]Loading train:  97%|█████████▋| 257/266 [00:59<00:02,  4.19it/s]Loading train:  97%|█████████▋| 258/266 [00:59<00:01,  4.22it/s]Loading train:  97%|█████████▋| 259/266 [00:59<00:01,  4.22it/s]Loading train:  98%|█████████▊| 260/266 [01:00<00:01,  4.24it/s]Loading train:  98%|█████████▊| 261/266 [01:00<00:01,  4.24it/s]Loading train:  98%|█████████▊| 262/266 [01:00<00:00,  4.26it/s]Loading train:  99%|█████████▉| 263/266 [01:00<00:00,  4.28it/s]Loading train:  99%|█████████▉| 264/266 [01:00<00:00,  4.27it/s]Loading train: 100%|█████████▉| 265/266 [01:01<00:00,  4.26it/s]Loading train: 100%|██████████| 266/266 [01:01<00:00,  4.26it/s]Loading train: 100%|██████████| 266/266 [01:01<00:00,  4.33it/s]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 6/266 [00:00<00:04, 53.67it/s]concatenating: train:   5%|▍         | 12/266 [00:00<00:04, 52.58it/s]concatenating: train:   7%|▋         | 18/266 [00:00<00:04, 53.07it/s]concatenating: train:   9%|▉         | 24/266 [00:00<00:04, 52.64it/s]concatenating: train:  11%|█         | 29/266 [00:00<00:04, 50.79it/s]concatenating: train:  13%|█▎        | 34/266 [00:00<00:04, 49.95it/s]concatenating: train:  15%|█▌        | 40/266 [00:00<00:04, 51.70it/s]concatenating: train:  17%|█▋        | 46/266 [00:00<00:04, 51.47it/s]concatenating: train:  20%|█▉        | 52/266 [00:01<00:04, 52.60it/s]concatenating: train:  22%|██▏       | 58/266 [00:01<00:04, 51.61it/s]concatenating: train:  24%|██▍       | 64/266 [00:01<00:03, 51.30it/s]concatenating: train:  26%|██▋       | 70/266 [00:01<00:03, 49.85it/s]concatenating: train:  28%|██▊       | 75/266 [00:01<00:03, 49.60it/s]concatenating: train:  30%|███       | 81/266 [00:01<00:03, 51.38it/s]concatenating: train:  33%|███▎      | 87/266 [00:01<00:03, 51.50it/s]concatenating: train:  35%|███▍      | 93/266 [00:01<00:03, 51.26it/s]concatenating: train:  37%|███▋      | 99/266 [00:01<00:03, 51.94it/s]concatenating: train:  40%|███▉      | 106/266 [00:02<00:02, 54.49it/s]concatenating: train:  42%|████▏     | 112/266 [00:02<00:02, 55.40it/s]concatenating: train:  45%|████▍     | 119/266 [00:02<00:02, 56.49it/s]concatenating: train:  47%|████▋     | 125/266 [00:02<00:02, 52.80it/s]concatenating: train:  49%|████▉     | 131/266 [00:02<00:02, 47.63it/s]concatenating: train:  51%|█████     | 136/266 [00:02<00:02, 46.88it/s]concatenating: train:  53%|█████▎    | 142/266 [00:02<00:02, 48.87it/s]concatenating: train:  56%|█████▌    | 149/266 [00:02<00:02, 51.61it/s]concatenating: train:  58%|█████▊    | 155/266 [00:02<00:02, 53.55it/s]concatenating: train:  61%|██████    | 162/266 [00:03<00:01, 56.03it/s]concatenating: train:  64%|██████▎   | 169/266 [00:03<00:01, 58.60it/s]concatenating: train:  66%|██████▌   | 175/266 [00:03<00:01, 58.69it/s]concatenating: train:  68%|██████▊   | 181/266 [00:03<00:01, 57.25it/s]concatenating: train:  70%|███████   | 187/266 [00:03<00:01, 56.27it/s]concatenating: train:  73%|███████▎  | 193/266 [00:03<00:01, 57.06it/s]concatenating: train:  75%|███████▍  | 199/266 [00:03<00:01, 57.90it/s]concatenating: train:  77%|███████▋  | 205/266 [00:03<00:01, 56.43it/s]concatenating: train:  79%|███████▉  | 211/266 [00:03<00:00, 55.30it/s]concatenating: train:  82%|████████▏ | 217/266 [00:04<00:00, 53.31it/s]concatenating: train:  84%|████████▍ | 223/266 [00:04<00:00, 51.21it/s]concatenating: train:  86%|████████▌ | 229/266 [00:04<00:00, 50.40it/s]concatenating: train:  88%|████████▊ | 235/266 [00:04<00:00, 50.54it/s]concatenating: train:  91%|█████████ | 241/266 [00:04<00:00, 52.07it/s]concatenating: train:  93%|█████████▎| 247/266 [00:04<00:00, 50.92it/s]concatenating: train:  95%|█████████▌| 253/266 [00:04<00:00, 46.90it/s]concatenating: train:  97%|█████████▋| 258/266 [00:04<00:00, 46.24it/s]concatenating: train:  99%|█████████▉| 264/266 [00:05<00:00, 47.09it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 52.07it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:00<00:00,  4.16it/s]Loading test:  50%|█████     | 2/4 [00:00<00:00,  4.21it/s]Loading test:  75%|███████▌  | 3/4 [00:00<00:00,  4.23it/s]Loading test: 100%|██████████| 4/4 [00:00<00:00,  4.23it/s]Loading test: 100%|██████████| 4/4 [00:00<00:00,  4.25it/s]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 382.27it/s]
Loading trainS:   0%|          | 0/266 [00:00<?, ?it/s]Loading trainS:   0%|          | 1/266 [00:00<01:14,  3.55it/s]Loading trainS:   1%|          | 2/266 [00:00<01:14,  3.53it/s]Loading trainS:   1%|          | 3/266 [00:00<01:09,  3.78it/s]Loading trainS:   2%|▏         | 4/266 [00:01<01:11,  3.67it/s]Loading trainS:   2%|▏         | 5/266 [00:01<01:12,  3.62it/s]Loading trainS:   2%|▏         | 6/266 [00:01<01:10,  3.69it/s]Loading trainS:   3%|▎         | 7/266 [00:01<01:08,  3.76it/s]Loading trainS:   3%|▎         | 8/266 [00:02<01:08,  3.79it/s]Loading trainS:   3%|▎         | 9/266 [00:02<01:07,  3.81it/s]Loading trainS:   4%|▍         | 10/266 [00:02<01:06,  3.84it/s]Loading trainS:   4%|▍         | 11/266 [00:02<01:07,  3.79it/s]Loading trainS:   5%|▍         | 12/266 [00:03<01:06,  3.82it/s]Loading trainS:   5%|▍         | 13/266 [00:03<01:05,  3.83it/s]Loading trainS:   5%|▌         | 14/266 [00:03<01:05,  3.85it/s]Loading trainS:   6%|▌         | 15/266 [00:03<01:04,  3.87it/s]Loading trainS:   6%|▌         | 16/266 [00:04<01:04,  3.88it/s]Loading trainS:   6%|▋         | 17/266 [00:04<01:04,  3.87it/s]Loading trainS:   7%|▋         | 18/266 [00:04<01:04,  3.83it/s]Loading trainS:   7%|▋         | 19/266 [00:04<01:04,  3.86it/s]Loading trainS:   8%|▊         | 20/266 [00:05<01:03,  3.87it/s]Loading trainS:   8%|▊         | 21/266 [00:05<01:03,  3.84it/s]Loading trainS:   8%|▊         | 22/266 [00:05<01:03,  3.84it/s]Loading trainS:   9%|▊         | 23/266 [00:06<01:03,  3.85it/s]Loading trainS:   9%|▉         | 24/266 [00:06<01:01,  3.91it/s]Loading trainS:   9%|▉         | 25/266 [00:06<01:00,  3.96it/s]Loading trainS:  10%|▉         | 26/266 [00:06<01:00,  3.97it/s]Loading trainS:  10%|█         | 27/266 [00:07<01:00,  3.97it/s]Loading trainS:  11%|█         | 28/266 [00:07<00:59,  4.00it/s]Loading trainS:  11%|█         | 29/266 [00:07<00:58,  4.02it/s]Loading trainS:  11%|█▏        | 30/266 [00:07<00:58,  4.05it/s]Loading trainS:  12%|█▏        | 31/266 [00:08<00:58,  4.05it/s]Loading trainS:  12%|█▏        | 32/266 [00:08<00:57,  4.04it/s]Loading trainS:  12%|█▏        | 33/266 [00:08<00:57,  4.06it/s]Loading trainS:  13%|█▎        | 34/266 [00:08<00:57,  4.04it/s]Loading trainS:  13%|█▎        | 35/266 [00:09<00:58,  3.98it/s]Loading trainS:  14%|█▎        | 36/266 [00:09<00:58,  3.91it/s]Loading trainS:  14%|█▍        | 37/266 [00:09<00:57,  3.96it/s]Loading trainS:  14%|█▍        | 38/266 [00:09<00:57,  4.00it/s]Loading trainS:  15%|█▍        | 39/266 [00:10<00:56,  4.03it/s]Loading trainS:  15%|█▌        | 40/266 [00:10<00:55,  4.05it/s]Loading trainS:  15%|█▌        | 41/266 [00:10<00:55,  4.07it/s]Loading trainS:  16%|█▌        | 42/266 [00:10<00:52,  4.29it/s]Loading trainS:  16%|█▌        | 43/266 [00:10<00:49,  4.46it/s]Loading trainS:  17%|█▋        | 44/266 [00:11<00:48,  4.58it/s]Loading trainS:  17%|█▋        | 45/266 [00:11<00:47,  4.67it/s]Loading trainS:  17%|█▋        | 46/266 [00:11<00:46,  4.73it/s]Loading trainS:  18%|█▊        | 47/266 [00:11<00:45,  4.78it/s]Loading trainS:  18%|█▊        | 48/266 [00:11<00:45,  4.79it/s]Loading trainS:  18%|█▊        | 49/266 [00:12<00:45,  4.77it/s]Loading trainS:  19%|█▉        | 50/266 [00:12<00:45,  4.77it/s]Loading trainS:  19%|█▉        | 51/266 [00:12<00:45,  4.70it/s]Loading trainS:  20%|█▉        | 52/266 [00:12<00:45,  4.72it/s]Loading trainS:  20%|█▉        | 53/266 [00:12<00:45,  4.71it/s]Loading trainS:  20%|██        | 54/266 [00:13<00:44,  4.73it/s]Loading trainS:  21%|██        | 55/266 [00:13<00:44,  4.70it/s]Loading trainS:  21%|██        | 56/266 [00:13<00:44,  4.73it/s]Loading trainS:  21%|██▏       | 57/266 [00:13<00:44,  4.74it/s]Loading trainS:  22%|██▏       | 58/266 [00:14<00:44,  4.71it/s]Loading trainS:  22%|██▏       | 59/266 [00:14<00:44,  4.64it/s]Loading trainS:  23%|██▎       | 60/266 [00:14<00:45,  4.54it/s]Loading trainS:  23%|██▎       | 61/266 [00:14<00:45,  4.48it/s]Loading trainS:  23%|██▎       | 62/266 [00:14<00:45,  4.50it/s]Loading trainS:  24%|██▎       | 63/266 [00:15<00:44,  4.54it/s]Loading trainS:  24%|██▍       | 64/266 [00:15<00:44,  4.56it/s]Loading trainS:  24%|██▍       | 65/266 [00:15<00:43,  4.59it/s]Loading trainS:  25%|██▍       | 66/266 [00:15<00:43,  4.57it/s]Loading trainS:  25%|██▌       | 67/266 [00:16<00:43,  4.60it/s]Loading trainS:  26%|██▌       | 68/266 [00:16<00:43,  4.60it/s]Loading trainS:  26%|██▌       | 69/266 [00:16<00:42,  4.62it/s]Loading trainS:  26%|██▋       | 70/266 [00:16<00:42,  4.64it/s]Loading trainS:  27%|██▋       | 71/266 [00:16<00:41,  4.65it/s]Loading trainS:  27%|██▋       | 72/266 [00:17<00:41,  4.65it/s]Loading trainS:  27%|██▋       | 73/266 [00:17<00:41,  4.65it/s]Loading trainS:  28%|██▊       | 74/266 [00:17<00:41,  4.65it/s]Loading trainS:  28%|██▊       | 75/266 [00:17<00:41,  4.65it/s]Loading trainS:  29%|██▊       | 76/266 [00:17<00:40,  4.64it/s]Loading trainS:  29%|██▉       | 77/266 [00:18<00:40,  4.63it/s]Loading trainS:  29%|██▉       | 78/266 [00:18<00:42,  4.46it/s]Loading trainS:  30%|██▉       | 79/266 [00:18<00:42,  4.35it/s]Loading trainS:  30%|███       | 80/266 [00:18<00:43,  4.26it/s]Loading trainS:  30%|███       | 81/266 [00:19<00:44,  4.20it/s]Loading trainS:  31%|███       | 82/266 [00:19<00:44,  4.15it/s]Loading trainS:  31%|███       | 83/266 [00:19<00:44,  4.13it/s]Loading trainS:  32%|███▏      | 84/266 [00:19<00:45,  4.02it/s]Loading trainS:  32%|███▏      | 85/266 [00:20<00:45,  4.00it/s]Loading trainS:  32%|███▏      | 86/266 [00:20<00:44,  4.00it/s]Loading trainS:  33%|███▎      | 87/266 [00:20<00:44,  3.99it/s]Loading trainS:  33%|███▎      | 88/266 [00:20<00:44,  3.96it/s]Loading trainS:  33%|███▎      | 89/266 [00:21<00:46,  3.83it/s]Loading trainS:  34%|███▍      | 90/266 [00:21<00:45,  3.88it/s]Loading trainS:  34%|███▍      | 91/266 [00:21<00:45,  3.86it/s]Loading trainS:  35%|███▍      | 92/266 [00:21<00:44,  3.94it/s]Loading trainS:  35%|███▍      | 93/266 [00:22<00:43,  3.98it/s]Loading trainS:  35%|███▌      | 94/266 [00:22<00:42,  4.00it/s]Loading trainS:  36%|███▌      | 95/266 [00:22<00:42,  4.04it/s]Loading trainS:  36%|███▌      | 96/266 [00:22<00:41,  4.12it/s]Loading trainS:  36%|███▋      | 97/266 [00:23<00:42,  3.97it/s]Loading trainS:  37%|███▋      | 98/266 [00:23<00:41,  4.02it/s]Loading trainS:  37%|███▋      | 99/266 [00:23<00:39,  4.26it/s]Loading trainS:  38%|███▊      | 100/266 [00:23<00:38,  4.27it/s]Loading trainS:  38%|███▊      | 101/266 [00:24<00:37,  4.37it/s]Loading trainS:  38%|███▊      | 102/266 [00:24<00:37,  4.43it/s]Loading trainS:  39%|███▊      | 103/266 [00:24<00:37,  4.36it/s]Loading trainS:  39%|███▉      | 104/266 [00:24<00:36,  4.39it/s]Loading trainS:  39%|███▉      | 105/266 [00:25<00:36,  4.40it/s]Loading trainS:  40%|███▉      | 106/266 [00:25<00:36,  4.43it/s]Loading trainS:  40%|████      | 107/266 [00:25<00:35,  4.42it/s]Loading trainS:  41%|████      | 108/266 [00:25<00:35,  4.47it/s]Loading trainS:  41%|████      | 109/266 [00:25<00:35,  4.46it/s]Loading trainS:  41%|████▏     | 110/266 [00:26<00:35,  4.45it/s]Loading trainS:  42%|████▏     | 111/266 [00:26<00:35,  4.40it/s]Loading trainS:  42%|████▏     | 112/266 [00:26<00:34,  4.41it/s]Loading trainS:  42%|████▏     | 113/266 [00:26<00:34,  4.40it/s]Loading trainS:  43%|████▎     | 114/266 [00:27<00:34,  4.41it/s]Loading trainS:  43%|████▎     | 115/266 [00:27<00:34,  4.42it/s]Loading trainS:  44%|████▎     | 116/266 [00:27<00:33,  4.45it/s]Loading trainS:  44%|████▍     | 117/266 [00:27<00:33,  4.48it/s]Loading trainS:  44%|████▍     | 118/266 [00:27<00:33,  4.48it/s]Loading trainS:  45%|████▍     | 119/266 [00:28<00:34,  4.25it/s]Loading trainS:  45%|████▌     | 120/266 [00:28<00:36,  4.01it/s]Loading trainS:  45%|████▌     | 121/266 [00:28<00:36,  3.98it/s]Loading trainS:  46%|████▌     | 122/266 [00:28<00:36,  3.98it/s]Loading trainS:  46%|████▌     | 123/266 [00:29<00:36,  3.96it/s]Loading trainS:  47%|████▋     | 124/266 [00:29<00:35,  3.95it/s]Loading trainS:  47%|████▋     | 125/266 [00:29<00:36,  3.90it/s]Loading trainS:  47%|████▋     | 126/266 [00:30<00:36,  3.84it/s]Loading trainS:  48%|████▊     | 127/266 [00:30<00:36,  3.80it/s]Loading trainS:  48%|████▊     | 128/266 [00:30<00:36,  3.82it/s]Loading trainS:  48%|████▊     | 129/266 [00:30<00:35,  3.82it/s]Loading trainS:  49%|████▉     | 130/266 [00:31<00:35,  3.84it/s]Loading trainS:  49%|████▉     | 131/266 [00:31<00:35,  3.85it/s]Loading trainS:  50%|████▉     | 132/266 [00:31<00:34,  3.88it/s]Loading trainS:  50%|█████     | 133/266 [00:31<00:34,  3.88it/s]Loading trainS:  50%|█████     | 134/266 [00:32<00:33,  3.90it/s]Loading trainS:  51%|█████     | 135/266 [00:32<00:33,  3.90it/s]Loading trainS:  51%|█████     | 136/266 [00:32<00:33,  3.92it/s]Loading trainS:  52%|█████▏    | 137/266 [00:32<00:31,  4.04it/s]Loading trainS:  52%|█████▏    | 138/266 [00:33<00:30,  4.14it/s]Loading trainS:  52%|█████▏    | 139/266 [00:33<00:30,  4.23it/s]Loading trainS:  53%|█████▎    | 140/266 [00:33<00:29,  4.29it/s]Loading trainS:  53%|█████▎    | 141/266 [00:33<00:28,  4.32it/s]Loading trainS:  53%|█████▎    | 142/266 [00:33<00:28,  4.35it/s]Loading trainS:  54%|█████▍    | 143/266 [00:34<00:28,  4.37it/s]Loading trainS:  54%|█████▍    | 144/266 [00:34<00:27,  4.38it/s]Loading trainS:  55%|█████▍    | 145/266 [00:34<00:27,  4.40it/s]Loading trainS:  55%|█████▍    | 146/266 [00:34<00:27,  4.40it/s]Loading trainS:  55%|█████▌    | 147/266 [00:35<00:27,  4.40it/s]Loading trainS:  56%|█████▌    | 148/266 [00:35<00:26,  4.41it/s]Loading trainS:  56%|█████▌    | 149/266 [00:35<00:26,  4.40it/s]Loading trainS:  56%|█████▋    | 150/266 [00:35<00:26,  4.40it/s]Loading trainS:  57%|█████▋    | 151/266 [00:36<00:26,  4.39it/s]Loading trainS:  57%|█████▋    | 152/266 [00:36<00:25,  4.41it/s]Loading trainS:  58%|█████▊    | 153/266 [00:36<00:25,  4.40it/s]Loading trainS:  58%|█████▊    | 154/266 [00:36<00:25,  4.41it/s]Loading trainS:  58%|█████▊    | 155/266 [00:36<00:24,  4.62it/s]Loading trainS:  59%|█████▊    | 156/266 [00:37<00:22,  4.81it/s]Loading trainS:  59%|█████▉    | 157/266 [00:37<00:22,  4.88it/s]Loading trainS:  59%|█████▉    | 158/266 [00:37<00:21,  4.99it/s]Loading trainS:  60%|█████▉    | 159/266 [00:37<00:21,  5.04it/s]Loading trainS:  60%|██████    | 160/266 [00:37<00:20,  5.10it/s]Loading trainS:  61%|██████    | 161/266 [00:38<00:20,  5.13it/s]Loading trainS:  61%|██████    | 162/266 [00:38<00:20,  5.15it/s]Loading trainS:  61%|██████▏   | 163/266 [00:38<00:20,  5.14it/s]Loading trainS:  62%|██████▏   | 164/266 [00:38<00:19,  5.14it/s]Loading trainS:  62%|██████▏   | 165/266 [00:38<00:19,  5.12it/s]Loading trainS:  62%|██████▏   | 166/266 [00:39<00:19,  5.12it/s]Loading trainS:  63%|██████▎   | 167/266 [00:39<00:19,  5.15it/s]Loading trainS:  63%|██████▎   | 168/266 [00:39<00:18,  5.18it/s]Loading trainS:  64%|██████▎   | 169/266 [00:39<00:18,  5.16it/s]Loading trainS:  64%|██████▍   | 170/266 [00:39<00:18,  5.17it/s]Loading trainS:  64%|██████▍   | 171/266 [00:39<00:18,  5.19it/s]Loading trainS:  65%|██████▍   | 172/266 [00:40<00:18,  5.17it/s]Loading trainS:  65%|██████▌   | 173/266 [00:40<00:18,  4.99it/s]Loading trainS:  65%|██████▌   | 174/266 [00:40<00:18,  4.89it/s]Loading trainS:  66%|██████▌   | 175/266 [00:40<00:18,  4.82it/s]Loading trainS:  66%|██████▌   | 176/266 [00:41<00:18,  4.77it/s]Loading trainS:  67%|██████▋   | 177/266 [00:41<00:18,  4.71it/s]Loading trainS:  67%|██████▋   | 178/266 [00:41<00:18,  4.70it/s]Loading trainS:  67%|██████▋   | 179/266 [00:41<00:18,  4.66it/s]Loading trainS:  68%|██████▊   | 180/266 [00:41<00:18,  4.65it/s]Loading trainS:  68%|██████▊   | 181/266 [00:42<00:18,  4.60it/s]Loading trainS:  68%|██████▊   | 182/266 [00:42<00:18,  4.59it/s]Loading trainS:  69%|██████▉   | 183/266 [00:42<00:18,  4.57it/s]Loading trainS:  69%|██████▉   | 184/266 [00:42<00:17,  4.58it/s]Loading trainS:  70%|██████▉   | 185/266 [00:42<00:17,  4.55it/s]Loading trainS:  70%|██████▉   | 186/266 [00:43<00:17,  4.57it/s]Loading trainS:  70%|███████   | 187/266 [00:43<00:17,  4.56it/s]Loading trainS:  71%|███████   | 188/266 [00:43<00:17,  4.56it/s]Loading trainS:  71%|███████   | 189/266 [00:43<00:16,  4.60it/s]Loading trainS:  71%|███████▏  | 190/266 [00:44<00:16,  4.65it/s]Loading trainS:  72%|███████▏  | 191/266 [00:44<00:16,  4.57it/s]Loading trainS:  72%|███████▏  | 192/266 [00:44<00:16,  4.59it/s]Loading trainS:  73%|███████▎  | 193/266 [00:44<00:16,  4.53it/s]Loading trainS:  73%|███████▎  | 194/266 [00:45<00:16,  4.28it/s]Loading trainS:  73%|███████▎  | 195/266 [00:45<00:16,  4.38it/s]Loading trainS:  74%|███████▎  | 196/266 [00:45<00:15,  4.47it/s]Loading trainS:  74%|███████▍  | 197/266 [00:45<00:15,  4.55it/s]Loading trainS:  74%|███████▍  | 198/266 [00:45<00:14,  4.58it/s]Loading trainS:  75%|███████▍  | 199/266 [00:46<00:14,  4.61it/s]Loading trainS:  75%|███████▌  | 200/266 [00:46<00:14,  4.62it/s]Loading trainS:  76%|███████▌  | 201/266 [00:46<00:14,  4.61it/s]Loading trainS:  76%|███████▌  | 202/266 [00:46<00:13,  4.60it/s]Loading trainS:  76%|███████▋  | 203/266 [00:46<00:13,  4.62it/s]Loading trainS:  77%|███████▋  | 204/266 [00:47<00:13,  4.63it/s]Loading trainS:  77%|███████▋  | 205/266 [00:47<00:13,  4.59it/s]Loading trainS:  77%|███████▋  | 206/266 [00:47<00:13,  4.56it/s]Loading trainS:  78%|███████▊  | 207/266 [00:47<00:12,  4.55it/s]Loading trainS:  78%|███████▊  | 208/266 [00:48<00:12,  4.57it/s]Loading trainS:  79%|███████▊  | 209/266 [00:48<00:12,  4.59it/s]Loading trainS:  79%|███████▉  | 210/266 [00:48<00:12,  4.61it/s]Loading trainS:  79%|███████▉  | 211/266 [00:48<00:11,  4.59it/s]Loading trainS:  80%|███████▉  | 212/266 [00:48<00:11,  4.60it/s]Loading trainS:  80%|████████  | 213/266 [00:49<00:11,  4.65it/s]Loading trainS:  80%|████████  | 214/266 [00:49<00:11,  4.69it/s]Loading trainS:  81%|████████  | 215/266 [00:49<00:11,  4.64it/s]Loading trainS:  81%|████████  | 216/266 [00:49<00:10,  4.57it/s]Loading trainS:  82%|████████▏ | 217/266 [00:50<00:10,  4.53it/s]Loading trainS:  82%|████████▏ | 218/266 [00:50<00:10,  4.57it/s]Loading trainS:  82%|████████▏ | 219/266 [00:50<00:10,  4.64it/s]Loading trainS:  83%|████████▎ | 220/266 [00:50<00:09,  4.69it/s]Loading trainS:  83%|████████▎ | 221/266 [00:50<00:09,  4.73it/s]Loading trainS:  83%|████████▎ | 222/266 [00:51<00:09,  4.72it/s]Loading trainS:  84%|████████▍ | 223/266 [00:51<00:09,  4.74it/s]Loading trainS:  84%|████████▍ | 224/266 [00:51<00:08,  4.74it/s]Loading trainS:  85%|████████▍ | 225/266 [00:51<00:08,  4.75it/s]Loading trainS:  85%|████████▍ | 226/266 [00:51<00:08,  4.77it/s]Loading trainS:  85%|████████▌ | 227/266 [00:52<00:08,  4.75it/s]Loading trainS:  86%|████████▌ | 228/266 [00:52<00:07,  4.76it/s]Loading trainS:  86%|████████▌ | 229/266 [00:52<00:07,  4.75it/s]Loading trainS:  86%|████████▋ | 230/266 [00:52<00:07,  4.76it/s]Loading trainS:  87%|████████▋ | 231/266 [00:52<00:07,  4.70it/s]Loading trainS:  87%|████████▋ | 232/266 [00:53<00:07,  4.67it/s]Loading trainS:  88%|████████▊ | 233/266 [00:53<00:07,  4.65it/s]Loading trainS:  88%|████████▊ | 234/266 [00:53<00:06,  4.63it/s]Loading trainS:  88%|████████▊ | 235/266 [00:53<00:06,  4.63it/s]Loading trainS:  89%|████████▊ | 236/266 [00:54<00:06,  4.63it/s]Loading trainS:  89%|████████▉ | 237/266 [00:54<00:06,  4.61it/s]Loading trainS:  89%|████████▉ | 238/266 [00:54<00:06,  4.61it/s]Loading trainS:  90%|████████▉ | 239/266 [00:54<00:05,  4.61it/s]Loading trainS:  90%|█████████ | 240/266 [00:54<00:05,  4.61it/s]Loading trainS:  91%|█████████ | 241/266 [00:55<00:05,  4.61it/s]Loading trainS:  91%|█████████ | 242/266 [00:55<00:05,  4.61it/s]Loading trainS:  91%|█████████▏| 243/266 [00:55<00:04,  4.61it/s]Loading trainS:  92%|█████████▏| 244/266 [00:55<00:04,  4.60it/s]Loading trainS:  92%|█████████▏| 245/266 [00:55<00:04,  4.61it/s]Loading trainS:  92%|█████████▏| 246/266 [00:56<00:04,  4.61it/s]Loading trainS:  93%|█████████▎| 247/266 [00:56<00:04,  4.62it/s]Loading trainS:  93%|█████████▎| 248/266 [00:56<00:03,  4.61it/s]Loading trainS:  94%|█████████▎| 249/266 [00:56<00:03,  4.48it/s]Loading trainS:  94%|█████████▍| 250/266 [00:57<00:03,  4.39it/s]Loading trainS:  94%|█████████▍| 251/266 [00:57<00:03,  4.33it/s]Loading trainS:  95%|█████████▍| 252/266 [00:57<00:03,  4.29it/s]Loading trainS:  95%|█████████▌| 253/266 [00:57<00:03,  4.27it/s]Loading trainS:  95%|█████████▌| 254/266 [00:58<00:02,  4.25it/s]Loading trainS:  96%|█████████▌| 255/266 [00:58<00:02,  4.23it/s]Loading trainS:  96%|█████████▌| 256/266 [00:58<00:02,  4.22it/s]Loading trainS:  97%|█████████▋| 257/266 [00:58<00:02,  4.20it/s]Loading trainS:  97%|█████████▋| 258/266 [00:59<00:01,  4.18it/s]Loading trainS:  97%|█████████▋| 259/266 [00:59<00:01,  4.18it/s]Loading trainS:  98%|█████████▊| 260/266 [00:59<00:01,  4.16it/s]Loading trainS:  98%|█████████▊| 261/266 [00:59<00:01,  4.07it/s]Loading trainS:  98%|█████████▊| 262/266 [01:00<00:00,  4.05it/s]Loading trainS:  99%|█████████▉| 263/266 [01:00<00:00,  4.07it/s]Loading trainS:  99%|█████████▉| 264/266 [01:00<00:00,  4.09it/s]Loading trainS: 100%|█████████▉| 265/266 [01:00<00:00,  4.10it/s]Loading trainS: 100%|██████████| 266/266 [01:00<00:00,  4.11it/s]Loading trainS: 100%|██████████| 266/266 [01:00<00:00,  4.36it/s]
Loading testS:   0%|          | 0/4 [00:00<?, ?it/s]Loading testS:  25%|██▌       | 1/4 [00:00<00:00,  4.09it/s]Loading testS:  50%|█████     | 2/4 [00:00<00:00,  4.17it/s]Loading testS:  75%|███████▌  | 3/4 [00:00<00:00,  4.17it/s]Loading testS: 100%|██████████| 4/4 [00:00<00:00,  4.15it/s]Loading testS: 100%|██████████| 4/4 [00:00<00:00,  4.18it/s]
---------------------- check Layers Step ------------------------------
 N: [1]  | GPU: 2  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [1]  | GPU: 2  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 2.0      1-THALAMUS
Error in label values min 0.0 max 2.0      1-THALAMUS
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 108, 116, 1)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 108, 116, 20) 200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 108, 116, 20) 80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 108, 116, 20) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 108, 116, 20) 3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 108, 116, 20) 80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 108, 116, 20) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 54, 58, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 54, 58, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 54, 58, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 54, 58, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 54, 58, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 54, 58, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 54, 58, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 58, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 54, 58, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 27, 29, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 27, 29, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 27, 29, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 27, 29, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 27, 29, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 27, 29, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 27, 29, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 27, 29, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 27, 29, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 27, 29, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 54, 58, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 58, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 54, 58, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 54, 58, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 54, 58, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 54, 58, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 54, 58, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 58, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 54, 58, 140)  0           concatenate_3[0][0]              
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________2020-01-21 02:52:45.513025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 02:52:45.513118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 02:52:45.513131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 02:52:45.513138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 02:52:45.513447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)

dropout_4 (Dropout)             (None, 54, 58, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 108, 116, 20) 11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 108, 116, 40) 0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 108, 116, 20) 7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 108, 116, 20) 80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 108, 116, 20) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 108, 116, 20) 3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 108, 116, 20) 80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 108, 116, 20) 0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 108, 116, 60) 0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 108, 116, 60) 0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 108, 116, 2)  122         dropout_5[0][0]                  
==================================================================================================
Total params: 223,162
Trainable params: 222,362
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [0.97286605 0.02713395]
Train on 17214 samples, validate on 256 samples
Epoch 1/300
 - 49s - loss: 0.1327 - acc: 0.9834 - mDice: 0.7460 - val_loss: 0.3039 - val_acc: 0.9907 - val_mDice: 0.3972

Epoch 00001: val_mDice improved from -inf to 0.39720, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 2/300
 - 45s - loss: 0.0854 - acc: 0.9911 - mDice: 0.8342 - val_loss: 0.2865 - val_acc: 0.9929 - val_mDice: 0.4299

Epoch 00002: val_mDice improved from 0.39720 to 0.42995, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 3/300
 - 45s - loss: 0.0756 - acc: 0.9920 - mDice: 0.8530 - val_loss: 0.2814 - val_acc: 0.9913 - val_mDice: 0.4387

Epoch 00003: val_mDice improved from 0.42995 to 0.43872, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 4/300
 - 44s - loss: 0.0699 - acc: 0.9926 - mDice: 0.8640 - val_loss: 0.2664 - val_acc: 0.9933 - val_mDice: 0.4578

Epoch 00004: val_mDice improved from 0.43872 to 0.45782, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 5/300
 - 44s - loss: 0.0663 - acc: 0.9930 - mDice: 0.8711 - val_loss: 0.1777 - val_acc: 0.9931 - val_mDice: 0.4584

Epoch 00005: val_mDice improved from 0.45782 to 0.45841, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 6/300
 - 45s - loss: 0.0620 - acc: 0.9934 - mDice: 0.8794 - val_loss: 0.2553 - val_acc: 0.9922 - val_mDice: 0.4434

Epoch 00006: val_mDice did not improve from 0.45841
Epoch 7/300
 - 49s - loss: 0.0601 - acc: 0.9936 - mDice: 0.8832 - val_loss: 0.2670 - val_acc: 0.9928 - val_mDice: 0.4622

Epoch 00007: val_mDice improved from 0.45841 to 0.46218, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 8/300
 - 60s - loss: 0.0574 - acc: 0.9937 - mDice: 0.8884 - val_loss: 0.1804 - val_acc: 0.9929 - val_mDice: 0.4770

Epoch 00008: val_mDice improved from 0.46218 to 0.47703, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 9/300
 - 63s - loss: 0.0559 - acc: 0.9939 - mDice: 0.8913 - val_loss: 0.0591 - val_acc: 0.9936 - val_mDice: 0.4884

Epoch 00009: val_mDice improved from 0.47703 to 0.48842, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 10/300
 - 63s - loss: 0.0566 - acc: 0.9940 - mDice: 0.8898 - val_loss: 0.3038 - val_acc: 0.9808 - val_mDice: 0.4033

Epoch 00010: val_mDice did not improve from 0.48842
Epoch 11/300
 - 64s - loss: 0.0519 - acc: 0.9942 - mDice: 0.8992 - val_loss: 0.0505 - val_acc: 0.9928 - val_mDice: 0.4595

Epoch 00011: val_mDice did not improve from 0.48842
Epoch 12/300
 - 63s - loss: 0.0546 - acc: 0.9941 - mDice: 0.8937 - val_loss: 0.0538 - val_acc: 0.9941 - val_mDice: 0.5014

Epoch 00012: val_mDice improved from 0.48842 to 0.50144, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 13/300
 - 64s - loss: 0.0498 - acc: 0.9944 - mDice: 0.9032 - val_loss: 0.0998 - val_acc: 0.9939 - val_mDice: 0.4926

Epoch 00013: val_mDice did not improve from 0.50144
Epoch 14/300
 - 65s - loss: 0.0497 - acc: 0.9945 - mDice: 0.9034 - val_loss: -2.7988e-02 - val_acc: 0.9926 - val_mDice: 0.4692

Epoch 00014: val_mDice did not improve from 0.50144
Epoch 15/300
 - 65s - loss: 0.0494 - acc: 0.9946 - mDice: 0.9039 - val_loss: -1.0683e-02 - val_acc: 0.9943 - val_mDice: 0.5105

Epoch 00015: val_mDice improved from 0.50144 to 0.51052, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/1-THALAMUS/sd2/best_model_weights.h5
Epoch 16/300
 - 65s - loss: 0.0488 - acc: 0.9946 - mDice: 0.9051 - val_loss: 0.1615 - val_acc: 0.9940 - val_mDice: 0.5072

Epoch 00016: val_mDice did not improve from 0.51052
Epoch 17/300
 - 66s - loss: 0.0480 - acc: 0.9947 - mDice: 0.9068 - val_loss: 4.8939e-04 - val_acc: 0.9935 - val_mDice: 0.4999

Epoch 00017: val_mDice did not improve from 0.51052
Epoch 18/300
 - 65s - loss: 0.0477 - acc: 0.9947 - mDice: 0.9072 - val_loss: -5.4797e-03 - val_acc: 0.9939 - val_mDice: 0.5046

Epoch 00018: val_mDice did not improve from 0.51052
Epoch 19/300
 - 65s - loss: 0.0464 - acc: 0.9948 - mDice: 0.9099 - val_loss: -2.3335e-03 - val_acc: 0.9941 - val_mDice: 0.5033

Epoch 00019: val_mDice did not improve from 0.51052
Epoch 20/300
 - 66s - loss: 0.0463 - acc: 0.9948 - mDice: 0.9100 - val_loss: 0.0567 - val_acc: 0.9914 - val_mDice: 0.4361

Epoch 00020: val_mDice did not improve from 0.51052
Epoch 21/300
 - 66s - loss: 0.0456 - acc: 0.9949 - mDice: 0.9115 - val_loss: 0.0521 - val_acc: 0.9942 - val_mDice: 0.5078

Epoch 00021: val_mDice did not improve from 0.51052
Epoch 22/300
 - 63s - loss: 0.0461 - acc: 0.9948 - mDice: 0.9104 - val_loss: -4.5773e-03 - val_acc: 0.9941 - val_mDice: 0.4967

Epoch 00022: val_mDice did not improve from 0.51052
Epoch 23/300
 - 64s - loss: 0.0443 - acc: 0.9950 - mDice: 0.9140 - val_loss: 0.1356 - val_acc: 0.9933 - val_mDice: 0.4983

Epoch 00023: val_mDice did not improve from 0.51052
Epoch 24/300
 - 66s - loss: 0.0450 - acc: 0.9950 - mDice: 0.9125 - val_loss: -2.7055e-02 - val_acc: 0.9928 - val_mDice: 0.4641

Epoch 00024: val_mDice did not improve from 0.51052
Epoch 25/300
 - 66s - loss: 0.0446 - acc: 0.9950 - mDice: 0.9134 - val_loss: 0.0705 - val_acc: 0.9940 - val_mDice: 0.5013

Epoch 00025: val_mDice did not improve from 0.51052
Epoch 26/300
 - 66s - loss: 0.0435 - acc: 0.9951 - mDice: 0.9154 - val_loss: 0.0280 - val_acc: 0.9939 - val_mDice: 0.5040

Epoch 00026: val_mDice did not improve from 0.51052
Epoch 27/300
 - 66s - loss: 0.0437 - acc: 0.9951 - mDice: 0.9151 - val_loss: 0.0882 - val_acc: 0.9939 - val_mDice: 0.5031

Epoch 00027: val_mDice did not improve from 0.51052
Epoch 28/300
 - 65s - loss: 0.0446 - acc: 0.9951 - mDice: 0.9132 - val_loss: -7.5639e-04 - val_acc: 0.9942 - val_mDice: 0.4880

Epoch 00028: val_mDice did not improve from 0.51052
Epoch 29/300
 - 63s - loss: 0.0424 - acc: 0.9952 - mDice: 0.9176 - val_loss: 0.0549 - val_acc: 0.9935 - val_mDice: 0.5034

Epoch 00029: val_mDice did not improve from 0.51052
Epoch 30/300
 - 61s - loss: 0.0427 - acc: 0.9951 - mDice: 0.9171 - val_loss: -1.9322e-02 - val_acc: 0.9939 - val_mDice: 0.4753

Epoch 00030: val_mDice did not improve from 0.51052

Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 31/300
 - 64s - loss: 0.0407 - acc: 0.9954 - mDice: 0.9210 - val_loss: -3.3216e-03 - val_acc: 0.9941 - val_mDice: 0.5038

Epoch 00031: val_mDice did not improve from 0.51052
Epoch 32/300
 - 65s - loss: 0.0394 - acc: 0.9955 - mDice: 0.9234 - val_loss: -6.7014e-03 - val_acc: 0.9944 - val_mDice: 0.5072

Epoch 00032: val_mDice did not improve from 0.51052
Epoch 33/300
 - 65s - loss: 0.0391 - acc: 0.9955 - mDice: 0.9241 - val_loss: 0.0376 - val_acc: 0.9943 - val_mDice: 0.5091

Epoch 00033: val_mDice did not improve from 0.51052
Epoch 34/300
 - 60s - loss: 0.0390 - acc: 0.9955 - mDice: 0.9243 - val_loss: 0.2340 - val_acc: 0.9897 - val_mDice: 0.4632

Epoch 00034: val_mDice did not improve from 0.51052
Epoch 35/300
 - 48s - loss: 0.0380 - acc: 0.9955 - mDice: 0.9263 - val_loss: 0.0031 - val_acc: 0.9939 - val_mDice: 0.4838

Epoch 00035: val_mDice did not improve from 0.51052
Epoch 36/300
 - 48s - loss: 0.0383 - acc: 0.9955 - mDice: 0.9256 - val_loss: -3.4932e-03 - val_acc: 0.9940 - val_mDice: 0.4950

Epoch 00036: val_mDice did not improve from 0.51052
Epoch 37/300
 - 48s - loss: 0.0383 - acc: 0.9955 - mDice: 0.9255 - val_loss: 0.0467 - val_acc: 0.9934 - val_mDice: 0.4942

Epoch 00037: val_mDice did not improve from 0.51052
Epoch 38/300
 - 46s - loss: 0.0382 - acc: 0.9956 - mDice: 0.9258 - val_loss: -8.2361e-03 - val_acc: 0.9944 - val_mDice: 0.5101

Epoch 00038: val_mDice did not improve from 0.51052
Epoch 39/300
 - 46s - loss: 0.0382 - acc: 0.9956 - mDice: 0.9257 - val_loss: 0.0316 - val_acc: 0.9938 - val_mDice: 0.5010

Epoch 00039: val_mDice did not improve from 0.51052
Epoch 40/300
 - 46s - loss: 0.0380 - acc: 0.9956 - mDice: 0.9263 - val_loss: 0.0085 - val_acc: 0.9941 - val_mDice: 0.5022

Epoch 00040: val_mDice did not improve from 0.51052
Epoch 41/300
 - 46s - loss: 0.0377 - acc: 0.9956 - mDice: 0.9268 - val_loss: 0.0370 - val_acc: 0.9941 - val_mDice: 0.5047

Epoch 00041: val_mDice did not improve from 0.51052
Epoch 42/300
 - 46s - loss: 0.0374 - acc: 0.9956 - mDice: 0.9273 - val_loss: 0.1407 - val_acc: 0.9924 - val_mDice: 0.4847

Epoch 00042: val_mDice did not improve from 0.51052
Epoch 43/300
 - 47s - loss: 0.0380 - acc: 0.9956 - mDice: 0.9262 - val_loss: 0.0027 - val_acc: 0.9943 - val_mDice: 0.5064

Epoch 00043: val_mDice did not improve from 0.51052
Epoch 44/300
 - 46s - loss: 0.0384 - acc: 0.9956 - mDice: 0.9254 - val_loss: 0.1377 - val_acc: 0.9940 - val_mDice: 0.5007

Epoch 00044: val_mDice did not improve from 0.51052
Epoch 45/300
 - 45s - loss: 0.0382 - acc: 0.9956 - mDice: 0.9257 - val_loss: 0.1009 - val_acc: 0.9933 - val_mDice: 0.4943

Epoch 00045: val_mDice did not improve from 0.51052

Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 46/300
 - 46s - loss: 0.0358 - acc: 0.9957 - mDice: 0.9305 - val_loss: 0.0664 - val_acc: 0.9942 - val_mDice: 0.5004

Epoch 00046: val_mDice did not improve from 0.51052
Epoch 47/300
 - 46s - loss: 0.0360 - acc: 0.9958 - mDice: 0.9301 - val_loss: 0.0686 - val_acc: 0.9943 - val_mDice: 0.4954

Epoch 00047: val_mDice did not improve from 0.51052
Epoch 48/300
 - 46s - loss: 0.0353 - acc: 0.9958 - mDice: 0.9315 - val_loss: 0.0194 - val_acc: 0.9941 - val_mDice: 0.4963

Epoch 00048: val_mDice did not improve from 0.51052
Epoch 49/300
 - 46s - loss: 0.0353 - acc: 0.9958 - mDice: 0.9315 - val_loss: 0.0082 - val_acc: 0.9941 - val_mDice: 0.4958

Epoch 00049: val_mDice did not improve from 0.51052
Epoch 50/300
 - 46s - loss: 0.0370 - acc: 0.9958 - mDice: 0.9280 - val_loss: 0.0105 - val_acc: 0.9940 - val_mDice: 0.5027

Epoch 00050: val_mDice did not improve from 0.51052
Epoch 51/300
 - 46s - loss: 0.0350 - acc: 0.9958 - mDice: 0.9321 - val_loss: 0.0360 - val_acc: 0.9940 - val_mDice: 0.5005

Epoch 00051: val_mDice did not improve from 0.51052
Epoch 52/300
 - 46s - loss: 0.0352 - acc: 0.9958 - mDice: 0.9317 - val_loss: 0.0621 - val_acc: 0.9940 - val_mDice: 0.4995

Epoch 00052: val_mDice did not improve from 0.51052
Epoch 53/300
 - 46s - loss: 0.0352 - acc: 0.9958 - mDice: 0.9318 - val_loss: 0.0370 - val_acc: 0.9943 - val_mDice: 0.5046

Epoch 00053: val_mDice did not improve from 0.51052
Epoch 54/300
 - 46s - loss: 0.0347 - acc: 0.9958 - mDice: 0.9326 - val_loss: 0.0641 - val_acc: 0.9941 - val_mDice: 0.5015

Epoch 00054: val_mDice did not improve from 0.51052
Epoch 55/300
 - 46s - loss: 0.0350 - acc: 0.9958 - mDice: 0.9321 - val_loss: 0.1029 - val_acc: 0.9942 - val_mDice: 0.4956

predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:00<00:01,  1.64it/s]predicting test subjects:  50%|█████     | 2/4 [00:00<00:00,  2.17it/s]predicting test subjects:  75%|███████▌  | 3/4 [00:00<00:00,  2.79it/s]predicting test subjects: 100%|██████████| 4/4 [00:00<00:00,  3.47it/s]predicting test subjects: 100%|██████████| 4/4 [00:00<00:00,  4.13it/s]
predicting train subjects:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects:   0%|          | 1/266 [00:00<00:47,  5.54it/s]predicting train subjects:   1%|          | 2/266 [00:00<00:44,  5.90it/s]predicting train subjects:   1%|          | 3/266 [00:00<00:46,  5.66it/s]predicting train subjects:   2%|▏         | 4/266 [00:00<00:47,  5.46it/s]predicting train subjects:   2%|▏         | 5/266 [00:00<00:46,  5.56it/s]predicting train subjects:   2%|▏         | 6/266 [00:01<00:44,  5.86it/s]predicting train subjects:   3%|▎         | 7/266 [00:01<00:42,  6.10it/s]predicting train subjects:   3%|▎         | 8/266 [00:01<00:41,  6.29it/s]predicting train subjects:   3%|▎         | 9/266 [00:01<00:40,  6.42it/s]predicting train subjects:   4%|▍         | 10/266 [00:01<00:39,  6.56it/s]predicting train subjects:   4%|▍         | 11/266 [00:01<00:38,  6.66it/s]predicting train subjects:   5%|▍         | 12/266 [00:01<00:37,  6.72it/s]predicting train subjects:   5%|▍         | 13/266 [00:02<00:37,  6.81it/s]predicting train subjects:   5%|▌         | 14/266 [00:02<00:36,  6.85it/s]predicting train subjects:   6%|▌         | 15/266 [00:02<00:36,  6.91it/s]predicting train subjects:   6%|▌         | 16/266 [00:02<00:36,  6.82it/s]predicting train subjects:   6%|▋         | 17/266 [00:02<00:36,  6.85it/s]predicting train subjects:   7%|▋         | 18/266 [00:02<00:36,  6.88it/s]predicting train subjects:   7%|▋         | 19/266 [00:02<00:35,  6.94it/s]predicting train subjects:   8%|▊         | 20/266 [00:03<00:35,  6.96it/s]predicting train subjects:   8%|▊         | 21/266 [00:03<00:35,  6.95it/s]predicting train subjects:   8%|▊         | 22/266 [00:03<00:35,  6.94it/s]predicting train subjects:   9%|▊         | 23/266 [00:03<00:35,  6.91it/s]predicting train subjects:   9%|▉         | 24/266 [00:03<00:34,  6.95it/s]predicting train subjects:   9%|▉         | 25/266 [00:03<00:34,  6.96it/s]predicting train subjects:  10%|▉         | 26/266 [00:03<00:34,  7.02it/s]predicting train subjects:  10%|█         | 27/266 [00:04<00:33,  7.04it/s]predicting train subjects:  11%|█         | 28/266 [00:04<00:33,  7.07it/s]predicting train subjects:  11%|█         | 29/266 [00:04<00:33,  7.09it/s]predicting train subjects:  11%|█▏        | 30/266 [00:04<00:33,  7.08it/s]predicting train subjects:  12%|█▏        | 31/266 [00:04<00:33,  7.07it/s]predicting train subjects:  12%|█▏        | 32/266 [00:04<00:33,  7.07it/s]predicting train subjects:  12%|█▏        | 33/266 [00:04<00:32,  7.07it/s]predicting train subjects:  13%|█▎        | 34/266 [00:05<00:32,  7.07it/s]predicting train subjects:  13%|█▎        | 35/266 [00:05<00:32,  7.07it/s]predicting train subjects:  14%|█▎        | 36/266 [00:05<00:32,  7.10it/s]predicting train subjects:  14%|█▍        | 37/266 [00:05<00:33,  6.92it/s]predicting train subjects:  14%|█▍        | 38/266 [00:05<00:33,  6.79it/s]predicting train subjects:  15%|█▍        | 39/266 [00:05<00:33,  6.86it/s]predicting train subjects:  15%|█▌        | 40/266 [00:05<00:32,  6.97it/s]predicting train subjects:  15%|█▌        | 41/266 [00:06<00:32,  6.97it/s]predicting train subjects:  16%|█▌        | 42/266 [00:06<00:30,  7.32it/s]predicting train subjects:  16%|█▌        | 43/266 [00:06<00:29,  7.64it/s]predicting train subjects:  17%|█▋        | 44/266 [00:06<00:28,  7.86it/s]predicting train subjects:  17%|█▋        | 45/266 [00:06<00:27,  8.05it/s]predicting train subjects:  17%|█▋        | 46/266 [00:06<00:27,  8.12it/s]predicting train subjects:  18%|█▊        | 47/266 [00:06<00:26,  8.26it/s]predicting train subjects:  18%|█▊        | 48/266 [00:06<00:26,  8.37it/s]predicting train subjects:  18%|█▊        | 49/266 [00:07<00:25,  8.42it/s]predicting train subjects:  19%|█▉        | 50/266 [00:07<00:26,  8.02it/s]predicting train subjects:  19%|█▉        | 51/266 [00:07<00:26,  8.09it/s]predicting train subjects:  20%|█▉        | 52/266 [00:07<00:26,  8.19it/s]predicting train subjects:  20%|█▉        | 53/266 [00:07<00:25,  8.24it/s]predicting train subjects:  20%|██        | 54/266 [00:07<00:25,  8.22it/s]predicting train subjects:  21%|██        | 55/266 [00:07<00:25,  8.21it/s]predicting train subjects:  21%|██        | 56/266 [00:07<00:25,  8.30it/s]predicting train subjects:  21%|██▏       | 57/266 [00:07<00:24,  8.38it/s]predicting train subjects:  22%|██▏       | 58/266 [00:08<00:24,  8.42it/s]predicting train subjects:  22%|██▏       | 59/266 [00:08<00:24,  8.40it/s]predicting train subjects:  23%|██▎       | 60/266 [00:08<00:24,  8.34it/s]predicting train subjects:  23%|██▎       | 61/266 [00:08<00:24,  8.30it/s]predicting train subjects:  23%|██▎       | 62/266 [00:08<00:24,  8.32it/s]predicting train subjects:  24%|██▎       | 63/266 [00:08<00:24,  8.35it/s]predicting train subjects:  24%|██▍       | 64/266 [00:08<00:24,  8.24it/s]predicting train subjects:  24%|██▍       | 65/266 [00:08<00:24,  8.31it/s]predicting train subjects:  25%|██▍       | 66/266 [00:09<00:23,  8.40it/s]predicting train subjects:  25%|██▌       | 67/266 [00:09<00:23,  8.36it/s]predicting train subjects:  26%|██▌       | 68/266 [00:09<00:23,  8.38it/s]predicting train subjects:  26%|██▌       | 69/266 [00:09<00:23,  8.40it/s]predicting train subjects:  26%|██▋       | 70/266 [00:09<00:23,  8.37it/s]predicting train subjects:  27%|██▋       | 71/266 [00:09<00:23,  8.39it/s]predicting train subjects:  27%|██▋       | 72/266 [00:09<00:23,  8.39it/s]predicting train subjects:  27%|██▋       | 73/266 [00:09<00:23,  8.36it/s]predicting train subjects:  28%|██▊       | 74/266 [00:10<00:23,  8.03it/s]predicting train subjects:  28%|██▊       | 75/266 [00:10<00:23,  8.19it/s]predicting train subjects:  29%|██▊       | 76/266 [00:10<00:22,  8.30it/s]predicting train subjects:  29%|██▉       | 77/266 [00:10<00:22,  8.38it/s]predicting train subjects:  29%|██▉       | 78/266 [00:10<00:23,  7.98it/s]predicting train subjects:  30%|██▉       | 79/266 [00:10<00:24,  7.69it/s]predicting train subjects:  30%|███       | 80/266 [00:10<00:24,  7.55it/s]predicting train subjects:  30%|███       | 81/266 [00:10<00:25,  7.37it/s]predicting train subjects:  31%|███       | 82/266 [00:11<00:25,  7.29it/s]predicting train subjects:  31%|███       | 83/266 [00:11<00:25,  7.24it/s]predicting train subjects:  32%|███▏      | 84/266 [00:11<00:25,  7.20it/s]predicting train subjects:  32%|███▏      | 85/266 [00:11<00:25,  7.14it/s]predicting train subjects:  32%|███▏      | 86/266 [00:11<00:25,  7.18it/s]predicting train subjects:  33%|███▎      | 87/266 [00:11<00:25,  7.14it/s]predicting train subjects:  33%|███▎      | 88/266 [00:11<00:25,  7.05it/s]predicting train subjects:  33%|███▎      | 89/266 [00:12<00:25,  7.02it/s]predicting train subjects:  34%|███▍      | 90/266 [00:12<00:24,  7.06it/s]predicting train subjects:  34%|███▍      | 91/266 [00:12<00:25,  6.98it/s]predicting train subjects:  35%|███▍      | 92/266 [00:12<00:24,  7.02it/s]predicting train subjects:  35%|███▍      | 93/266 [00:12<00:24,  7.07it/s]predicting train subjects:  35%|███▌      | 94/266 [00:12<00:24,  7.13it/s]predicting train subjects:  36%|███▌      | 95/266 [00:12<00:24,  7.12it/s]predicting train subjects:  36%|███▌      | 96/266 [00:13<00:27,  6.29it/s]predicting train subjects:  36%|███▋      | 97/266 [00:13<00:28,  5.98it/s]predicting train subjects:  37%|███▋      | 98/266 [00:13<00:27,  6.03it/s]predicting train subjects:  37%|███▋      | 99/266 [00:13<00:27,  6.04it/s]predicting train subjects:  38%|███▊      | 100/266 [00:13<00:24,  6.65it/s]predicting train subjects:  38%|███▊      | 101/266 [00:13<00:23,  7.03it/s]predicting train subjects:  38%|███▊      | 102/266 [00:14<00:22,  7.36it/s]predicting train subjects:  39%|███▊      | 103/266 [00:14<00:21,  7.61it/s]predicting train subjects:  39%|███▉      | 104/266 [00:14<00:20,  7.83it/s]predicting train subjects:  39%|███▉      | 105/266 [00:14<00:20,  7.90it/s]predicting train subjects:  40%|███▉      | 106/266 [00:14<00:20,  7.89it/s]predicting train subjects:  40%|████      | 107/266 [00:14<00:19,  8.00it/s]predicting train subjects:  41%|████      | 108/266 [00:14<00:19,  8.07it/s]predicting train subjects:  41%|████      | 109/266 [00:14<00:19,  8.10it/s]predicting train subjects:  41%|████▏     | 110/266 [00:14<00:19,  8.12it/s]predicting train subjects:  42%|████▏     | 111/266 [00:15<00:19,  8.13it/s]predicting train subjects:  42%|████▏     | 112/266 [00:15<00:18,  8.17it/s]predicting train subjects:  42%|████▏     | 113/266 [00:15<00:18,  8.21it/s]predicting train subjects:  43%|████▎     | 114/266 [00:15<00:19,  7.99it/s]predicting train subjects:  43%|████▎     | 115/266 [00:15<00:18,  8.07it/s]predicting train subjects:  44%|████▎     | 116/266 [00:15<00:18,  8.02it/s]predicting train subjects:  44%|████▍     | 117/266 [00:15<00:18,  8.05it/s]predicting train subjects:  44%|████▍     | 118/266 [00:15<00:18,  8.10it/s]predicting train subjects:  45%|████▍     | 119/266 [00:16<00:19,  7.72it/s]predicting train subjects:  45%|████▌     | 120/266 [00:16<00:19,  7.48it/s]predicting train subjects:  45%|████▌     | 121/266 [00:16<00:19,  7.29it/s]predicting train subjects:  46%|████▌     | 122/266 [00:16<00:20,  7.16it/s]predicting train subjects:  46%|████▌     | 123/266 [00:16<00:20,  7.09it/s]predicting train subjects:  47%|████▋     | 124/266 [00:16<00:20,  7.01it/s]predicting train subjects:  47%|████▋     | 125/266 [00:16<00:20,  6.99it/s]predicting train subjects:  47%|████▋     | 126/266 [00:17<00:20,  6.92it/s]predicting train subjects:  48%|████▊     | 127/266 [00:17<00:20,  6.91it/s]predicting train subjects:  48%|████▊     | 128/266 [00:17<00:20,  6.77it/s]predicting train subjects:  48%|████▊     | 129/266 [00:17<00:20,  6.77it/s]predicting train subjects:  49%|████▉     | 130/266 [00:17<00:20,  6.65it/s]predicting train subjects:  49%|████▉     | 131/266 [00:17<00:19,  6.77it/s]predicting train subjects:  50%|████▉     | 132/266 [00:18<00:19,  6.78it/s]predicting train subjects:  50%|█████     | 133/266 [00:18<00:20,  6.64it/s]predicting train subjects:  50%|█████     | 134/266 [00:18<00:19,  6.65it/s]predicting train subjects:  51%|█████     | 135/266 [00:18<00:19,  6.68it/s]predicting train subjects:  51%|█████     | 136/266 [00:18<00:19,  6.69it/s]predicting train subjects:  52%|█████▏    | 137/266 [00:18<00:18,  6.91it/s]predicting train subjects:  52%|█████▏    | 138/266 [00:18<00:18,  7.09it/s]predicting train subjects:  52%|█████▏    | 139/266 [00:19<00:17,  7.21it/s]predicting train subjects:  53%|█████▎    | 140/266 [00:19<00:17,  7.24it/s]predicting train subjects:  53%|█████▎    | 141/266 [00:19<00:17,  7.18it/s]predicting train subjects:  53%|█████▎    | 142/266 [00:19<00:17,  7.22it/s]predicting train subjects:  54%|█████▍    | 143/266 [00:19<00:16,  7.30it/s]predicting train subjects:  54%|█████▍    | 144/266 [00:19<00:16,  7.22it/s]predicting train subjects:  55%|█████▍    | 145/266 [00:19<00:16,  7.37it/s]predicting train subjects:  55%|█████▍    | 146/266 [00:19<00:16,  7.45it/s]predicting train subjects:  55%|█████▌    | 147/266 [00:20<00:15,  7.48it/s]predicting train subjects:  56%|█████▌    | 148/266 [00:20<00:15,  7.50it/s]predicting train subjects:  56%|█████▌    | 149/266 [00:20<00:15,  7.46it/s]predicting train subjects:  56%|█████▋    | 150/266 [00:20<00:15,  7.32it/s]predicting train subjects:  57%|█████▋    | 151/266 [00:20<00:15,  7.25it/s]predicting train subjects:  57%|█████▋    | 152/266 [00:20<00:15,  7.34it/s]predicting train subjects:  58%|█████▊    | 153/266 [00:20<00:15,  7.39it/s]predicting train subjects:  58%|█████▊    | 154/266 [00:21<00:15,  7.37it/s]predicting train subjects:  58%|█████▊    | 155/266 [00:21<00:14,  7.90it/s]predicting train subjects:  59%|█████▊    | 156/266 [00:21<00:13,  8.35it/s]predicting train subjects:  59%|█████▉    | 157/266 [00:21<00:12,  8.63it/s]predicting train subjects:  59%|█████▉    | 158/266 [00:21<00:12,  8.75it/s]predicting train subjects:  60%|█████▉    | 159/266 [00:21<00:12,  8.82it/s]predicting train subjects:  60%|██████    | 160/266 [00:21<00:11,  8.98it/s]predicting train subjects:  61%|██████    | 161/266 [00:21<00:11,  8.99it/s]predicting train subjects:  61%|██████    | 162/266 [00:21<00:11,  8.96it/s]predicting train subjects:  61%|██████▏   | 163/266 [00:22<00:11,  9.02it/s]predicting train subjects:  62%|██████▏   | 164/266 [00:22<00:11,  9.18it/s]predicting train subjects:  62%|██████▏   | 165/266 [00:22<00:10,  9.20it/s]predicting train subjects:  62%|██████▏   | 166/266 [00:22<00:10,  9.16it/s]predicting train subjects:  63%|██████▎   | 167/266 [00:22<00:10,  9.18it/s]predicting train subjects:  63%|██████▎   | 168/266 [00:22<00:10,  9.01it/s]predicting train subjects:  64%|██████▎   | 169/266 [00:22<00:10,  9.14it/s]predicting train subjects:  64%|██████▍   | 170/266 [00:22<00:10,  9.29it/s]predicting train subjects:  64%|██████▍   | 171/266 [00:22<00:10,  9.40it/s]predicting train subjects:  65%|██████▍   | 172/266 [00:23<00:10,  9.36it/s]predicting train subjects:  65%|██████▌   | 173/266 [00:23<00:10,  9.06it/s]predicting train subjects:  65%|██████▌   | 174/266 [00:23<00:10,  8.78it/s]predicting train subjects:  66%|██████▌   | 175/266 [00:23<00:10,  8.51it/s]predicting train subjects:  66%|██████▌   | 176/266 [00:23<00:10,  8.45it/s]predicting train subjects:  67%|██████▋   | 177/266 [00:23<00:10,  8.44it/s]predicting train subjects:  67%|██████▋   | 178/266 [00:23<00:10,  8.21it/s]predicting train subjects:  67%|██████▋   | 179/266 [00:23<00:10,  8.22it/s]predicting train subjects:  68%|██████▊   | 180/266 [00:23<00:10,  8.21it/s]predicting train subjects:  68%|██████▊   | 181/266 [00:24<00:10,  8.31it/s]predicting train subjects:  68%|██████▊   | 182/266 [00:24<00:10,  8.09it/s]predicting train subjects:  69%|██████▉   | 183/266 [00:24<00:10,  8.19it/s]predicting train subjects:  69%|██████▉   | 184/266 [00:24<00:10,  8.07it/s]predicting train subjects:  70%|██████▉   | 185/266 [00:24<00:09,  8.13it/s]predicting train subjects:  70%|██████▉   | 186/266 [00:24<00:09,  8.21it/s]predicting train subjects:  70%|███████   | 187/266 [00:24<00:09,  8.32it/s]predicting train subjects:  71%|███████   | 188/266 [00:24<00:09,  8.35it/s]predicting train subjects:  71%|███████   | 189/266 [00:25<00:09,  8.35it/s]predicting train subjects:  71%|███████▏  | 190/266 [00:25<00:09,  8.36it/s]predicting train subjects:  72%|███████▏  | 191/266 [00:25<00:08,  8.36it/s]predicting train subjects:  72%|███████▏  | 192/266 [00:25<00:10,  7.07it/s]predicting train subjects:  73%|███████▎  | 193/266 [00:25<00:09,  7.43it/s]predicting train subjects:  73%|███████▎  | 194/266 [00:25<00:09,  7.42it/s]predicting train subjects:  73%|███████▎  | 195/266 [00:25<00:09,  7.70it/s]predicting train subjects:  74%|███████▎  | 196/266 [00:26<00:08,  7.78it/s]predicting train subjects:  74%|███████▍  | 197/266 [00:26<00:08,  7.67it/s]predicting train subjects:  74%|███████▍  | 198/266 [00:26<00:08,  7.82it/s]predicting train subjects:  75%|███████▍  | 199/266 [00:26<00:08,  7.89it/s]predicting train subjects:  75%|███████▌  | 200/266 [00:26<00:08,  8.05it/s]predicting train subjects:  76%|███████▌  | 201/266 [00:26<00:07,  8.18it/s]predicting train subjects:  76%|███████▌  | 202/266 [00:26<00:07,  8.19it/s]predicting train subjects:  76%|███████▋  | 203/266 [00:26<00:07,  8.11it/s]predicting train subjects:  77%|███████▋  | 204/266 [00:26<00:07,  8.11it/s]predicting train subjects:  77%|███████▋  | 205/266 [00:27<00:07,  7.84it/s]predicting train subjects:  77%|███████▋  | 206/266 [00:27<00:07,  8.00it/s]predicting train subjects:  78%|███████▊  | 207/266 [00:27<00:07,  8.08it/s]predicting train subjects:  78%|███████▊  | 208/266 [00:27<00:07,  8.17it/s]predicting train subjects:  79%|███████▊  | 209/266 [00:27<00:06,  8.19it/s]predicting train subjects:  79%|███████▉  | 210/266 [00:27<00:06,  8.18it/s]predicting train subjects:  79%|███████▉  | 211/266 [00:27<00:06,  8.20it/s]predicting train subjects:  80%|███████▉  | 212/266 [00:27<00:06,  8.20it/s]predicting train subjects:  80%|████████  | 213/266 [00:28<00:06,  8.35it/s]predicting train subjects:  80%|████████  | 214/266 [00:28<00:06,  8.45it/s]predicting train subjects:  81%|████████  | 215/266 [00:28<00:05,  8.51it/s]predicting train subjects:  81%|████████  | 216/266 [00:28<00:05,  8.46it/s]predicting train subjects:  82%|████████▏ | 217/266 [00:28<00:05,  8.44it/s]predicting train subjects:  82%|████████▏ | 218/266 [00:28<00:05,  8.42it/s]predicting train subjects:  82%|████████▏ | 219/266 [00:28<00:05,  8.48it/s]predicting train subjects:  83%|████████▎ | 220/266 [00:28<00:05,  8.52it/s]predicting train subjects:  83%|████████▎ | 221/266 [00:29<00:05,  8.56it/s]predicting train subjects:  83%|████████▎ | 222/266 [00:29<00:05,  8.48it/s]predicting train subjects:  84%|████████▍ | 223/266 [00:29<00:05,  8.44it/s]predicting train subjects:  84%|████████▍ | 224/266 [00:29<00:04,  8.49it/s]predicting train subjects:  85%|████████▍ | 225/266 [00:29<00:04,  8.40it/s]predicting train subjects:  85%|████████▍ | 226/266 [00:29<00:04,  8.42it/s]predicting train subjects:  85%|████████▌ | 227/266 [00:29<00:04,  8.44it/s]predicting train subjects:  86%|████████▌ | 228/266 [00:29<00:04,  8.39it/s]predicting train subjects:  86%|████████▌ | 229/266 [00:29<00:04,  8.47it/s]predicting train subjects:  86%|████████▋ | 230/266 [00:30<00:04,  8.52it/s]predicting train subjects:  87%|████████▋ | 231/266 [00:30<00:04,  8.41it/s]predicting train subjects:  87%|████████▋ | 232/266 [00:30<00:04,  8.39it/s]predicting train subjects:  88%|████████▊ | 233/266 [00:30<00:03,  8.37it/s]predicting train subjects:  88%|████████▊ | 234/266 [00:30<00:03,  8.35it/s]predicting train subjects:  88%|████████▊ | 235/266 [00:30<00:03,  8.34it/s]predicting train subjects:  89%|████████▊ | 236/266 [00:30<00:03,  8.28it/s]predicting train subjects:  89%|████████▉ | 237/266 [00:30<00:03,  8.24it/s]predicting train subjects:  89%|████████▉ | 238/266 [00:31<00:03,  8.23it/s]predicting train subjects:  90%|████████▉ | 239/266 [00:31<00:03,  8.39it/s]predicting train subjects:  90%|█████████ | 240/266 [00:31<00:03,  8.46it/s]predicting train subjects:  91%|█████████ | 241/266 [00:31<00:02,  8.54it/s]predicting train subjects:  91%|█████████ | 242/266 [00:31<00:02,  8.63it/s]predicting train subjects:  91%|█████████▏| 243/266 [00:31<00:02,  8.68it/s]predicting train subjects:  92%|█████████▏| 244/266 [00:31<00:02,  8.65it/s]predicting train subjects:  92%|█████████▏| 245/266 [00:31<00:02,  8.51it/s]predicting train subjects:  92%|█████████▏| 246/266 [00:31<00:02,  8.50it/s]predicting train subjects:  93%|█████████▎| 247/266 [00:32<00:02,  8.37it/s]predicting train subjects:  93%|█████████▎| 248/266 [00:32<00:02,  8.28it/s]predicting train subjects:  94%|█████████▎| 249/266 [00:32<00:02,  7.86it/s]predicting train subjects:  94%|█████████▍| 250/266 [00:32<00:02,  7.69it/s]predicting train subjects:  94%|█████████▍| 251/266 [00:32<00:02,  7.29it/s]predicting train subjects:  95%|█████████▍| 252/266 [00:32<00:01,  7.02it/s]predicting train subjects:  95%|█████████▌| 253/266 [00:32<00:01,  7.01it/s]predicting train subjects:  95%|█████████▌| 254/266 [00:33<00:01,  7.04it/s]predicting train subjects:  96%|█████████▌| 255/266 [00:33<00:01,  6.90it/s]predicting train subjects:  96%|█████████▌| 256/266 [00:33<00:01,  7.06it/s]predicting train subjects:  97%|█████████▋| 257/266 [00:33<00:01,  7.12it/s]predicting train subjects:  97%|█████████▋| 258/266 [00:33<00:01,  7.18it/s]predicting train subjects:  97%|█████████▋| 259/266 [00:33<00:00,  7.08it/s]predicting train subjects:  98%|█████████▊| 260/266 [00:33<00:00,  7.16it/s]predicting train subjects:  98%|█████████▊| 261/266 [00:34<00:00,  7.24it/s]predicting train subjects:  98%|█████████▊| 262/266 [00:34<00:00,  7.31it/s]predicting train subjects:  99%|█████████▉| 263/266 [00:34<00:00,  7.27it/s]predicting train subjects:  99%|█████████▉| 264/266 [00:34<00:00,  7.23it/s]predicting train subjects: 100%|█████████▉| 265/266 [00:34<00:00,  7.26it/s]predicting train subjects: 100%|██████████| 266/266 [00:34<00:00,  7.29it/s]predicting train subjects: 100%|██████████| 266/266 [00:34<00:00,  7.65it/s]
predicting test subjects sagittal:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects sagittal:  25%|██▌       | 1/4 [00:00<00:00,  8.29it/s]predicting test subjects sagittal:  50%|█████     | 2/4 [00:00<00:00,  8.38it/s]predicting test subjects sagittal:  75%|███████▌  | 3/4 [00:00<00:00,  8.49it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  8.27it/s]predicting test subjects sagittal: 100%|██████████| 4/4 [00:00<00:00,  8.34it/s]
predicting train subjects sagittal:   0%|          | 0/266 [00:00<?, ?it/s]predicting train subjects sagittal:   0%|          | 1/266 [00:00<00:38,  6.91it/s]predicting train subjects sagittal:   1%|          | 2/266 [00:00<00:37,  7.04it/s]predicting train subjects sagittal:   1%|          | 3/266 [00:00<00:35,  7.46it/s]predicting train subjects sagittal:   2%|▏         | 4/266 [00:00<00:33,  7.72it/s]predicting train subjects sagittal:   2%|▏         | 5/266 [00:00<00:34,  7.46it/s]predicting train subjects sagittal:   2%|▏         | 6/266 [00:00<00:36,  7.10it/s]predicting train subjects sagittal:   3%|▎         | 7/266 [00:00<00:37,  6.99it/s]predicting train subjects sagittal:   3%|▎         | 8/266 [00:01<00:37,  6.90it/s]predicting train subjects sagittal:   3%|▎         | 9/266 [00:01<00:37,  6.91it/s]predicting train subjects sagittal:   4%|▍         | 10/266 [00:01<00:38,  6.71it/s]predicting train subjects sagittal:   4%|▍         | 11/266 [00:01<00:38,  6.62it/s]predicting train subjects sagittal:   5%|▍         | 12/266 [00:01<00:37,  6.71it/s]predicting train subjects sagittal:   5%|▍         | 13/266 [00:01<00:38,  6.64it/s]predicting train subjects sagittal:   5%|▌         | 14/266 [00:02<00:37,  6.70it/s]predicting train subjects sagittal:   6%|▌         | 15/266 [00:02<00:37,  6.75it/s]predicting train subjects sagittal:   6%|▌         | 16/266 [00:02<00:37,  6.73it/s]predicting train subjects sagittal:   6%|▋         | 17/266 [00:02<00:37,  6.69it/s]predicting train subjects sagittal:   7%|▋         | 18/266 [00:02<00:36,  6.78it/s]predicting train subjects sagittal:   7%|▋         | 19/266 [00:02<00:37,  6.63it/s]predicting train subjects sagittal:   8%|▊         | 20/266 [00:02<00:36,  6.73it/s]predicting train subjects sagittal:   8%|▊         | 21/266 [00:03<00:37,  6.51it/s]predicting train subjects sagittal:   8%|▊         | 22/266 [00:03<00:37,  6.58it/s]predicting train subjects sagittal:   9%|▊         | 23/266 [00:03<00:37,  6.53it/s]predicting train subjects sagittal:   9%|▉         | 24/266 [00:03<00:36,  6.72it/s]predicting train subjects sagittal:   9%|▉         | 25/266 [00:03<00:35,  6.77it/s]predicting train subjects sagittal:  10%|▉         | 26/266 [00:03<00:35,  6.85it/s]predicting train subjects sagittal:  10%|█         | 27/266 [00:03<00:34,  6.89it/s]predicting train subjects sagittal:  11%|█         | 28/266 [00:04<00:34,  6.96it/s]predicting train subjects sagittal:  11%|█         | 29/266 [00:04<00:33,  7.05it/s]predicting train subjects sagittal:  11%|█▏        | 30/266 [00:04<00:33,  7.12it/s]predicting train subjects sagittal:  12%|█▏        | 31/266 [00:04<00:33,  7.12it/s]predicting train subjects sagittal:  12%|█▏        | 32/266 [00:04<00:32,  7.13it/s]predicting train subjects sagittal:  12%|█▏        | 33/266 [00:04<00:32,  7.14it/s]predicting train subjects sagittal:  13%|█▎        | 34/266 [00:04<00:32,  7.10it/s]predicting train subjects sagittal:  13%|█▎        | 35/266 [00:05<00:32,  7.10it/s]predicting train subjects sagittal:  14%|█▎        | 36/266 [00:05<00:32,  7.14it/s]predicting train subjects sagittal:  14%|█▍        | 37/266 [00:05<00:32,  7.15it/s]predicting train subjects sagittal:  14%|█▍        | 38/266 [00:05<00:32,  7.06it/s]predicting train subjects sagittal:  15%|█▍        | 39/266 [00:05<00:32,  7.07it/s]predicting train subjects sagittal:  15%|█▌        | 40/266 [00:05<00:32,  7.06it/s]predicting train subjects sagittal:  15%|█▌        | 41/266 [00:05<00:32,  6.85it/s]predicting train subjects sagittal:  16%|█▌        | 42/266 [00:06<00:31,  7.09it/s]predicting train subjects sagittal:  16%|█▌        | 43/266 [00:06<00:30,  7.30it/s]predicting train subjects sagittal:  17%|█▋        | 44/266 [00:06<00:29,  7.49it/s]predicting train subjects sagittal:  17%|█▋        | 45/266 [00:06<00:28,  7.65it/s]predicting train subjects sagittal:  17%|█▋        | 46/266 [00:06<00:28,  7.78it/s]predicting train subjects sagittal:  18%|█▊        | 47/266 [00:06<00:28,  7.79it/s]predicting train subjects sagittal:  18%|█▊        | 48/266 [00:06<00:27,  8.07it/s]predicting train subjects sagittal:  18%|█▊        | 49/266 [00:06<00:26,  8.25it/s]predicting train subjects sagittal:  19%|█▉        | 50/266 [00:07<00:26,  8.24it/s]predicting train subjects sagittal:  19%|█▉        | 51/266 [00:07<00:25,  8.40it/s]predicting train subjects sagittal:  20%|█▉        | 52/266 [00:07<00:25,  8.41it/s]predicting train subjects sagittal:  20%|█▉        | 53/266 [00:07<00:25,  8.44it/s]predicting train subjects sagittal:  20%|██        | 54/266 [00:07<00:24,  8.49it/s]predicting train subjects sagittal:  21%|██        | 55/266 [00:07<00:25,  8.28it/s]predicting train subjects sagittal:  21%|██        | 56/266 [00:07<00:25,  8.36it/s]predicting train subjects sagittal:  21%|██▏       | 57/266 [00:07<00:24,  8.46it/s]predicting train subjects sagittal:  22%|██▏       | 58/266 [00:07<00:24,  8.57it/s]predicting train subjects sagittal:  22%|██▏       | 59/266 [00:08<00:24,  8.59it/s]predicting train subjects sagittal:  23%|██▎       | 60/266 [00:08<00:24,  8.53it/s]predicting train subjects sagittal:  23%|██▎       | 61/266 [00:08<00:24,  8.51it/s]predicting train subjects sagittal:  23%|██▎       | 62/266 [00:08<00:23,  8.52it/s]predicting train subjects sagittal:  24%|██▎       | 63/266 [00:08<00:23,  8.52it/s]predicting train subjects sagittal:  24%|██▍       | 64/266 [00:08<00:23,  8.54it/s]predicting train subjects sagittal:  24%|██▍       | 65/266 [00:08<00:23,  8.58it/s]predicting train subjects sagittal:  25%|██▍       | 66/266 [00:08<00:23,  8.54it/s]predicting train subjects sagittal:  25%|██▌       | 67/266 [00:09<00:23,  8.54it/s]predicting train subjects sagittal:  26%|██▌       | 68/266 [00:09<00:23,  8.49it/s]predicting train subjects sagittal:  26%|██▌       | 69/266 [00:09<00:23,  8.55it/s]predicting train subjects sagittal:  26%|██▋       | 70/266 [00:09<00:22,  8.53it/s]predicting train subjects sagittal:  27%|██▋       | 71/266 [00:09<00:22,  8.51it/s]predicting train subjects sagittal:  27%|██▋       | 72/266 [00:09<00:22,  8.53it/s]predicting train subjects sagittal:  27%|██▋       | 73/266 [00:09<00:22,  8.55it/s]predicting train subjects sagittal:  28%|██▊       | 74/266 [00:09<00:22,  8.59it/s]predicting train subjects sagittal:  28%|██▊       | 75/266 [00:09<00:22,  8.59it/s]predicting train subjects sagittal:  29%|██▊       | 76/266 [00:10<00:22,  8.53it/s]predicting train subjects sagittal:  29%|██▉       | 77/266 [00:10<00:22,  8.53it/s]predicting train subjects sagittal:  29%|██▉       | 78/266 [00:10<00:23,  8.01it/s]predicting train subjects sagittal:  30%|██▉       | 79/266 [00:10<00:24,  7.68it/s]predicting train subjects sagittal:  30%|███       | 80/266 [00:10<00:24,  7.48it/s]predicting train subjects sagittal:  30%|███       | 81/266 [00:10<00:26,  7.08it/s]predicting train subjects sagittal:  31%|███       | 82/266 [00:10<00:26,  6.92it/s]predicting train subjects sagittal:  31%|███       | 83/266 [00:11<00:26,  6.79it/s]predicting train subjects sagittal:  32%|███▏      | 84/266 [00:11<00:26,  6.82it/s]predicting train subjects sagittal:  32%|███▏      | 85/266 [00:11<00:26,  6.82it/s]predicting train subjects sagittal:  32%|███▏      | 86/266 [00:11<00:26,  6.72it/s]predicting train subjects sagittal:  33%|███▎      | 87/266 [00:11<00:26,  6.75it/s]predicting train subjects sagittal:  33%|███▎      | 88/266 [00:11<00:26,  6.83it/s]predicting train subjects sagittal:  33%|███▎      | 89/266 [00:11<00:25,  6.92it/s]predicting train subjects sagittal:  34%|███▍      | 90/266 [00:12<00:25,  7.00it/s]predicting train subjects sagittal:  34%|███▍      | 91/266 [00:12<00:25,  6.99it/s]predicting train subjects sagittal:  35%|███▍      | 92/266 [00:12<00:24,  6.99it/s]predicting train subjects sagittal:  35%|███▍      | 93/266 [00:12<00:24,  7.08it/s]predicting train subjects sagittal:  35%|███▌      | 94/266 [00:12<00:24,  7.10it/s]predicting train subjects sagittal:  36%|███▌      | 95/266 [00:12<00:23,  7.14it/s]predicting train subjects sagittal:  36%|███▌      | 96/266 [00:12<00:22,  7.46it/s]predicting train subjects sagittal:  36%|███▋      | 97/266 [00:13<00:23,  7.29it/s]predicting train subjects sagittal:  37%|███▋      | 98/266 [00:13<00:22,  7.41it/s]predicting train subjects sagittal:  37%|███▋      | 99/266 [00:13<00:21,  7.95it/s]predicting train subjects sagittal:  38%|███▊      | 100/266 [00:13<00:20,  8.10it/s]predicting train subjects sagittal:  38%|███▊      | 101/266 [00:13<00:20,  8.19it/s]predicting train subjects sagittal:  38%|███▊      | 102/266 [00:13<00:19,  8.26it/s]predicting train subjects sagittal:  39%|███▊      | 103/266 [00:13<00:19,  8.29it/s]predicting train subjects sagittal:  39%|███▉      | 104/266 [00:13<00:19,  8.30it/s]predicting train subjects sagittal:  39%|███▉      | 105/266 [00:14<00:19,  8.31it/s]predicting train subjects sagittal:  40%|███▉      | 106/266 [00:14<00:19,  8.33it/s]predicting train subjects sagittal:  40%|████      | 107/266 [00:14<00:19,  8.31it/s]predicting train subjects sagittal:  41%|████      | 108/266 [00:14<00:19,  8.03it/s]predicting train subjects sagittal:  41%|████      | 109/266 [00:14<00:19,  7.96it/s]predicting train subjects sagittal:  41%|████▏     | 110/266 [00:14<00:19,  7.97it/s]predicting train subjects sagittal:  42%|████▏     | 111/266 [00:14<00:19,  7.95it/s]predicting train subjects sagittal:  42%|████▏     | 112/266 [00:14<00:19,  8.03it/s]predicting train subjects sagittal:  42%|████▏     | 113/266 [00:15<00:18,  8.11it/s]predicting train subjects sagittal:  43%|████▎     | 114/266 [00:15<00:18,  8.17it/s]predicting train subjects sagittal:  43%|████▎     | 115/266 [00:15<00:18,  8.22it/s]predicting train subjects sagittal:  44%|████▎     | 116/266 [00:15<00:18,  8.26it/s]predicting train subjects sagittal:  44%|████▍     | 117/266 [00:15<00:18,  8.25it/s]predicting train subjects sagittal:  44%|████▍     | 118/266 [00:15<00:17,  8.24it/s]predicting train subjects sagittal:  45%|████▍     | 119/266 [00:15<00:19,  7.61it/s]predicting train subjects sagittal:  45%|████▌     | 120/266 [00:15<00:19,  7.36it/s]predicting train subjects sagittal:  45%|████▌     | 121/266 [00:16<00:20,  7.21it/s]predicting train subjects sagittal:  46%|████▌     | 122/266 [00:16<00:20,  7.07it/s]predicting train subjects sagittal:  46%|████▌     | 123/266 [00:16<00:20,  6.93it/s]predicting train subjects sagittal:  47%|████▋     | 124/266 [00:16<00:20,  6.90it/s]predicting train subjects sagittal:  47%|████▋     | 125/266 [00:16<00:20,  6.91it/s]predicting train subjects sagittal:  47%|████▋     | 126/266 [00:16<00:20,  6.89it/s]predicting train subjects sagittal:  48%|████▊     | 127/266 [00:16<00:20,  6.85it/s]predicting train subjects sagittal:  48%|████▊     | 128/266 [00:17<00:20,  6.86it/s]predicting train subjects sagittal:  48%|████▊     | 129/266 [00:17<00:19,  6.90it/s]predicting train subjects sagittal:  49%|████▉     | 130/266 [00:17<00:19,  6.92it/s]predicting train subjects sagittal:  49%|████▉     | 131/266 [00:17<00:19,  6.83it/s]predicting train subjects sagittal:  50%|████▉     | 132/266 [00:17<00:19,  6.86it/s]predicting train subjects sagittal:  50%|█████     | 133/266 [00:17<00:19,  6.88it/s]predicting train subjects sagittal:  50%|█████     | 134/266 [00:17<00:19,  6.62it/s]predicting train subjects sagittal:  51%|█████     | 135/266 [00:18<00:19,  6.76it/s]predicting train subjects sagittal:  51%|█████     | 136/266 [00:18<00:18,  6.88it/s]predicting train subjects sagittal:  52%|█████▏    | 137/266 [00:18<00:18,  7.13it/s]predicting train subjects sagittal:  52%|█████▏    | 138/266 [00:18<00:17,  7.28it/s]predicting train subjects sagittal:  52%|█████▏    | 139/266 [00:18<00:17,  7.40it/s]predicting train subjects sagittal:  53%|█████▎    | 140/266 [00:18<00:17,  7.40it/s]predicting train subjects sagittal:  53%|█████▎    | 141/266 [00:18<00:16,  7.39it/s]predicting train subjects sagittal:  53%|█████▎    | 142/266 [00:19<00:16,  7.42it/s]predicting train subjects sagittal:  54%|█████▍    | 143/266 [00:19<00:16,  7.44it/s]predicting train subjects sagittal:  54%|█████▍    | 144/266 [00:19<00:16,  7.52it/s]predicting train subjects sagittal:  55%|█████▍    | 145/266 [00:19<00:15,  7.57it/s]predicting train subjects sagittal:  55%|█████▍    | 146/266 [00:19<00:15,  7.57it/s]predicting train subjects sagittal:  55%|█████▌    | 147/266 [00:19<00:15,  7.56it/s]predicting train subjects sagittal:  56%|█████▌    | 148/266 [00:19<00:15,  7.56it/s]predicting train subjects sagittal:  56%|█████▌    | 149/266 [00:19<00:15,  7.59it/s]predicting train subjects sagittal:  56%|█████▋    | 150/266 [00:20<00:15,  7.56it/s]predicting train subjects sagittal:  57%|█████▋    | 151/266 [00:20<00:15,  7.46it/s]predicting train subjects sagittal:  57%|█████▋    | 152/266 [00:20<00:15,  7.39it/s]predicting train subjects sagittal:  58%|█████▊    | 153/266 [00:20<00:15,  7.39it/s]predicting train subjects sagittal:  58%|█████▊    | 154/266 [00:20<00:15,  7.39it/s]predicting train subjects sagittal:  58%|█████▊    | 155/266 [00:20<00:14,  7.88it/s]predicting train subjects sagittal:  59%|█████▊    | 156/266 [00:20<00:13,  8.27it/s]predicting train subjects sagittal:  59%|█████▉    | 157/266 [00:20<00:12,  8.49it/s]predicting train subjects sagittal:  59%|█████▉    | 158/266 [00:21<00:12,  8.71it/s]predicting train subjects sagittal:  60%|█████▉    | 159/266 [00:21<00:12,  8.81it/s]predicting train subjects sagittal:  60%|██████    | 160/266 [00:21<00:11,  8.87it/s]predicting train subjects sagittal:  61%|██████    | 161/266 [00:21<00:11,  8.96it/s]predicting train subjects sagittal:  61%|██████    | 162/266 [00:21<00:11,  9.04it/s]predicting train subjects sagittal:  61%|██████▏   | 163/266 [00:21<00:11,  9.03it/s]predicting train subjects sagittal:  62%|██████▏   | 164/266 [00:21<00:11,  9.04it/s]predicting train subjects sagittal:  62%|██████▏   | 165/266 [00:21<00:11,  9.04it/s]predicting train subjects sagittal:  62%|██████▏   | 166/266 [00:21<00:11,  8.97it/s]predicting train subjects sagittal:  63%|██████▎   | 167/266 [00:22<00:11,  9.00it/s]predicting train subjects sagittal:  63%|██████▎   | 168/266 [00:22<00:10,  8.94it/s]predicting train subjects sagittal:  64%|██████▎   | 169/266 [00:22<00:10,  9.05it/s]predicting train subjects sagittal:  64%|██████▍   | 170/266 [00:22<00:10,  9.09it/s]predicting train subjects sagittal:  64%|██████▍   | 171/266 [00:22<00:10,  8.91it/s]predicting train subjects sagittal:  65%|██████▍   | 172/266 [00:22<00:10,  9.01it/s]predicting train subjects sagittal:  65%|██████▌   | 173/266 [00:22<00:10,  8.75it/s]predicting train subjects sagittal:  65%|██████▌   | 174/266 [00:22<00:10,  8.64it/s]predicting train subjects sagittal:  66%|██████▌   | 175/266 [00:23<00:10,  8.57it/s]predicting train subjects sagittal:  66%|██████▌   | 176/266 [00:23<00:10,  8.55it/s]predicting train subjects sagittal:  67%|██████▋   | 177/266 [00:23<00:10,  8.45it/s]predicting train subjects sagittal:  67%|██████▋   | 178/266 [00:23<00:10,  8.42it/s]predicting train subjects sagittal:  67%|██████▋   | 179/266 [00:23<00:10,  8.34it/s]predicting train subjects sagittal:  68%|██████▊   | 180/266 [00:23<00:10,  7.86it/s]predicting train subjects sagittal:  68%|██████▊   | 181/266 [00:23<00:10,  7.93it/s]predicting train subjects sagittal:  68%|██████▊   | 182/266 [00:23<00:10,  8.03it/s]predicting train subjects sagittal:  69%|██████▉   | 183/266 [00:24<00:10,  8.00it/s]predicting train subjects sagittal:  69%|██████▉   | 184/266 [00:24<00:10,  8.09it/s]predicting train subjects sagittal:  70%|██████▉   | 185/266 [00:24<00:09,  8.18it/s]predicting train subjects sagittal:  70%|██████▉   | 186/266 [00:24<00:09,  8.28it/s]predicting train subjects sagittal:  70%|███████   | 187/266 [00:24<00:09,  8.34it/s]predicting train subjects sagittal:  71%|███████   | 188/266 [00:24<00:09,  8.36it/s]predicting train subjects sagittal:  71%|███████   | 189/266 [00:24<00:09,  8.36it/s]predicting train subjects sagittal:  71%|███████▏  | 190/266 [00:24<00:09,  8.34it/s]predicting train subjects sagittal:  72%|███████▏  | 191/266 [00:24<00:09,  8.27it/s]predicting train subjects sagittal:  72%|███████▏  | 192/266 [00:25<00:08,  8.43it/s]predicting train subjects sagittal:  73%|███████▎  | 193/266 [00:25<00:08,  8.18it/s]predicting train subjects sagittal:  73%|███████▎  | 194/266 [00:25<00:09,  7.85it/s]predicting train subjects sagittal:  73%|███████▎  | 195/266 [00:25<00:08,  7.93it/s]predicting train subjects sagittal:  74%|███████▎  | 196/266 [00:25<00:08,  8.03it/s]predicting train subjects sagittal:  74%|███████▍  | 197/266 [00:25<00:08,  8.08it/s]predicting train subjects sagittal:  74%|███████▍  | 198/266 [00:25<00:08,  8.11it/s]predicting train subjects sagittal:  75%|███████▍  | 199/266 [00:25<00:08,  8.17it/s]predicting train subjects sagittal:  75%|███████▌  | 200/266 [00:26<00:08,  8.19it/s]predicting train subjects sagittal:  76%|███████▌  | 201/266 [00:26<00:07,  8.24it/s]predicting train subjects sagittal:  76%|███████▌  | 202/266 [00:26<00:07,  8.22it/s]predicting train subjects sagittal:  76%|███████▋  | 203/266 [00:26<00:07,  8.18it/s]predicting train subjects sagittal:  77%|███████▋  | 204/266 [00:26<00:07,  8.16it/s]predicting train subjects sagittal:  77%|███████▋  | 205/266 [00:26<00:07,  8.16it/s]predicting train subjects sagittal:  77%|███████▋  | 206/266 [00:26<00:07,  8.04it/s]predicting train subjects sagittal:  78%|███████▊  | 207/266 [00:26<00:07,  7.95it/s]predicting train subjects sagittal:  78%|███████▊  | 208/266 [00:27<00:07,  7.92it/s]predicting train subjects sagittal:  79%|███████▊  | 209/266 [00:27<00:07,  7.91it/s]predicting train subjects sagittal:  79%|███████▉  | 210/266 [00:27<00:07,  7.94it/s]predicting train subjects sagittal:  79%|███████▉  | 211/266 [00:27<00:07,  7.80it/s]predicting train subjects sagittal:  80%|███████▉  | 212/266 [00:27<00:07,  7.68it/s]predicting train subjects sagittal:  80%|████████  | 213/266 [00:27<00:06,  7.97it/s]predicting train subjects sagittal:  80%|████████  | 214/266 [00:27<00:06,  8.16it/s]predicting train subjects sagittal:  81%|████████  | 215/266 [00:27<00:06,  8.37it/s]predicting train subjects sagittal:  81%|████████  | 216/266 [00:28<00:05,  8.53it/s]predicting train subjects sagittal:  82%|████████▏ | 217/266 [00:28<00:05,  8.62it/s]predicting train subjects sagittal:  82%|████████▏ | 218/266 [00:28<00:05,  8.59it/s]predicting train subjects sagittal:  82%|████████▏ | 219/266 [00:28<00:05,  8.52it/s]predicting train subjects sagittal:  83%|████████▎ | 220/266 [00:28<00:05,  8.41it/s]predicting train subjects sagittal:  83%|████████▎ | 221/266 [00:28<00:05,  8.40it/s]predicting train subjects sagittal:  83%|████████▎ | 222/266 [00:28<00:05,  8.44it/s]predicting train subjects sagittal:  84%|████████▍ | 223/266 [00:28<00:05,  8.45it/s]predicting train subjects sagittal:  84%|████████▍ | 224/266 [00:28<00:04,  8.54it/s]predicting train subjects sagittal:  85%|████████▍ | 225/266 [00:29<00:04,  8.38it/s]predicting train subjects sagittal:  85%|████████▍ | 226/266 [00:29<00:04,  8.47it/s]predicting train subjects sagittal:  85%|████████▌ | 227/266 [00:29<00:04,  8.54it/s]predicting train subjects sagittal:  86%|████████▌ | 228/266 [00:29<00:04,  8.57it/s]predicting train subjects sagittal:  86%|████████▌ | 229/266 [00:29<00:04,  8.64it/s]predicting train subjects sagittal:  86%|████████▋ | 230/266 [00:29<00:04,  8.60it/s]predicting train subjects sagittal:  87%|████████▋ | 231/266 [00:29<00:04,  8.36it/s]predicting train subjects sagittal:  87%|████████▋ | 232/266 [00:29<00:04,  8.36it/s]predicting train subjects sagittal:  88%|████████▊ | 233/266 [00:30<00:04,  8.25it/s]predicting train subjects sagittal:  88%|████████▊ | 234/266 [00:30<00:03,  8.22it/s]predicting train subjects sagittal:  88%|████████▊ | 235/266 [00:30<00:03,  8.25it/s]predicting train subjects sagittal:  89%|████████▊ | 236/266 [00:30<00:03,  8.31it/s]predicting train subjects sagittal:  89%|████████▉ | 237/266 [00:30<00:03,  8.37it/s]predicting train subjects sagittal:  89%|████████▉ | 238/266 [00:30<00:03,  8.27it/s]predicting train subjects sagittal:  90%|████████▉ | 239/266 [00:30<00:03,  8.12it/s]predicting train subjects sagittal:  90%|█████████ | 240/266 [00:30<00:03,  8.18it/s]predicting train subjects sagittal:  91%|█████████ | 241/266 [00:31<00:03,  8.18it/s]predicting train subjects sagittal:  91%|█████████ | 242/266 [00:31<00:02,  8.20it/s]predicting train subjects sagittal:  91%|█████████▏| 243/266 [00:31<00:02,  8.28it/s]predicting train subjects sagittal:  92%|█████████▏| 244/266 [00:31<00:02,  8.31it/s]predicting train subjects sagittal:  92%|█████████▏| 245/266 [00:31<00:02,  8.31it/s]predicting train subjects sagittal:  92%|█████████▏| 246/266 [00:31<00:02,  8.14it/s]predicting train subjects sagittal:  93%|█████████▎| 247/266 [00:31<00:02,  8.18it/s]predicting train subjects sagittal:  93%|█████████▎| 248/266 [00:31<00:02,  8.21it/s]predicting train subjects sagittal:  94%|█████████▎| 249/266 [00:32<00:02,  7.83it/s]predicting train subjects sagittal:  94%|█████████▍| 250/266 [00:32<00:02,  7.58it/s]predicting train subjects sagittal:  94%|█████████▍| 251/266 [00:32<00:02,  7.25it/s]predicting train subjects sagittal:  95%|█████████▍| 252/266 [00:32<00:01,  7.22it/s]predicting train subjects sagittal:  95%|█████████▌| 253/266 [00:32<00:01,  7.14it/s]predicting train subjects sagittal:  95%|█████████▌| 254/266 [00:32<00:01,  7.14it/s]predicting train subjects sagittal:  96%|█████████▌| 255/266 [00:32<00:01,  7.09it/s]predicting train subjects sagittal:  96%|█████████▌| 256/266 [00:33<00:01,  7.00it/s]predicting train subjects sagittal:  97%|█████████▋| 257/266 [00:33<00:01,  6.71it/s]predicting train subjects sagittal:  97%|█████████▋| 258/266 [00:33<00:01,  6.82it/s]predicting train subjects sagittal:  97%|█████████▋| 259/266 [00:33<00:01,  6.93it/s]predicting train subjects sagittal:  98%|█████████▊| 260/266 [00:33<00:00,  7.02it/s]predicting train subjects sagittal:  98%|█████████▊| 261/266 [00:33<00:00,  7.05it/s]predicting train subjects sagittal:  98%|█████████▊| 262/266 [00:33<00:00,  7.09it/s]predicting train subjects sagittal:  99%|█████████▉| 263/266 [00:34<00:00,  7.15it/s]predicting train subjects sagittal:  99%|█████████▉| 264/266 [00:34<00:00,  7.18it/s]predicting train subjects sagittal: 100%|█████████▉| 265/266 [00:34<00:00,  7.22it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:34<00:00,  7.10it/s]predicting train subjects sagittal: 100%|██████████| 266/266 [00:34<00:00,  7.72it/s]
saving BB  test1-THALAMUS:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS: 100%|██████████| 4/4 [00:00<00:00, 79.62it/s]
saving BB  train1-THALAMUS:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS:   3%|▎         | 8/266 [00:00<00:03, 74.57it/s]saving BB  train1-THALAMUS:   6%|▌         | 15/266 [00:00<00:03, 72.84it/s]saving BB  train1-THALAMUS:   8%|▊         | 22/266 [00:00<00:03, 71.78it/s]saving BB  train1-THALAMUS:  11%|█▏        | 30/266 [00:00<00:03, 71.93it/s]saving BB  train1-THALAMUS:  14%|█▍        | 38/266 [00:00<00:03, 74.13it/s]saving BB  train1-THALAMUS:  18%|█▊        | 47/266 [00:00<00:02, 76.95it/s]saving BB  train1-THALAMUS:  21%|██        | 56/266 [00:00<00:02, 79.25it/s]saving BB  train1-THALAMUS:  24%|██▍       | 65/266 [00:00<00:02, 81.48it/s]saving BB  train1-THALAMUS:  28%|██▊       | 74/266 [00:00<00:02, 81.35it/s]saving BB  train1-THALAMUS:  31%|███       | 82/266 [00:01<00:02, 80.27it/s]saving BB  train1-THALAMUS:  34%|███▍      | 90/266 [00:01<00:02, 79.30it/s]saving BB  train1-THALAMUS:  37%|███▋      | 98/266 [00:01<00:02, 74.97it/s]saving BB  train1-THALAMUS:  40%|████      | 107/266 [00:01<00:02, 76.95it/s]saving BB  train1-THALAMUS:  43%|████▎     | 115/266 [00:01<00:01, 76.27it/s]saving BB  train1-THALAMUS:  46%|████▌     | 123/266 [00:01<00:01, 76.79it/s]saving BB  train1-THALAMUS:  49%|████▉     | 131/266 [00:01<00:01, 76.68it/s]saving BB  train1-THALAMUS:  52%|█████▏    | 139/266 [00:01<00:01, 76.46it/s]saving BB  train1-THALAMUS:  55%|█████▌    | 147/266 [00:01<00:01, 76.04it/s]saving BB  train1-THALAMUS:  58%|█████▊    | 155/266 [00:02<00:01, 76.44it/s]saving BB  train1-THALAMUS:  62%|██████▏   | 165/266 [00:02<00:01, 80.33it/s]saving BB  train1-THALAMUS:  66%|██████▌   | 175/266 [00:02<00:01, 83.30it/s]saving BB  train1-THALAMUS:  69%|██████▉   | 184/266 [00:02<00:00, 84.77it/s]saving BB  train1-THALAMUS:  73%|███████▎  | 193/266 [00:02<00:00, 85.05it/s]saving BB  train1-THALAMUS:  76%|███████▌  | 202/266 [00:02<00:00, 84.01it/s]saving BB  train1-THALAMUS:  79%|███████▉  | 211/266 [00:02<00:00, 80.51it/s]saving BB  train1-THALAMUS:  83%|████████▎ | 220/266 [00:02<00:00, 81.19it/s]saving BB  train1-THALAMUS:  86%|████████▌ | 229/266 [00:02<00:00, 80.06it/s]saving BB  train1-THALAMUS:  89%|████████▉ | 238/266 [00:03<00:00, 80.03it/s]saving BB  train1-THALAMUS:  93%|█████████▎| 247/266 [00:03<00:00, 82.13it/s]saving BB  train1-THALAMUS:  96%|█████████▌| 256/266 [00:03<00:00, 77.73it/s]saving BB  train1-THALAMUS:  99%|█████████▉| 264/266 [00:03<00:00, 76.05it/s]saving BB  train1-THALAMUS: 100%|██████████| 266/266 [00:03<00:00, 78.64it/s]
saving BB  test1-THALAMUS Sagittal:   0%|          | 0/4 [00:00<?, ?it/s]saving BB  test1-THALAMUS Sagittal: 100%|██████████| 4/4 [00:00<00:00, 74.91it/s]
saving BB  train1-THALAMUS Sagittal:   0%|          | 0/266 [00:00<?, ?it/s]saving BB  train1-THALAMUS Sagittal:   3%|▎         | 8/266 [00:00<00:03, 72.61it/s]saving BB  train1-THALAMUS Sagittal:   6%|▌         | 15/266 [00:00<00:03, 69.50it/s]saving BB  train1-THALAMUS Sagittal:   8%|▊         | 22/266 [00:00<00:03, 68.48it/s]saving BB  train1-THALAMUS Sagittal:  11%|█▏        | 30/266 [00:00<00:03, 70.50it/s]saving BB  train1-THALAMUS Sagittal:  14%|█▍        | 38/266 [00:00<00:03, 72.36it/s]saving BB  train1-THALAMUS Sagittal:  18%|█▊        | 47/266 [00:00<00:02, 74.98it/s]saving BB  train1-THALAMUS Sagittal:  21%|██        | 55/266 [00:00<00:02, 76.37it/s]saving BB  train1-THALAMUS Sagittal:  24%|██▍       | 64/266 [00:00<00:02, 79.73it/s]saving BB  train1-THALAMUS Sagittal:  27%|██▋       | 73/266 [00:00<00:02, 82.18it/s]saving BB  train1-THALAMUS Sagittal:  31%|███       | 82/266 [00:01<00:02, 81.70it/s]saving BB  train1-THALAMUS Sagittal:  34%|███▍      | 91/266 [00:01<00:02, 79.93it/s]saving BB  train1-THALAMUS Sagittal:  37%|███▋      | 99/266 [00:01<00:02, 79.93it/s]saving BB  train1-THALAMUS Sagittal:  40%|████      | 107/266 [00:01<00:02, 70.87it/s]saving BB  train1-THALAMUS Sagittal:  43%|████▎     | 115/266 [00:01<00:02, 70.31it/s]saving BB  train1-THALAMUS Sagittal:  46%|████▌     | 123/266 [00:01<00:01, 72.52it/s]saving BB  train1-THALAMUS Sagittal:  49%|████▉     | 131/266 [00:01<00:01, 73.45it/s]saving BB  train1-THALAMUS Sagittal:  52%|█████▏    | 139/266 [00:01<00:01, 73.59it/s]saving BB  train1-THALAMUS Sagittal:  55%|█████▌    | 147/266 [00:01<00:01, 74.30it/s]saving BB  train1-THALAMUS Sagittal:  58%|█████▊    | 155/266 [00:02<00:01, 75.24it/s]saving BB  train1-THALAMUS Sagittal:  62%|██████▏   | 165/266 [00:02<00:01, 79.17it/s]saving BB  train1-THALAMUS Sagittal:  65%|██████▌   | 174/266 [00:02<00:01, 82.02it/s]saving BB  train1-THALAMUS Sagittal:  69%|██████▉   | 184/266 [00:02<00:00, 84.33it/s]saving BB  train1-THALAMUS Sagittal:  73%|███████▎  | 193/266 [00:02<00:00, 74.75it/s]saving BB  train1-THALAMUS Sagittal:  76%|███████▌  | 201/266 [00:02<00:00, 65.96it/s]saving BB  train1-THALAMUS Sagittal:  79%|███████▊  | 209/266 [00:02<00:00, 68.41it/s]saving BB  train1-THALAMUS Sagittal:  82%|████████▏ | 217/266 [00:02<00:00, 69.41it/s]saving BB  train1-THALAMUS Sagittal:  85%|████████▍ | 226/266 [00:03<00:00, 73.12it/s]saving BB  train1-THALAMUS Sagittal:  88%|████████▊ | 235/266 [00:03<00:00, 76.64it/s]saving BB  train1-THALAMUS Sagittal:  92%|█████████▏| 244/266 [00:03<00:00, 78.09it/s]saving BB  train1-THALAMUS Sagittal:  95%|█████████▌| 253/266 [00:03<00:00, 78.89it/s]saving BB  train1-THALAMUS Sagittal:  98%|█████████▊| 261/266 [00:03<00:00, 78.58it/s]saving BB  train1-THALAMUS Sagittal: 100%|██████████| 266/266 [00:03<00:00, 75.61it/s]
Loading train:   0%|          | 0/266 [00:00<?, ?it/s]Loading train:   0%|          | 1/266 [00:01<04:49,  1.09s/it]Loading train:   1%|          | 2/266 [00:02<04:41,  1.07s/it]Loading train:   1%|          | 3/266 [00:02<04:21,  1.01it/s]Loading train:   2%|▏         | 4/266 [00:03<04:04,  1.07it/s]Loading train:   2%|▏         | 5/266 [00:04<04:09,  1.05it/s]Loading train:   2%|▏         | 6/266 [00:05<03:45,  1.15it/s]Loading train:   3%|▎         | 7/266 [00:06<03:29,  1.24it/s]Loading train:   3%|▎         | 8/266 [00:06<03:10,  1.35it/s]Loading train:   3%|▎         | 9/266 [00:07<02:58,  1.44it/s]Loading train:   4%|▍         | 10/266 [00:07<02:51,  1.49it/s]Loading train:   4%|▍         | 11/266 [00:08<02:48,  1.52it/s]Loading train:   5%|▍         | 12/266 [00:09<02:45,  1.54it/s]Loading train:   5%|▍         | 13/266 [00:09<02:43,  1.55it/s]Loading train:   5%|▌         | 14/266 [00:10<02:44,  1.53it/s]Loading train:   6%|▌         | 15/266 [00:11<02:43,  1.53it/s]Loading train:   6%|▌         | 16/266 [00:11<02:43,  1.53it/s]Loading train:   6%|▋         | 17/266 [00:12<02:39,  1.56it/s]Loading train:   7%|▋         | 18/266 [00:12<02:37,  1.58it/s]Loading train:   7%|▋         | 19/266 [00:13<02:35,  1.59it/s]Loading train:   8%|▊         | 20/266 [00:14<02:32,  1.61it/s]Loading train:   8%|▊         | 21/266 [00:14<02:31,  1.62it/s]Loading train:   8%|▊         | 22/266 [00:15<02:30,  1.62it/s]Loading train:   9%|▊         | 23/266 [00:15<02:27,  1.64it/s]Loading train:   9%|▉         | 24/266 [00:16<02:28,  1.63it/s]Loading train:   9%|▉         | 25/266 [00:17<02:25,  1.66it/s]Loading train:  10%|▉         | 26/266 [00:17<02:21,  1.69it/s]Loading train:  10%|█         | 27/266 [00:18<02:21,  1.69it/s]Loading train:  11%|█         | 28/266 [00:18<02:21,  1.69it/s]Loading train:  11%|█         | 29/266 [00:19<02:19,  1.70it/s]Loading train:  11%|█▏        | 30/266 [00:20<02:20,  1.68it/s]Loading train:  12%|█▏        | 31/266 [00:20<02:20,  1.68it/s]Loading train:  12%|█▏        | 32/266 [00:21<02:16,  1.72it/s]Loading train:  12%|█▏        | 33/266 [00:21<02:13,  1.74it/s]Loading train:  13%|█▎        | 34/266 [00:22<02:12,  1.75it/s]Loading train:  13%|█▎        | 35/266 [00:22<02:12,  1.74it/s]Loading train:  14%|█▎        | 36/266 [00:23<02:12,  1.74it/s]Loading train:  14%|█▍        | 37/266 [00:24<02:12,  1.73it/s]Loading train:  14%|█▍        | 38/266 [00:24<02:12,  1.72it/s]Loading train:  15%|█▍        | 39/266 [00:25<02:12,  1.72it/s]Loading train:  15%|█▌        | 40/266 [00:25<02:10,  1.73it/s]Loading train:  15%|█▌        | 41/266 [00:26<02:10,  1.73it/s]Loading train:  16%|█▌        | 42/266 [00:26<02:06,  1.77it/s]Loading train:  16%|█▌        | 43/266 [00:27<02:04,  1.79it/s]Loading train:  17%|█▋        | 44/266 [00:27<01:57,  1.88it/s]Loading train:  17%|█▋        | 45/266 [00:28<01:53,  1.95it/s]Loading train:  17%|█▋        | 46/266 [00:28<01:50,  1.99it/s]Loading train:  18%|█▊        | 47/266 [00:29<01:51,  1.97it/s]Loading train:  18%|█▊        | 48/266 [00:29<01:49,  1.98it/s]Loading train:  18%|█▊        | 49/266 [00:30<01:49,  1.99it/s]Loading train:  19%|█▉        | 50/266 [00:30<01:48,  1.98it/s]Loading train:  19%|█▉        | 51/266 [00:31<01:50,  1.95it/s]Loading train:  20%|█▉        | 52/266 [00:31<01:48,  1.96it/s]Loading train:  20%|█▉        | 53/266 [00:32<01:50,  1.94it/s]Loading train:  20%|██        | 54/266 [00:33<01:47,  1.96it/s]Loading train:  21%|██        | 55/266 [00:33<01:47,  1.96it/s]Loading train:  21%|██        | 56/266 [00:34<01:46,  1.97it/s]Loading train:  21%|██▏       | 57/266 [00:34<01:45,  1.98it/s]Loading train:  22%|██▏       | 58/266 [00:35<01:46,  1.96it/s]Loading train:  22%|██▏       | 59/266 [00:35<01:45,  1.96it/s]Loading train:  23%|██▎       | 60/266 [00:36<01:46,  1.93it/s]Loading train:  23%|██▎       | 61/266 [00:36<01:42,  2.00it/s]Loading train:  23%|██▎       | 62/266 [00:37<01:40,  2.04it/s]Loading train:  24%|██▎       | 63/266 [00:37<01:38,  2.07it/s]Loading train:  24%|██▍       | 64/266 [00:37<01:37,  2.07it/s]Loading train:  24%|██▍       | 65/266 [00:38<01:38,  2.05it/s]Loading train:  25%|██▍       | 66/266 [00:38<01:37,  2.06it/s]Loading train:  25%|██▌       | 67/266 [00:39<01:36,  2.07it/s]Loading train:  26%|██▌       | 68/266 [00:39<01:36,  2.05it/s]Loading train:  26%|██▌       | 69/266 [00:40<01:36,  2.04it/s]Loading train:  26%|██▋       | 70/266 [00:40<01:35,  2.05it/s]Loading train:  27%|██▋       | 71/266 [00:41<01:33,  2.09it/s]Loading train:  27%|██▋       | 72/266 [00:41<01:33,  2.07it/s]Loading train:  27%|██▋       | 73/266 [00:42<01:33,  2.05it/s]Loading train:  28%|██▊       | 74/266 [00:42<01:32,  2.07it/s]Loading train:  28%|██▊       | 75/266 [00:43<01:31,  2.09it/s]Loading train:  29%|██▊       | 76/266 [00:43<01:30,  2.10it/s]Loading train:  29%|██▉       | 77/266 [00:44<01:29,  2.12it/s]Loading train:  29%|██▉       | 78/266 [00:44<01:36,  1.94it/s]Loading train:  30%|██▉       | 79/266 [00:45<01:38,  1.91it/s]Loading train:  30%|███       | 80/266 [00:45<01:40,  1.85it/s]Loading train:  30%|███       | 81/266 [00:46<01:42,  1.80it/s]Loading train:  31%|███       | 82/266 [00:47<01:43,  1.78it/s]Loading train:  31%|███       | 83/266 [00:47<01:44,  1.75it/s]Loading train:  32%|███▏      | 84/266 [00:48<01:43,  1.76it/s]Loading train:  32%|███▏      | 85/266 [00:48<01:42,  1.77it/s]Loading train:  32%|███▏      | 86/266 [00:49<01:44,  1.73it/s]Loading train:  33%|███▎      | 87/266 [00:50<01:42,  1.75it/s]Loading train:  33%|███▎      | 88/266 [00:50<01:39,  1.78it/s]Loading train:  33%|███▎      | 89/266 [00:51<01:38,  1.79it/s]Loading train:  34%|███▍      | 90/266 [00:51<01:39,  1.77it/s]Loading train:  34%|███▍      | 91/266 [00:52<01:38,  1.77it/s]Loading train:  35%|███▍      | 92/266 [00:52<01:38,  1.77it/s]Loading train:  35%|███▍      | 93/266 [00:53<01:38,  1.76it/s]Loading train:  35%|███▌      | 94/266 [00:53<01:39,  1.73it/s]Loading train:  36%|███▌      | 95/266 [00:54<01:37,  1.76it/s]Loading train:  36%|███▌      | 96/266 [00:55<01:51,  1.53it/s]Loading train:  36%|███▋      | 97/266 [00:56<02:07,  1.33it/s]
Epoch 00055: val_mDice did not improve from 0.51052
Restoring model weights from the end of the best epoch
Epoch 00055: early stopping
{'val_loss': [0.30386778415413573, 0.2864649781258777, 0.28143906680634245, 0.2663515196181834, 0.17770332307554781, 0.2553379413438961, 0.2669856834691018, 0.18044867715798318, 0.05910932598635554, 0.303827277966775, 0.05047165940050036, 0.053780235175509006, 0.09978198382304981, -0.027987860550638288, -0.01068266702350229, 0.16145343374228105, 0.0004893864970654249, -0.005479704006575048, -0.0023335142177529633, 0.05668746127048507, 0.05211161047918722, -0.004577275307383388, 0.1355733245727606, -0.02705491497181356, 0.0704835929791443, 0.02798581955721602, 0.0881712893024087, -0.0007563948165625334, 0.054853447480127215, -0.019322065520100296, -0.003321551950648427, -0.006701444799546152, 0.0376352149178274, 0.23397130705416203, 0.0030922710429877043, -0.0034932392300106585, 0.04674965993035585, -0.008236088789999485, 0.03159414615947753, 0.008517469628714025, 0.0369756449945271, 0.1407077395124361, 0.0027304391842335463, 0.13767217053100467, 0.10093034792225808, 0.06636026356136426, 0.06863303110003471, 0.01943988009588793, 0.008161941834259778, 0.010483823367394507, 0.03599561471492052, 0.06208408786915243, 0.03701668855501339, 0.064127279270906, 0.1028553961077705], 'val_acc': [0.990691791754216, 0.9928750237450004, 0.991334103513509, 0.9933405476622283, 0.9930976424366236, 0.9922124459408224, 0.9928354192525148, 0.9929255312308669, 0.9935535015538335, 0.9808217063546181, 0.9927842896431684, 0.9941468611359596, 0.9938718532212079, 0.992558853700757, 0.9942799983546138, 0.993984098546207, 0.9934867792762816, 0.9938584421761334, 0.9940508254803717, 0.9913668376393616, 0.9941580952145159, 0.9941010260954499, 0.9932526126503944, 0.9927596561610699, 0.9940134095959365, 0.9938550125807524, 0.9938840162940323, 0.9942039186134934, 0.9934905171394348, 0.9938730988651514, 0.9941384443081915, 0.9943638732656837, 0.9943242757581174, 0.989714290946722, 0.9939226801507175, 0.9939541704952717, 0.9934057001955807, 0.9943598220124841, 0.9937761295586824, 0.9940813798457384, 0.994128777179867, 0.9923998392187059, 0.9942584848031402, 0.9940464622341096, 0.9933240162208676, 0.994232605677098, 0.9942522519268095, 0.994127219542861, 0.9941216045990586, 0.9940130952745676, 0.994027754291892, 0.9940489591099322, 0.9942893558181822, 0.9940648581832647, 0.9942188896238804], 'val_mDice': [0.39719757428359204, 0.4299460909827122, 0.4387188574770198, 0.45781969238377307, 0.4584123812310282, 0.44341349537717323, 0.4621832096017897, 0.477030070615001, 0.48842027550563216, 0.4033375525614247, 0.4594795824959874, 0.5014386336551979, 0.49260747036896646, 0.4691550513962284, 0.510520680109039, 0.5071860970929265, 0.4998980788514018, 0.5045876291114837, 0.5033335898770019, 0.43611754197627306, 0.507826047251001, 0.49666499020531774, 0.49833883880637586, 0.4641427821479738, 0.5012956017162651, 0.5040339281549677, 0.5030790675664343, 0.487995299645263, 0.5033685493981466, 0.47534424695186317, 0.5038476275512949, 0.50722963467706, 0.509131217841059, 0.4631839145440608, 0.48383329937700115, 0.49496044986881316, 0.4942267609294504, 0.5100556771503761, 0.5010016623418778, 0.5022337206173688, 0.5046586913522333, 0.4846880480182034, 0.5064338375814259, 0.5007441149791703, 0.49425164819695055, 0.5003719090018421, 0.4954409640049562, 0.4963202547514811, 0.49579473270568997, 0.5026852560748395, 0.5004761589225382, 0.49950155487750053, 0.5046018195571379, 0.5015445046592504, 0.49555847013834864], 'loss': [0.1326693001620803, 0.08535549099923592, 0.07563170283492661, 0.06992371849939912, 0.06627420400584964, 0.06198382131089673, 0.06007198826823761, 0.05738299486043493, 0.05589587322002478, 0.05664834531550625, 0.05185148851913318, 0.05463259099210418, 0.04980836823137709, 0.04969756614771318, 0.04943880326703989, 0.04884850647483565, 0.047960609657272474, 0.04773926288512743, 0.046363263690362234, 0.04630568463225889, 0.04555146808205712, 0.046114456746045536, 0.0442565741131942, 0.04502352549578374, 0.04456337144185237, 0.04353494275124546, 0.04368250379132189, 0.04462865856316525, 0.042403958981095585, 0.04267914016961885, 0.04065921010785614, 0.039443725651013, 0.03905356732691929, 0.03896380044139885, 0.037965222785451266, 0.038337633591108485, 0.038344356674924744, 0.038188582508492254, 0.03823500265362676, 0.03795569896891902, 0.03768400998187284, 0.03742702084751346, 0.038003810207139076, 0.03841691283353103, 0.038243010887717956, 0.03581418106714386, 0.03598044323591567, 0.035311784029554046, 0.0352647655293046, 0.03702805778789675, 0.03500045124240989, 0.03519562194263449, 0.03515208614430694, 0.034747469917924836, 0.03499761821518823], 'acc': [0.9834446775258121, 0.9910726522864732, 0.9920291528270208, 0.9926296295889221, 0.9929829264221754, 0.9933806496227717, 0.9935779774855193, 0.9937349269444676, 0.9939242828608362, 0.993975951909271, 0.9942494185181103, 0.9940935879602961, 0.9944461469895696, 0.994511613429486, 0.9945804949655508, 0.9945832964531065, 0.9946885563721429, 0.9947362567498402, 0.9948155725682337, 0.9948300261578937, 0.994905410666658, 0.9948420222522739, 0.994963502370066, 0.9949771348879121, 0.9950342667536327, 0.9950800154133514, 0.9950787181691922, 0.9951039003109813, 0.9951570319481202, 0.9951446323122202, 0.995355480193981, 0.9954526404255813, 0.9954842689755599, 0.9954794930902053, 0.9955216014829817, 0.9954995707212404, 0.9955279859250551, 0.9955696688028555, 0.9955715046079092, 0.9955720570952521, 0.9955906136271097, 0.9956012330806475, 0.9956131166484944, 0.9955764753804385, 0.9955732019171699, 0.9957241506835262, 0.9957634436207475, 0.9957930146228204, 0.9957970761942562, 0.9957831596142602, 0.9958298087494013, 0.9958245782427246, 0.9958214338155135, 0.9958005079176557, 0.9958112702228972], 'mDice': [0.7460046880856326, 0.8342101082755022, 0.852972401779404, 0.8640082395410623, 0.8710902086292703, 0.8794452631975554, 0.8831570369053915, 0.8884455236433496, 0.8913166599815343, 0.8897904034007561, 0.8992265538532476, 0.8937467600558608, 0.9032114079858113, 0.9033960843768394, 0.9038762030139457, 0.905051354013631, 0.9067708125883962, 0.9071890619299172, 0.9098994284054537, 0.9100030707062309, 0.9114702091415654, 0.9103828833343344, 0.9140291581159964, 0.9124863519338389, 0.9133780878537134, 0.915407355417285, 0.9151200817269676, 0.9132192438472448, 0.9176276207598769, 0.9170859278103637, 0.9210118355513643, 0.9233824160714813, 0.9241448660438811, 0.9243273368083434, 0.9263006347707995, 0.9255652378747254, 0.9255391462513749, 0.9258281710367523, 0.9257294788335111, 0.9262941870191337, 0.9268267719992448, 0.9273340025817364, 0.9261762685299031, 0.92536054903658, 0.9257173712617556, 0.9304861851538858, 0.9301369460504119, 0.9314607366931636, 0.9315460840279292, 0.9280350473696842, 0.932060858659577, 0.931677240309184, 0.9317545147117761, 0.9325710619743851, 0.9320743036101168], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
---------------------- check Layers Step ------------------------------
 N: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 2  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1

 #layer 3 #layers changed False
---------------------------------------------------------------
 Nucleus: [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  | GPU: 2  | SD 2  | Dropout 0.5  | LR 0.001  | NL 3  |  Cascade |  FM 20 |  Upsample 1
Experiment: exp6
SubExperiment: sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a
---------------------------------------------------------------
Error in label values min 0.0 max 7.0      2-AV
Error in label values min 0.0 max 4.0      4-VA
Error in label values min 0.0 max 16.0      5-VLa
Error in label values min 0.0 max 20.0      6-VLP
Error in label valuesLoading train:  37%|███▋      | 98/266 [00:57<02:07,  1.31it/s]Loading train:  37%|███▋      | 99/266 [00:57<02:01,  1.37it/s]Loading train:  38%|███▊      | 100/266 [00:58<02:02,  1.36it/s]Loading train:  38%|███▊      | 101/266 [00:59<01:50,  1.49it/s]Loading train:  38%|███▊      | 102/266 [00:59<01:43,  1.58it/s]Loading train:  39%|███▊      | 103/266 [01:00<01:38,  1.65it/s]Loading train:  39%|███▉      | 104/266 [01:00<01:34,  1.71it/s]Loading train:  39%|███▉      | 105/266 [01:01<01:32,  1.74it/s]Loading train:  40%|███▉      | 106/266 [01:01<01:29,  1.80it/s]Loading train:  40%|████      | 107/266 [01:02<01:26,  1.84it/s]Loading train:  41%|████      | 108/266 [01:02<01:26,  1.83it/s]Loading train:  41%|████      | 109/266 [01:03<01:24,  1.86it/s]Loading train:  41%|████▏     | 110/266 [01:03<01:22,  1.88it/s]Loading train:  42%|████▏     | 111/266 [01:04<01:22,  1.88it/s]Loading train:  42%|████▏     | 112/266 [01:04<01:21,  1.89it/s]Loading train:  42%|████▏     | 113/266 [01:05<01:20,  1.89it/s]Loading train:  43%|████▎     | 114/266 [01:05<01:20,  1.89it/s]Loading train:  43%|████▎     | 115/266 [01:06<01:19,  1.91it/s]Loading train:  44%|████▎     | 116/266 [01:07<01:18,  1.91it/s]Loading train:  44%|████▍     | 117/266 [01:07<01:20,  1.84it/s]Loading train:  44%|████▍     | 118/266 [01:08<01:23,  1.77it/s]Loading train:  45%|████▍     | 119/266 [01:08<01:27,  1.69it/s]Loading train:  45%|████▌     | 120/266 [01:09<01:27,  1.67it/s]Loading train:  45%|████▌     | 121/266 [01:10<01:29,  1.62it/s]Loading train:  46%|████▌     | 122/266 [01:10<01:27,  1.64it/s]Loading train:  46%|████▌     | 123/266 [01:11<01:26,  1.66it/s]Loading train:  47%|████▋     | 124/266 [01:11<01:24,  1.67it/s]Loading train:  47%|████▋     | 125/266 [01:12<01:24,  1.67it/s]Loading train:  47%|████▋     | 126/266 [01:13<01:24,  1.65it/s]Loading train:  48%|████▊     | 127/266 [01:13<01:24,  1.64it/s]Loading train:  48%|████▊     | 128/266 [01:14<01:24,  1.64it/s]Loading train:  48%|████▊     | 129/266 [01:14<01:22,  1.66it/s]Loading train:  49%|████▉     | 130/266 [01:15<01:21,  1.66it/s]Loading train:  49%|████▉     | 131/266 [01:16<01:22,  1.63it/s]Loading train:  50%|████▉     | 132/266 [01:16<01:20,  1.66it/s]Loading train:  50%|█████     | 133/266 [01:17<01:20,  1.65it/s]Loading train:  50%|█████     | 134/266 [01:17<01:19,  1.67it/s]Loading train:  51%|█████     | 135/266 [01:18<01:19,  1.65it/s]Loading train:  51%|█████     | 136/266 [01:19<01:20,  1.61it/s]Loading train:  52%|█████▏    | 137/266 [01:19<01:21,  1.59it/s]Loading train:  52%|█████▏    | 138/266 [01:20<01:18,  1.63it/s]Loading train:  52%|█████▏    | 139/266 [01:21<01:16,  1.66it/s]Loading train:  53%|█████▎    | 140/266 [01:21<01:16,  1.64it/s]Loading train:  53%|█████▎    | 141/266 [01:22<01:14,  1.68it/s]Loading train:  53%|█████▎    | 142/266 [01:22<01:12,  1.71it/s]Loading train:  54%|█████▍    | 143/266 [01:23<01:10,  1.75it/s]Loading train:  54%|█████▍    | 144/266 [01:23<01:09,  1.77it/s]Loading train:  55%|█████▍    | 145/266 [01:24<01:07,  1.80it/s]Loading train:  55%|█████▍    | 146/266 [01:24<01:06,  1.82it/s]Loading train:  55%|█████▌    | 147/266 [01:25<01:05,  1.83it/s]Loading train:  56%|█████▌    | 148/266 [01:26<01:06,  1.78it/s]Loading train:  56%|█████▌    | 149/266 [01:26<01:05,  1.78it/s]Loading train:  56%|█████▋    | 150/266 [01:27<01:05,  1.78it/s]Loading train:  57%|█████▋    | 151/266 [01:27<01:06,  1.73it/s]Loading train:  57%|█████▋    | 152/266 [01:28<01:06,  1.72it/s]Loading train:  58%|█████▊    | 153/266 [01:28<01:03,  1.77it/s]Loading train:  58%|█████▊    | 154/266 [01:29<01:02,  1.78it/s]Loading train:  58%|█████▊    | 155/266 [01:29<00:59,  1.85it/s]Loading train:  59%|█████▊    | 156/266 [01:30<00:57,  1.93it/s]Loading train:  59%|█████▉    | 157/266 [01:30<00:55,  1.98it/s]Loading train:  59%|█████▉    | 158/266 [01:31<00:53,  2.03it/s]Loading train:  60%|█████▉    | 159/266 [01:31<00:51,  2.06it/s]Loading train:  60%|██████    | 160/266 [01:32<00:50,  2.11it/s]Loading train:  61%|██████    | 161/266 [01:32<00:49,  2.12it/s]Loading train:  61%|██████    | 162/266 [01:33<00:48,  2.13it/s]Loading train:  61%|██████▏   | 163/266 [01:33<00:48,  2.13it/s]Loading train:  62%|██████▏   | 164/266 [01:34<00:48,  2.10it/s]Loading train:  62%|██████▏   | 165/266 [01:34<00:48,  2.06it/s]Loading train:  62%|██████▏   | 166/266 [01:35<00:48,  2.04it/s]Loading train:  63%|██████▎   | 167/266 [01:35<00:47,  2.08it/s]Loading train:  63%|██████▎   | 168/266 [01:36<00:47,  2.06it/s]Loading train:  64%|██████▎   | 169/266 [01:36<00:47,  2.05it/s]Loading train:  64%|██████▍   | 170/266 [01:37<00:46,  2.07it/s]Loading train:  64%|██████▍   | 171/266 [01:37<00:45,  2.08it/s]Loading train:  65%|██████▍   | 172/266 [01:38<00:44,  2.10it/s]Loading train:  65%|██████▌   | 173/266 [01:38<00:46,  1.99it/s]Loading train:  65%|██████▌   | 174/266 [01:39<00:46,  1.98it/s]Loading train:  66%|██████▌   | 175/266 [01:39<00:46,  1.96it/s]Loading train:  66%|██████▌   | 176/266 [01:40<00:47,  1.90it/s]Loading train:  67%|██████▋   | 177/266 [01:40<00:46,  1.91it/s]Loading train:  67%|██████▋   | 178/266 [01:41<00:44,  1.96it/s]Loading train:  67%|██████▋   | 179/266 [01:41<00:44,  1.98it/s]Loading train:  68%|██████▊   | 180/266 [01:42<00:42,  2.01it/s]Loading train:  68%|██████▊   | 181/266 [01:42<00:41,  2.04it/s]Loading train:  68%|██████▊   | 182/266 [01:43<00:41,  2.05it/s]Loading train:  69%|██████▉   | 183/266 [01:43<00:40,  2.03it/s]Loading train:  69%|██████▉   | 184/266 [01:44<00:40,  2.04it/s]Loading train:  70%|██████▉   | 185/266 [01:44<00:39,  2.04it/s]Loading train:  70%|██████▉   | 186/266 [01:45<00:39,  2.03it/s]Loading train:  70%|███████   | 187/266 [01:45<00:38,  2.04it/s]Loading train:  71%|███████   | 188/266 [01:46<00:38,  2.05it/s]Loading train:  71%|███████   | 189/266 [01:46<00:37,  2.03it/s]Loading train:  71%|███████▏  | 190/266 [01:47<00:39,  1.93it/s]Loading train:  72%|███████▏  | 191/266 [01:48<00:46,  1.62it/s]Loading train:  72%|███████▏  | 192/266 [01:48<00:47,  1.56it/s]Loading train:  73%|███████▎  | 193/266 [01:49<00:48,  1.49it/s]Loading train:  73%|███████▎  | 194/266 [01:50<00:53,  1.33it/s]Loading train:  73%|███████▎  | 195/266 [01:50<00:49,  1.44it/s]Loading train:  74%|███████▎  | 196/266 [01:51<00:46,  1.52it/s]Loading train:  74%|███████▍  | 197/266 [01:52<00:42,  1.61it/s]Loading train:  74%|███████▍  | 198/266 [01:52<00:41,  1.65it/s]Loading train:  75%|███████▍  | 199/266 [01:53<00:39,  1.71it/s]Loading train:  75%|███████▌  | 200/266 [01:53<00:37,  1.77it/s]Loading train:  76%|███████▌  | 201/266 [01:54<00:35,  1.81it/s]Loading train:  76%|███████▌  | 202/266 [01:54<00:34,  1.85it/s]Loading train:  76%|███████▋  | 203/266 [01:55<00:34,  1.84it/s]Loading train:  77%|███████▋  | 204/266 [01:55<00:33,  1.86it/s]Loading train:  77%|███████▋  | 205/266 [01:56<00:32,  1.85it/s]Loading train:  77%|███████▋  | 206/266 [01:56<00:32,  1.85it/s]Loading train:  78%|███████▊  | 207/266 [01:57<00:31,  1.87it/s]Loading train:  78%|███████▊  | 208/266 [01:57<00:30,  1.89it/s]Loading train:  79%|███████▊  | 209/266 [01:58<00:29,  1.90it/s]Loading train:  79%|███████▉  | 210/266 [01:59<00:29,  1.87it/s]Loading train:  79%|███████▉  | 211/266 [01:59<00:29,  1.85it/s]Loading train:  80%|███████▉  | 212/266 [02:00<00:29,  1.80it/s]Loading train:  80%|████████  | 213/266 [02:00<00:28,  1.83it/s]Loading train:  80%|████████  | 214/266 [02:01<00:28,  1.85it/s]Loading train:  81%|████████  | 215/266 [02:01<00:26,  1.93it/s]Loading train:  81%|████████  | 216/266 [02:02<00:25,  1.99it/s]Loading train:  82%|████████▏ | 217/266 [02:02<00:24,  2.04it/s]Loading train:  82%|████████▏ | 218/266 [02:03<00:23,  2.05it/s]Loading train:  82%|████████▏ | 219/266 [02:03<00:23,  2.03it/s]Loading train:  83%|████████▎ | 220/266 [02:04<00:22,  2.04it/s]Loading train:  83%|████████▎ | 221/266 [02:04<00:22,  2.01it/s]Loading train:  83%|████████▎ | 222/266 [02:05<00:21,  2.01it/s]Loading train:  84%|████████▍ | 223/266 [02:05<00:21,  2.04it/s]Loading train:  84%|████████▍ | 224/266 [02:06<00:20,  2.05it/s]Loading train:  85%|████████▍ | 225/266 [02:06<00:20,  2.04it/s]Loading train:  85%|████████▍ | 226/266 [02:07<00:19,  2.06it/s]Loading train:  85%|████████▌ | 227/266 [02:07<00:20,  1.94it/s]Loading train:  86%|████████▌ | 228/266 [02:08<00:25,  1.49it/s]Loading train:  86%|████████▌ | 229/266 [02:09<00:28,  1.28it/s]Loading train:  86%|████████▋ | 230/266 [02:10<00:32,  1.11it/s]Loading train:  87%|████████▋ | 231/266 [02:13<00:53,  1.54s/it]Loading train:  87%|████████▋ | 232/266 [02:17<01:14,  2.19s/it]Loading train:  88%|████████▊ | 233/266 [02:21<01:26,  2.64s/it]Loading train:  88%|████████▊ | 234/266 [02:24<01:34,  2.95s/it]Loading train:  88%|████████▊ | 235/266 [02:28<01:34,  3.04s/it]Loading train:  89%|████████▊ | 236/266 [02:35<02:12,  4.43s/it]Loading train:  89%|████████▉ | 237/266 [02:40<02:10,  4.50s/it]Loading train:  89%|████████▉ | 238/266 [02:47<02:28,  5.31s/it]Loading train:  90%|████████▉ | 239/266 [02:55<02:44,  6.10s/it]Loading train:  90%|█████████ | 240/266 [03:02<02:42,  6.24s/it]Loading train:  91%|█████████ | 241/266 [03:08<02:36,  6.25s/it]Loading train:  91%|█████████ | 242/266 [03:19<03:02,  7.62s/it]Loading train:  91%|█████████▏| 243/266 [03:28<03:04,  8.02s/it]Loading train:  92%|█████████▏| 244/266 [03:37<03:02,  8.31s/it]Loading train:  92%|█████████▏| 245/266 [03:43<02:43,  7.80s/it]Loading train:  92%|█████████▏| 246/266 [03:50<02:30,  7.51s/it]Loading train:  93%|█████████▎| 247/266 [03:59<02:28,  7.79s/it]Loading train:  93%|█████████▎| 248/266 [04:07<02:25,  8.08s/it]Loading train:  94%|█████████▎| 249/266 [04:21<02:45,  9.73s/it]Loading train:  94%|█████████▍| 250/266 [04:34<02:52, 10.79s/it]Loading train:  94%|█████████▍| 251/266 [04:45<02:41, 10.75s/it]Loading train:  95%|█████████▍| 252/266 [04:54<02:22, 10.15s/it]Loading train:  95%|█████████▌| 253/266 [05:04<02:11, 10.11s/it]Loading train:  95%|█████████▌| 254/266 [05:17<02:14, 11.20s/it]Loading train:  96%|█████████▌| 255/266 [05:33<02:16, 12.41s/it]Loading train:  96%|█████████▌| 256/266 [05:47<02:08, 12.89s/it]Loading train:  97%|█████████▋| 257/266 [06:01<01:59, 13.30s/it]Loading train:  97%|█████████▋| 258/266 [06:12<01:40, 12.56s/it]Loading train:  97%|█████████▋| 259/266 [06:21<01:21, 11.62s/it]Loading train:  98%|█████████▊| 260/266 [06:35<01:13, 12.29s/it]Loading train:  98%|█████████▊| 261/266 [06:49<01:03, 12.70s/it]Loading train:  98%|█████████▊| 262/266 [07:01<00:49, 12.48s/it]Loading train:  99%|█████████▉| 263/266 [07:13<00:37, 12.39s/it]Loading train:  99%|█████████▉| 264/266 [07:23<00:23, 11.84s/it]Loading train: 100%|█████████▉| 265/266 [07:34<00:11, 11.60s/it]Loading train: 100%|██████████| 266/266 [07:48<00:00, 12.10s/it]Loading train: 100%|██████████| 266/266 [07:48<00:00,  1.76s/it]
concatenating: train:   0%|          | 0/266 [00:00<?, ?it/s]concatenating: train:   2%|▏         | 5/266 [00:00<00:05, 49.68it/s]concatenating: train:   4%|▍         | 10/266 [00:00<00:05, 49.24it/s]concatenating: train:   6%|▌         | 15/266 [00:00<00:05, 49.19it/s]concatenating: train:   8%|▊         | 20/266 [00:00<00:05, 48.87it/s]concatenating: train:   9%|▉         | 25/266 [00:00<00:04, 48.64it/s]concatenating: train:  11%|█▏        | 30/266 [00:00<00:04, 48.25it/s]concatenating: train:  14%|█▎        | 36/266 [00:00<00:04, 48.77it/s]concatenating: train:  16%|█▌        | 42/266 [00:00<00:04, 49.89it/s]concatenating: train:  18%|█▊        | 48/266 [00:00<00:04, 51.30it/s]concatenating: train:  20%|██        | 54/266 [00:01<00:04, 51.52it/s]concatenating: train:  23%|██▎       | 60/266 [00:01<00:03, 51.63it/s]concatenating: train:  25%|██▍       | 66/266 [00:01<00:03, 51.59it/s]concatenating: train:  27%|██▋       | 72/266 [00:01<00:03, 52.98it/s]concatenating: train:  29%|██▉       | 78/266 [00:01<00:03, 52.86it/s]concatenating: train:  32%|███▏      | 84/266 [00:01<00:03, 52.88it/s]concatenating: train:  34%|███▍      | 90/266 [00:01<00:03, 52.80it/s]concatenating: train:  36%|███▌      | 96/266 [00:01<00:03, 51.84it/s]concatenating: train:  38%|███▊      | 102/266 [00:01<00:03, 52.22it/s]concatenating: train:  41%|████      | 108/266 [00:02<00:03, 51.66it/s]concatenating: train:  43%|████▎     | 114/266 [00:02<00:02, 51.04it/s]concatenating: train:  45%|████▌     | 120/266 [00:02<00:02, 50.19it/s]concatenating: train:  47%|████▋     | 126/266 [00:02<00:02, 49.01it/s]concatenating: train:  49%|████▉     | 131/266 [00:02<00:02, 48.06it/s]concatenating: train:  51%|█████     | 136/266 [00:02<00:02, 47.51it/s]concatenating: train:  53%|█████▎    | 142/266 [00:02<00:02, 48.64it/s]concatenating: train:  56%|█████▌    | 148/266 [00:02<00:02, 49.68it/s]concatenating: train:  58%|█████▊    | 154/266 [00:03<00:02, 50.34it/s]concatenating: train:  60%|██████    | 160/266 [00:03<00:02, 51.16it/s]concatenating: train:  62%|██████▏   | 166/266 [00:03<00:01, 52.37it/s]concatenating: train:  65%|██████▍   | 172/266 [00:03<00:01, 53.46it/s]concatenating: train:  67%|██████▋   | 178/266 [00:03<00:01, 53.42it/s]concatenating: train:  69%|██████▉   | 184/266 [00:03<00:01, 52.51it/s]concatenating: train:  72%|███████▏  | 191/266 [00:03<00:01, 53.89it/s]concatenating: train:  74%|███████▍  | 197/266 [00:03<00:01, 52.35it/s]concatenating: train:  76%|███████▋  | 203/266 [00:03<00:01, 51.17it/s]concatenating: train:  79%|███████▊  | 209/266 [00:04<00:01, 48.23it/s]concatenating: train:  80%|████████  | 214/266 [00:04<00:01, 47.97it/s]concatenating: train:  82%|████████▏ | 219/266 [00:04<00:00, 47.25it/s]concatenating: train:  85%|████████▍ | 225/266 [00:04<00:00, 48.66it/s]concatenating: train:  86%|████████▋ | 230/266 [00:04<00:00, 47.91it/s]concatenating: train:  89%|████████▊ | 236/266 [00:04<00:00, 50.14it/s]concatenating: train:  91%|█████████ | 242/266 [00:04<00:00, 50.56it/s]concatenating: train:  93%|█████████▎| 248/266 [00:04<00:00, 50.84it/s]concatenating: train:  95%|█████████▌| 254/266 [00:05<00:00, 49.14it/s]concatenating: train:  97%|█████████▋| 259/266 [00:05<00:00, 47.25it/s]concatenating: train:  99%|█████████▉| 264/266 [00:05<00:00, 46.32it/s]concatenating: train: 100%|██████████| 266/266 [00:05<00:00, 50.24it/s]
Loading test:   0%|          | 0/4 [00:00<?, ?it/s]Loading test:  25%|██▌       | 1/4 [00:12<00:36, 12.30s/it]Loading test:  50%|█████     | 2/4 [00:14<00:18,  9.40s/it]Loading test:  75%|███████▌  | 3/4 [00:15<00:06,  6.80s/it]Loading test: 100%|██████████| 4/4 [00:16<00:00,  5.03s/it]Loading test: 100%|██████████| 4/4 [00:16<00:00,  4.14s/it]
concatenating: validation:   0%|          | 0/4 [00:00<?, ?it/s]concatenating: validation: 100%|██████████| 4/4 [00:00<00:00, 71.14it/s] min 0.0 max 13.0      7-VPL
Error in label values min 0.0 max 3.0      9-LGN
Error in label values min 0.0 max 2.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 5.0      12-MD-Pf
Error in label values min 0.0 max 30.0      13-Hb
Error in label values min 0.0 max 4.0      14-MTT
Error in label values min 0.0 max 9.0      2-AV
Error in label values min 0.0 max 2.0      4-VA
Error in label values min 0.0 max 4.0      5-VLa
Error in label values min 0.0 max 9.0      6-VLP
Error in label values min 0.0 max 5.0      7-VPL
Error in label values min 0.0 max 4.0      9-LGN
Error in label values min 0.0 max 4.0      10-MGN
Error in label values min 0.0 max 2.0      11-CM
Error in label values min 0.0 max 4.0      12-MD-Pf
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 52, 84, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 52, 84, 20)   200         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 52, 84, 20)   80          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 52, 84, 20)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 52, 84, 20)   3620        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 52, 84, 20)   80          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 52, 84, 20)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 26, 42, 20)   0           activation_2[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 26, 42, 20)   0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 26, 42, 40)   7240        dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 26, 42, 40)   160         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 26, 42, 40)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 26, 42, 40)   14440       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 26, 42, 40)   160         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 26, 42, 40)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 26, 42, 60)   0           dropout_1[0][0]                  
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 13, 21, 60)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 13, 21, 60)   0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 13, 21, 80)   43280       dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 13, 21, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 13, 21, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 13, 21, 80)   57680       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 13, 21, 80)   320         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 13, 21, 80)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 13, 21, 140)  0           dropout_2[0][0]                  
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 13, 21, 140)  0           concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 26, 42, 40)   22440       dropout_3[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 42, 100)  0           conv2d_transpose_1[0][0]         
                                                                 concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 26, 42, 40)   36040       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 26, 42, 40)   160         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 26, 42, 40)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 26, 42, 40)   14440       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 26, 42, 40)   160         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 26, 42, 40)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 26, 42, 140)  0           concatenate_3[0][0]              2020-01-21 03:54:54.731594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 03:54:54.731684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 03:54:54.731696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 03:54:54.731703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 03:54:54.889271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)

                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 26, 42, 140)  0           concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 52, 84, 20)   11220       dropout_4[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 52, 84, 40)   0           conv2d_transpose_2[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 52, 84, 20)   7220        concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 52, 84, 20)   80          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 52, 84, 20)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 52, 84, 20)   3620        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 52, 84, 20)   80          conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 52, 84, 20)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 52, 84, 60)   0           concatenate_5[0][0]              
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 52, 84, 60)   0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 52, 84, 13)   793         dropout_5[0][0]                  
==================================================================================================
Total params: 223,833
Trainable params: 223,033
Non-trainable params: 800
__________________________________________________________________________________________________
------------------------------------------------------------------
class_weights [6.33424814e-02 3.28295286e-02 7.67673374e-02 9.53872769e-03
 2.76072500e-02 7.22265877e-03 8.42010023e-02 1.14102009e-01
 8.95925983e-02 1.36122334e-02 2.90476826e-01 1.90448266e-01
 2.59081770e-04]
Train on 9721 samples, validate on 141 samples
Epoch 1/300
 - 25s - loss: 0.6821 - acc: 0.8609 - mDice: 0.2658 - val_loss: 0.6534 - val_acc: 0.9188 - val_mDice: 0.2948

Epoch 00001: val_mDice improved from -inf to 0.29480, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 2/300
 - 20s - loss: 0.4762 - acc: 0.9193 - mDice: 0.4875 - val_loss: 0.6636 - val_acc: 0.9407 - val_mDice: 0.2815

Epoch 00002: val_mDice did not improve from 0.29480
Epoch 3/300
 - 20s - loss: 0.4166 - acc: 0.9280 - mDice: 0.5517 - val_loss: 0.5940 - val_acc: 0.9346 - val_mDice: 0.3399

Epoch 00003: val_mDice improved from 0.29480 to 0.33991, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 4/300
 - 19s - loss: 0.3890 - acc: 0.9323 - mDice: 0.5815 - val_loss: 0.5762 - val_acc: 0.9477 - val_mDice: 0.3541

Epoch 00004: val_mDice improved from 0.33991 to 0.35409, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 5/300
 - 19s - loss: 0.3707 - acc: 0.9351 - mDice: 0.6012 - val_loss: 0.5805 - val_acc: 0.9413 - val_mDice: 0.3389

Epoch 00005: val_mDice did not improve from 0.35409
Epoch 6/300
 - 19s - loss: 0.3607 - acc: 0.9369 - mDice: 0.6119 - val_loss: 0.5227 - val_acc: 0.9351 - val_mDice: 0.3394

Epoch 00006: val_mDice did not improve from 0.35409
Epoch 7/300
 - 19s - loss: 0.3497 - acc: 0.9387 - mDice: 0.6238 - val_loss: 0.3773 - val_acc: 0.9497 - val_mDice: 0.3479

Epoch 00007: val_mDice did not improve from 0.35409
Epoch 8/300
 - 19s - loss: 0.3428 - acc: 0.9391 - mDice: 0.6312 - val_loss: 0.4199 - val_acc: 0.9489 - val_mDice: 0.3353

Epoch 00008: val_mDice did not improve from 0.35409
Epoch 9/300
 - 19s - loss: 0.3362 - acc: 0.9405 - mDice: 0.6382 - val_loss: 0.3973 - val_acc: 0.9501 - val_mDice: 0.3500

Epoch 00009: val_mDice did not improve from 0.35409
Epoch 10/300
 - 19s - loss: 0.3280 - acc: 0.9414 - mDice: 0.6472 - val_loss: 0.3701 - val_acc: 0.9451 - val_mDice: 0.3490

Epoch 00010: val_mDice did not improve from 0.35409
Epoch 11/300
 - 19s - loss: 0.3239 - acc: 0.9421 - mDice: 0.6515 - val_loss: 0.2792 - val_acc: 0.9418 - val_mDice: 0.3519

Epoch 00011: val_mDice did not improve from 0.35409
Epoch 12/300
 - 19s - loss: 0.3209 - acc: 0.9424 - mDice: 0.6548 - val_loss: 0.2753 - val_acc: 0.9481 - val_mDice: 0.3726

Epoch 00012: val_mDice improved from 0.35409 to 0.37264, saving model to /array/ssd/msmajdi/experiments/keras/exp6/models/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_wLRScheduler_CSFn2_NoInit_wBiasCorrection_CV_a/MultiClass_24567891011121314/sd2/best_model_weights.h5
Epoch 13/300
 - 19s - loss: 0.3174 - acc: 0.9431 - mDice: 0.6585 - val_loss: 0.2437 - val_acc: 0.9418 - val_mDice: 0.3607

Epoch 00013: val_mDice did not improve from 0.37264
Epoch 14/300
 - 19s - loss: 0.3141 - acc: 0.9433 - mDice: 0.6621 - val_loss: 0.2444 - val_acc: 0.9530 - val_mDice: 0.3664

Epoch 00014: val_mDice did not improve from 0.37264
Epoch 15/300
 - 19s - loss: 0.3084 - acc: 0.9441 - mDice: 0.6682 - val_loss: 0.2277 - val_acc: 0.9492 - val_mDice: 0.3459

Epoch 00015: val_mDice did not improve from 0.37264
Epoch 16/300
 - 19s - loss: 0.3064 - acc: 0.9443 - mDice: 0.6704 - val_loss: 0.2911 - val_acc: 0.9489 - val_mDice: 0.3614

Epoch 00016: val_mDice did not improve from 0.37264
Epoch 17/300
 - 19s - loss: 0.3071 - acc: 0.9447 - mDice: 0.6696 - val_loss: 0.2395 - val_acc: 0.9504 - val_mDice: 0.3603

Epoch 00017: val_mDice did not improve from 0.37264
Epoch 18/300
 - 19s - loss: 0.3102 - acc: 0.9442 - mDice: 0.6663 - val_loss: 0.1825 - val_acc: 0.9448 - val_mDice: 0.3558

Epoch 00018: val_mDice did not improve from 0.37264
Epoch 19/300
 - 19s - loss: 0.3048 - acc: 0.9450 - mDice: 0.6721 - val_loss: 0.2881 - val_acc: 0.9403 - val_mDice: 0.3491

Epoch 00019: val_mDice did not improve from 0.37264
Epoch 20/300
 - 19s - loss: 0.2971 - acc: 0.9457 - mDice: 0.6805 - val_loss: 0.2412 - val_acc: 0.9508 - val_mDice: 0.3543

Epoch 00020: val_mDice did not improve from 0.37264
Epoch 21/300
 - 19s - loss: 0.2995 - acc: 0.9459 - mDice: 0.6778 - val_loss: 0.1484 - val_acc: 0.9451 - val_mDice: 0.3506

Epoch 00021: val_mDice did not improve from 0.37264
Epoch 22/300
 - 19s - loss: 0.2970 - acc: 0.9458 - mDice: 0.6806 - val_loss: 0.1816 - val_acc: 0.9502 - val_mDice: 0.3517

Epoch 00022: val_mDice did not improve from 0.37264
Epoch 23/300
 - 19s - loss: 0.2944 - acc: 0.9461 - mDice: 0.6833 - val_loss: 0.1134 - val_acc: 0.9536 - val_mDice: 0.3429

Epoch 00023: val_mDice did not improve from 0.37264
Epoch 24/300
 - 19s - loss: 0.2920 - acc: 0.9463 - mDice: 0.6859 - val_loss: 0.2272 - val_acc: 0.9508 - val_mDice: 0.3558

Epoch 00024: val_mDice did not improve from 0.37264
Epoch 25/300
 - 19s - loss: 0.2931 - acc: 0.9465 - mDice: 0.6848 - val_loss: 0.1319 - val_acc: 0.9531 - val_mDice: 0.3582

Epoch 00025: val_mDice did not improve from 0.37264
Epoch 26/300
 - 19s - loss: 0.2902 - acc: 0.9465 - mDice: 0.6879 - val_loss: 0.1139 - val_acc: 0.9507 - val_mDice: 0.3578

Epoch 00026: val_mDice did not improve from 0.37264
Epoch 27/300
 - 20s - loss: 0.2919 - acc: 0.9466 - mDice: 0.6861 - val_loss: 0.2078 - val_acc: 0.9482 - val_mDice: 0.3445

Epoch 00027: val_mDice did not improve from 0.37264

Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 28/300
 - 19s - loss: 0.2816 - acc: 0.9478 - mDice: 0.6972 - val_loss: 0.1528 - val_acc: 0.9512 - val_mDice: 0.3651

Epoch 00028: val_mDice did not improve from 0.37264
Epoch 29/300
 - 19s - loss: 0.2774 - acc: 0.9485 - mDice: 0.7017 - val_loss: 0.1023 - val_acc: 0.9508 - val_mDice: 0.3559

Epoch 00029: val_mDice did not improve from 0.37264
Epoch 30/300
 - 20s - loss: 0.2755 - acc: 0.9486 - mDice: 0.7037 - val_loss: 0.1101 - val_acc: 0.9531 - val_mDice: 0.3624

Epoch 00030: val_mDice did not improve from 0.37264
Epoch 31/300
 - 20s - loss: 0.2765 - acc: 0.9485 - mDice: 0.7027 - val_loss: 0.2093 - val_acc: 0.9524 - val_mDice: 0.3368

Epoch 00031: val_mDice did not improve from 0.37264
Epoch 32/300
 - 20s - loss: 0.2744 - acc: 0.9488 - mDice: 0.7050 - val_loss: 0.1160 - val_acc: 0.9514 - val_mDice: 0.3698

Epoch 00032: val_mDice did not improve from 0.37264
Epoch 33/300
 - 20s - loss: 0.2688 - acc: 0.9492 - mDice: 0.7110 - val_loss: 0.1643 - val_acc: 0.9509 - val_mDice: 0.3626

Epoch 00033: val_mDice did not improve from 0.37264
Epoch 34/300
 - 20s - loss: 0.2721 - acc: 0.9491 - mDice: 0.7074 - val_loss: 0.1495 - val_acc: 0.9517 - val_mDice: 0.3524

Epoch 00034: val_mDice did not improve from 0.37264
Epoch 35/300
 - 19s - loss: 0.2684 - acc: 0.9493 - mDice: 0.7114 - val_loss: 0.2974 - val_acc: 0.9317 - val_mDice: 0.3186

Epoch 00035: val_mDice did not improve from 0.37264
Epoch 36/300
 - 19s - loss: 0.2697 - acc: 0.9492 - mDice: 0.7101 - val_loss: 0.2144 - val_acc: 0.9459 - val_mDice: 0.3419

Epoch 00036: val_mDice did not improve from 0.37264
Epoch 37/300
 - 20s - loss: 0.2697 - acc: 0.9495 - mDice: 0.7100 - val_loss: 0.1394 - val_acc: 0.9538 - val_mDice: 0.3684

Epoch 00037: val_mDice did not improve from 0.37264
Epoch 38/300
 - 19s - loss: 0.2675 - acc: 0.9493 - mDice: 0.7124 - val_loss: 0.1623 - val_acc: 0.9508 - val_mDice: 0.3611

Epoch 00038: val_mDice did not improve from 0.37264
Epoch 39/300
 - 19s - loss: 0.2706 - acc: 0.9494 - mDice: 0.7090 - val_loss: 0.1230 - val_acc: 0.9522 - val_mDice: 0.3651

Epoch 00039: val_mDice did not improve from 0.37264
Epoch 40/300
 - 19s - loss: 0.2658 - acc: 0.9495 - mDice: 0.7142 - val_loss: 0.2189 - val_acc: 0.9497 - val_mDice: 0.3613

Epoch 00040: val_mDice did not improve from 0.37264
Epoch 41/300
 - 19s - loss: 0.2666 - acc: 0.9496 - mDice: 0.7133 - val_loss: 0.1109 - val_acc: 0.9535 - val_mDice: 0.3528

Epoch 00041: val_mDice did not improve from 0.37264
Epoch 42/300
 - 19s - loss: 0.2635 - acc: 0.9497 - mDice: 0.7167 - val_loss: 0.1180 - val_acc: 0.9513 - val_mDice: 0.3238

Epoch 00042: val_mDice did not improve from 0.37264

Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 43/300
 - 19s - loss: 0.2593 - acc: 0.9500 - mDice: 0.7213 - val_loss: 0.1247 - val_acc: 0.9524 - val_mDice: 0.3653

Epoch 00043: val_mDice did not improve from 0.37264
Epoch 44/300
 - 19s - loss: 0.2581 - acc: 0.9504 - mDice: 0.7225 - val_loss: 0.1314 - val_acc: 0.9532 - val_mDice: 0.3403

Epoch 00044: val_mDice did not improve from 0.37264
Epoch 45/300
 - 19s - loss: 0.2574 - acc: 0.9505 - mDice: 0.7233 - val_loss: 0.1315 - val_acc: 0.9520 - val_mDice: 0.3606

Epoch 00045: val_mDice did not improve from 0.37264
Epoch 46/300
 - 19s - loss: 0.2564 - acc: 0.9506 - mDice: 0.7244 - val_loss: 0.1173 - val_acc: 0.9529 - val_mDice: 0.3561

Epoch 00046: val_mDice did not improve from 0.37264
Epoch 47/300
 - 19s - loss: 0.2551 - acc: 0.9508 - mDice: 0.7258 - val_loss: 0.1298 - val_acc: 0.9516 - val_mDice: 0.3659

Epoch 00047: val_mDice did not improve from 0.37264
Epoch 48/300
 - 19s - loss: 0.2565 - acc: 0.9507 - mDice: 0.7243 - val_loss: 0.1330 - val_acc: 0.9526 - val_mDice: 0.3532

Epoch 00048: val_mDice did not improve from 0.37264
Epoch 49/300
 - 19s - loss: 0.2561 - acc: 0.9506 - mDice: 0.7247 - val_loss: 0.1953 - val_acc: 0.9531 - val_mDice: 0.3368

Epoch 00049: val_mDice did not improve from 0.37264
Epoch 50/300
 - 19s - loss: 0.2555 - acc: 0.9508 - mDice: 0.7254 - val_loss: 0.1721 - val_acc: 0.9500 - val_mDice: 0.3615

Epoch 00050: val_mDice did not improve from 0.37264
Epoch 51/300
 - 19s - loss: 0.2542 - acc: 0.9509 - mDice: 0.7268 - val_loss: 0.1546 - val_acc: 0.9528 - val_mDice: 0.3557

Epoch 00051: val_mDice did not improve from 0.37264
Epoch 52/300
 - 19s - loss: 0.2542 - acc: 0.9509 - mDice: 0.7268 - val_loss: 0.1432 - val_acc: 0.9504 - val_mDice: 0.3620

predicting test subjects:   0%|          | 0/4 [00:00<?, ?it/s]predicting test subjects:  25%|██▌       | 1/4 [00:01<00:04,  1.39s/it]predicting test subjects:  50%|█████     | 2/4 [00:02<00:02,  1.21s/it]predicting test subjects:  75%|███████▌  | 3/4 [00:02<00:01,  1.08s/it]predicting test subjects: 100%|██████████| 4/4 [00:03<00:00,  1.04s/it]predicting test subjects: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]cp: cannot stat '/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM40_Res_Unet2_NL3_LS_MyDice_US1_CSFn2_NoInit_wBiasCorrection_CV_a/sd0/vimp*': No such file or directory
cp: cannot stat '/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM30_Res_Unet2_NL3_LS_MyDice_US1_CSFn2_NoInit_wBiasCorrection_CV_a/sd1/vimp*': No such file or directory
cp: cannot stat '/array/ssd/msmajdi/experiments/keras/exp6/results/sE12_Cascade_FM20_Res_Unet2_NL3_LS_MyDice_US1_CSFn2_NoInit_wBiasCorrection_CV_a/sd2/vimp*': No such file or directory

  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.22it/s] 50%|█████     | 2/4 [00:00<00:00,  3.36it/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.50it/s]100%|██████████| 4/4 [00:01<00:00,  3.40it/s]100%|██████████| 4/4 [00:01<00:00,  3.48it/s]

Epoch 00052: val_mDice did not improve from 0.37264
Restoring model weights from the end of the best epoch
Epoch 00052: early stopping
{'val_loss': [0.6533997109595765, 0.6635581094322475, 0.5940089978224842, 0.5761969795463778, 0.580509937189995, 0.5227049206588286, 0.3773253363497714, 0.41992888002530904, 0.39733175584610475, 0.37006854123257577, 0.27923090734803085, 0.27532962593662796, 0.24367936919872643, 0.2444147137885398, 0.22772599836969945, 0.29111201448220736, 0.23945688746922406, 0.18246303963745739, 0.2881026798528982, 0.24124088553639117, 0.14842816127847272, 0.18155113634065534, 0.11335521242878538, 0.22723269494290046, 0.13194032488985263, 0.11393143979053125, 0.20778546563567632, 0.1527613907497614, 0.10225632348170517, 0.11007758108436638, 0.2093152954963082, 0.11595139789877208, 0.16433914745530337, 0.1495420940167515, 0.2973769671475211, 0.21438614215622556, 0.13936532798705373, 0.1623412995790759, 0.12302172338550395, 0.21894330230825548, 0.11091103539504904, 0.11799593444200272, 0.1246714858034401, 0.13137896565046717, 0.13150881083206611, 0.11726317862511104, 0.1297908088205554, 0.13304097556793099, 0.1953026528622136, 0.17205406955264985, 0.15463040156144622, 0.14319796572234614], 'val_acc': [0.9188050407889887, 0.9407440931239026, 0.934611493814076, 0.9476966621182489, 0.9412993970492207, 0.9350969630775722, 0.9496742935045391, 0.948927400805426, 0.9501402834628491, 0.9451020254310987, 0.9418140894977759, 0.9480912110484239, 0.941805973120615, 0.9530417910704376, 0.9492310251749999, 0.9489371489125786, 0.9503773408578643, 0.9447837958099149, 0.9403170702305246, 0.9507995066067851, 0.9451312528434375, 0.9502101054428317, 0.9536051944637975, 0.9507800146197596, 0.9530823771835218, 0.9507459131538445, 0.9482438369845667, 0.9512460045780696, 0.9508189774574117, 0.9530823729562421, 0.9523955625845185, 0.951435978530992, 0.9509326447831824, 0.9517055014346508, 0.9316742572378605, 0.9458554112319405, 0.9537967874648723, 0.9507881394514801, 0.9522299504449182, 0.94965967557109, 0.9534980413761545, 0.9513482962939757, 0.9523760748247728, 0.9531668086423941, 0.9519799068464455, 0.9528696731472692, 0.9516210742030583, 0.9526098941234832, 0.9530563963220474, 0.9499698014969521, 0.9528177198788799, 0.9504098148210675], 'val_mDice': [0.29479707703522756, 0.2814801095436651, 0.33990937555935363, 0.35408888514160264, 0.3388857323649927, 0.3393692158638163, 0.3479028974019044, 0.33530320523055734, 0.35003782903894465, 0.34902535114727967, 0.35193596325867565, 0.3726429656042275, 0.36066186660570454, 0.3664327439264203, 0.34588564794959753, 0.361375204214813, 0.36034983523348546, 0.35583150672151687, 0.3490974252105605, 0.35428138383736846, 0.35059230653106743, 0.3517207059454411, 0.3428757000476756, 0.35583271202466166, 0.358217567839521, 0.3578056902327436, 0.3444749888376141, 0.36511032399556315, 0.3559479962849448, 0.36241964192677895, 0.33679974840042437, 0.36975531871859907, 0.3625515874818707, 0.352438599505323, 0.3186174762164447, 0.3419250968500232, 0.3684441903804211, 0.361075407647072, 0.3650605245262173, 0.36128235729873603, 0.3528217411210351, 0.3238363363218646, 0.36533777595411804, 0.34033100919943327, 0.3605789342459212, 0.35613264597899524, 0.36588228393531014, 0.35324266510652313, 0.33682722096324813, 0.36152754745162125, 0.3557138224231436, 0.36202131063802867], 'loss': [0.6821215627566494, 0.476157288954295, 0.41664719560756786, 0.3889507407416607, 0.37074022143604013, 0.360720016512161, 0.3496652684692689, 0.34276877687480417, 0.33624968012411055, 0.3279590357013377, 0.32392064060047443, 0.32085031113093826, 0.3173890190842637, 0.3141044753040015, 0.3084203145739694, 0.30639077995419683, 0.3070713716418566, 0.3102207319007564, 0.3048193407008437, 0.2970631940566443, 0.2995208554240414, 0.29697864153860726, 0.2944058337251832, 0.29203044724076904, 0.2930658913795478, 0.2901784046701094, 0.2918816145960532, 0.2815678990613211, 0.2773930911615103, 0.2755334801100468, 0.2765007370502514, 0.2743688993046355, 0.2688200424702468, 0.27210874212473685, 0.2684128497218585, 0.2696584170124141, 0.26970599722180416, 0.26751793385557576, 0.2706045413836446, 0.2658395887481959, 0.2666258929584169, 0.263506820311383, 0.259278360125386, 0.2581259306168044, 0.25742945813884976, 0.25639786132323056, 0.25510474708379266, 0.2564594662063174, 0.2561417335609683, 0.2554740342457664, 0.2541745163852106, 0.25417900259278214], 'acc': [0.8608505365874386, 0.9193337293407479, 0.9280028397464762, 0.9322523306382682, 0.9350970370781028, 0.9369410214300522, 0.938656227436101, 0.9391214291311907, 0.9405356332262704, 0.9413757857478287, 0.942103954729334, 0.9423893668219675, 0.943082468259017, 0.9433076376594176, 0.9440809527109121, 0.9443491270617931, 0.9447472296726457, 0.9442249898642697, 0.9449597757032606, 0.9456576346284187, 0.9459170000185975, 0.9458294389804883, 0.9460755218732089, 0.9463222155648486, 0.9464712219772461, 0.9465209863433842, 0.9465582656610185, 0.9478281971702187, 0.948542330131623, 0.9486489451271236, 0.9485253739131354, 0.9488050159560887, 0.9491760133715915, 0.9491170655367687, 0.9492783885337653, 0.94922226574818, 0.9494892853472093, 0.9493341798179497, 0.9493787626329364, 0.9495330677026584, 0.9495838895806461, 0.9497339793186523, 0.9500063923562632, 0.9504285637469783, 0.9505486263744184, 0.9506042299048914, 0.9508114540948506, 0.9507162138067049, 0.9506152517731059, 0.9508184485138229, 0.9508555413040528, 0.9509344594663289], 'mDice': [0.26575416546204206, 0.48751368289707053, 0.5516704569755911, 0.581530988982842, 0.6011683597161304, 0.6119138365075882, 0.6238075477398665, 0.6312488349358274, 0.638228448661018, 0.6471668834604607, 0.6514987513835312, 0.6548090863345569, 0.658526627687365, 0.6620725724513106, 0.6682141788787477, 0.6704015279978762, 0.6696413750879535, 0.6662513664040763, 0.6720906378813039, 0.6804683227841225, 0.6778023013431782, 0.6805556621366096, 0.6833482363926459, 0.6859180104915233, 0.6847735566190825, 0.6879058232763375, 0.6860803070903229, 0.6971982325915936, 0.7016774804279915, 0.7037041791487765, 0.7026530668843052, 0.7049588179799242, 0.7109559505040502, 0.7073998507750328, 0.711414266933741, 0.7100779421350836, 0.7100123195488283, 0.7123790758036989, 0.7090354699519985, 0.7141941819443843, 0.7133429715287819, 0.7167212900869175, 0.7212781358397972, 0.722510133770596, 0.723267878463712, 0.7243746134948122, 0.7257656386136861, 0.72430655542635, 0.7246502998029483, 0.7253670031954974, 0.7267687781769949, 0.7267575919192805], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]}
CrossVal ['a']
2020-01-21 04:11:52.002486: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-21 04:11:56.407027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:08:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-21 04:11:56.407085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 04:11:56.817712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 04:11:56.817775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 04:11:56.817785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 04:11:56.819157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['b']
TypeExperiment 10
CrossVal ['b']
Traceback (most recent call last):
  File "main.py", line 1905, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1886, in Run_Csfn_with_Best_WMn_architecture
    predict_Thalamus_For_SD0(UserInfoB)
  File "main.py", line 1842, in predict_Thalamus_For_SD0
    Run(UserI, IV)
  File "main.py", line 230, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 99, in Loop_Over_Nuclei
    Flag_Thalmaus_NLayers = check_if_num_Layers_fit(UserI)
  File "main.py", line 49, in check_if_num_Layers_fit
    temp_params = preAnalysis(temp_params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 569, in preAnalysis
    params = find_correctNumLayers(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 501, in find_correctNumLayers
    MinInputSize = func_MinInputSize(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 495, in func_MinInputSize
    inputSizes = params.directories.Test.Input.inputSizes if params.WhichExperiment.TestOnly else np.concatenate((params.directories.Train.Input.inputSizes , params.directories.Test.Input.inputSizes),axis=0)
AttributeError: type object 'Input' has no attribute 'inputSizes'
2020-01-21 04:12:00.021120: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-21 04:12:04.441076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:08:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-21 04:12:04.441135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 04:12:04.854219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 04:12:04.854293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 04:12:04.854304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 04:12:04.854784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['c']
TypeExperiment 10
CrossVal ['c']
Traceback (most recent call last):
  File "main.py", line 1905, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1886, in Run_Csfn_with_Best_WMn_architecture
    predict_Thalamus_For_SD0(UserInfoB)
  File "main.py", line 1842, in predict_Thalamus_For_SD0
    Run(UserI, IV)
  File "main.py", line 230, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 99, in Loop_Over_Nuclei
    Flag_Thalmaus_NLayers = check_if_num_Layers_fit(UserI)
  File "main.py", line 49, in check_if_num_Layers_fit
    temp_params = preAnalysis(temp_params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 569, in preAnalysis
    params = find_correctNumLayers(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 501, in find_correctNumLayers
    MinInputSize = func_MinInputSize(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 495, in func_MinInputSize
    inputSizes = params.directories.Test.Input.inputSizes if params.WhichExperiment.TestOnly else np.concatenate((params.directories.Train.Input.inputSizes , params.directories.Test.Input.inputSizes),axis=0)
AttributeError: type object 'Input' has no attribute 'inputSizes'
2020-01-21 04:12:08.041292: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-21 04:12:12.551773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:08:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2020-01-21 04:12:12.551839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 04:12:13.055882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 04:12:13.055959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 04:12:13.055972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 04:12:13.056498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15117 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
/array/ssd/msmajdi/anaconda3/envs/new-env-fb/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
----------+++ 
CrossVal ['d']
TypeExperiment 10
CrossVal ['d']
Traceback (most recent call last):
  File "main.py", line 1905, in <module>
    Run_Csfn_with_Best_WMn_architecture(UserInfoB)
  File "main.py", line 1886, in Run_Csfn_with_Best_WMn_architecture
    predict_Thalamus_For_SD0(UserInfoB)
  File "main.py", line 1842, in predict_Thalamus_For_SD0
    Run(UserI, IV)
  File "main.py", line 230, in Run
    else: Loop_Over_Nuclei(UserInfoB)
  File "main.py", line 99, in Loop_Over_Nuclei
    Flag_Thalmaus_NLayers = check_if_num_Layers_fit(UserI)
  File "main.py", line 49, in check_if_num_Layers_fit
    temp_params = preAnalysis(temp_params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 569, in preAnalysis
    params = find_correctNumLayers(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 501, in find_correctNumLayers
    MinInputSize = func_MinInputSize(params)
  File "/array/ssd/msmajdi/code/thalamus/keras/otherFuncs/datasets.py", line 495, in func_MinInputSize
    inputSizes = params.directories.Test.Input.inputSizes if params.WhichExperiment.TestOnly else np.concatenate((params.directories.Train.Input.inputSizes , params.directories.Test.Input.inputSizes),axis=0)
AttributeError: type object 'Input' has no attribute 'inputSizes'
